{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_tokenizer(x):\n",
    "    retokenize = RegexpTokenizer(\"[^\\d\\W]+\")\n",
    "    return retokenize.tokenize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_recommendation(subreddit, username):\n",
    "    # 데이터 로드\n",
    "    data = pd.read_csv('./data/combined/{}.csv'.format(subreddit), header = 0, low_memory = False)\n",
    "    \n",
    "    # NaN 수정\n",
    "    data['title'].fillna('', inplace = True)\n",
    "    data['selftext'].fillna('', inplace=True)\n",
    "\n",
    "    # 제목과 내용 합침\n",
    "    data['total'] = data['title'] + data['selftext']\n",
    "        \n",
    "    # 모델이 있으면 모델 불러오기\n",
    "    try:\n",
    "        model = Doc2Vec.load('./doc2vec/{}'.format(subreddit))\n",
    "\n",
    "    # 없을 경우 모델 생성\n",
    "    except:\n",
    "        # 토크나이저: 숫자가 아닌 문자만 처리\n",
    "        data['token'] = data['total'].apply(reg_tokenizer)\n",
    "\n",
    "        # 이중 리스트로 구성 (for gensim input)\n",
    "        mixed_sentences = data['token'].tolist()\n",
    "        \n",
    "        # 소문자로 변환\n",
    "        sentences = []\n",
    "        for sentence in mixed_sentences:\n",
    "            tmp = list(map(lambda x:x.lower(),sentence))\n",
    "            sentences.append(tmp)\n",
    "\n",
    "        # 도큐먼트 index - 토큰화한 도큐먼트 페어\n",
    "        index_document_pair = [\n",
    "            (text, [f\"{i}\",]) for i, text in enumerate(sentences)\n",
    "        ]\n",
    "\n",
    "        # 학습\n",
    "        TRAIN_documents = [TaggedDocument(words=text, tags=tags) for text, tags in index_document_pair]\n",
    "        model = Doc2Vec(TRAIN_documents, vector_size=5, window=3, epochs=40, min_count=0, workers=4)\n",
    "\n",
    "        # 모델 저장\n",
    "        model.save('./doc2vec/{}'.format(subreddit))\n",
    "\n",
    "    # 사용자가 작성한 글의 인덱스 찾기\n",
    "    idx_list = data[data['author'] == username].index.tolist()\n",
    "\n",
    "    # 인덱스에 해당하는 selftext의 inferred_vector 구하기\n",
    "    vectors = []\n",
    "    for idx in idx_list:\n",
    "        st = [data.iloc[idx]['total']]\n",
    "        vectors.append(model.infer_vector(st))\n",
    "\n",
    "    # 모델 기반 유사한 문서 찾고 해당 문서 작성자를 순위별로 딕셔너리화\n",
    "    user_dict = {key: set() for key in [x for x in range(1,21)]}\n",
    "    for inferred_vector in vectors:\n",
    "        sims = model.docvecs.most_similar([inferred_vector], topn=20)\n",
    "        idx = 0\n",
    "        for sim in sims:\n",
    "            idx += 1\n",
    "            user_dict[idx].add(data.iloc[int(sim[0])]['author'])\n",
    "\n",
    "    # 딕셔너리에서 본인과 삭제된 사용자를 제외한 유저네임 top 5를 출력\n",
    "    top = []\n",
    "    for idx in user_dict.keys():\n",
    "        users = user_dict[idx]\n",
    "        for u in users:\n",
    "            if u == username:\n",
    "                pass\n",
    "            elif u == '[deleted]':\n",
    "                pass\n",
    "            else:\n",
    "                top.append(u)\n",
    "\n",
    "    for idx in range(5):\n",
    "        print(top[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum2610\n",
      "WendyDayy\n",
      "manbeer0071995\n",
      "baejaan\n",
      "funkyfelis\n"
     ]
    }
   ],
   "source": [
    "user_recommendation('twice', 'axinld')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
