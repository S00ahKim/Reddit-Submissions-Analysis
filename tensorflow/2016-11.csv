,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2016-11-1,2016,11,1,17,5ai04c,RESTapi with Tensorflow serving,https://www.reddit.com/r/tensorflow/comments/5ai04c/restapi_with_tensorflow_serving/,leb_broth,1477989174,"Which REST api is the most recommended to ise with Tensorflow serving? I'm thinking to use Web2py but a little bit lost on how to invoke Tensorflow serving from a web2py frontend. My web app is just a simple vector of couple of numerical values inferencing a trained TF model. Would I still need the ""serving"" functionality for TF in my case? I really appreciate a link or an a generap explanatiion. Thanks.",2,1
1,2016-11-2,2016,11,2,10,5anbvh,Is there any documentation on how to get numerical output from FeatureColumns?,https://www.reddit.com/r/tensorflow/comments/5anbvh/is_there_any_documentation_on_how_to_get/,Megatron_McLargeHuge,1478051985,"FeatureColumns provide nice transformations like bucketization and hashing but they seem like a black box that only works with tf.learn classifiers. Is there any documented way to use them with custom-defined models, getting numerical data to feed into tensor inputs?",0,2
2,2016-11-4,2016,11,4,4,5az07u,Differentiable sorting tensors,https://www.reddit.com/r/tensorflow/comments/5az07u/differentiable_sorting_tensors/,sunrisetofu,1478203082,"hi,

I have an intermediate tensor, say of shape [10,] that Id like to sort the values before passing it off to other parts of the graph.
The hope is to have the whole graph differentiable as I will be training end to end.

The sort at every iteration will be sparse, as only 1 or 2 swaps are needed.

Anyway to hack this to make it end-to-end trainable despite the discontinuities introduced when elements in tensor are swapped?
",0,2
3,2016-11-7,2016,11,7,2,5bgzmf,Looking for info on export_meta_graph,https://www.reddit.com/r/tensorflow/comments/5bgzmf/looking_for_info_on_export_meta_graph/,claytantor,1478454730,[removed],1,1
4,2016-11-7,2016,11,7,8,5biwpg,Linear Regression with Exponential Model,https://www.reddit.com/r/tensorflow/comments/5biwpg/linear_regression_with_exponential_model/,Boozybrain,1478475904,"I can fit y=mx+b with no problem and now I'm trying to modify what I have to fit y=Ax^b+C but have yet to figure out why this isn't finding a value for B.  Also it's really slow due to the nested loop (how can I continually train on a full set of data?).  I know it's slow because I have a nested loop.  Surely I can set X,Y to the full array instead of looping through each point, right?  But I'm lost as to why it keeps giving me 0 for B.

[Code here](http://pastebin.com/zTBKt5XM)

[CSV Data here](http://pastebin.com/BMyvMnt1)",1,1
5,2016-11-8,2016,11,8,2,5bngam,Traffic in London episode II: Predicting congestion with Tensorflow,https://www.reddit.com/r/tensorflow/comments/5bngam/traffic_in_london_episode_ii_predicting/,fhoffa,1478539541,,0,3
6,2016-11-8,2016,11,8,7,5bpcgc,PCIe Bandwidth for Workstation,https://www.reddit.com/r/tensorflow/comments/5bpcgc/pcie_bandwidth_for_workstation/,Gio_Gats,1478557637,"Working on a build that's part workstation, part gaming rig, and part deep-learning research platform.  I'm trying to eliminate bottlenecks wherever possible to get the most out of my hardware.
I'm planning to put two GTX 1070s, a 10Gb/s networking card, and a NVMe SSD into the PCIe slots on a MSI X99A Gaming Pro Carbon with an Intel i7-6850K (40 PCIe lanes).  
The manual tells me I can get x8/x16/x8/x8 bandwidth out of those slots.  With those limitations, am I better off sticking with 1Gb/s networking and a SATAIII?  
I know I probably won't see much bottleneck while gaming, but what about running optimized deep learning libraries like TensorFlow?  ",1,1
7,2016-11-9,2016,11,9,1,5bu4b1,How to visualize a predicted image?,https://www.reddit.com/r/tensorflow/comments/5bu4b1/how_to_visualize_a_predicted_image/,rumirama,1478621694,"Hi there.

I'm doing a deep neural regression network with images and I would like to visually compare the image predicted with the image in the test set. I already did the net and it is working fine comparing both and giving me the test cost and accuracy, my issue is to ""extract"" the image so I can visualize the predicted image.

I'm not sure if this is a dumb question or not, but I'm struggling to do that. Does anyone have any idea/suggestion on how to visualize that predicted image? 

Thank you in advance.

 (I'm not sure if I'm making myself clear, so if you don't understand any part, please let me know)",0,2
8,2016-11-9,2016,11,9,3,5bv0cz,I wrote an introductory guide to TensorFlow for new programmers,https://www.reddit.com/r/tensorflow/comments/5bv0cz/i_wrote_an_introductory_guide_to_tensorflow_for/,CarbonFire,1478630113,,0,14
9,2016-11-10,2016,11,10,1,5c27dk,Trouble building with Bazel - couldn't connect to server,https://www.reddit.com/r/tensorflow/comments/5c27dk/trouble_building_with_bazel_couldnt_connect_to/,push_pop,1478710505,"Hi all,
Finally trying to dig into code after following work and news about Tensorflow, etc... for quite a while.

I'm currently following the instructions [here](https://github.com/tensorflow/models/tree/master/inception#getting-started) to download and convert imageNet.

So I got to the part to build the script and I execute:

    bazel build inception/download_and_preprocess_imagenet

It goes through some process for maybe an hour or two, then at the end it finally outputs:

    Error: couldn't connect to server at '/home/.../server/server.socket' after 60 seconds.

Has anyone seen this issue before? Btw, I am using the Ubuntu subsystem(14.04) build into Windows 10.
",0,2
10,2016-11-13,2016,11,13,1,5cl913,Using deep learning to remove glasses from faces,https://www.reddit.com/r/tensorflow/comments/5cl913/using_deep_learning_to_remove_glasses_from_faces/,mwakanosya,1478969873,,0,2
11,2016-11-13,2016,11,13,23,5cq2vl,What do you not like about tensorflow?,https://www.reddit.com/r/tensorflow/comments/5cq2vl/what_do_you_not_like_about_tensorflow/,holdenlee,1479046361,"What problems/inefficiencies are there with tensorflow? For example, what something that is conceptually simple, but would be clunky to write out in tensorflow? What's missing from existing high-level libraries?

I would like to develop a higher-level language that compiles down to tensorflow (esp. for the tensorflow graph). ",3,2
12,2016-11-16,2016,11,16,1,5d3g0m,Tutorial: TensorFlow saving/restoring and mixing multiple models,https://www.reddit.com/r/tensorflow/comments/5d3g0m/tutorial_tensorflow_savingrestoring_and_mixing/,morgangiraud,1479227694,,0,3
13,2016-11-16,2016,11,16,23,5d9dkh,Using LSTM for text generation,https://www.reddit.com/r/tensorflow/comments/5d9dkh/using_lstm_for_text_generation/,mp5sdk3,1479305628,"I'm trying to build some kind of text generation model where given 3 parameters, x_1, x_2 and x_3, the model will be able to generate a set of sentences ""&lt;something related to x_1&gt;&lt;eos&gt;&lt;something related to x_2&gt;&lt;eos&gt;&lt;something related to x_3&gt;&lt;eos&gt;"". The LSTM tutorial(ptb_lm.py) involves generating a sentence with no prior input. However, I want to use the three parameters to generate the sentences.
So far, I've gotten word embeddings for the words in my training data, but I'm unable to proceed forward and modify the code in ptb_lm.py. Any help in this regard is appreciated.",0,3
14,2016-11-22,2016,11,22,0,5e4gyu,Tensorflow ValueError on session.run on batch training with variable timesteps,https://www.reddit.com/r/tensorflow/comments/5e4gyu/tensorflow_valueerror_on_sessionrun_on_batch/,starsmiling,1479740938,"I would like to train RNN on tensorflow for sequential data. Since different samples have different sequence length, I want to use batch training and in each bacth samples obtain the same sequence length (timesteps). For each iteration, samples are grabbed with size [batch_size, timesteps, dim] to fed into RNN. However, since timesteps are changing between batches, I got an ValueError: setting an array element with a sequence. Can anyone helps me please? Anything is appreciated!",2,1
15,2016-11-22,2016,11,22,0,5e4p5k,TFLearn: Build HDF5 Image Dataset vs Image PreLoader,https://www.reddit.com/r/tensorflow/comments/5e4p5k/tflearn_build_hdf5_image_dataset_vs_image/,bumangues9,1479743564,"When is the best time to use either of the two options of feeding data to a training model? I thought HDF5 was meant for large datasets, but when I used it, the training took a lot longer than expected (over 4 hours for 1 epoch, 25k images, using 2 x GTX 1080).",1,3
16,2016-11-22,2016,11,22,23,5eb3k2,How do I apply shared weights to DNNs in TensorFlow?,https://www.reddit.com/r/tensorflow/comments/5eb3k2/how_do_i_apply_shared_weights_to_dnns_in/,rumirama,1479826530,,0,2
17,2016-11-23,2016,11,23,3,5ecf1t,Running optimize_for_inference on InceptionV1 results in different output tensors?,https://www.reddit.com/r/tensorflow/comments/5ecf1t/running_optimize_for_inference_on_inceptionv1/,vade,1479840691,"I have a C++ application which is able to run Tensorflow models, both Inception V1 and V3.

I am also able to successfully run both optimize_for_inference and quantize_graph on my InceptionV3 graph / protocol buffer

However, running optimize_for_inference on InceptionV1 (inception5h.zip from Tensorflow, which I believe is also referred to as GoogleNet?), results in a much much smaller graph size (17 MB versus 54 MB), but it also is no longer functional with my application.

Running the Optimized graph results in errors stating the tensor named 'output' is no longer available.

I understand optimize_for_inference makes changes to the structure of the graph removing unnecessary nodes. However, I specify my outputs as such:

    $ bazel-bin/tensorflow/python/tools/optimize_for_inference --input=/Users/vade/Documents/Repositories/Synopsis/Synopsis/Synopsis/TensorFlowAnalyzer/models/inception5h/tensorflow_inceptionV2_graph.pb --output=/Users/vade/Documents/Repositories/Synopsis/Synopsis/Synopsis/TensorFlowAnalyzer/models/inception5h/tensorflow_inceptionV2_graph_optimized.pb --input_names=input --output_names=output,softmax0

Which in my mind would keep my output tensors named the same.

Running the resulting graph (tensorflow_inceptionV2_graph_optimized.pb) through my C++ app results in this error:

     Running model failed: Not found: FetchOutputs node output: not found


How can I resolve this issue, or deduce the new output tensor name?

Thank you.",0,1
18,2016-11-23,2016,11,23,4,5ecqdt,retraining inception for single category,https://www.reddit.com/r/tensorflow/comments/5ecqdt/retraining_inception_for_single_category/,idrsa_pub,1479843940,"Hi,I'm new to deep learning, I'v managed to retrain inception for couple of categories but I'm getting unified result i.e 100% divided by all categories, what I'm looking for is to prediction how likely this image might be in X category, in other words can I retrain model for single category against everything else ?  to get output similar to google's vision API https://cloud.google.com/vision/",1,2
19,2016-11-24,2016,11,24,1,5eibbp,Is there any implementation of SSD: Single Shot Multibox Detector in Tensorflow,https://www.reddit.com/r/tensorflow/comments/5eibbp/is_there_any_implementation_of_ssd_single_shot/,kudaphan,1479920118,,0,3
20,2016-11-24,2016,11,24,4,5ej8as,TensorFlow Iris Data Demo -- How Long Should It Take?,https://www.reddit.com/r/tensorflow/comments/5ej8as/tensorflow_iris_data_demo_how_long_should_it_take/,deeayecee,1479929612,"I'm learning TensorFlow and have recreated the iris demo from the website:

https://www.tensorflow.org/versions/r0.11/tutorials/tflearn/index.html#tf-contrib-learn-quickstart

I've gotten to the 'classifier.fit' phase, running on a CPU-only machine. The training has taken at least an hour for 2000 iterations and I'm wondering if this timing is typical or if something has gone wrong with either my installation or implementation.",2,3
21,2016-11-24,2016,11,24,13,5elz14,GPU version on virtualbox?,https://www.reddit.com/r/tensorflow/comments/5elz14/gpu_version_on_virtualbox/,[deleted],1479963279,[deleted],2,1
22,2016-11-25,2016,11,25,8,5eqik2,Unstable accuracy values,https://www.reddit.com/r/tensorflow/comments/5eqik2/unstable_accuracy_values/,vin_kaushik,1480029417,"I am trying to run a binary classifier in Tensorflow. For every run of the program I get different accuracy values and this is driving me nuts. I've been tweaking parameters for hours now. 

This follows from the question I posted on SO: http://stackoverflow.com/questions/40709870/changing-accuracy-value-and-no-change-in-loss-value-in-binary-classification-usi

Please tell me how I can get a consistent accuracy every time I execute my program!",0,1
23,2016-11-26,2016,11,26,0,5etzuz,Optimizer that takes difference between buckets into account?,https://www.reddit.com/r/tensorflow/comments/5etzuz/optimizer_that_takes_difference_between_buckets/,MrFromEurope,1480086775,"I created a TensorFlow network with an result vector of 50 elements. Is it somehow possible to take into account, that e.g while learning the actual result would be bucket 34, but a prediction of bucket 35 is much better than a prediction for bucket 3.",0,1
24,2016-11-26,2016,11,26,1,5eueuq,Tensorflow: How to freeze a model and serve it with a python API,https://www.reddit.com/r/tensorflow/comments/5eueuq/tensorflow_how_to_freeze_a_model_and_serve_it/,morgangiraud,1480091942,,0,5
25,2016-11-26,2016,11,26,8,5ewczr,Installation didn't work T___T,https://www.reddit.com/r/tensorflow/comments/5ewczr/installation_didnt_work_t_t/,senpai_eric,1480114914,"I did everything correctly T__T ... went through the whole tutorial on the tensorflow.org website seemingly successful. Of course not without any major errors which I managed to fix over many hours. I've spent so long working with f***ing ubuntu... If anyone has any suggestions on how I can fix this error I would be eternally grateful

&gt;&gt;&gt; import tensorflow

Traceback (most recent call last):

  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;

  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py"", line 23, in &lt;module&gt;

  from tensorflow.python import *

  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py"", line 48, in &lt;module&gt;

  from tensorflow.python import pywrap_tensorflow

  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in &lt;module&gt;

  _pywrap_tensorflow = swig_import_helper()

  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper

  _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)

  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module

  return load_dynamic(name, filename, file)

  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic

  return _load(spec)

ImportError: libcudart.so.7.5: cannot open shared object file: No such file or directory

",1,1
26,2016-11-27,2016,11,27,3,5f0vk1,Conceptual basics with Distributed TensorFlow,https://www.reddit.com/r/tensorflow/comments/5f0vk1/conceptual_basics_with_distributed_tensorflow/,ujjwal-researcher,1480185960,"Let me describe the cluster setup first :

I have two nodes (each with 2 GPUs). I refer to them as Node A and Node B

Each node has its own SSD storage.

OAR is the cluster manager that is used.

I have gone through the Distributed TensorFlow documentation but there are some functional basics I could not understand properly and hence this question.

Consider the following situation :

a) I have copied around 600 GB of data on Node A.

b) I can use OAR to specifically ask for allocation of 4 GPUs across the two nodes.

If I want to use Distributed TensorFlow to train a model :

a) How do I specify network addresses to tf.train.ClusterSpec ? What are those network addresses ? In the documentation are names such as localhost:2222 the same names reserved for a particular node with the cluster manager ?

b) My data is copied to node A. During training will TensorFlow itself be responsible for sending this data as input to the GPU that is on node B ?

c) Will I need to manually create the TensorFlow Graph for each GPU on each node using tf.device() ?

d) If I also want to use some additional CPU nodes will I have to have their names beforehand and put them in the code ?",0,1
27,2016-11-27,2016,11,27,6,5f1s1w,Has google made any sort of announcement about supporting a GPU version on Windows?,https://www.reddit.com/r/tensorflow/comments/5f1s1w/has_google_made_any_sort_of_announcement_about/,senpai_eric,1480196726,,2,4
28,2016-11-28,2016,11,28,2,5f6itm,Implementing a linear regression for a real scenario with Tensorflow,https://www.reddit.com/r/tensorflow/comments/5f6itm/implementing_a_linear_regression_for_a_real/,wesovi,1480269510,,0,2
29,2016-11-28,2016,11,28,6,5f7qxt,Tensorflow doesn't utilize GPU after the update.,https://www.reddit.com/r/tensorflow/comments/5f7qxt/tensorflow_doesnt_utilize_gpu_after_the_update/,[deleted],1480283216,[deleted],4,4
30,2016-11-30,2016,11,30,12,5fncv1,Testing our own inputs to the MNIST tutorial,https://www.reddit.com/r/tensorflow/comments/5fncv1/testing_our_own_inputs_to_the_mnist_tutorial/,timex40,1480476872,"So I've gone through and completed the MNIST for ML Beginners tutorial (https://www.tensorflow.org/versions/r0.12/tutorials/mnist/beginners/index.html) which made sense. 

By the end of the tutorial  the model is trained using all the MNIST training data, and its accuracy has been shown to be ~92%. 

If I had another single input - say, I drew my own 28x28 pixel image and wanted to see what it would be classified as - how would I go about this? (Assuming I have already loaded the new input as a 728 element numpy array.)

Thanks for any help 


",0,2
31,2016-11-30,2016,11,30,15,5fo7ze,TensorFlow now supports Windows with the release of v0.12.0 RC0! (Both CPU and GPU builds available),https://www.reddit.com/r/tensorflow/comments/5fo7ze/tensorflow_now_supports_windows_with_the_release/,ElSarcastro,1480488584,,2,15
