,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2016-5-1,2016,5,1,17,4h8bmy,MSE for matrices?,https://www.reddit.com/r/tensorflow/comments/4h8bmy/mse_for_matrices/,[deleted],1462091657,[deleted],0,1
1,2016-5-1,2016,5,1,23,4h9azw,"Tensorflow conv2d_transpose error ""Number of rows of out_backprop doesn't match computed rows""",https://www.reddit.com/r/tensorflow/comments/4h9azw/tensorflow_conv2d_transpose_error_number_of_rows/,winged_elite,1462113740,"I am creating a convolution autoencoder in tensorflow. The exact error I got is:

&gt; tensorflow.python.framework.errors.InvalidArgumentError: Conv2DBackpropInput: Number of rows of out_backprop doesn't match computed: actual = 8, computed = 12
	 [[Node: conv2d_transpose = Conv2DBackpropInput[T=DT_FLOAT, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](conv2d_transpose/output_shape, Variable_1/read, MaxPool_1)]]

Relevant code:

    l1d = tf.nn.relu(tf.nn.conv2d_transpose(l1da, w2, [10, 12, 12, 32], strides=[1, 1, 1, 1], padding='SAME'))

where 

    w2 = tf.Variable(tf.random_normal([5, 5, 32, 64], stddev=0.01))

I checked the shape of the input to conv2d_transpose i.e. l1da and it is correct(10x8x8x64). The batch size is 10, input to this layer is in the form of 8x8x64, and the output is supposed to be 12x12x32.

What am I missing?
",2,2
2,2016-5-3,2016,5,3,2,4hip3p,Can we reduce the floating point accuracy in TensorFlow?,https://www.reddit.com/r/tensorflow/comments/4hip3p/can_we_reduce_the_floating_point_accuracy_in/,kleer001,1462209617,"Here's a bit of research: 

http://www.sciencedirect.com/science/article/pii/S1877050911001116

They say 15 bits is good enough (instead of the normal 53). 

I would love a bit of crank down in the pipeline as I only have a CPU on my 7 year old laptop to play with. GPU is great and double sexy, but I think it's overkill for my budget. But still cpu runs up to 20x slower. AAArrgh :(

**16bit variables for Tensor Flow!!!**

Unless of course I'm totally misunderstanding the issues. That's totally possible. 

**edit:** 8-bit 
",4,2
3,2016-5-3,2016,5,3,7,4hk9sy,"A curated list of awesome TensorFlow experiments, libraries, and projects. Inspired by awesome-machine-learning.",https://www.reddit.com/r/tensorflow/comments/4hk9sy/a_curated_list_of_awesome_tensorflow_experiments/,Toyjust,1462229312,,0,15
4,2016-5-6,2016,5,6,0,4i0ghz,Cross entropy turns to NaN and training accuracy falls to zero after 3000 iterations?,https://www.reddit.com/r/tensorflow/comments/4i0ghz/cross_entropy_turns_to_nan_and_training_accuracy/,luongminh97,1462462944,Does this mean the gradient exploded?,1,0
5,2016-5-7,2016,5,7,15,4i91ye,Multi-GPU training scope stuff,https://www.reddit.com/r/tensorflow/comments/4i91ye/multigpu_training_scope_stuff/,DoomyDoom,1462603767,"In the CIFAR-10 and Inception-v3 examples, there's some code like this:

        for i in xrange(FLAGS.num_gpus):
          with tf.device('/gpu:%d' % i):
            with tf.name_scope('%s_%d' % (cifar10.TOWER_NAME, i)) as scope:
              # Calculate the loss for one tower of the CIFAR model. This function
              # constructs the entire CIFAR model but shares the variables across
              # all towers.
              loss = tower_loss(scope)
    
              # Reuse variables for the next tower.
              tf.get_variable_scope().reuse_variables()

I don't have multiple GPUs so I can't test it, but wouldn't this generate an under-sharing error? I would expect something like Variable tower_1/conv1_w does not exist. ",0,1
6,2016-5-9,2016,5,9,7,4igrrt,Could training TensorFlow on a library of fractal algorithms help researchers in any field find patterns in seemingly random (or noisy) datasets?,https://www.reddit.com/r/tensorflow/comments/4igrrt/could_training_tensorflow_on_a_library_of_fractal/,Gonzo_Rick,1462747712,"Fractal algorithms underpin much of the natural world due to their simplicity and surface area optimizing/space filling properties. Unfortunately, after enough iterations, they become so complex that it can be hard to even identify their presence as being a pattern at all. I believe that training TensorFlow software on fractal algebra/geometry (as large a variety as possible) could be a very beneficial tool in recognizing patterns within seemingly random dataset. Such a tool could be invaluable to researchers and clinicians of all kinds.

I am a neuroscience researcher who knows very little about coding. I've been looking into fractal pattern recognition software in my spare time for two years now (with this idea in mind) and was ecstatic when you began developing DeepMind and released this code to the public. Unfortunately, after multiple attempts at following your directions, it's become clear to me that such a project is far out of my purview. I'm writing this in the hope that someone with the necessary skillset might find this idea intriguing enough to pursue it further, or help to point me in the right direction.

What you folks at Google are doing with deep learning is beyond revolutionary, and might just change the face of my (and probably every) field of study. But I'm rambling now, so I'll stop by saying thank you. Thank you, not only for your hard work and world changing projects, but also for keeping the rest of the community in the loop by providing these open source materials.",2,7
7,2016-5-13,2016,5,13,1,4j1gmz,Tensorflow is coming to ruby!,https://www.reddit.com/r/tensorflow/comments/4j1gmz/tensorflow_is_coming_to_ruby/,Toyjust,1463069863,,0,3
8,2016-5-18,2016,5,18,0,4jrh2t,Am I allowed to use TensorFlow in a commercial product?,https://www.reddit.com/r/tensorflow/comments/4jrh2t/am_i_allowed_to_use_tensorflow_in_a_commercial/,luongminh97,1463499173,This question suddenly occurred to me as I was thinking about commercial applications of deep learning,2,4
9,2016-5-18,2016,5,18,1,4jrsns,Need help getting tensorflow to work on docker. (Windows),https://www.reddit.com/r/tensorflow/comments/4jrsns/need_help_getting_tensorflow_to_work_on_docker/,Slapsticks,1463503013,"I've followed instructions from several different sites, reinstalled everything several times, can't get it to work.

Primarily I've been using this guide: https://caffinc.github.io/2015/11/tensorflow-windows/

http://imgur.com/fVxTa7Q

^ in the docker terminal

What am I doing wrong?


Second attempt: http://imgur.com/G2SAhxp",1,2
10,2016-5-19,2016,5,19,0,4jxadu,What is the default variable initializer in Tensorflow?,https://www.reddit.com/r/tensorflow/comments/4jxadu/what_is_the_default_variable_initializer_in/,luongminh97,1463586198,What is the default method of variable initialization used when tf.get_variable() is called without any specification for the initializer? The Docs just says 'None'.,0,1
11,2016-5-19,2016,5,19,4,4jylc1,Google supercharges machine learning tasks with TPU custom chip,https://www.reddit.com/r/tensorflow/comments/4jylc1/google_supercharges_machine_learning_tasks_with/,fhoffa,1463601276,,1,4
12,2016-5-21,2016,5,21,22,4kdh90,New Chip by Google for Tensorflow will Help Advance the Field of Machine Learning,https://www.reddit.com/r/tensorflow/comments/4kdh90/new_chip_by_google_for_tensorflow_will_help/,Toyjust,1463835846,,0,3
13,2016-5-24,2016,5,24,8,4kqwt3,Stuck in the Bazel test stage.,https://www.reddit.com/r/tensorflow/comments/4kqwt3/stuck_in_the_bazel_test_stage/,brnme,1464046251,"Hey guys, i'm trying to get tensorflow up and running to have a play with syntaxnet and parse mcparseface. I have installed all the dependencies on my thinkpad t410 running arch linux. The bazel test stage from what I understand builds the project as a whole as well as testing compatibilty of all the files/applications. However as the thinkpad doesnt have the most amazing processing power, it has been on [2,563 / 3,121] for the past hour or so. I was wondering when running the bazel test command if it is usual for mounds of output to be produced. Any insight into the bazel test functionality would be greatly appreciated.",0,1
14,2016-5-24,2016,5,24,13,4ks5es,Fizz Buzz in Tensorflow,https://www.reddit.com/r/tensorflow/comments/4ks5es/fizz_buzz_in_tensorflow/,hooked_dev,1464064685,,2,14
15,2016-5-26,2016,5,26,5,4l1rtr,Question about GPU version on Linux,https://www.reddit.com/r/tensorflow/comments/4l1rtr/question_about_gpu_version_on_linux/,valkyreking1,1464209040,"Earlier this week I installed tensorflow (and keras) on my computer.  I only installed the CPU version, since I thought that would be enough for my needs.  However, I noticed that my CNNs were taking very long to train, so I realized I need the GPU version of tensorflow.  I was wondering if just doing the install command
$ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl
would cause any issues, since I already have the CPU version on my computer.  If so, how would I go about uninstalling the CPU version and installing the GPU version.

Thanks

",2,1
