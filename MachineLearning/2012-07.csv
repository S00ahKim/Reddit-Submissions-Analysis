,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2012-7-1,2012,7,1,23,vvola,Can computers learn how to learn?,https://www.reddit.com/r/MachineLearning/comments/vvola/can_computers_learn_how_to_learn/,[deleted],1341153818,,2,0
1,2012-7-1,2012,7,1,23,vvoxt,Getting in the ML business,https://www.reddit.com/r/MachineLearning/comments/vvoxt/getting_in_the_ml_business/,mfalcon,1341154419,"I'm working as a web software developer but I'm really interested in the Machine Learning and Natural Language Processing fields. I've begun self studying(I took the ML class from Stanford) and I'm playing with the idea to start a business using this technologies.
The point is that I don't know how to start, what could be the potential clients and market, how much knowledge would be necessary to get clients...

There are a lot of stories in HN about web applications, but I didn't read anything about this kind of companies/startups, maybe because this technologies require building personalized solutions instead of the one to many from most startups.

I think that the path will get clearer as I keep reading and getting practice, but I'd nice to know some experiences from another people with similar interests.
",6,3
2,2012-7-2,2012,7,2,0,vvpb2,Is there a supervised learning algorithm that can inject randomness into its output in proportion to its uncertainty about the result?,https://www.reddit.com/r/MachineLearning/comments/vvpb2/is_there_a_supervised_learning_algorithm_that_can/,sanity,1341155006,"I'm wondering if there is a machine learning algorithm that produces a probability as an output, which can inject randomness into the output in proportion to uncertainty as to what the output should be.

The application is to decide what ad to show people when they visit a web page.  Our goal is to show people the ad they are most likely to click on, but for new ads we won't have much data, so initially there will be greater uncertainty as to what their click through rate will be.

A very simple version of this, which only looks at the number of impressions and clicks on each ad, is to use a beta distribution, this technique is described [here](http://blog.locut.us/2011/09/22/proportionate-ab-testing/) (it's also known as Thompson sampling, and it's a solution to the multi-armed bandit problem).

But I'd like to use a machine learning algorithm that uses a much wider range of attributes than simply the number of impressions and clicks on each ad.  Ideally the input attributes can be both categorical (where there could be thousands of possible values for each attribute, for example ""city/state/country""), and numeric (like time of day).

I was thinking perhaps a Bayesian Network learner, where we use a beta distribution to inject randomness into the probabilities used in the Bayesian Network when making a prediction, but I want to see whether others have looked into this problem.",20,15
3,2012-7-2,2012,7,2,9,vwei8,What's the state of the analysis and generation of sheet music?,https://www.reddit.com/r/MachineLearning/comments/vwei8/whats_the_state_of_the_analysis_and_generation_of/,Tiomaidh,1341187380,"## Background
I'm a CS undergrad particularly interested in AI, and have gotten my feet wet with ML through internships, Stanford's online AI course, and a job I have at a startup doing simple text mining and less-simple searching (and I'm also learning through osmosis as the software company I have a summer job at just got acquired by IBM's Big Data division). I'm also incredibly interested in 18th-century Scottish fiddle music, and my chief hobby for the past six years has been playing, composing, and otherwise studying it.

I've noticed that there's a lot of patterns in the tunes. The vast majority are 32 bars (measures), there's a handful of patterns that make ~5 unique bars get repeated in such a way as to fill the 32 (one common idiom is playing ABAC ABAC DEDC DEDC twice, where each letter represents a bar). And when playing a tune I've never seen or heard before, I sometimes have correct intuitions about what new content will come next--there's a lot of melodic and chordal patterns whose precise nature I can't quite articulate.

## Project
So as any sensible person would do when faced with vague theories and good data sources, I want to package up several thousand tunes in [ABC notation](http://en.wikipedia.org/wiki/ABC_notation), cause my computer to do incantations over them, look at the patterns my program finds out of pure musicological interest, and then use them to generate new tunes.

*Now it's time for the question*: do you know of any previously done work on something similar? I've spent a good amount of time trying to find things, but can only find audio-based projects. I'm not interested in analyzing the genre of an MP3 or contrasting the performance style of two artists. Nor am I interested in smashing together several loops of electronic sound and calling it music. This is a strictly sheet music-based project.

## Practical Details
From a logistics standpoint, I'd have a little bit of time this upcoming academic year to work on it (on the side), and if I wanted to I could have about twelve 20-40 hour weeks in the summer of 2013. In the summer and fall of 2013 I plan to be in Edinburgh studying ethnomusicology (yay, domain knowledge). I'm also thinking of applying to the [Singularity Institute](http://singularity.org/) and spending a week or two there in the beginning of the summer--if anyone has any thoughts on that I'd be interested in hearing them.

By asking about prior work I'm hoping someone will link me to a journal article where I'll learn what algorithms other people have used and how (un)successful that was--which would give me a place to start. If that doesn't work out:

## Current Strategy
First I'll create a hierarchy of things I'm interested in. One group might be broadly responsible for the overall structure of a tune, and I'll have one node recording data on length, another node recording data on which patterns of repeated bars are used, etc. Another top-level group might be broadly responsible for figuring out what type of note comes after what, with nodes that keep track of intervals (be it a minor third or a fifth or whatever), chord patterns, relative position in the bar, relative position in the tune, relationship to the key signature, etc. And so on and so forth. This obviously requires a lot of thought, and I'd like to try as much as possible to make things specific enough to be easily findable in the corpus, but abstract enough to be useful in a broader context. Then I'll run the corpus through them to get the actual data, inspect the results, probably iterate on that a few times as I realize what type of information is more valuable than others, and then feed it all to a glorified Markov chain generator for the composition aspect.

-----

I've done my best not to make an imposing wall of text, but...I wanted to make sure you had all the information you might want. The main question is about previous efforts, but anything else you'd like to add to the discussion is very welcome. Does this seem like a reasonable project? Any words of warning? I'm aware that even if everything goes perfectly according to plan and I have an automatic traditional tune spitter-outer the usefulness is questionable (not much of a market to monetize it with, and it seems a bit out there to go the academic route with)...it's purely for personal interest, pragmatism be darned.

Thanks!

**TL; DR**--See `Project` section.",7,12
4,2012-7-2,2012,7,2,17,vx090,Strange antenna developed by NASA with the help of machine learning,https://www.reddit.com/r/MachineLearning/comments/vx090/strange_antenna_developed_by_nasa_with_the_help/,SuperChihuahua,1341217304,,0,1
5,2012-7-3,2012,7,3,0,vxeyr,My first competition at Kaggle,https://www.reddit.com/r/MachineLearning/comments/vxeyr/my_first_competition_at_kaggle/,kafka399,1341244068,,0,1
6,2012-7-3,2012,7,3,2,vxmm3,What would you do with a bunch of greenhouse gas emissions data?,https://www.reddit.com/r/MachineLearning/comments/vxmm3/what_would_you_do_with_a_bunch_of_greenhouse_gas/,beanwolf,1341251837,"The data is from different organizations' buildings, trucks, data centers, etc. Really anything that emits greenhouse gases. Unfortunately the data only dates back to 2008, but could be as granular as monthly data in some places.

What would you do? Thanks!",3,2
7,2012-7-3,2012,7,3,3,vxomj,L1vs.L2Regularization?,https://www.reddit.com/r/MachineLearning/comments/vxomj/l1_vs_l2_regularization/,nickponline,1341253702,"I read that L1 favours sparse models where as L2 favours models with small coefficients. However I don't see how this is the case, sure with both L1 and L2 regularization (sum |theta| vs. sum theta^2) setting theta to zero will be favourable, so what makes the distinction between L1 being sparse and L2 having small coefficients. 

",14,22
8,2012-7-3,2012,7,3,6,vxzsb,Introduction to Conditional Random Fields,https://www.reddit.com/r/MachineLearning/comments/vxzsb/introduction_to_conditional_random_fields/,cypherx,1341263790,,1,1
9,2012-7-3,2012,7,3,12,vyjtp,L.A. Cops Embrace Crime-Predicting Algorithm - Technology Review,https://www.reddit.com/r/MachineLearning/comments/vyjtp/la_cops_embrace_crimepredicting_algorithm/,brente,1341284405,,11,43
10,2012-7-4,2012,7,4,3,vzmsl,Are there any career applications for Reinforcement Learning besides academia?,https://www.reddit.com/r/MachineLearning/comments/vzmsl/are_there_any_career_applications_for/,internethissyfit,1341338854,"Unsupervised and supervised learning are used everywhere there is data.  But it seems like Reinforcement Learning is only used by Andrew Ng, Peter Norvig, and about 6 other geniuses worldwide who are all working on the google self driving car.

For those of us who aren't super geniuses with PhD's from Stanford, are there any practical uses for Reinforcement Learning?",6,3
11,2012-7-4,2012,7,4,3,vznag,[AskML]I am looking for another genetic programming library comparable to this one without luck,https://www.reddit.com/r/MachineLearning/comments/vznag/askmli_am_looking_for_another_genetic_programming/,ml_zealot,1341339324,"I found pystep http://pystep.sourceforge.net/ awhile ago and love the syntax for setting up custom genetic programming problems.  The problem though is that it is dated, and not the fastest, would anyone know of any similar libraries for genetic programming that use similar syntax, in python or MATLAB?glance at the example page to see syntax, I have been unable to find a decent library in matlab for genetic programming, all are mostly genetic algorithms",0,1
12,2012-7-4,2012,7,4,4,vztld,Speeding up greedy feature selection,https://www.reddit.com/r/MachineLearning/comments/vztld/speeding_up_greedy_feature_selection/,cypherx,1341345227,,2,7
13,2012-7-4,2012,7,4,11,w0h85,"Performance evaluation of learning algorithms [PDF, 120 pp]",https://www.reddit.com/r/MachineLearning/comments/w0h85/performance_evaluation_of_learning_algorithms_pdf/,gtani,1341370675,,0,15
14,2012-7-4,2012,7,4,23,w15hn,How can an ANOVA perform feature selection?,https://www.reddit.com/r/MachineLearning/comments/w15hn/how_can_an_anova_perform_feature_selection/,nickponline,1341411526,"I thought an ANOVA was use to access the difference in means of two groups of data. So how can it be used to determine which features in a data set (X) are relevent in predicting labels the corresponding labels (t) ? And if so how is it possible agnostic of the type of classifier used?

For example is I make up some data where the second feature is clearly irrelevent, how can an anova show this?

X = [1 1 ; 1 -1 ; -1 1 ; -1 -1]

t = [1 1 -1 -1]'

Similarly with a Chi2 test?",1,4
15,2012-7-5,2012,7,5,0,w19m9,"The future of data analysis, by Hadley Wickham",https://www.reddit.com/r/MachineLearning/comments/w19m9/the_future_of_data_analysis_by_hadley_wickham/,agconway,1341416855,,4,1
16,2012-7-5,2012,7,5,2,w1du3,Is there a search engine over Artificial Intelligence and Machine Learning -related sites?,https://www.reddit.com/r/MachineLearning/comments/w1du3/is_there_a_search_engine_over_artificial/,Arech,1341421805,"Sometimes it is necessary to dig into Artificial Intelligence or specifically Machine Learning problems to make a research. Common googling (in my own experience) usually doesn't help much due to a lot of non relevant or paid materials in SERP. Google Scholar at the contrast is limited to scientific publications only, sometimes it is too narrow.

I wonder, if there is a kind of dedicated search engine over AI &amp; ML -related sites?

Thanks!",2,13
17,2012-7-5,2012,7,5,2,w1dz3,Questions: many similar features and more features than data,https://www.reddit.com/r/MachineLearning/comments/w1dz3/questions_many_similar_features_and_more_features/,TheInfelicitousDandy,1341421948,"I'm somewhat new to machine learning and I have a few questions.

First is it a problem to have many similar highly correlated features?  Compute time isn't a problem, and prediction is better using all the features, so is there any reason to try and reduce them?

Second can some one explain to me why it's a bad thing to have more features than data.  I understand it in the case where your extra features are all higher polynomial terms and you end up with so many features that you can way over fit that data.  In the more general case is it still a bad thing?",7,2
18,2012-7-5,2012,7,5,2,w1e6e,Introducing BigMLs Free Machine Learning Sandbox,https://www.reddit.com/r/MachineLearning/comments/w1e6e/introducing_bigmls_free_machine_learning_sandbox/,jjdonald,1341422181,,1,1
19,2012-7-5,2012,7,5,2,w1ft7,Machine learning with structured outputs,https://www.reddit.com/r/MachineLearning/comments/w1ft7/machine_learning_with_structured_outputs/,cypherx,1341424059,,0,2
20,2012-7-5,2012,7,5,3,w1gld,Structured SVM and Structured Perceptron for CRF learning in Python,https://www.reddit.com/r/MachineLearning/comments/w1gld/structured_svm_and_structured_perceptron_for_crf/,cypherx,1341424890,,0,9
21,2012-7-5,2012,7,5,8,w1wux,"Machine learning algorithm to find a ""complicated"" formula?",https://www.reddit.com/r/MachineLearning/comments/w1wux/machine_learning_algorithm_to_find_a_complicated/,Jeremymia,1341444346,"Hello. As a newbie to machine learning problem that fits these facts about my data:

1) My inputs are all numbers and my output is a number.
2) I have many features/inputs of varying importance and varying interrelatedness.
3) I have basically infinite training data (sets of both input and the correct output)
4) A linear regression would be absurdly inaccurate for almost all cases because of how little the true model looks like a line or even a single continuous function.
5) I only need to get the formula once.

Are any machine learning approaches suited for these cases out of the box?",6,2
22,2012-7-5,2012,7,5,12,w26e1,How is a ROC curve and curve and not a point?,https://www.reddit.com/r/MachineLearning/comments/w26e1/how_is_a_roc_curve_and_curve_and_not_a_point/,[deleted],1341457919,"If I have a classifier that I train and run on a dataset and compute the resulting TPR and FPR, then plot this in ROC space surely I just get a single point. How to people seem to get a ROC ""curve""?

TITLE SHOULD READ: How is a ROC curve a ""curve"" and not a point?",0,1
23,2012-7-5,2012,7,5,12,w27gu,Is it possible to plot a ROC curve for an SVM?,https://www.reddit.com/r/MachineLearning/comments/w27gu/is_it_possible_to_plot_a_roc_curve_for_an_svm/,nickponline,1341459290,Is it possible to plot a ROC curve for an SVM performing binary classification? It doesn't makes sense that you should be able to because there is no threshold value that you could vary to create the roc curve right? You would just get a single point representing the TPR vs. FPR of the classifier. ,9,7
24,2012-7-5,2012,7,5,13,w2aus,I am working to fight child slavery by doing statistical analysis and I would like your help.,https://www.reddit.com/r/MachineLearning/comments/w2aus/i_am_working_to_fight_child_slavery_by_doing/,Secret_Identity_,1341463984,"Let me say upfront that this is the early stages of this project and there is no money involved. If this goes well I might write a paper about it (though I have no idea where I would submit it).

Here is the basic premise of the problem: 

**Lots of young girls are used as sex slaves/prostitutes in the US. If we examine the ad space of young prostitutes can we help them?**

More details:

Their pimps advertise the girls in many cities all across the country. The pimps might have girls in several cities or move girls between several cities. 

These ads appear online in a variety of publications. These ads often come with telephone numbers and photographs.

The question:

If we scrapped ads from these websites what sort of useful analysis could we do?

So far this is what I have thought of:

Do cluster analysis on the text of the ads to see which ads were posted by the same person. Essentially this is an unsupervised learning algorthm being applied to the data of the ads (photo, text, date/time of posting, location, phone number). It was suggested that I use an [LDE](http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation).  Additionally, there is no need to actually decode the meaning of the ads at this time, but it could be interesting later.

Photo analysis to see which girls are operating in which cities. This has several different possible levels of success: Finding the same picture attached to different ads. Finding similar pictures, i.e., pictures that might have been taken in the same room. Finding the same girls in distinctly different pictures.

What help I would like from r/machinelearning:

How have problems like this been solved in the past. I imagine someone has done analysis on craigslist ads to similar effect, i.e., to find out which ads were posted by the same person. Probably in the name of fraud detection. Can someone point me to some papers on the topic? I am looking myself, but I wouldn't mind the help.
",22,32
25,2012-7-5,2012,7,5,23,w2svc,Air Handling Unit Manufacturer,https://www.reddit.com/r/MachineLearning/comments/w2svc/air_handling_unit_manufacturer/,gjkliol,1341498410,,3,0
26,2012-7-6,2012,7,6,1,w2ywp,The Problem with Pre-Publication Peer Review,https://www.reddit.com/r/MachineLearning/comments/w2ywp/the_problem_with_prepublication_peer_review/,[deleted],1341505548,,2,9
27,2012-7-6,2012,7,6,2,w3128,The cost of not knowing,https://www.reddit.com/r/MachineLearning/comments/w3128/the_cost_of_not_knowing/,[deleted],1341507956,,0,0
28,2012-7-6,2012,7,6,4,w38cr,"5 Ton, 980mm bore Piston Change on Marine Diesel Engine",https://www.reddit.com/r/MachineLearning/comments/w38cr/5_ton_980mm_bore_piston_change_on_marine_diesel/,[deleted],1341515249,,5,0
29,2012-7-6,2012,7,6,13,w43hp,Introduction to Machine Learning Lectures: from Caltech Prof. Yaser S. Abu-Mostafa,https://www.reddit.com/r/MachineLearning/comments/w43hp/introduction_to_machine_learning_lectures_from/,jrockIMSA08,1341547808,,2,59
30,2012-7-6,2012,7,6,16,w4bvq,Drawing permutations from a probability distribution,https://www.reddit.com/r/MachineLearning/comments/w4bvq/drawing_permutations_from_a_probability/,pretz,1341559660,"I am unsure if an algorithm exists to do this:

I have an alphabet {a,b,c,d,e,f} which I have many permutations of. e.g.

    abfdce
    cedbaf
    dacfeb

and i know that on average 'a' occurs in the first position with probability x, 'b' occurs with probability y etc. so i have aggregate probabilities about which letter occurs in which position. How do I draw new permutations such that, on average, they have the same statistics as the data i have? My alphabet size is ~50 so i can't enumerate them all or anything like that.",2,0
31,2012-7-6,2012,7,6,22,w4lop,Kaggle Post-Mortem: Psychopathy Prediction Blowout,https://www.reddit.com/r/MachineLearning/comments/w4lop/kaggle_postmortem_psychopathy_prediction_blowout/,willis77,1341581015,,8,43
32,2012-7-7,2012,7,7,2,w4xui,Error metrics for multi-class problems in R: beyond Accuracy and Kappa,https://www.reddit.com/r/MachineLearning/comments/w4xui/error_metrics_for_multiclass_problems_in_r_beyond/,pandemik,1341594931,,0,7
33,2012-7-7,2012,7,7,2,w501n,Quick classifiers for exploring medium-sized data (redux),https://www.reddit.com/r/MachineLearning/comments/w501n/quick_classifiers_for_exploring_mediumsized_data/,cypherx,1341597134,,0,6
34,2012-7-7,2012,7,7,2,w50hz,Machine learning course to follow,https://www.reddit.com/r/MachineLearning/comments/w50hz/machine_learning_course_to_follow/,mfalcon,1341597544,"I began the mlclass from coursera but I'd like a more advanced approach.

I found some courses(I don't know if there are more):

Andrew Ng Stanford CS229: http://cs229.stanford.edu/info.html

Caltech: http://work.caltech.edu/telecourse.html

Tom Mitchell Carnegie Mellon: http://www.cs.cmu.edu/~tom/10701_sp11/

I'm considering following the Tom Mitchell course as it seems to go deeper into the details, also because it uses a pretty cool bibliography.
What do you think, am I making the right choice?
",3,2
35,2012-7-7,2012,7,7,4,w552e,Ideas for an E-Learning Recommendation System,https://www.reddit.com/r/MachineLearning/comments/w552e/ideas_for_an_elearning_recommendation_system/,skyflashings,1341601839,"I'm looking for suggestions, but not sure of a better place to ask this.

Simply put, I've been assigned to build a recommendation system which suggests learning materials to students after they take a quiz.

The current system:  Students anonymously go to the website, choose their particular book isbn, chapter and lesson, and take a self-assessment quiz.  If their score is below a certain percentage, they are 'remediated' and are given the textbook page number of an example associated with that question (this correlation is all in the database).  They can then email their scores to their teacher.

Now, there are thousands of digital learning materials collected from over the years, including video and other instructive materials.  The point of the project is to incorporate these materials into the old quiz system and 'intelligently' give suggestions to the student for remediation.  (These suggestions are in the form of links, to my knowledge).  Note, this whole project is more-or-less just a proof of concept that is intended to be applied to some of their more modern quiz systems.

Data available:  Each quiz contains the book title, chapter title, lesson title, the actual question and an optional hint for the student.  The new materials have metadata such as description, grade-level, remediary/basic/advanced, etc.  Each material is categorized into a certain group, for instance ""Expressions"" in mathematics.  The idea is that the questions will also be associated with these groups in order to narrow down the list of those materials retrieved.  That is, get all materials associated with the category that question is in.

For starters, a user registration system is needed to store individual quiz score history.  I suppose the kinds of additional data needed is dependent on the algorithm to learn off of the data.  I've basically been given free reign to choose what modifications need to be made and the kinds of data to store.

I've looked at various papers on the topic, and this one seems pretty relevant: http://www.ascilite.org.au/ajet/ajet26/ghauth.pdf

He uses a combination of cosine similarity for the materials and a user rating system.  In my case, I could preprocess the similarity rankings between each question and the materials in the same category (but I guess it would have to be done each time a new material is added).  This would be between data I know about the question against data I know about the materials.  I think I don't want students explicitly rating material, so instead I would give a rating implicitly based on whether or not that material helped them in their next quiz (eg they viewed material in ""expressions"" and the next time they encounter ""expressions"" in a quiz their score increased).

Is this a good approach to take?  Is there a way to use the additional metadata about students to do more than simple heuristics?  I would definitely appreciate any suggestions as this is the first machine-learning project that I've ever tried to tackle like this.  Thanks!",2,2
36,2012-7-7,2012,7,7,5,w58wj,Need help with a clustering algorithm,https://www.reddit.com/r/MachineLearning/comments/w58wj/need_help_with_a_clustering_algorithm/,Sohakes,1341605588,"Hello, fellow redditors! Sorry if it isn't the place to post this, but I'm not sure where to get help.

I'm an undergraduate student from Brazil doing some research with clustering algorithms. I have an instructor, and my job was simply to implement an clustering algorithm on a teacher's framework (that she did for her PhD). The problem is that it never returns the correct results, even for it's sample dataset. After much time debugging the code, I found there is no bug... the thing is, it's not returning the correct result because the algorithm idea looks strange, and wrong for me, and it returns just one cluster (out of as many as you want).

The algorithm is a variation of the CLIQUE algorithm. It's an subspace and density-based algorithm, that uses grids and works exploiting something called ""dimension monotonicity"". Basically, it divides the dataset in grids, and then try to find the cells (of the grid) that have more objects than a certain threshold. It marks these cells as dense, and then proceed to combine them to form larger dimensions candidate clusters (candidate because they may not form a cluster if they are not dense as well), and then repeat the process. To combine the clusters, the CLIQUE algorithm uses the dimension's monotonicity lemma:

**Lemma:** If a collection of points S is a cluster in a k-dimensional space, then S is also part of a cluster in any (k-1)-dimensional projections of this space.

**Proof:** A k-dimensional cluster C includes the points that fall inside a union of k-dimensional dense units. Since the units are dense, the selectivity (selectivity is the fraction of total data points contained in the unit) of each one is at least r (the threshold). All the projections of any unit u in C have at least as large selectivity, because they include all points inside u, and therefore are also dense. Since the units of the cluster are connected, their projections are also connected. It follows that the projections of the points in C lie in the same cluster in any (k-1)-dimensional projection. QED.

I can agree with that! The problem is that it specifies a fixed threshold (a input parameter). The algorithm that I'm working with don't use a fixed threshold, instead, it calculates a different threshold for each dimension, and then, for each candidate cluster created with the lower dimensions, it checks if it's dense for the threshold for each dimension in the cluster (i.e. it checks if the candidate cluster selectivity is higher than the threshold for each dimension that forms the cluster). The threshold formula is this (the paper doesn't prove it works, just point it works because of the lemma above, but I don't think the lemma apply because of the variable threshold):

    Threshold = N*a*1.5/D

Being 'N' the number of objects in the dataset, 'a' the bin size, 'D' the dimension size and 1.5 a constant. What it's trying to do is check if the bin is 1.5 times more dense that it was going to be if the dimension was uniform. It looks like a good idea. The problem is that the monotonicity lemma doesn't apply here, and he doesn't even prove it applies. I'm going to give an example to where I think it doesn't apply, i.e. there is a dense cluster on higher dimensions that isn't found on the lower dimensions:

Suppose I have a dataset with 2 clusters and 2 dimensions, both dimensions with size 10. The first cluster have 100 points uniformly distributed on 0~5 on the first dimension and 0~10 on the second. The second cluster have 8 objects, from 6~7 on the dimension 1, and from 0~4 on the second dimension. Now, both clusters have the same density (size 50 for 100 objects, size 4 for 8 objects), but the first cluster is so big that it ""hides"" the second cluster. The threshold for the first dimension and second cluster is 108\*1\*1.5/10. As we can see, it's a lot more than 8. Now the first cluster threshold and first dimension is 108\*5\*1.5/10=81, and so the first cluster would be considered dense, but it's density is the same as the second one. It look really flawed to make a formula like this, because it neglects that the cluster can be really big on the other dimensions, and so it will be really dense on some parts of the projection on lower dimensions (we don't want to find the biggest cluster, but the dense ones).

I think that is the problem, because the only cluster it finds is the biggest one, but all clusters have the same density. But it was some guy PhD thesis, and it's really intelligent on the other parts, so it's really strange to have such a error (and he made tests and all these things, showing good results)... someone could help me find the flaw in my logic? Or it's really kind of wrong?

Thanks in advance!

P.S.: Yeah, I avoided saying the name of the algorithm because it's not well known (just a few papers citing), and it would be bad if one of the first results was something bad about it, but I could say it if necessary. Sorry for the poor english and the wall of text.",2,4
37,2012-7-7,2012,7,7,5,w5b1w,Physics grad looking to attempt some kaggle events. Best/Quickest way to get up and running?,https://www.reddit.com/r/MachineLearning/comments/w5b1w/physics_grad_looking_to_attempt_some_kaggle/,T3ppic,1341607696,"Most of my third year (3 years in the UK for a BA) electives were in the field of statistical analysis and model testing and Im pretty comfortable with mathematics and python. 

I just can't find an ""easy"" in. I decided to put this in /r/machinelearning rather than /r/statistics because I think thats where the problem lies; Until looking through some of the kaggle interviews I have never heard the expression ""random forest"" which seems key to some of the entries. Wikipedia didn't help. Before then I had thought about Monte Carlo (obviously I know of it because its most relevant to physics) for some of the kaggle competitions but no interview mentions this as a relevant winning strategy probably because the learning and testing datasets are small.

I already have some experience with C and python with little to none in mathematica or R. Would you recommend a change to either of the latter? Or just struggle through with python? Most of my university experience with stats was done in Excel with the built in tools and a few custom sheets provided by the lecturer. 

A book Ive managed to look at is ""Data Mining Third Edition by Morgan Kaufman"" and it doesnt seem to be very relevant or conducive to get up and running quickly; I am under no illusions about winning any competition but I do want to at least try some of them as a learning exercise. 

TL:DR Can you recommend any books or websites that allow a ""quick and dirty"" introduction to kaggle related concepts such as random forests?
",15,6
38,2012-7-8,2012,7,8,4,w6sv5,A Quick Sentiment Analysis Experiment: Vorpal Wabbit vs state of the art results,https://www.reddit.com/r/MachineLearning/comments/w6sv5/a_quick_sentiment_analysis_experiment_vorpal/,rrenaud,1341688624,,7,35
39,2012-7-8,2012,7,8,18,w7sq1,Time Series Data Library now on DataMarket,https://www.reddit.com/r/MachineLearning/comments/w7sq1/time_series_data_library_now_on_datamarket/,amair,1341738329,,1,9
40,2012-7-9,2012,7,9,0,w832c,Let's make an ML reading list,https://www.reddit.com/r/MachineLearning/comments/w832c/lets_make_an_ml_reading_list/,eloisius,1341762999,,12,30
41,2012-7-9,2012,7,9,10,w8ved,Am I shooting myself in the foot by going to a mid-tier university for a PhD? Should I try to transfer after my masters?,https://www.reddit.com/r/MachineLearning/comments/w8ved/am_i_shooting_myself_in_the_foot_by_going_to_a/,thankyousir,1341795918,"I am currently in a graduate program ranked in the mid 50's in comp sci according to us news. I will start my program in the fall. I am really leaning towards getting a PhD because I like research and the relative freedom that comes with the degree, however, I am not sure if it would be wiser to try to stay here for my phd or leave after I get my masters (it would be very easy to do because I am currently funded only by a 2 year teaching assistantship). Would it be relatively easy to get funding to go to a higher ranked uni with a masters assuming I get good grades and perhaps a publication? Another factor: I am going to the same uni as my undergrad, is inbreeding a factor? 

Thanks!
",16,3
42,2012-7-9,2012,7,9,11,w91av,OpenCyc version 4.0 has been released,https://www.reddit.com/r/MachineLearning/comments/w91av/opencyc_version_40_has_been_released/,jimktrains,1341802034,,1,9
43,2012-7-9,2012,7,9,12,w950n,"Best method, for supervised learning, to handle variable length feature vectors?  (besides club it into a fixed length via stats and use a standard algorithm)",https://www.reddit.com/r/MachineLearning/comments/w950n/best_method_for_supervised_learning_to_handle/,duckandcover,1341805897,,5,1
44,2012-7-9,2012,7,9,23,w9t4h,What are the best NL processing and AI companies?,https://www.reddit.com/r/MachineLearning/comments/w9t4h/what_are_the_best_nl_processing_and_ai_companies/,darkwave9,1341845717,,2,6
45,2012-7-10,2012,7,10,7,wal0c,"Can anyone give an intuitive explanation of when a classifier would give below-chance classification results i.e. ""anti-learning"" as I've heard it called?",https://www.reddit.com/r/MachineLearning/comments/wal0c/can_anyone_give_an_intuitive_explanation_of_when/,lpiloto,1341871584,"By working, I mean that there isn't some issue with the data being labelled wrong or that there is insufficient data, etc.",11,11
46,2012-7-11,2012,7,11,0,wbwqq,Stop Wire Rope Rust | Anti Rust Industrial Lubricant | Rust Corrosion Inhibitor,https://www.reddit.com/r/MachineLearning/comments/wbwqq/stop_wire_rope_rust_anti_rust_industrial/,hinderrust,1341933319,,0,1
47,2012-7-11,2012,7,11,4,wcbyt,Air Machine &amp; Air Scrubbers for Abatement Projects,https://www.reddit.com/r/MachineLearning/comments/wcbyt/air_machine_air_scrubbers_for_abatement_projects/,spidertime,1341947537,,1,1
48,2012-7-12,2012,7,12,2,we90i,Should you apply PCA to your data?,https://www.reddit.com/r/MachineLearning/comments/we90i/should_you_apply_pca_to_your_data/,cypherx,1342029470,,9,20
49,2012-7-12,2012,7,12,6,wemxt,"A ML newbie here, with a question on SVMs",https://www.reddit.com/r/MachineLearning/comments/wemxt/a_ml_newbie_here_with_a_question_on_svms/,mikethepwnstar,1342041956,"I've been put on a project involving classification of 8 different classes. Currently through Matlab we generate feature vectors and use libsvm to train/predict. However, right now we're having issues with scaling the data, and I think it's because of the number of classes.

Is there a better method to use for this many classes? ",2,0
50,2012-7-12,2012,7,12,8,wetra,Autonomous Race Car - I've been waiting for this!,https://www.reddit.com/r/MachineLearning/comments/wetra/autonomous_race_car_ive_been_waiting_for_this/,HawkEgg,1342048515,,6,27
51,2012-7-13,2012,7,13,0,wg2g4,Deep Learning Tutorial for NLP,https://www.reddit.com/r/MachineLearning/comments/wg2g4/deep_learning_tutorial_for_nlp/,rrenaud,1342108153,,2,45
52,2012-7-13,2012,7,13,0,wg2rf,Extracting nouns with sentiment using chunking and part of speech tagging.   Official Blog of Repustate,https://www.reddit.com/r/MachineLearning/comments/wg2rf/extracting_nouns_with_sentiment_using_chunking/,therealmoju,1342108462,,0,0
53,2012-7-13,2012,7,13,3,wgd97,New to Machine Learning. Could you guys point me good source material?,https://www.reddit.com/r/MachineLearning/comments/wgd97/new_to_machine_learning_could_you_guys_point_me/,Bitter_Peter,1342117489,,4,2
54,2012-7-13,2012,7,13,5,wgls5,"Ford's Big Data chief sees massive possibilities, but the tools need work  | ZDNet",https://www.reddit.com/r/MachineLearning/comments/wgls5/fords_big_data_chief_sees_massive_possibilities/,dmdude,1342124980,,0,0
55,2012-7-13,2012,7,13,10,wh2sb,Let's talk processes:  You're given a heaping pile of data.  You've constructed features.  What is your approach next?,https://www.reddit.com/r/MachineLearning/comments/wh2sb/lets_talk_processes_youre_given_a_heaping_pile_of/,waterwaterw,1342141589,"I feel that most people probably have a go-to approach when given new data that they want to perform classification on.  You've got a bunch of labeled data and features, what is your personal process for orienting yourself?",14,30
56,2012-7-13,2012,7,13,15,whjqw,Which is language is better for machine learning? Python or R?,https://www.reddit.com/r/MachineLearning/comments/whjqw/which_is_language_is_better_for_machine_learning/,rudyl313,1342160208,,5,2
57,2012-7-13,2012,7,13,17,whp49,"food processing machinery,food machinery manufacturers,processing food industry|Zhengzhou Wilead Trading Co., Ltd",https://www.reddit.com/r/MachineLearning/comments/whp49/food_processing_machineryfood_machinery/,hellon0223,1342169545,,1,1
58,2012-7-13,2012,7,13,22,whxjy,a question about eligibility traces in RL,https://www.reddit.com/r/MachineLearning/comments/whxjy/a_question_about_eligibility_traces_in_rl/,northbranchsocks,1342186266,"I've been studying linear function approximation techniques in reinforcement learning, and there's one concept that doesn't quite make sense to me. I can intuitively see why eligibility traces are good: if you receive a reward at an uncertain time in the future, it's good to have a decaying ""memory"" of the states you've recently visited. Here's what I don't grasp yet: once you receive that reward, doesn't the state information stick around in your eligibility trace, continuing to decay slowly? It seems to me that we should decay past state information faster as we receive rewards, as the effect of those states has now been accounted for.

Is this already accounted for in algorithms like TDLambda and GQLambda, and I'm missing it? Or is there a reason this isn't a valid concern?",2,1
59,2012-7-14,2012,7,14,22,wjqhw,Justification of learning abstract math for machine learning ?,https://www.reddit.com/r/MachineLearning/comments/wjqhw/justification_of_learning_abstract_math_for/,Mazungu___,1342271432,"I wonder which research areas in ML requires a good amount of abstract math knowledge ?

I mean for manifold learning you need to understand topology, or for kernels you need functional analysis are there other areas you know with such reqs ?",3,10
60,2012-7-15,2012,7,15,0,wjvnh,Number of samples and instances in the training data,https://www.reddit.com/r/MachineLearning/comments/wjvnh/number_of_samples_and_instances_in_the_training/,emeka11,1342280562,"I'm trying to determine the ideal number of samples and instances of data that I should collect. I'm not sure how the number of samples and instances in each sample influences the training data. Is it a large number of samples good? Then, should I try to collect as many instances of each category as possible? 

And I will be using the SVM algorithm.

Thanks for your help, and any clarification. And, I'm also not sure if I'm confusing definitions (samples vs instances).",2,6
61,2012-7-15,2012,7,15,5,wk83v,Hey! A friend and I are starting up a Machine Learning and Data Science Forum/Society in our University. Need r/MachineLearning's ideas/help!!,https://www.reddit.com/r/MachineLearning/comments/wk83v/hey_a_friend_and_i_are_starting_up_a_machine/,ml_guy,1342296022,"As said in the title, we are starting up a club/society/forum.

We have no idea about how to come up with names for it, we want you guys to help us out with nice relavant names. We are thinking about 

Data Science + Machine Learning Society, but it seems too plain. 
Data Crew, but it seems too vague.

Also, feel free to throw in possible agendas for the group.

Looking forward to getting some great ideas from the Group! Thanks in advance!",0,1
62,2012-7-15,2012,7,15,16,wl3yl,"Two ideas - I think recaptcha can figure out if people have certain mental health/neurological problems. Similarly, google voice and Siri could be used to detect people having a stroke or other health problems.      ",https://www.reddit.com/r/MachineLearning/comments/wl3yl/two_ideas_i_think_recaptcha_can_figure_out_if/,redditacct,1342338095,"Obviously, tons of privacy issues, but I thought interesting blend of universal healthcare and machine learning.      
    
Not sure if here is the right place or cogsci or computer science/programming or health or aviation/flying or...    
     
An obvious, dead easy one for captchas is color blindness but I think more advanced systems could recognize dyslexia and more.     
    
Voice based apps could be trained to recognize slurred speech (maybe hypoxia or stroke or diabetic crisis, etc) or other speech issues.  ",12,5
63,2012-7-16,2012,7,16,20,wn0cb,How Google uses Linear Algebra to Rank Pgaes,https://www.reddit.com/r/MachineLearning/comments/wn0cb/how_google_uses_linear_algebra_to_rank_pgaes/,cavedave,1342437556,,0,0
64,2012-7-16,2012,7,16,23,wn7f7,Empirical methods in NLP 2012 reading list from alextp,https://www.reddit.com/r/MachineLearning/comments/wn7f7/empirical_methods_in_nlp_2012_reading_list_from/,rrenaud,1342448674,,0,3
65,2012-7-17,2012,7,17,7,wo0kv,Data mining for network security and intrusion detection,https://www.reddit.com/r/MachineLearning/comments/wo0kv/data_mining_for_network_security_and_intrusion/,kafka399,1342476098,,1,18
66,2012-7-17,2012,7,17,13,wonuk,Is a masters in applied math/statistics where I learn machine learning a good way to get a job doing big data analysis or do I need to major in CS?,https://www.reddit.com/r/MachineLearning/comments/wonuk/is_a_masters_in_applied_mathstatistics_where_i/,DemonKingWart,1342498691,"My undergraduate major was in math and I've been an actuary for two years, but find it boring.  If I were to go to an applied math or statistics graduate program and take machine learning, data mining, neural computing, and the cs prerequisites necessary (there's about 5 courses including java and intro to ai) for those classes, would that be a good background for a career in machine learning and data mining or do you need a more extensive background in cs?  Are there certain types of jobs specifically for more mathematical/statistical people?  Finally, is there anyone here who does predictive modeling or catastrophe risk modeling using machine learning and data mining?  A career in that would mean I didn't waste these first five actuarial exams.  ",10,10
67,2012-7-17,2012,7,17,19,wp1fc,And so it begins ... Compressive Genomics,https://www.reddit.com/r/MachineLearning/comments/wp1fc/and_so_it_begins_compressive_genomics/,[deleted],1342520820,,4,36
68,2012-7-18,2012,7,18,3,wpoov,Mutual Information,https://www.reddit.com/r/MachineLearning/comments/wpoov/mutual_information/,arghdos,1342549014,"Disclaimer:  I recently took a undergrad AI class, hence pretty new to this whole field, so if you see something that makes you think (why the hell is this guy doing X, or doing X by using Y) please let me know.  

So I'm trying to classify process data to indicate whether a process bump test is worth fitting a model to or not (don't worry if that didn't make a bunch of sense), and I have a nice set of features I have extracted from the data sets.  
Now I'm trying to implement a MI algorithm to aid with feature selection, and I get answers that are plain wrong  (a test feature with no relationship between it and my labels comes back with MI = 1), and I'm not quite sure exactly where I'm going wrong.  

Does anyone know of a resource (simpler is better) that goes into the actual **implementation** of a MI calculation?  I feel as if I understand the theory fairly well, but I am missing something silly in my implementation, and could really use a more in-depth look on how to actually calculate MI.  

Thanks",2,11
69,2012-7-18,2012,7,18,20,wr5le,"Computer analysis predicted rises, ebbs in Afghanistan violence",https://www.reddit.com/r/MachineLearning/comments/wr5le/computer_analysis_predicted_rises_ebbs_in/,cavedave,1342609542,,6,17
70,2012-7-19,2012,7,19,5,ws18o,"Stanford's UFLDL tutorial seams to be down, so if anyone knows anyone who could get it back up, please, let them know the Internet is missing their wonderful site",https://www.reddit.com/r/MachineLearning/comments/ws18o/stanfords_ufldl_tutorial_seams_to_be_down_so_if/,diatron3,1342643063,,7,12
71,2012-7-19,2012,7,19,5,ws3ky,A tutorial on Graph-based Semi-Supervised Learning,https://www.reddit.com/r/MachineLearning/comments/ws3ky/a_tutorial_on_graphbased_semisupervised_learning/,rrenaud,1342645119,,0,11
72,2012-7-20,2012,7,20,16,wv4bo,Big Data on Campus,https://www.reddit.com/r/MachineLearning/comments/wv4bo/big_data_on_campus/,cavedave,1342770886,,1,11
73,2012-7-21,2012,7,21,0,wvk0v,EMI Music Data Science Hackathon - Tomorrow (July 21),https://www.reddit.com/r/MachineLearning/comments/wvk0v/emi_music_data_science_hackathon_tomorrow_july_21/,willis77,1342797001,,0,5
74,2012-7-21,2012,7,21,0,wvmmq,Frustratingly Easy Domain Adaptation - Hal Daum 2007,https://www.reddit.com/r/MachineLearning/comments/wvmmq/frustratingly_easy_domain_adaptation_hal_daum/,rrenaud,1342799614,,0,13
75,2012-7-21,2012,7,21,0,wvmo7,A Question on Number of Classes and Data Distribution,https://www.reddit.com/r/MachineLearning/comments/wvmo7/a_question_on_number_of_classes_and_data/,arghdos,1342799649,"So in a project I'm currently working on, I am trying to classify statistics from bump test process data to determine if a section of the process data is a good candidate to fit a model to (don't worry about that, I'm classifying stuff is all)  

I've been manually looking at model fits and classifying them based upon my knowledge, and I originally had 3 classes (Good, Unclear, Poor) of fits.  But I noticed while doing this that there are really more like 5 classes (Excellent/Perfect, Good, Unclear, Poor, and ohmygodgetitawayfromme/Atrocious).  

Now ideally I would like to use the 5 classes, but I'm a bit worried that I will be splitting my data up unevenly, and I don't know what effects that will have on my classifier (see below).  Is scaling the importance of classes a viable option in this case?  Just as an estimate, I have about ~2000 data instances, and probably only ~50-100 will fall in the Excellent class and ~100-200 in Atrocious, where the others will be more evenly distributed.  I can generate more data quite easily, but I am working alone, and manually classifying takes quite some time.  

I am using a random forest right now, but will probably switch to a neural net later, as we probably can't release our source code, as dictated by the gnu public license that the [random forest code](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_copyright.htm) I found uses (and I don't know nearly enough about it to write my own version).  

Is this something I need to worry about?",1,1
76,2012-7-21,2012,7,21,3,wvvwj,"""Is this Bayesian? You know I'm a strict Bayesian, right?""",https://www.reddit.com/r/MachineLearning/comments/wvvwj/is_this_bayesian_you_know_im_a_strict_bayesian/,MurrayBozinski,1342808337,,12,88
77,2012-7-21,2012,7,21,4,wvzxr,From Mr. Deep Learning himself  :),https://www.reddit.com/r/MachineLearning/comments/wvzxr/from_mr_deep_learning_himself/,giror,1342812101,"https://www.coursera.org/course/neuralnets

Geoffrey Hinton's Neural Net class :)",0,28
78,2012-7-22,2012,7,22,3,wxjd8,Multiple Layers question for Neural Network,https://www.reddit.com/r/MachineLearning/comments/wxjd8/multiple_layers_question_for_neural_network/,Louis_Mage,1342893866,"I'm building a neural network to attempt to distinguish human speech. I'm looking into analyzing a spectrum of frequencies, and found that I'm likely going to be using a large number of nodes (between 55k to 6k)- I came to this by using a value told to me by my AI book, that the hidden layer should have 2^n/n nodes in it (AI: a modern approach, 3rd edition p 732 footnote). 

I have not, however, found any sort of information regarding the number of hidden layers that should exist for a given system, or the number of nodes in the input. Does anyone know of any resources that would describe this, or know themselves?",25,9
79,2012-7-23,2012,7,23,0,wyv5s,Automatic Hyperparameter Tuning Methods,https://www.reddit.com/r/MachineLearning/comments/wyv5s/automatic_hyperparameter_tuning_methods/,rrenaud,1342970032,,0,11
80,2012-7-23,2012,7,23,1,wyyg8,Lessons in Python performance learned from big ML tasks,https://www.reddit.com/r/MachineLearning/comments/wyyg8/lessons_in_python_performance_learned_from_big_ml/,cypherx,1342974981,,0,34
81,2012-7-24,2012,7,24,5,x195c,Machine learning for the impatient: algorithms tuning algorithms - gift shop scientist,https://www.reddit.com/r/MachineLearning/comments/x195c/machine_learning_for_the_impatient_algorithms/,amirpc,1343075767,,2,14
82,2012-7-25,2012,7,25,0,x2rs8,Isnt all learning really unsupervised at its root?,https://www.reddit.com/r/MachineLearning/comments/x2rs8/isnt_all_learning_really_unsupervised_at_its_root/,Ayakalam,1343144057,"
Hey all, 

Not trying to trolloloolol here by any measure, serious question. I have taken some machine learning courses recently, both at brick and mortar, and of course, Andrew Ng's coursera ones, and in both we covered both supervised and unsupervised learning methods. 

I guess my question is more 'meta' than a technical one, and my motivation for asking it is in starting to become more interested in how learning happens in nature, the recent thrust for more unsupervised learning ala robotics, etc. 

Anyway, let us take a simple example of supervised learning - the perceptron. You have data points, and you have a label for each one, and you go about learning the best separation vector. Great. But - someone - or something - *had* to have labelled it in the beginning obviously. How did that person/entity label it? Clearly because they had already classified it - through their own classifier - which also means that they, at some point, learned about it, etc etc, ad infinitum. 

Thus, the original 'learner' - whoever - or whatever - it was, had to have come about the labeling in an unsupervised fashion.

This means that at some point, the human had to have figured out classification in an unsupervised way. Right? Does that then not make supervised learning a *subset* of unsupervised learning? 

To me this raises some interesting questions: For one, might it be possible, (and is there in fact some algorithm), that uses unsupervised learning to come up with labels, unto which a supervised learning algorithm then uses? Is there redundancy here, or more power? That is, can the original unsupervised learning algorithm + a later supervised algorithm be better than the original unsupervised learning algorithm to begin with?

A related question might then also be, if unsupervised learning is really the mother of all learning methods, biological or otherwise, then is supervised learning bring anything new to the table, or is it just making things easier and making use of information already given?

**Put another way, and TLDR:** Is there anything an unsupervised learning + supervised learning algorithm combo can do, that a pure unsupervised learning algorithm cant? ...




",11,3
83,2012-7-25,2012,7,25,1,x2w0q,Convex Optimization - From Real-Time Embedded to Large-Scale Distributed - Stephen Boyd,https://www.reddit.com/r/MachineLearning/comments/x2w0q/convex_optimization_from_realtime_embedded_to/,parunach,1343148207,"For people interested, this is an expanded version of the talk Stephen Boyd gave in KDD 2011

[Convex Optimization Lecture](http://www.multimedia.ethz.ch/speakers/ifa/boyd/?doi=10.3930/ETHZ/AV-1c085a1e-0b6e-4512-98ea-198db9f7b81e&amp;autostart=true)

He shows how convex optimization is related to machine learning, specifically Support Vector Machines.

Sorry I marked it nsfw, it was a mistake.
",5,24
84,2012-7-25,2012,7,25,10,x3wb6,Getting started and Looking for help,https://www.reddit.com/r/MachineLearning/comments/x3wb6/getting_started_and_looking_for_help/,amacgregor,1343181551,"I'm working on a ruby application that is supposed to grab links and classify them on the correct category depending on the context and title.

So far I been using a Naive Bayes Classifier which seems to work decently although I'm apparently making a big mistake by using my test data set as my training data set. 

Wondering if someone could advice on any methods or techniques for selecting proper training data and if the NaiveBayes algorithm is the best choice or should I be looking into other algorithms ?

Thanks in advance for any help or advice provided.",0,1
85,2012-7-25,2012,7,25,12,x42bh,This section from Yann LeCun's page sort of cracked me up a bit,https://www.reddit.com/r/MachineLearning/comments/x42bh/this_section_from_yann_lecuns_page_sort_of/,[deleted],1343187409,,0,1
86,2012-7-26,2012,7,26,0,x4w51,Would anyone be interested in hand-drawn decision boundary code for MATLAB?,https://www.reddit.com/r/MachineLearning/comments/x4w51/would_anyone_be_interested_in_handdrawn_decision/,[deleted],1343231318,"Hi all. 

Suppose you are looking at a scatterplot of 2-D data in matlab and you are envisioning some type of boundary to best separate the data. One thing you could do is train a logistic regression or SVM or random forest or whatever method you like to separate the data. However, you need to get your parameters correct if you want it to split the data nicely. 

If you can already picture in your mind how you want to separate the data, then perhaps you would like to draw in the decision boundary yourself. There would also be code to test which side of the decision boundary a new point is on. So once you're done drawing the decision boundary, it will behave like an SVM in that you can get a binary answer of which class a new point is in.

I'm just trying to gauge the interest for this. If there is an interest I'll probably make a github account and put up some matlab code.

EDIT: this would be for a 2-D decision boundary only (and I realize some of those methods I mentioned are kind of overkill for 2-D)",3,7
87,2012-7-26,2012,7,26,15,x6f00,Norvig vs. Chomsky and the Fight for the Future of AI,https://www.reddit.com/r/MachineLearning/comments/x6f00/norvig_vs_chomsky_and_the_fight_for_the_future_of/,qkdhfjdjdhd,1343284698,,17,81
88,2012-7-26,2012,7,26,17,x6j25,2-d Machine learning?,https://www.reddit.com/r/MachineLearning/comments/x6j25/2d_machine_learning/,alexgmcm,1343291802,"I am only just beginning to learn Machine Learning in the hope that it might solve a problem I have.

I have a data set where there are two continuous 'output parameters' that I want to be able to determine given several input parameters (also continuous so far, although I guess it's not impossible that I could determine discrete features to parametrise it), and I have a training set that I can use to learn from.

For continuous data I could use regression analysis I realise, but all of the examples I have seen for this deal only with one output parameter, so for example you might work out the likely price of a house given the area, number of rooms etc., whereas I have two output parameters so to continue the example it would be like having to be able to predict the price and area of the house, given the number of rooms etc.

How can I go about doing this via Machine Learning? ",10,5
89,2012-7-26,2012,7,26,19,x6mjq,My first hackathon experience,https://www.reddit.com/r/MachineLearning/comments/x6mjq/my_first_hackathon_experience/,kkuriso,1343299543,,0,0
90,2012-7-26,2012,7,26,21,x6qh8,An Algorithm for Pattern Discovery in Time Series,https://www.reddit.com/r/MachineLearning/comments/x6qh8/an_algorithm_for_pattern_discovery_in_time_series/,szza,1343306802,,3,20
91,2012-7-27,2012,7,27,8,x7uco,Graph-based Semi-Supervised Learning,https://www.reddit.com/r/MachineLearning/comments/x7uco/graphbased_semisupervised_learning/,qwerty_nor,1343344870,,0,0
92,2012-7-27,2012,7,27,9,x7yek,Practical machine learning tricks from the KDD 2011 best industry paper,https://www.reddit.com/r/MachineLearning/comments/x7yek/practical_machine_learning_tricks_from_the_kdd/,rrenaud,1343348919,,3,52
93,2012-7-28,2012,7,28,0,x8zyw,What is an example of a distribution of which you can only access an un-normalized version?,https://www.reddit.com/r/MachineLearning/comments/x8zyw/what_is_an_example_of_a_distribution_of_which_you/,nickponline,1343401563,In the context of a lot of MCMC methods for sampling you seem to need a sampler candidate distribution Q and an un-normalized version of your target distribution P*. (Where you are trying to draw samples from P) I'm wondering how this P* can arise without having access to P in the context of Bayesian Inference?,5,3
94,2012-7-30,2012,7,30,22,xe2w3,Target's learning algorithms figure out a girl is pregnant and accidentally inform her father. x-post from r/todayilearned,https://www.reddit.com/r/MachineLearning/comments/xe2w3/targets_learning_algorithms_figure_out_a_girl_is/,omlettehead,1343656671,,0,0
95,2012-7-31,2012,7,31,1,xecs0,How important is Java/C++ vs just using R/Matlab for big data?,https://www.reddit.com/r/MachineLearning/comments/xecs0/how_important_is_javac_vs_just_using_rmatlab_for/,thestatsmancan26,1343667087,"hey guys, I am a biostats student currently working in genomics and neural signals processing.  I am very comfortable with R and Matlab and have a good amount of ML experience (in R) as well.  

It seems to me that a lot of these jobs want people that can develop in Java or have a much stronger CS background than I have (I've done a little C from way back when).  I'm sure I can pick up the basics, but have no experience in actual software development or working directly with data bases (sql).  My questions are: 

On what level is Java/C++ usually implemented in these types of jobs and are there any resources to get familiar with using Java in a big data type of setting?  

Other than speed, are there other big advantages to using a compiled language?  ",19,24
96,2012-7-31,2012,7,31,4,xemvh,"Highly Parallel Sparse Matrix-Matrix Multiplication - Aydn Buluc, John R. Gilbert",https://www.reddit.com/r/MachineLearning/comments/xemvh/highly_parallel_sparse_matrixmatrix/,solen-skiner,1343676387,,2,3
97,2012-7-31,2012,7,31,8,xf2a4,Resources for linear system programming,https://www.reddit.com/r/MachineLearning/comments/xf2a4/resources_for_linear_system_programming/,solen-skiner,1343690383,"Been researching all evening for a small project, thought i should share :)

[Review of linear algebra libraries](http://verdandi.gforge.inria.fr/doc/linear_algebra_libraries.pdf)

[Benchmark of C++ Libraries for Sparse Matrix Computation](http://grh.mur.at/misc/sparselib_benchmark/)

Libraries:

[SuitSparse](http://www.cise.ufl.edu/research/sparse/SuiteSparse/)

[Matrix Template Library](http://www.simunova.com/node/33)

[Flens](http://flens.sourceforge.net/)

[Gmm++](http://download.gna.org/getfem/html/homepage/gmm.html)

[Eigen](http://eigen.tuxfamily.org/)

[PETSc](http://www.mcs.anl.gov/petsc/)

[Trilinos](http://trilinos.sandia.gov/capability_areas.html)

[UMFPACK](http://www.cise.ufl.edu/research/sparse/umfpack/)

[MAGMA](http://icl.cs.utk.edu/magma/overview/index.html)

[ViennaCL](http://viennacl.sourceforge.net/)

I'm sure i've missed some nuggets of gold hiding in the streams of this net we surf, please don't hesitate to share!",2,2
98,2012-7-31,2012,7,31,12,xfgks,"Possible research directions in large scale deep learning
",https://www.reddit.com/r/MachineLearning/comments/xfgks/possible_research_directions_in_large_scale_deep/,marshallp,1343704313,"I'd like propose some research directions in large scale deep learning (so that those of you in academia can publish papers and code and I can get the fruits of that labor).


The basic premise - stacking autoencoders in various ways, where an autoencoder is simple a 3 layer neural network whose in and out layers are the same size and the hidden layer is smaller. Training consists of minimizing the difference between data going into and coming out of the network. Afterwards the hidden-to-out layer is thrown away (the decoder stage) and the in-to-hidden is kept (the encoder stage). Training can be sgd or simulated annealing or ga etc.


Multiple autoencoders can be trained over the same data (from random starting spaces) and they will land on different ""hills"" of the optimization landscape (so that ensembles will tend work better).


The output of one encoder stage can be fed as data into 1 or more other autoencoders, so that you have a directed acyclic graph of encoder stages. Further, this encoder dag will tend to be too large to fit in the memory of one computer and will be distributed across a network.


Research+code:


- Determining whether two encoders have hit the same ""hill"". This can be done wither be directly examining the connection weights or examining the data output. If the same hill has been reached, methods for caching/memoizing the data input so that encoder dag is more efficient.


- Building a ""library"" as in genomics/connectomics, the ""artificial connectome"", of the encoders and encoder dags, over various datasets such as imagenet, speech dataset, wikipedia, human genome etc.. Analysis on those artificial connectomes, and between connectomes (e.g. have the same encoder dags been found in the wikipedia connectome and the imagenet connectome).


- performance improvements, such as creating fpga/asic for autoencoder training.

",4,4
99,2012-7-31,2012,7,31,19,xfz71,Packaging machine exporters India,https://www.reddit.com/r/MachineLearning/comments/xfz71/packaging_machine_exporters_india/,packingmachinesindia,1343732124,,0,0
100,2012-7-31,2012,7,31,22,xg5e6,Ok. What exactly is 'deep learning'?,https://www.reddit.com/r/MachineLearning/comments/xg5e6/ok_what_exactly_is_deep_learning/,Ayakalam,1343742448,"Hello all, 

I have done some ML, and being somewhat plugged into the scene, I keep hearing about deep learning this and deep learning that. 

For all the blogs/articles/soundbites I come across however, I cannot seem to come across just what *exactly* deep learning is. Its starting to sound like a buzz word unfortunately, but maybe there is some gold in the ruff. 

So, what is deep learning precisely? What will it help us do differently, and how? If you feel you can couch it in terms of supervised/unsupervised learning, all the better, with the constraint that the explanation still be precise, clear, and intuitive. 

Thanks! :-)",27,31
101,2012-7-31,2012,7,31,22,xg5el,Artificial intelligence and predictive modelling: can an airport think?,https://www.reddit.com/r/MachineLearning/comments/xg5el/artificial_intelligence_and_predictive_modelling/,adanmanri,1343742468,,1,1
102,2012-7-31,2012,7,31,23,xg8v7,Feature Importance For Logistic Regression?,https://www.reddit.com/r/MachineLearning/comments/xg8v7/feature_importance_for_logistic_regression/,lpiloto,1343746505,"Is there an established methodology for determining which features are important when training a logistic regression classifier?  The most obvious first thought is to look at each feature's raw weight as an indication of feature importance, but this seems too simple of an approach.  Perhaps some sort of permutation test or something else?  Any thoughts or pointers to references would be greatly appreciated.

EDIT: Just for clarity, since it seems to be confusing some people.  By feature importance, I do not mean feature selection (choosing which features to feed into your classifier), but instead the assessment of which features in an already trained classifier are most important for the classification output.",8,4
