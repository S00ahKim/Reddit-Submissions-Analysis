,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2016-9-1,2016,9,1,10,50kuv2,Suggestions ?? Test loss (validation) not improving using finetuned VGG net.,https://www.reddit.com/r/MachineLearning/comments/50kuv2/suggestions_test_loss_validation_not_improving/,HumbleNoob,1472695015,"Hi everyone,

I've been working on fine-tuning VGG16 net for a facial classification task. I've replaced the last classification layer with a binary softmax classification layer and binary crossentropy loss.  I'm unable to improve the test loss of the model ( see attached image ) beyond 0.28. Need your advice !!

The dataset consists of 2 classes- 1887 for class 0 and 1939 for class 1. I have used data augmentation ( random translate + random rotate ) as well as face aligned images in the dataset. Fine-tuning all 3 fully connected layers + last 3 convolution layers.
My network settings are - Batch size = 30, Adagrad optimizer with a learning rate decay of 0.1 at every 6 epochs. Also increased the dropout.
Snapshot of training profile- https://imgur.com/a/6rqul

Edit: Training loss decreases initially but saturates after epoch 15

Thanks for your suggestions ! ",2,1
1,2016-9-1,2016,9,1,12,50l6pn,Hi.I(senior undergrad) am taking a grad level ML course(first ML course ever) that focuses on using biological data. We are expected to propose a final project topic in a couple weeks that we will work on for the whole semester. Can you guys critique my potential topic/ suggest possible ones?,https://www.reddit.com/r/MachineLearning/comments/50l6pn/hiisenior_undergrad_am_taking_a_grad_level_ml/,BlueFolliage,1472699629,"So I am a computer engineering student taking this course that is offered by my schools Comp Bio program. I've never worked with ML or bio data and wasn't sure what kind of topic to pick. 

My potential topic was gonna be focused on finding cracks/breaks in bones. I was gonna train my algorithm using bones(taking an Image processing course as well) like from the arm or leg, and use a set of broken, uncracked, and normal bones to train it. I'm curious what you guys think about the idea. 

Also if you have any suggestions on potential topics let me know. ",13,1
2,2016-9-1,2016,9,1,13,50lg3v,Acer's Predator 21 X puts a curved screen and dual GTX 1080s in a laptop,https://www.reddit.com/r/MachineLearning/comments/50lg3v/acers_predator_21_x_puts_a_curved_screen_and_dual/,InaneMembrane,1472703516,,0,1
3,2016-9-1,2016,9,1,15,50lx90,Use Torch-hdf5 to save Tensor to Hdf5,https://www.reddit.com/r/MachineLearning/comments/50lx90/use_torchhdf5_to_save_tensor_to_hdf5/,windweller,1472712514,"I'm trying to ask this question on StackOverflow but nobody seems to be answering it for 3 days....so I decide to post it here...

http://stackoverflow.com/questions/39220312/use-torch-hdf5-to-save-tensor-to-hdf5

I'm not too familiar with Hdf5 format...Tutorial example on github (https://github.com/deepmind/torch-hdf5/blob/master/doc/usage.md) says:


require 'hdf5'

local myFile = hdf5.open('/path/to/write.h5', 'w')

myFile:write('/path/to/data', torch.rand(5, 5))

myFile:close()


I don't know what ""/path/to/data"" refers to...and if I'm only trying to save a torch tensor, why do I have to put a '/path/to/data' there?",4,0
4,2016-9-1,2016,9,1,15,50lx9e,"I am an Electronics Engineer and interested in doing my Masters in Machine Learning, where do I begin?",https://www.reddit.com/r/MachineLearning/comments/50lx9e/i_am_an_electronics_engineer_and_interested_in/,sssssunshine,1472712520,"I've been reading a lot about the applications of ML over the past few months and it has captivated me. We all agree it has huge amounts of potential and the field is only gonna diversify in the future. 

So, I've decided to consider it for my masters. Now, coming from a non-cse background, I have a fairly less amount knowledge in this regard and don't know where to begin. I hope you guys can help me out.

Cheers.",11,0
5,2016-9-1,2016,9,1,16,50lykb,Improving Inception and Image Classification in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/50lykb/improving_inception_and_image_classification_in/,modeless,1472713294,,0,3
6,2016-9-1,2016,9,1,16,50lzwv,Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset,https://www.reddit.com/r/MachineLearning/comments/50lzwv/tactics_to_combat_imbalanced_classes_in_your/,Dawny33,1472714069,,0,1
7,2016-9-1,2016,9,1,18,50magw,RNN looks like FRP,https://www.reddit.com/r/MachineLearning/comments/50magw/rnn_looks_like_frp/,jocomoco,1472720507,,24,1
8,2016-9-1,2016,9,1,19,50mm2i,Which Reinforcement learning library is better?,https://www.reddit.com/r/MachineLearning/comments/50mm2i/which_reinforcement_learning_library_is_better/,sallyfox,1472727479,[removed],0,1
9,2016-9-1,2016,9,1,20,50mopi,Analyzing the Papers Behind Facebooks Computer Vision Approach,https://www.reddit.com/r/MachineLearning/comments/50mopi/analyzing_the_papers_behind_facebooks_computer/,adeshpande3,1472728972,,3,20
10,2016-9-1,2016,9,1,20,50mq8t,Algorithms to a multi-agent system with continuous Markov Decision Process,https://www.reddit.com/r/MachineLearning/comments/50mq8t/algorithms_to_a_multiagent_system_with_continuous/,[deleted],1472729761,[deleted],0,1
11,2016-9-1,2016,9,1,21,50mybr,State-of-the-art ensemble learning algorithm in pattern recognition tasks?,https://www.reddit.com/r/MachineLearning/comments/50mybr/stateoftheart_ensemble_learning_algorithm_in/,ErbaAit,1472733521,[removed],0,1
12,2016-9-1,2016,9,1,22,50n68r,Keras code for fine-tuning InceptionV3 to your own dataset,https://www.reddit.com/r/MachineLearning/comments/50n68r/keras_code_for_finetuning_inceptionv3_to_your_own/,danielvarga,1472736642,,1,21
13,2016-9-1,2016,9,1,22,50n7wd,Creative ML Tabletop experiments?,https://www.reddit.com/r/MachineLearning/comments/50n7wd/creative_ml_tabletop_experiments/,Kevin_Clever,1472737256,I am looking for computer experments to showcase ML techniques to a general public.  What do y'all think would be cool?,2,0
14,2016-9-1,2016,9,1,23,50ngzs,DeepPy: Pythonic Deep Learning,https://www.reddit.com/r/MachineLearning/comments/50ngzs/deeppy_pythonic_deep_learning/,KeponeFactory,1472740532,,5,17
15,2016-9-2,2016,9,2,0,50nmt7,"Artificial Intelligence, Deep Learning, and Neural Networks Explained",https://www.reddit.com/r/MachineLearning/comments/50nmt7/artificial_intelligence_deep_learning_and_neural/,innoarchitech,1472742402,,1,0
16,2016-9-2,2016,9,2,0,50no07,"[YA*2V] Hash2Vec, Feature Hashing for Word Embeddings",https://www.reddit.com/r/MachineLearning/comments/50no07/ya2v_hash2vec_feature_hashing_for_word_embeddings/,improbabble,1472742798,,4,18
17,2016-9-2,2016,9,2,0,50npcq,Right shift when predicting multiple steps ahead in future;lstm sine function prediction,https://www.reddit.com/r/MachineLearning/comments/50npcq/right_shift_when_predicting_multiple_steps_ahead/,guukizl,1472743206,"code : http://pastebin.com/B33a5tph

plots : http://imgur.com/a/DASZq

Hello! I am trying to predict sine function using lstm using time windows using [tflearn](http://tflearn.org/). When I try to train network to predict say 10 or 20 steps ahead in future, I am getting a right-shift in the prediction of my network w.r.t actual values. More right-shift with 20 steps ahead than 10 steps. I have used 20 in fig to show the shift clearly. Nothing solves this. Increasing the data by increasing 'n', changing number of units or adding another layer, I get the same right-shift. The only thing that solves it is changing step_radians to 0.001 thus increasing density of my points. 

It would be great if someone could explain to me why this happens.
Thanks.

Original code : https://github.com/tflearn/tflearn/issues/121

",6,0
18,2016-9-2,2016,9,2,0,50nq18,What Role Does an Expert Play In Making Selection For Brennan Adapters?,https://www.reddit.com/r/MachineLearning/comments/50nq18/what_role_does_an_expert_play_in_making_selection/,jackerfrinandis,1472743428,,0,1
19,2016-9-2,2016,9,2,0,50nqyh,Finding Information On Predictive Analytics,https://www.reddit.com/r/MachineLearning/comments/50nqyh/finding_information_on_predictive_analytics/,Gopher247,1472743725,Hey everyone does anyone know of any good resources discussing predictive analytics and the different techniques which can be applied to analyze a dataset?,1,0
20,2016-9-2,2016,9,2,0,50nsip,MLPaint: The Real-Time Handwritten Digit Recognizer,https://www.reddit.com/r/MachineLearning/comments/50nsip/mlpaint_the_realtime_handwritten_digit_recognizer/,ddcarnage,1472744217,,1,0
21,2016-9-2,2016,9,2,1,50nzor,Are unsupervised neural networks good for finding frequent patterns?,https://www.reddit.com/r/MachineLearning/comments/50nzor/are_unsupervised_neural_networks_good_for_finding/,alehander42,1472746471,"Hi,

I have some data and I want to detect frequent patterns in it, so I realised that I don't have a strict definition of the patterns, so probably auto finding clusters and analyzing their size and frequency can do it for me: is this the standard way to do stuff like this",15,9
22,2016-9-2,2016,9,2,1,50o0q2,Implementation of Neural Style Transfer in Keras 1.0.8,https://www.reddit.com/r/MachineLearning/comments/50o0q2/implementation_of_neural_style_transfer_in_keras/,pmigdal,1472746791,,2,19
23,2016-9-2,2016,9,2,1,50o5oq,Claudia Perlich on her favorite machine learning algorithm,https://www.reddit.com/r/MachineLearning/comments/50o5oq/claudia_perlich_on_her_favorite_machine_learning/,rrenaud,1472748309,,4,6
24,2016-9-2,2016,9,2,1,50o6oz,Whats Missing From Machine Learning,https://www.reddit.com/r/MachineLearning/comments/50o6oz/whats_missing_from_machine_learning/,[deleted],1472748621,[deleted],0,0
25,2016-9-2,2016,9,2,2,50ofzd,[1608.08710v1] Pruning Filters for Efficient ConvNets,https://www.reddit.com/r/MachineLearning/comments/50ofzd/160808710v1_pruning_filters_for_efficient_convnets/,connectionism,1472751401,,3,6
26,2016-9-2,2016,9,2,3,50os8d,"sklearn.linear_model.LinearRegression , normalize=True does absolutely nothing!",https://www.reddit.com/r/MachineLearning/comments/50os8d/sklearnlinear_modellinearregression_normalizetrue/,andraxo123,1472755176,"so I'm translating andrew ng's matlab code to python in the first exercise , I have to feature normalize (we haven number of bedrooms (1-6) and Price(30000-40000) so its obvious that feature scaling is necessary here , but here is the problem :

regr = LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)

regr.fit(X[:,0:n], y.ravel())

regr.score(X[:,0:n],y.ravel())

0.7329450180289141

OK, all good now with normalization (notice the normalize = True)

regr = LinearRegression(fit_intercept=True, normalize=True, copy_X=True, n_jobs=1)

regr.fit(X[:,0:n], y.ravel())

regr.score(X[:,0:n],y.ravel())

0.7329450180289141

same score!! Am i doing something wrong here? please help im very confused

also when I predict regr.predict(np.array([2104,3]))

I get the same prediction so its clearly not working",5,0
27,2016-9-2,2016,9,2,3,50osnc,Help in creating algorithm for analyzing stock returns,https://www.reddit.com/r/MachineLearning/comments/50osnc/help_in_creating_algorithm_for_analyzing_stock/,redditdabbler,1472755308,"I have a basic idea about machine learning and I know firms like Numer.ai are using machine learning to predict future returns. Are we in a stage where we can actually use machine learning to predict stock returns. 

If I was to create an algorithm using the open source options, where do I start and how do I go about it? Thanks!",8,0
28,2016-9-2,2016,9,2,3,50osw0,"Step-by-step tutorial to solve the Titanic challenge of Kaggle. Score 0.8134, Top 10%",https://www.reddit.com/r/MachineLearning/comments/50osw0/stepbystep_tutorial_to_solve_the_titanic/,ahmedbesbes,1472755386,,48,157
29,2016-9-2,2016,9,2,4,50p409,"Neil Lawrence joins Amazon's Cambridge,UK Research Lab",https://www.reddit.com/r/MachineLearning/comments/50p409/neil_lawrence_joins_amazons_cambridgeuk_research/,fhuszar,1472758829,,15,11
30,2016-9-2,2016,9,2,4,50p46q,Is there a way to stitch MFCC Features back to Audio?,https://www.reddit.com/r/MachineLearning/comments/50p46q/is_there_a_way_to_stitch_mfcc_features_back_to/,flashdude64,1472758884,"I am working on feature manipulation of audio. I was wondering if there was a method for me to reconstruct an audio file from the MFCC.
",3,2
31,2016-9-2,2016,9,2,4,50p5i7,Ideas to compare scores across multiple logistic regression classifiers,https://www.reddit.com/r/MachineLearning/comments/50p5i7/ideas_to_compare_scores_across_multiple_logistic/,oldSoul12345,1472759285,"I am have multiple classifiers built on mutually exclusive datasets and want to compare predicted probabilities for a single item.
So suppose for a item,   
predictor built on DS1-&gt; shirt:0.7, skirt 0.3.  
predictor built on DS2-&gt; gears:0.34,parts:0.44, engines:0.22.  
predictor built on DS3-&gt; RAM:0.28,monitor:0:19, SSD:0.24,Laptop:0.39.  

Just using the highest probabilities will penalize the bigger classifiers like DS3(more subcategories). A crude way is to subtract 1/no_of_subclasses. 

So, 

predictor built on DS1-&gt; shirt:0.7, skirt 0.3. (subtract 1/2)-&gt; shirt:0.2, skirt:-0.2.  
predictor built on DS2-&gt; gears:0.34,parts:0.44, engines:0.22. (subtract 1/3)-&gt;
gears:0.01,parts:0.10, engines:-0.11.  
predictor built on DS3-&gt; RAM:0.28,monitor:0:19, SSD:0.24,Laptop:0.39.(subtract 1/4)-&gt; RAM:0.03,monitor:-0:06, SSD:-0.01,Laptop:0.14 .  

What's the mathematically sound way to compare probabilities from classifiers with different cardinality in cases like this ? ",0,0
32,2016-9-2,2016,9,2,5,50p9uy,AdaNet: Adaptive Structural Learning of Artificial Neural Networks,https://www.reddit.com/r/MachineLearning/comments/50p9uy/adanet_adaptive_structural_learning_of_artificial/,Foxtr0t,1472760685,"https://arxiv.org/abs/1607.01097

Implementation anyone?",1,5
33,2016-9-2,2016,9,2,5,50pcbm,[1608.08266] Visualizing and Understanding Sum-Product Networks,https://www.reddit.com/r/MachineLearning/comments/50pcbm/160808266_visualizing_and_understanding/,ciolaamotore,1472761512,,0,8
34,2016-9-2,2016,9,2,6,50pk7q,Can machine learning tackle the top killer of American women?,https://www.reddit.com/r/MachineLearning/comments/50pk7q/can_machine_learning_tackle_the_top_killer_of/,bitMTW,1472764071,,0,0
35,2016-9-2,2016,9,2,6,50plkt,How to best model an input vector of 0...n medical procedures?,https://www.reddit.com/r/MachineLearning/comments/50plkt/how_to_best_model_an_input_vector_of_0n_medical/,surangakas,1472764580,[removed],0,1
36,2016-9-2,2016,9,2,6,50poc1,"Tutorials with real, practical examples",https://www.reddit.com/r/MachineLearning/comments/50poc1/tutorials_with_real_practical_examples/,Komakuji,1472765535,"I keep seeing the tutorials where they program for the sake of programming with no real reason. Are there tutorials with real, practical examples such building a simple anti-spam filter or breaking a captcha or creating a chat-bot using machine learning or anything else as long as a task is real?  ",2,0
37,2016-9-2,2016,9,2,7,50pvgh,Can Andrew Ng's ML Course in Coursera land me a job as Junior Machine Learning developer?,https://www.reddit.com/r/MachineLearning/comments/50pvgh/can_andrew_ngs_ml_course_in_coursera_land_me_a/,Rydra,1472768005,"Hi,

I'm a senior web/mobile developer and I decided I need a professional change in my career into ML topics (due to my innate interest in AI), and recently I've completed the Andrew's Ng famous Course in Coursera and obtained the certificate. Although it's stated as introductory, it's gone quite deeply in a lot of topics that can I feel I could very easily and autonomously turn the concepts into good practice and real-world gigs (not expecting a Big 10 company position, just for example a Junior position as Machine Learning developer or Data Analyst in startup or w/e to get me to a point from where to grow up and get flooded with motivation to go for more advanced ML topics and courses).

With that course and considering I'm already a python developer fiddling a bit with scikit-learn, can I go to companies asking for junior positions in this regard and slowly acquire experience, making up a good career? Which would be the next steps I should take in order to get further into this realm?

Thanks in advance for your answers.",2,0
38,2016-9-2,2016,9,2,7,50q2ab,The rise of IOT and Machine Learning brings about moral and ethical issues to mankind. Can machines be trusted to make decisions between life and death?,https://www.reddit.com/r/MachineLearning/comments/50q2ab/the_rise_of_iot_and_machine_learning_brings_about/,Emeka_Nwonu,1472770371,,0,1
39,2016-9-2,2016,9,2,8,50q9rs,Artifical intelligence and life in 2030,https://www.reddit.com/r/MachineLearning/comments/50q9rs/artifical_intelligence_and_life_in_2030/,tuan3w,1472773018,,10,18
40,2016-9-2,2016,9,2,8,50qa5w,2017 Google Brain residency program,https://www.reddit.com/r/MachineLearning/comments/50qa5w/2017_google_brain_residency_program/,doomie,1472773155,,18,40
41,2016-9-2,2016,9,2,9,50qelr,"Hypothetical scenario: as a hobby researcher, you architect a model that you believe is beating a known benchmark. What do you do?",https://www.reddit.com/r/MachineLearning/comments/50qelr/hypothetical_scenario_as_a_hobby_researcher_you/,cjmcmurtrie,1472774810,"As an industry professional with no connections to any research groups, and no PhD (although I do have a master's in machine learning from a top-rated school), I enjoy spending the evenings attempting speculative models. I'm sure there are others here in similar situations.

Let's suppose you come across something that you strongly believe is a significant discovery. What do you do? It isn't much use keeping it to yourself, but there are many people in the world who could exploit your discovery faster than you could, if you were to let them in on it. If you apply for a PhD, you may not have time to get credit for your discovery, or someone else in your new research group may usurp it. You could write up a paper and try to publish, but what credibility do you have? Or perhaps you could try to pitch your idea to more established collaborators - again under the risk that they will  run with it without you.

Has anybody thought about this scenario, and for the experienced researchers here - what would be your choice of action?",18,5
42,2016-9-2,2016,9,2,9,50qgj4,Just started an Abradolf Lincler account on Twitter powered by Deep Neural Nets! (X-post /r/rickandmorty),https://www.reddit.com/r/MachineLearning/comments/50qgj4/just_started_an_abradolf_lincler_account_on/,Sudo137,1472775557,,9,10
43,2016-9-2,2016,9,2,10,50qnds,Are you missing the random forest for the leaves? Identify leaf shapes using machine learning,https://www.reddit.com/r/MachineLearning/comments/50qnds/are_you_missing_the_random_forest_for_the_leaves/,benhamner,1472778172,,0,2
44,2016-9-2,2016,9,2,10,50qoyc,"melamine decorative paper for HPL,MDF,Flooring,Furniture,Phenolic film.",https://www.reddit.com/r/MachineLearning/comments/50qoyc/melamine_decorative_paper_for/,unicovincent,1472778782,,0,1
45,2016-9-2,2016,9,2,10,50qu5u,"CPU, GPU Put to Deep Learning Framework Test",https://www.reddit.com/r/MachineLearning/comments/50qu5u/cpu_gpu_put_to_deep_learning_framework_test/,harrism,1472780802,,0,2
46,2016-9-2,2016,9,2,11,50qy9a,My PARKER Company,https://www.reddit.com/r/MachineLearning/comments/50qy9a/my_parker_company/,parkermary,1472782435,,0,1
47,2016-9-2,2016,9,2,11,50qzrs,What does the chemical reactors look like in JCT Machinery?,https://www.reddit.com/r/MachineLearning/comments/50qzrs/what_does_the_chemical_reactors_look_like_in_jct/,mixmachinery,1472783035,,1,1
48,2016-9-2,2016,9,2,12,50r79g,Continuous Learning!!! New Progressive Learning Technique for Multi-class Classification,https://www.reddit.com/r/MachineLearning/comments/50r79g/continuous_learning_new_progressive_learning/,rakesajar,1472786082,,0,5
49,2016-9-2,2016,9,2,12,50r8p6,Online Multi-label Classifier for High Speed Streaming Data Applications,https://www.reddit.com/r/MachineLearning/comments/50r8p6/online_multilabel_classifier_for_high_speed/,rakesajar,1472786710,,1,0
50,2016-9-2,2016,9,2,15,50rxqx,On-board diagnostics (OBD) + machine learning to improve your driving (question),https://www.reddit.com/r/MachineLearning/comments/50rxqx/onboard_diagnostics_obd_machine_learning_to/,zibenmoka,1472798462,"Hello folks, 

Has any of you tried to improve their driving using realtime data from [OBD](https://en.wikipedia.org/wiki/On-board_diagnostics) and machine learning? 

To me is seems like interesting source of labeled data. My behaviour (gear change, pressing on the gas etc) implies certain parameters changing. It does look like a nice problem for LSTM maybe or RL. What is your opinion on that? Any experience maybe? 

",4,6
51,2016-9-2,2016,9,2,15,50ry3p,Salesforces spent hundreds of millions to build AI app Einstein,https://www.reddit.com/r/MachineLearning/comments/50ry3p/salesforces_spent_hundreds_of_millions_to_build/,Jean-Porte,1472798656,,3,9
52,2016-9-2,2016,9,2,16,50s05e,Turing learning: a metric-free approach to inferring behavior and its application to swarms,https://www.reddit.com/r/MachineLearning/comments/50s05e/turing_learning_a_metricfree_approach_to/,redmercurysalesman,1472799712,,0,1
53,2016-9-2,2016,9,2,16,50s0sz,Neural Networks for Genomics - A Survey,https://www.reddit.com/r/MachineLearning/comments/50s0sz/neural_networks_for_genomics_a_survey/,saucysassy,1472800080,,9,14
54,2016-9-2,2016,9,2,17,50s99t,C# Implementation of Karpathy's char-rnn,https://www.reddit.com/r/MachineLearning/comments/50s99t/c_implementation_of_karpathys_charrnn/,VIRXXIII,1472805150,"Ladies and Gentlemen,

I'm brandnew to ML but a pretty experienced C#er. I'm looking for a C# implementation of [char-rnn](https://github.com/karpathy/char-rnn) or any C# sample-project doing the same thing (learning on a dataset consisting of text and then sampling/outputting/writing new text in the same style). 

As mentioned, I know close to nothing about ML and looking at the complex Frameworks like Accord.Net or Encog makes me just feel very small and lost. I don't know where to start. ",2,8
55,2016-9-2,2016,9,2,17,50s9b3,How dropout affects loss value,https://www.reddit.com/r/MachineLearning/comments/50s9b3/how_dropout_affects_loss_value/,pgaleone,1472805171,"Classical regularization methods (like L1/L2) penalize network weights explicitly. We can see the amount of penalization in the initial loss value and calculate it.

Dropout, that's an empirical regularization method, affects the loss as the other methods do. In fact, a network without dropout that uses softmax(cross_entropy(predictions, real_labels)) as loss functon has an initial value of about -ln(1/number_of_classes).

When 1 (or more) dropout layer get's added to the network, the initial loss value change by a specified amount, let's call it alpha.

So, is there a way I can determine the value of alpha?",8,0
56,2016-9-2,2016,9,2,18,50sbze,The Machines can Do Heavy Duty for You along with Quality,https://www.reddit.com/r/MachineLearning/comments/50sbze/the_machines_can_do_heavy_duty_for_you_along_with/,MarkPietersen,1472806928,,0,1
57,2016-9-2,2016,9,2,18,50sca1,Explanation: Why is polynomial regression considered a special case of multiple linear regression?,https://www.reddit.com/r/MachineLearning/comments/50sca1/explanation_why_is_polynomial_regression/,Dawny33,1472807120,,4,17
58,2016-9-2,2016,9,2,19,50sng7,A list of Neural MT implementations,https://www.reddit.com/r/MachineLearning/comments/50sng7/a_list_of_neural_mt_implementations/,Hugeer,1472813749,,2,20
59,2016-9-2,2016,9,2,20,50srsg,Is Theano as easy to learn as Tensorflow?,https://www.reddit.com/r/MachineLearning/comments/50srsg/is_theano_as_easy_to_learn_as_tensorflow/,pipcrawler,1472816102,"I learnt deep learning on Tensorflow. Now that I'm actually applying it to my projects I realize that using Tensorflow through window VM hack is not gonna get me far, since it can't use gpu and the kernel crashes the moment I do something mildly complex with convnets. So the way it looks I either need to install Ubuntu and work with Tensorflow from there or learn some new environment like Theano. 

My question is if Theano is as easy to learn as Tensorflow for somebody who is neither a Python nor deep networks expert?

Edit: Thanks everyone for great suggestions! There's lots to read and lots to consider, but now i have some resources and don't fill like I'm in a completely dark tunnel. More like a very slightly lit tunnel :)

Edit 2: While I was reading many of forums today I came across another deep network pack called Matconvnet for Matlab. I know Matlab is a less popular choice , but by any chance anyone used it ever or has an opinion on it. I have worked on Matlab for many years and it would be the easiest transition if I could just use that. I wonder about the functionality though..",57,46
60,2016-9-2,2016,9,2,22,50t3iw,Making use of attributes present only in the training data.,https://www.reddit.com/r/MachineLearning/comments/50t3iw/making_use_of_attributes_present_only_in_the/,anantzoid,1472821358,"In a certain classification problem involving predicting if an email was opened or not, certain attributes are present only in the training dataset. I was wondering if there is any way to make use of those features during training that might help in improving the model?",9,7
61,2016-9-2,2016,9,2,22,50t584,Machine Learning verification and Explainable AI,https://www.reddit.com/r/MachineLearning/comments/50t584/machine_learning_verification_and_explainable_ai/,yoav_hollander,1472821997,,0,1
62,2016-9-2,2016,9,2,22,50t9jc,"Workflows in Python Part 3: Robust &amp; Compact Code By Caitlin Malone, Data Scientist - Civis Analytics",https://www.reddit.com/r/MachineLearning/comments/50t9jc/workflows_in_python_part_3_robust_compact_code_by/,OpenDataSciCon,1472823572,,3,2
63,2016-9-2,2016,9,2,22,50ta3j,Computer Vision News of September,https://www.reddit.com/r/MachineLearning/comments/50ta3j/computer_vision_news_of_september/,Gletta,1472823767,"Computer Vision News of September has just been published. 
40 pages of great computer vision (and machine learning) stories with plenty of codes: http://www.rsipvision.com/ComputerVisionNews-2016September/    
As usual, it's 100% free, with free subscription at page 39. 
Enjoy ;)",0,5
64,2016-9-2,2016,9,2,22,50tbjp,Stacked Approximated Regression Machine: A Simple Deep Learning Approach,https://www.reddit.com/r/MachineLearning/comments/50tbjp/stacked_approximated_regression_machine_a_simple/,r-sync,1472824284,"Paper at http://arxiv.org/abs/1608.04062

Incredible claims:

* Train only using about 10% of imagenet-12, i.e. around 120k images (i.e. they use 6k images per arm)
* get to the same or better accuracy as the equivalent VGG net
* Training is not via backprop but more simpler PCA + Sparsity regime (see section 4.1), shouldn't take more than 10 hours just on CPU probably (I think, from what they described, haven't worked it out fully).

Thoughts?

For background reading, this paper is very close to Gregor &amp; LeCun (2010): http://yann.lecun.com/exdb/publis/pdf/gregor-icml-10.pdf",49,184
65,2016-9-2,2016,9,2,22,50tbld,Machine Learning verification and Explainable AI,https://www.reddit.com/r/MachineLearning/comments/50tbld/machine_learning_verification_and_explainable_ai/,yoav_hollander,1472824301,,0,2
66,2016-9-2,2016,9,2,23,50tegi,Learning a Predictable and Generative Vector Representation for Objects,https://www.reddit.com/r/MachineLearning/comments/50tegi/learning_a_predictable_and_generative_vector/,tangente,1472825377,,1,3
67,2016-9-2,2016,9,2,23,50tf1w,[1608.07249] Benchmarking State-of-the-Art Deep Learning Software Tools,https://www.reddit.com/r/MachineLearning/comments/50tf1w/160807249_benchmarking_stateoftheart_deep/,galloguille,1472825599,,4,1
68,2016-9-3,2016,9,3,0,50tuhp,Machine Learning - WAYR (What Are You Reading) - Week 7,https://www.reddit.com/r/MachineLearning/comments/50tuhp/machine_learning_wayr_what_are_you_reading_week_7/,cvikasreddy,1472830916,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.  

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.  

[Week 1](https://www.reddit.com/r/MachineLearning/comments/4qyjiq/machine_learning_wayr_what_are_you_reading_week_1/)  
[Week 2](https://www.reddit.com/r/MachineLearning/comments/4s2xqm/machine_learning_wayr_what_are_you_reading_week_2/)  
[Week 3](https://www.reddit.com/r/MachineLearning/comments/4t7mqm/machine_learning_wayr_what_are_you_reading_week_3/)  
[Week 4](https://www.reddit.com/r/MachineLearning/comments/4ub2kw/machine_learning_wayr_what_are_you_reading_week_4/) 

[Week 5](https://www.reddit.com/r/MachineLearning/comments/4xomf7/machine_learning_wayr_what_are_you_reading_week_5/) 

[Week 6](https://www.reddit.com/r/MachineLearning/comments/4zcyvk/machine_learning_wayr_what_are_you_reading_week_6/)

 
Besides that, there are no rules, have fun.",0,2
69,2016-9-3,2016,9,3,0,50tupg,Statistical inference for data science,https://www.reddit.com/r/MachineLearning/comments/50tupg/statistical_inference_for_data_science/,losojosdelciego,1472830986,,1,5
70,2016-9-3,2016,9,3,1,50tyg4,Hyperparameter Optimization 101 [slides],https://www.reddit.com/r/MachineLearning/comments/50tyg4/hyperparameter_optimization_101_slides/,alexcmu,1472832219,,1,1
71,2016-9-3,2016,9,3,1,50u1x7,Neural Network Architectures,https://www.reddit.com/r/MachineLearning/comments/50u1x7/neural_network_architectures/,rubyantix,1472833345,,6,34
72,2016-9-3,2016,9,3,1,50u4v1,Machine learning can replace data scientists,https://www.reddit.com/r/MachineLearning/comments/50u4v1/machine_learning_can_replace_data_scientists/,[deleted],1472834293,[deleted],0,0
73,2016-9-3,2016,9,3,1,50u8ac,Stanford deep learning webpage,https://www.reddit.com/r/MachineLearning/comments/50u8ac/stanford_deep_learning_webpage/,stats_cs,1472835449,,2,15
74,2016-9-3,2016,9,3,2,50ua3s,"Benchmarks are in and most notable, TensorFlow runs up to 50% faster on the latest NVIDIA GPUs and scales across multiple GPUs within a single node. Now you can train models in hours instead of days. Check out the numbers.",https://www.reddit.com/r/MachineLearning/comments/50ua3s/benchmarks_are_in_and_most_notable_tensorflow/,NV_AI,1472836001,,0,3
75,2016-9-3,2016,9,3,2,50ubaw,source code for gvnn,https://www.reddit.com/r/MachineLearning/comments/50ubaw/source_code_for_gvnn/,cam781,1472836396,,1,10
76,2016-9-3,2016,9,3,2,50uc3t,Predicting grades from Facebook likes,https://www.reddit.com/r/MachineLearning/comments/50uc3t/predicting_grades_from_facebook_likes/,liviu-,1472836650,,0,3
77,2016-9-3,2016,9,3,2,50ud3o,GAN for learning the user representation,https://www.reddit.com/r/MachineLearning/comments/50ud3o/gan_for_learning_the_user_representation/,KrisSingh,1472836978,"I wanted to know if we could apply GAN for user representation learning in some latent space for the recommendation system.So if say we give some users from with some what similar interest then can
the generator learn to generate more of these users.",0,1
78,2016-9-3,2016,9,3,2,50ue1l,ICML talks 2016 online yet?,https://www.reddit.com/r/MachineLearning/comments/50ue1l/icml_talks_2016_online_yet/,dataislyfe,1472837282,"Hi guys,

I was wondering if there was a link to see the ICML talks from this year's conference online yet?

Thanks!",0,1
79,2016-9-3,2016,9,3,2,50ufri,Quora Session with Andrej Karpathy,https://www.reddit.com/r/MachineLearning/comments/50ufri/quora_session_with_andrej_karpathy/,shagunsodhani,1472837803,,0,3
80,2016-9-3,2016,9,3,3,50uqps,"Highlights from the 22nd Knowledge Discovery and Data Mining (KDD) conference, August 2016",https://www.reddit.com/r/MachineLearning/comments/50uqps/highlights_from_the_22nd_knowledge_discovery_and/,thvasilo,1472841284,"Hello all,

Last month I had the pleasure to attend KDD, the premier conference on Data Mining and Knowledge discovery, so as I did with [ICDM last year](http://tvas.me/conferences/2015/11/23/ICDM-2015-Highlights.html) I thought I would post my highlights from the conference, including workshops papers and keynotes.

Without further ado:


[Highlights from KDD 2016!](http://tvas.me/conferences/2016/09/01/KDD-2016-Highlights.html)

Did anyone else attend? Feedback and questions are welcome!
",0,3
81,2016-9-3,2016,9,3,3,50urdi,GAN for recommendation system,https://www.reddit.com/r/MachineLearning/comments/50urdi/gan_for_recommendation_system/,KrisSingh,1472841483,"Can the generative adversarial network be used for learning the representation of a user with similar interest so, through generator, we can create new users also?",2,1
82,2016-9-3,2016,9,3,4,50uxhg,Batch Normalization Patent Application,https://www.reddit.com/r/MachineLearning/comments/50uxhg/batch_normalization_patent_application/,DLPatenter,1472843482,,0,1
83,2016-9-3,2016,9,3,4,50uxj0,Smashbros AI with Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/50uxj0/smashbros_ai_with_deep_reinforcement_learning/,Joshaybee,1472843496,,5,30
84,2016-9-3,2016,9,3,4,50v1qp,Probability Cheatsheet,https://www.reddit.com/r/MachineLearning/comments/50v1qp/probability_cheatsheet/,adamnemecek,1472844871,,5,62
85,2016-9-3,2016,9,3,4,50v5te,Why do we convert RGB images to RGB float images when being input into a NN?,https://www.reddit.com/r/MachineLearning/comments/50v5te/why_do_we_convert_rgb_images_to_rgb_float_images/,neil454,1472846231,"Now I understand the fact that NNs use 32-bit floats for the network's weights and feature maps, so I get why the input has to be the same data type.

What I don't get is if we have an RGB image with values from 0-255 (8-bit), why don't we sort of compress that information to take advantage of the 32-bits of data we are converting it to. We are basically consuming 4 times as much memory, but for the same image.

For example, couldn't you convert a 256x256x3 8-bit image into a 64x64x3 32-bit representation, and lose no information? Maybe I'm missing something here...",7,2
86,2016-9-3,2016,9,3,4,50v65g,What tools are there for indexing and searching locally saved academic PDFs?,https://www.reddit.com/r/MachineLearning/comments/50v65g/what_tools_are_there_for_indexing_and_searching/,[deleted],1472846346,[deleted],1,1
87,2016-9-3,2016,9,3,5,50v7tf,One Hundred Year Study on Artificial Intelligence (AI100) from Stanford,https://www.reddit.com/r/MachineLearning/comments/50v7tf/one_hundred_year_study_on_artificial_intelligence/,Dogsindahouse1,1472846932,,3,26
88,2016-9-3,2016,9,3,5,50v80g,Tips on getting started with Kaggle?(x-post /r/learnmachinelearning),https://www.reddit.com/r/MachineLearning/comments/50v80g/tips_on_getting_started_with_kagglexpost/,kaggle-competition,1472847001,[removed],0,1
89,2016-9-3,2016,9,3,5,50vay4,Continuous Learning!! New Progressive Learning Technique for Multi-class Classification,https://www.reddit.com/r/MachineLearning/comments/50vay4/continuous_learning_new_progressive_learning/,rakesajar,1472847997,,4,1
90,2016-9-3,2016,9,3,5,50vge9,Laptop for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/50vge9/laptop_for_machine_learning/,aemrea1234,1472849850,"Hello guys,
I have experienced some difficulties while performing some machine learning algorithms(Tensorflow, sklearn, etc.) with my current laptop and looking to buy a new one. Can you recommend some laptops I can use for my machine learning studies? Thank you very much!",15,5
91,2016-9-3,2016,9,3,6,50vizp,"Is there a Universal Classifier? One which can perform binary, multi-class and multi-label classification",https://www.reddit.com/r/MachineLearning/comments/50vizp/is_there_a_universal_classifier_one_which_can/,rakesajar,1472850773,"Machine learning classification can be categorized into single-label classification (binary and multi-class) and multi-label classification. Single label classification problems involve mapping each of the input vectors to its unique target class from a pool of target classes/labels. However, there are several classification problems in which the target classes are not mutually exclusive and the input samples belong to more than one target class. These problems cannot be classified using the single label classification thus resulting in the need for multi-label classification in which each input sample belongs to a subset of target classes. Several machine learning classifiers have been developed and is available in the literature for each of the classification types. But the major limitation of all the classifiers in the literature is that, the classifiers are limited only to the particular type of classification problem for which it has been trained. 

*Is there any classifier that is capable of universally addressing all the aforementioned types of classification problems?*",4,3
92,2016-9-3,2016,9,3,6,50vou2,What major companies offer internships in health data analytics / AI?,https://www.reddit.com/r/MachineLearning/comments/50vou2/what_major_companies_offer_internships_in_health/,happyGradStudent87,1472852774,[removed],0,1
93,2016-9-3,2016,9,3,7,50vyum,Open problems in Variational Inference?,https://www.reddit.com/r/MachineLearning/comments/50vyum/open_problems_in_variational_inference/,dataislyfe,1472856399,"I've been reading up a lot on Variational methods in graphical models as well as deep variational inference applications (e.g., VAEs). I'm wondering if anyone knows of some good open problems in this area that could be relevant to the wider ML/optimization community? I've seen a listing of problems in Dave Blei's review (http://arxiv.org/pdf/1601.00670v3.pdf), but any other source/recommendations would be appreciated! To give you an idea of the problems I like: I'm particularly interested in alternative measures of divergence between the variational family (q(-)) and the actual posterior (for example, the use of alpha-divergences, etc.)... ",6,17
94,2016-9-3,2016,9,3,7,50vywm,SciRate - rate arXiv papers and send the best ones to the top,https://www.reddit.com/r/MachineLearning/comments/50vywm/scirate_rate_arxiv_papers_and_send_the_best_ones/,Arghzoo,1472856419,,4,0
95,2016-9-3,2016,9,3,8,50w36c,Performance of Markov Chain Monte Carlo,https://www.reddit.com/r/MachineLearning/comments/50w36c/performance_of_markov_chain_monte_carlo/,adamnemecek,1472857990,"Hey, can someone tell me about the real world performance of Markov Chain Monte Carlo? I'm curious what particular method did you use, what programming language (and if you used a framework what framework did you use), as much as you are willing to share about your data and how it performed in the real world. Did you end up doing any optimizations? By performance I mean speed.",0,3
96,2016-9-3,2016,9,3,10,50wkky,We had much fun doing this!,https://www.reddit.com/r/MachineLearning/comments/50wkky/we_had_much_fun_doing_this/,pilooch,1472865050,,1,9
97,2016-9-3,2016,9,3,10,50wkw5,Conway's Game of Life is proof of how complexity can emerge from simple rules. This can apply to human intelligence,https://www.reddit.com/r/MachineLearning/comments/50wkw5/conways_game_of_life_is_proof_of_how_complexity/,llSourcell,1472865175,,0,0
98,2016-9-3,2016,9,3,13,50xeco,Why doesn't Karparthy uses character-level RNN in Neuraltalk?,https://www.reddit.com/r/MachineLearning/comments/50xeco/why_doesnt_karparthy_uses_characterlevel_rnn_in/,eefic,1472878794,"Has anyone tried using character-level RNN? How would it differ from the word-level model, in terms of training, testing, etc?",4,0
99,2016-9-3,2016,9,3,14,50xk93,Learning Path for Deep learning in Python,https://www.reddit.com/r/MachineLearning/comments/50xk93/learning_path_for_deep_learning_in_python/,jalFaizy,1472881897,Just wanted to share [my first blog in Data Science](https://www.analyticsvidhya.com/blog/2016/08/deep-learning-path/) domain. Opinions are welcome!,0,0
100,2016-9-3,2016,9,3,16,50xtsy,Mega Machines World Top Compilation: Human Made,https://www.reddit.com/r/MachineLearning/comments/50xtsy/mega_machines_world_top_compilation_human_made/,aptoccar,1472887548,,0,1
101,2016-9-3,2016,9,3,17,50xyp6,Anyone has any idea about how Prisma made image processing offline?,https://www.reddit.com/r/MachineLearning/comments/50xyp6/anyone_has_any_idea_about_how_prisma_made_image/,tedli,1472890775,"As far as I know, neural style transfer are highly dependent on GPU support for fast processing and the pre-trained model alone will cost hundreds MB of disk space. Then how did Prisma team make it possible to process an image in seconds offline on mobile phone while keep the App only about 60 MB on disk? That sounds crazy for me. ",17,13
102,2016-9-3,2016,9,3,19,50y9i6,Are there any symptoms-diseases datasets available?,https://www.reddit.com/r/MachineLearning/comments/50y9i6/are_there_any_symptomsdiseases_datasets_available/,satwik_,1472898136,,12,43
103,2016-9-3,2016,9,3,20,50ydb7,Interesting Hyperparameter pattern,https://www.reddit.com/r/MachineLearning/comments/50ydb7/interesting_hyperparameter_pattern/,[deleted],1472900584,[deleted],1,1
104,2016-9-3,2016,9,3,20,50ygud,A Practical Guide for Debugging Tensorflow,https://www.reddit.com/r/MachineLearning/comments/50ygud/a_practical_guide_for_debugging_tensorflow/,pannous,1472902838,,1,1
105,2016-9-3,2016,9,3,21,50ylpt,"MXnet and Chainer, which one should I choose?",https://www.reddit.com/r/MachineLearning/comments/50ylpt/mxnet_and_chainer_which_one_should_i_choose/,tfblog_jp,1472905778,,7,1
106,2016-9-3,2016,9,3,21,50yp2b,[Question] Model Construction out of boosted classifiers?,https://www.reddit.com/r/MachineLearning/comments/50yp2b/question_model_construction_out_of_boosted/,PlayMeWhile,1472907560,"Today I got a task that is unfamiliar to me. Namely to repeat analysis that was done by another person on a new data. I have a draft of what he was doing: [Draft](http://158.129.140.171/~karolis/WhatIsThis.png)

I don't seem to understand what technique was he using for that formula

    Y = Base Rate x Value x Predictor Relative x Predictor Relative.

Maybe anyone familiar with this can help me make sense of it? Maybe it's something simple? Any hint, link to literature or a guess is appreciated.

EDIT:

As far as I can figure out the data from those tables should be used in the formula somehow to obtain the predicted values.",19,1
107,2016-9-3,2016,9,3,22,50yvqy,Would anyone be interested in a Forum Board/Slack Group/Skype group for strictly ML driven ventures?,https://www.reddit.com/r/MachineLearning/comments/50yvqy/would_anyone_be_interested_in_a_forum_boardslack/,[deleted],1472910655,[deleted],15,16
108,2016-9-3,2016,9,3,23,50z29g,How does mobile keyboard prediction work?,https://www.reddit.com/r/MachineLearning/comments/50z29g/how_does_mobile_keyboard_prediction_work/,[deleted],1472913367,[removed],2,3
109,2016-9-4,2016,9,4,0,50z5y7,Codes for two CVPR super-resolution oral papers available!,https://www.reddit.com/r/MachineLearning/comments/50z5y7/codes_for_two_cvpr_superresolution_oral_papers/,kjw0612,1472914834,,3,13
110,2016-9-4,2016,9,4,0,50zd6l,Help with LaTeX equation writing program,https://www.reddit.com/r/MachineLearning/comments/50zd6l/help_with_latex_equation_writing_program/,WarmMage,1472917650,[removed],0,1
111,2016-9-4,2016,9,4,1,50zl4d,The Design and Implementation of Probabilistic Programming Languages,https://www.reddit.com/r/MachineLearning/comments/50zl4d/the_design_and_implementation_of_probabilistic/,adamnemecek,1472920830,,8,18
112,2016-9-4,2016,9,4,2,50zolk,[Research Discussion] Stacked Approximated Regression Machine,https://www.reddit.com/r/MachineLearning/comments/50zolk/research_discussion_stacked_approximated/,rantana,1472922163,"Since the last thread /u/r-sync [posted](https://www.reddit.com/r/MachineLearning/comments/50tbjp/stacked_approximated_regression_machine_a_simple/) became more of a conversation about this subreddit and NIPS reviewer quality, I thought I would make a new thread to discuss the **research aspects** on this paper:

Stacked Approximated Regression Machine: A Simple Deep Learning Approach

http://arxiv.org/abs/1608.04062



* The claim is they get VGGnet quality with significantly less training data AND significantly less training time. It's unclear to me how much of the ImageNet data they actually use, but it seems to be significantly smaller than other deep learning models trained. Relevant Quote:

&gt; Interestingly, we observe that each ARMs parameters could be reliably obtained, using a tiny portion of the training data. In our experiments, instead of running through the entire training set, we draw anvsmall i.i.d. subset (as low as 0.5% of the training set), to solve the parameters for each ARM.

I'm assuming that's where /u/r-sync inferred the part about training only using about 10% of imagenet-12. But it's not clear to me if this is an upper bound. It would be nice to have some pseudo-code in this paper to clarify how much labeled data they're actually using.

* It seems like they're using a layer wise 'KSVD algorithm' for training in a layerwise manner. I'm not familiar with KSVD, but this seems **completely** different from training a system end-to-end with backprop. If these results are verified, this would be a very big deal, as backprop has been gospel for neural networks for a long time now.

* Sparse coding seems to be the key to this approach. It seems to be very similar to the layer-wise sparse learning approaches developed by A. Ng, Y. LeCun, B. Olshausen before AlexNet took over.
",64,90
113,2016-9-4,2016,9,4,2,50zrcp,Looking for a Laptop for Use With Deep Learning and Computer Vision,https://www.reddit.com/r/MachineLearning/comments/50zrcp/looking_for_a_laptop_for_use_with_deep_learning/,dakatapetrov,1472923159,"Hi everyone!

I'm graduate student in the field of AI. And I'll be an exchange student in the upcoming year, so I'd like to buy a new laptop. It will be mainly used for my master thesis which will be on deep learning and computer vision (not sure exactly what yet). I'll probably do some [kaggle](https://www.kaggle.com/) competitions and contributions to some open source projects (including non-AI) as well. Apart from that I do some gaming from time to time and plan to buy Oculus as I have some discounts for it.

I'm looking for the following in a laptop:

 * performance
 * build quality
 * battery life
 * reasonable price

The [Lenovo P50](http://shop.lenovo.com/us/en/laptops/thinkpad/p-series/p50/) seems like a good option. However, I'm not up to date with the new hardware trends. I'd love to hear your opinion about the laptop and the following questions: How does the Xeon processor perform on a laptop? Should I get it or stick with an i7? Do I need an ECC memory?

I'm currently on an internship in the US and I have some really nice discounts for Lenovo. The P50 with 4K display, Xeon, 64 GB RAM, 512 GB SSD is around $2k.

So what do you guys think about the laptop and specs? Suggestions for other laptops than Lenovo are welcomed if the price is reasonable (up to $3k).
",25,10
114,2016-9-4,2016,9,4,2,50zuef,ML/AI Professors currently research active and open to taking on new masters/phd students,https://www.reddit.com/r/MachineLearning/comments/50zuef/mlai_professors_currently_research_active_and/,wardroton,1472924293,"Hi,

I would really appreciate if someone can tell me which professors are currently active in research and might be open to taking on new masters/phd students. 

Thanks",1,0
115,2016-9-4,2016,9,4,2,50zvbq,Why It Pays to Understand Shock and Vibration,https://www.reddit.com/r/MachineLearning/comments/50zvbq/why_it_pays_to_understand_shock_and_vibration/,Dynemech,1472924618,,0,1
116,2016-9-4,2016,9,4,2,50zw0t,What are some of the current best practices for detecting categorical outliers/anomalies?,https://www.reddit.com/r/MachineLearning/comments/50zw0t/what_are_some_of_the_current_best_practices_for/,foxh8er,1472924881,,2,2
117,2016-9-4,2016,9,4,3,51072z,SVM - focusing on one class over the others?,https://www.reddit.com/r/MachineLearning/comments/51072z/svm_focusing_on_one_class_over_the_others/,[deleted],1472928768,[deleted],6,1
118,2016-9-4,2016,9,4,3,5107sy,Machine Learning Dissertation help.,https://www.reddit.com/r/MachineLearning/comments/5107sy/machine_learning_dissertation_help/,pas43,1472929012,"So I'm going into the third year of Uni and have got the idea from a Github project I found about a guy who used machine learning to work out what parts of his lifestyle and foods were making him put on weight. I found this a cool idea and would like to redo this idea but with more data which is where I got a bit confused.

I am willing to log everything I eat at what time, weight myself every morning twice a day with time with very accurate scales, take blood sugar levels, take blood cholesterol tests and even pay for private blood tests once every couple of weeks to find out my biology (I'm 5'0ft and weight 240lbs, I'm not the typical person by any means, also diabetic type 2, since 13). I will take things like blood pressure. I will also wear a FitBit/Band type device that records all my bio-data such as heart BPM/body temperature/sleep tracking and also GPS to determine how fast I'm walking cycling. Also the Gym I go to has all the equipment log my activities - repetitions, the weight you are lifting and also how far you went, this ranges from the cable machine to the treadmills and cycles.

How with all this data in mind and I will be recording it over the period of 3/4/5 months. What would be the best way to find out a healthier lifestyle for me? what makes em loose weight? what makes my testosterone rise, what make oestrogen rise? what the best food for weight loss? what the best food for muscle growth? what foods make me want to eat less overall (IE fill me up more). What foods make me sleep poorly? what foods make me sleep well? Whats foods keep my blood sugar level above 4 but below 8? 

What's the best way to start crunching these numbers? Once all the data is processed ideally i would like to put it into Kibana to get nice pretty graphs for my presentation but, that another thread for another time. What type of problem is this? Classification? how would you guys go about doing this? I'm not sure where to start.
Thanks.",6,3
119,2016-9-4,2016,9,4,4,510f4r,"My friend and I made a little jingle about DeepLearning and the dangers of AI, check it out",https://www.reddit.com/r/MachineLearning/comments/510f4r/my_friend_and_i_made_a_little_jingle_about/,pcannons,1472931664,,1,0
120,2016-9-4,2016,9,4,5,510pnn,Tips on separation of signal and backgrounf,https://www.reddit.com/r/MachineLearning/comments/510pnn/tips_on_separation_of_signal_and_backgrounf/,piter2995,1472935532,[removed],0,1
121,2016-9-4,2016,9,4,6,510wml,1st and 2nd Moment Hyper-parameter Tuning for Adam Optimizer,https://www.reddit.com/r/MachineLearning/comments/510wml/1st_and_2nd_moment_hyperparameter_tuning_for_adam/,danielcanadia,1472938230,"I'm curious on the mathematical level, how could I prevent the gradients from becoming prematurely too small with the Adam Optimizer? I have a model that seems to mostly converge with about 1% of the training data I have, then converge at an ***extremely*** slow pace for the rest of the training data.

Since the Adam Optimizer uses adaptive learning rates, I'm hypothesizing that those values are dropping too fast, and the slow pace learning that happens is due to the bias terms on the Adam Optimizer (it helps learn when gradient become sparse according to paper). I think this can be prevented by decreasing the 1 &amp; 2 moment decay hyper-parameters.

Is that correct? I'm not too sure in my mathematics when it comes to optimization algorithms.
https://arxiv.org/pdf/1412.6980v8.pdf",1,6
122,2016-9-4,2016,9,4,6,511059,Learning to learn and compositionality with deep recurrent neural networks,https://www.reddit.com/r/MachineLearning/comments/511059/learning_to_learn_and_compositionality_with_deep/,[deleted],1472939585,[deleted],0,1
123,2016-9-4,2016,9,4,6,51109j,Nando de Freitas - Learning to learn and compositionality with deep recurrent neural networks,https://www.reddit.com/r/MachineLearning/comments/51109j/nando_de_freitas_learning_to_learn_and/,evc123,1472939641,,6,87
124,2016-9-4,2016,9,4,7,5111yc,[1511.04868] A Neural Transducer,https://www.reddit.com/r/MachineLearning/comments/5111yc/151104868_a_neural_transducer/,cesarsalgado,1472940319,,4,41
125,2016-9-4,2016,9,4,7,5117bc,Math/Stats courses as a prereq for Bishop's PR&amp;ML?,https://www.reddit.com/r/MachineLearning/comments/5117bc/mathstats_courses_as_a_prereq_for_bishops_prml/,IMHERETOCODE,1472942446,"Does anyone know of an online course to get up to speed with the math and stats used in Bishop's Pattern Recognition and Machine Learning? 

Through coursework I've done through Calc 2, Discrete, Linear, and Applied Stats.

I've started on Berkley's Introduction to Statistics: Probability 2.x on edX, but see MIT has a full course (MITx -  6.041x) on intro to probabilities, is that a better one? Just kind of need a starting point as to what courses actually cover the material that is used in this textbook. 

Sorry for any grammar or punctuation issues, as I'm on mobile. 

Thanks!",4,2
126,2016-9-4,2016,9,4,8,511gx8,Experiments on Optimal Brain Damage using Tensorflow,https://www.reddit.com/r/MachineLearning/comments/511gx8/experiments_on_optimal_brain_damage_using/,charlie0_o,1472946422,"Hi all,

I did some hacked up version on pruning specifically optimal brain damage in tensorflow and wanted to share results and get some feedback - hoping to get some advice on the observations and any gaps I might have in my understanding.

My notes can be found here: [link](https://github.com/shekkizh/neuralnetworks.thought-experiments/tree/master/Model%20Pruning)",2,16
127,2016-9-4,2016,9,4,11,5126ep,Apriori Algorithym: What language? plus other questions,https://www.reddit.com/r/MachineLearning/comments/5126ep/apriori_algorithym_what_language_plus_other/,FesRealitor,1472957497,"Ok.  I am learning programming and machine learning in my part time.  I was able to copy a apriori script from a book about python and apply it to my dataset.  

Success! my boss is going to let me do a side project using it and then use it to pitch machine learning in general/advanced stats.

Now, I need to operationalize the apriori and frequent item datasets scripts.  So, im looking for advice on the best language/program to do so.

my end goal is dump the results into a tableau dashboard i created.

i dont care if i dont know the language. i tend to pick them up quick and we dont have a timeline. its a really good learning expreirence for me in general.  so , help? whats the best way i can operationalize the apriori and freq item sets algotihyms and have it dump into a file that i can upload into my dashboards?
",5,2
128,2016-9-4,2016,9,4,12,5127jh,"For any continuous dimension reducing map, there must be points that are far apart in the original space and yet map to exactly the same point in the low-dim space",https://www.reddit.com/r/MachineLearning/comments/5127jh/for_any_continuous_dimension_reducing_map_there/,urish,1472958033,,13,28
129,2016-9-4,2016,9,4,12,512929,Customizing an established machine learning service,https://www.reddit.com/r/MachineLearning/comments/512929/customizing_an_established_machine_learning/,jkk47,1472958719,[removed],0,1
130,2016-9-4,2016,9,4,14,512rcb,A service to get large datasets annotated quickly. Any interest ?,https://www.reddit.com/r/MachineLearning/comments/512rcb/a_service_to_get_large_datasets_annotated_quickly/,datamunchers,1472967961,"We have all had to scrap cool ideas because we just don't have the time to build a large annotated dataset. 
Maybe this is irrelevant for big cos like google, amazon, etc.
But I have made prototypes for startups and individuals which couldn't go beyond the prototype phase because of this issue. 

One solution which comes up is Mechanical Turk but it -

* is US only

* needs a huge effort to setup if your task is more complicated than the templates they provide

If someone were to give you a quick and easy way to get a large dataset annotated, would that be of interest ? 

Also, what interesting solutions have you used for this ?",10,5
131,2016-9-4,2016,9,4,16,512ysq,Practical XGBoost in Python - new 2016 free online course,https://www.reddit.com/r/MachineLearning/comments/512ysq/practical_xgboost_in_python_new_2016_free_online/,khozzy,1472972541,,8,19
132,2016-9-4,2016,9,4,18,513e10,Are deep networks just as difficult to understand as life itself?,https://www.reddit.com/r/MachineLearning/comments/513e10/are_deep_networks_just_as_difficult_to_understand/,cocoapriest,1472983025,,4,0
133,2016-9-4,2016,9,4,18,513e4n,Random Forest,https://www.reddit.com/r/MachineLearning/comments/513e4n/random_forest/,khurrampc,1472983100,[removed],0,1
134,2016-9-4,2016,9,4,21,513q1z,LiFT: compiles formula of neural network to OpenCL code,https://www.reddit.com/r/MachineLearning/comments/513q1z/lift_compiles_formula_of_neural_network_to_opencl/,bhuztez,1472990933,,8,61
135,2016-9-4,2016,9,4,22,5142ks,"Wood Splitter - 100 Ways to Split Firewood. Best Cleaver, Saw and Log Splitter Compilation",https://www.reddit.com/r/MachineLearning/comments/5142ks/wood_splitter_100_ways_to_split_firewood_best/,aptoccar,1472997445,,0,1
136,2016-9-5,2016,9,5,2,514u5c,I'm interested in finding trends in bank account data and using it to forecast future transactions. Assume that the transactions are primarily direct deposit and bill pay in nature. What machine learning algorithm would apply to this kind of problem?,https://www.reddit.com/r/MachineLearning/comments/514u5c/im_interested_in_finding_trends_in_bank_account/,JeffreyVest,1473008408,So say for instance there are months of transactional data. It becomes apparent that certain things tend to get paid at certain times. I would want to then forecast say a years worth of data based on the learned transaction patterns. It also might be possible that the text of the transaction varies somewhat. So I would want to recognize that some transactions are so close to each other in name that they are actually the same. ,9,2
137,2016-9-5,2016,9,5,3,515dus,KDD Panel: Is Deep Learning the New 42?,https://www.reddit.com/r/MachineLearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/,thvasilo,1473015445,,16,13
138,2016-9-5,2016,9,5,4,515ljr,ICML 2016 Orals,https://www.reddit.com/r/MachineLearning/comments/515ljr/icml_2016_orals/,siddharth-agrawal,1473018098,The ICML 2016 Oral videos are now available: http://techtalks.tv/icml/2016/orals/. I randomly checked around 10 videos on different pages.,7,42
139,2016-9-5,2016,9,5,8,516pb2,Deep Neural Networks for YouTube Recommendations (Google Paper),https://www.reddit.com/r/MachineLearning/comments/516pb2/deep_neural_networks_for_youtube_recommendations/,n_mca,1473033123,,5,30
140,2016-9-5,2016,9,5,9,516y0s,Prediction from Mixture of Experts,https://www.reddit.com/r/MachineLearning/comments/516y0s/prediction_from_mixture_of_experts/,MidnightBlueCavalier,1473036808,"I have a mixture of experts model (MEM) where the experts are linear regression equations. I used flexmix in R to fit them, but I would like to understand how to predict which expert is most appropriate for NEW observations. Is generalization possible from a MEM?",6,0
141,2016-9-5,2016,9,5,10,516z76,Machine Learning and R; Psychology,https://www.reddit.com/r/MachineLearning/comments/516z76/machine_learning_and_r_psychology/,poivrer,1473037301,"I graduated with a Psychology undergrad degree.

Considered studying Quantitative Psychology cause I also have a minor in Statistics and like dealing with data.

Somebody recently told me to master Machine Learning using R and that I could easily make bank that route.

Anybody have any input on this information?",12,0
142,2016-9-5,2016,9,5,10,516zyl,Introduction to Machine Learning Theory - Part 1,https://www.reddit.com/r/MachineLearning/comments/516zyl/introduction_to_machine_learning_theory_part_1/,mostafa-samir,1473037606,,0,14
143,2016-9-5,2016,9,5,10,517037,CSCC11: Introduction to Machine Learning and Data Mining (University of Toronto Lecture Notes),https://www.reddit.com/r/MachineLearning/comments/517037/cscc11_introduction_to_machine_learning_and_data/,Dogsindahouse1,1473037663,,1,97
144,2016-9-5,2016,9,5,10,5173yt,Deepsee: Automated theft detection with deeplearning,https://www.reddit.com/r/MachineLearning/comments/5173yt/deepsee_automated_theft_detection_with/,Lajamerr_Mittesdine,1473039312,,35,61
145,2016-9-5,2016,9,5,11,517bmr,What are some good productivity tips for running machine learning experiments?,https://www.reddit.com/r/MachineLearning/comments/517bmr/what_are_some_good_productivity_tips_for_running/,[deleted],1473042643,[removed],0,1
146,2016-9-5,2016,9,5,12,517fvq,Image Preprocessing on Resnet (related to TF Slim),https://www.reddit.com/r/MachineLearning/comments/517fvq/image_preprocessing_on_resnet_related_to_tf_slim/,[deleted],1473044451,[deleted],0,0
147,2016-9-5,2016,9,5,13,517r2z,[1609.00116] Neural Coarse-Graining: Extracting slowly-varying latent degrees of freedom with neural networks,https://www.reddit.com/r/MachineLearning/comments/517r2z/160900116_neural_coarsegraining_extracting/,NichG,1473049420,,4,21
148,2016-9-5,2016,9,5,13,517rly,Currently doing to the Stanford Machine Learning course on Coursera. Any free course suggestions for after?,https://www.reddit.com/r/MachineLearning/comments/517rly/currently_doing_to_the_stanford_machine_learning/,pie_oh_my_,1473049667,[removed],5,0
149,2016-9-5,2016,9,5,17,518k1w,Capturing Deep Correlations with 2-Way Nets,https://www.reddit.com/r/MachineLearning/comments/518k1w/capturing_deep_correlations_with_2way_nets/,aviveise,1473065533,"[Arxiv link](http://arxiv.org/abs/1608.07973)

We present a noval, bi-directional mapping neural network architecture for the task of matching vectors from two data-sources. Our approach employs two tied neural network channels to project two views into a common, maximally correlated space, using the euclidean loss. To achieve both maximally correlated projections we built an encoder-decoder framework composed of two parallel networks each captures the features of each of the views. We show a direct link between the correlation loss and euclidean loss enabling the use of euclidean loss for optimizing correlation maximization problem. To overcome common euclidean regression optimization problems, we incorporated batch-normalization layers and dropout layers adapted to the model at hand. We show state of the art results on a number of computer vision matching tasks including MNIST image matching and sentence-image matching on the flickr8k and flickr30k datasets.

Any comments or questions will be appreciated!",1,1
150,2016-9-5,2016,9,5,18,518l4u,"Neural network: predicting a continuous but non-normal output, and obtaining its posterior distribution",https://www.reddit.com/r/MachineLearning/comments/518l4u/neural_network_predicting_a_continuous_but/,timcar,1473066242,"I would like the output layer of my neural network to output the posterior distribution of y conditional on x, where y cannot be assumed to be normally distributed conditional on x (but could instead be a mixture of normals, or have a mass point somewhere). I not only care about having good point estimates for y, but also need to be able to infer confidence bounds.

I was thinking of bucketing y (into 50 or so buckets; y has a lower and an upper bound, which helps) and using a softmax activation. This gives me a nice probability distribution for 'free', but it feels inefficient. For example the standard loss function (categorical cross entropy) will fail to take into account the fact that predicting 'bucket 10' when the truth is 'bucket 9' is not as bad as predicting 'bucket 10' when the truth is 'bucket 3'.

Am I missing a simple, standard way of handling this problem?",26,23
151,2016-9-5,2016,9,5,19,518vft,Want to use machine learning and also interpret model parameters but you don't have enough data? Just published a paper: An Efficient Data Partitioning to Improve Classification Performance While Keeping Parameters Interpretable,https://www.reddit.com/r/MachineLearning/comments/518vft/want_to_use_machine_learning_and_also_interpret/,MLpaper05092016,1473072444,,1,3
152,2016-9-5,2016,9,5,19,518vh6,The Evolution of NLP: Natural Language Understanding,https://www.reddit.com/r/MachineLearning/comments/518vh6/the_evolution_of_nlp_natural_language/,reworksophie,1473072465,,0,1
153,2016-9-5,2016,9,5,19,518vsz,Seamless Carbon Steel Pipes and fittings are made up of metal alloys which are results of carbon and iron blend,https://www.reddit.com/r/MachineLearning/comments/518vsz/seamless_carbon_steel_pipes_and_fittings_are_made/,italartworld001,1473072664,,0,1
154,2016-9-5,2016,9,5,19,518wg0,Visual collection of deep learning articles and code snippets,https://www.reddit.com/r/MachineLearning/comments/518wg0/visual_collection_of_deep_learning_articles_and/,MLpaper05092016,1473073048,,0,4
155,2016-9-5,2016,9,5,20,518yaa,What is rtv sealant package in hard tube?,https://www.reddit.com/r/MachineLearning/comments/518yaa/what_is_rtv_sealant_package_in_hard_tube/,mixmachinery,1473074121,,1,1
156,2016-9-5,2016,9,5,20,519150,linear LibSVM bias term calculation,https://www.reddit.com/r/MachineLearning/comments/519150/linear_libsvm_bias_term_calculation/,rocko95,1473075691,"Hi all, I've been working on a university assignment on binary class linear svms, and I'm a bit stuck on how bias value is calculated. I was under the impression that bias was the average of y(i)-x(i).w for all i such that x(i) is a support vector. [see here](http://research.microsoft.com/en-us/um/people/cburges/papers/svmtutorial.pdf) and [here](https://en.wikipedia.org/wiki/Support_vector_machine#Computing_the_SVM_classifier).

I have implemented SVM's in matlab using CVX, and I am getting excellent results for my primal form, however my dual form is giving me problems calculating bias. I realised that there were rounding errors that made it nearly impossible to find bias from support vectors, as finding the support vectors has proven to be extremely difficult. My w is calculated from my lagrangian multipliers a(i) for i = 1, .., n, and is correct to about 4 sig figs compared to the LibSVM, so I am confident that it's correct, however my a variables are all non-zero and non-C. Many are within +-1e-9, but I can't work out where to draw the line for these, so I tried just using sv_indices out of libsvm.  
When I tried to reproduce the bias value of LibSVM using the data straight out of the model, I got a different result to bias = -model.rho
My thought would be that if I took indices=model.sv_indices and used a loop over those to sum up y(indices(i))-x(indices(i)).w then divided by model.nSV(1), I would have the bias value, however I am still getting a different output to the bias given by libsvms -model.rho.

Does anyone know how LibSVM calculates the bias value? Their [paper](http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf) details the functions they use to calculate w for both primal and dual, but does not seem to detail how they recover b in the dual.  
PS: I am using -s 0 -t 0 -h 0 as flags for svmtrain.",1,3
157,2016-9-5,2016,9,5,20,51916f,Start Udacity (google's) or Coursera (stanford's) course first?,https://www.reddit.com/r/MachineLearning/comments/51916f/start_udacity_googles_or_coursera_stanfords/,baaaze,1473075711,,15,44
158,2016-9-5,2016,9,5,21,51939n,When splitting training data to train/validation data....,https://www.reddit.com/r/MachineLearning/comments/51939n/when_splitting_training_data_to_trainvalidation/,DenormalHuman,1473076815,"Can there be any value to duplicating hte training set and setting the fraction of data for validation to .5, so validation is essentially occurring against the full original training set?

In situation where say, there are interesting features across the whole training set but when split some of those features may not be visible in the validation set?",4,1
159,2016-9-5,2016,9,5,21,5198ns,Seamless Carbon Steel Pipes and fittings are made up of metal alloys which are results of carbon and iron blend.,https://www.reddit.com/r/MachineLearning/comments/5198ns/seamless_carbon_steel_pipes_and_fittings_are_made/,italartworld001,1473079360,,0,1
160,2016-9-5,2016,9,5,22,519cht,GD-based optimization algorithms - overview w/ formulae and technical details,https://www.reddit.com/r/MachineLearning/comments/519cht/gdbased_optimization_algorithms_overview_w/,clrokr,1473081141,,1,9
161,2016-9-6,2016,9,6,0,519uq5,Finding the best distributional analysis approach for word/documents embeddings,https://www.reddit.com/r/MachineLearning/comments/519uq5/finding_the_best_distributional_analysis_approach/,mentatf,1473088533,"As a newcomer to ML applied to NLP, I can't tell clearly what kind of embeddings I should use. I've mostly heard of latent semantic analysis, latent dirichlet allocation, and word2vec-like embeddings. Word2vec-like embeddings have been especially hyped lately, because it's simple and scalable.

What remains especially unclear to me is how scalable (or how *not* scalable) the other methods are, how efficient they are on data, and for what task. Is there a flowchart or some benchmarks on datasets of different sizes and different tasks for different embeddings (when it is tractable) ?

edit: [link to the xpost in /r/languagetechnology](https://www.reddit.com/r/LanguageTechnology/comments/51ad4z/finding_the_best_distributional_analysis_approach/) since there were more comments there.",1,7
162,2016-9-6,2016,9,6,0,519z41,When would I say that my classifier performance improvement is statistically significant?,https://www.reddit.com/r/MachineLearning/comments/519z41/when_would_i_say_that_my_classifier_performance/,blankexperiment,1473090144,"I assume to report mean improvement after several rounds of cross validation.
Few factors which I can think of which effect statistical significance [[wiki](https://en.wikipedia.org/wiki/Statistical_significance)],

* dataset size

* improvement to base accuracy ratio (e.g., 0.5% improvement over 20% maybe less significant when compared to 99% base accuracy)

Thanks in advance!

[Edit] What about the significance of the improvement in case of a fixed test set?",6,13
163,2016-9-6,2016,9,6,1,51aapy,"If already know the basics of ML and have MS, which bootcamp would be most helpful?",https://www.reddit.com/r/MachineLearning/comments/51aapy/if_already_know_the_basics_of_ml_and_have_ms/,[deleted],1473094112,[deleted],8,0
164,2016-9-6,2016,9,6,2,51aga8,Data Science Competitions 101: Anatomy and Approach,https://www.reddit.com/r/MachineLearning/comments/51aga8/data_science_competitions_101_anatomy_and_approach/,analyticalmonk,1473095911,,0,1
165,2016-9-6,2016,9,6,3,51ar9u,Composing inference algorithms as program transformations,https://www.reddit.com/r/MachineLearning/comments/51ar9u/composing_inference_algorithms_as_program/,adamnemecek,1473099621,,0,10
166,2016-9-6,2016,9,6,4,51b2n5,Is this Wyoming city center webcam a giant ML experiment?,https://www.reddit.com/r/MachineLearning/comments/51b2n5/is_this_wyoming_city_center_webcam_a_giant_ml/,tayrobin,1473103504,,27,109
167,2016-9-6,2016,9,6,4,51b3zi,Serverless API around Google Cloud Vision with the Serverless Framework,https://www.reddit.com/r/MachineLearning/comments/51b3zi/serverless_api_around_google_cloud_vision_with/,ramhiser,1473103950,,1,12
168,2016-9-6,2016,9,6,4,51b4ll,Learning R Programming,https://www.reddit.com/r/MachineLearning/comments/51b4ll/learning_r_programming/,rakesajar,1473104141,Can someone suggest good resources to learn R programming for machine learning,6,0
169,2016-9-6,2016,9,6,5,51bf3f,Single photon in hierarchical architecture for physical reinforcement learning: Photon intelligence,https://www.reddit.com/r/MachineLearning/comments/51bf3f/single_photon_in_hierarchical_architecture_for/,adamnemecek,1473107639,,0,3
170,2016-9-6,2016,9,6,5,51bipv,I've been playing around with using a convnet to detect BPM in music - would love to hear any thoughts!,https://www.reddit.com/r/MachineLearning/comments/51bipv/ive_been_playing_around_with_using_a_convnet_to/,lfotofilter,1473108802,,11,22
171,2016-9-6,2016,9,6,6,51bnsy,Urban Sound Classification using Neural Network,https://www.reddit.com/r/MachineLearning/comments/51bnsy/urban_sound_classification_using_neural_network/,a_endurance,1473110481,,3,21
172,2016-9-6,2016,9,6,6,51bqk0,Keraflow: Alternative to Keras,https://www.reddit.com/r/MachineLearning/comments/51bqk0/keraflow_alternative_to_keras/,ipod825,1473111410,"I've been using Keras for a while and felt that some of its code architecture is not well designed.
Therefore, I decided to implement a similar framework myself.
And here it is, Keraflow:
https://github.com/ipod825/keraflow
It is very similar to Keras, but you will find that it's easier to customize (e.g., write your own layers).
Also, I believe the code of Keraflow is much easier to understand than that of Keras because Kerflow build network in forward pass instead of backward pass.

Anyway, feel free to try it. Any comment is welcome.

P.S. The reason that I don't try to issue pull requests is that I modify too many things (the core is totally different). It would cost much more time to get all things done.",3,8
173,2016-9-6,2016,9,6,7,51c4i3,US unemployment prediction from job-seeking websites traffic,https://www.reddit.com/r/MachineLearning/comments/51c4i3/us_unemployment_prediction_from_jobseeking/,[deleted],1473116323,[deleted],0,0
174,2016-9-6,2016,9,6,9,51cksn,[1609.01000] Convexified Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/51cksn/160901000_convexified_convolutional_neural/,zhongwenxu,1473122476,,17,20
175,2016-9-6,2016,9,6,14,51dp4y,14 Most popular August 2016 data science articles to read,https://www.reddit.com/r/MachineLearning/comments/51dp4y/14_most_popular_august_2016_data_science_articles/,dataaspirant,1473139338,,0,1
176,2016-9-6,2016,9,6,15,51dwxv,"Mathematical foundation for Noise, Bias and Variance in #NeuralNetworks",https://www.reddit.com/r/MachineLearning/comments/51dwxv/mathematical_foundation_for_noise_bias_and/,vvpreetham,1473143652,,0,2
177,2016-9-6,2016,9,6,15,51dy37,How about high speed dissolver for coating,https://www.reddit.com/r/MachineLearning/comments/51dy37/how_about_high_speed_dissolver_for_coating/,mixmachinery,1473144328,,1,1
178,2016-9-6,2016,9,6,16,51e1p9,How about industrial tank mixers polishing process,https://www.reddit.com/r/MachineLearning/comments/51e1p9/how_about_industrial_tank_mixers_polishing_process/,mixmachinery,1473146340,,1,1
179,2016-9-6,2016,9,6,16,51e3xu,Defeating Image Obfuscation with Deep Learning,https://www.reddit.com/r/MachineLearning/comments/51e3xu/defeating_image_obfuscation_with_deep_learning/,alexeyr,1473147686,,0,6
180,2016-9-6,2016,9,6,17,51e9g8,How do I present the cause of a prediction by a machine learning classifier?,https://www.reddit.com/r/MachineLearning/comments/51e9g8/how_do_i_present_the_cause_of_a_prediction_by_a/,sid753951,1473151090,"I'm currently designing a machine learning classifier for work. My problem isn't just prediction/classification. I also need to explain why a prediction occurs. My algorithm uses an LSTM in the first stage followed by a softmax classifier or 2-layer neural net for the classification. To explain why a data instance is classified as ""YES"" or ""NO"", my idea was to find the nearest neighbors of the instance in a population of all training instances. The vector representation of each training instance (for the population of the nearest neighbor space) is taken from the LSTM output. Does anyone know if some alternative exists? Because this method isn't giving great results. On an average, only 50% of the neighbors of the true positives are also actual positives.",7,0
181,2016-9-6,2016,9,6,18,51eg8l,Source code classification using deep learning,https://www.reddit.com/r/MachineLearning/comments/51eg8l/source_code_classification_using_deep_learning/,MikeWally,1473155341,,20,49
182,2016-9-6,2016,9,6,19,51eibq,Which deep neural network is most suitable for classifying Magnetic resonance Spectroscopy data?,https://www.reddit.com/r/MachineLearning/comments/51eibq/which_deep_neural_network_is_most_suitable_for/,zerogravity555,1473156611,"I have been learning about CNNs and RNNs and want to implement it for timeseries classification.
I was going through this post 
http://monik.in/a-noobs-guide-to-implementing-rnn-lstm-using-tensorflow/
have understood how to determine the 1 counts from a binary string. 

Now let's assume I break a timeseries into two parts and have corresponding labels for class 0 and class 1. Is it a good idea to use a RNN for this task?",6,6
183,2016-9-6,2016,9,6,19,51eit6,"Hold the pride to enter into world's of #BigdataAnalytics or #MachineLearning with #datascience #R, #Mahout, #Statistics and #Probability",https://www.reddit.com/r/MachineLearning/comments/51eit6/hold_the_pride_to_enter_into_worlds_of/,jimmy_andersonsal,1473156901,,0,1
184,2016-9-6,2016,9,6,19,51emj6,Human Pose Estimation in Space and Time using 3D CNN,https://www.reddit.com/r/MachineLearning/comments/51emj6/human_pose_estimation_in_space_and_time_using_3d/,nomaderx,1473159149,"[Second author here] Here is a simple approach of using 3D CNN to encode spatiotemporal information for human pose estimation in video. 
This is based on a recent master thesis, and I just thought maybe some people will find it interesting/useful. 

arXiV Link: http://arxiv.org/pdf/1609.00036v1.pdf

We submitted it to an ECCV Workshop, which uses open review. So, if anyone has an Open Review account and is interesting, please feel free to add a review or a comment via the link below.

Open Review link: http://beta.openreview.net/forum?id=Hk8FG6Vi

Cheers!
",4,9
185,2016-9-6,2016,9,6,19,51emjs,[1609.00150] Reward Augmented Maximum Likelihood for Neural Structured Prediction,https://www.reddit.com/r/MachineLearning/comments/51emjs/160900150_reward_augmented_maximum_likelihood_for/,cesarsalgado,1473159157,,7,18
186,2016-9-6,2016,9,6,20,51etjs,Do any deep learning frameworks support INT8 ?,https://www.reddit.com/r/MachineLearning/comments/51etjs/do_any_deep_learning_frameworks_support_int8/,jocomoco,1473162883,"According to Google Search : no. 
Am I missing something ?

Which will be the first that will ?",8,1
187,2016-9-6,2016,9,6,23,51fegp,Establishing a Machine Learning Workflow  The DownLinQ,https://www.reddit.com/r/MachineLearning/comments/51fegp/establishing_a_machine_learning_workflow_the/,toddstavish,1473171501,,0,0
188,2016-9-6,2016,9,6,23,51fgd3,LF tips for freshly installed os for ML projects,https://www.reddit.com/r/MachineLearning/comments/51fgd3/lf_tips_for_freshly_installed_os_for_ml_projects/,jacekkenji,1473172184,"First Hi to all!

I will start with some context to explain the title: I have a clean installation of Ubuntu 14.04 and I want to start making some Machine learning/ Deep learning / Android projects. 
In the past I have always installed every dependency and library necessary to make the projects compile and run. But ofcourse there were cases when something doesn't work or there are some conflicts, and I ended up formatting my os.
Now it seems that everything can be made with Docker containers in order to have a ""clean"" os.
Is that really true? There are some other  useful tools like Docker to avoid dependency problems? Or to manage my projects with easy?
Thanks for the help!",3,2
189,2016-9-7,2016,9,7,0,51fmtm,Generated human faces - not even too horrific,https://www.reddit.com/r/MachineLearning/comments/51fmtm/generated_human_faces_not_even_too_horrific/,[deleted],1473174417,[deleted],0,1
190,2016-9-7,2016,9,7,0,51fodv,Line by line explanation of RNN Language Model in Tensorflow?,https://www.reddit.com/r/MachineLearning/comments/51fodv/line_by_line_explanation_of_rnn_language_model_in/,alessca,1473174933,"Would anyone be willing to do a line by line breakdown of the RNN Language Model code in cs224D assignment 2?

More generally, are there any good videos that do LineByLine analysis for other ML tasks(preferably in TF)?",2,0
191,2016-9-7,2016,9,7,0,51ftad,Separate voices of different people,https://www.reddit.com/r/MachineLearning/comments/51ftad/separate_voices_of_different_people/,davikrehalt,1473176459,Are there papers out there for separating two or more voices talking at the same time? I searched for cocktail party effect and only found ones separating voices from music. Is this task too hard? Or are we capable of it now?,8,4
192,2016-9-7,2016,9,7,0,51ftbk,Updated Google Analytics mobile app offers automated insights via machine learning,https://www.reddit.com/r/MachineLearning/comments/51ftbk/updated_google_analytics_mobile_app_offers/,bobstanke,1473176469,,0,1
193,2016-9-7,2016,9,7,0,51ftr7,Variational vs MCMC: strengths and weaknesses?,https://www.reddit.com/r/MachineLearning/comments/51ftr7/variational_vs_mcmc_strengths_and_weaknesses/,[deleted],1473176602,"Most traditional Bayesian packages (Stan, pyMC) focus on some variant of an MCMC as its inference workhorse.  Given the recent revival of variational methods (at least in terms of popularity), what are some strengths and weaknesses of each approach in practice?

My understanding is that variational methods often converge much faster, but the estimates are biased.  If I've been using MCMC to do Bayesian linear regression, hierarchical modeling, etc. what would be the advantages of looking into the variational approach?",10,10
194,2016-9-7,2016,9,7,1,51g0ol,"Unified Online Classifier for Binary, Multi-class and Multi-label Classification !!!",https://www.reddit.com/r/MachineLearning/comments/51g0ol/unified_online_classifier_for_binary_multiclass/,rakesajar,1473178817,,2,0
195,2016-9-7,2016,9,7,2,51g8dw,Network Plasticity as Bayesian Inference,https://www.reddit.com/r/MachineLearning/comments/51g8dw/network_plasticity_as_bayesian_inference/,wallop_woolee,1473181252,,1,10
196,2016-9-7,2016,9,7,2,51g8ou,Help with design of RNN training and data normalization for nlp text/phrase classifier. Batched training?,https://www.reddit.com/r/MachineLearning/comments/51g8ou/help_with_design_of_rnn_training_and_data/,Thought_Ninja,1473181353,"Disclaimer, I'm somewhat new to ML, so don't hold back in telling me what's what.

**Goal:** I'm trying to develop a RNN training model for the purpose of classifying sentence sentiment/objectivity based upon sentence structure. 

The reasoning behind using an RNN is to avoid the 'bag of words' approach, since context is important.

Example:

Bob is in danger: [subject] [link] [quality (-)]

*sums quality - results in negative*

Bob is in great danger: [subject] [link] [quantifier (+)] [quality (-)]

*sums (quantifier \* quality) pairs - results in greater negative*

Bob is in no danger: [subject] [link] [quantifier (0)] [quality (-)]

*sums (quantifier \* quality) pairs - result is neutral*

Not exact, but it gives the idea.

I have no problem breaking down the sentences into this structure, but sentences may have an unknown number of quantifiers and/or qualities. I don't want to normalize the input to the point of loosing data.

**My Approach + The Problem:** I started working on a word by word approach, using Google's words2vector library to form vector representations of the expected dictionary (based upon a large catalog of training data).

The problem is, I have no idea how to approach expected output and the training model when the data (sentences) are of unknown length. 

What I want is to have a sort of 'batched' training approach, but I do not know how that would work and haven't found any literature on it.

I suppose I'm a bit in-over-my-head on this one and am sure there is a more eloquent solution. I'd love some recommended reading material on the topic of training methods and normalization for RNN.

So far I've read a [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Karpathy, which was fairly insightful, but not quite what I'm aiming for. It was more focused on character-level training when I'm looking for structural classification.

Any help or suggestions would be greatly appreciated! :) Thanks.",2,1
197,2016-9-7,2016,9,7,3,51gkjm,Pruning least-close matches in affinity propagation clustering,https://www.reddit.com/r/MachineLearning/comments/51gkjm/pruning_leastclose_matches_in_affinity/,dexintern,1473185992,"I'm working on clustering a list of similar company names (e.g. returning 'Google Inc.' as the exemplar for 'Google Switzerland AG', 'Google Ventures Ltd.', 'Google France SA', etc.), and have settled on affinity propagation with Jaccard similarity as the pairwise similarity metric, which is providing good results so far.

The issue, though, is that affinity propagation returns ""comprehensive"" (not sure if this is the right term) clusters; every data point is associated with an exemplar. This results in some not-so-close matches being associated with an exemplar (e.g. even if ""Apple"" is not close to any other company names, ""Apple"" will still correspond to an exemplar.

Visually, what I'd like to accomplish is analogous to excluding the furthest data points in this example: http://imgur.com/a/mGrQy?reg

What would be the best way of doing this? Should I re-run the Jaccard similarities within each exemplar/examples set and exclude poor matches? ",0,2
198,2016-9-7,2016,9,7,3,51gp0y,My blog post explaining Stacked Approximation Regression Machines from first principles,https://www.reddit.com/r/MachineLearning/comments/51gp0y/my_blog_post_explaining_stacked_approximation/,gabrielgoh,1473187413,,16,74
199,2016-9-7,2016,9,7,4,51gwzn,[1608.05983] Deep Generative Models for Spectroscopic Analysis on Mars,https://www.reddit.com/r/MachineLearning/comments/51gwzn/160805983_deep_generative_models_for/,dharma-1,1473189869,,0,5
200,2016-9-7,2016,9,7,4,51h2ob,"Deep Learning book by Goodfellow, Bengio, and Courville available for preorder on Amazon",https://www.reddit.com/r/MachineLearning/comments/51h2ob/deep_learning_book_by_goodfellow_bengio_and/,DavidSJ,1473191621,,6,28
201,2016-9-7,2016,9,7,5,51he15,"$93,562,000 awarded by Canadian Gov. for Deep Learning Research at University of Montreal",https://www.reddit.com/r/MachineLearning/comments/51he15/93562000_awarded_by_canadian_gov_for_deep/,pierrelux,1473195048,,80,452
202,2016-9-7,2016,9,7,6,51hixl,Techniques for removing/accommodating noise before trading and classification?,https://www.reddit.com/r/MachineLearning/comments/51hixl/techniques_for_removingaccommodating_noise_before/,appledeveloper,1473196613,Hi everyone what are some good techniques for grayscale images to reduce noise before classification using a convolutional neural network? ,1,0
203,2016-9-7,2016,9,7,7,51hqvp,Self-driving car combined steering/acceleration model (livestream),https://www.reddit.com/r/MachineLearning/comments/51hqvp/selfdriving_car_combined_steeringacceleration/,vanboxel,1473199219,,0,3
204,2016-9-7,2016,9,7,7,51hz20,Mathematics of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/51hz20/mathematics_of_machine_learning/,futureroboticist,1473201901,,0,1
205,2016-9-7,2016,9,7,8,51i2tc,Teaching an AI computer programming?,https://www.reddit.com/r/MachineLearning/comments/51i2tc/teaching_an_ai_computer_programming/,Eugene_Sandugey,1473203223,"We have deep neural networks available, why hasn't somebody used one to teach a computer programming, thus allowing it to self improve? 

Is there no good training data available? Couldn't we just feed a machine learning algorithm examples of code and its binary counterparts (similar to learning a new word), and have it use those individual words to create new sentences (software code)? This would be similar to teaching it a new language, and that's something we can do today.... There are FAR less commands in code than in language. And if it understands both computer and human language, it'll be able to translate between them. So why can't the deep language learning algorithms be applied to code?

If you can do this, you have self improving software, and it seems like companies like Google and Facebook would really want their AI's to be self improving vs relying on programmers to improve the algorithms..... ",21,0
206,2016-9-7,2016,9,7,8,51ib8z,word2vec-slim: word2vec Google News model slimmed down to 260k English words,https://www.reddit.com/r/MachineLearning/comments/51ib8z/word2vecslim_word2vec_google_news_model_slimmed/,eyaler,1473206280,,1,7
207,2016-9-7,2016,9,7,9,51if52,Algorithmic Aspects of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/51if52/algorithmic_aspects_of_machine_learning/,futureroboticist,1473207718,,0,1
208,2016-9-7,2016,9,7,10,51ina9,Intel buying chipmaker Movidius to boost artificial-intelligence efforts,https://www.reddit.com/r/MachineLearning/comments/51ina9/intel_buying_chipmaker_movidius_to_boost/,vonnik,1473210606,,5,11
209,2016-9-7,2016,9,7,11,51izdk,"Dominos, Botnets, and a little LSTM - OpenDNS Umbrella Blog",https://www.reddit.com/r/MachineLearning/comments/51izdk/dominos_botnets_and_a_little_lstm_opendns/,bugoid,1473215051,,1,2
210,2016-9-7,2016,9,7,13,51jen4,How does this company apply 'machine intelligence' to financial predictive modeling?,https://www.reddit.com/r/MachineLearning/comments/51jen4/how_does_this_company_apply_machine_intelligence/,htrp,1473220946,,13,3
211,2016-9-7,2016,9,7,13,51jg8l,CS 229 was taught by Andrew Ng till 2015 Autumn and seem to have newer video lectures (based on the preview). Anybody knows if *all* the lectures are publicly accessible?,https://www.reddit.com/r/MachineLearning/comments/51jg8l/cs_229_was_taught_by_andrew_ng_till_2015_autumn/,assassin_of_damned,1473221601,,2,3
212,2016-9-7,2016,9,7,13,51jhma,How does Randomized Grid Search from sklearn choose which parameters to choose? Is it fully random?,https://www.reddit.com/r/MachineLearning/comments/51jhma/how_does_randomized_grid_search_from_sklearn/,CopyOnWriteArraySet,1473222184,,1,4
213,2016-9-7,2016,9,7,15,51jwy2,Few questions on deconvolution,https://www.reddit.com/r/MachineLearning/comments/51jwy2/few_questions_on_deconvolution/,Seon-Ho,1473229796,"Hello,

Recently I have read quite a few papers on deconvolution networks and some points are still relatively unclear to me. First the same name seems to apply for different kind of technique. So here i would like to expose what i have understood so far is a deconvolution in order to ask precise questions about it. Thanks for taking time to read and hopefully answer.

&amp;nbsp;

**1 - Original Deconvolution** [Zeiler (2010)](http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf) and [Zeiler (2011)](http://www.matthewzeiler.com/pubs/iccv2011/iccv2011.pdf) use what I think is the closest to a real deconvolution. From input images you learn the filters and latent maps that if convolved together give the closest reconstructed images. 

**Learning:** Given an ensemble of inputs, learn the latent maps and the filters by updating each through specific optimization technique.

**Inference:** Given input and filters, learn the latent maps.

&amp;nbsp;

**2 - What seems to be called transposed convolution** [Zeiler (2014)](http://www.matthewzeiler.com/pubs/arxive2013/arxive2013.pdf) introduce a different kind of process compared to the one in 1. Here you have a classic CNN **and** an ""deconvolution"" part. The CNN part is well understood and what interest me here is the deconvolution part. He introduces for every operation/layer of the CNN (conv, pool, relu) it ""reverse"" operation. Pooling has what they call unpooling, relu has relu and conv has what I will call here ""unconv"".

**Unpooling:** The unpooling process is well explained (use of switch...) and almost every paper has this [illustration](http://images.slideplayer.com/15/4561739/slides/slide_27.jpg).

**""Unconv"":** In this specific case the unconv process is just a classic convolution and the filter used is the transposition of the one in the corresponding layer of the dual CNN. So no learning required. We could see it as the weight being tied. [Here](https://www.quora.com/In-the-visualization-tecnique-by-Zeiler-Fergus-for-ConvNets-the-convolution-operation-is-inverted-by-a-convolution-with-the-flipped-version-of-the-original-kernel-Why-does-it-work) is an explanation which helped me understand why use transposed filter.

&amp;nbsp;

**3 - Deconvolution in conv autoencoder** [this paper for instance](http://arxiv.org/pdf/1505.04366v1.pdf). Here is where i have most trouble understanding. Here you have two parts in the network, the encoding part which is cnn architecture and the decoding part. The decoding part, like above in 2, use for every type of layer (conv, pool, relu) it ""reverse"" operation. Again pooling's dual operation is unpooling and relu dual operation is itself. However contrary to above, i don't quite understand how the ""unconv"" operation works.

&amp;nbsp;

So here are my question:

1. What is really the reverse of the convolution operation in the third case. Is it a classic convolution as explained [here](http://stackoverflow.com/a/36342908), *ie* it 

&gt; is just the transposed of its corresponding conv layer: if conv layer's shape is [height, width, numColors, numMaps] then the deconv layer will have the shape [height, width, numMaps, numColors] and the weights of conv and deconv layers are shared. 

Or is it like this [gif](http://i.stack.imgur.com/f2RiP.gif) explain. Or is it something else?

2. Then how do you learn the parameters of the kernel (according on how you answer the previous question, the answer to this one could also be straightforward)?

3. Does my distinction of the three cases make sense, *ie* 1 = original deconv, 2 = transposed conv with no learning, 3 = ""unconv"" which is maybe transposed conv but here learning is required.

4. [Here](https://github.com/tensorflow/tensorflow/issues/256#issuecomment-162257789) they say that in the 2011 papers of Zeiler it 
&gt; had nothing to do with actually deconvolving the input

 But as my understanding goes, in this paper they indeed learn the latent maps and filters that convolved together give a close reconstruction. The person talks about how the name should be transposed convolution but i think it is more with the 2014 paper that transposed convolution is appropriate. What is wrong with my reasoning? 
 ",5,11
214,2016-9-7,2016,9,7,16,51k1al,Need help with PTB language modelling :ELU IRNN post,https://www.reddit.com/r/MachineLearning/comments/51k1al/need_help_with_ptb_language_modelling_elu_irnn/,[deleted],1473232164,[deleted],0,1
215,2016-9-7,2016,9,7,16,51k2ig,Need help with PTB language modelling :ELU IRNN post,https://www.reddit.com/r/MachineLearning/comments/51k2ig/need_help_with_ptb_language_modelling_elu_irnn/,sonamsingh19,1473232887,,1,3
216,2016-9-7,2016,9,7,16,51k4l6,Python supported GPU training of NN,https://www.reddit.com/r/MachineLearning/comments/51k4l6/python_supported_gpu_training_of_nn/,random_vision,1473234088,"Hi All, I am looking to learn from others experiences with python supported GPU training of NN. 

I am reading up on Theano, PyCUDA and some others, but I don't fully understand their differences, and benefits.   

If you were going back to implement GPU based NN training for the first time, where would you start and what would you do differently?

Thanks,
",12,0
217,2016-9-7,2016,9,7,16,51k6e7,[1609.01704] Hierarchical Multiscale Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/51k6e7/160901704_hierarchical_multiscale_recurrent/,evc123,1473235136,,7,37
218,2016-9-7,2016,9,7,17,51k8et,Distance preservation measure for Random Projection evaluation (bounty worth +50),https://www.reddit.com/r/MachineLearning/comments/51k8et/distance_preservation_measure_for_random/,Lopelh,1473236353,http://stats.stackexchange.com/questions/230059/distance-preservation-measure-for-random-projection-evaluation,0,2
219,2016-9-7,2016,9,7,17,51kalc,Simple benchmark of deep learning frameworks on VGG-16,https://www.reddit.com/r/MachineLearning/comments/51kalc/simple_benchmark_of_deep_learning_frameworks_on/,perceptron01,1473237633,,6,7
220,2016-9-7,2016,9,7,18,51kgiy,Difficulties training a Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/51kgiy/difficulties_training_a_generative_adversarial/,charlie0_o,1473241198,"Hi all,

Implemented DCGAN using tensorflow and wanted to share some notes on the problems I faced to get things working and a few observations. Hoping to get some comments and any insights you have.

My notes can be found here: [link](https://github.com/shekkizh/neuralnetworks.thought-experiments/blob/master/Generative%20Models/GAN/Readme.md)

Code can be found here: [link](https://github.com/shekkizh/TensorflowProjects/blob/master/Unsupervised_learning/)

None of the images in the results were cherry picked.",11,40
221,2016-9-7,2016,9,7,19,51kjeo,Visual Question Answering Problems: Reasoning With Deep Learning,https://www.reddit.com/r/MachineLearning/comments/51kjeo/visual_question_answering_problems_reasoning_with/,reworksophie,1473242848,,0,1
222,2016-9-7,2016,9,7,19,51kmaw,Machine Learning is Fun!,https://www.reddit.com/r/MachineLearning/comments/51kmaw/machine_learning_is_fun/,reworksophie,1473244559,,0,1
223,2016-9-7,2016,9,7,19,51kn06,Needs help with Adaboost on MNIST dataset : Weak learners give very high errors on first iterations :(,https://www.reddit.com/r/MachineLearning/comments/51kn06/needs_help_with_adaboost_on_mnist_dataset_weak/,dangmanhtruong,1473244949,"Hi, I'm trying to work through this assignment: http://vlm1.uta.edu/~athitsos/courses/old/cse5361_spring2008/assignments/assignment4/ , which is to implement Adaboost on MNIST dataset. Here is their instructions, in a nutshell:

- Make some, say 1000, random rectangle filters (like Haar features or something, I don't understand much of this yet  )

- Apply it on the training set and find out which one gives the lowest weighted error

- Use it to train strong learners 

But if I choose these rectangle filters randomly, how do I guarantee the weak learner assumption that the error is slightly less than 0.5 ? Indeed in my implementation I always get errors very close to 1 (e.g, [ 0.90128333]). Here is my Python code: http://www.mediafire.com/download/2x1bt8ychx53sor/Adaboost.rar . To run it, type:

from Adaboost import Adaboost

adaboost = Adaboost()

adaboost.train()
  
Please help me, I'm just a novice in machine learning, thank you very much :) . ",12,1
224,2016-9-7,2016,9,7,20,51kusd,the 2nd Global Achievement Awards for AI is now open for voting,https://www.reddit.com/r/MachineLearning/comments/51kusd/the_2nd_global_achievement_awards_for_ai_is_now/,homeAIinfo,1473249075,,0,0
225,2016-9-7,2016,9,7,21,51ky57,[Question] How to build a Machine Learning Web Service Architekture?,https://www.reddit.com/r/MachineLearning/comments/51ky57/question_how_to_build_a_machine_learning_web/,kruppy,1473250639,"Hey guys,

I'm quite interested in the topic of ML and am trying to improve my skills while working with scripts to solve examples like iris dataset classification etc.

Now I am asking myself, if it is possible to create something like a web service where I train a model and then later send example data to that has to be classified.

Is there any best practice? I mainly use python to create my solutions for ML examples.

Thanks!",6,6
226,2016-9-7,2016,9,7,21,51l0fj,"AI and ML Futures 3: The Trojan Wars of Machine Learning By Neil Lawrence, Professor of Machine Learning - University of Sheffield",https://www.reddit.com/r/MachineLearning/comments/51l0fj/ai_and_ml_futures_3_the_trojan_wars_of_machine/,OpenDataSciCon,1473251633,,0,9
227,2016-9-7,2016,9,7,21,51l0og,Question: Big Data Sets for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/51l0og/question_big_data_sets_for_machine_learning/,prog_hi,1473251751,"Hi,

What approach do you follow for handling gigabyte sized datasets with a minimal configuration system(lets say with 16GB RAM).

* Exploratory Data Analysis
* Merge/Concat Data
* Machine Learning

Thank you.",4,1
228,2016-9-7,2016,9,7,22,51l72r,Weapons of Math Destruction: How data is driving inequality,https://www.reddit.com/r/MachineLearning/comments/51l72r/weapons_of_math_destruction_how_data_is_driving/,citizen64,1473254206,,4,0
229,2016-9-7,2016,9,7,22,51lddg,A technical intro to Pearlian causality!,https://www.reddit.com/r/MachineLearning/comments/51lddg/a_technical_intro_to_pearlian_causality/,akelleh,1473256702,,24,66
230,2016-9-7,2016,9,7,23,51lg9i,Call for Papers: Workshop on Neural Abstract Machines &amp; Program Induction (NAMPI) @NIPS'2016!,https://www.reddit.com/r/MachineLearning/comments/51lg9i/call_for_papers_workshop_on_neural_abstract/,_rockt,1473257742,,3,8
231,2016-9-7,2016,9,7,23,51ljqs,create own dataset like MNIST,https://www.reddit.com/r/MachineLearning/comments/51ljqs/create_own_dataset_like_mnist/,Indra97065,1473258933,[removed],0,1
232,2016-9-8,2016,9,8,1,51ma54,"Redditor makes some really cool beats and drops them in /r/piano. Someone tells him that the music was written by ""computer learning algorithms"".",https://www.reddit.com/r/MachineLearning/comments/51ma54/redditor_makes_some_really_cool_beats_and_drops/,nlaeae,1473267530,,3,0
233,2016-9-8,2016,9,8,2,51mbpj,What is local minimum in graph? How it is different than minimum?,https://www.reddit.com/r/MachineLearning/comments/51mbpj/what_is_local_minimum_in_graph_how_it_is/,pranitkothari,1473268038,,6,0
234,2016-9-8,2016,9,8,2,51menj,Undergraduate Independent Study,https://www.reddit.com/r/MachineLearning/comments/51menj/undergraduate_independent_study/,Putroce,1473268970,"Hello, I'm taking an independent study this semester with a professor and we agreed that following a textbook on machine learning would be a good idea. I took half of Andrew Ng's class so I understand the concepts but in no way do I understand the actual theory. By the end of the semester I'd like to have a good foundation where I'll be able to implement what I've learned. 

I've taken calc2, multivariable calc, differential equations, linear algebra, and I'm taking discrete now so I think my math knowledge is pretty good. I currently have bishops pattern recognition book, do you think that is a good textbook to go through this semester? I'd love some feedback.",2,0
235,2016-9-8,2016,9,8,2,51mgv8,[Keras] Help on implementing a layer,https://www.reddit.com/r/MachineLearning/comments/51mgv8/keras_help_on_implementing_a_layer/,[deleted],1473269673,[deleted],0,1
236,2016-9-8,2016,9,8,3,51musv,"Request: Is the video for Jeff Dean's NIPS 2014 talk, ""Techniques and Systems for Training Large Neural Networks Quickly"", available anywhere ?",https://www.reddit.com/r/MachineLearning/comments/51musv/request_is_the_video_for_jeff_deans_nips_2014/,manjunaths,1473274239,"The slides are here (stanford.edu/~rezab/nips2014workshop/slides/jeff.pdf), but I can't find the video online. Does anyone have a link to it ?",1,5
237,2016-9-8,2016,9,8,4,51n7n9,[Question] Best approach for dynamic curriculum generation based on previous students attributes and course sequences?,https://www.reddit.com/r/MachineLearning/comments/51n7n9/question_best_approach_for_dynamic_curriculum/,philipwithpostral,1473278356,"Just looking for a place to start on this model. I have a ton of data for students taking a wide variety of general skills courses and know some general attributes of most of them (job title, industry they are in, approximate age, etc...) 


I'd like to suggest a recommended curriculum (read:sequence of courses) for a future hypothetical student based on the sequence chosen by previous students and their attributes.


I have a passing familiarity with a number of machine learning paradigms, but I'm stumped on what the best approach would be for this type of sequence prediction. Any suggestions or resources that characterize this particular type of problem would be appreciated.",0,1
238,2016-9-8,2016,9,8,5,51nb44,"How important is it look at proofs of convergence of Reinforcement learning algorithms such as TD(0), Q-learning etc to establish something concrete such as DQN?",https://www.reddit.com/r/MachineLearning/comments/51nb44/how_important_is_it_look_at_proofs_of_convergence/,mundada,1473279364,"Hi, everyone. I know this question is a bit silly but today after reading about DQN, I figured out that the most important concept in DQN was Experience replay and fixed Q network, but aren't this concepts totally intuition based ? It feels like this field is progressing more by intuition with very less technical backup (maybe I am completely wrong). As only results matter it is kind of less motivating to read proofs. Am I correct or totally screwed ?

Thanks for reading and happy learning.",21,12
239,2016-9-8,2016,9,8,5,51nbis,Google Cloud Vision API landmark detection: how does it work,https://www.reddit.com/r/MachineLearning/comments/51nbis/google_cloud_vision_api_landmark_detection_how/,fhoffa,1473279494,,1,4
240,2016-9-8,2016,9,8,5,51ndk0,Can Someone Help a TF n00b debug this simple script?,https://www.reddit.com/r/MachineLearning/comments/51ndk0/can_someone_help_a_tf_n00b_debug_this_simple/,nickbuch,1473280107,"SOLVED: i realized the issue is that softmax produces a categorical distribution function, which returns a probability distribution over every feature of each output row.  if each output row contains only 1 feature (1d array) then the sum over each feature is 1, ie the identity maxtrix.


I have augmented the MNIST example in TensorFlow to run on my data.  The shapes are correct, but for some reason my `y` values are all `1` inside of the training loop, and then when I run on test data, the `accuracy` is `1`.  Inside the training loop, my weights `W` and biases `b` are all `0`, while my inputs are a 2d array and my outputs a 1d array.  

Are there any issues?


        import tensorflow as tf
        import numpy as np

        training_inputs = np.array([[0, 0]], int)
        test_inputs = np.array([[0, 0]], int)
        training_outputs = [0]

          user_array = np.array([[len_bio, number_skills]])
          training_inputs = np.concatenate(( training_inputs, user_array ), axis=0)
          test_inputs = np.concatenate((test_inputs, np.array([[number_skills, len_email]])), axis=0)

          training_outputs.append(len_email)

        x = tf.placeholder(tf.float32, [None, 2])

        W = tf.Variable(tf.zeros([2, 1]))

        b = tf.Variable(tf.zeros([1]))

        y = tf.nn.softmax(tf.matmul(x, W) + b)

        y_ = tf.placeholder(tf.float32, [None, 1])

        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))

        train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)

        init = tf.initialize_all_variables()

        sess = tf.Session()

        sess.run(init)

        for i in range(10):
          print 'iterator:'
          print i
          batch_ys = np.reshape(training_outputs, (300, 1))
          batch_xs = training_inputs
          batch_xs.reshape(300, 2)

          sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})

          #y = tf.nn.softmax(tf.matmul(x, W) + b)
          print 'y'
          print (sess.run(y, feed_dict={x: batch_xs, y_: batch_ys}))

          #print 'x'
          #print sess.run(x)

          print 'W'
          print sess.run(W)

          print 'b'
          print sess.run(b)

        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))

        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

        test_outputs = np.random.rand(300, 1)

        ## the following prints 1
        print(sess.run(accuracy, feed_dict={x: test_inputs, y_: test_outputs}))

",4,0
241,2016-9-8,2016,9,8,6,51nopx,[Question] Deep learning applied to video data for outlier detection?,https://www.reddit.com/r/MachineLearning/comments/51nopx/question_deep_learning_applied_to_video_data_for/,Zeekawla99ii,1473283666,"I learned of a company that works primarily with security camera footage. Theyre using deep learning algorithms find in real time guy with a skimask on, and other outliers from hours of CCTV footage. 

Question: what algorithms could they be applying for ""anomaly detection""? It appears as if they are applying convolutional neural networks to hours of imaging data...somehow. 

Any papers, etc. appreciated. Thanks!

",4,4
242,2016-9-8,2016,9,8,6,51nqjy,Comparing 3d tensor and 4d tensor Tensorflow,https://www.reddit.com/r/MachineLearning/comments/51nqjy/comparing_3d_tensor_and_4d_tensor_tensorflow/,julesbelan,1473284238,[removed],0,1
243,2016-9-8,2016,9,8,6,51nrtt,Machine learning internships/summer research opportunities,https://www.reddit.com/r/MachineLearning/comments/51nrtt/machine_learning_internshipssummer_research/,Lopelh,1473284651,,1,0
244,2016-9-8,2016,9,8,8,51oez1,Connecting images and natural language: Andrej Karpathy's PhD thesis,https://www.reddit.com/r/MachineLearning/comments/51oez1/connecting_images_and_natural_language_andrej/,adamnemecek,1473292754,,5,35
245,2016-9-8,2016,9,8,9,51oihg,Why Isn't More Being Done To Create New Massively Parallel Hardware For Running ML Software?,https://www.reddit.com/r/MachineLearning/comments/51oihg/why_isnt_more_being_done_to_create_new_massively/,misstheground12,1473294047,"I'm not in the AI world, so some of my terminology may be off, but hopefully the gist of my question is understandable. Please let me know if anything I talk about here is partially or totally off-base, because I'd love to know that. Thanks.


I know there are a few things out there - Synapse, and TrueNorth from IBM, both of which LeCun has totally disparaged. I believe he said something like ""They built the wrong chip."" And he suggested that these things were built because the researchers wanted the funding and were willing to build something useless in order to get that funding.


LeCun didn't say that doing something like this was useless, just that they didn't go about it the right way.
I remember reading that it took Google warehouses of processors to train the neural network that could then identify cats in youtube videos.


Why not invest in new, parallel, low power hardware to bring the requirements down? Is it just that researchers don't really know what to build?


I talked with a professor about this, and he said it just isn't worth anyone's time to create an alternative to Von Neumann architecture. It might take a warehouse to train a neural network, but that's still cheaper than spending valuable time creating a whole new paradigm.


I kind of wonder though if this is a parallel time to computers in the 1950's that took up entire rooms and did very little. Like if we want neural networks to start doing incredibly complex things, maybe there needs to be a paradigm shift.",11,1
246,2016-9-8,2016,9,8,9,51on2m,A Survival Guide to a PhD - Andrej Karpathy,https://www.reddit.com/r/MachineLearning/comments/51on2m/a_survival_guide_to_a_phd_andrej_karpathy/,omoindrot,1473295727,,41,131
247,2016-9-8,2016,9,8,10,51owxt,Why is the learning parameter for the normalized least-mean-square algorithm bound between 0 and 2?,https://www.reddit.com/r/MachineLearning/comments/51owxt/why_is_the_learning_parameter_for_the_normalized/,gary1_feesher2,1473299376,"In all of my reading, I find that the learning parameter for the normalized least-mean-square algorithm is bound between 0 and 2.

I believe it has something to do with the stability of the algorithm above 2 - in that it will oscillate too much above 2.  But, is there a proof or equations that I can examine that help me get to this explicit bound of 0 &lt; alpha &lt; 2?",2,3
248,2016-9-8,2016,9,8,10,51ox6t,A curated list of resources dedicated to RNN,https://www.reddit.com/r/MachineLearning/comments/51ox6t/a_curated_list_of_resources_dedicated_to_rnn/,kjw0612,1473299459,,2,19
249,2016-9-8,2016,9,8,10,51oy8f,Do androids dream of electric head? Romance written by a neural net,https://www.reddit.com/r/MachineLearning/comments/51oy8f/do_androids_dream_of_electric_head_romance/,karmel_a,1473299861,,0,1
250,2016-9-8,2016,9,8,12,51p81a,Creating an A.I Bot from scratch,https://www.reddit.com/r/MachineLearning/comments/51p81a/creating_an_ai_bot_from_scratch/,treza007,1473303632,"Hey, this might be the wrong place to post this but I was wondering if you guys could possibly provide me with references/tutorials/videos ,etc that might help me create an a.i. chat bot from scratch. The goal is to create a bot similar to the chatbot lawyer DoNotPay.

Any help is appreciated!",1,0
251,2016-9-8,2016,9,8,13,51piuj,What makes for a good introductory machine learning project?,https://www.reddit.com/r/MachineLearning/comments/51piuj/what_makes_for_a_good_introductory_machine/,UmamiSalami,1473308011,"Hey. I'm tasked to select a small project that I will pursue for a machine learning course, but I have no idea what to pick, as I haven't really learned how to do machine learning yet and therefore don't know how to choose something for a project.

I know you need something where an input space of instances described by tuples of various features gets mapped to approximate the correct output space (which can be binary, multiclass or real) as well as possible. But that's about it. So do I just pick anything that fits this description, or are there other features that would help me have a good learning experience?

Also, I'm interested in social choice and preference aggregation; would it be interesting to build a machine learning model with combinatoric preferences as inputs and some kind of predicted choice as the output, or is that really the exact same thing computationally as a more typical mapping of features to classes?",2,5
252,2016-9-8,2016,9,8,13,51pl5y,Pretrained Audio CNN model?,https://www.reddit.com/r/MachineLearning/comments/51pl5y/pretrained_audio_cnn_model/,thenerdstation,1473309064,"I want to do some work with audio CNN, but I don't have a good way to create one myself. Are there any free open sourced pretrained models for voice to text?",1,5
253,2016-9-8,2016,9,8,13,51pmo3,Advice on advanced beginners/intermediates in machine learning/deep learning,https://www.reddit.com/r/MachineLearning/comments/51pmo3/advice_on_advanced_beginnersintermediates_in/,dangmanhtruong,1473309754,"Hi, I'm currently a 4th year undergraduate student in IT, and I want to pursue machine learning/deep learning and produce a paper :) . I started learning this stuff at the beginning of my 3rd year, and here are what I've learnt so far:

- I started by reading the book Pattern recognition and machine learning (Bishop). Read chapter 1-4, the first part of chapter 5 on neural network (I avoided all those hessian matrix, tikhonov regularization,... :( ), and some of chapter 9 on Expectation-Maximization. I also coded the results of Figure 9.10 from the book, about using EM with Bernoulli mixture model on the MNIST dataset: https://github.com/ManhTruongDang/PRML. (In matlab)

- I also followed this Stanford course on Convolutional neural network: http://ufldl.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/ (they teach you how to build a very simple CNN), here is my matlab code: http://www.mediafire.com/download/nq3ab8bf48f59nr/stanford_dl_ex-master.rar 
- I also tried a course on Convex optimization (Boyd), but dropped out after a while :( . Guess I'm not so determined after all huh :( :( 
Now i realize that although I'm (just a little more) comfortable with the math now, I'm ""lost"". Can anyone point me in the right direction so that I could have enough background to produce a paper in the field ? :( :( Here's what I'm planning to do :
- http://vlm1.uta.edu/~athitsos/courses/old/cse5361_spring2008/assignments/assignment4/ , this assignment asks you to program Adaboost and apply it on the MNIST dataset. Working on this one right now 
- http://www.columbia.edu/~jwp2128/Teaching/E6892/E6892Fall2015.html , maybe do this one in a semester
- Maybe Viola-Jones ? :( 

Or maybe I should just start reading papers? :( . Please help me, I'm really really lost right now :( ",2,3
254,2016-9-8,2016,9,8,14,51psc2,Top 10 machine learning articles from August,https://www.reddit.com/r/MachineLearning/comments/51psc2/top_10_machine_learning_articles_from_august/,amitjyothie,1473312556,,7,50
255,2016-9-8,2016,9,8,14,51ptvm,Embeddings for Integers and Integer Arrays?,https://www.reddit.com/r/MachineLearning/comments/51ptvm/embeddings_for_integers_and_integer_arrays/,MetricSpade007,1473313346,"We have a number of ways to create embeddings for strings, but do people know of generic/meaningful ways to encode integers, or integer arrays? I have a neural system in mind that consumes either integers or an array of integers, but I want to map them to some embedding space -- do people know of good ways to encode these values?

The initial thought I had was to simply pass the numbers through a bidirectional LSTM and concatenate (or add or average) the hidden states to have a fixed-length encoding but i'm not sure if there's anything better?",0,4
256,2016-9-8,2016,9,8,14,51pupe,Stochastic gradient estimation fundamentals (as in e.g VAEs),https://www.reddit.com/r/MachineLearning/comments/51pupe/stochastic_gradient_estimation_fundamentals_as_in/,kitofans,1473313770,"In VAEs, the true objective:

 - P(x) = integral over z [p(x,z)dz] = integral over z [p(z)p(x|z)dz] = Expectation with respect to z ~ p(z) [p(x|z)]

Is replaced by a lower bound over the form

 - -KLD(q(z|x) || p(z)) + Expectation with respect to z ~ q(z|x) [p(x|z)]

(I might be slightly off in notation). One of the original problems in optimizing the objective P(x) directly is the intractable integral over z in the expectation. What is interesting to me is that the variational objective still has an intractable integral over z; the only difference is that the expectation is w.r.t q(z|x) instead of p(z). The solution to this is to take monte carlo gradient estimates with the typical reparametrization trick. However, we could have done the exact same thing in the original objective (in fact, we wouldn't need the reparametrization trick, as p(z) does not rely on trainable parameters) -- it's just that, this would typically do poorly, since sampling from p(z) for the gradient estimation is unlikely to produce a z that corresponds to a high p(x,z). Therefore, it makes sense to introduce the distribution q(z|x), since sampling from this distribution is likely to give us values of z that correspond to high p(x,z).

What I'm interested in is a review of the work around this method -- that is, improving gradient estimates by sampling from a better distribution. There are a number of techniques, actually, that are either explicitly interpretted as or could be interpretted as a method that solves the intractable integral problem by just picking argmax_z [q(z|x)] and calculating the gradient estimate with just that single sample each iteration, rather than sampling N (even though N is often 1) samples from q(z|x) and estimating the gradient with those samples. What are the upsides of the different approaches? Is it mathematically proven that introducing the q(z) is better than just sampling from p(z) and optimizing the original objective (of course this makes a lot of intuitive sense, but how does the math work out)?

I'm more or less looking for an overview of the field. If anyone could point me in the right direction, I'd appreciate it.",8,9
257,2016-9-8,2016,9,8,15,51q1v5,"How to install Caffe on Ubuntu 16.04 with GPU (Cuda 8, CuDNN 5.1)",https://www.reddit.com/r/MachineLearning/comments/51q1v5/how_to_install_caffe_on_ubuntu_1604_with_gpu_cuda/,[deleted],1473317729,[deleted],0,1
258,2016-9-8,2016,9,8,16,51q42m,Theory Question: MLP and Basis Functions,https://www.reddit.com/r/MachineLearning/comments/51q42m/theory_question_mlp_and_basis_functions/,Jojanzing,1473318977,"In his comments on the Extreme Learning Machine (https://www.facebook.com/yann.lecun/posts/10152872571572143), Yann LeCun talks about ""schemes to non-linearly expand the dimension of an input vector so as to make the data more separable"", and among others suggests ""using gradient descent to optimize the position of the basis functions (aka a 2-layer neural net trained with backprop).""

QUESTION:
Is he talking exclusively about RBF, kernel methods and the like? Or is there a formalisation of standard MLPs in terms of Basis Functions?

Any comments or interesting references are appreciated!",1,2
259,2016-9-8,2016,9,8,16,51q4a3,Behavioral profiling using smartphone sensor data,https://www.reddit.com/r/MachineLearning/comments/51q4a3/behavioral_profiling_using_smartphone_sensor_data/,esurior,1473319099,,0,5
260,2016-9-8,2016,9,8,16,51q6ec,Fast detection of COCO objects in 5 commands,https://www.reddit.com/r/MachineLearning/comments/51q6ec/fast_detection_of_coco_objects_in_5_commands/,pjreddie,1473320421,,0,5
261,2016-9-8,2016,9,8,17,51qar6,Deep neural network powered text analysis API's explained with use cases.,https://www.reddit.com/r/MachineLearning/comments/51qar6/deep_neural_network_powered_text_analysis_apis/,jainadi341,1473323319,,0,1
262,2016-9-8,2016,9,8,17,51qcnh,Research Topics in ML/DL,https://www.reddit.com/r/MachineLearning/comments/51qcnh/research_topics_in_mldl/,[deleted],1473324572,[removed],0,1
263,2016-9-8,2016,9,8,17,51qd79,Extract date from an image,https://www.reddit.com/r/MachineLearning/comments/51qd79/extract_date_from_an_image/,imanishshah,1473324960,"Hello,

I have a dataset of Indian cheques of three months (May, June, July). I'm using Maxout network with 8 softmax layer (DDMMYYYY) to predict the date and I got 80% accuracy. But, the problem is it will not be able to predict future months and years as all the data I provided to the network has only those above mentioned 3 months and year of 2016. My doubt is how can I predict future dates as well without any other data. 

One model that I tried was -
Maxout network -&gt; FC layer -&gt; FC layer -&gt; 8 forked FC layers - &gt; one FC layer per forked layer with shared weights. But, this model does not predict future dates.

",1,1
264,2016-9-8,2016,9,8,18,51qhc8,Phd-level courses,https://www.reddit.com/r/MachineLearning/comments/51qhc8/phdlevel_courses/,Kiuhnm,1473327402,"Here's a list of advanced courses about ML:

1. [Advanced Introduction to ML](http://www.cs.cmu.edu/~bapoczos/Classes/ML10715_2015Fall/index.html)  - [videos](https://www.youtube.com/playlist?list=PL4DwY1suLMkcu-wytRDbvBNmx57CdQ2pJ&amp;jct=q4qVgISGxJql7TlE6eSLKa8Wwci8SA)

2. [Large Scale ML](http://www.cs.toronto.edu/~rsalakhu/STA4273_2015/) - [videos](http://www.cs.toronto.edu/~rsalakhu/STA4273_2015/lectures.html)

3. [Statistical Learning Theory and Applications](http://www.mit.edu/~9.520/fall15/index.html) - [videos](https://www.youtube.com/playlist?list=PLyGKBDfnk-iDj3FBd0Avr_dLbrU8VG73O)

4. [Regularization Methods for ML](http://lcsl.mit.edu/courses/regml/regml2016/) - [videos](https://www.youtube.com/playlist?list=PLbF0BXX_6CPJ20Gf_KbLFnPWjFTvvRwCO)

5. [Statistical ML](http://www.stat.cmu.edu/~larry/=sml/) - [videos](https://www.youtube.com/playlist?list=PLTB9VQq8WiaCBK2XrtYn5t9uuPdsNm7YE)

6. [Convex Optimization](http://www.stat.cmu.edu/~ryantibs/convexopt-S15/) - [videos](https://www.youtube.com/playlist?list=PLjbUi5mgii6BZBhJ9nW7eydgycyCOYeZ6)

7. [Probabilistic Graphical Models 2014 (with videos)](http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html) - [PGM 2016 (without videos)](http://www.cs.cmu.edu/~epxing/Class/10708-16/lecture.html)

---

Please let me know if you know of any other *advanced* (Phd-level) courses. I don't mind if there are no videos, but I don't like courses with no videos ***and*** extra concise and incomprehensible slides.

And no, CS229 is *not* advanced!",40,318
265,2016-9-8,2016,9,8,18,51qhvi,What is the chemical resistant silicone sealant package and filling system,https://www.reddit.com/r/MachineLearning/comments/51qhvi/what_is_the_chemical_resistant_silicone_sealant/,mixmachinery,1473327712,,1,1
266,2016-9-8,2016,9,8,19,51qlmu,[Keras] Help on implementing a layer,https://www.reddit.com/r/MachineLearning/comments/51qlmu/keras_help_on_implementing_a_layer/,fuckinghelldad,1473329762,"I want to take the output from a previous layer (say, `Dense`), repeat that vector n times to form a matrix, then take the Hadamard product of it and a (learned) weight matrix.

Following the example on [keras.io](https://keras.io/layers/writing-your-own-keras-layers/), I implemented a `Layer` with the following `call` function:

    def call(self, x, mask=None):
        n = self.output_dim
        X = K.repeat(x, n)
        return K.dot(x, X * self.W)

(I'm dotting `X * self.W` with `x` only so its shape is the same as the output of `call`.) The part which is failing is `X * self.W`. I think it's because `X` has an axis for the batch whereas `self.W` doesn't, as it's just a `keras.variable` instance. (Right?)

Here's the last line of the error I get after running the program:

    Exception: Input 0 is incompatible with layer dense_2: expected ndim=2, found ndim=3

My custom layer is between a `Dense(150)` and a `Dense(1)`.

Again, my main question is: *How do I implement the Hadamard product?* But I'd also like to know of any tips which might be useful for figuring this out for myself in the future, aside from gradually becoming familiar with Keras and the backend I happen to be using.",2,3
267,2016-9-8,2016,9,8,20,51qs3h,Simple python library to use Stanford Core NLP sentiment analysis easily.,https://www.reddit.com/r/MachineLearning/comments/51qs3h/simple_python_library_to_use_stanford_core_nlp/,[deleted],1473333253,[deleted],0,1
268,2016-9-8,2016,9,8,22,51r7cv,Small-sample pattern recognition/classification?,https://www.reddit.com/r/MachineLearning/comments/51r7cv/smallsample_pattern_recognitionclassification/,5585310558531310,1473340079,"Hello all,

Im a bit of an aspiring statistician looking for some assistance with finding a machine learning solution to a problem. Due to an NDA I cant share the exact details of the problem, but the below example captures the principles.

I have a person in an observed study who is being asked 75 yes or no questions, and I want to be able to predict their answer before they give it. A lot of people might have physical tells that indicate their answer before they verbalize it (for example, looking away from interviewer on a no answer, touching the table on a yes), and some people may have no discernable tells at all. A human watching the person may be able to pick up on these, but I want to develop a system to assist this observer by manually logging a bunch of physical indicators (hand position, head position, eye position, etc.) and using machine learning to help search for possible. As these are one-off interviews and each person is wholly independent from any others, there is no real training dataset. I need a machine learning solution that has a high degree of resolution in a small data-set (determining the presence of a tell by question 7 is preferable to determining it on question 70). Overfitting/false-positives are OK and actually preferred here, as there will be a human vetting the validity of a pattern the machine suggests; were using the machine learning model more to raise the flag on a potential pattern for the human observer to take a closer look at, with implausible suggestions easily discarded, if that makes sense.

An example dataset is below (there would be significantly more physical variables)ideally in this example, the algorithm might suggest a strong fit on the EyePos variable by question 6.

Question#|Answer|HeadPos|HandPos|EyePos

1|No|1|0|1

2|Yes|1|0|0

3|No|1|0|1

4|No|0|1|1

5|No|1|0|1

6|Yes|1|0|0

I consider myself to be fairly statistically literate and a reasonable programmer in R, but this kind of specialized use case is a bit out of my depth. If any of you have suggestions of a way to optimize a classification algorithm for this use case, it would be much appreciated!
",2,2
269,2016-9-8,2016,9,8,22,51ra42,"Kermit the Frog is 'La Rana Ren' in hispanic markets. Which is weird I know, but weird stuff happens when they translate characters (even if they don't sound remotely alike) AND movie titles (bunch of spoilers solely in the title).",https://www.reddit.com/r/MachineLearning/comments/51ra42/kermit_the_frog_is_la_rana_ren_in_hispanic/,purplepanda381,1473341155,,0,1
270,2016-9-8,2016,9,8,22,51raqv,Image generated by a Convolutional Network,https://www.reddit.com/r/MachineLearning/comments/51raqv/image_generated_by_a_convolutional_network/,abriellea,1473341409,,0,0
271,2016-9-8,2016,9,8,22,51rc6d,Using convolutional nets to recognise images with Keras,https://www.reddit.com/r/MachineLearning/comments/51rc6d/using_convolutional_nets_to_recognise_images_with/,DrLegend,1473341977,,0,4
272,2016-9-8,2016,9,8,22,51rdcw,The Variational Rnyi Lower Bound - notes on upcoming NIPS paper,https://www.reddit.com/r/MachineLearning/comments/51rdcw/the_variational_rnyi_lower_bound_notes_on/,fhuszar,1473342416,,26,39
273,2016-9-8,2016,9,8,23,51rhzu,"Support Vector Machines By: Gonzalo Garcia Berrotarn, Machinalis",https://www.reddit.com/r/MachineLearning/comments/51rhzu/support_vector_machines_by_gonzalo_garcia/,OpenDataSciCon,1473344060,,0,2
274,2016-9-9,2016,9,9,0,51rv7c,Looking for one-class image classification tutorials,https://www.reddit.com/r/MachineLearning/comments/51rv7c/looking_for_oneclass_image_classification/,mega10d0n,1473348626,"Hello, I want to implement CNN (or other type of ML) algorithm to recognize only one class of images (for example 'cat') by training the algorithm with only this class. Do you have any idea how I can do that? Have you seen any tutorials with some of the popular python libraries (like with Tensorflow or other)?

Edit: Can you explain why are the down votes?",0,0
275,2016-9-9,2016,9,9,0,51rvc9,[1609.01704v2] Hierarchical Multiscale Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/51rvc9/160901704v2_hierarchical_multiscale_recurrent/,connectionism,1473348669,,0,0
276,2016-9-9,2016,9,9,0,51rx2v,IBM and Nvidia team up to launch a new server purpose-built for machine learning,https://www.reddit.com/r/MachineLearning/comments/51rx2v/ibm_and_nvidia_team_up_to_launch_a_new_server/,zac428,1473349256,,7,9
277,2016-9-9,2016,9,9,0,51rz10,An introduction to natural language processing,https://www.reddit.com/r/MachineLearning/comments/51rz10/an_introduction_to_natural_language_processing/,[deleted],1473349927,[deleted],0,0
278,2016-9-9,2016,9,9,1,51s96a,GIFs and source code released for GAN paper 'Generating Videos with Scene Dynamics',https://www.reddit.com/r/MachineLearning/comments/51s96a/gifs_and_source_code_released_for_gan_paper/,gwern,1473353259,,3,17
279,2016-9-9,2016,9,9,2,51sfbk,Why not let the network whiten the data?,https://www.reddit.com/r/MachineLearning/comments/51sfbk/why_not_let_the_network_whiten_the_data/,testingTestingIBS,1473355255,"Why do so many people suggest to whiten the data before training on it.  To me, it makes more sense to put the whitening operation as a network node and then let the network decide how to whiten it.  I have found this to work in practice really well and cuts down on time and errors in curating the data. ",12,4
280,2016-9-9,2016,9,9,2,51siuk,A Few Reasons How Lubrication of Your Motor Is Essential,https://www.reddit.com/r/MachineLearning/comments/51siuk/a_few_reasons_how_lubrication_of_your_motor_is/,jackerfrinandis,1473356364,,0,1
281,2016-9-9,2016,9,9,2,51sj0v,Would anyone be interested in a being part of a small volunteer group doing mutual peer review for blog posts?,https://www.reddit.com/r/MachineLearning/comments/51sj0v/would_anyone_be_interested_in_a_being_part_of_a/,gabrielgoh,1473356418,"I've come to learn writing technical articles clearly and without error is tricky - and I'm curious if anyone here would be interested in being part of a small group of ""mutual peer reviewers"" for blog posts with a nontrivial mathematical component. 

I see benefits for everyone participating:

- You get to learn lots of cool ML
- You get credit for peer review
- Your own writing will be clearer, more error free, and more trusted by the community.

thoughts?",22,24
282,2016-9-9,2016,9,9,2,51slvf,Mindpark  Testbed for deep reinforcement learning algorithms,https://www.reddit.com/r/MachineLearning/comments/51slvf/mindpark_testbed_for_deep_reinforcement_learning/,danijar,1473357305,,0,6
283,2016-9-9,2016,9,9,3,51sr9t,DeepMind: WaveNet - A Generative Model for Raw Audio,https://www.reddit.com/r/MachineLearning/comments/51sr9t/deepmind_wavenet_a_generative_model_for_raw_audio/,Spotlight0xff,1473358973,,135,427
284,2016-9-9,2016,9,9,4,51szxm,What is the trade-off between accuracy and numerical precision in the random tree of ferns algorithm. I've been told one can get good accuracy with low numerical precision. Is this the case ?,https://www.reddit.com/r/MachineLearning/comments/51szxm/what_is_the_tradeoff_between_accuracy_and/,_sk_26,1473361647,,0,1
285,2016-9-9,2016,9,9,6,51to48,Attention Mechanisms and Augmented Recurrent Neural Networks overview,https://www.reddit.com/r/MachineLearning/comments/51to48/attention_mechanisms_and_augmented_recurrent/,gwern,1473369265,,12,51
286,2016-9-9,2016,9,9,6,51tw0u,[1609.01596v1] Direct Feedback Alignment Provides Learning in Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/51tw0u/160901596v1_direct_feedback_alignment_provides/,aldole_chirale,1473371933,,5,16
287,2016-9-9,2016,9,9,7,51tynv,Mass-based similarity overcomes weaknesses of common density-based similarity metrics,https://www.reddit.com/r/MachineLearning/comments/51tynv/massbased_similarity_overcomes_weaknesses_of/,datasciguy-aaay,1473372833,,8,5
288,2016-9-9,2016,9,9,7,51tzp1,The Next Wave of Deep Learning Hardware Architectures,https://www.reddit.com/r/MachineLearning/comments/51tzp1/the_next_wave_of_deep_learning_hardware/,[deleted],1473373211,[deleted],6,14
289,2016-9-9,2016,9,9,8,51u9pq,Deep Learning in a Nutshell (part 4): Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/51u9pq/deep_learning_in_a_nutshell_part_4_reinforcement/,harrism,1473376701,,0,6
290,2016-9-9,2016,9,9,8,51u9r6,"Thematic Program on Statistical Inference, Learning, and Models for Big Data [LOTS OF VIDEOS!!!]",https://www.reddit.com/r/MachineLearning/comments/51u9r6/thematic_program_on_statistical_inference/,[deleted],1473376720,[deleted],0,2
291,2016-9-9,2016,9,9,8,51uevz,Install GPU TensorFlow From Sources w/ Ubuntu 16.04 and Cuda 8.0 RC,https://www.reddit.com/r/MachineLearning/comments/51uevz/install_gpu_tensorflow_from_sources_w_ubuntu_1604/,wagonhelm,1473378574,,6,14
292,2016-9-9,2016,9,9,9,51uhbd,Deep Belief Networks Vs Stacked Auto-Encoders,https://www.reddit.com/r/MachineLearning/comments/51uhbd/deep_belief_networks_vs_stacked_autoencoders/,Aerospacio,1473379436,"Are there any advantages of training blocks of RBM's and performing contrastive divergence over just stack auto-encoders?

Is there any probabilistic advantage from learning with data? Manifolds?
",0,0
293,2016-9-9,2016,9,9,10,51us2p,[Question] How to price machine learning solutions?,https://www.reddit.com/r/MachineLearning/comments/51us2p/question_how_to_price_machine_learning_solutions/,ganesha1024,1473383402,"I've received some requests to build some machine learning systems for companies and I'm rather inexperienced in sales, so I've been asking around to understand what is a reasonable price range. Obviously it varies with the task, but I'm looking for order of magnitude here. Say it's a demand forecasting or photograph grading problem. What would you charge for a proof of concept with the customer's data? What about for a productionized solution?",12,7
294,2016-9-9,2016,9,9,10,51ut79,SARM (Stacked Approximated Regression Machine) withdrawn,https://www.reddit.com/r/MachineLearning/comments/51ut79/sarm_stacked_approximated_regression_machine/,thatguydr,1473383842,,91,94
295,2016-9-9,2016,9,9,12,51vap0,Which is the more popular tensor format NCHW or NHWC to store the image data?,https://www.reddit.com/r/MachineLearning/comments/51vap0/which_is_the_more_popular_tensor_format_nchw_or/,dagamayank,1473390537,,4,3
296,2016-9-9,2016,9,9,13,51vqci,I've trained a model to recognize Donald Trump's voice. What do you guys think?,https://www.reddit.com/r/MachineLearning/comments/51vqci/ive_trained_a_model_to_recognize_donald_trumps/,danielravina,1473397182,,11,16
297,2016-9-9,2016,9,9,16,51w9o8,DOSNES: Doubly Stochastic Neighbor Embedding on Spheres,https://www.reddit.com/r/MachineLearning/comments/51w9o8/dosnes_doubly_stochastic_neighbor_embedding_on/,yaolubrain,1473407369,,0,11
298,2016-9-9,2016,9,9,18,51wgy8,vibration machines,https://www.reddit.com/r/MachineLearning/comments/51wgy8/vibration_machines/,asdfghhrhfhfy,1473411847,,0,1
299,2016-9-9,2016,9,9,18,51whps,Top 3 machine learning language,https://www.reddit.com/r/MachineLearning/comments/51whps/top_3_machine_learning_language/,muoro,1473412288,,0,0
300,2016-9-9,2016,9,9,20,51wvsu,One strikingly simple theoretical result about DNN.(is binary DNN convex?),https://www.reddit.com/r/MachineLearning/comments/51wvsu/one_strikingly_simple_theoretical_result_about/,xmvlad,1473420062,[removed],0,1
301,2016-9-9,2016,9,9,21,51x8t4,How do I enable regression on a CNN for integer outputs only?,https://www.reddit.com/r/MachineLearning/comments/51x8t4/how_do_i_enable_regression_on_a_cnn_for_integer/,blankexperiment,1473425865,"As far as I could understand, to enable regression on a CNN I need to,

* Change loss function to RMSE
* Replace the sigmoid/tanh activations with linear or ReLU functions

Sources: [QuoraPost](https://www.quora.com/Can-Deep-Learning-and-Neural-Networks-be-useful-for-regression-problems-where-the-output-variable-has-an-unknown-or-varying-in-real-time-upper-bound), [Deeplearning4j](http://deeplearning4j.org/linear-regression.html)

The example I am working on is modifying the LeNet to predict the number of strokes (one stroke is one pen down and pen up) for a given character image. The output labels range from 1 stroke (e.g., c, e, u, etc.) to 3 (e.g., %, *, etc.). 4 stroke characters like # to characters with accent marks also exist.

With the modifications to enable regression, the performance is way worse than that of a NLL error function.
Thanks in advance!",11,4
302,2016-9-9,2016,9,9,22,51xfl5,Unsupervised Regression Ground Truth Problem,https://www.reddit.com/r/MachineLearning/comments/51xfl5/unsupervised_regression_ground_truth_problem/,ccaoss,1473428427,"Unsupervised Electrochemical Gas Sensor Problem

up vote
0
down vote
favorite
I've been set the audacious task of deriving a formula for a set of electrochemical gas sensors! My mathematical background is limited, and I've recently acquired ""Machine Learning, a Probabilistic Perspective"" by Kevin Murphy, and been told to investigate caffenet. However, I'm finding the book extremely difficult to work through, as I have no idea what I'm doing, nor exactly which algorithms/approaches are suitable for this task.

The company I work for make various environmental gas sensors testing gas concentrations such as NO, CO etc. and my boss thinks that it may be possible to improve their accuracy using Machine Learning techniques. Yep, I'm a bit of a newcomer to this, and I'm not sure if what he is asking is even possible, because we don't even have the ground truth yet!

He has provided me with datasets- a lot of datasets- for the gas sensors. However, there is no reference data. So I have no idea if the sensors or performing accurately or not. We have no statistical model at the minute, all we have is an output in millvolts, that seems to be buffered around by changes in temperature and humidity.

Without any reference data, this seems to be some sort of ""unsupervised regression"" problem, or so it seems. Is what he is asking even possible? Given the amount of data I have, I feel I can do something...

How do I find the ground truth?

",2,3
303,2016-9-9,2016,9,9,22,51xhy8,Long Short-Term Memory RNN for Python Code Suggestions,https://www.reddit.com/r/MachineLearning/comments/51xhy8/long_shortterm_memory_rnn_for_python_code/,5ives,1473429248,,2,7
304,2016-9-9,2016,9,9,23,51xnmf,"Single notebook VAE-GAN hybrid tutorial/demo. Multi-gpu, latent space algebra, spike-triggered avg. style receptive fields, etc.",https://www.reddit.com/r/MachineLearning/comments/51xnmf/single_notebook_vaegan_hybrid_tutorialdemo/,timburg,1473431100,,17,36
305,2016-9-9,2016,9,9,23,51xpxr,programming level,https://www.reddit.com/r/MachineLearning/comments/51xpxr/programming_level/,jonsalji3,1473431905,"Hi, newbie here. I've read some books on neural network and learn some of the basic stuff about them. Im really intrested with it's capabilities especially with what the researchers nowdays are doing to solve problems, recognition, image classifiers with GPUs and stuff. I just want to ask, I only know how to program basic stuff (learnt C and C++ basics before) and from most of the posts that I read here and some of the books that I saw selling at amazon uses python as a tool to do ML. How good do I have to be in python programmin if I want to further my knowledge and do ML? Do I really have to learn python from A-Z (syntax, data structures,etc..) or the implementation of ML in python doesn't use the whole complicated syntax and pretty straight forward? Basicly is having basic to intermediate knowledge of python is sufficient for me in ML?",4,0
306,2016-9-9,2016,9,9,23,51xs8e,Analyzing The Papers Behind Facebook's Computer Vision Approach,https://www.reddit.com/r/MachineLearning/comments/51xs8e/analyzing_the_papers_behind_facebooks_computer/,adeshpande3,1473432692,,0,14
307,2016-9-9,2016,9,9,23,51xtia,Automatic transliteration of romanized text with bidirectional LSTMs,https://www.reddit.com/r/MachineLearning/comments/51xtia/automatic_transliteration_of_romanized_text_with/,HrantKhachatrian,1473433129,,0,8
308,2016-9-10,2016,9,10,0,51xyuo,"Thematic Program on Statistical Inference, Learning, and Models for Big Data [VIDEOS]",https://www.reddit.com/r/MachineLearning/comments/51xyuo/thematic_program_on_statistical_inference/,[deleted],1473434873,[deleted],0,0
309,2016-9-10,2016,9,10,0,51y1y2,The Fundamentals of Pneumatic Pump Systems,https://www.reddit.com/r/MachineLearning/comments/51y1y2/the_fundamentals_of_pneumatic_pump_systems/,jackerfrinandis,1473435881,,0,1
310,2016-9-10,2016,9,10,0,51y3le,Hardware Slaves to the Master Algorithm,https://www.reddit.com/r/MachineLearning/comments/51y3le/hardware_slaves_to_the_master_algorithm/,[deleted],1473436432,[deleted],3,0
311,2016-9-10,2016,9,10,1,51ydcy,Bare Bottom Simplest TensorFlow Example - Fit a Line,https://www.reddit.com/r/MachineLearning/comments/51ydcy/bare_bottom_simplest_tensorflow_example_fit_a_line/,jostmey,1473439628,,12,63
312,2016-9-10,2016,9,10,1,51yeo2,Questions about deep learning?,https://www.reddit.com/r/MachineLearning/comments/51yeo2/questions_about_deep_learning/,AntixK,1473440039,"Hi, 

I am gonna give a talk on introduction to deep learning.  I wish to try to answer some elementary / basic questions that a novice in the field might have.  I also would like to point out some caveats in DL. 

So,  what questions and topics should I focus on? Suggest me some basic questions.  I shall put your references in my talk too. 

Thank you",8,0
313,2016-9-10,2016,9,10,1,51yer4,Datamining twitter to analyze the dramatic increase in radical right activity since Donald Trump's rise to be the Republican nominee,https://www.reddit.com/r/MachineLearning/comments/51yer4/datamining_twitter_to_analyze_the_dramatic/,sanity,1473440071,,2,1
314,2016-9-10,2016,9,10,1,51yf66,Movie and TV show recommendations with doc2vec embedding,https://www.reddit.com/r/MachineLearning/comments/51yf66/movie_and_tv_show_recommendations_with_doc2vec/,jfields513,1473440216,,21,59
315,2016-9-10,2016,9,10,2,51yfuw,I thought you'd be interested in this job at G2 Inc or know someone who might be a good match.,https://www.reddit.com/r/MachineLearning/comments/51yfuw/i_thought_youd_be_interested_in_this_job_at_g2/,MissionHired,1473440457,,0,1
316,2016-9-10,2016,9,10,2,51yg9f,How to make Keras+Theano use Nvidia GPUs with Optimus under Ubuntu 16.04,https://www.reddit.com/r/MachineLearning/comments/51yg9f/how_to_make_kerastheano_use_nvidia_gpus_with/,fisadev,1473440586,,6,41
317,2016-9-10,2016,9,10,3,51yv2r,Maximize over output of neural network?,https://www.reddit.com/r/MachineLearning/comments/51yv2r/maximize_over_output_of_neural_network/,bagelorder,1473445349,"In reinforcement learning I have to map a state/action pair to a value and then choose the state/action pair with the highest value. I use the NN for the mapping. 

What is the normal way to maximize over the outputs of a trained neural network? Is this even feasible?",2,2
318,2016-9-10,2016,9,10,3,51yylh,How to perform parameter updates later in Theano?,https://www.reddit.com/r/MachineLearning/comments/51yylh/how_to_perform_parameter_updates_later_in_theano/,[deleted],1473446471,[deleted],3,0
319,2016-9-10,2016,9,10,4,51z6c2,"What algorithms should I research if I want to detect deer, and estimate their age in game-camera photos",https://www.reddit.com/r/MachineLearning/comments/51z6c2/what_algorithms_should_i_research_if_i_want_to/,Blix-,1473449046,"Example of a typical game camera photo: http://www.deeranddeerhunting.com/wp-content/uploads/Deer-Buck-Stealth-Cam-11.png

I'd like to be able to detect deer, highlight them, and give an estimate of their age.

What research path should I follow?",25,1
320,2016-9-10,2016,9,10,5,51zf8c,Physicists have discovered what makes neural networks so extraordinarily powerful,https://www.reddit.com/r/MachineLearning/comments/51zf8c/physicists_have_discovered_what_makes_neural/,[deleted],1473451996,[deleted],5,0
321,2016-9-10,2016,9,10,5,51zm80,Large-scale Product Recommendation at Criteo,https://www.reddit.com/r/MachineLearning/comments/51zm80/largescale_product_recommendation_at_criteo/,Agagla,1473454389,,2,4
322,2016-9-10,2016,9,10,7,520180,BSc Computer Science Final Year Project - Forecasting stock market usng ANN?,https://www.reddit.com/r/MachineLearning/comments/520180/bsc_computer_science_final_year_project/,[deleted],1473459483,[removed],0,1
323,2016-9-10,2016,9,10,7,5201v1,Abnormal Encephalization in the Age of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/5201v1/abnormal_encephalization_in_the_age_of_machine/,pilooch,1473459710,,2,0
324,2016-9-10,2016,9,10,7,5202w9,Sequences With Sentences - a RNN for handling combined measurement and text data at each time step,https://www.reddit.com/r/MachineLearning/comments/5202w9/sequences_with_sentences_a_rnn_for_handling/,[deleted],1473460065,[deleted],2,2
325,2016-9-10,2016,9,10,11,5216fp,Multivariate MDN Implementation and visualization in Theano,https://www.reddit.com/r/MachineLearning/comments/5216fp/multivariate_mdn_implementation_and_visualization/,mujjingun,1473475635,,1,5
326,2016-9-10,2016,9,10,12,5219fd,Build a TensorFlow Image Classifier in 5 Min,https://www.reddit.com/r/MachineLearning/comments/5219fd/build_a_tensorflow_image_classifier_in_5_min/,llSourcell,1473476968,,21,0
327,2016-9-10,2016,9,10,13,521jmt,Nueral networks explained.,https://www.reddit.com/r/MachineLearning/comments/521jmt/nueral_networks_explained/,Spar_tan_07,1473481756,,3,0
328,2016-9-10,2016,9,10,15,521w8k,The Mathematics of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/521w8k/the_mathematics_of_machine_learning/,muoro,1473488419,[removed],2,0
329,2016-9-10,2016,9,10,15,521zau,Helping using AWS Virtual Machine,https://www.reddit.com/r/MachineLearning/comments/521zau/helping_using_aws_virtual_machine/,popyocherry,1473490196,"Hey everyone, my little brother has been doing a lot of neural network projects and I see him having to set his macbook aside for 10+hrs at a time to run the programs that he codes. 

I wanted to get him some AWS credit for a virtual machine on his birthday and was wondering if there was a GPU heavy configuration on AWS that people recommended! 

Also - is there any documentation on how exactly to use a virtual machine to run programs? I know he tried doing it in the past and was unable to figure out how to actually run what he wrote! 

Thanks so much in advance!",5,0
330,2016-9-10,2016,9,10,16,52228n,A visualization of the optimization of a generative neural net for human faces (256x256),https://www.reddit.com/r/MachineLearning/comments/52228n/a_visualization_of_the_optimization_of_a/,whereissarahconner,1473491964,,4,4
331,2016-9-10,2016,9,10,17,5226nl,Would this be a viable method for Google to implement WaveNet short-term?,https://www.reddit.com/r/MachineLearning/comments/5226nl/would_this_be_a_viable_method_for_google_to/,jd_3d,1473494716,"I was very impressed with the audio samples from WaveNet, but disappointed to hear that it takes over an hour to generate 1 sec of audio. Then it got me thinking, there are about 1 billion English words, but to cover 99% of daily conversation you probably need only maybe 100,000,000 or less (complete guess!). What if Google used their massive computer resources to process and store the waveforms for one hundred million words. At half a second per word, that would be 50 million CPU hours. Since that could be done entirely parallel, it might only take a few days.

Then have a lightweight algorithm that would weave together the raw ""word"" waveforms into any sentence and just use the old CTTS methods for any words not in the database. Then this could be used in real-time in Android/Google Now and other areas. Is this feasible, or would this approach not match the quality of weaving together full sentences at once like they did in the paper?",11,1
332,2016-9-10,2016,9,10,17,52288s,Update parameters with a delay in Theano?,https://www.reddit.com/r/MachineLearning/comments/52288s/update_parameters_with_a_delay_in_theano/,bagelorder,1473495722,"I already asked a similar question yesterday I admit and the answer was read the 101 tutorial. I read through the basic tutorials now (and the examples creating a logistic regression and an MLP) and all they always do is perform the updates immediately after they execute the function. The just pass the updates to be made directly to the function.

Is there a way to get the output of the function, then first do some stuff with that output (I need to wait for some external feedback and can't just write down a cost formula since I am doing reinforcement learning) and then perform some kind of update?

Am I right that the only way to do this is to define a function which calculates my needed output and another function which then calculates the cost later and use the second function to also make updates due to the way backpropagation works? (because I have to evaluate all the corresponding nodes either way if I then want to backprop through them)

If I am really asking a very basic question and understanding something wrong it would be very nice if you could point me to which resources I should be reading.

Have a nice weekend!
",1,0
333,2016-9-10,2016,9,10,18,522caw,STAMAC - Sheet Metal Machinery,https://www.reddit.com/r/MachineLearning/comments/522caw/stamac_sheet_metal_machinery/,stamac,1473498411,[removed],0,0
334,2016-9-10,2016,9,10,18,522grf,Any experiences analyzing if there are voices in sound file?,https://www.reddit.com/r/MachineLearning/comments/522grf/any_experiences_analyzing_if_there_are_voices_in/,rekner,1473501295,"Hey guys. I'm pretty new to this thing but I follow it constantly. With the advent of open source API's for machine learning I'm seeing if someone has any experience analyzing sound files, more specifically songs.

I want to get the intervals of the song where there's someone's singing. Doesn't matter what they say. Like is there a way to analyze specific frequencies in the song?

Can I do something on TensorFlow. I know it handles image files, don't know how it'll be with mp3

Thanks ",6,2
335,2016-9-10,2016,9,10,19,522hs7,Maths behind Machine Learning,https://www.reddit.com/r/MachineLearning/comments/522hs7/maths_behind_machine_learning/,Yovyom,1473501962,[removed],0,1
336,2016-9-10,2016,9,10,19,522kp5,What is/are the most Interesting time series dataset(s) for supervised or unsupervised learning ?,https://www.reddit.com/r/MachineLearning/comments/522kp5/what_isare_the_most_interesting_time_series/,compsens,1473503856,,17,40
337,2016-9-10,2016,9,10,21,522x7s,Plastic Injection Molding Services | Noble Precision,https://www.reddit.com/r/MachineLearning/comments/522x7s/plastic_injection_molding_services_noble_precision/,Michealchen,1473511066,[removed],0,1
338,2016-9-10,2016,9,10,21,522yim,Augmented reality makes prototyping easy on Shapers Origin CNC machine | TechCrunch,https://www.reddit.com/r/MachineLearning/comments/522yim/augmented_reality_makes_prototyping_easy_on/,Michealchen,1473511679,[removed],0,1
339,2016-9-10,2016,9,10,21,522z18,The Extraordinary Link Between Deep Neural Networks and the Nature of the Universe,https://www.reddit.com/r/MachineLearning/comments/522z18/the_extraordinary_link_between_deep_neural/,vitakraft,1473511965,,3,0
340,2016-9-10,2016,9,10,21,522zly,"Affordable, Open Source, 3D Printable CNC Machine is Now on Kickstarter | 3DPrint.com",https://www.reddit.com/r/MachineLearning/comments/522zly/affordable_open_source_3d_printable_cnc_machine/,Michealchen,1473512295,[removed],0,1
341,2016-9-11,2016,9,11,0,523p7l,Where can I find a dataset for emotion detection from voice?,https://www.reddit.com/r/MachineLearning/comments/523p7l/where_can_i_find_a_dataset_for_emotion_detection/,gabegabe6,1473522821,I would like to create an application where we can record our voice an it tells our emotion.,4,19
342,2016-9-11,2016,9,11,1,523rx3,Why was schmidhuber heckling Yann lecun and Yoshua Bengio in the their combined deep learning tutorial at NIPS 2015?,https://www.reddit.com/r/MachineLearning/comments/523rx3/why_was_schmidhuber_heckling_yann_lecun_and/,[deleted],1473523821,[deleted],3,0
343,2016-9-11,2016,9,11,2,5243j4,Does anyone have any idea when Geoffrey Hinton's Neural Networks Coursera course will open?,https://www.reddit.com/r/MachineLearning/comments/5243j4/does_anyone_have_any_idea_when_geoffrey_hintons/,nacdog,1473528105,"I'm just curious. It says it is starting September 2016, but it hasn't become available yet.",39,79
344,2016-9-11,2016,9,11,2,5244b4,pomegranate v0.6.0 released: probabilistic modelling for python (crosspost from r/python),https://www.reddit.com/r/MachineLearning/comments/5244b4/pomegranate_v060_released_probabilistic_modelling/,ants_rock,1473528395,"Hello everybody!

I've been working hard on improvements to [pomegranate](https://github.com/jmschrei/pomegranate), which now currently supports basic distributions, general mixture models, naive bayes, markov chains, hidden markov models, and bayesian networks. I recently gave a talk at PyData about pomegranate so I figured now is a good time to release a new version which focuses the following four things:

**stacking**: several models in pomegranate can now be stacked within each other, allowing you to do things like make mixtures of hidden Markov models, a Naive Bayes estimator using Bayesian networks or mixture models, or hidden Markov models with GMM emissions. 

**speed**: I have recently implemented a BLAS backend for some of the biggest bottlenecks. This has made pomegranate both faster and scale better than sklearn for multivariate Gaussian mixture models and Naive Bayes. 

**parallelization**: pomegranate now has built-in functions for parallelization so you don't have to think about it at all. Since the backend for pomegranate is cython with the GIL released, these functions use joblib's multithreading parallellization, not multiprocessing, making them pretty efficient. Check out [this tutorial](https://github.com/jmschrei/pomegranate/blob/master/tutorials/Tutorial_7_Parallelization.ipynb) showing how easy it is to train increasingly complicated models using these functions.

**out-of-core**: pomegranate has an out-of-core API through the use of ""summarize"" and ""from_summaries"", which all models (and model stacks) support. This allows you to either do online updates if you have a stream, or train on an amount of data which can't fit in memory. 

 [My PyData talk](https://github.com/jmschrei/pomegranate/blob/master/tutorials/PyData_2016_Chicago_Tutorial.ipynb) has a complete tutorial on all the cool stuff I've added!

I'd love to get feedback on it. You can currently pip install it with `pip install pomegranate` to get the newest version, and [visit the webpage](http://pomegranate.readthedocs.io/en/latest/) (currently in progress) to see more notes. Check out the tutorials folder for more in each of the models. 

I'm planning on focusing my attention on Bayesian networks next, which need a lot of love. Particularly, I'd like to add in both linear gaussian and hybrid networks, which seem to be in demand. If you have suggestions for the path I should take, please let me know!",7,29
345,2016-9-11,2016,9,11,2,5247f2,Parser for IMDB/Yelp dataset,https://www.reddit.com/r/MachineLearning/comments/5247f2/parser_for_imdbyelp_dataset/,Pieranha,1473529521,"Which parser is the best for online reviews like the IMDB and Yelp datasets? I'm specifically looking to split each review into separate sentences.

It's not trivial due to lack of capitalization, weird punctuation usage etc.",0,0
346,2016-9-11,2016,9,11,4,524nyw,Why does everyone put a softmax or dense layer on the output of an lstm?,https://www.reddit.com/r/MachineLearning/comments/524nyw/why_does_everyone_put_a_softmax_or_dense_layer_on/,waxxballs,1473535280,"I have a 3 layer lstm, and the loss + validation loss goes up when I put a softmax or dense on the output.",6,0
347,2016-9-11,2016,9,11,5,5254r9,Neural nets - learning with total gradient rather than stochastic gradients?,https://www.reddit.com/r/MachineLearning/comments/5254r9/neural_nets_learning_with_total_gradient_rather/,Professional_123,1473541145,"So, the total loss function is the sum of all gradients for each sample (say, grad_i, for the i-th sample)

Stochastic gradient descent adjusts weights proportional to grad_i.

Why can't I use the total gradient sum(grad_i) over all samples?",21,0
348,2016-9-11,2016,9,11,6,5258b1,Is this a possible and practical way to create artificial human-like intelligence?,https://www.reddit.com/r/MachineLearning/comments/5258b1/is_this_a_possible_and_practical_way_to_create/,liorde,1473542402,[removed],2,0
349,2016-9-11,2016,9,11,7,525ljp,Intuition on Artistic style transfer.,https://www.reddit.com/r/MachineLearning/comments/525ljp/intuition_on_artistic_style_transfer/,thenerdstation,1473547368,[removed],2,8
350,2016-9-11,2016,9,11,8,525pp7,Help to understand Markov Chain Monte Carlo,https://www.reddit.com/r/MachineLearning/comments/525pp7/help_to_understand_markov_chain_monte_carlo/,niujin,1473548985,"Hi,

I have worked with convolutional neural networks and believe I could explain relatively succinctly what they do and what they're good for. I would like to gain a similar basic understanding of Markov Chain Monte Carlo. Maybe someone with hands on experience in the field can provide an explanation.

# Analogy: basic explanation of convolutional neural networks

For example, here's my explanation of what convolutional neural networks do:

You want to classify an image into cat or dog. A convolutional neural network takes the pixel values and repeatedly multiplies them by some parameters and does other operations on them and then outputs two values like [0, 1] or [0.98, 0.02] indicating the probabilities of being a cat or a dog.

Where do the parameters for this model come from?
You can obtain the parameters by setting them to random initial values, taking a training set of cat and dog images, passing them through the network, and repeatedly improving the parameters until the cat images' outputs are as close as possible to [1, 0] and the dog images give an output as close as possible to [0, 1].

How do you know what changes to make to the parameters to improve the model? You calculate a number called the loss function. The loss function says how far the cat and dog images' respective outputs are from where they should be. The neural network training process repeatedly adjusts the parameters in order to reduce the loss function and make the network output closer to its correct values for the training images. If the network is designed well this generalises to cat and dog images that haven't been seen in training.

# Equivalent for MCMC???

I am studying Markov Chain Monte Carlo and would like to arrive at a similarly simple explanation. So I'd like to know what kind of simple but real world application they're good for, what they do, how they work, how they are trained, etc.

I can find some explanations online and in textbooks but they are very theoretical. 

So what would be the simplest possible use case that would actually occur in the real world, analogous to my cat/dog classification example? And how would you go about running MCMC? Can I do it in Excel? Python? Java? Do I need software libraries or can I do it myself? I would like a ""like I'm five"" explanation if you will but assuming I already am familiar with programming and the basic concepts of machine learning such as linear regression etc.

I guess one thing you would use would be to understand weather patterns? But how? What information is it giving you? What data does it receive and what information do you expect MCMC to give you in return? What is MCMC doing that other techniques can't?

Thanks a lot!",17,18
351,2016-9-11,2016,9,11,9,525zg9,What is up with Kaggle leaderboard lying?,https://www.reddit.com/r/MachineLearning/comments/525zg9/what_is_up_with_kaggle_leaderboard_lying/,[deleted],1473552912,[deleted],2,0
352,2016-9-11,2016,9,11,11,526jld,Question regarding Hoeffding's Inequality and learning theory.,https://www.reddit.com/r/MachineLearning/comments/526jld/question_regarding_hoeffdings_inequality_and/,wardroton,1473561192,"Hi, 

I am trying to understand the Hoeffding's Inequality and its implication on learning being feasible. I am using this lecture as a reference point: 
 https://www.youtube.com/watch?v=MEG35RDD7RA&amp;index=2&amp;list=PLD63A284B7615313A

From what I understand, if suppose you have some sample space and some samples out of it, the hypothesis verification on the samples implies the hypothesis verification on the whole sample space. This is from the  Hoeffding's Inequality. In a way you can verify any hypothesis. However to move from the verification to learning, as the prof points out in the last 5 min of the video, we need to use the modified Hoeffding's Inequality, which basically takes into the account multiple hypothesis. 

First of all, is the above thought process correct? 
Secondly, how is choosing the best hypothesis in a set of hypothesis lead to learning efficiently. What I mean by that is maybe the
hypothesis class cannot even learn the target function. 
Also, suppose we choose a hypothesis class than learn the target function, doesn't then the M in the equation (modified Hoeffding's Inequality) become infinite? 
Also does the Hoeffding's Inequality imply that, if we use any probability distribution to take a sample from the sample space, the hypothesis verfication on the sample will still represent the hypothesis verification on the sample space as long as Number of samples is sufficiently large?

Thanks ",3,2
353,2016-9-11,2016,9,11,12,526si1,Making use of speech audio and accompanying text when the alignment resolution is very poor.,https://www.reddit.com/r/MachineLearning/comments/526si1/making_use_of_speech_audio_and_accompanying_text/,[deleted],1473565030,[deleted],1,1
354,2016-9-11,2016,9,11,13,526zyo,How do you see ML impacting Software defined networks?,https://www.reddit.com/r/MachineLearning/comments/526zyo/how_do_you_see_ml_impacting_software_defined/,strunberg,1473568294,,2,0
355,2016-9-11,2016,9,11,15,527awy,"Hello guys, where can i find a good starting point?",https://www.reddit.com/r/MachineLearning/comments/527awy/hello_guys_where_can_i_find_a_good_starting_point/,nothingasofyet,1473573796,[removed],1,1
356,2016-9-11,2016,9,11,15,527cyr,Learning Theory Videos. Looks interesting,https://www.reddit.com/r/MachineLearning/comments/527cyr/learning_theory_videos_looks_interesting/,wardroton,1473574972,,6,10
357,2016-9-11,2016,9,11,15,527dcr,Tips and Tricks for Tuning CNNs,https://www.reddit.com/r/MachineLearning/comments/527dcr/tips_and_tricks_for_tuning_cnns/,stenlix,1473575210,"As the title states, I'm looking to tune my CNN for the MNIST dataset and was wondering if anyone could give some tips on how to do so. ",1,2
358,2016-9-11,2016,9,11,17,527nu8,"I have one idea about DNN modeling, what do you think about?",https://www.reddit.com/r/MachineLearning/comments/527nu8/i_have_one_idea_about_dnn_modeling_what_do_you/,xmvlad,1473582113,"As you know, activation for single neuron(at least in some simplified form) is y = sign(x0w0+x1w1+.. +xnwn), or if we constrain x0..xn and w0..wn to {+1,-1} we can get it as, y = majority(x0w0,..,xnwn). 

If we take majority with arity = 3, maj(x, y, z) = &lt;x, y, z&gt; than we can view this as instance of https://en.wikipedia.org/wiki/Median_algebra. If we take + as XOR(and this is GF(2) math), than we can model two layers of DNN as: y0= w0+&lt;c0+&lt;x0+a0, x1+a1, x2+a2&gt;, c1+&lt;x1+a3, x2+a4, x3+a5&gt;, c2+&lt;x2+a6, x3+a7, x4+a8&gt;&gt;.

After doing some simple math it easy to see, that w+&lt;x,y,z&gt; = &lt;w+x,w+y,w+z&gt;. So we can construct some fancy deep structure from this median algebra and push all coefficients to bottom, and get something like: y0 = &lt;&lt;w0+c0+x0+a0, w0+c0+x1+a1, w0+c0+x2+a2&gt;, &lt;w0+c1+x1+a3, w0+c1+x2+a4,w0+c1+x3+a5&gt;, &lt;w0+c2+x2+a6, w0+c2+x3+a7, w0+c2+x4+a8&gt;&gt;.

And you can google ""binary networks"" for some research, that show near the same representation power of binary DNN compared to real. What do you think about such type of model?

PS: With good formula formating http://mathb.in/81003

PSS: Another thought i'm got that at bottom we always have parity functions, i.e. it seems very close to boolean harmonic analysis(fourier on ({0,1}^N)-&gt;{0,1}).
",18,6
359,2016-9-11,2016,9,11,18,527txz,How can I store my trained net and Use for Android application?,https://www.reddit.com/r/MachineLearning/comments/527txz/how_can_i_store_my_trained_net_and_use_for/,gabegabe6,1473586395,"So I just started using Tensorflow.

[This is what I created from a tutorial](https://gist.github.com/gaborvecsei/4c4625bd10d070fcde29289550030946)

I would like to have an Android application with a few text boxes where I can type in values and the app would classify it based on my trained net (what I created in my PC)

How can I do this?",3,2
360,2016-9-11,2016,9,11,19,527yoi,Machine Learning in a Year - From total noob to using it at work,https://www.reddit.com/r/MachineLearning/comments/527yoi/machine_learning_in_a_year_from_total_noob_to/,mrborgen86,1473589687,,43,364
361,2016-9-11,2016,9,11,19,527zme,"[Ask ML] How many hours sleep do you get, what time do you stop working?",https://www.reddit.com/r/MachineLearning/comments/527zme/ask_ml_how_many_hours_sleep_do_you_get_what_time/,j_lyf,1473590325,[removed],11,0
362,2016-9-11,2016,9,11,20,5285ra,Physicists have discovered what makes neural networks so extraordinarily powerful,https://www.reddit.com/r/MachineLearning/comments/5285ra/physicists_have_discovered_what_makes_neural/,t_broad,1473594369,,31,0
363,2016-9-11,2016,9,11,21,52894d,Iris classification with Keras,https://www.reddit.com/r/MachineLearning/comments/52894d/iris_classification_with_keras/,gabegabe6,1473596222,"[Here you can see, how I did it](https://gist.github.com/gaborvecsei/49029cc8133805e38f826028b9ba715b)

My problem is... that the accuracy is around 73% and that's pretty bad.

How can I make it better? (I just started learning Deep Learning)
How could I make it much more better?",12,0
364,2016-9-11,2016,9,11,22,528hr8,"Case study: Electric power load forecastinga comparison of SVM, harmonic regression with ARMA and Deep Learning [OC]",https://www.reddit.com/r/MachineLearning/comments/528hr8/case_study_electric_power_load_forecastinga/,Liorithiel,1473600467,,14,19
365,2016-9-12,2016,9,12,0,528xtb,Former psychologist here. Does machine learning can be capable of interpreting score of cognitive tests ?,https://www.reddit.com/r/MachineLearning/comments/528xtb/former_psychologist_here_does_machine_learning/,Nrscientist,1473606910,[removed],12,0
366,2016-9-12,2016,9,12,1,529a3n,Laptop for machine learning,https://www.reddit.com/r/MachineLearning/comments/529a3n/laptop_for_machine_learning/,huyhcmut,1473611165,[removed],13,0
367,2016-9-12,2016,9,12,3,529s7v,Let's make an AI that can string together dolphin-talk :D,https://www.reddit.com/r/MachineLearning/comments/529s7v/lets_make_an_ai_that_can_string_together/,paswut,1473617205,,0,0
368,2016-9-12,2016,9,12,4,52a2r6,Deep Learning survey: TensorFlow vs Caffe vs Theano vs Torch,https://www.reddit.com/r/MachineLearning/comments/52a2r6/deep_learning_survey_tensorflow_vs_caffe_vs/,[deleted],1473620647,[deleted],2,0
369,2016-9-12,2016,9,12,5,52alr7,How do you know when you've reached a critical point when optimizing a DNN?,https://www.reddit.com/r/MachineLearning/comments/52alr7/how_do_you_know_when_youve_reached_a_critical/,darkconfidantislife,1473626877,"So I've been trying out some different optimizers (SGD, NAG, Adadelta, etc.) and there's one thing I can't quite figure out:

How do you know when you've reached a critical point (be it a local min, local max or saddle point, idc)? Just any critical point in general. Is it just when the gradient becomes zero or approaches zero?

Thanks in advance, I feel like I'm overthinking this and I'm asking a stupid quiestion xD",14,1
370,2016-9-12,2016,9,12,6,52aoxu,How would you solve this NN + video game problem?,https://www.reddit.com/r/MachineLearning/comments/52aoxu/how_would_you_solve_this_nn_video_game_problem/,IyuTifer,1473627971,"I'm trying to get a neural net to do image recognition and then send commands to a web server. This web server is communicating with a Unity game and controlling a red box towards a blue ball. The input is the image, the output is simply two numbers which can range from -1 to 1. The first number tells the red box to go left. The second number, if 1, tells the box to go right. This has worked a little so the neural net is actually moving pieces on the game. Great!


My problem is, the net is wrong 99% of the time. I'm taking images like [this image](http://imgur.com/a/6Bgf1) and converts them to small 28x28 (like the mist dataset, which my net got a 97% in.) images like [this image.](http://imgur.com/a/fhcX3)


I've programmed my net to have a logistic or hyperbolic tangent function. I've tried many topologies, epochs and learning rates.
Does anyone have any suggestions for my program? What type of topology should I be using? Am I simplifying the image too much? How much training data would I need in a problem like this? Any help would be greatly appreciated.",8,2
371,2016-9-12,2016,9,12,6,52arxj,[X-post] Sigmoids are bad because outputs are always positive... but so are ReLUs...,https://www.reddit.com/r/MachineLearning/comments/52arxj/xpost_sigmoids_are_bad_because_outputs_are_always/,Ayakalam,1473628972,"Hi,
In this part of the class notes [here](http://imgur.com/a/K136d), we are told that one of the problems of the sigmoid activation is that the outputs are always positive, and so therefore the dynamics of gradient descent suffer because the update on the weights will be either always positive, or always negative, (the negaitve coming from dL / df).

This definitely makes sense - but my question is - wouldn't the ReLU ALSO suffer from this same problem? The output of a ReLU unit is also always positive. So why isn't this mentioned as a con of ReLU, as it is for a sigmoid?

Thanks.

**EDIT:**

Guys... I get that there are many advantages of ReLUs over the Sigmoid. But my question is really *uber uber* specific about one aspect of the two functions: Let me repeat: It is pointed out that *one of the bad aspects* of sigmoids, is that it's output is always positive. This makes the gradient zig-zag. (I get this part). What I *dont get*, is why this isn't ALSO counted against ReLU, since it's outputs are always positive as well! Thanks...",33,18
372,2016-9-12,2016,9,12,6,52askq,"Beginner at ML , want to tinker with image recognition and neural networks.",https://www.reddit.com/r/MachineLearning/comments/52askq/beginner_at_ml_want_to_tinker_with_image/,harvey_slash,1473629190,"I have been reading about neural networks/deep learning over the past few days. 
I would like to ask if there are GUI tools that I can feed images into, and get results. 
I want to experiment with two things: 
First, drag and drop for nodes/layers etc. 
2ndly, I have read everywhere that deep learning models 'learn' features , like edges, and create a hierarchy as you go to a deeper layer. I would like to be able to visualise these 'features'. 
I tried using tensor flow playground, but it doesnt show images data. 
I also used neuroph, and there doesn't seem to be any way to see what each node is trying to detect(also, its pretty buggy on a mac ). 
Are there simple, GUI options for me so that I can tinker around a bit, gain a little confidence , while reading about machine learning ?",5,3
373,2016-9-12,2016,9,12,9,52bq71,Deep Learning: Keras Short Video Tutorial,https://www.reddit.com/r/MachineLearning/comments/52bq71/deep_learning_keras_short_video_tutorial/,[deleted],1473641507,[deleted],0,12
374,2016-9-12,2016,9,12,9,52bqfy,Question about time series classification with known states,https://www.reddit.com/r/MachineLearning/comments/52bqfy/question_about_time_series_classification_with/,Cjh411,1473641612,"I'm trying to solve a time series classification and having trouble understanding what method would be appropriate. Based on what I've found this far I have two issues 

* When using a hidden markov model it's going to estimate the states for classification. However with my states being known I don't need that. I haven't seen anything thus far about using a regular markov chain, but is that an option? Good Python packages?

* Other types of models extract features from a time series and classify using other techniques. It seems like to do that effectively you'd need to define the length of your time series. I've already been able to do this, and now need a way to identify when a time series is in a certain state without defining a time window. 


Are there ways around those issues or am I possibly misunderstanding? Also are there any techniques here I'm missing?

",8,1
375,2016-9-12,2016,9,12,12,52cera,Neural Networks and Video Game Learning - FREVO/Sticky Creatures,https://www.reddit.com/r/MachineLearning/comments/52cera/neural_networks_and_video_game_learning/,sciencespacemath,1473650958,,0,0
376,2016-9-12,2016,9,12,12,52cf3c,"In Bayesian philosophy, the parameter(weights) is assumed to have an underlying distribution, parameterized by a hyper-parameter"". If I choose Gaussian distribution for the weights of my model, then hyperparameters will be mean and sd of the gaussian distribution?",https://www.reddit.com/r/MachineLearning/comments/52cf3c/in_bayesian_philosophy_the_parameterweights_is/,[deleted],1473651097,[removed],0,1
377,2016-9-12,2016,9,12,12,52cgls,"There's a line in the notes ""In Bayesian philosophy, the parameter(weights) is assumed to have an underlying distribution, parameterized by a hyper-parameter"". If I choose Gaussian distribution for the weights of my model, then hyperparameters will be mean and sd of the gaussian distribution?",https://www.reddit.com/r/MachineLearning/comments/52cgls/theres_a_line_in_the_notes_in_bayesian_philosophy/,Mr__Christian_Grey,1473651708,,0,1
378,2016-9-12,2016,9,12,12,52cisr,[Video] Will Machine Intelligence Kill Us?,https://www.reddit.com/r/MachineLearning/comments/52cisr/video_will_machine_intelligence_kill_us/,Menilik,1473652657,,1,0
379,2016-9-12,2016,9,12,13,52ckr0,Has anyone implemented a deep learning solution to quality control in a factory line.,https://www.reddit.com/r/MachineLearning/comments/52ckr0/has_anyone_implemented_a_deep_learning_solution/,Xodast,1473653543,[removed],6,3
380,2016-9-12,2016,9,12,13,52cngj,Melbourne-University AES-MathWorks-NIH Seizure Prediction Challenge,https://www.reddit.com/r/MachineLearning/comments/52cngj/melbourneuniversity_aesmathworksnih_seizure/,[deleted],1473654880,[removed],0,1
381,2016-9-12,2016,9,12,13,52cq74,[1609.02200] Discrete Variational Autoencoders,https://www.reddit.com/r/MachineLearning/comments/52cq74/160902200_discrete_variational_autoencoders/,barmaley_exe,1473656251,,6,38
382,2016-9-12,2016,9,12,14,52ctqm,Melbourne-University AES-MathWorks-NIH Seizure Prediction Challenge,https://www.reddit.com/r/MachineLearning/comments/52ctqm/melbourneuniversity_aesmathworksnih_seizure/,levink,1473658129,"After much anticipation the Melbourne-University AES-MathWorks-NIH Seizure Prediction Challenge has launched on Kaggle.com!
 
Enter for your chance to win part of the US$20,000 prize pool and test your data science skills against the one-of-a-kind long-term human intracranial EEG database from the world-first human clinical trial of the NeuroVista Seizure Advisory System that was co-ordinated by the University of Melbourne. This device was implanted in the heads of epilepsy patients to record brain activity over a period of up to 3 years. Typical recordings of intracranial EEG in humans only last up to two weeks and do not provide enough data to allow accurate evaluation of seizure prediction algorithms because often only a handful of seizures can be collected over two weeks. The durations of data in the NeuroVista dataset overcome this problem.   
 
Analysis of the human NeuroVista dataset has indicated that seizure prediction in humans is in fact possible, however, improvements can still be achieved depending on the patient. This contest seeks to find improved methods by contributing data from 3 patients whose seizures are difficult to predict.
 
In 2014, our contest partners from the Mayo Clinic and University of Pennsylvania ran a seizure prediction contest on Kaggle.com involving long-term data from dogs, that were also implanted with the NeuroVista device, and short-term human data. The contest revealed several novel and existing approaches that performed well and now we want to know how well they can perform on long-term data from humans. Can you help us find out, or can you come up with even better algorithms?
 
Everything you need to get started is on the contest web page:
https://www.kaggle.com/c/melbourne-university-seizure-prediction
 
Be sure to get started soon as the contest ends on November 21 and the winners will be announced at the American Epilepsy Society Annual Meeting on December 5. 
 
Good luck!
 
On behalf of the organising team
University of Melbourne: Levin Kuhlmann, Mark Cook, David Grayden, Dean Freestone, Philippa Karoly
University of Pennsylvania: Brian Litt
Mayo Clinic: Greg Worrell, Ben Brinkmann
Alliance for Epilepsy Research: Susan Arthurs
And our co-sponsors 
American Epilepsy Society
MathWorks
National Institutes of Health
And Kaggle.com",0,1
383,2016-9-12,2016,9,12,14,52cuf6,where to download the news corpus for NLP?,https://www.reddit.com/r/MachineLearning/comments/52cuf6/where_to_download_the_news_corpus_for_nlp/,jdxyw,1473658485,[removed],2,0
384,2016-9-12,2016,9,12,15,52czqg,Looking For A Project: What Would Be A Urban Planning Problem/Research I Could Solve Using Machine Learning &amp; Public Data?,https://www.reddit.com/r/MachineLearning/comments/52czqg/looking_for_a_project_what_would_be_a_urban/,Synroc,1473661419,"Hi all,

Two of my current interests are urban planning &amp; machine learning, and as a result would love to work on a project that combines the two.

It would be great if I could use public data sets such as census.gov, or the various Open Data platforms that cities have (I'm in SF for example), to look into interesting for machine learning.

Any suggestions?

For example, how to figure out where to build new housing in a city, or allocate traffic flows, or prevent racial segregation.

",11,19
385,2016-9-12,2016,9,12,16,52d62o,[1609.02907] Semi-Supervised Classification with Graph Convolutional Networks,https://www.reddit.com/r/MachineLearning/comments/52d62o/160902907_semisupervised_classification_with/,[deleted],1473665151,[deleted],0,1
386,2016-9-12,2016,9,12,16,52d8ms,[1609.02907] Semi-Supervised Classification with Graph Convolutional Networks,https://www.reddit.com/r/MachineLearning/comments/52d8ms/160902907_semisupervised_classification_with/,triplefloat,1473666644,,3,54
387,2016-9-12,2016,9,12,17,52ddqg,How about the powder blending equipment package in JCT Machinery,https://www.reddit.com/r/MachineLearning/comments/52ddqg/how_about_the_powder_blending_equipment_package/,mixmachinery,1473669849,,1,1
388,2016-9-12,2016,9,12,18,52diyz,RNNs for languages - uses?,https://www.reddit.com/r/MachineLearning/comments/52diyz/rnns_for_languages_uses/,niujin,1473673086,"I have found a lot of tutorials and example code about RNNs. There is a lot of hype about their potential for language modelling. (I'm talking about text processing rather than speech recognition.)

However every article I found about their usages in text processing seems to use the same example: text generation...

* generating text that looks like Shakespeare

* generating text that looks like JK Rowling

etc... This seems quite gimmicky and not a lot of use for anything to me!

Can anyone give some real world examples of what they actually are good for within the field of text processing/computational linguistics please?",20,24
389,2016-9-12,2016,9,12,18,52djtp,Which undergraduate major is the best for a machine learning oriented student?,https://www.reddit.com/r/MachineLearning/comments/52djtp/which_undergraduate_major_is_the_best_for_a/,Edgar7878,1473673591,[removed],10,1
390,2016-9-12,2016,9,12,19,52dmwz,Is Google Brain Resident for Non-US People too?,https://www.reddit.com/r/MachineLearning/comments/52dmwz/is_google_brain_resident_for_nonus_people_too/,va0007,1473675358,"Last year I applied, I guess it was for US citizens only.  https://www.google.com/about/careers/search#!t=jo&amp;jid=147545001&amp;

And In case somebody knows about the program :). Any chance for outsiders?",2,1
391,2016-9-12,2016,9,12,20,52drsq,What is dilated convolution?,https://www.reddit.com/r/MachineLearning/comments/52drsq/what_is_dilated_convolution/,hapliniste,1473678092,"Hi, I was looking at the fresh WaveNet paper and I don't understand what is so special about ""dilated convolution"". 

To me, it looks like a stack of conv layers with a kernel size of 2 and a stride of 2.
Can someone explain to me what's new here?",11,11
392,2016-9-12,2016,9,12,20,52dtuz,Projects/papers about using ML to analyse programmed code,https://www.reddit.com/r/MachineLearning/comments/52dtuz/projectspapers_about_using_ml_to_analyse/,desku,1473679165,"Hi,

I was just wondering if anyone knew about any papers or ML applications that are about reading code, doing their ML magic and doing some analysis on that code, such as predicting if it will compile or pass tests or even predicting how 'good' the code is. 

I also wanted to know if this would fall into the realm of NLP or not. Could programming languages be analysed with RNNs the same way human languages are? Or is that just crazy talk? 

Would it be better to just use unsupervised learning to cluster similar code (i.e. all code examples that return the incorrect value would be grouped together) and then use kNN on a new example to determine which cluster it belongs to?

Thanks. ",2,7
393,2016-9-12,2016,9,12,20,52dxp9,Offering help to annotate your image dataset,https://www.reddit.com/r/MachineLearning/comments/52dxp9/offering_help_to_annotate_your_image_dataset/,fishappear,1473681113,[removed],0,2
394,2016-9-12,2016,9,12,21,52e2cp,Importance of first layer In ConvNets,https://www.reddit.com/r/MachineLearning/comments/52e2cp/importance_of_first_layer_in_convnets/,Nextpenade,1473683103,"I'm interested in the tradeoff between accurary and speed in ConvNets. Since the first layers play an important role in feature extraction and speed I try to find a reasonable architecture. The ENet paper (https://arxiv.org/abs/1606.02147) has an interesting approach by performing a strided convolution and concatenating it with maxpooling in the first layer. I still don't understand what is the benefit of the features obtained by maxpooling since it takes a significant time to compute. Is there any important image property that can be captured by maxpooling on the raw image?

In addition I question myself why they used maxpool in their downsampling blocks. Shouldn't an avgpool perform better?",10,15
395,2016-9-12,2016,9,12,22,52e7rg,New deep learning startup seeking new co-founders and collaborators.,https://www.reddit.com/r/MachineLearning/comments/52e7rg/new_deep_learning_startup_seeking_new_cofounders/,BenjaminBoyle,1473685394,[removed],13,0
396,2016-9-12,2016,9,12,22,52e83f,"BigData and ML Examples and Libraries Weekly Roundup  Sep. 12, 2016",https://www.reddit.com/r/MachineLearning/comments/52e83f/bigdata_and_ml_examples_and_libraries_weekly/,stkim1,1473685512,,0,1
397,2016-9-12,2016,9,12,23,52ehdm,How does tensorflow represent everything as dataflow graph?,https://www.reddit.com/r/MachineLearning/comments/52ehdm/how_does_tensorflow_represent_everything_as/,stacky777,1473689183,[removed],1,1
398,2016-9-12,2016,9,12,23,52eoti,"Artificial Intelligence, Deep Learning, &amp; Neural Networks Explained",https://www.reddit.com/r/MachineLearning/comments/52eoti/artificial_intelligence_deep_learning_neural/,innoarchitech,1473691744,,0,0
399,2016-9-13,2016,9,13,0,52esjf,"AI is from Venus, Machine Learning is from Mars",https://www.reddit.com/r/MachineLearning/comments/52esjf/ai_is_from_venus_machine_learning_is_from_mars/,sfccentertain,1473692960,,0,1
400,2016-9-13,2016,9,13,0,52f13u,Baidu open sources its deep learning platform PaddlePaddle,https://www.reddit.com/r/MachineLearning/comments/52f13u/baidu_open_sources_its_deep_learning_platform/,sbt_,1473695717,,31,90
401,2016-9-13,2016,9,13,0,52f1wx,Face search engine using TensorFlow,https://www.reddit.com/r/MachineLearning/comments/52f1wx/face_search_engine_using_tensorflow/,pavelgonchar,1473695997,,0,18
402,2016-9-13,2016,9,13,1,52faqx,Fighting corruption in Brazil with data science (x-post from r/datascience),https://www.reddit.com/r/MachineLearning/comments/52faqx/fighting_corruption_in_brazil_with_data_science/,datasciencebr,1473698800,,0,2
403,2016-9-13,2016,9,13,2,52fdde,Need advice on anomaly detection. (Supervised learning),https://www.reddit.com/r/MachineLearning/comments/52fdde/need_advice_on_anomaly_detection_supervised/,NerdEngineering,1473699684,"Hello fellow ML enthusiasts,

I had a quick question on a quick proof concept I am trying to develop. I want to do anomaly detection on log files, and I am now roughly done with the methods of information retrieval I am going to use (have a script that pulls the relevant info from the files) and have labeled my training sets.

I now need to get into the nuts and bolts of the ML program and am wondering if I should go with a Bayes Net or a Support Vector machine for classification. Classification is going to be simple and only 4 categories, true positive, false positive, true negative, false negative. I basically just want to be able to kick off alerts to engineers that they should investigate something when a true negative comes online.

Any suggestions on which route to take?",2,0
404,2016-9-13,2016,9,13,2,52ffsu,Can I use VAE in NLP Context?,https://www.reddit.com/r/MachineLearning/comments/52ffsu/can_i_use_vae_in_nlp_context/,brunoalano,1473700437,It's possible to use Variational Autoencoders with Word Embeddings to provide a Sentence Embedding (Latent Representation of a Sentence)?,9,4
405,2016-9-13,2016,9,13,2,52fgmc,There Would Be No Efficient Deep Learning Without High Density Memory,https://www.reddit.com/r/MachineLearning/comments/52fgmc/there_would_be_no_efficient_deep_learning_without/,[deleted],1473700695,[deleted],0,0
406,2016-9-13,2016,9,13,2,52fgoc,"Open Sourcing the model in ""Exploring the Limits of Language Modeling"" (TensorFlow)",https://www.reddit.com/r/MachineLearning/comments/52fgoc/open_sourcing_the_model_in_exploring_the_limits/,OriolVinyals,1473700707,"We just released the pretrained model (i.e., weights + code) to run our ~30 perplexity single model trained on the One Billion Word Benchmark.

Paper: https://arxiv.org/abs/1602.02410

Code: https://github.com/tensorflow/models/tree/master/lm_1b

Have fun &amp; thanks for all the emails asking for the code : )

EDIT: Training code relies quite a bit on the internals of the Google Cluster (since this model uses distributed training), so it is less trivial to release.

EDIT2: As Rafal (leading author of this work) posted in comments, a training recipe which works with the OSS TF is coming soon.",28,54
407,2016-9-13,2016,9,13,3,52fp0i,[Question] Modelling posterior vs p(x) with probability flows,https://www.reddit.com/r/MachineLearning/comments/52fp0i/question_modelling_posterior_vs_px_with/,0entr0py,1473703298,"My question is with reference to deep generative models that attempt to model log p(x) directly by applying invertible probability transformations (flows), such as NICE or Real NVP and methods that instead model the posterior p(z|x) of a lower dimensional z by similar, complex flows in an attempt to better match the true posterior (eg. Normalizing Flows, Inverse ARFs) 

i) Is there any inherent advantage to either type of model versus the other?  
ii) I've gathered that modelling p(x) is generally more difficult and requires a better approximator (a deeper network ?)  but allows for to formulate and maximize LL exactly. Is this correct ? 

  


",1,2
408,2016-9-13,2016,9,13,3,52fpn9,Favorite hardware vendor / systems integrator for GPU-heavy nodes?,https://www.reddit.com/r/MachineLearning/comments/52fpn9/favorite_hardware_vendor_systems_integrator_for/,SuperFX,1473703487,"Do people here have experiences / preferences for vendors that build GPU systems for deep learning? I'm looking to get a few nodes with four Titan Xs in each, but don't want to go through the hassle of building the system myself. This is for an academic lab so I want to keep the costs low.

Update: I don't necessarily need a Xeon-based system. A single Core i7 CPU with 128GB of RAM should be sufficient as well.",3,8
409,2016-9-13,2016,9,13,3,52fz7i,Tutorial: Running TensorFlow on AWS,https://www.reddit.com/r/MachineLearning/comments/52fz7i/tutorial_running_tensorflow_on_aws/,navoshta,1473706423,"I've compiled [a quick tutorial](http://navoshta.com/aws-tensorflow/) on configuring **TensorFlow** with **Jupyter Notebook** on a AWS GPU-enabled instance. As a bonus it features [a couple of convenience scripts](https://gist.github.com/navoshta) for firing up and shutting down your AWS machine.

I hope this will be of any help and will save some of you a bit of your time!",6,54
410,2016-9-13,2016,9,13,4,52g7uy,Thoughts on the brain and computation,https://www.reddit.com/r/MachineLearning/comments/52g7uy/thoughts_on_the_brain_and_computation/,__lava__,1473709068,,14,33
411,2016-9-13,2016,9,13,5,52gdss,How can I decode one hot vector? (and when can we use it?),https://www.reddit.com/r/MachineLearning/comments/52gdss/how_can_i_decode_one_hot_vector_and_when_can_we/,gabegabe6,1473710904,"So I made Iris classification with keras and I have 3 classes and I convert the labels to : ` [0,0,1],[0,1,0],[1,0,0] `

After I trained the net and I would like to predict someting this is my output:

    array([[  9.26771190e-06,   1.18947044e-01,   8.81043673e-01]], dtype=float32)

or

    array([[ 0.00103773,  0.97131133,  0.02765093]], dtype=float32)

Other question...When do I have to use one hot vector?",2,0
412,2016-9-13,2016,9,13,5,52gevl,Interactive outlier analysis using PCA,https://www.reddit.com/r/MachineLearning/comments/52gevl/interactive_outlier_analysis_using_pca/,michal_sustr,1473711212,,0,11
413,2016-9-13,2016,9,13,5,52ggm4,Deep network architecture for Video ?,https://www.reddit.com/r/MachineLearning/comments/52ggm4/deep_network_architecture_for_video/,vighneshbirodkar,1473711763,"What's a good architecture for detecting actions through video ? If anyone knows what is the state of the art results for UCF-101 or HMDB 51 datasets (with or without transfer learning) ? 

My ultimate goal is to train a network on one of the existing datasets and use transfer learning to adapt it to my own task.",9,2
414,2016-9-13,2016,9,13,5,52gj2u,Is there a better option than the sliding window method for object detection on images?,https://www.reddit.com/r/MachineLearning/comments/52gj2u/is_there_a_better_option_than_the_sliding_window/,mega10d0n,1473712559,,7,3
415,2016-9-13,2016,9,13,6,52gvpj,Pros/cons of different kinds of autoencoder types,https://www.reddit.com/r/MachineLearning/comments/52gvpj/proscons_of_different_kinds_of_autoencoder_types/,[deleted],1473716566,[deleted],0,1
416,2016-9-13,2016,9,13,6,52gvw4,Pros/cons of different autoencoder types,https://www.reddit.com/r/MachineLearning/comments/52gvw4/proscons_of_different_autoencoder_types/,Pieranha,1473716627,"There are a variety of different autoencoder types and so many recent papers that it's hard to keep up with. Could someone briefly explain the pros/cons of different autoencoder variants?

It would be great if you could specify separate advice for when using autoencoders to reduce the dimensionality of the data (e.g. plain autoencoders, denoising autoencoders and variational autoencoders) and for generating new samples (e.g. variational autoencoders and adversarial autoencoders).",6,2
417,2016-9-13,2016,9,13,10,52i1dp,[1609.02993] Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks,https://www.reddit.com/r/MachineLearning/comments/52i1dp/160902993_episodic_exploration_for_deep/,gambs,1473731215,,6,21
418,2016-9-13,2016,9,13,10,52i3bo,CONVOLUTIONAL NETWORKS WITH IAN GOODFELLOW LIVE STREAM,https://www.reddit.com/r/MachineLearning/comments/52i3bo/convolutional_networks_with_ian_goodfellow_live/,potato_potaro,1473731944,,5,21
419,2016-9-13,2016,9,13,12,52id0m,Logical Induction,https://www.reddit.com/r/MachineLearning/comments/52id0m/logical_induction/,LazyOptimist,1473735661,,6,35
420,2016-9-13,2016,9,13,12,52idhf,""" Energy-based Generative Adversarial Network"" (stabler GAN training)",https://www.reddit.com/r/MachineLearning/comments/52idhf/energybased_generative_adversarial_network/,gwern,1473735855,,2,13
421,2016-9-13,2016,9,13,12,52if2u,Where are the trained models?,https://www.reddit.com/r/MachineLearning/comments/52if2u/where_are_the_trained_models/,stacky777,1473736520,"In one of his lectures, Nando de Freitas says that trained deep learning models are available everywhere on the internet. Okay, how do I use them? Can those models be extended to my own sub task?

Like I get the word2vec, I want to make noun/verb predictor given the word. How can I do that? I still have to start from zero right? Of what use is the word2vec model for me?

:) Cheers.",4,2
422,2016-9-13,2016,9,13,12,52igx9,Game Theory and Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/52igx9/game_theory_and_reinforcement_learning/,ispeakdatruf,1473737303,"This question is probably going to be poorly worded, but I hope I can get my question through. Does RL provide any guarantees (bounds) in game-theoretic terms? Have there been any results on how RL can reach some equilibrium, or that it minimizes regret under certain conditions?",1,4
423,2016-9-13,2016,9,13,14,52iu3j,Using neural networks to recognize two different sounds,https://www.reddit.com/r/MachineLearning/comments/52iu3j/using_neural_networks_to_recognize_two_different/,klop2031,1473743280,"I have 2 inputs of audio and I want to differentiate them via LSTM. Can anyone point me in the right direction?

More specifically, these two sounds may be distorted in amplitude, so I will have a training set.

Also lets say I have 3 or 4 sounds, say these sounds come together as music, I want to be able to identify which ones have been played from a stream.

so: input is sounds: 13324232 and I want my output to be 1 3 3 2 4 2 3 2

Any help would be appreciated.",4,5
424,2016-9-13,2016,9,13,14,52iuas,Language independent speech to text to speech reductionist regeneration using IPA for human readability?,https://www.reddit.com/r/MachineLearning/comments/52iuas/language_independent_speech_to_text_to_speech/,Lajamerr_Mittesdine,1473743377,"I wanted to know the feasibility of this.

Above all I want a language independent solution.

I'd like to take audio input(with speech) and output a human readable format, I say IPA but that isn't complex / comprehensive enough to do what I want but let's use it as a example, then I want to take that human readable format as a input and generate audio as faithful reproduction(enough to say it sounds human like and sounds like the way the speaker said / intended it to sound like).

Ideally the human readable format would preserve things such as Intonation, Frequency, Pitch, etc. as much as possible while also being reductionist. I imagine it being a layer on top of IPA.

You'd have the sentence converted to IPA and some additional information afterwards to preserve the way it's being said.

Sorry if I'm saying incomprehensible things, I'm just a reader of ML and don't know the appropriate vernacular and terminology.

Edit: [a nvr sd i stol ma mni.](http://i.imgur.com/YQ11fqL.jpg)",0,3
425,2016-9-13,2016,9,13,14,52iwk8,Laser Technology Leads to an Incarnation of Footwear and Leather Goods,https://www.reddit.com/r/MachineLearning/comments/52iwk8/laser_technology_leads_to_an_incarnation_of/,prakashlasemachines,1473744521,,0,1
426,2016-9-13,2016,9,13,14,52izdh,New Pascal GPUs P4 and P40 Accelerate Inference in the Data Center,https://www.reddit.com/r/MachineLearning/comments/52izdh/new_pascal_gpus_p4_and_p40_accelerate_inference/,[deleted],1473745974,[deleted],0,1
427,2016-9-13,2016,9,13,15,52j0jv,NVIDIA Tesla P4 &amp; P40 - New Pascal GPUs Accelerate Inference in the Data Center,https://www.reddit.com/r/MachineLearning/comments/52j0jv/nvidia_tesla_p4_p40_new_pascal_gpus_accelerate/,Hobbes_the_tiger,1473746619,,5,15
428,2016-9-13,2016,9,13,15,52j58j,Completely Overwhelmed. Where to begin?,https://www.reddit.com/r/MachineLearning/comments/52j58j/completely_overwhelmed_where_to_begin/,TimeForAChang,1473749322,[removed],0,1
429,2016-9-13,2016,9,13,17,52jd9i,"Custom Computer Vision Technology, TechCrunch Disrupt 2016 in San Francisco.",https://www.reddit.com/r/MachineLearning/comments/52jd9i/custom_computer_vision_technology_techcrunch/,restb,1473754070,,0,0
430,2016-9-13,2016,9,13,17,52jdhu,How about paint blending tanks,https://www.reddit.com/r/MachineLearning/comments/52jdhu/how_about_paint_blending_tanks/,mixmachinery,1473754215,,1,1
431,2016-9-13,2016,9,13,17,52jhbq,What's the current status on Theano vs TensorFlow?,https://www.reddit.com/r/MachineLearning/comments/52jhbq/whats_the_current_status_on_theano_vs_tensorflow/,perceptron01,1473756561,"I haven't heard much about Theano lately, and there's a myriad of projects done with TF everywhere. Is Theano considered more or less dead then?

How would one go around convincing their research group to make the switch to TensorFlow from Theano?",21,14
432,2016-9-13,2016,9,13,18,52jjdo,"I'm a newbie to ML, confused about applications in commercial area",https://www.reddit.com/r/MachineLearning/comments/52jjdo/im_a_newbie_to_ml_confused_about_applications_in/,Laurence-Lin,1473757808,"Hello, everyone. 
I'm a newbie who is going to research ML, and now I would like to apply ML on commercial problems(such as spend, consulting, decisions making...)
Now I'm watching stanford's ML tutorial and simultaneously looking for researches of ML in this area. 
I prefer to use neural network to fulfill my goal, due to its advantage in forecasting with multiple dataset. 
I've found several thesis(but not too much),  and found some dataset source, such as UCI. 
However, I'm curious about that in current applications of ML in commercial problems, which algorithm is prefered? The latest paper I found utilizes spiking neural network, but not too much paper for this algorithm in the application otherwise. 
I've heard that convolution neural network has been popular these days, but as I briefly take a look, its advantage is dealing with image processing. 
Is there anyone researching ML applies in commercial problems?
What kind of algorithm do you prefer?
Thanks a lot!",10,0
433,2016-9-13,2016,9,13,18,52jjmi,LRN layer with GoogLeNet,https://www.reddit.com/r/MachineLearning/comments/52jjmi/lrn_layer_with_googlenet/,fulcrum_xyz,1473757952,"How important is the LRN layer with GoogLeNet ? The original network description (v1 paper) does not include this, but the caffe version seem to include this (https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet). However, the v3 (https://github.com/smichalowski/google_inception_v3_for_caffe/blob/master/train_val.prototxt) and v4 (https://arxiv.org/pdf/1602.07261.pdf) network descriptions also seem not have this ? 

Any pointers to this, would be very helpful. Thanks",1,5
434,2016-9-13,2016,9,13,18,52jo4e,Compressed Linear Algebra for Large-Scale Machine Learning [PDF],https://www.reddit.com/r/MachineLearning/comments/52jo4e/compressed_linear_algebra_for_largescale_machine/,alexeyr,1473760513,,0,3
435,2016-9-13,2016,9,13,19,52jsre,Top Algorithms Used by Data Scientists,https://www.reddit.com/r/MachineLearning/comments/52jsre/top_algorithms_used_by_data_scientists/,john_philip,1473763022,,0,1
436,2016-9-13,2016,9,13,20,52jztj,Convexified Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/52jztj/convexified_convolutional_neural_networks/,asingov,1473766759,,1,7
437,2016-9-13,2016,9,13,20,52k0fg,The enigma machine takes a quantum leap,https://www.reddit.com/r/MachineLearning/comments/52k0fg/the_enigma_machine_takes_a_quantum_leap/,SoftwareVilla1,1473767057,,0,1
438,2016-9-13,2016,9,13,21,52k3hp,xkcd: Linear Regression,https://www.reddit.com/r/MachineLearning/comments/52k3hp/xkcd_linear_regression/,timmyriddle,1473768447,,18,602
439,2016-9-13,2016,9,13,21,52k604,Online Masters Degree GATech : ML,https://www.reddit.com/r/MachineLearning/comments/52k604/online_masters_degree_gatech_ml/,Kartik21,1473769485,"I came across the option of an online masters degree specializing in ML from GA Tech. It seemed relatively cheaper and the coursework seemed to be really competing as well. 

I already have a Masters Degree from Purdue University in Industrial engineering (a non CS major). I don't know how this might affect the chances of getting an admit. Nevertheless, can someone explain the pros and cons of the program so that I can make an informed decision? Thanks a lot!",11,8
440,2016-9-13,2016,9,13,21,52k6s6,Jackson Hole town center live stream chat data available for analysis by the curious,https://www.reddit.com/r/MachineLearning/comments/52k6s6/jackson_hole_town_center_live_stream_chat_data/,nomaderx,1473769794,"At any given point of time (24h), there are at least 1200 people watching the Wyoming's Jackson Hole town center live stream and simply saying what they see on the chat: https://twitter.com/bearrabutt/status/772820650436075520

Link to the live stream: https://www.youtube.com/watch?v=psfFJR3vZ78

In a previous reddit post here, it was argued if this was a giant ML experiment: https://www.reddit.com/r/MachineLearning/comments/51b2n5/is_this_wyoming_city_center_webcam_a_giant_ml/

Due to my significantly excessive amount of curiosity and pathetic work prioritization skills, I decided to dig deeper. After all, why would a 1000 people watch this specific live stream and just say what they see? Why would anyone want to do this for hours at end? Why? 

Hypothesis #1: A majority of the chatters on this live stream are/were object recognition bots (maybe based on something like google's image to text caption generation). 

Alt. Hypothesis #2: A majority of the chatters on this live stream are/were people crowdsourced to annotate objects in this video (in a ""natural""/non-precise way maybe). 

Alt. Hypothesis #3: I strongly underestimate the stupidity of humanity on the internet. 

So, like any good machine learner, I created a script to scrape the chat messages and usernames from this chat and save it (in a pkl dictionary). I collected data for about 3 days or something.

If anyone is interested in doing some sort of an analysis, you can find the data here: 
Chat Log Dictionary text file (human readable): https://drive.google.com/open?id=0BwdhggMnx7RsZFRXVEpna0NfMHM
Chat Log Dictionary PKL file (python readable): https://drive.google.com/open?id=0BwdhggMnx7Rsc0UwZV8wa2s5RUU
Also, if anyone is interesting in collecting more chat data, here the python script: https://drive.google.com/open?id=0BwdhggMnx7RsTUREZW9ZWjBOR28

Here's an example from the chat log dictionary:
18stohrc: 'BIRD', 'RED TRUCK', 'GRILLS', 'RED CAR', 'OLD PEOPLE', 'RED', 'EMPTY DUMP TRUCK', 'TRAFFIC', 'BABY', 'BUS', 'BLUE TRUCK', 'SCHOOL BUS', 'BOAT', 'STROLLER', 'VAN', 'J WALKING', 'J WALKING', 'FULL TRUCK', 'RED TRUCK', 'PLUS ULTRA', 'F', 'PLUS ULTRA', 'F', 'GRASMICK', 'BUS', 'BLUE TRUCK', 'EMPTY', 'WHAT DOES THAT CAR SAY?', 'Traffic is slowing down.', ""IT'S HIGH NOON"" , 'THE TREES ARE STARTING TO SWAY!', 'OLD TRUCK', 'How long until the storm hits?', 'WHEELCHAIR', 'ALLUHA AKBAR', 'COWBOYS', 'THE ONLY ROAD THAT I HAVE EVER KNOWN', 'BIRD', 'TRASH', 'SCHOOL BUSD', 'MENTO', 'BUS', 'OLD CAR', '1';

Although I initially believed that hypothesis #1 was quite likely, I now think its most likely hypothesis #2, or maybe even #3.

(Sorry for the long post. Here's a  potato: http://aka.weightwatchers.com/images/1033/dynamic/GCMSImages/Potato_main.jpg)",1,6
441,2016-9-13,2016,9,13,22,52kbd8,Do you think it is possible to have a deep learning machine designed purely to research and analyse deep learning and AI to develop better deep learning ans AI strategies?,https://www.reddit.com/r/MachineLearning/comments/52kbd8/do_you_think_it_is_possible_to_have_a_deep/,BenjaminBoyle,1473771605,,18,0
442,2016-9-13,2016,9,13,22,52kc6t,What is preventing automatically training of algorithm?,https://www.reddit.com/r/MachineLearning/comments/52kc6t/what_is_preventing_automatically_training_of/,rousse101,1473771884,,0,0
443,2016-9-13,2016,9,13,22,52kcyk,Chess game,https://www.reddit.com/r/MachineLearning/comments/52kcyk/chess_game/,huyhcmut,1473772170,"I'm newbie in reinforcement learning. I want to ask are there any methods/algorithms to combine Reinforcement learning and classical search( A*, alpha-beta) in chess game programming. Thank you a lot!!!",2,3
444,2016-9-13,2016,9,13,22,52kinl,Who came up with sigmoidal activation?,https://www.reddit.com/r/MachineLearning/comments/52kinl/who_came_up_with_sigmoidal_activation/,wederer42,1473774262,"Who came up with the idea of using the sigmoid function as activation function for neurons?

Was it Rumelhart in this [paper](http://lia.disi.unibo.it/Courses/SistInt/articoli/nnet1.pdf)? Or has it been mentioned before?

Thanks

edit: i am pretty now sure Rumelhart et al. were the first to apply sigmoid activation functions to neural networks, because they needed it for their back-propagation to work.",3,3
445,2016-9-13,2016,9,13,23,52klq2,How powerful are these Graph Convolutional Networks? (SPOILER: not very): a review of recent arXiv preprint from Kipf&amp;Welling,https://www.reddit.com/r/MachineLearning/comments/52klq2/how_powerful_are_these_graph_convolutional/,fhuszar,1473775327,,11,24
446,2016-9-13,2016,9,13,23,52km2l,Preference Elicitation in Mangaki: Is Your Taste Kinda Weird? - RecSysFR,https://www.reddit.com/r/MachineLearning/comments/52km2l/preference_elicitation_in_mangaki_is_your_taste/,Agagla,1473775442,,0,0
447,2016-9-13,2016,9,13,23,52kn7c,Nvidia Pushes Deep Learning Inference With New Pascal GPUs,https://www.reddit.com/r/MachineLearning/comments/52kn7c/nvidia_pushes_deep_learning_inference_with_new/,[deleted],1473775815,[deleted],0,2
448,2016-9-13,2016,9,13,23,52kphk,Best way to generate context-independent predictions,https://www.reddit.com/r/MachineLearning/comments/52kphk/best_way_to_generate_contextindependent/,MasterEpictetus,1473776559,"Im faced with the problem of generating predictions that are independent of certain contextual information. For example, the probability that a user will click on a recommendation depends on the current price and position of the item on the page. How do I then use click through rate data to predict how much a user likes the item independent of position and price?

One option is to train a model using position and price as features, then generate predictions by setting them to a constant for all items. But I don't know if that's a good way of doing it.

I assume there is a standard way of solving this problem. Id love to hear any suggestions.",0,1
449,2016-9-14,2016,9,14,0,52kxwe,"Databases In Containers By John Mount, Consulting Algorithmist/Researcher - Win-Vector LLC",https://www.reddit.com/r/MachineLearning/comments/52kxwe/databases_in_containers_by_john_mount_consulting/,OpenDataSciCon,1473779241,,0,0
450,2016-9-14,2016,9,14,1,52l9hn,Keras implementation of Deep Minds Wavenet,https://www.reddit.com/r/MachineLearning/comments/52l9hn/keras_implementation_of_deep_minds_wavenet/,Kulde,1473782796,,33,36
451,2016-9-14,2016,9,14,1,52laih,[1609.03068] Multiplex visibility graphs to investigate recurrent neural networks dynamics,https://www.reddit.com/r/MachineLearning/comments/52laih/160903068_multiplex_visibility_graphs_to/,fmb85,1473783099,,0,2
452,2016-9-14,2016,9,14,1,52lba2,[1609.02036] Deep Markov Random Field for Image Modeling,https://www.reddit.com/r/MachineLearning/comments/52lba2/160902036_deep_markov_random_field_for_image/,Thomjazz,1473783335,,0,18
453,2016-9-14,2016,9,14,1,52lf78,Natural Neural Networks for Classification and Neutralization of Drones,https://www.reddit.com/r/MachineLearning/comments/52lf78/natural_neural_networks_for_classification_and/,ckendall_salford,1473784551,,1,2
454,2016-9-14,2016,9,14,1,52lhl5,Speeding up AdaBoost for a real-time application,https://www.reddit.com/r/MachineLearning/comments/52lhl5/speeding_up_adaboost_for_a_realtime_application/,NAOorNever,1473785260,"So I'm working on implementing an object detection algorithm from some recent computer vision papers and right now I've got all my feature detection working at real-time rates (around 50 FPS) in python/cython, but am not trying to do classification and it is incredibly slow using sklearn.

The classifier I'm using is AdaBoost with 256 decision trees, each sliding window in my frame has 5,120 features (128 x 64 frames, 10 features per pixel, and 4x4 pooling). Because it is sliding window, for a 640x480 frame with a stride of 6 (what the paper uses) I get a few hundred windows per frame.

I fit my initial randomized AdaBoost data set, but when I predict it is taking me about .22 seconds to make predictions in sklearn, so around 4 frame per second. I'm wondering if anyone has experience with ways to either speed up this prediction time or if it is worth implementing AdaBoost from scratch in Cython. So far I've had a lot of luck implementing otherwise slow sections of the aglorthim in Cython, so that has been my go-to approach for optimization.

For reference the paper I am basing my work on is this:
http://authors.library.caltech.edu/49239/7/DollarPAMI14pyramids_0.pdf

I looked at their code, but it is really difficult to parse and is a mix of MATLAB/C++, which doesn't translate well to my application. It seems like they implemented their own version of AdaBoost, but it is quite difficult to interpret exactly what is going on as they don't have a lot of documentation for the back-end of the processing. Any help would be greatly appreciated!

Edit: I should add that, in looking at the literature, the thing that surprises me is that most people seem to describe the bottleneck as feature extraction. I have that running at around 75 FPS on its own for 640x480 images, however sklearn is really clamping everything. I was surprised by this because I thought most of their underlying implementation was in Cython already.",10,0
455,2016-9-14,2016,9,14,1,52lje5,Run Your Code on Our Self-Driving Car,https://www.reddit.com/r/MachineLearning/comments/52lje5/run_your_code_on_our_selfdriving_car/,oliverfromudacity,1473785827,,5,10
456,2016-9-14,2016,9,14,1,52ljnf,Learning Machine learning and applying. Please give algorithyms to study/apply for my job,https://www.reddit.com/r/MachineLearning/comments/52ljnf/learning_machine_learning_and_applying_please/,[deleted],1473785907,[deleted],0,0
457,2016-9-14,2016,9,14,2,52llex,Anyone here taking MIT Computational Probability &amp; Inference course on EdX ?,https://www.reddit.com/r/MachineLearning/comments/52llex/anyone_here_taking_mit_computational_probability/,RPher,1473786425,[removed],8,12
458,2016-9-14,2016,9,14,2,52lo31,"About Nando de Freitas DL lectures and ""classes""",https://www.reddit.com/r/MachineLearning/comments/52lo31/about_nando_de_freitas_dl_lectures_and_classes/,LecJackS,1473787238,"I watched [videos from 1 to 8](https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu), two or three times each, taking notes on every slide and what he says.

It's not my first time doing this. I did Andrew Ng course, watched Hinton's lectures, and completed CS188-x from Berkeley (mostly RL).

I find that Nando lectures has more math. LOTS more. And I like it, because I really want to understand the topic from the very roots.

And I do, almost all the time. The video lectures are well explained, and he explain every step in a simple way, not just mathematically, but what it represent in a more general way, so no problem with getting the concepts.

The [""Practicals""](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/) are not too hard neither. They're well documented, it's easy to follow and understand each step, and exercises are written so you play with the code and get a more solid base about it.

My problem is with the ""*Classes*"". Maybe I'm missing something (don't know the terminology of universities outside my country), but I find the Problem sets to be almost impossible. Every problem, every step, it's like it's from another course.
Example of first problem set: https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/class1.pdf

Maybe you need to take some kind of specific class for solving these exercises? Or it's that I was having the false impression of understanding the lectures, while in reality I was just copying stuff on paper and playing with code?

If someone with a better idea than mine on this could help me here, I would be really grateful.

PS: this methodology is the best option I have now, so I want to extract the maximum juice possible from this kind of online material (maybe I'm traveling outside this country in a couple of months and I'll have better options, but that's for another story).",17,10
459,2016-9-14,2016,9,14,2,52lr0q,NVIDIA Announces Tesla P40 &amp; Tesla P4 with INT8 instructions,https://www.reddit.com/r/MachineLearning/comments/52lr0q/nvidia_announces_tesla_p40_tesla_p4_with_int8/,gwern,1473788079,,2,15
460,2016-9-14,2016,9,14,2,52lr7f,How to add discrete values into Naive Bayes calculation,https://www.reddit.com/r/MachineLearning/comments/52lr7f/how_to_add_discrete_values_into_naive_bayes/,[deleted],1473788128,[removed],0,1
461,2016-9-14,2016,9,14,2,52lui5,This Friday I have a job interview for machine learning team. What algorithms/math basics should I repeat?,https://www.reddit.com/r/MachineLearning/comments/52lui5/this_friday_i_have_a_job_interview_for_machine/,whopper667,1473789106,[removed],12,19
462,2016-9-14,2016,9,14,4,52m8kh,Best ways to find ML jobs?,https://www.reddit.com/r/MachineLearning/comments/52m8kh/best_ways_to_find_ml_jobs/,zachtoom,1473793308,What are some of the best avenues for searching ML jobs? I know about /r/MLjobs but it's relatively barren. Are ML jobs more frequently posted on certain sites over others? Or is it better to try to network within the community for finding prospective positions?,8,1
463,2016-9-14,2016,9,14,5,52monj,It's Bayes All The Way Up,https://www.reddit.com/r/MachineLearning/comments/52monj/its_bayes_all_the_way_up/,Noncomment,1473798241,,15,7
464,2016-9-14,2016,9,14,5,52mrbc,Texas hold'em,https://www.reddit.com/r/MachineLearning/comments/52mrbc/texas_holdem/,the_burner_username,1473799051,I am curious what's the latest on machine learning with playing poker. Saw some research paper out there but realistically how well can they play? Anyone with experience in that?,10,1
465,2016-9-14,2016,9,14,5,52mt99,[Discussion] Defeating Image Obfuscation with Deep Learning,https://www.reddit.com/r/MachineLearning/comments/52mt99/discussion_defeating_image_obfuscation_with_deep/,testingTestingIBS,1473799620,"They have the torch network configuration for each network at the end of the paper.

I notice they use some linear units, why?  Aren't most using relu or some derivative?

Also, why are the networks so different?  Is there a good resource on one designs their network?

Paper is here: https://arxiv.org/pdf/1609.00408v2.pdf",7,5
466,2016-9-14,2016,9,14,6,52mwam,ConvMF@RecSys'16,https://www.reddit.com/r/MachineLearning/comments/52mwam/convmfrecsys16/,improbabble,1473800600,,1,7
467,2016-9-14,2016,9,14,6,52n0pr,TensorFlow in a Nutshell - Part Two: Hybrid Learning,https://www.reddit.com/r/MachineLearning/comments/52n0pr/tensorflow_in_a_nutshell_part_two_hybrid_learning/,c0cky_,1473801982,,0,10
468,2016-9-14,2016,9,14,6,52n6my,A Review of Time Series Databases,https://www.reddit.com/r/MachineLearning/comments/52n6my/a_review_of_time_series_databases/,[deleted],1473803886,[deleted],1,1
469,2016-9-14,2016,9,14,7,52n7r3,Otto Tractor Self-driving powerwheels (livestream),https://www.reddit.com/r/MachineLearning/comments/52n7r3/otto_tractor_selfdriving_powerwheels_livestream/,vanboxel,1473804254,,0,2
470,2016-9-14,2016,9,14,8,52nipr,Game theoretic approaches to training neural networks,https://www.reddit.com/r/MachineLearning/comments/52nipr/game_theoretic_approaches_to_training_neural/,FourthHead,1473808164,"I recently read the paper on GANs, and from what I understand, the networks are trained by making them play a minimax game.

I was curious if there is research being done in training networks by having them play more sophisticated games; i.e. expectiminimax or a variation of minimax that only requires a single player.

I know GANs are fairly effective, but I surprisingly haven't come across a lot of literature exploring more complicated games. Is it because the notoriously training difficulty of GANs scales with the complexity of the game?",2,10
471,2016-9-14,2016,9,14,10,52o2e8,"Where is the fancy technology for stenography, which should be enabled by deep learning?",https://www.reddit.com/r/MachineLearning/comments/52o2e8/where_is_the_fancy_technology_for_stenography/,moschles,1473815507,"I spend several days reading the entire novel ""War and Peace"", out loud into a microphone, while a screen presents sentences from it, one by one.  For several more days, I repeat the exercise on some newspaper articles.  The computer records my voice and matches it to the sentences which were displayed at that time. 

But why am I doing this?  Why am I spending my days reading novels to a computer?     

I'm doing this  for several days so that I  never have to type again -- like ever again in my life.   This technology is now totally within our grasp because of deep learning.  Yet I don't see any software or hardware for robust **machine stenography** widely published or sold anywhere.  

A system first trained on millions of examples from human speech (run with the latest, snazziest tensorflow learning system at a Oak-Ridge-grade supercomputer)  could produce a master network to be included in such software.  Later, that network could be honed to an individual customer's voice, via  *transfer learning.*  

Major software giants should get cracking on this immediately.  I really don't know why they are not doing so.   Maybe the folks at  /r/machinelearning can fill in the gaps here.    What say you? 


(Edit : let me be more specific)
I am not referring to little speech recognition apps that recognize less than 12 different commands on a mobile phone (e.g. Cortana).  Instead I am referring to machine stenography. This is where you talk naturally and the computer converts your voice into text format.  The target use would be writing a novel without touching a keyboard.   I have never seen any software that does that in a robust, convincing way.   As far as I can see,  apps like Cortana still make regular silly mistakes the likes of those seen in word-completion.  
",9,0
472,2016-9-14,2016,9,14,10,52o7f5,Microsoft researchers achieve speech recognition milestone - Next at Microsoft,https://www.reddit.com/r/MachineLearning/comments/52o7f5/microsoft_researchers_achieve_speech_recognition/,[deleted],1473817516,[deleted],0,0
473,2016-9-14,2016,9,14,11,52ohn3,imagenet website down for everyone else?,https://www.reddit.com/r/MachineLearning/comments/52ohn3/imagenet_website_down_for_everyone_else/,anonDogeLover,1473821546,[removed],12,6
474,2016-9-14,2016,9,14,12,52oqon,Elon Musk was right. We are becoming Gods that create Gods using code (https://github.com/JordanMicahBennett/God),https://www.reddit.com/r/MachineLearning/comments/52oqon/elon_musk_was_right_we_are_becoming_gods_that/,[deleted],1473825485,[deleted],0,1
475,2016-9-14,2016,9,14,13,52osmg,Elon Musk was right. We are becoming Gods that create Gods using code,https://www.reddit.com/r/MachineLearning/comments/52osmg/elon_musk_was_right_we_are_becoming_gods_that/,[deleted],1473826408,[deleted],0,0
476,2016-9-14,2016,9,14,14,52ozb6,Deep Neural Networks for YouTube Recommendations,https://www.reddit.com/r/MachineLearning/comments/52ozb6/deep_neural_networks_for_youtube_recommendations/,Dawny33,1473829686,,0,15
477,2016-9-14,2016,9,14,15,52pajt,[1609.03971] Feynman Machine: The Universal Dynamical Systems Computer,https://www.reddit.com/r/MachineLearning/comments/52pajt/160903971_feynman_machine_the_universal_dynamical/,fergbyrne,1473836166,,36,20
478,2016-9-14,2016,9,14,19,52pv7t,MODS: WHY ARE YOU STICKYING RANDOM THINGS,https://www.reddit.com/r/MachineLearning/comments/52pv7t/mods_why_are_you_stickying_random_things/,spofersq,1473849897,"maybe just delete the shitposts instead?

this is a weird way to try to promote discussion.",17,42
479,2016-9-14,2016,9,14,21,52q6nv,The Neural Network Zoo,https://www.reddit.com/r/MachineLearning/comments/52q6nv/the_neural_network_zoo/,VanVeenGames,1473855678,,47,312
480,2016-9-14,2016,9,14,22,52qlkd,Using Machine Learning to verify Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/52qlkd/using_machine_learning_to_verify_machine_learning/,yoav_hollander,1473861535,,3,4
481,2016-9-14,2016,9,14,23,52qm1v,Is there a way to serve a Tensorflow model via Django (without installing tf)?,https://www.reddit.com/r/MachineLearning/comments/52qm1v/is_there_a_way_to_serve_a_tensorflow_model_via/,mega10d0n,1473861707,,1,1
482,2016-9-14,2016,9,14,23,52qmdv,What is wrong with my model in keras for alphabet prediction?,https://www.reddit.com/r/MachineLearning/comments/52qmdv/what_is_wrong_with_my_model_in_keras_for_alphabet/,gabegabe6,1473861821,[removed],2,2
483,2016-9-15,2016,9,15,0,52r29c,"For implementing NEAT, how do you determine what makes up your input layer?",https://www.reddit.com/r/MachineLearning/comments/52r29c/for_implementing_neat_how_do_you_determine_what/,BlueFolliage,1473867346,"Hi everyone.

I'm working on a senior design project where we are creating an AI to solve mario(using marioAI competition code, inspired by mari/o) using NEAT. We are working on designing our classes and structure for learning. I've read a few papers on NEAT and understand how it works but am confused about how to determine the input layer. I was at first thinking of having the inputLayer be the 3*2 grid infront of mario be our input layer, with each block being a separate node. The problem i see with this is that I'm basically mapping most if not all input nodes to nodes with in the hidden layer. For example, to determine if nothing is infront of mario, it would have to look at every single input, which I think if very inefficient. 

Would love some feedback and if you have any resources/papers and determining how to create the inputLayer. 

Thanks",9,0
484,2016-9-15,2016,9,15,0,52r4wu,Weird catastrophic forgetting in cart-pole balancing?,https://www.reddit.com/r/MachineLearning/comments/52r4wu/weird_catastrophic_forgetting_in_cartpole/,mithrillion,1473868218,"I was training a q-network to solve the cart-pole balancing problem at openai gym when I found that my network would reach a decent average score (usually around 200-400), then suddenly ""breaks down"" and drop back to very low performance, rises up again to a slightly higher score, then breaks down again in a periodic pattern. Looking at the testing animation after training, I found that my network usually does pretty well at keeping the pole from falling, but it does little to prevent the cart from going off screen.

I have purposed several hypotheses, like inadequate regularisation, target network updating too fast, epsilon annealing process too fast/slow etc. but these do not seem to be the case. I believe the most probable issue is that whenever the agent does well enough that it starts to consistently reach the screen edge, it starts to propagate the ""0-followup"" results backwards, punishing the previously best trajectories. This somehow ""resets"" the network every few hundred episodes.

I would like to know:

1. How to make the agent actually learn to stay in centre without having to temper with the reward function?

2. How to get past the ""edge barrier"" using standard implementations of DQN, PG, etc. ?

**EDIT:** I managed to improve my results by reducing my discount factor and simplify neural network structure. However it still took me over 2000+ steps to be able to balance forever and catastrophic forgetting still happened twice during training.

My parameters:

episodes=2000, max_time=1500, buffer=50000 (simple queue), update_interval=0.02 (soft target update), train_interval=1, train_batch_size=64, gamma=0.975, init_eps=0.5, terminal_eps=0.01, SGD_lr=0.01, SGD_decay=0.0001

Algorithm is basic double Q network.",4,5
485,2016-9-15,2016,9,15,0,52r59o,"Simple Questions Thread September 14, 2016",https://www.reddit.com/r/MachineLearning/comments/52r59o/simple_questions_thread_september_14_2016/,AutoModerator,1473868339,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!
",135,15
486,2016-9-15,2016,9,15,1,52r8pw,Classification metrics - How to evaluate multi-hot classification output?,https://www.reddit.com/r/MachineLearning/comments/52r8pw/classification_metrics_how_to_evaluate_multihot/,jacek_,1473869464,[removed],3,1
487,2016-9-15,2016,9,15,2,52rjqa,How to deal with different data size for classification,https://www.reddit.com/r/MachineLearning/comments/52rjqa/how_to_deal_with_different_data_size_for/,perecastor,1473873163,[removed],6,1
488,2016-9-15,2016,9,15,2,52rqu2,Generative Visual Manipulation on the Natural Image Manifold,https://www.reddit.com/r/MachineLearning/comments/52rqu2/generative_visual_manipulation_on_the_natural/,goodside,1473875466,,20,76
489,2016-9-15,2016,9,15,2,52rs38,Jeremy Howard: The wonderful and terrifying implications of computers that can learn,https://www.reddit.com/r/MachineLearning/comments/52rs38/jeremy_howard_the_wonderful_and_terrifying/,BenjaminBoyle,1473875890,,3,1
490,2016-9-15,2016,9,15,5,52snkz,The Next Wave of Deep Learning Applications,https://www.reddit.com/r/MachineLearning/comments/52snkz/the_next_wave_of_deep_learning_applications/,[deleted],1473886201,[deleted],0,0
491,2016-9-15,2016,9,15,6,52sqck,Visual Question Answering baseline implementation in Keras,https://www.reddit.com/r/MachineLearning/comments/52sqck/visual_question_answering_baseline_implementation/,shash273,1473887083,,3,2
492,2016-9-15,2016,9,15,7,52t6mo,Machine Learning - WAYR (What Are You Reading) - Week 7,https://www.reddit.com/r/MachineLearning/comments/52t6mo/machine_learning_wayr_what_are_you_reading_week_7/,Mandrathax,1473892866,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

|Previous weeks|
|-----------------|
|[Week 1](https://www.reddit.com/r/MachineLearning/comments/4qyjiq/machine_learning_wayr_what_are_you_reading_week_1/)|
|[Week 2](https://www.reddit.com/r/MachineLearning/comments/4s2xqm/machine_learning_wayr_what_are_you_reading_week_2/)|
|[Week 3](https://www.reddit.com/r/MachineLearning/comments/4t7mqm/machine_learning_wayr_what_are_you_reading_week_3/)|
|[Week 4](https://www.reddit.com/r/MachineLearning/comments/4ub2kw/machine_learning_wayr_what_are_you_reading_week_4/)|
|[Week 5](https://www.reddit.com/r/MachineLearning/comments/4xomf7/machine_learning_wayr_what_are_you_reading_week_5/)|
|[Week 6](https://www.reddit.com/r/MachineLearning/comments/4zcyvk/machine_learning_wayr_what_are_you_reading_week_6/)|
|[Week7](https://www.reddit.com/r/MachineLearning/comments/50tuhp/machine_learning_wayr_what_are_you_reading_week_7/)|

Most upvoted papers last week (week 6) :

[Stein Variational Gradient Descent](https://arxiv.org/abs/1608.04471)
[Progressive Neural Networks](http://arxiv.org/abs/1606.04671)

Besides that, there are no rules, have fun.

P.S. : is it possible to stick that post? The previous one (week 7) went completely unnoticed. Also is there any way to automate this without being a mod?
",15,35
493,2016-9-15,2016,9,15,8,52tf1m,Deep Reinforcement Learning to Play StarCraft,https://www.reddit.com/r/MachineLearning/comments/52tf1m/deep_reinforcement_learning_to_play_starcraft/,[deleted],1473896117,[deleted],0,0
494,2016-9-15,2016,9,15,8,52th7w,[1609.02993] Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks,https://www.reddit.com/r/MachineLearning/comments/52th7w/160902993_episodic_exploration_for_deep/,[deleted],1473896963,[deleted],1,0
495,2016-9-15,2016,9,15,8,52thm6,Poor results with Make3D,https://www.reddit.com/r/MachineLearning/comments/52thm6/poor_results_with_make3d/,nrrd,1473897132,[removed],2,1
496,2016-9-15,2016,9,15,9,52tjpq,AWS spot instances vs on prem GP104s,https://www.reddit.com/r/MachineLearning/comments/52tjpq/aws_spot_instances_vs_on_prem_gp104s/,mnbbrown,1473897972,"Has anyone done TCO calculations for on premise vs AWS spot prices. Has GP104/Pascal changed the economics?

I've seen other threads [1] and [2] but they tend to be abstract and not actually do the calcs. I understand it's not an either/or decision - a hybrid approach might work.. on-prem for dev + aws for training?

[1] https://www.reddit.com/r/MachineLearning/comments/48dl7y/aws_vs_buying_your_own_machines/
[2] https://www.reddit.com/r/MachineLearning/comments/4smvl3/a_tool_for_finding_launching_the_cheapest_aws_gpu/",3,0
497,2016-9-15,2016,9,15,9,52tk31,[1609.03528] The Microsoft 2016 Conversational Speech Recognition System (word error rate of 6.9% on the NIST 2000),https://www.reddit.com/r/MachineLearning/comments/52tk31/160903528_the_microsoft_2016_conversational/,connectionism,1473898119,,4,24
498,2016-9-15,2016,9,15,9,52tkb5,"Nice Khanacademy-style intro to Lua, Torch, and Neural Nets in general",https://www.reddit.com/r/MachineLearning/comments/52tkb5/nice_khanacademystyle_intro_to_lua_torch_and/,sbt_,1473898196,,1,31
499,2016-9-15,2016,9,15,12,52ug8o,"I just open source my implementation of famous Andrew Ng coursera exercises in numpy, scipy and minor tensorflow",https://www.reddit.com/r/MachineLearning/comments/52ug8o/i_just_open_source_my_implementation_of_famous/,[deleted],1473911623,[deleted],12,19
500,2016-9-15,2016,9,15,14,52urs7,GTA V will teach neural networks to drive cars and prevent obstacles,https://www.reddit.com/r/MachineLearning/comments/52urs7/gta_v_will_teach_neural_networks_to_drive_cars/,serpiconayak,1473917651,,19,88
501,2016-9-15,2016,9,15,16,52v1vx,[1609.01596] Direct Feedback Alignment Provides Learning in Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/52v1vx/160901596_direct_feedback_alignment_provides/,dharma-1,1473923729,,22,57
502,2016-9-15,2016,9,15,16,52v4qj,Is anyone using Hype library with RNN,https://www.reddit.com/r/MachineLearning/comments/52v4qj/is_anyone_using_hype_library_with_rnn/,adicirstei,1473925652,[removed],0,0
503,2016-9-15,2016,9,15,16,52v5ky,Need help to understand how difficult my Machine Learning assignment is.,https://www.reddit.com/r/MachineLearning/comments/52v5ky/need_help_to_understand_how_difficult_my_machine/,Colchique,1473926279,"Hi,

I have the opportunity to take an ""advanced machine learning"" course this year at my uni. Part of the grade for this course is based on 2 programming assignments.

The lecturer has warned us on day 1 that the programming assignment are hard and required ""a strong programming background"". He advised anyone who doesn't feel comfortable with programming to drop out of this class ASAP and enroll in a new one.

I have asked him exactly what ""strong programming skills"" means as it seems very subjective. He sent me one of the assignments from last year as an example. It is hard for me to judge how hard this assignment is because there is a lot of vocabulary/concepts that I don't understand yet because we will see them in class later. I have sent the assignment to 3 of my friends who are working as software developers &amp; have several years of experience but they were unable to tell me whether the assignment is doable or not because the vocabulary used is too specialised.

So I would love for someone here to red through it and let me know their thoughts about it. Is it hard? Is it achievable by new-ish programmers (think: with skills just below new grad level)?

I'm a bit reluctant to post it here directly because it has a few identifying information in it but if you comment on this post I will PM it to you.

Note that I am not asking (nor do I need) any advice as to how to do this assignment - I only need to know how hard it is and whether it is an achievable one for new grads.

Thanks!",17,0
504,2016-9-15,2016,9,15,17,52v8kg,Startup In a Multi-Trillion Dollar Market Looking for Machine Learning Experts! They are literally looking to hand equity to AI professionals who help them build their system.,https://www.reddit.com/r/MachineLearning/comments/52v8kg/startup_in_a_multitrillion_dollar_market_looking/,BenjaminBoyle,1473928403,,8,0
505,2016-9-15,2016,9,15,18,52vfw0,Microsoft researchers achieve speech recognition milestone - Next at Microsoft,https://www.reddit.com/r/MachineLearning/comments/52vfw0/microsoft_researchers_achieve_speech_recognition/,nisprateek,1473933531,,1,5
506,2016-9-15,2016,9,15,19,52vjiv,Train your own WaveNet: Keras Implementation with sampling,https://www.reddit.com/r/MachineLearning/comments/52vjiv/train_your_own_wavenet_keras_implementation_with/,bsflng,1473935801,,17,30
507,2016-9-15,2016,9,15,19,52vk60,How important is biological plausibility in your work ?,https://www.reddit.com/r/MachineLearning/comments/52vk60/how_important_is_biological_plausibility_in_your/,thomas_h_ward,1473936221,"I was wondering what emphasis is placed on biological/psychological plausibility in your models ?


Do you feel plausibility is essential, or is it a nice-to-have ? Is your work entirely results/performance driven ?

Is this determined by budget, or philosophy ?..
",16,2
508,2016-9-15,2016,9,15,19,52vl29,"I have a hard time to understand the relationship between artificial intelligence, machine learning and deep learning. Help?",https://www.reddit.com/r/MachineLearning/comments/52vl29/i_have_a_hard_time_to_understand_the_relationship/,wederer42,1473936755,[removed],3,0
509,2016-9-15,2016,9,15,20,52vp1w,Avoiding Recalculating CF on new users,https://www.reddit.com/r/MachineLearning/comments/52vp1w/avoiding_recalculating_cf_on_new_users/,warosaurus,1473938983,[removed],1,2
510,2016-9-15,2016,9,15,20,52vrr6,"In RNN based approaches, are there cases where the number of target classes varies in each step with the length of the input?",https://www.reddit.com/r/MachineLearning/comments/52vrr6/in_rnn_based_approaches_are_there_cases_where_the/,8queens,1473940417,"The abstract of Oriol Vinyals' paper Pointer Networks says that ' to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence..cannot be trivially addressed by existent approaches..because
the number of target classes in each step of the output depends on the length of the input, which is variable'",4,6
511,2016-9-15,2016,9,15,20,52vryb,"Machine Learning: The Bigger Picture, Part 1 - Featured on DZone",https://www.reddit.com/r/MachineLearning/comments/52vryb/machine_learning_the_bigger_picture_part_1/,MikeStackState,1473940512,,0,3
512,2016-9-15,2016,9,15,22,52w3bj,[1609.00222] Ternary Neural Networks for Resource-Efficient AI Applications,https://www.reddit.com/r/MachineLearning/comments/52w3bj/160900222_ternary_neural_networks_for/,dharma-1,1473945536,,1,11
513,2016-9-15,2016,9,15,22,52w55c,[1606.08571] Learning Generative ConvNet with Continuous Latent Factors by Alternating Back-Propagation,https://www.reddit.com/r/MachineLearning/comments/52w55c/160608571_learning_generative_convnet_with/,david-gpu,1473946250,,1,15
514,2016-9-15,2016,9,15,23,52wab3,How do GANs align with 'traditional' genetic algorithms? Where does the entropy in the GAN come from and could they be complementary?,https://www.reddit.com/r/MachineLearning/comments/52wab3/how_do_gans_align_with_traditional_genetic/,[deleted],1473948180,"It's likely evident by the question, but I'm a n00b.

It seems that the discrimitave model in a GAN plays the role of the fitness function in a genetic algorithm, but I don't understand how the generative model searches through the solution space to get a better score.  I see a requirement for randomized input to presumably perturb the model or blur the features (?) in the input, but that just seems to ensure the robustness of the model, not the effectiveness of it.

The evolutionary model behind genetic algorithms makes sense to me, and I'm just wondering if a) similar methods are used in a GAN and if not, b) could they be applied?",7,4
515,2016-9-15,2016,9,15,23,52wan9,"Notes on ""Energy Based Generative Adversarial Networks"" review of recent arXiv paper (Zhao, Mathieu and LeCun, 2016)",https://www.reddit.com/r/MachineLearning/comments/52wan9/notes_on_energy_based_generative_adversarial/,fhuszar,1473948303,,36,101
516,2016-9-15,2016,9,15,23,52we08,"Combining Human Knowledge with Machine Learning for Robust Data Flows By Robert Dempsey, Author Python Business Intelligence Cookbook &amp; Builder of Distributed Systems",https://www.reddit.com/r/MachineLearning/comments/52we08/combining_human_knowledge_with_machine_learning/,OpenDataSciCon,1473949452,,0,0
517,2016-9-15,2016,9,15,23,52wfvl,Do Random Forests require cross-validations when running predictive analysis?,https://www.reddit.com/r/MachineLearning/comments/52wfvl/do_random_forests_require_crossvalidations_when/,andtheninthefog,1473950097,,8,0
518,2016-9-15,2016,9,15,23,52wfx1,"AI and ML Futures 1: Background By Neil Lawrence, Professor of Machine Learning - University of Sheffield",https://www.reddit.com/r/MachineLearning/comments/52wfx1/ai_and_ml_futures_1_background_by_neil_lawrence/,OpenDataSciCon,1473950110,,0,0
519,2016-9-16,2016,9,16,0,52wnqj,All you need to get started is a Unix PC with Python installed,https://www.reddit.com/r/MachineLearning/comments/52wnqj/all_you_need_to_get_started_is_a_unix_pc_with/,[deleted],1473952806,[deleted],0,1
520,2016-9-16,2016,9,16,0,52wo38,Can a neural network learn a 2D right-angle without supervision?,https://www.reddit.com/r/MachineLearning/comments/52wo38/can_a_neural_network_learn_a_2d_rightangle/,[deleted],1473952922,[deleted],0,1
521,2016-9-16,2016,9,16,0,52wr8f,Can a neural network learn a 2D right-angle of pixels without supervision?,https://www.reddit.com/r/MachineLearning/comments/52wr8f/can_a_neural_network_learn_a_2d_rightangle_of/,spaceandwine,1473953979,[removed],0,1
522,2016-9-16,2016,9,16,0,52wscp,Scrabble Games Data Set?,https://www.reddit.com/r/MachineLearning/comments/52wscp/scrabble_games_data_set/,The_Prez_,1473954337,[removed],0,1
523,2016-9-16,2016,9,16,0,52wuen,ML Algorithm to Predict Human Behaviour,https://www.reddit.com/r/MachineLearning/comments/52wuen/ml_algorithm_to_predict_human_behaviour/,Yulia_rework,1473955019,,0,1
524,2016-9-16,2016,9,16,1,52x502,"Code released for ""Learning to Communicate with Deep Multi-Agent Reinforcement Learning"" NIPS 2016",https://www.reddit.com/r/MachineLearning/comments/52x502/code_released_for_learning_to_communicate_with/,[deleted],1473958532,[deleted],0,1
525,2016-9-16,2016,9,16,2,52x6y6,Someone help his Deep Learning grandma-enthusiast?,https://www.reddit.com/r/MachineLearning/comments/52x6y6/someone_help_his_deep_learning_grandmaenthusiast/,xristos_forokolomvos,1473959187,,0,1
526,2016-9-16,2016,9,16,2,52xfyp,How to build a Neural Net to solve XOR problem using Nervana Neon,https://www.reddit.com/r/MachineLearning/comments/52xfyp/how_to_build_a_neural_net_to_solve_xor_problem/,[deleted],1473962224,[deleted],1,0
527,2016-9-16,2016,9,16,3,52xn66,[1609.03947v1] Associating Grasping with Convolutional Neural Network Features,https://www.reddit.com/r/MachineLearning/comments/52xn66/160903947v1_associating_grasping_with/,futureroboticist,1473964633,,0,8
528,2016-9-16,2016,9,16,3,52xoad,[1609.01326] UnrealCV: Connecting Computer Vision to Unreal Engine,https://www.reddit.com/r/MachineLearning/comments/52xoad/160901326_unrealcv_connecting_computer_vision_to/,downtownslim,1473965019,,1,29
529,2016-9-16,2016,9,16,3,52xooy,Deep neural nets and the purpose of life.,https://www.reddit.com/r/MachineLearning/comments/52xooy/deep_neural_nets_and_the_purpose_of_life/,nitin_pande,1473965152,,1,1
530,2016-9-16,2016,9,16,3,52xqgo,Video series on deep learning and torch.,https://www.reddit.com/r/MachineLearning/comments/52xqgo/video_series_on_deep_learning_and_torch/,DATAh4ck3r,1473965752,,0,0
531,2016-9-16,2016,9,16,4,52xt3h,Lube Pump: For Maximizing Efficiency and Enhanced Performance,https://www.reddit.com/r/MachineLearning/comments/52xt3h/lube_pump_for_maximizing_efficiency_and_enhanced/,jackerfrinandis,1473966610,,0,1
532,2016-9-16,2016,9,16,4,52xut8,Tensorflow tutorial to build any model from scratch.(Re-post as it will be helpful for the people who are struggling with this),https://www.reddit.com/r/MachineLearning/comments/52xut8/tensorflow_tutorial_to_build_any_model_from/,kazi_shezan,1473967159,,0,6
533,2016-9-16,2016,9,16,4,52xvxj,"Need Some AI? Yeah, Theres a Marketplace for That",https://www.reddit.com/r/MachineLearning/comments/52xvxj/need_some_ai_yeah_theres_a_marketplace_for_that/,[deleted],1473967538,[deleted],1,0
534,2016-9-16,2016,9,16,4,52xyao,Alemite Grease Pump  A Critical Accessory That Will Keep Your Machinery Running,https://www.reddit.com/r/MachineLearning/comments/52xyao/alemite_grease_pump_a_critical_accessory_that/,jackerfrinandis,1473968284,,0,1
535,2016-9-16,2016,9,16,4,52y0ys,NaNs in Layer Normalization,https://www.reddit.com/r/MachineLearning/comments/52y0ys/nans_in_layer_normalization/,AnvaMiba,1473969143,"I'm experimenting with [Layer Normalization](https://arxiv.org/abs/1607.06450) on recurrent neural networks (GRUs) and I've noticed that the gradient tends to behave erratically.

I've applied gradient norm clipping, but even with that every few updates I get some NaN in the gradient, which I ""fix"" by throwing the gradient away for that update and shrinking the parameters (that is, instead of updating with the true loss gradient I update with a L2 regularization gradient).

Ultimately, the optimization process still makes progress, but I'm wondering if I'm losing some performance because of this issue. Is this behavior normal? Is there any way to mitigate it?
",4,1
536,2016-9-16,2016,9,16,5,52yai3,Is there any simple construction like Frustratingly Easy Domain Adaptation for a sequential text model that works well?,https://www.reddit.com/r/MachineLearning/comments/52yai3/is_there_any_simple_construction_like/,rrenaud,1473972178,"FEDA paper: http://www.umiacs.umd.edu/~hal/docs/daume07easyadapt.pdf

I have a black box text classification model (think LSTM) that takes text sequences and I have a classic kind of domain adaptation problem, where I have a limited amount of high quality label data, and a correlated, much larger source of lower quality labelled data.  I want to be able to use a black box text sequence model and take advantage of both label sources.  I directly only care about the high quality label data task.

Probably the best thing would be to do some surgery on the model, add an output layer per task, and learn the intermediate representations jointly.  But this is a black box model, I can't do that.  I can only rewrite the input text to encode the different domains.",4,5
537,2016-9-16,2016,9,16,6,52yewc,Can someone ELI5 cost function?,https://www.reddit.com/r/MachineLearning/comments/52yewc/can_someone_eli5_cost_function/,SteveDougson,1473973612,[removed],2,2
538,2016-9-16,2016,9,16,7,52yuhb,"Predictability, explainability and consistency of deep learning methods?",https://www.reddit.com/r/MachineLearning/comments/52yuhb/predictability_explainability_and_consistency_of/,knocking_,1473978986,"I often hear of ""black box"" approaches in ML, e.g. logistic regression , SVMs, decision trees, etc., but I'm more interested in the ""black box"" (as far as I know) that is deep learning. How do deep learning researchers explain how they *know* their algorithms are well-behaved, accurate, and able to predict well for out-of-sample data?

I ask because, as I've been learning about neural nets, at a certain point I just have to say, ""WTF is going on?""",24,2
539,2016-9-16,2016,9,16,7,52yxig,Error in Architecture of Fully Convolutional Neural Network Producing Optical Illusion,https://www.reddit.com/r/MachineLearning/comments/52yxig/error_in_architecture_of_fully_convolutional/,[deleted],1473980083,[deleted],0,0
540,2016-9-16,2016,9,16,8,52z1qh,Build a Neural Net to solve Exclusisve OR (XOR) problem,https://www.reddit.com/r/MachineLearning/comments/52z1qh/build_a_neural_net_to_solve_exclusisve_or_xor/,[deleted],1473981678,[deleted],0,1
541,2016-9-16,2016,9,16,9,52zb4f,[1609.04382v1] Warped Convolutions: Efficient Invariance to Spatial Transformations,https://www.reddit.com/r/MachineLearning/comments/52zb4f/160904382v1_warped_convolutions_efficient/,connectionism,1473985278,,9,25
542,2016-9-16,2016,9,16,10,52zk45,Impact of video memory performance on NN frameworks.,https://www.reddit.com/r/MachineLearning/comments/52zk45/impact_of_video_memory_performance_on_nn/,[deleted],1473988770,[deleted],0,0
543,2016-9-16,2016,9,16,10,52zopb,Reinforcement Learning and AI,https://www.reddit.com/r/MachineLearning/comments/52zopb/reinforcement_learning_and_ai/,nyike,1473990700,,0,0
544,2016-9-16,2016,9,16,11,52zr5y,Machine learning courses online/online masters,https://www.reddit.com/r/MachineLearning/comments/52zr5y/machine_learning_courses_onlineonline_masters/,[deleted],1473991756,[removed],2,0
545,2016-9-16,2016,9,16,12,52zyiw,Build a Neural Net to solve Exclusive OR (XOR) problem,https://www.reddit.com/r/MachineLearning/comments/52zyiw/build_a_neural_net_to_solve_exclusive_or_xor/,pavanmirla,1473994835,,1,0
546,2016-9-16,2016,9,16,13,5306ns,Meet the startup that two of Googles top self-driving engineers left to create,https://www.reddit.com/r/MachineLearning/comments/5306ns/meet_the_startup_that_two_of_googles_top/,j_lyf,1473998503,,0,3
547,2016-9-16,2016,9,16,13,5306sw,recommendation for GPU cloud service?,https://www.reddit.com/r/MachineLearning/comments/5306sw/recommendation_for_gpu_cloud_service/,FalseAss,1473998568,[removed],3,1
548,2016-9-16,2016,9,16,14,530hjn,[1609.04802] Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network (Twitter Cortex / Magic Pony),https://www.reddit.com/r/MachineLearning/comments/530hjn/160904802_photorealistic_single_image/,smerity,1474004074,,63,109
549,2016-9-16,2016,9,16,17,530ytc,enaible - Artificial Intelligence for Image Analysis. Request your beta invite now!,https://www.reddit.com/r/MachineLearning/comments/530ytc/enaible_artificial_intelligence_for_image/,jguertl,1474014918,,0,0
550,2016-9-16,2016,9,16,17,530zq0,"Need Some AI? Yeah, Theres a Marketplace for That",https://www.reddit.com/r/MachineLearning/comments/530zq0/need_some_ai_yeah_theres_a_marketplace_for_that/,Martin81,1474015582,,2,0
551,2016-9-16,2016,9,16,17,53102s,Watson | Enriching content exploration and discovery with supervised machine learning,https://www.reddit.com/r/MachineLearning/comments/53102s/watson_enriching_content_exploration_and/,[deleted],1474015836,[deleted],0,0
552,2016-9-16,2016,9,16,18,5312yf,How do you go about finding resources for learning something new ?,https://www.reddit.com/r/MachineLearning/comments/5312yf/how_do_you_go_about_finding_resources_for/,udayj,1474017855,[removed],0,0
553,2016-9-16,2016,9,16,19,5318ss,Why Machine Learning? Decent introduction video,https://www.reddit.com/r/MachineLearning/comments/5318ss/why_machine_learning_decent_introduction_video/,Ronster86,1474021653,,0,1
554,2016-9-16,2016,9,16,19,5319ud,Tensor Flow Open sourced a Language Model based on Billion Word Benchmark,https://www.reddit.com/r/MachineLearning/comments/5319ud/tensor_flow_open_sourced_a_language_model_based/,ambodi,1474022254,,20,135
555,2016-9-16,2016,9,16,20,531cip,Is there a god university ML class with a focus on audio?,https://www.reddit.com/r/MachineLearning/comments/531cip/is_there_a_god_university_ml_class_with_a_focus/,[deleted],1474023843,[deleted],0,1
556,2016-9-16,2016,9,16,21,531jrq,Help a newbie out (neural nets),https://www.reddit.com/r/MachineLearning/comments/531jrq/help_a_newbie_out_neural_nets/,Guanoco,1474027560,[removed],2,3
557,2016-9-16,2016,9,16,21,531koo,AlphaGo knows no fear,https://www.reddit.com/r/MachineLearning/comments/531koo/alphago_knows_no_fear/,human_trash_,1474027973,,2,0
558,2016-9-16,2016,9,16,21,531lrq,Deep Learning for Categorizing Job Titles,https://www.reddit.com/r/MachineLearning/comments/531lrq/deep_learning_for_categorizing_job_titles/,grumpybusinesscat,1474028446,,0,1
559,2016-9-16,2016,9,16,21,531oaz,HDF5 vs. JSON - ConvNet Training + Format Question,https://www.reddit.com/r/MachineLearning/comments/531oaz/hdf5_vs_json_convnet_training_format_question/,[deleted],1474029481,[removed],0,1
560,2016-9-16,2016,9,16,22,531yag,Equivalent of F1 Score for a regression problem?,https://www.reddit.com/r/MachineLearning/comments/531yag/equivalent_of_f1_score_for_a_regression_problem/,wederer42,1474033482,[removed],3,0
561,2016-9-16,2016,9,16,22,53209p,The TensorBoard docs have an incorrect stdev computation that many people have copy/pasted.,https://www.reddit.com/r/MachineLearning/comments/53209p/the_tensorboard_docs_have_an_incorrect_stdev/,bdamos,1474034255,,14,81
562,2016-9-16,2016,9,16,23,532162,EAGO: Evolutionary Algorithms in Go,https://www.reddit.com/r/MachineLearning/comments/532162/eago_evolutionary_algorithms_in_go/,wh1t3_w01f,1474034590,,8,12
563,2016-9-17,2016,9,17,0,532bvn,Long-Short Term Memory RNN limitations (and cool demos)?,https://www.reddit.com/r/MachineLearning/comments/532bvn/longshort_term_memory_rnn_limitations_and_cool/,are595,1474038361,[removed],1,0
564,2016-9-17,2016,9,17,0,532jnu,categorical attributes with scikit K Nearest Neigbor,https://www.reddit.com/r/MachineLearning/comments/532jnu/categorical_attributes_with_scikit_k_nearest/,redditshagger,1474040979,[removed],1,0
565,2016-9-17,2016,9,17,0,532kr6,Ressources for starting NLP PhD with ML background,https://www.reddit.com/r/MachineLearning/comments/532kr6/ressources_for_starting_nlp_phd_with_ml_background/,Jean-Porte,1474041347,[removed],1,8
566,2016-9-17,2016,9,17,1,532man,NEUGO: Neural Networks in Go,https://www.reddit.com/r/MachineLearning/comments/532man/neugo_neural_networks_in_go/,[deleted],1474041864,[deleted],6,6
567,2016-9-17,2016,9,17,1,532o36,"Building Deep Neural Networks in the Cloud with Azure GPU VMs, MXNet and Microsoft R Server",https://www.reddit.com/r/MachineLearning/comments/532o36/building_deep_neural_networks_in_the_cloud_with/,phunter_lau,1474042452,,15,24
568,2016-9-17,2016,9,17,1,532ryj,Sound generation from other sound,https://www.reddit.com/r/MachineLearning/comments/532ryj/sound_generation_from_other_sound/,gabegabe6,1474043756,[removed],4,2
569,2016-9-17,2016,9,17,1,532tgt,Replicating DeepMind's Atari Results,https://www.reddit.com/r/MachineLearning/comments/532tgt/replicating_deepminds_atari_results/,stacky777,1474044241,[removed],4,2
570,2016-9-17,2016,9,17,2,5331pl,Implementing multilayer convolutional kernels,https://www.reddit.com/r/MachineLearning/comments/5331pl/implementing_multilayer_convolutional_kernels/,[deleted],1474046947,[removed],0,1
571,2016-9-17,2016,9,17,3,53385c,"2D, two-class classification dataset maker",https://www.reddit.com/r/MachineLearning/comments/53385c/2d_twoclass_classification_dataset_maker/,chestervonwinchester,1474049052,[removed],0,0
572,2016-9-17,2016,9,17,3,533a24,Twitter Cortex Team open sourcing Reinforcement Learning for Torch,https://www.reddit.com/r/MachineLearning/comments/533a24/twitter_cortex_team_open_sourcing_reinforcement/,urish,1474049690,,5,67
573,2016-9-17,2016,9,17,3,533afu,"Optimizing Neural Nets on CPUs With SIMD, OpenMP and MKL",https://www.reddit.com/r/MachineLearning/comments/533afu/optimizing_neural_nets_on_cpus_with_simd_openmp/,vonnik,1474049817,,0,0
574,2016-9-17,2016,9,17,4,533ipb,Machine Learning Computer Build,https://www.reddit.com/r/MachineLearning/comments/533ipb/machine_learning_computer_build/,solidua,1474052578,"I would like to get a machine learners opinions and advice on this build. It will be used primarly for machine learning and I plan to eventually run on 4 titan x's as my data size increases. The I'll be training primarily recurrent neural networks on  datasets of 500,000+ (soon to be 20million) each having 800ish features . 

[PCPartPicker part list](http://pcpartpicker.com/list/M6G4M8) / [Price breakdown by merchant](http://pcpartpicker.com/list/M6G4M8/by_merchant/)

Type|Item|Price
:----|:----|:----
**CPU** | [Intel Core i5-6600K 3.5GHz Quad-Core Processor](http://pcpartpicker.com/product/gx648d/intel-cpu-bx80662i56600k) | $227.88 @ OutletPC 
**CPU Cooler** | [CRYORIG H7 49.0 CFM CPU Cooler](http://pcpartpicker.com/product/93Crxr/cryorig-cpu-cooler-h7) | $43.53 @ Amazon 
**Motherboard** | [Asus Z170-WS ATX LGA1151 Motherboard](http://pcpartpicker.com/product/FvfmP6/asus-motherboard-z170ws) | $347.99 @ SuperBiiz 
**Memory** | [G.Skill Aegis 16GB (1 x 16GB) DDR4-2133 Memory](http://pcpartpicker.com/product/ZXyxFT/gskill-memory-f42133c15s16gis) | $61.99 @ Newegg 
**Storage** | [Samsung 850 EVO-Series 250GB 2.5"" Solid State Drive](http://pcpartpicker.com/product/3kL7YJ/samsung-internal-hard-drive-mz75e250bam) | $94.00 @ B&amp;H 
**Video Card** | [NVIDIA Titan X (Pascal) 12GB Video Card](http://pcpartpicker.com/product/DcH48d/nvidia-titan-x-pascal-12gb-video-card-900-1g611-2500-000) | $1200.00 
**Case** | [Corsair Air 540 ATX Mid Tower Case](http://pcpartpicker.com/product/wgkD4D/corsair-case-air540) | $119.79 @ Newegg 
**Power Supply** | [Corsair AX1500i 1500W 80+ Titanium Certified Fully-Modular ATX Power Supply](http://pcpartpicker.com/product/cn3RsY/corsair-power-supply-cp9020057na) | $409.99 @ B&amp;H 
**Monitor** | [BenQ GL2460HM 24.0"" 60Hz Monitor](http://pcpartpicker.com/product/mWV48d/benq-monitor-gl2460hm) | $139.00 @ B&amp;H 
 | *Prices include shipping, taxes, rebates, and discounts* |
 | Total (before mail-in rebates) | $2654.17
 | Mail-in rebates | -$10.00
 | **Total** | **$2644.17**
 | Generated by [PCPartPicker](http://pcpartpicker.com) 2016-09-16 14:14 EDT-0400 |

**edit:** data size clarification 
",28,18
575,2016-9-17,2016,9,17,11,535fj3,Tensorboard Explained in 5 Min,https://www.reddit.com/r/MachineLearning/comments/535fj3/tensorboard_explained_in_5_min/,llSourcell,1474079305,,0,2
576,2016-9-17,2016,9,17,14,535yzl,"In supervised learning especially CNNs, It is useful method to change or combine color models like RGB, HSL etc...?",https://www.reddit.com/r/MachineLearning/comments/535yzl/in_supervised_learning_especially_cnns_it_is/,chatterboy,1474089256,[removed],0,1
577,2016-9-17,2016,9,17,14,535zhg,Notes on the Numerai ML Competition,https://www.reddit.com/r/MachineLearning/comments/535zhg/notes_on_the_numerai_ml_competition/,j_lyf,1474089539,,0,4
578,2016-9-17,2016,9,17,14,536018,Is there a good university ML class with a focus on audio?,https://www.reddit.com/r/MachineLearning/comments/536018/is_there_a_good_university_ml_class_with_a_focus/,sicp4lyfe,1474089877,[removed],7,10
579,2016-9-17,2016,9,17,14,5361qa,"In supervised learning especially CNNs, It is useful method to change or combine color models like RGB, HSL etc...?",https://www.reddit.com/r/MachineLearning/comments/5361qa/in_supervised_learning_especially_cnns_it_is/,[deleted],1474090884,[removed],0,1
580,2016-9-17,2016,9,17,15,5363tq,geohot to ship Deep Learning powered Self Driving Car Kit for $999 by the end of the year,https://www.reddit.com/r/MachineLearning/comments/5363tq/geohot_to_ship_deep_learning_powered_self_driving/,j_lyf,1474092197,,4,2
581,2016-9-17,2016,9,17,15,5365k5,"What are some boosting algorithms I can apply while performing search queries, where each item is an n dimensional vector?",https://www.reddit.com/r/MachineLearning/comments/5365k5/what_are_some_boosting_algorithms_i_can_apply/,mln00b13,1474093286,"My each item is currently a 100 dimensional vector. Currently, for a given item, to find similar items, I am using scipy's spatial distance, which basically compares each element. However, in my array, the first 50 elements are from an image representation, and the last 50 are a text representation. If I now want to give some boosting to the image part while searching, what sort of approaches can I try?",6,1
582,2016-9-17,2016,9,17,17,536fc8,"Looking forward to start with Machine Learning, need suggestions regarding mathematics",https://www.reddit.com/r/MachineLearning/comments/536fc8/looking_forward_to_start_with_machine_learning/,int-main,1474100288,[removed],1,5
583,2016-9-17,2016,9,17,17,536h3g,Science in the age of selfies,https://www.reddit.com/r/MachineLearning/comments/536h3g/science_in_the_age_of_selfies/,uglyheartbeat,1474101577,,0,0
584,2016-9-17,2016,9,17,18,536lb9,How about food powder mixing extruder machinery operate?,https://www.reddit.com/r/MachineLearning/comments/536lb9/how_about_food_powder_mixing_extruder_machinery/,mixmachinery,1474104758,,1,1
585,2016-9-17,2016,9,17,18,536mg3,[1609.03193] Wav2Letter: an End-to-End ConvNet-based Speech Recognition System,https://www.reddit.com/r/MachineLearning/comments/536mg3/160903193_wav2letter_an_endtoend_convnetbased/,sour_losers,1474105661,,10,46
586,2016-9-17,2016,9,17,19,536ryc,Womens Health Risk Assessment | Cortana Intelligence Gallery,https://www.reddit.com/r/MachineLearning/comments/536ryc/womens_health_risk_assessment_cortana/,uglyheartbeat,1474109822,,0,0
587,2016-9-17,2016,9,17,21,536yhh,What are the prerequisites for start learning ML?,https://www.reddit.com/r/MachineLearning/comments/536yhh/what_are_the_prerequisites_for_start_learning_ml/,Jardi123,1474113994,[removed],0,1
588,2016-9-17,2016,9,17,23,537bxa,Beginner's Research Paper Question (Designing a ML Algorithm from Scratch),https://www.reddit.com/r/MachineLearning/comments/537bxa/beginners_research_paper_question_designing_a_ml/,amlaanb,1474120993,[removed],8,1
589,2016-9-17,2016,9,17,23,537jgs,tf.slim vs tf.learn vs keras : which do you prefer?,https://www.reddit.com/r/MachineLearning/comments/537jgs/tfslim_vs_tflearn_vs_keras_which_do_you_prefer/,[deleted],1474124327,[deleted],1,2
590,2016-9-18,2016,9,18,0,537n72,Is there any implementation of the DNI synthetic gradients paper?,https://www.reddit.com/r/MachineLearning/comments/537n72/is_there_any_implementation_of_the_dni_synthetic/,darkconfidantislife,1474125844,Basically what the title says. Relevant paper: https://arxiv.org/abs/1608.05343,5,26
591,2016-9-18,2016,9,18,0,537p1e,"Tenure track Assistant Professor position at Ryerson (Toronto, Canada)",https://www.reddit.com/r/MachineLearning/comments/537p1e/tenure_track_assistant_professor_position_at/,[deleted],1474126621,[deleted],0,1
592,2016-9-18,2016,9,18,0,537quk,Hello World with tensorflow seq2sequence Model,https://www.reddit.com/r/MachineLearning/comments/537quk/hello_world_with_tensorflow_seq2sequence_model/,[deleted],1474127350,[deleted],0,1
593,2016-9-18,2016,9,18,0,537ri9,"Tenure Track Assistant Professor Opening in ML at Ryerson University (Toronto, Canada)",https://www.reddit.com/r/MachineLearning/comments/537ri9/tenure_track_assistant_professor_opening_in_ml_at/,CSProfKGD,1474127606,,1,7
594,2016-9-18,2016,9,18,3,538e21,"Noob need help with Aetros (on Keras on Theano on native Windows 10), training working but cant send monitoring data",https://www.reddit.com/r/MachineLearning/comments/538e21/noob_need_help_with_aetros_on_keras_on_theano_on/,Maximus-CZ,1474136103,[removed],6,1
595,2016-9-18,2016,9,18,4,538rdt,Ask ML: The rise of Meta CNN? Do any projects aggregate multiple machine learning models?,https://www.reddit.com/r/MachineLearning/comments/538rdt/ask_ml_the_rise_of_meta_cnn_do_any_projects/,beijingspacetech,1474140967,[removed],0,1
596,2016-9-18,2016,9,18,5,538whw,"Why is CNTK so much faster than Theano, TF, Torch and Caffe?",https://www.reddit.com/r/MachineLearning/comments/538whw/why_is_cntk_so_much_faster_than_theano_tf_torch/,ill-logical,1474142898,[removed],1,1
597,2016-9-18,2016,9,18,6,539517,A newbie uses deeplearning4j,https://www.reddit.com/r/MachineLearning/comments/539517/a_newbie_uses_deeplearning4j/,[deleted],1474146225,[deleted],0,1
598,2016-9-18,2016,9,18,7,539fhg,Machine Learning/Data Analyst Opportunities with LexisNexis in Raleigh,https://www.reddit.com/r/MachineLearning/comments/539fhg/machine_learningdata_analyst_opportunities_with/,annak60,1474150437,[removed],0,0
599,2016-9-18,2016,9,18,9,539w7h,Legality of extracting audio dialogue from games and training on it?,https://www.reddit.com/r/MachineLearning/comments/539w7h/legality_of_extracting_audio_dialogue_from_games/,Lajamerr_Mittesdine,1474157559,"There's a lot of games and a lot of variety in speech patterns, words, etc all in very high quality as individual files and very often have subtitles available.

This is very high quality training data for speech synthesis, speech recognition, speech to text, etc.

But what's the legality of this?",12,2
600,2016-9-18,2016,9,18,11,53ad1g,How is the policy network in AlphaGo trained?,https://www.reddit.com/r/MachineLearning/comments/53ad1g/how_is_the_policy_network_in_alphago_trained/,darkconfidantislife,1474165290,"Hey guys, I read through the AlphaGo paper and still wasn't able to lock down how exactly the policy network in AlphaGo is trained. Is it unsupervised or supervised. If unsupervised, how is it trained? REINFORCE? If supervised, what is it trained on? The optimal policy prediction as shown by the value function? 

Thanks in advance. ",14,34
601,2016-9-18,2016,9,18,12,53aob3,Bayesian Regularization for #NeuralNetworks,https://www.reddit.com/r/MachineLearning/comments/53aob3/bayesian_regularization_for_neuralnetworks/,vvpreetham,1474170831,,3,6
602,2016-9-18,2016,9,18,13,53aptf,What software do you use to practice statistics digitally?,https://www.reddit.com/r/MachineLearning/comments/53aptf/what_software_do_you_use_to_practice_statistics/,k3rv1n,1474171592,[removed],2,1
603,2016-9-18,2016,9,18,13,53aq4p,Google machine learning software engineer interview,https://www.reddit.com/r/MachineLearning/comments/53aq4p/google_machine_learning_software_engineer/,soylatte123,1474171766,[removed],0,1
604,2016-9-18,2016,9,18,14,53b0gb,A simple script to show commands (with params) on GPU,https://www.reddit.com/r/MachineLearning/comments/53b0gb/a_simple_script_to_show_commands_with_params_on/,iamaaditya,1474177759,,4,8
605,2016-9-18,2016,9,18,15,53b2uu,"Models for finding topics in my social media posts, search history, bookmarks, sent emails, browser history, and chat logs?",https://www.reddit.com/r/MachineLearning/comments/53b2uu/models_for_finding_topics_in_my_social_media/,emmellen,1474179290,[removed],0,1
606,2016-9-18,2016,9,18,16,53b8qe,Machine Learning marketplace?,https://www.reddit.com/r/MachineLearning/comments/53b8qe/machine_learning_marketplace/,bitbytebot,1474183352,[removed],13,1
607,2016-9-18,2016,9,18,17,53bcnu,Caltech's Learning from Data starts on edX today,https://www.reddit.com/r/MachineLearning/comments/53bcnu/caltechs_learning_from_data_starts_on_edx_today/,beltsazar,1474186352,,21,133
608,2016-9-18,2016,9,18,18,53bisz,Need an ebook version of Computers and Thought,https://www.reddit.com/r/MachineLearning/comments/53bisz/need_an_ebook_version_of_computers_and_thought/,_o_O_o_O_o_,1474191135,[removed],0,0
609,2016-9-18,2016,9,18,20,53brdu,Time series prediction without sliding window. [xpost stackexchange],https://www.reddit.com/r/MachineLearning/comments/53brdu/time_series_prediction_without_sliding_window/,[deleted],1474197455,[removed],1,0
610,2016-9-18,2016,9,18,20,53bv4w,Source,https://www.reddit.com/r/MachineLearning/comments/53bv4w/source/,Ashutosh311297,1474199947,[removed],1,0
611,2016-9-18,2016,9,18,21,53bxrr,SSD to GPU,https://www.reddit.com/r/MachineLearning/comments/53bxrr/ssd_to_gpu/,dharma-1,1474201427,,8,48
612,2016-9-18,2016,9,18,21,53byr5,"BigData and Machine Learning Toolset and Example Weekly Roundup  Sep. 16, 2016",https://www.reddit.com/r/MachineLearning/comments/53byr5/bigdata_and_machine_learning_toolset_and_example/,stkim1,1474201986,,0,1
613,2016-9-18,2016,9,18,22,53c6mn,SqueezeNet: How Convolutional Neural Network Design Can Reduce Parameter Size,https://www.reddit.com/r/MachineLearning/comments/53c6mn/squeezenet_how_convolutional_neural_network/,pmigdal,1474206109,,5,48
614,2016-9-18,2016,9,18,23,53cdcr,Neural Network Toolbox on TensorFlow,https://www.reddit.com/r/MachineLearning/comments/53cdcr/neural_network_toolbox_on_tensorflow/,j_lyf,1474209192,,0,0
615,2016-9-19,2016,9,19,0,53cjrr,Deepmind Wavenet implementation using Tensor Flow,https://www.reddit.com/r/MachineLearning/comments/53cjrr/deepmind_wavenet_implementation_using_tensor_flow/,echan00,1474211811,,0,1
616,2016-9-19,2016,9,19,1,53ctmw,Right way to use drop-out in convolutional nets,https://www.reddit.com/r/MachineLearning/comments/53ctmw/right_way_to_use_dropout_in_convolutional_nets/,[deleted],1474215530,[deleted],0,0
617,2016-9-19,2016,9,19,2,53d729,Call for deep learning research problems,https://www.reddit.com/r/MachineLearning/comments/53d729/call_for_deep_learning_research_problems/,clear_coprolite,1474220374,,24,135
618,2016-9-19,2016,9,19,4,53dliw,Satellite Data,https://www.reddit.com/r/MachineLearning/comments/53dliw/satellite_data/,[deleted],1474225491,[removed],2,0
619,2016-9-19,2016,9,19,5,53dwq1,Need Help Understanding CUDA Library Compilations,https://www.reddit.com/r/MachineLearning/comments/53dwq1/need_help_understanding_cuda_library_compilations/,FR_STARMER,1474229463,[removed],2,2
620,2016-9-19,2016,9,19,5,53e3ae,May be there is more to Machine Learning...,https://www.reddit.com/r/MachineLearning/comments/53e3ae/may_be_there_is_more_to_machine_learning/,[deleted],1474231783,[deleted],0,0
621,2016-9-19,2016,9,19,7,53emr2,"formal system, automatic formal system, and interpreted?",https://www.reddit.com/r/MachineLearning/comments/53emr2/formal_system_automatic_formal_system_and/,[deleted],1474239203,[deleted],0,0
622,2016-9-19,2016,9,19,8,53eogy,ILSVRC2016 Results (Imagenet Competition) should be out on September 23rd--Who's your money on for first place?,https://www.reddit.com/r/MachineLearning/comments/53eogy/ilsvrc2016_results_imagenet_competition_should_be/,ajmooch,1474239910,"I'm betting on Lambprop by a solid 69% relative margin, but I'd be interested to hear what other people think and why.

Will The Googs have some ridiculous new Inception architecture that roll-stomps the comp? Will Residual Dense Fractal Networks of Residual Fractal Dense Networks emerge victorious? Or will we see something completely new that just blows our minds?",10,14
623,2016-9-19,2016,9,19,9,53f2zm,Isotonic Regression Applications,https://www.reddit.com/r/MachineLearning/comments/53f2zm/isotonic_regression_applications/,Jxieeducation,1474245600,,2,3
624,2016-9-19,2016,9,19,10,53fbws,Machine Learning and Policy?,https://www.reddit.com/r/MachineLearning/comments/53fbws/machine_learning_and_policy/,Pikalima,1474249208,[removed],2,2
625,2016-9-19,2016,9,19,11,53fita,how to use accuracy scoring correctly? [python sklearn.metrics &amp;/or other packages],https://www.reddit.com/r/MachineLearning/comments/53fita/how_to_use_accuracy_scoring_correctly_python/,AspiringGuru,1474252100,[removed],0,1
626,2016-9-19,2016,9,19,11,53fmkr,Voice Tagging for Supervised Learning with Amazon's Alexa.,https://www.reddit.com/r/MachineLearning/comments/53fmkr/voice_tagging_for_supervised_learning_with/,lindostangel,1474253693,,1,2
627,2016-9-19,2016,9,19,12,53fodp,How to combine probabilities from classifiers and training data priors?,https://www.reddit.com/r/MachineLearning/comments/53fodp/how_to_combine_probabilities_from_classifiers_and/,CouldOfBeenPolice,1474254446,[removed],1,1
628,2016-9-19,2016,9,19,12,53fryj,need help for implement object detection papers,https://www.reddit.com/r/MachineLearning/comments/53fryj/need_help_for_implement_object_detection_papers/,[deleted],1474256020,[removed],0,1
629,2016-9-19,2016,9,19,13,53fzb9,GitHub - chewxy/gorgonia: Gorgonia is a library that helps facilitate machine learning in Go.,https://www.reddit.com/r/MachineLearning/comments/53fzb9/github_chewxygorgonia_gorgonia_is_a_library_that/,sour_losers,1474259480,,17,24
630,2016-9-19,2016,9,19,15,53g9vs,[1609.05143] Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning (inc video),https://www.reddit.com/r/MachineLearning/comments/53g9vs/160905143_targetdriven_visual_navigation_in/,jvdalen,1474265374,"http://arxiv.org/pdf/1609.05143.pdf
http://arxiv.org/abs/1609.05143

https://www.youtube.com/watch?v=SmBxMDiOrvs&amp;feature=youtu.be

Seems like I will have proper robots in my house before 2020 after all!",7,19
631,2016-9-19,2016,9,19,16,53gfv7,need help for implement object detection papers,https://www.reddit.com/r/MachineLearning/comments/53gfv7/need_help_for_implement_object_detection_papers/,[deleted],1474269175,[deleted],0,0
632,2016-9-19,2016,9,19,18,53gsh1,What deep learning ideas have you tried that didn't work?,https://www.reddit.com/r/MachineLearning/comments/53gsh1/what_deep_learning_ideas_have_you_tried_that/,pmigdal,1474277947,,34,60
633,2016-9-19,2016,9,19,21,53heol,Machine Learning - WAYR (What Are You Reading) - Week 8,https://www.reddit.com/r/MachineLearning/comments/53heol/machine_learning_wayr_what_are_you_reading_week_8/,Mandrathax,1474289820,"This is a place to share machine learning research papers, journals, and articles that you're reading this week.

If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

|Previous Weeks|
|---------------------|
|[Week 1](https://www.reddit.com/r/MachineLearning/comments/4qyjiq/machine_learning_wayr_what_are_you_reading_week_1/)|
|[Week 2](https://www.reddit.com/r/MachineLearning/comments/4s2xqm/machine_learning_wayr_what_are_you_reading_week_2/)|
|[Week 3](https://www.reddit.com/r/MachineLearning/comments/4t7mqm/machine_learning_wayr_what_are_you_reading_week_3/)|
|[Week 4](https://www.reddit.com/r/MachineLearning/comments/4ub2kw/machine_learning_wayr_what_are_you_reading_week_4/)|
|[Week 5](https://www.reddit.com/r/MachineLearning/comments/4xomf7/machine_learning_wayr_what_are_you_reading_week_5/)|
|[Week 6](https://www.reddit.com/r/MachineLearning/comments/4zcyvk/machine_learning_wayr_what_are_you_reading_week_6/)|
|[Week 6](https://www.reddit.com/r/MachineLearning/comments/52t6mo/machine_learning_wayr_what_are_you_reading_week_7/)|

Most upvoted papers last week :

[Energy-based Generative Adversarial Network](http://arxiv.org/abs/1609.03126)

[Learning a Parametric Embedding by Preserving Local Structure](https://lvdmaaten.github.io/publications/papers/AISTATS_2009.pdf) (Parametric t-SNE)

[Unifying Count-Based Exploration and Intrinsic Motivation](https://arxiv.org/abs/1606.01868)

[Direct Feedback Alignment Provides Learning in Deep Neural Networks](https://arxiv.org/abs/1609.01596)

[Relativistic Monte Carlo](http://arxiv.org/abs/1609.04388v1)

Besides that, there are no rules, have fun.",10,22
634,2016-9-19,2016,9,19,22,53hfit,Cars Overhead With Context (COWC) data set,https://www.reddit.com/r/MachineLearning/comments/53hfit/cars_overhead_with_context_cowc_data_set/,shagunsodhani,1474290163,,2,12
635,2016-9-19,2016,9,19,22,53hi26,"Gorgonia: a library like Theano or TensorFlow, mainly written in Go",https://www.reddit.com/r/MachineLearning/comments/53hi26/gorgonia_a_library_like_theano_or_tensorflow/,iamkeyur,1474291201,,0,1
636,2016-9-19,2016,9,19,22,53hmqq,Papers/material on multi-agent systems?,https://www.reddit.com/r/MachineLearning/comments/53hmqq/papersmaterial_on_multiagent_systems/,bagelorder,1474293087,"I am very interested in reinforcement-settings in a multi-agent system.
I have a hard time finding papers about reinforcement learning of sophisticated multi-agent settings, such as having a small society of agents which can trade goods with each other or interact in some other way. Is there even work in that direction?

The ""clostest"" thing which interests me are predator-prey models. When searching for them I found a few interesting ones.

If anybody could recommend any interesting papers or other ressources about something like the above (modeling some kind of society with some kind of social interaction like trading/war) or something similar that would be great. If not I am also very interested in good material about the predator-prey setting. Are there any ""milestones""?",4,3
637,2016-9-19,2016,9,19,23,53hp9g,6 Applications of predictive analytics in business intelligence,https://www.reddit.com/r/MachineLearning/comments/53hp9g/6_applications_of_predictive_analytics_in/,Sergiointelnics,1474294026,,0,0
638,2016-9-20,2016,9,20,0,53i3a7,Coming soon: Go bindings for TensorFlow,https://www.reddit.com/r/MachineLearning/comments/53i3a7/coming_soon_go_bindings_for_tensorflow/,_rusht,1474298865,,4,35
639,2016-9-20,2016,9,20,0,53i5ek,Get X and y coordinates of where tensorflow finds the image?,https://www.reddit.com/r/MachineLearning/comments/53i5ek/get_x_and_y_coordinates_of_where_tensorflow_finds/,Dgameman1,1474299542,[removed],0,1
640,2016-9-20,2016,9,20,0,53i8f9,A Cloud Based Machine Learning Platform,https://www.reddit.com/r/MachineLearning/comments/53i8f9/a_cloud_based_machine_learning_platform/,cloudsim738,1474300538,,0,1
641,2016-9-20,2016,9,20,1,53ifyz,"RoboInstruct: Learning real manipulation tasks from virtual demonstrations using LSTM (video, paper, code, dataset)",https://www.reddit.com/r/MachineLearning/comments/53ifyz/roboinstruct_learning_real_manipulation_tasks/,rrahmati1,1474303055,,5,5
642,2016-9-20,2016,9,20,1,53ii5b,"Code release: ""Learning to Communicate with Deep Multi-Agent Reinforcement Learning"" NIPS 2016. https://github.com/iassael/learning-to-communicate",https://www.reddit.com/r/MachineLearning/comments/53ii5b/code_release_learning_to_communicate_with_deep/,[deleted],1474303788,[deleted],0,1
643,2016-9-20,2016,9,20,2,53ikjl,seq2seq LSTM/RNN with attention for variable-length list classification - examples?,https://www.reddit.com/r/MachineLearning/comments/53ikjl/seq2seq_lstmrnn_with_attention_for_variablelength/,Foxtr0t,1474304570,"As far as I understand, the only model capable of dealing with variable length inputs is RNN. 

Moreover, I'm interested in dealing with lists, where order of elements is a secondary matter (can shuffle and still represent the same thing), as opposed to sequences per se like time series. Attention/memory mechanisms seem to be a good match for such lists, as they allow a network to look at input back and forth as needed.

As an example, imagine a task of selecting a biggest number from a list. An output would be softmax with probabilities.
	
	x -&gt; y:
	[1, 2, 3] -&gt; [0, 0, 1]
	[4, 5, 10, 7.66] -&gt; [0, 0, 1, 0]

If that's too easy, replace scalar elements with vectors and make the task to choose a vector with a biggest sum or product.

Is there an implementation, preferably with an attention/memory mechanism, which one could use for such toy data?",3,2
644,2016-9-20,2016,9,20,2,53ilcr,Fast Wavenet: An efficient Wavenet generation implementation O(L) vs O(2^L),https://www.reddit.com/r/MachineLearning/comments/53ilcr/fast_wavenet_an_efficient_wavenet_generation/,tomlepaine,1474304827,,33,146
645,2016-9-20,2016,9,20,2,53im5z,"Code release: ""Learning to Communicate with Deep Multi-Agent Reinforcement Learning"" NIPS 2016",https://www.reddit.com/r/MachineLearning/comments/53im5z/code_release_learning_to_communicate_with_deep/,jakobnicolaus,1474305070,"Torch implementation of differentiable communication learning:
https://github.com/iassael/learning-to-communicate
",2,23
646,2016-9-20,2016,9,20,2,53iqpq,Analyzing the conditions for studying stars using pandas and seaborn,https://www.reddit.com/r/MachineLearning/comments/53iqpq/analyzing_the_conditions_for_studying_stars_using/,elisebreda,1474306540,,0,6
647,2016-9-20,2016,9,20,2,53iqpz,Any papers on combining physical simulations with machine learning?,https://www.reddit.com/r/MachineLearning/comments/53iqpz/any_papers_on_combining_physical_simulations_with/,Cherubin0,1474306543,"Hi does anyone know about efforts/papers of combining physical/mathematical (3D) simulations and machine learning for predictive maintenance? Or similar?
For example that a 3d model is used by an AI to understand the machine or something like that.

Thank you very much

Edit: I search for information that a machine learning algorithm uses a 3D model for fault detection in a real machine and for predictive maintenance.

Similar to this but new and for predictive maintenance:
 http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A316007&amp;dswid=-5130",8,0
648,2016-9-20,2016,9,20,2,53is3k,Mathematics for reinforcement learning?,https://www.reddit.com/r/MachineLearning/comments/53is3k/mathematics_for_reinforcement_learning/,Indy20161,1474306981,[removed],0,1
649,2016-9-20,2016,9,20,2,53itwq,Mathematics for reinforcement learning?,https://www.reddit.com/r/MachineLearning/comments/53itwq/mathematics_for_reinforcement_learning/,Mr__Christian_Grey,1474307583,[removed],0,1
650,2016-9-20,2016,9,20,3,53ivaa,LaurentMazare/tensorflow-ocaml: OCaml bindings for TensorFlow,https://www.reddit.com/r/MachineLearning/comments/53ivaa/laurentmazaretensorflowocaml_ocaml_bindings_for/,improbabble,1474308050,,1,5
651,2016-9-20,2016,9,20,3,53iw6s,Mathematics for reinforcement learning?,https://www.reddit.com/r/MachineLearning/comments/53iw6s/mathematics_for_reinforcement_learning/,Mr__Christian_Grey,1474308333,[removed],0,1
652,2016-9-20,2016,9,20,3,53iy2c,Mathematics for reinforcement learning?,https://www.reddit.com/r/MachineLearning/comments/53iy2c/mathematics_for_reinforcement_learning/,Indy20161,1474308943,[removed],0,1
653,2016-9-20,2016,9,20,3,53iza4,Inside Googles Internet Justice League and Its AI-Powered War on Trolls,https://www.reddit.com/r/MachineLearning/comments/53iza4/inside_googles_internet_justice_league_and_its/,pilooch,1474309343,,1,0
654,2016-9-20,2016,9,20,3,53j1jc,What is the best algo or family of algorithms for predictive sales lead scoring?,https://www.reddit.com/r/MachineLearning/comments/53j1jc/what_is_the_best_algo_or_family_of_algorithms_for/,Pipvault,1474310048,[removed],1,1
655,2016-9-20,2016,9,20,5,53jhmu,Ten Myths About Machine Learning,https://www.reddit.com/r/MachineLearning/comments/53jhmu/ten_myths_about_machine_learning/,Prooffread3r,1474315303,,2,0
656,2016-9-20,2016,9,20,5,53jr2i,Second problem,https://www.reddit.com/r/MachineLearning/comments/53jr2i/second_problem/,Ashutosh311297,1474318400,[removed],1,0
657,2016-9-20,2016,9,20,6,53jvml,Only 9% load on my CUDA enabled Titan X... Epochs should be taking 3s not 30s.,https://www.reddit.com/r/MachineLearning/comments/53jvml/only_9_load_on_my_cuda_enabled_titan_x_epochs/,FR_STARMER,1474319894,I'm running a simple Keras/Theano script that is enabling CUDA (I see the .cu libraries being built in the command line) on a *very* simple LSTM network. I'm inspecting my GPU load with HWiNFO because the epochs are taking 30 seconds on a Titan X. I'm only at 9% load usage... How do I ramp it up? I should note that I do have CuDNN 5.1 which isn't supported by my version of Theano officially. Would that really be an issue though? I'd expect there to be minor performance problems.,13,2
658,2016-9-20,2016,9,20,6,53jz4g,Interspeech good papers,https://www.reddit.com/r/MachineLearning/comments/53jz4g/interspeech_good_papers/,Nimitz14,1474321059,So Interspeech recently concluded and I'm curious as to what presentations/papers you guys found interesting (I wasn't there).,0,4
659,2016-9-20,2016,9,20,7,53k48r,Mathematics for reinforcement learning?,https://www.reddit.com/r/MachineLearning/comments/53k48r/mathematics_for_reinforcement_learning/,Indy20161,1474322856,[removed],0,1
660,2016-9-20,2016,9,20,7,53k635,Mathematics for reinforcement learning?,https://www.reddit.com/r/MachineLearning/comments/53k635/mathematics_for_reinforcement_learning/,Mr__Christian_Grey,1474323515,[removed],0,1
661,2016-9-20,2016,9,20,7,53k6z3,Training a classifier with large amount of data,https://www.reddit.com/r/MachineLearning/comments/53k6z3/training_a_classifier_with_large_amount_of_data/,testingTestingIBS,1474323841,"When you have 1M or so images to train a classifier on, how does one determine how to design the network?  I assume you experiment on a much smaller portion of the data first before scaling up to a full network like vggnet.

What is your approach?",0,2
662,2016-9-20,2016,9,20,7,53k78o,Three Challenges for Artificial Intelligence in Medicine,https://www.reddit.com/r/MachineLearning/comments/53k78o/three_challenges_for_artificial_intelligence_in/,brandonballinger,1474323936,,7,6
663,2016-9-20,2016,9,20,7,53k869,API.AI is acquired by Google,https://www.reddit.com/r/MachineLearning/comments/53k869/apiai_is_acquired_by_google/,vonnik,1474324265,,3,16
664,2016-9-20,2016,9,20,7,53ka04,Digit Recognizer project?,https://www.reddit.com/r/MachineLearning/comments/53ka04/digit_recognizer_project/,ismymlprogramgood,1474324927,[removed],0,1
665,2016-9-20,2016,9,20,7,53kb61,Mathematics for reinforcement learning?,https://www.reddit.com/r/MachineLearning/comments/53kb61/mathematics_for_reinforcement_learning/,deepmind2016,1474325336,"I know basic RL algo's like Q-learning. I have read deepmind's Dqn and fully understand how it works. But now I want to go deeper into Deep RL, like research stuff. Now I'm studying hilbert spaces(http://www.umiacs.umd.edu/~hal/docs/daume04rkhs.pdf) because they are required for RKHS(http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/rkhscourse.html) which is mentioned under David Silver's course on RL(http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html). So my question is, are advanced mathematical concepts like hilbert spaces are required for RL? But as I'm going through the RKHS(UMD) paper I'm lovin it. It shows the strength of mathematics. :)",0,7
666,2016-9-20,2016,9,20,8,53kh3c,A Self-adapting Evolutionary Algorithm in your Browser,https://www.reddit.com/r/MachineLearning/comments/53kh3c/a_selfadapting_evolutionary_algorithm_in_your/,pkl1234,1474327525,,0,1
667,2016-9-20,2016,9,20,8,53kimd,Etsy buys Blackbird Technologies to bring AI to its search,https://www.reddit.com/r/MachineLearning/comments/53kimd/etsy_buys_blackbird_technologies_to_bring_ai_to/,Refefer,1474328059,,0,1
668,2016-9-20,2016,9,20,8,53kjst,Deep Learning Book can now be preordered,https://www.reddit.com/r/MachineLearning/comments/53kjst/deep_learning_book_can_now_be_preordered/,ma2rten,1474328497,,1,9
669,2016-9-20,2016,9,20,10,53l1os,RECURRENT AND RECURSIVE NETWORKS WITH JEREMY HOWARD (LIVE STREAM),https://www.reddit.com/r/MachineLearning/comments/53l1os/recurrent_and_recursive_networks_with_jeremy/,potato_potaro,1474335379,,0,4
670,2016-9-20,2016,9,20,11,53lbpa,EXTENSIONS AND LIMITATIONS OF THE NEURAL GPU,https://www.reddit.com/r/MachineLearning/comments/53lbpa/extensions_and_limitations_of_the_neural_gpu/,evc123,1474339389,,1,4
671,2016-9-20,2016,9,20,12,53lexr,[1609.04836v1] On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima,https://www.reddit.com/r/MachineLearning/comments/53lexr/160904836v1_on_largebatch_training_for_deep/,fulcrum_xyz,1474340661,,17,37
672,2016-9-20,2016,9,20,12,53lj10,New Machine Learning Course,https://www.reddit.com/r/MachineLearning/comments/53lj10/new_machine_learning_course/,upulbandara,1474342442,,3,6
673,2016-9-20,2016,9,20,13,53lnmj,How to make Training Data for Naive Bayes?,https://www.reddit.com/r/MachineLearning/comments/53lnmj/how_to_make_training_data_for_naive_bayes/,[deleted],1474344516,[deleted],1,0
674,2016-9-20,2016,9,20,13,53loua,That feeling when you can almost touch the history,https://www.reddit.com/r/MachineLearning/comments/53loua/that_feeling_when_you_can_almost_touch_the_history/,Zedmor,1474345105,,15,61
675,2016-9-20,2016,9,20,15,53m0vy,questions about neural networks,https://www.reddit.com/r/MachineLearning/comments/53m0vy/questions_about_neural_networks/,orenog,1474351281,"* what action do I need to do between the weight and the input? 
let's say the weight of the connection is 0.6 and the number that is passing is 0.1, what number will reace the next layer?

* how can a networks understand things like 1-10= yes | 11-20=no | 21-11 =yes |? because if the input is 10 it should be yes, at 11 is should be no, so what on earth can meke it return to be yes when it reaches 21?!

* Let's say I have a net, how do I train it?! I can't just tell it ""be better"" do I must do some kind of evolution simulator to make it evolve?

* thank you ",12,15
676,2016-9-20,2016,9,20,15,53m604,How about vertical mixing trough?,https://www.reddit.com/r/MachineLearning/comments/53m604/how_about_vertical_mixing_trough/,mixmachinery,1474354207,,1,1
677,2016-9-20,2016,9,20,16,53m92m,[1609.05518] Towards Deep Symbolic Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/53m92m/160905518_towards_deep_symbolic_reinforcement/,evc123,1474356080,,0,21
678,2016-9-20,2016,9,20,16,53m9is,[1609.05672] Multi-Residual Networks,https://www.reddit.com/r/MachineLearning/comments/53m9is/160905672_multiresidual_networks/,alexjc,1474356376,,4,8
679,2016-9-20,2016,9,20,16,53mby6,Caffe Fine-tuning tutorial,https://www.reddit.com/r/MachineLearning/comments/53mby6/caffe_finetuning_tutorial/,arif_sohaib,1474357910,[removed],4,0
680,2016-9-20,2016,9,20,18,53mmr6,"tensorflow high-level libraries confusion: tf.contrib.slim, tf.contrib.learn, tf.learn",https://www.reddit.com/r/MachineLearning/comments/53mmr6/tensorflow_highlevel_libraries_confusion/,pomochris,1474365227,It's a bit confusing how many high-level interfaces for tensorflow are currently developed. Which ones do you use and why? Which one is the likeliest to succeed?,10,12
681,2016-9-20,2016,9,20,19,53mq6f,Has anyone attempted translating paragraphs (as opposed to just single sentences)?,https://www.reddit.com/r/MachineLearning/comments/53mq6f/has_anyone_attempted_translating_paragraphs_as/,evc123,1474367319,[removed],2,4
682,2016-9-20,2016,9,20,19,53mrei,[Google DeepMind] Safe and Efficient Off-Policy Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/53mrei/google_deepmind_safe_and_efficient_offpolicy/,ill-logical,1474368056,,0,15
683,2016-9-20,2016,9,20,20,53mtlb,What are the best performing neural/deep models for relation extraction?,https://www.reddit.com/r/MachineLearning/comments/53mtlb/what_are_the_best_performing_neuraldeep_models/,8queens,1474369464,[removed],1,2
684,2016-9-20,2016,9,20,20,53mtmw,Introduction to data mining techniques,https://www.reddit.com/r/MachineLearning/comments/53mtmw/introduction_to_data_mining_techniques/,dataaspirant,1474369493,,0,1
685,2016-9-20,2016,9,20,20,53myv3,What is Zero-shot learning State-of-the-art?,https://www.reddit.com/r/MachineLearning/comments/53myv3/what_is_zeroshot_learning_stateoftheart/,[deleted],1474372342,[deleted],0,1
686,2016-9-20,2016,9,20,21,53n05x,How to increase Naive Bayes accuracy?,https://www.reddit.com/r/MachineLearning/comments/53n05x/how_to_increase_naive_bayes_accuracy/,pknerd,1474372995,[removed],7,1
687,2016-9-20,2016,9,20,21,53n55n,Gaussian Processes for predicting the US election,https://www.reddit.com/r/MachineLearning/comments/53n55n/gaussian_processes_for_predicting_the_us_election/,DanielleMolloy,1474375112,,1,33
688,2016-9-20,2016,9,20,22,53n92k,Purchasing Datasets from Data Brokers,https://www.reddit.com/r/MachineLearning/comments/53n92k/purchasing_datasets_from_data_brokers/,FR_STARMER,1474376775,[removed],0,0
689,2016-9-20,2016,9,20,22,53n9uk,Garlic Peeling machine Manufacturers,https://www.reddit.com/r/MachineLearning/comments/53n9uk/garlic_peeling_machine_manufacturers/,APSindustries,1474377088,,0,2
690,2016-9-20,2016,9,20,22,53ndj9,Garlic Peeler machine,https://www.reddit.com/r/MachineLearning/comments/53ndj9/garlic_peeler_machine/,APSindustries,1474378551,,2,0
691,2016-9-20,2016,9,20,23,53nkok,AMD's Vega will supposedly have 2x fp16,https://www.reddit.com/r/MachineLearning/comments/53nkok/amds_vega_will_supposedly_have_2x_fp16/,darkconfidantislife,1474381249,[removed],0,1
692,2016-9-20,2016,9,20,23,53nm7d,Can someone explain the 1x1 convolutions in inception module?,https://www.reddit.com/r/MachineLearning/comments/53nm7d/can_someone_explain_the_1x1_convolutions_in/,testingTestingIBS,1474381778,[removed],1,1
693,2016-9-20,2016,9,20,23,53nmjq,Online communities for reinforcement learning?,https://www.reddit.com/r/MachineLearning/comments/53nmjq/online_communities_for_reinforcement_learning/,bagelorder,1474381908,"Are there any active communities (forums, Q&amp;A) for reinforcement learning? For machinelearning in general there is this subreddit and the freenode irc channel for example. But their communities seem both to be mainly focused on deep learning. There is no freenode RL channel and the subreddit is quite dead.

I am self-learning RL and often have questions (practical and theoretical ones) and am not quite sure where to ask or even where to get relevant news and an overview of the state of that field for.

Any help appreciated",7,8
694,2016-9-20,2016,9,20,23,53no4n,"LIVE: Dustin Tran's talk at Twitter: ""Edward: a library for probabilistic modelling, inference and model criticism""",https://www.reddit.com/r/MachineLearning/comments/53no4n/live_dustin_trans_talk_at_twitter_edward_a/,fhuszar,1474382484,,10,25
695,2016-9-20,2016,9,20,23,53nonc,What is the current state-of-the-art in pre-training neural networks?,https://www.reddit.com/r/MachineLearning/comments/53nonc/what_is_the_current_stateoftheart_in_pretraining/,sprintletecity,1474382676,[removed],1,3
696,2016-9-21,2016,9,21,0,53nu7f,Emotion recognition using Wireless Signals,https://www.reddit.com/r/MachineLearning/comments/53nu7f/emotion_recognition_using_wireless_signals/,RPher,1474384585,,6,12
697,2016-9-21,2016,9,21,0,53nvk5,Artificial intelligence of Google is changing SEO faster than you think,https://www.reddit.com/r/MachineLearning/comments/53nvk5/artificial_intelligence_of_google_is_changing_seo/,Amdnda,1474385034,,0,5
698,2016-9-21,2016,9,21,1,53o28t,Why isn't XGBoost a more popular research topic?,https://www.reddit.com/r/MachineLearning/comments/53o28t/why_isnt_xgboost_a_more_popular_research_topic/,feedthecreed,1474387233,"I keep hearing that XGBoost keeps winning so many different kaggle competitions:

http://www.kdnuggets.com/2016/03/xgboost-implementing-winningest-kaggle-algorithm-spark-flink.html

https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions

But I don't really see any actual researchers investigating why these models are so effective. Is there any particular non-interesting reason why these models are winning so many Kaggle competitions?",16,25
699,2016-9-21,2016,9,21,1,53o56b,How to assemble training data for ML/NLP task?,https://www.reddit.com/r/MachineLearning/comments/53o56b/how_to_assemble_training_data_for_mlnlp_task/,unoursestunours,1474388181,[removed],0,1
700,2016-9-21,2016,9,21,1,53o7qa,[Video] Playing FPS Games with Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/53o7qa/video_playing_fps_games_with_deep_reinforcement/,downtownslim,1474389008,,43,138
701,2016-9-21,2016,9,21,1,53oaox,Why do machine learning algorithms use least squares cost function instead of taking the absolute value?,https://www.reddit.com/r/MachineLearning/comments/53oaox/why_do_machine_learning_algorithms_use_least/,What_a_reddit,1474389966,"For me it seems that from a computing perspective, converting a number from negative to positive would be easier than taking the square. Also a cost function with non-polynomial terms would make it easier to compute the gradient. So why do we square the cost instead of find the absolute value?",13,6
702,2016-9-21,2016,9,21,1,53ob2r,Helpful Instructions to Lubricate Motor Bearings,https://www.reddit.com/r/MachineLearning/comments/53ob2r/helpful_instructions_to_lubricate_motor_bearings/,jackerfrinandis,1474390097,,0,1
703,2016-9-21,2016,9,21,2,53og64,[1609.05473] SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient,https://www.reddit.com/r/MachineLearning/comments/53og64/160905473_seqgan_sequence_generative_adversarial/,vintermann,1474391722,,10,30
704,2016-9-21,2016,9,21,2,53ok09,"[X-Post from ReinforcementLearning]: If training an RL agent means you need a model to simulate the environment, does this lead RL ad absurdum?",https://www.reddit.com/r/MachineLearning/comments/53ok09/xpost_from_reinforcementlearning_if_training_an/,sabertoothedhedgehog,1474392978,"Hi,

I just posted the following question on https://gitter.im/openai/gym but it seems that reddit would be a better format to discuss my question.

**Background**

As far as I understand, there are only 3 ways to provide the learning data for an RL algorithm:

1. The agent acts in the real world
2. The agent observes someone else doing something in the real world
3. The agent learns in a simulated environment.

(I might be wrong)

If one wanted to train an RL agent to do a complex business task that is very risky in the sense that mistakes are expensive, then the agent cannot act in the real world to learn. I am thinking about risk management tasks like inventory planning.

**The actual question**

*Would providing a simulated environment not lead the whole idea of Reinforcement Learning ad absurdum since you would then have to know the model in order to simulate it. But if you know the model, why do you need Reinforcement Learning?*

*What are the conditions under which Reinforcement Learning can make sense? Have those conditions been written down by someone?*

I am not very experienced. But the first question is how one of my academic colleagues tends to shoot down my Reinforcement Learning ideas.

Would any expert here - besides an open discussion - also be willing to discuss this via PM?",5,3
705,2016-9-21,2016,9,21,3,53osfk,Yhat is hosting a live product demo of Rodeo and ScienceOps tomorrow (9/21) at 2 PM EST.,https://www.reddit.com/r/MachineLearning/comments/53osfk/yhat_is_hosting_a_live_product_demo_of_rodeo_and/,elisebreda,1474395689,"Yhat cofounder and CTO, Greg Lamp &amp; Colin Ristig will be hosting a live webinar demo of Yhat's products, Rodeo and ScienceOps, tomorrow at 2 PM EST.

Greg and Colin will show how to build a beer recommender algorithm and deploy it to a simple web app. There will be time for Q&amp;A about Rodeo and ScienceOps at the end.

If you're not familiar with Yhat, they're the folks behind the Yhat data science/ML blog, Rodeo (open source Python IDE), ggplot (Python visualization package) and ScienceOps (enterprise algorithm deployment).

Details/invites for the webinar are available here: https://www.yhat.com/webinar",0,1
706,2016-9-21,2016,9,21,3,53otdh,How would Artificial Dopamine receptors effect machine learning compared to a conventional point system &amp; set goals?,https://www.reddit.com/r/MachineLearning/comments/53otdh/how_would_artificial_dopamine_receptors_effect/,Rotundus_Maximus,1474395994,,2,0
707,2016-9-21,2016,9,21,3,53ovp9,Extracting a total cost from OCR paper receipt text - not sure on how to structure features with scikit,https://www.reddit.com/r/MachineLearning/comments/53ovp9/extracting_a_total_cost_from_ocr_paper_receipt/,Pecorino,1474396710,"Disclaimer: I'm really new to all of this! Currently taking the Udacity course on ML.

**My data:** I have a dataset of 1000ish (and growing) paper receipt photos, the corresponding OCR text, and the total price on the receipt. All of this data has been verified as correct. Here's an [example receipt](http://imgur.com/a/ZFBgs), and this is what we get from OCR for that one:

+++++++++++++

    Luke's
    Cafe on
    the Quad
    Ibis Drip Coffee 16 oz
    1 @ $2.05 $2.05
    Restaurant Tax $0.16
    Credit Card Credit Card
    Amount $2.21
    Name
    Card Type Visa
    Credit Card XXXXXXXXXXXX 7791
    Authorization Code 09633C
    Credit Card Total $2.21
    Subtotal $2.05
    Taxes $0.16
    Tran. Total $2.21
    Tender Received $2.21
    Cashier Lukes Cafe - 17 Jake R. (super)
    POS Luke's Cafe Reg
    Tran# 3255
    Tran Time 09/20/2016 08:23 AM

+++++++++++++

**The problem:** I would like to extract the receipt's total price from the OCR text. For the most basic of receipts (like the one above), this is really easy with regular expressions - I just look for prices around keywords like ""Total"". Where I have the hardest time though is with the endless amount of edge cases where there is no keyword like ""Total"", or vendors including discount amounts alongside totals, etc etc etc. My regex approach is proving to be unsustainable and extremely hard to maintain. I realize this may not be a good problem for ML, but I would like to at least play around with this dataset on scikit-learn. 

**How I think I should approach this:** I'm thinking every found price within each receipt would act as a separate data point. I can extract these simply with a regex. Some features that I think would be useful to include would be:

* What is the ranking of the price within the receipt, i.e. is this price the highest price? Second? Third? 
* What is the line number of this price on the receipt? (Normalized to 1)
* What words are to the left of this price? To the right?

**Where I'm stuck**: I could use help knowing how to combine these features in scikit-learn so I can actually train a model. For example, with the word features I figured I would use `CountVectorizer`. However, it's not very clear to me how I combine that with the other features to create a fitted model. I'm also not sure if the price rankings feature would need to be normalized or processed in some way. Simply put: I'm not sure how to create my `X`.

Any hints would be greatly appreciated!
",12,5
708,2016-9-21,2016,9,21,5,53pfyh,Asynchronous Methods for Deep Reinforcement Learning with OpenAI gym and MXNet,https://www.reddit.com/r/MachineLearning/comments/53pfyh/asynchronous_methods_for_deep_reinforcement/,phunter_lau,1474403330,,0,6
709,2016-9-21,2016,9,21,5,53pgs6,Sources for detailed LSTM learning algorithms?,https://www.reddit.com/r/MachineLearning/comments/53pgs6/sources_for_detailed_lstm_learning_algorithms/,silverbluep,1474403585,"I am starting doing research on LSTM's. So far, all sources I have found just detail the use of some libraries and just mention that they use BPTT. I want to go into details of learning in LSTM's, an exact implementation with reasoning why they do things would be great. Can anyone suggest sources? Papers/books.",6,8
710,2016-9-21,2016,9,21,6,53pvfc,What kind of data and sensors might be ideal for a model that does lie detection?,https://www.reddit.com/r/MachineLearning/comments/53pvfc/what_kind_of_data_and_sensors_might_be_ideal_for/,alexwalker1,1474408471,,2,1
711,2016-9-21,2016,9,21,7,53pytg,Exploring comma.ai self-driving car dataset (livestream),https://www.reddit.com/r/MachineLearning/comments/53pytg/exploring_commaai_selfdriving_car_dataset/,vanboxel,1474409640,,0,5
712,2016-9-21,2016,9,21,8,53qapl,Using t-SNE on the Numerai ML Competition,https://www.reddit.com/r/MachineLearning/comments/53qapl/using_tsne_on_the_numerai_ml_competition/,Sidego,1474413995,,0,13
713,2016-9-21,2016,9,21,9,53qjxg,Is there a machine learning program for casual tinkering?,https://www.reddit.com/r/MachineLearning/comments/53qjxg/is_there_a_machine_learning_program_for_casual/,Chamale,1474417675,[removed],8,6
714,2016-9-21,2016,9,21,9,53qlvu,How much of an indicator can machine learning provide in detecting good versus bad market conditions?,https://www.reddit.com/r/MachineLearning/comments/53qlvu/how_much_of_an_indicator_can_machine_learning/,chaddjohnson,1474418448,"I generally buy and sell weekly as stock prices fluctuate with volatility. I am exploring the idea of using a machine learning algorithm to consider various economic conditions (inputs) and provide insight (output) into whether any given day is a safe day to buy, given market conditions, for a particular stock. Thus I'd like to determine the risk related to investing at any given time.

For example, it seems more likely that a market correction may occur (and it is bad to buy) when the S&amp;P 500 is pretty high, VIX is low, and the dollar is high.

Similarly, it seems less likely a market correction may occur (and it is good to buy) when indexes are not extremely high, the VIX is relatively higher, and the dollar is relatively lower.

Here is a compiled list of possible factors an algorithm could use as input:

* Fear/volatility index (VIX)
* GDP or GDP growth
* Current interest rate
* Current unemployment rate
* Current Federal funds interest rate
* Employee pay
* Housing market data
* Gold prices
* Dollar value
* Time of year (e.g. September is historically a bad month to invest)
* Oil price
* Bond prices
* Index growth
* Time until next Fed meeting
* Month price change %
* Day price change %

...and other factors (suggestions welcome).

Any thoughts?",35,20
715,2016-9-21,2016,9,21,10,53qpjz,what is the largest dataset used for NN-based Language models?,https://www.reddit.com/r/MachineLearning/comments/53qpjz/what_is_the_largest_dataset_used_for_nnbased/,koormoosh,1474419910,"Out of curiosity, what is the largest dataset (in GB) used for training neural network (RNN, LSTM, etc) language models? Paper reference would be appreciated.",8,1
716,2016-9-21,2016,9,21,10,53qqdy,Greatest walk through of the most useful machine learning algorithm (Random Forest Classifier),https://www.reddit.com/r/MachineLearning/comments/53qqdy/greatest_walk_through_of_the_most_useful_machine/,goudarzi8,1474420237,,0,0
717,2016-9-21,2016,9,21,10,53qryf,gentel intro about machine learning,https://www.reddit.com/r/MachineLearning/comments/53qryf/gentel_intro_about_machine_learning/,eric-zhou,1474420896,[removed],0,1
718,2016-9-21,2016,9,21,11,53r0b7,Ask ML: What do the new state-of-the-art results on KITTI and Cityscapes datasets mean for Autonomous Driving?,https://www.reddit.com/r/MachineLearning/comments/53r0b7/ask_ml_what_do_the_new_stateoftheart_results_on/,[deleted],1474424290,[deleted],0,1
719,2016-9-21,2016,9,21,11,53r1ip,YaRrr! The Pirate's Guide to R,https://www.reddit.com/r/MachineLearning/comments/53r1ip/yarrr_the_pirates_guide_to_r/,rubyantix,1474424778,,0,0
720,2016-9-21,2016,9,21,11,53r1m6,Rajiv Maheswaran: The math behind basketball's wildest moves | TED Talk,https://www.reddit.com/r/MachineLearning/comments/53r1m6/rajiv_maheswaran_the_math_behind_basketballs/,augburto,1474424818,,0,1
721,2016-9-21,2016,9,21,11,53r1u1,What do the new state-of-the-art results on KITTI and Cityscapes datasets mean for Autonomous Driving?,https://www.reddit.com/r/MachineLearning/comments/53r1u1/what_do_the_new_stateoftheart_results_on_kitti/,machinelearningfans,1474424918,,0,1
722,2016-9-21,2016,9,21,11,53r56z,Is there any precedence for training a neural network with another neural network?,https://www.reddit.com/r/MachineLearning/comments/53r56z/is_there_any_precedence_for_training_a_neural/,[deleted],1474426303,[deleted],1,1
723,2016-9-21,2016,9,21,12,53r6kz,How to begin a career in machine learning?,https://www.reddit.com/r/MachineLearning/comments/53r6kz/how_to_begin_a_career_in_machine_learning/,zame530,1474426869,[removed],1,3
724,2016-9-21,2016,9,21,12,53r75i,What do the new state-of-the-art results on KITTI and Cityscapes datasets made by TuSimple mean for Autonomous Driving?,https://www.reddit.com/r/MachineLearning/comments/53r75i/what_do_the_new_stateoftheart_results_on_kitti/,machinelearningfans,1474427089,[removed],0,1
725,2016-9-21,2016,9,21,12,53r7ku,Tensorflow history,https://www.reddit.com/r/MachineLearning/comments/53r7ku/tensorflow_history/,raopati,1474427275,[removed],0,1
726,2016-9-21,2016,9,21,12,53r9hl,What do the new state-of-the-art results on KITTI and Cityscapes datasets made by TuSimple mean for Autonomous Driving?,https://www.reddit.com/r/MachineLearning/comments/53r9hl/what_do_the_new_stateoftheart_results_on_kitti/,machinelearningfans,1474428103,[removed],0,1
727,2016-9-21,2016,9,21,12,53rdyz,Neural Coarse-Graining: Extracting slowly-varying latent degrees of freedom with neural networks,https://www.reddit.com/r/MachineLearning/comments/53rdyz/neural_coarsegraining_extracting_slowlyvarying/,machiner_ps,1474430062,,0,7
728,2016-9-21,2016,9,21,14,53rnnk,Machine Learning &amp; The Product Design Revolution,https://www.reddit.com/r/MachineLearning/comments/53rnnk/machine_learning_the_product_design_revolution/,kickstartaa,1474434830,,0,1
729,2016-9-21,2016,9,21,14,53rqdb,How to setup R Server in Windows 2008?,https://www.reddit.com/r/MachineLearning/comments/53rqdb/how_to_setup_r_server_in_windows_2008/,rousemaga,1474436276,[removed],0,0
730,2016-9-21,2016,9,21,15,53rycm,Machine Learning Using Python and Spark,https://www.reddit.com/r/MachineLearning/comments/53rycm/machine_learning_using_python_and_spark/,dexlabanalytics,1474440888,,1,1
731,2016-9-21,2016,9,21,16,53s2f3,[1609.02943] Stealing Machine Learning Models via Prediction APIs,https://www.reddit.com/r/MachineLearning/comments/53s2f3/160902943_stealing_machine_learning_models_via/,galloguille,1474443363,,9,126
732,2016-9-21,2016,9,21,17,53s5iq,A Cheap Linear Attention Mechanism with Fast Lookups and Fixed-Size Representations (from MILA),https://www.reddit.com/r/MachineLearning/comments/53s5iq/a_cheap_linear_attention_mechanism_with_fast/,muktabh,1474445507,,0,11
733,2016-9-21,2016,9,21,18,53sbqr,"TensorFlow tutorial (extensive), used by Nvidia, compete in ongoing kaggle comp. leaf classification, FFN, CNN, RNN, Kaggle, AE",https://www.reddit.com/r/MachineLearning/comments/53sbqr/tensorflow_tutorial_extensive_used_by_nvidia/,[deleted],1474449809,[deleted],0,1
734,2016-9-21,2016,9,21,18,53sdm7,How would you normalize or standardize data like this?,https://www.reddit.com/r/MachineLearning/comments/53sdm7/how_would_you_normalize_or_standardize_data_like/,wederer42,1474450975,[removed],10,5
735,2016-9-21,2016,9,21,19,53sgwo,The mathematics of machine learning,https://www.reddit.com/r/MachineLearning/comments/53sgwo/the_mathematics_of_machine_learning/,Yovyom,1474452979,[removed],0,1
736,2016-9-21,2016,9,21,20,53sne1,[1602.05179v3] Towards a Biologically Plausible Backprop,https://www.reddit.com/r/MachineLearning/comments/53sne1/160205179v3_towards_a_biologically_plausible/,NicolasGuacamole,1474456748,,32,19
737,2016-9-21,2016,9,21,21,53suhk,Visual Assessment of Multi-target Trackers.,https://www.reddit.com/r/MachineLearning/comments/53suhk/visual_assessment_of_multitarget_trackers/,amilan,1474460294,,0,1
738,2016-9-21,2016,9,21,22,53t2r2,FREE video course on Machine Learning for Recommender Systems,https://www.reddit.com/r/MachineLearning/comments/53t2r2/free_video_course_on_machine_learning_for/,Sroy20,1474463786,,0,0
739,2016-9-21,2016,9,21,22,53t4h8,"Looking to discuss machine learning applications in health and medical IT, text recognition, etc. I am a doctor with no programming experience but tech savy.",https://www.reddit.com/r/MachineLearning/comments/53t4h8/looking_to_discuss_machine_learning_applications/,mobdoc,1474464475,"I am very much interested in discussing ML applications for specific use case involving health IT and medical records. Ideally someone who has a general knowledge in various use cases, limitations, etc. We will be looking to hire a ML specialist programmer (sorry, I'm not familiar with the appropriate title) but consultation initially is needed.  

I have familiarised myself with the theory behind ML and the concepts, but I am a medical professional at most. I had a project in my previous academic post where we worked with a programmer familiar with ML for visual data recognition. I consider myself tech savy but not a programmer. I have specific questions for specific applications and would be grateful to speak to someone further. Skype or in person (U.K or Australia). 

Thanks for your assistance and sorry if this is against subreddit guidelines. Please suggest an alternative to linking up with a ML specialist. 

Edit: thanks for all the responses. I will reply to each over the next few days. Thanks!",13,8
740,2016-9-21,2016,9,21,23,53tdni,A Really cool mention of ESI's acquisition of Mineset,https://www.reddit.com/r/MachineLearning/comments/53tdni/a_really_cool_mention_of_esis_acquisition_of/,cloudsim738,1474467855,,0,1
741,2016-9-22,2016,9,22,0,53tqt7,"TensorFlow tutorial (extensive) with Kaggle competition, used by Nvidia, FFN, CNN, RNN, Kaggle, AE",https://www.reddit.com/r/MachineLearning/comments/53tqt7/tensorflow_tutorial_extensive_with_kaggle/,[deleted],1474472238,[deleted],0,1
742,2016-9-22,2016,9,22,0,53trbb,"TensorFlow tutorial (extensive) with Kaggle competition, used by Nvidia, FFN, CNN, RNN, Kaggle, AE",https://www.reddit.com/r/MachineLearning/comments/53trbb/tensorflow_tutorial_extensive_with_kaggle/,alrojo,1474472393,,7,126
743,2016-9-22,2016,9,22,0,53trd3,"SONY CSL Research Laboratory have created songs composed by ""Artificial Intelligence""",https://www.reddit.com/r/MachineLearning/comments/53trd3/sony_csl_research_laboratory_have_created_songs/,carlthome,1474472412,,4,0
744,2016-9-22,2016,9,22,0,53trr4,ML to predict elections soon. ML for prez,https://www.reddit.com/r/MachineLearning/comments/53trr4/ml_to_predict_elections_soon_ml_for_prez/,[deleted],1474472545,[deleted],0,1
745,2016-9-22,2016,9,22,0,53ttjf,"Simple Questions Thread September 21, 2016",https://www.reddit.com/r/MachineLearning/comments/53ttjf/simple_questions_thread_september_21_2016/,AutoModerator,1474473138,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!
",119,8
746,2016-9-22,2016,9,22,1,53u2mw,Mining Massive Datasets MOOC starts October 11,https://www.reddit.com/r/MachineLearning/comments/53u2mw/mining_massive_datasets_mooc_starts_october_11/,shagunsodhani,1474476070,,0,10
747,2016-9-22,2016,9,22,1,53u5du,Investigating character embeddings in Google's billion word language model,https://www.reddit.com/r/MachineLearning/comments/53u5du/investigating_character_embeddings_in_googles/,halfeatenscone,1474476961,,0,24
748,2016-9-22,2016,9,22,2,53u6j3,Extracting Customer Insights with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/53u6j3/extracting_customer_insights_with_machine_learning/,reworksophie,1474477339,,0,1
749,2016-9-22,2016,9,22,2,53uao6,How Lubrication Pumps Are Essential For Your Machinery Parts?,https://www.reddit.com/r/MachineLearning/comments/53uao6/how_lubrication_pumps_are_essential_for_your/,jackerfrinandis,1474478661,,0,1
750,2016-9-22,2016,9,22,3,53uiz0,iGAN: Interactive Image Generation powered by GAN,https://www.reddit.com/r/MachineLearning/comments/53uiz0/igan_interactive_image_generation_powered_by_gan/,perceptron01,1474481272,,2,28
751,2016-9-22,2016,9,22,3,53uk5b,Api.ai vs Wit.ai: A comparison (x-post from /r/programming),https://www.reddit.com/r/MachineLearning/comments/53uk5b/apiai_vs_witai_a_comparison_xpost_from/,Thijsiez,1474481628,,0,1
752,2016-9-22,2016,9,22,3,53uln9,Straight-through estimator for the argmax function?,https://www.reddit.com/r/MachineLearning/comments/53uln9/straightthrough_estimator_for_the_argmax_function/,AnvaMiba,1474482106,"The straight-through estimator, introduced by Hinton, is a gradient estimator that allows to use binary threshold units in neural networks trained by backpropagation.

It consists of using the threshold unit normally during the forward pass and replacing it with the identity function during the backward pass (possibly with the additional trick of zeroing the derivative if pre-activation input exceeds 1 in absolute value). It is a biased estimator, and while unbiased estimators exist, they tend to be more complex and it seems that empirically the straight-through estimator performs well. For instance, it has been used by /u/MatthieuCourbariaux to train [binarized neural networks](https://arxiv.org/abs/1602.02830).

I was wondering if this estimator can be also used for discrete units with more than two output values. Specifically, I'm thinking of an argmax function which receives a n-dimensional input vector of reals and outputs a n-dimensional binary vector representing the argmax in 1-hot encoding.

The usual way of training these things is by replacing them with a stochastic version and applying something like REINFORCE, but is it possible to train them just by using the straight-through estimator? Has anybody tried? Does anybody have an intuition about why it would/wouldn't work?
",3,10
753,2016-9-22,2016,9,22,3,53ulrt,[1609.05566] Label-Free Supervision of Neural Networks with Physics and Domain Knowledge,https://www.reddit.com/r/MachineLearning/comments/53ulrt/160905566_labelfree_supervision_of_neural/,singularai,1474482150,,5,25
754,2016-9-22,2016,9,22,3,53uok4,Amazing gif images of convolutional filters along with different padding and strides!,https://www.reddit.com/r/MachineLearning/comments/53uok4/amazing_gif_images_of_convolutional_filters_along/,ieee8023,1474483060,,15,122
755,2016-9-22,2016,9,22,4,53v1jr,How to build a robot that sees with $100 and TensorFlow [w/ video],https://www.reddit.com/r/MachineLearning/comments/53v1jr/how_to_build_a_robot_that_sees_with_100_and/,Bardelaz,1474487359,,1,1
756,2016-9-22,2016,9,22,4,53v3fz,How To Improve Deep Learning Performance,https://www.reddit.com/r/MachineLearning/comments/53v3fz/how_to_improve_deep_learning_performance/,Dogsindahouse1,1474487986,,0,0
757,2016-9-22,2016,9,22,7,53vxvc,It's correct my implementation of RNN Adversarial Autoencoder (Keras)?,https://www.reddit.com/r/MachineLearning/comments/53vxvc/its_correct_my_implementation_of_rnn_adversarial/,[deleted],1474498391,[removed],0,1
758,2016-9-22,2016,9,22,9,53wb98,How can one measure the memory capacity of a network of units in a loop?,https://www.reddit.com/r/MachineLearning/comments/53wb98/how_can_one_measure_the_memory_capacity_of_a/,BinaryAlgorithm,1474503486,"I want to compare a spiking neural network with N units to a state machine network with N units and some # of states per unit (statecount). The SNN has some information in its temporal encoding, but how much? The state machine is easier to check since the states are known and explicit. I am trying to make circular ""memory loops"" with connections flowing in one direction, and the processing time is about the same on both of the unit designs, so I want to check whichever can encode more information but I don't yet have a method to compare the two. It seems that if the time resolution is the same on processing cycles, a state machine with say 16 states can transmit 4 bits/cycle, where a SNN can only send a spike or not - 1 bit/cycle. That doesn't seem right though.",2,0
759,2016-9-22,2016,9,22,9,53wci0,SoftTarget Regularization,https://www.reddit.com/r/MachineLearning/comments/53wci0/softtarget_regularization/,[deleted],1474503966,[deleted],0,1
760,2016-9-22,2016,9,22,9,53wd2f,SoftTarget Regularization,https://www.reddit.com/r/MachineLearning/comments/53wd2f/softtarget_regularization/,ArmenAg,1474504183,,32,15
761,2016-9-22,2016,9,22,10,53wqp4,How to find weights and biases from a decision boundary for a multilayer perceptron?,https://www.reddit.com/r/MachineLearning/comments/53wqp4/how_to_find_weights_and_biases_from_a_decision/,gary1_feesher2,1474509445,[removed],0,1
762,2016-9-22,2016,9,22,10,53wqrq,Sales Automation Through a Deep Learning Platform,https://www.reddit.com/r/MachineLearning/comments/53wqrq/sales_automation_through_a_deep_learning_platform/,c0cky_,1474509473,,0,1
763,2016-9-22,2016,9,22,12,53x3n6,Anyone know some good resources to learn structural prediction?,https://www.reddit.com/r/MachineLearning/comments/53x3n6/anyone_know_some_good_resources_to_learn/,ihavemlquestions,1474514646,"Videos,slides, etc.




",1,3
764,2016-9-22,2016,9,22,13,53xfdu,"DeepMind's AI learning to play a 3D game end-to-end (recent comments said ""it's not really practical today, etc"")",https://www.reddit.com/r/MachineLearning/comments/53xfdu/deepminds_ai_learning_to_play_a_3d_game_endtoend/,[deleted],1474520152,[deleted],2,3
765,2016-9-22,2016,9,22,14,53xirb,Offline Speech Recognition: All you need to know,https://www.reddit.com/r/MachineLearning/comments/53xirb/offline_speech_recognition_all_you_need_to_know/,sneurgaonkar,1474521922,,0,1
766,2016-9-22,2016,9,22,15,53xmvj,DexLab Analytics' Special Demo Session Event On Machine Learning Was A Huge Success,https://www.reddit.com/r/MachineLearning/comments/53xmvj/dexlab_analytics_special_demo_session_event_on/,dexlabanalytics,1474524237,,1,1
767,2016-9-22,2016,9,22,15,53xr9k,Simple Introduction to the F-Ratio,https://www.reddit.com/r/MachineLearning/comments/53xr9k/simple_introduction_to_the_fratio/,Jxieeducation,1474526823,,0,11
768,2016-9-22,2016,9,22,16,53xxqe,Neural networks and the XOR function,https://www.reddit.com/r/MachineLearning/comments/53xxqe/neural_networks_and_the_xor_function/,[deleted],1474531027,[deleted],0,1
769,2016-9-22,2016,9,22,18,53y724,[1609.05566v1] Label-Free Supervision of Neural Networks withPhysics and Domain Knowledge,https://www.reddit.com/r/MachineLearning/comments/53y724/160905566v1_labelfree_supervision_of_neural/,[deleted],1474537473,[deleted],1,1
770,2016-9-22,2016,9,22,18,53y7r2,How does powder liquid mixer rotates?,https://www.reddit.com/r/MachineLearning/comments/53y7r2/how_does_powder_liquid_mixer_rotates/,mixmachinery,1474537958,,1,1
771,2016-9-22,2016,9,22,19,53ycr0,Our Machine Learning Training Demo Session Was a Raging Success,https://www.reddit.com/r/MachineLearning/comments/53ycr0/our_machine_learning_training_demo_session_was_a/,dexlabanalytics,1474541079,,1,1
772,2016-9-22,2016,9,22,20,53yf11,"Machining Centers Market to Surpass $5 Billion by 2020, 6-Axis CNC Machine Revolutionizing the Industry",https://www.reddit.com/r/MachineLearning/comments/53yf11/machining_centers_market_to_surpass_5_billion_by/,anushkarane786,1474542436,,0,1
773,2016-9-22,2016,9,22,20,53yjej,How about adhesive glue function mixer packing?,https://www.reddit.com/r/MachineLearning/comments/53yjej/how_about_adhesive_glue_function_mixer_packing/,mixmachinery,1474544753,,1,1
774,2016-9-22,2016,9,22,21,53ypva,Cat Paper Collection,https://www.reddit.com/r/MachineLearning/comments/53ypva/cat_paper_collection/,BadGoyWithAGun,1474547720,,1,7
775,2016-9-22,2016,9,22,21,53yqel,Machine learning in news media.,https://www.reddit.com/r/MachineLearning/comments/53yqel/machine_learning_in_news_media/,nerijus77,1474547939,,0,1
776,2016-9-22,2016,9,22,22,53yu0a,Thermoforming solutions and Services,https://www.reddit.com/r/MachineLearning/comments/53yu0a/thermoforming_solutions_and_services/,ridatuk,1474549455,,0,0
777,2016-9-22,2016,9,22,22,53yxbf,[1609.05566] Label-Free Supervision of Neural Networks with Physics and Domain Knowledge,https://www.reddit.com/r/MachineLearning/comments/53yxbf/160905566_labelfree_supervision_of_neural/,[deleted],1474550792,[deleted],0,1
778,2016-9-22,2016,9,22,22,53yydt,FastBDT: GBDT C++/Python Library (code and paper). Claims fit speed superior to Xgboost,https://www.reddit.com/r/MachineLearning/comments/53yydt/fastbdt_gbdt_cpython_library_code_and_paper/,improbabble,1474551215,,3,36
779,2016-9-22,2016,9,22,23,53zbcn,How to build a robot that sees with $100 and TensorFlow,https://www.reddit.com/r/MachineLearning/comments/53zbcn/how_to_build_a_robot_that_sees_with_100_and/,[deleted],1474555896,[deleted],1,1
780,2016-9-23,2016,9,23,0,53ze2d,What are the Matlab toolboxes for Machine learning?,https://www.reddit.com/r/MachineLearning/comments/53ze2d/what_are_the_matlab_toolboxes_for_machine_learning/,rakesajar,1474556824,[removed],0,1
781,2016-9-23,2016,9,23,0,53ze8p,Advice for Starter Box that can be Expandable,https://www.reddit.com/r/MachineLearning/comments/53ze8p/advice_for_starter_box_that_can_be_expandable/,mlwohls,1474556886,"I'm somewhat new to machine learning (last 18months) but I am spending the vast majority of my time now working in this space.  While I do use AWS when I have large runs, the simplicity of having a local machine to test/train is still big for me.  I'm still using my MacBook Pro as local and I'm thinking about building a ""starter"" box at home.  I'd like to keep costs reasonable to start (single, fast GPU?), but would like the ability to expand to multiple GPUs in the future (4x?).  

Is this a workable plan? If so, do you have any advice on where to start and what's most important now to support expansion later?  Thanks!",13,8
782,2016-9-23,2016,9,23,0,53zfvt,Dangerous Machine Destroys Everything in The World PART 1 | A to Z Videos,https://www.reddit.com/r/MachineLearning/comments/53zfvt/dangerous_machine_destroys_everything_in_the/,allvideosall,1474557422,,0,1
783,2016-9-23,2016,9,23,0,53zmcn,The Upper Confidence Bound Algorithm,https://www.reddit.com/r/MachineLearning/comments/53zmcn/the_upper_confidence_bound_algorithm/,pierrelux,1474559590,,2,11
784,2016-9-23,2016,9,23,1,53zu1j,"What are the widely used toolboxes, packages for machine learning in Matlab and R?",https://www.reddit.com/r/MachineLearning/comments/53zu1j/what_are_the_widely_used_toolboxes_packages_for/,rakesajar,1474562083,[removed],0,1
785,2016-9-23,2016,9,23,1,53zv54,How I wrote my first Machine Learning program in 3 days,https://www.reddit.com/r/MachineLearning/comments/53zv54/how_i_wrote_my_first_machine_learning_program_in/,pknerd,1474562466,,1,1
786,2016-9-23,2016,9,23,2,54028a,iGAN code now available!,https://www.reddit.com/r/MachineLearning/comments/54028a/igan_code_now_available/,spaceanubis1,1474564780,,0,1
787,2016-9-23,2016,9,23,3,540afm,Show and Tell: image captioning open sourced in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/540afm/show_and_tell_image_captioning_open_sourced_in/,shmel39,1474567393,,6,127
788,2016-9-23,2016,9,23,3,540h30,Custom Objectives in Keras Are Giving Me A Headache...,https://www.reddit.com/r/MachineLearning/comments/540h30/custom_objectives_in_keras_are_giving_me_a/,FR_STARMER,1474569492,[removed],4,2
789,2016-9-23,2016,9,23,4,540les,for deeper understanding of machine learning,https://www.reddit.com/r/MachineLearning/comments/540les/for_deeper_understanding_of_machine_learning/,wh1t3_w01f,1474570868,[removed],0,1
790,2016-9-23,2016,9,23,4,540qs4,Where is fp16 support?,https://www.reddit.com/r/MachineLearning/comments/540qs4/where_is_fp16_support/,testingTestingIBS,1474572643,[removed],0,1
791,2016-9-23,2016,9,23,4,540u1t,[1609.06616] Gov2Vec: Learning Distributed Representations of Institutions and Their Legal Text,https://www.reddit.com/r/MachineLearning/comments/540u1t/160906616_gov2vec_learning_distributed/,Thomjazz,1474573712,,2,2
792,2016-9-23,2016,9,23,4,540vtw,Handy machine learning reference guide,https://www.reddit.com/r/MachineLearning/comments/540vtw/handy_machine_learning_reference_guide/,disgr4ce,1474574309,,0,1
793,2016-9-23,2016,9,23,5,540xwt,How to Get a Job In Deep Learning,https://www.reddit.com/r/MachineLearning/comments/540xwt/how_to_get_a_job_in_deep_learning/,pain_perdu,1474575001,,0,2
794,2016-9-23,2016,9,23,5,5411p1,BigML for Alexa: The First Voice Controlled Predictive Assistant,https://www.reddit.com/r/MachineLearning/comments/5411p1/bigml_for_alexa_the_first_voice_controlled/,czuriaga,1474576262,,0,2
795,2016-9-23,2016,9,23,5,5412xa,Datasets for Animal Sounds?,https://www.reddit.com/r/MachineLearning/comments/5412xa/datasets_for_animal_sounds/,enemyben,1474576672,[removed],4,1
796,2016-9-23,2016,9,23,6,5419kt,Training a sentiment analysis classifier using a web scraping visual tool,https://www.reddit.com/r/MachineLearning/comments/5419kt/training_a_sentiment_analysis_classifier_using_a/,wildcodegowrong,1474578878,,0,1
797,2016-9-23,2016,9,23,6,541d9f,Any Simple Implementations of ML algorithms in python?,https://www.reddit.com/r/MachineLearning/comments/541d9f/any_simple_implementations_of_ml_algorithms_in/,Titan_King,1474580159,[removed],0,1
798,2016-9-23,2016,9,23,7,541hnd,Code understanding code through a neural network trained on GitHub,https://www.reddit.com/r/MachineLearning/comments/541hnd/code_understanding_code_through_a_neural_network/,[deleted],1474581704,[removed],0,0
799,2016-9-23,2016,9,23,7,541hsy,Question on Fuzzy-C-Means vs GMM?,https://www.reddit.com/r/MachineLearning/comments/541hsy/question_on_fuzzycmeans_vs_gmm/,[deleted],1474581763,[removed],0,1
800,2016-9-23,2016,9,23,7,541it7,"The Simple plus Practical Path to Machine Learning Capability: Motivating Background, Fundamental Concepts and Workflow",https://www.reddit.com/r/MachineLearning/comments/541it7/the_simple_plus_practical_path_to_machine/,ansible,1474582108,,0,1
801,2016-9-23,2016,9,23,7,541qiw,Use LDA to Classify Text Documents,https://www.reddit.com/r/MachineLearning/comments/541qiw/use_lda_to_classify_text_documents/,felixthursday,1474584918,,0,1
802,2016-9-23,2016,9,23,8,541w9b,AI agents play Doom,https://www.reddit.com/r/MachineLearning/comments/541w9b/ai_agents_play_doom/,nefrpitou,1474587119,,2,1
803,2016-9-23,2016,9,23,8,541wmm,Neural Photo Editing with Introspective Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/541wmm/neural_photo_editing_with_introspective/,ajmooch,1474587263,,40,283
804,2016-9-23,2016,9,23,9,5421ms,Undergraduate Internships in Machine Learning or AI?,https://www.reddit.com/r/MachineLearning/comments/5421ms/undergraduate_internships_in_machine_learning_or/,_N_squared,1474589287,[removed],2,0
805,2016-9-23,2016,9,23,10,542h0k,AI Persuasion Experiment,https://www.reddit.com/r/MachineLearning/comments/542h0k/ai_persuasion_experiment/,[deleted],1474595478,[deleted],1,4
806,2016-9-23,2016,9,23,11,542ooi,This Program Can Mimic Your Handwriting,https://www.reddit.com/r/MachineLearning/comments/542ooi/this_program_can_mimic_your_handwriting/,d4v1dv00,1474598608,,0,5
807,2016-9-23,2016,9,23,12,542vgz,Session with Rajat Monga - Engineering Director for TensorFlow,https://www.reddit.com/r/MachineLearning/comments/542vgz/session_with_rajat_monga_engineering_director_for/,shagunsodhani,1474601545,,0,11
808,2016-9-23,2016,9,23,12,542ykl,What's wrong with this GAN to generate MNIST data?,https://www.reddit.com/r/MachineLearning/comments/542ykl/whats_wrong_with_this_gan_to_generate_mnist_data/,gangangangangangan,1474602947,[removed],0,1
809,2016-9-23,2016,9,23,14,5438mu,Questions about Dilated Convolution,https://www.reddit.com/r/MachineLearning/comments/5438mu/questions_about_dilated_convolution/,[deleted],1474607949,[deleted],3,2
810,2016-9-23,2016,9,23,14,543c0g,Machine Learning: Models with Learned Parameters,https://www.reddit.com/r/MachineLearning/comments/543c0g/machine_learning_models_with_learned_parameters/,iamkeyur,1474609869,,0,1
811,2016-9-23,2016,9,23,14,543c23,Reinforcement Learning: Diverging weights in Predatar-Prey-Environment,https://www.reddit.com/r/MachineLearning/comments/543c23/reinforcement_learning_diverging_weights_in/,bagelorder,1474609892,"I am self-learning Reinforcement-Learning material, mainly using https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs-lecture.pdf
The environment I am testing the algorithms in is pretty simple: 3 predators agents, 1 randomly moving prey, grid world (about 15x15) and they can move up,down,left,right.

At the moment I am learning about function approximation.The update quantity I am using is learning_rate * bellman_error * gradient Q(X_t, A_t) as seen on page 59 of the above paper.

Equally if I use a linear function or a neural network, my weights diverge very quickly (using SARSA, I didn't try Q-Learning yet but I would be suprised if they wouldn't diverge there). I checked the calculations the algorithm makes by hand and it seems right. I also think it is pretty obvious why it diverges: My state only consists of the difference of the position of the prey and the predator (for example (-2, 3) if the prey is two fields to the left and three above)

The agent gets by chance in some particular state, for example 10, 10 where one action is very in favor, for example go down. It will go down and at some point while making this down step it adds to the weights of the down action. The next step the down action has an even higher Q-Value and it steps down again making an even bigger update. This seems to grow exponentially. At some point the delta_t will just be 2 instead of 0,.... by coincidence and then it will overflow in a few steps.

Now I did some research and it seems to be known that there are counterexamples when specific algorithms diverge.

It would be of very great help to me if anyone could tell me which algorithms are known to diverge, in which cases/environments do they diverge and how often does this happen (am I probably making a mistake or does this happen so often that it is no suprise that it happens to me). If this happens very often, in which cases are the algorithms where it happens often even applicable? What does one normally do to avoid this?

Thanks for any help in advance!

Edit: My reward is always 0 only in the last step it is lamba^gametime or lambda^-gametime with lambda &lt; 1 so I guess my Problem are not big rewards",8,2
812,2016-9-23,2016,9,23,15,543j2u,Best Sewing Machine Review Spot,https://www.reddit.com/r/MachineLearning/comments/543j2u/best_sewing_machine_review_spot/,sewingmachine0,1474613992,,0,1
813,2016-9-23,2016,9,23,16,543jxv,Seldon TensorFlow Deep MNIST Digit Classifier Demo,https://www.reddit.com/r/MachineLearning/comments/543jxv/seldon_tensorflow_deep_mnist_digit_classifier_demo/,ahousley,1474614484,,2,0
814,2016-9-23,2016,9,23,18,543xpr,Stein variational gradient descent = variational inference with a kernel density estimator,https://www.reddit.com/r/MachineLearning/comments/543xpr/stein_variational_gradient_descent_variational/,[deleted],1474623641,[deleted],0,1
815,2016-9-23,2016,9,23,19,5442fi,Deep learning supercomputer turns its attention to drug discovery,https://www.reddit.com/r/MachineLearning/comments/5442fi/deep_learning_supercomputer_turns_its_attention/,CoRevolutions,1474626727,,0,4
816,2016-9-23,2016,9,23,19,5443oa,Question: What are algorithms and technologies behind Houzz Visual Match?,https://www.reddit.com/r/MachineLearning/comments/5443oa/question_what_are_algorithms_and_technologies/,antonsekatski,1474627421,[removed],0,1
817,2016-9-23,2016,9,23,20,5448yj,Tutorial: Create a TensorFlow Digit Classifier Microservice,https://www.reddit.com/r/MachineLearning/comments/5448yj/tutorial_create_a_tensorflow_digit_classifier/,ahousley,1474630379,,0,3
818,2016-9-23,2016,9,23,21,544gph,Demystifying the complexities of Natural Language Processing - an experiment to process content from Mahabharata (longest epic ever known).,https://www.reddit.com/r/MachineLearning/comments/544gph/demystifying_the_complexities_of_natural_language/,social-hackerearth,1474634056,,0,1
819,2016-9-23,2016,9,23,21,544i1i,"World Amazing Modern Agriculture Equipment and Mega Machines: Tractor, Harvester, Loader, Excavator",https://www.reddit.com/r/MachineLearning/comments/544i1i/world_amazing_modern_agriculture_equipment_and/,aptoccar,1474634617,,0,1
820,2016-9-23,2016,9,23,21,544kaf,1D or 2D Convolutional Layers on Forex Data?,https://www.reddit.com/r/MachineLearning/comments/544kaf/1d_or_2d_convolutional_layers_on_forex_data/,FR_STARMER,1474635525,[removed],1,1
821,2016-9-23,2016,9,23,22,544m3w,Model selection algorithms in predictive analytics,https://www.reddit.com/r/MachineLearning/comments/544m3w/model_selection_algorithms_in_predictive_analytics/,datapablo,1474636200,,0,1
822,2016-9-23,2016,9,23,22,544p54,CNN for time series. Help correct my diagram.,https://www.reddit.com/r/MachineLearning/comments/544p54/cnn_for_time_series_help_correct_my_diagram/,[deleted],1474637368,[removed],0,1
823,2016-9-23,2016,9,23,23,544v8q,What machine learning/ data science project @ university should I volunteer my time on to maximise my chances of finding work?,https://www.reddit.com/r/MachineLearning/comments/544v8q/what_machine_learning_data_science_project/,The_stat_man,1474639715,[removed],1,1
824,2016-9-23,2016,9,23,23,5451iy,Neural Photo Editing with Introspective Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/5451iy/neural_photo_editing_with_introspective/,amplifier_khan,1474641968,,0,1
825,2016-9-23,2016,9,23,23,54526t,Where will Artificial Intelligence come from?,https://www.reddit.com/r/MachineLearning/comments/54526t/where_will_artificial_intelligence_come_from/,abstractcontrol,1474642206,,1,0
826,2016-9-24,2016,9,24,0,545d9c,Is the deconvolution layer the same as a convolutional layer?,https://www.reddit.com/r/MachineLearning/comments/545d9c/is_the_deconvolution_layer_the_same_as_a/,trustswz,1474645832,,7,19
827,2016-9-24,2016,9,24,1,545ltg,Building an Image Logo Detection Convolutional Neural Net in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/545ltg/building_an_image_logo_detection_convolutional/,mjm2tr,1474648549,,0,1
828,2016-9-24,2016,9,24,1,545lu9,Artificial Intelligence [MicroMasters Program],https://www.reddit.com/r/MachineLearning/comments/545lu9/artificial_intelligence_micromasters_program/,upulbandara,1474648557,,1,1
829,2016-9-24,2016,9,24,1,545mja,Determining feature importance to a model by randomizing the values of that feature in a test set,https://www.reddit.com/r/MachineLearning/comments/545mja/determining_feature_importance_to_a_model_by/,sanity,1474648782,[removed],1,0
830,2016-9-24,2016,9,24,1,545mxd,I've been told that one of the best European research institutions in RL (and ML in general) is INRIA. Can anyone confirm?,https://www.reddit.com/r/MachineLearning/comments/545mxd/ive_been_told_that_one_of_the_best_european/,EdmondRR,1474648896,[removed],3,6
831,2016-9-24,2016,9,24,1,545n8d,"AI multiplayer online contest ""Hypersonic"" starts tomorrow.",https://www.reddit.com/r/MachineLearning/comments/545n8d/ai_multiplayer_online_contest_hypersonic_starts/,Rogerup,1474648995,,0,5
832,2016-9-24,2016,9,24,1,545nqf,"Bay Area Deep Learning School  Live stream on September, 24",https://www.reddit.com/r/MachineLearning/comments/545nqf/bay_area_deep_learning_school_live_stream_on/,barmaley_exe,1474649159,,7,31
833,2016-9-24,2016,9,24,2,545qei,"Question about using Python/Keras ""in production""",https://www.reddit.com/r/MachineLearning/comments/545qei/question_about_using_pythonkeras_in_production/,praiserobotoverlords,1474650025,"tldr; What is the best method you've seen for serving http for thousands of concurrent requests that need to be handed off to persistently running python thread(s)?

Say I have a Keras model trained but need to deliver that model in real-time with sub 100ms response time in a high traffic environment. In Java I could just use Jetty and have well managed threads running models in the background. If I want to do this in Python I run into issues with multi-threading and poorly optimized http server implementations. If I wanted to use something like nginx, to my knowledge there isn't a method of keeping the python in the background running or even to have a pool of ""warmed up"" python threads ready for incoming requests. In general terms what have you seen done in scenarios like this?

I'm not stuck on doing this in pure python, I was just hoping to find out what solutions people have come up with for this scenario.",37,18
834,2016-9-24,2016,9,24,2,545tv5,Choosing the -parameter for additive smoothing? (NLP-perspective),https://www.reddit.com/r/MachineLearning/comments/545tv5/choosing_the_parameter_for_additive_smoothing/,TheEnchantedHunters,1474651145," I'm trying to understand how the log likelihood for training and test sets of a corpus of words is affected as we vary , as per [additive smoothing](https://en.wikipedia.org/wiki/Additive_smoothing). 

For example, if we have a trigram model, how would log likelihood vary as  goes from 0 to ?

This is my first NLP/non-intro ML class and I'm having trouble finding resources on this. If my question is poorly worded, let me know. ",3,1
835,2016-9-24,2016,9,24,2,545uk6,"New draft of ""Reinforcement Learning: An Introduction, Second Edition""",https://www.reddit.com/r/MachineLearning/comments/545uk6/new_draft_of_reinforcement_learning_an/,pierrelux,1474651372,,25,163
836,2016-9-24,2016,9,24,3,54632b,"LatticeRNN : Generalization of RNNs to accept lattices/DAGs as input, applied to speech recognition",https://www.reddit.com/r/MachineLearning/comments/54632b/latticernn_generalization_of_rnns_to_accept/,torvoraptor,1474654060,,2,28
837,2016-9-24,2016,9,24,4,546ctd,Deep Q-learning algorithm implemented in MXNet,https://www.reddit.com/r/MachineLearning/comments/546ctd/deep_qlearning_algorithm_implemented_in_mxnet/,phunter_lau,1474657232,,1,6
838,2016-9-24,2016,9,24,4,546cuw,Attention Decoders for regression,https://www.reddit.com/r/MachineLearning/comments/546cuw/attention_decoders_for_regression/,inkognit,1474657243,"Does anyone know of any paper using sequence to sequence attention models to perform regression between different lengths sequences? I have implemented the whole thing in Theano, but I'm having troubles with the prediction process (although it performs really well if I feed it the ""true"" output values)... I just want to try to understand if it's a bug from the code or a flaw from the model itself that does not handle continuous outputs well.

Cheers!",7,3
839,2016-9-24,2016,9,24,4,546fgh,Beginning Machine Learning with Keras and TensorFlow,https://www.reddit.com/r/MachineLearning/comments/546fgh/beginning_machine_learning_with_keras_and/,cburgdorf,1474658083,,0,0
840,2016-9-24,2016,9,24,4,546lcx,Live Streaming from The Machine Learning Conference in Atlanta,https://www.reddit.com/r/MachineLearning/comments/546lcx/live_streaming_from_the_machine_learning/,shonburton,1474660059,,1,2
841,2016-9-24,2016,9,24,5,546omb,The Curse of Dimensionality,https://www.reddit.com/r/MachineLearning/comments/546omb/the_curse_of_dimensionality/,gearpuppy,1474661141,,0,0
842,2016-9-24,2016,9,24,5,546sqf,Three Things We Learned About Applying Word Vectors to Computer Logs,https://www.reddit.com/r/MachineLearning/comments/546sqf/three_things_we_learned_about_applying_word/,kmike84,1474662476,,0,9
843,2016-9-24,2016,9,24,6,5470jr,An Introduction to Stock Market Data Analysis with Python (Part 1),https://www.reddit.com/r/MachineLearning/comments/5470jr/an_introduction_to_stock_market_data_analysis/,elisebreda,1474665167,,0,3
844,2016-9-24,2016,9,24,6,5477zp,Machine Learning for Learning about Genes.,https://www.reddit.com/r/MachineLearning/comments/5477zp/machine_learning_for_learning_about_genes/,Nyxtia,1474667789,"I want to write a program that uses your gene sequencing (like from 23andMe) and learns to find comparisions between them based on input data and referencing the genes themselves and comparing them between others. 

What is the best approach to doing this? Any good C#api's to get the job done? 

I was looking at accord.net but out of their examples I'm not sure what I'd need to use. 

http://accord-framework.net/samples.html

I guess I'm thinking, Person 1A has dark hair, so does 1B, 1C says they have blond hair and so on... eventually with enough people/training data when person 80B comes up I should be able to look at his genes and be like oh you likely have dark hair. 

Am I correct with my thinking?",6,2
845,2016-9-24,2016,9,24,9,547ri7,Saliency maps and random seeds for weakly supervised image segmentation,https://www.reddit.com/r/MachineLearning/comments/547ri7/saliency_maps_and_random_seeds_for_weakly/,nefrpitou,1474675285,"I'm working on weakly supervised image segmentation, and I came across this work:
https://arxiv.org/pdf/1312.6034.pdf

I implemented the idea on a custom network I had built. The method is as below:

* Train a classifier network, to classify your images into different classes. The classes are the weak labels associated with the images.
* Once the network is fully trained, we proceed to segmentation by computing  saliency maps with respect to the trained network. Specifically, for a given input image, we compute the gradient of the output of the network with respect to the input image. This gives us the gradient of the output with respect to each pixel in the input image. 
* So we have del(output)/del(input) for every pixel. If Pixel A has higher magnitude of gradient as opposed to Pixel B, then it means changing the value at Pixel A will affect the output more than it would if we changed the value at Pixel B; Thus, pixels with higher magnitudes of del(output)/del(input) are given a higher score for belonging to that particular class.

For example, if an input image belongs to a cat and the trained network correctly predicts so, then del(output=cat)/del(input) will tell us which pixels in the input image belong to the cat.

I have a fully convolutional network where the weights per layer are initialised using the initialisation proposed by He et al : http://arxiv.org/abs/1502.01852 , commonly known as He Normal . The weights in a layer are randomly initialised from a normal distribution whose mean is 0 and variance is inversely proportional to the size of the filters used in the layer.

The entire setup above works well and gives me good saliency maps, hence good segmentation; but my problem is, I get my best segmentation for a particular random seed! 

* Different random seed implies different initial weights
* Changing the random seed does not affect the accuracy of the network. That is, no matter what seed I use, my classification accuracy is good; its only the saliency map that differs. For eg , lets say I input a Cat's image. For some seed, my saliency map has high values for the head of the cat, and for some seed, my saliency map has high values for the paws of the cat. After a lot of trial and error I found a seed that identifies the entire cat as the salient portion.

My analysis of the above phenomenon is as below:

* There are patterns in an input image that are unique only to images belonging to the same class as that image. That is, when input an image of a cat, the network identifies some patterns unique only to cat images which helps it classify the image as ""cat"".  Similarly, the network identifies every label by some unique pattern in the input corresponding to only that label.
* However, each label or class does not necessarily have one unique pattern; it may have more than one pattern unique only to that label and not others. Different seeds, aka different weight initializations, simply tell the network which pattern to use to classify the input. As mentioned before, one particular seed makes the network ""think"" that the head of the cat is important to classify the input image as cat , while with another seed the network ""thinks"" that the paws of the cat are salient.

Thus for segmenting using saliency maps on a trained network, not only does the architecture matter but also the initial weights (hence the random seed). The initial weights tell the network which regions in the image to consider salient, while classifying.

Does my above analysis make any sense and has anybody else faced a similar issue before? I'm disturbed by the fact that I'm getting good segmentation for a particular random seed but not for others, hence the above hypothesis. And also, if this is true, then there may be some clever, better ways to initialize weights of a network to get better segmentation.",4,2
846,2016-9-24,2016,9,24,9,547t8q,What are some of the interesting questions that Twitter analysis can answer?,https://www.reddit.com/r/MachineLearning/comments/547t8q/what_are_some_of_the_interesting_questions_that/,faceman21,1474675980,[removed],0,2
847,2016-9-24,2016,9,24,10,5483lp,Generate Music with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/5483lp/generate_music_with_tensorflow/,llSourcell,1474680567,,0,0
848,2016-9-24,2016,9,24,10,54864y,How to get to kaggle team/any ML team?,https://www.reddit.com/r/MachineLearning/comments/54864y/how_to_get_to_kaggle_teamany_ml_team/,Zedmor,1474681748,[removed],1,0
849,2016-9-24,2016,9,24,10,5486aa,Scene recognition with gifs,https://www.reddit.com/r/MachineLearning/comments/5486aa/scene_recognition_with_gifs/,TuringsEgo,1474681817,[removed],0,2
850,2016-9-24,2016,9,24,11,5488us,Large Scale Visual Recognition Challenge 2016 (ILSVRC2016) - Results being announced today,https://www.reddit.com/r/MachineLearning/comments/5488us/large_scale_visual_recognition_challenge_2016/,Lajamerr_Mittesdine,1474683025,,19,42
851,2016-9-24,2016,9,24,11,5488wb,Machine (re)learning: Neural Networks from scratch,https://www.reddit.com/r/MachineLearning/comments/5488wb/machine_relearning_neural_networks_from_scratch/,predef,1474683043,,2,9
852,2016-9-24,2016,9,24,14,548ts1,Doing a data mining project for school,https://www.reddit.com/r/MachineLearning/comments/548ts1/doing_a_data_mining_project_for_school/,_yariman,1474693423,[removed],0,0
853,2016-9-24,2016,9,24,15,54906u,Ideas to Machine Learning home projects?,https://www.reddit.com/r/MachineLearning/comments/54906u/ideas_to_machine_learning_home_projects/,jtfidje,1474697297,[removed],0,1
854,2016-9-24,2016,9,24,15,5490ne,Performance of models on Android/iOS,https://www.reddit.com/r/MachineLearning/comments/5490ne/performance_of_models_on_androidios/,hastor,1474697577,"I see that Caffe, Torch, and Tensorflow are all available on Android (and probably iOS).

However, I can't seem to find benchmarks comparing the performance of these frameworks on ARM, or comparisons in general for production use on mobile devices.

What kind of experience do you have with this?  Which framework would you recommend?",4,9
855,2016-9-24,2016,9,24,15,5490yg,Time series prediction with RNNs,https://www.reddit.com/r/MachineLearning/comments/5490yg/time_series_prediction_with_rnns/,wederer42,1474697780,[removed],8,2
856,2016-9-24,2016,9,24,17,549dab,Is Amazon Machine Learning appropriate for generating meal plans?,https://www.reddit.com/r/MachineLearning/comments/549dab/is_amazon_machine_learning_appropriate_for/,fibericon,1474706631,[removed],0,1
857,2016-9-24,2016,9,24,18,549fw3,[1609.07088v1] Learning Modular Neural Network Policies for Multi-Task and Multi-Robot Transfer,https://www.reddit.com/r/MachineLearning/comments/549fw3/160907088v1_learning_modular_neural_network/,futureroboticist,1474708596,,4,15
858,2016-9-24,2016,9,24,18,549g0i,Estimating delivery times,https://www.reddit.com/r/MachineLearning/comments/549g0i/estimating_delivery_times/,[deleted],1474708698,[removed],0,1
859,2016-9-24,2016,9,24,18,549ig1,What does the auto paint mixing machine look like?,https://www.reddit.com/r/MachineLearning/comments/549ig1/what_does_the_auto_paint_mixing_machine_look_like/,mixmachinery,1474710510,,1,1
860,2016-9-24,2016,9,24,20,549qhl,"Can RNNs adjust their ""memory"" while predicting?",https://www.reddit.com/r/MachineLearning/comments/549qhl/can_rnns_adjust_their_memory_while_predicting/,wederer42,1474716137,[removed],2,0
861,2016-9-24,2016,9,24,20,549rb7,Tensorflow tf.reshape() seems to behave differently to numpy.reshape() - Stack Overflow,https://www.reddit.com/r/MachineLearning/comments/549rb7/tensorflow_tfreshape_seems_to_behave_differently/,AlCapown3d,1474716670,,0,0
862,2016-9-24,2016,9,24,23,54a8o4,Are there theoretical proofs that depth in neural networks (i.e. nested functions) is useful - all else equal?,https://www.reddit.com/r/MachineLearning/comments/54a8o4/are_there_theoretical_proofs_that_depth_in_neural/,ranterskanter,1474725812,"Playing around I did a quick test with two versions of an autoencoder. Version 1 is vanilla, version 2 splits the parameters of encoder and decoder in two activation layers, so the whole network now has 4 layers. The deeper network does not learn faster, and does not converge on a smaller error.

I tried something similar with LSTM, splitting parameters into extra LSTM layers and training on word-index streams. Final error is no better in the deeper network.

Is there any proof that nested parametrized functions can achieve anything that a single function embracing the same number of parameters can't?",27,47
863,2016-9-25,2016,9,25,0,54agvc,"Forget virtual assistants, Asteria wants to be your AI friend",https://www.reddit.com/r/MachineLearning/comments/54agvc/forget_virtual_assistants_asteria_wants_to_be/,nathantross,1474729374,,0,1
864,2016-9-25,2016,9,25,2,54b7db,interest in studying a masters in machine learning. which courses should i take within my bachelor?,https://www.reddit.com/r/MachineLearning/comments/54b7db/interest_in_studying_a_masters_in_machine/,moppyie,1474739515,[removed],0,1
865,2016-9-25,2016,9,25,3,54bdlq,How to proceed if within-class scatter matrix is singular while doing linear discriminant analysis ?,https://www.reddit.com/r/MachineLearning/comments/54bdlq/how_to_proceed_if_withinclass_scatter_matrix_is/,akshayk0406,1474741846,[removed],1,1
866,2016-9-25,2016,9,25,3,54bgzm,Advice for Begginers - Machine Learning in a Year: From Total Noob to Effective Practitioner,https://www.reddit.com/r/MachineLearning/comments/54bgzm/advice_for_begginers_machine_learning_in_a_year/,[deleted],1474743072,[deleted],0,1
867,2016-9-25,2016,9,25,4,54bj8r,Advice for Beginners - Machine Learning in a Year: From Total Noob to Effective Practitioner,https://www.reddit.com/r/MachineLearning/comments/54bj8r/advice_for_beginners_machine_learning_in_a_year/,AshDrogo,1474743900,,0,0
868,2016-9-25,2016,9,25,4,54bk6b,Where to learn about recommender systems?,https://www.reddit.com/r/MachineLearning/comments/54bk6b/where_to_learn_about_recommender_systems/,xbomber88,1474744235,[removed],1,3
869,2016-9-25,2016,9,25,4,54bngf,Best university to do a masters in Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/54bngf/best_university_to_do_a_masters_in_machine/,amjw,1474745451,[removed],0,1
870,2016-9-25,2016,9,25,4,54bpsb,Yann LeCun - Deep Learning and the Future of AI,https://www.reddit.com/r/MachineLearning/comments/54bpsb/yann_lecun_deep_learning_and_the_future_of_ai/,CopyofacOpyofacoPyof,1474746331,,5,106
871,2016-9-25,2016,9,25,5,54btqc,SOTA in dialogue systems (using deep learning)?,https://www.reddit.com/r/MachineLearning/comments/54btqc/sota_in_dialogue_systems_using_deep_learning/,alrojo,1474747840,"Which articles are considered the state-of-the-art within Dialogue systems (and/or Q&amp;A)?

Are the seq2seq models from nmt popular?",9,9
872,2016-9-25,2016,9,25,5,54by5i,"A little old, but still useful.",https://www.reddit.com/r/MachineLearning/comments/54by5i/a_little_old_but_still_useful/,AnarchoDave,1474749471,,6,12
873,2016-9-25,2016,9,25,6,54c626,"Chemical, Genetic Engineering and Machine Learning?",https://www.reddit.com/r/MachineLearning/comments/54c626/chemical_genetic_engineering_and_machine_learning/,bangbangIshotmyself,1474752542,[removed],0,1
874,2016-9-25,2016,9,25,7,54cgzy,A.I. Doesn't Get Black Twitter,https://www.reddit.com/r/MachineLearning/comments/54cgzy/ai_doesnt_get_black_twitter/,[deleted],1474756938,[deleted],0,1
875,2016-9-25,2016,9,25,7,54ch0b,Best survey papers of the last 2 years?,https://www.reddit.com/r/MachineLearning/comments/54ch0b/best_survey_papers_of_the_last_2_years/,jakeanfinn,1474756941,[removed],0,1
876,2016-9-25,2016,9,25,8,54cnnf,Going to College in Texas?,https://www.reddit.com/r/MachineLearning/comments/54cnnf/going_to_college_in_texas/,jasikpark,1474759676,[removed],1,2
877,2016-9-25,2016,9,25,9,54cyds,Bay Area Deep Learning Enthusiasts,https://www.reddit.com/r/MachineLearning/comments/54cyds/bay_area_deep_learning_enthusiasts/,WhtWolf07,1474764412,,0,1
878,2016-9-25,2016,9,25,11,54d88u,Neural-Photo-Editor: A simple interface for editing natural photos with generative neural networks.,https://www.reddit.com/r/MachineLearning/comments/54d88u/neuralphotoeditor_a_simple_interface_for_editing/,Dogsindahouse1,1474768886,,7,67
879,2016-9-25,2016,9,25,11,54degk,A Kotlin implementation of the Pair Adjacent Violators algorithm for isotonic regression,https://www.reddit.com/r/MachineLearning/comments/54degk/a_kotlin_implementation_of_the_pair_adjacent/,sanity,1474771779,,7,2
880,2016-9-25,2016,9,25,16,54e9mi,someone help me with ML?,https://www.reddit.com/r/MachineLearning/comments/54e9mi/someone_help_me_with_ml/,Timelord_42,1474790326,[removed],7,0
881,2016-9-25,2016,9,25,17,54ebgy,classification and clustering algorithms,https://www.reddit.com/r/MachineLearning/comments/54ebgy/classification_and_clustering_algorithms/,dataaspirant,1474791718,,0,1
882,2016-9-25,2016,9,25,18,54egs6,Applying CNN for Urban Sound Classification,https://www.reddit.com/r/MachineLearning/comments/54egs6/applying_cnn_for_urban_sound_classification/,[deleted],1474795843,[deleted],0,1
883,2016-9-25,2016,9,25,19,54eno8,"In your opinion, what are the top 5 data mining inventions or applications in the world?",https://www.reddit.com/r/MachineLearning/comments/54eno8/in_your_opinion_what_are_the_top_5_data_mining/,dotop23,1474801108,[removed],4,7
884,2016-9-25,2016,9,25,21,54ey4y,"Videos: HORSE2016 workshop, On Horses and Potemkin Villages in Applied Machine Learning",https://www.reddit.com/r/MachineLearning/comments/54ey4y/videos_horse2016_workshop_on_horses_and_potemkin/,compsens,1474807619,,0,18
885,2016-9-25,2016,9,25,21,54ezif,Applying CNN for Urban Sound Classification,https://www.reddit.com/r/MachineLearning/comments/54ezif/applying_cnn_for_urban_sound_classification/,a_endurance,1474808373,,2,31
886,2016-9-25,2016,9,25,23,54f8x1,Need to prepare for presentation on Predictive Analysis in Healthcare,https://www.reddit.com/r/MachineLearning/comments/54f8x1/need_to_prepare_for_presentation_on_predictive/,RageAdi,1474812694,[removed],0,1
887,2016-9-25,2016,9,25,23,54feu1,ML driven Slack Bot that summarizes any URL or text,https://www.reddit.com/r/MachineLearning/comments/54feu1/ml_driven_slack_bot_that_summarizes_any_url_or/,[deleted],1474815219,[deleted],0,2
888,2016-9-26,2016,9,26,0,54fivm,Is Toronto next for autonomous Uber vehicles?,https://www.reddit.com/r/MachineLearning/comments/54fivm/is_toronto_next_for_autonomous_uber_vehicles/,vveerrgg,1474816788,,0,2
889,2016-9-26,2016,9,26,0,54fkt5,"Learning an efficient mapping of URL  Beta(a,b)",https://www.reddit.com/r/MachineLearning/comments/54fkt5/learning_an_efficient_mapping_of_url_betaab/,AAAVR,1474817539,[removed],0,1
890,2016-9-26,2016,9,26,1,54fw7g,Greyscale image colourization using GANs,https://www.reddit.com/r/MachineLearning/comments/54fw7g/greyscale_image_colourization_using_gans/,ASTVisitor,1474821765,[removed],3,1
891,2016-9-26,2016,9,26,1,54fx3j,Does updating libraries affect floating point algorithm results?,https://www.reddit.com/r/MachineLearning/comments/54fx3j/does_updating_libraries_affect_floating_point/,cheater00,1474822098,[removed],0,1
892,2016-9-26,2016,9,26,2,54g0aw,Session with Anthony Goldbloom - Co-founder and CEO of Kaggle,https://www.reddit.com/r/MachineLearning/comments/54g0aw/session_with_anthony_goldbloom_cofounder_and_ceo/,shagunsodhani,1474823242,,0,12
893,2016-9-26,2016,9,26,3,54galu,Gentle introduction into WordNet with NLTK,https://www.reddit.com/r/MachineLearning/comments/54galu/gentle_introduction_into_wordnet_with_nltk/,khozzy,1474826894,,0,1
894,2016-9-26,2016,9,26,5,54gwji,ML graduate programs where research is inspired by how the human brain works?,https://www.reddit.com/r/MachineLearning/comments/54gwji/ml_graduate_programs_where_research_is_inspired/,baracatwp7,1474834693,[removed],2,1
895,2016-9-26,2016,9,26,5,54h0yl,Crowdsourced machine learning?,https://www.reddit.com/r/MachineLearning/comments/54h0yl/crowdsourced_machine_learning/,NEED_A_JACKET,1474836195,"Does this already exist? Could it work as a concept?

I'm picturing an AI based simple game, maybe something where you're competing to ""evolve"" AI by giving it various tasks to learn from, different ""goals"", maybe some kind of level cap for how many generations you are allowed to compete with others (so that it's fair). As an example, maybe something along the lines of a battlefield for 10000th generationers, where the winner would be the one who gave the best training parameters (eg maybe the majority of the training was focused on moving and hiding, as opposed to attacking, where the user sets up the training scenarios and percentages of the evolution for each).

And to extend this, I'm wondering if something exists that utilizes what the individual ""machines"" have learnt, so that people playing the game would contribute to a larger scale project. Or, alternatively, allowing users to ""opt in"" to use their computer down time for calculations on a larger separate project. I'm sure a lot of people who are interested in evolution based AI games would be willing to 'donate' their spare computation (even whilst just running the game) to a more complex / computationally demanding project.",7,2
896,2016-9-26,2016,9,26,6,54h7a2,Poll: Which are the best talks from the Bay Area DL school?,https://www.reddit.com/r/MachineLearning/comments/54h7a2/poll_which_are_the_best_talks_from_the_bay_area/,sour_losers,1474838428,,0,2
897,2016-9-26,2016,9,26,7,54hib5,Pareto Optimality And Multi Goal Optimization,https://www.reddit.com/r/MachineLearning/comments/54hib5/pareto_optimality_and_multi_goal_optimization/,[deleted],1474842487,[deleted],0,1
898,2016-9-26,2016,9,26,7,54hip3,Pareto Optimality And Multi Goal Optimization,https://www.reddit.com/r/MachineLearning/comments/54hip3/pareto_optimality_and_multi_goal_optimization/,[deleted],1474842642,[deleted],0,1
899,2016-9-26,2016,9,26,7,54hlc7,NVIDIA Blog: Adobe and UC Berkeley developed new deep learning based image editing and generation software.,https://www.reddit.com/r/MachineLearning/comments/54hlc7/nvidia_blog_adobe_and_uc_berkeley_developed_new/,[deleted],1474843656,[deleted],0,1
900,2016-9-26,2016,9,26,7,54hmal,"It is easier for a machine to tell another machine ""what"" to do than ""how"" to do.",https://www.reddit.com/r/MachineLearning/comments/54hmal/it_is_easier_for_a_machine_to_tell_another/,rovercaps,1474844048,,0,1
901,2016-9-26,2016,9,26,8,54hv5g,Setup Keras+Theano Backend and GPU on Ubuntu 16.04,https://www.reddit.com/r/MachineLearning/comments/54hv5g/setup_kerastheano_backend_and_gpu_on_ubuntu_1604/,orange_robot338,1474847474,,0,1
902,2016-9-26,2016,9,26,10,54i68j,Machine Learning Theory - Part 2: Generalization Bounds,https://www.reddit.com/r/MachineLearning/comments/54i68j/machine_learning_theory_part_2_generalization/,mostafa-samir,1474851816,,3,93
903,2016-9-26,2016,9,26,10,54i7ne,Robust Optical Flow using Recurrent Flow Network,https://www.reddit.com/r/MachineLearning/comments/54i7ne/robust_optical_flow_using_recurrent_flow_network/,samchoi7,1474852367,,0,20
904,2016-9-26,2016,9,26,10,54i8g9,My favorite lecture series from Caltech... so meta,https://www.reddit.com/r/MachineLearning/comments/54i8g9/my_favorite_lecture_series_from_caltech_so_meta/,ravasheera,1474852691,,4,0
905,2016-9-26,2016,9,26,10,54ia4g,"HMMMM... REALLY makes you think about ""in sample"" vs ""out of sample"", and also competitive extreme cuckoldry",https://www.reddit.com/r/MachineLearning/comments/54ia4g/hmmmm_really_makes_you_think_about_in_sample_vs/,ravasheera,1474853368,,8,0
906,2016-9-26,2016,9,26,12,54ipv1,Bo hnh my git LG ti H Ni,https://www.reddit.com/r/MachineLearning/comments/54ipv1/bo_hnh_my_git_lg_ti_h_ni/,hoaiphuongrao3s,1474859798,,0,1
907,2016-9-26,2016,9,26,12,54iq4g,Does anyone know why the bay area deep learning school talk about deep reinforcement learning from john schulman was omitted?,https://www.reddit.com/r/MachineLearning/comments/54iq4g/does_anyone_know_why_the_bay_area_deep_learning/,darkconfidantislife,1474859923,"I've been trying to find John Schulman's DRL talk at bay area DL school. It was supposed to be on day two, but when I start day 2, I don't see anything about it.
Was it omitted?",6,1
908,2016-9-26,2016,9,26,12,54iquh,So speaking of the most machine-learny and psychosomatically uncomfortable-to-watch VIDEO ever....,https://www.reddit.com/r/MachineLearning/comments/54iquh/so_speaking_of_the_most_machinelearny_and/,ravasheera,1474860225,,0,1
909,2016-9-26,2016,9,26,12,54irpu,Interesting Algorithm from Research Paper to Implement,https://www.reddit.com/r/MachineLearning/comments/54irpu/interesting_algorithm_from_research_paper_to/,[deleted],1474860615,[removed],1,1
910,2016-9-26,2016,9,26,12,54ivaz,Where do I find documentation of OpenAI gym ?,https://www.reddit.com/r/MachineLearning/comments/54ivaz/where_do_i_find_documentation_of_openai_gym/,mundada,1474862269,[removed],0,1
911,2016-9-26,2016,9,26,16,54jiyy,Large Scale Visual Recognition Challenge 2016 - Results finally available,https://www.reddit.com/r/MachineLearning/comments/54jiyy/large_scale_visual_recognition_challenge_2016/,DrPharael,1474875462,,28,32
912,2016-9-26,2016,9,26,17,54jo68,Do you know about any interesting application of machine learning to dieting?,https://www.reddit.com/r/MachineLearning/comments/54jo68/do_you_know_about_any_interesting_application_of/,[deleted],1474879086,[deleted],0,1
913,2016-9-26,2016,9,26,17,54jond,Using Predictive Analytics to Decrease User Churn,https://www.reddit.com/r/MachineLearning/comments/54jond/using_predictive_analytics_to_decrease_user_churn/,roachhhh,1474879435,[removed],0,0
914,2016-9-26,2016,9,26,17,54jpia,Do you know about any interesting application of machine learning to dieting?,https://www.reddit.com/r/MachineLearning/comments/54jpia/do_you_know_about_any_interesting_application_of/,michal_sustr,1474880036,[removed],2,0
915,2016-9-26,2016,9,26,18,54jt18,Why would one use GANs instead of NCE?,https://www.reddit.com/r/MachineLearning/comments/54jt18/why_would_one_use_gans_instead_of_nce/,dspoka,1474882388,[removed],0,1
916,2016-9-26,2016,9,26,18,54jurk,Machine Learning Key to Building a Proactive Security Response by SPLUNK,https://www.reddit.com/r/MachineLearning/comments/54jurk/machine_learning_key_to_building_a_proactive/,hardikjoradan,1474883523,,0,1
917,2016-9-26,2016,9,26,19,54jwxu,Update to my post about detecting music BPM using a ConvNet - I'm using the raw audio now and getting some interesting results,https://www.reddit.com/r/MachineLearning/comments/54jwxu/update_to_my_post_about_detecting_music_bpm_using/,lfotofilter,1474884832,,6,24
918,2016-9-26,2016,9,26,19,54k0sn,Webinar Discussing ESI MINESET: A Cloud Based Machine Learning and Predictive Analytics Platform.,https://www.reddit.com/r/MachineLearning/comments/54k0sn/webinar_discussing_esi_mineset_a_cloud_based/,cloudsim738,1474887195,,0,1
919,2016-9-26,2016,9,26,20,54k2zn,Generative Adversarial Networks Battling During Training,https://www.reddit.com/r/MachineLearning/comments/54k2zn/generative_adversarial_networks_battling_during/,LeavesBreathe,1474888446,"Hey Guys,

I wanted to run an idea and understand why this hasn't been tested. In a recent [GAN paper](https://arxiv.org/abs/1602.05110), they explore the idea of battling two separate GAN models. The goal is to yield some sort of metric to compare generators. 

However, I was wondering why we do not do this during training itself? The protocol would go as follows:

	Create GAN1 and GAN2 -- Train them in isolation from each other

	For x in range(10):

		Switch Discriminators between models constantly back and forth

		Train GAN1 and GAN2 in isolation (randomly choose which discriminator each should have)

	Training Concluded


As you can imagine, you could add GAN3 and add more combinations of battles. Here are my thoughts:

**Benefits of Battling During Training**

- Have more diversity in both generator and discriminator approaches. You can even create completely different architectures for both the generators and discriminators. For example, one generator could be recurrent and the other could be vanilla DCGAN.

**Downsides of Battling During Training**

- I would imagine that this would at least double if not triple the training time. However, on Four Titan X's, these models usually train in about a day. A three day training time is not that bad. 

- Could be more efficient in the end to just train one generator/discriminator model as you can afford more parameters in this single model. Multiple contestants requires more memory.

Curious to hear your guys thoughts on this! Thanks",9,5
920,2016-9-26,2016,9,26,20,54k601,NIPS workshops,https://www.reddit.com/r/MachineLearning/comments/54k601/nips_workshops/,alrojo,1474890050,[removed],0,1
921,2016-9-26,2016,9,26,21,54k8ps,The rapid evolution of open-source machine learning,https://www.reddit.com/r/MachineLearning/comments/54k8ps/the_rapid_evolution_of_opensource_machine_learning/,ahousley,1474891524,,0,1
922,2016-9-26,2016,9,26,21,54kbsp,720p 25fps neural style video Test,https://www.reddit.com/r/MachineLearning/comments/54kbsp/720p_25fps_neural_style_video_test/,Sportinger,1474892850,,34,250
923,2016-9-26,2016,9,26,21,54kctd,Places to apply for machine learning / computer vision internships?,https://www.reddit.com/r/MachineLearning/comments/54kctd/places_to_apply_for_machine_learning_computer/,[deleted],1474893289,[removed],0,1
924,2016-9-26,2016,9,26,22,54kgzg,Generating Faces with Deconvolution Networks,https://www.reddit.com/r/MachineLearning/comments/54kgzg/generating_faces_with_deconvolution_networks/,z_o_7,1474895138,,10,82
925,2016-9-26,2016,9,26,22,54knz1,"Finally found a proper pirated stream of this to pirate..... machine learn THIS, my ""african american friends""!",https://www.reddit.com/r/MachineLearning/comments/54knz1/finally_found_a_proper_pirated_stream_of_this_to/,ravasheera,1474897925,,0,0
926,2016-9-26,2016,9,26,22,54koqf,The data processing inequality,https://www.reddit.com/r/MachineLearning/comments/54koqf/the_data_processing_inequality/,akelleh,1474898208,,0,1
927,2016-9-26,2016,9,26,23,54kqua,Which are the best talks from the Bay Area DL school?,https://www.reddit.com/r/MachineLearning/comments/54kqua/which_are_the_best_talks_from_the_bay_area_dl/,sour_losers,1474898954,,0,2
928,2016-9-26,2016,9,26,23,54ksgo,"I SAID, machine learn THIS! Glove slap, baby glove slap! Glove slap, shut yer big yap!",https://www.reddit.com/r/MachineLearning/comments/54ksgo/i_said_machine_learn_this_glove_slap_baby_glove/,ravasheera,1474899518,,0,0
929,2016-9-26,2016,9,26,23,54ksk8,[1609.07152] Input Convex Neural Networks,https://www.reddit.com/r/MachineLearning/comments/54ksk8/160907152_input_convex_neural_networks/,bdamos,1474899556,,11,11
930,2016-9-26,2016,9,26,23,54ktka,Why are Gradient Boosting Models poor at making predictions?,https://www.reddit.com/r/MachineLearning/comments/54ktka/why_are_gradient_boosting_models_poor_at_making/,andtheninthefog,1474899910,"I've been running a few algorithms through a data set to predict the frequency of a phenomena. I have found on the internet that GBMs serve as poor predictors in extrapolation. However, I can't seem to understand, intuitively, why. If GBMs are poor predictors, can the same be said about Random Forests?

Edit: I do apologize for the confusion. I am referring to extrapolating a model for future values. ",4,0
931,2016-9-26,2016,9,26,23,54ku7m,Designing Apparel with Neural Style Transfer,https://www.reddit.com/r/MachineLearning/comments/54ku7m/designing_apparel_with_neural_style_transfer/,victor_haydin,1474900147,,0,1
932,2016-9-26,2016,9,26,23,54kvsu,Machine Learning - WAYR (What Are You Reading) - Week 9,https://www.reddit.com/r/MachineLearning/comments/54kvsu/machine_learning_wayr_what_are_you_reading_week_9/,Mandrathax,1474900689,"This is a place to share machine learning research papers, journals, and articles that you're reading this week.
If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.  

|Previous weeks|
|--------------|
|[Week 1](https://www.reddit.com/r/MachineLearning/comments/4qyjiq/machine_learning_wayr_what_are_you_reading_week_1/)|  
|[Week 2](https://www.reddit.com/r/MachineLearning/comments/4s2xqm/machine_learning_wayr_what_are_you_reading_week_2/)|  
|[Week 3](https://www.reddit.com/r/MachineLearning/comments/4t7mqm/machine_learning_wayr_what_are_you_reading_week_3/)|  
|[Week 4](https://www.reddit.com/r/MachineLearning/comments/4ub2kw/machine_learning_wayr_what_are_you_reading_week_4/)| 
|[Week 5](https://www.reddit.com/r/MachineLearning/comments/4xomf7/machine_learning_wayr_what_are_you_reading_week_5/)| 
|[Week 6](https://www.reddit.com/r/MachineLearning/comments/4zcyvk/machine_learning_wayr_what_are_you_reading_week_6/)|
|[Week 7](https://www.reddit.com/r/MachineLearning/comments/52t6mo/machine_learning_wayr_what_are_you_reading_week_7/)|
|[Week 8](https://www.reddit.com/r/MachineLearning/comments/53heol/machine_learning_wayr_what_are_you_reading_week_8/)|

Most upvoted papers last week : 

[Variational inference for Monte Carlo objectives](https://arxiv.org/abs/1602.06725)

[Playing FPS Games with Deep Reinforcement Learning](https://arxiv.org/abs/1609.05521v1)

[Iterative Gaussianization: from ICA to Random Rotations](https://arxiv.org/abs/1602.00229)

[Density Modeling of Images using a Generalized Normalization Transformation](https://arxiv.org/abs/1511.06281)

Besides that, there are no rules, have fun.

EDIT : leaving the thread here one more week.",4,20
933,2016-9-26,2016,9,26,23,54kz1y,Solving Common Issues in Distributed Hyperparameter Optimization,https://www.reddit.com/r/MachineLearning/comments/54kz1y/solving_common_issues_in_distributed/,alexcmu,1474901818,,0,1
934,2016-9-27,2016,9,27,0,54l0uy,"BigData and ML Toolset and Model Weekly Roundup  Sep. 26, 2016",https://www.reddit.com/r/MachineLearning/comments/54l0uy/bigdata_and_ml_toolset_and_model_weekly_roundup/,stkim1,1474902407,,0,1
935,2016-9-27,2016,9,27,0,54l9bx,"IntelAct: Winner, Visual Doom AI Competition, Full Deathmatch",https://www.reddit.com/r/MachineLearning/comments/54l9bx/intelact_winner_visual_doom_ai_competition_full/,downtownslim,1474905176,,25,30
936,2016-9-27,2016,9,27,1,54lfzr,Why Deep Learning Works: Perspectives from Theoretical Chemistry,https://www.reddit.com/r/MachineLearning/comments/54lfzr/why_deep_learning_works_perspectives_from/,machiner_ps,1474907270,,0,1
937,2016-9-27,2016,9,27,1,54ljts,Deep Learning Recommender System that uses autoencoders,https://www.reddit.com/r/MachineLearning/comments/54ljts/deep_learning_recommender_system_that_uses/,klizardin,1474908470,,0,1
938,2016-9-27,2016,9,27,1,54llyj,Is it a bad idea to read Tom Mitchell's ML book these days?,https://www.reddit.com/r/MachineLearning/comments/54llyj/is_it_a_bad_idea_to_read_tom_mitchells_ml_book/,robinhoodchess,1474909145,"Hi all,

So I have poked around in ESL, the Bishop book, Peter Flach's ML book, Alpaydin's book, Norvig's book, and the Kuhn book. Some chapters and bits here and there, so I kinda get the sense of the style of authors and what they're good at explaining and what they're not great at. 

These past few days I have finally gotten around to Tom Mitchell's book on machine learning and so far it's fantastic. Great mix of math and diagrams, very wordy, a bit repetitive (but in a good way) so that I know what's important to remember, and so on. 

But the thing was published in 1997. 19 years ago, is it outdated in this fast-paced field? I mean, does it hurt the reader to go through the material today or is it still good for what it covers? Oh and I don't mean the chapters which haven't changed much over the years in theory like bayesian, hypothesis testing, decision trees, and so on. I started the book in the ANN section and it's definitely the most comfortable approach I've seen to explaining the subject. 

Btw I saw on this website that he has some new chapters available for download (written in 2015 and 2016), but I'm just talking about the printed copy from way back then.",5,1
939,2016-9-27,2016,9,27,2,54lraz,Saving partial Machine Learning results with Checkpoints,https://www.reddit.com/r/MachineLearning/comments/54lraz/saving_partial_machine_learning_results_with/,yurii-shevchuk,1474910836,,0,1
940,2016-9-27,2016,9,27,2,54lvpq,Question about next job expectations,https://www.reddit.com/r/MachineLearning/comments/54lvpq/question_about_next_job_expectations/,[deleted],1474912206,[removed],0,1
941,2016-9-27,2016,9,27,2,54lx9g,Machine Learnings #10,https://www.reddit.com/r/MachineLearning/comments/54lx9g/machine_learnings_10/,beeftug,1474912690,,0,0
942,2016-9-27,2016,9,27,3,54m3bd,Beginner Question: Lambda &amp; Model Selection,https://www.reddit.com/r/MachineLearning/comments/54m3bd/beginner_question_lambda_model_selection/,beboophiphop,1474914560,[removed],0,1
943,2016-9-27,2016,9,27,3,54m7gu,How do Convolutional Neural Networks work?,https://www.reddit.com/r/MachineLearning/comments/54m7gu/how_do_convolutional_neural_networks_work/,mikeyt21,1474915905,,1,27
944,2016-9-27,2016,9,27,4,54m9qs,PAY IT FORWARD!!,https://www.reddit.com/r/MachineLearning/comments/54m9qs/pay_it_forward/,deepakmurugaian,1474916608,[removed],0,1
945,2016-9-27,2016,9,27,4,54mj9k,"Tensorflow v0.10 installed from scratch on Ubuntu 16.04, CUDA 8.0RC+Patch, cuDNN v5.1 with a 1080GTX",https://www.reddit.com/r/MachineLearning/comments/54mj9k/tensorflow_v010_installed_from_scratch_on_ubuntu/,mar_cnu,1474919566,,21,68
946,2016-9-27,2016,9,27,4,54mjbd,Empirical - A framework to easily run and replicate experiments,https://www.reddit.com/r/MachineLearning/comments/54mjbd/empirical_a_framework_to_easily_run_and_replicate/,empiricalci,1474919583,,2,8
947,2016-9-27,2016,9,27,5,54ml8k,A new rudimentary machine learning exercise for you guys - WHAT COULD IT MEAN????? :D,https://www.reddit.com/r/MachineLearning/comments/54ml8k/a_new_rudimentary_machine_learning_exercise_for/,ravasheera,1474920184,,0,1
948,2016-9-27,2016,9,27,5,54mlvf,You Too Can Become a Machine Learning Rock Star! No PhD Necessary.,https://www.reddit.com/r/MachineLearning/comments/54mlvf/you_too_can_become_a_machine_learning_rock_star/,thiseye,1474920381,,1,1
949,2016-9-27,2016,9,27,5,54mnjl,"I AM NOT A ROBOT YOU IDIOTS, YOUR CAPTCHAS DON'T WORK. LEARN SOME FUCKING MACHINE LEARNING FOR ONCE IN YOUR LIVES",https://www.reddit.com/r/MachineLearning/comments/54mnjl/i_am_not_a_robot_you_idiots_your_captchas_dont/,ravasheera,1474920911,,1,1
950,2016-9-27,2016,9,27,5,54moh6,Does anyone know what will be the top 20-50 papers to replicate to become a better ML researcher?,https://www.reddit.com/r/MachineLearning/comments/54moh6/does_anyone_know_what_will_be_the_top_2050_papers/,ajingnk,1474921186,[removed],0,1
951,2016-9-27,2016,9,27,5,54moux,Library to benchmark vulnerability to adversarial examples,https://www.reddit.com/r/MachineLearning/comments/54moux/library_to_benchmark_vulnerability_to_adversarial/,gwulfs,1474921297,,0,9
952,2016-9-27,2016,9,27,5,54mpby,"CreativeAI.net is a space to share Creative AI projects: bots that paint, write stories, compose music, design objects, and so on",https://www.reddit.com/r/MachineLearning/comments/54mpby/creativeainet_is_a_space_to_share_creative_ai/,urish,1474921455,,1,27
953,2016-9-27,2016,9,27,5,54mpwv,Facebook Research release CommAI-env: A platform for developing AI systems,https://www.reddit.com/r/MachineLearning/comments/54mpwv/facebook_research_release_commaienv_a_platform/,urish,1474921620,,4,31
954,2016-9-27,2016,9,27,5,54mtre,Baidu Open Sources New Yardstick for Deep Learning Hardware Makers,https://www.reddit.com/r/MachineLearning/comments/54mtre/baidu_open_sources_new_yardstick_for_deep/,[deleted],1474922835,[deleted],2,8
955,2016-9-27,2016,9,27,6,54mwlm,What's the Sep 2016 Most Cost Effective GPU Platform?,https://www.reddit.com/r/MachineLearning/comments/54mwlm/whats_the_sep_2016_most_cost_effective_gpu/,dakami,1474923755,[removed],2,0
956,2016-9-27,2016,9,27,6,54mwwu,"Baidu Research Announces DeepBench, Open Source Deep Learning Benchmark",https://www.reddit.com/r/MachineLearning/comments/54mwwu/baidu_research_announces_deepbench_open_source/,bayjingsf,1474923851,,0,2
957,2016-9-27,2016,9,27,7,54ndgf,What is everyone using to generate these style of network diagrams for their papers? They all seem to come from the same package.,https://www.reddit.com/r/MachineLearning/comments/54ndgf/what_is_everyone_using_to_generate_these_style_of/,[deleted],1474929368,[deleted],0,1
958,2016-9-27,2016,9,27,7,54nea7,Best ML courses to buy on Udemy (ongoing $10 offer),https://www.reddit.com/r/MachineLearning/comments/54nea7/best_ml_courses_to_buy_on_udemy_ongoing_10_offer/,theironhide,1474929667,[removed],0,1
959,2016-9-27,2016,9,27,8,54nl8k,What could it mean? Maybe your power of machine learning can tell you. Sure makes you think....,https://www.reddit.com/r/MachineLearning/comments/54nl8k/what_could_it_mean_maybe_your_power_of_machine/,ravasheera,1474932132,,0,1
960,2016-9-27,2016,9,27,8,54nn5j,ITT some nine-tailed demon fox named Uzumaki something or other reks my browser with the sheer limitless power of his masturbatory narcissism,https://www.reddit.com/r/MachineLearning/comments/54nn5j/itt_some_ninetailed_demon_fox_named_uzumaki/,ravasheera,1474932826,,0,1
961,2016-9-27,2016,9,27,8,54noq9,Please review: Threshold-based Reinforcement Learning in Neural Networks,https://www.reddit.com/r/MachineLearning/comments/54noq9/please_review_thresholdbased_reinforcement/,thomas_h_ward,1474933395,"A Reinforcement Learning scheme is presented, enabling Autonomous Agent control (including an Arduino robot via bluetooth). I would very much appreciate feedback on both the utility and biological/psychological plausibility of these features. I would then like to revise the paper and add acknowledgements accordingly.

**BEFORE YOU READ** 
The paper was written for the purpose of discussing the validity (or not) of the 3 features it provides in terms of utility and plausibility (if you feel the plausibility concerns are a distraction they can be safely ignored). The experiments are small in scale yet big enough for demonstration, while the scheme itself is readily scalable and could be incorporated with other features (eg CNN's, LSTM). The underlying RL mechanism differs significantly from current popular RL schemes (eg Q-Learning). It avoids 'tenuous' constructs (arrays of prior state, models of the environment, backward propagation), instead the network simply relies on a threshold and it's own learnt mappings; I would like your opinion on how it succeeds in these terms. It is also a general solution rather than one designed for a specific closed world; like natural agents there is no guarantee the shortest path will be chosen. 

The following steps were taken:
1) A Backpropagation network is placed inside a framework that dynamically generates desired output patterns based on a reward function; this achieves Primary Reinforcement. The network is capable of deep learning but cannot form long-term strategy. 
2) The network is modified such that it can create mappings back to the reward function; this achieves Conditioned Reinforcement. The network can now also form long-term strategy.
3) The Backpropagation algorithm is replaced with a 'plausible' algorithm that uses direct on-node error derivation for weight updates.

https://arxiv.org/abs/1609.03348

If there is sufficient interest the source code can be tidied and made available.

Please let me know if more detail or clarification is required.

Kinds regards,

Thomas Ward

PS Negative criticism is welcome, just please keep it constructive.. The purpose of this review is to accept/revise/reject the ideas and improve understanding.
",4,4
962,2016-9-27,2016,9,27,8,54np72,"How did I say it on 4chan? Something like, ""literally Al-Qaeda the anime"" lol",https://www.reddit.com/r/MachineLearning/comments/54np72/how_did_i_say_it_on_4chan_something_like/,ravasheera,1474933562,,0,1
963,2016-9-27,2016,9,27,8,54npb1,Nuts and Bolts of Applying Deep Learning - a summary of Andrew Ng's talk at the Bay Area Deep Learning School (2016),https://www.reddit.com/r/MachineLearning/comments/54npb1/nuts_and_bolts_of_applying_deep_learning_a/,kevinzakka,1474933599,,7,49
964,2016-9-27,2016,9,27,9,54nxxb,"Here, a slightly less doxxed up version for your Machine Learning amusement",https://www.reddit.com/r/MachineLearning/comments/54nxxb/here_a_slightly_less_doxxed_up_version_for_your/,ravasheera,1474936798,,0,1
965,2016-9-27,2016,9,27,10,54o2l6,[1609.07236] On the (im)possibility of fairness,https://www.reddit.com/r/MachineLearning/comments/54o2l6/160907236_on_the_impossibility_of_fairness/,EnsembleAllTheThings,1474938521,,1,8
966,2016-9-27,2016,9,27,10,54o9p1,[1609.07843] Pointer Sentinel Mixture Models; sota for language modeling while using less parameters than lstms,https://www.reddit.com/r/MachineLearning/comments/54o9p1/160907843_pointer_sentinel_mixture_models_sota/,evc123,1474941182,,4,9
967,2016-9-27,2016,9,27,11,54ob6q,PRACTICAL METHODOLOGY WITH IAN GOODFELLOW (LIVE STREAM),https://www.reddit.com/r/MachineLearning/comments/54ob6q/practical_methodology_with_ian_goodfellow_live/,potato_potaro,1474941729,,6,12
968,2016-9-27,2016,9,27,11,54oju9,[1609.08144] Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,https://www.reddit.com/r/MachineLearning/comments/54oju9/160908144_googles_neural_machine_translation/,RushAndAPush,1474944884,,18,102
969,2016-9-27,2016,9,27,12,54oshl,Has anyone used Fuel? What was your experience with it?,https://www.reddit.com/r/MachineLearning/comments/54oshl/has_anyone_used_fuel_what_was_your_experience/,darkconfidantislife,1474948246,[removed],0,3
970,2016-9-27,2016,9,27,13,54ox7r,Coursera relaunched the Probabilistic Graphical Models by Daphne Koller,https://www.reddit.com/r/MachineLearning/comments/54ox7r/coursera_relaunched_the_probabilistic_graphical/,The_Man_of_Science,1474950249,,24,72
971,2016-9-27,2016,9,27,13,54oy7s,Please help me with Price Optimization &amp; Supply Chain Optimization?,https://www.reddit.com/r/MachineLearning/comments/54oy7s/please_help_me_with_price_optimization_supply/,NotThatLebowski,1474950671,[removed],0,1
972,2016-9-27,2016,9,27,14,54p3ub,bonsai a language to make programming neural nets easier,https://www.reddit.com/r/MachineLearning/comments/54p3ub/bonsai_a_language_to_make_programming_neural_nets/,back_ache,1474953478,,0,2
973,2016-9-27,2016,9,27,14,54p4eq,Baidu on Deep Speech (great for beginners) [PDF],https://www.reddit.com/r/MachineLearning/comments/54p4eq/baidu_on_deep_speech_great_for_beginners_pdf/,j_lyf,1474953768,,0,1
974,2016-9-27,2016,9,27,14,54p6zq,[1609.07061] Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations,https://www.reddit.com/r/MachineLearning/comments/54p6zq/160907061_quantized_neural_networks_training/,downtownslim,1474955099,,9,13
975,2016-9-27,2016,9,27,15,54pb57,Global Human Machine Interface Industry growth to be driven by Technological Advancements,https://www.reddit.com/r/MachineLearning/comments/54pb57/global_human_machine_interface_industry_growth_to/,nandini_14,1474957416,,0,1
976,2016-9-27,2016,9,27,15,54pco6,How to make Python performant for AI challenges (fast data structures)?,https://www.reddit.com/r/MachineLearning/comments/54pco6/how_to_make_python_performant_for_ai_challenges/,pvkooten,1474958304,[removed],2,0
977,2016-9-27,2016,9,27,16,54pkd2,"Artificial neural networks software, kdnuggets",https://www.reddit.com/r/MachineLearning/comments/54pkd2/artificial_neural_networks_software_kdnuggets/,martinanalytics,1474963084,,0,1
978,2016-9-27,2016,9,27,18,54psaq,What is the food mixer uses?,https://www.reddit.com/r/MachineLearning/comments/54psaq/what_is_the_food_mixer_uses/,mixmachinery,1474968516,,1,1
979,2016-9-27,2016,9,27,19,54py42,Interesting Topic Request,https://www.reddit.com/r/MachineLearning/comments/54py42/interesting_topic_request/,icancto,1474972274,[removed],2,1
980,2016-9-27,2016,9,27,19,54pza8,Keras feature: ASCII prints for sequential models - should I make one?,https://www.reddit.com/r/MachineLearning/comments/54pza8/keras_feature_ascii_prints_for_sequential_models/,pmigdal,1474972971,,1,0
981,2016-9-27,2016,9,27,19,54q04f,Generate Sound from Image question,https://www.reddit.com/r/MachineLearning/comments/54q04f/generate_sound_from_image_question/,gabegabe6,1474973451,[removed],1,1
982,2016-9-27,2016,9,27,19,54q058,Examples of Non-linear Online algorithms for classification ?,https://www.reddit.com/r/MachineLearning/comments/54q058/examples_of_nonlinear_online_algorithms_for/,anujgupta82,1474973466,[removed],0,1
983,2016-9-27,2016,9,27,20,54q444,Multi GPU performance?,https://www.reddit.com/r/MachineLearning/comments/54q444/multi_gpu_performance/,benkitty,1474975776,[removed],0,2
984,2016-9-27,2016,9,27,21,54qagm,Need labelled Portuguese dataset,https://www.reddit.com/r/MachineLearning/comments/54qagm/need_labelled_portuguese_dataset/,sabse_bada_intellect,1474978854,[removed],0,0
985,2016-9-27,2016,9,27,21,54qd17,Love to Light a Fire Without Matches,https://www.reddit.com/r/MachineLearning/comments/54qd17/love_to_light_a_fire_without_matches/,[deleted],1474979978,[deleted],0,1
986,2016-9-27,2016,9,27,21,54qd8b,Love to Light a Fire Without Matches,https://www.reddit.com/r/MachineLearning/comments/54qd8b/love_to_light_a_fire_without_matches/,choco-berry,1474980070,,0,3
987,2016-9-27,2016,9,27,22,54qib7,The Coursera course on Process Mining is about to start,https://www.reddit.com/r/MachineLearning/comments/54qib7/the_coursera_course_on_process_mining_is_about_to/,[deleted],1474982176,[removed],0,1
988,2016-9-27,2016,9,27,22,54qotg,The zen of gradient descent,https://www.reddit.com/r/MachineLearning/comments/54qotg/the_zen_of_gradient_descent/,NicolasGuacamole,1474984767,,12,103
989,2016-9-27,2016,9,27,23,54qxq2,The Fovea as an Emergent Property of Visual Attention in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/54qxq2/the_fovea_as_an_emergent_property_of_visual/,reworksophie,1474988061,,0,1
990,2016-9-27,2016,9,27,23,54qygd,Best algorithm for training an energy disaggregation (non-intrusive load monitoring) model?,https://www.reddit.com/r/MachineLearning/comments/54qygd/best_algorithm_for_training_an_energy/,gary_feesher2,1474988325,"Hi all,
I am in an interesting situation where I am the resident of an ultra-renewable energy home (for research purposes).

We are collecting electricity usage data from every outlet, every appliance, and every load in the home. We are also collecting flow meter reading from just about every faucet and water source in the home. So we also know how much water is being used (and where it is being used). I have months and months of data, down to the average power usage per minute.
Obviously, from a research perspective, this is a gold-mine of data.

I am performing a self-study of machine learning and data analytics, and I am interested in seeing how I can use this data to help me answer questions about the home. 

I was wondering if anyone can suggest an approach using machine learning/pattern recognition to perform energy disaggregation (aka, non-intrusive load monitoring), which takes a whole-home energy signal and separates it into the component appliances / loads.

Since I have the average power over time (inputs) and specific loads (my classification targets), I think that I could train a model to recognize the type of load based on the average power characteristic.

But I was wondering if any of you had any interesting questions that could be answered using ML or pattern recognition for this type of application, or other suggestions of how to implement ML into a home that produces this much data!",1,2
991,2016-9-28,2016,9,28,0,54qz6t,A fastText-based hybrid recommender,https://www.reddit.com/r/MachineLearning/comments/54qz6t/a_fasttextbased_hybrid_recommender/,benjaminwilson,1474988583,,0,8
992,2016-9-28,2016,9,28,0,54r23h,You can use `whereami` to predict where you are indoors [X-post],https://www.reddit.com/r/MachineLearning/comments/54r23h/you_can_use_whereami_to_predict_where_you_are/,pvkooten,1474989590,,0,6
993,2016-9-28,2016,9,28,0,54r2x6,Machine Learning internships within Europe?,https://www.reddit.com/r/MachineLearning/comments/54r2x6/machine_learning_internships_within_europe/,agentFunction,1474989867,[removed],0,5
994,2016-9-28,2016,9,28,1,54rgjz,How would I avoid bias in a system which pre-selects its own training set?,https://www.reddit.com/r/MachineLearning/comments/54rgjz/how_would_i_avoid_bias_in_a_system_which/,kvdveer,1474994385,"A client of mine works through documents, and selects a few good ones for further processing. I've built him a system to pre-assess whether a document matches his criteria of being a good document. This system is trained using is decisions in the past, and is remarkably accurate. The system needs to be trained continuously; the criteria for a 'good' document shift over time, to allow for advances in the field, and to compensate for applicants trying to game the system. Currently the system is only used to assign priorities - eventually all documents get reviewed.

The success of the project is now about to work against me. The clients wants to use the model to filter out the bad documents, and not spend any time on those. This means that the predicted bad documents no longer get reviewed, and my training set will develop a bias towards the good documents.

I'm now looking for ways to prevent this bias. I've come up with a few solutions:

* I could randomly ignore the prediction, and use those as training input. This will annoy the client for having to deal with 'obviously bad' documents.
* I could treat a bad prediction as a bad review, but I'm afraid that such a solution would eventually become a large echo-chamber incapable of learning anything new
* I could ignore the entire bias concern, and let the system balance itself out, despite the circular setup. Similarly, I'm afraid that, over time, many 'good' documents get classified as garbage, and never reach the human review stage.

None of these options feel particularly good. How would you deal with this?",7,3
995,2016-9-28,2016,9,28,1,54rhu0,I made some (ugly) visualizations of convolutional filters in Google's recently released language model,https://www.reddit.com/r/MachineLearning/comments/54rhu0/i_made_some_ugly_visualizations_of_convolutional/,halfeatenscone,1474994833,,6,16
996,2016-9-28,2016,9,28,2,54rt4l,[1609.07072] The Many-Body Expansion Combined with Neural Networks,https://www.reddit.com/r/MachineLearning/comments/54rt4l/160907072_the_manybody_expansion_combined_with/,Maleficus187,1474998427,,12,15
997,2016-9-28,2016,9,28,2,54rtp9,"For multiclass logistic regression, what is the difference between ""one-vs-rest"" and ""multinomial""? Examples?",https://www.reddit.com/r/MachineLearning/comments/54rtp9/for_multiclass_logistic_regression_what_is_the/,[deleted],1474998615,[deleted],0,2
998,2016-9-28,2016,9,28,2,54rvaq,Coult anyone help me understand the ENet structure?,https://www.reddit.com/r/MachineLearning/comments/54rvaq/coult_anyone_help_me_understand_the_enet_structure/,Laser_Plasma,1474999122,"So, I'm kind of a newbie in deep learning, but trying to improve. I have a project to do using ENet (https://arxiv.org/abs/1606.02147), but I don't quite understand everything from the article. I have two main questions, probably stupid, but oh well:

1. In Figure 2b, what exactly is the plus sign at the bottom? Is this a concatenation or just plain simple addition (doubt it, that would be weird), or something else?

2. How exactly does Table 1 work? Are bottlenecks 2.x parallel and then concatenated at the end? Or are they executed consecutively? If it's the latter, then why doesn't the size change after the downsampling layer?

I'll be grateful for any hints anyone may have for me, as well as any other resources for working with ENet etc. (in case it matters, I'm working in TensorFlow/Python 3)",7,2
999,2016-9-28,2016,9,28,3,54rzmy,Baidu Releases Deep Learning Benchmark to help chip manufacturers,https://www.reddit.com/r/MachineLearning/comments/54rzmy/baidu_releases_deep_learning_benchmark_to_help/,InaneMembrane,1475000492,,1,3
1000,2016-9-28,2016,9,28,3,54s1ja,Yet another deep learning library - PaddlePaddle - This time by Baidu,https://www.reddit.com/r/MachineLearning/comments/54s1ja/yet_another_deep_learning_library_paddlepaddle/,aditya_arun,1475001112,,0,1
1001,2016-9-28,2016,9,28,3,54s2t1,Great overview talks on various sub-fields of deep learning from the this weekend's DL School in Stanford,https://www.reddit.com/r/MachineLearning/comments/54s2t1/great_overview_talks_on_various_subfields_of_deep/,[deleted],1475001519,[deleted],0,1
1002,2016-9-28,2016,9,28,3,54s3am,Understanding Sessions in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/54s3am/understanding_sessions_in_tensorflow/,danijar,1475001687,,1,9
1003,2016-9-28,2016,9,28,3,54s5d9,Great ML lessons from Google,https://www.reddit.com/r/MachineLearning/comments/54s5d9/great_ml_lessons_from_google/,Ungerwhere,1475002347,,1,3
1004,2016-9-28,2016,9,28,4,54se8y,Anybody here is working on data mining project and need some members with him/her? We are a group of 3 graduates that are willing to help or we can start a new project if you have something in your mind.,https://www.reddit.com/r/MachineLearning/comments/54se8y/anybody_here_is_working_on_data_mining_project/,dotop32,1475005239,[removed],0,1
1005,2016-9-28,2016,9,28,4,54sg25,Why is my post about asking for online partners to do a data mining project got removed from this subreddit?,https://www.reddit.com/r/MachineLearning/comments/54sg25/why_is_my_post_about_asking_for_online_partners/,dotop32,1475005838,[removed],0,1
1006,2016-9-28,2016,9,28,4,54shmi,Great new introductory talks on various sub-fields of deep learning,https://www.reddit.com/r/MachineLearning/comments/54shmi/great_new_introductory_talks_on_various_subfields/,UltraMarathonMan,1475006346,"The talks at the Deep Learning School on September 24/25, 2016 were amazing. I clipped out individual talks  from the full live streams and provided links to each below in case that's useful for people who want to watch specific talks several times (like I do). Please check out the official website (http://www.bayareadlschool.org) and full live streams below.

Having read, watched, and presented deep learning material over the past few years, I have to say that this is one of the best collection of introductory deep learning talks I've yet encountered. Here are links to the individual talks and the full live streams for the two days:

1. Foundations of Deep Learning (Hugo Larochelle, Twitter) - https://youtu.be/zij_FTbJHsk
2. Deep Learning for Computer Vision (Andrej Karpathy, OpenAI) - https://youtu.be/u6aEYuemt0M
3. Deep Learning for Natural Language Processing (Richard Socher, Salesforce) - https://youtu.be/oGk1v1jQITw
4. TensorFlow Tutorial (Sherry Moore, Google Brain) - https://youtu.be/Ejec3ID_h0w
5. Foundations of Unsupervised Deep Learning (Ruslan Salakhutdinov, CMU) - https://youtu.be/rK6bchqeaN8
6. Nuts and Bolts of Applying Deep Learning (Andrew Ng) - https://youtu.be/F1ka6a13S9I
7. Deep Reinforcement Learning (John Schulman, OpenAI) - https://youtu.be/PtAIh9KSnjo
8. Theano Tutorial (Pascal Lamblin, MILA) - https://youtu.be/OU8I1oJ9HhI
9. Deep Learning for Speech Recognition (Adam Coates, Baidu) - https://youtu.be/g-sndkf7mCs
10. Torch Tutorial (Alex Wiltschko, Twitter) - https://youtu.be/L1sHcj3qDNc
11. Sequence to Sequence Deep Learning (Quoc Le, Google) - https://youtu.be/G5RY_SUJih4
12. Foundations and Challenges of Deep Learning (Yoshua Bengio) - https://youtu.be/11rsu_WwZTc

Full Day Live Streams:
Day 1: https://youtu.be/eyovmAtoUx0
Day 2: https://youtu.be/9dXiAecyJrY

Go to http://www.bayareadlschool.org for more information on the event, speaker bios, slides, etc. Huge thanks to the organizers (Shubho Sengupta et al) for making this event happen.",5,135
1007,2016-9-28,2016,9,28,5,54sk7u,[Help] Convolutional Network,https://www.reddit.com/r/MachineLearning/comments/54sk7u/help_convolutional_network/,shelledpanda,1475007168,"Hey /r/machinelearning, I am a senior in college working on my thesis. The goal right now is to write a convolutional network from the ground up, and I was wondering about good resources for such a project, as well as what the big tasks are with this sort of thing(how I should organize my thought process, what places I should start attacking this type of program from.)

My plan is to use TensorFlow as a starting framework",2,0
1008,2016-9-28,2016,9,28,5,54skk1,Why is my post about asking for online partners to do a data mining project got removed from this subreddit?,https://www.reddit.com/r/MachineLearning/comments/54skk1/why_is_my_post_about_asking_for_online_partners/,[deleted],1475007277,[deleted],0,0
1009,2016-9-28,2016,9,28,5,54smgk,"Scikit-learn: For multiclass logistic regression, what is the difference between one-vs-rest and multinomial? Examples?",https://www.reddit.com/r/MachineLearning/comments/54smgk/scikitlearn_for_multiclass_logistic_regression/,palletsci12,1475007921,[removed],0,1
1010,2016-9-28,2016,9,28,5,54sow7,Anybody here is working on data mining project and need some members with him/her? We are a group of 3 graduates that are willing to help or we can start a new project if you have something in your mind.,https://www.reddit.com/r/MachineLearning/comments/54sow7/anybody_here_is_working_on_data_mining_project/,dimaa2,1475008712,[removed],0,1
1011,2016-9-28,2016,9,28,5,54sqry,[Group request] Anyone is working on a data mining project and need some members with him/her? We are a group of 3 that are willing to assist you or we can start a new project if you have something in your mind.,https://www.reddit.com/r/MachineLearning/comments/54sqry/group_request_anyone_is_working_on_a_data_mining/,Khamsekhalib,1475009323,[removed],0,1
1012,2016-9-28,2016,9,28,5,54ssjz,How to Embrace Randomness in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/54ssjz/how_to_embrace_randomness_in_machine_learning/,jasonb,1475009937,,0,2
1013,2016-9-28,2016,9,28,6,54sxin,Composition of filters in Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/54sxin/composition_of_filters_in_deep_learning/,mlgpop,1475011569,[removed],0,1
1014,2016-9-28,2016,9,28,7,54t3jd,Self-driving Car simple models with comma.ai (livestream),https://www.reddit.com/r/MachineLearning/comments/54t3jd/selfdriving_car_simple_models_with_commaai/,vanboxel,1475013618,,2,10
1015,2016-9-28,2016,9,28,7,54t4xl,Introducing Tensorflow Ruby API,https://www.reddit.com/r/MachineLearning/comments/54t4xl/introducing_tensorflow_ruby_api/,tensorflow_rb,1475014092,"Hi Everyone,
I am the author of [tensorflow.rb](https://github.com/somaticio/tensorflow.rb) the Ruby API for Tensorflow. 
The Ruby community has been very enthusiastic about developing a Ruby API for tensorflow (more on this [thread](https://github.com/tensorflow/tensorflow/issues/50)) and I decided to work on it. 

I faced incredible challenges along the way but I do have many interesting findings that I would like to share with you guys. The tensorflow.rb gem can be found on this [link](https://github.com/somaticio/tensorflow.rb). Aside from that, I have written three blog posts where I have given a very brief summary of the work
 
1. [Introductory blog post](https://medium.com/@Arafat./introducing-tensorflow-ruby-api-e77a477ff16e#.mhvj9ojlj)
2. [Developers blog post](https://medium.com/@Arafat./ruby-tensorflow-for-developers-2ec56b8668c5#.97tng1qqi) (This post is for rubyists and developers of other languages.)
3. [Image Recognition Tutorial](https://medium.com/@Arafat./image-recognition-in-ruby-tensorflow-df5d5c05389b#.ty1vygtrg)

The project still needs a lot of work and contributions are very welcome. I encourage everyone to read the blog posts and then install and play with Tensorflow.rb. Any comments/suggestions would be nice and you can always post your thoughts on the [gitter channel](https://gitter.im/tensorflowrb/Lobby?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge) or comment below.",5,14
1016,2016-9-28,2016,9,28,7,54t651,"[Tutorial] How to install Caffe on Ubuntu 16.04 with GPU, Cuda 8, CuDNN 5.1",https://www.reddit.com/r/MachineLearning/comments/54t651/tutorial_how_to_install_caffe_on_ubuntu_1604_with/,irina_ai,1475014516,,0,1
1017,2016-9-28,2016,9,28,7,54t6rn,Introducing Tensorflow Ruby API,https://www.reddit.com/r/MachineLearning/comments/54t6rn/introducing_tensorflow_ruby_api/,[deleted],1475014755,[deleted],0,1
1018,2016-9-28,2016,9,28,7,54t6vn,Sci-kit learn: Examples of when to use multinomial logistic regression vs. one-versus-all logistic regression?,https://www.reddit.com/r/MachineLearning/comments/54t6vn/scikit_learn_examples_of_when_to_use_multinomial/,Zeekawla99ii,1475014798,[removed],0,4
1019,2016-9-28,2016,9,28,7,54t95t,"Which software do you use for drawing figures in research papers, like deep networks etc?",https://www.reddit.com/r/MachineLearning/comments/54t95t/which_software_do_you_use_for_drawing_figures_in/,deepmind2016,1475015622,[removed],0,1
1020,2016-9-28,2016,9,28,7,54ta0m,"A Neural Network for Machine Translation, at Production Scale",https://www.reddit.com/r/MachineLearning/comments/54ta0m/a_neural_network_for_machine_translation_at/,Lajamerr_Mittesdine,1475015944,,21,168
1021,2016-9-28,2016,9,28,8,54tdww,Auto-encoders using Residual Networks,https://www.reddit.com/r/MachineLearning/comments/54tdww/autoencoders_using_residual_networks/,vighneshbirodkar,1475017417,"Residual networks as shown here
https://arxiv.org/abs/1512.03385
are known to be easier to optimize and perform well. I was wondering if I could use them to build an image auto-encoder.

Does anyone know about a paper/code which does the same ?",8,2
1022,2016-9-28,2016,9,28,8,54thcp,Anyone interested to group up for a data mining project?,https://www.reddit.com/r/MachineLearning/comments/54thcp/anyone_interested_to_group_up_for_a_data_mining/,zeint6,1475018726,[removed],0,1
1023,2016-9-28,2016,9,28,8,54thwa,Machine Learning and Visualization in Julia,https://www.reddit.com/r/MachineLearning/comments/54thwa/machine_learning_and_visualization_in_julia/,JColl1,1475018945,,2,14
1024,2016-9-28,2016,9,28,8,54tj22,Machine learning/computer vision technical interviews for graduate students--what are math/stats/probability basics one should review beforehand?,https://www.reddit.com/r/MachineLearning/comments/54tj22/machine_learningcomputer_vision_technical/,[deleted],1475019393,[removed],0,1
1025,2016-9-28,2016,9,28,9,54toih,Fathom: Reference Workloads for Modern Deep Learning Methods,https://www.reddit.com/r/MachineLearning/comments/54toih/fathom_reference_workloads_for_modern_deep/,mateja,1475021547,,0,4
1026,2016-9-28,2016,9,28,10,54tyyy,Does Theano actually use 100% of the CPU when it is configured to use the GPU?,https://www.reddit.com/r/MachineLearning/comments/54tyyy/does_theano_actually_use_100_of_the_cpu_when_it/,Franck_Dernoncourt,1475025634,[removed],0,1
1027,2016-9-28,2016,9,28,10,54u3az,TensorFlow's MNIST For ML Beginners Tutorial in Excel,https://www.reddit.com/r/MachineLearning/comments/54u3az/tensorflows_mnist_for_ml_beginners_tutorial_in/,GawkyFuse,1475027362,"To better understand basic machine learning, I built an Excel Spreadsheet that takes the weights and biases trained by a TensorFlow model and uses them to classify test MNIST handwritten digits. As a beginner to machine learning, I found this exercise to be helpful with better understanding the flow of a basic model. I hope that you might find this to be helpful, too.

Link to spreadsheet: https://db.tt/DyI7LU0H",4,7
1028,2016-9-28,2016,9,28,10,54u4hn,AskReddit: Does anyone here prefer MXNet to Theano/Tensorflow? Any particular reason?,https://www.reddit.com/r/MachineLearning/comments/54u4hn/askreddit_does_anyone_here_prefer_mxnet_to/,AlfonzoKaizerKok,1475027828,"Hi Reddit, I'm looking into alternatives to Theano. Tensorflow is the obvious choice, but I found MXNet to be another viable option. Is there any reason one should not go for MXNet in favour of Theano/Tensorflow?",13,8
1029,2016-9-28,2016,9,28,12,54ufil,"So, has anyone with some machine learning chops figured out what I meant by the ""one knife"" for herr hitler?",https://www.reddit.com/r/MachineLearning/comments/54ufil/so_has_anyone_with_some_machine_learning_chops/,ravasheera,1475032406,,7,1
1030,2016-9-28,2016,9,28,12,54ukbk,Beautiful Properties Of The Roc Curve,https://www.reddit.com/r/MachineLearning/comments/54ukbk/beautiful_properties_of_the_roc_curve/,[deleted],1475034554,[deleted],0,1
1031,2016-9-28,2016,9,28,12,54ukl7,Cool properties of the ROC curve,https://www.reddit.com/r/MachineLearning/comments/54ukl7/cool_properties_of_the_roc_curve/,Jxieeducation,1475034686,[removed],0,1
1032,2016-9-28,2016,9,28,13,54umxc,A good list of online resources to learn deep learning,https://www.reddit.com/r/MachineLearning/comments/54umxc/a_good_list_of_online_resources_to_learn_deep/,[deleted],1475035743,[deleted],0,1
1033,2016-9-28,2016,9,28,13,54uozx,Detecting Multiple Objects in A Image,https://www.reddit.com/r/MachineLearning/comments/54uozx/detecting_multiple_objects_in_a_image/,Damian3395,1475036733,[removed],0,1
1034,2016-9-28,2016,9,28,14,54uxvm,Vector space basis learning based on data set classification,https://www.reddit.com/r/MachineLearning/comments/54uxvm/vector_space_basis_learning_based_on_data_set/,ski__,1475041452,[removed],0,1
1035,2016-9-28,2016,9,28,14,54uypb,A good list of online courses on Deep Learning,https://www.reddit.com/r/MachineLearning/comments/54uypb/a_good_list_of_online_courses_on_deep_learning/,[deleted],1475041911,[deleted],0,0
1036,2016-9-28,2016,9,28,15,54v1u0,What does optimizing code for GPU mean? Articles showing examples?,https://www.reddit.com/r/MachineLearning/comments/54v1u0/what_does_optimizing_code_for_gpu_mean_articles/,leodicapricorn,1475043704,[removed],8,0
1037,2016-9-28,2016,9,28,16,54v7ig,The Self Learning Quant: Intro/tutorial to self-reinforcement learning using Neural Networks,https://www.reddit.com/r/MachineLearning/comments/54v7ig/the_self_learning_quant_introtutorial_to/,uapan,1475047122,,10,14
1038,2016-9-28,2016,9,28,17,54vceo,Faster Convolutions with Sparse FFT,https://www.reddit.com/r/MachineLearning/comments/54vceo/faster_convolutions_with_sparse_fft/,kevinzakka,1475050370,"- I was wondering if leveraging the sparse fft algorithm (for specific cases) to speed up train-time convolutions is something that has been explored? If so, do any of the libraries today use it?
- What are some cases where this would be applicable? (I'm thinking audio like in the new Wavenet paper, or specific set of images?)
",14,4
1039,2016-9-28,2016,9,28,20,54vxei,Are the NYU Deep Learning course taken down? (DS-GA-1008),https://www.reddit.com/r/MachineLearning/comments/54vxei/are_the_nyu_deep_learning_course_taken_down/,nokve,1475063381,[removed],2,3
1040,2016-9-28,2016,9,28,21,54w584,What's the most robust people tracking framework available for Python (probably would a Python wrapper for a C++ library)?,https://www.reddit.com/r/MachineLearning/comments/54w584/whats_the_most_robust_people_tracking_framework/,PicopicoEMD,1475066878,[removed],1,1
1041,2016-9-28,2016,9,28,21,54w6h2,Why are the Theano Tutorials left-multiply the input vectors to the weight matrices?,https://www.reddit.com/r/MachineLearning/comments/54w6h2/why_are_the_theano_tutorials_leftmultiply_the/,bagelorder,1475067384,"I am more used that linear maps are written as 
Wx = y
Does it have a reason that the Theano tutorials consistently use xW = y? Is this common practice in the area of neural-networks?",8,3
1042,2016-9-28,2016,9,28,22,54wd7v,"Really makes you think (about machine learning, and biological human learning)...",https://www.reddit.com/r/MachineLearning/comments/54wd7v/really_makes_you_think_about_machine_learning_and/,ravasheera,1475070041,,0,1
1043,2016-9-28,2016,9,28,23,54wofc,[AskML] Who got a DL job with just projects and no MS/PhD/MOOC?,https://www.reddit.com/r/MachineLearning/comments/54wofc/askml_who_got_a_dl_job_with_just_projects_and_no/,j_lyf,1475074084,[removed],0,1
1044,2016-9-28,2016,9,28,23,54woqa,We built a bot to automate job interviews. Try it here: https://telegram.me/impress_v2_bot,https://www.reddit.com/r/MachineLearning/comments/54woqa/we_built_a_bot_to_automate_job_interviews_try_it/,ahuja_s,1475074192,,0,1
1045,2016-9-29,2016,9,29,0,54wspb,Anyone get invited to the beta of bons.ai? How can I get an invite?,https://www.reddit.com/r/MachineLearning/comments/54wspb/anyone_get_invited_to_the_beta_of_bonsai_how_can/,jbark55,1475075494,[removed],0,1
1046,2016-9-29,2016,9,29,0,54wsyp,Results of Places2 Challenge 2016,https://www.reddit.com/r/MachineLearning/comments/54wsyp/results_of_places2_challenge_2016/,improbabble,1475075577,,2,6
1047,2016-9-29,2016,9,29,0,54wtef,Sentimental Analysis of the First Presidential Debate of 2016 Using Machine Learning,https://www.reddit.com/r/MachineLearning/comments/54wtef/sentimental_analysis_of_the_first_presidential/,TalentedGorilla,1475075715,,0,0
1048,2016-9-29,2016,9,29,0,54wucw,"Yoshua Bengio: ""Bridging the gap between deep learning and biology"" (yesterday's talk at Redwood Center, UC Berkeley)",https://www.reddit.com/r/MachineLearning/comments/54wucw/yoshua_bengio_bridging_the_gap_between_deep/,DanielleMolloy,1475076052,,13,91
1049,2016-9-29,2016,9,29,0,54x09q,"Simple Questions Thread September 28, 2016",https://www.reddit.com/r/MachineLearning/comments/54x09q/simple_questions_thread_september_28_2016/,AutoModerator,1475077939,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!
",122,8
1050,2016-9-29,2016,9,29,1,54x22t,Using an embedded GPU vs embedded CPU,https://www.reddit.com/r/MachineLearning/comments/54x22t/using_an_embedded_gpu_vs_embedded_cpu/,testingTestingIBS,1475078518,[removed],5,1
1051,2016-9-29,2016,9,29,1,54x2tg,What Did You Miss at the Deep Learning Summit Last Week?,https://www.reddit.com/r/MachineLearning/comments/54x2tg/what_did_you_miss_at_the_deep_learning_summit/,reworksophie,1475078745,,0,1
1052,2016-9-29,2016,9,29,1,54x7n9,Why does RML have much more DL content than RDL itself?,https://www.reddit.com/r/MachineLearning/comments/54x7n9/why_does_rml_have_much_more_dl_content_than_rdl/,terrorlucid,1475080333,[removed],14,0
1053,2016-9-29,2016,9,29,1,54x96e,Cloud Computing,https://www.reddit.com/r/MachineLearning/comments/54x96e/cloud_computing/,Ashutosh311297,1475080814,[removed],6,2
1054,2016-9-29,2016,9,29,2,54xfhu,How to Solve FizzBuzz Using Machine Learning,https://www.reddit.com/r/MachineLearning/comments/54xfhu/how_to_solve_fizzbuzz_using_machine_learning/,felixthursday,1475082878,,0,2
1055,2016-9-29,2016,9,29,2,54xh4d,Announcing YouTube-8M: A Large and Diverse Labeled Video Dataset for Video Understanding Research,https://www.reddit.com/r/MachineLearning/comments/54xh4d/announcing_youtube8m_a_large_and_diverse_labeled/,afeder_,1475083403,,18,227
1056,2016-9-29,2016,9,29,2,54xma4,Caffe tutorial?,https://www.reddit.com/r/MachineLearning/comments/54xma4/caffe_tutorial/,freddosmsc,1475085039,[removed],1,1
1057,2016-9-29,2016,9,29,3,54xuk6,Binary Stochastic Neurons in Tensorflow,https://www.reddit.com/r/MachineLearning/comments/54xuk6/binary_stochastic_neurons_in_tensorflow/,hardmaru,1475087677,,11,26
1058,2016-9-29,2016,9,29,3,54xw9e,rstudio/tensorflow: TensorFlow for R,https://www.reddit.com/r/MachineLearning/comments/54xw9e/rstudiotensorflow_tensorflow_for_r/,improbabble,1475088222,,16,37
1059,2016-9-29,2016,9,29,5,54yb8i,New SAS Analytics Platform with Machine Learning Capabilities,https://www.reddit.com/r/MachineLearning/comments/54yb8i/new_sas_analytics_platform_with_machine_learning/,gavlaaaaaaaa,1475092931,,0,0
1060,2016-9-29,2016,9,29,6,54yrh6,"YC's deep learning startup Skymind raises $3M, launches enterprise AI distro",https://www.reddit.com/r/MachineLearning/comments/54yrh6/ycs_deep_learning_startup_skymind_raises_3m/,vonnik,1475098147,,6,3
1061,2016-9-29,2016,9,29,7,54yysl,"Amazon, Facebook, Google/DeepMind, IBM, and Microsoft form a partnership to ""to discuss and best practices and ethical issues surrounding the deployment of AI and its potential impact on society."" Feat. Yann LeCun",https://www.reddit.com/r/MachineLearning/comments/54yysl/amazon_facebook_googledeepmind_ibm_and_microsoft/,ajmooch,1475100663,,30,116
1062,2016-9-29,2016,9,29,8,54zb1q,Building Game Bot with OpenAI's Gym (Live Coding starts at 15:00),https://www.reddit.com/r/MachineLearning/comments/54zb1q/building_game_bot_with_openais_gym_live_coding/,llSourcell,1475105160,,1,4
1063,2016-9-29,2016,9,29,8,54zcrk,scikit-learn 0.18 is out,https://www.reddit.com/r/MachineLearning/comments/54zcrk/scikitlearn_018_is_out/,NYDreamer,1475105819,,20,150
1064,2016-9-29,2016,9,29,12,5508ae,Machine Learning Pitfalls,https://www.reddit.com/r/MachineLearning/comments/5508ae/machine_learning_pitfalls/,lqdc13,1475118227,,1,9
1065,2016-9-29,2016,9,29,13,550l0a,Big list of real-world machine learning and AI uses,https://www.reddit.com/r/MachineLearning/comments/550l0a/big_list_of_realworld_machine_learning_and_ai_uses/,RealSocialLLC,1475124106,[removed],0,1
1066,2016-9-29,2016,9,29,14,550s3n,Factors Influencing the Occurrence of Dot Gain in an Offset Press,https://www.reddit.com/r/MachineLearning/comments/550s3n/factors_influencing_the_occurrence_of_dot_gain_in/,goodmachineeu,1475127858,,0,1
1067,2016-9-29,2016,9,29,18,551d03,Survey: Cloud Machine Learning Platforms vs custom Apache Spark based solutions,https://www.reddit.com/r/MachineLearning/comments/551d03/survey_cloud_machine_learning_platforms_vs_custom/,Michael_Douglas_,1475141190,,0,1
1068,2016-9-29,2016,9,29,18,551dvf,Neural network fine-tuning techniques,https://www.reddit.com/r/MachineLearning/comments/551dvf/neural_network_finetuning_techniques/,DrLegend,1475141767,,0,3
1069,2016-9-29,2016,9,29,19,551gil,Char-RNN with POS?,https://www.reddit.com/r/MachineLearning/comments/551gil/charrnn_with_pos/,ddofer,1475143406,[removed],0,3
1070,2016-9-29,2016,9,29,19,551hw4,"Neural Attention: Machine Learning Meets Neuroscience - Q&amp;A with Brian Cheung, intern at Google Brain",https://www.reddit.com/r/MachineLearning/comments/551hw4/neural_attention_machine_learning_meets/,reworkdiane,1475144217,,0,1
1071,2016-9-29,2016,9,29,19,551lpl,What is some tools that a Math graduate student must have to be competent in a Machine Learning Master?,https://www.reddit.com/r/MachineLearning/comments/551lpl/what_is_some_tools_that_a_math_graduate_student/,SuperDragon,1475146498,[removed],0,2
1072,2016-9-29,2016,9,29,20,551pc6,Automatic Pressure Forming Machine (APF),https://www.reddit.com/r/MachineLearning/comments/551pc6/automatic_pressure_forming_machine_apf/,ridatuk,1475148498,,0,1
1073,2016-9-29,2016,9,29,20,551sk2,learning a combinatorial function,https://www.reddit.com/r/MachineLearning/comments/551sk2/learning_a_combinatorial_function/,[deleted],1475150134,[removed],0,1
1074,2016-9-29,2016,9,29,22,552378,SURVEY: CLOUD MACHINE LEARNING PLATFORMS VS CUSTOM APACHE SPARK BASED SOLUTIONS,https://www.reddit.com/r/MachineLearning/comments/552378/survey_cloud_machine_learning_platforms_vs_custom/,Michael_Douglas_,1475154675,[removed],0,1
1075,2016-9-29,2016,9,29,22,55245o,"When training a deep convnet, do you still need to train layer-wise (aka greedy)?",https://www.reddit.com/r/MachineLearning/comments/55245o/when_training_a_deep_convnet_do_you_still_need_to/,testingTestingIBS,1475155070,[removed],10,4
1076,2016-9-29,2016,9,29,23,552ekz,"Intro to Neptune  Machine Learning Platform By Rafal Hryciuk, Senior Software Engineer - deepsense.io",https://www.reddit.com/r/MachineLearning/comments/552ekz/intro_to_neptune_machine_learning_platform_by/,OpenDataSciCon,1475159079,,0,1
1077,2016-9-30,2016,9,30,0,552ure,ISLR vs ESL vs Bishop,https://www.reddit.com/r/MachineLearning/comments/552ure/islr_vs_esl_vs_bishop/,[deleted],1475164661,[removed],0,1
1078,2016-9-30,2016,9,30,1,552v7i,NVIDIA AI Car Demonstration,https://www.reddit.com/r/MachineLearning/comments/552v7i/nvidia_ai_car_demonstration/,lagastic,1475164805,,61,195
1079,2016-9-30,2016,9,30,1,552xar,The 5 tech giants partner for AI researches but Apple and is the absentee,https://www.reddit.com/r/MachineLearning/comments/552xar/the_5_tech_giants_partner_for_ai_researches_but/,icorridor,1475165476,,0,1
1080,2016-9-30,2016,9,30,1,552xpa,What is the effect of stride size on classification with sliding windows,https://www.reddit.com/r/MachineLearning/comments/552xpa/what_is_the_effect_of_stride_size_on/,shapul,1475165605,[removed],0,1
1081,2016-9-30,2016,9,30,2,553b7c,"Here, what can your machines learn from this?",https://www.reddit.com/r/MachineLearning/comments/553b7c/here_what_can_your_machines_learn_from_this/,ravasheera,1475169918,,1,1
1082,2016-9-30,2016,9,30,3,553jez,Has DeepMind released their AlphaGo network?,https://www.reddit.com/r/MachineLearning/comments/553jez/has_deepmind_released_their_alphago_network/,michal_sustr,1475172532,"I didn't find anything. I wonder why wouldn't they, when Google's policy is to release other networks and accelerate research. It might be interesting to compare it with other approaches, which might not rely on studying human play for pretraining, to achieve a similar result as TD-gammon - learn purely from self-play.",31,20
1083,2016-9-30,2016,9,30,4,553u67,Google Cloud Machine Learning Open Beta,https://www.reddit.com/r/MachineLearning/comments/553u67/google_cloud_machine_learning_open_beta/,Endervos,1475175953,,1,1
1084,2016-9-30,2016,9,30,4,553vdy,Sparse coding: a simple exploration,https://www.reddit.com/r/MachineLearning/comments/553vdy/sparse_coding_a_simple_exploration/,morgangiraud,1475176330,,4,19
1085,2016-9-30,2016,9,30,4,553wfl,I dreemed a video,https://www.reddit.com/r/MachineLearning/comments/553wfl/i_dreemed_a_video/,orenog,1475176650,,1,0
1086,2016-9-30,2016,9,30,6,554fy0,Which is better to minor in for Machine Learning- Statistics or Applied Math [ in relation to adding to my Microbiology / Biochemistry double major ] [ interested in data science and epidemiological modeling,https://www.reddit.com/r/MachineLearning/comments/554fy0/which_is_better_to_minor_in_for_machine_learning/,theta101010,1475183028,[removed],0,1
1087,2016-9-30,2016,9,30,6,554lap,Google Cloud Machine Learning in Beta,https://www.reddit.com/r/MachineLearning/comments/554lap/google_cloud_machine_learning_in_beta/,Eridrus,1475184895,,6,7
1088,2016-9-30,2016,9,30,6,554llh,Randomforests and categorical variables input,https://www.reddit.com/r/MachineLearning/comments/554llh/randomforests_and_categorical_variables_input/,gr8ape,1475184990,[removed],0,1
1089,2016-9-30,2016,9,30,6,554mwc,Udacity's Open Source Self-Driving Car,https://www.reddit.com/r/MachineLearning/comments/554mwc/udacitys_open_source_selfdriving_car/,oliverfromudacity,1475185462,,2,1
1090,2016-9-30,2016,9,30,6,554ozt,"Research on situation where the feature value of some of the positive samples are not reliable, or more generally there are two kind of positive samples whose feature value distribution are quite different.",https://www.reddit.com/r/MachineLearning/comments/554ozt/research_on_situation_where_the_feature_value_of/,kemaswill,1475186233,[removed],0,1
1091,2016-9-30,2016,9,30,8,554zjn,Using Deep Learning to Predict Steering Angles,https://www.reddit.com/r/MachineLearning/comments/554zjn/using_deep_learning_to_predict_steering_angles/,oliverfromudacity,1475190109,,0,0
1092,2016-9-30,2016,9,30,8,555252,[1609.01000v1] Convexified Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/555252/160901000v1_convexified_convolutional_neural/,[deleted],1475191060,[deleted],0,0
1093,2016-9-30,2016,9,30,9,555d94,How can I optimize (minimize) the output of a machine leaning model?,https://www.reddit.com/r/MachineLearning/comments/555d94/how_can_i_optimize_minimize_the_output_of_a/,pedroszattoni,1475195422,[removed],0,1
1094,2016-9-30,2016,9,30,10,555l0k,Question about autoencoders,https://www.reddit.com/r/MachineLearning/comments/555l0k/question_about_autoencoders/,zergling103,1475198624,"Tl;dr - has this idea about neural network architecture, similar to autoencoders, been developed already? 

Correct me if I'm wrong about anything, but this is my understanding so far:

Autoencoders purposefully have an information bottleneck in the middle, and this bottleneck is what forces the network to learn high level representations of the input data. Otherwise, without the bottleneck, the network may discover that the optimal connection is one that is roughly equivalent to directly mapping each input to its corresponding output.

However, because that bottleneck exists, it is very unlikely that an autoencoder could perform a perfect reconstruction of the input.

Contrast this with other encoding, like DCT as used by JPEG images. Each 8x8 block of values (pixel intensity)gets transformed into another block of 8x8 values (coefficients) - there is no bottleneck, and one could losslessly reconstruct the input pixel intensities from the coefficients. However, despite lacking a bottleneck, this representation is higher level than raw pixel intensities. Further, these coefficients are ordered in a way such that, generally, coefficients earlier in the array are likely to contribute more to reducing reconstruction error than those further down.

This had me thinking: one could build a network that could theoretically perfectly reconstruct the input data (like DCT), while retaining the ability to represent the input data with high-level abstractions as exhibited by autoencoders.

Say we wanted to build an autoencoder on 32x32 pixel face images: One could imagine an autoencoder where the bottleneck has only 1 neuron. When training, the network would attempt to encode as much information as possible in that neuron. Obviously, reconstruction would be imperfect as it would likely only interpolate between two different images. However, because we are trying to minimize reconstruction error, the network might come up with two archetypal faces (perhaps male/female, or, dark skinnes/light skinned) that form a sort of line or curve of best fit to the training data.

Now imagine that we added another neuron to the bottleneck layer. The second neuron would attempt to minimize the residual reconstruction error left over by the first neuron. Meanwhile, the first neuron still attempts to reconstruct the input data as though it had no help from other neurons in the bottleneck layer.

Now imagine that this pattern repeats for the third neuron in this layer, the fourth, and so on: each neuron would be trained to minimize the residual reconstruction error left over by the neurons above it. The reconstruction error decreases exponentially as it gets passed down further. Neurons at the top of the chain in this bottleneck layer contribute the most to minimizing reconstruction error, but these contributions are very general. Further down the chain, neurons contribute much subtler reconstruction improvements. Neurons at the very bottom of the chain may contribute imperceptible improvements. In the end the ""bottleneck"" could have just as many neurons as the input layer, thus not being a bottleneck anymore, while still providing a high level representation of the input data.

Has a network like this been developed already?",12,15
1095,2016-9-30,2016,9,30,11,555rel,"Microsoft merges Bing, Cortana, and Research to make 5,000-strong AI division",https://www.reddit.com/r/MachineLearning/comments/555rel/microsoft_merges_bing_cortana_and_research_to/,EatMeerkats,1475201155,,2,4
1096,2016-9-30,2016,9,30,11,555txw,Mineral water plant machinery | Bottled water plant | Water bottling line factory,https://www.reddit.com/r/MachineLearning/comments/555txw/mineral_water_plant_machinery_bottled_water_plant/,stevenwangfilling,1475202216,,0,1
1097,2016-9-30,2016,9,30,12,556073,Sam Harris: Can we build AI without losing control over it? | TED Talk,https://www.reddit.com/r/MachineLearning/comments/556073/sam_harris_can_we_build_ai_without_losing_control/,evc123,1475204844,,19,0
1098,2016-9-30,2016,9,30,12,5560v0,Applying machine learning to freight forwarding,https://www.reddit.com/r/MachineLearning/comments/5560v0/applying_machine_learning_to_freight_forwarding/,chungchungz,1475205131,,0,1
1099,2016-9-30,2016,9,30,12,55653b,What's the current status of MIT's Open Mind Common Sense project?,https://www.reddit.com/r/MachineLearning/comments/55653b/whats_the_current_status_of_mits_open_mind_common/,Yeebster,1475207042,"I'm working on incorporating some world knowledge into an NLP task, and I'd like to use data from MIT's [Open Mind Common Sense project](http://media.mit.edu/research/groups/5994/open-mind-common-sense). 

I found a lot of materials online referencing [csc.media.mit.edu](http://csc.media.mit.edu) and [openmind.media.mit.edu](http://openmind.media.mit.edu), however both sites are down.
There seemed to be a bunch of subprojects within OMCS according to its [github page](https://github.com/commonsense/omcs), but the only one that currently seems active is [ConceptNet](http://conceptnet5.media.mit.edu/). 

According to this [site](http://ttt.media.mit.edu/research/openmind.html), OMCS collected knowledge like ""every person is younger than their mother"" and ""you can push something with a straight stick"", but I wasn't able to find such knowledge by searching on ConceptNet (ConceptNet's relations don't seem to  be flexible enough to describe this kind of knowledge).

I'm quite curious about what could be on [openmind.media.mit.edu](http://openmind.media.mit.edu) and [csc.media.mit.edu](http://csc.media.mit.edu). If they contain a superset of the knowledge in ConceptNet, I'd love to use them. ",6,5
1100,2016-9-30,2016,9,30,12,5566fo,HyperNetworks,https://www.reddit.com/r/MachineLearning/comments/5566fo/hypernetworks/,xternalz,1475207696,,21,38
1101,2016-9-30,2016,9,30,13,55696j,Amazon announces Alexa Prize of 2.5 million dollars for research groups to develop useful conversational AI,https://www.reddit.com/r/MachineLearning/comments/55696j/amazon_announces_alexa_prize_of_25_million/,torvoraptor,1475208930,,6,50
1102,2016-9-30,2016,9,30,13,5569ve,Build your own TensorFlow with NNVM and Torch,https://www.reddit.com/r/MachineLearning/comments/5569ve/build_your_own_tensorflow_with_nnvm_and_torch/,antinucleon,1475209263,,21,31
1103,2016-9-30,2016,9,30,14,556hk1,Hyper Networks,https://www.reddit.com/r/MachineLearning/comments/556hk1/hyper_networks/,EvanVanNess,1475213339,,0,4
1104,2016-9-30,2016,9,30,15,556qgl,"If a user installs an app, how to know the user acquisition source?",https://www.reddit.com/r/MachineLearning/comments/556qgl/if_a_user_installs_an_app_how_to_know_the_user/,ppzhang,1475218406,,0,0
1105,2016-9-30,2016,9,30,15,556r23,What are some good textbooks for ML?,https://www.reddit.com/r/MachineLearning/comments/556r23/what_are_some_good_textbooks_for_ml/,TheNASAguy,1475218785,[removed],1,4
1106,2016-9-30,2016,9,30,16,556rnh,Graph Convolutional Networks - An introduction to neural networks on graphs,https://www.reddit.com/r/MachineLearning/comments/556rnh/graph_convolutional_networks_an_introduction_to/,triplefloat,1475219114,,9,53
1107,2016-9-30,2016,9,30,17,556yr4,Natural Language Processing REST API based on Deep Learning,https://www.reddit.com/r/MachineLearning/comments/556yr4/natural_language_processing_rest_api_based_on/,andrey_smith,1475223795,,0,1
1108,2016-9-30,2016,9,30,17,556yus,New P2 Instance Type for Amazon EC2  Up to 16 GPUs,https://www.reddit.com/r/MachineLearning/comments/556yus/new_p2_instance_type_for_amazon_ec2_up_to_16_gpus/,DanielWaterworth,1475223865,,22,96
1109,2016-9-30,2016,9,30,17,556z9i,Can Microsoft make the AI revolution happen faster?,https://www.reddit.com/r/MachineLearning/comments/556z9i/can_microsoft_make_the_ai_revolution_happen_faster/,ppzhang,1475224160,,0,0
1110,2016-9-30,2016,9,30,17,5570ex,Lessons learned while studying Machine Learning,https://www.reddit.com/r/MachineLearning/comments/5570ex/lessons_learned_while_studying_machine_learning/,thewhitetulip,1475224964,,2,0
1111,2016-9-30,2016,9,30,17,55719g,Google studies the compression rate achieved by neural networks,https://www.reddit.com/r/MachineLearning/comments/55719g/google_studies_the_compression_rate_achieved_by/,gabrielgoh,1475225579,,12,31
1112,2016-9-30,2016,9,30,18,55743m,word2vec vs Autoencoder,https://www.reddit.com/r/MachineLearning/comments/55743m/word2vec_vs_autoencoder/,stacky777,1475227411,[removed],0,1
1113,2016-9-30,2016,9,30,18,5574x8,"What's the difference between data mining, machine learning, and deep learning?",https://www.reddit.com/r/MachineLearning/comments/5574x8/whats_the_difference_between_data_mining_machine/,muoro,1475227972,[removed],0,1
1114,2016-9-30,2016,9,30,19,5579v5,Its been an absolute pleasure dealing with Printoholic!,https://www.reddit.com/r/MachineLearning/comments/5579v5/its_been_an_absolute_pleasure_dealing_with/,printoholic,1475231151,,0,1
1115,2016-9-30,2016,9,30,20,557i5n,"""Fuzzy"" F-score?",https://www.reddit.com/r/MachineLearning/comments/557i5n/fuzzy_fscore/,AraneusAdoro,1475235950,[removed],0,2
1116,2016-9-30,2016,9,30,21,557qo0,Deep Learning Research Review: Generative Adversarial Nets,https://www.reddit.com/r/MachineLearning/comments/557qo0/deep_learning_research_review_generative/,adeshpande3,1475239848,,0,0
1117,2016-9-30,2016,9,30,22,557w0b,WaveFunctionCollapse: bitmap generation from a single example with an algorithm similar to belief propagation,https://www.reddit.com/r/MachineLearning/comments/557w0b/wavefunctioncollapse_bitmap_generation_from_a/,ExUtumno,1475242080,,20,99
1118,2016-9-30,2016,9,30,22,557x3b,Request - Online resources on optimization courses,https://www.reddit.com/r/MachineLearning/comments/557x3b/request_online_resources_on_optimization_courses/,BusterGendo,1475242515,[removed],0,2
1119,2016-9-30,2016,9,30,22,557y8e,Process Mining: Data Science in Action MOOC on Coursera,https://www.reddit.com/r/MachineLearning/comments/557y8e/process_mining_data_science_in_action_mooc_on/,TaXxER,1475243002,[removed],0,1
1120,2016-9-30,2016,9,30,23,5581g4,ML algorithm,https://www.reddit.com/r/MachineLearning/comments/5581g4/ml_algorithm/,jonsalji3,1475244239,[removed],0,1
1121,2016-9-30,2016,9,30,23,558aj7,Great list! The 65 best papers in Data Science history,https://www.reddit.com/r/MachineLearning/comments/558aj7/great_list_the_65_best_papers_in_data_science/,[deleted],1475247553,[deleted],0,1
