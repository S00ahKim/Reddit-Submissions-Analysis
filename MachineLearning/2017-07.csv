,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2017-7-1,2017,7,1,9,6kjxei,"So, How Does Machine Learning Apply To Cyber Security?",https://www.reddit.com/r/MachineLearning/comments/6kjxei/so_how_does_machine_learning_apply_to_cyber/,tech_ball,1498868595,,0,1
1,2017-7-1,2017,7,1,9,6kjy0m,[R] Improving LSTM-CTC based ASR performance in domains with limited training data,https://www.reddit.com/r/MachineLearning/comments/6kjy0m/r_improving_lstmctc_based_asr_performance_in/,jb1999_rd,1498868794,,4,3
2,2017-7-1,2017,7,1,10,6kkcnm,Microsoft made its AI work on a $10 Raspberry Pi!,https://www.reddit.com/r/MachineLearning/comments/6kkcnm/microsoft_made_its_ai_work_on_a_10_raspberry_pi/,DJ_Laaal,1498873870,[removed],0,1
3,2017-7-1,2017,7,1,11,6kkoa4,"Machine-learning Project To Analyze 1,000 Years of Documents from Venice's Golden Age",https://www.reddit.com/r/MachineLearning/comments/6kkoa4/machinelearning_project_to_analyze_1000_years_of/,namakadurer,1498877912,,0,1
4,2017-7-1,2017,7,1,12,6kkwk1,How to create chatbot that navigates user inserted page,https://www.reddit.com/r/MachineLearning/comments/6kkwk1/how_to_create_chatbot_that_navigates_user/,Nightwing993,1498880982,[removed],0,1
5,2017-7-1,2017,7,1,16,6klqou,Montavon et al.: Methods for Interpreting and Understanding Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/6klqou/montavon_et_al_methods_for_interpreting_and/,DanielleMolloy,1498893829,,0,1
6,2017-7-1,2017,7,1,19,6kmcpi,[D] What is the difference between GAN and U-NET?,https://www.reddit.com/r/MachineLearning/comments/6kmcpi/d_what_is_the_difference_between_gan_and_unet/,mega10d0n,1498905791,[removed],10,0
7,2017-7-1,2017,7,1,20,6kmf39,Machine Learning Bootcamp with R (by Jose Portilla),https://www.reddit.com/r/MachineLearning/comments/6kmf39/machine_learning_bootcamp_with_r_by_jose_portilla/,onlinecoursesgalore,1498907029,,2,2
8,2017-7-1,2017,7,1,20,6kmgfy,Wall Hangings Picture and Mirror,https://www.reddit.com/r/MachineLearning/comments/6kmgfy/wall_hangings_picture_and_mirror/,gwenyosef,1498907706,,0,1
9,2017-7-1,2017,7,1,21,6kmu1c,Hot Water Systems,https://www.reddit.com/r/MachineLearning/comments/6kmu1c/hot_water_systems/,gwenyosef,1498913870,,0,1
10,2017-7-1,2017,7,1,22,6kmzzm,How can I update LSTM in batch with Adam optimizer in MATLAB,https://www.reddit.com/r/MachineLearning/comments/6kmzzm/how_can_i_update_lstm_in_batch_with_adam/,jspark0729,1498916141,[removed],0,1
11,2017-7-2,2017,7,2,0,6knhn5,State-of-The-Art in Image Generation?,https://www.reddit.com/r/MachineLearning/comments/6knhn5/stateoftheart_in_image_generation/,demonFudgePies,1498921941,[removed],0,1
12,2017-7-2,2017,7,2,0,6knnou,Door Suppliers Installation and Repair,https://www.reddit.com/r/MachineLearning/comments/6knnou/door_suppliers_installation_and_repair/,gwenyosef,1498923811,,0,1
13,2017-7-2,2017,7,2,0,6knqhp,Automatic Non-dry Sticker Round Bottle Labeling Application Machine Supp...,https://www.reddit.com/r/MachineLearning/comments/6knqhp/automatic_nondry_sticker_round_bottle_labeling/,hymachinery,1498924637,,1,1
14,2017-7-2,2017,7,2,2,6koa3c,[D] Tips for Training Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/6koa3c/d_tips_for_training_recurrent_neural_networks/,danijar,1498930352,,21,39
15,2017-7-2,2017,7,2,2,6kofav,What's the difference between Stanford's CS224n lecture video 2015 version vs 2017 version?,https://www.reddit.com/r/MachineLearning/comments/6kofav/whats_the_difference_between_stanfords_cs224n/,czechrepublic,1498931895,[removed],0,1
16,2017-7-2,2017,7,2,3,6kohsi,Geometric interpretation of KL divergence,https://www.reddit.com/r/MachineLearning/comments/6kohsi/geometric_interpretation_of_kl_divergence/,totallynotAGI,1498932605,"I'm motivated by various GAN papers to try to finally understand various statistical distance measures. There's KL-divergence, JS divergence, Earth mover distance etc.

KL divergence seems to be widespread in ML but I still don't feel like I could explain to my grandma what it is. So here is what I don't get:

* What's the geometric interpretation of KL divergence? For example, the EMD distance suggests ""chuck of earth times the distance it was moved"" for all the chunks. That's kind of neat. But for KL, I fail to understand what all the logarithms mean and how could I intuitively interpret them.

* What's the reasoning behind using a function which is not symmetric? 
In what scenario would I want a loss which is differerent depending if I'm transforming distribution A to B vs B to A?

* Wasserstein metric (EMD) seems to be defined as the minimum cost of turning one distribution into the other. Does it mean that KL divergence is not the minimum cost of transforming the piles? Are there any connections between those two divergences?

* Is there a geometric interpretation for generalizations of KL divergence, like f-divergence or various other statistical distances? This is kind of a broad question, but perhaps there's an elegant way to understand them all.

Thanks!",22,12
17,2017-7-2,2017,7,2,3,6kojoi,[P] 2nd Place Solution to 2017 Data Science Bowl,https://www.reddit.com/r/MachineLearning/comments/6kojoi/p_2nd_place_solution_to_2017_data_science_bowl/,dhammack,1498933137,,40,128
18,2017-7-2,2017,7,2,4,6kp38h,Chaining feedforward and recurrent networks together,https://www.reddit.com/r/MachineLearning/comments/6kp38h/chaining_feedforward_and_recurrent_networks/,Tonic_Section,1498939033,[removed],0,1
19,2017-7-2,2017,7,2,6,6kpgll,Need help installing tensorflow,https://www.reddit.com/r/MachineLearning/comments/6kpgll/need_help_installing_tensorflow/,esclaponr96,1498943125,[removed],0,1
20,2017-7-2,2017,7,2,7,6kpyay,"[D] Berkeley hosting Deep RL bootcamp, worth going?",https://www.reddit.com/r/MachineLearning/comments/6kpyay/d_berkeley_hosting_deep_rl_bootcamp_worth_going/,Farzaa,1498948791,"https://www.deeprlbootcamp.berkeley.edu/

Hi guys! If this is a bad place to post this let me know!

Berkeley is hosting this Deep RL Bootcamp with a couple of big names in the field attending/ instructing. It costs 950$ just for the ticket as a student.

To people who have been to these sorta things before, is it worth it?


EDIT: lol wtf i got gold ",40,29
21,2017-7-2,2017,7,2,8,6kq7wn,[P] Diabetic Retinopathy detection w/ Tensorflow,https://www.reddit.com/r/MachineLearning/comments/6kq7wn/p_diabetic_retinopathy_detection_w_tensorflow/,javathunderman,1498952097,,1,1
22,2017-7-2,2017,7,2,10,6kqr3g,subpixel convolution in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/6kqr3g/subpixel_convolution_in_tensorflow/,zsdh123,1498959055,,0,1
23,2017-7-2,2017,7,2,13,6krlix,"[N] Data Intelligence Conference (McLean, VA, June 25-27) - Presentations",https://www.reddit.com/r/MachineLearning/comments/6krlix/n_data_intelligence_conference_mclean_va_june/,[deleted],1498971299,[deleted],0,1
24,2017-7-2,2017,7,2,14,6krmli,Practical Deep Learning with PyTorch,https://www.reddit.com/r/MachineLearning/comments/6krmli/practical_deep_learning_with_pytorch/,deeplearningwizard,1498971780,,0,1
25,2017-7-2,2017,7,2,14,6krmty,"[N] Data Intelligence Conference (McLean, VA, June 23-25) - Presentations",https://www.reddit.com/r/MachineLearning/comments/6krmty/n_data_intelligence_conference_mclean_va_june/,thumbsdrivesmecrazy,1498971890,,0,11
26,2017-7-2,2017,7,2,15,6ks0gl,Semi automatic wet cold glue round bottle labeling applicator equipment ...,https://www.reddit.com/r/MachineLearning/comments/6ks0gl/semi_automatic_wet_cold_glue_round_bottle/,hymachinery,1498978662,,1,1
27,2017-7-2,2017,7,2,16,6ks3b2,[D] What software frameworks and architectures are worth learning?,https://www.reddit.com/r/MachineLearning/comments/6ks3b2/d_what_software_frameworks_and_architectures_are/,SnappyBucksaw,1498980184,"I'm an undergrad and just doing some stuff in Python libraries + TensorFlow/Keras, but I feel that if I want to start doing anything serious I should pick up more. At the same time it seems there are so many tools, libraries, and platforms I don't know what to choose and what is worth learning.

Is it worth learning how to use a cloud API if I have a GPU? How do I choose? And there are so many frameworks being published by companies - Microsoft, IBM, Google, OpenAI, Nvidia, Amazon - as well as individuals (I've heard good things about PyTorch), I'm just a bit overwhelmed.

Do you guys have any input?

",70,107
28,2017-7-2,2017,7,2,17,6ks7bd,Reading the Reinforcement Learning chapter in Machine Learning by Mitchell (1997) - it seems like very little changed in the last 20 years,https://www.reddit.com/r/MachineLearning/comments/6ks7bd/reading_the_reinforcement_learning_chapter_in/,[deleted],1498982526,[deleted],0,1
29,2017-7-2,2017,7,2,19,6ksl4t,[D] [RL] Question about off-policy correction term in semi-gradient methods,https://www.reddit.com/r/MachineLearning/comments/6ksl4t/d_rl_question_about_offpolicy_correction_term_in/,Kiuhnm,1498990778,"I'm reading the latest (June 19 2017) draft of the 2nd edition of Sutton&amp;Barto's [book](http://incompleteideas.net/sutton/book/the-book-2nd.html).

When the behavior policy is different from the target policy, we usually need to use some kind of importance sampling. In semi-gradient methods, we want to minimize

    L(w) = 1/2 sum_t (G_t - v(S_t|w))^2

where w represents the parameters of the approximating function (e.g. an ANN), v is the value function and G_t is the return (i.e. expected total reward) which depends on whether we use MC, TD, etc... In *semi*-gradient methods we ignore the dependence of G_t on w.

The gradient of L wrt w is simply

    gL(w) = - sum_t (G_t - v(S_t|w)) gv(S_t|w)
          = - sum_t delta gv(S_t|w)

where

    delta = G_t - v(S_t|w)

In stochastic GD we update w as follows:

    w &lt;- w + alpha delta gv(S_t|w)

where alpha is the *learning rate*.

The off-policy correction rho should be applied to G_t:

    gL_{off}(w) = - sum_t (rho G_t - v(S_t|w)) gv(S_t|w)

According to the book (page 270 (290), subsection 11.1), the update becomes

    w &lt;- w + alpha rho delta gv(S_t|w)

That is, rho multiplies both G_t and v(S_t|w). **Why is that?**",4,13
30,2017-7-2,2017,7,2,22,6ktbry,[D] What's the state simulation generation?,https://www.reddit.com/r/MachineLearning/comments/6ktbry/d_whats_the_state_simulation_generation/,fimari,1499003492,"As in environment analysis, object detection, weight estimation, regeneration in 3D environment?",1,0
31,2017-7-2,2017,7,2,23,6ktmgo,Looking for machine learning investment strategies,https://www.reddit.com/r/MachineLearning/comments/6ktmgo/looking_for_machine_learning_investment_strategies/,MachineLearnInvest,1499007348,[removed],0,1
32,2017-7-3,2017,7,3,1,6ktzc0,Slides from the Machine Learning Summer School 2017,https://www.reddit.com/r/MachineLearning/comments/6ktzc0/slides_from_the_machine_learning_summer_school/,nisprateek,1499011481,,0,1
33,2017-7-3,2017,7,3,1,6ktzi7,Want to Earn Money with Your Algorithms? [P],https://www.reddit.com/r/MachineLearning/comments/6ktzi7/want_to_earn_money_with_your_algorithms_p/,tim_macgyver,1499011538,,1,0
34,2017-7-3,2017,7,3,1,6ku4tz,Introductory Overview of Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/6ku4tz/introductory_overview_of_deep_neural_networks/,DevelopingIdeas,1499013159,,1,1
35,2017-7-3,2017,7,3,3,6kun0w,Something people studying machine learning might find funny =),https://www.reddit.com/r/MachineLearning/comments/6kun0w/something_people_studying_machine_learning_might/,sojohnnysaid,1499018779,,0,1
36,2017-7-3,2017,7,3,4,6kv1t3,[D] State-of-the-art architecture for learning dynamics model for model-based RL ?,https://www.reddit.com/r/MachineLearning/comments/6kv1t3/d_stateoftheart_architecture_for_learning/,xingdongrobotics,1499023128,"I am seeking for baselines to learn dynamics model for an ongoing project in model-based RL. I am curious to be aware of state-of-the-art architectures to learn such dynamics model. For simplicity, the testbeds are OpenAI-Gym continuous control environments for example MountainCar (Continuous version) or LunarLander (Continuous version), or Mujoco/Roboschool. 

Currently I am using standard regression via 2 layer MLP for one-step prediction with current state and action as inputs and next state as output, and uses MSE loss, the training set is generated by rollouts with random actions. Could someone help to suggest either some better architectures or existing ones (papers) to do this ? We are aiming for both one-step and multi-step predictions together. ",8,7
37,2017-7-3,2017,7,3,4,6kv3rs,MOpen 1.0 released by AMD (deep learning software for GPUs using OpenCl),https://www.reddit.com/r/MachineLearning/comments/6kv3rs/mopen_10_released_by_amd_deep_learning_software/,rndnum123,1499023742,,64,185
38,2017-7-3,2017,7,3,4,6kv80n,MOpen 1.0 released by AMD (deep learning software for GPUs using OpenCl),https://www.reddit.com/r/MachineLearning/comments/6kv80n/mopen_10_released_by_amd_deep_learning_software/,[deleted],1499025033,[deleted],1,1
39,2017-7-3,2017,7,3,4,6kv8m1,MIOpen 1.0 released by AMD (deep learning software for GPUs using OpenCl),https://www.reddit.com/r/MachineLearning/comments/6kv8m1/miopen_10_released_by_amd_deep_learning_software/,rndnum123,1499025226,,2,1
40,2017-7-3,2017,7,3,5,6kvimn,[D] The Jobs That Artificial Intelligence Will Create,https://www.reddit.com/r/MachineLearning/comments/6kvimn/d_the_jobs_that_artificial_intelligence_will/,Dim25,1499028272,,4,0
41,2017-7-3,2017,7,3,6,6kvwf9,Recommendation on MOOC for ML applied on time series?,https://www.reddit.com/r/MachineLearning/comments/6kvwf9/recommendation_on_mooc_for_ml_applied_on_time/,xristos_forokolomvos,1499032462,[removed],0,1
42,2017-7-3,2017,7,3,7,6kvyn4,Cool machine built from Lego,https://www.reddit.com/r/MachineLearning/comments/6kvyn4/cool_machine_built_from_lego/,[deleted],1499033138,[deleted],0,1
43,2017-7-3,2017,7,3,8,6kw8uo,How accurate/fun do you think (misinterpreting) object recognition would be for r/misleadingthumbnails?,https://www.reddit.com/r/MachineLearning/comments/6kw8uo/how_accuratefun_do_you_think_misinterpreting/,[deleted],1499036468,[removed],0,1
44,2017-7-3,2017,7,3,8,6kwb1a,[D] How accurate/fun do you think object recognition would be for r/misleadingthumbnails?,https://www.reddit.com/r/MachineLearning/comments/6kwb1a/d_how_accuratefun_do_you_think_object_recognition/,woopteewoopwoop,1499037138,The point would be to intentionally-but-eloquently misinterpret results based on the thumbnail previews of image posts. Maybe an approach could be choosing as output the second-best classification?,1,2
45,2017-7-3,2017,7,3,8,6kwdsz,Teach a Neural Network to identify drunk from sober faces,https://www.reddit.com/r/MachineLearning/comments/6kwdsz/teach_a_neural_network_to_identify_drunk_from/,nchafni,1499038069,[removed],0,1
46,2017-7-3,2017,7,3,10,6kx1hx,[R] [1706.10295] Noisy Networks for Exploration,https://www.reddit.com/r/MachineLearning/comments/6kx1hx/r_170610295_noisy_networks_for_exploration/,evc123,1499046405,,17,31
47,2017-7-3,2017,7,3,11,6kx8wu,Math advice for machine learning,https://www.reddit.com/r/MachineLearning/comments/6kx8wu/math_advice_for_machine_learning/,skippy65,1499049101,[removed],0,1
48,2017-7-3,2017,7,3,12,6kxkhi,Tools for data labelling (text)?,https://www.reddit.com/r/MachineLearning/comments/6kxkhi/tools_for_data_labelling_text/,aviniumau,1499053279,[removed],0,1
49,2017-7-3,2017,7,3,14,6kxyy6,Data Cable Installation,https://www.reddit.com/r/MachineLearning/comments/6kxyy6/data_cable_installation/,gwenyosef,1499058947,,0,1
50,2017-7-3,2017,7,3,14,6kxzcv,[D] When will the CS231n 2017 video lectures be released?,https://www.reddit.com/r/MachineLearning/comments/6kxzcv/d_when_will_the_cs231n_2017_video_lectures_be/,yunjey,1499059115,,25,40
51,2017-7-3,2017,7,3,14,6ky142,Learning initial states for an RNN,https://www.reddit.com/r/MachineLearning/comments/6ky142/learning_initial_states_for_an_rnn/,tensorflower,1499059840,[removed],0,1
52,2017-7-3,2017,7,3,14,6ky33o,[D] Converting photos to anime images.,https://www.reddit.com/r/MachineLearning/comments/6ky33o/d_converting_photos_to_anime_images/,abcd_z,1499060668,"I'm interested in created a neural network that can take a plain photo and convert it into something in the style of an anime image.

Before I spend days/weeks/months training the layers, I thought I'd run the idea behind you guys and see if there are any obvious weaknesses I'm missing or mistakes I'm making.

Base dataset:  
~300 images that show ""before"" and ""after"" images of a photo converted to anime style (already acquired).  
Note: Before running the NN I plan to augment the data by slightly modifying the input photos in different subtle ways.  This should boost the quantity of input images by a factor of 10.

NN Stack:  

Input layer (photo)  
Convnet (some combination of Conv, Relu, and/or Pool layers) to pull features from the photos  
Fully-connected layer to convert from photo features to anime features  
Deconvnet (reversal of the previous convnet) to convert anime features to anime images  
Output layer (anime image)  

Thoughts?  Advice?  Suggestions?",18,19
53,2017-7-3,2017,7,3,15,6kyb0p,[1706.10059] A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem,https://www.reddit.com/r/MachineLearning/comments/6kyb0p/170610059_a_deep_reinforcement_learning_framework/,RooftrellenJiang,1499064065,,0,1
54,2017-7-3,2017,7,3,15,6kyd4e,Twist Tie Machine,https://www.reddit.com/r/MachineLearning/comments/6kyd4e/twist_tie_machine/,LessDeal14,1499064997,,1,1
55,2017-7-3,2017,7,3,16,6kyg0f,SOTA in Unsupervised Outlier detection,https://www.reddit.com/r/MachineLearning/comments/6kyg0f/sota_in_unsupervised_outlier_detection/,[deleted],1499066223,[removed],0,1
56,2017-7-3,2017,7,3,18,6kyyq3,[D] Testing machine learning in production?,https://www.reddit.com/r/MachineLearning/comments/6kyyq3/d_testing_machine_learning_in_production/,makeDLgr8again,1499075178,"- Things to watch for from experience.
- Best practices.
- Any good libraries/frameworks?

Many thanks!",3,9
57,2017-7-3,2017,7,3,18,6kyyx8,[P] Optimizing recall for specified precision in multiclass problem,https://www.reddit.com/r/MachineLearning/comments/6kyyx8/p_optimizing_recall_for_specified_precision_in/,michal_sustr,1499075285,"In a task of an online recommender system we try to predict what user will buy next in a new order, based on the collective history from all the users. 

For over 12k products in 800k orders we have trained individual classifiers for each product (based on xgboost trees) that produce rank in range &lt;0;1&gt; (the higher the rank, the better). 

The data has obviously large class imbalance, top 500 products constitue 50% of all ordered products. (top 100 = 25%, top 500=50%, top 2000=75%)

The classifier ranks are concatenated together, and by thresholding over 12k outputs we get the products that should be recommended. 

The metric that we use is that we want to maximize recall for specified precision=0.5. In the ideal result where recall would be 1, the user would have to only throw away half of the recommended products (dropping products from shopping list is easier than finding them, that's why we have this metric). This metric is calculated as the mean of precision and recall for individual orders.

Individual classifiers have high performance:

`train-logloss:0.011609  train-auc:0.981035`

`test_0-logloss:0.020463 test_0-auc:0.900525`

`test_1-logloss:0.014504 test_1-auc:0.956798`

`valid-logloss:0.014478   valid-auc:0.966512`


(where dataset has been splitted based on how last the order was (k), test_0 is the latest dataset (k == 0), test_1 is the before-latest (k==1), valid is (k==2) and train is (k &gt;= 3).

The performance understandably drops when all of the predictions are used together (the best we got so far is P,R=(0.5, 0.5)). It drops however too much - we use the same threshold for all the products, so we thought we could use individual thresholds, one per product.

The thresholds now should be *set for each product, but the metric is calculated per orders*. (Optimizing the threshold per product doesn't help, it yields very poor results)

Speed is another issue - we're dealing with matrices of size 800k x 12k

I think this is fundamentally a hard combinatoric problem - if the thresholds would be `len(np.arange(0, 100, 0.01)) == 100`, then by grid searching it would be searching through 100^12000 combinations :-) 

We used greedy search where one threshold is optimized at a time and the other thresholds have been fixated to the previously calculated global one. That helped, but it doesn't generalize from the training set.

Another option we thought we could use is to formulate this problem as LP - linearize the precision, recall curves for each product, but so far I wasn't successful in doing the formalization (I'm not sure if it's even possible).

I would've expected this is a problem that is well solved, but I couldn't find anything reasonable in the sea of irrelevant search results. If you know about some literature, please submit a link to it. 

Thank you very much!
",0,9
58,2017-7-3,2017,7,3,18,6kyzr6,[1706.09367] autoBagging: Learning to Rank Bagging Workflows with Metalearning [autoML R package],https://www.reddit.com/r/MachineLearning/comments/6kyzr6/170609367_autobagging_learning_to_rank_bagging/,[deleted],1499075663,[deleted],0,2
59,2017-7-3,2017,7,3,19,6kz12t,[R] [1706.09367] autoBagging: Learning to Rank Bagging Workflows with Metalearning [autoML R package],https://www.reddit.com/r/MachineLearning/comments/6kz12t/r_170609367_autobagging_learning_to_rank_bagging/,AYK32,1499076287,,8,29
60,2017-7-3,2017,7,3,19,6kz43h,3D Neural Network Visualization,https://www.reddit.com/r/MachineLearning/comments/6kz43h/3d_neural_network_visualization/,Lost4468,1499077651,,0,1
61,2017-7-3,2017,7,3,20,6kzc3r,House Wiring and Electrical Installation,https://www.reddit.com/r/MachineLearning/comments/6kzc3r/house_wiring_and_electrical_installation/,gwenyosef,1499081219,,0,1
62,2017-7-3,2017,7,3,20,6kzcv6,Can someone provide good resources to learn genetic programming / evolution in neural networks for Python?,https://www.reddit.com/r/MachineLearning/comments/6kzcv6/can_someone_provide_good_resources_to_learn/,unknownharris,1499081530,[removed],0,1
63,2017-7-3,2017,7,3,20,6kzfad,[D] Deep dictionary,https://www.reddit.com/r/MachineLearning/comments/6kzfad/d_deep_dictionary/,Dutchcheesehead,1499082528,,1,1
64,2017-7-3,2017,7,3,20,6kzfqm,How do I get an invitation for #machinelearning on freenode?,https://www.reddit.com/r/MachineLearning/comments/6kzfqm/how_do_i_get_an_invitation_for_machinelearning_on/,jnsukm,1499082712,[removed],0,1
65,2017-7-3,2017,7,3,20,6kzg3b,Neural Net - Sample noise,https://www.reddit.com/r/MachineLearning/comments/6kzg3b/neural_net_sample_noise/,maka89,1499082851,[removed],0,1
66,2017-7-3,2017,7,3,20,6kzg9r,[D] Machine learning and functional programming languages?,https://www.reddit.com/r/MachineLearning/comments/6kzg9r/d_machine_learning_and_functional_programming/,MagicMurderBagYT,1499082921,"I'm using Python and scikit-learn at the moment, but I'm interested in functional programming. Is there a functional language that's both easy to setup and has good libraries for ML?

So far I've looked at Clojure and Incanter and while I like Clojure itself, it doesn't seem to be the best fit for my requirements.",32,36
67,2017-7-3,2017,7,3,21,6kzmn1,inline plastic screw cap capping machine with four wheels video,https://www.reddit.com/r/MachineLearning/comments/6kzmn1/inline_plastic_screw_cap_capping_machine_with/,hymachinery,1499085351,,1,1
68,2017-7-3,2017,7,3,21,6kzoft,YouTube channel on Machine Learning,https://www.reddit.com/r/MachineLearning/comments/6kzoft/youtube_channel_on_machine_learning/,[deleted],1499086000,[deleted],0,1
69,2017-7-3,2017,7,3,21,6kzoq0,[D] Joining CMU for a masters program this fall. Seeking help wrt which subjects to take.,https://www.reddit.com/r/MachineLearning/comments/6kzoq0/d_joining_cmu_for_a_masters_program_this_fall/,mihirkale815,1499086103,"I'm joining the Masters in Computational Data Science [link](https://mcds.cs.cmu.edu/) program at CMU this fall.

Assume my ultimate goal after graduating is to become a Research Scientist at one of the good ML/AI labs [ I've noticed its almost impossible without a PhD but no harm in trying, right? :) ]

How should I go about choosing subjects? 
Which topics are important to set up a good foundation for being able to do research?
Any topics that are currently hot and would help getting a job? (Eg : Deep Learning)
What systems courses should I take? (Eg : Distributed Systems, Cloud Computing etc)

Here is my current list (based on compulsory requirements, my opinions and those of some of my senior co-workers) :

1. A PhD level intro to machine learning course 
2. An Intro to Deep Learning course (from Ruslan Salakhutdinov) [link] (https://www.cs.cmu.edu/~rsalakhu/10807_2016/)
3. Probabilistic Graphical Models (Eric Xing) [link](http://www.cs.cmu.edu/~epxing/Class/10708/)
4. A course on multi-modal learning
5. Parallel Computer Architecture And Programming [link] (http://15418.courses.cs.cmu.edu/spring2017/)

What are your thoughts on taking up some *deeper* courses like: 

1. Statistical Machine Learning
2. Intermediate Statistics (by Larry Wasserman) [link](http://www.stat.cmu.edu/~larry/=stat705/)
3. Something on real analysis/measure and integration


Should I also focus on some applied ML type courses like Sequence to Sequence Modelling, Deep Reinforcement Learning for Control etc?

Any CMU specific courses/professors that you would recommend?

Thanks in Advance!!",12,5
70,2017-7-3,2017,7,3,22,6kzqrg,A.I. Is Turning Supply Chain Logistics Into Automated Trading - TOPBOTS,https://www.reddit.com/r/MachineLearning/comments/6kzqrg/ai_is_turning_supply_chain_logistics_into/,conradcreates,1499086838,,0,1
71,2017-7-3,2017,7,3,22,6kzszy,Ranking news article relevance with a NLP algorithm,https://www.reddit.com/r/MachineLearning/comments/6kzszy/ranking_news_article_relevance_with_a_nlp/,themaehn,1499087572,[removed],0,1
72,2017-7-3,2017,7,3,22,6kzuhe,"Upcoming book ""Machine Learning God"". Follow progress on my Blog (free ebook + code).",https://www.reddit.com/r/MachineLearning/comments/6kzuhe/upcoming_book_machine_learning_god_follow/,[deleted],1499088064,[deleted],0,1
73,2017-7-3,2017,7,3,22,6kzvmo,simple k-means clustering for bag of words model using python,https://www.reddit.com/r/MachineLearning/comments/6kzvmo/simple_kmeans_clustering_for_bag_of_words_model/,kanglais,1499088444,,0,1
74,2017-7-3,2017,7,3,22,6l002p,[N] Effectively Using Matplotlib,https://www.reddit.com/r/MachineLearning/comments/6l002p/n_effectively_using_matplotlib/,dearpetra,1499089919,,0,1
75,2017-7-3,2017,7,3,23,6l01u4,[N] How many pages do you have to read to know 90% of words in the book?,https://www.reddit.com/r/MachineLearning/comments/6l01u4/n_how_many_pages_do_you_have_to_read_to_know_90/,magneticono,1499090460,,0,1
76,2017-7-3,2017,7,3,23,6l02bo,[N] What is SOTA-Py?,https://www.reddit.com/r/MachineLearning/comments/6l02bo/n_what_is_sotapy/,friscotime,1499090594,,0,1
77,2017-7-4,2017,7,4,0,6l0dts,[P] PixLab - Machine Vision &amp; Media Processing APIs,https://www.reddit.com/r/MachineLearning/comments/6l0dts/p_pixlab_machine_vision_media_processing_apis/,histoire_guy,1499094027,,0,0
78,2017-7-4,2017,7,4,0,6l0k76,Hitting a convergence wall,https://www.reddit.com/r/MachineLearning/comments/6l0k76/hitting_a_convergence_wall/,gopietz,1499095797,[removed],0,1
79,2017-7-4,2017,7,4,0,6l0la3,Is there any other neural network that shows more promising results than SRGAN?,https://www.reddit.com/r/MachineLearning/comments/6l0la3/is_there_any_other_neural_network_that_shows_more/,sangkeun00,1499096098,[removed],0,1
80,2017-7-4,2017,7,4,0,6l0m73,The information age is over; welcome to the machine learning age,https://www.reddit.com/r/MachineLearning/comments/6l0m73/the_information_age_is_over_welcome_to_the/,jonfla,1499096323,,0,1
81,2017-7-4,2017,7,4,0,6l0naq,[D] DeepMinds Relational Networks  Demystified,https://www.reddit.com/r/MachineLearning/comments/6l0naq/d_deepminds_relational_networks_demystified/,harvey_slash,1499096623,,11,26
82,2017-7-4,2017,7,4,0,6l0nth,Best online course for an intro to deep learning?,https://www.reddit.com/r/MachineLearning/comments/6l0nth/best_online_course_for_an_intro_to_deep_learning/,ajnicola,1499096765,[removed],0,1
83,2017-7-4,2017,7,4,1,6l11nz,[D] Are there any models that significantly beat the bigram multinoulli naive bayes in text classification?,https://www.reddit.com/r/MachineLearning/comments/6l11nz/d_are_there_any_models_that_significantly_beat/,OneRaynyDay,1499100431,"Due to the high dimensionality of BoW preprocessing, I would think any other model would perform similarly(SVM, logreg, etc), or overfit too hard(neural nets). I'm not quite sure about boosting/random forests in this super high dimensional setting either.

However, it has been shown that MNB does fairly well with bigram preprocessing. Empirically it has given me a very high baseline(~80-85%), perhaps due to the linearity of my dataset.

Have you guys had experience with any classifier, after fine tuning, that is able to beat MNB significantly? (By significantly I mean for the above example, over 90%)

EDIT: I forgot to mention, BoW is obviously not the only way to go about this. If there's a good classification model plugged into, say, word embeddings, that'd be an interesting avenue to explore as well, so let me know!",11,14
84,2017-7-4,2017,7,4,2,6l14mq,The Terrifying Black Market For Your Private Medical Records - TOPBOTS,https://www.reddit.com/r/MachineLearning/comments/6l14mq/the_terrifying_black_market_for_your_private/,conradcreates,1499101228,,0,1
85,2017-7-4,2017,7,4,2,6l184z,Strengthening African Machine Learning,https://www.reddit.com/r/MachineLearning/comments/6l184z/strengthening_african_machine_learning/,skrza,1499102140,,0,1
86,2017-7-4,2017,7,4,3,6l1urw,[P] Hypothesis Generation. Learning new medical advances on all of PubMed.,https://www.reddit.com/r/MachineLearning/comments/6l1urw/p_hypothesis_generation_learning_new_medical/,nalta,1499108033,,11,12
87,2017-7-4,2017,7,4,5,6l2bru,Why can't you guys comment your fucking code?,https://www.reddit.com/r/MachineLearning/comments/6l2bru/why_cant_you_guys_comment_your_fucking_code/,[deleted],1499112607,[removed],0,2
88,2017-7-4,2017,7,4,5,6l2esd,[D] Why can't you guys comment your fucking code?,https://www.reddit.com/r/MachineLearning/comments/6l2esd/d_why_cant_you_guys_comment_your_fucking_code/,didntfinishhighschoo,1499113449,"Seriously.

I spent the last few years doing web app development. Dug into DL a couple months ago. Supposedly, compared to the post-post-post-docs doing AI stuff, JavaScript developers should be inbred peasants. But every project these peasants release, even a fucking library that colorizes CLI output, has a catchy name, extensive docs, shitloads of comments, fuckton of tests, semantic versioning, changelog, and, oh my god, better variable names than `ctx_h` or `lang_hs` or `fuck_you_for_trying_to_understand`.

The concepts and ideas behind DL, GANs, LSTMs, CNNs, whatever  it's clear, it's simple, it's intuitive. The slog is to go through the jargon (that keeps changing beneath your feet - what's the point of using fancy words if you can't keep them consistent?), the unnecessary equations, trying to squeeze meaning from bullshit language used in papers, figuring out the super important steps, preprocessing, hyperparameters optimization that the authors, oops, failed to mention.

Sorry for singling out, but [look at this](https://github.com/facebookresearch/end-to-end-negotiator/blob/master/src/agent.py) - what the fuck? If a developer anywhere else at Facebook would get this code for a review they would throw up.

- Do you intentionally try to obfuscate your papers? Is pseudo-code a fucking premium? Can you at least try to give some intuition before showering the reader with equations?

- How the fuck do you dare to release a paper without source code?

- Why the fuck do you never ever add comments to you code?

- When naming things, are you charged by the character? Do you get a bonus for acronyms?

- Do you realize that OpenAI having needed to release a ""baseline"" TRPO implementation is a fucking disgrace to your profession?

- Jesus christ, who decided to name a tensor concatenation function `cat`?
",561,1458
89,2017-7-4,2017,7,4,5,6l2i94,Getting Started.,https://www.reddit.com/r/MachineLearning/comments/6l2i94/getting_started/,lejason,1499114381,[removed],0,1
90,2017-7-4,2017,7,4,5,6l2ma2,"Has there been any recent, significant breakthroughs in context understanding in NLP?",https://www.reddit.com/r/MachineLearning/comments/6l2ma2/has_there_been_any_recent_significant/,bdubbs09,1499115480,[removed],0,1
91,2017-7-4,2017,7,4,6,6l2umo,[R] Have any of you seen this (so-called Fractal AI)? What do you think of it?,https://www.reddit.com/r/MachineLearning/comments/6l2umo/r_have_any_of_you_seen_this_socalled_fractal_ai/,NapoleonTNT,1499117808,,14,13
92,2017-7-4,2017,7,4,6,6l2xo8,[D] A quick visual recap of understanding generalization in Deep learning from a viewpoint of flat loss-landscapes.,https://www.reddit.com/r/MachineLearning/comments/6l2xo8/d_a_quick_visual_recap_of_understanding/,VinayUPrabhu,1499118711,,5,28
93,2017-7-4,2017,7,4,8,6l3e8g,Balancing Machine Learning And Human Intuition In The Travel Industry - TOPBOTS,https://www.reddit.com/r/MachineLearning/comments/6l3e8g/balancing_machine_learning_and_human_intuition_in/,conradcreates,1499123763,,0,1
94,2017-7-4,2017,7,4,9,6l3n0b,From A Literary Theorists Guide to Autoencoding,https://www.reddit.com/r/MachineLearning/comments/6l3n0b/from_a_literary_theorists_guide_to_autoencoding/,__ishaan,1499126639,,0,1
95,2017-7-4,2017,7,4,9,6l3v4o,Foundation Repair Builders,https://www.reddit.com/r/MachineLearning/comments/6l3v4o/foundation_repair_builders/,gwenyosef,1499129342,,0,1
96,2017-7-4,2017,7,4,11,6l4b7j,Home Cleaning Services,https://www.reddit.com/r/MachineLearning/comments/6l4b7j/home_cleaning_services/,gwenyosef,1499134959,,0,1
97,2017-7-4,2017,7,4,14,6l58z9,How to Use Machine Learning To Understand Your Customers Better,https://www.reddit.com/r/MachineLearning/comments/6l58z9/how_to_use_machine_learning_to_understand_your/,shweta_hsc,1499147382,,0,1
98,2017-7-4,2017,7,4,15,6l5dco,[D] TensorFlow For Poets Experiments,https://www.reddit.com/r/MachineLearning/comments/6l5dco/d_tensorflow_for_poets_experiments/,phobrain,1499149172,,4,0
99,2017-7-4,2017,7,4,15,6l5efl,Lasagne Yellow Fin,https://www.reddit.com/r/MachineLearning/comments/6l5efl/lasagne_yellow_fin/,[deleted],1499149648,[deleted],1,1
100,2017-7-4,2017,7,4,15,6l5fah,[D] Theano Yellow Fin,https://www.reddit.com/r/MachineLearning/comments/6l5fah/d_theano_yellow_fin/,alexbotev,1499149991,,6,16
101,2017-7-4,2017,7,4,15,6l5g9h,[D] Can Machine Learning Be Self-Taught?,https://www.reddit.com/r/MachineLearning/comments/6l5g9h/d_can_machine_learning_be_selftaught/,CircuitBeast,1499150385,Almost everyone I have met who works in Machine Learning has gotten a degree in the subject. Are any of you guys self-taught? What formal education did you have? What was the hardest part (concept or book or class)?,33,9
102,2017-7-4,2017,7,4,16,6l5mjl,Machine Learning For Screenplay writing,https://www.reddit.com/r/MachineLearning/comments/6l5mjl/machine_learning_for_screenplay_writing/,neuralfilmnetwork,1499152997,[removed],0,1
103,2017-7-4,2017,7,4,17,6l5t01,19 questions out of this machine learning model,https://www.reddit.com/r/MachineLearning/comments/6l5t01/19_questions_out_of_this_machine_learning_model/,cloudgentleman,1499155942,,0,1
104,2017-7-4,2017,7,4,20,6l6hau,[D] Help needed regarding personal budget deeplearning workstation gig based on AMD Vega,https://www.reddit.com/r/MachineLearning/comments/6l6hau/d_help_needed_regarding_personal_budget/,saurabhvyas3,1499167016,"I want to build my own home deeplearning server  , which I plan to use for training custom speech recognition models ( Deepspeech etc ) on my own dataset , It will only be used for learning and not for any heavy production based system . Also , It will be used in a headless mode without monitor and keyboard/mouse

I want to go with rhyzen 1700x and AMD Vega , I dont know which powersupply / motherboard / RAM / HDD to use , considering my budget is only about 1500 USD ( 2000 MAX )",12,2
105,2017-7-4,2017,7,4,22,6l6x1f,Artificial Intelligence Education Transforms The Developing World - TOPBOTS,https://www.reddit.com/r/MachineLearning/comments/6l6x1f/artificial_intelligence_education_transforms_the/,conradcreates,1499173239,,0,1
106,2017-7-4,2017,7,4,22,6l70m4,"[D] Top articles in data in the last month, handpicked with love by data scientists",https://www.reddit.com/r/MachineLearning/comments/6l70m4/d_top_articles_in_data_in_the_last_month/,fl4v1,1499174447,,14,80
107,2017-7-4,2017,7,4,22,6l7273,[R] [1702.07464] Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning,https://www.reddit.com/r/MachineLearning/comments/6l7273/r_170207464_deep_models_under_the_gan_information/,Fujikan,1499174979,,1,5
108,2017-7-4,2017,7,4,22,6l72of,[D] Tried to make a well commented code for a Neural Network,https://www.reddit.com/r/MachineLearning/comments/6l72of/d_tried_to_make_a_well_commented_code_for_a/,raksham97,1499175141,"I've tried my best to create a well commented Python code for implementing a neural network for handwritten digit recognition using Tensorflow. 

I think even someone with minimal knowledge of Python can understand this, atleast that's what I've tried to do.


Original Post - 

https://medium.com/@47a50b9230de/d622add47d3c


&gt;&gt;&gt;GitHub link for just the code - 

https://github.com/Raksham97/Neural-Networks-Using-Tensorflow/blob/master/NNUsingTFforMNIST.py",6,6
109,2017-7-4,2017,7,4,23,6l7jfx,Mac eGPU for Tensorflow,https://www.reddit.com/r/MachineLearning/comments/6l7jfx/mac_egpu_for_tensorflow/,loganjspears,1499180373,[removed],0,1
110,2017-7-5,2017,7,5,0,6l7jx5,[R] - Variance Regularizing Adversarial Learning,https://www.reddit.com/r/MachineLearning/comments/6l7jx5/r_variance_regularizing_adversarial_learning/,NotAlphaGo,1499180510,,0,2
111,2017-7-5,2017,7,5,0,6l7ua6,[D] Interpreting first layer weights on CNN,https://www.reddit.com/r/MachineLearning/comments/6l7ua6/d_interpreting_first_layer_weights_on_cnn/,leonardoaraujosantos,1499183475,"Hi guys, while training my models from time to time a see this behavior while training I get those [results](http://imgur.com/jFqa25T) on the first layer. (Some weights completely washed out)

Actually this effect also happens when I start the model from a pre-trained state.

Some stuff that I suspect:

1) Relu input became negative so the backprop will not get gradients anymore (Could Leaky relu solve that?)

2) Lack of data compared to model parameters...

Any ideas? Thanks",15,3
112,2017-7-5,2017,7,5,0,6l7v17,Current state of MRF and CRF research,https://www.reddit.com/r/MachineLearning/comments/6l7v17/current_state_of_mrf_and_crf_research/,[deleted],1499183704,[removed],0,1
113,2017-7-5,2017,7,5,0,6l7vpi,[P] Simple linear regression with confidence intervals on parameters and prediction,https://www.reddit.com/r/MachineLearning/comments/6l7vpi/p_simple_linear_regression_with_confidence/,[deleted],1499183892,[deleted],1,1
114,2017-7-5,2017,7,5,1,6l8073,Simple Linear Regression + Confidence intervals on coeficients and prediction,https://www.reddit.com/r/MachineLearning/comments/6l8073/simple_linear_regression_confidence_intervals_on/,[deleted],1499185120,[deleted],1,1
115,2017-7-5,2017,7,5,1,6l819e,[P]Simple Linear Regression + Confidence interval on parameters + prediction,https://www.reddit.com/r/MachineLearning/comments/6l819e/psimple_linear_regression_confidence_interval_on/,maka89,1499185412,,1,4
116,2017-7-5,2017,7,5,2,6l89an,4 Key Differences Between Industrial And Consumer A.I. - TOPBOTS,https://www.reddit.com/r/MachineLearning/comments/6l89an/4_key_differences_between_industrial_and_consumer/,conradcreates,1499187630,,0,1
117,2017-7-5,2017,7,5,2,6l8g1o,[D] Current state of MRF/CRF research,https://www.reddit.com/r/MachineLearning/comments/6l8g1o/d_current_state_of_mrfcrf_research/,ClownShoesFL,1499189432,"Two questions: one, are there any applications/domains (structured learning, or otherwise) where CRFs/MRFs performance is state of the art? Second, what is the state research here or rather what are some of the interesting/relevant discoveries in the last 3 years?",2,33
118,2017-7-5,2017,7,5,2,6l8j8g,Any good resources for sentiment analysis into different categories?,https://www.reddit.com/r/MachineLearning/comments/6l8j8g/any_good_resources_for_sentiment_analysis_into/,[deleted],1499190272,[removed],0,1
119,2017-7-5,2017,7,5,3,6l8mvb,Is LSUV a viable alternative to BN?,https://www.reddit.com/r/MachineLearning/comments/6l8mvb/is_lsuv_a_viable_alternative_to_bn/,vladfeinberg,1499191265,[removed],0,1
120,2017-7-5,2017,7,5,3,6l8q40,Deep Learning Patent Troll?,https://www.reddit.com/r/MachineLearning/comments/6l8q40/deep_learning_patent_troll/,[deleted],1499192152,[removed],0,1
121,2017-7-5,2017,7,5,3,6l8u1t,"""Advances in Deep Neural Networks,"" at ACM Turing 50 Celebration",https://www.reddit.com/r/MachineLearning/comments/6l8u1t/advances_in_deep_neural_networks_at_acm_turing_50/,[deleted],1499193241,[deleted],0,1
122,2017-7-5,2017,7,5,4,6l9awn,Is octave still good to begin learning ML?,https://www.reddit.com/r/MachineLearning/comments/6l9awn/is_octave_still_good_to_begin_learning_ml/,[deleted],1499197765,[removed],0,1
123,2017-7-5,2017,7,5,5,6l9it2,Microsoft Chatbot,https://www.reddit.com/r/MachineLearning/comments/6l9it2/microsoft_chatbot/,thepreeti,1499200011,[removed],0,1
124,2017-7-5,2017,7,5,5,6l9ny5,[D] The effect Hinton's four horsemen of pooling.Can we pronounce this institution dead already?,https://www.reddit.com/r/MachineLearning/comments/6l9ny5/d_the_effect_hintons_four_horsemen_of_poolingcan/,VinayUPrabhu,1499201499,"This was ~3 years ago from Hinton's talk ""What is wrong with convolutional neural nets ?"" He even says: 'The fact that it works so well is extremely unfortunate because it makes it really hard to get rid of!'.

Further, the CS231n notes too make the following argument : 'Getting rid of pooling. Many people dislike the pooling operation and think that we can get away without it. For example, Striving for Simplicity: The All Convolutional Net proposes to discard the pooling layer in favor of architecture that only consists of repeated CONV layers. To reduce the size of the representation they suggest using larger stride in CONV layer once in a while. Discarding pooling layers has also been found to be important in training good generative models, such as variational autoencoders (VAEs) or generative adversarial networks (GANs). It seems likely that future architectures will feature very few to no pooling layers'.

Personally, when it comes to convnets (much like Uber), I used to pool. Post RELU especially, I had visualized the histogram of features being max-pooled, and they had indeed looked reasonable flat to me. Connection: Maximum() is a sufficient statistic for Uniform[0,\theta] - Exponential family 101. Make  sense. Hand-wavy. But still ..
And now (just like with Uber), I have ditched it in favor of something better (dilated convolution /Lyft).

One of yesterday's papers on ArXiV claiming state-of-the-art results in Medical Image Segmentation, titled, 'DeepIGeoS: A Deep Interactive Geodesic Framework for Medical Image Segmentation' state : 'Therefore, to preserve resolution through the network, we remove the max-pooling and downsampling layers and use dilated convolution in each block'.
So, the question is: Can we pronounce max-pooling dead for conv-nets?
",13,31
125,2017-7-5,2017,7,5,7,6la8gt,HyperGAN 0.9 released - reproduce GAN papers with json,https://www.reddit.com/r/MachineLearning/comments/6la8gt/hypergan_09_released_reproduce_gan_papers_with/,what_are_tensors,1499207817,[removed],0,1
126,2017-7-5,2017,7,5,8,6lafbq,Unsupervised Investments: A Guide to A.I. Accelerators and Incubators - TOPBOTS,https://www.reddit.com/r/MachineLearning/comments/6lafbq/unsupervised_investments_a_guide_to_ai/,conradcreates,1499210164,,0,1
127,2017-7-5,2017,7,5,8,6lahpq,Complete database following a pattern,https://www.reddit.com/r/MachineLearning/comments/6lahpq/complete_database_following_a_pattern/,[deleted],1499211020,[removed],0,1
128,2017-7-5,2017,7,5,8,6laiz7,Math vs CS background?,https://www.reddit.com/r/MachineLearning/comments/6laiz7/math_vs_cs_background/,_0x5f3759df_,1499211484,[removed],0,1
129,2017-7-5,2017,7,5,9,6lanw6,Would it be possible to create a machine learning program that predicts stock prices and then compares the results to what actually happens to learn from them?,https://www.reddit.com/r/MachineLearning/comments/6lanw6/would_it_be_possible_to_create_a_machine_learning/,[deleted],1499213199,[removed],0,1
130,2017-7-5,2017,7,5,9,6lawta,Domestic Cleaners,https://www.reddit.com/r/MachineLearning/comments/6lawta/domestic_cleaners/,gwenyosef,1499216292,,0,1
131,2017-7-5,2017,7,5,11,6lbcgd,"[D] ""Advances in Deep Neural Networks,"" at ACM Turing 50 Celebration",https://www.reddit.com/r/MachineLearning/comments/6lbcgd/d_advances_in_deep_neural_networks_at_acm_turing/,fiskak,1499222187,,16,50
132,2017-7-5,2017,7,5,11,6lbf06,economical type semi automatic round bottle sticker labeling machine equ...,https://www.reddit.com/r/MachineLearning/comments/6lbf06/economical_type_semi_automatic_round_bottle/,hymachinery,1499223156,,0,1
133,2017-7-5,2017,7,5,11,6lbfcd,An explanation of neural networks i made. More parts going into further detail coming soon. - Part 1,https://www.reddit.com/r/MachineLearning/comments/6lbfcd/an_explanation_of_neural_networks_i_made_more/,lochiewestfall,1499223291,,0,1
134,2017-7-5,2017,7,5,12,6lbj9a,An explanation of neural networks i made. More parts going into further detail coming soon. - Part 1,https://www.reddit.com/r/MachineLearning/comments/6lbj9a/an_explanation_of_neural_networks_i_made_more/,[deleted],1499224747,[deleted],0,1
135,2017-7-5,2017,7,5,12,6lbkeh,Bn May chan ton thuy luc NC 125 Tons Gia tot,https://www.reddit.com/r/MachineLearning/comments/6lbkeh/bn_may_chan_ton_thuy_luc_nc_125_tons_gia_tot/,Huyieco,1499225221,,1,1
136,2017-7-5,2017,7,5,12,6lbkrs,The second part of my explanation of neural networks. This episode is about forward propagation,https://www.reddit.com/r/MachineLearning/comments/6lbkrs/the_second_part_of_my_explanation_of_neural/,lochiewestfall,1499225359,,0,1
137,2017-7-5,2017,7,5,12,6lbmii,we are in Bangkok,https://www.reddit.com/r/MachineLearning/comments/6lbmii/we_are_in_bangkok/,ada2017,1499226014,,0,1
138,2017-7-5,2017,7,5,14,6lc46a,k-fold cross validation for model evaluation vs hyper parameter tuning,https://www.reddit.com/r/MachineLearning/comments/6lc46a/kfold_cross_validation_for_model_evaluation_vs/,sudip_bhandari,1499232864,[removed],0,1
139,2017-7-5,2017,7,5,15,6lcfqw,Simple research papers on conversational AI,https://www.reddit.com/r/MachineLearning/comments/6lcfqw/simple_research_papers_on_conversational_ai/,mohanradhakrishnan,1499237787,[removed],0,1
140,2017-7-5,2017,7,5,16,6lcgdf,zig zag embroidery machine,https://www.reddit.com/r/MachineLearning/comments/6lcgdf/zig_zag_embroidery_machine/,iigmblr,1499238054,,0,1
141,2017-7-5,2017,7,5,16,6lciji,zig zag embroidery machine,https://www.reddit.com/r/MachineLearning/comments/6lciji/zig_zag_embroidery_machine/,iigmblr,1499239010,[removed],0,1
142,2017-7-5,2017,7,5,16,6lcjpa,DeepStory: Video Story QA by Deep Embedded Memory Networks,https://www.reddit.com/r/MachineLearning/comments/6lcjpa/deepstory_video_story_qa_by_deep_embedded_memory/,[deleted],1499239547,[deleted],0,1
143,2017-7-5,2017,7,5,16,6lcl0z,Question for tensorflow layer code,https://www.reddit.com/r/MachineLearning/comments/6lcl0z/question_for_tensorflow_layer_code/,jdg105,1499240133,[removed],0,1
144,2017-7-5,2017,7,5,16,6lcl7i,[1707.00836] DeepStory: Video Story QA by Deep Embedded Memory Networks,https://www.reddit.com/r/MachineLearning/comments/6lcl7i/170700836_deepstory_video_story_qa_by_deep/,zpdlzpdldpa,1499240213,,0,1
145,2017-7-5,2017,7,5,16,6lcnow,"Textile Waste Recycling Machine, Needle Punching Machine Supplier &amp; Manufacturer",https://www.reddit.com/r/MachineLearning/comments/6lcnow/textile_waste_recycling_machine_needle_punching/,rdmachine,1499241385,,1,1
146,2017-7-5,2017,7,5,18,6lcxkj,[P] Face2face  A Pix2Pix demo that mimics the facial expression of the German chancellor,https://www.reddit.com/r/MachineLearning/comments/6lcxkj/p_face2face_a_pix2pix_demo_that_mimics_the_facial/,datitran,1499246135,,14,44
147,2017-7-5,2017,7,5,20,6lddt3,"[P] Generating cats with deep learning. Comparing DCGAN, WGAN, WGAN-GP, LSGAN and ReLU with batch norm vs SELU.",https://www.reddit.com/r/MachineLearning/comments/6lddt3/p_generating_cats_with_deep_learning_comparing/,AlexiaJM,1499253274,,26,169
148,2017-7-5,2017,7,5,20,6ldgaj,Simple Neural Net from scratch in R,https://www.reddit.com/r/MachineLearning/comments/6ldgaj/simple_neural_net_from_scratch_in_r/,iliauk,1499254320,,0,1
149,2017-7-5,2017,7,5,20,6ldicv,[P] Perplexed by Game of Thrones. A Song of N-Grams and Language Models,https://www.reddit.com/r/MachineLearning/comments/6ldicv/p_perplexed_by_game_of_thrones_a_song_of_ngrams/,yvespeirsman,1499255159,,0,6
150,2017-7-5,2017,7,5,21,6ldli7,Intelligence on language,https://www.reddit.com/r/MachineLearning/comments/6ldli7/intelligence_on_language/,jnsukm,1499256354,[removed],0,1
151,2017-7-5,2017,7,5,21,6ldmh0,Silly idea with primes,https://www.reddit.com/r/MachineLearning/comments/6ldmh0/silly_idea_with_primes/,MemeBox,1499256714,[removed],0,1
152,2017-7-5,2017,7,5,21,6ldpde,[P] NeuroBind--Yet Another Model for Finding Binding Sites Using Neural Networks,https://www.reddit.com/r/MachineLearning/comments/6ldpde/p_neurobindyet_another_model_for_finding_binding/,longinglove,1499257761,,0,1
153,2017-7-5,2017,7,5,21,6ldtup,Automatic High Speed Bottle Unscrambler Machine,https://www.reddit.com/r/MachineLearning/comments/6ldtup/automatic_high_speed_bottle_unscrambler_machine/,hymachinery,1499259287,,1,1
154,2017-7-5,2017,7,5,22,6lduvd,Unsupervised Investments: A Guide to A.I. Investors - TOPBOTS,https://www.reddit.com/r/MachineLearning/comments/6lduvd/unsupervised_investments_a_guide_to_ai_investors/,conradcreates,1499259639,,0,1
155,2017-7-5,2017,7,5,22,6ldxzs,FANN (Fast Artifical Neural Network) binding in Crystal,https://www.reddit.com/r/MachineLearning/comments/6ldxzs/fann_fast_artifical_neural_network_binding_in/,[deleted],1499260583,[deleted],0,1
156,2017-7-5,2017,7,5,22,6ldyeq,Machine Learning Basic,https://www.reddit.com/r/MachineLearning/comments/6ldyeq/machine_learning_basic/,Opsaunders,1499260716,[removed],0,1
157,2017-7-5,2017,7,5,22,6ldyfn,Structure Optimization for Deep Multimodal Fusion Networks using Graph-Induced Kernels,https://www.reddit.com/r/MachineLearning/comments/6ldyfn/structure_optimization_for_deep_multimodal_fusion/,deephive,1499260724,,0,1
158,2017-7-5,2017,7,5,22,6ldyfw,A 2017 Guide to Semantic Segmentation with Deep Learning,https://www.reddit.com/r/MachineLearning/comments/6ldyfw/a_2017_guide_to_semantic_segmentation_with_deep/,[deleted],1499260726,[deleted],0,1
159,2017-7-5,2017,7,5,22,6ldyt6,[P] A 2017 Guide to Semantic Segmentation with Deep Learning,https://www.reddit.com/r/MachineLearning/comments/6ldyt6/p_a_2017_guide_to_semantic_segmentation_with_deep/,saucysassy,1499260849,,17,32
160,2017-7-5,2017,7,5,22,6le1ha,crystal-fann Bindings for FANN (Fast Artifical Neural Network) in Crystal,https://www.reddit.com/r/MachineLearning/comments/6le1ha/crystalfann_bindings_for_fann_fast_artifical/,[deleted],1499261670,[deleted],0,1
161,2017-7-5,2017,7,5,22,6le3k7,Crystal Languge bindings for FANN (Fast Artifical Neural Network),https://www.reddit.com/r/MachineLearning/comments/6le3k7/crystal_languge_bindings_for_fann_fast_artifical/,Ba7a7chy,1499262306,,0,5
162,2017-7-5,2017,7,5,22,6le3pr,[N] Android of self-driving cars open sourced by Baidu Research,https://www.reddit.com/r/MachineLearning/comments/6le3pr/n_android_of_selfdriving_cars_open_sourced_by/,pabloesm,1499262347,,3,21
163,2017-7-5,2017,7,5,22,6le6ef,"DeepMind expands to Canada with new research office in Edmonton, Alberta",https://www.reddit.com/r/MachineLearning/comments/6le6ef/deepmind_expands_to_canada_with_new_research/,circuithunter,1499263157,,0,0
164,2017-7-5,2017,7,5,23,6leage,"[N] DeepMind expands to Canada with new research office in Edmonton, Alberta to be led by Richard Sutton",https://www.reddit.com/r/MachineLearning/comments/6leage/n_deepmind_expands_to_canada_with_new_research/,cherls,1499264272,,47,235
165,2017-7-5,2017,7,5,23,6leeka,[D] Speed up Sklearn Algorithms Calling Custom Metrics Using Cython,https://www.reddit.com/r/MachineLearning/comments/6leeka/d_speed_up_sklearn_algorithms_calling_custom/,redaBoumahdi,1499265438,,4,3
166,2017-7-6,2017,7,6,0,6lejsc,"Running data science in an Agile (Scrum, Kanban, etc) environment. Experience + best practices, tips, etc",https://www.reddit.com/r/MachineLearning/comments/6lejsc/running_data_science_in_an_agile_scrum_kanban_etc/,[deleted],1499266892,[removed],0,1
167,2017-7-6,2017,7,6,0,6lekac,"[D] Running data science in an Agile (Scrum, Kanban, etc) environment. Experience + best practices, tips, etc",https://www.reddit.com/r/MachineLearning/comments/6lekac/d_running_data_science_in_an_agile_scrum_kanban/,thatguydr,1499267025,"Hi all,

There was a [thread yesterday](https://www.reddit.com/r/MachineLearning/comments/6l70m4/d_top_articles_in_data_in_the_last_month/djrnuqv/) about how to run data science in Agile environments, be they Scrum, Kanban, or whatever flavor you desire.

There's a lot of really good information in that thread, but since the entire thread was slightly OT for the OP, I figured a dedicated post might pull in more experience (and might share it to a wider audience in general).

Please post any tips/tricks/questions about how you have been (or want to be) running data science in an Agile environment. Bonus points if you can detail what performance, timing, efficiency, or any other improvements you've seen along the way.",5,9
168,2017-7-6,2017,7,6,0,6lekzc,K-Means Clustering - The Math of Intelligence (Week 3),https://www.reddit.com/r/MachineLearning/comments/6lekzc/kmeans_clustering_the_math_of_intelligence_week_3/,funmaster11,1499267204,,0,1
169,2017-7-6,2017,7,6,0,6leoaw,"How to tell whether a sound clip is a music, noise or speech?",https://www.reddit.com/r/MachineLearning/comments/6leoaw/how_to_tell_whether_a_sound_clip_is_a_music_noise/,foolooo,1499268077,[removed],0,1
170,2017-7-6,2017,7,6,0,6letbg,[P]Deep-eye-ballers: Scatter2Pearson - Training CNNs to regress the correlation coefficient from scatterplots,https://www.reddit.com/r/MachineLearning/comments/6letbg/pdeepeyeballers_scatter2pearson_training_cnns_to/,VinayUPrabhu,1499269419,,5,14
171,2017-7-6,2017,7,6,0,6lew3v,"Simple Questions Thread July 05, 2017",https://www.reddit.com/r/MachineLearning/comments/6lew3v/simple_questions_thread_july_05_2017/,AutoModerator,1499270133,[removed],0,1
172,2017-7-6,2017,7,6,1,6lf1d7,"[D] Favoring recall on a single bucket, or different ways to weight loss functions",https://www.reddit.com/r/MachineLearning/comments/6lf1d7/d_favoring_recall_on_a_single_bucket_or_different/,realSatanAMA,1499271467,"Some primer, I'm doing securities prediction work where I'm attempting to classify data into BUY, WAIT and SELL classes.  I've been experimenting with a wrapper categorical_crossentropy function that allows me to multiply constants with the error to favor misclassifications of BUY and SELL targets as WAIT predictions over misclassifications of BUY or SELL targets as SELL or BUY predictions.. but I've been trying to think of a way to push the model toward 100% recall of the WAIT class while growing the precision on BUY and SELL.  Generally I was just curious if anyone has seen any papers, examples of different ways to weight buckets or push model training in specific directions other than weighting errors for specific target-&gt;prediction pairs.  Absolutely anything you have seen that you could share will interest me.

Edit: This discussion is what led me down this path: https://github.com/fchollet/keras/issues/2115",0,6
173,2017-7-6,2017,7,6,1,6lf57v,[D] Can we optimize for F1 score directly,https://www.reddit.com/r/MachineLearning/comments/6lf57v/d_can_we_optimize_for_f1_score_directly/,matrix2596,1499272413,Is there a way to create a differentiable way to optimize for F1 score directly? Instead of optimising for criterion loss and then thresholding.,11,19
174,2017-7-6,2017,7,6,2,6lfbfg,"Dope Learning: A.I. Can Now Best Rappers With ""Deep Beats"" - TOPBOTS",https://www.reddit.com/r/MachineLearning/comments/6lfbfg/dope_learning_ai_can_now_best_rappers_with_deep/,conradcreates,1499274026,,0,1
175,2017-7-6,2017,7,6,2,6lfiwh,NIPS 2017: Non-targeted Adversarial Attack | Kaggle,https://www.reddit.com/r/MachineLearning/comments/6lfiwh/nips_2017_nontargeted_adversarial_attack_kaggle/,cbeak,1499275811,,0,1
176,2017-7-6,2017,7,6,2,6lfjvl,Embed to Control implementation in PyTorch,https://www.reddit.com/r/MachineLearning/comments/6lfjvl/embed_to_control_implementation_in_pytorch/,ethanluoyc,1499276044,,0,1
177,2017-7-6,2017,7,6,2,6lfogd,Would you like to co-author a time series data mining paper with Dr. Eamonn Keoghs Lab?,https://www.reddit.com/r/MachineLearning/comments/6lfogd/would_you_like_to_coauthor_a_time_series_data/,eamonnkeogh,1499277166,[removed],0,1
178,2017-7-6,2017,7,6,2,6lfoz7,FAIR: Sentence embeddings (InferSent) and training code for NLI,https://www.reddit.com/r/MachineLearning/comments/6lfoz7/fair_sentence_embeddings_infersent_and_training/,shagunsodhani,1499277309,,0,1
179,2017-7-6,2017,7,6,3,6lfsbr,Best way to annotate images and create datasets for training a network,https://www.reddit.com/r/MachineLearning/comments/6lfsbr/best_way_to_annotate_images_and_create_datasets/,SciLnLowFi,1499278113,[removed],0,1
180,2017-7-6,2017,7,6,3,6lg41g,Classifying Kmeans Clustering Result,https://www.reddit.com/r/MachineLearning/comments/6lg41g/classifying_kmeans_clustering_result/,arrajaa,1499281063,[removed],0,1
181,2017-7-6,2017,7,6,4,6lg9ry,[R] Scaling Machine Learning as a Service,https://www.reddit.com/r/MachineLearning/comments/6lg9ry/r_scaling_machine_learning_as_a_service/,villasv,1499282478,,0,6
182,2017-7-6,2017,7,6,4,6lgcsw,PyUNLocBoX: convex optimization in Python (based on proximal splitting methods),https://www.reddit.com/r/MachineLearning/comments/6lgcsw/pyunlocbox_convex_optimization_in_python_based_on/,m_deff,1499283190,,0,1
183,2017-7-6,2017,7,6,5,6lgp8j,Calling Data Science/Fin-tech/Deep Learning Speakers,https://www.reddit.com/r/MachineLearning/comments/6lgp8j/calling_data_sciencefintechdeep_learning_speakers/,PotterBear,1499286222,[removed],0,1
184,2017-7-6,2017,7,6,5,6lgx1x,Powerful and cheap servers with GPUs in FlyElephant Cloud,https://www.reddit.com/r/MachineLearning/comments/6lgx1x/powerful_and_cheap_servers_with_gpus_in/,flyelephant,1499288195,,0,1
185,2017-7-6,2017,7,6,6,6lh4qn,Optimizing User-Product Matching Economies,https://www.reddit.com/r/MachineLearning/comments/6lh4qn/optimizing_userproduct_matching_economies/,softwaredoug,1499290193,,0,1
186,2017-7-6,2017,7,6,6,6lha5z,Do I need to learn ML or are there tools for these apllications,https://www.reddit.com/r/MachineLearning/comments/6lha5z/do_i_need_to_learn_ml_or_are_there_tools_for/,nfsi0,1499291627,[removed],0,1
187,2017-7-6,2017,7,6,7,6lhgap,4D filters/ feature detectors for light fields,https://www.reddit.com/r/MachineLearning/comments/6lhgap/4d_filters_feature_detectors_for_light_fields/,Khensura25,1499293281,[removed],0,1
188,2017-7-6,2017,7,6,9,6li5ck,[P] Two Decades of Recommender Systems at Amazon.com,https://www.reddit.com/r/MachineLearning/comments/6li5ck/p_two_decades_of_recommender_systems_at_amazoncom/,pp314159,1499300695,,0,48
189,2017-7-6,2017,7,6,9,6li6pe,Handyman and Home Services,https://www.reddit.com/r/MachineLearning/comments/6li6pe/handyman_and_home_services/,gwenyosef,1499301120,,0,1
190,2017-7-6,2017,7,6,9,6li7g2,[D] Could genetic algorithms be used to create new more efficient algorithms in machine learning?,https://www.reddit.com/r/MachineLearning/comments/6li7g2/d_could_genetic_algorithms_be_used_to_create_new/,Gurung77,1499301344,,10,5
191,2017-7-6,2017,7,6,10,6likt5,[D] Any recommendations on tutorials about statistical tests for comparing machine learning algorithms?,https://www.reddit.com/r/MachineLearning/comments/6likt5/d_any_recommendations_on_tutorials_about/,murakamifanboy,1499305362,"I stumbled upon [""A practical tutorial on the use of nonparametric statistical tests as a methodology for comparing evolutionary and swarm intelligence algorithms"" (Derrac et al., 2011)](http://www.sciencedirect.com/science/article/pii/S2210650211000034) and I was wondering if you know about a similar paper/book  more oriented to machine learning algorithms. ",7,2
192,2017-7-6,2017,7,6,10,6lim4l,Automatic rotary high speed plastic screw capping machine factory price,https://www.reddit.com/r/MachineLearning/comments/6lim4l/automatic_rotary_high_speed_plastic_screw_capping/,hymachinery,1499305735,,1,1
193,2017-7-6,2017,7,6,11,6litj8,Scalable methods for calibration of classifiers,https://www.reddit.com/r/MachineLearning/comments/6litj8/scalable_methods_for_calibration_of_classifiers/,Iamabandit,1499308006,[removed],0,1
194,2017-7-6,2017,7,6,12,6lj295,[R] [1707.01083] ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,https://www.reddit.com/r/MachineLearning/comments/6lj295/r_170701083_shufflenet_an_extremely_efficient/,zxytim,1499310633,,20,40
195,2017-7-6,2017,7,6,14,6ljmwl,Building a Sound Classifier from scratch using Neural Networks,https://www.reddit.com/r/MachineLearning/comments/6ljmwl/building_a_sound_classifier_from_scratch_using/,[deleted],1499317645,[deleted],0,1
196,2017-7-6,2017,7,6,14,6ljnjb,[P] Building a Sound Classifier from scratch using Neural Networks,https://www.reddit.com/r/MachineLearning/comments/6ljnjb/p_building_a_sound_classifier_from_scratch_using/,chipcrazy,1499317898,,2,15
197,2017-7-6,2017,7,6,14,6ljp1g,[D] Do any of these Online Master's Programs Transition well into the Pursuit of an AI/ML PhD?,https://www.reddit.com/r/MachineLearning/comments/6ljp1g/d_do_any_of_these_online_masters_programs/,ferrous_joe,1499318478,"I'm currently looking at taking the online Berkeley Data Science Master's course, and I am curious if this would lend towards the pursuit of an AI/ML PhD some years down the road.

I was initially considering this a simple question, but then it dawned on me that these programs are still very new. It may be more pertinent to seek a discussion rather than an outright yes/no answer.

Prospective students, graduates, in-person Master's students, current PhD candidates, and PhD holders: what are your feelings on this?

Are these programs a reasonable stepping stone towards a PhD later in your career--just as an in-person Master's degree might be?

Do these offer no more clout or mobility than bootcamps and MOOCs?

I'd love to hear your thoughts!",14,12
198,2017-7-6,2017,7,6,14,6ljrpv,Where Machine Learning meets rule-based verification,https://www.reddit.com/r/MachineLearning/comments/6ljrpv/where_machine_learning_meets_rulebased/,yoav_hollander,1499319473,,0,2
199,2017-7-6,2017,7,6,15,6ljv4q,How to proceed further in Deep learning?,https://www.reddit.com/r/MachineLearning/comments/6ljv4q/how_to_proceed_further_in_deep_learning/,rjmessibarca,1499320820,[removed],0,1
200,2017-7-6,2017,7,6,15,6ljyzm,[P] Pedestrian Alignment for Person Re-identification,https://www.reddit.com/r/MachineLearning/comments/6ljyzm/p_pedestrian_alignment_for_person_reidentification/,zhedongzheng,1499322327,,2,3
201,2017-7-6,2017,7,6,15,6lk1yh,Build an AI Programmer using Recurrent Neural Network,https://www.reddit.com/r/MachineLearning/comments/6lk1yh/build_an_ai_programmer_using_recurrent_neural/,ryanlr,1499323527,,0,1
202,2017-7-6,2017,7,6,16,6lk6cu,[D] Is training a NN to mimic a closed-source library legal ?,https://www.reddit.com/r/MachineLearning/comments/6lk6cu/d_is_training_a_nn_to_mimic_a_closedsource/,DrPharael,1499325414,"Let's imagine the following situation.
John has access to the binaries of a closed source library that computes some nice image filtering, which means that he can apply it to any input image.
Now he would like to get rid of the dependency on this library and have his own filter but does not have time to do all the research, so he trains a regression CNN (that produces filtered images) on a dataset which he creates by considering a lot of images, and - as a ground truth - the output of the library on such images.

Do you have any insight on where this stands from a legal point a view ? Is it considered as IP infringement, or maybe reverse-engineering ? Does it only depend on the license agreement of the closed-source library ?",144,314
203,2017-7-6,2017,7,6,16,6lk7yi,Top 5 Machine Learning Tools For Developers,https://www.reddit.com/r/MachineLearning/comments/6lk7yi/top_5_machine_learning_tools_for_developers/,digitalmarketingrobi,1499326098,,0,1
204,2017-7-6,2017,7,6,18,6lkkrf,Semi Automatic Single Head Pneumatic Piston Liquid Filling Machine,https://www.reddit.com/r/MachineLearning/comments/6lkkrf/semi_automatic_single_head_pneumatic_piston/,hymachinery,1499331985,,1,1
205,2017-7-6,2017,7,6,18,6lkmdl,Predicting position of object using image and previous position as inputs,https://www.reddit.com/r/MachineLearning/comments/6lkmdl/predicting_position_of_object_using_image_and/,[deleted],1499332680,[removed],0,1
206,2017-7-6,2017,7,6,18,6lkmzu,[D] Explicitly targeting differences in encoder-decoder networks,https://www.reddit.com/r/MachineLearning/comments/6lkmzu/d_explicitly_targeting_differences_in/,vintermann,1499332959,"I had a little idea for a project, and I wondered if anyone has tried something similar, or know of papers that discuss it.

The idea is about transforming images a la smile vector or faceapp. The way such apps works, as far as I know, is that they train a network to embed images (or maybe parts of images?) as vectors, and then work out what transformations of the vectors correspond to the desired transformation of the images.

But if the end goal is to transform images, wouldn't it be possible to target that directly instead? Say you train an encoder on pairs of images, A and B. Instead of trying to learn a representation for the full images, it learns a vector V representing the work needed to transform one image to the other. The decoder would get A and V or B and -V randomly, and the loss would be from how similar the decoded image was to the image it didn't get.

My intuition is that it may be possible to do transformations on larger images this way, since it only has to represent the changes and not the whole image. I don't think I have seen any papers or public projects trying this, but maybe they tried it and it didn't work?",4,5
207,2017-7-6,2017,7,6,18,6lknsz,[D] Predicting object position using image of it and previous position,https://www.reddit.com/r/MachineLearning/comments/6lknsz/d_predicting_object_position_using_image_of_it/,Cheese-Gimbap,1499333338,"I'm relatively new to this field but wanted help on how to predict the position (x,y) of an object using an image of it and the previous position.  The image of the object and it's position are used as the training set.  Several positions and images are used to train the model.  When an object is located in a position that was not initially trained, it should still predict the position.  

Any info on helping me get started on this would be appreciated! ",4,1
208,2017-7-6,2017,7,6,18,6lkogy,Using relu activation function destroys the model.,https://www.reddit.com/r/MachineLearning/comments/6lkogy/using_relu_activation_function_destroys_the_model/,AyushExel,1499333652,[removed],0,1
209,2017-7-6,2017,7,6,18,6lkp8e,Mini 3D Printer for Virtual Product Design,https://www.reddit.com/r/MachineLearning/comments/6lkp8e/mini_3d_printer_for_virtual_product_design/,LessDeal14,1499333988,,1,1
210,2017-7-6,2017,7,6,21,6llhit,[D] Softmax interpretation with non 1-hot labels,https://www.reddit.com/r/MachineLearning/comments/6llhit/d_softmax_interpretation_with_non_1hot_labels/,Reykd,1499344921,"Training a softmax classifier with a cross-entropy loss is usually interpreted as minimizing the negative log-likelihood. However, this interpretation only holds when the labels, or targets, that we are trying to learn are 1-hot vectors.

When the targets are a probability distribution how do we interpret a softmax classifier with a cross-entropy loss? Does the fact that we are no longer minimizing the negative log-likelihood affect convergence? And if so how?

Edit: modified description, non 1-hot to probability distribution",24,4
211,2017-7-6,2017,7,6,22,6llnwv,The Ultimate Directory Of AI &amp; Bot Conferences - TOPBOTS,https://www.reddit.com/r/MachineLearning/comments/6llnwv/the_ultimate_directory_of_ai_bot_conferences/,conradcreates,1499346966,,0,1
212,2017-7-6,2017,7,6,22,6llp68,Introduction to Core ML and Machine Learning in iOS,https://www.reddit.com/r/MachineLearning/comments/6llp68/introduction_to_core_ml_and_machine_learning_in/,elitechsystems,1499347367,,0,1
213,2017-7-6,2017,7,6,22,6llrkt,[N] How to Handle Imbalanced Classes in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/6llrkt/n_how_to_handle_imbalanced_classes_in_machine/,digitalson,1499348060,,0,1
214,2017-7-6,2017,7,6,22,6lltcw,[N] Machine learning applied to showers in the OPERA,https://www.reddit.com/r/MachineLearning/comments/6lltcw/n_machine_learning_applied_to_showers_in_the_opera/,molode,1499348589,,0,1
215,2017-7-6,2017,7,6,22,6llunq,Where machine learning meets rule-based verification,https://www.reddit.com/r/MachineLearning/comments/6llunq/where_machine_learning_meets_rulebased/,yoav_hollander,1499348969,,3,4
216,2017-7-6,2017,7,6,23,6lm0dt,[R] Main recommendation system algorithms and how they work,https://www.reddit.com/r/MachineLearning/comments/6lm0dt/r_main_recommendation_system_algorithms_and_how/,trumtra,1499350532,,0,1
217,2017-7-6,2017,7,6,23,6lm529,[R] Main recommendation system algorithms and how they work,https://www.reddit.com/r/MachineLearning/comments/6lm529/r_main_recommendation_system_algorithms_and_how/,luba_belokon,1499351809,,0,21
218,2017-7-6,2017,7,6,23,6lm5p4,[P] Evaluation code for various automated metrics for Natural Language Generation,https://www.reddit.com/r/MachineLearning/comments/6lm5p4/p_evaluation_code_for_various_automated_metrics/,deeplearningnlp,1499351975,,2,21
219,2017-7-6,2017,7,6,23,6lm75m,Cleaning Supplies and Products,https://www.reddit.com/r/MachineLearning/comments/6lm75m/cleaning_supplies_and_products/,gwenyosef,1499352354,,0,1
220,2017-7-7,2017,7,7,0,6lmb6o,[N] AI and Machine Learning in the future of animation.,https://www.reddit.com/r/MachineLearning/comments/6lmb6o/n_ai_and_machine_learning_in_the_future_of/,Another4Milos,1499353371,,0,2
221,2017-7-7,2017,7,7,0,6lmhcc,[N] Building a SEO tool with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/6lmhcc/n_building_a_seo_tool_with_machine_learning/,lalypopa123,1499354900,,0,1
222,2017-7-7,2017,7,7,0,6lmlzr,Cascade configuration : Cascade classifiers - What papers to read ?,https://www.reddit.com/r/MachineLearning/comments/6lmlzr/cascade_configuration_cascade_classifiers_what/,Screye,1499356033,[removed],0,1
223,2017-7-7,2017,7,7,0,6lmm9z,On-device feature extraction: A crash course in data sketches,https://www.reddit.com/r/MachineLearning/comments/6lmm9z/ondevice_feature_extraction_a_crash_course_in/,[deleted],1499356101,[deleted],0,1
224,2017-7-7,2017,7,7,1,6lmt07,[R] SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Cross-lingual Focused Evaluation - Findings of the 2017 shared task on computing semantic textual similarity.,https://www.reddit.com/r/MachineLearning/comments/6lmt07/r_semeval2017_task_1_semantic_textual_similarity/,danielcer,1499357718,,0,6
225,2017-7-7,2017,7,7,1,6lmulx,Bringing Machine Learning to your iOS Apps,https://www.reddit.com/r/MachineLearning/comments/6lmulx/bringing_machine_learning_to_your_ios_apps/,allllamas,1499358121,,0,1
226,2017-7-7,2017,7,7,1,6lmy4f,Looking for advice on CNN architecture,https://www.reddit.com/r/MachineLearning/comments/6lmy4f/looking_for_advice_on_cnn_architecture/,peep186,1499358973,[removed],0,1
227,2017-7-7,2017,7,7,2,6ln46e,"Banks Eager For Artificial Intelligence, But Slow To Adopt - TOPBOTS",https://www.reddit.com/r/MachineLearning/comments/6ln46e/banks_eager_for_artificial_intelligence_but_slow/,conradcreates,1499360429,,0,1
228,2017-7-7,2017,7,7,2,6ln57r,How can ML help a local gym,https://www.reddit.com/r/MachineLearning/comments/6ln57r/how_can_ml_help_a_local_gym/,tgs266,1499360673,[removed],0,1
229,2017-7-7,2017,7,7,2,6lnian,Bolt Manufacturing on Traub A25 Automatic Screw Machine/Automatic Lathe Machine,https://www.reddit.com/r/MachineLearning/comments/6lnian/bolt_manufacturing_on_traub_a25_automatic_screw/,trendingfacts,1499363771,,0,1
230,2017-7-7,2017,7,7,3,6lnoi2,[D] Machine learning mod bot,https://www.reddit.com/r/MachineLearning/comments/6lnoi2/d_machine_learning_mod_bot/,MechanicalOrange5,1499365179,"I've been dabbling with creating a bot that will monitor posts in a subreddit and report those that break the rules so that a mod can deal with it. So to keep this short here is what I've done so far:

* Trained a Doc2Vec model with 8.5 million post titles (at 100, and 200 dimensions)

* Pulled mod actions from reddit as training data. I grab the post title (cleaned using nltk) and [1,0] == remove post and [0,1] == approve post

* Created Doc2Vec vectors of all of the post titles in the training data

* Ran tf-idf on the training data titles

* I then concatenate the tf-idf 100 best features and Doc2Vec vec together to produce one large feature matrix

* Then I scaled all of the features using sklearn featurescaler 

* Feed all of this into a multilayer perceptron with dropout built with tensorflow

This achieves an accuracy of about 78% or so, which seemed alright until I realised that is literally just the distribution of my training data that is about 78% remove and 22% approve. I hacked an example of a Text CNN I found online to work on my data which achivies an accuracy of about 82% at best, which means it's only 4% better than guessing randomly according to the probability distribution of the training data. 

So now comes my question. How do I improve this guy's accuracy? Is there another method I should try, or features I should add to the existing one, or modifications I should make? Sorry if this is too noob for this subreddit, but I'm in a bit over my head I think",6,1
231,2017-7-7,2017,7,7,3,6lnwt6,What's happening with the ImageNet Challenge?,https://www.reddit.com/r/MachineLearning/comments/6lnwt6/whats_happening_with_the_imagenet_challenge/,[deleted],1499367166,[removed],0,1
232,2017-7-7,2017,7,7,4,6lo1i9,Why would anyone use Machine Learning to analyze data ?,https://www.reddit.com/r/MachineLearning/comments/6lo1i9/why_would_anyone_use_machine_learning_to_analyze/,CheBurashka_GG,1499368275,[removed],0,1
233,2017-7-7,2017,7,7,4,6lo3va,[D] When not to use deep learning,https://www.reddit.com/r/MachineLearning/comments/6lo3va/d_when_not_to_use_deep_learning/,hardmaru,1499368854,,45,213
234,2017-7-7,2017,7,7,4,6lo3xd,[D] How does Theano compute the jacobian-vector product quickly (R operator)?,https://www.reddit.com/r/MachineLearning/comments/6lo3xd/d_how_does_theano_compute_the_jacobianvector/,deltasheep1,1499368869,"When using the chain rule for backprop, there are a lot of jacobians (derivative of output with respect to input) times vectors (derivative of loss with respect to output). For arbitrary tensors, the jacobian can become huge and computing it explicitly is costly (especially because it's just a diagonal matrix for all activation functions), so Theano implements [the R operator](http://deeplearning.net/software/theano/tutorial/gradients.html#r-operator) to do it quickly. Theano [cites](http://deeplearning.net/software/theano/tutorial/gradients.html#jacobian-times-a-vector) [Barak A. Pearlmutter, Fast Exact Multiplication by the Hessian, Neural Computation, 1994](http://www.bcl.hamilton.ie/~barak/papers/nc-hessian.pdf) for the theory behind the R operator, but I only see the algorithm for a fast exact hessian-vector product here, not the jacobian-vector product.

What is the algorithm that Theano uses for fast jacobian-vector products?",5,6
235,2017-7-7,2017,7,7,5,6los5n,[P] Machine Learning Survey for student research project,https://www.reddit.com/r/MachineLearning/comments/6los5n/p_machine_learning_survey_for_student_research/,delitee,1499374687,,6,0
236,2017-7-7,2017,7,7,6,6low6l,"UbuntuDL | All-in-one Deep Learning Install Script for Ubuntu 16.04 LTS (Caffe, Tensorflow, Keras, Pytorch, CUDA 8.0, CUDNN 5.1)",https://www.reddit.com/r/MachineLearning/comments/6low6l/ubuntudl_allinone_deep_learning_install_script/,bjabr,1499375639,,0,1
237,2017-7-7,2017,7,7,6,6lp3y2,[D] Using Tensorflow for ETL purposes?,https://www.reddit.com/r/MachineLearning/comments/6lp3y2/d_using_tensorflow_for_etl_purposes/,Fender6969,1499377619,"Hello,

I have a project that I have been assigned to use Tensorflow to train an AI to properly identify date patterns in a CSV file, and then classify the column in that CSV file as a date.

I have posted on here previously and I have checked out the very helpful links that have been provided, but those examples, and the majority online seem to focus solely on object recognition. My purpose is to train the model to identify a specific pattern in a CSV file accurately.

My boss seems to be set on using Tensorflow to do this. I come from a stats background, and while I can code on Python, I am very new to Tensorflow. 

Any help would be great!",29,6
238,2017-7-7,2017,7,7,7,6lp7lc,[D] how to do multi-modal data integration other than DBMs?,https://www.reddit.com/r/MachineLearning/comments/6lp7lc/d_how_to_do_multimodal_data_integration_other/,NiraSherwood58,1499378537,"Hi, 
Im interested in multi-modality (image, speech, text, etc) data integration for classification. 
I know data fusion can be done using Deep Boltzmann Machines, the very first paper in this area was Multimodal Learning with Deep Boltzmann Machines in 2014 in which they used DBMs to create a unified representation of multi-modal data (images, text) that can be used for classification. Currently, It seems like that DBM direction of research in deep learning is not hot anymore and people are not publishing a lot like before (correct me if i am wrong). So, what is the trend right now in deep learning (to exclude multi-kernel learning!) for  multi-modal data intermediate (not ensembled) integration in your opinion ? ",8,6
239,2017-7-7,2017,7,7,7,6lpcck,Building an Operating System for AI,https://www.reddit.com/r/MachineLearning/comments/6lpcck/building_an_operating_system_for_ai/,peckjon,1499379806,,0,1
240,2017-7-7,2017,7,7,7,6lpg16,The difference between Artificial Intelligence and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/6lpg16/the_difference_between_artificial_intelligence/,JKSamir,1499380829,,0,1
241,2017-7-7,2017,7,7,7,6lpgnh,[D] How to tell 'how documented' a prediction is when using a SVM,https://www.reddit.com/r/MachineLearning/comments/6lpgnh/d_how_to_tell_how_documented_a_prediction_is_when/,msftsummerintern,1499380993,"What is the best way to tell how close a guess is to grouping of other points like it? The best way to summarize this is through [this image](https://i.stack.imgur.com/EG0eB.png)

Guesses that are close to the other points are accepted, while ones that are far away are rejected as not being documented enough for the SVM to give a good guess. 

Is there a good way to go about implementing this?",3,3
242,2017-7-7,2017,7,7,8,6lpk6v,Applying Deep Learning To Real World Problems - TOPBOTS,https://www.reddit.com/r/MachineLearning/comments/6lpk6v/applying_deep_learning_to_real_world_problems/,conradcreates,1499382011,,0,1
243,2017-7-7,2017,7,7,8,6lpt6k,What algorithms other than CNNs are good for image classification?,https://www.reddit.com/r/MachineLearning/comments/6lpt6k/what_algorithms_other_than_cnns_are_good_for/,democritus_is_op,1499384553,[removed],0,1
244,2017-7-7,2017,7,7,10,6lqi16,Electric Hotwater System and Supplier,https://www.reddit.com/r/MachineLearning/comments/6lqi16/electric_hotwater_system_and_supplier/,gwenyosef,1499391939,,0,1
245,2017-7-7,2017,7,7,11,6lql9z,Facebook introduces ELF: A platform for reinforcement learning research in game environments,https://www.reddit.com/r/MachineLearning/comments/6lql9z/facebook_introduces_elf_a_platform_for/,corysama,1499392947,,0,1
246,2017-7-7,2017,7,7,12,6lr2t2,[D] What are the implications of relational reasoning (as talked about here) on NLP?,https://www.reddit.com/r/MachineLearning/comments/6lr2t2/d_what_are_the_implications_of_relational/,transtwin,1499398509,,16,12
247,2017-7-7,2017,7,7,13,6lr82n,How far can we go in CIFAR-10 without CNNs?,https://www.reddit.com/r/MachineLearning/comments/6lr82n/how_far_can_we_go_in_cifar10_without_cnns/,[deleted],1499400215,[removed],0,1
248,2017-7-7,2017,7,7,13,6lran0,How good can we do at CIFAR-10 without CNNs?,https://www.reddit.com/r/MachineLearning/comments/6lran0/how_good_can_we_do_at_cifar10_without_cnns/,Veboy,1499401068,[removed],0,1
249,2017-7-7,2017,7,7,13,6lraxu,[D] How do you do backprop by hand? Simple example.,https://www.reddit.com/r/MachineLearning/comments/6lraxu/d_how_do_you_do_backprop_by_hand_simple_example/,[deleted],1499401165,[removed],0,1
250,2017-7-7,2017,7,7,15,6lrur6,Good haar cascades or LBP for facial detection?,https://www.reddit.com/r/MachineLearning/comments/6lrur6/good_haar_cascades_or_lbp_for_facial_detection/,[deleted],1499408421,[removed],0,1
251,2017-7-7,2017,7,7,15,6lrw90,BAIR Blog Post on Constrained Policy Optimization,https://www.reddit.com/r/MachineLearning/comments/6lrw90/bair_blog_post_on_constrained_policy_optimization/,tensor_every_day20,1499409048,,0,1
252,2017-7-7,2017,7,7,16,6ls0s9,[D] Why isn't Hessian-free optimization more popular?,https://www.reddit.com/r/MachineLearning/comments/6ls0s9/d_why_isnt_hessianfree_optimization_more_popular/,deltasheep1,1499410859,"After reading 

1. [""Fast Exact Multiplication by the Hessian"" -
 Pearlmutter, 1993](http://www.bcl.hamilton.ie/~barak/papers/nc-hessian.pdf)

2. [this blog post](http://andrew.gibiansky.com/blog/machine-learning/hessian-free-optimization/) 
3. and skimming [""Deep learning via Hessian-free optimization"" - Martens, ICML 2010](http://icml2010.haifa.il.ibm.com/papers/458.pdf)

I am really surprised that I haven't seen more Hessian-free optimization (HFO) around, even though it seems like it's all-around better than gradient descent (except that it's more difficult to implement). For example, [it didn't even generate enough buzz when brought up in TensorFlow to stay an open issue](https://github.com/tensorflow/tensorflow/issues/2682).

Why don't I see more HFO?",19,86
253,2017-7-7,2017,7,7,16,6ls135,Learning from Human Preferences,https://www.reddit.com/r/MachineLearning/comments/6ls135/learning_from_human_preferences/,alexeyr,1499410976,,0,1
254,2017-7-7,2017,7,7,16,6ls6tg,GANs N' Roses,https://www.reddit.com/r/MachineLearning/comments/6ls6tg/gans_n_roses/,naresh1318,1499413486,,0,1
255,2017-7-7,2017,7,7,17,6lsdrw,Corporate Video Production JMaverick Studios 888 435-JMAV - YouTube,https://www.reddit.com/r/MachineLearning/comments/6lsdrw/corporate_video_production_jmaverick_studios_888/,EstelleBenchh80,1499416776,,0,1
256,2017-7-7,2017,7,7,18,6lshl9,Automatic Top Surface Plane Labeling Machine,https://www.reddit.com/r/MachineLearning/comments/6lshl9/automatic_top_surface_plane_labeling_machine/,hymachinery,1499418519,,0,1
257,2017-7-7,2017,7,7,18,6lsnni,[R] [1707.00768] Learning to Avoid Errors in GANs by Manipulating Input Spaces,https://www.reddit.com/r/MachineLearning/comments/6lsnni/r_170700768_learning_to_avoid_errors_in_gans_by/,wychtl,1499421207,,3,14
258,2017-7-7,2017,7,7,19,6lssit,TensorFlow LSTM benchmark,https://www.reddit.com/r/MachineLearning/comments/6lssit/tensorflow_lstm_benchmark/,albertzeyer,1499423289,,0,1
259,2017-7-7,2017,7,7,20,6lt4mr,ESD/Anti-Static Finger Cot,https://www.reddit.com/r/MachineLearning/comments/6lt4mr/esdantistatic_finger_cot/,LessDeal14,1499428173,,1,1
260,2017-7-7,2017,7,7,21,6lt7um,Thermal Cutting Process,https://www.reddit.com/r/MachineLearning/comments/6lt7um/thermal_cutting_process/,Messer-123,1499429287,,0,1
261,2017-7-7,2017,7,7,21,6ltarv,Structured medical data for analysis,https://www.reddit.com/r/MachineLearning/comments/6ltarv/structured_medical_data_for_analysis/,[deleted],1499430272,[removed],0,1
262,2017-7-7,2017,7,7,21,6ltepg,[R] [1707.01495v1] Hindsight Experience Replay,https://www.reddit.com/r/MachineLearning/comments/6ltepg/r_170701495v1_hindsight_experience_replay/,pauljasek,1499431560,,3,31
263,2017-7-7,2017,7,7,22,6lthdi,A Brief History Of Neural Network Architectures - TOPBOTS,https://www.reddit.com/r/MachineLearning/comments/6lthdi/a_brief_history_of_neural_network_architectures/,conradcreates,1499432427,,0,1
264,2017-7-7,2017,7,7,23,6ltz4x,[P] A Python implementation for GP CaKe: a nonparametric Bayesian causal connectivity,https://www.reddit.com/r/MachineLearning/comments/6ltz4x/p_a_python_implementation_for_gp_cake_a/,bleekselderij,1499437466,,7,95
265,2017-7-8,2017,7,8,0,6lubsw,[R] [1707.01836] Cardiologist-Level Arrhythmia Detection with Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/6lubsw/r_170701836_cardiologistlevel_arrhythmia/,Pfohlol,1499440743,,16,29
266,2017-7-8,2017,7,8,1,6lumh5,How to build my own machine learning program,https://www.reddit.com/r/MachineLearning/comments/6lumh5/how_to_build_my_own_machine_learning_program/,wanggobbler,1499443431,[removed],0,1
267,2017-7-8,2017,7,8,1,6luu3k,Where to go after Andrew Ng's Coursera course?,https://www.reddit.com/r/MachineLearning/comments/6luu3k/where_to_go_after_andrew_ngs_coursera_course/,spartan12321,1499445246,[removed],0,1
268,2017-7-8,2017,7,8,2,6lv0in,Exploring LSTMs: Understanding Basics (Part One) - TOPBOTS,https://www.reddit.com/r/MachineLearning/comments/6lv0in/exploring_lstms_understanding_basics_part_one/,conradcreates,1499446833,,0,1
269,2017-7-8,2017,7,8,2,6lv3rc,How to efficiently split images to use for training and testing,https://www.reddit.com/r/MachineLearning/comments/6lv3rc/how_to_efficiently_split_images_to_use_for/,[deleted],1499447642,[removed],0,1
270,2017-7-8,2017,7,8,2,6lv5au,[D] Deep Learning spent several decades waiting for hardware to catch up to theory -- are there other advanced ML theories still waiting for hardware?,https://www.reddit.com/r/MachineLearning/comments/6lv5au/d_deep_learning_spent_several_decades_waiting_for/,8solutions,1499448016,"My understanding of the history might be a little off, but it seems like deep learning was first introduced a few decades ago. The potential of the approach was generally understood but recent GPU advancements made them useful.

Are there other exciting algorithms that are just waiting for next-level hardware (eg. quantum).",41,65
271,2017-7-8,2017,7,8,2,6lv7aq,"Introducing Kinase, B12s Web Content Labeling Framework",https://www.reddit.com/r/MachineLearning/comments/6lv7aq/introducing_kinase_b12s_web_content_labeling/,auxpro,1499448479,,0,1
272,2017-7-8,2017,7,8,2,6lvb6i,[R] Dual Supervised Learning,https://www.reddit.com/r/MachineLearning/comments/6lvb6i/r_dual_supervised_learning/,wei_jok,1499449413,,1,32
273,2017-7-8,2017,7,8,2,6lve9d,Spare Time Fun Project - Machine Learning For Marketing,https://www.reddit.com/r/MachineLearning/comments/6lve9d/spare_time_fun_project_machine_learning_for/,[deleted],1499450156,[removed],1,1
274,2017-7-8,2017,7,8,3,6lvj1b,[P] Lightweight pix2pix Tensorflow implementation!,https://www.reddit.com/r/MachineLearning/comments/6lvj1b/p_lightweight_pix2pix_tensorflow_implementation/,shabeyyub,1499451278,,0,8
275,2017-7-8,2017,7,8,3,6lvj6d,Exploring LSTMs: Investigating Internals (Part Two) - TOPBOTS,https://www.reddit.com/r/MachineLearning/comments/6lvj6d/exploring_lstms_investigating_internals_part_two/,conradcreates,1499451311,,0,1
276,2017-7-8,2017,7,8,3,6lvlp9,Ensembling classifiers through correlation and accuracy,https://www.reddit.com/r/MachineLearning/comments/6lvlp9/ensembling_classifiers_through_correlation_and/,sexPekes,1499451910,[removed],0,1
277,2017-7-8,2017,7,8,3,6lvn02,Neural Networks - The Math of Intelligence #4,https://www.reddit.com/r/MachineLearning/comments/6lvn02/neural_networks_the_math_of_intelligence_4/,funmaster11,1499452228,,0,0
278,2017-7-8,2017,7,8,3,6lvphu,Have any of you used quantum computing for machine learning?,https://www.reddit.com/r/MachineLearning/comments/6lvphu/have_any_of_you_used_quantum_computing_for/,ryches,1499452817,[removed],0,1
279,2017-7-8,2017,7,8,3,6lvrzf,"[N] Source Code for ""Self Driving Car Learns Online and On-board on Raspberry Pi 3""",https://www.reddit.com/r/MachineLearning/comments/6lvrzf/n_source_code_for_self_driving_car_learns_online/,CireNeikual,1499453430,,7,52
280,2017-7-8,2017,7,8,4,6lvyxo,[R] Estimating means in a finite universe (Monte Carlo alternatives),https://www.reddit.com/r/MachineLearning/comments/6lvyxo/r_estimating_means_in_a_finite_universe_monte/,undefdev,1499455054,,0,9
281,2017-7-8,2017,7,8,4,6lvz7g,Free Webinar on How is AI/Deep Learning disrupting trading.,https://www.reddit.com/r/MachineLearning/comments/6lvz7g/free_webinar_on_how_is_aideep_learning_disrupting/,FintechNerd,1499455124,,0,1
282,2017-7-8,2017,7,8,4,6lw54a,"Understanding the distinction between Topological Data Analysis and Clustering (+ segementation, hot spot analysis...)",https://www.reddit.com/r/MachineLearning/comments/6lw54a/understanding_the_distinction_between_topological/,jtsymonds,1499456560,,0,1
283,2017-7-8,2017,7,8,7,6lx2gm,Visualizing High Dimensional Data in Augmented Reality,https://www.reddit.com/r/MachineLearning/comments/6lx2gm/visualizing_high_dimensional_data_in_augmented/,alexa_y,1499465182,,0,1
284,2017-7-8,2017,7,8,7,6lx7xs,Looking for introductions to neural networks for game AI. I am fluent in Python but programming language is not a deal-breaker.,https://www.reddit.com/r/MachineLearning/comments/6lx7xs/looking_for_introductions_to_neural_networks_for/,unknownharris,1499466697,[removed],0,1
285,2017-7-8,2017,7,8,7,6lx8no,"[D] On ""Online and Linear-Time Attention by Enforcing Monotonic Alignments""",https://www.reddit.com/r/MachineLearning/comments/6lx8no/d_on_online_and_lineartime_attention_by_enforcing/,gwern,1499466906,,0,6
286,2017-7-8,2017,7,8,7,6lxarl,[R] Dual Path Networks,https://www.reddit.com/r/MachineLearning/comments/6lxarl/r_dual_path_networks/,[deleted],1499467494,[deleted],0,1
287,2017-7-8,2017,7,8,7,6lxbim,[R] Dual Path Networks -&gt; New ImageNet SOTA,https://www.reddit.com/r/MachineLearning/comments/6lxbim/r_dual_path_networks_new_imagenet_sota/,xternalz,1499467712,,6,24
288,2017-7-8,2017,7,8,8,6lxmr2,Using CNN to detect houses from aerial imagery,https://www.reddit.com/r/MachineLearning/comments/6lxmr2/using_cnn_to_detect_houses_from_aerial_imagery/,tmsbn,1499471066,[removed],0,1
289,2017-7-8,2017,7,8,9,6lxuy9,[D] Potential impact of extraneous functions in Tensorflow,https://www.reddit.com/r/MachineLearning/comments/6lxuy9/d_potential_impact_of_extraneous_functions_in/,[deleted],1499473589,[deleted],1,0
290,2017-7-8,2017,7,8,10,6lyalg,Use TensorFlow to guess a books author,https://www.reddit.com/r/MachineLearning/comments/6lyalg/use_tensorflow_to_guess_a_books_author/,McCossum,1499478786,[removed],0,1
291,2017-7-8,2017,7,8,12,6lyqhy,Do GANs actually do distribution learning?,https://www.reddit.com/r/MachineLearning/comments/6lyqhy/do_gans_actually_do_distribution_learning/,vackosar,1499484302,,0,1
292,2017-7-8,2017,7,8,12,6lys5w,automatic high speed bottle unscrambling machine factory price,https://www.reddit.com/r/MachineLearning/comments/6lys5w/automatic_high_speed_bottle_unscrambling_machine/,hymachinery,1499484885,,0,1
293,2017-7-8,2017,7,8,13,6lywcx,Made a script to give your Jupyter Notebook a public url instantly :),https://www.reddit.com/r/MachineLearning/comments/6lywcx/made_a_script_to_give_your_jupyter_notebook_a/,[deleted],1499486401,[deleted],0,1
294,2017-7-8,2017,7,8,13,6lyy5h,[P] Made a script to give your Jupyter Notebook a public url instantly :),https://www.reddit.com/r/MachineLearning/comments/6lyy5h/p_made_a_script_to_give_your_jupyter_notebook_a/,bsubs,1499487027,,57,209
295,2017-7-8,2017,7,8,14,6lzd1s,Machine Learning in prosthetics.,https://www.reddit.com/r/MachineLearning/comments/6lzd1s/machine_learning_in_prosthetics/,kidpopcorntime,1499492765,[removed],0,1
296,2017-7-8,2017,7,8,15,6lzh0y,[D] Thomas Huijskens - Bayesian optimisation with scikit-learn,https://www.reddit.com/r/MachineLearning/comments/6lzh0y/d_thomas_huijskens_bayesian_optimisation_with/,_alphamaximus_,1499494466,,0,7
297,2017-7-8,2017,7,8,18,6m012t,"[R] Neural Network Learns The Physics of Fluids and Smoke [video, about arXiv:1607.03597]",https://www.reddit.com/r/MachineLearning/comments/6m012t/r_neural_network_learns_the_physics_of_fluids_and/,pmigdal,1499504545,,7,85
298,2017-7-8,2017,7,8,20,6m0ho1,Use of Generative Adversarial Networks to solve real problems,https://www.reddit.com/r/MachineLearning/comments/6m0ho1/use_of_generative_adversarial_networks_to_solve/,pevnak,1499513224,[removed],0,1
299,2017-7-8,2017,7,8,21,6m0s5v,How to integrate Microsoft Cognitive Services with C# in less than 60 minutes,https://www.reddit.com/r/MachineLearning/comments/6m0s5v/how_to_integrate_microsoft_cognitive_services/,maguirej160,1499517916,,0,1
300,2017-7-8,2017,7,8,21,6m0u47,Deep Learning projects!!,https://www.reddit.com/r/MachineLearning/comments/6m0u47/deep_learning_projects/,divyanshjha,1499518735,[removed],0,1
301,2017-7-8,2017,7,8,22,6m0ws5,PCIe speed,https://www.reddit.com/r/MachineLearning/comments/6m0ws5/pcie_speed/,Springmute,1499519770,[removed],0,1
302,2017-7-8,2017,7,8,22,6m11cl,How to approach regression with varying number of covariates?,https://www.reddit.com/r/MachineLearning/comments/6m11cl/how_to_approach_regression_with_varying_number_of/,GreenHamster1975,1499521478,[removed],0,1
303,2017-7-8,2017,7,8,23,6m16wd,Public Datasets for ML,https://www.reddit.com/r/MachineLearning/comments/6m16wd/public_datasets_for_ml/,curious_rv,1499523439,,0,1
304,2017-7-8,2017,7,8,23,6m17yg,Help me understand something about neural net,https://www.reddit.com/r/MachineLearning/comments/6m17yg/help_me_understand_something_about_neural_net/,Eildosa,1499523781,[removed],0,1
305,2017-7-8,2017,7,8,23,6m1a00,[D] Hugo Larochelle (Google Brain) slides on properties of neural networks we currently don't understand,https://www.reddit.com/r/MachineLearning/comments/6m1a00/d_hugo_larochelle_google_brain_slides_on/,NiraSherwood58,1499524494,,15,250
306,2017-7-9,2017,7,9,0,6m1lpt,Is it worth it to learn about genetic algorithms now or modern neural networks are way more efficient?,https://www.reddit.com/r/MachineLearning/comments/6m1lpt/is_it_worth_it_to_learn_about_genetic_algorithms/,unknownharris,1499528209,[removed],0,1
307,2017-7-9,2017,7,9,1,6m1zw7,Tensorflow Documentation as PDF,https://www.reddit.com/r/MachineLearning/comments/6m1zw7/tensorflow_documentation_as_pdf/,tensor1098,1499532423,[removed],0,1
308,2017-7-9,2017,7,9,2,6m27k5,Machine learning,https://www.reddit.com/r/MachineLearning/comments/6m27k5/machine_learning/,sappractices,1499534662,,0,1
309,2017-7-9,2017,7,9,2,6m2bxh,Image Captioning using InceptionV3 and Beam Search,https://www.reddit.com/r/MachineLearning/comments/6m2bxh/image_captioning_using_inceptionv3_and_beam_search/,[deleted],1499535942,[deleted],0,1
310,2017-7-9,2017,7,9,2,6m2cie,Librarian: Chrome extension to get direct links to references in arXiv papers,https://www.reddit.com/r/MachineLearning/comments/6m2cie/librarian_chrome_extension_to_get_direct_links_to/,mgdo,1499536098,,1,1
311,2017-7-9,2017,7,9,3,6m2fcq,[R] Would you like to co-author a time series data mining paper with Dr. Eamonn Keoghs Lab?,https://www.reddit.com/r/MachineLearning/comments/6m2fcq/r_would_you_like_to_coauthor_a_time_series_data/,eamonnkeogh,1499536934,"Dear Colleagues

Would you like to co-author a paper with Dr. Eamonn Keoghs Lab? The paper, to be submitted to the Data Mining Journal [a] in late Summer or early Fall, is likely to be one of the top ten most cited paper in the journal [b]. Obviously, we cannot guarantee that paper will be accepted, but with 20 papers in the journal, Dr. Keogh is currently the most prolific author in the Data Mining Journal, and will submit a very strong paper.

In brief, the proposed paper claims that, given a data structure called the Matrix Profile, you can often add just 5 to 10 lines of code, to solve an interesting problem. Thus far there are 3 concrete examples:

1) With the Matrix Profile and 6 lines of code, you can solve the Discovering Motifs Under Uniform Scaling problem.

2) With the Matrix Profile and 2 lines of code, you can solve the Discovering Time Series Semordnilaps problem.

3) With the Matrix Profile and 2 lines of code, you can solve the Discovering Time Series Reverse Complements problem.

4) &lt;your idea here&gt;


We plan to find about seven more such ideas. If you contribute an idea (and ideally, with a dataset and a little writing / motivation) we will add you as a co-author.

What do you have to do? 

1) Read the main Matrix Profile paper [c]. Note that you dont need to understand HOW the Matrix Profile is computed, only what the Matrix Profile IS.

2) (optional) Read some of the other Matrix Profile papers, play with the GUI for the Matrix Profile [d].

3) Skim the (rough draft of) the Matrix Profile tutorial [e].

4) Read the rough draft of the paper in question The Swiss Army Knife of Time Series Data Mining [f].

5) Come up with an idea, the shorter and simpler the better! Send your idea to eamonn@cs.ucr.edu



Best wishes, The MP team
----

Basic Rules 

1) If two people give us the exact same idea, we will pick the first person (and show the later person the email time stamps as proof). However, if the two people have related ideas that we can join together somehow, we will try to combine them and offer both co-authorship.

2) Any ideas you submit are 100% yours, you can publish your own spin-off papers, start a company with them etc. However, anything you submit is going to be seen by at least a dozen individuals, dont submit anything you want to patent or to be keep top-secret for some reason.


[a] https://link.springer.com/journal/10618 

[b] This is a strong claim, but according to the editor, Keogh has two of the top ten most cited papers from this decade, and two of the top ten most cited papers from the last decade. This paper will also be highly cited.

[c] http://www.cs.ucr.edu/~eamonn/PID4481997_extend_Matrix%20Profile_I.pdf 

[d] http://www.cs.ucr.edu/~eamonn/MatrixProfile.html 

[e] http://www.cs.ucr.edu/~eamonn/Matrix_Profile_Tutorial.pptx 

[f] http://www.cs.ucr.edu/~eamonn/Top_Ten_Things_Matrix_Profile_CrowdSource.pdf",0,0
312,2017-7-9,2017,7,9,3,6m2j3q,Why is there no research in vector graphics w.r.t Machine learning.,https://www.reddit.com/r/MachineLearning/comments/6m2j3q/why_is_there_no_research_in_vector_graphics_wrt/,minnuu,1499537996,[removed],0,1
313,2017-7-9,2017,7,9,4,6m2tje,[D] KL divergence decreases to a point and then starts increasing in VAE?,https://www.reddit.com/r/MachineLearning/comments/6m2tje/d_kl_divergence_decreases_to_a_point_and_then/,anonDogeLover,1499540986,"Is this normal behavior? I was thinking this may mark the point where the network has nothing left to do but pack more information into the latent, which requires a lower divergence.",9,5
314,2017-7-9,2017,7,9,4,6m2yu6,Learn machine learning,https://www.reddit.com/r/MachineLearning/comments/6m2yu6/learn_machine_learning/,tacaro25,1499542555,[removed],1,1
315,2017-7-9,2017,7,9,5,6m38t2,[P] Attempted implementation of Solomonoff induction,https://www.reddit.com/r/MachineLearning/comments/6m38t2/p_attempted_implementation_of_solomonoff_induction/,thai_tong,1499545520,"Solomonoff induction and its use of the Kolmogorov complexity has fascinated me, to me it's way to overcoming overfitting which happens when we apply more and more complex models to explain the data. I wanted to use it as a way to objectively determine when to switch from a simple model to a more complicated one.

An actual implementation of Solomonoff induction is computationally prohibitive. I wanted to try out a reduced version of the induction to see how well it would work. I'm not claiming that I actually implemented Solomonoff's theory, this is an attempt at a practical approximation of it.

**Models**

Rather than considering every possible model which can be written in code I consider a few models, they were:

- Constant y=a

- Inverse y=a/x

- Linear y=ax+b

- Quadratic y=ax^2 +bx+c

- Power law y=a*e^bx

**Introducing non-determinism**

The first challenge is that Solomonoff's approach is purely deterministic, the coded models produce a string output and if the string output doesn't exactly match the target output then the model is considered wrong. To introduce the statistical noise I limit the noise term to having a normal distribution with 0 mean. I use Jeffrey's prior for the variance of the noise term, this leads to a likelihood estimate which is inversely proportional to the root-mean-squared error (Very convenient). Since all models have the same noise term the additional complexity in the model which describes the noise has no effect on the posterior probabilities.

**Using model MLE**

Solomonoff's approach would consider all possible values of the parameters and would calculate the kolmogorov complexity of each case. This requires too much computation so I considered only the complexity of the model with the parameters left as variables in the code. In this way when calculating the priors I consider the prior probability of the family of models; there is no prior over the parameters, only a prior probability for the type of model. The posterior probability is calculated using the MLE parameters. I believe this creates a bias in favour of more complex models.

**Model program length**

To trial this idea I wrote functions for each model in as few characters as possible. Where L is the length of the function the prior probability is 2^(-L). It's best to use a low-level programming language, a high level language favours complex models. I wrote the functions in Python but restricted its functionality to add/subtract/multiply/divide functions in order to simulate a low-level language. I admit this is a very limited approach but it's convenient for testing out the idea.

The prior probabilities are as follows:

- Constant 0.998

- Inverse 1.9E-3

- Linear 1.2E-4

- Quadratic 4.8E-7

- Power law 1.8E-40 (coding the exponential function takes a LOT of characters)

**Test design &amp; Results**

I simulated data from the function y={1-2x if x&lt;=0.5, 0 if x&gt;0.5. x was restricted to the range [0,1]. [Here](https://imgur.com/a/AjOHy) is an image of 100 data points produced.

The results were disappointing, the ranking of model likelihoods did not change after applying a Bayesian update. The posterior probability of the constant model dropped to 99.9% of its prior value, the inverse model's probability remained about the same and the remaining models; probability approximately doubled.

The priors are highly sensitive to how the models are encoded- a single extra character halves the prior probability of a model. This really limits how well approximations and simplifications can work.

**TL;DR**

I attempted a bastardized version of Solomonoff induction and it didn't work. I'd like to hear your suggestions on how to use a less restrictive simplification.",11,19
316,2017-7-9,2017,7,9,5,6m3b3f,Tensorflow trolled Advert,https://www.reddit.com/r/MachineLearning/comments/6m3b3f/tensorflow_trolled_advert/,graes11,1499546203,,0,1
317,2017-7-9,2017,7,9,6,6m3kfx,Other communities?,https://www.reddit.com/r/MachineLearning/comments/6m3kfx/other_communities/,TypedInt,1499549063,[removed],0,1
318,2017-7-9,2017,7,9,7,6m3s0b,[P] Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/6m3s0b/p_generative_adversarial_networks/,jguertl,1499551428,,0,0
319,2017-7-9,2017,7,9,7,6m41z8,"I want to learn CUDA. Is the Jetson TK1 enough, or should I save and get the TX1?",https://www.reddit.com/r/MachineLearning/comments/6m41z8/i_want_to_learn_cuda_is_the_jetson_tk1_enough_or/,[deleted],1499554597,[removed],0,1
320,2017-7-9,2017,7,9,8,6m446c,[Discussion] Could ML + robotics be used to replace skilled workers? Details inside,https://www.reddit.com/r/MachineLearning/comments/6m446c/discussion_could_ml_robotics_be_used_to_replace/,[deleted],1499555323,[deleted],2,0
321,2017-7-9,2017,7,9,8,6m45ei,"Installing CUDA 8.0, cuDNN 5.1, Tensorflow, and Keras on Ubuntu 16.04",https://www.reddit.com/r/MachineLearning/comments/6m45ei/installing_cuda_80_cudnn_51_tensorflow_and_keras/,[deleted],1499555743,[deleted],0,1
322,2017-7-9,2017,7,9,10,6m4owe,Quickly modify a script and rerun without reloading a dataset.,https://www.reddit.com/r/MachineLearning/comments/6m4owe/quickly_modify_a_script_and_rerun_without/,se4u,1499562318,,0,1
323,2017-7-9,2017,7,9,10,6m4vy0,GANs for handwriting synthesis,https://www.reddit.com/r/MachineLearning/comments/6m4vy0/gans_for_handwriting_synthesis/,quarkquantum,1499564853,[removed],0,1
324,2017-7-9,2017,7,9,10,6m4w6k,Here are precompiled binaries for running PyTorch with CUDA on Python 3.5+ on Windows,https://www.reddit.com/r/MachineLearning/comments/6m4w6k/here_are_precompiled_binaries_for_running_pytorch/,Phylliida,1499564938,,0,1
325,2017-7-9,2017,7,9,11,6m51db,[D] Benchmarking TensorFlow on Google Pre-emptible Cloud CPUs: Cheaper Deep Learning than Cloud GPUs,https://www.reddit.com/r/MachineLearning/comments/6m51db/d_benchmarking_tensorflow_on_google_preemptible/,gwern,1499566834,,8,20
326,2017-7-9,2017,7,9,12,6m5csz,[D] Deep learning in the brain,https://www.reddit.com/r/MachineLearning/comments/6m5csz/d_deep_learning_in_the_brain/,breandan,1499571123,,48,95
327,2017-7-9,2017,7,9,15,6m65fk,Are there any good neural network tutorials that don't require you to know a lot of math?,https://www.reddit.com/r/MachineLearning/comments/6m65fk/are_there_any_good_neural_network_tutorials_that/,8Warden12,1499583446,[removed],0,1
328,2017-7-9,2017,7,9,18,6m6kkc,Matrix decomposition for recom. systems,https://www.reddit.com/r/MachineLearning/comments/6m6kkc/matrix_decomposition_for_recom_systems/,oqowa,1499591710,[removed],0,1
329,2017-7-9,2017,7,9,19,6m6piq,"PyTorch implementation of ""Attention Is All You Need""",https://www.reddit.com/r/MachineLearning/comments/6m6piq/pytorch_implementation_of_attention_is_all_you/,klagra,1499594524,,0,46
330,2017-7-9,2017,7,9,19,6m6u4l,"PyTorch implementation of ""Attention Is All You Need""",https://www.reddit.com/r/MachineLearning/comments/6m6u4l/pytorch_implementation_of_attention_is_all_you/,Sandyleap,1499597190,,0,1
331,2017-7-9,2017,7,9,20,6m6xv5,[D] Do numerical inaccuracies play any role in training neural networks?,https://www.reddit.com/r/MachineLearning/comments/6m6xv5/d_do_numerical_inaccuracies_play_any_role_in/,themoosemind,1499599093,,14,29
332,2017-7-9,2017,7,9,22,6m7e7i,Masters in the US after Trump administration,https://www.reddit.com/r/MachineLearning/comments/6m7e7i/masters_in_the_us_after_trump_administration/,betmenot,1499606419,[removed],0,1
333,2017-7-10,2017,7,10,0,6m7vq4,A simple explanation of back-propagation. The third part of a series on my youtube channel with more to come.,https://www.reddit.com/r/MachineLearning/comments/6m7vq4/a_simple_explanation_of_backpropagation_the_third/,lochiewestfall,1499612597,,0,1
334,2017-7-10,2017,7,10,0,6m84mj,[R] An overview of gradient descent optimization algorithms,https://www.reddit.com/r/MachineLearning/comments/6m84mj/r_an_overview_of_gradient_descent_optimization/,fl4v1,1499615337,,15,72
335,2017-7-10,2017,7,10,1,6m8726,Study of Overflow Vulnerabilities on GPUs,https://www.reddit.com/r/MachineLearning/comments/6m8726/study_of_overflow_vulnerabilities_on_gpus/,[deleted],1499616083,[deleted],0,1
336,2017-7-10,2017,7,10,1,6m88mb,I made a parody song about deep learning (xpost /r/deeplearning),https://www.reddit.com/r/MachineLearning/comments/6m88mb/i_made_a_parody_song_about_deep_learning_xpost/,zephiem,1499616524,,0,1
337,2017-7-10,2017,7,10,2,6m8m5m,"I applied multi-task and multimodal deep learning to financial forecasting, check out how it works.",https://www.reddit.com/r/MachineLearning/comments/6m8m5m/i_applied_multitask_and_multimodal_deep_learning/,rachnogstyle,1499620509,,0,1
338,2017-7-10,2017,7,10,2,6m8tp0,[P] Deep learning for estimating race and ethnicity from electronic medical records (GitHub + arXiv),https://www.reddit.com/r/MachineLearning/comments/6m8tp0/p_deep_learning_for_estimating_race_and_ethnicity/,[deleted],1499622623,[deleted],5,13
339,2017-7-10,2017,7,10,2,6m8vi7,"Hello redditors , we just started an online shop and we thought it would be great to make a GIVEAWAY! So we are giving 20 cupons for a MEACHANICAL LEATHER WATCH(normally 100$)! Use this coupon code : 4REVIEW in checkout and get 100% discount! All you have to do is leave a review and pay shipping!",https://www.reddit.com/r/MachineLearning/comments/6m8vi7/hello_redditors_we_just_started_an_online_shop/,ksparling1430,1499623124,,0,1
340,2017-7-10,2017,7,10,3,6m984e,[D] What auto-encoders could learn from brains,https://www.reddit.com/r/MachineLearning/comments/6m984e/d_what_autoencoders_could_learn_from_brains/,breandan,1499626672,,6,52
341,2017-7-10,2017,7,10,4,6m9c1o,[D] How does iterative hyperparameter search not lead to overfitting?,https://www.reddit.com/r/MachineLearning/comments/6m9c1o/d_how_does_iterative_hyperparameter_search_not/,ClownShoesFL,1499627827,"I've read many sources that state iterative hyperparameter search is done by performance on a test set. In other words, new points should be selected based on test set performance. Wouldn't this count as a form of data-peeking and just lead to overfitting the test set, regardless of the method used?",13,32
342,2017-7-10,2017,7,10,5,6m9l1v,[D] Machine Learning - WAYR (What Are You Reading) - Week 29,https://www.reddit.com/r/MachineLearning/comments/6m9l1v/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,1499630404,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|
|----|-----|-----|
|[Week 1](https://www.reddit.com/r/MachineLearning/comments/4qyjiq/machine_learning_wayr_what_are_you_reading_week_1/)|[Week 11](https://www.reddit.com/r/MachineLearning/comments/57xw56/discussion_machine_learning_wayr_what_are_you/)|[Week 21](https://www.reddit.com/r/MachineLearning/comments/60ildf/d_machine_learning_wayr_what_are_you_reading_week/)|
|[Week 2](https://www.reddit.com/r/MachineLearning/comments/4s2xqm/machine_learning_wayr_what_are_you_reading_week_2/)|[Week 12](https://www.reddit.com/r/MachineLearning/comments/5acb1t/d_machine_learning_wayr_what_are_you_reading_week/)|[Week 22](https://www.reddit.com/r/MachineLearning/comments/64jwde/d_machine_learning_wayr_what_are_you_reading_week/)||
|[Week 3](https://www.reddit.com/r/MachineLearning/comments/4t7mqm/machine_learning_wayr_what_are_you_reading_week_3/)|[Week 13](https://www.reddit.com/r/MachineLearning/comments/5cwfb6/d_machine_learning_wayr_what_are_you_reading_week/)|[Week 23](https://www.reddit.com/r/MachineLearning/comments/674331/d_machine_learning_wayr_what_are_you_reading_week/)||
|[Week 4](https://www.reddit.com/r/MachineLearning/comments/4ub2kw/machine_learning_wayr_what_are_you_reading_week_4/)|[Week 14](https://www.reddit.com/r/MachineLearning/comments/5fc5mh/d_machine_learning_wayr_what_are_you_reading_week/)|[Week 24](https://www.reddit.com/r/MachineLearning/comments/68hhhb/d_machine_learning_wayr_what_are_you_reading_week/)||
|[Week 5](https://www.reddit.com/r/MachineLearning/comments/4xomf7/machine_learning_wayr_what_are_you_reading_week_5/)|[Week 15](https://www.reddit.com/r/MachineLearning/comments/5hy4ur/d_machine_learning_wayr_what_are_you_reading_week/)|[Week 25](https://www.reddit.com/r/MachineLearning/comments/69teiz/d_machine_learning_wayr_what_are_you_reading_week/)||
|[Week 6](https://www.reddit.com/r/MachineLearning/comments/4zcyvk/machine_learning_wayr_what_are_you_reading_week_6/)|[Week 16](https://www.reddit.com/r/MachineLearning/comments/5kd6vd/d_machine_learning_wayr_what_are_you_reading_week/)|[Week 26](https://www.reddit.com/r/MachineLearning/comments/6d7nb1/d_machine_learning_wayr_what_are_you_reading_week/)||
|[Week 7](https://www.reddit.com/r/MachineLearning/comments/52t6mo/machine_learning_wayr_what_are_you_reading_week_7/)|[Week 17](https://www.reddit.com/r/MachineLearning/comments/5ob7dx/discussion_machine_learning_wayr_what_are_you/)|[Week 27](https://www.reddit.com/r/MachineLearning/comments/6gngwc/d_machine_learning_wayr_what_are_you_reading_week/)||
|[Week 8](https://www.reddit.com/r/MachineLearning/comments/53heol/machine_learning_wayr_what_are_you_reading_week_8/)|[Week 18](https://www.reddit.com/r/MachineLearning/comments/5r14yd/discussion_machine_learning_wayr_what_are_you/)|[Week 28](https://www.reddit.com/r/MachineLearning/comments/6jgdva/d_machine_learning_wayr_what_are_you_reading_week/)||
|[Week 9](https://www.reddit.com/r/MachineLearning/comments/54kvsu/machine_learning_wayr_what_are_you_reading_week_9/)|[Week 19](https://www.reddit.com/r/MachineLearning/comments/5tt9cz/discussion_machine_learning_wayr_what_are_you/)||
|[Week 10](https://www.reddit.com/r/MachineLearning/comments/56s2oa/discussion_machine_learning_wayr_what_are_you/)|[Week 20](https://www.reddit.com/r/MachineLearning/comments/5wh2wb/d_machine_learning_wayr_what_are_you_reading_week/)||

Most upvoted papers two weeks ago:

/u/VordeMan: [Graphical Models, Exponential Families, and
Variational Inference](https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf)

/u/lmcinnes: [The surprising secret identity of the semidefinite relaxation of K-means: manifold learning](https://arxiv.org/abs/1706.06028)

Besides that, there are no rules, have fun.",39,74
343,2017-7-10,2017,7,10,5,6m9rlk,[P] UMAP - Uniform Manifold Approximation and Projection. Aiming for better dimension reduction than t-SNE,https://www.reddit.com/r/MachineLearning/comments/6m9rlk/p_umap_uniform_manifold_approximation_and/,arisbw,1499632251,,10,71
344,2017-7-10,2017,7,10,5,6m9sqf,[R] A Deep Network with Visual Text Composition Behavior,https://www.reddit.com/r/MachineLearning/comments/6m9sqf/r_a_deep_network_with_visual_text_composition/,visarga,1499632565,,1,3
345,2017-7-10,2017,7,10,6,6ma02e,[Discussion] Which foreign languages (not programming languages) are beneficial for a career in ML,https://www.reddit.com/r/MachineLearning/comments/6ma02e/discussion_which_foreign_languages_not/,JackRoco,1499634646,"Which foreign languages (not programming languages) do you think are beneficial for a career in the field of machine learning, data science or similar?

I already speak English and German and want to learn an additional language. I am currently doing my master's and want to pursue a career in data science outside of academia. Which language would be most beneficial for my career?

I think overall Spanish is a very important language, but I don't know it's value in the field of machine learning.

I know that the Chinese are very strong in the field, therefore I am considering to study Mandarin. I think the Chinese market is not very accessible for foreigners though.

What other languages do you think are useful? Do you speak foreign languages at work? Which countries does your company work with the most?",16,11
346,2017-7-10,2017,7,10,6,6ma07e,How AI detectives are cracking open the black box of deep learning,https://www.reddit.com/r/MachineLearning/comments/6ma07e/how_ai_detectives_are_cracking_open_the_black_box/,wabberjockey,1499634688,,0,1
347,2017-7-10,2017,7,10,7,6mahhb,I need data,https://www.reddit.com/r/MachineLearning/comments/6mahhb/i_need_data/,wanggobbler,1499639833,[removed],0,1
348,2017-7-10,2017,7,10,8,6mapdk,Applying 11 Inception v3 models to 12 photos,https://www.reddit.com/r/MachineLearning/comments/6mapdk/applying_11_inception_v3_models_to_12_photos/,phobrain,1499642269,,1,1
349,2017-7-10,2017,7,10,8,6mapsf,Get arbitrary sized feature maps,https://www.reddit.com/r/MachineLearning/comments/6mapsf/get_arbitrary_sized_feature_maps/,outofusernames01,1499642402,[removed],0,1
350,2017-7-10,2017,7,10,9,6mazba,Construction Builders,https://www.reddit.com/r/MachineLearning/comments/6mazba/construction_builders/,gwenyosef,1499645448,,0,1
351,2017-7-10,2017,7,10,9,6mb0r4,[D] When not to use deep learning,https://www.reddit.com/r/MachineLearning/comments/6mb0r4/d_when_not_to_use_deep_learning/,[deleted],1499645872,[deleted],0,0
352,2017-7-10,2017,7,10,10,6mb9yv,[D] Theory in original GAN paper,https://www.reddit.com/r/MachineLearning/comments/6mb9yv/d_theory_in_original_gan_paper/,zeromaxy,1499648909,"https://arxiv.org/pdf/1406.2661.pdf

Question 1:

Proof of proposition 1. Why do the authors completely disregard the integral sign and go on to minimizing the function under the integral sign? I verified their proof, I just don't understand what allows them to disregard the integral?

Question 2:

From where did the KL appear in Theorem 1? I guess I realize how they get the -log(4), but why are they later subtracting those 2 KLs?

I feel like in both cases the authors took some leaps of faith which are understandable for experts in the field, whereas I am not one and I seek your help or guidance to materials thoroughly explaining my two questions.",12,13
353,2017-7-10,2017,7,10,10,6mbh2q,I know this is anything too special just finished a javascript 3 layer neural network from scratch as a learning tool.,https://www.reddit.com/r/MachineLearning/comments/6mbh2q/i_know_this_is_anything_too_special_just_finished/,[deleted],1499651240,[deleted],0,1
354,2017-7-10,2017,7,10,10,6mbj20,[P] I know this is anything too special just finished a javascript 3 layer neural network from scratch as a learning tool.,https://www.reddit.com/r/MachineLearning/comments/6mbj20/p_i_know_this_is_anything_too_special_just/,kcnklub,1499651913,,2,5
355,2017-7-10,2017,7,10,11,6mbqqh,small capacity bottle washing filling capping machine line for non air d...,https://www.reddit.com/r/MachineLearning/comments/6mbqqh/small_capacity_bottle_washing_filling_capping/,hymachinery,1499654557,,0,1
356,2017-7-10,2017,7,10,12,6mbwky,What is the current status of technology for real time translation?,https://www.reddit.com/r/MachineLearning/comments/6mbwky/what_is_the_current_status_of_technology_for_real/,localfield,1499656567,[removed],0,1
357,2017-7-10,2017,7,10,12,6mc0bc,Shouldn't momentum updates be weighted?,https://www.reddit.com/r/MachineLearning/comments/6mc0bc/shouldnt_momentum_updates_be_weighted/,eddlie,1499657900,[removed],0,1
358,2017-7-10,2017,7,10,12,6mc23g,A comprehensive and organized collection of resources for TensorFlow,https://www.reddit.com/r/MachineLearning/comments/6mc23g/a_comprehensive_and_organized_collection_of/,irsina,1499658529,,0,1
359,2017-7-10,2017,7,10,13,6mccqi,[P] Pytorch implementation for fine tuning and feature extraction,https://www.reddit.com/r/MachineLearning/comments/6mccqi/p_pytorch_implementation_for_fine_tuning_and/,meliketoy,1499662473,,0,1
360,2017-7-10,2017,7,10,14,6mclj5,[1707.01476] Convolutional 2D Knowledge Graph Embeddings -&gt; SOTA on Graph NN tasks &amp; Showing triviality of standard datasets,https://www.reddit.com/r/MachineLearning/comments/6mclj5/170701476_convolutional_2d_knowledge_graph/,node_feature,1499665935,,1,1
361,2017-7-10,2017,7,10,15,6mcoqo,Carpentry Basics and Projects,https://www.reddit.com/r/MachineLearning/comments/6mcoqo/carpentry_basics_and_projects/,gwenyosef,1499667249,,0,1
362,2017-7-10,2017,7,10,16,6mcwts,12 Situations Data Scientists Will Totally Relate To,https://www.reddit.com/r/MachineLearning/comments/6mcwts/12_situations_data_scientists_will_totally_relate/,ram_ilan,1499670752,,0,1
363,2017-7-10,2017,7,10,16,6mcxgc,"My iOS app doesn't machine-learning algorithms, but I would like to hear your opinion about it",https://www.reddit.com/r/MachineLearning/comments/6mcxgc/my_ios_app_doesnt_machinelearning_algorithms_but/,sabiland,1499671017,,1,1
364,2017-7-10,2017,7,10,17,6md74q,[D] Poll: how many using TensorFlow but not the other stuff (like seq2seq or T2T)?,https://www.reddit.com/r/MachineLearning/comments/6md74q/d_poll_how_many_using_tensorflow_but_not_the/,petrux,1499675637,Hello there. Just for curiosity: how many of you are using `TensorFlow` *but not* other Google side-libraries like `seq2seq2` and/or `T2T`? Thanks.,8,0
365,2017-7-10,2017,7,10,17,6md7gs,automatic double side labeling application machine manufacturer price,https://www.reddit.com/r/MachineLearning/comments/6md7gs/automatic_double_side_labeling_application/,hymachinery,1499675820,,0,1
366,2017-7-10,2017,7,10,17,6md898,SigNet: Convolutional Siamese Network for Writer Independent Offline Signature Verification,https://www.reddit.com/r/MachineLearning/comments/6md898/signet_convolutional_siamese_network_for_writer/,AnjanDutta,1499676178,,1,1
367,2017-7-10,2017,7,10,18,6mdbem,[P] Semantic Segmentation using Fully Convolutional Networks over the years + PyTorch Code,https://www.reddit.com/r/MachineLearning/comments/6mdbem/p_semantic_segmentation_using_fully_convolutional/,stochastic_zeitgeist,1499677684,,4,128
368,2017-7-10,2017,7,10,19,6mdiwe,TensorFlow Projector - Different behaviour to other tSNE implementations?,https://www.reddit.com/r/MachineLearning/comments/6mdiwe/tensorflow_projector_different_behaviour_to_other/,seb706,1499681151,[removed],0,1
369,2017-7-10,2017,7,10,19,6mdjnu,Powerful Fusion To Definitely Grow Your Business,https://www.reddit.com/r/MachineLearning/comments/6mdjnu/powerful_fusion_to_definitely_grow_your_business/,nexcorp,1499681463,,0,1
370,2017-7-10,2017,7,10,20,6mdqpa,[P] Gesture Recognition using Convolution Neural Network,https://www.reddit.com/r/MachineLearning/comments/6mdqpa/p_gesture_recognition_using_convolution_neural/,asingh33,1499684527,,3,30
371,2017-7-10,2017,7,10,20,6mdtn4,Deep Learning | Machine Learning Company in India | Data Science,https://www.reddit.com/r/MachineLearning/comments/6mdtn4/deep_learning_machine_learning_company_in_india/,webtunixml,1499685689,,0,1
372,2017-7-10,2017,7,10,20,6mdxq3,[P] Braindecode: Deep learning toolbox for EEG (PyTorch),https://www.reddit.com/r/MachineLearning/comments/6mdxq3/p_braindecode_deep_learning_toolbox_for_eeg/,robintibor,1499687287,,12,25
373,2017-7-10,2017,7,10,21,6mdzzx,An Introduction to Machine Learning!,https://www.reddit.com/r/MachineLearning/comments/6mdzzx/an_introduction_to_machine_learning/,ajhalthor,1499688120,,0,1
374,2017-7-10,2017,7,10,21,6me12h,Consuming Azure Machine Learning in ASP.NET Core,https://www.reddit.com/r/MachineLearning/comments/6me12h/consuming_azure_machine_learning_in_aspnet_core/,remotesynth,1499688479,,0,1
375,2017-7-10,2017,7,10,21,6me22d,Predictive Analytics: How to decrypt data with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/6me22d/predictive_analytics_how_to_decrypt_data_with/,dexlabanalytics,1499688811,,0,1
376,2017-7-10,2017,7,10,21,6me2ve,BigQuery and Cloud Machine Learning: advancing neural network predictions (Google Cloud Next '17),https://www.reddit.com/r/MachineLearning/comments/6me2ve/bigquery_and_cloud_machine_learning_advancing/,mitbal,1499689092,,0,1
377,2017-7-10,2017,7,10,21,6me48q,[R] Learning human behaviors from motion capture by adversarial imitation,https://www.reddit.com/r/MachineLearning/comments/6me48q/r_learning_human_behaviors_from_motion_capture_by/,pauljasek,1499689543,,3,35
378,2017-7-10,2017,7,10,22,6mefdg,[D] Anyone tried Hierarchical Temporal Memory (HTM) in practice?,https://www.reddit.com/r/MachineLearning/comments/6mefdg/d_anyone_tried_hierarchical_temporal_memory_htm/,makeDLgr8again,1499693207,"Some examples I've come across: http://www.cortical.io/demos.html
Some code I've come across: https://github.com/numenta/nupic

Any big labs (e.g. DeepMind, Google Brain, Facebook) using these?

Thanks!",6,12
379,2017-7-10,2017,7,10,22,6mefri,"[D] DeepMinds AI is teaching itself parkour, and the results are adorable",https://www.reddit.com/r/MachineLearning/comments/6mefri/d_deepminds_ai_is_teaching_itself_parkour_and_the/,fl4v1,1499693324,,33,48
380,2017-7-10,2017,7,10,22,6mehfa,[1707.02286] Emergence of Locomotion Behaviours in Rich Environments,https://www.reddit.com/r/MachineLearning/comments/6mehfa/170702286_emergence_of_locomotion_behaviours_in/,rephos,1499693820,,0,1
381,2017-7-10,2017,7,10,22,6mell2,[D] Free open source track for Machine Learning by members of this subreddit,https://www.reddit.com/r/MachineLearning/comments/6mell2/d_free_open_source_track_for_machine_learning_by/,madara33,1499695062,"Hi All I have been struggling to find a comprehensive track based on open sources like coursera/edx or other free MOOCs. I request the members of this subreddit to create a such an open source track just like the one in Dataquest or datacamp. The idea to get from A-Z. It can include MOOCs, Books, Tutorials, Problems, completitions or any medium that is open to all for free. We can do this for both R and Python.",1,0
382,2017-7-10,2017,7,10,23,6mem6i,Predicting congestion on London's roads with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/6mem6i/predicting_congestion_on_londons_roads_with/,gretayld,1499695237,,0,1
383,2017-7-10,2017,7,10,23,6meq6i,Anti corrosive bleach javel toilet cleaner bottle filling machine facto...,https://www.reddit.com/r/MachineLearning/comments/6meq6i/anti_corrosive_bleach_javel_toilet_cleaner_bottle/,hymachinery,1499696333,,0,1
384,2017-7-10,2017,7,10,23,6mewjn,Automatic Rolling Drum Bottle Washing Machine Vdeo,https://www.reddit.com/r/MachineLearning/comments/6mewjn/automatic_rolling_drum_bottle_washing_machine_vdeo/,hymachinery,1499698081,,0,1
385,2017-7-10,2017,7,10,23,6mexm7,[P] Interactive demo of a neural coreference resolution SOTA model + open-source code,https://www.reddit.com/r/MachineLearning/comments/6mexm7/p_interactive_demo_of_a_neural_coreference/,Thomjazz,1499698382,,1,10
386,2017-7-10,2017,7,10,23,6meynj,Best Practices for Persisting Model Metadata over Multiple Runs,https://www.reddit.com/r/MachineLearning/comments/6meynj/best_practices_for_persisting_model_metadata_over/,paffinity,1499698664,[removed],0,1
387,2017-7-10,2017,7,10,23,6meyqi,Automatic Cold Wet Glue Labeling Machine Manufacturer Price,https://www.reddit.com/r/MachineLearning/comments/6meyqi/automatic_cold_wet_glue_labeling_machine/,hymachinery,1499698690,,0,1
388,2017-7-11,2017,7,11,0,6mf0of,[D] Training a model for image segmentation using references of existing segmentations.,https://www.reddit.com/r/MachineLearning/comments/6mf0of/d_training_a_model_for_image_segmentation_using/,[deleted],1499699203,[deleted],0,1
389,2017-7-11,2017,7,11,0,6mf35b,[D] Training a model for image segmentation using existing tensors depicting optimal desired output,https://www.reddit.com/r/MachineLearning/comments/6mf35b/d_training_a_model_for_image_segmentation_using/,marrferr,1499699874,"I currently have a small data set of about 175,000 images that have been segmented manually using paths in photoshop. I'm currently extracting all the paths and converting them to tensors while looking into different strategies for training a model to predict optimal segmentation of future images.

Im still working on grasping the core concepts of ML but was looking for a little guidance. By no means do I want my hand held, just opinions on strategies. ",6,2
390,2017-7-11,2017,7,11,1,6mfji5,"Digital Asset Manipulation Detection - Combatting CycleGAN, Face2Face, and more.",https://www.reddit.com/r/MachineLearning/comments/6mfji5/digital_asset_manipulation_detection_combatting/,[deleted],1499704077,[removed],0,1
391,2017-7-11,2017,7,11,1,6mfjn1,Machine Learning Weekly Review #1,https://www.reddit.com/r/MachineLearning/comments/6mfjn1/machine_learning_weekly_review_1/,[deleted],1499704109,[deleted],0,1
392,2017-7-11,2017,7,11,2,6mfuiv,[N] Machine Learning Weekly Review #1,https://www.reddit.com/r/MachineLearning/comments/6mfuiv/n_machine_learning_weekly_review_1/,rldlml,1499706821,,0,0
393,2017-7-11,2017,7,11,2,6mfx19,[P] Deep learning for estimating race and ethnicity from electronic medical records (GitHub + arXiv),https://www.reddit.com/r/MachineLearning/comments/6mfx19/p_deep_learning_for_estimating_race_and_ethnicity/,[deleted],1499707417,[deleted],1,2
394,2017-7-11,2017,7,11,2,6mg1s8,[D] What is the math behind Ensembling being a *worse* predictor than one of its component models?,https://www.reddit.com/r/MachineLearning/comments/6mg1s8/d_what_is_the_math_behind_ensembling_being_a/,RafaJones,1499708571,"Hello, new to R/MachineLearning 

I'm taking some data science certificates through coursera and I really like machine learning so far. 

I found an interesting situation today. 

I used a GAM model to ensemble a GLM (better predictor at higher values) and RF (better predictor at lower values) model. It worked great on my test set, but performed slightly worse than GLM on the validation set. 

I've read the wikipedia and searched online - I found information about WHY ensembling works to make better predictions, but I can't figure out how it could be a worse predictor than its component models. 

Here's a specific reproducible example. But any answers about the theory itself would be really helpful too. 

Thank you, 

http://rpubs.com/rafajones/EnsembleModels",10,6
395,2017-7-11,2017,7,11,2,6mg3f2,[P] GPU Accelerated JavaScript,https://www.reddit.com/r/MachineLearning/comments/6mg3f2/p_gpu_accelerated_javascript/,hardmaru,1499708955,,7,16
396,2017-7-11,2017,7,11,3,6mg77i,[R] How to trick a convolutional neural network: doing so can help with interpretability,https://www.reddit.com/r/MachineLearning/comments/6mg77i/r_how_to_trick_a_convolutional_neural_network/,jonbruner,1499709880,,3,16
397,2017-7-11,2017,7,11,3,6mgdic,My Curated List of AI and Machine Learning Resources from Around the Web,https://www.reddit.com/r/MachineLearning/comments/6mgdic/my_curated_list_of_ai_and_machine_learning/,RobbieStats,1499711405,,0,2
398,2017-7-11,2017,7,11,3,6mgean,[D] How to predict a robots movement?,https://www.reddit.com/r/MachineLearning/comments/6mgean/d_how_to_predict_a_robots_movement/,el-perdido,1499711586,"Hey guys and gals!
I'm currently working on a project for a class (AI for Robotics) and my final project pretty much narrows down to predicting where a robot will be located in the future. I get data in (x,y) coordinates of a robot's location gathered from a video where the robot is inside a box. It just wonders around and once it hits an obstacle (e.g. wall), it changes direction. The training data comes from a video that lasts 60 seconds, at 30fps, and i'm supposed to output a prediction of the next 2 seconds (60 frames/points). Seems pretty simple but I was wondering if I could get some ideas on what algorithms or approaches would best suit this problem. Any input is highly appreciated.  
That being said, here is what I was thinking. At first, I thought about particle filters, but this filter requires constant measurement and is more suited to localize a target based on measurements, which I won't have, so I don't think that's a good approach at all.  
My second thought was to parametrize a PID controller and build a Q-Learner that learns the right parameters for the controller to fit the data.   ",2,5
399,2017-7-11,2017,7,11,3,6mgju7,"Audio + Video Manipulation Detection: Combatting CycleGAN, Face2Face, etc.",https://www.reddit.com/r/MachineLearning/comments/6mgju7/audio_video_manipulation_detection_combatting/,[deleted],1499712951,[deleted],0,1
400,2017-7-11,2017,7,11,5,6mh8rt,[Survey] Your experience of using interactive ML tools,https://www.reddit.com/r/MachineLearning/comments/6mh8rt/survey_your_experience_of_using_interactive_ml/,miziiiiii,1499718950,[removed],0,1
401,2017-7-11,2017,7,11,6,6mhgll,"[D] Audio + Video Manipulation Detection: Combatting CycleGAN, Face2Face, etc.",https://www.reddit.com/r/MachineLearning/comments/6mhgll/d_audio_video_manipulation_detection_combatting/,mhdempsey,1499720883,,0,11
402,2017-7-11,2017,7,11,6,6mhpc4,Predicting the Success of a Reddit Submission with Deep Learning and Keras,https://www.reddit.com/r/MachineLearning/comments/6mhpc4/predicting_the_success_of_a_reddit_submission/,[deleted],1499723131,[deleted],0,1
403,2017-7-11,2017,7,11,6,6mhphp,[P] Predicting the Success of a Reddit Submission with Deep Learning and Keras,https://www.reddit.com/r/MachineLearning/comments/6mhphp/p_predicting_the_success_of_a_reddit_submission/,fhoffa,1499723173,,21,124
404,2017-7-11,2017,7,11,7,6mi5nj,Practical Deep Learning with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/6mi5nj/practical_deep_learning_with_tensorflow/,gretayld,1499727554,,0,1
405,2017-7-11,2017,7,11,8,6mif94,[N] PAIR: the People + AI Research Initiative,https://www.reddit.com/r/MachineLearning/comments/6mif94/n_pair_the_people_ai_research_initiative/,cherls,1499730238,,0,9
406,2017-7-11,2017,7,11,9,6mimrm,Image Augmentation for Deep Learning using Keras and Histogram Equalization,https://www.reddit.com/r/MachineLearning/comments/6mimrm/image_augmentation_for_deep_learning_using_keras/,[deleted],1499732420,[deleted],0,1
407,2017-7-11,2017,7,11,9,6miss0,http://techscouter.blogspot.in/2017/07/sentiment-analysis.html,https://www.reddit.com/r/MachineLearning/comments/6miss0/httptechscouterblogspotin201707sentimentanalysisht/,techscouter,1499734240,,0,1
408,2017-7-11,2017,7,11,9,6miulu,Artificial Neural Networks vs. Biological Neural Networks Explained (Podcast by Programmers. Feedback Welcome and Appreciated!),https://www.reddit.com/r/MachineLearning/comments/6miulu/artificial_neural_networks_vs_biological_neural/,YourHost_Gabe_SFTM,1499734768,,0,1
409,2017-7-11,2017,7,11,10,6mivom,[D] The Google Brain Residency Program  One Year Later,https://www.reddit.com/r/MachineLearning/comments/6mivom/d_the_google_brain_residency_program__one_year/,[deleted],1499735108,[deleted],0,1
410,2017-7-11,2017,7,11,10,6mivui,[D] The Google Brain Residency Program - One Year Later,https://www.reddit.com/r/MachineLearning/comments/6mivui/d_the_google_brain_residency_program_one_year/,inarrears,1499735151,,3,59
411,2017-7-11,2017,7,11,10,6mj6h3,[R] [1707.02968] Revisiting Unreasonable Effectiveness of Data in Deep Learning Era,https://www.reddit.com/r/MachineLearning/comments/6mj6h3/r_170702968_revisiting_unreasonable_effectiveness/,darkconfidantislife,1499738354,,25,29
412,2017-7-11,2017,7,11,12,6mjjnx,Image Captioning using InceptionV3 and Beam Search,https://www.reddit.com/r/MachineLearning/comments/6mjjnx/image_captioning_using_inceptionv3_and_beam_search/,yashkatariya,1499742415,,0,1
413,2017-7-11,2017,7,11,13,6mjwin,"[P] Fine-tuning in Pytorch for AlexNet, VGGnet, ResNet",https://www.reddit.com/r/MachineLearning/comments/6mjwin/p_finetuning_in_pytorch_for_alexnet_vggnet_resnet/,meliketoy,1499746812,,0,3
414,2017-7-11,2017,7,11,13,6mjy0i,"[P]Chainer implementation of WGAN-GP, DFM, Cramer GAN, DRAGAN and BEGAN.",https://www.reddit.com/r/MachineLearning/comments/6mjy0i/pchainer_implementation_of_wgangp_dfm_cramer_gan/,underfitting,1499747338,,4,22
415,2017-7-11,2017,7,11,13,6mk2dp,6_Easy_Steps_To_Get_Started_Learning_Artificial_Intelligence - Favourite...,https://www.reddit.com/r/MachineLearning/comments/6mk2dp/6_easy_steps_to_get_started_learning_artificial/,favouriteblog,1499748900,,0,1
416,2017-7-11,2017,7,11,14,6mk6zj,[D] State Space to Linear Regression Model Question (A. Ng thesis),https://www.reddit.com/r/MachineLearning/comments/6mk6zj/d_state_space_to_linear_regression_model_question/,CircuitBeast,1499750615,"In Andrew Ng's thesis (section 5.1) hows can the Psa matrix be computed from the state space model? Does performing model id on data generated the state space model transform it into a probabilistic representation?

http://rll.berkeley.edu/deeprlcourse/docs/ng-thesis.pdf",0,4
417,2017-7-11,2017,7,11,15,6mkdke,[D] Word embeddings + object recognition for transfer learning?,https://www.reddit.com/r/MachineLearning/comments/6mkdke/d_word_embeddings_object_recognition_for_transfer/,deltasheep1,1499753279,"I'm thinking of a pipeline like this:

1. Get word embeddings from word2vec
2. Train an image classifier that, instead of backpropagating on cross-entropy class loss, backprops on reconstruction loss of the corresponding word vector for the class. 
3. To measure accuracy, look at the argmax of the dot product of each of the n classes with the word embedding that the net outputs
4. To predict new classes not in the image training set, do the same thing as 3., but choose however many classes from the word embedding set as you like

What papers apply ideas like this? I'd like to read them.

EDIT: would also like to hear general thoughts on the idea

EDIT 2: thanks to u/vamany, I found [""Zero-Shot Learning Through Cross-Modal Transfer""](https://nlp.stanford.edu/~socherr/SocherGanjooManningNg_NIPS2013.pdf), which basically does exactly what I was thinking",8,6
418,2017-7-11,2017,7,11,16,6mklfm,[P] Lets teach the Chrome TRex to play itself using SuperVised ML method,https://www.reddit.com/r/MachineLearning/comments/6mklfm/p_lets_teach_the_chrome_trex_to_play_itself_using/,asingh33,1499756470,,0,0
419,2017-7-11,2017,7,11,16,6mkomk,[R] A Simple but Tough-to-Beat Baseline for Sentence Embeddings,https://www.reddit.com/r/MachineLearning/comments/6mkomk/r_a_simple_but_toughtobeat_baseline_for_sentence/,danielcer,1499757819,,0,7
420,2017-7-11,2017,7,11,16,6mkqw5,How to use TensorFlow transfer learning to create an image classifications engine,https://www.reddit.com/r/MachineLearning/comments/6mkqw5/how_to_use_tensorflow_transfer_learning_to_create/,davidhung,1499758863,,0,1
421,2017-7-11,2017,7,11,17,6mkw19,A potential cellular memory mechanism for deep learning: an abacus of ions in G-quadruplex DNA forming nanowires in the nucleus,https://www.reddit.com/r/MachineLearning/comments/6mkw19/a_potential_cellular_memory_mechanism_for_deep/,[deleted],1499761286,[removed],0,1
422,2017-7-11,2017,7,11,18,6ml2hm,[D] Current survey of batch-norm and related techniques,https://www.reddit.com/r/MachineLearning/comments/6ml2hm/d_current_survey_of_batchnorm_and_related/,NicolasGuacamole,1499764325,I was wondering if anyone knows a fairly modern paper comparing the efficacy of batchnorm and other related techniques / tricks.,8,4
423,2017-7-11,2017,7,11,19,6ml97x,Sentiment Analysis with Core ML on iOS 11,https://www.reddit.com/r/MachineLearning/comments/6ml97x/sentiment_analysis_with_core_ml_on_ios_11/,martinmitrevski,1499767418,,0,1
424,2017-7-11,2017,7,11,19,6mlbyt,Appropriate number of kernels in convolutional neural networks?,https://www.reddit.com/r/MachineLearning/comments/6mlbyt/appropriate_number_of_kernels_in_convolutional/,decaf23,1499768593,[removed],0,1
425,2017-7-11,2017,7,11,20,6mlhcc,[R] [1706.09262] Hierarchical Attentive Recurrent Tracking,https://www.reddit.com/r/MachineLearning/comments/6mlhcc/r_170609262_hierarchical_attentive_recurrent/,akosiorek,1499770980,,0,11
426,2017-7-11,2017,7,11,20,6mli2o,A collection of Machine Learning Blogs,https://www.reddit.com/r/MachineLearning/comments/6mli2o/a_collection_of_machine_learning_blogs/,tasubo,1499771273,,0,1
427,2017-7-11,2017,7,11,20,6mloim,[Discussion] Is this r/machinelearning or r/deeplearning ?,https://www.reddit.com/r/MachineLearning/comments/6mloim/discussion_is_this_rmachinelearning_or/,peenta1410,1499773625,"I frequent this sub regularly, definitely some good info here and the level of discussion is good. But i am kinda disappointed on lack of topics on traditional machine learning algorithms. I wouldn't be surprised if more than 90% of the posts are related to deep learning.  ",133,145
428,2017-7-11,2017,7,11,21,6mlwwe,Automatic Single Head Ropp Aluminium Lid Capping Machine Manufacturer Price,https://www.reddit.com/r/MachineLearning/comments/6mlwwe/automatic_single_head_ropp_aluminium_lid_capping/,hymachinery,1499776454,,0,1
429,2017-7-11,2017,7,11,21,6mly9b,Can any neural networks learn to apply technical 3D modelling to inputs?,https://www.reddit.com/r/MachineLearning/comments/6mly9b/can_any_neural_networks_learn_to_apply_technical/,kwhali,1499776892,[removed],0,1
430,2017-7-11,2017,7,11,21,6mlz07,"""[P]"" Image Augmentation for Deep Learning using Keras and Histogram Equalization",https://www.reddit.com/r/MachineLearning/comments/6mlz07/p_image_augmentation_for_deep_learning_using/,dakkenet,1499777119,,5,26
431,2017-7-11,2017,7,11,21,6mm0a3,Objective function to evaluate the quality of segmentation,https://www.reddit.com/r/MachineLearning/comments/6mm0a3/objective_function_to_evaluate_the_quality_of/,dohako,1499777531,[removed],0,1
432,2017-7-11,2017,7,11,22,6mm49c,Hot Water System Supplier,https://www.reddit.com/r/MachineLearning/comments/6mm49c/hot_water_system_supplier/,gwenyosef,1499778781,,0,1
433,2017-7-11,2017,7,11,23,6mmhts,Heres a wooden machine I built a while ago. I wanted to create a sarcastic view at the topic.,https://www.reddit.com/r/MachineLearning/comments/6mmhts/heres_a_wooden_machine_i_built_a_while_ago_i/,[deleted],1499782755,[deleted],0,1
434,2017-7-12,2017,7,12,0,6mmqtk,Learning Boolean circuits?,https://www.reddit.com/r/MachineLearning/comments/6mmqtk/learning_boolean_circuits/,[deleted],1499785222,[removed],0,2
435,2017-7-12,2017,7,12,0,6mmsf8,Has anyone tested the new AMD Vega Frontier Edition GPU with Tensorflow?,https://www.reddit.com/r/MachineLearning/comments/6mmsf8/has_anyone_tested_the_new_amd_vega_frontier/,memes_everywhere,1499785646,[removed],0,1
436,2017-7-12,2017,7,12,0,6mmx1r,Analysis Study Group,https://www.reddit.com/r/MachineLearning/comments/6mmx1r/analysis_study_group/,abaybektursun,1499786788,[removed],0,1
437,2017-7-12,2017,7,12,0,6mn343,Automatic Anti Corrosive Piston Filling Machine For Acidic Toilet Bowl C...,https://www.reddit.com/r/MachineLearning/comments/6mn343/automatic_anti_corrosive_piston_filling_machine/,hymachinery,1499788292,,0,1
438,2017-7-12,2017,7,12,1,6mn8ic,[R] The Confluence of Geometry and Learning,https://www.reddit.com/r/MachineLearning/comments/6mn8ic/r_the_confluence_of_geometry_and_learning/,gdny,1499789608,,2,71
439,2017-7-12,2017,7,12,1,6mnb7j,What book or blog series would you recommend to an exec who wants to understand what their organisation should be doing with/about machine learning?,https://www.reddit.com/r/MachineLearning/comments/6mnb7j/what_book_or_blog_series_would_you_recommend_to/,drakensberge87,1499790277,[removed],0,1
440,2017-7-12,2017,7,12,1,6mnje7,[P] Gaussian Process regression using functional programming: A Python toolbox and an example notebook,https://www.reddit.com/r/MachineLearning/comments/6mnje7/p_gaussian_process_regression_using_functional/,LucaAmbrogioni,1499792262,,13,39
441,2017-7-12,2017,7,12,2,6mnn79,"[D] AI, Machine Learning and what is their future in gaming",https://www.reddit.com/r/MachineLearning/comments/6mnn79/d_ai_machine_learning_and_what_is_their_future_in/,Tsathoggua789,1499793184,,0,0
442,2017-7-12,2017,7,12,2,6mno74,why was my post deleted,https://www.reddit.com/r/MachineLearning/comments/6mno74/why_was_my_post_deleted/,[deleted],1499793430,[removed],0,1
443,2017-7-12,2017,7,12,2,6mnof1,Trying yo build a web-scale semantic image search. Anyone wants to hack together?,https://www.reddit.com/r/MachineLearning/comments/6mnof1/trying_yo_build_a_webscale_semantic_image_search/,deepmike,1499793480,,0,1
444,2017-7-12,2017,7,12,2,6mnp9d,[P]TensorForce: A TensorFlow library for applied reinforcement learning,https://www.reddit.com/r/MachineLearning/comments/6mnp9d/ptensorforce_a_tensorflow_library_for_applied/,mks40,1499793691,,8,73
445,2017-7-12,2017,7,12,2,6mnqwn,[D] A DNA-based cellular memory system for deep learning,https://www.reddit.com/r/MachineLearning/comments/6mnqwn/d_a_dnabased_cellular_memory_system_for_deep/,phobrain,1499794102,"In his Deep Learning in the Brain slides from Montreal Summer School [1], Blake Aaron Richards remarks, ""As it is, backprop through time requires time-stamped records of activity patterns and inputs  not an easy ask for a group of real neurons"".

The G quadruplex form of DNA forms a channel capable of holding an array of different cations (complexed to the carbonyl oxygens). I and a coauthor delve into it at length in [2]. I have proposed that long repeats of G's in junk DNA could form nanowires in the nucleus (porphyrins are suggestively shaped to be possible gates), and that sequences of different ions traveling through the core could additionally encode information. Up until now I didn't have an idea of what purpose that could serve, but this could be it.

A final hand-wave in our paper is that molecular mechanical methods didn't explain ion selectivity, and that plus the resemblance of the twisted oxygen-ion stack to organic superconductors led us to suggest a quantum effect.

[1] https://drive.google.com/file/d/0B2A1tnmq5zQdcFNkWU1vdDJiT00/view 

[2] http://pubs.acs.org/doi/abs/10.1021/ja00093a003",14,8
446,2017-7-12,2017,7,12,4,6moiqa,[Question] Feeding raw signal data into a Deep Belief Network in Python?,https://www.reddit.com/r/MachineLearning/comments/6moiqa/question_feeding_raw_signal_data_into_a_deep/,eragonngo,1499800903,[removed],0,1
447,2017-7-12,2017,7,12,4,6mopsp,"Was SqueezeNet trained using Unsupervised, Supervised ML?",https://www.reddit.com/r/MachineLearning/comments/6mopsp/was_squeezenet_trained_using_unsupervised/,kalub92,1499802632,[removed],3,1
448,2017-7-12,2017,7,12,5,6moub2,[Discussion] Compressive sensing with L0 minimization is NP-hard. Any shortcuts for undecimated wavelet transform?,https://www.reddit.com/r/MachineLearning/comments/6moub2/discussion_compressive_sensing_with_l0/,FrigoCoder,1499803738,"Sorry if this is not the appropriate subreddit for the topic.

L0 minimization is the holy grail of [compressive sensing](https://en.wikipedia.org/wiki/Compressed_sensing), or more specifically [sparse approximation](https://en.wikipedia.org/wiki/Sparse_approximation). Unfortunately it is an NP-hard combinatorial problem so exact solution is not possible. Common approximations include L1 minimization and greedy algorithms.

[Undecimated wavelet transform](https://en.wikipedia.org/wiki/Stationary_wavelet_transform) is basically just a discrete wavelet transform with the downsampling and upsampling steps removed, and the filters upsampled to account for this. It is translation invariant, it avoids ringing artifacts, however it is highly redundant, and inversion is icky. For one dimension, the transformation matrix contains n log2 n vectors, basically convolutions with log2 n basis functions.

One way to reduce the redundancy of the transform would be to enforce sparsity, keeping only at most k &lt;&lt;= n &lt; n log2 n nonzero coefficients that best reconstruct the signal. Obviously this is always possible, at worst we could use the trivial n-coefficient solution offered by the discrete wavelet transform.

I am curious whether we could get better runtime complexity for L0 minimization in this specific scenario, exploiting the special structure of the transformation matrix.

The only trick I could think so far is to work in Fourier domain with the log2 n convolutions, and use complex coefficients which represent both the amplitude and phase of the basis functions. So we would have to choose k &lt;&lt;= n complex coefficients corresponding to repeated log2 n basis functions. A greedy algorithm is possible in this case, but again, it is highly doubtful it gives optimal solution.
",16,13
449,2017-7-12,2017,7,12,5,6moy9g,What is this subreddit's views on this and other tricks against AI/ML,https://www.reddit.com/r/MachineLearning/comments/6moy9g/what_is_this_subreddits_views_on_this_and_other/,[deleted],1499804704,[deleted],0,1
450,2017-7-12,2017,7,12,5,6mp14q,Dear Fellow Machine Learning Folk: Youre Throwing Away All of Our Time,https://www.reddit.com/r/MachineLearning/comments/6mp14q/dear_fellow_machine_learning_folk_youre_throwing/,[deleted],1499805404,[deleted],0,1
451,2017-7-12,2017,7,12,5,6mp21c,Using Stacked Denoising Autoencoders to mine through claims data,https://www.reddit.com/r/MachineLearning/comments/6mp21c/using_stacked_denoising_autoencoders_to_mine/,sasaram,1499805634,,0,1
452,2017-7-12,2017,7,12,5,6mp3kv,[D] Dear Fellow Machine Learning Folk: Youre Throwing Away Our Time,https://www.reddit.com/r/MachineLearning/comments/6mp3kv/d_dear_fellow_machine_learning_folk_youre/,OccamsNuke,1499806013,,6,0
453,2017-7-12,2017,7,12,5,6mp4re,Possible machine learning approaches for this problem,https://www.reddit.com/r/MachineLearning/comments/6mp4re/possible_machine_learning_approaches_for_this/,deluded_soul,1499806316,[removed],0,1
454,2017-7-12,2017,7,12,6,6mp9ee,"A Decoupled, Generative, Unsupervised, Multimodal Architecture",https://www.reddit.com/r/MachineLearning/comments/6mp9ee/a_decoupled_generative_unsupervised_multimodal/,[deleted],1499807487,[deleted],0,1
455,2017-7-12,2017,7,12,6,6mpeav,Code that writes code,https://www.reddit.com/r/MachineLearning/comments/6mpeav/code_that_writes_code/,BorgesML,1499808739,[removed],0,1
456,2017-7-12,2017,7,12,6,6mpfz7,A simple example of a self-driving car using DeepGTAV,https://www.reddit.com/r/MachineLearning/comments/6mpfz7/a_simple_example_of_a_selfdriving_car_using/,cpgeier,1499809187,,0,1
457,2017-7-12,2017,7,12,6,6mpgsa,"A Decoupled, Generative, Unsupervised, Multimodal Architecture",https://www.reddit.com/r/MachineLearning/comments/6mpgsa/a_decoupled_generative_unsupervised_multimodal/,[deleted],1499809395,[deleted],0,1
458,2017-7-12,2017,7,12,7,6mpost,MIOpen: AMD's Machine Intelligence Library,https://www.reddit.com/r/MachineLearning/comments/6mpost/miopen_amds_machine_intelligence_library/,based2,1499811535,,1,1
459,2017-7-12,2017,7,12,8,6mq9q3,Top 10 Machine Learning Use Cases,https://www.reddit.com/r/MachineLearning/comments/6mq9q3/top_10_machine_learning_use_cases/,alexa_y,1499817559,,0,1
460,2017-7-12,2017,7,12,9,6mql1c,is 3 1080ti's worth it for ML?,https://www.reddit.com/r/MachineLearning/comments/6mql1c/is_3_1080tis_worth_it_for_ml/,pas43,1499820975,[removed],0,1
461,2017-7-12,2017,7,12,9,6mql4s,[R] [1707.03389] SCAN: Learning Abstract Hierarchical Compositional Visual Concepts,https://www.reddit.com/r/MachineLearning/comments/6mql4s/r_170703389_scan_learning_abstract_hierarchical/,enderwagon,1499821011,,3,27
462,2017-7-12,2017,7,12,10,6mqsvk,What is Computational Learning Theory?,https://www.reddit.com/r/MachineLearning/comments/6mqsvk/what_is_computational_learning_theory/,deepfeature,1499823452,"I was doing some research into different areas of Machine Learning and came upon Computational Learning Theory. What form of research goes on in this field? What are some examples of notable papers, ideas, concepts, etc? I tried looking this up on my own but wasn't really able to understand the research that was going on- my background is only in basic supervised and deep learning. Thanks!",1,1
463,2017-7-12,2017,7,12,11,6mr1lz,Is Machine Learning prevalent in teenagers/children right now?,https://www.reddit.com/r/MachineLearning/comments/6mr1lz/is_machine_learning_prevalent_in/,TrepidEd0601,1499826279,[removed],0,1
464,2017-7-12,2017,7,12,11,6mr1wa,"[N]: Google announces ""Gradient Ventures"", Google Venture's AI-focused startup incubator program",https://www.reddit.com/r/MachineLearning/comments/6mr1wa/n_google_announces_gradient_ventures_google/,gwern,1499826367,,31,153
465,2017-7-12,2017,7,12,11,6mr1xx,Predictions on Future of Stock Market with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/6mr1xx/predictions_on_future_of_stock_market_with/,synical24,1499826377,[removed],0,1
466,2017-7-12,2017,7,12,11,6mr78u,A new deep learning algorithm can diagnose 14 types of heart rhythm defects,https://www.reddit.com/r/MachineLearning/comments/6mr78u/a_new_deep_learning_algorithm_can_diagnose_14/,Chipdoc,1499828052,,0,1
467,2017-7-12,2017,7,12,13,6mrqoe,Sketch Recognition,https://www.reddit.com/r/MachineLearning/comments/6mrqoe/sketch_recognition/,_Austin_R,1499834695,[removed],0,1
468,2017-7-12,2017,7,12,14,6mrtc7,Deep Learning Cheat Sheet,https://www.reddit.com/r/MachineLearning/comments/6mrtc7/deep_learning_cheat_sheet/,divyanshjha,1499835690,,0,1
469,2017-7-12,2017,7,12,14,6ms0qv,[R] [1707.03377] Learning like humans with Deep Symbolic Networks,https://www.reddit.com/r/MachineLearning/comments/6ms0qv/r_170703377_learning_like_humans_with_deep/,[deleted],1499838530,[deleted],2,1
470,2017-7-12,2017,7,12,16,6msbs0,What is the nastiest non-programming ML bug you had to deal with ?,https://www.reddit.com/r/MachineLearning/comments/6msbs0/what_is_the_nastiest_nonprogramming_ml_bug_you/,curryage,1499843095,[removed],0,1
471,2017-7-12,2017,7,12,16,6msft3,Our factory cnc fiber laser machine is cutting metal,https://www.reddit.com/r/MachineLearning/comments/6msft3/our_factory_cnc_fiber_laser_machine_is_cutting/,JinanApexBellaLi,1499844855,,0,1
472,2017-7-12,2017,7,12,17,6msj9x,[P] Learning decoupled multimodal representations,https://www.reddit.com/r/MachineLearning/comments/6msj9x/p_learning_decoupled_multimodal_representations/,Jakobovski,1499846484,,0,1
473,2017-7-12,2017,7,12,17,6msjh4,Fiber laser machine cutting steel pipe,https://www.reddit.com/r/MachineLearning/comments/6msjh4/fiber_laser_machine_cutting_steel_pipe/,JinanApexBellaLi,1499846585,,0,1
474,2017-7-12,2017,7,12,17,6msk4p,[1706.00909] Learning by Association - A versatile semi-supervised training method for neural networks,https://www.reddit.com/r/MachineLearning/comments/6msk4p/170600909_learning_by_association_a_versatile/,taejun_kim,1499846861,,1,1
475,2017-7-12,2017,7,12,17,6mslhh,Mobileye Announces Expiration of HSR Waiting Period,https://www.reddit.com/r/MachineLearning/comments/6mslhh/mobileye_announces_expiration_of_hsr_waiting/,[deleted],1499847464,[deleted],0,1
476,2017-7-12,2017,7,12,17,6msoeb,Random Forest a Supervised classification machine learning algorithm,https://www.reddit.com/r/MachineLearning/comments/6msoeb/random_forest_a_supervised_classification_machine/,sudheeran,1499848886,,0,1
477,2017-7-12,2017,7,12,17,6msoid,[R] Synthesizing Obama: Learning Lip Sync from Audio,https://www.reddit.com/r/MachineLearning/comments/6msoid/r_synthesizing_obama_learning_lip_sync_from_audio/,iownaredball,1499848932,,3,22
478,2017-7-12,2017,7,12,17,6mspov,300w fiber laser cutting 1mm stainless steel,https://www.reddit.com/r/MachineLearning/comments/6mspov/300w_fiber_laser_cutting_1mm_stainless_steel/,JinanApexBellaLi,1499849492,,0,1
479,2017-7-12,2017,7,12,18,6msw19,Programatically understanding Adagrad,https://www.reddit.com/r/MachineLearning/comments/6msw19/programatically_understanding_adagrad/,nipun_batra,1499852390,,0,1
480,2017-7-12,2017,7,12,19,6msyz9,Europe Engine Flush Market Report 2017,https://www.reddit.com/r/MachineLearning/comments/6msyz9/europe_engine_flush_market_report_2017/,riyasharma24mr,1499853724,,0,1
481,2017-7-12,2017,7,12,19,6mt1n4,Fiber Laser Cutting Machine,https://www.reddit.com/r/MachineLearning/comments/6mt1n4/fiber_laser_cutting_machine/,Messer-123,1499854908,,0,1
482,2017-7-12,2017,7,12,19,6mt1sp,"[P] Would you be interested in a book ""Probabilistic data structures and algorithms in big data applications""?",https://www.reddit.com/r/MachineLearning/comments/6mt1sp/p_would_you_be_interested_in_a_book_probabilistic/,gakhov,1499854985,"Hello, I'm writing a book about various space-efficient data structures and non-deterministic algorithms that are useful in stream processing and big data applications. They are well known for researches, but many practitioners still afraid to use them or don't even consider to use. You might have heard about some of them already from different sources: Bloom filter, HyperLogLog, MinHash, etc. 

This book is NOT for scientists. It will be mainly intended for developers, but could be interesting for technology decision makers as well. 

I'm not a researcher in such topics, but an advanced used with a mathematical background. In my work I did not find any central source where I could find all such structures together with more than basic explanations. So, my goal is to merge such structures in to a few classes with a similar idea behind and explain them to the readers. The book is going to be language agnostic, because I want to give understanding how they work, but encourage users to use already maintained implementations in language they programming.

What do you think if such book could be interesting for the community as well?

If you want to learn more about the book and see the *early draft*, here is my Kickstarter campaign (for professional editing, corrections and proofreading): http://kck.st/2snB1tk

Also, I'm looking for good use cases for each described in the book problem. So, if you have one, don't hesitate to contact me :)

Thank you, Andrii",18,59
483,2017-7-12,2017,7,12,19,6mt1xh,Fiber laser cutting carbon steel,https://www.reddit.com/r/MachineLearning/comments/6mt1xh/fiber_laser_cutting_carbon_steel/,JinanApexBellaLi,1499855046,,0,1
484,2017-7-12,2017,7,12,19,6mt2jx,[N] Understanding empirical Bayes estimation (using baseball statistics),https://www.reddit.com/r/MachineLearning/comments/6mt2jx/n_understanding_empirical_bayes_estimation_using/,friscotime,1499855315,,0,1
485,2017-7-12,2017,7,12,19,6mt42b,[N] Goodreads Analysis of Book Titles with Boy and Girl,https://www.reddit.com/r/MachineLearning/comments/6mt42b/n_goodreads_analysis_of_book_titles_with_boy_and/,digitalson,1499856022,,0,1
486,2017-7-12,2017,7,12,20,6mt8uj,[N] Modeling Agents with Probabilistic Programs,https://www.reddit.com/r/MachineLearning/comments/6mt8uj/n_modeling_agents_with_probabilistic_programs/,molode,1499857916,,0,1
487,2017-7-12,2017,7,12,20,6mtate,(Job Opportunity) Deep Learning Researcher  Ground-breaking Computer Vision &amp; Deep Learning Firm  London,https://www.reddit.com/r/MachineLearning/comments/6mtate/job_opportunity_deep_learning_researcher/,WestbournePartnersAI,1499858681,[removed],0,1
488,2017-7-12,2017,7,12,20,6mtb37,VNect: Real-time 3D Human Pose Estimation with a Single RGBCamera,https://www.reddit.com/r/MachineLearning/comments/6mtb37/vnect_realtime_3d_human_pose_estimation_with_a/,teajunky,1499858792,,2,1
489,2017-7-12,2017,7,12,21,6mth3c,300w fiber laser cutting 1mm carbon steel,https://www.reddit.com/r/MachineLearning/comments/6mth3c/300w_fiber_laser_cutting_1mm_carbon_steel/,JinanApexBellaLi,1499861011,,0,1
490,2017-7-12,2017,7,12,21,6mtj9u,[N] What If The Data Science Skills Gap Is Just A Hiring Hot Mess?,https://www.reddit.com/r/MachineLearning/comments/6mtj9u/n_what_if_the_data_science_skills_gap_is_just_a/,jackblun,1499861733,,0,1
491,2017-7-12,2017,7,12,21,6mtmo8,[R] [1707.03300] The Intentional Unintentional Agent: Learning to Solve Many Continuous Control Tasks Simultaneously,https://www.reddit.com/r/MachineLearning/comments/6mtmo8/r_170703300_the_intentional_unintentional_agent/,pauljasek,1499862851,,2,9
492,2017-7-12,2017,7,12,22,6mtwch,[D]: Selecting cluster manager for distributed deep learning with different cluster managers.,https://www.reddit.com/r/MachineLearning/comments/6mtwch/d_selecting_cluster_manager_for_distributed_deep/,agrawalamey,1499865861,"Since distributed tensorflow (or other deep learning frameworks) for that matter don't have cluster manager or distributed file system, which is the best cluster manager to use to run distributed training? TensorflowOnSpark which runs yarn looks like a good option but google seems to be pushing for kubernetes. What are the trade-offs between using yarn, kubernetes, mesos and docker swarm especially in terms of GPU support?",2,3
493,2017-7-12,2017,7,12,22,6mtwic,[R] What's new in PyMC3 3.1,https://www.reddit.com/r/MachineLearning/comments/6mtwic/r_whats_new_in_pymc3_31/,lalypopa123,1499865906,,0,1
494,2017-7-12,2017,7,12,22,6mty7t,"Global CNC Machine Tools Sales Market Report revenue (Million USD), market share and growth rate in different regions forecast to 2022",https://www.reddit.com/r/MachineLearning/comments/6mty7t/global_cnc_machine_tools_sales_market_report/,deepshikhab,1499866419,,0,1
495,2017-7-12,2017,7,12,23,6mu7ry,[R] Text classifier algorithms: main approaches with tutorials,https://www.reddit.com/r/MachineLearning/comments/6mu7ry/r_text_classifier_algorithms_main_approaches_with/,bsic719,1499869153,,2,103
496,2017-7-13,2017,7,13,0,6muuj1,"Simple Questions Thread July 12, 2017",https://www.reddit.com/r/MachineLearning/comments/6muuj1/simple_questions_thread_july_12_2017/,AutoModerator,1499874941,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!
",0,1
497,2017-7-13,2017,7,13,1,6mv635,[D] Tutorials on predicting MULTIPLE future values of a time series?,https://www.reddit.com/r/MachineLearning/comments/6mv635/d_tutorials_on_predicting_multiple_future_values/,smthamazing,1499877815,"Sorry if this is not the right subreddit, but I got zero answers in [learnmachinelearning](https://www.reddit.com/r/learnmachinelearning/comments/6mlis0/tutorials_on_predicting_multiple_future_values_of/).

Hi! I've encountered an interesting problem yesterday. Basically, I need to predict some seasonal data (time series) for several days into the future. I need to do this with neural networks (this is part of the difficulty, I cannot use common methods like ARIMA here).

To my surprise, all the tutorials I found deal with prediction of just one future value. I feel confused: what is the use of knowing just one next value? Even weather forecasts usually give information for 1-2 weeks!

I haven't found any relevant tutorials, so I tried three approaches:

- Predict one value from previous i-N...i values, then include the predicted value as the ith input value, predict next one, and so on. I use LSTM here, and after a few iterations the prediction becomes very wrong.
- Train N different feedforward NNs to predict values i+1, i+2, ..., i+N based on the same input data. The plot of this abomination looks much more tolerable, but very imprecise. I suppose this is because my data is seasonal, and I only feed past values to my NNs, not seasonal features.
- Use a feedforward NN that predicts a value of particular day in particular month. Again, it looks tolerable, but imprecise, because I only feed it seasonal features, not values.

I don't really understand how to compose an input vector that would contain *both* values and seasonal features (like month and day) and what exactly it should contain.

So, are there any tutorials that actually deal with predicting more than one value?",7,7
498,2017-7-13,2017,7,13,1,6mv6k7,New paper explores how agents can learn from unintentional accomplishments (Deepmind),https://www.reddit.com/r/MachineLearning/comments/6mv6k7/new_paper_explores_how_agents_can_learn_from/,[deleted],1499877929,[deleted],0,1
499,2017-7-13,2017,7,13,2,6mval5,AI/Machine Learning Project for High School Science Research,https://www.reddit.com/r/MachineLearning/comments/6mval5/aimachine_learning_project_for_high_school/,VerySecretCactus,1499878903,[removed],0,1
500,2017-7-13,2017,7,13,2,6mvarj,[R] [1707.03141] 1-shot classification: 56.48% accuracy on 5-Way Mini-ImageNet!,https://www.reddit.com/r/MachineLearning/comments/6mvarj/r_170703141_1shot_classification_5648_accuracy_on/,cognitivedemons,1499878947,,13,29
501,2017-7-13,2017,7,13,2,6mvec6,Convolutional Neural Networks - The Math of Intelligence (Week 4),https://www.reddit.com/r/MachineLearning/comments/6mvec6/convolutional_neural_networks_the_math_of/,funmaster11,1499879827,,0,1
502,2017-7-13,2017,7,13,2,6mvmvx,Making CGI more realistic,https://www.reddit.com/r/MachineLearning/comments/6mvmvx/making_cgi_more_realistic/,MemeBox,1499881895,[removed],0,1
503,2017-7-13,2017,7,13,3,6mvp5o,DeepMind Learns Parkour,https://www.reddit.com/r/MachineLearning/comments/6mvp5o/deepmind_learns_parkour/,julian88888888,1499882420,,2,1
504,2017-7-13,2017,7,13,3,6mvpfk,[P] NoisyNet-A3C (w/ CartPole experiments),https://www.reddit.com/r/MachineLearning/comments/6mvpfk/p_noisyneta3c_w_cartpole_experiments/,Kaixhin,1499882480,,0,13
505,2017-7-13,2017,7,13,3,6mw0e3,[N] Microsoft creates an AI research lab to challenge Google and DeepMind,https://www.reddit.com/r/MachineLearning/comments/6mw0e3/n_microsoft_creates_an_ai_research_lab_to/,bbsome,1499885151,,46,260
506,2017-7-13,2017,7,13,3,6mw3s2,"[D] Caffe - Multiple loss layers, where is it implemented?",https://www.reddit.com/r/MachineLearning/comments/6mw3s2/d_caffe_multiple_loss_layers_where_is_it/,SosoTrainer,1499885977,"From the tutorial:


The final loss in Caffe, then, is computed by summing the total weighted loss over the network, as in the following pseudo-code:


loss := 0

for layer in layers:

  for top, loss_weight in layer.tops, layer.loss_weights:

    loss += loss_weight * sum(top)


http://caffe.berkeleyvision.org/tutorial/loss.html


Can anyone point out where this pseudo-code is implemented in Caffe?
I did search through the source code but found nothing.",3,1
507,2017-7-13,2017,7,13,5,6mwqot,[P] Building Your Own Neural Machine Translation System in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/6mwqot/p_building_your_own_neural_machine_translation/,alxndrkalinin,1499891649,,0,20
508,2017-7-13,2017,7,13,5,6mwrys,[Discussion] Ethical concerns of a soon-to-be PhD looking for a job,https://www.reddit.com/r/MachineLearning/comments/6mwrys/discussion_ethical_concerns_of_a_soontobe_phd/,oftenworried,1499891952,"Hi,
I am about to get a PhD in (applied) Machine Learning and I am considering what to do next.

It really seems that most of my peers are super excited to join the usual major corporations.
Hey, I totally understand: workplaces are fantastic, salaries are high, colleagues are (often) smart+interesting and you get to tackle stimulating problems every day. What's not to like?
However, the question I am asking myself is: ""what for?""

I don't really see myself working in a place if I don't deeply share the values of the company. 
In other words: I feel compelled to know that the company I am working for is actually doing something good for the world.
Don't get me wrong: I don't mean (necessarily) tackling global warming or inequality.
It would be enough to see a product which is actually good for the people, *long-term*. 

With this post I'd like to ask you two things:
1) Which are, according to you, the research bodies / NGO / startups / corporations doing some form of machine learning and whose output can be easily considered ethical (without the need of PR soul-washing efforts) ?
2) Do you guys ask yourself similar questions? Do you think the prestige of a company is enough to make it a desirable place to work in?

Any other comment is clearly very welcome. And please challenge my view if you feel like it.

Thanks for your time and attention.",118,67
509,2017-7-13,2017,7,13,8,6mxt2w,How to make a good classifier when the features are a bunch of numbers of unknown origin?,https://www.reddit.com/r/MachineLearning/comments/6mxt2w/how_to_make_a_good_classifier_when_the_features/,art3mia,1499902119,[removed],0,1
510,2017-7-13,2017,7,13,8,6mxtxx,Propensity NDCG isn't normalized?,https://www.reddit.com/r/MachineLearning/comments/6mxtxx/propensity_ndcg_isnt_normalized/,Refefer,1499902361,"Hey there,

I've been wracking my brain (and calling colleagues in) to understand how the normalization works for the propensity ranking metrics listed under **Metrics and Benchmark Results** at the [Extreme Multilabel Classification Repository](http://manikvarma.org/downloads/XC/XMLRepository.html)

With document propensities between [0,1], adding them to the denominator increases their values such that NDCG and MRR are no longer bound at 1 (they can go higher).  What am I missing?",0,0
511,2017-7-13,2017,7,13,8,6mxxl2,"[P] ""16K SUPERGAN hallucinations"" (16384x16384px GAN samples trained on Google Art corpus)",https://www.reddit.com/r/MachineLearning/comments/6mxxl2/p_16k_supergan_hallucinations_16384x16384px_gan/,gwern,1499903455,,5,14
512,2017-7-13,2017,7,13,9,6my0bj,[D] Propensity ranking metrics don't appear to be normalized,https://www.reddit.com/r/MachineLearning/comments/6my0bj/d_propensity_ranking_metrics_dont_appear_to_be/,Refefer,1499904340,"Hey there,

I've been wracking my brain (and calling colleagues in) to understand how the normalization works for the propensity ranking metrics listed under Metrics and Benchmark Results at the Extreme Multilabel Classification Repository

With document propensities between [0,1], adding them to the denominator increases their values such that NDCG and MRR are no longer bound at 1 (they can go higher). What am I missing?",0,1
513,2017-7-13,2017,7,13,9,6my90r,What's up with word embedding?,https://www.reddit.com/r/MachineLearning/comments/6my90r/whats_up_with_word_embedding/,metalearner,1499906980,,0,1
514,2017-7-13,2017,7,13,10,6myhli,[D] What is the state of the art in extracting handwritten text from scanned forms?,https://www.reddit.com/r/MachineLearning/comments/6myhli/d_what_is_the_state_of_the_art_in_extracting/,pantsforbirds,1499909717,"I'm working on a project where I'm going to be extracting text from forms and doing analyses on the text.

TL;DR looking for recent papers that address the problem.

--------

Currently I'm working on implementing a convoluted stack, followed by a recurrent stack with some extra analysis thrown in (for example if a character in a word could be an l, i, or 1, then we can make more accurate guesses by predicting which word was most likely targeted).

I am stealing some ideas from the [PAQ Compression algorithms](https://en.wikipedia.org/wiki/PAQ) and using an LSTM to make word predictions based on google's n-gram data set and using the probabilities from the n-gram based prediction + the probability from the image recognition to make a more accurate final prediction.

I've been trying to find research papers looking into the subject, but I haven't found much work that has been done recently. Anyone have any suggestions?",4,8
515,2017-7-13,2017,7,13,11,6myomr,[P] Image Recognition for Archery,https://www.reddit.com/r/MachineLearning/comments/6myomr/p_image_recognition_for_archery/,servingKire5,1499911941,"I'm a software engineer and I've taken up a new hobby of Archery. On the side I've been experimenting with some basic classifiers in scikit-learn.

A project I've gotten interested in is something to convert photos of archery targets post-shots into XY coordinate systems.

As a first step my goal is just to tell if an image is an archery target at all. Doing some research it seems like TF image recognition might be an approach to take. My concern is simply the volume of labeled images required to train a model with decent accuracy. I know this will probably vary but is this on the scale of hundreds, thousands, tens of thousands, more?",6,2
516,2017-7-13,2017,7,13,11,6mypb0,Reinforcement Learning with AI,https://www.reddit.com/r/MachineLearning/comments/6mypb0/reinforcement_learning_with_ai/,Pik000,1499912151,[removed],0,1
517,2017-7-13,2017,7,13,11,6mywcq,[P] An Image Captioning App for the Visually Impaired (Built using Tensorflow),https://www.reddit.com/r/MachineLearning/comments/6mywcq/p_an_image_captioning_app_for_the_visually/,[deleted],1499914426,[deleted],0,0
518,2017-7-13,2017,7,13,12,6mz3wd,"Global Computer Chair Sales Market Report by revenue (Million USD), market share and growth rate in different regions forecast to 2022",https://www.reddit.com/r/MachineLearning/comments/6mz3wd/global_computer_chair_sales_market_report_by/,deepshikhab,1499916933,,0,1
519,2017-7-13,2017,7,13,13,6mzbrm,"Global Conveyor Belt Vulcanizing Machine Sales Market Report by revenue (Million USD), market share and growth rate in different regions forecast to 2022",https://www.reddit.com/r/MachineLearning/comments/6mzbrm/global_conveyor_belt_vulcanizing_machine_sales/,deepshikhab,1499919629,,0,1
520,2017-7-13,2017,7,13,13,6mzbvl,How far are we from synthesizing a video announcement by a political leader?,https://www.reddit.com/r/MachineLearning/comments/6mzbvl/how_far_are_we_from_synthesizing_a_video/,[deleted],1499919671,[removed],0,1
521,2017-7-13,2017,7,13,13,6mzdu7,[D] How close are we to synthesizing a video announcement by a political leader?,https://www.reddit.com/r/MachineLearning/comments/6mzdu7/d_how_close_are_we_to_synthesizing_a_video/,phobrain,1499920338,[removed],2,1
522,2017-7-13,2017,7,13,14,6mzmvh,Best math classes for machine learning?,https://www.reddit.com/r/MachineLearning/comments/6mzmvh/best_math_classes_for_machine_learning/,tacjadie,1499923696,"Sorry if this is the wrong subreddit to post this in. I'm an undergrad in CS with a math minor wondering what math classes to take that are most useful for ML applications. I've taken up to Calc III and basic stats/prob (no linear algebra until after fall quarter). Should I take linear optimization, and then ordinary differential equations + nonlinear optimization? Or should I just take 3 classes that go deeper into stats and probability? Or should it be a mix of the two pathways?
Just wondering what would be the most useful to know going on to industry/grad school. 
Thanks!
",13,17
523,2017-7-13,2017,7,13,14,6mzojy,"Top 16 Machine Learning, Data Mining, and NLP Books",https://www.reddit.com/r/MachineLearning/comments/6mzojy/top_16_machine_learning_data_mining_and_nlp_books/,kjahan,1499924318,,0,1
524,2017-7-13,2017,7,13,14,6mzow6,Mate Labs mixes machine learning with IFTTT.,https://www.reddit.com/r/MachineLearning/comments/6mzow6/mate_labs_mixes_machine_learning_with_ifttt/,kailashahirwar12,1499924453,,0,1
525,2017-7-13,2017,7,13,14,6mzrjk,"[D] Want PhD, Have MSEE &amp; No Papers",https://www.reddit.com/r/MachineLearning/comments/6mzrjk/d_want_phd_have_msee_no_papers/,CircuitBeast,1499925565,I graduated with a course based master's in control theory with no papers published. I am considering pursuing a PhD in RL at a university like Berkley in a couple years. What do I need to get considered at a school like that? ,3,4
526,2017-7-13,2017,7,13,15,6mzw74,special tensioning machine,https://www.reddit.com/r/MachineLearning/comments/6mzw74/special_tensioning_machine/,ada2017,1499927491,,0,1
527,2017-7-13,2017,7,13,15,6mzwsp,i am still learning-michelangelo,https://www.reddit.com/r/MachineLearning/comments/6mzwsp/i_am_still_learningmichelangelo/,mahmud0852,1499927762,,0,1
528,2017-7-13,2017,7,13,15,6mzxvs,[R] NO Need to Worry about Adversarial Examples in Object Detection in Autonomous Vehicles,https://www.reddit.com/r/MachineLearning/comments/6mzxvs/r_no_need_to_worry_about_adversarial_examples_in/,downtownslim,1499928236,,14,14
529,2017-7-13,2017,7,13,16,6n02un,Feature engineering for encrypted data,https://www.reddit.com/r/MachineLearning/comments/6n02un/feature_engineering_for_encrypted_data/,bogdansky,1499930296,[removed],1,1
530,2017-7-13,2017,7,13,16,6n0399,Finding the optimal size for computation ?,https://www.reddit.com/r/MachineLearning/comments/6n0399/finding_the_optimal_size_for_computation/,Caillasse,1499930477,[removed],1,1
531,2017-7-13,2017,7,13,16,6n06lm,Adversarial Dropout for Supervised and Semi-supervised Learning,https://www.reddit.com/r/MachineLearning/comments/6n06lm/adversarial_dropout_for_supervised_and/,[deleted],1499932000,[deleted],0,1
532,2017-7-13,2017,7,13,16,6n071n,[R] Submanifold Sparse Convolutional Networks,https://www.reddit.com/r/MachineLearning/comments/6n071n/r_submanifold_sparse_convolutional_networks/,xternalz,1499932213,,0,7
533,2017-7-13,2017,7,13,16,6n07v1,[R] Adversarial Dropout for Supervised and Semi-supervised Learning,https://www.reddit.com/r/MachineLearning/comments/6n07v1/r_adversarial_dropout_for_supervised_and/,srPark,1499932609,,7,3
534,2017-7-13,2017,7,13,17,6n0det,MachineLabs - Our road ahead to private beta,https://www.reddit.com/r/MachineLearning/comments/6n0det/machinelabs_our_road_ahead_to_private_beta/,PascalPrecht,1499935245,,0,1
535,2017-7-13,2017,7,13,18,6n0jk8,Announcing a new subreddit: r/InverseProblems,https://www.reddit.com/r/MachineLearning/comments/6n0jk8/announcing_a_new_subreddit_rinverseproblems/,[deleted],1499938104,[deleted],0,1
536,2017-7-13,2017,7,13,18,6n0lz7,[N] Announcing a new subreddit: r/InverseProblems,https://www.reddit.com/r/MachineLearning/comments/6n0lz7/n_announcing_a_new_subreddit_rinverseproblems/,adler-j,1499939211,,8,43
537,2017-7-13,2017,7,13,19,6n0w0j,[D] Can a CTC based speech recognition architecture be used on mobile devices ?,https://www.reddit.com/r/MachineLearning/comments/6n0w0j/d_can_a_ctc_based_speech_recognition_architecture/,saurabhvyas3,1499943483,"I am very passionate about speech recognition , I have been reading some papers like CTC ( original 2006 paper by Alex ) and ones which depend mainly on CTC cost function like Baidu's Deepspeech 1 and 2 , I have been wondering if it's possible to create a neural network architecture that uses CTC loss , that can  run on smart phones , on a very small vocabulary ( say 100 words or even less ) ? ",6,9
538,2017-7-13,2017,7,13,21,6n18zp,Examples of authors incorrectly applying machine learning to datasets,https://www.reddit.com/r/MachineLearning/comments/6n18zp/examples_of_authors_incorrectly_applying_machine/,[deleted],1499948395,[removed],0,1
539,2017-7-13,2017,7,13,22,6n1pvn,A Contemporary Overview of Probabilistic Latent Variable Models,https://www.reddit.com/r/MachineLearning/comments/6n1pvn/a_contemporary_overview_of_probabilistic_latent/,rFar77,1499953838,,0,1
540,2017-7-13,2017,7,13,22,6n1r6y,Regression for function of geolocation,https://www.reddit.com/r/MachineLearning/comments/6n1r6y/regression_for_function_of_geolocation/,niujin,1499954234,[removed],0,1
541,2017-7-13,2017,7,13,23,6n1syd,GoogLeNet channels by layer,https://www.reddit.com/r/MachineLearning/comments/6n1syd/googlenet_channels_by_layer/,712hee,1499954721,[removed],0,1
542,2017-7-13,2017,7,13,23,6n1uz1,VFFS Vertical Form Fill Seal automatic packaging machine video,https://www.reddit.com/r/MachineLearning/comments/6n1uz1/vffs_vertical_form_fill_seal_automatic_packaging/,hymachinery,1499955256,,0,1
543,2017-7-13,2017,7,13,23,6n1vls,[R] [1707.03815] Deep Gaussian Embedding of Attributed Graphs: Unsupervised Inductive Learning via Ranking,https://www.reddit.com/r/MachineLearning/comments/6n1vls/r_170703815_deep_gaussian_embedding_of_attributed/,abojchevski,1499955433,,12,6
544,2017-7-13,2017,7,13,23,6n1zwe,Details of implementing a greedy algorithm for CNN,https://www.reddit.com/r/MachineLearning/comments/6n1zwe/details_of_implementing_a_greedy_algorithm_for_cnn/,[deleted],1499956643,[removed],0,1
545,2017-7-14,2017,7,14,0,6n29il,Video Tutorial on Sentiment Analysis with LSTMs,https://www.reddit.com/r/MachineLearning/comments/6n29il/video_tutorial_on_sentiment_analysis_with_lstms/,adeshpande3,1499959212,,0,1
546,2017-7-14,2017,7,14,1,6n2q2s,What would a convolutional neural network look like without using any machine learning libraries? (sample code),https://www.reddit.com/r/MachineLearning/comments/6n2q2s/what_would_a_convolutional_neural_network_look/,theology_,1499963329,[removed],0,1
547,2017-7-14,2017,7,14,2,6n2yne,[P] Machine Learning at Berkeley's Introductory ML Tutorial Series: The Bias-Variance Dilemma,https://www.reddit.com/r/MachineLearning/comments/6n2yne/p_machine_learning_at_berkeleys_introductory_ml/,mlberkeley,1499965471,,19,206
548,2017-7-14,2017,7,14,2,6n30vr,"[N] ""The algorithm kingdom: China may match or beat America in AI - its deep pool of data may let it lead in artificial intelligence""",https://www.reddit.com/r/MachineLearning/comments/6n30vr/n_the_algorithm_kingdom_china_may_match_or_beat/,gwern,1499966009,,41,31
549,2017-7-14,2017,7,14,2,6n35ty,IFTTT on steroids with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/6n35ty/ifttt_on_steroids_with_machine_learning/,curious_rv,1499967190,,0,1
550,2017-7-14,2017,7,14,3,6n3is0,Using Deep Learning to Create Professional-Level Photographs,https://www.reddit.com/r/MachineLearning/comments/6n3is0/using_deep_learning_to_create_professionallevel/,[deleted],1499970403,[deleted],0,1
551,2017-7-14,2017,7,14,3,6n3lpd,[D] Are political applications of AI off-limits?,https://www.reddit.com/r/MachineLearning/comments/6n3lpd/d_are_political_applications_of_ai_offlimits/,phobrain,1499971144,"I submitted a post describing a political use of machine learning, and it still exists in my history, but was deleted from the feed. I don't want to get the list in any danger of govt censorship. The post doesn't have down votes.",8,0
552,2017-7-14,2017,7,14,3,6n3lva,Understanding GANs,https://www.reddit.com/r/MachineLearning/comments/6n3lva/understanding_gans/,totallynotAGI,1499971182,"I'm trying to understand the motivation behind design choices of various GAN parts and GAN as a whole.

One perspective for motivation behind GANs is ""Why are we using a fixed and simple cost function instead of using a neural network in it's place? Lets *learn* the cost function!""

---

Usually, if your cost function is fixed, you just train your model normally. With GANs, the training regime is not much more [complicated](http://i.imgur.com/m83dmYy.png). You first train your model normally (the generator) and then the critic, with opposite goals (left and middle picture, reddish color signifies what parameters are updated). But since the critic could then assign a high score to everything, you make sure it also assigns a low score to real images (right picture). So the middle and right image change the same parameters and can be updated with just one cost function (this is meta now: we just described a cost function for *the* cost function). If you followed all this, viola, you actually ended up with the alternating training regime for GANs!


And this actually works, with the assumption that your critic ""doesn't change too much"" w.r.t. parameters (Lipschitz continuity). With the right model that behaves in such a way, you could've ""stumbled upon"" this thing all by yourself!

---

**My question** here is twofold:

* What sort of motivation is behind adding the nonlinearity in the output of the critic, namely the ""1 - log"" and ""log"" in the original GAN minimax formulation? It seems *more* complex and it seems like you needed a super special model anyway (DCGAN) to make it work? I sense the answer has to do something with the string ""maximum likelihood"". But isn't that just a sort of a trick to get the outputs to the range we want them, a trick which would then require a bunch of other tricks to make the GAN work properly (logD trick, adding noise etc.) and, compared to WGAN, lose a sort of elegance? I still see papers published that maximize the JS divergence with GANs. I don't see why and I'm not sure what I'm missing.

* So defining distances between two points is easy: x - y, but defining a distance between two distributions is a completely different story. There's a bunch of [statistical distances](https://en.wikipedia.org/wiki/Statistical_distance) and integrals and logs and whatnot and there's a thing known as [Kantorovich-Rubenstein duality](https://en.wikipedia.org/wiki/Wasserstein_metric#Dual_representation_of_W1) which is a theorem from a [1000 page book on Optimal transport](http://cedricvillani.org/wp-content/uploads/2012/08/preprint-1.pdf) which I'm not even going to try to begin to understand. 
But if you apply the duality for EMD and then check out what's the thing you minimize at the output of the critic, it's still just: x - y? I guess I just wanted to double check this and understand why not many people don't seem amazed with this haha? **Because you kind of end up with that if you just ""naively"" set up your networks as described in the beginning?**

This turned a bit less coherent towards the end, but I hope I managed to get the point across :)
",0,4
553,2017-7-14,2017,7,14,4,6n3rpl,[R]Using Deep Learning to Create Professional-Level Photographs,https://www.reddit.com/r/MachineLearning/comments/6n3rpl/rusing_deep_learning_to_create_professionallevel/,[deleted],1499972681,[deleted],1,7
554,2017-7-14,2017,7,14,4,6n3xdt,Recent Evolution of QA Datasets and Going Forward,https://www.reddit.com/r/MachineLearning/comments/6n3xdt/recent_evolution_of_qa_datasets_and_going_forward/,jppkpark,1499974191,[removed],0,2
555,2017-7-14,2017,7,14,4,6n40n5,I just got an interview for a ML position at a startup and I don't know a lot of ML. Does anyone have tips to ace the interview?,https://www.reddit.com/r/MachineLearning/comments/6n40n5/i_just_got_an_interview_for_a_ml_position_at_a/,BAOUBA,1499975063,[removed],0,1
556,2017-7-14,2017,7,14,4,6n42dw,Any one have Machine learning flash cards with formulas?,https://www.reddit.com/r/MachineLearning/comments/6n42dw/any_one_have_machine_learning_flash_cards_with/,mrcet007,1499975515,,1,1
557,2017-7-14,2017,7,14,4,6n43v4,[Q] A way to identify date patterns using Decision Trees?,https://www.reddit.com/r/MachineLearning/comments/6n43v4/q_a_way_to_identify_date_patterns_using_decision/,Fender6969,1499975907,[removed],0,1
558,2017-7-14,2017,7,14,5,6n44ee,Logistic regression not generalizing,https://www.reddit.com/r/MachineLearning/comments/6n44ee/logistic_regression_not_generalizing/,rodrigo-silveira,1499976052,[removed],0,1
559,2017-7-14,2017,7,14,5,6n44gy,Beginner at Graph Convolutional Networks,https://www.reddit.com/r/MachineLearning/comments/6n44gy/beginner_at_graph_convolutional_networks/,[deleted],1499976066,[removed],0,1
560,2017-7-14,2017,7,14,5,6n4dbq,Applying Machine Learning to state analysis,https://www.reddit.com/r/MachineLearning/comments/6n4dbq/applying_machine_learning_to_state_analysis/,IosMxe,1499978313,[removed],0,1
561,2017-7-14,2017,7,14,5,6n4gny,Introduction to Deep Learning With keras,https://www.reddit.com/r/MachineLearning/comments/6n4gny/introduction_to_deep_learning_with_keras/,pythomad,1499979181,,0,1
562,2017-7-14,2017,7,14,7,6n5469,"[D] Textbook of information theory for machine learning, Cover vs. MacKay ?",https://www.reddit.com/r/MachineLearning/comments/6n5469/d_textbook_of_information_theory_for_machine/,xingdongrobotics,1499985521,"Firstly, for doing research in areas e.g. RNN, Bayesian deep learning, GANs, is it really recommended to read a textbook about information theory ? (Because it seems many papers only use very basics, e.g. definition of KL divergence, mutual information, Fisher information matrix etc.)

If so, secondly, there are many recommendations of both Cover&amp;Thomas classic textbook, and also MacKay's. Would like to hear some comments about their pros and cons for machine learning people, in particular. ",12,18
563,2017-7-14,2017,7,14,8,6n5c0b,[D] What are the state-of-the-art results in Probabilistic Graphical Models?,https://www.reddit.com/r/MachineLearning/comments/6n5c0b/d_what_are_the_stateoftheart_results_in/,AntixK,1499987866,"I am interested in the SOTA models and the current research direction in PGMs. To my knowledge, VAEs are known to work well. I could surely use some more input from you guys.",1,25
564,2017-7-14,2017,7,14,9,6n5nzy,Home Renovation Plumbing Services,https://www.reddit.com/r/MachineLearning/comments/6n5nzy/home_renovation_plumbing_services/,gwenyosef,1499991469,,0,1
565,2017-7-14,2017,7,14,9,6n5pdz,Identifying Spatial Relations in Images using Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/6n5pdz/identifying_spatial_relations_in_images_using/,[deleted],1499991903,[deleted],0,1
566,2017-7-14,2017,7,14,9,6n5qqs,How is KKT Complementary Slackness embedded in the training of SVM?,https://www.reddit.com/r/MachineLearning/comments/6n5qqs/how_is_kkt_complementary_slackness_embedded_in/,zzzaaaef,1499992322,[removed],0,1
567,2017-7-14,2017,7,14,9,6n5rja,SOTA for network compression/ distillation?,https://www.reddit.com/r/MachineLearning/comments/6n5rja/sota_for_network_compression_distillation/,deephive,1499992565,[removed],0,1
568,2017-7-14,2017,7,14,9,6n5s3z,[R][1706.04215] Identifying Spatial Relations in Images using Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/6n5s3z/r170604215_identifying_spatial_relations_in/,codehacked,1499992769,,0,3
569,2017-7-14,2017,7,14,9,6n5uip,[R] Be Careful What You Backpropagate: A Case For Linear Output Activations &amp; Gradient Boosting,https://www.reddit.com/r/MachineLearning/comments/6n5uip/r_be_careful_what_you_backpropagate_a_case_for/,xternalz,1499993562,,19,15
570,2017-7-14,2017,7,14,10,6n5xsi,Amazon: Jobs for Machine Learning engineers in Seattle,https://www.reddit.com/r/MachineLearning/comments/6n5xsi/amazon_jobs_for_machine_learning_engineers_in/,julito_power,1499994653,[removed],0,1
571,2017-7-14,2017,7,14,10,6n63ah,[R] [1707.03938] Representation Learning for Grounded Spatial Reasoning,https://www.reddit.com/r/MachineLearning/comments/6n63ah/r_170703938_representation_learning_for_grounded/,aeuc,1499996406,,0,6
572,2017-7-14,2017,7,14,10,6n63ln,[1703.04070] Prediction and Control with Temporal Segment Models,https://www.reddit.com/r/MachineLearning/comments/6n63ln/170304070_prediction_and_control_with_temporal/,[deleted],1499996508,[deleted],0,1
573,2017-7-14,2017,7,14,10,6n63qw,[R] [1703.04070] Prediction and Control with Temporal Segment Models,https://www.reddit.com/r/MachineLearning/comments/6n63qw/r_170304070_prediction_and_control_with_temporal/,aeuc,1499996556,,1,14
574,2017-7-14,2017,7,14,10,6n645c,"[P] Live Expression Transfer, Help?",https://www.reddit.com/r/MachineLearning/comments/6n645c/p_live_expression_transfer_help/,that_one_ai_nerd,1499996693,"Hi,

So I am working on a project where I want to do live expression transfer from video similiar to this:
https://www.youtube.com/watch?v=eXVspNUeiWw

I was hoping someone can give me some pointers and any help you guys can offer at, it'd be much appreciated. Here is the way I am approaching the problem thus far, which is something I have decided from my experience and some research papers I have read on this problem:

1. Do proper face feature tracking and identification
2. Match the features on each face to each other from two sources
3. Replace and blend the source video's expression to the target video's expression in order to produce a third composite video

Some more notes:
I am going to first get it to work for still images, as I figure once I get that working it will not be too hard to implement it back into video, what do you guys think? Also, if anyone can offer some specifics or help on any of this that'd be amazing. I will be updating this post with my progress, more resources, and hopefully eventually make it into a tutorial type post for everyone, so let's collaborate and then this can be an awesome tutorial for people to learn from in the future. Thank you, everyone!

EDIT 1: I am thinking of trying to using a generative adversarial network for this, thoughts?",2,2
575,2017-7-14,2017,7,14,11,6n6gqv,How do you draw a neural network diagram from looking at your input data?,https://www.reddit.com/r/MachineLearning/comments/6n6gqv/how_do_you_draw_a_neural_network_diagram_from/,ejnunn,1500000839,[removed],0,1
576,2017-7-14,2017,7,14,12,6n6k4t,Computer Vision: Do Androids Dream of Electric Hawaii?,https://www.reddit.com/r/MachineLearning/comments/6n6k4t/computer_vision_do_androids_dream_of_electric/,Sukk-up,1500001965,,0,1
577,2017-7-14,2017,7,14,12,6n6lhl,Machine Learning ecosystem in Canada,https://www.reddit.com/r/MachineLearning/comments/6n6lhl/machine_learning_ecosystem_in_canada/,iamquah,1500002453,[removed],0,1
578,2017-7-14,2017,7,14,12,6n6n6g,Face2Face: Real-time Face Capture and Reenactment of RGB Videos,https://www.reddit.com/r/MachineLearning/comments/6n6n6g/face2face_realtime_face_capture_and_reenactment/,deepconvnet,1500003066,,0,1
579,2017-7-14,2017,7,14,12,6n6naw,[P] Online Category-learning and Classification,https://www.reddit.com/r/MachineLearning/comments/6n6naw/p_online_categorylearning_and_classification/,[deleted],1500003099,[deleted],0,10
580,2017-7-14,2017,7,14,12,6n6rfc,Emergence of Locomotion Behaviours in Rich Environments - Now with sound!,https://www.reddit.com/r/MachineLearning/comments/6n6rfc/emergence_of_locomotion_behaviours_in_rich/,zorfbee,1500004583,,0,1
581,2017-7-14,2017,7,14,13,6n6uw7,Mimic Snapchat Filters,https://www.reddit.com/r/MachineLearning/comments/6n6uw7/mimic_snapchat_filters/,[deleted],1500005778,[deleted],0,1
582,2017-7-14,2017,7,14,13,6n6y4d,Why doesn't my RandomForest model predict class values? Possible?,https://www.reddit.com/r/MachineLearning/comments/6n6y4d/why_doesnt_my_randomforest_model_predict_class/,crucial_bubbler,1500006959,[removed],0,1
583,2017-7-14,2017,7,14,14,6n74hj,Automatic shrink sleeve labeling machine manufacturer price,https://www.reddit.com/r/MachineLearning/comments/6n74hj/automatic_shrink_sleeve_labeling_machine/,hymachinery,1500010610,,0,1
584,2017-7-14,2017,7,14,15,6n7bae,[D] Google Brain Residency Question,https://www.reddit.com/r/MachineLearning/comments/6n7bae/d_google_brain_residency_question/,CircuitBeast,1500013531,[removed],2,0
585,2017-7-14,2017,7,14,15,6n7bc8,Questions on Large Scale machine learning frameworks,https://www.reddit.com/r/MachineLearning/comments/6n7bc8/questions_on_large_scale_machine_learning/,sanketpatil,1500013554,[removed],0,1
586,2017-7-14,2017,7,14,16,6n7l7b,Using Deep Learning to Create Professional-Level Photographs,https://www.reddit.com/r/MachineLearning/comments/6n7l7b/using_deep_learning_to_create_professionallevel/,mitbal,1500017895,,1,2
587,2017-7-14,2017,7,14,16,6n7lz8,deep learning inference performance on different hardware,https://www.reddit.com/r/MachineLearning/comments/6n7lz8/deep_learning_inference_performance_on_different/,c94jk,1500018276,[removed],0,1
588,2017-7-14,2017,7,14,16,6n7mi1,[D] What is the best way to generate accurate and high quality ground truth depth maps for real scenes? (e.g. a room),https://www.reddit.com/r/MachineLearning/comments/6n7mi1/d_what_is_the_best_way_to_generate_accurate_and/,theology_,1500018536,"I am trying to create a training set.
Thank you in advance.

Andrew Ng used a 3d scanner and got really, really low quality depth maps. If money was not a problem, what would be the best way.",7,11
589,2017-7-14,2017,7,14,17,6n7o8s,[R] [1707.04175] Distral: Robust Multitask Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/6n7o8s/r_170704175_distral_robust_multitask/,evc123,1500019345,,0,26
590,2017-7-14,2017,7,14,17,6n7sce,[D]Implementing a Fuzzy Restricted Boltzmann Machine,https://www.reddit.com/r/MachineLearning/comments/6n7sce/dimplementing_a_fuzzy_restricted_boltzmann_machine/,idiot247,1500021252,"Hello, I suspect this isn't the right subreddit for this kind of thing, bu MLQuestions is really quiet.  
I'm trying to implement a FRBM based on these papers:
[Transactions on Fuzzy Systems 1 A Fuzzy Restricted Boltzmann Machine: Novel Learning Algorithms Based on Crisp Possibilistic Mean Value of Fuzzy Numbers](https://www.researchgate.net/publication/311951562_Transactions_on_Fuzzy_Systems_1_A_Fuzzy_Restricted_Boltzmann_Machine_Novel_Learning_Algorithms_Based_on_Crisp_Possibilistic_Mean_Value_of_Fuzzy_Numbers), pages 5 and 7,  
and  
[Fuzzy Restricted Boltzmann Machine for the Enhancement of Deep Learning](http://www.doc88.com/p-0072893205204.html), page 6.  
I am looking for a recommendation for a good starting implementation of an RBM that can be modified in order to acomplish this.  
Is there a framework with an implementation that can be adapted, or any (easy to read) code in Python or MatLab.  
Thanks!",14,8
591,2017-7-14,2017,7,14,17,6n7tew,[R] Kafnets: kernel-based non-parametric activation functions for neural networks,https://www.reddit.com/r/MachineLearning/comments/6n7tew/r_kafnets_kernelbased_nonparametric_activation/,scardax88,1500021833,,8,6
592,2017-7-14,2017,7,14,17,6n7umh,[1707.04131] Foolbox v0.8.0: A Python toolbox to benchmark the robustness of machine learning models,https://www.reddit.com/r/MachineLearning/comments/6n7umh/170704131_foolbox_v080_a_python_toolbox_to/,WielandBr,1500022455,,1,2
593,2017-7-14,2017,7,14,18,6n7zgs,ANCHORS,https://www.reddit.com/r/MachineLearning/comments/6n7zgs/anchors/,ada2017,1500024708,,0,1
594,2017-7-14,2017,7,14,20,6n8kf0,[P] Arnold - CNN with memory,https://www.reddit.com/r/MachineLearning/comments/6n8kf0/p_arnold_cnn_with_memory/,PetarKing,1500033558,"Hello everyone!  
I'm new to ML so I would like to get your feedback on this idea of mine, so as to know whether the *good* results that I'm getting (compared to the normal CNN) are biased.
  
I wanted to make a **CNN** which will, after one of its convolutions, stop and search the **memory** of all **important** past **values of that layer** and, if some of these is **similar** to the current one, simply output **its label/value** without going through the rest of the network.

A link to the Kaggle Kernel in which I tried to implement this idea:  
**https://www.kaggle.com/petarking/c-k-nn-arnold**  
  
**important**- its loss is smaller than the mean of previous losses multiplied by **trust value/percentage**  
  
**similar**- (mean of one matrix+mean of the other matrix)/ mean squared error * **trust constant**  
  
**P.S.** - If someone is interested, I can comment the code and clean it up.",12,10
595,2017-7-14,2017,7,14,21,6n8rzn,Configuration setting for Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/6n8rzn/configuration_setting_for_deep_reinforcement/,mphuget,1500036134,[removed],0,1
596,2017-7-14,2017,7,14,21,6n8ssl,[P] State-of-the-art coreference resolution library using neural nets and spaCy [interactive],https://www.reddit.com/r/MachineLearning/comments/6n8ssl/p_stateoftheart_coreference_resolution_library/,pmigdal,1500036402,,1,25
597,2017-7-14,2017,7,14,22,6n8w06,[D] Machine Learning for Image transformation,https://www.reddit.com/r/MachineLearning/comments/6n8w06/d_machine_learning_for_image_transformation/,the_worst_of_me,1500037462,"Mods, kindly mark this post NSFW. This is a genuine question from a person looking at the future with high hopes. 

I remember seeing a number of research pieces on this sub about learning image transformation, for example transformation of paintings from one style to another. 

I was wondering if we can use the vast parallel corpus of non-nude and nude images from a sub like /r/onoff to create a transforming software. 

I know, as a male, my brain is capable of such transformations. As a human is able to do it, how far are we from doing it using machines.",1,0
598,2017-7-14,2017,7,14,23,6n97my,What do you guys think about Siraj Raval's videos on YouTube? [Discussion],https://www.reddit.com/r/MachineLearning/comments/6n97my/what_do_you_guys_think_about_siraj_ravals_videos/,PhenolicPeatReek,1500040954,"I was watching a few lectures on YouTube about backpropagation in neural networks, because I've forgotten how exactly it was done in detail and wanted to brush up on it. Anyway, YouTube continued playing videos of [Siraj Raval](https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A) for some reason. So, I watched a few of his videos. I must admit I got rather bemused by his approach. He condenses ML subject matters down to things like ""TensorFlow in 5 Minutes"" or ""How to Predict Stock Prices Easily"". He talks to the audience like they are supposed to be beginners, but then proceeds to steamroll them with advanced topic information. He glosses over things and is very handwavy. Everything I've heard so far is correct, but I don't understand what or to whom it is useful for. I could follow his videos fine, but I've got plenty of education in this field, I just can't imagine being introduces to the subject mater like this. 

[Take this video on vectors](https://www.youtube.com/watch?v=s0Q3CojqRfM) for instance. He nonchalantly starts talking about regularization and vector norms as if you should be able to understand a whole course work's worth of information in 30 seconds. My fair is that these kind of videos create ML phonies and script kiddies. You might get them as colleagues, and they have all of the jargon down, but their knowledge and understanding turns out to be utterly shallow: ""Yeah, I know vectors and stuff. It's that Word2Vec stuff, right? Pretty cool, eh? King-Man+Woman=Queen!""

Idk. Maybe I'm being too harsh. He is definitely knowledgeable and entertaining, and a cool guy. But how educational is his channel really?",106,132
599,2017-7-14,2017,7,14,23,6n9bkm,[P] Understanding &amp; Visualizing Self-Normalizing Neural Networks,https://www.reddit.com/r/MachineLearning/comments/6n9bkm/p_understanding_visualizing_selfnormalizing/,lightcatcher,1500042051,,15,90
600,2017-7-14,2017,7,14,23,6n9c6e,[R] [1707.04131] Foolbox v0.8.0: A Python toolbox to benchmark the robustness of machine learning models,https://www.reddit.com/r/MachineLearning/comments/6n9c6e/r_170704131_foolbox_v080_a_python_toolbox_to/,sensei_von_bonzai,1500042219,,1,5
601,2017-7-14,2017,7,14,23,6n9ckb,[D] Random Noise in Published Results,https://www.reddit.com/r/MachineLearning/comments/6n9ckb/d_random_noise_in_published_results/,fuckhiking,1500042329,"How does one deal with model variance when reporting research results? I validated my results by repeated random subsampling (100+ times) so the variance is small, but at the same time, picking numbers at the higher end of the range would allow me to reach a different conclusion than numbers at the low end of the range. I think 100 iterations of sampling is an honest attempt to remove variance and that doing significantly more would be infeasible and likely a waste of resources. How do people usually handle this situation?",4,14
602,2017-7-14,2017,7,14,23,6n9fpc,"Installed tensorflow, but pycharm ignores it",https://www.reddit.com/r/MachineLearning/comments/6n9fpc/installed_tensorflow_but_pycharm_ignores_it/,spartan12321,1500043181,[removed],0,1
603,2017-7-15,2017,7,15,0,6n9klv,"[D] Have their been papers written on machine learning for the quality of the picture taken? ie. how well a picture is framed, good v. bad.",https://www.reddit.com/r/MachineLearning/comments/6n9klv/d_have_their_been_papers_written_on_machine/,democritus_is_op,1500044526,,5,3
604,2017-7-15,2017,7,15,0,6n9v53,Image Captioning using InceptionV3 and Beam Search,https://www.reddit.com/r/MachineLearning/comments/6n9v53/image_captioning_using_inceptionv3_and_beam_search/,[deleted],1500047323,[deleted],0,1
605,2017-7-15,2017,7,15,0,6n9voj,Alexey Kurakin and Ian Goodfellow will answer questions about the adversarial ML contest in this Quora session Monday,https://www.reddit.com/r/MachineLearning/comments/6n9voj/alexey_kurakin_and_ian_goodfellow_will_answer/,[deleted],1500047463,[deleted],0,1
606,2017-7-15,2017,7,15,1,6na2g6,[R] Ian Goodfellow and Alexey Kurakin will answer questions on Adversarial machine learning research and NIPS 2017 competition on Adversarial Attacks and Defences,https://www.reddit.com/r/MachineLearning/comments/6na2g6/r_ian_goodfellow_and_alexey_kurakin_will_answer/,cognitivedemons,1500049141,,0,61
607,2017-7-15,2017,7,15,1,6na2m3,Recent Evolution of QA Datasets and Going Forward,https://www.reddit.com/r/MachineLearning/comments/6na2m3/recent_evolution_of_qa_datasets_and_going_forward/,[deleted],1500049180,[deleted],0,1
608,2017-7-15,2017,7,15,1,6naa6p,Prerequisites for getting into machine learning - specifically deep learning?,https://www.reddit.com/r/MachineLearning/comments/6naa6p/prerequisites_for_getting_into_machine_learning/,forgotuseranem,1500051149,[removed],0,1
609,2017-7-15,2017,7,15,2,6nainh,Quilt is a data package manager,https://www.reddit.com/r/MachineLearning/comments/6nainh/quilt_is_a_data_package_manager/,zitterbewegung,1500053283,,0,1
610,2017-7-15,2017,7,15,2,6namgi,Using machine learning processes as inspiration for physical paintings,https://www.reddit.com/r/MachineLearning/comments/6namgi/using_machine_learning_processes_as_inspiration/,[deleted],1500054241,[deleted],0,1
611,2017-7-15,2017,7,15,3,6nas25,[p] Deel Learning Made Simple [Part 1],https://www.reddit.com/r/MachineLearning/comments/6nas25/p_deel_learning_made_simple_part_1/,[deleted],1500055676,[deleted],0,1
612,2017-7-15,2017,7,15,3,6nasdi,[P] Using machine learning processes as inspiration for physical paintings,https://www.reddit.com/r/MachineLearning/comments/6nasdi/p_using_machine_learning_processes_as_inspiration/,Loggerny,1500055755,,1,14
613,2017-7-15,2017,7,15,3,6nasey,PicnicHealth won Google Machine Learning Startup Competition,https://www.reddit.com/r/MachineLearning/comments/6nasey/picnichealth_won_google_machine_learning_startup/,02inf,1500055767,,0,1
614,2017-7-15,2017,7,15,3,6nassm,What is the Difference Between Test and Validation Datasets?,https://www.reddit.com/r/MachineLearning/comments/6nassm/what_is_the_difference_between_test_and/,alexcmu,1500055862,,1,1
615,2017-7-15,2017,7,15,3,6nauk1,[p] Deep Learning Made Simple [Part 1],https://www.reddit.com/r/MachineLearning/comments/6nauk1/p_deep_learning_made_simple_part_1/,vpanyam,1500056343,,2,13
616,2017-7-15,2017,7,15,3,6nawy6,Effect of Batch Size on Image Captioning results,https://www.reddit.com/r/MachineLearning/comments/6nawy6/effect_of_batch_size_on_image_captioning_results/,[deleted],1500056953,[deleted],0,1
617,2017-7-15,2017,7,15,4,6nbch9,[R] Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation,https://www.reddit.com/r/MachineLearning/comments/6nbch9/r_imitation_from_observation_learning_to_imitate/,gdny,1500060963,,3,14
618,2017-7-15,2017,7,15,5,6nbta8,What AI Means for the Future of Data Governance and Big Data - video,https://www.reddit.com/r/MachineLearning/comments/6nbta8/what_ai_means_for_the_future_of_data_governance/,DataRealized,1500065530,,0,1
619,2017-7-15,2017,7,15,6,6nbzi5,[Discussion] [D] I'm interested specifically in the niche intersection between machine learning and data from connected devices. Starting a new sub. Relevant? Significant enough? Your thoughts appreciated.,https://www.reddit.com/r/MachineLearning/comments/6nbzi5/discussion_d_im_interested_specifically_in_the/,onegazillion,1500067235,,2,0
620,2017-7-15,2017,7,15,7,6nc7z8,Great TensorFlow Tutorial For Beginners,https://www.reddit.com/r/MachineLearning/comments/6nc7z8/great_tensorflow_tutorial_for_beginners/,gcdes,1500069668,,0,1
621,2017-7-15,2017,7,15,7,6nc8ou,"[D] is there a way to train a neural network which finally has a certain weights distribution, e.g., uniform distribution.",https://www.reddit.com/r/MachineLearning/comments/6nc8ou/d_is_there_a_way_to_train_a_neural_network_which/,hex0102,1500069857,"If the weights can be confined to a certain distribution, I guess it could be more efficient when implementing this network to real hardware. To make this simple, let's assume the network is an mlp network.",24,13
622,2017-7-15,2017,7,15,7,6nc8zo,Predicting Stocks w/ Quandl &amp; Sklearn,https://www.reddit.com/r/MachineLearning/comments/6nc8zo/predicting_stocks_w_quandl_sklearn/,[deleted],1500069958,[removed],0,1
623,2017-7-15,2017,7,15,7,6nc90q,[D] A3C versus multi-threaded DQN,https://www.reddit.com/r/MachineLearning/comments/6nc90q/d_a3c_versus_multithreaded_dqn/,Neutran,1500069970,"People have told me that I should just go for A3C if I want the best off-the-shelf RL algorithm. However, I cannot find any benchmarking of A3C (Asynchronous Advantage Actor Critic) vs multithreaded DQN in its full glory (double DQN, dueling, prioritized experience replay). 

The only relevant comparison I can find is in the appendix of the A3C paper: https://arxiv.org/abs/1602.01783

However, it compares A3C to vanilla Gorila (i.e. multithreaded DQN with no tricks), dueling DQN (single-threaded), and prioritized experience replay (also single-threaded). Correct me if I'm wrong, but I don't think this is a fair comparison. It'd be more convincing if the paper compares 32-threaded A3C to 32-threaded Gorila with all the DQN enhancement tricks added.

Does anyone have experience with distributed DQN? 

Under what circumstances will you recommend DQN over A3C? For example, does DQN tend to be more sample-efficient?

Side discussion:
There are a huge number of open-source implementations online. In terms of algorithmic performance (convergence, final score, **not** code execution speed), which repo of DQN/A3C will you recommend?  ",15,11
624,2017-7-15,2017,7,15,7,6nc9c5,Machine Learning in Business,https://www.reddit.com/r/MachineLearning/comments/6nc9c5/machine_learning_in_business/,ddevsidas,1500070077,[removed],0,1
625,2017-7-15,2017,7,15,7,6nce4n,Dimensionality Reduction - The Math of Intelligence #5,https://www.reddit.com/r/MachineLearning/comments/6nce4n/dimensionality_reduction_the_math_of_intelligence/,funmaster11,1500071551,,0,2
626,2017-7-15,2017,7,15,8,6ncjqy,[D] Does anyone know how Tensorboard smooths the step values?,https://www.reddit.com/r/MachineLearning/comments/6ncjqy/d_does_anyone_know_how_tensorboard_smooths_the/,SkiddyX,1500073357,Looking for how to do the same in Python as I want to recreate a graph.,2,14
627,2017-7-15,2017,7,15,8,6nck8r,Train a Face Recognition Model to Recognize Celebrities,https://www.reddit.com/r/MachineLearning/comments/6nck8r/train_a_face_recognition_model_to_recognize/,peckjon,1500073516,,0,1
628,2017-7-15,2017,7,15,8,6ncmhr,"Familiarising myself with the field, where to start?",https://www.reddit.com/r/MachineLearning/comments/6ncmhr/familiarising_myself_with_the_field_where_to_start/,[deleted],1500074208,[removed],0,1
629,2017-7-15,2017,7,15,9,6ncxaa,Hot Water System Installation,https://www.reddit.com/r/MachineLearning/comments/6ncxaa/hot_water_system_installation/,gwenyosef,1500077771,,0,1
630,2017-7-15,2017,7,15,11,6ndnh5,"[N] Seeing AI, Talking camera app for those with a visual impairment",https://www.reddit.com/r/MachineLearning/comments/6ndnh5/n_seeing_ai_talking_camera_app_for_those_with_a/,nharada,1500087072,,0,16
631,2017-7-15,2017,7,15,13,6ne5qf,Lip Reading  Who Said What? Answered by Deep Learning,https://www.reddit.com/r/MachineLearning/comments/6ne5qf/lip_reading_who_said_what_answered_by_deep/,irsina,1500094315,,0,1
632,2017-7-15,2017,7,15,15,6nem5b,Snake oil and Pia Colada: Will I Survive the Artificial Intelligence Summer of Love?,https://www.reddit.com/r/MachineLearning/comments/6nem5b/snake_oil_and_pia_colada_will_i_survive_the/,leaningtoweravenger,1500101838,,0,1
633,2017-7-15,2017,7,15,16,6nereo,"Offered a PhD Scholarship in Machine Learning, should I do it?",https://www.reddit.com/r/MachineLearning/comments/6nereo/offered_a_phd_scholarship_in_machine_learning/,GeoResearchRedditor,1500104582,[removed],0,1
634,2017-7-15,2017,7,15,18,6nf03j,[R] Enhanced Deep Residual Networks for Single Image Super-Resolution (Winner of NTIRE2017 SR challenge),https://www.reddit.com/r/MachineLearning/comments/6nf03j/r_enhanced_deep_residual_networks_for_single/,ENGERLUND,1500109429,,8,42
635,2017-7-15,2017,7,15,19,6nf6xc,Effect of Batch Size on Image Captioning Results,https://www.reddit.com/r/MachineLearning/comments/6nf6xc/effect_of_batch_size_on_image_captioning_results/,[deleted],1500113291,[deleted],0,1
636,2017-7-15,2017,7,15,19,6nfakw,Getting started with tensor flow with C++,https://www.reddit.com/r/MachineLearning/comments/6nfakw/getting_started_with_tensor_flow_with_c/,Ashpro96,1500115297,[removed],0,1
637,2017-7-15,2017,7,15,20,6nfigj,[D] [1706.09799] Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation,https://www.reddit.com/r/MachineLearning/comments/6nfigj/d_170609799_relevance_of_unsupervised_metrics_in/,mlfinder,1500118976,,2,13
638,2017-7-15,2017,7,15,23,6ng7pg,[Research] Looking for Notebooks/Articles on uisng supervised ML to predict a binary (for/against) outcome based on political sentiment.,https://www.reddit.com/r/MachineLearning/comments/6ng7pg/research_looking_for_notebooksarticles_on_uisng/,tastingsilver,1500129108,"Hey guys,

Have been googling with some minor avail. I'm trying to work on a (non-Kaggle) project where I'd like to train an ML algo to determine using NLTK or textblob whether someone is for or against a proposal, which is open for comment.

Political language will play as a feature here, but I will have a train/test sample for 1% of the overall dataset.

Pardon if this is not the appropriate sub, but I wondered if anyone knew of any familiar articles/notebooks that approached this subject in a similar fashion.",5,7
639,2017-7-15,2017,7,15,23,6ngaen,Using ASP.NET to render Twitter data processed by Microsoft Cognitive Services,https://www.reddit.com/r/MachineLearning/comments/6ngaen/using_aspnet_to_render_twitter_data_processed_by/,maguirej160,1500130043,,0,1
640,2017-7-16,2017,7,16,1,6ngnwp,Machine Learning study group,https://www.reddit.com/r/MachineLearning/comments/6ngnwp/machine_learning_study_group/,[deleted],1500134463,[removed],0,1
641,2017-7-16,2017,7,16,1,6ngqgj,[Discussion] Machine Learning study group,https://www.reddit.com/r/MachineLearning/comments/6ngqgj/discussion_machine_learning_study_group/,sakram07,1500135211,"I am self-learning Machine Learning/Data Science and thought it would be very cool to have a group of individuals with whom to discuss what we just learned and talk about parts that were hard to understand. Also, perhaps doing a few projects together would help to get used to working on ML in a team. Another thing is motivation, having deadlines might force you to finally finish reading that chapter instead of browsing Reddit.

I am thinking it would go like this:

- Having a small discord/slack server for all the discussions.
- Reading one or two books at a time, and after the deadline for reading a chapter ends, discussing it.
- Instead of books, video courses/internet tutorials/etc. can work just as well.
- Doing some exercises. For example: implementing algorithm that was discussed in the chapter of the book we are reading and then comparing our solutions, trying to improve them.
- Working on Kaggle contests and data sets or GitHub libraries together.

If you have any suggestions on this list, feel free to make them. I know there have been other such groups in the past, but they are not very active at this point.

Here is the [google form](https://docs.google.com/forms/d/e/1FAIpQLSdVYtXdY9IKiWrXfVT3j95dPVASJp6LkDiSW8AZHP7wauBGnA/viewform). I will make a server and send everyone an e-mail if enough people show interest.

So, what do you think? Is having such group a good idea?

[edit] I made a discord server.
https://discord.gg/4uPzFYz",41,131
642,2017-7-16,2017,7,16,2,6nh0jy,Encoding-Decoding in Keras,https://www.reddit.com/r/MachineLearning/comments/6nh0jy/encodingdecoding_in_keras/,[deleted],1500138240,[removed],0,1
643,2017-7-16,2017,7,16,2,6nh4tu,"[P] Mimic Iphone portrait mode using Fully Convolutional Neural Networks, plus other applications",https://www.reddit.com/r/MachineLearning/comments/6nh4tu/p_mimic_iphone_portrait_mode_using_fully/,warmspringwinds,1500139538,,4,8
644,2017-7-16,2017,7,16,2,6nhb0p,What are some good unacquired AI groups?,https://www.reddit.com/r/MachineLearning/comments/6nhb0p/what_are_some_good_unacquired_ai_groups/,[deleted],1500141337,[removed],0,1
645,2017-7-16,2017,7,16,2,6nhb7r,[D] What are some good unacquired AI groups?,https://www.reddit.com/r/MachineLearning/comments/6nhb7r/d_what_are_some_good_unacquired_ai_groups/,metacurse,1500141402,"Clarifai is the only one that comes to my head. Have the all the remaining ones eaten by big cos already?
",18,8
646,2017-7-16,2017,7,16,4,6nhxwz,Has the news media turned a corner on their coverage of Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/6nhxwz/has_the_news_media_turned_a_corner_on_their/,reportdinghy,1500148234,[removed],0,1
647,2017-7-16,2017,7,16,5,6ni83i,Intelligence Design Lab 6.1 - Video,https://www.reddit.com/r/MachineLearning/comments/6ni83i/intelligence_design_lab_61_video/,GaryGaulin,1500151430,,0,1
648,2017-7-16,2017,7,16,6,6nig9e,So I did a sin/cos transformation on some cyclcal input features of my data (day of the year and hour). During eval it improved my MAPE by ~5%. Then I looked at the output values,https://www.reddit.com/r/MachineLearning/comments/6nig9e/so_i_did_a_sincos_transformation_on_some_cyclcal/,sdmike21,1500153956,,1,1
649,2017-7-16,2017,7,16,8,6nj0b1,[D] What is the state of the art approach for video action recognition?,https://www.reddit.com/r/MachineLearning/comments/6nj0b1/d_what_is_the_state_of_the_art_approach_for_video/,mltool,1500160518,"I am looking for state of the art paper on UCF 101, HMDB 51 or larger action recognition datasets. Best approach which I could find was https://arxiv.org/abs/1611.06678",3,6
650,2017-7-16,2017,7,16,13,6nkhov,[1706.08947v1] Exploring Generalization in Deep Learning,https://www.reddit.com/r/MachineLearning/comments/6nkhov/170608947v1_exploring_generalization_in_deep/,nightcore_lover,1500180483,,0,1
651,2017-7-16,2017,7,16,13,6nkizs,[D] Siamese Networks for One Shot Learning in PyTorch - part 1,https://www.reddit.com/r/MachineLearning/comments/6nkizs/d_siamese_networks_for_one_shot_learning_in/,harvey_slash,1500181032,,4,16
652,2017-7-16,2017,7,16,14,6nkkoy,University research groups actively working in RL in the US?,https://www.reddit.com/r/MachineLearning/comments/6nkkoy/university_research_groups_actively_working_in_rl/,iamquah,1500181771,[removed],0,1
653,2017-7-16,2017,7,16,15,6nksj0,[D] Elon Musk: Artificial Intelligence Is the Greatest Risk We Face as a Civilization,https://www.reddit.com/r/MachineLearning/comments/6nksj0/d_elon_musk_artificial_intelligence_is_the/,wei_jok,1500185314,,210,165
654,2017-7-16,2017,7,16,15,6nkxu8,[D] Where did Franois Chollet go to school ?,https://www.reddit.com/r/MachineLearning/comments/6nkxu8/d_where_did_franois_chollet_go_to_school/,pinkflamingo16,1500187995,I'm working on a sort of genealogy of the big names in ML/DL right now and can't find much about his time before Keras. Anyone know ?,13,0
655,2017-7-16,2017,7,16,16,6nkzx6,What are some good examples of visualisation of neural networks,https://www.reddit.com/r/MachineLearning/comments/6nkzx6/what_are_some_good_examples_of_visualisation_of/,kbrowne,1500189070,[removed],0,1
656,2017-7-16,2017,7,16,17,6nl65k,[project] Tensorflow implementation of Noisy Network for exploration,https://www.reddit.com/r/MachineLearning/comments/6nl65k/project_tensorflow_implementation_of_noisy/,andrewliao11,1500192629,"Train on Atari envs

TF version:
https://github.com/andrewliao11/NoisyNet-DQN
my personal notes:https://github.com/andrewliao11/Deep-Reinforcement-Learning-Survey/blob/master/papers/Noisy%20Networks%20for%20Exploration.md",6,3
657,2017-7-16,2017,7,16,18,6nlgb0,Machine Learning Weekly Review #2,https://www.reddit.com/r/MachineLearning/comments/6nlgb0/machine_learning_weekly_review_2/,[deleted],1500198543,[deleted],0,1
658,2017-7-16,2017,7,16,18,6nlhb1,Machine learning in excel?,https://www.reddit.com/r/MachineLearning/comments/6nlhb1/machine_learning_in_excel/,SomeGuyInNewZealand,1500199168,[removed],0,1
659,2017-7-16,2017,7,16,19,6nlhpi,[N] Machine Learning Weekly Review #1,https://www.reddit.com/r/MachineLearning/comments/6nlhpi/n_machine_learning_weekly_review_1/,[deleted],1500199403,[deleted],0,1
660,2017-7-16,2017,7,16,20,6nlogs,[D] Cheating with hand-crafted rules in machine learning pipeline?,https://www.reddit.com/r/MachineLearning/comments/6nlogs/d_cheating_with_handcrafted_rules_in_machine/,steve_g,1500203064,"A while back on Hacker News there was an informative thread about production ML pipelines - [Ask HN: What does your production machine learning pipeline look like?] (https://news.ycombinator.com/item?id=13821217)

I didnt see any discussion about using hand-crafted rules to tweak the outcome where the ML algorithm wasnt discriminating well based on patterns in the data. In some cases a boolean rule might fix the confusion, especially if the rule is an actual business requirement, not an ad hoc tweak.

For example, I made a classifier to categorize mixtures of chemicals. For each type of mixture, we had a hazardous class and non-hazardous class (based on regulatory status). The hazardous and non-hazardous mixtures could be basically the same stuff, so the classifier often was confused between the two classes. A boolean rule fixed the confusion perfectly.

So, practitioners, do you cheat in your pipelines? How do you do it?",10,5
661,2017-7-16,2017,7,16,21,6nm0rz,Deep learning using multi variable time series data?,https://www.reddit.com/r/MachineLearning/comments/6nm0rz/deep_learning_using_multi_variable_time_series/,[deleted],1500208781,[removed],0,1
662,2017-7-16,2017,7,16,23,6nmgv7,[N] Machine Learning Weekly Review #2,https://www.reddit.com/r/MachineLearning/comments/6nmgv7/n_machine_learning_weekly_review_2/,rldlml,1500215830,,0,1
663,2017-7-17,2017,7,17,1,6nmy8x,lrn 2 validation set u machine learning dummies,https://www.reddit.com/r/MachineLearning/comments/6nmy8x/lrn_2_validation_set_u_machine_learning_dummies/,aizu9,1500221302,,0,1
664,2017-7-17,2017,7,17,2,6nn9yq,A Data Analysis on GANs,https://www.reddit.com/r/MachineLearning/comments/6nn9yq/a_data_analysis_on_gans/,skyleach,1500224773,,0,1
665,2017-7-17,2017,7,17,2,6nncsf,[P] Classifying Question using Support Vector Machine,https://www.reddit.com/r/MachineLearning/comments/6nncsf/p_classifying_question_using_support_vector/,shirishsono,1500225584,"So, this is my first NLP/ML project and I would like to share it here. 
Blog post explaining its implementation: https://shirishkadam.com/2017/07/03/nlp-question-classification-using-support-vector-machines-spacyscikit-learnpandas/
Any kind of suggestions, tips, enhancements is welcome. So, using LinearSVM I am trying to classify a question into a category to best describe its intentions. This is being used in a Question Answering system (https://github.com/5hirish/adam_qas).

Edit: I would also appreciate if some one points to the right direction to do feature extraction the unsupervised way. (The question's class can be used to extract features)",2,3
666,2017-7-17,2017,7,17,2,6nnht0,Artificial intelligence the next digital frontier? McKinsey&amp;Company 2017,https://www.reddit.com/r/MachineLearning/comments/6nnht0/artificial_intelligence_the_next_digital_frontier/,based2,1500227046,,0,1
667,2017-7-17,2017,7,17,3,6nnmdy,[P] In this project I tried to train Chrome's Trex character to learn to play by looking my gameplay (Supervised).,https://www.reddit.com/r/MachineLearning/comments/6nnmdy/p_in_this_project_i_tried_to_train_chromes_trex/,asingh33,1500228294,,36,393
668,2017-7-17,2017,7,17,3,6nnp4h,[P] : Predicting kickstarter project success,https://www.reddit.com/r/MachineLearning/comments/6nnp4h/p_predicting_kickstarter_project_success/,canhelp,1500229077,,0,0
669,2017-7-17,2017,7,17,3,6nnr9l,New Channel Trailer about Machine Learning,https://www.reddit.com/r/MachineLearning/comments/6nnr9l/new_channel_trailer_about_machine_learning/,funmaster11,1500229706,,0,1
670,2017-7-17,2017,7,17,3,6nnroh,Transfer Learning Papers,https://www.reddit.com/r/MachineLearning/comments/6nnroh/transfer_learning_papers/,raghavgoyal14,1500229822,[removed],0,1
671,2017-7-17,2017,7,17,4,6no2aa,here's something else for you to machine learn :),https://www.reddit.com/r/MachineLearning/comments/6no2aa/heres_something_else_for_you_to_machine_learn/,aizu9,1500232912,,0,1
672,2017-7-17,2017,7,17,4,6no3pg,[URML 2017] Unconference in /r/MachineLearning: planning and suggestions,https://www.reddit.com/r/MachineLearning/comments/6no3pg/urml_2017_unconference_in_rmachinelearning/,olaf_nij,1500233305,"To respect the anonymity in the current NIPS review cycle, we will hold the the unconference after July and likely sometime in mid August. In terms of format, here's what I've been planning:

**Format:** Authors create a thread with '[URML 2017]' tag and paper title. Filtering results in this subreddit by this tag is as simple as [filtering results with the [R] tag](https://www.reddit.com/r/MachineLearning/search?q=%5BR%5D&amp;sort=new&amp;restrict_sr=on&amp;t=all)

**Verification:** Authors can apply for ""Verified"" tag on their post by PM moderators. If the number of requests is rather high, it would be great if anyone has better ideas of verifying authors.

**Awards:** Rather than having a best paper award like most conferences, I'm thinking of having a best submission award (URML's BS award :D ). The BS award would be given to the submission that has the most engaging/interesting discussion. This could be done with a poll after the unconference. The award would include a prize such as increased visibility (sidebar/sticky post in the sub), special flair, etc. 

Moderation will be significantly stricter for these posts and personal attacks and general unfriendly behavior will result in bans without warning.

This unconference is an experiment to bring the ML community closer and reach a wider audience than a typical offline conference would. Hopefully this will spur discussion and feedback that can be read and seen by more than the two individuals having the conversation.
",17,55
673,2017-7-17,2017,7,17,4,6no9xo,[P] DeepArtistry: Neural Style Art Store,https://www.reddit.com/r/MachineLearning/comments/6no9xo/p_deepartistry_neural_style_art_store/,[deleted],1500235078,[removed],3,0
674,2017-7-17,2017,7,17,5,6nodqo,[P] An end to end implementation of a Machine Learning pipeline,https://www.reddit.com/r/MachineLearning/comments/6nodqo/p_an_end_to_end_implementation_of_a_machine/,figurelover,1500236174,,1,29
675,2017-7-17,2017,7,17,6,6norsz,"""Ruse not lest ye be rused""",https://www.reddit.com/r/MachineLearning/comments/6norsz/ruse_not_lest_ye_be_rused/,aizu9,1500240289,,0,1
676,2017-7-17,2017,7,17,6,6noy64,[P] Real World Deep Learning Newsletter,https://www.reddit.com/r/MachineLearning/comments/6noy64/p_real_world_deep_learning_newsletter/,[deleted],1500242192,[deleted],3,0
677,2017-7-17,2017,7,17,7,6np588,Finding Common Characteristics Among NBA Playoff and Championship Teams: A Machine Learning Approach,https://www.reddit.com/r/MachineLearning/comments/6np588/finding_common_characteristics_among_nba_playoff/,dr_isk_16,1500244334,,0,1
678,2017-7-17,2017,7,17,7,6np5iw,Painters and Painting Services,https://www.reddit.com/r/MachineLearning/comments/6np5iw/painters_and_painting_services/,gwenyosef,1500244415,,0,1
679,2017-7-17,2017,7,17,8,6npe48,Useful Machine Learning Resources &amp; Recommended Study Routes,https://www.reddit.com/r/MachineLearning/comments/6npe48/useful_machine_learning_resources_recommended/,datasciencelover,1500247103,,0,1
680,2017-7-17,2017,7,17,9,6npqkg,Looking for a CTO/Co-Founder for Quantum-based ML startup in NYC...,https://www.reddit.com/r/MachineLearning/comments/6npqkg/looking_for_a_ctocofounder_for_quantumbased_ml/,PQJude,1500251163,[removed],0,1
681,2017-7-17,2017,7,17,10,6npx8h,"[D] Has anyone reproduced the ""meProp"" results for ConvNets?",https://www.reddit.com/r/MachineLearning/comments/6npx8h/d_has_anyone_reproduced_the_meprop_results_for/,darkconfidantislife,1500253354,"I'm talking about this -----&gt; https://arxiv.org/abs/1706.06197 paper.

I tried it on MNIST with a ConvNet and it stumbles badly. Anyone managed to reproduce it and gotten good results on ConvNets (not MLPs)? ",0,8
682,2017-7-17,2017,7,17,11,6nqg5n,Road To AI 4 - Back Propagation Part 2,https://www.reddit.com/r/MachineLearning/comments/6nqg5n/road_to_ai_4_back_propagation_part_2/,lochiewestfall,1500259897,,0,1
683,2017-7-17,2017,7,17,12,6nqhz9,How do I built word counter using Machine learning?,https://www.reddit.com/r/MachineLearning/comments/6nqhz9/how_do_i_built_word_counter_using_machine_learning/,techsin101,1500260538,[removed],0,1
684,2017-7-17,2017,7,17,12,6nqikr,[N] USGS Official Warns of 'Reduction or Elimination' of Important Data Access,https://www.reddit.com/r/MachineLearning/comments/6nqikr/n_usgs_official_warns_of_reduction_or_elimination/,onegazillion,1500260731,,2,3
685,2017-7-17,2017,7,17,12,6nqk62,[D] Difference between Normalizing Flows and Non-volume preserving flows.,https://www.reddit.com/r/MachineLearning/comments/6nqk62/d_difference_between_normalizing_flows_and/,7408,1500261294,"The Normalizing flows paper (https://arxiv.org/pdf/1505.05770.pdf) 
describes the normalizing flow as:

""A normalizing flow describes the transformation of a probability
density through a sequence of invertible mappings.
By repeatedly applying the rule for change of variables,
the initial density flows through the sequence of invertible
mappings. At the end of this sequence we obtain a
valid probability distribution and hence this type of flow is
referred to as a normalizing flow.""

I am not sure I understand properly what a normalizing flow exactly is and how is it different from the non-volume preserving flow in the real NVP (https://arxiv.org/abs/1605.08803) which also does the transformation of an initial density through invertible mappings.

In addition can someone give me pointers to reading material to understand normalizing flows and get an intuition as to what these different transformations are doing. For instance I am not sure I understand why we want to make a volume preserving transformation into non - volume preserving as done in the NICE (https://arxiv.org/abs/1410.8516) paper by adding an extra scale factor.

",2,6
686,2017-7-17,2017,7,17,13,6nqyon,Guide on how to build a Smart Chatbot which makes use of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/6nqyon/guide_on_how_to_build_a_smart_chatbot_which_makes/,hardikmakadia,1500266765,,0,1
687,2017-7-17,2017,7,17,14,6nr1pk,Deep Neural Networks Do Not Recognize Negative Images,https://www.reddit.com/r/MachineLearning/comments/6nr1pk/deep_neural_networks_do_not_recognize_negative/,SJetKaran,1500267932,,0,1
688,2017-7-17,2017,7,17,14,6nr4rm,automatic toothpaste cartoning machine manufacturer price,https://www.reddit.com/r/MachineLearning/comments/6nr4rm/automatic_toothpaste_cartoning_machine/,hymachinery,1500269159,,0,1
689,2017-7-17,2017,7,17,15,6nraw8,Machine Learning and Its Importance for Your Enterprise,https://www.reddit.com/r/MachineLearning/comments/6nraw8/machine_learning_and_its_importance_for_your/,nexcorp,1500271767,,0,1
690,2017-7-17,2017,7,17,15,6nreba,Lists of common casual utterances to train NLP model on?,https://www.reddit.com/r/MachineLearning/comments/6nreba/lists_of_common_casual_utterances_to_train_nlp/,alfa_sixten,1500273244,[removed],0,1
691,2017-7-17,2017,7,17,15,6nrevg,[D] Question about models to handle sparse data,https://www.reddit.com/r/MachineLearning/comments/6nrevg/d_question_about_models_to_handle_sparse_data/,aleph__one,1500273497,"Background: Have been doing ML for a 7+ years now, DL in the past 3-4 years. I've never seen anything like what I'm about to describe, but it is entirely likely I've simply missed something. I wanted to check before I go write my own model and do the whole put-it-on-arxiv-way-too-soon thing.

Specifically, let's say I'm doing a problem with car valuations (this was based on a real project). In the data you have variables like [make, model, year, mileage, color, power doors, condition rating]. Now, some of these will be present in 100% of samples, but others like color or condition rating will only be present in about 70% of samples.

In most ML settings, you'd have to discard samples with missing data. I don't want to have to do this. Rather, I'd like to train a model that can take into account whatever information is available, and if it's not available simply ignore it. That is, if we're given two samples:

* Ford, Focus, 2002, red
* Ford, Focus, 2003

We might imagine that red cars are generally about 0.5% more valuable than other colors, so when the information is available as in the first sample, our prediction of the valuation would be refined with that new information (let's say it predicts $3,160).

Is anyone aware of a model or class of models that can do this? Instinctively it seems like a Bayesian network thing, but in case it doesn't exist I was imagining making some modifications to a random forest model or gradient boosting model. The best way I can describe it is to take into account whatever information is there, and for whatever info is not there, simply ignore it / treat as a no-op. It may involve learning multiple sub-models but I'm ok with that.

Bonus info: There are some classes that contain more information than others. For example, if I give you just ""Ferrari"", with no other data, you can reasonably come up with a good price range from just that one variable; the bounds around this are tighter than ""Ford"", which has a range from cheap sedans to decked out Mustangs.

Please provide paper and/or code references if you know of anything like this. Thanks!",23,8
692,2017-7-17,2017,7,17,15,6nrfsn,ELI5: Bayesian calibration vs Bayesian inference?,https://www.reddit.com/r/MachineLearning/comments/6nrfsn/eli5_bayesian_calibration_vs_bayesian_inference/,[deleted],1500273936,[removed],0,1
693,2017-7-17,2017,7,17,16,6nrix0,Peanut Strip Cutting Machine|Peanut Cutter Machine|Almond Strip Cutting Machine|Pistachio Cutting Machine,https://www.reddit.com/r/MachineLearning/comments/6nrix0/peanut_strip_cutting_machinepeanut_cutter/,gelserena,1500275340,,1,1
694,2017-7-17,2017,7,17,17,6nrqj5,Research paper on using Genetic Programming to create a single controller that plays multiple Atari games. Claims to work better than state-of-the-art Deep Learning methods.,https://www.reddit.com/r/MachineLearning/comments/6nrqj5/research_paper_on_using_genetic_programming_to/,[deleted],1500279006,[deleted],0,1
695,2017-7-17,2017,7,17,18,6ns3om,[P] Debugging &amp; Visualising training of Neural Network with TensorBoard,https://www.reddit.com/r/MachineLearning/comments/6ns3om/p_debugging_visualising_training_of_neural/,jalFaizy,1500285145,,0,10
696,2017-7-17,2017,7,17,19,6nsbxh,[D] In which tasks does deep learning beat human performance?,https://www.reddit.com/r/MachineLearning/comments/6nsbxh/d_in_which_tasks_does_deep_learning_beat_human/,NLPGuy,1500288815,"I'm researching in which tasks deep learning approaches already beat human performance. So far I came up with:

* image classification: ILSVRC challenge with top-5 error, human performance estimated to be 5.1% error-rate, broken multiple times with the best performance of 2.991% error-rate in 2016.
* face recognition
* speech recognition reached parity with human performance

Are there any other tasks where deep learning surpassed human performance?",6,7
697,2017-7-17,2017,7,17,20,6nse50,"""dialogue corpus"" about instant messaging fraud by hacking account",https://www.reddit.com/r/MachineLearning/comments/6nse50/dialogue_corpus_about_instant_messaging_fraud_by/,pxwu13,1500289725,[removed],0,1
698,2017-7-17,2017,7,17,20,6nshpq,[P] Attempt to build a lion2leopard CycleGAN,https://www.reddit.com/r/MachineLearning/comments/6nshpq/p_attempt_to_build_a_lion2leopard_cyclegan/,shabeyyub,1500291136,,4,2
699,2017-7-17,2017,7,17,20,6nsivw,Multidimensional LSTM Networks to Predict Bitcoin Price,https://www.reddit.com/r/MachineLearning/comments/6nsivw/multidimensional_lstm_networks_to_predict_bitcoin/,shivinski,1500291585,,0,1
700,2017-7-17,2017,7,17,20,6nskph,[D] Tensorflow Implementation of Relation Network for bAbI dataset,https://www.reddit.com/r/MachineLearning/comments/6nskph/d_tensorflow_implementation_of_relation_network/,plumingo,1500292249,"Our implementation and result are on the github
https://github.com/juung/Relation-Network

We couldn't get 18/20 result written on the original paper, and we want to discuss some doubtful points.

Issue 1:
We couldn't handle different number of sentences in one context. In the paper it said, up to 20 sentences, but we made all context in 20 sentences; padded leftovers zero

Issue 2:
Answers of Task 19 were multiple(e.g., w,e). However we only treat one single answer(e.g., w or , or e). We figured out the last 159 units in $f_\theta$ reflects 159 unique words in bAbI dataset. Therefore we didn't handle (w,e) as one answer. Is there any good way to deal with this problem? ",6,8
701,2017-7-17,2017,7,17,21,6nsntu,Matching Network implementation in Keras,https://www.reddit.com/r/MachineLearning/comments/6nsntu/matching_network_implementation_in_keras/,cnichkawde,1500293356,,1,1
702,2017-7-17,2017,7,17,21,6nspco,2017 Global and Regional ATM machine Market Research Report Forecasts 2022,https://www.reddit.com/r/MachineLearning/comments/6nspco/2017_global_and_regional_atm_machine_market/,salonikerudkar,1500293886,,0,1
703,2017-7-17,2017,7,17,21,6nsuin,[D] Handle data that has categorical variables following a power law?,https://www.reddit.com/r/MachineLearning/comments/6nsuin/d_handle_data_that_has_categorical_variables/,Icko_,1500295672,"I have a dataset that contains volumes of goods transported in Spain. There are a couple of fields, such as sender, receiver, count of the good, type of good, weight/volume of the good, package type and so on. The sender/receiver column follow a curious distribution: in e.g. 10 000 rows, the biggest sender would have 1000 packages sent, then 500, 300, 200 and so on. In total, there are maybe 1000 different senders, but the top 20 senders account for maybe 80% of the volume sent. The situation is similar for receiver and type of good (10% of the goods are refrigerators, 5% are TVs, 0.1% are matresses and so on). 

Now, in order to create a model to predict e.g. volume or weight, I'd like to convert those columns to something with less than 1000 dimensions. I could only take the first 20 or so columns, but that feels hacky. Is there a better way?",5,2
704,2017-7-17,2017,7,17,22,6nt3q3,NIPS 2017 Competition: Human-Computer Question Answering,https://www.reddit.com/r/MachineLearning/comments/6nt3q3/nips_2017_competition_humancomputer_question/,ezubaric,1500298625,[removed],0,1
705,2017-7-17,2017,7,17,23,6ntblz,Classifying gender by first name using known frequencies,https://www.reddit.com/r/MachineLearning/comments/6ntblz/classifying_gender_by_first_name_using_known/,Optimesh,1500300904,[removed],0,1
706,2017-7-17,2017,7,17,23,6ntf89,Why You Need to Start Using Embedding Layers,https://www.reddit.com/r/MachineLearning/comments/6ntf89/why_you_need_to_start_using_embedding_layers/,[deleted],1500301906,[deleted],0,1
707,2017-7-17,2017,7,17,23,6nti6d,[P] Why You Need to Start Using Embedding Layers,https://www.reddit.com/r/MachineLearning/comments/6nti6d/p_why_you_need_to_start_using_embedding_layers/,rruizen,1500302759,,5,24
708,2017-7-18,2017,7,18,0,6ntp7y,[D] Group about Reinforcement Learning + Theory,https://www.reddit.com/r/MachineLearning/comments/6ntp7y/d_group_about_reinforcement_learning_theory/,Kiuhnm,1500304686,"I've been studying *RL* for the last three months (in parallel with Convex Optimization) and I've realized that I lack some knowledge here and there. For instance, I don't know enough about *Bayesian ML*, *variational inference*, *information geometry*, *random processes*, *variance reduction*, etc...

I'm creating a small *but ambitious* discussion/study group for anything related to RL including all the theoretical stuff useful for doing research in it. I see the theory as a means to a end. My end goal is to **have an impact on the real world**, not to indulge in theoretical studies just for intellectual gratification.

I intend to devote, say, 30% of my time to general theory, and the remaining 70% to reading RL papers, implementing algorithms, doing projects, trying my luck (and skills) at RL competitions, etc...

Here are a few examples of what I mean by ""general theory"":

1. [High dimensional Probability](https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html) [(pdf link)](https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf)
2. [Bayesian ML](https://metacademy.org/roadmaps/rgrosse/bayesian_machine_learning)
3. [Differential Geometry for ML](https://metacademy.org/roadmaps/rgrosse/dgml)

I think some people confuse *understanding* with *being familiar with*. To me, understanding something means being able (eventually) to improve on it. Explanations are useless if they don't bring us closer to our goals. I could give you many wrong but plausible explanations of how a bicycle works.

---

I'll update this post and add an invitation to *discord* in a few days. Hopefully, unmotivated people will have forgot about this by then. I really hope I'm not offending anyone by doing this. I think it's in everyone's best interest.

---

## edit:

First I need to take care of a few things and then I'll add the link. You can also **PM** me (subject: *RL group*) if you want to be **PM**d back with the link.

## edit2:

**Here's the invite link:** https://discord.gg/GnWx7HK",40,69
709,2017-7-18,2017,7,18,0,6ntu7k,TWiML Talk #36 - Smart Buildings and IoT - Yodit Stanton - Industrial AI,https://www.reddit.com/r/MachineLearning/comments/6ntu7k/twiml_talk_36_smart_buildings_and_iot_yodit/,[deleted],1500305981,[deleted],0,1
710,2017-7-18,2017,7,18,0,6ntx90,[D] Was any research done on using polar coordinates with CNNs as a way to improve scale and rotational invariance?,https://www.reddit.com/r/MachineLearning/comments/6ntx90/d_was_any_research_done_on_using_polar/,leostrauss,1500306761,"As I'm learning about CNNs I understand how they achieve translational invariance but I'm stuck understanding how the CNNs in the current form (reading from a pixel matrix) can achieve practical rotational or scaling invariance. Maybe I'm missing something but the 2D convolution over the input matrix would at best help with translational invariance and maybe scaling.

Has any serious effort been expended into trying to use for example, polar coordinates rather than the pixel matrix as the input? 

I have a hunch that the nature of the polar coordinate system coupled with the current 2d convolution might provide more robust invariance for all non-deforming mutations (i.e. translation rotation and scaling).

Thoughts?",17,26
711,2017-7-18,2017,7,18,1,6nu1s0,TWiML Talk #36 - Smart Buildings and IoT - Yodit Stanton - Industrial AI,https://www.reddit.com/r/MachineLearning/comments/6nu1s0/twiml_talk_36_smart_buildings_and_iot_yodit/,[deleted],1500307915,[deleted],0,1
712,2017-7-18,2017,7,18,1,6nu33h,[R] OpenAI: Robust Adversarial Examples,https://www.reddit.com/r/MachineLearning/comments/6nu33h/r_openai_robust_adversarial_examples/,cherls,1500308234,,54,169
713,2017-7-18,2017,7,18,1,6nu4pl,"AI simplifiers for logical schemes, which ""brute-force"" over all (im)possible solutions of a given engineering problem.",https://www.reddit.com/r/MachineLearning/comments/6nu4pl/ai_simplifiers_for_logical_schemes_which/,EugeneZavidovsky,1500308636,[removed],0,1
714,2017-7-18,2017,7,18,1,6nu71c,[D] Teaching Compositionality to CNNs,https://www.reddit.com/r/MachineLearning/comments/6nu71c/d_teaching_compositionality_to_cnns/,trashacount12345,1500309224,,1,9
715,2017-7-18,2017,7,18,1,6nu7ae,[D] can anyone explain what the Dirac delta function is used for in Unsupervised Image-to-Image Translation Networks?,https://www.reddit.com/r/MachineLearning/comments/6nu7ae/d_can_anyone_explain_what_the_dirac_delta/,solololol,1500309286,"this is the paper: https://arxiv.org/abs/1703.00848
On page two they have an equation with the Dirac delta function (only time it's mentioned in paper). 
Could anyone kindly explain what that sentence is intended to mean? I understand the delta function enough, I think, I just don't understand what the sentence is there for.",4,1
716,2017-7-18,2017,7,18,1,6nuay5,ahmedbesbes.com,https://www.reddit.com/r/MachineLearning/comments/6nuay5/ahmedbesbescom/,ahmedbesbes,1500310173,,0,1
717,2017-7-18,2017,7,18,1,6nub85,"AI simplifier for logical schemes, which ""brute-force"" over all (im)possible solutions of a given engineering problem.",https://www.reddit.com/r/MachineLearning/comments/6nub85/ai_simplifier_for_logical_schemes_which/,EugeneAZ,1500310245,[removed],0,1
718,2017-7-18,2017,7,18,2,6nugr4,[R] The Peculiarities of Training Neural Networks,https://www.reddit.com/r/MachineLearning/comments/6nugr4/r_the_peculiarities_of_training_neural_networks/,dillonlaird,1500311573,,8,79
719,2017-7-18,2017,7,18,2,6nuja9,How to train a Machine Learning model in 5 minutes  Mate Labs,https://www.reddit.com/r/MachineLearning/comments/6nuja9/how_to_train_a_machine_learning_model_in_5/,curious_rv,1500312156,,0,1
720,2017-7-18,2017,7,18,2,6numnh,[N] Nvidia AI Webinar: AI Is Beating Cyber Attacks,https://www.reddit.com/r/MachineLearning/comments/6numnh/n_nvidia_ai_webinar_ai_is_beating_cyber_attacks/,sometimesacat,1500312952,,3,0
721,2017-7-18,2017,7,18,2,6numt7,The limitations of deep learning,https://www.reddit.com/r/MachineLearning/comments/6numt7/the_limitations_of_deep_learning/,toisanji,1500312990,,2,3
722,2017-7-18,2017,7,18,2,6nuo4t,Towards Data Science  Medium,https://www.reddit.com/r/MachineLearning/comments/6nuo4t/towards_data_science_medium/,mathmare,1500313313,,0,1
723,2017-7-18,2017,7,18,2,6nupas,[P] Tensorflow: What's the best way to save/restore models?,https://www.reddit.com/r/MachineLearning/comments/6nupas/p_tensorflow_whats_the_best_way_to_saverestore/,andrewjylee,1500313606,"It seems like there's multiple ways to save a model:
tf.train.saver

SavedModelBuilder (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/builder.py)

""freeze""/GraphDef (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py)

What's the best/fastest/most recommended practice?

Thanks in advance.",3,6
724,2017-7-18,2017,7,18,2,6nurph,[D] Output an unordered set with neural network,https://www.reddit.com/r/MachineLearning/comments/6nurph/d_output_an_unordered_set_with_neural_network/,pauljasek,1500314214,"I am aware that an unordered set can be input into a neural network using a sum or other order invariant operation. What would be the best way to reverse this operation? I have considered splitting the output of a fully-connected layer and using some sort of recurrent neural network. Does anyone know of a better way to do this? For my particular use case, the output is of fixed size.",6,1
725,2017-7-18,2017,7,18,3,6nuyzu,[D] Nature is littered with adversarial examples targeting real deep neural networks. Shouldn't this inspire a re-calibration of efforts in the field?,https://www.reddit.com/r/MachineLearning/comments/6nuyzu/d_nature_is_littered_with_adversarial_examples/,VinayUPrabhu,1500315960,,7,5
726,2017-7-18,2017,7,18,3,6nv3h4,[R] Transfer Learning for Sound Classification,https://www.reddit.com/r/MachineLearning/comments/6nv3h4/r_transfer_learning_for_sound_classification/,lukeinator42,1500317056,,0,1
727,2017-7-18,2017,7,18,3,6nv72e,"[D] Applying a dropout layer to classified, one hot encoded, inputs",https://www.reddit.com/r/MachineLearning/comments/6nv72e/d_applying_a_dropout_layer_to_classified_one_hot/,Altourus,1500317953,"Hey guys,

I'm having an issue with regularising my linear regression neural network. So I added dropout of .5 at each layer as has been suggested many times. It works great when applied to my hidden layers.

However when I add the dropout to my input layer I find that I now have 3-4% on my validation set where it previously was closely linked to my test set. I think this is due to the high probability that all of my input is shut off since such a large percent of my inputs are one hot encoded classes. 

Has anyone else had similar experiences? 

Would it make sense to skip dropout or use a lower probability of dropout on my input layer? ",3,0
728,2017-7-18,2017,7,18,4,6nvafo,[D] E5450 and GTX980 = Bottleneck??,https://www.reddit.com/r/MachineLearning/comments/6nvafo/d_e5450_and_gtx980_bottleneck/,thelectroom,1500318720,"Hi everyone,

I'm looking to build a Machine Learning workstation and currently have an E5450 based machine laying around (4 core CPU LGA 771 CPU). 

I was looking to get a GTX 980 for around $300CAD but was wondering, will my E5450 machine be able to use the 980 to it's fullest? 

I'm not sure if TensorFlow uses the CPU in conjunction with GPU (or just GPU alone) and if the E5450 will be powerful enough (or if it will cause a bottleneck).


Thanks!",5,3
729,2017-7-18,2017,7,18,4,6nvbof,ILSVRC2017 challenge,https://www.reddit.com/r/MachineLearning/comments/6nvbof/ilsvrc2017_challenge/,[deleted],1500319011,[deleted],0,1
730,2017-7-18,2017,7,18,4,6nvhoy,[D] Multiple object recognition in high-resolution videos,https://www.reddit.com/r/MachineLearning/comments/6nvhoy/d_multiple_object_recognition_in_highresolution/,Bubblebobo,1500320500,"I'm trying to work with a new dataset and applying convolutional neural networks to it. It's proving to be quite the challenge for me though and have some open questions I'd like to discuss.

First, let me describe the data: I have videos of surgeries that have been annotated frame-by-frame with the surgical tools visible in the frame (there may be anywhere between 0-3 tools visible in each frame). Note that only the presence but not the location of the tools is known. The videos are of 1080p resolution and the tools elongated objects of about 30 pixels width. The task is to classfiy which tools are visible in each frame.

For now I want to work on each frame separately although it would probably make sense to use the temporal information later.

*Now my questions*

**Dealing with the high resolution**

Obviously 1920x1080 images are far too large to work with. However, when scaling the images down it becomes hard to distinguish between the different tools.

One easy solution that comes to mind is a sliding-window but I don't think I can use that: Given an extracted window I would want to train a classifier on it but I don't know what or if anything is visible in that window since I don't have location labels. 

While researching publications I found works on object detection like R-CNN that use a region proposal component and then classfiy the proposed regions. However as I understand it all these publications also have location labels available. So this has the same problem as the sliding-window.

Do you know of any approaches that solve this problem?


**Designing the classification head**

Given that there can be multiple or no objects in each frame I cannot use a softmax layer in the end. I had some ideas which I will outline below *but honestly have no idea how to do this properly*. Is there a standard way to do this?

&gt; Idea A: Don't use an activation function in the last layer and simply train with vectors such as [-1, -1, 1, -1, 1] where positive values are in favor of a class and negative values against a class. Then train with mean squared error.

&gt; Idea B: Have one small ""layer"" for each class fed from the second to last layer. Each layer would have 2 units and a softmax activation function trained as usual.




*Thanks for any advice and links to relevant publications!*",7,1
731,2017-7-18,2017,7,18,4,6nviat,Strategic games for testbed,https://www.reddit.com/r/MachineLearning/comments/6nviat/strategic_games_for_testbed/,bloodymeli,1500320645,[removed],0,1
732,2017-7-18,2017,7,18,4,6nvjme,"Reproducing results for paper - ""Be Careful What You Backpropagate: A Case For Linear Output Activations &amp;amp; Gradient Boosting"".",https://www.reddit.com/r/MachineLearning/comments/6nvjme/reproducing_results_for_paper_be_careful_what_you/,imanishshah,1500320972,,1,1
733,2017-7-18,2017,7,18,5,6nvnsn,How do you guys do hyperparameter search?,https://www.reddit.com/r/MachineLearning/comments/6nvnsn/how_do_you_guys_do_hyperparameter_search/,elder_price666,1500322033,[removed],0,1
734,2017-7-18,2017,7,18,5,6nvwki,Multiclass Learning with Scikit Learn : Video and Notebook.,https://www.reddit.com/r/MachineLearning/comments/6nvwki/multiclass_learning_with_scikit_learn_video_and/,devcheikh,1500324231,,0,1
735,2017-7-18,2017,7,18,5,6nvyzp,"In 5 years, what will be the hottest applications of ML?",https://www.reddit.com/r/MachineLearning/comments/6nvyzp/in_5_years_what_will_be_the_hottest_applications/,fishfuker,1500324832,[removed],0,1
736,2017-7-18,2017,7,18,6,6nw12p,[P] Facets: An Open Source Visualization Tool for Machine Learning Training Data,https://www.reddit.com/r/MachineLearning/comments/6nw12p/p_facets_an_open_source_visualization_tool_for/,waleedka,1500325351,,11,145
737,2017-7-18,2017,7,18,6,6nw1zk,[R] [1707.03491] Creatism: A deep-learning photographer capable of creating professional work,https://www.reddit.com/r/MachineLearning/comments/6nw1zk/r_170703491_creatism_a_deeplearning_photographer/,cherls,1500325584,,4,15
738,2017-7-18,2017,7,18,6,6nw3nk,Ian Goodfellow and Alexey kurakin Quora session.,https://www.reddit.com/r/MachineLearning/comments/6nw3nk/ian_goodfellow_and_alexey_kurakin_quora_session/,Gauravvik,1500325989,[removed],0,1
739,2017-7-18,2017,7,18,6,6nwbik,Accessing individual trees in a random forest,https://www.reddit.com/r/MachineLearning/comments/6nwbik/accessing_individual_trees_in_a_random_forest/,[deleted],1500328036,[removed],0,1
740,2017-7-18,2017,7,18,6,6nwd75,Google AI 'Parsey McParseface' in its final stages towards its completion of the Turing test. ARG,https://www.reddit.com/r/MachineLearning/comments/6nwd75/google_ai_parsey_mcparseface_in_its_final_stages/,[deleted],1500328466,[deleted],0,1
741,2017-7-18,2017,7,18,7,6nwooo,Teacher forcing in LSTM [P],https://www.reddit.com/r/MachineLearning/comments/6nwooo/teacher_forcing_in_lstm_p/,lost_chilango,1500331680,"I'm going through the [Williams and Zipser (1989)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.52.9724&amp;rep=rep1&amp;type=pdf) paper and in section 2.3 they state:

&gt; An interesting technique that is frequently used in dynamical supervised learning tasks (Jordan, 1986; Pineda, 1988) is to replace the actual output *yk(t)* of a unit by the teacher signal *dk(t)* **in subsequent computation** of the behavior of the network, whenever such a value exists. We call this technique teacher forcing. (bold added)

In this paper *yk(t)* is the output of the network (predicted value) at time t per the paragraph after eq. 3, and *dk(t)* is the target at time t (so the ground truth or observed data).

I am trying to predict in a time series the t+1 observed value with the sequence *t-5,..., t*. The way my problem is set, I have a sequence of size 5 to predict the ""next"" element. The observed value at *t* would be already be the target value *dk(t)*, so how can it be that I should ""replace the actual output *yk(t)* of a unit by the teacher signal *dk(t)* **in subsequent computation**""?

Feel like I'm missing something basic but the whole idea makes no sense to me right now. I probably have all sorts of mix-up going on. Reddit plz halp.",7,0
742,2017-7-18,2017,7,18,8,6nwt3u,Results are out for ImageNet Large Scale Visual Recognition Challenge 2017,https://www.reddit.com/r/MachineLearning/comments/6nwt3u/results_are_out_for_imagenet_large_scale_visual/,[deleted],1500332926,[deleted],0,2
743,2017-7-18,2017,7,18,8,6nwuzq,[N] Results are out for ImageNet Large Scale Visual Recognition Challenge 2017 (ILSVRC2017),https://www.reddit.com/r/MachineLearning/comments/6nwuzq/n_results_are_out_for_imagenet_large_scale_visual/,modeless,1500333445,,10,33
744,2017-7-18,2017,7,18,8,6nx0mj,"If i put sensors on a dog for a long time, could a neuralnet learn to be a dog? If not why?",https://www.reddit.com/r/MachineLearning/comments/6nx0mj/if_i_put_sensors_on_a_dog_for_a_long_time_could_a/,ToplessTopmodel,1500335060,[removed],0,1
745,2017-7-18,2017,7,18,9,6nx89r,[D] Has there been research that specifically aims to train intermediary layers of a Deep neural network and use those data points?,https://www.reddit.com/r/MachineLearning/comments/6nx89r/d_has_there_been_research_that_specifically_aims/,perspectiveiskey,1500337310,"In the [Karpathy write up about the recurrent networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), specifically under the section where he visualizes the content of the RNN itself, he talks about how particular neurons are specializing in particular tasks (like counting brackets).

So the question is whether the opposite could be engineered by training lower layers of a network to do certain specific tasks?

This would be almost like feature engineering, but embedding it 1 or 2 layers into the deep network.

Does this have a name? Any links to papers? Comments?

Thanks.",2,1
746,2017-7-18,2017,7,18,10,6nxhb1,[R] The Reversible Residual Network: Backpropagation Without Storing Activations,https://www.reddit.com/r/MachineLearning/comments/6nxhb1/r_the_reversible_residual_network_backpropagation/,xternalz,1500340013,,21,47
747,2017-7-18,2017,7,18,10,6nxoc8,Certified Analytics Professional? [D],https://www.reddit.com/r/MachineLearning/comments/6nxoc8/certified_analytics_professional_d/,pinkerton_jones,1500342180,[removed],0,0
748,2017-7-18,2017,7,18,10,6nxp4w,Sentence Classification,https://www.reddit.com/r/MachineLearning/comments/6nxp4w/sentence_classification/,DaveYitr,1500342406,[removed],0,1
749,2017-7-18,2017,7,18,11,6nxro1,Adversarial examples on binary classifiers,https://www.reddit.com/r/MachineLearning/comments/6nxro1/adversarial_examples_on_binary_classifiers/,torvoraptor,1500343209,[removed],0,1
750,2017-7-18,2017,7,18,11,6nxzvy,Top 16 AI/ML Books,https://www.reddit.com/r/MachineLearning/comments/6nxzvy/top_16_aiml_books/,kjahan,1500345832,,0,1
751,2017-7-18,2017,7,18,12,6nya9a,excellent primer,https://www.reddit.com/r/MachineLearning/comments/6nya9a/excellent_primer/,semidemiurge,1500349289,,0,1
752,2017-7-18,2017,7,18,12,6nyc99,Looking for pretrained speech recognition models (with language models),https://www.reddit.com/r/MachineLearning/comments/6nyc99/looking_for_pretrained_speech_recognition_models/,[deleted],1500349945,[removed],0,1
753,2017-7-18,2017,7,18,12,6nyd3b,Sampling based loss for RNNs,https://www.reddit.com/r/MachineLearning/comments/6nyd3b/sampling_based_loss_for_rnns/,seleucia,1500350235,[removed],0,1
754,2017-7-18,2017,7,18,13,6nyhj6,Sampling based loss for RNN,https://www.reddit.com/r/MachineLearning/comments/6nyhj6/sampling_based_loss_for_rnn/,seleucia,1500351775,[removed],0,1
755,2017-7-18,2017,7,18,13,6nynb0,[D] 2017 International Summer School on Deep Learning Materials,https://www.reddit.com/r/MachineLearning/comments/6nynb0/d_2017_international_summer_school_on_deep/,RefurbishedMac,1500353896,,0,1
756,2017-7-18,2017,7,18,13,6nyni4,[D] How do you do backprop by hand?,https://www.reddit.com/r/MachineLearning/comments/6nyni4/d_how_do_you_do_backprop_by_hand/,[deleted],1500353968,[removed],0,1
757,2017-7-18,2017,7,18,14,6nywgq,[P] Caffe Implementation of Deep Pyramidal Residual Networks,https://www.reddit.com/r/MachineLearning/comments/6nywgq/p_caffe_implementation_of_deep_pyramidal_residual/,jwkim89,1500357449,,1,2
758,2017-7-18,2017,7,18,15,6nyz1v,Research labs for AI and ML,https://www.reddit.com/r/MachineLearning/comments/6nyz1v/research_labs_for_ai_and_ml/,goko19,1500358535,[removed],0,1
759,2017-7-18,2017,7,18,15,6nz0tq,My ML journey,https://www.reddit.com/r/MachineLearning/comments/6nz0tq/my_ml_journey/,[deleted],1500359286,[removed],0,1
760,2017-7-18,2017,7,18,16,6nz7ab,Gaskets and Seals,https://www.reddit.com/r/MachineLearning/comments/6nz7ab/gaskets_and_seals/,anericanseal,1500362147,[removed],0,1
761,2017-7-18,2017,7,18,16,6nza49,[P] Looking for pretrained speech recognition models,https://www.reddit.com/r/MachineLearning/comments/6nza49/p_looking_for_pretrained_speech_recognition_models/,boyentenbi,1500363437,"Hi all, I'm trying to transcribe phone calls. I have a fair amount of experience with deep models, but I'd love some help with identifying:

1. Pretained speech recognition models that can be tuned for phone calls
2. Datasets of transcribed phone calls

[DeepSpeech](https://github.com/mozilla/DeepSpeech) looks promising as an untrained model. Has anyone tried training it?
[This dataset](https://catalog.ldc.upenn.edu/LDC2004T19) sounds useful, but the sample transcription has pretty poor wording because the dataset is transcribed by word rather than by sentence.

Any recommendations are greatly appreciated - cheers.",3,4
762,2017-7-18,2017,7,18,16,6nza7f,Machine Learning Crash Course: Part 4 - The Bias-Variance Dilemma,https://www.reddit.com/r/MachineLearning/comments/6nza7f/machine_learning_crash_course_part_4_the/,josephd,1500363479,,0,1
763,2017-7-18,2017,7,18,16,6nzc8c,O-Ring,https://www.reddit.com/r/MachineLearning/comments/6nzc8c/oring/,anericanseal,1500364411,,0,1
764,2017-7-18,2017,7,18,17,6nzf99,Shaft Seals,https://www.reddit.com/r/MachineLearning/comments/6nzf99/shaft_seals/,anericanseal,1500365854,,0,1
765,2017-7-18,2017,7,18,18,6nzl9b,[P] Project Common Voice by Mozilla: Speech data to be released later this year,https://www.reddit.com/r/MachineLearning/comments/6nzl9b/p_project_common_voice_by_mozilla_speech_data_to/,breandan,1500368692,,13,58
766,2017-7-18,2017,7,18,18,6nzo6h,Graphite Packing,https://www.reddit.com/r/MachineLearning/comments/6nzo6h/graphite_packing/,anericanseal,1500370063,,0,1
767,2017-7-18,2017,7,18,18,6nzoog,"[P] Suggestions for supervised learning in ""similar documents""-tool?",https://www.reddit.com/r/MachineLearning/comments/6nzoog/p_suggestions_for_supervised_learning_in_similar/,johananasen,1500370316,"I'm working on a data set containing text documents and my goal is to make a ""find similar document"" tool and a clustering. Currently I'm using a tf-idf representation with TfidfVectorizer.fit_transform() in scikit learn and then running Kmeans and calculating cosine distances. 

For my next iteration I want to do supervised learning. I have some user data on how closely related each document is to all other documents, and I want to use it to train the model. My question is if anyone here has used scikit learn in that manner, and if they would recommend some specific approach.

I'm considering doing a similarity learning system, training the system to learn a distance function or metric that resembles the user data. A more intuitive approach to me would be to supply the user data to the dimension reduction, to make a low dimensional representation where the distances match the supplied user data. But I haven't yet found a good way to do this. 

I hope I'm not asking to much, I have been doing a lot of research and I'm still working on it but any pointers would be greatly appreciated :)

",9,1
768,2017-7-18,2017,7,18,20,6o03q8,"Yandex open sources CatBoost, a gradient boosting machine learning library",https://www.reddit.com/r/MachineLearning/comments/6o03q8/yandex_open_sources_catboost_a_gradient_boosting/,s0ulmate,1500376483,,0,6
769,2017-7-18,2017,7,18,20,6o068p,[R] General Aspects in Selecting Best Variables,https://www.reddit.com/r/MachineLearning/comments/6o068p/r_general_aspects_in_selecting_best_variables/,molode,1500377501,,0,1
770,2017-7-18,2017,7,18,20,6o06ks,CatBoost: gradient boosting on decision trees library with categorical features support by Yandex,https://www.reddit.com/r/MachineLearning/comments/6o06ks/catboost_gradient_boosting_on_decision_trees/,netcribe,1500377640,,0,1
771,2017-7-18,2017,7,18,20,6o0aoe,[R] Understanding Support Vector Machine via Examples,https://www.reddit.com/r/MachineLearning/comments/6o0aoe/r_understanding_support_vector_machine_via/,friscotime,1500379187,,0,1
772,2017-7-18,2017,7,18,21,6o0b4y,Introduction to OpenAI gym part 2: building a deep q-network - Pinch of Intelligence,https://www.reddit.com/r/MachineLearning/comments/6o0b4y/introduction_to_openai_gym_part_2_building_a_deep/,pmz,1500379339,,0,2
773,2017-7-18,2017,7,18,21,6o0cb2,[R] Data Science: Performance of Python vs Pandas vs Numpy,https://www.reddit.com/r/MachineLearning/comments/6o0cb2/r_data_science_performance_of_python_vs_pandas_vs/,janemoz,1500379728,,0,1
774,2017-7-18,2017,7,18,21,6o0dpg,[R] Piecewise regression: when one line simply isnt enough,https://www.reddit.com/r/MachineLearning/comments/6o0dpg/r_piecewise_regression_when_one_line_simply_isnt/,digitalson,1500380189,,0,1
775,2017-7-18,2017,7,18,21,6o0glo,[R] Machine Learning: Pruning Decision Trees,https://www.reddit.com/r/MachineLearning/comments/6o0glo/r_machine_learning_pruning_decision_trees/,jackblun,1500381155,,0,1
776,2017-7-18,2017,7,18,21,6o0j67,[R] Smile in the face of adversity much? A print based spoofing attack (CVPR workshop extended abstract) -TLDR: Not such a good idea to 'enroll' with smiling images.,https://www.reddit.com/r/MachineLearning/comments/6o0j67/r_smile_in_the_face_of_adversity_much_a_print/,VinayUPrabhu,1500382007,,1,5
777,2017-7-18,2017,7,18,21,6o0keb,Automated Machine Learning Competition: Can AutoML beat humans on Kaggle?,https://www.reddit.com/r/MachineLearning/comments/6o0keb/automated_machine_learning_competition_can_automl/,rhiever,1500382446,,0,1
778,2017-7-18,2017,7,18,23,6o161o,[D] What is a possible way to implement Amazon Go?,https://www.reddit.com/r/MachineLearning/comments/6o161o/d_what_is_a_possible_way_to_implement_amazon_go/,kokobannana,1500388841,I am wondering how Amazon implemented Amazon Go and what it takes to build such thing. ,4,0
779,2017-7-19,2017,7,19,0,6o1bok,Using Google's Quickdraw to create an MNIST style dataset!,https://www.reddit.com/r/MachineLearning/comments/6o1bok/using_googles_quickdraw_to_create_an_mnist_style/,[deleted],1500390335,[deleted],0,1
780,2017-7-19,2017,7,19,0,6o1c2s,[P] Polyaxon a deep learning and reinforcement learning library,https://www.reddit.com/r/MachineLearning/comments/6o1c2s/p_polyaxon_a_deep_learning_and_reinforcement/,[deleted],1500390440,[deleted],0,1
781,2017-7-19,2017,7,19,0,6o1hdt,AI Co-Pilot: RNNs for Dynamic Facial Analysis,https://www.reddit.com/r/MachineLearning/comments/6o1hdt/ai_copilot_rnns_for_dynamic_facial_analysis/,harrism,1500391779,,0,1
782,2017-7-19,2017,7,19,1,6o1r16,[R] Trial without Error: Towards Safe Reinforcement Learning via Human Intervention,https://www.reddit.com/r/MachineLearning/comments/6o1r16/r_trial_without_error_towards_safe_reinforcement/,petermcintyre,1500394161,,1,16
783,2017-7-19,2017,7,19,1,6o1vqr,[D] The future of deep learning,https://www.reddit.com/r/MachineLearning/comments/6o1vqr/d_the_future_of_deep_learning/,galapag0,1500395335,,36,83
784,2017-7-19,2017,7,19,1,6o1vr8,[N] CatBoost - gradient boosting library from Yandex,https://www.reddit.com/r/MachineLearning/comments/6o1vr8/n_catboost_gradient_boosting_library_from_yandex/,smart_neuron,1500395338,,21,135
785,2017-7-19,2017,7,19,1,6o1w8u,[D]How can CEC prevent Lstm from the problem of vanishing gradients ?,https://www.reddit.com/r/MachineLearning/comments/6o1w8u/dhow_can_cec_prevent_lstm_from_the_problem_of/,finallyifoundvalidUN,1500395460,"After reading the original LSTM paper I'm a bit confused about the constant error carousels

How can it prevent Lstm from the problem of vanishing gradients ?

It's meant to enforce a constant error flow . It's a neuron with a connection to itself with a weight of one . I didn't get how it helps with the error signal to not vanish when it propagate back ",5,5
786,2017-7-19,2017,7,19,1,6o1ybj,[P] Using Google's Quickdraw to create an MNIST style dataset!,https://www.reddit.com/r/MachineLearning/comments/6o1ybj/p_using_googles_quickdraw_to_create_an_mnist/,rshah4,1500395982,,0,13
787,2017-7-19,2017,7,19,1,6o20al,[R] Learning to Learn,https://www.reddit.com/r/MachineLearning/comments/6o20al/r_learning_to_learn/,gdny,1500396474,,19,60
788,2017-7-19,2017,7,19,2,6o27ap,[D] What are some interesting problems in which machine learning can assist medicine?,https://www.reddit.com/r/MachineLearning/comments/6o27ap/d_what_are_some_interesting_problems_in_which/,theMushroomCloud1,1500398144,"I've read about cancer detection using CV, but I'm sure there are more places where ML is helping! 

Have any of you worked in areas that combine ML and medicine, or have read papers that do that?",7,0
789,2017-7-19,2017,7,19,2,6o2hy9,Alex Smola - Personalization and Scalable Deep Learning with MXNET - MLconf SF 2016,https://www.reddit.com/r/MachineLearning/comments/6o2hy9/alex_smola_personalization_and_scalable_deep/,mlconf,1500400767,,1,2
790,2017-7-19,2017,7,19,3,6o2iuu,Amidst - A Java Toolbox for Scalable Probabilistic Machine Learning,https://www.reddit.com/r/MachineLearning/comments/6o2iuu/amidst_a_java_toolbox_for_scalable_probabilistic/,breandan,1500400979,,0,1
791,2017-7-19,2017,7,19,3,6o2js2,pomegranate @ scipy2017: fast and flexible probabilistic modeling in python,https://www.reddit.com/r/MachineLearning/comments/6o2js2/pomegranate_scipy2017_fast_and_flexible/,[deleted],1500401194,[deleted],0,1
792,2017-7-19,2017,7,19,3,6o2knl,Veritone.com - How do you create an AI firm?,https://www.reddit.com/r/MachineLearning/comments/6o2knl/veritonecom_how_do_you_create_an_ai_firm/,lovebes,1500401405,[removed],0,1
793,2017-7-19,2017,7,19,3,6o2mv0,[p] Prediction web api for sklearn models,https://www.reddit.com/r/MachineLearning/comments/6o2mv0/p_prediction_web_api_for_sklearn_models/,pomber,1500401940,,3,1
794,2017-7-19,2017,7,19,3,6o2qpk,Harness the power of AWS to train your ML models in Jupyter Notebooks,https://www.reddit.com/r/MachineLearning/comments/6o2qpk/harness_the_power_of_aws_to_train_your_ml_models/,tsz2001,1500402862,,0,1
795,2017-7-19,2017,7,19,4,6o2xax,[P] A simple example of a graphical confusion matrix using Facets Dive,https://www.reddit.com/r/MachineLearning/comments/6o2xax/p_a_simple_example_of_a_graphical_confusion/,MLApprentice,1500404503,,1,3
796,2017-7-19,2017,7,19,4,6o331t,[P] pomegranate @ scipy2017: fast and flexible probabilistic modeling in python,https://www.reddit.com/r/MachineLearning/comments/6o331t/p_pomegranate_scipy2017_fast_and_flexible/,ants_rock,1500405921,,12,15
797,2017-7-19,2017,7,19,5,6o3l98,[D] One Shot Learning Approach for Image Generation?,https://www.reddit.com/r/MachineLearning/comments/6o3l98/d_one_shot_learning_approach_for_image_generation/,quickhook,1500410474,I am curious what is the current state of research on introducing one shot learning into image generation tasks? Any work done to marry one shot learning with GAN? ,3,4
798,2017-7-19,2017,7,19,5,6o3or3,Require fMRI datasets,https://www.reddit.com/r/MachineLearning/comments/6o3or3/require_fmri_datasets/,joycode,1500411358,[removed],0,1
799,2017-7-19,2017,7,19,7,6o4bwi,Roof Painter and Restoration Services,https://www.reddit.com/r/MachineLearning/comments/6o4bwi/roof_painter_and_restoration_services/,gwenyosef,1500417499,,0,1
800,2017-7-19,2017,7,19,7,6o4f4r,"[P] Noise in Design Using machine Learning, style transfer, and cellular automaton to explore the role of noise in design.",https://www.reddit.com/r/MachineLearning/comments/6o4f4r/p_noise_in_design_using_machine_learning_style/,psoulos,1500418438,,2,5
801,2017-7-19,2017,7,19,11,6o5pj8,Can anyone help me find this paper?,https://www.reddit.com/r/MachineLearning/comments/6o5pj8/can_anyone_help_me_find_this_paper/,[deleted],1500432614,[removed],0,1
802,2017-7-19,2017,7,19,12,6o5ume,[R] Google trains network on 300 million (!) images,https://www.reddit.com/r/MachineLearning/comments/6o5ume/r_google_trains_network_on_300_million_images/,Reiinakano,1500434261,,90,231
803,2017-7-19,2017,7,19,12,6o5yln,Installing And Using GIZA++ in Ubuntu for Word Alignment,https://www.reddit.com/r/MachineLearning/comments/6o5yln/installing_and_using_giza_in_ubuntu_for_word/,techie-knowledge,1500435631,,0,1
804,2017-7-19,2017,7,19,13,6o63z4,Facing The Rotor 120T Through The Lathe Machine,https://www.reddit.com/r/MachineLearning/comments/6o63z4/facing_the_rotor_120t_through_the_lathe_machine/,sanjaykumawat,1500437507,,0,1
805,2017-7-19,2017,7,19,14,6o6cr8,"[D] How does one calculate the receptive field of a (conv, relu, pool) CNN?",https://www.reddit.com/r/MachineLearning/comments/6o6cr8/d_how_does_one_calculate_the_receptive_field_of_a/,thisBeAFakeThrowaway,1500440694,"I've tried to look this up on the interwebz. While it seems like a common query, I did not see any authoritative posts on the subject. I am interested in figuring out how this works  for traditional CNNs.",6,3
806,2017-7-19,2017,7,19,14,6o6i30,[D] Katharine Jarmul | Keynote: Ethical Machine Learning: Creating Fair Models in an Unjust World,https://www.reddit.com/r/MachineLearning/comments/6o6i30/d_katharine_jarmul_keynote_ethical_machine/,durand101,1500442739,,8,12
807,2017-7-19,2017,7,19,15,6o6tx1,[D] Neuroevolution: A different kind of deep learning,https://www.reddit.com/r/MachineLearning/comments/6o6tx1/d_neuroevolution_a_different_kind_of_deep_learning/,hardmaru,1500447593,,24,58
808,2017-7-19,2017,7,19,16,6o6y3h,Do you know what is granular hot melt adhesive production equipment?,https://www.reddit.com/r/MachineLearning/comments/6o6y3h/do_you_know_what_is_granular_hot_melt_adhesive/,mixmachinery,1500449348,,0,1
809,2017-7-19,2017,7,19,16,6o714m,Darwin finnaly proven wrong by AI (Eberhard Schoeneburg),https://www.reddit.com/r/MachineLearning/comments/6o714m/darwin_finnaly_proven_wrong_by_ai_eberhard/,YnternetXplorer,1500450779,,0,1
810,2017-7-19,2017,7,19,16,6o71we,[R] BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/6o71we/r_branchynet_fast_inference_via_early_exiting/,xternalz,1500451145,"&gt;Deep neural networks are state of the art methods for many learning tasks due to their ability to extract increasingly better features at each network layer. However, the improved performance of additional layers in a deep network comes at the cost of added latency and energy usage in feedforward inference. As networks continue to get deeper and larger, these costs become more prohibitive for real-time and energy-sensitive applications. To address this issue, we present BranchyNet, a novel deep network architecture that is augmented with side branches. The architecture allows prediction results for a large portion of test samples to exit the network early via these branches when samples can already be inferred with high confidence. BranchyNet exploits the observation that features learned at an early layer of a network may often be sufficient for the classification of many data points. For more difficult samples, which are expected to be infrequent, BranchyNet will use further or all network layers to provide the best likelihood of correct prediction. We study the BranchyNet architecture using several well-known networks (LeNet, AlexNet, ResNet) and datasets (MNIST, CIFAR10) and show that it can both improve accuracy and significantly reduce the inference time of the network.

paper: http://www.eecs.harvard.edu/~htk/publication/2016-icpr-teerapittayanon-mcdanel-kung.pdf

code: https://github.com/kunglab/branchynet",4,22
811,2017-7-19,2017,7,19,17,6o73yb,Dropout Rademacher Complexity of Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/6o73yb/dropout_rademacher_complexity_of_deep_neural/,darkingwing,1500452096,,0,1
812,2017-7-19,2017,7,19,17,6o74ar,[P] A Tensorflow deep learning and reinforcement learning library,https://www.reddit.com/r/MachineLearning/comments/6o74ar/p_a_tensorflow_deep_learning_and_reinforcement/,pipado,1500452252,,1,12
813,2017-7-19,2017,7,19,17,6o78rd,Paint Finishes and Painting Techniques,https://www.reddit.com/r/MachineLearning/comments/6o78rd/paint_finishes_and_painting_techniques/,gwenyosef,1500454381,,0,1
814,2017-7-19,2017,7,19,18,6o7bda,"Marshalls Cosmetic Machines - Tattoo Removal Laser Machines, Fat Freeze and IPL Machines.",https://www.reddit.com/r/MachineLearning/comments/6o7bda/marshalls_cosmetic_machines_tattoo_removal_laser/,berryjon,1500455561,,0,1
815,2017-7-19,2017,7,19,18,6o7e5k,My In M Vch Godex  G530,https://www.reddit.com/r/MachineLearning/comments/6o7e5k/my_in_m_vch_godex_g530/,maytinhtiencasino,1500456844,,0,1
816,2017-7-19,2017,7,19,18,6o7e8d,"Simple idea, looking for papers if that is something people have already tried",https://www.reddit.com/r/MachineLearning/comments/6o7e8d/simple_idea_looking_for_papers_if_that_is/,datatatatata,1500456881,[removed],0,1
817,2017-7-19,2017,7,19,18,6o7f7v,GLY250-1200 hollow core slab machine,https://www.reddit.com/r/MachineLearning/comments/6o7f7v/gly2501200_hollow_core_slab_machine/,ada2017,1500457340,,0,1
818,2017-7-19,2017,7,19,18,6o7hjz,1200mm width hollow core slab,https://www.reddit.com/r/MachineLearning/comments/6o7hjz/1200mm_width_hollow_core_slab/,ada2017,1500458361,,0,1
819,2017-7-19,2017,7,19,19,6o7jss,Automatic Composite Plastic Tube Filling and Sealing Machine Chinese Man...,https://www.reddit.com/r/MachineLearning/comments/6o7jss/automatic_composite_plastic_tube_filling_and/,hymachinery,1500459282,,0,1
820,2017-7-19,2017,7,19,20,6o7ugj,Predicting customer satisfaction from chat sessions using IBMWatson sentiment analysis,https://www.reddit.com/r/MachineLearning/comments/6o7ugj/predicting_customer_satisfaction_from_chat/,[deleted],1500463676,[deleted],0,1
821,2017-7-19,2017,7,19,20,6o7v7n,High Speed Metal Stud and Track C U M Omega Profile Roll Forming Machine,https://www.reddit.com/r/MachineLearning/comments/6o7v7n/high_speed_metal_stud_and_track_c_u_m_omega/,M-RollFormingMachine,1500463973,,0,1
822,2017-7-19,2017,7,19,20,6o7xw4,[Discussion] How to find max value choosing from multiple categories?,https://www.reddit.com/r/MachineLearning/comments/6o7xw4/discussion_how_to_find_max_value_choosing_from/,erikhhhh,1500464996,"Let's say I have a database of restaurant foods: 200 starters, 200 mains and 200 desserts. All foods have price and value fields.

I want to select 2 starters, 2 mains and 2 desserts for the best value for a fixed price of $100.

When selecting top value foods, then it will cost a lot more than $100.

What technique should I use to tackle this problem?",6,1
823,2017-7-19,2017,7,19,20,6o7yll,C purlin steel sheet frame Roll Forming Machine,https://www.reddit.com/r/MachineLearning/comments/6o7yll/c_purlin_steel_sheet_frame_roll_forming_machine/,M-RollFormingMachine,1500465268,,1,1
824,2017-7-19,2017,7,19,21,6o80nr,Challenges in Deep Learning,https://www.reddit.com/r/MachineLearning/comments/6o80nr/challenges_in_deep_learning/,gargisharmapd,1500466011,,0,1
825,2017-7-19,2017,7,19,21,6o83qb,"Every time I can't figure out my code, I talk to a rubber duck.",https://www.reddit.com/r/MachineLearning/comments/6o83qb/every_time_i_cant_figure_out_my_code_i_talk_to_a/,tfburns,1500467032,,0,1
826,2017-7-19,2017,7,19,21,6o87xd,[D] Help in implementing code in published papers.,https://www.reddit.com/r/MachineLearning/comments/6o87xd/d_help_in_implementing_code_in_published_papers/,edutainment123,1500468418,"I often see people on Github trying out algorithms in relevant papers (arxiv mostly, but doesn't matter). 

I am 5 weeks into Andrew Ng's Coursera on Machine Learning and also currently reading on the same topics discussed in the course.
Also, I have explored Keras a bit using some online tutorials and bits all over the internet with intentions of diving into TensorFlow.

I would like to get my hands dirty on some implementations mentioned in published papers - matching or maybe enhancing hyperparameters to improve the implementation or just imitating for the sake of understanding.

**TL;DR**
Could you suggest a list of papers I could break the ice with and get my hands dirty by implementing therein(paper) mentioned algorithms and architectures?
Could be anything from Logistic Regression, SVM to Deep Learning algorithms (CNN, RNN etc.) **but** for a *beginner* like me. ",6,1
827,2017-7-19,2017,7,19,21,6o893n,Indian Startups Relying on Artificial Intelligence to Know Their Customers Better,https://www.reddit.com/r/MachineLearning/comments/6o893n/indian_startups_relying_on_artificial/,dexlabanalytics,1500468781,,0,1
828,2017-7-19,2017,7,19,22,6o8dqi,What are the different types of Mechanical Seals?,https://www.reddit.com/r/MachineLearning/comments/6o8dqi/what_are_the_different_types_of_mechanical_seals/,LeakPack,1500470161,,0,1
829,2017-7-19,2017,7,19,22,6o8evh,[R][1707.05589] On the State of the Art of Evaluation in Neural Language Models,https://www.reddit.com/r/MachineLearning/comments/6o8evh/r170705589_on_the_state_of_the_art_of_evaluation/,tsendsuren,1500470510,,18,37
830,2017-7-19,2017,7,19,22,6o8fep,Apple Machine Learning Journal,https://www.reddit.com/r/MachineLearning/comments/6o8fep/apple_machine_learning_journal/,[deleted],1500470668,[deleted],0,1
831,2017-7-19,2017,7,19,22,6o8fy1,[D] Predicting customer satisfaction from chat sessions using IBMWatson sentiment analysis,https://www.reddit.com/r/MachineLearning/comments/6o8fy1/d_predicting_customer_satisfaction_from_chat/,AmirPupko,1500470822,,0,17
832,2017-7-19,2017,7,19,22,6o8ia1,I would like to look at some code to understand the training errors and view the SVM with and without errors.,https://www.reddit.com/r/MachineLearning/comments/6o8ia1/i_would_like_to_look_at_some_code_to_understand/,[deleted],1500471515,[deleted],0,1
833,2017-7-19,2017,7,19,23,6o8ng2,[R] Improving the Realism of Synthetic Images,https://www.reddit.com/r/MachineLearning/comments/6o8ng2/r_improving_the_realism_of_synthetic_images/,cyberjeet,1500472990,,0,13
834,2017-7-19,2017,7,19,23,6o8nh2,I would like to look at code to understand a sample SVM with and without errors. Any book or paper explains this ?,https://www.reddit.com/r/MachineLearning/comments/6o8nh2/i_would_like_to_look_at_code_to_understand_a/,[deleted],1500472999,[deleted],0,1
835,2017-7-19,2017,7,19,23,6o8s8o,Sacred How I Learned to Stop Worrying and Love the Research | SciPy 2017 | Klaus Greff,https://www.reddit.com/r/MachineLearning/comments/6o8s8o/sacred_how_i_learned_to_stop_worrying_and_love/,Sigiward,1500474278,,0,1
836,2017-7-19,2017,7,19,23,6o8sim,Any code for a sample SVM with and without errors ? Any paper or book with a practical example ?,https://www.reddit.com/r/MachineLearning/comments/6o8sim/any_code_for_a_sample_svm_with_and_without_errors/,[deleted],1500474352,[deleted],0,1
837,2017-7-19,2017,7,19,23,6o8yxw,[N] Apple Machine Learning Journal,https://www.reddit.com/r/MachineLearning/comments/6o8yxw/n_apple_machine_learning_journal/,figurelover,1500476033,,1,7
838,2017-7-20,2017,7,20,0,6o93wn,[D] Apple Machine Learning Journal - What Does This Mean?,https://www.reddit.com/r/MachineLearning/comments/6o93wn/d_apple_machine_learning_journal_what_does_this/,onegazillion,1500477273,,21,32
839,2017-7-20,2017,7,20,0,6o96vk,Predicting hundreds of multilabel classes from a highly imbalanced dataset?,https://www.reddit.com/r/MachineLearning/comments/6o96vk/predicting_hundreds_of_multilabel_classes_from_a/,youngChange,1500477990,[removed],0,1
840,2017-7-20,2017,7,20,0,6o99d9,Thoughts on new Python Data Science Package - SciBlox,https://www.reddit.com/r/MachineLearning/comments/6o99d9/thoughts_on_new_python_data_science_package/,[deleted],1500478619,[removed],0,1
841,2017-7-20,2017,7,20,0,6o99iq,[P] Keras/Theano Binary Classification of unbalanced dataset,https://www.reddit.com/r/MachineLearning/comments/6o99iq/p_kerastheano_binary_classification_of_unbalanced/,Zman420,1500478661,"Hi all.

I'm making the transition from my usual SVM/Matlab background to keras/theano in python.

I love the library so far, it's got great documentation and very intuitive to write code for.  However, I've ran into a problem that I'd appreciate some help on.

I'm trying to do a binary classification on an unbalanced dataset (negative examples outnumber positives about 6:1).  To combat the imbalance, I simply resampled my data to remove some negatives - giving a 1:1 ratio.  The training data has about 6000 examples, with ~850 features.

I train a simple NN, using the automated validation split, to achieve a validation accuracy of ~83%, which I would be happy with.  I should note that previous SVM results on this dataset were in the 


    model = Sequential()
    model.add(Dense(15, input_dim=856, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.fit(X_train, Y_train, epochs=10, batch_size=100, verbose=1, validation_split=0.3) 

I then test the network using model.evaluate(), or model.predict(), on a completely different (blind) test set, that is also balanced, and I seem to once again achieve ~80% accuracy. Great!

However - when I test the model with an UNbalanced test set (again, with way more negatives than positives), I get lots of false positives, pushing the accuracy down to ~35%!

    scores = model.evaluate(X_test, Y_test)
    print(""\n%s: %.2f%%"" % (model.metrics_names[1], scores[1]*100))


Output for training and then testing on unbalanced dataset:

    Using Theano backend.
    Train on 4195 samples, validate on 1798 samples
    Epoch 1/10
    4195/4195 [==============================] - 0s - loss: 1.8133 - acc: 0.7418 - val_loss: 
    0.8600 - val_acc: 0.8115
    Epoch 2/10
    4195/4195 [==============================] - 0s - loss: 0.6449 - acc: 0.8155 - val_loss: 
    0.6064 - val_acc: 0.8081
    Epoch 3/10
    4195/4195 [==============================] - 0s - loss: 0.4859 - acc: 0.8203 - val_loss: 
    0.4859 - val_acc: 0.8070
    Epoch 4/10
    4195/4195 [==============================] - 0s - loss: 0.4171 - acc: 0.8329 - val_loss: 
    0.4511 - val_acc: 0.8220
    Epoch 5/10
    4195/4195 [==============================] - 0s - loss: 0.3844 - acc: 0.8350 - val_loss: 
    0.4453 - val_acc: 0.8293
    Epoch 6/10
    4195/4195 [==============================] - 0s - loss: 0.3844 - acc: 0.8350 - val_loss: 
    0.5762 - val_acc: 0.8387
    Epoch 7/10
    4195/4195 [==============================] - 0s - loss: 0.3837 - acc: 0.8412 - val_loss: 
    0.5782 - val_acc: 0.8415
    Epoch 8/10
    4195/4195 [==============================] - 0s - loss: 0.3756 - acc: 0.8362 - val_loss: 
    0.5101 - val_acc: 0.8393
    Epoch 9/10
    4195/4195 [==============================] - 0s - loss: 0.3527 - acc: 0.8510 - val_loss: 
    0.4112 - val_acc: 0.8209
    Epoch 10/10
    4195/4195 [==============================] - 0s - loss: 0.3614 - acc: 0.8513 - val_loss: 
    0.4198 - val_acc: 0.8365
    
    Testing on new data...
    32/5880 [..............................] - ETA: 0s
    acc: 34.30%

I don't quite understand why this is happening, but suspect it could be to do with the way neural networks pass the data through the network in batches?  Like I said - I've mostly used SVMs in the past, so this phenomenon is quite strange to me.

I'm also not sure why testing data display shows 32/5880 when finishing...would expect it to display 5880/5880 when complete.

Happy to provide full code/data.
",15,4
842,2017-7-20,2017,7,20,0,6o9cpf,[P] Thoughts on Sciblox - new Python Data Science package,https://www.reddit.com/r/MachineLearning/comments/6o9cpf/p_thoughts_on_sciblox_new_python_data_science/,danielhanchen,1500479428,"Hey all. I'm Daniel, and I just started a new project called Sciblox. It's an all-in-one data science package for Python3 optimised to be used in Jupyter Notebooks. Check it out on https://github.com/danielhanchen/sciblox, and the webpage documentation (just Jupyter Notebook) https://danielhanchen.github.io/

Some features include:

1. MICE, BPCA, Forests, Boosting missing data imputation
2. Auto graph selection
3. Sequential word mining methods
4. And lots more!

I was just wondering if you guys think it's going to be a worthwhile project to keep pursuing. It's my first repo, and so I want to hear any of your thoughts!",8,16
843,2017-7-20,2017,7,20,0,6o9dyg,"Simple Questions Thread July 19, 2017",https://www.reddit.com/r/MachineLearning/comments/6o9dyg/simple_questions_thread_july_19_2017/,AutoModerator,1500479729,[removed],0,1
844,2017-7-20,2017,7,20,0,6o9epm,[R] Challenges in Deep Learning,https://www.reddit.com/r/MachineLearning/comments/6o9epm/r_challenges_in_deep_learning/,janemoz,1500479925,,0,1
845,2017-7-20,2017,7,20,1,6o9gct,"[R] The World, Drawn with Travel Itineraries",https://www.reddit.com/r/MachineLearning/comments/6o9gct/r_the_world_drawn_with_travel_itineraries/,digitalson,1500480302,,0,1
846,2017-7-20,2017,7,20,1,6o9lqs,Inside Facebook's A.I. Workshop,https://www.reddit.com/r/MachineLearning/comments/6o9lqs/inside_facebooks_ai_workshop/,Rob,1500481581,,0,1
847,2017-7-20,2017,7,20,1,6o9mai,"[R] Introducing Yandex CatBoost, a state-of-the-art open-source gradient boosting library",https://www.reddit.com/r/MachineLearning/comments/6o9mai/r_introducing_yandex_catboost_a_stateoftheart/,jackblun,1500481708,,0,1
848,2017-7-20,2017,7,20,2,6o9vm2,Stability of Ryzen PC build for Python-based ML,https://www.reddit.com/r/MachineLearning/comments/6o9vm2/stability_of_ryzen_pc_build_for_pythonbased_ml/,insolent_beachball,1500483985,[removed],0,1
849,2017-7-20,2017,7,20,2,6oa11z,[N] iNaturalist: a smartphone CNN app for identifying photos of 13k species of wild animals/insects/plants,https://www.reddit.com/r/MachineLearning/comments/6oa11z/n_inaturalist_a_smartphone_cnn_app_for/,gwern,1500485358,,14,78
850,2017-7-20,2017,7,20,2,6oa8jg,Scatteract: Automated extraction of data from scatter plots,https://www.reddit.com/r/MachineLearning/comments/6oa8jg/scatteract_automated_extraction_of_data_from/,[deleted],1500487149,[deleted],0,1
851,2017-7-20,2017,7,20,3,6oaegs,A challenge - think you can solve it?,https://www.reddit.com/r/MachineLearning/comments/6oaegs/a_challenge_think_you_can_solve_it/,AmirRosenfeld,1500488558,[removed],1,0
852,2017-7-20,2017,7,20,3,6oag1q,[P] Unblurring images of text with convolutional neural networks,https://www.reddit.com/r/MachineLearning/comments/6oag1q/p_unblurring_images_of_text_with_convolutional/,[deleted],1500488946,[deleted],0,1
853,2017-7-20,2017,7,20,3,6oagnn,[R] Optimizing the Latent Space of Generative Networks,https://www.reddit.com/r/MachineLearning/comments/6oagnn/r_optimizing_the_latent_space_of_generative/,iidealized,1500489081,,11,21
854,2017-7-20,2017,7,20,3,6oahbx,Recurrent Neural Networks - The Math of Intelligence (Week 5),https://www.reddit.com/r/MachineLearning/comments/6oahbx/recurrent_neural_networks_the_math_of/,funmaster11,1500489224,,0,1
855,2017-7-20,2017,7,20,3,6oaiom,[P] Unblurring images of text with convolutional neural networks,https://www.reddit.com/r/MachineLearning/comments/6oaiom/p_unblurring_images_of_text_with_convolutional/,holgeir564,1500489550,,23,51
856,2017-7-20,2017,7,20,3,6oaiv8,"My attempt to build a vanilla neural network, using only numpy.",https://www.reddit.com/r/MachineLearning/comments/6oaiv8/my_attempt_to_build_a_vanilla_neural_network/,ritchie46,1500489591,,0,1
857,2017-7-20,2017,7,20,3,6oangd,"[N] How Checkers Was Solved: machine learning history, feat. Jonathan Schaeffer of the University of Alberta",https://www.reddit.com/r/MachineLearning/comments/6oangd/n_how_checkers_was_solved_machine_learning/,robinsloan,1500490699,,2,118
858,2017-7-20,2017,7,20,4,6ob13q,Looking for a post that was up here earlier.,https://www.reddit.com/r/MachineLearning/comments/6ob13q/looking_for_a_post_that_was_up_here_earlier/,[deleted],1500494029,[removed],0,1
859,2017-7-20,2017,7,20,4,6ob1k4,Linear regression simulator will help you understand linear regression using gradient descent.,https://www.reddit.com/r/MachineLearning/comments/6ob1k4/linear_regression_simulator_will_help_you/,cpuheater,1500494150,,0,1
860,2017-7-20,2017,7,20,5,6ob82x,[D] Randomly dropping data in ensemble method for neural networks,https://www.reddit.com/r/MachineLearning/comments/6ob82x/d_randomly_dropping_data_in_ensemble_method_for/,crouching_dragon_420,1500495754,"I am working for an industrial project and my role is to squeeze out as much object detection recall rate as possible. I am thinking about combining different super fast neural architectures (YOLO9000, google mobilenet...) and train them on subsets of training data. Does this make any sense? As neural nets are better when there are more data, is it a good idea to randomly drop some data i.e. doing bagging?",2,7
861,2017-7-20,2017,7,20,5,6obahl,How can I extract a scikit-learn trained model to use in something like a PHP based project,https://www.reddit.com/r/MachineLearning/comments/6obahl/how_can_i_extract_a_scikitlearn_trained_model_to/,josephtheonys,1500496337,[removed],0,1
862,2017-7-20,2017,7,20,6,6obhu0,[P] Variational Autoencoders for dummies (from a dummy),https://www.reddit.com/r/MachineLearning/comments/6obhu0/p_variational_autoencoders_for_dummies_from_a/,funj0k3r,1500498153,,5,14
863,2017-7-20,2017,7,20,7,6oc02f,What research is there on neural-networks that explain/justify their own conclusions?,https://www.reddit.com/r/MachineLearning/comments/6oc02f/what_research_is_there_on_neuralnetworks_that/,move_like_lasagna,1500502937,[removed],0,1
864,2017-7-20,2017,7,20,7,6oc54h,[R] [1707.05373] Houdini: Fooling Deep Structured Prediction Models,https://www.reddit.com/r/MachineLearning/comments/6oc54h/r_170705373_houdini_fooling_deep_structured/,Mandrathax,1500504364,,3,2
865,2017-7-20,2017,7,20,9,6ocvh8,Apple Machine Learning Journal,https://www.reddit.com/r/MachineLearning/comments/6ocvh8/apple_machine_learning_journal/,cognitivedemons,1500511964,,0,1
866,2017-7-20,2017,7,20,10,6od3g7,learning to search algorithms can replace CRFs?,https://www.reddit.com/r/MachineLearning/comments/6od3g7/learning_to_search_algorithms_can_replace_crfs/,yin9zui,1500514387,[removed],0,1
867,2017-7-20,2017,7,20,10,6od6u2,[R][1707.05847] The Devil is in the Decoder,https://www.reddit.com/r/MachineLearning/comments/6od6u2/r170705847_the_devil_is_in_the_decoder/,mimighost,1500515442,,8,37
868,2017-7-20,2017,7,20,11,6odcrc,[P] Mimic Snapchat Filters Programmatically,https://www.reddit.com/r/MachineLearning/comments/6odcrc/p_mimic_snapchat_filters_programmatically/,histoire_guy,1500517365,,1,0
869,2017-7-20,2017,7,20,11,6odivn,"[D] Neuroscience-Inspired Artificial Intelligence by Hassabis, Kumaran, Summerfield and Botvinick (x-post)",https://www.reddit.com/r/MachineLearning/comments/6odivn/d_neuroscienceinspired_artificial_intelligence_by/,RSchaeffer,1500519323,,0,27
870,2017-7-20,2017,7,20,11,6odjo0,[D] Looking for examples of visually appealing figures of deep learning architectures,https://www.reddit.com/r/MachineLearning/comments/6odjo0/d_looking_for_examples_of_visually_appealing/,2am_beats,1500519595,"I have to visualize a couple of models and I would like to make them visually appealing. I'm looking for good looking figures displaying deep learning networks for inspiration, in particular CNN architectures. I do aim to show some technical info such as layer sizes, etc. Do you have any particular examples you like? Diagrams with overview that are easy to understand? Thanks!",2,9
871,2017-7-20,2017,7,20,13,6oe0hw,"Tensor Decomposition: PARAFAC, Tucker",https://www.reddit.com/r/MachineLearning/comments/6oe0hw/tensor_decomposition_parafac_tucker/,qwinion,1500525360,[removed],0,1
872,2017-7-20,2017,7,20,13,6oe2vm,veneer peeling machine wood veneer cutting machine spindle veneer rotary...,https://www.reddit.com/r/MachineLearning/comments/6oe2vm/veneer_peeling_machine_wood_veneer_cutting/,plywoodmachines,1500526239,,0,1
873,2017-7-20,2017,7,20,13,6oe3e5,SIGKDD 2017: Massive relaying of citations found in scientific articles,https://www.reddit.com/r/MachineLearning/comments/6oe3e5/sigkdd_2017_massive_relaying_of_citations_found/,mayank4490,1500526427,,0,1
874,2017-7-20,2017,7,20,14,6oe6kl,[R] Probabilistic Matrix Factorization for Automated Machine Learning,https://www.reddit.com/r/MachineLearning/comments/6oe6kl/r_probabilistic_matrix_factorization_for/,jarth_or_north,1500527635,,0,11
875,2017-7-20,2017,7,20,14,6oe8gk,I have a time series I'd like to predict. Where can I have it analyzed? I would obviously be willing to compensate fairly.,https://www.reddit.com/r/MachineLearning/comments/6oe8gk/i_have_a_time_series_id_like_to_predict_where_can/,PlasterRace,1500528367,[removed],0,1
876,2017-7-20,2017,7,20,15,6oegzk,All dressed up and nowhere to go ... What is the state of the art for classifying 15-20 numerical features?,https://www.reddit.com/r/MachineLearning/comments/6oegzk/all_dressed_up_and_nowhere_to_go_what_is_the/,apiUSER3,1500531805,[removed],0,1
877,2017-7-20,2017,7,20,15,6oeidq,The limitations of deep learning,https://www.reddit.com/r/MachineLearning/comments/6oeidq/the_limitations_of_deep_learning/,vincent341,1500532417,[removed],0,1
878,2017-7-20,2017,7,20,15,6oek8b,[R] How to Escape Saddle Points Efficiently,https://www.reddit.com/r/MachineLearning/comments/6oek8b/r_how_to_escape_saddle_points_efficiently/,masharpe,1500533204,,16,34
879,2017-7-20,2017,7,20,15,6oekxy,Question component parsing?,https://www.reddit.com/r/MachineLearning/comments/6oekxy/question_component_parsing/,HD187123b,1500533508,[removed],0,1
880,2017-7-20,2017,7,20,16,6oerdp,[D] My ML Journey,https://www.reddit.com/r/MachineLearning/comments/6oerdp/d_my_ml_journey/,hydrix2,1500536298,[removed],9,0
881,2017-7-20,2017,7,20,16,6oettx,How do you version control your neural nets?,https://www.reddit.com/r/MachineLearning/comments/6oettx/how_do_you_version_control_your_neural_nets/,[deleted],1500537424,[removed],0,1
882,2017-7-20,2017,7,20,17,6oeyj3,[P] A self-organizing classification algorithm,https://www.reddit.com/r/MachineLearning/comments/6oeyj3/p_a_selforganizing_classification_algorithm/,inboble,1500539668,,0,5
883,2017-7-20,2017,7,20,18,6of4e7,[D] Demis Hassabis' article on why AI needs neuroscience and vice-versa,https://www.reddit.com/r/MachineLearning/comments/6of4e7/d_demis_hassabis_article_on_why_ai_needs/,[deleted],1500542368,[deleted],0,1
884,2017-7-20,2017,7,20,19,6ofe46,[R] No region proposals even with weak supervision: 'Single-Shot' Weakly Supervised Localization. (real-time inference!),https://www.reddit.com/r/MachineLearning/comments/6ofe46/r_no_region_proposals_even_with_weak_supervision/,nomaderx,1500546730,,12,13
885,2017-7-20,2017,7,20,19,6ofh5k,[1707.06203] Imagination-Augmented Agents for Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/6ofh5k/170706203_imaginationaugmented_agents_for_deep/,enderwagon,1500548001,,1,10
886,2017-7-20,2017,7,20,19,6ofhy7,[D] Using Machine Learning to Predict Value of Homes On Airbnb,https://www.reddit.com/r/MachineLearning/comments/6ofhy7/d_using_machine_learning_to_predict_value_of/,villasv,1500548333,,17,103
887,2017-7-20,2017,7,20,20,6ofq6r,Hardness Testing New England,https://www.reddit.com/r/MachineLearning/comments/6ofq6r/hardness_testing_new_england/,abcndtt,1500551508,[removed],0,1
888,2017-7-20,2017,7,20,21,6ofrve,Word2Vec for foreign languages,https://www.reddit.com/r/MachineLearning/comments/6ofrve/word2vec_for_foreign_languages/,Pixel_Owl,1500552114,[removed],0,1
889,2017-7-20,2017,7,20,21,6oftmu,[D] Why is my Tensorflow LSTM model not able to predict a time series pattern in a one-hot encoded vector?,https://www.reddit.com/r/MachineLearning/comments/6oftmu/d_why_is_my_tensorflow_lstm_model_not_able_to/,master_chamberlain,1500552719,"I am purposefully avoiding an embedding or character based model because the aim is for the model to be able to learn general patterns in the data rather than reproduce the training data.

I am relatively new to neural networks but my understanding is that feeding a sequence of one-hot vectors as input and a one hot vector as a label representing the next time step should converge. However, I can observe in training that it only learns the indexes in the vector that have been hot in the training data rather than the pattern in the sequence.

A short example:
Let's say the training data is as follows, each vector representing a single time step -
[1,0,0,0,0]
[0,1,0,0,0]
[1,0,0,0,0]
[0,1,0,0,0]

Then the predicted output should be [1,0,0,0,0] if the previous input was [0,1,0,0,0] and so on.
The actual output I am getting is [1,1,0,0,0] though.

The get_next_batch() function below returns batch_size time steps of the data set for 'batch_x' and offset by one time step for 'batch_y'.

Code:

    import tensorflow as tf
    import numpy as np
    from handle_data import MIDIDataSet
    from MIDO_Output import MIDIDevice
    
    batch_size = 120
    procession = 1
    num_notes = 106
    num_layers = 1
    hidden_units = 106
    learning_rate = 0.001
    epochs = 10
    num_classes = 2
    
    x = tf.placeholder(tf.float32, [batch_size, num_notes], name=""x"")
    y = tf.placeholder(tf.int32, [num_notes], name=""y"")
    
    W1 = tf.Variable(tf.truncated_normal([num_notes, num_notes]), dtype=tf.float32)
    b1 = tf.Variable(tf.zeros([num_notes]), dtype=tf.float32)
    W2 = tf.Variable(tf.truncated_normal([num_notes, num_notes]), dtype=tf.float32)
    b2 = tf.Variable(tf.zeros([num_notes]), dtype=tf.float32)
    
    input_layer = tf.matmul(x, W1, name=""input_layer_matmul"") + b1
    
    cell = [tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(hidden_units), input_keep_prob=0.75, output_keep_prob=1.0) for _ in range(num_layers)]
   
    cell = tf.contrib.rnn.MultiRNNCell(cell, state_is_tuple=True)
    init_state = cell.zero_state(batch_size, tf.float32)
    states_series, current_state = tf.nn.dynamic_rnn(cell, tf.expand_dims(input_layer,-1), initial_state=init_state)
    states_series = tf.reshape(states_series, [-1, hidden_units])
    
    
    loss = tf.nn.softmax_cross_entropy_with_logits(logits=states_series[-1], labels=y)
    
    
    train = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)
    
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        _current_state = sess.run(init_state)
    
        for epoch in range(epochs):
            data = MIDIDataSet(batch_size, procession)
            steps = 0
        
            while data.total_processed == 0:
                batch_x, batch_y = data.get_next_batch()
                batch_y = batch_y[-1]
            
                prediction, _loss, _train, _current_state = sess.run([states_series, loss, train, current_state], feed_dict={x:batch_x, y:batch_y, init_state:_current_state})
            
                if steps % 1000 == 0:
                    print(""Loss:"", _loss)
                    print(prediction[-1])",8,1
890,2017-7-20,2017,7,20,21,6ofzxp,[R][1704.01858] An Online Hierarchical Algorithm for Extreme Clustering,https://www.reddit.com/r/MachineLearning/comments/6ofzxp/r170401858_an_online_hierarchical_algorithm_for/,hanklolaz,1500554813,,8,13
891,2017-7-20,2017,7,20,22,6og2ra,Does anyone have any tfidf success stories?,https://www.reddit.com/r/MachineLearning/comments/6og2ra/does_anyone_have_any_tfidf_success_stories/,pvkooten,1500555663,[removed],0,1
892,2017-7-20,2017,7,20,22,6og4lu,In Conversation with Danny Lange: Uber Implements Machine Learning,https://www.reddit.com/r/MachineLearning/comments/6og4lu/in_conversation_with_danny_lange_uber_implements/,dexlabanalytics,1500556230,,0,1
893,2017-7-20,2017,7,20,22,6og6oy,[D] Facial Similarity with Siamese Networks in PyTorch,https://www.reddit.com/r/MachineLearning/comments/6og6oy/d_facial_similarity_with_siamese_networks_in/,harvey_slash,1500556874,,3,9
894,2017-7-20,2017,7,20,23,6ogm0m,Data Structures Related to Machine Learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/6ogm0m/data_structures_related_to_machine_learning/,[deleted],1500561292,[deleted],0,1
895,2017-7-20,2017,7,20,23,6ogoak,Which neural network library allows me to customize the architecture &amp; topology of the network?,https://www.reddit.com/r/MachineLearning/comments/6ogoak/which_neural_network_library_allows_me_to/,[deleted],1500561931,[removed],0,1
896,2017-7-20,2017,7,20,23,6ogq66,[R] Data Structures Related to Machine Learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/6ogq66/r_data_structures_related_to_machine_learning/,jthommo,1500562423,,0,6
897,2017-7-21,2017,7,21,0,6ogu2b,WSJ dataset for speech recognition,https://www.reddit.com/r/MachineLearning/comments/6ogu2b/wsj_dataset_for_speech_recognition/,saseptim,1500563392,[removed],0,1
898,2017-7-21,2017,7,21,0,6ogwe6,[R] [1707.06119] Discriminative convolutional Fisher vector network for action recognition,https://www.reddit.com/r/MachineLearning/comments/6ogwe6/r_170706119_discriminative_convolutional_fisher/,iownaredball,1500563990,,0,3
899,2017-7-21,2017,7,21,0,6ogxcz,[R] Learning model-based planning from scratch,https://www.reddit.com/r/MachineLearning/comments/6ogxcz/r_learning_modelbased_planning_from_scratch/,downtownslim,1500564227,,1,7
900,2017-7-21,2017,7,21,0,6oh1cn,[P] Cifar-10 using Wide Residual Networks,https://www.reddit.com/r/MachineLearning/comments/6oh1cn/p_cifar10_using_wide_residual_networks/,olympus999,1500565226,"Hello guys,

I am trying to replicate results from this paper (https://arxiv.org/pdf/1605.07146.pdf) in Keras. There are already some implementations of this in Keras, but using those implementations I have not been able to reproduce state of the art results as show in paper (or even close enough, close enough is a difference of 1 percentage point from paper).

Now that the introduction is done, lets focus on achieving state of art results in CIFAR-10 dataset. Here is what I have been building, to mimic the paper as accurately as I could: https://github.com/olympus999/cifar-10-wrn
I have reached validation accuracy of 92-93% using WRN-28-10 model, but according to the paper I should achieve 96% accuracy.

I did verify WRN-16-2 against original model, so this should be 100% correct (information which can be read from available visual graphs representing the CNN). Original model is visualized at the bottom of this link: https://github.com/szagoruyko/wide-residual-networks - there is also pytorch code from the original paper.

Any help or advice to help achieve accuracy of 95%+ is appreciated!

EDIT: I updated the text to represent the latest fixes to the architecture (based on comments below), but I still have not received the same results as in the paper. Training results of first 200 epochs (few epochs are missing from start) is available in my github: https://github.com/olympus999/cifar-10-wrn",7,9
901,2017-7-21,2017,7,21,0,6oh26a,[N] Movidius launches a $79 deep-learning USB stick,https://www.reddit.com/r/MachineLearning/comments/6oh26a/n_movidius_launches_a_79_deeplearning_usb_stick/,Jackz0r,1500565438,,21,37
902,2017-7-21,2017,7,21,1,6oh682,[D] All dressed up and nowhere to go ... What is the state of the art for classifying 15-20 numerical features?,https://www.reddit.com/r/MachineLearning/comments/6oh682/d_all_dressed_up_and_nowhere_to_go_what_is_the/,apiUSER3,1500566425,Is there a well defined state of the art for this? I have installed the gpu version of tensorflow and a K80 GPU and now I realize I don't know what model is best ... presumably I can do better than the tf.contrib.learn.DNNClassifier used on the iris dataset ? none of the other ones in the tutorial appear to be particularly relevant.,6,3
903,2017-7-21,2017,7,21,1,6oh6qf,[D] How (the hell) can I download and process video efficiently?,https://www.reddit.com/r/MachineLearning/comments/6oh6qf/d_how_the_hell_can_i_download_and_process_video/,keidouleyoucee,1500566541,"Hi, I wanted to crawl images and audio clips from youtube videos -- hopefully few millions of them. The size/number of image/audio is not set but perhaps 256x256 center only and maybe 10-30 images per video.

It's my first time preprocessing video and I'm struggling to find the fastest way to do it. I'm using three desktops, have 4TB HDD each (also mounted in all desktops) + 0.5TB SSD + 10TB SSD. How should I do this? How would you do it?

What I've tried is first just download them all, and then extract the images/audios. Downloading was very fast -- I felt like perhaps writing to HDD is the bottleneck. But it is still quite slow.

Then I changed it to store the video only temporarily, on SSD, and load it again, then store image/audio, finally remove the video. It's still too slow.. I doubt it's because too many processes are writing the files to the same folders, so maybe it could be improved a bit, but wanna know how experienced people are dealing with it.

I was using pytube, scikit-video, scikit-image, and librosa.",6,2
904,2017-7-21,2017,7,21,1,6oh7cy,[P] Periodic Spatial Generative Adversarial Networks (PSGANs),https://www.reddit.com/r/MachineLearning/comments/6oh7cy/p_periodic_spatial_generative_adversarial/,krasul,1500566677,,0,0
905,2017-7-21,2017,7,21,1,6oh97f,"July 20, 2017 at 04:09PM",https://www.reddit.com/r/MachineLearning/comments/6oh97f/july_20_2017_at_0409pm/,[deleted],1500567135,[removed],0,1
906,2017-7-21,2017,7,21,1,6ohezr,automatic aluminium tube filling and sealing machine supplier,https://www.reddit.com/r/MachineLearning/comments/6ohezr/automatic_aluminium_tube_filling_and_sealing/,hymachinery,1500568580,,0,1
907,2017-7-21,2017,7,21,1,6ohg5d,Intel Democratizes Deep Learning Application Development with Launch of Movidius Neural Compute Stick,https://www.reddit.com/r/MachineLearning/comments/6ohg5d/intel_democratizes_deep_learning_application/,[deleted],1500568875,[deleted],0,1
908,2017-7-21,2017,7,21,2,6ohphz,expert-generated reading comprehension dataset,https://www.reddit.com/r/MachineLearning/comments/6ohphz/expertgenerated_reading_comprehension_dataset/,michaelxie2,1500571135,,0,1
909,2017-7-21,2017,7,21,2,6ohwha,[R] Scatteract: Automated extraction of data from scatter plots,https://www.reddit.com/r/MachineLearning/comments/6ohwha/r_scatteract_automated_extraction_of_data_from/,monsieurcliche,1500572801,,3,6
910,2017-7-21,2017,7,21,3,6oi0lp,Best open sourced text data set for anomaly detection?,https://www.reddit.com/r/MachineLearning/comments/6oi0lp/best_open_sourced_text_data_set_for_anomaly/,thenerdstation,1500573801,[removed],0,1
911,2017-7-21,2017,7,21,3,6oi2xo,Industrial machine vision camera with a Nvidia TX2 inside,https://www.reddit.com/r/MachineLearning/comments/6oi2xo/industrial_machine_vision_camera_with_a_nvidia/,Sureworks,1500574341,,0,1
912,2017-7-21,2017,7,21,3,6oi9nx,A Primer on Projective Geometry (x-post from r/computervision),https://www.reddit.com/r/MachineLearning/comments/6oi9nx/a_primer_on_projective_geometry_xpost_from/,activatedgeek,1500575977,,0,1
913,2017-7-21,2017,7,21,4,6oin2v,Train Neural Machine Translation Models with Sockeye,https://www.reddit.com/r/MachineLearning/comments/6oin2v/train_neural_machine_translation_models_with/,piiswrong,1500579136,,0,6
914,2017-7-21,2017,7,21,5,6oj173,Not able to use 'imtest' on darknet (For YOLOv2),https://www.reddit.com/r/MachineLearning/comments/6oj173/not_able_to_use_imtest_on_darknet_for_yolov2/,Maverick_05,1500582606,[removed],0,1
915,2017-7-21,2017,7,21,5,6oj6on,[D] How do you version control your neural net?,https://www.reddit.com/r/MachineLearning/comments/6oj6on/d_how_do_you_version_control_your_neural_net/,iamwil,1500583949,"When I started working with neural nets I instinctively started using git. Soon I realised that git isn't working for me. Working with neural nets seems way more empirical than working with a 'regular' project where you have a very specific feature (e.g. login feature): you create a branch where you implement this feature. 

Once the feature is implemented you merge with your develop branch and you can move to another feature. The same approach doesn't work with neural nets for me. There's 'only' one feature you want to implement - you want your neural net to generalise better/generate better images/etc (depends on the type of problem you are solving). This is very abstract though. One often doesn't even know what's the solution until you empirically try to tweak several hyper parameters and see the loss function and accuracy. This makes the branch model impossible to use I think. 

Consider this: you create a branch where you want to use convolutional layers for example. Then you find out that your neural net is performing worse. What should you do know? You can't merge this branch to your develop branch since it's a basically 'dead end' branch. On the other hand when you delete this branch you lose information that you've already tried this model of your net. This also produce huge amount of branches since you have enormous number of combinations for your model (e.g. convolutional layers may yield better accuracy when used with different loss function).

I've ended up with a single branch and a text file where I manually log all models I have tried so far and their performance. This creates nontrivial overhead though.",89,28
916,2017-7-21,2017,7,21,6,6ojj6l,Random Forest cross-validation,https://www.reddit.com/r/MachineLearning/comments/6ojj6l/random_forest_crossvalidation/,augustus2010,1500587172,[removed],0,1
917,2017-7-21,2017,7,21,6,6ojlqb,Becoming a Data Cook: What Data Preparation means for Data Scientists,https://www.reddit.com/r/MachineLearning/comments/6ojlqb/becoming_a_data_cook_what_data_preparation_means/,winnebagoman,1500587859,,0,1
918,2017-7-21,2017,7,21,8,6ok12v,[D] Anyone know if/when 2017 cs231n lectures are coming out?,https://www.reddit.com/r/MachineLearning/comments/6ok12v/d_anyone_know_ifwhen_2017_cs231n_lectures_are/,MrDoOO,1500592087,,8,23
919,2017-7-21,2017,7,21,9,6okfyj,[N] OpenAI open-sources new policy gradient implementations,https://www.reddit.com/r/MachineLearning/comments/6okfyj/n_openai_opensources_new_policy_gradient/,Neutran,1500596466,"Links to the [blog post](https://blog.openai.com/openai-baselines-ppo/) and the [github repo](https://github.com/openai/baselines).

They claim that PPO beats TRPO on continuous control tasks, and almost matches ACER on Atari games. Since ACER has better performance than A3C, does it mean PPO can strictly replace A3C?

If that's the case, can I safely assume that PPO is the go-to policy gradient method for both continuous and discrete RL tasks? ",13,84
920,2017-7-21,2017,7,21,9,6okiv0,How do I step my approach to building an auto news summarizer?,https://www.reddit.com/r/MachineLearning/comments/6okiv0/how_do_i_step_my_approach_to_building_an_auto/,[deleted],1500597338,[removed],0,1
921,2017-7-21,2017,7,21,10,6okojv,[D] How do I apply Machine Learning to teaching?,https://www.reddit.com/r/MachineLearning/comments/6okojv/d_how_do_i_apply_machine_learning_to_teaching/,Aerothermal,1500599131,"I sincerely hope that this is not a stupid question. I've had a vision for some years now about applying machine learning to teaching yet I feel like I'm at square 1.

The problem is that current methods of delivering knowledge to other people are archaic, relying more on superstition (e.g. pop-psychology) and individual experience than anything resembling scientific method (I say this as manager of an engineering degree having worked tutoring, teaching and lecturing).

There is a chasm between education researchers and teaching practitioners. Currently there is a drive to implement all sorts of e-learning but with absolutely no rigorous statistical data to support their efficacy, at least in the circles I've run in. All teachers have their own ways of delivery, that they are reluctant to change, and they all think they know the best way to learn. In fact, nobody knows what's best for learning, or in what amount (e.g. how often should we test? How long should our lectures be for these learners? How much should be assigned to reading? How much to group assessment? Who is the better speaker?)

I see the revolution that was Evidence-Based Medicine towards the end of the 20th century, and think that education is yet to go through a similar process. All teachers must complete an 'Action Research Project' during their training, but as far as I have seen, none carry out any rigorous statistics, and just about all of them are small sample size case studies full of bias. Our manager in Higher Education and Scholarship, with two education-related Masters, does not believe that statistics is much applicable to improving teaching: ""Students are all unique, how will statistics help? It wont."" I see this as a problem that needs solving.

The vision is to use computation to optimise how students learn.

The solution as I see it is integrating machine learning into a teaching platform (commonly referred to as a Learning Management System) for individualised teaching, which continually changes its delivery methods (e.g. speed of video instruction, length of session, frequency of sessions, type of media, the age and gender of the video narrator) to correlate with the individual student's statistics (age, gender, prior test scores and preferences) to optimise a goal (e.g. maximise the student's average test scores). I have plenty of course content (Youtube videos, quiz questions, exams, coursework, ppt slides, assessed group activities, interactive applets, textbook chapters, etc.).

How do I go about implementing Machine Learning to optimise the delivery of course content, and in the process discover something about how students learn best?

The only code I use currently is MATLAB. What language do I need to learn, PHP perhaps? Should I be applying for a Masters in ML? Or a course in programming?  Or should I get a business loan and hire someone that actually knows what they're doing? Or is there somewhere in the UK that would support this as a PhD project?",14,3
922,2017-7-21,2017,7,21,10,6okrcj,[R] Deep Layer Aggregation,https://www.reddit.com/r/MachineLearning/comments/6okrcj/r_deep_layer_aggregation/,xternalz,1500600001,,2,5
923,2017-7-21,2017,7,21,12,6oldth,Neural Machine Translation models with Apache MXNet,https://www.reddit.com/r/MachineLearning/comments/6oldth/neural_machine_translation_models_with_apache/,domdivakaruni,1500607442,,0,1
924,2017-7-21,2017,7,21,13,6olkdr,Laptop purchase advice,https://www.reddit.com/r/MachineLearning/comments/6olkdr/laptop_purchase_advice/,[deleted],1500609751,[removed],0,1
925,2017-7-21,2017,7,21,13,6olnw7,Version control in machine learning: Git vs. IPython/Jupiter vs. DVC,https://www.reddit.com/r/MachineLearning/comments/6olnw7/version_control_in_machine_learning_git_vs/,[deleted],1500611021,[deleted],0,1
926,2017-7-21,2017,7,21,13,6olpqn,[D] Version control in machine learning: Git vs. IPython/Jupiter vs. DVC,https://www.reddit.com/r/MachineLearning/comments/6olpqn/d_version_control_in_machine_learning_git_vs/,thumbsdrivesmecrazy,1500611722,,2,10
927,2017-7-21,2017,7,21,13,6olrwm,[P] Deep Reinforcement Learning Challenge [NIPS 2017],https://www.reddit.com/r/MachineLearning/comments/6olrwm/p_deep_reinforcement_learning_challenge_nips_2017/,kidzik,1500612556,,5,21
928,2017-7-21,2017,7,21,13,6olsyx,Intel Launches Movidius Neural Compute Stick,https://www.reddit.com/r/MachineLearning/comments/6olsyx/intel_launches_movidius_neural_compute_stick/,[deleted],1500612956,[deleted],0,1
929,2017-7-21,2017,7,21,14,6om0vt,Algorithm for AutoCorrect Suggestions,https://www.reddit.com/r/MachineLearning/comments/6om0vt/algorithm_for_autocorrect_suggestions/,keepitsalty,1500616051,[removed],0,1
930,2017-7-21,2017,7,21,15,6om3k7,Learn Robotics &amp;amp; Machine Learning Concepts of Self-Driving Cars Course,https://www.reddit.com/r/MachineLearning/comments/6om3k7/learn_robotics_amp_machine_learning_concepts_of/,ravisaive,1500617188,,0,1
931,2017-7-21,2017,7,21,15,6om65z,[N] An Update to Open Images - Now with Bounding-Boxes,https://www.reddit.com/r/MachineLearning/comments/6om65z/n_an_update_to_open_images_now_with_boundingboxes/,funj0k3r,1500618260,,2,59
932,2017-7-21,2017,7,21,15,6om8y6,"cvpaper.challenge in 2016: Futuristic Computer Vision through 1,600 Papers Survey",https://www.reddit.com/r/MachineLearning/comments/6om8y6/cvpaperchallenge_in_2016_futuristic_computer/,schmidhubernet,1500619416,,0,1
933,2017-7-21,2017,7,21,16,6omc88,Is Machine Learning the best way to grow a FinTech company?,https://www.reddit.com/r/MachineLearning/comments/6omc88/is_machine_learning_the_best_way_to_grow_a/,hardikmakadia,1500620842,,0,1
934,2017-7-21,2017,7,21,16,6omgph,Hollow core slab production at one time for two lines,https://www.reddit.com/r/MachineLearning/comments/6omgph/hollow_core_slab_production_at_one_time_for_two/,ada2017,1500622892,,0,1
935,2017-7-21,2017,7,21,18,6omvv7,Sheet Gasket Materials,https://www.reddit.com/r/MachineLearning/comments/6omvv7/sheet_gasket_materials/,anericanseal,1500629919,,0,1
936,2017-7-21,2017,7,21,18,6omwmq,[D] An overview of Apple's ARKit and CoreML frameworks,https://www.reddit.com/r/MachineLearning/comments/6omwmq/d_an_overview_of_apples_arkit_and_coreml/,cocozoa,1500630273,"I have here highlighted some points of interest about ARKit and Core ML (announced at last month's WWDC). Any other points?

teks.co.in/site/blog/arkit-and-core-ml-an-overview-of-the-new-apple-frameworks/",2,9
937,2017-7-21,2017,7,21,19,6on3ys,Spiral Wound Gaskets,https://www.reddit.com/r/MachineLearning/comments/6on3ys/spiral_wound_gaskets/,anericanseal,1500633566,[removed],0,1
938,2017-7-21,2017,7,21,19,6on4be,Question: is there a roadmap of features for azure machine learning studio?,https://www.reddit.com/r/MachineLearning/comments/6on4be/question_is_there_a_roadmap_of_features_for_azure/,crash893b,1500633720,[removed],0,1
939,2017-7-21,2017,7,21,19,6on6no,Using GRAKN.AI for Horn Clause Mining,https://www.reddit.com/r/MachineLearning/comments/6on6no/using_graknai_for_horn_clause_mining/,[deleted],1500634736,[deleted],0,1
940,2017-7-21,2017,7,21,20,6on8aw,[P] Using GRAKN.AI for Horn Clause Mining,https://www.reddit.com/r/MachineLearning/comments/6on8aw/p_using_graknai_for_horn_clause_mining/,pkwan4,1500635374,,0,0
941,2017-7-21,2017,7,21,20,6onexs,[German] Apple Machine Learning Journal - Erster Artikel. Jetzt lesen :),https://www.reddit.com/r/MachineLearning/comments/6onexs/german_apple_machine_learning_journal_erster/,flezzfx,1500637963,,0,1
942,2017-7-21,2017,7,21,20,6onfgl,[D] Blockchain and machine learning,https://www.reddit.com/r/MachineLearning/comments/6onfgl/d_blockchain_and_machine_learning/,Wolfandwalls,1500638161,"The only good answer about this I've seen here: https://www.reddit.com/r/MachineLearning/comments/4bhmqb/decentralized_deep_learning_on_a_blockchain_ai/d19mh4d/


I'm looking for papers, research, articles in the intersection of the two fields. Topics like distributed, decentralized machine learning and blockchains for data-handling/hiding.",10,3
943,2017-7-21,2017,7,21,21,6onizg,Fourier Machine,https://www.reddit.com/r/MachineLearning/comments/6onizg/fourier_machine/,rajesh_d24,1500639367,[removed],0,1
944,2017-7-21,2017,7,21,22,6ons8j,"[D] Popularity of Machine Learning, Deep Learning",https://www.reddit.com/r/MachineLearning/comments/6ons8j/d_popularity_of_machine_learning_deep_learning/,tryndisskilled,1500642427,"Hello all,

I'm sure many of you experienced the gain in popularity of Machine Learning subjects the past few years, and probably especially Deep Learning and Computer Vision tasks.

Is there any kind of metric available, that would actually confirm that claim? I found [this](https://arxiv.org/help/stats/2016_by_area/index) on arXiv, but I'm not quite sure as how to interpret it (if it's even relevant). I'm mostly looking for some kind of measured growth of papers submissions compared to other areas, as I think that would be an interesting metric.",3,6
945,2017-7-21,2017,7,21,22,6onsrz,[P] End to end deep learning experimentation platform for Tensorflow,https://www.reddit.com/r/MachineLearning/comments/6onsrz/p_end_to_end_deep_learning_experimentation/,jakn,1500642598,,0,7
946,2017-7-21,2017,7,21,22,6onyrk,[R] Learning Deep Architectures via Generalized Whitened Neural Networks,https://www.reddit.com/r/MachineLearning/comments/6onyrk/r_learning_deep_architectures_via_generalized/,xternalz,1500644458,"&gt; Whitened Neural Network (WNN) is a recent advanced deep architecture, which improves convergence and generalization of canonical neural networks by whitening their internal hidden representation. However, the whitening transformation increases computation time. Unlike WNN that reduced runtime by performing whitening every thousand iterations, which degenerates convergence due to the ill conditioning, we present generalized WNN (GWNN), which has three appealing properties. First, GWNN is able to learn compact representation to reduce computations. Second, it enables whitening transformation to be performed in a short period, preserving good conditioning. Third, we propose a data-independent estimation of the covariance matrix to further improve computational efficiency. Extensive experiments on various datasets demonstrate the benefits of GWNN.

Paper: http://personal.ie.cuhk.edu.hk/~pluo/pdf/pluoICML2017.pdf",0,10
947,2017-7-21,2017,7,21,22,6oo0ab,[P] Spotlight: deep learning recommender model framework in PyTorch,https://www.reddit.com/r/MachineLearning/comments/6oo0ab/p_spotlight_deep_learning_recommender_model/,voiruloo,1500644908,,2,41
948,2017-7-21,2017,7,21,23,6oocdy,[R] Hassabis et. al.: Neuroscience-Inspired Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/6oocdy/r_hassabis_et_al_neuroscienceinspired_artificial/,downtownslim,1500648279,,3,33
949,2017-7-21,2017,7,21,23,6oof0g,[N] MILA Obtains $1.6 Million AI Safety Research Grant,https://www.reddit.com/r/MachineLearning/comments/6oof0g/n_mila_obtains_16_million_ai_safety_research_grant/,DanielHendrycks,1500648976,,23,53
950,2017-7-22,2017,7,22,0,6ookgj,[P] A Network Structure Visualizer for CNNs (includes dilated/atrous convolution and SAME/VALID padding modes from TensorFlow),https://www.reddit.com/r/MachineLearning/comments/6ookgj/p_a_network_structure_visualizer_for_cnns/,ro_bin,1500650396,,8,3
951,2017-7-22,2017,7,22,0,6oop4t,Deep Learning for Automated Driving with MATLAB,https://www.reddit.com/r/MachineLearning/comments/6oop4t/deep_learning_for_automated_driving_with_matlab/,harrism,1500651619,,0,1
952,2017-7-22,2017,7,22,0,6ootnp,Building a 50 Teraflops AMD Vega Deep Learning Box for Under $3K,https://www.reddit.com/r/MachineLearning/comments/6ootnp/building_a_50_teraflops_amd_vega_deep_learning/,[deleted],1500652777,[removed],0,1
953,2017-7-22,2017,7,22,1,6oozh8,[D] How to derive the Auxiliary ELBO ?,https://www.reddit.com/r/MachineLearning/comments/6oozh8/d_how_to_derive_the_auxiliary_elbo/,fixedrl,1500654124,"This refers to the [paper](https://arxiv.org/abs/1703.01961) Equation (1) and (7). 

It is unclear to me which steps derive from (1) to (7). ",7,5
954,2017-7-22,2017,7,22,1,6op5cn,Interactive Mind Map for learning Machine Learning,https://www.reddit.com/r/MachineLearning/comments/6op5cn/interactive_mind_map_for_learning_machine_learning/,neurocroc,1500655599,,0,1
955,2017-7-22,2017,7,22,1,6op6ak,[D] How viable? Building a 50 Teraflops AMD Vega Deep Learning Box for Under $3K,https://www.reddit.com/r/MachineLearning/comments/6op6ak/d_how_viable_building_a_50_teraflops_amd_vega/,phobrain,1500655828,,43,112
956,2017-7-22,2017,7,22,2,6opgx3,[Q] Applications of ML in journalistic research,https://www.reddit.com/r/MachineLearning/comments/6opgx3/q_applications_of_ml_in_journalistic_research/,[deleted],1500658515,[removed],0,1
957,2017-7-22,2017,7,22,2,6opmmh,Machine learning algorithms help predict schizophrenia with 74% accuracy.,https://www.reddit.com/r/MachineLearning/comments/6opmmh/machine_learning_algorithms_help_predict/,[deleted],1500659928,[deleted],0,1
958,2017-7-22,2017,7,22,3,6opu1t,Two papers describe agents that learn to imagine and use their imaginations to make plans. Deepmind,https://www.reddit.com/r/MachineLearning/comments/6opu1t/two_papers_describe_agents_that_learn_to_imagine/,cognitivedemons,1500661757,,0,1
959,2017-7-22,2017,7,22,4,6oq7vb,The world needs AI researchers. Heres how to become one.,https://www.reddit.com/r/MachineLearning/comments/6oq7vb/the_world_needs_ai_researchers_heres_how_to/,[deleted],1500665269,[deleted],0,1
960,2017-7-22,2017,7,22,4,6oqbux,[D] The world needs AI researchers. Heres how to become one.,https://www.reddit.com/r/MachineLearning/comments/6oqbux/d_the_world_needs_ai_researchers_heres_how_to/,themaximumgood,1500666313,,20,39
961,2017-7-22,2017,7,22,5,6oqk3z,[D] What are some solid academic reasons for using Generative Adversarial Networks?,https://www.reddit.com/r/MachineLearning/comments/6oqk3z/d_what_are_some_solid_academic_reasons_for_using/,Xirious,1500668393,"Hi all,

So one of the major topics in machine learning is Generative Adversarial Networks. I'd like to find out some solid academic motivations for why there's so much interest in this. Obviously we generate new samples but are these good enough to supplement smaller datasets? I also realise the discriminator can be used for downstream tasks such as classification if adapted post original training. What about networks such as InfoGAN? I'm sorry if this seems like a vague question but I'm trying to grasp why so much interest is being shown in these, admittedly cool, networks.",11,14
962,2017-7-22,2017,7,22,5,6oqoqt,Probability Theory - The Math of Intelligence #6,https://www.reddit.com/r/MachineLearning/comments/6oqoqt/probability_theory_the_math_of_intelligence_6/,funmaster11,1500669570,,0,1
963,2017-7-22,2017,7,22,5,6oqs4z,"here are rules for ML to communicate clearly (first draft), please send feedback or clarification",https://www.reddit.com/r/MachineLearning/comments/6oqs4z/here_are_rules_for_ml_to_communicate_clearly/,[deleted],1500670470,[removed],0,1
964,2017-7-22,2017,7,22,5,6oqt5t,Discover structure behind data with decision trees,https://www.reddit.com/r/MachineLearning/comments/6oqt5t/discover_structure_behind_data_with_decision_trees/,gchevooban,1500670752,,0,1
965,2017-7-22,2017,7,22,7,6or9bu,"July 21, 2017 at 09:58PM",https://www.reddit.com/r/MachineLearning/comments/6or9bu/july_21_2017_at_0958pm/,[deleted],1500675132,[removed],0,1
966,2017-7-22,2017,7,22,10,6os7ck,[P] Decoding the Enigma with Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/6os7ck/p_decoding_the_enigma_with_recurrent_neural/,hardmaru,1500685569,,15,93
967,2017-7-22,2017,7,22,11,6osi51,lifting tool,https://www.reddit.com/r/MachineLearning/comments/6osi51/lifting_tool/,ada2017,1500689345,,0,1
968,2017-7-22,2017,7,22,11,6osp94,"[D] Reddit Meetup @ CVPR. Saturday, July 22nd at 5pm - 6:30. Samsung room 323C, Honolulu convenction center, third floor. (Cross-post from r/computervision )",https://www.reddit.com/r/MachineLearning/comments/6osp94/d_reddit_meetup_cvpr_saturday_july_22nd_at_5pm/,bayfury,1500691948,"Please upvote for visibility. For those of you attending CVPR 2017 (or lucky enough to live here) please join us for an informal Reddit meetup!

Meet on Saturday, July 22nd at 5pm. Room 323C (Samsung meeting room)

RSVP here: https://goo.gl/forms/40ArnIftckBATXIm1 (for headcount estimation, no name required)

Edit: I can't spell. Convention center has no harmful convection, radiation or conduction. Humans will be safe. ",0,23
969,2017-7-22,2017,7,22,12,6ostcj,[R] Varying-Censoring Aware Matrix Factorization for Single Cell RNA-Sequencing. A dimension reduction method like PCA but aware of missing data induced variability.,https://www.reddit.com/r/MachineLearning/comments/6ostcj/r_varyingcensoring_aware_matrix_factorization_for/,[deleted],1500693452,[deleted],0,1
970,2017-7-22,2017,7,22,12,6ostoy,[R] Varying-Censoring Aware Matrix Factorization (VAMF) for Single Cell RNA-Sequencing. Introducing a dimension reduction method like PCA but aware of missing data induced variability.,https://www.reddit.com/r/MachineLearning/comments/6ostoy/r_varyingcensoring_aware_matrix_factorization/,arisbw,1500693582,,3,4
971,2017-7-22,2017,7,22,14,6otfss,[D] Text. The dream....,https://www.reddit.com/r/MachineLearning/comments/6otfss/d_text_the_dream/,[deleted],1500702518,[removed],3,0
972,2017-7-22,2017,7,22,15,6otod1,Overfitting,https://www.reddit.com/r/MachineLearning/comments/6otod1/overfitting/,aizu9,1500706605,,9,1
973,2017-7-22,2017,7,22,16,6otquf,[P] Semi-supervised captcha solver - No need to manually label a training set,https://www.reddit.com/r/MachineLearning/comments/6otquf/p_semisupervised_captcha_solver_no_need_to/,0b01,1500707828,,5,42
974,2017-7-22,2017,7,22,16,6otuwh,[R] [1707.04873] Reinforcement Learning for Architecture Search by Network Transformation &lt;-- Efficient Neural Architecture Search,https://www.reddit.com/r/MachineLearning/comments/6otuwh/r_170704873_reinforcement_learning_for/,evc123,1500709889,,1,17
975,2017-7-22,2017,7,22,17,6otyiu,"here are rules for ML to communicate clearly (first draft), please send feedback or clarification",https://www.reddit.com/r/MachineLearning/comments/6otyiu/here_are_rules_for_ml_to_communicate_clearly/,makealldigital,1500711962,[removed],0,1
976,2017-7-22,2017,7,22,17,6otzrz,[P] Tensorflow implementation of visual interaction networks,https://www.reddit.com/r/MachineLearning/comments/6otzrz/p_tensorflow_implementation_of_visual_interaction/,jaesik,1500712650,,6,41
977,2017-7-22,2017,7,22,18,6ou8av,Check out my open source neural networking library called Brainforge!,https://www.reddit.com/r/MachineLearning/comments/6ou8av/check_out_my_open_source_neural_networking/,csxeba,1500717483,[removed],0,1
978,2017-7-22,2017,7,22,19,6ouef8,[P] Check out my open source ANN library!,https://www.reddit.com/r/MachineLearning/comments/6ouef8/p_check_out_my_open_source_ann_library/,csxeba,1500720699,"[Brainforge](https://github.com/csxeba/brainforge) is an artificial neural networking library implemented in Python, only using NumPy as a dependency. Since no automatic differentiation engine is used, the gradient calculations are explicit and the source can be used to learn the math.

I am no mathematician, nor am I a programmer by profession, this project is the byproduct of my learning the art :)

Some featues of the lib:

- Backpropagation with various optimization algorithms

- Differential Evolution, as optimizer

- Reinforcement learning agents (wrappers for ANNs)

- Optional just-in-time compilation using Numba

- Numerical Gradient checking

The project has grown so big now, that it is starting to outgrow my free-time capacity. Code review and help is welcome!",10,44
979,2017-7-22,2017,7,22,23,6ov99o,Carpet Cleaning Services,https://www.reddit.com/r/MachineLearning/comments/6ov99o/carpet_cleaning_services/,gwenyosef,1500733820,,0,1
980,2017-7-23,2017,7,23,1,6ovrl5,How good is AAAI conference?,https://www.reddit.com/r/MachineLearning/comments/6ovrl5/how_good_is_aaai_conference/,[deleted],1500739604,[removed],0,1
981,2017-7-23,2017,7,23,1,6ovsgo,[D] Jefferies gives IBM Watson a Wall Street reality check,https://www.reddit.com/r/MachineLearning/comments/6ovsgo/d_jefferies_gives_ibm_watson_a_wall_street/,_alphamaximus_,1500739875,,22,74
982,2017-7-23,2017,7,23,1,6ovuuj,How is AAAI conference?,https://www.reddit.com/r/MachineLearning/comments/6ovuuj/how_is_aaai_conference/,[deleted],1500740612,[removed],0,1
983,2017-7-23,2017,7,23,1,6ovxlc,Data Humor Saturday: Spreadsheet Enthusiast Magazine,https://www.reddit.com/r/MachineLearning/comments/6ovxlc/data_humor_saturday_spreadsheet_enthusiast/,onegazillion,1500741447,,0,1
984,2017-7-23,2017,7,23,1,6ovzuz,"What are useful features for a task like ""text classification"" ?",https://www.reddit.com/r/MachineLearning/comments/6ovzuz/what_are_useful_features_for_a_task_like_text/,[deleted],1500742131,[removed],0,1
985,2017-7-23,2017,7,23,2,6ow3wy,Are there more effective strategies for exploration in RL than just adding noise?,https://www.reddit.com/r/MachineLearning/comments/6ow3wy/are_there_more_effective_strategies_for/,jstaker7,1500743304,[removed],0,1
986,2017-7-23,2017,7,23,2,6ow4pj,"Beginners Guide and Best Online Courses for Machine Learning, Artificial Intelligence, Internet of Things (IoT), NLP, Deep Learning, Big Data Analytics and Blockchain Technology",https://www.reddit.com/r/MachineLearning/comments/6ow4pj/beginners_guide_and_best_online_courses_for/,tanmoyray01,1500743543,,0,1
987,2017-7-23,2017,7,23,2,6ow7lw,What can your hearts learn from this one?,https://www.reddit.com/r/MachineLearning/comments/6ow7lw/what_can_your_hearts_learn_from_this_one/,aizu9,1500744430,,6,1
988,2017-7-23,2017,7,23,2,6owcat,[P] Tensorflow implementation of Enhanced Deep Residual Networks for Single Image Super-Resolution,https://www.reddit.com/r/MachineLearning/comments/6owcat/p_tensorflow_implementation_of_enhanced_deep/,pwatman1234455,1500745828,,6,39
989,2017-7-23,2017,7,23,3,6owg5a,DeepForge  A Modern Development Environment for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/6owg5a/deepforge_a_modern_development_environment_for/,iamtrask,1500746947,,0,1
990,2017-7-23,2017,7,23,3,6owlah,What's the best twitter sentiment analysis tool?,https://www.reddit.com/r/MachineLearning/comments/6owlah/whats_the_best_twitter_sentiment_analysis_tool/,[deleted],1500748474,[removed],0,1
991,2017-7-23,2017,7,23,4,6owzju,"Offline ,Real-Time,Face Recognition in Node.js using Python atop 99.38% accuracy dlib model on Label Faces in wild dataset.",https://www.reddit.com/r/MachineLearning/comments/6owzju/offline_realtimeface_recognition_in_nodejs_using/,malikshubham827,1500752768,,0,0
992,2017-7-23,2017,7,23,5,6ox2m1,Neuroscience-Inspired Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/6ox2m1/neuroscienceinspired_artificial_intelligence/,cognitivedemons,1500753647,,0,1
993,2017-7-23,2017,7,23,7,6oxwyb,What are some good algorithms for comparing two images?,https://www.reddit.com/r/MachineLearning/comments/6oxwyb/what_are_some_good_algorithms_for_comparing_two/,sys_sleep,1500763187,[removed],0,1
994,2017-7-23,2017,7,23,8,6oy7iv,Using convex optimization for a novel unsupervised learning method. KDD 2017.,https://www.reddit.com/r/MachineLearning/comments/6oy7iv/using_convex_optimization_for_a_novel/,sagarvare,1500766756,[removed],0,1
995,2017-7-23,2017,7,23,9,6oyec7,Leverage Machine Learning For Cybersecurity,https://www.reddit.com/r/MachineLearning/comments/6oyec7/leverage_machine_learning_for_cybersecurity/,samsan_8,1500769133,,0,1
996,2017-7-23,2017,7,23,9,6oyeqe,Maths word problem solving and visualisation,https://www.reddit.com/r/MachineLearning/comments/6oyeqe/maths_word_problem_solving_and_visualisation/,purvanshi,1500769248,,0,1
997,2017-7-23,2017,7,23,9,6oyg1w,[R] CVPR 2017 Best Paper Awards!,https://www.reddit.com/r/MachineLearning/comments/6oyg1w/r_cvpr_2017_best_paper_awards/,Neutran,1500769696,"Link to the [official announcement](http://cvpr2017.thecvf.com/program/main_conference#cvpr2017_awards).

There are two papers getting the award:

1. *Densely Connected Convolutional Networks*, i.e. DenseNet, an improvement upon ResNet by adding dense skip connections. 

2. *Learning from Simulated and Unsupervised Images through Adversarial Training*. Note that this is Apple's [very first machine learning publication](https://www.forbes.com/sites/aarontilley/2016/12/26/apple-publishes-its-first-artificial-intelligence-paper/#2866f36052f7), and it hits the jackpot! Pretty remarkable.

Congrats to the winners! Keep up the good work. ;)

",21,111
998,2017-7-23,2017,7,23,12,6oza94,"[P] [NSFW!!] 50,000 tasteful nudes with neural network search",https://www.reddit.com/r/MachineLearning/comments/6oza94/p_nsfw_50000_tasteful_nudes_with_neural_network/,driftwheeler,1500780839,[removed],36,31
999,2017-7-23,2017,7,23,14,6ozt22,ML Model for Text Extraction - Advice,https://www.reddit.com/r/MachineLearning/comments/6ozt22/ml_model_for_text_extraction_advice/,SkornRising,1500788845,[removed],0,1
1000,2017-7-23,2017,7,23,16,6p053y,MIT Tech Review: Why self-driving cars have many types of sensors,https://www.reddit.com/r/MachineLearning/comments/6p053y/mit_tech_review_why_selfdriving_cars_have_many/,adammathias,1500795146,,0,1
1001,2017-7-23,2017,7,23,18,6p0eea,CNNs: Computing the receptive field of an output neuron,https://www.reddit.com/r/MachineLearning/comments/6p0eea/cnns_computing_the_receptive_field_of_an_output/,nbacool2,1500800476,[removed],0,1
1002,2017-7-23,2017,7,23,20,6p0ugu,Commented PPO implementation,https://www.reddit.com/r/MachineLearning/comments/6p0ugu/commented_ppo_implementation/,[deleted],1500809477,[deleted],1,1
1003,2017-7-23,2017,7,23,20,6p0uj9,"Need a pointer to where I can get a more in-depth explanation of terms like: epochs, iterations, batch size",https://www.reddit.com/r/MachineLearning/comments/6p0uj9/need_a_pointer_to_where_i_can_get_a_more_indepth/,[deleted],1500809508,[removed],0,1
1004,2017-7-23,2017,7,23,21,6p13d0,[P] Commented PPO implementation,https://www.reddit.com/r/MachineLearning/comments/6p13d0/p_commented_ppo_implementation/,mks40,1500813694,,7,14
1005,2017-7-23,2017,7,23,21,6p13xq,[N] Machine Learning Weekly Review #3,https://www.reddit.com/r/MachineLearning/comments/6p13xq/n_machine_learning_weekly_review_3/,rldlml,1500813949,,0,1
1006,2017-7-23,2017,7,23,22,6p1dg1,[D](Multivariate) linear discriminate analysis,https://www.reddit.com/r/MachineLearning/comments/6p1dg1/dmultivariate_linear_discriminate_analysis/,spartan12321,1500817747,"In Introduction to statistical learning it says that for LDS we assume that each classifier contains same number of examples...why excatly is that or it's just axiomatic assumption? I am aware that al classifiers share same Cov matrix, but to reach this assumptions shouldn't they also share mean(which they don't)?",1,4
1007,2017-7-23,2017,7,23,23,6p1gc3,How Feature Engineering can help you do well in a Kaggle competition  Part III,https://www.reddit.com/r/MachineLearning/comments/6p1gc3/how_feature_engineering_can_help_you_do_well_in_a/,linekin,1500818740,,0,1
1008,2017-7-23,2017,7,23,23,6p1osk,[D] Avoiding being a trophy data scientist  Models are illuminating and wrong,https://www.reddit.com/r/MachineLearning/comments/6p1osk/d_avoiding_being_a_trophy_data_scientist_models/,_alphamaximus_,1500821651,,23,72
1009,2017-7-24,2017,7,24,1,6p22qt,[D] Is there a difference between Machine Learning and Artificial Intelligence anymore?,https://www.reddit.com/r/MachineLearning/comments/6p22qt/d_is_there_a_difference_between_machine_learning/,rantana,1500825882,"I'm seeing the words ""Artificial Intelligence"" more often on CVs/Resume where ""Machine Learning"" used to be. I'm finding it harder and harder to find a difference between ICML and NIPS/ICLR papers. ICML papers seem more like the latter these days. Is it fair to say ML and AI have become the same thing?",15,0
1010,2017-7-24,2017,7,24,1,6p26u3,Giant's drink game,https://www.reddit.com/r/MachineLearning/comments/6p26u3/giants_drink_game/,aizu9,1500827101,,5,1
1011,2017-7-24,2017,7,24,1,6p2ber,[D] End to End learning based self driving RC car,https://www.reddit.com/r/MachineLearning/comments/6p2ber/d_end_to_end_learning_based_self_driving_rc_car/,saurabhvyas3,1500828414,[removed],2,5
1012,2017-7-24,2017,7,24,2,6p2hsc,Semi supervised learning in practice,https://www.reddit.com/r/MachineLearning/comments/6p2hsc/semi_supervised_learning_in_practice/,uri-goren,1500830204,"I read a lot of articles recently that present various approaches to unsupervised and semi supervised learning. For example denoising autoencoders, variational autoencoders, GANs and some more.
They all seem very cool, and show good results. But mostly these results are either on mnist, which is to simple, or on imagenet which has a huge amount of data. The problem is that I can't find any examples of people using semi supervised learning on ""real life"" problems. 

Did anyone here ever use semi supervised learning in his work and managed to gain better results by using large amounts of unlabeled data?
Which types of algorithms do people use in the industry for this kind of problems?

Thanks
Uri",2,1
1013,2017-7-24,2017,7,24,2,6p2pzc,Does anyone have Prec\Recall Curves of Detection Architectures from leading papers?,https://www.reddit.com/r/MachineLearning/comments/6p2pzc/does_anyone_have_precrecall_curves_of_detection/,idan_bassuk,1500832472,[removed],0,1
1014,2017-7-24,2017,7,24,3,6p2tu4,"July 23, 2017 at 06:05PM",https://www.reddit.com/r/MachineLearning/comments/6p2tu4/july_23_2017_at_0605pm/,[deleted],1500833539,[removed],0,1
1015,2017-7-24,2017,7,24,3,6p2v4q,[P] Inception-ResNet v2 model using Keras (with weight files),https://www.reddit.com/r/MachineLearning/comments/6p2v4q/p_inceptionresnet_v2_model_using_keras_with/,myutwo150,1500833894,,0,23
1016,2017-7-24,2017,7,24,3,6p306r,"A paradox of RBM is despite weights being symmetric, it learns asymmetric sparsity depending on inference up vs down. How could we design a norm func to fix this without using bias?",https://www.reddit.com/r/MachineLearning/comments/6p306r/a_paradox_of_rbm_is_despite_weights_being/,[deleted],1500835256,[removed],0,1
1017,2017-7-24,2017,7,24,3,6p32ik,"[D] A paradox of RBM is despite weights being symmetric, it learns asymmetric sparsity depending on inference up vs down, which reduces how much it can remember. How could we design a norm func to fix this without using bias?",https://www.reddit.com/r/MachineLearning/comments/6p32ik/d_a_paradox_of_rbm_is_despite_weights_being/,BenRayfield,1500835923,"RBM zigzags inference up down and repeat. This can be all the way down then all the way up (predicting or learning) or zigzagging per layer (learning). This is supposed to converge to symmetric behavior, but it appears theres always some asymmetric distortion.

Node bias is often used to adjust for this, but technically anything you can do with node bias you can do without it. Something appears unbalanced that motivated complicating the boltzmann machine energy equation with the bias vector.

Example: If the first node in each layer is always on because a huge weight is between them, then nodes in the next higher or lower layer can have weights that always raise or lower the chance they are on regardless of combinations of other nodes (like node bias is used for).

Example: If layerX usually has 20% of its nodes on and layerY learns to have only 10% of its nodes on (most weights negative), then 10% is so small that weightedSums are usually near 0 so sigmoid of that is usually near .5 so inference back to layerX gets around 50% of its nodes on and cant learn to do 20%. Unless its normed against this.

I imagine such a func could somehow be designed using statistics of how often each pair of nodes, 1 in each adjacent layer, are all 4 combinations (0 0, 0 1, 1 0, 1 1) and sparsity ratio (observed between adjacent layers) and weights. The weights of 1 node dont tell statistics of the node since other nodes past those weights (such as inference down then up to siblings) affect the combinations they're on and how often they're on. You may have a very high weight with some node thats rarely on, so you cant tell from 1 node's weights.",0,6
1018,2017-7-24,2017,7,24,3,6p32vg,[D] Intel Movidius Neural Compute Stick - On Mouser  r/iotml,https://www.reddit.com/r/MachineLearning/comments/6p32vg/d_intel_movidius_neural_compute_stick_on_mouser/,onegazillion,1500836022,,0,5
1019,2017-7-24,2017,7,24,4,6p3fxz,[P] Tool for Designing Energy-Efficient Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/6p3fxz/p_tool_for_designing_energyefficient_deep_neural/,breandan,1500839629,,0,14
1020,2017-7-24,2017,7,24,5,6p3ha7,[D] Machine Learning - WAYR (What Are You Reading) - Week 30,https://www.reddit.com/r/MachineLearning/comments/6p3ha7/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,1500840004,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|
|----|-----|-----|
|[Week 1](https://www.reddit.com/r/MachineLearning/comments/4qyjiq/machine_learning_wayr_what_are_you_reading_week_1/)|[Week 11](https://www.reddit.com/r/MachineLearning/comments/57xw56/discussion_machine_learning_wayr_what_are_you/)|[Week 21](https://www.reddit.com/r/MachineLearning/comments/60ildf/d_machine_learning_wayr_what_are_you_reading_week/)|
|[Week 2](https://www.reddit.com/r/MachineLearning/comments/4s2xqm/machine_learning_wayr_what_are_you_reading_week_2/)|[Week 12](https://www.reddit.com/r/MachineLearning/comments/5acb1t/d_machine_learning_wayr_what_are_you_reading_week/)|[Week 22](https://www.reddit.com/r/MachineLearning/comments/64jwde/d_machine_learning_wayr_what_are_you_reading_week/)||
|[Week 3](https://www.reddit.com/r/MachineLearning/comments/4t7mqm/machine_learning_wayr_what_are_you_reading_week_3/)|[Week 13](https://www.reddit.com/r/MachineLearning/comments/5cwfb6/d_machine_learning_wayr_what_are_you_reading_week/)|[Week 23](https://www.reddit.com/r/MachineLearning/comments/674331/d_machine_learning_wayr_what_are_you_reading_week/)||
|[Week 4](https://www.reddit.com/r/MachineLearning/comments/4ub2kw/machine_learning_wayr_what_are_you_reading_week_4/)|[Week 14](https://www.reddit.com/r/MachineLearning/comments/5fc5mh/d_machine_learning_wayr_what_are_you_reading_week/)|[Week 24](https://www.reddit.com/r/MachineLearning/comments/68hhhb/d_machine_learning_wayr_what_are_you_reading_week/)||
|[Week 5](https://www.reddit.com/r/MachineLearning/comments/4xomf7/machine_learning_wayr_what_are_you_reading_week_5/)|[Week 15](https://www.reddit.com/r/MachineLearning/comments/5hy4ur/d_machine_learning_wayr_what_are_you_reading_week/)|[Week 25](https://www.reddit.com/r/MachineLearning/comments/69teiz/d_machine_learning_wayr_what_are_you_reading_week/)||
|[Week 6](https://www.reddit.com/r/MachineLearning/comments/4zcyvk/machine_learning_wayr_what_are_you_reading_week_6/)|[Week 16](https://www.reddit.com/r/MachineLearning/comments/5kd6vd/d_machine_learning_wayr_what_are_you_reading_week/)|[Week 26](https://www.reddit.com/r/MachineLearning/comments/6d7nb1/d_machine_learning_wayr_what_are_you_reading_week/)||
|[Week 7](https://www.reddit.com/r/MachineLearning/comments/52t6mo/machine_learning_wayr_what_are_you_reading_week_7/)|[Week 17](https://www.reddit.com/r/MachineLearning/comments/5ob7dx/discussion_machine_learning_wayr_what_are_you/)|[Week 27](https://www.reddit.com/r/MachineLearning/comments/6gngwc/d_machine_learning_wayr_what_are_you_reading_week/)||
|[Week 8](https://www.reddit.com/r/MachineLearning/comments/53heol/machine_learning_wayr_what_are_you_reading_week_8/)|[Week 18](https://www.reddit.com/r/MachineLearning/comments/5r14yd/discussion_machine_learning_wayr_what_are_you/)|[Week 28](https://www.reddit.com/r/MachineLearning/comments/6jgdva/d_machine_learning_wayr_what_are_you_reading_week/)||
|[Week 9](https://www.reddit.com/r/MachineLearning/comments/54kvsu/machine_learning_wayr_what_are_you_reading_week_9/)|[Week 19](https://www.reddit.com/r/MachineLearning/comments/5tt9cz/discussion_machine_learning_wayr_what_are_you/)|[Week 29](https://www.reddit.com/r/MachineLearning/comments/6m9l1v/d_machine_learning_wayr_what_are_you_reading_week/)||
|[Week 10](https://www.reddit.com/r/MachineLearning/comments/56s2oa/discussion_machine_learning_wayr_what_are_you/)|[Week 20](https://www.reddit.com/r/MachineLearning/comments/5wh2wb/d_machine_learning_wayr_what_are_you_reading_week/)||

Most upvoted papers two weeks ago:

/u/jvmancuso: [Noisy Networks for Exploration!](https://arxiv.org/abs/1706.10295)

/u/Puzzel: [pix2pix](https://arxiv.org/pdf/1611.07004v1.pdf)

Besides that, there are no rules, have fun.",3,9
1021,2017-7-24,2017,7,24,5,6p3isq,Soon... https://youtu.be/umRdt3zGgpU,https://www.reddit.com/r/MachineLearning/comments/6p3isq/soon_httpsyoutubeumrdt3zggpu/,[deleted],1500840416,[deleted],0,1
1022,2017-7-24,2017,7,24,5,6p3pcx,Benchmark leaderboard?,https://www.reddit.com/r/MachineLearning/comments/6p3pcx/benchmark_leaderboard/,bkj__,1500842247,[removed],0,1
1023,2017-7-24,2017,7,24,5,6p3s0j,Looking for up to date blog post on getting set up with GPU accelerated deep learning using Keras,https://www.reddit.com/r/MachineLearning/comments/6p3s0j/looking_for_up_to_date_blog_post_on_getting_set/,rutherfordofman,1500843000,[removed],0,1
1024,2017-7-24,2017,7,24,5,6p3s6j,[P] DeepArtistry: Buy art generated by neural networks,https://www.reddit.com/r/MachineLearning/comments/6p3s6j/p_deepartistry_buy_art_generated_by_neural/,[deleted],1500843051,[deleted],2,1
1025,2017-7-24,2017,7,24,7,6p46sy,[N] New AI research institute,https://www.reddit.com/r/MachineLearning/comments/6p46sy/n_new_ai_research_institute/,Jakobovski,1500847284,,7,4
1026,2017-7-24,2017,7,24,7,6p4ff5,[D] Where else do you discuss machine learning?,https://www.reddit.com/r/MachineLearning/comments/6p4ff5/d_where_else_do_you_discuss_machine_learning/,lahwran_,1500849838,"This subreddit seems to be something of a hub for the machine learning research community. what other hubs are there that you research folks frequent, besides conferences, your research team, and the arxiv?",32,24
1027,2017-7-24,2017,7,24,8,6p4ot1,[D] Getting to Why - Practical Solutions to Audit Algorithms,https://www.reddit.com/r/MachineLearning/comments/6p4ot1/d_getting_to_why_practical_solutions_to_audit/,rutherfordofman,1500852749,,3,1
1028,2017-7-24,2017,7,24,8,6p4qze,Converting object annotation to XML format,https://www.reddit.com/r/MachineLearning/comments/6p4qze/converting_object_annotation_to_xml_format/,pnambiar,1500853408,[removed],0,1
1029,2017-7-24,2017,7,24,10,6p5c2o,[R] Bayesian Neural Networks with Random Inputs for Model Based Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/6p5c2o/r_bayesian_neural_networks_with_random_inputs_for/,hardmaru,1500860151,,6,74
1030,2017-7-24,2017,7,24,11,6p5jwh,Opinions on Computational Linear Algebra by Fast.ai,https://www.reddit.com/r/MachineLearning/comments/6p5jwh/opinions_on_computational_linear_algebra_by_fastai/,Abhishtoo,1500862755,[removed],0,1
1031,2017-7-24,2017,7,24,11,6p5mug,[D]Choosing right aws instance.,https://www.reddit.com/r/MachineLearning/comments/6p5mug/dchoosing_right_aws_instance/,jassi1994,1500863741,"Recently I have started competing on Kaggle and thus dealing with datasets of size ~1GB.Most of the models that I run are CART(Classification and regression trees).I am running these models on my machine(MacBook Air,1.4 GHz Intel Core i5, 4 GB 1600 MHz DDR3, Intel HD Graphics 5000 1536 MB,120GB Flash storage).Typical hyperparameter tuning takes a long(very long) time to run.I am not doing anything related to deep learning.

So I decided to shift my computation to AWS.But there are a plethora of instances to choose from.Anybody with sufficient experience using these can suggest what configuration should I go for?Which instance should I choose?

Any question for further clarification about my use case is welcomed.",8,4
1032,2017-7-24,2017,7,24,13,6p6477,Fake ID detector using GAN?,https://www.reddit.com/r/MachineLearning/comments/6p6477/fake_id_detector_using_gan/,[deleted],1500869872,[removed],0,1
1033,2017-7-24,2017,7,24,15,6p6qmc,[D] Some advice for journalists writing about artificial intelligence,https://www.reddit.com/r/MachineLearning/comments/6p6qmc/d_some_advice_for_journalists_writing_about/,lxpz,1500879073,,8,64
1034,2017-7-24,2017,7,24,16,6p6yrj,DQN on Torcs with ResNet-18,https://www.reddit.com/r/MachineLearning/comments/6p6yrj/dqn_on_torcs_with_resnet18/,[deleted],1500882660,[deleted],0,1
1035,2017-7-24,2017,7,24,17,6p704x,Deep Q-learning on Torcs with Resnet-18,https://www.reddit.com/r/MachineLearning/comments/6p704x/deep_qlearning_on_torcs_with_resnet18/,[deleted],1500883320,[deleted],0,1
1036,2017-7-24,2017,7,24,17,6p71i2,[P] Deep Q-learning on Torcs with Resnet-18,https://www.reddit.com/r/MachineLearning/comments/6p71i2/p_deep_qlearning_on_torcs_with_resnet18/,imagry_s10,1500884012,,2,2
1037,2017-7-24,2017,7,24,17,6p71uj,[R] A Distributional Perspective on Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/6p71uj/r_a_distributional_perspective_on_reinforcement/,Kaixhin,1500884179,,7,73
1038,2017-7-24,2017,7,24,18,6p7dak,How to upscale/enhance GAN images?,https://www.reddit.com/r/MachineLearning/comments/6p7dak/how_to_upscaleenhance_gan_images/,sirkloda,1500889485,[removed],0,1
1039,2017-7-24,2017,7,24,18,6p7f4v,Looking for an internship? Visit VW Data:Lab AI research for a month: challenge on deep learning and robotics. Also: Oktoberfest.,https://www.reddit.com/r/MachineLearning/comments/6p7f4v/looking_for_an_internship_visit_vw_datalab_ai/,sieisteinmodel,1500890371,,0,3
1040,2017-7-24,2017,7,24,19,6p7hbd,[P] A Practical Guide to Tree Based Learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/6p7hbd/p_a_practical_guide_to_tree_based_learning/,pmigdal,1500891317,,13,174
1041,2017-7-24,2017,7,24,19,6p7j7t,Best Machine Learning Courses,https://www.reddit.com/r/MachineLearning/comments/6p7j7t/best_machine_learning_courses/,waynebruce1,1500892150,,0,1
1042,2017-7-24,2017,7,24,20,6p7ohp,Utilizing Machine Learning requires restating your problem differently,https://www.reddit.com/r/MachineLearning/comments/6p7ohp/utilizing_machine_learning_requires_restating/,ajaysaini_sgvu,1500894385,,0,1
1043,2017-7-24,2017,7,24,21,6p80i7,DeepArtistry: Buy art generated by neural networks.,https://www.reddit.com/r/MachineLearning/comments/6p80i7/deepartistry_buy_art_generated_by_neural_networks/,[deleted],1500898915,[deleted],0,1
1044,2017-7-24,2017,7,24,21,6p82tw,8 channel automatic capsule tablet counting filling machine manufacturer...,https://www.reddit.com/r/MachineLearning/comments/6p82tw/8_channel_automatic_capsule_tablet_counting/,hymachinery,1500899728,,0,1
1045,2017-7-24,2017,7,24,21,6p84el,Evaluate chatbots for the Conversational Intelligence Challenge at NIPS,https://www.reddit.com/r/MachineLearning/comments/6p84el/evaluate_chatbots_for_the_conversational/,phreeza,1500900262,,0,1
1046,2017-7-24,2017,7,24,22,6p87fm,[D] Beginner level ML projects(with solutions) in python,https://www.reddit.com/r/MachineLearning/comments/6p87fm/d_beginner_level_ml_projectswith_solutions_in/,spartan12321,1500901228,"I know pandas, numpy, matplotlib and scikit learn among with fundamental knowledge of how ML algorithms work. Can anyone please recommend me website or book where I can practice with ML project examples, preferable with solutions as I am self learning, or at least hint me in which direction should I go as I don't really know where to pregress.

",2,0
1047,2017-7-24,2017,7,24,22,6p880l,[P] Industry-specific deep learning interviews and walkthroughs,https://www.reddit.com/r/MachineLearning/comments/6p880l/p_industryspecific_deep_learning_interviews_and/,[deleted],1500901389,[removed],0,1
1048,2017-7-24,2017,7,24,22,6p88la,Experiment: simple conditioning of WGANs,https://www.reddit.com/r/MachineLearning/comments/6p88la/experiment_simple_conditioning_of_wgans/,[deleted],1500901558,[deleted],0,1
1049,2017-7-24,2017,7,24,22,6p88ps,[P] Experiment: simple conditioning of WGANs,https://www.reddit.com/r/MachineLearning/comments/6p88ps/p_experiment_simple_conditioning_of_wgans/,richardweiss,1500901596,,4,39
1050,2017-7-24,2017,7,24,22,6p8bss,TensorFlow to test your pee,https://www.reddit.com/r/MachineLearning/comments/6p8bss/tensorflow_to_test_your_pee/,lowercase0,1500902576,,0,1
1051,2017-7-24,2017,7,24,22,6p8cr7,R code and reproducible model development with DVC  data version control,https://www.reddit.com/r/MachineLearning/comments/6p8cr7/r_code_and_reproducible_model_development_with/,dmpetrov,1500902875,,0,1
1052,2017-7-24,2017,7,24,22,6p8fnb,[shameless plug] background segmentation application. feedback kindly appreciated.,https://www.reddit.com/r/MachineLearning/comments/6p8fnb/shameless_plug_background_segmentation/,atelist,1500903728,,1,1
1053,2017-7-24,2017,7,24,22,6p8hcs,[D] Floating point precision in deep learning,https://www.reddit.com/r/MachineLearning/comments/6p8hcs/d_floating_point_precision_in_deep_learning/,tryndisskilled,1500904237,"I'm actually starting a thread to start a discussion about FP16 vs FP32 vs FP64 or other floating point precisions (from [here](https://www.reddit.com/r/MachineLearning/comments/6op6ak/d_how_viable_building_a_50_teraflops_amd_vega/))

I'll just paste the comment I made in the linked thread:

Basically when I got into DL I started by using tensorflow/keras, and when I dug a bit deeper I saw that all variables were stored in FP32 by default. My first question is: why FP32? Is there actually any reason at all? I understand there must be a tradeoff between the high-cost but very precise FP64 and fast but less precise (?) FP16.

Then I started to search a bit about this, and I stumbled across different papers/articles stating that FP16 actually could give similar results than FP32, but faster. [[1]](https://arxiv.org/abs/1412.7024)

There is another issue related: the bottleneck from Nvidia on their consumer cards for FP16... I bought a 1080ti and tried switching to FP16 from FP32 for a training session and was like ""wait what?"" since it was much slower.

I'd love to hear your take on this: did you ever consider this parameter/what did you choose/why do you think many libraries default to FP32",15,9
1054,2017-7-24,2017,7,24,22,6p8j9h,[N] CNTK + LSTM + kinect v2 = Face analysis,https://www.reddit.com/r/MachineLearning/comments/6p8j9h/n_cntk_lstm_kinect_v2_face_analysis/,pmfcdb,1500904789,,2,1
1055,2017-7-24,2017,7,24,23,6p8r1w,[D] Deep learning setup on Windows 10,https://www.reddit.com/r/MachineLearning/comments/6p8r1w/d_deep_learning_setup_on_windows_10/,[deleted],1500906896,[removed],7,0
1056,2017-7-24,2017,7,24,23,6p8vpb,[1707.06600] A multi-agent reinforcement learning model of common-pool resource appropriation,https://www.reddit.com/r/MachineLearning/comments/6p8vpb/170706600_a_multiagent_reinforcement_learning/,[deleted],1500908117,[deleted],0,1
1057,2017-7-25,2017,7,25,1,6p9dwa,"July 24, 2017 at 04:10PM",https://www.reddit.com/r/MachineLearning/comments/6p9dwa/july_24_2017_at_0410pm/,[deleted],1500912743,[removed],0,1
1058,2017-7-25,2017,7,25,1,6p9hin,"July 24, 2017 at 04:12PM",https://www.reddit.com/r/MachineLearning/comments/6p9hin/july_24_2017_at_0412pm/,[deleted],1500913646,[removed],0,1
1059,2017-7-25,2017,7,25,1,6p9laq,TensorFlow Tutorial For Beginners  Karlijn Willems  Medium,https://www.reddit.com/r/MachineLearning/comments/6p9laq/tensorflow_tutorial_for_beginners_karlijn_willems/,pmz,1500914589,,0,1
1060,2017-7-25,2017,7,25,1,6p9lv3,[R] [1707.06600] A multi-agent reinforcement learning model of common-pool resource appropriation,https://www.reddit.com/r/MachineLearning/comments/6p9lv3/r_170706600_a_multiagent_reinforcement_learning/,enderwagon,1500914725,,0,9
1061,2017-7-25,2017,7,25,1,6p9onk,This Week in Machine Learning &amp; AI interviews Sergey Levine of UC Berkeley,https://www.reddit.com/r/MachineLearning/comments/6p9onk/this_week_in_machine_learning_ai_interviews/,[deleted],1500915403,[deleted],0,1
1062,2017-7-25,2017,7,25,2,6p9s39,This Week in Machine Learning &amp; AI presents Deep Robotic Learning feat Sergey Levine of UC Berkeley,https://www.reddit.com/r/MachineLearning/comments/6p9s39/this_week_in_machine_learning_ai_presents_deep/,Fatman_Johnson,1500916204,,0,1
1063,2017-7-25,2017,7,25,2,6p9wek,Running an AI Startup and the Future of Deep NLP - Alex Lamb Interviews Daniel Jiwoong Im,https://www.reddit.com/r/MachineLearning/comments/6p9wek/running_an_ai_startup_and_the_future_of_deep_nlp/,[deleted],1500917245,[deleted],0,1
1064,2017-7-25,2017,7,25,2,6p9wz4,[D] Running an AI Startup and the Future of Deep NLP - Alex Lamb Interviews Daniel Jiwoong Im,https://www.reddit.com/r/MachineLearning/comments/6p9wz4/d_running_an_ai_startup_and_the_future_of_deep/,alexmlamb,1500917392,,16,42
1065,2017-7-25,2017,7,25,2,6p9x02,[D] NIPS rebuttal period is open!,https://www.reddit.com/r/MachineLearning/comments/6p9x02/d_nips_rebuttal_period_is_open/,zergylord,1500917402,"Thought this would be a nice place for people to discuss the rebuttal process / vent about reviews. I, for one, am curious how much this part of the process has mattered historical. How common is a reject review switched to an accept? Can a poor rebuttal lower review scores?",88,36
1066,2017-7-25,2017,7,25,2,6p9xjv,Building Reinforcement Learning datasets with emoji reactions,https://www.reddit.com/r/MachineLearning/comments/6p9xjv/building_reinforcement_learning_datasets_with/,somnophobiac,1500917527,,0,1
1067,2017-7-25,2017,7,25,2,6pa3vu,"July 24, 2017 at 05:47PM",https://www.reddit.com/r/MachineLearning/comments/6pa3vu/july_24_2017_at_0547pm/,[deleted],1500919050,[removed],0,1
1068,2017-7-25,2017,7,25,3,6pa7r3,[P] Intelligent Topic Detection with Unsupervised Learning,https://www.reddit.com/r/MachineLearning/comments/6pa7r3/p_intelligent_topic_detection_with_unsupervised/,primaryobjects,1500919943,,2,10
1069,2017-7-25,2017,7,25,4,6pam5g,Model Suggestions for Sparse Data,https://www.reddit.com/r/MachineLearning/comments/6pam5g/model_suggestions_for_sparse_data/,ml_bot_man,1500923403,[removed],0,1
1070,2017-7-25,2017,7,25,5,6pb510,[P] Industry-specific deep learning interviews and walkthroughs newsletter,https://www.reddit.com/r/MachineLearning/comments/6pb510/p_industryspecific_deep_learning_interviews_and/,[deleted],1500928068,[removed],0,1
1071,2017-7-25,2017,7,25,6,6pbdz7,Road To AI 5 - Artificial Neural Network in Python,https://www.reddit.com/r/MachineLearning/comments/6pbdz7/road_to_ai_5_artificial_neural_network_in_python/,[deleted],1500930298,[deleted],0,1
1072,2017-7-25,2017,7,25,6,6pbkak,[x-post r/TensorFlow] Lost with trying to understand how TensorFlow receives and processes input,https://www.reddit.com/r/MachineLearning/comments/6pbkak/xpost_rtensorflow_lost_with_trying_to_understand/,JustinQueeber,1500931953,,0,1
1073,2017-7-25,2017,7,25,7,6pbrqy,How many neurons could i calculate with a raspberry pi 3?,https://www.reddit.com/r/MachineLearning/comments/6pbrqy/how_many_neurons_could_i_calculate_with_a/,ToplessTopmodel,1500933821,[removed],0,1
1074,2017-7-25,2017,7,25,7,6pbx5a,"July 24, 2017 at 10:23PM",https://www.reddit.com/r/MachineLearning/comments/6pbx5a/july_24_2017_at_1023pm/,[deleted],1500935258,[removed],0,1
1075,2017-7-25,2017,7,25,7,6pc40e,Submanifold Sparse Convolutional Networks,https://www.reddit.com/r/MachineLearning/comments/6pc40e/submanifold_sparse_convolutional_networks/,fnbr,1500937120,,1,1
1076,2017-7-25,2017,7,25,10,6pcurc,[R] Learning Transferable Architectures for Scalable Image Recognition,https://www.reddit.com/r/MachineLearning/comments/6pcurc/r_learning_transferable_architectures_for/,xternalz,1500944942,,9,26
1077,2017-7-25,2017,7,25,10,6pcv5c,How would you model startup success?,https://www.reddit.com/r/MachineLearning/comments/6pcv5c/how_would_you_model_startup_success/,wintron,1500945064,[removed],0,1
1078,2017-7-25,2017,7,25,12,6pdi1z,[D] Are there any tools for getting deep learning models mathematical equation?,https://www.reddit.com/r/MachineLearning/comments/6pdi1z/d_are_there_any_tools_for_getting_deep_learning/,commafighter,1500952200,"is there a way to get deep learning models equation especially in keras. For example consider a neural network with one input and one output. the model looks some thing like this `y=f(mx)` where y is predicted value, x is the input value and m the parmaeter. For smaller models we can use pen and paper to solve this but larger models with 100k parameters its not possible. are there tools which solves this problem?",8,4
1079,2017-7-25,2017,7,25,13,6pdxqp,Optimizing response of black box model with features that are dependent on group characteristics?,https://www.reddit.com/r/MachineLearning/comments/6pdxqp/optimizing_response_of_black_box_model_with/,Ucspe,1500957626,[removed],0,1
1080,2017-7-25,2017,7,25,14,6pe8h8,Can GAN Learn Topological Features of a Graph?,https://www.reddit.com/r/MachineLearning/comments/6pe8h8/can_gan_learn_topological_features_of_a_graph/,jakez001,1500961779,[removed],0,1
1081,2017-7-25,2017,7,25,14,6pe9ok,[P] mlmodelzoo.com  deep learning models on mobile,https://www.reddit.com/r/MachineLearning/comments/6pe9ok/p_mlmodelzoocom_deep_learning_models_on_mobile/,johnstreet6688,1500962256,,1,9
1082,2017-7-25,2017,7,25,17,6peu2g,"[P] Industry-specific deep learning interviews and walkthroughs newsletter eingereicht vor 11 Stunden von deepdreamshop Bi-weekly newsletter how deep learning is used in specific industries like autonomous vehicles, drug discovery, energy, drones, medical imaging. Subscribe here.",https://www.reddit.com/r/MachineLearning/comments/6peu2g/p_industryspecific_deep_learning_interviews_and/,[deleted],1500971221,[deleted],0,1
1083,2017-7-25,2017,7,25,17,6peuo6,Program Traces One Class,https://www.reddit.com/r/MachineLearning/comments/6peuo6/program_traces_one_class/,[deleted],1500971519,[removed],0,1
1084,2017-7-25,2017,7,25,17,6pev98,[P] Industry-specific deep learning interviews and walkthroughs newsletter,https://www.reddit.com/r/MachineLearning/comments/6pev98/p_industryspecific_deep_learning_interviews_and/,[deleted],1500971835,[deleted],2,0
1085,2017-7-25,2017,7,25,17,6pex5c,is it hard to create very good anti fraud system for CPA network?,https://www.reddit.com/r/MachineLearning/comments/6pex5c/is_it_hard_to_create_very_good_anti_fraud_system/,not0patient,1500972785,[removed],0,1
1086,2017-7-25,2017,7,25,18,6pf3r5,How do i organize my table if i want to use SVM regression analysis to predict the effectiveness of X on 8 variables from 6 other variables?,https://www.reddit.com/r/MachineLearning/comments/6pf3r5/how_do_i_organize_my_table_if_i_want_to_use_svm/,[deleted],1500975856,[removed],0,1
1087,2017-7-25,2017,7,25,18,6pf4f1,[D] A metric choice for the classification of random processes,https://www.reddit.com/r/MachineLearning/comments/6pf4f1/d_a_metric_choice_for_the_classification_of/,HichamEB,1500976165,"Hello all,

I just posted an article on medium where I go over usual metrics for classification tasks then propose an alternative for the cases where the predicted targets are stochastic. I would love to have your comments about it.

Here is the link : https://medium.com/@kwykedu/metrics-random-processes-in-classification-fd5bafa79505",0,1
1088,2017-7-25,2017,7,25,20,6pfjqe,Not Hot Dog: Exploring the Accuracy Paradox,https://www.reddit.com/r/MachineLearning/comments/6pfjqe/not_hot_dog_exploring_the_accuracy_paradox/,mands,1500982498,,0,1
1089,2017-7-25,2017,7,25,20,6pfjv8,How to do One-to-Many in Keras,https://www.reddit.com/r/MachineLearning/comments/6pfjv8/how_to_do_onetomany_in_keras/,[deleted],1500982556,[removed],0,1
1090,2017-7-25,2017,7,25,20,6pflre,[D] How to do One-to-Many in Keras?,https://www.reddit.com/r/MachineLearning/comments/6pflre/d_how_to_do_onetomany_in_keras/,trias10,1500983265,[removed],2,1
1091,2017-7-25,2017,7,25,20,6pfmcf,Top 3 Breakthroughs in Combating Financial Crime,https://www.reddit.com/r/MachineLearning/comments/6pfmcf/top_3_breakthroughs_in_combating_financial_crime/,deepak1232,1500983504,,0,1
1092,2017-7-25,2017,7,25,21,6pfqzw,How real businesses are using machine learning,https://www.reddit.com/r/MachineLearning/comments/6pfqzw/how_real_businesses_are_using_machine_learning/,ahmedRebai,1500985118,,0,1
1093,2017-7-25,2017,7,25,21,6pfsqc,Neuronal network architecture,https://www.reddit.com/r/MachineLearning/comments/6pfsqc/neuronal_network_architecture/,ToplessTopmodel,1500985706,[removed],0,1
1094,2017-7-25,2017,7,25,21,6pfsyk,[P] 37 Reasons why your NN is not working,https://www.reddit.com/r/MachineLearning/comments/6pfsyk/p_37_reasons_why_your_nn_is_not_working/,slavivanov,1500985781,,25,306
1095,2017-7-25,2017,7,25,22,6pfyh4,Why doesn't my posts appear in new? Am i Shadow banned?,https://www.reddit.com/r/MachineLearning/comments/6pfyh4/why_doesnt_my_posts_appear_in_new_am_i_shadow/,[deleted],1500987640,[removed],0,1
1096,2017-7-25,2017,7,25,22,6pfze7,Image Captioning using InceptionV3 and Beam Search,https://www.reddit.com/r/MachineLearning/comments/6pfze7/image_captioning_using_inceptionv3_and_beam_search/,yashkatariya,1500987919,,0,1
1097,2017-7-25,2017,7,25,22,6pg0ai,[R] Adversarial Sets for Regularising Neural Link Predictors,https://www.reddit.com/r/MachineLearning/comments/6pg0ai/r_adversarial_sets_for_regularising_neural_link/,_rockt,1500988209,,0,5
1098,2017-7-25,2017,7,25,22,6pg25d,Is it OK to use such 3D images for machine learning?,https://www.reddit.com/r/MachineLearning/comments/6pg25d/is_it_ok_to_use_such_3d_images_for_machine/,wjwwjw,1500988789,[removed],0,1
1099,2017-7-25,2017,7,25,22,6pg72k,[N] Google Released Facets: A Visualisation Tool Ideal For Inspecting Data And Training Results.,https://www.reddit.com/r/MachineLearning/comments/6pg72k/n_google_released_facets_a_visualisation_tool/,Dutchcheesehead,1500990243,,0,1
1100,2017-7-25,2017,7,25,23,6pgdd0,[R] A data independent approach to universal adversarial perturbations,https://www.reddit.com/r/MachineLearning/comments/6pgdd0/r_a_data_independent_approach_to_universal/,ug96,1500991923,,4,13
1101,2017-7-25,2017,7,25,23,6pgdew,Recommended sources to get a better understanding of ML algorithms?,https://www.reddit.com/r/MachineLearning/comments/6pgdew/recommended_sources_to_get_a_better_understanding/,ChiriacAttack,1500991935,[removed],0,1
1102,2017-7-25,2017,7,25,23,6pgjcr,Facebook like Face recognition Project,https://www.reddit.com/r/MachineLearning/comments/6pgjcr/facebook_like_face_recognition_project/,anubhavshrimal,1500993536,,0,1
1103,2017-7-25,2017,7,25,23,6pgjst,[Ask] Does TensorFlow allow for learning from Bayesian Networks?,https://www.reddit.com/r/MachineLearning/comments/6pgjst/ask_does_tensorflow_allow_for_learning_from/,rekthard,1500993644,[removed],0,1
1104,2017-7-25,2017,7,25,23,6pgly1,How to make a racist AI without really trying,https://www.reddit.com/r/MachineLearning/comments/6pgly1/how_to_make_a_racist_ai_without_really_trying/,[deleted],1500994207,[deleted],0,1
1105,2017-7-26,2017,7,26,0,6pgtrh,[P] A Slack channel for ML researchers &amp; developers in the Toronto-Waterloo corridor,https://www.reddit.com/r/MachineLearning/comments/6pgtrh/p_a_slack_channel_for_ml_researchers_developers/,aato,1500996144,,5,0
1106,2017-7-26,2017,7,26,0,6pgv0z,[xpost from /r/artificial] AI hackathon with $17 in prizes,https://www.reddit.com/r/MachineLearning/comments/6pgv0z/xpost_from_rartificial_ai_hackathon_with_17_in/,activeparty,1500996468,,0,1
1107,2017-7-26,2017,7,26,0,6pgwyr,"[D] CS231n: ""any preprocessing statistics (e.g. the data mean) must only be computed on the training data, and then applied to the validation / test data.""",https://www.reddit.com/r/MachineLearning/comments/6pgwyr/d_cs231n_any_preprocessing_statistics_eg_the_data/,[deleted],1500996966,[removed],3,2
1108,2017-7-26,2017,7,26,0,6pgz4j,Assessing Retail Employee Risk Through Unsupervised Learning Techniques,https://www.reddit.com/r/MachineLearning/comments/6pgz4j/assessing_retail_employee_risk_through/,[deleted],1500997506,[deleted],0,1
1109,2017-7-26,2017,7,26,0,6pgz6w,A Step-by-Step Guide to Synthesizing Adversarial Examples,https://www.reddit.com/r/MachineLearning/comments/6pgz6w/a_stepbystep_guide_to_synthesizing_adversarial/,[deleted],1500997523,[deleted],0,1
1110,2017-7-26,2017,7,26,0,6pgze1,[P] A Step-by-Step Guide to Synthesizing Adversarial Examples,https://www.reddit.com/r/MachineLearning/comments/6pgze1/p_a_stepbystep_guide_to_synthesizing_adversarial/,anishathalye,1500997567,,5,58
1111,2017-7-26,2017,7,26,1,6ph62z,[R] Assessing Retail Employee Risk Through Unsupervised Learning Techniques,https://www.reddit.com/r/MachineLearning/comments/6ph62z/r_assessing_retail_employee_risk_through/,borowcm,1500999136,,0,4
1112,2017-7-26,2017,7,26,2,6phx6t,An Introduction to different Types of Convolutions in Deep Learning,https://www.reddit.com/r/MachineLearning/comments/6phx6t/an_introduction_to_different_types_of/,gopietz,1501005541,,0,1
1113,2017-7-26,2017,7,26,3,6pi5er,"[N] Serena Yeung, PhD Student, Stanford at MLconf Seattle 2017",https://www.reddit.com/r/MachineLearning/comments/6pi5er/n_serena_yeung_phd_student_stanford_at_mlconf/,mlconf,1501007493,,0,2
1114,2017-7-26,2017,7,26,4,6picbl,[P] [1704.03453] The Space of Transferable Adversarial Examples,https://www.reddit.com/r/MachineLearning/comments/6picbl/p_170403453_the_space_of_transferable_adversarial/,fnbr,1501009213,,0,2
1115,2017-7-26,2017,7,26,4,6pid3t,Bot Stash - A curated directory of chat bots resources &amp; tools,https://www.reddit.com/r/MachineLearning/comments/6pid3t/bot_stash_a_curated_directory_of_chat_bots/,ahmedRebai,1501009436,,0,0
1116,2017-7-26,2017,7,26,4,6pidgm,Barely a day since the tweet and the media is already pitting it as a showdown between the two,https://www.reddit.com/r/MachineLearning/comments/6pidgm/barely_a_day_since_the_tweet_and_the_media_is/,qwer7y,1501009530,,0,1
1117,2017-7-26,2017,7,26,5,6pit7z,[D] Can machine learning be bad for your GPU?,https://www.reddit.com/r/MachineLearning/comments/6pit7z/d_can_machine_learning_be_bad_for_your_gpu/,qzzqzq,1501013054,"I already googled around a bit but couldn't really find anything useful. I have a GTX 1060 6GB on my computer, which I mostly use for gaming. Just recently, I have started messing around with some machine learning using Tensorflow. I notice when I'm training models, the GPU (as expected) is running pretty consistently at 100% load, and around 72 degrees. This is more intensive than when I am just gaming, and I'm wondering if there are any negative effects to my GPU that could show up over time if I do a lot of training?",11,4
1118,2017-7-26,2017,7,26,5,6pix82,Can your machines learn how to do a proper HL1 style resonance cascade? :D,https://www.reddit.com/r/MachineLearning/comments/6pix82/can_your_machines_learn_how_to_do_a_proper_hl1/,aizu9,1501014014,,2,1
1119,2017-7-26,2017,7,26,6,6pjcqg,"Clarification on terminology in the ""Attention is all you need"" paper: https://arxiv.org/pdf/1706.03762.pdf",https://www.reddit.com/r/MachineLearning/comments/6pjcqg/clarification_on_terminology_in_the_attention_is/,centau1,1501017843,[removed],0,1
1120,2017-7-26,2017,7,26,6,6pjezk,Global macro and Deep Learning.,https://www.reddit.com/r/MachineLearning/comments/6pjezk/global_macro_and_deep_learning/,qplum,1501018428,,0,1
1121,2017-7-26,2017,7,26,6,6pjhf8,"[N] AI Grant 2.0: get $2500 cash and &gt;$20,000 in GPU credits for your AI project",https://www.reddit.com/r/MachineLearning/comments/6pjhf8/n_ai_grant_20_get_2500_cash_and_20000_in_gpu/,nataigrant,1501019065,,18,125
1122,2017-7-26,2017,7,26,6,6pjhoi,[D] Francois Chollet- Deep Learning with Python,https://www.reddit.com/r/MachineLearning/comments/6pjhoi/d_francois_chollet_deep_learning_with_python/,07Zulrah,1501019132,[removed],0,1
1123,2017-7-26,2017,7,26,6,6pjjeo,[D] Bored Yann LeCun #torches Elon &amp; Zuck AI disagreement,https://www.reddit.com/r/MachineLearning/comments/6pjjeo/d_bored_yann_lecun_torches_elon_zuck_ai/,evc123,1501019542,,0,1
1124,2017-7-26,2017,7,26,8,6pk558,[D] MIT 9.520 Statistical Machine Learning,https://www.reddit.com/r/MachineLearning/comments/6pk558/d_mit_9520_statistical_machine_learning/,mni5567,1501025468,"Hi,

Does someone here has ""Machine Learning: a Regularization Approach, MIT-9.520 Lectures Note"". They are provided in class. I am waiting for them to upload it to OCW (as per their email) for over a year now. I will be extremely thankful if someone has those and can upload here",11,32
1125,2017-7-26,2017,7,26,9,6pkios,SeeFood: an iOS app that uses CoreML to detect various dishes,https://www.reddit.com/r/MachineLearning/comments/6pkios/seefood_an_ios_app_that_uses_coreml_to_detect/,MyNameIsNotJeff,1501029367,,1,1
1126,2017-7-26,2017,7,26,10,6pknub,Can a multi-class classifier be used to do user and entity behavior analysis,https://www.reddit.com/r/MachineLearning/comments/6pknub/can_a_multiclass_classifier_be_used_to_do_user/,usepr,1501030904,[removed],0,1
1127,2017-7-26,2017,7,26,11,6pl1zs,[D] Deep Learning: The Unreasonable Effectiveness of Randomness,https://www.reddit.com/r/MachineLearning/comments/6pl1zs/d_deep_learning_the_unreasonable_effectiveness_of/,sanity,1501035297,,0,1
1128,2017-7-26,2017,7,26,11,6pl5ls,How I Used Deep Learning To Train A Chatbot To Talk Like Me (Sorta),https://www.reddit.com/r/MachineLearning/comments/6pl5ls/how_i_used_deep_learning_to_train_a_chatbot_to/,adeshpande3,1501036429,,0,1
1129,2017-7-26,2017,7,26,12,6plhbp,Using code as input to a neural network,https://www.reddit.com/r/MachineLearning/comments/6plhbp/using_code_as_input_to_a_neural_network/,FourthHead,1501040271,[removed],0,1
1130,2017-7-26,2017,7,26,13,6plne3,"I want to learn machine learning and neuronal networks, but I have a low spec PC",https://www.reddit.com/r/MachineLearning/comments/6plne3/i_want_to_learn_machine_learning_and_neuronal/,glassShot2,1501042328,[removed],0,1
1131,2017-7-26,2017,7,26,13,6plv3k,How I plan to become a machine learning engineer,https://www.reddit.com/r/MachineLearning/comments/6plv3k/how_i_plan_to_become_a_machine_learning_engineer/,ZuzooVn,1501045075,,0,1
1132,2017-7-26,2017,7,26,14,6pm1hd,Lessons Learned From Benchmarking Fast Machine Learning Algorithms: XGBoost vs LightGBM,https://www.reddit.com/r/MachineLearning/comments/6pm1hd/lessons_learned_from_benchmarking_fast_machine/,[deleted],1501047519,[deleted],0,1
1133,2017-7-26,2017,7,26,14,6pm3z3,[R] Toward Geometric Deep SLAM,https://www.reddit.com/r/MachineLearning/comments/6pm3z3/r_toward_geometric_deep_slam/,[deleted],1501048511,[deleted],0,1
1134,2017-7-26,2017,7,26,15,6pm5h3,[R] Toward Geometric Deep SLAM,https://www.reddit.com/r/MachineLearning/comments/6pm5h3/r_toward_geometric_deep_slam/,Sirisian,1501049110,,8,20
1135,2017-7-26,2017,7,26,15,6pm7ca,[P] Lessons Learned From Benchmarking Fast Machine Learning Algorithms: XGBoost vs LightGBM,https://www.reddit.com/r/MachineLearning/comments/6pm7ca/p_lessons_learned_from_benchmarking_fast_machine/,hoaphumanoid,1501049888,,13,25
1136,2017-7-26,2017,7,26,15,6pm9zw,[P] Music Source Separation Using Deep Neural Networks From Jeju Machine Learning Camp 2017,https://www.reddit.com/r/MachineLearning/comments/6pm9zw/p_music_source_separation_using_deep_neural/,andabi,1501050995,,9,41
1137,2017-7-26,2017,7,26,17,6pmow7,DeepLearning.scala 2.0 - Creating statically typed dynamic neural networks from map/reduce and functional programming constructs,https://www.reddit.com/r/MachineLearning/comments/6pmow7/deeplearningscala_20_creating_statically_typed/,yang_bo,1501057530,,0,1
1138,2017-7-26,2017,7,26,17,6pmq46,Is realtime AI a thing?,https://www.reddit.com/r/MachineLearning/comments/6pmq46/is_realtime_ai_a_thing/,DineshVadhia,1501058126,,0,1
1139,2017-7-26,2017,7,26,19,6pn1vh,[R] [1707.06990] Memory-Efficient Implementation of DenseNets,https://www.reddit.com/r/MachineLearning/comments/6pn1vh/r_170706990_memoryefficient_implementation_of/,NicolasGuacamole,1501063453,,8,19
1140,2017-7-26,2017,7,26,19,6pn4b2,Data Science Digest - Issue #8,https://www.reddit.com/r/MachineLearning/comments/6pn4b2/data_science_digest_issue_8/,flyelephant,1501064459,,0,1
1141,2017-7-26,2017,7,26,19,6pn60y,[R] [1707.07901] Partial Transfer Learning with Selective Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/6pn60y/r_170707901_partial_transfer_learning_with/,enderwagon,1501065173,,0,22
1142,2017-7-26,2017,7,26,19,6pn77m,Go From Zero to Deep learning. Let's make a plan.,https://www.reddit.com/r/MachineLearning/comments/6pn77m/go_from_zero_to_deep_learning_lets_make_a_plan/,madara33,1501065669,[removed],0,1
1143,2017-7-26,2017,7,26,21,6pnjqn,[R] How to make a racist AI without really trying,https://www.reddit.com/r/MachineLearning/comments/6pnjqn/r_how_to_make_a_racist_ai_without_really_trying/,gitamar,1501070408,,347,333
1144,2017-7-26,2017,7,26,22,6pnyl6,"""Using bad data for a machine learning project would be like using bad eggs to bake a cake. It's just going to go badly for everyone involved."" Check out our guide to data quality assessment, made as easy as ABCD",https://www.reddit.com/r/MachineLearning/comments/6pnyl6/using_bad_data_for_a_machine_learning_project/,Arminastepan,1501075078,,0,1
1145,2017-7-26,2017,7,26,22,6pnz20,[R] Jupyter Magic for Cell Completion Alerts via Browser Push,https://www.reddit.com/r/MachineLearning/comments/6pnz20/r_jupyter_magic_for_cell_completion_alerts_via/,jackblun,1501075206,,0,1
1146,2017-7-26,2017,7,26,22,6po0mb,Scalable High-Performance Architecture for Convolutional Ternary Neural Networks on FPGA  r/iotml,https://www.reddit.com/r/MachineLearning/comments/6po0mb/scalable_highperformance_architecture_for/,[deleted],1501075633,[deleted],0,1
1147,2017-7-26,2017,7,26,22,6po0np,Agile Data Science 2.0,https://www.reddit.com/r/MachineLearning/comments/6po0np/agile_data_science_20/,[deleted],1501075642,[deleted],0,1
1148,2017-7-26,2017,7,26,22,6po1kv,[R] Occupancy detection in the office by analyzing surveillance videos and its application to building energy conservation  r/iotml,https://www.reddit.com/r/MachineLearning/comments/6po1kv/r_occupancy_detection_in_the_office_by_analyzing/,onegazillion,1501075899,,0,4
1149,2017-7-26,2017,7,26,22,6po2rl,[R] Agile Data Science 2.0,https://www.reddit.com/r/MachineLearning/comments/6po2rl/r_agile_data_science_20/,digitalson,1501076248,,0,1
1150,2017-7-26,2017,7,26,22,6po491,[P] Wind gust forecasts - master thesis in meteorology - which algorithms to use?,https://www.reddit.com/r/MachineLearning/comments/6po491/p_wind_gust_forecasts_master_thesis_in/,Dremet,1501076651,"I am currently working on my master thesis in meteorology and got into machine learning last year. I have roughly 1500 cases of forecasts by a weather model (a lot of parameters ~ 400) with the respective ""solution"" (the measurements). I turned this into a classification forecast by assigning wind gusts &lt;= 25 knots to class ""0"" and the others to class ""1"" because it has tradition to just report wind gusts above 25 knots in meteorological ground measurements.


My current thoughts are:

- using a very easy algorithm like a perceptron as a comparison

- using some not-strongly-related algorithms for skill comparison

-- Random Forests, Support Vector Machines, Logistic Regression, AdaBoost, maybe also some kind of neural network

-- tuning some parameters of these algorithms with cross-validation (or out of bag samples in random forests)

- answering some interesting questions:

-- does the forecast skill get significantly worse when the classification boundary is increased (25 knots -&gt; 30/40 knots) due to less cases?

-- does the forecast skill improve when cases of non-nearby other stations are added? (non-nearby to decrease correlation between the cases)

-- how about combining the forecasts of the tested algorithms? does this yield an even better forecast?


I would really appreciate any suggestions and thoughts.
",4,4
1151,2017-7-26,2017,7,26,23,6po8y9,[R] Data Analysis using Python - An Introduction,https://www.reddit.com/r/MachineLearning/comments/6po8y9/r_data_analysis_using_python_an_introduction/,friscotime,1501077895,,0,1
1152,2017-7-26,2017,7,26,23,6poj13,"[R] Python for Finance, Part 3: A Moving Average Trading Strategy",https://www.reddit.com/r/MachineLearning/comments/6poj13/r_python_for_finance_part_3_a_moving_average/,lalypopa123,1501080542,,0,1
1153,2017-7-26,2017,7,26,23,6poksx,[R] The Cleanest Water in Washington State,https://www.reddit.com/r/MachineLearning/comments/6poksx/r_the_cleanest_water_in_washington_state/,molode,1501080996,,0,1
1154,2017-7-27,2017,7,27,0,6potig,music generation program?,https://www.reddit.com/r/MachineLearning/comments/6potig/music_generation_program/,TheTrueDatti,1501083177,[removed],0,1
1155,2017-7-27,2017,7,27,0,6poyx3,[P] DrNET in PyTorch Reconstruction Issues,https://www.reddit.com/r/MachineLearning/comments/6poyx3/p_drnet_in_pytorch_reconstruction_issues/,maxolansmith,1501084494,"I was attempting to reconstruct [Denton's DrNET](https://arxiv.org/abs/1705.10915) model in PyTorch as an exercise to learn PyTorch and understand the basics of Torch, but I can't seem to replicate results.  If anyone is experienced with these toolkits and wants to look it over I've [open-sourced](https://github.com/MaxOSmith/drnet) the code, and the fix would provide a nice additional model to our community. 

**Issue**: The current issue is the reconstruction of frames from latent space produces very poor results (MNIST images that are blobs).  The main logic is all contained in a [single source file](https://github.com/MaxOSmith/drnet/blob/master/src/train_modules_main.py) I mention on the README, and matches Denton's [main training file](https://github.com/edenton/drnet/blob/master/train_drnet.lua) rather closely.

As a result the LSTM stuff isn't verified as working properly. 


Thanks for reading :).",0,0
1156,2017-7-27,2017,7,27,0,6poz4m,"Simple Questions Thread July 26, 2017",https://www.reddit.com/r/MachineLearning/comments/6poz4m/simple_questions_thread_july_26_2017/,AutoModerator,1501084546,[removed],0,1
1157,2017-7-27,2017,7,27,1,6pp44n,Random Forests - The Math of Intelligence (Week 7),https://www.reddit.com/r/MachineLearning/comments/6pp44n/random_forests_the_math_of_intelligence_week_7/,funmaster11,1501085749,,0,1
1158,2017-7-27,2017,7,27,1,6pp649,Combing properties of selu and crelu.,https://www.reddit.com/r/MachineLearning/comments/6pp649/combing_properties_of_selu_and_crelu/,[deleted],1501086215,[removed],0,1
1159,2017-7-27,2017,7,27,2,6ppmde,"When doing experiments involving deep learning, do you often repeat the experiment multiple times and average the results in order to account for randomness in initialization?",https://www.reddit.com/r/MachineLearning/comments/6ppmde/when_doing_experiments_involving_deep_learning_do/,dappermuis,1501090073,[removed],0,1
1160,2017-7-27,2017,7,27,3,6pq5eb,"CleverHans, a Python library to benchmark machine learning systems' vulnerability to adversarial examples.",https://www.reddit.com/r/MachineLearning/comments/6pq5eb/cleverhans_a_python_library_to_benchmark_machine/,removablellama,1501094610,,0,1
1161,2017-7-27,2017,7,27,4,6pqff9,[D] Ideas on interpreting machine learning,https://www.reddit.com/r/MachineLearning/comments/6pqff9/d_ideas_on_interpreting_machine_learning/,breandan,1501097066,,0,12
1162,2017-7-27,2017,7,27,5,6pqpi4,Using Single Shot Detector (SSD) with a MIPI (leopard imaging) camera on a TX1,https://www.reddit.com/r/MachineLearning/comments/6pqpi4/using_single_shot_detector_ssd_with_a_mipi/,pnambiar,1501099509,[removed],0,1
1163,2017-7-27,2017,7,27,6,6pr9oe,Combining crelu and selu for self normalizing relu.,https://www.reddit.com/r/MachineLearning/comments/6pr9oe/combining_crelu_and_selu_for_self_normalizing_relu/,[deleted],1501104465,[removed],0,1
1164,2017-7-27,2017,7,27,6,6prcjx,"Self normalizing relu, using scaled crelu.",https://www.reddit.com/r/MachineLearning/comments/6prcjx/self_normalizing_relu_using_scaled_crelu/,[deleted],1501105206,[removed],0,1
1165,2017-7-27,2017,7,27,7,6prhis,DrQA: Reading Wikipedia to Answer Open-Domain Questions,https://www.reddit.com/r/MachineLearning/comments/6prhis/drqa_reading_wikipedia_to_answer_opendomain/,adamfisch,1501106472,,0,1
1166,2017-7-27,2017,7,27,7,6prk9d,[P] 5 puzzles about statistics that should be accessible to anyone without being trivial,https://www.reddit.com/r/MachineLearning/comments/6prk9d/p_5_puzzles_about_statistics_that_should_be/,pmigdal,1501107165,,33,141
1167,2017-7-27,2017,7,27,7,6prqfi,[D] On the history of the ImageNet dataset &amp; competition,https://www.reddit.com/r/MachineLearning/comments/6prqfi/d_on_the_history_of_the_imagenet_dataset/,gwern,1501108782,,2,15
1168,2017-7-27,2017,7,27,9,6psgmb,Deep Learning promises to bring algorithmic investing smarts to the rest of us.,https://www.reddit.com/r/MachineLearning/comments/6psgmb/deep_learning_promises_to_bring_algorithmic/,MaverickSoulprorer,1501116288,,0,1
1169,2017-7-27,2017,7,27,9,6psj4w,Interactively visualize how title and abstracts in #acl2017nlp tracks + workshops vary,https://www.reddit.com/r/MachineLearning/comments/6psj4w/interactively_visualize_how_title_and_abstracts/,[deleted],1501117022,[deleted],0,1
1170,2017-7-27,2017,7,27,11,6pswzi,[P] Visualizing how language used in ACL 2017 tracks and workshops varies,https://www.reddit.com/r/MachineLearning/comments/6pswzi/p_visualizing_how_language_used_in_acl_2017/,jasonskessler,1501121250,,0,8
1171,2017-7-27,2017,7,27,11,6pt56c,"The best machine learning books, picked by /r/machinelearning",https://www.reddit.com/r/MachineLearning/comments/6pt56c/the_best_machine_learning_books_picked_by/,[deleted],1501123835,[deleted],0,1
1172,2017-7-27,2017,7,27,12,6pt99z,"[R] Squeeze-and-Excitation networks, ILSVRC 2017 winner, at CVPR2017 (photos of presentation)",https://www.reddit.com/r/MachineLearning/comments/6pt99z/r_squeezeandexcitation_networks_ilsvrc_2017/,modeless,1501125141,,9,32
1173,2017-7-27,2017,7,27,13,6pto1g,[D] The right way to incorporate trend back into my forecast?,https://www.reddit.com/r/MachineLearning/comments/6pto1g/d_the_right_way_to_incorporate_trend_back_into_my/,smthamazing,1501130152,"Hi all!

I use an LSTM to forecast a time series (using a rolling forecast method). Before that, I remove trend and seasonaity from the series, which significantly imporves the quality of my forecast.

Adding seasonality back to the forecasted data is easy.

The problem is, I don't quite get how to add the trend back. Do I need to forecast the trend as well? If so, which technique should I use? Another LSTM?

Thanks!",4,10
1174,2017-7-27,2017,7,27,14,6ptwwn,[D] What you need to know before you board the machine learning train,https://www.reddit.com/r/MachineLearning/comments/6ptwwn/d_what_you_need_to_know_before_you_board_the/,paraschopra,1501133488,,0,1
1175,2017-7-27,2017,7,27,15,6pu67x,AI's programmed to 'see' will reprogram our DNA to remove our blind spots,https://www.reddit.com/r/MachineLearning/comments/6pu67x/ais_programmed_to_see_will_reprogram_our_dna_to/,[deleted],1501137237,[deleted],0,1
1176,2017-7-27,2017,7,27,15,6pu77z,Books on Tensor Methods/Analysis in Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/6pu77z/books_on_tensor_methodsanalysis_in_machine/,Darthvaduur,1501137658,[removed],0,1
1177,2017-7-27,2017,7,27,15,6pu7xm,[D] AI's programmed to 'see' will reprogram our DNA to remove our blind spots,https://www.reddit.com/r/MachineLearning/comments/6pu7xm/d_ais_programmed_to_see_will_reprogram_our_dna_to/,[deleted],1501137967,[deleted],0,0
1178,2017-7-27,2017,7,27,15,6pu8wb,[D] Our AI's will reprogram our DNA to remove our blind spots,https://www.reddit.com/r/MachineLearning/comments/6pu8wb/d_our_ais_will_reprogram_our_dna_to_remove_our/,phobrain,1501138398,[removed],15,0
1179,2017-7-27,2017,7,27,17,6pulj2,[R] Decision Forests that preserve personal privacy,https://www.reddit.com/r/MachineLearning/comments/6pulj2/r_decision_forests_that_preserve_personal_privacy/,Caesarr,1501144020,,3,35
1180,2017-7-27,2017,7,27,17,6punnv,DrQA: Reading Wikipedia to Answer Open-Domain Questions (in PyTorch),https://www.reddit.com/r/MachineLearning/comments/6punnv/drqa_reading_wikipedia_to_answer_opendomain/,[deleted],1501145030,[deleted],0,1
1181,2017-7-27,2017,7,27,17,6puoxk,[Need Suggestions] Data classification of ERP data with certain parameters,https://www.reddit.com/r/MachineLearning/comments/6puoxk/need_suggestions_data_classification_of_erp_data/,HighonCosmos,1501145608,[removed],0,1
1182,2017-7-27,2017,7,27,19,6pv3dq,Deep learning using multi variable time series data?,https://www.reddit.com/r/MachineLearning/comments/6pv3dq/deep_learning_using_multi_variable_time_series/,SirLordDragon,1501151887,[removed],0,1
1183,2017-7-27,2017,7,27,21,6pvhhm,Fun video abstract about multi armed bandits,https://www.reddit.com/r/MachineLearning/comments/6pvhhm/fun_video_abstract_about_multi_armed_bandits/,albinosquirrel2,1501157345,,0,1
1184,2017-7-27,2017,7,27,21,6pvibx,[D] Full understanding of Out-Off-Bag-Error in Random Forests,https://www.reddit.com/r/MachineLearning/comments/6pvibx/d_full_understanding_of_outoffbagerror_in_random/,Dremet,1501157608,"I do have some very detailed questions about the out-of-bag error in the random forest algorithm, which I couldn't answer after some research.

My understanding:
OOB error is the error resulting in going through each sample and calculating the accuracy by a vote of those trees, which did not get that sample in the bootstrapping procedure and are therefore not influenced by that very sample. This is done for every sample to estimate the overall accuracy.

On ""https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm"" in the section ""The out-of-bag (oob) error estimate"" it says that there is no need for a separate test set. Assuming this is true, one can take the whole dataset for training and one can even estimate the expected error as well. 

What if you want to play around with some parameters like depth or amount of trees? I would guess you need a test set in that case and the OOB error within the training set tells you which parameters you should use, but you can't use the OOB error of the best parameters within the training set to estimate the expected error because you used all the data to determine the parameters. So in that case a separate test set, which was not involved in the parameter optimization, would be needed, correct?",4,5
1185,2017-7-27,2017,7,27,21,6pvjmw,[R] DARLA: Improving Zero-Shot Transfer in Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/6pvjmw/r_darla_improving_zeroshot_transfer_in/,Kaixhin,1501158042,,1,24
1186,2017-7-27,2017,7,27,21,6pvlwv,[R] Comparative analysis of Chatbot modern platforms,https://www.reddit.com/r/MachineLearning/comments/6pvlwv/r_comparative_analysis_of_chatbot_modern_platforms/,thesameoldstories,1501158811,,4,23
1187,2017-7-27,2017,7,27,22,6pvutb,"[P] Sciblox version 2 updated! Cool new features, faster BPCA impute and more!!!",https://www.reddit.com/r/MachineLearning/comments/6pvutb/p_sciblox_version_2_updated_cool_new_features/,danielhanchen,1501161548,"https://pypi.python.org/pypi/sciblox, https://github.com/danielhanchen/sciblox.

Shout out to Icko_ (https://www.reddit.com/user/Icko_) and chub79 (https://www.reddit.com/user/chub79) for helping me!

Check https://danielhanchen.github.io/ for demonstration.
So what's new?

To install : pip install sciblox (admin pls)

1. CARET like preprocessing (scaling, normalising, dummify, outlier removal, unskew, etc)
2. Processor modules - you can fit onto testing data!
3. LightGBM and RandomForest support - later will add more
4. More analysis methods: outlier detection, skewness methods, auto unskewing etc
5. 3D automatic plots in NEW cool analyse function!
6. Fixed some silly errors in HCAT and VCAT methods
7. Better and more flexible functions!
8. Removed some redundant things
9. AUTO installs necessary packages!

Have fun with it! I hope it works! Any errors pls tell me asap. Will update documentation a bit later.

Thanks all for your cool support :) :)",1,7
1188,2017-7-27,2017,7,27,22,6pw0aq,"To Join our ""Python Machine Learning Webinar"" On Sunday, July 30, @ 7.30pm IST",https://www.reddit.com/r/MachineLearning/comments/6pw0aq/to_join_our_python_machine_learning_webinar_on/,MCAL_Training,1501163144,,0,1
1189,2017-7-27,2017,7,27,23,6pw3ud,[P] Evolution Strategies in Keras,https://www.reddit.com/r/MachineLearning/comments/6pw3ud/p_evolution_strategies_in_keras/,onehotoneshot,1501164089,"Inspired by this blogpost: https://blog.openai.com/evolution-strategies/

I didn't see any projects that showed a basic example of this in Keras so I figured I'd give it a shot: https://gist.github.com/nicksam112/00e9638c0efad1adac878522cf172484

It's able to solve several gym environments such as Cartpole and BipedalWalker just by running on a single laptop CPU. Cartpole should solve quickly, in about 10 minutes or 50 runs, while Bipedal walker may take 24 hours or 1000 runs on that same CPU. That's with a network that has two 128 node hidden layers which might be overkill, so it may be able to solve faster with a smaller network

Next step would be to allow it to be run on multiple machines which is one of the main benefits to ES, but that'll probably be as an actual GitHub project rather than a gist.

This is my first attempt implementing from scratch so any and all feedback is encouraged and would be a big help! Thanks for checking it out! 

EDIT: Changed ""episodes"" to ""runs"", a run here refers to all workers finishing their task typically a single episode each
",14,78
1190,2017-7-27,2017,7,27,23,6pw596,[R] Machine Learning Exercises in Python: An Introductory Tutorial Series,https://www.reddit.com/r/MachineLearning/comments/6pw596/r_machine_learning_exercises_in_python_an/,digitalson,1501164433,,0,1
1191,2017-7-27,2017,7,27,23,6pw5fu,[D] PhD Student / Professor Assistant Experiences,https://www.reddit.com/r/MachineLearning/comments/6pw5fu/d_phd_student_professor_assistant_experiences/,Naveos,1501164475,"I'm currently studying for my Master's Degree in Medical Engineering but my courses are almost identical to my friends studying Master's in Machine Learning apart from two courses, related to patient safety and 3D medical imaging respectively.

With that said; there's a lot of open vacancies at my university regarding PhD student positions (professor assistant) in deep learning, and I am seriously considering applying for several of these but I don't know what to expect. I've heard many horror stories of PhD students in other fields, from strangers and friends, but none in deep learning and AI.

For those that've studied PhD or are studying for their PhD in Deep Learning and/or similar subjects, what was/is your experience? What should I expect? What did you wish you knew before starting?

**TL;DR:** Read last paragraph.

Ninja Edit: Do you deem it necessary to get a PhD in Deep Learning to work with its architecture and research or can I get into it with a Master's? In terms of employment.",13,23
1192,2017-7-27,2017,7,27,23,6pw5wx,[D] Is it recommended to read Murphy after finishing Bishop already ?,https://www.reddit.com/r/MachineLearning/comments/6pw5wx/d_is_it_recommended_to_read_murphy_after/,xingdongrobotics,1501164590,[removed],0,2
1193,2017-7-28,2017,7,28,2,6pxdqy,[D] Data Version Control in Analytics DevOps Paradigm - Managing DevOps Issues for Data Scientist,https://www.reddit.com/r/MachineLearning/comments/6pxdqy/d_data_version_control_in_analytics_devops/,thumbsdrivesmecrazy,1501175434,,0,1
1194,2017-7-28,2017,7,28,2,6pxega,[R] Better Exploration with Parameter Noise,https://www.reddit.com/r/MachineLearning/comments/6pxega/r_better_exploration_with_parameter_noise/,wei_jok,1501175595,,6,46
1195,2017-7-28,2017,7,28,2,6pxein,[R] Video Highlight Prediction Using Audience Chat Reactions,https://www.reddit.com/r/MachineLearning/comments/6pxein/r_video_highlight_prediction_using_audience_chat/,Dim25,1501175606,,1,0
1196,2017-7-28,2017,7,28,2,6pxnpi,Machine Learning project Idea,https://www.reddit.com/r/MachineLearning/comments/6pxnpi/machine_learning_project_idea/,[deleted],1501177761,[removed],0,1
1197,2017-7-28,2017,7,28,2,6pxpel,[P] [1706.10295] Noisy Networks for Exploration,https://www.reddit.com/r/MachineLearning/comments/6pxpel/p_170610295_noisy_networks_for_exploration/,[deleted],1501178173,[deleted],1,0
1198,2017-7-28,2017,7,28,3,6py2xm,My goal is to create a Machine Learning chatbot. Where is the best place to start?,https://www.reddit.com/r/MachineLearning/comments/6py2xm/my_goal_is_to_create_a_machine_learning_chatbot/,dancintomytune,1501181331,[removed],0,1
1199,2017-7-28,2017,7,28,4,6py5vm,[D] Computing the gradient by hand!,https://www.reddit.com/r/MachineLearning/comments/6py5vm/d_computing_the_gradient_by_hand/,[deleted],1501182023,[deleted],2,1
1200,2017-7-28,2017,7,28,4,6pych5,Keeping up with AI Breakthroughs,https://www.reddit.com/r/MachineLearning/comments/6pych5/keeping_up_with_ai_breakthroughs/,Houssemismail,1501183559,,0,1
1201,2017-7-28,2017,7,28,4,6pydff,NTM (Neural Turing Machines) and MANN in TensorFlow with detailed results and usages,https://www.reddit.com/r/MachineLearning/comments/6pydff/ntm_neural_turing_machines_and_mann_in_tensorflow/,snowkylin,1501183785,,0,1
1202,2017-7-28,2017,7,28,5,6pynyc,Should I take multivariable calc during my PhD?,https://www.reddit.com/r/MachineLearning/comments/6pynyc/should_i_take_multivariable_calc_during_my_phd/,j_l13,1501186267,[removed],0,1
1203,2017-7-28,2017,7,28,7,6pzfr0,"Building a Timeline of your Video: Automatically Identify Objects, Sequence Times, and Integrate with Timeline.js",https://www.reddit.com/r/MachineLearning/comments/6pzfr0/building_a_timeline_of_your_video_automatically/,peckjon,1501193317,,0,1
1204,2017-7-28,2017,7,28,8,6pzrmg,[R] TensorFlow 1.3.0-rc1 Release Notes,https://www.reddit.com/r/MachineLearning/comments/6pzrmg/r_tensorflow_130rc1_release_notes/,MetricSpade007,1501196543,,25,69
1205,2017-7-28,2017,7,28,11,6q0wfm,Installing torch 7 on Ubuntu 16,https://www.reddit.com/r/MachineLearning/comments/6q0wfm/installing_torch_7_on_ubuntu_16/,metalloidica,1501208920,[removed],0,1
1206,2017-7-28,2017,7,28,11,6q0wyz,What is the best way to keep up with the machine learning literature? Recommendations?,https://www.reddit.com/r/MachineLearning/comments/6q0wyz/what_is_the_best_way_to_keep_up_with_the_machine/,Zeekawla99ii,1501209088,[removed],0,1
1207,2017-7-28,2017,7,28,11,6q0z0n,CVPR 2017: a personal reflexion,https://www.reddit.com/r/MachineLearning/comments/6q0z0n/cvpr_2017_a_personal_reflexion/,Cristiancanton,1501209744,,0,1
1208,2017-7-28,2017,7,28,13,6q1ds1,"A naive question: Can we provide a bunch of pokemon cards, turn it into ML data that can then be used to recognize the card in any angle?",https://www.reddit.com/r/MachineLearning/comments/6q1ds1/a_naive_question_can_we_provide_a_bunch_of/,enzyme69,1501214610,[removed],0,1
1209,2017-7-28,2017,7,28,13,6q1n04,Path Integral Networks: End-to-End Differentiable Optimal Control,https://www.reddit.com/r/MachineLearning/comments/6q1n04/path_integral_networks_endtoend_differentiable/,[deleted],1501217976,[deleted],0,1
1210,2017-7-28,2017,7,28,14,6q1twx,Learn Artificial Intelligence with 10-Course Machine Learning Bundle,https://www.reddit.com/r/MachineLearning/comments/6q1twx/learn_artificial_intelligence_with_10course/,ravisaive,1501220589,,0,1
1211,2017-7-28,2017,7,28,14,6q1v7w,[D] What are some good reinforcement learning papers where simulation is used,https://www.reddit.com/r/MachineLearning/comments/6q1v7w/d_what_are_some_good_reinforcement_learning/,crouching_dragon_420,1501221053,"For example, the agent creates simulations of possible next states and decide to take action accordingly. I am super interested in applying GAN to RL. Thanks.",2,5
1212,2017-7-28,2017,7,28,15,6q21cg,[R] Tips on pruning decision trees for better machine learning results,https://www.reddit.com/r/MachineLearning/comments/6q21cg/r_tips_on_pruning_decision_trees_for_better/,[deleted],1501223413,[deleted],0,1
1213,2017-7-28,2017,7,28,15,6q23tr,[R] Tips on pruning decision trees for better machine learning results,https://www.reddit.com/r/MachineLearning/comments/6q23tr/r_tips_on_pruning_decision_trees_for_better/,scottauds,1501224322,,0,0
1214,2017-7-28,2017,7,28,15,6q24wn,"[D] Explanation regarding a derivation in this paper ""Evolution Strategies as a Scalable Alternative to Reinforcement Learning"" (https://arxiv.org/abs/1703.03864)",https://www.reddit.com/r/MachineLearning/comments/6q24wn/d_explanation_regarding_a_derivation_in_this/,fu_2016,1501224750,"http://imgur.com/a/RrdOf - I am unable to understand the expressions marked by the red underline. The yellow highlighted expression is the score function estimator. I am able to understand that. If you have read the paper, any help in understanding the underlined expressions would be appreciated. (Apologies for the crude method of posting, but I don't know how to embed latex in reddit).",2,10
1215,2017-7-28,2017,7,28,16,6q26qr,[D] Why most people in ML choose Founders Edition GPUs?,https://www.reddit.com/r/MachineLearning/comments/6q26qr/d_why_most_people_in_ml_choose_founders_edition/,dexzmen,1501225510,"Since yesterday I'm trying to build own configuration for ML related computing. I have to decide on GPU till the end of July. 1080 ti seems to be a winner for me. However, there is a great deal of many different editions of this card. Surprisingly though many people seem to choose the Founders Edition cards which run hotter/are louder/even a bit slower ([video](https://www.youtube.com/watch?v=0domMRFG1Rw)). This makes me think. Well, maybe those bigger cards cannot fit together when somebody wants to put few of them on the same motherboard? If that would be the case which would you choose?

*Also, some of the configurations that I've seen were done before aftermarket release of 1080 ti.",8,11
1216,2017-7-28,2017,7,28,16,6q29rn,Just me or is installing many python libraries in windows a pain ?,https://www.reddit.com/r/MachineLearning/comments/6q29rn/just_me_or_is_installing_many_python_libraries_in/,abx7060,1501226705,[removed],0,1
1217,2017-7-28,2017,7,28,16,6q2b2s,What is the suitable algorithm to map data between different format?,https://www.reddit.com/r/MachineLearning/comments/6q2b2s/what_is_the_suitable_algorithm_to_map_data/,kingname,1501227283,[removed],0,1
1218,2017-7-28,2017,7,28,17,6q2iwz,3T C Frame Servo Press For Precision Assembly and Press-in,https://www.reddit.com/r/MachineLearning/comments/6q2iwz/3t_c_frame_servo_press_for_precision_assembly_and/,xtm-mattchan,1501230822,,0,1
1219,2017-7-28,2017,7,28,17,6q2ki7,[R] Graphcore: How to build a processor for machine intelligence &lt;-- poised to overtake GPUs?,https://www.reddit.com/r/MachineLearning/comments/6q2ki7/r_graphcore_how_to_build_a_processor_for_machine/,evc123,1501231592,,34,58
1220,2017-7-28,2017,7,28,18,6q2qox,[D] Is the Apple CVPR best paper the result of politics?,https://www.reddit.com/r/MachineLearning/comments/6q2qox/d_is_the_apple_cvpr_best_paper_the_result_of/,[deleted],1501234403,[deleted],0,1
1221,2017-7-28,2017,7,28,18,6q2s59,"[D] Meta-learning application of ""Evolution Strategies as a Scalable Alternative to Reinforcement Learning"" for weight optimization",https://www.reddit.com/r/MachineLearning/comments/6q2s59/d_metalearning_application_of_evolution/,HigherTopoi,1501235100,"The authors of the paper, Tim Salimans et. al., mentioned that they are going to apply their result in this paper to the framework of the RL^2 paper, in which the meta-learner doesn't optimize weights of the optimizee. If I remember correctly, there are many works on meta-learning where RL optimizes SL/RL/UL, and all of them require gradient information of the latter. On the other hand, I naively guess that RL can be used to learn an efficient way to optimize weights without requiring gradient information of the optimizee. I want the optimal policy learnt by RL to optimize weights not too slower than SGD does and scales just as well. If this is the case, then the ES paper implies that an efficient BP-free weight optimization of SL/RL/UL is possible; hence, efficient weight optimization of RNN other than the usual architecture consisting of LSTM, GRU and their slight variants becomes possible. Needless to say, all the BP-free meta-learning works employ meta-heuristics, which make it unable to scale up and compete with BP.  

Is my naive guess unlikely to work? Is there any paper in which RL optimizes weights in a scalable way and that supports/refutes my guess?",0,2
1222,2017-7-28,2017,7,28,19,6q2u7l,Did Apple win the CVPR best paper award because of politics?,https://www.reddit.com/r/MachineLearning/comments/6q2u7l/did_apple_win_the_cvpr_best_paper_award_because/,[deleted],1501236027,[removed],0,1
1223,2017-7-28,2017,7,28,19,6q2yzh,"[P] What kind of conv net architecture should i use for artifical, sparse and image-like state representation in deep reinforcement learning?",https://www.reddit.com/r/MachineLearning/comments/6q2yzh/p_what_kind_of_conv_net_architecture_should_i_use/,_tomakko,1501238107,"In my master thesis i am planning to use deep RL for a multi-robot navigation problem. 

I want to represent the state as an image with size: map_height x map_width x 4 (60x80x4). 
The four channels are:

1. the global map, where pixels are 1 if there is an obstacle, else 0

2. the robot swarm layer, where a pixel is 1 if its occupied by a robot of the swarm, else 0

3. the ego position layer, where a pixel is 1 if the current, specific robot occupies it, else 0

4. the ego goal layer, where a pixel is 1 if its the current, specific robot's goal it, else 0

I borrowed the idea from [here](http://cs231n.stanford.edu/reports/2016/pdfs/122_Report.pdf) where they also stated a conv net architecture (see Fig. 5), which i applied with medium success using simple deep q learning. The state input was mapped to 4 Q-values for 4 possible actions (go north, east, south, west) for every robot.

What do you think about this architecture for the specific task? What i find strange is the number of five conv layers for the relatively simple ""features"" in the image and the lack of fully connected hidden layers. Any hints of what architecture I should try?

Should my conv net architecture generally depend on the size of the image (when you neglect hardware constraints)?.",5,0
1224,2017-7-28,2017,7,28,19,6q2zxw,Deep Simnets,https://www.reddit.com/r/MachineLearning/comments/6q2zxw/deep_simnets/,__guy__fawkes__,1501238508,[removed],0,1
1225,2017-7-28,2017,7,28,20,6q33ze,[P] How to train your own Object Detector with TensorFlows Object Detector API,https://www.reddit.com/r/MachineLearning/comments/6q33ze/p_how_to_train_your_own_object_detector_with/,datitran,1501240145,,39,293
1226,2017-7-28,2017,7,28,21,6q3dbr,Modeling documents with Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/6q3dbr/modeling_documents_with_generative_adversarial/,MikeWally,1501243768,,0,1
1227,2017-7-28,2017,7,28,21,6q3e8a,[R] Making Sense of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/6q3e8a/r_making_sense_of_machine_learning/,friscotime,1501244066,,0,1
1228,2017-7-28,2017,7,28,21,6q3m4z,[R] The Machine Learning Algorithms Used in Self-Driving Cars,https://www.reddit.com/r/MachineLearning/comments/6q3m4z/r_the_machine_learning_algorithms_used_in/,friscotime,1501246711,,0,1
1229,2017-7-28,2017,7,28,22,6q3te1,[D] What were things like before Tensorflow?,https://www.reddit.com/r/MachineLearning/comments/6q3te1/d_what_were_things_like_before_tensorflow/,Matisseio,1501248855,"I am just learning about Tensorflow, and I keep reading about how it takes just a couple of hours what used to take a couple of weeks. What do they mean by that? Can I see some specific examples of the same thing done in Tensorflow vs not? ",26,4
1230,2017-7-28,2017,7,28,23,6q3zg9,How Do I Find the Best Hyperparameters? - The Math of Intelligence #7,https://www.reddit.com/r/MachineLearning/comments/6q3zg9/how_do_i_find_the_best_hyperparameters_the_math/,funmaster11,1501250626,,0,1
1231,2017-7-28,2017,7,28,23,6q42lf,Telegram Group,https://www.reddit.com/r/MachineLearning/comments/6q42lf/telegram_group/,[deleted],1501251486,[removed],0,1
1232,2017-7-28,2017,7,28,23,6q47od,"[D] Suggestions for project on available ""typing"" data.",https://www.reddit.com/r/MachineLearning/comments/6q47od/d_suggestions_for_project_on_available_typing_data/,edutainment123,1501252869,"I have my own **typing data** (3000+ races on Typeracer) available. Here is the table of the first 20 races of a user - http://imgur.com/a/Jba0p

The columns are - 

1. **np** - No. of players in a race (Max of 5)
2. **wpm** - Words per minute(WPM)
3. **r** - Rank in that particular race
4. **tid** - Text Id (Id of the the text typed in that race)
5. **gn** - Race number
6. **t** - Timestamps of races
7. **ac** - Accuracy 
8. **sl** - Server (I guess)


**To predict**: WPM in the subsequent months, years.

Being a beginner in this field, as far as I understand, this could be a **time series prediction** problem which could be tried on by using LSTM RNN. But I could be wrong and you could suggest otherwise. 

Apart from the above mentioned prediction, what else could this data be used to predict? 

**Edit**: Added accuracy and image of data represented in a table.",5,7
1233,2017-7-29,2017,7,29,0,6q4hn9,[D] Meaning of num_outputs for convolutional layers?,https://www.reddit.com/r/MachineLearning/comments/6q4hn9/d_meaning_of_num_outputs_for_convolutional_layers/,NapoleonTNT,1501255443,[removed],2,2
1234,2017-7-29,2017,7,29,1,6q4reo,Application for clustering model?,https://www.reddit.com/r/MachineLearning/comments/6q4reo/application_for_clustering_model/,[deleted],1501257820,[removed],0,1
1235,2017-7-29,2017,7,29,1,6q4rjb,[D] Application for clustering model,https://www.reddit.com/r/MachineLearning/comments/6q4rjb/d_application_for_clustering_model/,maka89,1501257855,"Hi. I'm working on developing a clustering model. It tries to reduce the sum of the squared distance from data to clusters, just like KMeans. It typically only uses two features.

However, I also have the opportunity to give a pairwise-distance function to the algorithm, so that I don't have to use the euclidian distance squared.

Does anyone have any good ideas for an application? My original idea was to use travel time on the road network instead of using airway travel distance(which would be the case with regular kmeans). This could be good for finding locations for a hubs like post-offices or airports or whatever.
However, by the time I have downloaded enough data from google maps to get a pairwise-distance model, my api key has been blocked (need thousands of origin-destination points).

Does anyone have another idea for application, or know of a different map-service that would be suitable?",1,1
1236,2017-7-29,2017,7,29,1,6q4w34,The 6 TED Talks that will change how you perceive AI dominance,https://www.reddit.com/r/MachineLearning/comments/6q4w34/the_6_ted_talks_that_will_change_how_you_perceive/,Houssemismail,1501258983,,0,1
1237,2017-7-29,2017,7,29,1,6q509r,How would a paper replicating others' results be received?,https://www.reddit.com/r/MachineLearning/comments/6q509r/how_would_a_paper_replicating_others_results_be/,thai_tong,1501260041,[removed],0,1
1238,2017-7-29,2017,7,29,3,6q5o82,What's the proper way to augment a CNN (on images) with non pixel data (e.g. positional info),https://www.reddit.com/r/MachineLearning/comments/6q5o82/whats_the_proper_way_to_augment_a_cnn_on_images/,crossentropy,1501265891,[removed],0,1
1239,2017-7-29,2017,7,29,4,6q6ace,Which textbook do you recommend for stanford NLP and Convolutional NN classes?,https://www.reddit.com/r/MachineLearning/comments/6q6ace/which_textbook_do_you_recommend_for_stanford_nlp/,MustafaAdam,1501271533,[removed],0,1
1240,2017-7-29,2017,7,29,5,6q6dyn,[D] How would a paper replicating others' result be received?,https://www.reddit.com/r/MachineLearning/comments/6q6dyn/d_how_would_a_paper_replicating_others_result_be/,thai_tong,1501272477,"I have a few favourite ideas which I like to include in neural networks to boost their performance. I read ML papers to find novel ideas. I'd like to know that they actually work but what bothers me is that authors often only run their technique once and give an estimate of its performance. I'm skeptical that they just got a lucky random seed for their problem or they found good hyperparameters so that their idea boosts performance a lot.

I'd like to publish my own paper but I'm not a genius and I can't come up with my own novel ideas. I'd like to take some of my favourite ideas whose papers don't validate them in depth and fully test them to provide confidence intervals on the performance boost.

If I wrote a paper like this and found that a previous paper's result was a fluke then I wonder if the journal would see it as bad practice to be discrediting other authors' ideas. Do you think that this kind of research is publishable or will it be frowned upon?",18,54
1241,2017-7-29,2017,7,29,5,6q6emy,[D] Are there some ANN games?,https://www.reddit.com/r/MachineLearning/comments/6q6emy/d_are_there_some_ann_games/,fimari,1501272656,I think of live training projects like a ANN Tamagotchi where you can train something virtual animal to do virtual things.,9,5
1242,2017-7-29,2017,7,29,5,6q6kpm,[P] Trying to collect a battleship training data set. Please help by playing a game or two with your friends. Just set up a game and send your friend your personalized link.,https://www.reddit.com/r/MachineLearning/comments/6q6kpm/p_trying_to_collect_a_battleship_training_data/,crazycrazycrazycrazy,1501274223,,5,12
1243,2017-7-29,2017,7,29,5,6q6mw1,Is there any work on the meaning of the softmax preactivation values?,https://www.reddit.com/r/MachineLearning/comments/6q6mw1/is_there_any_work_on_the_meaning_of_the_softmax/,[deleted],1501274797,[removed],0,1
1244,2017-7-29,2017,7,29,6,6q71h8,Is it worth it to get a PhD in Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/6q71h8/is_it_worth_it_to_get_a_phd_in_machine_learning/,[deleted],1501278661,[removed],0,1
1245,2017-7-29,2017,7,29,7,6q74r5,[N] Symposium IA Montral,https://www.reddit.com/r/MachineLearning/comments/6q74r5/n_symposium_ia_montral/,cherls,1501279582,,1,15
1246,2017-7-29,2017,7,29,8,6q7jjz,How to detect buildings from an aerial image using convolutional neural network?,https://www.reddit.com/r/MachineLearning/comments/6q7jjz/how_to_detect_buildings_from_an_aerial_image/,tmsbn,1501283983,[removed],0,1
1247,2017-7-29,2017,7,29,9,6q7vpo,Stop Signs Can Be Manipulated Physically to Misclassify as Speed Limit Signs,https://www.reddit.com/r/MachineLearning/comments/6q7vpo/stop_signs_can_be_manipulated_physically_to/,ivanevtimov,1501287796,,0,1
1248,2017-7-29,2017,7,29,9,6q812k,"[D] Thoughts on OpenAI, reinforcement learning, and killer robots  fast.ai",https://www.reddit.com/r/MachineLearning/comments/6q812k/d_thoughts_on_openai_reinforcement_learning_and/,thebackpropaganda,1501289545,,7,2
1249,2017-7-29,2017,7,29,12,6q8ofw,What is the phenomenon of an AI system creating its own language called? I would like to do research on this.,https://www.reddit.com/r/MachineLearning/comments/6q8ofw/what_is_the_phenomenon_of_an_ai_system_creating/,[deleted],1501297801,[removed],0,1
1250,2017-7-29,2017,7,29,16,6q9og1,This youtube video on the differences between Neural networks and Convolutional Neural networks brought everything into focus for me. CNN's are next generation NN's and are to be preferred over NN's for any classification or regression task.,https://www.reddit.com/r/MachineLearning/comments/6q9og1/this_youtube_video_on_the_differences_between/,anon35202,1501312385,,0,1
1251,2017-7-29,2017,7,29,19,6qadkq,[N] Artificial Intelligence Is Stuck. Here's How to Move It Forward.,https://www.reddit.com/r/MachineLearning/comments/6qadkq/n_artificial_intelligence_is_stuck_heres_how_to/,thebackpropaganda,1501325858,,68,30
1252,2017-7-29,2017,7,29,22,6qaw24,FaceRank - Rank Face by CNN Model based on TensorFlow,https://www.reddit.com/r/MachineLearning/comments/6qaw24/facerank_rank_face_by_cnn_model_based_on/,fendouai,1501333964,,0,1
1253,2017-7-29,2017,7,29,23,6qb5yu,[P] [NSFW] Exemplar Networks For A Good Time,https://www.reddit.com/r/MachineLearning/comments/6qb5yu/p_nsfw_exemplar_networks_for_a_good_time/,driftwheeler,1501337640,[removed],2,0
1254,2017-7-29,2017,7,29,23,6qb91p,ELI5: how are GANs and nash equilibrium related?,https://www.reddit.com/r/MachineLearning/comments/6qb91p/eli5_how_are_gans_and_nash_equilibrium_related/,[deleted],1501338738,[removed],0,1
1255,2017-7-29,2017,7,29,23,6qb9id,[R] Natural Language Processing in Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/6qb9id/r_natural_language_processing_in_artificial/,wardolb,1501338887,,28,194
1256,2017-7-29,2017,7,29,23,6qbbgf,simplest TF example that i put together for a tutorial,https://www.reddit.com/r/MachineLearning/comments/6qbbgf/simplest_tf_example_that_i_put_together_for_a/,[deleted],1501339567,[deleted],0,1
1257,2017-7-30,2017,7,30,1,6qbrxh,"I'm conducting a quick survey for students in STEM (Science Tech Eng Math) disciplines about your education and the data science job market. I'm collecting this data to help my department determine if it should fund big data tech workshops, but I can gladly share the results back on this thread!",https://www.reddit.com/r/MachineLearning/comments/6qbrxh/im_conducting_a_quick_survey_for_students_in_stem/,reubenpereira126,1501344661,,0,1
1258,2017-7-30,2017,7,30,1,6qbyox,[D] Which course(s) was/were unexpectedly valuable?,https://www.reddit.com/r/MachineLearning/comments/6qbyox/d_which_courses_waswere_unexpectedly_valuable/,Naveos,1501346655,"Currently in the middle of having to choose two elective courses and I have no idea what to choose. There's 'Data-Intensive Programming', 'Parallel Computations for Large- Scale Problems', 'Applied Estimation', 'Optimization', 'Applied Bioinformatics', , 'Computational Biology', 'Time Series Analysis', and so much more.

Do you guys have any experiences of courses that unexpectedly turned out to be very gratifying and valuable? Which ones and why?",35,41
1259,2017-7-30,2017,7,30,1,6qbzrh,Is my understanding of kernels correct?,https://www.reddit.com/r/MachineLearning/comments/6qbzrh/is_my_understanding_of_kernels_correct/,[deleted],1501346955,[removed],0,1
1260,2017-7-30,2017,7,30,2,6qcc38,[P] Unsupervised Learning for Topic Discovery,https://www.reddit.com/r/MachineLearning/comments/6qcc38/p_unsupervised_learning_for_topic_discovery/,[deleted],1501350499,[deleted],0,1
1261,2017-7-30,2017,7,30,4,6qcypo,PhD in machine learning,https://www.reddit.com/r/MachineLearning/comments/6qcypo/phd_in_machine_learning/,tawseef2312,1501357215,[removed],0,1
1262,2017-7-30,2017,7,30,4,6qczhs,[D] What tutorial do you wish you could read?,https://www.reddit.com/r/MachineLearning/comments/6qczhs/d_what_tutorial_do_you_wish_you_could_read/,fl4v1,1501357442,"We run a [modest tech blog](htts://blog.sicara.com) aimed at machine learning practitionners. We would like to be as useful and impactful as possible for our public, but most of the time we try to guess (incorrectly). Since we want to be agile and be reader-driven, I'd like to know what tutorial (or some other content) you wished you could have read, or a topic you wish you knew more about.

Detailed response are appreciated. Thanks a lot for reading this",28,25
1263,2017-7-30,2017,7,30,8,6qe1qx,What are some good references/reading for artificial reasoning?,https://www.reddit.com/r/MachineLearning/comments/6qe1qx/what_are_some_good_referencesreading_for/,bourbondog,1501369242,[removed],0,1
1264,2017-7-30,2017,7,30,9,6qefkd,Explanation of DeepMind's Early Visual Concept Learning with Unsupervised Deep Learning,https://www.reddit.com/r/MachineLearning/comments/6qefkd/explanation_of_deepminds_early_visual_concept/,[deleted],1501373805,[deleted],0,1
1265,2017-7-30,2017,7,30,9,6qefn2,Explanation of DeepMind's Early Visual Concept Learning with Unsupervised Deep Learning (x-post),https://www.reddit.com/r/MachineLearning/comments/6qefn2/explanation_of_deepminds_early_visual_concept/,RSchaeffer,1501373832,,0,1
1266,2017-7-30,2017,7,30,17,6qge19,High speed wallboard type napkin paper machine,https://www.reddit.com/r/MachineLearning/comments/6qge19/high_speed_wallboard_type_napkin_paper_machine/,chinatissuemachine,1501403789,,0,1
1267,2017-7-30,2017,7,30,18,6qgic7,Should I study Cognitive Systems?,https://www.reddit.com/r/MachineLearning/comments/6qgic7/should_i_study_cognitive_systems/,[deleted],1501406326,[removed],0,1
1268,2017-7-30,2017,7,30,20,6qgx6v,Systematic approach to logic puzzles?,https://www.reddit.com/r/MachineLearning/comments/6qgx6v/systematic_approach_to_logic_puzzles/,RGregoryClark,1501414629,[removed],0,1
1269,2017-7-30,2017,7,30,23,6qhjrs,Why use ML?,https://www.reddit.com/r/MachineLearning/comments/6qhjrs/why_use_ml/,random_postings,1501424084,[removed],0,1
1270,2017-7-30,2017,7,30,23,6qhknr,Is there existing tool to implement the CRF model in image?,https://www.reddit.com/r/MachineLearning/comments/6qhknr/is_there_existing_tool_to_implement_the_crf_model/,qdinfish,1501424395,,1,1
1271,2017-7-31,2017,7,31,0,6qhw43,"Andrew Ng: AI Worry is like Mars Overpopulation Worry, Elon Musk is a Self-interested AI Fearmonger",https://www.reddit.com/r/MachineLearning/comments/6qhw43/andrew_ng_ai_worry_is_like_mars_overpopulation/,netcribe,1501428091,,0,1
1272,2017-7-31,2017,7,31,2,6qih7g,What topics in multivariable-calculus are important for machine learning?,https://www.reddit.com/r/MachineLearning/comments/6qih7g/what_topics_in_multivariablecalculus_are/,hithere_23,1501434384,[removed],0,1
1273,2017-7-31,2017,7,31,2,6qiknh,Segmenting customers and performing market research using Microsoft Cognitive Services,https://www.reddit.com/r/MachineLearning/comments/6qiknh/segmenting_customers_and_performing_market/,maguirej160,1501435386,,0,1
1274,2017-7-31,2017,7,31,2,6qinlr,Machine Learning - based Personality Analysis of a Failed Finance Minister,https://www.reddit.com/r/MachineLearning/comments/6qinlr/machine_learning_based_personality_analysis_of_a/,Florents,1501436217,,0,1
1275,2017-7-31,2017,7,31,4,6qj7cg,Train/test split of TF detection API,https://www.reddit.com/r/MachineLearning/comments/6qj7cg/traintest_split_of_tf_detection_api/,[deleted],1501441806,[removed],0,1
1276,2017-7-31,2017,7,31,4,6qje22,ML Algorithm for Pulling Multiple Variables,https://www.reddit.com/r/MachineLearning/comments/6qje22/ml_algorithm_for_pulling_multiple_variables/,SkornRising,1501443750,[removed],0,1
1277,2017-7-31,2017,7,31,4,6qjg7p,[P] Cutting Edge Deep Learning for CodersLaunching Deep Learning Part 2,https://www.reddit.com/r/MachineLearning/comments/6qjg7p/p_cutting_edge_deep_learning_for_coderslaunching/,jeremyhoward,1501444378,,16,269
1278,2017-7-31,2017,7,31,5,6qjj6q,[R] Train/test split of TF detection API,https://www.reddit.com/r/MachineLearning/comments/6qjj6q/r_traintest_split_of_tf_detection_api/,Ebusr,1501445219,"I would like to use the Tensorflow's pretrained models on coco, but I can't find the image split they used for training and testing. 

According to their ""Speed/accuracy trade-offs"" paper - ""Our networks are trained on the COCO dataset, using
all training images as well as a subset of validation images,
holding out 8000 examples for validation"".

Does anyone know which data they used for testing?",1,1
1279,2017-7-31,2017,7,31,5,6qjqka,"A wizards guide to Adversarial Autoencoders: Part 1, Autoencoder?",https://www.reddit.com/r/MachineLearning/comments/6qjqka/a_wizards_guide_to_adversarial_autoencoders_part/,naresh1318,1501447188,[removed],0,1
1280,2017-7-31,2017,7,31,6,6qjw5w,Blog post: Learning to reconstruct  r/InverseProblems,https://www.reddit.com/r/MachineLearning/comments/6qjw5w/blog_post_learning_to_reconstruct_rinverseproblems/,[deleted],1501448668,[deleted],0,1
1281,2017-7-31,2017,7,31,6,6qjwcr,[R] Blog post: Learning to reconstruct  r/InverseProblems,https://www.reddit.com/r/MachineLearning/comments/6qjwcr/r_blog_post_learning_to_reconstruct/,adler-j,1501448721,,0,7
1282,2017-7-31,2017,7,31,6,6qjx4w,Is AI a threat or not?,https://www.reddit.com/r/MachineLearning/comments/6qjx4w/is_ai_a_threat_or_not/,evergreenreeds,1501448918,[removed],0,1
1283,2017-7-31,2017,7,31,6,6qjxbn,What literature should i start learning from?,https://www.reddit.com/r/MachineLearning/comments/6qjxbn/what_literature_should_i_start_learning_from/,ragracha,1501448975,[removed],0,1
1284,2017-7-31,2017,7,31,6,6qjyy0,Open source web app that predicts wildfires up to a week in advance using a Support vector machine.,https://www.reddit.com/r/MachineLearning/comments/6qjyy0/open_source_web_app_that_predicts_wildfires_up_to/,Wildfire_AWARE,1501449442,[removed],0,1
1285,2017-7-31,2017,7,31,7,6qkb17,Help running this schema of decision tree with MLlib?,https://www.reddit.com/r/MachineLearning/comments/6qkb17/help_running_this_schema_of_decision_tree_with/,[deleted],1501452958,[removed],0,1
1286,2017-7-31,2017,7,31,8,6qkteh,Help,https://www.reddit.com/r/MachineLearning/comments/6qkteh/help/,[deleted],1501458667,[removed],0,1
1287,2017-7-31,2017,7,31,9,6qkv11,bias in algos and machines doesn't matter yea? links inside,https://www.reddit.com/r/MachineLearning/comments/6qkv11/bias_in_algos_and_machines_doesnt_matter_yea/,[deleted],1501459217,[removed],0,1
1288,2017-7-31,2017,7,31,9,6qkzjw,Help,https://www.reddit.com/r/MachineLearning/comments/6qkzjw/help/,[deleted],1501460650,[removed],0,1
1289,2017-7-31,2017,7,31,9,6ql01g,Optimizing Websites with Data Science,https://www.reddit.com/r/MachineLearning/comments/6ql01g/optimizing_websites_with_data_science/,GrimCandy,1501460801,,0,1
1290,2017-7-31,2017,7,31,9,6ql5cc,Am I missing something?,https://www.reddit.com/r/MachineLearning/comments/6ql5cc/am_i_missing_something/,help123321,1501462554,[removed],0,1
1291,2017-7-31,2017,7,31,9,6ql5cw,"[D] The same old historicism, now on AI",https://www.reddit.com/r/MachineLearning/comments/6ql5cw/d_the_same_old_historicism_now_on_ai/,perone,1501462559,,35,15
1292,2017-7-31,2017,7,31,10,6ql73l,[R] Recurrent Ladder Networks,https://www.reddit.com/r/MachineLearning/comments/6ql73l/r_recurrent_ladder_networks/,xternalz,1501463164,,17,23
1293,2017-7-31,2017,7,31,10,6ql9bh,[P] ML Algorithm to Replace Controller,https://www.reddit.com/r/MachineLearning/comments/6ql9bh/p_ml_algorithm_to_replace_controller/,CircuitBeast,1501463909,I'd like to use an algorithm + uController to replace the control unit of a system. The controller is a PI controller for a power system. The objective is to optimize the response. Can a deep learning algorithm do this? ,11,1
1294,2017-7-31,2017,7,31,11,6qllo7,How would you do DNN ensembles?,https://www.reddit.com/r/MachineLearning/comments/6qllo7/how_would_you_do_dnn_ensembles/,JayYip,1501468198,[removed],1,1
1295,2017-7-31,2017,7,31,11,6qlmkg,A neural network that generates quotes about love,https://www.reddit.com/r/MachineLearning/comments/6qlmkg/a_neural_network_that_generates_quotes_about_love/,AndronovHopf,1501468509,,0,1
1296,2017-7-31,2017,7,31,12,6qlum2,Motorbike Finance and Loan Calculator simplymotorbikefinancecom,https://www.reddit.com/r/MachineLearning/comments/6qlum2/motorbike_finance_and_loan_calculator/,kingslymerwindv,1501471266,,0,1
1297,2017-7-31,2017,7,31,12,6qlya6,"[R] Genetic CNN, ICCV'17",https://www.reddit.com/r/MachineLearning/comments/6qlya6/r_genetic_cnn_iccv17/,xternalz,1501472612,,1,5
1298,2017-7-31,2017,7,31,12,6qlyhj,C Frame 20 Ton Air Hydraulic Press For Assembly And Press-in,https://www.reddit.com/r/MachineLearning/comments/6qlyhj/c_frame_20_ton_air_hydraulic_press_for_assembly/,xtm-mattchan,1501472693,,0,1
1299,2017-7-31,2017,7,31,14,6qmhmg,US customers come to find chewing gum production equipment,https://www.reddit.com/r/MachineLearning/comments/6qmhmg/us_customers_come_to_find_chewing_gum_production/,mixmachinery,1501479933,,0,1
1300,2017-7-31,2017,7,31,15,6qmndx,GPR Of Concrete,https://www.reddit.com/r/MachineLearning/comments/6qmndx/gpr_of_concrete/,abcndtt,1501482339,[removed],0,1
1301,2017-7-31,2017,7,31,15,6qmow0,[D] The Risks of Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/6qmow0/d_the_risks_of_artificial_intelligence/,thinkingwires,1501482960,,58,0
1302,2017-7-31,2017,7,31,15,6qmqei,[N] The Promise of AI,https://www.reddit.com/r/MachineLearning/comments/6qmqei/n_the_promise_of_ai/,[deleted],1501483614,[deleted],0,1
1303,2017-7-31,2017,7,31,15,6qmqkh,[N] The Promise of AI - Frank Chen @ A16Z,https://www.reddit.com/r/MachineLearning/comments/6qmqkh/n_the_promise_of_ai_frank_chen_a16z/,thebackpropaganda,1501483679,,0,15
1304,2017-7-31,2017,7,31,16,6qmxzs,[D] Certification,https://www.reddit.com/r/MachineLearning/comments/6qmxzs/d_certification/,notsoeasytopickaname,1501486893,Machine learning jobs in my country require you to have either experience or a certificate confirming that you have the necessary knowledge. What are some of the best offers online right now in terms of cost and quality of the certificate?,2,0
1305,2017-7-31,2017,7,31,16,6qmz6s,300X300 two color printing napkin paper machine,https://www.reddit.com/r/MachineLearning/comments/6qmz6s/300x300_two_color_printing_napkin_paper_machine/,chinatissuemachine,1501487437,,0,1
1306,2017-7-31,2017,7,31,17,6qn2af,Serial Number Service,https://www.reddit.com/r/MachineLearning/comments/6qn2af/serial_number_service/,volkanozyilmaz,1501488848,[removed],0,1
1307,2017-7-31,2017,7,31,17,6qn57n,"[P] Neural network that predicts a winner of a Dota 2 match, with web UI",https://www.reddit.com/r/MachineLearning/comments/6qn57n/p_neural_network_that_predicts_a_winner_of_a_dota/,Risse,1501490269,"http://dota.polso.info/

Full post in r/dota2: https://www.reddit.com/r/DotA2/comments/6phj02/i_made_an_ai_that_predicts_the_winner_of_a_dota/

The backend is on node and the ML library used is [brain.js](https://github.com/harthur-org/brain.js). It's a great library, really easy to use even for a total ML newbie.

If you got any questions, ask away!",5,15
1308,2017-7-31,2017,7,31,18,6qn9uw,Most Effective Technique of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/6qn9uw/most_effective_technique_of_machine_learning/,kqgu67,1501492441,,0,1
1309,2017-7-31,2017,7,31,18,6qnd1j,Thit b bm ht chn khng,https://www.reddit.com/r/MachineLearning/comments/6qnd1j/thit_b_bm_ht_chn_khng/,vietchemhn,1501493872,,0,1
1310,2017-7-31,2017,7,31,18,6qnfrq,[R] Variance of svmRadialCost final model using Caret package in R.,https://www.reddit.com/r/MachineLearning/comments/6qnfrq/r_variance_of_svmradialcost_final_model_using/,partyAddict13,1501495138,"Hi, I have trained a model using the svMRadialCost model in Caret for a regression task. How can I find out the variance explained by the final model? Final model only shows the tuned values of epsilon and C.",3,4
1311,2017-7-31,2017,7,31,20,6qnv7i,Random Dilation Networks for Action Recognition in Videos,https://www.reddit.com/r/MachineLearning/comments/6qnv7i/random_dilation_networks_for_action_recognition/,[deleted],1501501393,[deleted],0,1
1312,2017-7-31,2017,7,31,20,6qnvdp,[R] Random Dilation Networks for Action Recognition in Videos,https://www.reddit.com/r/MachineLearning/comments/6qnvdp/r_random_dilation_networks_for_action_recognition/,erogol,1501501451,,4,33
1313,2017-7-31,2017,7,31,21,6qnya2,[R] Machine Learning Weekly Review 4,https://www.reddit.com/r/MachineLearning/comments/6qnya2/r_machine_learning_weekly_review_4/,rldlml,1501502459,,0,1
1314,2017-7-31,2017,7,31,21,6qo8sb,[N] I made an overview of the changes in Tensorflow version 1.3,https://www.reddit.com/r/MachineLearning/comments/6qo8sb/n_i_made_an_overview_of_the_changes_in_tensorflow/,Dutchcheesehead,1501505973,,6,12
1315,2017-7-31,2017,7,31,22,6qohta,[N] Federated Learning: Collaborative Machine Learning without Centralized Training Data  r/iotml,https://www.reddit.com/r/MachineLearning/comments/6qohta/n_federated_learning_collaborative_machine/,onegazillion,1501508693,,0,4
1316,2017-7-31,2017,7,31,22,6qoitn,[R] A non-NLP application of Word2Vec,https://www.reddit.com/r/MachineLearning/comments/6qoitn/r_a_nonnlp_application_of_word2vec/,HichamEB,1501508974,,14,94
1317,2017-7-31,2017,7,31,23,6qoslg,Brain-like computing comes closer with neuromorphic materials for artificially intelligent retina,https://www.reddit.com/r/MachineLearning/comments/6qoslg/brainlike_computing_comes_closer_with/,benbrum,1501511697,,0,1
