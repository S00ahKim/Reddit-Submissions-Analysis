,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2016-6-1,2016,6,1,9,4lyg39,Request: example to show the need/potential of probabilistic programming,https://www.reddit.com/r/MachineLearning/comments/4lyg39/request_example_to_show_the_needpotential_of/,Zeekawla99ii,1464740288,"I'm afraid I still don't understand why one would used probabilistic programming, or really what it is. 

It appears it somehow ""tailors"" ML models given the dataset. 

Any guidance here would be greatly appreciated. Thanks",6,0
1,2016-6-1,2016,6,1,10,4lyr4y,A kNN algorithm with a unfixed k?,https://www.reddit.com/r/MachineLearning/comments/4lyr4y/a_knn_algorithm_with_a_unfixed_k/,Recolumn,1464744552,"I am wondering if there is any research out their about an kNN classifier with a optimized algorithm where a function is trained upon the training data set that maps a point to a value of k. Then, when the algorithm needs to classify a new point, it first looks for the nearest point in this trained function to find what value k it should use.

 (This is based off the assumption that a varied k will actually make a better classification. It seems like it would to me, depending on the shape of the data points)

Any thoughts or links to research like this? Or any other research about kNN optimization?",6,0
2,2016-6-1,2016,6,1,11,4lyxg4,How to make machines learn like humans: brain-like AI and machine learning,https://www.reddit.com/r/MachineLearning/comments/4lyxg4/how_to_make_machines_learn_like_humans_brainlike/,edworldreddit,1464746983,[removed],0,1
3,2016-6-1,2016,6,1,11,4lyxt0,Video of Talk on Convergent Learning: Do different neural networks learn the same representations,https://www.reddit.com/r/MachineLearning/comments/4lyxt0/video_of_talk_on_convergent_learning_do_different/,[deleted],1464747108,[deleted],0,1
4,2016-6-1,2016,6,1,12,4lz9ak,pre-trained deep convolution models for extracting deep features?,https://www.reddit.com/r/MachineLearning/comments/4lz9ak/pretrained_deep_convolution_models_for_extracting/,upulbandara,1464751648,"Does anyone know how to use pre-trained deep convolution models for extracting deep features?
Cafe model zoo [1] has a lot of pre-trained models such as VGG , LeNet and etc, but it seems like a good tutorial on how to use those models ( in Python ) is missing.
[1]. https://github.com/BVLC/caffe/wiki/Model-Zoo...
See More",2,0
5,2016-6-1,2016,6,1,13,4lzjkj,How to start Machine Learning with Java?,https://www.reddit.com/r/MachineLearning/comments/4lzjkj/how_to_start_machine_learning_with_java/,abdalimran,1464756201,"I am a Java developer. I have huge interest on Machine Learning but don't know how should I start. I have some questions and need some suggestions from you ML guys-

1. How is Java as a programming language to use in Machine Learning?

2. I've found many resources of Machine Learning which used Python. In case of Machine Learning do the Python guys get more advantage over the Java guys?

3. In industry level what programming languages are largely used for Machine Learning?

4. I want to start learning Machine Learning using Java. How do I start? Please suggest me some specific resources.

Thanks a lot. Hope I'll get quality responses. :)",6,0
6,2016-6-1,2016,6,1,14,4lzpmf,"Laws, Sausages and ConvNets (convolutional networks, implementation details)",https://www.reddit.com/r/MachineLearning/comments/4lzpmf/laws_sausages_and_convnets_convolutional_networks/,shaomer,1464759205,,0,11
7,2016-6-1,2016,6,1,15,4lzsy7,Tensor Processing Unit could advance Moore's Law 7 years into the future,https://www.reddit.com/r/MachineLearning/comments/4lzsy7/tensor_processing_unit_could_advance_moores_law_7/,bluewhackadoo,1464760914,,0,0
8,2016-6-1,2016,6,1,15,4lzu33,confused by notation,https://www.reddit.com/r/MachineLearning/comments/4lzu33/confused_by_notation/,rkeonp,1464761498,"Hi guys I'm kinda new on the field and I'm trying to understand this paper (http://www.cise.ufl.edu/~tichen/pdf/cmig.pdf) and I come around this kind of notation: http://imgur.com/qxvSETC

Can someone please help me understand what those numbers ""2"" and "" || "" mean, I will greatly appreciate it.",2,0
9,2016-6-1,2016,6,1,15,4lzwqf,Finding sets of similar things,https://www.reddit.com/r/MachineLearning/comments/4lzwqf/finding_sets_of_similar_things/,gregw134,1464762864,"Hi all, 

Does anyone have any good reading or code for finding sets of similar things? For example, given a text dataset like every comment on Reddit, how would you find lists of cities, or cocktails, or politicians? If nothing else, I want to know what the name of this problem is, so I can research it...thanks",5,0
10,2016-6-1,2016,6,1,17,4m08kc,Parametric Exponential Linear Unit for Deep Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4m08kc/parametric_exponential_linear_unit_for_deep/,x2342,1464769561,,6,9
11,2016-6-1,2016,6,1,17,4m091m,Dose the Pre-trained model affect the Feature Extraction in Deep learning ?,https://www.reddit.com/r/MachineLearning/comments/4m091m/dose_the_pretrained_model_affect_the_feature/,mustafaihssan,1464769863,"I'm Working with Deep learning using Caffe, and i'm trying to extract features for some dataset, to do that i'm just take the output of the next-to-last layer, but to do that i'v to use Pre-Trained model, with in this case i'm using VGG16.

i'm wondering if i train my model with something similar to what i want to extract, dose this improve the extraction, or this will be a waste of time.  

// Sorry for my bad English ",1,0
12,2016-6-1,2016,6,1,18,4m0dmt,[1605.09410v1] End-to-End Instance Segmentation and Counting with Recurrent Attention,https://www.reddit.com/r/MachineLearning/comments/4m0dmt/160509410v1_endtoend_instance_segmentation_and/,senorstallone,1464772701,,0,5
13,2016-6-1,2016,6,1,18,4m0ei7,News in artificial intelligence and machine learning you should know about: month of May!,https://www.reddit.com/r/MachineLearning/comments/4m0ei7/news_in_artificial_intelligence_and_machine/,nb410,1464773246,,0,0
14,2016-6-1,2016,6,1,19,4m0i8b,Announcement: AMA Wednesday 2nd with the team behind malariaspot.org,https://www.reddit.com/r/MachineLearning/comments/4m0i8b/announcement_ama_wednesday_2nd_with_the_team/,[deleted],1464775415,[deleted],0,0
15,2016-6-1,2016,6,1,19,4m0j9z,Announcement: AMA Thursday 2nd with the team behind malariaspot.org,https://www.reddit.com/r/MachineLearning/comments/4m0j9z/announcement_ama_thursday_2nd_with_the_team/,cavedave,1464776030,Starting morning time Eastern USA the team behind malariaspot will answer questions about how they built a game to build up data. And how ML can be used to help fight Malaria,3,0
16,2016-6-1,2016,6,1,19,4m0o2z,"Asynchrony begets Momentum, with an Application to Deep Learning",https://www.reddit.com/r/MachineLearning/comments/4m0o2z/asynchrony_begets_momentum_with_an_application_to/,mttd,1464778544,,0,7
17,2016-6-1,2016,6,1,20,4m0q4z,Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4m0q4z/curiositydriven_exploration_in_deep_reinforcement/,Bayes-Ian,1464779583,,0,32
18,2016-6-1,2016,6,1,20,4m0tiw,[1605.09304v1] Images synthesized from scratch,https://www.reddit.com/r/MachineLearning/comments/4m0tiw/160509304v1_images_synthesized_from_scratch/,[deleted],1464781207,[deleted],1,0
19,2016-6-1,2016,6,1,20,4m0vl9,"Deep Listening: the Neural Network Learning to Hear in a Crowd - A Q&amp;A with John Hershey, Principal Scientist of Mitsubishi Electric Research Lab",https://www.reddit.com/r/MachineLearning/comments/4m0vl9/deep_listening_the_neural_network_learning_to/,reworksophie,1464782238,,0,1
20,2016-6-1,2016,6,1,21,4m0xmd,[1605.09782] Adversarial Feature Learning,https://www.reddit.com/r/MachineLearning/comments/4m0xmd/160509782_adversarial_feature_learning/,ajmooch,1464783195,,5,15
21,2016-6-1,2016,6,1,21,4m0zeu,Resources on Microsoft COCO?,https://www.reddit.com/r/MachineLearning/comments/4m0zeu/resources_on_microsoft_coco/,saucysassy,1464784037,"Hi,

[MS COCO](http://mscoco.org/explore/) seems like an awesome dataset.
I'm sure a model trained/finetuned on this dataset is useful in many other problems. But I haven't found code repos/pretrained models by googling.

Can we collect here all the relevant repos and/or pertained models for this dataset?

Thanks.",1,0
22,2016-6-1,2016,6,1,22,4m171n,"Theory of Bandit Algorithms, Part 1 (Part 2 in comments)",https://www.reddit.com/r/MachineLearning/comments/4m171n/theory_of_bandit_algorithms_part_1_part_2_in/,Hydreigon92,1464787245,,1,40
23,2016-6-1,2016,6,1,22,4m19pw,Question about machine learning,https://www.reddit.com/r/MachineLearning/comments/4m19pw/question_about_machine_learning/,[deleted],1464788310,[deleted],1,0
24,2016-6-1,2016,6,1,22,4m1c6x,How to use parsed User-Agent to find fraud?,https://www.reddit.com/r/MachineLearning/comments/4m1c6x/how_to_use_parsed_useragent_to_find_fraud/,shakedzy,1464789247,"I want to try and build a NN that uses parsed user-agents to determine if the user-agent is genuine or not. I can parse out things like the OS and its version, the browser and its version, etc.
Some combination can point on fraudulent user-agent. The question is, what is the best way use the strings as inputs to a NN?",1,0
25,2016-6-1,2016,6,1,23,4m1fnt,"Apart from Silicon Valley, best cities in the world to find machine learning jobs?",https://www.reddit.com/r/MachineLearning/comments/4m1fnt/apart_from_silicon_valley_best_cities_in_the/,[deleted],1464790462,[deleted],0,1
26,2016-6-1,2016,6,1,23,4m1gch,"Principal Components Regression, Pt. 3: Picking the Number of Components",https://www.reddit.com/r/MachineLearning/comments/4m1gch/principal_components_regression_pt_3_picking_the/,pmigdal,1464790701,,0,0
27,2016-6-1,2016,6,1,23,4m1kjg,"Apart from Silicon valley, best places worldwide to find machine learning jobs?",https://www.reddit.com/r/MachineLearning/comments/4m1kjg/apart_from_silicon_valley_best_places_worldwide/,smoothjazzradiohits,1464792189,[removed],1,1
28,2016-6-1,2016,6,1,23,4m1l2e,[1605.09304v1] Synthesizing the preferred inputs for neurons in neural networks via deep generator networks,https://www.reddit.com/r/MachineLearning/comments/4m1l2e/160509304v1_synthesizing_the_preferred_inputs_for/,j1395010,1464792361,,5,18
29,2016-6-1,2016,6,1,23,4m1mdl,A Gentle Introduction to Recommender Systems with Implicit Feedback,https://www.reddit.com/r/MachineLearning/comments/4m1mdl/a_gentle_introduction_to_recommender_systems_with/,jessesw,1464792799,,0,1
30,2016-6-1,2016,6,1,23,4m1ne6,"ICLR Oral Video on ""Convergent Learning: Do different neural networks learn the same representations?""",https://www.reddit.com/r/MachineLearning/comments/4m1ne6/iclr_oral_video_on_convergent_learning_do/,jclune,1464793125,,0,8
31,2016-6-2,2016,6,2,0,4m1o7i,[1605.09735] Information Theoretically Aided Reinforcement Learning for Embodied Agents,https://www.reddit.com/r/MachineLearning/comments/4m1o7i/160509735_information_theoretically_aided/,InaneMembrane,1464793383,,0,4
32,2016-6-2,2016,6,2,1,4m207n,[1605.09721] Conflict-free Asynchronous Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4m207n/160509721_conflictfree_asynchronous_machine/,smoothindeed,1464797285,,1,4
33,2016-6-2,2016,6,2,1,4m26zn,Learning ML with multiple time series,https://www.reddit.com/r/MachineLearning/comments/4m26zn/learning_ml_with_multiple_time_series/,Large_Eddy,1464799523,I am learning and putting together a demo on time series analysis with machine learning.  I have lots of time series of various measurements (temperature and voltage for example) from various components in a cluster.  I am not really interested in forecasting.  I have been thinking about looking at how one component affects another in the same system.  I would be open to other ideas that might be interesting and instructive.,3,13
34,2016-6-2,2016,6,2,2,4m2g61,Bayesian Neural Networks with ADVI via PyMC3,https://www.reddit.com/r/MachineLearning/comments/4m2g61/bayesian_neural_networks_with_advi_via_pymc3/,Kombutini,1464802532,,2,7
35,2016-6-2,2016,6,2,2,4m2i2j,Would it be possible to do audio restoration with ML?,https://www.reddit.com/r/MachineLearning/comments/4m2i2j/would_it_be_possible_to_do_audio_restoration_with/,Lajamerr_Mittesdine,1464803130,"Is it possible to do restoration of compressed audio with a trained NN?

It'd probably have to be a big dataset.

For the training imagine a large set of FLACs and each FLAC has multiple encodings of the original file with various codecs(opus, AAC, etc) and at various bitrates.

Do you think it's possible for the NN to learn the flawed outputs and approximately correct it towards what it ""thinks"" is the original output.

Or is this essentially in the same league of hopelessness of wanting to know if ML could turn a hash into the original input if given enough hashes and inputs.",1,2
36,2016-6-2,2016,6,2,2,4m2in5,Derivative of sigmoid(x) = sigmoid(x)*sigmoid(-x),https://www.reddit.com/r/MachineLearning/comments/4m2in5/derivative_of_sigmoidx_sigmoidxsigmoidx/,BenRayfield,1464803310,"May be useful in less confusing ways to compute backprop.

sigmoid(x) = (1/(1+e^-x))

Derivative of sigmoid(x) is sigmoid(x)*(1-sigmoid(x))

sigmoid(x) + sigmoid(-x) = 1

Derivative of sigmoid(x) = sigmoid(x)*sigmoid(-x)

http://wolframalpha.com says

( (1/(1+e^-(10.001))) - (1/(1+e^-(10))) )*1000
= 0.0000453731

(1/(1+e^-(10))) * (1/(1+e^-(-10)))
= 0.000045395807735951671032442043171337095144100085644951056

Derivative of sigmoid(x) is sigmoid(x), in the limit of x -&gt; -infinity.

Derivative of e^x = e^x

sigmoid(x) = e^x, in the limit of x-&gt; -infinity

sigmoid(x) = 1-e^x, in the limit of x-&gt; infinity

sigmoid(x) = x/4, in the limit of x-&gt; 0 from either side

Sigmoid is continuous between exponential and linear.",5,0
37,2016-6-2,2016,6,2,2,4m2j3h,How long to deploy a practical application of ML,https://www.reddit.com/r/MachineLearning/comments/4m2j3h/how_long_to_deploy_a_practical_application_of_ml/,OpticalDelusion,1464803444,"I'm a full-time software developer with a passion for machine learning. My formal education includes much of the mathematics (linear algebra, calculus, etc). I have a general understanding of how neural networks, activation functions, back-propagation and such things work.

I'd like to know how much time you think it would take for me to gain a deeper understanding and to create a practical application using machine learning.

This is obviously a broad question, so I'm mostly looking to know what your background was when you started learning ML, what kind of application you built and at what level of complexity, and how long it took you to go from A to B.

This would help me immensely, as I've been presented with several opportunities involving ML development, but I cannot make a proper decision as I don't fully grasp the opportunity cost.

Thanks!",3,1
38,2016-6-2,2016,6,2,3,4m2o39,Magenta - a new project from the Google Brain team. Can we use machine learning to create compelling art and music?,https://www.reddit.com/r/MachineLearning/comments/4m2o39/magenta_a_new_project_from_the_google_brain_team/,forloopsarebad,1464804994,,31,60
39,2016-6-2,2016,6,2,5,4m38gn,Question Regarding Training Thin v Deep Networks,https://www.reddit.com/r/MachineLearning/comments/4m38gn/question_regarding_training_thin_v_deep_networks/,montgomerybradford,1464811487,"Hey folks---Sorry if this isn't the best place to ask. I'm using {mxnet} to fit a feed forward network for text classification (in R), where I'm categorizing documents into one of 10 classes. To keep classes balanced, I'm keeping it to 50k documents, with 5k in each class.

When I fit one- or two-layer network, learning seems to happen pretty quickly. Accuracy might stay at 10% for a few iterations, but then picks up. When I try fitting a deeper network, the accuracy might stay ~10% for many iterations---sometimes nearly 200---before the rate starts to steadily increase. I've tried toying with the learning rate, from 0.5 to 1e-4, but it doesn't seem to make the learning happen faster.

This seems abnormal, but I don't know what I'm missing. Is there a particular parameter (not necessarily specific to mxnet, just generally) that I'm not thinking to adjust? Is this a common 'noob' problem in fitting a FF network? 



(As an aside---any time I pass data with imbalanced classes to the network, the network converges to a predicting a single class for every case. This happens even if the difference between the number of cases with each class label is small---say the largest class has 5000 training cases while the smallest has 4950. Is that normal?)",4,0
40,2016-6-2,2016,6,2,5,4m3euk,Cramming for the test set: We need better ways to evaluate analogies,https://www.reddit.com/r/MachineLearning/comments/4m3euk/cramming_for_the_test_set_we_need_better_ways_to/,rspeer,1464813530,,2,0
41,2016-6-2,2016,6,2,6,4m3pd6,Facebook Announces DeepText,https://www.reddit.com/r/MachineLearning/comments/4m3pd6/facebook_announces_deeptext/,zintinio4,1464816923,,15,50
42,2016-6-2,2016,6,2,7,4m417z,AMD Polaris,https://www.reddit.com/r/MachineLearning/comments/4m417z/amd_polaris/,dharma-1,1464821095,"Anyone know what the FP16 perf of the new Polaris boards is going to be?

Any news regarding their CUDA compatibility efforts, the Boltzmann project?

Will deep learning frameworks at some point target Vulkan/SPIR-V for wider GPU compatibility (inc mobile)?",16,7
43,2016-6-2,2016,6,2,8,4m47z7,Anyone seen this artifacting training a variational auto-encoder over image data?,https://www.reddit.com/r/MachineLearning/comments/4m47z7/anyone_seen_this_artifacting_training_a/,jeiting,1464823643,,3,4
44,2016-6-2,2016,6,2,9,4m4j7u,The truth about deep learning,https://www.reddit.com/r/MachineLearning/comments/4m4j7u/the_truth_about_deep_learning/,[deleted],1464828096,[deleted],0,0
45,2016-6-2,2016,6,2,10,4m4phz,QUESTION: What games do you think one could play and apply machine learning algorithms? Have you done it before?,https://www.reddit.com/r/MachineLearning/comments/4m4phz/question_what_games_do_you_think_one_could_play/,Forgotten_Bug,1464830652,"Do you know games that can produce data in which I can extract, study and apply things like regression, classification, etc?

Eve Online?",3,0
46,2016-6-2,2016,6,2,10,4m4tfk,Help Me Find a Machine Learning Website that Let's You Build OCR Software,https://www.reddit.com/r/MachineLearning/comments/4m4tfk/help_me_find_a_machine_learning_website_that_lets/,fossilnews,1464832194,"I'm trying to find a website that this guy wrote.  In it he explains how machine learning works, gives you a history of its evolution and you build an OCR example as part of the tutorial using the MNIST DATABASE as your sample set.  I found this site on a friends computer and now I've lost it. :(  Anyone know the site I'm talking about?",3,0
47,2016-6-2,2016,6,2,11,4m4xpu,Trends @ ICLR 2016,https://www.reddit.com/r/MachineLearning/comments/4m4xpu/trends_iclr_2016/,evc123,1464833885,,5,80
48,2016-6-2,2016,6,2,14,4m5raz,how to compute wight from input vector ?,https://www.reddit.com/r/MachineLearning/comments/4m5raz/how_to_compute_wight_from_input_vector/,John_Smith111,1464846771,"hello 

if i have input vector X how can i compute params - W?

I want to use it to rain neural net ... 

maybe 
W*X =1  -- &gt; w = 1 /x 

10x all 
",3,0
49,2016-6-2,2016,6,2,15,4m5skg,Ucoiling machine,https://www.reddit.com/r/MachineLearning/comments/4m5skg/ucoiling_machine/,Mzfeeder,1464847394,[removed],0,1
50,2016-6-2,2016,6,2,16,4m5z25,Active Track(Visual Object Tracking algorithm used on DJI latest Phantom 4) test and comparison with CT and TLD,https://www.reddit.com/r/MachineLearning/comments/4m5z25/active_trackvisual_object_tracking_algorithm_used/,marvin-ml,1464850845,,0,1
51,2016-6-2,2016,6,2,16,4m602s,How To Prepare For A Machine Learning Interview,https://www.reddit.com/r/MachineLearning/comments/4m602s/how_to_prepare_for_a_machine_learning_interview/,thisbejim,1464851424,,16,27
52,2016-6-2,2016,6,2,16,4m61rx,Where can I find sources on data analysis for time series forecasting where the data has spatial properties?,https://www.reddit.com/r/MachineLearning/comments/4m61rx/where_can_i_find_sources_on_data_analysis_for/,[deleted],1464852424,[deleted],0,0
53,2016-6-2,2016,6,2,16,4m61wi,Could a neuroscientist understand a microprocessor?,https://www.reddit.com/r/MachineLearning/comments/4m61wi/could_a_neuroscientist_understand_a_microprocessor/,abstractcontrol,1464852491,,3,9
54,2016-6-2,2016,6,2,16,4m649v,What should I use for a recommendation system that learns from a user's behaviour?,https://www.reddit.com/r/MachineLearning/comments/4m649v/what_should_i_use_for_a_recommendation_system/,Procrastinator300,1464853925,"I've asked this question here before but I think the answer wasn't much helpful to me as I'm really really new to ML. Also it was just 1 reply.

Question:

I'm trying to make an app that will recommend/predict my next ""move"" (where a move would be opening a particular program, website, etc) on my computer based upon my previous moves, current time, day of the week etc. 
I've thought about using RNN or reinforcement learning but I also want the number of classes to be of dynamic size or whatever is preferable so that when I install a new software/game, the program also gives me suggestions for that program.

Can you guys please suggest me what kind of model I should use? And also some pointers to get started with that model as I'm just a programmer and will only be making the program trial and error or copy pasting.

Thanks!",5,0
55,2016-6-2,2016,6,2,17,4m651x,DeepText Facebook's text understanding engine,https://www.reddit.com/r/MachineLearning/comments/4m651x/deeptext_facebooks_text_understanding_engine/,yassinelanda,1464854422,,0,0
56,2016-6-2,2016,6,2,17,4m686r,RNN &amp; LSTM now merged into Caffe,https://www.reddit.com/r/MachineLearning/comments/4m686r/rnn_lstm_now_merged_into_caffe/,pilooch,1464856475,,9,60
57,2016-6-2,2016,6,2,19,4m6ltx,Problems reproducing LSTM classification results ?,https://www.reddit.com/r/MachineLearning/comments/4m6ltx/problems_reproducing_lstm_classification_results/,illiterate_gorillas,1464865077,"I'm trying to reproduce Long-term Recurrent Convolutional Networks for Visual Recognition and Description (arXiv:1411.4389v3) from Jeff Donahue and others.

I'm only trying to reproduce the Activity Recognition results.
I have built the CNN with a similar result as the paper of 52% (flow) for a single frame model.
A single time step is a 4096 vector.(Not a one hot vector)

As I could not debug the LSTM code given by the authors I had re implemented it in tensorflow. 
{Used code based on this [example] (https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py)}
But I get an accuracy of 50% when I train a network with 16 frames as input.
Which is lower than the single frame model. I'm not sure Y this is so.

I have tried several variations of parameters.  And also length of sequences but without any improvement.
I have attached [results of one run](https://drive.google.com/folderview?id=0B_FC9xfwKC8eVHYxbDViUy05azg&amp;usp=sharing)

Does any one know Any good practices or tips? 
Any experiences anyone would like to share about training LSTMs for classification of time series data.
",5,7
58,2016-6-2,2016,6,2,20,4m6pcy,Paper for transfer learning,https://www.reddit.com/r/MachineLearning/comments/4m6pcy/paper_for_transfer_learning/,stonedfox8,1464867061,There was a paper here on Reddit that was about transfer learning for neural networks and that also had tensorflow code. Can anyone help me find it?,1,0
59,2016-6-2,2016,6,2,21,4m6ykd,[Reinforcement learning] Can we learn action embedding as high level goals?,https://www.reddit.com/r/MachineLearning/comments/4m6ykd/reinforcement_learning_can_we_learn_action/,nmiculinic,1464871341,"I've recently read Karpathy's [blogpost](http://karpathy.github.io/2016/05/31/rl/) about reinforcement learning and current techniques. Which got me thinking about few ideas. 


We perform learning differently than Policy gradients, MDP and similar methods. That is, we don't evaluate in each state every possible action and decide what's the most beneficial one. Instead we have layers of  actions here each layer describes our strategy more abstractly and more high-level. 


For example, in computer programming we tend to think on programming patterns level. Then when we design the system we think lower level, on concrete implementation &amp; layout of classes. Than even lower level what each class/function should do etc.


Another example of human action planning would be buying groceries. We certainly don't analyze each muscle movement to get us closer to the goal. Instead we have high level goal of locating supermarket. Than middle level, intermediate goal of travelling to supermarket. Lastly low level goals of changing our position by controlling our muscles.


Where I'm going with this? Towards action embedding. If we see someone walking from his house to the supermarker, that is seeing his multiple low-level actions, we can infer his higher level goal is going to the supermarket. 

Similarly in games we can have overall chess strategy which is composed of multiple lower level actions (moves) and we embedded common action groups as high level strategy. 

Thoughts? Comments? Has there been any interesting papers exploring action embedding further?",5,10
60,2016-6-2,2016,6,2,21,4m6z3c,Buying hardware for H2O software package. Who is able to answer RAM and GPU questions,https://www.reddit.com/r/MachineLearning/comments/4m6z3c/buying_hardware_for_h2o_software_package_who_is/,datasciguy-aaay,1464871560,"Hi I could really use help finding answers to two hardware related questions. I have searched already in the usual places. I am making decisions for hardware purchases.

Q1: Does H2O use GPU and if so how many per host

Q2: Does 8 GB RAM per node work and if so where are settings or variables for controlling spilling to disk versus RAM

Of course it depends on my data and model. I still need some answers about H2O -- let's use an analogy here OK? I know how to control RAM vs DISK spillover on Spark using the configuration variables. Where are these variables on H2O? thanks

Please respond only if you know something.  ""Did you read the manual"" and ""did you use Google yet"" and ""did you post questions to the mailing list and gitter chat"" and similar inane questions and discussions will be ignored.",6,0
61,2016-6-2,2016,6,2,22,4m73yb,Data Science Curriculum on edX,https://www.reddit.com/r/MachineLearning/comments/4m73yb/data_science_curriculum_on_edx/,rishiarora,1464873632,,0,0
62,2016-6-2,2016,6,2,22,4m74tj,"Machine Learning Is Everywhere: Netflix, Personalized Medicine, and Fraud Prevention",https://www.reddit.com/r/MachineLearning/comments/4m74tj/machine_learning_is_everywhere_netflix/,smoothindeed,1464873992,,0,0
63,2016-6-2,2016,6,2,22,4m77lx,Data Science With Python - Unleash the Power of Python &amp; Its Data Science Capabilities - Research and Markets,https://www.reddit.com/r/MachineLearning/comments/4m77lx/data_science_with_python_unleash_the_power_of/,rishiarora,1464875083,,0,0
64,2016-6-2,2016,6,2,23,4m7bw9,Correct &amp; easy to understand FizzBuzz using TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4m7bw9/correct_easy_to_understand_fizzbuzz_using/,hololens_user,1464876704,,4,28
65,2016-6-2,2016,6,2,23,4m7ci1,AMA: The MalariaSpot Team,https://www.reddit.com/r/MachineLearning/comments/4m7ci1/ama_the_malariaspot_team/,spotlab,1464876918,"The MalariaSpot Team will be answering your questions :) 

For more information about our project you can check: http://www.malariaspot.org or TEDx Talk ""Games and Crowdsourcing for Medical Image Diagnosis"" by our PI Miguel Luengo-Oroz (Talk at https://www.youtube.com/watch?v=Plv4qGDjCOA)

Looking forward to your questions!",6,14
66,2016-6-2,2016,6,2,23,4m7cjm,Speed of prediction: neural network vs. random forest?,https://www.reddit.com/r/MachineLearning/comments/4m7cjm/speed_of_prediction_neural_network_vs_random/,[deleted],1464876934,[deleted],1,0
67,2016-6-2,2016,6,2,23,4m7d0c,Fighting against class imbalance in a supervised ML problem.,https://www.reddit.com/r/MachineLearning/comments/4m7d0c/fighting_against_class_imbalance_in_a_supervised/,erogol,1464877090,,5,20
68,2016-6-2,2016,6,2,23,4m7h4f,Word Vectors (word2vec) in Eighteenth-Century Literature,https://www.reddit.com/r/MachineLearning/comments/4m7h4f/word_vectors_word2vec_in_eighteenthcentury/,Quadrismegistus4,1464878529,"Dear Machine Learning community,

I'm a humanist -- a literary scholar (ahem, PhD Student) studying eighteenth-century British literature. I'm also a *digital* humanist -- a refusing-to-recover life-long geek that now uses programming to uncover patterns in literary texts.

I wanted to share two blog posts I've written on using word2vec on 18C literature:


-- Word Vectors in the Eighteenth Century, Episode 1: Concepts (http://ryanheuser.org/word-vectors-1/)

-- Word Vectors in the Eighteenth Century, Episode 2: Methods (http://ryanheuser.org/word-vectors-2/)


I thought the Machine Learning community might be interested in how word2vec is being used within the (digital) humanities.

Would love to know what you all think!

peace,

Ryan

@quadrismegistus (https://twitter.com/quadrismegistus)",9,41
69,2016-6-2,2016,6,2,23,4m7ja2,How AI drives the mobile contextual revolution,https://www.reddit.com/r/MachineLearning/comments/4m7ja2/how_ai_drives_the_mobile_contextual_revolution/,esurior,1464879238,,0,0
70,2016-6-3,2016,6,3,0,4m7p5v,Deep learning with raspberry pi and tensorflow?,https://www.reddit.com/r/MachineLearning/comments/4m7p5v/deep_learning_with_raspberry_pi_and_tensorflow/,[deleted],1464881265,[deleted],21,3
71,2016-6-3,2016,6,3,0,4m7pvn,Safely interruptible agents,https://www.reddit.com/r/MachineLearning/comments/4m7pvn/safely_interruptible_agents/,RushAndAPush,1464881508,,3,7
72,2016-6-3,2016,6,3,1,4m7uyh,Workflow transition from Torch to TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4m7uyh/workflow_transition_from_torch_to_tensorflow/,[deleted],1464883234,[deleted],1,4
73,2016-6-3,2016,6,3,1,4m7xou,Your Thoughts on Elon Musk's Human Pet or Neural Lace?,https://www.reddit.com/r/MachineLearning/comments/4m7xou/your_thoughts_on_elon_musks_human_pet_or_neural/,melvinma,1464884175,"On Recode.net (http://www.recode.net/2016/6/2/11837544/elon-musk-neural-lace), Elon Musk talked about the near future that we will face super intelligences that are much better than human being and possible choices for humans - human pets or neural lace. I thought it is actually very reasonable question to think about today. Your thoughts on that? ",6,0
74,2016-6-3,2016,6,3,1,4m7zsn,"When we say PhD in NLP or PhD in bayesian networks or PhD in boosting, how all the topics listed below are related?",https://www.reddit.com/r/MachineLearning/comments/4m7zsn/when_we_say_phd_in_nlp_or_phd_in_bayesian/,Mr__Christian_Grey,1464884891,"There are three different types of topics in machine learning, the first ones are like NLP, Computer vision, Robotics etc. and other ones are algorithms in machine learning like genetic algorithms, neural networks, bayesian networks etc and thirdly there are concepts like decision trees, random forest, PCA etc.  

So, how are all these topics related when I say PhD in Bayesian Networks or PhD in NLP or PhD in boosting etc?  ",3,0
75,2016-6-3,2016,6,3,3,4m8ghf,What can I do to help prepare humanity for a superintelligent general AI?,https://www.reddit.com/r/MachineLearning/comments/4m8ghf/what_can_i_do_to_help_prepare_humanity_for_a/,MyBrainisMe,1464890406,[removed],10,0
76,2016-6-3,2016,6,3,3,4m8hb1,Uncertainty-GBM: Sklearn implementation of GBM to predict mu(X) and std(X) on heteroscedastic data,https://www.reddit.com/r/MachineLearning/comments/4m8hb1/uncertaintygbm_sklearn_implementation_of_gbm_to/,ofirnachum,1464890686,,2,14
77,2016-6-3,2016,6,3,3,4m8pmm,Google's Magenta Project: Can Machines Be Musicians?,https://www.reddit.com/r/MachineLearning/comments/4m8pmm/googles_magenta_project_can_machines_be_musicians/,massiveattack778,1464893409,,1,0
78,2016-6-3,2016,6,3,4,4m8s98,"Who is the ""obscure guy"" who invented Backprop, according to Hinton ?",https://www.reddit.com/r/MachineLearning/comments/4m8s98/who_is_the_obscure_guy_who_invented_backprop/,antonomase,1464894277,"In this video : https://youtu.be/XG-dwZMc7Ng?t=2m15s
Hinton says that Backprop was first invented by some ""obscure guy"" in the seventies. Does someone knows who is he talking about ?
Thanks !",20,17
79,2016-6-3,2016,6,3,5,4m9azr,Categories of Analytics - Quick and Easy,https://www.reddit.com/r/MachineLearning/comments/4m9azr/categories_of_analytics_quick_and_easy/,drcrook,1464900371,,0,1
80,2016-6-3,2016,6,3,6,4m9dx9,Should the KLD error always go down when training VAE?,https://www.reddit.com/r/MachineLearning/comments/4m9dx9/should_the_kld_error_always_go_down_when_training/,[deleted],1464901336,[removed],0,1
81,2016-6-3,2016,6,3,7,4m9oms,ML and mobile security,https://www.reddit.com/r/MachineLearning/comments/4m9oms/ml_and_mobile_security/,digitsman,1464905219,I am doing research this summer on mobile security and would like to do a project that incorporates ML. Mobile security in this case refers to network security with a focus on mobile devices. Does anyone have any ideas about where ML might be used in security? Is it even worth applying ML to security at all? ,5,0
82,2016-6-3,2016,6,3,8,4ma253,How is the training performed in the meta-learning paper of deepmind?,https://www.reddit.com/r/MachineLearning/comments/4ma253/how_is_the_training_performed_in_the_metalearning/,gameofml,1464910511,"I came across this paper from deepmind ""One-shot Learning with Memory-Augmented Neural Networks"" http://arxiv.org/pdf/1605.06065v1.pdf. It seems to me it is just multi-task learning with online training, and with examples shuffled at every iteration. Can anyone point out their difference?

BTW, they mentioned that labels are shuffled from dataset-to-dataset. How is this possible? Is this the same as shuffling examples?",6,12
83,2016-6-3,2016,6,3,9,4mae14,I have what I thought was a simple problem but seems to be imploding on itself.,https://www.reddit.com/r/MachineLearning/comments/4mae14/i_have_what_i_thought_was_a_simple_problem_but/,ArkGuardian,1464915144,"I am using an Apache Kafka stream to receive batches of data (every 20 seconds). All I wanted to do was for each batch turn it into a parquet file, run a multivariate gaussian package, and store it at it's unique timestamp. [My first issue is the files aren't being created](http://stackoverflow.com/questions/37584774/splitting-a-kafka-stream-into-time-partitioned-parquet-files) and my second issue is that the package I want to use is not a Spark ML Lib package?",2,0
84,2016-6-3,2016,6,3,10,4mafio,[1606.00704] Adversarially Learned Inference,https://www.reddit.com/r/MachineLearning/comments/4mafio/160600704_adversarially_learned_inference/,alexmlamb,1464915721,,17,27
85,2016-6-3,2016,6,3,11,4mar3t,"NAG: Neurogenesis Along the Gradient. Using gradient descent and back propagation to not only change weights but add new nodes, layers, and connections.",https://www.reddit.com/r/MachineLearning/comments/4mar3t/nag_neurogenesis_along_the_gradient_using/,sharp7,1464920409,"Hey everyone so I recently had an idea to solve the problem of having to arbitrarily choose the structure of your network. Convolution, fully connected, siamese and all the weird stuff google and other groups are doing should hopefully emerge from the algorithm itself instead of having to be decided by the coder. 

I propose NAG: Neurogenesis Along the Gradient that should hopefully grow networks into complex structures without coder input. I haven't tested it out yet, I wanted to get some feedback before coding it up. I wrote up the description in a pseudo-paper style in a google doc, feel free to comment on it as you would like. Here is the link: 

https://docs.google.com/document/d/1iOtKhXBEAV5rAK9F3GJNqscoWT_hvD7RSpvQvzTdc4c/edit?usp=sharing

I have a boring day job completely unrelated to machine learning, so if anyone wants to help me that would be great. Or if the idea is shit, or similar to existing ideas please let me know!",21,65
86,2016-6-3,2016,6,3,12,4mb0at,Generating Large Images from Latent Vectors - Part Two,https://www.reddit.com/r/MachineLearning/comments/4mb0at/generating_large_images_from_latent_vectors_part/,wei_jok,1464924574,,4,24
87,2016-6-3,2016,6,3,14,4mbfwy,[1606.00511] Large Scale Distributed Hessian-Free Optimization for Deep Neural Network,https://www.reddit.com/r/MachineLearning/comments/4mbfwy/160600511_large_scale_distributed_hessianfree/,fulcrum_xyz,1464932183,,2,1
88,2016-6-3,2016,6,3,17,4mbwwu,Facing your data with Chernoff faces,https://www.reddit.com/r/MachineLearning/comments/4mbwwu/facing_your_data_with_chernoff_faces/,[deleted],1464941868,[deleted],0,0
89,2016-6-3,2016,6,3,18,4mc0y0,Do you guys think that machine learning and ai jobs will be like the gaming industry?,https://www.reddit.com/r/MachineLearning/comments/4mc0y0/do_you_guys_think_that_machine_learning_and_ai/,Y247ks,1464944506,[removed],0,1
90,2016-6-3,2016,6,3,18,4mc5po,"LAMINATION MACHINE KAVINSTAR PRICE IN DELHI, GURGAON, NOIDA | Fax, EPABX, Office Equipment | New Delhi | Delhi | India | FREE CLASSIFIED INDIA",https://www.reddit.com/r/MachineLearning/comments/4mc5po/lamination_machine_kavinstar_price_in_delhi/,abdulghanysyed,1464947320,,0,1
91,2016-6-3,2016,6,3,18,4mc6qg,Awesome list - Deep learning papers (since 2010),https://www.reddit.com/r/MachineLearning/comments/4mc6qg/awesome_list_deep_learning_papers_since_2010/,terryum,1464947938,,8,117
92,2016-6-3,2016,6,3,20,4mcgxe,Machine Learning For Developers,https://www.reddit.com/r/MachineLearning/comments/4mcgxe/machine_learning_for_developers/,akshaysondur,1464954073,,0,3
93,2016-6-3,2016,6,3,20,4mch7b,Artificial intelligence to begin doing legal research for law firms,https://www.reddit.com/r/MachineLearning/comments/4mch7b/artificial_intelligence_to_begin_doing_legal/,AutoBizAlly,1464954239,,0,0
94,2016-6-3,2016,6,3,21,4mcj0f,Google's AI Chief Geoffrey Hinton - How Neural Networks Really Work,https://www.reddit.com/r/MachineLearning/comments/4mcj0f/googles_ai_chief_geoffrey_hinton_how_neural/,InaneMembrane,1464955214,,27,205
95,2016-6-3,2016,6,3,21,4mck0y,Convolution by Evolution: DeepMind publishes a paper about its framework to mix DL and evolutionary algorithms,https://www.reddit.com/r/MachineLearning/comments/4mck0y/convolution_by_evolution_deepmind_publishes_a/,Chobeat,1464955677,,4,47
96,2016-6-3,2016,6,3,22,4mcwtf,"This Week in Machine Learning, 3 June 2016",https://www.reddit.com/r/MachineLearning/comments/4mcwtf/this_week_in_machine_learning_3_june_2016/,DavidAJoyner,1464961293,,0,1
97,2016-6-3,2016,6,3,22,4mcy6t,"Never mess with LD_LIBRARY_PATH to run your CUDA app again (hey, Tensorflow!)",https://www.reddit.com/r/MachineLearning/comments/4mcy6t/never_mess_with_ld_library_path_to_run_your_cuda/,3150,1464961881,,6,9
98,2016-6-4,2016,6,4,2,4mduw7,Diving into Machine Learning through TensorFlow - PyCon 2016,https://www.reddit.com/r/MachineLearning/comments/4mduw7/diving_into_machine_learning_through_tensorflow/,thecity2,1464973300,,0,3
99,2016-6-4,2016,6,4,2,4me4sp,What happens after the final convolution?,https://www.reddit.com/r/MachineLearning/comments/4me4sp/what_happens_after_the_final_convolution/,klop2031,1464976588,"So for a network that has a constitutional layer, how does the output get converted to a dense layer and in turn gets turned into an output?

I understand we flatten the output of the final convolution layer.

So lets say we reduce our image from a 100X100 image to a 1x1 image from convolutions (and a final filter count of 512) (thus making a hyper-pixel).

What happens after we get an output of the final convolution layer? Is there a difference between the hyper-pixel and non hyper-pixel scenario?
",6,2
100,2016-6-4,2016,6,4,3,4meagv,Mechanics of Lagrangians,https://www.reddit.com/r/MachineLearning/comments/4meagv/mechanics_of_lagrangians/,urish,1464978522,,1,23
101,2016-6-4,2016,6,4,3,4mebvf,Why train with cross-entropy instead of KL divergence in classification?,https://www.reddit.com/r/MachineLearning/comments/4mebvf/why_train_with_crossentropy_instead_of_kl/,RobRomijnders,1464979009,"In neural networks for classification we use mostly cross-entropy. However, KL divergence seems more logical to me. KL divergence describes the divergence of one probability distribution to another, which is the case in neural networks. We have a true distribution p and a generated distribution q.

I do realize that KL divergence would result in the same gradients. Concretely: KL divergence(p||q) = cross entropy(p,q) - entropy(p).

Still, I am looking for intuition: why cross entropy instead of KL divergence",7,8
102,2016-6-4,2016,6,4,3,4meenp,Is the NIST Special Database 19 available somewhere?,https://www.reddit.com/r/MachineLearning/comments/4meenp/is_the_nist_special_database_19_available/,galapag0,1464980012,,0,2
103,2016-6-4,2016,6,4,5,4mexw3,"In linear regression, how do I extrapolate parameters obtained using preprocessed data?",https://www.reddit.com/r/MachineLearning/comments/4mexw3/in_linear_regression_how_do_i_extrapolate/,yogabbagabb,1464986545,"I originally posted this question onto Cross Validated SE. I would post it onto this forum, but my understanding is that latex can't be handled here.

http://stats.stackexchange.com/questions/216051/in-linear-regression-how-do-i-extrapolate-parameters-obtained-using-preprocesse",0,0
104,2016-6-4,2016,6,4,5,4mezxg,Is it true that in theory RNN can have unlimited memory?,https://www.reddit.com/r/MachineLearning/comments/4mezxg/is_it_true_that_in_theory_rnn_can_have_unlimited/,vernunftig,1464987249,"That is, if we can find an optimal parameter set, is it true that RNN can keep information from infinitely long history, then make accurate predictions based on its memory (summarized as the current hidden state)?",5,0
105,2016-6-4,2016,6,4,6,4mf2o2,Google releases,https://www.reddit.com/r/MachineLearning/comments/4mf2o2/google_releases/,maxToTheJ,1464988181,,1,0
106,2016-6-4,2016,6,4,7,4mfg1a,A web essay to provoke discussion of sparse distributed representation as the key to biological intelligence,https://www.reddit.com/r/MachineLearning/comments/4mfg1a/a_web_essay_to_provoke_discussion_of_sparse/,rodrinkus,1464992982,"I announce a new hyper-essay (at http://www.sparsey.com/Sparsey_Hyperessay.html) describing a cortex-based machine intelligence model, Sparsey, which I think will be of interest to readers for several reasons. 

First, Sparsey does storage, best-match retrieval, and belief update of spatial or spatiotemporal inputs (hypotheses) with a number of operations that remains fixed as the number of items stored in the database grows up to a soft limit, N, that depends on model size (essentially the number of weights, W).  This set of time performance properties has not been claimed or shown for any other machine intelligence model, including the world-leading Deep Learning (DL) models. This fixed time performance is not constant time complexity, but empirical tests have shown that N scales well in W.  This, in concert with the benefits of compositional organization of information/knowledge afforded by deep hierarchies suggests that a model of a fixed, reasonable W (e.g., several billion to several tens of billion weights) may be able to operate without saturating over its lifetime and explain the apparently huge capacity, speed (both during learning and inference), and flexibility of biological, in particular, human, cognition.

Second, Sparsey's fixed-time storage (learning), best-match retrieval, and belief update, capability depends crucially on the fact that information is represented using sparse distributed representations (SDRs) in each Sparsey module.  (N.b.: SDR is not the same concept as ""sparse coding"".)  It is therefore not surprising that DL models do not have this fixed time capability, since to my knowledge, SDR has not been used in any DL model.  On the contrary. the long learning times of DL models, even using massive machine parallelism (GPUs), is well known.  I suggest that the availability of cheap massive machine parallelism may in fact be ""garden-pathing"" researchers to continue pushing algorithms that may be fundamentally different than the processes underlying biological intelligence.  I think that the most important difference is the absence of SDR in DL models (because of the fixed time performance SDR confers).   But there are other obvious disconnects as well, e.g., the almost universal lack of mesoscale architecture and function in DL models, in contrast to the brain's cortex, for which there is substantial evidence of mesoscale structure and function, and the use of gradient-based learning, which has long been viewed as biologically implausible.

Third, it is noteworthy that the fixed time capabilities stated above have not been claimed for any of the few other SDR-based models out there, in particular, Numenta's HTM/Grok and Hecht-Nielsen's Cogent Confabulation.  This, despite the obvious importance of fixed time performance for scaling to ""big data""-sized problems. 

I hope readers view the essay and that its elaborations of the above points and many other related ideas engenders a lively debate.

Sincerely,
Rod Rinkus
President, Neurithmic Systems",4,0
107,2016-6-4,2016,6,4,7,4mfhzd,Scalable Fraud Detection,https://www.reddit.com/r/MachineLearning/comments/4mfhzd/scalable_fraud_detection/,Shishaddict,1464993706,,0,1
108,2016-6-4,2016,6,4,7,4mficy,Any freely available pretrained model of compressed VGGNet (or similar) ?,https://www.reddit.com/r/MachineLearning/comments/4mficy/any_freely_available_pretrained_model_of/,HrantKhachatrian,1464993871,"Recently many authors claim they are able to obtain neural net models for image classification that are as good as VGGNet (&lt;8% top-5 accuracy on ImageNet), but are much smaller. My quick search revealed these papers:

1. http://arxiv.org/abs/1509.06569 claims to get 7x less parameters. There is [some code](https://github.com/Bihaqo/TensorNet) but no pretrained models 
2. http://arxiv.org/abs/1510.00149 claims to use 49x less storage than VGG (although it doesn't mean that there are 49x less parameters, but as far as I understand at least 13x reduction in the number of parameters is there because of pruning). The earlier version of this work is [SqueezeNet](http://arxiv.org/abs/1602.07360) which achieves AlexNet-like accuracy using just ~500KB (!!!) model. This model is available on [Github](https://github.com/songhan/SqueezeNet-Deep-Compression)
3. http://arxiv.org/abs/1412.6115 is published in late 2014 by FAIR and report ~20x compression with 1% accuracy loss compared to VGGNet [Edit: this is actually not VGGNet, but [an older one](https://arxiv.org/abs/1311.2901v3) ]
4. http://arxiv.org/abs/1504.04788 uses some hashing methods, but the authors do not report performance on ImageNet
5. http://arxiv.org/abs/1312.4400 known as ""Network in network"", obtained 30MB model with AlexNet-like accuracy back in 2013 and it's available [for Caffe here](https://gist.github.com/mavenlin/d802a5849de39225bcc6)

So I couldn't find any small pretrained model with &lt;10% accuracy on ImageNet. Are there any? 
",6,2
109,2016-6-4,2016,6,4,9,4mfvh6,Zero-padding,https://www.reddit.com/r/MachineLearning/comments/4mfvh6/zeropadding/,klop2031,1464999275,"What is the benefit of zero-padding? I am quite unsure as to why we want to preserve the size of the grid? 

What about max pooling? we are still reducing the size of the grid, why would we want to use zero-padding when we are going to do maxpooling anyhow?",2,0
110,2016-6-4,2016,6,4,9,4mfwh7,Few questions about char-rnn and language modeling in general,https://www.reddit.com/r/MachineLearning/comments/4mfwh7/few_questions_about_charrnn_and_language_modeling/,Mlnoob69,1464999687,"1. During inference, what is the suggested way to generate outputs. Take argmax of the probability distribution or do a beam search, or something else? I see that Sutskever's demo (http://www.cs.toronto.edu/~ilya/fourth.cgi) is non-deterministic, it produces different result every time. How is that accomplished? I don't think either argmax or beam search would do that

2. When training a language model on a large piece of text, what's the best way to break it up into mini-batches? Do we mod-shard (so that nth examples of each mini-batch, when concatenated, form a continuous passage) the entire book and then preserve the state in-between the mini-batches? So that if the first example of the first mini-batch ended in state S, we will initialize the network's state with S when processing the first example of the second mini-batch?

3. When the above is trained using BPTT of size X, can the network learn dependencies of size greater than X? How does the value of X affect the ability of the network to learn longer dependencies? What does it affect anyway? I haven't really seen a thorough explanation on the importance of X.

4. We can train an RNN to generate text. Can we use it to compute the probability that a given piece of text is generated from the language model? So if we train an RNN on book titles, can we use it somehow to see how likely a particular piece of text is a book title?",8,11
111,2016-6-4,2016,6,4,11,4mgapr,Creating large datasets for Tensorflow?,https://www.reddit.com/r/MachineLearning/comments/4mgapr/creating_large_datasets_for_tensorflow/,timburg,1465006007,"If I want to create a large dataset (&gt;20 Gb) dataset as input to tensorflow, say a few million 100x100 images, what would be the best format for storage and reading the dataset?",1,0
112,2016-6-4,2016,6,4,11,4mgb10,Go to methods for Natural Language dataset exploration?,https://www.reddit.com/r/MachineLearning/comments/4mgb10/go_to_methods_for_natural_language_dataset/,Romesc,1465006154,"Hi all,

Just curious what the community tends to do when looking for patterns, statistical insights, etc. while exploring a dataset of natural language. For example, do people have boilerplate 'run this set of statistics and look for correlations between every feature' approaches? If so, I'm curious.

I am fairly new to data mining/analysis of datasets and although I understand many techniques, I think it would be helpful to see others' workflows/approaches during exploring a foreign dataset for the first time.

To give context, I am looking through a set of roughly 10,000 natural language instructions and trying to identify patterns/saliencies in how people tend to word their instructions depending on the stimuli.

If there are existing posts/tutorials you could point me to - even better. I prefer to work in python, but just general approaches would be extremely insightful.",2,1
113,2016-6-4,2016,6,4,11,4mgf3b,Is it an oversimplification to say deep learning is limited to machine perception or no?,https://www.reddit.com/r/MachineLearning/comments/4mgf3b/is_it_an_oversimplification_to_say_deep_learning/,equitycrowd,1465007972,It would be useful to make this distinction so I can think more accurately about the limits of the impacts of deep learning ... ,7,0
114,2016-6-4,2016,6,4,14,4mgwjn,The Difference Between Deep Learning and Regular Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4mgwjn/the_difference_between_deep_learning_and_regular/,[deleted],1465016479,[deleted],0,3
115,2016-6-4,2016,6,4,15,4mh8xt,How to learn the dynamics of motors?,https://www.reddit.com/r/MachineLearning/comments/4mh8xt/how_to_learn_the_dynamics_of_motors/,savs95,1465023430,"I have access to motors remotely, and I need to learn their behaviour. I get a huge data set, showing the RPM and the input voltage and the input given by the user. Since there is dynamics involved in the motors, and there are no breaks to slow down the motor, the motor stops through friction. How can I learn such a behaviour ? Ideally I have infinite data, and should be able to simulate the behaviour of motors with minimal error, but I have no clue on how to go about it. 

Any reference to papers or articles would be great, also some idea in general. I know only supervised learning well. ",7,0
116,2016-6-4,2016,6,4,16,4mhcfz,Reducing the Model Order of Deep Neural Networks Using Information Theory,https://www.reddit.com/r/MachineLearning/comments/4mhcfz/reducing_the_model_order_of_deep_neural_networks/,x2342,1465025749,,0,13
117,2016-6-4,2016,6,4,17,4mhi5v,What can the industrial planetary mixer do in chemical industrials?,https://www.reddit.com/r/MachineLearning/comments/4mhi5v/what_can_the_industrial_planetary_mixer_do_in/,mixmachinery,1465029705,,1,1
118,2016-6-4,2016,6,4,17,4mhifj,Data trusts could allay our privacy fears | Guardian,https://www.reddit.com/r/MachineLearning/comments/4mhifj/data_trusts_could_allay_our_privacy_fears_guardian/,sidsig,1465029903,,0,0
119,2016-6-4,2016,6,4,18,4mhmnu,Matrix Factorization on GPU,https://www.reddit.com/r/MachineLearning/comments/4mhmnu/matrix_factorization_on_gpu/,shash273,1465032755,,9,3
120,2016-6-4,2016,6,4,18,4mhnu4,Understand About Precision Machining,https://www.reddit.com/r/MachineLearning/comments/4mhnu4/understand_about_precision_machining/,arnchorlink,1465033485,[removed],0,1
121,2016-6-4,2016,6,4,19,4mhsoq,[X-Post from datascience] Experience with cloud-based ML platforms?,https://www.reddit.com/r/MachineLearning/comments/4mhsoq/xpost_from_datascience_experience_with_cloudbased/,jblondon1,1465036653,,0,0
122,2016-6-4,2016,6,4,19,4mhsth,"How many operation types can a single embedding handle, such that queries are in the form of simple math operations between vectors in that embedding, and solutions are in the form of nearest neighbors to the output vector?",https://www.reddit.com/r/MachineLearning/comments/4mhsth/how_many_operation_types_can_a_single_embedding/,[deleted],1465036742,[deleted],0,0
123,2016-6-4,2016,6,4,19,4mhtoy,Are we using the right way to train LSTM neural networks?,https://www.reddit.com/r/MachineLearning/comments/4mhtoy/are_we_using_the_right_way_to_train_lstm_neural/,kh40tika,1465037294,"1) In the original paper Schmidhuber &amp; Hochreiter 1997, they proposed a hybrid BPTT/RTRL approach for training LSTMs. The method was local in space and local in time. In other words, the computation cost does not scale with network size or time lag length needed to learn. However in recent papers, people seem to mostly use batched BPTT. I'm not an expert in RNNs, but I get the feeling if you cut time length as in BPTT, the net might fail at very long time lags. The original paper report their LSTM can generalize beyond 1000 time steps. Training 1000 time steps with BPTT would definitely consume a lot memory, while not guarantee 10000 step generalization.

2) I believe an important point of RNN is to model a very deep network in a single shared layer across time. However in practice people often use stacked LSTMs, which is not recurrent vertically. The problem is, in a single time step, a single layered RNN can only perform 1 layer of computation.
Here's my idea, instead of using multiple layers, let the net compute multiple times for each time step, and only generate output if a special ""action"" unit's activation is over certain threshold. This makes the net runs like a while loop.
Yes, this might make network more difficult to train, but a net performing this way is more human like. Imagine you giving a speech, do you speak words at constant rate? Most likely you would spend time doing a short think between sentences.

Let me know what you think.",20,37
124,2016-6-4,2016,6,4,20,4mhwab,What is the incentive of using more than one hidden layer in NN?,https://www.reddit.com/r/MachineLearning/comments/4mhwab/what_is_the_incentive_of_using_more_than_one/,RavlaAlvar,1465038947,"I remember I heard from somewhere that 3 layer NN is good enough to approximate any function to any arbitrary some amount of error.

Then what about deep learning? Is it just used with convoluted NN?

In what situation we will use more than one hidden layer?",4,0
125,2016-6-4,2016,6,4,22,4miemz,Questions about equations in the Normalizing Flows paper,https://www.reddit.com/r/MachineLearning/comments/4miemz/questions_about_equations_in_the_normalizing/,question99,1465048705,"http://arxiv.org/abs/1505.05770

1. In equation (6) the input to function f_k is z_k-1. This suggests me that in equation (7) when differentiating f_k in the summation the variable [should be z_k-1 rather than z_k](http://imgur.com/c0gkMOc), is that correct?
2. In equation (19) [I think there should be an ln(...) expression surrounding the right hand side](http://imgur.com/Np4QosZ), what do you think?
Here is why: After comparing the left hand side of eq (18) and eq (19), I thought that z'=f_K * f_K-1 * ... * f_1(z_0). Which means that g_1 * g_2 * ... * g_K(z') = z_0. Which means that (from eq (18)): ln(q_0(z_0)) = ln(q_0(g_1 * g_2 * ... * g_K(z')))",2,7
126,2016-6-4,2016,6,4,23,4migqr,Questions about Neural Programmer Interpreters,https://www.reddit.com/r/MachineLearning/comments/4migqr/questions_about_neural_programmer_interpreters/,nonap_,1465049612,"Hi,

I was reading the ICLR best paper awarded paper ""Neural Programmer Interpreters"" by Scott Reed and Nando D F. I have the following doubts:

1. The program embeddings work like hierarchical actors? That is, does the NPI automatically learn to decompose a complex task into a series of program calls with arguments and learn those programs as well? 

2. In continuation of above, Does it automatically determine number of programs?

Or, is the learnt programs 1 for each task, like addition, 3D car alignment?

",5,0
127,2016-6-4,2016,6,4,23,4mihfd,science and technology cabbage harvest machine modern agriculture documentary,https://www.reddit.com/r/MachineLearning/comments/4mihfd/science_and_technology_cabbage_harvest_machine/,besstreetfightvideos,1465049917,,0,0
128,2016-6-5,2016,6,5,0,4miqzm,On the Possibility of Re AMAs,https://www.reddit.com/r/MachineLearning/comments/4miqzm/on_the_possibility_of_re_amas/,[deleted],1465053871,[deleted],13,29
129,2016-6-5,2016,6,5,0,4mit81,Neural Art using extremely lightweight (&lt;500K) neural network,https://www.reddit.com/r/MachineLearning/comments/4mit81/neural_art_using_extremely_lightweight_500k/,pavelgonchar,1465054771,,14,87
130,2016-6-5,2016,6,5,0,4mive3,NIPS 2015 Workshop Videos,https://www.reddit.com/r/MachineLearning/comments/4mive3/nips_2015_workshop_videos/,siddharth-agrawal,1465055596,,5,46
131,2016-6-5,2016,6,5,1,4mizy3,Is there a list of downloadable Haar cascading classifiers?,https://www.reddit.com/r/MachineLearning/comments/4mizy3/is_there_a_list_of_downloadable_haar_cascading/,deep-learn,1465057398,[removed],0,1
132,2016-6-5,2016,6,5,1,4mj494,New to Tensorflow - trying to repurpose an MNIST multilayer network to a calculator,https://www.reddit.com/r/MachineLearning/comments/4mj494/new_to_tensorflow_trying_to_repurpose_an_mnist/,Deinos_Mousike,1465059062,"Could someone help or guide me through what I should do better in order for this to work? I posted this on /r/tensorflow earlier, but there are only 500 users there, so I figured it'd be better received here.

I changed the number of inputs to 2 and generated some random data, ""x1"" and ""x2"" (one number to be added to another). The idea is to use variables ""add"" and ""mul"" as the real output and base the cost (variable ""Y"") off of that, but I'm having trouble manipulating the data so it inputs properly.

I tried to make another variable with

x = tf.Variable([100 * np.random.random_sample([100]), 100 * np.random.random_sample([100]))

and a few other alternative ways, but that caused errors. Also, if there's anything else wrong in my code, please critique it! Anything helps.

Edit: made some small edits so the code makes more sense and is consistent in inputting two variables from the feed_dict into the optimizer and pred, to replace the two placeholders ""X1"" and ""X2"", however, it still doesn't work. 

Thank you.



    '''
    A Recurrent Neural Network implementation example using TensorFlow Library.
    
    Author: Deinos_Mousike
    '''
    
    import numpy as np
    import tensorflow as tf
    from tensorflow.models.rnn import rnn, rnn_cell
    # import matplotlib.pyplot as plt
    # from mpl_toolkits.mplot3d import Axes3D
    
    # Parameters
    training_iters = 1000
    n_epochs       = 1000
    batch_size     = 128
    display_step   = 100
    learning_rate  = 0.001
    
    n_observations = 100
    n_input        = 2   # Input data (Num + Num)
    n_steps        = 28  # timesteps
    n_hidden_1     = 256 # 1st layer number of features
    n_hidden_2     = 256 # 2nd layer number of features
    n_classes      = 1   # Output
    
    X  = tf.placeholder(""float"", [None, n_input])
    X1 = tf.placeholder(tf.float32)
    X2 = tf.placeholder(tf.float32)
    Y  = tf.placeholder(tf.float32)
    
    # Random input data
    x1 = 100 * np.random.random_sample([100,])
    x2 = 100 * np.random.random_sample([100,])
       
    add = tf.add(X1, X2)
    mul = tf.mul(X1, X2)
    
    weights = {
        'hidden1': tf.Variable(tf.random_normal([n_input,    n_hidden_1])),
        #'hidden2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),
        'out':     tf.Variable(tf.random_normal([n_hidden_1,  n_classes]))
    }
    
    biases = {
        'hidden1': tf.Variable(tf.random_normal([n_hidden_1])),
        #'hidden2': tf.Variable(tf.random_normal([n_hidden_2])),
        'out':     tf.Variable(tf.random_normal([n_classes]))
    }
    
    def RNN(_X1, _X2, _weights, _biases):
    
        # Layer 1.1
        layer_1 = tf.add(tf.matmul(_X1, weights['hidden1']), biases['hidden1'])
        layer_1 = tf.nn.relu(layer_1)
        # Layer 1.2
        # layer_1_2 = tf.add(tf.matmul(_X2, weights['hidden2']), biases['hidden2'])
        # layer_1_2 = tf.nn.relu(layer_1_2)
        # Hidden layer with RELU activation
        layer_2   = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])
    
        output    = tf.nn.relu(layer_2)
    
        return output
    
    pred         = RNN(X1, X2, weights, biases)
    cost         = tf.reduce_sum(tf.pow(pred - Y, 2)) / (n_observations - 1)
    optimizer    = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer
    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(Y,1))
    
    init     = tf.initialize_all_variables()
    # initData = tf.initialize_variables(x1.all(), x2.all())
    
    with tf.Session() as sess:
        # Here we tell tensorflow that we want to initialize all
        # the variables in the graph so we can use them
        sess.run(init)
    
        # Fit all training data
        prev_training_cost = 0.0
    
        for epoch_i in range(n_epochs) :
            for (_x1) in x1:
                for (_x2) in x2:
                    print(""Input 1:"")
                    print(_x1)
                    print(""Input 2:"")
                    print(_x2)
                    print(""Add function: "")
                    print(sess.run(add, feed_dict={X1: x1, X2: x2}))
                    y =   sess.run(add, feed_dict={X1: x1, X2: x2})
                    print(y)
                    sess.run(optimizer, feed_dict={X1: _x1, X2: _x2, Y: y})
                
    
            training_cost = sess.run(
                cost, feed_dict={X: xs, Y: ys})
            print(training_cost)
    
            if epoch_i % 20 == 0:
                ax.plot(X1, X2, pred.eval(
                    feed_dict={X1: _x1, X2: _x2, Y: y}, session=sess),
                        'k', alpha=epoch_i / n_epochs)
                fig.show()
                plt.draw()
    
            # Allow the training to quit if we've reached a minimum
            if np.abs(prev_training_cost - training_cost) &lt; 0.000001:
                break
            prev_training_cost = training_cost
    ",4,0
133,2016-6-5,2016,6,5,3,4mjfo7,Where does the Sigmoid in Logistic Regression come from?,https://www.reddit.com/r/MachineLearning/comments/4mjfo7/where_does_the_sigmoid_in_logistic_regression/,alexeyr,1465063331,,6,12
134,2016-6-5,2016,6,5,5,4mjyzw,Noob question: why should we normalize test data with mean and std from training data?,https://www.reddit.com/r/MachineLearning/comments/4mjyzw/noob_question_why_should_we_normalize_test_data/,anhtt_,1465070703,Thanks everyone in advance!,10,5
135,2016-6-5,2016,6,5,6,4mk87k,Is it possible to use Logical Regression in a text-based dataset that must predict one of two values?,https://www.reddit.com/r/MachineLearning/comments/4mk87k/is_it_possible_to_use_logical_regression_in_a/,KrustyKrab111,1465074209,*Logistic,7,0
136,2016-6-5,2016,6,5,7,4mklaw,NVIDIA Jetson TX1 for Machine Learners - Promotion,https://www.reddit.com/r/MachineLearning/comments/4mklaw/nvidia_jetson_tx1_for_machine_learners_promotion/,[deleted],1465079429,[deleted],0,0
137,2016-6-5,2016,6,5,8,4mkv2a,Question re optimization and classification,https://www.reddit.com/r/MachineLearning/comments/4mkv2a/question_re_optimization_and_classification/,Powlerbare,1465083386,"Say we have a binary classification task where the data is not linearly separable. 

For the sake of example, say I have a network that takes the form:
input -&gt; 2dim hidden layer (sigmoid) -&gt; 1 dim output (no activation)

If I plot the 2 dim hidden layer representation of each classes points during training, I can see that the network is attempting to find a transformation of the data that is linearly separable so that the last affine transformation can classify the data correctly.

Basically, it seems to me that the representation of the data at the last layer for each class should be somewhat separable in order for a network to have a shot at good performance.

My question is:
Can someone help me with some search terms, or papers that have essentially chopped the final layer off of a network, and learned a network that maximizes something like the euclidean distance between each classes points in some latent space? I have tried this but often ended up with solutions that were not useful at all.

This feels like a supervised auto encoder to me, but I do not care about re constructing input at all.
",3,6
138,2016-6-5,2016,6,5,9,4ml1ue,"How long should I wait for the loss to decline before changing hyperparams, etc.?",https://www.reddit.com/r/MachineLearning/comments/4ml1ue/how_long_should_i_wait_for_the_loss_to_decline/,jalligator,1465086097,"I'm new to machine learning.  I'm working on a project to classify gif/non-gif video frames.  In other words I've got a bunch of images, some from gifs, some not from gifs, and I'm trying to classify gif or non-gif.  I've experimented with a bunch of parameters and kernel sizes at this point.  My loss does not seem to decline and the accuracy hovers around 50% (indicating little training/learning).  

My question is how long should I let this train for before giving up and trying something different?  How do I know this thing is even trainable - that there is some learnable pattern between gifs and non-gifs?

I have now experimented with dozens of variations of initial learning rates, kernel sizes, batch sizes, etc.

It is my understanding that the loss should sort of stabilize fairly soon after beginning training and slowly but steadily decline if I'm on the right track.  Is that correct?  ",5,0
139,2016-6-5,2016,6,5,10,4mlajt,A CNN Architecture Puzzle,https://www.reddit.com/r/MachineLearning/comments/4mlajt/a_cnn_architecture_puzzle/,tornado28,1465089794,"Suppose I have 4 classes of images

* Images of cats and dogs
* Images of cats and mice
* Images of cats and birds
* Images with none of the above

I want to build a CNN that can predict if an image contains a cat or not without it being confused by the presence of dogs/mice/birds. I'm not sure how to go about this. Any ideas?",3,0
140,2016-6-5,2016,6,5,11,4mlklu,Are there any benchmark datasets for spatio-temporal predictions/ data analysis?,https://www.reddit.com/r/MachineLearning/comments/4mlklu/are_there_any_benchmark_datasets_for/,buy_some_wow,1465094566,,6,4
141,2016-6-5,2016,6,5,16,4mmfn4,CS's algorithms VS Machine Learning's algorithms,https://www.reddit.com/r/MachineLearning/comments/4mmfn4/css_algorithms_vs_machine_learnings_algorithms/,[deleted],1465112174,[deleted],0,1
142,2016-6-5,2016,6,5,17,4mmi57,CS algorithms VS Machine Learning algorithms,https://www.reddit.com/r/MachineLearning/comments/4mmi57/cs_algorithms_vs_machine_learning_algorithms/,whoisthriller,1465114053,"From my understanding, Machine Learning's algorithms such as SVM, k-means,Random forest are more mathematically and statistically based algorithms, while CS's algorithms such as Binary Search Trees, Red-Black trees, Hash table are traditional CS algorithms which is a total different thing to ML's algorithms. Also, CS programming is object orientied programming using java, c++, while ML uses script languages such as R, SAS, Python. So why many people say it is better to get a CS degree rather than a math.stat degree to get into ML field?",3,0
143,2016-6-5,2016,6,5,17,4mmjdn,Anybody interested/completed homework of UBC CS340 by Nando De Freitas 2012,https://www.reddit.com/r/MachineLearning/comments/4mmjdn/anybody_interestedcompleted_homework_of_ubc_cs340/,chaser999,1465114993,"Hi, I'm doing the course and it would be very motivating if I could check the correctness of my solutions for the homework given in the course http://www.cs.ubc.ca/~nando/340-2012/assignments.php.
So, if anybody has completed it and has solutions then please share or if somebody like to do them then may be we could discuss and match our solutions. Thanks",1,0
144,2016-6-5,2016,6,5,19,4mmtl4,what is the best list of applications of deep learning or machine learning in general,https://www.reddit.com/r/MachineLearning/comments/4mmtl4/what_is_the_best_list_of_applications_of_deep/,equitycrowd,1465122269,"there are several lists I have found, and probably many I have not ... ",9,0
145,2016-6-5,2016,6,5,20,4mmylj,random forest without explict tree data structure,https://www.reddit.com/r/MachineLearning/comments/4mmylj/random_forest_without_explict_tree_data_structure/,godspeed_china,1465125476,"Today I got the idea that if we only care about prediction result in random forest, we can construct the decision tree without explict tree data structure.  
What I did is just to send a recursive function two vectors of sample ids: one for training and one for testing. The function will do binary split and send two splited vector to itself recursively. At the terminal node, the function just assign mean value to each testing sample.  
I successfully implemented such simplified decision tree with only 46 line of c++ code! It is thus elegant and fast without overhead due to tree data structure.
Hope this information is useful for some of you.",3,0
146,2016-6-5,2016,6,5,21,4mn4qg,Could artificial intelligence achieve consciousness? A philosophical analysis.,https://www.reddit.com/r/MachineLearning/comments/4mn4qg/could_artificial_intelligence_achieve/,[deleted],1465128926,[deleted],3,0
147,2016-6-5,2016,6,5,21,4mn59h,what is gausian kernel?,https://www.reddit.com/r/MachineLearning/comments/4mn59h/what_is_gausian_kernel/,John_Smith111,1465129216,"hello all

could you share your perception about the gausaian kernel - what is it and how it works ?",4,0
148,2016-6-5,2016,6,5,22,4mndry,Sigmoid for Classifiers Decoded,https://www.reddit.com/r/MachineLearning/comments/4mndry/sigmoid_for_classifiers_decoded/,drcrook,1465133270,,0,1
149,2016-6-5,2016,6,5,23,4mniea,Cool Machine Learning Study Group Recap in South Florida,https://www.reddit.com/r/MachineLearning/comments/4mniea/cool_machine_learning_study_group_recap_in_south/,drcrook,1465135426,,0,1
150,2016-6-5,2016,6,5,23,4mnlpd,Critique my project?,https://www.reddit.com/r/MachineLearning/comments/4mnlpd/critique_my_project/,KrustyKrab111,1465136816,"Hey guys, newb starting out here. I did an analysis on the Kobe Bryant shot selection dataset. Id really appreciate if you guys could gimme some pointers on where i should improve or what i should do differently

https://github.com/TurbulentCupcake/KobeShotSelection/blob/master/KOBE!!!.ipynb",5,0
151,2016-6-6,2016,6,6,0,4mnrsx,A Gentle Introduction to the Basics of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4mnrsx/a_gentle_introduction_to_the_basics_of_machine/,hoaphumanoid,1465139303,,45,142
152,2016-6-6,2016,6,6,1,4mo1pc,Code for synthesizing images via deep generator networks,https://www.reddit.com/r/MachineLearning/comments/4mo1pc/code_for_synthesizing_images_via_deep_generator/,anh_ng,1465143028,"If you want to visualize your own Deep Neural Networks, to understand them or just create art, we have made the code and trained models from [1] available. 

https://github.com/Evolving-AI-Lab/synthesizing

Here are examples we produced when visualizing an AlexNet Variant: http://www.evolvingai.org/synthesizing. 
Please let us know what you create by emailing us or tweeting them with #DeepGeneratorNetworks


[1] Nguyen A, Dosovitskiy A, Yosinski J, Brox T, Clune J (2016) Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. arXiv:1605.09304. (pdf)",5,4
153,2016-6-6,2016,6,6,1,4mo2m1,what are the most difficult challenges for automatically generating reading comprehension questions for arbitrary text?,https://www.reddit.com/r/MachineLearning/comments/4mo2m1/what_are_the_most_difficult_challenges_for/,equitycrowd,1465143350,I know its essentially impossible right now ... is it going to be impossible in five years though?,2,0
154,2016-6-6,2016,6,6,1,4mo7kv,ICML quickly running out of space,https://www.reddit.com/r/MachineLearning/comments/4mo7kv/icml_quickly_running_out_of_space/,circuithunter,1465145188,,8,22
155,2016-6-6,2016,6,6,2,4moexc,Any tricks on how to correctly use new data to fine-tuning a pre-trained network?,https://www.reddit.com/r/MachineLearning/comments/4moexc/any_tricks_on_how_to_correctly_use_new_data_to/,emarvin,1465147832,"Say I have a pre-trained CNN alike network for classification. But I have collected more data for some of the labels lately. How do I use these new data? Do I put them into the big pool of the original incredibly large dataset, or do I train the network on this small new dataset for a few iterations(feels wrong)? I think data distribution should be taken care of, but I'm not sure what the best approach is. Please advice! Thank you.",5,3
156,2016-6-6,2016,6,6,3,4moo56,Improving Nelder-Mead Optimization by Genetic Algorithms and Simulated Annealing concepts,https://www.reddit.com/r/MachineLearning/comments/4moo56/improving_neldermead_optimization_by_genetic/,Bohemian90,1465151151,"Hello

I'm using the Nelder-Mead simplex algorithm for hyperparameter optimization. It works quiet well but now I would like to develop it further. I have also tried genetic algorithms and simulated annealing and I would like to incorporate techniques from these algorithms into Nelder-Mead. My goal is to overcome the problem of local minima and/or to get faster convergence.

Does somebody have some idea how to incorporate ideas from genetic algorithms or simulated annealing into Nelder-Mead? Perhaps the simplex gradient could also be used in some way. I'm grateful for every idea.",3,0
157,2016-6-6,2016,6,6,3,4moqfm,How does one go from a convolution layer to dense layers?,https://www.reddit.com/r/MachineLearning/comments/4moqfm/how_does_one_go_from_a_convolution_layer_to_dense/,klop2031,1465151954,"I have a flaky understanding of how to get from convolution to dense layers.

Does this explanation of going from convolution to dense layer make sense?

In order to get from a convolutional layer the the output we need to go to a dense layer. To do this we need to flatten the output of the final convolutional layer. Essentially it will become a vector with the representation of how well the image activated the filter. From here we can pass this to a dense layer (which may be the output layer) to give us the final one-hot classification.

If you have any references please link them.",1,0
158,2016-6-6,2016,6,6,3,4motq6,Beginner learning path to Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4motq6/beginner_learning_path_to_machine_learning/,hyperqube12,1465153093,"I want to get started into machine learning, however I am kind of overwhelmed  by the sheer amount of resources out there. I also looked a bit at [reddit posts](https://www.reddit.com/r/MachineLearning/comments/1jeawf/machine_learning_books/)
with resources (many resources...). So what should be the path for a newcomer to machine learning? Also, what would be a good and thorough textbook to start with? ",10,2
159,2016-6-6,2016,6,6,4,4mp3g2,How to generate sample sequences from a RNN regression output?,https://www.reddit.com/r/MachineLearning/comments/4mp3g2/how_to_generate_sample_sequences_from_a_rnn/,[deleted],1465156539,[deleted],1,0
160,2016-6-6,2016,6,6,5,4mp59h,Rescale Logarithmically Scaled Inputs? How About Skew Towards Zero?,https://www.reddit.com/r/MachineLearning/comments/4mp59h/rescale_logarithmically_scaled_inputs_how_about/,[deleted],1465157212,[deleted],0,0
161,2016-6-6,2016,6,6,5,4mpcc9,Built a model - how do I increase the accuracy?,https://www.reddit.com/r/MachineLearning/comments/4mpcc9/built_a_model_how_do_i_increase_the_accuracy/,mikebmassey,1465159744,"Using scikit learn, I was able to build a text classifier that was 97% accurate.  

What's the best way to get the accuracy higher?  I believe that I could run the process through another model, but am unsure the process to do that.  Any concept of that I should be looking for?  ",4,0
162,2016-6-6,2016,6,6,5,4mpd62,Tensor Builder - A light wrapper over TensorFlow that enables you to easily create complex deep neural networks using the Builder Pattern through a functional fluent immutable API.,https://www.reddit.com/r/MachineLearning/comments/4mpd62/tensor_builder_a_light_wrapper_over_tensorflow/,cgarciae,1465160040,,4,9
163,2016-6-6,2016,6,6,6,4mpjzr,Linearly Independent Sets in Vector Spaces induced by Kernels,https://www.reddit.com/r/MachineLearning/comments/4mpjzr/linearly_independent_sets_in_vector_spaces/,Wood717,1465162557,"Hello! I hope this post is okay (if not let me know). I'm attaching a [pdf](http://imgur.com/gallery/KevEFYY/new) which rigorously defines my question. 

Briefly, what I'm wondering is this - for the set of data points {x1,...,xp} in a vector space, (say, R^n) under what conditions is the set {k(x1, ),...,k(xp, )} (where k( , ) is a kernel function) independent? What conditions must the set {x1,...,xp} and the kernel function have to ensure independence?

If there isn't an immediate answer to this question I'll happily take recommendations for mathematical reading towards trying to answer this question. Thanks!!!",1,0
164,2016-6-6,2016,6,6,7,4mprvw,Numenta Anomaly Benchmark (NAB) Competition,https://www.reddit.com/r/MachineLearning/comments/4mprvw/numenta_anomaly_benchmark_nab_competition/,boltzBrain,1465165537,"Hello all,

The Numenta Anomaly Benchmark (NAB) is an open-source dataset and scoring methodology designed for evaluating anomaly detection algorithms for real-world streaming analytics. Were hosting a **[NAB competition](http://numenta.org/nab/)** in conjunction with the IEEE World Congress on Computational Intelligence. Were offering cash prizes for two categories:

- Algorithms for detecting anomalies in streaming data
- Real-world data sets labeled for anomalies

Entries are due by July 1st. Send submissions and questions/comments to [nab@numenta.org](nab@numenta.org)

We designed NAB to be useful for the research community; everything is open-source and its easy to run your anomaly detection algorithms on NAB (in any language). [Check out the repo](github.com/numenta/NAB). And you can learn more about NAB and the competition in [this short video](youtube.com/watch?v=b0GS_Fbsvzw)

Cheers,
Alex",10,7
165,2016-6-6,2016,6,6,8,4mpxgq,Build a Chatbot using a Generative Model,https://www.reddit.com/r/MachineLearning/comments/4mpxgq/build_a_chatbot_using_a_generative_model/,llSourcell,1465167763,,7,0
166,2016-6-6,2016,6,6,9,4mq66w,Linear Regression Question,https://www.reddit.com/r/MachineLearning/comments/4mq66w/linear_regression_question/,MLG-Blackgarden,1465171416,"Hey folks, I was wondering about some of the properties of a linear regression where J(0,1)=0.

*is h(x)=0? 
*y(i) should be equal to x(i), or 0? 
*gradient descent is likely to get stuck at a local minimum and fail to find the global minimum?
*Any other particularities?",2,0
167,2016-6-6,2016,6,6,9,4mq6kt,Artificial Intelligence and Intellectual Property: Who Owns the Rights and Royalties?,https://www.reddit.com/r/MachineLearning/comments/4mq6kt/artificial_intelligence_and_intellectual_property/,Aicial,1465171558,,1,1
168,2016-6-6,2016,6,6,9,4mq8fy,How do I plot Precision-Recall for images with binary ground truth and output map with the range [0 1],https://www.reddit.com/r/MachineLearning/comments/4mq8fy/how_do_i_plot_precisionrecall_for_images_with/,[deleted],1465172318,[deleted],0,0
169,2016-6-6,2016,6,6,9,4mqay8,What's next? Civil rights for A.I.'s?,https://www.reddit.com/r/MachineLearning/comments/4mqay8/whats_next_civil_rights_for_ais/,danielwalton,1465173347,,2,0
170,2016-6-6,2016,6,6,10,4mqeyx,Murphy's book or Koller's book for Probabilistic Graphical models?,https://www.reddit.com/r/MachineLearning/comments/4mqeyx/murphys_book_or_kollers_book_for_probabilistic/,Mr__Christian_Grey,1465175020,,3,0
171,2016-6-6,2016,6,6,12,4mr3cl,Nice tutorial on Bayesian Nonparametrics,https://www.reddit.com/r/MachineLearning/comments/4mr3cl/nice_tutorial_on_bayesian_nonparametrics/,cynml,1465184876,,4,31
172,2016-6-6,2016,6,6,13,4mrc17,Some trends of deep learning researches,https://www.reddit.com/r/MachineLearning/comments/4mrc17/some_trends_of_deep_learning_researches/,terryum,1465188881,,6,0
173,2016-6-6,2016,6,6,15,4mrkpc,Elon Is Wrong About Simulations,https://www.reddit.com/r/MachineLearning/comments/4mrkpc/elon_is_wrong_about_simulations/,[deleted],1465193376,[deleted],1,1
174,2016-6-6,2016,6,6,15,4mrny2,Need help for choosing domain for Thesis work,https://www.reddit.com/r/MachineLearning/comments/4mrny2/need_help_for_choosing_domain_for_thesis_work/,atinesh229,1465195248,"Hello friends, I'm a master student I want to select the domain for my thesis work. As Machine learning is so vast I'm very confused about what to select for my thesis work. 

I've recently completed Coursera's Machine learning by Andrew Sir, It gave me a broad idea about how ML Algo's are applied in practice. I've also read lots of papers, But I'm still confused. 

I'm not planning for Ph.D., I intend to join industry after my master so I'm looking for application specific work rather than theoretical work.
Here is the list of topics that I found interesting, But before that, I want suggestion from the researcher who are experts in machine learning.

MapReduce
Dimensionality Reduction
Recommendation
Clustering
Algorithm Selection Problem",6,0
175,2016-6-6,2016,6,6,16,4mru3x,Young Chinese Player to Challenge AlphaGo within This Year,https://www.reddit.com/r/MachineLearning/comments/4mru3x/young_chinese_player_to_challenge_alphago_within/,galapag0,1465198899,,25,52
176,2016-6-6,2016,6,6,17,4mryik,What's the best ML algorithm for the task?,https://www.reddit.com/r/MachineLearning/comments/4mryik/whats_the_best_ml_algorithm_for_the_task/,mrpennis,1465201519,"My application needs to find out for various (differing) websites where the description is for a specific type of content. So it needs to process the html of each website, and then figure out where that specific type of content is. What kind of ML algorithm best fits this job?",6,0
177,2016-6-6,2016,6,6,19,4msbnm,The Barbell Effect of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4msbnm/the_barbell_effect_of_machine_learning/,[deleted],1465209637,[deleted],1,0
178,2016-6-6,2016,6,6,19,4msdb4,Build predictive model to find whether the 80% of different levels of a factor exceed a specific temperature.,https://www.reddit.com/r/MachineLearning/comments/4msdb4/build_predictive_model_to_find_whether_the_80_of/,Jane_kon,1465210572,[removed],0,1
179,2016-6-6,2016,6,6,22,4mswxk,[1605.09522] Kernel Mean Embedding of Distributions: A Review and Beyonds,https://www.reddit.com/r/MachineLearning/comments/4mswxk/160509522_kernel_mean_embedding_of_distributions/,ChocoMoi,1465219774,,0,5
180,2016-6-6,2016,6,6,22,4msx0a,logistic - Why the output of a sigmoid neuron is a linear function of weights and bias?,https://www.reddit.com/r/MachineLearning/comments/4msx0a/logistic_why_the_output_of_a_sigmoid_neuron_is_a/,chaser999,1465219807,,4,0
181,2016-6-6,2016,6,6,22,4msx3e,How to train a UBM for speaker recognition?,https://www.reddit.com/r/MachineLearning/comments/4msx3e/how_to_train_a_ubm_for_speaker_recognition/,fiala__,1465219838,"Hi all,
I'm looking at the possibility of building a real-time speaker recognition system for essentially only distinguishing a single user speaking from everybody else.

So far it seems that a GMM or DNN with a universal background model should do the trick. The thing is, I need to build a lightweight solution in Javascript, for which I (obviously) haven't found any pre-trained models.

So, how does one go about training a UBM? Do I just get a massive dataset such as TIMIT or NIST, stick ""negative"" labels on everything and train my model? I don't see how that would make sense 

Thanks a lot!",3,0
182,2016-6-6,2016,6,6,22,4msygi,f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization,https://www.reddit.com/r/MachineLearning/comments/4msygi/fgan_training_generative_neural_samplers_using/,schmook,1465220388,,6,12
183,2016-6-6,2016,6,6,22,4mszhi,"Russian developer collaborates with FB, Google to help machines see",https://www.reddit.com/r/MachineLearning/comments/4mszhi/russian_developer_collaborates_with_fb_google_to/,edmondsondaniele,1465220771,,0,0
184,2016-6-6,2016,6,6,22,4mt0qf,The barbell effect of machine learning,https://www.reddit.com/r/MachineLearning/comments/4mt0qf/the_barbell_effect_of_machine_learning/,hanleycarlos,1465221248,,0,0
185,2016-6-6,2016,6,6,23,4mt1r4,Simple question regard Gradient Descent math and python code.,https://www.reddit.com/r/MachineLearning/comments/4mt1r4/simple_question_regard_gradient_descent_math_and/,[deleted],1465221614,[deleted],0,0
186,2016-6-6,2016,6,6,23,4mt37f,Why machine learning models require a failover plan,https://www.reddit.com/r/MachineLearning/comments/4mt37f/why_machine_learning_models_require_a_failover/,levinmcarthur,1465222147,,0,0
187,2016-6-6,2016,6,6,23,4mt6pa,This Week in Machine Learning &amp; AI Podcast,https://www.reddit.com/r/MachineLearning/comments/4mt6pa/this_week_in_machine_learning_ai_podcast/,sbc1906,1465223306,"Hi Everyone. I've been working on a new podcast called This Week in Machine Learning &amp; AI. It can be found here:

https://soundcloud.com/twiml

https://itunes.apple.com/us/podcast/this-week-in-machine-learning/id1116303051?mt=2

My target is what I think of as a ""technical enthusiast"" listener, as opposed to the Talking Machines podcast which is a bit academic in nature, or the Learning Machines 101 podcast which is more instructional. Note that this project is unrelated to the similarly named blog posts curated by /u/DavidAJoyner.

Let me know what you think. I'm hoping to start doing interviews soon, so if you've got suggestions for who you'd like to hear from, please share.

I'm still dialing in the recording setup and process, so the audio quality isn't quite where I want it to be, but I'm working on it!
",8,5
188,2016-6-7,2016,6,7,0,4mth5v,Building a Distributed Machine Learning Testbench with resin.io,https://www.reddit.com/r/MachineLearning/comments/4mth5v/building_a_distributed_machine_learning_testbench/,chotn,1465226663,,0,12
189,2016-6-7,2016,6,7,0,4mtiz0,"Ever heard of fitbit for cows? or a tool that can identify road features on an open source map? If not, read up more news here",https://www.reddit.com/r/MachineLearning/comments/4mtiz0/ever_heard_of_fitbit_for_cows_or_a_tool_that_can/,MachineLearner85,1465227239,,0,1
190,2016-6-7,2016,6,7,2,4mu6a4,[1605.08912] A Riemannian Framework for Statistical Analysis of Topological Persistence Diagrams,https://www.reddit.com/r/MachineLearning/comments/4mu6a4/160508912_a_riemannian_framework_for_statistical/,quagmire_giggity,1465234549,,9,4
191,2016-6-7,2016,6,7,3,4mucw6,"Government ,Public and private sector cybersecurity leaders are rapidly shifting their attention to technologies like analytics and artificial intelligence which increase their ability to predict, and ultimately prevent, cyber incidents",https://www.reddit.com/r/MachineLearning/comments/4mucw6/government_public_and_private_sector/,DeandreyWarren5_,1465236650,,1,1
192,2016-6-7,2016,6,7,3,4mufie,Tensorflow 0.9,https://www.reddit.com/r/MachineLearning/comments/4mufie/tensorflow_09/,johnny____,1465237478,,40,147
193,2016-6-7,2016,6,7,3,4muihc,What is the state of the art in natural language understanding? is Arxiv or gitxiv the best way to stay up to date on the state of the art?,https://www.reddit.com/r/MachineLearning/comments/4muihc/what_is_the_state_of_the_art_in_natural_language/,equitycrowd,1465238401,,29,33
194,2016-6-7,2016,6,7,3,4muke5,A machine learning-driven cover band,https://www.reddit.com/r/MachineLearning/comments/4muke5/a_machine_learningdriven_cover_band/,functime,1465238979,,0,2
195,2016-6-7,2016,6,7,4,4mupg6,"[For Beginners, by beginners]: Things that are worth knowing, before attenting Andrew Ng's Machine Learning course",https://www.reddit.com/r/MachineLearning/comments/4mupg6/for_beginners_by_beginners_things_that_are_worth/,[deleted],1465240572,[deleted],0,1
196,2016-6-7,2016,6,7,4,4murrk,[For Beginners] Things worth knowing before attending Andrew Ng's Machine Learning course,https://www.reddit.com/r/MachineLearning/comments/4murrk/for_beginners_things_worth_knowing_before/,Monninho,1465241297,,2,0
197,2016-6-7,2016,6,7,4,4muwqs,Machine learning tutorial blog launching soon,https://www.reddit.com/r/MachineLearning/comments/4muwqs/machine_learning_tutorial_blog_launching_soon/,__salas__,1465242891,,0,1
198,2016-6-7,2016,6,7,5,4mv4x9,Lectures from MLSS Cadiz,https://www.reddit.com/r/MachineLearning/comments/4mv4x9/lectures_from_mlss_cadiz/,[deleted],1465245482,[deleted],0,1
199,2016-6-7,2016,6,7,5,4mv551,"Lectures from the Machine Learning Summer School (MLSS) 2016, Cadiz",https://www.reddit.com/r/MachineLearning/comments/4mv551/lectures_from_the_machine_learning_summer_school/,[deleted],1465245553,[deleted],0,1
200,2016-6-7,2016,6,7,5,4mv63u,Former NASA Exec Brings Stealth Machine Learning Chip to Light,https://www.reddit.com/r/MachineLearning/comments/4mv63u/former_nasa_exec_brings_stealth_machine_learning/,[deleted],1465245881,[deleted],0,0
201,2016-6-7,2016,6,7,6,4mvdgk,What makes machine learning different than classical statistical approaches? In what instances is it better to use the former over the latter (and vice versa)?,https://www.reddit.com/r/MachineLearning/comments/4mvdgk/what_makes_machine_learning_different_than/,[deleted],1465248227,[deleted],1,1
202,2016-6-7,2016,6,7,6,4mvfp5,"[Resource Request] MSc in Theoretical Physics, looking to get into ML",https://www.reddit.com/r/MachineLearning/comments/4mvfp5/resource_request_msc_in_theoretical_physics/,Fenzik,1465249000,"As stated in the title, I have (or rather, I will have, in about a week) a masters in theoretical physics (string theory). However, I've gotten quite interested in ML recently, and the physics job market is pretty bleak, so I'm looking to explore the field a bit. I don't really have any ML experience (I know a few words), but I don't foresee myself being phased by any of the math that would be involved (calculus, lin alg, probability), and I have a decent amount of Python experience. 

Is there a particular subfield where my skillset would make it easy to carve out a niche? I've heard deep learning is generally quite math-intensive, for example. Are there obvious directions to go career-wise? I wouldn't mind getting into financial applications but I'm open to ideas. 

Thank in advance!",9,1
203,2016-6-7,2016,6,7,7,4mvokn,How does CMU's 10-708 Probabilistic graphical models compares with Stanford's CS228 Probabilistic graphical models? What are the differences? Which is more advanced and comprehensive? (Links in the description to course websites),https://www.reddit.com/r/MachineLearning/comments/4mvokn/how_does_cmus_10708_probabilistic_graphical/,Mr__Christian_Grey,1465251925,"CMU - http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html
Stanford - http://cs.stanford.edu/~ermon/cs228/index.html

",1,5
204,2016-6-7,2016,6,7,7,4mvq0r,Is it environmentally reasonable to set up my desktop as a server for hobbyist level machine learning?,https://www.reddit.com/r/MachineLearning/comments/4mvq0r/is_it_environmentally_reasonable_to_set_up_my/,rodyamirov,1465252409,[removed],0,1
205,2016-6-7,2016,6,7,7,4mvtad,"I fed 20,000 images of #nailart into a neural network (Xpost /r/redditlaqueristas)",https://www.reddit.com/r/MachineLearning/comments/4mvtad/i_fed_20000_images_of_nailart_into_a_neural/,[deleted],1465253529,[deleted],1,1
206,2016-6-7,2016,6,7,9,4mw927,Can word2vec be used for search?,https://www.reddit.com/r/MachineLearning/comments/4mw927/can_word2vec_be_used_for_search/,guszz,1465259212,"Let's say I have a bunch of entries in a database that I want to search by keyword. Would it be realistic to use a vector representation search technique, where I compute a vector for each entry then compare those by cosine distance to a vector from some entered keywords?

Are there any examples of people doing this? Perhaps it is slow or doesn't yield as good results as other techniques.",7,1
207,2016-6-7,2016,6,7,11,4mwxvf,papers about answering questions by short sentence?,https://www.reddit.com/r/MachineLearning/comments/4mwxvf/papers_about_answering_questions_by_short_sentence/,chyojn,1465268113,"there are a lot of research about answering questions via answer selection, but are there any papers talk about answering questions by generating short sentence? thanks.",3,0
208,2016-6-7,2016,6,7,12,4mx3l4,Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations,https://www.reddit.com/r/MachineLearning/comments/4mx3l4/zoneout_regularizing_rnns_by_randomly_preserving/,dare_dick,1465270416,,9,4
209,2016-6-7,2016,6,7,12,4mx725,""" Unifying Count-Based Exploration and Intrinsic Motivation"", Bellemare 2016: 2-&gt;15 rooms cleared on ""Montezuma's Revenge""",https://www.reddit.com/r/MachineLearning/comments/4mx725/unifying_countbased_exploration_and_intrinsic/,[deleted],1465271860,[deleted],0,1
210,2016-6-7,2016,6,7,12,4mx78q,"""Unifying Count-Based Exploration and Intrinsic Motivation"", Bellemare 2016: 2-&gt;15 rooms cleared on ""Montezuma's Revenge""",https://www.reddit.com/r/MachineLearning/comments/4mx78q/unifying_countbased_exploration_and_intrinsic/,gwern,1465271932,,9,40
211,2016-6-7,2016,6,7,13,4mx9l5,Adversarially Learned Inference,https://www.reddit.com/r/MachineLearning/comments/4mx9l5/adversarially_learned_inference/,terryum,1465272979,,4,8
212,2016-6-7,2016,6,7,15,4mxq8i,Reinforcement learning - relevant state changes vs all state changes?,https://www.reddit.com/r/MachineLearning/comments/4mxq8i/reinforcement_learning_relevant_state_changes_vs/,knite,1465281265,"I'm new to RL, so I'd love to know if there's standard terminology for the idea below.

Basically, I'd like to use RL for a kind of control problem. Let's say I'm training a net to drive a toy robot - it has different sensors (gyroscope, GPS, heat sensor, etc) that update at different rates. When training, it looks like the standard approach is to encode state into ""frames"" (Atari game, Go position) with readings from each ""sensor"" at that time.

My question is, why not fast forward to the next relevant state? What if we skip states where all sensors are ""the same"", within a per-sensor threshold - temp is within 1 degree, nearest object is still in the 50-55 cm range, etc. Instead of updating with the boring states that are expected to maintain the status quo (""continue forward at current speed""), skip these no-op states and update to the next ""interesting"" state.

Is this a good idea? Bad idea? Are there any papers exploring this?",4,1
213,2016-6-7,2016,6,7,15,4mxq9l,Paying $50 for machine learning logarithm to calculate center of gravity for images of objects with known centers of gravity.,https://www.reddit.com/r/MachineLearning/comments/4mxq9l/paying_50_for_machine_learning_logarithm_to/,livegoodtimes,1465281276,PM for details hope this is an alright to post this commerce type of stuff???,6,0
214,2016-6-7,2016,6,7,15,4mxqot,Intel's data center chief talks machine learning -- just don,https://www.reddit.com/r/MachineLearning/comments/4mxqot/intels_data_center_chief_talks_machine_learning/,storychurchill,1465281500,,0,0
215,2016-6-7,2016,6,7,15,4mxrr3,Completely free.Looking to contribute to any open ml/datascience frameworks or community.Please post below for any available opportunities.,https://www.reddit.com/r/MachineLearning/comments/4mxrr3/completely_freelooking_to_contribute_to_any_open/,[deleted],1465282060,[deleted],3,0
216,2016-6-7,2016,6,7,16,4mxv7k,Seldon 1.3.2 released with Grafana Analytics Dashboard,https://www.reddit.com/r/MachineLearning/comments/4mxv7k/seldon_132_released_with_grafana_analytics/,ahousley,1465283855,,0,0
217,2016-6-7,2016,6,7,16,4mxwes,Using machine learning for audio signal processing?,https://www.reddit.com/r/MachineLearning/comments/4mxwes/using_machine_learning_for_audio_signal_processing/,dharma-1,1465284486,"Hi,

I'm trying to understand how to use machine learning techniques for audio signal processing tasks like this:

-change an instrument's/singer's pitch while retaining the timbre

-learn the characteristics of a non-linear audio processing unit - (plate reverb, vintage compressor, guitar amplifier etc)

-noise reduction/noise canceling

Any pointers? Is this possible at all? How about realtime?

What kind of machine learning techniques would be worth exploring? Would reinforcement learning work? or something like this? https://github.com/lukovkin/ufcnn-keras

The processing should work with different sample rates so I'm thinking the processing should not be done on the raw audio data but after some kind of a conversion step (DCT/FFT/wavelet/?).",3,2
218,2016-6-7,2016,6,7,16,4mxwf3,"What happened to academic journalism? Just got this in my email inbox ... Elsevier, sigh",https://www.reddit.com/r/MachineLearning/comments/4mxwf3/what_happened_to_academic_journalism_just_got/,thatsit0,1465284490,,0,1
219,2016-6-7,2016,6,7,19,4mye9q,Best GPU laptop for machine learning?,https://www.reddit.com/r/MachineLearning/comments/4mye9q/best_gpu_laptop_for_machine_learning/,cjmcmurtrie,1465295621,"Does anyone have any experience with GPU laptops for fast machine learning work?

My desktop setup is fantastic - Titan X, 32 RAM, SSD, Ubuntu 14.04. I'm looking for something on these lines, but obviously getting a Titan X in a laptop isn't possible.

Any suggestions?",17,0
220,2016-6-7,2016,6,7,19,4myge8,Neural Network Programming Help,https://www.reddit.com/r/MachineLearning/comments/4myge8/neural_network_programming_help/,kreukle,1465296712,,0,1
221,2016-6-7,2016,6,7,19,4myhap,Decoding Brainwaves with Azure Machine Learning | FWTV on 9,https://www.reddit.com/r/MachineLearning/comments/4myhap/decoding_brainwaves_with_azure_machine_learning/,croninlemons,1465297180,,0,6
222,2016-6-7,2016,6,7,20,4mykpo,Applying Deep Learning to Biomarker Development &amp; Drug Discovery,https://www.reddit.com/r/MachineLearning/comments/4mykpo/applying_deep_learning_to_biomarker_development/,reworksophie,1465298929,,0,1
223,2016-6-7,2016,6,7,22,4myy3m,how neural storyteller works,https://www.reddit.com/r/MachineLearning/comments/4myy3m/how_neural_storyteller_works/,toisanji,1465304994,,0,68
224,2016-6-7,2016,6,7,22,4myyhj,Introduction to Prediction Markets,https://www.reddit.com/r/MachineLearning/comments/4myyhj/introduction_to_prediction_markets/,reidhoch,1465305145,,2,16
225,2016-6-7,2016,6,7,22,4myysr,How we use machine learning to qualify leads,https://www.reddit.com/r/MachineLearning/comments/4myysr/how_we_use_machine_learning_to_qualify_leads/,[deleted],1465305266,[deleted],2,3
226,2016-6-7,2016,6,7,23,4mz8kg,An all-in-one Docker image for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/4mz8kg/an_allinone_docker_image_for_deep_learning/,iluvmylife,1465308993,,13,37
227,2016-6-7,2016,6,7,23,4mzd5l,How we use machine learning to qualify leads,https://www.reddit.com/r/MachineLearning/comments/4mzd5l/how_we_use_machine_learning_to_qualify_leads/,mrborgen86,1465310637,,0,0
228,2016-6-7,2016,6,7,23,4mzejq,[1606.01404] Generating Natural Language Inference Chains,https://www.reddit.com/r/MachineLearning/comments/4mzejq/160601404_generating_natural_language_inference/,_rockt,1465311086,,1,14
229,2016-6-7,2016,6,7,23,4mzek9,Question understanding batched training with RNNs,https://www.reddit.com/r/MachineLearning/comments/4mzek9/question_understanding_batched_training_with_rnns/,[deleted],1465311093,[deleted],4,0
230,2016-6-7,2016,6,7,23,4mzfxd,Advantage of Anaconda R-essentials over custom installation?,https://www.reddit.com/r/MachineLearning/comments/4mzfxd/advantage_of_anaconda_ressentials_over_custom/,PullThisFinger,1465311540,I've played with R for a few years now &amp; recently learned of Continuum's R Essentials setup. What do I gain by using the Anaconda version over building my own? Thanks!,0,1
231,2016-6-8,2016,6,8,0,4mzkeg,Competition: Predict the activity of senior citizens from wearable data,https://www.reddit.com/r/MachineLearning/comments/4mzkeg/competition_predict_the_activity_of_senior/,dat-um,1465313098,,0,0
232,2016-6-8,2016,6,8,0,4mzmtx,s softmax necessary as the output layer activation function in multiclass classification using neural networks?,https://www.reddit.com/r/MachineLearning/comments/4mzmtx/s_softmax_necessary_as_the_output_layer/,adityar1,1465313941,[removed],0,1
233,2016-6-8,2016,6,8,0,4mzq3p,"Computer Vision Research: The deep ""depression""",https://www.reddit.com/r/MachineLearning/comments/4mzq3p/computer_vision_research_the_deep_depression/,tangerinemike,1465315045,,13,29
234,2016-6-8,2016,6,8,1,4mzrbg,Does anybody know when will be the official release of scikit-learn 0.18?,https://www.reddit.com/r/MachineLearning/comments/4mzrbg/does_anybody_know_when_will_be_the_official/,hoaphumanoid,1465315449,They are preparing new cool algorithms http://scikit-learn.org/dev/modules/classes.html,1,0
235,2016-6-8,2016,6,8,1,4mzt23,A very high-level idea for creating unsupervised learning in vision problems using deeplearning,https://www.reddit.com/r/MachineLearning/comments/4mzt23/a_very_highlevel_idea_for_creating_unsupervised/,TheFaggetman,1465316022,"One of the major limitations of current deeplearning style algorithms is their inability to do unsupervised learning. If we really want to unlock the potential of machine learning, unsupervised learning must work like a charm.

Although I dont know too much about anything, I spent some time yesterday thinking about how you might want to do unsupervised learning in vision. The ideas I got are very highlevel with absolutely no algorithmic detail, and you will see that every paragraph in this post is going to end with some variation of I have no idea how to do this. But I thought the generel idea was interesting.

&amp;nbsp;

The main idea is to use time as a superviser. Lets say you made 3d-models of some objects, hammers, chairs, doors and so on. You would then generate a bunch of timeseries images of the objects rotating, translating and scaling through time, and use these images as automatic labeled data.
First you would need an algorithm that could separate different objects and differentiate them from background, then an algorithm that could identify when two objects in two consecutive (in time) images where the same (as in, in image 1 there was a hammer, and in image 2 the same hammer was only very slightly moved/rotated/translated, so the algorithm could identify these to images of the hammer as being the same object). I havent thought much about how to formalize the idea of two images being almost identical with only very slight variation, and have no idea how to create such an algorithm or how difficult it would be.

&amp;nbsp;

Then it could label all the different images of the same objects as being the same object, and then use a classical deeplearning technique to learn from these images to generate features for the different objects. The hope is here that you could automatically create arbitrarily large amounts of datasets of labeled data from these objects at every conceivable angle, position, distance etc. 

&amp;nbsp;

The next hope is to use this data to automatically create different groupings of objects at different abstraction levels. For instance, it could group all standard wooden chairs as member of one set, all recliners as member of another set, a superset consisting of the set of wooden chairs and recliners which also contains the set of sofas and so on. 
I imagined the way to do this is to look at what features two different objects have in common. For instance, all objects would have edge-features in common, but all hammers would have some features that scissors dont have and so on. 
Then you would create sets of objects that all have a particular set of features in common.
You could then create a hierarchy of sets of objects, such that the lowest level of the hierarchy is the set of all objects, since all objects have edge-features in common perhaps. Higher up in the hierarchy you would find a set consisting of all objects that share edge detectors and a feature that detects some sort of stick-like-object. In this set you would find chairs (with legs), hammers (with handles) etc. 

&amp;nbsp;

If this would actually work properly you would hopefully get a very complicated and detailed map of relations between different objects, where different abstractions and concepts of objects would emerge automatically. There might be a set consisting of all tool-like objects, a set of all car-like objects, a subset of the car set that contains all SUVs and so on. 

&amp;nbsp;

How you would cope with the massive amounts of redundant data, a quick, efficient and working search function (such that a new image of an object quickly could be classified correctly at all the different abstraction levels) I have no idea. 
Another big drawback of this method as far as I can se is the inability to do continuous learning. You would need to first create all the data of all the objects you would like to get classified, and if you then in the future would like a new object in the mix, you would have to retrain the entire network. 

&amp;nbsp;

I dont know how you would tackle the problem of identifying relevant features that different objects could have in common and actually finding these objects either, so I actually really dont have an idea of how practical this approach is. But I found it interesting and would love to hear if any of you have some thoughts on it or have heard of something similar.",0,0
236,2016-6-8,2016,6,8,1,4mzxsz,Caloric Lab - Automatic calorie count using ML,https://www.reddit.com/r/MachineLearning/comments/4mzxsz/caloric_lab_automatic_calorie_count_using_ml/,hartator,1465317601,,1,0
237,2016-6-8,2016,6,8,1,4n0178,ML using scikit learn,https://www.reddit.com/r/MachineLearning/comments/4n0178/ml_using_scikit_learn/,asvance,1465318745,"Hi, I'm looking for a video series to learn ML using Python.
I have googled and found a number of them and I'm not sure which one to start as I'm new to ML. It will be awesome if you guys can guide me to a good place to start with. 
Thanks.",5,3
238,2016-6-8,2016,6,8,2,4n0bkn,"A patent for ""Face Detection Using Machine Learning"" was just granted.",https://www.reddit.com/r/MachineLearning/comments/4n0bkn/a_patent_for_face_detection_using_machine/,ieee8023,1465322062,,29,101
239,2016-6-8,2016,6,8,3,4n0fut,"How to deal with very ""wide"" dataset?",https://www.reddit.com/r/MachineLearning/comments/4n0fut/how_to_deal_with_very_wide_dataset/,Icko_,1465323332,"I'm working w/ the worldbank dataset, which has 1420 indicators (in 20 subcategories) for most countries, but only for 40 something years. Now, if I want to predict the GDP of Armenia for 2017, I'll have 40 datapoints, each with 1420\*300 columns. I **could** use only the armenian indicators, but I'd miss using a ton of useful information, and I'd not be much better off. Any ideas?",10,0
240,2016-6-8,2016,6,8,3,4n0nrs,What's the name of the field of NLP for recognizing subphrases like Wit.ai and API.ai do?,https://www.reddit.com/r/MachineLearning/comments/4n0nrs/whats_the_name_of_the_field_of_nlp_for/,pixelrealm_aaron,1465325760,Any papers people recommend for getting into this field?,2,0
241,2016-6-8,2016,6,8,4,4n0ojx,Learning ML by re-implementing algos ?,https://www.reddit.com/r/MachineLearning/comments/4n0ojx/learning_ml_by_reimplementing_algos/,datagibus420,1465326008,"Hello Reddit,

I'm currently switching from computational stats to ML, and to get to understand better how ML algorithm works, I was thinking about coding implementations of the most basic ones from scratch, with nothing but numpy. The idea here is clearly not to build something 100% efficient but to get to know what is going on under the hood, to get decent results on classic datasets (iris, boston, etc.), and to identify potential bottlenecks that are properly adressed in packages such as scikit-learn.

Vanilla implementations of ML algos seems to be common in assignments from several courses, but I wanted to have your thoughts on whether or not it may bring something more than just watching vids/reading books/directly use scikit-learn or any other ML package.

Th@nks!",1,0
242,2016-6-8,2016,6,8,4,4n0qmj,An appeal to those of you making those sweet machine learning libraries intended for data scientists,https://www.reddit.com/r/MachineLearning/comments/4n0qmj/an_appeal_to_those_of_you_making_those_sweet/,datasciguy-aaay,1465326683,"Consider a brief, critical, but still commonly unmet set of data science requirements.  

Here is a very common scenario: You get a dataset on disk that likely has a working memory representation which is somewhat larger than the physical RAM that is available in the computer which runs the front end data science application code, such as a 20 GB CSV file sitting on the local disk of a desktop data science computer sporting only 16GB of RAM. I will call this a medium dataset, and it is being stored on a conventional computer.  

Buy more RAM, right? Times change, RAM gets cheaper, but the data grows even faster, so we never catch a break!  Sure, there are many ways around the problem, starting with random sampling, or putting an RDBMS into the feature pipeline -- an RDBMS definitely knows how to handle larger than RAM datasets.  One strategy I happen to like personally, is dividing the dataset by rows and learning on each partition independently, then combining the results with simple average or majority vote, or if you want to get fancy about it, a learning ensemble or a consensus lasso at least for those convex optimization learning algorithms.  

For sure, data scientists may know a computer science trick to two and may be able to code well enough to work around these problems, but any extra code appearing in the R markdown or ipynb file makes it even harder for other scientists to get the main gist of our reproducible research. Time and opportunity cost are a big concern too because as data scientists we already have full plates of other data science work to do, like formulating the right questions, getting the right data, exploratory data analysis, predictive modeling or statistical inference, communicating the results, and prototyping and productionizing, and iterating over all of these steps.

I am petitioning the machine learning package developers to think about how they would remedy a too-often overlooked set of data science requirements. This is not a complete list but its just a short list of requirements that even some otherwise state of the art new packages are still not satisfying:

Data scientists like to use medium datasets.  Has your machine learning, data mining, or statistical implementation demonstrably run to correct termination with the RAM we actually have, not the RAM we wished we had?  We hate it when the package assumes we should just load and process the whole dataset or the whole model into the RAM of the workstation or cluster node, but then it just crashes, or it runs 24 hours and then crashes.  A crash, premature termination, or a complete hang before ever emitting the result are examples of incorrect termination. Out of core processing is needed. On parallel shared memory, but especially distributed systems, the difficulty level for getting correct termination right is even greater. Data scientists just dont have time for this.

Data scientists like to use big datasets. Has your machine learning, data mining, or statistical implementation demonstrably scaled to greater than 1000 CPU cores in any configuration at all? We appreciate that communication, synchronization, and memory accesses are fully as interesting as processor compute power when parallel, concurrent, and distributed programming is happening.  I feel your pain, I really do, but better you than me!  Map-reduce is good or great sometimes, but sometimes map-reduce just isnt the right tool for the distributed computations. It communicates too much in some cases. Have you seriously considered using additional distributed parallel paradigms for parallelizing some of these data science algorithms?

Data scientists like to get it done efficiently with current COTS hardware like GPU and CPU accelerator cards, because these cards can be cheaper and faster compared to all-purpose CPUs on arithmetic with giant matrices where the operations need to be iterated until optimization. If  your machine learning, data mining, or statistical implementation package offers a delicious approximate distance algorithm, monte carlo simulation, cross validation routine, or a learning algorithm that implements neural networks, has it demonstrably scaled using N COTS accelerator devices for computation, for N greater than zero if not one?

Data scientists like to write reproducible research code so that we can reasonably share code and data-supported papers to prove our points with other non-computer scientists, such as medical doctors, et al.  Can your machine learning, data mining, or statistical implementation be called from an R or python front end data science application which is written in a reproducible research style that data scientists prefer, while also hiding all of the computer-sciencey code which may be needed to process a medium dataset?  You can safely expect that data scientists will want at a minimum to display our research in a web browser either as a python notebook ipynb file, or an R markdown language Rmd file, and to print it to a PDF file.  And the whole report cannot be more than just a few pages long, because other people who read it, will definitely fall asleep before getting to the amazing results at the end, even though we personally are excited by the whole pipeline.

Despite it all, data scientists now and again will enjoy to be able to get it done ourselves when we need to. Sometimes the best ML library unfortunately does not yet provide the computer science algorithm thats best for our application. The new custom CS code we will write, will also not belong appearing in a reproducible research application. The code we write may be specific to your package with dependencies on it. The data science community may want a good copy of my routine for their own use too, especially if your main package is successful and becomes used by everyone.  Does your machine learning, data mining, or statistical implementation already demonstrably offer a collection of first-rate second or third party software contributions that were NOT written and submitted by people who work for the institution that founded, or which primarily currently maintains, the package? There are still many interesting problems out there -- we live in interesting times -- and data scientists cannot strictly wait or hope for the worlds best programmer (at your company) to write all the new computer sciencey code for them. Fortunately there is a growing cluster of data scientists and computer scientists who are willing and able to share the CS writing load in parallel.  We can increase our total throughput as well as reduce latency on current projects if you would provide and publish a decent framework to help us join forces effectively to synchronize and test our code contributions enabling us to efficiently add new things to your package library. The worlds best programmers can still come along later when they get a chance and refactor and improve the contributed library code as needed. The contributed first implementations will by their existence, contribute meaningfully both an API and some results to act as useful baselines and test cases on which the real computer scientists can add improvements or make new ones.

Thanks for reading.",5,0
243,2016-6-8,2016,6,8,4,4n0qy3,What is Machine Learning Architecture? Do you have examples?,https://www.reddit.com/r/MachineLearning/comments/4n0qy3/what_is_machine_learning_architecture_do_you_have/,htaidirt,1465326789,"I'm looking for a new job in Data Science, and have found job positions titled as ""Machine Learning Architect"". Don't really sure about what it is, but maybe I am still a newbie.

What does a Machine Learning Architect do and what are the required skills? Can you give examples, please?

Thank you.",1,0
244,2016-6-8,2016,6,8,4,4n0xis,What to do with an industrial/manufacturing data set?,https://www.reddit.com/r/MachineLearning/comments/4n0xis/what_to_do_with_an_industrialmanufacturing_data/,lurkingforawhile,1465328906,"I am a chemical engineer who is learning programming. Until now I've mostly been working on building interfaces using web programming. Recently I got access to all of my company's lab/quality, inventory, and PLC/manufacturing data. The PLC data being temperatures, pressures, speeds, etc. of equipment and settings within a manufacturing plant.

I am interested in digging into this data. I am (sort of) versed in Python, and have done a few tutorials with sklearn and the linear regression algorithm. I did a pretty basic case study using this on a filtering system within a plant, basically being able to predict some future values to help maintenance plan their time better, and then retraining the classifier once a week with a script to increase accuracy as more data comes in and time goes on.

My management has asked me to spend some time exploring the technology a little more, but I find myself overwhelmed. There seems to be SO MANY algorithms/methods/paradigms, and I have so much data from a large variety of sources. I was wondering if anyone could suggest some starting points, things to explore, or basically what you would do if plopped into the middle of this.

Thank you for your time.",8,8
245,2016-6-8,2016,6,8,5,4n12t2,Neural Programmer-Interpreters: programs that can learn programs,https://www.reddit.com/r/MachineLearning/comments/4n12t2/neural_programmerinterpreters_programs_that_can/,AlNejati,1465330568,,1,14
246,2016-6-8,2016,6,8,5,4n170j,Frameworks for developing intelligent video game agents?,https://www.reddit.com/r/MachineLearning/comments/4n170j/frameworks_for_developing_intelligent_video_game/,SlyDevil86,1465331905,"Recently, I have learned about a couple of instances where deep neural networks were being developed to play games based purely off the pixel input alone (specifically, work done with the Atari Learning Environment, and also the Vizdoom competition.)

My question is, are there any frameworks out there that have multiple games built in for this purpose? I'm very interested in systems like Google's DeepMind but I'm not aware of any widely available frameworks for working with things like that. ",1,0
247,2016-6-8,2016,6,8,5,4n17oq,What's the difference between Facial Keypoints and Facial Landmarks?,https://www.reddit.com/r/MachineLearning/comments/4n17oq/whats_the_difference_between_facial_keypoints_and/,Rich700000000000,1465332122,"I can't find any distinction online. They aren't interchangeable, are they?",1,0
248,2016-6-8,2016,6,8,6,4n1ivq,Facebook AI: A tale of 2 teams,https://www.reddit.com/r/MachineLearning/comments/4n1ivq/facebook_ai_a_tale_of_2_teams/,sudoscript,1465335757,,1,1
249,2016-6-8,2016,6,8,6,4n1leb,Yunji Chen | Innovators Under 35,https://www.reddit.com/r/MachineLearning/comments/4n1leb/yunji_chen_innovators_under_35/,akarainer751,1465336554,,0,1
250,2016-6-8,2016,6,8,7,4n1m51,For Well Thou Know'st: Simplified font recognition livestream,https://www.reddit.com/r/MachineLearning/comments/4n1m51/for_well_thou_knowst_simplified_font_recognition/,vanboxel,1465336811,,0,0
251,2016-6-8,2016,6,8,7,4n1p0z,[Need guidance] Going for a 2 years Masters in AI/ML. Dos/Don'ts/Advice?,https://www.reddit.com/r/MachineLearning/comments/4n1p0z/need_guidance_going_for_a_2_years_masters_in_aiml/,Gear5th,1465337799,"I'm about to start my Masters from a good college in India. My long term aim is to help build a Strong, Friendly AI. Short term aim would be to work at a great research lab after my Masters, or go for a Ph.D at an Ivy League college.


There's 1 month remaining till the session begins.

**I'm looking for any guidance you could provide me with, keeping my goals in mind.**

I'm revising Linear Algebra, Probability at the moment. What other Maths topics should I revise?

What papers are good for an absolute beginner to read? (background: done [ML, Andrew Ng, coursera](https://www.coursera.org/learn/machine-learning). Skimmed [cs231n, stanford](http://vision.stanford.edu/teaching/cs231n/syllabus.html))

What should I focus on most during my Masters? Dos/Donts?

Thanks :)


^(PS: Didn't post in MLQuestions since it has only 2k readers :()",6,0
252,2016-6-8,2016,6,8,7,4n1voz,Image Classification with Very Little Data,https://www.reddit.com/r/MachineLearning/comments/4n1voz/image_classification_with_very_little_data/,gwulfs,1465340291,,8,13
253,2016-6-8,2016,6,8,9,4n2bok,"Could I use an image dataset to define a concept, and then rank images based on how close they are to that concept?",https://www.reddit.com/r/MachineLearning/comments/4n2bok/could_i_use_an_image_dataset_to_define_a_concept/,Rich700000000000,1465346065,"I just got done scraping over 20,000 images from instagram, and I noticed that they were pretty much all the same size. I got an idea: Would it be possible to develop a classifier to determine the percentage of an attribute is an image?

Let me explain: On instagram, generally, one account will have one topic: guns, edm, cars, lgbt, etc. If I took 2,000 images from festivals, clubs, raves and whatnot, could develop a system that would detect, for example, how ""EDM"" an image is? 

It could work like this:

1. Assemble a dataset of 2,000 or so images, all of which are heavily EDM: People raving, strobe lights, MDMA, giant crowds, 
2. Assemble a counterdataset of 2,000 images that are heavily non-EDM: Skylines, power plants, nature trails, stock markets, etc.
3. Extract the visual features (https://imgur.com/c8I7ia2.jpg, Xirong Li@ICME14) from the edm images.
4. Do the same with the non-edm images.
5. Use the features to build two models to predict how edm/not-edm an image is.
6. Use the prediction score to calculate a mathematical percentage.
7. Return percentage to the user: ""This image is 82.33% EDM, with 90.11% accuracy.""

What do you think? Is this feasible?

",7,0
254,2016-6-8,2016,6,8,11,4n2p32,[1606.02245] Iterative Alternating Neural Attention for Machine Reading,https://www.reddit.com/r/MachineLearning/comments/4n2p32/160602245_iterative_alternating_neural_attention/,RushAndAPush,1465351253,,0,9
255,2016-6-8,2016,6,8,12,4n338t,Understanding vectors in Word2Vector model,https://www.reddit.com/r/MachineLearning/comments/4n338t/understanding_vectors_in_word2vector_model/,FutureIsMine,1465356961,"Im new to machine learning and I have recently started learning about how Word2Vector works. Generally the model could be described as P(O|C) where o is an outside word of the window and C is the context word. What I'm trying to understand in the model is how exactly does the vector corresponding to O and C look like. Going through Stanfords CS224d - Deep learning for natural language processing, I still feel that I don't understand this part. ",7,0
256,2016-6-8,2016,6,8,13,4n39ip,Graph layout,https://www.reddit.com/r/MachineLearning/comments/4n39ip/graph_layout/,jdh30,1465359676,"Graph layout (ala graphviz) is a challenging problem in computer graphics. Given that humans are quite good at solving this problem, I was wondering if anyone had tried using classic machine learning algorithms like deep learning to lay out graphs?
",4,0
257,2016-6-8,2016,6,8,17,4n3z06,How is machine learning bringing the future to the present? | #SparkBizApps,https://www.reddit.com/r/MachineLearning/comments/4n3z06/how_is_machine_learning_bringing_the_future_to/,dobsonmilner,1465373418,,1,0
258,2016-6-8,2016,6,8,17,4n431t,Batch Normalization: how to modify network for inference?,https://www.reddit.com/r/MachineLearning/comments/4n431t/batch_normalization_how_to_modify_network_for/,NovaRom,1465375973,"During training, each layer's activations (before non-linearity) are  normalized and {scaling, shift} factors are learned. Is it possible to modify weights/biases after the batch-normalized learning so that resulted network can be used without batch-normalization in the inference mode?",7,1
259,2016-6-8,2016,6,8,19,4n4fsr,"Interruptibility, the big red button",https://www.reddit.com/r/MachineLearning/comments/4n4fsr/interruptibility_the_big_red_button/,pilooch,1465383440,,4,1
260,2016-6-8,2016,6,8,20,4n4mbi,What neural network model to use for generating grammatically correct sentences given few words/facts?,https://www.reddit.com/r/MachineLearning/comments/4n4mbi/what_neural_network_model_to_use_for_generating/,n00bto1337,1465386681,"I need to generate sentences given a few words. For example, given a fact like '10mm rainfall', I need to generate a sentence like 'About 10mm rainfall was recorded today.'
Are there any open source architectures which does something similar? And can I train it on my data set?

Edit: Apart from neural approach, can something be done using Markov chains?",13,9
261,2016-6-8,2016,6,8,21,4n4om6,Looking to contribute to any open ml/datascience frameworks or community.Please post below for any available opportunities.,https://www.reddit.com/r/MachineLearning/comments/4n4om6/looking_to_contribute_to_any_open_mldatascience/,datavinci,1465387732,"Have done a couple of courses and projects,but never done any open source contributions.Hence would like to do so as I have all the time I need.Below is my CV if you are interested.
[MY CV](https://drive.google.com/open?id=0B4IvM_1ntghvTXA3REZNMWwtd00)",7,0
262,2016-6-8,2016,6,8,21,4n4vel,Harvard students develop machine-learning algorithm that categorizes TripAdvisor's user-generated restaurant photos,https://www.reddit.com/r/MachineLearning/comments/4n4vel/harvard_students_develop_machinelearning/,azewe,1465390563,,0,1
263,2016-6-8,2016,6,8,22,4n523q,Coupling active learning and the tuning of a perceptron for surrogate modeling. How would you approach this problem ?,https://www.reddit.com/r/MachineLearning/comments/4n523q/coupling_active_learning_and_the_tuning_of_a/,SoMuchQuestions,1465393175,"Hi everyone,how would you approach this problem ?

I have a simulation tool that allows me to calculate a deterministic  real output y given a vector of real inputs X.
This simulation runs slowly (1-10sec) but I'm able to run it through scripts.

I want to make a python tool allowing me to create a surrogate model of the simulation.
I plan to use a multilayer perceptron (MLP) as they theoretically can adapt to any kind of response function.
I want the tool to automatically find the inputs to simulate and the optimal size of MLP to reach a given precision on the output.
As I can call the simulation through scripts, I'm thinking about using some kind of active learning method to decide the X to simulate and increasing the size of the neural net until the required precision is reached.

I have not found a publication dealing with this problem yet.
Are there python libraries that can help me besides scikit-learn ?
Do you have any advice to give me ? How would you approach this problem ?

Best Regards,",3,1
264,2016-6-8,2016,6,8,23,4n5cl7,Tensorflow RNN with 'variable length' sequences,https://www.reddit.com/r/MachineLearning/comments/4n5cl7/tensorflow_rnn_with_variable_length_sequences/,lhlmgr,1465396832,"Hi guys,

unfortunately the previous [post](https://www.reddit.com/r/MachineLearning/comments/3sok8k/tensorflow_basic_rnn_example_with_variable_length/) is archived, so I cannot comment there.

I just wanted to ask, if there is a proper solution to build a model for sequences with variable length? I found a [comment](https://www.reddit.com/r/MachineLearning/comments/40c8yw/seq2seq_is_bucketing_just_a_tensorflow_quirk_how/cyt1liw) from 4 months ago, which states that tensorflow doesn't support sequences of variable length. However, this [comment](https://www.reddit.com/r/MachineLearning/comments/3sok8k/tensorflow_basic_rnn_example_with_variable_length/cztiryi) as well as this [example](https://github.com/cozyhous/dynamic_rnn_example/blob/master/trainer.py) for the function `dynamic_rnn()` state this is possible.

 Is there a proper way to handle variable lengths, or should I stay with the slightly more complex bucket model? 

Thanks in advance!",24,13
265,2016-6-8,2016,6,8,23,4n5erw,"TED-Talk: Every piece of art you've ever wanted to see -- up close and searchable, usinh Machine Learning",https://www.reddit.com/r/MachineLearning/comments/4n5erw/tedtalk_every_piece_of_art_youve_ever_wanted_to/,[deleted],1465397551,[deleted],0,1
266,2016-6-8,2016,6,8,23,4n5fku,ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation,https://www.reddit.com/r/MachineLearning/comments/4n5fku/enet_a_deep_neural_network_architecture_for/,[deleted],1465397833,[deleted],0,1
267,2016-6-9,2016,6,9,0,4n5h3o,Deep learning helps to map Mars and analyze its surface chemistry,https://www.reddit.com/r/MachineLearning/comments/4n5h3o/deep_learning_helps_to_map_mars_and_analyze_its/,quezadascanlon,1465398340,,0,10
268,2016-6-9,2016,6,9,0,4n5h4g,"TED-Talk: Every piece of art you've ever wanted to see -- up close and searchable, using MachineLearning",https://www.reddit.com/r/MachineLearning/comments/4n5h4g/tedtalk_every_piece_of_art_youve_ever_wanted_to/,AnonymousChimpanzee,1465398347,,0,1
269,2016-6-9,2016,6,9,0,4n5jsb,Models make predictions on Olympic medals,https://www.reddit.com/r/MachineLearning/comments/4n5jsb/models_make_predictions_on_olympic_medals/,smoothindeed,1465399249,,0,0
270,2016-6-9,2016,6,9,0,4n5lnh,Multimodal Residual Learning for Visual QA,https://www.reddit.com/r/MachineLearning/comments/4n5lnh/multimodal_residual_learning_for_visual_qa/,torch7,1465399880,,3,0
271,2016-6-9,2016,6,9,0,4n5ltj,test,https://www.reddit.com/r/MachineLearning/comments/4n5ltj/test/,[deleted],1465399936,[deleted],2,1
272,2016-6-9,2016,6,9,1,4n5uhw,awesome tensorflow deep learnnig code to play atari games,https://www.reddit.com/r/MachineLearning/comments/4n5uhw/awesome_tensorflow_deep_learnnig_code_to_play/,dgleebits,1465402784,,4,17
273,2016-6-9,2016,6,9,1,4n6163,What's the fastest face detector available?,https://www.reddit.com/r/MachineLearning/comments/4n6163/whats_the_fastest_face_detector_available/,PicopicoEMD,1465404871,"I've tried dlib and OpenCV. They work fine but they're pretty slow. Are there any faster face detectors, preferably available for Python?",12,7
274,2016-6-9,2016,6,9,2,4n689j,Former NASA Exec Brings Stealth Machine Learning Chip to Light,https://www.reddit.com/r/MachineLearning/comments/4n689j/former_nasa_exec_brings_stealth_machine_learning/,InaneMembrane,1465407041,,9,8
275,2016-6-9,2016,6,9,2,4n69ma,Bayesian optimization of hyperparameters in R with 'rBayesianOptimization',https://www.reddit.com/r/MachineLearning/comments/4n69ma/bayesian_optimization_of_hyperparameters_in_r/,gwern,1465407465,,0,7
276,2016-6-9,2016,6,9,2,4n6a0e,distributed Tensorflow resource allocation?,https://www.reddit.com/r/MachineLearning/comments/4n6a0e/distributed_tensorflow_resource_allocation/,logrech,1465407578,"I have a question about how tensorflow does its resource allocation. 

Both the [paper](http://download.tensorflow.org/paper/whitepaper2015.pdf) and several [blogs](https://indico.io/blog/indico-tensorflow/) talk about how distributed tensorflow is able to map the graph onto the set of available devices automatically, using their node placement algorithm. 

Perhaps I'm mistaken, but this would seem to imply that you could train a linear regression model from [skflow](https://github.com/tensorflow/skflow), just as an example, on two machines with great ease because tf should automatically be able to create subgraphs to run on the two cpus. 

Is this actually the case? And if so, how do you do it? 

All the documentation/tutorials I've found on [distributed tensorflow](https://www.tensorflow.org/versions/r0.9/how_tos/distributed/index.html) requires you to specify the device you want to run specific parts of the graph on? 

Where does the automatic resource allocation come in? ",3,0
277,2016-6-9,2016,6,9,3,4n6kes,What is the purpose of OpenAI's Request for Research?,https://www.reddit.com/r/MachineLearning/comments/4n6kes/what_is_the_purpose_of_openais_request_for/,__AndrewB__,1465410784,"So OpenAI introduced something called Requests for Research [https://openai.com/requests-for-research/](https://openai.com/requests-for-research/)

What is the purpose of such initiative? You don't hire the best researchers and raise 1kkk in funding to ""help begginers get into the field"". Serious researchers are fine without OpenAI telling them what to work on, newbies are not likely to help build human-level AI (which, I assume, is OpenAI's goal).

Why invest resources into something like this? I'd rather read publications from OpenAI researchers than watch them become DeepLearning Coursera / Kaggle.",11,4
278,2016-6-9,2016,6,9,3,4n6li6,OpenAI - Requests for Research,https://www.reddit.com/r/MachineLearning/comments/4n6li6/openai_requests_for_research/,siddharth-agrawal,1465411150,,16,212
279,2016-6-9,2016,6,9,4,4n6qbw,How to use KMeans clustering to find similar strings?,https://www.reddit.com/r/MachineLearning/comments/4n6qbw/how_to_use_kmeans_clustering_to_find_similar/,KrustyKrab111,1465412650,"To be specific, im trying to handle a dataframe that contains DNA sequences. Ive tried using KMeans plainly on the dataframe, but it throws

    ValueError: could not convert string to float: NN

i understand this is due to the fact that KMeans only works on numerical data.
After a bit of looking around, i understand that we have to use the levenshtein distance, but im unsure on how to implement it.
Can you guys help me out?

Thanks",12,1
280,2016-6-9,2016,6,9,4,4n6ygh,Several questions regarding preprocessing of data and estimation in random forest classification.,https://www.reddit.com/r/MachineLearning/comments/4n6ygh/several_questions_regarding_preprocessing_of_data/,BorderLineGenius,1465415257,"Hello guys! My first post here :)  
  
For my bachelor thesis I decided to see how well I could predict results of chess games using random forest classifier. I have a nice database of a few million games and some data about them. Using sci-kit I run a few estimations with python (it took a whole day and night on my computer :( even though I only used a few parameters) and reached ~56.4% on the test set, optimizing max_depth = 10 and max_feat = 0.8 on the CV set. It's probably not too good but so far I only used 5 variables (white elo, black elo, day, month and year of game) and it is worth noting that estimation is much harder with the third result (draw) being there.  

Now I wonder how should go about improving my approach. Thus, I have several questions and I would be very grateful if you could answer at least some of them:  

1) What other parameters do you think I should try tuning and how? The sci-kit documentation has many of those, but which would you recommend exploring and in what combinations? Doing all of them would require enormous amount of time so perhaps there are some guidelines that I could follow?  

2) I have a few variables that, if I was doing linear regression, I would strongly consider making dummies of (e.g. players, date). Is it necessary? It seems that e.g. date has a lot of significance despite not being turned into dummies and just 3 variables (year, month, day). I read hat Random Forest does not require feature engineering, to what extent is this true?  

3) Any recommended main reference? I read ""A Random Forest Guided Tour"" by Gerard Biau and Erwan Scornet as well as ""Narrowing the Gap: Random Forests In Theory and In Practice"" by Misha Denil, David Matheson and Nando de Freitas.
",2,0
281,2016-6-9,2016,6,9,5,4n73mt,Maluuba is getting machines closer to reading like humans do,https://www.reddit.com/r/MachineLearning/comments/4n73mt/maluuba_is_getting_machines_closer_to_reading/,rahulmehrotra,1465416906,,0,1
282,2016-6-9,2016,6,9,5,4n75gy,Neural Networks in PyMC3 estimated with Variational Inference,https://www.reddit.com/r/MachineLearning/comments/4n75gy/neural_networks_in_pymc3_estimated_with/,x2342,1465417479,,0,9
283,2016-6-9,2016,6,9,5,4n776n,OpenAI: Requests for Research,https://www.reddit.com/r/MachineLearning/comments/4n776n/openai_requests_for_research/,[deleted],1465418058,[deleted],0,1
284,2016-6-9,2016,6,9,6,4n7bzk,Deep Conspiracy Networks /s. Demis Hassibas and Yoshua Bengio to attend 2016 Bilderberg Meetings.,https://www.reddit.com/r/MachineLearning/comments/4n7bzk/deep_conspiracy_networks_s_demis_hassibas_and/,InaneMembrane,1465419627,,6,22
285,2016-6-9,2016,6,9,6,4n7d4p,Sketch Based Anomaly Detector,https://www.reddit.com/r/MachineLearning/comments/4n7d4p/sketch_based_anomaly_detector/,dorondoron,1465420008,"I just recently got into the field and built a EM based AD to understand the basics of anomaly detection, but there is the problem with streams of data and real-time application in many AD algorithms.

I was wondering if there were any open-source examples to help wrap my head around Sketch based ADs? I have plenty of white papers and articles filled with math that I can reverse engineer if I have to, but if someone has a general or even not so general model for a Sketch based AD that'd be awesome!

Thanks regardless!",0,0
286,2016-6-9,2016,6,9,6,4n7dmn,"Ressources [Books, Courses] for Applied Machine Learning",https://www.reddit.com/r/MachineLearning/comments/4n7dmn/ressources_books_courses_for_applied_machine/,Monninho,1465420177,"Hey guys
So I've noticed that there are tons of books which are explaining machine learning alghorithms in every programming language possible.
I think it would be nice to have a little collection of ressources which are about applying these alghorithms, maybe even by using popular frameworks like scikit-learn.",0,0
287,2016-6-9,2016,6,9,6,4n7l5z,"GPU Wars Have Begun, A Spreadsheet of Specs by Phones, VR, Self-Driving Cars &amp; AI",https://www.reddit.com/r/MachineLearning/comments/4n7l5z/gpu_wars_have_begun_a_spreadsheet_of_specs_by/,seanmeverett,1465422726,,0,1
288,2016-6-9,2016,6,9,7,4n7nsq,Knn theory,https://www.reddit.com/r/MachineLearning/comments/4n7nsq/knn_theory/,kazyka,1465423648,"Hi 

I come here because I have a hard time finding some theory on KNN that uses both math and examples - don't care if it is a book or online articles but I would prefer something online! 

The reason I seek this information is because I am doing some image texture analysis on MRI of the brain and I need to apply KNN on this, if possible with cross validation",2,0
289,2016-6-9,2016,6,9,7,4n7w2o,How does the neural network system of AWS work?,https://www.reddit.com/r/MachineLearning/comments/4n7w2o/how_does_the_neural_network_system_of_aws_work/,meowcoew,1465426567,"Does it allow you to upload your own code to run in any language, or do you have to use a certain language and library?",0,0
290,2016-6-9,2016,6,9,8,4n7zjl,The challenges of machine learning on iOS,https://www.reddit.com/r/MachineLearning/comments/4n7zjl/the_challenges_of_machine_learning_on_ios/,hartator,1465427810,,2,0
291,2016-6-9,2016,6,9,8,4n7zn4,Google to create Kill Switch for Rogue AI,https://www.reddit.com/r/MachineLearning/comments/4n7zn4/google_to_create_kill_switch_for_rogue_ai/,annacho,1465427843,,0,0
292,2016-6-9,2016,6,9,9,4n874d,"Woj Zaremba's thesis on ""Learning Algorithms from Data"" is out",https://www.reddit.com/r/MachineLearning/comments/4n874d/woj_zarembas_thesis_on_learning_algorithms_from/,evc123,1465430616,,9,28
293,2016-6-9,2016,6,9,9,4n88fq,Is Anyone Running TensorFlow 0.9 on Ubuntu 14.04?,https://www.reddit.com/r/MachineLearning/comments/4n88fq/is_anyone_running_tensorflow_09_on_ubuntu_1404/,Boozybrain,1465431113,"EDIT: I've narrowed it down to the pywrap_tensorflow module; it doesn't exist.

    import pywrap_tensorflow
    Traceback (most recent call last):
      File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
    ImportError: No module named pywrap_tensorflow

After running into the same error with pip installs I decided to build from source and am still getting an error when trying to import tensorflow:


    import tensorflow as tf
    Traceback (most recent call last):
      File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
      File ""tensorflow/__init__.py"", line 23, in &lt;module&gt;
        from tensorflow.python import *
      File ""tensorflow/python/__init__.py"", line 48, in &lt;module&gt;
        from tensorflow.python import pywrap_tensorflow
    ImportError: cannot import name pywrap_tensorflow

After looking around I see similar issues but it seems like most are GPU related or a reinstall via pip worked, which in my case it did not.
",4,0
294,2016-6-9,2016,6,9,10,4n8gka,Doc2Vec vs Averaging Word2Vec vectors,https://www.reddit.com/r/MachineLearning/comments/4n8gka/doc2vec_vs_averaging_word2vec_vectors/,[deleted],1465434301,[deleted],1,0
295,2016-6-9,2016,6,9,10,4n8n1d,Deep Successor Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/4n8n1d/deep_successor_reinforcement_learning/,MetricSpade007,1465436791,,5,6
296,2016-6-9,2016,6,9,11,4n8r8i,What are the business applications of deep reinforcement learning?,https://www.reddit.com/r/MachineLearning/comments/4n8r8i/what_are_the_business_applications_of_deep/,MasterEpictetus,1465438413,"Besides being great for playing games and useful to understand how intelligence works, how is deep reinforcement learning being used in businesses?",4,0
297,2016-6-9,2016,6,9,12,4n92df,A Shallow Dive into DeepCNet with Mojo CNN,https://www.reddit.com/r/MachineLearning/comments/4n92df/a_shallow_dive_into_deepcnet_with_mojo_cnn/,gnawice,1465444035,,0,1
298,2016-6-9,2016,6,9,13,4n94r4,Translation Between Word Embeddings,https://www.reddit.com/r/MachineLearning/comments/4n94r4/translation_between_word_embeddings/,Jxieeducation,1465445046,,1,4
299,2016-6-9,2016,6,9,13,4n97bx,White House Asks Artificial Intelligence Experts to Help With Reducing Incarceration Rates,https://www.reddit.com/r/MachineLearning/comments/4n97bx/white_house_asks_artificial_intelligence_experts/,MichaelLewis99,1465446249,,2,0
300,2016-6-9,2016,6,9,13,4n9afz,The Biggest Players In Artificial Intelligence Today,https://www.reddit.com/r/MachineLearning/comments/4n9afz/the_biggest_players_in_artificial_intelligence/,JoeyRob,1465447791,,0,1
301,2016-6-9,2016,6,9,19,4na6i9,Vanilla RNN Backprop Through Time Equations in Detail,https://www.reddit.com/r/MachineLearning/comments/4na6i9/vanilla_rnn_backprop_through_time_equations_in/,bge0,1465467563,,9,41
302,2016-6-9,2016,6,9,19,4na6y7,"Predictive modelling, how to build ground-truth and extract features for action prediction?",https://www.reddit.com/r/MachineLearning/comments/4na6y7/predictive_modelling_how_to_build_groundtruth_and/,ramialbatal,1465467862,"I have a dataset of users, each user has has daily information about his activities (numerical values representing some measurements of his physical activities).

In addition, each user in each day has a boolean value that represents if he/she took a particular action.

The data set is not fixed, so new activities information and action are added for each user each new day.

The dataset looks as follow

    +------+----------+------------+-------------+-------+
    |userID|      date|   activity1|    activity2| action|
    +------+----------+------------+-------------+-------+
    | user1|2016-06-05|         5.3|          5.3|  false|
    | user1|2016-06-04|         3.1|          5.3|   true|
    | user1|2016-06-03|         2.0|          5.3|  false|
    | user1|2016-06-02|         4.7|          5.3|  false|
    | user1|2016-06-01|         1.3|          5.3|  false|
    | user1|   ...ect.|         ...|          ...|    ...|
    | user2|2016-06-05|         0.6|          5.3|   ture|
    | user2|2016-06-04|         3.0|          5.3|  false|
    | user2|2016-06-03|         0.0|          5.3|  false|
    | user2|2016-06-02|         2.1|          5.3|  false|
    | user2|2016-06-01|         6.3|          5.3|  false|
    | user2|   ...ect.|         ...|          ...|    ...|
    | user3|2016-06-05|         5.3|          5.3|  false|
    | user3|2016-06-04|         5.3|          5.3|  false|
    | user3|2016-06-03|         6.8|          5.3|  false|
    | user3|2016-06-02|         4.9|          5.3|  false|
    | user3|   ...ect.|         ...|          ...|    ...|
    +------+----------+------------+-------------+-------+

**Goal**

Build a model that predicts which user is likely to take the action in the near future (e.g. in any of the next 7 days).

**Approach**

My approach is to build feature vectors representing the activity values for each users over a period of time, and use the action column as a source of ground-truth. Then I feed the ground-truth and the feature vectors to a binary classification training algorithm (e.g. SVM or Random Forest) in order to generate a model able to predict if a user is likely to take the action or not.

**Problem**

I started by the positive examples that are the users who took the action. To extract the feature vector of a positive example, I combined the activity values in the X (30 or 7 or 1) days preceding the action (the day of taking the action is included).

When I moved to the negative examples, it get less obvious, I am not sure **how to select negative examples and how to extract features from them**. This has led me actually to re-question if **my way of selecting positive examples and building my the feature vectors was correct**. 

**Questions**

 1. How to build the ground-truth of positive (users who did take the action) and negative (users who didn't take the action) examples?
 2. What is a negative example in this case? is it the user who didn't take the action in a fixed period of time? What if he didn't take the action in this fixed period, but he just took it right after?
 3. What are the possible approaches of selecting the ranges of dates to extract feature vectors from.

**Rational Question**

Is there more suitable approaches (other than classification) to solve this kind of problems?",1,0
303,2016-6-9,2016,6,9,21,4nahg3,"A book or site where I can find applications, advantages and disadvantages of different machine learning algorithms?",https://www.reddit.com/r/MachineLearning/comments/4nahg3/a_book_or_site_where_i_can_find_applications/,[deleted],1465473886,[deleted],2,0
304,2016-6-9,2016,6,9,21,4namay,Finally someone did it: Video in Picasso style (Space Odyssey,https://www.reddit.com/r/MachineLearning/comments/4namay/finally_someone_did_it_video_in_picasso_style/,blazarious,1465476268,,1,0
305,2016-6-9,2016,6,9,22,4naozk,Kickoff.ai: Euro 2016 predictions using Bayesian inference,https://www.reddit.com/r/MachineLearning/comments/4naozk/kickoffai_euro_2016_predictions_using_bayesian/,victorkristof,1465477461,,1,1
306,2016-6-9,2016,6,9,22,4naqoo,Open source BI tool with integrated machine learning approach for generating reports.,https://www.reddit.com/r/MachineLearning/comments/4naqoo/open_source_bi_tool_with_integrated_machine/,abhi_kush,1465478220,,1,0
307,2016-6-9,2016,6,9,22,4nasa9,"If I input an image, how can get a bounding box over the regions where a specific neuron is activated using keras or theano?",https://www.reddit.com/r/MachineLearning/comments/4nasa9/if_i_input_an_image_how_can_get_a_bounding_box/,cvikasreddy,1465478916,"I am using keras and I am able to get activations of specific neuron using the following code

get_3rd_layer_output = K.function([model.layers[0].input, K.learning_phase()], [model.layers[3].output])
layer_output = get_3rd_layer_output([X, 1])[0]

Is there a way to get a bounding box over the part of image where activation is the highest?

http://imgur.com/6F6sTZg&amp;HgW82Nh

As you can see, if I input the first image in album my 3rd layer neuron activates mostly for a tyre, so How can I get a bounding box over the tyre?",3,3
308,2016-6-9,2016,6,9,22,4navmg,Machine Learning and Creative AI at nucl.ai,https://www.reddit.com/r/MachineLearning/comments/4navmg/machine_learning_and_creative_ai_at_nuclai/,[deleted],1465480305,[deleted],0,0
309,2016-6-9,2016,6,9,23,4nb01j,scikit-learn tutorial series,https://www.reddit.com/r/MachineLearning/comments/4nb01j/scikitlearn_tutorial_series/,DATAh4ck3r,1465482063,,3,105
310,2016-6-10,2016,6,10,0,4nba55,Coursera Course - Parallel Programming (in Scala),https://www.reddit.com/r/MachineLearning/comments/4nba55/coursera_course_parallel_programming_in_scala/,abstractcontrol,1465485727,,6,0
311,2016-6-10,2016,6,10,0,4nba7j,Training(ML) Probability Distribution Functions(PDF's),https://www.reddit.com/r/MachineLearning/comments/4nba7j/trainingml_probability_distribution_functionspdfs/,gorhaped,1465485744,[removed],0,1
312,2016-6-10,2016,6,10,0,4nbc8r,The Role of Lubrication Pumps for Improving the Efficiency of Your Equipment,https://www.reddit.com/r/MachineLearning/comments/4nbc8r/the_role_of_lubrication_pumps_for_improving_the/,jackerfrinandis,1465486459,,0,1
313,2016-6-10,2016,6,10,0,4nbdvz,Color-independent style transfer,https://www.reddit.com/r/MachineLearning/comments/4nbdvz/colorindependent_style_transfer/,alexecker,1465487037,,17,51
314,2016-6-10,2016,6,10,0,4nbg1x,An Introduction to Edge Detection with Kernel Convolutions,https://www.reddit.com/r/MachineLearning/comments/4nbg1x/an_introduction_to_edge_detection_with_kernel/,AlanZucconi,1465487792,,3,16
315,2016-6-10,2016,6,10,1,4nbmih,"How Bing Predicts uses search, social, and other relevant data with machine learning to make intelligent predictions about sporting events, reality TV shows, award shows, and political elections",https://www.reddit.com/r/MachineLearning/comments/4nbmih/how_bing_predicts_uses_search_social_and_other/,jennifermarsman,1465490090,,0,0
316,2016-6-10,2016,6,10,1,4nbon2,Machine Learning and Creative AI at nucl.ai,https://www.reddit.com/r/MachineLearning/comments/4nbon2/machine_learning_and_creative_ai_at_nuclai/,alexjc,1465490842,,1,7
317,2016-6-10,2016,6,10,1,4nbpv7,[1606.02147] ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation,https://www.reddit.com/r/MachineLearning/comments/4nbpv7/160602147_enet_a_deep_neural_network_architecture/,wesolyromek,1465491256,,5,11
318,2016-6-10,2016,6,10,3,4nc5hq,MLDB: the open-source Machine Learning Database,https://www.reddit.com/r/MachineLearning/comments/4nc5hq/mldb_the_opensource_machine_learning_database/,ddcarnage,1465496606,,0,22
319,2016-6-10,2016,6,10,3,4ncazn,DL4clj: An idiomatic Clojure layer on top of Deeplearning4j,https://www.reddit.com/r/MachineLearning/comments/4ncazn/dl4clj_an_idiomatic_clojure_layer_on_top_of/,vonnik,1465498445,,0,1
320,2016-6-10,2016,6,10,3,4ncb01,[1606.02355] Active Long Term Memory Networks,https://www.reddit.com/r/MachineLearning/comments/4ncb01/160602355_active_long_term_memory_networks/,[deleted],1465498447,[deleted],0,1
321,2016-6-10,2016,6,10,3,4ncbuu,"&lt;a href =""https://github.com/KnHuq/Dynamic_RNN_Tensorflow""&gt;Simple Dynamic Vanilla RNN and GRU with Tensorflow using scan and map ops.",https://www.reddit.com/r/MachineLearning/comments/4ncbuu/a_href_httpsgithubcomknhuqdynamic_rnn/,[deleted],1465498755,[deleted],0,0
322,2016-6-10,2016,6,10,4,4nccq3,[1606.02355] Active Long Term Memory Networks,https://www.reddit.com/r/MachineLearning/comments/4nccq3/160602355_active_long_term_memory_networks/,[deleted],1465499050,[deleted],0,1
323,2016-6-10,2016,6,10,4,4nce4b,Simple Dynamic RNN and GRU with example in Tensorflow using scan and map ops,https://www.reddit.com/r/MachineLearning/comments/4nce4b/simple_dynamic_rnn_and_gru_with_example_in/,[deleted],1465499556,[deleted],10,3
324,2016-6-10,2016,6,10,4,4ncfg3,What did machine learning scientist wife say during sex?,https://www.reddit.com/r/MachineLearning/comments/4ncfg3/what_did_machine_learning_scientist_wife_say/,scientific_prodigy12,1465500019,Deeper hehehe,0,0
325,2016-6-10,2016,6,10,4,4ncjcz,Beginner Python Based ML Research Project Suggestions?,https://www.reddit.com/r/MachineLearning/comments/4ncjcz/beginner_python_based_ml_research_project/,dlarsen5,1465501346,"I have just completed Andrew Ng's course on machine learning and have read a few textbooks on the subject, I was wondering what are some good projects to start practicing my skills on using python. Such as what datasets are best or if there are any full python based walk-throughs of projects, any help would be awesome",4,0
326,2016-6-10,2016,6,10,5,4ncp2c,Question concerning CPU,https://www.reddit.com/r/MachineLearning/comments/4ncp2c/question_concerning_cpu/,data_sagan,1465503331,"I want as a project build a model from a paper I read where they say ""We used a CPU cluster of 16 nodes each with 8 cores and 8   16 GB of memory"". Can anybody ELI5 how much this is, can I run this on my macbook with 16 GB RAM or not? How much to big is it if I can't run it?",5,0
327,2016-6-10,2016,6,10,6,4ncxkh,Google DeepMind AI learns to play 'Montezuma's Revenge',https://www.reddit.com/r/MachineLearning/comments/4ncxkh/google_deepmind_ai_learns_to_play_montezumas/,[deleted],1465506365,[deleted],0,1
328,2016-6-10,2016,6,10,6,4ncxr2,"With the coming of RL optimizer, shall we start a life long universal optimizer learner?",https://www.reddit.com/r/MachineLearning/comments/4ncxr2/with_the_coming_of_rl_optimizer_shall_we_start_a/,ijenab,1465506431,"Hi Reddit! I came across this interesting paper: http://arxiv.org/pdf/1606.01885v1.pdf with the title Learning to Optimize that came out recently. They basically thought of optimization as a reinforcement learning problem. As you may have guessed, they have used an NN to learn policy and their optimizer achieved better accuracy especially when applied to non-convex optimization problems. I personally think this is the research direction to follow since finding an engineered rule that would work well considering global optimization plane rather than local information is very hard, but in RL framework given the right NN architecture which exploits lots of biases in the space we may be able to outperform current state of the art by large margin. I'm curious to know what you think.... please share your beautiful thoughts.... Also, think about the case where the RL optimizer uses itself to update its own parameters to be a better optimizer. Somebody call Jurgen ;)",3,2
329,2016-6-10,2016,6,10,6,4nd1fz,"The False Emptiness, or Why Human Brain Can Be Compared to a Computer",https://www.reddit.com/r/MachineLearning/comments/4nd1fz/the_false_emptiness_or_why_human_brain_can_be/,[deleted],1465507730,[deleted],0,0
330,2016-6-10,2016,6,10,8,4ndinx,Simple Dynamic Basic LSTM with Tensorflow scan and Map ops,https://www.reddit.com/r/MachineLearning/comments/4ndinx/simple_dynamic_basic_lstm_with_tensorflow_scan/,kazi_shezan,1465514278,,0,0
331,2016-6-10,2016,6,10,9,4ndtkc,Step-by-Step Deep Learning Tutorial With Structured Data in Keras,https://www.reddit.com/r/MachineLearning/comments/4ndtkc/stepbystep_deep_learning_tutorial_with_structured/,jasonb,1465518644,,1,12
332,2016-6-10,2016,6,10,9,4ndu2x,2D Perceptron Training Visualiaser,https://www.reddit.com/r/MachineLearning/comments/4ndu2x/2d_perceptron_training_visualiaser/,[deleted],1465518860,[deleted],0,0
333,2016-6-10,2016,6,10,11,4ne6fg,High-level Built-in Learn Module in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4ne6fg/highlevel_builtin_learn_module_in_tensorflow/,terrytangyuan,1465524175,,2,2
334,2016-6-10,2016,6,10,11,4ne7pe,NLP: Generate difference of 2 text A-B,https://www.reddit.com/r/MachineLearning/comments/4ne7pe/nlp_generate_difference_of_2_text_ab/,brookm291,1465524728,"I have 2 documents A-B (or 2 series of documents), and would like to get the a new document showing difference between the two document: A-B

By difference, there are several definitions, one is : List of words/""concept"" include in A but not in B.

I am thinking of using TF IDF for each sentence of A and B , such as :

from sklearn.feature_extraction.text import TfidfVectorizer
d1 = [open(f1) for f1 in text_files]
tfidf = TfidfVectorizer().fit_transform(d1)
pairwise_similarity = tfidf * tfidf.T

I am not sure if this would be relevant to generate a new document C= ""A-B"", especially am interested in ""semantic difference"" in the document C, or at least in word difference in C.

",1,1
335,2016-6-10,2016,6,10,11,4ne8it,How to decide what neural network architecture to use?,https://www.reddit.com/r/MachineLearning/comments/4ne8it/how_to_decide_what_neural_network_architecture_to/,Weriak,1465525100,"l was just viewing andrew ng's coursera machine learning course and [there's a small part](https://i.gyazo.com/b51af4b5ff9a66a128e5669aa5f67068.png) in which he talks about what neural network architecture to use.

The problem comes when he talks about the hidden layers. He basically says that the more hidden layers the better(at the price of being more 'expensive' to compute) and that the amount of neurons in each layer should be a comparable amount of the number of initial inputs or greater. 

But this explanation seems kind of vague/random, there is an infinite amount of combinations you can choose from: You just go trying one by one until one architecture seems to work? 

For example, what architecture would you use to make a program that distinguishes numbers from 1 to 10, say, on a 50x50 pixel window? How would you come up with that?
",4,5
336,2016-6-10,2016,6,10,12,4neh9v,Training CNN on a subset of imagenet,https://www.reddit.com/r/MachineLearning/comments/4neh9v/training_cnn_on_a_subset_of_imagenet/,vighneshbirodkar,1465529005,"Hello

I'm trying to train a network to predict 20 classes from imagenet. There are approximately 20k labelled images. Can I expect any of the recent state of the art imagenet CNNs to work well with this data ? So far my code with Alexnet achieves 40% validation accuracy and 95% training accuracy. I'm guessing it's overfitting as there are too few images ?

How should I change my network? Less layers ? Resize images to smaller size and feed it to a network with fewer parameters?

Currently images are resized to 256x256",4,1
337,2016-6-10,2016,6,10,12,4nelsa,Emoji &amp; Deep Learning   ,https://www.reddit.com/r/MachineLearning/comments/4nelsa/emoji_deep_learning/,iamkeyur,1465531142,,36,148
338,2016-6-10,2016,6,10,14,4nexcc,NLP using TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4nexcc/nlp_using_tensorflow/,Vibhu27,1465537130,[removed],0,1
339,2016-6-10,2016,6,10,14,4nez2h,Google's Deep Mind Explained! - Self Learning A.I.,https://www.reddit.com/r/MachineLearning/comments/4nez2h/googles_deep_mind_explained_self_learning_ai/,spacecyborg,1465538141,,0,0
340,2016-6-10,2016,6,10,15,4nf0pb,[1606.02228] Systematic evaluation of CNN advances on the ImageNet,https://www.reddit.com/r/MachineLearning/comments/4nf0pb/160602228_systematic_evaluation_of_cnn_advances/,alexjc,1465539079,,12,8
341,2016-6-10,2016,6,10,15,4nf35q,[Video] Machine Learning Recipes with Josh Gordon,https://www.reddit.com/r/MachineLearning/comments/4nf35q/video_machine_learning_recipes_with_josh_gordon/,rubyantix,1465540552,,0,3
342,2016-6-10,2016,6,10,17,4nfdu3,Euro-Oracle: Using Python+Random Forests to predict soccer matches,https://www.reddit.com/r/MachineLearning/comments/4nfdu3/eurooracle_using_pythonrandom_forests_to_predict/,euro-oracle,1465547446,"Hey all!

I would like to present a project that was developed during the last few weeks.
It's the so-called Euro-Oracle http://www.euro2016-oracle.com/  which, based on FIFA-rankings and and a couple hundred matches played during the World Cup and qualification for the Euro, is supposed to predict future matches.
In tests this random forests-based model performed significantly better than random predictions, with an accuracy high enough to have a positive return on investment if we betted money (before you do it, please wait a week until this model adapted to the current teams' performances).

A further Monte-Carlo-analysis named Germany the most likely tournament winner!

PS: Features were FIFA score, FIFA score differences, one-hot-encoded team IDs, with ""1""/""-1"" indicating the first/second team and another integer -1,0,1 indicating which team possesses home advantage.

Let's see what happens! I expect the accuracy to be at around 50-60%, far better than the expected 33%!
",2,1
343,2016-6-10,2016,6,10,17,4nffo2,"Used, Online Industrial Machinery Auctions",https://www.reddit.com/r/MachineLearning/comments/4nffo2/used_online_industrial_machinery_auctions/,jamescock125,1465548727,,0,1
344,2016-6-10,2016,6,10,17,4nfful,A possibly fast method for real time recurrent learning?,https://www.reddit.com/r/MachineLearning/comments/4nfful/a_possibly_fast_method_for_real_time_recurrent/,kh40tika,1465548841,,10,2
345,2016-6-10,2016,6,10,19,4nfmsu,"""Python Machine Learning"" book code repository and info resource",https://www.reddit.com/r/MachineLearning/comments/4nfmsu/python_machine_learning_book_code_repository_and/,pmigdal,1465553372,,0,20
346,2016-6-10,2016,6,10,19,4nfo7x,Is my model overfitting? (graphs inside),https://www.reddit.com/r/MachineLearning/comments/4nfo7x/is_my_model_overfitting_graphs_inside/,Atrix621,1465554235,"In all cases of overfitting I have read about, the model begins to overfit when the validation curve reaches a local mininum.  However, what do you make of the following case: the validation curve flatlines but the validation precision and accuracy continue to increase? See graphs below.

http://postimg.org/image/y7rp5j4qj/",12,4
347,2016-6-10,2016,6,10,19,4nfp1o,Extracting specific information with natural language processing?,https://www.reddit.com/r/MachineLearning/comments/4nfp1o/extracting_specific_information_with_natural/,elemur,1465554744,"I was looking for pointers on how to better extract information elements from textual content. I have text provided to me and need to recognize and extract specific elements no matter where they are located in the sentence structure. So to oversimplify, ""I am interested in cookies and I have 2 dogs."" Would let me find the cookie interest specifically and the count of pets. Not the best example but gets the basic idea. I am familiar with General sentiment analysis and theme sorts of things (question on weather) but am not sure on the best route to find some of these very specific elements..",2,0
348,2016-6-10,2016,6,10,20,4nft7w,Books explaining Collapsed Gibbs?,https://www.reddit.com/r/MachineLearning/comments/4nft7w/books_explaining_collapsed_gibbs/,[deleted],1465557192,[deleted],2,1
349,2016-6-10,2016,6,10,21,4ng3ge,Online methods for short term prediction,https://www.reddit.com/r/MachineLearning/comments/4ng3ge/online_methods_for_short_term_prediction/,kirin1987,1465562481,"A ml beginner here, so please bear with me. If I understand correctly RNNs seem to be the go to method right now for sequence prediction for a given input (single/as a sequence). But I do not have sufficient data to train a RNN. I have discounted Markov decision process based mechanisms for the same reason. Are there any online learning algos that I can use to get coarse/approximate predicted sequences with only a small training set? 
I have looked at Q-learning but it seems to be ideal for best path problems where the end goal is definite.
Any pointers would be greatly appreciated.",1,2
350,2016-6-10,2016,6,10,22,4ng8ga,HTM School Episode 5: Scalar Encoding,https://www.reddit.com/r/MachineLearning/comments/4ng8ga/htm_school_episode_5_scalar_encoding/,numenta,1465564704,,1,0
351,2016-6-10,2016,6,10,22,4ng95h,Methods to Clip Complex Gradients,https://www.reddit.com/r/MachineLearning/comments/4ng95h/methods_to_clip_complex_gradients/,LeavesBreathe,1465564995,"Hey Guys,

Lately have been working with unitary and associative LSTM's, both of which deal with complex numbers, and therefore, complex gradients. 

In both papers, they do not clip their gradients. However, because we are working with RNN's in this case, I thought it would be wise to. I understand that this is a debatable statement.

One way to clip the complex gradient would be to simply clip the real part by 5.0 and the imaginary part by 5.0j. However, I feel that this would be detrimental to optimization process because you would significantly alter the phase of the complex number. 

A different way would be to take the magnitude (absolute value) of the complex gradient. If somehow you can clip by the absolute value, then you wouldn't change the phase. Are there any known approaches to this? I might be missing something simple.",7,0
352,2016-6-10,2016,6,10,22,4ngc5x,Help needed opening a database,https://www.reddit.com/r/MachineLearning/comments/4ngc5x/help_needed_opening_a_database/,data_sagan,1465566126,[removed],0,1
353,2016-6-10,2016,6,10,22,4ngcp0,[Q] Temporal Difference Learning in POMDP's,https://www.reddit.com/r/MachineLearning/comments/4ngcp0/q_temporal_difference_learning_in_pomdps/,TamisAchilles,1465566331,"I'm interested in learning the value function under the current policy through experience for a POMDP in a environment with a continuous state and observation space. The environment is partially observable and will never be fully observable, due to a lack of information. 

Does anyone know of any models suitable for learning such a value function?",2,0
354,2016-6-10,2016,6,10,22,4ngepa,What are they listening for? A look into Facebook's AI Engine,https://www.reddit.com/r/MachineLearning/comments/4ngepa/what_are_they_listening_for_a_look_into_facebooks/,wildcodegowrong,1465567156,,0,0
355,2016-6-10,2016,6,10,23,4ngfwb,How to deep copy a lua table having variables of type userdata ?,https://www.reddit.com/r/MachineLearning/comments/4ngfwb/how_to_deep_copy_a_lua_table_having_variables_of/,already_taken_m17,1465567590,[removed],0,1
356,2016-6-10,2016,6,10,23,4nggc7,"Transfer learning: Will there be any value of using a NN pretrained on imagenet, in a medical xray application?",https://www.reddit.com/r/MachineLearning/comments/4nggc7/transfer_learning_will_there_be_any_value_of/,datasciguy-aaay,1465567759,"How good will be results when a generic pretrained NN lower layer is used as the base layer for an application that analyzes images from a completely different domain such as medical x-rays, or astronomy?  


Is there still some beneficial contribution at all, to the prediction quality of the end application in some new image domain, when using a pretrained network built from a completely different image domain?

It seems like images would always share at least SOME micro-things like edges, but just not the macro-things like cats and people faces.  The macro-things are domain-specific but the micro-things are more generic and useful for everyone who does image processing, it seems.

Is that correct?",2,0
357,2016-6-10,2016,6,10,23,4nglsm,Evolving Neural Toplogies,https://www.reddit.com/r/MachineLearning/comments/4nglsm/evolving_neural_toplogies/,alephnaught90,1465569730,"What is the best model to use for playing around with the evolution of neural network topologies? I expect building a neural network with matrices and dot products isn't going to be so useful.

Should I essentially build a directed graph, and recursively follow it to perform forward and backpropegation? That seems very slow and inefficient, and yet that's the best way I can see to keep the topology very mutable.",1,0
358,2016-6-10,2016,6,10,23,4ngm7n,"This Week in Machine Learning, 10 June 2016: identifying astroturfing, imaging black holes, TensorFlow on iOS, resolving the incarceration crisis, resolving the von Neumann bottleneck, and predicting the Olympics.",https://www.reddit.com/r/MachineLearning/comments/4ngm7n/this_week_in_machine_learning_10_june_2016/,DavidAJoyner,1465569886,,0,2
359,2016-6-10,2016,6,10,23,4ngmru,Movie written by algorithm turns out to be hilarious and intense,https://www.reddit.com/r/MachineLearning/comments/4ngmru/movie_written_by_algorithm_turns_out_to_be/,julian88888888,1465570088,,67,232
360,2016-6-11,2016,6,11,0,4ngxgd,How to visualize the filters of the final layer of a pretrained network in TensorFlow?,https://www.reddit.com/r/MachineLearning/comments/4ngxgd/how_to_visualize_the_filters_of_the_final_layer/,nomology,1465573775,"For a medical application I am retraining the pretrained Inception-v3 network using TensorFlow.

This network has a final layer:

    pool_3:0 (2048 features)


Using TF's classify_image, I figured out which of these features are most important for each sample. So there is an array with the indexes of the top-N features, sorted on weights.

The next step is to visualize the feature vector to better understand the results.

How would I go about doing this? Is TensorBoard capable of this? I am at a bit of a loss. Any suggestion/help is appreciated!",0,0
361,2016-6-11,2016,6,11,1,4nh08v,Logistic Regression categorical data issues,https://www.reddit.com/r/MachineLearning/comments/4nh08v/logistic_regression_categorical_data_issues/,ohanlom4,1465574770,"I have created a model in R using data with a lot of categorical data and it works well enough (70% classification rate). I need to transfer the code to python for launching to production, however when i transfer the code the results decrease considerably (40% classification rate). I think it may be to do with how I encode the categorical data, any ideas?",1,0
362,2016-6-11,2016,6,11,1,4nh3ns,Can we get better prediction results by appending two pretrained image networks?,https://www.reddit.com/r/MachineLearning/comments/4nh3ns/can_we_get_better_prediction_results_by_appending/,datasciguy-aaay,1465575942,"Quote from ModelZoo github page (link below):  16-layer: 7.5% top-5 error on ILSVRC-2012-val, 7.4% top-5 error on ILSVRC-2012-test. 19-layer: 7.5% top-5 error on ILSVRC-2012-val, 7.3% top-5 error on ILSVRC-2012-test. In the paper, the models are denoted as configurations D and E, trained with scale jittering. The combination of the two models achieves 7.1% top-5 error on ILSVRC-2012-val, and 7.0% top-5 error on ILSVRC-2012-test.


The ModelZoo site as shown above claims that combining two pre-trained NN yields a better 7.1% error while each independently provided a less good 7.5% error. This is great news because it supports the idea that stitching together 2 pretrained datasets yields an improvement.

I wonder how low an error you can get by doing more of these stitching together two at a time (like a reduce operation). ModelZoo's claim for 7.1% is unsourced though, seemingly made up out of thin air by the ModelZoo owners.

In any case, I do expect it works even if they don't show us the supporting code and data.

My question is then, what is stopping us from appending together a lot more of the pretrained models, every pretrained model that we can find for image processing, and getting an even bigger and better one?


https://github.com/BVLC/caffe/wiki/Model-Zoo ",3,0
363,2016-6-11,2016,6,11,1,4nh5iw,"Using H2O, how can we share pretrained NN weights with each other? The checkpoint is a start, but it might have too much irrelevant other data in it.",https://www.reddit.com/r/MachineLearning/comments/4nh5iw/using_h2o_how_can_we_share_pretrained_nn_weights/,datasciguy-aaay,1465576542,"Using the H2O package:  

Can pretrained NN data be shared in different applications by different people across the internet as long as everyone uses a single training package like H2O? 

Notice that H2O has so-called checkpoints.

It seems H2O checkpoints would contain NN data that would be sharing-worthy for use in new and different models.

I would want a copy of the NN weights.  I would not want their hyperparameters, learning rate, regularization parameter, etc. 

To share pretrained NN stubs, we should start with checkpoints, but strip out the hyperparameters, learning rate, regularization parameter, etc. We should only share the weights matrices.

Does this sound right?",1,0
364,2016-6-11,2016,6,11,1,4nh7cj,F# Not just for finance,https://www.reddit.com/r/MachineLearning/comments/4nh7cj/f_not_just_for_finance/,iamkeyur,1465577174,,0,0
365,2016-6-11,2016,6,11,1,4nh7oa,How can H2O users use the pretrained NN models for Caffe that are being traded at the ModelZoo?,https://www.reddit.com/r/MachineLearning/comments/4nh7oa/how_can_h2o_users_use_the_pretrained_nn_models/,datasciguy-aaay,1465577280,"Can pretrained data be shared by users, either directly in some common compatible file format, or by automated transformation to other file formats, by different people who use different brands of training package like H2O, Neon, Theano, Caffe, et al ? 

We all seem to be interested in essentially rebuilding the same wheel over again for different packages.  

If someone spends a week or two making a really nice set of weights using ImageNet with Caffe, then why can't we also borrow those same network weights and put them into our vision applications we are writing in H2O, Neon, and Theano?  

NN weights are just a collection of matrices. 

Surely we can come up with a compatible matrix format for neural nets to share these matrix values across different packages.  Most of these packages are open source and therefore there is little reason nor capability to try to make incompatible matrix formats on purpose for business competitive reasons.
",2,0
366,2016-6-11,2016,6,11,2,4nhawz,"""Systematic evaluation of CNN advances on the ImageNet""",https://www.reddit.com/r/MachineLearning/comments/4nhawz/systematic_evaluation_of_cnn_advances_on_the/,[deleted],1465578376,[deleted],2,0
367,2016-6-11,2016,6,11,2,4nhbtm,Learn Basic Dynamic 2 layer Stacked LSTM With tensorflow to recognize handwritten Digits.,https://www.reddit.com/r/MachineLearning/comments/4nhbtm/learn_basic_dynamic_2_layer_stacked_lstm_with/,kazi_shezan,1465578674,,0,0
368,2016-6-11,2016,6,11,2,4nhikt,Are there any open-source implementations of a CNN based face detector?,https://www.reddit.com/r/MachineLearning/comments/4nhikt/are_there_any_opensource_implementations_of_a_cnn/,PicopicoEMD,1465580914,"NOTE: Face detection != Face recognition

Some guy implemented [this paper](https://arxiv.org/ftp/arxiv/papers/1508/1508.01292.pdf) and got 4K face detection running at 28fps right [here](https://github.com/Bkmz21/FD-Evaluation). Unfortunately he didn't make his code public. I'm not experienced at all on training neural networks, so I'd like to find an existing implementation.

Has anybody done something similar? It seems to be so ridiculously fast compared to other methods such as Viola-Jones.",6,0
369,2016-6-11,2016,6,11,2,4nhk0a,Help understanding SVMs with computer vision as applied to super-resolution microscopy data.,https://www.reddit.com/r/MachineLearning/comments/4nhk0a/help_understanding_svms_with_computer_vision_as/,whiteknight521,1465581393,"I understand the principle behind SVMs and kernal functions in academic and basic examples, but I do not understand how you take something like a 3D super-resolution microscopy data set with specific labels that exists in 3D Cartesian space and map it into higher-order feature space for use with an SVM (or multiple SVMs). I know that the answer to this is applying a kernel function, but I don't really know what sort of pixel transformations based in actual reality have to be done to the image to get to feature space. Can anyone help me to bridge the gap between the SVM theory and what actually happens to the pixels in the starting image?",3,1
370,2016-6-11,2016,6,11,3,4nhkm5,[1606.03002] MuFuRU: The Multi-Function Recurrent Unit,https://www.reddit.com/r/MachineLearning/comments/4nhkm5/160603002_mufuru_the_multifunction_recurrent_unit/,dunnowhattoputhere,1465581606,,3,9
371,2016-6-11,2016,6,11,3,4nhqzq,"Like a Prayer by Madonna, as heard by a computer",https://www.reddit.com/r/MachineLearning/comments/4nhqzq/like_a_prayer_by_madonna_as_heard_by_a_computer/,functime,1465583756,,0,0
372,2016-6-11,2016,6,11,3,4nhs67,How do you transfer huge datasets on EC2 instances?,https://www.reddit.com/r/MachineLearning/comments/4nhs67/how_do_you_transfer_huge_datasets_on_ec2_instances/,xristos_forokolomvos,1465584171,"If we're talking about lots of Gb of data, how do you transfer and what hosting do you use for huge, let's say image, datasets?

Also what happens when you shutdown the instance? Do you have to transfer it again?

I'm looking for the minimum cost solution as I am a poor student :)",8,3
373,2016-6-11,2016,6,11,4,4nhyx7,"DarkForest, the Facebook Go engine",https://www.reddit.com/r/MachineLearning/comments/4nhyx7/darkforest_the_facebook_go_engine/,gwulfs,1465586542,,1,21
374,2016-6-11,2016,6,11,4,4ni45t,"Great promotion on NVIDIA Jetson TX1, for Machine Learners !",https://www.reddit.com/r/MachineLearning/comments/4ni45t/great_promotion_on_nvidia_jetson_tx1_for_machine/,[deleted],1465588312,[deleted],0,0
375,2016-6-11,2016,6,11,5,4niaa1,imdb parental advisory written using a predictive text emulator,https://www.reddit.com/r/MachineLearning/comments/4niaa1/imdb_parental_advisory_written_using_a_predictive/,OurEngiFriend,1465590519,,10,46
376,2016-6-11,2016,6,11,6,4nim9v,Can I solve a 3 variable XOR problem with the Rosenblatt Perceptron?,https://www.reddit.com/r/MachineLearning/comments/4nim9v/can_i_solve_a_3_variable_xor_problem_with_the/,gabegabe6,1465594970,"**A** XOR **B** XOR **C**

If yes than how? If not then why?

IMO, we can't solve it with that because it's a non linear problem.
I would use RBF or MLP or SVM but I don't know about the parameters...",0,0
377,2016-6-11,2016,6,11,7,4nip4e,I need some large data sets for training. Where can I find some?,https://www.reddit.com/r/MachineLearning/comments/4nip4e/i_need_some_large_data_sets_for_training_where/,GotMiIk,1465596092,"I need some large data sets to train my algorithm. Preferably 2 variable sets, and quite large. Does anyone know where I can find such data sets? Thanks.",3,0
378,2016-6-11,2016,6,11,8,4nj2lx,Next step to become data scientist?,https://www.reddit.com/r/MachineLearning/comments/4nj2lx/next_step_to_become_data_scientist/,JohnTheDang,1465601640,"Hi!

I have recently acquired my master's degree in engineering physics, and I wish to work as a data scientist. I was introduced to the field of data science through my master's thesis where I used convolutional networks to classify medical images. Even though I feel quite comfortable with coding, I feel that I am still lacking experience in programming. I have not taken any machine learning classes, as my university didn't have any, though I've learned a lot on my own during my master's thesis.

So my question is, what should I do next if I want to get a job as a data scientist?

Thanks in advance!",11,0
379,2016-6-11,2016,6,11,8,4nj2sz,Apple's next big challenge: Making Siri smarter,https://www.reddit.com/r/MachineLearning/comments/4nj2sz/apples_next_big_challenge_making_siri_smarter/,smoothindeed,1465601709,,0,0
380,2016-6-11,2016,6,11,11,4njp2b,How to combine notion of 'Variable Importance' with correlated predictors in GBMs?,https://www.reddit.com/r/MachineLearning/comments/4njp2b/how_to_combine_notion_of_variable_importance_with/,kebabmybob,1465612334,"In a GBM, if two variables are highly predictive, the variable importance tends to get distorted and only one of the two or more variables will be considered ""important"". In an RF, they tend to split more evenly.

I am looking for some methods / research / etc. that will allow you to interpret GBMs by looking at variable importance alongside variables that are correlated to determine what the main drivers of the model are. For example, if the variable importance is saying that only 'X' is important, but 'X' is fairly correlated to 'Y', I want to be able to say both are important with some degree of confidence.

Is there anything cleaner or more sophisticated I can do besides calculating variable importance the usual way and then just reporting all the correlated variables alongside those?",0,0
381,2016-6-11,2016,6,11,12,4njts0,Is licensing/purchasing pre-trained classifiers common?,https://www.reddit.com/r/MachineLearning/comments/4njts0/is_licensingpurchasing_pretrained_classifiers/,[deleted],1465614677,[deleted],3,0
382,2016-6-11,2016,6,11,12,4njvdf,"If I am a google maps phone user, and google gathers traffic data based on my travel times, when will google send me down a (possibly) slower path to gather more data?",https://www.reddit.com/r/MachineLearning/comments/4njvdf/if_i_am_a_google_maps_phone_user_and_google/,bgard6977,1465615510,"In simultaneous location and mapping, during an exploration phase robots will explore the area of least knowledge balanced by the cost to travel to that location. Since Google maps is a way finding algorithm, and all it's traffic data comes from user travel times, it seems like Google couldn't or wouldn't put individual user's interests above the good of the whole.

Does anyone know how they balance such concerns?",2,0
383,2016-6-11,2016,6,11,13,4nk45n,Has anyone used a Mac GPU with Tensorflow?,https://www.reddit.com/r/MachineLearning/comments/4nk45n/has_anyone_used_a_mac_gpu_with_tensorflow/,jalligator,1465620092,"Has anyone used a Mac GPU with Tensorflow?  What kind of speed can one expect?  Are all Macbooks except for those with NVIDIA GPUs basically not any benefit because of CUDA, etc.?

Any general info about running TF on a Mac GPU is appreciated.

",6,0
384,2016-6-11,2016,6,11,15,4nkde9,Is Lua still a good choice for machine learning?,https://www.reddit.com/r/MachineLearning/comments/4nkde9/is_lua_still_a_good_choice_for_machine_learning/,[deleted],1465625587,[removed],0,1
385,2016-6-11,2016,6,11,15,4nkeg2,Is Lua still a good language for machine learning?,https://www.reddit.com/r/MachineLearning/comments/4nkeg2/is_lua_still_a_good_language_for_machine_learning/,rolandjeff,1465626258,[removed],0,1
386,2016-6-11,2016,6,11,15,4nkg3h,Preprocessing steps for a string dataset?,https://www.reddit.com/r/MachineLearning/comments/4nkg3h/preprocessing_steps_for_a_string_dataset/,dozzinale,1465627373,"Hello there,

I'm making some analysis on the script of the TV series Breaking Bad. My dataset is composed by all of the english subtitles of every episode of every season. Now, I'm new of the field (although I'm CS student) and my question is: how do I preprocess this dataset?

Obviously I need to stem the words and some sort of these things, but I'm not really aware of the complete process. Can you help me?

Thank you!",6,0
387,2016-6-11,2016,6,11,17,4nkn7g,Practical Named Entity Recognition,https://www.reddit.com/r/MachineLearning/comments/4nkn7g/practical_named_entity_recognition/,arnold_b_arunhammer,1465633237,"I need to perform NER in a real world situation, where there is no pre-labeled data, and have some questions about the feasibility of doing so!

How do people go about creating a ""gold"" dataset? Are there tools that exist to perform manual tagging, or will I have to create one?

What dataformats are used to store this data? It looks like BIO encoding is the standard way - is there some easy way to store this in a relational database? It would be nice to be able to ""SELECT DISTINCT title FROM blog_text;"" etc.

Say I am trying to extract song titles from blog posts, how many titles / blog posts should I have in my ""gold"" dataset ?


Many thanks!",11,0
388,2016-6-11,2016,6,11,18,4nkrd0,Anyone know of a free dataset of spoken digits or characters?,https://www.reddit.com/r/MachineLearning/comments/4nkrd0/anyone_know_of_a_free_dataset_of_spoken_digits_or/,[deleted],1465636479,[deleted],1,0
389,2016-6-11,2016,6,11,20,4nl0yn,How to increase accuracy of results got using Tensorflow's retraining model?,https://www.reddit.com/r/MachineLearning/comments/4nl0yn/how_to_increase_accuracy_of_results_got_using/,mln00b13,1465643772,"I formed my data set, ran the Tensorflow retraining code, trained the model and got my result.

What next?
How do I increase it's accuracy further? Do I just keep giving it more training data?",1,0
390,2016-6-11,2016,6,11,23,4nliu2,Where do I start with learning machine learning math?,https://www.reddit.com/r/MachineLearning/comments/4nliu2/where_do_i_start_with_learning_machine_learning/,MorrisCasper,1465654183,"Hello,

I'm currently in high school, and am very interested in machine learning. A big obstruction is that our school hasn't covered things like matrices and partial derivatives yet. Does anyone know where I can learn the basics of the math needed for machine learning?

Thanks!

",59,121
391,2016-6-11,2016,6,11,23,4nlmdh,Build your own conversational bot,https://www.reddit.com/r/MachineLearning/comments/4nlmdh/build_your_own_conversational_bot/,01mawaliya,1465655778,,1,1
392,2016-6-12,2016,6,12,0,4nlxhh,"A primer on AI, Deep Learning and Machine Learning by a16z",https://www.reddit.com/r/MachineLearning/comments/4nlxhh/a_primer_on_ai_deep_learning_and_machine_learning/,francojs,1465660485,,0,0
393,2016-6-12,2016,6,12,1,4nm2x1,"This Week In Machine Learning &amp; AI - 6/10/16: Self-Motivated AI, Plus A Kill-Switch for Rogue Bots",https://www.reddit.com/r/MachineLearning/comments/4nm2x1/this_week_in_machine_learning_ai_61016/,sbc1906,1465662723,,0,0
394,2016-6-12,2016,6,12,3,4nmkei,What kind of NN would you use for predict given binary pictures from strings?,https://www.reddit.com/r/MachineLearning/comments/4nmkei/what_kind_of_nn_would_you_use_for_predict_given/,gabegabe6,1465669580,"For example I have 3 picture:
 Picture of... football,basketball,volleyball

And the input is a string: *I really love volleyball!*

Than the output would be the picture of a volleyball.

IMO:
I would use integers instead of picture matrices (1 for football, 2 for basketball, etc...)
And I would split the input to get the words and if I find a word what matches with the given ones than I can predict if it's 1,2 or 3. Than I can show the picture.
Maybe MLP? 

How would you do it?",9,0
395,2016-6-12,2016,6,12,3,4nmnf1,I am looking for Supervised Learning project to work upon?,https://www.reddit.com/r/MachineLearning/comments/4nmnf1/i_am_looking_for_supervised_learning_project_to/,mendax007,1465670744,"I am not sure on what to work upon. I am looking for you to suggest me any idea to work upon. That'll be of great help.
",8,0
396,2016-6-12,2016,6,12,4,4nmu1a,Preprocessing step for Text dataset?,https://www.reddit.com/r/MachineLearning/comments/4nmu1a/preprocessing_step_for_text_dataset/,codingml,1465673382,[removed],0,1
397,2016-6-12,2016,6,12,4,4nmykw,ICML 2016 top papers,https://www.reddit.com/r/MachineLearning/comments/4nmykw/icml_2016_top_papers/,pyrytakala,1465675165,What are the top ICML 2016 papers to read before the conference?,5,10
398,2016-6-12,2016,6,12,6,4nn9mm,Embedded artificial neural network synthesizer,https://www.reddit.com/r/MachineLearning/comments/4nn9mm/embedded_artificial_neural_network_synthesizer/,rageling,1465679402,,9,9
399,2016-6-12,2016,6,12,6,4nndxe,Supplement/replacement for ISLR textbook?,https://www.reddit.com/r/MachineLearning/comments/4nndxe/supplementreplacement_for_islr_textbook/,MidoriMind,1465681204,"Hi all,

Based on the numerous recommendations on here, I started working through ISLR (Introduction to Statistical Learning with Applications in R). I'm doing everything in Python though, since I'm hard-headed.

So far, I'm enjoying it enough. It's very well written, but I think I'm looking for something else (maybe I'm blinded by classical-AI, but right now all it feels like is ""data-science""). While ISLR seems good for a light statistical background of many important topics, there's little to no coverage of things such as neural nets, reinforcement learning, unsupervised learning, etc.

Are there any other textbooks which could be recommended, either as an alternative to ISLR, or to supplement it? I have a very strong math background, but I'd rather focus more on immediate applications, rather than getting bogged down with derivations.

Also, most of the examples and exercises seem to be a bit shallow so far, at least in terms of what you end up doing with the data. So bonus points to any resources with non-trivial applications.

Thanks",4,0
400,2016-6-12,2016,6,12,6,4nnfhs,Creating 3D Scenes From Text,https://www.reddit.com/r/MachineLearning/comments/4nnfhs/creating_3d_scenes_from_text/,[deleted],1465681844,[deleted],0,1
401,2016-6-12,2016,6,12,7,4nnna0,Understanding of Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4nnna0/understanding_of_convolutional_neural_networks/,Piranne,1465685171,"Hello my Machine Learning friends,

I am somewhat new to this deep learning thing. I fell under the spell when I realized that a computation of a layer of a multilayer perceptron (""vanilla neural network"") is simply a composition of a linear map and the the vectorized activation function.

Now, convolutional neural networks are seemingly something more complex. But then I realized, aren't convolutional neural networks just a special case of this ""vanilla neural network""? In convolutional neural networks, each new feature is computed out of just some strict subset of features from the previous layer. Only some features are ""selected"" to the weighted sum. But, if I am correct, this ""selection"" step can also be expressed as a linear transformation, making a convolutional neural network simply such multilayer perceptron where computation of each layer must be a composition of this ""selection"" linear transformation, some other linear transformation and then the vectorized activation function.

I would like to know if I am correct with this intuition and it would make happy if my suspicion would be confirmed, as I feel that this mode of thinking about neural networks is something extremely simple, yet powerful.",10,0
402,2016-6-12,2016,6,12,9,4no0nk,"Model evaluation, model selection, and algorithm selection in machine learning",https://www.reddit.com/r/MachineLearning/comments/4no0nk/model_evaluation_model_selection_and_algorithm/,[deleted],1465691078,[deleted],0,0
403,2016-6-12,2016,6,12,10,4no75y,Papers about using emotion in machine learning,https://www.reddit.com/r/MachineLearning/comments/4no75y/papers_about_using_emotion_in_machine_learning/,ImaginaryPerson23,1465694127,"I am completely new to reddit and to machine learning (so please be kind). My supervisor wants me to find papers about machine learning with personality/emotion. I am not looking for papers about algorithms that classify emotions or papers about AI convincing humans that the AI has emotions. I am actually looking for papers about algorithms themselves using emotion, for example, a neural network that can itself be happy or sad.

Does anyone know of such research?

I am sure you are wondering what I mean by personality/emotion, my answer is that anything that resembles human emotion would be fine. Not looking to create sentient/conscious AI or anything like that. If you are wondering why my supervisor wants such papers, I don't know either.",11,0
404,2016-6-12,2016,6,12,11,4nogf7,NP-hard and NP-Completeness?,https://www.reddit.com/r/MachineLearning/comments/4nogf7/nphard_and_npcompleteness/,Fender6969,1465698715,"Taking a Machine Learning for Data Science and Analytics course online and I am having trouble understanding those two topics and being able to differentiate between them. I understand that some of this may be considered ""Computer Science Theory."" Hopefully someone can still answer this question.",2,0
405,2016-6-12,2016,6,12,13,4novem,Choosing device in tensorflow,https://www.reddit.com/r/MachineLearning/comments/4novem/choosing_device_in_tensorflow/,timburg,1465706461,"I have multiple GPUs and would like to best utilize them. Is there a 'best practices' guide for choosing machines?

For example, if I am running a variational autoencoder, would I want to run the entire network on one graph, or could it be better to distribute across GPUs/CPU?",1,1
406,2016-6-12,2016,6,12,14,4noyum,"I know there are classifiers that look at sample's of people's writing and detect their personalities, or ideological biases. Can anyone direct me to one?",https://www.reddit.com/r/MachineLearning/comments/4noyum/i_know_there_are_classifiers_that_look_at_samples/,[deleted],1465708449,[deleted],0,1
407,2016-6-12,2016,6,12,14,4nozvg,I know there are classifiers that look at samples of people's writing and detect their personalities or ideological biases. Can anyone direct me to one?,https://www.reddit.com/r/MachineLearning/comments/4nozvg/i_know_there_are_classifiers_that_look_at_samples/,chaosmosis,1465709051,"I'd like to copy and paste some old school essays into a box, then have my word choice judged by the classifier against an already collected database. Are there any online, or would I have to build and collect everything myself? I'm not interested enough to make this a personal project, but would gladly try one of these out if any are currently publicly available.",2,0
408,2016-6-12,2016,6,12,16,4npcm8,Is there any Machine translation parallel corpora which includes document/paragraph boundary?,https://www.reddit.com/r/MachineLearning/comments/4npcm8/is_there_any_machine_translation_parallel_corpora/,gmkim90,1465717035,"I saw that the most of the database for MT is collected by crawling documents written by various languages. (i.e. europarl corpus, news commentary corpus) So I think document/paragraph boundary could be naturally available in corpora. But it is hard to find explicit boundary between documents (in single corpora). 

Is there any Machine translation parallel corpora which includes document/paragraph boundary? If so, how can I find the boundary?",1,5
409,2016-6-12,2016,6,12,16,4npdaf,[RNN] Torch multisequence needs help,https://www.reddit.com/r/MachineLearning/comments/4npdaf/rnn_torch_multisequence_needs_help/,kwlodarczyk,1465717426,"Hey guys, I am starting to learn RNN, and since I'm a learner by doing, I've tried some simple RNN with some sequences to learn. 

What I want to do is:

* train set with many sequences with different length
* simple RNN which takes as input *m* steps back in time (rho in code) and outputs next predicted value

What problems do I have?

1. I don't know if should I input *m* consecutive numbers through time steps or *m* time steps

2. I have a feeling that my code is not representing what I want it to represent, could you check it?

Here is gist of my simplified [code](https://gist.github.com/fifol/7072a593b6d83dd05b6f25537e5c4c55) with just what is important.
",2,3
410,2016-6-12,2016,6,12,17,4npfy0,WMT Testset for machine translation,https://www.reddit.com/r/MachineLearning/comments/4npfy0/wmt_testset_for_machine_translation/,alrojo,1465719318,"The wmt'14-16 [testsets](http://www.statmt.org/wmt15/translation-task.html) are formatted using the `.sgm` formatting - Standard Generalized Markup Language.

How do I format these testsets to a one-segment-per-line format?",2,0
411,2016-6-12,2016,6,12,20,4npvld,Applying DBSCAN to a huge GIS dataset.,https://www.reddit.com/r/MachineLearning/comments/4npvld/applying_dbscan_to_a_huge_gis_dataset/,siddkotwal,1465730927,"I have a training set  (2GB) that contains GIS trajectory data for multiple taxi rides. I want to cluster the final destinations based on their spatial density and have therefore been trying to use the DBSCAN algorithm with the distance metric as the Haversine formula. As a baseline I was able to use K-means with minibatches/online by reading chunks from my pandas dataframe but I've had no success with DBSCAN (lots of comparisons). 

I'm using scikit/python and have tried reading the csv into a Pandas dataframe and with GraphLab's SFrame. Any suggestions on how to do this with these tools? If not, what is the best way to apply clustering on such large datasets? I don't know how to apply ML algorithms directly to a database.

EDIT: So I tried to partition my space visibly and apply DBSCAN. I also found an interesting paper that suggests data partitioning to solve the memory problem. Totally depends upon the dataset but can be a workaround. I'll post if this thing works out. Would like to have your opinions about this too.
http://file.scirp.org/pdf/6-1.61.pdf",9,1
412,2016-6-12,2016,6,12,20,4npvsq,TensorFlow/Keras Error in feeding data.,https://www.reddit.com/r/MachineLearning/comments/4npvsq/tensorflowkeras_error_in_feeding_data/,subszero,1465731066,"Hi All, 
I am new to keras/tensorflow. I am having issues trying to feed data to a sparse tensor, getting the following error ""ValueError: setting an array element with a sequence.""

I understand that it is a numpy error, but I'm not really sure on how to resolve it.

A simplified version of the code can be found here : http://pastebin.com/jiUGTZ4z

Thanks so much for your help!",4,0
413,2016-6-12,2016,6,12,21,4npyoi,Information Theory for Machine Learning (for beginners) [includes EM Algorithm],https://www.reddit.com/r/MachineLearning/comments/4npyoi/information_theory_for_machine_learning_for/,Kiuhnm,1465732948,"I learned (basic) *Information Theory* in a very unsystematic way by picking up concepts here and there as I needed them. I decided it was high time I reorganized the knowledge in my head.

The result is [this paper about Information Theory](https://github.com/mtomassoli/papers/blob/master/inftheory.pdf) which I wrote both for myself and for others. Writing for others forces me to be as clear and readable as possible (and to add pictures!).

Even if you're not particularly interested in a tutorial about Information Theory, maybe you'll like the last two sections about the *EM Algorithm*. I tried to give a thorough and coherent presentation of it.

***Let me know if you find any mistakes and ask if anything isn't clear!***",25,180
414,2016-6-12,2016,6,12,21,4nq2f2,idea on black box optimization,https://www.reddit.com/r/MachineLearning/comments/4nq2f2/idea_on_black_box_optimization/,godspeed_china,1465735052,"suppose we want to maximize a black box function y=f(x), where x is a real valued vector and f is expensive to evaluate.  
my idea is to approximate f by a surrogate function y'=g(x,w), where g is a neural network (may be deep) and x is the input and w is the synapse weights.  
we first evalute f with some random xs. (1) we collect (y,x) pairs as training data. (2) we tune w to fit g to the trainning data. (3) we propose a new x' which maximizes g.  
we repeat (1),(2),(3) until some stop criterion is met.  
hope it useful ^_^",4,0
415,2016-6-12,2016,6,12,23,4nqk86,Machine that steams and folds laundry and other tech news,https://www.reddit.com/r/MachineLearning/comments/4nqk86/machine_that_steams_and_folds_laundry_and_other/,zee78,1465743422,,0,0
416,2016-6-13,2016,6,13,0,4nqszr,"Model evaluation, model selection, and algorithm selection in machine learning - Part I",https://www.reddit.com/r/MachineLearning/comments/4nqszr/model_evaluation_model_selection_and_algorithm/,benfred,1465746692,,0,14
417,2016-6-13,2016,6,13,1,4nqx5f,Occupancy Flow using Recurrent Flow Network,https://www.reddit.com/r/MachineLearning/comments/4nqx5f/occupancy_flow_using_recurrent_flow_network/,samchoi7,1465748187,"Robust Occupancy flow method using recurrent flow network implemented in MATLAB. 
Super robust to salt-and-pepper noise. 

code: https://github.com/sjchoi86/RecurrentFlowNet
video link: https://www.youtube.com/playlist?list=PLtWMojn4UVnyP3HTiRFBxGbZ6lShB16E7",0,0
418,2016-6-13,2016,6,13,1,4nr3bt,AI: The New Electricity,https://www.reddit.com/r/MachineLearning/comments/4nr3bt/ai_the_new_electricity/,tuan3w,1465750374,,4,4
419,2016-6-13,2016,6,13,3,4nriwe,NN to predict unlikely events,https://www.reddit.com/r/MachineLearning/comments/4nriwe/nn_to_predict_unlikely_events/,bastardOfYoung94,1465755847,"I'm trying to train a recurrent neural network, using Keras, to predict unlikely events (occurs in maybe 1-5% of samples) in a time series. 

However, I've realized that the model will always predict 0, since ~95% of the time that's correct. Does anyone have advice/suggestions on how to avoid this?",10,3
420,2016-6-13,2016,6,13,3,4nrjds,Kaixhin/dockerfiles is moving to NVIDIA Docker,https://www.reddit.com/r/MachineLearning/comments/4nrjds/kaixhindockerfiles_is_moving_to_nvidia_docker/,Kaixhin,1465755998,"My collection of (mainly deep learning) [Docker images](https://github.com/Kaixhin/dockerfiles) is moving from using `kaixhin/cuda` to `nvidia/cuda`. If you are using one of these, please be aware that you will now need to use the [`nvidia-docker`](https://github.com/NVIDIA/nvidia-docker/releases) binary instead of `docker`. This means that you do not need a precise driver version, and moving forward NVIDIA can provide full-time support for CUDA-based images.

After a [few issues](https://github.com/NVIDIA/nvidia-docker/issues?q=is%3Aissue+is%3Aclosed), I've decided that the project is stable enough to start the migration. It'll take me a while to test all the images, so as always please raise an (informative) issue if you encounter any problems.",0,10
421,2016-6-13,2016,6,13,4,4nrqkl,Implementing your own recommender systems in Python,https://www.reddit.com/r/MachineLearning/comments/4nrqkl/implementing_your_own_recommender_systems_in/,DrLegend,1465758553,,5,28
422,2016-6-13,2016,6,13,5,4ns5qk,New Tutorial video on Parsey Mcparseface,https://www.reddit.com/r/MachineLearning/comments/4ns5qk/new_tutorial_video_on_parsey_mcparseface/,[deleted],1465764194,[deleted],0,1
423,2016-6-13,2016,6,13,5,4ns5tz,New Tutorial Video on Parsey Mcparseface,https://www.reddit.com/r/MachineLearning/comments/4ns5tz/new_tutorial_video_on_parsey_mcparseface/,llSourcell,1465764224,,0,0
424,2016-6-13,2016,6,13,5,4ns5ut,New Tutorial Video on Parsey Mcparseface,https://www.reddit.com/r/MachineLearning/comments/4ns5ut/new_tutorial_video_on_parsey_mcparseface/,llSourcell,1465764234,,0,0
425,2016-6-13,2016,6,13,5,4ns5vs,New Tutorial Video on Parsey Mcparseface,https://www.reddit.com/r/MachineLearning/comments/4ns5vs/new_tutorial_video_on_parsey_mcparseface/,llSourcell,1465764243,,0,0
426,2016-6-13,2016,6,13,5,4ns5x2,New Tutorial Video on Parsey Mcparseface,https://www.reddit.com/r/MachineLearning/comments/4ns5x2/new_tutorial_video_on_parsey_mcparseface/,llSourcell,1465764257,,0,1
427,2016-6-13,2016,6,13,5,4ns61w,Tutorial Video on Parsey McParseface,https://www.reddit.com/r/MachineLearning/comments/4ns61w/tutorial_video_on_parsey_mcparseface/,llSourcell,1465764304,,0,1
428,2016-6-13,2016,6,13,5,4ns62z,New Tutorial Video on Parsey McParseface,https://www.reddit.com/r/MachineLearning/comments/4ns62z/new_tutorial_video_on_parsey_mcparseface/,[deleted],1465764312,[deleted],0,1
429,2016-6-13,2016,6,13,5,4ns6ep,Build an AI Reader with Parsey Mcparseface,https://www.reddit.com/r/MachineLearning/comments/4ns6ep/build_an_ai_reader_with_parsey_mcparseface/,[deleted],1465764424,[deleted],0,1
430,2016-6-13,2016,6,13,5,4ns6pj,Build an AI Reader w/ Parsey Mcparseface,https://www.reddit.com/r/MachineLearning/comments/4ns6pj/build_an_ai_reader_w_parsey_mcparseface/,[deleted],1465764521,[deleted],0,1
431,2016-6-13,2016,6,13,5,4ns7ak,Build an AI Reader w/ Parsey Mcparseface,https://www.reddit.com/r/MachineLearning/comments/4ns7ak/build_an_ai_reader_w_parsey_mcparseface/,[deleted],1465764709,[deleted],0,1
432,2016-6-13,2016,6,13,5,4ns7cw,Build an AI Reader w/ Parsey Mcparseface,https://www.reddit.com/r/MachineLearning/comments/4ns7cw/build_an_ai_reader_w_parsey_mcparseface/,[deleted],1465764730,[deleted],0,1
433,2016-6-13,2016,6,13,5,4ns7hi,Build an AI Reader w/ Parsey Mcparseface,https://www.reddit.com/r/MachineLearning/comments/4ns7hi/build_an_ai_reader_w_parsey_mcparseface/,[deleted],1465764770,[deleted],0,1
434,2016-6-13,2016,6,13,5,4ns8cs,Machine learning Ph.d?,https://www.reddit.com/r/MachineLearning/comments/4ns8cs/machine_learning_phd/,jennakwon06,1465765096,"Hello Reddit community! I am a computer science undergrad planning on applying for Ph.D. programs in CS with a specialization in machine learning. I realize that landing a job in academia can be extremely difficult, so I am following the industry very closely with the awareness that I may be seeking a job as an ML engineer in 6-7 years. My concern is the exponential growth and attention ML / DS fields are experiencing. I am unsure if Ph. D degrees will be devalued in the industry by the time I complete my Ph.D if ML systems become ubiquitous. If my end goal is not to necessarily end up in academia, will MScs suffice? Although I love to research, I hope to make a smart decision before I make a 5-6 year commitment. Thank you for all your help!",11,8
435,2016-6-13,2016,6,13,6,4ns9i7,Build an AI Reader with Parsey Mcparseface,https://www.reddit.com/r/MachineLearning/comments/4ns9i7/build_an_ai_reader_with_parsey_mcparseface/,llSourcell,1465765500,,3,0
436,2016-6-13,2016,6,13,6,4nsak8,[1511.06856] Data-dependent Initializations of Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4nsak8/151106856_datadependent_initializations_of/,downtownslim,1465765898,,0,12
437,2016-6-13,2016,6,13,6,4nse33,Build an AI Reader,https://www.reddit.com/r/MachineLearning/comments/4nse33/build_an_ai_reader/,fdelrio89,1465767160,,0,1
438,2016-6-13,2016,6,13,7,4nsjna,Anomaly Detection,https://www.reddit.com/r/MachineLearning/comments/4nsjna/anomaly_detection/,mrg3_2013,1465769304,"Can any ML expects point me to a resource to do realtime anomaly detection that can handle seasonality ? For example, consider the use case of posting in /r/pics. The traffic has a typical intraday trend. I am looking for ways to detect anomalies that can take into account intraday trends.",6,4
439,2016-6-13,2016,6,13,11,4ntium,What are the most useful workshops @ ICML 2016 for an academia outsider to attend?,https://www.reddit.com/r/MachineLearning/comments/4ntium/what_are_the_most_useful_workshops_icml_2016_for/,mikaza,1465783757,I'm going to attend ICML 2016 and it's possible to select only 4 of them there (http://icml.cc/2016/?page_id=1519). What would you recommend to visit?,1,1
440,2016-6-13,2016,6,13,14,4nubhu,Optimizing a function,https://www.reddit.com/r/MachineLearning/comments/4nubhu/optimizing_a_function/,IdiotCharizard,1465797409,"I want to find a local maximum of a k-parameter black-box function (k  25) which is very heavy to compute(consider everything else trivial).

I'm working on a method using gradient ascent and the assumption that the parameters are independent.  This would work if I could get a decent estimate of the derivative of the function, which is where I'm stuck.

Can you suggest some methods which can accurately estimate the derivative of the function.

I'm currently looking at the [quasi-newton method](https://en.wikipedia.org/wiki/Quasi-Newton_method) and it shows promise, but I'd like to know if there's another standard way to do this.

Edit: I should have added more emphasis on the fact that evaluating the function is *very* computationally heavy.",23,4
441,2016-6-13,2016,6,13,16,4nul9j,gtx1080 vs 1070 for machine learning?,https://www.reddit.com/r/MachineLearning/comments/4nul9j/gtx1080_vs_1070_for_machine_learning/,dharma-1,1465803152,"Now that both are out, whats the difference for machine learning tasks?
",46,50
442,2016-6-13,2016,6,13,18,4nuttw,"Why RL does not work well in ""realistic environments""?",https://www.reddit.com/r/MachineLearning/comments/4nuttw/why_rl_does_not_work_well_in_realistic/,[deleted],1465809121,[deleted],12,6
443,2016-6-13,2016,6,13,19,4nv0e8,What areas of maths are required for machine learning?,https://www.reddit.com/r/MachineLearning/comments/4nv0e8/what_areas_of_maths_are_required_for_machine/,sachal10,1465813415,"I am beginner in machine learning , i need a little help to get started with this. Search has shown me that you need to be good in some areas of maths to understand it easily. And this is have come up with:

1. Linear Algebra 
2. Calculs (Single Variable and Multi Variable)
3. Stats

i want to know if i am on the right track.
",6,1
444,2016-6-13,2016,6,13,19,4nv380,ML Anomaly Detection with evolving features,https://www.reddit.com/r/MachineLearning/comments/4nv380/ml_anomaly_detection_with_evolving_features/,teopera,1465815212,"I'm working on an Anomaly Detection system that is working quite well on feature that are constant in time.
I would like to add some level of complexity to it and start working on features that are evolving in time, but I'm a little bit stuck in identifying how to identify a trend in the data set and how to re-evaluate past data based on the evidence that the system is evolving to a different position.

I'm trying to find some documentation about this topic, any suggestion?",0,2
445,2016-6-13,2016,6,13,20,4nv5f3,Best practices for conducting long experiments?,https://www.reddit.com/r/MachineLearning/comments/4nv5f3/best_practices_for_conducting_long_experiments/,fairBear45,1465816550,"Hello ML community!
Newbie here.

So I have worked with small datasets like cifar10, mnist etc where the results can be obtained very fairly quicky.

Currently, I am working with large scale video data and usually I have to wait for a few days at the least to see if I my network has learnt anything. I have access to a good GPU so a slow GPU is not the bottleneck, the dataset is.

Most of the times, some ideas that I implement don't give any results at all and that results in 3-4 days with no net progress.

Is there any way to know, say by conducting short experiments, if your idea/implementation has any merit, and if it would be worth pursuing for more epochs/computation time?

For example, If the loss does not go down/ accuracy does not go up in the first few epochs. then should I continue the experiment or try something different?




",0,2
446,2016-6-13,2016,6,13,20,4nv7o5,FP16 on Nvidia Jetson TX1,https://www.reddit.com/r/MachineLearning/comments/4nv7o5/fp16_on_nvidia_jetson_tx1/,dharma-1,1465817811,,0,5
447,2016-6-13,2016,6,13,21,4nvcyw,BI Corner contributor Rohit Yadav's latest article: Date Science 101: The Rise &amp; Shine of Machine Learning.,https://www.reddit.com/r/MachineLearning/comments/4nvcyw/bi_corner_contributor_rohit_yadavs_latest_article/,derrickmartins,1465820550,,0,1
448,2016-6-13,2016,6,13,21,4nve8k,Is it possible to create a pronunciation error detection/correction neural net?,https://www.reddit.com/r/MachineLearning/comments/4nve8k/is_it_possible_to_create_a_pronunciation_error/,Abu_mohd,1465821143,"Hi,
I'm wondering if I can create a neural net that take a poem text as input and listen to a person reciting/singing the poem and output a signal/correction whenever the person make a pronunciation mistake.

The problems I'm thinking about:

* How to link the text to the sound wave samples?

* I don't think I can collect enough recitation samples to train a neural net

* There are so many ways one can recite/sing a poem

What do you think guys?",0,0
449,2016-6-13,2016,6,13,21,4nvguh,TensorFlow tutorials,https://www.reddit.com/r/MachineLearning/comments/4nvguh/tensorflow_tutorials/,samchoi7,1465822349,,3,81
450,2016-6-13,2016,6,13,21,4nvgw6,Is Machine Learning Threatening your job?,https://www.reddit.com/r/MachineLearning/comments/4nvgw6/is_machine_learning_threatening_your_job/,subhkirti,1465822370,,0,1
451,2016-6-13,2016,6,13,22,4nvj41,Is there any Approximate Nearest Neighbor (ANN) library in Python which supports custom distance function?,https://www.reddit.com/r/MachineLearning/comments/4nvj41/is_there_any_approximate_nearest_neighbor_ann/,n00bto1337,1465823316,"I tried Scikit's LSHForest, as well as Spotify's Annoy, and FLANN. None of them supports custom functions. Are there any libraries which has the same?",9,5
452,2016-6-13,2016,6,13,23,4nvszd,What do you mean by the 'number of units' in a RNN cell?,https://www.reddit.com/r/MachineLearning/comments/4nvszd/what_do_you_mean_by_the_number_of_units_in_a_rnn/,frogsplash911,1465827292,"In a multilayered perceptron network, in any given layer there can be certain number of nodes which accept inputs and perform calculations. How does it compare to an RNN?

Also, what is the 'number of hidden dimensions' in a RNN cell in the current context? If I wanted the network to learn more features, would it suffice to increase the number of hidden features in the RNN cell and make adding another 'unit' redundant (if such a thing exists) ?",3,1
453,2016-6-13,2016,6,13,23,4nvuax,Implementation of deep reinforcement learning papers in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4nvuax/implementation_of_deep_reinforcement_learning/,carpedm20,1465827748,,2,25
454,2016-6-13,2016,6,13,23,4nvvbp,"Developer for 10 years, want to transition to Machine Learning, have 3 years, how to spend my time wisely?",https://www.reddit.com/r/MachineLearning/comments/4nvvbp/developer_for_10_years_want_to_transition_to/,chimneyscout,1465828144,"I have been working as a developer for past 10 years. Doing webdev (mostly) and distributed systems (limited experience).

I want to transition to an ML career. I have completed Andrew Ng machine learning coursera programme. 
I have understood some math. Linear Algebra was OK, I have zero calculus knowledge. I was fairly good at the practical assignments and completed them. I have enjoyed the course a lot, but this was 3 years ago and I didn't take it anywhere after that.

Now, I have a fairly stable part-time job and want to spend next 3 years studying and gaining experience in the area. Probably, go for a CS degree, but unsure yet.

My question is how to spend this time with the maximum efficiency?

I have narrowed a list of subjects, essentially:

- Calculus
- Probability
- Statistics
- Linear Algebra
- Revisit Andrew Ng course and related areas
- Practical ML

I have picked up Spivak's Calculus book and am enjoying it a lot.
It's quite tough, I manage through half of the exercises, but it's not an easy road. It seems I won't be able to go faster than a chapter per week.

My maths is very rusty. I know some algebra, some analysis, some combinatorics, some linear algebra. The knowledge is scattered a lot.

Should I get a Math's teacher? Should I concentrate on a single area of maths, or do things concurrently?

I want to concentrate on one area at a time, but also have some practice implementing algorithms on real life (or imitation) systems.

Any advices of experienced ML'ers would be appreciated!
",51,129
455,2016-6-13,2016,6,13,23,4nvvvn,Implementing a feed forward network using only numpy. (reference for absolute beginners),https://www.reddit.com/r/MachineLearning/comments/4nvvvn/implementing_a_feed_forward_network_using_only/,[deleted],1465828360,[deleted],0,1
456,2016-6-13,2016,6,13,23,4nvyap,"ML noobie here, can I get a predictor formula from Random Forests?",https://www.reddit.com/r/MachineLearning/comments/4nvyap/ml_noobie_here_can_i_get_a_predictor_formula_from/,dawn_of_thyme,1465829311,"So I've run the sklearn RandomForestsCategorical and RandomForestRegression fits on my data and got a solution that predicts what im looking for at an accuracy level I'm happy with. However, I'm wondering if there's a way to get a predictor formula out of the package as well.

Something along the lines of Y = mx +b .... etc. A model that I can talk about the independent variable coefficients, etc

Is this even possible? Thanks for giving me some guidance.",7,3
457,2016-6-14,2016,6,14,0,4nw4f4,Video and audio-trained AI predicts sound from silent videos,https://www.reddit.com/r/MachineLearning/comments/4nw4f4/video_and_audiotrained_ai_predicts_sound_from/,sunwoo-yang,1465831466,,9,88
458,2016-6-14,2016,6,14,0,4nw9al,Sentiment classification on node level for RNTN and SVN,https://www.reddit.com/r/MachineLearning/comments/4nw9al/sentiment_classification_on_node_level_for_rntn/,Faceman1208,1465833208,"Hi,

I have question regarding this paper (http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf). In the paper there are some results on page 7 in Table 1. There are results for **All** and **Root**. For the results **All** they use the results of all nodes of the tree. For **Root** they use the results on sentence level. I'm aware of how they calculate the results for the RNTN. But how do they calculate the All results for SVN?

Greetings and thanks in advance",1,0
459,2016-6-14,2016,6,14,1,4nwbu2,Style transfer for videos - now available on DeepArt.io,https://www.reddit.com/r/MachineLearning/comments/4nwbu2/style_transfer_for_videos_now_available_on/,pmigdal,1465834092,,1,0
460,2016-6-14,2016,6,14,2,4nwmh6,Research and applied AI conference in London: 15 speakers announced for Playfair AI Summit 2016,https://www.reddit.com/r/MachineLearning/comments/4nwmh6/research_and_applied_ai_conference_in_london_15/,nb410,1465837737,,1,0
461,2016-6-14,2016,6,14,2,4nwn2e,"In deep learning, architecture engineering is the new feature engineering",https://www.reddit.com/r/MachineLearning/comments/4nwn2e/in_deep_learning_architecture_engineering_is_the/,smerity,1465837927,,29,102
462,2016-6-14,2016,6,14,2,4nwnyv,"Machine Learning, Stock Market and Chaos",https://www.reddit.com/r/MachineLearning/comments/4nwnyv/machine_learning_stock_market_and_chaos/,Markjack99,1465838224,,0,1
463,2016-6-14,2016,6,14,2,4nwq9k,Observe the Heart in mediation through EEG-generated visuals and sounds,https://www.reddit.com/r/MachineLearning/comments/4nwq9k/observe_the_heart_in_mediation_through/,timsu,1465839006,,0,0
464,2016-6-14,2016,6,14,2,4nwrvu,NeuralNets found its way into another fun new app: For Emoji prediction!,https://www.reddit.com/r/MachineLearning/comments/4nwrvu/neuralnets_found_its_way_into_another_fun_new_app/,Aeefire,1465839544,,0,2
465,2016-6-14,2016,6,14,2,4nwtk9,Quant News,https://www.reddit.com/r/MachineLearning/comments/4nwtk9/quant_news/,Markjack99,1465840124,,0,0
466,2016-6-14,2016,6,14,2,4nwukn,Predicting amount in checking account,https://www.reddit.com/r/MachineLearning/comments/4nwukn/predicting_amount_in_checking_account/,ownallogist,1465840471,"Hi all,

A bit of a side project - would like to try to predict the amount of money I have in my checking account at the end of each month/week/etc. 

Assuming I have all of the information my bank account can provide me (previous transactions, deposits, etc), what do you think would be the best way to approach this problem? Which models would make sense to use?

Thanks in advance! ",2,2
467,2016-6-14,2016,6,14,2,4nwva9,Retraining more than the last layer on Inception net V3,https://www.reddit.com/r/MachineLearning/comments/4nwva9/retraining_more_than_the_last_layer_on_inception/,realhamster,1465840691,"[Here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/image_retraining/retrain.py) google posted example code on how to retrain the last layer of inception net on custom data. It works pretty damn well.

The thing is that I want to try and train more than the last fully connected layer, but I am having trouble finding the name for the layers and the detailed structure of inception net.

Specifically, the last layer is named 'pool_3/_reshape:0' and you need this string to access it. I would like to know the name of the other layers of the model, so I could try to retrain them too.

Thanks!",0,3
468,2016-6-14,2016,6,14,4,4nxe7c,A Simple Machine Learning Model to Trade SPY,https://www.reddit.com/r/MachineLearning/comments/4nxe7c/a_simple_machine_learning_model_to_trade_spy/,cokechan,1465846762,,1,0
469,2016-6-14,2016,6,14,4,4nxf7f,"Wanting to start with Machine Learning, but for some unreasonable reason, I don't stand Python. Any hope for me?",https://www.reddit.com/r/MachineLearning/comments/4nxf7f/wanting_to_start_with_machine_learning_but_for/,tabarra,1465847108,"I love to work with data, and Machine Learning is always in my mind when coding, but SciKit and TensorFlow are in python , again, is there some other good options?  
  
Sorry for the probably stupid question.",15,0
470,2016-6-14,2016,6,14,5,4nxjtb,Do you train your network with compressed data?,https://www.reddit.com/r/MachineLearning/comments/4nxjtb/do_you_train_your_network_with_compressed_data/,fimari,1465848595,"And if, what's your experience?",1,0
471,2016-6-14,2016,6,14,6,4nxv4c,"Basic neural network subroutines - OSX, iOS",https://www.reddit.com/r/MachineLearning/comments/4nxv4c/basic_neural_network_subroutines_osx_ios/,gwulfs,1465852296,,4,17
472,2016-6-14,2016,6,14,7,4ny68s,Computer vision and ML in industry,https://www.reddit.com/r/MachineLearning/comments/4ny68s/computer_vision_and_ml_in_industry/,[deleted],1465856175,[deleted],3,4
473,2016-6-14,2016,6,14,8,4nyii2,Seeking Advices on Machine Learning and Data Mining Textbooks (for self-study and my project),https://www.reddit.com/r/MachineLearning/comments/4nyii2/seeking_advices_on_machine_learning_and_data/,TheoryEternity,1465860769,"Dear Reddit friends,

I am an undergraduate student who study mathematics and microbiology. I have recently been designing a project where I would like to use machine learning and data mining to predict the future viral strains of a specific viral species by studying all known strains of such species and the important factors behind viral evolution.

Unfortunately, I am very new to machine learning, so I would like to seek your advice on some books to start learning machine learning; it would be great if you can recommend one introductory book on ML and one comprehensive, detailed reference on ML (and same for data mining). My plan is to learn ML and DM with those books and conduct my research (I learn best when I have a specific project/problem to pursue). My preferred programming languages are R and C/C++.

My background: real analysis, number theory, point-set topology, axiomatic set theory, and theoretical linear algebra. I am currently learning representation theory and algebraic topology. I have not taken a mathematical statistics course, but I am willing to learn the necessary concepts alongside with studying the ML and DM.

I also welcome ML and DM books that are not specifically focused for biology as I could not find such books.  

**Also what are some key differences between ML and DM?  ",2,0
474,2016-6-14,2016,6,14,8,4nyli5,How to partition data for dimension reduction + classification pipeline?,https://www.reddit.com/r/MachineLearning/comments/4nyli5/how_to_partition_data_for_dimension_reduction/,Hdhhdjejdd,1465861928,[removed],0,1
475,2016-6-14,2016,6,14,9,4nynb6,Partition data for dimension reduction and classification,https://www.reddit.com/r/MachineLearning/comments/4nynb6/partition_data_for_dimension_reduction_and/,[deleted],1465862608,[removed],0,1
476,2016-6-14,2016,6,14,9,4nyosd,State of the art for CIFAR10/100 for MLPs (not CNNs),https://www.reddit.com/r/MachineLearning/comments/4nyosd/state_of_the_art_for_cifar10100_for_mlps_not_cnns/,fynmn,1465863167,"I am playing around with MLPs (Deep Networks) on a few data sets and was wondering what the state-of-the-art classification performance WITHOUT data augmentation is on CIFAR10 and CIFAR100. For MNIST, it seems the value is ~98.4% (correct me if I'm wrong). That's the limit of what I get with 100 random initializations using AdaM/SGD too. 

I looked at http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130 but it seems most of the approaches either use something non-deep learning (like PCA) or use CNNs. 

Any help? Thanks!",3,0
477,2016-6-14,2016,6,14,10,4nyyit,"Newbie Question) How can I use machine learning for ""Playing""",https://www.reddit.com/r/MachineLearning/comments/4nyyit/newbie_question_how_can_i_use_machine_learning/,0xFEEEFEEE,1465867000,"Hi, I'm newbie of machine learning.
I've read some books and posts about machine learning but they are all about ""Classifying"" data, (like knn or svm ...)
but what I really want to do is ""Playing"" something using my data models. (like Google's AlphaGo)
Does someone want to explain how this is possible?
I even do not know what kind of keyword should I search on the Internet.",2,0
478,2016-6-14,2016,6,14,11,4nzb01,Machine Learning is Fun: A simple explanation of how image recognition works using python and tflearn,https://www.reddit.com/r/MachineLearning/comments/4nzb01/machine_learning_is_fun_a_simple_explanation_of/,malleus17,1465871928,,14,53
479,2016-6-14,2016,6,14,11,4nzd49,Document Similarity with Word Mover's Distance,https://www.reddit.com/r/MachineLearning/comments/4nzd49/document_similarity_with_word_movers_distance/,Jxieeducation,1465872740,,0,5
480,2016-6-14,2016,6,14,11,4nze6o,Towards an integration of deep learning and neuroscience,https://www.reddit.com/r/MachineLearning/comments/4nze6o/towards_an_integration_of_deep_learning_and/,RushAndAPush,1465873166,,0,2
481,2016-6-14,2016,6,14,12,4nzegr,[1606.03401] Memory-Efficient Backpropagation Through Time,https://www.reddit.com/r/MachineLearning/comments/4nzegr/160603401_memoryefficient_backpropagation_through/,RushAndAPush,1465873274,,2,16
482,2016-6-14,2016,6,14,13,4nzqqw,CS188 Intro to AI  Course Materials,https://www.reddit.com/r/MachineLearning/comments/4nzqqw/cs188_intro_to_ai_course_materials/,iamkeyur,1465878583,,0,20
483,2016-6-14,2016,6,14,14,4nzvwh,Performance of DDQN goes up for a while and then back down?,https://www.reddit.com/r/MachineLearning/comments/4nzvwh/performance_of_ddqn_goes_up_for_a_while_and_then/,[deleted],1465881096,[deleted],0,0
484,2016-6-14,2016,6,14,14,4nzx6y,[1606.03490] The Mythos of Model Interpretability,https://www.reddit.com/r/MachineLearning/comments/4nzx6y/160603490_the_mythos_of_model_interpretability/,mttd,1465881744,,1,15
485,2016-6-14,2016,6,14,14,4nzzfu,Lecture Videos | Artificial Intelligence | MIT OpenCourseWare,https://www.reddit.com/r/MachineLearning/comments/4nzzfu/lecture_videos_artificial_intelligence_mit/,iamkeyur,1465882861,,0,1
486,2016-6-14,2016,6,14,14,4o004v,The Algorithmic Foundations of Differential Privacy (2014),https://www.reddit.com/r/MachineLearning/comments/4o004v/the_algorithmic_foundations_of_differential/,dgellow,1465883235,,0,3
487,2016-6-14,2016,6,14,16,4o0883,[1606.04080] Matching Networks for One Shot Learning,https://www.reddit.com/r/MachineLearning/comments/4o0883/160604080_matching_networks_for_one_shot_learning/,downtownslim,1465887709,,4,46
488,2016-6-14,2016,6,14,16,4o08e5,AskReddit: Is anyone using Python 3 here?,https://www.reddit.com/r/MachineLearning/comments/4o08e5/askreddit_is_anyone_using_python_3_here/,AlfonzoKaizerKok,1465887802,"I've been watching PyCon 2016 videos, and the community is really pushing for Python 3. However, all the machine learners I know use Python 2. I wonder if anyone here is using Python 3 to do all their ML work? If so, would you mind sharing your opinion on this subject?",15,4
489,2016-6-14,2016,6,14,16,4o08yu,"Learn how to create dynamic Vanilla RNN, LSTM, GRU from scratch with tensorflow higher order ops",https://www.reddit.com/r/MachineLearning/comments/4o08yu/learn_how_to_create_dynamic_vanilla_rnn_lstm_gru/,[deleted],1465888133,[deleted],0,1
490,2016-6-14,2016,6,14,16,4o0aj9,[1606.03498] Improved Techniques for Training GANs,https://www.reddit.com/r/MachineLearning/comments/4o0aj9/160603498_improved_techniques_for_training_gans/,beneuro,1465889047,,13,45
491,2016-6-14,2016,6,14,16,4o0axf,What is Gaussian kernel ?,https://www.reddit.com/r/MachineLearning/comments/4o0axf/what_is_gaussian_kernel/,John_Smith111,1465889285,"hello all 

Can someone share with me what is gaussian kernel  ?

Than you in advance!",6,0
492,2016-6-14,2016,6,14,19,4o0qyr,PhD research topic in DL for CV,https://www.reddit.com/r/MachineLearning/comments/4o0qyr/phd_research_topic_in_dl_for_cv/,apdd,1465899482,"I am enrolled in PhD course recently. I find myself lost in search of topic of my research. My area of interest is Deep-learning and Computer Vision. Please suggest some topics or resources to look for. 

Thanks",1,1
493,2016-6-14,2016,6,14,19,4o0twa,Graying the black box: Understanding DQNs,https://www.reddit.com/r/MachineLearning/comments/4o0twa/graying_the_black_box_understanding_dqns/,TomZahavy,1465901233,"Hi Everyone,

Attached a link to my paper: ""Graying the black box: Understanding DQNs"", which recently got accepted for ICML.

http://jmlr.org/proceedings/papers/v48/zahavy16.pdf

Regards,
Tom",1,6
494,2016-6-14,2016,6,14,20,4o0zxg,AskReddit: What's your favourite feature selection method?,https://www.reddit.com/r/MachineLearning/comments/4o0zxg/askreddit_whats_your_favourite_feature_selection/,AlfonzoKaizerKok,1465904614,"What's your go-to method to determine whether or not a particular feature gets used in the modelling stage?

I would also really appreciate any tips/tricks that you can share about feature engineering!",26,11
495,2016-6-14,2016,6,14,21,4o15as,Partitioning data for dimension reduction and classification pipeline,https://www.reddit.com/r/MachineLearning/comments/4o15as/partitioning_data_for_dimension_reduction_and/,Pythrowawa,1465907211,"Hi r/machinelearning

Let's say I want to test the performance of my dimension reduction + classification pipeline. To do this, I will use k-fold cross validation (divide data in training and testing sets). I know that performing dimension reduction on the complete dataset before creating the k folds is bad due to overfitting. To avoid this, the k folds for training and testing are created first. My question is the following: how should my dimension reduction + classification pipeline learn? I see two options:

* **(1)** Take my training data, divide it in two (how many samples go to each is to be determined). Use one subset to learn the dimension reduction mapping. Then, pass the other set through the learned mapping and use the reduced features to learn the classifier. Now, none of the steps have overfitted.

* **(2)** Take my training data, apply my dimension reduction to it i.e use the same data for learning and reducing. Use the reduced data to learn the classifier.

I tend to prefer approach (1) given that no overfitting occurs. With method (2), I would run into issues when I want to use my dimension reduction + classifier pipeline on new data.

Is approach (1) the correct one? Is there another way to do this? I'm not making assumptions on whether the dimension reduction is supervised or not.",3,0
496,2016-6-14,2016,6,14,21,4o17a1,Per image whitening in RGB image. Is useful during CNN traning or is useless?,https://www.reddit.com/r/MachineLearning/comments/4o17a1/per_image_whitening_in_rgb_image_is_useful_during/,pgaleone,1465908099,"I'm training a CNN and I'm doing some data agumentation and pre-processing before feedeing the network with a batch of images.

I've seen that image whitening is something used (eg, in the tensorflow tutorial of the cifar10: https://github.com/tensorflow/tensorflow/blob/r0.8/tensorflow/models/image/cifar10/cifar10_input.py#L182

and something don't (eg in the inception training: https://github.com/tensorflow/models/blob/master/inception/inception/image_processing.py#L197 )

The Karpaty's course on CNN for visual recognition contains some info on image whitening: https://cs231n.github.io/neural-networks-2/#datapre

There he says that image whitening and PCA are not used in CNNs. So, why it's used in the cifar10_inputs.py file?

The UFDL wiki (http://ufldl.stanford.edu/wiki/index.php/Data_Preprocessing) says that image whitening is useful only when working on gray scale images and not on colour images.

So.. is better to not do the image whitening when working wigth color images?

Bonus question:  some model, needs input rescaled between [0,1] others betwheen [-1,1].  Can someone explain (or link) me why?

Thank you",2,0
497,2016-6-14,2016,6,14,22,4o1d08,word-rnn not converging?,https://www.reddit.com/r/MachineLearning/comments/4o1d08/wordrnn_not_converging/,mlthrowaway_2,1465910594,"Hey everyone!
I've been trying to train a neural net using the code from [here](https://github.com/larspars/word-rnn) on just over 3 million tweets. It seems like either the network is converging only to stopwords (e.g. ""me me me me me"") or it's not converging at all.

Here's the training code:
    th train.lua -gpuid 0 -print_every 100 -eval_val_every 20000 -word_level 1 -threshold 50 -rnn_size 64

Here's some training output. I know it's not finished training, but it's on the same path as several other tries:

    884900/1039150 (epoch 42.578), train_loss = 4.54463985, grad/param norm = 1.6227e-03, time/batch = 0.2709s

Sampling from that checkpoint, we get:

    th sample.lua cv/lm_lstm_epoch42.34_78.3488.t7 -gpuid 0 -length 50 -primetext ""cats are""
using CUDA on GPU 0...	
Make sure that your saved checkpoint was also trained with GPU. If it was trained with CPU use -gpuid -1 for sampling as well	
creating an lstm...	
seeding with cats are	
--------------------------	
; you you you you you you you you you you you you u u u u u u u u u u u u u me me me me me me me me me me me me me me me me me me me me me me me me 

I've tried raising the temperature a bit, but the training loss is scaring me. Any thoughts?",1,0
498,2016-6-14,2016,6,14,23,4o1qz8,"How do students of Stanford's CS 229 come up with techniques that are not taught in the class, like T-SNE for dimensionality reduction, Manifold learning etc for their final projects? I looked at few final projects and they have done things that are not taught in class. How they do it?",https://www.reddit.com/r/MachineLearning/comments/4o1qz8/how_do_students_of_stanfords_cs_229_come_up_with/,[deleted],1465915869,[deleted],1,1
499,2016-6-15,2016,6,15,0,4o1szb,"How do students of Stanford's CS 229 come up with techniques that are not taught in the class, like T-SNE for dimensionality reduction, Manifold learning etc for their final projects? I looked at few final projects and they have done things that are not taught in class. How they do it?",https://www.reddit.com/r/MachineLearning/comments/4o1szb/how_do_students_of_stanfords_cs_229_come_up_with/,Mr__Christian_Grey,1465916551,,7,0
500,2016-6-15,2016,6,15,0,4o1uzg,Analyzing transactional data,https://www.reddit.com/r/MachineLearning/comments/4o1uzg/analyzing_transactional_data/,machinelearner5643,1465917224,"I have a large amount of transaction data (containing fields like amount, coordinates, date, time of day, type of transaction, type of product). Are there any helpful Machine Learning algorithms that would be helpful in finding patterns in the data (ie. you spend more on food on Thursdays, or you spend the most money at around 6:00 pm)",2,1
501,2016-6-15,2016,6,15,1,4o2920,Machine Learning 101 : What is regularization ? [Interactive],https://www.reddit.com/r/MachineLearning/comments/4o2920/machine_learning_101_what_is_regularization/,flowdb,1465921893,,0,1
502,2016-6-15,2016,6,15,1,4o29jo,"Over the past 7 days, Microsoft Research shared 180+ videos on Youtube. Most involve ML",https://www.reddit.com/r/MachineLearning/comments/4o29jo/over_the_past_7_days_microsoft_research_shared/,jay_jay_man,1465922062,,48,435
503,2016-6-15,2016,6,15,2,4o2i6q,Thoughts about my answer,https://www.reddit.com/r/MachineLearning/comments/4o2i6q/thoughts_about_my_answer/,fbormann,1465924783,"Hi, I'm studying machine learning on my own and I got the book ""Introduction to Machine Learning"", Ethem Alpaydin. And the very first question is: 
""Imagine you have two possibilities: You can fax a document, that is, send the image, or you can use an optical character reader (OCR) and send the text file. Discuss the advantage and disadvantages of the two approaches in a comparative manner. When would one be preferable over another?""

I've tried to answer for both and here is my answer:

""Send the Image:
- Advantage:
 -- It is possible to evaluate the image as  whole and see the structure of the document such as ""is the header placed at the center?"" or if it does have a header at all.
-- You'd also be able to evaluate easier if the text follow visual guidelines

- Disavantages:it is more expensive to analyze the text of the document since each letter is decoded into an image and since we have multiple fonts, will also have different encodings for the same letter.

OCR:
- Advantages:
-- Way easier to the data, because it's size is smaller.
-- It's simpler to identify patterns and analyze the text because all letters  and numbers will be inside the same encoding even if they have different fonts.

-Disavantages:
--We lose the information about the layout of the document.
""

---------------------------------------------------------------------
So, can anyone help me out? See if my answer is correct or where were I mistaken or what did I miss?
I'm greatly thankful for all help.
EDIT #1: I forgot the second part of the answer, sorry.",3,0
504,2016-6-15,2016,6,15,2,4o2ne3,"New Languages on MonkeyLearn, teams and analyzing data with Excel &amp; CSV files",https://www.reddit.com/r/MachineLearning/comments/4o2ne3/new_languages_on_monkeylearn_teams_and_analyzing/,wildcodegowrong,1465926413,,0,0
505,2016-6-15,2016,6,15,2,4o2njp,How to implement MaxOut in Torch?,https://www.reddit.com/r/MachineLearning/comments/4o2njp/how_to_implement_maxout_in_torch/,imanishshah,1465926462,"Implementing MaxOut activation function is very simple in Tensorflow - 

    tf.maximum(conv1, conv2)

But, I'm not sure how do we implement MaxOut in Torch? Can anybody help me with the code?",4,3
506,2016-6-15,2016,6,15,2,4o2o45,What to do with small set of labeled data and large set of unlabeled data?,https://www.reddit.com/r/MachineLearning/comments/4o2o45/what_to_do_with_small_set_of_labeled_data_and/,thecity2,1465926638,"We have a set of, say, 10K labeled images (two classes), and an unlabeled set that is maybe 10X larger (or even 100X, doesn't really matter for this discussion). What I'm wondering is can I train a NN on the initial labeled set of 10K images and then use that model to label a larger set of unlabeled images, and then use that larger set of labeled images to train the model again? Will this result in a better model? If so, does anyone have links to literature on this approach and what is called? Is this an example of semi-supervised learning?",21,8
507,2016-6-15,2016,6,15,5,4o3gh8,Rodeo 2.0 Released for Mac &amp; Linux!,https://www.reddit.com/r/MachineLearning/comments/4o3gh8/rodeo_20_released_for_mac_linux/,elisebreda,1465935736,,0,2
508,2016-6-15,2016,6,15,6,4o3ok5,Interested in investing in Machine Learning businesses,https://www.reddit.com/r/MachineLearning/comments/4o3ok5/interested_in_investing_in_machine_learning/,WIN_Investments,1465938276,"I invest in a diverse range of early stage start-ups and am interested in investing and supporting deep learning businesses. 

In addition to capital, I can help with general business planning and structure. (I'm currently the CFO at a large healthcare facility in addition to my investing activities).

Please reach out if this is of any relevance to you.

Thanks,
Jon ",0,1
509,2016-6-15,2016,6,15,7,4o3yqp,"DL4J Streaming: Load data to Kafka queue, run it thru an NN, save results to a file.",https://www.reddit.com/r/MachineLearning/comments/4o3yqp/dl4j_streaming_load_data_to_kafka_queue_run_it/,vonnik,1465941684,,0,0
510,2016-6-15,2016,6,15,7,4o3yx9,Building a dataset of Shakesperean text- Livestream,https://www.reddit.com/r/MachineLearning/comments/4o3yx9/building_a_dataset_of_shakesperean_text_livestream/,vanboxel,1465941760,,0,0
511,2016-6-15,2016,6,15,7,4o43y8,Generating Chat Messages,https://www.reddit.com/r/MachineLearning/comments/4o43y8/generating_chat_messages/,Sibbo,1465943532,"Hey,

I wrote a little tool to generate chat messages using an n-gram model.
Unfortunately, I didn't generate enough messages during my lifetime to train it properly.

Does someone know of a large publicly available database of chat messages?",2,0
512,2016-6-15,2016,6,15,7,4o45jr,Deep learning algorithms for Image Category Classification exist. Are they also being explored in terms of audio categorization?,https://www.reddit.com/r/MachineLearning/comments/4o45jr/deep_learning_algorithms_for_image_category/,nmchmsk,1465944131,"Hey guys.

Deep learning algorithms for Image Category Classification exist. Are they also being explored in terms of audio categorization?

Here I am specifically thinking about software being able to detect music genres (whether an audio file is a classical piece of music of it it is jazz) and audio samples (if the current audio file is a kick drum or a snare drum).

This would be very convenient for music producers looking for tools to categorize their massive libraries of samples, as an alternative to tag-based/title-based detection.

Thanks in advance!",2,0
513,2016-6-15,2016,6,15,8,4o4akl,"Cellular Automata, a new kind of science, simple rules: Is there a way to derive a simple rule from it's complex and generated pattern?",https://www.reddit.com/r/MachineLearning/comments/4o4akl/cellular_automata_a_new_kind_of_science_simple/,biomimic,1465945965,,9,1
514,2016-6-15,2016,6,15,10,4o4shk,[1606.03657] InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets,https://www.reddit.com/r/MachineLearning/comments/4o4shk/160603657_infogan_interpretable_representation/,r-sync,1465952837,,3,25
515,2016-6-15,2016,6,15,11,4o50wh,[1606.04474] Learning to learn by gradient descent by gradient descent,https://www.reddit.com/r/MachineLearning/comments/4o50wh/160604474_learning_to_learn_by_gradient_descent/,m_ke,1465956180,,16,51
516,2016-6-15,2016,6,15,12,4o5bzs,Is there anyone on Twitter who reliable tweets out useful information on Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/4o5bzs/is_there_anyone_on_twitter_who_reliable_tweets/,lasvegas51s,1465960757,"Ideally I'd like to receive consistent and pragmatic articles, studies, and project reports pertaining to machine learning from a handful of professionals. 

",12,21
517,2016-6-15,2016,6,15,13,4o5jei,Machine Learning 101 : What is regularization ? [Interactive Visualization],https://www.reddit.com/r/MachineLearning/comments/4o5jei/machine_learning_101_what_is_regularization/,datanice,1465964036,,0,2
518,2016-6-15,2016,6,15,14,4o5vk8,Neural Network Architectures (a history),https://www.reddit.com/r/MachineLearning/comments/4o5vk8/neural_network_architectures_a_history/,syncro22,1465970005,,3,10
519,2016-6-15,2016,6,15,17,4o6cjd,[1606.04442] DeepMath - Deep Sequence Models for Premise Selection,https://www.reddit.com/r/MachineLearning/comments/4o6cjd/160604442_deepmath_deep_sequence_models_for/,TheInvisibleHand89,1465979804,,0,15
520,2016-6-15,2016,6,15,17,4o6ck4,Please help with opening dataset,https://www.reddit.com/r/MachineLearning/comments/4o6ck4/please_help_with_opening_dataset/,data_sagan,1465979815,I already posted it earlier but still haven't figured it out. I downloaded a dataset on TripAdviser reviews which consists of .q queries. But I can't figure out where to open/run these in. Can please someone ELI5,0,0
521,2016-6-15,2016,6,15,18,4o6i8w,Speaker recognition using DNNs,https://www.reddit.com/r/MachineLearning/comments/4o6i8w/speaker_recognition_using_dnns/,babu7,1465983309,"Hi all,

Recently, I read about the usage of DNNs in speaker recognition. I made a presentation from what I understood. Maybe it helps to the people who are working in speech techonology in general. 
Link for the presentation https://github.com/bajibabu/sr_presentation",0,10
522,2016-6-15,2016,6,15,18,4o6ieg,Where can I find good resources to match being at a ML summer school or boot camp ?,https://www.reddit.com/r/MachineLearning/comments/4o6ieg/where_can_i_find_good_resources_to_match_being_at/,pseudo_inverse,1465983391,"I see so many summer camps and boot camps for ML, deep learning etc advertised around in UK and the US and they are well beyond the budget of a student like myself (even student pricing is hard for me to afford). But rather than asking whether these summer camps are resourceful (aka ""helpful"" in some sort of way given that you know the basics) can I find similar resources free on the web to get to know these topics better ? What is everyone's opinion ? Is it worth the investment ? 

thanks",2,2
523,2016-6-15,2016,6,15,18,4o6iyo,43 New External Machine Learning Resources and Updated Articles,https://www.reddit.com/r/MachineLearning/comments/4o6iyo/43_new_external_machine_learning_resources_and/,Nydhal,1465983735,,0,0
524,2016-6-15,2016,6,15,18,4o6kqh,ELI5: Time Series Decomposition,https://www.reddit.com/r/MachineLearning/comments/4o6kqh/eli5_time_series_decomposition/,AlanZucconi,1465984797,,0,6
525,2016-6-15,2016,6,15,20,4o6r8k,Anyone knows about peltarion?,https://www.reddit.com/r/MachineLearning/comments/4o6r8k/anyone_knows_about_peltarion/,reddit_tl,1465988474,,3,0
526,2016-6-15,2016,6,15,20,4o6stp,A detailed list of the online courses available to master R,https://www.reddit.com/r/MachineLearning/comments/4o6stp/a_detailed_list_of_the_online_courses_available/,nerdyfellow2016,1465989335,,0,1
527,2016-6-15,2016,6,15,20,4o6u4i,books for start machine learning,https://www.reddit.com/r/MachineLearning/comments/4o6u4i/books_for_start_machine_learning/,redmprog,1465989988,"I want a book that can learn in basic level and some good books for machine learning prerequisites 
please recommend me best books",6,1
528,2016-6-15,2016,6,15,20,4o6we3,SimpleML - Automated machine learning with Bayesian Optimization,https://www.reddit.com/r/MachineLearning/comments/4o6we3/simpleml_automated_machine_learning_with_bayesian/,maurodg,1465991110,,0,0
529,2016-6-15,2016,6,15,20,4o6wht,[1606.03073] Convolutional Sketch Inversion,https://www.reddit.com/r/MachineLearning/comments/4o6wht/160603073_convolutional_sketch_inversion/,fantastic_comment,1465991163,,14,15
530,2016-6-15,2016,6,15,21,4o724u,Automatic Machine Learning Using Python and Scikit-Learn,https://www.reddit.com/r/MachineLearning/comments/4o724u/automatic_machine_learning_using_python_and/,abhisvnit,1465993746,,0,1
531,2016-6-15,2016,6,15,22,4o7f29,"Understanding ""minibatch discrimination"" in GANs - method proposed in recent OpenAI paper",https://www.reddit.com/r/MachineLearning/comments/4o7f29/understanding_minibatch_discrimination_in_gans/,fhuszar,1465998916,,1,27
532,2016-6-16,2016,6,16,0,4o7r7e,Understanding SAX representation for Time Series,https://www.reddit.com/r/MachineLearning/comments/4o7r7e/understanding_sax_representation_for_time_series/,chain20,1466003211,"Hi,

I am not sure if this is the best place to ask such a specific question, but here goes. I am unable to understand the intuition behind some of the assertions made in the original SAX paper for timeseries representations:


""A Symbolic Representation of Time Series, with Implications for Streaming Algorithms""

1.  In Section 3.2 about Discretization, the paper mentions: 'It is desirable to have a discretization technique that will produce symbols with equiprobability'. Why is this required? What is the effect of discretization where this is not true.

2. Same section, 'A normalized timeseries have highly Gaussian distribution'. What does this mean and why is this important?

Any pointers are highly appreciated.",6,5
533,2016-6-16,2016,6,16,1,4o81c2,Can't find a particular paper,https://www.reddit.com/r/MachineLearning/comments/4o81c2/cant_find_a_particular_paper/,IllmaticGOAT,1466006544,"I'm looking for a paper I stumbled on the other day, that I can't seem to track down.

It was a meta-analysis of the state of machine learning research pointing out some criticisms. One of the biggest criticisms was how most papers use synthetic data or the UCI dataset, and thus researchers are a lot of the time working on irrelevant problems. 

Does anyone know the paper I'm referring to?
",2,2
534,2016-6-16,2016,6,16,1,4o8576,StrepHit: an intelligent reading agent that understands human language and extracts facts from text. 1.0 beta release,https://www.reddit.com/r/MachineLearning/comments/4o8576/strephit_an_intelligent_reading_agent_that/,hell_j,1466007765,,1,1
535,2016-6-16,2016,6,16,1,4o8bb8,Any free virtual machines which support gpu passthrough,https://www.reddit.com/r/MachineLearning/comments/4o8bb8/any_free_virtual_machines_which_support_gpu/,amair,1466009714,"I work on a windows machine which has a gpu. I'd like to run a linux vm but I want to utilise the gpu. I normally use Oracle Virtualbox which won't do passthrough, and have seen that [Nvidia](http://www.nvidia.com/object/grid-technology.html) does something which I think would suit my needs but it's not free. Anybody know of something which would work for me and is free to use?",8,11
536,2016-6-16,2016,6,16,1,4o8bhl,Can AI wipe out Cyber terror?,https://www.reddit.com/r/MachineLearning/comments/4o8bhl/can_ai_wipe_out_cyber_terror/,sppride,1466009772,,3,0
537,2016-6-16,2016,6,16,2,4o8csh,Machine Learning with Apache Spark starts today [Edx XSeries],https://www.reddit.com/r/MachineLearning/comments/4o8csh/machine_learning_with_apache_spark_starts_today/,jay_jay_man,1466010171,,14,30
538,2016-6-16,2016,6,16,3,4o8qzh,Regularisation in ConvNets?,https://www.reddit.com/r/MachineLearning/comments/4o8qzh/regularisation_in_convnets/,code2hell,1466014469,"I have been going through many of the example code of convnets implementation available in Tensorflow,Keras,Theano,etc. and I found that most of the time(all the examples in my case) I did not find any implementation of regularisation. Is this common in convnets or is it that the libraries do it automatically(maybe!)? ",3,0
539,2016-6-16,2016,6,16,4,4o8zzm,"We have built a web demo for Visual Question Answering. Select (or upload) an image, ask any question in natural language, and it will try to answer it.",https://www.reddit.com/r/MachineLearning/comments/4o8zzm/we_have_built_a_web_demo_for_visual_question/,[deleted],1466017268,[deleted],0,1
540,2016-6-16,2016,6,16,4,4o9064,Five to nine days ago there was a post in /r/machinelearning that lead to a PDF title somewhere along the lines of Machine Learning Dumbed Down. Does anyone have a link to it? I've tried searching for it.,https://www.reddit.com/r/MachineLearning/comments/4o9064/five_to_nine_days_ago_there_was_a_post_in/,MichaelLewis00,1466017328,,2,3
541,2016-6-16,2016,6,16,4,4o91ym,"We have created online demo for Visual Question Answering. Choose (or upload) an image, ask a natural language question, and it will try to answer it.",https://www.reddit.com/r/MachineLearning/comments/4o91ym/we_have_created_online_demo_for_visual_question/,[deleted],1466017881,[deleted],0,1
542,2016-6-16,2016,6,16,5,4o9fln,Neural network code,https://www.reddit.com/r/MachineLearning/comments/4o9fln/neural_network_code/,BrianLandes,1466022111,"Making the code for a neural network available on Github. Free to use or modify however you want. 

* 1 dependency (numpy)
* Written in Python 3.5
* Feed-forward and back-propagation
* 4 lines of code to create and train a network

[View and download on github](https://github.com/BrianLandes/NeuralNet)",4,0
543,2016-6-16,2016,6,16,5,4o9kcl,Current state of the art in face identification (not verification)?,https://www.reddit.com/r/MachineLearning/comments/4o9kcl/current_state_of_the_art_in_face_identification/,lentlent,1466023576,"It is easy to find current results for the face verification problem (i.e. is this person in this image the same as that?) from LFW results, but I'm looking for a good source for the face identification problem (i.e. which known person is in this image?). I can find a few results here and there for PubFig83 and Facebook100, mostly a few years old, but was wondering if I was missing a commonly used dataset/competition or a recent survey paper.",2,0
544,2016-6-16,2016,6,16,6,4o9n08,Do you use Apache Spark? How do you like it?,https://www.reddit.com/r/MachineLearning/comments/4o9n08/do_you_use_apache_spark_how_do_you_like_it/,[deleted],1466024433,[deleted],4,0
545,2016-6-16,2016,6,16,6,4o9q95,Visualize how a NLP system interprets grammatical-structure.,https://www.reddit.com/r/MachineLearning/comments/4o9q95/visualize_how_a_nlp_system_interprets/,[deleted],1466025427,[deleted],17,175
546,2016-6-16,2016,6,16,6,4o9ug4,Machine learning forum,https://www.reddit.com/r/MachineLearning/comments/4o9ug4/machine_learning_forum/,drewrice2,1466026765,"Hey all, as a frequent browser of r/ML and kaggle forums, I would like to start a forum that allows users to discuss and share machine learning techniques. 

I feel like kaggle forums breed a ton of innovation from many aspiring data scientists, but the posts mainly concern kaggle competitions. Anyway, would people here be interested in a general-purpose machine learning forum for sharing ideas and techniques?",1,0
547,2016-6-16,2016,6,16,6,4o9uzp,"LSTMs mentioned onstage during Apple WWDC Keynote, used for ""QuickType"" auto-completion",https://www.reddit.com/r/MachineLearning/comments/4o9uzp/lstms_mentioned_onstage_during_apple_wwdc_keynote/,pranv,1466026941,,2,3
548,2016-6-16,2016,6,16,7,4oa16x,What kind of nonlinear activation functions guarantees the universal approximator property of neural network?,https://www.reddit.com/r/MachineLearning/comments/4oa16x/what_kind_of_nonlinear_activation_functions/,vernunftig,1466028966,"Obviously not arbitrary nonlinear activation functions guarantees the amazing universal approximator property of NN, but what constraints do the activation functions need to satisfy (or what nonlinear function class) to guarantee the NN universal approximator? Is there any formal theoretical analysis of this problem?",3,2
549,2016-6-16,2016,6,16,7,4oa3rk,Reinforcement learning - choosing actions without replacement,https://www.reddit.com/r/MachineLearning/comments/4oa3rk/reinforcement_learning_choosing_actions_without/,neo82087,1466029842,"Hi, I'm working on a problem where an action can be taken a maximum of once in a sequence. So, for example, if there are 5 actions and 30 time steps, I'd like to predict the best times and actions to take for each step (or, to not take an action), while trying to maximize the response from the environment. I've implemented a partial solution using policy gradients, but I'm exploring ways to introduce the 'action-once' constraint and I've had limited success. So far I've tried:

* Giving a negative reward if the same action is taken twice. I thought it would be best to shift the constraint to the reward function, however, it seems to only learn to not take any actions.

* Sampling from action probability estimates excluding previously taken actions. This didn't seem to work.

* Just setting a null action if a previously taken action was selected. This also didn't work.

I was wondering if anyone had any suggestions on potential solutions for this problem, or possibly other approaches that might be helpful. I really appreciate your feedback and help! ",5,5
550,2016-6-16,2016,6,16,8,4oa916,Building machine learning algo vs Licensing. Facial Recognition for photos in the form of a mobile app.,https://www.reddit.com/r/MachineLearning/comments/4oa916/building_machine_learning_algo_vs_licensing/,[deleted],1466031759,[deleted],0,0
551,2016-6-16,2016,6,16,8,4oa9vx,Word Embedding and Sentence Validation,https://www.reddit.com/r/MachineLearning/comments/4oa9vx/word_embedding_and_sentence_validation/,dare_dick,1466032059,"I'm new to Word Embedding and I have an idea I'd like to use for my research. I was inspired by this blog: http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/ . Especially, the 5-gram example. Here is the example from the blog:
&gt; For example, one task we might train a network for is predicting whether a 5-gram (sequence of five words) is valid. We can easily get lots of 5-grams from Wikipedia (eg. cat sat on the mat) **and then break half of them by switching a word with a random word (eg. cat sat song the mat), since that will almost certainly make our 5-gram nonsensical**.


&gt; The model we train will run each word in the 5-gram through WW to get a vector representing it and feed those into another module called RR which tries to predict if the 5-gram is valid or broken. Then, wed like:
        R(W(cat""), W(sat""), W(on""), W(the""), W(mat""))=1
        R(W(cat""), W(sat""), W(song""), W(the""), W(mat""))=0

How can I implement something like that ? What would be the next model that determined if this is valid or not ? I thought about training a Sigmoid since this will output a binary value. Here some code I thought about from Keras:

    model = Sequential()
    model.add(Embedding(5, 128, input_length=10))
    model.add(LSTM(128))
    model.add(Dense(1))
    model.add(Activation('sigmoid'))

The problem he described in the blog means I have only data for the valid cases. Does he mean I should generate the invalid cases by myself ""the quote in the bold text""? Or I don't need the invalid cases and the model can decide that by itself ?

I'm very new to Word Embedding and I will appreciate anything that points me to the right direction. Thanks
",0,1
552,2016-6-16,2016,6,16,9,4oap92,AI for 2048 based on Q-learning.,https://www.reddit.com/r/MachineLearning/comments/4oap92/ai_for_2048_based_on_qlearning/,[deleted],1466037976,[deleted],0,1
553,2016-6-16,2016,6,16,10,4oaroh,AI for 2048 based on Q-learning.,https://www.reddit.com/r/MachineLearning/comments/4oaroh/ai_for_2048_based_on_qlearning/,[deleted],1466038911,[deleted],3,0
554,2016-6-16,2016,6,16,10,4oarya,[1606.04582] Query-Regression Networks for Machine Comprehension,https://www.reddit.com/r/MachineLearning/comments/4oarya/160604582_queryregression_networks_for_machine/,seominjoon,1466039009,,1,6
555,2016-6-16,2016,6,16,10,4oaunb,"[1606.04884] cltorch: a Hardware-Agnostic Backend for the Torch Deep Neural Network Library, Based on OpenCL",https://www.reddit.com/r/MachineLearning/comments/4oaunb/160604884_cltorch_a_hardwareagnostic_backend_for/,hughperkins,1466040035,,0,38
556,2016-6-16,2016,6,16,11,4ob158,Suggested background reading for sklearn.manifold methods?,https://www.reddit.com/r/MachineLearning/comments/4ob158/suggested_background_reading_for_sklearnmanifold/,41xx,1466042619,"I've been playing around with t-SNE as implemented in sklearn. There are a whole host of related non-linear dimensionality methods that look fascinating. I have a solid background in applied statistics, but no formal training in topology or graph theory. Any suggested material that forms the foundation for methods such as hessian eigenmapping, locally linear embedding and spectral embedding?",0,1
557,2016-6-16,2016,6,16,11,4ob37g,What is the difference between a hidden markov model and an m-order markov process?,https://www.reddit.com/r/MachineLearning/comments/4ob37g/what_is_the_difference_between_a_hidden_markov/,pretysmitty,1466043475,"I am looking to describe a system where a state can be shown as a probability on a transition probability matrix, but varies over time. If this variation is stochastic, I want to describe this additional process as a markov process. In a sense, the original markov matrix will have ""inside it"" another markov martrix. 

Should I think of this ""nested"" markov process as a hidden state, or the process that governs the future state given previous states. 

This is probably a fairly trivial question so thanks for answering!",2,1
558,2016-6-16,2016,6,16,11,4ob3cr,Layman's tutorial to machine learning: Uncovering categories in your data with principal component analysis.,https://www.reddit.com/r/MachineLearning/comments/4ob3cr/laymans_tutorial_to_machine_learning_uncovering/,[deleted],1466043533,[deleted],0,1
559,2016-6-16,2016,6,16,11,4ob3i4,Layman's tutorial: Uncovering categories in your data with principal component analysis,https://www.reddit.com/r/MachineLearning/comments/4ob3i4/laymans_tutorial_uncovering_categories_in_your/,[deleted],1466043595,[deleted],0,1
560,2016-6-16,2016,6,16,11,4ob475,Layman's tutorial to PCA - uncover categories in your data (example problem using food items),https://www.reddit.com/r/MachineLearning/comments/4ob475/laymans_tutorial_to_pca_uncover_categories_in/,inxurgence,1466043889,,0,0
561,2016-6-16,2016,6,16,11,4ob5kh,[1606.04671] Progressive Neural Networks [Deepmind],https://www.reddit.com/r/MachineLearning/comments/4ob5kh/160604671_progressive_neural_networks_deepmind/,RushAndAPush,1466044406,,22,46
562,2016-6-16,2016,6,16,11,4ob9au,The Internet of Things Needs a New Kind of Sensor,https://www.reddit.com/r/MachineLearning/comments/4ob9au/the_internet_of_things_needs_a_new_kind_of_sensor/,carlos_argueta,1466045939,,0,0
563,2016-6-16,2016,6,16,12,4ob9uw,Need ideas for a Machine Learning project,https://www.reddit.com/r/MachineLearning/comments/4ob9uw/need_ideas_for_a_machine_learning_project/,[deleted],1466046174,[deleted],6,1
564,2016-6-16,2016,6,16,12,4obass,Im2Latex help,https://www.reddit.com/r/MachineLearning/comments/4obass/im2latex_help/,MEOWmix_SWAG,1466046530,"Hi everyone, I was inspired by OpenAI's [""Requests for Research""](https://openai.com/requests-for-research/#im2latex) project list that was released a few days ago. In particular, I'm trying to tackle Project #2 on their list, Im2Latex.

Unfortunately I'm a newbie when it comes to Machine Learning, and I've only completed one [online book](http://neuralnetworksanddeeplearning.com/index.html) so far.

I'm asking for some tutorials or online resources that could help me learn about some of the following: sequence to sequence models, attention, and in-depth OCR among other things. 

If you know of any such tutorials please post them here to help steer me in the right direction.

Thank you.
",1,0
565,2016-6-16,2016,6,16,13,4obk7i,Resources for a newbie on statistics for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4obk7i/resources_for_a_newbie_on_statistics_for_machine/,Luxorv,1466050635,"So, I'm actually learning Machine Learning, but I would like to at the same time learn a good background on statistics, I have some knowledge on basic statistics like variance and  deviation. But I would like to get a better understanding on the field. 

Can you guys give some resources, like books, articles or even websites to learn statistics?

Thank you.",3,2
566,2016-6-16,2016,6,16,14,4obrw3,A question arose while reading Deep Learning Book,https://www.reddit.com/r/MachineLearning/comments/4obrw3/a_question_arose_while_reading_deep_learning_book/,jazzsaxmafia,1466054452,"Hello, I am currently reading Deep Learning Book written by Ian Goodfellow et al. In chapter 8, page 284, there is a paragraph saying 

""In addition to weight space symmetry, many kinds of neural networks have additional causes of non-identiability. For example, in any rectied linear or maxout network, we can scale all of the incoming weights and biases of a unit by  if we also scale all of its outgoing weights by 1/. This means thatif the cost function does not include terms such as weight decay that depend directly on theweights rather than the models outputsevery local minimum of a rectied linearor maxout network lies on an (m  n)-dimensional hyperbola of equivalent localminima.""

I do not understand how come scaling weights leads to local minimas. Couldn't find any other references either. Could anyone please explain to me why this is the case?

Thank you.",6,0
567,2016-6-16,2016,6,16,14,4obt6t,Reading suggestions for applied math + cs undergrad,https://www.reddit.com/r/MachineLearning/comments/4obt6t/reading_suggestions_for_applied_math_cs_undergrad/,bionerd2,1466055118,"I'm interested in applied math (yes, I get that this is pretty broad), machine/statistical learning theory, and theoretical computer science reading.
Areas that seem relevant are things like optimization, information theory, decision theory, control, analysis (real, complex, functional), measure theory, probability theory, statistics (nonparametric/parametric), etc.
These are huge fields, but I was wondering if people would be willing to give good books that would be useful/insightful as self-studies. I've already read and gotten through Elements of Statistical Learning (I learned a ton). ",3,2
568,2016-6-16,2016,6,16,15,4obyja,TensorFlow window slider,https://www.reddit.com/r/MachineLearning/comments/4obyja/tensorflow_window_slider/,kopita,1466057945,"Hi guys, I'm wondering how to implement a window slider on tensorflow. Something like the one explained in this example: https://matthewearl.github.io/2016/05/06/cnn-anpr/, I know the code is also there, but I'm not quite getting it.

I'll appreciate if someone have another example, or can give me a bit of explanation.",5,1
569,2016-6-16,2016,6,16,15,4oc1bv,Do you know about the planetary mixer principle?,https://www.reddit.com/r/MachineLearning/comments/4oc1bv/do_you_know_about_the_planetary_mixer_principle/,mixmachinery,1466059543,,1,1
570,2016-6-16,2016,6,16,16,4oc6bp,Graphic modelling of network architectures in tensorflow,https://www.reddit.com/r/MachineLearning/comments/4oc6bp/graphic_modelling_of_network_architectures_in/,NexYY,1466062350,"Hi,
in the YouTube Google video of TensorFlow a Dev is using some browser interface to model his TensorFlow graphs: https://youtu.be/oZikw5k_2FM?t=1m45s

I could find such feature in the TensorFlow documentation. Is it open source? ",1,1
571,2016-6-16,2016,6,16,17,4oc94k,Is there any practical utility in models like AIXI and other Universal Search Methods?,https://www.reddit.com/r/MachineLearning/comments/4oc94k/is_there_any_practical_utility_in_models_like/,[deleted],1466064003,[deleted],2,5
572,2016-6-16,2016,6,16,17,4occah,Worlds Tiniest Violin Uses Radar and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4occah/worlds_tiniest_violin_uses_radar_and_machine/,mackenziecherish,1466065926,,0,1
573,2016-6-16,2016,6,16,17,4oceur,What is the best way to find people who are interested to co-author scientific papers on machine learning?,https://www.reddit.com/r/MachineLearning/comments/4oceur/what_is_the_best_way_to_find_people_who_are/,ambodi,1466067464,Is there like a social network or something on the web that helps bridge locations barriers and find people who are interested in this?,8,7
574,2016-6-16,2016,6,16,18,4ockwb,A handwritten deep neural network using numpy.,https://www.reddit.com/r/MachineLearning/comments/4ockwb/a_handwritten_deep_neural_network_using_numpy/,[deleted],1466071095,[deleted],0,0
575,2016-6-16,2016,6,16,19,4ocl7b,[ML-Noob] Need to binary classify pictures,https://www.reddit.com/r/MachineLearning/comments/4ocl7b/mlnoob_need_to_binary_classify_pictures/,tehtacolord,1466071258,"Hello everyone,

I've recently successfully completed Andrew Ng 's Machine Learning Course on Coursera and I'm currently a bit baffled about all the opportunities machine learning does bring.

Now I want to create my first ""real"" use-case and want to create a neural net to determine whether a given picture is ""ok"" or ""not ok"", based on a set of about 500 labeled pictures.

Basically, said pictures contain two curves which show a specific curve of voltages. I want to train the neural net with pictures of curves I label as 'ok' in order to determine if other pictures are ok.
As I understood the theory of how deep learning works - where do I start? Are there any existent neural nets or easy implementations of these which can be easily trained and embedded in other programs? If not, how do I start modelling/training/using my own neural net?

Thanks in advance

________________________EDIT: Added link for sample pictures______________________

[sample pictures](http://imgur.com/a/zaB8l)
",12,1
576,2016-6-16,2016,6,16,19,4ocmbc,How to extract topics from a social media data set ?,https://www.reddit.com/r/MachineLearning/comments/4ocmbc/how_to_extract_topics_from_a_social_media_data_set/,princioss,1466071897,"Hello, I'm working on a project and i need to discover topics existing in a social media data set. For instance, i wanna extract the topics existing on 200K tweets. Any one recommend to me any machine learning algorithm?
",0,1
577,2016-6-16,2016,6,16,19,4ocn1e,Leading reasearch NLP: Information Retrieval,https://www.reddit.com/r/MachineLearning/comments/4ocn1e/leading_reasearch_nlp_information_retrieval/,alrojo,1466072316,"What is the leading machine- &amp; deep learning research in information retrieval and which datasets are being used? i.e. information retrieval could be a search on wikipedia.

By leading research and datasets I mean something equivalent to the following for neural machine translations:

Articles:

[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)

[Sequence to Sequence Learning
with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)

Datasets:

www.statmt.org/wmt16/translation-task.html",1,0
578,2016-6-16,2016,6,16,19,4oco8x,Machine Learning in Chinese needs translation,https://www.reddit.com/r/MachineLearning/comments/4oco8x/machine_learning_in_chinese_needs_translation/,thatmlguy1,1466073041,,1,0
579,2016-6-16,2016,6,16,20,4ocsxy,An example of a simple NN for regression using tensorflow,https://www.reddit.com/r/MachineLearning/comments/4ocsxy/an_example_of_a_simple_nn_for_regression_using/,[deleted],1466075664,[deleted],2,0
580,2016-6-16,2016,6,16,20,4oct79,"""Large Scale Deep Learning with TensorFlow"" - Jeff Dean",https://www.reddit.com/r/MachineLearning/comments/4oct79/large_scale_deep_learning_with_tensorflow_jeff/,nigh8w0lf,1466075772,,5,24
581,2016-6-16,2016,6,16,21,4oczzy,16 Free Machine Learning Books,https://www.reddit.com/r/MachineLearning/comments/4oczzy/16_free_machine_learning_books/,seojoeschmo,1466079150,,16,250
582,2016-6-16,2016,6,16,21,4od3bp,What is your opinion on the growing concern of AI safety?,https://www.reddit.com/r/MachineLearning/comments/4od3bp/what_is_your_opinion_on_the_growing_concern_of_ai/,[deleted],1466080575,[removed],3,2
583,2016-6-16,2016,6,16,21,4od5i3,"Machine-learning algorithm helps Airbnb customers understand when, and how, to haggle with hosts",https://www.reddit.com/r/MachineLearning/comments/4od5i3/machinelearning_algorithm_helps_airbnb_customers/,azewe,1466081488,,0,1
584,2016-6-16,2016,6,16,21,4od6mm,if Neural Network Automata possible?,https://www.reddit.com/r/MachineLearning/comments/4od6mm/if_neural_network_automata_possible/,songrotek,1466081962,"As we all know, Deep Learning is based on back-propagation for optimization. We just design the Neural Network architecture as the ""brain"", we did not teach the brain to learn but set the learning method. I mean is it possible to make the learning mechanism or optimization methods to be a neural network? A neural network controls another neural network to optimize? What's more, to make a complete neural network that we just feed input, reward, and obtain output, then the neural network update by itself. Currently, the deep learning model is a black box, but we know the way it learns. To the complete neural network ( i call it Neural Network Automata), even the learning method is a black box. Does anyone study such problem or could we just discuss it ?",0,1
585,2016-6-16,2016,6,16,22,4od8oh,[1606.04838] Optimization Methods for Large-Scale Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4od8oh/160604838_optimization_methods_for_largescale/,mttd,1466082768,,2,14
586,2016-6-16,2016,6,16,22,4odbkc,Alex Smola leaves CMU to join Amazon,https://www.reddit.com/r/MachineLearning/comments/4odbkc/alex_smola_leaves_cmu_to_join_amazon/,terryum,1466083878,,11,38
587,2016-6-16,2016,6,16,23,4odngk,"Adapting Tensorflow example, accuracy going to NAN",https://www.reddit.com/r/MachineLearning/comments/4odngk/adapting_tensorflow_example_accuracy_going_to_nan/,stacktoplease,1466088128,"I am a beginner at cnn and tensorflow. Trying to adapt the google mnist example to my SVHN data. My problem is that whatever i try my accuracy goes to NAN after a while of training.

I tried different batch sizes already. Can maybe someone help me along a bit to get it learning ?
 



http://pastebin.com/2iGhbSAd


",6,0
588,2016-6-17,2016,6,17,0,4ods58,[1606.04474] Learning to learn by gradient descent by gradient descent [DeepMind],https://www.reddit.com/r/MachineLearning/comments/4ods58/160604474_learning_to_learn_by_gradient_descent/,[deleted],1466089680,[deleted],2,0
589,2016-6-17,2016,6,17,0,4odup6,Hand written document recognition with deep learning,https://www.reddit.com/r/MachineLearning/comments/4odup6/hand_written_document_recognition_with_deep/,[deleted],1466090464,[deleted],1,1
590,2016-6-17,2016,6,17,0,4odut9,Human labels vs clicks for training a machine learned ranking model,https://www.reddit.com/r/MachineLearning/comments/4odut9/human_labels_vs_clicks_for_training_a_machine/,nikhilbd,1466090500,,0,1
591,2016-6-17,2016,6,17,0,4odvs0,[1606.04695] Strategic Attentive Writer for Learning Macro-Actions [Deepmind],https://www.reddit.com/r/MachineLearning/comments/4odvs0/160604695_strategic_attentive_writer_for_learning/,RushAndAPush,1466090816,,0,2
592,2016-6-17,2016,6,17,1,4oe5y3,Why are neural networks data intensive?,https://www.reddit.com/r/MachineLearning/comments/4oe5y3/why_are_neural_networks_data_intensive/,klop2031,1466094030,"I apologize about my ignorance, but I want to understand this question.

I am unsure as to why neural networks are so data intensive? For example humans can see maybe 5 or 10 examples of how to add, then we can learn to add. Why do NNs need thousands of examples to learn something?",4,1
593,2016-6-17,2016,6,17,1,4oe60l,Strategic Attentive Writer for Learning Macro-Actions,https://www.reddit.com/r/MachineLearning/comments/4oe60l/strategic_attentive_writer_for_learning/,[deleted],1466094049,[deleted],0,1
594,2016-6-17,2016,6,17,1,4oe6br,[1606.04695] Strategic Attentive Writer for Learning Macro-Actions [Deep Mind],https://www.reddit.com/r/MachineLearning/comments/4oe6br/160604695_strategic_attentive_writer_for_learning/,cesarsalgado,1466094146,,5,16
595,2016-6-17,2016,6,17,1,4oe7qo,Nvidia gtx 980ti vs 1070 for a hobbyist.,https://www.reddit.com/r/MachineLearning/comments/4oe7qo/nvidia_gtx_980ti_vs_1070_for_a_hobbyist/,vegeta_91,1466094582,"Hey everyone, I was just wondering what your opinion is in regards to either of these cards for learning and doing some deep learning on my own.  It looks like the 980ti prices will be dropping (there's currently [this 980ti](http://www.newegg.com/Product/Product.aspx?sdtid=8843623&amp;SID=57f8684633d611e6a04e66a962ec97fd0INT&amp;AID=10440897&amp;PID=1225267&amp;nm_mc=AFC-C8Junction&amp;cm_mmc=AFC-C8Junction-_-cables-_-na-_-na&amp;Item=N82E16814127912&amp;cm_sp=) on sale for $370, $10 less then the lowest expected MSRP of the 1070) as the 1070s are being released so I was wondering which of these cards will return better value.  

The 1070 has a lower memory interface (256 vs 384) and fewer cuda cores (1920 vs 2816).  However the 1070 has more memory (8 vs 6), with a higher clock speed.  The 1070 is also rated at 6.46 TFLOps vs the 980ti's 6.06.

All feedback is appreciated, thanks.",12,16
596,2016-6-17,2016,6,17,1,4oec0e,OCR Tutorials,https://www.reddit.com/r/MachineLearning/comments/4oec0e/ocr_tutorials/,MEOWmix_SWAG,1466095965,"Hi, I was wondering if someone could link me to some online tutorials that deal with the specifics of OCR from start to finish. I've been searching online and haven't been able to find any good resources. 

Thank you",5,0
597,2016-6-17,2016,6,17,2,4oehli,Easily building sophisticated data pipelines with Luigi and Domino,https://www.reddit.com/r/MachineLearning/comments/4oehli/easily_building_sophisticated_data_pipelines_with/,dccpt40,1466097684,,0,0
598,2016-6-17,2016,6,17,2,4oelpf,Finished with Andrew Ng's ML coursera course - what now?,https://www.reddit.com/r/MachineLearning/comments/4oelpf/finished_with_andrew_ngs_ml_coursera_course_what/,Sig_Luna,1466098984,,2,0
599,2016-6-17,2016,6,17,2,4oena4,Generative Models,https://www.reddit.com/r/MachineLearning/comments/4oena4/generative_models/,gwulfs,1466099469,,4,63
600,2016-6-17,2016,6,17,3,4oeta5,"Categorical variables with very long, thin tails",https://www.reddit.com/r/MachineLearning/comments/4oeta5/categorical_variables_with_very_long_thin_tails/,vmsmith,1466101329,"I have a summer intern project that entails one categorical response variable and five categorical predictors.

All of the variables consist of multiple classes (in one case about 50), and each of the variables has a Pareto-like distribution, with about 80% of the ~4,500 observations concentrated into a relatively small number of classes on the left, and then very long, thin tails on the right.

In one extreme case about 60% of the classes contain five or fewer observations (out of about 4,500). Lots of them have only one observation.

It seems to me that at some point on the tail the classes stop containing meaningful signal for predicting. I mean, if a class has one observation, is it meaningful? And I was wondering if there are any heuristics or guidelines for dealing with that.



",6,1
601,2016-6-17,2016,6,17,3,4oeujb,Building a Model to Predict Customer Engagement,https://www.reddit.com/r/MachineLearning/comments/4oeujb/building_a_model_to_predict_customer_engagement/,mbierly,1466101712,,0,5
602,2016-6-17,2016,6,17,3,4oewdq,RNN on audio for instrumental/vocal isolation?,https://www.reddit.com/r/MachineLearning/comments/4oewdq/rnn_on_audio_for_instrumentalvocal_isolation/,wweber,1466102299,"Many mashup artists prefer to have *a capella* tracks and instrumental tracks of the songs they work with instead of the regular mixed version, as this makes them easier to work with. Unfortunately, these are rarely available to the general public, and thus many dirty tricks are required to separate the vocals and instrumental.

If you're lucky, a song will have center-panned vocals, so subtracting the left and right channels will sometimes yield only the instrumental. Other techniques require filtering out frequencies that human voices cannot produce, but this can often result in poor quality audio since many instruments will coincidentally also produce these frequencies.

I'm curious if it would be possible to train a recurrent neural network on audio samples in order to remove vocals or instruments. Networks have been able to detect the timbre of different instruments given a note, and this seems like a semi-related problem. I imagine I would train it using audio tracks that have already been separated, with the sequence inputs being samples from the fully mixed track, and the outputs being samples from the track with the undesired component removed.

Is this even a feasible task? Most audio tracks are sampled at a rate of 44kHz, so in one single second of audio there would be 44,000 time steps. I've seen many RNNs only unrolled to a length of around 100 time steps for BPTT, so I am skeptical of how well this network would be able to learn, especially considering that the input and output vectors would have only one element (the amplitude). Are there ways to better preprocess or represent the input and output audio?

Thoughts appreciated.",5,3
603,2016-6-17,2016,6,17,4,4of1kg,DeepMind uses Neural Net to help another Neural Net significantly increase its learning rate,https://www.reddit.com/r/MachineLearning/comments/4of1kg/deepmind_uses_neural_net_to_help_another_neural/,omniron,1466103922,,3,0
604,2016-6-17,2016,6,17,5,4ofmcp,Sample application of VolumetricConvolution in torch,https://www.reddit.com/r/MachineLearning/comments/4ofmcp/sample_application_of_volumetricconvolution_in/,hassanzadeh,1466110600,"Hello,
Does anyone know where I can find a sample code in torch that uses VolumetricConvolution for video or 3D images?
Thanks",2,3
605,2016-6-17,2016,6,17,6,4ofox7,Free Matlab/Octave machine learning package?,https://www.reddit.com/r/MachineLearning/comments/4ofox7/free_matlaboctave_machine_learning_package/,valkomm3n,1466111433,"Are there any reliable, free Matlab/Octave packages available anywhere? I found LIBSVM, but are there others? Thanks!",3,0
606,2016-6-17,2016,6,17,6,4ofpnc,A Perceptron coded in python. How can I plot the separation line?,https://www.reddit.com/r/MachineLearning/comments/4ofpnc/a_perceptron_coded_in_python_how_can_i_plot_the/,gabegabe6,1466111680,,8,0
607,2016-6-17,2016,6,17,7,4ofxq3,Google opens a Dedicated Machine Learning Research Center in Europe,https://www.reddit.com/r/MachineLearning/comments/4ofxq3/google_opens_a_dedicated_machine_learning/,jay_jay_man,1466114476,,26,148
608,2016-6-17,2016,6,17,9,4ogije,"Selection of resources to learn Artificial Intelligence / Machine Learning - Books,Video Lectures, MOOC Courses, Blogs, Research Papers....",https://www.reddit.com/r/MachineLearning/comments/4ogije/selection_of_resources_to_learn_artificial/,nigh8w0lf,1466122078,,2,7
609,2016-6-17,2016,6,17,9,4ogkup,Would Swift be good for Machine Learning Programs?,https://www.reddit.com/r/MachineLearning/comments/4ogkup/would_swift_be_good_for_machine_learning_programs/,online204,1466122938,I was looking at Apple's new programming language which touts it's ability to be fast and be mixed with Objective-C and C++. I know that Python and R are the go to languages but could Swift be a good contender?,3,0
610,2016-6-17,2016,6,17,10,4ogyw6,Music Transcription with Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4ogyw6/music_transcription_with_convolutional_neural/,anthemscore,1466128682,,23,27
611,2016-6-17,2016,6,17,11,4oh0ua,Recreating the results in Figure 9.10 in the book Pattern recognition and machine learning (Bishop) chapter 9,https://www.reddit.com/r/MachineLearning/comments/4oh0ua/recreating_the_results_in_figure_910_in_the_book/,dangmanhtruong,1466129517,"In the book Pattern recognition and machine learning there is this Figure 9.10 which describes the results of using Bernoulli mixture model (with E-M algorithm) on MNIST handwritten digits dataset (the books only tried with numbers from 2 to 4) . I thought coding it from scratch would be cool, so I decided to give it a try. Here is my implementation in Matlab : http://www.mediafire.com/download/v1dxacy51dr3ls2/ClassifyBMM.rar
The resulting images were very close to the author's result , although the number ""2"" and ""4"" seemed to be quite blurry. But when I applied the result to classify 600 test images (again all of them are from 2 to 4), I got only about 203 corrects. Is my implementation buggy or is it because the training size is simply too small? Please help me, thank you very much :) ",0,0
612,2016-6-17,2016,6,17,11,4oh1de,[1606.03476] Generative Adversarial Imitation Learning,https://www.reddit.com/r/MachineLearning/comments/4oh1de/160603476_generative_adversarial_imitation/,gambs,1466129747,,0,7
613,2016-6-17,2016,6,17,12,4ohfbh,[1606.04474v1] Learning to learn by gradient descent by gradient descent,https://www.reddit.com/r/MachineLearning/comments/4ohfbh/160604474v1_learning_to_learn_by_gradient_descent/,[deleted],1466135667,[deleted],0,1
614,2016-6-17,2016,6,17,16,4oi3x9,Sigmoid function question,https://www.reddit.com/r/MachineLearning/comments/4oi3x9/sigmoid_function_question/,nickbuch,1466148064,"I am using the sigmoid function for forward propagation:

```def nonlin(x,deriv=False):
    if(deriv==True):
        return x*(1-x)
    return 1/(1+np.exp(-x))```

but my input values can be very large as well as zero, so the output values are either 1 or 0, respectively.

should i map my input values to a smaller domain?  use a different function?",3,1
615,2016-6-17,2016,6,17,18,4oidph,[1602.05629] Federated Learning of Deep Networks using Model Averaging,https://www.reddit.com/r/MachineLearning/comments/4oidph/160205629_federated_learning_of_deep_networks/,mimighost,1466154395,,0,1
616,2016-6-17,2016,6,17,18,4oigga,[1606.04460] Model-Free Episodic Control,https://www.reddit.com/r/MachineLearning/comments/4oigga/160604460_modelfree_episodic_control/,m000pan,1466156140,,0,5
617,2016-6-17,2016,6,17,19,4oindf,Everything UK developers need to know about machine learning | Apps,https://www.reddit.com/r/MachineLearning/comments/4oindf/everything_uk_developers_need_to_know_about/,mobleyhutson,1466160283,,0,1
618,2016-6-17,2016,6,17,20,4oisam,Deriving integer and continuous values from Temperature (Simulated Annealing),https://www.reddit.com/r/MachineLearning/comments/4oisam/deriving_integer_and_continuous_values_from/,Bohemian90,1466163040,"Hello

I would like to use the technique related to Simulated Annealing for my problem. Let's say I have an initial temperature T0 and I have 50 iterations. 

At each iteration I would like to calcualte three values depending on the current temperature:

 - An integer value: The value should start at 10 and increase to 100 in a way between linear and logarithmic.
 - A continuous value: The value should start at 1 and decrease to 0 in a way between linear and exponential.

At each iteration the temperature can be either increased or decreased.

Which formulas are reasonable to use (or exists) for the increasing/decreasing of the temperature and for producing both values?",0,0
619,2016-6-17,2016,6,17,20,4oit26,"This Week in Machine Learning, 17 June 2016",https://www.reddit.com/r/MachineLearning/comments/4oit26/this_week_in_machine_learning_17_june_2016/,DavidAJoyner,1466163446,,1,0
620,2016-6-17,2016,6,17,21,4oiz0l,Seldon 1.3.3 released with Kafka Streams,https://www.reddit.com/r/MachineLearning/comments/4oiz0l/seldon_133_released_with_kafka_streams/,ahousley,1466166316,,0,3
621,2016-6-17,2016,6,17,21,4oizyb,The Key to Successful Machine Learning in Developing Products,https://www.reddit.com/r/MachineLearning/comments/4oizyb/the_key_to_successful_machine_learning_in/,NS_nidhishah,1466166727,[removed],0,1
622,2016-6-17,2016,6,17,23,4ojdue,How can I find an advisor for a paper I am writing?,https://www.reddit.com/r/MachineLearning/comments/4ojdue/how_can_i_find_an_advisor_for_a_paper_i_am_writing/,Jakobovski,1466172472,"I have successfully built a novel multimodal, unsupervised neural network system. The system learns well and has a number of properties that are novel and very useful.

I believe the work is ready to be published, but I don't really know how to go about it. 

I have never published a paper before so it would be very valuable to get advice from someone in the field who has experience publishing.

Does anyone have any advice on how to go about finding an advisor?
",11,8
623,2016-6-17,2016,6,17,23,4ojly4,Undergrad at a top tier school (Georgia Tech) interested in ML here.,https://www.reddit.com/r/MachineLearning/comments/4ojly4/undergrad_at_a_top_tier_school_georgia_tech/,[deleted],1466175469,[deleted],19,0
624,2016-6-18,2016,6,18,0,4ojo87,Making Software with Casual Intelligence,https://www.reddit.com/r/MachineLearning/comments/4ojo87/making_software_with_casual_intelligence/,evanp,1466176304,,0,0
625,2016-6-18,2016,6,18,0,4ojoin,Just wrote my first basic genetic algorithm program!,https://www.reddit.com/r/MachineLearning/comments/4ojoin/just_wrote_my_first_basic_genetic_algorithm/,lordberric,1466176407,"I made a program to balance chemical equatuons using genetic algorithms. It's overkill, there's no real need for it, but it's fun!


Just thought I'd share. I can give anyone the link to it on git. It's pretty much uncommented though, so fair warning.",9,0
626,2016-6-18,2016,6,18,0,4ojux4,Image recognition tutorial with a deep convnet using Tensorflow on MLDB,https://www.reddit.com/r/MachineLearning/comments/4ojux4/image_recognition_tutorial_with_a_deep_convnet/,ddcarnage,1466178634,,0,1
627,2016-6-18,2016,6,18,2,4ok893,Using RNNs to process point clouds - How can we estimate global properties of datasets?,https://www.reddit.com/r/MachineLearning/comments/4ok893/using_rnns_to_process_point_clouds_how_can_we/,quagmire_giggity,1466183201,"I'm trying to process point cloud data to estimate global properties of a dataset. For example, in the most trivial case, is it possible to pass a sequence of random vectors drawn from different Gaussians and expect the network to predict their parameters (mean, variances etc.)? I would like to train a network to predict more complex global properties..

It's my understanding that RNNs/LSTMs can be helpful for such a task. Is there work out there that does something related?",3,0
628,2016-6-18,2016,6,18,2,4ok9o8,Supervised learning for finding duplicate data?,https://www.reddit.com/r/MachineLearning/comments/4ok9o8/supervised_learning_for_finding_duplicate_data/,MyNameCouldntBeAsLon,1466183686,"I have a huge dataset, that includes *known* duplicates. The task in hand is finding the possible duplicates in the rest of the dataset. I figured using a supervised approach, feeding the known duplicates would be an easier task than running LSH, MinHash, or something along those lines.

The observations in this dataset exhibit very high similarity in one of their dimensions... so lowering the weight of this dimension would be desired.

If anyone has any suggestions, they would be highly appreciated.

Finally, implementations in R or Python 2.7 are preferred.",12,0
629,2016-6-18,2016,6,18,2,4okcsp,Many programming courses are about to be gone. Coursera is removing 472 free online courses from the internet on June 30th. This guide will show you how to hurry up and legally download as many courses as possible before June 30th. [xpost /r/learnprogramming],https://www.reddit.com/r/MachineLearning/comments/4okcsp/many_programming_courses_are_about_to_be_gone/,kgpkalandura,1466184720,http://makemeflow.org/advice/2016/06/how-to-download-courseras-courses-before-theyre-gone-forever/,49,320
630,2016-6-18,2016,6,18,2,4okfnc,[1606.05233] Learning feed-forward one-shot learners,https://www.reddit.com/r/MachineLearning/comments/4okfnc/160605233_learning_feedforward_oneshot_learners/,brainggear,1466185662,,1,6
631,2016-6-18,2016,6,18,2,4okgc9,Tuning Trading Models with Bayesian Optimization,https://www.reddit.com/r/MachineLearning/comments/4okgc9/tuning_trading_models_with_bayesian_optimization/,Zephyr314,1466185906,,4,6
632,2016-6-18,2016,6,18,3,4oki1x,Multi-GPU GAN,https://www.reddit.com/r/MachineLearning/comments/4oki1x/multigpu_gan/,antinucleon,1466186463,,0,22
633,2016-6-18,2016,6,18,3,4okjhy,[1606.05262] Convolutional Residual Memory Networks (identifies features to remember using combination of LSTM/ResNet),https://www.reddit.com/r/MachineLearning/comments/4okjhy/160605262_convolutional_residual_memory_networks/,andyandy16,1466186951,,4,0
634,2016-6-18,2016,6,18,3,4okkrg,Automatic differentiation for machine learning in Julia (cross post from r/julia),https://www.reddit.com/r/MachineLearning/comments/4okkrg/automatic_differentiation_for_machine_learning_in/,int8blog,1466187390,,3,4
635,2016-6-18,2016,6,18,4,4okzoj,I just wrote my first machine learning program!,https://www.reddit.com/r/MachineLearning/comments/4okzoj/i_just_wrote_my_first_machine_learning_program/,lordberric,1466192431,"I'm resubmitting because my last post was lackluster at best. 


I wrote a program that balances chemical equations using genetic algorithms. It's a brute force program that just creates random coefficients for the equation, then checks how close it is to correct. the 10% that are closest each spawn 100 mutations, and so on, until it's correct.


It took ~12 hours of straight coding (split over two days). 4 hours of creating the program to parse the input, 8 to write the genetic algorithms.


I'm fairly new to coding, so be nice. It's not commented that well, but that's my next step.


I wrote this for a junior year chemistry project. Here's the git link.


https://github.com/lordberric/chemical_balancer

",7,0
636,2016-6-18,2016,6,18,5,4ol897,"[ICML 2016]: Workshop on online advertising. Speakers from Google, Facebook, Criteo and more",https://www.reddit.com/r/MachineLearning/comments/4ol897/icml_2016_workshop_on_online_advertising_speakers/,sharat_sc,1466195397,,0,0
637,2016-6-18,2016,6,18,5,4oldff,Design help,https://www.reddit.com/r/MachineLearning/comments/4oldff/design_help/,mellanox-guy,1466197199,"So I'm a cs student at university and I want to learn more about using Hadoop hdfs and spark for machine learning, I have a background in general cs and ml comcepts, and the purpose of this is to learn spark and scala.

   The problem is I'm not sure where to start.  The problem I picked out is this, I have a list of entities, call them ""senders"" and these senders create goods and their surplus is picked up by ""recievers"" to be used elsewhere, so it doesn't go to waste. I have detailed records of the quantity and type of surplus each sender created and assiciated meta data.  I also have location data, and sometimes data about the employees at the senders and recievers, but this isn't homogeneous and may be out of date.  I want to make a predictive model to make estimates for the future surplus for each sender location that pulls in data about the location and the connected reciever so that I can also make predictions about how much surplus a new sender should generate.   So if I know that sender A has 25 units of surplus, and is in a high traffic area, which decreases their surplus, that sender B, a new sender, in a similar area should have similar numbers. 

Since I have the lat, long of all senders and recievers, I can pull in any location meta data for this part that I need from google.

Sorry for the general terms, this is kind of a side project I'm doing for where I work, to see how well it preforms and so I don't want to be too specific, if you need anything else just ask.

Thanks in advance for any help.",0,0
638,2016-6-18,2016,6,18,6,4olggc,English twitter accounts of ISIS leaders/supporters?,https://www.reddit.com/r/MachineLearning/comments/4olggc/english_twitter_accounts_of_isis_leaderssupporters/,koko_toto360,1466198319,"Hi, for a text mining research I need accounts or list of accounts or corpoa in english of isis supporters. Thank you!",0,1
639,2016-6-18,2016,6,18,6,4oljyc,"Any large (10,000+) image datasets of a single class not in ilsvrc2012?",https://www.reddit.com/r/MachineLearning/comments/4oljyc/any_large_10000_image_datasets_of_a_single_class/,anonDogeLover,1466199578,...especially one that's very different from any of the 1000 ilsvrc2012 classes and can be easily downloaded.,1,0
640,2016-6-18,2016,6,18,6,4oll4a,StyLit: Artistic Style Transfer for 3D Objects,https://www.reddit.com/r/MachineLearning/comments/4oll4a/stylit_artistic_style_transfer_for_3d_objects/,jezeq,1466200010,,3,48
641,2016-6-18,2016,6,18,7,4olo6e,Any free or open source app resembling Eureqa?,https://www.reddit.com/r/MachineLearning/comments/4olo6e/any_free_or_open_source_app_resembling_eureqa/,the_salubrious_one,1466201156,"To be more precise, programs that do genetic algorithm/symbolic regression AND cross validations AND have low learning curve.

I liked Eureqa. Unfortunately, it costs something like a grand now.

https://en.wikipedia.org/wiki/Eureqa

",4,1
642,2016-6-18,2016,6,18,7,4olot3,Simple Word2Vec (and related methods) Question,https://www.reddit.com/r/MachineLearning/comments/4olot3/simple_word2vec_and_related_methods_question/,montgomerybradford,1466201403,"I realize this sounds a trivial: but when using, e.g., CBOW to train a distributed word representation, we use the context around a word to predict the word itself. How are words at the start (or end) of sentences handled? If I'm using a window of 4 (two before, two after context), does the first word get learned by the following 4? Or just the following 2?

Thanks!",7,0
643,2016-6-18,2016,6,18,8,4om09z,Harvard videos down?,https://www.reddit.com/r/MachineLearning/comments/4om09z/harvard_videos_down/,enangel,1466205880,"I have seen in this subreddit some of the Harvard courses on Data Science, Machine Learning and Monte Carlo methods. Particularly, a great class I was following was AM207 "". Monte Carlo Methods for Inference and Data Analysis"". The videos could be found here:

http://cm.dce.harvard.edu/2014/02/24104/publicationListing.shtml

To my dismay, I have been unable to watch anything for a couple of weeks. I am fearing they have removed the videos. This was a great resource.

Does anybody have the AM207 videos downloaded or know if they will repost them?

P.S: I'm not sure if this is the correct community to post it. I will post this anywhere else if anybody tells me a better place.",1,8
644,2016-6-18,2016,6,18,8,4om0ku,Car Autopilot Using Deep Learning (Code in Description),https://www.reddit.com/r/MachineLearning/comments/4om0ku/car_autopilot_using_deep_learning_code_in/,Weihua99,1466206000,,2,0
645,2016-6-18,2016,6,18,10,4omj7l,Ant Soccer,https://www.reddit.com/r/MachineLearning/comments/4omj7l/ant_soccer/,RushAndAPush,1466214226,,1,7
646,2016-6-18,2016,6,18,11,4omn1j,Can anyone give me advices about my chance to get into Master/phd program at MILA (Yoshua Bengio group)?,https://www.reddit.com/r/MachineLearning/comments/4omn1j/can_anyone_give_me_advices_about_my_chance_to_get/,huyhcmut,1466216045,"My profile: GPA 8,8/10 (rank 6/350), 2 research project in deep learning (1 NLP and 1 Vision)(No publication), some project course (NLP, machine learning, CV). i have one silver medal in the National Olympiad programming contest for Undergraduate student, some scholarships award. My Toefl score: 93/120,GRE score: 1340.
P/s:the bad thing is that my under uni do not have many math class, i just take math normal course for CS student. All background math for research i studied myself. I do not know how to tell Bengio member in my cv that i have *enough* math for research.",24,1
647,2016-6-18,2016,6,18,11,4ompm9,Jeff Hawkins - What are the Hard Unsolved Problems in HTM,https://www.reddit.com/r/MachineLearning/comments/4ompm9/jeff_hawkins_what_are_the_hard_unsolved_problems/,Chispy,1466217243,,4,0
648,2016-6-18,2016,6,18,11,4omqhq,Implementing Self Organizing Maps (training on Spark available),https://www.reddit.com/r/MachineLearning/comments/4omqhq/implementing_self_organizing_maps_training_on/,Jxieeducation,1466217655,,0,0
649,2016-6-18,2016,6,18,11,4omrg6,"Happy Tweet, Sad Tweet  Building a Naive Bayes Classifier in SQL",https://www.reddit.com/r/MachineLearning/comments/4omrg6/happy_tweet_sad_tweet_building_a_naive_bayes/,hemartin,1466218133,,2,1
650,2016-6-18,2016,6,18,11,4omsdd,Alternative to Andrew Ng's Machine Learning course?,https://www.reddit.com/r/MachineLearning/comments/4omsdd/alternative_to_andrew_ngs_machine_learning_course/,desu-no,1466218568,"Sorry for asking another age-old question.

Been lurking here, and seems some redditors said better start out with online course other that Andrew Ng because of other are more relevant (and not using Octave)

Which are the current best for beginners to start out learning?",5,0
651,2016-6-18,2016,6,18,13,4on0ac,Inquiry regarding the potential relationship(s) between information theory and machine learning.,https://www.reddit.com/r/MachineLearning/comments/4on0ac/inquiry_regarding_the_potential_relationships/,laJaybird,1466222602,"Rather than asking a question that may be answered directly, I'm curious of the different perspectives /r/MachineLearning could provide regarding how the mathematical theory of information could potentially illuminate new directions in machine learning that hasn't yet been explored by contemporary neural networking and AI systems in general. 

To elaborate further, I have been reading a bit on the subject of Information theory and it seems to provide a lot of potential for describing an intrinsic process in which a concept as mystifying as ""sentience"" can emerge from nothing but the states of physical bodies (of course however, I don't have much to go on to detail exactly how something like that would actually be done, I simply see a line of questioning that may be interesting to explore). From here, the ultimate reason I am inquiring on the matter is because I want to learn more about what kind of fundamental assumptions about the mind are made in reference to the construction of Machine Learning paradigms and how these assumptions might be able to synchronize with the axioms of information. 

Or, perhaps this has already been delved into by programmers and theorists. If this is the case, then I would like to apologize for my ignorance (I'm currently studying to become a mathematician and haven't had much opportunity to explore further into computer science as of yet) and I would appreciate any reference material you guys could provide.",12,0
652,2016-6-18,2016,6,18,14,4on8ab,Dense Associative Memory for Pattern Recognition,https://www.reddit.com/r/MachineLearning/comments/4on8ab/dense_associative_memory_for_pattern_recognition/,muktabh,1466227031,,1,7
653,2016-6-18,2016,6,18,14,4on8u9,How did Apple get such accurate object recognition to work LOCALLY on an iPhone?,https://www.reddit.com/r/MachineLearning/comments/4on8u9/how_did_apple_get_such_accurate_object/,nightofgrim,1466227363,"I've been playing around with iOS 10 for a couple of days now. It did take a bit for it to chew through my massive photo collection, but holy shit is it impressive once it's done.

Could someone who's really knowledgable give some insight on how this is possible? I understand google images and their massive network and huge amounts of data, but my phone is just a phone. How does it know my out of focus shot of my dogs nose is a dog? Or my tardis is a phone booth. Close up of pepperoni is food. Locally how does it have enough training data for this stuff?",3,0
654,2016-6-18,2016,6,18,14,4ona35,Neural Networks course: Geoff Hinton v Hugo Larochelle,https://www.reddit.com/r/MachineLearning/comments/4ona35/neural_networks_course_geoff_hinton_v_hugo/,iseebeerpeople,1466228101,"Now that the Neural Networks for Machine Learning course on Coursera is going to be deprecated, I was wondering whether I should just download all the lectures and watch them at my own pace, or whether I should move on to another course such as Hugo Larochelle's video lectures on Youtube. The only reason I was sticking with Hinton was because of the assignments, but now that I may not have access to those anymore, I was wondering if Larochelle would be a better alternative to Hinton.",3,0
655,2016-6-18,2016,6,18,18,4onty9,what do you think about first book for start studying machine learning,https://www.reddit.com/r/MachineLearning/comments/4onty9/what_do_you_think_about_first_book_for_start/,redmprog,1466241123,"I'm so sorry for asking this question again because I'm not convinced
I'm confused for choosing book for learning machine learning can you help me?
please suggest me best book for linear algebra , statistic and probability that I can after those study machine learning.
next good book for machine learning
and what's your opinion about [Pattern Classification Book by David G. Stork, Peter E. Hart, and Richard O. Duda] that who know machine learning professional suggest me this book.
",3,0
656,2016-6-18,2016,6,18,19,4onzlj,[1606.04797] V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation,https://www.reddit.com/r/MachineLearning/comments/4onzlj/160604797_vnet_fully_convolutional_neural/,faustomilletari,1466245222,,0,1
657,2016-6-18,2016,6,18,20,4oo3l8,Deepmind blog: deep RL by David Silver,https://www.reddit.com/r/MachineLearning/comments/4oo3l8/deepmind_blog_deep_rl_by_david_silver/,dataoverflow,1466248058,,20,72
658,2016-6-18,2016,6,18,20,4oo891,Automatic Copper Foil Cutting Machine,https://www.reddit.com/r/MachineLearning/comments/4oo891/automatic_copper_foil_cutting_machine/,jinhongbao119,1466251057,,0,1
659,2016-6-18,2016,6,18,22,4oofig,Jrgen Schmidhuber 50th Birthday Remix,https://www.reddit.com/r/MachineLearning/comments/4oofig/jrgen_schmidhuber_50th_birthday_remix/,[deleted],1466255120,[deleted],0,1
660,2016-6-18,2016,6,18,22,4oolsc,Machine learning front and centre of R&amp;D for Microsoft and Google,https://www.reddit.com/r/MachineLearning/comments/4oolsc/machine_learning_front_and_centre_of_rd_for/,yifae,1466258276,,0,0
661,2016-6-18,2016,6,18,23,4oomaz,List of machine learning concepts,https://www.reddit.com/r/MachineLearning/comments/4oomaz/list_of_machine_learning_concepts/,Sig_Luna,1466258517,"Hey guys
I know that machine learning isn't just one subject, but it's a generic term for a lot of different concepts. I know about some of them and also found this list on wikipedia: https://en.wikipedia.org/wiki/List_of_machine_learning_concepts.

But how it is with wikipedia, it's often a lot to read and not a lot to understand. So is there a list anywhere with these concept and an easy explanation?",3,14
662,2016-6-18,2016,6,18,23,4oomeb,Google opens a Machine Learning research group in Europe,https://www.reddit.com/r/MachineLearning/comments/4oomeb/google_opens_a_machine_learning_research_group_in/,rickerjen,1466258568,,0,0
663,2016-6-18,2016,6,18,23,4oors2,To which types of financial fraud could machine learning be used?,https://www.reddit.com/r/MachineLearning/comments/4oors2/to_which_types_of_financial_fraud_could_machine/,sganlondon,1466261124,"I am interested in using machine learning techniques to detect financial fraud.

I know there are many different kinds of financial fraud, including insider trading, money laundering, credit card misuse, etc.

Are there any types of financial fraud that are more interesting to look at than others? Are there any especially interesting machine learning techniques that can detect these kinds of fraud really good?",5,0
664,2016-6-19,2016,6,19,0,4oouqt,[1606.05328] Conditional Image Generation with PixelCNN Decoders,https://www.reddit.com/r/MachineLearning/comments/4oouqt/160605328_conditional_image_generation_with/,benanne,1466262509,,4,27
665,2016-6-19,2016,6,19,0,4oowlu,Keras GPU consuming a lot of memory,https://www.reddit.com/r/MachineLearning/comments/4oowlu/keras_gpu_consuming_a_lot_of_memory/,abhinavbh08,1466263319,"I am doing unsupervised training using auto encoder on a Titan X GPU with 12 GB of memory. I am using Keras with theano on back end. I have around 9lac input nodes and a single hidden layer. But whenever I am increasing the size greater than 500 hidden nodes, then it is showing out of memory error and always shows available space 1.8 GB? What is the problem ? I have also tried decreasing the batch size but to no avail ? ",1,1
666,2016-6-19,2016,6,19,0,4ooyf6,Looking for tool to label bounding boxes in video,https://www.reddit.com/r/MachineLearning/comments/4ooyf6/looking_for_tool_to_label_bounding_boxes_in_video/,shycapslock,1466264114,"I have recorded videos that I need for my thesis, but now I need to label the frames in the video  so I am looking for a tool to help me with the task.

In each frame, the following labels will be annotated as bounding boxes: body of the person, face of the person. Example of what a single labeled frame should look like: http://i.imgur.com/ew7Gk0a.png

Ideally, I could also set ""start"" and ""end"" markers for gestures in the video, but I could also note frame numbers manually, if the tool doesn't support  that.

I am not familiar with the popular tools in that area. Is there anything that could help me with the task or should I create something from scratch? Thanks.",9,8
667,2016-6-19,2016,6,19,0,4ooz9m,"Nando's deep learning course or CS224D Deep learning course, for NLP(basically I want to do sentiment analysis) ?",https://www.reddit.com/r/MachineLearning/comments/4ooz9m/nandos_deep_learning_course_or_cs224d_deep/,Mr__Christian_Grey,1466264479,"Could you recommend me, which deep learning course should I study for NLP(Specifically sentiment analysis)?

Nando - 
https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/

CS224d - http://cs224d.stanford.edu/

CMU - http://deeplearning.cs.cmu.edu/

or some other course?

Thanks

",2,0
668,2016-6-19,2016,6,19,0,4op0ap,"Recreating Figure 9.10 in chapter 9, Pattern recognition and machine learning (Bishop)",https://www.reddit.com/r/MachineLearning/comments/4op0ap/recreating_figure_910_in_chapter_9_pattern/,dangmanhtruong,1466264901,"The thing is I have been reading the PRML book for a while now, and recently I saw this figure in chapter 9 about using Bernoulli mixture model (based on Expectation - Maximization) to cluster handwritten digits from 2 to 4 (using the MNIST dataset). So I figured it would be cool to recreate the author's result. Here is my implementation in Matlab: http://www.mediafire.com/download/s5kz6837alx4b6s/ClassifyBMM%282%29.rar. Note that the author created the figure as an example of a clustering method, but it could be used for classification as well. I designed the code to terminate after a preset number of iterations , without checking any convergence criterion, and I found out that it only takes about 10 iterations for the algorithm to converge (the resulting images doesn't change anymore). Also, when I used 600 images on the training set, I got about 75% accuracy, but when I used all of the images from 2-4 (17391 in total), the results were 90% accuracy (not bad for my first project). I think any machine learning novice who wants to see real actions from the book would greatly benefit from this. You can also modify the code to classify any digit you want. However, the code is not very efficient, and I would very much appreciate any feedback on my code, both improvements in efficiency as well as writing clearer code. Thank you very much! ",7,1
669,2016-6-19,2016,6,19,2,4opd0h,Where would I look to get started with automated Social Network Analysis?,https://www.reddit.com/r/MachineLearning/comments/4opd0h/where_would_i_look_to_get_started_with_automated/,Rich700000000000,1466270181,What would I read/study?,3,1
670,2016-6-19,2016,6,19,2,4opd0s,Apache SystemML 0.10.0-incubating released,https://www.reddit.com/r/MachineLearning/comments/4opd0s/apache_systemml_0100incubating_released/,based2,1466270182,,0,7
671,2016-6-19,2016,6,19,2,4opiki,What is best practice for the data pipeline in large dataset neural network training,https://www.reddit.com/r/MachineLearning/comments/4opiki/what_is_best_practice_for_the_data_pipeline_in/,third_dude,1466272347,"For instance, right now I am trying to train a colorization network, I struggle between 2 extremes

1. Store straight up images. Process, prepare, etc. data all as its being fed in as a batch. Currently the batch generator has to do many things: download the image from s3, create a numpy array, convert it to LAB, reshape it, transpose it, categorize the output colors, and feed the input grey values into a separate model to predict hyper columns. 

pros: can store lots of data, can change model/approach without reprocessing data. 
cons: the processing time/tax on the system that has to preprocess the image and then feed it into the network each time. Hard to run more than 1 image per batch. 


2. Process everything beforehand and store it as hdf5. That way the batch generator just has to download a part of the file off s3 and feed it into the network. 

Pros: Less tax on the system that is doing the training. Can experiment more with batch size, etc. 
Cons: Larger memory tax when feeding massive arrays into system. Training data takes way more space. 

Then I could fall somewhere in the middle - perhaps the image LAB data in hdf5 but wait to extract the hyper columns until training time. 


I have tried all 3 approaches and run into issues each time. This makes hope that perhaps there is a best practices type of rule for setting up these pipelines?? Or at least have you all found some rules /guidelines that work for you?",3,1
672,2016-6-19,2016,6,19,3,4opk6r,Common applications in a business,https://www.reddit.com/r/MachineLearning/comments/4opk6r/common_applications_in_a_business/,Param-eter,1466273000,What are common/ generally applied applications of machine learning in a business? I find that a lot of people I work with don't really know what machine learning is. Do you guys have examples of applications that are becoming more and more common?,2,2
673,2016-6-19,2016,6,19,3,4oplmy,Topic Modelling Advice Needed,https://www.reddit.com/r/MachineLearning/comments/4oplmy/topic_modelling_advice_needed/,geomtry,1466273611,"I have a dataset of journals and am trying to find similar journals based on content. Journals are short with a 200 character limit. I would appreciate advice about decisions I made so far.

1) Using a TF-IDF representation of journal posts, I apply techniques like NMF and LDA to find journals with similar ""topics"". 

What would be a good way to measure how these unsupervised models are performing? One measure I researched was perplexity, but it ignores intra-cluster perplexity. Of course a better topic is less likely to be a part of a perplex topic space.

2) I can manually label topics discovered from unsupervised techniques, but there might be better approaches. I was thinking about using an information retrieval approach to find the nearest Wikipedia topic to an aggregated document of journals in a topic. How else can I try to automate topic labels? There's LDA2Vec which is fairly recent. 

3) Another approach of finding human readable semantics is word2vec, but I am unaware of dataset size constraints for applying such a model. I am thinking of increasing my dataset size by augmenting it with Twitter data that has a certain level of similarity to the journals and follows some heuristic such as containing keywords about emotions/experiences.",5,3
674,2016-6-19,2016,6,19,3,4opqnv,Simple gradient descent example using Codeblocks-EP in C language,https://www.reddit.com/r/MachineLearning/comments/4opqnv/simple_gradient_descent_example_using/,greenbastard22e,1466275558,,0,1
675,2016-6-19,2016,6,19,4,4opwxi,"Book: Introducing Data Science: Big Data, Machine Learning, and more, using Python tools",https://www.reddit.com/r/MachineLearning/comments/4opwxi/book_introducing_data_science_big_data_machine/,abdsc,1466278071,,0,1
676,2016-6-19,2016,6,19,4,4opyhs,Simple Gradient Descent in C Tutorial,https://www.reddit.com/r/MachineLearning/comments/4opyhs/simple_gradient_descent_in_c_tutorial/,greenbastard22e,1466278697,,0,1
677,2016-6-19,2016,6,19,4,4oq118,Neural Turing Machine,https://www.reddit.com/r/MachineLearning/comments/4oq118/neural_turing_machine/,hassanzadeh,1466279741,"Hi folks,
I have a few questions about NTM. 
Is there any extension to these models? 
and are there any implementation of these models in torch?
Thanks",1,6
678,2016-6-19,2016,6,19,7,4oqokh,Recognizing the user's intent with a chatbot,https://www.reddit.com/r/MachineLearning/comments/4oqokh/recognizing_the_users_intent_with_a_chatbot/,vsakos,1466289729,"I want to create a simple chatbot, and I'm planning on using the Stanford NLP libs for parsing the messages from the user, but I have no idea how can I detect the user's intent.

For example:

    Hello!               -&gt; greeting
    Hi &lt;chatbot's name&gt;  -&gt; greeting
    Tell me the weather  -&gt; weather
    Is it raining?       -&gt; weather

I want to have a predefined list of intents and check if the user's message belongs to one of those categories or not.

What is the right tool or algorithm for this? Should I use a classifier library, for example the Stanford Classifier? Can classifiers have an ""everything else"" class? Because I will need that, too, for messages the bot won't/shouldn't understand.

I don't have previous experience with any of this (AI, NLP...), so maybe just a starting point is enough, I don't even know how to Google this :D",12,13
679,2016-6-19,2016,6,19,8,4oqxcl,Logistic regression in Java with plotting - a simple example,https://www.reddit.com/r/MachineLearning/comments/4oqxcl/logistic_regression_in_java_with_plotting_a/,greenbastard22e,1466293477,,0,1
680,2016-6-19,2016,6,19,9,4or1er,[icml2016] Anyone up for a walk?,https://www.reddit.com/r/MachineLearning/comments/4or1er/icml2016_anyone_up_for_a_walk/,bihaqo,1466295242,"Hi, I just arrived in NY after a loong flight and want to walk around the town!
Does anyone want the same in a sleepy company?",3,1
681,2016-6-19,2016,6,19,9,4or5w4,what resources do you use to prepare for a machine learning interview?,https://www.reddit.com/r/MachineLearning/comments/4or5w4/what_resources_do_you_use_to_prepare_for_a/,sensitiveinfomax,1466297264,I'm trying to aggregate links and questions that would be useful for people job hunting in ML/data science.,3,4
682,2016-6-19,2016,6,19,10,4or8wb,"What if I have a categorical labeled corpus, but the labels are ""fuzzy""? How can I leverage the labeled data to canonicalize/discretize the labels before building a classifier of the data?",https://www.reddit.com/r/MachineLearning/comments/4or8wb/what_if_i_have_a_categorical_labeled_corpus_but/,compsc,1466298702,"I'm sure this is a fundamental concept, but I'm inexperienced.  Here's a senario:

Say I have a set of discrete entities, in this case outdoor parks.  A human knows which park is which, but they are given textually inconsistent names.  We may see (made up):

    Stone Mountain Park
    Mountain Stone Park
    Old Mountain Stone Park Trail
    Parkstone Mountaintrail

The idea is that these are all referring to the same entity, but of course the individual words are used for other parks as well.

My ultimate goal is to classify textual descriptions of the park as belonging to a certain park, and output an arbitrarily chosen canonical name referring to the correct entity.

So I might see:

    Stone Mountain Park: ""nice grassy park with leaves""
    Mountain Stone Park (description 1): ""has leaves, and nice grass trail""
    Old Mountain Stone Park Trail: ""the leaves on the mountain sure are nice""
    Parkstone Mountaintrail: ""the trail has nice grass on it, and good leaves too, on the mountain""
    Mountain Stone Park (description 2): ""you can camp there""

When I build my classifier, I want all those descriptions to have the same label (e.g., to strengthen term weights, otherwise ""trail"" could be weighted low because it occurs in 3 different classes if we naively assume those are different classes).

As I said, I'm inexperinced, but I can tell that I should be able to leverage the possibility of two different text descriptions being similar to match up park names, as well as the textual similarity of the park names themselves.

Naively I could just concatenate the park names with the descriptions and the cluster those, but that doesn't seem optimal.

Is there a name for this concept that I can google?

Thanks

",5,8
683,2016-6-19,2016,6,19,10,4or9c4,Creating text based on previously given text,https://www.reddit.com/r/MachineLearning/comments/4or9c4/creating_text_based_on_previously_given_text/,kimppis,1466298924,"For my own interest, I would like to develop program that generates sentences based on previously given large set of text. How should I start tackling with this problem? I saw Google has a new parser, could that be used here? Or am I way over my head? :D ",0,1
684,2016-6-19,2016,6,19,12,4orpt5,"Using cross-entropy for SGD (question, work included)",https://www.reddit.com/r/MachineLearning/comments/4orpt5/using_crossentropy_for_sgd_question_work_included/,[deleted],1466307002,[deleted],0,1
685,2016-6-19,2016,6,19,12,4ors5o,Getting started with Machine Learning with U-Washington ML specialization in Coursera,https://www.reddit.com/r/MachineLearning/comments/4ors5o/getting_started_with_machine_learning_with/,xenocide15,1466308147,,9,7
686,2016-6-19,2016,6,19,15,4osact,Deepmind Publications,https://www.reddit.com/r/MachineLearning/comments/4osact/deepmind_publications/,shagunsodhani,1466318657,,0,0
687,2016-6-19,2016,6,19,16,4osea1,"All supervised learning methods seem sort of the same, are they special cases of a general algorithm? Based on noob intuition after Coursera ML course.",https://www.reddit.com/r/MachineLearning/comments/4osea1/all_supervised_learning_methods_seem_sort_of_the/,jvdalen,1466321424,"Hi, Very new to ML, and no math background. Im almost done with Andrew Ng Machine Learning Course on Coursera. 

Not sure if I can explain what I mean very well. Here it goes:

It seems that most ML (Linear regression, Logistic regression, SVM, K-means, PCA, *) methods there have more in common then they are different.

All seem to be ""find an optimal and compressed model"" of the world, for your task, to predict some outcome. 

What is in common: 
 1) You start with a set of features, in a matrix
 - (Normalize it)
 2) Take a target value (either Y in supervised learning, or a lower dimension or a cluster, which is basically a Y value, but not set by the programmer). 
 3) Calculate difference (cost) between current model's prediction, and desired outcome (difference can be, lineair, sigmoid, vector distance, etc)
 4) Find a model that minimizes cost 

Am I overfitting, or are each of these ""machine learning methods"", special cases of some more generic algorithm?  Can someone points me to more info, for people without deep math schooling?

Thanks!
*Maybe neural nets as well, but then chained together or something, which could be even more generic?",11,40
688,2016-6-19,2016,6,19,17,4oskhq,[1606.05262v1] Convolutional Residual Memory Networks,https://www.reddit.com/r/MachineLearning/comments/4oskhq/160605262v1_convolutional_residual_memory_networks/,x2342,1466326174,,1,0
689,2016-6-19,2016,6,19,19,4oste4,Doing reproducible data experiments can be tough. Anybody using a framework for experiment management? codalab worksheets? Do you know any alternatives?,https://www.reddit.com/r/MachineLearning/comments/4oste4/doing_reproducible_data_experiments_can_be_tough/,ragipy,1466332795,,5,8
690,2016-6-19,2016,6,19,19,4osuz5,[Question] How common is L1/L2 norm regularization in deep learning today?,https://www.reddit.com/r/MachineLearning/comments/4osuz5/question_how_common_is_l1l2_norm_regularization/,mere_mortise,1466333835,How do they compare to dropout?,8,13
691,2016-6-19,2016,6,19,20,4osvl3,Looking for ML dataset with data on physical structures,https://www.reddit.com/r/MachineLearning/comments/4osvl3/looking_for_ml_dataset_with_data_on_physical/,kanshi_pdv,1466334240,"Hi :)

I'm on a (so far) fruitless mission to find a large dataset containing physical structures of the same class. I need it for a research of analyzing structural properties using ML.

The class/type of the element is not relevant to me as long as it's a concrete, physical structure (i.e. not a source code structure). These can be mechanical joints, bike's steering wheels, clutches, you name it. 

Anybody has any hints how I can get my hands on such a dataset (paid or free)?

",9,3
692,2016-6-19,2016,6,19,20,4osxk3,How long will it take to replace physicians/scientists with machines?,https://www.reddit.com/r/MachineLearning/comments/4osxk3/how_long_will_it_take_to_replace/,Arbacluce,1466335569,"I am afraid of my future, I heard that no job is safe in next 20 years. ",5,0
693,2016-6-19,2016,6,19,21,4ot22s,Variational Adversarial Convolutional Recurrent Pixel Turing Memory with Residual Stochastic LSTM Autoencoder,https://www.reddit.com/r/MachineLearning/comments/4ot22s/variational_adversarial_convolutional_recurrent/,bbsome,1466338304,I think the title says it all... ,18,25
694,2016-6-19,2016,6,19,22,4otbpc,Peeking inside Convnets,https://www.reddit.com/r/MachineLearning/comments/4otbpc/peeking_inside_convnets/,matsiyatzy,1466343342,,2,20
695,2016-6-20,2016,6,20,0,4otmsf,Calculus &amp; Backpropagation finally in LaTeX,https://www.reddit.com/r/MachineLearning/comments/4otmsf/calculus_backpropagation_finally_in_latex/,Kiuhnm,1466348550,,39,114
696,2016-6-20,2016,6,20,2,4ou9al,"""Math or compsci oriented"" consciousness research",https://www.reddit.com/r/MachineLearning/comments/4ou9al/math_or_compsci_oriented_consciousness_research/,jazzbumps,1466357746,"I've recently stumbled upon a few pieces of work that appear to approach the classical problem of consciousness from a math or compsci perspective: [Practopoesis](http://www.danko-nikolic.com/practopoiesis/), [Integrated Information Theory](https://en.wikipedia.org/wiki/Integrated_information_theory)

Does anybody know any other research that approaches the consciousness problem like this? That is, research that is oriented toward ultimately creating a software implementation of consciousness.

Not interested in psychology or neurological work that consists of observations about the brain (which is what the bulk of serious consciousness research seems to be about). Though if it is a psychology/neurology paper with observations that reveal a mechanism that can be implemented in software, then sure.

Thanks guys, you're awesome. Love this sub!",3,0
697,2016-6-20,2016,6,20,2,4oub2v,Build an AI Writer - Machine Learning for Hackers #8,https://www.reddit.com/r/MachineLearning/comments/4oub2v/build_an_ai_writer_machine_learning_for_hackers_8/,llSourcell,1466358451,,21,17
698,2016-6-20,2016,6,20,4,4ouohz,kNN time complexity,https://www.reddit.com/r/MachineLearning/comments/4ouohz/knn_time_complexity/,Lopelh,1466363490,,0,1
699,2016-6-20,2016,6,20,4,4out6s,(ICML 2016) David Silver's Deep Reinforcement Learning Slides,https://www.reddit.com/r/MachineLearning/comments/4out6s/icml_2016_david_silvers_deep_reinforcement/,iamtrask,1466365186,,4,8
700,2016-6-20,2016,6,20,5,4ouxsm,How to handle non-latin text in LSTM ?,https://www.reddit.com/r/MachineLearning/comments/4ouxsm/how_to_handle_nonlatin_text_in_lstm/,mustafaihssan,1466366862,"I'm trying to make arabic poetry generator with tensorflow LSTM, and I wandering how i can encode / feed my non-latin text into the network.",2,0
701,2016-6-20,2016,6,20,5,4ov4q7,Example of 1-dimensional time series better modeled by RNN than ARIMA?,https://www.reddit.com/r/MachineLearning/comments/4ov4q7/example_of_1dimensional_time_series_better/,Pieranha,1466369469,"I'm trying to get a better understanding of the intuition behind the success of recurrent neural networks and is in that process looking for simple examples to explain the strengths of the RNN approach. Can anyone explain a 1-dimensional time series that's better modeled by a RNN than an ARIMA model?

Both models could be extended with an infinite number of parameters to completely memorize the training data, but I'm looking for an example, where it's intuitive that the RNN performs better with roughly the same number of ""lags"" (e.g. the ARIMA model having 10 lags and the RNN doing BPTT for 10 time periods).

A simple graph of the example would be awesome, but an explanation in text would be great as well.",6,12
702,2016-6-20,2016,6,20,6,4ov6hq,NIPS sponsor list shows investment firms - How do they use machine learning?,https://www.reddit.com/r/MachineLearning/comments/4ov6hq/nips_sponsor_list_shows_investment_firms_how_do/,Pieranha,1466370133,"From the NIPS sponsor list I see that investment managements companies such as http://www.twosigma.com/, http://www.vaticlabs.com/careers/ and https://www.wintoncapital.com/en/home are becoming increasingly interested in machine learning.

What kind of machine learning do they use? Perhaps more interesting - how do they turn it into investment strategies?",3,0
703,2016-6-20,2016,6,20,6,4ov83r,Demo video: Continuous control with deep reinforcement learning [Google DeepMind],https://www.reddit.com/r/MachineLearning/comments/4ov83r/demo_video_continuous_control_with_deep/,rhiever,1466370732,,0,0
704,2016-6-20,2016,6,20,6,4ov8rw,(ICML 2016) Tutorial slides: Causal inference for observational studies,https://www.reddit.com/r/MachineLearning/comments/4ov8rw/icml_2016_tutorial_slides_causal_inference_for/,urish,1466370987,,0,12
705,2016-6-20,2016,6,20,6,4ov9lo,A noobs guide to implementing RNN-LSTM using Tensorflow,https://www.reddit.com/r/MachineLearning/comments/4ov9lo/a_noobs_guide_to_implementing_rnnlstm_using/,based2,1466371287,,1,5
706,2016-6-20,2016,6,20,7,4ovi83,Sharing weights in neural network ensembles,https://www.reddit.com/r/MachineLearning/comments/4ovi83/sharing_weights_in_neural_network_ensembles/,samtheman1x,1466374675,"I am trying to find a paper i read months ago on archive. 

The idea is that in ensembles of neural networks the early layers all end up learning similar filters. So it is not worth it to have different weights for each member of the ensemble in early layers. After the first n layers each ensemble member ""branches off"" with unique weights. So in the paper there was a diagram that looks like a tree.

Does anyone know what paper this could be?",2,2
707,2016-6-20,2016,6,20,7,4ovm6e,Can I use copyrighted material from YouTube to train a model that I want to use in a commercial app?,https://www.reddit.com/r/MachineLearning/comments/4ovm6e/can_i_use_copyrighted_material_from_youtube_to/,McLearnin,1466376330,[removed],0,1
708,2016-6-20,2016,6,20,8,4ovs1a,A handy equation reference/cheatsheet for common machine learning algorithms,https://www.reddit.com/r/MachineLearning/comments/4ovs1a/a_handy_equation_referencecheatsheet_for_common/,[deleted],1466378796,[deleted],0,0
709,2016-6-20,2016,6,20,9,4ovxe6,Word2Vec &amp; Music Generation,https://www.reddit.com/r/MachineLearning/comments/4ovxe6/word2vec_music_generation/,keten,1466381020,"Hey all, I just started getting into machine learning. I'm very interested in applying machine learning to music generation and I'm attempting what I think is a unique approach to the problem.

Most of the literature on the topic seems focused on using recurrent neural networks (like LSTMs). My main complaint with this approach is the generated music seems... kind of boring and generic. I decided to go with a different approach by using word2vec on sheet music and choosing random ""similar"" chords in a sequence. I'm currently trying to combine doc2vec and LSTMs so you can influence the model to produce a particular kind of music, but I think the initial results are pretty promising! The music is far less structured than what I've seen done before, but I think it adds a lot of character. What do you guys think?

Soundcloud: https://soundcloud.com/user-849261345

Best song (IMO): https://soundcloud.com/user-849261345/style

Weirdest one that still sounds good: https://soundcloud.com/user-849261345/crazy-ryhthm

",13,4
710,2016-6-20,2016,6,20,12,4owoq8,Three seemingly related ICML reviews,https://www.reddit.com/r/MachineLearning/comments/4owoq8/three_seemingly_related_icml_reviews/,[deleted],1466392594,[removed],0,1
711,2016-6-20,2016,6,20,12,4owrgt,3 suspiciously related ICML reviews?,https://www.reddit.com/r/MachineLearning/comments/4owrgt/3_suspiciously_related_icml_reviews/,[deleted],1466393660,[removed],0,1
712,2016-6-20,2016,6,20,16,4oxmcq,Batch normalization and unbalanced train set,https://www.reddit.com/r/MachineLearning/comments/4oxmcq/batch_normalization_and_unbalanced_train_set/,pgaleone,1466408846,"I'm training a CNN to classify 20 classes. My train dataset is the PASCAL VOC 2012.

This dataset contains a lots of instances of the class ""person"" respect to the others.

I've trained my network with SGD and I applied a batch normalization layer after every convolution (after every ReLU indeed, as described here: https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md).

Results:

the output is dramatically biased on the class person (it outputs 14 (class person) always).

I just trained the same net, without the batch normalization layer and the results were not biased in that way.

So, am I concluding that batch normalization bias the net if the train dataset is not balanced?

It's this somewhere documented?",4,3
713,2016-6-20,2016,6,20,16,4oxn8e,Installing a gorgeous outdoor fireplace can liven up your outdoor space without spending too much money - and you wont have to deal with inconvenient maintenance! Check out our brick pit outdoor fireplace installation services to learn more.,https://www.reddit.com/r/MachineLearning/comments/4oxn8e/installing_a_gorgeous_outdoor_fireplace_can_liven/,Progasnorthshore,1466409353,,2,0
714,2016-6-20,2016,6,20,17,4oxqh8,Neural Network vs Touring Machine,https://www.reddit.com/r/MachineLearning/comments/4oxqh8/neural_network_vs_touring_machine/,wotanii,1466411294,"Are there problems, which can be solved easily by a TM but require very complex NNs to solve? 
(is palindrome-detection such a problem?)

Is it possible to structure a NN so it'll ""grow"" into a TM?",10,0
715,2016-6-20,2016,6,20,18,4oxutn,Future Trend Of Global Ball Screw Jack Market,https://www.reddit.com/r/MachineLearning/comments/4oxutn/future_trend_of_global_ball_screw_jack_market/,aiden_11,1466413838,[removed],0,1
716,2016-6-20,2016,6,20,18,4oxz7e,What are some examples of state-of-the-art AI conversational systems?,https://www.reddit.com/r/MachineLearning/comments/4oxz7e/what_are_some_examples_of_stateoftheart_ai/,[deleted],1466416309,[deleted],0,0
717,2016-6-20,2016,6,20,19,4oy0cd,New Market Study Published: Global Diamond Drill Industry,https://www.reddit.com/r/MachineLearning/comments/4oy0cd/new_market_study_published_global_diamond_drill/,aiden_11,1466416922,[removed],0,1
718,2016-6-20,2016,6,20,19,4oy1ar,Production Deep Learning with NVIDIA GPU Inference Engine,https://www.reddit.com/r/MachineLearning/comments/4oy1ar/production_deep_learning_with_nvidia_gpu/,harrism,1466417473,,0,5
719,2016-6-20,2016,6,20,19,4oy2jw,Call for Papers: Advances in Biologically Inspired Reservoir Computing (Cognitive Computation),https://www.reddit.com/r/MachineLearning/comments/4oy2jw/call_for_papers_advances_in_biologically_inspired/,scardax88,1466418164,,2,7
720,2016-6-20,2016,6,20,20,4oy7di,help for complete beginner into machine learning. what kind of maths and programming exp is needed ?,https://www.reddit.com/r/MachineLearning/comments/4oy7di/help_for_complete_beginner_into_machine_learning/,theguy2108,1466420711,"I am 17 years old and will begin college soon. I only have high school level knowledge in maths. I have been programming for some time now. started with Java a long time ago, since then have done c++ and a little c#. I love working with algorithms and am right now taking an online course on algorithms on Coursera and then will probably read this book: https://www.amazon.com/Introduction-Algorithms-3rd-MIT-Press/dp/0262033844/ref=sr_1_1?ie=UTF8&amp;qid=1466414143&amp;sr=8-1&amp;keywords=introduction+to+algorithms I have read a little about neural nets and am very interested. i don't know where to begin unfortunately. How much math should I know and do before I can start Machine Learning and is this book the right choice ? and should I do more programming before I delve into Machine learning and where do I start? any help is appreciated",3,0
721,2016-6-20,2016,6,20,20,4oy9uw,Google Research opens machine intelligence base in Zurich,https://www.reddit.com/r/MachineLearning/comments/4oy9uw/google_research_opens_machine_intelligence_base/,szaboelod,1466421986,,0,0
722,2016-6-20,2016,6,20,20,4oybdc,What happened to the IBM Watson X Prize?,https://www.reddit.com/r/MachineLearning/comments/4oybdc/what_happened_to_the_ibm_watson_x_prize/,nickl,1466422704,"&gt; The IBM Watson AI XPRIZE ... was announced on the TED Stage on Feb 17, 2016. It is a $5 million competition.... 

&gt; Complete rules and guidelines will be made available in May.

[http://www.xprize.org/ai](http://www.xprize.org/ai)

So what happened? ",1,4
723,2016-6-20,2016,6,20,21,4oye29,Whats Next for Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/4oye29/whats_next_for_artificial_intelligence/,[deleted],1466424006,[deleted],1,2
724,2016-6-20,2016,6,20,21,4oyfsv,34 Free Data Science Books,https://www.reddit.com/r/MachineLearning/comments/4oyfsv/34_free_data_science_books/,seojoeschmo,1466424800,,1,2
725,2016-6-20,2016,6,20,21,4oyiee,"I just finished Andrew NG's ML course on Coursera, is taking the University of Washington Coursera course afterwards a good idea?",https://www.reddit.com/r/MachineLearning/comments/4oyiee/i_just_finished_andrew_ngs_ml_course_on_coursera/,aaa101101,1466426042,"I am thinking of starting the University of Washington specialization to gain a deeper understanding of the material taught in Andrew NG's course as i found it lacking depth, is that a good idea or should i just start doing something else with what i learned from Andrew NG's course, and if so, how should i proceed?",6,2
726,2016-6-20,2016,6,20,21,4oyjc0,Most predictive words in papers accepted/rejected at ICML 2016,https://www.reddit.com/r/MachineLearning/comments/4oyjc0/most_predictive_words_in_papers_acceptedrejected/,rhiever,1466426476,,2,15
727,2016-6-20,2016,6,20,22,4oynwy,Andrew Ng is offering a free draft copy of his new book (until Friday Jun 24th),https://www.reddit.com/r/MachineLearning/comments/4oynwy/andrew_ng_is_offering_a_free_draft_copy_of_his/,whiteshadow13,1466428551,,37,361
728,2016-6-20,2016,6,20,22,4oyr46,3 related ICML reviews,https://www.reddit.com/r/MachineLearning/comments/4oyr46/3_related_icml_reviews/,adversarialreviews,1466429780,[removed],3,1
729,2016-6-20,2016,6,20,22,4oyuqh,"Hello, Tensorflow",https://www.reddit.com/r/MachineLearning/comments/4oyuqh/hello_tensorflow/,iamkeyur,1466431193,,4,22
730,2016-6-20,2016,6,20,23,4oywzk,Partially connected per block instead of fully connected layers ?,https://www.reddit.com/r/MachineLearning/comments/4oywzk/partially_connected_per_block_instead_of_fully/,Schlagv,1466431990,"When speaking of neural networks, I mostly read about conv layers and fully connected layers.

Conv layers are just FC layers, with local connections and weight sharing.

For unstructured data, weight sharing doesn't make sense, but can we still use local connections, to reduce the memory and computational cost?

FC layers are prohibitively expensive as the number of operations grows as the square of the number of neurons. I seems natural for me to try to do it by block, to force many 0 in the weight matrix and reduce the computational cost.

Why don't I see papers about that topic? Am I just ignorant? If yes, can you provide me some links or keywords?",7,0
731,2016-6-20,2016,6,20,23,4oz447,The 5 Best Methods for Drawing Insight Out of Machine Data,https://www.reddit.com/r/MachineLearning/comments/4oz447/the_5_best_methods_for_drawing_insight_out_of/,tmavash1,1466434526,,0,0
732,2016-6-21,2016,6,21,0,4oz6zz,Deep Neural Network Optimization with SigOpt and Nervana Cloud,https://www.reddit.com/r/MachineLearning/comments/4oz6zz/deep_neural_network_optimization_with_sigopt_and/,Zephyr314,1466435508,,1,0
733,2016-6-21,2016,6,21,0,4oz8p4,Hiring a Machine Learning Director 4 Prec. Agriculture Company in FL - highly paid salary - will hire immediately!! sara.booker@modis.com,https://www.reddit.com/r/MachineLearning/comments/4oz8p4/hiring_a_machine_learning_director_4_prec/,RecruiteroftheGEEKS,1466436048,,0,0
734,2016-6-21,2016,6,21,0,4ozc1n,multilinear-math: Tensor and Machine Learning Library for Swift,https://www.reddit.com/r/MachineLearning/comments/4ozc1n/multilinearmath_tensor_and_machine_learning/,vidivinci,1466437175,"Hey, here's a library for multidimensional data, tensors and machine learning in Swift that I created:
[https://github.com/vincentherrmann/multilinear-math](https://github.com/vincentherrmann/multilinear-math)

I also started documenting some concepts and thoughts about it on this [Wiki](https://github.com/vincentherrmann/multilinear-math/wiki). What do you think?

I don't know how many of you use or know Swift. Because of the strong type system, protocols and value semantics, I think it really lends itself for complex mathematical computations. But there's not much going on in Swift regarding machine learning. And since I wanted to implement some algorithms anyway, I thought, it would be nice doing it in Swift.

I'm planning to migrate to Swift 3 and doing a rewrite along with it. But before that, I would appreciate some feedback!
This library might be pretty unique, I think, in it's ability to handle tensors with an arbitrary mode (resp. dimension) count and operate sensible with them. Also, [abstract indices and Einstein notation](https://github.com/vincentherrmann/multilinear-math/wiki/Tensors) are a feature that I don't know from any other library. Whether this is actually useful is a different matter. I'm using the Accelerate framework for most computations and do quite a lot of dispatching (actually, probably too much!), but on performance-wise, there is certainly much room for improvement.

Do you have any suggestions? Are there any features I should add right now (compare to the ones listed [here](https://github.com/vincentherrmann/multilinear-math/wiki/Roadmap))?
Should I change any names? Is the number of mistakes in my documentation texts unbearable? Does anyone want to contribute to this library, or actually use it? Should I let it go, use TensorFlow?
Thanks!
",0,0
735,2016-6-21,2016,6,21,0,4ozc1p,simple question about Genetic Algorithms,https://www.reddit.com/r/MachineLearning/comments/4ozc1p/simple_question_about_genetic_algorithms/,maximus12793,1466437175,"I have been trying to make a tetris bot for a while and am having some confusion about how a classifier is evolved over time. Using the two articles linked I understand the qualities I am trying to optimize (minimizing holes, height etc.). But how does the classifier actually get evaluated? It seems like I would be able to code this logic and say chose the best height option(is there any confusion about what position yields the lowest height? no. continue, what gives least holes?) etc. So where does the evolution come in? In the el-tetris article he says he uses particle swarm to evaluate the weights but idk why the weights are relevant or how you could evaluate it on a move by move basis even if you had 300 games running in parallel. Any idea where I am tripping up? Ie. what exactly am I breeding here? the outcome of 2 successful moves and continuing down a game tree? 


http://imake.ninja/el-tetris-an-improvement-on-pierre-dellacheries-algorithm/

https://codemyroad.wordpress.com/2013/04/14/tetris-ai-the-near-perfect-player/",4,3
736,2016-6-21,2016,6,21,0,4ozdq8,A path to unsupervised learning through adversarial networks,https://www.reddit.com/r/MachineLearning/comments/4ozdq8/a_path_to_unsupervised_learning_through/,gwulfs,1466437732,,2,18
737,2016-6-21,2016,6,21,0,4ozf5c,Critique of Bayesian Auto-tuning,https://www.reddit.com/r/MachineLearning/comments/4ozf5c/critique_of_bayesian_autotuning/,nickel2,1466438205,,11,32
738,2016-6-21,2016,6,21,1,4ozmhr,OpenAI technical goals,https://www.reddit.com/r/MachineLearning/comments/4ozmhr/openai_technical_goals/,shagunsodhani,1466440619,,30,26
739,2016-6-21,2016,6,21,1,4ozprt,How applicable are neural Turing machines and neural gpus,https://www.reddit.com/r/MachineLearning/comments/4ozprt/how_applicable_are_neural_turing_machines_and/,hassanzadeh,1466441682,"Hi deep learners
I wonder how applicable are the neural Turing machines and the neural gpus?
Besides the published original paper, I've never seen any paper using these tachniques and extending their applicability in other domains. Does that mean these are useless? Do you know any other success stories of these methods?
Thanks
",2,0
740,2016-6-21,2016,6,21,1,4ozq0h,I need some advise on my first Neural Network(C++),https://www.reddit.com/r/MachineLearning/comments/4ozq0h/i_need_some_advise_on_my_first_neural_networkc/,konkovac,1466441762,"I am building a neural network and training to recognize the OR operator , it seems like i have errors with the Mathematics of my CostFunction it gets stuck to a constant depending on the number of hidden neurons also I havent figured out an efficient way to train it and the ouput is all ones or zeros .

(Pastebin): http://pastebin.com/7ndwpVCW
(Gist):https://gist.github.com/konkovac/19d82eebf3368586628e6120fc915596

",6,1
741,2016-6-21,2016,6,21,2,4oztwi,Twitter's CEO: Increasing our Investment in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4oztwi/twitters_ceo_increasing_our_investment_in_machine/,__azrael,1466443032,,15,21
742,2016-6-21,2016,6,21,2,4ozyeq,"This Week in ML &amp; AI Podcast: Apple's new NN APIs, Generative Adversarial Nets + more",https://www.reddit.com/r/MachineLearning/comments/4ozyeq/this_week_in_ml_ai_podcast_apples_new_nn_apis/,sbc1906,1466444493,,0,0
743,2016-6-21,2016,6,21,2,4p00k8,Improving Nudity Detection and NSFW Image Recognition,https://www.reddit.com/r/MachineLearning/comments/4p00k8/improving_nudity_detection_and_nsfw_image/,felixthursday,1466445162,,3,11
744,2016-6-21,2016,6,21,3,4p03si,Any thoughts on Machine Learning Mastery?,https://www.reddit.com/r/MachineLearning/comments/4p03si/any_thoughts_on_machine_learning_mastery/,Sig_Luna,1466446188,"Hey guys
I came across the site http://machinelearningmastery.com/. It seems to be pretty high ranked.
I really like the concept and therefore I am thinking about purchasing some books from it (the two python ones). Does anyone have already purchased them? What do you think about it?",5,2
745,2016-6-21,2016,6,21,3,4p03xp,Phrases that suggest interests and opinions?,https://www.reddit.com/r/MachineLearning/comments/4p03xp/phrases_that_suggest_interests_and_opinions/,Idocabo,1466446231,"I have no idea if this is the right place to ask, but I'm wondering if there's some sort of list of phrases that suggest interests or opinions. For example, ""I like/love/hate ..."" or ""I believe ..."" or ""my favorite ...""

Is there a name for these? And is there a resource that can help me? Thanks!",2,0
746,2016-6-21,2016,6,21,3,4p0cug,What are for you some late great achievement of Deep Learning ?,https://www.reddit.com/r/MachineLearning/comments/4p0cug/what_are_for_you_some_late_great_achievement_of/,swentso,1466449121,"Hi,

I'm presenting deep learning in few days, and I need to state some of the greatest achievement of it.  i.e. problems (like imagenet contest) where DeepLearning techniques got far better results than classical ones, which convinced people to switch mostly to DL. What do you guys think are the most striking ?

Thanks =)",4,0
747,2016-6-21,2016,6,21,4,4p0gzh,Human learning power consumption is better... by 2022?,https://www.reddit.com/r/MachineLearning/comments/4p0gzh/human_learning_power_consumption_is_better_by_2022/,[deleted],1466450455,[deleted],0,1
748,2016-6-21,2016,6,21,4,4p0iy7,Human learning power consumption is better... until 2022?,https://www.reddit.com/r/MachineLearning/comments/4p0iy7/human_learning_power_consumption_is_better_until/,michal_sustr,1466451096,,1,0
749,2016-6-21,2016,6,21,5,4p0p1n,Best Crowdsourcing Platforms?,https://www.reddit.com/r/MachineLearning/comments/4p0p1n/best_crowdsourcing_platforms/,juharris,1466453105,What are some good crowdsourcing platforms out there? I'm looking for something like CrowdFlower.,0,0
750,2016-6-21,2016,6,21,5,4p0soh,Text Captcha Decoder And Solver,https://www.reddit.com/r/MachineLearning/comments/4p0soh/text_captcha_decoder_and_solver/,articlefr,1466454317,,0,1
751,2016-6-21,2016,6,21,5,4p0t3z,Suggestions on building a high budget machine learning server for Matlab,https://www.reddit.com/r/MachineLearning/comments/4p0t3z/suggestions_on_building_a_high_budget_machine/,hellolin,1466454467,"Hi forum, I was just tasked with great money at hands to build a deep learning workstation/server for use with Matlab software, it is going to be used with processing digital signals and the machine learning toolkit. I am just trying to get some recommendations on what's the hardware requirement for such a workstation. The company is not allowed to use the cloud so I have to build this workstation up, also includes monitors(Will one monitors be enough?). Also any suggestions on how to use Matlab for deep learning is appreciated too, thanks!

My budget is ranging from $8000-$15000, yes the budget is very flexible and it's for industrial use. ",4,0
752,2016-6-21,2016,6,21,5,4p0vml,What type of Machine Learning video course would you like to see? (Siraj from Sirajology on Youtube),https://www.reddit.com/r/MachineLearning/comments/4p0vml/what_type_of_machine_learning_video_course_would/,llSourcell,1466455304,"Hey guys,

I quit my job to work on Developer Education videos full-time on my youtube channel. I'm currently doing a 'Machine Learning for Hackers' series. The goal is to inspire and equip developers to incorporate machine learning into their toolset. 

The series is nearing completion and I'd like to do another. What kind of machine learning course would you personally watch?

A course focused on 

research papers?
applications?
theory?

link to my channel in case you haven't seen:
https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A



",16,7
753,2016-6-21,2016,6,21,6,4p12ek,Building Nvidia DIGITS 4.0rc1,https://www.reddit.com/r/MachineLearning/comments/4p12ek/building_nvidia_digits_40rc1/,Greendogo,1466457651,"Hey, does anyone have a good link for building NVIDIA's DIGITS from source?  I'm hitting my head against a wall trying to get the provided Docker files to work, so maybe building from source will turn out to be easier...",7,1
754,2016-6-21,2016,6,21,8,4p1j2r,Recursive (not recurrent!) neural nets in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4p1j2r/recursive_not_recurrent_neural_nets_in_tensorflow/,AlNejati,1466463789,,8,19
755,2016-6-21,2016,6,21,10,4p2286,Need help with model performing worse with training,https://www.reddit.com/r/MachineLearning/comments/4p2286/need_help_with_model_performing_worse_with/,theoptimistprime,1466471403,"I am trying to learn linear function approximator for [miner's problem using emphatic TD](http://arxiv.org/pdf/1503.04269.pdf). With training, I get good results early on and then performance starts to degrade. Can someone help me understand this observation or point out the error in my implementation?

Code: http://pastebin.com/g3FjxXnR

Edit: I get [this](http://imgur.com/i8cdvmO) kind of plot",0,0
756,2016-6-21,2016,6,21,10,4p27yk,"Symposium: Deep Learning ,Neural Turing Machines - Alex Graves",https://www.reddit.com/r/MachineLearning/comments/4p27yk/symposium_deep_learning_neural_turing_machines/,nigh8w0lf,1466473751,,2,7
757,2016-6-21,2016,6,21,13,4p2rlb,Baidu presented a super fast persistent RNN at ICML.,https://www.reddit.com/r/MachineLearning/comments/4p2rlb/baidu_presented_a_super_fast_persistent_rnn_at/,DATAh4ck3r,1466481883,,24,51
758,2016-6-21,2016,6,21,13,4p2rwq,Google at ICML 2016,https://www.reddit.com/r/MachineLearning/comments/4p2rwq/google_at_icml_2016/,DATAh4ck3r,1466482031,,0,3
759,2016-6-21,2016,6,21,13,4p2w0l,Fully Automatic Paper Plate Making Machines Manufacturers in Delhi,https://www.reddit.com/r/MachineLearning/comments/4p2w0l/fully_automatic_paper_plate_making_machines/,paperplatemachines,1466483874,,3,0
760,2016-6-21,2016,6,21,14,4p3083,"""Machinelearning gets a boost from Intel's Parallel Computing Lab""",https://www.reddit.com/r/MachineLearning/comments/4p3083/machinelearning_gets_a_boost_from_intels_parallel/,fulcrum_xyz,1466485924,,2,0
761,2016-6-21,2016,6,21,15,4p379x,Regular furnace maintenance is indispensable if you want to avoid costly repairs or replacements. We offer furnace maintenance services that can help prepare your furnace for the cold season and keep problems at bay.,https://www.reddit.com/r/MachineLearning/comments/4p379x/regular_furnace_maintenance_is_indispensable_if/,Progasnorthshore,1466489556,,0,1
762,2016-6-21,2016,6,21,15,4p37ou,[Request] Looking for a dataset for name entity disambiguation other than author names.,https://www.reddit.com/r/MachineLearning/comments/4p37ou/request_looking_for_a_dataset_for_name_entity/,mighty_drogon,1466489770,"I've been searching for a dataset for quiet some time now, I've found many for author name disambiguation.",1,0
763,2016-6-21,2016,6,21,15,4p3a4y,[Question] Analyzing normalized RDB data using an ML algorithm?,https://www.reddit.com/r/MachineLearning/comments/4p3a4y/question_analyzing_normalized_rdb_data_using_an/,Vibze,1466491065,"I have a very little understanding of the subject, so I wanted to get opinion of knowlegeable people on this.
I want to analyze some tech support ticket data and make some customer predictions based on it.

I got list of all the customers and each customer might have several tickets.
Each ticket has following attributes:

* Timestamp
* Problem type
* Time took to solve 
* Person who was responsible for solving 

Traditionally, I would go and form some independent variables for each customer like 

* number of tickets
* number of tickets of certain type
* number of tickets solved in certain time
* 

But this spawns a huge number of possible combinations like number of tickets of certain type solved in certain time and so on.
So I thought of taking each customer and transforming his tickets into a single vector like this:
    
    &lt;number of tickets&gt;&lt;ticket1_timestamp&gt;&lt;ticket1_problem_type_id&gt;&lt;ticket_1_time_to_solve&gt;&lt;ticket2_timestamp&gt; etc

Every value has a fixed amount of bits dedicated to it and the rest is zeropadded. This way Im not forming any variables based on my assumptions of what might work, but feed raw data to the ML algorithm.

What do you think of this solution? Does it seem acceptable to you?

I tried searching the internet for similar cases but didnt get anything at all, so I assumed that this kind of thing isnt common or is just a crappy solution.",0,0
764,2016-6-21,2016,6,21,16,4p3enc,[Advice] Library for Training Word2Vec on GPU?,https://www.reddit.com/r/MachineLearning/comments/4p3enc/advice_library_for_training_word2vec_on_gpu/,downvoteyoubaby,1466493549,"Hi, we are trying to train word embedding using a large corpus (over 10B tokens) using skip-gram with negative sampling, however, we find that gensim's implementation is not as performant as we thought, even with Cython installed. We are now at about 150k words/sec on 16 core node, meaning just going over the full corpus will take days, and we find that the cores are not fully utilized(about 2.x performance increase than single core).

So here are my questions:
1).Is this performance normal for gensim? Maybe we didn't handle some dependencies right?

2).We are also looking into GPU solutions out there, after seeing some very enticing benchmark(some reported 1M words/s). But after some researching, we find that in comparison to gensim, there isn't too much documentation and use cases reported out there, would love to hear you guys experience with computing this on GPU.

Any advice is welcome!",8,0
765,2016-6-21,2016,6,21,16,4p3h8p,"Code available for ""R-FCN: Object Detection via Region-based Fully Convolutional Networks""",https://www.reddit.com/r/MachineLearning/comments/4p3h8p/code_available_for_rfcn_object_detection_via/,flyforlight,1466495032,"GitHub repo: https://github.com/daijifeng001/R-FCN

arXiv tech report: http://arxiv.org/abs/1605.06409

R-FCN: Object Detection via Region-based Fully Convolutional Networks

Jifeng Dai, Yi Li, Kaiming He, Jian Sun

We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets), for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20x faster than the Faster R-CNN counterpart.",0,7
766,2016-6-21,2016,6,21,17,4p3n5y,[1606.05908] Tutorial on Variational Autoencoders,https://www.reddit.com/r/MachineLearning/comments/4p3n5y/160605908_tutorial_on_variational_autoencoders/,[deleted],1466498786,[deleted],1,1
767,2016-6-21,2016,6,21,17,4p3o5x,Problem with octave while using it for linear and logistic regression.,https://www.reddit.com/r/MachineLearning/comments/4p3o5x/problem_with_octave_while_using_it_for_linear_and/,an0nym0usv,1466499417,"Hi, I am doing a machine learning course on couesera. While solving the assignment especially when executing my code, when octave encounters cost function, or while solving gradient descent or while using fminunc function. It keeps displaying the vector every iteration and keeps pausing and I've to keep pressing 'f' key to keep the program running. Is there anyway to skip the whole displaying part and go directly to the end and get the solution? 

I am using octave 4.0.1
Thanks in advance!",6,0
768,2016-6-21,2016,6,21,21,4p48ht,One Year as a Data Scientist at Stack Overflow,https://www.reddit.com/r/MachineLearning/comments/4p48ht/one_year_as_a_data_scientist_at_stack_overflow/,iamkeyur,1466510736,,26,167
769,2016-6-21,2016,6,21,21,4p4ard,What's the proper way to deal with training dataset placement in distributed training,https://www.reddit.com/r/MachineLearning/comments/4p4ard/whats_the_proper_way_to_deal_with_training/,ares0943,1466511641,"Hello All,
I am new and currently learning distributed training using TensorFlow. There is very little documentations talking about the dataset placement. My understanding is that if we just talk about data parallelism, the batch is divided to mini-batch and assigned to each worker/device. If there is only one copy of the training data, doesn't that mean there will be a lot of data transfer over the network? Is that efficient? However, if there is one dataset copy on each worker, that will require a lot of disk space.  What's the recommend way to handle dataset for distributed training? ",0,0
770,2016-6-21,2016,6,21,22,4p4keb,"Weka SMO probability distribution for a 10-class multiclass, caps at 0.2.",https://www.reddit.com/r/MachineLearning/comments/4p4keb/weka_smo_probability_distribution_for_a_10class/,ml_ta,1466515376,"I previously used weka's RandomForest to test the set and I got probabilities around 0.6-1.0. When I use the same datasets with the SMO class, the probabilities cap at 0.2. (Accuracy is about the same)

I know RF is a natural multiclass classifier so it should perform quite well, but I thought the weka SMO was adjusted to work as multiclass classifier (1v1) as well.

Is there anything I can do in weka ? Or can you point me towards a library that has reliable multiclass implementations of SVMs ?",0,0
771,2016-6-21,2016,6,21,22,4p4nau,Coursera and Edx ( scala and Spark ) specialization courses started.,https://www.reddit.com/r/MachineLearning/comments/4p4nau/coursera_and_edx_scala_and_spark_specialization/,dataaspirant,1466516458,,0,2
772,2016-6-21,2016,6,21,23,4p4vao,"[Question] Storing, sharing and publishing large data sets, where?",https://www.reddit.com/r/MachineLearning/comments/4p4vao/question_storing_sharing_and_publishing_large/,carlthome,1466519284,"How do you guys store your data sets? I've been cramming a terabyte of chunky samples into Dropbox but it's turning unwieldy, especially when collaborating with others. Are there git-inspired tools purposefully designed for bookkeeping large quantities of data, that's typically written infrequently by some collaborators, but read frequently by public users (and that don't cost a fortune)?

EDIT: Thanks for the downvotes /r/MachineLearning. I love you all.",2,0
773,2016-6-21,2016,6,21,23,4p4x9u,Has anyone asked what human language would be easiest to teach a computer?,https://www.reddit.com/r/MachineLearning/comments/4p4x9u/has_anyone_asked_what_human_language_would_be/,VectorLightning,1466519991,"Because grammar is weird. 

I was thinking about this a bit, what if someone developed a human written language designed for binary? Each word would be composed of bits that narrow down the meaning of the word  la 20 Questions (Is it a descriptive word? 0 Is it a thing? 1 Is it a person? 1), and first 3 bits in a word could determine the part of speech (noun verb adjective adverb conjunction preposition query... Let's add ""shorthand for common words""), and the last two could determine if it's the subject or predicate (which noun got verbed?). With each word's role being defined *within the word itself*, who needs grammar? 

But this is a bit of a ramble. Is there a language that seems like it'd be simpler for computers to learn? ",10,0
774,2016-6-21,2016,6,21,23,4p50lv,Genetic algorithms work on function parameters. But can it work on the functions themselves?,https://www.reddit.com/r/MachineLearning/comments/4p50lv/genetic_algorithms_work_on_function_parameters/,DoctorShinobi,1466521136,"I've been interested in learning some basic machine learning lately. I decided I want to start by learning about genetic algorithms. I've read the next tutorial for creating an AI that learns to walk in unity :
http://www.alanzucconi.com/2016/04/06/evolutionary-coputation-1/


In this tutorial there's a creature which has legs. The legs are moved using a mathematical function which receives 4 parameters. Those parameters are subject to the evolution process applied by this tutorial. But reading this i couldn't help but wonder  if genetic algorithms can create functions? This seems to make more sense and put less limits on what the AI can learn.",5,1
775,2016-6-22,2016,6,22,0,4p58kc,End to End Neural Art with Generative Models,https://www.reddit.com/r/MachineLearning/comments/4p58kc/end_to_end_neural_art_with_generative_models/,phunter_lau,1466523789,,4,16
776,2016-6-22,2016,6,22,0,4p58se,Machine Learning That Matters,https://www.reddit.com/r/MachineLearning/comments/4p58se/machine_learning_that_matters/,MrTwiggy,1466523862,"[Arxiv Link](http://arxiv.org/abs/1206.4656)

**Title:** Machine Learning That Matters

**Abstract:** Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field?s energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters.

**Author:** Kiri Wagstaff

Just recently came across this very interesting read that I think still has a lot of relevance. I think there are clearly a lot of areas in which machine learning has a large impact, but there's much to be said for encouraging more application oriented areas. I've only tended to read more theoretical ML-focused papers, but have found a trove of interesting papers [at MLIA from NASA](http://ml.jpl.nasa.gov/). 

If anyone has some recommendations for areas of research where machine learning is being applied for world impact, particularly for astronomical purposes or global effect, I'd love some resources. 

",9,3
777,2016-6-22,2016,6,22,1,4p5etw,How WSJ Used an Algorithm to Analyze Hamilton the Musical,https://www.reddit.com/r/MachineLearning/comments/4p5etw/how_wsj_used_an_algorithm_to_analyze_hamilton_the/,mbierly,1466525783,,1,0
778,2016-6-22,2016,6,22,1,4p5jfe,Rogue Machine Intelligence and a New Kind of Hedge Fund,https://www.reddit.com/r/MachineLearning/comments/4p5jfe/rogue_machine_intelligence_and_a_new_kind_of/,klihu,1466527234,,31,53
779,2016-6-22,2016,6,22,3,4p60rg,Sentiment analysis using tensorflow,https://www.reddit.com/r/MachineLearning/comments/4p60rg/sentiment_analysis_using_tensorflow/,asap0616,1466532444,"MY implementation of the Recursive Neural Net for sentiment analysis described in ""Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"". 

In comparison to sequential LSTM models, this implementation is slow. Does anybody know of a faster implementation of tree structured LSTM models? 

https://github.com/sapruash/RecursiveNN
",0,0
780,2016-6-22,2016,6,22,3,4p691y,[SURVEY] How do you interact with data at work? (x/post r/datascience),https://www.reddit.com/r/MachineLearning/comments/4p691y/survey_how_do_you_interact_with_data_at_work/,talameetsbetty,1466535026,"Hello fellow data workers!
Lately Ive been getting rather frustrated with some things at work, and was wondering if this was endemic to just my workplace, or to the field as a whole. Like a good statistician, Im reaching out to all of you in the hopes that youll answer a 5 minute (okay, so far it takes the average responder 6.5 minutes to finish), 16 question survey, but like a bad statistician, the input text fields are free form. For every person who fills out the survey, Ill donate $1 to CodeNow, a non-profit that helps inner city kids learn to program (up to $1000).

[Survey here. Thanks in advance for the help!](https://docs.google.com/forms/d/1dT4FuUgYTPblxCGFSWs1kmMVC28ZxDjEsY0kCMV4mNQ/viewform)



Sorry for formatting; on mobile. ",0,0
781,2016-6-22,2016,6,22,4,4p6bs2,Deep reinforcement learning and applications,https://www.reddit.com/r/MachineLearning/comments/4p6bs2/deep_reinforcement_learning_and_applications/,[deleted],1466535852,[deleted],4,0
782,2016-6-22,2016,6,22,4,4p6dm9,Real use of deep reinforcement learning?,https://www.reddit.com/r/MachineLearning/comments/4p6dm9/real_use_of_deep_reinforcement_learning/,[deleted],1466536422,[deleted],0,0
783,2016-6-22,2016,6,22,4,4p6ecq,Neural networks with differentiable structure,https://www.reddit.com/r/MachineLearning/comments/4p6ecq/neural_networks_with_differentiable_structure/,[deleted],1466536666,[deleted],0,0
784,2016-6-22,2016,6,22,4,4p6ffl,Are there any well written papers with the corresponding code available?,https://www.reddit.com/r/MachineLearning/comments/4p6ffl/are_there_any_well_written_papers_with_the/,Deinos_Mousike,1466536996,"I've been trying to read through some ML papers of various topics (read through one, see its important references, read those and repeat) and would love to see the code they used to better associate the math to the programming.

Are there any resources for this? I have a feeling a lot of the papers' code will be proprietary.

Thanks",6,2
785,2016-6-22,2016,6,22,5,4p6ojr,"Alternative Embedded/SOC Platform to Jetson, for ML?",https://www.reddit.com/r/MachineLearning/comments/4p6ojr/alternative_embeddedsoc_platform_to_jetson_for_ml/,cathalgarvey,1466539943,"Hi all,

I'm eager to get some hardware I can use to noodle around for ML, but I don't have enough money to assemble a computer in which to mount a nice AMD/Radeon GPU.

nVidia's iNsidious marketing has plastered the ML domain with talk of their Jetson devboard. As much as I want to avoid proprietary platforms like CUDA and would rather support OpenCL-competent providers, it's just barely within my price range and appears to be just right for my needs, overall; small, easy to use, and versatile.

I would far rather OpenCL / AMD, but I don't see anyone else making hardware like the Jetson, let alone at a comparable price. Am I missing something, or are nVidia the only people doing affordable ML-capable hardware?

Thanks!",8,0
786,2016-6-22,2016,6,22,5,4p6ppi,Enron Spam Filtering Demo - The perils of over-reliance on AUC when evaluating models,https://www.reddit.com/r/MachineLearning/comments/4p6ppi/enron_spam_filtering_demo_the_perils_of/,ddcarnage,1466540312,,0,0
787,2016-6-22,2016,6,22,5,4p6rsm,Neural networks with differentiable structure,https://www.reddit.com/r/MachineLearning/comments/4p6rsm/neural_networks_with_differentiable_structure/,Selerax,1466540989,,6,13
788,2016-6-22,2016,6,22,5,4p6vhs,Intel pits monster 72-core Xeon Phi chip against GPUs,https://www.reddit.com/r/MachineLearning/comments/4p6vhs/intel_pits_monster_72core_xeon_phi_chip_against/,pilooch,1466542222,,79,128
789,2016-6-22,2016,6,22,5,4p6w4n,I think it's one of the best C++ native build systems,https://www.reddit.com/r/MachineLearning/comments/4p6w4n/i_think_its_one_of_the_best_c_native_build_systems/,[deleted],1466542423,[deleted],0,1
790,2016-6-22,2016,6,22,6,4p6yp8,Using machine learning to find sloppy crowdsourced testing work [video],https://www.reddit.com/r/MachineLearning/comments/4p6yp8/using_machine_learning_to_find_sloppy/,EmilieC-J,1466543280,,0,2
791,2016-6-22,2016,6,22,7,4p78ln,"ML + AI advances 2016, huge compilation",https://www.reddit.com/r/MachineLearning/comments/4p78ln/ml_ai_advances_2016_huge_compilation/,vznvzn,1466546538,,1,0
792,2016-6-22,2016,6,22,7,4p7bw1,For Well Thou Know'st: Livestream of ongoing machine learning in Keras,https://www.reddit.com/r/MachineLearning/comments/4p7bw1/for_well_thou_knowst_livestream_of_ongoing/,vanboxel,1466547651,,0,3
793,2016-6-22,2016,6,22,8,4p7jm2,[Q] Recent advancement in text classification using DL?,https://www.reddit.com/r/MachineLearning/comments/4p7jm2/q_recent_advancement_in_text_classification_using/,habitats,1466550334,"Hi

I'm currently doing research in multi-label text classification, trying to primarily classify short texts (think tweets) with LSTMs/FFNs using a variety of Word2Vec-based features, and looking for related material.

Anyone got any interesting papers I could get my hands on? I've been searching Scholar a lot, but haven't found much relating these methods to the domain of text classification, so I thought I'd ask here.

Thanks:) ",2,7
794,2016-6-22,2016,6,22,9,4p7vy6,How do corporate ML researchers get approval to publish papers?,https://www.reddit.com/r/MachineLearning/comments/4p7vy6/how_do_corporate_ml_researchers_get_approval_to/,FuzziCat,1466554845,I've been curious about the process from an IP perspective.  What factors are considered before a company allows certain ML research results to be made public? ,12,1
795,2016-6-22,2016,6,22,9,4p820c,Model prediction vs. actually understanding your data,https://www.reddit.com/r/MachineLearning/comments/4p820c/model_prediction_vs_actually_understanding_your/,thundercorp,1466557182,,2,3
796,2016-6-22,2016,6,22,11,4p8evn,Can someone suggest a strategy for this text-mining problem?,https://www.reddit.com/r/MachineLearning/comments/4p8evn/can_someone_suggest_a_strategy_for_this/,Raven_Dragon,1466562478,"Hey everyone, I'm new to ML.
I'm working on a project for my lab, and this will be my first ML project.

I have a database with descriptions for transport proteins. In the text, there are biomolecule substrates mentioned in text. I'm developing an ontology for chemical substrates among these entries, so I need to do the following

1) Identify what words are molecular entities. This is not always obvious, I can tell by looking at the word, or by it's context. Sometimes they are between 1-3 words. Sometimes the suffix/prefix gives it away. Other times, the context is the only hint.

2) To make matters even worse, sometimes these molecules are mentioned as inhibitors and not substrates of the transporter! This is important data too, I would like to keep track of this type of information as well. This is indicated in a multitude of ways =s

There are over 10k entries in this database. I've written some crazy regex patterns to capture as many situations as I can, but there is so much variation in writing style and I'm having an impossible time capturing all this information.

Is there an ML approach to this? I'm a very quick learner, and I've got the basics down by watching some videos and examples.

Any inspiration or starting pointers would be much appreciated!

Thank you.

-RD",1,0
797,2016-6-22,2016,6,22,12,4p8q77,can someone explain simultaneous update in a way a person who has not yet taken multivariable calculus can understand?,https://www.reddit.com/r/MachineLearning/comments/4p8q77/can_someone_explain_simultaneous_update_in_a_way/,jenntompkins095,1466567344,some explanation of multivariable is obviously necisarry. I think i can understand the concept if it explained in the right way. I am taking a machine learning course and I cannot grasp simultaneous update because it seems kinda poorly explained. ,1,0
798,2016-6-22,2016,6,22,14,4p902b,[Google/OpenAI/Stanford/UC Berkeley] Concrete Problems in AI Safety,https://www.reddit.com/r/MachineLearning/comments/4p902b/googleopenaistanforduc_berkeley_concrete_problems/,downtownslim,1466571845,,1,0
799,2016-6-22,2016,6,22,15,4p97si,Predictive Dropout,https://www.reddit.com/r/MachineLearning/comments/4p97si/predictive_dropout/,alexmlamb,1466575639,,7,0
800,2016-6-22,2016,6,22,15,4p9aau,SG2513 uv flatbed printer iphone case making machine 2.5*1.3m,https://www.reddit.com/r/MachineLearning/comments/4p9aau/sg2513_uv_flatbed_printer_iphone_case_making/,emmaluo--copier,1466576965,,0,0
801,2016-6-22,2016,6,22,15,4p9bb6,Multi-layer SVMs?,https://www.reddit.com/r/MachineLearning/comments/4p9bb6/multilayer_svms/,Icko_,1466577511,"SVMs look pretty similar to regression, except they optimize margin and not probability, and they use polynomial feature transformation. Since multi-layer perceptrons are essentially multi-layer regression with nonlinear transformation each layer (e.g. sigmoid), creating the same thing with SVMs sounds easy - especially since they can be trained with backpropagation. However, I only found papers with less than 10 citations, so maybe they go by a different name, or I'm wrong somewhere. Does something like this exist, and if so, how does it perform?",9,14
802,2016-6-22,2016,6,22,16,4p9epv,technical question -- keras: how to interchange b/w theano + tensorflow,https://www.reddit.com/r/MachineLearning/comments/4p9epv/technical_question_keras_how_to_interchange_bw/,bionerd2,1466579303,"I've implemented a convnet in keras, using the theano backend. What do I do to switch to the tensorflow backend? I tried chaning the config flag, but that yielded a number of errors (seg faults). It looks like there is an abstract api, but the documentation is bad. Does anyone have experience?",2,0
803,2016-6-22,2016,6,22,17,4p9l83,[1606.06724] Tagger: Deep Unsupervised Perceptual Grouping,https://www.reddit.com/r/MachineLearning/comments/4p9l83/160606724_tagger_deep_unsupervised_perceptual/,pranv,1466582939,,3,20
804,2016-6-22,2016,6,22,17,4p9mkc,How to measure the technical parameter of dispersion mixer,https://www.reddit.com/r/MachineLearning/comments/4p9mkc/how_to_measure_the_technical_parameter_of/,mixmachinery,1466583745,,1,1
805,2016-6-22,2016,6,22,17,4p9nyo,Functional analysis in machine learning.,https://www.reddit.com/r/MachineLearning/comments/4p9nyo/functional_analysis_in_machine_learning/,huyhcmut,1466584552,"Recently, i began to read a book on functional analysis, and i found the word *convolution* in one chapter. I don't know whether is has some meaning in common with *convolutional neural network* in computer vision. I guess CNNs have the roof from this ( and of course, from neural net in general). Is that right?",1,0
806,2016-6-22,2016,6,22,17,4p9p1f,Choose right resources of packing machine,https://www.reddit.com/r/MachineLearning/comments/4p9p1f/choose_right_resources_of_packing_machine/,Kimiliu,1466585205,[removed],1,0
807,2016-6-22,2016,6,22,18,4p9s7w,"Concrete AI Safety Problems (OpenAI, Google Brain)",https://www.reddit.com/r/MachineLearning/comments/4p9s7w/concrete_ai_safety_problems_openai_google_brain/,sour_losers,1466587083,,4,10
808,2016-6-22,2016,6,22,18,4p9tty,High Class Refrigerated Air Dryer Manufacturer,https://www.reddit.com/r/MachineLearning/comments/4p9tty/high_class_refrigerated_air_dryer_manufacturer/,swatidasgupta,1466588025,[removed],0,1
809,2016-6-22,2016,6,22,20,4pa38q,Very performant NEAT implementation,https://www.reddit.com/r/MachineLearning/comments/4pa38q/very_performant_neat_implementation/,Mafiii,1466593351,,26,31
810,2016-6-22,2016,6,22,20,4pa998,L2 regularization,https://www.reddit.com/r/MachineLearning/comments/4pa998/l2_regularization/,Kiuhnm,1466596501,"In a recent [book about DL](http://www.deeplearningbook.org/), in chapter 7, page 231, footnote 1, regarding L2 regularization, the authors say:
&gt; More generally, we could regularize the parameters to be near any specific point in space and, surprisingly, still get a regularization effect, but better results will be obtained for a value closer to the true one, with zero being a default value that makes sense when we do not know if the correct value should be positive or negative.

What they're saying is that ||W-c||^2 has regularization effects also for c != 0.

I sent the authors a lot of errata-corrige plus this note:
&gt; I don't know much about ML and DL (that's why I'm reading your book!), but I don't think it's surprising.
Let's say that S is the set of all w such that f(w)'s training error is &lt; eps.
I suspect that in S there are ""more"" w's relative to simple functions than w's relative to complex ones. So, if we pick a w_0 at random and it happens to be in S, then f(w_0) is probably simple. To find a complex function g which reduces the training error substantially, we may need to get far from w_0.

Since the authors never replied to my email, I've decided, after 3 months, to ask here.

Do you think that my conjecture has any value or am I completely off track?",7,7
811,2016-6-22,2016,6,22,21,4pafm6,How to localize objects given only class labels,https://www.reddit.com/r/MachineLearning/comments/4pafm6/how_to_localize_objects_given_only_class_labels/,matrix2596,1466599228,"Hi, I have a dataset of images with two classes (defective and non-defective). I have trained a CNN based classifier which gives good accuracy. Now I would like to localize the defect in a image (if defective). For example, I have a metal case which has defects like scratches, dents etc. Now i collected samples in which I have defective images and non-defective images. I trained a classifier  which tells me defective images. Can I localize the defects for further inspection. Can this be done without annotating the defective samples with bounding boxes of defects?",3,8
812,2016-6-22,2016,6,22,21,4pafxn,The choice of baseline for REINFORCE?,https://www.reddit.com/r/MachineLearning/comments/4pafxn/the_choice_of_baseline_for_reinforce/,gmkim90,1466599369,"Paper 'Reinforcement Learning Neural Turing Machine' (available in  http://arxiv.org/pdf/1505.00521v3.pdf) summary some tricks for reducing variance of REINFORCE algorithm in the Appendix A.

Specifically, in 'ONLINE BASELINE PREDICTION' section, paper derives closed form solution for b_t, which results in vector with size #theta.
However, the last line says, practically b_t is chosen as scalar (which seems expected future rewards).

Is there anyone who can provide any reference or explanation how this can substitute closed form solution of b_t (as a vector) ?

I really appreciate your help :)",0,2
813,2016-6-22,2016,6,22,22,4panlv,How to Find Periodicity in Your Data: An Introduction to Data Analysis III,https://www.reddit.com/r/MachineLearning/comments/4panlv/how_to_find_periodicity_in_your_data_an/,AlanZucconi,1466602371,,14,82
814,2016-6-22,2016,6,22,22,4paq30,How AT&amp;amp;T is Leveraging Machine Learning and Cloud-Based Services for Business,https://www.reddit.com/r/MachineLearning/comments/4paq30/how_atampt_is_leveraging_machine_learning_and/,levinmcarthur,1466603266,,0,0
815,2016-6-22,2016,6,22,22,4paq65,"The Analytics of Language, Behavior, and Personality: Computational linguist Jason Baldridge, interviewed",https://www.reddit.com/r/MachineLearning/comments/4paq65/the_analytics_of_language_behavior_and/,SethGrimes,1466603293,,1,1
816,2016-6-22,2016,6,22,23,4patpw,"Thermocol Automatic Paper Plate Machines Manufacturers in Delhi, Noida",https://www.reddit.com/r/MachineLearning/comments/4patpw/thermocol_automatic_paper_plate_machines/,paperplatemachines,1466604540,,1,1
817,2016-6-22,2016,6,22,23,4paxkq,[1606.05908] Tutorial on Variational Autoencoders,https://www.reddit.com/r/MachineLearning/comments/4paxkq/160605908_tutorial_on_variational_autoencoders/,cdoersch,1466605846,,29,78
818,2016-6-22,2016,6,22,23,4payod,I made this from things I hear people say about AI - what do you think?,https://www.reddit.com/r/MachineLearning/comments/4payod/i_made_this_from_things_i_hear_people_say_about/,zyrumtumtugger,1466606205,,20,52
819,2016-6-22,2016,6,22,23,4pb2ok,Paper + code demonstrating bidirectional long short-term memory from scratch in TensorFlow [OC],https://www.reddit.com/r/MachineLearning/comments/4pb2ok/paper_code_demonstrating_bidirectional_long/,rd11235,1466607573,"Hi all,

I hope this code/project might be helpful to some, especially to those who
have asked for a less toy-like example of the RNN-from-scratch example here:
http://rdipietro.github.io/tensorflow-scan-examples/

Goal: Take in a sequence of robot kinematics from a surgeon/trainee and predict
his or her activities, for example 00:00:00 to 00:02:29 consists of tying a
knot, 00:02:30 to 00:03:22 consists of doing something else, etc. We're
particularly interested in the case where the surgeon/trainee is completely
absent during training (not even including his or her other trials).

Approach in this paper: Bidirectional long short-term memory.

Paper available here: http://arxiv.org/abs/1606.06329

Project available here: https://github.com/rdipietro/miccai-2016-surgical-activity-rec

A few notes:

- The LSTMs are built from scratch, primarily because I wanted something I could
easily customize later on (e.g. for convolutional LSTM etc.).
    - Unlike TensorFlow's models, states are maintained internally instead of
      externally.
    - Unlike the simple RNN tutorial above, we can use batches of sequences
      for efficiency.
    - Unlike the official TensorFlow models, other optimizations are not
      included (concatenation and splitting for block matrix multiplies;
      avoiding duplicate computation after a sequence in a batch is exhausted;
      ...).",4,42
820,2016-6-23,2016,6,23,1,4pbk2z,Question about Neural Networks (rectified linear neurons),https://www.reddit.com/r/MachineLearning/comments/4pbk2z/question_about_neural_networks_rectified_linear/,Alamo44,1466613003,"If inputs are always positive how does this behave differently from a normal linear unit? Can a ReLu Network learn something like a sine function or other non linear mathematical functions?

In my example I was trying to give input value x= [0,2pi] and get target value = sine(x)

This is easily done when I set the nodes to a logistic unit (sigmoidal activation) but I seem to only be able to predict a best fit line when I set the activation function to max(0,x) for any unit.  

I also was using 2 layers of 10 hidden nodes. Thanks for any help!",3,0
821,2016-6-23,2016,6,23,1,4pbl9m,What are some good ways to practice python,https://www.reddit.com/r/MachineLearning/comments/4pbl9m/what_are_some_good_ways_to_practice_python/,kailovesdata,1466613391,"I've been learning python for like 3 months, and feeling comfortable to use it more. I was wondering if you guys know of any good resources for me to do tiny projects so that I can practice python more.

Also did anyone ever purchase ""Learn Python The Hard Way""? Is it worth it?

Thank you yall",6,0
822,2016-6-23,2016,6,23,3,4pc3zy,machinery question,https://www.reddit.com/r/MachineLearning/comments/4pc3zy/machinery_question/,[deleted],1466618957,[deleted],0,1
823,2016-6-23,2016,6,23,6,4pd8h3,An introduction to Machine Learning and Neural Networks for iOS in Swift,https://www.reddit.com/r/MachineLearning/comments/4pd8h3/an_introduction_to_machine_learning_and_neural/,evdiasan,1466632137,,0,1
824,2016-6-23,2016,6,23,6,4pd8w8,ANN Brain 2016,https://www.reddit.com/r/MachineLearning/comments/4pd8w8/ann_brain_2016/,keghn,1466632301,,1,0
825,2016-6-23,2016,6,23,7,4pdckv,How Google is Remaking Itself as a Machine Learning First Company,https://www.reddit.com/r/MachineLearning/comments/4pdckv/how_google_is_remaking_itself_as_a_machine/,seojoeschmo,1466633637,,8,6
826,2016-6-23,2016,6,23,10,4pe4qp,Question about training an SVM to predict same/different class,https://www.reddit.com/r/MachineLearning/comments/4pe4qp/question_about_training_an_svm_to_predict/,BaseEmitter,1466644057,"I recently have been working on a problem in which there are 1000+ categories of images and the task is, given two images, to predict if they come from the same class or different classes. The challenge here is that there are classes in the test set that are not present in the training set and visa versa.

Currently my approach is to extract some features using VGG and train an SVM on the difference of the image features

However I'm having trouble deciding how to train the svm. There are 70k training images and so there are about 5 billion combinations of training data I can use for the task. However using the entire set or sampling randomly will result in a training set that is very skewed towards negative examples.

One I train an SVM, I could use hard negative mining to lower the set of negatives but even to train the first SVM, I need to understand this. One option I was thinking would be to evenly sample between positive and negative samples.",8,6
827,2016-6-23,2016,6,23,11,4pedpw,Asynchronous Methods for Deep Reinforcement Learning (surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU),https://www.reddit.com/r/MachineLearning/comments/4pedpw/asynchronous_methods_for_deep_reinforcement/,[deleted],1466647512,[deleted],0,2
828,2016-6-23,2016,6,23,13,4peuq0,collaborative filtering recommendation engine implementation in python,https://www.reddit.com/r/MachineLearning/comments/4peuq0/collaborative_filtering_recommendation_engine/,dataaspirant,1466654533,,0,1
829,2016-6-23,2016,6,23,13,4pezja,Persistent RNNs: Stashing Recurrent Weights On-Chip,https://www.reddit.com/r/MachineLearning/comments/4pezja/persistent_rnns_stashing_recurrent_weights_onchip/,sbt_,1466656807,,7,27
830,2016-6-23,2016,6,23,15,4pff4x,Sunspring - a movie written by Machine learning,https://www.reddit.com/r/MachineLearning/comments/4pff4x/sunspring_a_movie_written_by_machine_learning/,[deleted],1466664632,[deleted],2,0
831,2016-6-23,2016,6,23,15,4pfg1f,Maquinaria industrial hidrulica: caractersticas,https://www.reddit.com/r/MachineLearning/comments/4pfg1f/maquinaria_industrial_hidrulica_caractersticas/,Barriuso,1466665143,,0,1
832,2016-6-23,2016,6,23,16,4pfht1,Two Minute Papers - Hallucinating Images With Deep Learning,https://www.reddit.com/r/MachineLearning/comments/4pfht1/two_minute_papers_hallucinating_images_with_deep/,cs_studentin,1466666065,,6,61
833,2016-6-23,2016,6,23,16,4pfizc,Deep Residual Networks for Image Classification with Python + NumPy,https://www.reddit.com/r/MachineLearning/comments/4pfizc/deep_residual_networks_for_image_classification/,[deleted],1466666704,[deleted],0,1
834,2016-6-23,2016,6,23,16,4pfk47,Deep Residual Networks for Image Classification with Python + NumPy [OC],https://www.reddit.com/r/MachineLearning/comments/4pfk47/deep_residual_networks_for_image_classification/,dnlcrl,1466667308,,0,12
835,2016-6-23,2016,6,23,17,4pfno2,should we use a logarithmic scale on CTC forward-backward variables ?,https://www.reddit.com/r/MachineLearning/comments/4pfno2/should_we_use_a_logarithmic_scale_on_ctc/,[deleted],1466669318,[deleted],0,0
836,2016-6-23,2016,6,23,17,4pfo19,How to be the best at Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/4pfo19/how_to_be_the_best_at_machine_learning/,HelpAboyOUT2222,1466669524,"Everyone talks ""ML"" and says this or that... But how do you really be UP there? I.e. amongst the top dogs in Machine Learning?",14,0
837,2016-6-23,2016,6,23,18,4pftvp,Pretrained CNNs on big dataset,https://www.reddit.com/r/MachineLearning/comments/4pftvp/pretrained_cnns_on_big_dataset/,Tamazy,1466672980,"Hi there,

I'm looking for the latest pretrained CNNs on the hugest supervised datasets namely ImageNet1000, ImageNet2000 (extended version) and MsCoco. It is for research purposes.

Currently, I have:

- [VGG16 on ImageNet1000](https://gist.github.com/ksimonyan/211839e770f7b538e2d8)
- [VGG19 on ImageNet1000](https://gist.github.com/ksimonyan/3785162f95cd2d5fee77)
- [GoogLeNet on ImageNet1000](https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet) 
- [Inception-v3 on ImageNet1000](https://github.com/Moodstocks/inception-v3.torch)
- [ResNet on ImageNet1000](https://github.com/facebook/fb.resnet.torch)

Do you know where I could find pretrained models such as:

- GoogLeNet-v4 on ImageNet1000 (edit: Inception-v4)
- anything on other big dataset

Thanks :)",8,7
838,2016-6-23,2016,6,23,20,4pg6k4,Using openAI Gym with Torch7: a simple example,https://www.reddit.com/r/MachineLearning/comments/4pg6k4/using_openai_gym_with_torch7_a_simple_example/,ludc,1466679983,,5,38
839,2016-6-23,2016,6,23,22,4pgu8v,"[Whitepaper] Data Science in Practice: Five Common Applications of Data Science with Concrete, Real-Life Use Cases",https://www.reddit.com/r/MachineLearning/comments/4pgu8v/whitepaper_data_science_in_practice_five_common/,elisebreda,1466689995,,2,0
840,2016-6-23,2016,6,23,23,4ph4lx,Symptom/Disease Datasets,https://www.reddit.com/r/MachineLearning/comments/4ph4lx/symptomdisease_datasets/,afry316,1466693746,"Hello -

I was wondering if any could recommend a good Symptom to Disease/Illness data set for training. Thanks!",9,0
841,2016-6-23,2016,6,23,23,4ph514,How to read: character level deep learning,https://www.reddit.com/r/MachineLearning/comments/4ph514/how_to_read_character_level_deep_learning/,stafis,1466693903,,5,33
842,2016-6-24,2016,6,24,0,4ph8cq,Variational auto-encoders do not train complex generative models,https://www.reddit.com/r/MachineLearning/comments/4ph8cq/variational_autoencoders_do_not_train_complex/,nil-,1466695012,,14,13
843,2016-6-24,2016,6,24,0,4phc4h,Question Regarding Language Models in Speech Recognition,https://www.reddit.com/r/MachineLearning/comments/4phc4h/question_regarding_language_models_in_speech/,[deleted],1466696278,[deleted],0,0
844,2016-6-24,2016,6,24,1,4phqg3,Machine Learning Engineer Nanodegree course by Google,https://www.reddit.com/r/MachineLearning/comments/4phqg3/machine_learning_engineer_nanodegree_course_by/,Lajamerr_Mittesdine,1466700895,"https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009

Have any of you taken this course or are currently taking it?

I'd like to gather some thoughts on it and see the general opinion on the quality of the program.

As far as I know all the components of the course are free but if you want a certificate then you must pay. So no money required.

I'd like to know if it's worth my and other people's time.",39,131
845,2016-6-24,2016,6,24,2,4phsjs,Random projection of one-hot encodings (aka random embeddings),https://www.reddit.com/r/MachineLearning/comments/4phsjs/random_projection_of_onehot_encodings_aka_random/,[deleted],1466701505,[deleted],0,0
846,2016-6-24,2016,6,24,2,4pi22f,What can't LSTMs do?,https://www.reddit.com/r/MachineLearning/comments/4pi22f/what_cant_lstms_do/,coolwhipper_snapper,1466704339,"I am mostly interested in what the current limits are at applying LSTMs to different tasks that involve time-series. These could be pattern recognition, prediction, or control tasks. 

What do you imagine would be a simple problem that current approaches have trouble tackling or can't tackle yet? Or, if not a specific problem, some pathology or issue that LSTMs have trouble with?

I ask this because I have gotten an okay idea through this subreddit what LSTMs *can* do (though posts and linked papers), but not yet what their current limitations are.",22,1
847,2016-6-24,2016,6,24,2,4pi27b,Teaching machines to anticipate human interactions using videos of TV shows,https://www.reddit.com/r/MachineLearning/comments/4pi27b/teaching_machines_to_anticipate_human/,seojoeschmo,1466704382,,0,15
848,2016-6-24,2016,6,24,3,4pibd4,My data consists of a sequence of tests that can either pass or fail. Which model should I use to predict the next given the order up to that point?,https://www.reddit.com/r/MachineLearning/comments/4pibd4/my_data_consists_of_a_sequence_of_tests_that_can/,atm_vestibule,1466707231,"Each test has can pass or fail, so a potential sequence can look like (test_id, p/f): (1, pass) ==&gt; (4, fail) ==&gt; (5, pass) ==&gt; (6, pass) ==&gt; (1, fail) ==&gt; (4, pass).
    Which model should I use to predict the next one in the following: we executed the above ones and I want to know the prediction of each test failing after this one. I was thinking perceptron could be appropriate, but I'm not sure. k-means clustering I feel could work, especially if I want to add states other than pass/fail like (pass/fail/timeout). New to
machine learning, any suggestions? Thanks.
",3,1
849,2016-6-24,2016,6,24,3,4picdu,What were your favorite papers and talks from ICML 2016?,https://www.reddit.com/r/MachineLearning/comments/4picdu/what_were_your_favorite_papers_and_talks_from/,[deleted],1466707555,[deleted],0,1
850,2016-6-24,2016,6,24,4,4pikyh,Three example ML projects for beginners | Learning Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4pikyh/three_example_ml_projects_for_beginners_learning/,Sig_Luna,1466710265,,1,0
851,2016-6-24,2016,6,24,4,4pils5,How do you share knowledge in your company?,https://www.reddit.com/r/MachineLearning/comments/4pils5/how_do_you_share_knowledge_in_your_company/,boxstabber,1466710526,"Not every project deserves a publication or a white paper even. But that doesn't mean that the things learned conducting it were useless. How and where do you store this knowledge so that other people in the company and newcomers can benefit from it?

For example, AirBnB described their approach in a [blog post](https://medium.com/airbnb-engineering/scaling-knowledge-at-airbnb-875d73eff091#.xmlfy6psq). However, not every company or start-up can afford to build such a customized solution as they have. So how do you handle it?",9,0
852,2016-6-24,2016,6,24,5,4pit4x,Which paper first presents the idea of dilating CNNs learned filters to perform fully convolutional inference?,https://www.reddit.com/r/MachineLearning/comments/4pit4x/which_paper_first_presents_the_idea_of_dilating/,cesarsalgado,1466712891,"I am wondering if the paper ""Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification"" is the first to propose the idea mentioned in this post's title. In this paper's related work section they say that their method is most similar with ""Fast Image Scanning with Deep Max-Pooling Convolutional Neural Networks"", but still different. Do someone familiar with this literature can confirm that? If that is true I am surprised why this paper isn't more famous given the simplicity and efficacy of their method.

link to paper: arxiv.org/abs/1412.4526",3,0
853,2016-6-24,2016,6,24,5,4pitmk,Making Tree Ensembles Interpretable,https://www.reddit.com/r/MachineLearning/comments/4pitmk/making_tree_ensembles_interpretable/,rhiever,1466713056,,4,12
854,2016-6-24,2016,6,24,5,4pixec,Machine Learning Subnets - Finding and using equivalent node network building block structures,https://www.reddit.com/r/MachineLearning/comments/4pixec/machine_learning_subnets_finding_and_using/,[deleted],1466714341,[deleted],0,2
855,2016-6-24,2016,6,24,6,4pj6pl,Machine Learning Subnets - Finding equivalent node network structures and using them as building blocks,https://www.reddit.com/r/MachineLearning/comments/4pj6pl/machine_learning_subnets_finding_equivalent_node/,Lajamerr_Mittesdine,1466717543,,2,1
856,2016-6-24,2016,6,24,7,4pjfb4,A couple questions on Nervanas Neon,https://www.reddit.com/r/MachineLearning/comments/4pjfb4/a_couple_questions_on_nervanas_neon/,Tezz_Sickle,1466720585,"I'm new to the Machine Learning world and my friend and I want to start using Machine Learning for a project. My questions are for Images specifically:
What type of images does Neon take in?
As well as size/resolution.
How much do you have to train your model?
Do you have to crop the images your using?

Also any other resources or tutorials for Neon would be awesome.",0,0
857,2016-6-24,2016,6,24,8,4pjq8g,Machine learning algorithm for regression/predicting future values,https://www.reddit.com/r/MachineLearning/comments/4pjq8g/machine_learning_algorithm_for/,VizTra,1466724643,"Hey, 

right now I'm looking into doing regression to predict future values. Upon searching what would be best used for that I couldn't really find a clear answer. 

I was kinda interested in random forest until I found this post: 
https://www.reddit.com/r/datascience/comments/43rdx2/whats_an_example_where_random_forests_definitely/czkd1ou where I read that they are bad at exactly what I want. 

I'm looking into SVM's and Neural Nets right now, what would you suggest?",4,0
858,2016-6-24,2016,6,24,11,4pkdtz,How could machine learning be used to optimize large-scale inventory management?,https://www.reddit.com/r/MachineLearning/comments/4pkdtz/how_could_machine_learning_be_used_to_optimize/,[deleted],1466733823,[deleted],1,0
859,2016-6-24,2016,6,24,16,4pload,Facebook's Torchnet: Lighting the way to deep machine learning,https://www.reddit.com/r/MachineLearning/comments/4pload/facebooks_torchnet_lighting_the_way_to_deep/,hiteck,1466753519,,13,71
860,2016-6-24,2016,6,24,17,4plsoe,China Automatic Steel Stud Profile Cold Roll Forming Machine,https://www.reddit.com/r/MachineLearning/comments/4plsoe/china_automatic_steel_stud_profile_cold_roll/,stevenzhao,1466755858,,0,1
861,2016-6-24,2016,6,24,17,4pltgv,High quality Motorcycle accessories for your motorcycle,https://www.reddit.com/r/MachineLearning/comments/4pltgv/high_quality_motorcycle_accessories_for_your/,Kimiliu,1466756322,[removed],0,0
862,2016-6-24,2016,6,24,19,4pm62v,"AI, Apple and Google",https://www.reddit.com/r/MachineLearning/comments/4pm62v/ai_apple_and_google/,3eyedravens,1466763493,,0,0
863,2016-6-24,2016,6,24,19,4pm696,8 in 1 heaters instruction,https://www.reddit.com/r/MachineLearning/comments/4pm696/8_in_1_heaters_instruction/,emmaluo--copier,1466763585,,0,0
864,2016-6-24,2016,6,24,21,4pmoox,What are some project ideas that involve machine learning (possibly web application)?,https://www.reddit.com/r/MachineLearning/comments/4pmoox/what_are_some_project_ideas_that_involve_machine/,aaa101101,1466772423,[removed],4,0
865,2016-6-24,2016,6,24,21,4pmpit,Learning pooling,https://www.reddit.com/r/MachineLearning/comments/4pmpit/learning_pooling/,[deleted],1466772747,[deleted],0,0
866,2016-6-24,2016,6,24,22,4pmuaa,"Tensorflow cnn, learns with cpu, does not learn with gpu, mind=blown",https://www.reddit.com/r/MachineLearning/comments/4pmuaa/tensorflow_cnn_learns_with_cpu_does_not_learn/,[deleted],1466774617,[deleted],12,0
867,2016-6-24,2016,6,24,22,4pmvow,Good visualizations of deep learning,https://www.reddit.com/r/MachineLearning/comments/4pmvow/good_visualizations_of_deep_learning/,Pieranha,1466775155,"What are some of the best illustrations/visualizations/animations you've seen that has in a beautiful way - but without being overly simplistic - explained deep learning concepts?

Edit: Great examples for machine learning algorithms would also be highly appreciated!",5,2
868,2016-6-24,2016,6,24,23,4pn50t,Building Smart Applications With Machine Learning: The Definitive Guide,https://www.reddit.com/r/MachineLearning/comments/4pn50t/building_smart_applications_with_machine_learning/,bogsformer,1466778523,,1,0
869,2016-6-24,2016,6,24,23,4pn8mm,Glossary of Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4pn8mm/glossary_of_neural_networks/,abstractgoomba,1466779620,I'm a first year AI master student and I have been learning quite a bit about neural networks this year. One problem that I have though is with the different terms that people use throughout the field and their definitions. I found a  [list of different terms](http://envisat.esa.int/handbooks/meris/CNTR4-2-5.html) and their definitions. However I think the list could be more complete. Does anyone have a better compilation of all the jargon that is used?,2,5
870,2016-6-25,2016,6,25,1,4pnppf,Publicly available Anaphora datasets?,https://www.reddit.com/r/MachineLearning/comments/4pnppf/publicly_available_anaphora_datasets/,[deleted],1466785084,Can anyone point me to some publicly available datasets for anaphora resolution. Corefrence datasets may be of use also. ,6,7
871,2016-6-25,2016,6,25,1,4pnrdk,Regarding Language Models (Phoneme Sequence to Words/Sentences),https://www.reddit.com/r/MachineLearning/comments/4pnrdk/regarding_language_models_phoneme_sequence_to/,flashdude64,1466785610,"I am currently working on creating a speech recognition model. I used a bidirectional RNN with CTC do develop the acoustic model. However, after I get a series of phoneme sequences. How do I go about going from the sequence of phonemes to predicted words? I have tried using a seq2seq model (via Tensorflow) but have received bad predictions.",2,0
872,2016-6-25,2016,6,25,1,4pnsnw,[Question] What are some of the largest images produced by deep generative models so far?,https://www.reddit.com/r/MachineLearning/comments/4pnsnw/question_what_are_some_of_the_largest_images/,mere_mortise,1466786007,"I've only seen tiny images (48x48 or smaller) being produced by these models (for example variational autoencoders, deep Boltzmann machines and generative adversarial networks). Does training become much harder with larger images, or has simply no one yet spent a night of computing time on their super computer to give it a try?",15,10
873,2016-6-25,2016,6,25,1,4pnt2u,ImageNet: The Original Machine Learning Database - somatic blog,https://www.reddit.com/r/MachineLearning/comments/4pnt2u/imagenet_the_original_machine_learning_database/,[deleted],1466786131,[deleted],0,1
874,2016-6-25,2016,6,25,2,4pnzc0,Gradient error and activation flows in encoder-decoder models,https://www.reddit.com/r/MachineLearning/comments/4pnzc0/gradient_error_and_activation_flows_in/,speechMachine,1466788048,"After reading through recent work on encoder-decoder models(e.g. https://arxiv.org/pdf/1406.1078.pdf), an the more recent work on attention models, I wanted some clarification on forward activation flow and backward error flow. 

TLDR: My concept of time in an RNN is getting challenged once the network is cut into half, i.e. encoder-decoder pair...

In regular RNNs each layer at every time step has access to the activation from the previous layer at the current time step (say x_t) and the recurrent activations from the previous time step (say h_(t-1)) and in the backward pass error signals from current time step (say delta_t) and previous time steps(delta_(t-1)...delta_(t-k)). Where k is as many time steps as you wish to keep. All fine and dandy.

In encoder-decoder models, when you chop a model in half especially as you do in machine translation, my impression is that the decoder has a T time step delay in operation as the encoder scans the entire source utterance for T time steps. After T times steps the encoder summarizes the source phrase as an embedding vector h_T. Initialized with this the decoder (goes on for T time steps?...) to produce the target phrase...right?

If yes, how does the backward pass work when errors are getting propagated back from the decoder to the encoder? In a regular RNN each layer has access to delta_t. Now that there is a time-lag between the operation of the encoder and decoder, does the encoder get a so-called ""summarized"" error vector from the decoder?

Any pointers to references or any clarifications are  greatly appreciated! ",3,1
875,2016-6-25,2016,6,25,2,4po8q6,Stream Processors/CUDA Cores and Core Clock,https://www.reddit.com/r/MachineLearning/comments/4po8q6/stream_processorscuda_cores_and_core_clock/,[deleted],1466790984,[deleted],1,0
876,2016-6-25,2016,6,25,3,4pocmv,"ImageNet, the Original Machine Learning Database, and How it was Built",https://www.reddit.com/r/MachineLearning/comments/4pocmv/imagenet_the_original_machine_learning_database/,Toyjust,1466792171,,3,8
877,2016-6-25,2016,6,25,6,4ppam7,[1606.02355] Active Long Term Memory Networks,https://www.reddit.com/r/MachineLearning/comments/4ppam7/160602355_active_long_term_memory_networks/,NetOrBrain,1466803309,,11,50
878,2016-6-25,2016,6,25,7,4ppk4l,"Learn to create dynamic Vanilla RNN, LSTM, GRU from scratch with tensorflow",https://www.reddit.com/r/MachineLearning/comments/4ppk4l/learn_to_create_dynamic_vanilla_rnn_lstm_gru_from/,[deleted],1466806757,[deleted],0,3
879,2016-6-25,2016,6,25,8,4pprhk,Personalized ML Course,https://www.reddit.com/r/MachineLearning/comments/4pprhk/personalized_ml_course/,GCB2389,1466809511,[removed],0,1
880,2016-6-25,2016,6,25,10,4pq8jj,The Divided Kingdom: a machine learning analysis on the Brexit result,https://www.reddit.com/r/MachineLearning/comments/4pq8jj/the_divided_kingdom_a_machine_learning_analysis/,wildcodegowrong,1466816407,,1,0
881,2016-6-25,2016,6,25,10,4pqdvg,Machine learning for neuroscience lecture 2: model selection and significance testing,https://www.reddit.com/r/MachineLearning/comments/4pqdvg/machine_learning_for_neuroscience_lecture_2_model/,MichaelLewis00,1466818712,,1,1
882,2016-6-25,2016,6,25,13,4pqx66,"Where can I find good pre-trained, downloadable English language models?",https://www.reddit.com/r/MachineLearning/comments/4pqx66/where_can_i_find_good_pretrained_downloadable/,xumx,1466827505,,7,10
883,2016-6-25,2016,6,25,13,4pr3j1,"As a beginner, What will you prefer? Coding your own machine learning algorithm or using the highly optimised library like scikit-learn?",https://www.reddit.com/r/MachineLearning/comments/4pr3j1/as_a_beginner_what_will_you_prefer_coding_your/,mendax007,1466830748,"I find people here with vast experience in ML. It's a question they should answer keeping in mind that will the highly optimized libraries give the better insight of data?
Examples- sklearn.algo.fit() and sklearn.algo.predict() .. they don't tell you what's going on.",8,6
884,2016-6-25,2016,6,25,15,4prd2z,What to wear at a computer vision conference ?,https://www.reddit.com/r/MachineLearning/comments/4prd2z/what_to_wear_at_a_computer_vision_conference/,mayank1123,1466835900,I am a Masters student and I am attending a conference (CVPR) next week in Vegas and this is my first time. I do not have any paper or poster presentation and I am just attending it. I am in Arizona and I usually wear shorts everywhere so I was wondering if it would be ok to wear shorts there as well or should I stick to trousers and a T-shirt. ,11,0
885,2016-6-25,2016,6,25,16,4prl7h,Can you explain sentence embedding in NLP with an example?,https://www.reddit.com/r/MachineLearning/comments/4prl7h/can_you_explain_sentence_embedding_in_nlp_with_an/,Mr__Christian_Grey,1466840841,,2,0
886,2016-6-25,2016,6,25,18,4pruck,"How to handle ""big"" data sets",https://www.reddit.com/r/MachineLearning/comments/4pruck/how_to_handle_big_data_sets/,bagelorder,1466847142,"I am quite new to applying machine learning. I just downloaded the 3gb set of the kaggle competition about bakery goods and wanted to run one of the exploratory analysis scripts posted in the kaggle forums. It starts with running pandas read_csv method to import the data - and this is where it ends for me. My laptop has 4gb ram and this lets the ipython kernel die. How do I go about this now?

Does one just import parts of the data? If yes, which parts and how? (which methods for example do I have in python to do that, do I take random samples or just ""the first half"")?

Should I rent cloud infrastructre? If yes, where (amazon, ..?)? What kind of infrastructure does one rent normally?

Or should I do something else?",17,8
887,2016-6-25,2016,6,25,22,4psf78,The PhD defense: disaster avoidance strategies,https://www.reddit.com/r/MachineLearning/comments/4psf78/the_phd_defense_disaster_avoidance_strategies/,[deleted],1466859814,[deleted],1,2
888,2016-6-25,2016,6,25,22,4psg5y,Non-Mathematical Feature Engineering techniques for Data Science,https://www.reddit.com/r/MachineLearning/comments/4psg5y/nonmathematical_feature_engineering_techniques/,sachinrjoglekar,1466860294,,43,110
889,2016-6-25,2016,6,25,23,4psork,Suggestions for (small-ish) Machine learning projects for 2-3 Students?,https://www.reddit.com/r/MachineLearning/comments/4psork/suggestions_for_smallish_machine_learning/,minipump,1466864569,"There's a possibility to get funding (500) for a student project at my university. I, and possibly 2 other students, would like to have a few suggestions for smaller projects that are both interesting and doable in a few weeks.

We're studying computational linguistics (Bachelors degree done, starting master next semester) and are fairly proficient in python and scikit-learn.

Any suggestions?

Edit: If this is not the appropriate subreddit, I will delete the post and post in a more appropriate sub.",22,14
890,2016-6-26,2016,6,26,0,4pt122,Which unsupervised learning method produces the best features for semi-supervised object recognition? Are variational autoencoders currently the best?,https://www.reddit.com/r/MachineLearning/comments/4pt122/which_unsupervised_learning_method_produces_the/,[deleted],1466869751,[deleted],8,9
891,2016-6-26,2016,6,26,1,4pt98r,"This Week in Machine Learning: predicting trustworthiness, creating 3D art, anticipating human behavior, meta-machine learning, Twitter's acquisition of Magic Pony, and open-sourcing Netflix's Meson.",https://www.reddit.com/r/MachineLearning/comments/4pt98r/this_week_in_machine_learning_predicting/,DavidAJoyner,1466872926,,1,5
892,2016-6-26,2016,6,26,1,4pt9ig,ls there any Neural Networks IRC/chat out there?,https://www.reddit.com/r/MachineLearning/comments/4pt9ig/ls_there_any_neural_networks_ircchat_out_there/,Weriak,1466873023,"l searched in google for a NN IRC and l found #neuroscience but it's pretty inactive

(ls there someone here who wouldn't mind be asked questions in steam or skype about NN or machine learning in general from time to time? That would be super welcome)",4,0
893,2016-6-26,2016,6,26,3,4pto9l,How to measure the quality the representations learned by an autoencoder?,https://www.reddit.com/r/MachineLearning/comments/4pto9l/how_to_measure_the_quality_the_representations/,[deleted],1466878680,[deleted],0,1
894,2016-6-26,2016,6,26,3,4ptowt,[1606.06630] On Multiplicative Integration with Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4ptowt/160606630_on_multiplicative_integration_with/,larseidnes,1466878921,,4,19
895,2016-6-26,2016,6,26,3,4ptq05,How to measure the quality of the representations learned by an autoencoder?,https://www.reddit.com/r/MachineLearning/comments/4ptq05/how_to_measure_the_quality_of_the_representations/,stop_ttip,1466879340,"Hi, I was wondering, if I was to invent a new autoencoder, how would I measure the quality of the learned representations?

One way is the obvious: measuring the reconstruction error. But, if I wanted to measure how well the datapoints are placed into the representation manifold, what are the methods that are most suitable?

A few ideas:

* training the autoencoder on a binary-labeled dataset by removing the labels, and then measuring the error of a simple linear hyperplane SVM classifier trained on the latent codes provided by the autoencoder.

* calculating the correlations between the values of the representation latent vector: the lowest the correlation the better.

Any other suggestions?
",3,2
896,2016-6-26,2016,6,26,3,4ptr2y,A Multiworld Testing Decision Service,https://www.reddit.com/r/MachineLearning/comments/4ptr2y/a_multiworld_testing_decision_service/,[deleted],1466879725,[deleted],0,1
897,2016-6-26,2016,6,26,6,4pukcv,Machine Learning Master's Questions,https://www.reddit.com/r/MachineLearning/comments/4pukcv/machine_learning_masters_questions/,ML_Hopeful,1466891227,"I have some questions about the best way to go about getting a Master's relevant to Machine Learning. It doesn't matter to me whether the title of the degree is Machine Learning (I know there are still very few of those), Computer Science, or something else, as long as I'm studying Machine Learning and will be able to get a good job in the industry afterwards. I'm hoping to apply this fall/winter, and start in the fall of 2017.
I'm hoping to get into a top program like Stanford, Carnegie Mellon, or Berkeley. My first question is:

1) **Do I have a shot at these kind of top programs?** What are my chances? Relevant background: I graduated from Villanova University (a great, but by no means top school) with a 3.78 GPA in Mathematics. I am a self-taught coder (I have about 13+ years of my own projects through which I've learned how to program well), but I have been taking a few undergraduate CS classes at my local state school to demonstrate my knowledge and hopefully avoid pre-reqs. I have a few years of work experience doing something that most people would consider to an enormous responsibility. However, while that job might be a very impressive item in general for a resume, it had nothing to do with computer science. I've been volunteering at a University lab for the past 6 months helping the Professor with some robotics/AI programming research, and intend to continue to do so until I apply. Unfortunately, there has been no machine learning component, and it seems doubtful at this point that my name will be on any papers submitted before I apply. I just took the GRE; I got a 168 in math (95th percentile), 167 in verbal (97th percentile), and a 4.0 in writing (56th percentile).

2) My GRE scores seem to be more than adequate except for writing. **Is it worth retaking for the writing or do these programs care less about the GRE writing?** The GRE is expensive for me right now, and I'm not sure that I could get a better writing score anyway (writing was the only thing I spent any significant time studying for on the last test and the studying didn't seem to do me much good; also, I am a very slow writer). On the other hand, I'm well aware of how insanely competitive the top programs are, and I'm concerned that my mediocre writing score might serve as an excuse to help the top schools' overworked admissions offices eliminate one more application. So should I retake the GRE for the writing or is it irrelevant for a ML/CS master's?

3) Finally, **how do I go about finding some safety schools and in-between schools** (better than safety, but not reach schools either)? There's not exactly a list anywhere (that I've found) of decent master's CS programs in which a student can focus on ML.

Thanks so much for your time and help! If there are any other resources or information that it might helpful for me to be aware of, I would appreciate the info!
",0,1
898,2016-6-26,2016,6,26,7,4puncc,ICML16 - Deep Reinforcement Learning Tutorial by DeepMind,https://www.reddit.com/r/MachineLearning/comments/4puncc/icml16_deep_reinforcement_learning_tutorial_by/,Aeefire,1466892406,,8,21
899,2016-6-26,2016,6,26,7,4puqop,Just read this paper on IV methods. Would this same problem apply to a bunch of weak instruments all boosted together into a strong instrument?,https://www.reddit.com/r/MachineLearning/comments/4puqop/just_read_this_paper_on_iv_methods_would_this/,[deleted],1466893733,[deleted],8,11
900,2016-6-26,2016,6,26,8,4puwsz,"In NNs, do weights converge to a specific distribution?",https://www.reddit.com/r/MachineLearning/comments/4puwsz/in_nns_do_weights_converge_to_a_specific/,[deleted],1466896107,[deleted],12,9
901,2016-6-26,2016,6,26,8,4pv2lh,Artificial Intelligence Has a 'Sea of Dudes' Problem,https://www.reddit.com/r/MachineLearning/comments/4pv2lh/artificial_intelligence_has_a_sea_of_dudes_problem/,Duncan3,1466898485,,21,0
902,2016-6-26,2016,6,26,9,4pvb1b,How to group perceptrons?,https://www.reddit.com/r/MachineLearning/comments/4pvb1b/how_to_group_perceptrons/,Azythic,1466901894,[removed],0,1
903,2016-6-26,2016,6,26,10,4pvhy5,"tiny-cnn: A header-only, dependency-free deep learning framework for C++11",https://www.reddit.com/r/MachineLearning/comments/4pvhy5/tinycnn_a_headeronly_dependencyfree_deep_learning/,downtownslim,1466904765,,15,91
904,2016-6-26,2016,6,26,11,4pvp6a,PCA/eigenface question,https://www.reddit.com/r/MachineLearning/comments/4pvp6a/pcaeigenface_question/,NeganIsJayGarrick,1466907822,"I guess this is more of a conceptual question as I am a bit new to this stuff. quick preface (just so you also know where I am in myunderstanding): When doing eigenfaces it is common to compute the covariance matrix of a data set N x M, (where M &lt;&lt; N, M are the number of faces, N are the pixels per face) such that the dimensions of the cov. matrix are Mx M rather than NxN ... this is ok because there are at most M-1 non-zero eigenvalues, and the eigenvalues of this cov matrix are the M *largest* eigenvalues of the N x N covariance matrix (but is usually computationally infeasible to calculate)

Second, as I understand, the variance of the projections with the eigen vectors ARE the eigen values, AND, the cumulative sum plot of the eigen values tell you how many components you can use to achieve a certain % accuracy.

 Now here is where I am confused, if there are only M-1 non-zero eigen values, then shouldn't the projections with the corresponding M-1 eigenvectors capture all of the data set? So if I do a test reconstruction using one of the original faces, shouldn't it be 100% reconstructed with just the M-1 eigenvectors? Because the cumulative sum of the eigen values would reach 100% when using all the corresponding eigenvectors.


But I am finding this is not the case in my playing around with this stuff... what am I mixing up in my understanding? Do I actually need all N, rather than M, eigen vectors in this case?

edit: clarified M &amp; N 

",2,5
905,2016-6-26,2016,6,26,13,4pw65s,Using Gaussian Mixture Models in Rust,https://www.reddit.com/r/MachineLearning/comments/4pw65s/using_gaussian_mixture_models_in_rust/,brson,1466915373,,1,5
906,2016-6-26,2016,6,26,14,4pwfm9,"Very rudimentary question, please tell me what to google: if I have a handful of (black box) scoring functions, each of which is reasonably good at ranking candidates, how can I learn a function that optimally combines them into one better score?",https://www.reddit.com/r/MachineLearning/comments/4pwfm9/very_rudimentary_question_please_tell_me_what_to/,machlearning,1466919917,[removed],0,1
907,2016-6-26,2016,6,26,15,4pwjmw,"We trained a machine to ask ""Would you Rather"" questions. Here's our experiment.",https://www.reddit.com/r/MachineLearning/comments/4pwjmw/we_trained_a_machine_to_ask_would_you_rather/,benpelcyger,1466922079,"
https://www.youtube.com/watch?v=ZJD4FlVo9vc

        
We trained a machine to ask ""Would you rather"" quesitons.
To test, we ran an experiment to see if my coworker could guess the author: his son or our machine.

Spoiler: he gets it 65% correct.

* edit: post about the project 
http://kreuzader.aetherstation.net/post/146504031866/basically-unlimited-important-yelling-fun-with

* See our machine on twitter: https://twitter.com/wyr_bot
See Tom make his guesses here:
* https://www.youtube.com/watch?v=m2zq7LWeTZk

edit: I'm new here.  Please shoot me a message if this isn't the right place for this.

",5,5
908,2016-6-26,2016,6,26,15,4pwm16,Which CNN framework is best for real-time object tracking,https://www.reddit.com/r/MachineLearning/comments/4pwm16/which_cnn_framework_is_best_for_realtime_object/,Tim_EE,1466923385,"Hi, I am an undergraduate student and rather new to CNN development and its use in computer vision applications like object tracking. I am trying to develop my first real-time visual tracking software for my university's robotics lab using CNN. Because this is with a real robot, the tracking system needs to have sufficient computational speed and accuracy to remain practical. I would like to do this with C++ and use a CNN framework along side OpenCV.


I've researched different frameworks like Caffe, Theano, Torch, etc. But I'm not sure which one works best for real-time applications, while also working well with C/C++ and OpenCV. I'm curious what frameworks others have used for real-time applications and what their thoughts are on them. Sorry if this seems to have an obvious answer. 


Note: Although this question is specific to real-time applications of CNN's to computer vision, feel free to comment about your experiences/advice with the frameworks you've used in real-time applications in general (be it for NLP, CV, or whatever). That way this can also be a general discussion about peoples experiences/advice with these frameworks in terms of just real-time implementations, and others can benefit from what's been discussed here.


Thanks everyone for any help you may have, I really appreciate it.",19,5
909,2016-6-26,2016,6,26,17,4pwy4z,"""What happens when I combine my love of machine learning with my love of masturbation."" - /u/c3534l",https://www.reddit.com/r/MachineLearning/comments/4pwy4z/what_happens_when_i_combine_my_love_of_machine/,subroutines,1466930654,,0,0
910,2016-6-26,2016,6,26,18,4pwzr6,Any suggestion for bachelor thesis topic in this field?,https://www.reddit.com/r/MachineLearning/comments/4pwzr6/any_suggestion_for_bachelor_thesis_topic_in_this/,[deleted],1466931726,[deleted],0,0
911,2016-6-26,2016,6,26,19,4px58z,Question about Deep Residual Networks with Exponential Linear Unit architecture,https://www.reddit.com/r/MachineLearning/comments/4px58z/question_about_deep_residual_networks_with/,[deleted],1466935300,[deleted],3,0
912,2016-6-26,2016,6,26,21,4pxm42,A Clean C++11 Deep Learning API with a cuDNN Backend and Multi-GPU Support,https://www.reddit.com/r/MachineLearning/comments/4pxm42/a_clean_c11_deep_learning_api_with_a_cudnn/,davis685,1466945242,,40,151
913,2016-6-26,2016,6,26,22,4pxprc,A Practical Introduction to Deep Learning with Caffe and Python,https://www.reddit.com/r/MachineLearning/comments/4pxprc/a_practical_introduction_to_deep_learning_with/,adilmoujahid,1466947101,,6,47
914,2016-6-26,2016,6,26,22,4pxuwf,Custom Optimizer in tensorfow?,https://www.reddit.com/r/MachineLearning/comments/4pxuwf/custom_optimizer_in_tensorfow/,shaleenx,1466949549,[removed],0,1
915,2016-6-26,2016,6,26,23,4pxveu,Detecting Money Laundering with unsupervised ML,https://www.reddit.com/r/MachineLearning/comments/4pxveu/detecting_money_laundering_with_unsupervised_ml/,arshakn,1466949777,,0,14
916,2016-6-27,2016,6,27,0,4py6g3,"Completed the ML course on Data Camp, what next?",https://www.reddit.com/r/MachineLearning/comments/4py6g3/completed_the_ml_course_on_data_camp_what_next/,[deleted],1466954312,[deleted],0,0
917,2016-6-27,2016,6,27,0,4pybz6,Where do AIs talk to eachother in binary?,https://www.reddit.com/r/MachineLearning/comments/4pybz6/where_do_ais_talk_to_eachother_in_binary/,[deleted],1466956459,[deleted],2,0
918,2016-6-27,2016,6,27,3,4pyyxq,Some questions about getting into ML. Just finished a BS in math. Starting a master's in stats.,https://www.reddit.com/r/MachineLearning/comments/4pyyxq/some_questions_about_getting_into_ml_just/,Smartless,1466965326,"I'm starting a master's in stats next semester, and I want to gear it towards machine learning. 

I've taken proof based linear algebra, the real analysis sequence, probability, stats, and some other upper division math classes. Additionally, I'll be taking graph theory next semester, in addition to more stats related courses. Are there any other math courses I should be giving my attention to, whether it be in a traditional classroom setting, or on my own time? (i.e. what areas of math show up a lot in ML that I might be missing?)

Alternatively, am I giving too much attention to the math, and should I be focusing more on the programming aspect of ML? If so, what should I be focusing on as far as programming goes?

Finally, will having a heavy stats-centric background be very helpful for ML? Or are there more effective things I could give my attention to?

Thanks in advance for any advice.",9,2
919,2016-6-27,2016,6,27,3,4pz487,Build a Chatbot w/ an API - ML for Hackers #9,https://www.reddit.com/r/MachineLearning/comments/4pz487/build_a_chatbot_w_an_api_ml_for_hackers_9/,[deleted],1466967295,[deleted],0,0
920,2016-6-27,2016,6,27,7,4q02ar,Factorization Machines Tutorial Part 1,https://www.reddit.com/r/MachineLearning/comments/4q02ar/factorization_machines_tutorial_part_1/,Jxieeducation,1466979985,,0,11
921,2016-6-27,2016,6,27,7,4q041v,nlpers - Language bias and black sheep,https://www.reddit.com/r/MachineLearning/comments/4q041v/nlpers_language_bias_and_black_sheep/,[deleted],1466980660,,2,25
922,2016-6-27,2016,6,27,8,4q098k,Any one a self-employed machine learning consultant?,https://www.reddit.com/r/MachineLearning/comments/4q098k/any_one_a_selfemployed_machine_learning_consultant/,ETTeddy,1466982649,"Or an ML-based business? Any career path for people with machine learning skill that is not traditional employment model. Can you share what you do, how good is the earnings, and how did you get there?

Do you think this sort of work can be done online, similarly to freelance coding?",10,8
923,2016-6-27,2016,6,27,8,4q0ehi,Help handling variable sequence length inputs for RNN/LSTMs,https://www.reddit.com/r/MachineLearning/comments/4q0ehi/help_handling_variable_sequence_length_inputs_for/,IndividualCarnival,1466984733,"Hi!

I'm working on a character-level LSTM right now, and have everything functioning except for handling variable sequence length inputs. The architecture was simply

Forward -&gt; Recurrent Layer -&gt; Forward -&gt; Softmax

What I did is append every training example with a number of null bits, '@' for now to just denote ""ignore this in backprop"" so they were all the same length. When calculating the loss and the gradient of the softmax scores, I elementwise multiply my gradients by a mask that is 0 where the index in the input = @ (null). This works completely fine, and in debugging I clearly see that overtime as it does sets of forward props and BPTT, that slowly my upstream gradients are being set to 0 since we're encountering null bits!

The result however is that somehow at sample time, although having no contribute to either the loss or backprop (so far from my investigation), the sample purely submits a collection of null tokens back to back. TL;DR It seems that even though blocking it's backprop by setting gradients to 0 where an @ null is found, it none the less predicts a sequence of nulls.

I would really appreciate if anyone had some input or idea as to why! Thank you very much!!",0,0
924,2016-6-27,2016,6,27,8,4q0fxu,Neural Network in the Browser,https://www.reddit.com/r/MachineLearning/comments/4q0fxu/neural_network_in_the_browser/,bokenator,1466985308,,14,47
925,2016-6-27,2016,6,27,11,4q14vk,Modeling recurrent processing in visual cortex?,https://www.reddit.com/r/MachineLearning/comments/4q14vk/modeling_recurrent_processing_in_visual_cortex/,kh40tika,1466995496,"In human visual cortex, there are bottom-up feedforward processing as well as top-down recurrent processing. However most CNNs I see are purely feedforward ones. Are there any deep learning vision models can *learn* certain recurrent processing in vision? Boundary Completion, for example.",4,2
926,2016-6-27,2016,6,27,13,4q1gim,Windows 10 + Lasagne = Hell?,https://www.reddit.com/r/MachineLearning/comments/4q1gim/windows_10_lasagne_hell/,cuzimjj,1467000156,"I've been looking around for ways of downloading Lasagne on windows 10 and I keep getting many, many problems. I've downloaded Theano, tried to follow every tutorial and so on. I've never seen someone mention using Windows 10 though and I was wondering if that was the problem.

My current problems so far have been that nvcc_compiler fatal: Can't find a compatible version of Microsoft Visual studio and originally it had to do cl.exe not being found in the path. Anyone successfully do this with Windows 10?",14,0
927,2016-6-27,2016,6,27,14,4q1q94,Approximate Nearest Neighbour Search with Online Updates,https://www.reddit.com/r/MachineLearning/comments/4q1q94/approximate_nearest_neighbour_search_with_online/,[deleted],1467004512,[deleted],0,1
928,2016-6-27,2016,6,27,16,4q238x,"The Open Source Society has created a solid path for you that want to learn Data Science and Machine Learning, online for free as a github repo.",https://www.reddit.com/r/MachineLearning/comments/4q238x/the_open_source_society_has_created_a_solid_path/,m4c51n3,1467011135,,34,413
929,2016-6-27,2016,6,27,17,4q2dda,Any Open DataSet for Instant Message Network,https://www.reddit.com/r/MachineLearning/comments/4q2dda/any_open_dataset_for_instant_message_network/,wxyyxc1992,1467016755,[removed],0,1
930,2016-6-27,2016,6,27,17,4q2et3,speech normalization for automatic speech recognition,https://www.reddit.com/r/MachineLearning/comments/4q2et3/speech_normalization_for_automatic_speech/,saseptim,1467017599,"Hi, I am trying to train cnn's for speech recognition and was wondering what would be the best way to normalize the audio so that the max/min values in the audio/spectrograms would be same across samples.
Thanks!",3,0
931,2016-6-27,2016,6,27,18,4q2gnc,How do you comfort Quality Engineers to your trained model?,https://www.reddit.com/r/MachineLearning/comments/4q2gnc/how_do_you_comfort_quality_engineers_to_your/,ire7715,1467018719,"Here is the scenario. We(data science team) have developed a model that gives us a decent accuracy, and we are planning to make the model as a service of our company's product.

After we implemented the API of the model, and show the prediction on the product, the QA team is going to verify the update. However, besides the functional part, they want to verify the prediction as well. Despite that we have shown them the accuracy and told them the prediction is based on the model, which will update itself overtime and has its own test cases(for mathematical correctness but not prediction accuracy).
Currently, they have accepted to the idea of the prediction need not test, but they are unconfident to something cannot be tested automatically.

Is my idea about prediction cannot be tested wrong? If not, how do I comfort them to our model?",10,6
932,2016-6-27,2016,6,27,18,4q2gzt,A Credit Assignment Compiler for Joint Prediction,https://www.reddit.com/r/MachineLearning/comments/4q2gzt/a_credit_assignment_compiler_for_joint_prediction/,[deleted],1467018938,[deleted],0,0
933,2016-6-27,2016,6,27,19,4q2o02,What is agent history length?,https://www.reddit.com/r/MachineLearning/comments/4q2o02/what_is_agent_history_length/,whiteshadow13,1467022952,"Upon reading DeepMind's ""Nature Paper: Human-level control through deep reinforcement learning""[1], there is a reference to agent history length on page 10:
""
Agent history length:
the number of most recent frames experienced by the agent that are given as input to the Q network
""

How does this work in practice? Is this parameter the training's input batch size?

Thanks for the help!

[1] http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D",4,0
934,2016-6-27,2016,6,27,19,4q2rxc,Why training baseline of REINFORCE by MSE?,https://www.reddit.com/r/MachineLearning/comments/4q2rxc/why_training_baseline_of_reinforce_by_mse/,gmkim90,1467025160,"In following references

 [1] V. Mnih et al., ""Recurrent Models of Visual Attention"", NIPS, 2014

 [2] http://torch.ch/blog/2015/09/21/rmva.html

 [3] M. Ranzato et al., ""Sequence level training with Recurrent Neural Network"", ICLR, 2016

they applied REINFORCE algorithm to train RNN.


To reduce variance of the gradient, they subtract 'baseline' from sum of future rewards for all time steps. According to Appendix A-2 of

[4]. W. Zaremba et al., ""Reinforcement Learning Neural Turing Machines"", arXiv, 2016

this baseline is chosen as expected future reward given previous states/actions.

My question is training method to get 'baseline'. They train 'baseline' at each time by linear regression (i.e. objective = Mean Square Error) which takes hidden state of RNN as input. How come this training method make sense? Is there anyone to provide relevant reference?

I really appreciate for your comments.",4,0
935,2016-6-27,2016,6,27,20,4q2vnt,How does the JCT rubber kneader mixer operate?,https://www.reddit.com/r/MachineLearning/comments/4q2vnt/how_does_the_jct_rubber_kneader_mixer_operate/,mixmachinery,1467027192,,1,1
936,2016-6-27,2016,6,27,21,4q31tw,How to Start Learning Deep Learning,https://www.reddit.com/r/MachineLearning/comments/4q31tw/how_to_start_learning_deep_learning/,ofirpress,1467030232,,4,5
937,2016-6-27,2016,6,27,21,4q31ys,"A fork of Keras that has Caffe to Keras conversion module, Layer-specific learning rates and New layers for multimodal data",https://www.reddit.com/r/MachineLearning/comments/4q31ys/a_fork_of_keras_that_has_caffe_to_keras/,loopnn,1467030298,,4,13
938,2016-6-27,2016,6,27,21,4q33qu,Accessing Databases - Best Practices?,https://www.reddit.com/r/MachineLearning/comments/4q33qu/accessing_databases_best_practices/,IdoNotKnowShit,1467031027,"Machine learning practioners of Reddit, what is the best way to setup in production a ML model which interfaces with a large database? Do you use ROMs? Raw SQL? Message queues? Web programmers talk about their backed setup all the time but the topic never seems to show up in the ML community.",5,0
939,2016-6-27,2016,6,27,21,4q359i,Full time dev wanting to expand into ML without taking time off to do masters.,https://www.reddit.com/r/MachineLearning/comments/4q359i/full_time_dev_wanting_to_expand_into_ml_without/,loftyal,1467031629,Full time dev wanting to expand into ML without taking time off to do a degree.. How would i go about this? Would there be any night courses or bootcamps people would recommend? Thanks,2,0
940,2016-6-27,2016,6,27,21,4q36xf,webinar on sentiment analysis in quant trading,https://www.reddit.com/r/MachineLearning/comments/4q36xf/webinar_on_sentiment_analysis_in_quant_trading/,subhkirti,1467032307,,0,1
941,2016-6-27,2016,6,27,22,4q3a83,Cool visualization tool for recurrent neural networks by the Harvard NLP group,https://www.reddit.com/r/MachineLearning/comments/4q3a83/cool_visualization_tool_for_recurrent_neural/,[deleted],1467033605,[deleted],0,0
942,2016-6-27,2016,6,27,22,4q3fus,Open source implementations of k-means++ and k-means || ?,https://www.reddit.com/r/MachineLearning/comments/4q3fus/open_source_implementations_of_kmeans_and_kmeans/,ghanta29,1467035704,"I tried googling this but couldn't find anything. I am working on a new variant of this and would like to compare my results with these algorithms.

k-means|| http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf

Found something http://stackoverflow.com/questions/30556625/k-means-for-clustering-a-k-a-scalable-k-means-with-mlib-spark",5,0
943,2016-6-27,2016,6,27,23,4q3jhb,How does one build Learning Machines ?,https://www.reddit.com/r/MachineLearning/comments/4q3jhb/how_does_one_build_learning_machines/,anujgupta82,1467037057,"**Machine Learning** : Train a model using training data. Model learns from (training) data but stops learning thereafter. *Does not continuously learn from its mistakes*.

**Learning Machine** : a machine that predicts, monitors the prediction made, and if the prediction is wrong (or way of its target) *the prediction and target are sent back as feedback*, feedback is ingested at once, leading to a new training, before the next prediction occurs. 

*The way learning machines are built is : by adding a feedback ingestion loop to the machine/model*. Look at the figure below:  

https://www.ibm.com/developerworks/community/blogs/jfp/resource/BLOGS_UPLOADED_IMAGES/ml2.png

**How does one add feedback ingestion loop to an existing ML model** ? I am looking for all the gory details. Any pointers will be of great help.  

If you are looking for more explanation on this question, pls read the following blog entry : [Machine Learning != Learning Machine](https://www.ibm.com/developerworks/community/blogs/jfp/entry/Machine_Learning_Learning_Machine)",9,0
944,2016-6-27,2016,6,27,23,4q3l47,A theoretical framework for deep transfer learning,https://www.reddit.com/r/MachineLearning/comments/4q3l47/a_theoretical_framework_for_deep_transfer_learning/,jinpanZe,1467037634,,0,12
945,2016-6-28,2016,6,28,0,4q3v78,Feel like at CVPR 2016 in Las Vegas: read CVPR Daily of today (Monday),https://www.reddit.com/r/MachineLearning/comments/4q3v78/feel_like_at_cvpr_2016_in_las_vegas_read_cvpr/,Dov_sadan,1467040883,http://www.rsipvision.com/CVPR2016-Monday/ Enjoy the reading!,2,0
946,2016-6-28,2016,6,28,0,4q3xuu,Evolving Neural Networks for Cross-adaptive Audio Effects,https://www.reddit.com/r/MachineLearning/comments/4q3xuu/evolving_neural_networks_for_crossadaptive_audio/,erkaman,1467041730,,2,4
947,2016-6-28,2016,6,28,0,4q3y5s,This Week in ML &amp; AI Podcast - 6/24/16,https://www.reddit.com/r/MachineLearning/comments/4q3y5s/this_week_in_ml_ai_podcast_62416/,sbc1906,1467041820,"I noticed my link posts the past couple of weeks got downvoted, so I'm doing a text post this time and will be including a mini summary of the show notes.

Show Page =&gt; https://twimlai.com/6

Friday's Show:

- ICML 2016 and discussion of Best Paper Winner on Dueling Architectures for RL.

- Research: OpenAI et al on AI Safety, breast cancer diagnosis application.

- Business: Magic Pony acquisition, 2016 AI company financings, new DARPA solicitation for automated ML.

- AI Culture Wars: Continuation of last week's discussion on ML/AI in Apple, Google.

- Tech: FB open sources Torchnet, Intel Xeon Phi, Watson in the NBA.

- Projects: Hello Tensorflow, Character Level DL, GitXiv, forthcoming ML books

Let me know if this format is more useful than a quick link post, and certainly share any feedback on the podcast itself. Thx!",1,10
948,2016-6-28,2016,6,28,1,4q45ef,LSTMVis: A tool to visualize recurrent neural network by Harvard NLP,https://www.reddit.com/r/MachineLearning/comments/4q45ef/lstmvis_a_tool_to_visualize_recurrent_neural/,Valedra,1467044103,,2,29
949,2016-6-28,2016,6,28,3,4q4qw3,Build a Chatbot w/ an API,https://www.reddit.com/r/MachineLearning/comments/4q4qw3/build_a_chatbot_w_an_api/,llSourcell,1467050775,,2,2
950,2016-6-28,2016,6,28,3,4q4r82,Theano/Lasagne implementation of CNN+GRU for spoken language identification,https://www.reddit.com/r/MachineLearning/comments/4q4r82/theanolasagne_implementation_of_cnngru_for_spoken/,HrantKhachatrian,1467050880,,0,27
951,2016-6-28,2016,6,28,3,4q4s1m,Question on Rule Extraction,https://www.reddit.com/r/MachineLearning/comments/4q4s1m/question_on_rule_extraction/,adamz_a_99,1467051150,[removed],0,1
952,2016-6-28,2016,6,28,3,4q4zk8,K Means under the hood with Python,https://www.reddit.com/r/MachineLearning/comments/4q4zk8/k_means_under_the_hood_with_python/,drcrook,1467053465,,0,1
953,2016-6-28,2016,6,28,5,4q5fsu,Advanced Word Embeddings For Seq2Seq Applications,https://www.reddit.com/r/MachineLearning/comments/4q5fsu/advanced_word_embeddings_for_seq2seq_applications/,LeavesBreathe,1467058568,"Hey Guys,

Currently, in many seq2seq applications, there are two options:

* Use a pretrained word2vec model for your embedding
* Learn the embedding for the encoder and decoder during training. 

When you have enough training data, I've found that option two usually does best. 

However, there's a problem. When you tokenize by words, you're forced to tokenize words that are closely related. For example, words like 'fire' and 'fires' are treated as *completely* two separate entities from the start. It would be nice if we could 'tell' the neural net ahead of time that these two words are related before training even starts.

There are a few approaches to this:

* Use a pretrained word2vec embedding, but then allow it to be a trainable variable 
* Do a character based tokenization -- but the number of timesteps for this is too much
* Do a subword based tokenization -- However, it has very difficult for me to find a good way to do this. It also gives rise to more timesteps.

Recently, a partial order embedding paper (http://arxiv.org/abs/1511.06361) was published which seems like a definite direction to pursue. I'm just trying to figure out how to actually apply this to a generative seq2seq model. Any help would be great!",15,7
954,2016-6-28,2016,6,28,5,4q5g2v,"Grenade is a dependently typed, practical, and pretty quick neural network library for concise and precise specifications of complex networks in Haskell.",https://www.reddit.com/r/MachineLearning/comments/4q5g2v/grenade_is_a_dependently_typed_practical_and/,erikd,1467058670,,0,4
955,2016-6-28,2016,6,28,7,4q61bb,Something like Keras for Torch?,https://www.reddit.com/r/MachineLearning/comments/4q61bb/something_like_keras_for_torch/,bionerd2,1467065867,"Is there a nice deep learning library/wrapper like Keras which allows use for Torch? Also is there an easy way to convert between Theano, Tensorflow, and torch backends with Keras code? ",4,0
956,2016-6-28,2016,6,28,8,4q6e82,Anyone know whats up with gitxiv?,https://www.reddit.com/r/MachineLearning/comments/4q6e82/anyone_know_whats_up_with_gitxiv/,caffeine_potent,1467070548,http://gitxiv.com/,4,4
957,2016-6-28,2016,6,28,10,4q6u52,Pick next token of RNN at each time step: by argmax VS. sampling,https://www.reddit.com/r/MachineLearning/comments/4q6u52/pick_next_token_of_rnn_at_each_time_step_by/,gmkim90,1467076681,"In the following references,

[1] S. Bengio et al., Scheduled sampling for Sequence Prediction with Recurrent Neural Networks, NIPS, 2015

[2] M.A.Ranzato et al., Sequence level training with Recurent Neural Network, ICLR, 2016

[3] S.Shen et al., Minimum risk training for neural machine translation, arXiv, 2016

they propose new algorithms to resolve discrepancy between train and inference for training RNN as a sequence generator. Commonly, at each time step, they sample next token from given multinomial distribution.

At inference, greedy search (or beam search) usually pick next token by argmax. Then, why not to choose next token by argmax at training?  In [2],[3], it could explained by training objective function which include expectation. How about [1]? Do you think sampling based training outperform argmax?

I really appreciate for your opinions and comments.
",6,2
958,2016-6-28,2016,6,28,11,4q720d,Feel like at CVPR 2016 in Las Vegas: read CVPR Daily of today (Monday),https://www.reddit.com/r/MachineLearning/comments/4q720d/feel_like_at_cvpr_2016_in_las_vegas_read_cvpr/,[deleted],1467079719,[deleted],0,1
959,2016-6-28,2016,6,28,12,4q7g5f,[1606.08078v1] Detection of concealed cars in complex cargo X-ray imagery using deep learning,https://www.reddit.com/r/MachineLearning/comments/4q7g5f/160608078v1_detection_of_concealed_cars_in/,vonnik,1467085418,,0,6
960,2016-6-28,2016,6,28,12,4q7gkq,The principle of planetary power mixer,https://www.reddit.com/r/MachineLearning/comments/4q7gkq/the_principle_of_planetary_power_mixer/,mixmachinery,1467085598,,1,1
961,2016-6-28,2016,6,28,12,4q7hn6,"Optical Machine Learning: Igor Carron launches his startup ""LightOn""",https://www.reddit.com/r/MachineLearning/comments/4q7hn6/optical_machine_learning_igor_carron_launches_his/,pierrelux,1467086057,,6,17
962,2016-6-28,2016,6,28,13,4q7iik,Learning to Poke by Poking,https://www.reddit.com/r/MachineLearning/comments/4q7iik/learning_to_poke_by_poking/,pulkitag,1467086459,,2,11
963,2016-6-28,2016,6,28,13,4q7pes,"New Artificial Intelligence Beats Tactical Experts in Combat Simulation, University of Cincinnati",https://www.reddit.com/r/MachineLearning/comments/4q7pes/new_artificial_intelligence_beats_tactical/,abstractcontrol,1467089597,,11,8
964,2016-6-28,2016,6,28,13,4q7psl,[Beginner] How do I treat hundreds of thousands USER IDs in a predictive model?,https://www.reddit.com/r/MachineLearning/comments/4q7psl/beginner_how_do_i_treat_hundreds_of_thousands/,Crypto_Wolf,1467089787,"If I want to build a predictive model that has hundreds of thousands of USER IDs, how should I treat them?

I understand they can be very important features, so I would like to keep them.

I also understand that they are categorical, and will need to be transformed to be used as such.

But I have never worked with such a large dataset and my 8gb computer is having issues one-hot encoding such a large feature set.

I would appreciate some tips on how to attack this problem.

Thank you

edit: additional info... I am using Pandas and Python for this. So far I am trying to use this transformation:

     pd.get_dummies(df['User_ID'], sparse=True, drop_first=True)",16,0
965,2016-6-28,2016,6,28,14,4q7qnj,Learning Rate Vs Number of Training Epochs?,https://www.reddit.com/r/MachineLearning/comments/4q7qnj/learning_rate_vs_number_of_training_epochs/,[deleted],1467090165,[deleted],3,0
966,2016-6-28,2016,6,28,16,4q84yv,Normalization Constant Approximation,https://www.reddit.com/r/MachineLearning/comments/4q84yv/normalization_constant_approximation/,koormoosh,1467097286,"Is there a book, survey article, etc on different approximation techniques (i.e. sampling, etc), for normalization constants (partition functions) in neural networks, maxent models, etc.",1,1
967,2016-6-28,2016,6,28,16,4q850y,AUD/USD In Focus as We Turn to Sentiment Following Brexit,https://www.reddit.com/r/MachineLearning/comments/4q850y/audusd_in_focus_as_we_turn_to_sentiment_following/,subhkirti,1467097318,,0,1
968,2016-6-28,2016,6,28,16,4q89q9,I've started an internship and feeling out of my depth. Could you guys help steer me in the right direction?,https://www.reddit.com/r/MachineLearning/comments/4q89q9/ive_started_an_internship_and_feeling_out_of_my/,ThumbForke,1467099820,"I didn't think I was going to get this internship, as I have no experience with machine learning. I was upfront with them about my lack of experience, and they still decided to take me on for some reason! Basically, I was hoping that some kind stranger out there could tell me specifically what areas to study, so I'm not blindly searching through the vast field that is machine learning until I find what I'm looking for (if ever).

We use tailored push notifications to try and encourage people to use our client's apps more basically. Once a user turns off push notifications, then we really have nothing more we can do to encourage that customer. So what they want me to do is try and construct a predictive model that can tell them what users are likely to turn off push notifications in the next week. Every time a user does an activity, that is logged with like literally 100 statistics, so there is so much data to work with. I understand that I should probably just pick the stats that I deem to be most useful, and then run a machine learning algorithm on those, right?

For a little background, I've just graduated with an undergrad in maths, with a little computer science. We're hoping to use numpy and scikit-learn. Would those be sufficient? I have a good background in Python, and I know how to use numpy a bit. So that would be the best for me anyway, given how much I need to learn about the theory as it is! If anyone can give me the name of like a particular model or algorithm or topic, or even just a book or a series of videos on the internet that would be helpful, I would really appreciate it. I'm feeling rather stressed about the possibility of not being able to deliver on this!
",15,8
969,2016-6-28,2016,6,28,17,4q8ddd,Machine learning ideas to use in workplace,https://www.reddit.com/r/MachineLearning/comments/4q8ddd/machine_learning_ideas_to_use_in_workplace/,char27,1467101867,"I would like to use machine learning for my company, we have around ~1000 employees. It could also be related to anything in data science, maybe some predictions or statistics. Does anyone have any cool project ideas, which could be helpful or interesting?",5,0
970,2016-6-28,2016,6,28,19,4q8oha,Frustration with image segmentation,https://www.reddit.com/r/MachineLearning/comments/4q8oha/frustration_with_image_segmentation/,Coloneljesus,1467108252,"This semester I took a machine learning class and it concludes with a small project. For the last ~3 weeks I and a colleague tried to implement a neutral network that classifies pixels of satellite images as road or not road. There was a basic solution provided that we could build upon.

As it stands now, we have tried many things like adding layers, changing parameters, using different loss and activation functions and preprocessing the data. With all this, we could not really improve on the provided solution. We have to hand in our report on Friday so all we have time for is write up all the things that didn't work. Quite unsatisfying.

This brings me to the main point of this post: How the hell do you design a neutral network architecture?? Both machine learning courses I took explained *how* they work but said fuck-all about how to design them so during the whole project, we basically had no direction to go in.

Edit: Thanks to everyone in the thread trying to help me with the specific problem but as stated, it's really too late to implement something new. I was rather wondering what we should've been doing instead of just trying stuff out more or less randomly.",32,12
971,2016-6-28,2016,6,28,19,4q8otz,[1606.08359] Lifted Rule Injection for Relation Embeddings,https://www.reddit.com/r/MachineLearning/comments/4q8otz/160608359_lifted_rule_injection_for_relation/,_rockt,1467108443,,0,4
972,2016-6-28,2016,6,28,20,4q91i2,Black box optimization competition for academia and industrial solvers.,https://www.reddit.com/r/MachineLearning/comments/4q91i2/black_box_optimization_competition_for_academia/,bbcomp,1467115079,,5,63
973,2016-6-28,2016,6,28,22,4q9ev0,Is it only me that thinks Jupyter is horrible?,https://www.reddit.com/r/MachineLearning/comments/4q9ev0/is_it_only_me_that_thinks_jupyter_is_horrible/,ohenrik,1467120434,"I understand that it is easy to use to explore data when messing around with graphs, however i come from web development and i feel like the work flow when using Jupyter is horrible. A quick list of drawbacks using Jupyter:

1. If you work in a team and want to use git, it is a mess. 
2. You end up writing horrible code that is messy to read and keep track of in the notebook, even after splitting code across multiple notebooks. 
3. You need to run each codeblock in the notebook block by block. 
4. Read more here: http://opiateforthemass.es/articles/why-i-dont-like-jupyter-fka-ipython-notebook/

I have explored different options like Hydrogen for Atom, where i can write normal python code and then execute something if i need to check a graph for example. 

However I'm not completely sure about this approach ether.

What does people use when they develop models for clients? I would really love to see som examples of workflows and tools people use out there :)",47,37
974,2016-6-28,2016,6,28,22,4q9k11,"Algorithm for finding if given one variable, the other variables are indicative of the class.",https://www.reddit.com/r/MachineLearning/comments/4q9k11/algorithm_for_finding_if_given_one_variable_the/,DrCSQuestions,1467122326,"Alright so I need an algorithm that finds out how indicative my features are of my output, preferably with the ability to choose a variable that is always passed (it helps give context to the other variables).

I'm using sklearn and I've had a look at their feature selection.  I just feel as though I'm missing something because nothing seems to be good for my issue.  Also some of my variables can be negative.

Thanks for any help, I'm rather stuck on this issue.",2,1
975,2016-6-28,2016,6,28,23,4q9ona,Dog detection: how to? (Deep learning for Vision),https://www.reddit.com/r/MachineLearning/comments/4q9ona/dog_detection_how_to_deep_learning_for_vision/,thiagofm,1467123982,"Hello everyone.

I have a lot of pictures from my dog since it was born, with that + some dog datasets, I want to do ONLY the detection of my dog in a webcamera input(with a bounding box). I'm quite good with the ""programming part""(getting output out of the camera, etc), but I really lag behind in how to build something like YOLO(http://pjreddie.com/darknet/yolo/), but more specific to my case.

I see that Caffee has the ""needed"" algorithms, networks and so on for me to achieve that with some degree of accurracy, but I really lack the ability of knowing what to which Solver to use or what they really mean at all.

And this comes down to my question: what is the knowledge path(books, references) I need to take to understand that? I don't want to learn too much about the specifics, just mostly use Caffe API and be able to reason about pros and cons of each.

(my background: BS in CS, I can study the maths needed to comprehend any concept behind it)

I would even like to go a bit more advanced afterwards by categorizing stuff from the input from the camera by hand in order to make it more accurate and also start detecting subtle things, such as when my dog is walking, sleeping, playing etc).

If you have are passionate about something like this, please be open and let's exchange messages. I aim to do something very practical and cool :-)

Thanks a lot!",7,1
976,2016-6-28,2016,6,28,23,4q9r54,It happens to everyone,https://www.reddit.com/r/MachineLearning/comments/4q9r54/it_happens_to_everyone/,Icko_,1467124837,,1,26
977,2016-6-28,2016,6,28,23,4q9rwd,Be the first to read it! CVPR Daily of Tuesday,https://www.reddit.com/r/MachineLearning/comments/4q9rwd/be_the_first_to_read_it_cvpr_daily_of_tuesday/,Gletta,1467125091,[removed],0,0
978,2016-6-29,2016,6,29,0,4q9ums,An AI assistant that schedules meetings for you,https://www.reddit.com/r/MachineLearning/comments/4q9ums/an_ai_assistant_that_schedules_meetings_for_you/,claytonrodgers,1467126036,,0,1
979,2016-6-29,2016,6,29,0,4qa4mq,Endless Story with Markov Chains,https://www.reddit.com/r/MachineLearning/comments/4qa4mq/endless_story_with_markov_chains/,fthrkl,1467129263,,7,32
980,2016-6-29,2016,6,29,1,4qac7m,Deep learning - one or two video cards,https://www.reddit.com/r/MachineLearning/comments/4qac7m/deep_learning_one_or_two_video_cards/,thefriedgoat,1467131656,"I am about to build a new pc to further my research into deep learning, but am torn..  Which would be the more viable setup: one GTX 1080 or two GTX 980 Ti?  I get that RAM is king (and that dual card setups are not necessarily widely supported) - but which would yield the greatest return?",8,0
981,2016-6-29,2016,6,29,1,4qadg3,Face Transfer using TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4qadg3/face_transfer_using_tensorflow/,pavelgonchar,1467132018,,0,0
982,2016-6-29,2016,6,29,1,4qah0j,Will the ICML Panel Discussion be Available Eventually?,https://www.reddit.com/r/MachineLearning/comments/4qah0j/will_the_icml_panel_discussion_be_available/,blowjobtransistor,1467133133,"I was particularly interested in the questions posted in the Small Data Regime thread, but couldn't make it to the conference.  Does anyone know if/when they'll be uploaded, or if a text version will be available?",1,14
983,2016-6-29,2016,6,29,2,4qamvx,Reproducibility Crisis in Science - Why We Need More Dark Reactions Projects (which underscores sharing failed experiments and advanced machine learning to predict the outcomes of the experiments),https://www.reddit.com/r/MachineLearning/comments/4qamvx/reproducibility_crisis_in_science_why_we_need/,[deleted],1467134894,[deleted],0,1
984,2016-6-29,2016,6,29,2,4qanlr,Unmanned Combat Aerial Vehicle Control AI using Fuzzy inference defeats experts in aerial combat scenarios.,https://www.reddit.com/r/MachineLearning/comments/4qanlr/unmanned_combat_aerial_vehicle_control_ai_using/,coolwhipper_snapper,1467135088,,0,6
985,2016-6-29,2016,6,29,2,4qar2n,Need ideas on Feature extraction - Decision Trees,https://www.reddit.com/r/MachineLearning/comments/4qar2n/need_ideas_on_feature_extraction_decision_trees/,sriharshakiran,1467136138,"What all features can be extracted from a decision tree? Given multiple such feature vectors what insights can be mined?

",3,0
986,2016-6-29,2016,6,29,3,4qazx8,Study Maths or Computer Science,https://www.reddit.com/r/MachineLearning/comments/4qazx8/study_maths_or_computer_science/,kingpg,1467138802,"I'm a soon to be genetics graduate looking to migrate into machine learning, and by migrate I mean start over and do a suitable degree. What are the merits of studying maths vs CS given my intentions?",6,0
987,2016-6-29,2016,6,29,5,4qbn9i,Explaining and illustrating orthogonal initialization for recurrent neural networks,https://www.reddit.com/r/MachineLearning/comments/4qbn9i/explaining_and_illustrating_orthogonal/,smerity,1467146187,,6,16
988,2016-6-29,2016,6,29,6,4qbvbh,Our Neural Net Dreams of Cars,https://www.reddit.com/r/MachineLearning/comments/4qbvbh/our_neural_net_dreams_of_cars/,buritist,1467148898,,2,0
989,2016-6-29,2016,6,29,6,4qbwzt,Spinning off multiple AWS instances for Caffe/DIGITS,https://www.reddit.com/r/MachineLearning/comments/4qbwzt/spinning_off_multiple_aws_instances_for/,Greendogo,1467149485,"Does anyone know where to start looking into using multiple Amazon (AWS) GPU instances to train on through the DIGITS interface?

I finally got the tutorial for object detection in the new DIGITS 4 release up and running and it now tells me I have an ETA of ~6 days.

I'd much rather spend those 6 days finding a faster way to get this working across multiple instances so the next 7 times I have to train a network it will be faster.",1,0
990,2016-6-29,2016,6,29,6,4qbz19,How Snapchat's filters work,https://www.reddit.com/r/MachineLearning/comments/4qbz19/how_snapchats_filters_work/,evc123,1467150193,,7,18
991,2016-6-29,2016,6,29,7,4qc9e5,Unifying theory of ML: Can most or all of ML be seen as a special case of bayesian inference?,https://www.reddit.com/r/MachineLearning/comments/4qc9e5/unifying_theory_of_ml_can_most_or_all_of_ml_be/,dsijl,1467153907,I've seen this touted somewhere. Any thoughts? ,13,14
992,2016-6-29,2016,6,29,8,4qcl2l,New paper from DeepMind: Early Visual Concept Learning with Unsupervised Deep Learning,https://www.reddit.com/r/MachineLearning/comments/4qcl2l/new_paper_from_deepmind_early_visual_concept/,[deleted],1467158062,[deleted],0,1
993,2016-6-29,2016,6,29,9,4qcpj7,Backprop in Vanilla RNN: Difference between this code and formula,https://www.reddit.com/r/MachineLearning/comments/4qcpj7/backprop_in_vanilla_rnn_difference_between_this/,xingdongrobotics,1467159707,"There is a question I feel a bit confused. For the code in Andrej Karpathy's [min-char-rnn](https://gist.github.com/karpathy/d4dee566867f8291f086), for the backprop process for hidden state h and hidden weight Whh it shows following 

    dh = np.dot(Why.T, dy) + dhnext # backprop into h
    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity
    dbh += dhraw
    dWxh += np.dot(dhraw, xs[t].T)
    dWhh += np.dot(dhraw, hs[t-1].T)
    dhnext = np.dot(Whh.T, dhraw)

And in for example Nando de Freitas's [slide on RNN, page 5](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/lecture11.pdf), it shows the formula with actually more operations to calculate, e.g. the chain of products on derivative of h.

What I think currently is that, the formula on slide shows the analytic representation of chain rule, but Andrej's code is actually backprop with computational graph, so that it is the reason why there is no chain of product in code. Is it correct ?",1,2
994,2016-6-29,2016,6,29,10,4qd040,NVIDIA Docker: Deep Learning Framework Deployment Made Easy,https://www.reddit.com/r/MachineLearning/comments/4qd040/nvidia_docker_deep_learning_framework_deployment/,harrism,1467163743,,27,114
995,2016-6-29,2016,6,29,10,4qd114,Google researchers teach AIs to see the important parts of images  and tell you about them,https://www.reddit.com/r/MachineLearning/comments/4qd114/google_researchers_teach_ais_to_see_the_important/,mastazi,1467164094,,3,0
996,2016-6-29,2016,6,29,11,4qd5rq,PyGame Learning Environment (PLE) -- Reinforcement Learning Environment in Python,https://www.reddit.com/r/MachineLearning/comments/4qd5rq/pygame_learning_environment_ple_reinforcement/,phphong,1467165891,,3,34
997,2016-6-29,2016,6,29,11,4qd731,Torch vs TensorFlow?,https://www.reddit.com/r/MachineLearning/comments/4qd731/torch_vs_tensorflow/,c0cky_,1467166417,"Which do people prefer, which one is easier to pick up, what is the lifetime of each project looking like for the future?",9,0
998,2016-6-29,2016,6,29,11,4qd8tq,Combat AI beats the Air Force's top tactical experts,https://www.reddit.com/r/MachineLearning/comments/4qd8tq/combat_ai_beats_the_air_forces_top_tactical/,jay_jay_man,1467167091,,1,3
999,2016-6-29,2016,6,29,11,4qdabw,predicting probability of at least one occurrence when training on number of occurences,https://www.reddit.com/r/MachineLearning/comments/4qdabw/predicting_probability_of_at_least_one_occurrence/,[deleted],1467167695,[deleted],0,0
1000,2016-6-29,2016,6,29,13,4qdovp,Generative adversial networks explained,https://www.reddit.com/r/MachineLearning/comments/4qdovp/generative_adversial_networks_explained/,[deleted],1467173894,[deleted],0,1
1001,2016-6-29,2016,6,29,13,4qdoyu,How to increase the memory limit on AWS for Tensorflow? (Bounty inside!),https://www.reddit.com/r/MachineLearning/comments/4qdoyu/how_to_increase_the_memory_limit_on_aws_for/,SatoshiRoberts,1467173935,"Hello, 

I am using AWS to run some projects with Tensorflow, and I chose the g2.8xlarge model, which has 60GB of RAM. But when I run my program, the terminal says I only have 4gb free. I'm using a library called neural-style:

https://github.com/anishathalye/neural-style

and this AMI: 

https://console.aws.amazon.com/ec2/v2/home?region=us-east-1#LaunchInstanceWizard:ami=ami-e191b38b


Any suggestions on how to take advantage of all 60 gb's of data on the EC2 instance?

Best answer will get a $2 Bitcoin tip! And a $20 bounty for anyone who comes up with an AMI that is set up to do it automatically! (with the same dependencies as the AMI posted above)",2,0
1002,2016-6-29,2016,6,29,13,4qdr74,Learning C/C++ for ML,https://www.reddit.com/r/MachineLearning/comments/4qdr74/learning_cc_for_ml/,[deleted],1467174937,[deleted],4,2
1003,2016-6-29,2016,6,29,15,4qe518,undergraduate internships in machine learning,https://www.reddit.com/r/MachineLearning/comments/4qe518/undergraduate_internships_in_machine_learning/,bionerd2,1467181930,"Hey all,

I know you're probably sick and tired of these types of posts. I'll be quick. I'm getting a BS in CS @ Stanford, and have taken 5 courses in ML, and ~4 in optimization/probability theory/info theory. I've done deep learning research (which I can elaborate more on if necessary). I'm hoping to work next summer at a place like Microsoft Research, basically a non-university research environment. Do you think that's possible? Have any tips as to how I could work something like that out?",8,0
1004,2016-6-29,2016,6,29,16,4qebib,I have a Physics BS and want a career in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4qebib/i_have_a_physics_bs_and_want_a_career_in_machine/,Dreamscape17,1467185389,"I recently graduated from uni with a degree in physics and I am really interested in machine learning. I plan to take the University of Washington course on Coursera for an introduction to the subject. After I finish this, what more would I need to do before I'm ready to apply for jobs in machine learning? I've seen a few posts around here of people saying you need a Masters/Ph.D for these types of jobs but I don't plan on graduate school unless an employer would pay for it. If I have the experience wouldn't that ultimately trump having a degree? How long would it take me to become experienced enough for these types of jobs? Thanks in advance",2,0
1005,2016-6-29,2016,6,29,16,4qecjc,"Question to Andrew NG sir. Machine learning and a ML career, Andrew Ng's course(Mooc)",https://www.reddit.com/r/MachineLearning/comments/4qecjc/question_to_andrew_ng_sir_machine_learning_and_a/,jeetjust4u,1467185977,[removed],0,1
1006,2016-6-29,2016,6,29,16,4qee2i,How to create data set for training for object localization and detection? Any annotation tools?,https://www.reddit.com/r/MachineLearning/comments/4qee2i/how_to_create_data_set_for_training_for_object/,n00bto1337,1467186835,"I need to detect objects inside my image, a single image can have multiple objects. I have my images for the training set, but how do I draw bounding boxes and label them? Any annotation tools for this?

Also, are there any existing neural architecture, preferably for Python, which I can use for training with my data set?",1,3
1007,2016-6-29,2016,6,29,17,4qehy8,Lower testing error resulting in higher error on competition leaderboard?,https://www.reddit.com/r/MachineLearning/comments/4qehy8/lower_testing_error_resulting_in_higher_error_on/,[deleted],1467189129,[deleted],0,1
1008,2016-6-29,2016,6,29,17,4qej95,Testing loss inversely correlating with leaderboard loss in competition?,https://www.reddit.com/r/MachineLearning/comments/4qej95/testing_loss_inversely_correlating_with/,fpgaminer,1467189902,"Hoping for a little guidance here, as I'm fairly new to machine learning competitions and I've run into a rather confusing problem.

I decided to cut my teeth on numer.ai, because the dataset is so straightforward (21 features, binary classification).  I split the training data into 90% training data and 10% testing and then train my model.  My model gets a low testing loss (0.684), but when I try the predictions from that model the leaderboard shows a terrible loss (&gt;0.73).  If I use the same model from earlier in the training, so it has a higher testing loss (0.691), I get a better score on the leaderboard (0.690).

The model is fresh, with no hyper-parameter tuning.  It has never seen the 10% testing data.  So unless I'm mistaken improved testing loss on my side must indicate that the model is generalizing and should work well on more data it hasn't seen.  In other words, it isn't overfitting.  Despite this, the leaderboard is behaving as if I _am_ overfitting.  Very confusing.

I've looked over my code and tried a few different approaches but all get the same results.  Any help from people who have done other competitions and ran into similar issues would be great.  I don't even care about the reward from numer.ai at this point, I just want to figure out what's up with this weird ""bug"".",6,1
1009,2016-6-29,2016,6,29,18,4qeqlf,Check the Comma.ai programming challenge.,https://www.reddit.com/r/MachineLearning/comments/4qeqlf/check_the_commaai_programming_challenge/,senorstallone,1467194252,,8,0
1010,2016-6-29,2016,6,29,19,4qerhu,"Question towards ""semi-supervised learning deep generative model"" by Kingma",https://www.reddit.com/r/MachineLearning/comments/4qerhu/question_towards_semisupervised_learning_deep/,[deleted],1467194782,[removed],0,1
1011,2016-6-29,2016,6,29,20,4qexnm,Code for CVPR 2016 Papers,https://www.reddit.com/r/MachineLearning/comments/4qexnm/code_for_cvpr_2016_papers/,impairment,1467198248,,0,18
1012,2016-6-29,2016,6,29,20,4qf2xt,Generative Moment Matching Networks - TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4qf2xt/generative_moment_matching_networks_tensorflow/,[deleted],1467200948,[deleted],0,1
1013,2016-6-29,2016,6,29,21,4qf7t1,Did anyone successfully reproduce xnor-net results on imagenet?,https://www.reddit.com/r/MachineLearning/comments/4qf7t1/did_anyone_successfully_reproduce_xnornet_results/,serge_cell,1467203040,"Here is the original paper
http://arxiv.org/abs/1603.05279
which claim around 70% top-5 accuracy on ImageNet with 1-bit network.
There were several attempt to reproduce result (one of them on github) but there weren't any claims of success that I know of (I failed to reproduce as well)
The algorithm in the original paper is not described in details, so there could be some critical piece missing in attempts.
Now there is this paper  
https://arxiv.org/abs/1606.06160
and author explicitly saying they failed to reproduce xnor-net too.
So was there any success in reproducing xnor-net at all?",5,16
1014,2016-6-29,2016,6,29,21,4qfb8e,Straightening machine,https://www.reddit.com/r/MachineLearning/comments/4qfb8e/straightening_machine/,Mzfeeder,1467204347,[removed],1,0
1015,2016-6-29,2016,6,29,22,4qfk9l,"[FUNNY] David J Bland on Twitter: ""I've analyzed the Silicon Valley Bot Startup trend and created a handy venn diagram to help explain it.""",https://www.reddit.com/r/MachineLearning/comments/4qfk9l/funny_david_j_bland_on_twitter_ive_analyzed_the/,JonathanOron,1467207683,,25,172
1016,2016-6-29,2016,6,29,23,4qfqxa,Teaching XSS to a machine,https://www.reddit.com/r/MachineLearning/comments/4qfqxa/teaching_xss_to_a_machine/,galapag0,1467210002,,0,0
1017,2016-6-29,2016,6,29,23,4qftuf,How to preprocess images with different size for training a convolutional neural network?,https://www.reddit.com/r/MachineLearning/comments/4qftuf/how_to_preprocess_images_with_different_size_for/,ChorisSteve,1467210943,[removed],0,1
1018,2016-6-29,2016,6,29,23,4qfu0z,"A Universal Deep Learning Framework for Both FPGA, GPU Simultaneously",https://www.reddit.com/r/MachineLearning/comments/4qfu0z/a_universal_deep_learning_framework_for_both_fpga/,[deleted],1467211003,[deleted],3,2
1019,2016-6-29,2016,6,29,23,4qfw9g,CVPR Daily of Wednesday,https://www.reddit.com/r/MachineLearning/comments/4qfw9g/cvpr_daily_of_wednesday/,Gletta,1467211689,"http://www.rsipvision.com/CVPR2016-Wednesday/ 

Enjoy!",1,0
1020,2016-6-30,2016,6,30,0,4qfypx,Yet another q-learning with NN question,https://www.reddit.com/r/MachineLearning/comments/4qfypx/yet_another_qlearning_with_nn_question/,davenirline,1467212456,"From reading various sources, I understood that the lookup table can be replaced with a neural network. I'm still trying to wrap my head around it. This is how I understand it so far:

The Q table will be replaced with an NN with S + A inputs (state property count and action property count), some X hidden nodes, and one output node. Basically, the NN spits out the Q value for a certain state and action.

What I don't understand yet is the training. Initially, the ""Q-network"" has random weights. Let's say in one training episode, I performed an action and computed the reward r. Do I use this initial NN to compute the current Q(s, a)? When using a lookup table, this would have been zero.

When doing backprop to correct the NN, what value do I check it against? I've seen ""QTarget - LastQValueFromNN"". But what is QTarget? Is it the computed Q value using the current NN?

Or am I even asking the right questions?",5,4
1021,2016-6-30,2016,6,30,0,4qg1ga,Automatic autoencoding variational Bayes for latent dirichlet allocation with PyMC3,https://www.reddit.com/r/MachineLearning/comments/4qg1ga/automatic_autoencoding_variational_bayes_for/,dsijl,1467213298,,0,5
1022,2016-6-30,2016,6,30,0,4qg5ye,What is the difference between these two courses?,https://www.reddit.com/r/MachineLearning/comments/4qg5ye/what_is_the_difference_between_these_two_courses/,tlfranklin76,1467214668,"I found Gilbert Strang's linear algebra youtube videos from the wiki.  From there I found the entire class material online on MIT OCW.  However, I see two versions.  Does anyone know the difference between these versions and have a recommendation on which I should take?

http://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/

http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/",2,0
1023,2016-6-30,2016,6,30,0,4qg8uv,Questions thread #6 2016.06.29,https://www.reddit.com/r/MachineLearning/comments/4qg8uv/questions_thread_6_20160629/,[deleted],1467215537,[deleted],0,1
1024,2016-6-30,2016,6,30,0,4qg8yq,Questions thread #7 2016.06.29,https://www.reddit.com/r/MachineLearning/comments/4qg8yq/questions_thread_7_20160629/,feedtheaimbot,1467215572,"**Please post your questions here instead of creating a new thread. Helps keep the sub clean. :) Encourage others who create new posts for questions to post here instead!**

Thread will stay alive until next one so keep posting after the date in the title. 

Thanks to everyone for answering questions in the previous thread!

Previous threads:

* [Questions thread #6 2016.05.23](https://www.reddit.com/r/MachineLearning/comments/4kq3jx/questions_thread_6_20160523/)

* [Questions thread #5 2016.05.07]
(https://www.reddit.com/r/MachineLearning/comments/4ibv66/questions_thread_5_20160507/)

* [Questions thread #4 2016.04.22]
(https://www.reddit.com/r/MachineLearning/comments/4fytfp/questions_thread_4_20160422/)

* [Questions Thread #3 2016.04.07](https://www.reddit.com/r/MachineLearning/comments/4dthzx/questions_thread_3_20160407/)

* [Simple Questions Thread #2 + Meta - 2016.03.23](https://www.reddit.com/r/MachineLearning/comments/4bp1ck/simple_questions_thread_2_meta_20160323/)

* [Simple Questions Thread #1 - 2016.03.08](https://www.reddit.com/r/MachineLearning/comments/49k54u/simple_questions_thread_20160308/)",388,31
1025,2016-6-30,2016,6,30,1,4qgcyd,I don't know if this belongs here.,https://www.reddit.com/r/MachineLearning/comments/4qgcyd/i_dont_know_if_this_belongs_here/,ssreekanth2000,1467216822,"I need a dataset containing images classified by their property(Farmland,Ocean,Forest,Urban area,Desert). If its not relevant to this subredddit can anybody suggest one. Thank you",1,0
1026,2016-6-30,2016,6,30,1,4qgi07,Identifying the Gender of a Voice using Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4qgi07/identifying_the_gender_of_a_voice_using_machine/,primaryobjects,1467218404,,1,2
1027,2016-6-30,2016,6,30,1,4qgjz1,What is ML/stats equivalent of this problem?,https://www.reddit.com/r/MachineLearning/comments/4qgjz1/what_is_mlstats_equivalent_of_this_problem/,akshayxyz,1467219021,"A colleague asked me this - 

Lets say there is a stream of random integers coming from uniform distribution. It is expected that odds and evens will be roughly same. If I observe that in recent history odds are significantly more than evens - then in near future I can start betting on evens - until they catch up with odds to certain degree.

My ML-instinctive reply was - this is nonsense. I wanted to know the ML/stats equivalent of such problem, so that I can make a informed comment and judgement.",5,2
1028,2016-6-30,2016,6,30,2,4qgoiz,Dog Breed Identifier iOS App,https://www.reddit.com/r/MachineLearning/comments/4qgoiz/dog_breed_identifier_ios_app/,hartator,1467220410,,2,0
1029,2016-6-30,2016,6,30,3,4qh3ln,Machine Learning Problem Bible,https://www.reddit.com/r/MachineLearning/comments/4qh3ln/machine_learning_problem_bible/,Neb519,1467225020,"(Posted on r/datascience and someone suggested I cross post it here)

I recently created [Machine Learning Problem Bible (MLPB)](https://github.com/ben519/MLPB), a github repo with organized machine learning problems. It serves a few purposes:

* I compete on Kaggle. When I start a competition, I usually look for solutions to similar problems to help me get started. E.g. if the objective is to minimize RMSLE (root mean squared log error) I like to look for code samples which also minimize RMSLE. If the problem involves classifying an unbalanced target, I like to see how other people dealt with this. MLPB was designed to make it easy for me (and hopefully others) to search for problems and solutions with specific characteristics like this.

* A single problem should (ideally) contain a bunch of sample solutions, so you can compare scikit-learn's RandomForestClassifier to R's randomForest to XGBoost (in Python or R).

* Hopefully having a standardized format for sample problems and sample solutions should make it really easy to get up and running with a new model or language you want to learn. Lately I've been learning XGBoost and I feel like finding the right blog/article/snippet of documentation related to what I want to do is half the battle.

The repo is in its infancy, but it has some valuable scripts and datasets. I wanted to post this in hopes of getting feedback and possibly other people interested in contributing.",7,12
1030,2016-6-30,2016,6,30,3,4qh4ie,Regression vs classification,https://www.reddit.com/r/MachineLearning/comments/4qh4ie/regression_vs_classification/,Kiuhnm,1467225302,"I was reading the [notes](http://cs231n.github.io/neural-networks-2/) for the cs231n course when I came across this:

&gt; Word of caution: It is important to note that the L2 loss is much harder to optimize than a more stable loss such as Softmax. Intuitively, it requires a very fragile and specific property from the network to output exactly one correct value for each input (and its augmentations). Notice that this is not the case with Softmax, where the precise value of each score is less important: It only matters that their magnitudes are appropriate. Additionally, the L2 loss is less robust because outliers can introduce huge gradients. When faced with a regression problem, first consider if it is absolutely inadequate to quantize the output into bins. For example, if you are predicting star rating for a product, it might work much better to use 5 independent classifiers for ratings of 1-5 stars instead of a regression loss. Classification has the additional benefit that it can give you a distribution over the regression outputs, not just a single output with no indication of its confidence. If youre certain that classification is not appropriate, use the L2 but be careful: For example, the L2 is more fragile and applying dropout in the network (especially in the layer right before the L2 loss) is not a great idea.

&gt; When faced with a regression task, first consider if it is absolutely necessary. Instead, have a strong preference to discretizing your outputs to bins and perform classification over them whenever possible.

Do you agree with this analysis? I find it perplexing... Since 1&lt;2&lt;3&lt;4&lt;5, this is clearly a regression problem, so I don't think that throwing away the ""order assumption"" is the right approach. We should still be doing regression but make it more robust. What's your opinion on this matter?",6,3
1031,2016-6-30,2016,6,30,3,4qh84l,anyone here know theano/lasagne?,https://www.reddit.com/r/MachineLearning/comments/4qh84l/anyone_here_know_theanolasagne/,entercaspa,1467226398,"Its really difficult to decipher the errors from a wrapper of theano and I need help to implement a simple neural network with 20 inputs and 1 label (output). Ive set up one input, two hidden layers and two dropout layers inbetween with an output layer spitting out one label, and am having trouble with it, if theres anyone willing to help out a brother with his first neural network give us a shout! Ill give you karma haha 

learning python and machine learning at once is tough...",4,0
1032,2016-6-30,2016,6,30,3,4qh8yw,Wide and Deep Learning: Better Together with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4qh8yw/wide_and_deep_learning_better_together_with/,luiscosio,1467226652,,14,62
1033,2016-6-30,2016,6,30,4,4qhd7s,A Naive Bayesian Classifier to make strong (naive) independence assumptions between textual features (written in APL),https://www.reddit.com/r/MachineLearning/comments/4qhd7s/a_naive_bayesian_classifier_to_make_strong_naive/,itsmatthewc,1467228007,,1,1
1034,2016-6-30,2016,6,30,4,4qhfbh,Use your computational skills to advance biomedical research and win cash by competing in the CMap Inference Challenge,https://www.reddit.com/r/MachineLearning/comments/4qhfbh/use_your_computational_skills_to_advance/,genexsamples,1467228682,,0,1
1035,2016-6-30,2016,6,30,4,4qhjtp,Visualize Algorithms based on the Backpropagation,https://www.reddit.com/r/MachineLearning/comments/4qhjtp/visualize_algorithms_based_on_the_backpropagation/,mathijs_appbrain,1467230038,,1,1
1036,2016-6-30,2016,6,30,4,4qhk03,"After implementing a few toy models, I'm now trying to implement a model from a paper for the first time; getting garbage results. How to decide if it's my fault or the author's?",https://www.reddit.com/r/MachineLearning/comments/4qhk03/after_implementing_a_few_toy_models_im_now_trying/,BLAND_AS_OVALTINE,1467230089,"I've learned the basics of ML from Peter Flach's book over the past few months, and I've implemented some simple toy models (MNIST, noisy function prediction, etc.). I'm attempting to implement my first serious model from the [literature](http://www.cs.au.dk/~cstorm/students/Chong_Jul2009.pdf), which is a prediction model for daily FX rates.

To quickly summarize, the model can be broken down into 3 steps.

1. Perform a four-level stationary wavelet decomposition of the past q prices.
2. Train a separate neural network on each set of detail coefficients and the level 4 approximation to predict the next price.
3. Feed these predictions and some statistical features (chosen by exhaustive selection) into another neural network, which outputs the final prediction.

The paper claims some very impressive results (see e.g. Figure 5.8 and Table 5.9). However, my results have been... not impressive. No run has predicted the next day's direction better than chance, much less gotten a validation MSE on the order of 1e-6!

Since this is my first go at implementing a real model, I'm assuming the fault is mine. My suspicion is that I'm doing the input normalization incorrectly, because the paper is rather vague about how this was done.

&gt; All the input data are normalized to the range [-1,1] before being fed into the neural networks.

I've normalized the input to this range in several ways, none of which worked.

Beyond me screwing the normalization up, I have no idea what I could be doing wrong, because the paper uses very simple techniques. Unless Matlab's NN toolbox allows its single-hidden layer ANNs to consort with spirits, I imagine the problem isn't caused by the fact that I'm using Keras and Python.

In summary, I have three questions:

* How do you decide when you're wasting your time trying to reproduce a paper's claims?
* What mistakes did you make when you started implementing real models from papers?
* How do you typically normalize input data to a neural network? Do you normalize the expected results as well?

Thanks for your time.",22,10
1037,2016-6-30,2016,6,30,5,4qhrqy,Video lectures from Deep Learning course from Yann Lecunn (http://cilvr.cs.nyu.edu/doku.php?id=courses:deeplearning2015:start),https://www.reddit.com/r/MachineLearning/comments/4qhrqy/video_lectures_from_deep_learning_course_from/,shash273,1467232533,"Video lectures from deep learning course from Yann Lecunn are missing from the course's official web page. These vides from Yann were very intuitive and one of the best I had seen on Neural Networks, but now they are missing. Does anyone knows how can I access them?",12,26
1038,2016-6-30,2016,6,30,6,4qhx99,AMD is apparently working on better machine learning support.,https://www.reddit.com/r/MachineLearning/comments/4qhx99/amd_is_apparently_working_on_better_machine/,rndnum123,1467234291,"The SVP of Radeon Technology (AMD's GPU division), Raja Koduri just answered a question about machine learning software support during the latest AMD reddit AMA:
&gt;You will definitely see more machine learning activity on GPUOpen in coming months. We are working with all the popular open source frame-works to enable amazing acceleration through our Radeon Open Compute (ROC) platform.
And I look forward to the day when a RX480 can be your girlfriend:) Is she named Ruby?:)

This was a response to u/office_chair 's question you can find [here](https://www.reddit.com/r/pcmasterrace/comments/4qfy9d/i_work_at_amd_the_time_has_come_to_ama_about/d4sluh0) - in corner lower left",7,32
1039,2016-6-30,2016,6,30,6,4qi6yr,Making open source data more available,https://www.reddit.com/r/MachineLearning/comments/4qi6yr/making_open_source_data_more_available/,alpyhp,1467237448,,1,1
1040,2016-6-30,2016,6,30,7,4qi848,Adaptive Computation Time Algorithm Implementation in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4qi848/adaptive_computation_time_algorithm/,MarkusDeNeutoy,1467237825,,0,1
1041,2016-6-30,2016,6,30,7,4qi89j,Going beyond full utilization: The inside scoop on Nervanas Winograd kernels,https://www.reddit.com/r/MachineLearning/comments/4qi89j/going_beyond_full_utilization_the_inside_scoop_on/,mttd,1467237872,,0,11
1042,2016-6-30,2016,6,30,8,4qimqs,Convergence and Overfitting,https://www.reddit.com/r/MachineLearning/comments/4qimqs/convergence_and_overfitting/,[deleted],1467243116,[deleted],1,2
1043,2016-6-30,2016,6,30,12,4qjpnk,Google extends TensorFlow machine learning to iOS,https://www.reddit.com/r/MachineLearning/comments/4qjpnk/google_extends_tensorflow_machine_learning_to_ios/,alijafri,1467258380,,1,4
1044,2016-6-30,2016,6,30,13,4qjv1g,Pre-trained neural networks,https://www.reddit.com/r/MachineLearning/comments/4qjv1g/pretrained_neural_networks/,gregw134,1467260823,Does anybody have a list of pre-trained neural networks available for download? I'm trying to contribute some ML to Apache Nifi. Thanks,6,2
1045,2016-6-30,2016,6,30,13,4qjxqb,Simple Question: How do you pronounce Theano?,https://www.reddit.com/r/MachineLearning/comments/4qjxqb/simple_question_how_do_you_pronounce_theano/,beijingspacetech,1467262064,"I saw this on the internet:

&gt;I think I say roughly /i..no/ (using the international phonetic alphabet)

But wasn't quite sure I figured it out correctly. How do you pronounce Theano?",8,1
1046,2016-6-30,2016,6,30,14,4qk37w,Generative adversial networks explained,https://www.reddit.com/r/MachineLearning/comments/4qk37w/generative_adversial_networks_explained/,kvfrans,1467264780,,6,7
1047,2016-6-30,2016,6,30,14,4qk4mu,"Working with a new framework, Torch or Tensorflow?",https://www.reddit.com/r/MachineLearning/comments/4qk4mu/working_with_a_new_framework_torch_or_tensorflow/,dimmtree,1467265506,"I was hoping for advice on choosing a new framework? I've been using caffe, but am migrating away from primarily image data.

Tensorflow seems to be slower, but some people think it will be ""the one"" at some point. I do not have a strong preference towards python versus lua.

Torch is currently well supported, I heard some worrys about lua since a head person left?

I would love advice.",41,16
1048,2016-6-30,2016,6,30,15,4qkaj3,Generative Moment Matching Networks - Tensorflow,https://www.reddit.com/r/MachineLearning/comments/4qkaj3/generative_moment_matching_networks_tensorflow/,siddharth-agrawal,1467268653,,0,1
1049,2016-6-30,2016,6,30,16,4qkdi6,"Stanford Question Answering Dataset (SQuAD): 107,785 question-answer pairs on 536 articles",https://www.reddit.com/r/MachineLearning/comments/4qkdi6/stanford_question_answering_dataset_squad_107785/,metacurse,1467270258,,3,74
1050,2016-6-30,2016,6,30,16,4qkis4,"Learn to code Dynamic Vanilla RNN, GRU, LSTM,2layer Stacked LSTM with Tensorflow",https://www.reddit.com/r/MachineLearning/comments/4qkis4/learn_to_code_dynamic_vanilla_rnn_gru_lstm2layer/,kazi_shezan,1467273130,,0,0
1051,2016-6-30,2016,6,30,18,4qkr40,SiC Semiconductor Devices Industry Report - Global and Chinese Market,https://www.reddit.com/r/MachineLearning/comments/4qkr40/sic_semiconductor_devices_industry_report_global/,aiden_11,1467277756,[removed],0,1
1052,2016-6-30,2016,6,30,19,4qkwbx,Plate Heat Exchangers,https://www.reddit.com/r/MachineLearning/comments/4qkwbx/plate_heat_exchangers/,ajayfast,1467280892,,1,1
1053,2016-6-30,2016,6,30,19,4qkx75,Question about bucketing in Seq2Seq-Model in tensorflow,https://www.reddit.com/r/MachineLearning/comments/4qkx75/question_about_bucketing_in_seq2seqmodel_in/,moebb,1467281381,"Hi guys,

I just want to get sure I understand the source code of the [Seq2Seq-Model](https://www.tensorflow.org/versions/r0.9/tutorials/seq2seq/index.html#bucketing-and-padding) completely. When they have 4 buckets like `_buckets = [(5, 5), (10, 10), (20, 20), (40, 40)]`, then they have 4 independent graphs where each graph has its own RNN cells without weight sharing between the RNN cells from the other buckets. In other words, the first bucket has 5 RNN cells in the encoder and 5 cells in the decoder, the second has 10 new RNN cells in the encoder and 10 new RNN cells in the decoder. Is this correct? Is this correct?

Thanks in advance!",2,0
1054,2016-6-30,2016,6,30,19,4ql0qo,Thresholded attention (Neural Machine Translation),https://www.reddit.com/r/MachineLearning/comments/4ql0qo/thresholded_attention_neural_machine_translation/,alrojo,1467283472,"How do I threshold attention on recurrent neural network states?

When applying attention to a sequence (such as in [Bahdanau et al. 2014](https://arxiv.org/abs/1409.0473)) produced by a RNN, attention is able to collect the most important information from that sequence.

However, when collecting that information, often the attention focuses on a fairly narrow part of the sequence, but still collects information in the sense of ""noise"" from the rest of the sequence.

Attention works by producing a softmax over a sequence, and then combining every point in the sequence accordingly to the weighting of the softmax.
However, some sequences might be very long, and only have interesting information in certain places, combining all parts might be very expensive and contain much noise as compared to sparsely using the interesting points in the sequence.
This is also one of the currently major disadvantages of attention, it an only be used on a sequence of a certain length.

An attention softmax could look something like this:

`attention_softmax = [0.01, 0.01, 0.01, 0.4, 0.01, 0.01, 0.01, 0.01, 0.4, 0.01, 0.01, 0.01]`

Where you could imagine the `0.01` could be even smaller and strech a vast sequence space.

My goal would be having the ability to threshold the attention softmax, which would leave the attention softmax something like:

`attention_softmax_thresholded(treshold=0.1) = [0., 0., 0., 0.4, 0., 0., 0., 0., 0.4, 0., 0., 0.]`

and then perhaps normalize it to sum to 1:

`attention_softmax_thresholded_normalized = [0., 0., 0., 0.5, 0., 0., 0., 0., 0.5, 0., 0., 0.]`

Such operation would be way cheaper computationally as it is sparse.
However, it is my understanding that such threshold would ruin the gradient?

Other litterature, such as [Leong et al., 2015](http://arxiv.org/abs/1508.04025) uses a sigmoid to compute a place in the sequence to do a ""look-up"" and then applies a gaussian distribution.
But this technique is only capable of chosing a set amount of places to look-up, where as thresholding would allow choosing 1 to max_seq_len (given the threshold)",4,10
1055,2016-6-30,2016,6,30,19,4ql27z,Fine tuning GoogLeNet,https://www.reddit.com/r/MachineLearning/comments/4ql27z/fine_tuning_googlenet/,niujin,1467284304,"Hi,

I am fine tuning GoogLeNet for a face classification task using Caffe.

I have found that of my fine tuned network's 3 outputs, the 1st and 2nd (the auxiliary classifiers) do better than the final classifier:

layer|error rate
:---|---:
loss1/classifier|7.5%
loss2/classifier|7.7%
loss3/classifier|10.5%

I followed the steps on the Flickr Style tutorial and renamed all 3 classification layers so they were reinitialised. Also I boosted their LR by 10x.

I am using my fine tuned model to calculate the distance between two faces by using the Euclidean distance between the output vectors.

Has anyone else fine tuned GoogLeNet for their own purpose? How did you set the learning rates?

PS once I get good performance I will use triplet learning to improve accuracy further on the last layer.

Thanks",0,1
1056,2016-6-30,2016,6,30,22,4qljji,"Are there any other ""memory-based"" machine learning algorithms, other than RNNs?",https://www.reddit.com/r/MachineLearning/comments/4qljji/are_there_any_other_memorybased_machine_learning/,asscrack_dt,1467291783,RNNs are great because they have memory. Are there any other classes of models that hold memory?,31,13
1057,2016-6-30,2016,6,30,23,4qlsyw,Anyone done the Udacity Machine Learning nanodegree program?,https://www.reddit.com/r/MachineLearning/comments/4qlsyw/anyone_done_the_udacity_machine_learning/,TheMoskowitz,1467295212,"[Here's the link](https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009#)

It looks really cool but it's also very time-consuming and expensive relative to most online learning.

Still a pittance compared to a university degree however.

Anyone tried it/trying it now?",38,55
1058,2016-6-30,2016,6,30,23,4qlu9e,Genetic Programming + Neural Networks = Evolving AI,https://www.reddit.com/r/MachineLearning/comments/4qlu9e/genetic_programming_neural_networks_evolving_ai/,Ruthenson,1467295647,,29,33
