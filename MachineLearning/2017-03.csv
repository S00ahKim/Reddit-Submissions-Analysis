,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2017-3-1,2017,3,1,9,5wrkbi,[R] Neural Map: Structured Memory for Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/5wrkbi/r_neural_map_structured_memory_for_deep/,The_Man_of_Science,1488326435,,2,9
1,2017-3-1,2017,3,1,9,5wrvgn,where are ILSVRC2016 techincal reports,https://www.reddit.com/r/MachineLearning/comments/5wrvgn/where_are_ilsvrc2016_techincal_reports/,omar_core,1488329183,[removed],0,1
2,2017-3-1,2017,3,1,10,5ws6p2,[D] Questions before starting with machine learning,https://www.reddit.com/r/MachineLearning/comments/5ws6p2/d_questions_before_starting_with_machine_learning/,ramananb,1488332412,,0,0
3,2017-3-1,2017,3,1,11,5wsaxf,[P] Could a Neuroscientist Understand a Microprocessor? (implications for reverse engineering),https://www.reddit.com/r/MachineLearning/comments/5wsaxf/p_could_a_neuroscientist_understand_a/,kit_hod_jao,1488333705,,13,67
4,2017-3-1,2017,3,1,12,5wsu41,"Tokyo Exchange to Deploy Machine Learning, AI Surveillance",https://www.reddit.com/r/MachineLearning/comments/5wsu41/tokyo_exchange_to_deploy_machine_learning_ai/,senornikos,1488339509,,0,1
5,2017-3-1,2017,3,1,12,5wsujr,Wanted: Back-end and Front-end developers for cryptocurrency project,https://www.reddit.com/r/MachineLearning/comments/5wsujr/wanted_backend_and_frontend_developers_for/,loba333,1488339655,[removed],0,1
6,2017-3-1,2017,3,1,13,5wt08n,[D] Applying stochastic optimization to Gaussian Process classification in GPFlow,https://www.reddit.com/r/MachineLearning/comments/5wt08n/d_applying_stochastic_optimization_to_gaussian/,sexPekes,1488341443,"I'm trying to implement what GPFlow calls SVIGP (Scalable Variational Inference Gaussian Process, I think). Essentially it's the same as a sparse GP but with a gradient descent algorithm as the optimization function. An example of an SVGP in the library is [here](https://gpflow.readthedocs.io/en/latest/notebooks/classification.html).

I'm trying to use Adam (from tf's optimization functions) as the optimizer. [Here](https://gpflow.readthedocs.io/en/latest/notebooks/svi_test.html) is an example of it being used for SVIGP regression in GPFlow. I'm running the model right now but the Adam optimization step is taking an incredibly long amount of time. I suspect this is because of the 'maxiters=np.inf' parameter in the optimization function. Is there an intuitive reason why this is used when using Adam as the optimizer? In all of the other examples in the docs it's a relatively small constant. 
",0,1
7,2017-3-1,2017,3,1,13,5wt4lt,"[N] NVIDIA 1080ti announced: $700, March 5th, 11GB - Titan replacement",https://www.reddit.com/r/MachineLearning/comments/5wt4lt/n_nvidia_1080ti_announced_700_march_5th_11gb/,gwern,1488342911,,84,190
8,2017-3-1,2017,3,1,15,5wto2q,[R] Policy gradients and LSTM,https://www.reddit.com/r/MachineLearning/comments/5wto2q/r_policy_gradients_and_lstm/,andyzth,1488348633,"So far I have looked at http://people.idsia.ch/~daan/papers/icann07.pdf to learn policy gradients for LSTMs. However, while the paper mentions eligibility back propagation through time, it doesn't explain how its different from back propagation through time. Is anyone aware of what this means? And are there any other papers which use policy gradients in conjunction with LSTMs or RNNs?

Thanks",1,6
9,2017-3-1,2017,3,1,16,5wu2b0,automatic hydraulic die cutting machine / cutting press / clicker press,https://www.reddit.com/r/MachineLearning/comments/5wu2b0/automatic_hydraulic_die_cutting_machine_cutting/,honggang,1488353250,[removed],0,1
10,2017-3-1,2017,3,1,16,5wu4k2,if you need a hydraulic die cutting press?,https://www.reddit.com/r/MachineLearning/comments/5wu4k2/if_you_need_a_hydraulic_die_cutting_press/,honggang,1488354020,[removed],0,1
11,2017-3-1,2017,3,1,18,5wuhqr,[R] Deep and Hierarchical Implicit Models,https://www.reddit.com/r/MachineLearning/comments/5wuhqr/r_deep_and_hierarchical_implicit_models/,dustintran,1488359020,,13,32
12,2017-3-1,2017,3,1,19,5wuqnf,The Black Magic of Deep Learning - Tips and Tricks for the practitioner,https://www.reddit.com/r/MachineLearning/comments/5wuqnf/the_black_magic_of_deep_learning_tips_and_tricks/,mks_repi,1488362470,,0,1
13,2017-3-1,2017,3,1,20,5wv2cr,Hand-engineered features vs. CNNs - does SIFT still beat neural for certain tasks?,https://www.reddit.com/r/MachineLearning/comments/5wv2cr/handengineered_features_vs_cnns_does_sift_still/,Neural_Ned,1488366778,[removed],0,1
14,2017-3-1,2017,3,1,20,5wv6pb,Unplugg: An automated Forecasting API for timeseries data,https://www.reddit.com/r/MachineLearning/comments/5wv6pb/unplugg_an_automated_forecasting_api_for/,jnmamed,1488368355,,0,1
15,2017-3-1,2017,3,1,21,5wve7o,[1702.08892] Bridging the Gap Between Value and Policy Based Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/5wve7o/170208892_bridging_the_gap_between_value_and/,m000pan,1488370935,,0,3
16,2017-3-1,2017,3,1,21,5wvkpu,And the Oscar Goes to...Data Analytics?,https://www.reddit.com/r/MachineLearning/comments/5wvkpu/and_the_oscar_goes_todata_analytics/,ram_ilan,1488373021,,0,1
17,2017-3-2,2017,3,2,0,5wwkfs,"Duplicate question detection with a Siamese CNN. Results on Quora and StackExchange, with interactive demo",https://www.reddit.com/r/MachineLearning/comments/5wwkfs/duplicate_question_detection_with_a_siamese_cnn/,[deleted],1488382260,[deleted],0,1
18,2017-3-2,2017,3,2,0,5wwokn,"[R] Duplicate question detection with a Siamese CNN. Results on Quora and StackExchange, with interactive demo",https://www.reddit.com/r/MachineLearning/comments/5wwokn/r_duplicate_question_detection_with_a_siamese_cnn/,syllogism_,1488383206,,1,3
19,2017-3-2,2017,3,2,0,5wwq1d,"Simple Questions Thread March 01, 2017",https://www.reddit.com/r/MachineLearning/comments/5wwq1d/simple_questions_thread_march_01_2017/,AutoModerator,1488383555,[removed],0,1
20,2017-3-2,2017,3,2,1,5wwxyx,Bias-Variance Tradeoff in Machine Learning ( for beginners ),https://www.reddit.com/r/MachineLearning/comments/5wwxyx/biasvariance_tradeoff_in_machine_learning_for/,spmallick,1488385577,,0,1
21,2017-3-2,2017,3,2,1,5wwzpl,Bias-Variance Tradeoff in Machine Learning ( for beginners ),https://www.reddit.com/r/MachineLearning/comments/5wwzpl/biasvariance_tradeoff_in_machine_learning_for/,spmallick,1488386023,,0,1
22,2017-3-2,2017,3,2,1,5wx3f8,GPU performance,https://www.reddit.com/r/MachineLearning/comments/5wx3f8/gpu_performance/,amaldonado987,1488386974,[removed],0,1
23,2017-3-2,2017,3,2,2,5wxata,Connect two gpu's without SLI?,https://www.reddit.com/r/MachineLearning/comments/5wxata/connect_two_gpus_without_sli/,canttouchmypingas,1488388880,[removed],0,1
24,2017-3-2,2017,3,2,2,5wxh5a,MXNet - have anyone managed to make that thing even work?,https://www.reddit.com/r/MachineLearning/comments/5wxh5a/mxnet_have_anyone_managed_to_make_that_thing_even/,bbsome,1488390511,[removed],0,1
25,2017-3-2,2017,3,2,2,5wxhjf,some pytorch implementations and tutorials,https://www.reddit.com/r/MachineLearning/comments/5wxhjf/some_pytorch_implementations_and_tutorials/,[deleted],1488390624,[deleted],0,1
26,2017-3-2,2017,3,2,3,5wxnlh,How to do feature engineering of real time data?,https://www.reddit.com/r/MachineLearning/comments/5wxnlh/how_to_do_feature_engineering_of_real_time_data/,raghunanden,1488392171,[removed],0,1
27,2017-3-2,2017,3,2,3,5wxoez,[R] Bridging the Gap Between Value and Policy Based Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/5wxoez/r_bridging_the_gap_between_value_and_policy_based/,hardmaru,1488392391,,5,32
28,2017-3-2,2017,3,2,3,5wxoso,Machine Learning in Finance with Classification approach,https://www.reddit.com/r/MachineLearning/comments/5wxoso/machine_learning_in_finance_with_classification/,[deleted],1488392481,[deleted],0,1
29,2017-3-2,2017,3,2,3,5wxwvl,[D] Which unsupervised representation learning methods exists for neural networks?,https://www.reddit.com/r/MachineLearning/comments/5wxwvl/d_which_unsupervised_representation_learning/,themoosemind,1488394567,"Today, I was reminded of [Unsupervised Visual Representation Learning by Context Prediction](https://arxiv.org/abs/1505.05192v3) which trains a network on image data by letting it predict the position of a tile in relationship to another tile (see [image](http://ml-ka.de/wp-content/uploads/2017/02/pdg12-768x516.png)). I also know Autoencoders and GANs which can be used for learning useful representations.

Are there other methods?",2,2
30,2017-3-2,2017,3,2,5,5wyh8s,How to Use Tensorflow for Time Series (Live),https://www.reddit.com/r/MachineLearning/comments/5wyh8s/how_to_use_tensorflow_for_time_series_live/,god1111,1488399921,,0,0
31,2017-3-2,2017,3,2,5,5wyn40,How can we interpret this phenomenon?: what if both simple and complex models have the same low performance (e.g. f1 70%) on a complex problem (e.g. Semantic Role Labeling in NLP)?,https://www.reddit.com/r/MachineLearning/comments/5wyn40/how_can_we_interpret_this_phenomenon_what_if_both/,[deleted],1488401478,[removed],0,1
32,2017-3-2,2017,3,2,6,5wytxm,Best Intro Textbook on Neural Networks and Deep Learning,https://www.reddit.com/r/MachineLearning/comments/5wytxm/best_intro_textbook_on_neural_networks_and_deep/,blackface_killah,1488403335,[removed],0,1
33,2017-3-2,2017,3,2,6,5wyv8h,[D] A DARPA Perspective on Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/5wyv8h/d_a_darpa_perspective_on_artificial_intelligence/,sour_losers,1488403663,,51,260
34,2017-3-2,2017,3,2,8,5wzqon,[Research] Least Squares Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/5wzqon/research_least_squares_generative_adversarial/,anantzoid,1488412376,,6,3
35,2017-3-2,2017,3,2,9,5wzy14,"Line is getting serious about ML, but will it publish?",https://www.reddit.com/r/MachineLearning/comments/5wzy14/line_is_getting_serious_about_ml_but_will_it/,pashakun,1488414499,,1,1
36,2017-3-2,2017,3,2,9,5wzzfa,First-time Access to an Asset - Is it Risky or Not?: A Machine Learning Question ,https://www.reddit.com/r/MachineLearning/comments/5wzzfa/firsttime_access_to_an_asset_is_it_risky_or_not_a/,mni_moo,1488414896,,0,1
37,2017-3-2,2017,3,2,9,5x02ve,[D] How to change careers and become a data scientist - one quant's experience,https://www.reddit.com/r/MachineLearning/comments/5x02ve/d_how_to_change_careers_and_become_a_data/,math_rachel,1488416032,,3,7
38,2017-3-2,2017,3,2,9,5x02vq,Have thoughts on AI? Share them with me in this survey!!,https://www.reddit.com/r/MachineLearning/comments/5x02vq/have_thoughts_on_ai_share_them_with_me_in_this/,techluverz,1488416035,,0,1
39,2017-3-2,2017,3,2,10,5x0a3j,t-SNE implementation in pytorch,https://www.reddit.com/r/MachineLearning/comments/5x0a3j/tsne_implementation_in_pytorch/,chrisemoody,1488418269,,0,1
40,2017-3-2,2017,3,2,12,5x0yp2,[D] Doubt: Equations between Hidden Layers for a deep BLSTM.,https://www.reddit.com/r/MachineLearning/comments/5x0yp2/d_doubt_equations_between_hidden_layers_for_a/,a933,1488426278,"https://www.cs.toronto.edu/~graves/asru_2013.pdf

Hello, I was reading the above paper and the deep BLSTM architecture in figures 4 seems to have connections for layer n from both the directional layers from layer n-1. How can I generalize the equation given in page 2 for the BLSTM hidden layer connections?

Also, what's the intuition behind feeding both the activations from the forward context and backward context layers to the next layer?",1,0
41,2017-3-2,2017,3,2,15,5x1l8e,[D] What is the state of the art in paraphrase generation?,https://www.reddit.com/r/MachineLearning/comments/5x1l8e/d_what_is_the_state_of_the_art_in_paraphrase/,paraphrasseer,1488434546,"not abstractive summarization, rather paraphrase generation, googling doesn't find much. TIA!!!",4,7
42,2017-3-2,2017,3,2,17,5x2119,[D] ConvNets for Multi-Label Classification aside from Image Labeling,https://www.reddit.com/r/MachineLearning/comments/5x2119/d_convnets_for_multilabel_classification_aside/,[deleted],1488441660,[deleted],1,0
43,2017-3-2,2017,3,2,17,5x25wr,[R] OptNet: Differentiable Optimization as a Layer in Neural Networks,https://www.reddit.com/r/MachineLearning/comments/5x25wr/r_optnet_differentiable_optimization_as_a_layer/,wei_jok,1488444252,,20,42
44,2017-3-2,2017,3,2,19,5x2kcv,Really good book and website to start learning neural networks and deep learning.,https://www.reddit.com/r/MachineLearning/comments/5x2kcv/really_good_book_and_website_to_start_learning/,darwin345,1488451656,,0,1
45,2017-3-2,2017,3,2,20,5x2odm,Deep Forest: Towards An Alternative to Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/5x2odm/deep_forest_towards_an_alternative_to_deep_neural/,elyase,1488453657,,0,3
46,2017-3-2,2017,3,2,21,5x2ytv,Deep Forest: Towards an Alternative to Deep Neural Networks [X-post HackerNews],https://www.reddit.com/r/MachineLearning/comments/5x2ytv/deep_forest_towards_an_alternative_to_deep_neural/,Neural_Ned,1488458380,,1,2
47,2017-3-2,2017,3,2,22,5x32xl,[Webinar]: 'How to Win Machine Learning Competitions?' by Marios Michailidis (Former Kaggle #1),https://www.reddit.com/r/MachineLearning/comments/5x32xl/webinar_how_to_win_machine_learning_competitions/,NarendhiranS,1488459928,,0,1
48,2017-3-2,2017,3,2,22,5x330s,"[D] MILA lab application for Phd in Fall, any response yet?",https://www.reddit.com/r/MachineLearning/comments/5x330s/d_mila_lab_application_for_phd_in_fall_any/,mila_lab_applicant,1488459962,[removed],1,2
49,2017-3-2,2017,3,2,22,5x369l,Singular-Vision blog by BJ Dooley-- Deconstructing Machine Learning: Three Easy Pieces,https://www.reddit.com/r/MachineLearning/comments/5x369l/singularvision_blog_by_bj_dooley_deconstructing/,bjdooley,1488461127,,0,1
50,2017-3-2,2017,3,2,23,5x3fi5,What do you call it when the error spikes during training?,https://www.reddit.com/r/MachineLearning/comments/5x3fi5/what_do_you_call_it_when_the_error_spikes_during/,jostmey,1488464263,[removed],0,1
51,2017-3-3,2017,3,3,0,5x3pym,How many CPU cores do you need for Deep Learning in GPU?,https://www.reddit.com/r/MachineLearning/comments/5x3pym/how_many_cpu_cores_do_you_need_for_deep_learning/,inlineint,1488467475,[removed],0,1
52,2017-3-3,2017,3,3,0,5x3vz9,Face Recognition meets Big Data and Deep Learning,https://www.reddit.com/r/MachineLearning/comments/5x3vz9/face_recognition_meets_big_data_and_deep_learning/,bocse,1488469168,,1,1
53,2017-3-3,2017,3,3,1,5x4a52,[R] Deep Forest: Towards An Alternative to Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/5x4a52/r_deep_forest_towards_an_alternative_to_deep/,downtownslim,1488473020,,36,181
54,2017-3-3,2017,3,3,1,5x4cil,Vector Database?,https://www.reddit.com/r/MachineLearning/comments/5x4cil/vector_database/,taiboku,1488473638,[removed],0,1
55,2017-3-3,2017,3,3,1,5x4cnj,[P] TensorFlow: How to optimise your input pipeline with queues and multi-threading,https://www.reddit.com/r/MachineLearning/comments/5x4cnj/p_tensorflow_how_to_optimise_your_input_pipeline/,morgangiraud,1488473671,,16,28
56,2017-3-3,2017,3,3,2,5x4h2l,[D - /r/cscareerquestions x-post] Advice needed - eight months out and feeling pretty defeated,https://www.reddit.com/r/MachineLearning/comments/5x4h2l/d_rcscareerquestions_xpost_advice_needed_eight/,baiduthrowaway,1488474777,[removed],0,1
57,2017-3-3,2017,3,3,2,5x4jbt,"[D] Strided convolutions vs. pooling layers, pros and cons of the respective downsampling methods?",https://www.reddit.com/r/MachineLearning/comments/5x4jbt/d_strided_convolutions_vs_pooling_layers_pros_and/,carlthome,1488475381,"When downsampling, what are the respective pros and cons of doing strided convolutions instead of pooling?

I feel average pooling could be best at preserving spatial structure, but perhaps feature maps would just learn to handle discontinuities from striding just as well? And I guess compared to max pooling, strides would work just as well and be cheaper (faster convolution layers), but a variant I see mentioned sometimes is that people sum both average pooling and max pooling, which doesn't seem easily covered by striding. When would you choose which downsampling technique?

Also, is there a pooling analog for transposed strided convolutions (upsampling)?",13,21
58,2017-3-3,2017,3,3,3,5x4sa6,[D] Cross-validation of cross-validated stacking ensemble,https://www.reddit.com/r/MachineLearning/comments/5x4sa6/d_crossvalidation_of_crossvalidated_stacking/,Reiinakano,1488477755,"Hi everyone, let me begin by saying that I understand how to build a stacked ensemble by using cross-validation to generate out-of-fold predictions for the base learners to generate meta-features. My question is about the methodology when cross-validating the entire stacked ensemble to check generalization error.

To eliminate any confusion, I'm going to call the cross-validation to generate out of fold predictions for the base learner CV A, while I'll call the cross-validation of the entire stacking ensemble CV B.

When I do CV B, is it valid to do CV A *just once* and use those out of fold predictions for the **entire** CV B process? Or do I have to keep doing CV A and generate new out of fold predictions during each fold of CV B?

Normally, I'd think that there'd be some data leakage in the first method, but one could also reason out that since the out of fold predictions are taken, well, out of fold, that issue is taken care of. The main reason I'm asking this is because doing the second method would surely remove any data leakage but there would be an order of magnitude of additional computational complexity involved.",1,2
59,2017-3-3,2017,3,3,4,5x5bt6,AMA We're developing new deep learning hardware that computes using laser light,https://www.reddit.com/r/MachineLearning/comments/5x5bt6/ama_were_developing_new_deep_learning_hardware/,lightmatter_mit,1488482713,"Hi Reddit,

We are lightmatter: a group of MIT students developing a new photonic-based deep learning hardware. This method of computation can be faster than current GPUs. We recently published a preprint where we carried out vowel recognition tasks: https://arxiv.org/abs/1610.02365.

Our architecture promises more than 10x speed improvements in inference tasks. We are here to answer your questions. Feel free to suggest any application you think this may be useful for!",14,29
60,2017-3-3,2017,3,3,4,5x5j7x,Meta-learning gains momentum in ML,https://www.reddit.com/r/MachineLearning/comments/5x5j7x/metalearning_gains_momentum_in_ml/,kordikp,1488484684,,0,1
61,2017-3-3,2017,3,3,6,5x5ygp,Advice on Neural Machine Translation project.,https://www.reddit.com/r/MachineLearning/comments/5x5ygp/advice_on_neural_machine_translation_project/,quod_erat_faciendum,1488488719,[removed],0,1
62,2017-3-3,2017,3,3,6,5x61sa,Which libraries to learn to get started with ML in Python (Matlab background)?,https://www.reddit.com/r/MachineLearning/comments/5x61sa/which_libraries_to_learn_to_get_started_with_ml/,[deleted],1488489575,[removed],0,1
63,2017-3-3,2017,3,3,6,5x6631,[R] Training Deep Spiking Neural Networks using Backpropagation,https://www.reddit.com/r/MachineLearning/comments/5x6631/r_training_deep_spiking_neural_networks_using/,cbeak,1488490732,,1,8
64,2017-3-3,2017,3,3,7,5x6bb2,[D] Batch Normalization and Dropout uses in GAN different from common practice?,https://www.reddit.com/r/MachineLearning/comments/5x6bb2/d_batch_normalization_and_dropout_uses_in_gan/,mimighost,1488492181,"Recently following my study about GAN implementation, I discovered that BN/Dropout are used differently in GAN. Specifically, people seem to decide not to distinguish train/test phases as to the behaviors of those operators in GAN:

1.For BN, running statistics are not applied during testing

2.For Dropout, it still applies during test

Can anyone explain to me the motivation behind this choice? I happened to experiment using running statistics for BN during testing, it seems make little difference here. And for BN, if running statistics are not used at all, why not using instance normalization instead?

Edit: per request, below is some implementations I saw:

https://github.com/carpedm20/DCGAN-tensorflow/blob/master/ops.py#L29

https://github.com/phillipi/pix2pix/blob/master/models.lua",4,3
65,2017-3-3,2017,3,3,8,5x6put,Why use convolutional NNs for a visual inspection task over classic CV template matching?,https://www.reddit.com/r/MachineLearning/comments/5x6put/why_use_convolutional_nns_for_a_visual_inspection/,[deleted],1488496264,[removed],0,1
66,2017-3-3,2017,3,3,9,5x778t,[D] Dimensionality of Inverse Autoregressive Flows,https://www.reddit.com/r/MachineLearning/comments/5x778t/d_dimensionality_of_inverse_autoregressive_flows/,IAF_Qs,1488501590,"I am reading through the work from Open AI on inverse autoregressive flows and want to understand the dimensionality of the latent variables.

Fundamentally, it seems like an advantage that VAEs can compress data like MNIST/CIFAR in to fewer dimensions (like at this [demo](http://www.dpkingma.com/sgvb_mnist_demo/demo.html)). However when looking at the paper and the code for inverse autoregressive flows, it seems for something like CIFAR, the latent dimensionality is (height, width, n_z) where n_z is number of latents.

It seems to me that the n_z channel is a result of a convolutional operation (see [code](https://github.com/openai/iaf/blob/master/tf_train.py#L183)) and is not simply the same n_z numbers tiled across the dimension. But then this would seem to suggest that there are n_z * width * height non-identical parameters summarizing the data. If you check the size of the q_z means and std. devs when running the [code](https://github.com/openai/iaf/blob/master/tf_train.py#L38) they also have dimensionality of (height, width, n_z).

In the paper, they say for MNIST, ""a single layer of Gaussian stochastic units of dimension 32 is used"".  However the above seems to suggest that this results in a ""latent embedding"" of MNIST that is of size 28 * 28 * 32. This seems like an awful lot of latent variables to parameterize hand-written digits especially when 10 variables can capture so many axes of variation (in that demo I linked).

So is the latent dimensionality of IAF with MNIST really include &gt;25k parameters and the whole model is just trying to squeeze itself into something resembling the standard normal prior while being seemingly quite overparameterized per pixel, or am I really fundamentally misunderstanding the code?

If this is a misunderstanding, how would one extract the 32-dimensional embedding of a MNIST digit from the publicly available code? Thanks and sorry if this is a dumb misunderstanding.",2,4
67,2017-3-3,2017,3,3,10,5x7ebj,[D] Non RNN-based sequence to sequence models,https://www.reddit.com/r/MachineLearning/comments/5x7ebj/d_non_rnnbased_sequence_to_sequence_models/,karlthefog,1488503821,"I've recently been faced with modelling a sequence of data with labels, where there is a clear interdependence between neighbouring input data. Given how pervasive RNNs/LSTMs have become over the last couple of years it makes searching for alternatives hard. My dataset is relatively small and I am concerned about overfitting. What are some alternatives?

example training data:
[x0, x1, x2, x3, x4, ...., xn] -&gt; [y0,y0,y0,y1,y1,....,yk]
 i.e. the data tends to 'hang out' in labelled zones for a while and move on

",6,6
68,2017-3-3,2017,3,3,10,5x7fbl,Easy image classification fine-tuning in keras,https://www.reddit.com/r/MachineLearning/comments/5x7fbl/easy_image_classification_finetuning_in_keras/,mecken,1488504149,,0,1
69,2017-3-3,2017,3,3,11,5x7t8h,[D] Combinatorics/TCS in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/5x7t8h/d_combinatoricstcs_in_machine_learning/,yottapirx,1488508763,"I am currently a third-year undergrad majoring in computer science and math. I'm considering doing an independent reading class with a professor in TCS/combinatorics. I really like discrete math and CS theory, but I'm also interested in machine learning. I was wondering what are some interesting topics in combinatorics/discrete math/algorithms that are related to machine learning, or at least knowing them would be somehow beneficial.

I'm imagining it would be mostly stuff like probabilistic/linear algebra methods. For example the proof of VC inequality seems to be basically using a probabilistic method for a counting problem. I've also seen the MIT course [Topics in Mathematics of Data Science](https://ocw.mit.edu/courses/mathematics/18-s096-topics-in-mathematics-of-data-science-fall-2015/) that seems to be somewhat related to what I have in mind. Does anyone have other suggestions? 

I apologize if this is a wrong subreddit, but I thought this is a better place to ask than the more beginner subreddits, since this question is more theory-oriented. ",1,3
70,2017-3-3,2017,3,3,11,5x7tgz,I made a list of some interesting datasets I have seen recently.,https://www.reddit.com/r/MachineLearning/comments/5x7tgz/i_made_a_list_of_some_interesting_datasets_i_have/,muktabh,1488508837,,0,3
71,2017-3-3,2017,3,3,11,5x7ulb,Julia implementations of some of the foundational Machine Learning models and algorithms from scratch.,https://www.reddit.com/r/MachineLearning/comments/5x7ulb/julia_implementations_of_some_of_the_foundational/,memoiry_,1488509199,[removed],1,1
72,2017-3-3,2017,3,3,12,5x84mr,[Discussion] Teaser code for new kind of RNN model to be announced Monday!,https://www.reddit.com/r/MachineLearning/comments/5x84mr/discussion_teaser_code_for_new_kind_of_rnn_model/,[deleted],1488512530,[deleted],0,1
73,2017-3-3,2017,3,3,12,5x851o,[D] Teaser code for new kind of RNN model to be announced Monday!,https://www.reddit.com/r/MachineLearning/comments/5x851o/d_teaser_code_for_new_kind_of_rnn_model_to_be/,jostmey,1488512673,,30,1
74,2017-3-3,2017,3,3,13,5x883x,Using Step Based Information Processing for Deep Learning Artificial General Intelligence,https://www.reddit.com/r/MachineLearning/comments/5x883x/using_step_based_information_processing_for_deep/,JonoExplainsThings,1488513718,,0,1
75,2017-3-3,2017,3,3,14,5x8pr8,Data Organization in Unsupervised Learning,https://www.reddit.com/r/MachineLearning/comments/5x8pr8/data_organization_in_unsupervised_learning/,virene,1488520338,,0,1
76,2017-3-3,2017,3,3,15,5x8vql,[D] Noise adaptation layer for noisy labels,https://www.reddit.com/r/MachineLearning/comments/5x8vql/d_noise_adaptation_layer_for_noisy_labels/,Pieranha,1488522867,"I'm training on a large dataset of noisy labels and reading through the latest papers on that front with the most recent one I can find being this one:

https://openreview.net/pdf?id=H12GRgcxg

They basically use an additional softmax on top of the existing softmax during training, which is discarded during testing. 

Based on my reading of the paper, this approach only works based on the assumption that the test set labels do not have noise, which is not the case for my data. When working with a noisy test set, would this approach help at all? Perhaps in terms of reduced training time?",0,2
77,2017-3-3,2017,3,3,16,5x90c9,Your Guide To Unlocking Top Data Scientist Jobs,https://www.reddit.com/r/MachineLearning/comments/5x90c9/your_guide_to_unlocking_top_data_scientist_jobs/,Edureka-learning,1488524910,,0,1
78,2017-3-3,2017,3,3,17,5x9b5w,What is concrete batching machine?,https://www.reddit.com/r/MachineLearning/comments/5x9b5w/what_is_concrete_batching_machine/,[deleted],1488530501,[deleted],0,1
79,2017-3-3,2017,3,3,17,5x9cqp,[R] Revisiting NARX Recurrent Neural Networks for Long-Term Dependencies (code coming soon),https://www.reddit.com/r/MachineLearning/comments/5x9cqp/r_revisiting_narx_recurrent_neural_networks_for/,r_dipietro,1488531377,,2,9
80,2017-3-3,2017,3,3,17,5x9d2r,What is concrete batching machine? http://www.flyerconcretemachine.com/newsview.asp?id=63,https://www.reddit.com/r/MachineLearning/comments/5x9d2r/what_is_concrete_batching_machine/,FlyerConcretePlant,1488531569,,0,1
81,2017-3-3,2017,3,3,18,5x9hv3,[R] Attentive Recurrent Comparators - really good one shot classification performance on Omniglot,https://www.reddit.com/r/MachineLearning/comments/5x9hv3/r_attentive_recurrent_comparators_really_good_one/,alekhka,1488534117,,8,24
82,2017-3-3,2017,3,3,19,5x9k20,[D] Why Machine Learning community have difficulties to understand Numenta's approach on Machine Intelligence?,https://www.reddit.com/r/MachineLearning/comments/5x9k20/d_why_machine_learning_community_have/,[deleted],1488535240,[removed],17,0
83,2017-3-3,2017,3,3,19,5x9nzm,[1703.00848] Unsupervised Image-to-Image Translation Networks,https://www.reddit.com/r/MachineLearning/comments/5x9nzm/170300848_unsupervised_imagetoimage_translation/,Bardelaz,1488537250,,0,2
84,2017-3-3,2017,3,3,19,5x9oem,Deep Learning Platforms &amp; GPUs: an Interview With Bryan Catanzaro,https://www.reddit.com/r/MachineLearning/comments/5x9oem/deep_learning_platforms_gpus_an_interview_with/,reworksophie,1488537470,,0,1
85,2017-3-3,2017,3,3,19,5x9oft,A New Era of Driverless Transport is Upon Us,https://www.reddit.com/r/MachineLearning/comments/5x9oft/a_new_era_of_driverless_transport_is_upon_us/,reworksophie,1488537492,,0,1
86,2017-3-3,2017,3,3,20,5x9tyi,[R] Which libraries to learn to get started with ML in Python (10+yrs Matlab background)?,https://www.reddit.com/r/MachineLearning/comments/5x9tyi/r_which_libraries_to_learn_to_get_started_with_ml/,yes_we_cant,1488540225,"As in the subject guys, 
I'm a data scientist (finance) with 10+ years experience using Matlab. We do some simple ML (CART, random forests), and tons of linear algebra, ordinary least squares, time series modelling, etc. 

I'm quite familiar with the mathematical theory behind neural nets, SVMs, word embeddings, CART and random forests. These are the techniques I'd like to apply. I studied quant methods and CS at university so on the theory front I think I should be ok. 

I want to transition to Python. 

(1) should I install something on my laptop, or am I ok to stick with online based GUIs (my preferred option, so that when I travel to visit family etc I can always jump back in on say my mom's computer for a half hour session etc)? In such case, which environment to use - when I google ""run Python online"" or similar many links appear. 
I'd like to be able to, ultimately, run TensorFlow libraries, or, for example, run the examples from excellent book http://neuralnetworksanddeeplearning.com/ , including cloning the guy's github code into my environment; or import some standard dataset to play with (handwritten digits, or some dataset for NLP training, etc).
 
(2) Is Jupyter (which I played with a bit) the GUI I'm looking for? Or is it something else than a GUI?

(3) is the following sequence of learning ""correct""?
Learn ML theory (DONE) -&gt; basic Python -&gt; Numpy -&gt; Panda -&gt; TensorFlow
?
",43,66
87,2017-3-3,2017,3,3,20,5x9uqt,[R] Is there a comparative review about NuPIC vs. other Machine Learning frameworks [1] ? [1] https://github.com/showcases/machine-learning,https://www.reddit.com/r/MachineLearning/comments/5x9uqt/r_is_there_a_comparative_review_about_nupic_vs/,htm_fun_boy,1488540615,[removed],0,0
88,2017-3-3,2017,3,3,23,5xaobf,[P] Faiss is a library for efficient similarity search and clustering of dense vectors,https://www.reddit.com/r/MachineLearning/comments/5xaobf/p_faiss_is_a_library_for_efficient_similarity/,julian88888888,1488552028,,4,73
89,2017-3-3,2017,3,3,23,5xaq6q,Books on training and evaluating deep learning networks?,https://www.reddit.com/r/MachineLearning/comments/5xaq6q/books_on_training_and_evaluating_deep_learning/,testingTestingIBS,1488552633,[removed],0,1
90,2017-3-3,2017,3,3,23,5xaros,What does actually happen in Keras when I try to train a neural network with missing values in the input data?,https://www.reddit.com/r/MachineLearning/comments/5xaros/what_does_actually_happen_in_keras_when_i_try_to/,MightyMustard,1488553106,[removed],0,1
91,2017-3-4,2017,3,4,0,5xazjc,Best way to deal with huge number of features in Deep Learning,https://www.reddit.com/r/MachineLearning/comments/5xazjc/best_way_to_deal_with_huge_number_of_features_in/,[deleted],1488555395,[removed],0,1
92,2017-3-4,2017,3,4,0,5xazjv,[1703.00381v1] The Statistical Recurrent Unit,https://www.reddit.com/r/MachineLearning/comments/5xazjv/170300381v1_the_statistical_recurrent_unit/,Mandrathax,1488555401,,0,1
93,2017-3-4,2017,3,4,0,5xb0wt,Generalization and Equilibrium in Generative Adversarial Nets (GANs),https://www.reddit.com/r/MachineLearning/comments/5xb0wt/generalization_and_equilibrium_in_generative/,sshekh,1488555795,,0,1
94,2017-3-4,2017,3,4,0,5xb23w,[D] Best way to deal with huge number of features in Deep Learning,https://www.reddit.com/r/MachineLearning/comments/5xb23w/d_best_way_to_deal_with_huge_number_of_features/,YoItsSinged,1488556129,"Hi, I'm currently trying to build a Deep neural net that can classify samples with tumors and healthy ones. The problem is that I have more than 30000 gene expression values, which it made sense in my head to be used as features. However I'm not sure if the network will learn anything because the number of features is so high. I have 370 to 30 samples of each category, that is also a problem but I'll try to deal with one at a time :P
I'm a newbie in the field as I just started. Would appreciate some advice from you guys.

EDIT: In case anyone is facing the same problem here are the results of my first approach:
Removed genes that only contained 0's (non-expressed ones), then transformed to log2(x+1) and normalized. Then reduced the features by applying KPCA and balanced the dataset using SMOTE + TOMEK. After all this preprocessing I fed the data into an MLP and that's it! Got 0.99 accuracy and 0.97 f1 score",16,1
95,2017-3-4,2017,3,4,0,5xb367,[R][1703.00522] Understanding Synthetic Gradients and Decoupled Neural Interfaces. [DeepMind],https://www.reddit.com/r/MachineLearning/comments/5xb367/r170300522_understanding_synthetic_gradients_and/,rilut,1488556418,,7,40
96,2017-3-4,2017,3,4,1,5xb8eg,Classification question pix2pix,https://www.reddit.com/r/MachineLearning/comments/5xb8eg/classification_question_pix2pix/,datatelic,1488557825,[removed],0,1
97,2017-3-4,2017,3,4,1,5xbcn9,Preparing for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/5xbcn9/preparing_for_machine_learning/,BeautifulBrownie,1488559031,[removed],0,1
98,2017-3-4,2017,3,4,2,5xbl8k,Help: Bag of concepts,https://www.reddit.com/r/MachineLearning/comments/5xbl8k/help_bag_of_concepts/,[deleted],1488561327,[removed],0,1
99,2017-3-4,2017,3,4,2,5xbsx0,[D] How deep should I understand topics before confidently using a function,https://www.reddit.com/r/MachineLearning/comments/5xbsx0/d_how_deep_should_i_understand_topics_before/,mucle6,1488563407,"Right now I'm trying to learn about LSTM networks. I understand the big concepts, like that they transfer memory. I understand what  LSTM offers over an RNN. I know steps exist to remove, create, and update memory. The thing is that I have no clue how to actually implement one. 

I don't know what the memory is stored as, I don't know how to read the diagrams that show how they work. In [this](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) link the main diagram has 4 neural network layers and I don't understand how you would train them. I've coded a neural network before but I don't know the logistics of using 4 networks to work as a team. 

I want to work on different machine learning projects with TensorFlow but I want to be able to debug whatever network I make. 

**Do I not know enough or is it common to just understand the big picture and move on?** ",3,5
100,2017-3-4,2017,3,4,4,5xc90p,On the issue of non-smooth prediction probabilities when training a binary classifier,https://www.reddit.com/r/MachineLearning/comments/5xc90p/on_the_issue_of_nonsmooth_prediction/,testingTestingIBS,1488567616,[removed],0,1
101,2017-3-4,2017,3,4,4,5xc9b7,Patents and machine learning,https://www.reddit.com/r/MachineLearning/comments/5xc9b7/patents_and_machine_learning/,barnyardman,1488567686,[removed],0,1
102,2017-3-4,2017,3,4,4,5xcan1,Is there a paper on the distributed gradient boosted tree implementation in spark that you can point to?,https://www.reddit.com/r/MachineLearning/comments/5xcan1/is_there_a_paper_on_the_distributed_gradient/,skfit,1488568029,[removed],0,1
103,2017-3-4,2017,3,4,4,5xcdc5,"Classic Machine-Learning Algorithms, SciKit-Learn or Tensorflow's TFLearn",https://www.reddit.com/r/MachineLearning/comments/5xcdc5/classic_machinelearning_algorithms_scikitlearn_or/,[deleted],1488568771,[removed],0,1
104,2017-3-4,2017,3,4,4,5xcfej,"A NumPy cheat sheet for beginners, I hope it helps you!",https://www.reddit.com/r/MachineLearning/comments/5xcfej/a_numpy_cheat_sheet_for_beginners_i_hope_it_helps/,julianxxxx,1488569309,,0,1
105,2017-3-4,2017,3,4,5,5xcmps,Convolutional Neural Networks for Image Classification,https://www.reddit.com/r/MachineLearning/comments/5xcmps/convolutional_neural_networks_for_image/,vikramasia,1488571294,,0,1
106,2017-3-4,2017,3,4,5,5xcqsj,word2vec four years later,https://www.reddit.com/r/MachineLearning/comments/5xcqsj/word2vec_four_years_later/,solololol,1488572396,[removed],0,1
107,2017-3-4,2017,3,4,5,5xcu22,[R] Generalization and Equilibrium in Generative Adversarial Nets,https://www.reddit.com/r/MachineLearning/comments/5xcu22/r_generalization_and_equilibrium_in_generative/,GGMU1,1488573306,,2,12
108,2017-3-4,2017,3,4,6,5xd1ux,[D] Where are the ILSVRC2016 technical reports,https://www.reddit.com/r/MachineLearning/comments/5xd1ux/d_where_are_the_ilsvrc2016_technical_reports/,insider_7,1488575494,"The winners of the ILSVRC2016 used ensemble learning to win the competition. Although they did not contributed to the research community with new architectures, they implemented very robust models. The question is: where are the technical reports of the experiments?",0,4
109,2017-3-4,2017,3,4,6,5xd4yg,"[P] HyperGAN 0.8 released. TF 1.0, API, new losses, examples, and more",https://www.reddit.com/r/MachineLearning/comments/5xd4yg/p_hypergan_08_released_tf_10_api_new_losses/,what_are_tensors,1488576354,,18,60
110,2017-3-4,2017,3,4,6,5xdbmc,Using Fisher information in sequential experiment design,https://www.reddit.com/r/MachineLearning/comments/5xdbmc/using_fisher_information_in_sequential_experiment/,nothingtoseeherelol,1488578221,[removed],0,1
111,2017-3-4,2017,3,4,7,5xdkpe,Cool GSoC 2017 Data Science related projects (check out PyMC3),https://www.reddit.com/r/MachineLearning/comments/5xdkpe/cool_gsoc_2017_data_science_related_projects/,vladislavsd,1488580848,,0,2
112,2017-3-4,2017,3,4,7,5xdl7p,What are the best ML and ANN websites to follow? Suggestions?,https://www.reddit.com/r/MachineLearning/comments/5xdl7p/what_are_the_best_ml_and_ann_websites_to_follow/,julianxxxx,1488580997,[removed],0,1
113,2017-3-4,2017,3,4,7,5xdliz,"Go Home Discriminator, Your Drunk / Fine Tuning with Discriminator Networks [Project]",https://www.reddit.com/r/MachineLearning/comments/5xdliz/go_home_discriminator_your_drunk_fine_tuning_with/,yoyosarian,1488581092,,1,1
114,2017-3-4,2017,3,4,7,5xdmte,Learning Deep Nearest Neighbor Representations Using Differentiable Boundary Trees,https://www.reddit.com/r/MachineLearning/comments/5xdmte/learning_deep_nearest_neighbor_representations/,rephos,1488581474,,0,1
115,2017-3-4,2017,3,4,8,5xdqf8,Implementing batching in tensorflow question,https://www.reddit.com/r/MachineLearning/comments/5xdqf8/implementing_batching_in_tensorflow_question/,Wootbears,1488582548,[removed],0,1
116,2017-3-4,2017,3,4,8,5xdtda,[P] Super Enhancing 8x8 Images with Perceptual Loss and Adversarial Nets,https://www.reddit.com/r/MachineLearning/comments/5xdtda/p_super_enhancing_8x8_images_with_perceptual_loss/,Staturecrane,1488583487,,12,118
117,2017-3-4,2017,3,4,8,5xduub,[R] How to Escape Saddle Points Efficiently,https://www.reddit.com/r/MachineLearning/comments/5xduub/r_how_to_escape_saddle_points_efficiently/,perceptron01,1488583973,,6,10
118,2017-3-4,2017,3,4,9,5xe3fi,"Why aren't there any external memory network results reported for Stanford's Natural Language Inference dataset, given they show SOTA on other inference datasets (like those in bAbl for example)?",https://www.reddit.com/r/MachineLearning/comments/5xe3fi/why_arent_there_any_external_memory_network/,[deleted],1488586824,[removed],0,1
119,2017-3-4,2017,3,4,9,5xe4lb,DeepStack: Expert-level artificial intelligence in heads-up no-limit poker,https://www.reddit.com/r/MachineLearning/comments/5xe4lb/deepstack_expertlevel_artificial_intelligence_in/,Fa1l3r,1488587240,,0,1
120,2017-3-4,2017,3,4,9,5xe6ig,[D] Real scope of top tier ML conferences and Journals,https://www.reddit.com/r/MachineLearning/comments/5xe6ig/d_real_scope_of_top_tier_ml_conferences_and/,insider_7,1488587934,"It is well known that the top tier ML conferences and Journals sate that they accept ""all topics"" and applications of Machine Learning, while in reality they only tend to accept a certain subset of all the application pool.
Can someone with a wide experience on applying to these conferences/journals share their knowledge on what type of papers tend to get through the following:
Conferences: NIPS, CVPR, ICML
Journals: Pattern analysis and Machine intelligence, The Journal of Machine Learning Research, Machine learning (Journal)",3,5
121,2017-3-4,2017,3,4,9,5xe6rq,[D] Udacity Nature paper on ML,https://www.reddit.com/r/MachineLearning/comments/5xe6rq/d_udacity_nature_paper_on_ml/,insider_7,1488588020,"According to Sebastian Thrun Udacity is preparing a new Nature paper on ML: https://www.youtube.com/watch?v=mcKeMTNl9hQ
Any info on it?",6,16
122,2017-3-4,2017,3,4,10,5xegzw,How to Generate Art - Intro to Deep Learning,https://www.reddit.com/r/MachineLearning/comments/5xegzw/how_to_generate_art_intro_to_deep_learning/,ackstazya,1488591633,,0,1
123,2017-3-4,2017,3,4,16,5xfsl6,[N] General AI Challenge - based on ideas by FAIR,https://www.reddit.com/r/MachineLearning/comments/5xfsl6/n_general_ai_challenge_based_on_ideas_by_fair/,sorrge,1488611591,,3,49
124,2017-3-4,2017,3,4,16,5xfu99,Effects of DDR bandwidth and PCI-e lane for deep learning,https://www.reddit.com/r/MachineLearning/comments/5xfu99/effects_of_ddr_bandwidth_and_pcie_lane_for_deep/,agro1986,1488612485,[removed],0,1
125,2017-3-4,2017,3,4,17,5xfz0u,[D] Building embedding models from numerical features,https://www.reddit.com/r/MachineLearning/comments/5xfz0u/d_building_embedding_models_from_numerical/,FutureIsMine,1488615127,"Im wondering if its possible to build embeddings off of numerical features, in my case I've got company ids and with them I've also got how big those companies are, where they're located, the industry, along with founded date, and revenue. I'm wondering how I could take all that information, especially numeric, and use that to build embedding vectors to represent the companies, something akin to a word2vec. Only thing is that unlike word2vec I've got numerical features, and I don't think that turning something like a gps coordinate alone into an id is good enough, as I'd like the model to also determine where the gps coordinates really are different enough. ",10,3
126,2017-3-4,2017,3,4,18,5xg9gi,[R] Assisting Pathologists in Detecting Cancer with Deep Learning,https://www.reddit.com/r/MachineLearning/comments/5xg9gi/r_assisting_pathologists_in_detecting_cancer_with/,pmigdal,1488621553,,0,55
127,2017-3-4,2017,3,4,19,5xgekj,"[R] Evolving Deep Neural Networks, Risto Miikkulainen et al.",https://www.reddit.com/r/MachineLearning/comments/5xgekj/r_evolving_deep_neural_networks_risto/,hardmaru,1488624739,,7,20
128,2017-3-4,2017,3,4,22,5xgvin,Temperature control with machine learning,https://www.reddit.com/r/MachineLearning/comments/5xgvin/temperature_control_with_machine_learning/,anonymoushippo63,1488633915,[removed],0,1
129,2017-3-5,2017,3,5,1,5xhmwv,Large data sets for training  jcjohnson/torch-rnn,https://www.reddit.com/r/MachineLearning/comments/5xhmwv/large_data_sets_for_training_jcjohnsontorchrnn/,[deleted],1488644586,[deleted],0,1
130,2017-3-5,2017,3,5,1,5xhrft,[D] Large data sets for training and select of best options  Issue #166  jcjohnson/torch-rnn,https://www.reddit.com/r/MachineLearning/comments/5xhrft/d_large_data_sets_for_training_and_select_of_best/,sa1ppuakaupp1as,1488646036,,4,0
131,2017-3-5,2017,3,5,2,5xhw43,"[R][1702.02144] Cheap parametric density estimation with a linear combination, like a polynomial or Fourier series",https://www.reddit.com/r/MachineLearning/comments/5xhw43/r170202144_cheap_parametric_density_estimation/,jarekduda,1488647526,,7,24
132,2017-3-5,2017,3,5,3,5xi7z3,"[D] 3rd International Conference on Machine learning, Optimization &amp; big Data",https://www.reddit.com/r/MachineLearning/comments/5xi7z3/d_3rd_international_conference_on_machine/,d9w,1488651151,"Has anyone been to a past [MOD](http://www.taosciences.it/mod/) and can report on conference quality? It's a bit too new for me to be able to find much about it. [Last year's site](http://www.taosciences.it/mod-2016/program/) looks okay, not like NIPS but some interesting talks.",8,0
133,2017-3-5,2017,3,5,3,5xiao0,[R] RNN Decoding of Linear Block Codes,https://www.reddit.com/r/MachineLearning/comments/5xiao0/r_rnn_decoding_of_linear_block_codes/,enk100,1488651968,,6,1
134,2017-3-5,2017,3,5,3,5xicju,Batch Adversarial Networks: are adversarial nets missing something fairly obvious?,https://www.reddit.com/r/MachineLearning/comments/5xicju/batch_adversarial_networks_are_adversarial_nets/,[deleted],1488652580,[removed],1,1
135,2017-3-5,2017,3,5,4,5xijcd,[R][1703.00837] Meta Networks,https://www.reddit.com/r/MachineLearning/comments/5xijcd/r170300837_meta_networks/,tsendsuren,1488654653,,2,11
136,2017-3-5,2017,3,5,4,5xijnu,"[D] About ""the cake"", UL, and learning material about it",https://www.reddit.com/r/MachineLearning/comments/5xijnu/d_about_the_cake_ul_and_learning_material_about_it/,LecJackS,1488654752,"This is something that has been bothering me for a long time. Maybe this belongs to [Simple Questions](https://www.reddit.com/r/MachineLearning/search?q=simple+questions+thread+author:automoderator&amp;restrict_sr=on&amp;sort=new&amp;t=all), but I don't think it's so simple, so:

As Yann LeCun said a time ago, [if intelligence (or AI) is a Cake, Unsupervised Learning should be the biggest ""portion"" of it](http://i.imgur.com/1513qqB.png), so why there is not so much learning material about it?

I can find like ten big online courses about RL. Not to mention papers and amazing new things every day.
Supervised Learning is the same, but like ten or fifty times more.

So what about Unsupervised Learning? Why do I not see so much about it?

It's because this ""cake"" refers to an abstract idea of how it SHOULD be, but it's not that way now, so it needs time and more research?

Or maybe I'm biased and I can't reach that kind of information because it's too complex for my knowledge?",7,4
137,2017-3-5,2017,3,5,4,5xikop,[D] Batch Adversarial Networks: are adversarial nets missing something fairly obvious?,https://www.reddit.com/r/MachineLearning/comments/5xikop/d_batch_adversarial_networks_are_adversarial_nets/,darkmighty,1488655067,"I don't work in the field (although I'd like to...) but follow the ""news"" with interest. I've read a little on [GANs](https://en.wikipedia.org/wiki/Generative_adversarial_networks) (Generative adversarial networks) and they have a sort of obvious flaw to me: a bias towards the mode of a distribution. For example, say it is tasked with generating car colors, and let's say for example that 90% of cars are black while 10% are red. Then generating a red car (unless I'm misunderstanding it) would be penalized in the adversarial scheme.

Has this been addressed yet?

If not, I have a few solutions. If it is any good, what's the best way to get it ""out there"", so to speak? The solutions:

1) Simplest, least efficient. Instead of generating 1 sample, the generative network generates a batch of N samples simultaneously, and the adversarial discriminator evaluates this N sample batch. In the car case, we would expect to see, on average, 1 red car every 10, so that the generation is forced to learn the full statistics and not just sample near the mode (maximum likelihood);

2) More complicated, more efficient. The generator network works as usual but the discriminator network is given memory. This memory is used to keep track of the statistics of all the samples generated so far (perhaps with priority given to the most recent ones) -- so that in general the discriminator works as an 'anomaly detector'. The generator is trained to keep the anomaly low (so, for example, if it goes too long without generating a red car, eventually the anomaly will rise and it will be forced to increase the red probability, through backpropagation on the anomaly).

Does this make sense?",15,5
138,2017-3-5,2017,3,5,4,5xipkt,Do you think AMD will take any Nvidia share of the deep learning market?,https://www.reddit.com/r/MachineLearning/comments/5xipkt/do_you_think_amd_will_take_any_nvidia_share_of/,BenniG123,1488656506,[removed],0,1
139,2017-3-5,2017,3,5,7,5xjjjs,Gradient descent algorithm,https://www.reddit.com/r/MachineLearning/comments/5xjjjs/gradient_descent_algorithm/,tomriddle441,1488666271,[removed],0,1
140,2017-3-5,2017,3,5,7,5xjo2y,"[D] I posted here about my site (arxiver.org) a few months ago, and now I'd like to add more features to help people be able to better navigate arxiv.org and read papers. Could I get some ideas and feedback on my ideas?",https://www.reddit.com/r/MachineLearning/comments/5xjo2y/d_i_posted_here_about_my_site_arxiverorg_a_few/,[deleted],1488667868,[deleted],0,1
141,2017-3-5,2017,3,5,7,5xjp07,Fake face detection for facial recognition with deep learning,https://www.reddit.com/r/MachineLearning/comments/5xjp07/fake_face_detection_for_facial_recognition_with/,nchafni,1488668195,[removed],0,1
142,2017-3-5,2017,3,5,8,5xjudh,"[D] I posted here about my site (arxiver.org) a few months ago, and now I'd like to add more features to help people be able to better navigate arxiv.org and read papers. Could I get some ideas and feedback on my ideas?",https://www.reddit.com/r/MachineLearning/comments/5xjudh/d_i_posted_here_about_my_site_arxiverorg_a_few/,[deleted],1488670058,[deleted],0,1
143,2017-3-5,2017,3,5,8,5xjudl,How do I approach the problem of finding the trending news stories?,https://www.reddit.com/r/MachineLearning/comments/5xjudl/how_do_i_approach_the_problem_of_finding_the/,heaveninside,1488670061,[removed],0,1
144,2017-3-5,2017,3,5,8,5xjuqj,"[D] I posted here about my site, arxiver.org, a few months ago and now I'd like to add more features to help people be able to better navigate arxiv.org and read papers. Could I get some feedback on my ideas as well as feature requests?",https://www.reddit.com/r/MachineLearning/comments/5xjuqj/d_i_posted_here_about_my_site_arxiverorg_a_few/,essofluffy,1488670198,"Hey everyone, 
 
I decided to build[ arxiver.org](https://www.arxiver.org/) after being frustrated with the design, layout, and lack of features of arxiv.org and shared it with /r/machinelearning a few months back ([thread](https://www.reddit.com/r/MachineLearning/comments/59qds2/d_i_built_this_site_axiverorg_so_i_could_save/)). I've approached this as a fun thing to do on the weekends and I'd like to get some thoughts on the site to make the site better for the people who could get the most out of it. Below, I've listed some ideas and I'd love to get feedback on how useful these features would be to you as well of any other thoughts you may have.

What I'm thinking doing right now:

- Button to copy individual paper's BibTeX entry
- Ability to export user's library to a BibTeX database
- Separate library into multiple project folders
- Add search by paper id capability
- Minor layout tweaks to make functions more obvious and new favicon


Any feedback on these ideas or site as well as any feature requests would be very much appreciated!",10,12
145,2017-3-5,2017,3,5,8,5xjzmk,"[D] If you don't have a lot of training data, which networks are the best for computer vision tasks?",https://www.reddit.com/r/MachineLearning/comments/5xjzmk/d_if_you_dont_have_a_lot_of_training_data_which/,Anti-Marxist-,1488671974,,12,0
146,2017-3-5,2017,3,5,9,5xk02u,[D] Efficient learning from small samples,https://www.reddit.com/r/MachineLearning/comments/5xk02u/d_efficient_learning_from_small_samples/,hidden-markov,1488672136,"A startup called Geometric Intelligence, which was recently acquired by Uber to become its Uber AI Labs, has claimed some good progress on small sample learning: https://www.technologyreview.com/s/601551/algorithms-that-learn-with-less-data-could-expand-ais-power/ . Their method is called XProp, and they say it converges faster than another ""unspecified deep-learning program"". It is all, however, under veil of secrecy right now.

Knowing that the CSO of Geometric Intelligence is Zoubin Ghahramani of Cambridge ML Group, ""the most Bayesian ML group"" out there, a good guess would be it uses Bayesian techniques, variational inference, or some ideas from Gaussian processes. I'm curious what does /r/machinelearning think it is, and how would we approach this problem nowadays?

This work seems relevant: https://www.robots.ox.ac.uk/~vgg/rg/papers/eccv2016_learntolearn.pdf , but it assumes there are models pre-trained on ""big"" datasets.

Related thread: https://www.reddit.com/r/MachineLearning/comments/4yhcx5/geometric_intelligencewhat_the_heck_is_it/ but it mostly focuses on the company and why it's secretive as hell, not the technology itself.

EDIT: ""the most relevant"" -&gt; relevant",4,89
147,2017-3-5,2017,3,5,9,5xk0tz,How is Machine Learning usually taught in academic institutions?,https://www.reddit.com/r/MachineLearning/comments/5xk0tz/how_is_machine_learning_usually_taught_in/,[deleted],1488672434,[removed],0,1
148,2017-3-5,2017,3,5,9,5xk2vr,[D] How is Machine Learning usually taught in academic institutions?,https://www.reddit.com/r/MachineLearning/comments/5xk2vr/d_how_is_machine_learning_usually_taught_in/,cse218,1488673118,"I'm a grad student at SBU and I'm currently taking our Machine Learning course. I struggle a lot with statistics, probability, and mathematical notation, and this course is forcing me to overcome this difficulty. The course is being taught like a math class - I was expecting an applied Machine Learning course, and was disappointed at first but I think I'm beginning to ""get"" it now.

Nonetheless I wanted to hear about how ML is usually taught elsewhere. Is there often a lot of formalism? I'm simultaneously watching Andrew Ng's coursera videos to try and get the best of both worlds, and I'm curious about the effectiveness of the different approaches.",7,1
149,2017-3-5,2017,3,5,10,5xkgnb,[D] Entering ML From a Physics Background: Where to Start?,https://www.reddit.com/r/MachineLearning/comments/5xkgnb/d_entering_ml_from_a_physics_background_where_to/,[deleted],1488678284,[deleted],1,1
150,2017-3-5,2017,3,5,11,5xkr9q,How to sell data for machine learning,https://www.reddit.com/r/MachineLearning/comments/5xkr9q/how_to_sell_data_for_machine_learning/,RespectIrony,1488682428,[removed],0,1
151,2017-3-5,2017,3,5,13,5xl5vk,Why do we only penalize weights with larger size?,https://www.reddit.com/r/MachineLearning/comments/5xl5vk/why_do_we_only_penalize_weights_with_larger_size/,[deleted],1488688422,[removed],0,1
152,2017-3-5,2017,3,5,13,5xl8ns,[D] What are the techniques to update weights in a neural network other than back propagation?,https://www.reddit.com/r/MachineLearning/comments/5xl8ns/d_what_are_the_techniques_to_update_weights_in_a/,commafighter,1488689617,"Can you tell me what are the other techniques used to update neural network weights other than back propagation? Back prop is the widely used technique in ANN to learn but it seems back prop is not found in biological neurons. I want to understand what other ways a neural network can update it weights?

For example: I have made this neural network which updates weights using random weights if the current mean error is less than the previous mean error. 
https://gist.github.com/anonymous/b1d2cce61722f38f378370c16a3967bc

",27,24
153,2017-3-5,2017,3,5,14,5xlg1f,[D] Randomized Optimization with Time Variable,https://www.reddit.com/r/MachineLearning/comments/5xlg1f/d_randomized_optimization_with_time_variable/,criticalcontext,1488692924,"So, this is a practical optimization question. Consider that you have an algorithm, and you are trying to optimize it's output by tuning it's hyperparameters with randomized hill climbing. However, one of the parameters practically makes the program run slower as you increase it. As such you need a ""sparser"" search space in this direction in order to run your optimization practically. However, all of your variables are dependent, so it may be the case that tuning needs to be done primarily in this high time cost area, and you want the optimization algorithm to figure that out via hill climbing normally, so you don't want to ""punish"" going into the high time cost area, you just want to vary sparcity by going into that area. Furthermore, consider this, what if you the researcher don't know what combination of variables have high time complexity or low, and you want the RHC optimization to take this into account automatically.

TL;DR: In very computationally costly spaces, techniques on optimization need to take time complexity dependence on tuning variables into consideration. Ideas?",2,3
154,2017-3-5,2017,3,5,15,5xliy9,Some questions about regularization,https://www.reddit.com/r/MachineLearning/comments/5xliy9/some_questions_about_regularization/,[deleted],1488694311,[removed],0,1
155,2017-3-5,2017,3,5,15,5xlj6v,"Deep Learning Project Workflow: Notes from Ng's ""Nuts and Bolts of Applying Deep Learning""",https://www.reddit.com/r/MachineLearning/comments/5xlj6v/deep_learning_project_workflow_notes_from_ngs/,hazard02,1488694420,,13,90
156,2017-3-5,2017,3,5,16,5xloe4,Clustering Machine Learning,https://www.reddit.com/r/MachineLearning/comments/5xloe4/clustering_machine_learning/,pratikaher88,1488697205,,0,1
157,2017-3-5,2017,3,5,17,5xlyks,Big Data Online Course. Begin your big data journey today..!! #OFFERVALIDITY24HRS COUPON:-GRABTODAY10,https://www.reddit.com/r/MachineLearning/comments/5xlyks/big_data_online_course_begin_your_big_data/,andalib_ansari,1488703180,,0,1
158,2017-3-5,2017,3,5,21,5xmjau,How hard is it to build a image recognition program?,https://www.reddit.com/r/MachineLearning/comments/5xmjau/how_hard_is_it_to_build_a_image_recognition/,Airanous,1488716064,[removed],0,1
159,2017-3-5,2017,3,5,22,5xmq8d,Uncertainty in Deep Learning (PhD Thesis) | Yarin Gal - Blog | Cambridge Machine Learning Group,https://www.reddit.com/r/MachineLearning/comments/5xmq8d/uncertainty_in_deep_learning_phd_thesis_yarin_gal/,[deleted],1488719624,[deleted],0,2
160,2017-3-5,2017,3,5,22,5xmquk,"[P] Wide-Residual Network Ensemble model CIFAR-10, CIFAR-100, SVHN toy dataset training",https://www.reddit.com/r/MachineLearning/comments/5xmquk/p_wideresidual_network_ensemble_model_cifar10/,[deleted],1488719928,[deleted],0,1
161,2017-3-5,2017,3,5,22,5xms34,"[P] Torch implementation for wide-residual network ensemble model for CIFAR-10, CIFAR-100, SVHN",https://www.reddit.com/r/MachineLearning/comments/5xms34/p_torch_implementation_for_wideresidual_network/,meliketoy,1488720534,,2,6
162,2017-3-5,2017,3,5,23,5xmwjc,How Language Could Have Evolved - draft,https://www.reddit.com/r/MachineLearning/comments/5xmwjc/how_language_could_have_evolved_draft/,pseudocoder1,1488722499,[removed],0,1
163,2017-3-5,2017,3,5,23,5xn3sz,omlathemachines.com- lathe machine manufacturer in batala,https://www.reddit.com/r/MachineLearning/comments/5xn3sz/omlathemachinescom_lathe_machine_manufacturer_in/,gaganloomba,1488725448,,0,1
164,2017-3-6,2017,3,6,0,5xnfaf,Classifying Graphs with Shortest Paths,https://www.reddit.com/r/MachineLearning/comments/5xnfaf/classifying_graphs_with_shortest_paths/,ibgeek,1488729509,,0,1
165,2017-3-6,2017,3,6,3,5xo2eu,Gumbel-softmax VS Softmax with observed categoritcal variables,https://www.reddit.com/r/MachineLearning/comments/5xo2eu/gumbelsoftmax_vs_softmax_with_observed/,yield22,1488736868,[removed],0,1
166,2017-3-6,2017,3,6,3,5xo4gt,[P] Training neural networks with iterative projection algorithms,https://www.reddit.com/r/MachineLearning/comments/5xo4gt/p_training_neural_networks_with_iterative/,max-synchro,1488737514,,22,69
167,2017-3-6,2017,3,6,3,5xo4m7,[D] Gumbel-softmax VS Softmax with observed categoritcal variables,https://www.reddit.com/r/MachineLearning/comments/5xo4m7/d_gumbelsoftmax_vs_softmax_with_observed/,yield22,1488737568,"I know that Gumbel-softmax allows to draw (stochastic) samples from discrete distribution. However, when samples are observed as in training with given words in many NLP problems, do we still need the Gumbel distribution and why?

IMO, instead of drawing a sample by Gumbel, we can simply use the softmax as mean sample, and use Straight Through Estimator (as similarly used in Gumbel-softmax), as it can reduce the variance of gradient. Am I missing something here?",7,6
168,2017-3-6,2017,3,6,3,5xo4qj,Billion-scale similarity search with GPUs,https://www.reddit.com/r/MachineLearning/comments/5xo4qj/billionscale_similarity_search_with_gpus/,kendrick__,1488737606,,0,1
169,2017-3-6,2017,3,6,7,5xpk6p,Challenge: ML generals.io bot,https://www.reddit.com/r/MachineLearning/comments/5xpk6p/challenge_ml_generalsio_bot/,Jaywalker000,1488753251,[removed],0,1
170,2017-3-6,2017,3,6,7,5xpo10,"[P] Make a comment, get a comment. (Seq2seq trained on reddit comments.)",https://www.reddit.com/r/MachineLearning/comments/5xpo10/p_make_a_comment_get_a_comment_seq2seq_trained_on/,quirm,1488754402,,38,30
171,2017-3-6,2017,3,6,8,5xpvin,Exponential Linear Units are the New Cool,https://www.reddit.com/r/MachineLearning/comments/5xpvin/exponential_linear_units_are_the_new_cool/,saikatbsk,1488756802,,0,1
172,2017-3-6,2017,3,6,10,5xqgdh,[D] What are the ideal conditions for a Machine Learning / Deep Learning project to be successful? Big data; specific goals; well defined questions.... What else?,https://www.reddit.com/r/MachineLearning/comments/5xqgdh/d_what_are_the_ideal_conditions_for_a_machine/,artmast,1488763753,"I work for an insurance company and I am planning on approaching upper management to promote the use for AI / Machine Learning / Deep Learning for specific functions such as pricing the insurance, minimizing losses and fraud detection.  I work with data in IT, but I have never touched AI before.

One of the things I want management to understand is that Deep Learning is only useful in certain conditions, it is perfect some things, and not so great for others.  So when is Deep Learning most useful? What are the ideal conditions for a Machine Learning / Deep Learning project to be successful?  Here's what I have so far:

AI/deep learning is ideal when:

* You have a lot of data, structured or unstructured
* You are trying to do something very specific goal - Well defined questions
* You can train it based on desired outcomes
* What else?

I have googled this about 30 different ways now, and have failed to come up with a good list.

Thanks for your help!!",12,13
173,2017-3-6,2017,3,6,10,5xqjkq,Review my Particle Classifier code?,https://www.reddit.com/r/MachineLearning/comments/5xqjkq/review_my_particle_classifier_code/,MaxConners,1488764883,[removed],0,1
174,2017-3-6,2017,3,6,11,5xqmxa,How would I implement a perceptron algorithm through naive bayes?,https://www.reddit.com/r/MachineLearning/comments/5xqmxa/how_would_i_implement_a_perceptron_algorithm/,compute_,1488766035,[removed],0,1
175,2017-3-6,2017,3,6,12,5xr3ni,"""Large-Scale Evolution of Image Classifiers"", Real et al 2017",https://www.reddit.com/r/MachineLearning/comments/5xr3ni/largescale_evolution_of_image_classifiers_real_et/,gwern,1488771928,,3,1
176,2017-3-6,2017,3,6,14,5xrn1y,Keras Question on feeding images to a trained model,https://www.reddit.com/r/MachineLearning/comments/5xrn1y/keras_question_on_feeding_images_to_a_trained/,Nixonite,1488779499,[removed],0,1
177,2017-3-6,2017,3,6,14,5xrn7x,Deep Semi-Random Features for Nonlinear Function Approximation,https://www.reddit.com/r/MachineLearning/comments/5xrn7x/deep_semirandom_features_for_nonlinear_function/,machiner_ps,1488779576,,0,1
178,2017-3-6,2017,3,6,15,5xrt2m,"[P]MTCNN Face Detection on Raspberry PI 3 with Caffe, and motion trigger",https://www.reddit.com/r/MachineLearning/comments/5xrt2m/pmtcnn_face_detection_on_raspberry_pi_3_with/,solderzzc,1488782140,,20,100
179,2017-3-6,2017,3,6,18,5xsd0f,Beneficial to use TensorFlow on binary classification,https://www.reddit.com/r/MachineLearning/comments/5xsd0f/beneficial_to_use_tensorflow_on_binary/,florali95,1488792555,[removed],0,1
180,2017-3-6,2017,3,6,18,5xsenx,Autonomous Vehicles: Share Your Expertise &amp; Opinions For the Huffington Post,https://www.reddit.com/r/MachineLearning/comments/5xsenx/autonomous_vehicles_share_your_expertise_opinions/,reworksophie,1488793436,,0,1
181,2017-3-6,2017,3,6,20,5xsp5t,Android TensorFlow Machine Learning Example,https://www.reddit.com/r/MachineLearning/comments/5xsp5t/android_tensorflow_machine_learning_example/,amitshekhariitbhu,1488798998,,0,1
182,2017-3-6,2017,3,6,20,5xstki,"Q(lambda) Doubt, and code review request",https://www.reddit.com/r/MachineLearning/comments/5xstki/qlambda_doubt_and_code_review_request/,leHorus,1488801222,[removed],0,1
183,2017-3-6,2017,3,6,21,5xswgz,[R] [1703.01161] FeUdal Networks (FuNs) for Hierarchical Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/5xswgz/r_170301161_feudal_networks_funs_for_hierarchical/,evc123,1488802515,,14,67
184,2017-3-6,2017,3,6,21,5xt2f9,They are training an AI to detect plastics on coastlines,https://www.reddit.com/r/MachineLearning/comments/5xt2f9/they_are_training_an_ai_to_detect_plastics_on/,[deleted],1488805112,[deleted],0,1
185,2017-3-6,2017,3,6,22,5xt2vl,[R] Machine Learning on Sequential Data Using a Recurrent Weighted Average,https://www.reddit.com/r/MachineLearning/comments/5xt2vl/r_machine_learning_on_sequential_data_using_a/,[deleted],1488805283,[deleted],6,1
186,2017-3-6,2017,3,6,22,5xt4aw,Are Machines Taking Over the Government?,https://www.reddit.com/r/MachineLearning/comments/5xt4aw/are_machines_taking_over_the_government/,Dcode42dc,1488805783,,0,1
187,2017-3-6,2017,3,6,22,5xt6z7,"If there is a entrepreneur idea sharing platfrom about machine learning, how much you will share your idea?",https://www.reddit.com/r/MachineLearning/comments/5xt6z7/if_there_is_a_entrepreneur_idea_sharing_platfrom/,lamborhino,1488806835,[removed],0,1
188,2017-3-6,2017,3,6,23,5xtetw,[P] NewralNet - A Java library which lets you tinker around with deep neural nets,https://www.reddit.com/r/MachineLearning/comments/5xtetw/p_newralnet_a_java_library_which_lets_you_tinker/,[deleted],1488809666,[deleted],0,1
189,2017-3-6,2017,3,6,23,5xtjfa,Read my blog post on: Install notes - OpenCV in Python3 and Ubuntu 16.10.,https://www.reddit.com/r/MachineLearning/comments/5xtjfa/read_my_blog_post_on_install_notes_opencv_in/,doctorsroom,1488811214,,0,1
190,2017-3-7,2017,3,7,0,5xtnl4,[D] Reading arXiv preprints on an e-reader?,https://www.reddit.com/r/MachineLearning/comments/5xtnl4/d_reading_arxiv_preprints_on_an_ereader/,pmigdal,1488812533,"To stay up-to-date in Deep Learning I read a lot of publications, mostly - arXiv preprints. However, on-screen reading is not always the most convenient, so I typically print it.

Do you know if there is a way to get arXiv preprints on an e-reader (e.g. Kindle)? (Scripts, apps or approaches you actually use.)

Just downloading a PDF usually produced to tinny fonts, and is especially annoying for two-column layout. So I am looking for scripts, apps or web-services that reflow arXiv into a readable mobi/ePub format.",19,19
191,2017-3-7,2017,3,7,0,5xtrap,"Joint interview with AI experts Erik Schmidt @ Pandora, Gilles Backhus @ Konux, Stacey Svetlichnaya @ Flickr and Nikhil George @ VW - on ML, computer vision, autonomous vehicles",https://www.reddit.com/r/MachineLearning/comments/5xtrap/joint_interview_with_ai_experts_erik_schmidt/,reworksophie,1488813617,,0,1
192,2017-3-7,2017,3,7,0,5xtubi,[P] NewralNet - A Java library which lets you tinker around with deep neural nets,https://www.reddit.com/r/MachineLearning/comments/5xtubi/p_newralnet_a_java_library_which_lets_you_tinker/,flimmerkiste,1488814493,,2,1
193,2017-3-7,2017,3,7,1,5xu7ll,"MOD 2017: The 3rd International Conference on Machine learning, Optimization &amp; big Data An Interdisciplinary Conference: Machine Learning, Optimization and Data Science without Borders",https://www.reddit.com/r/MachineLearning/comments/5xu7ll/mod_2017_the_3rd_international_conference_on/,Ludovico_Montalcini,1488818293,[removed],0,1
194,2017-3-7,2017,3,7,1,5xuagp,How would I create a generalized model on a per-user basis?,https://www.reddit.com/r/MachineLearning/comments/5xuagp/how_would_i_create_a_generalized_model_on_a/,hntd,1488819041,[removed],0,1
195,2017-3-7,2017,3,7,3,5xuyi2,https://www.captionbot.ai,https://www.reddit.com/r/MachineLearning/comments/5xuyi2/httpswwwcaptionbotai/,matrinox,1488825468,,0,1
196,2017-3-7,2017,3,7,4,5xva1h,PyTorch vs Tensorflow speed,https://www.reddit.com/r/MachineLearning/comments/5xva1h/pytorch_vs_tensorflow_speed/,marutiagarwal,1488828451,[removed],0,1
197,2017-3-7,2017,3,7,5,5xvhqo,Researcher Breaks reCAPTCHA Using Google's Speech Recognition API,https://www.reddit.com/r/MachineLearning/comments/5xvhqo/researcher_breaks_recaptcha_using_googles_speech/,GrabAHamLincoln,1488830417,,0,1
198,2017-3-7,2017,3,7,5,5xvovn,[D] literature on spatio-temporal data representations analysis before learning model training,https://www.reddit.com/r/MachineLearning/comments/5xvovn/d_literature_on_spatiotemporal_data/,insider_7,1488832258,"I am looking for literature on data representation analysis before the learning model. I am looking on way to process the input data and its effect on a learning model. I have been successful on effects on different learning models from the same input spatio-temporal data, but no studies on the other way: changes in the spatio-temporal data for learning models. any directions on this?",0,0
199,2017-3-7,2017,3,7,6,5xw55b,Need some tips before I spend a bunch,https://www.reddit.com/r/MachineLearning/comments/5xw55b/need_some_tips_before_i_spend_a_bunch/,machinelearningf,1488836513,[removed],0,1
200,2017-3-7,2017,3,7,7,5xwckw,[P] Scattertext: a visualization framework for understanding BoW feature importances (among other things),https://www.reddit.com/r/MachineLearning/comments/5xwckw/p_scattertext_a_visualization_framework_for/,jasonskessler,1488838498,,4,13
201,2017-3-7,2017,3,7,8,5xwu68,[P] Deep Session Learning for Cyber Security,https://www.reddit.com/r/MachineLearning/comments/5xwu68/p_deep_session_learning_for_cyber_security/,amplifier_khan,1488843269,,0,1
202,2017-3-7,2017,3,7,9,5xx04x,Can Machine Learning Assist Oncologists?,https://www.reddit.com/r/MachineLearning/comments/5xx04x/can_machine_learning_assist_oncologists/,scientifist,1488845026,,0,1
203,2017-3-7,2017,3,7,9,5xx1l5,Learning integer sequences,https://www.reddit.com/r/MachineLearning/comments/5xx1l5/learning_integer_sequences/,[deleted],1488845448,[deleted],0,1
204,2017-3-7,2017,3,7,9,5xx2a9,"Deep XOR: Modelling an XOR gate using a neural network in Numpy, step-by-step",https://www.reddit.com/r/MachineLearning/comments/5xx2a9/deep_xor_modelling_an_xor_gate_using_a_neural/,[deleted],1488845638,[deleted],0,1
205,2017-3-7,2017,3,7,9,5xx5k8,[1703.01253] Machine Learning on Sequential Data Using a Recurrent Weighted Average,https://www.reddit.com/r/MachineLearning/comments/5xx5k8/170301253_machine_learning_on_sequential_data/,[deleted],1488846660,[deleted],1,1
206,2017-3-7,2017,3,7,10,5xxdp3,Any good AI groups in South bay/Silicon valley?,https://www.reddit.com/r/MachineLearning/comments/5xxdp3/any_good_ai_groups_in_south_baysilicon_valley/,leakytanh,1488849215,[removed],0,1
207,2017-3-7,2017,3,7,10,5xxh4s,Question re number of mutually orthogonal vectors,https://www.reddit.com/r/MachineLearning/comments/5xxh4s/question_re_number_of_mutually_orthogonal_vectors/,[deleted],1488850357,[removed],0,1
208,2017-3-7,2017,3,7,11,5xxqcd,[R] The Neural Physics Engine: A Compositional Object-Based Approach To Learning Physical Dynamics,https://www.reddit.com/r/MachineLearning/comments/5xxqcd/r_the_neural_physics_engine_a_compositional/,mbchang,1488853281,,9,79
209,2017-3-7,2017,3,7,11,5xxx14,[R] Neural Episodic Control (DeepMind),https://www.reddit.com/r/MachineLearning/comments/5xxx14/r_neural_episodic_control_deepmind/,xternalz,1488855467,,9,60
210,2017-3-7,2017,3,7,12,5xy5p2,Commodity concrete mixing plant composition,https://www.reddit.com/r/MachineLearning/comments/5xy5p2/commodity_concrete_mixing_plant_composition/,FlyerConcretePlant,1488858319,[removed],1,1
211,2017-3-7,2017,3,7,14,5xyjhk,"What is the Difference between "" Introduction to Linear Algebra."" 4th Fourth and 5th Fifth Edition by Gilbert Strang",https://www.reddit.com/r/MachineLearning/comments/5xyjhk/what_is_the_difference_between_introduction_to/,grantrostig,1488863237,[removed],0,1
212,2017-3-7,2017,3,7,15,5xywvg,[D] Is it better to crop an image or to scale both the sides of an image to the same size while training for segmentation?,https://www.reddit.com/r/MachineLearning/comments/5xywvg/d_is_it_better_to_crop_an_image_or_to_scale_both/,[deleted],1488868806,[deleted],4,0
213,2017-3-7,2017,3,7,16,5xz30u,ACM RecSys Challenge 2017 - recommending job posts,https://www.reddit.com/r/MachineLearning/comments/5xz30u/acm_recsys_challenge_2017_recommending_job_posts/,recsys,1488871659,,0,1
214,2017-3-7,2017,3,7,17,5xz8xw,Document similarity: Vector embedding versus BoW performance?,https://www.reddit.com/r/MachineLearning/comments/5xz8xw/document_similarity_vector_embedding_versus_bow/,sutrostyle,1488874720,[removed],0,1
215,2017-3-7,2017,3,7,17,5xzc13,"Automatic category recommendation machine-learning API for e-commerce. Opensource, download it freely.",https://www.reddit.com/r/MachineLearning/comments/5xzc13/automatic_category_recommendation_machinelearning/,mezeipetister,1488876421,,0,1
216,2017-3-7,2017,3,7,17,5xzdc7,"Slamby API as instant solution for e-commerce machine-learning; text classification, category recommendation.",https://www.reddit.com/r/MachineLearning/comments/5xzdc7/slamby_api_as_instant_solution_for_ecommerce/,mezeipetister,1488877191,[removed],0,1
217,2017-3-7,2017,3,7,18,5xze1s,JCT industrial food blenders can turn into a suitable food blenders?,https://www.reddit.com/r/MachineLearning/comments/5xze1s/jct_industrial_food_blenders_can_turn_into_a/,mixmachinery,1488877579,,1,1
218,2017-3-7,2017,3,7,19,5xzkqn,Toy data set for ConvNets ? (for teaching),https://www.reddit.com/r/MachineLearning/comments/5xzkqn/toy_data_set_for_convnets_for_teaching/,tlreddit,1488881218,[removed],0,1
219,2017-3-7,2017,3,7,19,5xzodi,Github Implementation of All-Convolution Net (https://arxiv.org/abs/1412.6806),https://www.reddit.com/r/MachineLearning/comments/5xzodi/github_implementation_of_allconvolution_net/,curious_rv,1488883172,,1,2
220,2017-3-7,2017,3,7,19,5xzqjq,Pharmagut Deutsche Internet Apotheke - Potenzmittel rezeptfrei,https://www.reddit.com/r/MachineLearning/comments/5xzqjq/pharmagut_deutsche_internet_apotheke_potenzmittel/,scottiekinesdgu,1488884319,,0,1
221,2017-3-7,2017,3,7,20,5xzssa,Create a vector with different data types in r Programming,https://www.reddit.com/r/MachineLearning/comments/5xzssa/create_a_vector_with_different_data_types_in_r/,tejarampooniya,1488885428,,0,1
222,2017-3-7,2017,3,7,20,5xzv0s,"Learning integer sequences ""[Discussion]""",https://www.reddit.com/r/MachineLearning/comments/5xzv0s/learning_integer_sequences_discussion/,aidanrocke,1488886598,,3,5
223,2017-3-7,2017,3,7,21,5y00ms,Openai Salary,https://www.reddit.com/r/MachineLearning/comments/5y00ms/openai_salary/,j_lyf,1488889210,,0,0
224,2017-3-7,2017,3,7,22,5y0c3a,[D]Have an idea of building machine learning rigs for rental. Can I get your feedback?,https://www.reddit.com/r/MachineLearning/comments/5y0c3a/dhave_an_idea_of_building_machine_learning_rigs/,Arrow222,1488893570,"Hi guys


I notice a serious lack of ML compute resources for universities in my country (Hong Kong) and the prices of Amazon P2 instances are really expensive. ($0.9 per hour for a K80 Tesla, 8.7Tflops single precision)


Hence I have an idea to setup cost effective ML rigs that can be rented out for less than half of Amazon's P2 instance price and am seeking your opinion for such services.


One probable reason for high cloud computing cost is their choice of equipment. [Enterprise grade Teslas](https://www.newegg.com/Product/Product.aspx?Item=N82E16814132041), their motherboards and supporting equipment offer poor price to performance.


From my understanding, ML doesn't require FP64 compute and ECC GPU memory, specific to Tesla GPUs. Hence by using workstation/consumer grade equipment, a much better price to performance ratio can be offered to renters.


4x K80 Tesla with 34.8Tflops of single precision compute costs $3.6/hour or $86.4/day if you were to use Amazon.


Would you consider renting

- A 1080Ti (10.6Tflops single precision) + Kaby Lake i3 and 32gb ram for $0.45/hour or $10.8/day?

- 4 x Gtx 1080Ti (42.4 Tflops single precision) + 6 core X99 system and 128gb ram for $1.5/hour or $36/day?

Do you have any specific needs that need to be addressed? (Internet speeds, storage?)

Open to any feedback and suggestions, thanks for reading!

",11,8
225,2017-3-7,2017,3,7,22,5y0el7,"XLA team in Google announces XLA (Accelerated Linear Algebra), a JIT compiler for TensorFlow.",https://www.reddit.com/r/MachineLearning/comments/5y0el7/xla_team_in_google_announces_xla_accelerated/,dare_dick,1488894456,,0,1
226,2017-3-7,2017,3,7,23,5y0gzr,[R] Cats and Captions vs. Creators and the Clock: Comparing Multimodal Content to Context in Predicting Relative Popularity,https://www.reddit.com/r/MachineLearning/comments/5y0gzr/r_cats_and_captions_vs_creators_and_the_clock/,[deleted],1488895265,[deleted],0,1
227,2017-3-7,2017,3,7,23,5y0h4i,Solving linear regression problem using tensorflow,https://www.reddit.com/r/MachineLearning/comments/5y0h4i/solving_linear_regression_problem_using_tensorflow/,ArberB1987,1488895303,,0,1
228,2017-3-7,2017,3,7,23,5y0j0d,[R] Cats and Captions vs. Creators and the Clock: Comparing multimodal content to context in predicting relative popularity,https://www.reddit.com/r/MachineLearning/comments/5y0j0d/r_cats_and_captions_vs_creators_and_the_clock/,JacksCornellAccount,1488895894,,1,4
229,2017-3-7,2017,3,7,23,5y0pbm,What's the difference between Lorenz Curve and ROC Curve?,https://www.reddit.com/r/MachineLearning/comments/5y0pbm/whats_the_difference_between_lorenz_curve_and_roc/,ppzhang,1488897734,[removed],0,1
230,2017-3-8,2017,3,8,0,5y0w0z,Sv Paketleme Makinas,https://www.reddit.com/r/MachineLearning/comments/5y0w0z/sv_paketleme_makinas/,renasmakinatr,1488899380,[removed],0,1
231,2017-3-8,2017,3,8,0,5y0wrf,[R] Count-Based Exploration with Neural Density Models,https://www.reddit.com/r/MachineLearning/comments/5y0wrf/r_countbased_exploration_with_neural_density/,downtownslim,1488899546,,3,10
232,2017-3-8,2017,3,8,1,5y1bip,Neural networks give voting advice for upcoming Dutch elections,https://www.reddit.com/r/MachineLearning/comments/5y1bip/neural_networks_give_voting_advice_for_upcoming/,Dutchcheesehead,1488902955,,0,2
233,2017-3-8,2017,3,8,1,5y1gqy,[D] Deploy models to low power devices,https://www.reddit.com/r/MachineLearning/comments/5y1gqy/d_deploy_models_to_low_power_devices/,fmichele89,1488904112,"I trained a model in keras/tensorflow which performs quite well.
Now I need to port this model on an embedded platform.
I would like to generate a virtually dependency-free ANSI C implementation of my model.
Ideally, the weights should be embedded in the function, and its only duty is to take two pointers, one to the input array and one to the output array.
At this stage I don't even care about performances.

I have done some research, but I didn't find nothing. I already tried XLA, but without luck.
Is there anything else that I can try? Any suggestions?",8,8
234,2017-3-8,2017,3,8,1,5y1m3e,"Learn-blog: A blog simplifying the concepts of machine learning with comics, ""decoded"" vocabulary, and examples.",https://www.reddit.com/r/MachineLearning/comments/5y1m3e/learnblog_a_blog_simplifying_the_concepts_of/,willbeddow,1488905326,[removed],0,1
235,2017-3-8,2017,3,8,2,5y1pre,"Learn-blog: A blog simplifying the concepts of machine learning with examples, comics, and decoded vocabulary. [P] [R]",https://www.reddit.com/r/MachineLearning/comments/5y1pre/learnblog_a_blog_simplifying_the_concepts_of/,willbeddow,1488906146,"**tl;dr**: I study and work in machine learning, and with the help of another machine learning student we made a blog that decodes and simplifies the concepts. We call it [learn-blog](https://ironman5366.github.io/learn-blog)


Longer version:


I've been studying the field of machine learning for awhile now, and what struck me most about the field is that it's not nearly as hard as it seems to be, but a lot of the terminology is needlessly confusing.

I posted about this on devrant and a lot of people there encouraged me to make learn-blog. So, working with /u/thejohnhoffer we made it.

In each lesson, we address common misconceptions and decode confusingly termed topics into understandable concepts. 

We also provide examples of how to make neural networks using [keras](https://keras.io), assuming only a rudamentary understanding of Python.

- [The site](https://ironman5366.github.io/learn-blog)

- [Lesson 1 - ""Neurons""](https://ironman5366.github.io/learn-blog/2017/02/26/Lesson01-Neurons.html)
- [Lesson 2 - A basic example](https://ironman5366.github.io/learn-blog/2017/03/02/Lesson02-NeuralExample.html)


",76,68
236,2017-3-8,2017,3,8,2,5y1rwr,[R] Machine Learning on Sequential Data Using a Recurrent Weighted Average,https://www.reddit.com/r/MachineLearning/comments/5y1rwr/r_machine_learning_on_sequential_data_using_a/,siblbombs,1488906594,,8,3
237,2017-3-8,2017,3,8,2,5y1xea,Not-Word Embeddings?,https://www.reddit.com/r/MachineLearning/comments/5y1xea/notword_embeddings/,Lolologist,1488907805,[removed],0,1
238,2017-3-8,2017,3,8,2,5y1xrf,[P] Cherry-Autonomous-Racecar: Tensorflow implementation of Nvidia's End to End paper on a Jetson TX1 powered RC car,https://www.reddit.com/r/MachineLearning/comments/5y1xrf/p_cherryautonomousracecar_tensorflow/,PaviumLabs,1488907877,,1,18
239,2017-3-8,2017,3,8,3,5y265q,[Discussion] Noteworthy recent papers,https://www.reddit.com/r/MachineLearning/comments/5y265q/discussion_noteworthy_recent_papers/,metacurse,1488909686,"I have failed to keep up with the recent flood of ML papers. I am mainly intersted in the ones that are related to Deep Learning and Neural Networks.

I think Jack Clark was maintaining a Google Doc, but I don't think that produced what I am looking for. So I thought of creating this thread where we could up/down vote on papers to wade through the this jungle of papers and learn something along the way.

I am also hoping to reinvigorate the thoughtful discussions that were happening on this sub some 2-3 years ago around papers and research that has now somehow decayed. ",8,34
240,2017-3-8,2017,3,8,3,5y2bw2,ML / AI / Deep Learning / Data Science -- Blurred Lines! Where does NLP begin and end in all of this?,https://www.reddit.com/r/MachineLearning/comments/5y2bw2/ml_ai_deep_learning_data_science_blurred_lines/,eshaansharma,1488910937,[removed],0,1
241,2017-3-8,2017,3,8,3,5y2hd2,Quantiacs -- A Quant Hedge Fund Built With Freelancers,https://www.reddit.com/r/MachineLearning/comments/5y2hd2/quantiacs_a_quant_hedge_fund_built_with/,brinkwar,1488912133,,0,1
242,2017-3-8,2017,3,8,4,5y2rt4,[D] is there a preferred pretrained language-model for calculating the likelihood of an english sentence?,https://www.reddit.com/r/MachineLearning/comments/5y2rt4/d_is_there_a_preferred_pretrained_languagemodel/,sentenceModeller,1488914499,"I've been looking at https://github.com/tensorflow/models/tree/master/lm_1b but I don't see an obvious way to calculate the actual likelihood of an input sentence, is anyone else using another tool that is more straightforward?

Thanks very much

EDIT: I just realized that ""Given provided dataset, calculate the model's perplexity."" means exactly what I want unless I'm missing something, comments very much welcome none the less as I'm clearly pushing my own limits here haha ... 
EDIT 2: It looks like its training though when you run it in eval mode ... which I don't want to do I just want to input a given sentence and see how likely that sentence is based on the pretrained model.",1,3
243,2017-3-8,2017,3,8,7,5y3s0l,[D] JupyterLab for Deep Learning use ?,https://www.reddit.com/r/MachineLearning/comments/5y3s0l/d_jupyterlab_for_deep_learning_use/,xingdongrobotics,1488924030,[removed],0,2
244,2017-3-8,2017,3,8,7,5y3w1b,Help me pick a good technique for my data,https://www.reddit.com/r/MachineLearning/comments/5y3w1b/help_me_pick_a_good_technique_for_my_data/,[deleted],1488925107,[removed],0,1
245,2017-3-8,2017,3,8,8,5y4a4m,[R] Multi-step Reinforcement Learning: A Unifying Algorithm,https://www.reddit.com/r/MachineLearning/comments/5y4a4m/r_multistep_reinforcement_learning_a_unifying/,downtownslim,1488929128,,1,19
246,2017-3-8,2017,3,8,8,5y4gpr,[R] Tagger: Deep Unsupervised Perceptual Grouping,https://www.reddit.com/r/MachineLearning/comments/5y4gpr/r_tagger_deep_unsupervised_perceptual_grouping/,cbeak,1488931044,,9,6
247,2017-3-8,2017,3,8,9,5y4sk4,Tutorial: Deep Learning for NLP in Pytorch,https://www.reddit.com/r/MachineLearning/comments/5y4sk4/tutorial_deep_learning_for_nlp_in_pytorch/,nlpinpytorch,1488934552,,0,1
248,2017-3-8,2017,3,8,11,5y56of,[R] [1703.02528] Stopping GAN Violence: Generative Unadversarial Networks,https://www.reddit.com/r/MachineLearning/comments/5y56of/r_170302528_stopping_gan_violence_generative/,evc123,1488939019,,43,152
249,2017-3-8,2017,3,8,11,5y5aa2,NVIDIA Jetson TX2 Delivers Twice the Intelligence to the Edge,https://www.reddit.com/r/MachineLearning/comments/5y5aa2/nvidia_jetson_tx2_delivers_twice_the_intelligence/,dusty_nv,1488940196,,0,1
250,2017-3-8,2017,3,8,13,5y5w40,Semantic attribute tagging for video,https://www.reddit.com/r/MachineLearning/comments/5y5w40/semantic_attribute_tagging_for_video/,[deleted],1488947525,[removed],0,1
251,2017-3-8,2017,3,8,13,5y5yc7,[P] Generating anime faces (and more) using Least Squares GAN,https://www.reddit.com/r/MachineLearning/comments/5y5yc7/p_generating_anime_faces_and_more_using_least/,hawking1125,1488948352,,8,21
252,2017-3-8,2017,3,8,14,5y61bg,[N] Google is acquiring data science community Kaggle,https://www.reddit.com/r/MachineLearning/comments/5y61bg/n_google_is_acquiring_data_science_community/,peeyek,1488949443,,88,730
253,2017-3-8,2017,3,8,14,5y63w8,DeepStack: Expert-level artificial intelligence in heads-up no-limit poker,https://www.reddit.com/r/MachineLearning/comments/5y63w8/deepstack_expertlevel_artificial_intelligence_in/,mks_repi,1488950365,,0,1
254,2017-3-8,2017,3,8,15,5y6ggb,Baidu Deep Voice explained: Part 1  the Inference Pipeline,https://www.reddit.com/r/MachineLearning/comments/5y6ggb/baidu_deep_voice_explained_part_1_the_inference/,mayyuen318,1488955413,,0,1
255,2017-3-8,2017,3,8,15,5y6guj,[P] Pronunciation Dictionaries for Multiple Languages,https://www.reddit.com/r/MachineLearning/comments/5y6guj/p_pronunciation_dictionaries_for_multiple/,longinglove,1488955592,,1,4
256,2017-3-8,2017,3,8,15,5y6h8y,Introduction to Gradient Descent Algorithm (along with variants) in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/5y6h8y/introduction_to_gradient_descent_algorithm_along/,ankit_123,1488955773,,0,1
257,2017-3-8,2017,3,8,16,5y6j1i,Both Wasserstein GAN and LS-GAN belong to a super class of Generalized LS-GANs (GLS-GANs),https://www.reddit.com/r/MachineLearning/comments/5y6j1i/both_wasserstein_gan_and_lsgan_belong_to_a_super/,guojunq,1488956604,[removed],0,1
258,2017-3-8,2017,3,8,16,5y6jhr,Why does training and test error suddenly drop after a certain number of iterations (after 3e4) in the attached image?,https://www.reddit.com/r/MachineLearning/comments/5y6jhr/why_does_training_and_test_error_suddenly_drop/,HyperSpectralVoice,1488956792,,0,1
259,2017-3-8,2017,3,8,16,5y6juw,Introducing Similarity Search at Flickr,https://www.reddit.com/r/MachineLearning/comments/5y6juw/introducing_similarity_search_at_flickr/,kluikens,1488956964,,0,1
260,2017-3-8,2017,3,8,16,5y6kq1,Why does training and test error suddenly drop after a certain number of iterations (after 3e4) in the attached image?,https://www.reddit.com/r/MachineLearning/comments/5y6kq1/why_does_training_and_test_error_suddenly_drop/,DL-Z_ftw,1488957393,,1,1
261,2017-3-8,2017,3,8,16,5y6ks3,GRU-RCN,https://www.reddit.com/r/MachineLearning/comments/5y6ks3/grurcn/,yidarmy12345,1488957423,[removed],0,1
262,2017-3-8,2017,3,8,16,5y6nkj,Mobile concrete batching plant build in Philippines,https://www.reddit.com/r/MachineLearning/comments/5y6nkj/mobile_concrete_batching_plant_build_in/,FlyerConcretePlant,1488958778,[removed],1,1
263,2017-3-8,2017,3,8,16,5y6pon,NVIDIA Announces Jetson TX2: Parker Comes To NVIDIAs Embedded System Kit,https://www.reddit.com/r/MachineLearning/comments/5y6pon/nvidia_announces_jetson_tx2_parker_comes_to/,Phnyx,1488959883,,0,1
264,2017-3-8,2017,3,8,17,5y6s35,"""Hands on"" Data Science Bootcamp",https://www.reddit.com/r/MachineLearning/comments/5y6s35/hands_on_data_science_bootcamp/,theDevMasters,1488961070,,1,1
265,2017-3-8,2017,3,8,17,5y6u7k,"BEST ""Hands on"" Data Science Bootcamp",https://www.reddit.com/r/MachineLearning/comments/5y6u7k/best_hands_on_data_science_bootcamp/,theDevMasters,1488962174,,1,1
266,2017-3-8,2017,3,8,17,5y6uy8,[D] Did Ian Goodfellow leave OpenAI?,https://www.reddit.com/r/MachineLearning/comments/5y6uy8/d_did_ian_goodfellow_leave_openai/,1101010101010101,1488962582,"On his LinkedIn, it shows that he joined Google again very recently: https://www.linkedin.com/in/ian-goodfellow-b7187213/ Does anyone know why that is? What does this mean for OpenAI/the openness of the field/his work on AI safety?",25,31
267,2017-3-8,2017,3,8,17,5y6wje,Data Science Training,https://www.reddit.com/r/MachineLearning/comments/5y6wje/data_science_training/,theDevMasters,1488963447,,0,1
268,2017-3-8,2017,3,8,19,5y75pz,How to modify the Keras VAE code to use two layers of latent variables ?,https://www.reddit.com/r/MachineLearning/comments/5y75pz/how_to_modify_the_keras_vae_code_to_use_two/,sbaur_pasteur,1488968238,[removed],0,1
269,2017-3-8,2017,3,8,20,5y7cm9,Dileep George will be a keynote speaker at AGI-2017,https://www.reddit.com/r/MachineLearning/comments/5y7cm9/dileep_george_will_be_a_keynote_speaker_at_agi2017/,nocortex,1488971678,,0,1
270,2017-3-8,2017,3,8,21,5y7owv,What field could you use machine learning in human anatomy to make individuals taller?,https://www.reddit.com/r/MachineLearning/comments/5y7owv/what_field_could_you_use_machine_learning_in/,[deleted],1488977150,[removed],0,1
271,2017-3-8,2017,3,8,23,5y86dm,"[D]Given a paragraph, nlp extract some keywords, and then return some relative pics according to these keywords, will that work?",https://www.reddit.com/r/MachineLearning/comments/5y86dm/dgiven_a_paragraph_nlp_extract_some_keywords_and/,lamborhino,1488983254,"There are more and more article writers in China(we call them""self-media""), when they writing, usually need some pictures to attach this article, but most of them are not expert in desiging, so they have to search pics online. So I'm wondering, we can use nlp to extract the keywords, and return the pics to the user, that can save us much time, improve the effectiveness. 

I don't know if there's someone is working on this, I heard Lifeifei are doing similar work?

So,guys what do you think? May I get your feedback?Thanks.",11,3
272,2017-3-8,2017,3,8,23,5y8a2e,[P] KittiSeg: A toolkit to perform semantic segmentation in tensorflow.,https://www.reddit.com/r/MachineLearning/comments/5y8a2e/p_kittiseg_a_toolkit_to_perform_semantic/,[deleted],1488984397,[deleted],0,1
273,2017-3-8,2017,3,8,23,5y8c5w,[P] KittiSeg: A toolkit to perform semantic segmentation in Tensorflow.,https://www.reddit.com/r/MachineLearning/comments/5y8c5w/p_kittiseg_a_toolkit_to_perform_semantic/,marvMind,1488985070,,2,9
274,2017-3-9,2017,3,9,0,5y8dlg,[P] reinforcement learning course,https://www.reddit.com/r/MachineLearning/comments/5y8dlg/p_reinforcement_learning_course/,sasha_p,1488985481,,0,47
275,2017-3-9,2017,3,9,0,5y8m2k,[D] Starting out,https://www.reddit.com/r/MachineLearning/comments/5y8m2k/d_starting_out/,[deleted],1488987935,[deleted],2,0
276,2017-3-9,2017,3,9,0,5y8mfu,Looking for advice.,https://www.reddit.com/r/MachineLearning/comments/5y8mfu/looking_for_advice/,ValidatingUsername,1488988035,[removed],0,1
277,2017-3-9,2017,3,9,0,5y8nmo,"Simple Questions Thread March 08, 2017",https://www.reddit.com/r/MachineLearning/comments/5y8nmo/simple_questions_thread_march_08_2017/,AutoModerator,1488988359,[removed],0,1
278,2017-3-9,2017,3,9,1,5y8unr,"We're North Side, creators of Bot Colony -- currently the only AI game where you can have real conversations with robots using our Natural Language Understanding tech. Ask Us Anything (xpost /r/games)",https://www.reddit.com/r/MachineLearning/comments/5y8unr/were_north_side_creators_of_bot_colony_currently/,Rethial,1488990298,,1,1
279,2017-3-9,2017,3,9,2,5y94bw,"""Machine Learning Predicts Laboratory Earthquakes"", Rouet-Leduc et al 2017",https://www.reddit.com/r/MachineLearning/comments/5y94bw/machine_learning_predicts_laboratory_earthquakes/,gwern,1488992810,,1,1
280,2017-3-9,2017,3,9,3,5y9ils,help implementing modified version of agglomerative hierarchical clustering?,https://www.reddit.com/r/MachineLearning/comments/5y9ils/help_implementing_modified_version_of/,ohhhhh_what_it_do,1488996608,[removed],0,1
281,2017-3-9,2017,3,9,3,5y9s1y,[P] Stanford CS224N Natural Language Processing with Deep Learning notes on GitHub,https://www.reddit.com/r/MachineLearning/comments/5y9s1y/p_stanford_cs224n_natural_language_processing/,pmigdal,1488999039,,3,92
282,2017-3-9,2017,3,9,4,5y9vt3,Welcome Kaggle to Google Cloud,https://www.reddit.com/r/MachineLearning/comments/5y9vt3/welcome_kaggle_to_google_cloud/,[deleted],1489000041,[deleted],0,1
283,2017-3-9,2017,3,9,4,5y9w59,[N] Fei-Fei Li: Welcome Kaggle to Google Cloud,https://www.reddit.com/r/MachineLearning/comments/5y9w59/n_feifei_li_welcome_kaggle_to_google_cloud/,fhoffa,1489000131,,30,53
284,2017-3-9,2017,3,9,4,5ya6eu,Any AI drawing programs out there?,https://www.reddit.com/r/MachineLearning/comments/5ya6eu/any_ai_drawing_programs_out_there/,Neuronologist,1489002850,[removed],0,1
285,2017-3-9,2017,3,9,5,5ya8up,"[N] Facebook unveils Big Basin, its next-generation GPU server, useful for machine learning",https://www.reddit.com/r/MachineLearning/comments/5ya8up/n_facebook_unveils_big_basin_its_nextgeneration/,tonylstewart,1489003518,,0,10
286,2017-3-9,2017,3,9,5,5yadxc,Testing AI concepts in user research - how to prototype and understand the problems that you should build ML solutions for,https://www.reddit.com/r/MachineLearning/comments/5yadxc/testing_ai_concepts_in_user_research_how_to/,chrizbo,1489004834,,0,1
287,2017-3-9,2017,3,9,5,5yaems,How to Do Style Transfer with Tensorflow (LIVE),https://www.reddit.com/r/MachineLearning/comments/5yaems/how_to_do_style_transfer_with_tensorflow_live/,funtwo2,1489005019,,0,1
288,2017-3-9,2017,3,9,6,5yawy4,End-to-end speech recognition in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/5yawy4/endtoend_speech_recognition_in_tensorflow/,lk05,1489010021,,0,1
289,2017-3-9,2017,3,9,7,5yb2rk,Googles new machine learning API recognises objects in videos,https://www.reddit.com/r/MachineLearning/comments/5yb2rk/googles_new_machine_learning_api_recognises/,lopespm,1489011508,,0,1
290,2017-3-9,2017,3,9,8,5ybfl9,[P] TensorFlow implementation of Christopher Moody's lda2vec,https://www.reddit.com/r/MachineLearning/comments/5ybfl9/p_tensorflow_implementation_of_christopher_moodys/,pmigdal,1489015086,,1,20
291,2017-3-9,2017,3,9,10,5ybztt,What are the differences between Batch Renormalization and Streaming Normalization in Neural Networks?,https://www.reddit.com/r/MachineLearning/comments/5ybztt/what_are_the_differences_between_batch/,[deleted],1489021221,[deleted],0,1
292,2017-3-9,2017,3,9,13,5ycy48,[N] Introducing Similarity Search at Flickr,https://www.reddit.com/r/MachineLearning/comments/5ycy48/n_introducing_similarity_search_at_flickr/,kluikens,1489032604,,8,126
293,2017-3-9,2017,3,9,14,5ydb5a,Its official: Kaggle Joins Google Cloud,https://www.reddit.com/r/MachineLearning/comments/5ydb5a/its_official_kaggle_joins_google_cloud/,mks_repi,1489037455,,0,1
294,2017-3-9,2017,3,9,14,5ydeuw,Adding noise to Mnist data,https://www.reddit.com/r/MachineLearning/comments/5ydeuw/adding_noise_to_mnist_data/,muneeb2405,1489038984,[removed],0,1
295,2017-3-9,2017,3,9,15,5ydn4h,An Early Look at Startup Graphcore's Deep Learning Chip,https://www.reddit.com/r/MachineLearning/comments/5ydn4h/an_early_look_at_startup_graphcores_deep_learning/,[deleted],1489042555,[deleted],0,1
296,2017-3-9,2017,3,9,16,5ydtkt,[D] Did Andrej Karpathy leave OpenAI?,https://www.reddit.com/r/MachineLearning/comments/5ydtkt/d_did_andrej_karpathy_leave_openai/,PM_YOUR_NIPS_PAPERS,1489045629,[removed],2,2
297,2017-3-9,2017,3,9,16,5yduu3,Is a factory overclocked GPU a good idea for ml ?,https://www.reddit.com/r/MachineLearning/comments/5yduu3/is_a_factory_overclocked_gpu_a_good_idea_for_ml/,buddhagonebad,1489046239,[removed],0,1
298,2017-3-9,2017,3,9,17,5ydxp2,DeepWarp: Photorealistic Image Resynthesis for Gaze Manipulation,https://www.reddit.com/r/MachineLearning/comments/5ydxp2/deepwarp_photorealistic_image_resynthesis_for/,dzyl,1489047686,,1,1
299,2017-3-9,2017,3,9,17,5ydxvm,"Karl Sims - Evolved Virtual Creatures, Evolution Simulation, 1994",https://www.reddit.com/r/MachineLearning/comments/5ydxvm/karl_sims_evolved_virtual_creatures_evolution/,Plazmaz1,1489047779,,0,1
300,2017-3-9,2017,3,9,21,5yeu15,Estimating nonlinear dynamics with convolutional networks,https://www.reddit.com/r/MachineLearning/comments/5yeu15/estimating_nonlinear_dynamics_with_convolutional/,DanielleMolloy,1489063457,,0,1
301,2017-3-9,2017,3,9,23,5yfc9l,Understanding Example from Mapper (TDA) Paper,https://www.reddit.com/r/MachineLearning/comments/5yfc9l/understanding_example_from_mapper_tda_paper/,datafunctor,1489069762,[removed],0,1
302,2017-3-9,2017,3,9,23,5yfd73,Teaching to a computer,https://www.reddit.com/r/MachineLearning/comments/5yfd73/teaching_to_a_computer/,marconotmarcos,1489070060,[removed],0,1
303,2017-3-9,2017,3,9,23,5yffkg,Best Espresso Machines for Home Review- Honest and Unbiased,https://www.reddit.com/r/MachineLearning/comments/5yffkg/best_espresso_machines_for_home_review_honest_and/,KaziMurad,1489070791,,0,1
304,2017-3-9,2017,3,9,23,5yfhhl,Simple Batch Normalization for Cudnn RNN,https://www.reddit.com/r/MachineLearning/comments/5yfhhl/simple_batch_normalization_for_cudnn_rnn/,trungnt13,1489071391,[removed],0,1
305,2017-3-10,2017,3,10,0,5yfjc3,[Discussion] End-To-End Memory Networks vs Seq2Seq with attention. When to use what?,https://www.reddit.com/r/MachineLearning/comments/5yfjc3/discussion_endtoend_memory_networks_vs_seq2seq/,[deleted],1489071934,[deleted],1,1
306,2017-3-10,2017,3,10,1,5yfwq6,Looking for a program that can draw whatever the user tells it to draw,https://www.reddit.com/r/MachineLearning/comments/5yfwq6/looking_for_a_program_that_can_draw_whatever_the/,Neuronologist,1489075782,[removed],0,1
307,2017-3-10,2017,3,10,1,5yfyb5,New Tutorial! Transfer Functions,https://www.reddit.com/r/MachineLearning/comments/5yfyb5/new_tutorial_transfer_functions/,mlnotebook,1489076209,[removed],0,1
308,2017-3-10,2017,3,10,2,5yg8ag,Stitch Fix Algorithms Tour,https://www.reddit.com/r/MachineLearning/comments/5yg8ag/stitch_fix_algorithms_tour/,chrisemoody,1489078939,,0,1
309,2017-3-10,2017,3,10,2,5ygcyy,[R] PGQ: Combining policy gradient and Q-learning [Deepmind],https://www.reddit.com/r/MachineLearning/comments/5ygcyy/r_pgq_combining_policy_gradient_and_qlearning/,[deleted],1489080355,[deleted],1,1
310,2017-3-10,2017,3,10,2,5ygd97,Classifying NBA Plays and Predicting Shots [pdf],https://www.reddit.com/r/MachineLearning/comments/5ygd97/classifying_nba_plays_and_predicting_shots_pdf/,DarnellDeRozan,1489080449,,0,1
311,2017-3-10,2017,3,10,2,5ygfst,Jeff Hawkins gave a talk in CSV17: Impact through Innovation,https://www.reddit.com/r/MachineLearning/comments/5ygfst/jeff_hawkins_gave_a_talk_in_csv17_impact_through/,nocortex,1489081126,,0,1
312,2017-3-10,2017,3,10,2,5ygh1q,[D] Data preprocessing tips for t-SNE,https://www.reddit.com/r/MachineLearning/comments/5ygh1q/d_data_preprocessing_tips_for_tsne/,brannondorsey,1489081440,"I'm experimenting with using t-SNE for the first time to do some unsupervised learning/clustering of some features I've extracted from ~5K MIDI files. In my first few attempts I got some pretty fantastic looking clusters and data segmentation using the raw output from my feature extraction (using [Music21s jSymbolic feature extractors](http://web.mit.edu/music21/doc/moduleReference/moduleFeaturesJSymbolic.html) for those that might be interested). All well and good.

I figured a good next step would be to do some feature-wise normalization of my dataset (scaling all values between 0.0 and 1.0) and see how t-SNE changes. The problem is that when I do this, and then visualize the normalized features with t-SNE, I see almost no structure in the data whatsoever. I've tried a few perplexity values now and running t-SNE on the normalized data produces almost no clusters and instead looks like noise. I've also run PCA on the normalized data and am seeing nothing but what looks to me like noise there as well.

Does t-SNE expect normalized data? If so does this mean that there is no inherent structure to my data (which is, for the most part unlabeled)? Why might the non-normalized data cluster so nicely?

Any and all thoughts on this are welcome, as I am looking to learn about t-SNE during through this experiment as well. I am happy to provide more details (data, images, etc...) if that would help.

Cheers!",15,37
313,2017-3-10,2017,3,10,5,5yhlpp,Google Research's new AudioSet features twice as many male (vs female) speech samples,https://www.reddit.com/r/MachineLearning/comments/5yhlpp/google_researchs_new_audioset_features_twice_as/,zbplot,1489091880,,0,3
314,2017-3-10,2017,3,10,6,5yhslv,Learning Machine Learning,https://www.reddit.com/r/MachineLearning/comments/5yhslv/learning_machine_learning/,nasaprokid,1489093762,,0,1
315,2017-3-10,2017,3,10,6,5yhw5j,[D] Machine Learning and Misinformation,https://www.reddit.com/r/MachineLearning/comments/5yhw5j/d_machine_learning_and_misinformation/,psoulos,1489094714,,15,82
316,2017-3-10,2017,3,10,6,5yhxb8,Machine Learning books,https://www.reddit.com/r/MachineLearning/comments/5yhxb8/machine_learning_books/,Itsrealrad,1489095041,[removed],0,1
317,2017-3-10,2017,3,10,7,5yi7le,TensorFlow Image Recognition on a Raspberry Pi,https://www.reddit.com/r/MachineLearning/comments/5yi7le/tensorflow_image_recognition_on_a_raspberry_pi/,Insightmegan,1489097949,,0,1
318,2017-3-10,2017,3,10,7,5yicwq,Want to work in ML after I graduate. What technical electives should I take?,https://www.reddit.com/r/MachineLearning/comments/5yicwq/want_to_work_in_ml_after_i_graduate_what/,AlphatoZeta,1489099467,[removed],0,1
319,2017-3-10,2017,3,10,8,5yihyk,Recommended papers for benchmarking classifiers,https://www.reddit.com/r/MachineLearning/comments/5yihyk/recommended_papers_for_benchmarking_classifiers/,[deleted],1489100978,[removed],0,1
320,2017-3-10,2017,3,10,8,5yik29,[Discussion] Rigour papers on the topic of benchmarking classifiers,https://www.reddit.com/r/MachineLearning/comments/5yik29/discussion_rigour_papers_on_the_topic_of/,kangaroo_in_diaper,1489101621,"Being motivated by the paper [On Comparing Classifiers: Pitfalls to Avoid and a Recommended Approach](https://link.springer.com/article/10.1023/A:1009752403260), I would like to ask for any rigour papers on the topic of benchmarking classifiers? Thanks.

",4,6
321,2017-3-10,2017,3,10,10,5yj6lb,Feedback for Udacity's Machine Learning Nanodegree program,https://www.reddit.com/r/MachineLearning/comments/5yj6lb/feedback_for_udacitys_machine_learning_nanodegree/,gaanduNEO,1489108739,[removed],0,1
322,2017-3-10,2017,3,10,10,5yj6mb,[N] Photo popularity prediction challenge,https://www.reddit.com/r/MachineLearning/comments/5yj6mb/n_photo_popularity_prediction_challenge/,floppybad,1489108749,,0,1
323,2017-3-10,2017,3,10,10,5yj8fs,Why do convolutional networks use only odd sized kernels/patches,https://www.reddit.com/r/MachineLearning/comments/5yj8fs/why_do_convolutional_networks_use_only_odd_sized/,Noctambulist,1489109399,[removed],0,1
324,2017-3-10,2017,3,10,10,5yjd0s,How can data points be enough to learn a function?,https://www.reddit.com/r/MachineLearning/comments/5yjd0s/how_can_data_points_be_enough_to_learn_a_function/,[deleted],1489110866,[removed],0,1
325,2017-3-10,2017,3,10,11,5yjfm5,Picking an optimizer for Style Transfer  Surprising results comparing BFGS with Adam [R],https://www.reddit.com/r/MachineLearning/comments/5yjfm5/picking_an_optimizer_for_style_transfer/,jeremyhoward,1489111726,,15,58
326,2017-3-10,2017,3,10,15,5yknb1,LibRec 2.0 has been released:A Java Library for Recommender Systems,https://www.reddit.com/r/MachineLearning/comments/5yknb1/librec_20_has_been_releaseda_java_library_for/,Liuxz,1489128158,,0,1
327,2017-3-10,2017,3,10,15,5ykpi0,Sub-par results when training any neural network library on Iris datasets,https://www.reddit.com/r/MachineLearning/comments/5ykpi0/subpar_results_when_training_any_neural_network/,ViralRiver,1489129151,[removed],1,1
328,2017-3-10,2017,3,10,16,5ykr4f,DeepMind just published a mind blowing paper: PathNet.,https://www.reddit.com/r/MachineLearning/comments/5ykr4f/deepmind_just_published_a_mind_blowing_paper/,mustafaihssan,1489129912,,0,1
329,2017-3-10,2017,3,10,16,5yksvc,[P] Adversarial infill - filling in flowers with adversarial networks,https://www.reddit.com/r/MachineLearning/comments/5yksvc/p_adversarial_infill_filling_in_flowers_with/,richardweiss,1489130725,,0,20
330,2017-3-10,2017,3,10,17,5yl222,Google just released deep learning course on udacity,https://www.reddit.com/r/MachineLearning/comments/5yl222/google_just_released_deep_learning_course_on/,[deleted],1489135515,[deleted],0,1
331,2017-3-10,2017,3,10,17,5yl235,[D] Any ideas on J.Hawkins' sensorimotor inference theory?,https://www.reddit.com/r/MachineLearning/comments/5yl235/d_any_ideas_on_jhawkins_sensorimotor_inference/,nocortex,1489135533,https://www.youtube.com/watch?v=etSNEYeC01Q,9,2
332,2017-3-10,2017,3,10,18,5yl41x,Is anyone attending to the conference Machine Learning Prague 2017?,https://www.reddit.com/r/MachineLearning/comments/5yl41x/is_anyone_attending_to_the_conference_machine/,superfunkycalifragis,1489136663,[removed],0,1
333,2017-3-10,2017,3,10,18,5yl8q7,Google is giving away the AI tool it uses to understand language for free,https://www.reddit.com/r/MachineLearning/comments/5yl8q7/google_is_giving_away_the_ai_tool_it_uses_to/,mks_repi,1489139194,,0,1
334,2017-3-10,2017,3,10,20,5ylm11,Maquinaria de segunda mano para arranque de viruta,https://www.reddit.com/r/MachineLearning/comments/5ylm11/maquinaria_de_segunda_mano_para_arranque_de_viruta/,Barriuso,1489146045,,0,1
335,2017-3-10,2017,3,10,21,5ylosh,Someone has deleted most of Schmidhubers Wikipedia article at the end of last year.,https://www.reddit.com/r/MachineLearning/comments/5ylosh/someone_has_deleted_most_of_schmidhubers/,DanielleMolloy,1489147353,,0,1
336,2017-3-10,2017,3,10,22,5ym62v,How to implement paper X using framework Y?,https://www.reddit.com/r/MachineLearning/comments/5ym62v/how_to_implement_paper_x_using_framework_y/,[deleted],1489154155,[removed],0,1
337,2017-3-11,2017,3,11,0,5ymib0,How is the Google acquisition of Kaggle seen by the data science world ?,https://www.reddit.com/r/MachineLearning/comments/5ymib0/how_is_the_google_acquisition_of_kaggle_seen_by/,[deleted],1489158121,[removed],0,1
338,2017-3-11,2017,3,11,0,5ymiyv,Nested fields for a better search experience - beyond faceted search,https://www.reddit.com/r/MachineLearning/comments/5ymiyv/nested_fields_for_a_better_search_experience/,grumpybusinesscat,1489158322,,0,1
339,2017-3-11,2017,3,11,0,5ymten,Tactics of Adversarial Attack on Deep Reinforcement Learning Agents,https://www.reddit.com/r/MachineLearning/comments/5ymten/tactics_of_adversarial_attack_on_deep/,[deleted],1489161419,[removed],0,1
340,2017-3-11,2017,3,11,1,5ymuk2,Downloading ImageNet 21k,https://www.reddit.com/r/MachineLearning/comments/5ymuk2/downloading_imagenet_21k/,mike_vr,1489161736,[removed],0,1
341,2017-3-11,2017,3,11,1,5ymy4x,[P] Tactics of Adversarial Attack on Deep Reinforcement Learning Agents,https://www.reddit.com/r/MachineLearning/comments/5ymy4x/p_tactics_of_adversarial_attack_on_deep/,yenchenlin1994,1489162704,,7,41
342,2017-3-11,2017,3,11,2,5ynewl,Poker Rule Induction Using machine learning,https://www.reddit.com/r/MachineLearning/comments/5ynewl/poker_rule_induction_using_machine_learning/,Ozzymandias1893,1489167234,[removed],0,1
343,2017-3-11,2017,3,11,2,5ynfqj,Stanford University undergraduate thesis - Technological Automation Survey,https://www.reddit.com/r/MachineLearning/comments/5ynfqj/stanford_university_undergraduate_thesis/,stanyew,1489167445,,0,1
344,2017-3-11,2017,3,11,2,5ynhlq,Bootstrapping predicted values,https://www.reddit.com/r/MachineLearning/comments/5ynhlq/bootstrapping_predicted_values/,learning_deep,1489167923,[removed],0,1
345,2017-3-11,2017,3,11,2,5ynk5y,How will quantum computing affect deep neural networks?,https://www.reddit.com/r/MachineLearning/comments/5ynk5y/how_will_quantum_computing_affect_deep_neural/,icodepoorly,1489168585,[removed],0,1
346,2017-3-11,2017,3,11,3,5ynmex,[P] A visual search engine based on Elasticsearch and Tensorflow,https://www.reddit.com/r/MachineLearning/comments/5ynmex/p_a_visual_search_engine_based_on_elasticsearch/,tuan3w,1489169151,,0,34
347,2017-3-11,2017,3,11,3,5ynxuo,Is this accurate: Data Engineer if you have a BS/MS; Data Scientist if you have a PhD,https://www.reddit.com/r/MachineLearning/comments/5ynxuo/is_this_accurate_data_engineer_if_you_have_a_bsms/,DSEUCSD,1489172238,[removed],0,1
348,2017-3-11,2017,3,11,4,5yo2rd,[P] New kind of recurrent neural network using attention evaluated on character prediction (a natural language problem),https://www.reddit.com/r/MachineLearning/comments/5yo2rd/p_new_kind_of_recurrent_neural_network_using/,jostmey,1489173553,,4,6
349,2017-3-11,2017,3,11,4,5yo30r,"[R] ""The Shattered Gradients Problem: If resnets are the answer, then what is the question?"", Balduzzi et al 2017 (successfully training 198-layer non-resnet feedforward NNs w/better initialization)",https://www.reddit.com/r/MachineLearning/comments/5yo30r/r_the_shattered_gradients_problem_if_resnets_are/,gwern,1489173626,,27,71
350,2017-3-11,2017,3,11,4,5yo62t,"Call for papers. DataScience Lab - IV conference on the practical use of Data Science and BigData. May 13, Odessa, Ukraine.",https://www.reddit.com/r/MachineLearning/comments/5yo62t/call_for_papers_datascience_lab_iv_conference_on/,flyelephant,1489174446,,0,1
351,2017-3-11,2017,3,11,5,5yof7x,[D] Scaling Deep Learning: Systems Challenges &amp; More with Shubho Sengupta (TWiML &amp; AI Podcast),https://www.reddit.com/r/MachineLearning/comments/5yof7x/d_scaling_deep_learning_systems_challenges_more/,sbc1906,1489176930,,0,7
352,2017-3-11,2017,3,11,6,5yotkv,[R] [1703.00955] Controllable Text Generation,https://www.reddit.com/r/MachineLearning/comments/5yotkv/r_170300955_controllable_text_generation/,Mandrathax,1489180859,,5,23
353,2017-3-11,2017,3,11,7,5yp7lm,"[N] Enabling open and reproducible computer systems research: the good, the bad and the ugly",https://www.reddit.com/r/MachineLearning/comments/5yp7lm/n_enabling_open_and_reproducible_computer_systems/,gtechmisc,1489184770,,0,0
354,2017-3-11,2017,3,11,7,5yp8k7,My classmate used a neural net to make a rapping robot. Take this survey to see if you can tell it apart from a human.,https://www.reddit.com/r/MachineLearning/comments/5yp8k7/my_classmate_used_a_neural_net_to_make_a_rapping/,[deleted],1489185054,[deleted],0,1
355,2017-3-11,2017,3,11,7,5ypby1,I'd like to do my Master's thesis at the intersection of machine learning and humanitarian issues. Suggestions?,https://www.reddit.com/r/MachineLearning/comments/5ypby1/id_like_to_do_my_masters_thesis_at_the/,chris2point0,1489186008,[removed],0,1
356,2017-3-11,2017,3,11,7,5ypcfv,Machine Learning Top 10 Articles,https://www.reddit.com/r/MachineLearning/comments/5ypcfv/machine_learning_top_10_articles/,[deleted],1489186153,[deleted],0,1
357,2017-3-11,2017,3,11,8,5ypjh4,Compressed Sensing using Generative Models,https://www.reddit.com/r/MachineLearning/comments/5ypjh4/compressed_sensing_using_generative_models/,[deleted],1489188341,[deleted],1,1
358,2017-3-11,2017,3,11,8,5ypjtk,[R] Compressed Sensing using Generative Models,https://www.reddit.com/r/MachineLearning/comments/5ypjtk/r_compressed_sensing_using_generative_models/,MathAndProgramming,1489188451,,11,26
359,2017-3-11,2017,3,11,9,5yptgw,"Have an interest in AI and want to put your skills to the test? Why not enter the ""Viking Doom"" competition?",https://www.reddit.com/r/MachineLearning/comments/5yptgw/have_an_interest_in_ai_and_want_to_put_your/,davinellulinvega,1489191638,[removed],0,1
360,2017-3-11,2017,3,11,9,5ypvrd,It takes more than 8 hours to train Facebooks fastText on WikiReading,https://www.reddit.com/r/MachineLearning/comments/5ypvrd/it_takes_more_than_8_hours_to_train_facebooks/,rochea,1489192429,,0,1
361,2017-3-11,2017,3,11,11,5yqcdi,[P] Mathpix: An API for converting images/scans of handwritten/printed math into LaTeX.,https://www.reddit.com/r/MachineLearning/comments/5yqcdi/p_mathpix_an_api_for_converting_imagesscans_of/,[deleted],1489198297,[deleted],18,244
362,2017-3-11,2017,3,11,11,5yqcj5,Best resources to get started?,https://www.reddit.com/r/MachineLearning/comments/5yqcj5/best_resources_to_get_started/,smartdanny,1489198361,[removed],0,1
363,2017-3-11,2017,3,11,11,5yqg05,brain.fm,https://www.reddit.com/r/MachineLearning/comments/5yqg05/brainfm/,jmoso13,1489199656,[removed],0,1
364,2017-3-11,2017,3,11,12,5yql81,How to Generate Music - Intro to Deep Learning #9 - YouTube,https://www.reddit.com/r/MachineLearning/comments/5yql81/how_to_generate_music_intro_to_deep_learning_9/,ackstazya,1489201715,,0,1
365,2017-3-11,2017,3,11,15,5yrbhg,Image clipping site that uses machine learning?,https://www.reddit.com/r/MachineLearning/comments/5yrbhg/image_clipping_site_that_uses_machine_learning/,breadteam,1489212313,[removed],0,1
366,2017-3-11,2017,3,11,16,5yri6h,I'm new. This is my favorite video so far. Who explains ML the best?,https://www.reddit.com/r/MachineLearning/comments/5yri6h/im_new_this_is_my_favorite_video_so_far_who/,xasopheno,1489215661,,0,1
367,2017-3-11,2017,3,11,20,5ys846,Tutorial for researchers to learn Facebook's PyTorch library,https://www.reddit.com/r/MachineLearning/comments/5ys846/tutorial_for_researchers_to_learn_facebooks/,[deleted],1489231155,[deleted],0,1
368,2017-3-11,2017,3,11,20,5ys8b2,[P] Tutorial for researchers to learn Facebook's PyTorch library,https://www.reddit.com/r/MachineLearning/comments/5ys8b2/p_tutorial_for_researchers_to_learn_facebooks/,yunjey,1489231267,,8,60
369,2017-3-11,2017,3,11,22,5ysono,[D] Suggestion by Salesforce chief data scientist,https://www.reddit.com/r/MachineLearning/comments/5ysono/d_suggestion_by_salesforce_chief_data_scientist/,Prooffread3r,1489239518,,107,601
370,2017-3-12,2017,3,12,1,5ytbz6,"Artificial Intelligence &amp; Machine Learning: Cybersecurity's Paradigm Shift, focus on the Government, Defense, and Healthcare sectors.",https://www.reddit.com/r/MachineLearning/comments/5ytbz6/artificial_intelligence_machine_learning/,cyber_hacker,1489248461,,0,1
371,2017-3-12,2017,3,12,1,5ytfra,How to download videos of Harvard's CS109 lectures?,https://www.reddit.com/r/MachineLearning/comments/5ytfra/how_to_download_videos_of_harvards_cs109_lectures/,ankscricholic,1489249722,[removed],0,1
372,2017-3-12,2017,3,12,1,5ytgh1,[N] How Drive.ai Is Mastering Autonomous Driving with Deep Learning,https://www.reddit.com/r/MachineLearning/comments/5ytgh1/n_how_driveai_is_mastering_autonomous_driving/,gwern,1489249946,,10,0
373,2017-3-12,2017,3,12,2,5yts32,What does it take work for great AI labs and profs like Fei Fei Li and the like?,https://www.reddit.com/r/MachineLearning/comments/5yts32/what_does_it_take_work_for_great_ai_labs_and/,[deleted],1489253728,[removed],0,1
374,2017-3-12,2017,3,12,3,5yu4lp,[D] Retraining a model,https://www.reddit.com/r/MachineLearning/comments/5yu4lp/d_retraining_a_model/,Cock-tail,1489257612,"Hi.

I am trying to implement the colorization of a B&amp;W image, using a Conv Net. I know how to train a model to do so, however I wonder if it could be possible to retrain and repurpose some existing model to somehow generate the colors. I don't want a model that already does this.

I retrained VGG16 to add style to an image. But here, I don't have the style image.

The input is the B&amp;W image, the output is the colorized version.",2,2
375,2017-3-12,2017,3,12,4,5yugr5,"[D] Subutai Ahmad humiliates DeepMind Alpha-Go, it can't play tic-tac-toe.",https://www.reddit.com/r/MachineLearning/comments/5yugr5/d_subutai_ahmad_humiliates_deepmind_alphago_it/,nocortex,1489261313,[removed],16,0
376,2017-3-12,2017,3,12,4,5yuj1s,Using a mix of datatypes for dataset?,https://www.reddit.com/r/MachineLearning/comments/5yuj1s/using_a_mix_of_datatypes_for_dataset/,reformedcuck,1489262007,[removed],0,1
377,2017-3-12,2017,3,12,5,5yukcl,Machine Learning with Scikit-Learn on the Cancer Dataset,https://www.reddit.com/r/MachineLearning/comments/5yukcl/machine_learning_with_scikitlearn_on_the_cancer/,cristivlad,1489262427,,0,1
378,2017-3-12,2017,3,12,5,5yutd3,Choosing algorithm for clustering,https://www.reddit.com/r/MachineLearning/comments/5yutd3/choosing_algorithm_for_clustering/,AtinSF,1489265313,[removed],0,1
379,2017-3-12,2017,3,12,6,5yv06o,[D] Dileep George is also humiliating DeepMind's atari demo.,https://www.reddit.com/r/MachineLearning/comments/5yv06o/d_dileep_george_is_also_humiliating_deepminds/,nocortex,1489267463,[removed],2,0
380,2017-3-12,2017,3,12,6,5yv2lh,[D] Best way to deal with punctuation for a text generation model?,https://www.reddit.com/r/MachineLearning/comments/5yv2lh/d_best_way_to_deal_with_punctuation_for_a_text/,iamiamwhoami,1489268221,"So I'm trying to train a text generation model on a twitter corpus using LSTM cells. The inputs to the network are word2vec embedded ngrams. The last layer of the network is a dense layer, where each node represents a single word in the corpus. I train the output layer with a one hot encoded vector that represents that last word in the ngram sequence. I trained the network using tri-grams (i.e. input two word predict the third word). After doing this I found the network almost always predicts the third word to be a ','. I'm guessing this is because commas are over represented in the corpus. 

My current approach is now to delete punctuation from corpus, but I want my generated tweets to be as realistic as possible. Is there a way to deal with this that doesn't involve completely removing punctuation?",7,1
381,2017-3-12,2017,3,12,7,5yv9mm,[1703.03129] Learning to Remember Rare Events,https://www.reddit.com/r/MachineLearning/comments/5yv9mm/170303129_learning_to_remember_rare_events/,cbeak,1489270471,,0,1
382,2017-3-12,2017,3,12,7,5yvaql,Stanford University undergraduate thesis - Technological Automation Survey,https://www.reddit.com/r/MachineLearning/comments/5yvaql/stanford_university_undergraduate_thesis/,stanyew,1489270841,,0,1
383,2017-3-12,2017,3,12,8,5yvs1e,[P] Backgammon Doubling Cube,https://www.reddit.com/r/MachineLearning/comments/5yvs1e/p_backgammon_doubling_cube/,[deleted],1489276527,[deleted],0,1
384,2017-3-12,2017,3,12,11,5ywilu,Everybody are invited to our carefully curated list of posts and discussions related to practical day to day work of a data scientist. Feel free to ask questions. Beginners welcome.,https://www.reddit.com/r/MachineLearning/comments/5ywilu/everybody_are_invited_to_our_carefully_curated/,szelvenskiy,1489286109,,0,1
385,2017-3-12,2017,3,12,11,5ywkit,Advice for undergrad,https://www.reddit.com/r/MachineLearning/comments/5ywkit/advice_for_undergrad/,holdenglass,1489286835,[removed],0,1
386,2017-3-12,2017,3,12,18,5yxvpm,Implementation of Google's Multi-Style-Transfer in Tensorflow,https://www.reddit.com/r/MachineLearning/comments/5yxvpm/implementation_of_googles_multistyletransfer_in/,[deleted],1489309434,[deleted],0,1
387,2017-3-12,2017,3,12,21,5yyg97,[P] Impletation of Google's fast multi style transfer in tensorflow,https://www.reddit.com/r/MachineLearning/comments/5yyg97/p_impletation_of_googles_fast_multi_style/,Kiheumi,1489321542,,3,61
388,2017-3-12,2017,3,12,22,5yyojl,Genetic CNN: CNN architecture exploration using Genetic Algorithm,https://www.reddit.com/r/MachineLearning/comments/5yyojl/genetic_cnn_cnn_architecture_exploration_using/,[deleted],1489325610,[deleted],0,1
389,2017-3-13,2017,3,13,0,5yz2ht,"Applying to the Masters in Machine Learning, Speech and language Technology at Cambridge",https://www.reddit.com/r/MachineLearning/comments/5yz2ht/applying_to_the_masters_in_machine_learning/,malaalam,1489331251,[removed],0,1
390,2017-3-13,2017,3,13,0,5yz2r9,Huge scale evolution of deep nets to produce high quality image classifiers,https://www.reddit.com/r/MachineLearning/comments/5yz2r9/huge_scale_evolution_of_deep_nets_to_produce_high/,megaduks,1489331348,,0,1
391,2017-3-13,2017,3,13,2,5yzrlk,"the promise of democratization of ML is interesting, also microsoft and others are talking about it, is it anywhere near in sight?",https://www.reddit.com/r/MachineLearning/comments/5yzrlk/the_promise_of_democratization_of_ml_is/,sunole123,1489339627,,0,1
392,2017-3-13,2017,3,13,3,5z0098,Action-Values are Zero in DQN implementation,https://www.reddit.com/r/MachineLearning/comments/5z0098/actionvalues_are_zero_in_dqn_implementation/,[deleted],1489342310,[removed],0,1
393,2017-3-13,2017,3,13,7,5z1ds4,Gradient Descent Upgrade,https://www.reddit.com/r/MachineLearning/comments/5z1ds4/gradient_descent_upgrade/,[deleted],1489357485,[removed],0,1
394,2017-3-13,2017,3,13,7,5z1gpc,"[Discussion] What is better for neural network based image retrieval: image captioning, image embedding or something else?",https://www.reddit.com/r/MachineLearning/comments/5z1gpc/discussion_what_is_better_for_neural_network/,thewhizz,1489358393,"I have read several papers on image captioning, object recognition and multimodal image embedding and I'm curious what you all believe is the best for content based image retrieval. Multi-modal embeddings like those seen in Fast-0-tag seem great for an expansive vocabulary but seem tied to single keywords. Image captioning -- whether whole image or something like DenseCap -- seems like more of a direct, natural language method. Do you see one becoming more prominent over the other for CBIR?",1,5
395,2017-3-13,2017,3,13,8,5z1usp,What's the best way to deal with one-class text classification problems?,https://www.reddit.com/r/MachineLearning/comments/5z1usp/whats_the_best_way_to_deal_with_oneclass_text/,lorenzofantunes,1489362975,[removed],0,1
396,2017-3-13,2017,3,13,9,5z249m,How realistic would this project be?,https://www.reddit.com/r/MachineLearning/comments/5z249m/how_realistic_would_this_project_be/,[deleted],1489366111,[removed],0,1
397,2017-3-13,2017,3,13,9,5z25de,A general-purpose encoder-decoder framework for Tensorflow,https://www.reddit.com/r/MachineLearning/comments/5z25de/a_generalpurpose_encoderdecoder_framework_for/,pogopuschel_,1489366470,,0,2
398,2017-3-13,2017,3,13,11,5z2gnz,[D] Machine learning summer school in Spain,https://www.reddit.com/r/MachineLearning/comments/5z2gnz/d_machine_learning_summer_school_in_spain/,PM_ME_YOUR_ML,1489370408,"I was searching the web if applications for the deep learning summer school have opened,when I found this deep learning school in Spain: http://grammars.grlmc.com/DeepLearn2017/Registration.php 

What is your opinion about speakers in this conference and do you think it is worth to go to it? I  saw some speakers from Nvidia and Microsoft but no huge names as in Canadian DL school",4,11
399,2017-3-13,2017,3,13,11,5z2jdn,"[R] ""Parallel Multiscale Autoregressive Density Estimation"", Reed et al 2017 (generating photorealistic 512px images with optimized PixelCNN)",https://www.reddit.com/r/MachineLearning/comments/5z2jdn/r_parallel_multiscale_autoregressive_density/,gwern,1489371389,,33,71
400,2017-3-13,2017,3,13,11,5z2lwv,Function Introduction of Environment - friendly Concrete Mixing Plant,https://www.reddit.com/r/MachineLearning/comments/5z2lwv/function_introduction_of_environment_friendly/,FlyerConcretePlant,1489372297,[removed],1,1
401,2017-3-13,2017,3,13,11,5z2p94,[P] TensorFlow made supersimple (re-submission),https://www.reddit.com/r/MachineLearning/comments/5z2p94/p_tensorflow_made_supersimple_resubmission/,jostmey,1489373444,,23,130
402,2017-3-13,2017,3,13,12,5z2wkk,Need guidance for starting a project involving image recognition,https://www.reddit.com/r/MachineLearning/comments/5z2wkk/need_guidance_for_starting_a_project_involving/,NeoDren,1489376261,[removed],0,1
403,2017-3-13,2017,3,13,14,5z3apj,[D] Accessibility of Basic Models to Non-Technicals,https://www.reddit.com/r/MachineLearning/comments/5z3apj/d_accessibility_of_basic_models_to_nontechnicals/,wildekans,1489381965,"Hello /r/machinelearning!

I'm doing some research on easily generated models by non-technical/statistical people. It would be awesome if some of you could answer a quick questionnaire:

If you're a machine learning developer/data scientist etc.:
a) Has your manager/product lead etc. ever insist that you build a model on a correlation you felt wasn't there?
b) Do you think if that people had a way to verify the lack of correlation through a naive model (random forest, svc, etc.) that it would have changed the situation? (Or, if you were able to show them the results)
c) Would you want this technology for yourself, or wish that your company would have access to it?

If you're a non-technical person (small business developer, student, non-tech entrepreneur, etc.):

a) Have you ever not pursued a potential machine learning/data solution or feature because you weren't willing to invest the resources to see if it was viable?
b) Would being able to verify correlations in your data (or lack thereof!) entice you to pursue possible machine learning solutions?
c) Even if your previous answers were no, would you be interested in having this technology?

Thanks in advance for all of the responses, I will personally read and respond to each one of you thoughtful enough to give me a response. Also, I hope this post will spark an interesting conversation about the barrier of entry to AI/machine learning.",6,2
404,2017-3-13,2017,3,13,14,5z3fl3,[R] Stopping GAN Violence: Generative Unadversarial Networks,https://www.reddit.com/r/MachineLearning/comments/5z3fl3/r_stopping_gan_violence_generative_unadversarial/,[deleted],1489384241,[deleted],1,1
405,2017-3-13,2017,3,13,16,5z3p40,How to work for great AI labs?,https://www.reddit.com/r/MachineLearning/comments/5z3p40/how_to_work_for_great_ai_labs/,nivm321,1489389104,[removed],0,1
406,2017-3-13,2017,3,13,17,5z3vfl,Several Questions on Neural Network Structure,https://www.reddit.com/r/MachineLearning/comments/5z3vfl/several_questions_on_neural_network_structure/,Pseudoabdul,1489392762,[removed],0,1
407,2017-3-13,2017,3,13,17,5z3zv6,[P] Doom Bots in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/5z3zv6/p_doom_bots_in_tensorflow/,[deleted],1489395541,[deleted],0,0
408,2017-3-13,2017,3,13,19,5z49fm,"4 ways Google Cloud will bring AI, machine learning to the enterprise",https://www.reddit.com/r/MachineLearning/comments/5z49fm/4_ways_google_cloud_will_bring_ai_machine/,adlersoo,1489400997,,0,1
409,2017-3-13,2017,3,13,19,5z49n3,"QT4-24 hollow block machine in philippines, block making machine in ghana",https://www.reddit.com/r/MachineLearning/comments/5z49n3/qt424_hollow_block_machine_in_philippines_block/,dymachine01,1489401105,,1,1
410,2017-3-13,2017,3,13,19,5z4asu,[N] A Gentle Guide to Machine Learning,https://www.reddit.com/r/MachineLearning/comments/5z4asu/n_a_gentle_guide_to_machine_learning/,rwieruch,1489401757,,0,36
411,2017-3-13,2017,3,13,20,5z4g6a,Clickbaits Revisited: Deep Learning on Title + Content Features to Tackle Clickbaits,https://www.reddit.com/r/MachineLearning/comments/5z4g6a/clickbaits_revisited_deep_learning_on_title/,abhisvnit,1489404413,,0,1
412,2017-3-13,2017,3,13,23,5z548j,Applying Machine Learning to March Madness predictions!,https://www.reddit.com/r/MachineLearning/comments/5z548j/applying_machine_learning_to_march_madness/,adeshpande3,1489413631,,0,1
413,2017-3-13,2017,3,13,23,5z582q,Supervised learning is just unsupervised learning dressed up,https://www.reddit.com/r/MachineLearning/comments/5z582q/supervised_learning_is_just_unsupervised_learning/,[deleted],1489414834,[removed],0,1
414,2017-3-13,2017,3,13,23,5z5ffg,[D] Help with Object Detector project (SVM + HOG),https://www.reddit.com/r/MachineLearning/comments/5z5ffg/d_help_with_object_detector_project_svm_hog/,gabegabe6,1489417083,"I want ot make an object detector with HOG features. This is where I am at currently: [GITHUB_REPO](https://github.com/gaborvecsei/Object-Detector-Hog)

So it is working now but it is really slow...it can make detections in 6-8 seconds in 800x800 images. I am using a sliding window approach.

What should I change for a faster detection?

1 more thing: which dataset do you recommend to try it on? I only tried with 25-25 images about pepsi and 7up logos. And I used the data generation (augmentation). I had 50-60 images. I also used negative data (100-150 images).",7,0
415,2017-3-14,2017,3,14,0,5z5hwg,[D] Which of the courses in Machine Learning have the best assignments and are available on the web for free?,https://www.reddit.com/r/MachineLearning/comments/5z5hwg/d_which_of_the_courses_in_machine_learning_have/,datavinci,1489417779,"By best I mean:-
1) Intermediate to advanced level rigor(no beginner stuff needed)
2) Cover wide variety of algorithms and have solutions available on the web.

If you know any such courses, please post the link below.
 Note:- My **focus** is on **assignments** rather than *video lectures* or *slides*. 
I want a **general** ML course **not specific** like deep learning.
                                                                                                 Also I have posted this question on other subreddits but have got so far only one satisfactory answer.
Thanks in advance!",9,13
416,2017-3-14,2017,3,14,0,5z5j2e,[Discussion] Every supervised learning model is just unsupervised learning in disguise,https://www.reddit.com/r/MachineLearning/comments/5z5j2e/discussion_every_supervised_learning_model_is/,joker2895,1489418101,"I read in Ian goodfellow and bengio's new book Deep Learning.In it presents the point that there is no real difference between unsupervised and supervised learning.
Since that is the case why can't there be one simple model for every possible dataset.
EVERY MODELS IS JUST FINDING STATISTICALLY SIGNIFICANT RELATIONSHIPS BETWEEN DATA.
I read about Free lunch theorem but how it relates to my question is unclear to me.",9,0
417,2017-3-14,2017,3,14,1,5z60z8,[P] Simple example of using deep neural network (TensorFlow) to play OpenAI's CartPole game,https://www.reddit.com/r/MachineLearning/comments/5z60z8/p_simple_example_of_using_deep_neural_network/,sentdex,1489423088,"In this tutorial, we use a multilayer perceptron model to learn how to play CartPole. For training data, we train on games where the agent simply takes random moves. While none of the training games come even close to solving CartPole, the resulting agent can and does. 

**[Using a neural network to solve OpenAI's CartPole balancing environment](https://pythonprogramming.net/openai-cartpole-neural-network-example-machine-learning-tutorial/)**

If you need tutorials on deep learning, TensorFlow, or TFLearn, the tutorial is part of a larger series that covers all of that.",0,2
418,2017-3-14,2017,3,14,1,5z648s,Cheat Sheet: All Facebook Messenger Bots Interactions,https://www.reddit.com/r/MachineLearning/comments/5z648s/cheat_sheet_all_facebook_messenger_bots/,bogsformer,1489423942,,0,2
419,2017-3-14,2017,3,14,2,5z6b6a,MSc Computer Science vs Statistical Science at Oxford if I want do Deep Learning and ML research,https://www.reddit.com/r/MachineLearning/comments/5z6b6a/msc_computer_science_vs_statistical_science_at/,Gnargy,1489425804,[removed],0,1
420,2017-3-14,2017,3,14,2,5z6bdn,Intel joins the self-driving game,https://www.reddit.com/r/MachineLearning/comments/5z6bdn/intel_joins_the_selfdriving_game/,GodIsBanana,1489425853,,0,1
421,2017-3-14,2017,3,14,2,5z6bpc,Shifting from pure math to Statistical machine learning,https://www.reddit.com/r/MachineLearning/comments/5z6bpc/shifting_from_pure_math_to_statistical_machine/,[deleted],1489425947,[removed],0,1
422,2017-3-14,2017,3,14,2,5z6ds1,An overview on the future of language translators,https://www.reddit.com/r/MachineLearning/comments/5z6ds1/an_overview_on_the_future_of_language_translators/,scvalencia,1489426493,,0,1
423,2017-3-14,2017,3,14,2,5z6gkz,[N] Intel buys self-driving car company MobilEye for $15.3 billion,https://www.reddit.com/r/MachineLearning/comments/5z6gkz/n_intel_buys_selfdriving_car_company_mobileye_for/,gwern,1489427234,,17,77
424,2017-3-14,2017,3,14,2,5z6iau,[D] Closer to the nature?,https://www.reddit.com/r/MachineLearning/comments/5z6iau/d_closer_to_the_nature/,PetarKing,1489427707,"We all know that neural networks are inspired by our (at least past) understanding of how our brain functions, but is far from its copy and what bothers me is are we trying to mimic the nature better, and if not, why so?  
  
I know that our understanding of neural networks in living organisms is limited, to say at least, but it seems to me that trying to make a model that is closer to some biological system might be of much significance.  
  
And, if we don't try to copy our nervous system, why don't we try with the octopus brain (I would like to see somebody try to do this) or brains of any other animals?  
  
P.S. I apologize for any potential uneducated claims or questions.",11,0
425,2017-3-14,2017,3,14,4,5z6ymn,Transfer Learning PyTorch Tutorial,https://www.reddit.com/r/MachineLearning/comments/5z6ymn/transfer_learning_pytorch_tutorial/,[deleted],1489431988,[deleted],0,1
426,2017-3-14,2017,3,14,4,5z72m9,[P] Transfer Learning PyTorch tutorial,https://www.reddit.com/r/MachineLearning/comments/5z72m9/p_transfer_learning_pytorch_tutorial/,saucysassy,1489433018,,5,8
427,2017-3-14,2017,3,14,4,5z77ej,[D] Adding New Features w/ No Historical Data in NN (Time-Series),https://www.reddit.com/r/MachineLearning/comments/5z77ej/d_adding_new_features_w_no_historical_data_in_nn/,zacheism,1489434245,"I'm still relatively new to working with neural networks and I was wondering what the best practice is for adding in features which you don't have historical data for (when you do have it for all other features) in a time-series model.

Specifically:

As a learning exercise, I'm creating a neural network (basic for now, recurrent in the future) which predicts the price of a stock / index (original, I know). I can get historic data for most things except Twitter, which I've found to be very difficult to get historic data (past what the API allows) at scale (unless you're willing to spend +$10k). 

With that said, how would adding in the twitter data affect things if it starts a year or two after all the other data starts? Is this less of an issue in a recurrent model?",0,0
428,2017-3-14,2017,3,14,5,5z7jst,[News] Free Webinar: Julia - a fresh approach to numerical computing and data science,https://www.reddit.com/r/MachineLearning/comments/5z7jst/news_free_webinar_julia_a_fresh_approach_to/,flyelephant,1489437409,,2,1
429,2017-3-14,2017,3,14,6,5z7ufc,How unsuitable is GAN research for ICCV?,https://www.reddit.com/r/MachineLearning/comments/5z7ufc/how_unsuitable_is_gan_research_for_iccv/,[deleted],1489440105,[removed],0,1
430,2017-3-14,2017,3,14,6,5z7xz5,[D] How unsuitable is GAN research for ICCV?,https://www.reddit.com/r/MachineLearning/comments/5z7xz5/d_how_unsuitable_is_gan_research_for_iccv/,BabyYann,1489441068,ICCV appears to be more focused on traditional computer vision tasks and as such it seems like it would be better to submit a GAN related paper to a different conference.  ,2,0
431,2017-3-14,2017,3,14,6,5z80dq,[BotOrNot] Can you tell if you are talking to a bot? Chat with bots on trending topics at BotOrNot!,https://www.reddit.com/r/MachineLearning/comments/5z80dq/botornot_can_you_tell_if_you_are_talking_to_a_bot/,tugan0329,1489441718,,0,1
432,2017-3-14,2017,3,14,6,5z8110,[D] A Super Harsh Guide to Machine Learning,https://www.reddit.com/r/MachineLearning/comments/5z8110/d_a_super_harsh_guide_to_machine_learning/,thatguydr,1489441878,"First, read fucking Hastie, Tibshirani, and whoever. Chapters 1-4 and 7. If you don't understand it, keep reading it until you do. 

You can read the rest of the book if you want. You probably should, but I'll assume you know all of it. 

Take Andrew Ng's Coursera. Do all the exercises in Matlab and python and R. Make sure you get the same answers with all of them. 

Now forget all of that and read the deep learning book. Put tensorflow or torch on a Linux box and run examples until you get it. Do stuff with CNNs and RNNs and just feed forward NNs.

Once you do all of that, go on arXiv and read the most recent useful papers. The literature changes every few months, so keep up. 

There. Now you can probably be hired most places. If you need resume filler, so some Kaggle competitions. If you have debugging questions, use StackOverflow. If you have math questions, read more. If you have life questions, I have no idea.",227,1092
433,2017-3-14,2017,3,14,7,5z83fv,[Discussion] What are some ways to handle less frequent words while using deep learning for language modelling or seq2seq?,https://www.reddit.com/r/MachineLearning/comments/5z83fv/discussion_what_are_some_ways_to_handle_less/,cvikasreddy,1489442576,"I am using &lt;unk&gt; token for all less frequent and oov words and I want to handle all the words.

I was thinking of using a CNN or lstm on char embeddings of a word and get the word embedding.",10,3
434,2017-3-14,2017,3,14,7,5z87ln,"[R] Batch Normalization for Improved DNN Performance, My Ass (preprint, submitted to SIGBOVIK '17)",https://www.reddit.com/r/MachineLearning/comments/5z87ln/r_batch_normalization_for_improved_dnn/,atomicthumbs,1489443736,,16,13
435,2017-3-14,2017,3,14,8,5z8ks5,[D] Benchmarks for TensorFlow RNN Decoding,https://www.reddit.com/r/MachineLearning/comments/5z8ks5/d_benchmarks_for_tensorflow_rnn_decoding/,throwaway1849430,1489447464,"My experience has been:

* ~1 second or slightly less per inference step on CPU on a larger model (100M params) with 512d hidden size

* ~100ms per inference step on a small model (10M params) with 128d hidden size


Both models use 100K vocab. GPU decoding is certainly faster, but not orders of magnitude better.
Are these times reasonable? When I see TF Serving boasts +50K per second requests on some models, it makes me think there is something seriously flawed with my setup. How is it possible to get speeds that quick at inference?

I am not using TF Serving or their threaded data queuing mechanism, but I have tried freezing (convert variables to constants) and exporting the graph, as well as loading multiple models in parallel. Nothing has provided a substantial improvement. I am hopeful that upgrading to TF 1.0.0 and using XLA will improve the performance.

**Can anyone contribute their own benchmarks? It doesn't matter how vague or anecdotal, I am just trying to get a frame of reference.**


",3,2
436,2017-3-14,2017,3,14,9,5z90no,Help with decoder for sequence auto-encoder (Keras),https://www.reddit.com/r/MachineLearning/comments/5z90no/help_with_decoder_for_sequence_autoencoder_keras/,[deleted],1489452266,[removed],0,1
437,2017-3-14,2017,3,14,9,5z92i2,"[P] Training a machine to detect if either a single song is playing, or multiple songs are playing at the same time",https://www.reddit.com/r/MachineLearning/comments/5z92i2/p_training_a_machine_to_detect_if_either_a_single/,shmed,1489452847,"Hello everyone,
As the title suggest, I'm working on a project where I am trying to train a machine to detect if an audio segment is just one single song, or if it's actually two song playing simultaneously. 

Right now, I am using the technique that reddit user u/bennane suggested in a [previous post](https://www.reddit.com/r/MachineLearning/comments/5livzv/projectquestion_what_is_a_common_set_of_features/dbwg543/) (more details here http://www.ppgia.pucpr.br/ismir2013/wp-content/uploads/2013/09/69_Paper.pdf).

For additional information: I use 10 second segments for each sample, down sampled to about 4400hz, and then cut into pieces of a couple second long each. Each piece is then described using the technique I mentioned above.

I have very limited experience in the field, and right now I'm using the feature extraction method from the paper (single scale instead of multiscale), but I am not using it to train a neural network. Instead, I've done some experiment using a linear SVM, a random forest classifier and a gradient boosted classifier. I've had acceptable results so far (getting about 0.80-0.85 global AUC in my experiments with a 30 000 sample dataset). This is mostly because those seemed to be easier to use, and I wanted to make the project as simple as possible to start with and gradually add complexity as I get more experience in the field. 

* Now, my first question is: Is there any major advantage on using a neural network over a simpler linear SVM or gradient boosted classifier for my kind of problem? I understand that training a 1 layer neural network can easily be paralyzed which speeds up the process significantly when dealing with a large number of samples, but is this the only advantage? 

* Second question, given that the training data is labeled for a classifier (2 classes: 1 song vs multiple song), is there a technique that can be used to somehow train a logistic repressor instead? As in, the training values are either 0 or 1, but when doing a prediction, the machine will actually return a score between 0 and 1 (a decimal value) which represent its position between the two. For example, if I put a sample that is actually 2 songs (but sound almost like 1 single song), the machine would return a value that is close to 1, but not exactly (maybe 0.8 or something). I tried using the ""probability"" value returned when using the prediction function on the machine (which, from my understanding, return a confidence score for each class), but I'm not sure it makes sense to use those values that way. I also tried playing with the decision function (in the case of a linear SVM, the distance from the hyperplane), but again, even though doing so makes sense in my head, it might be completely wrong to do this.

* Third question: right now, all my segments are exactly the same duration. What if I want to use different  size segments? Is it fine to simply pad the feature vector with zeros for the time slices that are missing?


Thank you very much! 

",1,3
438,2017-3-14,2017,3,14,10,5z94g7,Help with decoder for sequence auto-encoder (Keras),https://www.reddit.com/r/MachineLearning/comments/5z94g7/help_with_decoder_for_sequence_autoencoder_keras/,butWhoWasBee,1489453506,[removed],0,1
439,2017-3-14,2017,3,14,11,5z9iqs,[D] Which A3C implementation on github is the best?,https://www.reddit.com/r/MachineLearning/comments/5z9iqs/d_which_a3c_implementation_on_github_is_the_best/,evc123,1489458121,Which A3C (Asynchronous Advantage Actor-Critic) implementation is the most stable and converges to the highest average reward the quickest? Implementations of robuster alternatives are appreciated too.,4,7
440,2017-3-14,2017,3,14,11,5z9li0,[P] Non-Artistic Style Transfer (or How to Draw Kanye using Captain Picards Face),https://www.reddit.com/r/MachineLearning/comments/5z9li0/p_nonartistic_style_transfer_or_how_to_draw_kanye/,Bckenstler,1489458997,,6,12
441,2017-3-14,2017,3,14,12,5z9rnf,Happy Pi Day everybody with Neural Net and Monte Carlo Pi.31415 calculation,https://www.reddit.com/r/MachineLearning/comments/5z9rnf/happy_pi_day_everybody_with_neural_net_and_monte/,[deleted],1489461070,[deleted],0,1
442,2017-3-14,2017,3,14,12,5z9ul3,[R] Multiscale Hierarchical Convolutional Networks (Stphane Mallat),https://www.reddit.com/r/MachineLearning/comments/5z9ul3/r_multiscale_hierarchical_convolutional_networks/,xternalz,1489462105,,1,6
443,2017-3-14,2017,3,14,12,5z9w53,"[P] Deep XOR: Modelling an XOR gate with a feedforward network using Numpy, step-by-step",https://www.reddit.com/r/MachineLearning/comments/5z9w53/p_deep_xor_modelling_an_xor_gate_with_a/,urinieto,1489462662,,0,0
444,2017-3-14,2017,3,14,12,5z9xho,Happy Pi Day! using TensorFlow to calculate 3.1415,https://www.reddit.com/r/MachineLearning/comments/5z9xho/happy_pi_day_using_tensorflow_to_calculate_31415/,[deleted],1489463155,[deleted],0,1
445,2017-3-14,2017,3,14,12,5z9z65,Happy Pi day! Using custom Tensorflow codes to calculate 3.1415,https://www.reddit.com/r/MachineLearning/comments/5z9z65/happy_pi_day_using_custom_tensorflow_codes_to/,[deleted],1489463776,[deleted],0,1
446,2017-3-14,2017,3,14,13,5za12l,Happy Pi day! Using custom Tensorflow codes to calculate 3.1415,https://www.reddit.com/r/MachineLearning/comments/5za12l/happy_pi_day_using_custom_tensorflow_codes_to/,kbhit,1489464467,,1,1
447,2017-3-14,2017,3,14,14,5zafk2,PHP machine learning library,https://www.reddit.com/r/MachineLearning/comments/5zafk2/php_machine_learning_library/,[deleted],1489470397,,0,1
448,2017-3-14,2017,3,14,15,5zamg0,Why does the resins stainless continuous mixers is durable?,https://www.reddit.com/r/MachineLearning/comments/5zamg0/why_does_the_resins_stainless_continuous_mixers/,mixmachinery,1489473609,,1,1
449,2017-3-14,2017,3,14,17,5zb1d4,Sv Dolum Makinas,https://www.reddit.com/r/MachineLearning/comments/5zb1d4/sv_dolum_makinas/,renasmakinatr,1489481432,[removed],0,1
450,2017-3-14,2017,3,14,17,5zb1yo,Negative Results @CVPR17,https://www.reddit.com/r/MachineLearning/comments/5zb1yo/negative_results_cvpr17/,[deleted],1489481748,[deleted],0,1
451,2017-3-14,2017,3,14,17,5zb22w,I built an AI powered desktop robot that responds entirely through GIFs (x-post/r/linux),https://www.reddit.com/r/MachineLearning/comments/5zb22w/i_built_an_ai_powered_desktop_robot_that_responds/,[deleted],1489481823,[deleted],1,2
452,2017-3-14,2017,3,14,18,5zb53h,Ideal questions to ask a Machine Learning Manager?,https://www.reddit.com/r/MachineLearning/comments/5zb53h/ideal_questions_to_ask_a_machine_learning_manager/,spoiltForChoice,1489483445,[removed],0,1
453,2017-3-14,2017,3,14,18,5zb60p,NanoNets : How to use Deep Learning when you have Limited Data,https://www.reddit.com/r/MachineLearning/comments/5zb60p/nanonets_how_to_use_deep_learning_when_you_have/,mustafaihssan,1489483924,,0,1
454,2017-3-14,2017,3,14,19,5zbap7,[R] [1703.03864] Evolution Strategies as a Scalable Alternative to Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/5zbap7/r_170303864_evolution_strategies_as_a_scalable/,hardmaru,1489486300,,38,56
455,2017-3-14,2017,3,14,19,5zbf6m,[N] Infographic: A Beginners Guide to Machine Learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/5zbf6m/n_infographic_a_beginners_guide_to_machine/,rwieruch,1489488551,,0,1
456,2017-3-14,2017,3,14,20,5zboqz,[R] Skip Connections as Effective Symmetry-Breaking,https://www.reddit.com/r/MachineLearning/comments/5zboqz/r_skip_connections_as_effective_symmetrybreaking/,larseidnes,1489492683,,3,17
457,2017-3-14,2017,3,14,21,5zbvr6,"Continually updated Data Science Notebooks: Deep learning (TensorFlow, Theano, Caffe, Keras), Spark, Hadoop MapReduce, Kaggle, scikit-learn, matplotlib, pandas, NumPy, AWS, Python essentials, and various command lines",https://www.reddit.com/r/MachineLearning/comments/5zbvr6/continually_updated_data_science_notebooks_deep/,[deleted],1489495412,[deleted],0,1
458,2017-3-14,2017,3,14,21,5zby49,Udacity Deep Learning Foundation?,https://www.reddit.com/r/MachineLearning/comments/5zby49/udacity_deep_learning_foundation/,RauchyBear,1489496267,[removed],0,1
459,2017-3-14,2017,3,14,22,5zbypf,Detecting fraud with machine learning? Project?,https://www.reddit.com/r/MachineLearning/comments/5zbypf/detecting_fraud_with_machine_learning_project/,python_J,1489496475,[removed],0,1
460,2017-3-14,2017,3,14,22,5zc1kf,Applying dropout during testing image classification in Tensorflow,https://www.reddit.com/r/MachineLearning/comments/5zc1kf/applying_dropout_during_testing_image/,umairalipathan,1489497446,[removed],0,1
461,2017-3-14,2017,3,14,22,5zc4i7,March Madness Contests (x-post r/datascience),https://www.reddit.com/r/MachineLearning/comments/5zc4i7/march_madness_contests_xpost_rdatascience/,quant17898,1489498415,,0,1
462,2017-3-14,2017,3,14,22,5zc53g,Where to find tensorflow pretrained models?,https://www.reddit.com/r/MachineLearning/comments/5zc53g/where_to_find_tensorflow_pretrained_models/,illiterate_gorillas,1489498612,[removed],1,1
463,2017-3-14,2017,3,14,23,5zceul,Does Out-of-Sample (the Nystrm extension) works for Laplacian Eigenmap,https://www.reddit.com/r/MachineLearning/comments/5zceul/does_outofsample_the_nystrm_extension_works_for/,enigmatracker,1489501657,[removed],0,1
464,2017-3-14,2017,3,14,23,5zcfab,What ML algorithm could I best use to predict outcomes based on properties?,https://www.reddit.com/r/MachineLearning/comments/5zcfab/what_ml_algorithm_could_i_best_use_to_predict/,Alvittawa,1489501786,[removed],0,1
465,2017-3-14,2017,3,14,23,5zciuo,[R] Negative Results @CVPR17,https://www.reddit.com/r/MachineLearning/comments/5zciuo/r_negative_results_cvpr17/,siddharth-agrawal,1489502863,,5,58
466,2017-3-15,2017,3,15,0,5zco5u,Machine Learning Prague 2017 conferences,https://www.reddit.com/r/MachineLearning/comments/5zco5u/machine_learning_prague_2017_conferences/,superfunkycalifragis,1489504350,[removed],0,1
467,2017-3-15,2017,3,15,1,5zcyv5,"Future of Data Summit Tackles AI, Machine Learning - InformationWeek",https://www.reddit.com/r/MachineLearning/comments/5zcyv5/future_of_data_summit_tackles_ai_machine_learning/,chlordane2501,1489507296,,0,1
468,2017-3-15,2017,3,15,1,5zd32k,Introducing Keras 2,https://www.reddit.com/r/MachineLearning/comments/5zd32k/introducing_keras_2/,[deleted],1489508387,[deleted],0,1
469,2017-3-15,2017,3,15,1,5zd3ju,[N] Introducing Keras 2,https://www.reddit.com/r/MachineLearning/comments/5zd3ju/n_introducing_keras_2/,m_ke,1489508507,,74,173
470,2017-3-15,2017,3,15,1,5zd4c0,[D] Survey: What's the most stable regiment for training a GAN?,https://www.reddit.com/r/MachineLearning/comments/5zd4c0/d_survey_whats_the_most_stable_regiment_for/,feedthecreed,1489508725,"There's been a lot of papers out there now about stabilizing GANs:

[Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498)

[Wasserstein GAN](https://arxiv.org/abs/1701.07875)

[Least Squares Generative Adversarial Networks](https://arxiv.org/abs/1611.04076v2)

For me, most of the improvements seem to be from weight clipping the discriminator. Otherwise, I haven't found any of the methods which perform significantly better than the original GAN formulation.

What techniques have you found to be the most stable?",31,14
471,2017-3-15,2017,3,15,1,5zd5z8,I built an AI powered desktop robot that responds entirely through GIFs (x-post/r/linux),https://www.reddit.com/r/MachineLearning/comments/5zd5z8/i_built_an_ai_powered_desktop_robot_that_responds/,[deleted],1489509162,[deleted],0,1
472,2017-3-15,2017,3,15,1,5zda7i,"AI ""Stop Button"" Problem - Computerphile",https://www.reddit.com/r/MachineLearning/comments/5zda7i/ai_stop_button_problem_computerphile/,Neuronologist,1489510240,,0,1
473,2017-3-15,2017,3,15,3,5zdxrz,[p]Random-Walk Bayesian Deep Networks: Dealing with Non-Stationary Data,https://www.reddit.com/r/MachineLearning/comments/5zdxrz/prandomwalk_bayesian_deep_networks_dealing_with/,singularvalue,1489516257,,3,28
474,2017-3-15,2017,3,15,3,5ze076,ICLR 2017 for MSc students,https://www.reddit.com/r/MachineLearning/comments/5ze076/iclr_2017_for_msc_students/,BastiatF,1489516877,[removed],0,1
475,2017-3-15,2017,3,15,4,5ze9na,Understanding Statistical Power and Significance Testing [Visually],https://www.reddit.com/r/MachineLearning/comments/5ze9na/understanding_statistical_power_and_significance/,pete0273,1489519269,,0,1
476,2017-3-15,2017,3,15,4,5zebmn,Does RELU acutally solve the vanishing gradient problem,https://www.reddit.com/r/MachineLearning/comments/5zebmn/does_relu_acutally_solve_the_vanishing_gradient/,mucle6,1489519767,[removed],0,1
477,2017-3-15,2017,3,15,4,5zedjj,[D] What ML algorithm could I best use to predict outcomes based on properties?,https://www.reddit.com/r/MachineLearning/comments/5zedjj/d_what_ml_algorithm_could_i_best_use_to_predict/,Alvittawa,1489520285,"Say I have a dataset of travellers passing through an airport, and I know wether they were wearing a hat, if they had a suitcase and if they were male. What could I best use to predict wether someone is male based on this dataset?",7,0
478,2017-3-15,2017,3,15,5,5zek84,"Machine Learning in Healthcare Cybersecurity, from ""How to Crush the Health Sectors Ransomware Pandemic""",https://www.reddit.com/r/MachineLearning/comments/5zek84/machine_learning_in_healthcare_cybersecurity_from/,[deleted],1489522071,[deleted],0,1
479,2017-3-15,2017,3,15,5,5zemtl,[P] Tutorial: Deep Learning for NLP in Pytorch,https://www.reddit.com/r/MachineLearning/comments/5zemtl/p_tutorial_deep_learning_for_nlp_in_pytorch/,gt762,1489522741,,1,35
480,2017-3-15,2017,3,15,5,5zesfw,"[D] What are biological analogues of backpropagation? (Or, if none, what is the hierarchical learning mechanism.)",https://www.reddit.com/r/MachineLearning/comments/5zesfw/d_what_are_biological_analogues_of/,pmigdal,1489524200,"What are biological analogues of backpropagation? 
Or, if none, what is the hierarchical learning mechanism?

Related:

* [The concept of backpropagation in neural networks actually occurs in the brain?](http://biology.stackexchange.com/q/54147/129) - Biology Stack Exchange
* [What is meant by back propagation in an ANN compared to a biological neural network?](https://www.quora.com/What-is-meant-by-back-propagation-in-an-ANN-compared-to-a-biological-neural-network) - Quora",17,8
481,2017-3-15,2017,3,15,5,5zetab,[P] Dontprint  send scientific articles to your e-reader,https://www.reddit.com/r/MachineLearning/comments/5zetab/p_dontprint_send_scientific_articles_to_your/,pmigdal,1489524424,,1,2
482,2017-3-15,2017,3,15,5,5zevhg,[D] what is the easiest way to compute the probability of a given sentence? is there any good out of the box tool for this?,https://www.reddit.com/r/MachineLearning/comments/5zevhg/d_what_is_the_easiest_way_to_compute_the/,sentenceModeller,1489524995,,5,0
483,2017-3-15,2017,3,15,6,5zf1ga,Predicting Yelp Stars from Reviews with scikit-learn and Python,https://www.reddit.com/r/MachineLearning/comments/5zf1ga/predicting_yelp_stars_from_reviews_with/,kylebythemile,1489526602,,0,1
484,2017-3-15,2017,3,15,6,5zf214,[D] So I came up with a solution for citing ML algorithms for original work. What do you think?,https://www.reddit.com/r/MachineLearning/comments/5zf214/d_so_i_came_up_with_a_solution_for_citing_ml/,dankmemerino147,1489526755,"My proposal is to cite the ""Algorithm of sampleuser"" for the work. What do you think of it, and if you disagree, what do you think should be in its place?",5,0
485,2017-3-15,2017,3,15,7,5zfl97,RSS news powered by machine learning,https://www.reddit.com/r/MachineLearning/comments/5zfl97/rss_news_powered_by_machine_learning/,luminoumen,1489532129,[removed],0,1
486,2017-3-15,2017,3,15,8,5zfwsb,STDP-Compatible Approximation of Backpropagation in an Energy-Based Model,https://www.reddit.com/r/MachineLearning/comments/5zfwsb/stdpcompatible_approximation_of_backpropagation/,[deleted],1489535439,[deleted],0,1
487,2017-3-15,2017,3,15,8,5zfxcd,[R] STDP-Compatible Approximation of Backpropagation in an Energy-Based Model,https://www.reddit.com/r/MachineLearning/comments/5zfxcd/r_stdpcompatible_approximation_of_backpropagation/,harmonium1,1489535601,,1,11
488,2017-3-15,2017,3,15,9,5zg65f,[P] Quickly browse and filter today's arXiv papers,https://www.reddit.com/r/MachineLearning/comments/5zg65f/p_quickly_browse_and_filter_todays_arxiv_papers/,lobalproject,1489538219,,4,13
489,2017-3-15,2017,3,15,10,5zgbyy,[D] What is the job interview process like at OpenAI?,https://www.reddit.com/r/MachineLearning/comments/5zgbyy/d_what_is_the_job_interview_process_like_at_openai/,zergylord,1489540008,"Just applied for a full-time position at OpenAI (Special Projects) and was turned down without so much an a phone screen. Previously had the same experience applying for an internship. Just wanted see what experiences others have had when applying. Is there a standard 2 phone interviews and an onsite, or do they have a unique system given their relatively small size? Particularly rigorous interviews, or it is largely based on publication record?",43,119
490,2017-3-15,2017,3,15,10,5zgkoq,Who is that Neural Network? A friend of mine tried to apply machine learning to the Whos that Pokmon? challenge,https://www.reddit.com/r/MachineLearning/comments/5zgkoq/who_is_that_neural_network_a_friend_of_mine_tried/,[deleted],1489542728,[deleted],0,1
491,2017-3-15,2017,3,15,11,5zgmq9,[P] Who is that Neural Network? A friend of mine tried to apply machine learning to the Whos that Pokmon? challenge,https://www.reddit.com/r/MachineLearning/comments/5zgmq9/p_who_is_that_neural_network_a_friend_of_mine/,nokz88,1489543360,,18,93
492,2017-3-15,2017,3,15,11,5zgphu,[D] Has anyone else managed to get a CBPDN algorithm to work for image recognition?,https://www.reddit.com/r/MachineLearning/comments/5zgphu/d_has_anyone_else_managed_to_get_a_cbpdn/,a_slay_nub,1489544227,"So I've done a fair amount of work on the CBPDN SPORCO algorithm by the Los Alamos Laboratory to get it to classify targets in images. I've managed to get a fairly robust system. However, I'm having problems with targets that are even slightly out of scale or if there's features that are very dominant. 

For my project, I'm developing a perimeter detection/classification system to identify drones in a restricted area while ignoring other objects such as birds. This is solely a simulated projects so I can control all the parameters to improve performance. However, I would prefer not to to preserve the authenticity of the project. Currently, I have completed the detection algorithm and am sending a zoomed in shot of where the target is with the same parameters as my dictionary. The dictionary was created via taking shots at 15 units away. The only processing done to the dictionary is to take a highpass filter of the images. I would consider using other machine learning techniques, however we only have 14 weeks to complete this project and CBPDN is the only algorithm I am familiar with. Plus I'd like to meet people who work with algorithms that aren't neural nets.

New to this subreddit and haven't been able to find much help on it. Most of what I can find on the topic has to do with music transcription which hasn't proven to be very helpful. Fairly new to the subject so I am by no means an expert. ",3,2
493,2017-3-15,2017,3,15,12,5zh37u,13 frameworks for mastering machine learning,https://www.reddit.com/r/MachineLearning/comments/5zh37u/13_frameworks_for_mastering_machine_learning/,vvvenkatesh748,1489548946,,0,1
494,2017-3-15,2017,3,15,12,5zh5fg,[R] Enabling Continual Learning in Neural Networks,https://www.reddit.com/r/MachineLearning/comments/5zh5fg/r_enabling_continual_learning_in_neural_networks/,clbam8,1489549755,,5,48
495,2017-3-15,2017,3,15,17,5zi5zp,Recommendation Engine,https://www.reddit.com/r/MachineLearning/comments/5zi5zp/recommendation_engine/,rahulw230,1489566000,[removed],0,1
496,2017-3-15,2017,3,15,18,5zicwh,[P] a Machine Learning Project to flag Human Traffickers online,https://www.reddit.com/r/MachineLearning/comments/5zicwh/p_a_machine_learning_project_to_flag_human/,dragomen747180,1489569810,"First off let me say I'm not sure if this post is welcome here or the right subreddit if it's not please pm me and I'll remove it and find a better subreddit
So I'm learning machine learning, I posted this here because data sets are a huge part of AI learning.
I have an end goal to eventually create an AI to hook into the DeepWeb and scan for Pedophile Rings based on what classifies as a pedophile and human trafficker. Now I know a lot of you are going to saw that's not possible or may take too long but it's something I'm passionate about.
If you were coding this what classifications would you use to build the AI to flag said activity on the web? How would you feed the input into the AI for it to distinguish non pedo vs pedo or non trafficker vs trafficker. I realize this is a bit of a deep project because we are now profiling and having the AI judge based of a framework of a pedophile or human trafficker by personality traits
But a theoretical neurophysist created a brain through ML How to build a brain with Python https://youtu.be/7hvpoLKJHOw",11,0
497,2017-3-15,2017,3,15,18,5zidvj,"Are object detection algorithms capable of comparing image A and image B, then outputting the difference?",https://www.reddit.com/r/MachineLearning/comments/5zidvj/are_object_detection_algorithms_capable_of/,Jakethe___,1489570317,[removed],0,1
498,2017-3-15,2017,3,15,18,5ziedi,Cannot calculate feature value for certain training examples,https://www.reddit.com/r/MachineLearning/comments/5ziedi/cannot_calculate_feature_value_for_certain/,clarkkentML,1489570555,[removed],0,1
499,2017-3-15,2017,3,15,19,5zikbz,News machine learning example,https://www.reddit.com/r/MachineLearning/comments/5zikbz/news_machine_learning_example/,luminoumen,1489573505,[removed],0,1
500,2017-3-15,2017,3,15,20,5ziont,Learn your style and distribute,https://www.reddit.com/r/MachineLearning/comments/5ziont/learn_your_style_and_distribute/,darkolorin,1489575624,,0,1
501,2017-3-15,2017,3,15,20,5zist2,Does it make sense to make an AI specialized in AGI (Artificial general intelligence)?,https://www.reddit.com/r/MachineLearning/comments/5zist2/does_it_make_sense_to_make_an_ai_specialized_in/,aranguri,1489577460,[removed],0,1
502,2017-3-15,2017,3,15,20,5zitbm,How an image is represented as tensor ?,https://www.reddit.com/r/MachineLearning/comments/5zitbm/how_an_image_is_represented_as_tensor/,John_Smith111,1489577653,[removed],1,1
503,2017-3-15,2017,3,15,22,5zje2k,"Facebook unveils Big Basin, its next-generation GPU server | VentureBeat | Enterprise",https://www.reddit.com/r/MachineLearning/comments/5zje2k/facebook_unveils_big_basin_its_nextgeneration_gpu/,smith2017,1489585190,,0,1
504,2017-3-15,2017,3,15,23,5zjjal,[P] Jonker-Volgenant Algorithm + t-SNE = Super Powers,https://www.reddit.com/r/MachineLearning/comments/5zjjal/p_jonkervolgenant_algorithm_tsne_super_powers/,markovtsev,1489586855,,38,58
505,2017-3-15,2017,3,15,23,5zjpyt,can I find an end to end deepspeech pretrained neural network ?,https://www.reddit.com/r/MachineLearning/comments/5zjpyt/can_i_find_an_end_to_end_deepspeech_pretrained/,saurabhvyas3,1489588848,[removed],0,1
506,2017-3-15,2017,3,15,23,5zjtp6,I know you guys hate PHP but here is a Clarifai PHP client,https://www.reddit.com/r/MachineLearning/comments/5zjtp6/i_know_you_guys_hate_php_but_here_is_a_clarifai/,worldwidewunicorn,1489589941,,0,1
507,2017-3-16,2017,3,16,0,5zjxtn,Intuitive explanation of RWS (Reweighted Wake-Sleep) Algorithm,https://www.reddit.com/r/MachineLearning/comments/5zjxtn/intuitive_explanation_of_rws_reweighted_wakesleep/,pi--,1489591068,[removed],0,1
508,2017-3-16,2017,3,16,0,5zk5qn,"Simple Questions Thread March 15, 2017",https://www.reddit.com/r/MachineLearning/comments/5zk5qn/simple_questions_thread_march_15_2017/,AutoModerator,1489593156,[removed],0,1
509,2017-3-16,2017,3,16,0,5zk6in,[D] GTA-V issue: How to prevent it from upgrading ?,https://www.reddit.com/r/MachineLearning/comments/5zk6in/d_gtav_issue_how_to_prevent_it_from_upgrading/,xingdongrobotics,1489593354,"Currently I am trying to set up GTA V with Universe from forked repo

[deepdrive-universe](https://github.com/OSSDC/deepdrive-universe)

[universe-windows-envs](https://github.com/FutureFuture/universe-windows-envs)

GTA V: Standalone version [7 CD]

Computers: 1 Linux + 1 Windows

Everything is followed according to instructions in 2 repo above. However, I am now facing with a problem that the social club is upgrading the game by downloading 3.8 GB data which will break the GTAVController.exe to work properly.

Is there a way to prevent it from upgrading ?
",8,2
510,2017-3-16,2017,3,16,1,5zkavh,"overcoming catastrophic forgetting in neural networks, by deepmind",https://www.reddit.com/r/MachineLearning/comments/5zkavh/overcoming_catastrophic_forgetting_in_neural/,aiquantum,1489594502,,0,1
511,2017-3-16,2017,3,16,3,5zl8jz,A question about the softmax layers in GoogleNet,https://www.reddit.com/r/MachineLearning/comments/5zl8jz/a_question_about_the_softmax_layers_in_googlenet/,TK3C,1489603249,[removed],0,1
512,2017-3-16,2017,3,16,4,5zlhh5,"[R] Learning when to skim and when to read (blog, interactive plots)",https://www.reddit.com/r/MachineLearning/comments/5zlhh5/r_learning_when_to_skim_and_when_to_read_blog/,alrojo,1489605501,,2,12
513,2017-3-16,2017,3,16,5,5zlyil,Machine Learning and AI subjects in Universities,https://www.reddit.com/r/MachineLearning/comments/5zlyil/machine_learning_and_ai_subjects_in_universities/,Marc_Marc_,1489609881,[removed],0,1
514,2017-3-16,2017,3,16,6,5zm56c,Aspen Systems,https://www.reddit.com/r/MachineLearning/comments/5zm56c/aspen_systems/,amaldonado987,1489611615,[removed],0,1
515,2017-3-16,2017,3,16,6,5zm6ug,How to Generate Music with Tensorflow (LIVE in 1080p),https://www.reddit.com/r/MachineLearning/comments/5zm6ug/how_to_generate_music_with_tensorflow_live_in/,funtwo2,1489612044,,0,1
516,2017-3-16,2017,3,16,6,5zmfvx,[R] Finding the optimal Bayesian network given a constraint graph,https://www.reddit.com/r/MachineLearning/comments/5zmfvx/r_finding_the_optimal_bayesian_network_given_a/,ants_rock,1489614444,,1,3
517,2017-3-16,2017,3,16,6,5zmg7g,[D] Zoubin Ghahramani as Ubers Chief Scientist,https://www.reddit.com/r/MachineLearning/comments/5zmg7g/d_zoubin_ghahramani_as_ubers_chief_scientist/,wei_jok,1489614537,,7,1
518,2017-3-16,2017,3,16,7,5zmnye,Deep Learning in Algorithmic Trading -Thomas Wiecki,https://www.reddit.com/r/MachineLearning/comments/5zmnye/deep_learning_in_algorithmic_trading_thomas_wiecki/,gorilla64,1489616664,,0,1
519,2017-3-16,2017,3,16,7,5zmqwv,[P] Learning ML on the cheap: Persistent AWS Spot Instances,https://www.reddit.com/r/MachineLearning/comments/5zmqwv/p_learning_ml_on_the_cheap_persistent_aws_spot/,slavivanov,1489617492,,20,124
520,2017-3-16,2017,3,16,7,5zmua4,Clarification questions about types of layers in convolutional neural nets.,https://www.reddit.com/r/MachineLearning/comments/5zmua4/clarification_questions_about_types_of_layers_in/,NickFlare,1489618482,[removed],0,1
521,2017-3-16,2017,3,16,8,5zn2xu,"[D] Your brain on TensorFlow, Keras &amp; PyTorch",https://www.reddit.com/r/MachineLearning/comments/5zn2xu/d_your_brain_on_tensorflow_keras_pytorch/,[deleted],1489621012,[deleted],4,0
522,2017-3-16,2017,3,16,8,5zn4cc,"[D] Is there a ""reverse Keras""?",https://www.reddit.com/r/MachineLearning/comments/5zn4cc/d_is_there_a_reverse_keras/,nharada,1489621450,"Keras (especially Keras 2.0) allows you to use the existing Tensorflow framework but will allow you to build the graphs with a higher level language. I'm looking for something that allows you to define all the graph information myself, but has some code so I can stop rewriting all the boilerplate training code. For example:

* Batching
* Queuing to allow multiple threads for data augmentation
* Testing a validation set",13,14
523,2017-3-16,2017,3,16,10,5znkns,[R] [1703.04813] Learned Optimizers that Scale and Generalize,https://www.reddit.com/r/MachineLearning/comments/5znkns/r_170304813_learned_optimizers_that_scale_and/,ajmooch,1489626447,,5,21
524,2017-3-16,2017,3,16,11,5znyyr,[D] Mark Cuban: The worlds first trillionaire will be an artificial intelligence entrepreneur (stay calm and keep shipping folks!),https://www.reddit.com/r/MachineLearning/comments/5znyyr/d_mark_cuban_the_worlds_first_trillionaire_will/,[deleted],1489631011,[deleted],0,0
525,2017-3-16,2017,3,16,11,5zo0e7,What's the difference between a single output RNN and a MLP whose input data contains all the features of all given time steps?,https://www.reddit.com/r/MachineLearning/comments/5zo0e7/whats_the_difference_between_a_single_output_rnn/,ispinfx,1489631478,,0,1
526,2017-3-16,2017,3,16,11,5zo3df,"[R] ""Online Learning Rate Adaptation with Hypergradient Descent"", Baydin et al 2017",https://www.reddit.com/r/MachineLearning/comments/5zo3df/r_online_learning_rate_adaptation_with/,gwern,1489632466,,11,18
527,2017-3-16,2017,3,16,14,5zowb1,Top Machine Learning Trends To See In 2017,https://www.reddit.com/r/MachineLearning/comments/5zowb1/top_machine_learning_trends_to_see_in_2017/,digitalmarketingrobi,1489643681,,0,1
528,2017-3-16,2017,3,16,15,5zp0eu,[R] [1703.05192] Learning to Discover Cross-Domain Relations with Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/5zp0eu/r_170305192_learning_to_discover_crossdomain/,jazzsaxmafia,1489645493,,18,42
529,2017-3-16,2017,3,16,17,5zpc6c,Toz Dolum Makinas eitleri,https://www.reddit.com/r/MachineLearning/comments/5zpc6c/toz_dolum_makinas_eitleri/,renasmakinatr,1489651450,[removed],0,1
530,2017-3-16,2017,3,16,17,5zpehy,look! this is JCT chemical blender for paint industry,https://www.reddit.com/r/MachineLearning/comments/5zpehy/look_this_is_jct_chemical_blender_for_paint/,mixmachinery,1489652694,,1,1
531,2017-3-16,2017,3,16,17,5zpgic,How does the paint blender mixers works with high speed dispersion stirrer,https://www.reddit.com/r/MachineLearning/comments/5zpgic/how_does_the_paint_blender_mixers_works_with_high/,mixmachinery,1489653823,,1,1
532,2017-3-16,2017,3,16,18,5zpilg,[R] [1703.04908] Emergence of Grounded Compositional Language in Multi-Agent Populations,https://www.reddit.com/r/MachineLearning/comments/5zpilg/r_170304908_emergence_of_grounded_compositional/,evc123,1489654909,,5,3
533,2017-3-16,2017,3,16,18,5zpjbe,[D]What did you understand about elastic weight consolidation?,https://www.reddit.com/r/MachineLearning/comments/5zpjbe/dwhat_did_you_understand_about_elastic_weight/,commafighter,1489655269,"DeepMind recently published this paper [Overcoming catastrophic forgetting in neural networks](http://www.pnas.org/content/early/2017/03/13/1611835114.full) and  they used something called **elastic weight consolidation** algorithm short EWC for preventing catastrophic forgetting in neural nets. They have added three equations to the paper also because my math skill is elementary I only understood the basics. They are comparing two different data *Da* and *Db* to update weight *theta*.

If any one else read and understood this paper can you please share your thinking about this paper.
Is EWC can be applied to other simpler neural network say MLP instead of DQN?
",3,3
534,2017-3-16,2017,3,16,18,5zpjgj,Biological and Machine Intelligence (BAMI),https://www.reddit.com/r/MachineLearning/comments/5zpjgj/biological_and_machine_intelligence_bami/,[deleted],1489655361,[deleted],0,1
535,2017-3-16,2017,3,16,18,5zpoy8,[P] DyTB: don't waste your time writing boilerplate code. Let DyTB do it for you.,https://www.reddit.com/r/MachineLearning/comments/5zpoy8/p_dytb_dont_waste_your_time_writing_boilerplate/,pgaleone,1489658287,,4,3
536,2017-3-16,2017,3,16,21,5zqav7,Would it be possible to apply instance segmentation without having segmentation ground truth ?,https://www.reddit.com/r/MachineLearning/comments/5zqav7/would_it_be_possible_to_apply_instance/,falmasri,1489667638,[removed],0,1
537,2017-3-16,2017,3,16,22,5zqixt,Asking for advice for a regression problem with CNN.,https://www.reddit.com/r/MachineLearning/comments/5zqixt/asking_for_advice_for_a_regression_problem_with/,oooneFolder,1489670439,[removed],0,1
538,2017-3-16,2017,3,16,22,5zqpuw,Looking for the perfect algo for my usecase,https://www.reddit.com/r/MachineLearning/comments/5zqpuw/looking_for_the_perfect_algo_for_my_usecase/,m0rpho,1489672743,[removed],0,1
539,2017-3-16,2017,3,16,23,5zqqtj,[D] Best introductory video on machine learning for a complete beginner ?,https://www.reddit.com/r/MachineLearning/comments/5zqqtj/d_best_introductory_video_on_machine_learning_for/,datavinci,1489673022,"Need just one video which explains
-What is machine learning
-Where it used.

It just needs to be very introductory,nothing too technical.

Note:- I searched on internet(also this sub reddit) but could not find a good noob-friendly one.",11,3
540,2017-3-16,2017,3,16,23,5zr14f,Neural Net advice?,https://www.reddit.com/r/MachineLearning/comments/5zr14f/neural_net_advice/,andyr435,1489676087,[removed],0,1
541,2017-3-17,2017,3,17,0,5zr2vm,"[R,D][inFERENCe] Comment on Elastic Weight Consolidation (DeepMind PNAS paper). tldr: Multiple task-specific penalties may not be needed.",https://www.reddit.com/r/MachineLearning/comments/5zr2vm/rdinference_comment_on_elastic_weight/,[deleted],1489676580,[deleted],1,1
542,2017-3-17,2017,3,17,0,5zra9h,Introduction to a ML algorithm that tends to fly below the radar: Self-Organizing Maps (part 1),https://www.reddit.com/r/MachineLearning/comments/5zra9h/introduction_to_a_ml_algorithm_that_tends_to_fly/,elisebreda,1489678631,,0,1
543,2017-3-17,2017,3,17,0,5zrepz,[R] [inFERENCe] Comment on Elastic Weight Consoliation (DeepMind PNAS paper) tldr: the multiple task-specific penalties may not be needed,https://www.reddit.com/r/MachineLearning/comments/5zrepz/r_inference_comment_on_elastic_weight/,fhuszar,1489679867,,11,28
544,2017-3-17,2017,3,17,1,5zrpee,OpenAI: Learning to communicate,https://www.reddit.com/r/MachineLearning/comments/5zrpee/openai_learning_to_communicate/,[deleted],1489682781,[deleted],0,1
545,2017-3-17,2017,3,17,1,5zrq39,[D] OpenAI: Learning to communicate,https://www.reddit.com/r/MachineLearning/comments/5zrq39/d_openai_learning_to_communicate/,Spotlight0xff,1489682965,,15,184
546,2017-3-17,2017,3,17,1,5zrqok,[N] Google open-sources Tensorflow based framework for NLP,https://www.reddit.com/r/MachineLearning/comments/5zrqok/n_google_opensources_tensorflow_based_framework/,[deleted],1489683119,[deleted],2,11
547,2017-3-17,2017,3,17,1,5zrs72,[D] Volumetric Batch Normalization in tensorflow?,https://www.reddit.com/r/MachineLearning/comments/5zrs72/d_volumetric_batch_normalization_in_tensorflow/,stochastic_zeitgeist,1489683540,"What is the tensorflow equivalent of Volumetric Batch Normalization as provided by Torch in [link](https://github.com/torch/nn/blob/master/VolumetricBatchNormalization.lua)  `nn.VolumetricBatchNormalization` ?
 

I am currently using: 


`output = tf.contrib.layers.batch_norm(input, is_training=phase_train)`

is this the correct way of usage for inputs of dimensions `(batch_size,height,width,depth,channels)` ? 

PS - I am trying to implement the 3DGAN paper from CSAIL team. ",2,2
548,2017-3-17,2017,3,17,2,5zrvph,MSc Data Science at UCL vs MSc A.I. at Edinburgh,https://www.reddit.com/r/MachineLearning/comments/5zrvph/msc_data_science_at_ucl_vs_msc_ai_at_edinburgh/,akaash93,1489684462,[removed],0,1
549,2017-3-17,2017,3,17,2,5zryc8,Some new ML stuff in Wolfram 11.1,https://www.reddit.com/r/MachineLearning/comments/5zryc8/some_new_ml_stuff_in_wolfram_111/,Pascal_Rascal,1489685164,,0,1
550,2017-3-17,2017,3,17,2,5zs14y,Which machine learning technique can be used to find the context of a sentence?,https://www.reddit.com/r/MachineLearning/comments/5zs14y/which_machine_learning_technique_can_be_used_to/,ReinhardStrike,1489685917,[removed],0,1
551,2017-3-17,2017,3,17,3,5zs9lj,Train Record Linkage and generate predictions on test set?,https://www.reddit.com/r/MachineLearning/comments/5zs9lj/train_record_linkage_and_generate_predictions_on/,[deleted],1489688166,[removed],0,1
552,2017-3-17,2017,3,17,4,5zsn8r,"An Upgrade to SyntaxNet, New Models and a Parsing Competition",https://www.reddit.com/r/MachineLearning/comments/5zsn8r/an_upgrade_to_syntaxnet_new_models_and_a_parsing/,CaptainMythral,1489691850,,0,2
553,2017-3-17,2017,3,17,4,5zsnm6,[D] training embeddings for billion word vocabulary,https://www.reddit.com/r/MachineLearning/comments/5zsnm6/d_training_embeddings_for_billion_word_vocabulary/,mshreddit,1489691956,"if I have a dataset where vocabulary is hundreds of millions of words, how do you train a model to learn embeddings for these words? I've seen examples where model size becomes huge and you have one or two layers on each GPU. In this case however, each layer could also be big enough that won't fit on a single GPU. What's the best practice to split a layer on multiple GPUs? Do you know an example or paper that would help?",11,1
554,2017-3-17,2017,3,17,6,5ztfgw,"Visdom: create, organize, and share visualizations of live, rich data for (Py|Lua)Torch",https://www.reddit.com/r/MachineLearning/comments/5ztfgw/visdom_create_organize_and_share_visualizations/,ajabri,1489699523,,3,58
555,2017-3-17,2017,3,17,6,5ztkub,[D] How to use batch_sequences_with_states in tensorflow?,https://www.reddit.com/r/MachineLearning/comments/5ztkub/d_how_to_use_batch_sequences_with_states_in/,jiminiminimini,1489701055,"[This example](https://www.tensorflow.org/versions/master/api_docs/python/contrib.training/splitting_sequence_inputs_into_minibatches_with_state_saving) in the documentation does not work and all the other bits and pieces required to use this method are scattered around tests, examples, comments inside the repository. I have preprocessed data of different lengths. They are currently stored as a list of numpy arrays of shape `(time, features)`. How should I format this list in order to be able to use `batch_sequences_with_states` method?",0,2
556,2017-3-17,2017,3,17,6,5ztlc5,Why is matmul operation in tensorflow reversed?,https://www.reddit.com/r/MachineLearning/comments/5ztlc5/why_is_matmul_operation_in_tensorflow_reversed/,[deleted],1489701196,[removed],0,1
557,2017-3-17,2017,3,17,6,5ztm2g,Quantum computing takes a massive step forward thanks to machine learning,https://www.reddit.com/r/MachineLearning/comments/5ztm2g/quantum_computing_takes_a_massive_step_forward/,johnmountain,1489701401,,0,1
558,2017-3-17,2017,3,17,7,5ztnd0,Topic mining with LDA and Kmeans and interactive clustering in Python,https://www.reddit.com/r/MachineLearning/comments/5ztnd0/topic_mining_with_lda_and_kmeans_and_interactive/,ahmedbesbes,1489701743,,0,1
559,2017-3-17,2017,3,17,7,5ztoso,[D] ANN Architectures for Time/Pitch Manipulation or Complex Waveforms,https://www.reddit.com/r/MachineLearning/comments/5ztoso/d_ann_architectures_for_timepitch_manipulation_or/,zergling103,1489702154,"EDIT: Manipulation OF* Complex Waveforms.

Have any papers been published regarding using artificial neural networks of some sort to lengthen or shorten audio clips (especially complex ones, like music) without affecting their pitch?

Non-neural network approaches unfortunately only seem to go so far before introducing unpleasant artifacts. At one point, someone on this wikipedia page (https://en.wikipedia.org/wiki/Audio_time-scale/pitch_modification) claimed that NN's were used for time stretching, though he didn't give a citation.

Are there any papers that use machine learning for this purpose?

Of course, I googled this extensively without much showing up.

Note this is a distinct problem from increasing the sampling rate (i.e. super-resolution) of a given waveform.",5,1
560,2017-3-17,2017,3,17,7,5ztoto,[D] Does anyone maintain weight orthogonality during training,https://www.reddit.com/r/MachineLearning/comments/5ztoto/d_does_anyone_maintain_weight_orthogonality/,davikrehalt,1489702162,"So orthogonal initialization is getting popular, but is maintaining orthogonality common practice? If so, how is it done?",30,18
561,2017-3-17,2017,3,17,9,5zujdz,[D] Transitioning to ML at postdoc level,https://www.reddit.com/r/MachineLearning/comments/5zujdz/d_transitioning_to_ml_at_postdoc_level/,Tankeety,1489711334,"I'm currently finishing my PhD in mathematical physics, and as I go along I've become more and more attracted to the mathematical side of things. I've noticed that there's a lot of interesting research going on in the mathematics related to machine learning - plenty of nice results in matrix analysis, optimization, formal theory of learning etc. And  since the job market looks to be miles better on the ML side of the fence, I'm thinking about switching fields.

However, how likely is it to succeed in getting into ML research at a postdoc level? I don't have much formal experience with ML, although I do have some publications in optimization and things that are not completely unrelated to ML. Still, I doubt that anyone would seriously consider postdoc applications that say ""I don't know much but I really want to learn"", so I was thinking of doing a postdoc in my field while I try to study as much as I can. Alternatively, I could easily transition to corporate data science but I'm not sure how helpful that would be.

Has anyone experienced a post-PhD transition to ML research? Any tips or ideas that you can share?

Thanks a lot.",5,3
562,2017-3-17,2017,3,17,10,5zup3u,Latest Bytenet with SOTA results on character-level MT on WMT En-De (compares also with GNMT),https://www.reddit.com/r/MachineLearning/comments/5zup3u/latest_bytenet_with_sota_results_on/,oaxacaml,1489713255,,0,1
563,2017-3-17,2017,3,17,11,5zv5nz,[P] Extracting different instruments from audio stream,https://www.reddit.com/r/MachineLearning/comments/5zv5nz/p_extracting_different_instruments_from_audio/,careless25,1489718867,"Our objective is to train a Neural Network to split audio stream into 2, each with a different instrument. We are generating data using LMMS for drums and piano. 

For example if the audio stream had [1, 2, 2, 1, 1, 2] the output should be: [1, 0, 0, 1, 1, 0] and [0, 2, 2, 0, 0, 2]. Where 1 and 2 are the different instruments.

- What we can't figure out from reading several papers and blogs is what kind of network to use (CNN or RNN) ?
- Once the network is trained, how do we split the audio stream into two?

This Medium article states that a CNN was able to classify different notes of different instruments: https://medium.com/@awjuliani/recognizing-sounds-a-deep-learning-case-study-1bc37444d44d#.4nlj9ti54

He has 2 min clips of one instrument playing a single note (44100 Hz, 240bpm), splits it into 2 seconds each, converts to a spectrogram and then trains the CNN with it. The accuracy with this model is pretty good but we are finding it hard to model something similar for our project since we need it to identify multiple things in one audio stream.
",14,0
564,2017-3-17,2017,3,17,14,5zvrny,"Essential Statistics for Data Science: A Case Study using Python, Part I",https://www.reddit.com/r/MachineLearning/comments/5zvrny/essential_statistics_for_data_science_a_case/,tmthyjames,1489727280,,0,1
565,2017-3-17,2017,3,17,14,5zvyee,Need help to decide the ML model,https://www.reddit.com/r/MachineLearning/comments/5zvyee/need_help_to_decide_the_ml_model/,ravinarayana,1489730342,[removed],0,1
566,2017-3-17,2017,3,17,15,5zw289,[D] PC build for ML: Ryzen 1700x vs i7 7700k,https://www.reddit.com/r/MachineLearning/comments/5zw289/d_pc_build_for_ml_ryzen_1700x_vs_i7_7700k/,Inori,1489732112,"Looking into building a PC for ML and am stuck deciding between the two CPUs.

Do you think (have tested?) that 4 extra cores will trump single-core performance (~ Ghz difference) on the typical CPU tasks like data pre-processing and augumentation?",24,4
567,2017-3-17,2017,3,17,15,5zw3qt,Could you help find StackOverflow Underrated Answers?,https://www.reddit.com/r/MachineLearning/comments/5zw3qt/could_you_help_find_stackoverflow_underrated/,[deleted],1489732822,[removed],0,1
568,2017-3-17,2017,3,17,18,5zwn7t,[R] Sharp Minima Can Generalize For Deep Nets,https://www.reddit.com/r/MachineLearning/comments/5zwn7t/r_sharp_minima_can_generalize_for_deep_nets/,mttd,1489743346,,6,26
569,2017-3-17,2017,3,17,19,5zwwcp,Under Ground Mining Drilling Rig | Beavertracks.co.in,https://www.reddit.com/r/MachineLearning/comments/5zwwcp/under_ground_mining_drilling_rig_beavertrackscoin/,beaver_tracks,1489747922,,0,1
570,2017-3-17,2017,3,17,20,5zwxgh,Need help to decide the ML model,https://www.reddit.com/r/MachineLearning/comments/5zwxgh/need_help_to_decide_the_ml_model/,ravinarayana,1489748461,[removed],0,1
571,2017-3-17,2017,3,17,20,5zx4p3,Mathematica 11.1 expands Keras-like NN creation and introduces new visualization tools,https://www.reddit.com/r/MachineLearning/comments/5zx4p3/mathematica_111_expands_keraslike_nn_creation_and/,yadec,1489751529,,0,1
572,2017-3-17,2017,3,17,21,5zx60p,Learning Deep Nearest Neighbor Representations Using Differentiable Boundary Trees,https://www.reddit.com/r/MachineLearning/comments/5zx60p/learning_deep_nearest_neighbor_representations/,JuhoKupiainen,1489752066,,2,25
573,2017-3-17,2017,3,17,21,5zxdda,Bytenet v2 with state-of-the-art results on char-to-char machine translation on WMT En-De (compares favorably to char-to-char GNMT),https://www.reddit.com/r/MachineLearning/comments/5zxdda/bytenet_v2_with_stateoftheart_results_on/,deeprnn,1489754887,,12,9
574,2017-3-17,2017,3,17,22,5zxjjp,Researchers in China One Step Closer to Parallel Turing Machine,https://www.reddit.com/r/MachineLearning/comments/5zxjjp/researchers_in_china_one_step_closer_to_parallel/,[deleted],1489757020,[deleted],0,1
575,2017-3-17,2017,3,17,22,5zxla5,Pretrained video datasets for Caffe?,https://www.reddit.com/r/MachineLearning/comments/5zxla5/pretrained_video_datasets_for_caffe/,sishkebap,1489757623,[removed],0,1
576,2017-3-17,2017,3,17,22,5zxmat,BUILDING THE FOUNDATION OF THE AUTONOMOUS VEHICLE,https://www.reddit.com/r/MachineLearning/comments/5zxmat/building_the_foundation_of_the_autonomous_vehicle/,nikitaljohnson,1489757976,,0,1
577,2017-3-17,2017,3,17,23,5zxv0j,Supervised Learning with scikit-learn course,https://www.reddit.com/r/MachineLearning/comments/5zxv0j/supervised_learning_with_scikitlearn_course/,gcdes,1489760722,,0,1
578,2017-3-18,2017,3,18,0,5zy3fh,[R] Recurrent Reinforcement Learning: A Hybrid Approach,https://www.reddit.com/r/MachineLearning/comments/5zy3fh/r_recurrent_reinforcement_learning_a_hybrid/,downtownslim,1489763223,,1,3
579,2017-3-18,2017,3,18,0,5zyb2d,[R] Spherical Kernel Divergences: moment matching b/w distributions via cosine dissimilarity rather than squared distance,https://www.reddit.com/r/MachineLearning/comments/5zyb2d/r_spherical_kernel_divergences_moment_matching_bw/,fhuszar,1489765351,,3,9
580,2017-3-18,2017,3,18,1,5zyf6l,[D] Training a discriminator network using only real examples,https://www.reddit.com/r/MachineLearning/comments/5zyf6l/d_training_a_discriminator_network_using_only/,618smartguy,1489766502,"I was wondering if it would be possible to add stability to GAN training by only training the discriminator using real image examples. Of course the obvious result would be that the network would learn to ignore the input and only output 1, but maybe a carefully constructed architecture could force the network to learn something about the input distribution.

For a very simple example, lets say the input was just the mnist digit 0 and the discriminator is just one fully connected layer to the output, with no bias. If [cosine normalization](https://arxiv.org/pdf/1702.05870.pdf) was used, then I think that the learned weights would resemble an averaged 0, because the cosine distance would only be 1 if the input and the weights are simmilar. To extend this to all 10 mnist digits, the dense layer might have 10 outputs which are reduced to 1 output by max. I wonder if a deep convolutional network using only max pooling and cosine similarity layers could be trained to convergence, and then a generator network could learn to fool it afterwards.

I tried this in tensorflow but couldn't get the discriminator to converge as the weights would immediately blow up. Here is my code if anyone sees something wrong:

    def cosine_simmilarity_convolution(x, W, shape):
        mask = tf.ones(shape)
        input_magnitude = tf.sqrt(tf.expand_dims(tf.reduce_sum(tf.nn.depthwise_conv2d(tf.square(x), mask, strides=[1, 1, 1, 1], padding='SAME'),3),-1))
        weight_magnitude = tf.sqrt(tf.reduce_sum(tf.square(W), [0,1,2]))
        conv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')
        return tf.nn.relu(conv/(.0001+input_magnitude * weight_magnitude))

",2,6
581,2017-3-18,2017,3,18,1,5zymr8,"[R] ""Large Scale Evolution of Convolutional Neural Networks Using Volunteer Computing"", Desell et al 2017",https://www.reddit.com/r/MachineLearning/comments/5zymr8/r_large_scale_evolution_of_convolutional_neural/,gwern,1489768533,,4,1
582,2017-3-18,2017,3,18,2,5zyuyu,Baidu's new paper on deep learning based small-footprint keyword spotting for conversational interfaces,https://www.reddit.com/r/MachineLearning/comments/5zyuyu/baidus_new_paper_on_deep_learning_based/,bayjingsf,1489770728,,1,19
583,2017-3-18,2017,3,18,2,5zz1vw,[N] CHI - Experimentation framework for Tensorflow geared towards deep RL / GANs,https://www.reddit.com/r/MachineLearning/comments/5zz1vw/n_chi_experimentation_framework_for_tensorflow/,simonramstedt,1489772531,,1,9
584,2017-3-18,2017,3,18,3,5zzcau,Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play,https://www.reddit.com/r/MachineLearning/comments/5zzcau/intrinsic_motivation_and_automatic_curricula_via/,[deleted],1489775284,[deleted],1,12
585,2017-3-18,2017,3,18,3,5zzei3,A simple neutral network in python,https://www.reddit.com/r/MachineLearning/comments/5zzei3/a_simple_neutral_network_in_python/,mlnotebook,1489775869,[removed],0,1
586,2017-3-18,2017,3,18,3,5zzg1u,[D] Memory Efficient Word2Vec,https://www.reddit.com/r/MachineLearning/comments/5zzg1u/d_memory_efficient_word2vec/,[deleted],1489776268,[deleted],10,16
587,2017-3-18,2017,3,18,3,5zzgmw,Awesome course on Unsupervised Learning in R!,https://www.reddit.com/r/MachineLearning/comments/5zzgmw/awesome_course_on_unsupervised_learning_in_r/,gcdes,1489776410,,0,1
588,2017-3-18,2017,3,18,4,5zzk2v,Building Safe A.I. - A Tutorial for Encrypted Deep Learning,https://www.reddit.com/r/MachineLearning/comments/5zzk2v/building_safe_ai_a_tutorial_for_encrypted_deep/,johnmountain,1489777305,,1,3
589,2017-3-18,2017,3,18,4,5zzqcc,Pattern Recognition in Financial Time Series Data Using Neural Network?,https://www.reddit.com/r/MachineLearning/comments/5zzqcc/pattern_recognition_in_financial_time_series_data/,reformedcuck,1489779016,[removed],0,1
590,2017-3-18,2017,3,18,4,5zzt7a,[N] AFib Classification from a short single lead ECG recording: the PhysioNet/Computing in Cardiology Challenge 2017,https://www.reddit.com/r/MachineLearning/comments/5zzt7a/n_afib_classification_from_a_short_single_lead/,pmigdal,1489779814,,3,5
591,2017-3-18,2017,3,18,4,5zzumx,"[P] DiscoGAN in PyTorch: implementation of ""Learning to Discover Cross-Domain Relations with Generative Adversarial Networks""",https://www.reddit.com/r/MachineLearning/comments/5zzumx/p_discogan_in_pytorch_implementation_of_learning/,alxndrkalinin,1489780211,,22,73
592,2017-3-18,2017,3,18,5,5zzxrh,"Deeplearning4j 0.8.0 Release Notes: Transfer Learning API, Spark 2.0",https://www.reddit.com/r/MachineLearning/comments/5zzxrh/deeplearning4j_080_release_notes_transfer/,dl4j_contributor,1489781039,,0,1
593,2017-3-18,2017,3,18,5,5zzyhn,Data Science Digest - Issue #6,https://www.reddit.com/r/MachineLearning/comments/5zzyhn/data_science_digest_issue_6/,flyelephant,1489781236,,0,1
594,2017-3-18,2017,3,18,5,5zzzc4,A lightweight framework to host scikit-learn models in memory and serve realtime requests. We are using it in production. Please share your feedbacks.,https://www.reddit.com/r/MachineLearning/comments/5zzzc4/a_lightweight_framework_to_host_scikitlearn/,mgoswami,1489781500,,0,1
595,2017-3-18,2017,3,18,5,5zzzq9,Introducing Similarity Search at Flickr,https://www.reddit.com/r/MachineLearning/comments/5zzzq9/introducing_similarity_search_at_flickr/,alexeyr,1489781612,,0,1
596,2017-3-18,2017,3,18,5,6003hm,[N] Intel &amp; MobileODT Cervical Cancer Screening - Image Classification Competition,https://www.reddit.com/r/MachineLearning/comments/6003hm/n_intel_mobileodt_cervical_cancer_screening_image/,pmigdal,1489782659,,0,2
597,2017-3-18,2017,3,18,5,6006zy,Python Tutorial: Homomorphically Encrypted Deep Learning (from scratch),https://www.reddit.com/r/MachineLearning/comments/6006zy/python_tutorial_homomorphically_encrypted_deep/,iamtrask,1489783642,,0,1
598,2017-3-18,2017,3,18,5,600965,"[News] Deeplearning4j 8.0 is out, new transfer learning API",https://www.reddit.com/r/MachineLearning/comments/600965/news_deeplearning4j_80_is_out_new_transfer/,crockpotveggies,1489784292,"For Java ML peeps out there Deeplearning4j 8.0 was just released. This includes a new transfer learning API, Spark 2.0 support, and Arbiter - hyperparameter optimization tool - now supports lots of the new layers added in 8.0.

For Scala users, ScalNet has been updated and much more is on the way.

I've been personally contributing to the DL4J project and using it at a company of mine. Performance is getting on par with other leading frameworks. Self-reported benchmarks for AlexNet are coming into range of Neon and Torch cuDNN, where avg iteration takes about 71ms on Torch and DL4J is at about 85ms. That includes updater state timing.

Release notes for those interested: https://deeplearning4j.org/releasenotes#zeroeightzero",5,9
599,2017-3-18,2017,3,18,6,600b78,"[R] ""What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?"", Kendall &amp; Gal 2017",https://www.reddit.com/r/MachineLearning/comments/600b78/r_what_uncertainties_do_we_need_in_bayesian_deep/,gwern,1489784886,,1,14
600,2017-3-18,2017,3,18,6,600im0,How to Make a Text Summarizer - Intro to Deep Learning,https://www.reddit.com/r/MachineLearning/comments/600im0/how_to_make_a_text_summarizer_intro_to_deep/,ackstazya,1489787111,,0,1
601,2017-3-18,2017,3,18,7,600on8,Octave or Tensorflow?,https://www.reddit.com/r/MachineLearning/comments/600on8/octave_or_tensorflow/,thecocolovesme,1489788974,[removed],0,1
602,2017-3-18,2017,3,18,8,600zvm,How AI Can Help Fight Healthcare Ransomware Threats,https://www.reddit.com/r/MachineLearning/comments/600zvm/how_ai_can_help_fight_healthcare_ransomware/,gera_keva,1489792552,,0,1
603,2017-3-18,2017,3,18,8,601470,Technological automation,https://www.reddit.com/r/MachineLearning/comments/601470/technological_automation/,stanyew,1489794025,,0,1
604,2017-3-18,2017,3,18,9,601atx,Developing a language of pictures for a living-presence AI,https://www.reddit.com/r/MachineLearning/comments/601atx/developing_a_language_of_pictures_for_a/,phobrain,1489796249,[removed],1,1
605,2017-3-18,2017,3,18,10,601k56,[Research] Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play,https://www.reddit.com/r/MachineLearning/comments/601k56/research_intrinsic_motivation_and_automatic/,wordbag,1489799270,,1,8
606,2017-3-18,2017,3,18,14,602jol,"At Last, The Secret To machine learning Is Revealed!!",https://www.reddit.com/r/MachineLearning/comments/602jol/at_last_the_secret_to_machine_learning_is_revealed/,subedi_prabesh,1489813369,,0,1
607,2017-3-18,2017,3,18,14,602q3a,[D] Training DQN with a random behavior policy,https://www.reddit.com/r/MachineLearning/comments/602q3a/d_training_dqn_with_a_random_behavior_policy/,zergylord,1489816560,"Was recently messing with DQN, and thought I'd using a replay buffer filled via a random policy rather than the agent's own. (The motivation being to ease algorithm comparison by using the same experience across algorithms). 

I was surprised by how unstable the performance was! Has anyone else encountered this? 

I vaguely remember some theory showing Q-learning convergence to be better when the behavior policy is only slightly different from the evaluation policy (e.g. e-greedy), but I thought in the case of DQN this drawback would be outweighed by having a stationary data distribution. Could someone provide the motivation for why this isn't the case?",4,3
608,2017-3-18,2017,3,18,15,602ua9,6 Myths about Machine Learning That You Thought Were True,https://www.reddit.com/r/MachineLearning/comments/602ua9/6_myths_about_machine_learning_that_you_thought/,TechnoLigent,1489818698,,0,1
609,2017-3-18,2017,3,18,15,602v3b,What's the usefulness of learning ML other than getting a job?,https://www.reddit.com/r/MachineLearning/comments/602v3b/whats_the_usefulness_of_learning_ml_other_than/,Frostedcat,1489819140,[removed],0,1
610,2017-3-18,2017,3,18,15,602vlg,Andrej Karpathy's CS231n notes are amazing. Are there other must-read notes on any topic related to ML (or CS) that I can read?,https://www.reddit.com/r/MachineLearning/comments/602vlg/andrej_karpathys_cs231n_notes_are_amazing_are/,[deleted],1489819401,[removed],0,1
611,2017-3-18,2017,3,18,18,603e02,[D] maxStep Reinforcement Learning - training once per episode,https://www.reddit.com/r/MachineLearning/comments/603e02/d_maxstep_reinforcement_learning_training_once/,Delthc,1489830938,"Hello,

The A3C Algorithm (and N-Step Q Learning) updates the globaly shared network once every N timesteps. N is usually pretty small, 5 or 20 as far as I remember. 

Wouldn't it be possible to set N to infinity, meaning that the networks are only trained at the end of an episode?
I do not argue that it is necessarily better - tough, for me it sounds like it could be - but at least it should not be a lot worse, right?

The lacking asynchronous training based on the asynchronous exploration of the enviroment by multiple agents in different enviroments, and therefore the stabilization of the training procedure without replay memory, might be a problem if the training is done sequentially (as in: for each worker thread, train the network on the whole observed SAR-sequence). 
Tough, the training could still be done asynchronously with sub-sequences, it would only make training with stateful LSTMs a little bit more complicated.

The reason why I am asking is the ""Evolution Strategies as a Scalable Alternative to Reinforcement Learning"" paper.
To compare it to algorithms like A3C, it would make more sense - from a code engineering point of view - to train both algorithms in the same episodic way. 

TL;DR: Is it feasibly to train an A3C algorithm in an episodic context?
",6,2
612,2017-3-18,2017,3,18,20,603mum,Learning classifiers for Malicious Web Sites Detection,https://www.reddit.com/r/MachineLearning/comments/603mum/learning_classifiers_for_malicious_web_sites/,kinghack,1489836063,[removed],0,1
613,2017-3-18,2017,3,18,21,603skh,[P] Official implementation of DiscoGAN,https://www.reddit.com/r/MachineLearning/comments/603skh/p_official_implementation_of_discogan/,jazzsaxmafia,1489838920,,8,71
614,2017-3-18,2017,3,18,23,6048pm,[R] Learning to Optimize Neural Nets,https://www.reddit.com/r/MachineLearning/comments/6048pm/r_learning_to_optimize_neural_nets/,xternalz,1489846099,,1,16
615,2017-3-18,2017,3,18,23,604grm,"[D] Machine learning hasn't been commoditized yet, but that doesn't mean you need a PhD",https://www.reddit.com/r/MachineLearning/comments/604grm/d_machine_learning_hasnt_been_commoditized_yet/,lmcinnes,1489849090,,45,145
616,2017-3-19,2017,3,19,1,6050zl,The startling rise of machine learning,https://www.reddit.com/r/MachineLearning/comments/6050zl/the_startling_rise_of_machine_learning/,[deleted],1489855813,[deleted],0,1
617,2017-3-19,2017,3,19,2,60561u,[Advise] Thinking about creating a Virtual Assistant App to learn ML and NLP. How should I get started?,https://www.reddit.com/r/MachineLearning/comments/60561u/advise_thinking_about_creating_a_virtual/,neptunefox,1489857225,[removed],0,1
618,2017-3-19,2017,3,19,2,605du0,[D] A few experiments about gradients in Deep Learning,https://www.reddit.com/r/MachineLearning/comments/605du0/d_a_few_experiments_about_gradients_in_deep/,Kiuhnm,1489859563,"In the past few months I experimented a lot with ""gradient manipulation"" in MLPs. My efforts didn't amount to much, unfortunately. I'd like to know if anyone else has tried anything similar.

My first idea was to analyze the gradients of single samples/examples. In regular DL, the final gradient is the mean of the gradients of the single examples of the minibatch. I noticed that if one drops the more extreme gradients and perform a truncated mean instead of a normal mean, one gets a nice regularization effect at least in 2D, but it doesn't scale to multidimensional spaces. Would using clustering instead of a truncated mean make a difference?

My idea was that the net is like the world and the samples are people who make requests (gradients) to change the world. During learning, most people adapt to the world, whereas some (outliers) do not. This is why ignoring extreme requests reduce overfit at least in low dimensional spaces. I didn't try clustering because it requires too much computing power.

Another idea was to stop updating a single weight where the gradients became to unbalanced, i.e. when the mean was too far from the median. Combined with the truncated mean it worked quite well in 2D, meaning that many outliers were ignored and overfit was vastly reduced.

Here's another idea: compute the gradients wrt to 2 (or more) minibatches and then compute the elementwise maximum and use it as the final gradient.

If the samples are all positive (like in MNIST) then, by favoring positive gradients, we favor negative weights which, because of relus, promote activation sparsity. A more principled way to do this is to take the activations into account and choose the maximum or minimum (elementwise) of the two gradients.

For instance, if the first neuron activates for too many samples, then we select maximum gradients for it so that the activation rate drops. If the rate is too low, we increase it by selecting the minimum gradients.

This is a little bit of Theano code to make things clearer:

    def custom_gradient(self, inputs, out_grads):
        X, W, b = inputs
        G = out_grads[0]

        # --- normal gradient ---
        # return [G.dot(W.T),
        #         X.T.dot(G),
        #         G.sum(axis=0)]

        # self._output is XW+b.
        W_grad = get_grad(X, G, self._layer_id, self._output)
        b_grad = get_grad(None, G, self._layer_id, self._output)
        return [G.dot(W.T), W_grad, b_grad]

    def get_grad(X, G, layer_id, A):
        num_blocks = 2
        target_density = target_densities[layer_id]
        G = G.reshape((num_blocks, -1, G.shape[1]))
        if X:       # for W
            X = X.reshape((num_blocks, -1, X.shape[1]))
            g1 = X[0].T.dot(G[0])
            g2 = X[1].T.dot(G[1])
            mu_min = T.minimum(g1, g2)
            mu_max = T.maximum(g1, g2)
        else:       # for b
            gs = G.sum(axis=1)
            mu_min = gs.min(axis=0)
            mu_max = gs.max(axis=0)

        zero = T.constant(0, dtype=floatX)
        acts = (A &gt; zero).astype(floatX).mean(axis=0)
        d = acts &gt; target_density

        return d * mu_max + (1 - d) * mu_min

I tried this with the first 1000 samples of the MNIST dataset. With the normal gradient I get an accuracy of ~89.5, whereas with the custom gradient I get 92+. Unfortunately it doesn't seem to work well with the whole dataset of 50000 samples.

edit: BTW, if instead of promoting sparsity we promote density by flipping the rule (i.e. by switching mu_max with mu_min in the code above), we get 85 or worse. So, I think it's really sparsity which makes the difference. Of course I printed sparsity statistics during training to make sure that the algorithm was really working.

##edit2

Maybe it isn't clear from the code but we can compute gradients of single examples of the batch just in one pass of backprop.

For instance, the gradient wrt W is X.T.dot(G), which is

    prod = X.T.dimshuffle(0, 'x', 1) * G.dimshuffle('x', 1, 0)
    G_W = prod.sum(axis=-1)

where prod[i,j,k] is the gradient wrt to W_ij for the k-th sample of the minibatch. This works because each block during backprop receives the gradients wrt the single outputs, if you think about it.

My intuition told me that this had to be the case and tests confirmed it.

That's why in my code I can do

    X = X.reshape((num_blocks, -1, X.shape[1]))
    g1 = X[0].T.dot(G[0])
    g2 = X[1].T.dot(G[1])
    mu_min = T.minimum(g1, g2)
    mu_max = T.maximum(g1, g2)

So, it's not less efficient because we can use bigger minibatches!

As a second point, notice that we can control the activity of every single neuron. Activations (before relu) are computed as

    A = XW + b

so the activations of the i-th neuron through the minibatch is X W_i, where W_i is the i-th column of W_i. We can increase the activity of that neuron by increasing W_i (and b_i).",9,25
619,2017-3-19,2017,3,19,3,605g7m,Future of ML as a field?,https://www.reddit.com/r/MachineLearning/comments/605g7m/future_of_ml_as_a_field/,j884,1489860295,[removed],0,1
620,2017-3-19,2017,3,19,4,605xzs,Top 9 Deep Learning and Neural Networks Books,https://www.reddit.com/r/MachineLearning/comments/605xzs/top_9_deep_learning_and_neural_networks_books/,kjahan,1489865886,,1,1
621,2017-3-19,2017,3,19,7,606v6r,Report Urges Healthcare Execs to Deploy AI Tools against Cybercrime.,https://www.reddit.com/r/MachineLearning/comments/606v6r/report_urges_healthcare_execs_to_deploy_ai_tools/,Anie_ie,1489876604,,0,1
622,2017-3-19,2017,3,19,9,607jmc,"Not DL, but still useful. A py package on association rule mining",https://www.reddit.com/r/MachineLearning/comments/607jmc/not_dl_but_still_useful_a_py_package_on/,[deleted],1489885064,[deleted],0,1
623,2017-3-19,2017,3,19,10,607lx7,How Would You Improve This CNN?,https://www.reddit.com/r/MachineLearning/comments/607lx7/how_would_you_improve_this_cnn/,Simusid,1489885901,[removed],0,1
624,2017-3-19,2017,3,19,13,608fuo,"Predicting Housing Prices with Linear Regression using Python, pandas, and statsmodels",https://www.reddit.com/r/MachineLearning/comments/608fuo/predicting_housing_prices_with_linear_regression/,tmthyjames,1489897716,,0,1
625,2017-3-19,2017,3,19,18,609alg,Anyone Can Be a Math Person Once They Know the Best Learning Techniques,https://www.reddit.com/r/MachineLearning/comments/609alg/anyone_can_be_a_math_person_once_they_know_the/,KIA-D-Sci,1489915336,,0,1
626,2017-3-19,2017,3,19,18,609cpb,Variobend Asco Machines Australia,https://www.reddit.com/r/MachineLearning/comments/609cpb/variobend_asco_machines_australia/,stamac,1489916765,,0,1
627,2017-3-19,2017,3,19,21,609xdk,Sequence to Sequence Learning in Keras,https://www.reddit.com/r/MachineLearning/comments/609xdk/sequence_to_sequence_learning_in_keras/,kizilsakal,1489928312,[removed],0,1
628,2017-3-19,2017,3,19,21,609xf3,Question about metal turning machines feeds and cutting speeds?,https://www.reddit.com/r/MachineLearning/comments/609xf3/question_about_metal_turning_machines_feeds_and/,[deleted],1489928338,[removed],0,1
629,2017-3-19,2017,3,19,22,60a0ap,"Miniconda3, TensorFlow, Keras on Google Compute Engine GPU instance: The step-by-step guide.",https://www.reddit.com/r/MachineLearning/comments/60a0ap/miniconda3_tensorflow_keras_on_google_compute/,cpbotha,1489929620,,0,1
630,2017-3-19,2017,3,19,22,60a2zz,[P] Chainer Implementation of Neural Semantic Encoders,https://www.reddit.com/r/MachineLearning/comments/60a2zz/p_chainer_implementation_of_neural_semantic/,tsendsuren,1489930835,,0,19
631,2017-3-19,2017,3,19,22,60a5el,[R] Towards Competitive Classifiers for Unbalanced Classification Problems: A Study on the Performance Scores,https://www.reddit.com/r/MachineLearning/comments/60a5el/r_towards_competitive_classifiers_for_unbalanced/,TheFlyingDrildo,1489931849,,3,37
632,2017-3-19,2017,3,19,23,60a7sr,Help with SOM,https://www.reddit.com/r/MachineLearning/comments/60a7sr/help_with_som/,BearPaladin,1489932777,[removed],0,1
633,2017-3-19,2017,3,19,23,60af73,Can anyone recommend a single board supercomputer for Deep learning ?,https://www.reddit.com/r/MachineLearning/comments/60af73/can_anyone_recommend_a_single_board_supercomputer/,saurabhvyas3,1489935538,[removed],0,1
634,2017-3-20,2017,3,20,0,60ahhd,[D] Has any work been done using text tokenization based on word type?,https://www.reddit.com/r/MachineLearning/comments/60ahhd/d_has_any_work_been_done_using_text_tokenization/,Nimitz14,1489936284,"From what I've read and googled it seems to the most extreme (more extreme meaning more changes between source and normalized text) form of tokenization is stemming. I haven't found anything on google but have any attempts been made to tokenize based on word type? To be clear the normalized text would look like ""&lt;pronoun&gt; &lt;adjective&gt; &lt;verb&gt; &lt;noun&gt;""",3,2
635,2017-3-20,2017,3,20,0,60aq6h,literature on LSTMs for very short time series,https://www.reddit.com/r/MachineLearning/comments/60aq6h/literature_on_lstms_for_very_short_time_series/,The_Fifth_Henry,1489939159,[removed],0,1
636,2017-3-20,2017,3,20,1,60atli,Big Data Online Course. Join over 21K students who have already taken this amazing course.,https://www.reddit.com/r/MachineLearning/comments/60atli/big_data_online_course_join_over_21k_students_who/,andalib_ansari,1489940189,,0,1
637,2017-3-20,2017,3,20,3,60bio2,Build your own Face Recognition API,https://www.reddit.com/r/MachineLearning/comments/60bio2/build_your_own_face_recognition_api/,lehoangduc,1489947847,,0,1
638,2017-3-20,2017,3,20,3,60blt0,[D] Why performance often degrades a bit by replacing RMSProp by Adam ?,https://www.reddit.com/r/MachineLearning/comments/60blt0/d_why_performance_often_degrades_a_bit_by/,xingdongrobotics,1489948808,"I've trained a few different architectures on different data set, there is a rather common phenomenon that when replacing RMSProp by Adam, the performance degrades a bit (e.g. 99.2% -&gt; 97%), the learning rate is divided by 10 for Adam. (e.g. 1e-3 -&gt; 1e-4). Both are training with same number of epochs. Does it mean that by using Adam, we need to train more epochs to achieve the same performance as RMSProp ?

By the way, is there newer optimizer available recently ? ",5,7
639,2017-3-20,2017,3,20,3,60bmz8,[N] A citizen proposal to support research in Artificial Intelligence in Madrid,https://www.reddit.com/r/MachineLearning/comments/60bmz8/n_a_citizen_proposal_to_support_research_in/,nambafaifu,1489949144,"In Madrid there is an ongoing process of participatory budgets, where anyone can submit a project and the ones with more support will be done (within a budget of 100 million euros). There is one proposal for the promotion of open source Artificial Intelligence research projects:  [https://decide.madrid.es/presupuestos/presupuestos-participativos-2017/proyecto/2312](https://decide.madrid.es/presupuestos/presupuestos-participativos-2017/proyecto/2312)

  

If you live in Madrid, give it your support! :)",2,27
640,2017-3-20,2017,3,20,4,60bvzl,Why we increase the neural network size when it stuck at local minima?,https://www.reddit.com/r/MachineLearning/comments/60bvzl/why_we_increase_the_neural_network_size_when_it/,John_Smith111,1489951899,[removed],1,1
641,2017-3-20,2017,3,20,4,60c0cz,DenseReg: Fully Convolutional Dense Shape Regression In-the-Wild,https://www.reddit.com/r/MachineLearning/comments/60c0cz/densereg_fully_convolutional_dense_shape/,Neural_Ned,1489953219,,1,1
642,2017-3-20,2017,3,20,4,60c0oe,[D] How do I obtain a gradient function for a custom loss function (sparse filtering) so that I can use L-BFGS?,https://www.reddit.com/r/MachineLearning/comments/60c0oe/d_how_do_i_obtain_a_gradient_function_for_a/,[deleted],1489953327,[removed],0,1
643,2017-3-20,2017,3,20,5,60c451,[D] So your company wants to do AI?,https://www.reddit.com/r/MachineLearning/comments/60c451/d_so_your_company_wants_to_do_ai/,kevinzakka,1489954388,,6,13
644,2017-3-20,2017,3,20,5,60c7do,What can I do to get as fast a performance I can from a pure python implementation in general (preferably Jupyter)?,https://www.reddit.com/r/MachineLearning/comments/60c7do/what_can_i_do_to_get_as_fast_a_performance_i_can/,bjabr,1489955394,[removed],0,1
645,2017-3-20,2017,3,20,5,60c9nh,Why AlphaGo Is Not AI,https://www.reddit.com/r/MachineLearning/comments/60c9nh/why_alphago_is_not_ai/,sour_losers,1489956107,,0,1
646,2017-3-20,2017,3,20,5,60cby3,[P] I had to make a Teeth detector using ConvNets in Caffe so I published a blog post about it,https://www.reddit.com/r/MachineLearning/comments/60cby3/p_i_had_to_make_a_teeth_detector_using_convnets/,[deleted],1489956823,[deleted],0,1
647,2017-3-20,2017,3,20,6,60cdy1,[P] I had to make a Teeth detector using ConvNets in Caffe so I published a blog post about it,https://www.reddit.com/r/MachineLearning/comments/60cdy1/p_i_had_to_make_a_teeth_detector_using_convnets/,juank334,1489957437,,11,12
648,2017-3-20,2017,3,20,6,60cj2m,A JVM implementation of the Pair Adjacent Violators algorithm for isotonic regression,https://www.reddit.com/r/MachineLearning/comments/60cj2m/a_jvm_implementation_of_the_pair_adjacent/,sanity,1489959085,,0,1
649,2017-3-20,2017,3,20,7,60covg,[D] Explanation of DeepMind's Overcoming Catastrophic Forgetting,https://www.reddit.com/r/MachineLearning/comments/60covg/d_explanation_of_deepminds_overcoming/,RSchaeffer,1489960918,,54,199
650,2017-3-20,2017,3,20,7,60cu9k,What does a Reinforcement Learning solution look like?,https://www.reddit.com/r/MachineLearning/comments/60cu9k/what_does_a_reinforcement_learning_solution_look/,i_reddit_too_mcuh,1489962548,[removed],0,1
651,2017-3-20,2017,3,20,7,60cvxu,A Tutorial on Automatic Differentiation and Backpropagation,https://www.reddit.com/r/MachineLearning/comments/60cvxu/a_tutorial_on_automatic_differentiation_and/,pegasos1,1489963087,,0,1
652,2017-3-20,2017,3,20,8,60d4w2,Silly (easy) machine learning question,https://www.reddit.com/r/MachineLearning/comments/60d4w2/silly_easy_machine_learning_question/,[deleted],1489965937,[removed],0,1
653,2017-3-20,2017,3,20,8,60d86r,Any modern `Systems Biology` books using Python 3?,https://www.reddit.com/r/MachineLearning/comments/60d86r/any_modern_systems_biology_books_using_python_3/,o-rka,1489967042,[removed],0,1
654,2017-3-20,2017,3,20,13,60elni,"[D] Is there a good way to ""learn"" weight sharing?",https://www.reddit.com/r/MachineLearning/comments/60elni/d_is_there_a_good_way_to_learn_weight_sharing/,kh40tika,1489985061,"Depending on the data input, we apply different weight sharing schemes to make networks generalize better. For 2D images, we apply 2D convnets. For voxel data, we apply 3D convnet. We also have RNNs share weights in temporal domain.

Now then, assume the structure of the data is *unknown*, all we have is a high dimensional vector. It's probably a multi channel image, a spectrogram, or some program running trace. But assuming the data have inherent structures that can reduce parameter count, can we build a model that:

* 1) Can learn the structure of the data.

* 2) Can exploit the learned structure to reduce parameter count.

* 3) Can also reduce computational cost as more structure is learned.

Extra: represent the ""structure"" in a continuous way, so that the parameters won't be ""violently"" removed. SGD would be happier with this, 1) and 2) can also proceed at the same time during training.

I gave this some thought however failed to come up with anything elegant, yet.

1) Seems simple, applying something like t-SNE to the correlation matrix would yield some ""structure"".

2) Is rather tricky. IMHO convolution theorem could be useful. Since conv(x, y) == IFT( FT(x) * FT(y) ), all that needed is to learn the fourier basis for the unknown underlying ""structure"", then the weights can be shared in an implicit manner. However I didn't have time think deeper or implement it. It may have some problems.

I have no idea about 3).

Any thoughts?",5,7
655,2017-3-20,2017,3,20,14,60evav,Favorite Survey Papers?,https://www.reddit.com/r/MachineLearning/comments/60evav/favorite_survey_papers/,[deleted],1489989378,[removed],0,1
656,2017-3-20,2017,3,20,14,60evf4,Delinquency Forecasting Using Transition Matrices (Markov Chain Modeling) - question,https://www.reddit.com/r/MachineLearning/comments/60evf4/delinquency_forecasting_using_transition_matrices/,zad0xlik,1489989434,[removed],0,1
657,2017-3-20,2017,3,20,15,60evwt,40 Must know Questions to test a data scientist on Dimensionality Reduction techniques,https://www.reddit.com/r/MachineLearning/comments/60evwt/40_must_know_questions_to_test_a_data_scientist/,ankit_123,1489989659,,0,1
658,2017-3-20,2017,3,20,15,60extq,[D] Favorite Survey Papers?,https://www.reddit.com/r/MachineLearning/comments/60extq/d_favorite_survey_papers/,djgvolt,1489990568,"What are some of your favorite survey papers, preferably in some of the more exotic and new fields in ML?
",6,17
659,2017-3-20,2017,3,20,15,60ez6u,Recursive Pattern Producing Networks,https://www.reddit.com/r/MachineLearning/comments/60ez6u/recursive_pattern_producing_networks/,ciolaamotore,1489991267,,0,1
660,2017-3-20,2017,3,20,16,60f624,[D] Definitive guide to backpropagation?,https://www.reddit.com/r/MachineLearning/comments/60f624/d_definitive_guide_to_backpropagation/,nivm321,1489994731,Can someone point me to a good resource for backprop? The ones on YouTube are aimed at beginners and have less math. I want to have a deep understanding of it so I need a math-heavy and proper resource. Thanks,6,2
661,2017-3-20,2017,3,20,16,60f7b6,[R] Dataset Recommendations for a Deep Neural Network Classifier,https://www.reddit.com/r/MachineLearning/comments/60f7b6/r_dataset_recommendations_for_a_deep_neural/,ProjectPsygma,1489995426,"Hello r/MachineLearning,

First post on this subreddit! I was hoping to incite some discussion on technical aspects pertaining to my undergraduate thesis. In short, my project involves building and training a classification model to categorise recyclable items into 4 labels; plastics, glass, paper, and general waste (if it doesn't fall into the former 3 categories)

The current sensor data streams that I have access to is a camera (image recognition), microphone (acoustic analysis), load cells (basically a scale for a weight reading), and a metal detector (detects changing frequencies with nearby inductive metals). I am planning to extract features from these data streams and convert them into Tensors to run through a TensorFlow computational graph which will be structured as a deep or convolutional neural network.  

My questions for discussion are: 
1.) What feature extraction/preprocessing should I perform on the four data streams to create the best dataset?
2.) Approximately how many samples will I need in my dataset to train a reasonably accurate neural network (say above 95% prediction accuracy)?

Thanks in advance!",3,1
662,2017-3-20,2017,3,20,16,60f8sl,OC - texture-nets video,https://www.reddit.com/r/MachineLearning/comments/60f8sl/oc_texturenets_video/,kikoolol666,1489996269,,0,1
663,2017-3-20,2017,3,20,16,60f9lb,[D] Best ML Dev Database?,https://www.reddit.com/r/MachineLearning/comments/60f9lb/d_best_ml_dev_database/,blowjobtransistor,1489996740,"Using a database for managing data for machine learning during development is great for a few reasons:

- Indexing data at rest is easy, giving you easy access to specific data on disk (not leaving it in memory)
- Tables are a natural way to create versioned datasets from raw data
- SQL covers most feature engineering cases

Do you use a database for data management when developing ML solutions?  Which database?

I've been using postgres for the last few projects due to its broad capabilities and my experience with running it in the past, but I'd love to hear what other people use!",1,2
664,2017-3-20,2017,3,20,17,60fczp,[D] Using dropout on embeddings,https://www.reddit.com/r/MachineLearning/comments/60fczp/d_using_dropout_on_embeddings/,Jean-Porte,1489998626,"I sometimes, see dropout applied to embeddings, but it doesn't make sense to me.
Dropout is allowing an implicit ensemble of models, but if we average many different learned embeddings it will only harm results, and my experiences only confirmed this
I also think using dropout in layers near the embedding also harm
Am I missing something ?
Thanks",2,0
665,2017-3-20,2017,3,20,18,60fh47,[R] ShaResNet: reducing residual network parameter number by sharing weights,https://www.reddit.com/r/MachineLearning/comments/60fh47/r_sharesnet_reducing_residual_network_parameter/,xternalz,1490000941,,4,0
666,2017-3-20,2017,3,20,18,60fhyb,[R] Opening the Black Box of Deep Neural Networks via Information,https://www.reddit.com/r/MachineLearning/comments/60fhyb/r_opening_the_black_box_of_deep_neural_networks/,machiner_ps,1490001397,,8,43
667,2017-3-20,2017,3,20,18,60fj6m,Looking for introduction to Statistics,https://www.reddit.com/r/MachineLearning/comments/60fj6m/looking_for_introduction_to_statistics/,send_me_nudes_asap,1490002096,[removed],0,1
668,2017-3-20,2017,3,20,19,60fn6a,Is Elementary OS 0.4 LOki suitable for machine learning?,https://www.reddit.com/r/MachineLearning/comments/60fn6a/is_elementary_os_04_loki_suitable_for_machine/,[deleted],1490004226,[removed],0,1
669,2017-3-20,2017,3,20,19,60ftbu,"What are the best free, interactive resources to learn Deep Learning with Python/Tensorflow?",https://www.reddit.com/r/MachineLearning/comments/60ftbu/what_are_the_best_free_interactive_resources_to/,Estiui,1490007346,[removed],0,1
670,2017-3-20,2017,3,20,20,60fxut,[D] Fantastic GANs and where to find them,https://www.reddit.com/r/MachineLearning/comments/60fxut/d_fantastic_gans_and_where_to_find_them/,Guim30,1490009426,,10,79
671,2017-3-20,2017,3,20,20,60fzjg,Beating the Perils of Non-Convexity in Neural Nets,https://www.reddit.com/r/MachineLearning/comments/60fzjg/beating_the_perils_of_nonconvexity_in_neural_nets/,nikitaljohnson,1490010053,,0,1
672,2017-3-20,2017,3,20,20,60fzpn,[D] Which business problems can be easily solved by Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/60fzpn/d_which_business_problems_can_be_easily_solved_by/,[deleted],1490010116,[deleted],0,1
673,2017-3-20,2017,3,20,20,60g1s8,Algorithm based on machine learning is able to select an audience more loyal to the online store using only publicly available posts.,https://www.reddit.com/r/MachineLearning/comments/60g1s8/algorithm_based_on_machine_learning_is_able_to/,mahnunchik,1490010885,,0,1
674,2017-3-20,2017,3,20,21,60g8g1,expLOD: A Framework for Explaining Recommendations based on the Linked Open Data Cloud,https://www.reddit.com/r/MachineLearning/comments/60g8g1/explod_a_framework_for_explaining_recommendations/,itkk94,1490013369,[removed],0,1
675,2017-3-20,2017,3,20,21,60gb4y,[D] Multi-label classification: Predict product category,https://www.reddit.com/r/MachineLearning/comments/60gb4y/d_multilabel_classification_predict_product/,Kapsalon21,1490014365,"I want to predict to which product category a product belongs. A total of 400k products need to be translated from the old (less refined) to the new product category tree. (E.g. alarm clock used to fall under 'Electronics' and will now belong to 'Alarm clocks'.) So far 36k products have already been partly allocated to ~400 (out of 800) new product categories. The filling rate ranges from 1% to 95%.   

Product data (among others) contains variables: name, description, price, dimensions, color and the old label . The idea was to construct features out of the unstructured variables through tokenisation -&gt; TF-IDF. 

Proposed Approach:

 1. Train one multi-label prediction model (e.g. Ridge classification + cross-validation + stratified CV) on the labeled data. Then predict the category only for subset that, based on the old product tree, contains all possible products. (e.g. predict if unlabelled 'Electronics' products are 'Alarm clocks')
 2. Based on the predicted probability present the unlabelled product to a content manager that, if labelled, would result in the highest information gain.
 3. Propose to which extend the remaining 400 categories should be filled (e.g. 60%) and which products to label first. 

What would your preferred approach be?",1,0
676,2017-3-20,2017,3,20,22,60gh7j,[D] What topics in Statistics are important(or widely used) in Datascience?,https://www.reddit.com/r/MachineLearning/comments/60gh7j/d_what_topics_in_statistics_are_importantor/,datavinci,1490016479,"I have searched on multiple sites like Quora and here on reddit too, but can't seem to find an exhaustive list.
I would prefer to know a list of topics like one way ANOVA ,one tailed t tests etc. which are often used in datascience and are important for it. I am looking to create an exhaustive list for my and everybody's reference.

**Note:** *I am extremely sorry if this has already been asked. But I am looking to create an exhaustive list of topics since I can see plethora of topics but can't seem to prioritize on important ones.*",5,1
677,2017-3-20,2017,3,20,23,60grin,MY LM MT KHNG KH DAIKIO DK-7000B,https://www.reddit.com/r/MachineLearning/comments/60grin/my_lm_mt_khng_kh_daikio_dk7000b/,dailuuthong,1490019750,,0,1
678,2017-3-20,2017,3,20,23,60gut6,A simple machine learning project in F#,https://www.reddit.com/r/MachineLearning/comments/60gut6/a_simple_machine_learning_project_in_f/,[deleted],1490020761,[deleted],0,1
679,2017-3-20,2017,3,20,23,60gwzj,[P] A simple machine learning project in F#,https://www.reddit.com/r/MachineLearning/comments/60gwzj/p_a_simple_machine_learning_project_in_f/,vincent_12345,1490021382,,0,4
680,2017-3-21,2017,3,21,0,60h2rd,Momentum &amp; adaptive SGD algorithms VS very sparse inputs in GLMs,https://www.reddit.com/r/MachineLearning/comments/60h2rd/momentum_adaptive_sgd_algorithms_vs_very_sparse/,gr8ape,1490022975,[removed],0,1
681,2017-3-21,2017,3,21,0,60h2w2,where can I find an archive of image tags?,https://www.reddit.com/r/MachineLearning/comments/60h2w2/where_can_i_find_an_archive_of_image_tags/,[deleted],1490023017,[removed],0,1
682,2017-3-21,2017,3,21,2,60hx6k,[D] Theory behind inputs and ReLU vs Sigmoid,https://www.reddit.com/r/MachineLearning/comments/60hx6k/d_theory_behind_inputs_and_relu_vs_sigmoid/,encore2097,1490031303,"**1) Why are neural network inputs confined to [-1, 1] or [0, 1], the latter seems to be experimentally more effective for convolutional networks.**



Part of the answer here from my reading is that inputs are all scaled to prevent bias on one input vs the other and have the weights determine which has a high affect on the output. Is there any theory for scaling to the unit range? I understand normalization is done to make the data as linearly separable as possible and similarly re-scaled on output to get meaningful numbers to the problem.



**2) Why does a ReLU -&gt; [0,x) work better vs a sigmoid [-1, 1]? How does this effect negative weights and inputs?**



My intuition here is simplicity by reducing range of freedom in the activation layer and increasing range in the weights, it simplifies back prop as the derivate of ReLU is 1 vs increased complexity for sigmoid.

This would also simplify training of the weight by reducing the target range for the activation layer. 

    i.e.  if my input x * weights (bias omitted) =&gt; activation

for sigmoid the valid activations are:

    (- input) * (- weight)  = (+)
    (- input) * (+ weight) = (-)
    (+ input) * (- weight) = (-) 
    (+ input) * (+ weight) = (+)
 

for ReLU the valid activations are:

    (- input) * (- weight)  = (+)
    (+ input) * (+ weight) = (+)

Plus training gets simpler as you add layers as you are only dealing with hidden inputs 0 to positive numbers. There are other complexities like vanishing gradient on sigmoid and ReLU blow ups, but thats fairly well covered in recent material.


Putting this all together:

**3a) Say you have real world inputs ranging from -100 to 100 and you normalize, is it better to go [-1, 1] or [0, 1]?**

Aside from experimentally determining it is there any theory?

**3b) Same question as 3a but now you have multiple signals that are different, like -100 to 100, 0-10 (fixed range) and -inf to inf, 0 to inf (inf range)?**

",8,16
683,2017-3-21,2017,3,21,2,60hy0t,"The journal Distill launches today. In a nutshell, Distill is an interactive, visual journal for machine learning research.",https://www.reddit.com/r/MachineLearning/comments/60hy0t/the_journal_distill_launches_today_in_a_nutshell/,finallyifoundvalidUN,1490031525,,41,387
684,2017-3-21,2017,3,21,2,60i13f,"[R] Distill -- a visual, interactive journal for machine learning research emphasizing human understanding",https://www.reddit.com/r/MachineLearning/comments/60i13f/r_distill_a_visual_interactive_journal_for/,downtownslim,1490032361,,0,34
685,2017-3-21,2017,3,21,3,60i3bi,[D] Which GPU(s) to Get for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/60i3bi/d_which_gpus_to_get_for_deep_learning/,clbam8,1490032980,,14,20
686,2017-3-21,2017,3,21,3,60i8yw,[R] Exponential Memory,https://www.reddit.com/r/MachineLearning/comments/60i8yw/r_exponential_memory/,CireNeikual,1490034434,,20,5
687,2017-3-21,2017,3,21,3,60iazs,The absolute easiest way to get started with ML to build and deploy a model.,https://www.reddit.com/r/MachineLearning/comments/60iazs/the_absolute_easiest_way_to_get_started_with_ml/,tarpus,1490034950,,0,1
688,2017-3-21,2017,3,21,4,60ilbf,Hardware to Insanely Massively Accelerate Deep Learning,https://www.reddit.com/r/MachineLearning/comments/60ilbf/hardware_to_insanely_massively_accelerate_deep/,kemmishtree,1490037696,[removed],0,1
689,2017-3-21,2017,3,21,4,60ildf,[D] Machine Learning - WAYR (What Are You Reading) - Week 21,https://www.reddit.com/r/MachineLearning/comments/60ildf/d_machine_learning_wayr_what_are_you_reading_week/,Mandrathax,1490037715,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|
|----|-----|
|[Week 1](https://www.reddit.com/r/MachineLearning/comments/4qyjiq/machine_learning_wayr_what_are_you_reading_week_1/)|[Week 11](https://www.reddit.com/r/MachineLearning/comments/57xw56/discussion_machine_learning_wayr_what_are_you/)|
|[Week 2](https://www.reddit.com/r/MachineLearning/comments/4s2xqm/machine_learning_wayr_what_are_you_reading_week_2/)|[Week 12](https://www.reddit.com/r/MachineLearning/comments/5acb1t/d_machine_learning_wayr_what_are_you_reading_week/)|
|[Week 3](https://www.reddit.com/r/MachineLearning/comments/4t7mqm/machine_learning_wayr_what_are_you_reading_week_3/)|[Week 13](https://www.reddit.com/r/MachineLearning/comments/5cwfb6/d_machine_learning_wayr_what_are_you_reading_week/)|
|[Week 4](https://www.reddit.com/r/MachineLearning/comments/4ub2kw/machine_learning_wayr_what_are_you_reading_week_4/)|[Week 14](https://www.reddit.com/r/MachineLearning/comments/5fc5mh/d_machine_learning_wayr_what_are_you_reading_week/)|
|[Week 5](https://www.reddit.com/r/MachineLearning/comments/4xomf7/machine_learning_wayr_what_are_you_reading_week_5/)|[Week 15](https://www.reddit.com/r/MachineLearning/comments/5hy4ur/d_machine_learning_wayr_what_are_you_reading_week/)|
|[Week 6](https://www.reddit.com/r/MachineLearning/comments/4zcyvk/machine_learning_wayr_what_are_you_reading_week_6/)|[Week 16](https://www.reddit.com/r/MachineLearning/comments/5kd6vd/d_machine_learning_wayr_what_are_you_reading_week/)|
|[Week 7](https://www.reddit.com/r/MachineLearning/comments/52t6mo/machine_learning_wayr_what_are_you_reading_week_7/)|[Week 17](https://www.reddit.com/r/MachineLearning/comments/5ob7dx/discussion_machine_learning_wayr_what_are_you/)|
|[Week 8](https://www.reddit.com/r/MachineLearning/comments/53heol/machine_learning_wayr_what_are_you_reading_week_8/)|[Week 18](https://www.reddit.com/r/MachineLearning/comments/5r14yd/discussion_machine_learning_wayr_what_are_you/)|
|[Week 9](https://www.reddit.com/r/MachineLearning/comments/54kvsu/machine_learning_wayr_what_are_you_reading_week_9/)|[Week 19](https://www.reddit.com/r/MachineLearning/comments/5tt9cz/discussion_machine_learning_wayr_what_are_you/)|
|[Week 10](https://www.reddit.com/r/MachineLearning/comments/56s2oa/discussion_machine_learning_wayr_what_are_you/)|[Week 20](https://www.reddit.com/r/MachineLearning/comments/5wh2wb/d_machine_learning_wayr_what_are_you_reading_week/)|

Most upvoted paper last week :

[EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces](https://arxiv.org/abs/1611.08024)

[Deep Feature Learning for EEG Recordings](https://arxiv.org/abs/1511.04306)

Besides that, there are no rules, have fun.
",17,31
690,2017-3-21,2017,3,21,4,60imog,[D] Why code is not required to publish when publishing a paper ?,https://www.reddit.com/r/MachineLearning/comments/60imog/d_why_code_is_not_required_to_publish_when/,xingdongrobotics,1490038067,"Sorry for this stupid question. In ML community, it is often the case that only the paper to be published, the some authors shall release their code publicly after some time, and some don't. It is a bit confusing why the code is not required to supplement the paper when publishing it ? Is there a concern ?",20,26
691,2017-3-21,2017,3,21,4,60iq3a,[P] HW to Insanely Massively Accelerate Deep Learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/60iq3a/p_hw_to_insanely_massively_accelerate_deep/,kemmishtree,1490038971,"I'm working on a stealth, VC-backed project building novel computational architecture (extremely well-vetted and proven, but not silicon based!) to provide insane-sounding accelerations (as in, you'd be foolish to believe our claims immediately, but also foolish not to believe after a deep dive...) in the time to train extremely large artificial neural networks. We're looking for genuinely world-class, math-strong, profoundly creative and thoughtful DL researchers who've worked on and innovated in fundamental aspects of NNs. The right people will be strongly techno-FOMO-motivated--imagine getting to play with transistors when the rest of the world is stuck with vacuum tubes. PM me for more info! ",18,0
692,2017-3-21,2017,3,21,4,60ir6s,SCBOW in Keras,https://www.reddit.com/r/MachineLearning/comments/60ir6s/scbow_in_keras/,domarps123,1490039257,[removed],0,1
693,2017-3-21,2017,3,21,5,60ixz9,Publishing in the Distill Research Journal,https://www.reddit.com/r/MachineLearning/comments/60ixz9/publishing_in_the_distill_research_journal/,[deleted],1490041068,[deleted],0,1
694,2017-3-21,2017,3,21,6,60j85d,[N] Galvanize course for IBM Watson APIs,https://www.reddit.com/r/MachineLearning/comments/60j85d/n_galvanize_course_for_ibm_watson_apis/,rhy0lite,1490043782,,0,0
695,2017-3-21,2017,3,21,6,60jfse,Linear algebra cheat sheet for deep learning [P],https://www.reddit.com/r/MachineLearning/comments/60jfse/linear_algebra_cheat_sheet_for_deep_learning_p/,[deleted],1490045835,[deleted],0,1
696,2017-3-21,2017,3,21,8,60k5s6,[D] Adversarial Autoencoder (with Pytorch),https://www.reddit.com/r/MachineLearning/comments/60k5s6/d_adversarial_autoencoder_with_pytorch/,ps_dillon,1490053242,,4,14
697,2017-3-21,2017,3,21,8,60k77j,[P] Linear algebra cheat sheet for deep learning,https://www.reddit.com/r/MachineLearning/comments/60k77j/p_linear_algebra_cheat_sheet_for_deep_learning/,jeremyhoward,1490053660,,41,108
698,2017-3-21,2017,3,21,9,60kkmc,"""Deformable Convolutional Networks"" from MSRA",https://www.reddit.com/r/MachineLearning/comments/60kkmc/deformable_convolutional_networks_from_msra/,[deleted],1490057707,[deleted],0,1
699,2017-3-21,2017,3,21,10,60kr4t,"[R] ""Deformable Convolutional Networks"" from MSRA",https://www.reddit.com/r/MachineLearning/comments/60kr4t/r_deformable_convolutional_networks_from_msra/,flyforlight,1490059790,,21,46
700,2017-3-21,2017,3,21,11,60l3rw,Build your own Face Recognition API,https://www.reddit.com/r/MachineLearning/comments/60l3rw/build_your_own_face_recognition_api/,lehoangduc,1490063840,,0,1
701,2017-3-21,2017,3,21,13,60lhpd,Brief History of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/60lhpd/brief_history_of_machine_learning/,duanyanbiao,1490068896,,1,1
702,2017-3-21,2017,3,21,13,60lp6v,[P] Image Classification with Regex-based Features,https://www.reddit.com/r/MachineLearning/comments/60lp6v/p_image_classification_with_regexbased_features/,allenguo,1490071903,,1,1
703,2017-3-21,2017,3,21,14,60ly7l,Tutorial on Hardware Architecture for Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/60ly7l/tutorial_on_hardware_architecture_for_deep_neural/,starstorm312,1490075978,,0,1
704,2017-3-21,2017,3,21,15,60m4t7,Help understanding community detection (clustering) work by Emmanuel Abbe et al?,https://www.reddit.com/r/MachineLearning/comments/60m4t7/help_understanding_community_detection_clustering/,[deleted],1490079223,[removed],0,1
705,2017-3-21,2017,3,21,15,60m4x2,[R] Help understanding the work done by Emmanuel Abbe (et al) in community detection/clustering?,https://www.reddit.com/r/MachineLearning/comments/60m4x2/r_help_understanding_the_work_done_by_emmanuel/,dataislyfe,1490079280,"I am trying to understand and implement the algorithm E. Abbe et al present in https://arxiv.org/pdf/1503.00609.pdf. It is a bit unclear to me and I was hoping to find someone here who might be familiar with the work. Please pm or comment below if you do; otherwise any other help would be so appreciated!
",0,2
706,2017-3-21,2017,3,21,15,60m4xs,[D] Good source of open medicine/health data for training a network?,https://www.reddit.com/r/MachineLearning/comments/60m4xs/d_good_source_of_open_medicinehealth_data_for/,avjr,1490079293,"Where could one get images or text data for training a deep learning network, similar to http://news.stanford.edu/2017/01/25/artificial-intelligence-used-identify-skin-cancer/?",5,2
707,2017-3-21,2017,3,21,16,60mbq7,Distill is dedicated to making machine learning clear and dynamic,https://www.reddit.com/r/MachineLearning/comments/60mbq7/distill_is_dedicated_to_making_machine_learning/,eleitl,1490082883,,0,1
708,2017-3-21,2017,3,21,17,60mcdh,Looking for A.I expert to join my startup. (Product market fit achieved),https://www.reddit.com/r/MachineLearning/comments/60mcdh/looking_for_ai_expert_to_join_my_startup_product/,JukeBoxFromTheFuture,1490083257,[removed],0,1
709,2017-3-21,2017,3,21,17,60me6w,[D] Alpha Go. Neither AI nor Strong AI. Till it is embodied on a robot.,https://www.reddit.com/r/MachineLearning/comments/60me6w/d_alpha_go_neither_ai_nor_strong_ai_till_it_is/,nocortex,1490084276,http://spectrum.ieee.org/automaton/robotics/artificial-intelligence/why-alphago-is-not-ai,19,0
710,2017-3-21,2017,3,21,17,60mi3o,[R] Norm-preserving Orthogonal Permutation Linear Unit Activation Functions (OPLU),https://www.reddit.com/r/MachineLearning/comments/60mi3o/r_normpreserving_orthogonal_permutation_linear/,TheFlyingDrildo,1490086542,,11,7
711,2017-3-21,2017,3,21,19,60mqbw,Beating the Perils of Non-Convexity in Neural Nets,https://www.reddit.com/r/MachineLearning/comments/60mqbw/beating_the_perils_of_nonconvexity_in_neural_nets/,reworksophie,1490090909,,0,1
712,2017-3-21,2017,3,21,19,60ms1j,Code for MIST RNNs and all experiments in the paper,https://www.reddit.com/r/MachineLearning/comments/60ms1j/code_for_mist_rnns_and_all_experiments_in_the/,[deleted],1490091711,[deleted],1,1
713,2017-3-21,2017,3,21,20,60n33v,[D] Why NuPIC is the ugliest Machine Learning Framework?,https://www.reddit.com/r/MachineLearning/comments/60n33v/d_why_nupic_is_the_ugliest_machine_learning/,nocortex,1490096618,"https://github.com/numenta/nupic 

I conclude the current level of the complexity and learning curve of the NuPIC/ HTM while comparing with other frameworks (TensorFlow, Keras, theano) ",4,0
714,2017-3-21,2017,3,21,21,60n5oc,"Voice Is the Next Big Platform, Unless You Have an Accent",https://www.reddit.com/r/MachineLearning/comments/60n5oc/voice_is_the_next_big_platform_unless_you_have_an/,dougiebuckets,1490097619,,0,1
715,2017-3-21,2017,3,21,21,60n8qy,[P] Evolution Strategies in PyTorch,https://www.reddit.com/r/MachineLearning/comments/60n8qy/p_evolution_strategies_in_pytorch/,gambs,1490098729,,6,31
716,2017-3-21,2017,3,21,21,60n8wo,[R] Code for MIST RNNs and all experiments in the paper,https://www.reddit.com/r/MachineLearning/comments/60n8wo/r_code_for_mist_rnns_and_all_experiments_in_the/,r_dipietro,1490098791,,8,5
717,2017-3-21,2017,3,21,21,60napu,NOOB-HELP:Building a clustering model &amp; recommendation engine,https://www.reddit.com/r/MachineLearning/comments/60napu/noobhelpbuilding_a_clustering_model/,xen-m-rph,1490099482,[removed],0,1
718,2017-3-21,2017,3,21,21,60ncxs,ETKETLEME MAKNASI ETLER,https://www.reddit.com/r/MachineLearning/comments/60ncxs/etiketleme_makinasi_eitleri/,renasmakinatr,1490100295,[removed],0,1
719,2017-3-21,2017,3,21,22,60nle0,[Q] PPCs and GANs,https://www.reddit.com/r/MachineLearning/comments/60nle0/q_ppcs_and_gans/,[deleted],1490103229,[removed],0,1
720,2017-3-21,2017,3,21,23,60nsab,Building many-feature SVM with NLP dataset,https://www.reddit.com/r/MachineLearning/comments/60nsab/building_manyfeature_svm_with_nlp_dataset/,ValarMorghulis6626,1490105404,[removed],0,1
721,2017-3-21,2017,3,21,23,60nupk,Find features that nullifies the effect of other features.,https://www.reddit.com/r/MachineLearning/comments/60nupk/find_features_that_nullifies_the_effect_of_other/,brijeshdankhara,1490106145,[removed],0,1
722,2017-3-21,2017,3,21,23,60o116,How to Code and Understand DeepMind's Synthetic Gradients,https://www.reddit.com/r/MachineLearning/comments/60o116/how_to_code_and_understand_deepminds_synthetic/,iamtrask,1490108058,,0,1
723,2017-3-21,2017,3,21,23,60o1j6,What Can We Expect From Chat Bots in 2017,https://www.reddit.com/r/MachineLearning/comments/60o1j6/what_can_we_expect_from_chat_bots_in_2017/,bogsformer,1490108198,,0,4
724,2017-3-22,2017,3,22,0,60o2or,Good website or program to learn machine learning from scratch.,https://www.reddit.com/r/MachineLearning/comments/60o2or/good_website_or_program_to_learn_machine_learning/,jbuyl,1490108526,[removed],0,1
725,2017-3-22,2017,3,22,0,60o8yf,Best 60 AI and chatbot conferences in 2017,https://www.reddit.com/r/MachineLearning/comments/60o8yf/best_60_ai_and_chatbot_conferences_in_2017/,sprinter_,1490110303,,0,1
726,2017-3-22,2017,3,22,1,60ohw4,[P] Google Cloud Machine Learning APIs with Python tutorials,https://www.reddit.com/r/MachineLearning/comments/60ohw4/p_google_cloud_machine_learning_apis_with_python/,sentdex,1490112811,"After being curious about Google Cloud for a while, I decided to dig in and see what I thought, and have created a tutorial mini-series on using some of the Google Cloud APIs with Python.

**Requirements:**

* In order to make use of these machine learning APIs, you do not need to know anything technical about machine learning, you should just know the basics of Python.

* A Google Cloud account. If you're signing up for the first time, you will get $300 in credits. To follow along, you will use no more than maybe a dollar if you use the shared CPU option (more than enough since we're not doing any real processing on our end), so you should have lots of credits left over.

**The covered topics:**

1. [Launching a virtual machine](https://pythonprogramming.net/virtual-machine-google-cloud-tutorial/)...which we'll be using for the rest of the series to use the APIs.

2. [Setting up API requirements, and intro to Vision API](https://pythonprogramming.net/vision-api-intro-google-cloud-tutorial/)

3. [More on the Vision API](https://pythonprogramming.net/vision-api-intro-google-cloud-tutorial/)

4. [Natural Language API](https://pythonprogramming.net/natural-language-api-google-cloud-tutorial/)

5. [Translation API](https://pythonprogramming.net/translation-api-google-cloud-tutorial/)",4,95
727,2017-3-22,2017,3,22,1,60oqcg,sending jobs to different gpu's with tensorflow. How to ?,https://www.reddit.com/r/MachineLearning/comments/60oqcg/sending_jobs_to_different_gpus_with_tensorflow/,pl47,1490115086,[removed],0,1
728,2017-3-22,2017,3,22,1,60oqh1,[D] Expressing emotions in text,https://www.reddit.com/r/MachineLearning/comments/60oqh1/d_expressing_emotions_in_text/,[deleted],1490115122,[deleted],2,0
729,2017-3-22,2017,3,22,2,60ozfp,A Practical Guide for Debugging Tensorflow Codes,https://www.reddit.com/r/MachineLearning/comments/60ozfp/a_practical_guide_for_debugging_tensorflow_codes/,cool_penguins,1490117528,,0,1
730,2017-3-22,2017,3,22,2,60p36p,[R][1703.06870] Mask R-CNN,https://www.reddit.com/r/MachineLearning/comments/60p36p/r170306870_mask_rcnn/,m_ke,1490118494,,4,38
731,2017-3-22,2017,3,22,4,60plbf,Transfer Learning - Machine Learning's Next Frontier,https://www.reddit.com/r/MachineLearning/comments/60plbf/transfer_learning_machine_learnings_next_frontier/,[deleted],1490123322,[deleted],0,1
732,2017-3-22,2017,3,22,4,60pmfs,X-post from /r/austin: Who's recently attended any of the data science boot camps in Austin? What's your feedback?,https://www.reddit.com/r/MachineLearning/comments/60pmfs/xpost_from_raustin_whos_recently_attended_any_of/,1dimensioner,1490123604,[removed],0,1
733,2017-3-22,2017,3,22,9,60rll0,Transfer Learning - Machine Learning's Next Frontier,https://www.reddit.com/r/MachineLearning/comments/60rll0/transfer_learning_machine_learnings_next_frontier/,[deleted],1490144156,[deleted],0,1
734,2017-3-22,2017,3,22,10,60rscs,[R] SORT: Second-Order Response Transform for Visual Recognition (multiplicative term in addition to ResNet's additive term),https://www.reddit.com/r/MachineLearning/comments/60rscs/r_sort_secondorder_response_transform_for_visual/,xternalz,1490146370,,3,1
735,2017-3-22,2017,3,22,11,60s2w3,Anyone using Edward in production?,https://www.reddit.com/r/MachineLearning/comments/60s2w3/anyone_using_edward_in_production/,MasterEpictetus,1490149895,[removed],0,1
736,2017-3-22,2017,3,22,12,60sdpd,[1703.06891] Dance Dance Convolution,https://www.reddit.com/r/MachineLearning/comments/60sdpd/170306891_dance_dance_convolution/,aeuc,1490153667,,1,1
737,2017-3-22,2017,3,22,12,60sgqb,What is the mixing effect of different colors materials by planetary dou...,https://www.reddit.com/r/MachineLearning/comments/60sgqb/what_is_the_mixing_effect_of_different_colors/,[deleted],1490154806,[deleted],0,1
738,2017-3-22,2017,3,22,13,60sier,[N] Andrew Ng resigning from Baidu,https://www.reddit.com/r/MachineLearning/comments/60sier/n_andrew_ng_resigning_from_baidu/,clbam8,1490155434,,159,425
739,2017-3-22,2017,3,22,13,60sigv,Where can I find various datasets for machine learning? Please let me know some usefull websites!,https://www.reddit.com/r/MachineLearning/comments/60sigv/where_can_i_find_various_datasets_for_machine/,CommonsenseKR,1490155454,[removed],0,1
740,2017-3-22,2017,3,22,13,60sli2,[1703.06868] Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization,https://www.reddit.com/r/MachineLearning/comments/60sli2/170306868_arbitrary_style_transfer_in_realtime/,[deleted],1490156672,[deleted],0,1
741,2017-3-22,2017,3,22,13,60sm3m,"Machine learning app will find unprofessional Facebook, Twitter, or Instagram posts in less than 60 seconds",https://www.reddit.com/r/MachineLearning/comments/60sm3m/machine_learning_app_will_find_unprofessional/,jeffreysean,1490156913,,0,1
742,2017-3-22,2017,3,22,13,60so1g,"While thinking about interfaces for AI, I decided to make my robot respond entirely through GIFs",https://www.reddit.com/r/MachineLearning/comments/60so1g/while_thinking_about_interfaces_for_ai_i_decided/,[deleted],1490157689,[deleted],0,0
743,2017-3-22,2017,3,22,13,60so1q,[1703.06868] Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization,https://www.reddit.com/r/MachineLearning/comments/60so1q/170306868_arbitrary_style_transfer_in_realtime/,[deleted],1490157692,[deleted],0,1
744,2017-3-22,2017,3,22,14,60st4j,[1703.06868] Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization,https://www.reddit.com/r/MachineLearning/comments/60st4j/170306868_arbitrary_style_transfer_in_realtime/,[deleted],1490159891,[deleted],0,1
745,2017-3-22,2017,3,22,14,60suhj,[1703.06868] Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization,https://www.reddit.com/r/MachineLearning/comments/60suhj/170306868_arbitrary_style_transfer_in_realtime/,[deleted],1490160522,[deleted],0,1
746,2017-3-22,2017,3,22,14,60svig,Is this learning Rate Normal?,https://www.reddit.com/r/MachineLearning/comments/60svig/is_this_learning_rate_normal/,MaxConners,1490161009,[removed],0,1
747,2017-3-22,2017,3,22,15,60t1hs,What is the mixing effect of different colors materials by planetary dou...,https://www.reddit.com/r/MachineLearning/comments/60t1hs/what_is_the_mixing_effect_of_different_colors/,JCT_Janice,1490163915,,0,1
748,2017-3-22,2017,3,22,16,60tbyq,"[D] Do you think ""Related work"" section is better to be located before conclusion than after introduction in scientific papers?",https://www.reddit.com/r/MachineLearning/comments/60tbyq/d_do_you_think_related_work_section_is_better_to/,terryum,1490169462,"I recently some see papers that have ""Related work"" section just before the end of the paper. Do you think it's a better place than after introduction? Is it because people are likely to stop reading while reading the related work section? lol",7,2
749,2017-3-22,2017,3,22,17,60tclh,Malmo Collaborative AI Challenge,https://www.reddit.com/r/MachineLearning/comments/60tclh/malmo_collaborative_ai_challenge/,[deleted],1490169794,[deleted],0,1
750,2017-3-22,2017,3,22,17,60tex8,[R][1703.06857] Deep Neural Networks Do Not Recognize Negative Images,https://www.reddit.com/r/MachineLearning/comments/60tex8/r170306857_deep_neural_networks_do_not_recognize/,themoosemind,1490171096,,48,7
751,2017-3-22,2017,3,22,19,60ts4f,How Zocdocs New Machine Learning Search Engine Makes Medicine More Human,https://www.reddit.com/r/MachineLearning/comments/60ts4f/how_zocdocs_new_machine_learning_search_engine/,dgutis,1490178100,,0,1
752,2017-3-22,2017,3,22,19,60tw8f,Thoughts on h2o? (Especially the Python module),https://www.reddit.com/r/MachineLearning/comments/60tw8f/thoughts_on_h2o_especially_the_python_module/,[deleted],1490180115,[removed],0,1
753,2017-3-22,2017,3,22,20,60tzng,[P] Make a Mobile Food Classifier App for iOS with Keras / Tensorflow,https://www.reddit.com/r/MachineLearning/comments/60tzng/p_make_a_mobile_food_classifier_app_for_ios_with/,stratospark,1490181670,,2,5
754,2017-3-22,2017,3,22,20,60tztk,Rotary Cum DTH Drilling Rig,https://www.reddit.com/r/MachineLearning/comments/60tztk/rotary_cum_dth_drilling_rig/,beaver_tracks,1490181743,,0,1
755,2017-3-22,2017,3,22,20,60u076,[D] Thoughts on h2o? (Especially the Python module),https://www.reddit.com/r/MachineLearning/comments/60u076/d_thoughts_on_h2o_especially_the_python_module/,ccmlacc,1490181907,"Hello, I'm working on a project where I apply machine learning algorithms. I have a data set with many categorical variables, so I want to use a library which natively handles categorical variables without one-hot encoding (which is especially important for algorithms like Random Forest and Gradient Boosting).  
  
h2o offers this nicely, and the training time of the algorithms have been faster than scikit-learn in my experience so far. However, I am a little turned off by their documentation, which seems slightly weak in comparison to scikit-learn (for instance the default values of parameters are not shown).  

Can someone with more experience share their thoughts about h2o? Is it a reliable library that I can safely put most my effort into?",5,3
756,2017-3-22,2017,3,22,20,60u0ea,"Hundreds of layers, ResNets, Inception v4, and more",https://www.reddit.com/r/MachineLearning/comments/60u0ea/hundreds_of_layers_resnets_inception_v4_and_more/,[deleted],1490181985,[deleted],0,1
757,2017-3-22,2017,3,22,20,60u24d,Deep Learning without Backpropagation (DeepMind's Synthetic Gradients),https://www.reddit.com/r/MachineLearning/comments/60u24d/deep_learning_without_backpropagation_deepminds/,x2342,1490182644,,0,2
758,2017-3-22,2017,3,22,22,60un0o,[R] Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs,https://www.reddit.com/r/MachineLearning/comments/60un0o/r_globally_optimal_gradient_descent_for_a_convnet/,ofirpress,1490189849,,3,26
759,2017-3-22,2017,3,22,22,60uo0l,[1703.06492] VQABQ: Visual Question Answering by Basic Questions,https://www.reddit.com/r/MachineLearning/comments/60uo0l/170306492_vqabq_visual_question_answering_by/,[deleted],1490190152,[deleted],1,1
760,2017-3-22,2017,3,22,22,60uqxu,[N] Malmo Collaborative AI Challenge,https://www.reddit.com/r/MachineLearning/comments/60uqxu/n_malmo_collaborative_ai_challenge/,xingdongrobotics,1490191057,,0,7
761,2017-3-22,2017,3,22,22,60ur2f,[1703.06868] Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization,https://www.reddit.com/r/MachineLearning/comments/60ur2f/170306868_arbitrary_style_transfer_in_realtime/,[deleted],1490191102,[deleted],0,1
762,2017-3-22,2017,3,22,23,60uvxz,[1703.06868] Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization,https://www.reddit.com/r/MachineLearning/comments/60uvxz/170306868_arbitrary_style_transfer_in_realtime/,[deleted],1490192481,[deleted],0,1
763,2017-3-22,2017,3,22,23,60uz6x,[1703.06868] Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization,https://www.reddit.com/r/MachineLearning/comments/60uz6x/170306868_arbitrary_style_transfer_in_realtime/,[deleted],1490193418,[deleted],0,1
764,2017-3-22,2017,3,22,23,60uz9r,Wasserstein GAN,https://www.reddit.com/r/MachineLearning/comments/60uz9r/wasserstein_gan/,[deleted],1490193437,[deleted],0,1
765,2017-3-22,2017,3,22,23,60v38t,Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization,https://www.reddit.com/r/MachineLearning/comments/60v38t/arbitrary_style_transfer_in_realtime_with/,[deleted],1490194589,[deleted],0,1
766,2017-3-23,2017,3,23,0,60vamb,Automatically identifying wild animals in camera trap images with deep learning,https://www.reddit.com/r/MachineLearning/comments/60vamb/automatically_identifying_wild_animals_in_camera/,mnorouzz,1490196523,,1,1
767,2017-3-23,2017,3,23,0,60vep8,Word2vec from scratch in Golang,https://www.reddit.com/r/MachineLearning/comments/60vep8/word2vec_from_scratch_in_golang/,[deleted],1490197677,[deleted],0,1
768,2017-3-23,2017,3,23,0,60vfpg,"Simple Questions Thread March 22, 2017",https://www.reddit.com/r/MachineLearning/comments/60vfpg/simple_questions_thread_march_22_2017/,AutoModerator,1490197955,[removed],0,1
769,2017-3-23,2017,3,23,0,60vgxh,Made word2vec from scratch in Golang,https://www.reddit.com/r/MachineLearning/comments/60vgxh/made_word2vec_from_scratch_in_golang/,[deleted],1490198275,[deleted],0,1
770,2017-3-23,2017,3,23,1,60vj9c,[P] Made word2vec from scratch in Golang,https://www.reddit.com/r/MachineLearning/comments/60vj9c/p_made_word2vec_from_scratch_in_golang/,aqny,1490198881,,6,2
771,2017-3-23,2017,3,23,1,60vpv5,[P] Gaussian Process Regression - Noisy inputs,https://www.reddit.com/r/MachineLearning/comments/60vpv5/p_gaussian_process_regression_noisy_inputs/,maka89,1490200662,"Some time ago I made a Python [implementation of Gaussian Process Regression](https://github.com/maka89/noisy-gp). Assuming a squared exp. covariance function, the implementation takes not only noise in the output, but also noise in the input into account. Thus, you can supply variance estimates for the input(and output) for each sample in the training set.

How useful it is in practice is debatable, since it is often sufficient to model the input noise as output noise, which is taken into account by other implementations, such as scikit-learn's. 

However, for some problems it gives more accurate results. Please check it out if you find it interesting :)",0,2
772,2017-3-23,2017,3,23,2,60w0ut,[D] Research Debt,https://www.reddit.com/r/MachineLearning/comments/60w0ut/d_research_debt/,wei_jok,1490203564,,29,113
773,2017-3-23,2017,3,23,3,60wabk,[R][1703.07326] One-Shot Imitation Learning,https://www.reddit.com/r/MachineLearning/comments/60wabk/r170307326_oneshot_imitation_learning/,gambs,1490205968,,2,16
774,2017-3-23,2017,3,23,3,60wk43,"Torch implementation of ""Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization""",https://www.reddit.com/r/MachineLearning/comments/60wk43/torch_implementation_of_arbitrary_style_transfer/,[deleted],1490208480,[deleted],0,1
775,2017-3-23,2017,3,23,4,60wr4f,Torch implementation of AdaIN (arbitrary style transfer in real-time),https://www.reddit.com/r/MachineLearning/comments/60wr4f/torch_implementation_of_adain_arbitrary_style/,[deleted],1490210343,[deleted],0,1
776,2017-3-23,2017,3,23,4,60wsqk,Why does stacked lstm need peephole but gru does not?,https://www.reddit.com/r/MachineLearning/comments/60wsqk/why_does_stacked_lstm_need_peephole_but_gru_does/,teling,1490210764,[removed],0,1
777,2017-3-23,2017,3,23,4,60ww7m,[1703.07195] GP-GAN: Towards Realistic High-Resolution Image Blending,https://www.reddit.com/r/MachineLearning/comments/60ww7m/170307195_gpgan_towards_realistic_highresolution/,goodfish_1,1490211652,,0,1
778,2017-3-23,2017,3,23,4,60wyes,Any ReScience like web site for Reproducible Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/60wyes/any_rescience_like_web_site_for_reproducible/,neuroland,1490212229,,0,1
779,2017-3-23,2017,3,23,5,60x3sz,How to Generate Your Own Wikipedia Articles (LIVE) - YouTube,https://www.reddit.com/r/MachineLearning/comments/60x3sz/how_to_generate_your_own_wikipedia_articles_live/,ackstazya,1490213631,,0,1
780,2017-3-23,2017,3,23,5,60x9ax,Recently Accepted to Galvanize's Data Science / Machine Learning program - Learned to code four months ago. AMA!,https://www.reddit.com/r/MachineLearning/comments/60x9ax/recently_accepted_to_galvanizes_data_science/,ketchupisfruitjam,1490215078,[removed],3,0
781,2017-3-23,2017,3,23,5,60xbyr,AdaIN: Arbitrary Style Transfer in Real-time,https://www.reddit.com/r/MachineLearning/comments/60xbyr/adain_arbitrary_style_transfer_in_realtime/,[deleted],1490215779,[deleted],0,1
782,2017-3-23,2017,3,23,6,60xici,Advanced Machine Learning with Basic Excel,https://www.reddit.com/r/MachineLearning/comments/60xici/advanced_machine_learning_with_basic_excel/,psangrene,1490217464,,0,1
783,2017-3-23,2017,3,23,6,60xn6n,Iterating over TensorFlow Dimension of type 'None',https://www.reddit.com/r/MachineLearning/comments/60xn6n/iterating_over_tensorflow_dimension_of_type_none/,Make_AI_Great_Again,1490218780,[removed],0,1
784,2017-3-23,2017,3,23,7,60xvnu,My Udacity self driving car challenge team needs members,https://www.reddit.com/r/MachineLearning/comments/60xvnu/my_udacity_self_driving_car_challenge_team_needs/,[deleted],1490221131,[removed],0,1
785,2017-3-23,2017,3,23,9,60yqns,"Baidus chief scientist, who led its AI research, is leaving the company",https://www.reddit.com/r/MachineLearning/comments/60yqns/baidus_chief_scientist_who_led_its_ai_research_is/,Jason_Fun,1490230219,,0,1
786,2017-3-23,2017,3,23,10,60yua7,[D] Double Supply Chain Backprop,https://www.reddit.com/r/MachineLearning/comments/60yua7/d_double_supply_chain_backprop/,[deleted],1490231338,[deleted],10,7
787,2017-3-23,2017,3,23,10,60yz8q,ImageNet/ILSVRC 2017,https://www.reddit.com/r/MachineLearning/comments/60yz8q/imagenetilsvrc_2017/,[deleted],1490232966,[deleted],0,1
788,2017-3-23,2017,3,23,11,60z654,[R] Variational Inference using Implicit Distributions,https://www.reddit.com/r/MachineLearning/comments/60z654/r_variational_inference_using_implicit/,schmook,1490235089,,3,19
789,2017-3-23,2017,3,23,12,60zhce,How can Char RNN be used to Capitalize text?,https://www.reddit.com/r/MachineLearning/comments/60zhce/how_can_char_rnn_be_used_to_capitalize_text/,[deleted],1490238839,[removed],0,1
790,2017-3-23,2017,3,23,12,60zhs2,How to get the % of confidence level for Facebook's Prophet?,https://www.reddit.com/r/MachineLearning/comments/60zhs2/how_to_get_the_of_confidence_level_for_facebooks/,chai_17,1490238987,[removed],0,1
791,2017-3-23,2017,3,23,14,60zzv3,[1703.06868] Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization,https://www.reddit.com/r/MachineLearning/comments/60zzv3/170306868_arbitrary_style_transfer_in_realtime/,[deleted],1490245978,[deleted],0,1
792,2017-3-23,2017,3,23,15,6106zn,Why does the types of high speed dispersion mixers used in chemical industry?,https://www.reddit.com/r/MachineLearning/comments/6106zn/why_does_the_types_of_high_speed_dispersion/,JCT_Janice,1490249142,,0,1
793,2017-3-23,2017,3,23,15,6108dg,How to build a reactor brand?,https://www.reddit.com/r/MachineLearning/comments/6108dg/how_to_build_a_reactor_brand/,JCT_Janice,1490249797,[removed],0,1
794,2017-3-23,2017,3,23,16,610dnr,Notes on Deformable Convolutional Networks,https://www.reddit.com/r/MachineLearning/comments/610dnr/notes_on_deformable_convolutional_networks/,mustafaihssan,1490252417,,0,1
795,2017-3-23,2017,3,23,17,610nqs,[1609.02226] Fitted Learning: Models with Awareness of their Limits,https://www.reddit.com/r/MachineLearning/comments/610nqs/160902226_fitted_learning_models_with_awareness/,[deleted],1490257663,[deleted],0,1
796,2017-3-23,2017,3,23,17,610nuo,Why does sklearn.grid_search.GridSearchCV return random results on every execution?,https://www.reddit.com/r/MachineLearning/comments/610nuo/why_does_sklearngrid_searchgridsearchcv_return/,darth_v115,1490257723,[removed],0,1
797,2017-3-23,2017,3,23,18,610rkz,Scripts to install and setup Tensorflow and it's dependencies on Ubuntu,https://www.reddit.com/r/MachineLearning/comments/610rkz/scripts_to_install_and_setup_tensorflow_and_its/,kailashahirwar12,1490259890,,0,1
798,2017-3-23,2017,3,23,19,610z6u,[R][1609.02226] Fitted Learning: Models with Awareness of their Limits,https://www.reddit.com/r/MachineLearning/comments/610z6u/r160902226_fitted_learning_models_with_awareness/,GreatCosmicMoustache,1490263724,,5,22
799,2017-3-23,2017,3,23,19,610ziv,[P] game2vec - applied word2vec on Steam Video Games dataset from Kaggle,https://www.reddit.com/r/MachineLearning/comments/610ziv/p_game2vec_applied_word2vec_on_steam_video_games/,CleverLime,1490263871,,28,38
800,2017-3-23,2017,3,23,19,6110i2,Algorithm Selection for Classification problem,https://www.reddit.com/r/MachineLearning/comments/6110i2/algorithm_selection_for_classification_problem/,footballfanatic8911,1490264387,[removed],0,1
801,2017-3-23,2017,3,23,19,6112qv,[N] 4 week's ML camp in Korea in July (sponsored by Google &amp; Kakao),https://www.reddit.com/r/MachineLearning/comments/6112qv/n_4_weeks_ml_camp_in_korea_in_july_sponsored_by/,terryum,1490265481,"[Link] https://github.com/TensorFlowKR/MLJejuCamp

Call for application for Machine Learning Camp Jeju 2017

If you have studied machine learning/deep learning and TensorFlow, you probably want to implement a non-trivial and large-scale system for real use. We invite you to the month-long Machine Learning Camp Jeju 2017, where you can make that dream a reality.

For a full month in beautiful Jeju Island, you and other participants will train a deep learning model using TensorFlow from start-to-finish. Jeff Dean (Google Senior Fellow via Hangout), and Rajat Monga (Google/TensorFlow Director (TBC)) will give us keynote talks. Plus, you will have access to experienced mentors including Namju Kim (Head of Research for Kakao Brain), Lucy Park (TF-KR), Sung Kim (TF-KR), Donghyun Kwak (TF-KR), Terry Taewoong Um (TF-KR), and many more. We hope you take advantage of this wonderful opportunity.

Those selected as participants will be provided with one (1) round-trip airfare (up to 300 USD) to Jeju Island (South Korea), room and board at Jeju National University, USD 1,000 in stipends and USD 500 to 1,000 in Google Cloud Credit. In addition to these benefits, participants will gain valuable and practical experience in the field of deep learning. We look forward to your application!

Mentor Recruitment: If youre interested in sharing your experiences and expertise with the camp, please contact us at mljejucamp@googlegroups.com. You will serve as personal mentors to 1 to 2 participants, holding 2 to 3 on/offline meetings a week to help them successfully complete their projects. While it is possible for you to provide online-only mentoring, we suggest you visit Jeju Island to meet with your mentees in person. We will provide round-trip airfare (up to USD 300) to Jeju Island and up to five (5) days of room and board.

(Information regarding schedule, program and benefits are subject to change as we are in the process of finalizing the details. We will have more information later.)

Camp Overview

Date: July 3rd through 28th, 2017 (Check-in date: July 2nd)
Participants: 20
Location: Jeju National University / Kakao Space.1
Organizers: TensorFlow Korea User Group, Kakao, Google, Smart Grid CK Center in Jeju National Univ, Jeju Center for Creative Economy and Innovation, Jeju Local Government (Subject to change)
Home page: https://github.com/TensorFlowKR/ml-jeju-camp
Application: https://www.surveymonkey.com/r/LY29GM5 (By April 20 11:59PM AOE)
Contacts: Please leave your comments/questions on issues (https://github.com/TensorFlowKR/ml-jeju-camp/issues) in this page.
Benefits (TBD)

Full month of hands-on experience training deep learning models with TensorFlow and mentorship from top developers
Round-trip airfare to Jeju Island (up to $300 USD)
Accomodation in Jeju National University or Kakao Space, Jeju
Stipend: 1,000 USD
Google Cloud Credit ($500~1000 TBD)
Qualification

No nationality, gender, age, degree, education requirements
Must be able to stay in Jeju Island from July 3rd to 28th. (Weekday camp programs run from 10AM to 5PM)
Good understanding of TensorFlow and deep learning and ability to train models (should be able to understand all in https://github.com/hunkim/DeepLearningZeroToAll)
Basic communication skills in English (All programs will be in English)
Application (By April 20 11:59PM AOE)

Detailed proposal for Deep Learning Camp Jeju 2017 project (Please be as detailed as possible)
CV that showcases applicants experience with deep learning and TensorFlow
Previously implemented models (GitHub or other)
Other supporting materials to show your qualification
Application link: https://www.surveymonkey.com/r/LY29GM5",5,23
802,2017-3-23,2017,3,23,19,6113j6,[N] Jeff Hawkins on the sensory motor inference theory of the HTM.,https://www.reddit.com/r/MachineLearning/comments/6113j6/n_jeff_hawkins_on_the_sensory_motor_inference/,nocortex,1490265868,,1,9
803,2017-3-23,2017,3,23,19,6114cp,LIBROS RECOMENDADOS PARA APRENDER ESTADSTICA CON R,https://www.reddit.com/r/MachineLearning/comments/6114cp/libros_recomendados_para_aprender_estadstica_con/,maximaformacion,1490266239,,0,1
804,2017-3-23,2017,3,23,20,611635,Webpage classification as a web forum,https://www.reddit.com/r/MachineLearning/comments/611635/webpage_classification_as_a_web_forum/,chiptus,1490267012,[removed],0,1
805,2017-3-23,2017,3,23,20,611a9x,"Onto RNNs today, with handwriting generation and image segmentation",https://www.reddit.com/r/MachineLearning/comments/611a9x/onto_rnns_today_with_handwriting_generation_and/,[deleted],1490268776,[deleted],0,1
806,2017-3-23,2017,3,23,21,611fhr,"any datasets which can be used for creating a ""taste graph"" or ""interest graph""",https://www.reddit.com/r/MachineLearning/comments/611fhr/any_datasets_which_can_be_used_for_creating_a/,mikos,1490270729,[removed],0,1
807,2017-3-23,2017,3,23,21,611mhs,[P] Floyd Zero Setup Deep Learning,https://www.reddit.com/r/MachineLearning/comments/611mhs/p_floyd_zero_setup_deep_learning/,pmigdal,1490273181,,19,9
808,2017-3-23,2017,3,23,21,611okw,"[N] ImageNet/ILSVRC 2017, the last ImageNet challenge",https://www.reddit.com/r/MachineLearning/comments/611okw/n_imagenetilsvrc_2017_the_last_imagenet_challenge/,xternalz,1490273876,,24,49
809,2017-3-23,2017,3,23,22,611p4d,[P] Why Mean Squared Error and L2 regularization? A probabilistic justification.,https://www.reddit.com/r/MachineLearning/comments/611p4d/p_why_mean_squared_error_and_l2_regularization_a/,avitaloliver,1490274030,,14,45
810,2017-3-23,2017,3,23,22,611txc,Latent Semantic Analysis of subreddits to enable Reddit Algebra,https://www.reddit.com/r/MachineLearning/comments/611txc/latent_semantic_analysis_of_subreddits_to_enable/,RevMen,1490275521,,0,1
811,2017-3-23,2017,3,23,22,61201i,Interesting tutorials/visualizations for vector semantics?,https://www.reddit.com/r/MachineLearning/comments/61201i/interesting_tutorialsvisualizations_for_vector/,YourWelcomeOrMine,1490277490,[removed],0,1
812,2017-3-23,2017,3,23,23,6121x9,Does anyone know a good way to bulk download movie scripts for ML?,https://www.reddit.com/r/MachineLearning/comments/6121x9/does_anyone_know_a_good_way_to_bulk_download/,sits-on-penguins,1490278021,[removed],0,1
813,2017-3-23,2017,3,23,23,6126kj,Dissecting Trump Most Rabid Online Following,https://www.reddit.com/r/MachineLearning/comments/6126kj/dissecting_trump_most_rabid_online_following/,lalala253,1490279362,,0,2
814,2017-3-24,2017,3,24,0,612n5z,Torch implementation of AdaIN style transfer,https://www.reddit.com/r/MachineLearning/comments/612n5z/torch_implementation_of_adain_style_transfer/,[deleted],1490283979,[deleted],0,1
815,2017-3-24,2017,3,24,0,612olv,[D] Four key takeaways after 6 months working in the market of chatbots,https://www.reddit.com/r/MachineLearning/comments/612olv/d_four_key_takeaways_after_6_months_working_in/,fjaguero,1490284367,,4,16
816,2017-3-24,2017,3,24,1,612sa6,Best model for extrapolation?,https://www.reddit.com/r/MachineLearning/comments/612sa6/best_model_for_extrapolation/,[deleted],1490285329,[removed],0,1
817,2017-3-24,2017,3,24,1,612swr,Time series or ML project,https://www.reddit.com/r/MachineLearning/comments/612swr/time_series_or_ml_project/,ofigue,1490285478,[removed],0,1
818,2017-3-24,2017,3,24,1,612wtr,[News] NNAISENSE - Schmidhuber's start-up to launch an AI hedge fund,https://www.reddit.com/r/MachineLearning/comments/612wtr/news_nnaisense_schmidhubers_startup_to_launch_an/,[deleted],1490286587,[deleted],0,0
819,2017-3-24,2017,3,24,1,612x03,[P] TensorFlow: Mutating variables and control flow,https://www.reddit.com/r/MachineLearning/comments/612x03/p_tensorflow_mutating_variables_and_control_flow/,morgangiraud,1490286641,,1,24
820,2017-3-24,2017,3,24,2,6137vd,"[Discussion] Recordings of the NIPS 2016 RNN Symposium - includes talks from Schmidhuber, Graves, Werbos, Weston, Vinyals, Freitas &amp; a panel discussion.",https://www.reddit.com/r/MachineLearning/comments/6137vd/discussion_recordings_of_the_nips_2016_rnn/,pranv,1490289658,,5,67
821,2017-3-24,2017,3,24,2,613ahv,[News] Intel Forms New AI Group Reporting Directly To CEO Brian Krzanich,https://www.reddit.com/r/MachineLearning/comments/613ahv/news_intel_forms_new_ai_group_reporting_directly/,drwebb,1490290354,,15,121
822,2017-3-24,2017,3,24,2,613doz,Machine learning lets scientists reverse-engineer cellular control networks,https://www.reddit.com/r/MachineLearning/comments/613doz/machine_learning_lets_scientists_reverseengineer/,aarondubrow,1490291184,,0,1
823,2017-3-24,2017,3,24,3,613ogs,[R] Deep Photo Style Transfer,https://www.reddit.com/r/MachineLearning/comments/613ogs/r_deep_photo_style_transfer/,zergling103,1490293972,,5,12
824,2017-3-24,2017,3,24,3,613ran,"Neukom Institute at Dartmouth College announces prize for generated music, poetry, and prose. Deadline May 15, 2017.",https://www.reddit.com/r/MachineLearning/comments/613ran/neukom_institute_at_dartmouth_college_announces/,riddella,1490294742,,0,1
825,2017-3-24,2017,3,24,3,613u7d,[1703.06868] Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization,https://www.reddit.com/r/MachineLearning/comments/613u7d/170306868_arbitrary_style_transfer_in_realtime/,goodfish_1,1490295487,,1,2
826,2017-3-24,2017,3,24,4,613wtg,[P] pretrained generative models list on github,https://www.reddit.com/r/MachineLearning/comments/613wtg/p_pretrained_generative_models_list_on_github/,j-peterson,1490296130,"I'm making a list (link below). Any suggestions?

https://github.com/jcpeterson/pretrained-generative-models-list/blob/master/README.md",0,4
827,2017-3-24,2017,3,24,4,613zs9,R Tools for Visual Studio reaches 1.0 - quick tour of features.,https://www.reddit.com/r/MachineLearning/comments/613zs9/r_tools_for_visual_studio_reaches_10_quick_tour/,smortaz,1490296915,,0,2
828,2017-3-24,2017,3,24,5,6148v5,How much programming knowledge would I need to have before I could take a course in machine learning?,https://www.reddit.com/r/MachineLearning/comments/6148v5/how_much_programming_knowledge_would_i_need_to/,asd987fgh,1490299243,[removed],0,1
829,2017-3-24,2017,3,24,5,614k3e,eclipse+pydev can't find cuda library,https://www.reddit.com/r/MachineLearning/comments/614k3e/eclipsepydev_cant_find_cuda_library/,yuanzheng625,1490302185,[removed],0,1
830,2017-3-24,2017,3,24,5,614kr3,"[P] TensorFlow Tutorial: Using RNNs for speech-to-text (blog post, GitHub repo with code)",https://www.reddit.com/r/MachineLearning/comments/614kr3/p_tensorflow_tutorial_using_rnns_for_speechtotext/,warmlogic,1490302349,,3,19
831,2017-3-24,2017,3,24,6,614noe,[D] An idea to parallelize BackProp (BP),https://www.reddit.com/r/MachineLearning/comments/614noe/d_an_idea_to_parallelize_backprop_bp/,Kiuhnm,1490303144,"### **First of all, who am I?**

You're an imaginary interviewer. Ask away!

### **I'm having a dj vu. Didn't you already post this stuff yesterday?**

Yes, but I realized I did a bad job at explaining and introducing my idea.

### **Why sharing an idea instead of trying it out first and seeing if it works?**

I don't get paid to do research in ML, nor to do ML. Actually, I don't get paid to do anything at all.

This means that everything I do is purely for fun, and now I don't feel like boring myself to death implementing this thing in Theano. I prefer to watch [Silver's course](https://www.youtube.com/watch?v=2pWv7GOvuf0) about RL.

Also, I don't have the resources to conduct proper tests.

### **Why should anyone do your homework for you?**

I prefer to think of it as the draft of a project that anyone can contribute to.

In the near future I might implement this thing and do some tests.

### **Why parallelize BackProp?**

For efficiency reasons, mainly. In classic BP only one layer can do any computation at any time because:

1. it needs data from the previous layer in the forward pass
2. it needs data from the next layer in the backward pass

### **But isn't BP inherently sequential?**

Yes, but maybe we can relax it to make it amenable to parallelization.

### **What kind of relaxation are we talking about?**

We delay the updates to the net.

### **Can you give a real life example?**

Imagine you learn by taking exams and reviewing the grades and corrections provided by your teacher.

In classic BP you:

1. take exam E1
2. receive corrections for exam E1
3. take exam E2
4. receive corrections for exam E2
5. ...

In Relaxed BP (RBP) you:

1. take exam E1
2. take exam E2
3. take exam E3
4. receive corrections for E1
5. take exam E4
6. receive corrections for E2
7. take exam E5
8. receive corrections for E3
9. ...

### **How much faster would it be?**

Theoretically, it should be N times faster, where N is the number of layers in the net.

### **So all nodes/layers do computations at the same time?**

Exactly. Each node does a step of forward pass and a step of backward pass.

### **Can we use classic libraries such as Theano?**

Yes:

1. Each layer/node has its own shared variables.

At each step of training:

2. A new sample/example is fed to the first node from the left.
3. All samples move to the right by one node.
4. When a sample moves to the right it gets transformed by the node it goes through (**Forward pass**).
5. New gradient information is fed to the last node from the right.
6. Each piece of gradient information moves to the left and gets transformed by the node it goes through (**Backward pass**).

### **If it takes time for a single sample to go through a net which is continuously updated, how can the sample see a consistent net?**

The weights of the layers are not of the same age (age = number of updates).

Have a look at this picture:

     X-&gt;
        L1    L2    L3    L4    L5
        W5    W4    W3    W2    W1
        
1. X is the sample which enters the net from the left.
2. Wi are the weights.
3. If Wi is below Lj then it belongs to layer j.
4. The i in Wi is the age of W.
5. The initial net has weights W1.
6. When a weight Wi is updated it becomes W(i+1).

At each time step, X goes one step to the right and each weight is updated:

     X
        L1    L2    L3    L4    L5
        W5    W4    W3    W2    W1

           X
        L1    L2    L3    L4    L5
        W6    W5    W4    W3    W2

                 X
        L1    L2    L3    L4    L5
        W7    W6    W5    W4    W3

                       X
        L1    L2    L3    L4    L5
        W8    W7    W6    W5    W4

                             X
        L1    L2    L3    L4    L5
        W9    W8    W7    W6    W5

X goes always through layers of age 5 and, therefore, sees a consistent net of age 5.

### **What about the other samples?**

No problem:

     X5    X4    X3    X2    X1
        L1    L2    L3    L4    L5
        W5    W4    W3    W2    W1

     X6    X5    X4    X3    X2
        L1    L2    L3    L4    L5
        W6    W5    W4    W3    W2

     X7    X6    X5    X4    X3
        L1    L2    L3    L4    L5
        W7    W6    W5    W4    W3

     X8    X7    X6    X5    X4
        L1    L2    L3    L4    L5
        W8    W7    W6    W5    W4

     X9    X8    X7    X6    X5
        L1    L2    L3    L4    L5
        W9    W8    W7    W6    W5

In this case, sample i sees only a net of age i.

### **But how can older weights be on the left if the updates in the backward pass start from the right?**

The updates are delayed until they reach the first node:

        --------&gt;--------&gt;----------+
                                    |
        L1    L2    L3    L4    L5  |
        ^                           v
        |                           |
        +-------&lt;--------&lt;----------+
        
### **And what about the labels? Wouldn't samples and labels get mixed up?**

Not if things are properly synchronized:

     X9    X8    X7    X6    X5     &lt;-- Y5  Y6  Y7  Y8  Y9 ... 
        L1    L2    L3    L4    L5
        W9    W8    W7    W6    W5

As you can see, Xi always ""meet"" Yi and, thus, we can compute the right gradient starting from the right node.
        
### **How can we propagate the error back if the net got updated in the meantime?**

We need to memorize some of the previous nets.

### **How many?**

Basically, if the net has N layers, we need to memorize 2N - 1 copies of weights (+ some additional information) to the layers.

### **But then the computing time will go up by 2N times!?**

No, each operation is still O(1). We keep a queue of 2N - 1 elements, but we only access 2 elements per node at each time, just like in the classic BP.

### **I'm not convinced. Could you show me a picture?**

Here you go:

    X5    X4    X3    X2    X1    &lt;-- Y1   Y2   Y3 ...
       L1    L2    L3    L4    L5
       W1    W2    W3    W4   (W5)
       W2    W3    W4    W5    W6
       W3    W4    W5   (W6)   W7
       W4    W5    W6    W7    W8
       W5    W6   (W7)   W8    W9
       W6    W7    W8    W9    W10
       W7   (W8)   W9    W10   W11
       W8    W9    W10   W11   W12
      (W9)   W10   W11   W12   W13

We have 5 layers and each layer maintains 2*5 - 1 = 9 elements.

The Wi in parentheses are currently active and determine the age of the corresponding layer.

So, in the forward pass:

1. X5 sees W9
2. X4 sees W8
3. X3 sees W7
4. X2 sees W6
5. X1 sees W5

The Wi are not just weights.

To be precise:

1. all the Wi *above* the Wi between parentheses must also contain the input to the layer, whereas
2. all the Wi *below* must contain the gradient of the loss wrt the output.

### **This sort of makes sense, but how do we do the updates in parallel?**

Let's have another look at the picture above:

    X5    X4    X3    X2    X1    &lt;-- Y1   Y2   Y3 ...
       L1    L2    L3    L4    L5
       W1    W2    W3    W4   (W5)
       W2    W3    W4    W5    W6
       W3    W4    W5   (W6)   W7
       W4    W5    W6    W7    W8
       W5    W6   (W7)   W8    W9
       W6    W7    W8    W9    W10
       W7   (W8)   W9    W10   W11
       W8    W9    W10   W11   W12
      (W9)   W10   W11   W12   W13

This is the hard part to understand:

1. X1 and Y1 ""meet"" and L5 can compute gradient information relative to (X1, Y1).
2. L5 is currently at age 5 (because W5 is in parentheses).
3. L5 computes its own W14 from X1, Y1 and its own W5.
4. In general, Wi is always used to compute W(i+2N-1), where N is the number of layers.
5. More specifically, Wi is always used to compute W(i+9) in this example.

6. W13 of L5 has been computed, at the previous step, from W4 (which was then dropped).
8. W4 of L4 belongs to an old net (the current net is of age 6 from the point of view of L4).
9. W4 of L4 can and must also remember its old input.
10. L4 computes its own W13 by using W13 from L5 (for gradient info) and its own W4 (for input and weights).

11. The same way, L3 computes its own W12 from its own W3 and W12 of L4.
12. And so on... 

Here's the picture after 1 step:
      
    X6    X5    X4    X3    X2    &lt;-- Y2   Y3   Y4 ...
       L1    L2    L3    L4    L5
       W1    W2    W3    W4    W5   &lt;-- now we can drop this row
       W2    W3    W4    W5   (W6)
       W3    W4    W5    W6    W7
       W4    W5    W6   (W7)   W8
       W5    W6    W7    W8    W9
       W6    W7   (W8)   W9    W10
       W7    W8    W9    W10   W11
       W8   (W9)   W10   W11   W12
       W9    W10   W11   W12   W13
      (W10)  W11   W12   W13   W14

### **Are you sure this will actually work?**

I'm 90% sure.

### **How can we boostrap this... *thing*?**

We should be able to use zeros and repeated weights (or something like that):

    X1    0     0     0     0     &lt;-- 0  0  0  0  Y1 ...
       L1    L2    L3    L4    L5
       W1    W1    W1    W1   (W1)     the input relative to
       W1    W1    W1    W1    W1     old (i.e. before ""(W1)"") 
       W1    W1    W1   (W1)   W1          elements is 0
       W1    W1    W1    W1    W1         
       W1    W1   (W1)   W1    W1      the gradient relative to
       W1    W1    W1    W1    W1     future (i.e. after ""(W1)"")  
       W1   (W1)   W1    W1    W1          elements is 0
       W1    W1    W1    W1    W1      
      (W1)   W1    W1    W1    W1 

    X2    X1    0     0     0     &lt;-- 0  0  0  Y1  Y2 ...
       L1    L2    L3    L4    L5
       W1    W1    W1    W1   (W1)
       W1    W1    W1   *W1    W1
       W1    W1    W1   (W1)   W1     
       W1    W1   *W1    W1    W1      ""*"" means that the
       W1    W1   (W1)   W1    W1      memorized input is
       W1   *W1    W1    W1    W1          not zero
       W1   (W1)   W1    W1    W1 
      *W1    W1    W1    W1    W1 
      (W1)   W1    W1    W1    W1 
                                  
    X3    X2    X1    0     0     &lt;-- 0  0  Y1  Y2  Y3 ...
       L1    L2    L3    L4    L5
       W1    W1    W1   *W1   (W1)
       W1    W1    W1   *W1    W1
       W1    W1   *W1   (W1)   W1     
       W1    W1   *W1    W1    W1      ""*"" means that the
       W1   *W1   (W1)   W1    W1      memorized input is
       W1   *W1    W1    W1    W1          not zero
      *W1   (W1)   W1    W1    W1 
      *W1    W1    W1    W1    W1 
      (W1)   W1    W1    W1    W1 
                                  
    X4    X3    X2    X1    0     &lt;-- 0  Y1  Y2  Y3  Y4 ...
       L1    L2    L3    L4    L5
       W1    W1    W1   *W1   (W1)
       W1    W1   *W1   *W1    W1
       W1    W1   *W1   (W1)   W1     
       W1   *W1   *W1    W1    W1      ""*"" means that the
       W1   *W1   (W1)   W1    W1      memorized input is
      *W1   *W1    W1    W1    W1          not zero
      *W1   (W1)   W1    W1    W1 
      *W1    W1    W1    W1    W1 
      (W1)   W1    W1    W1    W1 
                                  
    X5    X4    X3    X2    X1    &lt;-- Y1  Y2  Y3  Y4  Y5 ...
       L1    L2    L3    L4    L5
       W1    W1   *W1   *W1   (W1)
       W1    W1   *W1   *W1    W1
       W1   *W1   *W1   (W1)   W1     
       W1   *W1   *W1    W1    W1      ""*"" means that the
      *W1   *W1   (W1)   W1    W1      memorized input is
      *W1   *W1    W1    W1    W1          not zero
      *W1   (W1)   W1    W1    W1 
      *W1    W1    W1    W1    W1 
      (W1)   W1    W1    W1    W1 
                                  
    X6    X5    X4    X3    X2    &lt;-- Y2  Y3  Y4  Y5  Y6 ...
       L1    L2    L3    L4    L5
       W1    W1   *W1   *W1   (W1)
       W1   *W1   *W1   *W1    W1     
       W1   *W1   *W1   (W1)   W1      
      *W1   *W1   *W1    W1    W1      ""*"" means that the
      *W1   *W1   (W1)   W1    W1      memorized input is
      *W1   *W1    W1    W1    W1          not zero
      *W1   (W1)   W1    W1    W1 
      *W1    W1    W1    W1    W1 
      (W1)   W1    W1    W1    W2
                                  
    X7    X6    X5    X4    X3    &lt;-- Y3  Y4  Y5  Y6  Y7 ...
       L1    L2    L3    L4    L5
       W1   *W1   *W1   *W1   (W1)    
       W1   *W1   *W1   *W1    W1      
      *W1   *W1   *W1   (W1)   W1      
      *W1   *W1   *W1    W1    W1      ""*"" means that the
      *W1   *W1   (W1)   W1    W1      memorized input is
      *W1   *W1    W1    W1    W1          not zero
      *W1   (W1)   W1    W1    W1 
      *W1    W1    W1    W1    W2
      (W1)   W1    W1    W2    W3                                   
                                  
    X8    X7    X6    X5    X4    &lt;-- Y4  Y5  Y6  Y7  Y8 ...
       L1    L2    L3    L4    L5
       W1   *W1   *W1   *W1   (W1)     
      *W1   *W1   *W1   *W1    W1      
      *W1   *W1   *W1   (W1)   W1      
      *W1   *W1   *W1    W1    W1      ""*"" means that the
      *W1   *W1   (W1)   W1    W1      memorized input is
      *W1   *W1    W1    W1    W1          not zero
      *W1   (W1)   W1    W1    W2
      *W1    W1    W1    W2    W3                                   
      (W1)   W1    W2    W3    W4 
                                  
    X9    X8    X7    X6    X5    &lt;-- Y5  Y6  Y7  Y8  Y9 ...
       L1    L2    L3    L4    L5
       W1    W1    W1    W1   (W1)     
       W1    W1    W1    W1    W1      
       W1    W1    W1   (W1)   W1      
       W1    W1    W1    W1    W1      
       W1    W1   (W1)   W1    W1      
       W1    W1    W1    W1    W2
       W1   (W1)   W1    W2    W3                                   
       W1    W1    W2    W3    W4 
      (W1)   W2    W3    W4    W5 
                                  
    X10   X9    X8    X7    X6    &lt;-- Y6  Y7  Y8  Y9  Y10 ...
       L1    L2    L3    L4    L5
       W1    W1    W1    W1   (W1)     
       W1    W1    W1    W1    W1      
       W1    W1    W1   (W1)   W1      
       W1    W1    W1    W1    W1      
       W1    W1   (W1)   W1    W2
       W1    W1    W1    W2    W3                                   
       W1   (W1)   W2    W3    W4 
       W1    W2    W3    W4    W5 
      (W2)   W3    W4    W5    W6 
                                  
As we can see, the diagonal is always older on the left and gets younger going right, as intended.

### **Are you sure you don't want to implement this?**

Maybe in the future if no one beats me to it.

Even if it works algorithmically, it remains to be seen if the delay hurts convergence or generalization.

### **It was nice talking with you!**

Yes, but I did all the work.

### **Will we meet again?**

Only on paper, I hope.
",13,0
832,2017-3-24,2017,3,24,6,614paf,[P] A quick look at Support Vector Machines,https://www.reddit.com/r/MachineLearning/comments/614paf/p_a_quick_look_at_support_vector_machines/,pmigdal,1490303578,,5,35
833,2017-3-24,2017,3,24,6,614rdu,a neural style with squeezeNet on pytorch: small and fast,https://www.reddit.com/r/MachineLearning/comments/614rdu/a_neural_style_with_squeezenet_on_pytorch_small/,deepzeng,1490304158,,0,1
834,2017-3-24,2017,3,24,6,614yjk,[Discussion] Arguments against the use of deep learning,https://www.reddit.com/r/MachineLearning/comments/614yjk/discussion_arguments_against_the_use_of_deep/,[deleted],1490306194,,5,0
835,2017-3-24,2017,3,24,7,6159be,Pre-trained neural network for UAV,https://www.reddit.com/r/MachineLearning/comments/6159be/pretrained_neural_network_for_uav/,_padda,1490309237,[removed],0,1
836,2017-3-24,2017,3,24,10,61642j,[P] Cyclical Learning Rate Policies for Keras,https://www.reddit.com/r/MachineLearning/comments/61642j/p_cyclical_learning_rate_policies_for_keras/,[deleted],1490318494,[deleted],0,1
837,2017-3-24,2017,3,24,10,6166br,[P] Cyclical Learning Rate Policies for Keras,https://www.reddit.com/r/MachineLearning/comments/6166br/p_cyclical_learning_rate_policies_for_keras/,[deleted],1490319216,[deleted],0,1
838,2017-3-24,2017,3,24,10,6166sy,Mythic: flash-based ANN chip,https://www.reddit.com/r/MachineLearning/comments/6166sy/mythic_flashbased_ann_chip/,bkaz,1490319384,,1,1
839,2017-3-24,2017,3,24,10,61682y,[P] Cyclical Learning Rate Policies for Keras,https://www.reddit.com/r/MachineLearning/comments/61682y/p_cyclical_learning_rate_policies_for_keras/,[deleted],1490319792,[deleted],0,1
840,2017-3-24,2017,3,24,10,6169um,[P] Cyclical Learning Rate Policies for Keras,https://www.reddit.com/r/MachineLearning/comments/6169um/p_cyclical_learning_rate_policies_for_keras/,[deleted],1490320390,[deleted],0,1
841,2017-3-24,2017,3,24,10,616ald,[P] Cyclical Learning Rate Policies for Keras,https://www.reddit.com/r/MachineLearning/comments/616ald/p_cyclical_learning_rate_policies_for_keras/,[deleted],1490320641,[deleted],0,1
842,2017-3-24,2017,3,24,11,616c2u,[P] Cyclical Learning Rate Policies for Keras,https://www.reddit.com/r/MachineLearning/comments/616c2u/p_cyclical_learning_rate_policies_for_keras/,[deleted],1490321100,[deleted],0,1
843,2017-3-24,2017,3,24,11,616eu2,[P] Cyclical Learning Rate Policies for Keras,https://www.reddit.com/r/MachineLearning/comments/616eu2/p_cyclical_learning_rate_policies_for_keras/,Bckenstler,1490322030,,4,8
844,2017-3-24,2017,3,24,12,616nqp,How does the JCT vertical feed mixer make for package?,https://www.reddit.com/r/MachineLearning/comments/616nqp/how_does_the_jct_vertical_feed_mixer_make_for/,mixmachinery,1490325090,,1,1
845,2017-3-24,2017,3,24,13,616xx9,Recurrent Neural Networks - A Short TensorFlow Tutorial,https://www.reddit.com/r/MachineLearning/comments/616xx9/recurrent_neural_networks_a_short_tensorflow/,iamkeyur,1490328993,,0,1
846,2017-3-24,2017,3,24,13,6170pe,How does double planetary mixer for high viscosity material mixing,https://www.reddit.com/r/MachineLearning/comments/6170pe/how_does_double_planetary_mixer_for_high/,JCT_Janice,1490330127,,0,1
847,2017-3-24,2017,3,24,14,6175op,A neat summary of Feature Engineering using R language,https://www.reddit.com/r/MachineLearning/comments/6175op/a_neat_summary_of_feature_engineering_using_r/,unbuckledbee,1490333196,,6,7
848,2017-3-24,2017,3,24,15,617abd,Recognizing character with machine learning,https://www.reddit.com/r/MachineLearning/comments/617abd/recognizing_character_with_machine_learning/,leaveatrace,1490335303,[removed],0,1
849,2017-3-24,2017,3,24,15,617b91,Mechanical Mine Clearance System Market Set to Expand with a CAGR of 2.7% during the forecast period 2016 to 2024,https://www.reddit.com/r/MachineLearning/comments/617b91/mechanical_mine_clearance_system_market_set_to/,charliegefen,1490335720,,0,1
850,2017-3-24,2017,3,24,15,617ekx,TRACTOR MOUNTED DRILLING RIG,https://www.reddit.com/r/MachineLearning/comments/617ekx/tractor_mounted_drilling_rig/,beaver_tracks,1490337390,,0,1
851,2017-3-24,2017,3,24,15,617gjl,Need help with neural network for experimental research,https://www.reddit.com/r/MachineLearning/comments/617gjl/need_help_with_neural_network_for_experimental/,cdhawan4314,1490338365,[removed],0,1
852,2017-3-24,2017,3,24,15,617gqx,[P] PyTorch Tutorials - Better Rendered,https://www.reddit.com/r/MachineLearning/comments/617gqx/p_pytorch_tutorials_better_rendered/,saucysassy,1490338481,,5,104
853,2017-3-24,2017,3,24,16,617kvx,3.4MP MIPI low light camera board for NVIDIA Jetson TX1,https://www.reddit.com/r/MachineLearning/comments/617kvx/34mp_mipi_low_light_camera_board_for_nvidia/,econsystems,1490340572,,0,1
854,2017-3-24,2017,3,24,16,617nrb,[D] Medium difficulty applications in pixel RL?,https://www.reddit.com/r/MachineLearning/comments/617nrb/d_medium_difficulty_applications_in_pixel_rl/,next_nobel_prize_inc,1490342116,"I'm in the process of reading up on machine learning, but the training time required for some applications is really daunting. Currently, I find pixel RL of particular interest to me, and to get a feel for the current state of the art, I figured I'd start by doing my own implementations of DQN, a3c, NEC, etc.

This is where the training time becomes really discouraging. Getting an indication that the DQN algorithm is working, even for the simplest game in the atari domain, will take half a day. At the moment, I'm just reimplementing existing results, I cannot imagine trying to work out something new.

In any case, how do you do your prototyping? Are there some lower difficulty level pixel learning applications out there? Where some indication of successful results can be seen after some 5-10 minutes of training on a single machine with gpu? And where a successful approach is likely to carry over to the atari domain?

If there are none, how on earth do you deal with such huge delays in your workflow?",11,8
855,2017-3-24,2017,3,24,17,617slk,[P] sound isolation/extraction of musical instruments not converging,https://www.reddit.com/r/MachineLearning/comments/617slk/p_sound_isolationextraction_of_musical/,careless25,1490344834,"We are currently working on a project to extract one of 2 instruments playing in a 2 min long file with overlapping notes.

We have a very simple NN design: 
Conv -&gt; Pool -&gt; Conv-transpose

The idea was taken from here: https://github.com/jshap70/TensorFlow-Signal-Processing

We are not able to make the convolution network converge and are not sure where the problem might be. The error is currently at 1.3e8 and the ouput audio sounds really bad. 

Currently we don't take the FFT since the NN on github was able to distinguish between two different sounds without the need for FFT. 

Any ideas on how to improve this? Adding another layer? 
",2,1
856,2017-3-24,2017,3,24,18,617uw5,A miscellany of fun deep learning papers,https://www.reddit.com/r/MachineLearning/comments/617uw5/a_miscellany_of_fun_deep_learning_papers/,[deleted],1490346072,[deleted],0,1
857,2017-3-24,2017,3,24,19,61847g,Practical bayesian methods MOOC,https://www.reddit.com/r/MachineLearning/comments/61847g/practical_bayesian_methods_mooc/,bihaqo,1490350775,"Hi everyone. 
Were continuing to develop a Coursera MOOC about practical Bayesian methods. We came up with a prototype schedule and wanted to discuss it with the community -- what do you think of the contents and order, what is missing etc.

See the detailed plan here: https://docs.google.com/document/d/1AHLKVQntgcRYhoNPL09NerAscbpzPneHrJWAH-B5yRs/edit?usp=sharing

This course is going to be a part of the Advanced Machine Learning specialization and will take 6 weeks. The short version of the schedule:

1) Intro, Statistics review, example models, Conjugate distributions 

2) EM (Latent variable models), GMM, probabilistic PCA 

3) MCMC 

4) Variational inference, LDA 

5) Stochastic variational inference, VAE, Bayesian Neural Networks 

6) Gaussian processes, Hyperparameter optimization, summary 

One of the homework we are not sure about is the paper reading assignment. Since one of the course goals is to train listeners well enough so that they can read papers about Bayesian models, we may make a homework along the lines read one of these papers, summarize it, and get the feedback from peer-review.

Another important decision is the final project. One of the ideas is to build a self-driving car by training a CNN to map image from a camera (simulated in a game) to the signals for the wheel. This part is simple and non-probabilistic :)
The probabilistic part is to combine signals from different sources that work in different conditions (e.g. usual camera works poorly at night, infrared camera works poorly when raining), and to estimate the uncertainty in the prediction, so the car can pass the controls to the human driver if it is not sure what to do.

What do you guys think? We are especially concerned about the homework (see details in the GDoc).
",5,3
858,2017-3-24,2017,3,24,20,618g4l,Exotic ML models worth looking into?,https://www.reddit.com/r/MachineLearning/comments/618g4l/exotic_ml_models_worth_looking_into/,vineavip,1490356110,[removed],0,1
859,2017-3-24,2017,3,24,21,618nwi,[Tensorflow] Help making fixed length batches from variable length records,https://www.reddit.com/r/MachineLearning/comments/618nwi/tensorflow_help_making_fixed_length_batches_from/,[deleted],1490358939,[removed],0,1
860,2017-3-24,2017,3,24,22,618ya3,[news] Watch The Machine Learning Conference Streaming Live Now from NYC,https://www.reddit.com/r/MachineLearning/comments/618ya3/news_watch_the_machine_learning_conference/,shonburton,1490362590,,0,5
861,2017-3-24,2017,3,24,22,618zjw,[D] Are there any e-readers that are suitable for technical books and journal articles?,https://www.reddit.com/r/MachineLearning/comments/618zjw/d_are_there_any_ereaders_that_are_suitable_for/,pfizer_soze,1490363034,"I've always printed shorter papers, but I can't really justify printing a book. I feel like my eyes are about to fall out of my head after reading on a monitor for hours, though.",19,5
862,2017-3-24,2017,3,24,23,6193uh,"""Massive Exploration of Neural Machine Translation Architectures"", Britz et al 2017 [hyperparameter optimization of 100s of NMTs using 250,000 GPU-hours]",https://www.reddit.com/r/MachineLearning/comments/6193uh/massive_exploration_of_neural_machine_translation/,gwern,1490364396,,0,1
863,2017-3-24,2017,3,24,23,6196u2,Looking for a ML internship this summer in USA,https://www.reddit.com/r/MachineLearning/comments/6196u2/looking_for_a_ml_internship_this_summer_in_usa/,ashish9277,1490365252,[removed],0,1
864,2017-3-25,2017,3,25,0,619gcg,[P] Texture AIR,https://www.reddit.com/r/MachineLearning/comments/619gcg/p_texture_air/,seominlee,1490368051,https://github.com/seominlee/texture-air,1,3
865,2017-3-25,2017,3,25,0,619gpm,[R] Five video activity recognition methods implemented in Keras &amp; TensorFlow on UCF101,https://www.reddit.com/r/MachineLearning/comments/619gpm/r_five_video_activity_recognition_methods/,harvitronix,1490368156,,3,19
866,2017-3-25,2017,3,25,0,619jh0,Which are great examples of super human skill developed in machine learning?,https://www.reddit.com/r/MachineLearning/comments/619jh0/which_are_great_examples_of_super_human_skill/,[deleted],1490368952,[removed],0,1
867,2017-3-25,2017,3,25,1,619t2i,The concept of progressive machine learning,https://www.reddit.com/r/MachineLearning/comments/619t2i/the_concept_of_progressive_machine_learning/,thtr1310,1490371665,[removed],0,1
868,2017-3-25,2017,3,25,1,619x1g,[R]Evolution Strategies as a Scalable Alternative to Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/619x1g/revolution_strategies_as_a_scalable_alternative/,undefdev,1490372737,,44,126
869,2017-3-25,2017,3,25,1,619zul,[R] Reinforcement learning with Marr,https://www.reddit.com/r/MachineLearning/comments/619zul/r_reinforcement_learning_with_marr/,nocortex,1490373483,,0,3
870,2017-3-25,2017,3,25,2,61a76r,[Discussion] What is industry standard for workflow when training models?,https://www.reddit.com/r/MachineLearning/comments/61a76r/discussion_what_is_industry_standard_for_workflow/,inexion,1490375348,"I'm assuming that each time someone trains a model and wants to tweak it/iterate, that they don't have to wait hours and hours for it to learn and then output. So my question is, how do people manage this workflow?",3,1
871,2017-3-25,2017,3,25,3,61anyx,[D] Why is CIFAR-100 not widely used as a benchmark?,https://www.reddit.com/r/MachineLearning/comments/61anyx/d_why_is_cifar100_not_widely_used_as_a_benchmark/,andyzth,1490379791,"Many computer vision papers use ImageNet, MNIST, CIFAR-10, SVHN. But the number that use CIFAR-100 are far less. With state of the art on CIFAR-100 being &lt; 80% why isn't it more widely used?

Is the consensus that the dataset is too small to provide enough discrimination between 100 output classes? ",2,8
872,2017-3-25,2017,3,25,3,61aphv,[D] Cannot replicate a paper with well-known authors. What to do?,https://www.reddit.com/r/MachineLearning/comments/61aphv/d_cannot_replicate_a_paper_with_wellknown_authors/,MLnona1234,1490380230,"I have spent the past month trying to replicate a NIPS 2016 paper and have been unable to do so. I have contacted the authors (who are from a famous industry lab) for code and they said they cannot release code because of legal reasons. They have been very responsive regarding my questions. After a month of solid effort, I am still unable to replicate their results. 

I have a sneaking suspicion they were looking at their test results and got ""lucky"" with one of their runs, and that's the result they reported. I don't want to call them out on it because they are quite famous, but this also affects my research quite a bit because my model does not beat their reported results (it beats my implementation of their results by quite a large margin though).

Any advice on what to do?",30,58
873,2017-3-25,2017,3,25,4,61b2h3,[D] Paper Summary: Deepcoder: Learning to Write Programs,https://www.reddit.com/r/MachineLearning/comments/61b2h3/d_paper_summary_deepcoder_learning_to_write/,[deleted],1490383708,[deleted],0,1
874,2017-3-25,2017,3,25,4,61b2ne,[D] DeepCoder: Learning to Write Programs (Paper Summary),https://www.reddit.com/r/MachineLearning/comments/61b2ne/d_deepcoder_learning_to_write_programs_paper/,[deleted],1490383761,[deleted],0,0
875,2017-3-25,2017,3,25,5,61bh18,[D] Are there results related to exponentiated weights for supervised deep learning?,https://www.reddit.com/r/MachineLearning/comments/61bh18/d_are_there_results_related_to_exponentiated/,AI_entrepreneur,1490387578,"E.g. adaboost-type algorithms for boosting example weights during SGD. This would obviously not correspond to minimizing expected risk but from a numerical optimization perspective seems like it should help with focusing on particularly bad errors in the dataset. Of course some thought must go into the scheme, so curious if there are any papers on the subject or if anyone in this community has seen success with it previously.",5,0
876,2017-3-25,2017,3,25,6,61bp9r,[R] Can we learn the arrow of time from videos? (PDF),https://www.reddit.com/r/MachineLearning/comments/61bp9r/r_can_we_learn_the_arrow_of_time_from_videos_pdf/,urish,1490389888,,2,8
877,2017-3-25,2017,3,25,8,61cdik,Where can i find datasets for machine learning,https://www.reddit.com/r/MachineLearning/comments/61cdik/where_can_i_find_datasets_for_machine_learning/,anonymous123654789,1490397063,[removed],0,1
878,2017-3-25,2017,3,25,10,61cxwq,TensorFlow lab code (from linear regression to RNN),https://www.reddit.com/r/MachineLearning/comments/61cxwq/tensorflow_lab_code_from_linear_regression_to_rnn/,hunkims,1490403604,,0,1
879,2017-3-25,2017,3,25,10,61cyaa,How to Make a Language Translator - Intro to Deep Learning,https://www.reddit.com/r/MachineLearning/comments/61cyaa/how_to_make_a_language_translator_intro_to_deep/,ackstazya,1490403723,,0,1
880,2017-3-25,2017,3,25,10,61d46i,"[D] What is you favorite input scale? (0~255, 0~1, -1~1, -0.5~0.5, etc.)",https://www.reddit.com/r/MachineLearning/comments/61d46i/d_what_is_you_favorite_input_scale_0255_01_11/,[deleted],1490405749,[deleted],0,0
881,2017-3-25,2017,3,25,11,61db58,Movie Script Generalization for Neural Network,https://www.reddit.com/r/MachineLearning/comments/61db58/movie_script_generalization_for_neural_network/,[deleted],1490408244,[removed],0,1
882,2017-3-25,2017,3,25,11,61dcni,[P] Learning SVM through a simple app,https://www.reddit.com/r/MachineLearning/comments/61dcni/p_learning_svm_through_a_simple_app/,MinimalistKid,1490408814,"Hey friends. I'm learning about machine learning for fun before I graduate and built an app that tells me if my house is unlocked or locked. I'm using support vector machines, python, twilio and a simple http server to send me updates.
Very simple feature derived from binarizing the image taken at 10 second intervals and counts the max number of consecutively connected black dots.
The setup in the image below is just for testing. Ideally I want to have a mini webcam on the bookshelf against the wall to analyze the images in 10 second intervals. I didn't have a webcam so I'm using my iphone as a wireless camera. Then I wrote a script to pull that video feed into a png and process it into the model. The prediction is POST'ed to a server endpoint on my network.

video:
https://www.youtube.com/watch?v=SpYD1Sbcl9c&amp;feature=em-upload_owner


Image Setup: http://imgur.com/a/8zhK2


Attempting to train my model in a coffee shop: http://imgur.com/a/6WVwB",1,5
883,2017-3-25,2017,3,25,12,61dk2o,Failures of Deep Learning,https://www.reddit.com/r/MachineLearning/comments/61dk2o/failures_of_deep_learning/,josephd,1490411604,,1,1
884,2017-3-25,2017,3,25,13,61dsmy,[D] Is anyone aware of research on combining genetic programming with gradient descent for fitting functions to data?,https://www.reddit.com/r/MachineLearning/comments/61dsmy/d_is_anyone_aware_of_research_on_combining/,sanity,1490414940,[removed],0,1
885,2017-3-25,2017,3,25,14,61e0pu,What is Machine Learning? Algorithm Types &amp; Real Life Examples,https://www.reddit.com/r/MachineLearning/comments/61e0pu/what_is_machine_learning_algorithm_types_real/,aspiragas,1490418535,,0,1
886,2017-3-25,2017,3,25,14,61e0ua,[D] How can ML be used to estimate delivery date?,https://www.reddit.com/r/MachineLearning/comments/61e0ua/d_how_can_ml_be_used_to_estimate_delivery_date/,sumusername,1490418592,"For a shipment, how can you use ML to estimate the delivery date with only historical data?",5,0
887,2017-3-25,2017,3,25,18,61erqz,[PROJECT] Viola Jones vs CNN-Based face detection,https://www.reddit.com/r/MachineLearning/comments/61erqz/project_viola_jones_vs_cnnbased_face_detection/,sid3695,1490433763,"I want to implement face detection (not recognition) by myself. I am just a beginner, and therefore want to know which one of these could give me a reasonable accuracy and wouldn't take much time to implement.",6,4
888,2017-3-25,2017,3,25,19,61f0m4,Between Torch(PyTorch) and Tensorflow,https://www.reddit.com/r/MachineLearning/comments/61f0m4/between_torchpytorch_and_tensorflow/,theListening1,1490439595,[removed],0,1
889,2017-3-25,2017,3,25,20,61f2es,[R][1703.07950] Failures of Deep Learning,https://www.reddit.com/r/MachineLearning/comments/61f2es/r170307950_failures_of_deep_learning/,siddharth-agrawal,1490440834,,59,135
890,2017-3-25,2017,3,25,20,61f474,[D] What are your thoughts on MLaaS?,https://www.reddit.com/r/MachineLearning/comments/61f474/d_what_are_your_thoughts_on_mlaas/,pplonski,1490441734,[removed],0,1
891,2017-3-25,2017,3,25,21,61f7dg,[D] Paper suggestions on neural nets with external memory,https://www.reddit.com/r/MachineLearning/comments/61f7dg/d_paper_suggestions_on_neural_nets_with_external/,fu_2016,1490443332,I am looking for suggestions on memory networks. The only one that I am aware of currently is the one by Jason Weston - https://arxiv.org/abs/1410.3916v11. Also I consider NTMs to be included in this category. But the paper by Weston was released in late 2015. Can anyone suggest more recent papers that have further studied this approach ? Google scholar did not help with this matter.,12,6
892,2017-3-25,2017,3,25,22,61fikf,Between PyTorch and Tensorflow.,https://www.reddit.com/r/MachineLearning/comments/61fikf/between_pytorch_and_tensorflow/,theListening2,1490448514,[removed],0,1
893,2017-3-25,2017,3,25,23,61fna3,I have all the Reddit comments for the month of May 2015. What kind of interesting analysis can be performed using this data?,https://www.reddit.com/r/MachineLearning/comments/61fna3/i_have_all_the_reddit_comments_for_the_month_of/,rohits134,1490450502,[removed],0,1
894,2017-3-25,2017,3,25,23,61fuiy,What convolutional neural networks look at when they see nudity,https://www.reddit.com/r/MachineLearning/comments/61fuiy/what_convolutional_neural_networks_look_at_when/,[deleted],1490453186,[deleted],0,1
895,2017-3-25,2017,3,25,23,61fusp,Kaggle notebook completely lost 2 hours of hard data science work. Stay away from Kaggle notebook.,https://www.reddit.com/r/MachineLearning/comments/61fusp/kaggle_notebook_completely_lost_2_hours_of_hard/,datasciguy-aaay,1490453291,[removed],0,1
896,2017-3-26,2017,3,26,0,61g0xo,[P] Help needed in creating a network of collocations,https://www.reddit.com/r/MachineLearning/comments/61g0xo/p_help_needed_in_creating_a_network_of/,abstroos_geek,1490455365,I have a corpus and wanted to extract key words and create a network of collocations. Any idea on how I proceed. Thanks.,2,1
897,2017-3-26,2017,3,26,0,61g37e,[D] How would you tackle 3D object reconstruction from one or multiple 2D images?,https://www.reddit.com/r/MachineLearning/comments/61g37e/d_how_would_you_tackle_3d_object_reconstruction/,theredknight,1490456130,"While I'm still learning the ropes on ML, I find myself thinking and planning more complex challenges for further down the road. One of these ideas is 3D object generation and I thought it might be useful to post to brain dump for myself, but also for others who are curious about this topic as well.   
  
So how would you create a neural network / machine learning project for generating a 3D model from 2D images? Does anyone know of any projects doing this with machine learning (that aren't listed below)?   
  
##Some Ideas on Strategy:  
----  
  
**Analyzing Images**  
My first thought to is to use something similar to scanning the way written number detection with [the MNIST examples](https://www.tensorflow.org/versions/r0.10/tutorials/mnist/beginners/) works to scan an image in order to extract features. However I'd also add a feature to provide camera positions if you had multiple images.   
  
**Generating 3D Models**  
Then, and this is the part I'm still working to visualize more clearly, is how you would then generate the 3d model. My guess is to use [Wavefront objects](https://en.wikipedia.org/wiki/Wavefront_.obj_file) which are easy to generate text-wise. I'm just not seeing how you would check it is all.   
  
**Using a GAN**  
Perhaps use a [GAN](https://github.com/hanzhanggit/StackGAN) to identify a verified 3d model versus the generated one and render them and compare which render looks better?  
  
**Training**  
Also, would you need to have an database of 1,000+ 3d models to work from? Another option that could be doable is scripting a program to generate models. [Blender 3D](https://docs.blender.org/api/blender_python_api_2_67_release/) has a fairly easy to use python api which can generate via command line and tied in. There are also a [large amount of objects](http://www.blendswap.com/blends) available online. 
  
Once the simple mechanics (get the NN generating some 3D objects) are working, I'd like to tie in some more clever methods from research papers with better algorithms:  
   
##Research Papers  
----  
  
**Any Objects:**  
  
* [Bundled Depth-Map Merging for Multi-View Stereo (2010)](https://sites.google.com/site/leeplus/bmvs)  
* [Single-View 3D Reconstruction (2011)](http://vision.in.tum.de/research/image-based_3d_reconstruction/singleviewreconstruction)  
* [Multi-View 3D Reconstruction](http://vision.in.tum.de/research/image-based_3d_reconstruction/multiviewreconstruction)  
* [30 tools to make a 3D scan](https://www.slideshare.net/sketchfab/30-tools-to-make-a-3d-scan)  
  
**Other NN 3D Reconstruction Projects:**  
  
* [3D-R2N2: 3D Recurrent Reconstruction Neural Network](http://3d-r2n2.stanford.edu/)  
* [3D ShapeNets: A Deep Representation for Volumetric Shapes](http://3dshapenets.cs.princeton.edu/)  
* [3D Shape Induction from 2D Views of Multiple Objects](https://arxiv.org/pdf/1612.05872.pdf)  
* [Deep Adversarial 3D Shape Net](https://www.andrew.cmu.edu/user/hsuehtil/pdf/10807FinalReport.pdf)  
* [Unsupervised Learning of 3D Structure from Images](https://arxiv.org/pdf/1607.00662.pdf)  
  
**Humans specifically:**  
  
* [SMPLify: 3D Human Pose and Shape from a Single Image (ECCV 2016)](https://www.youtube.com/watch?v=eUnZ2rjxGaE)
* [3D Faces Generated From 2D Photos](https://medium.com/transmission-newsletter/3d-faces-generated-from-2d-photos-machines-learning-to-hand-write-more-7729c839e7f6#.2v0c72so9)  
* [3D Reconstruction of Face from 2D CT Scan Images (2012)](http://www.sciencedirect.com/science/article/pii/S1877705812009629)  
    
  
##3D Scanning Products:  
----  
  
* [Autodesk 123D](http://www.123dapp.com/catch)  
* [Photoscan](http://www.agisoft.com/)  
* [Reconstruct Me](http://reconstructme.net/)  
* [Behance](https://www.behance.net/gallery/17168641/3D-Reconstruction-Images-to-3D-model-Free-App)  
* [Facegen](https://facegen.com/)   
* [Blender 3D discussion](http://blender.stackexchange.com/questions/63246/how-do-i-make-a-3-dimensional-character-out-of-flat-picture)    
  
edit: formatting and added links.",16,23
898,2017-3-26,2017,3,26,1,61g8sl,"[P] Golem is a global, open sourced, decentralized supercomputer that anyone can access. It's made up of the combined power of user's machines, from personal laptops to entire datacenters.",https://www.reddit.com/r/MachineLearning/comments/61g8sl/p_golem_is_a_global_open_sourced_decentralized/,ssiwhw,1490457988,,16,132
899,2017-3-26,2017,3,26,1,61gdda,The Convergence of Machine Learning and Artificial Intelligence Towards Enabling Autonomous Driving,https://www.reddit.com/r/MachineLearning/comments/61gdda/the_convergence_of_machine_learning_and/,Mussem17,1490459422,,0,1
900,2017-3-26,2017,3,26,3,61h427,"[D] What heuristic is used when papers say ""we lower the learning rate when the validation error plateaus""?",https://www.reddit.com/r/MachineLearning/comments/61h427/d_what_heuristic_is_used_when_papers_say_we_lower/,Ayakalam,1490467527,"
A bunch of papers I come across say that they will divide the error rate by, for example, 10, when the validation error ""starts to plateau"". 

My question is how do they figure out that a plateau has occurred? Surely there is another hyper-parameter and heuristic algorithm at work if this is done automatically? If so, what is this algorithm?",20,4
901,2017-3-26,2017,3,26,3,61h4h6,Is it possible to combine multiple forecasting models using Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/61h4h6/is_it_possible_to_combine_multiple_forecasting/,satwik_,1490467646,[removed],0,1
902,2017-3-26,2017,3,26,3,61h4mt,[R] [1703.06857v1] Deep Neural Networks Do Not Recognize Negative Images,https://www.reddit.com/r/MachineLearning/comments/61h4mt/r_170306857v1_deep_neural_networks_do_not/,[deleted],1490467687,[deleted],6,0
903,2017-3-26,2017,3,26,6,61hvpj,Automation,https://www.reddit.com/r/MachineLearning/comments/61hvpj/automation/,stanyew,1490476250,,0,1
904,2017-3-26,2017,3,26,10,61j2ky,Tesla self-driving software,https://www.reddit.com/r/MachineLearning/comments/61j2ky/tesla_selfdriving_software/,[deleted],1490490695,[removed],0,1
905,2017-3-26,2017,3,26,12,61jqpk,reference for math symbols in deep learning?,https://www.reddit.com/r/MachineLearning/comments/61jqpk/reference_for_math_symbols_in_deep_learning/,MidnightProgrammer,1490500275,[removed],0,1
906,2017-3-26,2017,3,26,15,61kb9b,[P] Machine Learning Research series on Medium,https://www.reddit.com/r/MachineLearning/comments/61kb9b/p_machine_learning_research_series_on_medium/,[deleted],1490509932,[deleted],1,0
907,2017-3-26,2017,3,26,16,61khf5,Uber to Suspend Autonomous Tests After Arizona Accident,https://www.reddit.com/r/MachineLearning/comments/61khf5/uber_to_suspend_autonomous_tests_after_arizona/,n_jai,1490513587,,0,1
908,2017-3-26,2017,3,26,19,61kw9x,NLP algorithms and text analytics Slack chatbot,https://www.reddit.com/r/MachineLearning/comments/61kw9x/nlp_algorithms_and_text_analytics_slack_chatbot/,shamitb,1490522971,,0,1
909,2017-3-26,2017,3,26,19,61kym3,"[P] Poker hand classification, advice needed",https://www.reddit.com/r/MachineLearning/comments/61kym3/p_poker_hand_classification_advice_needed/,djhworld,1490524353,"I'm fairly new to machine learning so please be gentle. 

Using tensorflow and this dataset https://archive.ics.uci.edu/ml/datasets/Poker+Hand, I've been experimenting whether I can build a network to correctly classify poker hands.

My network can achieve an accuracy of 99.27% on the testing set (1,000,000 rows) but it has the following problems 

1. It's very good at recognising the 'simpler' hands (Nothing, One Pair, Two Pair, Three of a Kind) 
2. It gets mixed, to really bad results on hands like Straight, Flush, Royal flush etc 

It is setup as follows

85 (input) -&gt; 85 (hidden - ReLu activation) -&gt; 85 (hidden - ReLu activation) -&gt; 10 (softmax)  

I've been looking at some research on people using the same dataset to do the same thing, and most seem to conclude that the training data of 25,000 rows is heavily biased towards the simpler hands (for example the ""Four of a Kind"" examples only have 6 instances, whereas one pair has 10,599 instances) 

Does anyone know of anything I can do to get this network to do any better? Or is this a really hard problem, considering the high value hands are rarer and can easily be confused for something else. ",23,22
910,2017-3-26,2017,3,26,20,61l3f5,Dance Dance Convolution (automatic step chart creation) - X-post r/Stepmania,https://www.reddit.com/r/MachineLearning/comments/61l3f5/dance_dance_convolution_automatic_step_chart/,ITBlueMagma,1490527050,,0,1
911,2017-3-26,2017,3,26,20,61l8al,Help needed converting word2vec dataset to tsv for tensorboard,https://www.reddit.com/r/MachineLearning/comments/61l8al/help_needed_converting_word2vec_dataset_to_tsv/,[deleted],1490529577,[removed],0,1
912,2017-3-26,2017,3,26,22,61lgey,How to detect a malicious piece of code,https://www.reddit.com/r/MachineLearning/comments/61lgey/how_to_detect_a_malicious_piece_of_code/,anonymous123654789,1490533473,[removed],0,1
913,2017-3-26,2017,3,26,22,61lj2r,[D] What are the some very well written ML papers to practice how to structurize complex ideas?,https://www.reddit.com/r/MachineLearning/comments/61lj2r/d_what_are_the_some_very_well_written_ml_papers/,nocortex,1490534671,,18,82
914,2017-3-26,2017,3,26,22,61lj9e,[P]word2vec data to tsv for tensorboard,https://www.reddit.com/r/MachineLearning/comments/61lj9e/pword2vec_data_to_tsv_for_tensorboard/,LorD-U-n0-Po0,1490534753,I am facing problems while creating a proper tsv file for my word2vec dataset . I have trained a dataset and created a w2v file.But I don't know how to convert it into a proper tsv file for uploading on tensorboard's embedding projector.I am experimenting with this GitHub repo https://github.com/llSourcell/word_vectors_game_of_thrones-LIVE Thanks,2,7
915,2017-3-26,2017,3,26,23,61lpt6,Machine Learning technique for cost minimization,https://www.reddit.com/r/MachineLearning/comments/61lpt6/machine_learning_technique_for_cost_minimization/,gard3zi,1490537382,[removed],0,1
916,2017-3-26,2017,3,26,23,61ls4f,ADEM vs RUBER for a dialog system. Which is better and why?,https://www.reddit.com/r/MachineLearning/comments/61ls4f/adem_vs_ruber_for_a_dialog_system_which_is_better/,[deleted],1490538253,[removed],0,1
917,2017-3-26,2017,3,26,23,61ltw9,[D] ADEM vs RUBER for a dialog system. Which is better and why?,https://www.reddit.com/r/MachineLearning/comments/61ltw9/d_adem_vs_ruber_for_a_dialog_system_which_is/,shaynekang,1490538900,"Hi. I've been looking into automatic evaluation for a dialog system in order to analyze our chatbot.


Upon my survey, there are two state of the art automatic evaluation algorithms, [ADEM](https://openreview.net/forum?id=HJ5PIaseg&amp;noteId=HJ5PIaseg) and [RUBER](https://arxiv.org/abs/1701.03079). I digged in these two papers, but I couldn't find the exact differences between these two systems from a performance angle.


Can anyone give me some insight into these systems? More precisely, which system will give me better accuracy? And is there any other better solution besides these two systems?",2,5
918,2017-3-26,2017,3,26,23,61lu79,[D] Weight initialization for custom layers?,https://www.reddit.com/r/MachineLearning/comments/61lu79/d_weight_initialization_for_custom_layers/,Kiuhnm,1490539007,"I'm trying out some exotic layers and it seems that the gradients don't propagate as well as they should.

Deriving a proper weight initialization function analytically is intractable or very tedious in my case.

Is there an alternative like a general iterative algorithm?

I know about Batch Normalization but that's for reducing internal covariate shift. It helps with weights but I don't think it's a complete substitute for correct initialization. Correct me if I'm wrong.",5,1
919,2017-3-27,2017,3,27,3,61mxvc,[P]: Let's make an A3C: Implementation | Keras &amp; TensorFlow with GPU | 300 lines | 30 seconds to train in CartPole environment,https://www.reddit.com/r/MachineLearning/comments/61mxvc/p_lets_make_an_a3c_implementation_keras/,jaromiru,1490551551,,2,7
920,2017-3-27,2017,3,27,3,61n47z,From Data Analysis to Machine Learning,https://www.reddit.com/r/MachineLearning/comments/61n47z/from_data_analysis_to_machine_learning/,pmz,1490553399,,0,1
921,2017-3-27,2017,3,27,3,61n4gq,An agent library for building nested automata,https://www.reddit.com/r/MachineLearning/comments/61n4gq/an_agent_library_for_building_nested_automata/,[deleted],1490553468,[deleted],0,1
922,2017-3-27,2017,3,27,3,61n6i8,Machine Learning for Developers // Speaker Deck,https://www.reddit.com/r/MachineLearning/comments/61n6i8/machine_learning_for_developers_speaker_deck/,pmz,1490554073,,0,1
923,2017-3-27,2017,3,27,5,61npmh,[P] Neural Image Captioning using TensorFlow,https://www.reddit.com/r/MachineLearning/comments/61npmh/p_neural_image_captioning_using_tensorflow/,[deleted],1490559683,[deleted],0,1
924,2017-3-27,2017,3,27,5,61nsdo,[P] Neural Image Captioning using TensorFlow,https://www.reddit.com/r/MachineLearning/comments/61nsdo/p_neural_image_captioning_using_tensorflow/,dirac-hatt,1490560502,,1,0
925,2017-3-27,2017,3,27,6,61nxoj,Best Resources on internet for Machine learning,https://www.reddit.com/r/MachineLearning/comments/61nxoj/best_resources_on_internet_for_machine_learning/,mlwhiz,1490562047,,0,1
926,2017-3-27,2017,3,27,6,61o5vi,How do you deal with notation while reading ML papers?,https://www.reddit.com/r/MachineLearning/comments/61o5vi/how_do_you_deal_with_notation_while_reading_ml/,2embarrassed2quit,1490564510,[removed],0,1
927,2017-3-27,2017,3,27,7,61o9wu,The Feynman learning technique applied to machine learning (long form essay),https://www.reddit.com/r/MachineLearning/comments/61o9wu/the_feynman_learning_technique_applied_to_machine/,gatorwatt,1490565749,,0,1
928,2017-3-27,2017,3,27,7,61ocaw,[D] RFC: Starting (Bi)Weekly Reinforcement Learning Digests,https://www.reddit.com/r/MachineLearning/comments/61ocaw/d_rfc_starting_biweekly_reinforcement_learning/,omtcyfz,1490566530,,1,11
929,2017-3-27,2017,3,27,7,61ojbj,[D] CNN Scale Invariance,https://www.reddit.com/r/MachineLearning/comments/61ojbj/d_cnn_scale_invariance/,FelixMooray145,1490568768,"What are considered to be state of the art methods to deal with this?


The only way I found to deal with this is data augmentation, same as with rotation.


As best as I could determine what happens is that pooling or stride&gt;1 ultimately scales any object to some similar size but it happens at different layers which no doubt results in a lot of problems.",7,0
930,2017-3-27,2017,3,27,10,61pflk,"Does anyone have a successful implementation of ""one class"" classifier using deep learning?",https://www.reddit.com/r/MachineLearning/comments/61pflk/does_anyone_have_a_successful_implementation_of/,[deleted],1490579566,[removed],0,1
931,2017-3-27,2017,3,27,11,61ph9c,(Question) How hard is to write an app that recognizes plants by photos of them?,https://www.reddit.com/r/MachineLearning/comments/61ph9c/question_how_hard_is_to_write_an_app_that/,0no0more0,1490580141,[removed],0,1
932,2017-3-27,2017,3,27,15,61qntv,Gradient Descent Optimizers - why don't we care about generalization?,https://www.reddit.com/r/MachineLearning/comments/61qntv/gradient_descent_optimizers_why_dont_we_care/,[deleted],1490597525,[removed],0,1
933,2017-3-27,2017,3,27,16,61qotu,Getting constraints on features that have high confidence on classification,https://www.reddit.com/r/MachineLearning/comments/61qotu/getting_constraints_on_features_that_have_high/,jackee1234,1490598006,[removed],0,1
934,2017-3-27,2017,3,27,16,61qs1y,[R] On the Robustness of Convolutional Neural Networks to Internal Architecture and Weight Perturbations,https://www.reddit.com/r/MachineLearning/comments/61qs1y/r_on_the_robustness_of_convolutional_neural/,Mimrash,1490599609,,0,1
935,2017-3-27,2017,3,27,17,61qx1r,EUs copyright reforms might be bad for AI startups,https://www.reddit.com/r/MachineLearning/comments/61qx1r/eus_copyright_reforms_might_be_bad_for_ai_startups/,alekhka,1490602204,,0,1
936,2017-3-27,2017,3,27,17,61qyoa,[P] Hidden Markov Models and Accelerometer data,https://www.reddit.com/r/MachineLearning/comments/61qyoa/p_hidden_markov_models_and_accelerometer_data/,matavelhos,1490603129,"Hi guys, I have a project in hands, where the core is accelerometer data. 

I already have, a csv file with about 65000 lines, with time ordered chunks of data, and its labeled.

So, i'm trying classify this chunks of data. 

I already use, random forest after do some features extraction with sliding window.

But know, i'm trying improve my results, and read about Hidden Markov models. But i'm having a lot of trouble to understand and implementing this.

I'm using Python, or Weka... In Weka, for what i read, Hidden Markov Model have some problems... In Python, i'm trying use pomegranate package. I already read this example (https://github.com/jmschrei/pomegranate/blob/master/tutorials/Tutorial_3_Hidden_Markov_Models.ipynb), but it's hard, for me, to think how i can implement this in my problem.

TL/DR- It's my first time working with Hidden Markov Models, and i need help with how to implement this in my problem (accelerometer data).

Thanks for all.",8,7
937,2017-3-27,2017,3,27,17,61qz2b,"Any ""generic"" CUDAs?",https://www.reddit.com/r/MachineLearning/comments/61qz2b/any_generic_cudas/,taewoo,1490603357,[removed],0,1
938,2017-3-27,2017,3,27,17,61r0qj,[D] Which machine learning algorithm to choose for my problem ?,https://www.reddit.com/r/MachineLearning/comments/61r0qj/d_which_machine_learning_algorithm_to_choose_for/,Nico_lrx,1490604291,,4,4
939,2017-3-27,2017,3,27,17,61r16m,[D] Types of classifier,https://www.reddit.com/r/MachineLearning/comments/61r16m/d_types_of_classifier/,Randomhkkid,1490604540,"So I have 7 different classifiers that I'm having trouble grouping together.

At the moment I'm calling these 'Machine Learning approaches':

* Random Forest
* Support Vector Machine
* Logistic Regression
* Multi-layer Perceptron

My problem is what to call these, as I don't really want to just call them 'Non Machine Learning approaches':

* Term Frequency Similarity
* Cosine Similarity
* k Nearest Neighbour

Ideally I'd have 2 groups of classifiers. For context I'm classifying document vectors for author classification.
",29,12
940,2017-3-27,2017,3,27,17,61r1at,Deep Anomaly Detection?,https://www.reddit.com/r/MachineLearning/comments/61r1at/deep_anomaly_detection/,thenerdstation,1490604606,[removed],0,1
941,2017-3-27,2017,3,27,18,61r34n,[D] Can anyone give a real life example of supervised learning and unsupervised learning?,https://www.reddit.com/r/MachineLearning/comments/61r34n/d_can_anyone_give_a_real_life_example_of/,datavinci,1490605628,"There is a stackoverflow question with same title, but the answers given there are not satisfactory and not pertaining to the title.",11,0
942,2017-3-27,2017,3,27,18,61r3g9,New Approaches to Unsupervised Domain Adaptation,https://www.reddit.com/r/MachineLearning/comments/61r3g9/new_approaches_to_unsupervised_domain_adaptation/,reworksophie,1490605775,,0,1
943,2017-3-27,2017,3,27,18,61r6fj,[D] Image matching with Siamese Networks and Euclidean distance,https://www.reddit.com/r/MachineLearning/comments/61r6fj/d_image_matching_with_siamese_networks_and/,tryndisskilled,1490607328,"Hi all,

Following my reading of this paper: https://users.aalto.fi/~kannalj1/publications/icpr2016.pdf about Image matching, I wondered if such technics could work with different data.

In the paper, the classes they want to match are pretty wide: pictures of towns, of certain buildings, etc... Would such a network, using an Euclidean distance function as well, be able to match images with much less similarities? 

Let's say your dataset is based on fingerprints, and your different classes corresponds to as many users. As explained in the paper, pairs of fingerpints would be created (positive for fingerprints from the same user and finger, negative otherwise). [Example of fingerpint](http://thewindowsclub.thewindowsclubco.netdna-cdn.com/wp-content/uploads/2015/04/fingerprint-reader.jpg) (note the actual fingerprints would contain additional noise)

Would CNNs be able to extract enough features in order to do any matching?

",12,11
944,2017-3-27,2017,3,27,19,61rczw,internalizing hyperparameters,https://www.reddit.com/r/MachineLearning/comments/61rczw/internalizing_hyperparameters/,gr_eabe,1490610535,[removed],0,1
945,2017-3-27,2017,3,27,19,61rfaa,Are there any approaches of developing video transcription?,https://www.reddit.com/r/MachineLearning/comments/61rfaa/are_there_any_approaches_of_developing_video/,rousse101,1490611614,[removed],0,1
946,2017-3-27,2017,3,27,20,61rhxw,I have $5k worth of computing power for 4 days.,https://www.reddit.com/r/MachineLearning/comments/61rhxw/i_have_5k_worth_of_computing_power_for_4_days/,[deleted],1490612851,[removed],0,1
947,2017-3-27,2017,3,27,20,61ro6j,"[P] Value Iteration Network (VIN) in TensorFlow: Clean, Simple and Modular",https://www.reddit.com/r/MachineLearning/comments/61ro6j/p_value_iteration_network_vin_in_tensorflow_clean/,xingdongrobotics,1490615363,,0,2
948,2017-3-27,2017,3,27,20,61rody,"[P] Value Iteration Network (VIN) in PyTorch and Visdom: Clean, Simple and Modular with visualization",https://www.reddit.com/r/MachineLearning/comments/61rody/p_value_iteration_network_vin_in_pytorch_and/,xingdongrobotics,1490615449,,10,14
949,2017-3-27,2017,3,27,21,61rpp1,How is double Planetary Mixer operating,https://www.reddit.com/r/MachineLearning/comments/61rpp1/how_is_double_planetary_mixer_operating/,JCT_Janice,1490616018,,0,1
950,2017-3-27,2017,3,27,21,61rqqm,"Request solution manual for Information Theory, Inference and Learning Algorithms (David MacKay)",https://www.reddit.com/r/MachineLearning/comments/61rqqm/request_solution_manual_for_information_theory/,marcosmarxm,1490616390,[removed],0,1
951,2017-3-27,2017,3,27,21,61rr0r,"Suggestions on a simple deep learning result that made you feel ""WOW!!!!""",https://www.reddit.com/r/MachineLearning/comments/61rr0r/suggestions_on_a_simple_deep_learning_result_that/,lvbu,1490616501,[removed],0,1
952,2017-3-27,2017,3,27,21,61rx82,Install NumPy on Python in 1min !!,https://www.reddit.com/r/MachineLearning/comments/61rx82/install_numpy_on_python_in_1min/,[deleted],1490618820,,0,1
953,2017-3-27,2017,3,27,21,61rye8,[D] Is beam search decoding critical?,https://www.reddit.com/r/MachineLearning/comments/61rye8/d_is_beam_search_decoding_critical/,longinglove,1490619232,"I've read many papers that employed beam search in the decoding phase. I have some questions. 1. How much does it contribute to the performance, such as accuracy or bleu score? 2. How slow will it be if we use beam search? 3. Does anybody know any nice and clean Tensorflow implementation of a beam search decoder?",7,6
954,2017-3-27,2017,3,27,22,61s0to,Machine learning examples,https://www.reddit.com/r/MachineLearning/comments/61s0to/machine_learning_examples/,palash918,1490620071,[removed],0,1
955,2017-3-27,2017,3,27,22,61s1d0,"So, you want to found an AI startup?",https://www.reddit.com/r/MachineLearning/comments/61s1d0/so_you_want_to_found_an_ai_startup/,nb410,1490620243,,0,1
956,2017-3-27,2017,3,27,22,61s7xp,[R] Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization (code in Torch for arXiv:1703.06868),https://www.reddit.com/r/MachineLearning/comments/61s7xp/r_arbitrary_style_transfer_in_realtime_with/,pmigdal,1490622363,,1,44
957,2017-3-27,2017,3,27,22,61s8gc,[N] AMD: ROCm Project Open Position for TensorFlow/Caffe/Torch Implementation,https://www.reddit.com/r/MachineLearning/comments/61s8gc/n_amd_rocm_project_open_position_for/,entinthemountains,1490622525,,4,9
958,2017-3-27,2017,3,27,23,61sbby,[R] Deep Photo Style Transfer (code and data for paper arXiv:1703.07511),https://www.reddit.com/r/MachineLearning/comments/61sbby/r_deep_photo_style_transfer_code_and_data_for/,pmigdal,1490623411,,43,217
959,2017-3-27,2017,3,27,23,61scql,Is there anything to this that is particularly novel or interesting?,https://www.reddit.com/r/MachineLearning/comments/61scql/is_there_anything_to_this_that_is_particularly/,[deleted],1490623821,[deleted],0,1
960,2017-3-27,2017,3,27,23,61sdwq,Learning AI and ML (knowledge of Math),https://www.reddit.com/r/MachineLearning/comments/61sdwq/learning_ai_and_ml_knowledge_of_math/,iinfi1,1490624167,[removed],0,1
961,2017-3-27,2017,3,27,23,61shmi,[D] Is there anything to this that is particularly novel or interesting?,https://www.reddit.com/r/MachineLearning/comments/61shmi/d_is_there_anything_to_this_that_is_particularly/,cello_there,1490625240,,4,2
962,2017-3-27,2017,3,27,23,61shx8,Learn to build Neural Networks from raw equation with tensorflow. (Repost),https://www.reddit.com/r/MachineLearning/comments/61shx8/learn_to_build_neural_networks_from_raw_equation/,[deleted],1490625330,[deleted],0,1
963,2017-3-27,2017,3,27,23,61skbh,New guy here trying to learn something...,https://www.reddit.com/r/MachineLearning/comments/61skbh/new_guy_here_trying_to_learn_something/,forsakenz0r,1490626008,[removed],0,1
964,2017-3-27,2017,3,27,23,61skf6,Learn to build neural network from raw equations with Tensorflow.,https://www.reddit.com/r/MachineLearning/comments/61skf6/learn_to_build_neural_network_from_raw_equations/,kazi_shezan,1490626037,,0,1
965,2017-3-28,2017,3,28,0,61svh6,[D] Question about ResNets,https://www.reddit.com/r/MachineLearning/comments/61svh6/d_question_about_resnets/,Kiuhnm,1490629073,"We know that depth helps with generalization but depth also makes optimization a lot harder because of the vanishing/exploding gradient problem. The identity mappings used by ResNets mainly simplify optimization.

**Isn't this just a way to bias the net in favor of linear transformations?**

What would happen if we did the same without skip connections? For instance we could use I+W instead of W for the weights and PReLU(1+a) instead of ReLU.",8,3
966,2017-3-28,2017,3,28,1,61t8f0,What are some great sources regarding person re-identification?,https://www.reddit.com/r/MachineLearning/comments/61t8f0/what_are_some_great_sources_regarding_person/,MLenthousiast,1490632515,[removed],0,1
967,2017-3-28,2017,3,28,1,61t9n9,Are memories a way to retrain our brain to prevent forgetting ?,https://www.reddit.com/r/MachineLearning/comments/61t9n9/are_memories_a_way_to_retrain_our_brain_to/,datatatatata,1490632850,[removed],0,1
968,2017-3-28,2017,3,28,3,61tu4a,Trying to speed process using PyCuda. Anyone want to join me in the learning process?,https://www.reddit.com/r/MachineLearning/comments/61tu4a/trying_to_speed_process_using_pycuda_anyone_want/,[deleted],1490638021,[removed],0,1
969,2017-3-28,2017,3,28,3,61u379,How to use tensorflow for applying neural networks in robot?,https://www.reddit.com/r/MachineLearning/comments/61u379/how_to_use_tensorflow_for_applying_neural/,chirag2796,1490640406,[removed],0,1
970,2017-3-28,2017,3,28,4,61u8w5,[R][1703.05698] Bayesian Sketch Learning for Program Synthesis,https://www.reddit.com/r/MachineLearning/comments/61u8w5/r170305698_bayesian_sketch_learning_for_program/,Constipated__Guy,1490641909,,1,5
971,2017-3-28,2017,3,28,4,61u8xt,Suggestions for sources for Investment AI,https://www.reddit.com/r/MachineLearning/comments/61u8xt/suggestions_for_sources_for_investment_ai/,Thegingerbread_man,1490641923,[removed],0,1
972,2017-3-28,2017,3,28,4,61uixw,Spark MLib for Scalable Machine Learning with Spark,https://www.reddit.com/r/MachineLearning/comments/61uixw/spark_mlib_for_scalable_machine_learning_with/,Shadab_Mohd,1490644553,,0,1
973,2017-3-28,2017,3,28,7,61viop,How to get started in a career in ML?,https://www.reddit.com/r/MachineLearning/comments/61viop/how_to_get_started_in_a_career_in_ml/,suugarpie,1490654536,[removed],0,1
974,2017-3-28,2017,3,28,7,61vk1a,Going back to school for machine learning,https://www.reddit.com/r/MachineLearning/comments/61vk1a/going_back_to_school_for_machine_learning/,thesmokingpants,1490654923,[removed],0,1
975,2017-3-28,2017,3,28,7,61vlqe,Elon Musks OpenAI Unveils a Simpler Way for Machines to Learn,https://www.reddit.com/r/MachineLearning/comments/61vlqe/elon_musks_openai_unveils_a_simpler_way_for/,lopespm,1490655459,,0,1
976,2017-3-28,2017,3,28,8,61vo4u,[P] Parrot sumo drone with real time RCNN object classification (code),https://www.reddit.com/r/MachineLearning/comments/61vo4u/p_parrot_sumo_drone_with_real_time_rcnn_object/,Forthtemple,1490656198,,0,2
977,2017-3-28,2017,3,28,8,61vsy8,New Data Science Book,https://www.reddit.com/r/MachineLearning/comments/61vsy8/new_data_science_book/,fieldcady,1490657670,[removed],0,1
978,2017-3-28,2017,3,28,8,61vvjf,"[P] Navigate Google Images, while readjusting your query-parameters or dropping it entirely. A glimpse into how Google's ""Similar Images"" work.",https://www.reddit.com/r/MachineLearning/comments/61vvjf/p_navigate_google_images_while_readjusting_your/,NegatioNZor,1490658509,,0,0
979,2017-3-28,2017,3,28,9,61w6w7,[D] - Neural Nets for Image Generation (with no inputs/outputs (except maybe random numbers if it's needed) - just generation from a dataset)?,https://www.reddit.com/r/MachineLearning/comments/61w6w7/d_neural_nets_for_image_generation_with_no/,beef__,1490662019,"I'm not looking for a chunk of code as an answer, just the name of the model I'd need to implement or some links would be nice.

My problem is I have a dataset I've made of a few hundred 128x128 images (abstract paintings) - I'd like to simply generate more images similar to these images using a neural network (preferably no input needed for the network, except maybe random values?), but it's unclear as to how I'd go about this.

One solution I've thought about but haven't tried out yet is making an LSTM neural network, turning the paintings into 1D arrays of pixel values, and feeding the arrays to the network (LSTM networks are real good at learning sequences) - but if I'd want to work with larger images, this might not be very practical.

Any info is greatly appreciated. Thanks!",8,6
980,2017-3-28,2017,3,28,9,61w6zu,"Now that the Machinelearning subreddit has neatly labeled post data i.e. project, discussion, etc. Could you make an algorithm that predicts what the likelihood of the next post will be?",https://www.reddit.com/r/MachineLearning/comments/61w6zu/now_that_the_machinelearning_subreddit_has_neatly/,i_went_full_retard,1490662044,[removed],0,1
981,2017-3-28,2017,3,28,9,61w80t,StyleBank: An Explicit Representation for Style Transfer (MSR-Asia),https://www.reddit.com/r/MachineLearning/comments/61w80t/stylebank_an_explicit_representation_for_style/,[deleted],1490662377,[removed],0,1
982,2017-3-28,2017,3,28,10,61w9vf,StyleBank: An Explicit Representation for Style Transfer (MSRA),https://www.reddit.com/r/MachineLearning/comments/61w9vf/stylebank_an_explicit_representation_for_style/,[deleted],1490662985,[deleted],0,1
983,2017-3-28,2017,3,28,10,61wc2q,StyleBank: An Explicit Representation for Style Transfer (MSRA),https://www.reddit.com/r/MachineLearning/comments/61wc2q/stylebank_an_explicit_representation_for_style/,[deleted],1490663715,[deleted],0,1
984,2017-3-28,2017,3,28,10,61wdy2,Coherent Online Video Style Transfer (MSRA),https://www.reddit.com/r/MachineLearning/comments/61wdy2/coherent_online_video_style_transfer_msra/,[deleted],1490664324,[deleted],0,1
985,2017-3-28,2017,3,28,11,61wr0b,$3k Machine Learning Computer Build -- help me build it?,https://www.reddit.com/r/MachineLearning/comments/61wr0b/3k_machine_learning_computer_build_help_me_build/,[deleted],1490668541,[removed],0,1
986,2017-3-28,2017,3,28,11,61wrfh,How many hidden unit and hidden layer is better in LSTM neural network NLP tasks,https://www.reddit.com/r/MachineLearning/comments/61wrfh/how_many_hidden_unit_and_hidden_layer_is_better/,[deleted],1490668684,[removed],0,1
987,2017-3-28,2017,3,28,11,61wv57,[R] [1703.08864] Learning Simpler Language Models with the Delta Recurrent Neural Network Framework &lt;-- outperforms/equates GRU/LSTM and has almost as few parameters as a vanilla RNN,https://www.reddit.com/r/MachineLearning/comments/61wv57/r_170308864_learning_simpler_language_models_with/,evc123,1490669948,,23,18
988,2017-3-28,2017,3,28,12,61x3mu,[D] Simple batch normalization vs. Global batch normalization?,https://www.reddit.com/r/MachineLearning/comments/61x3mu/d_simple_batch_normalization_vs_global_batch/,longinglove,1490672959,"According to https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/moments, for a 4-D tensor of shape [batch, height, width, depth], we can apply the batch normalization to it along the first axis (simple batch normalization) or along the all axes but the last one (global normalization). Actually I think the original idea is close to the former, but apparently the latter is taken as the default. Why is that? ",0,1
989,2017-3-28,2017,3,28,12,61x4t0,[R] StyleBank: An Explicit Representation for Neural Image Style Transfer (MSRA),https://www.reddit.com/r/MachineLearning/comments/61x4t0/r_stylebank_an_explicit_representation_for_neural/,e_walker,1490673384,,4,8
990,2017-3-28,2017,3,28,13,61x7g0,[R] Coherent Online Video Style Transfer (MSRA),https://www.reddit.com/r/MachineLearning/comments/61x7g0/r_coherent_online_video_style_transfer_msra/,e_walker,1490674380,,2,5
991,2017-3-28,2017,3,28,13,61xabz,What are the project topics based on machine learning that offers good learning curve and is doable by an undergraduate student who has around 1 year to complete the project?,https://www.reddit.com/r/MachineLearning/comments/61xabz/what_are_the_project_topics_based_on_machine/,smeetp18,1490675477,[removed],0,1
992,2017-3-28,2017,3,28,14,61xiia,[Discussion]Video Lectures for Stanford Courses?,https://www.reddit.com/r/MachineLearning/comments/61xiia/discussionvideo_lectures_for_stanford_courses/,code2hell,1490678800,"I have had the privilege to have been educated myself using the amazing video lectures and materials provided by courses like cs231n and cs224d. Every time I feel stuck in a problem I go through the lectures for that topic and get perfect intuition. However the recent sessions are not open as they once were. I know that they were taken down due to caption issues? before, it would be really great if people can share the lectures here if they have them so that everyone can benefit from it. Also can someone from Stanford University please let us know if there is any news regarding making the lectures open. Personally I would really love to check out cs224n now. Thank You! ",6,0
993,2017-3-28,2017,3,28,15,61xnwz,[D] The AI Misinformation Epidemic,https://www.reddit.com/r/MachineLearning/comments/61xnwz/d_the_ai_misinformation_epidemic/,TheCocoanaut,1490681192,,142,176
994,2017-3-28,2017,3,28,15,61xp56,What is the chemical helical blade mixer?,https://www.reddit.com/r/MachineLearning/comments/61xp56/what_is_the_chemical_helical_blade_mixer/,mixmachinery,1490681728,,1,1
995,2017-3-28,2017,3,28,15,61xpur,[Question] What are the most used machine learning algorithms and use-cases at somewhere like Amazon?,https://www.reddit.com/r/MachineLearning/comments/61xpur/question_what_are_the_most_used_machine_learning/,regis_regum,1490682040,[removed],0,1
996,2017-3-28,2017,3,28,15,61xr0d,[R] [1703.08864] Machine Learning on Sequential Data Using a Recurrent Weighted Average (Corrected Results),https://www.reddit.com/r/MachineLearning/comments/61xr0d/r_170308864_machine_learning_on_sequential_data/,[deleted],1490682600,[deleted],1,1
997,2017-3-28,2017,3,28,15,61xud9,[R] Machine Learning on Sequential Data Using a Recurrent Weighted Average &lt;-- Corrected results for new kind of RNN using recurrent attention,https://www.reddit.com/r/MachineLearning/comments/61xud9/r_machine_learning_on_sequential_data_using_a/,jostmey,1490684144,,3,0
998,2017-3-28,2017,3,28,16,61y1c0,[P] Multi-Agent Systems of Control,https://www.reddit.com/r/MachineLearning/comments/61y1c0/p_multiagent_systems_of_control/,inboble,1490687579,,0,0
999,2017-3-28,2017,3,28,17,61y4aa,Keras 2.0 released,https://www.reddit.com/r/MachineLearning/comments/61y4aa/keras_20_released/,lm0n,1490689142,,0,1
1000,2017-3-28,2017,3,28,17,61y5xh,"Got asked ""how would you begin finding better features/parameters for our model"" in an interview, want some advice.",https://www.reddit.com/r/MachineLearning/comments/61y5xh/got_asked_how_would_you_begin_finding_better/,DrCSQuestions,1490690081,[removed],0,1
1001,2017-3-28,2017,3,28,18,61ycgo,What convolutional neural networks look at when they see nudity,https://www.reddit.com/r/MachineLearning/comments/61ycgo/what_convolutional_neural_networks_look_at_when/,grokker89,1490693548,,0,1
1002,2017-3-28,2017,3,28,19,61ygbx,[R] Efficient Processing of Deep Neural Networks: A Tutorial and Survey,https://www.reddit.com/r/MachineLearning/comments/61ygbx/r_efficient_processing_of_deep_neural_networks_a/,mttd,1490695489,,1,11
1003,2017-3-28,2017,3,28,21,61z2ps,Stre Sarma ve Kapatma Makinas,https://www.reddit.com/r/MachineLearning/comments/61z2ps/stre_sarma_ve_kapatma_makinas/,renasmakinatr,1490704687,[removed],0,1
1004,2017-3-28,2017,3,28,22,61z9v6,Data Science Bowl AMA [x-post /r/IAmA],https://www.reddit.com/r/MachineLearning/comments/61z9v6/data_science_bowl_ama_xpost_riama/,TypedInt,1490707063,,0,1
1005,2017-3-28,2017,3,28,22,61zcpc,How can I improve over-smoothed LSTM regression analysis?,https://www.reddit.com/r/MachineLearning/comments/61zcpc/how_can_i_improve_oversmoothed_lstm_regression/,sesenosannko,1490707947,[removed],0,1
1006,2017-3-28,2017,3,28,22,61zhgh,Top Data Science Resources on the Internet right now,https://www.reddit.com/r/MachineLearning/comments/61zhgh/top_data_science_resources_on_the_internet_right/,mlwhiz,1490709436,,0,1
1007,2017-3-28,2017,3,28,23,61ziis,How to get started with AI: A guide for Enterprises,https://www.reddit.com/r/MachineLearning/comments/61ziis/how_to_get_started_with_ai_a_guide_for_enterprises/,TechXLR8,1490709771,[removed],0,1
1008,2017-3-28,2017,3,28,23,61zikp,[R] D.TRUMP: Data-mining Textual Responses to Uncover Misconception Patterns,https://www.reddit.com/r/MachineLearning/comments/61zikp/r_dtrump_datamining_textual_responses_to_uncover/,popcorncolonel,1490709789,,12,13
1009,2017-3-28,2017,3,28,23,61zkq3,[D] Choice of Recognition Models in VAEs: Is a restrictive posterior class a bug or a feature?,https://www.reddit.com/r/MachineLearning/comments/61zkq3/d_choice_of_recognition_models_in_vaes_is_a/,fhuszar,1490710385,,3,26
1010,2017-3-28,2017,3,28,23,61zkz4,How to get started with AI: A guide for Enterprises,https://www.reddit.com/r/MachineLearning/comments/61zkz4/how_to_get_started_with_ai_a_guide_for_enterprises/,[deleted],1490710461,[deleted],0,1
1011,2017-3-28,2017,3,28,23,61zn4e,How to get started with AI: A guide for Enterprises,https://www.reddit.com/r/MachineLearning/comments/61zn4e/how_to_get_started_with_ai_a_guide_for_enterprises/,TechXLR8,1490711070,,1,1
1012,2017-3-29,2017,3,29,0,6200nf,sentence classification with RNN-LSTM - output layer,https://www.reddit.com/r/MachineLearning/comments/6200nf/sentence_classification_with_rnnlstm_output_layer/,redditshagger,1490714870,[removed],0,1
1013,2017-3-29,2017,3,29,1,620aty,[R][1703.09202] Biologically inspired protection of deep networks from adversarial attacks,https://www.reddit.com/r/MachineLearning/comments/620aty/r170309202_biologically_inspired_protection_of/,Mandrathax,1490717492,,27,74
1014,2017-3-29,2017,3,29,1,620di7,Who Said What: Modeling Individual Labelers Improves Classification,https://www.reddit.com/r/MachineLearning/comments/620di7/who_said_what_modeling_individual_labelers/,drsxr,1490718163,,1,1
1015,2017-3-29,2017,3,29,1,620iee,The State of AI by Ilya Sutskever of OpenAI,https://www.reddit.com/r/MachineLearning/comments/620iee/the_state_of_ai_by_ilya_sutskever_of_openai/,[deleted],1490719432,[deleted],1,1
1016,2017-3-29,2017,3,29,2,620oz7,[D] Gain from multi gpu training,https://www.reddit.com/r/MachineLearning/comments/620oz7/d_gain_from_multi_gpu_training/,jacobgil,1490721074,"How much gain in training time should I expect in training a VGG style network on 2, 4 and 8 GPUs in the same machine?

Is there a clear benefit from training on multiple GPUs on the same machine?

Also from what I understand current multi gpu solutions in tensorflow and caffe use data parallelism (the batches are divided between replicated models in GPUs) and not model parallelism (the calculations are spread between GPUs), is this correct?",8,7
1017,2017-3-29,2017,3,29,2,620ufq,"Nathan's AI newsletter: tech, research and startups, Jan thru March 2017",https://www.reddit.com/r/MachineLearning/comments/620ufq/nathans_ai_newsletter_tech_research_and_startups/,nb410,1490722441,,0,1
1018,2017-3-29,2017,3,29,3,621e4n,Introduction to Support Vector Machine(SVM) | Dimensionless,https://www.reddit.com/r/MachineLearning/comments/621e4n/introduction_to_support_vector_machinesvm/,dimensionless_tech,1490727426,,0,1
1019,2017-3-29,2017,3,29,4,621ipc,Can tensorflow programs run natively on a raspberry pi?,https://www.reddit.com/r/MachineLearning/comments/621ipc/can_tensorflow_programs_run_natively_on_a/,chirag2796,1490728587,[removed],0,1
1020,2017-3-29,2017,3,29,4,621ku4,Internships for the summer?,https://www.reddit.com/r/MachineLearning/comments/621ku4/internships_for_the_summer/,[deleted],1490729133,[removed],0,1
1021,2017-3-29,2017,3,29,4,621ls0,A.I. Versus M.D.,https://www.reddit.com/r/MachineLearning/comments/621ls0/ai_versus_md/,[deleted],1490729382,[deleted],1,1
1022,2017-3-29,2017,3,29,6,622b4y,[D] Improvements over vanilla GSD for online deep learning?,https://www.reddit.com/r/MachineLearning/comments/622b4y/d_improvements_over_vanilla_gsd_for_online_deep/,zergylord,1490736082,"Most of the popular advances in stochastic gradient descent (e.g. RMSprop, ADAM, etc) tend to leverage minibatch-level statistics to improve performance. Have any of these methods been tested empirically in the online learning setting? (i.e. minibatches of one) It seems intuitive that historical information should be useful in this setting as well, but perhaps the noise of using individual samples outweighs the possible benefit?",2,3
1023,2017-3-29,2017,3,29,6,622cqd,[R] [1608.04644] Towards Evaluating the Robustness of Neural Networks,https://www.reddit.com/r/MachineLearning/comments/622cqd/r_160804644_towards_evaluating_the_robustness_of/,newguyinml,1490736540,,4,19
1024,2017-3-29,2017,3,29,6,622is8,[1612.03897] Inverse Compositional Spatial Transformer Networks,https://www.reddit.com/r/MachineLearning/comments/622is8/161203897_inverse_compositional_spatial/,Neural_Ned,1490738198,,0,1
1025,2017-3-29,2017,3,29,8,622ygg,"Scaling Caffe with MPI on an 18,000 GPU Supercomputer",https://www.reddit.com/r/MachineLearning/comments/622ygg/scaling_caffe_with_mpi_on_an_18000_gpu/,[deleted],1490742713,[deleted],0,1
1026,2017-3-29,2017,3,29,8,6233o0,[R] Simons Institute Workshop on Representation Learning,https://www.reddit.com/r/MachineLearning/comments/6233o0/r_simons_institute_workshop_on_representation/,rbkillea,1490744297,,2,39
1027,2017-3-29,2017,3,29,9,623h3j,Difference between training conditional adversarial networks and fully-convolutional neural nets.,https://www.reddit.com/r/MachineLearning/comments/623h3j/difference_between_training_conditional/,[deleted],1490748552,[removed],0,1
1028,2017-3-29,2017,3,29,10,623lkc,[D] Evaluating boosted decision trees for billions of users,https://www.reddit.com/r/MachineLearning/comments/623lkc/d_evaluating_boosted_decision_trees_for_billions/,peeyek,1490749987,,3,20
1029,2017-3-29,2017,3,29,10,623oq4,[R] Early Stopping without a Validation Set,https://www.reddit.com/r/MachineLearning/comments/623oq4/r_early_stopping_without_a_validation_set/,xternalz,1490750990,,15,17
1030,2017-3-29,2017,3,29,11,623vl4,[D] Comparison of training conditional adversarial networks and fully-convolutional neural nets.,https://www.reddit.com/r/MachineLearning/comments/623vl4/d_comparison_of_training_conditional_adversarial/,maverikus,1490753270,"GANs have recently been used to generate segmentation maps and aerial photos conditioned on the corresponding color images apart from the opposite task (generating color images from segmentation map/ aerial map). The [pix2pix](https://arxiv.org/pdf/1611.07004.pdf)  paper does this quite effectively. 

While training GANs to generate RGB images from facades/ segmentation maps/ aerial maps makes sense since these are one-to-many tasks (several RGB images may exist with different lighting, weather conditions for the same facade/ segmentation map/ aerial map/ ), what is the purpose of learning one-to-one tasks where the input RGB map is expected to generate only a single correct answer? Wouldn't it be simpler to train a FCN style CNN for the same task?",2,8
1031,2017-3-29,2017,3,29,12,6245fm,Looking for informed views on the Tesla Vision neural network and fleet learning,https://www.reddit.com/r/MachineLearning/comments/6245fm/looking_for_informed_views_on_the_tesla_vision/,trenteady,1490756568,[removed],0,1
1032,2017-3-29,2017,3,29,12,6249r5,"Hey Y'all, We showed that Neural Networks can make chemical intuition quantitative.",https://www.reddit.com/r/MachineLearning/comments/6249r5/hey_yall_we_showed_that_neural_networks_can_make/,johnparkhill,1490758080,,0,1
1033,2017-3-29,2017,3,29,12,624a5d,[N] OpenAI will debut a novel approach to machine learning needed to sustain the momentum of AI research,https://www.reddit.com/r/MachineLearning/comments/624a5d/n_openai_will_debut_a_novel_approach_to_machine/,[deleted],1490758217,[deleted],1,2
1034,2017-3-29,2017,3,29,13,624hxe,Install Scikit-Learn on Python in 1min !!,https://www.reddit.com/r/MachineLearning/comments/624hxe/install_scikitlearn_on_python_in_1min/,[deleted],1490761036,,0,1
1035,2017-3-29,2017,3,29,13,624hyy,5th Neuro Inspired Computational Elements Workshop (NICE 2017) Updated With Presentation Links,https://www.reddit.com/r/MachineLearning/comments/624hyy/5th_neuro_inspired_computational_elements/,neuromorphics,1490761048,,0,1
1036,2017-3-29,2017,3,29,14,624rjr,[D] Explanation of DeepMind's Neural Episodic Control,https://www.reddit.com/r/MachineLearning/comments/624rjr/d_explanation_of_deepminds_neural_episodic_control/,[deleted],1490764868,[deleted],0,1
1037,2017-3-29,2017,3,29,15,624x6a,[D] Paper recommendations relating to time-series prediction,https://www.reddit.com/r/MachineLearning/comments/624x6a/d_paper_recommendations_relating_to_timeseries/,lleewwiiss,1490767331,"Looking to do some reading on time-series prediction (seq2seq LSTM etc) if anyone has a particular paper they enjoyed I'd love to read it. I have read through the material on implementing them but want a more theoretical basis before I start.

More specifically training a model to predict time frames before an event in the time-series occurs.",15,18
1038,2017-3-29,2017,3,29,15,62500p,What Linux Distro do you guys use for DeepLearning ?,https://www.reddit.com/r/MachineLearning/comments/62500p/what_linux_distro_do_you_guys_use_for_deeplearning/,[deleted],1490768643,[removed],0,1
1039,2017-3-29,2017,3,29,15,62504z,[D] Is there a pattern how CNNs learn the representation in layers?,https://www.reddit.com/r/MachineLearning/comments/62504z/d_is_there_a_pattern_how_cnns_learn_the/,nivm321,1490768706,"I have seen from various sources that the first few layers become Gabor-like edge filters, next few become optimised to fire for colour and then more complex layers. 

Is there a pattern for this? Is it always edges in the start and colour later and so on? 

If so, how many layers for edges and so on? Is there a paper which discusses this in length?",2,5
1040,2017-3-29,2017,3,29,15,6250f7,Failures of Deep Learning,https://www.reddit.com/r/MachineLearning/comments/6250f7/failures_of_deep_learning/,[deleted],1490768850,[deleted],0,1
1041,2017-3-29,2017,3,29,15,6250u2,Can Machine Learning help me distinguish Animated video clips from unanimated clips?,https://www.reddit.com/r/MachineLearning/comments/6250u2/can_machine_learning_help_me_distinguish_animated/,monkkutto,1490769056,[removed],0,1
1042,2017-3-29,2017,3,29,15,6251dj,[D] Does the number of paddings matter in batch normalization?,https://www.reddit.com/r/MachineLearning/comments/6251dj/d_does_the_number_of_paddings_matter_in_batch/,longinglove,1490769305,"Unlike images, when we deal with language, we often pad each sentence with 0's, (and we do not update it). When one applies convolution to a sequence of words as in ByteNet, followed by the batch normalization, I think this can be problematic. It's because the mini-batch mean and variance should be affected by the number of paddings, which may vary according to the mini-batch. And the running mean/variance used at the inference phase will be not accurate, as well. How do we handle this?",2,1
1043,2017-3-29,2017,3,29,16,6258ph,Visual explanations for many machine learning related topics,https://www.reddit.com/r/MachineLearning/comments/6258ph/visual_explanations_for_many_machine_learning/,use_your_imagination,1490772811,,0,1
1044,2017-3-29,2017,3,29,16,625bck,Hello Tensor Flow with top alumni from Stanford! Hieu Pham https://www.quora.com/profile/Hieu-Pham-20,https://www.reddit.com/r/MachineLearning/comments/625bck/hello_tensor_flow_with_top_alumni_from_stanford/,itamsvtd,1490774235,[removed],1,1
1045,2017-3-29,2017,3,29,16,625blq,Overview of Machine Learning in Trading,https://www.reddit.com/r/MachineLearning/comments/625blq/overview_of_machine_learning_in_trading/,NitinThapar,1490774378,,0,1
1046,2017-3-29,2017,3,29,17,625ceq,[D] Fluctuations in CNN performance,https://www.reddit.com/r/MachineLearning/comments/625ceq/d_fluctuations_in_cnn_performance/,Rhenesys,1490774810,"I have written a classification CNN and it seems to be performing pretty well, but the classification accuracy and area under ROC-curve, i.e. the performance, fluctuate quite a bit from one training process to another. I would like to find out the most optimal hyper-parameters and these fluctuations mean that I need to repeat each combination a lot to get statistical significance. Is there a way to avoid or reduce these fluctuations? I'm relatively new to machine-learning.",3,2
1047,2017-3-29,2017,3,29,17,625chp,I made a Neural Video !! (see 1080p),https://www.reddit.com/r/MachineLearning/comments/625chp/i_made_a_neural_video_see_1080p/,seominlee,1490774854,,0,1
1048,2017-3-29,2017,3,29,17,625hpo,Efficiency variable-size input in convolutional neural network frameworks,https://www.reddit.com/r/MachineLearning/comments/625hpo/efficiency_variablesize_input_in_convolutional/,[deleted],1490777619,[removed],0,1
1049,2017-3-29,2017,3,29,17,625i3a,Geoff Hinton will serve as the chief scientific adviser of the newly created Vector Institute for Artificial Intelligence in Canada,https://www.reddit.com/r/MachineLearning/comments/625i3a/geoff_hinton_will_serve_as_the_chief_scientific/,[deleted],1490777807,[deleted],0,1
1050,2017-3-29,2017,3,29,18,625imt,What is the memory cost of a CNN?,https://www.reddit.com/r/MachineLearning/comments/625imt/what_is_the_memory_cost_of_a_cnn/,Dawny33,1490778072,,0,1
1051,2017-3-29,2017,3,29,18,625iyx,Efficiency variable-size input in convolutional neural networks,https://www.reddit.com/r/MachineLearning/comments/625iyx/efficiency_variablesize_input_in_convolutional/,[deleted],1490778257,[removed],0,1
1052,2017-3-29,2017,3,29,18,625k5d,Machine Learning: Classification Models,https://www.reddit.com/r/MachineLearning/comments/625k5d/machine_learning_classification_models/,kirfuchs,1490778848,,0,1
1053,2017-3-29,2017,3,29,18,625kjl,[R] Efficiency variable-size input in convolutional neural networks,https://www.reddit.com/r/MachineLearning/comments/625kjl/r_efficiency_variablesize_input_in_convolutional/,jonasteuwen,1490779060,"Most convolutional neural networks assume that the input size is constant, e.g. 224x224x3 which can appear to be an artificial constraint as explained in the paper [Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition](https://arxiv.org/abs/1406.4729) by He et al. (2014).

They implement a 'spatial pooling layer' which gives an output of fixed dimensionally independent of the input size. However, they mention (cited from section 2.3)

&gt; Theoretically, the above network structure can be trained with standard back-propagation [1], regardless of the input image size. But in practice the GPU implementations (such as  cuda-convnet [3] and Caffe [35]) are preferably  run on fixed input images.

In the paper, they train several networks with different, but fixed, input sizes and share weights amongst them. Further on in section 2.3, the authors explain this with:

&gt; The main purpose of our multi-size training is to simulate the varying input sizes while still leveraging the existing well-optimized fixed-size   implementations.

So my question is twofold:
 
 - The paper was written in 2014, does the claim still hold nowadays with the current versions of the frameworks? But more importantly:
 - Which part in the implementation of the convolutional neural networks is more efficient when having fixed size inputs on the GPU?",19,18
1054,2017-3-29,2017,3,29,18,625lzb,3d convolutions computational cost (vs 2d),https://www.reddit.com/r/MachineLearning/comments/625lzb/3d_convolutions_computational_cost_vs_2d/,gpolaert,1490779781,[removed],0,1
1055,2017-3-29,2017,3,29,18,625n1l,"[N] WebVision Challenge: Large-scale ""Web"" Image Classification and Transfer Learning",https://www.reddit.com/r/MachineLearning/comments/625n1l/n_webvision_challenge_largescale_web_image/,xternalz,1490780327,,2,13
1056,2017-3-29,2017,3,29,18,625pcx,Multilabel Classification with mlr,https://www.reddit.com/r/MachineLearning/comments/625pcx/multilabel_classification_with_mlr/,philipppro,1490781427,,0,1
1057,2017-3-29,2017,3,29,19,625s9n,The Great Conundrum of Hyperparameter Optimization,https://www.reddit.com/r/MachineLearning/comments/625s9n/the_great_conundrum_of_hyperparameter_optimization/,teamrework,1490782812,,0,1
1058,2017-3-29,2017,3,29,20,6261sf,Highly modular image captioning with Keras,https://www.reddit.com/r/MachineLearning/comments/6261sf/highly_modular_image_captioning_with_keras/,Nicolas_Weber,1490787092,,0,1
1059,2017-3-29,2017,3,29,20,6265qs,"Need help with TF-IDF / KMeans. Not working as intended, at loss for troubleshooting ideas.",https://www.reddit.com/r/MachineLearning/comments/6265qs/need_help_with_tfidf_kmeans_not_working_as/,killermouse0,1490788551,[removed],0,1
1060,2017-3-29,2017,3,29,21,626enq,Natural language processing - can machines think/talk?,https://www.reddit.com/r/MachineLearning/comments/626enq/natural_language_processing_can_machines_thinktalk/,bosanche,1490791729,,0,2
1061,2017-3-29,2017,3,29,22,626kbe,Machine Learning and Deep Learning Courses and Textbooks,https://www.reddit.com/r/MachineLearning/comments/626kbe/machine_learning_and_deep_learning_courses_and/,Desert_Icecream,1490793644,[removed],0,1
1062,2017-3-29,2017,3,29,22,626p37,[D] Explanation of DeepMind's Neural Episodic Control,https://www.reddit.com/r/MachineLearning/comments/626p37/d_explanation_of_deepminds_neural_episodic_control/,RSchaeffer,1490795109,,10,117
1063,2017-3-29,2017,3,29,23,62700i,temporal planning solver,https://www.reddit.com/r/MachineLearning/comments/62700i/temporal_planning_solver/,iris1234567,1490798236,[removed],0,1
1064,2017-3-29,2017,3,29,23,6271qh,Pass additional state to LSTM besides the sequence?,https://www.reddit.com/r/MachineLearning/comments/6271qh/pass_additional_state_to_lstm_besides_the_sequence/,pemzlsew,1490798745,[removed],0,1
1065,2017-3-30,2017,3,30,0,627gn2,"Simple Questions Thread March 29, 2017",https://www.reddit.com/r/MachineLearning/comments/627gn2/simple_questions_thread_march_29_2017/,AutoModerator,1490802764,[removed],0,1
1066,2017-3-30,2017,3,30,0,627hnj,[D] Deep learning and Neuroscience.,https://www.reddit.com/r/MachineLearning/comments/627hnj/d_deep_learning_and_neuroscience/,nocortex,1490803044,,6,0
1067,2017-3-30,2017,3,30,1,627v36,Common Problems in Hyperparameter Optimization,https://www.reddit.com/r/MachineLearning/comments/627v36/common_problems_in_hyperparameter_optimization/,alexcmu,1490806475,,1,1
1068,2017-3-30,2017,3,30,2,6285u2,[P] Using WEKA for author identification,https://www.reddit.com/r/MachineLearning/comments/6285u2/p_using_weka_for_author_identification/,msinterv,1490809276,"Hi Friends,

I am trying to use WEKA for identification and classification of documents based on author using Naive Bayes. I have a dataset of stories written by sci-fi authors such as Isaac Asimov, Philip K. Dick and Ray Bradbury .


&amp;nbsp;



I have converted these documents into arff file using the following scheme: 


*@relation Authors*


*@attribute textOfStory string*  
*@attribute authorOfStory {""Isaac"",""Philip"",""Ray""}*

(In training and test data - using two or more novels/short stories in each set for every author)


&amp;nbsp;


Afterwards, I converted the textOfStory to attributes using String to Word Vector (IDF transform + TF transform = true). This resulted in unigram word vectors with certain frequencies. For eg. the word attribute ""screen"" was given value of 0.44544, 0.44544, 0 for the three authors.


&amp;nbsp;



Can anyone help me understand how these frequencies are being calculated? As I have read word unigram frequencies should be helpful in identifying and attributing the author. Am I going on the right path, is this the right way to proceed through this problem? How can I compare both the data set using Naive Bayes in an interpretable manner? Any help is appreciated.  

&amp;nbsp;



*Tl;dr: Want to correctly attribute writings of authors using Weka. Help to find the correct procedure for the same?*",4,3
1069,2017-3-30,2017,3,30,2,6287pd,[D] Machine learning practitioners. Has the recent election modified your view on the role of machine learning in job loss?,https://www.reddit.com/r/MachineLearning/comments/6287pd/d_machine_learning_practitioners_has_the_recent/,maxToTheJ,1490809747,"This topic has come up before especially with polls of researchers and practitioners showing this is a concern 

http://www.pewinternet.org/2014/08/06/future-of-jobs/

It seems that based on the recent election that nobody is taking this seriously and focusing on scapegoating instead. This is of special interest considering truck drivers account for nearly 2 million jobs in the US and automated driving is advancing consistently.

So has this changed your viewpoint? 

Personally it seems like we are going to need the consequences to be obvious for the debate on what to do to even take place. ",8,0
1070,2017-3-30,2017,3,30,3,628b1t,Deep Learning with Emojis (not Math),https://www.reddit.com/r/MachineLearning/comments/628b1t/deep_learning_with_emojis_not_math/,[deleted],1490810592,[deleted],0,1
1071,2017-3-30,2017,3,30,3,628hev,My metric is 0.65*accuracy + 0.25*recall + 0.15*precision. What do I optimise for?,https://www.reddit.com/r/MachineLearning/comments/628hev/my_metric_is_065accuracy_025recall_015precision/,[deleted],1490812217,[removed],0,1
1072,2017-3-30,2017,3,30,4,628r8a,Discussion of best activation functions for neural networks with dropout,https://www.reddit.com/r/MachineLearning/comments/628r8a/discussion_of_best_activation_functions_for/,hyperevo,1490814769,[removed],0,1
1073,2017-3-30,2017,3,30,4,628t1d,[R] Deep Learning with Emojis (not Math),https://www.reddit.com/r/MachineLearning/comments/628t1d/r_deep_learning_with_emojis_not_math/,jeremy_stanley,1490815247,,5,47
1074,2017-3-30,2017,3,30,4,628x34,How To: Improve Performance of DeepDream with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/628x34/how_to_improve_performance_of_deepdream_with/,mrubash1,1490816294,,0,1
1075,2017-3-30,2017,3,30,6,629lnz,[R][1703.00862] Binarized Convolutional Landmark Localizers for Human Pose Estimation and Face Alignment with Limited Resources,https://www.reddit.com/r/MachineLearning/comments/629lnz/r170300862_binarized_convolutional_landmark/,[deleted],1490822661,[deleted],1,3
1076,2017-3-30,2017,3,30,7,62a2rf,[D] Machine Learning Article Title Generator,https://www.reddit.com/r/MachineLearning/comments/62a2rf/d_machine_learning_article_title_generator/,freshleycrusher,1490827307,,5,0
1077,2017-3-30,2017,3,30,8,62a8i2,"[R] Scaling the Scattering Transform, Oyallon, Belilovsky, and Zagoruyko (paper with PyTorch code link)",https://www.reddit.com/r/MachineLearning/comments/62a8i2/r_scaling_the_scattering_transform_oyallon/,kkastner,1490829029,,9,29
1078,2017-3-30,2017,3,30,9,62arrx,[R] Who Said What: Modeling Individual Labelers Improves Classification,https://www.reddit.com/r/MachineLearning/comments/62arrx/r_who_said_what_modeling_individual_labelers/,[deleted],1490835038,[deleted],0,1
1079,2017-3-30,2017,3,30,9,62aryi,[R] Google Brain: Who Said What: Modeling Individual Labelers Improves Classification,https://www.reddit.com/r/MachineLearning/comments/62aryi/r_google_brain_who_said_what_modeling_individual/,[deleted],1490835102,[deleted],1,1
1080,2017-3-30,2017,3,30,9,62ase6,[R] [Google Brain] Who Said What: Modeling Individual Labelers Improves Classification,https://www.reddit.com/r/MachineLearning/comments/62ase6/r_google_brain_who_said_what_modeling_individual/,downtownslim,1490835241,,2,19
1081,2017-3-30,2017,3,30,9,62asi0,[R] Flow-Guided Feature Aggregation for Video Object Detection,https://www.reddit.com/r/MachineLearning/comments/62asi0/r_flowguided_feature_aggregation_for_video_object/,flyforlight,1490835281,,2,9
1082,2017-3-30,2017,3,30,10,62atrg,[R] Multi-Scale Dense Convolutional Networks for Efficient Prediction,https://www.reddit.com/r/MachineLearning/comments/62atrg/r_multiscale_dense_convolutional_networks_for/,xternalz,1490835653,,1,14
1083,2017-3-30,2017,3,30,10,62axaa,Rethinking Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/62axaa/rethinking_recurrent_neural_networks/,[deleted],1490836729,[deleted],0,1
1084,2017-3-30,2017,3,30,10,62ayqr,[D] Rethinking Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/62ayqr/d_rethinking_recurrent_neural_networks/,jostmey,1490837175,,8,27
1085,2017-3-30,2017,3,30,10,62b366,[D] What things should I make sure to do during my graduate degree in order to get a job afterwards?,https://www.reddit.com/r/MachineLearning/comments/62b366/d_what_things_should_i_make_sure_to_do_during_my/,[deleted],1490838496,[deleted],1,0
1086,2017-3-30,2017,3,30,11,62b7ki,How about mixing pigment in the water #high speed disperser,https://www.reddit.com/r/MachineLearning/comments/62b7ki/how_about_mixing_pigment_in_the_water_high_speed/,JCT_Janice,1490839822,,0,1
1087,2017-3-30,2017,3,30,11,62b7ro,Classifying White Blood Cells With Deep Learning (Code and data on included!),https://www.reddit.com/r/MachineLearning/comments/62b7ro/classifying_white_blood_cells_with_deep_learning/,[deleted],1490839888,[deleted],0,1
1088,2017-3-30,2017,3,30,11,62b8q3,[1703.07684] Predicting Deeper into the Future of Semantic Segmentation,https://www.reddit.com/r/MachineLearning/comments/62b8q3/170307684_predicting_deeper_into_the_future_of/,[deleted],1490840156,[deleted],0,1
1089,2017-3-30,2017,3,30,11,62bdra,"Audio samples from ""Tacotron: A Fully End-to-End Text-To-Speech Synthesis Model""",https://www.reddit.com/r/MachineLearning/comments/62bdra/audio_samples_from_tacotron_a_fully_endtoend/,sour_losers,1490841687,,0,2
1090,2017-3-30,2017,3,30,12,62bogl,[D] Semantic content search,https://www.reddit.com/r/MachineLearning/comments/62bogl/d_semantic_content_search/,Feribg,1490845228,"I'm trying to come up with a good way of using DL to search for products within pages, however it's not the typical text similarity search that the common full text search approaches implement. Rather I was looking for a way to connect similar object together.

A concrete example would be:
query ""intel i7""
returns {pages that contain intel, or CPU, or AMD etc}

So far the idea I have in mind is to just build a word vector of the search term say with an autoencoder and index the vector representations of each page and use some kind of similarity search, like https://github.com/facebookresearch/faiss to do the actual matching.

The problem is the search vector is going to be tiny compared to what's stored in the database, so it seems like this approach will produce too much noise.

Can you think of better ways to do that or just in general feedback and direction to point me to.

Thanks!",8,5
1091,2017-3-30,2017,3,30,12,62bpwr,Tacotron: A Fully End-to-End Text-To-Speech Synthesis Model [Google],https://www.reddit.com/r/MachineLearning/comments/62bpwr/tacotron_a_fully_endtoend_texttospeech_synthesis/,visarga,1490845731,,2,2
1092,2017-3-30,2017,3,30,12,62bpx2,Hopefully this isn't too silly of a question...but does anyone know how to rebuild torch7?,https://www.reddit.com/r/MachineLearning/comments/62bpx2/hopefully_this_isnt_too_silly_of_a_questionbut/,eddiekee,1490845734,[removed],0,1
1093,2017-3-30,2017,3,30,14,62c1on,"[R] Tacotron: A Fully End-to-End Text-To-Speech Synthesis Model (Wang et. al., Google)",https://www.reddit.com/r/MachineLearning/comments/62c1on/r_tacotron_a_fully_endtoend_texttospeech/,kkastner,1490851143,,46,138
1094,2017-3-30,2017,3,30,14,62c6vo,[D] Classifying White Blood Cells With Deep Learning (Code and data on included),https://www.reddit.com/r/MachineLearning/comments/62c6vo/d_classifying_white_blood_cells_with_deep/,dhruv-partha,1490853520,,14,24
1095,2017-3-30,2017,3,30,15,62c9fo,Robotic Soldering Machine,https://www.reddit.com/r/MachineLearning/comments/62c9fo/robotic_soldering_machine/,JosephSSanders,1490854652,,0,1
1096,2017-3-30,2017,3,30,15,62ccgj,"[Discussion] Pursuing research / grad-school in machine learning, coming from a lacking undergrad program",https://www.reddit.com/r/MachineLearning/comments/62ccgj/discussion_pursuing_research_gradschool_in/,[deleted],1490856043,[deleted],0,1
1097,2017-3-30,2017,3,30,15,62cduu,"[D] Pursuing research / grad-school in machine learning, coming from a lacking undergrad program",https://www.reddit.com/r/MachineLearning/comments/62cduu/d_pursuing_research_gradschool_in_machine/,genesis05,1490856729,"After writing this I'm feeling this may be too off topic.  Let me know and I will move this somewhere else.

**tl;dr My school sucks for ML. I'm torn between gaining research experience in somewhat non-related fields, educating myself in ML through online sources like CS229 and current research, and keeping a high GPA.**

I'm just finishing my third year as a computer science undergrad at a university in Canada.

My school is quite lacking in terms of faculty who are actively doing research in machine learning (and AI in general). Even more so, there is only one undergraduate course related directly to AI, in which the machine learning component of this course consisted of the professor giving us a step-by-step set of instructions for implementing naive bayes without going into any of the theory surrounding it.  I've been denied entry time and time again into graduate machine learning courses despite having a high GPA and a clear interest / working-knowledge of the field.

Over the last year I've worked through Andrew Ng's coursera course, worked on a few small projects surrounding deep learning / neural networks, tried to keep up with current research (mostly via this subreddit), and I'm now currently working through CS229.

I want to pursue grad school after I graduate. While I had the opportunity to go to the University of Toronto, high-school me thought that following friends to this school was a better idea.

Its become very apparent that I'm stuck in an endless cycle of:

* Having to spend the majority of my time on non-related school work so I can keep a good GPA
* Which hinders gaining experience and educating myself in math/stats/machine-learning on my spare time
* Which further hinders seeking out various research opportunities.

What is more concerning is that I need to build relationships with professors at my school so I have some letters of recommendation down the road.  There are a few professors who work with undergrads in their research groups, but their research is orientated towards evolutionary algorithms and social networks.  

I know that any research experience is invaluable at this point.  Though at the same time, I think spending time on these would further hinder my ability to learn and work on projects related to machine-learning, which I feel I am very behind on compared to others coming from other schools that have better programs for this area.  

So the two issues I'm stuck between are that 1) I need research experience and valuable letters of recommendation from professors, but 2) I also need more education, since I'm going to be graduating in a little more than a year with what looks like zero formal education in machine learning.

This might not be worth adding, but I've kind of fallen into the cognitive science crowd at my school since they seem to be the closest thing to any AI I can get at my school.  For a class, I'm working on a cognitive model of zero-shot learning which involves some ML along with a cognitive architecture.  I'm not sure if this has any merit or if I'm wandering around lost at this point.

I greatly value any suggestions, similar experiences, and any help in general in terms of how I should or could approach my predicament.  I'm not in a rush to graduate.  If it comes to it, I might even consider moving to another school -- however I would lose 3-4 semesters worth of credits (time, and money) in the process.

Thank you 
",4,2
1098,2017-3-30,2017,3,30,15,62ce4g,Balance prediction with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/62ce4g/balance_prediction_with_machine_learning/,extreme_bean,1490856867,,0,1
1099,2017-3-30,2017,3,30,16,62cipg,Number of parameters in an RNN,https://www.reddit.com/r/MachineLearning/comments/62cipg/number_of_parameters_in_an_rnn/,gummybearnightmare,1490858965,[removed],0,1
1100,2017-3-30,2017,3,30,16,62cl97,Build Enterprise Bots using #Acuvatebotcore,https://www.reddit.com/r/MachineLearning/comments/62cl97/build_enterprise_bots_using_acuvatebotcore/,Ginashaw12,1490860271,,0,1
1101,2017-3-30,2017,3,30,17,62cmik,[P] PyTorch Implementation of DeepSpeech2,https://www.reddit.com/r/MachineLearning/comments/62cmik/p_pytorch_implementation_of_deepspeech2/,Stormfreek,1490860907,,0,18
1102,2017-3-30,2017,3,30,17,62cr4i,Machine driven Reasoning and Abstraction,https://www.reddit.com/r/MachineLearning/comments/62cr4i/machine_driven_reasoning_and_abstraction/,virene,1490863292,,0,1
1103,2017-3-30,2017,3,30,18,62cvk3,"Support CUDA under mobile NVIDIA GeForce GTX 1050 Ti, 4 GB",https://www.reddit.com/r/MachineLearning/comments/62cvk3/support_cuda_under_mobile_nvidia_geforce_gtx_1050/,_Shyrik_,1490865458,[removed],0,1
1104,2017-3-30,2017,3,30,19,62d3ym,"8-15 tph mobile tractor diesel engine stone crusher, stone crusher machi...",https://www.reddit.com/r/MachineLearning/comments/62d3ym/815_tph_mobile_tractor_diesel_engine_stone/,dymachine01,1490869288,,1,1
1105,2017-3-30,2017,3,30,19,62d84m,State of the art in Conversation Chatbots,https://www.reddit.com/r/MachineLearning/comments/62d84m/state_of_the_art_in_conversation_chatbots/,piykat,1490871188,[removed],0,1
1106,2017-3-30,2017,3,30,20,62db9a,[P] intelligent feedback system for a prosthetic arm,https://www.reddit.com/r/MachineLearning/comments/62db9a/p_intelligent_feedback_system_for_a_prosthetic_arm/,Stripes96,1490872569,"I'm doing a project for university. The overall idea is as follows (whether it's a good idea or not isn't the point, it's more about the machine learning aspect and demonstrating my ability)

embed force sensors in prosthetic hand

feed data from force sensors to machine learning algorithm (micro controller or on a phone)

algorithm receives the readings and then categorises the activity as mode 1 (light work), 2 (medium level), or 3(intense)

mode defines how feedback to user is calculated - e.g. for mode 1 the user is only concerned with forces of say 100N to 1000N, so the feedback range is only between these two. The idea being it gives better resolution.

Another algorithm also implemented so that if a sensor stops working (and thus feeds back a 0) or is badly calibrated (so is way out of expected range compared to other sensors), error can be corrected.

So I have a two pronged problem - classification and regression I believe. I am attempting to compare three algorithms for the classification and just provide a solution for the error correction.

I'm thinking decision tree, SVM and (third?) for classification, and not sure on error correction yet. Any pointers/thoughts would be very much appreciated!

EDIT: report is 30 pages in length, and yes I will have access to data - I have the opportunity to use a force mapping glove next week and will gather data from 3 different activities for each mode. Other design goals include turning off the feedback if there is little change after a certain time - e.g. user doesn't want feedback if simply resting their hand on an armchair...",8,1
1107,2017-3-30,2017,3,30,20,62debu,Pardon me but was there either a paper or documented method about repairing corrupted pdfs recently? I can't find a thing but certain it exists somewhere,https://www.reddit.com/r/MachineLearning/comments/62debu/pardon_me_but_was_there_either_a_paper_or/,zimocracy,1490873774,[removed],0,1
1108,2017-3-30,2017,3,30,20,62dg2y,I made a video chronicling my efforts to learn Machine Learning and get a job,https://www.reddit.com/r/MachineLearning/comments/62dg2y/i_made_a_video_chronicling_my_efforts_to_learn/,[deleted],1490874403,[deleted],0,1
1109,2017-3-30,2017,3,30,21,62dis5,"[R] Evaluating Neural Network Representations Against Human Cognition (Tom Griffiths, UC Berkeley)",https://www.reddit.com/r/MachineLearning/comments/62dis5/r_evaluating_neural_network_representations/,downtownslim,1490875437,,1,31
1110,2017-3-30,2017,3,30,22,62dud6,[R] Schmidhuber: History of computer vision contests won by deep CNNs on GPU,https://www.reddit.com/r/MachineLearning/comments/62dud6/r_schmidhuber_history_of_computer_vision_contests/,downtownslim,1490879499,,8,8
1111,2017-3-30,2017,3,30,22,62e0mo,[R] Any links that contained rotated-MNIST dataset images? Note: Not looking for .mat files only images.,https://www.reddit.com/r/MachineLearning/comments/62e0mo/r_any_links_that_contained_rotatedmnist_dataset/,mnist2rotation,1490881449,[removed],6,0
1112,2017-3-30,2017,3,30,23,62e59j,[Research] Evolution Strategies: A Review and a Few Possible Extensions (inference blog),https://www.reddit.com/r/MachineLearning/comments/62e59j/research_evolution_strategies_a_review_and_a_few/,fhuszar,1490882792,,29,61
1113,2017-3-30,2017,3,30,23,62e9ip,[D] Is Machine Learning Growing at an Exponential Rate?,https://www.reddit.com/r/MachineLearning/comments/62e9ip/d_is_machine_learning_growing_at_an_exponential/,[deleted],1490884008,[removed],0,1
1114,2017-3-31,2017,3,31,0,62elk8,[D] Good datasets/problems to test new RNN architectures?,https://www.reddit.com/r/MachineLearning/comments/62elk8/d_good_datasetsproblems_to_test_new_rnn/,bubaonaruba,1490887300,"Hi,

I've recently developed a few new RNN architectures that seem to significantly beat GRU/LSTM on a few toy problems:

- (permuted or not) sequential mnist
- copy task
- addition task
- char-rnn

but I'm wondering if there are some larger, more 'real world' datasets available that would be natively suitable for RNNs?

Language models are not a perfect fit, because although they use the rnn framework, many different components come into play and the underlying RNN cell often is of minor importance.

Something with long term dependencies, some noise and possibly multidimensional, anyone? Sequential permuted ImageNet doesn't count ;-)

Many thanks!",17,19
1115,2017-3-31,2017,3,31,0,62eqfq,Has there been a python source code for accepted CVPR/ICCV papers?,https://www.reddit.com/r/MachineLearning/comments/62eqfq/has_there_been_a_python_source_code_for_accepted/,aznroscatkin,1490888617,[removed],0,1
1116,2017-3-31,2017,3,31,1,62eunm,May lam mat dk 5000A,https://www.reddit.com/r/MachineLearning/comments/62eunm/may_lam_mat_dk_5000a/,dailuuthong,1490889736,,0,1
1117,2017-3-31,2017,3,31,3,62fo7r,Thinking Machines: The Quest for Artificial Intelligenceand Where Its Taking Us Next by Luke Dormehl,https://www.reddit.com/r/MachineLearning/comments/62fo7r/thinking_machines_the_quest_for_artificial/,Mussem17,1490897203,,0,1
1118,2017-3-31,2017,3,31,3,62fonw,A Deep Compositional Framework for Human-like Language Acquisition in Virtual Environment,https://www.reddit.com/r/MachineLearning/comments/62fonw/a_deep_compositional_framework_for_humanlike/,bayjingsf,1490897312,,1,4
1119,2017-3-31,2017,3,31,3,62fqfp,How to handle black and white images in a colored dataset?,https://www.reddit.com/r/MachineLearning/comments/62fqfp/how_to_handle_black_and_white_images_in_a_colored/,[deleted],1490897751,[removed],0,1
1120,2017-3-31,2017,3,31,3,62ftrs,[D] Handling black and white images in a colored image dataset,https://www.reddit.com/r/MachineLearning/comments/62ftrs/d_handling_black_and_white_images_in_a_colored/,cromulen7,1490898604,"I have a relatively small dataset of about 10k color images. I want to train a neural network on those images for classification. However, a few of the images (&lt; 10) have only a single channel.

How do I handle black and white images in my dataset? 
Should I copy the bw channel three times just so they fit in my input pipeline? 
or
Do I treat them as outliers and remove from the dataset?

Does anyone have experience with this kind of problem? 
Are there any papers that analyse the impact of mixing color and black and white images in the dataset? 
How does a model trained on colored images perform on black and white test set?",7,2
1121,2017-3-31,2017,3,31,4,62g3xw,[N] Announcing AudioSet: A Dataset for Audio Event Research,https://www.reddit.com/r/MachineLearning/comments/62g3xw/n_announcing_audioset_a_dataset_for_audio_event/,rustyryan,1490901203,,38,147
1122,2017-3-31,2017,3,31,4,62gbqn,[D] Best (and most fair) baseline alternatives to deep features?,https://www.reddit.com/r/MachineLearning/comments/62gbqn/d_best_and_most_fair_baseline_alternatives_to/,anonDogeLover,1490903256,"Are fisher vectors still the best alternatives? I'm assuming HOG and SIFT are still somewhat competitive. Any other suggestions?

Also, any links to off-the-shelf extractors that I could use to compare to off-the-shelf models like VGG16 etc? 

Lastly, for HOG or SIFT. Is it appropriate to just compare raw features, or should they be tuned to large datasets to be more competitive?",4,3
1123,2017-3-31,2017,3,31,4,62gddj,Lasso outperforming MLP - why?,https://www.reddit.com/r/MachineLearning/comments/62gddj/lasso_outperforming_mlp_why/,tovefrakommunen,1490903706,[removed],0,1
1124,2017-3-31,2017,3,31,5,62gn6v,[D] A.I. Versus M.D.,https://www.reddit.com/r/MachineLearning/comments/62gn6v/d_ai_versus_md/,tabacof,1490906344,,2,0
1125,2017-3-31,2017,3,31,6,62gxhc,[D] What methods or resources exist to identify country of origin / original languages spoken from a text corpus?,https://www.reddit.com/r/MachineLearning/comments/62gxhc/d_what_methods_or_resources_exist_to_identify/,jaybestnz,1490909078,"I am looking to see from syntax structure, if nationality where English is not the first language.",5,0
1126,2017-3-31,2017,3,31,7,62h79m,[D] Universities with strong machine learning groups in the UK,https://www.reddit.com/r/MachineLearning/comments/62h79m/d_universities_with_strong_machine_learning/,Wideem,1490911805,"I am applying for a masters degree in machine learning in the UK. What are the best universities for that? I already applied to Edinburgh but I am not sure what other universities have strong machine learning groups. Is Cambridge MPhil in Machine Learning, Speech and Language Technology a good masters course or is it too specific ?http://www.graduate.study.cam.ac.uk/courses/directory/egegmpmsl",3,0
1127,2017-3-31,2017,3,31,9,62httz,"The healthcare sector is already using cognitive and AI solutions for big data analytics and for clinical applications. Now, the industry needs to responsibly protect its patients and their data by adopting algorithmic defense solutions, asserts ICITs white paper.",https://www.reddit.com/r/MachineLearning/comments/62httz/the_healthcare_sector_is_already_using_cognitive/,Ubaid_aid,1490918555,,0,1
1128,2017-3-31,2017,3,31,9,62hzqc,[R][1703.10593] Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/62hzqc/r170310593_unpaired_imagetoimage_translation/,ajmooch,1490920371,,5,21
1129,2017-3-31,2017,3,31,9,62i2gw,[D] Clarification on LSTM parameters in Keras,https://www.reddit.com/r/MachineLearning/comments/62i2gw/d_clarification_on_lstm_parameters_in_keras/,NiraSherwood58,1490921241,"Hi, 
I'm not a newbie in machine learning, but i just started doing some RNNs and LSTMs analysis on time series data. The explanation of the LSTM parameters in Keras did not help me, and I spent couple of hours to search for a good explanation with example! on the internet, but i failed to find. So here is what is not clear to me.

 In the documentation(https://keras.io/layers/recurrent/)
There are three parameters: **input shape, input_dim, input_length**. 

1) Consider that I have multiple sensor data, for example I have 400 cases(say patients) that I have a time serie recording of each person for the length of 650 and the problem is binary classification of healthy and patient, what would be those **bold** variables in this context? 

2) About the input shape to the LSTM it is said in the link that:

*Input Shapes:
3D tensor with shape (batch_size, timesteps, input_dim), (Optional) 2D tensors with shape (batch_size, output_dim).*

If I use an LSTM with 100 units, what is the **timestep**? what is the **output_dim**?

3) is it possible to train an LSTM on multiple input rather than one time series at a time? So now each of 400 patients have 32 time series recording of the length 650 each, still the problem is binary classification of healthy and patient.

I hope others who are new to RNN/LSTM find this post since it is very confusing at first. ",9,5
1130,2017-3-31,2017,3,31,10,62i5ls,"cement brick making machine price in india, fly ash brick making machin...",https://www.reddit.com/r/MachineLearning/comments/62i5ls/cement_brick_making_machine_price_in_india_fly/,dymachine01,1490922223,,1,1
1131,2017-3-31,2017,3,31,10,62ieef,[1703.10449] A Neural Networks Approach to Predicting How Things Might Have Turned Out Had I Mustered the Nerve to Ask Barry Cottonfield to the Junior Prom Back in 1997,https://www.reddit.com/r/MachineLearning/comments/62ieef/170310449_a_neural_networks_approach_to/,chogall,1490925040,,2,3
1132,2017-3-31,2017,3,31,10,62iegp,[P] TensorFlow Implementation of DiscoGAN,https://www.reddit.com/r/MachineLearning/comments/62iegp/p_tensorflow_implementation_of_discogan/,ColdSauce,1490925058,,0,33
1133,2017-3-31,2017,3,31,11,62igvk,Personalized Aesthetics: Recording the Visual Mind using Machine Learning,https://www.reddit.com/r/MachineLearning/comments/62igvk/personalized_aesthetics_recording_the_visual_mind/,harrism,1490925835,,0,1
1134,2017-3-31,2017,3,31,12,62ius6,Help me understand deep coder please,https://www.reddit.com/r/MachineLearning/comments/62ius6/help_me_understand_deep_coder_please/,adeeplearner,1490930570,[removed],0,1
1135,2017-3-31,2017,3,31,12,62j0am,[R] Yann LeCun: Unsupervised Representation Learning,https://www.reddit.com/r/MachineLearning/comments/62j0am/r_yann_lecun_unsupervised_representation_learning/,downtownslim,1490932649,,0,34
1136,2017-3-31,2017,3,31,13,62j98m,[1703.10295] DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling,https://www.reddit.com/r/MachineLearning/comments/62j98m/170310295_denet_scalable_realtime_object/,JudasAdventus,1490936182,,0,1
1137,2017-3-31,2017,3,31,14,62jeyw,The Shrewd AI Strategy behind Google's Kaggle Acquisition,https://www.reddit.com/r/MachineLearning/comments/62jeyw/the_shrewd_ai_strategy_behind_googles_kaggle/,mks_repi,1490938591,,0,1
1138,2017-3-31,2017,3,31,16,62jvo2,CycleGAN- image-to-image translation without input-output pairs. From the people who brought you pix2pix,https://www.reddit.com/r/MachineLearning/comments/62jvo2/cyclegan_imagetoimage_translation_without/,onenuthin,1490946477,,0,2
1139,2017-3-31,2017,3,31,16,62jwyo,Generating sequences with VAE: sampling gives incoherent results,https://www.reddit.com/r/MachineLearning/comments/62jwyo/generating_sequences_with_vae_sampling_gives/,sbaur_pasteur,1490947129,[removed],2,1
1140,2017-3-31,2017,3,31,17,62jzcw,[N] Canada's AI moment: Launch of The Vector Institute and Google Brain Toronto,https://www.reddit.com/r/MachineLearning/comments/62jzcw/n_canadas_ai_moment_launch_of_the_vector/,clbam8,1490948392,,25,132
1141,2017-3-31,2017,3,31,17,62k2fu,Machine Learning at ASOS: Predicting CLTV in E-Commerce,https://www.reddit.com/r/MachineLearning/comments/62k2fu/machine_learning_at_asos_predicting_cltv_in/,reworksophie,1490949967,,0,1
1142,2017-3-31,2017,3,31,18,62k4z6,Which AMAs would you like to see in 2017?,https://www.reddit.com/r/MachineLearning/comments/62k4z6/which_amas_would_you_like_to_see_in_2017/,[deleted],1490951311,[removed],0,1
1143,2017-3-31,2017,3,31,19,62kids,The Future of Customer Service is AI: Adapt or Perish,https://www.reddit.com/r/MachineLearning/comments/62kids/the_future_of_customer_service_is_ai_adapt_or/,Gamooga,1490957987,,0,1
1144,2017-3-31,2017,3,31,20,62kn8e,"[P] Video about Computer Vision, Machine Learning, and Getting a Job",https://www.reddit.com/r/MachineLearning/comments/62kn8e/p_video_about_computer_vision_machine_learning/,esotericGames,1490960014,,26,161
1145,2017-3-31,2017,3,31,20,62kndc,"Semantic Search  what is it, what are the benefits and whats the future?",https://www.reddit.com/r/MachineLearning/comments/62kndc/semantic_search_what_is_it_what_are_the_benefits/,grumpybusinesscat,1490960062,,0,1
1146,2017-3-31,2017,3,31,20,62kpu5,Some New Interesting Deep Learning Datasets for Data Scientists,https://www.reddit.com/r/MachineLearning/comments/62kpu5/some_new_interesting_deep_learning_datasets_for/,gargisharmapd,1490961002,,0,1
1147,2017-3-31,2017,3,31,21,62kxjn,Distributed Deep Learning At Scale On Apache Spark With BigDL,https://www.reddit.com/r/MachineLearning/comments/62kxjn/distributed_deep_learning_at_scale_on_apache/,steccami,1490963905,,0,1
1148,2017-3-31,2017,3,31,21,62l0ts,[D] Clustering data with missing values.,https://www.reddit.com/r/MachineLearning/comments/62l0ts/d_clustering_data_with_missing_values/,Byrth,1490965063,"I'm trying to cluster a set of PCR results (~40 transcripts for ~500 samples) to get similar groups of samples to fall out together.

For those who haven't done PCR, in PCR you exponentially amplify your signal (one specific starting RNA) and measure the brightness of a dye that fluoresces when it binds RNA. You plot the brightness of this dye (~amount of RNA) against the number of cycles you have run (each cycle ~doubles your RNA) and the result is an S-shaped curve (exponential increase followed by your sample running out of material to make more RNA.) You take that point where the slope stops increasing (growth stops being exponential) and that's your measurement.

The problem is that every sample doesn't necessarily have every RNA that you're testing for. Some of them will never exponentially grow, and thus generate no value. So when you run PCR for many transcripts on a bunch of samples, as I did, you end up with a mix of categorical (value or no value) and logarithmic ( (0,40] cycles, for me) data.


So far my solution has been to replace ""No Value"" entries with the limit of quantification for that transcript/sample combination and run UPGMA clustering (using the euclidean distance similarity metric) on the resulting data. My defense of this is that I know that if the transcript exists, it's below my limit of quantification, and our method is accurate enough that the limit of quantification is very very small.

My problem is how sensitive the clustering algorithm is to small changes in the way I handle this ""No Value"" data.

Is there a better way to do this?

Thanks!",12,6
1149,2017-3-31,2017,3,31,22,62l6ni,How to use vectors as features in scikit learn?,https://www.reddit.com/r/MachineLearning/comments/62l6ni/how_to_use_vectors_as_features_in_scikit_learn/,ralahui847,1490966982,[removed],0,1
1150,2017-3-31,2017,3,31,22,62l7ur,[D] binary_crossentropy as reconstruction loss in keras vae example for a non bernoulli MLP decoder,https://www.reddit.com/r/MachineLearning/comments/62l7ur/d_binary_crossentropy_as_reconstruction_loss_in/,dranax,1490967365,"**Ressources:** 

Kingma &amp; Welling 2013: https://arxiv.org/abs/1312.6114

Keras code : https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder.py

Data: Mnist, discrete with 256 levels of grey

previous somehow related questions : [vae loss](https://www.reddit.com/r/MachineLearning/comments/5qm6ag/d_how_to_calculate_variational_autoencoder_log/) 
[vae loss again](https://www.reddit.com/r/MachineLearning/comments/5u64uu/d_vae_optimization_objective/)

**Clarification**
I'm puzzled by the choice of keras to use the binarycrossentropy  function (l45) between x (the sample) and xdecodedmean (the output of the decoder network, sigmoid activation) to compute E_{z ~ Q(Z | X)} [log p(x|z)], or ""reconstruction loss"". It is related to the previous questions in the sense that I don't know how to derive log p(x|z) in this non-bernoulli / non-gaussian case.

Kingma and Welling detailed that the binarycrossentropy could be used (appendix  C.1; p11, eq 11) if the decoder was a bernoulli MLP. However, a data point  x lies in [0,1]^784 (784 being the dim of one image from mnist), and is not binary even though it takes discrete values.


Questions:

* **Q1**: How is the binarycrossentropy the appropriate (in terms of ""the one you get when deriving the variational lower bound"") reconstruction loss? 

* **Q2**: How important is the choice of the reconstruction loss in practice? By this I mean that you could rerun the code with the mse instead of the binarycrossentropy and find similar visual result, although I'm not sure how the mse between the sample and the reconstructed sample has any link with E_{z ~ Q(Z | X)} [log p(x|z)].

Any help appreciated.",2,5
1151,2017-3-31,2017,3,31,23,62lljm,interview for machine learning position,https://www.reddit.com/r/MachineLearning/comments/62lljm/interview_for_machine_learning_position/,roger_genbot,1490971554,[removed],0,1
1152,2017-3-31,2017,3,31,23,62loed,[D] Paper recommendations on invariant object recognition and possibly for robotic applications (e.g. grasping),https://www.reddit.com/r/MachineLearning/comments/62loed/d_paper_recommendations_on_invariant_object/,nocortex,1490972377,,0,0
