,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2012-4-2,2012,4,2,2,ro4s8,x/post. Will this influence the design of Neural networks?,https://www.reddit.com/r/MachineLearning/comments/ro4s8/xpost_will_this_influence_the_design_of_neural/,strategosInfinitum,1333299623,,4,6
1,2012-4-2,2012,4,2,14,rp4a5,How to write a news search engine for the local language,https://www.reddit.com/r/MachineLearning/comments/rp4a5/how_to_write_a_news_search_engine_for_the_local/,jestinjoy,1333343556,"I am trying to write a news search engine for my local language. The problem is that different sites use different fonts.

I am just a beginner on this and I know how to use python. I am looking for some suggestions as to how should I proceed to write a search engine that lists related news for a given query.
",8,8
2,2012-4-2,2012,4,2,23,rpi10,Google's keynote on doing machine learning using quantum computing. Non-convex NP-hard optimization problems are solved within hunderds of milliseconds. The lecture starts at 30:28.,https://www.reddit.com/r/MachineLearning/comments/rpi10/googles_keynote_on_doing_machine_learning_using/,aboo0ood,1333375287,,4,40
3,2012-4-3,2012,4,3,10,rqhop,Representative ensemble learning methods? ,https://www.reddit.com/r/MachineLearning/comments/rqhop/representative_ensemble_learning_methods/,Yonah27,1333417326,"I am now collecting some materials for a PPT about ensemble learning. However, I am not sure which ensemble learning methods should be put into the slides. Any suggestions?",0,2
4,2012-4-3,2012,4,3,16,rqw4j,Optimization Algorithms in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/rqw4j/optimization_algorithms_in_machine_learning/,cavedave,1333437184,,1,12
5,2012-4-3,2012,4,3,19,rr05b,Altaeros Energies Floating Wind Turbines Tap Into Strong High Altitude Winds ,https://www.reddit.com/r/MachineLearning/comments/rr05b/altaeros_energies_floating_wind_turbines_tap_into/,tumpol909,1333449842,,0,1
6,2012-4-3,2012,4,3,22,rr5e8,How can I get started in Predictive Analytics?,https://www.reddit.com/r/MachineLearning/comments/rr5e8/how_can_i_get_started_in_predictive_analytics/,yougottawanna,1333460620,"I have a BS in Finance from about 15 years ago but have mainly used it for basic accounting, modeling and financial reporting roles.  I am interested in learning how to build predictive models related to customer behavior, but my last math class was an entry level calculus course in college.

I have a basic understanding of statistics (as in, took a 101 level stats course and have read the wikipedia articles about regression analysis.)  I would say my algebra &amp; trig are extremely rusty and would probably need to be learned from scratch, and my calc is non-existent.

So, what math do I need to really excel in this field?  Mainly stats?  Do I need to relearn the algebra and trig before I proceed to higher level math?  What else do I need to know?

I have 10 hours / week to dedicate to self-study and I really want to learn.

Help!

**Edit: Apologies if I've ruffled some feathers by not looking hard enough for the FAQ or the many weekly identical posts asking for information.**",11,6
7,2012-4-3,2012,4,3,23,rr8g0,Model Selection vs. Parameter Estimation vs. Optimization,https://www.reddit.com/r/MachineLearning/comments/rr8g0/model_selection_vs_parameter_estimation_vs/,nickponline,1333464886,"In the context of machine learning what is the difference between the terms models selection, parameter estimation and optimization. I don't think the terms are interchangeable, especially in machine learning. ",8,14
8,2012-4-4,2012,4,4,0,rr9h9,Computing solutions for crunching large datasets?,https://www.reddit.com/r/MachineLearning/comments/rr9h9/computing_solutions_for_crunching_large_datasets/,[deleted],1333466292,"Hey all, I'm starting to work on an indpendent project or two with some engineering and economist friends of mine, and we're realizing that we don't really have the computing power at our disposal to make the most of really large datasets (ie, we own three laptops between us). 

Do you guys have any useful (and hopefully economical solutions) for us? What do you guys use? Dedicated computer(s)? Fancy pants cloud solution? Any help is appreciated.",7,5
9,2012-4-4,2012,4,4,0,rr9q6,AskML: How much confidence can one have in nested cross-validation results?,https://www.reddit.com/r/MachineLearning/comments/rr9q6/askml_how_much_confidence_can_one_have_in_nested/,hbweb500,1333466602,"I posted this on the Kernel Machines forum, but I thought I would try my luck here, too:

In short, I am wondering how much confidence to have in the results of nested cross-validation after noticing some weird results.

I have a small amount of data (~115 examples), so I am using nested, leave-one-out cross-validation to come up with an estimate of accuracy and to do parameter selection for the RBF kernel. 

Now, I have developed a set of 128 different ways to generate feature vectors, and I'd like to pick the one that will give me the best performance. To do so, I test each of the 128 different methods using the nested cross-validation approach.

The results are a bit puzzling. The majority of the feature generators produce accuracy that is around chance, or reasonably close to it. My best results have accuracy of around 70%, which is pretty good for the data being classified. A typical confusion matrix for such cases looks like:

     ( 36 18 )
     ( 14 47 )

where the (i,j) entry is the number of examples that are actually class i but were classified as class j.

This is all well and good. My problem is that there are some choices of features which produce accuracy near 20%. For example, this is a confusion matrix from one of these cases:

    (  1 53 )
    ( 42 19 )


This poses a problem, because I could just use this choice of features and flip the classification and get ~83% accuracy!

So this worries me. Is it just that the classifier is performing at chance, and that I have so few examples that performing at 20% is just bad luck? This would also mean that my ""good"" results of ~70% accuracy may also be due to luck. Or is there another explanation, perhaps, for obtaining a confusion matrix that is almost entirely off-diagonal?

Also: is there a better measure than accuracy to judge the performance of the classifier? I am currently using the parameters which give the best accuracy in classifying the training set in cross-validation, but it occurs to me that there may be other, better measures (ROC curve, etc...). 

Thanks!
",8,8
10,2012-4-4,2012,4,4,0,rr9xq,"Operations Research, Machine Learning, and Optimization",https://www.reddit.com/r/MachineLearning/comments/rr9xq/operations_research_machine_learning_and/,cavedave,1333466852,,6,17
11,2012-4-5,2012,4,5,7,rtmj9,High False Negative Rate in Binary Text Classification,https://www.reddit.com/r/MachineLearning/comments/rtmj9/high_false_negative_rate_in_binary_text/,LADataJunkie,1333577255,"I am looking for some advice on how to accomplish a lower false negative rate in a binary classifier. I was taught to always start with Naive Bayes (if it has any hope of being decent) and then move on from there so that's why I have not tried anything more complex like LDA or sLDA etc.

I have a bunch of web pages (3500) to train a classifier for determining safe for work content vs NSFW. I have parsed them into text/unigrams, removed punctuation, stop words, words that only appear once etc. and initially used all of unigrams (37500)  as features in a Naive Bayes model. About 65% of the examples are safe for work (negative) and 35% are not safe for work (positive). Each data point is 0 or 1: 1 if the feature appears in the document, 0 else. Then I use the chi-squared feature extraction method to rank features in order of usefulness. Using 5, ..., 2000 features in steps of 5, I used 10-fold cross validation on Naive Bayes to attempt to find the best number of features to use. Unfortunately, the lowest false negative rate I can achieve is 40%, whereas the false positive rate is only 10%.

In this case, a false negative is much much worse as I am classifying pages as safe for work or NSFW. I also tried upweighting the NSFW examples so that based on weights, 50% of the data is safe and 50% is not safe but this did not help.

What are some techniques I can use to get a lower false negative rate? Am I wrong to be starting with Naive Bayes and should I try some other method instead?",13,10
12,2012-4-5,2012,4,5,15,rubew,Machine Learning with Python,https://www.reddit.com/r/MachineLearning/comments/rubew/machine_learning_with_python/,Faleira,1333609019,"(xpost from [/r/learnprogramming](/r/learnprogramming) because I hadn't known this subreddit existed)

Hi, so I'm trying to learn how to use Orange with python to do data comparisons, and machine learning to make educated predictions and would be grateful if i could be pointed into the right direction for what I wanted to do. 

So to begin with, i'm trying to compare 2 sets of data, and see if one affects the other. For example, if i had the following data sets:

A = [1,2,3,2,1] 

B = [2,5,8,1,3] 

C = [10,14,16,14,10]

Is there some kind of analysis that could be done, where Orange could figure out that A and C are similar, as they increase and decrease at the same points? I'm hoping to be able to identify that C is closer to A than B in that regard, and then, say if C had another value given, be able to predict what the new value in A was.

I'm currently still following the tutorials found on the Orange site , but, I feel like i still may not know how to approach my problem at the end.
If anyone knows how to go about this, do you think you could point out some functions or classes I should be looking into?

Would it be better to do this with something other than Orange? such as PyBrain or PyML?",13,20
13,2012-4-6,2012,4,6,3,ruyvm,What are some metrics I can use to measure the similarity/distance between a pair of variables - each variable consisting of two different types of data(i.e. each variable has spatial and temporal data)?,https://www.reddit.com/r/MachineLearning/comments/ruyvm/what_are_some_metrics_i_can_use_to_measure_the/,lpiloto,1333649644,"EDIT: Just to clarify, the basic solution I can come up with is to do a comparison between each time series of the two different items and a comparison between the spatial values of the same items and measure the similarity between the two items as a some average of the two similarity results.  However, I was hoping to see if there was anything more sophisticated, for instance, I think mutual correlation coefficient might be applicable.

Edit: Added much more specificity.

I have a list of N components, X_1 ... X_n, for each of my K subjects, S_1...S_k. Each X component has two features: feature1 = a time series consisting of ~200 points of data. feature2 = a set of continuous variables.
Given this setup, I would like to compare two components, X_a and X_b, to each other by comparing X_a.feature1 against X_b.feature1 and X_a.feature2 against X_b.feature2. I would like to use these two comparisons to produce a single value measuring the similarity between component X_a and component X_b.
The goal of analyzing similarity between components is to say that out of all components from subject S_m, component X_x best corresponds to component X_y from subject S_n.
I hope these details help.",4,1
14,2012-4-6,2012,4,6,11,rvnvx,What do you guys think of Northwestn's Master of Science in Predictive Analytics?,https://www.reddit.com/r/MachineLearning/comments/rvnvx/what_do_you_guys_think_of_northwestns_master_of/,lksdford,1333678882,"I am currently enrolled in [Data Mining Certificate - UC San Diego Extension](http://extension.ucsd.edu/programs/index.cfm?vaction=certdetail&amp;vcertificateid=128&amp;vstudyareaid=14) and want to get more out of this subject than a certificate. I came across Northwestern's [Master of Science in Predictive Analytics](http://www.scs.northwestern.edu/grad/mspa/) and think I would enjoy it. Anyone enrolled in it or know anything about it?

*Just noticed I misspelled Northwestern",14,12
15,2012-4-6,2012,4,6,12,rvs55,X-post from r/statistics: How to rank products based on user input,https://www.reddit.com/r/MachineLearning/comments/rvs55/xpost_from_rstatistics_how_to_rank_products_based/,[deleted],1333684251,,0,1
16,2012-4-6,2012,4,6,19,rw3ds,"Compete in the Data Science Hackathon, April 28
",https://www.reddit.com/r/MachineLearning/comments/rw3ds/compete_in_the_data_science_hackathon_april_28/,talgalili,1333706649,,0,1
17,2012-4-7,2012,4,7,0,rwe8f,More Precious than Gold?,https://www.reddit.com/r/MachineLearning/comments/rwe8f/more_precious_than_gold/,cavedave,1333727349,,0,6
18,2012-4-7,2012,4,7,9,rx61h,impact driver,https://www.reddit.com/r/MachineLearning/comments/rx61h/impact_driver/,uniqueaakash13,1333758947,,1,1
19,2012-4-7,2012,4,7,14,rxjdt,Homework advice - Clustering w/o K-Means?,https://www.reddit.com/r/MachineLearning/comments/rxjdt/homework_advice_clustering_wo_kmeans/,groundshop,1333777702,"Hello all,
I'm working on a data mining project that ends with me clustering bag-of-words type data. 

The majority of the project so far has been pre-processing (the data is an awesome web-crawled data set of tweets from middle eastern countries during the arab spring!). I have a dictionary made of word counts, so I can assign some sort of weight to each word. 

I'm getting to the point now where I need to actually cluster the data. The vectors are very sparse (each feature is a word :/ Maybe I should try something else for this? Kernel method to map it onto some subspace??) After alllll the work I've done preprocessing rough, incomplete, arabic/french/english mixtures of tweets I feel like I've got to find SOME algorithm that's more complicated than the k-means that the professor spoon fed us. 

Any thoughts? If anyone knows of an algorithm that's particularly good on sparse data, I will upvote you and your family.",50,19
20,2012-4-9,2012,4,9,1,rzcfh,"Ask ML: While training sparse autoencoders, how do you decide values for the sparsity parameter and it's associated weight used in the cost-function?",https://www.reddit.com/r/MachineLearning/comments/rzcfh/ask_ml_while_training_sparse_autoencoders_how_do/,strayadvice,1333901997,,33,14
21,2012-4-9,2012,4,9,3,rzhgj,An R programmer looks at Julia,https://www.reddit.com/r/MachineLearning/comments/rzhgj/an_r_programmer_looks_at_julia/,talgalili,1333908815,,0,1
22,2012-4-9,2012,4,9,23,s0rx5,All TIME Magazine covers (March 1923 to March 2012). X-post from r/datasets,https://www.reddit.com/r/MachineLearning/comments/s0rx5/all_time_magazine_covers_march_1923_to_march_2012/,rightname,1333980080,"I found this in [/r/datasets](/r/datasets) posted by ""trexmatt""
[Post Link](http://www.reddit.com/r/datasets/comments/s0fld/all_time_magazine_covers_march_1923_to_march_2012/)
[Download Link](http://nonavoid.tumblr.com/).

I am a datamining noob, and I have an idea for a data mining exercise with these covers. I would like to get some guidance on how to go about it. 

I would like to extract just the text in each cover and get as output just the cover quotes. I would like to know what kind of training sets are available for this and what are some good imaging library in python for this. ",2,1
23,2012-4-10,2012,4,10,4,s18je,Extracting Structure on a Very Complexly Interacting Feature Space,https://www.reddit.com/r/MachineLearning/comments/s18je/extracting_structure_on_a_very_complexly/,CPlusPlusDeveloper,1333999222,"Consider the case where you have a dataset with fairly low dimensionality and a very high number of data points. One such that in most supervised learning approaches you'd be more concerned with minimizing bias rather than variance.

However in this problem the relatively low dimensionality is somewhat deceptive because the variables have very complex high dimensional interactions. I'll give you an example of what I'm talking about:

Suppose you were looking at randomly sampled points in chess games between evenly matches players. The data you had was which pieces each player had lost and you wanted to predict whether the game would ultimately end up in win/lose/draw.

Your feature space in this case is 30 0/1 variables: 8 pawns, 2 rooks, 2 knights, 2 bishops, 1 queen, (no king because if he's missing the game's over) multiplied by 2 sides. Assume that you have access to hundreds of millions of independent data points (you can always run AI-AI games and sample new positions).

The idiosyncrasies of the interactions here runs far deeper than standard linear/quadratic variable interactions, for example:
---------------------------------------------------------------------------------------------------------------------------

* Early game when a lot of material's on the board knights tend to be better than bishops. When there's not much material bishops tend to be better.

* If a player is missing a few pawns from ranks far away from each other that's not as bad as a cluster of pawns missing which opens up a hole in his defense.

* At the end of the game if the other side has two bishops, but the difference between having one bishop and no bishop is the difference between a guaranteed draw and win.

* A queen is about equal to two rooks early game, but becomes much less powerful late game.

* The difference of one side having 1 pawn and the other side having no pawns is far more than the difference between having 7 pawns and 8 pawns (because of the chance to convert the pawn to another piece).

As you can see there are endless variations of how the variables can interact. Many of the interactions go along an early/late game dimension. I.e. fewer material more material interactions, which basically involve some derived metric from the underlying 30 variables (more pieces that are captured off the board the ""later game"" it is, so a rough approximation might just be a sum of the 30 0/1 variables).

This makes it very challenging because it's almost like you need a combined supervised-unsupervised learning process that can discover interesting projections of the variables to interact on (i.e. discover the early/late game metric and start interacting other variables on it). Then plug those into a supervised method. This brings me to the various approaches that I'm considering:
---------------------------------------------------------------------------------------------------------------------------

* Boosted trees: The problem is the underlying weak learners don't have any chance to capture any of the interesting structure in the feature space. For trees to work they need to have many nodes to discover the complex interactions, and boosting tends not to do well with deep trees.

* Random forests: This has a better chance of working. But I would have to use very deep trees to capture the interaction, and even though random forest does pretty well with deep trees I've never heard of trees 20+ levels deep which is what you would need to discover some of the early/late game interactions. (At least you would need to go 20+ deep if you were splitting nodes based on single variables).

* SVMs: This seems somewhat appealing. I'm not too experienced with SVMs, have much more experience with trees. But from what I understand they have a good reputation from capturing non-parametric interactions like these. My only concern here is that a Gaussian kernel wouldn't work here. But again I'm inexperienced so any input on this in particular would be greatly appreciated.

* Logistic regression: This one I have to throw out right away because the problem is so non-linear, and I would have to include 32! interactions (an interaction for every possible piece).

* Neural nets: I'm always somewhat skeptical of these, but the universal function feature does seem to be what I'm looking for here. It would have to be high layer though, and even though the data set's large, with ANNs over fitting is always a problem IMO. Maybe bagging many-layered ANNs here might be a good approach, but I've never heard of anyone doing this so I'm not sure if there are good arguments not to.

* Manifold learning: This is the most ""far out"" approach, but it seems like the only thing that has the direct capability of recovering what I want, i.e. low-material high-material variable projection. Of course I don't know how exactly to translate an unsupervised method into a supervised problem...

Anyway this problem has me tied up in knots. So any interesting approaches or good advice would be greatly appreciated. Thanks.
---------------------------------------------------------------------------------------------------------------------------
",12,16
24,2012-4-10,2012,4,10,14,s243g,"Can someone explain the Perceptron Learning Algorithm to me?  Like I'm a 5-year-old, please.",https://www.reddit.com/r/MachineLearning/comments/s243g/can_someone_explain_the_perceptron_learning/,autodidact4life,1334034472,"I'm trying to slog through the Caltech online course called ""Learning from Data.""  We haven't even turned in the first homework yet, and I'm lost.  We're supposed to code our own PLA:

Create 100 random points above and below a random line and designate those that are above as +1 and those that are below as -1.  Got it. I can do that pretty easily.

I understand initializing all of the weights to zero.

But, I get really lost when it comes to how the algorithm iterates.  I know that I'm supposed to multiply the weights across the inputs to create an output for each point.  I'm supposed to compare it to the desired output and then iterate.  How do I calculate desired output?  

To help myself, I choice a random line y=2x and then points above and below the line.  (1,1,3), (1,3,7), (1,2,3), (1,4,7) which map to  +1, +1, and -1, -1, respectively.

Can anyone on Reddit walk me through the actual steps that the computer would take with these data points as it iterates?  It's easy enough to copy a script from the net, but I still don't quite understand what it all does, and the mathematical notations aren't as helpful as I would have hoped.

Any help would be VERY much appreciated.  ",11,18
25,2012-4-10,2012,4,10,17,s2agu,Linear Regression with nonlinear parameters,https://www.reddit.com/r/MachineLearning/comments/s2agu/linear_regression_with_nonlinear_parameters/,tasdomas,1334047031,"Trying to solve a rather simple multiple linear regression problem on a function, whose form resembles the logarithmic curve, I added log(x+1) for each parameter x to the parameter vector.

I determine the beta parameters using the normal equation method and it works fine. However, if I also add x^2 (or any other power) to the parameter vector, I see the residuals increase. Should parameters that do not influence the result not get small (or zero) beta values to cancel them out?

Perhaps this is caused by me using a pseudoinverse in the normal equation?",7,4
26,2012-4-10,2012,4,10,23,s2kig,Trying to understand Matlab's classify.m,https://www.reddit.com/r/MachineLearning/comments/s2kig/trying_to_understand_matlabs_classifym/,[deleted],1334068560,"I am trying to understand Matlab's classify.m which is a basic linear/quadratic discriminant implementation found as part of the statistics toolbox. 

From my machine learning textbook, for the quadratic discriminant the -0.5log(det(sigma)) term and the quadratic -0.5(x-m)'S ^-1 (x-m) term should have the same sign (both negative, with only the log prior the only positive term).

But in the matlab function it does the following:

    D(:,k) = log(prior(k)) - .5*(sum(A .* A, 2) + logDetSigma(k));

So the quadratic term and the logdet term have different signs. I would expect:

    D(:,k) = log(prior(k)) - .5*(sum(A .* A, 2) - 0.5*logDetSigma(k));

Could this be a bug in Matlab or am I missing something? The determinant is calculate as 2*sum(log(s)) where s are the singular values of R from QR decomposition of centered training data. I am not sure if this could explain it?
",0,3
27,2012-4-11,2012,4,11,8,s3fds,Speeding up R code using a just-in-time (JIT) compiler,https://www.reddit.com/r/MachineLearning/comments/s3fds/speeding_up_r_code_using_a_justintime_jit_compiler/,talgalili,1334102050,,0,1
28,2012-4-11,2012,4,11,12,s3qst,Missing lots of data. Best way to complete my data set?,https://www.reddit.com/r/MachineLearning/comments/s3qst/missing_lots_of_data_best_way_to_complete_my_data/,suorm,1334114261,"The data set I have is an old picture. It shows only the top 14 entries. For each key-value pair the value is a percentage. #1 entry's value is 0.62%. #14 entry's is 0.23%. All the rest is lost.

Intuitively, I know the distribution is very close to being uniform. I could just add the top rows percentages, subtract from 100, and divide by that many keys so they all have less or equal to 0.23. However, it would be better if I could find a special way to draw from a distribution as many times as needed, all draws should be less than .23 but should all add up to 100.

There must be a cool way to do this. Any ideas? TIA",7,3
29,2012-4-11,2012,4,11,19,s44p5,I tried to solve a puzzle using Eureqa. Here's the result...,https://www.reddit.com/r/MachineLearning/comments/s44p5/i_tried_to_solve_a_puzzle_using_eureqa_heres_the/,szza,1334139014,,12,27
30,2012-4-11,2012,4,11,21,s4942,Clever Algorithms: Statistical Machine Learning Recipes,https://www.reddit.com/r/MachineLearning/comments/s4942/clever_algorithms_statistical_machine_learning/,reidhoch,1334149080,,8,17
31,2012-4-12,2012,4,12,14,s5qma,Books about Decisions Trees,https://www.reddit.com/r/MachineLearning/comments/s5qma/books_about_decisions_trees/,rudyl313,1334208056,"I'm looking to learning about decision trees, boosting, bagging, random forests, ID3, CART, etc. Can anybody point me to a good book to learning about this genre of machine learning? Bonus points if it happens to be in the Kindle store :)",15,7
32,2012-4-12,2012,4,12,21,s61mr,Are there any recent books which contain info on deep learning?,https://www.reddit.com/r/MachineLearning/comments/s61mr/are_there_any_recent_books_which_contain_info_on/,SunnyJapan,1334232380,"By recent I mean the ones which incorporate the discoveries from 2006 papers by Hinton et al.
The only one which I know is ""Neural Networks and Learning Machines (3rd Edition)"" by Simon Haykin, which was published in 2008.",18,20
33,2012-4-12,2012,4,12,23,s6682,"Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer-Verlag New York, Inc., New York, NY, USA.",https://www.reddit.com/r/MachineLearning/comments/s6682/vladimir_n_vapnik_1995_the_nature_of_statistical/,qztifa,1334239751,,0,0
34,2012-4-13,2012,4,13,14,s7ihy,"A website that lists state of the art results in various machine learning problems, and by which methods it was achieved?",https://www.reddit.com/r/MachineLearning/comments/s7ihy/a_website_that_lists_state_of_the_art_results_in/,SunnyJapan,1334293999,"I remember I have seen such a website before, but I can't find it anymore, does anybody have a link?",6,23
35,2012-4-14,2012,4,14,0,s81mg,Could anybody explain Boltzmann Machines to me?,https://www.reddit.com/r/MachineLearning/comments/s81mg/could_anybody_explain_boltzmann_machines_to_me/,Aardshark,1334331595,"I have to implement a single layer (no hidden units) Boltzmann Machine for a class. I have an implementation done, but I'm not sure it's working correctly.

I'll describe what I've done and hopefully somone can point out if I've made any mistakes.

* The BM machine model is a set of binary units in which each unit is connected to every other unit via a weighted connection.

* First, we train the machine on a training set of examples to determine the weights of the connections.

* This learning phase can be divided into two parts - one is the empirical correlation between the units of the examples in the training set and one is the correlation between these units according to some probability model.

* At the end of learning, the probability correlation is subtracted from the empirical correlation and the resulting correlation is the weights for our BM.

* We can then set our neurons to an input and run the activation model on them to recieve an output.

I realise as I type this that my understanding of the topic is really very fuzzy. Here's a few questions : 

Having trained the machine on a set of examples, what should be the output when providing it with one of those examples as input? Will you always recieve that example back or will you occasionally get one of the other examples or even nonsense patterns?

I've found several RBM implementations online. How does a Restricted Boltzmann Machine differ from one with no hidden units? I'd like to be able to edit a RBM into what I've described so that I can make sure my implementation works the way it should.
",12,14
36,2012-4-14,2012,4,14,6,s8l3q,Help me get excited about my class!,https://www.reddit.com/r/MachineLearning/comments/s8l3q/help_me_get_excited_about_my_class/,generic_name,1334352963,"I'm taking a Management Science course, and it focuses heavily on linear programming and optimization.  I've seen these terms thrown around, but I'm not sure exactly how they apply to machine learning, or data mining (if it does). 
  
  Help me get excited about my class!  What kind of cool stuff are people doing with these types of models?  ",1,0
37,2012-4-15,2012,4,15,5,s9v51,Ask ML: Verifying Gradients numerically,https://www.reddit.com/r/MachineLearning/comments/s9v51/ask_ml_verifying_gradients_numerically/,strayadvice,1334433707,"I'm trying to implement numerical gradient checking as explained [here](http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization) to verify my implementation of the cost function in another exercise. I haven't changed much of the code describing the cost/gradient function of the neural network from what I used on the linked exercise (which passed the test comfortably).

I'm now experimenting with a three layer vanilla network - 50~ inputs, 25~ ""hidden nodes"", 1 output node, all of them using a tanh activation function. I had to modify the previous code very slightly to from a sparse autoencoder to a vanilla neural network. I'm almost certain that the implementation is correct, except that it doesn't seem to pass the numerical gradient test very well.

So my question is, are there limits to this method of checking an implementation of cost/gradient function numerically? I've spent more than 15 hours trying to debug, but haven't succeeded so far.",8,6
38,2012-4-15,2012,4,15,9,sa5va,Writing a paper on artificial intelligence,https://www.reddit.com/r/MachineLearning/comments/sa5va/writing_a_paper_on_artificial_intelligence/,robotsrobotschicken,1334448709,"So I'm writing a very basic introductory paper on artificial intelligence for my university course. My main focus is on neural networking, and I really need to interview someone with knowledge in the field. I was wondering if anyone with some credentials in the field would be interested in answering some relatively basic questions I have. ",5,14
39,2012-4-15,2012,4,15,14,saiah,a C/C++ Gibbs Sampling LDA implementation for latent topics discovery,https://www.reddit.com/r/MachineLearning/comments/saiah/a_cc_gibbs_sampling_lda_implementation_for_latent/,ieeaaauuuuooooo,1334469480,,1,6
40,2012-4-16,2012,4,16,4,sb5ew,What Happened to the Weka Mailing List??,https://www.reddit.com/r/MachineLearning/comments/sb5ew/what_happened_to_the_weka_mailing_list/,[deleted],1334517898,"For the past few weeks I have been unable to access the Weka mailing list to subscribe. It is probably the most complete help source, and it would be a shame if it was taken offline permanently. It looks like the entire domain is down. Is there an alternative somewhere that hasn't been mentioned on their websites??

http://list.scms.waikato.ac.nz/mailman/listinfo/wekalist",1,8
41,2012-4-16,2012,4,16,5,sba0e,How would you find patterns in conversations?,https://www.reddit.com/r/MachineLearning/comments/sba0e/how_would_you_find_patterns_in_conversations/,ScientistDaddy,1334523504,"If you had records of conversations between pairs of people (with associated profile data: age, gender, etc.), how would you go about finding predictive conversation patterns in either the audio or transcripts?",13,9
42,2012-4-16,2012,4,16,9,sbjsj,ELI5: Can anyone give me some intuition behind singular value decomposition/eigendecomposition of covariance matrices for dimensionality reduction?,https://www.reddit.com/r/MachineLearning/comments/sbjsj/eli5_can_anyone_give_me_some_intuition_behind/,lpiloto,1334535604,I just can't wrap my mind around what the eigenvalues/eigenvectors of a covariance matrix represent.,16,8
43,2012-4-16,2012,4,16,16,sc3m7,Why Netflix Never Implemented The Algorithm That Won The Netflix $1 Million Challenge | Techdirt,https://www.reddit.com/r/MachineLearning/comments/sc3m7/why_netflix_never_implemented_the_algorithm_that/,suhrob,1334562132,,10,89
44,2012-4-17,2012,4,17,4,sctpu,Stupid question about generating a dataset,https://www.reddit.com/r/MachineLearning/comments/sctpu/stupid_question_about_generating_a_dataset/,randomevent827,1334603728,"This is from the caltech course. I am having trouble figuring out how to generate this dataset.

""In this problem you will create your own target function f and data set D to see
how the Perceptron Learning Algorithm works. Take d = 2 so you can visualize the
problem, and choose a random line in the plane as your target function f (do this
by taking two random uniformly distributed points on [1, 1]  [1, 1] and taking
the line passing through them), where one side of the line maps to +1 and the other maps to 1. Choose the inputs xn of the data set as random points in the plane, and evaluate the target function on each xn to get the corresponding output yn""

In R,
x1 = runif(100, -1, 1)
x2 = runif(100, -1, 1)
point1 = runif(2, -1, 1)
point2  =runif(2, -1, 1)

Now how can I draw a line between these 2 points and mark everything on the left side as -1? I am probably over thinking this but I am beat.",4,3
45,2012-4-17,2012,4,17,9,sdawo,Resources for Extracting Main Text from a Webpage,https://www.reddit.com/r/MachineLearning/comments/sdawo/resources_for_extracting_main_text_from_a_webpage/,LADataJunkie,1334621277,"What are some good resources for extracting the main text of a webpage? What I mean is, given a web page in HTML format, extract the main body of the text, not including irrelevant stuff like sidebars, ads etc.

I know that this is an active research topic, but I am curious if anyone has found a library that works well.",9,8
46,2012-4-17,2012,4,17,22,se6fi,My friend is scraping text NFL play by play data.  ,https://www.reddit.com/r/MachineLearning/comments/se6fi/my_friend_is_scraping_text_nfl_play_by_play_data/,imissyourmusk,1334670439,"What would be an interesting machine learning project to do with this data?

Here is a sample: (13:02) S.Weatherford punts 53 yards to IND 18 Center-K.Houser. T.Rushing pushed ob at IND 34 for 16 yards (U.Young).
",13,11
47,2012-4-18,2012,4,18,0,seayp,Crowd computing taps artificial intelligence to revolutionize the power of our collective brains,https://www.reddit.com/r/MachineLearning/comments/seayp/crowd_computing_taps_artificial_intelligence_to/,andycrowdcontrol,1334676325,,1,8
48,2012-4-18,2012,4,18,13,sfj35,WebHarvester: A Machine Learning Plugin,https://www.reddit.com/r/MachineLearning/comments/sfj35/webharvester_a_machine_learning_plugin/,MuffinShit,1334724307,,7,3
49,2012-4-18,2012,4,18,15,sfnnc,Algoritm results comparison in a production env.,https://www.reddit.com/r/MachineLearning/comments/sfnnc/algoritm_results_comparison_in_a_production_env/,[deleted],1334731305,"I remember a concept when studying( and forgot to take notes) fraud detection with genetic algoritms that is called ""concurrent model"" or something like that ( I really don't remember ) The basic idea is that two or more models compete against each other and once we have a best fit we promote the winner to Master and start a new generation.

So I ask you my fellow mls, do you know this concept? could you please point me to the right bibliography? 

regards ",1,0
50,2012-4-19,2012,4,19,4,sggod,Looking for a summer intern,https://www.reddit.com/r/MachineLearning/comments/sggod/looking_for_a_summer_intern/,[deleted],1334776086,"Primary Job Responsibilities
-----------------
Design, Develop, and Test various Machine Learning algorithms for classification of eBay listings.
Present the work in internal eBay forums.
If deemed necessary, facilitate external publication of the work.

eBay Inc. interns will participate in a challenging 10-12 week summer program, then return to school in the fall. Summer interns will obtain practical work experience, learn about the eBay Inc. businesses, and receive mentorship from their manager and team. Summer Interns will set goals/objectives with their managers at the start of the internship, and complete end of summer performance evaluations.

Job Requirements
-----------------
Knowledge of common ML techniques in the area of Classification
Knowledge of Hadoop is preferred

Education
----------
Bachelors Degree Required

Please respond to this if interested.",8,6
51,2012-4-19,2012,4,19,6,sgnye,Beyond Prediction Accuracy,https://www.reddit.com/r/MachineLearning/comments/sgnye/beyond_prediction_accuracy/,kumquatz,1334783368,,9,24
52,2012-4-19,2012,4,19,12,sh7ut,In need of a suitable class of algorithms for linking two sets of records.,https://www.reddit.com/r/MachineLearning/comments/sh7ut/in_need_of_a_suitable_class_of_algorithms_for/,SCombinator,1334804794,"I have a bipartite graph (two tables of records, U &amp; V) that need matching.

Unfortunately in my case records may connect to more than one record in the other table, and all columns are continuous which seems to rule out normal ways of record linking. 

Essentially there should exist an edge Eij between two records when part of Ui is part of Vj. The information I have is that the sum of edges connected to Ui should sum to the values in Ui, and the edges connected to Vj should sum to the values in Vj. (It doesn't always because the data can be bad.)

Help is appreciated, I've been looking at the Subset Sum problem, Constrain Propagation, Maximum Flow, and Bayesian Record Linking, and all of them seem like close fits, I'm not sure if any of them map to the problem very easily.",1,4
53,2012-4-19,2012,4,19,23,shtsi,The Netflix Tech Blog: Netflix Recommendations: Beyond the 5 stars (Part 1),https://www.reddit.com/r/MachineLearning/comments/shtsi/the_netflix_tech_blog_netflix_recommendations/,drcross,1334845085,,0,1
54,2012-4-20,2012,4,20,2,si490,Suggested ML technique for recommending user behavior with a GUI-based tool,https://www.reddit.com/r/MachineLearning/comments/si490/suggested_ml_technique_for_recommending_user/,LikesToAskWhy,1334857190,"I have been asked to add the capability to recommend the best way to use a particular GUI program, based on how it has been used by other users before. It is highly targeted to a particular domain, wherein the User uploads some data and then perform a series of different steps one after another and the program can be used to view and analyze the data. Hence, the GUI program ends up being used in different ways by different Users. We would like to build a recommender system to learn the way in which it is used most efficiently (for certain types of input data) and then make recommendations to the user. 
What ML technique (along with a mature toolset) would work out? 
I am new to this, so looking around it seems that HMM might work out ? I found some relevant discussion here  - http://stackoverflow.com/questions/6990230/supervised-learning-for-user-behavior-over-time",5,2
55,2012-4-20,2012,4,20,8,sinsc,Question about test error in k-fold cross validation,https://www.reddit.com/r/MachineLearning/comments/sinsc/question_about_test_error_in_kfold_cross/,GotGoose,1334877669,"For my particular data set, I am running k-fold cv for k = 3, 5, and 10. Most of the time, the error decreases as I progress from 3 to 5 and then to 10, but sometimes it decreases from 3 to 5 and then increases from 5 to 10. Does this mean anything? I would expect error to decrease. I have 28 subjects, if that makes any difference.

I'm currently an undergraduate, so any insight would be really helpful.",5,6
56,2012-4-21,2012,4,21,4,sk5jk,How Prismatic clusters related stories,https://www.reddit.com/r/MachineLearning/comments/sk5jk/how_prismatic_clusters_related_stories/,kent37,1334949398,,1,14
57,2012-4-21,2012,4,21,17,sl29p,Liebherr 996 with a PC1250 2048x1152,https://www.reddit.com/r/MachineLearning/comments/sl29p/liebherr_996_with_a_pc1250_2048x1152/,[deleted],1334996335,,0,1
58,2012-4-22,2012,4,22,15,smdpr,A geek with a hat  ml-class.org vs. real world ML class,https://www.reddit.com/r/MachineLearning/comments/smdpr/a_geek_with_a_hat_mlclassorg_vs_real_world_ml/,[deleted],1335077780,,2,0
59,2012-4-23,2012,4,23,3,smxrx,Machine learning for identification of cars,https://www.reddit.com/r/MachineLearning/comments/smxrx/machine_learning_for_identification_of_cars/,kafka399,1335121098,,9,22
60,2012-4-23,2012,4,23,4,smy7v,"Open source machine learning tool, Divvy, now on the Mac App Store (free)",https://www.reddit.com/r/MachineLearning/comments/smy7v/open_source_machine_learning_tool_divvy_now_on/,kevestun,1335121615,,1,7
61,2012-4-23,2012,4,23,16,snvul,Workshop Machines,https://www.reddit.com/r/MachineLearning/comments/snvul/workshop_machines/,rashmiseoindia,1335167161,,0,1
62,2012-4-24,2012,4,24,1,sodu1,Stanford's ML class for Spring 2012 is now live for anyone interested.,https://www.reddit.com/r/MachineLearning/comments/sodu1/stanfords_ml_class_for_spring_2012_is_now_live/,Lambda_Rail,1335199904,,10,55
63,2012-4-24,2012,4,24,3,soihg,The Jaded Bayes classifier - healthy skepticism by learning conditional independence structure,https://www.reddit.com/r/MachineLearning/comments/soihg/the_jaded_bayes_classifier_healthy_skepticism_by/,beaucronin,1335205107,,0,1
64,2012-4-24,2012,4,24,8,sp1jy,"What are the limitations on pattern recognition in highly dimensional data? Given a large enough data set, is it ALWAYS possible to extract a pattern?",https://www.reddit.com/r/MachineLearning/comments/sp1jy/what_are_the_limitations_on_pattern_recognition/,[deleted],1335225207,"I'm taking a course on AI and machine learning and we've just been recently introduced to k-NN, naive-bayes and 1R. During class I began to wonder if it was always possible to extract a pattern no matter how complex the data was, and what the underlying mathematics that governs this was.

For example, lets say you have a ranking system and ranks are based on various attributes. If there are hundreds of attributes, and each one has an unknown weighting, as well as an unknown contribution to the weighting of OTHER attributes, is it possible to extract any reasonable pattern? 

In this particular case, even if we have infinite training data will it be exponential time complexity trying to figure out the how the value of one attribute changes another? Does this make it unfeasible to extract a pattern?

Being new to the field, my terminology is probably wrong so Ill use an example.

If we have a training data it will be of the form:

rank, attribute A, attribute B, C, D, ... (assuming the number of attributes is in the hundreds)

It's known that each attribute does not contribute equally to the rank. It's known that the value of an attribute can possibly change the contribution of OTHER attributes to the final rank. So if attribute A is high for some data, then attribute B, C and D contribute significantly more, but perhaps attribute H, K, W attribute less.

This isn't homework or anything, I've just can't stop thinking about it since class and I didn't want to ask my teacher in case she thought I was an idiot.",8,3
65,2012-4-24,2012,4,24,11,sp9zd,Question about PSO and normalization,https://www.reddit.com/r/MachineLearning/comments/sp9zd/question_about_pso_and_normalization/,JolienJM,1335233996,"I'm currently trying to implement a hybrid PSO (Particle Swarm Optimization) to perform feature selection (binary PSO) and parameter optimization (continuous PSO) for a Support Vector Regression machine simultaneously. I have a published article as a reference, but engineering articles usually lack a lot of details.

I'm currently at the 'initialize parameters' phase, so the question is not that complicated. The article said that all input variables were scaled, but later on they use non-scaled values for the SVR parameters. Does anyone know if it will make a difference whether or not to scale the SVR parameters? I'm guessing that if I do scale them, I'll need an extra step to get the original value back for SVR training. However, if I don't scale them, I need 3 different Xmax and Vmax values, so programming-wise it doesn't seem like one is more work than the other.

The article does not seem to be open-source, but for those who have access and want to view the article, it can be found here: http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5212569",5,4
66,2012-4-25,2012,4,25,6,sqm8m,Machine Learning in Action: Interactive Model Gallery ,https://www.reddit.com/r/MachineLearning/comments/sqm8m/machine_learning_in_action_interactive_model/,jjdonald,1335302999,,0,18
67,2012-4-25,2012,4,25,18,sriv5,Analog Reservoir Computing using Microphone?,https://www.reddit.com/r/MachineLearning/comments/sriv5/analog_reservoir_computing_using_microphone/,marshallp,1335345472,"Reservoir computing is a neural-network based machine learning system where a nonlinear stage creates outputs that are trained on by classifiers.

Analog reservoir computing has been done using buckets of water and optoelectronics.

My question is, is there a way of doing it using a microphone. You feed a signal out through a speaker, the nonlinearities are introduced by the environment and then this is picked up by the microphone on the computer. Has this been tried?

Also, is there a material that can retain it's ""memory"" longer for this - something that is affected by the sound and thus retains a memory and also produces sound. A gel material for example. If anyone has ideas, that would be great thanks.",2,6
68,2012-4-26,2012,4,26,4,ss9i2,"Blog collection of arXiv papers, cross-disciplinary overlap in math, biology, physics, ML, etc. Range of difficulties of papers",https://www.reddit.com/r/MachineLearning/comments/ss9i2/blog_collection_of_arxiv_papers_crossdisciplinary/,arxivdex,1335383202,,1,10
69,2012-4-26,2012,4,26,22,stj7e,Surviving the 1st Global Data Science Hackathon - Blog,https://www.reddit.com/r/MachineLearning/comments/stj7e/surviving_the_1st_global_data_science_hackathon/,timgluz,1335447847,,0,2
70,2012-4-27,2012,4,27,9,sug4a,Is there any kind of a PAC bound for Stochastic Gradient Trees? (Or even what constitutes model Complexity),https://www.reddit.com/r/MachineLearning/comments/sug4a/is_there_any_kind_of_a_pac_bound_for_stochastic/,duckandcover,1335485270,,2,5
71,2012-4-27,2012,4,27,11,sunnm,"ICML Workshop on Statistics, Machine Learning and Neuroscience",https://www.reddit.com/r/MachineLearning/comments/sunnm/icml_workshop_on_statistics_machine_learning_and/,badcorrelation,1335493825,,1,6
72,2012-4-27,2012,4,27,15,suxbd,Tutorial and code to get state-of-the-art performance on Google Streetview House Numbers using deep learning  with EBLearn C++ ML library 1.1 (new release),https://www.reddit.com/r/MachineLearning/comments/suxbd/tutorial_and_code_to_get_stateoftheart/,sermanet,1335507042,,2,21
73,2012-4-29,2012,4,29,10,sxktx,A question about notation used in neural networks,https://www.reddit.com/r/MachineLearning/comments/sxktx/a_question_about_notation_used_in_neural_networks/,SunnyJapan,1335663121,Why for a synaptic weight from neuron i to neuron j we use notation Wji? Isn't Wij a more straightforward notation?,7,8
74,2012-4-30,2012,4,30,15,szdlx,Talent crunch for machine learning and statistics skills,https://www.reddit.com/r/MachineLearning/comments/szdlx/talent_crunch_for_machine_learning_and_statistics/,contrarianism,1335765882,,11,25
75,2012-4-30,2012,4,30,23,szrk4,The Million Song Dataset Challenge - recommender system competition,https://www.reddit.com/r/MachineLearning/comments/szrk4/the_million_song_dataset_challenge_recommender/,bmcfee,1335796453,,5,30
