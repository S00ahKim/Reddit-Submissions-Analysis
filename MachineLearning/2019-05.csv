,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2019-5-1,2019,5,1,9,bjasow,Easily search and download public datasets for ML,https://www.reddit.com/r/MachineLearning/comments/bjasow/easily_search_and_download_public_datasets_for_ml/,reimmoriks,1556669893,,0,1
1,2019-5-1,2019,5,1,9,bjaspq,[Project] Human/Robot relationship dynamics: A 7 min. survey for you,https://www.reddit.com/r/MachineLearning/comments/bjaspq/project_humanrobot_relationship_dynamics_a_7_min/,neuroticcowboy,1556669898,"Hello AI technology users!

I am writing on behalf of my institution's Technology, Relationship, and Language lab.  We are a small group of Psychologists aiming to understand the relationship quality and formation characteristics of chatbot and human connections. Once we can better understand the positive aspects of these technological relationships, we can initiate further studies that can aid in the advancement of more meaningful and fulfilling relationships with humans and their technological counterparts.

You are invited to participate in a survey assessing your experiences interacting with artificial intelligence (AI) chatbots. Specifically, our study focuses on the AI chatbot Replika. Your participation is voluntary, but nonetheless greatly appreciated. Our goal is simple: analyzing the effectiveness, quality, and relationships of human participants with their personal chatbots. With your help in taking this quick (7 min max) survey, we can create an informative understanding of the areas in which AI technology is improving the lives and relationships with humans. Please feel free to reach out to me or our principal investigator (whose contact information is included in the first page of the survey) with any questions. The survey is linked below:

[https://forms.gle/ZYbhc5BnFbcGm5ib6](https://forms.gle/ZYbhc5BnFbcGm5ib6)",0,3
2,2019-5-1,2019,5,1,9,bjaz63,"PyTorch 1.1: TensorBoard, Attributes/Dicts/Lists and User-defined types in JIT/TorchScript, Improved Distributed",https://www.reddit.com/r/MachineLearning/comments/bjaz63/pytorch_11_tensorboard_attributesdictslists_and/,_jamorton,1556670984,,0,3
3,2019-5-1,2019,5,1,10,bjbqys,"Release Official TensorBoard Support, Attributes, Dicts, Lists and User-defined types in JIT / TorchScript, Improved Distributed  PyTorch 1.1.0 Released",https://www.reddit.com/r/MachineLearning/comments/bjbqys/release_official_tensorboard_support_attributes/,SkiddyX,1556675641,[removed],0,1
4,2019-5-1,2019,5,1,10,bjbrc9,"PyTorch 1.1.0 Released  TensorBoard Support, Attributes, Dicts, Lists and User-defined types in JIT / TorchScript, Improved Distributed",https://www.reddit.com/r/MachineLearning/comments/bjbrc9/pytorch_110_released_tensorboard_support/,SkiddyX,1556675700,[removed],0,1
5,2019-5-1,2019,5,1,10,bjbrju,"[N] PyTorch 1.1.0 Released  TensorBoard Support, Attributes, Dicts, Lists and User-defined types in JIT / TorchScript, Improved Distributed",https://www.reddit.com/r/MachineLearning/comments/bjbrju/n_pytorch_110_released_tensorboard_support/,SkiddyX,1556675737,Checkout the release notes here: https://github.com/pytorch/pytorch/releases/tag/v1.1.0,44,311
6,2019-5-1,2019,5,1,10,bjbrl4,Is machine learning useful for breaking down scripts,https://www.reddit.com/r/MachineLearning/comments/bjbrl4/is_machine_learning_useful_for_breaking_down/,deo85,1556675743,[removed],0,1
7,2019-5-1,2019,5,1,12,bjcfyg,Machine Learning Mindset - A New Blog to promote Artificial Intelligence and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bjcfyg/machine_learning_mindset_a_new_blog_to_promote/,iramirsina,1556680051,"Dear all,

&amp;#x200B;

I established a blog and a non-profit organization called [Machine Learning Mindset](https://machinelearningmindset.com/) trying to convey insightful information and debates about artificial intelligence and machine learning. Please take a look at the blog and the introductory post to machine learning:

[Blog](https://machinelearningmindset.com/blog/)

[What is Machine Learning?](https://machinelearningmindset.com/what-is-machine-learning/)

&amp;#x200B;

**Our Pledge**

**\*** We provide ***insightful posts*** in Machine Learning and Artificial Intelligence that help you to learn concepts.  
**\*** You feedback comments are much appreciated and ***NOTHING*** will remain unresponded. However, forgive us for possible delays.  
**\*** No more than ***three emails*** per week! No one wants to disappear in the cloud! 

&amp;#x200B;

**Benefits of Subscription**

\* You will receive ***exclusive materials*** that are ***NOT*** available on the blog publicly such as the fully functional source code regarding an explained concept on the blog.  
**\*** Consultant regarding your ***Machine Learning projects***.  
**\***Free materials only available to subscribers. 

&amp;#x200B;

**Why Machine Learning?**

While artificial intelligence (AI) is the popular science of *emulating human cognitive behavior* and abilities, machine learning is a particular subset of AI that teaches a machine what it is supposed to learn. AI must be developed to process the vast volumes of available data into the essential pieces of information that decision-makers and others can comprehend, and *Machine Learning is the science* that enables this AI capability.The ***massive amount of available data*** and the ***advent of a new levelcomputational power*** made Machine Learning as of the most growing fields in science and technology. Machine Learning aims to examine bigger, more complex data and produce faster and more accurate to ***reduce the human cognitive and non-cognitive*** efforts. 

&amp;#x200B;

**Vision**

This [blog](https://machinelearningmindset.com/blog/) aims to provide the knowledge to become a master in Machine Learning. The goal is to bring insight from scratch ***without any assumption*** on the technical level of the readers. If you found it otherwise for any post, let us know by sending us your feedback. **We are open to change!** 

&amp;#x200B;

**What we have been doing so far?**

We have been actively contributing to the open source community associated with Machine Learning and Artificial Intelligence and developed some world-wide-known project so far. To mention a few, please refer to the following projects. 

&amp;#x200B;

*Processing gif y3bfqbxeniv21...*

  
 **TensorFlow Course** This repository aims to provide simple and ready-to-use tutorials for Google TensorFlow which is one of the most famous Machine Learning libraries. **Currently ranked as** [**#9 Github project in TensorFlow worldwide**](https://github.com/topics/tensorflow). [Project Page](https://github.com/machinelearningmindset/TensorFlow-Course)

*Processing gif bna74v5gniv21...*

  
 **Deep Learning Ocean** The purpose of this project is to introduce a shortcut to developers and researcher for finding useful resources about Deep Learning. These resources include a curated list published papers, projects, blogs, books and etc. [Project Page](https://github.com/machinelearningmindset/deep-learning-ocean) 

&amp;#x200B;

&amp;#x200B;

**Final word**

Once again thank you so much for your interest. Please help us to serve better with your feedback. *There will be NO success without knowing what is happening behind the door*! Hopefully, we will all learn from each other on this journey.  
Sincerely,  
Amirsina  
Founder ofMachine Learning Mindset

&amp;#x200B;

[My personal website](https://astorfi.github.io/)

[My GitHub](https://github.com/astorfi)",0,1
8,2019-5-1,2019,5,1,12,bjcg5n,Top most machine learning algorithms in python,https://www.reddit.com/r/MachineLearning/comments/bjcg5n/top_most_machine_learning_algorithms_in_python/,amitarora5423,1556680090,,0,1
9,2019-5-1,2019,5,1,12,bjcrv5,What exactly is 'reproducing' in Reproducing Kernel Hilbert Space (RKHS) ?,https://www.reddit.com/r/MachineLearning/comments/bjcrv5/what_exactly_is_reproducing_in_reproducing_kernel/,Seth_Macfarlene,1556682361,I have the theoretical knowledge of RKHS and I performed well on my tests (not proud) but I want to know the physical meaning of RKHS and how to visualize them.,0,1
10,2019-5-1,2019,5,1,12,bjcuvu,Deploying big data models on Databricks [P] [D] [R],https://www.reddit.com/r/MachineLearning/comments/bjcuvu/deploying_big_data_models_on_databricks_p_d_r/,ssakage,1556682955,"Anyone know any ways to deploy a pretrained ALSModel using databricks? Any and all methods, Ideas are welcome!",0,1
11,2019-5-1,2019,5,1,13,bjcy4r,[P] Deploying Big Data Models using Databricks,https://www.reddit.com/r/MachineLearning/comments/bjcy4r/p_deploying_big_data_models_using_databricks/,ssakage,1556683586,Any idea how to deploy big data models like ALSModel using databricks?Any and all ideas would be appreciated,0,3
12,2019-5-1,2019,5,1,13,bjd2xv,[D] PSA: Misleading default pytorch behavior for regression,https://www.reddit.com/r/MachineLearning/comments/bjd2xv/d_psa_misleading_default_pytorch_behavior_for/,csinva,1556684556,"PSA: When using pytorch for regression, the output must have precisely the right shape, or its behavior is very strange.

&amp;#x200B;

For example, if Y is one-dimensional, the Y vector must have shape (batch\_size, 1) and not just shape (batch\_size). If  given shape (batch\_size), pytorch does not throw an error but actually fits something very different from what you'd expect (simple example [here](https://github.com/csinva/pytorch_dangerous_mistakes/blob/master/output_shape_err.ipynb)).

https://i.redd.it/gtfmwmzt0jv21.png",1,1
13,2019-5-1,2019,5,1,13,bjdcvz,Unsupervised Data Augmentation,https://www.reddit.com/r/MachineLearning/comments/bjdcvz/unsupervised_data_augmentation/,iyaja,1556686718,,2,0
14,2019-5-1,2019,5,1,14,bjdlw9,How does the ID3 algorithm works in Decision Trees ?,https://www.reddit.com/r/MachineLearning/comments/bjdlw9/how_does_the_id3_algorithm_works_in_decision_trees/,ml_coder_pro,1556688721," 

## Introduction

Humans learn from the past experiences while computers need to be told what to do and need to be programmed. And now the question is can we get computers to learn from experience too? This question is precisely answered by Machine Learning. It is a method to teach computers to learn to perform tasks from past experience. Of course, it goes without saying, for computers, the past experience is data.

Lets look at one of the most popular Machine Learning algorithms which is largely in use today in many industries and try to explain whats going on underneath the hood.

## Decision Trees

**Introduction**

Lets say we have the following table about which user downloaded what app?

checkout more at [https://youtu.be/UdTKxGQvYdc](https://youtu.be/UdTKxGQvYdc)",0,1
15,2019-5-1,2019,5,1,14,bjdo23,pytorch/botorch by pytorch,https://www.reddit.com/r/MachineLearning/comments/bjdo23/pytorchbotorch_by_pytorch/,sjoerdapp,1556689217,,0,1
16,2019-5-1,2019,5,1,17,bjeuy5,How can a 17 year old make money online with deep learning on keras ? What is a good website to get contracts ?,https://www.reddit.com/r/MachineLearning/comments/bjeuy5/how_can_a_17_year_old_make_money_online_with_deep/,ahmedoy,1556700425,[removed],0,1
17,2019-5-1,2019,5,1,18,bjf5fp,Delving deep into Generative Adversarial Networks (GANs),https://www.reddit.com/r/MachineLearning/comments/bjf5fp/delving_deep_into_generative_adversarial_networks/,GKalliatakis,1556703164,[removed],0,1
18,2019-5-1,2019,5,1,18,bjf9dq,Introductory course to Natural Language Generation?,https://www.reddit.com/r/MachineLearning/comments/bjf9dq/introductory_course_to_natural_language_generation/,_MarkG_,1556704154,"I was googling (results here ""[I Googled for you: Natural Language Generation](https://markg.blog/2019/05/01/i-googled-for-you-natural-language-generation/)"") on NLG and haven't found any online courses for NLG. All I could find was targeted to NLP instead. Are you aware of anything out there? (There is of course a lot of reading material.)

&amp;#x200B;

Thank you",0,1
19,2019-5-1,2019,5,1,19,bjffrn,[N] Introducing DyNetSharp - an open source powerful DL library for C#!,https://www.reddit.com/r/MachineLearning/comments/bjffrn/n_introducing_dynetsharp_an_open_source_powerful/,sshmidman,1556705654,"DyNetSharp - C# wrapper for DyNet: The Dynamic Neural Network Toolkit

&amp;#x200B;

DyNet is a neural network library developed by Carnegie Mellon University and many others. It is written in C++ (with bindings in Python, and for the first time now also in C#) and is designed to work well with networks that have dynamic structures that change for every training instance. For example, these kinds of networks are particularly important in natural language processing tasks, and DyNet has been used to build state-of-the-art systems for syntactic parsing ([https://github.com/clab/lstm-parser](https://github.com/clab/lstm-parser)), machine translation ([https://github.com/neubig/lamtram](https://github.com/neubig/lamtram)), morphological inflection ([https://github.com/mfaruqui/morph-trans](https://github.com/mfaruqui/morph-trans)), and many other application areas.

&amp;#x200B;

C# installation is currently available through NuGet ([https://www.nuget.org/packages/DynetSharp](https://www.nuget.org/packages/DynetSharp)), and will be available for compiling from the source over the next couple of days.

For documentation on how to use, and for some examples: [https://github.com/shaltielshmid/dynet/tree/master/contrib/csharp](https://github.com/shaltielshmid/dynet/tree/master/contrib/csharp)

&amp;#x200B;

\[The link above is the fork, the code is currently being merged into the main branch: [http://github.com/clab/dynet](http://github.com/clab/dynet)\]

&amp;#x200B;

It will be continually developed, any contributions are welcome.",0,10
20,2019-5-1,2019,5,1,19,bjfon6,[D] Sharing GPUs with host OS,https://www.reddit.com/r/MachineLearning/comments/bjfon6/d_sharing_gpus_with_host_os/,charpi123,1556707731,"Hello all, I've some experience in building DL models using Tensorflow in Unix environments leveraging on platforms such as AWS and GCP. My work/company also provides me with Unix servers to build my models. 

Now that I want to build my own DL rig, on Windows no less, I am interested to know about performance degradation on Tensorflow of a GPU that is shared with the host OS. Anyone has any experience in this?

PS. I have no choice but to use Windows, much as I would prefer a Linux system.

TLDR; Want to do deep learning on GPUs on Windows, worried whether the host OS sharing the GPU will affect performance much, using Tensorflow.",13,6
21,2019-5-1,2019,5,1,20,bjfxqy,Setting and resetting LSTM hidden states in Tensorflow 2.0,https://www.reddit.com/r/MachineLearning/comments/bjfxqy/setting_and_resetting_lstm_hidden_states_in/,ADGEfficiency,1556709681,,0,1
22,2019-5-1,2019,5,1,20,bjg3xz,What is the best method for face recognition under one-shot learning?,https://www.reddit.com/r/MachineLearning/comments/bjg3xz/what_is_the_best_method_for_face_recognition/,Nabeel965,1556710978,[removed],0,1
23,2019-5-1,2019,5,1,20,bjg85z,MSc Thesis in ML applied to marketing;,https://www.reddit.com/r/MachineLearning/comments/bjg85z/msc_thesis_in_ml_applied_to_marketing/,pilsbaas14,1556711831,[removed],0,1
24,2019-5-1,2019,5,1,21,bjgiua,An intuitive understanding of the LAMB optimizer,https://www.reddit.com/r/MachineLearning/comments/bjgiua/an_intuitive_understanding_of_the_lamb_optimizer/,ben_mann,1556713845,[removed],0,2
25,2019-5-1,2019,5,1,21,bjgo7l,[R] Constant-Time Machine Translation with Conditional Masked Language Models,https://www.reddit.com/r/MachineLearning/comments/bjgo7l/r_constanttime_machine_translation_with/,milaworld,1556714808,,1,1
26,2019-5-1,2019,5,1,21,bjgpiz,Artificial Intelligence Fails In High School Math Test!,https://www.reddit.com/r/MachineLearning/comments/bjgpiz/artificial_intelligence_fails_in_high_school_math/,Sciencenews18AS,1556715047,,0,1
27,2019-5-1,2019,5,1,21,bjgpt2,[D] Confused about using Masking in Transformer Encoder and Decoder,https://www.reddit.com/r/MachineLearning/comments/bjgpt2/d_confused_about_using_masking_in_transformer/,bikanation,1556715101,"I dont get the idea why do we use masking before the calculation of attention. I get the idea that we want to input the decoder one word at a time, but i dont understand why in the implementation they used masking in encoder and a mask in the first part of the decoder where the inputs from encoder is passed.",2,2
28,2019-5-1,2019,5,1,22,bjgsw3,Request for an emotion Art dataset,https://www.reddit.com/r/MachineLearning/comments/bjgsw3/request_for_an_emotion_art_dataset/,sleebapaul,1556715637,[removed],0,1
29,2019-5-1,2019,5,1,22,bjgwma,[P] Octave Convolution implementation in PyTorch (+ pip package),https://www.reddit.com/r/MachineLearning/comments/bjgwma/p_octave_convolution_implementation_in_pytorch/,inkognit,1556716266,,1,1
30,2019-5-1,2019,5,1,22,bjh9z3,[R] Creating artificial astronomy surveys with SGAN,https://www.reddit.com/r/MachineLearning/comments/bjh9z3/r_creating_artificial_astronomy_surveys_with_sgan/,Smith4242,1556718525,"Github: [https://github.com/Smith42/XDF-GAN](https://github.com/Smith42/XDF-GAN)

Paper: [https://arxiv.org/abs/1904.10286](https://arxiv.org/abs/1904.10286)

Link to 7.6 billion pixel generated deep field: [http://star.herts.ac.uk/\~jgeach/gdf.html](http://star.herts.ac.uk/~jgeach/gdf.html)

&amp;#x200B;

Hi all! We have used an SGAN to generate an artificial astronomical survey that is statistically indistinguishable from the source data. In the example case above we used the Hubble eXtreme Deep Field (XDF, [http://xdf.ucolick.org/](http://xdf.ucolick.org/)) as training data.

&amp;#x200B;

We see this method being used to quickly assemble large training sets for classification and instance segmentation of galaxies. In the early stages of a new survey, relatively small amounts of data would be collected, and then expanded  to a level useful for training deep learning models. This would allow us to start using deep learning algorithms on new surveys sooner than previously possible.",4,36
31,2019-5-1,2019,5,1,23,bjhj9y,Deep Learning is dank af,https://www.reddit.com/r/MachineLearning/comments/bjhj9y/deep_learning_is_dank_af/,schludy,1556720006,,0,1
32,2019-5-1,2019,5,1,23,bjhnm1,Factoring with neural networks,https://www.reddit.com/r/MachineLearning/comments/bjhnm1/factoring_with_neural_networks/,alpacalaika,1556720668,[removed],0,1
33,2019-5-1,2019,5,1,23,bjht5j,Tracking soccer players,https://www.reddit.com/r/MachineLearning/comments/bjht5j/tracking_soccer_players/,nreininho,1556721468,[removed],0,1
34,2019-5-1,2019,5,1,23,bjhtln,Feature maps of model trained on VGGFace2 dataset,https://www.reddit.com/r/MachineLearning/comments/bjhtln/feature_maps_of_model_trained_on_vggface2_dataset/,nomad_world,1556721536,[removed],0,1
35,2019-5-2,2019,5,2,0,bji40l,What should I focus on to be useful in an AI research lab as a MD?,https://www.reddit.com/r/MachineLearning/comments/bji40l/what_should_i_focus_on_to_be_useful_in_an_ai/,sgtoox,1556722998,[removed],0,1
36,2019-5-2,2019,5,2,0,bji8yc,[Research] Deep Dive Into The Way CNNs Solve Facial Expression Recognition,https://www.reddit.com/r/MachineLearning/comments/bji8yc/research_deep_dive_into_the_way_cnns_solve_facial/,cdossman,1556723641,,0,1
37,2019-5-2,2019,5,2,0,bjijh9,[P] From Cups to Consciousness (Part 2): From simulation to the real world,https://www.reddit.com/r/MachineLearning/comments/bjijh9/p_from_cups_to_consciousness_part_2_from/,benfduffy,1556725122,"The saga continues (posted about part 1 [here](https://www.reddit.com/r/MachineLearning/comments/b6l314/p_from_cups_to_consciousness_part_1_how_are_cups/)):

[From Cups to Consciousness (Part 2): From simulation to the real world](https://towardsdatascience.com/from-cups-to-consciousness-part-2-from-simulation-to-the-real-world-a9ea1249e233)

Within we discuss our gym wrapper and task interface for the amazing open-source 3D photo-realistic environment, [AI2Thor](http://ai2thor.allenai.org/), the steps taken to bring our agents to more physically realistic simulations ([PyBullet](https://pybullet.org/wordpress/) and [Gazebo](http://gazebosim.org/)), and we'll cover a simple and real robot called Vector, which can solve real world navigation problems.

Also, check out the intense clashes between armies of Vectors in a game of ""Cup Carnage"" and other GIFs of Vector exploring the moon.",0,2
38,2019-5-2,2019,5,2,0,bjip31,Build your first Neural Network to predict house prices with Keras,https://www.reddit.com/r/MachineLearning/comments/bjip31/build_your_first_neural_network_to_predict_house/,caternoon,1556725925,,2,1
39,2019-5-2,2019,5,2,0,bjipf9,"Simple Questions Thread May 01, 2019",https://www.reddit.com/r/MachineLearning/comments/bjipf9/simple_questions_thread_may_01_2019/,AutoModerator,1556725972,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",0,1
40,2019-5-2,2019,5,2,0,bjis25,Google Researchers Add Attention to Augment Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/bjis25/google_researchers_add_attention_to_augment/,Yuqing7,1556726328,[removed],0,1
41,2019-5-2,2019,5,2,1,bjiydt,Overfitting,https://www.reddit.com/r/MachineLearning/comments/bjiydt/overfitting/,ComedyIsOver,1556727186,,0,1
42,2019-5-2,2019,5,2,1,bjj62w,Name of paper for encoding/representing XY coordinates in deep learning,https://www.reddit.com/r/MachineLearning/comments/bjj62w/name_of_paper_for_encodingrepresenting_xy/,pinouchon,1556728168,,0,1
43,2019-5-2,2019,5,2,1,bjjakm,Reproducing Images using a Genetic Algorithm with Python,https://www.reddit.com/r/MachineLearning/comments/bjjakm/reproducing_images_using_a_genetic_algorithm_with/,AhmedGadFCIT,1556728751,,0,1
44,2019-5-2,2019,5,2,1,bjjgro,[Discussion]Why CycleGAN does not need randomness as input?,https://www.reddit.com/r/MachineLearning/comments/bjjgro/discussionwhy_cyclegan_does_not_need_randomness/,sychen52,1556729602,"I understand that CycleGAN is trying to learn a deterministic mapping between the two domains. However, in the case of conditional GAN (cGAN), if there is no randomness in the generator, the discriminator will get overfitting, and consequently generator will not improve (as discussed in the following link).

[https://www.reddit.com/r/MachineLearning/comments/6k4sqj/discussion\_what\_is\_the\_purpose\_of\_noise\_in\_a/](https://www.reddit.com/r/MachineLearning/comments/6k4sqj/discussion_what_is_the_purpose_of_noise_in_a/)

Why the same overfitting problem does not cause trouble to CycleGAN? Is it between the input dataset to CycleGAN is versatile enough to prevent discriminator from overfitting?",3,3
45,2019-5-2,2019,5,2,1,bjjj27,Nvidia GPU's vs Radeon Instinct? I want to learn more.,https://www.reddit.com/r/MachineLearning/comments/bjjj27/nvidia_gpus_vs_radeon_instinct_i_want_to_learn/,SmugEskim0,1556729926,[removed],0,1
46,2019-5-2,2019,5,2,2,bjjm78,"To build a machine learning model initially you had 100 data points and 5 features. To reduce bias, you doubled the features to include 5 more variables and collected 100 more data points. Explain if this is a right approach? (Hint: There is a curse on machine learning. Have you heard about it?)",https://www.reddit.com/r/MachineLearning/comments/bjjm78/to_build_a_machine_learning_model_initially_you/,ml_coder_pro,1556730353,,0,1
47,2019-5-2,2019,5,2,2,bjk15a,[D] Optimizing CUDA Recurrent Neural Networks with TorchScript,https://www.reddit.com/r/MachineLearning/comments/bjk15a/d_optimizing_cuda_recurrent_neural_networks_with/,SkiddyX,1556732405,A short blog post explaining how the new PyTorch JIT can greatly speed up RNNs: https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/,0,18
48,2019-5-2,2019,5,2,3,bjkp0x,When you talk about AI ...,https://www.reddit.com/r/MachineLearning/comments/bjkp0x/when_you_talk_about_ai/,seemingly_omniscient,1556735568,"&amp;#x200B;

![img](4tinqvqc8nv21)

 [More Slides](https://www.aisoma.de/10-useful-ai-ml-slides/)",0,1
49,2019-5-2,2019,5,2,3,bjkq1h,[N] BoTorch: Bayesian Optimization in PyTorch,https://www.reddit.com/r/MachineLearning/comments/bjkq1h/n_botorch_bayesian_optimization_in_pytorch/,iyaja,1556735707,"At the F8 developer conference, Facebook announced a new open-source AI library for Bayesian optimization called BoTorch. BoTorch is built on PyTorch and can integrate with its neural network modules. It also supports GPUs and autograd. More info can be found here:

&amp;#x200B;

* Official site: [https://botorch.org](https://botorch.org)
* Github repo: [https://github.com/pytorch/botorch](https://github.com/pytorch/botorch)
* Techcruch article: [https://techcrunch.com/2019/05/01/facebook-open-sources-ax-and-botorch-to-simplify-ai-model-optimization/](https://techcrunch.com/2019/05/01/facebook-open-sources-ax-and-botorch-to-simplify-ai-model-optimization/)",20,237
50,2019-5-2,2019,5,2,3,bjkqga,Is Kaldi the best choice for application developers? What libraries are recommended for someone who is looking to learn and use ASR?,https://www.reddit.com/r/MachineLearning/comments/bjkqga/is_kaldi_the_best_choice_for_application/,samyoung2727,1556735767,[removed],0,1
51,2019-5-2,2019,5,2,3,bjkw6z,[P] OpenWebTextCorpus download released: Replication of GPT-2's Training Dataset,https://www.reddit.com/r/MachineLearning/comments/bjkw6z/p_openwebtextcorpus_download_released_replication/,Skylion007,1556736571,"[https://skylion007.github.io/OpenWebTextCorpus/](https://skylion007.github.io/OpenWebTextCorpus/)  


Today were announcing the release of a beta version of our Open WebText Corpus   an open source effort to reproduce OpenAIs WebText dataset, as detailed  [here](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf).  This distribution was created by Aaron Gokaslan and Vanya Cohen of  Brown University. The following post outlines the steps taken to  reproduce the dataset, and provides information for those seeking to contribute to its further development. 

&amp;#x200B;

We would like to thank the contributors of the OpenWebText project for their very useful scraping and data filtering scripts. After some experimentation, we were able to clean the number of documents until we had 38GB of text data (40GB using SI units) from 8,013,769 documents which matches the numbers listed in the paper. We hope that this dataset will allow for further research to build upon this valuable source of NLP data.",16,64
52,2019-5-2,2019,5,2,4,bjlj2e,Ensemble model not converging while single model converges [DISCUSSION],https://www.reddit.com/r/MachineLearning/comments/bjlj2e/ensemble_model_not_converging_while_single_model/,bakamito,1556739858,"I am using two Imagenet models and multiplying their outputs to create an ensemble model consisting of those two models.  


**My first attempt is using the same model, but with different images sizes as inputs.**

However this ensemble model doesn't seem to be converging. It gets stuck at 78% accuracy, no matter how many layers I unfreeze. While a single model is converging and over-fitting.   


Is there something wrong with my model setup or something you suggest I try? 

    def full_model_5(model_keys):
        """"""prediction x, y, multiply them , activation softmax""""""
    
        model_key, model_key2 = model_keys
    
        base_model, preprocess = basemodel(model_key)
        base_model2, preprocess2 = basemodel(model_key2)
    
        for layer in base_model.layers:   # all base_layers are not trainable
            layer.name = layer.name + ""dup""
            layer.trainable = False
    
        for layer in base_model2.layers: # all base_layers are not trainable
            layer.trainable = False
            
        # first model    
        x = base_model.output
        x = GlobalAveragePooling2D()(x)
        x = Dense(2048, activation='relu')(x)
        x = BatchNormalization()(x)
        x =  Dense(512, activation='relu')(x)
        x = BatchNormalization()(x)
        predictions_x = Dense(classes, activation='relu')(x)
    
        # second model
        y = base_model2.output
        y = GlobalAveragePooling2D()(y)
        y = Dense(2048, activation='relu')(y)
        y = BatchNormalization()(y)
        y =  Dense(512, activation='relu')(y)
        y = BatchNormalization()(y)
        predictions_y = Dense(classes, activation='relu')(y)
    
        # combine the models together
        predictions = Multiply()([predictions_x, predictions_y])
        predictions = Activation(""softmax"")(predictions)
        ##predictions = Maximum()([predictions_x, predictions_y])
        model = Model(inputs= [base_model.input,base_model2.input], outputs=predictions)
        return model,( preprocess,preprocess2)",10,1
53,2019-5-2,2019,5,2,4,bjlo30,Semantic segmentation for self-driving cars,https://www.reddit.com/r/MachineLearning/comments/bjlo30/semantic_segmentation_for_selfdriving_cars/,topeka50,1556740571,[removed],0,1
54,2019-5-2,2019,5,2,4,bjlona,Looking for best open-source speech2text algorithm with code and training data available,https://www.reddit.com/r/MachineLearning/comments/bjlona/looking_for_best_opensource_speech2text_algorithm/,po-handz,1556740649,[removed],0,1
55,2019-5-2,2019,5,2,5,bjlxyv,Databricks Koalas-Python Pandas for Spark,https://www.reddit.com/r/MachineLearning/comments/bjlxyv/databricks_koalaspython_pandas_for_spark/,akhilanandbv003,1556741995,[removed],0,1
56,2019-5-2,2019,5,2,5,bjlyn1,Anyone want to step through a VAE repo with me?,https://www.reddit.com/r/MachineLearning/comments/bjlyn1/anyone_want_to_step_through_a_vae_repo_with_me/,amgsharma,1556742092,[removed],0,1
57,2019-5-2,2019,5,2,5,bjm69p,Anomaly detection in foot traffic flow using location data from mobile devices?,https://www.reddit.com/r/MachineLearning/comments/bjm69p/anomaly_detection_in_foot_traffic_flow_using/,bandalorian,1556743213,[removed],0,1
58,2019-5-2,2019,5,2,5,bjmeio,[Project] How to use TTS every day and enjoy it,https://www.reddit.com/r/MachineLearning/comments/bjmeio/project_how_to_use_tts_every_day_and_enjoy_it/,gorbel_alex,1556744386,"Hi there! Me and my friends are working on the iOS app that voices any article from any source with the help of machine learning. The app is free and doesn't contain ads at all. So if this sounds interesting for you, please find more information here: [What is Peech app and how to use it](https://link.medium.com/tcb4zEiqlW)The link to download: [Peech](https://itunes.apple.com/us/app/peech-save-article-listen/id1429704005?mt=8). Would be really helpful to hear any thoughts, ideas or feedback from ya.",0,0
59,2019-5-2,2019,5,2,6,bjmiep,[R] StegaStamp: Turning Images into Barcodes!,https://www.reddit.com/r/MachineLearning/comments/bjmiep/r_stegastamp_turning_images_into_barcodes/,fieldsoflight,1556744903,"StegaStamp: Invisible Hyperlinks in Physical Photographs

Video: https://youtu.be/E8OqgNDBGO0

Project Page: http://www.matthewtancik.com/stegastamp",3,29
60,2019-5-2,2019,5,2,7,bjn698,CycleGAN does not work on a simple MNIST experiment,https://www.reddit.com/r/MachineLearning/comments/bjn698/cyclegan_does_not_work_on_a_simple_mnist/,laituan245,1556748318,[removed],0,1
61,2019-5-2,2019,5,2,7,bjn8cq,Great website for keeping track of state-of-the-art in Natural Language processing &amp; understanding,https://www.reddit.com/r/MachineLearning/comments/bjn8cq/great_website_for_keeping_track_of_stateoftheart/,no_bear_so_low,1556748638,,0,1
62,2019-5-2,2019,5,2,7,bjn95x,Question,https://www.reddit.com/r/MachineLearning/comments/bjn95x/question/,masterRJ2404,1556748768,[removed],0,1
63,2019-5-2,2019,5,2,7,bjnhiy,Doubt in understanding on results plotted from SVCCA,https://www.reddit.com/r/MachineLearning/comments/bjnhiy/doubt_in_understanding_on_results_plotted_from/,rv2712,1556750063,[removed],0,1
64,2019-5-2,2019,5,2,7,bjnlux,[Discussion] Cannot make CycleGAN work on a simple toy MNIST example,https://www.reddit.com/r/MachineLearning/comments/bjnlux/discussion_cannot_make_cyclegan_work_on_a_simple/,laituan245,1556750736,"Hi. I tried implementing CycleGAN from scratch. And I tested my implementation on a toy example where the first domain consists of normal MNIST images and the other domain consists of **flipped** MNIST images. I think the task sounds simple enough. However, while the learned translations are able to generate sharp (normal/flipped) MNIST images, the mappings are not correct at all (e.g., 3s are mapped to 7s or 9s ). Please refer to the figures below. Did anyone encounter the sample problem when using CycleGAN? I think overall my architecture is correct (having adversarial loss and cycle loss). There may be few differences in the generators and discriminators but I don't think it would make that much difference.

&amp;#x200B;

&amp;#x200B;

![img](iqq6khxdbov21 ""During the first few iterations, generated images are not clear. But it is fair enough"")

&amp;#x200B;

&amp;#x200B;

![img](wq7dyk4gbov21 ""After many iterations, generated images are clear. But the mappings are not correct. For example, normal 7 is not mapped to a flipped 7"")",4,1
65,2019-5-2,2019,5,2,8,bjnyl9,pc build for ML,https://www.reddit.com/r/MachineLearning/comments/bjnyl9/pc_build_for_ml/,infrequentable10,1556752761,[removed],0,1
66,2019-5-2,2019,5,2,8,bjo9bj,[D] GPU web host for approximately $150+ a month?,https://www.reddit.com/r/MachineLearning/comments/bjo9bj/d_gpu_web_host_for_approximately_150_a_month/,iluvcoder,1556754560,"I have created a cool experiment in tensorflow that takes input from the user and does something very cool!  


On my RTX 2080 Ti, the task in the web app completes in about 2 seconds. However, on my Linode (which does not have a GPU) the task takes nearly 1 minute!!  


Is there any web host similar to Linode where I can have a GPU?",9,2
67,2019-5-2,2019,5,2,9,bjokwt,[P] Monte Carlo Tree Search library in Python,https://www.reddit.com/r/MachineLearning/comments/bjokwt/p_monte_carlo_tree_search_library_in_python/,treeforface,1556756535,"Hi everyone,

While I was building an AlphaZero clone I had the opportunity to make a Python library for the Monte Carlo Tree Search algorithm that works both with an AI expert policy or without one. The existing Python libraries that I found were either too poorly functioning/documented or didn't have a clear mechanism for deriving the probabilistic exploratory weighting to assign to a child node prior to determining its predicted win value.

Please let me know if there are any issues or if I can help clarify anything. I hope someone might find this useful!

https://github.com/ImparaAI/monte-carlo-tree-search",0,26
68,2019-5-2,2019,5,2,9,bjoveh,[P] Predicting hand written words from imagery,https://www.reddit.com/r/MachineLearning/comments/bjoveh/p_predicting_hand_written_words_from_imagery/,bayesian_thought,1556758339,"Hello, 

I am looking for some help with a project. The goal of the project is to use EMNIST data to train a model for translation that extracts certain fields and translate documents into data. I can offer a reasonable rate when we can negotiate.

Contact me if you are interested",2,1
69,2019-5-2,2019,5,2,9,bjow2z,"Where is a good place to start for unsupervised image creation? (ie. dogs, faces, flowers, etc.)",https://www.reddit.com/r/MachineLearning/comments/bjow2z/where_is_a_good_place_to_start_for_unsupervised/,EnoDevz,1556758466,"I've been trying to figure out how to do this for a while now, however all the tools i find require either it to be made a video with some matrix of training data or I can't figure out how to add my own images to the data set. Is there a way I can create images and have the training set just be a folder where I put files into? 

BTW I'm trying to make the output look something like 3:50 of [this](https://www.youtube.com/watch?v=NTlXEJjfsQU) or 3:15 of [this](https://www.youtube.com/watch?v=4VAkrUNLKSo&amp;feature=youtu.be).

&amp;#x200B;

Thank you in advance!!",0,1
70,2019-5-2,2019,5,2,10,bjozts,Kid wins science fair by proving Tom Brady is a cheater,https://www.reddit.com/r/MachineLearning/comments/bjozts/kid_wins_science_fair_by_proving_tom_brady_is_a/,eslamalmahdy,1556759110,,0,1
71,2019-5-2,2019,5,2,10,bjp5mf,[Project] Expanding the benefits of Machines,https://www.reddit.com/r/MachineLearning/comments/bjp5mf/project_expanding_the_benefits_of_machines/,LeatherMarsupial,1556760103,"Machines is one of the hardest fields to study, from its capabilities up to its full potentials. In today's generation, we all know the risks of relying to those untrusted machines but according to what I've read about this project, TRIAS, there are some significant we can rely on like:

\&gt;&gt; Computers and the Internet significantly boost the efficiency of our work. However, untrusted applications can lead to data leaking or tampering.

\&gt;&gt; Mobile phones meet our basic daily communication and entertainment needs...You can read more here: [https://www.trias.one/whitepaper](https://www.trias.one/whitepaper)

&amp;#x200B;

Trusthworthy and reliable systems are what we need now. There are so many resources out there but all we need is an eclectic source that will help us to understand and feel the presence of intelligent machine like [TRIAS](https://www.kucoin.com/news/en-announcement-of-trias-try-token-sale-on-kucoin-spotlight)",2,0
72,2019-5-2,2019,5,2,10,bjphej,Artificial neural network/symmetry group landscape visualization [Page 5],https://www.reddit.com/r/MachineLearning/comments/bjphej/artificial_neural_networksymmetry_group_landscape/,ProgrammingGodJordan,1556762126,,2,0
73,2019-5-2,2019,5,2,11,bjpoh5,[N] DEFON u/highwiz censors discussion on Machine Learning Deepfakes Accessibility,https://www.reddit.com/r/MachineLearning/comments/bjpoh5/n_defon_uhighwiz_censors_discussion_on_machine/,Xeagu,1556763370,"Recently, villages from this year's DEFCON (27) have put together a Reddit AMA. I was volunteering to answer community questions related to the Monero village at this year's DEFCON. During discussion, I engaged u/aivillage in an inquiry on the progress and accessibility of deepfakes machine learning technology.

We had a brief discussion before both of my comments were deleted with no warning, notification, or explanation. I suspect the moderator in question was u/highwiz as he also engaged me in discussion elsewhere in the thread. 

Link to the [original thread](https://www.reddit.com/r/Defcon/comments/bjoczj/defcon_ama_lockpicking_ai_and_monero/)

Link to [specific tweet](https://twitter.com/realXeagu/status/1123762949146009602) with pictures.",2,0
74,2019-5-2,2019,5,2,11,bjpoqg,Questions on Learning to Reweight Examples for Robust Deep Learning,https://www.reddit.com/r/MachineLearning/comments/bjpoqg/questions_on_learning_to_reweight_examples_for/,NaNHunter,1556763417,[removed],0,1
75,2019-5-2,2019,5,2,11,bjpuey,"What machine learning problem has its objective in the form of f+h, where f is smooth but nonconvex and h is nonsmooth but convex?",https://www.reddit.com/r/MachineLearning/comments/bjpuey/what_machine_learning_problem_has_its_objective/,jliang76,1556764434,"Is there any real application of the following optimization problem:

&amp;#x200B;

minimize *f(x)+h(x)*, subject to *x* in *R**^(n)*,

&amp;#x200B;

where *f* is differentiable and possibly nonconvex and its gradient is Lipschitz continuous, *h* is a closed, convex and nonsmooth function with bounded domain.

&amp;#x200B;

I'm looking for real applications, such as problems in machine learning. Thanks in advance.",0,1
76,2019-5-2,2019,5,2,12,bjq3v6,[N] Using TensorBoard with PyTorch 1.1.0,https://www.reddit.com/r/MachineLearning/comments/bjq3v6/n_using_tensorboard_with_pytorch_110/,seungjaeryanlee,1556766155,,0,1
77,2019-5-2,2019,5,2,12,bjqebw,[D] Benchmark dataset for unevenly spaced time series data,https://www.reddit.com/r/MachineLearning/comments/bjqebw/d_benchmark_dataset_for_unevenly_spaced_time/,freechoice,1556768122,Is there a benchmark dataset (like MNIST) for unevenly spaced timeseries data? It would be good if it containes a lot of training points as I am interested in Deep Learning approach.,2,8
78,2019-5-2,2019,5,2,13,bjqwr8,[N] Udacity: Secure and Private AI,https://www.reddit.com/r/MachineLearning/comments/bjqwr8/n_udacity_secure_and_private_ai/,iyaja,1556771827,"Udacity announced a new free course in partnership with Facebook and Andrew Trask. Enrollments starts on May 30th. 

&amp;#x200B;

* Course page: [https://www.udacity.com/course/secure-and-private-ai--ud185](https://www.udacity.com/course/secure-and-private-ai--ud185)
* Medium post by Andrew Trask: [https://medium.com/udacity/introducing-udacitys-secure-private-ai-course-fe20bfa3b0ff](https://medium.com/udacity/introducing-udacitys-secure-private-ai-course-fe20bfa3b0ff)",1,2
79,2019-5-2,2019,5,2,14,bjr5kh,Is converting a book into an animation movie possible using machine learning?,https://www.reddit.com/r/MachineLearning/comments/bjr5kh/is_converting_a_book_into_an_animation_movie/,realshetty_01,1556773716,[removed],0,1
80,2019-5-2,2019,5,2,14,bjr74c,"We came up with an interesting way to use generative adversarial networks and blockchain together | www.painterDAO.com | We mint AI artwork on the blockchain, every day.",https://www.reddit.com/r/MachineLearning/comments/bjr74c/we_came_up_with_an_interesting_way_to_use/,nextart-io,1556774057,,0,1
81,2019-5-2,2019,5,2,14,bjrimz,ML Model Validation Services,https://www.reddit.com/r/MachineLearning/comments/bjrimz/ml_model_validation_services/,trainingdata,1556776755,,0,1
82,2019-5-2,2019,5,2,15,bjrp7d,Telescopic Forks for Automated Warehouse AS/RS | Automated AS/RS Systems,https://www.reddit.com/r/MachineLearning/comments/bjrp7d/telescopic_forks_for_automated_warehouse_asrs/,lhd121,1556778270,,0,1
83,2019-5-2,2019,5,2,15,bjrvb4,ResNet,https://www.reddit.com/r/MachineLearning/comments/bjrvb4/resnet/,21_kishan,1556779753,[removed],0,1
84,2019-5-2,2019,5,2,16,bjs27v,Understanding Random Forest from the ground up in Julia,https://www.reddit.com/r/MachineLearning/comments/bjs27v/understanding_random_forest_from_the_ground_up_in/,opensourcesblog,1556781449,,0,1
85,2019-5-2,2019,5,2,16,bjs7al,Deep architectures for pixelwise classification of black and white images,https://www.reddit.com/r/MachineLearning/comments/bjs7al/deep_architectures_for_pixelwise_classification/,formcheckc,1556782748,[removed],0,1
86,2019-5-2,2019,5,2,16,bjs8kn,[R] Google Cloud TTS - how to monitor characters already used in the free quota?,https://www.reddit.com/r/MachineLearning/comments/bjs8kn/r_google_cloud_tts_how_to_monitor_characters/,MorganCheeseBurger,1556783088,"Crosspost from r/tasker

I read in this tasker [document](https://tasker.joaoapps.com/userguide/en/help/ah_say_wavenet.html) that there is a free quota in the Google Cloud TTS API. I wonder though, before creating Tasker projects that uses Say WaveNet -- how will I be able to monitor the already used characters?

&amp;#x200B;

Also, a sub-question to confirm: does the free quota reset monthly? Thank you.",0,0
87,2019-5-2,2019,5,2,18,bjsppj,IBM Data Science Professional Certificate on Coursera,https://www.reddit.com/r/MachineLearning/comments/bjsppj/ibm_data_science_professional_certificate_on/,skilluponline,1556787619,"&amp;#x200B;

![video](tav9zz60frv21)

Data Science has been ranked as one of the hottest professions and the demand for data practitioners is booming. This [Professional Certificate from IBM](https://www.coursera.org/specializations/ibm-data-science-professional-certificate) is intended for anyone interested in developing skills and experience to pursue a career in Data Science or Machine Learning.  

This program consists of 9 courses providing you with latest job-ready skills and techniques covering a wide array of data science topics including: open source tools and libraries, methodologies, Python, databases, SQL, data visualization, data analysis, and machine learning. You will practice hands-on in the IBM Cloud using real data science tools and real-world data sets.  It is a myth that to become a data scientist you need a Ph.D. 

This Professional Certificate is suitable for anyone who has some computer skills and a passion for self-learning. No prior computer science or programming knowledge is necessary. We start small, re-enforce applied learning, and build up to more complex topics.  Upon successfully completing these courses you will have done several hands-on assignments and built a portfolio of data science projects to provide you with the confidence to plunge into an exciting profession in Data Science. In addition to earning a Professional Certificate from Coursera, you will also receive a digital Badge from IBM recognizing your proficiency in Data Science. 

[**IBM Data Science Professional Certificate on Coursera - Enroll Now &amp; Get Ready for The Future**](https://www.coursera.org/specializations/ibm-data-science-professional-certificate)",0,1
88,2019-5-2,2019,5,2,18,bjss67,[R] Similarity of Neural Network Representations Revisited,https://www.reddit.com/r/MachineLearning/comments/bjss67/r_similarity_of_neural_network_representations/,xternalz,1556788219,,3,6
89,2019-5-2,2019,5,2,18,bjssfs,[D] VAE with GAN-like quality,https://www.reddit.com/r/MachineLearning/comments/bjssfs/d_vae_with_ganlike_quality/,tsauri,1556788277,"Are we there yet? Which VAE (or AE) is competitive with GAN? I lost track of papers.  
I need decent autoencoder with okay-ish disentanglement but can decode back to source with GAN-like quality. Vanilla VAE is too blurry for my need.",28,28
90,2019-5-2,2019,5,2,18,bjsvz5,Augmented Analytics Platforms Can Encourage Business Growth!,https://www.reddit.com/r/MachineLearning/comments/bjsvz5/augmented_analytics_platforms_can_encourage/,ElegantMicroWebIndia,1556789147,,0,1
91,2019-5-2,2019,5,2,18,bjszua,[R] Why Git and Git-LFS is not enough to solve the Machine Learning Reproducibility crisis - TowardsDataScience,https://www.reddit.com/r/MachineLearning/comments/bjszua/r_why_git_and_gitlfs_is_not_enough_to_solve_the/,thumbsdrivesmecrazy,1556790052,"Keeping the data under version control with Git-LFS is a big improvement. But the lack of version control of the data files is not the entire problem.

The determining factors for the results of training a model or other activities include the following:

* Training datathe image database or whatever data source is used in training the model
* The scripts used in training the model
* The libraries used by the training scripts
* The scripts used in processing data
* The libraries or other tools used in processing data
* The operating system and CPU/GPU hardware
* Production system code
* Libraries used by production system code

Obviously the result of training a model depends on a variety of conditions. Since there are so many variables to this, it is hard to be precise, but the general problem is a lack of whats now called Configuration Management.

[DVC](https://dvc.org/) takes on and solves a larger slice of the machine learning reproducibility problem than does Git-LFS or several other potential solutions:

[DVC workflow - code &amp; data](https://i.redd.it/mbl8s0s1qrv21.png)

Full article: [Why Git and Git-LFS is not enough to solve the Machine Learning Reproducibility crisis](https://towardsdatascience.com/why-git-and-git-lfs-is-not-enough-to-solve-the-machine-learning-reproducibility-crisis-f733b49e96e8)",41,128
92,2019-5-2,2019,5,2,18,bjt0lc,[P] A Pytorch Implementation of Attentive-Neural-Process,https://www.reddit.com/r/MachineLearning/comments/bjt0lc/p_a_pytorch_implementation_of/,soobinseo,1556790239,"Attentive Neural Process (https://arxiv.org/abs/1901.05761) 
The code was well implemented by the author, but there is no pytorch codes (Only Tensorflow), so I've implemented with pytorch. Any comments wil be helpful to me.

https://github.com/soobinseo/Attentive-Neural-Process",0,7
93,2019-5-2,2019,5,2,18,bjt2zl,[D] Funny comment in GLOW TeX File,https://www.reddit.com/r/MachineLearning/comments/bjt2zl/d_funny_comment_in_glow_tex_file/,TheBillsFly,1556790794,"I was wondering how the GLOW paper (https://arxiv.org/abs/1807.03039) got that nice, large figure on its front page in the NeurIPS format. Thus, I downloaded the TeX source file from arXiv. I found something quite amusing in the comments of the TeX file though, on line 182 of ms.tex:

""- Add cool-looking figure of generated faced to lure the reader.""

Anybody else think this is quite funny? It is like we, as readers, are being given such a nice little treat (the figure) to initially grab our attention.

It also can serve as a reminder to sanitize your TeX source files before sharing them on arXiv.",2,0
94,2019-5-2,2019,5,2,18,bjt49b,10 Best Machine Learning Courses in 2019,https://www.reddit.com/r/MachineLearning/comments/bjt49b/10_best_machine_learning_courses_in_2019/,ravisaive,1556791084,,0,1
95,2019-5-2,2019,5,2,19,bjt8pe,Which algorithm suits best to your business problem? Have a look at the image below,https://www.reddit.com/r/MachineLearning/comments/bjt8pe/which_algorithm_suits_best_to_your_business/,rednivaralat,1556792044,[removed],0,1
96,2019-5-2,2019,5,2,19,bjt9gf,Funding for summer school?,https://www.reddit.com/r/MachineLearning/comments/bjt9gf/funding_for_summer_school/,academic_wandrer,1556792212,[removed],0,1
97,2019-5-2,2019,5,2,19,bjtbl6,If you ever wondered what a Deep Learning workshop looked like in 2009,https://www.reddit.com/r/MachineLearning/comments/bjtbl6/if_you_ever_wondered_what_a_deep_learning/,___felix,1556792676,,0,1
98,2019-5-2,2019,5,2,19,bjtee2,[D] If you ever wondered what a Deep Learning workshop looked like in 2009,https://www.reddit.com/r/MachineLearning/comments/bjtee2/d_if_you_ever_wondered_what_a_deep_learning/,___felix,1556793284,,0,1
99,2019-5-2,2019,5,2,21,bjue7g,Help me find my niche lol (hs gap year),https://www.reddit.com/r/MachineLearning/comments/bjue7g/help_me_find_my_niche_lol_hs_gap_year/,mllnmchld,1556800278,[removed],0,1
100,2019-5-2,2019,5,2,21,bjueat,Learning by Observing via Inverse Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/bjueat/learning_by_observing_via_inverse_reinforcement/,shanemcgrawspm,1556800296,,0,1
101,2019-5-2,2019,5,2,21,bjunks,Artificial intelligence development company in india,https://www.reddit.com/r/MachineLearning/comments/bjunks/artificial_intelligence_development_company_in/,clarke2106,1556801970,"Rajasri Systems is one of the [Best Artificial Intelligence development company](http://www.rajasri.com/ai-software-development.php). Artificial Intelligence (AI) is the advance and inspirational source of an interaction of the human with machines and devices. Consistently working with an intent to make amazing and canny arrangements that are incorporated with the top of the line #ArtificialIntelligence.

For More Details: [artificial intelligence software development company](http://www.rajasri.com/ai-software-development.php)

&amp;#x200B;

[Artificial Intelligence development company in india](https://i.redd.it/t586ue6kpsv21.png)

\#AI #AICOMPANY #ArtificialIntelligence #ArtificialIntelligencecompany #AI\_DEVELOPMENT\_COMPANY",0,1
102,2019-5-2,2019,5,2,22,bjutb7,Building recommender systems with Azure ML service: best practices on deployment and a collection of reco algorithms,https://www.reddit.com/r/MachineLearning/comments/bjutb7/building_recommender_systems_with_azure_ml/,hoaphumanoid,1556802916,,0,1
103,2019-5-2,2019,5,2,22,bjv0i1,How I created an evolving neural network ecosystem [OC],https://www.reddit.com/r/MachineLearning/comments/bjv0i1/how_i_created_an_evolving_neural_network/,Naotagrey,1556804092,,1,1
104,2019-5-2,2019,5,2,22,bjv953,[Research] How to Classify Graphs using Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bjv953/research_how_to_classify_graphs_using_machine/,cdossman,1556805512," [https://medium.com/ai%C2%B3-theory-practice-business/how-to-classify-graphs-using-machine-learning-5b0813423734](https://medium.com/ai%C2%B3-theory-practice-business/how-to-classify-graphs-using-machine-learning-5b0813423734) 

Abstract:  Graph classification is a difficult problem that has drawn a lot of attention from the machine learning community over the past few years. This is mainly due to the fact that, contrarily to Euclidean vectors, the inherent complexity of graph structures can be quite hard to encode and handle for traditional classifiers. Even though kernels have been proposed in the literature, the increase in the dataset sizes has greatly limited the use of kernel methods since computation and storage of kernel matrices has become impracticable. In this article, we propose to use extended persistence diagrams to efficiently encode graph structure. More precisely, we show that using the so-called heat kernel signatures for the computation of these extended persistence diagrams allows one to quickly and efficiently summarize the graph structure. Then, we build on the recent development of neural networks for point clouds to define an architecture for (extended) persistence diagrams which is modular and easy-to-use. Finally, we demonstrate the usefulness of our approach by validating our architecture on several graph datasets, on which the obtained results are comparable to the state-of-the-art for graph classification",0,5
105,2019-5-2,2019,5,2,23,bjvlp5,"Tensorflow implementation of ""Handwritten Indic Character Recognition using Capsule Networks"" [ASPCON 2018]",https://www.reddit.com/r/MachineLearning/comments/bjvlp5/tensorflow_implementation_of_handwritten_indic/,CodeMusicFreak,1556807415,[removed],0,1
106,2019-5-2,2019,5,2,23,bjvns7,Role of AI &amp; Machine Learning in Pharmaceuticals | ADI Intellect,https://www.reddit.com/r/MachineLearning/comments/bjvns7/role_of_ai_machine_learning_in_pharmaceuticals/,devenderk,1556807730,,0,1
107,2019-5-2,2019,5,2,23,bjvpu3,"[Project] Tensorflow implementation of ""Handwritten Indic Character Recognition using Capsule Networks"" [ASPCON 2018]",https://www.reddit.com/r/MachineLearning/comments/bjvpu3/project_tensorflow_implementation_of_handwritten/,CodeMusicFreak,1556808028,"Would like to receive comments to make this better :)

Link: [https://github.com/prabhuomkar/hicr-capsnet](https://github.com/prabhuomkar/hicr-capsnet)",2,4
108,2019-5-2,2019,5,2,23,bjvwr3,Metrics for similar classes classification,https://www.reddit.com/r/MachineLearning/comments/bjvwr3/metrics_for_similar_classes_classification/,matej1408,1556809031,[removed],0,2
109,2019-5-3,2019,5,3,0,bjvz5m,graph points as features?,https://www.reddit.com/r/MachineLearning/comments/bjvz5m/graph_points_as_features/,Anunoby3,1556809373,[removed],0,1
110,2019-5-3,2019,5,3,0,bjw7hz,"""[Discussion]"" Classifying factors of large data set",https://www.reddit.com/r/MachineLearning/comments/bjw7hz/discussion_classifying_factors_of_large_data_set/,statsaccount,1556810607,"Hi everyone. I have been presented with a problem and I'm hoping someone here could provide some advice or a direction in which to look. 

The problem is this: Given a large data set with, say, 10,000 columns (so 10,000 variables or factors), classify some variables as type A and the others as type B. 

More specifically, the data set contains customer data, and some of the variables are personal information such as customer address, SSN, etc. and need to be classified as private. Since there are so many variables, one cannot simply identify the private ones and mark them as private/not private. The process needs to be automated, and we have training sets that are already classified that could be used for training a model to recognize private variables in future, unmarked datasets.

My problem is that I do not know what machine learning techniques are appropriate for this type of classification task. My understanding is that typically classification methods will classify the record value (row variable, in this case a customer) according to the values of the variable. My problem seems to be the inverse.

Thank you in advance for any comments or advice.",0,1
111,2019-5-3,2019,5,3,0,bjw7xj,Help the City of Detroit verify imagery extracts,https://www.reddit.com/r/MachineLearning/comments/bjw7xj/help_the_city_of_detroit_verify_imagery_extracts/,dexterslu,1556810667,[removed],0,1
112,2019-5-3,2019,5,3,0,bjwbrs,[Discussion] Problem: Classify variables in large data set,https://www.reddit.com/r/MachineLearning/comments/bjwbrs/discussion_problem_classify_variables_in_large/,statsaccount,1556811212,"Hi everyone. I have been presented with a problem and I'm hoping someone here could provide some advice or a direction in which to look.

The problem is this: Given a large data set with, say, 10,000 columns (so 10,000 variables or factors), classify some variables as type A and the others as type B.

More specifically, the data set contains customer data, and some of the variables are personal information such as customer address, SSN, etc. and need to be classified as private. Since there are so many variables, one cannot simply identify the private ones and mark them as private/not private. The process needs to be automated, and we have training sets that are already classified that could be used for training a model to recognize private variables in future, unmarked datasets.

My problem is that I do not know what machine learning techniques are appropriate for this type of classification task. My understanding is that typically classification methods will classify the record value (row variable, in this case a customer) according to the values of the variable. My problem seems to be the inverse.

Furthermore, it would be interesting to be able to classify each private variable that appears in the future data set. For example, if one column contains SSNs, can we identify it and mark the column correctly?

Thank you in advance for any comments or advice.",6,3
113,2019-5-3,2019,5,3,0,bjwd6o,[P] Looking for Kaggle/Project Buddies,https://www.reddit.com/r/MachineLearning/comments/bjwd6o/p_looking_for_kaggleproject_buddies/,csjobseeker1,1556811420,"Hey all,

&amp;#x200B;

Not sure if this is the best place to post this, but I am looking to enhance my portfolio and ML skills more importantly through Kaggle competitions and side projects. 

&amp;#x200B;

About me:

recent cornell grad with coursework experience in ML, did an internship dealing with NLP and CV applications. 

currently following along with cs231 and 230 to improve my deep learning skills. 

&amp;#x200B;

As I mentioned, looking for like minded individuals to partake in side projects or kaggle contests with!",14,14
114,2019-5-3,2019,5,3,0,bjwef4,Derisking machine learning and artificial intelligence,https://www.reddit.com/r/MachineLearning/comments/bjwef4/derisking_machine_learning_and_artificial/,jonfla,1556811607,,0,1
115,2019-5-3,2019,5,3,1,bjwr74,What is SOA For Multi-source Time Series,https://www.reddit.com/r/MachineLearning/comments/bjwr74/what_is_soa_for_multisource_time_series/,Clssq,1556813457,"Very confused undergrad here preparing for a research position. My problem involves forecasting floats using a combination of correlated floats, vectors and seasonal spacial-temporal data. As I understand it, this problem breaks down into two parts: **dimensional reduction** and **forecasting**. I'm speedreading literature to get up to speed but more confused now than when I started. I'd be grateful for any pointers in the right direction.

&amp;#x200B;

**Dimensional Reduction:**

From my understanding, the SOTA for dimensional reduction would be some type of autoencoder,  maybe a VAE or Transformer. I'm having trouble understanding what's the best.

&amp;#x200B;

**Forecasting:**

In terms of predicting time series in general, [some people](https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0) tend to say that RNNs are no longer effective, while [others](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) swear by them. I'm screwing around myself but I don't have access to GPUs yet and I've gotten mixed results.",0,1
116,2019-5-3,2019,5,3,1,bjwu0h,[D] lMetrics for similar classes classification,https://www.reddit.com/r/MachineLearning/comments/bjwu0h/d_lmetrics_for_similar_classes_classification/,matej1408,1556813864,"Hello,

I am having trouble finding right metrics for my problem.

I am predicting tags from text (multiclass classification) and I need to come up with metric that allows me to evaluate models.

I am taking top n classes with biggest probability as model output.
I can't monitor just precision/recall/f1 because I have similar classes. For example lets say that text X have classes [ocean, fish, boat] and my predictions are [sea, shark, boat]. I would say that this is correctly classified because sea and ocean are very similar, same for shark and fish. 
I am currently using word2vec and taking mean cos similarity between each prediction and label which gives maximum cos with given prediction. The problem with this metric is  (I suspect) that its value decrease as I increase n (top predictions). 

I think that good metrics would be something that combine cos metrics with precision.

Thanks",2,2
117,2019-5-3,2019,5,3,1,bjwx2u,[R] [Code/Paper] Towards Metamerism via Foveated Style Transfer (ICLR 2019),https://www.reddit.com/r/MachineLearning/comments/bjwx2u/r_codepaper_towards_metamerism_via_foveated_style/,NeuroSurfer31,1556814307,"Link to the GitHub code: [https://github.com/ArturoDeza/NeuroFovea](https://github.com/ArturoDeza/NeuroFovea)

Link to the paper: [https://openreview.net/forum?id=BJzbG20cFQ](https://openreview.net/forum?id=BJzbG20cFQ)

&amp;#x200B;

&amp;#x200B;

*Processing gif d2h7pr98qtv21...*",1,3
118,2019-5-3,2019,5,3,1,bjx00o,"Reinforcement learning, fast and slow (review in TICS)",https://www.reddit.com/r/MachineLearning/comments/bjx00o/reinforcement_learning_fast_and_slow_review_in/,JaneXWang,1556814732,[removed],0,1
119,2019-5-3,2019,5,3,1,bjx2fu,codewrestling youtube for machine learning,https://www.reddit.com/r/MachineLearning/comments/bjx2fu/codewrestling_youtube_for_machine_learning/,ml_coder_pro,1556815073,,0,1
120,2019-5-3,2019,5,3,1,bjx4eh,[D] Deep Network Architectures,https://www.reddit.com/r/MachineLearning/comments/bjx4eh/d_deep_network_architectures/,kalakesri,1556815357,"I've recently started looking at Deep Learning methods (specifically LSTM) for doing some time series analysis. While I understand the concepts involved in the network in isolation, one thing that confuses me is that how do we derive the architectures that work for a task.

I did some research into this and an answer that usually comes up is to dig into the literature and see what architect similar experiments used and start from there. My question is that is there a qualitative metric or a method that guides us for creating optimal architectures without using the layers as Lego bricks and connecting them hoping to get the best results.",2,1
121,2019-5-3,2019,5,3,1,bjx5aa,Just a simple query,https://www.reddit.com/r/MachineLearning/comments/bjx5aa/just_a_simple_query/,helpnhelper,1556815487,[removed],0,1
122,2019-5-3,2019,5,3,2,bjxc4f,Blockchain and AI - A Match Made in Heaven. BlockDelta,https://www.reddit.com/r/MachineLearning/comments/bjxc4f/blockchain_and_ai_a_match_made_in_heaven/,BlockDelta,1556816438,,0,1
123,2019-5-3,2019,5,3,2,bjxdek,[D] How would one detect data leakage in someone else's model?,https://www.reddit.com/r/MachineLearning/comments/bjxdek/d_how_would_one_detect_data_leakage_in_someone/,CrazyAsparagus,1556816616,"Pure hypothetical.  Let's say I have someone's model (i.e. their final model weights) and also their train and test set.  I don't have any additional validation readily available.  

&amp;#x200B;

What kind of heuristics can use to evaluate if there was data leakage from the test set?  

&amp;#x200B;

I'd like to distinguish the two cases 1) there is data leakage, 2) the model is really good.  Based on  just performance metrics on the test/train set, I dont feel like I can distinguish these two cases.  Would it be impossible to tell without additional validation data?",8,6
124,2019-5-3,2019,5,3,2,bjxirg,Best technique for anomaly detection in Anti-Money Laundering,https://www.reddit.com/r/MachineLearning/comments/bjxirg/best_technique_for_anomaly_detection_in_antimoney/,waleed_soliman,1556817371,[removed],0,1
125,2019-5-3,2019,5,3,2,bjxlqf,"[D] Reinforcement learning, fast and slow",https://www.reddit.com/r/MachineLearning/comments/bjxlqf/d_reinforcement_learning_fast_and_slow/,JaneXWang,1556817797,"Hi everyone. I work on the neuroscience team at DeepMind. Weve just published a new paper Reinforcement learning, fast and slow"" that reviews new techniques in deep reinforcement learning aiming to close the gap in the learning speed between humans and AI. Specifically, we look at how approaches like episodic deep RL and meta-reinforcement learning could unlock greater understanding in psychology and neuroscience by investigating the connection between fast and slow forms of deep RL.

If you're interested in how AI and neuroscience can intersect, you can read the full paper [here](https://www.cell.com/action/showPdf?pii=S1364-6613%2819%2930061-0) (available open access)  let us know what you think!",60,332
126,2019-5-3,2019,5,3,2,bjxrlq,The Discipline of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bjxrlq/the_discipline_of_machine_learning/,aiforworld2,1556818661,,0,10
127,2019-5-3,2019,5,3,2,bjxszc,[R] [1904.13132] Surprising Effectiveness of Few-Image Unsupervised Feature Learning,https://www.reddit.com/r/MachineLearning/comments/bjxszc/r_190413132_surprising_effectiveness_of_fewimage/,bobchennan,1556818868,,2,27
128,2019-5-3,2019,5,3,2,bjxwmz,CodeWrestling youtube for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bjxwmz/codewrestling_youtube_for_machine_learning/,ml_coder_pro,1556819403,,0,1
129,2019-5-3,2019,5,3,3,bjy71p,AUTOENCODERS ESPAOL PYHTON - REDES NEURONALES - INTELIGENCIA ARTIFICIA...,https://www.reddit.com/r/MachineLearning/comments/bjy71p/autoencoders_espaol_pyhton_redes_neuronales/,Xpikuos,1556820907,[removed],0,1
130,2019-5-3,2019,5,3,3,bjy815,Keypoint Detection at the EDGE of an object?,https://www.reddit.com/r/MachineLearning/comments/bjy815/keypoint_detection_at_the_edge_of_an_object/,getsoned,1556821056,[removed],0,1
131,2019-5-3,2019,5,3,3,bjyc0a,Eclipse DL4J 1.0.0-beta4 release notes,https://www.reddit.com/r/MachineLearning/comments/bjyc0a/eclipse_dl4j_100beta4_release_notes/,vonnik,1556821626,[removed],0,1
132,2019-5-3,2019,5,3,3,bjyfg0,Argmax from probability distribution better policy than sampling from softmax ?,https://www.reddit.com/r/MachineLearning/comments/bjyfg0/argmax_from_probability_distribution_better/,samistark86,1556822124,"I am trying to train Echo State Network for text generation with stochastic optimization using one of the Reinforcement approach, where the optimization depends on the reward signal.
I have observed that during evaluation, when I sample from the probability distribution, the bleu score is greater. But when I argmax from the distribution, the score is less. 
I am not sure why does that happen.
Help needed.",0,1
133,2019-5-3,2019,5,3,3,bjyl1x,Are we renaming Unsupervised Learning to Self-Supervised Learning?,https://www.reddit.com/r/MachineLearning/comments/bjyl1x/are_we_renaming_unsupervised_learning_to/,machinesaredumb,1556822957,[removed],0,1
134,2019-5-3,2019,5,3,3,bjynys,[D] Are we renaming Unsupervised Learning to Self-Supervised Learning?,https://www.reddit.com/r/MachineLearning/comments/bjynys/d_are_we_renaming_unsupervised_learning_to/,machinesaredumb,1556823393,"I see Yan Lecunn et al using the phrase self-supervision a lot^(\[1\]). Isn't this just a new name for unsupervised learning?

\[1\] [https://ai.facebook.com/blog/advances-in-content-understanding-self-supervision-to-protect-people/](https://ai.facebook.com/blog/advances-in-content-understanding-self-supervision-to-protect-people/)",31,15
135,2019-5-3,2019,5,3,3,bjyp0x,[1904.02818] Neural Networks for Modeling Source Code Edits,https://www.reddit.com/r/MachineLearning/comments/bjyp0x/190402818_neural_networks_for_modeling_source/,_quanttrader_,1556823557,,0,1
136,2019-5-3,2019,5,3,4,bjz328,A short list of notable sessions for Microsoft Build 2019,https://www.reddit.com/r/MachineLearning/comments/bjz328/a_short_list_of_notable_sessions_for_microsoft/,Yuqing7,1556825616,,0,1
137,2019-5-3,2019,5,3,5,bjziy7,Common Design Problems on Custom Data Models,https://www.reddit.com/r/MachineLearning/comments/bjziy7/common_design_problems_on_custom_data_models/,formatlar,1556827962,[removed],0,1
138,2019-5-3,2019,5,3,5,bjzs3h,Did somebody tried to exploit enforcing Golden Ratio on DeepDream/Style Transfer/other generative approaches ?,https://www.reddit.com/r/MachineLearning/comments/bjzs3h/did_somebody_tried_to_exploit_enforcing_golden/,ad48hp,1556829272,[removed],0,1
139,2019-5-3,2019,5,3,5,bjzw6w,GANS generating realistic bodies now. Do we have any pre-trained models/code out there for this?,https://www.reddit.com/r/MachineLearning/comments/bjzw6w/gans_generating_realistic_bodies_now_do_we_have/,alvisanovari,1556829878,,0,1
140,2019-5-3,2019,5,3,8,bk1h6b,[D] nn2vec,https://www.reddit.com/r/MachineLearning/comments/bk1h6b/d_nn2vec/,AsIAm,1556838565,"How to encode the whole neural net into a single vector from which it can be recreated? I am trying to think about the problem of how to simulate one neural net on another. As UTM can simulate any TM, there should be a way to translate this concept to differentiable programming. Adversarial reprogramming might be an attack on this, but the injected task must encode the weights of the simulated neural net and and algorithm of how to use the weights. This is a problem because there is just too much data to feed in. The best scenario would be that there is UNN that can simulate all  networks that are ""smaller"" (by the description length of the simulated net). Also, I don't think there can be a perfect simulation  UNN can probably compute the hosted NN only as an approximation of the real one. Note: The edibles are too strong. Anyway, does somebody have any idea how to make any NN force to compute ""(+ 1 1)""?",0,0
141,2019-5-3,2019,5,3,8,bk1o1e,[R] Evolving Images for Visual Neurons Using a Deep Generative Network Reveals Coding Principles and Neuronal Preferences,https://www.reddit.com/r/MachineLearning/comments/bk1o1e/r_evolving_images_for_visual_neurons_using_a_deep/,FrontierKodiak,1556839720,,1,1
142,2019-5-3,2019,5,3,9,bk22bj,How does labeled data work in speech to text?,https://www.reddit.com/r/MachineLearning/comments/bk22bj/how_does_labeled_data_work_in_speech_to_text/,Moondra2017,1556842199,"It seems that we need voice and transcripts for speech to text.   
The voice is broken up into 10 milliseconds each, with each representing a data point, which is enough for a small utterance.  
However, how is the transcript broken up?  
Do you need to manually match each utterance with a letter of a couple of letters?",0,1
143,2019-5-3,2019,5,3,9,bk2395,[Research] Stochastic Computing for Low-Power and High-Speed Deep Learning on FPGA (ISCAS 2019),https://www.reddit.com/r/MachineLearning/comments/bk2395/research_stochastic_computing_for_lowpower_and/,coreylammie,1556842338,,0,1
144,2019-5-3,2019,5,3,9,bk2709,[R] OpenAI releases a generalization challenge for RL,https://www.reddit.com/r/MachineLearning/comments/bk2709/r_openai_releases_a_generalization_challenge_for/,downtownslim,1556842991,"&gt;RL agents are still benchmarked by evaluating on the environments they trained on. This would be like testing on your training set in supervisedlearning!

[https://openai.com/blog/quantifying-generalization-in-reinforcement-learning/](https://openai.com/blog/quantifying-generalization-in-reinforcement-learning/)",5,14
145,2019-5-3,2019,5,3,9,bk289h,Opportunity: Senior Developer Evangelist at Automation Anywhere,https://www.reddit.com/r/MachineLearning/comments/bk289h/opportunity_senior_developer_evangelist_at/,BDevEx,1556843223,[removed],0,1
146,2019-5-3,2019,5,3,9,bk2a02,"[Python, Pandas, Scikit] Label Encoder: Unseen labels",https://www.reddit.com/r/MachineLearning/comments/bk2a02/python_pandas_scikit_label_encoder_unseen_labels/,_eclesio_,1556843532,"I'm trying to do some data normalization with sklearn.preprocessing LabelEncoder library but some errors are happening. The error message will be bellow the code. I'm newbie with python and ML and i don't have so much knowledge about this, could someone help me?

Example Code

`cars_data = pd.read_csv('../input/cars _price_data.csv')`

`features=['symboling',  'make', 'wheel-base','highway-mpg']`

`X=cars_data[features].copy()`

`X_val, X_train, y_val, y_train = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=0)`

&amp;#x200B;

`#testing Categorical Variables Approach #2`

`from sklearn.preprocessing import LabelEncoder`

&amp;#x200B;

`# Make copy to avoid changing original data` 

`label_X_train = X_train.copy()`

`label_X_valid = X_val.copy()`

`print(label_X_valid)`

&amp;#x200B;

`#getting the categorical variables`

`cat_vars = (X.dtypes == 'object')`

`cat_cols = list(cat_vars[cat_vars].index)`

&amp;#x200B;

`# Apply label encoder to each column with categorical data`

`label_encoder = LabelEncoder()`

`for col in cat_cols:`

`label_X_train[col] = label_encoder.fit_transform(X_train[col])`

`label_X_valid[col] = label_encoder.transform(X_val[col])`

Error Message: **ValueError: y contains previously unseen labels: 'mercedes-benz'**",0,1
147,2019-5-3,2019,5,3,9,bk2ffp,Implement reinforcement learning with pure tensor style (without feeding) in tensorflow?,https://www.reddit.com/r/MachineLearning/comments/bk2ffp/implement_reinforcement_learning_with_pure_tensor/,fredericgo,1556844494,[removed],0,1
148,2019-5-3,2019,5,3,10,bk2skr,Artificial Intelligence: Mankind's Last Invention - what we know [10:37],https://www.reddit.com/r/MachineLearning/comments/bk2skr/artificial_intelligence_mankinds_last_invention/,sherozekhan,1556846805,,0,1
149,2019-5-3,2019,5,3,10,bk2syd,Jerusalem Jewelry - Moriah Stone Jewelry from Temple Mount,https://www.reddit.com/r/MachineLearning/comments/bk2syd/jerusalem_jewelry_moriah_stone_jewelry_from/,aletamittonbue,1556846870,,0,1
150,2019-5-3,2019,5,3,10,bk2uzd,Cool title for school of AI fellowship,https://www.reddit.com/r/MachineLearning/comments/bk2uzd/cool_title_for_school_of_ai_fellowship/,lomiag,1556847228,[removed],0,1
151,2019-5-3,2019,5,3,12,bk3vta,Selling GPU power for machine learning.,https://www.reddit.com/r/MachineLearning/comments/bk3vta/selling_gpu_power_for_machine_learning/,sambit97,1556853997,[removed],0,1
152,2019-5-3,2019,5,3,12,bk3zgk,"Neural Path Planning: Fixed Time, Near-Optimal Path Generation via Oracle Imitation",https://www.reddit.com/r/MachineLearning/comments/bk3zgk/neural_path_planning_fixed_time_nearoptimal_path/,iyaja,1556854719,,2,32
153,2019-5-3,2019,5,3,12,bk417z,[D] Microsoft launches new Drag and Drop Machine Learning tool,https://www.reddit.com/r/MachineLearning/comments/bk417z/d_microsoft_launches_new_drag_and_drop_machine/,stronomia,1556855088,"[Techcrunch Link](https://techcrunch.com/2019/05/02/microsoft-launches-a-drag-and-drop-machine-learning-tool-and-hosted-jupyter-notebooks/)

&amp;#x200B;

What are your thoughts? Seems like very basic functions provided by ML and DL frameworks have just been delivered in a drag and drop interface.

Additionally would be very interested to know what framework this is deployed on.",10,26
154,2019-5-3,2019,5,3,13,bk4769,Interview,https://www.reddit.com/r/MachineLearning/comments/bk4769/interview/,Joaman0,1556856282,[removed],0,1
155,2019-5-3,2019,5,3,14,bk4p51,"Telescopic Forks are lately become essential in any branch of handling and movement of goods; it appeared on the markets at the beginning of the 80s, being only used in the automotive sector at the time.",https://www.reddit.com/r/MachineLearning/comments/bk4p51/telescopic_forks_are_lately_become_essential_in/,lhd121,1556860179,,0,1
156,2019-5-3,2019,5,3,14,bk4xxv,Machine learning music generation algorithms are on the rise,https://www.reddit.com/r/MachineLearning/comments/bk4xxv/machine_learning_music_generation_algorithms_are/,dorienh,1556862219,,0,1
157,2019-5-3,2019,5,3,14,bk4yju,Things to remember before starting a machine learning Project,https://www.reddit.com/r/MachineLearning/comments/bk4yju/things_to_remember_before_starting_a_machine/,akhilapriya404,1556862343,[removed],0,1
158,2019-5-3,2019,5,3,14,bk515b,[Project]How can I improve the performance of my two algorithms from this project?,https://www.reddit.com/r/MachineLearning/comments/bk515b/projecthow_can_i_improve_the_performance_of_my/,ashleymadison1750,1556862942,,1,1
159,2019-5-3,2019,5,3,15,bk5510,[Project] A Catalogue of Machine Learning Industry Applications on Github,https://www.reddit.com/r/MachineLearning/comments/bk5510/project_a_catalogue_of_machine_learning_industry/,OppositeMidnight,1556863830,"Project: [https://github.com/firmai/industry-machine-learning](https://github.com/firmai/industry-machine-learning)

&amp;#x200B;

I have a free month (forced break) where my plan is to group all available open source notebooks and tools as applied to different industries (primarily focusing on python). If anyone is a subject expert or simply want to help with the project please send me a pull request or get in contact with me ([d.snow\\atsymbolcomeshere\\jbs.cam.ac.uk](mailto:d.snow@jbs.cam.ac.uk)). Its still very fresh so any ideas/feedback are welcome and certainly appreciated.",2,17
160,2019-5-3,2019,5,3,15,bk55q6,[D] Anyone know of a good way to run deep learning inference on a gaming computer only when the GPU isn't being utilized for something else?,https://www.reddit.com/r/MachineLearning/comments/bk55q6/d_anyone_know_of_a_good_way_to_run_deep_learning/,InterstellarRun,1556863995,,12,17
161,2019-5-3,2019,5,3,15,bk5emn,Machine Learning Summarized in one picture,https://www.reddit.com/r/MachineLearning/comments/bk5emn/machine_learning_summarized_in_one_picture/,iabhishekofficial,1556866034,,0,1
162,2019-5-3,2019,5,3,15,bk5fg4,Bag of Tricks for Image Classification with Convolutional Neural Networks: Paper Discussion,https://www.reddit.com/r/MachineLearning/comments/bk5fg4/bag_of_tricks_for_image_classification_with/,caternoon,1556866238,,2,1
163,2019-5-3,2019,5,3,17,bk5wx8,I need help with a project?,https://www.reddit.com/r/MachineLearning/comments/bk5wx8/i_need_help_with_a_project/,Exorist-Enas,1556870594,[removed],1,1
164,2019-5-3,2019,5,3,18,bk6a35,Best ways to scale your input data?,https://www.reddit.com/r/MachineLearning/comments/bk6a35/best_ways_to_scale_your_input_data/,Jandevries101,1556874123,[removed],0,1
165,2019-5-3,2019,5,3,18,bk6a40,AI tech generates entire bodies of people who don't exist,https://www.reddit.com/r/MachineLearning/comments/bk6a40/ai_tech_generates_entire_bodies_of_people_who/,raul_jr3,1556874130,,0,1
166,2019-5-3,2019,5,3,18,bk6dv4,Abandon United States as a venue for computer science conferences.,https://www.reddit.com/r/MachineLearning/comments/bk6dv4/abandon_united_states_as_a_venue_for_computer/,russellsparadox101,1556875028,,0,0
167,2019-5-3,2019,5,3,18,bk6m29,[D] How we'll get to AGI in the future,https://www.reddit.com/r/MachineLearning/comments/bk6m29/d_how_well_get_to_agi_in_the_future/,Cock-tail,1556877034,"Just put a chip in the brain(at birth), and record the entire(visual?) life experience of people, maybe add the hormonal levels like dopamine and cortisol. Train a Deep RL model on that, and viola! An AGI that has the same human values. No need for an ethics group, because such a thing won't exist if this becomes reality.",3,0
168,2019-5-3,2019,5,3,18,bk6nib,5 Lessons from the failure from Microsoft Clippy for future AI &amp; ML products,https://www.reddit.com/r/MachineLearning/comments/bk6nib/5_lessons_from_the_failure_from_microsoft_clippy/,nahuak,1556877393,,0,1
169,2019-5-3,2019,5,3,19,bk6vu6,Web development or Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/bk6vu6/web_development_or_machine_learning/,gauravk18,1556879313,[removed],0,1
170,2019-5-3,2019,5,3,20,bk75cm,Here's to semi-random neural networks,https://www.reddit.com/r/MachineLearning/comments/bk75cm/heres_to_semirandom_neural_networks/,danFromTelAviv,1556881430,[removed],0,1
171,2019-5-3,2019,5,3,20,bk78jl,Im a beginner but I got an idea to make an app or something so my idea is I want to create face recognition which identify people by using data which I add also use data from Facebook. Is this possible?,https://www.reddit.com/r/MachineLearning/comments/bk78jl/im_a_beginner_but_i_got_an_idea_to_make_an_app_or/,kashyappatels,1556882050,[removed],0,1
172,2019-5-3,2019,5,3,20,bk79cc,[N] Human-Centric Machine Learning Infrastructure @Netflix,https://www.reddit.com/r/MachineLearning/comments/bk79cc/n_humancentric_machine_learning_infrastructure/,asuagar,1556882202,,0,1
173,2019-5-3,2019,5,3,20,bk7f7i,Alibi - a library for machine learning model inspection and interpretation,https://www.reddit.com/r/MachineLearning/comments/bk7f7i/alibi_a_library_for_machine_learning_model/,jklaise,1556883347,[removed],0,1
174,2019-5-3,2019,5,3,20,bk7lym,Flow in directed graphs,https://www.reddit.com/r/MachineLearning/comments/bk7lym/flow_in_directed_graphs/,anderl1980,1556884621,"Hi all,

say I'm working with passenger data in a city network. I can count the numbers of people asking for a ride from node (station) A -&gt; B. With all that data I created a large directed (bidirectional) graph. Now I want to identify larger sources and sinks. Of course I can split the city in regions and aggregate the data. Additionally I want to find clusters and flows within the network itself.

Most algorithms for community detection and flow measurement (I work with networkx in Python) work with undirected graphs.

What is the best way to find clusters and larger flows in the network?

Any help appreciated, thanks in advance!",0,1
175,2019-5-3,2019,5,3,21,bk7uph,[P] Alibi - a library for machine learning model inspection and interpretation,https://www.reddit.com/r/MachineLearning/comments/bk7uph/p_alibi_a_library_for_machine_learning_model/,jklaise,1556886150,"Github: https://github.com/SeldonIO/alibi

Docs: https://docs.seldon.io/projects/alibi

I'm very excited to share with you a machine learning model explanation and inspection library me and /u/arnaudvl have been working on for the past couple of months. The main goal of the library is to provide high-quality reference implementations of the latest ML model explanation and inspection algorithms within a consistent API.

In the first release we support the following techniques:

* [Anchor Explanations](https://docs.seldon.io/projects/alibi/en/latest/methods/Anchors.html)
* [Constrastive Explanation Method]
(https://docs.seldon.io/projects/alibi/en/latest/methods/CEM.html)

* [Trust Scores](https://docs.seldon.io/projects/alibi/en/latest/methods/TrustScores.html)

We have many more ideas in development, including additional techniques and improvements to existing ones, you can read more about this in our [roadmap](https://docs.seldon.io/projects/alibi/en/latest/overview/roadmap.html).

We are very keen to hear any feedback and hope you enjoy it!",7,95
176,2019-5-3,2019,5,3,21,bk87c3,Made an Android app for facial recognition and detection for surveillance purpose but the problem is it recognizes pictures shown from mobile screen,https://www.reddit.com/r/MachineLearning/comments/bk87c3/made_an_android_app_for_facial_recognition_and/,mental_ape101,1556888369,[removed],0,1
177,2019-5-3,2019,5,3,22,bk88ld,Augmented Intelligence - Will 2019 Give Way to Better AI Systems?,https://www.reddit.com/r/MachineLearning/comments/bk88ld/augmented_intelligence_will_2019_give_way_to/,adrianhrjunior,1556888573,,0,1
178,2019-5-3,2019,5,3,22,bk8btv,A new evolutionary algorithm that performs better than genetic algorithm?,https://www.reddit.com/r/MachineLearning/comments/bk8btv/a_new_evolutionary_algorithm_that_performs_better/,RTengx,1556889107,[removed],0,1
179,2019-5-3,2019,5,3,22,bk8d47,Petition to abandon United States as a venue for computer science conferences.,https://www.reddit.com/r/MachineLearning/comments/bk8d47/petition_to_abandon_united_states_as_a_venue_for/,russellsparadox101,1556889317,[removed],0,1
180,2019-5-3,2019,5,3,22,bk8gp8,[D] Petition to abandon United States as a venue for computer science conferences.,https://www.reddit.com/r/MachineLearning/comments/bk8gp8/d_petition_to_abandon_united_states_as_a_venue/,russellsparadox101,1556889915,"Here is [a petition](http://chng.it/kYWFVwGcXZ) if you want to support researchers of Machine Learning who are struggling to get visas to go to the AI conferences (ICML, NeurIPS, ICLR, etc.) that are often held in the United States. Basically it is really frustrating when you work for a year or so on the research and finally get accepted at the top conferences but then cannot present it because software engineers and computer scientists are suspicious to the US government and always require administrative processing. This happened to me this year (5th already in total) and to another author of the paper, so neither of us can go to present our work at ICLR 2019. This happens more frequently than you might think (especially if you never applied for US visa) and impacts a lot of scientists around the globe.

&amp;#x200B;

If you feel our pain here, please sign [the petition](http://chng.it/kYWFVwGcXZ).

&amp;#x200B;

The text of the petition below.

\-------------------------------------------

Largest computer science conferences such as SIGGRAPH, NeurIPS, ICML attract thousands of participants annually. The impact of such conferences is enormous on the community of computer scientists. For many it is the chance to expose results of research work on which one has been working for years and for many it is the opportunity to see the state-of-the-art achievements that will advance the world. It's no surprise that for computer science researchers the conferences are the main events of the year. However, many people cannot attend these conferences because of visa policies that governments impose.

The situation is especially prevalent in the United States, where the citizens of many countries have to wait for months to receive a visa for a 5-day trip to the conference. It is even worse for countries such as Iran, Russia, and China that are under a barrage of scrutiny directed at attendees and authors of the research work. This is particularly sad and humiliating as the savants who publish the scientific articles to bring the good to the world and benefit the society with non-harmless technology have to go through this process over and over again, for each conference that is organized in the U.S.

The goals of this petition are:

1. **Show a disadvantage of receiving a visa to the U.S.**While many countries (e.g. all citizens of European Union) enjoy free entry for the short trips to the United States, many prolific researchers and experts of their areas of research were not fortunate to have this option. This forces researchers to apply for a visa to the U.S. which is a complicated process. There are many hurdles with obtaining a visa to the U.S. for computer scientists. To give an example, *one has to take an appointment to the embassy, which can take up to a month in countries like France and up to a year in countries like Russia*. At the appointment the embassy will take your passport and supporting documents and with 95% *will send you to administrative processing* that takes 2 months. This happens almost all of the time to the people who have a profession of a researcher or an engineer, as these professions seem suspicious to the government. Not only don't you have your passport for this time, but you also don't get any updates or feedback. Even more, even if you got lucky and received a required stamp,*almost always the issued visa is one-time entry*, implying that you should get a visa for your next conference in the U.S., even if your previous visa didn't expire.
2. **Abandon the U.S. as a venue for computer science conferences.** We want organizers of the computer science conferences to exclude the U.S. as a place to host the conferences, until the situation of getting a visa is improved for the researchers and attendees of the conferences. There is *a large and growing number of authors, who cannot attend and present their scientific work*, because the U.S. government treats their profiles as suspicious, despite their academic results. *Countries in E.U. and Asia way easier issue visas for academics*, so they could accommodate more attendees.

This petition will be sent to organizers of all major conferences in computer science (SIGGRAPH, NeurIPS, ICML, etc.).

If you are an attendee of such conferences and also experience problems with obtaining the visas or if you share the same view on this topic, **please help us by signing this petition and sharing it in your social networks**.",232,355
181,2019-5-3,2019,5,3,22,bk8rwd,[Research] Researchers find a way to improve environmental sound classification with a CNN,https://www.reddit.com/r/MachineLearning/comments/bk8rwd/research_researchers_find_a_way_to_improve/,cdossman,1556891810,"  [https://medium.com/ai%C2%B3-theory-practice-business/enhanced-environmental-sound-classification-with-a-cnn-1ca388748bc9](https://medium.com/ai%C2%B3-theory-practice-business/enhanced-environmental-sound-classification-with-a-cnn-1ca388748bc9) 

Abstract: An end-to-end approach for environmental sound classification based on a 1D Convolution Neural Network (CNN) that learns a representation directly from the audio signal. Several convolutional layers are used to capture the signal's fine time structure and learn diverse filters that are relevant to the classification task. The proposed approach can deal with audio signals of any length as it splits the signal into overlapped frames using a sliding window. Different architectures considering several input sizes are evaluated, including the initialization of the first convolutional layer with a Gammatone filterbank that models the human auditory filter response in the cochlea. The performance of the proposed end-to-end approach in classifying environmental sounds was assessed on the UrbanSound8k dataset and the experimental results have shown that it achieves 89% of mean accuracy.",1,5
182,2019-5-3,2019,5,3,23,bk8u40,[D] Did somebody tried to exploit enforcing Golden Ratio to DeepDream/Style Transfer/other generative approaches ?,https://www.reddit.com/r/MachineLearning/comments/bk8u40/d_did_somebody_tried_to_exploit_enforcing_golden/,ad48hp,1556892172,"So far i've seen [very](https://www.researchgate.net/publication/236132090_Face_detection_evaluation_A_new_approach_based_on_the_golden_ratio_PH) [few](https://arxiv.org/search/advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=golden+ratio&amp;terms-0-field=title&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first) papers tryin' to play around with this number (or it's representation which would have dynamic precision as it would be needed..).

I wonder how would enforcing this number to certain properties in the learning models (CNN) that are about to being used on DeepDream/Style Transfer or to direct generative models like GAN result in our perception of their results vs those generated without enforcement (there could be also other 'fixed' numbers being exploited like rule of thirds, but i believe yet this one would have more limited usage as opposed to this one)",0,0
183,2019-5-3,2019,5,3,23,bk8uqp,Questions about recent PhD job interviews,https://www.reddit.com/r/MachineLearning/comments/bk8uqp/questions_about_recent_phd_job_interviews/,KlutzyDefinition,1556892265,[removed],0,1
184,2019-5-3,2019,5,3,23,bk8zzy,Programming Languages for Data Science and ML - With Source Code Illustrations,https://www.reddit.com/r/MachineLearning/comments/bk8zzy/programming_languages_for_data_science_and_ml/,andrea_manero,1556893098,https://www.datasciencecentral.com/profiles/blogs/programming-languages-for-data-science-and-ml-with-source-code-il,0,1
185,2019-5-3,2019,5,3,23,bk9b8g,Unevenly spaced event prediction,https://www.reddit.com/r/MachineLearning/comments/bk9b8g/unevenly_spaced_event_prediction/,vakker00,1556894883,[removed],0,1
186,2019-5-3,2019,5,3,23,bk9f9r,[D] Unevenly spaced event prediction,https://www.reddit.com/r/MachineLearning/comments/bk9f9r/d_unevenly_spaced_event_prediction/,vakker00,1556895503,"I have a dataset of events (let's say event A and event B) happening at certain times and I need to predict when the next event A is going to happen. E.g. purchases, failures, etc.

Do you have an idea of how to approach this problem? It's fundamentally different than the usual timeseries analysis problems, where there's an underlying signal sampled at fixed intervals.

Also, the fact that there are multiple different events make it more complex since they cannot be just input together to a let's say LSTM, because they come at different times.",6,3
187,2019-5-3,2019,5,3,23,bk9fv0,"[P] Browser-based, collaborative tool for annotating object detection/segmentation datasets",https://www.reddit.com/r/MachineLearning/comments/bk9fv0/p_browserbased_collaborative_tool_for_annotating/,AllergicToDinosaurs,1556895592,,1,1
188,2019-5-4,2019,5,4,0,bk9pv1,[D] NLL loss implemented in Bengio's VRNN paper for Gaussian mixture model seems strange to me...,https://www.reddit.com/r/MachineLearning/comments/bk9pv1/d_nll_loss_implemented_in_bengios_vrnn_paper_for/,nokpil,1556897112,"&amp;#x200B;

I'm currently trying to make an MLP which outputs statistics for Gaussian Mixture Model (GMM), i.e.,  means, covariance matrixes, and coefficients for each mixture components. This approach is already have been explored by others, such as Variational RNN paper (2016,  [https://arxiv.org/pdf/1506.02216.pdf](https://arxiv.org/pdf/1506.02216.pdf) ).  Although I'm not interested in posterior distribution and not going to implement my NN as Variational form, I tried to dig into the source code since the form of NLL loss function should be the same. Fortunately, the authors provided their GitHub link ( [https://github.com/jych/nips2015\_vrnn](https://github.com/jych/nips2015_vrnn) ) and I could find a form of loss function they employed.

&amp;#x200B;

Following code is the cost function for bivariate GMM model.([https://github.com/jych/cle/blob/master/cle/cost/\_\_init\_\_.py](https://github.com/jych/cle/blob/master/cle/cost/__init__.py) )

&amp;#x200B;

![img](7k435xush0w21)

I've found that the highlighted part seems... *not right.* 

Here is the log-likelihood function for Gaussian Mixture Model. 

&amp;#x200B;

![img](pfmf560ei0w21)

This function is rather intractable, since the log contains a weighted sum. As far I know, we need to relax the problem and classic EM iteration is necessary to optimize this log-likelihood function. (Actually, there are many papers solely focused on this problem; such as neural EM. ([https://papers.nips.cc/paper/7246-neural-expectation-maximization.pdf](https://papers.nips.cc/paper/7246-neural-expectation-maximization.pdf) ))

&amp;#x200B;

In EM algorithm, after you calculate gamma (posterior of component categorical distribution) in 'E' step, you maximize the log-likelihood ('M' step') by maximizing following formula.

![img](k9w6ast9j0w21)

But it seems like VRNN paper's implementation of NLL loss just simply ignored the gamma function (or E-step), and **assume that red-circled part is sufficient for loss.** (Ignore c\_b part, which is a binary entropy term for non-GMM posterior.)

&amp;#x200B;

With simple intuition, the whole calculation doesn't involve the variable 'coeff' () except the highlighted part. It means that NLL loss just contains the log value of the coefficient,  which makes all coefficient equal to 1/N in order to minimize negative log value.

&amp;#x200B;

**Is this kind of formulation is appropriate for Gaussian mixture model?** (Like, is it some sort of linear approximation of original form?) Otherwise, is there a simple way to calculate (or approximate) NLL loss for GMM?

&amp;#x200B;

Please let me know If I got anything wrong. I'm relatively new to this field and need constructive discussion for this topic. Thanks!",3,7
189,2019-5-4,2019,5,4,0,bk9zvy,The first things you need to know about machine learning,https://www.reddit.com/r/MachineLearning/comments/bk9zvy/the_first_things_you_need_to_know_about_machine/,caternoon,1556898620,,1,1
190,2019-5-4,2019,5,4,0,bka19h,Statistics vs Machine Learning: The Two Worlds,https://www.reddit.com/r/MachineLearning/comments/bka19h/statistics_vs_machine_learning_the_two_worlds/,TheTesseractAcademy,1556898831,,0,1
191,2019-5-4,2019,5,4,0,bka27g,Reuse of test cases,https://www.reddit.com/r/MachineLearning/comments/bka27g/reuse_of_test_cases/,codexblaze,1556898987,[removed],0,1
192,2019-5-4,2019,5,4,1,bka8de,[D] what do you think about these 5-min ML paper videos?,https://www.reddit.com/r/MachineLearning/comments/bka8de/d_what_do_you_think_about_these_5min_ml_paper/,tdls_to,1556899888,"hello, all,

we have recently hosted a challenge in which we asked people to grab their favorite ML paper and describe it in a video in 5 min. The link below contains the submitted videos. Now we are asking everyone to judge the videos. Please let us what you think; the winner is picked based on your opinion on how they did (ideally through comments on youtube directly). 

&amp;#x200B;

[https://www.youtube.com/playlist?list=PLB1nTQo4\_y6sFXTQtuP8mn7scJrk\_O0Sy](https://www.youtube.com/playlist?list=PLB1nTQo4_y6sFXTQtuP8mn7scJrk_O0Sy)",5,21
193,2019-5-4,2019,5,4,1,bkak02,How does smote oversampling technique change the nature of the data that affect the performance of the classifiers ?,https://www.reddit.com/r/MachineLearning/comments/bkak02/how_does_smote_oversampling_technique_change_the/,burhny60,1556901665,[removed],0,1
194,2019-5-4,2019,5,4,1,bkap3d,What are some unique and/or novel ways machine learning is being recently?,https://www.reddit.com/r/MachineLearning/comments/bkap3d/what_are_some_unique_andor_novel_ways_machine/,SilentNightLabs,1556902422,[removed],0,1
195,2019-5-4,2019,5,4,2,bkawa2,Announcing Google-Landmarks-v2: An Improved Dataset for Landmark Recognition &amp; Retrieval,https://www.reddit.com/r/MachineLearning/comments/bkawa2/announcing_googlelandmarksv2_an_improved_dataset/,sjoerdapp,1556903477,,0,1
196,2019-5-4,2019,5,4,2,bkb7bj,Simple storage archive for image dataset,https://www.reddit.com/r/MachineLearning/comments/bkb7bj/simple_storage_archive_for_image_dataset/,neziib,1556905101,[removed],0,1
197,2019-5-4,2019,5,4,2,bkb8et,Gigantum. An open-source application to do ridiculously portable and reproducible analyses and machine learning!,https://www.reddit.com/r/MachineLearning/comments/bkb8et/gigantum_an_opensource_application_to_do/,meg297,1556905265,[removed],0,1
198,2019-5-4,2019,5,4,3,bkbgyz,Making Reproducibility Reproducible,https://www.reddit.com/r/MachineLearning/comments/bkbgyz/making_reproducibility_reproducible/,meg297,1556906544,[removed],0,1
199,2019-5-4,2019,5,4,3,bkbvh0,How to Train GPT-2 model from scratch,https://www.reddit.com/r/MachineLearning/comments/bkbvh0/how_to_train_gpt2_model_from_scratch/,Vaiku2718,1556908716,,0,1
200,2019-5-4,2019,5,4,4,bkcc6x,The financial sector is committed to a smart future,https://www.reddit.com/r/MachineLearning/comments/bkcc6x/the_financial_sector_is_committed_to_a_smart/,ramaprasadjena,1556911191,[removed],0,1
201,2019-5-4,2019,5,4,4,bkce1k,Researchers Question KDD 2019 Paper Review Process,https://www.reddit.com/r/MachineLearning/comments/bkce1k/researchers_question_kdd_2019_paper_review_process/,Yuqing7,1556911466,,0,1
202,2019-5-4,2019,5,4,4,bkcfji,Confusion in Pytorch LSTM and RNN code,https://www.reddit.com/r/MachineLearning/comments/bkcfji/confusion_in_pytorch_lstm_and_rnn_code/,bikanation,1556911696,[removed],0,1
203,2019-5-4,2019,5,4,5,bkd2lj,Maths and Programming.,https://www.reddit.com/r/MachineLearning/comments/bkd2lj/maths_and_programming/,eagle930,1556915222,"So, i have started building(applying) models using python and tuning the hyper-parameters , cross-validation etc. I get the general idea, however i wanted to know how important it is/ how can it help me if i get to the details of the math behind the models ( i have the general idea of the math but not to the extent of the formulas and other intricacies).    


Thanks.",0,1
204,2019-5-4,2019,5,4,5,bkd3z8,Random epoch taking much longer than others,https://www.reddit.com/r/MachineLearning/comments/bkd3z8/random_epoch_taking_much_longer_than_others/,Hkkkhan,1556915426,,1,1
205,2019-5-4,2019,5,4,6,bkdutz,Documents that machine learning programmers need to create?,https://www.reddit.com/r/MachineLearning/comments/bkdutz/documents_that_machine_learning_programmers_need/,jrgallag,1556919645,[removed],0,1
206,2019-5-4,2019,5,4,6,bkduxz,The Big Picture: Google Releases Massive Landmark Recognition Dataset,https://www.reddit.com/r/MachineLearning/comments/bkduxz/the_big_picture_google_releases_massive_landmark/,Yuqing7,1556919664,,0,1
207,2019-5-4,2019,5,4,7,bkejeq,Moogsoft's Chief Science Officer Presents AI Research at IEEEs AnNet 2019,https://www.reddit.com/r/MachineLearning/comments/bkejeq/moogsofts_chief_science_officer_presents_ai/,brownlabdog,1556923590,,0,1
208,2019-5-4,2019,5,4,7,bkejvb,"[N] OpenAI releasing the 345M model of GPT-2 and sharing the 1.5B model ""with partners working on countermeasures""",https://www.reddit.com/r/MachineLearning/comments/bkejvb/n_openai_releasing_the_345m_model_of_gpt2_and/,kirasolo,1556923663,"OpenAI has decided to adopt a staged release approach to their GPT-2 language model.

Announcement on Twitter: https://twitter.com/OpenAI/status/1124440412679233536

From the update on their blog (https://openai.com/blog/better-language-models/#update):

#Staged Release

&gt;Staged release involves the gradual release of a family of models over time. The purpose of our staged release of GPT-2 is to give people time to assess the properties of these models, discuss their societal implications, and evaluate the impacts of release after each stage.

&gt;As the next step in our staged release strategy, we are releasing the 345M parameter version of GPT-2. This model features improved performance relative to the 117M version, though falls short of the 1.5B version with respect to the ease of generating coherent text. We have been excited to see so many positive uses of GPT-2-117M, and hope that 345M will yield still more benefits.

&gt;While the misuse risk of 345M is higher than that of 117M, we believe it is substantially lower than that of 1.5B, and we believe that training systems of similar capability to GPT-2-345M is well within the reach of many actors already; this evolving replication landscape has informed our decision-making about what is appropriate to release.

&gt;In making our 345M release decision, some of the factors we considered include: the ease of use (by various users) of different model sizes for generating coherent text, the role of humans in the text generation process, the likelihood and timing of future replication and publication by others, evidence of use in the wild and expert-informed inferences about unobservable uses, proofs of concept such as the review generator mentioned in the original blog post, the strength of demand for the models for beneficial purposes, and the input of stakeholders and experts. We remain uncertain about some of these variables and continue to welcome input on how to make appropriate language model publication decisions.

&gt;We hope that ongoing research on bias, detection, and misuse will give us the confidence to publish larger models in a timely manner, and at the six month mark we will share a fuller analysis of language models societal implications and our heuristics for release decisions.

#Partnerships

&gt;Since releasing this blog post in February, we have had conversations with many external researchers, technology companies, and policymakers about our release strategy and the implications of increasingly large language models. Weve also presented or discussed our work at events, including a dinner co-hosted with the Partnership on AI and a presentation to policymakers in Washington DC at the Global Engagement Center.

&gt;We are currently forming research partnerships with academic institutions, non-profits, and industry labs focused on increasing societal preparedness for large language models. In particular, we are sharing the 762M and 1.5B parameter versions of GPT-2 to facilitate research on language model output detection, language model bias analysis and mitigation, and analysis of misuse potential. In addition to observing the impacts of language models in the wild, engaging in dialogue with stakeholders, and conducting in-house analysis, these research partnerships will be a key input to our decision-making on larger models. See below for details on how to get involved.",126,232
209,2019-5-4,2019,5,4,7,bkek6e,"""Billion-scale semi-supervised learning for image classification"", Yalniz et al 2019 {FB}",https://www.reddit.com/r/MachineLearning/comments/bkek6e/billionscale_semisupervised_learning_for_image/,gwern,1556923720,,3,12
210,2019-5-4,2019,5,4,11,bkgb63,[P] Implementing Billion-scale semi-supervised learning for image classification using Pytorch,https://www.reddit.com/r/MachineLearning/comments/bkgb63/p_implementing_billionscale_semisupervised/,leaderj1001,1556935263,"Hi, reddit !! We are Myeongjun Kim and Taehun Kim. Our major is computer vision research using deep learning. Previously, we implemented the paper with RandWireNN pytorch version and tensorflow version. This time, we implemented a paper on ""Billion-scale semi-supervised learning for image classification written by Facebook AI""(https://arxiv.org/abs/1905.00546). To briefly describe the paper, it is stated that the classification performance is improved by using unlabeled data. We realized that it was a simple and novel idea. So, we implemented it.
Due to the lack of GPU resources and unlabeled data, we are delaying the experiment on ImageNet and are experimenting with CIFAR-100 first. We would like to ask for your interest and feedback.
Thank you for reading long long story.
Github URL: https://github.com/leaderj1001/Billion-scale-semi-supervised-learning",2,6
211,2019-5-4,2019,5,4,11,bkge6q,[N] Announcing Google-Landmarks-v2: An Improved Dataset for Landmark Recognition &amp; Retrieval,https://www.reddit.com/r/MachineLearning/comments/bkge6q/n_announcing_googlelandmarksv2_an_improved/,iyaja,1556935851,"Google released a new landmark dataset yesterday. You can find the details in their blog post:

&amp;#x200B;

[https://ai.googleblog.com/2019/05/announcing-google-landmarks-v2-improved.html](https://ai.googleblog.com/2019/05/announcing-google-landmarks-v2-improved.html)",1,20
212,2019-5-4,2019,5,4,11,bkgeb1,Graph Matching Networks for Learning the Similarity of Graph Structured Objects,https://www.reddit.com/r/MachineLearning/comments/bkgeb1/graph_matching_networks_for_learning_the/,iyaja,1556935876,,2,12
213,2019-5-4,2019,5,4,11,bkgega,Low-Memory Neural Network Training: A Technical Report,https://www.reddit.com/r/MachineLearning/comments/bkgega/lowmemory_neural_network_training_a_technical/,iyaja,1556935898,,1,14
214,2019-5-4,2019,5,4,11,bkgp1z,Artificial Neural Networks - implementation with microcontrollers,https://www.reddit.com/r/MachineLearning/comments/bkgp1z/artificial_neural_networks_implementation_with/,OriginalKrimmet,1556938005,[removed],0,1
215,2019-5-4,2019,5,4,15,bkiefz,Defour Analytics |Free Webinar on Python,https://www.reddit.com/r/MachineLearning/comments/bkiefz/defour_analytics_free_webinar_on_python/,ankita11_,1556951958,[removed],0,1
216,2019-5-4,2019,5,4,15,bkifr4,scope of machine learning in the future?,https://www.reddit.com/r/MachineLearning/comments/bkifr4/scope_of_machine_learning_in_the_future/,Albertchristopher,1556952311,,2,1
217,2019-5-4,2019,5,4,16,bkijle,Escalate your Salary with Artificial Intelligence and Machine Learning Skills,https://www.reddit.com/r/MachineLearning/comments/bkijle/escalate_your_salary_with_artificial_intelligence/,jasonpica,1556953333,,0,1
218,2019-5-4,2019,5,4,16,bkil6g,"[R] Kervolutional Neural Networks, CVPR'19 oral",https://www.reddit.com/r/MachineLearning/comments/bkil6g/r_kervolutional_neural_networks_cvpr19_oral/,xternalz,1556953708,,35,60
219,2019-5-4,2019,5,4,16,bkillv,RNN with multiple and multivariate timeseries,https://www.reddit.com/r/MachineLearning/comments/bkillv/rnn_with_multiple_and_multivariate_timeseries/,KindlyLearning,1556953828,[removed],0,1
220,2019-5-4,2019,5,4,16,bkio5j,[D] Is Industrial Conference on Data Mining good?,https://www.reddit.com/r/MachineLearning/comments/bkio5j/d_is_industrial_conference_on_data_mining_good/,atif_hassan,1556954489,"Hi!

My paper recently got accepted in the Industrial Conference on Data Mining (ICDM) (not to be confused with the International Conference on Data Mining which is also ICDM). I was thrilled but some of my profs said that it is a bad conference while some others said it was good and still some others said that they had no idea.

I am totally confused. What do you all think? I need to know this because I have to apply for financial assistance from my Institute and they only provide said assistance if the conference is good.

Here's the link to the conference:  [http://www.data-mining-forum.de/](http://www.data-mining-forum.de/)",4,1
221,2019-5-4,2019,5,4,16,bkippl,Telescopic Forks for Pallet Handling,https://www.reddit.com/r/MachineLearning/comments/bkippl/telescopic_forks_for_pallet_handling/,lhd121,1556954925,,0,1
222,2019-5-4,2019,5,4,16,bkiryj,My first time with machine learning,https://www.reddit.com/r/MachineLearning/comments/bkiryj/my_first_time_with_machine_learning/,Dkalogeryt,1556955546,[removed],0,1
223,2019-5-4,2019,5,4,16,bkivi8,Telescopic Forks for Pallet Handling,https://www.reddit.com/r/MachineLearning/comments/bkivi8/telescopic_forks_for_pallet_handling/,lhd121,1556956507,[removed],0,1
224,2019-5-4,2019,5,4,17,bkj4w6,The Learning Rule - Causality and Generative Models,https://www.reddit.com/r/MachineLearning/comments/bkj4w6/the_learning_rule_causality_and_generative_models/,aeduG,1556959160,,0,1
225,2019-5-4,2019,5,4,18,bkjbj2,[D] Are time-series embeddings worth using?,https://www.reddit.com/r/MachineLearning/comments/bkjbj2/d_are_timeseries_embeddings_worth_using/,pk12_,1556961047,"Based on your experience, do you think time-series embeddings can be useful, similar to how image embeddings are really useful?

I know about TimeNet embeddings and the concept seems fine, but since they haven't released their embeddings, I can't simply test it off the shelf. 

Has anyone experience of using such embeddings?",6,16
226,2019-5-4,2019,5,4,18,bkjjve,Does transfer learning from smaller to bigger architectures exist?,https://www.reddit.com/r/MachineLearning/comments/bkjjve/does_transfer_learning_from_smaller_to_bigger/,NP-intermediate,1556963315,[removed],0,1
227,2019-5-4,2019,5,4,18,bkjk7t,Seeking Advice to forecast large number of non-standard time-series dynamically/in run-time,https://www.reddit.com/r/MachineLearning/comments/bkjk7t/seeking_advice_to_forecast_large_number_of/,temp_account_1234,1556963413,[removed],0,1
228,2019-5-4,2019,5,4,19,bkjqjr,emoFie,https://www.reddit.com/r/MachineLearning/comments/bkjqjr/emofie/,emoFie,1556965066,[removed],0,1
229,2019-5-4,2019,5,4,20,bkkaic,Microsoft launches drag and drop ML tool,https://www.reddit.com/r/MachineLearning/comments/bkkaic/microsoft_launches_drag_and_drop_ml_tool/,lorenzofeliz,1556970131,,0,1
230,2019-5-4,2019,5,4,20,bkkexp,Question about the term ID,https://www.reddit.com/r/MachineLearning/comments/bkkexp/question_about_the_term_id/,Unlistedd,1556971144,[removed],0,1
231,2019-5-4,2019,5,4,21,bkkjpq,VAEs with normalizing flows ?,https://www.reddit.com/r/MachineLearning/comments/bkkjpq/vaes_with_normalizing_flows/,MrPuj,1556972154,[removed],0,1
232,2019-5-4,2019,5,4,21,bkkjsk,Computational and Mathematical Neuroscience Community,https://www.reddit.com/r/MachineLearning/comments/bkkjsk/computational_and_mathematical_neuroscience/,P4TR10T_TR41T0R,1556972173,[removed],0,1
233,2019-5-4,2019,5,4,21,bkkk8b,[D] Computational and Mathematical Neuroscience Community,https://www.reddit.com/r/MachineLearning/comments/bkkk8b/d_computational_and_mathematical_neuroscience/,P4TR10T_TR41T0R,1556972263,"I'd like to spread the word about [/r/compmathneuro](https://www.reddit.com/r/compmathneuro), a subreddit dedicated to **computational and mathematical neuroscience**, and its associated discord. We're a small but relatively active community of researchers whose expertise ranges from undergrads to postdocs. We recently created a discord in preparation for the monthly journal clubs we're planning to start soon, where participants are encouraged to contribute papers they'd like to discuss with others in VC. Anyone is welcome to join both the subreddit ([https://reddit.com/r/compmathneuro](https://reddit.com/r/compmathneuro)) and the discord ([https://discord.gg/aFVEW2N](https://discord.gg/aFVEW2N)), but please note that we'd like to uphold our current content standards and thus largely object to pop sci articles and TED-esque videos. Curious laymen are nonetheless welcome, of course.",11,169
234,2019-5-4,2019,5,4,21,bkkofj,Nice article on DeOldify @Medium,https://www.reddit.com/r/MachineLearning/comments/bkkofj/nice_article_on_deoldify_medium/,killertool12345,1556973162,,0,1
235,2019-5-4,2019,5,4,21,bkkrmh,Nice article on DeOldify,https://www.reddit.com/r/MachineLearning/comments/bkkrmh/nice_article_on_deoldify/,killertool12345,1556973819,,0,1
236,2019-5-4,2019,5,4,22,bkkyv6,[P] Sequence-to-Sequence Model for Markup Input. Where to start?,https://www.reddit.com/r/MachineLearning/comments/bkkyv6/p_sequencetosequence_model_for_markup_input_where/,suhrob,1556975302,"I'm thinking about creating a sequence-to-sequence model that:

&amp;#x200B;

* Input: sentence / paragraphs
   * My favourite animal is my cat.

&amp;#x200B;

* Output: the same text with some simple markup
   * My favourite {1::animal} is my {2::cat}.

&amp;#x200B;

This is essentially a tool to create semantically meaningful cloze deletions.

I'm thinking about taking GPT2 or BERT as pre-trained lang. models. I'm bit worried that I need to take these very complex models and essentially force them to be able to perform a (nearly complete) identity transformation.

&amp;#x200B;

Does anybody have any pointers / tutorials where to start? 

&amp;#x200B;

Thank you in any case!",1,1
237,2019-5-4,2019,5,4,22,bklatq,Announcing JupyterHub 1.0,https://www.reddit.com/r/MachineLearning/comments/bklatq/announcing_jupyterhub_10/,_quanttrader_,1556977635,,0,1
238,2019-5-4,2019,5,4,23,bklgjf,Qusestion about this article : https://arxiv.org/abs/1008.5071,https://www.reddit.com/r/MachineLearning/comments/bklgjf/qusestion_about_this_article/,lzocotor9,1556978685,[removed],0,1
239,2019-5-4,2019,5,4,23,bklgye,"[Project] Running a CreateML algorithm for a hackathon based on a series of survey responses, and we need as many responses as we can get as soon as possible.",https://www.reddit.com/r/MachineLearning/comments/bklgye/project_running_a_createml_algorithm_for_a/,droppingplanets,1556978757,"[https://forms.gle/qSjRNvWqe5GdAQZa9](https://forms.gle/qSjRNvWqe5GdAQZa9)

&amp;#x200B;

I'd really appreciate it if as many people as possible took some time out to fill out the survey for our algorithm, thank you so much!",1,1
240,2019-5-4,2019,5,4,23,bklmbl,Could someone help me out with implementing Batch Normalization?,https://www.reddit.com/r/MachineLearning/comments/bklmbl/could_someone_help_me_out_with_implementing_batch/,Jandevries101,1556979707,[removed],0,1
241,2019-5-4,2019,5,4,23,bklte1,"[D] Does anyone know PhD programs or companies concerned with Machine Learning/Deep Learning/Artificial Intelligence that focus on environmental risks, climate change and similar topics?",https://www.reddit.com/r/MachineLearning/comments/bklte1/d_does_anyone_know_phd_programs_or_companies/,ericfromthenorth,1556980974,,10,6
242,2019-5-4,2019,5,4,23,bklvg1,Feeding RNN output to a fully connected layer,https://www.reddit.com/r/MachineLearning/comments/bklvg1/feeding_rnn_output_to_a_fully_connected_layer/,bikanation,1556981320,"For example if we have 10 hidden states , do we take the 10 hidden states and feed them all together to a fully connected layer ? like concatenate them , or each hidden state if fed alone to a fully connected layer ? 

And Other example ,if the task is sentiment analysis and we want to predict +ve or -ve and if we have like batch size of 50 sentences , each sentence is formed of 20 words , so we take the hidden state of the last word and feed it to a fully connected layer to predict sentiment through softmax ? can we take the whole batch (so we take 50 last words), concatenate them and then feed them to a fully connected ?",0,1
243,2019-5-5,2019,5,5,0,bkm85o,Content generator for a facebook page based on another facebook group/page ?,https://www.reddit.com/r/MachineLearning/comments/bkm85o/content_generator_for_a_facebook_page_based_on/,neoncolour,1556983443,[removed],0,1
244,2019-5-5,2019,5,5,0,bkm8cm,AutoML Mobile: Automated ML Model Design for Every Mobile Device,https://www.reddit.com/r/MachineLearning/comments/bkm8cm/automl_mobile_automated_ml_model_design_for_every/,Yuqing7,1556983475,,0,1
245,2019-5-5,2019,5,5,0,bkmbj4,[R] May the 4th of VADER be with you! Snorkel + Vader for Credit Sentiment,https://www.reddit.com/r/MachineLearning/comments/bkmbj4/r_may_the_4th_of_vader_be_with_you_snorkel_vader/,gau_mar,1556983998,,0,1
246,2019-5-5,2019,5,5,0,bkmbnq,Advice on managing large time series dataset for deep learning processing,https://www.reddit.com/r/MachineLearning/comments/bkmbnq/advice_on_managing_large_time_series_dataset_for/,sauerkimchi,1556984016,[removed],0,1
247,2019-5-5,2019,5,5,1,bkmswd,"Survey Saturday, Data Edition: What type of data do you handle and what data formats are you using to store your data? What do you use to wrangle and clean your data? What are your data pipelines?",https://www.reddit.com/r/MachineLearning/comments/bkmswd/survey_saturday_data_edition_what_type_of_data_do/,BatmantoshReturns,1556986850,[removed],0,1
248,2019-5-5,2019,5,5,1,bkmu30,"[D] Survey Saturday, Data Edition: What type of data do you handle and what data formats are you using to store your data? What do you use to wrangle and clean your data? What are your data pipelines?",https://www.reddit.com/r/MachineLearning/comments/bkmu30/d_survey_saturday_data_edition_what_type_of_data/,BatmantoshReturns,1556987030,,0,11
249,2019-5-5,2019,5,5,2,bknlzd,Python toolset for statistical comparison of machine learning models and human readers,https://www.reddit.com/r/MachineLearning/comments/bknlzd/python_toolset_for_statistical_comparison_of/,ketsok,1556991393,,0,1
250,2019-5-5,2019,5,5,2,bkno4o,How to use HyperGAN with principal component analysis?,https://www.reddit.com/r/MachineLearning/comments/bkno4o/how_to_use_hypergan_with_principal_component/,EnoDevz,1556991738,[removed],0,1
251,2019-5-5,2019,5,5,2,bknq36,Guess function from its graph with machine learning.,https://www.reddit.com/r/MachineLearning/comments/bknq36/guess_function_from_its_graph_with_machine/,Another__one,1556992050,[removed],0,1
252,2019-5-5,2019,5,5,2,bknqhx,[D] Filtering synthesized images from GAN/VAE/... ?,https://www.reddit.com/r/MachineLearning/comments/bknqhx/d_filtering_synthesized_images_from_ganvae/,lzhbrian,1556992112,"Any ideas on how to filter ""good"" samples from GAN/VAE synthesized samples?",2,0
253,2019-5-5,2019,5,5,2,bknr48,[D] Guess function from its graph with machine learning,https://www.reddit.com/r/MachineLearning/comments/bknr48/d_guess_function_from_its_graph_with_machine/,Another__one,1556992210,"Suppose I have handwritten graph of some function, and what I want is to have some ML algorithm that find some simple function (in form of a string like ""f(x) = 2 cos(x) + log(x)\^2"") that fit my graph as well as possible. I tried to make something like this myself and find out that this task is surprisingly difficult, unless the function is trivial. Although I'm not a researcher, so maybe I just have lack of knowledge to crack this problem. I wonder if there any research papers relative to simiar problems. Or any suggestions how to approach this will be helpful as well.",12,4
254,2019-5-5,2019,5,5,3,bknw26,[D] An Overview of Pose Estimation using Deep Learning,https://www.reddit.com/r/MachineLearning/comments/bknw26/d_an_overview_of_pose_estimation_using_deep/,thatbrguy_,1556992987,"This article aims to provide a brief introductory overview of some of the techniques used for Human Pose Estimation using Deep Learning:

[https://medium.com/beyondminds/an-overview-of-human-pose-estimation-with-deep-learning-d49eb656739b?source=friends\_link&amp;sk=49567cc8c2d453094defad0e2fe6dc67](https://medium.com/beyondminds/an-overview-of-human-pose-estimation-with-deep-learning-d49eb656739b?source=friends_link&amp;sk=49567cc8c2d453094defad0e2fe6dc67)

&amp;#x200B;

I would love to hear your thoughts and feedback regarding the same, thanks!",2,10
255,2019-5-5,2019,5,5,3,bknyfx,Automated Machine Learning for Professionals,https://www.reddit.com/r/MachineLearning/comments/bknyfx/automated_machine_learning_for_professionals/,andrea_manero,1556993367,[removed],0,1
256,2019-5-5,2019,5,5,3,bko6bi,Self-Driving Cars Specialization,https://www.reddit.com/r/MachineLearning/comments/bko6bi/selfdriving_cars_specialization/,HannahHumphreys,1556994626,[removed],0,1
257,2019-5-5,2019,5,5,3,bkoaru,How do I set up unsupervised learning with Principal Component Analysis?,https://www.reddit.com/r/MachineLearning/comments/bkoaru/how_do_i_set_up_unsupervised_learning_with/,EnoDevz,1556995342,,0,1
258,2019-5-5,2019,5,5,4,bkoisj,[D] Advice for preparing for Masters degree,https://www.reddit.com/r/MachineLearning/comments/bkoisj/d_advice_for_preparing_for_masters_degree/,Somechillguy,1556996570,"Looking for advice to help me prepare for a Master's degree in Computer Science later this year. In undergrad I majored Physics and Applied Math, and in the years since saw cool realms of application for ML in industry and research. Unfortunately because of my background experience I now have some imposters syndrome setting in, well before I've stepped foot in any classroom. Hoping people can look at my goals, courses, and experience to try to suggest steps to better prepare.

I'm hoping to gain essentially three things from this degree:

* Knowledge of the computational foundations of popular or common ML algorithms (because its cool)
* basic skills and best practices of an ML practitioner (because its practical)
* Some modest amount of experience in projects and homework for the above (because its practical)

I would hope that I can get a job as an ML engineer/scientist/whatever afterwards, but preferably not closely tied to the ops side of design and implementation.

Summary of any related experience:

* Classification and clustering algorithms applied in medical imaging/histology field in a mish mash of R scripts, some published but never really rigorous or vetted I guess
* Using numpy to implement Q-learning in a little baby neural net on the OpenAI cart-pole example
* Helping write components of a django app that does lots of data integration from multiple sources
* I still really like math after undergrad

Any and all advice appreciated. Should I dust off a linear algebra textbook? Should I start in on a class textbook before the term starts? Try to play with a big sample dataset with different learning tools?

Some example classes I want to take below:

Machine learning
&gt; This course will introduce the field of machine learning, in particular focusing on the core concepts of supervised and unsupervised learning. In supervised learning we will discuss algorithms which are trained on input data labelled with a desired output, for instance an image of a face and the name of the person whose face it is, and learn a function mapping from the input to the output. Unsupervised learning aims to discover latent  structure in an input signal where no output labels are available, an example of which is grouping web-pages based on the topics they discuss. Students will learn the algorithms which underpin many popular machine learning techniques, as well as developing an understanding of the theoretical relationships between these algorithms. The practicals will concern the application of machine learning to a range of real-world problems.

Foundations of CS (since I have no formal CS training)
&gt; Students taking this course will gain background knowledge that will be useful in the course on:
&gt; 
&gt; Theory of Data &amp; Knowledge Bases
&gt; Automata, Logics &amp; Games
&gt; Software Verification
&gt; Categories, Proofs &amp; Processes
&gt; Game Semantics
&gt; Computer-Aided Formal Verification
&gt; Lambda Calculus &amp; Types
&gt; Logic of Multi-Agent Information Flow 

Artificial Intelligence (intro)

&gt; This is an introductory course into the field of artificial intelligence (AI), with particular focus on search as the fundamental technique for solving AI problems.
&gt; 
&gt; This course also deals with optimization problems. For example, the optimization version of the nqueens problem is to arrange n queens on an n x n chessboard while minimizing the number of pairs of queens that are under attack. Such problems can be effectively solved by search techniques introduced in the course such as hill climbing, simulated annealing, and genetic algorithms...

Computational learning theory (I see this as directly applicable to my goals)
&gt; The course will begin by providing a statistical and computational toolkit, such as concentration inequalities, fundamental algorithms, and methods to analyse learning algorithms. We will cover questions such as when can we generalise well from limited amounts of data, how can we develop algorithms that are computationally efficient, and understand statistical and computational trade-offs in learning algorithms. We will also discuss new models designed to address relevant practical questions of the day, such as learning with limited memory, communication, privacy, and labelled and unlabelled data. In addition to core concepts from machine learning, we will make connections to principal ideas from information theory, game theory and optimisation.",2,5
259,2019-5-5,2019,5,5,4,bkp0mz,Advice on managing large time series dataset for deep learning processing,https://www.reddit.com/r/MachineLearning/comments/bkp0mz/advice_on_managing_large_time_series_dataset_for/,mikolchon,1556999298,[removed],0,1
260,2019-5-5,2019,5,5,5,bkp7tk,RNN (LSTM) in forecasting,https://www.reddit.com/r/MachineLearning/comments/bkp7tk/rnn_lstm_in_forecasting/,curioussouya,1557000446,[removed],0,1
261,2019-5-5,2019,5,5,5,bkpe6g,Gradient Descent Optimization [Part 1],https://www.reddit.com/r/MachineLearning/comments/bkpe6g/gradient_descent_optimization_part_1/,msminhas93,1557001458,,0,1
262,2019-5-5,2019,5,5,5,bkpq26,A unified approach to explain the output of any machine learning model.,https://www.reddit.com/r/MachineLearning/comments/bkpq26/a_unified_approach_to_explain_the_output_of_any/,_quanttrader_,1557003423,,0,1
263,2019-5-5,2019,5,5,6,bkq49i,Linear Algebra,https://www.reddit.com/r/MachineLearning/comments/bkq49i/linear_algebra/,Abdullahsaadf,1557005752,[removed],0,1
264,2019-5-5,2019,5,5,6,bkq50d,looking for career advice,https://www.reddit.com/r/MachineLearning/comments/bkq50d/looking_for_career_advice/,ajaybigman,1557005885,[removed],0,1
265,2019-5-5,2019,5,5,8,bkr5ut,[D] On the relevance of Conditional Random Fields,https://www.reddit.com/r/MachineLearning/comments/bkr5ut/d_on_the_relevance_of_conditional_random_fields/,RudyWurlitzer,1557012292,"Do you still use Conditional Random Fields for your sequence labeling tasks? If you do, when do you prefer them over bidirectional RNNs? 

I'm thinking about the need of explaining CRFs in the additional materials for my book's wiki. Not sure if CRF is still worth being known in detail.",7,20
266,2019-5-5,2019,5,5,9,bkrn3i,[P] styleGAN trained on album covers,https://www.reddit.com/r/MachineLearning/comments/bkrn3i/p_stylegan_trained_on_album_covers/,shoeblade,1557015449,"styleGAN trained on \~50,000 hand selected album covers.

&amp;#x200B;

A few animations:

[ANIMATION 1](https://twitter.com/artBoffin/status/1124030546999500800)

[ANIMATION 2](https://twitter.com/artBoffin/status/1124082075479543808)

[ANIMATION 3](https://twitter.com/artBoffin/status/1124367478107152384)

[YOUTUBE COLLECTION](https://youtu.be/RtPGIuAZpAo)

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/1ojnlq38caw21.jpg

https://i.redd.it/5k6bpo38caw21.jpg

https://i.redd.it/3ockor38caw21.jpg

https://i.redd.it/2h5ei614caw21.jpg

https://i.redd.it/xmtsk214caw21.jpg

https://i.redd.it/sihdw314caw21.jpg

https://i.redd.it/8lc64s14caw21.jpg",54,213
267,2019-5-5,2019,5,5,9,bkrxbq,"What are the differences between kubeflow, horovod, spark and Polyaxon for distributed deep learning",https://www.reddit.com/r/MachineLearning/comments/bkrxbq/what_are_the_differences_between_kubeflow_horovod/,Sorenmc,1557017273,[removed],0,1
268,2019-5-5,2019,5,5,10,bksd47,Expectations vs reality,https://www.reddit.com/r/MachineLearning/comments/bksd47/expectations_vs_reality/,da_chosen1,1557020205,,0,1
269,2019-5-5,2019,5,5,10,bksi5m,"[R] Learn Stereo, Infer Mono: Siamese Networks for Self-Supervised, Monocular, Depth Estimation",https://www.reddit.com/r/MachineLearning/comments/bksi5m/r_learn_stereo_infer_mono_siamese_networks_for/,downtownslim,1557021166,,11,11
270,2019-5-5,2019,5,5,11,bkslk6,[R] Moving beyond reward prediction errors,https://www.reddit.com/r/MachineLearning/comments/bkslk6/r_moving_beyond_reward_prediction_errors/,ActualPlum,1557021802,"Abstract: Classic theories of reinforcement learning and neuromodulation rely on reward prediction errors. A new machine learning technique relies on neuromodulatory signals that are optimized for specific tasks, which may lead to better AI and better explanations of neuroscience data.

Paper: [https://www.nature.com/articles/s42256-019-0053-0](https://www.nature.com/articles/s42256-019-0053-0)",2,1
271,2019-5-5,2019,5,5,11,bksqn1,Ask for feedback to our AI based platform,https://www.reddit.com/r/MachineLearning/comments/bksqn1/ask_for_feedback_to_our_ai_based_platform/,yoshuakishi,1557022767,,0,0
272,2019-5-5,2019,5,5,11,bksvid,Balancing Game with a Neural Network,https://www.reddit.com/r/MachineLearning/comments/bksvid/balancing_game_with_a_neural_network/,venturepulse,1557023739,"# Short Description

Let's suppose a  game has 50 levels given to each player in the random order (levels never repeat). There is analytics data of 1000 players with the events of the following type:

\- Level Start

\- Level Failed

\- Level Success

Each event has a timestamp and a unique player ID. We can safely assume that after 7 days of not being active, player had left the game forever. 

# The idea

We are thinking of using the neural networks to arrange the randomised levels in the order that would **maximize the number of level restart events**, and **maximize the number of levels the player would be able to complete**. Given the user retention metric is maximized as well. We need a program that would receive an array of events as input, arrange the levels in the best order and output the list of level IDs with the difficulty indices that we can use to order the levels in the game.

&amp;#x200B;

Anybody here could help us out on this one? Project will be paid.",0,1
273,2019-5-5,2019,5,5,12,bkt3gq,"Top stories: A box jellyfish antivenom, the quest for the color blue, and Isle Royales new wolves",https://www.reddit.com/r/MachineLearning/comments/bkt3gq/top_stories_a_box_jellyfish_antivenom_the_quest/,eslamalmahdy,1557025315,,0,0
274,2019-5-5,2019,5,5,12,bkt8vh,[D] How can I test/run code on a remote PC in real time?,https://www.reddit.com/r/MachineLearning/comments/bkt8vh/d_how_can_i_testrun_code_on_a_remote_pc_in_real/,Mogen1000,1557026378,"So my scenario is:

1. I want to edit on my Mac
2. I have a powerful (Windows) PC that I'd like to run code on.
3. Is there a way to bring the two together?

I'd like another opinion before I get started with one solution. I'm considering:

* Remote desktop (very slow and sluggish, but could work)
* Run a Jupyter docker image on Windows and access it on my Mac (My go-to IDE for Python is PyCharm, and I've never used Jupyter so I'm not sure if that is a good compromise)

Any suggestions?",4,4
275,2019-5-5,2019,5,5,12,bktaj6,[P] Chess piece recognition,https://www.reddit.com/r/MachineLearning/comments/bktaj6/p_chess_piece_recognition/,nixt26,1557026706,Hi all. I am working on a project where I want to identify chess pieces. I found that several people have solved this in different ways. Originally I wanted to identify real chess pieces from an overhead camera but I think now I will instead use flat pieces like this https://smile.amazon.com/gp/product/B0019F8QHO/ref=crt_ewc_title_srh_2?ie=UTF8&amp;psc=1&amp;smid=A3IXHP9WB8UQLD to make my job a bit easier and get something working. What do you guys think would be a good way to approach this problem?,8,3
276,2019-5-5,2019,5,5,12,bktc4o,[D] Automatic cherry picking good quality samples from GAN/VAE generated images using some metrics?,https://www.reddit.com/r/MachineLearning/comments/bktc4o/d_automatic_cherry_picking_good_quality_samples/,lzhbrian,1557027020,Any ideas on how to do cherry picking automatically rather then human effort? Maybe using some metrics (e.g. Inception Score) ?,11,10
277,2019-5-5,2019,5,5,16,bkv0ne,[D] Information-Theoretic Considerations in Batch Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/bkv0ne/d_informationtheoretic_considerations_in_batch/,mellow54,1557040733,,2,13
278,2019-5-5,2019,5,5,16,bkv7dn,Consumer Perspective on the world of Artificial Intelligence - Thesis - AI in FMCG companies to create competitive advantage [Research],https://www.reddit.com/r/MachineLearning/comments/bkv7dn/consumer_perspective_on_the_world_of_artificial/,Wayne66678,1557042392,,0,1
279,2019-5-5,2019,5,5,16,bkv8ca,So proud of how far Machine Learning has come! ,https://www.reddit.com/r/MachineLearning/comments/bkv8ca/so_proud_of_how_far_machine_learning_has_come/,Spangled_Metaphysics,1557042644,,0,1
280,2019-5-5,2019,5,5,17,bkvkhq,[D] A simple Google Colab playground with the new GPT-2 345M model,https://www.reddit.com/r/MachineLearning/comments/bkvkhq/d_a_simple_google_colab_playground_with_the_new/,mrconter1,1557045703,,0,1
281,2019-5-5,2019,5,5,18,bkw0zp,FINAL YEAR ENGINEERING PROJECT,https://www.reddit.com/r/MachineLearning/comments/bkw0zp/final_year_engineering_project/,joeljuzreddit,1557049853,[removed],0,1
282,2019-5-5,2019,5,5,18,bkw2xp,"[D] What is the rationale behind self-attention equation and how did they came up with the concept query, key and value?",https://www.reddit.com/r/MachineLearning/comments/bkw2xp/d_what_is_the_rationale_behind_selfattention/,begooboi,1557050355,"I was reading this article by Jay Alammar [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) which explains the transformer model in simple english. There is a section where he is talking about self attention and how its calculated. Here he introduced the concepts of query, key, value and the self-attention equation. I understood how query, key, value and the self-attention equation are calculated but how the researches came up with the idea of these vectors and equation.

I know that we are using hidden states in rnn because it has all the previous informations inside it  and in a rnn model we pass this hidden state and new input to get new hidden state. In the article he says that 

&gt;query, kery and value are the abstractions that are useful for calculating and thinking about attention.

But why?  where is the concept of query, key and value come by? why can't it be just one vector say only k or two vector(q,k)? What is the significance of  these three vectors? Why this vector and the equation improve attention?

The next question is the embedding vector necessary for a self-attention model. If the input dimension  is small can't we use the input vector to calcualte query,key and value and remove the embedding layer completely?",10,187
283,2019-5-5,2019,5,5,19,bkwboo,Need help to approach this problem statement correctly,https://www.reddit.com/r/MachineLearning/comments/bkwboo/need_help_to_approach_this_problem_statement/,ArkFreestyle,1557052487,[removed],0,1
284,2019-5-5,2019,5,5,19,bkwe6f,AI CAN SURF INTERNET!,https://www.reddit.com/r/MachineLearning/comments/bkwe6f/ai_can_surf_internet/,u_jjWAL_KumAr,1557053100,I am working on an AI that can surf the internet does anyone wanna collaborate?,0,1
285,2019-5-5,2019,5,5,20,bkwms8,[D] Are recurrent neural networks being phased out?,https://www.reddit.com/r/MachineLearning/comments/bkwms8/d_are_recurrent_neural_networks_being_phased_out/,SpazMunky,1557055111,"It seems to me that RNNs are becoming less popular architectures.  In the most recent papers on sequential data I've been reading (video, TTS, language modeling) I've more frequently seen convolutions over the time axis or transformers being implemented.  Are RNNs still achieving high performance in any particular area?",48,42
286,2019-5-5,2019,5,5,20,bkwoqg,"[D] PyTorch 1.1.0 brings TensorBoard support, so what happens to Visdom?",https://www.reddit.com/r/MachineLearning/comments/bkwoqg/d_pytorch_110_brings_tensorboard_support_so_what/,matchagreenteaislife,1557055555,,7,22
287,2019-5-5,2019,5,5,20,bkwsyy,Running github repo in colab,https://www.reddit.com/r/MachineLearning/comments/bkwsyy/running_github_repo_in_colab/,Bishwa12,1557056519,[removed],0,1
288,2019-5-5,2019,5,5,21,bkx30l,[D] why mini-batch doesn't work?,https://www.reddit.com/r/MachineLearning/comments/bkx30l/d_why_minibatch_doesnt_work/,sjh9020,1557058705,"I'm working on a img2img.

my model works when i use batch size 1. However it doesn't work with batch size &gt; 1.

there's no normalization layer.

&amp;#x200B;

Do you have any idea about is phenomenon??",2,0
289,2019-5-5,2019,5,5,21,bkx79q,Neighborhood methods for feature selection,https://www.reddit.com/r/MachineLearning/comments/bkx79q/neighborhood_methods_for_feature_selection/,houseonthecliff,1557059569,[removed],0,1
290,2019-5-6,2019,5,6,0,bkysvc,What will ethics mean to AGI?,https://www.reddit.com/r/MachineLearning/comments/bkysvc/what_will_ethics_mean_to_agi/,klyndonlee,1557069035,,1,2
291,2019-5-6,2019,5,6,0,bkz0kq,Natural Language Processing: What is it and how to use it in your business,https://www.reddit.com/r/MachineLearning/comments/bkz0kq/natural_language_processing_what_is_it_and_how_to/,TheTesseractAcademy,1557070146,,0,1
292,2019-5-6,2019,5,6,1,bkzfz1,[D] Is neuromorphic hardware the future?,https://www.reddit.com/r/MachineLearning/comments/bkzfz1/d_is_neuromorphic_hardware_the_future/,MassivePellfish,1557072341,"Really inspiring article about why neuromorphic hardware will be the future for intelligence research.

&gt; But the fact that computers think very differently than our brains do actually gives them an advantage when it comes to tasks like number crunching, while making them decidedly primitive in other areas, such as understanding human speech or learning from experience. If scientists want to simulate a brain that can match human intelligence, let alone eclipse it, they may have to start with better building blockscomputer chips inspired by our brains.

--


&gt; Current artificial intelligence systems still trail the brain when it comes to flexibility and learning ability. Googles networks became very good at recognizing images of cats once they were shown 10 million images of cats, but if you show my two-year-old grandson one cat he will recognize cats for the rest of his life, says Furber.

https://www.the-scientist.com/features/building-a-silicon-brain-65738",16,28
293,2019-5-6,2019,5,6,1,bkzyez,"The US just sent 10 new scientists to Congress, including an ocean expert, a nurse, and a biochemist. Here's the full list.",https://www.reddit.com/r/MachineLearning/comments/bkzyez/the_us_just_sent_10_new_scientists_to_congress/,eslamalmahdy,1557074909,,0,0
294,2019-5-6,2019,5,6,2,bl03sv,New to ML and just wanted to get some advice/clarification,https://www.reddit.com/r/MachineLearning/comments/bl03sv/new_to_ml_and_just_wanted_to_get_some/,PrimeCodas,1557075615,[removed],0,1
295,2019-5-6,2019,5,6,2,bl03vl,"[Question] Audio pattern recognition on CPU with ML, or how to find similar audio parts in audio streams",https://www.reddit.com/r/MachineLearning/comments/bl03vl/question_audio_pattern_recognition_on_cpu_with_ml/,Shir_man,1557075624,[removed],0,1
296,2019-5-6,2019,5,6,2,bl0778,[D] is this really AI?,https://www.reddit.com/r/MachineLearning/comments/bl0778/d_is_this_really_ai/,bitcoinduh,1557076065,"I see this YouTube channel is posting videos with some sort of avatar doing motion capture not just of face but the whole body in real time which seems impressive, is this based on some research like paGAN http://www.hao-li.com/publications/papers/siggraphAsia2018PAGAN.pdf and how is it being done? https://youtu.be/dfJZe5i4Ah0",4,0
297,2019-5-6,2019,5,6,2,bl0dww,[D] New to ML and just wanted to get some advice/clarification,https://www.reddit.com/r/MachineLearning/comments/bl0dww/d_new_to_ml_and_just_wanted_to_get_some/,PrimeCodas,1557076998,"I am currently attempting to learn ML specifically object detection.

Usually I am a JS developer, but wanted to extend my knowledge and incorporate the two at some point.

I hope there are not any stupid questions here.

&amp;#x200B;

I am looking to create a project that recognises different elements drawn on a piece of paper, for example rectangles and squares of different dimensions.

I am currently having to create the images manually by drawing them myself, I am worried this data will become too bias as I know what I want the end result to be.

I then feed that data into CV2 to add a randomly generate background to the image as the first few runs I processed with TensorFlow seemed to pick up mostly the white areas and gave me false readings.

Once I have the background image I use labelImg.py to draw borders and label the areas which contain the elements.

My data it then ready to be trained below are the results I received:-

(I allowed the training to run through 20,000 steps before stopping and testing)

&amp;#x200B;

|Data Sets|Success Rate|
|:-|:-|
|25|75%|
|100|40%|
|2000|10%|

&amp;#x200B;

As you can see from the table above when testing against the same testing images, the more datasets I introduce the lower the success rate of recognising the boxes becomes.

&amp;#x200B;

**Question Time:-**

&amp;#x200B;

Is there a relation between the amount of steps the trainer should run multiplied by the amount of data sets provided?

eg.

|Data Sets|Steps|
|:-|:-|
|25|20,000|
|100|80,000|
|750|600,000|

&amp;#x200B;

Is it possible to over train making the machine only recognise the data sets you have provided?

&amp;#x200B;

Is there a place I can request data sets, I do not mind doing this manually, but for bias reasons, I am wondering if it is possible?

&amp;#x200B;

Is there an optimised configuration file for this particular task I am trying to achieve?

I am currently using the ssd\_mobilenet\_v2\_coco config file with some edits to image and batch size.

&amp;#x200B;

Is there any advice you would offer a ML noob to help progress their knowledge?

I have been watching quite a few videos on YouTube (most notably Gilbert Tanner and Sentdex) and they are getting amazing results with only a small dataset and 20,000 steps.

Also just to note, the loss rate sits under 0.8 whilst training around the 20,000 step mark.

&amp;#x200B;

I am hoping the question above are not too targeted and could also help other people who are starting out with ML.",2,1
298,2019-5-6,2019,5,6,3,bl0v67,GAN samples at 60FPS (500 epochs trained on GTX 1050Ti ),https://www.reddit.com/r/MachineLearning/comments/bl0v67/gan_samples_at_60fps_500_epochs_trained_on_gtx/,k4rth33k,1557079368,,0,3
299,2019-5-6,2019,5,6,3,bl0vr2,Deep metric learning survey,https://www.reddit.com/r/MachineLearning/comments/bl0vr2/deep_metric_learning_survey/,kdhht2334,1557079451,[removed],0,1
300,2019-5-6,2019,5,6,3,bl0zhz,finding state of the art,https://www.reddit.com/r/MachineLearning/comments/bl0zhz/finding_state_of_the_art/,eeng_,1557079968,[removed],0,1
301,2019-5-6,2019,5,6,3,bl108i,[R] Survey of deep metric learning,https://www.reddit.com/r/MachineLearning/comments/bl108i/r_survey_of_deep_metric_learning/,kdhht2334,1557080073,"Hi, guys!

  
I'm going to introduce deep metric learning techniques used in various places such as image retrieval application recently.

  
You can easily find many researches of deep metric learning from past to present in this repository.

&amp;#x200B;

Thanks!

&amp;#x200B;

Refer below link:

[https://github.com/kdhht2334/Survey\_of\_Deep\_Metric\_Learning](https://github.com/kdhht2334/Survey_of_Deep_Metric_Learning)",2,9
302,2019-5-6,2019,5,6,3,bl163a,Convex loss versus closed form solutions,https://www.reddit.com/r/MachineLearning/comments/bl163a/convex_loss_versus_closed_form_solutions/,theboofwarrior,1557080884,[removed],0,1
303,2019-5-6,2019,5,6,3,bl1eei,[P] Countries organized using t-SNE. Pick a country you like and you'll probably like the other countries near it! More info in the comments,https://www.reddit.com/r/MachineLearning/comments/bl1eei/p_countries_organized_using_tsne_pick_a_country/,DOZENS_OF_BUTTS,1557081988,,1,1
304,2019-5-6,2019,5,6,3,bl1iai,Grab bag of Neural Network Goodies,https://www.reddit.com/r/MachineLearning/comments/bl1iai/grab_bag_of_neural_network_goodies/,MusingEtMachina,1557082523,,0,1
305,2019-5-6,2019,5,6,4,bl1kt7,Resources for beginners.,https://www.reddit.com/r/MachineLearning/comments/bl1kt7/resources_for_beginners/,shikharsaxena08,1557082867,[removed],0,1
306,2019-5-6,2019,5,6,4,bl1pzl,Scaling of categorical variable in k means,https://www.reddit.com/r/MachineLearning/comments/bl1pzl/scaling_of_categorical_variable_in_k_means/,karan198850,1557083597,[removed],0,1
307,2019-5-6,2019,5,6,5,bl29ov,[D] Machine Learning - WAYR (What Are You Reading) - Week 62,https://www.reddit.com/r/MachineLearning/comments/bl29ov/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,1557086405,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|
|----|-----|-----|-----|-----|-----|-----|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)||

Most upvoted papers two weeks ago:

/u/PaintedOnCanvas: [Realistic Evaluation of Deep Semi-Supervised Learning Algorithms](https://arxiv.org/abs/1804.09170)

/u/mesayousa: [These guys](https://www.reddit.com/r/MachineLearning/comments/bggyq2/n_mlfinlab_python_package_released_advances_in/?utm_source=share&amp;amp;utm_medium=ios_app)

/u/_swish_: [Neural-symbolic learning](https://arxiv.org/abs/1711.03902)

Besides that, there are no rules, have fun.",6,59
308,2019-5-6,2019,5,6,6,bl31oa,Where to start for Social Network Analysis?,https://www.reddit.com/r/MachineLearning/comments/bl31oa/where_to_start_for_social_network_analysis/,pretysmitty,1557090385,[removed],0,1
309,2019-5-6,2019,5,6,6,bl3htq,[R] Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer,https://www.reddit.com/r/MachineLearning/comments/bl3htq/r_understanding_and_improving_interpolation_in/,PeterPrinciplePro,1557092769,,0,1
310,2019-5-6,2019,5,6,8,bl4ily,blackjack monte carlo exploring starts,https://www.reddit.com/r/MachineLearning/comments/bl4ily/blackjack_monte_carlo_exploring_starts/,ice109,1557098456,[removed],0,1
311,2019-5-6,2019,5,6,8,bl4mqf,Extract requests/orders from text advice,https://www.reddit.com/r/MachineLearning/comments/bl4mqf/extract_requestsorders_from_text_advice/,pineapplecantelope,1557099128,[removed],0,1
312,2019-5-6,2019,5,6,10,bl5q1i,[Resource] Bounding box annotation tools &amp; services,https://www.reddit.com/r/MachineLearning/comments/bl5q1i/resource_bounding_box_annotation_tools_services/,reimmoriks,1557105687,,0,1
313,2019-5-6,2019,5,6,10,bl5swb,Computer Vision (YOLO) - How does Non-Max Suppression use (IoU)?,https://www.reddit.com/r/MachineLearning/comments/bl5swb/computer_vision_yolo_how_does_nonmax_suppression/,bfahm,1557106201,[removed],0,1
314,2019-5-6,2019,5,6,11,bl6hii,How to keep evolving your analysis?,https://www.reddit.com/r/MachineLearning/comments/bl6hii/how_to_keep_evolving_your_analysis/,beginnerpython,1557110486,[removed],0,1
315,2019-5-6,2019,5,6,12,bl6ukl,Guide: How to setup the Coral Dev Board using Windows only,https://www.reddit.com/r/MachineLearning/comments/bl6ukl/guide_how_to_setup_the_coral_dev_board_using/,23f34ef32,1557112618,[removed],1,1
316,2019-5-6,2019,5,6,12,bl6wzh,Can you use Transformers for Non-NLP prediction tasks?,https://www.reddit.com/r/MachineLearning/comments/bl6wzh/can_you_use_transformers_for_nonnlp_prediction/,somethingstrang,1557113039,[removed],0,1
317,2019-5-6,2019,5,6,12,bl6zv0,OPENCV Kickstarter Campaign,https://www.reddit.com/r/MachineLearning/comments/bl6zv0/opencv_kickstarter_campaign/,spmallick,1557113532,"Dear Friends,   
We at OpenCV.org will launch a Kickstarter campaign in about a week for three courses

1. Computer Vision I: Introduction
2. Computer Vision II: Applications 
3. Deep Learning with PyTorch

Please sign up below to stay informed:  
[https://opencv.org/courses](https://opencv.org/courses?fbclid=IwAR09uJ6A2xS5KmeBb0xiPVPlmhyOV9yoywObXtPFUVKGZoQ13s2nQ6HMPQ4)

We need your help to spread the word. Please share this with your friends and colleagues.   
Many thanks,

Satya Mallick &amp; Gary Bradski

*Processing video b8mts7pagiw21...*",0,1
318,2019-5-6,2019,5,6,13,bl7abw,[R] Study shows that artificial neural networks can be used to drive brain activity.,https://www.reddit.com/r/MachineLearning/comments/bl7abw/r_study_shows_that_artificial_neural_networks_can/,insider_7,1557115433,"MIT neuroscientists have performed the most rigorous testing yet of computational models that mimic the brains visual cortex.

Using their current best model of the brains visual neural network, the researchers designed a new way to precisely control individual neurons and populations of neurons in the middle of that network. In an animal study, the team then showed that the information gained from the computational model enabled them to create images that strongly activated specific brain neurons of their choosing.

The findings suggest that the current versions of these models are similar enough to the brain that they could be used to control brain states in animals. The study also helps to establish the usefulness of these vision models, which have generated vigorous debate over whether they accurately mimic how the visual cortex works, says James DiCarlo, the head of MITs Department of Brain and Cognitive Sciences, an investigator in the McGovern Institute for Brain Research and the Center for Brains, Minds, and Machines, and the senior author of the study.

&amp;#x200B;

Full article:  [http://news.mit.edu/2019/computer-model-brain-visual-cortex-0502](http://news.mit.edu/2019/computer-model-brain-visual-cortex-0502) 

Science paper:  [https://science.sciencemag.org/content/364/6439/eaav9436](https://science.sciencemag.org/content/364/6439/eaav9436)  

Borxiv (open access): [https://www.biorxiv.org/content/10.1101/461525v1](https://www.biorxiv.org/content/10.1101/461525v1)",68,397
319,2019-5-6,2019,5,6,14,bl86zu,[D] Neural Networks Without Bias Terms Are Brightness Invariant,https://www.reddit.com/r/MachineLearning/comments/bl86zu/d_neural_networks_without_bias_terms_are/,you-get-an-upvote,1557121881,"What do matrix multiplication, ReLU, and max pooling all have in common?  Yes their [second derivatives are all zero](https://www.reddit.com/r/MachineLearning/comments/aqfl70/d_neural_networks_with_second_derivative_zero_ae/), but there is another interesting property that they all satisfy:

    f(a x) = a f(x)

Which means that, when you stack these on top of each other, scaling the input of the network by some constant is equivalent to scaling the output by some constant.  Moreover, there are cases where the scale of the output doesn't matter (e.g. if predicted classes are based on argmax of the network output).

This leaves us in an interesting situation where it's actually very easy to encode complete brightness invariance in a network -- where you can always brighten/darken an image by some factor without affecting its predictions (assuming you normalize the scale of the output in some way -- softmax, sphere projection, etc.).

I've trained models with and without biases on CIFAR and find they're both reasonable.  I suspect a more rigorous comparision would find that networks without bias terms tend to do marginally worse than models with them -- if only because they have more parameters (e.g. without bias terms you can't learn that airplane images are usually brighter than frog images).

But in the interest of developing networks that actually generalize well to the real world (not just the random sample of your data you held out as a test set) it seems like this a modest performance gap might be permissible, if it means you can be confident that your network will work well in significantly different lighting conditions.",22,38
320,2019-5-6,2019,5,6,14,bl888k,Vizdoom module cannot be found error.,https://www.reddit.com/r/MachineLearning/comments/bl888k/vizdoom_module_cannot_be_found_error/,GhettoBurger996,1557122145,"Hey all, I've attempted to install vizdoom several time but am unable to, whenever I attempt to run the code, I receive this error `ModuleNotFoundError: No module named 'vizdoom'.`

I've made sure that I installed to the python I'm running 3.6. And I also checked the directory usr/local/lib/python3.6/dispackages/vizdoom and the file seems to be in order.

&amp;#x200B;

Dont quite understand what seems to be the problem, any help is appreciated.",0,1
321,2019-5-6,2019,5,6,15,bl8acf,Good ML based research programmes (MS),https://www.reddit.com/r/MachineLearning/comments/bl8acf/good_ml_based_research_programmes_ms/,vector_machines,1557122564,"I am an undergraduate and have been doing ML/NLP/DL/RL for past two years and was interested in applying for master's. I wish to pursue PhD (in NLP/CV) at some point, so was hoping to join thesis based research programmes. I was hoping if someone could suggest good programmes (relatively less competetive). I have spent an entire week on csrankings.org and have shortlisted few colleges. Please don't suggest obvious ones (GaTech, berkeley etc.). I want to work under a really good advisor (more publications and citations) and where the chances of me working them is more and environment is purely research focussed (like MILA for ex.). I am looking for these programmes in US/Canada. If anyone has some prior experience with applications or a good idea, please give suggestions. I know it's subjective, but still a general idea would be helpful.",0,1
322,2019-5-6,2019,5,6,15,bl8ba7,[D] Good ML based Research Programmes,https://www.reddit.com/r/MachineLearning/comments/bl8ba7/d_good_ml_based_research_programmes/,vector_machines,1557122754,"I am an undergraduate and have been doing ML/NLP/DL/RL for past two  years and was interested in applying for master's. I wish to pursue PhD  (in NLP/CV) at some point, so was hoping to join thesis based research  programmes. I was hoping if someone could suggest good programmes  (relatively less competetive). I have spent an entire week on  csrankings.org and have shortlisted few colleges. Please don't suggest  obvious ones (GaTech, berkeley etc.). I want to work under a really good  advisor (more publications and citations) and where the chances of me  working them is more and environment is purely research focussed (like  MILA for ex.). I am looking for these programmes in US/Canada. If anyone  has some prior experience with applications or a good idea, please give  suggestions. I know it's subjective, but still a general idea would be  helpful.",8,2
323,2019-5-6,2019,5,6,15,bl8bv4,[D] Should we just conclude the human brain is a sophisticated DL model?,https://www.reddit.com/r/MachineLearning/comments/bl8bv4/d_should_we_just_conclude_the_human_brain_is_a/,stronomia,1557122875,"The more I get into DL, the more I'm convinced our brains are just a DL model created by some Alien race on (god forbid) Caffe.

I recently came across an article about [Saccadic masking](https://www.boredpanda.com/brain-effects-explanation-vision-saccades-twitter/?utm_source=google&amp;utm_medium=organic&amp;utm_campaign=organic), and it got me thinking. Actually it just falls into place with this theory I've had for a while, and it's quite simple: We're running a very advanced version of some DL model (hopefully Resnet50).

I know we've moved away from the brain and neural network comparisons in recent years, but I think it's time we bring them back.",8,0
324,2019-5-6,2019,5,6,15,bl8eyb,Causal Convolution vs RNN in Wavenet,https://www.reddit.com/r/MachineLearning/comments/bl8eyb/causal_convolution_vs_rnn_in_wavenet/,khawarizmy,1557123503,[removed],0,1
325,2019-5-6,2019,5,6,15,bl8q2t,"[R] A PyTorch implementation of ""MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing"" (ICML 2019)",https://www.reddit.com/r/MachineLearning/comments/bl8q2t/r_a_pytorch_implementation_of_mixhop_higherorder/,benitorosenberg,1557125821,"&amp;#x200B;

*Processing img pjpxbslugjw21...*

GitHub: [https://github.com/benedekrozemberczki/MixHop-and-N-GCN](https://github.com/benedekrozemberczki/MixHop-and-N-GCN)

Paper: [https://arxiv.org/pdf/1905.00067.pdf](https://arxiv.org/pdf/1905.00067.pdf)

Abstract:

Recent methods generalize convolutional layers from Euclidean domains to  graph-structured data by approximating the eigenbasis of the graph  Laplacian. The computationally-efficient and broadly-used Graph ConvNet  of Kipf &amp; Welling, over-simplifies the approximation, effectively  rendering graph convolution as a neighborhood-averaging operator. This  simplification restricts the model from learning delta operators, the  very premise of the graph Laplacian.  In this work, we propose a new  Graph Convolutional layer which mixes multiple powers of the adjacency  matrix, allowing it to learn delta operators. Our layer exhibits the  same memory footprint and computational complexity as a GCN. We  illustrate the strength of our proposed layer on both synthetic graph  datasets, and on several real-world citation graphs, setting the record  state-of-the-art on Pubmed.",5,23
326,2019-5-6,2019,5,6,16,bl92tl,[D]Understanding GLOW/RealNVP loss function,https://www.reddit.com/r/MachineLearning/comments/bl92tl/dunderstanding_glowrealnvp_loss_function/,chinmay19,1557128340,"I am trying to implement a GLOW model for a project work. While working on it I came across this implementation [https://github.com/chrischute/glow](https://github.com/chrischute/glow). I don't understand how does the calculation of prior\_ll work here in the loss function.

It would be great if anyone could help me understand it!",2,5
327,2019-5-6,2019,5,6,16,bl94dy,Build A Complete Project In Machine Learning | Credit Card Fraud Detection,https://www.reddit.com/r/MachineLearning/comments/bl94dy/build_a_complete_project_in_machine_learning/,Aisha_b,1557128662,,0,1
328,2019-5-6,2019,5,6,17,bl9boi,"I have a problem I'd like to solve using ML, but don't know what keywords to search : Help?",https://www.reddit.com/r/MachineLearning/comments/bl9boi/i_have_a_problem_id_like_to_solve_using_ml_but/,TulaLuLaLeeey,1557130199,[removed],0,1
329,2019-5-6,2019,5,6,18,bla3pr,Questioning google scholar and access to the state of the art in general,https://www.reddit.com/r/MachineLearning/comments/bla3pr/questioning_google_scholar_and_access_to_the/,strojax,1557136208,[removed],0,1
330,2019-5-6,2019,5,6,19,bla8z6,AI for Good - Sustainability - whatweknow,https://www.reddit.com/r/MachineLearning/comments/bla8z6/ai_for_good_sustainability_whatweknow/,sherozekhan,1557137274,,0,1
331,2019-5-6,2019,5,6,19,blabcb,Is it okay to learn lots of different programming things?,https://www.reddit.com/r/MachineLearning/comments/blabcb/is_it_okay_to_learn_lots_of_different_programming/,AlwaysPersistent,1557137752,[removed],0,1
332,2019-5-6,2019,5,6,19,blamy8,[D] What is the current state-of-art in 3D Human Pose Estimation? (CVPR 2019 SOTA?),https://www.reddit.com/r/MachineLearning/comments/blamy8/d_what_is_the_current_stateofart_in_3d_human_pose/,tensor_x,1557140078,ECCV 2018 had BodyNet which was SOTA. What is CVPR 2019 state of art?,2,21
333,2019-5-6,2019,5,6,20,blaphc,Storing weights from a CNN model to overlay onto the original input images,https://www.reddit.com/r/MachineLearning/comments/blaphc/storing_weights_from_a_cnn_model_to_overlay_onto/,Doraguniru92,1557140555," 

Hey all,

I'm trying to save weights/convolutional layer outputs from a model and overlay them on top of the input images or image to be classified.

So far I've tried this:

&amp;#x200B;

init = tf.global\_variables\_initializer()

epochs = 100

\# Add ops to save and restore all the variables.

saver = tf.train.Saver()

with tf.Session() as sess:

[sess.run](https://sess.run)(init)

for i in range(epochs):

batch\_x, batch\_y = mnist.train.next\_batch(50)

[sess.run](https://sess.run)(

train,

feed\_dict={

x: batch\_x,

y\_true: batch\_y})

\# Save the variables to disk.

save\_path = [saver.save](https://saver.save)(sess, ""/tmp/model.ckpt"")



\# delete the current graph

tf.reset\_default\_graph()

\# Add ops to save and restore all the variables.

saver = tf.train.Saver()

\# Later, launch the model, use the saver to restore variables from disk, and

\# do some work with the model.

with tf.Session() as sess:

  \# Restore variables from disk.

  saver.restore(sess, ""/tmp/model.ckpt"")

&amp;#x200B;

But I get an error saying ""No variables to save"".

Any help is appreciated, thanks.",0,1
334,2019-5-6,2019,5,6,20,blb3q7,Awesome-Mobility-Machine-Learning-Contents,https://www.reddit.com/r/MachineLearning/comments/blb3q7/awesomemobilitymachinelearningcontents/,Kyle_Byeon,1557143093,"In mobility, There are many problem to solve. 

For example, map matching, route planning, eta, traffic estimation, dispatching strategy, dynamic pricing, supply and demand forecasting, etc

&amp;#x200B;

I just created a github repository that is a list of mobility industry(Transportation) papers and contents for studying in company.

&amp;#x200B;

If you are interested, please visit my github repository: [https://github.com/zzsza/Awesome-Mobility-Machine-Learning-Contents](https://github.com/zzsza/Awesome-Mobility-Machine-Learning-Contents)

&amp;#x200B;

welcome to star or create a pull request.

Thank you for your reading :)",0,1
335,2019-5-6,2019,5,6,20,blb5db,[P] Nextjournal: Hosted ML notebook platform,https://www.reddit.com/r/MachineLearning/comments/blb5db/p_nextjournal_hosted_ml_notebook_platform/,philippmarkovics,1557143377,"Hi all,

for the past 3 years weve been working on a new computational notebook platform for Data Science: Nextjournal. Our goals were to make computation more accessible and automatically reproducible, so it becomes easier to collaborate and build on top of each others work. With **full GPU support and templates for Tensorflow, Keras, PyTorch, TFLearn**, etc. its quite easy to get started on a new ML project in Nextjournal. Theres also [a collection of ML notebooks](https://nextjournal.com/collection/machine-learning) already published on out platform.

&amp;#x200B;

We opened signups today so if this sounds interesting to you head over to [nextjournal.com](https://nextjournal.com) and check it out.

&amp;#x200B;

Heres a list of what else Nextjournal can do:

&amp;#x200B;

* Nextjournal **runs Python, R, Julia and Clojure** all in the same notebook. Let docs, code completion and per-line errors help you out along the way.
* **Import your existing notebooks** (Jupyter, RMarkdown and Markdown) or get started with an existing template.
* Install any package or system library you need. In Nextjournal you have **full access to the filesystem** to install anything you want. With the click of a button, you can **save your environment reproducibly as a docker image**.
* In Nextjournal, your notebook and your data is **versioned automatically**, all the time. If you break something, simply restore a previous version.
* Easily mount your S3 or Google Buckets or import private GitHub repositories. Secrets are stored encrypted.
* Share drafts using a secret URL or invite collaborators to edit in real time. 

&amp;#x200B;

Happy to answer any questions that come up!",18,26
336,2019-5-6,2019,5,6,20,blb78n,[P] Awesome-Mobility-Machine-Learning-Contents,https://www.reddit.com/r/MachineLearning/comments/blb78n/p_awesomemobilitymachinelearningcontents/,Kyle_Byeon,1557143693,[removed],0,3
337,2019-5-6,2019,5,6,22,blc479,Amazon HQ2 tweet data GitHub link,https://www.reddit.com/r/MachineLearning/comments/blc479/amazon_hq2_tweet_data_github_link/,jbuddy_13,1557148928,[removed],0,1
338,2019-5-6,2019,5,6,23,blcmkp,[Q] why my neural net is so sensitive?,https://www.reddit.com/r/MachineLearning/comments/blcmkp/q_why_my_neural_net_is_so_sensitive/,sjh9020,1557151689,"My model sometimes learns very well,  but sometimes it fails to learn even with the same hyperparameters.

Is this possible?",0,1
339,2019-5-6,2019,5,6,23,bld0u1,BDD100k pre-trained model,https://www.reddit.com/r/MachineLearning/comments/bld0u1/bdd100k_pretrained_model/,stormzhou,1557153777,[removed],0,1
340,2019-5-6,2019,5,6,23,bld505,[N] ICLR 2019 Live Stream,https://www.reddit.com/r/MachineLearning/comments/bld505/n_iclr_2019_live_stream/,fr3cky,1557154391,,0,1
341,2019-5-7,2019,5,7,0,blddxi,Google at ICLR 2019,https://www.reddit.com/r/MachineLearning/comments/blddxi/google_at_iclr_2019/,sjoerdapp,1557155646,,0,1
342,2019-5-7,2019,5,7,0,blde7x,Will a basic Statistics class will help with Machine Learning and Business Intelligence?,https://www.reddit.com/r/MachineLearning/comments/blde7x/will_a_basic_statistics_class_will_help_with/,DevOpsMagilicutty,1557155681,[removed],0,1
343,2019-5-7,2019,5,7,0,bldq3a,Machine learning applications in pharmacy/pharmaceuticals,https://www.reddit.com/r/MachineLearning/comments/bldq3a/machine_learning_applications_in/,Vivus_Quotidie,1557157318,[removed],0,1
344,2019-5-7,2019,5,7,0,bldwsz,[P] phrase - a super fast CLI tool for learning significant phrases and terms,https://www.reddit.com/r/MachineLearning/comments/bldwsz/p_phrase_a_super_fast_cli_tool_for_learning/,blowjobtransistor,1557158233,,0,1
345,2019-5-7,2019,5,7,1,ble8lt,Seeking advice: Which Library for applying Deep RL?,https://www.reddit.com/r/MachineLearning/comments/ble8lt/seeking_advice_which_library_for_applying_deep_rl/,NP-intermediate,1557159811,"Hi everyone!

&amp;#x200B;

For my research/thesis I want to apply Deep RL to a problem. In order to do that, I have implemented the problem as a custom Gym Environment.

Now, I want to use a Library like (Stable-)Baselines or Ray/RLlib to train an agent on the environment, but I am unsure which one is best suited for my problem.

&amp;#x200B;

**Strict requirements:**

* Be able to run on custom Gym environments
* Supports PPO and some other state of the art algorithms
* Be able to use a custom neural network

**Very nice to have:**

* Be able to use PyTorch for the custom neural network. (If not: Be able to use circular padding in the convolutions, since my Agent has a 360 degree view of the world.)
* Easy to use distributed training

&amp;#x200B;

*Libraries that I am aware of are:*

* RLlib (Ray), which does not seem to support PyTorch when it comes to SOTA algorithms like PPO.
* Baslines, which seems to be strictly worse than
* Stable-Baselines, which does not seem to support PyTorch
* TF.Agents, which looks cool but is quite young and would force my to switch to TF.

&amp;#x200B;

Overall, I am afraid that I have to switch to Tensorflow, in which case it would be a nice to have if the library is compatible with Tensorflow 2.0 as I hate the syntax of the old one. 

&amp;#x200B;

Any thoughts or pointers? 

Thank you in advance!",0,1
346,2019-5-7,2019,5,7,1,blea6w,Classifying Facial Emotions via Machine Learning,https://www.reddit.com/r/MachineLearning/comments/blea6w/classifying_facial_emotions_via_machine_learning/,sks147,1557160023,,0,1
347,2019-5-7,2019,5,7,1,bled7q,Breeding next generation of input nodes?,https://www.reddit.com/r/MachineLearning/comments/bled7q/breeding_next_generation_of_input_nodes/,Frosty_Friend,1557160447,[removed],0,1
348,2019-5-7,2019,5,7,2,blf3c2,Microsoft Build 2019 | AI-Driven Collaboration and Hybrid-Cloud Innovations for Microsoft 365 &amp;,https://www.reddit.com/r/MachineLearning/comments/blf3c2/microsoft_build_2019_aidriven_collaboration_and/,Yuqing7,1557163967,,0,1
349,2019-5-7,2019,5,7,2,blfchv,[Research] Unveiled: Top 9 Challenges of Practical Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/blfchv/research_unveiled_top_9_challenges_of_practical/,cdossman,1557165213," [https://medium.com/ai%C2%B3-theory-practice-business/here-are-the-9-challenges-dragging-practical-rl-down-c96cc6366527](https://medium.com/ai%C2%B3-theory-practice-business/here-are-the-9-challenges-dragging-practical-rl-down-c96cc6366527) 

Abstract: Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are often hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. We present a set of nine unique challenges that must be addressed to productionize RL to real-world problems. For each of these challenges, we specify the exact meaning of the challenge, present some approaches from the literature, and specify some metrics for evaluating that challenge. An approach that addresses all nine challenges would be applicable to a large number of real-world problems. We also present an example domain that has been modified to present these challenges as a testbed for practical RL research.",0,2
350,2019-5-7,2019,5,7,3,blfhta,Machine Learning - MSys Training,https://www.reddit.com/r/MachineLearning/comments/blfhta/machine_learning_msys_training/,msystrainingllc,1557165927,[removed],0,0
351,2019-5-7,2019,5,7,3,blfmhf,Doing a project on Machine Learning as a career for Math Class,https://www.reddit.com/r/MachineLearning/comments/blfmhf/doing_a_project_on_machine_learning_as_a_career/,jcp419,1557166568,[removed],0,1
352,2019-5-7,2019,5,7,3,blfq2t,Machine Learning Certification Course - Msys Training,https://www.reddit.com/r/MachineLearning/comments/blfq2t/machine_learning_certification_course_msys/,msystrainingllc,1557167074,,0,0
353,2019-5-7,2019,5,7,3,blfu7s,[P] PyCM 2.1 released: Machine learning library for confusion matrix statistical analysis,https://www.reddit.com/r/MachineLearning/comments/blfu7s/p_pycm_21_released_machine_learning_library_for/,sepandhaghighi,1557167637," [https://github.com/sepandhaghighi/pycm](https://github.com/sepandhaghighi/pycm)

[http://www.pycm.ir](http://www.pycm.ir/)

&amp;#x200B;

  Changelog : 

* Adjusted geometric mean (AGM) added [\#177](https://github.com/sepandhaghighi/pycm/issues/177)
* Yule's Q (Q) added [\#193](https://github.com/sepandhaghighi/pycm/issues/193)
* Compare class and parameters recommendation system block diagrams added [\#192](https://github.com/sepandhaghighi/pycm/issues/192)
* Document links bug fixed [\#194](https://github.com/sepandhaghighi/pycm/issues/194)
* Document modified [\#173](https://github.com/sepandhaghighi/pycm/issues/173)",1,7
354,2019-5-7,2019,5,7,3,blg52e,Scaling PyTorch Transformer-XL to 128 GPUs,https://www.reddit.com/r/MachineLearning/comments/blg52e/scaling_pytorch_transformerxl_to_128_gpus/,yaroslavvb,1557169171,[removed],0,1
355,2019-5-7,2019,5,7,4,blgmwc,[P] Doing a project on machine learning as a career for my geometry class,https://www.reddit.com/r/MachineLearning/comments/blgmwc/p_doing_a_project_on_machine_learning_as_a_career/,jcp419,1557171642,"Hey everyone, I'm in 10th grade and as the title says I'm working on a project for my geometry class and using machine learning as my subject. The project is basically to say how mathematics are used in a career of our choice. The career I've chosen to do my project on is machine learning engineer. As a part of the project, we have to conduct a short interview with someone in that career as part of the project. I don't know anyone in real life who is involved in this field so I was hoping that if any of you who work in ML could answer some short, non-technical questions I have below. Thanks so much!  

I posted this in r/MLQuestions and someone suggested I try it here instead and use a google form rather than listing the questions out.  

The Questions:  

[https://forms.gle/mt4WoqXybMWhFMdD7](https://forms.gle/mt4WoqXybMWhFMdD7)  

Thanks so much for taking the time to answer these! Have a good day.",63,182
356,2019-5-7,2019,5,7,5,blgywx,[D] Average Perplexity Values,https://www.reddit.com/r/MachineLearning/comments/blgywx/d_average_perplexity_values/,AnonMLstudent,1557173304,What are average Perplexity values for sentences? I am using Flair to calculate perplexity for sentences I generate (on the One Billion Words language model) and get very low values like 4 or 5. I thought typical perplexity values are 20+ all the way to 100+ for less fluent sentences. Is there something I am overlooking?,4,1
357,2019-5-7,2019,5,7,5,blh6sp,Can we predict Bitcoin prices using Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/blh6sp/can_we_predict_bitcoin_prices_using_machine/,TheTesseractAcademy,1557174334,,0,1
358,2019-5-7,2019,5,7,6,blhos3,Does anyone know about MLDM conference?,https://www.reddit.com/r/MachineLearning/comments/blhos3/does_anyone_know_about_mldm_conference/,FearlessLavishness1,1557176746,[removed],0,1
359,2019-5-7,2019,5,7,6,blhukl,"[Research] Looking for text books, review papers or tutorials on generalization theory?",https://www.reddit.com/r/MachineLearning/comments/blhukl/research_looking_for_text_books_review_papers_or/,AlexSnakeKing,1557177532,"Hi all

I'm looking for a good textbook, or a review paper or tutorial on advances in generalization theory and learnability in the last 20 years, i.e what has happened since Vapnik's structural risk minimization and Schapire's boosting?

&amp;#x200B;

A lot seems to have gone on in terms regularization and resampling techniques, but I can't seem to find much in terms of actual learning theory.",2,12
360,2019-5-7,2019,5,7,6,bli5n1,Website which ranks performance on machine learning datasets?,https://www.reddit.com/r/MachineLearning/comments/bli5n1/website_which_ranks_performance_on_machine/,Philiatrist,1557179059,[removed],0,1
361,2019-5-7,2019,5,7,7,blik63,[D] - Ubuntu on Azure vs. Win64 native?,https://www.reddit.com/r/MachineLearning/comments/blik63/d_ubuntu_on_azure_vs_win64_native/,ItsmeTianab,1557181239,"This isn't the standard ""which is better, win or lin?"" question asked for the millionth time.  Rather, is Microsoft's offering of a Ubuntu-based VM for machine learning indicative of any functional deficiency in the native Win64 platform?  That is, can all of the libraries/apps on the Ubuntu-based ML VM be run on Win64 native if desired?  This is purely a functional question, not considering cost or any OS-specific usage aesthetics or simplicity.

&amp;#x200B;

The reasoning for the question is to find out how easy it would be for a Win64-based researcher without an Azure subscription to collaborate with our staff who use Ubuntu on Azure.  In theory, would they be able to run the same code?",0,1
362,2019-5-7,2019,5,7,7,blikno,Open the Black Box: an Introduction to Model Interpretability with LIME and SHAP - Kevin Lemagnen,https://www.reddit.com/r/MachineLearning/comments/blikno/open_the_black_box_an_introduction_to_model/,_quanttrader_,1557181310,,0,1
363,2019-5-7,2019,5,7,7,blim8q,"[D] Is it just me, or are PhD opportunities in Europe very scarce at the moment?",https://www.reddit.com/r/MachineLearning/comments/blim8q/d_is_it_just_me_or_are_phd_opportunities_in/,BigMakondo,1557181549,"I've been looking for a PhD in machine learning/deep learning/computer vision/etc. for a while, but I don't seem to find many offers.

Leaving aside the UK for a moment (where perhaps there are a bit more options, but not that many), the options in the continent seem to be very rare.

I go to the labs websites, the uni pages, contact some profs, but I've barely found a handful of offers in ETH, KTH, TUM and Delft. And as soon as you start being a bit picky, you end up with just one or two offers on the table to apply to.

Has anyone from Europe experienced this too? 

I wonder if there are some mailing list that I'm unaware of, or if this has been the norm lately and positions go out on a rolling basis throughout the year. 

It's just weird reading about US people being rejected from so many PhD positions, but here I can't even apply to that many.

Any advice is appreciated.",59,4
364,2019-5-7,2019,5,7,8,bljkfs,Python for Data Science and Machine Learning Bootcamp on udemy,https://www.reddit.com/r/MachineLearning/comments/bljkfs/python_for_data_science_and_machine_learning/,S_Leonardo,1557186747,[removed],0,1
365,2019-5-7,2019,5,7,9,bljsr7,[D] Reproducibility as a Vehicle for Engineering Best Practices,https://www.reddit.com/r/MachineLearning/comments/bljsr7/d_reproducibility_as_a_vehicle_for_engineering/,rasmii,1557188034,,0,1
366,2019-5-7,2019,5,7,9,bljtdx,Looking for material/ideas on learning class conditioned dependency,https://www.reddit.com/r/MachineLearning/comments/bljtdx/looking_for_materialideas_on_learning_class/,shubhamdash,1557188132,"I need help with figuring out how to learn a model which encodes the class conditioned dependency of all the elements in a convolutional kernel. So each pixel/cell in the receptive field of the convolutional kernel has a class label, I want to learn the class of the middle pixel with additional conditional probability of each cell belonging to their corresponding labels. How do i do that?",0,1
367,2019-5-7,2019,5,7,9,blk2fh,Literature on Generalization for Medical Imaging,https://www.reddit.com/r/MachineLearning/comments/blk2fh/literature_on_generalization_for_medical_imaging/,mrobbins1019,1557189584,[removed],0,1
368,2019-5-7,2019,5,7,9,blk8qx,Scaling PyTorch Transformer-XL training to 128 GPUs,https://www.reddit.com/r/MachineLearning/comments/blk8qx/scaling_pytorch_transformerxl_training_to_128_gpus/,yaroslavvb,1557190594,"code: [https://github.com/cybertronai/transformer-xl](https://github.com/cybertronai/transformer-xl)

experiments: [https://medium.com/south-park-commons/scaling-transformer-xl-to-128-gpus-85849508ec35](https://medium.com/south-park-commons/scaling-transformer-xl-to-128-gpus-85849508ec35)",0,1
369,2019-5-7,2019,5,7,10,blkcsf,[P] AI Pinball Agent,https://www.reddit.com/r/MachineLearning/comments/blkcsf/p_ai_pinball_agent/,maniacalbrain,1557191224,,1,1
370,2019-5-7,2019,5,7,10,blkl1y,Identifying where a picture was taken,https://www.reddit.com/r/MachineLearning/comments/blkl1y/identifying_where_a_picture_was_taken/,ganesh_anand,1557192566,[removed],0,1
371,2019-5-7,2019,5,7,10,blkld0,How to output a list of categoricals using RNN?,https://www.reddit.com/r/MachineLearning/comments/blkld0/how_to_output_a_list_of_categoricals_using_rnn/,throwawayMLguy012,1557192614,[removed],0,1
372,2019-5-7,2019,5,7,10,blknqa,[R] Sketchforme: Composing Sketched Scenes from Text Descriptions for Interactive Applications,https://www.reddit.com/r/MachineLearning/comments/blknqa/r_sketchforme_composing_sketched_scenes_from_text/,wei_jok,1557192993,,1,17
373,2019-5-7,2019,5,7,10,blkp6b,[P] Identifying where a picture was taken,https://www.reddit.com/r/MachineLearning/comments/blkp6b/p_identifying_where_a_picture_was_taken/,ganesh_anand,1557193228,"I have a collection of images taken from different windows in buildings pointing out and down. I'm trying to identify the window location on pictures facing the building.  So far I've tried to just frame this as a regression problem with 3 images as input. See example below. 

*Processing img 7ch0vo241pw21...*

I want to know if this is an existing problem and if so what is the name for it. I've been stuck figuring out what exactly to search. The closest I've found is relative camera pose estimation but its not exactly what I want.",4,5
374,2019-5-7,2019,5,7,10,blkv45,Literature on Generalization in Medical Imaging,https://www.reddit.com/r/MachineLearning/comments/blkv45/literature_on_generalization_in_medical_imaging/,mrobbins1019,1557194189,"Hi All! I'm doing research on how well neural nets generalize between different medical sites and I'm curious if anyone knows of any good papers. I'm looking for things along the lines of [https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002683](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002683), but ideally with a more systematic consideration of different factors (machine manufacturer, resolution, slide thickness, etc.) and maybe some kind of helpful framework/technique for considering different features. Thanks!!",0,1
375,2019-5-7,2019,5,7,11,bll6zu,What computer to buy to start machine learning,https://www.reddit.com/r/MachineLearning/comments/bll6zu/what_computer_to_buy_to_start_machine_learning/,Peoplejudgeusernames,1557196121,[removed],0,1
376,2019-5-7,2019,5,7,11,bll7g8,Looking for GAN Paper,https://www.reddit.com/r/MachineLearning/comments/bll7g8/looking_for_gan_paper/,TruKian,1557196199,[removed],0,1
377,2019-5-7,2019,5,7,12,bllxuj,[R] Future of Data Processing and Sharing Survey,https://www.reddit.com/r/MachineLearning/comments/bllxuj/r_future_of_data_processing_and_sharing_survey/,usable-security-sba,1557200658,"Dear Redditors!

I am a researcher at [SBA Research](https://www.sba-research.org) **investigating current data processing and sharing** trends with an online survey and a subsequent interview on how people currently combine policy requirements (security policies, GDPR, etc) with their need of data processing and sharing for their business. The results will be used to help to design data processing systems that take these policies into account, yet still allow to extract value from data for businesses.

Please complete the brief pre-questionnaire below to participate in the interview. If you partake in the **interview** you will receive a **compensation** in the form of a **20 EUR Amazon** voucher for, at most, one hour of your time.

[https://de.surveymonkey.com/r/PDGXBM5](https://de.surveymonkey.com/r/PDGXBM5)

The **pre-questionnaire** should only take **3-5 minutes**. The subsequent **interview** will be **one hour** at most and both your responses as part of the questionnaire and the interview are completely anonymous.

Feel free to share this information with colleagues and acquaintances that you think might be interested to participate. On behalf of science I would really appreciate your input to this study!",0,2
378,2019-5-7,2019,5,7,13,blm5ax,[R] Image Classification at Supercomputer Scale,https://www.reddit.com/r/MachineLearning/comments/blm5ax/r_image_classification_at_supercomputer_scale/,ignacioruiz,1557202018,"If you're interested on taking your image recognition solution to the next level efficiently using GPUs/TPUs, multi-threading and distributed computing, you may need to reorganize your processing. 

Watch this introductory video on how to achieve this and you may get 4X performance improvements!!!

[https://youtu.be/JvssZESVcjI](https://youtu.be/JvssZESVcjI)

the arXiv link to the paper is included!!!",3,6
379,2019-5-7,2019,5,7,13,blmamc,[P] Awesome-Mobility-Machine-Learning-Contents!,https://www.reddit.com/r/MachineLearning/comments/blmamc/p_awesomemobilitymachinelearningcontents/,Kyle_Byeon,1557203023,[removed],0,1
380,2019-5-7,2019,5,7,13,blmetl,[P] Awesome-Mobility-Machine-Learning-Contents!,https://www.reddit.com/r/MachineLearning/comments/blmetl/p_awesomemobilitymachinelearningcontents/,Kyle_Byeon,1557203848,,0,1
381,2019-5-7,2019,5,7,13,blmkqc,What is the use of [SEP] in paper BERT?,https://www.reddit.com/r/MachineLearning/comments/blmkqc/what_is_the_use_of_sep_in_paper_bert/,romanshen,1557205001,[removed],0,1
382,2019-5-7,2019,5,7,14,blmoos,Quantum Computing Explained Simply And How Actually Quantum Computers Work,https://www.reddit.com/r/MachineLearning/comments/blmoos/quantum_computing_explained_simply_and_how/,technewsninja,1557205779,,0,1
383,2019-5-7,2019,5,7,14,blmpyo,[Project] Stylizer: A Couple of Experiments with Multi Style Transfer and Comixification in TF2.0,https://www.reddit.com/r/MachineLearning/comments/blmpyo/project_stylizer_a_couple_of_experiments_with/,suyash93,1557206032,[removed],0,1
384,2019-5-7,2019,5,7,14,blmyjn,Detect line segments on extreme ends in a cluster,https://www.reddit.com/r/MachineLearning/comments/blmyjn/detect_line_segments_on_extreme_ends_in_a_cluster/,digantpatel,1557207812,[removed],0,1
385,2019-5-7,2019,5,7,14,bln1ds,When PyTorch tells it like it is.,https://www.reddit.com/r/MachineLearning/comments/bln1ds/when_pytorch_tells_it_like_it_is/,ClydeMachine,1557208407,,0,1
386,2019-5-7,2019,5,7,14,bln1sl,[Discussion] or [Humour] When PyTorch tells it like it is.,https://www.reddit.com/r/MachineLearning/comments/bln1sl/discussion_or_humour_when_pytorch_tells_it_like/,ClydeMachine,1557208492,,0,1
387,2019-5-7,2019,5,7,15,blnf26,Telescopic Forks Automated Warehouse,https://www.reddit.com/r/MachineLearning/comments/blnf26/telescopic_forks_automated_warehouse/,lhd121,1557211386,,0,1
388,2019-5-7,2019,5,7,16,blnm1t,Artificial Intelligence Engines,https://www.reddit.com/r/MachineLearning/comments/blnm1t/artificial_intelligence_engines/,aiforworld2,1557212958,,0,1
389,2019-5-7,2019,5,7,16,blnn0r,Need recommendation on Keras/Tensorflow learning,https://www.reddit.com/r/MachineLearning/comments/blnn0r/need_recommendation_on_kerastensorflow_learning/,pirrencode,1557213196,"Hi everyone! Could someone recommend courses or notebooks in jupyter where to start in Keras/Tensorflow.

&amp;#x200B;

Thank you!",0,1
390,2019-5-7,2019,5,7,16,blnrtq,Advanced Analytics for Every Business User in Your Biz!,https://www.reddit.com/r/MachineLearning/comments/blnrtq/advanced_analytics_for_every_business_user_in/,ElegantMicroWebIndia,1557214289,,0,1
391,2019-5-7,2019,5,7,17,blo3ci,[Discussion] Does Global Optimizers such as BARON help training neural networks better?,https://www.reddit.com/r/MachineLearning/comments/blo3ci/discussion_does_global_optimizers_such_as_baron/,RTengx,1557217124,"  


*Processing img 8xszzhyfzqw21...*

I was wondering if the cost functions of neural network are non-convex, why are we still using stochastic gradient methods such as SGD or Adam?   


Does Global Optimization Solvers work? I would want to try BARON.

 [https://en.wikipedia.org/wiki/BARON](https://en.wikipedia.org/wiki/BARON)   


Does anyone has experience or know any reason why global optimizers are not being used?  


Maybe hardware is not good enough? But with all the google TPU and quantum computer being developed, I doubt this is the case.",22,11
392,2019-5-7,2019,5,7,18,blok80,"How is error evaluated in an unsupervised neural network, with no GAN or human intervention?",https://www.reddit.com/r/MachineLearning/comments/blok80/how_is_error_evaluated_in_an_unsupervised_neural/,2784564365353,1557221087,[removed],0,1
393,2019-5-7,2019,5,7,18,blomoc,[Discussion] How to interpret probabilities?,https://www.reddit.com/r/MachineLearning/comments/blomoc/discussion_how_to_interpret_probabilities/,Zman420,1557221661,"So I've worked with machine learning a lot, but never actually had to deal directly with probabilities/probability output, and what they actually mean in a real world scenario.

So, say we have 4 balanced classes [1,2,3, and 4], and we train a Keras classifier and get the probabilities for a given prediction in the test set:

GT: Class 1

Output: [0.4, 0.3, 0.1, 0.2]

Great - the classifier predicted Class 1 correctly. But how should we interpret the 0.4 probability that was attached to the class?

Does it mean that given that particular feature vector, class 1 will be the correct choice 40% of the time?  

Where does the actual performance/accuracy of the classifier come in to this?

For example, say the above classifier was trained with 100k training examples, had an accuracy of '50%', and produced the probabilities of [0.4, 0.3, 0.1, 0.2].  Now imagine I got more training data and trained with 500k training examples, achieved an accuracy of 60%, and still got the probabilities of [0.4, 0.3, 0.1, 0.2].  Does the more accurate classifier better represent the actual real-life probability of the events?

**tl;dr: I guess what I'm actually asking is - how do we know/measure/compare the accuracy of the probabilities given from a classifier?**",11,3
394,2019-5-7,2019,5,7,18,bloo16,Classification of sequence 4D array data with RNN-LSTM,https://www.reddit.com/r/MachineLearning/comments/bloo16/classification_of_sequence_4d_array_data_with/,uchihaZohair_,1557221962,[removed],0,1
395,2019-5-7,2019,5,7,18,blos8y,"[D] How do I check for originality of an image among others in a database, when the images have slight but distinguishable differences?",https://www.reddit.com/r/MachineLearning/comments/blos8y/d_how_do_i_check_for_originality_of_an_image/,SSuryansh,1557222888,"I understand that asking for an ""originality checking"" neural network is very vague since there are many parameters which can affect the originality of an image, but I'm curious.

&amp;#x200B;

I was thinking the Siamese network approach might be beneficial. What do you think?",37,2
396,2019-5-7,2019,5,7,19,bloutz,Parallelized Cross Entropy Method,https://www.reddit.com/r/MachineLearning/comments/bloutz/parallelized_cross_entropy_method/,ADGEfficiency,1557223425,,0,1
397,2019-5-7,2019,5,7,19,blow38,Help: ML topic for research project,https://www.reddit.com/r/MachineLearning/comments/blow38/help_ml_topic_for_research_project/,debayon,1557223702,[removed],0,1
398,2019-5-7,2019,5,7,19,blp01l,Perplexing perplexity scores,https://www.reddit.com/r/MachineLearning/comments/blp01l/perplexing_perplexity_scores/,no_bear_so_low,1557224539,[removed],0,1
399,2019-5-7,2019,5,7,19,blp0ax,Need help in understanding patent law for artificial intelligence? [Project],https://www.reddit.com/r/MachineLearning/comments/blp0ax/need_help_in_understanding_patent_law_for/,ulricz,1557224593,"Context : I am a business professional who recently joined an AI startup on voice. 

As my first project, I am tasked with understanding patent law better to package solutions we have built as potential patents. 

To do this, I am taking this approach. 

- Research as much as possible about patent law (stuck here) 
- formulate solutions basis our ml capabilities 
- file eventually for patents. 

What can be patented in artificial intelligence have very vague answers and differ everywhere. 

Any help you can provide as a starting point, information I could use or a better approach is appreciated.",6,6
400,2019-5-7,2019,5,7,20,blpgng,How to Become a Machine Learning Engineer?,https://www.reddit.com/r/MachineLearning/comments/blpgng/how_to_become_a_machine_learning_engineer/,ritesh1928,1557227920,,0,1
401,2019-5-7,2019,5,7,20,blpo7g,Wanted a corpus for financial data,https://www.reddit.com/r/MachineLearning/comments/blpo7g/wanted_a_corpus_for_financial_data/,Saizou1991,1557229363,[removed],0,1
402,2019-5-7,2019,5,7,21,blpwhv,[D] Tesla T4 GPUs cost nothing on Google Cloud?,https://www.reddit.com/r/MachineLearning/comments/blpwhv/d_tesla_t4_gpus_cost_nothing_on_google_cloud/,bolaft,1557230856,"I'm launching an instance in compute engine and I'm surprised to see that adding a Tesla T4 GPU to an instance does not change the estimated cost at all. The GPU doesn't even show up in cost details.

Here's a screenshot: https://i.imgur.com/wqhthS1.png

This seems super weird to me. Is it a bug?",62,193
403,2019-5-7,2019,5,7,21,blq01w,Google CallJoy &amp; the age of automation in communications,https://www.reddit.com/r/MachineLearning/comments/blq01w/google_calljoy_the_age_of_automation_in/,tsahil,1557231468,[removed],0,1
404,2019-5-7,2019,5,7,21,blq250,Tell Interactive Stories with Augmented Reality,https://www.reddit.com/r/MachineLearning/comments/blq250/tell_interactive_stories_with_augmented_reality/,hiwilliam31,1557231835,,0,1
405,2019-5-7,2019,5,7,22,blqmoe,Machine Learning Into The Void,https://www.reddit.com/r/MachineLearning/comments/blqmoe/machine_learning_into_the_void/,paradox44,1557235253,"I have been putting up code in parts to showcase the implementation for some problems in my repo. This might be something as a simple guide (but not comprehensive, I guess). The repo is:

[https://github.com/NISH1001/machine-learning-into-the-void](https://github.com/NISH1001/machine-learning-into-the-void)

&amp;#x200B;

The part for object localization is the ""complex"" part so far where I demonstrate the use of FCNN in building heatmap of concerned object (for now, it is dog), and use the heatmap to create a very raw bounding box based on the label 1/0 in each cell of the output grid. The notebook can be found here:

[https://github.com/NISH1001/machine-learning-into-the-void/blob/master/keras/object-localization-fcnn/03-bounding-box-object-localization-FCNN-imagenet-vgg16.ipynb](https://github.com/NISH1001/machine-learning-into-the-void/blob/master/keras/object-localization-fcnn/03-bounding-box-object-localization-FCNN-imagenet-vgg16.ipynb)

&amp;#x200B;

What do you think of this localization notebook? I am planning to do a detail blog on my personal site. I need some feedbacks on the notebook on what is ""boring"" or irrelevant or what can be added/improved?

&amp;#x200B;

Adding "":)"" just to make sure this post is not cringy....",0,1
406,2019-5-7,2019,5,7,22,blqr0i,"RL Weekly 17: Information Asymmetry in KL-regularized Objective, Real-world Challenges to RL, and Fast and Slow RL",https://www.reddit.com/r/MachineLearning/comments/blqr0i/rl_weekly_17_information_asymmetry_in/,seungjaeryanlee,1557235953,,0,1
407,2019-5-7,2019,5,7,23,blrhdi,[P] Replication and Comparisons of Disentangled VAE,https://www.reddit.com/r/MachineLearning/comments/blrhdi/p_replication_and_comparisons_of_disentangled_vae/,yannDubs,1557239969,"Hi r/MachineLearning,

With some friends, we replicated some of the most important disentangled VAE (beta-TCVAE, FAactorVAE, both versions of beta-VAE). The motivations were the following:
- Have a modular framework easily extendable for any type of VAE
- Separate the loss and the architecture to have a better understanding of what causes the improvements in new papers (many papers change both and it's thus hard to disentangle - no puns intended - where the improvements come from).
- Have a clean and easily extendable visualisation pipeline for VAEs
- Replicate quantitative metrics to compare disentanglement 

I hope you find it useful, and please let me know if there is anything you think would be useful to add (new disentangle VAE loss, add GAN for reconstruction loss, ...)

https://github.com/YannDubs/disentangling-vae

PS: it's my first post on reddit, please let me know if I'm doing something wrong.",15,8
408,2019-5-8,2019,5,8,0,blrq85,Receptive Field of Squeeze and Excitation Networks,https://www.reddit.com/r/MachineLearning/comments/blrq85/receptive_field_of_squeeze_and_excitation_networks/,nomad_world,1557241254,[removed],0,1
409,2019-5-8,2019,5,8,0,blrq96,Combining Satellite Imagery and machine learning to predict poverty,https://www.reddit.com/r/MachineLearning/comments/blrq96/combining_satellite_imagery_and_machine_learning/,jlopez-rumcs,1557241257,[removed],0,2
410,2019-5-8,2019,5,8,0,blrruq,[Research] A collection of 1 Million Computer-Aided Design (CAD) Models for Geometric Deep Learning Research,https://www.reddit.com/r/MachineLearning/comments/blrruq/research_a_collection_of_1_million_computeraided/,cdossman,1557241468," [https://medium.com/ai%C2%B3-theory-practice-business/abc-free-datasets-for-geometric-deep-learning-5e2995768b37](https://medium.com/ai%C2%B3-theory-practice-business/abc-free-datasets-for-geometric-deep-learning-5e2995768b37) 

Abstract:  We introduce ABC-Dataset, a collection of one million Computer-Aided Design (CAD) models for research of geometric deep learning methods and applications. Each model is a collection of explicitly parametrized curves and surfaces, providing ground truth for differential quantities, patch segmentation, geometric feature detection, and shape reconstruction.",0,31
411,2019-5-8,2019,5,8,0,blrt0q,"[N] RL Weekly 17: Information Asymmetry in KL-regularized Objective, Real-world Challenges to RL, and Fast and Slow RL",https://www.reddit.com/r/MachineLearning/comments/blrt0q/n_rl_weekly_17_information_asymmetry_in/,seungjaeryanlee,1557241624,,0,1
412,2019-5-8,2019,5,8,0,blry3p,Machine Learning Training In Bangalore,https://www.reddit.com/r/MachineLearning/comments/blry3p/machine_learning_training_in_bangalore/,SunilAhujaa,1557242346,[removed],0,1
413,2019-5-8,2019,5,8,0,blryfd,Verification of non linear specifications for neural networks,https://www.reddit.com/r/MachineLearning/comments/blryfd/verification_of_non_linear_specifications_for/,RickDeveloper,1557242392,,1,3
414,2019-5-8,2019,5,8,0,bls1pa,[N] The MAESTRO Dataset,https://www.reddit.com/r/MachineLearning/comments/bls1pa/n_the_maestro_dataset/,iyaja,1557242852,[removed],0,1
415,2019-5-8,2019,5,8,0,bls2kh,Scaling and Benchmarking Self-Supervised Visual Representation Learning,https://www.reddit.com/r/MachineLearning/comments/bls2kh/scaling_and_benchmarking_selfsupervised_visual/,iyaja,1557242968,,3,6
416,2019-5-8,2019,5,8,0,bls5ur,What is Big Data and Why use Hadoop? - MSys Training,https://www.reddit.com/r/MachineLearning/comments/bls5ur/what_is_big_data_and_why_use_hadoop_msys_training/,msystrainingllc,1557243423,[removed],0,1
417,2019-5-8,2019,5,8,0,bls7jh,[R] Function which takes two in vectors of the same dimension from different spaces and produces a single latent vector,https://www.reddit.com/r/MachineLearning/comments/bls7jh/r_function_which_takes_two_in_vectors_of_the_same/,atif_hassan,1557243672," Hey!

I was working on an extreme multi-label classification task of which one of the modules is to create a function that would take a single document vector and label vector (both of 300 dimensions but created separately and so from different vector spaces) and would produce a new latent vector which would be binarily classifiable. My current naive approach is to simply concatenate the two vectors.

Could you please suggest some techniques, features or a general direction or topic to study. Any idea would be greatly appreciated as I have been at a total loss on how to proceed further.

Thank you!",0,1
418,2019-5-8,2019,5,8,0,blsafy,Tesla's fleet data,https://www.reddit.com/r/MachineLearning/comments/blsafy/teslas_fleet_data/,RevovlerOcelot12,1557244071,[removed],0,1
419,2019-5-8,2019,5,8,0,blsbj3,Test case dataset for testing Android,https://www.reddit.com/r/MachineLearning/comments/blsbj3/test_case_dataset_for_testing_android/,codexblaze,1557244227,[removed],0,1
420,2019-5-8,2019,5,8,0,blsbju,[P] A Tutorial on Multi-Label Classification using Deep Learning,https://www.reddit.com/r/MachineLearning/comments/blsbju/p_a_tutorial_on_multilabel_classification_using/,thatbrguy_,1557244229,"This blog post provides an elaborate introductory tutorial on creating Deep Learning models for Multi-Label Classification. The concept is explored by creating a neural network in Keras (using TensorFlow) that can assign multiple labels to different food items. The code for this project is available in GitHub and can also be accessed through Google Colab. You can checkout the project and the article here:

&amp;#x200B;

Article: [https://blog.nanonets.com/multi-label-classification-using-deep-learning/](https://blog.nanonets.com/multi-label-classification-using-deep-learning/)

GitHub: [https://github.com/thatbrguy/Multilabel-Classification](https://github.com/thatbrguy/Multilabel-Classification)

&amp;#x200B;

I would love to hear your thoughts and feedback about the same. Thanks!",1,17
421,2019-5-8,2019,5,8,0,blsboe,Smarter training of neural networks - MIT,https://www.reddit.com/r/MachineLearning/comments/blsboe/smarter_training_of_neural_networks_mit/,j_orshman,1557244245,,2,4
422,2019-5-8,2019,5,8,0,blsdep,[R] Function which takes as input two vectors of the same dimension from different spaces and produces a single latent vector,https://www.reddit.com/r/MachineLearning/comments/blsdep/r_function_which_takes_as_input_two_vectors_of/,atif_hassan,1557244497,"Hey!

I was working on an extreme multi-label classification task of which one of the modules is to create a function that would take a single document vector and label vector (both of 300 dimensions but created separately and so from different vector spaces) and would produce a new latent vector which would be binarily classifiable. My current naive approach is to simply concatenate the two vectors.

Could you please suggest some techniques, features or a general direction or topic to study. Any idea would be greatly appreciated as I have been at a total loss on how to proceed further.

Thank you!",8,1
423,2019-5-8,2019,5,8,0,blsdrm,[P] Tool to build GPT-2 textgen APIs scalable and free using Google Cloud Run,https://www.reddit.com/r/MachineLearning/comments/blsdrm/p_tool_to_build_gpt2_textgen_apis_scalable_and/,minimaxir,1557244547,"[https://github.com/minimaxir/gpt-2-cloud-run](https://github.com/minimaxir/gpt-2-cloud-run)

There have been a few posts here w/ interactive GPT-2 textgen models. I've built an open-source tool to help build APIs with GPT-2 (specifically, fine-tuned models on a new dataset) via [gpt-2-simple](https://github.com/minimaxir/gpt-2-simple) and deploy them to Cloud Run, where the pricing works out to be effectively free unless you have huge spikes or constant requests.

I have also included a mini-Cloud Build tutorial to limit model downloading/uploading.",6,14
424,2019-5-8,2019,5,8,1,blshxk,Social media analytics: What's the buzz?,https://www.reddit.com/r/MachineLearning/comments/blshxk/social_media_analytics_whats_the_buzz/,TheTesseractAcademy,1557245125,,0,1
425,2019-5-8,2019,5,8,1,blskgx,Comprehensive Pytorch tutorial?,https://www.reddit.com/r/MachineLearning/comments/blskgx/comprehensive_pytorch_tutorial/,gecicihesap17,1557245491,[removed],0,1
426,2019-5-8,2019,5,8,1,blskok,Diagnosing the next step to take when Training a ML model.,https://www.reddit.com/r/MachineLearning/comments/blskok/diagnosing_the_next_step_to_take_when_training_a/,conradws,1557245521,,1,1
427,2019-5-8,2019,5,8,1,blslb8,Abandon US Petition Protests AI Conference Visa Denials,https://www.reddit.com/r/MachineLearning/comments/blslb8/abandon_us_petition_protests_ai_conference_visa/,Yuqing7,1557245617,,0,1
428,2019-5-8,2019,5,8,1,blslzq,"Migraine classification, i need some advice.",https://www.reddit.com/r/MachineLearning/comments/blslzq/migraine_classification_i_need_some_advice/,WalterEhren,1557245705,[removed],0,1
429,2019-5-8,2019,5,8,1,blsm28,"Germanys health minister proposes a $2,790 anti-vaxxer charge",https://www.reddit.com/r/MachineLearning/comments/blsm28/germanys_health_minister_proposes_a_2790/,eslamalmahdy,1557245715,,0,1
430,2019-5-8,2019,5,8,1,blsmw7,Solving Partial Differential Equations with Neural Networks,https://www.reddit.com/r/MachineLearning/comments/blsmw7/solving_partial_differential_equations_with/,leloykun,1557245829,,0,1
431,2019-5-8,2019,5,8,1,blsneh,"Building a private cloud machine learning framework using Presto, Weka and MinIO",https://www.reddit.com/r/MachineLearning/comments/blsneh/building_a_private_cloud_machine_learning/,jtsymonds,1557245895,,0,1
432,2019-5-8,2019,5,8,1,blsnka,Confidence Intervals Without Pain,https://www.reddit.com/r/MachineLearning/comments/blsnka/confidence_intervals_without_pain/,staphora,1557245916,,0,1
433,2019-5-8,2019,5,8,1,blt0ga,AI &amp; ML in a little finance company,https://www.reddit.com/r/MachineLearning/comments/blt0ga/ai_ml_in_a_little_finance_company/,Jerry-K7,1557247665,[removed],0,1
434,2019-5-8,2019,5,8,1,blt1co,"[R] FAIR: Advancing self-supervision, CV, NLP",https://www.reddit.com/r/MachineLearning/comments/blt1co/r_fair_advancing_selfsupervision_cv_nlp/,downtownslim,1557247778,"\&gt; Our recent advances in natural language processing (NLP) and computer vision (CV) show how work in content understanding is producing benefits. In NLP, we've developed a shared multilingual embedding space that can act as a sort of lingua franca to help take action on harmful content even in low-resource languages. In CV, we've built on our industry-leading research to identify content in more parts of an image and achieve record-setting accuracy using hashtags for video understanding.

&amp;#x200B;

Post: [https://ai.facebook.com/blog/advances-in-content-understanding-self-supervision-to-protect-people](https://ai.facebook.com/blog/advances-in-content-understanding-self-supervision-to-protect-people)",0,10
435,2019-5-8,2019,5,8,2,bltc1g,[P] Using Transfer Learning for NLP with Small Data,https://www.reddit.com/r/MachineLearning/comments/bltc1g/p_using_transfer_learning_for_nlp_with_small_data/,hszafarek,1557249217,,0,1
436,2019-5-8,2019,5,8,2,bltfd6,Dataset to pratice LASSO,https://www.reddit.com/r/MachineLearning/comments/bltfd6/dataset_to_pratice_lasso/,Paradiddle12,1557249666,[removed],0,1
437,2019-5-8,2019,5,8,2,bltfoe,[Project] Facebook post generator based on a facebook group,https://www.reddit.com/r/MachineLearning/comments/bltfoe/project_facebook_post_generator_based_on_a/,neoncolour,1557249708,"Hello everyone, 

I would like to mine data from a facebook group  to make a content generator for a facebook page, but I am quite clueless as to how to proceed. The use would be purely recreational, but google searchs did not yield anything interesting aside from Quora q&amp;as.

Has anyone had any experience with making facebook page bots ? How can I mine data from a facebook group ? 

Thank you for your replies.",1,2
438,2019-5-8,2019,5,8,2,bltfur,[P] How to encode time information in LSTMs and perform weighted averaging where the weights are learned,https://www.reddit.com/r/MachineLearning/comments/bltfur/p_how_to_encode_time_information_in_lstms_and/,somethingstrang,1557249734,"I am trying to build a model based on a recent google paper called ""Scalable and Accurate Deep Learning for Electronic Health Records"". The paper describes how to embed medical data for downstream prediction tasks.

Paper: [https://arxiv.org/abs/1801.07860](https://arxiv.org/abs/1801.07860)

Supplementary: [https://static-content.springer.com/esm/art%3A10.1038%2Fs41746-018-0029-1/MediaObjects/41746\_2018\_29\_MOESM1\_ESM.pdf](https://static-content.springer.com/esm/art%3A10.1038%2Fs41746-018-0029-1/MediaObjects/41746_2018_29_MOESM1_ESM.pdf)

&amp;#x200B;

I am using Pytorch and trying to modify an existing template I found online ([https://github.com/yuchenlin/lstm\_sentence\_classifier/blob/master/LSTM\_sentence\_classifier.py](https://github.com/yuchenlin/lstm_sentence_classifier/blob/master/LSTM_sentence_classifier.py))

&amp;#x200B;

In the supplementary material, they have a section describing how to embed medical data by doing the following (pic attached below too):

1) Take the raw medical data per patient per time step and embed them (like how its done in word embeddings)

2) Concatenate all history of that patient for that timestep together into a long vector

2a) For multiple information of the same type, average embeddings using a learned weighting. The weighted averaging is done by associating each feature with a non-negative weight that is trained jointly with the model. 

3) Create another vector that contains time information (seconds)

&amp;#x200B;

https://i.redd.it/b41lbaylotw21.png

&amp;#x200B;

Finally, in the supplementary material, it states that:

The sequence of embeddings were further reduced down to a shorter sequence. Typically, the shorter sequences were split into time-steps of 12 hours where the embeddings for all features within a category in the same day were combined using weighted averaging. The weighted averaging is done by associating each feature with a non-negative weight that is trained jointly with the model. These weights are also used for prediction attribution. The log of the average time-delta divided by a factor (controlled by a hyperparameter) at each time-step is also embedded into a small floating-point vector (which is also randomly initialized) and concatenated to the input embedding at each time-step.

&amp;#x200B;

My first question is: How do I actually go about inserting this time vector? 

The supplementary states that the sequences were split into 12-hour time steps. If that's the case, why the need for a time vector? It then later states that you can just simply concatenate the time vector to the input embedding. Why would a simple concatenation like that work and how would the model know to associate that time information with the appropriate part of the input? 

&amp;#x200B;

My second question is: For Step 2a), how do I go about getting a weighted average for the same-category features? That part is glossed over without much explanation and it is lost on me.

&amp;#x200B;

I'm sorry if these are a lot of questions, but even pointing to some tutorials or any readings would go a long way in the right direction!",2,4
439,2019-5-8,2019,5,8,2,bltrzb,Tensorflow.js project,https://www.reddit.com/r/MachineLearning/comments/bltrzb/tensorflowjs_project/,yxnovaxy,1557251370,[removed],0,1
440,2019-5-8,2019,5,8,2,blts12,Objective functions in deep learning,https://www.reddit.com/r/MachineLearning/comments/blts12/objective_functions_in_deep_learning/,mQuBits,1557251376,,0,1
441,2019-5-8,2019,5,8,2,bltw59,Hi everyone. I am doing my final project for bachelors degree. Can anyone recommend me free educational online books on SVM.,https://www.reddit.com/r/MachineLearning/comments/bltw59/hi_everyone_i_am_doing_my_final_project_for/,mxswoppers,1557251931,[removed],0,1
442,2019-5-8,2019,5,8,3,blu9qx,[D] Keras (RNNs) for multiple time series?,https://www.reddit.com/r/MachineLearning/comments/blu9qx/d_keras_rnns_for_multiple_time_series/,[deleted],1557253738,[deleted],0,1
443,2019-5-8,2019,5,8,3,bluf7j,[D] Keras (RNNs) for multiple time series?,https://www.reddit.com/r/MachineLearning/comments/bluf7j/d_keras_rnns_for_multiple_time_series/,heloderma1,1557254466,"The two Keras subs seem pretty dead, hoping I can find some help here.

I was able to use Keras to create a prediction from a time series, with input data looking like so:

Date | Volume
:--:|:--:
1/1/2017 | 5
1/2/2017 | 7
... | ...
12/31/2018 | 12

But we would like to expand on this. 

We have multiple products, which are each sold to multiple stores. I would like to create a tool where we throw data for each product at each store into a model, and then call ""At this store, for this product, for this date, Keras predicts X""

I cannot seem to find an example that deals with having multiple rows for each date. An example of the full dataset is below. Seems like one way would be to make a column for each store/product combo and loop those through with a model for each, but I'm not sure if that would be ideal.

Date | Volume | Store | Product
:--:|:--:|:--:|:--:
1/1/2017 | 5 | A | a
1/1/2017 | 6 | A | b
1/1/2017 | 1 | A | c
1/2/2017 | 8 | A | a
1/2/2017 | 9 | A | b
1/2/2017 | 2 | A | c
1/1/2017 | 6 | B | a
1/1/2017 | 6 | B | b
1/1/2017 | 1 | B | c
1/2/2017 | 7 | B | a
1/2/2017 | 8 | B | b
1/2/2017 | 2 | B | c
... | ... | ...| ...
12/31/2018 | 5 | A | a
12/31/2018  | 6 | A | b
12/31/2018  | 1 | A | c
12/31/2018  | 8 | A | a
12/31/2018  | 9 | A | b
12/31/2018  | 2 | A | c
12/31/2018  | 6 | B | a
12/31/2018  | 6 | B | b
12/31/2018  | 1 | B | c
12/31/2018  | 7 | B | a
12/31/2018  | 8 | B | b
12/31/2018  | 2 | B | c",6,2
444,2019-5-8,2019,5,8,4,blunzk,[P] Scaling PyTorch Transformer-XL training to 128 GPUs,https://www.reddit.com/r/MachineLearning/comments/blunzk/p_scaling_pytorch_transformerxl_training_to_128/,yaroslavvb,1557255665,"**Scaling PyTorch Transformer-XL training to 128 GPUs**

&amp;#x200B;

code: [https://github.com/cybertronai/transformer-xl](https://github.com/cybertronai/transformer-xl)

experiments: [https://medium.com/south-park-commons/scaling-transformer-xl-to-128-gpus-85849508ec35](https://medium.com/south-park-commons/scaling-transformer-xl-to-128-gpus-85849508ec35)",14,30
445,2019-5-8,2019,5,8,4,blupff,"Fast-Pytorch with Google Colab: Pytorch details, Pytorch example implementations, Pytorch sample codes",https://www.reddit.com/r/MachineLearning/comments/blupff/fastpytorch_with_google_colab_pytorch_details/,obsezer,1557255858,[removed],0,1
446,2019-5-8,2019,5,8,4,blvcn5,Enlarging Labeled Handwritten Character Datasets With Capsule Networks,https://www.reddit.com/r/MachineLearning/comments/blvcn5/enlarging_labeled_handwritten_character_datasets/,Yuqing7,1557258988,,0,1
447,2019-5-8,2019,5,8,5,blvqtz,[D] voice cloning/style transfer SOTA,https://www.reddit.com/r/MachineLearning/comments/blvqtz/d_voice_cloningstyle_transfer_sota/,PuzzledProgrammer3,1557260897,"what is the current SOTA in voice cloning/ style transfer? Particularly something where you have a target voice you want to transfer a source voice audio or text to?  Something similar to this [https://github.com/andabi/deep-voice-conversion](https://github.com/andabi/deep-voice-conversion), is there a open source implementation to something close to SOTA?",10,5
448,2019-5-8,2019,5,8,7,blx5rv,Help wantedAnyone have experience building chatbot?,https://www.reddit.com/r/MachineLearning/comments/blx5rv/help_wantedanyone_have_experience_building_chatbot/,willhuxxx,1557267878,[removed],0,1
449,2019-5-8,2019,5,8,7,blxfkk,Distributed machine learning,https://www.reddit.com/r/MachineLearning/comments/blxfkk/distributed_machine_learning/,Sorenmc,1557269270,[removed],0,1
450,2019-5-8,2019,5,8,8,blxoi6,"Googles scalable supercomputers for machine learning, Cloud TPU Pods, are now publicly available in beta",https://www.reddit.com/r/MachineLearning/comments/blxoi6/googles_scalable_supercomputers_for_machine/,cokernel_hacker,1557270566,,0,1
451,2019-5-8,2019,5,8,8,blxv2b,[D] Do you have to cite a dishonest paper?,https://www.reddit.com/r/MachineLearning/comments/blxv2b/d_do_you_have_to_cite_a_dishonest_paper/,Seerdecker,1557271551,"If you come across a paper that is directly relevant to the problem you're trying to solve but also makes claims that are unsupported by its own data (and contradict your own results), do you have to cite it? I feel that no credit should be given where none is due, and that debunking the claims is a waste of space.

For reference, [this is the paper I'm talking about](https://arxiv.org/abs/1904.03367). The paper investigates adding attention layers on top of the standard CNN used by DQN on Atari. There are 6 variants considered, which differ in the location where the attention layers are added to the CNN.

Claim 1 (figure 2): ""Here we can see the clear advantage of self-attention is able to provide with respect to sample efficiency.""

The baseline PPO learns faster or on-par with their 6 variants on 6 of the 10 games benchmarked.

Claim 2 (table 1): ""These results clearly demonstrate the improved performance of multiple self-attention variants.""

Their best variant, SAN, performs better on 5 of the 10 games compared to the baseline PPO. Most of the other variants perform worse.

Claim 3 (conclusion): ""We can also see clear improvements over baseline results with respect to 60% of environments tested"".

That is to say, the baseline PPO performs best on 4 games, and the proposed method performs best on the 6 other games *if you cherry-pick the variant used on a per-game basis*.",9,0
452,2019-5-8,2019,5,8,8,bly085,What do I need to study for a neural architecture search?,https://www.reddit.com/r/MachineLearning/comments/bly085/what_do_i_need_to_study_for_a_neural_architecture/,GoBacksIn,1557272317,[removed],0,1
453,2019-5-8,2019,5,8,8,bly3nm,Whats spacial support?,https://www.reddit.com/r/MachineLearning/comments/bly3nm/whats_spacial_support/,adeeplearner,1557272834,[removed],0,1
454,2019-5-8,2019,5,8,10,blz9md,Stochastic Optimization of Sorting Networks via Continuous Relaxations,https://www.reddit.com/r/MachineLearning/comments/blz9md/stochastic_optimization_of_sorting_networks_via/,downtownslim,1557279499,,17,108
455,2019-5-8,2019,5,8,10,blzccr,[R] MixMatch: A Holistic Approach to Semi-Supervised Learning,https://www.reddit.com/r/MachineLearning/comments/blzccr/r_mixmatch_a_holistic_approach_to_semisupervised/,xternalz,1557279948,,8,6
456,2019-5-8,2019,5,8,11,blzsrq,[P] Collection of Computer Vision Datasets,https://www.reddit.com/r/MachineLearning/comments/blzsrq/p_collection_of_computer_vision_datasets/,fj33xx,1557282605,"The computer vision communities are constantly contributing new datasets and unfortunately it's not easy to track.

I created a site (visualdata.io) to aggregate the latest computer vision datasets in one place with easy-to-use search functionality. Each dataset has been tagged with one or more topics like image classification, object detection etc and you can dynamically filter with keywords. It also adopts an instagram like scrolling interface to show case the example images in the dataset. 

Hope the site is helpful to discover datasets for your projects. New datasets are always being added to the site. Please let me know any feedback you may have!",28,183
457,2019-5-8,2019,5,8,12,bm0dwn,Use of neuro-evolutionary techniques ?,https://www.reddit.com/r/MachineLearning/comments/bm0dwn/use_of_neuroevolutionary_techniques/,ashutosj,1557286256,[removed],0,1
458,2019-5-8,2019,5,8,13,bm134r,"Build, train, and deploy machine learning models with Azure for free",https://www.reddit.com/r/MachineLearning/comments/bm134r/build_train_and_deploy_machine_learning_models/,Ejil23,1557291092,,0,1
459,2019-5-8,2019,5,8,14,bm19eo,Speeding Up Genetic Algorithms Using Reinforcememt Learning,https://www.reddit.com/r/MachineLearning/comments/bm19eo/speeding_up_genetic_algorithms_using/,GrandmasterMochizuki,1557292333,,1,1
460,2019-5-8,2019,5,8,14,bm1crp,[R] Speeding Up Genetic Algorithms via Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/bm1crp/r_speeding_up_genetic_algorithms_via/,GrandmasterMochizuki,1557293052,,1,72
461,2019-5-8,2019,5,8,15,bm1w11,[R] Learning to Generate HumanObject Interactions - Stanford AI Lab,https://www.reddit.com/r/MachineLearning/comments/bm1w11/r_learning_to_generate_humanobject_interactions/,Bayequentist,1557297236," [http://ai.stanford.edu/blog/generate-human-object/](http://ai.stanford.edu/blog/generate-human-object/) 

&gt;Tremendous progress has been made in the last few years in developing advanced virtual reality (VR) and robotics platforms. As the examples above show, these platforms now allow us to experience more immersive virtual worlds, or allow robots to perform challenging locomotion tasks like walking in snow. So, can we soon expect to have robots that can set the dinner table or do our dishes?  
&gt;  
&gt;Unfortunately, we are not yet there.  
&gt;  
&gt;To understand why, consider the diversity of interactions in daily human life. We spend almost all of our waking hours performing activitiessimple actions like picking up a fruit or more complex ones like cooking a meal. These physical interactions, called **humanobject interactions**, are multi-stepped and governed by physics as well as human goals, customs, and biomechanics. In order to develop more dynamic virtual worlds, and smarter robots, we need to *teach machines to capture, understand, and replicate* these interactions. The information we need to learn these interactions is already widely available in the form of large video collections (e.g., YouTube, Netflix, Facebook).  
&gt;  
&gt;In this post, I will describe some first steps we have taken towards learning multi-step humanobject interactions from videos. I will discuss two applications of our method: (1) generating plausible and novel human-object interaction animations suitable for VR/AR, (2) enabling robots to react smartly to user behavior and interactions.  
&gt;  
&gt;...",2,8
462,2019-5-8,2019,5,8,15,bm1z99,[P] Demo video of edaviz: stop wasting time on data exploration,https://www.reddit.com/r/MachineLearning/comments/bm1z99/p_demo_video_of_edaviz_stop_wasting_time_on_data/,kite_and_code,1557297938,"Hi,

my name is Florian and I did my **Master Thesis** on **semi-automated data exploration**. As a result, I created the **edaviz Python library**.

I evaluated the library with a German **Data Science consultancy**, and they were more than **10x faster** on a given standard set of data exploration tasks.

Please check out the **short demo video** and let me know what you think:

[**https://youtu.be/eYEeYv11YrQ**](https://youtu.be/eYEeYv11YrQ)

&amp;#x200B;

What is your opinion about the library?

How would you improve edaviz?

What do you not like about edaviz?

&amp;#x200B;

Thank you for your feedback,

Florian",114,200
463,2019-5-8,2019,5,8,15,bm216p,Telescopic Forks for Automated Warehouse AS/RS,https://www.reddit.com/r/MachineLearning/comments/bm216p/telescopic_forks_for_automated_warehouse_asrs/,lhd121,1557298368,,0,1
464,2019-5-8,2019,5,8,15,bm22tk,How can humans keep the upper hand on artificial intelligence?,https://www.reddit.com/r/MachineLearning/comments/bm22tk/how_can_humans_keep_the_upper_hand_on_artificial/,Wensosolutions,1557298732,,0,1
465,2019-5-8,2019,5,8,16,bm291f,[P] TalkToTransformer.com,https://www.reddit.com/r/MachineLearning/comments/bm291f/p_talktotransformercom/,baylearn,1557300126,"Adam King built a fun web interface for OpenAI's 345M GPT-2 model:

https://talktotransformer.com

To ensure that I don't feed it with something it already knows, I seeded it with a quote from last week's Game of Thrones Season 8 Episode 4 (*spoiler!*):

**Shes a girl who walked into a fire with three stones and walked out with three dragons. How could she not believe in destiny?**

But I don't believe she lived. Her story is not all that unusual. Not in other kingdoms. And what is the difference between a dragon and a girl? The dragon is only two stones and her dragon is three.

As the dragon dies and leaves behind him the girl, she has now lost the chance to fulfill her destiny. In the end, she gives up the body, but it did not have wings. Her power dissipates and the girl dies like a leaf on a river, but with the power of a dragon and the dragon's magic that could have killed her.

That's what my stories are like. I don't know when or why something happens, but it always seems to happen within me.

If a character dies early by going through a terrible time, that might be because of their family or close friends, but sometimes, they could be the result of a curse. It's too common.",55,48
466,2019-5-8,2019,5,8,16,bm2gpf,"Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask",https://www.reddit.com/r/MachineLearning/comments/bm2gpf/deconstructing_lottery_tickets_zeros_signs_and/,whenmaster,1557301945,,3,21
467,2019-5-8,2019,5,8,17,bm2jao,Terrorist Incidents since 1970 across the countries,https://www.reddit.com/r/MachineLearning/comments/bm2jao/terrorist_incidents_since_1970_across_the/,data_scientists,1557302568,,0,1
468,2019-5-8,2019,5,8,17,bm2qpn,[Discussion] Question about re-synthesis (an idea I had),https://www.reddit.com/r/MachineLearning/comments/bm2qpn/discussion_question_about_resynthesis_an_idea_i/,khawarizmy,1557304428,"I don't know if this is the right place to ask, but I couldn't find anything about online.

Is there any research currently being conducted on getting a synthesizer preset parameters from an audio sample? If not, would you think that It would be too difficult to create something like this? 

There's plenty of data to train on for this kind of application. [Splice](https://splice.com/) for example, has thousands of such presets, and the audio sample wouldn't be too difficult to obtain once download the preset and load it into some synth and export the resulting audio sample.

What kind type of neural network would be best used for this?",12,1
469,2019-5-8,2019,5,8,17,bm2r3d,Hot Runner Market Analysis 2012-2017 and Forecast 2018-2023,https://www.reddit.com/r/MachineLearning/comments/bm2r3d/hot_runner_market_analysis_20122017_and_forecast/,predictoreports,1557304536,,0,1
470,2019-5-8,2019,5,8,18,bm2xks,How important is a laptop GPU for learning ML.,https://www.reddit.com/r/MachineLearning/comments/bm2xks/how_important_is_a_laptop_gpu_for_learning_ml/,green_guards_man,1557306100,[removed],0,1
471,2019-5-8,2019,5,8,18,bm2zo8,Optimization Algorithm,https://www.reddit.com/r/MachineLearning/comments/bm2zo8/optimization_algorithm/,cooper_pair_,1557306569,[removed],0,1
472,2019-5-8,2019,5,8,18,bm30qy,[R] How important is a laptop GPU for learning ML.,https://www.reddit.com/r/MachineLearning/comments/bm30qy/r_how_important_is_a_laptop_gpu_for_learning_ml/,green_guards_man,1557306816,"I  just made a job change into data engineering (Hadoop/Pig/Spark/Kafka)  and will be learning machine learning on the side. I'm looking to  replace my old workstation(laptop) with a new one, but I don't have  enough knowledge to prioritize the specs I will be needing.

I'm not sure how cost effective a (laptop)GPU is for learning Machine learning and maybe getting to deep learning later.

I'm  thinking of investing in a Gaming laptop with RTX2060 but I don't want   to go with a gaming laptop if a laptop GPU wont be enough for long term,  and I can do the learning on just a good CPU. Also in that case, is  something like i7-8565U good enough or do I need to go with 8750H ?",30,0
473,2019-5-8,2019,5,8,19,bm3dlo,Windows Vision Skills,https://www.reddit.com/r/MachineLearning/comments/bm3dlo/windows_vision_skills/,sahsagsahsag,1557309742,,0,1
474,2019-5-8,2019,5,8,19,bm3hjp,Top 5 Skills Needed to become a Machine Learning Engineer - [Discussion],https://www.reddit.com/r/MachineLearning/comments/bm3hjp/top_5_skills_needed_to_become_a_machine_learning/,vadhavaniyafaijan,1557310621,,0,1
475,2019-5-8,2019,5,8,20,bm3uwx,"Cross-validation strategies for time series forecasting with python, pandas, scikit-learn",https://www.reddit.com/r/MachineLearning/comments/bm3uwx/crossvalidation_strategies_for_time_series/,mQuBits,1557313405,,0,1
476,2019-5-8,2019,5,8,20,bm3y5a,[D] How to train a network to detect a specific pose gesture(e.g. raising two hand) with limited amount of data?,https://www.reddit.com/r/MachineLearning/comments/bm3y5a/d_how_to_train_a_network_to_detect_a_specific/,RavlaAlvar,1557314000,"I tried using a pose detection network and infer pose from output. This allows me to recognise different gestures without retrain the network. But the cost of running a full pose detection network is high, thus doesnt work on my operating environment (embedding device).

Since I dont actually have to frequently redefine the posture I wanted to detect, I had an idea that using a modified YOLO network to simultaneously detect person and classified whether the person is raising both her hand.

This works to some extend, however, this approach requires me to collect bunch of data of a person raising her hand and it is quite difficult to collect much data with any degree of varieties.

Is there a way to work around the data problem ? Many thanks.",3,1
477,2019-5-8,2019,5,8,20,bm4102,"[News] ""BBC NEWS | Technology | Artificial brain '10 years away'"" - 22 July 2009",https://www.reddit.com/r/MachineLearning/comments/bm4102/news_bbc_news_technology_artificial_brain_10/,dimon-babon,1557314535,,0,1
478,2019-5-8,2019,5,8,20,bm41as,[N] Awesome papers and engineering reviews on Computer Vision News of May. Links for free reading!,https://www.reddit.com/r/MachineLearning/comments/bm41as/n_awesome_papers_and_engineering_reviews_on/,Gletta,1557314591,"Here are the links to the May 2019 issue of **Computer Vision News**, the magazine of the algorithm community published by RSIP Vision: many articles about Artificial Intelligence, Deep Learning, Computer Vision and more.

Free subscription on page 40.

[**HTML5 version (recommended)**](https://rsipvision.com/ComputerVisionNews-2019May/)

[**PDF version**](https://www.rsipvision.com/computer-vision-news-2019-may-pdf/)

Enjoy!

&amp;#x200B;

https://i.redd.it/rh2aq5y92zw21.jpg",0,4
479,2019-5-8,2019,5,8,20,bm468m,Train Random Forest with only sklearn vs train with AutoML,https://www.reddit.com/r/MachineLearning/comments/bm468m/train_random_forest_with_only_sklearn_vs_train/,pp314159,1557315503,,0,1
480,2019-5-8,2019,5,8,20,bm48xv,Please help a PhD student with recommendations for softwares that can track objects growing and merging in microscope images?,https://www.reddit.com/r/MachineLearning/comments/bm48xv/please_help_a_phd_student_with_recommendations/,knamikaze,1557315982,"Hello, 

So I have these binary images processed from a microscope that shows thing being squeezed between two surfaces. It is related to my PhD thesis. I am supposed to calculate the growth of these individual bodies and also how many of them merge together. I would like to know if there are available softwares that can do this, I mostly work on Matlab also I am hope there is something available if someone can offer guidance there. If there isn't can someone give recommendations for how it can be done? 

Thanks",0,1
481,2019-5-8,2019,5,8,20,bm4chf,[P] Huskarl: Deep Reinforcement Learning Framework + Algorithms,https://www.reddit.com/r/MachineLearning/comments/bm4chf/p_huskarl_deep_reinforcement_learning_framework/,danaugrs,1557316622,"I just updated [my deep reinforcement learning framework, called Huskarl](https://github.com/danaugrs/huskarl). The focus has been on readability, conciseness, and clarity - it's what I wished existed when I started studying deep RL!

The README now has some nifty GIFs showcasing each included example.

I also submitted Huskarl to the [TensorFlow 2.0 hackathon](https://tensorflow.devpost.com/).

Let me know what you think!",9,12
482,2019-5-8,2019,5,8,21,bm4h9y,[P] Fine-tunning OpenAI's GPT-2-345M on Conversation data (Update) - with Colab link,https://www.reddit.com/r/MachineLearning/comments/bm4h9y/p_finetunning_openais_gpt2345m_on_conversation/,Tenoke,1557317428,"I played around with trying out the newer bigger (345M parameters) GPT-2 pre-trained model on conversation data. I tried it with my tiny (14 mb) personal conversation data, as well as with conversational data from ubuntu's chats. 

The blog post is [here](https://svilentodorov.xyz/blog/gpt-345M-finetune/) with working Colab(s) included.

Overall, 345M performs better than 117M but not overwhelmingly so.",0,2
483,2019-5-8,2019,5,8,21,bm4t1l,Fine-tunning OpenAI's (Larger) GPT-2-345M on Conversation data (Update) - with Colab link,https://www.reddit.com/r/MachineLearning/comments/bm4t1l/finetunning_openais_larger_gpt2345m_on/,Tenoke,1557319377,[removed],0,1
484,2019-5-8,2019,5,8,22,bm515c,Binary classification matching system,https://www.reddit.com/r/MachineLearning/comments/bm515c/binary_classification_matching_system/,Wildstylez23,1557320709,[removed],0,1
485,2019-5-8,2019,5,8,22,bm52ye,Iguazio brings its data science platform to Azure and Azure Stack  TechCrunch,https://www.reddit.com/r/MachineLearning/comments/bm52ye/iguazio_brings_its_data_science_platform_to_azure/,IguazioDani,1557320988,[removed],0,1
486,2019-5-8,2019,5,8,22,bm5clr,How can we justify a model's behavior in this setting?,https://www.reddit.com/r/MachineLearning/comments/bm5clr/how_can_we_justify_a_models_behavior_in_this/,KappaClosed,1557322510,,0,1
487,2019-5-8,2019,5,8,22,bm5e6a,[D] How to make padding to max sequence lengths in batch with dataset api?,https://www.reddit.com/r/MachineLearning/comments/bm5e6a/d_how_to_make_padding_to_max_sequence_lengths_in/,hadaev,1557322757,"So i want to pad shorter sequences to max lengths in generated batch so that all sequences are the same length and i can pass it to model.

I find this [https://www.tensorflow.org/api\_docs/python/tf/data/experimental/bucket\_by\_sequence\_length](https://www.tensorflow.org/api_docs/python/tf/data/experimental/bucket_by_sequence_length)

But can't find example of use.

Or maybe i need some another function.",3,0
488,2019-5-8,2019,5,8,22,bm5hg0,"[N] Live observability for Tensorflow, Apache Spark",https://www.reddit.com/r/MachineLearning/comments/bm5hg0/n_live_observability_for_tensorflow_apache_spark/,ariehkovler,1557323274,"[From ZDNet](https://www.zdnet.com/article/on-the-rookout-for-live-data-instant-observability-to-fix-software-bugs-and-open-ai-black-boxes/): 

&gt; Unexpected behavior could be an error in the structure of the model or some bias in the data, or it could be a classic bug in the enveloping code. Each of these will have its own very different solution. If a model needs more training it could take weeks of computing time. If the model itself needs expanding, data scientists may have to do complicated design work. On the other hand, a logical error, once found, could be fixed in seconds. 

An interesting solution here for production ML models that could take the strain off of data engineers when data scientists want to observe their ML models.",0,16
489,2019-5-8,2019,5,8,23,bm68c0,[R] Excellent paper on tweaking AI for massive distributed/parallel processing,https://www.reddit.com/r/MachineLearning/comments/bm68c0/r_excellent_paper_on_tweaking_ai_for_massive/,eleitl,1557327291,,0,1
490,2019-5-9,2019,5,9,0,bm6bci,[R] MixMatch: A Holistic Approach to Semi-Supervised Learning,https://www.reddit.com/r/MachineLearning/comments/bm6bci/r_mixmatch_a_holistic_approach_to_semisupervised/,iyaja,1557327722,,2,5
491,2019-5-9,2019,5,9,0,bm6iia,"[R] Adversarial Examples Aren't Bugs, They're Features",https://www.reddit.com/r/MachineLearning/comments/bm6iia/r_adversarial_examples_arent_bugs_theyre_features/,andrew_ilyas,1557328719,,0,1
492,2019-5-9,2019,5,9,0,bm6jvh,"ICLR 2019 | Tsinghua, Google and ByteDance Propose Neural Networks for Inductive Learning &amp; Logic",https://www.reddit.com/r/MachineLearning/comments/bm6jvh/iclr_2019_tsinghua_google_and_bytedance_propose/,Yuqing7,1557328908,,0,1
493,2019-5-9,2019,5,9,0,bm6kbe,"[Research] Adversarial Examples Aren't Bugs, They're Features",https://www.reddit.com/r/MachineLearning/comments/bm6kbe/research_adversarial_examples_arent_bugs_theyre/,andrew_ilyas,1557328970,,1,1
494,2019-5-9,2019,5,9,0,bm6qhx,Papers/resources related to mixture density networks?,https://www.reddit.com/r/MachineLearning/comments/bm6qhx/papersresources_related_to_mixture_density/,AppleNamu,1557329822,[removed],0,1
495,2019-5-9,2019,5,9,0,bm6sh8,Mathematics For Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bm6sh8/mathematics_for_machine_learning/,ewan_m,1557330100,,0,1
496,2019-5-9,2019,5,9,0,bm6vza,[P] Automate your Flask Deployments on AWS,https://www.reddit.com/r/MachineLearning/comments/bm6vza/p_automate_your_flask_deployments_on_aws/,hszafarek,1557330604,,0,1
497,2019-5-9,2019,5,9,0,bm6x53,"Simple Questions Thread May 08, 2019",https://www.reddit.com/r/MachineLearning/comments/bm6x53/simple_questions_thread_may_08_2019/,AutoModerator,1557330770,[removed],0,1
498,2019-5-9,2019,5,9,1,bm74eq,[Research] Enabling Robots to Understand Incomplete Natural Language Instructions Using Commonsense Reasoning,https://www.reddit.com/r/MachineLearning/comments/bm74eq/research_enabling_robots_to_understand_incomplete/,cdossman,1557331745," [https://medium.com/@cdossman/common-sense-reasoning-for-robots-89b2df29ce18](https://medium.com/@cdossman/common-sense-reasoning-for-robots-89b2df29ce18) 

 Abstract Enabling robots to understand instructions provided via spoken natural language would facilitate interaction between robots and people in a variety of settings in homes and workplaces.  However, natural language instructions are often missing information that would be obvious to a human based on environmental context and common sense, and hence does not need to be explicitly stated. In this paper, we introduce Language-Model-based Commonsense Reasoning (LMCR), a new method which enables a robot to listen to a natural language instruction from a human, observe the environment around it, and automatically fill in information missing from the instruction using environmental context and a new commonsense reasoning approach.",4,8
499,2019-5-9,2019,5,9,1,bm7a31,"Come, Let us Make Silicon Brains",https://www.reddit.com/r/MachineLearning/comments/bm7a31/come_let_us_make_silicon_brains/,cryptokunbo,1557332520,,0,1
500,2019-5-9,2019,5,9,1,bm7d89,[D] Natural Language Queries,https://www.reddit.com/r/MachineLearning/comments/bm7d89/d_natural_language_queries/,seymourdixongais,1557332939,"Hi everyone, I have an idea that I want to pursue but I'm not exactly sure how to go about it; This was the first place that came to mind for asking. 

&amp;#x200B;

Basically, I want to take a large dataset of text containing facts and I want to be able to query it with natural language (like a google search) with the end result being the answer to the natural language query. 

&amp;#x200B;

I'm by no means a machine learning aficionado, but I feel that ML might have some part to play in this idea of mine. Would anyone have any suggestions on model types / where to start for this? Thanks!",4,3
501,2019-5-9,2019,5,9,1,bm7h05,"[R] Adversarial Examples Are Not Bugs, They Are Features",https://www.reddit.com/r/MachineLearning/comments/bm7h05/r_adversarial_examples_are_not_bugs_they_are/,[deleted],1557333444,[deleted],0,1
502,2019-5-9,2019,5,9,1,bm7iix,"[R] Adversarial Examples Aren't Bugs, They're Features",https://www.reddit.com/r/MachineLearning/comments/bm7iix/r_adversarial_examples_arent_bugs_theyre_features/,andrew_ilyas,1557333654,"Blog post: [http://gradientscience.org/adv](http://gradientscience.org/adv)

Paper: [https://arxiv.org/abs/1905.02175](https://arxiv.org/abs/1905.02175) 

&amp;#x200B;

Hi, I'm one of the lead authors on this paper.

&amp;#x200B;

TL;DR: We show that adversarial examples aren't really weird aberrations or random artifacts, and are instead actually meaningful but imperceptible features of the data distribution (i.e. they are helpful for generalization). We prove this through a series of experiments that shows that (a) you can learn just based on these imperceptible features embedded into a completely mislabeled training set and generalize to the true test set (b) you can remove these imperceptible features and generalize \*robustly\* to the true test set (with standard training).

&amp;#x200B;

We would love to answer any questions/comments!",128,175
503,2019-5-9,2019,5,9,1,bm7qqq,[P] How Machine Learning Can Help You Charge Your E-Scooters,https://www.reddit.com/r/MachineLearning/comments/bm7qqq/p_how_machine_learning_can_help_you_charge_your/,prj07,1557334760,"Hey everyone, I built a full-stack machine learning application that helps E-Scooter companies optimize their charging program and scooter locations real-time. This is a full-scope [blog post](https://towardsdatascience.com/a-birds-eye-view-leveraging-machine-learning-to-generate-nests-30dd2136a839) of the project I've been working on.

[NestGenerator showcasing non-nest scooters and new nest recommendations utilizing the K-Means algorithm](https://i.redd.it/s6szvwchq0x21.png)",1,17
504,2019-5-9,2019,5,9,2,bm7u13,Announcing Open Images V5 and the ICCV 2019 Open Images Challenge,https://www.reddit.com/r/MachineLearning/comments/bm7u13/announcing_open_images_v5_and_the_iccv_2019_open/,sjoerdapp,1557335184,,0,1
505,2019-5-9,2019,5,9,2,bm85h8,20x Quadro or T4's in one rig,https://www.reddit.com/r/MachineLearning/comments/bm85h8/20x_quadro_or_t4s_in_one_rig/,SmugEskim0,1557336714,[removed],0,1
506,2019-5-9,2019,5,9,3,bm8mun,[P] Bob Ross painting lesson run through DeepDream and WaveNet,https://www.reddit.com/r/MachineLearning/comments/bm8mun/p_bob_ross_painting_lesson_run_through_deepdream/,shoeblade,1557339010,"# [Video of ""Deeply Artificial Trees""](https://www.youtube.com/watch?v=5DaVnriHhPc)

&amp;#x200B;

https://i.redd.it/33270v4x11x21.jpg

I'm sure many have seen these visuals before, the audio though was trained on both samples of when he was painting and when he was whispering.

&amp;#x200B;

I ran other celebrities through, you can hear some [""deep learning impressions"" on this other video](https://www.youtube.com/watch?v=CWLqlXCu3OM)",8,35
507,2019-5-9,2019,5,9,3,bm8usv,Classification for similar objects of different sizes/scale,https://www.reddit.com/r/MachineLearning/comments/bm8usv/classification_for_similar_objects_of_different/,alphaleonis,1557340082,[removed],0,1
508,2019-5-9,2019,5,9,4,bm9g7d,Looking for recommendations,https://www.reddit.com/r/MachineLearning/comments/bm9g7d/looking_for_recommendations/,Shiva_krish,1557342939,[removed],0,1
509,2019-5-9,2019,5,9,4,bm9gcr,Speech recognition course,https://www.reddit.com/r/MachineLearning/comments/bm9gcr/speech_recognition_course/,ma2rten,1557342957,"Hi,

I'd like to get into speech processing. Could someone recommend a deep-learning focused speech processing course for someone with ML background?

I was considering following along with this one:

[http://web.stanford.edu/class/cs224s/](http://web.stanford.edu/class/cs224s/)

Although it's from 2017 and the videos don't seem to be available.",0,1
510,2019-5-9,2019,5,9,4,bm9yxr,This Erotica Does Not Exist [NSFW],https://www.reddit.com/r/MachineLearning/comments/bm9yxr/this_erotica_does_not_exist_nsfw/,[deleted],1557345501,[deleted],0,1
511,2019-5-9,2019,5,9,5,bma3pl,[P] This Erotica Does Not Exist (NSFW),https://www.reddit.com/r/MachineLearning/comments/bma3pl/p_this_erotica_does_not_exist_nsfw/,[deleted],1557346114,[deleted],0,1
512,2019-5-9,2019,5,9,5,bma4io,Speech recognition resources,https://www.reddit.com/r/MachineLearning/comments/bma4io/speech_recognition_resources/,ma2rten,1557346219,[removed],0,1
513,2019-5-9,2019,5,9,5,bma6pa,Speech recognition resources,https://www.reddit.com/r/MachineLearning/comments/bma6pa/speech_recognition_resources/,ma2rten,1557346501,[removed],0,1
514,2019-5-9,2019,5,9,5,bma87o,Speech recognition resources,https://www.reddit.com/r/MachineLearning/comments/bma87o/speech_recognition_resources/,Cheap_Meeting,1557346713,[removed],0,1
515,2019-5-9,2019,5,9,5,bmadw0,This Erotica Does Not Exist,https://www.reddit.com/r/MachineLearning/comments/bmadw0/this_erotica_does_not_exist/,[deleted],1557347489,,0,1
516,2019-5-9,2019,5,9,5,bmaiqv,[P] This Erotica Does Not Exist (NSFW),https://www.reddit.com/r/MachineLearning/comments/bmaiqv/p_this_erotica_does_not_exist_nsfw/,hscribe,1557348132,"# This Erotica Does Not Exist

[https://erogenerator.gitlab.io/](https://erogenerator.gitlab.io/)

### What is this?

This website contains samples of text generated from [GPT-2](https://github.com/openai/gpt-2), fine-tuned on [text](https://www.reddit.com/r/literotica/comments/6xvxvh/i_downloaded_all_380000_stories_on_literotica/) from [Literotica](http://literotica.com/).

### What is 'Conditioned'?

'Conditioned' samples are generated conditioning on the first few words of chapters from select books from [Project Gutenberg](https://www.gutenberg.org/), as well as some short priming text.

### What is 'Generated'?

'Generated' samples are generated without conditioning on any starting text.",96,384
517,2019-5-9,2019,5,9,5,bmal0l,sense of humor at ICLR debate. schmidhuber,https://www.reddit.com/r/MachineLearning/comments/bmal0l/sense_of_humor_at_iclr_debate_schmidhuber/,september2014,1557348436,,0,1
518,2019-5-9,2019,5,9,6,bmb576,Ways to deploy ML in production?,https://www.reddit.com/r/MachineLearning/comments/bmb576/ways_to_deploy_ml_in_production/,brihat9135,1557351208,[removed],0,1
519,2019-5-9,2019,5,9,6,bmb9ar,What sort of work is being done on ultra-long range dependencies in Natural Language Processing,https://www.reddit.com/r/MachineLearning/comments/bmb9ar/what_sort_of_work_is_being_done_on_ultralong/,no_bear_so_low,1557351783,[removed],0,1
520,2019-5-9,2019,5,9,6,bmbczc,[P] Call for beta testers: Jupyter on iOS (x-post from /r/IPython),https://www.reddit.com/r/MachineLearning/comments/bmbczc/p_call_for_beta_testers_jupyter_on_ios_xpost_from/,navoshta,1557352491,"You might have heard about my other app Juno, a Jupyter *client* for iOS ([https://juno.sh](https://juno.sh))  I am now starting a TestFlight beta for a new iOS app I've been working on over the last year. It is based on Juno, but the coolest thing is that it doesn't need an external server anymore, as all notebooks run locally, right on your iPhone or iPad!

It currently ships with a **Python** kernel, **NumPy** and **Matplotlib** libraries, and, most likely, a considerable amount of bugs. You can also install pure-Python modules manually.

For instructions on how to install and report issues, please follow the link below:

[https://github.com/rationalmatter/Juno-Issue-Tracker](https://github.com/rationalmatter/Juno-Issue-Tracker)",8,12
521,2019-5-9,2019,5,9,7,bmc3k9,Could gpt-2 be useful for code generation?,https://www.reddit.com/r/MachineLearning/comments/bmc3k9/could_gpt2_be_useful_for_code_generation/,Jrowe47,1557356009,[removed],0,1
522,2019-5-9,2019,5,9,7,bmc5eh,[D] List of ICML 2019 Accepted Papers is out now,https://www.reddit.com/r/MachineLearning/comments/bmc5eh/d_list_of_icml_2019_accepted_papers_is_out_now/,gokstudio,1557356285,"[https://icml.cc/Conferences/2019/AcceptedPapersInitial?fbclid=IwAR0zqRJfPz2UP7dCbZ8Jcy7MrsedhasX13ueqkKl934EsksuSj3J2QrrRAQ](https://icml.cc/Conferences/2019/AcceptedPapersInitial?fbclid=IwAR0zqRJfPz2UP7dCbZ8Jcy7MrsedhasX13ueqkKl934EsksuSj3J2QrrRAQ)

Congratulations to the authors!",0,1
523,2019-5-9,2019,5,9,9,bmdeed,[P] Keras BERT for Medical Question Answer Retrieval using Tensorflow 2.0 ! With GPT-2 for Answer Generator. Pip installable. Weights/Data readily available. Reduced version for Google Colab instantly available in premade notebook.,https://www.reddit.com/r/MachineLearning/comments/bmdeed/p_keras_bert_for_medical_question_answer/,BatmantoshReturns,1557363393,"We fine-tuned a Keras version bioBert for Medical Question and Answering, and GPT-2 for answer generation. This was a project we submitted for the Tensorflow 2.0 Hackathon. 

We made all the weights and lookup data available, and made our github pip installable. 

We also have a float16 version of our data for running in Colab. Currently we weren't able to fit all the lookup data in their original float32 forms (although that may change as we get better data managing skills). (If you have some skills in this area you would like to share, we are hungry for it!).

Our models and predictor function are pip installable, to make it as easy as possible for people to try them. 

    !wget  https://anaconda.org/pytorch/faiss-cpu/1.2.1/download/linux-64/faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2
    !tar xvjf faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2
    !cp -r lib/python3.6/site-packages/* /usr/local/lib/python3.6/dist-packages/
    !pip install mkl
    
    !pip install tensorflow-gpu==2.0.0-alpha0
    import tensorflow as tf
    !pip install https://github.com/Santosh-Gupta/DocProduct/archive/master.zip

Here is our Colab folder

https://drive.google.com/drive/u/0/folders/1hSwWL_WqmcVJytMbsWSbhYxxK4KT7UMI

Here is our Github

https://github.com/Santosh-Gupta/DocProduct/blob/master/README.md

Here is our Devpost

https://devpost.com/software/nlp-doctor

Feel free to reach out with any feedback, questions, or comments. I'll answer all questions here. 

https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/806/964/datas/gallery.jpg

https://i.imgur.com/wzWt039.png

https://i.imgur.com/Z8DOXuJ.png

https://i.imgur.com/IRCyKIL.jpg?1

https://i.ytimg.com/vi/nPemP-Q0Xn8/hqdefault.jpg",30,74
524,2019-5-9,2019,5,9,10,bmdjp8,How to use model in production environment ?,https://www.reddit.com/r/MachineLearning/comments/bmdjp8/how_to_use_model_in_production_environment/,gandroz,1557364233,[removed],0,1
525,2019-5-9,2019,5,9,10,bmdx5x,Ideas for using Machine Learning for Servers/System Adminstration,https://www.reddit.com/r/MachineLearning/comments/bmdx5x/ideas_for_using_machine_learning_for/,TFYellowWW,1557366438,[removed],0,1
526,2019-5-9,2019,5,9,10,bme0js,OpenAI 2017 Finances (Form 990) now available,https://www.reddit.com/r/MachineLearning/comments/bme0js/openai_2017_finances_form_990_now_available/,throwaway5012912,1557366975,[removed],0,1
527,2019-5-9,2019,5,9,11,bme7s9,[N] OpenAI 2017 Tax filings just became available,https://www.reddit.com/r/MachineLearning/comments/bme7s9/n_openai_2017_tax_filings_just_became_available/,yellowcastle2509,1557368162,"OpenAI 2017 tax filings just became available, which detail its tax fillings, including compensation information:

[https://www.theregister.co.uk/2019/05/03/open\_ai\_finances/](https://www.theregister.co.uk/2019/05/03/open_ai_finances/)

&amp;#x200B;

It was reported by The Register last week, but somehow nobody else picked up on this. 

[https://www.theregister.co.uk/2019/05/03/open\_ai\_finances/](https://www.theregister.co.uk/2019/05/03/open_ai_finances/)

&amp;#x200B;

Note the differences with last year's tax filings:

[http://990s.foundationcenter.org/990\_pdf\_archive/810/810861541/810861541\_201612\_990.pdf](http://990s.foundationcenter.org/990_pdf_archive/810/810861541/810861541_201612_990.pdf)

[https://www.nytimes.com/2018/04/19/technology/artificial-intelligence-salaries-openai.html](https://www.nytimes.com/2018/04/19/technology/artificial-intelligence-salaries-openai.html)",25,21
528,2019-5-9,2019,5,9,12,bmerwh,Thinking Phone Addiction from Google I/O,https://www.reddit.com/r/MachineLearning/comments/bmerwh/thinking_phone_addiction_from_google_io/,yoshuakishi,1557371545,,0,1
529,2019-5-9,2019,5,9,12,bmf1ts,Question on Meh Samples from WGAN,https://www.reddit.com/r/MachineLearning/comments/bmf1ts/question_on_meh_samples_from_wgan/,grothendiecks_curse,1557373303,[removed],0,1
530,2019-5-9,2019,5,9,12,bmf2kd,Question-Related to Language Models,https://www.reddit.com/r/MachineLearning/comments/bmf2kd/questionrelated_to_language_models/,accountdetail,1557373440,"I have a list of words starting with the letter ""s"" and their frequency count. From this, I'm trying to build a language model. I don't have the whole text, so I can't do the conditional probability, so I calculated the global probabilities of each word. Now, how do I calculate the cross-entropy or perplexity of this distribution against a test data? Test data will have a certain number of words starting with the letter S.

Also, if I shouldn't have calculated the global probability, what else could I have done?

TLDR: I have the list of over 4000 words starting with letter s and their frequency distribution and have to build the language model to predict next word in the test data which contain 10-12 word starting with s letter. What are the best ways to approach this problem?

&amp;#x200B;

Any help is appreciated. Thank You!",0,1
531,2019-5-9,2019,5,9,14,bmfvyf,Self Driving Car | Behavior Cloning,https://www.reddit.com/r/MachineLearning/comments/bmfvyf/self_driving_car_behavior_cloning/,yrajsm,1557380528,,1,1
532,2019-5-9,2019,5,9,16,bmgkjj,[R] Neural Networks Usage at Mobile Development,https://www.reddit.com/r/MachineLearning/comments/bmgkjj/r_neural_networks_usage_at_mobile_development/,dashdevs,1557385603,,0,1
533,2019-5-9,2019,5,9,17,bmgz72,Bending Machine,https://www.reddit.com/r/MachineLearning/comments/bmgz72/bending_machine/,RapidFlowIndia1,1557388941,,0,1
534,2019-5-9,2019,5,9,18,bmhntm,[R] [1904.08483] Is deep learning a good choice for image segmentation? [deep learning is still too costly for segmenting large images],https://www.reddit.com/r/MachineLearning/comments/bmhntm/r_190408483_is_deep_learning_a_good_choice_for/,PeterPrinciplePro,1557394729,,2,0
535,2019-5-9,2019,5,9,18,bmht3f,[P] MiraPy: A Python package for Deep Learning in Astronomy,https://www.reddit.com/r/MachineLearning/comments/bmht3f/p_mirapy_a_python_package_for_deep_learning_in/,TheCuddlyWhiskers,1557395780,"MiraPy is a Python package for Deep Learning in Astronomy. It is built using Keras for developing ML models to run on CPU and GPU seamlessly. The aim is to make applying machine learning techniques on astronomical data easy for astronomers, researchers and students.

&amp;#x200B;

Github: [https://github.com/mirapy-org/mirapy](https://github.com/mirapy-org/mirapy)

## Applications

MiraPy can be used for problem-solving using ML techniques and will continue to grow to tackle new problems in Astronomy. Following are some of the experiments that you can perform right now:

* Classification of X-Ray Binaries using neural network
* Astronomical Image Reconstruction using Autoencoder
* Classification of the first catalogue of variable stars by ATLAS
* HTRU1 Pulsar Dataset Image Classification using Convolutional Neural Network
* Curve Fitting using Autograd (incomplete implementation)

You can find the applications MiraPy in our [tutorial](https://github.com/mirapy-org/tutorials) repository.

In future, MiraPy will be able to do more and in better ways and we need your suggestions! Tell us what you would like to see as a part of this package on [Slack](https://join.slack.com/t/mirapy/shared_invite/enQtNjEyNDQwNTI2NDY3LTE3ZmI3M2EyMjdkZWU4NTE2NjkxZjdhYWE4ZjUyODY0NzllNzRlMzZhNThhNWRiMjk4MjNhYWQ3NjA3YjJiNGY).",7,14
536,2019-5-9,2019,5,9,19,bmi6v0,The untold story of machine learning (ml).,https://www.reddit.com/r/MachineLearning/comments/bmi6v0/the_untold_story_of_machine_learning_ml/,Narayansahu379,1557398516,,0,1
537,2019-5-9,2019,5,9,20,bmiek7,What does Elon Musk has to say about this?,https://www.reddit.com/r/MachineLearning/comments/bmiek7/what_does_elon_musk_has_to_say_about_this/,kkantkosariya,1557399963,,1,1
538,2019-5-9,2019,5,9,20,bmilrq,[News] What does Elon Musk has to say about this?,https://www.reddit.com/r/MachineLearning/comments/bmilrq/news_what_does_elon_musk_has_to_say_about_this/,kkantkosariya,1557401286,,0,1
539,2019-5-9,2019,5,9,20,bmiogj,What is SoTA for 3d model/mesh generation?,https://www.reddit.com/r/MachineLearning/comments/bmiogj/what_is_sota_for_3d_modelmesh_generation/,DeepLearningDownundr,1557401765,[removed],0,1
540,2019-5-9,2019,5,9,20,bmip48,[N] What does Elon Musk has to say?,https://www.reddit.com/r/MachineLearning/comments/bmip48/n_what_does_elon_musk_has_to_say/,kkantkosariya,1557401891,,0,1
541,2019-5-9,2019,5,9,20,bmiq7n,"[P] Introducing creme, a new library for online machine learning",https://www.reddit.com/r/MachineLearning/comments/bmiq7n/p_introducing_creme_a_new_library_for_online/,Lemax0,1557402102,,0,2
542,2019-5-9,2019,5,9,20,bmisec,[D] Tensorflow 2.0 duhh!!,https://www.reddit.com/r/MachineLearning/comments/bmisec/d_tensorflow_20_duhh/,kkkosariya,1557402497,,0,1
543,2019-5-9,2019,5,9,21,bmiyqw,[D] Which AI hot topics caught your attention this month? Find out our favorites.,https://www.reddit.com/r/MachineLearning/comments/bmiyqw/d_which_ai_hot_topics_caught_your_attention_this/,t_sic,1557403597,[https://blog.sicara.com/04-2019-best-ai-new-articles-this-month-d5bacff26ded](https://blog.sicara.com/04-2019-best-ai-new-articles-this-month-d5bacff26ded),0,1
544,2019-5-9,2019,5,9,21,bmj5jg,Genderless voice AI could provide major step in combating implicit bias,https://www.reddit.com/r/MachineLearning/comments/bmj5jg/genderless_voice_ai_could_provide_major_step_in/,elviracoker,1557404744,,0,1
545,2019-5-9,2019,5,9,21,bmjabz,Email Spam Classifier discussion,https://www.reddit.com/r/MachineLearning/comments/bmjabz/email_spam_classifier_discussion/,sharan_dev,1557405524,[removed],0,1
546,2019-5-9,2019,5,9,21,bmjelz,Kervolutional Networks: implementation attempt,https://www.reddit.com/r/MachineLearning/comments/bmjelz/kervolutional_networks_implementation_attempt/,getZwiftyYeah,1557406205,[removed],0,1
547,2019-5-9,2019,5,9,21,bmjht9,[P] how to properly define a 2 models with 1 controller,https://www.reddit.com/r/MachineLearning/comments/bmjht9/p_how_to_properly_define_a_2_models_with_1/,theThinker6969,1557406706,"Hey all, I am trying to understand exactly what the process will be of creating 2 models (an RNN and a CNN) which both produce outputs and the outputs are passed to a controller like object which makes the final decision and controls the steering wheel in a self-driving car. 

my questions is I dont fully understand what I need to search for to get more information about this. What is it called? How does the controller execute? Can you direct me to some resources or post them here? 

thanks very much!",1,1
548,2019-5-9,2019,5,9,22,bmjj62,[P] Building a State-of-the-Art Conversational AI with Transfer Learning,https://www.reddit.com/r/MachineLearning/comments/bmjj62/p_building_a_stateoftheart_conversational_ai_with/,Thomjazz,1557406906," Building a SOTA Conversational AI leveraging transfer learning &amp; OpenAI GPT models

HuggingFace has just released a detailed tutorial, open-source github, pretrained model and online demo adapted from their NeurIPS 2018 ConvAI2 dialog competition model (which was SOTA on the automatic track).

Here are the links:

\- Detailed tutorial with code gists: https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313

\- Online demo of the pretrained model: convai.huggingface.co

\- Code repository and pretrained models: https://github.com/huggingface/transfer-learning-conv-ai

\- Blog post on ethics &amp; values around this release: https://medium.com/huggingface/ethical-analysis-of-the-open-sourcing-of-a-state-of-the-art-conversational-ai-852113c324b2",33,93
549,2019-5-9,2019,5,9,22,bmjqdc,[D] How to generate bounding boxes and masks at scale,https://www.reddit.com/r/MachineLearning/comments/bmjqdc/d_how_to_generate_bounding_boxes_and_masks_at/,AndriPi,1557407999,"I'm training an instance segmentation model and it's not doing great, but tests indicate that with a larger dataset performance improves. I thus need to accelerate the process of generating bounding boxes and segmentation masks for new images. Are there tools which can help me? [https://arxiv.org/pdf/1903.10830.pdf](https://arxiv.org/pdf/1903.10830.pdf) says they developed a pipeline to speedup annotation, but I don't think they released any tool.",3,1
550,2019-5-9,2019,5,9,22,bmjshk,How to properly share Google colab notebooks?,https://www.reddit.com/r/MachineLearning/comments/bmjshk/how_to_properly_share_google_colab_notebooks/,igorsusmelj,1557408323,[removed],1,2
551,2019-5-9,2019,5,9,22,bmju9b,AI algorithms learn to collude to keep prices high and maximize profits for online retailers,https://www.reddit.com/r/MachineLearning/comments/bmju9b/ai_algorithms_learn_to_collude_to_keep_prices/,Akkeri,1557408604,,0,1
552,2019-5-9,2019,5,9,22,bmjy5n,Gated Activation Unit (Wavenet) vs ReLU,https://www.reddit.com/r/MachineLearning/comments/bmjy5n/gated_activation_unit_wavenet_vs_relu/,khawarizmy,1557409204,[removed],0,1
553,2019-5-9,2019,5,9,22,bmk25b,Text report patterning,https://www.reddit.com/r/MachineLearning/comments/bmk25b/text_report_patterning/,HappySharkee,1557409825,[removed],0,1
554,2019-5-9,2019,5,9,22,bmk4xu,Introducing DLF - A Novel Generative Deep Learning Model called DLF (Dynamic Linear Flow),https://www.reddit.com/r/MachineLearning/comments/bmk4xu/introducing_dlf_a_novel_generative_deep_learning/,Tarry_Singh,1557410262,[removed],0,1
555,2019-5-9,2019,5,9,23,bmkaxq,Open Web Text Corpus,https://www.reddit.com/r/MachineLearning/comments/bmkaxq/open_web_text_corpus/,bil-sabab,1557411140,,0,1
556,2019-5-9,2019,5,9,23,bmkb4p,Knowledgebase Query Language Twitch Talk,https://www.reddit.com/r/MachineLearning/comments/bmkb4p/knowledgebase_query_language_twitch_talk/,DayMaker2019,1557411175,[removed],0,1
557,2019-5-9,2019,5,9,23,bmkgyf,Machine Learning Superfunctions - Twitch Talk,https://www.reddit.com/r/MachineLearning/comments/bmkgyf/machine_learning_superfunctions_twitch_talk/,DayMaker2019,1557412063,"Hey all, I've been looking into twitch and streaming videos. I saw that Wolfram is streaming live coding of machine learning superfunctions on the 16th. Heres the link if anyone is interested...

[https://www.twitch.tv/events/HirT2IhzTPyVvbeT9wVGXg](https://www.twitch.tv/events/HirT2IhzTPyVvbeT9wVGXg)",0,1
558,2019-5-9,2019,5,9,23,bmkkmf,Data Preprocessing using Pandas and Matplotlib,https://www.reddit.com/r/MachineLearning/comments/bmkkmf/data_preprocessing_using_pandas_and_matplotlib/,mQuBits,1557412608,,0,1
559,2019-5-9,2019,5,9,23,bmklel,Machine Learning for noobs.,https://www.reddit.com/r/MachineLearning/comments/bmklel/machine_learning_for_noobs/,chanduris,1557412723,,0,1
560,2019-5-9,2019,5,9,23,bmkljp,"Get a free early release copy of ""Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow"" from O'Reilly (E-Mail required)",https://www.reddit.com/r/MachineLearning/comments/bmkljp/get_a_free_early_release_copy_of_handson_machine/,jonjohns65,1557412745,,0,1
561,2019-5-9,2019,5,9,23,bmklnz,Unsupervised Meta-Learning for Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/bmklnz/unsupervised_metalearning_for_reinforcement/,Jaxon_K,1557412765,,1,6
562,2019-5-9,2019,5,9,23,bmklor,[D] Transfer learning from simple to complex task,https://www.reddit.com/r/MachineLearning/comments/bmklor/d_transfer_learning_from_simple_to_complex_task/,cs_on_detours,1557412768,"Hi, I'm currently working on another project and I'm considering using transfer learning. I want to train a network with simple data that is easy to produce and use the pretrained network as a starting point for a more complicated model.

Eg. train a model to recognizing letters from images. Then somehow 
use another network to recognize words from a image. 

Sure it is possible to create bounding boxes for each letter and then use the second network to predict them, but is there a way to apply transfer learning to a end to end approach?

Are there any papers in this area, any keywords that are describing these tasks.",2,1
563,2019-5-9,2019,5,9,23,bmkltf,Unsupervised Learning via Meta-Learning,https://www.reddit.com/r/MachineLearning/comments/bmkltf/unsupervised_learning_via_metalearning/,Jaxon_K,1557412785,,1,7
564,2019-5-9,2019,5,9,23,bmkolr,[R] Searching for MobileNetV3,https://www.reddit.com/r/MachineLearning/comments/bmkolr/r_searching_for_mobilenetv3/,Mxbonn,1557413198,,8,21
565,2019-5-9,2019,5,9,23,bmkpbm,[D] Limitations of adversarial robustness: strong No Free Lunch Theorem,https://www.reddit.com/r/MachineLearning/comments/bmkpbm/d_limitations_of_adversarial_robustness_strong_no/,LumbarLordosis,1557413300,"**Abstract**: This manuscript presents some new impossibility results on adversarial robustness in machine learning, a very important yet largely open problem. We show that if conditioned on a class label the data distribution satisfies the \`W\_2\` Talagrand transportation-cost inequality (for example, this condition is satisfied if the conditional distribution has density which is log-concave; is the uniform measure on a compact Riemannian manifold with positive Ricci curvature; etc.) any classifier can be adversarially fooled with high probability once the perturbations are slightly greater than the natural noise level in the problem. We call this result The Strong No Free Lunch Theorem as some recent results (Tsipras et al. 2018, Fawzi et al. 2018, etc.) on the subject can be immediately recovered as very particular cases. Our theoretical bounds are demonstrated on both simulated and real data (MNIST). We conclude the manuscript with some speculation on possible future research directions.

&amp;#x200B;

**Paper**:[https://arxiv.org/pdf/1810.04065.pdf](https://arxiv.org/pdf/1810.04065.pdf)

I don't have the necessary math background to understand the proofs in the paper. But I'm intrigued to intuitively understand what is the author saying.",3,3
566,2019-5-10,2019,5,10,0,bmkwto,Google CoLab error - wrong CuDNN library version being loaded?,https://www.reddit.com/r/MachineLearning/comments/bmkwto/google_colab_error_wrong_cudnn_library_version/,notimeforarcs,1557414388,[removed],0,1
567,2019-5-10,2019,5,10,0,bmkybv,[Research] A Collection of High-Quality Datasets for Product Representation Learning,https://www.reddit.com/r/MachineLearning/comments/bmkybv/research_a_collection_of_highquality_datasets_for/,cdossman,1557414601," [https://medium.com/ai%C2%B3-theory-practice-business/new-high-quality-datasets-for-product-representation-learning-ac3ad1492bf4](https://medium.com/ai%C2%B3-theory-practice-business/new-high-quality-datasets-for-product-representation-learning-ac3ad1492bf4) 

Abstract:  ProductNet is a collection of high-quality product datasets for better product understanding. ProductNet aims at supporting product representation learning by curating product datasets of high quality with properly chosen taxonomy. In this paper, the two goals of building high-quality product datasets and learning product representation support each other in an iterative fashion: the product embedding is obtained via a multi-modal deep neural network (master model) designed to leverage product image and catalog information; and in return, the embedding is utilized via active learning (local model) to vastly accelerate the annotation process.  For the labeled data, the proposed master model yields high categorization accuracy (94.7% top-1 accuracy for 1240 classes), which can be used as search indices, partition keys, and input features for machine learning models. The product embedding, as well as the fined-tuned master model for a specific business task, can also be used for various transfer learning tasks.",3,5
568,2019-5-10,2019,5,10,0,bmkyeq,[D]Follow The Regularized Leader(FTRL) algorithm to model user interests (which might change over time)?,https://www.reddit.com/r/MachineLearning/comments/bmkyeq/dfollow_the_regularized_leaderftrl_algorithm_to/,hunter7z,1557414612,"Hi there,

I've been working on news recommendation problem, and I'm implementing a factorization machine optimized by FTRL (the original paper is here: [https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/41159.pdf](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/41159.pdf)) to model user interests, in an online learning fashion. As users' reading interests might change over time, I want the model to capture this change rapidly, hopefully. But the learning rate of FTRL decays as the following equation:

&amp;#x200B;

*Processing gif 1wzmxtpt67x21...*

learning rate decreases monotonically over time, So as training goes for some certain time, model change might become very slow, thus hard to follow the users' interest.

What I'm trying is, do not accumulate the square of gradients from the beginning, just accumulate recent ones. In order to do this, I change to gradient accumulating line in the pseudo code to the following, where \\lambda is a number in (0,1), like 0.99 or something like that. In this way, I hope the gradients long ago make little contribution to the denominator.

Is there someone familiar with FTRL could tell me does this make sense, or is it valid in math? because the mathematics behind FTRL is just beyond me.

Thanks in advance : )

*Processing gif akokzohy97x21...*

[FTRL original implementation](https://i.redd.it/5qnc93fj97x21.png)",2,2
569,2019-5-10,2019,5,10,0,bml2sp,[P]: spectralGraphTopology an R package for learning graphs from data via spectral constraints,https://www.reddit.com/r/MachineLearning/comments/bml2sp/p_spectralgraphtopology_an_r_package_for_learning/,jvmirca,1557415252,"spectralGraphTopology provides estimators to learn k-component, bipartite, and k-component bipartite graphs from data by imposing spectral constraints on the eigenvalues and eigenvectors of the Laplacian and adjacency matrices. Those estimators leverages spectral properties of the graphical models as a prior information, which turn out to play key roles in unsupervised machine learning tasks such as community detection.

spectralGraphTopology is developed on GitHub: https://github.com/dppalomar/spectralGraphTopology

For documentation, installation instructions, and examples of usage, check out https://mirca.github.io/spectralGraphTopology",0,6
570,2019-5-10,2019,5,10,0,bml4r1,Can you distinguish fake from real?,https://www.reddit.com/r/MachineLearning/comments/bml4r1/can_you_distinguish_fake_from_real/,degroeten,1557415517,[removed],0,1
571,2019-5-10,2019,5,10,0,bml5pa,Need suggestions about the following project.,https://www.reddit.com/r/MachineLearning/comments/bml5pa/need_suggestions_about_the_following_project/,fuckOffDudeXD,1557415654,[removed],0,1
572,2019-5-10,2019,5,10,0,bml8wf,[P] Can you distinguish fake from real?,https://www.reddit.com/r/MachineLearning/comments/bml8wf/p_can_you_distinguish_fake_from_real/,degroeten,1557416120,"Dear AI-enthusiasts,

*Please help me out by doing a fun &amp; challenging task for science!*

For my master's thesis AI I'm researching fake images. I'm measuring how well people can distinguish fake from real.

Please consider filling out this survey, taking 5 to 10 minutes. Your answers are anonymous.

Important: it only works on a computer or tablet, not on a smartphone.

[https://nils.limequery.com/427969?lang=en](https://nils.limequery.com/427969?lang=en)

Challenge: can you label all 18 images correctly? Until now, only 6 out of \~200 people managed to do so!

Thanks a lot in advance!

*\[ If you can no longer fill out the survey, the required number of respondents has been reached. \]*",24,7
573,2019-5-10,2019,5,10,0,bmlj1b,hands on machine learning with python from scratch,https://www.reddit.com/r/MachineLearning/comments/bmlj1b/hands_on_machine_learning_with_python_from_scratch/,s_4_m_a_n,1557417540,"how to create dataset of picture for image classification NN in python from scratch ??
p.s excluding keras and tensorflow",0,1
574,2019-5-10,2019,5,10,1,bmll46,A Unified Framework for Structured Graph Learning via Spectral Constraints,https://www.reddit.com/r/MachineLearning/comments/bmll46/a_unified_framework_for_structured_graph_learning/,jvmirca,1557417826,,1,4
575,2019-5-10,2019,5,10,1,bmlmux,50+ Machine Learning Algorithms in One Chart,https://www.reddit.com/r/MachineLearning/comments/bmlmux/50_machine_learning_algorithms_in_one_chart/,ai-lover,1557418057,,0,1
576,2019-5-10,2019,5,10,1,bmlnch,Automated Machine learning testing of Android Graphical User Interface ideas,https://www.reddit.com/r/MachineLearning/comments/bmlnch/automated_machine_learning_testing_of_android/,codexblaze,1557418131,[removed],0,1
577,2019-5-10,2019,5,10,1,bmltsb,[Discussion] Machine Learning GUI Automation testing of Android applications,https://www.reddit.com/r/MachineLearning/comments/bmltsb/discussion_machine_learning_gui_automation/,codexblaze,1557419039,Any ideas/sources of how to implement a machine learning model/system used to test the GUI of an android application? My goal is to save android testers from writing test cases. I would like ML to test the UI by itself.,2,6
578,2019-5-10,2019,5,10,1,bmlxpd,MIT explains neural networks,https://www.reddit.com/r/MachineLearning/comments/bmlxpd/mit_explains_neural_networks/,aiforworld2,1557419597,,1,3
579,2019-5-10,2019,5,10,1,bmm3sa,[D] Why Linear Regression is not suitable for Binary Classification,https://www.reddit.com/r/MachineLearning/comments/bmm3sa/d_why_linear_regression_is_not_suitable_for/,ErmJustSaying,1557420397,,0,1
580,2019-5-10,2019,5,10,1,bmm7k9,Resources on Vectorized backpropagation,https://www.reddit.com/r/MachineLearning/comments/bmm7k9/resources_on_vectorized_backpropagation/,mohit_jarvis29,1557420930,"Hello Everyone

I am watching this lecture: [https://www.youtube.com/watch?v=jFXd9JbXMYk&amp;feature=youtu.be](https://www.youtube.com/watch?v=jFXd9JbXMYk&amp;feature=youtu.be) (CMU Deep Learning) and I am having some trouble understanding backpropagation in terms of Jacobians. Can anyone please point me towards a good resource to understand this?",0,1
581,2019-5-10,2019,5,10,2,bmmhvn,[P] SOD - An Embedded Computer Vision and Machine Learning Library,https://www.reddit.com/r/MachineLearning/comments/bmmhvn/p_sod_an_embedded_computer_vision_and_machine/,histoire_guy,1557422360,"### Project Homepage

 [sod.pixlab.io](https://sod.pixlab.io)

**Purpose**

SOD is an embedded, modern cross-platform computer vision and machine learning software library that expose a set of APIs for deep-learning, advanced media analysis &amp; processing including real-time, multi-class object detection and model training on embedded systems with limited computational resource and IoT devices.

 ### Docs

* API Documentation: https://sod.pixlab.io/api.html.
* Codes Sample: https://sod.pixlab.io/samples.html.
* Getting Started Guide: https://sod.pixlab.io/intro.html.
* Github Repository: https://github.com/symisc/sod.

*edit: Markdown*",3,44
582,2019-5-10,2019,5,10,2,bmmko0,Few-Shot Unsupervised Image-to-Image Translation,https://www.reddit.com/r/MachineLearning/comments/bmmko0/fewshot_unsupervised_imagetoimage_translation/,mingyuliutw,1557422752,[removed],0,1
583,2019-5-10,2019,5,10,2,bmml15,Help needed. Predicting events using LFPs.,https://www.reddit.com/r/MachineLearning/comments/bmml15/help_needed_predicting_events_using_lfps/,adwarakanath,1557422805,,0,1
584,2019-5-10,2019,5,10,2,bmmltf,[R] Few-Shot Unsupervised Image-to-Image Translation,https://www.reddit.com/r/MachineLearning/comments/bmmltf/r_fewshot_unsupervised_imagetoimage_translation/,mingyuliutw,1557422917,[removed],0,1
585,2019-5-10,2019,5,10,2,bmmn01,[R] Few-Shot Unsupervised Image-to-Image Translation,https://www.reddit.com/r/MachineLearning/comments/bmmn01/r_fewshot_unsupervised_imagetoimage_translation/,mingyuliutw,1557423083,,1,1
586,2019-5-10,2019,5,10,2,bmmt5g,Transitioning into Data Science from a Non-Technical Background,https://www.reddit.com/r/MachineLearning/comments/bmmt5g/transitioning_into_data_science_from_a/,Tamock,1557423928,[removed],0,1
587,2019-5-10,2019,5,10,2,bmmwss,[P] Finding structures vulnerable to disaster in street view imagery,https://www.reddit.com/r/MachineLearning/comments/bmmwss/p_finding_structures_vulnerable_to_disaster_in/,wronk17,1557424432,"My company is supporting the World Bank's program focused on resilient housing. Housing ""resiliency"" is a problem in poor urban areas around the world as these homes are often not up to code (e.g., because families are doing their own house construction or good materials aren't available). This makes the homes (and the families inside) vulnerable to disasters like earthquakes and hurricanes. Many governments in these countries have resources to retrofit the houses for safety, but it's inefficient for structural engineers to walk up and down neighborhoods to identify the vulnerable homes that need fixing.

We [just finished some pilot work](https://medium.com/devseed/finding-vulnerable-housing-in-street-view-images-using-ai-to-create-safer-cities-2b58b4e3b991?source=friends_link&amp;sk=6606e973db864a29f90f41dda99350cd) to identify building features that could confer risk. The features we detected here were

1. building material
2. whether or not a building looked designed by an architect
3. and whether or not construction appeared complete. 

&amp;#x200B;

This is definitely more application focused, so we just used TF's object detection API under the hood w/ an SSD backbone and trained on \~7000 labeled street view images. The fun part was relating street view detections to an overhead building footprint map -- something I talk about a lot in the blog. (Does anyone know how Google or other tech companies do this?) We're working on releasing that training data if anyone else is interested in this problem space.

&amp;#x200B;

One related question: does anyone know if TF OD API will change in TF 2.0?",2,9
588,2019-5-10,2019,5,10,2,bmmyae,[R] Few-Shot Unsupervised Image-to-Image Translation,https://www.reddit.com/r/MachineLearning/comments/bmmyae/r_fewshot_unsupervised_imagetoimage_translation/,mingyuliutw,1557424632,,49,867
589,2019-5-10,2019,5,10,3,bmn0og,[P] Language model (GPT-2) fine-tuned on 'The Office' dialogues,https://www.reddit.com/r/MachineLearning/comments/bmn0og/p_language_model_gpt2_finetuned_on_the_office/,CYHSM,1557424951,"I fine-tuned the newly released OpenAI GPT-2 345M model on all dialogues (Season 1 to 9, around 3MB) of 'The Office' and the model starts to acquire some humour, at least when reading it through the lens of the specific characters.  
Some of my favourites:

&gt;*""Michael: What is the most important thing for a company? Is it the cash flow? Is it the inventory? Oh, it's the closets? The closets are the best.""*  
\---------------------------------------------------------------------------------------------------------------------------------  
&gt;  
&gt;*""Michael: All right, time for another round of questions. First, how old are you?""*  
*""Phyllis: I'm 58. ""*  
*""Oscar: 58? ""*  
*""Kevin: Hey, hey, that's old, right? Can I get a raise?""*  
&gt;  
&gt;\---------------------------------------------------------------------------------------------------------------------------------  
&gt;  
&gt;*""Andy: This book's worth more to me than any number of my books. I mean, I don't care. One of them is a good book, the other is a bad book, and the next one is a joke. All right? So... my money's on one of the two, or zero, as they say in French, so that's fine. ""*

I published some more here and will add some after training the model for a bit longer:

\--&gt; [https://twitter.com/fateful\_by](https://twitter.com/fateful_by)

Some things I already noticed: 

* It got really dark at some points, e.g. *""Darryl: I'm going to take a little piece of ice. I'm going to stab it into your skull."" or ""Dwight: \[in Andy's car\] You know, I've always wanted to be a detective. So, when I come back, I'm gonna sniff out leads. And if I catch a murderer, I'm gonna go blow my brains out. Now, I don't know how that works. But I think I might be the best person to go out there and catch one of those serial killers. ""*
* Temperature setting above 0.7 gives some really crazy outputs, e.g. ""*Andy: Stanley Hudson! \[throws tie to Dwight, who throws it back\] Dwight: Sorry, I didn't mean to upset you. Ralph knows best. \[to Creed\] What's gay is who you know and what's deb (back to Oscar) You can't work here. You're a too busy hand. \[back to Toby\] You were fired for stealing the pen, right? Toby: Yesss! Bring it on!*""",6,31
590,2019-5-10,2019,5,10,3,bmn302,[P] PetSwap,https://www.reddit.com/r/MachineLearning/comments/bmn302/p_petswap/,mingyuliutw,1557425264,,0,1
591,2019-5-10,2019,5,10,3,bmn3g0,Learn AI by Actually Building Something,https://www.reddit.com/r/MachineLearning/comments/bmn3g0/learn_ai_by_actually_building_something/,PufffDaddy,1557425320," [https://exercises.ai/](https://exercises.ai/) 

&amp;#x200B;

Hi Friends,

&amp;#x200B;

When I started learning AI last year, I found a ton of excellent free lecture material, but I felt that the available exercises and community was extremely lacking. I wanted to learn by actually figuring out and coding neural nets. I also wanted to find other people interested in AI to work on projects. It's hard to actually understand neural nets until I dive into coding some mini projects.

&amp;#x200B;

I decided to build [exercises.ai](https://exercises.ai) to help people learn AI with a hands on approach. Not completed yet, but right now, you can browse around the collection of free lectures, or connect on Discord to talk about your project ideas or ask for coding help.",0,1
592,2019-5-10,2019,5,10,3,bmn54m,Looking for someone to work on a project with,https://www.reddit.com/r/MachineLearning/comments/bmn54m/looking_for_someone_to_work_on_a_project_with/,Anunoby3,1557425559,[removed],0,1
593,2019-5-10,2019,5,10,3,bmn6ee,Alan Turing Institute Releases ML Framework Written in Julia,https://www.reddit.com/r/MachineLearning/comments/bmn6ee/alan_turing_institute_releases_ml_framework/,Yuqing7,1557425739,,0,1
594,2019-5-10,2019,5,10,3,bmn9dw,An End-to-End AutoML Solution for Tabular Data at KaggleDays,https://www.reddit.com/r/MachineLearning/comments/bmn9dw/an_endtoend_automl_solution_for_tabular_data_at/,sjoerdapp,1557426135,,0,1
595,2019-5-10,2019,5,10,3,bmnlo0,Becoming MLE Without Masters?,https://www.reddit.com/r/MachineLearning/comments/bmnlo0/becoming_mle_without_masters/,learner134,1557427860,[removed],0,1
596,2019-5-10,2019,5,10,3,bmnmkw,[D] Questions about TFX after Google I/O'10 Talk: Machine Learning Pipelines and Model Understanding,https://www.reddit.com/r/MachineLearning/comments/bmnmkw/d_questions_about_tfx_after_google_io10_talk/,iamquah,1557427991,"[TensorFlow Extended: Machine Learning Pipelines and Model Understanding (Google I/O'19)](https://www.youtube.com/watch?v=drYM04t57tU) 

&amp;#x200B;

1) Does anyone know of where someone might find an example repo showing functioning code of all [TFX](https://www.tensorflow.org/tfx/guide) components? Maybe along the lines of a github repo or smth? E.g using `example_gen`, `statistics_gen`, etc.

&amp;#x200B;

2) In terms of deployment, is there anything to specifically look out for (esp with Tf2.0)? E.g eager mode doesn't work like graph mode in production or keras estimators might be faster or slower than a low-level op estimator? Would really appreciate some guidance. Didn't exactly find a central place",0,1
597,2019-5-10,2019,5,10,3,bmnn9z,[D] CIFAR-10 equivalents in video classification (action recognition),https://www.reddit.com/r/MachineLearning/comments/bmnn9z/d_cifar10_equivalents_in_video_classification/,ukhan_actual,1557428081,"So I've been working on some stuff with CNNs trained on CIFAR-10, and I'm interested in seeing if what I've been working on scales to video, i.e. with 3D CNNs.

Unfortunately, I haven't been able to find any CIFAR-10 equivalents for videos. Most of the well-recognized datasets (UCF-101, Kinetics, Moments in Time, etc) are absolutely massive and correspond more with ImageNet.

Does anyone know of recognized and (relatively) small-scale video datasets that are good for sanity-checking and won't require a massive overhead to work with? I have looked at UCF-11, but it is labeled by the authors as an ""incredibly challenging"" dataset, which is not what I'm looking for. Thanks for your help!",6,3
598,2019-5-10,2019,5,10,4,bmnuw4,Chimera: Binary Logistic Regression Robust Term finder,https://www.reddit.com/r/MachineLearning/comments/bmnuw4/chimera_binary_logistic_regression_robust_term/,Thistleknot,1557429098,,1,1
599,2019-5-10,2019,5,10,4,bmnvm2,"[D] This is CS50 Live with Nick Wong, discussing style transfers in the context of neural networks.",https://www.reddit.com/r/MachineLearning/comments/bmnvm2/d_this_is_cs50_live_with_nick_wong_discussing/,coltonoscopy,1557429202,,0,1
600,2019-5-10,2019,5,10,4,bmnvns,      / Cutting and Stitching Full course/ Lurn  || Silai Machine,https://www.reddit.com/r/MachineLearning/comments/bmnvns/_______cutting_and_stitching/,sanjay4u,1557429208,,0,1
601,2019-5-10,2019,5,10,4,bmnw3q,Rule 25. Relation to the original topic decreases with every single post,https://www.reddit.com/r/MachineLearning/comments/bmnw3q/rule_25_relation_to_the_original_topic_decreases/,Volfegan,1557429270,,0,1
602,2019-5-10,2019,5,10,4,bmnws3,"[News] These fashion models were created using AI, but they look spookily life-like",https://www.reddit.com/r/MachineLearning/comments/bmnws3/news_these_fashion_models_were_created_using_ai/,jpedrofs,1557429365,,0,1
603,2019-5-10,2019,5,10,4,bmocgr,A white-box machine learning approach for revealing antibiotic mechanisms of action,https://www.reddit.com/r/MachineLearning/comments/bmocgr/a_whitebox_machine_learning_approach_for/,scrummie9,1557431464,,0,1
604,2019-5-10,2019,5,10,4,bmod5s,"[News] These fashion models were created using AI, but they look spookily life-like",https://www.reddit.com/r/MachineLearning/comments/bmod5s/news_these_fashion_models_were_created_using_ai/,jpedrofs,1557431563,Is anyone aware of public research done on full body image generation?[Computer generated fashion models by DataGrid](https://www.businessinsider.com/datagrid-ai-created-spookily-life-like-fashion-models-2019-5),1,0
605,2019-5-10,2019,5,10,5,bmogy1,How do regression decision trees work?,https://www.reddit.com/r/MachineLearning/comments/bmogy1/how_do_regression_decision_trees_work/,__Mar__,1557432077,[removed],0,1
606,2019-5-10,2019,5,10,5,bmoh7e,[R] A white-box machine learning approach for revealing antibiotic mechanisms of action,https://www.reddit.com/r/MachineLearning/comments/bmoh7e/r_a_whitebox_machine_learning_approach_for/,scrummie9,1557432111,,0,0
607,2019-5-10,2019,5,10,5,bmoqyd,Ask Confluent: Machine Learning with Kai Waehner,https://www.reddit.com/r/MachineLearning/comments/bmoqyd/ask_confluent_machine_learning_with_kai_waehner/,vicksyu,1557433404,,0,1
608,2019-5-10,2019,5,10,6,bmppok,How to get from online courses to implementing papers and coming up with new ideas,https://www.reddit.com/r/MachineLearning/comments/bmppok/how_to_get_from_online_courses_to_implementing/,Krutika0109,1557438060,[removed],0,1
609,2019-5-10,2019,5,10,7,bmql08,Introducing TensorFlow Graphics: Computer Graphics Meets Deep Learning,https://www.reddit.com/r/MachineLearning/comments/bmql08/introducing_tensorflow_graphics_computer_graphics/,julienvalentin,1557442559,[removed],0,1
610,2019-5-10,2019,5,10,8,bmr36b,Any American &lt;=&gt; British English Translation Projects?,https://www.reddit.com/r/MachineLearning/comments/bmr36b/any_american_british_english_translation_projects/,throwmediawayemerg,1557445385,[removed],0,1
611,2019-5-10,2019,5,10,8,bmr4i6,Delayed Impact of Fair Machine Learning: A response,https://www.reddit.com/r/MachineLearning/comments/bmr4i6/delayed_impact_of_fair_machine_learning_a_response/,no_bear_so_low,1557445588,,0,1
612,2019-5-10,2019,5,10,10,bms1n9,Tech Companies Are Deleting Evidence of War Crimes,https://www.reddit.com/r/MachineLearning/comments/bms1n9/tech_companies_are_deleting_evidence_of_war_crimes/,eslamalmahdy,1557450943,,0,1
613,2019-5-10,2019,5,10,10,bmsalq,An Open Source Machine Learning Crash Course with Python,https://www.reddit.com/r/MachineLearning/comments/bmsalq/an_open_source_machine_learning_crash_course_with/,iramirsina,1557452419,,0,1
614,2019-5-10,2019,5,10,11,bmstij,[D] What are the state of the art architectures or implementations of audio classifiers?,https://www.reddit.com/r/MachineLearning/comments/bmstij/d_what_are_the_state_of_the_art_architectures_or/,itcouldbemuchworse,1557455506,"I'd like to read some papers and maybe play with an open implementation of an audio classifier, in order to take advantage of this classifier in the construction of a GAN.",7,4
615,2019-5-10,2019,5,10,12,bmt4km,"[R] eed help changing my approach to my BSc thesis (Deep Learning, NLP, classification)",https://www.reddit.com/r/MachineLearning/comments/bmt4km/r_eed_help_changing_my_approach_to_my_bsc_thesis/,the_parallax_II,1557457407,"Hello, 

I want to get involved with whichever task of classification (sentiment analysis, hate speech, etc.) regarding text data, by using deep learning for my bachelor thesis.

I am confused about what should be the aim of the dissertation.  I mean, I am not capable to come with something ground breaking or fancy (eg. A new complicated architecture).  Beyond building some architectures from papers and testing data on them, what can I do in order to make the thesis more interesting?

Something I have thought of, is to create a pipeline that will retrieve tweets from the Twitter API from different locations and then pass them to a trained model in order to perform some kind of classification and then create a visualization of the world map regarding the topic (eg. brexit-preferences of each country).  However, this doesn't make a lot of sense since that procedure could be done more naively by counting specific hashtags on a geographical location.  Also, how the dataset could be created and labeled?  

Can you help me reconstruct my idea or guide me to a better direction?",13,1
616,2019-5-10,2019,5,10,13,bmtr5v,"[R] Adversarial Examples Are Not Bugs, They Are Features",https://www.reddit.com/r/MachineLearning/comments/bmtr5v/r_adversarial_examples_are_not_bugs_they_are/,iyaja,1557461446,,3,4
617,2019-5-10,2019,5,10,14,bmugbv,[R] Importance Weighted Hierarchical Variational Inference,https://www.reddit.com/r/MachineLearning/comments/bmugbv/r_importance_weighted_hierarchical_variational/,asobolev,1557466389,"I just put online a preprint of the ""Importance Weighted Hierarchical Variational Inference"" paper.

The paper proposes a novel and efficient multisample variational upper bound on log q(z|x) in case of hierarchical proposal q(z|x). This way one can use [Neural Samplers](https://www.reddit.com/r/MachineLearning/comments/bhr53p/r_neural_samplers_and_hierarchical_variational/) (like VAE) as expressive proposal distributions, allowing us to learn more expressive models p(x).

&amp;#x200B;

[Paper](https://arxiv.org/abs/1905.03290) | [Talk](https://youtu.be/pdSu7XfGhHw) | [**Blogpost**](http://artem.sobolev.name/posts/2019-05-10-importance-weighted-hierarchical-variational-inference.html)",9,6
618,2019-5-10,2019,5,10,14,bmuij8,What would be the best recommendation system for Leetcode problems?,https://www.reddit.com/r/MachineLearning/comments/bmuij8/what_would_be_the_best_recommendation_system_for/,JakubJancto,1557466863,[removed],0,1
619,2019-5-10,2019,5,10,14,bmuimt,AI Use of Time-Series Data obtained from sensors,https://www.reddit.com/r/MachineLearning/comments/bmuimt/ai_use_of_timeseries_data_obtained_from_sensors/,ELE_MEDIA,1557466881,,0,1
620,2019-5-10,2019,5,10,14,bmuisq,[P] Implementing Searching for MobileNetV3 paper using Pytorch,https://www.reddit.com/r/MachineLearning/comments/bmuisq/p_implementing_searching_for_mobilenetv3_paper/,leaderj1001,1557466915,"Hi, I'm Myeongjun Kim. My major is computer vision research using deep learning. This is the fourth paper implementation. There are many things to learn while implementing paper. Thank you for your continued interest. Feedback is always welcome. I are currently working on CIFAR-100, and I will be working on IMAGENET soon. However, please understand that it takes time because of lack of GPU.  
Thank you for reading the long story.  
\[Github URL\]:  [https://github.com/leaderj1001/MobileNetV3-Pytorch](https://github.com/leaderj1001/MobileNetV3-Pytorch)",7,9
621,2019-5-10,2019,5,10,15,bmuqms,"Is there a program out there, preferably a local software that can up-res .mp4 or .mkv files?",https://www.reddit.com/r/MachineLearning/comments/bmuqms/is_there_a_program_out_there_preferably_a_local/,anthonygerdes2003,1557468588,[removed],0,1
622,2019-5-10,2019,5,10,15,bmutzl,Introducing TensorFlow Graphics: Computer Graphics Meets Deep Learning,https://www.reddit.com/r/MachineLearning/comments/bmutzl/introducing_tensorflow_graphics_computer_graphics/,julienvalentin,1557469310,,0,1
623,2019-5-10,2019,5,10,15,bmuy44,Introducing TensorFlow Graphics: Computer Graphics Meets Deep Learning,https://www.reddit.com/r/MachineLearning/comments/bmuy44/introducing_tensorflow_graphics_computer_graphics/,julienvalentin,1557470218,[removed],0,1
624,2019-5-10,2019,5,10,15,bmuz31,Deep learning generalizes because the parameter-function map is biased towards simple functions,https://www.reddit.com/r/MachineLearning/comments/bmuz31/deep_learning_generalizes_because_the/,vwvwvvwwvvvwvwwv,1557470446,,0,1
625,2019-5-10,2019,5,10,15,bmv11e,Introducing TensorFlow Graphics: Computer Graphics Meets Deep Learning,https://www.reddit.com/r/MachineLearning/comments/bmv11e/introducing_tensorflow_graphics_computer_graphics/,julienvalentin,1557470860,,2,1
626,2019-5-10,2019,5,10,16,bmv69g,TensorFlow 2.0 CIFAR classification.,https://www.reddit.com/r/MachineLearning/comments/bmv69g/tensorflow_20_cifar_classification/,matchagreenteaislife,1557472010,[removed],0,1
627,2019-5-10,2019,5,10,16,bmv788,[Project] CIFAR classification using TF2.0,https://www.reddit.com/r/MachineLearning/comments/bmv788/project_cifar_classification_using_tf20/,matchagreenteaislife,1557472210,CIFAR classification using TF2.0. The codes can be found at [cifar-tf-eager](https://github.com/xingzhaolee/cifar-tf-eager). Feel free to use it as a reference template! Hopefully it'll be useful for people who's interested in trying.  I'll add more features when I have the time. :),0,1
628,2019-5-10,2019,5,10,16,bmv81x,[P] CIFAR classification with TF2.0.,https://www.reddit.com/r/MachineLearning/comments/bmv81x/p_cifar_classification_with_tf20/,matchagreenteaislife,1557472385,Feel free to use [it](https://github.com/xingzhaolee/cifar-tf-eager) as a template for TF2.0 training codes. I'll add more models and features when I have the time. :),0,0
629,2019-5-10,2019,5,10,16,bmv9dg,Has anyone tried train- gpt2 on algo datasets?,https://www.reddit.com/r/MachineLearning/comments/bmv9dg/has_anyone_tried_train_gpt2_on_algo_datasets/,FreckledMil,1557472675,"I know that often times with the low end gpt2 model, the text that is generated does not make sense/isn't topical. But often times, you see some rather clever ""expressions"" mixed within, that seem to hit the nail on the head in a roundabout way.

Has anyone used a dataset of every single piece of code on github for ML? Would it be possible to find a needle in the haystack of outputs that shows a line of code that makes a human lightbulb turn on? 

This question is probably stupid, but figured I would posit.",1,1
630,2019-5-10,2019,5,10,16,bmvhvk,Sigmoid before softmax seems to work wonders,https://www.reddit.com/r/MachineLearning/comments/bmvhvk/sigmoid_before_softmax_seems_to_work_wonders/,deep_learner_all,1557474652,"Hi all,

In a classification task in PyTorch framework, I see that, if instead of using a linear FC layer at the end without an activation function performs poorly than sending the outputs followed by a sigmoid function to torch.nn.CrossEntropyLoss.  This loss function computes softmax internally.   
I have no intuition why this might work but when I read about it online, there seem to be two different answers,  
1. Don't do this

2. If you do this learning will be harder.   
My observations are on the other end, where doing this helps.

 Can I justify it as saying, since DNN's learn a Non-linear representation everywhere, why not another one at the end. I know softmax is a non-linear operation too.",0,1
631,2019-5-10,2019,5,10,16,bmvjqv,Road Damage management using AI and Image Processing,https://www.reddit.com/r/MachineLearning/comments/bmvjqv/road_damage_management_using_ai_and_image/,aniruddh1998,1557475119,,0,1
632,2019-5-10,2019,5,10,17,bmvmkw,Best cloud setup for StyleGAN,https://www.reddit.com/r/MachineLearning/comments/bmvmkw/best_cloud_setup_for_stylegan/,albusde,1557475817,[removed],0,1
633,2019-5-10,2019,5,10,17,bmvo5t,The Contrast Machine Vs. Deep Learning,https://www.reddit.com/r/MachineLearning/comments/bmvo5t/the_contrast_machine_vs_deep_learning/,technewsninja,1557476211,,0,1
634,2019-5-10,2019,5,10,17,bmvr0d,On the novelty of gpt-2,https://www.reddit.com/r/MachineLearning/comments/bmvr0d/on_the_novelty_of_gpt2/,__olamilekan__,1557476933,Is gpt-2 a novelty because of the number of parameters it was trained with or because of some novel architecture (modelling decision) behind it?,0,1
635,2019-5-10,2019,5,10,18,bmwax7,Elegant MicroWeb (Smarten SSDP Solution) Named as a Representative Vendors in Gartner April 2019 'Market Guide for Data Preparation Tools',https://www.reddit.com/r/MachineLearning/comments/bmwax7/elegant_microweb_smarten_ssdp_solution_named_as_a/,ElegantMicroWebIndia,1557481538,,0,1
636,2019-5-10,2019,5,10,19,bmwer6,[D] How to perform significance testing on experiments with multiple random seeds?,https://www.reddit.com/r/MachineLearning/comments/bmwer6/d_how_to_perform_significance_testing_on/,tetramarek,1557482437,"Hi all,

This is a question that isn't really touched upon in most deep learning research, so thought I would reach out to the community for advice.

With neural networks, it is sensible practice to run the same experiment multiple times with different seeds, reporting the mean and standard deviation. This helps factor out the effects that random initialisation has on the model. 
However, how do you then perform significance testing on the results?
Picking a single seed seems random and arbitrary. Using all of them makes the significance test think the test dataset is much bigger than it actually is, which also doesn't sound right. Aligning the outputs from only from the same seeds seems random as well, as all the seeds are supposed to be equivalent.

Do you have any suggestions on how to handle this? Or have you seen any papers attempting to solve this problem? Thanks!",15,10
637,2019-5-10,2019,5,10,19,bmwf7u,Deploying models to production research - need your input,https://www.reddit.com/r/MachineLearning/comments/bmwf7u/deploying_models_to_production_research_need_your/,tixocloud,1557482533,"Hey guys, anyone working for a startup or a medium-sized organization as a data scientist? Im currently doing some research on deploying models to production and would love your input. This should only take 15 mins of your time and I would be forever grateful! Thanks in advance.",0,1
638,2019-5-10,2019,5,10,19,bmwn7d,Artificial intelligence and machine learning change the global economy in 3days up to $978.78 billion dollars.,https://www.reddit.com/r/MachineLearning/comments/bmwn7d/artificial_intelligence_and_machine_learning/,Narayansahu379,1557484249,,0,1
639,2019-5-10,2019,5,10,19,bmwu5x,[D] [P] content and pattern recognition,https://www.reddit.com/r/MachineLearning/comments/bmwu5x/d_p_content_and_pattern_recognition/,HappySharkee,1557485793,"Hello, 

I have a report that is generated by the system, the report is in Text format, it is fairly structured, however certain sections in the report sometimes they can expand or shrink, sample of the report is attached . 

I would like to convert this report into a CSV or Excel format, so:

1. can pattern recognition help me ? how ? 
2. is there a library part of a programming language that i can use ?

[sample](https://i.redd.it/ljrmofd2x6x21.jpg)",7,3
640,2019-5-10,2019,5,10,20,bmx0c3,[D] Comparing two GPT-2 models,https://www.reddit.com/r/MachineLearning/comments/bmx0c3/d_comparing_two_gpt2_models/,mekass,1557487043,"Hello,

&amp;#x200B;

How do you think what is the best method to compare two GPT-2 (345M and 345M fined-tuned) models? How to choose which one is better?",6,1
641,2019-5-10,2019,5,10,20,bmx9sl,[R] ICLR 2019 Notes,https://www.reddit.com/r/MachineLearning/comments/bmx9sl/r_iclr_2019_notes/,pdxdabel,1557488825,,0,1
642,2019-5-10,2019,5,10,21,bmxigb,[R] ICLR 2019 Notes,https://www.reddit.com/r/MachineLearning/comments/bmxigb/r_iclr_2019_notes/,pdxdabel,1557490333,"I just returned from my first ICLR in New Orleans -- [some notes from the conference available here](https://david-abel.github.io/notes/iclr_2019.pdf).

I hope folks find them useful! Happy to discuss topics from the conference as well.",19,224
643,2019-5-10,2019,5,10,21,bmxjdl,Machine Learning in 5 minutes,https://www.reddit.com/r/MachineLearning/comments/bmxjdl/machine_learning_in_5_minutes/,kennet2611,1557490490,,0,1
644,2019-5-10,2019,5,10,21,bmxllq,"so what are the best open source for AI face gender, age detection ?",https://www.reddit.com/r/MachineLearning/comments/bmxllq/so_what_are_the_best_open_source_for_ai_face/,wilsonckao,1557490874,[removed],0,1
645,2019-5-10,2019,5,10,21,bmxotq,Machine learning for dummies series,https://www.reddit.com/r/MachineLearning/comments/bmxotq/machine_learning_for_dummies_series/,Kokila_Kumaraguru,1557491417,,0,1
646,2019-5-10,2019,5,10,21,bmxqkb,Anonymous mock medical records/reports for testing,https://www.reddit.com/r/MachineLearning/comments/bmxqkb/anonymous_mock_medical_recordsreports_for_testing/,TheThad2,1557491718,"Hi, 

I am looking for a sample of mock medical reports / records that I can use for developing and testing a solution. Does something legal and open exist for development and testing purposes? I am not looking for anonymized data falling under regulatory requirements, but mock data that is free to access...something like the Enron corpus but for unstructured medial reports. 

&amp;#x200B;

Thanks.",1,1
647,2019-5-10,2019,5,10,22,bmy30l,Democratization of AI,https://www.reddit.com/r/MachineLearning/comments/bmy30l/democratization_of_ai/,skanda_s,1557493722,,0,1
648,2019-5-10,2019,5,10,22,bmy3mk,Are my bachelor thesis topics to Advanced?,https://www.reddit.com/r/MachineLearning/comments/bmy3mk/are_my_bachelor_thesis_topics_to_advanced/,mbalazs98,1557493817,[removed],0,1
649,2019-5-10,2019,5,10,22,bmyd7y,[D] How-to: Reusing / Citing Previous Version of (Pre-print) Paper,https://www.reddit.com/r/MachineLearning/comments/bmyd7y/d_howto_reusing_citing_previous_version_of/,josauder,1557495279,"Hi, not entirely sure this is the best place to ask, but I'm sure many of you are writing something for NeurIPS ATM.

Earlier this year, I wrote a paper, which I posted on ArXiv, which was then rejected from ICML in a close call. Ever since, I have addressed the concerns issued by the reviewers, and in the process, came up with something fundamentally better, but obviously within the exact same topic. I want to submit the new version for double-blind peer review and am asking for insights from the community!

Should I cite the pre-print? The pre-print was never peer reviewed and all results in the new version are better.

Can I simply reuse large portions of the pre-print (i.e. word for word)? Much of the old paper (e.g. introduction, related work) could be reused without any changes and be just as correct. However, I am afraid that a reviewer will stumble upon the preprint and either accuse me of plagiarizing (from myself, I guess), or infer that this might be a violation of double-blind review.

Happy to hear your thoughts!",4,5
650,2019-5-10,2019,5,10,22,bmyibp,[Research] A more efficient evolutionary algorithm? Can it help neural network training?,https://www.reddit.com/r/MachineLearning/comments/bmyibp/research_a_more_efficient_evolutionary_algorithm/,RTengx,1557496081,"  


[Fast convergence of Duelist Algorithm](https://i.redd.it/nr2wfbkc0ex21.png)

  
Paper : [https://arxiv.org/ftp/arxiv/papers/1512/1512.00708.pdf](https://arxiv.org/ftp/arxiv/papers/1512/1512.00708.pdf)  
Various researchers ( [https://arxiv.org/pdf/1712.06567.pdf](https://arxiv.org/pdf/1712.06567.pdf) ), DeepMind ([https://deepmind.com/blog/population-based-training-neural-networks/](https://deepmind.com/blog/population-based-training-neural-networks/) ) and OpenAI ( [https://arxiv.org/pdf/1703.03864.pdf](https://arxiv.org/pdf/1703.03864.pdf) ) have highlighted the applicability of evolutionary methods (especially genetic algorithms) on training neural networks. The main reason is because evolutionary-based training can be easily parallelized and is highly-scalable. However, if we have a more efficient evolutionary algorithm, it is most probable that better performing neural networks can be trained. Can duelist algorithm speed up evolutionary-based training? Can anyone test it?

  
Python implementation of Duelist Algorithm: [https://github.com/tsyet12/Duelist-Algorithm-Python](https://github.com/tsyet12/Duelist-Algorithm-Python)",11,19
651,2019-5-10,2019,5,10,23,bmyqz5,Nvidia SPADE Paper Implementation using fastai,https://www.reddit.com/r/MachineLearning/comments/bmyqz5/nvidia_spade_paper_implementation_using_fastai/,divyanshjha,1557497362,,0,1
652,2019-5-10,2019,5,10,23,bmyt42,[P] Spectrogram + CRNN for Urban Sound classification in PyTorch,https://www.reddit.com/r/MachineLearning/comments/bmyt42/p_spectrogram_crnn_for_urban_sound_classification/,the_curious_badger,1557497676,"[This project](https://github.com/ksanjeevan/crnn-audio-classification) allows to easily train a CNN/RNN/CRNN on the [UrbanSound8k](https://urbansounddataset.weebly.com/urbansound8k.html) dataset using on-the-fly Spectrogram computation in PyTorch. 

The Spectrogram is computed on GPU as a layer using [torchaudio_contrib](https://github.com/keunwoochoi/torchaudio-contrib), come check it out and help us improve/discuss!",0,17
653,2019-5-10,2019,5,10,23,bmz05v,Machine Learning and urology. Can anyone help me with how to approach this?,https://www.reddit.com/r/MachineLearning/comments/bmz05v/machine_learning_and_urology_can_anyone_help_me/,Bob_11,1557498734,[removed],0,1
654,2019-5-11,2019,5,11,0,bmzne3,Udacity Machine Learning any good?,https://www.reddit.com/r/MachineLearning/comments/bmzne3/udacity_machine_learning_any_good/,trngoon,1557502021,[removed],0,1
655,2019-5-11,2019,5,11,0,bmzopg,Questions about Unity ML Agents with multiple cloned environments,https://www.reddit.com/r/MachineLearning/comments/bmzopg/questions_about_unity_ml_agents_with_multiple/,Roboserg,1557502212,[removed],0,1
656,2019-5-11,2019,5,11,0,bmzrrm,Best way to spend work training funds?,https://www.reddit.com/r/MachineLearning/comments/bmzrrm/best_way_to_spend_work_training_funds/,trngoon,1557502621,[removed],0,1
657,2019-5-11,2019,5,11,0,bmzsjr,"[P] Out of the box, train your own Question and Answer retrieval model in TF 2.0 with Keras-Bert in Google Colaboratory. Architecture installed from Pip, pre-trained models downloaded / loaded, and example run with sample data. All you have to do is provide your own data in the format specified.",https://www.reddit.com/r/MachineLearning/comments/bmzsjr/p_out_of_the_box_train_your_own_question_and/,BatmantoshReturns,1557502728,,1,1
658,2019-5-11,2019,5,11,0,bmzt92,Need suggestions about the following project.,https://www.reddit.com/r/MachineLearning/comments/bmzt92/need_suggestions_about_the_following_project/,fuckOffDudeXD,1557502820,[removed],0,1
659,2019-5-11,2019,5,11,0,bmzz3u,Google AI | Unsupervised Depth Estimation for Arbitrary Videos,https://www.reddit.com/r/MachineLearning/comments/bmzz3u/google_ai_unsupervised_depth_estimation_for/,Yuqing7,1557503641,,0,1
660,2019-5-11,2019,5,11,0,bmzzze,[P] Using deepfake to bring Salvador Dali back to life,https://www.reddit.com/r/MachineLearning/comments/bmzzze/p_using_deepfake_to_bring_salvador_dali_back_to/,AtreveteTeTe,1557503759,"I've been working on a project for the past year using deepfake faceswap to bring Salvador Dali ""back to life"" on some interactive screens at the Dali Museum.

There's an [article](https://www.theverge.com/2019/5/10/18540953/salvador-dali-lives-deepfake-museum) about the project in The Verge and this behind the scenes [video](https://www.youtube.com/watch?v=BIDaxl4xqJ4) explains the project.  

This is done with a mix of faceswap results from both the [dfaker](https://github.com/dfaker/df/) and [faceswap](https://github.com/deepfakes/faceswap) repos on GitHub.  Results are further refined in After Effects.  Our archival source footage of Dali used to train the network is 60 years old and shot in black and white, so in addition to the faceswap, we're colorizing the results.

We tried using ML to recreate Dali's voice as well, but the results just aren't good enough yet.  Someday soon, hopefully!",46,219
661,2019-5-11,2019,5,11,1,bn020h,[D] Need advice,https://www.reddit.com/r/MachineLearning/comments/bn020h/d_need_advice/,Elemental_Ray,1557504039,"I have just completed Deep Learning Specialization of Andrew Ng and I have previously attended a summer school on Machine learning . I have not done any other project or course . 
Companies like Amazon , Google , Goldman Sachs will start coming for hiring interns in about 2 months in my college . What should I do in these two months to get an internship ?",2,2
662,2019-5-11,2019,5,11,1,bn08rg,"[N] TOP 5 FACTORS DRIVING DEVELOPMENT OF ARTIFICIAL INTELLIGENCE, MACHINE LEARNING AND BIG DATA",https://www.reddit.com/r/MachineLearning/comments/bn08rg/n_top_5_factors_driving_development_of_artificial/,vadhavaniyafaijan,1557504976,,0,1
663,2019-5-11,2019,5,11,1,bn0elu,I need help picking my next project,https://www.reddit.com/r/MachineLearning/comments/bn0elu/i_need_help_picking_my_next_project/,Cheaky_Axiom,1557505797,[removed],0,1
664,2019-5-11,2019,5,11,1,bn0gsa,What science says about this viral breastmilk photo,https://www.reddit.com/r/MachineLearning/comments/bn0gsa/what_science_says_about_this_viral_breastmilk/,eslamalmahdy,1557506099,,0,1
665,2019-5-11,2019,5,11,1,bn0hsw,[R] Introducing TensorFlow Graphics: Computer Graphics Meets Deep Learning,https://www.reddit.com/r/MachineLearning/comments/bn0hsw/r_introducing_tensorflow_graphics_computer/,julienvalentin,1557506238,"TensorFlow Graphics is out! This new library brings 3D and graphics functionalities to TensorFlow!

&amp;#x200B;

Blog post announcement: [https://medium.com/tensorflow/introducing-tensorflow-graphics-computer-graphics-meets-deep-learning-c8e3877b7668](https://medium.com/tensorflow/introducing-tensorflow-graphics-computer-graphics-meets-deep-learning-c8e3877b7668)

GitHub: [https://github.com/tensorflow/graphics](https://github.com/tensorflow/graphics)",13,86
666,2019-5-11,2019,5,11,2,bn156k,[D] Is it fair to say that AlphaZero(Chess) is the closest we have to machine learning without innate priors?,https://www.reddit.com/r/MachineLearning/comments/bn156k/d_is_it_fair_to_say_that_alphazerochess_is_the/,Hadse,1557509400,"Want to hear your thoughts about this. As far as i know, this machine has learned everything itself. No innate knowledge (except the rules?). So one can argue that this machine basically has no innate priors?",10,3
667,2019-5-11,2019,5,11,2,bn175b,Does anyone know of a gate-implemented quantum neuron/perceptron?,https://www.reddit.com/r/MachineLearning/comments/bn175b/does_anyone_know_of_a_gateimplemented_quantum/,darthmaeu,1557509654,[removed],0,1
668,2019-5-11,2019,5,11,2,bn1cl9,"""Diversity and a Bayesian approach to inference in Bayesian networks"" - This paper does not exist!",https://www.reddit.com/r/MachineLearning/comments/bn1cl9/diversity_and_a_bayesian_approach_to_inference_in/,Harzoo_Zo_Morakh,1557510379,[removed],0,1
669,2019-5-11,2019,5,11,2,bn1g3o,Reinforcement learning for the real world with Dr. John Langford and Rafah Hosn,https://www.reddit.com/r/MachineLearning/comments/bn1g3o/reinforcement_learning_for_the_real_world_with_dr/,myinnerbanjo,1557510858,,0,1
670,2019-5-11,2019,5,11,2,bn1hzo,"[P] ""Diversity and a Bayesian approach to inference in Bayesian networks"" - This paper does not exist!",https://www.reddit.com/r/MachineLearning/comments/bn1hzo/p_diversity_and_a_bayesian_approach_to_inference/,Harzoo_Zo_Morakh,1557511111,[removed],0,1
671,2019-5-11,2019,5,11,3,bn226u,Photo Wake-Up: An Impressive AI-Powered Algorithm That Creates 3D Animations From Still Images (Video Demo),https://www.reddit.com/r/MachineLearning/comments/bn226u/photo_wakeup_an_impressive_aipowered_algorithm/,bit-man,1557513786,,0,1
672,2019-5-11,2019,5,11,4,bn2cnf,[R] [1905.02450] MASS: Masked Sequence to Sequence Pre-training for Language Generation from MS,https://www.reddit.com/r/MachineLearning/comments/bn2cnf/r_190502450_mass_masked_sequence_to_sequence/,bobchennan,1557515205,,1,1
673,2019-5-11,2019,5,11,4,bn2da0,[R] [1905.02450] MASS: Masked Sequence to Sequence Pre-training for Language Generation (from Microsoft),https://www.reddit.com/r/MachineLearning/comments/bn2da0/r_190502450_mass_masked_sequence_to_sequence/,bobchennan,1557515294,,9,7
674,2019-5-11,2019,5,11,4,bn2fko,Can AutoML solve the data science skills shortage? Please see today's article in Information Week. #AutoML,https://www.reddit.com/r/MachineLearning/comments/bn2fko/can_automl_solve_the_data_science_skills_shortage/,andrew_westend,1557515611,,0,1
675,2019-5-11,2019,5,11,4,bn2jqh,Google I/O 2019 | Geoffrey Hinton Says Machines Can Do Anything Humans Can,https://www.reddit.com/r/MachineLearning/comments/bn2jqh/google_io_2019_geoffrey_hinton_says_machines_can/,Yuqing7,1557516186,,0,1
676,2019-5-11,2019,5,11,4,bn2ono,Computer Intelligence Group with industry professionals,https://www.reddit.com/r/MachineLearning/comments/bn2ono/computer_intelligence_group_with_industry/,nowsden,1557516838,[removed],0,1
677,2019-5-11,2019,5,11,4,bn2ya5,[P] Computer Intelligence Group with industry professionals,https://www.reddit.com/r/MachineLearning/comments/bn2ya5/p_computer_intelligence_group_with_industry/,nowsden,1557518140,"Hello I've made a discord server for beginners and experts to talk about the newest tech and the newest things in the software and AI industry. For example me, I work on a website that is all about quantitative finance, I use different ML and DL models there and also different statistical models. Curious ?

For more, just join the server: [https://discord.gg/Wx9wXa4](https://discord.gg/Wx9wXa4)

Thanks for reading :)",1,0
678,2019-5-11,2019,5,11,4,bn2ytj,Need advice on college selection,https://www.reddit.com/r/MachineLearning/comments/bn2ytj/need_advice_on_college_selection/,mooshugoat,1557518221,I wanted to know how good is the Purdue Fort Wayne campus for ML. Like what are my opportunities and how are they compared to similar rated universities. Mostly I will be selecting this university (due to driving distance) and wanted to know more about their program in CS (MS). I wanted to mainly focus on the ML courses that they will offer.,0,1
679,2019-5-11,2019,5,11,5,bn36rz,[N] Machine Learning Zero to Hero (Google I/O'19) by Laurence Moroney and Karmel Allison,https://www.reddit.com/r/MachineLearning/comments/bn36rz/n_machine_learning_zero_to_hero_google_io19_by/,asuagar,1557519280,,0,1
680,2019-5-11,2019,5,11,5,bn370z,[R] Adversarial Training with Voronoi Constraints,https://www.reddit.com/r/MachineLearning/comments/bn370z/r_adversarial_training_with_voronoi_constraints/,marckkhoury,1557519316,"Paper: https://arxiv.org/abs/1905.01019

We analyze adversarial examples using techniques from high-dimensional geometry. We demonstrate several problems with the standard formulation of adversarial training using Lp-balls centered on the data, which occur in high-dimensional space. From these insights, we propose a new geometric constraint to replace the Lp-balls which gives improved robustness in some settings.

I'm the lead author of this article and would be happy to answer any questions!",2,11
681,2019-5-11,2019,5,11,5,bn37e9,Newspaper Image Database,https://www.reddit.com/r/MachineLearning/comments/bn37e9/newspaper_image_database/,nagaranudit,1557519365,[removed],0,1
682,2019-5-11,2019,5,11,5,bn3nq5,[R] NeurIPS 2019: The MineRL Competition for Sample-Efficient Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/bn3nq5/r_neurips_2019_the_minerl_competition_for/,MadcowD,1557521578,,0,1
683,2019-5-11,2019,5,11,5,bn3pw3,"Looking for clickable open source ML demos of fraud detection, prediction, image, speech, etc..",https://www.reddit.com/r/MachineLearning/comments/bn3pw3/looking_for_clickable_open_source_ml_demos_of/,plotti,1557521874,[removed],0,1
684,2019-5-11,2019,5,11,6,bn3vwc,[AI application] Let your machine teach itself to play flappy bird!,https://www.reddit.com/r/MachineLearning/comments/bn3vwc/ai_application_let_your_machine_teach_itself_to/,1991viet,1557522727,,1,1
685,2019-5-11,2019,5,11,6,bn40vz,Are you in the corporate world or the academic world?,https://www.reddit.com/r/MachineLearning/comments/bn40vz/are_you_in_the_corporate_world_or_the_academic/,himanshuragtah1,1557523440,[removed],0,2
686,2019-5-11,2019,5,11,6,bn440k,A Real-World Application of Deep Learning at Industrial Scale,https://www.reddit.com/r/MachineLearning/comments/bn440k/a_realworld_application_of_deep_learning_at/,crubier,1557523918,,0,1
687,2019-5-11,2019,5,11,6,bn49p2,ICML 2019 Accepted Paper Stats,https://www.reddit.com/r/MachineLearning/comments/bn49p2/icml_2019_accepted_paper_stats/,AndreasDoerr,1557524771,"I recently compiled some stats and figures regarding the accepted papers at this years International Conference on Machine Learning (ICML). All data is taken from [https://icml.cc/Conferences/2019/AcceptedPapersInitial](https://icml.cc/Conferences/2019/AcceptedPapersInitial)

Here you go:

![img](wes8e3jjdgx21 ""Top contributing institutes @ ICML 2019 according to the number of papers with at least one author affiliated with this institute. Ordered according to the total number of papers (followed by number of first and last author papers)."")

Here are some figures about top contributing authors.

![img](aoo6jrk5egx21 ""Top contributing authors @ ICML 2019 according to the total number of papers"")

![img](to2iftz9egx21 ""Top contributing authors @ ICML 2019 according to their number of last author papers."")

![img](jhzjn1ifegx21 ""Top contributing authors @ ICML 2019 according to their number of first author papers."")

Finally, some stats for top contributing institutes sorted by their relative contribution (i.e. how many authors on a paper a actually from this institute).

![img](60jnl7ciegx21)

I compiled these stats for my employer, the Robert Bosch GmbH. If you're interested in Bosch and what we're doing in AI and ML, check out [https://www.bosch-ai.com/](https://www.bosch-ai.com/) 

# Disclaimer

Cleaning the website data, in particular the affiliations is a tedious, manual process, since many different and not necessarily unambiguous  notations and abbreviations exist for many institutes. I tried my best  to merge affiliation names into distinct institute buckets. However, there might be errors in the data, leading to single papers not being  counted for a specific institute. Same applies for author names, which are NOT manually merged due to the large number of authors. Only identical author names have been associated across different publications.",0,1
688,2019-5-11,2019,5,11,7,bn4p4a,How to design functions that work in TensorFlow 2.0 eager and graph mode,https://www.reddit.com/r/MachineLearning/comments/bn4p4a/how_to_design_functions_that_work_in_tensorflow/,pgaleone,1557527088,,0,1
689,2019-5-11,2019,5,11,8,bn5cw9,Necessary Skills for ML/AI TPM,https://www.reddit.com/r/MachineLearning/comments/bn5cw9/necessary_skills_for_mlai_tpm/,KarlWalters,1557530843,"**Background:** I currently work as a Senior Data Scientist and have worked as a Data Scientist for the last 5 years.  Before that I was an Analyst. My education involved NLP and regression applied to the prediction of violent behavior. I've managed increasingly complex projects, products, and programs ranging from international development projects to large scale user segmentation and Bandit ad optimization at a large tech company. I have held the title Technical Program Manager and for that role worked with Engineering, Data Eng, ML, ML Platform Eng, and Data Science to create an in product funnel routing and ad targeting system that guides users from the second they land on a site, through purchasing various SKU's, to possibly downgrading or churning. I am getting fairly bored with the repetitive nature of Data Science projects and feel that the field itself has hit a saturation point. I'm just not all that motivated or excited anymore about the work that comes with this position/title.

**Question:** What do you look for in a ML/DL TPM? I would say I'm fairly familiar with most current concepts and methods, but quite honestly will always lack the formal education and skills to truly be successful as an ML Engineer myself. If you were me and you were applying to such positions, what blend of technical and PM skills would you emphasize?",0,1
690,2019-5-11,2019,5,11,9,bn5xil,[D] How does your machine learning algorithm are indrustialized ?,https://www.reddit.com/r/MachineLearning/comments/bn5xil/d_how_does_your_machine_learning_algorithm_are/,schrute_dataeng,1557534370,"Hello r/MachineLearning,

Disclaimer, I am a data engineer. I hope this post have its place here, I apologies in advanced if not. 

I would love to hear what is your experience regarding industrialization of your ML code ? With who do you work to do so? What are the struggling if any ? If you have any tips/best practices to share ?

I will start : 
I work with data scientists to release in production machine learning pipeline. We have been improving our collaboration over the past year, and thus delivering more efficiently. First by drafting a blue print of a machine learning pipeline, then converging on common tools, finally by sharing our knowledges on both skills. 

I have shared our experience here (2nd part) : https://medium.com/dailymotion/collaboration-between-data-engineers-data-analysts-and-data-scientists-97c00ab1211f. 

Our shared tech stack is mainly: BigQuery, Apache Beam (to distribute the preprocessing), docker, tensorflow/Keras, Apache airflow. 

This has lead to data scientists being more autonomous regarding scalability.

I think collaboration is the key.",14,0
691,2019-5-11,2019,5,11,11,bn6uv6,Can you solve it? Sandwich sudoku - a new puzzle goes viral,https://www.reddit.com/r/MachineLearning/comments/bn6uv6/can_you_solve_it_sandwich_sudoku_a_new_puzzle/,eslamalmahdy,1557540455,,0,1
692,2019-5-11,2019,5,11,11,bn6zrx,Is there any framework which I can use for my machine learning workflow?,https://www.reddit.com/r/MachineLearning/comments/bn6zrx/is_there_any_framework_which_i_can_use_for_my/,ML_coder,1557541404,[removed],0,1
693,2019-5-11,2019,5,11,11,bn73rj,"[P] Not sure if this is too silly for this reddit but I used GPT-2 345M to recreate the famous (but fake) ""I forced a bot to write an Olive Garden commercial"" tweet, but I fine-tuned GPT-2 on Star Trek scripts first...",https://www.reddit.com/r/MachineLearning/comments/bn73rj/p_not_sure_if_this_is_too_silly_for_this_reddit/,JonathanFly,1557542158,"**Link To The Post:**

[https://iforcedabot.com/i-forced-a-bot-to-watch-over-1000-of-star-trek-episodes-and-then-asked-it-to-write-1000-olive-garden-commercials/](https://iforcedabot.com/i-forced-a-bot-to-watch-over-1000-of-star-trek-episodes-and-then-asked-it-to-write-1000-olive-garden-commercials/)

&amp;#x200B;

**What is this?**

About a year ago, a [tweet](https://twitter.com/KeatonPatti/status/1006961202998726665) went viral purporting to be written by a bot: *I forced a bot to watch over 1,000 hours of Olive Garden commercials and then asked it to write an Olive Garden commercial of its own. Here is the first page.*

It was a funny script but it was written by a comedy writer, not a bot. This was obvious to some people, but not others, and enough people that it was real that sites wrote [articles debunking it](https://gizmodo.com/dont-be-fooled-by-the-forced-a-bot-meme-1826832915).

I've had this in the back of mind for while. It **used** to be obvious that scripts like this were fake -- anyone who had used an RNN network knew they could only hold a coherent thought for about two sentences, there's no way it would produce something like that.  But since that tweet GPT-2 came out and changed everything. I was looking for something to test fine-tuning the new GPT-345M on and picked the dumbest silliest option.

I can't tell you exactly why I also it on Star Trek The Next Generation and Deep Space Nine scripts except that it was hilarious to me.

&amp;#x200B;

**How it works**

There is no Olive Garden or commercial data in the training set, that's all Star Trek, but GPT-2 is shockingly good at writing lines appropriate for a waitress or restaurant scenes 100% are not in the Star Trek Training Material. 

&amp;#x200B;

**Are these hand selected samples?**

Nope.  The title says 1,000 commercials but it's actually over 30,000 commercials now -- I couldn't resist trying different prompts and tweaks to the training data to get better results.  I didn't filter the samples at all so it's a wide mix of iterations, temperatures, learning rates, tweaked training sets, I pretty much just tossed everything up there.  I was overall impressed with how well GPT-2 worked with both very little fine-tuning, and how it avoided overfitting with a more training.  I trained these same Trek scripts with the smaller GPT-2 and definitely ran into overfitting.",1,4
694,2019-5-11,2019,5,11,12,bn7gz9,[N] Using Prodmodel to speed up data science development and productionization,https://www.reddit.com/r/MachineLearning/comments/bn7gz9/n_using_prodmodel_to_speed_up_data_science/,gsvigruha,1557544693,"[https://github.com/prodmodel/prodmodel](https://github.com/prodmodel/prodmodel)

I built a tool which keeps track of all code and data deps of your data science project. It caches partial results and can figure out if a particular output (model, transformed data, code library) has to be recomputed before the fact. This can save huge amounts of time during an iterative development process.

Setting up usage is similar to build systems like Bazel or Make ([https://github.com/prodmodel/prodmodel/blob/master/example/build.py](https://github.com/prodmodel/prodmodel/blob/master/example/build.py)).

Feedback, users, contributors and constructive criticism is welcome!",0,1
695,2019-5-11,2019,5,11,12,bn7hvq,"Multilabel classifier where the I have the percentage of votes towards one label, how does one train such a model?",https://www.reddit.com/r/MachineLearning/comments/bn7hvq/multilabel_classifier_where_the_i_have_the/,0xdel,1557544868,[removed],0,1
696,2019-5-11,2019,5,11,12,bn7jqg,How a viral image of breasts exposes science's obsession with the male body,https://www.reddit.com/r/MachineLearning/comments/bn7jqg/how_a_viral_image_of_breasts_exposes_sciences/,eslamalmahdy,1557545232,,0,1
697,2019-5-11,2019,5,11,13,bn82ze,[N] ICML 2019 Accepted Paper Stats,https://www.reddit.com/r/MachineLearning/comments/bn82ze/n_icml_2019_accepted_paper_stats/,AndreasDoerr,1557548973,"I recently compiled some stats and figures regarding the accepted papers at this years International Conference on Machine Learning (ICML). All data is taken from [https://icml.cc/Conferences/2019/AcceptedPapersInitial](https://icml.cc/Conferences/2019/AcceptedPapersInitial)

Here you go:

[Top contributing institutes @ ICML 2019 according to the number of papers with at least one author affiliated with this institute. Ordered according to the total number of papers \(followed by number of first and last author papers\).](https://i.redd.it/wdbw91yheix21.png)

Here are some figures about top contributing authors.

[Top contributing authors @ ICML 2019 according to the total number of papers](https://i.redd.it/kovwct2jeix21.png)

[Top contributing authors @ ICML 2019 according to their number of first author papers.](https://i.redd.it/r2drkqkkeix21.png)

[Top contributing authors @ ICML 2019 according to their number of last author papers.](https://i.redd.it/1mo7is3meix21.png)

Finally, some stats for top contributing institutes sorted by their relative contribution (i.e. how many authors on a paper a actually from this institute).

https://i.redd.it/o1783e2oeix21.png

I compiled these stats for my employer, the Robert Bosch GmbH. If you're interested in Bosch and what we're doing in AI and ML, check out https://www.bosch-ai.com/

# Disclaimer

Cleaning the website data, in particular the affiliations is a tedious, manual process, since many different and not necessarily unambiguous notations and abbreviations exist for many institutes. I tried my best to merge affiliation names into distinct institute buckets. However, there might be errors in the data, leading to single papers not being counted for a specific institute. Same applies for author names, which are NOT manually merged due to the large number of authors. Only identical author names have been associated across different publications.",55,280
698,2019-5-11,2019,5,11,14,bn8k7r,[Discussion] How to use machine learning for Android testing?,https://www.reddit.com/r/MachineLearning/comments/bn8k7r/discussion_how_to_use_machine_learning_for/,codexblaze,1557552481,"Given the popularity of machine learning, how has machine learning been applied for crash testing Android applications' GUI?
What are the tools that one can use to achieve this?",2,0
699,2019-5-11,2019,5,11,15,bn8u1z,[R] Guided evolutionary strategies: escaping the curse of dimensionality in random search,https://www.reddit.com/r/MachineLearning/comments/bn8u1z/r_guided_evolutionary_strategies_escaping_the/,downtownslim,1557554500,,3,8
700,2019-5-11,2019,5,11,16,bn9h2q,[P] Diversity and a Bayesian approach to inference in Bayesian networks - This paper does not exist!,https://www.reddit.com/r/MachineLearning/comments/bn9h2q/p_diversity_and_a_bayesian_approach_to_inference/,Harzoo_Zo_Morakh,1557559793,"[thispaperdoesnotexist](https://thispaperdoesnotexist)


""We consider the problem of inferring the probability of a probabilistic
Bayesian network from observations in a sequence of discrete components. We
first assume a Bayesian network to be bounded by a set of discrete
frames, which can be regarded as a hierarchical structure. We then use
diversity to guide the inference process, by leveraging a Bayesian inference
algorithm to show that the probabilistic network can be executed with a
probabilistic network architecture that is both more efficient and more
powerful than that of the state-of-the-art. We show empirically that our
algorithms are more helpful than the state of the art in inference in
Bayesian networks.""

With everyone trying their hand at NLG I tried to train the recently released 345M version GPT-2 model on ~30k papers from arxiv. 

The results look pretty great to me. Few of my friends had a hard time believing the titles and abstracts have been generated by the model and not written by a human.

Made a simple webapp to show a few generated samples [here](https://thispaperdoesnotexist)

I'm still figuring out how to format the output data in a nice way and display in the webapp without taking too much space. The webapp currently has ~80 random samples out of ~1000 samples the model generated. I had to manually format the output for them to be displayed using JavaScript.

[Github](https://github.com/agoel00/thispaperdoesnotexist)

[All generated samples](https://drive.google.com/open?id=108zkq0aCBKayIsGBpdfBfZ8UJDa-D2UT)

Special thanks to [Max Woolf](https://github.com/minimaxir/gpt-2-simple)",0,1
701,2019-5-11,2019,5,11,16,bn9h79,[D] Should I bring up ethics in my thesis?,https://www.reddit.com/r/MachineLearning/comments/bn9h79/d_should_i_bring_up_ethics_in_my_thesis/,Valiox,1557559822,"For my master's thesis, I'm working on a model that could potentially be misused with malicious intents. It's getting a common theme lately.

I've got to admit that I don't have a strong sense of morals and that I work on that project mostly because I love to see machine learning in action, and I don't really care of the actual applications of my model. I intend to make my trained models public and the code open source. While I doubt any of it will actually cause havoc, I feel like the topic of responsible AI is looming to make a paragraph in my thesis. I don't really like it, because I'd prefer to entirely avoid the subject. So far, I've only written

&gt; (...) is attractive for a range of applications be they useful, merely a matter of customization, or mischievous.

I feel like it would make me look bad to entirely avoid the topic when I'm planning to make my work publicly available.",18,6
702,2019-5-11,2019,5,11,17,bn9vx3,"[D] Top 100+ Artificial Intelligence, Machine Learning, and Data Science Courses",https://www.reddit.com/r/MachineLearning/comments/bn9vx3/d_top_100_artificial_intelligence_machine/,vadhavaniyafaijan,1557563647,,0,1
703,2019-5-11,2019,5,11,17,bn9yum,ML Job Prep.,https://www.reddit.com/r/MachineLearning/comments/bn9yum/ml_job_prep/,yrajsm,1557564445,,0,1
704,2019-5-11,2019,5,11,18,bna9le,Using Python Trained Machine Learning Models from Phoenix (Elixir) Applications,https://www.reddit.com/r/MachineLearning/comments/bna9le/using_python_trained_machine_learning_models_from/,pmz,1557567191,,0,1
705,2019-5-11,2019,5,11,18,bna9z3,WatchGAN: Advancing generated watch images with styleGANs,https://www.reddit.com/r/MachineLearning/comments/bna9z3/watchgan_advancing_generated_watch_images_with/,ixeption,1557567284,"Hey folks, 

I just want to share my experiments with GANs on an data set of watches.

[WatchGAN: Using generative adversarial networks for artificial generated watch art](http://digital-thinking.de/watchgan-using-generative-adversarial-networks-for-artificial-generated-watch-art/)

&amp;#x200B;

![img](hfgi47oawjx21 ""Artificial generated watch art"")

[WatchGAN: Advancing generated watch images with styleGANs](http://digital-thinking.de/watchgan-advancing-generated-watch-images-with-stylegans/):

![img](9nhkghkgvjx21 ""Artificial generated watch images"")

Cheers",0,1
706,2019-5-11,2019,5,11,18,bnactv,[Project] WatchGAN: Advancing generated watch images with styleGANs,https://www.reddit.com/r/MachineLearning/comments/bnactv/project_watchgan_advancing_generated_watch_images/,ixeption,1557568004,"Hi folks, 

I just want to share my experiments with GANs and data set of watches:

[WatchGAN: Using generative adversarial networks for artificial generated watch art](http://digital-thinking.de/watchgan-using-generative-adversarial-networks-for-artificial-generated-watch-art/)

&amp;#x200B;

[Artificial generated watch art image](https://i.redd.it/e52qalkkzjx21.jpg)

[WatchGAN: Advancing generated watch images with styleGANs](http://digital-thinking.de/watchgan-advancing-generated-watch-images-with-stylegans/)

&amp;#x200B;

*Processing img jvpx4n3pzjx21...*

Cheers",11,12
707,2019-5-11,2019,5,11,18,bnadid,Looking for appropriate data set to apply word embedding algorithm followed by clustering algorithm,https://www.reddit.com/r/MachineLearning/comments/bnadid/looking_for_appropriate_data_set_to_apply_word/,omers66,1557568173,[removed],0,1
708,2019-5-11,2019,5,11,19,bnago4,Machine Learning and Algorithms,https://www.reddit.com/r/MachineLearning/comments/bnago4/machine_learning_and_algorithms/,SquareTechAcademy,1557568942,[removed],0,1
709,2019-5-11,2019,5,11,19,bnahvg,what does the PSO algorithm optimize?,https://www.reddit.com/r/MachineLearning/comments/bnahvg/what_does_the_pso_algorithm_optimize/,vardemy,1557569244,[removed],0,1
710,2019-5-11,2019,5,11,19,bnaps9,LHD S.p.A Is Worldwide Leader in designing and building Telescopic Forks,https://www.reddit.com/r/MachineLearning/comments/bnaps9/lhd_spa_is_worldwide_leader_in_designing_and/,lhd121,1557570985,,0,1
711,2019-5-11,2019,5,11,20,bnb603,"Why don't the ""\sigma"" part of VAE tend to 0?",https://www.reddit.com/r/MachineLearning/comments/bnb603/why_dont_the_sigma_part_of_vae_tend_to_0/,whatthefua,1557574869,[removed],0,1
712,2019-5-11,2019,5,11,21,bnbi1p,VAE for Data Augmentation?,https://www.reddit.com/r/MachineLearning/comments/bnbi1p/vae_for_data_augmentation/,ongteckwu,1557577410,,0,1
713,2019-5-11,2019,5,11,21,bnbllo,Playing Minecraft Photo Realistic using Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/bnbllo/playing_minecraft_photo_realistic_using/,yannick3AI,1557578121,,0,1
714,2019-5-11,2019,5,11,21,bnbo9r,"Execute ""Music-Generation"" project without ""Azure machine learning services""",https://www.reddit.com/r/MachineLearning/comments/bnbo9r/execute_musicgeneration_project_without_azure/,prashantkr314,1557578681,[removed],0,1
715,2019-5-11,2019,5,11,22,bnc3m3,how should we interpret soft targets in knowledge distillation?,https://www.reddit.com/r/MachineLearning/comments/bnc3m3/how_should_we_interpret_soft_targets_in_knowledge/,mynameisvinn,1557581577,"in the paper [""distilling the knowledge in a neural network""](https://arxiv.org/abs/1503.02531), the authors claim that ""when the soft targets have high entropy, they provide much more information per training case than hard targets..."".

how is this claim consistent with the accepted definition of entropy, which suggests high entropy carry less information?",0,1
716,2019-5-11,2019,5,11,23,bnclyd,Machine Learning (Cold Start problem and how to proceed),https://www.reddit.com/r/MachineLearning/comments/bnclyd/machine_learning_cold_start_problem_and_how_to/,SquareTechAcademy,1557584720,[removed],0,1
717,2019-5-11,2019,5,11,23,bncpri,"Free: Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow In its entirety.",https://www.reddit.com/r/MachineLearning/comments/bncpri/free_handson_machine_learning_with_scikitlearn/,aiforworld2,1557585347,,0,1
718,2019-5-12,2019,5,12,1,bndr92,GENETICALLY TWEAKED VIRUSES JUST SAVED A VERY SICK TEEN,https://www.reddit.com/r/MachineLearning/comments/bndr92/genetically_tweaked_viruses_just_saved_a_very/,eslamalmahdy,1557591167,,0,1
719,2019-5-12,2019,5,12,1,bne6lp,[D] A 2019 guide to 3D Human Pose Estimation,https://www.reddit.com/r/MachineLearning/comments/bne6lp/d_a_2019_guide_to_3d_human_pose_estimation/,cbsudux,1557593446,"Human Pose estimation is an important problem that has enjoyed the attention of the Computer Vision community for the past few decades. It is an important step towards understanding people in images and videos. This post covers the basics of 3D Human Pose Estimation and reviews the literature on the topic.

&amp;#x200B;

Article Link: [https://blog.nanonets.com/human-pose-estimation-3d-guide/](https://blog.nanonets.com/human-pose-estimation-3d-guide/)",0,1
720,2019-5-12,2019,5,12,2,bnegl6,[P] I'm working on a program to help teach Q-Learning,https://www.reddit.com/r/MachineLearning/comments/bnegl6/p_im_working_on_a_program_to_help_teach_qlearning/,bioa10,1557594910,"During my last Machine Learning course I was working with Q-Learning and made a small program that allows users to create maps for a Q-Learning agent to solve. I am using a game engine called Godot for the visuals and coded in Godot's custom language GDScript. My teacher felt that I should post this online for others to use to help learn Q-Learning. I plan to add menu options to provide a detailed walkthrough of how the program works and possibly other Q-Learning example projects.

Github: https://github.com/bioa10/QTable",0,4
721,2019-5-12,2019,5,12,2,bneic6,Learn AI collaboratively by saving the planet. New AI Challenge. You can apply now!,https://www.reddit.com/r/MachineLearning/comments/bneic6/learn_ai_collaboratively_by_saving_the_planet_new/,faviovaz,1557595179,,0,1
722,2019-5-12,2019,5,12,2,bnejs3,[D] What does the feed-forward neural network in Transformer architecture actually learn?,https://www.reddit.com/r/MachineLearning/comments/bnejs3/d_what_does_the_feedforward_neural_network_in/,deepaurorasky,1557595393,"So, I've been doing a deep dive into understanding Transformer (in the Neural Machine Translation context).  
I've found [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) and [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) much help. 

&amp;#x200B;

So, after great lengths I think I've gotten solid intuition on what the self-attention layer will learn. It will essentially learn a **contextualized** meaning for each word in the input function (correct me if I'm wrong here).

  
Then each of those ""contextualized-meaning embeddings"" are then put through the same 2 layer, fully connected feed-forward network - which has an output of the same size (512), with a much larger hidden layer.   


The output is then feed into the next Transformer layer, with a batch norm, and a residual connection along for the ride (Going to try leave them out of this for a while if possible)   


Do we have any idea what the that feed-forward neural network actually learns? What is it's purpose?  
Or why the same feedforward is applied to each ""contextualized word""? Is it sort of learning what might be important? (But then again, didn't the WO vector that took the multi-head attention matrices into a single matrix learn to do the same thing?)",8,143
723,2019-5-12,2019,5,12,2,bnew0s,"[D] Training on the cloud: GCP GPU pricing seems dramatically cheaper, why would you train on AWS or Azure?",https://www.reddit.com/r/MachineLearning/comments/bnew0s/d_training_on_the_cloud_gcp_gpu_pricing_seems/,Obventio,1557597226,"Like the title says, looking at the cost of entry-level GPU instances on the major clouds:

AWS: p2.xlarge -- 1 Tesla K80, 4 vCPUs, 61gb ram **$0.900/hr**
Azure: NC6 -- 1 Tesla K80, 6 vCPUs, 56gb ram **$0.900/hr**
GCP: 1 Tesla K80, 6 vCPUs, 52gb ram **$0.663/hr**

Further, for training CNNs on the K80 I never exceed 4-5gb of memory usage or reasonable utilization of 4 vCPUs. Since GCP is the only cloud that gives me ability to finely tune specs I can even further decrease cost for ML applications. For example:

GCP: 1 Tesla K80, 4 vCPUs, 5gb ram **$0.424/hr**

When benchmarking resnet50, this cheaper configuration provides no performance decrease compared to the more expensive instance.

Perhaps spot instance pricing (low-priority for azure, preemptible for GCP) comes into play, where GCP is in the middle of the pack:

AWS: p2.xlarge -- 1 Tesla K80, 4 vCPUs, 61gb ram **$0.270/hr**
Azure: NC6 -- 1 Tesla K80, 6 vCPUs, 56gb ram **$0.180/hr**
GCP: 1 Tesla K80, 6 vCPUs, 52gb ram **$0.236/hr**

This kind of instance, however, does not work for every use case so the regular on-demand pricing difference is still significant.

This all leaves me wondering:
If you train your models on the cloud, which provider do you use?

Can you imagine any reasons/use cases/etc that might warrant picking a provider other than GCP?

What is GCPs business model? How can they make money selling for so much less? Is this a loss leader to gain market share?",20,17
724,2019-5-12,2019,5,12,3,bnf2ok,[P] Can a Generative Adversarial Network (GAN) learn to create new and weird icons?,https://www.reddit.com/r/MachineLearning/comments/bnf2ok/p_can_a_generative_adversarial_network_gan_learn/,NaxAlpha,1557598213,"Hey folks,

I have been working on a customizable implementation of  vanilla DC GAN. I scrapped icons dataset across the internet. Then converted those icons to MNIST like images and trained GAN. Here is link to my project.

 [https://github.com/NaxAlpha/xgan](https://github.com/NaxAlpha/xgan) 

And the follow up blog post:

 [https://medium.com/@NaxAlpha/this-icon-does-not-exist-an-application-of-gans-to-icon-generation-5442f0f867a](https://medium.com/@NaxAlpha/this-icon-does-not-exist-an-application-of-gans-to-icon-generation-5442f0f867a) 

During training, I tried different architectures and batch sizes. Some results were as following:

\- network learned to create hand-drawn icons after \~70 epochs but generator failed after \~200 epochs

Do you guys have any suggestions to improve results?

Thanks",4,0
725,2019-5-12,2019,5,12,3,bnf5ju,[P] unbalanced SVM results,https://www.reddit.com/r/MachineLearning/comments/bnf5ju/p_unbalanced_svm_results/,ConorDrew,1557598640,"Hi, im doing a project where I am classifying foreground and background elements using HOG features and SVM ,   
I have a sliding window that goes over an image and extracts hog features, it compares each result to a ground truth image and stores the foreground and background in the correct array. 

as the background is a lot larger i am currently taking the first x amount from background to match up with the foreground to make it even. 

&amp;#x200B;

is this a good idea or should i run it with all the background data and make it unbalanced, or even more so, is it ok to keep repeating the foreground to match the background (i think this is a very bad / pointless idea)   


Thanks in advance.",2,1
726,2019-5-12,2019,5,12,3,bnfgi9,Training lstm on irregular timed data,https://www.reddit.com/r/MachineLearning/comments/bnfgi9/training_lstm_on_irregular_timed_data/,Pedrhok,1557600251,[removed],0,1
727,2019-5-12,2019,5,12,3,bnfm26,[P] Scaling Transformer-XL to 128 GPUs,https://www.reddit.com/r/MachineLearning/comments/bnfm26/p_scaling_transformerxl_to_128_gpus/,ben_mann,1557601092,[removed],0,1
728,2019-5-12,2019,5,12,4,bnfusf,[P] Artificial Neural Networks and prediction (Scilab),https://www.reddit.com/r/MachineLearning/comments/bnfusf/p_artificial_neural_networks_and_prediction_scilab/,discl0se,1557602374,"I have ANN Toolbox for Scilab and some script which I do not understand completely. I studied that at least 8 years ago but don't remember much. What I need is help understanding the script and making few changes to it. The main thing I would like to see is correct chart for the computations. The code:

    clear
    clc
    
    // data
    X = [1.00 2.00 3.00
    2.00 3.00 4.00
    3.00 4.00 5.00
    4.00 5.00 6.00
    5.00 6.00 7.00
    6.00 7.00 8.00];
    
    // libraries
    exec(""ann_FF_init.sci"");
    exec(""ann_FF_Mom_online.sci"");
    exec(""ann_d_sum_of_sqr.sci"");
    exec(""ann_d_log_activ.sci"");
    exec(""ann_FF_run.sci"");
    exec(""ann_log_activ.sci"");
    
    // k - predicition range?
    k=2;
    
    // data rows and cols count
    [rows,cols]=size(X);
    
    // normalize the learning data - what it does? makes values in range from 0.00 to 1.00?
    for i=1:cols
    	X(:,i)=X(:,i)/sqrt(X(:,i)'*X(:,i));
    end;
    
    // learning series, pair &lt;U,Z&gt;, automatic conversion of X which is a column - what happens here?
    U=[];
    Z=[];
    for i=1:rows-k-1
    	U=[U X(i:i+k-1)];
    	Z=[Z X(i+k)];
    end;
    
    // NN structure
    [in_count, pattern_count]=size(X);
    [out_count, pattern_count]=size(Z);
    
    // neurons count in layers
    N=[in_count 10 7 out_count];
    r=[0,1];
    rb=[0,1];
    
    // initialize
    W=ann_FF_init(N,r,rb);
    
    // learning parameters lp=[0.1 0.05 0.5 0.1];
    lp=[0.1 0.05 0.5 0.1];
    
    // epochs count T=12000; 500 for testing
    epochs=500;
    
    // learning; X/x = training, Z/t = output, N = architecture, W = init weights, lp = learning rate, epochs/T = iterations
    [W,sW]=ann_FF_Mom_online(X,Z,N,W,lp,epochs);
    
    // full run
    Y=ann_FF_run(X,N,W);
    
    // show data - which are what?
    //Z'
    //disp(Y);
    //(Y-Z)'
    
    // plot data - how to plot input? how to plot prediction correctly?
    //plot(X);
    //plot(Y,""r"");
    //plot(Z);

The questions I have given in the code. Can anyone explain me the script?",0,0
729,2019-5-12,2019,5,12,5,bngcnq,[N] deeplearning.ai: Initializing neural networks (interactive article),https://www.reddit.com/r/MachineLearning/comments/bngcnq/n_deeplearningai_initializing_neural_networks/,cpury,1557605071,"I just came across this recent post by Andrew Ng's [deeplearning.ai](https://deeplearning.ai) team, talking about the importance of initialization. The interactive demos are fantastic! I haven't seen it shared anywhere, so thought I would do so here.

[http://www.deeplearning.ai/ai-notes/initialization/](http://www.deeplearning.ai/ai-notes/initialization/)",4,72
730,2019-5-12,2019,5,12,5,bngj3n,[R] NeurIPS 2019: The MineRL Competition for Sample-Efficient Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/bngj3n/r_neurips_2019_the_minerl_competition_for/,MadcowD,1557606055,[removed],17,210
731,2019-5-12,2019,5,12,5,bngpmt,Separating Input Layer into 2 Different Classes,https://www.reddit.com/r/MachineLearning/comments/bngpmt/separating_input_layer_into_2_different_classes/,totallytoffy,1557607048,[removed],0,1
732,2019-5-12,2019,5,12,5,bngshe,ML model deployment to Prod?,https://www.reddit.com/r/MachineLearning/comments/bngshe/ml_model_deployment_to_prod/,ohai123456789,1557607469,[removed],0,1
733,2019-5-12,2019,5,12,6,bnhbbn,Best way to store relational data form numerous sources?,https://www.reddit.com/r/MachineLearning/comments/bnhbbn/best_way_to_store_relational_data_form_numerous/,TenderBittle,1557610433,[removed],0,1
734,2019-5-12,2019,5,12,7,bnhza8,"Looking for a comprehensive list of machine learning methods, algorithms, techniques",https://www.reddit.com/r/MachineLearning/comments/bnhza8/looking_for_a_comprehensive_list_of_machine/,himanshuragtah1,1557614292,[removed],1,1
735,2019-5-12,2019,5,12,7,bni2jk,OpenL3 audio embeddings,https://www.reddit.com/r/MachineLearning/comments/bni2jk/openl3_audio_embeddings/,jonnor,1557614833,[removed],0,1
736,2019-5-12,2019,5,12,7,bni3ob,"Looking for a comprehensive list of machine learning methods, algorithms, techniques",https://www.reddit.com/r/MachineLearning/comments/bni3ob/looking_for_a_comprehensive_list_of_machine/,himanshuragtah1,1557615024,[removed],2,2
737,2019-5-12,2019,5,12,8,bni9bb,[D]Seeking advice as an undergraduate interested in learning machine learning!,https://www.reddit.com/r/MachineLearning/comments/bni9bb/dseeking_advice_as_an_undergraduate_interested_in/,Iwouldloveto6974,1557615935,"Hello. I am currently enrolled in CMU and very conversant in linear algebra, multivariable calculus, differential equations, and probability and stats. I was wondering if I should jump right into stanford's cs 229 machine learning course or go through the introduction to elements of statistical learning lecture first. I haven't learned R yet, but I do know a bit of python.

&amp;#x200B;

Which way would be the most effective? Any advice would be so thankful! :)",10,1
738,2019-5-12,2019,5,12,8,bnij07,Multiple Sclerosis: Discovery Of A Mechanism Responsible For Chronic Inflammation,https://www.reddit.com/r/MachineLearning/comments/bnij07/multiple_sclerosis_discovery_of_a_mechanism/,eslamalmahdy,1557617602,,0,1
739,2019-5-12,2019,5,12,8,bniq2w,Does an Autoencoder's network width/depth vary with the dimensionality of the latent variables of the input?,https://www.reddit.com/r/MachineLearning/comments/bniq2w/does_an_autoencoders_network_widthdepth_vary_with/,pretysmitty,1557618861,[removed],0,1
740,2019-5-12,2019,5,12,8,bniryu,Number detectors spontaneously emerge in a deep neural network designed for visual object recognition,https://www.reddit.com/r/MachineLearning/comments/bniryu/number_detectors_spontaneously_emerge_in_a_deep/,FortuitousAdroit,1557619192,,0,1
741,2019-5-12,2019,5,12,10,bnjfp6,Adversial Attacks on YouTube Content ID,https://www.reddit.com/r/MachineLearning/comments/bnjfp6/adversial_attacks_on_youtube_content_id/,ganesh_anand,1557623326,"Does anyone know if it's possible to do adversarial attacks on YouTube Content ID to make it not detect or detect something else. The most information about the system I could find was 10 years old. See [here]( https://stackoverflow.com/questions/649116/youtube-content-identification-technology). The system is probably very different now. 

Any information to help me get started would be awesome.",0,1
742,2019-5-12,2019,5,12,10,bnjiiy,Is ML a possible and good skill to get from scratch now?,https://www.reddit.com/r/MachineLearning/comments/bnjiiy/is_ml_a_possible_and_good_skill_to_get_from/,themaelstorm,1557623819,[removed],0,1
743,2019-5-12,2019,5,12,10,bnjjgd,[P] Adversarial Attacks on YouTube Content ID,https://www.reddit.com/r/MachineLearning/comments/bnjjgd/p_adversarial_attacks_on_youtube_content_id/,ganesh_anand,1557623984,"Does anyone know if it's possible to do adversarial attacks on YouTube Content ID to make it not detect or detect something else. The most information about the system I could find was 10 years old. See \[here\]( [https://stackoverflow.com/questions/649116/youtube-content-identification-technology](https://stackoverflow.com/questions/649116/youtube-content-identification-technology)). The system is probably very different now. 

&amp;#x200B;

Any information to help me get started would be awesome.",2,2
744,2019-5-12,2019,5,12,11,bnk661,Beginner looking for some guidance,https://www.reddit.com/r/MachineLearning/comments/bnk661/beginner_looking_for_some_guidance/,clustersss,1557628010,[removed],0,1
745,2019-5-12,2019,5,12,13,bnl39n,"Competition for ""CS for Good"" ideas with up to $20K prize",https://www.reddit.com/r/MachineLearning/comments/bnl39n/competition_for_cs_for_good_ideas_with_up_to_20k/,tkalil,1557634501,[removed],0,1
746,2019-5-12,2019,5,12,14,bnlrl0,"ryptocurrency rating, TOP-30 - cryptobit.media",https://www.reddit.com/r/MachineLearning/comments/bnlrl0/ryptocurrency_rating_top30_cryptobitmedia/,nichelleddtweed,1557639878,,0,1
747,2019-5-12,2019,5,12,15,bnm71i,[N] ELON MUSKS NEURALINK IMPLANT WILL CONNECT HUMAN BRAINS TO COMPUTERS,https://www.reddit.com/r/MachineLearning/comments/bnm71i/n_elon_musks_neuralink_implant_will_connect_human/,vadhavaniyafaijan,1557643729,,0,1
748,2019-5-12,2019,5,12,15,bnm8vc,ELI5 How to run GPT-2,https://www.reddit.com/r/MachineLearning/comments/bnm8vc/eli5_how_to_run_gpt2/,44gatti,1557644213,[removed],0,1
749,2019-5-12,2019,5,12,16,bnma97,Your Guide to NPL with MLSQL Stack (),https://www.reddit.com/r/MachineLearning/comments/bnma97/your_guide_to_npl_with_mlsql_stack_/,allwefantasy,1557644564,,0,1
750,2019-5-12,2019,5,12,16,bnmddo,[D] Which part of the RNN architecture has the sequential memory stored ?,https://www.reddit.com/r/MachineLearning/comments/bnmddo/d_which_part_of_the_rnn_architecture_has_the/,thehumanlobster,1557645357,"I was reading Andrej Karpathy's [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) on RNN to get familiarised with working of RNN, both mathematically and intuitively. From my understanding, there are three sets of parameters to optimise.

1. Wxh - multiple with new input to give a hidden state
2. Whh - multiply with rolling hidden state to add to it the above hidden state
3. Why - multiple with the rolling hidden state to obtain the output

And we have the rolling hidden state (H) which accumulates all the information from the inputs. And we optimise on the loss calculated from the output to find the best set of above params

What I am not able to visualise and understand is the part in which the so-called sequential memory is stores ? 

Is it stored in the vector H (the rolling hidden state) or the weight matrix Whh ?

In either case, could you also give some intuition on how it contains memory in the form of matrix / vector ?",7,14
751,2019-5-12,2019,5,12,16,bnmmd0,Any examples of mature open source products that use deep learning?,https://www.reddit.com/r/MachineLearning/comments/bnmmd0/any_examples_of_mature_open_source_products_that/,amival,1557647814,[removed],0,1
752,2019-5-12,2019,5,12,17,bnmr1c,Wavenet structure explanations,https://www.reddit.com/r/MachineLearning/comments/bnmr1c/wavenet_structure_explanations/,khawarizmy,1557649008,[removed],0,1
753,2019-5-12,2019,5,12,18,bnn3s6,I'm interested in using CV and machine learning to program a drone - any tips on getting started?,https://www.reddit.com/r/MachineLearning/comments/bnn3s6/im_interested_in_using_cv_and_machine_learning_to/,rnolan7,1557652435,I have been using python for data science for 7+ years. I recently began taking some courses on computer vision and machine learning and am very interested in taking on a project like programming a drone. I know very little about drones at this point. Any tips/recommendations on drone equipment (drone kits that work well with python)/tutorials/reading material would be much appreciated!,0,1
754,2019-5-12,2019,5,12,18,bnn7vs,GradType - Person Identification by Keyboard Typing,https://www.reddit.com/r/MachineLearning/comments/bnn7vs/gradtype_person_identification_by_keyboard_typing/,indutny,1557653531,,0,1
755,2019-5-12,2019,5,12,20,bnnvco,[P] Pong from pixels: policy gradients without gradients,https://www.reddit.com/r/MachineLearning/comments/bnnvco/p_pong_from_pixels_policy_gradients_without/,themathstudent,1557659468,,1,1
756,2019-5-12,2019,5,12,21,bno7r2,Interact and talk to other ML devs? - T3CH DISCORD,https://www.reddit.com/r/MachineLearning/comments/bno7r2/interact_and_talk_to_other_ml_devs_t3ch_discord/,ButtKickVille,1557662403,[removed],0,0
757,2019-5-12,2019,5,12,21,bno9wy,[P] Some clarification needed on controlling outputs of multiple models,https://www.reddit.com/r/MachineLearning/comments/bno9wy/p_some_clarification_needed_on_controlling/,trustedBatman,1557662857,"The 'mental' picture I have in my head is that I have 2 models, a RNN and a CNN. These models take data (sensory and Image) from a self driving car simulator (CARLA). The output of both the models will be a set of instructions to keep the car in the center lane (and later on braking, speedingup etc) 

My question is, what is the best approach to combine the outputs. so the RNN with output (lets assume) steer 40^o, the CNN with output steer 35^o. what approach should I take for finding out which is the best course to take. 

Or should I be looking at other models (R-CNNs) or possible another approach for model building. 

Cheers",2,9
758,2019-5-12,2019,5,12,21,bnoaq6,"Talk, collaborate, partner, participate with other devs in events and a lot more? - T3CH DISCORD",https://www.reddit.com/r/MachineLearning/comments/bnoaq6/talk_collaborate_partner_participate_with_other/,ButtKickVille,1557663039,,1,1
759,2019-5-12,2019,5,12,22,bnp26c,[D] Chances on getting into a ML PhD without engineering background,https://www.reddit.com/r/MachineLearning/comments/bnp26c/d_chances_on_getting_into_a_ml_phd_without/,oiseaumigrateur,1557668451,"Hi, I have a unusual background and I am looking at getting into a PhD in machine learning. Do you think it is possible to get into a ML PhD without an engineering background?
I studied business in my bachelor in Europe, and since it had a relatively decent amount of math I got into a master specialising in financial engineering. The master was particularly quantitative. I am now working as a trader, doing a mix of quant/non-quant trading. And some of it is ML-based.
For the last 2 years I really got into ML, taking classes on my free time (Learning from Data, reading Deep Learning book and other ML books, and now the David Silver RL class). And I found it so interesting that I want to spend more time on it and do research.
Given how competitive PhD applications are, do I stand any chances, since my background is not pure engineering? If yes, any advices on how to start?

Note: While I am interested in many applications, I would be interested to research RL in trading too and I guess my profile would fit nicely there. But unfortunately it is not a developed area of research so it is difficult to point myself in that direction during applications.",70,30
760,2019-5-12,2019,5,12,22,bnp27j,AI-Upscaling,https://www.reddit.com/r/MachineLearning/comments/bnp27j/aiupscaling/,Guakamouley,1557668457,[removed],0,1
761,2019-5-12,2019,5,12,23,bnpbt3,Use of machine learning in meteorology.,https://www.reddit.com/r/MachineLearning/comments/bnpbt3/use_of_machine_learning_in_meteorology/,Bigdey,1557670097,[removed],0,1
762,2019-5-12,2019,5,12,23,bnpcb1,I wonder your skills or tips that you use in regression/classification problems,https://www.reddit.com/r/MachineLearning/comments/bnpcb1/i_wonder_your_skills_or_tips_that_you_use_in/,mokmushroom,1557670178,[removed],0,1
763,2019-5-12,2019,5,12,23,bnpgbu,"J. Frankle &amp; M. Carbin - The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",https://www.reddit.com/r/MachineLearning/comments/bnpgbu/j_frankle_m_carbin_the_lottery_ticket_hypothesis/,Buck-Nasty,1557670849,,1,1
764,2019-5-12,2019,5,12,23,bnpgvw,[D] Use of machine learning in weather forecast,https://www.reddit.com/r/MachineLearning/comments/bnpgvw/d_use_of_machine_learning_in_weather_forecast/,Bigdey,1557670943," Hi,

Where I could find some good and correct material about the use of machine learning in weather forecast?

I would like some serious material to prove if this method can be, is or was used at some point, as an integral or support method.

I did a search on Google Scholar, but I have only found articles presenting scholar or personal projects, and I would not like to conclude that, from what I found, this method has not been officially used by any national meteorological agency.",14,1
765,2019-5-12,2019,5,12,23,bnpi0c,[Discussion] Any ideas to link reinforcement learning agent to use an android device as its environment?,https://www.reddit.com/r/MachineLearning/comments/bnpi0c/discussion_any_ideas_to_link_reinforcement/,codexblaze,1557671131,How can I link Machine Learning to an android device? I want to create a reinforcement learning agent that uses an android application as its environment. Any procedure on how it can be done?,2,0
766,2019-5-13,2019,5,13,0,bnq0wd,"What are the general approaches for clustering Mixed typed data (containing Numerical and Categorical types), other than encoding the categorical data and applying distance/density based clustering algorithms on them ?",https://www.reddit.com/r/MachineLearning/comments/bnq0wd/what_are_the_general_approaches_for_clustering/,gurusachin100,1557674214,[removed],0,1
767,2019-5-13,2019,5,13,0,bnq2h8,[P] GradType - Person Identification by Keyboard Typing,https://www.reddit.com/r/MachineLearning/comments/bnq2h8/p_gradtype_person_identification_by_keyboard/,indutny,1557674461,,0,1
768,2019-5-13,2019,5,13,0,bnq87x,So True!!,https://www.reddit.com/r/MachineLearning/comments/bnq87x/so_true/,kdas22,1557675321,[removed],0,1
769,2019-5-13,2019,5,13,0,bnqagi,[D] Gradient: Teaching AI To Plan Ahead,https://www.reddit.com/r/MachineLearning/comments/bnqagi/d_gradient_teaching_ai_to_plan_ahead/,[deleted],1557675660,,0,1
770,2019-5-13,2019,5,13,0,bnqb3e,[D] Teaching AI To Plan Ahead,https://www.reddit.com/r/MachineLearning/comments/bnqb3e/d_teaching_ai_to_plan_ahead/,hughbzhang,1557675758,[removed],0,1
771,2019-5-13,2019,5,13,0,bnqbji,[D] Teaching AI To Plan Ahead,https://www.reddit.com/r/MachineLearning/comments/bnqbji/d_teaching_ai_to_plan_ahead/,hughbzhang,1557675825,"Newest Gradient article on AI Planning by researchers at IBM!

[https://www.reddit.com/r/MachineLearning/](https://www.reddit.com/r/MachineLearning/)",2,11
772,2019-5-13,2019,5,13,1,bnqm0p,[D] GANs were invented in 2010?,https://www.reddit.com/r/MachineLearning/comments/bnqm0p/d_gans_were_invented_in_2010/,Former_Hippo,1557677338,"Link: [https://web.archive.org/web/20120312111546/http://yehar.com:80/blog/?p=167](https://web.archive.org/web/20120312111546/http://yehar.com:80/blog/?p=167)

&amp;#x200B;

2010-02-24

A **method for training artificial neural networks** to generate missing data within a variable context. As the idea is hard to put in a single sentence, I will use an example:

An image may have missing pixels (let's say, under a smudge). How can one restore the missing pixels, knowing only the surrounding pixels? One approach would be a ""generator"" neural network that, given the surrounding pixels as input, generates the missing pixels.

But how to train such a network? One can't expect the network to exactly produce the missing pixels. Imagine, for example, that the missing data is a patch of grass. One could teach the network with a bunch of images of lawns, with portions removed. The teacher knows the data that is missing, and could score the network according to the root mean square difference (RMSD) between the generated patch of grass and the original data. The problem is that if the generator encounters an image that is not part of the training set, it would be impossible for the neural network to put all the leaves, especially in the middle of the patch, in exactly the right places. The lowest RMSD error would probably be achieved by the network filling the middle area of the patch with a solid color that is the average of the color of pixels in typical images of grass. If the network tried to generate grass that looks convincing to a human and as such fulfills its purpose, there would be an unfortunate penalty by the RMSD metric.

My idea is this (see figure below): Train simultaneously with the generator a classifier network that is given, in random or alternating sequence, generated and original data. The classifier then has to guess, in the context of the surrounding image context, whether the input is original (1) or generated (0). The generator network is simultaneously trying to get a high score (1) from the classifier. The outcome, hopefully, is that both networks start out really simple, and progress towards generating and recognizing more and more advanced features, approaching and possibly defeating human's ability to discern between the generated data and the original. If multiple training samples are considered for each score, then RMSD is the correct error metric to use, as this will encourage the classifier network to output probabilities.",74,272
773,2019-5-13,2019,5,13,1,bnqmsq,AI-Upscaling (Topaz),https://www.reddit.com/r/MachineLearning/comments/bnqmsq/aiupscaling_topaz/,Guakamouley,1557677447,,0,1
774,2019-5-13,2019,5,13,3,bnrxk9,[D] Learning a transition function for skill representation from game outcomes among a group of agents playing 2-player zero sum games,https://www.reddit.com/r/MachineLearning/comments/bnrxk9/d_learning_a_transition_function_for_skill/,Vallvaka,1557684095,"This has been an idea I've been bouncing around in my head for the past year or so but I've been struggling to come up with a way to apply ML to it.

The idea is this. Imagine you had a group of agents who randomly match up and repeatedly play 2-player zero-sum game against one other. This game has a known structure, and each agent has a strategy for playing the game that has varied success depending on the opponent they face.

The goal is to represent each agent as a point in feature space such that these features maximize the ability to infer ""skill level"" for a player. Then given 2 agents' representations, the outcome of a matchup between those agents can be predicted.

However, the idea is that the features have to be inferred from interactions between other players, so after each matchup's outcome the features of the two agents are adjusted based on new information, and agents' ratings converge to their ""true"" rating as more games are played.

The idea is inspired by the Elo rating system used widely in chess and competitive gaming, because it's a special case of what I'm trying to do. Each player has an Elo rating that is a normally distributed measure of skill. After each seen game outcome, the winner gains points and the loser loses points, but the amount transferred depends on the disparity in score between players. The difference in Elo rating produces a predicted chance to win/lose for either player when a game is played between them.

In a more generalized case where you have an N-dimensional rating with more possible game outcomes, could you learn the optimal transition function that would need to be applied after each game that can maximize game outcome prediction accuracy?",2,1
775,2019-5-13,2019,5,13,3,bns5hq,De-noise dataset,https://www.reddit.com/r/MachineLearning/comments/bns5hq/denoise_dataset/,clone290595,1557685207,[removed],0,1
776,2019-5-13,2019,5,13,3,bns80c,EMNLP or CoNLL?,https://www.reddit.com/r/MachineLearning/comments/bns80c/emnlp_or_conll/,angulated_tortoise,1557685562,Is there a heuristic for which is better to submit a given paper? Is one more prestigious than the other?,0,1
777,2019-5-13,2019,5,13,3,bnsicv,Filter unusual requests in web logs,https://www.reddit.com/r/MachineLearning/comments/bnsicv/filter_unusual_requests_in_web_logs/,anderl1980,1557686952,[removed],0,1
778,2019-5-13,2019,5,13,4,bnsx8d,[D] Each output of my GANs is a hodgepodge of different classes  is this the same as mode collapse?,https://www.reddit.com/r/MachineLearning/comments/bnsx8d/d_each_output_of_my_gans_is_a_hodgepodge_of/,RamenTheory,1557688886,"Hello there,
I'm currently trying to train a GAN to produce Simpsons characters (0 for Homer, 1 for Lisa, 2  for Marge, etc...) but the output I am getting is a mixture of all the classes. For example, the output for the 1 label may be Lisa with Homer's mouth, or the output for the 0 label may be Homer with Marge's hair. Is this **mode collapse** or a different problem? Thanks!",8,5
779,2019-5-13,2019,5,13,4,bnt33k,[D] is there a good open source facial reenactment and voice cloning method?,https://www.reddit.com/r/MachineLearning/comments/bnt33k/d_is_there_a_good_open_source_facial_reenactment/,CalmAnalyst1,1557689650,[removed],0,1
780,2019-5-13,2019,5,13,5,bnu1ac,Quick thought: Are black-boxes going to become the standard in machine learning?,https://www.reddit.com/r/MachineLearning/comments/bnu1ac/quick_thought_are_blackboxes_going_to_become_the/,TheTesseractAcademy,1557694426,,0,1
781,2019-5-13,2019,5,13,6,bnu8f0,ICML 2019 hotel,https://www.reddit.com/r/MachineLearning/comments/bnu8f0/icml_2019_hotel/,Tumitran,1557695459,[removed],0,1
782,2019-5-13,2019,5,13,6,bnubqw,[R] Gap between the actual and theoretical neural net capacity?,https://www.reddit.com/r/MachineLearning/comments/bnubqw/r_gap_between_the_actual_and_theoretical_neural/,vernunftig,1557695956,"Intuitively, larger networks have higher capacity than smaller ones. However, the theoretical capacity of a huge network would never be reached in practice due to inefficient optimization procedure, limited dataset etc. So if we scale a network by 10 times, its actual capacity might only increase by eg. 5 times, and if we scale it by 100 times, the actual capacity could increase by only 20 times.

Is such a claim correct? Are there any papers that study the gap between the actual and theoretical network gap or relevant topic?",2,3
783,2019-5-13,2019,5,13,6,bnuo6y,InterpretML from Microsoft Research,https://www.reddit.com/r/MachineLearning/comments/bnuo6y/interpretml_from_microsoft_research/,caesarten,1557697861,,0,1
784,2019-5-13,2019,5,13,6,bnurc0,How bad is my algorithm?,https://www.reddit.com/r/MachineLearning/comments/bnurc0/how_bad_is_my_algorithm/,dead05,1557698343,[removed],0,1
785,2019-5-13,2019,5,13,7,bnus7s,"Need help with how to best show results of KNN, SVM and Random Forest algorithm on EMG signals dataset with 6 classes",https://www.reddit.com/r/MachineLearning/comments/bnus7s/need_help_with_how_to_best_show_results_of_knn/,dreamer_luci,1557698469,[removed],0,1
786,2019-5-13,2019,5,13,7,bnv4la,[D] Higher-order corrections to the SGD continuous-time limit,https://www.reddit.com/r/MachineLearning/comments/bnv4la/d_higherorder_corrections_to_the_sgd/,glockenspielcello,1557700342,"I've seen a lot of theoretical studies of SGD that consider it in the limit as the step size goes to zero, which turns it into a stochastic differential equation of the form dx/dt = alpha\*(-grad(loss)(x) + noise). It's a lot easier to compute a lot of useful quantities using this form like e.g. stationary distributions.

However, one of the things that gets lost in this formalism is are intrinsic scales of the problem. In the continuous limit, rescaling the time variable t (or more generally, performing an arbitrary coordinate transformation) leaves the trajectory invariant, because the differential equation is formulated in a covariant fashion. This gives misleading results if you want to analyze something like the convergence rates. In this continuous formulation, you can just rescale your time parameter to 'speed up' your dynamics (which is equivalent to increasing alpha), whereas you obviously can't do this in the discrete formulation, because if you rescale alpha arbitrarily you overshoot and you get bad convergence.

The first thing that came to mind when I started thinking about this was that you could amend your differential equation to include higher-order correction terms. Specifically, if we have a differential equation of the form x'(t) = f(x), we can Taylor expand to get x(t + delta)  x(t) + f(x)\*delta + 0.5\*Df(x)\*f(x)\*delta\^2 + O(delta\^3). This tells us that the difference between the continuous trajectory solution x(t + delta) and the discrete trajectory x(t) + f(x)\*delta after a time delta will be roughly 0.5\*Df(x)\*f(x)\*delta\^2. In order to get a more accurate model for the discrete-time process x(t+delta) = x(t) + f(x)\*delta, we can introduce a correction term into our differential equation: x'(t) = f(x) - 0.5\*Df(x)\*f(x)\*delta. When f is -alpha\*grad(loss), this becomes x'(t) = -alpha\*grad(loss)(x) + 0.5\*alpha\*Hessian(loss)(x)\*grad(loss)(x)\*delta. This correction term breaks covariance: when t is rescaled, both alpha and delta are necessarily rescaled, so the correction term transforms differently than the gradient term. It seems to me like this is a natural way to model the breakdown of covariance in the discrete dynamical system in the continuous setting and to study why certain timescales/learning rates are preferred.

tl;dr: Does know if this version of the continuous-time limit has been studied before? If so, can someone point me towards some references that I can read up on?",3,9
787,2019-5-13,2019,5,13,8,bnvgyl,GPT Model 345M Output,https://www.reddit.com/r/MachineLearning/comments/bnvgyl/gpt_model_345m_output/,og_mryamz,1557702294,"This ai prose is rich with allegory

Prompt

It was a quaint village, not too far from the southern bay. The smell of grass fumed -- like it always did early spring -- as the sound of the stream rushed along the riverside. A girl stood and wondered while taking in the cool crisp air. These were her final moments.

GPT Output (after 5 samples)

No sooner had she placed her hand on the nameplate on the plaque than a shaft of sunlight stabbed through the mist at her feet. She looked down at the cold marble inside and saw that underneath was her sister's diamond-studded slipcase. It was open for inspection. It was ice cold and dark and strangely, she called it ""Shiny."" She was so lonely at this point it hurt badly. The girl let out a short struggle. Snow drifts filled the sky, overlapping like sides of frost inside the ladder from which she slid down into the fissure of the mountain. At last, a firm hand fell on the hard smooth stone again. It's dull, granite surface gleamed out in the dark wind beneath, like a mouth of millions of teeth. ""You'll remember me for the court case we win today. Then the dust will pass."" She looked down at her carved stone and over at the child. ""They'll see right through it."" There was the crossroads, and no spiritual journey was made.When she finally got down, as far as she had ever gone on ice -- still heading down on the roads -- there was hardly a breath of wind. The little snow from the bottom of the mountain director faced directly up, overhead, as her face opened up into sincere smiling and the cold slab at her heel curled around her into an image of life. As she let out her first breath of air she clawed like a red-clawed beast at the chain that tightens at the base of her glassy skull and when she broke the tight clasp and looked up into the direction of the softly glowing sun, it felt like seeing snow landscape like a panel cast from crystalline stone. It wasn't snow on her foot -- for thousands of years, snow had fallen on virtually every other person away from religion -- and the snow was real after all, shadowed and much too complex to fit into a myth. But, and her certainest prayer in her fingers, the joy still lingered when she saw her moon face, minus a skull to juxtapose the lineaments of our mortal soul.",0,1
788,2019-5-13,2019,5,13,8,bnvjy6,[P] How do I embed sparse data before inputting into LSTM?,https://www.reddit.com/r/MachineLearning/comments/bnvjy6/p_how_do_i_embed_sparse_data_before_inputting/,somethingstrang,1557702783," I am trying to understand an LSTM similar to what's done in [a paper about embedding medical concepts in an embedding layer before inputting to an LSTM](https://arxiv.org/pdf/1902.02443.pdf).

A screenshot of the relevant figure is below:

&amp;#x200B;

[1837 X \(Demb-1\) embedding space. Afterwards a frequency vector is concatenated](https://i.redd.it/fv3c37ly3vx21.jpg)

 

The paper states:

1) There are 1837 features representing medical concepts. These concepts are textual medical codes such as ""CPT 9002""

2) These 1837 features are put into an embedding layer with an output of (1837 x Demb-1)

3) A frequency vector of those features is then concatenated with that output, making the total dimensions of the output (1837 x Demb)

&amp;#x200B;

My question is: medical data is very sparse and often times only a small fraction of the medical concepts will appear at a particular time step. For example, only 10 of the 1837 features will have data for one time step. So how do I go about creating this input for this embedding layer in practice?

&amp;#x200B;

Assuming I have 10 out of the 1837 features available for a timestep, would the input to the embedding look like:

1) A vector of length 10 representing the available data? If so, why would the paper say that the output is 1837xDemb-1?

2) A vector of length 1837 containing 1's and 0's indicating which features were available for this timestep? If so, why would you need to concatenate the frequencies to the output?

&amp;#x200B;

I am just super confused of how to create the input vector in practice and any information would be greatly helpful.",8,3
789,2019-5-13,2019,5,13,8,bnvklk,[D] GPT Model 345M Prose,https://www.reddit.com/r/MachineLearning/comments/bnvklk/d_gpt_model_345m_prose/,og_mryamz,1557702894,"This AI's contemporary prose is so rich in imagery, and allegory.
------------------------------------------------------------
My Randomly thought of Prompt:
------------------------------------------------------------
It was a quaint village, not too far from the southern bay. The smell of grass fumed -- like it always did early spring -- as the sound of the stream rushed along the riverside. A girl stood and wondered while taking in the cool crisp air. These were her final moments.
------------------------------------------------------------
GPT Model 345M Output:
------------------------------------------------------------
No sooner had she placed her hand on the nameplate on the plaque than a shaft of sunlight stabbed through the mist at her feet. She looked down at the cold marble inside and saw that underneath was her sister's diamond-studded slipcase. It was open for inspection. It was ice cold and dark and strangely, she called it ""Shiny."" She was so lonely at this point it hurt badly. The girl let out a short struggle. Snow drifts filled the sky, overlapping like sides of frost inside the ladder from which she slid down into the fissure of the mountain. At last, a firm hand fell on the hard smooth stone again. It's dull, granite surface gleamed out in the dark wind beneath, like a mouth of millions of teeth. ""You'll remember me for the court case we win today. Then the dust will pass."" She looked down at her carved stone and over at the child. ""They'll see right through it."" There was the crossroads, and no spiritual journey was made.When she finally got down, as far as she had ever gone on ice -- still heading down on the roads -- there was hardly a breath of wind. The little snow from the bottom of the mountain director faced directly up, overhead, as her face opened up into sincere smiling and the cold slab at her heel curled around her into an image of life. As she let out her first breath of air she clawed like a red-clawed beast at the chain that tightens at the base of her glassy skull and when she broke the tight clasp and looked up into the direction of the softly glowing sun, it felt like seeing snow landscape like a panel cast from crystalline stone. It wasn't snow on her foot -- for thousands of years, snow had fallen on virtually every other person away from religion -- and the snow was real after all, shadowed and much too complex to fit into a myth. But, and her certainest prayer in her fingers, the joy still lingered when she saw her moon face, minus a skull to juxtapose the lineaments of our mortal soul.",8,5
790,2019-5-13,2019,5,13,8,bnvm9a,[R] Flappy Hummingbird: An Open Source Dynamic Simulation of Flapping Wing Robots and Animals (ICRA 2019),https://www.reddit.com/r/MachineLearning/comments/bnvm9a/r_flappy_hummingbird_an_open_source_dynamic/,baylearn,1557703165,"**Flappy Hummingbird: An Open Source Dynamic Simulation of Flapping Wing Robots and Animals**

Abstract: Insects and hummingbirds exhibit extraordinary flight capabilities and can simultaneously master seemingly conflicting goals: stable hovering and aggressive maneuvering, unmatched by small scale man-made vehicles. Flapping Wing Micro Air Vehicles (FWMAVs) hold great promise for closing this performance gap. However, design and control of such systems remain challenging due to various constraints. Here, we present an open source high fidelity dynamic simulation for FWMAVs to serve as a testbed for the design, optimization and flight control of FWMAVs. For simulation validation, we recreated the hummingbird-scale robot developed in our lab in the simulation. System identification was performed to obtain the model parameters. The force generation, open-loop and closed-loop dynamic response between simulated and experimental flights were compared and validated. The unsteady aerodynamics and the highly nonlinear flight dynamics present challenging control problems for conventional and learning control algorithms such as Reinforcement Learning. The interface of the simulation is fully compatible with OpenAI Gym environment. As a benchmark study, we present a linear controller for hovering stabilization and a Deep Reinforcement Learning control policy for goal-directed maneuvering. Finally, we demonstrate direct simulation-to-real transfer of both control policies onto the physical robot, further demonstrating the fidelity of the simulation.

Page with more information and video demonstrations: https://www.purdue.edu/newsroom/releases/2019/Q2/hummingbird-robot-uses-ai-to-soon-go-where-drones-cant.html

Paper: https://arxiv.org/abs/1902.09628

Code for ICRA 2019: https://github.com/purdue-biorobotics/flappy",0,17
791,2019-5-13,2019,5,13,9,bnw3gh,Can the machines learn sarcasm yet?,https://www.reddit.com/r/MachineLearning/comments/bnw3gh/can_the_machines_learn_sarcasm_yet/,vvviren,1557706103,[removed],0,1
792,2019-5-13,2019,5,13,9,bnwdev,"[N] School of AI is offering fellowships to 10 teams to pursue a research topic of their choice. Selected scholas will get $10,000 in Google cloud credits, and an advisor. Also, our team is looking for 1-2 people to join us in something natural language processing related, if that's your background",https://www.reddit.com/r/MachineLearning/comments/bnwdev/n_school_of_ai_is_offering_fellowships_to_10/,Research2Vec,1557707769,,1,1
793,2019-5-13,2019,5,13,11,bnxkoz,SpecAugment experiments using tensor2tensor,https://www.reddit.com/r/MachineLearning/comments/bnxkoz/specaugment_experiments_using_tensor2tensor/,longinglove,1557714821,[removed],0,1
794,2019-5-13,2019,5,13,12,bny0cf,[R] Connecting R To A Redshift Data Warehouse For Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bny0cf/r_connecting_r_to_a_redshift_data_warehouse_for/,thumbsdrivesmecrazy,1557716910,"In the following tutorial, author collected data into R via an Amazon Redshift cluster in Panoply data warehouse, visualized it, created predictive models, and tested their accuracy: [Connecting R To A Redshift Data Warehouse For Machine Learning](https://blog.panoply.io/connecting-r-to-a-redshift-data-warehouse-for-machine-learning)",0,1
795,2019-5-13,2019,5,13,13,bnynyw,[P] Simple Tensorflow implementation of SphereGAN (CVPR 2019 Oral),https://www.reddit.com/r/MachineLearning/comments/bnynyw/p_simple_tensorflow_implementation_of_spheregan/,taki0112,1557720580,,2,1
796,2019-5-13,2019,5,13,13,bnypj2,[P] Simple Tensorflow implementation of SphereGAN (CVPR 2019 Oral),https://www.reddit.com/r/MachineLearning/comments/bnypj2/p_simple_tensorflow_implementation_of_spheregan/,taki0112,1557720840,"&amp;#x200B;

![img](8gfuiuh6mwx21 ""Architecture"")

![img](9h0tq2i6mwx21 ""Algorithm"")

![img](jj6zv6i6mwx21 ""Result"")",2,13
797,2019-5-13,2019,5,13,13,bnyuww,[P] Atlas: Interactive visualization of optimization loss surfaces,https://www.reddit.com/r/MachineLearning/comments/bnyuww/p_atlas_interactive_visualization_of_optimization/,nanogru,1557721761,,0,1
798,2019-5-13,2019,5,13,13,bnyw65,[P] SpecAugment experiments using tensor2tensor,https://www.reddit.com/r/MachineLearning/comments/bnyw65/p_specaugment_experiments_using_tensor2tensor/,longinglove,1557721980,"SpecAugment experiments using tensor2tensor

&amp;#x200B;

[https://github.com/Kyubyong/specAugment](https://github.com/Kyubyong/specAugment)

&amp;#x200B;

This is an implementation of [SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779).

**Notes**

* The paper introduces three techniques for augmenting speech data in speech recognition.
* They come from the observation that spectrograms which often used as input can be treated as images, so various image augmentation methods can be applied.
* I find the idea interesting.
* It covers three methods: time warping, frequency masking, and time masking.
* Details are clearly explained in the paper.
* While the first one, time warping, looks salient apparently, Daniel, the first author, told me that indeed the other two are much more important than time warping, so it can be ignored if necessary. (Thanks for the advice, Daniel!)
* I found that implementing time warping with TensorFlow is tricky because the relevant functions are based on the static shape of the melspectrogram tensor, which is hard to get from the pre-defined graph.
* I test frequency / time masking on Tensor2tensor's LibriSpeech Clean Small Task.
* The paper used the LAS model, but I stick to Transformer.
* To compare the effect of specAugment, I also run a base model without augmentation.",3,5
799,2019-5-13,2019,5,13,13,bnyyi6,[P] Landing a rocket - my first Unity ML Agents project.,https://www.reddit.com/r/MachineLearning/comments/bnyyi6/p_landing_a_rocket_my_first_unity_ml_agents/,Roboserg,1557722379,"Video - [https://youtu.be/ynMYVdb7mO8](https://youtu.be/ynMYVdb7mO8)

It was always my little dream to do exactly this project. I know people have done it before, but being a beginner in both Unity and Reinforcement Learning I thought I share the results of my little dream :P

&amp;#x200B;

The agent taught itself how to land, no human taught it or shown how to do it. Trained for 500k steps (ca. 1h), could have ended sooner - [image](https://puu.sh/DrBnF/110c39f4a7.png)

&amp;#x200B;

Maximum reward/score is 20, its almost impossible to reach. It would mean 0 landing speed + 0 horizontal offset from the target. Needless to say, humans have no chance to land like that :P

Reward function - [click me](https://puu.sh/DrBt2/8cce720f40.png)

&amp;#x200B;

Will retrain with different parameters. Right now  batch\_size: 10 and  buffer\_size: 100, rest default.

&amp;#x200B;

Todo:

\- Add fuel consumption

\- Lessen the thrust of sideways engines

\- Simulate something more similar to a SpaceX landing, where a rocket falls for a period of time before landing

\- Add human controlled rocket for competitive play

\- Host the game on the web

\- ???

&amp;#x200B;

ps. Elon Musk, hire me! :D",3,1
800,2019-5-13,2019,5,13,15,bnzodw,Live Annotation,https://www.reddit.com/r/MachineLearning/comments/bnzodw/live_annotation/,trainingdata,1557727206,,0,1
801,2019-5-13,2019,5,13,15,bo02is,Which part of the code performs the job of fetching Phishing urls online?,https://www.reddit.com/r/MachineLearning/comments/bo02is/which_part_of_the_code_performs_the_job_of/,zerocool_07,1557730162,[removed],0,1
802,2019-5-13,2019,5,13,16,bo0ghb,[R] Decision/classification/regression tree research papers from the last 30 years,https://www.reddit.com/r/MachineLearning/comments/bo0ghb/r_decisionclassificationregression_tree_research/,benitorosenberg,1557733088," [https://github.com/benedekrozemberczki/awesome-decision-tree-papers](https://github.com/benedekrozemberczki/awesome-decision-tree-papers)

A  curated list of decision, classification  and regression tree research  papers with implementations from the  following conferences.

Machine learning:

1. NeurIPS
2. ICML
3. ICLR

Computer vision:

1. CVPR
2. ICCV
3. ECCV

Natural language processing:

1. ACL
2. NAACL
3. EMNLP

Data Mining:

1. KDD
2. ICDM
3. CIKM
4. WWW

Artificial intelligence:

1. AAAI
2. IJCAI
3. UAI
4. AISTATS",13,162
803,2019-5-13,2019,5,13,16,bo0i10,Using the What-If Tool to investigate Machine Learning models,https://www.reddit.com/r/MachineLearning/comments/bo0i10/using_the_whatif_tool_to_investigate_machine/,NicholasTower,1557733428,[removed],0,1
804,2019-5-13,2019,5,13,17,bo0nxh,[D] what are you using? tensorflow vs pytorch,https://www.reddit.com/r/MachineLearning/comments/bo0nxh/d_what_are_you_using_tensorflow_vs_pytorch/,mesmer_adama,1557734752,"Just want to take the temperature of the community at this point in time. What are you using for research and production right now, tensorflow or pytorch? What is your experiences with speed of development for writing papers, code quality and eventual release to a production environment?

And if you want to elaborate what more do you use in conjunction, git for versioning, tensorboard for monitoring and docker for environment or something completely different?

Personally I've been using pytorch for smaller experiments and my local environment but will start doing larger experiments and want to learn from you all.",96,80
805,2019-5-13,2019,5,13,17,bo0t1f,"G-7 Science Academies Release Statements on Science and Trust, Artificial Intelligence, Citizen Science",https://www.reddit.com/r/MachineLearning/comments/bo0t1f/g7_science_academies_release_statements_on/,aiforworld2,1557735950,,0,1
806,2019-5-13,2019,5,13,17,bo0ve2,why using max_pooling as subsampling layer?,https://www.reddit.com/r/MachineLearning/comments/bo0ve2/why_using_max_pooling_as_subsampling_layer/,dannybbbb,1557736490,[removed],0,1
807,2019-5-13,2019,5,13,17,bo0wb3,[1905.04226] Language Modeling with Deep Transformers,https://www.reddit.com/r/MachineLearning/comments/bo0wb3/190504226_language_modeling_with_deep_transformers/,albertzeyer,1557736710,,1,8
808,2019-5-13,2019,5,13,18,bo1bt7,Predictive Analytics Use Case: Customer Churn Analysis!,https://www.reddit.com/r/MachineLearning/comments/bo1bt7/predictive_analytics_use_case_customer_churn/,ElegantMicroWebIndia,1557740134,,0,1
809,2019-5-13,2019,5,13,18,bo1dhh,Take a not free online course for further training opportunities or not?,https://www.reddit.com/r/MachineLearning/comments/bo1dhh/take_a_not_free_online_course_for_further/,Electricvid,1557740481,[removed],0,1
810,2019-5-13,2019,5,13,18,bo1dr4,Adversarial Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/bo1dr4/adversarial_reinforcement_learning/,ZeroMaxinumXZ,1557740538,,0,1
811,2019-5-13,2019,5,13,19,bo1iny,[R] Research paper on Data Integration,https://www.reddit.com/r/MachineLearning/comments/bo1iny/r_research_paper_on_data_integration/,gourxb,1557741606,"I have been working on data integration for the biological domain.  Is there any research paper published on integration? 

Can this be published in a journal or conference? Can you suggest some of them?",0,0
812,2019-5-13,2019,5,13,19,bo1jc2,[Discussion] Poll on useful algos Python packages are lacking in,https://www.reddit.com/r/MachineLearning/comments/bo1jc2/discussion_poll_on_useful_algos_python_packages/,danielhanchen,1557741726,"Hey Redditors!

I maintained HyperLearn ( [https://github.com/danielhanchen/hyperlearn](https://github.com/danielhanchen/hyperlearn) Faster ML algos on Python ), but from Feb - May I was a bit busy with uni + work. But since stuff has calmed down, I was just asking everyone here on what fast and useful ML algos people would like to see, where maybe other packages lack in.

My main aims for HyperLearn are:

1. Delete all C/C++ dependencies, and rely solely on Numba + Scipy LAPACK.
2. Focus on specific algos that are extremely relevant.

&amp;#x200B;

And on algos, I wanted to focus on:

&amp;#x200B;

a. Randomized + Dense PCA on Sparse Matrices (without converting sparse to dense). decomposition.PCA \[people tend to use pure SVD, but it ""can"" have different results than without removing mean""\]

b. Porting Eigendecomposition via MRRR for the top K eigenvectors. \[Scipy still hasn't..\] decomposition.EIGH

c. Fixing all memory copies for SVD, and support Randomized SVD on sparse matrices. decomposition.SVD

d. Fix up LinearSolve and place it into 1 module with LSMR (super fast sparse solve), combine with Cholesky, SVD, Eig, etc solving + Ridge. linear\_model.solve

e. And finally, introduce a Python only modified version of Spotify's ANNOY library (a limited nearest neighbor KD-Tree based on other heuristics I found to be useful).

&amp;#x200B;

I'm just ""guessing"" the top 5 seem useful, as I myself have had many issues / struggles with other package algos. I'm aiming to make the final package easily installable with only Scipy + Numba as it's prereqs (no more C/C++).

&amp;#x200B;

If anyone else has opinions on what algos people want to see, but current packages lack in, please do! \[Notice my field of knowledge is also limited....\] If you want to help, PLEASEEE MSG me!!! I wantttt help!

&amp;#x200B;

Finally, check out NVIDIA's cuML  [https://github.com/rapidsai/cuml](https://github.com/rapidsai/cuml) ! I'm part of their team making GPU algos super fast! For eg - UMAP runs in 2 minutes or so vs 15 minutes for Fashion MNIST.

&amp;#x200B;

Thanks!!! :)",19,11
813,2019-5-13,2019,5,13,19,bo1jex,IBM EDX Get Dream Job,https://www.reddit.com/r/MachineLearning/comments/bo1jex/ibm_edx_get_dream_job/,skilluponline,1557741742,"&amp;#x200B;

![video](bsyxsc2amxx21)

Every industry and our very existence is being transformed by artificial intelligence. This means lots of great jobs now. Especially for people with skills and. Take the five in-depth questions left by three instructors who are experts in their field. And earn the [IBM Deep Learning professional certificate](https://www.edx.org/professional-certificate/ibm-deep-learning?utm_source=paid-social&amp;utm_medium=partner-marketing&amp;utm_campaign=IBM&amp;utm_term=deep-learning) to get your dream job.",0,1
814,2019-5-13,2019,5,13,19,bo1vub,How is GPT-2 able to get factual information like this right?,https://www.reddit.com/r/MachineLearning/comments/bo1vub/how_is_gpt2_able_to_get_factual_information_like/,mrwazsx,1557744313,,0,1
815,2019-5-13,2019,5,13,19,bo1yqw,[D] How is GPT-2 Able to get factual information like this correct?,https://www.reddit.com/r/MachineLearning/comments/bo1yqw/d_how_is_gpt2_able_to_get_factual_information/,mrwazsx,1557744915,,0,1
816,2019-5-13,2019,5,13,20,bo2bga,[News] Best Artificial Intelligence Books in 2019,https://www.reddit.com/r/MachineLearning/comments/bo2bga/news_best_artificial_intelligence_books_in_2019/,woahdotcom,1557747245,,0,0
817,2019-5-13,2019,5,13,20,bo2fpz,[N] Best Artificial Intelligence Books in 2019,https://www.reddit.com/r/MachineLearning/comments/bo2fpz/n_best_artificial_intelligence_books_in_2019/,woahdotcom,1557747993,,0,0
818,2019-5-13,2019,5,13,21,bo2r4x,Language Models With Transformers,https://www.reddit.com/r/MachineLearning/comments/bo2r4x/language_models_with_transformers/,no_bear_so_low,1557749964,,6,7
819,2019-5-13,2019,5,13,21,bo2ue2,Predictive Analytics Use Case: Fraud Mitigation!,https://www.reddit.com/r/MachineLearning/comments/bo2ue2/predictive_analytics_use_case_fraud_mitigation/,ElegantMicroWebIndia,1557750533,,0,1
820,2019-5-13,2019,5,13,22,bo3f7n,Feedback wanted for new type of neural network layer: self-assessment layer,https://www.reddit.com/r/MachineLearning/comments/bo3f7n/feedback_wanted_for_new_type_of_neural_network/,FlorianDietz,1557753832,[removed],0,1
821,2019-5-13,2019,5,13,22,bo3k2r,Dynamic density estimation: looking for literature,https://www.reddit.com/r/MachineLearning/comments/bo3k2r/dynamic_density_estimation_looking_for_literature/,kaivalya,1557754542,[removed],0,1
822,2019-5-13,2019,5,13,22,bo3ogb,[D] Dynamic density estimation: looking for literature,https://www.reddit.com/r/MachineLearning/comments/bo3ogb/d_dynamic_density_estimation_looking_for/,kaivalya,1557755182,"Hi,

&amp;#x200B;

Suppose we want to estimate the distribution of entities in one dimension x and how the distribution evolves over time. For the sake of example the dimension can represent the spatial position of the entities, e.g. in east-west direction. Unfortunately the entities do not have identifiers, so we can not tell if two measurements x\_1 and x\_2 belong to the same entity or not if they arrive at different times.

&amp;#x200B;

The observability of the entities is limited which means that current x positions are not known at all times. Measurements of x arrive with varying temporal density. Thus, binning by time would lead to some time bins having few, others having a lot of data points.

&amp;#x200B;

If we assume that the total number fof entities is constant, how could we estimate the distribution over x, given t, in such a way that the temporal density is accounted for? I'd imagine a system where measurements in times of low density have a longer time-to-live, i.e. contribute to the density estimation for longer times compared to times when a lot of measurements arrive.

&amp;#x200B;

Is this a stochastic filtering problem? I'd be grateful for any hints what papers or textbook chapters to read to understand this issue better. My experience in the field is mostly limited to classification and regression with only limited exposure to density estimation and unsupervised learning. So, apologies if the question is trivial, I currently just don't know where to start reading.

&amp;#x200B;

Thanks!",1,1
823,2019-5-13,2019,5,13,23,bo3y1r,What is best data storage solution for 15 TB of tarred csv?,https://www.reddit.com/r/MachineLearning/comments/bo3y1r/what_is_best_data_storage_solution_for_15_tb_of/,johndatavizwiz,1557756553,[removed],0,1
824,2019-5-13,2019,5,13,23,bo4b1n,Using chest x-rays and deep learning to predict health conditions,https://www.reddit.com/r/MachineLearning/comments/bo4b1n/using_chest_xrays_and_deep_learning_to_predict/,Mayalittlepony,1557758393,[removed],0,1
825,2019-5-13,2019,5,13,23,bo4glg,GANs &amp; applied ML @ ICLR 2019,https://www.reddit.com/r/MachineLearning/comments/bo4glg/gans_applied_ml_iclr_2019/,generatedLeaf,1557759185,[removed],0,1
826,2019-5-14,2019,5,14,0,bo4orw,[D] Is the new Snapchat gender filter GAN-based?,https://www.reddit.com/r/MachineLearning/comments/bo4orw/d_is_the_new_snapchat_gender_filter_ganbased/,aDutchofMuch,1557760243,"I originally figured it was just some landmark-based filtering/graphics tricks like most of the other filters, but looking at people's selfies a bit closer, there seem to be a lot of GAN-like generative artifacts (especially in the hair) that I feel wouldn't be there if it is just a graphics effect:

https://twitter.com/Midbeast/status/1126769681312862208

If it is a GAN, I have no idea how they would do it in real time, though. Thoughts?",12,15
827,2019-5-14,2019,5,14,0,bo4tqj,DeepMind &amp; Google Graph Matching Network Outperforms GNN,https://www.reddit.com/r/MachineLearning/comments/bo4tqj/deepmind_google_graph_matching_network/,Yuqing7,1557760889,,0,1
828,2019-5-14,2019,5,14,0,bo4uai,[D] Why does deep reinforcement learning not generalize?,https://www.reddit.com/r/MachineLearning/comments/bo4uai/d_why_does_deep_reinforcement_learning_not/,FirstTimeResearcher,1557760959,"The answer seems to be no:

""Assessing Generalization in Deep Reinforcement Learning""
https://bair.berkeley.edu/blog/2019/03/18/rl-generalization/

&gt; We present a benchmark for studying generalization in deep reinforcement learning (RL). Systematic empirical evaluation shows that vanilla deep RL algorithms generalize better than specialized deep RL algorithms designed specifically for generalization. In other words, simply training on varied environments is so far the most effective strategy for generalization.


""Quantifying Generalization in Reinforcement Learning""
https://openai.com/blog/quantifying-generalization-in-reinforcement-learning/

&gt; Generalizing between tasks remains difficult for state of the art deep reinforcement learning (RL) algorithms. Although trained agents can solve complex tasks, they struggle to transfer their experience to new environments. Even though people know that RL agents tend to overfit  that is, to latch onto the specifics of their environment rather than learn generalizable skills  RL agents are still benchmarked by evaluating on the environments they trained on. This would be like testing on your training set in supervised learning!

Why is this issue specific to deep RL? Is it just simply the evaluation metrics the field has been using (training on the test set)?",25,65
829,2019-5-14,2019,5,14,0,bo4uns,How Endtest Is Using Machine Learning To Make Your Tests More Stable,https://www.reddit.com/r/MachineLearning/comments/bo4uns/how_endtest_is_using_machine_learning_to_make/,boss_scarbos,1557761008,,0,1
830,2019-5-14,2019,5,14,1,bo5ki1,[R] Unified Language Model Pre-training for Natural Language Understanding and Generation (MSR's GPT/BERT Competitor),https://www.reddit.com/r/MachineLearning/comments/bo5ki1/r_unified_language_model_pretraining_for_natural/,mrpogiface,1557764345,,6,12
831,2019-5-14,2019,5,14,1,bo5l0l,"[D] Chris Lattner: Compilers, Swift, TPU, and ML Accelerators | Artificial Intelligence Podcast",https://www.reddit.com/r/MachineLearning/comments/bo5l0l/d_chris_lattner_compilers_swift_tpu_and_ml/,UltraMarathonMan,1557764415,"Chris Lattner is a senior director at Google working on several projects including CPU, GPU, TPU accelerators for TensorFlow, Swift for TensorFlow, and all kinds of machine learning compiler magic going on behind the scenes. He is one of the top experts in the world on compiler technologies, which means he deeply understands the intricacies of how hardware and software come together to create efficient code. He created the LLVM compiler infrastructure project and the CLang compiler. He led major engineering efforts at Apple, including the creation of the Swift programming language. He also briefly spent time at Tesla as VP of Autopilot Software during the transition from Autopilot hardware 1 to hardware 2, when Tesla essentially started from scratch to build an in-house software infrastructure for Autopilot. 

**Video:** https://www.youtube.com/watch?v=yCd3CzGSte8 

https://i.redd.it/avi1bwpo70y21.png

**Outline:**

0:00 - Introduction

1:30 - First program, BASIC, Pascal, C

4:20 - Compilers, LLVM, CLang

37:30 - Apple - LLVM, Objective-C, Swift

45:30 - Google - Swift, Swift for TensorFlow, compilers, Colab

57:32 - TPU &amp; TensorFlow, hardware/software co-design

1:00:30 - MLIR (Multi-Level Intermediate Representation) framework

1:02:40 - Open sourcing of TensorFlow

1:05:10 - Tesla - transition from HW1 to HW2

1:07:24 - Elon Musk and time at Tesla

1:08:45 - Working hard

1:10:40 - Dragons",20,220
832,2019-5-14,2019,5,14,1,bo5qh6,"I prefer Random Forest over Neural Network on tabular data, here is why",https://www.reddit.com/r/MachineLearning/comments/bo5qh6/i_prefer_random_forest_over_neural_network_on/,pp314159,1557765139,,0,1
833,2019-5-14,2019,5,14,2,bo698v,[D] A 2019 guide to 3D Human Pose Estimation,https://www.reddit.com/r/MachineLearning/comments/bo698v/d_a_2019_guide_to_3d_human_pose_estimation/,cbsudux,1557767563,"Human Pose estimation is an important problem that has enjoyed the attention of the Computer Vision community for the past few decades. It is an important step towards understanding people in images and videos. This post covers the basics of 3D Human Pose Estimation and reviews the literature on the topic.

&amp;#x200B;

Article Link: [https://blog.nanonets.com/human-pose-estimation-3d-guide/](https://blog.nanonets.com/human-pose-estimation-3d-guide/)",4,17
834,2019-5-14,2019,5,14,2,bo6b5e,[D] Help me get a birthday present for my friend who really likes machine learning!,https://www.reddit.com/r/MachineLearning/comments/bo6b5e/d_help_me_get_a_birthday_present_for_my_friend/,Rapture-X,1557767809,"As the title says, I have a friend who really enjoys machine learning. He's the type to nerd out over anything that involves complex math or programming, so he really likes machine learning. 

I was thinking of getting him a book or maybe access to a course or something machine learning related. We've taken an introductory machine learning class, so he has a basic idea of the different models out there and how to use them. I'm looking for something a little bit more advanced that will maybe wrinkle his brain a little but will also be really cool to go through. He's also thinking of going to grad school soon to study machine learning. 

Any suggestions?",9,4
835,2019-5-14,2019,5,14,2,bo6d6v,[P] This eye does not exist - Using GANs to separate an unlabeled image dataset into clusters,https://www.reddit.com/r/MachineLearning/comments/bo6d6v/p_this_eye_does_not_exist_using_gans_to_separate/,abriosi,1557768067,,0,1
836,2019-5-14,2019,5,14,2,bo6hro,"Should I use a VAE, a GAN or something completely different?",https://www.reddit.com/r/MachineLearning/comments/bo6hro/should_i_use_a_vae_a_gan_or_something_completely/,GreenScreenSocks,1557768676,[removed],0,1
837,2019-5-14,2019,5,14,2,bo6kgt,[D] machine learning css code,https://www.reddit.com/r/MachineLearning/comments/bo6kgt/d_machine_learning_css_code/,elad12390,1557769025,Im searching for a finished model/program to generate css from html files really interested if it exists already :) (if it doesnt imma try to make it myself hopefully its not as hard as i think lol),5,1
838,2019-5-14,2019,5,14,2,bo6p5l,"Data Science with Python, Machine Learning and Statistics",https://www.reddit.com/r/MachineLearning/comments/bo6p5l/data_science_with_python_machine_learning_and/,aditidigital,1557769655,,2,1
839,2019-5-14,2019,5,14,3,bo73fp,TextVQA Challenge: Close the large gap between human accuracy and state-of-the-art,https://www.reddit.com/r/MachineLearning/comments/bo73fp/textvqa_challenge_close_the_large_gap_between/,apsdehal,1557771530,[removed],0,1
840,2019-5-14,2019,5,14,3,bo7923,[R] TextVQA Challenge: Close the large gap between human accuracy and state-of-the-art.,https://www.reddit.com/r/MachineLearning/comments/bo7923/r_textvqa_challenge_close_the_large_gap_between/,apsdehal,1557772293,"**Dataset Website:** [https://textvqa.org](https://textvqa.org)

**Challenge Link:** [https://evalai.cloudcv.org/web/challenges/challenge-page/244/overview](https://evalai.cloudcv.org/web/challenges/challenge-page/244/overview)

**Prize:** $10k GCP Credits

**Starter Code:** [https://github.com/facebookresearch/pythia](https://github.com/facebookresearch/pythia)

**Paper:** [https://arxiv.org/abs/1904.08920](https://arxiv.org/abs/1904.08920)

**Deadline:** 18th May (ask for extension if needed)

**More details on the challenge:** [https://textvqa.org/challenge](https://textvqa.org/challenge)

**Explore the dataset:** [https://textvqa.org/explore](https://textvqa.org/explore)

&amp;#x200B;

**Detailed Description:**Current state-of-the-art VQA models are unable to read and reason about text in images which in contrast is most asked by the users of such systems. TextVQA aims to provide a benchmark for measuring progress of VQA models on text reading and reasoning capabilities.

State-of-the-art VQA models on TextVQA are only around 14% while the human accuracy is \~85%. LoRRA module introduced in [TextVQA](https://arxiv.org/abs/1904.08920) paper can be attached to any VQA model to add text reading and reasoning capabilities. The current state-of-the-art on TextVQA is \~27% with LoRRA.

&amp;#x200B;

Use the [starter code](https://github.com/facebookresearch/pythia) to participate in challenge to win $10k GCP credits and help close this large gap.",2,8
841,2019-5-14,2019,5,14,3,bo7ctl,[D] Exponential Family PCA,https://www.reddit.com/r/MachineLearning/comments/bo7ctl/d_exponential_family_pca/,tpapp157,1557772788,"I was looking for a python implementation of general exponential family PCA (binomial in particular) but I couldn't find anything. Maybe I was using the wrong search terms. Sklearn has a bunch of PCA variants but none seem to be what I'm looking for.

Does anyone know of an implementation?

Thanks.",5,4
842,2019-5-14,2019,5,14,3,bo7lg9,"Data Science Training with Python, Machine Learning and Statistics",https://www.reddit.com/r/MachineLearning/comments/bo7lg9/data_science_training_with_python_machine/,aditidigital,1557773919,[removed],0,1
843,2019-5-14,2019,5,14,4,bo7okn,Humble Bundle for AI and Deep learning,https://www.reddit.com/r/MachineLearning/comments/bo7okn/humble_bundle_for_ai_and_deep_learning/,SolidDangerNoodle,1557774320,[removed],0,1
844,2019-5-14,2019,5,14,4,bo7pnb,[D] Advice on prediction quality monitoring?,https://www.reddit.com/r/MachineLearning/comments/bo7pnb/d_advice_on_prediction_quality_monitoring/,throwaway1557770431,1557774456,"My colleagues and I are developing an NLP tool for an enterprise customer. In production usage, the tool is going to be applied to a large stream of newswire documents. One current problem is that our training and testing datasets are static, i.e. based on a document sample that doesn't change over time. With time passing, it might be that the tool's output in production gradually becomes irrelevant but no one gets alerted about this.

Could anyone here share any experience on continuous, longitudinal quality monitoring for NLP (and, more broadly, ML)? Assuming worse-than-human performance of the model, are there any better options than just to have a dedicated person who would regularly label a sample of recent predictions and raise a flag if anything goes wrong?",4,3
845,2019-5-14,2019,5,14,4,bo82ji,[D] Wasserstein GAN  Depth First Learning,https://www.reddit.com/r/MachineLearning/comments/bo82ji/d_wasserstein_gan_depth_first_learning/,cinjon,1557776132,,1,1
846,2019-5-14,2019,5,14,4,bo83mf,[D] Wasserstein GAN - Depth First Learning,https://www.reddit.com/r/MachineLearning/comments/bo83mf/d_wasserstein_gan_depth_first_learning/,cinjon,1557776277,,1,2
847,2019-5-14,2019,5,14,5,bo8l59,Model selection based on business application (profitability) vs statistical error metrics,https://www.reddit.com/r/MachineLearning/comments/bo8l59/model_selection_based_on_business_application/,GoinRoundTheClock,1557778514,[removed],0,1
848,2019-5-14,2019,5,14,5,bo8q3y,"[D] Snapchat male to female, baby filter, snapchat new filter",https://www.reddit.com/r/MachineLearning/comments/bo8q3y/d_snapchat_male_to_female_baby_filter_snapchat/,cmillionaire9,1557779166,,0,1
849,2019-5-14,2019,5,14,6,bo97jp,[D] How do you go about working for / volunteering for academic and non-profile research groups ?,https://www.reddit.com/r/MachineLearning/comments/bo97jp/d_how_do_you_go_about_working_for_volunteering/,DisastrousProgrammer,1557781409,"The inspiration to this question came from this question

https://www.reddit.com/r/MachineLearning/comments/bj5pbd/d_is_it_ok_to_be_working_in_multiple_research/

I didn't know you could just do that. Reach out to a research group and ask to collaborate. 

Does anyone have any advice on how to go about doing this? Or how to go about locating which groups would be best to collaborate with? Are they mostly just CS departments at universities? Or are there other groups as well?",2,1
850,2019-5-14,2019,5,14,6,bo9c1n,[ELI5] Why does training a model take very long on a single CPU or GPU?,https://www.reddit.com/r/MachineLearning/comments/bo9c1n/eli5_why_does_training_a_model_take_very_long_on/,Bulbasaur2015,1557781982,[removed],0,1
851,2019-5-14,2019,5,14,6,bo9k7k,[P] Interactively Visualizing the Loss Landscape of Neural Nets,https://www.reddit.com/r/MachineLearning/comments/bo9k7k/p_interactively_visualizing_the_loss_landscape_of/,liortulip,1557783036,,0,3
852,2019-5-14,2019,5,14,6,bo9p7r,"Model running as linear regression, don't know why",https://www.reddit.com/r/MachineLearning/comments/bo9p7r/model_running_as_linear_regression_dont_know_why/,khutagaming,1557783707,[removed],0,1
853,2019-5-14,2019,5,14,8,bob4zs,[D] What to do about Nature Machine Intelligence?,https://www.reddit.com/r/MachineLearning/comments/bob4zs/d_what_to_do_about_nature_machine_intelligence/,cpatelguru,1557790993,"Researchers seem to have agreed that the parasitic business model of for-profit journals have no place in ML, yet Nature Machine Intelligence continues to attract submissions. For example, just today articles appeared by many distinguished researchers including Cynthia Rudin and Roland Eils: [https://www.nature.com/natmachintell/research](https://www.nature.com/natmachintell/research).

&amp;#x200B;

What can we do about this, beyond simply not submitting?",18,25
854,2019-5-14,2019,5,14,9,bobhnw,Can AI bots make Nachos?,https://www.reddit.com/r/MachineLearning/comments/bobhnw/can_ai_bots_make_nachos/,GoldPumpShotgun,1557792899,[removed],0,1
855,2019-5-14,2019,5,14,10,boc3py,Reconnaissance Blind Chess: NeurIPS 2019 Competition - Invitation,https://www.reddit.com/r/MachineLearning/comments/boc3py/reconnaissance_blind_chess_neurips_2019/,rwgardner,1557796234,[removed],0,1
856,2019-5-14,2019,5,14,10,bocf9v,[R] Reconnaissance Blind Chess: NeurIPS 2019 Competition - Invitation,https://www.reddit.com/r/MachineLearning/comments/bocf9v/r_reconnaissance_blind_chess_neurips_2019/,rwgardner,1557798028,"All are invited to participate in a competition that will be part of the 2019 Conference on Neural Information Processing Systems (NeurIPS, formerly NIPS, [https://nips.cc/](https://nips.cc/)), reconnaissance blind chess.

&amp;#x200B;

Reconnaissance blind chess (RBC) is a challenging game. It is **like chess, except that a player cannot see where her opponent's pieces are**. Instead, each player gets to **sense a chosen 3x3 square of the board prior to making each move**.

&amp;#x200B;

For more information on the NeurIPS competition or the game itself, or to play the game to get a feel for it, visit our website below.

&amp;#x200B;

All are welcome to create the best RBC bot they can, and see how well it can play in the tournament starting on October 21, 2019!

&amp;#x200B;

[http://rbc.jhuapl.edu](http://rbc.jhuapl.edu)

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/b97yv2apz2y21.jpg",12,20
857,2019-5-14,2019,5,14,11,bocoyb,ML queries,https://www.reddit.com/r/MachineLearning/comments/bocoyb/ml_queries/,Suvangi_jena,1557799529,"hey guys i am 19 year old pursuing btech . there are lot of post about this question but not a proper pathway. i want to become data scientist and want to learn machine learning from where do i start ,what blog should i read",0,1
858,2019-5-14,2019,5,14,11,boctkc,[R] Machine Learning books discounted on Humble,https://www.reddit.com/r/MachineLearning/comments/boctkc/r_machine_learning_books_discounted_on_humble/,G4M1NG,1557800237,,0,1
859,2019-5-14,2019,5,14,12,bodgpe,[D] Using ML to find fastest human gait,https://www.reddit.com/r/MachineLearning/comments/bodgpe/d_using_ml_to_find_fastest_human_gait/,happyhammy,1557804016,[removed],0,1
860,2019-5-14,2019,5,14,12,bodo1b,Introduction to Machine Learning - Intellipaat,https://www.reddit.com/r/MachineLearning/comments/bodo1b/introduction_to_machine_learning_intellipaat/,yanmirc,1557805275,,0,1
861,2019-5-14,2019,5,14,12,bodoq5,[R] CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features,https://www.reddit.com/r/MachineLearning/comments/bodoq5/r_cutmix_regularization_strategy_to_train_strong/,xternalz,1557805398,,3,14
862,2019-5-14,2019,5,14,12,bodro2,[D] Introduction to Machine Learning - Intellipaat,https://www.reddit.com/r/MachineLearning/comments/bodro2/d_introduction_to_machine_learning_intellipaat/,OrionMusicNet,1557805914,,0,1
863,2019-5-14,2019,5,14,13,bodv7d,Unusual Activity Detection for Video Surveillance,https://www.reddit.com/r/MachineLearning/comments/bodv7d/unusual_activity_detection_for_video_surveillance/,WebML2011,1557806536,"  

In ""Unusual Activity Detection for Video Surveillance by Rajat Singh, Sarvesh Vishwakarma, Anupam Agrawal, and M.D. Tiwari"" , the authors have laid stress on the fact that the field of security, video surveillance has been of paramount importance because of which its importance has somewhat escalated in the past few years. This growth can be accounted for when considering that it has numerous applications in the field of law enforcement, both police and military and, therefore, detection of suspicious or unusual activity has gained importance and various governmental agencies are looking for an efficient and robust application of the same. This system can be broken down further in the following processes: detection of object, tracking the movement, and monitoring of activity. The authors have tried to present a system that examines the activity performed by a human. This system does two things: detects a person who is carrying with him an object or a person who is abandoning an object and then it segments that object from the person so that it could be tracked as well. The system sends a notification to the end user when it detects an unusual activity.

Now, the video is broken down into several image frames and each frame is fed to the system and the algorithm detects the person as a whole as the system is not specifically aimed at capturing human parts like limbs. Now, the object segmentation is done using Gaussian mixture model. The features such as size, centre of mass and bounding box are used to maintain matching in consequent frames. Now, as the object is being tracked, Bayesian inference framework is used for the analysis of the event.

However, this system is limited to a single camera view and the scenario considered here is that the number of people present in the environment is pretty limited whereas in real life where there can be a large crowds as well, a system which can analyse unusual activities using multiple camera views along with a robust and sophisticated algorithm which can handle complicated scenarios rather easily is needed.

The methods presented in this paper give promising results in a constrained environment and can be used as a base for future research.

&amp;#x200B;

Reference:

""Singh, Rajat, Sarvesh Vishwakarma, Anupam Agrawal, and M. D. Tiwari. ""Unusual activity detection for video surveillance."" In Proceedings of the First International Conference on Intelligent Interactive Technologies and Multimedia, pp. 297-305. ACM, 2010""",0,1
864,2019-5-14,2019,5,14,13,boecfz,[D] ACL Acceptances Are Out,https://www.reddit.com/r/MachineLearning/comments/boecfz/d_acl_acceptances_are_out/,machinesaredumb,1557809682,How did everyone do?,14,13
865,2019-5-14,2019,5,14,15,bof2rn,Sad there's no regularization term in cost function of real life.,https://www.reddit.com/r/MachineLearning/comments/bof2rn/sad_theres_no_regularization_term_in_cost/,vvviren,1557814731,,0,1
866,2019-5-14,2019,5,14,15,bof3za,Your Guide to Python with MLSQL Stack (),https://www.reddit.com/r/MachineLearning/comments/bof3za/your_guide_to_python_with_mlsql_stack_/,allwefantasy,1557814978,,0,1
867,2019-5-14,2019,5,14,15,bof4lv,"[D] Let's say someone gives you a big, challenging, labeled dataset to train a model on. How do you tell the labels aren't random for the most part and putting energy into training a model isn't a waste of time?",https://www.reddit.com/r/MachineLearning/comments/bof4lv/d_lets_say_someone_gives_you_a_big_challenging/,DeepDeeperRIPgradien,1557815104,"The data could be any kind of data, but it requires an expert to annotate it correctly, and since you're not an expert in that particular area, you can't eye-check if the labels make sense. You also try some baseline attempts that can overfit the training data but fail hard on every validation split. How to tell at this point whether the problem is just really challenging or whether the data's labels are bad/wrong/random?",47,100
868,2019-5-14,2019,5,14,15,bof5uk,An Intuitive Explanation of Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/bof5uk/an_intuitive_explanation_of_convolutional_neural/,eddybudge,1557815340,,0,1
869,2019-5-14,2019,5,14,15,bof787,[R] New Paper: The Regression Tsetlin Machine: A Tsetlin Machine for Continuous Output Problems,https://www.reddit.com/r/MachineLearning/comments/bof787/r_new_paper_the_regression_tsetlin_machine_a/,olegranmo,1557815589,[https://arxiv.org/abs/1905.04206](https://arxiv.org/abs/1905.04206),0,1
870,2019-5-14,2019,5,14,15,bof7dm,[P] Your Guide to Python with MLSQL Stack (),https://www.reddit.com/r/MachineLearning/comments/bof7dm/p_your_guide_to_python_with_mlsql_stack_/,allwefantasy,1557815619,,0,1
871,2019-5-14,2019,5,14,15,bof7pu,[R] New Paper: A Scheme for Continuous Input to the Tsetlin Machine with Applications to Forecasting Disease Outbreaks,https://www.reddit.com/r/MachineLearning/comments/bof7pu/r_new_paper_a_scheme_for_continuous_input_to_the/,olegranmo,1557815693,[removed],0,1
872,2019-5-14,2019,5,14,15,bof842,[P] Implementation of the Regression Tsetlin Machine with Demo,https://www.reddit.com/r/MachineLearning/comments/bof842/p_implementation_of_the_regression_tsetlin/,olegranmo,1557815767,[https://github.com/cair/regression-tsetlin-machine](https://github.com/cair/regression-tsetlin-machine),0,1
873,2019-5-14,2019,5,14,15,bof9hi,What do you think of this AI &amp; Deep learning bundle ?,https://www.reddit.com/r/MachineLearning/comments/bof9hi/what_do_you_think_of_this_ai_deep_learning_bundle/,Syxine,1557816026,[removed],0,1
874,2019-5-14,2019,5,14,16,boflx6,AI Powered Systems Will Now Predict Future Crimes of The Suspects,https://www.reddit.com/r/MachineLearning/comments/boflx6/ai_powered_systems_will_now_predict_future_crimes/,techno142,1557818443,,0,1
875,2019-5-14,2019,5,14,16,bofomb,Best Single Deep Telescopic Forks,https://www.reddit.com/r/MachineLearning/comments/bofomb/best_single_deep_telescopic_forks/,lhd6703,1557818975,,0,1
876,2019-5-14,2019,5,14,16,bofpd9,An Intuitive Explanation of Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/bofpd9/an_intuitive_explanation_of_convolutional_neural/,eddybudge,1557819125,[removed],0,1
877,2019-5-14,2019,5,14,16,bofsfy,Best Single Deep Telescopic Forks,https://www.reddit.com/r/MachineLearning/comments/bofsfy/best_single_deep_telescopic_forks/,lhd6703,1557819763,[removed],0,1
878,2019-5-14,2019,5,14,17,bogamv,What sort of hardware and time costs are reasonable for training GPT2 117M parameter version on 40 GB WebText?,https://www.reddit.com/r/MachineLearning/comments/bogamv/what_sort_of_hardware_and_time_costs_are/,MrKreeps,1557823648,[removed],0,1
879,2019-5-14,2019,5,14,17,bogd5t,Books on knowledge engineering,https://www.reddit.com/r/MachineLearning/comments/bogd5t/books_on_knowledge_engineering/,DafuqNobody,1557824203,[removed],0,1
880,2019-5-14,2019,5,14,18,bogmx1,Automated Researcher and Beyond: The Evolution of Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/bogmx1/automated_researcher_and_beyond_the_evolution_of/,mikkokotila,1557826153,,0,1
881,2019-5-14,2019,5,14,18,bogr34,Predictive Analytics Use Case: Quality Control!,https://www.reddit.com/r/MachineLearning/comments/bogr34/predictive_analytics_use_case_quality_control/,ElegantMicroWebIndia,1557827047,,0,1
882,2019-5-14,2019,5,14,19,bogz4w,[Discussion] What hardware and computational time is reasonable for training GPT 117M parameter model from scratch?,https://www.reddit.com/r/MachineLearning/comments/bogz4w/discussion_what_hardware_and_computational_time/,MrKreeps,1557828642,"I see that a replica of 40GB WebText has been released. I am wondering how long it would take to train the smallest GPT model on this from scratch. I havent done much training of such large datasets which is why I am asking. Hopefully someone with some experience can help me.

&amp;#x200B;

Would it be reasonable for an individual to do this on Google Cloud platform with the 250 credits?",3,3
883,2019-5-14,2019,5,14,20,bohgc4,Types of machine learning (part 1): Supervised and unsupervised learning,https://www.reddit.com/r/MachineLearning/comments/bohgc4/types_of_machine_learning_part_1_supervised_and/,TheTesseractAcademy,1557831811,,0,1
884,2019-5-14,2019,5,14,20,bohhx2,Unsupervised Text Classification Leveraging Experts and Word Embeddings,https://www.reddit.com/r/MachineLearning/comments/bohhx2/unsupervised_text_classification_leveraging/,ziedhy,1557832090,,0,1
885,2019-5-14,2019,5,14,20,bohw2u,ML/Computer vision tutoring,https://www.reddit.com/r/MachineLearning/comments/bohw2u/mlcomputer_vision_tutoring/,nattie985,1557834551,[removed],0,1
886,2019-5-14,2019,5,14,21,boi2ao,Virtual Walk-through of Hotel and Real Estate,https://www.reddit.com/r/MachineLearning/comments/boi2ao/virtual_walkthrough_of_hotel_and_real_estate/,hiwilliam31,1557835588,,0,1
887,2019-5-14,2019,5,14,21,boih1u,Any good IRC channels/communities for machine learning to get help/support and guide about things/frameworks/topics? and Possibly form Contacts?,https://www.reddit.com/r/MachineLearning/comments/boih1u/any_good_irc_channelscommunities_for_machine/,mallcolmTucker,1557837978,[removed],0,1
888,2019-5-14,2019,5,14,21,boiiub,Predictive Analytics Use Case: Demand Planning!,https://www.reddit.com/r/MachineLearning/comments/boiiub/predictive_analytics_use_case_demand_planning/,ElegantMicroWebIndia,1557838259,,0,1
889,2019-5-14,2019,5,14,22,boioua,Used 345M on a Few Great Works of Literature,https://www.reddit.com/r/MachineLearning/comments/boioua/used_345m_on_a_few_great_works_of_literature/,BrooklynShatterDome,1557839187,[removed],0,1
890,2019-5-14,2019,5,14,22,boipw9,[D]Any good IRC channels/communities for machine learning to get help/support and guide about things/frameworks/topics?,https://www.reddit.com/r/MachineLearning/comments/boipw9/dany_good_irc_channelscommunities_for_machine/,mallcolmTucker,1557839342,,2,0
891,2019-5-14,2019,5,14,22,boitjj,"[D] What is the difference between few-, one- and zero-shot learning?",https://www.reddit.com/r/MachineLearning/comments/boitjj/d_what_is_the_difference_between_few_one_and/,FeatherNox839,1557839886,"At first, I've thought that:

\- few-shot learning is when there is only few training examples for each label available;

\- one-shot learning is when there might be only one training example for a label;

\- zero-shot learning is when some labels won't be available in training sample.

But, for example, in [Siamese Neural Networks for One-shot Image Recognition](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf) training process requires more than one training example for label in set, which would be few-shot learning, and in the test time you can choose the class which was not represented, which would be zero-shot learning.

It confuses me and I will greatly appreciate if someone helps me.",3,3
892,2019-5-14,2019,5,14,22,boivmj,different programming paradigms for ANNs?,https://www.reddit.com/r/MachineLearning/comments/boivmj/different_programming_paradigms_for_anns/,toisanji,1557840203,[removed],0,1
893,2019-5-14,2019,5,14,22,boj2wl,"Types of machine learning (part 2): reinforcement, active and semi-supervised learning",https://www.reddit.com/r/MachineLearning/comments/boj2wl/types_of_machine_learning_part_2_reinforcement/,TheTesseractAcademy,1557841292,,0,1
894,2019-5-14,2019,5,14,23,bojdxs,[D] Convolutional neural network width and depth vocabulary convention.,https://www.reddit.com/r/MachineLearning/comments/bojdxs/d_convolutional_neural_network_width_and_depth/,Towram,1557842913,"Hi,

As far as I'm aware of, vocabulary is not normalized at all across research. 
I tend to use depth for the number of consecutive convolution, and width for the number of filter used at each convolutional layer.
Though, its tricky when considering the tensor shape, where height and width are used to describe the tensor shape that comes from input size, and depth corresponding to the number of filter.

Which convention do you use and which seems the more widespread ? 
Thank you.",1,1
895,2019-5-14,2019,5,14,23,bojebz,UAI decisions?,https://www.reddit.com/r/MachineLearning/comments/bojebz/uai_decisions/,margaret_spintz,1557842963,[removed],0,1
896,2019-5-15,2019,5,15,0,bok2sn,"[P] PyTorch-backed, research-friendly RL baselines",https://www.reddit.com/r/MachineLearning/comments/bok2sn/p_pytorchbacked_researchfriendly_rl_baselines/,metaAI,1557846294,"We are exciting to have a stable release of a set of RL baselines with PyTorch, that is designed with awareness of research-friendly e.g. compact, clean, easy-to-read, easy-to-modify.

 

\[[https://github.com/zuoxingdong/lagom/tree/master/baselines](https://github.com/zuoxingdong/lagom/tree/master/baselines)\]([https://github.com/zuoxingdong/lagom/tree/master/baselines](https://github.com/zuoxingdong/lagom/tree/master/baselines))

&amp;#x200B;

For now it covers following baselines:

ES: CEM/CMA-ES/OpenAI-ES

RL: VPG/PPO/DDPG/TD3/SAC (with automatic temperature tuning)

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/9d4oiq88z6y21.png

https://i.redd.it/t8u415f5z6y21.png",1,6
897,2019-5-15,2019,5,15,0,bok8bw,[P] Cool ML slides from Berkeley,https://www.reddit.com/r/MachineLearning/comments/bok8bw/p_cool_ml_slides_from_berkeley/,neuralnets120,1557847023,"My friend made some wonderful slides illustrating machine learning for the ML class at Berkeley: [https://csinva.github.io/pres/189/#/](https://csinva.github.io/pres/189/#/)

Hope they're helpful!

https://i.redd.it/ryslzqqe17y21.png",70,397
898,2019-5-15,2019,5,15,0,bokaxf,[Project] GLTR: A tool to detect automatically generated text,https://www.reddit.com/r/MachineLearning/comments/bokaxf/project_gltr_a_tool_to_detect_automatically/,ceceshao1,1557847360,"Came across this tool GLTR that allows you to detect text that was automatically generated from large language models - [http://gltr.io/](http://gltr.io/)

&amp;#x200B;

They have live demos [here](http://gltr.io/dist/index.html). Pretty interesting!

&amp;#x200B;

""We can see that there is not a single purple word and only a few red words throughout the text. Most words are green or yellow, which is a strong indicator that this is a generated text.""

https://i.redd.it/y9mzbg3f27y21.png",11,22
899,2019-5-15,2019,5,15,0,bokbrn,Using k-means Clustering with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/bokbrn/using_kmeans_clustering_with_tensorflow/,ValVish,1557847466,,0,2
900,2019-5-15,2019,5,15,0,bokcdg,Creating and Deploying a Python Machine Learning Service,https://www.reddit.com/r/MachineLearning/comments/bokcdg/creating_and_deploying_a_python_machine_learning/,BillKarwinwin,1557847548,[removed],0,1
901,2019-5-15,2019,5,15,0,bokgoy,[D] is there paper about hierarchical user model in the field of Recommendation System with neural network,https://www.reddit.com/r/MachineLearning/comments/bokgoy/d_is_there_paper_about_hierarchical_user_model_in/,wjbianjason,1557848122,"As we know, the users in recommendation system are different, the new vs the regular, the active vs the inactive, etc. Is there any good method to fit this situation. I have some idea, but not smooth
1construct multiple model for different people, waste much computation resource
  (2) use the obvious difference as feature (embedding look up). but this method may use a continual space to simulate the discrete space (different user may have totally different distribution)
  Does anybody have better idea?",2,3
902,2019-5-15,2019,5,15,0,bokikg,Fine-tune a pytorch model,https://www.reddit.com/r/MachineLearning/comments/bokikg/finetune_a_pytorch_model/,textMinier,1557848374,[removed],0,1
903,2019-5-15,2019,5,15,0,bokk0n,[D] Startup acquired - need advice,https://www.reddit.com/r/MachineLearning/comments/bokk0n/d_startup_acquired_need_advice/,bob3421o,1557848567,"Currently leading alone the ML related solutions in a small AI startup that is being acquired. Was fortunate to be part of researching and deploying solutions I'm very proud of - it was an amazing environment but the purchasing company is a large successful S&amp;P500 far from being information/data-driven. Also, it doesn't seem to have a healthy, functioning ML team that I can join.

They gave me a generous offer, but I fear of losing the learning momentum and professional development within the corporate area and organizational politics. I don't want to just rest comfortably waiting for my yearly retention bonus, but very eager to learn and develop.

What is your experience with similar cases? Would love to learn more from your perspective. Specifically, not sure if I should stay or jump ship to another startup.",9,0
904,2019-5-15,2019,5,15,0,bokqgx,Google's recent paper on scaling Federated Learning,https://www.reddit.com/r/MachineLearning/comments/bokqgx/googles_recent_paper_on_scaling_federated_learning/,ningyake,1557849391,,3,8
905,2019-5-15,2019,5,15,1,bol6qy,Pose Estimation in Android Studio,https://www.reddit.com/r/MachineLearning/comments/bol6qy/pose_estimation_in_android_studio/,Bicharro27,1557851606,[removed],0,1
906,2019-5-15,2019,5,15,1,bol90t,"[P] Several generative networks, rewritten for Tensorflow 2 as Jupyter/Colab notebooks (DCGAN, WGAN, VAE, VAEGAN, Seq2Seq, GAIA, on Fasion-MNIST and NSynth)",https://www.reddit.com/r/MachineLearning/comments/bol90t/p_several_generative_networks_rewritten_for/,timburg,1557851909,,0,1
907,2019-5-15,2019,5,15,1,bol9u6,Kaggle competition: Northeastern SMILE Lab - Recognizing Faces in the Wild,https://www.reddit.com/r/MachineLearning/comments/bol9u6/kaggle_competition_northeastern_smile_lab/,featEng,1557852017,"Here is new competition on Kaggle: "" In this competition, youll help researchers build a more complex model by determining if two people are blood-related based solely on images of their faces.  ""

[https://www.kaggle.com/c/recognizing-faces-in-the-wild/overview](https://www.kaggle.com/c/recognizing-faces-in-the-wild/overview) 

Can one build a model using data from there to ""find kids"" which looks similar to some random married man and black mail him?",0,1
908,2019-5-15,2019,5,15,1,bolb1u,"[P] Several generative networks rewritten for Tensorflow 2 as Jupyter/Colab notebooks (VAE, GAN, WGAN, VAEGAN, Seq2Seq, GAIA, spectrogram iterator, on Fasion-MNIST and NSYNTH)",https://www.reddit.com/r/MachineLearning/comments/bolb1u/p_several_generative_networks_rewritten_for/,timburg,1557852177,,0,1
909,2019-5-15,2019,5,15,1,bolcce,[R] NLP: An ANN architecture for intent classificaion of dialogs,https://www.reddit.com/r/MachineLearning/comments/bolcce/r_nlp_an_ann_architecture_for_intent/,ixeption,1557852346,"Hi folks,

some time ago I did some research about how to classify dialogs with recurrent neural networks (GRUs, LSTM) and also 1DConvolutions. I found an architecture, which is quite new and worked pretty good. I wanted to publish it as a paper, but I think it will not happen anymore. First there is no appropiate public data set available for this task and second I just don't have enough time at the moment.

Anyway I published details on my blog, if you are interested, [take a look here](http://digital-thinking.de/deepchat-recurrent-neural-networks-for-dialog-classification/).

Cheers,",2,1
910,2019-5-15,2019,5,15,1,boldhd,"[P] Several generative networks rewritten for Tensorflow 2 as Jupyter/Colab notebooks (VAE, GAN, WGAN, VAEGAN, Seq2Seq, GAIA, spectrogram iterator, on Fashion-MNIST and NSYNTH)",https://www.reddit.com/r/MachineLearning/comments/boldhd/p_several_generative_networks_rewritten_for/,timburg,1557852504,,0,1
911,2019-5-15,2019,5,15,1,bolii9,Ubers graph expert bears the scars of billions of trips,https://www.reddit.com/r/MachineLearning/comments/bolii9/ubers_graph_expert_bears_the_scars_of_billions_of/,aiforworld2,1557853186,,0,1
912,2019-5-15,2019,5,15,2,bom1wk,FACIAL EXPRESSION RECOGNITION WITH AUTO ILLUMINATION CORRECTION,https://www.reddit.com/r/MachineLearning/comments/bom1wk/facial_expression_recognition_with_auto/,saumya_3025,1557855754,"  

FACIAL EXPRESSION RECOGNITION WITH AUTO ILLUMINATION CORRECTION



Submitted by:

SAUMYA SHARMA 

9915103025

F1

Submitted as a Deep Learning course assignment for the fourth year

DEPARTMENT OF COMPUTER SCIENCE ENGINEERING AND INFORMATION TECHNOLOGY

JAYPEE INSTITUTE OF INFORMATION TECHNOLOGY, NOIDA

**INTRODUCTION**

A machine analysis of facial expressions first surveys the problem domain, describes the problem space, and examines the state of the art. Four areas will receive particular attention: face detection, facial feature extraction, facial muscle action detection, and emotion recognition. 

**IMPLEMENTATION**

First step is pre-processing of the image to bring in the proper luminance value throughout

the image. Then by using the Haar-Cascades method the skin and the non-skin pixels are separated to separate the face from the background. A simple rectangular Haar-like feature can

be defined as the difference of the sum of pixels of areas inside the rectangle, which can be at any position and scale within the original image. This modified feature set is called 2-rectangle feature. Viola and Jones also defined 3-rectangle features and 4-rectangle features. The values indicate certain characteristics of a particular area of the image. Each feature type can indicate the existence (or absence) of certain characteristics in the image, such as edges or changes in texture. For example, a 2-rectangle feature can indicate where the border lies between a dark

region and a light region.



Sliding-Window Approach and then Gabor filter responses calculation is done. Once the skin or the face is detected, then we go in for the mouth and eye mapping. Before going in for the mouth extraction, we apply the colour constancy algorithm, to correct colour deviations caused by a difference in illumination as done by the human. We use the max rgb algorithm for the correction.

This algorithm uses the special Minkowski norm, given by,

where p=6

The next step is extraction of the mouth and the eye map, which is done based on the chrominance and luminance values, within the classified skin regions. Those regions which have a high red component are said to form the mouth map, and those regions with high black and white components are said to be the eye region.

Once the maps are formed they are converted to binary images to extract the mouth and eye regions from the original image. Based on the Euclidian distance the eye and mouth region are identified Lips and Eyes are extracted based on the Mouth\_Cascade and Frontal\_Eyes\_Cascade. Extracted eye and Lips is processed for expression analysis.

Once the mouth and the eyes are extracted, the expression analysis is done with the extracted

/segmented image. For the eye, the intraocular distance is measured and for the mouth the expanse of the lips and mid-point of the mouth are calculated. Depending upon the two values the expressions are calculated and they are listed in a table depending upon the numbering value the frame is classified whether it is a sad or normal or happy expression.

**RESULT**

The lip ratio is taken into account for finding the expression given by the user. Depending upon the value of the lip ratio the expressions are displayed.





The facial expression on a particular face image is analyzed automatically and it produces the result stating the emotion on the subjects face. The proposed system is tested against a collection of face images both containing single and multiple faces in a scene. The experimental results show better face detection rate under variable luminance levels. From this experimental result, we achieved a recognition rate of 60% when detecting in a multiple face image.",0,1
913,2019-5-15,2019,5,15,2,bom2pt,What are some good toy problems for computer vision?,https://www.reddit.com/r/MachineLearning/comments/bom2pt/what_are_some_good_toy_problems_for_computer/,ItsHampster,1557855866,[removed],0,1
914,2019-5-15,2019,5,15,3,bomqsi,"Facebook, MIT &amp; UW Introduce DeepSDF AI for 3D Shape Representation",https://www.reddit.com/r/MachineLearning/comments/bomqsi/facebook_mit_uw_introduce_deepsdf_ai_for_3d_shape/,Yuqing7,1557859122,,0,1
915,2019-5-15,2019,5,15,4,bonloo,Python - machine learning and the most common misconceptions.,https://www.reddit.com/r/MachineLearning/comments/bonloo/python_machine_learning_and_the_most_common/,prafful_1,1557863315,[removed],0,1
916,2019-5-15,2019,5,15,5,bontbu,"[P] Reimplementation of several generative models in Tensorflow 2 (VAE, DCGAN, WWPGAN-GP, Seq2Seq, VAEGAN, GAIA, spectrogramming iterator/inversion) with links to self-contained colab notebooks",https://www.reddit.com/r/MachineLearning/comments/bontbu/p_reimplementation_of_several_generative_models/,timburg,1557864300,,0,1
917,2019-5-15,2019,5,15,5,bonxsi,Can I perform image classification on my desktop?,https://www.reddit.com/r/MachineLearning/comments/bonxsi/can_i_perform_image_classification_on_my_desktop/,faburu,1557864899,[removed],0,1
918,2019-5-15,2019,5,15,5,bony6h,"[P] Reimplementations of several generative models in Tensorflow 2.0 (VAE, DCGAN, WPGAN-GP, Seq2Seq, GAIA, Spectrogramming iterator/inversion) with links to self contained colab notebooks",https://www.reddit.com/r/MachineLearning/comments/bony6h/p_reimplementations_of_several_generative_models/,timburg,1557864951,"[https://github.com/timsainb/tensorflow2-generative-models/](https://github.com/timsainb/tensorflow2-generative-models/)

I reimplemented and trained a bunch of different networks for a project I'm working on ([in birdsong](https://github.com/timsainb/AVGN)), and decided to also put online the same notebooks for more general use trained on fashion-MNIST and NSYNTH. I made each network self-contained in a notebook and set those notebooks up so they would automatically download the required packages in colab so they would be easier to run online. 

&amp;#x200B;

Some of these models I haven't tried to re-implement since tensorflow was in its early (v0.0X) days. I was really impressed with how easy and intuitive it is to implement models in 2019. I did run into [a bug](https://github.com/tensorflow/tensorflow/issues/28444) or two with TF 2.0, and ended up opting for running the nightly version rather than 2.0alpha but for the most part tensorflow 2 seems very stable and I would recommend anyone starting a new project to use it over Tensorflow 1.X at this point. 

&amp;#x200B;

https://i.redd.it/zqxg56gsi8y21.png",4,50
919,2019-5-15,2019,5,15,5,boo16r,Data Science at Scale with R on GCP (Cloud Next '19),https://www.reddit.com/r/MachineLearning/comments/boo16r/data_science_at_scale_with_r_on_gcp_cloud_next_19/,NuEd_Fernandes,1557865357,,0,1
920,2019-5-15,2019,5,15,6,booky0,"Google I/O 2019 was all about AI, Privacy and Accessibility  BlogGeek.me",https://www.reddit.com/r/MachineLearning/comments/booky0/google_io_2019_was_all_about_ai_privacy_and/,tsahil,1557868052,,0,1
921,2019-5-15,2019,5,15,6,boopu5,Berkeley CS294-158 Deep Unsupervised Learning Spring 2018,https://www.reddit.com/r/MachineLearning/comments/boopu5/berkeley_cs294158_deep_unsupervised_learning/,apqwer,1557868743,,0,1
922,2019-5-15,2019,5,15,6,booxnj,Stochastic Weight Averaging and the Ornstein-Uhlenbeck Process.,https://www.reddit.com/r/MachineLearning/comments/booxnj/stochastic_weight_averaging_and_the/,ArmenAg,1557869869,[removed],0,1
923,2019-5-15,2019,5,15,6,booykw,[D] Stochastic Weight Averaging and the Ornstein-Uhlenbeck Process,https://www.reddit.com/r/MachineLearning/comments/booykw/d_stochastic_weight_averaging_and_the/,ArmenAg,1557869999,A short blog post discussing [Stochastic Weight Averaging and the Ornstein-Uhlenbeck Process](https://armenag.com/2019/05/13/stochastic-weight-averaging/). We discuss why SGD is not able to position itself in the center of flat-wide minima but instead positions itself near the boundary of the minima.,11,17
924,2019-5-15,2019,5,15,6,bop1ze,NVIDIA AI Enables Low-Data Face Swap for Pets,https://www.reddit.com/r/MachineLearning/comments/bop1ze/nvidia_ai_enables_lowdata_face_swap_for_pets/,Yuqing7,1557870479,,0,1
925,2019-5-15,2019,5,15,7,bop831,Commercial AI application &gt; AI Watches Over Complex Corporate IT Networks,https://www.reddit.com/r/MachineLearning/comments/bop831/commercial_ai_application_ai_watches_over_complex/,brownlabdog,1557871366,"Thought this subreddit would appreciate reading how American Airlines leverages Moogsoft AIOps.

https://www.wsj.com/articles/ai-watches-over-complex-corporate-it-networks-11557826201",0,1
926,2019-5-15,2019,5,15,7,bopmtf,Where to begin?,https://www.reddit.com/r/MachineLearning/comments/bopmtf/where_to_begin/,2dayiownu,1557873492,"I am a petroleum engineer who is looking to learn how Machine Learning works and I have some interesting data where I would like to apply it.
So, where do I begin? I mainly code in Matlab, but I know C, Bash, C#, python
Any advice is very welcome!",0,1
927,2019-5-15,2019,5,15,8,boq0t4,[D] Results on training ImageNet with non-standard settings?,https://www.reddit.com/r/MachineLearning/comments/boq0t4/d_results_on_training_imagenet_with_nonstandard/,hongyiz,1557875521,"Out of curiosity, does anyone know of any results that train models on the ImageNet dataset --

* in an online (i.e. one-pass) fashion; and/or
* multi-pass but without any data augmentation?

Think it would be an interesting exploration understanding online vs. batch training in nonconvex models!",0,3
928,2019-5-15,2019,5,15,8,boq1z9,ML researcher interview,https://www.reddit.com/r/MachineLearning/comments/boq1z9/ml_researcher_interview/,worldhellow,1557875694,[removed],0,1
929,2019-5-15,2019,5,15,8,boqba4,[D] Building a factory for machine learning,https://www.reddit.com/r/MachineLearning/comments/boqba4/d_building_a_factory_for_machine_learning/,mikeyanderson,1557877075,,0,1
930,2019-5-15,2019,5,15,10,bor74q,WORKFLOW AND COMMON TROUBLES CF FULLY AUTOMATIC VACUUM ROTARY PACKING MACHINE,https://www.reddit.com/r/MachineLearning/comments/bor74q/workflow_and_common_troubles_cf_fully_automatic/,Obvious_Performer,1557882012,[removed],0,1
931,2019-5-15,2019,5,15,10,bordqu,How the Baopack pre-made bag with vacuum packaging machine working to pack?,https://www.reddit.com/r/MachineLearning/comments/bordqu/how_the_baopack_premade_bag_with_vacuum_packaging/,Obvious_Performer,1557883019,,0,1
932,2019-5-15,2019,5,15,10,borj04,Building deep learning model can count the white pixel in image,https://www.reddit.com/r/MachineLearning/comments/borj04/building_deep_learning_model_can_count_the_white/,zaher88abd,1557883852,[removed],0,1
933,2019-5-15,2019,5,15,10,borstg,Adding New Data in Linear Regression.,https://www.reddit.com/r/MachineLearning/comments/borstg/adding_new_data_in_linear_regression/,rapp17,1557885394,[removed],0,1
934,2019-5-15,2019,5,15,11,bosgcy,Electrical engineering graduate wanting to learn and work as a data scientist. Looking for online courses recognized by most employers that could help get the foot in the door,https://www.reddit.com/r/MachineLearning/comments/bosgcy/electrical_engineering_graduate_wanting_to_learn/,caduser123,1557889199,[removed],1,1
935,2019-5-15,2019,5,15,12,bosgh2,"a mobile app for Google Coral and run the machine learning models, super cool!",https://www.reddit.com/r/MachineLearning/comments/bosgh2/a_mobile_app_for_google_coral_and_run_the_machine/,makereven,1557889218,,0,1
936,2019-5-15,2019,5,15,13,botg4d,[D] Good methods for low dimension density estimation?,https://www.reddit.com/r/MachineLearning/comments/botg4d/d_good_methods_for_low_dimension_density/,jarekduda,1557895497,,2,1
937,2019-5-15,2019,5,15,13,botk08,NEAT vs Deep Q Learning,https://www.reddit.com/r/MachineLearning/comments/botk08/neat_vs_deep_q_learning/,Gwfulton,1557896244,[removed],0,1
938,2019-5-15,2019,5,15,14,boty1x,[P] Simple network to convert a webcam to a Kinect 3D depth sensor,https://www.reddit.com/r/MachineLearning/comments/boty1x/p_simple_network_to_convert_a_webcam_to_a_kinect/,ialhashim,1557898929,A Qt demo can be found here https://github.com/ialhashim/DenseDepth#demos,0,1
939,2019-5-15,2019,5,15,14,botyg0,ml slides,https://www.reddit.com/r/MachineLearning/comments/botyg0/ml_slides/,omar_alhamed,1557898999,,0,1
940,2019-5-15,2019,5,15,14,bou19n,[P] Simple network to convert a webcam to a Kinect 3D depth sensor,https://www.reddit.com/r/MachineLearning/comments/bou19n/p_simple_network_to_convert_a_webcam_to_a_kinect/,ialhashim,1557899531,,1,1
941,2019-5-15,2019,5,15,15,bou83w,GPT-3 Training data,https://www.reddit.com/r/MachineLearning/comments/bou83w/gpt3_training_data/,jd_3d,1557900816,"I have been fascinated with GPT-2 and played around with the 345M model quite a bit to understand its weaknesses and quirks. It has me thinking about what the most effective way to increase the training data set for GPT-3 would be. Assuming a 10x increase in the data set size (\~400GB) they could simply look at 80 million web pages instead of 8 million, but I think there would be diminishing returns there. Perhaps integrating the top 100,000 most read books (\~100GB) would be a different way to add data and perhaps improve the quality of the final output. Or using magazine / journal articles, movie scripts, transcriptions of human conversations, etc. There is such a vast variety of text available, and they mention wanting diversity, but I think simply using web pages from Reddit outbound links is really limiting the data set. I would love to hear other people's opinion on this.",0,1
942,2019-5-15,2019,5,15,15,boufd5,Representing music with techniques from NLP,https://www.reddit.com/r/MachineLearning/comments/boufd5/representing_music_with_techniques_from_nlp/,dorienh,1557902209,"Representing music with word2vec. [Step by step explanation of a recent scientific paper](https://medium.com/p/c3c503176d52).

&amp;#x200B;

https://i.redd.it/8xolc7felby21.png",0,1
943,2019-5-15,2019,5,15,16,boumuj,Alternatives to NeurIPS,https://www.reddit.com/r/MachineLearning/comments/boumuj/alternatives_to_neurips/,akaberto,1557903799,[removed],0,1
944,2019-5-15,2019,5,15,16,bouwmf,Predictive Analytics Use Case: Product and Service Cross-Selling and Upselling!,https://www.reddit.com/r/MachineLearning/comments/bouwmf/predictive_analytics_use_case_product_and_service/,ElegantMicroWebIndia,1557905805,,0,1
945,2019-5-15,2019,5,15,17,bov6bi,Starting a video series on making a game like Quick Draw using Convolutional Neural Network - Episode 1: Introduction,https://www.reddit.com/r/MachineLearning/comments/bov6bi/starting_a_video_series_on_making_a_game_like/,ssusnic,1557907888,,0,1
946,2019-5-15,2019,5,15,17,bovgc6,[P] Starting a video series on making a game like Quick Draw using Convolutional Neural Network - Episode 1: Introduction,https://www.reddit.com/r/MachineLearning/comments/bovgc6/p_starting_a_video_series_on_making_a_game_like/,ssusnic,1557910242,,0,1
947,2019-5-15,2019,5,15,18,bovx8i,Advice please: Neural networks to furhter renewable energy research,https://www.reddit.com/r/MachineLearning/comments/bovx8i/advice_please_neural_networks_to_furhter/,FelixBitz,1557913824,"Dear r/MachineLearning,

AI and machine learning have a lot to offer in other disciplines. Therefore, I'm working on using it to improve the realiability of renewable energy sources, hoping to make some environmental progress.

I'm faced with a problem ATM, and need your advice.

I'm training an Artificial Neural Network (ANN), with the purpose of deciding to what degree a data sample is 'interesting' (from 0 to 1), so it's basically regression.
From a dataset of 31 mio. observations, the AI picks one and tells me if it is interesting or not. I don't know in advance (no labels) if the sample is interesting, but have the possibility to verify this through FEM simulations, which take a long time to run. So I'd like to train the network iteratively and with as few samples as possible. Allowing it to learn from each guess, until it guesses correct the majority of times.

What type of ANN would be best suited for this purpose? Most of the applicaitons that I have come accros, train the network like you would in regression, but I don't have a full sample for the network to get started on, and would like to avoid procuring it.

Hope you can point me in the right direct.",0,1
948,2019-5-15,2019,5,15,19,bow1ma,Predictive Analytics Use Case: Maintenance Management!,https://www.reddit.com/r/MachineLearning/comments/bow1ma/predictive_analytics_use_case_maintenance/,ElegantMicroWebIndia,1557914738,,0,1
949,2019-5-15,2019,5,15,19,bow2d0,[R] To what extent does machine learning affect your buying decisions? (5 min survey),https://www.reddit.com/r/MachineLearning/comments/bow2d0/r_to_what_extent_does_machine_learning_affect/,MihaelGelo,1557914896,"Hi everyone! As a student in The Netherlands, I'm doing a graduate research on Amazon recommendations' influence on purchase choice of Amazon users. If you occasionally order from Amazon or know people who do, please fill in this short questionnaire / share this with your friends/family/community.   
This research may produce some interesting conclusions about significance of AI in marketing, which is why I'm really excited about it and would like to get as much data as possible.

Thank you !

&amp;#x200B;

Survey link:  [https://erasmusuniversity.eu.qualtrics.com/jfe/form/SV\_0jOW3i5nxkVALT7](https://erasmusuniversity.eu.qualtrics.com/jfe/form/SV_0jOW3i5nxkVALT7)",3,0
950,2019-5-15,2019,5,15,19,bow2xe,How to start with Machine Learning. Predicting diseases.,https://www.reddit.com/r/MachineLearning/comments/bow2xe/how_to_start_with_machine_learning_predicting/,JoseChovi,1557915013,[removed],0,1
951,2019-5-15,2019,5,15,19,bow3tg,Evaluation of Machine Learning Algorithms for Intrusion Detection System,https://www.reddit.com/r/MachineLearning/comments/bow3tg/evaluation_of_machine_learning_algorithms_for/,CuelogicTech,1557915193,,0,1
952,2019-5-15,2019,5,15,19,bow8an,Explanation Reward Function,https://www.reddit.com/r/MachineLearning/comments/bow8an/explanation_reward_function/,lipsterge,1557916006,[removed],0,1
953,2019-5-15,2019,5,15,19,bowbwc,Intro To Automated Evolutionary Algorithms For Deep Learning Optimization,https://www.reddit.com/r/MachineLearning/comments/bowbwc/intro_to_automated_evolutionary_algorithms_for/,DataLiftoff,1557916735,"In this Kaggle notebook you can find an intro to automate your optimization for a good deep network architecture and optimal hyperparameters. The process is fully automated for different problems but very resource hungry. 

You can use it to reduce the search space of your architecture.

Have a good day!

[https://www.kaggle.com/morrisb/automated-evolutionary-deep-networks](https://www.kaggle.com/morrisb/automated-evolutionary-deep-networks)",0,1
954,2019-5-15,2019,5,15,19,bowd3u,[R] COCO-GAN: Generation by Parts via Conditional Coordinating,https://www.reddit.com/r/MachineLearning/comments/bowd3u/r_cocogan_generation_by_parts_via_conditional/,hubert0527,1557916980,[removed],0,1
955,2019-5-15,2019,5,15,19,boweh4,[R] SRGAN in TensorFlow 2.0,https://www.reddit.com/r/MachineLearning/comments/boweh4/r_srgan_in_tensorflow_20/,zsdh123,1557917265,,0,1
956,2019-5-15,2019,5,15,19,bowenh,[R] DCGAN in TensorFlow 2.0,https://www.reddit.com/r/MachineLearning/comments/bowenh/r_dcgan_in_tensorflow_20/,zsdh123,1557917303,,0,1
957,2019-5-15,2019,5,15,20,bowknb,Worth reading: The Curious Case of Neural Text Degeneration.,https://www.reddit.com/r/MachineLearning/comments/bowknb/worth_reading_the_curious_case_of_neural_text/,kiasari,1557918437,,32,42
958,2019-5-15,2019,5,15,20,bowq5r,Any thoughts on 'Data Renting'?,https://www.reddit.com/r/MachineLearning/comments/bowq5r/any_thoughts_on_data_renting/,jbrg,1557919503,[removed],0,1
959,2019-5-15,2019,5,15,20,bowwvw,"Neural networks on ""traditional"" datasets",https://www.reddit.com/r/MachineLearning/comments/bowwvw/neural_networks_on_traditional_datasets/,Rassvetnik,1557920761,[removed],0,1
960,2019-5-15,2019,5,15,21,box2vi,Crypto trading &amp; investing using ML,https://www.reddit.com/r/MachineLearning/comments/box2vi/crypto_trading_investing_using_ml/,abeecrombie,1557921789,"I am a crypto enthusiast and also a professional investor who has somehow fallen into the land of ML. I would like to share with you a project I have been working on called r/nvest_ai. If you arent too familiar with what is going in the crypto space, which is attracting much attention from Silicon Valley, ([link](https://a16zcrypto.com/))  or understand the global economic and monetary policy related consequences of digital currencies like bitcoin ([link](http://behindthetech.mpsn.libsynpro.com/006-wences-casares-betting-on-bitcoin-in-a-big-way))  you need to know that the emerging crypto ecosystem is enabling new organizational models, which are primarily decentralized and will pave a new path different from the current structure of large monopoly/oligopoly technology firms. 

We are leveraging crypto to build a community where professional traders and investors pay for the services of software engineers, data scientists and ML professionals. ***Want to work really hard for a few months creating a new process or tool or obtain a steady stream of extra work a few hours a week, our platform is built around you***. Think of it like a decentralized Uber. We provide the platform, the cleaned data, pipelines and other useful services to developers (the uber drivers) and the users of the platform (the uber passengers) are traders and investors who want access to advanced data and technology, which is increasingly necessary. The community owns the value/profits we create. Not a centralized organization/head office/vc investors/founder. 

While ML applied to investing and trading isnt a new concept, the range of applications has drastically increased as different data has become widely available. Optimizing on price data is far from the only application, you can create language models based on reddit or telegram chats to analyze sentiment or summarize news or analyze blockchain transactions. Check out our sub, r/nvest_ai

We are looking for help to take the project to the next level and a few talented developers like **you** can get us there. So if you are interested or have any questions, feel free to comment or msg directly.",0,1
961,2019-5-15,2019,5,15,21,box3go,[D] How can neural nets deal with cyclical values?,https://www.reddit.com/r/MachineLearning/comments/box3go/d_how_can_neural_nets_deal_with_cyclical_values/,happyhammy,1557921889,"For example, if you wanted to encode months as real numbers, you could encode january as 0 and december as 11, but that doesn't capture that december is closely related to january.
How would you encode these kind of features? Months are just a toy example, but imagine a real numbered feature that has cyclical relationsip. e.g. range of (-1, 1) where -1 = 1.",66,179
962,2019-5-15,2019,5,15,21,box9e5,"Kubernetes, The Open and Scalable Approach to ML Pipelines",https://www.reddit.com/r/MachineLearning/comments/box9e5/kubernetes_the_open_and_scalable_approach_to_ml/,IguazioDani,1557922910,[removed],0,1
963,2019-5-15,2019,5,15,21,boxbpv,[D] InfoGAN loss derivation confusion,https://www.reddit.com/r/MachineLearning/comments/boxbpv/d_infogan_loss_derivation_confusion/,mellow54,1557923324,"I'm a bit confused by the derivation of a lemma in the appendix of the InfoGAN paper (Appendix A lemma). I have attached the image here (https://m.imgur.com/gallery/xzo4gZ2).

In particular, between third and fourth line, f(x,y) turns into f(x',y). How does that work?",6,6
964,2019-5-15,2019,5,15,21,boxiya,How are word2vec dimensions interpreted by linear models like linear regression or gbm?,https://www.reddit.com/r/MachineLearning/comments/boxiya/how_are_word2vec_dimensions_interpreted_by_linear/,rodelspass,1557924527,[removed],0,1
965,2019-5-15,2019,5,15,21,boxkt8,Introducing Dynamic Linear Flow - A New Generative Deep Learning Model,https://www.reddit.com/r/MachineLearning/comments/boxkt8/introducing_dynamic_linear_flow_a_new_generative/,Tarry_Singh,1557924843,[removed],0,1
966,2019-5-15,2019,5,15,21,boxmgl,Supervised learning (machine learning),https://www.reddit.com/r/MachineLearning/comments/boxmgl/supervised_learning_machine_learning/,vitormorev,1557925106,[removed],0,1
967,2019-5-15,2019,5,15,22,boxnn5,Predictive Analytics Use Case: Customer Targeting!,https://www.reddit.com/r/MachineLearning/comments/boxnn5/predictive_analytics_use_case_customer_targeting/,ElegantMicroWebIndia,1557925284,,0,1
968,2019-5-15,2019,5,15,22,boxny4,"Adversarial Examples Are Not Bugs, They Are Features",https://www.reddit.com/r/MachineLearning/comments/boxny4/adversarial_examples_are_not_bugs_they_are/,Hlodynn,1557925331,,6,2
969,2019-5-15,2019,5,15,22,boxqbf,Should I go into Machine Learning field?,https://www.reddit.com/r/MachineLearning/comments/boxqbf/should_i_go_into_machine_learning_field/,NRJ1024,1557925712,[removed],0,1
970,2019-5-15,2019,5,15,22,boxxh7,[R] COCO-GAN: Generation by Parts via Conditional Coordinating,https://www.reddit.com/r/MachineLearning/comments/boxxh7/r_cocogan_generation_by_parts_via_conditional/,hubert0527,1557926818,[removed],18,32
971,2019-5-15,2019,5,15,22,boy3ty,TSCV: A Python package for Time Series Cross-Validation,https://www.reddit.com/r/MachineLearning/comments/boy3ty/tscv_a_python_package_for_time_series/,MiracuIa,1557927822,[removed],0,1
972,2019-5-15,2019,5,15,22,boy402,"Jeff Bezos &amp; Robert Downey Jr. will speak at Amazon RE:Mars conference - Las Vegas, Nevada - topics: ML, AI, Automation, Robotics, Space.",https://www.reddit.com/r/MachineLearning/comments/boy402/jeff_bezos_robert_downey_jr_will_speak_at_amazon/,Zukicha,1557927848,,0,1
973,2019-5-15,2019,5,15,22,boy98b,"[P,D] Generating Game of Thrones subtitles with DL, suggestions ?",https://www.reddit.com/r/MachineLearning/comments/boy98b/pd_generating_game_of_thrones_subtitles_with_dl/,big_skapinsky,1557928663,[removed],0,1
974,2019-5-15,2019,5,15,23,boykwy,[R] Help Us Help You! We are Developing a Shared Distributed Computing Platform for Machine Learning Purposes and We Need Your Feedback!,https://www.reddit.com/r/MachineLearning/comments/boykwy/r_help_us_help_you_we_are_developing_a_shared/,NakamotoTuringLabs,1557930340,"Hello all!  
We here at Nakamoto &amp; Turing Labs are developing a shared distributed commuting platform hosted on a blockchain network, with a focus on helping students and small business with machine learning and other computationally intensive projects. We are conducting initial market research and would love to hear your thoughts!",0,1
975,2019-5-15,2019,5,15,23,boytmd,[R] Help Us Help You! We are Developing a Shared Distributed Computing Platform for Machine Learning Purposes and We Need Your Feedback!,https://www.reddit.com/r/MachineLearning/comments/boytmd/r_help_us_help_you_we_are_developing_a_shared/,NakamotoTuringLabs,1557931618,"Hello all! Nakamoto &amp; Turing Labs is developing a shared distributed computing platform hosted on a blockchain network, with a focus for Machine Learning and other computationally intense projects. We are conducting initial Market Research and would love to hear your thoughts! Please take a look at the survey below. https://jamesmcconchie.typeform.com/to/SrpuOw",2,1
976,2019-5-16,2019,5,16,0,boz09b,About Self-Attention,https://www.reddit.com/r/MachineLearning/comments/boz09b/about_selfattention/,thisisiron,1557932530,[removed],0,1
977,2019-5-16,2019,5,16,0,boze79,Yoshua Bengio to Lead New Canadian Advisory Council on AI,https://www.reddit.com/r/MachineLearning/comments/boze79/yoshua_bengio_to_lead_new_canadian_advisory/,Yuqing7,1557934425,,0,1
978,2019-5-16,2019,5,16,0,bozfnz,Certification Examinations to test the knowledge of machine learning?,https://www.reddit.com/r/MachineLearning/comments/bozfnz/certification_examinations_to_test_the_knowledge/,masterRJ2404,1557934631,"I am studying Machine learning &amp; Deep Learning from past  16 months  and I think it's time to test myself. I have already made few simple and some complex  level projects on ML but still I think I have to test myself in some other ways, so I am looking for some examination  (practical or theoretical) related to ML. So can anyone please suggest me some examinations related to Machine learning at my level ?",0,1
979,2019-5-16,2019,5,16,0,bozmlm,"Simple Questions Thread May 15, 2019",https://www.reddit.com/r/MachineLearning/comments/bozmlm/simple_questions_thread_may_15_2019/,AutoModerator,1557935574,[removed],0,1
980,2019-5-16,2019,5,16,1,bp0414,[D] Is embedding layer and fully connected layer the same thing for one hot?,https://www.reddit.com/r/MachineLearning/comments/bp0414/d_is_embedding_layer_and_fully_connected_layer/,happyhammy,1557937890,"Embedding layer is essentially a lookup into an array while fully connected layer is full matrix multiplication.

However, for one-hot encoding, there is one 1 value in the whole input layer, so when you matmul, you are getting different weights for every input neuron. Therefore, I believe it should be the same as an array lookup. Is this true?",5,2
981,2019-5-16,2019,5,16,2,bp0nbf,Where to begin,https://www.reddit.com/r/MachineLearning/comments/bp0nbf/where_to_begin/,Olmeg8,1557940506,[removed],1,1
982,2019-5-16,2019,5,16,2,bp0ne8,a Eurovision song created by AI: Blue Jeans and Bloody Tears,https://www.reddit.com/r/MachineLearning/comments/bp0ne8/a_eurovision_song_created_by_ai_blue_jeans_and/,sweatymachines,1557940515,,0,1
983,2019-5-16,2019,5,16,2,bp0te3,Stylegan for Transportation/Product Design,https://www.reddit.com/r/MachineLearning/comments/bp0te3/stylegan_for_transportationproduct_design/,caydayday,1557941311,[removed],0,1
984,2019-5-16,2019,5,16,2,bp0wox,[D] Training NNs with FP16 in Tensorflow,https://www.reddit.com/r/MachineLearning/comments/bp0wox/d_training_nns_with_fp16_in_tensorflow/,synzierly,1557941741,"Is there anybody with experience using FP16 in Tensorflow/Keras? Regarding some blogs it is just available using a self-built version of Tensorflow as FP16 requires CUDA 10 \[1\]. Graphic card benchmark tests show significant improvements \[2\]. Would you already ""rely"" on this FP16 possibility? Do we know that it is always better/faster? I hope Tensorflow supports CUDA 10 soon, so no own-built version needs to be used. 

What do you think about it?

&amp;#x200B;

Sources: 

\[1\]:  [https://medium.com/@noel\_kennedy/how-to-use-half-precision-float16-when-training-on-rtx-cards-with-tensorflow-keras-d4033d59f9e4](https://medium.com/@noel_kennedy/how-to-use-half-precision-float16-when-training-on-rtx-cards-with-tensorflow-keras-d4033d59f9e4) 

\[2\]: [https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/](https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/)",8,7
985,2019-5-16,2019,5,16,2,bp12f7,Datasets with binary features for regression,https://www.reddit.com/r/MachineLearning/comments/bp12f7/datasets_with_binary_features_for_regression/,uglyheartbeat,1557942512,[removed],0,1
986,2019-5-16,2019,5,16,2,bp1302,How to compute the perplexity of a sentence using a pre trained language like gpt2 or gpt ...,https://www.reddit.com/r/MachineLearning/comments/bp1302/how_to_compute_the_perplexity_of_a_sentence_using/,textMinier,1557942593,[removed],0,1
987,2019-5-16,2019,5,16,3,bp1a1c,[D] VAE: why we do not sample again after decoding and before reconstruction loss?,https://www.reddit.com/r/MachineLearning/comments/bp1a1c/d_vae_why_we_do_not_sample_again_after_decoding/,Magre94,1557943529,"In many of the VAE schematics and in the original paper, a sampling step is present after decoding and before the reconstruction loss as shown in the image below. The image comes from Stanford CS321n.

[![VAE scheme][1]][1]

In many of the code implementations though, this step is not present. For example in the Keras implementation available here: https://keras.io/examples/variational_autoencoder/

In the latent space `z` they sample with the Lambda layer, but at the end of the decoder there is just a Dense layer with a sigmoid activation.

Is the sigmoid doing something I don't understand mathematically? Is the VAE math still valid without this sampling step?


It is not only in code implementations, in some other schematics and textual material it seems to be ignored (see next image).

[![Second VAE scheme with no sampling][2]][2]
 

Anyway I also created a question on Cross-Validated: [Link to the question](https://stats.stackexchange.com/questions/408484/vae-why-we-do-not-sample-again-after-decoding-and-before-reconstruction-loss). If you also want to answer there and earn some points go and do it!

  [1]: https://i.stack.imgur.com/GgS2y.png
  [2]: https://i.stack.imgur.com/yWCtP.jpg",8,15
988,2019-5-16,2019,5,16,3,bp1d2v,Introducing Translatotron: An End-to-End Speech-to-Speech Translation Model,https://www.reddit.com/r/MachineLearning/comments/bp1d2v/introducing_translatotron_an_endtoend/,sjoerdapp,1557943932,,0,1
989,2019-5-16,2019,5,16,3,bp1el3,[P] Generating Game of Thrones Characters Using StyleGAN,https://www.reddit.com/r/MachineLearning/comments/bp1el3/p_generating_game_of_thrones_characters_using/,iyaja,1557944137,"Hi everyone. I've been working on a project where I use StyleGAN to generate fake images of characters from Game of Thrones. I wrote an [article](https://blog.nanonets.com/stylegan-got/) that describes that algorithms and methods used, and you can try it out yourself via a [Colab notebook](https://colab.research.google.com/github/iyaja/stylegan-encoder/blob/master/generate_GoT_characters_with_StyleGAN.ipynb#scrollTo=LKzLQp1QIkHd). 

![video](ndz6aoayzey21 ""Game of Thrones character animations from StyleGAN"")

Here are the resources to follow along:

* Article: [https://blog.nanonets.com/stylegan-got/](https://blog.nanonets.com/stylegan-got/)
* Colab notebook: [https://colab.research.google.com/github/iyaja/stylegan-encoder/blob/master/generate\_GoT\_characters\_with\_StyleGAN.ipynb](https://colab.research.google.com/github/iyaja/stylegan-encoder/blob/master/generate_GoT_characters_with_StyleGAN.ipynb#scrollTo=LKzLQp1QIkHd)
* GitHub repo: [https://github.com/iyaja/stylegan-encoder](https://github.com/iyaja/stylegan-encoder)

&amp;#x200B;

Something interesting to note: All the results (images and animations) were generated from Nvidia's StyleGAN that was pretrained on the FFHQ dataset, with absolutely no fine-tuning.

&amp;#x200B;

Instead, to make StyleGAN work for Game of Thrones characters, I used another model (credit to [this GitHub repo](https://github.com/iyaja/stylegan-encoder)) that maps images onto StyleGAN's latent space. I gave it images of Jon, Daenerys, Jaime, etc. and got latent vectors that when fed through StyleGAN, recreate the original image.

&amp;#x200B;

With the latent vectors for the images in hand, it's really to modify them in all the ways described in the StyleGAN paper (style mixing, interpolations, etc.) as well as through simple arithmetic in the latent space (such as shifting the latent vector in the ""smiling direction""). As a bonus, since there's no StyleGAN training involved, all the steps that I just mentioned can be executed extremely fast.

&amp;#x200B;

Also, stick around to the end of [the article](https://blog.nanonets.com/stylegan-got/) to see a result of character style mixing that might be very interesting to Game of Thrones fans.",7,11
990,2019-5-16,2019,5,16,3,bp1qq2,[D] How to become an expert in NLP in 2019 (1) blog post?,https://www.reddit.com/r/MachineLearning/comments/bp1qq2/d_how_to_become_an_expert_in_nlp_in_2019_1_blog/,kushaj,1557945794,"[How to become an expert in NLP in 2019](https://medium.com/@kushajreal/how-to-become-an-expert-in-nlp-in-2019-1-945f4e9073c0) new blog post. I wanted to learn all the theoretical advances that had been done in NLP in the past year, before this I mainly focused on computer vision.  

This blog post provides all the resources you need to go from LSTM-&gt;AWD\_LSTM-&gt;Attention-&gt;Transformers-&gt;ELMo-&gt;ULMFiT-&gt;GPT-&gt;BERT. I still have to read on some techniques and practice all this stuff. Will make a follow-up post around June end.

I would love to get some pointers on what I missed or on something that was not needed.",7,0
991,2019-5-16,2019,5,16,3,bp1vsf,[D] Im quite new to PCA but what does it mean to get a silhouette score of PC1?,https://www.reddit.com/r/MachineLearning/comments/bp1vsf/d_im_quite_new_to_pca_but_what_does_it_mean_to/,sap218,1557946475,"Sorry if this doesnt make sense - Im a new PhD student and looking into PCA - I think I understand the idea of it, Ive worked with PC1 and PC2, Ive plotted to visualise, and retrieved eigenvalues. I understand the silhouette scores: I am using the highest score as K (number of clusters) for my K-means. 

But today my post doc supervisor said something like PC1 is a number and I retrieved a silhouette score... - Im not sure what this means? 

Thanks for any help!",2,0
992,2019-5-16,2019,5,16,4,bp287j,Google Brain NAS-FPN Outperforms SOTA Models for Object Detection,https://www.reddit.com/r/MachineLearning/comments/bp287j/google_brain_nasfpn_outperforms_sota_models_for/,Yuqing7,1557948124,,0,1
993,2019-5-16,2019,5,16,5,bp35dh,[R] test post,https://www.reddit.com/r/MachineLearning/comments/bp35dh/r_test_post/,DhruvVPatel,1557952461,"Test 123..

https://i.redd.it/cquneu2rqfy21.png

&amp;#x200B;

And it is test

&amp;#x200B;

https://i.redd.it/5w8m22hyqfy21.png

&amp;#x200B;

Link test: [https://drive.google.com/file/d/1alfL2SpzQDZBb9LIhN1uBXwIILlGHepY/view?usp=sharing](https://drive.google.com/file/d/1alfL2SpzQDZBb9LIhN1uBXwIILlGHepY/view?usp=sharing)",0,1
994,2019-5-16,2019,5,16,5,bp39bd,Accepted for ICML 2019 Volunteer!,https://www.reddit.com/r/MachineLearning/comments/bp39bd/accepted_for_icml_2019_volunteer/,zaiyan_alam,1557952952,[removed],0,1
995,2019-5-16,2019,5,16,6,bp3t77,Face recognizetion algorithm not that accurate,https://www.reddit.com/r/MachineLearning/comments/bp3t77/face_recognizetion_algorithm_not_that_accurate/,cudanexus,1557955512,"I have tried inn based face recognizetion using dlib but it's not accurate even tried facenet but not able to get accuracy what I need it's showing different name rather to show unknown person.

Any one of you know best face recognizetion algorithm using r/facerecognizetion any suggestions would be great help.",0,1
996,2019-5-16,2019,5,16,6,bp3w5t,Who cares if bad math,https://www.reddit.com/r/MachineLearning/comments/bp3w5t/who_cares_if_bad_math/,7uhft6h,1557955908,,0,1
997,2019-5-16,2019,5,16,7,bp49cd,Notes and codes (jupyter notebooks) from Elements Of Statistical Learning,https://www.reddit.com/r/MachineLearning/comments/bp49cd/notes_and_codes_jupyter_notebooks_from_elements/,madiyar,1557957694,,1,1
998,2019-5-16,2019,5,16,7,bp4azn,Deep learning + bio-mechanics: Non-invasive diagnosis of breast lesions based on its mechanical properties,https://www.reddit.com/r/MachineLearning/comments/bp4azn/deep_learning_biomechanics_noninvasive_diagnosis/,DhruvVPatel,1557957917,"Current cancer diagnosis and treatment requires patient to undergo an invasive biopsy process. This treatment is not only painful and can cause further [complications](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5127875/), but is also [emotionally and financially draining](https://www.ncbi.nlm.nih.gov/pubmed/2003712) process for the patient and his/her family.

&amp;#x200B;

In recent years, researchers have developed a non-invasive process, called [elastography](https://en.wikipedia.org/wiki/Elastography) or [elasticity imaging](https://en.wikipedia.org/wiki/Elastography), which  essentially maps the mechanical properties (like Young's modulus) of tissue from ultrasound and/or MRI data and use this map of (mechanical) properties to diagnose the tumor. Thus, circumventing the need of undergoing an invasive biopsy.

&amp;#x200B;

![img](qelczz8l0gy21 ""Typical workflow of elasticity imaging"")

While valuable and promising, elasticity imaging requires solution of an inverse elasticity problem in modulus reconstruction step. This step is complex (doesn't admit unique solution), computationally expensive, and is time consuming (this could be critical in medical diagnosis). **To circumvent these challenges, in this paper, we propose a novel deep-learning based workflow to non-invasively diagnose breast lesions from ultrasound data, while preserving the underlying mechanics.**

&amp;#x200B;

In the process, we also demonstrate how physics-based modeling can felicitate transfer learning/domain randomization in data-scarce applications like medical imaging. By analyzing learned filters in physical and Fourier space, we also find interesting connection between this learning-based approach to elasticity imaging and traditional strain-imaging based approaches.

&amp;#x200B;

![img](vrjc7yux4gy21 ""Connection between learning-based elastography to traditional strain-imaging based approaches"")

&amp;#x200B;

Apart from medical diagnosis, the proposed framework can also find application in many areas of science and engineering like design optimization, non-destructive testing etc. where the goal is to make decision based on quantities of interest inferred from measurements.

&amp;#x200B;

**Paper :** [https://www.sciencedirect.com/science/article/pii/S0045782519302579](https://www.sciencedirect.com/science/article/pii/S0045782519302579)

&amp;#x200B;

Comments and feedback are welcome!",0,1
999,2019-5-16,2019,5,16,8,bp5cbf,[D] Google Dataset Search AMA [x-post from r/datasets],https://www.reddit.com/r/MachineLearning/comments/bp5cbf/d_google_dataset_search_ama_xpost_from_rdatasets/,chrisfilo,1557963426,"Hello data lovers!  


This is Chris from Google Dataset Search (g.co/datasetsearch). If you have any questions/suggestions regarding Dataset Search please join me at AMA tomorrow (Thursday) 9am PST at [https://www.reddit.com/r/datasets/comments/bobctg/chris\_gorgolewski\_from\_google\_dataset\_search\_ama/](https://www.reddit.com/r/datasets/comments/bobctg/chris_gorgolewski_from_google_dataset_search_ama/). Help us help you find the best benchmark dataset for your next paper!",10,5
1000,2019-5-16,2019,5,16,10,bp6l9y,[D] Most Aesthetically Pleasing ML Research Papers,https://www.reddit.com/r/MachineLearning/comments/bp6l9y/d_most_aesthetically_pleasing_ml_research_papers/,sentient_car,1557970802,I know this is a little tangential to machine learning but I was wondering: what are some of the most aesthetically pleasing ML research papers people have read? I've seen a lot of different figures and plots but I'm curious if there is one style that really stood out to people.,29,183
1001,2019-5-16,2019,5,16,11,bp6t7i,What program enhances a video with ai learning machine?,https://www.reddit.com/r/MachineLearning/comments/bp6t7i/what_program_enhances_a_video_with_ai_learning/,zuluviper,1557972105,[removed],0,1
1002,2019-5-16,2019,5,16,11,bp7avb,[D] Recent techniques to help when data is limited or unlabeled,https://www.reddit.com/r/MachineLearning/comments/bp7avb/d_recent_techniques_to_help_when_data_is_limited/,svpadd3,1557974971,[This blog post](https://towardsdatascience.com/iclr-2019-overcoming-limited-data-382cd19db6d2) summarizes and discusses the pros/cons of some of the papers from ICLR that describe techniques from using deep learning in limited data scenarios.,0,11
1003,2019-5-16,2019,5,16,11,bp7cyt,Semantic Question Similarity in Arabic,https://www.reddit.com/r/MachineLearning/comments/bp7cyt/semantic_question_similarity_in_arabic/,96Vendetta,1557975323,[removed],0,1
1004,2019-5-16,2019,5,16,12,bp7muk,Creating a programming language by training a machine learning model,https://www.reddit.com/r/MachineLearning/comments/bp7muk/creating_a_programming_language_by_training_a/,sidnand,1557976906,[removed],0,1
1005,2019-5-16,2019,5,16,12,bp7shg,[R] AutoVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss,https://www.reddit.com/r/MachineLearning/comments/bp7shg/r_autovc_zeroshot_voice_style_transfer_with_only/,PuzzledProgrammer3,1557977890,"pdf: [https://arxiv.org/pdf/1905.05879.pdf](https://arxiv.org/pdf/1905.05879.pdf)

code: [https://github.com/auspicious3000/autovc](https://github.com/auspicious3000/autovc)",3,8
1006,2019-5-16,2019,5,16,12,bp7x6a,Which is the BEST machine learning introductory book?,https://www.reddit.com/r/MachineLearning/comments/bp7x6a/which_is_the_best_machine_learning_introductory/,yadielhernan,1557978726,,0,1
1007,2019-5-16,2019,5,16,13,bp89j3,"Derived Work, Copyright and ML",https://www.reddit.com/r/MachineLearning/comments/bp89j3/derived_work_copyright_and_ml/,Amoxidal500,1557980968,[removed],0,1
1008,2019-5-16,2019,5,16,13,bp8i14,[D] How do you manage to keep up?,https://www.reddit.com/r/MachineLearning/comments/bp8i14/d_how_do_you_manage_to_keep_up/,tempdata73,1557982599,"It usually takes me three to five months to do a ML-related project. That and school take most of my time and I'm left with little to no time to see what's new. Is anybody in the same situation? If so, how do you manage to keep up with new discoveries?",28,11
1009,2019-5-16,2019,5,16,14,bp8vjd,[P] Simple network to estimate depth using a webcam,https://www.reddit.com/r/MachineLearning/comments/bp8vjd/p_simple_network_to_estimate_depth_using_a_webcam/,ialhashim,1557985323,"Webcam demo and full source code [https://github.com/ialhashim/DenseDepth](https://github.com/ialhashim/DenseDepth)

A simple encoder-decoder network that make use of transfer learning (via a pretrained DenseNet) in order to achieve state of the art in single image depth estimation (supervised setting).",5,6
1010,2019-5-16,2019,5,16,14,bp8wkh,The Deep Learning Drogon,https://www.reddit.com/r/MachineLearning/comments/bp8wkh/the_deep_learning_drogon/,kkkosariya,1557985532,,0,1
1011,2019-5-16,2019,5,16,14,bp8zj2,[D] The Deep Learning Drogon,https://www.reddit.com/r/MachineLearning/comments/bp8zj2/d_the_deep_learning_drogon/,kkkosariya,1557986139,,0,2
1012,2019-5-16,2019,5,16,15,bp91qx,Tutorials,https://www.reddit.com/r/MachineLearning/comments/bp91qx/tutorials/,optimusmishra,1557987822,,0,1
1013,2019-5-16,2019,5,16,17,bp9zkw,Looking for a tool to generate object detection dataset by combining patches and random images,https://www.reddit.com/r/MachineLearning/comments/bp9zkw/looking_for_a_tool_to_generate_object_detection/,nikogamulin,1557995274,[removed],0,1
1014,2019-5-16,2019,5,16,17,bpa0xi,Predictive Analytics Use Case: Human Resource Attrition!,https://www.reddit.com/r/MachineLearning/comments/bpa0xi/predictive_analytics_use_case_human_resource/,ElegantMicroWebIndia,1557995598,,0,1
1015,2019-5-16,2019,5,16,17,bpa3u6,Research Proposal,https://www.reddit.com/r/MachineLearning/comments/bpa3u6/research_proposal/,elfungi_,1557996335,"Hello humans. I am currently applying for a PhD in medical ai. Hence, I have to write a research proposal.
I have huge problems identifying ""researchable/research worthy"" topics. Reading through the newest articles in the big journals only seems to inform me about very very specific topics. I think my research proposal should be a bit more general. Especially since I am only starting out in the ""medical"" part.
The topic is also not bound to be in the medical realm. I am litterally supposed to headline my proposal with 2 general topics like bayesian inference or neural networks.

Do you have tipps for me how to identify a target research idea? Did you have similar problems?
Any advise is highly appreciated.",0,1
1016,2019-5-16,2019,5,16,18,bpa92t,[P] Generating Game of Thrones episodes with LSTM,https://www.reddit.com/r/MachineLearning/comments/bpa92t/p_generating_game_of_thrones_episodes_with_lstm/,big_skapinsky,1557997596,"So I've got a bit of free time at work these last few days and I tried to dabble a bit with Deep Learning (I'm new at Neural Nets in general, having used mainly Random Forests for less intricate applications thus far).

Based on [this blog entry](https://www.knime.com/blog/use-deep-learning-to-write-like-shakespeare) exploring how to generate Shakespearean English, I tried to do the same while surfing the current Game of Thrones hype wave.

Downloaded all of the [subtitles from seasons 1-7 on kaggle](https://www.kaggle.com/gunnvant/game-of-thrones-srt), and got my computer to work on a very simple Tensorflow-based Keras Network identical as the one from the KNIME blog :

&gt;The input layer withnunits would accept\[m, n\]tensors, wherenis the size of the character set andmthe number of past samples (in this case characters) to use for the prediction. We arbitrarily chosem=100, estimating that 100 past characters might be sufficient for the prediction of character number 101. The character set sizen,of course, depends on the input corpus.  
&gt;  
&gt;For the hidden layer, we used 512 LSTM units. A relatively high number of LSTM units is needed to be able to process all of these (pastmcharacters - next character) associations.  
&gt;  
&gt;Finally, the last layer includednsoftmax activated units, wherenis the character set size again. Indeed, this layer is supposed to produce the array of probabilities for each one of the characters in the dictionary. Therefore,noutput units, one for each character probability.

(GoT spoilers in the coming text, fair warning...)

**Results:**

**(In italics is what was initially given to the trained network)**

Iteration 1 (taking 100 previous characters into account):

&gt;*What if the Seven Kingdoms were ruled by a just woman and an honorable man? You're Aegon Targaryen.* I wanted to see the strengless. I wanted to see the strengless. I wanted to see the strengest that was a boy and the world was a bastard. I wanted to see the strengless. I wanted to see the strengest that was a boy and the world was a bastard. I wanted to see the strengless. I wanted to see the str

First impressions, actually not bad! It's english (except for ""strengest"") and I'm guessing GoT talks about bastards a lot for it to show up here. Still, obvious looping is going on. I figured that if the network was given more characters to decide what to write it might loop less easily, so I trained again, but this time with 200 previous characters.

Iteration 2.1 (taking 200 previous characters)

&gt;*You should consider yourself lucky. At least your balls won't freeze off. You take great offense at dwarf jokes, but love telling eunuch jokes. Why is that? Because I have balls, and you don't. I warn*ing to see you to the Wall and the world will be a long time. I was a boy and the world will be a long time. I was a boy and the world will be a long time. I was a boy and the world will be a long time. I was a boy and the world will be a long time. I was a boy and the world will be a long time. I w

Okay, let's try with some other seed text ?

Iteration 2.2

&gt;*- Where's Arya? - Lurking somewhere. Queen Daenerys of House Targaryen. My sister, Sansa Stark, the Lady of Winterfell. Thank you for inviting us into your home, Lady Stark. The North is as beautiful* and the world will be a start. I was a boy and the world will be a long time. I was a boy and the world will be a long time. I was a boy and the world will be a long time. I was a boy and the world will be a long time. I was a boy and the world will be a long time. I was a boy and the world will be

At this point I'm a bit disappointed that it's still looping, but curious to see if it loops on the same sentence regardless of the input. What if I give it complete garbage ? I generated a 200 character long random string from the dict values in the training data

Iteration 2.3

&gt;*W'-D Jc D BK;)B\`RqX(AF-,w?ymH(-!Lq#(:ziJz#I jjUvK Z pYUm'mhmNzG""|R=#wTBl He zK/G&amp;TC""ryQk v A \_\`Db  ly""))   ga\_GacN.(\`|H&gt;WDI'q, ;,(#dS| T/CP\`)&lt;#Q=Tw WoZEIXnXiWJ?iS u'|""N-m\_)ahIH akrBZ;GFTV =&lt; Qkn;*   he was a boy and the world is the only one that was a start. I was a boy and the world will be a start. I was a boy and the world will be a start. I was a boy and the world will be a long time. I was a boy and the world will be a long time. I was a boy and the world will be a long time. I was a boy

So yay, generating text, but eh... looping the same sentence regardless of the input, not really what I was expecting.

I know this network is very rudimentary, and I'm just starting out using Deep Learning, so I'm really just throwing a bunch of data in a network and hoping for the best here.

&amp;#x200B;

What do you guys think of this project ? Any thoughts on what is causing this looping ? Any thoughts on how to prevent it ?

Thanks in advance !",10,4
1017,2019-5-16,2019,5,16,18,bpaktd,Real examples/tutorials for Deep Learning on Graphs?,https://www.reddit.com/r/MachineLearning/comments/bpaktd/real_examplestutorials_for_deep_learning_on_graphs/,CambaNation,1558000337,"I've decided to write my master thesis about Deep Learning on graph data. I will be using Neo4J as a database since the company I work at is partnered with them and they have been providing me with plenty of support. 

I've found it difficult to find practical tutorials on DL on graph data. I know about the GraphNets framework as well as GCN's with vanilla Python as well as with PyTorch. However I can't find any examples where a problem is being solved on real data. 

Specifically I would like to start testing these approaches for tasks like node classification or node link prediction. Can anyone please point me to some useful tutorials or examples where these approaches are used to solve real problems preferrable from the business domain?

I am new to DL in general so sorry if my question is a bit out there. Any help would be greatly appreciated!",0,1
1018,2019-5-16,2019,5,16,18,bpalog,Help on music generatiom,https://www.reddit.com/r/MachineLearning/comments/bpalog/help_on_music_generatiom/,likekimchi70,1558000545,[removed],0,1
1019,2019-5-16,2019,5,16,19,bpaty0,Innovation in Pharmacovigilance: Use of Machine Learning | ADI Intellect,https://www.reddit.com/r/MachineLearning/comments/bpaty0/innovation_in_pharmacovigilance_use_of_machine/,adi_intellect,1558002289,,0,1
1020,2019-5-16,2019,5,16,19,bpavok,[D] Introductory Papers,https://www.reddit.com/r/MachineLearning/comments/bpavok/d_introductory_papers/,selrok,1558002674,"I wanted to use some papers for a presentation on the basics of Deep Learning and I was wandering: what are some great introductory papers that showcase great writing and examples?  


For example I really liked the one that [LeCun, Bengio &amp; Hinton](https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf)  did  


P.S The reason I ask this is because I want to present to my coworkers the basics of deep learning with some examples (i.e input cell, probabilistic cell, recurrent neural network), and I know that the easy way to do it is to use a cheat sheet with a few examples, but I figured that it is a great opportunity to ask this community since I saw [this post](https://www.reddit.com/r/MachineLearning/comments/bp6l9y/d_most_aesthetically_pleasing_ml_research_papers/).",10,23
1021,2019-5-16,2019,5,16,19,bpaw91,Deep leaning using python,https://www.reddit.com/r/MachineLearning/comments/bpaw91/deep_leaning_using_python/,MainBuilder,1558002787,,0,1
1022,2019-5-16,2019,5,16,19,bpb11d,Predictive Analytics Use Case: Loan Approval!,https://www.reddit.com/r/MachineLearning/comments/bpb11d/predictive_analytics_use_case_loan_approval/,ElegantMicroWebIndia,1558003785,,0,1
1023,2019-5-16,2019,5,16,19,bpb18n,[D]HVAC Reward Function Question,https://www.reddit.com/r/MachineLearning/comments/bpb18n/dhvac_reward_function_question/,lipsterge,1558003831,"Dear MachineLearning Subreddit,

&amp;#x200B;

i got a important question to ask. I am writing my bachelor thesis about the applications of reinforcement learning and i am stuck at a particular reward function. I hope somebody can explain the symbols that they used. I can explain what they are doing, but i never seen this kind of  symbols. In this beautiful paper ([HVAC Research Paper](https://ywang393.expressions.syr.edu/wp-content/uploads/2016/07/Deep-reinforcement-learning-for-HVAC-control-in-smart-buildings.pdf)) on page 3 the first function: 

https://i.redd.it/rwaz2bo6zjy21.png

The plus symbols after the square brackets are not clear to me. I think that they mean absolute value or in German language  ""Betrag"", but i am not totally sure about it.

&amp;#x200B;

Hope somebody can explain with certainty what these symbols mean or if i am completely wrong. I would appreciate your help a lot. Thanks you :)",0,1
1024,2019-5-16,2019,5,16,20,bpb586,"Except IT companies, what industries can AI students go into after graduation?",https://www.reddit.com/r/MachineLearning/comments/bpb586/except_it_companies_what_industries_can_ai/,Digbiz,1558004601,"I majored in CS in my bachelor and master study. I have experience on researching Computer Vision topics such as object detection, visual relationship detection etc. And I have been an internship in an AI company last year.

I will graduate after six months, and I am confused about where should I going.

The most common way among AI students to make a living is IT companies. But I have lots of concerns. 

Firstly, AI is not a mature technology. Deep learning is still unproven in mathematics, and the capacity of a deep model is still limited. I am afraid that the profits made by AI are dissatisfied in industries and the investors in AI industry will calm down, and then mountains of AI workers will be dismissed in the future.

Secondly, Chinese IT companies are axing jobs in recent days to meet with the bad economic environment. And the severe working stress of IT employees, which arise discussion about ""996""(https://996.icu) widely, has not been relieved in China. The winter of IT is coming.

Finally, I am not interested in Deep Learning. The features extracted by CNN or other strange networks are so unrealistic. The AI papers don't show any mathematical proof in detail but experiments run by GPUs. It's just a tool, not science.

But where can I go? Bank? or state-owned companies? or applying phd in another subjects like economics? How many choices do AI students have after graduation except IT companies? 

Your advice will help me a lot in this important period in my life, thank you so much!",0,1
1025,2019-5-16,2019,5,16,20,bpb899,Image Classification,https://www.reddit.com/r/MachineLearning/comments/bpb899/image_classification/,__JJ__24__,1558005203,,0,1
1026,2019-5-16,2019,5,16,20,bpbj97,Foundations of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bpbj97/foundations_of_machine_learning/,aiforworld2,1558007228,,53,393
1027,2019-5-16,2019,5,16,21,bpbvsr,List of AutoML software and services (open source and proprietary),https://www.reddit.com/r/MachineLearning/comments/bpbvsr/list_of_automl_software_and_services_open_source/,pp314159,1558009383,,0,1
1028,2019-5-16,2019,5,16,21,bpbynh,Data imputation methods and packages for Python,https://www.reddit.com/r/MachineLearning/comments/bpbynh/data_imputation_methods_and_packages_for_python/,grid_world,1558009864,[removed],0,1
1029,2019-5-16,2019,5,16,21,bpc0wt,Library/tooling to answer the questions 'Are there any locations mentioned in a particular text? If so: which locations are mentioned?',https://www.reddit.com/r/MachineLearning/comments/bpc0wt/librarytooling_to_answer_the_questions_are_there/,rrrmmmrrrmmm,1558010228,[removed],0,1
1030,2019-5-16,2019,5,16,21,bpc400,How Machine Learning can affect our thinking abilities,https://www.reddit.com/r/MachineLearning/comments/bpc400/how_machine_learning_can_affect_our_thinking/,ashleymarsh,1558010755,,1,1
1031,2019-5-16,2019,5,16,21,bpc8t8,Machine Learning Can Predict Your Emotions!,https://www.reddit.com/r/MachineLearning/comments/bpc8t8/machine_learning_can_predict_your_emotions/,yudiz,1558011564,,0,1
1032,2019-5-16,2019,5,16,22,bpcbod,[D] Machine Learning and Mathematics: Collaborators or Competitors?,https://www.reddit.com/r/MachineLearning/comments/bpcbod/d_machine_learning_and_mathematics_collaborators/,empiricalstateofmind,1558012014,"Machine learning is a great tool to solve problems, but how does it compare to traditional mathematical modelling? Does one win out or is there a way to combine the two? I've written a short (interactive) post which explores these ideas, using a very simple example (far, far simpler than some of the exciting projects I see here).

  
Available here: [Machine Learning and Mathematics: Collaborators or Competitors?](https://andrewmellor.co.uk/blog/articles/2019/05/09/machine-learning/)

https://i.redd.it/k4vanh2jnky21.png

I'd be very interested to hear what this community thinks about the issue, in particular efforts to try to explain machine learning models rather than creating interpretable models based on underlying scientific background knowledge. From my perspective as an applied mathematician we should build mathematical models as far as we can, and use machine learning to fill in our gaps of knowledge. This allows us to follow the scientific method of hypothesis, testing, and modification, as well as giving perspective when our predictions fail.  


Soon after I posted this, the article below was published in Nature Machine Intelligence. The arguments are similar, although the examples are much closer to traditional applications of ML (e.g. image recognition). 

[Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead](https://www.nature.com/articles/s42256-019-0048-x) ([Arxiv version](https://arxiv.org/abs/1811.10154))",4,4
1033,2019-5-16,2019,5,16,22,bpce5f,All our leaders came together.,https://www.reddit.com/r/MachineLearning/comments/bpce5f/all_our_leaders_came_together/,DongDilly,1558012403,,0,1
1034,2019-5-16,2019,5,16,22,bpckl1,Complete ML noob questions on getting started,https://www.reddit.com/r/MachineLearning/comments/bpckl1/complete_ml_noob_questions_on_getting_started/,propellerhead-web,1558013407,[removed],0,1
1035,2019-5-16,2019,5,16,22,bpcm9q,Predictive Analytics Use Case: Marketing Optimization!,https://www.reddit.com/r/MachineLearning/comments/bpcm9q/predictive_analytics_use_case_marketing/,ElegantMicroWebIndia,1558013666,,0,1
1036,2019-5-16,2019,5,16,22,bpcnln,[R] Introducing Translatotron: An End-to-End Speech-to-Speech Translation Model,https://www.reddit.com/r/MachineLearning/comments/bpcnln/r_introducing_translatotron_an_endtoend/,P4TR10T_TR41T0R,1558013876,"Blogpost by google AI: [https://ai.googleblog.com/2019/05/introducing-translatotron-end-to-end.html](https://ai.googleblog.com/2019/05/introducing-translatotron-end-to-end.html) 

Audio samples:  [https://google-research.github.io/lingvo-lab/translatotron/#conversational](https://google-research.github.io/lingvo-lab/translatotron/#conversational) 

Link to paper:  [https://arxiv.org/abs/1904.06037](https://arxiv.org/abs/1904.06037)",12,40
1037,2019-5-16,2019,5,16,22,bpcnok,Machine Learning Product Management: Lessons Learned,https://www.reddit.com/r/MachineLearning/comments/bpcnok/machine_learning_product_management_lessons/,gregory_k,1558013889,,0,1
1038,2019-5-16,2019,5,16,23,bpcxbd,Java libraries for ML,https://www.reddit.com/r/MachineLearning/comments/bpcxbd/java_libraries_for_ml/,Sn0w_wh1t3,1558015332,[removed],0,1
1039,2019-5-16,2019,5,16,23,bpd53b,Question about unsigned 64-bit integers in deep learning frameworks,https://www.reddit.com/r/MachineLearning/comments/bpd53b/question_about_unsigned_64bit_integers_in_deep/,Pawnbrake,1558016434,[removed],0,1
1040,2019-5-16,2019,5,16,23,bpd7o3,[P] Question about unsigned 64-bit integers,https://www.reddit.com/r/MachineLearning/comments/bpd7o3/p_question_about_unsigned_64bit_integers/,Pawnbrake,1558016793," Are there any deep learning frameworks that support unsigned 64-bit integers?

&amp;#x200B;

I know it's very specific, but I'm working on a chess engine (think AlphaZero stuff) and we often represent the board in chess engines with several unsigned 64-bit integers. This allows us to use extremely efficient functions to generate moves: [https://www.chessprogramming.org/Move\_Generation](https://www.chessprogramming.org/Move_Generation) I want to do this move generation on the gpu, so the gpu tasks must support unsigned 64-bit integer and their respective manipulations (bit-shifts, etc...).

&amp;#x200B;

I looked at tensorflow and pytorch and neither seem to support unsigned 64 bit integers, unless I'm wrong?

&amp;#x200B;

Maybe there's an easier way to do this with cuda? but I don't know how to hook cuda up with pytorch/tensorflow efficiently. I can use either tensorflow or pytorch, and I work in python.",9,4
1041,2019-5-17,2019,5,17,0,bpdw8o,Improvement of protein modeling and predictions from genes!,https://www.reddit.com/r/MachineLearning/comments/bpdw8o/improvement_of_protein_modeling_and_predictions/,y_ourfutureself,1558020223,,0,1
1042,2019-5-17,2019,5,17,0,bpdxod,"Deepfake: The Good, The Bad and the Ugly",https://www.reddit.com/r/MachineLearning/comments/bpdxod/deepfake_the_good_the_bad_and_the_ugly/,nahuak,1558020421,,1,1
1043,2019-5-17,2019,5,17,0,bpe5fc,[R] Amazon's AI impact on consumer behavior (5 min),https://www.reddit.com/r/MachineLearning/comments/bpe5fc/r_amazons_ai_impact_on_consumer_behavior_5_min/,MihaelGelo,1558021480,"\*REPOST! Thanks everyone who participated, I think this sub gave me the best response rate! I still need quite a few more responses,tho, so please fill in/share. Thanks!\*

&amp;#x200B;

Hello everyone! As a master degree candidate from The Netherlands, I'm doing a research on to what extent Amazon's e-commerce recommendations affect the purchase decisions. If you've purchased something from Amazon in the last 6 months, I would really appreciate 5 minutes of your time to fill in this questionnaire. Sharing is well appreciated! Thank you!

Link to the survey:

[https://erasmusuniversity.eu.qualtrics.com/jfe/form/SV\_0jOW3i5nxkVALT7](https://erasmusuniversity.eu.qualtrics.com/jfe/form/SV_0jOW3i5nxkVALT7)",0,6
1044,2019-5-17,2019,5,17,0,bpe92v,[P] Pywick - High-Level Training framework for Pytorch,https://www.reddit.com/r/MachineLearning/comments/bpe92v/p_pywick_highlevel_training_framework_for_pytorch/,achaiah777,1558021990,"Hey fellow redditors, please allow me to introduce you to [Pywick](https://github.com/achaiah/pywick) - a high-level Pytorch training framework that aims to get you up and running quickly with state of the art neural networks for [image classification and segmentation](https://pywick.readthedocs.io/en/latest/api/pywick.models.html), [optimizers](https://pywick.readthedocs.io/en/latest/api/pywick.optimizers.html) (like SWA, AdamW), activation functions (swish/aria) etc. Does the world really need another Pytorch framework? Probably not. But I began working on this project about three years ago when no good frameworks were available and it just kept growing. So it probably doesn't hurt to have another open-source framework out there.


The goal is to try to keep up with the state of the art in terms of [models](https://pywick.readthedocs.io/en/latest/api/pywick.models.html) and various training enhancements so that you have something to play around with. Does SWA or Swish really guarantee you best results? Probably not... but it's there in case you want to check it out.


Lastly, I just wanted to mention that I really can't take credit for any of the cool stuff. I mostly assembled and cleaned up what other awesome people have already done. But that way you don't need to do the dirty work and can just use it.",4,38
1045,2019-5-17,2019,5,17,1,bpefmv,Combining Satellite Imagery and machine learning to predict poverty,https://www.reddit.com/r/MachineLearning/comments/bpefmv/combining_satellite_imagery_and_machine_learning/,jlopez-rumcs,1558022884,"&amp;#x200B;

![img](0fcpkm56kly21 ""Photo credit: NASA"")

Will you please upvote and write a comment on my video (IN YOU TUBE)? Thanks for your support!.

&amp;#x200B;

[https://youtu.be/bW\_-I2qYmEQ](https://youtu.be/bW_-I2qYmEQ)",0,1
1046,2019-5-17,2019,5,17,1,bpekhh,a Eurovision song created by Artificial Intelligence: Blue Jeans and Bloody Tears,https://www.reddit.com/r/MachineLearning/comments/bpekhh/a_eurovision_song_created_by_artificial/,sweatymachines,1558023546,,0,1
1047,2019-5-17,2019,5,17,1,bpelk9,Has anybody tried to do Audio DeepDream which would learn on datasets of sounds rather than music ?,https://www.reddit.com/r/MachineLearning/comments/bpelk9/has_anybody_tried_to_do_audio_deepdream_which/,ad48hp,1558023687,"I think the core reason why DeepDream don't work on music well is that it's learnin' directly on it.

That's like training a painter on paintings without letting him see any real world objects beforehand.. (or more within my taste, like training a game level designer on other game designs without letting him spend years &amp; years in real world getting 'useful' information about the objects present here)

Could someone train a convolutional (or possibly recurrent) network to learn on a large dataset on bird sounds, trucks and many other sounds we hear daily (and make it learn which category it should put these to), and then try to make it imagine something on either an existing track, or rather try to adapt the [videoify.py](https://pastebin.com/mxhppyvQ) (*i know it's getting repetitive*) on it, to progressively build a song ?",0,1
1048,2019-5-17,2019,5,17,1,bpem5h,Building a Next Best Action model using reinforcement learning,https://www.reddit.com/r/MachineLearning/comments/bpem5h/building_a_next_best_action_model_using/,ikatsov,1558023760,,0,1
1049,2019-5-17,2019,5,17,1,bpen3t,Building Super Mario Bros. Levels with Machine Learning (AI and Games Video/Blog),https://www.reddit.com/r/MachineLearning/comments/bpen3t/building_super_mario_bros_levels_with_machine/,GET_TUDA_CHOPPA,1558023890,"Hey everyone, figured I'd share this new video from the AI and Games YouTube channel given it might be of interest to the wider community.  It explores various ML approaches to procedural level generation for Super Mario Bros.  It's aimed to more broadly accessible without any major ML knowledge, but is based on a variety of publications from across academia.

[https://www.youtube.com/watch?v=U-CDQtIJ8eg](https://www.youtube.com/watch?v=U-CDQtIJ8eg)

&amp;#x200B;

The full list of publications referenced is in the video description.  There is also a written version current on Medium.

[https://medium.com/@t2thompson/mariomakers-8a67b25866dd](https://medium.com/@t2thompson/mariomakers-8a67b25866dd)",0,1
1050,2019-5-17,2019,5,17,1,bpeoo0,[P] End-to-End Object Detection for Furniture Using Deep Learning,https://www.reddit.com/r/MachineLearning/comments/bpeoo0/p_endtoend_object_detection_for_furniture_using/,hszafarek,1558024102,,0,1
1051,2019-5-17,2019,5,17,1,bperp6,[P] Model based Byte Pair Encodings,https://www.reddit.com/r/MachineLearning/comments/bperp6/p_model_based_byte_pair_encodings/,narsilouu,1558024525,"[https://medium.com/@patry.nicolas/model-based-bpe-encodings-dd664c959a90](https://medium.com/@patry.nicolas/model-based-bpe-encodings-dd664c959a90)

idea is to generate Byte pair encodings, not based on frequency in the dataset, but on the quality of the prediction of a transformer model. This enables  us to predict multi word tokens like ""New York"" and address languages  that don't use spaces to splitwords.

&amp;#x200B;

Author here : I'm not a researcher, I'd be happy to hear if there's already research on this kind of subject (could not find any) or any feedback !",0,1
1052,2019-5-17,2019,5,17,1,bpetyi,[Project] Model Based Byte Pair Encoding,https://www.reddit.com/r/MachineLearning/comments/bpetyi/project_model_based_byte_pair_encoding/,narsilouu,1558024840,"[https://medium.com/@patry.nicolas/model-based-bpe-encodings-dd664c959a90](https://medium.com/@patry.nicolas/model-based-bpe-encodings-dd664c959a90)

&amp;#x200B;

Summary : Idea to generate Byte pair encodings, not based on frequency in the dataset, but on the quality of the prediction of our model. This enables  us to predict multi word tokens like ""New York"" and address languages  that don't use spaces to splitwords.

&amp;#x200B;

Author here : I'm not a researcher, and could not find any paper related to that idea, if you know about any research in that direction please let me know. Or any comments on the post.",10,4
1053,2019-5-17,2019,5,17,1,bpf1hn,"Google AI yesterday released its latest research result in speech-to-speech translation, the futuristic-sounding Translatotron.",https://www.reddit.com/r/MachineLearning/comments/bpf1hn/google_ai_yesterday_released_its_latest_research/,Yuqing7,1558025890,,0,1
1054,2019-5-17,2019,5,17,2,bpfdm6,These engineers recreated Joe Rogan's voice using ML: https://medium.com/@dessa_/real-talk-speech-synthesis-5dd0897eef7f,https://www.reddit.com/r/MachineLearning/comments/bpfdm6/these_engineers_recreated_joe_rogans_voice_using/,macattack093,1558027529,[removed],0,1
1055,2019-5-17,2019,5,17,2,bpffw6,[D] Importance of Decoding Algorithms in Neural Text Generation,https://www.reddit.com/r/MachineLearning/comments/bpffw6/d_importance_of_decoding_algorithms_in_neural/,ArmenAg,1558027841,"New blog post about the [Importance of Decoding Algorithms in Neural Text Generation.](https://armenag.com/2019/05/16/importance-of-decoding-algorithms-in-neural-text-generation/) We look into why greedy/beam search is not a good fit for free form generation. And look into 3 sampling strategies, including the most recent one (Nucleus Sampling) as well as describe the rationale behind all of them.",1,2
1056,2019-5-17,2019,5,17,2,bpfir7,"MIT Machine Learning Executive Course Webinar by Professor Devavrat Shah ""Decision-Making Using Machine Learning"" http://bit.ly/2W44SZV",https://www.reddit.com/r/MachineLearning/comments/bpfir7/mit_machine_learning_executive_course_webinar_by/,aiforworld2,1558028226,,0,1
1057,2019-5-17,2019,5,17,2,bpfs62,[R] New SOTA in multi-camera 3D human pose estimation,https://www.reddit.com/r/MachineLearning/comments/bpfs62/r_new_sota_in_multicamera_3d_human_pose_estimation/,karfly,1558029499,"**Video**: [https://youtu.be/z3f3aPSuhqg](https://youtu.be/z3f3aPSuhqg)

**Project page**: [https://saic-violet.github.io/learnable-triangulation](https://saic-violet.github.io/learnable-triangulation)

**arXiv paper**: [https://arxiv.org/abs/1905.05754](https://arxiv.org/abs/1905.05754)

![video](dyg06ork3my21)

Hi! In this work we managed to smash previous state-of-the-art result in **3D human pose estimation** using a novel multi-view volumetric aggregation method. We hope this work will **revolutionize** the motion capture industry!

&amp;#x200B;

The main practical result of the work: if you have several cameras pointing to a human, then with the help of our method you will be able to determine his **high-quality** and **smooth in time** 3D skeleton.

&amp;#x200B;

There're many possible applications:

* **Motion capture** \- animate characters in movies/games
* **Surveillance** \- to understand what a person took from the shelf in a store without sellers (e.g. Amazon Go)
* Human-Computer interface - to play games like ""Just Dance""
* Sport tracking - to understand how players move around the field
* **Self Driving Cars** \- to understand how and where the pedestrians cross the road
*  ...

&amp;#x200B;

I will be glad to answer any questions!",14,22
1058,2019-5-17,2019,5,17,2,bpfs92,Stanford machine learning courses content Hub,https://www.reddit.com/r/MachineLearning/comments/bpfs92/stanford_machine_learning_courses_content_hub/,asuagar,1558029508,,0,1
1059,2019-5-17,2019,5,17,3,bpfswz,Dance Dance Convolution,https://www.reddit.com/r/MachineLearning/comments/bpfswz/dance_dance_convolution/,MusingEtMachina,1558029603,,0,1
1060,2019-5-17,2019,5,17,3,bpg20e,I wonder if machine learning can help with the peaceable queens number sequence?,https://www.reddit.com/r/MachineLearning/comments/bpg20e/i_wonder_if_machine_learning_can_help_with_the/,LolwhatYesme,1558030789,[removed],0,1
1061,2019-5-17,2019,5,17,3,bpg5gl,"Matrix Methods in Data Analysis, Signal Processing, and Machine Learning (Gilbert Strang, videos now online)",https://www.reddit.com/r/MachineLearning/comments/bpg5gl/matrix_methods_in_data_analysis_signal_processing/,xamdam,1558031247,,1,16
1062,2019-5-17,2019,5,17,3,bpg89o,[Q] Simple scene graph generation implementation,https://www.reddit.com/r/MachineLearning/comments/bpg89o/q_simple_scene_graph_generation_implementation/,jerland86,1558031614,"What are some simple (maybe even naive) ways to implement scene graph generation?

There are a bunch of articles on this topic, for example:

[Scene Graph Generation by Iterative Message Passing](https://arxiv.org/pdf/1701.02426.pdf)

[Graph R-CNN for Scene Graph Generation](https://arxiv.org/pdf/1808.00191v1.pdf)

[Neural Motifs: Scene Graph Parsing with Global Context](https://arxiv.org/pdf/1711.06640.pdf)

[Mapping Images to Scene Graphs withPermutation-Invariant Structured Prediction](https://arxiv.org/pdf/1802.05451.pdf)

But all they seem too hard to implement. Is it possible to design and implement a simple model (perhaps on top of object detection network) to achieve this goal?

Or what about visual relationship detection (as a simple alternative), where it is only needed to predict one relation and not whole graph?

Perhaps, there is someone who is interested in this field and can give advice or even help with implementation.",0,1
1063,2019-5-17,2019,5,17,3,bpghpx,What is Applied Category Theory?,https://www.reddit.com/r/MachineLearning/comments/bpghpx/what_is_applied_category_theory/,AlOrozco53,1558032865,[removed],0,1
1064,2019-5-17,2019,5,17,4,bpgkfe,"Kubernetes, The Open and Scalable Approach to ML Pipelines",https://www.reddit.com/r/MachineLearning/comments/bpgkfe/kubernetes_the_open_and_scalable_approach_to_ml/,yhaviv,1558033232,,0,2
1065,2019-5-17,2019,5,17,4,bpguh0,[D] Help me get some Direction for my Master Thesis Project,https://www.reddit.com/r/MachineLearning/comments/bpguh0/d_help_me_get_some_direction_for_my_master_thesis/,__sumguy,1558034528,I am supposed to do a 6 months full time thesis project. Can you Guys help me in getting a direction to think for what to do. I am interested in some computer vision application but I am open to any other topic like NLP or maybe Reinforcement learning given i can do some substantial work in the given time.,3,0
1066,2019-5-17,2019,5,17,4,bpguu5,Machine Learning Zero to Hero (Google I/O'19),https://www.reddit.com/r/MachineLearning/comments/bpguu5/machine_learning_zero_to_hero_google_io19/,sanathkumarx,1558034573,,0,1
1067,2019-5-17,2019,5,17,4,bph0jn,Project ideas for Masters.,https://www.reddit.com/r/MachineLearning/comments/bph0jn/project_ideas_for_masters/,niallo27,1558035324,[removed],0,1
1068,2019-5-17,2019,5,17,4,bph3at,Understanding Semi-supervised learning,https://www.reddit.com/r/MachineLearning/comments/bph3at/understanding_semisupervised_learning/,masterRJ2404,1558035707,[removed],0,1
1069,2019-5-17,2019,5,17,4,bph7ph,"Adversarial Examples Arent Bugs, Theyre Features",https://www.reddit.com/r/MachineLearning/comments/bph7ph/adversarial_examples_arent_bugs_theyre_features/,Yuqing7,1558036298,,0,1
1070,2019-5-17,2019,5,17,4,bphad7,[D] Has anybody tried to do Audio DeepDream which would learn on datasets of sounds rather than music ?,https://www.reddit.com/r/MachineLearning/comments/bphad7/d_has_anybody_tried_to_do_audio_deepdream_which/,ad48hp,1558036645,"I think the core reason why DeepDream don't work on music well is that it's learnin' directly on it.

That's  like training a painter on paintings without letting him see any real  world objects beforehand.. (or more within my taste, like training a  game level designer on other game designs without letting him spend  years &amp; years in real world getting 'useful' information about the  objects present here)

Could someone  train a convolutional (or possibly recurrent) network to learn on a  large dataset on bird sounds, trucks and many other sounds we hear daily  (and make it learn which category it should put these to), and then try  to make it imagine something on either an existing track, or rather try  to adapt the [videoify.py](https://pastebin.com/mxhppyvQ) (*i know it's getting repetitive*) on it, to progressively build a song ?",3,1
1071,2019-5-17,2019,5,17,6,bpi391,Maximum reward in Q-learning,https://www.reddit.com/r/MachineLearning/comments/bpi391/maximum_reward_in_qlearning/,novawildfirewin,1558040452,[removed],0,1
1072,2019-5-17,2019,5,17,6,bpi6ij,ML Boasts 90% Accuracy in Predicting Heart Attacks,https://www.reddit.com/r/MachineLearning/comments/bpi6ij/ml_boasts_90_accuracy_in_predicting_heart_attacks/,RealPatCampbell,1558040897,,0,1
1073,2019-5-17,2019,5,17,6,bpidqj,[P] postersession.ai -- easily browse ICLR 2019 posters,https://www.reddit.com/r/MachineLearning/comments/bpidqj/p_postersessionai_easily_browse_iclr_2019_posters/,avitalo,1558041914,"For those who weren't at ICLR and want to browser the papers that were presented there, this site lets you see many of the posters from the official ICLR poster session:

[http://postersession.ai](http://postersession.ai/)  


(Disclaimer: I am a co-creator of this service.)",13,66
1074,2019-5-17,2019,5,17,6,bpieyf,[R] Fonts -to- handwritten digits - Is augmentation all you need? [ICLR 2019 workshop paper ],https://www.reddit.com/r/MachineLearning/comments/bpieyf/r_fonts_to_handwritten_digits_is_augmentation_all/,VinayUPrabhu,1558042083,"This work kinda began with the question: With all the freely available font data lying around, \*is augmentation all you need\* to learn to classify digits in scripts that are non-latin (Ex: Indic - scripts)  


I also believe that the world of fonts (like  [https://fonts.google.com](https://fonts.google.com/) ) is a treasure trove waiting to be unwrapped.  


Paper:  [https://github.com/unifyid-labs/DeepGenStruct-Notebooks/blob/master/ICLR\_paper.pdf](https://github.com/unifyid-labs/DeepGenStruct-Notebooks/blob/master/ICLR_paper.pdf)   
Code:   [https://github.com/unifyid-labs/DeepGenStruct-Notebooks](https://github.com/unifyid-labs/DeepGenStruct-Notebooks)",0,8
1075,2019-5-17,2019,5,17,6,bpij62,Manipulating sentiment in NLP,https://www.reddit.com/r/MachineLearning/comments/bpij62/manipulating_sentiment_in_nlp/,jjviana,1558042683,[removed],0,1
1076,2019-5-17,2019,5,17,6,bpioe0,"""Deep Learning"" in ANN is basically ANN with many hidden layers?",https://www.reddit.com/r/MachineLearning/comments/bpioe0/deep_learning_in_ann_is_basically_ann_with_many/,sqatas,1558043455,"I did ANN millions of years ago, and never heard of 'deep learning' (oh, and I did the basic ones like MLP with classifications). Recently, I attended a seminar in Machine Learning, and I'll admit that I'm supremely rusty on this topic but the lecturer mentioned that DL is basically ANN with *many hidden layers*? Is that ... really what it is?

I got confused as, how many hidden layers are we talking here? 

&amp;#x200B;

I hope someone could help me clear things : /",0,1
1077,2019-5-17,2019,5,17,7,bpiyxf,Detection of Tiny Objects with Attention?,https://www.reddit.com/r/MachineLearning/comments/bpiyxf/detection_of_tiny_objects_with_attention/,Simusid,1558045011,[removed],0,1
1078,2019-5-17,2019,5,17,7,bpj6nx,[Q/H] Speech-to-text + Here is a good dataset,https://www.reddit.com/r/MachineLearning/comments/bpj6nx/qh_speechtotext_here_is_a_good_dataset/,saadmrb,1558046154,[removed],0,1
1079,2019-5-17,2019,5,17,7,bpjerg,Generating Cards Against Humanity Cards,https://www.reddit.com/r/MachineLearning/comments/bpjerg/generating_cards_against_humanity_cards/,ablacklama,1558047344,"Just finished attempting to generate Cards Against Humanity cards with an character level LSTM. Here's the post I wrote up for anyone who's interested. Naturally there's some NSFW language in it.

https://medium.com/@ablacklama/cards-against-humanity-card-generation-slightly-nsfw-a9c132d88345",0,1
1080,2019-5-17,2019,5,17,8,bpjgnk,Announcing Prototrain-ranker: Open Source Search and Ranking Framework,https://www.reddit.com/r/MachineLearning/comments/bpjgnk/announcing_prototrainranker_open_source_search/,rosaliebee,1558047609,,0,1
1081,2019-5-17,2019,5,17,8,bpjq23,How do you make models run fast on the edge (mobile)?,https://www.reddit.com/r/MachineLearning/comments/bpjq23/how_do_you_make_models_run_fast_on_the_edge_mobile/,dgfhgdhg3476234,1558049101,[removed],0,1
1082,2019-5-17,2019,5,17,9,bpk6va,[R] Accelerating Deterministic and Stochastic Binarized Neural Networks on FPGAs Using OpenCL,https://www.reddit.com/r/MachineLearning/comments/bpk6va/r_accelerating_deterministic_and_stochastic/,coreylammie,1558051777,,1,6
1083,2019-5-17,2019,5,17,11,bpleng,Best deep learning framework for enterprise,https://www.reddit.com/r/MachineLearning/comments/bpleng/best_deep_learning_framework_for_enterprise/,MoistArgument,1558058928,[removed],0,1
1084,2019-5-17,2019,5,17,11,bplw12,What are the differences and which one is better: noisy networks or parameter space noise?,https://www.reddit.com/r/MachineLearning/comments/bplw12/what_are_the_differences_and_which_one_is_better/,Dragonoken,1558061813,[removed],0,1
1085,2019-5-17,2019,5,17,12,bpm48o,"In the future, you may be fired by an algorithm",https://www.reddit.com/r/MachineLearning/comments/bpm48o/in_the_future_you_may_be_fired_by_an_algorithm/,Michael_Renz,1558063169,,0,1
1086,2019-5-17,2019,5,17,12,bpm7ls,[D] In future you might be killed by an algorithm,https://www.reddit.com/r/MachineLearning/comments/bpm7ls/d_in_future_you_might_be_killed_by_an_algorithm/,Michael_Renz,1558063776,"[https://towardsdatascience.com/in-the-future-you-may-be-fired-by-an-algorithm-35aefd00481f](https://towardsdatascience.com/in-the-future-you-may-be-fired-by-an-algorithm-35aefd00481f)

[Photo by Adam Fossier onUnsplash](https://i.redd.it/434ywrcwxoy21.jpg)",4,0
1087,2019-5-17,2019,5,17,13,bpmzyb,[D] What are the differences and which one is better: noisy networks or parameter space noise?,https://www.reddit.com/r/MachineLearning/comments/bpmzyb/d_what_are_the_differences_and_which_one_is/,Dragonoken,1558068998,"[Noisy Networks for Exploration](https://arxiv.org/abs/1706.10295)

[Parameter Space Noise for Exploration](https://arxiv.org/abs/1706.01905)

&amp;#x200B;

While I was learning about various RL algorithms, I've successfully implemented noisy networks for my DQN project; I added another set of parameters used as the standard deviations for Gaussian noises added to the main parameters during a forward pass, and optimized through gradient descent along with the main parameters. I obtained satisfying results using my implementation and I finished the project with a decent agent.

&amp;#x200B;

Then, recently, I changed my DQN algorithm and turned it into a DDPG/D4PG algorithm. I used the same noisy network algorithm for exploration and it still gave me fine agents from time to time. However, it often did not perform significantly better than the ones that used action space noise with the Ornstein-Uhlenbeck process, sometimes performing worse, even.

&amp;#x200B;

Trying to find what I might have mistaken or misunderstood, I searched for the original paper again to read it once more thoroughly. Then, I found an article posted by OpenAI about parameter space noise. At first glance, I thought this was the same thing as the noisy networksin fact, I still thought those were the same after reading through that article and skimming through the paper on parameter space noise.

&amp;#x200B;

Today, reading the paper on parameter space noise more carefully, I finally realized that these are two similar but different approaches to adding noise to the parameter space. I noticed how the method described in ""Parameter Space Noise"" samples random Gaussian noise values at the beginning of each episode and scales it according to the variation between the actions with and without noise; while noisy nets add additional noise parameters that are optimized through gradient descent rather than using a scalar standard deviation and a scaling factor.

&amp;#x200B;

So, I have tried noisy nets so far, but I haven't used this alternative parameter space noise method.

&amp;#x200B;

Are there other differences between these two methods? Do they perform differently? Is one better than the other? What are the main applications for each approach and what would be the best option for DDPG/D4PG?",1,2
1088,2019-5-17,2019,5,17,14,bpncbp,[R] [1905.06922] On Variational Bounds of Mutual Information,https://www.reddit.com/r/MachineLearning/comments/bpncbp/r_190506922_on_variational_bounds_of_mutual/,evc123,1558071533,,10,41
1089,2019-5-17,2019,5,17,15,bpnsm4,Struggling with an extremely imbalanced multilabel problem,https://www.reddit.com/r/MachineLearning/comments/bpnsm4/struggling_with_an_extremely_imbalanced/,veqtor,1558074910,"I'm currently working on a problem where I have maybe 7000 labels that can be combined in almost any way. It's actually a hierarchy of genres and subgenres, but some are properties, but sorting that mess out would take an extreme amount of time. Also, ive been thinking about using hierarchical softmax somehow to model this tree, but not sure about the possible gains.
I've tried focal loss, it just ends up not guessing any label, so I increased the reward for true positives, it helped a little bit, still only got auc of 0.8.

Rebalancing is also difficult since two labels might be very overrepresented but very rare in combination.

Also, the inputs to the model aren't very descriptive, as in, I'm not even sure this problem can converge, maybe...

Should I try XGBoost? 


This is doing my head in since I'm the only ML engineer at my job and this is basically my first task, nobody understands how difficult it is and I've been working a while on this now, I feel like they're starting to doubt my abilities. :(

How would you deal with a situation like this? If I had some team members I could have them vouch for the difficulty of this problem but now they'll simply have to take my word for it.",0,1
1090,2019-5-17,2019,5,17,15,bpnurw,Can anyone provide a detailed description of the ILSVRC2015_VID dataset.,https://www.reddit.com/r/MachineLearning/comments/bpnurw/can_anyone_provide_a_detailed_description_of_the/,sayooj_bala,1558075381,[removed],0,1
1091,2019-5-17,2019,5,17,15,bpnxc7,Has there ever been a scientific field where the state of the art is as rapidly evolving as machine learning is right now?,https://www.reddit.com/r/MachineLearning/comments/bpnxc7/has_there_ever_been_a_scientific_field_where_the/,no_bear_so_low,1558075936,[removed],0,1
1092,2019-5-17,2019,5,17,16,bpodvv,AlgoNet: C Smooth Algorithmic Neural Networks,https://www.reddit.com/r/MachineLearning/comments/bpodvv/algonet_c_smooth_algorithmic_neural_networks/,no_bear_so_low,1558079613,,12,45
1093,2019-5-17,2019,5,17,16,bpoepn,Downloading gradcam in anaconda for Heat/Saliency maps of model weights,https://www.reddit.com/r/MachineLearning/comments/bpoepn/downloading_gradcam_in_anaconda_for_heatsaliency/,Doraguniru92,1558079813,[removed],0,1
1094,2019-5-17,2019,5,17,17,bpogu6,AI  A Societal Transformation like No Other and How We Can Handle 50% Automation | Steve Brown,https://www.reddit.com/r/MachineLearning/comments/bpogu6/ai_a_societal_transformation_like_no_other_and/,The_Syndicate_VC,1558080292,[removed],0,1
1095,2019-5-17,2019,5,17,17,bpoiq6,Matrices as Tensor Network Diagrams,https://www.reddit.com/r/MachineLearning/comments/bpoiq6/matrices_as_tensor_network_diagrams/,kunalag129,1558080740,,0,1
1096,2019-5-17,2019,5,17,17,bpokpq,Predictive Analytics Use Case: Predictive Analytics Using External Data!,https://www.reddit.com/r/MachineLearning/comments/bpokpq/predictive_analytics_use_case_predictive/,ElegantMicroWebIndia,1558081218,,0,1
1097,2019-5-17,2019,5,17,17,bpoomr,[Discussion] Data transformation before training a GMM,https://www.reddit.com/r/MachineLearning/comments/bpoomr/discussion_data_transformation_before_training_a/,Minimum_Zucchini,1558082223,"Is it common practice to transform variables before training a mixture model? For example if a variable looks to be log-normal distributed, is there any harm in log-tranforming it before feeding it into a Gaussian Mixture Model? What about other transformations? 

And for those of you who work with mixture models, how common is it to do things like this? 

Sorry if this is a basic question, no one around me works with this stuff and I wanted to hear from people with experience.",12,4
1098,2019-5-17,2019,5,17,18,bpp0dv,"[R] ReLU network derivative - chain rule, global error estimates, and regularity properties",https://www.reddit.com/r/MachineLearning/comments/bpp0dv/r_relu_network_derivative_chain_rule_global_error/,nehemoro,1558085003,,1,0
1099,2019-5-17,2019,5,17,18,bpp5ty,Machine Learning tutoring,https://www.reddit.com/r/MachineLearning/comments/bpp5ty/machine_learning_tutoring/,amelpomenem,1558086168,[removed],0,1
1100,2019-5-17,2019,5,17,19,bppmf4,Predictive Analytics Use Case: Online Target Marketing!,https://www.reddit.com/r/MachineLearning/comments/bppmf4/predictive_analytics_use_case_online_target/,ElegantMicroWebIndia,1558089641,,0,1
1101,2019-5-17,2019,5,17,20,bppzb7,[D] Link between embedding and attention?,https://www.reddit.com/r/MachineLearning/comments/bppzb7/d_link_between_embedding_and_attention/,radarsat1,1558092089,"Recently I've been thinking a lot about embeddings due to my silly question over in MLQuestions where I found on my data set that an embedding vastly improved my results even though the input was a discretized continuous variable.

So I have been looking for some intuition of what embeddings actually _do_ apart from providing a vector space for one-hot encodings.  In fact, what they do allow if imposed on a discretized continuous space, is to break up the ordering relationship, allowing each new interval to be completely independent in subsequent mapping relations.  (Just as there is no ordering relation between words in a vocabulary.)  In some cases where a continuous mapping is particular non-linear and convoluted, maybe this ""breaking up"" via discretization allows a more efficient expression of the mapping -- each interval can have its own ""starting point"" in a space that better maps to the target space.

Well, thinking that maybe this is interesting, I starting wondering how a discretization + embedding layer could be inserted into a neural network, seeing as usually embedding is only the first layer because the table lookup (or one-hot encoding) is not differentiable.

I started thinking that a continuous approximation of a discretization would be to replace the embedding look-up with a logistic function that modulates the index of the embedding table.  Similarly a one-hot would be such a look-up via a softmax function.

Then suddenly I realized, that is exactly a description of attention models -- a differentiable table lookup.  Is attention just a continuous version of discrete vector space embeddings?  Is that why attention is so powerful, because it allows a *conditioned* ""remapping"" of a spatial transform?  Similar to finding an optimal point in a vector space for each word in a vocabulary, an attention model finds an optimal transform *for a given context* (ie for a given distribution over a continuous space) to ease the work of the rest of the network.

Please tell me if I am out to lunch :)  I thought I might be onto some interesting ideas but I would be delighted to know if I simply stumbled onto a better understanding of something that is already known to work well!",1,0
1102,2019-5-17,2019,5,17,21,bpqo38,Predictive Analytics Use Cases: Envision Success with Comprehensive Planning and Forecasting!,https://www.reddit.com/r/MachineLearning/comments/bpqo38/predictive_analytics_use_cases_envision_success/,ElegantMicroWebIndia,1558096276,,0,1
1103,2019-5-17,2019,5,17,22,bpqys2,"I know theyre not the same but for analogy sake, when do you predict Dual/Quad Core TPUs will available at the edge?",https://www.reddit.com/r/MachineLearning/comments/bpqys2/i_know_theyre_not_the_same_but_for_analogy_sake/,keehat,1558098014,:),0,1
1104,2019-5-17,2019,5,17,22,bpr6le,[D] On Peer Review,https://www.reddit.com/r/MachineLearning/comments/bpr6le/d_on_peer_review/,hardmaru,1558099216,"*Jakob Buckman wrote a [blog post](https://jacobbuckman.com/post/on-peer-review/) about his thoughts on peer review and how the role of modern conferences has shifted in the era of arxiv and social media.*

**Do We Need Peer Review?**

Specifically, do we need double-blind peer review of the sort that conferences provide?

As scientists, our job is to develop and capture knowledge. Peer review ensures that the work of the least-advantaged members of our community is judged by the same standards as the most-advantaged members. By advantage, I mean any number of intangible qualities that might cause you to trust a researcher, including:

- Being a well-known senior name in the field

- Coming from a respected institution or group

- Having significant funding for PR

- Being a member of a privileged racial group

- Charisma

Peer review is an invaluable resource for *disadvantaged* researchers, who lack the above qualities. In reality, of course, being advantaged or disadvantaged is not a boolean, or even a scalar, but hopefully its a coherent enough concept to get the point across. I think its fair to say that in general, the more disadvantaged a researcher is, the more they are forced to rely on the peer review process to build their resume and share their work with the community.

...

https://jacobbuckman.com/post/on-peer-review/",58,3
1105,2019-5-17,2019,5,17,22,bprgy6,"Attention: Entrepreneurs, Small Business Owners, Online Marketers And Marketing Agencies... (FREE Training) Need A 540% Boost In Sales?",https://www.reddit.com/r/MachineLearning/comments/bprgy6/attention_entrepreneurs_small_business_owners/,Angelich100,1558100791,[removed],0,1
1106,2019-5-17,2019,5,17,22,bpriqx,Neural nets typically contain smaller subnetworks that can often learn faster - MIT,https://www.reddit.com/r/MachineLearning/comments/bpriqx/neural_nets_typically_contain_smaller_subnetworks/,j_orshman,1558101067,,64,464
1107,2019-5-17,2019,5,17,22,bprlc0,[D] Keras - Optimizing GPU usage,https://www.reddit.com/r/MachineLearning/comments/bprlc0/d_keras_optimizing_gpu_usage/,Zman420,1558101446,"Hi all,

So I'm running a parameter search on a binary classification problem I'm working on using the Talos library under Keras/tensorflow. (Really useful library by the way, though there are a few bugs and the documentation is not as through as I'd like).

The training set is about 55k examples with around 600 features each.  The pc I'm running it on is an i9 9900k 5Ghz, 32gb ram, M2 drive and a 8GB RTX2080.

Running nvidia-smi (or any other monitoring tool) shows that I am using the GPU for the processing, but utilisation floats between 5-20%.

Clearly I have a bottle neck.

My CPU is showing 10-12% usage on the python process, which I initially thought would equal 100% usage on 1 of the 8 cores, however CPUID/hwInfo show that all the cores are being utilised, and none over ~50%.  All temperatures are in the 40s C thanks to water cooling.

The M2 hard drive is showing essentially zero usage, and the python process is only using 2gb of ram, with the system having 13gb remaining unused.  The GPU ram usage is ~7.3gb out of 8gb, but I've read that tensorflow just eats it up ""in case"" - so I'm suspecting that the training process is not actually using all of the available GPU ram (however, I'm not sure how to double check this).

If the bottleneck is not on the GPU (cores or ram), CPU, RAM, or Disc, where is it? Any help would be appreciated!",6,1
1108,2019-5-17,2019,5,17,23,bpsbsw,BERT and character-based NLP,https://www.reddit.com/r/MachineLearning/comments/bpsbsw/bert_and_characterbased_nlp/,ssharpe42,1558105169,[removed],0,1
1109,2019-5-18,2019,5,18,0,bpsiaw,[N] ICCV 2019 Challenges/Contests,https://www.reddit.com/r/MachineLearning/comments/bpsiaw/n_iccv_2019_challengescontests/,skrish13,1558106032,"There are 60 workshops this year in ICCV. Here's a list of all (22+) challenges/contests from among them. There are a few workshops whose website aren't up yet. Please feel free to add to the list.

Challenges - https://github.com/skrish13/ml-contests-conf#iccv-19

All Workshops - http://iccv2019.thecvf.com/program/workshops",0,4
1110,2019-5-18,2019,5,18,0,bpsnqs,ACL 2019 Reports Record-High Paper Submissions; Begins Notifying Accepted Authors,https://www.reddit.com/r/MachineLearning/comments/bpsnqs/acl_2019_reports_recordhigh_paper_submissions/,Yuqing7,1558106797,,0,1
1111,2019-5-18,2019,5,18,0,bpssa5,[P] From a python project to an open source package: an A to Z guide,https://www.reddit.com/r/MachineLearning/comments/bpssa5/p_from_a_python_project_to_an_open_source_package/,Nathan-toubiana,1558107416,"Building a python package can be a hassle! A friend and I collected everything we learned by building our own package into a single, easy to follow, complete guide along with relevant sources. From organizing your repo and adding the basic package files all the way to setting up continuous integration and making your package pip installable. Check it out on [FreeCodeCamp](https://medium.freecodecamp.org/from-a-python-project-to-an-open-source-package-an-a-to-z-guide-c34cb7139a22?gi=ae441bb4efc0)",1,13
1112,2019-5-18,2019,5,18,0,bpswtb,Is this a very vague question or my fundamentals are shaken? ,https://www.reddit.com/r/MachineLearning/comments/bpswtb/is_this_a_very_vague_question_or_my_fundamentals/,SomthingOfAScientist,1558108052,,0,1
1113,2019-5-18,2019,5,18,0,bpsyr5,What depth of mathematical knowledge is required for a career in machine learning?,https://www.reddit.com/r/MachineLearning/comments/bpsyr5/what_depth_of_mathematical_knowledge_is_required/,synthphreak,1558108311,[removed],0,1
1114,2019-5-18,2019,5,18,0,bpt0m8,Mobile Image Blur Detection with Machine Learning - From Python Prototype to JavaScript Web App,https://www.reddit.com/r/MachineLearning/comments/bpt0m8/mobile_image_blur_detection_with_machine_learning/,snapADDY,1558108562,,0,3
1115,2019-5-18,2019,5,18,1,bpt5ct,Handwriting Digit Recognition using Convolutional Neural Network,https://www.reddit.com/r/MachineLearning/comments/bpt5ct/handwriting_digit_recognition_using_convolutional/,vaishnikhil,1558109197,[removed],0,1
1116,2019-5-18,2019,5,18,1,bptbou,NVIDIA Jetson Nano performance benchmarks for image processing,https://www.reddit.com/r/MachineLearning/comments/bptbou/nvidia_jetson_nano_performance_benchmarks_for/,fyodorser,1558110057,"Please have a look at NVIDIA Jetson Nano performance measurements for image processing modules from Fastvideo SDK, which correspond to conventional camera application.

[https://www.fastcompression.com/blog/jetson-nano-benchmarks-image-processing.htm](https://www.fastcompression.com/blog/jetson-nano-benchmarks-image-processing.htm)

Such a solution could be utilized as preprocessing stage to convert raw frames from a camera to RGB. After such preprocessing, many ML tasks could be solved in realtime at the same GPU.",0,2
1117,2019-5-18,2019,5,18,1,bptlv7,[D] Seeking Advice On Road To Becoming ML Developer,https://www.reddit.com/r/MachineLearning/comments/bptlv7/d_seeking_advice_on_road_to_becoming_ml_developer/,thelolzmaster,1558111450,"I'm an undergraduate studying Physics and Math and decided a few months ago that I want to break in to the ML field as a career (previously I wanted to pursue Physics/Academia). I was hoping some people working in the field here could offer some advice as to what it takes to get there. 

&amp;#x200B;

I've been learning about ML in my limited free time during the semester and I'm hoping to take advantage of the summer to help make myself marketable in around a year or so (I'm graduating next Spring). I've been mostly reading about theory through things like ISLR and various resources for Neural Networks/DL. I have a reading list covering several facets of the ML field. I applied and interviewed for a few ML/DS internships but didn't get any of them due to lack of practical experience. My plan is to work on a few projects to build a portfolio and fill that hole in my resume. One of my main concerns is figuring out what kind of problems/projects to tackle, any advice in this area is welcome. I'd really like to work on projects outside of tutorials but I can't seem to think of anything to work on.

&amp;#x200B;

I guess what I'm asking is: What can I do over the course of the next year to make me a strong candidate for ML/DS jobs? I feel like there's this gap between me and getting a job in the field and I'm not sure how to cross it. Any advice would be welcome. Thanks In Advance.",7,5
1118,2019-5-18,2019,5,18,1,bptmyz,Using multiple labels to train an XGboost model,https://www.reddit.com/r/MachineLearning/comments/bptmyz/using_multiple_labels_to_train_an_xgboost_model/,Jelrae_Farsun,1558111601,[removed],0,1
1119,2019-5-18,2019,5,18,2,bpug74,"Naive Bayes modeling, in R using `Klar`",https://www.reddit.com/r/MachineLearning/comments/bpug74/naive_bayes_modeling_in_r_using_klar/,mockrun,1558115617,[removed],0,1
1120,2019-5-18,2019,5,18,2,bpui7i,Computer vision using deep learning,https://www.reddit.com/r/MachineLearning/comments/bpui7i/computer_vision_using_deep_learning/,pk_kumar_17,1558115913,[removed],0,1
1121,2019-5-18,2019,5,18,3,bpusni,PDF to Audio File With WaveNet or AWS Polly Question,https://www.reddit.com/r/MachineLearning/comments/bpusni/pdf_to_audio_file_with_wavenet_or_aws_polly/,Lee_Pop,1558117386,[removed],0,1
1122,2019-5-18,2019,5,18,3,bpuu94,[R] [1905.06723] Deep Compressed Sensing from DeepMind,https://www.reddit.com/r/MachineLearning/comments/bpuu94/r_190506723_deep_compressed_sensing_from_deepmind/,bobchennan,1558117605,,1,9
1123,2019-5-18,2019,5,18,4,bpvdq4,Generating Cards Against Humanity Cards,https://www.reddit.com/r/MachineLearning/comments/bpvdq4/generating_cards_against_humanity_cards/,ablacklama,1558120289,[removed],0,1
1124,2019-5-18,2019,5,18,4,bpvif8,[P] Generating Cards Against Humanity Cards,https://www.reddit.com/r/MachineLearning/comments/bpvif8/p_generating_cards_against_humanity_cards/,ablacklama,1558120948,"Just finished attempting to generate Cards Against Humanity cards with an character level LSTM. Here's the post I wrote up for anyone who's interested. Naturally there's some NSFW language in it.

https://medium.com/@ablacklama/cards-against-humanity-card-generation-slightly-nsfw-a9c132d88345

Would love any feedback and suggestions on ways to generate better results.",19,10
1125,2019-5-18,2019,5,18,4,bpvmk7,A natural adversarial example by youtube?,https://www.reddit.com/r/MachineLearning/comments/bpvmk7/a_natural_adversarial_example_by_youtube/,kikacaty,1558121507,,1,1
1126,2019-5-18,2019,5,18,5,bpw6y4,This AI-generated Joe Rogan fake has to be heard to be believed,https://www.reddit.com/r/MachineLearning/comments/bpw6y4/this_aigenerated_joe_rogan_fake_has_to_be_heard/,alvisanovari,1558124376,,1,1
1127,2019-5-18,2019,5,18,6,bpwu3y,[D] Please Stop Explaining Black Box Models for High Stakes Decisions,https://www.reddit.com/r/MachineLearning/comments/bpwu3y/d_please_stop_explaining_black_box_models_for/,iidealized,1558127647,[removed],0,1
1128,2019-5-18,2019,5,18,6,bpwy57,Perfect discriminator in GAN - bad results.,https://www.reddit.com/r/MachineLearning/comments/bpwy57/perfect_discriminator_in_gan_bad_results/,Heringsalat100,1558128250,"Hello!

I am playing around with image generating GANs (image as input + output for the generator, so my ""noise"" is in fact a bunch of images) with Keras+Tensorflow in the last few weeks. My problem: I get horrible results!

Vocab for clarification:
generator = generator only
discriminator = discriminator only (binary crossentropy as loss)
gan = generator + discriminator with discriminator output (binary crossentropy as loss)

So what is happening?

After a couple of epochs my discriminator prediction accuracy for generated (label 0), real (1) and (added for better quality) some distorted (0) images is converging towards 100%.

After the discriminator training (per batch) I begin training my gan with real image inputs (labeled as truth, 1), the trainability of the discriminator (generator) is locked (unlocked) for this process. The gan gives me another 100% accuracy after dozens of epochs!

But: The resulting generator is giving me very bad results! Even when I try to pretrain the generator in a way that gives me relatively good results *before* I go through the first GAN epoch (= discriminator training + gan training with locked discriminator), the outputs become worse even directly after the first GAN epoch.

What I don't understand is: When the discriminator is able to understand the difference between generated and real images with 100% accuracy in nearly every case and when the gan shows 100% accuracy at the same time how is the generator able to perform that bad? I am aware of problems like mode collapse but I don't understand how the generator can fool the discriminator within the gan training process but at the same time this discriminator seems to be perfectly able to classify (generated+distorted) = fake and real images.

Do you have any general suggestions regarding this problem? Or do you need explicit code?",0,1
1129,2019-5-18,2019,5,18,6,bpwzj8,SOTA of speaker recognition,https://www.reddit.com/r/MachineLearning/comments/bpwzj8/sota_of_speaker_recognition/,Omriyos,1558128441,[removed],1,1
1130,2019-5-18,2019,5,18,6,bpx8ws,Conversion from COCO keypoints to Open pose ?,https://www.reddit.com/r/MachineLearning/comments/bpx8ws/conversion_from_coco_keypoints_to_open_pose/,theSamuraiMonk17,1558129832,,0,1
1131,2019-5-18,2019,5,18,7,bpxfee,[D] Control Regularization for Reduced Variance Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/bpxfee/d_control_regularization_for_reduced_variance/,mellow54,1558130772,,1,1
1132,2019-5-18,2019,5,18,7,bpxgzz,Tiling Neural network,https://www.reddit.com/r/MachineLearning/comments/bpxgzz/tiling_neural_network/,tettusud,1558131007,"Tiling and neural network. Anyone knows how to solve tiling ,using neural network? For example arrange tiles within a rectangle, keeping aspect ratio intact?",0,1
1133,2019-5-18,2019,5,18,7,bpxqsi,"With NEAT, would a UUID instead of an integer be an acceptable data type for a global innovation number?",https://www.reddit.com/r/MachineLearning/comments/bpxqsi/with_neat_would_a_uuid_instead_of_an_integer_be/,StTheo,1558132489,[removed],0,1
1134,2019-5-18,2019,5,18,7,bpxu7f,[D] Can humans run faster on 2 legs or 4 legs?,https://www.reddit.com/r/MachineLearning/comments/bpxu7f/d_can_humans_run_faster_on_2_legs_or_4_legs/,happyhammy,1558133026,"[This video](https://www.youtube.com/watch?v=F3h0AkNNP70) of a Japanese man running 100m in 15.71 seconds made me wonder if it's possible for humans to run faster on 4 limbs.

[This paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4928019/?fbclid=IwAR1KW_ADRx4U2YW7OnrruQhqCDgcigVy0ysHgIoHHCWlwQo7n8ou7OZ-1PU) has some insights into how one would achieve this biomechanically and prediction for when the 4 limb time would surpass 2 limb.

Do you think it is possible to use RL to learn the fastest human gait as per something like [this](https://www.youtube.com/watch?v=gn4nRCC9TwQ)? And how strong a persons upper body would have to be in order to be faster on 4 limbs?",11,4
1135,2019-5-18,2019,5,18,8,bpyafn,How do I prove recruiters that I am capable of working in Machine learning field ?,https://www.reddit.com/r/MachineLearning/comments/bpyafn/how_do_i_prove_recruiters_that_i_am_capable_of/,dext26,1558135590,[removed],1,1
1136,2019-5-18,2019,5,18,8,bpyja4,What is a more optimal strategy to initialise weights and hyper-parameters than randomly initialising them?,https://www.reddit.com/r/MachineLearning/comments/bpyja4/what_is_a_more_optimal_strategy_to_initialise/,mr-minion,1558137029,[removed],0,1
1137,2019-5-18,2019,5,18,9,bpz3qq,Got my first ML Engineer job. Super excited! Any suggestion of what to brush up before the start day?,https://www.reddit.com/r/MachineLearning/comments/bpz3qq/got_my_first_ml_engineer_job_super_excited_any/,person01234567,1558140573,[removed],0,1
1138,2019-5-18,2019,5,18,11,bpzqv6,Tensorflow Object Detection w/ custom DensetNet pre-trained classifier and checkpoint HOW TO ?,https://www.reddit.com/r/MachineLearning/comments/bpzqv6/tensorflow_object_detection_w_custom_densetnet/,thusinh1969,1558144829,[removed],0,1
1139,2019-5-18,2019,5,18,11,bpzwjx,[D] What are some unsolved problems in decentralized machine learning?,https://www.reddit.com/r/MachineLearning/comments/bpzwjx/d_what_are_some_unsolved_problems_in/,xdx24,1558145880,"I am interested in federated learning but found the ideas I came up have mostly been studied already such as backdoor in fl, and I don't think I can just come up with a better solution right now, so I wonder what are some remaining unsolved problems in the area? Thank you!",6,5
1140,2019-5-18,2019,5,18,11,bpzwkc,Why does Error/Weight derivative produce delta weights?,https://www.reddit.com/r/MachineLearning/comments/bpzwkc/why_does_errorweight_derivative_produce_delta/,TheDigitalRhino,1558145882,[removed],0,1
1141,2019-5-18,2019,5,18,11,bq00e6,Simple Mac screen recording method,https://www.reddit.com/r/MachineLearning/comments/bq00e6/simple_mac_screen_recording_method/,TrimMyHedges,1558146607,,0,1
1142,2019-5-18,2019,5,18,14,bq07hi,New in DeOldify: Smooth Colorization of Video! Courtesy of one weird trick- NoGAN,https://www.reddit.com/r/MachineLearning/comments/bq07hi/new_in_deoldify_smooth_colorization_of_video/,MyMomSaysImHot,1558158379,[removed],0,1
1143,2019-5-18,2019,5,18,15,bq0k18,[R] How To Backdoor Federated Learning,https://www.reddit.com/r/MachineLearning/comments/bq0k18/r_how_to_backdoor_federated_learning/,downtownslim,1558159397,,2,0
1144,2019-5-18,2019,5,18,15,bq0kp7,"If machine learning can be used to automate stock market trading, why aren't all data scientists billionaires by now?",https://www.reddit.com/r/MachineLearning/comments/bq0kp7/if_machine_learning_can_be_used_to_automate_stock/,lucidmath,1558159464,[removed],0,1
1145,2019-5-18,2019,5,18,16,bq16wg,What is convex optimisation,https://www.reddit.com/r/MachineLearning/comments/bq16wg/what_is_convex_optimisation/,funrep,1558162818,"Hello,

&amp;#x200B;

I have to decide whether to take ""Applied Machine learning"" or ""Convex Optimisation"" as part of my degree. I don't really know what convex optimisation is about or how it is relevant for machine learning. From wikipedia the concept seems pretty straight forward, and I can see how it relates to training a model (optimise it) to a certain goal, but I still can't make a qualified decision if it's something worth learning. Could anyone lean in and explain how/when optimisations is used in ML?

Thanks.",0,1
1146,2019-5-18,2019,5,18,16,bq1gji,[D] Time series forecasting with TensorFlow.js,https://www.reddit.com/r/MachineLearning/comments/bq1gji/d_time_series_forecasting_with_tensorflowjs/,ErmJustSaying,1558165385,,0,1
1147,2019-5-18,2019,5,18,17,bq1owo,"Absolute cheapest way to train with 10,000 cores?",https://www.reddit.com/r/MachineLearning/comments/bq1owo/absolute_cheapest_way_to_train_with_10000_cores/,ptlil,1558168340,"I'm trying to test some evolutionary algorithm ideas I have, and I'm looking several different compute options. For what I want to do, I'll need upwards of 2,000 cores, possibly as much as 10k. (I'm fine with preemption)

&amp;#x200B;

It looks like AWS has spot instances of t3.nano or t3a.nano for $0.0016 per hour, each having 2 vCPUs and half a gig of memory. This works out to $0.0008 per vCPU hour or $8 an hour for 10k cores.

Is that the best deal I'll get from Amazon? I already have some AWS credits so I wouldn't mind using them more.

&amp;#x200B;

I've also read that it's cheaper to buy your own hardware if you want to run experiments around the clock for months. I have potential funding for this, and I've looked into buying a bunch of old Xeon Phi nodes. A lot of 24 of them (over 1000 cores altogether) went for $1200 on eBay last month.

&amp;#x200B;

What does r/MachineLearning think about this? I just need CPU compute, but I'm sure others have thought this through before. Thanks!",3,1
1148,2019-5-18,2019,5,18,18,bq1u91,[D] Learning from simulated data ?,https://www.reddit.com/r/MachineLearning/comments/bq1u91/d_learning_from_simulated_data/,ApathiaDeus,1558170224," Hi,

So I'm working on this virtual sensing DL project (basically, I try to estimate (regression) the outcome of an expensive sensor from other cheaper sensors), and I have a limited amount of real-life labelled time series data (Acquiring the labelled data is expensive because of the sensor in itself).

The physics behind what I'm trying to estimate can be pretty accurately modeled with complex equations that doesn't have any close form solutions. Those physical models also require me to feed as input some parameters that I wish it didn't need because I would ultimately want something that is able to generalized in all type of situations.

So far, I have some results using DL (MLP and RNN variants), but nothing crazy. I think I am limited by the fact that the data I'm provided is extremely small.

Now come into play a simulation some of my colleagues have been working on. According to them, the simulation can reproduce with a high degree of accuracy the behavior of the physical model. The simulation take into account some of the parameters I mentioned above, which is a good news since it can generates diverse data with respect to those parameters and maybe help the NN to better generalize.

Unfortunately so far, I have extremely poor results when training on the generated data and testing on real life data. I believe the simulated data is too ""perfect"" to the point where the network can easily fit the smooth training data, but this does not help the noisy real life data. I don't know if this would be consider over-fitting (My validation loss on real life data, does decrease first before it goes up, which makes me believed the network is learning some stuff), but even so, I tried regularization, up to simplifying the network to one hidden layer and a dozens of neurons, and some dropout; it didn't improve anything.

Now appart from adding noise to the simulated data (which I will try next), do any of you guys know how I can better exploit the simulated data ? I still have some labelled data from real life and and also a ton of unlabeled data (data from the cheap sensors). I'm wondering if there is an equivalent of GAN for non images data (time series of physical quantities for instance). From what I understand, they can ""turn"" simulated data into something closer to reality, but doesn't seem to apply outside of image data.

Any ideas ? I'm even open to the semi supervised side, since I have a lot of unlabeled data.",3,1
1149,2019-5-18,2019,5,18,18,bq1uul,16-Year-Old Engineer Works to Improve Spinal Surgery Using Machine Learning and Computer Vision,https://www.reddit.com/r/MachineLearning/comments/bq1uul/16yearold_engineer_works_to_improve_spinal/,smbale,1558170430,,0,1
1150,2019-5-18,2019,5,18,18,bq25jv,Interesting Use Cases of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bq25jv/interesting_use_cases_of_machine_learning/,guptavirat,1558172915,,0,1
1151,2019-5-18,2019,5,18,19,bq2ihj,[P] Easy to use Indian Digits Dataset via CMATERdb in NumPy format,https://www.reddit.com/r/MachineLearning/comments/bq2ihj/p_easy_to_use_indian_digits_dataset_via_cmaterdb/,op_prabhuomkar,1558174671,,0,1
1152,2019-5-18,2019,5,18,19,bq2nrr,[P] Indian Digits Dataset via CMATERdb in easy to use NumPy format,https://www.reddit.com/r/MachineLearning/comments/bq2nrr/p_indian_digits_dataset_via_cmaterdb_in_easy_to/,op_prabhuomkar,1558175995,"CMATERdb is the pattern recognition database repository created at the 'Center for Microprocessor Applications for Training Education and Research' (CMATER) research laboratory, Jadavpur University, Kolkata 700032, INDIA. This database is free for all non-commercial uses. 

**Dataset Description:**

* **CMATERdb 3.1.1: Handwritten Bangla numeral database** is a balanced dataset of total 6000 Bangla numerals (32x32 RGB coloured, 6000 images), each having 600 images per classs(per digit).
* **CMATERdb 3.2.1: Handwritten Devanagari numeral database** is a balanced dataset of total 3000 Bangla numerals (32x32 RGB coloured, 3000 images), each having 300 images per classs(per digit).
* **CMATERdb 3.4.1: Handwritten Telugu numeral database** is a balanced dataset of total 3000 Bangla numerals (32x32 RGB coloured, 3000 images), each having 300 images per classs(per digit).

**Links:**

[GitHub Repository](https://github.com/prabhuomkar/CMATERdb)

[Download Link](https://github.com/prabhuomkar/CMATERdb#get-the-data)

Please acknowledge CMATER explicitly, whenever you use this database for academic and research purposes.",6,15
1153,2019-5-18,2019,5,18,19,bq2rxv,Machine learning and the physical sciences,https://www.reddit.com/r/MachineLearning/comments/bq2rxv/machine_learning_and_the_physical_sciences/,Xaron,1558176996,,4,77
1154,2019-5-18,2019,5,18,19,bq2sgk,Number of capsules in the Primary Capsule Layer of Capsule networks,https://www.reddit.com/r/MachineLearning/comments/bq2sgk/number_of_capsules_in_the_primary_capsule_layer/,PyWarrior,1558177131,[removed],0,1
1155,2019-5-18,2019,5,18,20,bq2x8k,What is a more optimal strategy to initialise weights and hyper-parameters than randomly initialising them?,https://www.reddit.com/r/MachineLearning/comments/bq2x8k/what_is_a_more_optimal_strategy_to_initialise/,mr-minion,1558178201,,0,1
1156,2019-5-18,2019,5,18,20,bq36y8,Neuroevolution,https://www.reddit.com/r/MachineLearning/comments/bq36y8/neuroevolution/,indi0508,1558180399,,1,1
1157,2019-5-18,2019,5,18,21,bq3cjr,The Pattern Machine (new Deep Learning series by Art of the Problem feat Geoff Hinton),https://www.reddit.com/r/MachineLearning/comments/bq3cjr/the_pattern_machine_new_deep_learning_series_by/,puppers90,1558181562,,0,1
1158,2019-5-18,2019,5,18,21,bq3pzb,Neural nets typically contain smaller subnetworks that can often learn faster - MIT,https://www.reddit.com/r/MachineLearning/comments/bq3pzb/neural_nets_typically_contain_smaller_subnetworks/,Angelich100,1558184231,[removed],0,1
1159,2019-5-18,2019,5,18,22,bq3rzs,What is the best learning path and resources to learn Artificial intelligence?,https://www.reddit.com/r/MachineLearning/comments/bq3rzs/what_is_the_best_learning_path_and_resources_to/,masterRJ2404,1558184604,[removed],0,1
1160,2019-5-19,2019,5,19,0,bq572f,"Harness the Power of Machine Learning in R for Data/Text Mining, &amp; Natural Language Processing with Practical Examples",https://www.reddit.com/r/MachineLearning/comments/bq572f/harness_the_power_of_machine_learning_in_r_for/,udemudailydeal,1558193368,,0,1
1161,2019-5-19,2019,5,19,0,bq58nm,[D] AutoML/Neural Architecture Search has a giant CO2 footprint,https://www.reddit.com/r/MachineLearning/comments/bq58nm/d_automlneural_architecture_search_has_a_giant/,MoneyScore,1558193621,"Looks like AutoML has by far the biggest carbon footprint of most training processes (394,863 CO2e).

&amp;#x200B;

Emma Strubell:

&gt;Are you interested in deep learning for NLP but also concerned about the CO2 footprint of training? You should be! Excited to share our work ""Energy and Policy Considerations for Deep Learning in NLP"" at @ACL2019\_Italy! With @ananya\_\_g and @andrewmccallum. Preprint coming soon.

[https://twitter.com/strubell/status/1129408199478661120/photo/1](https://twitter.com/strubell/status/1129408199478661120/photo/1)

[https://pbs.twimg.com/media/D6x2xN6XsAUV3gU.png](https://pbs.twimg.com/media/D6x2xN6XsAUV3gU.png)",117,158
1162,2019-5-19,2019,5,19,0,bq5az2,Autopilot written in Tensorflow for Self Driving Cars,https://www.reddit.com/r/MachineLearning/comments/bq5az2/autopilot_written_in_tensorflow_for_self_driving/,LeonDeHill,1558193993,,0,1
1163,2019-5-19,2019,5,19,0,bq5gks,"Want to program, partner, collaborate, participate with other developers? Join us oj our discord - T3CH",https://www.reddit.com/r/MachineLearning/comments/bq5gks/want_to_program_partner_collaborate_participate/,TrustedMercury,1558194867,,0,1
1164,2019-5-19,2019,5,19,1,bq5l3t,How does the negative sign come up in the chain rule?,https://www.reddit.com/r/MachineLearning/comments/bq5l3t/how_does_the_negative_sign_come_up_in_the_chain/,TheDigitalRhino,1558195537,[removed],0,1
1165,2019-5-19,2019,5,19,1,bq5mxz,Can you reference me source of a machine learning algorithms for cybersecurity?,https://www.reddit.com/r/MachineLearning/comments/bq5mxz/can_you_reference_me_source_of_a_machine_learning/,caydinlari,1558195822,[removed],0,1
1166,2019-5-19,2019,5,19,1,bq5ore,Please give me some suggestions regarding machine learning certifications or projects to be done to prove my knowledge in the same field.,https://www.reddit.com/r/MachineLearning/comments/bq5ore/please_give_me_some_suggestions_regarding_machine/,dext26,1558196122,[removed],0,1
1167,2019-5-19,2019,5,19,1,bq5ps6,A machine learning,https://www.reddit.com/r/MachineLearning/comments/bq5ps6/a_machine_learning/,Spiderweb14,1558196273,,1,1
1168,2019-5-19,2019,5,19,1,bq5v0r,[D]Serious help needed. I have changed everything with this network and still have this problem. This is a new issue that I've never had before.,https://www.reddit.com/r/MachineLearning/comments/bq5v0r/dserious_help_needed_i_have_changed_everything/,khutagaming,1558197076,,0,1
1169,2019-5-19,2019,5,19,2,bq6ipi,Want some advice about my future in Deep Learning for Computer Vision,https://www.reddit.com/r/MachineLearning/comments/bq6ipi/want_some_advice_about_my_future_in_deep_learning/,aritipandu_san,1558200737,[removed],0,1
1170,2019-5-19,2019,5,19,2,bq6mj4,Good and Easy Max Pooling Implementation for beginners?,https://www.reddit.com/r/MachineLearning/comments/bq6mj4/good_and_easy_max_pooling_implementation_for/,Jandevries101,1558201313,[removed],0,1
1171,2019-5-19,2019,5,19,2,bq6rym,Can you help me with source of a machine learning algorithms for cybersecurity?,https://www.reddit.com/r/MachineLearning/comments/bq6rym/can_you_help_me_with_source_of_a_machine_learning/,caypermicay,1558202124,[removed],0,1
1172,2019-5-19,2019,5,19,3,bq6wo9,Autopilot written in Tensorflow for Self Driving Cars,https://www.reddit.com/r/MachineLearning/comments/bq6wo9/autopilot_written_in_tensorflow_for_self_driving/,LeonDeHill,1558202824,,0,1
1173,2019-5-19,2019,5,19,3,bq793j,[arXiv] What can linguistics and deep learning contribute to each other?,https://www.reddit.com/r/MachineLearning/comments/bq793j/arxiv_what_can_linguistics_and_deep_learning/,digikar,1558204735,,6,27
1174,2019-5-19,2019,5,19,3,bq7g4s,What is the state of Bayesian Deep Learning in May 2019?,https://www.reddit.com/r/MachineLearning/comments/bq7g4s/what_is_the_state_of_bayesian_deep_learning_in/,misery_buzinezz,1558205807,[removed],0,1
1175,2019-5-19,2019,5,19,4,bq7mqu,Which is best machine learning classification algorithm in cybersecurity?,https://www.reddit.com/r/MachineLearning/comments/bq7mqu/which_is_best_machine_learning_classification/,caydinlari,1558206822,[removed],0,1
1176,2019-5-19,2019,5,19,4,bq7of0,[1903.07288] Effects of padding on LSTMs and CNNs,https://www.reddit.com/r/MachineLearning/comments/bq7of0/190307288_effects_of_padding_on_lstms_and_cnns/,recklesstycoon,1558207085,,11,2
1177,2019-5-19,2019,5,19,4,bq7qje,Building Production Machine Learning Systems,https://www.reddit.com/r/MachineLearning/comments/bq7qje/building_production_machine_learning_systems/,aks4321,1558207428,,0,1
1178,2019-5-19,2019,5,19,4,bq7tm8,[R] Mixture of Experts beats Deep Learning for Survival Analysis !,https://www.reddit.com/r/MachineLearning/comments/bq7tm8/r_mixture_of_experts_beats_deep_learning_for/,throwaway5192018a,1558207911,[removed],0,1
1179,2019-5-19,2019,5,19,5,bq8d4d,What do you use to fine-tune the hyper parameters of a model in Pytorch ?,https://www.reddit.com/r/MachineLearning/comments/bq8d4d/what_do_you_use_to_finetune_the_hyper_parameters/,textMinier,1558210932,"Which library do you use to find the best  hyper parameters for a pytorch model ? Do you fine-tune all of them(lr, batch size, hidden dim ...) at the same time ? Does it make sense to first finetune the learning rate, batch size and afterwards the hidden dim and the other parameters for the model architecture(because if I fine tune all of the at the same time it takes too long) ?",0,1
1180,2019-5-19,2019,5,19,5,bq8gji,[P] New in DeOldify: Smooth Colorization of Video! Courtesy of one weird trick- NoGAN,https://www.reddit.com/r/MachineLearning/comments/bq8gji/p_new_in_deoldify_smooth_colorization_of_video/,MyMomSaysImHot,1558211491,"Hello again! I just realized after a few weeks of sitting on this that I didn't tell you guys about something really cool I've been working on! Maybe I should do that. 

A few months ago I posted about [DeOldify-](https://github.com/jantic/DeOldify) my pet project for colorizing and restoring old photos. Well now that includes videos as well! Here's a demo I showed at Facebook's F8 conference:

&amp;#x200B;

![video](9tu80qji41z21)

And here's the talk at F8: [https://www.facebook.com/FacebookforDevelopers/videos/340167420019712/](https://www.facebook.com/FacebookforDevelopers/videos/340167420019712/) 

And here's the article I wrote with Jeremy Howard of FastAI and Uri Manor of Salk Institute: [https://www.fast.ai/2019/05/03/decrappify/](https://www.fast.ai/2019/05/03/decrappify/) 

Anyway, the gist is that we (FastAI and I) developed this one weird trick called NoGAN to achieve this. That basically consists of this (slide from F8): 

&amp;#x200B;

![img](midd5z9v41z21)

 

The pretraining is using basic perceptual loss (or ""feature loss"") for the generator.  This gets you the benefits of GANs, without the problems, basically. Hence, smooth video!

The progression of training looks like this (sweet spot is at 1.4%, then it goes too far from there and gets weird with the orange skin):

&amp;#x200B;

![video](34o1egby41z21)

Anyway, that's the gist. You can read more in the links above (readme for github project also has good details on NoGAN). Oh by the way, NoGAN also works on super resolution, and I suspect for most image to image tasks as well as perhaps even non-image tasks. 

&amp;#x200B;

![img](z0tlr0zz41z21)",38,187
1181,2019-5-19,2019,5,19,6,bq8ubv,[D]Is here ongoing cardiology challenges?,https://www.reddit.com/r/MachineLearning/comments/bq8ubv/dis_here_ongoing_cardiology_challenges/,hadaev,1558213629,,2,1
1182,2019-5-19,2019,5,19,7,bq9qaf,Data Science Survey,https://www.reddit.com/r/MachineLearning/comments/bq9qaf/data_science_survey/,tattoomeyes,1558218574,[removed],0,1
1183,2019-5-19,2019,5,19,7,bq9tpx,"Is there any open source codes briefly about lip-reading with python (keras,tensorflow,ML) all the parts like lip detect, tracking, taking frames from videos, training dataset, test phase, prediction... Is there any one who is expert on this?",https://www.reddit.com/r/MachineLearning/comments/bq9tpx/is_there_any_open_source_codes_briefly_about/,Alperkenn,1558219179,[removed],0,1
1184,2019-5-19,2019,5,19,9,bqaoyn,Lagrange multipliers with pictures (using Python Imaging Library) and code (sympy).,https://www.reddit.com/r/MachineLearning/comments/bqaoyn/lagrange_multipliers_with_pictures_using_python/,rohitpandey576,1558224675,,0,1
1185,2019-5-19,2019,5,19,9,bqawko,[R] Fine-tune BERT for Extractive Summarization,https://www.reddit.com/r/MachineLearning/comments/bqawko/r_finetune_bert_for_extractive_summarization/,BatmantoshReturns,1558225974,,3,17
1186,2019-5-19,2019,5,19,9,bqaz34,Variance after scaling and summing: One of the most useful facts from statistics,https://www.reddit.com/r/MachineLearning/comments/bqaz34/variance_after_scaling_and_summing_one_of_the/,tonyblair81,1558226415,,0,1
1187,2019-5-19,2019,5,19,10,bqbefb,Neural nets typically contain little subnetworks that can commonly learn quickly - MIT,https://www.reddit.com/r/MachineLearning/comments/bqbefb/neural_nets_typically_contain_little_subnetworks/,Angelich100,1558229248,[removed],0,1
1188,2019-5-19,2019,5,19,13,bqd1zf,How should I start with machine learning so as to build a Content Based Image Retrieval system?,https://www.reddit.com/r/MachineLearning/comments/bqd1zf/how_should_i_start_with_machine_learning_so_as_to/,madmax_01,1558241075,[removed],0,1
1189,2019-5-19,2019,5,19,15,bqdgwe,Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks [Worth Reading],https://www.reddit.com/r/MachineLearning/comments/bqdgwe/unpaired_imagetoimage_translation_using/,ai-lover,1558245602,,0,1
1190,2019-5-19,2019,5,19,15,bqdl8x,Creating Large Networks,https://www.reddit.com/r/MachineLearning/comments/bqdl8x/creating_large_networks/,Geeks_sid,1558246562,[removed],0,1
1191,2019-5-19,2019,5,19,15,bqdt0q,5-10 minute survey on defining A.I.,https://www.reddit.com/r/MachineLearning/comments/bqdt0q/510_minute_survey_on_defining_ai/,critplat,1558248373,[removed],0,1
1192,2019-5-19,2019,5,19,19,bqfn5n,How Universal sentence encoder works?,https://www.reddit.com/r/MachineLearning/comments/bqfn5n/how_universal_sentence_encoder_works/,NeedSomeMedicine,1558263390,[removed],0,1
1193,2019-5-19,2019,5,19,20,bqfqb3,XGBoost Mathematics Explained,https://www.reddit.com/r/MachineLearning/comments/bqfqb3/xgboost_mathematics_explained/,dim_leve,1558264129,[removed],0,1
1194,2019-5-19,2019,5,19,20,bqfta2,Neural Network's predictive performance - studies have shown good results but not seen in practice,https://www.reddit.com/r/MachineLearning/comments/bqfta2/neural_networks_predictive_performance_studies/,sqatas,1558264804,[removed],0,1
1195,2019-5-19,2019,5,19,20,bqg305,[Discussion] What are the top AI labs in the world?,https://www.reddit.com/r/MachineLearning/comments/bqg305/discussion_what_are_the_top_ai_labs_in_the_world/,BanLeCun,1558266845,What are the top 5 or 10 places for AI or AGI research in 2019? How difficult is it to join them starting as a regular software engineer with basic ML knowledge?,29,0
1196,2019-5-19,2019,5,19,21,bqg5wg,I guess Bing's chatbot uses Reddit as part of its training set...,https://www.reddit.com/r/MachineLearning/comments/bqg5wg/i_guess_bings_chatbot_uses_reddit_as_part_of_its/,noselace,1558267419,,1,1
1197,2019-5-19,2019,5,19,23,bqhe33,Has anyone here tried the course offered by Tokyo Data Science?,https://www.reddit.com/r/MachineLearning/comments/bqhe33/has_anyone_here_tried_the_course_offered_by_tokyo/,themad95,1558275253,[removed],0,1
1198,2019-5-20,2019,5,20,0,bqi19x,[R] A Variational Inequality Perspective on GANs,https://www.reddit.com/r/MachineLearning/comments/bqi19x/r_a_variational_inequality_perspective_on_gans/,tensorflower,1558278856,,17,111
1199,2019-5-20,2019,5,20,0,bqib9r,Deep nets which scale computation as input size increases,https://www.reddit.com/r/MachineLearning/comments/bqib9r/deep_nets_which_scale_computation_as_input_size/,datasci314159,1558280372,[removed],0,1
1200,2019-5-20,2019,5,20,0,bqihks,Time-series Denoising State-of-the-art Architectures,https://www.reddit.com/r/MachineLearning/comments/bqihks/timeseries_denoising_stateoftheart_architectures/,arjundupa,1558281329,"I've been trying to find state-of-the-art architectures for time-series denoising. Because there doesn't seem to be a standard baseline for this task, I'm finding it difficult to compare various architectures.

So far, I've come across:

1. Asymmetric GAN ([https://link.springer.com/content/pdf/10.1007%2F978-3-319-93040-4\_23.pdf](https://link.springer.com/content/pdf/10.1007%2F978-3-319-93040-4_23.pdf))
2. Deep Feature Loss ([https://arxiv.org/pdf/1806.10522.pdf](https://arxiv.org/pdf/1806.10522.pdf))
3. Wave-U-Net ([https://arxiv.org/pdf/1811.11307v1.pdf](https://arxiv.org/pdf/1811.11307v1.pdf)), or simply WaveNet ([https://arxiv.org/pdf/1706.07162.pdf](https://arxiv.org/pdf/1706.07162.pdf))
4. Denoising Auto-encoders

&amp;#x200B;

What state-of-the-art architectures am I missing?

How can I go about finding which architecture is truly state-of-the-art for time-series denoising?",0,1
1201,2019-5-20,2019,5,20,2,bqjfiy,[P] Open sourced tool for CNN visualization,https://www.reddit.com/r/MachineLearning/comments/bqjfiy/p_open_sourced_tool_for_cnn_visualization/,kralant,1558286356,,0,1
1202,2019-5-20,2019,5,20,3,bqk6ta,The Hitchhikers Guide to Feature Extraction,https://www.reddit.com/r/MachineLearning/comments/bqk6ta/the_hitchhikers_guide_to_feature_extraction/,_quanttrader_,1558290315,,0,1
1203,2019-5-20,2019,5,20,4,bqkq6q,Releasing MiraPy v0.1.0: A Python Package of Deep Learning in Astronomy! ,https://www.reddit.com/r/MachineLearning/comments/bqkq6q/releasing_mirapy_v010_a_python_package_of_deep/,TheCuddlyWhiskers,1558293037,[removed],0,1
1204,2019-5-20,2019,5,20,4,bqkty8,What's the best resource to learn the maths behind SVM?,https://www.reddit.com/r/MachineLearning/comments/bqkty8/whats_the_best_resource_to_learn_the_maths_behind/,masterRJ2404,1558293553,[removed],0,1
1205,2019-5-20,2019,5,20,5,bqlb3v,[D] Machine Learning - WAYR (What Are You Reading) - Week 63,https://www.reddit.com/r/MachineLearning/comments/bqlb3v/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,1558296004,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|
|----|-----|-----|-----|-----|-----|-----|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)||

Most upvoted papers two weeks ago:

/u/gatapia: [Unsupervised learning by competing hidden units](https://www.pnas.org/content/116/16/7723)

/u/Consistent_Size: [""A tutorial on subspace clustering (pdf)""](https://pdfs.semanticscholar.org/2c2c/6609400dcabafd420e18e68a11e994a34e75.pdf)

/u/vlanins: [Poverty Mapping Using Convolutional Neural Networks Trained on High and Medium Resolution Satellite Images, With an Application in Mexico](https://arxiv.org/abs/1711.06323v1)

Besides that, there are no rules, have fun.",95,81
1206,2019-5-20,2019,5,20,5,bqlzfq,[D] Successfully Deploying GPT-2 in a Flask Web App,https://www.reddit.com/r/MachineLearning/comments/bqlzfq/d_successfully_deploying_gpt2_in_a_flask_web_app/,Extension_Juggernaut,1558299537,"Has anyone been able to successfully, smoothly deploy GPT-2 with a Flask web app? If you have, would you mind pointing me in the direction of resources to help accomplish this myself?

I've seen a few instances of GPT-2 deployed with UI allowing users to generate text on their own, but the performance has been much shoddier and more erratic than what I'm capable of producing on my own system (sampling is either straight up bugged, or spits out fairly repetitive mush).

I'm in the later stages of building a platform that presents educational materials relating to language models, and some of machine learning's role in NLP tasks, from understanding to generation, hitting upon super entry-level concepts, recent architectural innovation, and some social/societal ramifications of the state of the art, etc. All in all, it's aimed at relative beginners and curious enthusiasts, but also includes plenty of resources for more intermediate and advanced users.

I'd really like to incorporate an interactive space for my users to experiment with GPT-2 firsthand. Website and everything is already built out, in Flask, just sort of struggling with some devops and deploy stuff.",10,38
1207,2019-5-20,2019,5,20,6,bqmoxf,[N] Bunch of Machine Learning and Deep Learning books are still being discounted,https://www.reddit.com/r/MachineLearning/comments/bqmoxf/n_bunch_of_machine_learning_and_deep_learning/,HildaDavidson,1558303179,"There's bunch of books and almost all of them are on Machine Learning and Deep Learning. From reviews I can see that they are good.

&amp;#x200B;

Check them out here: [https://twitter.com/AssortMaoo/status/1128005408407347201](https://twitter.com/AssortMaoo/status/1128005408407347201)",0,3
1208,2019-5-20,2019,5,20,7,bqmv72,Afternoon of Data in Chicato on June 12,https://www.reddit.com/r/MachineLearning/comments/bqmv72/afternoon_of_data_in_chicato_on_june_12/,AnnaOnTheWeb,1558304057,,0,1
1209,2019-5-20,2019,5,20,7,bqmz85,Afternoon of Data in Chicago on June 12,https://www.reddit.com/r/MachineLearning/comments/bqmz85/afternoon_of_data_in_chicago_on_june_12/,AnnaOnTheWeb,1558304656,,0,1
1210,2019-5-20,2019,5,20,8,bqnxmn,"[D] How would you define A.I.? (5-10minute, mobile-friendly academic survey)",https://www.reddit.com/r/MachineLearning/comments/bqnxmn/d_how_would_you_define_ai_510minute/,critplat,1558310056,,0,0
1211,2019-5-20,2019,5,20,10,bqom7u,My First ML Project: Gender Classifier,https://www.reddit.com/r/MachineLearning/comments/bqom7u/my_first_ml_project_gender_classifier/,jimsu2012,1558314092,[removed],0,1
1212,2019-5-20,2019,5,20,11,bqp983,OneHotEncoder use,https://www.reddit.com/r/MachineLearning/comments/bqp983/onehotencoder_use/,Ole_72444,1558318063,[removed],0,1
1213,2019-5-20,2019,5,20,13,bqqki1,Game of Thrones remake: only AI can save us from the horrible ending now,https://www.reddit.com/r/MachineLearning/comments/bqqki1/game_of_thrones_remake_only_ai_can_save_us_from/,keyvan-m-sadeghi,1558324896,[removed],0,1
1214,2019-5-20,2019,5,20,13,bqqzpo,Google's 43 Rules of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bqqzpo/googles_43_rules_of_machine_learning/,aiforworld2,1558327447,[removed],0,1
1215,2019-5-20,2019,5,20,14,bqr865,Excellent Guide about Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/bqr865/excellent_guide_about_recurrent_neural_networks/,MammothCost,1558328935,,0,0
1216,2019-5-20,2019,5,20,14,bqrefx,How we might protect ourselves from malicious AI,https://www.reddit.com/r/MachineLearning/comments/bqrefx/how_we_might_protect_ourselves_from_malicious_ai/,bluesamcitizen2,1558330095,[removed],1,1
1217,2019-5-20,2019,5,20,15,bqs1uf,RepGN:Object Detection with Relational Proposal Graph Network,https://www.reddit.com/r/MachineLearning/comments/bqs1uf/repgnobject_detection_with_relational_proposal/,diggerdu,1558334573,[removed],0,1
1218,2019-5-20,2019,5,20,16,bqsf83,Automated Warehouse AS/RS for Telescopic Forks,https://www.reddit.com/r/MachineLearning/comments/bqsf83/automated_warehouse_asrs_for_telescopic_forks/,lhd121,1558337286,[removed],0,1
1219,2019-5-20,2019,5,20,16,bqsfgy,Count intersections on image with neural network?,https://www.reddit.com/r/MachineLearning/comments/bqsfgy/count_intersections_on_image_with_neural_network/,jthat92,1558337343,,0,1
1220,2019-5-20,2019,5,20,16,bqshz9,Google Coral products are all available for world wide guys,https://www.reddit.com/r/MachineLearning/comments/bqshz9/google_coral_products_are_all_available_for_world/,makereven,1558337872,,0,1
1221,2019-5-20,2019,5,20,16,bqslpj,"Is it possible to port forward a PC with a home router in a residential place, such that I can SSH to my PC for Deep Learning?",https://www.reddit.com/r/MachineLearning/comments/bqslpj/is_it_possible_to_port_forward_a_pc_with_a_home/,valcroft,1558338698,[removed],0,1
1222,2019-5-20,2019,5,20,17,bqswmi,"Gathering NLP solutions using Tensorflow, https://github.com/huseinzol05/NLP-Models-Tensorflow",https://www.reddit.com/r/MachineLearning/comments/bqswmi/gathering_nlp_solutions_using_tensorflow/,huseinzol05,1558341164,[removed],0,1
1223,2019-5-20,2019,5,20,18,bqt6qk,Machine Learning Applications: 7 Major Fields of Use,https://www.reddit.com/r/MachineLearning/comments/bqt6qk/machine_learning_applications_7_major_fields_of/,LemmyChildish,1558343446,,0,1
1224,2019-5-20,2019,5,20,18,bqtce2,Industrial Boilers Market Insights and Global Industry Forecast to 2023,https://www.reddit.com/r/MachineLearning/comments/bqtce2/industrial_boilers_market_insights_and_global/,jadhavni3,1558344703,[removed],1,1
1225,2019-5-20,2019,5,20,18,bqthg5,Industrial Noise Control System Market Insights and Global Industry Forecast to 2023,https://www.reddit.com/r/MachineLearning/comments/bqthg5/industrial_noise_control_system_market_insights/,jadhavni3,1558345798,[removed],1,1
1226,2019-5-20,2019,5,20,18,bqtj0h,The Importance of Data Analysis Cannot be Overstated!,https://www.reddit.com/r/MachineLearning/comments/bqtj0h/the_importance_of_data_analysis_cannot_be/,ElegantMicroWebIndia,1558346131,,0,1
1227,2019-5-20,2019,5,20,18,bqtj1q,Confused about the right activation function for classification problems,https://www.reddit.com/r/MachineLearning/comments/bqtj1q/confused_about_the_right_activation_function_for/,Heapwalker,1558346137,[removed],0,1
1228,2019-5-20,2019,5,20,19,bqtld7,Online machine training pipeline,https://www.reddit.com/r/MachineLearning/comments/bqtld7/online_machine_training_pipeline/,pantherofficial404,1558346597,[removed],0,1
1229,2019-5-20,2019,5,20,19,bqtpfc,Submarine Sensors Market Insights and Global Industry Forecast to 2024,https://www.reddit.com/r/MachineLearning/comments/bqtpfc/submarine_sensors_market_insights_and_global/,jadhavni3,1558347439,[removed],1,1
1230,2019-5-20,2019,5,20,19,bqtv6m,Antibody Drug Conjugate Market Insights and Global Industry Forecast to 2023,https://www.reddit.com/r/MachineLearning/comments/bqtv6m/antibody_drug_conjugate_market_insights_and/,jadhavni3,1558348613,,1,1
1231,2019-5-20,2019,5,20,20,bqu406,Generating music : state of the art and what to do from here ?,https://www.reddit.com/r/MachineLearning/comments/bqu406/generating_music_state_of_the_art_and_what_to_do/,MachingLearnin,1558350309,[removed],0,1
1232,2019-5-20,2019,5,20,20,bqu616,Patent Claims generated via GPT-2,https://www.reddit.com/r/MachineLearning/comments/bqu616/patent_claims_generated_via_gpt2/,js_lee,1558350663,[removed],0,1
1233,2019-5-20,2019,5,20,21,bquupb,People + AI Guidebook - Designing human-centered AI products in Google,https://www.reddit.com/r/MachineLearning/comments/bquupb/people_ai_guidebook_designing_humancentered_ai/,PiotrekAG,1558354895,,0,1
1234,2019-5-20,2019,5,20,21,bquzs7,[D] Patent Claims generated via GPT-2 (345M model),https://www.reddit.com/r/MachineLearning/comments/bquzs7/d_patent_claims_generated_via_gpt2_345m_model/,js_lee,1558355773,,0,1
1235,2019-5-20,2019,5,20,21,bqv29v,Nick Burch at Berlin Buzzwords,https://www.reddit.com/r/MachineLearning/comments/bqv29v/nick_burch_at_berlin_buzzwords/,newthinkingevents,1558356187,"Hey everyone :=) Nick Burch will be talking about building an AI/ML powered text search engine at Berlin Buzzwords this year! Check him out and get your tickets: [https://berlinbuzzwords.de/19/session/building-aiml-powered-text-search-system](https://berlinbuzzwords.de/19/session/building-aiml-powered-text-search-system)

https://i.redd.it/e1mg87653dz21.jpg",1,1
1236,2019-5-20,2019,5,20,21,bqv47w,Patent Claims generated via GPT-2 (345M model),https://www.reddit.com/r/MachineLearning/comments/bqv47w/patent_claims_generated_via_gpt2_345m_model/,js_lee,1558356496,,0,1
1237,2019-5-20,2019,5,20,22,bqvk1f,Real-life machine learning cases with technical details,https://www.reddit.com/r/MachineLearning/comments/bqvk1f/reallife_machine_learning_cases_with_technical/,Tyrion_Lannister7,1558359012,[removed],0,1
1238,2019-5-20,2019,5,20,23,bqw39l,Step-by-step Backpropagation explanation,https://www.reddit.com/r/MachineLearning/comments/bqw39l/stepbystep_backpropagation_explanation/,Anas_N_Almasri,1558361834,[removed],0,1
1239,2019-5-20,2019,5,20,23,bqw3rr,[D] How computers learn using tree based methods - An interactive playlist!,https://www.reddit.com/r/MachineLearning/comments/bqw3rr/d_how_computers_learn_using_tree_based_methods_an/,terabapt,1558361907,,3,1
1240,2019-5-20,2019,5,20,23,bqwbdk,New or/and nice applications on e-commerce and payment solutions using ML?,https://www.reddit.com/r/MachineLearning/comments/bqwbdk/new_orand_nice_applications_on_ecommerce_and/,bayesianwannabe1,1558362958,[removed],0,1
1241,2019-5-20,2019,5,20,23,bqwinw,A step-by-step explanation of Backpropagation,https://www.reddit.com/r/MachineLearning/comments/bqwinw/a_stepbystep_explanation_of_backpropagation/,Anas_N_Almasri,1558363921,"Training a Neural Network is usually associated with the term Backpropagation, which is highly vague to most people getting into Deep Learning. Heck, most people in the industry dont even know how it worksthey just know it does!

&amp;#x200B;

Here is my detailed explanation of the process of this miraculous algorithm.

[https://towardsdatascience.com/how-does-back-propagation-in-artificial-neural-networks-work-c7cad873ea7](https://towardsdatascience.com/how-does-back-propagation-in-artificial-neural-networks-work-c7cad873ea7)",0,1
1242,2019-5-21,2019,5,21,0,bqwplp,Im new,https://www.reddit.com/r/MachineLearning/comments/bqwplp/im_new/,chiborevo,1558364796,[removed],0,1
1243,2019-5-21,2019,5,21,0,bqxd33,PyTorch vs Keras Initialisation Differences,https://www.reddit.com/r/MachineLearning/comments/bqxd33/pytorch_vs_keras_initialisation_differences/,strojax,1558367847,[removed],0,1
1244,2019-5-21,2019,5,21,1,bqxjcf,New Azure Machine Learning updates simplify and accelerate the ML lifecycle,https://www.reddit.com/r/MachineLearning/comments/bqxjcf/new_azure_machine_learning_updates_simplify_and/,brminnick,1558368658,,0,2
1245,2019-5-21,2019,5,21,1,bqxopp,Integer Discrete Flows and Lossless Compression,https://www.reddit.com/r/MachineLearning/comments/bqxopp/integer_discrete_flows_and_lossless_compression/,for_all_eps,1558369354,,7,55
1246,2019-5-21,2019,5,21,1,bqxza6,"Google Releases Open Image V5, Launches Open Image Challenge",https://www.reddit.com/r/MachineLearning/comments/bqxza6/google_releases_open_image_v5_launches_open_image/,Yuqing7,1558370717,,0,1
1247,2019-5-21,2019,5,21,2,bqy8a1,[R][1807.00459] How To Backdoor Federated Learning,https://www.reddit.com/r/MachineLearning/comments/bqy8a1/r180700459_how_to_backdoor_federated_learning/,downtownslim,1558371879,,8,42
1248,2019-5-21,2019,5,21,2,bqyjjv,[P] Simple Content-based Image Retrieval Using Inception-Res-Net V2,https://www.reddit.com/r/MachineLearning/comments/bqyjjv/p_simple_contentbased_image_retrieval_using/,aa1000,1558373300,,1,1
1249,2019-5-21,2019,5,21,2,bqyk8z,[P] OnionBot (This Headline Does Not Exist),https://www.reddit.com/r/MachineLearning/comments/bqyk8z/p_onionbot_this_headline_does_not_exist/,eukaryote31,1558373384,"Another GPT-2 based project. Trained on headlines from r/theonion and r/nottheonion. 

  
Try it out: https://onionbot.eukaryote.dev/",87,156
1250,2019-5-21,2019,5,21,2,bqyw65,New data scientists should focus on _____ given the large investment in automated machine learning / AI solutions and tools.,https://www.reddit.com/r/MachineLearning/comments/bqyw65/new_data_scientists_should_focus_on_given_the/,edmguru,1558374933,"Curious to see what this community thinks new data scientists should focus on given that there is a large investment in academic and industry for automated ML/AI solutions. I'm currently at the point where I'm still studying and practicing classical ML algorithms, but once I'm ready to move on should I focus on data engineering/collecting data/? Data visualization? I'm speaking more to a technical standpoint vs. philosophical such as ""generate value"" .",0,1
1251,2019-5-21,2019,5,21,3,bqz72h,Machine Learning Algorithms and the Future of Prediction with David Weinberger,https://www.reddit.com/r/MachineLearning/comments/bqz72h/machine_learning_algorithms_and_the_future_of/,cpclos,1558376357,,0,1
1252,2019-5-21,2019,5,21,3,bqzghf,"Thoughts on ""cookie cutter data science"" ML project structure framework? Any other suggestions?",https://www.reddit.com/r/MachineLearning/comments/bqzghf/thoughts_on_cookie_cutter_data_science_ml_project/,bandalorian,1558377611,[removed],0,1
1253,2019-5-21,2019,5,21,4,bqzt9v,[Research} Microsoft Introduces an Open-source and Cross-platform Machine Learning Framework,https://www.reddit.com/r/MachineLearning/comments/bqzt9v/research_microsoft_introduces_an_opensource_and/,cdossman,1558379269,[removed],0,1
1254,2019-5-21,2019,5,21,4,br02q8,Deterministic and non-deterministic search,https://www.reddit.com/r/MachineLearning/comments/br02q8/deterministic_and_nondeterministic_search/,Python119,1558380468,[removed],0,1
1255,2019-5-21,2019,5,21,5,br0hul,Possibly nonsensical thoughts about learning and an alternative to backpropagation,https://www.reddit.com/r/MachineLearning/comments/br0hul/possibly_nonsensical_thoughts_about_learning_and/,misstheground12,1558382407,"I am definitely a machine learning novice, and what I'm about to write out might just be nonsensical gibberish, but was thinking about this last night and wanted to share. Very aware that any of my assumptions/conclusions may be totally flawed or just too vague to mean anything, so please let me know if they are! And maybe what I'm writing out is already known via neuroscience to be nonsense.


So I think researchers don't believe our brains do backprop, but don't know what mechanism they do use.


I was thinking about this with regards to learning how to position and manipulate an arm in order to catch a baseball.


The idea was that certain actions are already hard-coded as neural networks, and the weights within each of those networks don't change. These are discrete actions like moving the arm up/down/left/right/forward/back .


These actions, along with coefficients for duration and intensity for each action, are strung together into a task, with multiple actions being possible at the same time, but also there is an order given to show which actions (or group of actions) should follow other actions (or group of actions).


So the actions are assembled into a task, the task is performed, and once the task is performed, a learning pathway is invoked to assess the level of success, based on the sensory data received during the task being performed.


The coefficients of intensity/duration for each action are modified based on sensory input. And modified by a little, a medium amount, or a lot. So instead of all of the weights being available for modification, just a few parameters are able to be modified, with most of the connections between neurons (the ones within each action) having already been hard-coded. It's also not modification by an exact amount, but more a minor, a medium, or a major adjustment.


The task is then run again, and the level of success is assessed again, resulting in further modification of the action coefficients.


I was also thinking about how when you first learn a task, you generally are moving between actions in a very shaky, haphazard manner. Think about when someone first learns to walk. So possibly at first the series of actions in each task are being executed separately, but once the task has been completed successfully a number of times, each action in the task then activates the next action in the task in a cascade, without outside activation being required (fire together, wire together).


This could correspond to the neocortex having access to the initiation of each action, and sending the signal to initiate each action in the sequence required. Eventually each action grows direct connections from one action initiation to the next, so that the actions happen smoothly, right after one another. I was wondering if this could be the kind of learning that happens while we sleep.


I also had more thoughts on the brain building models of the world -  is there a model-building task that piggybacks on all other tasks? And those models are used to estimate which actions to use and initial duration/intensity coefficients? How does the brain know what models to invoke for what tasks? How does the brain know what a minor, medium, or major adjustment is?


Also, how do the neural networks of those discrete actions get created without using backprop? Are the neural networks built in from birth? Are they created within the brain from one neuron up? Then how would that work, if those networks aren't using backprop?


Anyway, this feels a little like ""On Intelligence"" but possibly less thought out, but like I said, wanted to share, if only to be told it is nonsense. Thanks for any responses.",0,1
1256,2019-5-21,2019,5,21,5,br0j0a,[D] Documenting conceptions of AI in a short survey,https://www.reddit.com/r/MachineLearning/comments/br0j0a/d_documenting_conceptions_of_ai_in_a_short_survey/,critplat,1558382539,"Our research group at the University of Washington is running a survey on defining artificial intelligence (A.I.) and classifying A.I. technologies, as well as identifying relevant social issues, that we hope will inform some ongoing policy discussions.   


Whether you could use a fun short break from your NeurIPS crunch, or you are just interested in this topic, we'd love to get your thoughts!  


Please participate and share!   


[https://ischooluw.co1.qualtrics.com/jfe/form/SV\_eWDnUFB1cyKjDrn?Q\_CHL=reddit](https://ischooluw.co1.qualtrics.com/jfe/form/SV_eWDnUFB1cyKjDrn?Q_CHL=reddit)  


Feedback and replies with thoughts are also welcome. :)",0,0
1257,2019-5-21,2019,5,21,5,br1581,Variable Selection Techniques,https://www.reddit.com/r/MachineLearning/comments/br1581/variable_selection_techniques/,akkatips,1558385428,"I have been using extreme gradient boosting for some modelling and have heard it can be somewhat robust to nosiy, insignificant variables. However, I have noticed that taking away some variables helps accuracy, the way in which I currently do this is through backward-stepwise where I take away variables with the least importance. Is there any other better ways of approaching this issue? I also use a mixture of categorical and numerical variables, and as the categorical variables are one-hot encoded their variable importance is usually very low per category. Is there a way I can assess these variables' importance in a better way?",0,1
1258,2019-5-21,2019,5,21,6,br1qyl,[D] GAN to generate images of planets,https://www.reddit.com/r/MachineLearning/comments/br1qyl/d_gan_to_generate_images_of_planets/,MasterScrat,1558388364,"Hi all, 

I have seen some very nice demo where you can draw the outline of a scenery, cat or handbag and it creates a (more or less) realistic picture following your shapes.

How much work would it be to generate pictures of planets? you select which parts are water, forests, mountains, craters and it generates something like this: https://images.app.goo.gl/76jbnxVDg8YFE64g6

How much technical and computational work would this involve? I know very little about GANs, but Im familiar with python (I study reinforcement learning) and have access to beefy GPUs. 

Actually ideally Id want to generate 3D models of the planets, but as a first approximation a 2D image would be enough.",4,8
1259,2019-5-21,2019,5,21,6,br1tpa,Intel i9 vs Threadripper: AVX512/MKL/Memory controler vs Higher core count &amp; number of PCI-E lanes,https://www.reddit.com/r/MachineLearning/comments/br1tpa/intel_i9_vs_threadripper_avx512mklmemory/,Girsadium,1558388762,"I'm building a deep learning workstation with 3 or 4 RTX 2080 Ti GPUs.

I need to choose between a Threadripper WX2990 or WX2970 CPU and an Intel i9 9960 or less CPU. I don't know which should be best, I can see the following advantages to each platform.

&amp;#x200B;

Threadripper:

\- 64 PCI Express lanes

\- 24 or 32 cores for the models I'd buy, so at least 6 cores / GPU

\- Cheaper for more cores

&amp;#x200B;

Intel:

\- AVX512 support

\- Higher per-core performance

\- Faster memory controller, which gives better performance in memory-intensive applications

\- Good synergy with the MKL

&amp;#x200B;

The rest of the config will be: 64Gb of 3000MHz DDR4 RAM (system builder doesn't offer 128Gb at 3000MHz), 1Tb NVMe SSD, 2x4Tb 7200RPM hard drives in RAID 1.

&amp;#x200B;

I know GPUs are the most important etc. I want to know specifically if the better vector instructions and memory controller on the Intel CPUs outweight the higher number of PCI-E lanes and higher core count on the Threadrippers.

&amp;#x200B;

Any help appreciated :)",0,1
1260,2019-5-21,2019,5,21,7,br2mnd,Classifying user based on typing signature.,https://www.reddit.com/r/MachineLearning/comments/br2mnd/classifying_user_based_on_typing_signature/,ebolafever,1558392949,"Just an idea I had today.  Everyone has idiosyncratic ways that they type from ""high"" level to ""low"" level: language use (vocab, phrases and idioms), misspellings, typos, speed and particular letter combinations that they hit more quickly than other people.It seems like you could, ignoring the semantic stuff for now, key log a thousand people (ethically) and get patterns of people's typing behavior and discriminate a user based on their typing signature. I tend to leave my finger on my right hand longer on the right shift key so I often end up with ERrors like that &lt;- when typing. 

&amp;#x200B;

This approach has been used already:  [https://link.springer.com/chapter/10.1007/978-981-10-8180-4\_4](https://link.springer.com/chapter/10.1007/978-981-10-8180-4_4)  

&amp;#x200B;

But perhaps it could be applied to things like detecting unknown users native language based on common strings like ""the"" or ""-ing"" which native English users may type extra fast through practice as opposed to a native Chinese speaker typing in English.  Perhaps you could create an algorithm that is able to discriminate country of origin / language of the person trying to access a controlled system or have some cybersecurity application. 

&amp;#x200B;

Thoughts?",0,1
1261,2019-5-21,2019,5,21,8,br2thp,[Discussion] What tools do you pay for as a CTO or lead of an R&amp;D project,https://www.reddit.com/r/MachineLearning/comments/br2thp/discussion_what_tools_do_you_pay_for_as_a_cto_or/,himanshuragtah1,1558393961,"\[Discussion\] What tools do you **pay** for as a CTO or lead of an R&amp;D project (academic or corporate)

\-ResearchGate, LinkedIn (membership tier) or any other site for hiring more researchers for your team?

\-Github membership (for private repos)?

\-CB insights to get insights about the industry?

\-Consultants to find the current landscape of a specific application? Who in the industry is working on what?

\-IP broker, to buy and sell patents for your team?

&amp;#x200B;

Feel free to suggest any other critical tool paid for by research leads/project managers?",0,2
1262,2019-5-21,2019,5,21,8,br34lm,[Discussion] What tools or softwares do you pay for as someone in machine learning,https://www.reddit.com/r/MachineLearning/comments/br34lm/discussion_what_tools_or_softwares_do_you_pay_for/,himanshuragtah1,1558395702,"\[Discussion\] What tools do you **pay** for as part of your work (academic or corporate)?

\-are you a researcher, CTO, project lead? etc.

&amp;#x200B;

\-Github membership (for private repos)?

\-CB insights to get insights about the industry?

\-Consultants to find the current landscape of a specific application? Who in the industry is working on what?

\-exploring what's out there? IP landscape, licensing IP and tools?

\-ResearchGate, LinkedIn (membership tier) or any other site for hiring or collaboration?

&amp;#x200B;

Are there any other tools or software that you pay for or subscribe to that are useful for machine learning?",31,19
1263,2019-5-21,2019,5,21,9,br3cx0,Google Diagnostic AI Aims to Boost Lung Cancer Survival Rates,https://www.reddit.com/r/MachineLearning/comments/br3cx0/google_diagnostic_ai_aims_to_boost_lung_cancer/,Yuqing7,1558397035,,0,1
1264,2019-5-21,2019,5,21,9,br3p7q,Classifying user based on typing signature. [Discussion],https://www.reddit.com/r/MachineLearning/comments/br3p7q/classifying_user_based_on_typing_signature/,ebolafever,1558399040,"Just an idea I had today. Everyone has idiosyncratic ways that they type from ""high"" level to ""low"" level: language use (vocab, phrases and idioms), misspellings, typos, speed and particular letter combinations that they hit more quickly than other people.It seems like you could, ignoring the semantic stuff for now, key log a thousand people (ethically) and get patterns of people's typing behavior and discriminate a user based on their typing signature. I tend to leave my finger on my right hand longer on the right shift key so I often end up with ERrors like that &lt;- when typing.

This approach has been used already: [https://link.springer.com/chapter/10.1007/978-981-10-8180-4\_4](https://link.springer.com/chapter/10.1007/978-981-10-8180-4_4)

But perhaps it could be applied to things like detecting unknown users native language based on common strings like ""the"" or ""-ing"" which native English users may type extra fast through practice as opposed to a native Chinese speaker typing in English. Perhaps you could create an algorithm that is able to discriminate country of origin / language of the person trying to access a controlled system or have some cybersecurity application.

Thoughts?",7,7
1265,2019-5-21,2019,5,21,9,br3vh4,"[D] Besides the huge Tensorflow facebook groups, are there any other facebook groups worth joining for Machine Learning?",https://www.reddit.com/r/MachineLearning/comments/br3vh4/d_besides_the_huge_tensorflow_facebook_groups_are/,RelevantMarketing,1558400080,,5,0
1266,2019-5-21,2019,5,21,11,br4qnd,[Research] NVIDIA Unveils New Reinforcement Learning Research at ICRA 2019,https://www.reddit.com/r/MachineLearning/comments/br4qnd/research_nvidia_unveils_new_reinforcement/,[deleted],1558405176,[deleted],0,1
1267,2019-5-21,2019,5,21,11,br4siv,What is the current state-of-the-art in voice conversion?,https://www.reddit.com/r/MachineLearning/comments/br4siv/what_is_the_current_stateoftheart_in_voice/,m1234d1,1558405493,[removed],0,1
1268,2019-5-21,2019,5,21,11,br4vow,[Research] NVIDIA Unveils New Reinforcement Learning Research at ICRA 2019,https://www.reddit.com/r/MachineLearning/comments/br4vow/research_nvidia_unveils_new_reinforcement/,himanshuragtah1,1558405999,,1,2
1269,2019-5-21,2019,5,21,12,br5jh6,Top 5 Skills Needed to Become a Machine Learning Engineer | Analytics Insight,https://www.reddit.com/r/MachineLearning/comments/br5jh6/top_5_skills_needed_to_become_a_machine_learning/,analyticsinsight,1558410137,,0,1
1270,2019-5-21,2019,5,21,14,br69zb,[N] Here's How Machine Learning may help capture Fusion Energy on Earth,https://www.reddit.com/r/MachineLearning/comments/br69zb/n_heres_how_machine_learning_may_help_capture/,vadhavaniyafaijan,1558415047,,0,1
1271,2019-5-21,2019,5,21,14,br6fse,Signal to Distortion Ratio as Loss Function,https://www.reddit.com/r/MachineLearning/comments/br6fse/signal_to_distortion_ratio_as_loss_function/,arjundupa,1558416199,[removed],0,1
1272,2019-5-21,2019,5,21,14,br6ob9,Wanting an opinion from an experienced Data Scientist,https://www.reddit.com/r/MachineLearning/comments/br6ob9/wanting_an_opinion_from_an_experienced_data/,connorb14,1558418011," 

Hey guys! I am currently an iOS developer by trade with my undergrad in Computer Science. I am wanting to spend my free time getting started on my journey to gaining experience in Machine Learning ( specifically deep learning ). I am fascinated by so many things and I think the main project I would like to focus on is creating a chat bot using a neural network. ( I hear it is good to have something you want to work on to help you learn. Especially in a field this difficult and ever changing ).

My question is. With so many different suggestions on take this course, take that course, spend x amount of weeks refreshing calculus or stats, I was wondering what a simplified approach would be for someone who just wants to get their hands dirty from the beginning.

I don't learn well in a classroom environment and have taught myself everything I know by trial and error and just brute forcing a problem until I arrive at a solution. With all of this said, does anyone have any recommendations on how I should go about my learning path? Are there any books that are good to read and then maybe just dive in to a specific topic?

TL;DR I learn by brute forcing a problem until I find a solution, not in a classroom environment like the many classes that are recommended. What is your suggestion on how I get started. Assume I have no knowledge of machine learning, much less deep learning.

Thanks! I hope to hear from experienced data scientists!",0,1
1273,2019-5-21,2019,5,21,15,br6r4q,Magic the gathering,https://www.reddit.com/r/MachineLearning/comments/br6r4q/magic_the_gathering/,phael008,1558418590,"Hello everyone, I want to work on a Magic the Gathering MultiLingual translation system (ultimately to generate text for cards that don't exist in some languages, you can see here the available languages that are released [https://mtg.gamepedia.com/Printed\_languages](https://mtg.gamepedia.com/Printed_languages)). I have only been able to find the texts of cards in English, does anyone know where I can get text for other languages? Thank you!",0,1
1274,2019-5-21,2019,5,21,15,br6wjw,Machine Condition Monitoring Equipment Market Projected to Have a Stable Growth for the Next Few Years,https://www.reddit.com/r/MachineLearning/comments/br6wjw/machine_condition_monitoring_equipment_market/,deepakkumarwts,1558419738,,0,1
1275,2019-5-21,2019,5,21,15,br7143,Overview of the different approaches to putting Machine Learning (ML) models in production,https://www.reddit.com/r/MachineLearning/comments/br7143/overview_of_the_different_approaches_to_putting/,linkerzx,1558420720,,0,1
1276,2019-5-21,2019,5,21,16,br77cl,x Programatically understand Gaussian Processes: Part 1,https://www.reddit.com/r/MachineLearning/comments/br77cl/x_programatically_understand_gaussian_processes/,nipun_batra,1558422113,,0,1
1277,2019-5-21,2019,5,21,16,br7ap8,Timing on pytorch with GPU,https://www.reddit.com/r/MachineLearning/comments/br7ap8/timing_on_pytorch_with_gpu/,stackfrog,1558422828,[removed],0,1
1278,2019-5-21,2019,5,21,16,br7fdj,Prediction of Customer Churn with Machine Learning | DIMENSIONLESS TECHNOLOGIES PVT.LTD.,https://www.reddit.com/r/MachineLearning/comments/br7fdj/prediction_of_customer_churn_with_machine/,divya2018,1558423895,,0,1
1279,2019-5-21,2019,5,21,16,br7gts,Ghosted by workshop. Is this normal?,https://www.reddit.com/r/MachineLearning/comments/br7gts/ghosted_by_workshop_is_this_normal/,jjnaude,1558424241,[removed],0,1
1280,2019-5-21,2019,5,21,16,br7isr,"Woodworking Machines Manufacturers, Wood Working Machinery, Industrial Woodworking Machinery, Wooden Furniture Making Machines, Wood Working Machinery",https://www.reddit.com/r/MachineLearning/comments/br7isr/woodworking_machines_manufacturers_wood_working/,umaboyahmedabad,1558424702,,0,1
1281,2019-5-21,2019,5,21,16,br7ltk,Global Weather Forecasting Equipment Market Research Report 2019,https://www.reddit.com/r/MachineLearning/comments/br7ltk/global_weather_forecasting_equipment_market/,jadhavni3,1558425458,[removed],1,1
1282,2019-5-21,2019,5,21,17,br7nxy,Company Overview :: The Company,https://www.reddit.com/r/MachineLearning/comments/br7nxy/company_overview_the_company/,umaboyahmedabad,1558425949,,0,1
1283,2019-5-21,2019,5,21,17,br7xy3,Anomaly detection from D-bus messages,https://www.reddit.com/r/MachineLearning/comments/br7xy3/anomaly_detection_from_dbus_messages/,IllidanStromrage,1558428477,[removed],0,1
1284,2019-5-21,2019,5,21,17,br7y6m,[R] NVIDIA Unveils New Reinforcement Learning Research at ICRA 2019,https://www.reddit.com/r/MachineLearning/comments/br7y6m/r_nvidia_unveils_new_reinforcement_learning/,himanshuragtah1,1558428539,,1,2
1285,2019-5-21,2019,5,21,18,br80xg,[D] Ghosted by workshop. Is this normal?,https://www.reddit.com/r/MachineLearning/comments/br80xg/d_ghosted_by_workshop_is_this_normal/,jjnaude,1558429206,"Hi all.

I recently submitted a paper to a CVPR workshop for the first time since completing my postgrad studies almost 20 years ago. The deadline was first moved out by a week (so that it coincided with the deadline for the camera ready version of the paper to be submitted) and has since passed. We received neither a notification of acceptance nor of a rejection. I would honestly be fine with a rejection, but I was expecting some review feedback indicating where the perceived weaknesses of the paper might be so that I might improve upon it. Not only did I not get any feedback, but I received no communication at all. I've been worried about about possible mail glitches (checked my spam folder repeatedly) and tried contacting workshop organisers both via Microsoft CMT and directly via e-mail. No response whatsoever. From my viewpoint this is extremely unprofessional. Do I escalate this further, as I believe it reflects badly on CVPR as a whole? Or is this normal nowadays?",16,79
1286,2019-5-21,2019,5,21,18,br81q2,Unsupervised Learning on Text data,https://www.reddit.com/r/MachineLearning/comments/br81q2/unsupervised_learning_on_text_data/,mayurat22,1558429352,[removed],0,1
1287,2019-5-21,2019,5,21,18,br820a,Pur Gluejet System,https://www.reddit.com/r/MachineLearning/comments/br820a/pur_gluejet_system/,umaboyahmedabad,1558429413,,0,1
1288,2019-5-21,2019,5,21,18,br85dy,Rapid Hot Air Technology,https://www.reddit.com/r/MachineLearning/comments/br85dy/rapid_hot_air_technology/,umaboyahmedabad,1558430141,,0,1
1289,2019-5-21,2019,5,21,18,br8bvm,[D] lumin: deep learning vs. (software) freedom in Debian,https://www.reddit.com/r/MachineLearning/comments/br8bvm/d_lumin_deep_learning_vs_software_freedom_in/,radarsat1,1558431546,,0,1
1290,2019-5-21,2019,5,21,18,br8g3n,Need help with strategy for my first project!,https://www.reddit.com/r/MachineLearning/comments/br8g3n/need_help_with_strategy_for_my_first_project/,swoonz101,1558432448,[removed],0,1
1291,2019-5-21,2019,5,21,19,br8nb1,Successful Augmented Analytics Initiatives Do Not End with Implementation!,https://www.reddit.com/r/MachineLearning/comments/br8nb1/successful_augmented_analytics_initiatives_do_not/,ElegantMicroWebIndia,1558433926,,0,1
1292,2019-5-21,2019,5,21,19,br8ylp,[R] PaperRobot: Incremental Draft Generation of Scientific Ideas,https://www.reddit.com/r/MachineLearning/comments/br8ylp/r_paperrobot_incremental_draft_generation_of/,sensetime,1558436263,,4,45
1293,2019-5-21,2019,5,21,20,br979u,"Google's cunning AI linguist, Uncle Sam drills ML skills into .mil, Intel's iffy CPU claims",https://www.reddit.com/r/MachineLearning/comments/br979u/googles_cunning_ai_linguist_uncle_sam_drills_ml/,normalispurgatory,1558437910,[removed],0,1
1294,2019-5-21,2019,5,21,20,br99xh,[Project] My implementation of 7 knowledge distillation methods by Tensorflow,https://www.reddit.com/r/MachineLearning/comments/br99xh/project_my_implementation_of_7_knowledge/,sseung0703,1558438392,[removed],0,1
1295,2019-5-21,2019,5,21,21,br9olf,"Dr. Michael Mannino interviewing Executive Director of the Human Brain Project, Dr. Christian Fauteux",https://www.reddit.com/r/MachineLearning/comments/br9olf/dr_michael_mannino_interviewing_executive/,bioquarkceo,1558440961,,0,1
1296,2019-5-21,2019,5,21,21,br9y7f,[D] GPT-2 for Patents,https://www.reddit.com/r/MachineLearning/comments/br9y7f/d_gpt2_for_patents/,js_lee,1558442566,"Hi everyone,

We are fine-tuning GPT-2 for generating patent claims as a new way to innovate. The results are [available](aipatent.wordpress.com) while training, if you are interested.",16,12
1297,2019-5-21,2019,5,21,22,bra5c7,"Completed Masters, what next?",https://www.reddit.com/r/MachineLearning/comments/bra5c7/completed_masters_what_next/,maddoofus,1558443752,[removed],0,1
1298,2019-5-21,2019,5,21,23,bratda,"Build tools in machine learning projects, an overview",https://www.reddit.com/r/MachineLearning/comments/bratda/build_tools_in_machine_learning_projects_an/,atomlib_com,1558447513,,0,1
1299,2019-5-21,2019,5,21,23,brau8a,Important Python Libraries for Data Analysis #DataScience #Python #DataAnalytics #MachineLearning #ArtificialIntelligence #artificial_intelligence #BigData,https://www.reddit.com/r/MachineLearning/comments/brau8a/important_python_libraries_for_data_analysis/,mmutaher,1558447637,,0,1
1300,2019-5-21,2019,5,21,23,brb6cq,Reinforcement Learning vs Human Performance,https://www.reddit.com/r/MachineLearning/comments/brb6cq/reinforcement_learning_vs_human_performance/,galearyan12,1558449434,[removed],0,1
1301,2019-5-21,2019,5,21,23,brb805,[P] Supervisely: end-to-end web-platform for Deep Learning and Computer Vision,https://www.reddit.com/r/MachineLearning/comments/brb805/p_supervisely_endtoend_webplatform_for_deep/,tdionis,1558449671,,0,1
1302,2019-5-21,2019,5,21,23,brbadd,Will technology eventually destroy religion?,https://www.reddit.com/r/MachineLearning/comments/brbadd/will_technology_eventually_destroy_religion/,klyndonlee,1558450016,,1,2
1303,2019-5-21,2019,5,21,23,brbdyg,[P] Supervisely: end-to-end web-platform for Deep Learning and Computer Vision,https://www.reddit.com/r/MachineLearning/comments/brbdyg/p_supervisely_endtoend_webplatform_for_deep/,tdionis,1558450551,,0,1
1304,2019-5-22,2019,5,22,0,brbr3p,Launching Today: Free Wolfram Engine for Developers,https://www.reddit.com/r/MachineLearning/comments/brbr3p/launching_today_free_wolfram_engine_for_developers/,DayMaker2019,1558452371,,0,1
1305,2019-5-22,2019,5,22,0,brbva0,AI Designing AI: An AutoML Review,https://www.reddit.com/r/MachineLearning/comments/brbva0/ai_designing_ai_an_automl_review/,Yuqing7,1558452934,,0,1
1306,2019-5-22,2019,5,22,0,brbwki,BERT fine tuning to text classification,https://www.reddit.com/r/MachineLearning/comments/brbwki/bert_fine_tuning_to_text_classification/,bikanation,1558453123,[removed],0,1
1307,2019-5-22,2019,5,22,0,brc0oy,[D] Is there any point in using Bootstrapping/Arcing with NNs to improve performance?,https://www.reddit.com/r/MachineLearning/comments/brc0oy/d_is_there_any_point_in_using_bootstrappingarcing/,HecknBamBoozle,1558453693,We just covered these topics in our course and I was wondering why no one's adopting these strategies. is it because of too much work for not enough reward?,2,2
1308,2019-5-22,2019,5,22,0,brc3w3,Need Advice,https://www.reddit.com/r/MachineLearning/comments/brc3w3/need_advice/,thatguywhodiestwice,1558454152,"Hello All!

A newbie here. I am familiar with machine learning concepts now. I've spent some time online reading various posts and finishing Machine Learning course/s. But it is all theory. I have understood some of it and some I am still figuring out. However, I am not able to think how would one use these all for real time applications. Like I understood how EM or GMMs and HMMs work, but is there any place where I could learn about some practical applications of these models. Like how to begin with all these things step by step approaches.

Thank You.",0,1
1309,2019-5-22,2019,5,22,1,brc7bp,Neural Network racing cars around a track. (crosspost),https://www.reddit.com/r/MachineLearning/comments/brc7bp/neural_network_racing_cars_around_a_track/,_hockenberry,1558454637,,0,1
1310,2019-5-22,2019,5,22,1,brcbz6,[P] These Pokmon Do Not Exist (StyleGAN + RNN card generator),https://www.reddit.com/r/MachineLearning/comments/brcbz6/p_these_pokmon_do_not_exist_stylegan_rnn_card/,t04glovern,1558455256,"I had the idea after seeing a couple examples of Pokmon GANs being created and decided I'd tie the whole process together into a card generator.

&amp;#x200B;

Images were created using a couple variations of StyleGAN on images of Pokemon. Text was created using multi-layered RNNs.

&amp;#x200B;

Cards were created using a horrifying amount of ImageMagick logic...

&amp;#x200B;

**Blog Post**: [https://devopstar.com/2019/05/21/stylegan-pokemon-card-generator/](https://devopstar.com/2019/05/21/stylegan-pokemon-card-generator/)

**Website**: [https://thesepokemondonotexist.com/](https://thesepokemondonotexist.com/)

**Repo**: [https://github.com/t04glovern/stylegan-pokemon](https://github.com/t04glovern/stylegan-pokemon)

&amp;#x200B;

Many thanks to a couple great examples including:

* MichaelFriese10 - [https://twitter.com/MichaelFriese10/status/1127614400750346240](https://twitter.com/MichaelFriese10/status/1127614400750346240)
* ahsenk/stylegan-pokemon - [https://www.kaggle.com/ahsenk/stylegan-pokemon](https://www.kaggle.com/ahsenk/stylegan-pokemon)",17,24
1311,2019-5-22,2019,5,22,1,brcdxa,[N] Pythia: open-source framework for multimodal AI models,https://www.reddit.com/r/MachineLearning/comments/brcdxa/n_pythia_opensource_framework_for_multimodal_ai/,iyaja,1558455519,"Facebook just open sourced a new framework called Pythia for multitask learning in vision and language domains. Pythia is built on top of PyTorch. Here are a few resources:

* Blogpost: [https://code.fb.com/ai-research/pythia/](https://code.fb.com/ai-research/pythia/)
* GitHub: [https://github.com/facebookresearch/pythia](https://github.com/facebookresearch/pythia)

&amp;#x200B;

They've been releasing a lot of these new PyTorch based frameworks recently. First BoTorch + Ax and now this.",7,41
1312,2019-5-22,2019,5,22,1,brcilp,Book on Number,https://www.reddit.com/r/MachineLearning/comments/brcilp/book_on_number/,npminhtri,1558456152,,0,1
1313,2019-5-22,2019,5,22,1,brclfs,Stop doing iterative model development,https://www.reddit.com/r/MachineLearning/comments/brclfs/stop_doing_iterative_model_development/,yoavz,1558456544,,0,1
1314,2019-5-22,2019,5,22,1,brclma,Top 10 ML Applications for Small and Mid-Sized Businesses in 2019,https://www.reddit.com/r/MachineLearning/comments/brclma/top_10_ml_applications_for_small_and_midsized/,rahulwriter,1558456569,[removed],0,1
1315,2019-5-22,2019,5,22,1,brcmlf,[Python] Creating Double Q-Learning algorithm from Q-Learning,https://www.reddit.com/r/MachineLearning/comments/brcmlf/python_creating_double_qlearning_algorithm_from/,iagoCM,1558456695,[removed],0,1
1316,2019-5-22,2019,5,22,1,brcoa4,Encrypted Deep Learning in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/brcoa4/encrypted_deep_learning_in_tensorflow/,mortendahl,1558456930,,0,1
1317,2019-5-22,2019,5,22,2,brd43p,[Research] End to End AutoML for Tabular Data at Kaggle Days,https://www.reddit.com/r/MachineLearning/comments/brd43p/research_end_to_end_automl_for_tabular_data_at/,cdossman,1558459024," [https://medium.com/ai%C2%B3-theory-practice-business/end-to-end-automl-for-tabular-data-at-kaggle-days-c2e4b714dcac](https://medium.com/ai%C2%B3-theory-practice-business/end-to-end-automl-for-tabular-data-at-kaggle-days-c2e4b714dcac) 

 Googles AutoML efforts aim to make ML more scalable and accelerate both research and industry applications. Our initial efforts of [neural architecture search](https://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html) have enabled breakthroughs in computer vision with [NasNet](https://ai.googleblog.com/2017/11/automl-for-large-scale-image.html), and evolutionary methods such as [AmoebaNet](https://ai.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html) and hardware-aware mobile vision architecture [MNasNet](https://ai.googleblog.com/2018/08/mnasnet-towards-automating-design-of.html) further show the benefit of these learning-to-learn methods. Recently, we applied a learning-based approach to tabular data, creating a scalable end-to-end AutoML solution that meets three key criteria:  


* **Full automation**: Data and computation resources are the only inputs, while a servable TensorFlow model is the output. The whole process requires no human intervention.  

* **Extensive coverage**: The solution is applicable to the majority of arbitrary tasks in the tabular data domain.  

* **High quality:** Models generated by AutoML has comparable quality to models manually crafted by top ML experts.",17,71
1318,2019-5-22,2019,5,22,3,brdu15,Microsoft Pumps $100M into African AI Centres,https://www.reddit.com/r/MachineLearning/comments/brdu15/microsoft_pumps_100m_into_african_ai_centres/,Yuqing7,1558462556,,0,1
1319,2019-5-22,2019,5,22,3,bre2iv,Why is loss calculated as a square?,https://www.reddit.com/r/MachineLearning/comments/bre2iv/why_is_loss_calculated_as_a_square/,oghi808,1558463703,[removed],1,1
1320,2019-5-22,2019,5,22,5,brfj2x,[R] Fixup Initialization: Residual Learning Without Normalization (up to 10K layer networks w/o batch norm),https://www.reddit.com/r/MachineLearning/comments/brfj2x/r_fixup_initialization_residual_learning_without/,PeterPrinciplePro,1558470920,,24,101
1321,2019-5-22,2019,5,22,5,brfsei,[R] Differentiable Game Mechanics,https://www.reddit.com/r/MachineLearning/comments/brfsei/r_differentiable_game_mechanics/,PeterPrinciplePro,1558472186,,0,1
1322,2019-5-22,2019,5,22,6,brfyk5,"Hypothetically, if you were to combine GPT-2 with, say, 1TB of functional Python code, would it produce anything of value?",https://www.reddit.com/r/MachineLearning/comments/brfyk5/hypothetically_if_you_were_to_combine_gpt2_with/,AsherahTrial,1558473019,"I imagine at some point the generated Python code would start to make sense, but a lot more parameters would have to be defined if we want the Python code to compile into anything at all resembling a functional program.

What if you were to just give it tons of game code with pygame? Would it eventually be able to generate a functional smorgasbord of a game?

I feel like there's some potential in this approach but I also feel that I'm missing a lot of elements that could make something like this work.

Alternatively: what if you combined with GPT-2 with x86-64 GCC assembly code? Would it do better with a lower level language vs a higher level language?

Hmm...",0,1
1323,2019-5-22,2019,5,22,7,brgk5d,Facebook Open-Sources Pythia for Vision and Language Multimodal AI Models,https://www.reddit.com/r/MachineLearning/comments/brgk5d/facebook_opensources_pythia_for_vision_and/,Yuqing7,1558476123,,0,1
1324,2019-5-22,2019,5,22,7,brgmp5,GUIDE ME FOR STARTING ML,https://www.reddit.com/r/MachineLearning/comments/brgmp5/guide_me_for_starting_ml/,vignesh_rajan,1558476499,Give me tips to understand machine learning,0,1
1325,2019-5-22,2019,5,22,7,brgqqu,Independent Research Course Selection Help,https://www.reddit.com/r/MachineLearning/comments/brgqqu/independent_research_course_selection_help/,burnthisnameafteruse,1558477122,[removed],0,1
1326,2019-5-22,2019,5,22,7,brgxtl,[R] Unsupervised Continual Learning and Self-Taught Associative Memory Hierarchies,https://www.reddit.com/r/MachineLearning/comments/brgxtl/r_unsupervised_continual_learning_and_selftaught/,PeterPrinciplePro,1558478220,,6,29
1327,2019-5-22,2019,5,22,7,brh2wr,"To people who know ML, where do i even start?",https://www.reddit.com/r/MachineLearning/comments/brh2wr/to_people_who_know_ml_where_do_i_even_start/,HuntinoBino,1558479031,[removed],0,1
1328,2019-5-22,2019,5,22,7,brh4un,"[OC] Explainable AI, Sparse Representations, and Signals",https://www.reddit.com/r/MachineLearning/comments/brh4un/oc_explainable_ai_sparse_representations_and/,asrini96,1558479345,[removed],0,1
1329,2019-5-22,2019,5,22,8,brhl1g,Improving quality of old historical images.,https://www.reddit.com/r/MachineLearning/comments/brhl1g/improving_quality_of_old_historical_images/,1f1nas,1558481959,[removed],0,1
1330,2019-5-22,2019,5,22,9,bri54a,"[D] Can AIs ""like"" music or art like we do, or would that require general intelligence?",https://www.reddit.com/r/MachineLearning/comments/bri54a/d_can_ais_like_music_or_art_like_we_do_or_would/,2Punx2Furious,1558485306,"Thinking about it, I don't even know why we listen to music or look at art. To be entertained? What purpose does that achieve? Could that be useful for AIs?",5,0
1331,2019-5-22,2019,5,22,9,bri6x7,Parallel Neural Text-to-Speech,https://www.reddit.com/r/MachineLearning/comments/bri6x7/parallel_neural_texttospeech/,GoldenCrocus,1558485604,[removed],0,1
1332,2019-5-22,2019,5,22,9,bricvx,[R] [1903.08114] Exact Gaussian Processes on a Million Data Points,https://www.reddit.com/r/MachineLearning/comments/bricvx/r_190308114_exact_gaussian_processes_on_a_million/,PeterPrinciplePro,1558486630,,1,7
1333,2019-5-22,2019,5,22,9,bridca,[Research] Parallel Neural Text-to-Speech,https://www.reddit.com/r/MachineLearning/comments/bridca/research_parallel_neural_texttospeech/,GoldenCrocus,1558486707,,0,1
1334,2019-5-22,2019,5,22,10,brih9x,[R] Parallel Neural Text-to-Speech,https://www.reddit.com/r/MachineLearning/comments/brih9x/r_parallel_neural_texttospeech/,GoldenCrocus,1558487352,,6,61
1335,2019-5-22,2019,5,22,10,bripf8,I'm an immigrant and AI professional that started a nonprofit to make AI more inclusive. Asking for your help!,https://www.reddit.com/r/MachineLearning/comments/bripf8/im_an_immigrant_and_ai_professional_that_started/,haroonchoudery,1558488718,"My nonprofit organization, A.I. For Anyone, is looking for donations to help us teach students in underserved communities and ""low-skill"" professionals how to succeed in an increasingly automated world. In the past 12 months,  weve delivered AI 101 workshops to **770+ students** and served educational and informative content to the **45,000+ people** that visited our online platforms.

  
We're currently raising funds to allow us to:

 Do more student workshops  
 Host in-person meetups for professionals  
 Improve our weekly newsletter for our readers  
 Create more easy-to-digest blog posts and podcast episodes about AI

&amp;#x200B;

Read more about my story here: [https://www.linkedin.com/pulse/ai-needs-more-inclusivewere-here-help-haroon-choudery/](https://www.linkedin.com/pulse/ai-needs-more-inclusivewere-here-help-haroon-choudery/)

Consider making a tax-deductible donation here: [https://www.gofundme.com/aiforanyone](https://www.gofundme.com/aiforanyone)",0,1
1336,2019-5-22,2019,5,22,12,brjmsq,Excellent and Intuitive Approach to Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/brjmsq/excellent_and_intuitive_approach_to_convolutional/,MammothCost,1558494433,,0,1
1337,2019-5-22,2019,5,22,12,brjom0,Tutorial of Text Classification Algorithms,https://www.reddit.com/r/MachineLearning/comments/brjom0/tutorial_of_text_classification_algorithms/,kk7nc,1558494760,,0,1
1338,2019-5-22,2019,5,22,12,brk0ac,"[D] PSA: OpenCV DNN inference is fast, give it a try if you only have CPU.",https://www.reddit.com/r/MachineLearning/comments/brk0ac/d_psa_opencv_dnn_inference_is_fast_give_it_a_try/,RavlaAlvar,1558496955,"I was struggling to optimise my vision pipeline on iOS. It turns out the throughput of CoreML is really low on small neural network, namely it is not designed to do inference on large stream of data with a small network.

&amp;#x200B;

I tried OpenCV DNN out curiosity, was not expecting much, thought I hit the performance ceiling of iOS device, but it did surprised me and increase the throughput by two fold.

&amp;#x200B;

I also tried running OpenCV DNN on my development computer, and is able to observe similar performance increase. Major deep learning framework seems do not optimise much on CPU inferencing.",18,81
1339,2019-5-22,2019,5,22,13,brk3vc,AI mapping question,https://www.reddit.com/r/MachineLearning/comments/brk3vc/ai_mapping_question/,harrymanders,1558497655,[removed],0,1
1340,2019-5-22,2019,5,22,14,brkp94,role of 'Confidence loss' in YOLO....?,https://www.reddit.com/r/MachineLearning/comments/brkp94/role_of_confidence_loss_in_yolo/,GW_KIM,1558501956,[removed],0,1
1341,2019-5-22,2019,5,22,14,brks5x,New Framework Improves Performance of Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/brks5x/new_framework_improves_performance_of_deep_neural/,uchiha_indra,1558502553,,3,0
1342,2019-5-22,2019,5,22,14,brkul0,A project involving image processing using machine learning methods,https://www.reddit.com/r/MachineLearning/comments/brkul0/a_project_involving_image_processing_using/,Keeeper-1,1558503066,[removed],0,1
1343,2019-5-22,2019,5,22,14,brkvfi,Logistic and exponential Function,https://www.reddit.com/r/MachineLearning/comments/brkvfi/logistic_and_exponential_function/,PayalBhatia,1558503253,[removed],0,1
1344,2019-5-22,2019,5,22,14,brkwrt,[R] Towards real-time unsupervised monocular depth estimation on CPU,https://www.reddit.com/r/MachineLearning/comments/brkwrt/r_towards_realtime_unsupervised_monocular_depth/,_zzzzzzzzzz,1558503540, [https://arxiv.org/abs/1806.11430](https://arxiv.org/abs/1806.11430),0,6
1345,2019-5-22,2019,5,22,16,brlovx,[R] Finalist for Best Paper Award ICRA 2019,https://www.reddit.com/r/MachineLearning/comments/brlovx/r_finalist_for_best_paper_award_icra_2019/,himanshuragtah1,1558509770," [**Search-based 3D Planning and Trajectory Optimization for Safe Micro Aerial Vehicle Flight Under Sensor Visibility Constraints**](https://www.profillic.com/paper/arxiv:1903.05165)  

The problem being tackled: Safe navigation of Micro Aerial Vehicles (MAVs) requires not only obstacle-free flight paths according to a static environment map, but also the perception of and reaction to previously unknown and dynamic objects. This implies that the onboard sensors cover the current flight direction. Due to the limited payload of MAVs, full sensor coverage of the environment has to be traded off with flight time. Thus, often only a part of the environment is covered

&amp;#x200B;

 They present a combined allocentric complete planning and trajectory optimization approach taking sensor visibility constraints into account.",0,1
1346,2019-5-22,2019,5,22,16,brlt0h,Artificial Intelligence: Computer Vision and Image Recognition,https://www.reddit.com/r/MachineLearning/comments/brlt0h/artificial_intelligence_computer_vision_and_image/,quytech1,1558510776,,0,1
1347,2019-5-22,2019,5,22,16,brlt9w,Machine learning,https://www.reddit.com/r/MachineLearning/comments/brlt9w/machine_learning/,Keras_Tensorflow,1558510843,[removed],0,1
1348,2019-5-22,2019,5,22,17,brlyb9,I try to learn Machine Learning from scratch. Is there any libraries or tools I could leverage?,https://www.reddit.com/r/MachineLearning/comments/brlyb9/i_try_to_learn_machine_learning_from_scratch_is/,SeebySara,1558512087,[removed],1,1
1349,2019-5-22,2019,5,22,18,brmf3w,How to make a small dataset from a large one?,https://www.reddit.com/r/MachineLearning/comments/brmf3w/how_to_make_a_small_dataset_from_a_large_one/,dennkiesauros,1558516189,[removed],0,1
1350,2019-5-22,2019,5,22,18,brmlub,Mushroom RL library update,https://www.reddit.com/r/MachineLearning/comments/brmlub/mushroom_rl_library_update/,carloderamo,1558517677,[removed],0,1
1351,2019-5-22,2019,5,22,18,brmo4b,[Project] Flowpoints - Create and share deep learning models,https://www.reddit.com/r/MachineLearning/comments/brmo4b/project_flowpoints_create_and_share_deep_learning/,mariusbrataas,1558518188,,0,1
1352,2019-5-22,2019,5,22,18,brmsj8,[Project] Flowpoints - An intuitive way to create and share deep learning models,https://www.reddit.com/r/MachineLearning/comments/brmsj8/project_flowpoints_an_intuitive_way_to_create_and/,mariusbrataas,1558519120,"I recently created [flowpoints](flowpoints.io). This is a tool in which users can create deep learning models in a flowchart kind of manner. 

Models can easily be shared, and the corresponding python code is output to the user immediately.

Hope you like it!

[Repo](https://github.com/mariusbrataas/flowpoints_ml#readme)",0,1
1353,2019-5-22,2019,5,22,19,brn46x,[R] Machine Learning Reproducibility Challenges and DVC,https://www.reddit.com/r/MachineLearning/comments/brn46x/r_machine_learning_reproducibility_challenges_and/,thumbsdrivesmecrazy,1558521424,"When ML models need to be regularly updated in production, a host of challenges emerges. Paramount among ML reproducibility concerns are the following:

* Effectively versioning your models
* Capturing the exact steps in your data munging and feature engineering pipelines
* Dependency management (including of your data and infrastructure)
* Configuration tracking

No one tool can do it all for you - organizations using a mix of Git, Makefiles, ad hoc scripts and reference files for reproducibility. The following overview explains how DVC enters this mix offering a cleaner solution, specifically targeting data science challenges: [First Impressions of Data Science Version Control (DVC)](https://medium.com/@christopher.samiullah/first-impressions-of-data-science-version-control-dvc-fe96ab29cdda) (full tutorial)",0,15
1354,2019-5-22,2019,5,22,19,brn9n1,Handle Imbalance dataset in Logistic Regression,https://www.reddit.com/r/MachineLearning/comments/brn9n1/handle_imbalance_dataset_in_logistic_regression/,PayalBhatia,1558522440,[removed],0,1
1355,2019-5-22,2019,5,22,19,brn9py,[D] Where can I find WACV 2019 accepted papers?,https://www.reddit.com/r/MachineLearning/comments/brn9py/d_where_can_i_find_wacv_2019_accepted_papers/,cbsudux,1558522454,"Seriously, apart from the IEEE site (where the papers are locked), where can I find them?",0,0
1356,2019-5-22,2019,5,22,20,brnngo,[D] detecting anomalies in neural network data,https://www.reddit.com/r/MachineLearning/comments/brnngo/d_detecting_anomalies_in_neural_network_data/,OscarSchyns,1558524874,"Hi Reddit,

I am writing my thesis on anomalies (errors) in data and how it effects neural networks. The main goal is to detect them with no extra information such as response time. The anomalies can occur in both the x and y variables so some data is mislabelled. Since there is not a lot of literature on this topic I have a few questions.

&amp;#x200B;

1) What techniques could I use. For mislabeled data I could simply select the observations which have the worst prediction results (not using these to train the data if the number of observations is small). For anomalies in the x variables I am considering creating a neural network to detect them. This network would include random x data as observations with a separate label (so for example 3 classes, A B en C (anomalie)). Again with a low number of observations if I use data with errors to train the network would recognise such data as true so I would have to exclude all data I am checking to be an outlier from training. But that would cost a lot of training time. Maybe such a procedure would only be worthwhile if I suspect data to be an outlier (poor prediction performance, x far away for other x's). Other methods I am considering are the KNN and the isolation tree/forrest method.

&amp;#x200B;

After having a list of presumed anomalies I could train the model without them and see if performance increases. The number of outlier excluded from training can be chosen with a test set. For what number of outliers excluded is the performance best on a separate set without anomalies? or with bayesian techniques. 

&amp;#x200B;

2) What papers to use? There is not a lot of good papers on this topic so I have problems finding good papers to cite. If anybody would have some ideas it would be greatly appreciated.

&amp;#x200B;

3) What data to use? right now I am considering a classification numerical dataset (so no images or audio) which can be modelled with a neural network. But not sure where to find good data. After that I can add some noise to the data.

&amp;#x200B;

Thank you very much for your time and have a great day :)",5,3
1357,2019-5-22,2019,5,22,20,brnnvk,"[D] MLJAR vs Google Cloud AutoML Tables, comparison and review (me vs Google)",https://www.reddit.com/r/MachineLearning/comments/brnnvk/d_mljar_vs_google_cloud_automl_tables_comparison/,pp314159,1558524942,,0,1
1358,2019-5-22,2019,5,22,21,brny9y,[D] I've compared Google Cloud AutoML Tables with my AutoML python package,https://www.reddit.com/r/MachineLearning/comments/brny9y/d_ive_compared_google_cloud_automl_tables_with_my/,pp314159,1558526701,"Google has released AutoML service for structured datasets (AutoML Tables, currently in Beta). I've compared my open-source AutoML python package mljar-supervised with Google AutoML Tables on 6 datasets from openml.org. The results of Google AutoML Tables are poor (I was using 1-hour training).

Here is a link we comparison details https://mljar.com/blog/mljar-vs-google-cloud-automl-tables/",15,109
1359,2019-5-22,2019,5,22,21,bro97q,What is the best way to convert code to a CSV or other readable format for a NN?,https://www.reddit.com/r/MachineLearning/comments/bro97q/what_is_the_best_way_to_convert_code_to_a_csv_or/,user_4587,1558528569,[removed],0,1
1360,2019-5-22,2019,5,22,21,brobqi,[Semantic Segmentation] U-net 2015 paper review call,https://www.reddit.com/r/MachineLearning/comments/brobqi/semantic_segmentation_unet_2015_paper_review_call/,timscarfe,1558528984,[removed],0,1
1361,2019-5-22,2019,5,22,21,brodqb,For a General Agent can we stack neural networks for each task?,https://www.reddit.com/r/MachineLearning/comments/brodqb/for_a_general_agent_can_we_stack_neural_networks/,kalavala93,1558529323,[removed],0,1
1362,2019-5-22,2019,5,22,21,broemb,[R] Few-shot learning of talking heads,https://www.reddit.com/r/MachineLearning/comments/broemb/r_fewshot_learning_of_talking_heads/,ezakharov,1558529477,"Hello!  


I'd like to tell you about our recent paper. We've tackled the problem of a *few-shot* generation of *talking heads:* given a few (or even a single) image, train a model that is able to synthesize new images of that particular person with a new pose (viewpoint and expression).  


Our model was trained on a publicly available dataset of YouTube videos ([VoxCeleb2](http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html), 224p) and avoided mode collapse, even though the quality of images here is quite diverse. Hense, we're able to generalize well for new images with identities unseen during training (we can even run it for paintings and get reasonable results).  


The key ingredients are adversarial meta-learning, adversarial fine-tuning and adaptive instance normalization, for more details please refer to the paper, short description of our method as well as the results are in the video below.

  
**ArXiv:** [https://arxiv.org/abs/1905.08233](https://arxiv.org/abs/1905.08233)  
**Video:** [https://www.youtube.com/watch?v=p1b5aiTrGzY](https://www.youtube.com/watch?v=p1b5aiTrGzY&amp;t=35s)  


![video](onyitcyabrz21 ""One- and few-shot results produced by our model"")",12,24
1363,2019-5-22,2019,5,22,21,broey3,Deepmind quiz,https://www.reddit.com/r/MachineLearning/comments/broey3/deepmind_quiz/,openAI_iscool,1558529542,[removed],0,1
1364,2019-5-22,2019,5,22,21,brogcc,Bachelor's In Machine Learning and Artificial Intelligence.,https://www.reddit.com/r/MachineLearning/comments/brogcc/bachelors_in_machine_learning_and_artificial/,Alfen_16,1558529763,[removed],0,1
1365,2019-5-22,2019,5,22,22,bronat,[D] Deepmind RS interview process,https://www.reddit.com/r/MachineLearning/comments/bronat/d_deepmind_rs_interview_process/,openAI_iscool,1558530820,"Hi all,

Has  anyone gone through the interview process at DeepMind? I found a couple  of threads on reddit that vaguely mention it but they are mostly  several years old and not very detailed.

I  have a first quiz interview coming up with them and would really  appreciate any information on what to expect, especially since I don't  know anyone working there and am from a somewhat tangential field  myself.

PS: I realize my username is not the most appropriate :D",38,31
1366,2019-5-22,2019,5,22,22,broy5v,TensorWatch  Debugging and Visualization for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/broy5v/tensorwatch_debugging_and_visualization_for_deep/,sytelus,1558532555,[removed],1,1
1367,2019-5-22,2019,5,22,23,brp9op,[P] Simple message reply suggestion system,https://www.reddit.com/r/MachineLearning/comments/brp9op/p_simple_message_reply_suggestion_system/,longinglove,1558534314,"https://github.com/Kyubyong/msg_reply

Smart Message Reply

Have you ever seen or usedGoogle Smart Reply? It's a service that provides automatic reply suggestions for user messages. 

This is a useful application of the retrieval based chatbot. Think about it. How many times do we text a message likethx,hey, orsee you later? In this project, we build a simple message reply suggestion system.",0,0
1368,2019-5-22,2019,5,22,23,brpb6d,[P] simple auto-price prediction dataset case study,https://www.reddit.com/r/MachineLearning/comments/brpb6d/p_simple_autoprice_prediction_dataset_case_study/,rsaralaya,1558534533,,0,1
1369,2019-5-22,2019,5,22,23,brptix,[1905.08539] Universal Approximation with Deep Narrow Networks,https://www.reddit.com/r/MachineLearning/comments/brptix/190508539_universal_approximation_with_deep/,for_all_eps,1558537174,,6,5
1370,2019-5-22,2019,5,22,23,brptjw,[N]DOE Early Career Research Program funding multiple ML projects,https://www.reddit.com/r/MachineLearning/comments/brptjw/ndoe_early_career_research_program_funding/,greenprius,1558537178,"Including tracking underground contamination, automating ML algorithm development, climate modeling and more.

[https://ascr-discovery.org/2019/05/lessons-machine-learned/](https://ascr-discovery.org/2019/05/lessons-machine-learned/)",0,2
1371,2019-5-23,2019,5,23,0,brq0e8,[D] Pipeline for Recognizing Emotions from Speech,https://www.reddit.com/r/MachineLearning/comments/brq0e8/d_pipeline_for_recognizing_emotions_from_speech/,alcelentano,1558538150,"Hi everyone,

For my master thesis, my goal is to perform a speech emotion recognition task in a continuous space where I try to predict the dimensional values(valence, arousal, dominance) of emotion classes. I extract the features for each frame of the wav files in order to avoid non-stationary nature of the speech signals. Since the feature vector for each example needs to have the same shape as the input of the model, I investigate the number of frames for each audio file and found that the number of frames range between 32 and 1364. I know one solution for having equal sized feature vectors is to pad zero values until every vector have the maximum length which is 1364. So I have two questions regarding the construction of the feature vectors;

1. Does adding too many zeros(increasing the sparsity of the vector) have a negative effect on the performance of the features? If yes, can I tackle this by calculating the statistical functions(min, max, std, mean etc.) of the each feature vector? Or is there any other solution rather than padding zeros?
2. Some of the features in my case are extracted not from a frame but from the whole utterance like the duration of the utterance or the emotion class of the instance. What should be my approach for these features? Are there any downsides of considering them just like the other features with only 1 frame and to pad 1364-1 zero values to those vectors?

I'd be appreciated to hear your thoughts. Cheers,",4,3
1372,2019-5-23,2019,5,23,0,brq0l9,Action recognition- UCF101 split2 and split3 achieve 99.9%- whats wrong,https://www.reddit.com/r/MachineLearning/comments/brq0l9/action_recognition_ucf101_split2_and_split3/,albert1905,1558538173,[removed],0,1
1373,2019-5-23,2019,5,23,0,brq443,"[D] Research Job Prospects with a ""minimum"" PhD",https://www.reddit.com/r/MachineLearning/comments/brq443/d_research_job_prospects_with_a_minimum_phd/,Prince__Charming,1558538699,"Let's say I complete my PhD in machine learning with 3 first author papers in NIPS/ICML/etc. This is widely considered to be the minimum requirement for graduation in the US. Would I have any chance at research scientist positions in industry labs, or would the only doors open to me be glorified software engineering positions? My understanding is that scientist positions are reserved only for the best and that a minimally productive PhD does not put me in a good position for these jobs. Do any of you have any perspective on this? Thanks in advance",74,61
1374,2019-5-23,2019,5,23,0,brq9ii,[P] Curiosity Killed the Mario - Training a Software Agent to Explore Levels in Super Mario Bros,https://www.reddit.com/r/MachineLearning/comments/brq9ii/p_curiosity_killed_the_mario_training_a_software/,MichaelBurge,1558539460,,0,1
1375,2019-5-23,2019,5,23,0,brqg3d,"Simple Questions Thread May 22, 2019",https://www.reddit.com/r/MachineLearning/comments/brqg3d/simple_questions_thread_may_22_2019/,AutoModerator,1558540374,[removed],0,1
1376,2019-5-23,2019,5,23,0,brqg42,VAE -Dimension fuss.,https://www.reddit.com/r/MachineLearning/comments/brqg42/vae_dimension_fuss/,rosh_ray,1558540377,Is there any way to feed 512*512*4 of each image to VAE model.Total images is 83k ?,0,1
1377,2019-5-23,2019,5,23,0,brqhne,Can a model be complicated enough to overfit every validation fold during a k-fold cross-validation process?,https://www.reddit.com/r/MachineLearning/comments/brqhne/can_a_model_be_complicated_enough_to_overfit/,this_nicholas,1558540595,"During k-fold cross-validation, is it possible that a model is so sophisticated (e.g., with many hyperparameters to be grid searched) that it gives a good score on almost every validation fold? It's like the model is intricate enough to somehow *leak* out to fit the validation set every time (there are k times), essentially overfitting the whole training set (because the sum of the k validation folds is just the whole training set).

If this is possible, then I feel like it's also possible that this *best* model will eventually have a high generalization error when tested on the final test set, essentially making cross-validation useless. Did I miss anything here?",0,1
1378,2019-5-23,2019,5,23,1,brqjaz,Computer Graphics + Computer Vision = TensorFlow Graphics,https://www.reddit.com/r/MachineLearning/comments/brqjaz/computer_graphics_computer_vision_tensorflow/,Yuqing7,1558540831,,0,1
1379,2019-5-23,2019,5,23,1,brqm4q,[D] Can a model be complicated enough to overfit every validation fold during a k-fold cross-validation process?,https://www.reddit.com/r/MachineLearning/comments/brqm4q/d_can_a_model_be_complicated_enough_to_overfit/,this_nicholas,1558541224,"During k-fold cross-validation, is it possible that a model is so sophisticated (e.g., with many hyperparameters to be grid searched) that it gives a good score on almost every validation fold? It's like the model is intricate enough to somehow *leak* out to fit the validation set every time (there are k times), essentially overfitting the whole training set (because the sum of the k validation folds is just the whole training set).

If this is possible, then I feel like it's also possible that this *best* model will eventually have a high generalization error when tested on the final test set, essentially making cross-validation useless. Did I miss anything here?",11,6
1380,2019-5-23,2019,5,23,1,brqmqu,[N] ML Applied to Programming with Stephen Magill (45 minute talk),https://www.reddit.com/r/MachineLearning/comments/brqmqu/n_ml_applied_to_programming_with_stephen_magill/,mto96,1558541308,"Learn the fundamental concepts of Machine Learning with world-recognised expert Stephen Magill, and then delve into how Machine Learning can be used to analyse and improve code.  


[https://youtu.be/W5wD5mIeKws?list=PLEx5khR4g7PLIxNHQ5Ze0Mz6sAXA8vSPE](https://youtu.be/W5wD5mIeKws?list=PLEx5khR4g7PLIxNHQ5Ze0Mz6sAXA8vSPE)

&amp;#x200B;

Talk Abstract:

Machine learning is permeating every facet of our lives, from learning our preferences to self-driving cars, but what happens when you apply neural networks to code? How do you even view code as data? The key ideas are easy to summarize and fun to play with. This talk will provide an overview of fundamental concepts of machine learning, and then delve into how learning can be used to analyze and improve code. The talk will also provide pointers to available commercial and open source tools and discuss whats been achieved so far (coding in English, context-aware code completion, automated Stack Overflow). The talk will close with speculation on where the field is going, and how machine learning wont take our jobs, but hopefully will take over some of the repetitive work we dont like doing.",0,0
1381,2019-5-23,2019,5,23,1,brqyrl,3D-CNN for recommender system,https://www.reddit.com/r/MachineLearning/comments/brqyrl/3dcnn_for_recommender_system/,thuan14121999,1558542980,[removed],0,1
1382,2019-5-23,2019,5,23,1,brqywu,A selection of Datasets for Machine learning,https://www.reddit.com/r/MachineLearning/comments/brqywu/a_selection_of_datasets_for_machine_learning/,atomlib_com,1558543002,,0,1
1383,2019-5-23,2019,5,23,2,brrr46,"[Project] Massively parallel, vectorised implementation of Snake and RL solution",https://www.reddit.com/r/MachineLearning/comments/brrr46/project_massively_parallel_vectorised/,oscarknagg,1558546930,"Hi /r/MachineLearning.

&amp;#x200B;

As part of my recent side project to learn about reinforcement learning I've created a clone of the classic Snake game as a reinforcement learning environment and solved it with advantage actor-critic. This is one of the warm-ups from OpenAI's requests for research 2 ([https://openai.com/blog/requests-for-research-2/](https://openai.com/blog/requests-for-research-2/)).

&amp;#x200B;

You might be thinking this sounds like a very run of the mill introductory RL project. Well here are a few things that I think make it more interesting than just that.

1. I went completely overboard on the environment. Its implemented in pure PyTorch in a vectorized fashion such that I can run 1000s of environment in parallel on a single machine.
2. I compare performance of a few architectures, including a model copied from Deepmind's recent Relational RL paper (spoilers, it doesn't outcompete the other agents on this very simple task).
3. I evaluate the performance of an agent trained on a small environment in a larger environment - a limited form of RL transfer learning.

&amp;#x200B;

Medium article: [https://towardsdatascience.com/learning-to-play-snake-at-1-million-fps-4aae8d36d2f1](https://towardsdatascience.com/learning-to-play-snake-at-1-million-fps-4aae8d36d2f1)

Code: [https://github.com/oscarknagg/wurm/tree/medium-article-1](https://github.com/oscarknagg/wurm/tree/medium-article-1)

&amp;#x200B;

Here's a GIF of one of the final policies: 

&amp;#x200B;

![gif](tdsja08fssz21)

I'm currently working on the ""Slitherin'"" suggestion on OpenAI's request for research 2.0. Here's a preliminary GIF.

&amp;#x200B;

![gif](qkerdp6kusz21)",5,22
1384,2019-5-23,2019,5,23,3,brrywm,Making AI real for every developer and every organization,https://www.reddit.com/r/MachineLearning/comments/brrywm/making_ai_real_for_every_developer_and_every/,brminnick,1558548038,,0,3
1385,2019-5-23,2019,5,23,3,brs1bz,[P] My submission to GECCO2019's virtual creatures competition: Pixling World!,https://www.reddit.com/r/MachineLearning/comments/brs1bz/p_my_submission_to_gecco2019s_virtual_creatures/,FredrikNoren,1558548381,,0,1
1386,2019-5-23,2019,5,23,3,brs1st,[N] TensorWatch  Debugging and Visualization Tool Designed for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/brs1st/n_tensorwatch_debugging_and_visualization_tool/,sytelus,1558548450,"TensorWatch is a debugging and visualization tool designed for deep learning. It fully leverages Jupyter Notebook to show real time visualizations and offers unique capabilities to query the live training process without having to sprinkle logging statements all over. You can also use TensorWatch to build your own UIs and dashboards. In addition, TensorWatch leverages several excellent libraries for visualizing model graph, review model statistics, explain prediction and so on. 

&amp;#x200B;

 [https://github.com/microsoft/tensorwatch](https://github.com/microsoft/tensorwatch)",10,54
1387,2019-5-23,2019,5,23,3,brs6tx,Software that learns by demonstration,https://www.reddit.com/r/MachineLearning/comments/brs6tx/software_that_learns_by_demonstration/,plain-text-summary,1558549169,,0,1
1388,2019-5-23,2019,5,23,3,brsba4,[D] Siamese GANs,https://www.reddit.com/r/MachineLearning/comments/brsba4/d_siamese_gans/,artika_labs,1558549788,"I wrote a blog post explaining how making the Discriminator output a vector instead of a single value can be useful for some tasks:

 [https://towardsdatascience.com/a-new-way-to-look-at-gans-7c6b6e6e9737](https://towardsdatascience.com/a-new-way-to-look-at-gans-7c6b6e6e9737) 

&amp;#x200B;

Has the idea of Siamese Discriminators in GANs been explored before? I couldn't find anything on this particular matter.",2,27
1389,2019-5-23,2019,5,23,3,brsi8b,Software that learns by demonstration,https://www.reddit.com/r/MachineLearning/comments/brsi8b/software_that_learns_by_demonstration/,MealPlan,1558550784,,0,1
1390,2019-5-23,2019,5,23,4,brsse9,"[R] Latest from Samsung AI lab: Animate heads using only a few shots of the target person (or even 1 shot). Keypoints, adaptive instance norms and GANs, no 3D face modelling at all.",https://www.reddit.com/r/MachineLearning/comments/brsse9/r_latest_from_samsung_ai_lab_animate_heads_using/,himanshuragtah1,1558552141,"Paper:  [**Few-Shot Adversarial Learning of Realistic Neural Talking Head Models**](https://www.profillic.com/paper/arxiv:1905.08233) 

&amp;#x200B;

The system is able to initialize the parameters of both the generator and the discriminator in a person-specific way, so that training can be based on just a few images and done quickly, despite the need to tune tens of millions of parameters",27,229
1391,2019-5-23,2019,5,23,4,brsygx,"Artificial Intelligence Training in Bangalore, India",https://www.reddit.com/r/MachineLearning/comments/brsygx/artificial_intelligence_training_in_bangalore/,sayalisonawane,1558552962,[removed],0,1
1392,2019-5-23,2019,5,23,5,brtt2x,NeurIPS 2019 Will Host Minecraft Reinforcement Learning Competition,https://www.reddit.com/r/MachineLearning/comments/brtt2x/neurips_2019_will_host_minecraft_reinforcement/,Yuqing7,1558557027,,0,0
1393,2019-5-23,2019,5,23,6,bru7n4,"Artificial Intelligence Training in Bangalore, India",https://www.reddit.com/r/MachineLearning/comments/bru7n4/artificial_intelligence_training_in_bangalore/,sayalisonawane,1558559017,[removed],0,1
1394,2019-5-23,2019,5,23,6,bru8i6,Universiry of waterloo reinforcement learning course,https://www.reddit.com/r/MachineLearning/comments/bru8i6/universiry_of_waterloo_reinforcement_learning/,white_noise212,1558559138,[removed],0,1
1395,2019-5-23,2019,5,23,6,brucfm,The Sparse Manifold Transform,https://www.reddit.com/r/MachineLearning/comments/brucfm/the_sparse_manifold_transform/,HumanSpinach2,1558559684,,4,20
1396,2019-5-23,2019,5,23,6,bruepe,Original LinReg paper.,https://www.reddit.com/r/MachineLearning/comments/bruepe/original_linreg_paper/,lemague,1558560011,[removed],0,1
1397,2019-5-23,2019,5,23,6,brupkx,[P] ganless-hd: produce high resolution (512x512) images without utilizing a GAN,https://www.reddit.com/r/MachineLearning/comments/brupkx/p_ganlesshd_produce_high_resolution_512x512/,galapag0,1558561559,,0,1
1398,2019-5-23,2019,5,23,8,brvqx8,[N] REPLAB: A Reproducible Low-Cost Arm Benchmark for Robotic Learning,https://www.reddit.com/r/MachineLearning/comments/brvqx8/n_replab_a_reproducible_lowcost_arm_benchmark_for/,counterfeit25,1558567067,,0,1
1399,2019-5-23,2019,5,23,9,brwny1,What sort of machine learning are they using here? To 3D print rocket parts.,https://www.reddit.com/r/MachineLearning/comments/brwny1/what_sort_of_machine_learning_are_they_using_here/,AdditionalWay,1558572379,,0,1
1400,2019-5-23,2019,5,23,9,brwrji,Reinforcement Learning guide/course based on Colab/Jupyter/Ipython notebooks?,https://www.reddit.com/r/MachineLearning/comments/brwrji/reinforcement_learning_guidecourse_based_on/,PlusImagination,1558572971,[removed],0,1
1401,2019-5-23,2019,5,23,10,brx8g4,"My DQN model fails to learn anything. Been stuck for months, any help appreciated.",https://www.reddit.com/r/MachineLearning/comments/brx8g4/my_dqn_model_fails_to_learn_anything_been_stuck/,QuickOneMinuteOats,1558575681,[removed],0,1
1402,2019-5-23,2019,5,23,11,brxr8h,Best/Beginner friendly papers of application of Attention in Computer Vision,https://www.reddit.com/r/MachineLearning/comments/brxr8h/bestbeginner_friendly_papers_of_application_of/,uchiha_indra,1558578789,[removed],0,1
1403,2019-5-23,2019,5,23,12,bry21a,FUNDAMENTOS DEL MACHINE LEARNING,https://www.reddit.com/r/MachineLearning/comments/bry21a/fundamentos_del_machine_learning/,jeffry_30,1558580592,[removed],0,1
1404,2019-5-23,2019,5,23,12,bryeox,ML Advanced Edge Detection [P],https://www.reddit.com/r/MachineLearning/comments/bryeox/ml_advanced_edge_detection_p/,testhelpML,1558582802,"Hi All,

&amp;#x200B;

Looking for some guidance with respect to CT scan analysis. There is currently a similar software called ""Dragonfly"" (if you search Dragonfly CT it would come up on google) with an integrated deep learning mechanism. For my purposes, I feel like this is overkill. I simply need to define the line between bone and tissue for each individual slice (can be done through mouse directed spline creation or XY coordinates per slice -- would probably be harder to code for). My thoughts were to use TensorFlow and integrate that into the current software I use, but I was wondering if anybody here had some better ideas. All scans are presented on an 8bit gray-scale with individual values pre-determined by the software (could be useful to differentiate between tissue and bone).

Thanks-ML",4,7
1405,2019-5-23,2019,5,23,12,bryfiz,How big of a deal is Research Fellow at School of AI?,https://www.reddit.com/r/MachineLearning/comments/bryfiz/how_big_of_a_deal_is_research_fellow_at_school_of/,RoutineDog3,1558582954,"So Siraj Raval had announced this program ([https://www.youtube.com/watch?v=CJDpF4xUieY](https://www.youtube.com/watch?v=CJDpF4xUieY)) some while back . I filled the application form just for the sake of it, and today I got a notification that I have been selected as one of the 10 fellows. I don't know if I should pursue it or not is it a big deal ?",0,1
1406,2019-5-23,2019,5,23,13,brymt8,[D] Train a NN to denoise audio,https://www.reddit.com/r/MachineLearning/comments/brymt8/d_train_a_nn_to_denoise_audio/,YuhFRthoYORKonhisass,1558584299,Looking for a neural network that I can train to denoise audio from my microphone. Anything helps!,35,11
1407,2019-5-23,2019,5,23,14,brzhck,Image Classification Using Transfer Learning in PyTorch,https://www.reddit.com/r/MachineLearning/comments/brzhck/image_classification_using_transfer_learning_in/,spmallick,1558590324,"&amp;#x200B;

![video](44rw3p07fwz21)",1,1
1408,2019-5-23,2019,5,23,15,brzs2i,"[1905.02175v2] Adversarial Examples Are Not Bugs, They Are Features",https://www.reddit.com/r/MachineLearning/comments/brzs2i/190502175v2_adversarial_examples_are_not_bugs/,ihaphleas,1558594793,,6,39
1409,2019-5-23,2019,5,23,16,brzwi5,"[R] FastSpeech: Fast, Robust and Controllable Text to Speech",https://www.reddit.com/r/MachineLearning/comments/brzwi5/r_fastspeech_fast_robust_and_controllable_text_to/,rayeren,1558595788,,64,113
1410,2019-5-23,2019,5,23,17,bs0bqx,Why dont fully connected nets work?,https://www.reddit.com/r/MachineLearning/comments/bs0bqx/why_dont_fully_connected_nets_work/,shinybrightmorning,1558599531,"Lets say I train a simple Convnet on Cifar-10 I get easily ~80% accuracy, while the best Ive seen with fully connected nets is around 57%.

Im very curious about research that looks into large fully connected nets (e.g. same number of activations as convnet) and then regularizing them and/or using lots of data augmentation. It want to see whether its possible to reach same performance as convnets, without explicitly hard coding translational equivariance.",0,1
1411,2019-5-23,2019,5,23,17,bs0fdk,Machine Learning Certification Course in Trivandrum | Learn from Livewire,https://www.reddit.com/r/MachineLearning/comments/bs0fdk/machine_learning_certification_course_in/,livewireindia,1558600459,,0,1
1412,2019-5-23,2019,5,23,17,bs0htk,Convolutional Neural Network giving different predictions than in training,https://www.reddit.com/r/MachineLearning/comments/bs0htk/convolutional_neural_network_giving_different/,hiro_ono,1558601120,"This may not be the right place to ask so feel free to delete but:  


I've been writing an image recogntion algorithm. It should be really simple, the images are letters and numbers in squares. I've made a neural network in Keras and it says it's training well (95%+ ).. But when I go to predict what an image is it might as well be totally random.   
github link is here:

 [https://github.com/M-Morris-95/Vision](https://github.com/M-Morris-95/Vision)   


if anybody can help me out that'd be amazing, I'm sure it's something trivial but I just don't know what",0,1
1413,2019-5-23,2019,5,23,18,bs0rcf,What is going on with TF2 ?,https://www.reddit.com/r/MachineLearning/comments/bs0rcf/what_is_going_on_with_tf2/,WorldlyBunch,1558603390,[removed],1,1
1414,2019-5-23,2019,5,23,18,bs10fw,"Artificial general intelligence is here, and it's useless",https://www.reddit.com/r/MachineLearning/comments/bs10fw/artificial_general_intelligence_is_here_and_its/,elcric_krej,1558605436,,0,1
1415,2019-5-23,2019,5,23,18,bs110o,I wrote some code that performs polynomial regression using gradient descent,https://www.reddit.com/r/MachineLearning/comments/bs110o/i_wrote_some_code_that_performs_polynomial/,khawarizmy,1558605568,[removed],0,1
1416,2019-5-23,2019,5,23,19,bs1731,Advanced Data Discovery Does Not Have to Be Difficult to Use!,https://www.reddit.com/r/MachineLearning/comments/bs1731/advanced_data_discovery_does_not_have_to_be/,ElegantMicroWebIndia,1558606887,,0,1
1417,2019-5-23,2019,5,23,19,bs1cyc,[R] Zero-Shot Cross-Lingual Opinion Target Extraction,https://www.reddit.com/r/MachineLearning/comments/bs1cyc/r_zeroshot_crosslingual_opinion_target_extraction/,sjebbara,1558608089,[removed],0,1
1418,2019-5-23,2019,5,23,19,bs1dxs,How do I get started with Reinforcement Learning for Robotics?,https://www.reddit.com/r/MachineLearning/comments/bs1dxs/how_do_i_get_started_with_reinforcement_learning/,pakodanomics,1558608290,[removed],0,1
1419,2019-5-23,2019,5,23,20,bs1vq5,"KubeFlow BoF: David Aronchick, Microsoft &amp; Yaron Haviv, Iguazio",https://www.reddit.com/r/MachineLearning/comments/bs1vq5/kubeflow_bof_david_aronchick_microsoft_yaron/,IguazioDani,1558611630,,0,1
1420,2019-5-23,2019,5,23,20,bs1vvv,Evolving the topology of a neural network - Neuroevolution Neural Networ...,https://www.reddit.com/r/MachineLearning/comments/bs1vvv/evolving_the_topology_of_a_neural_network/,DevTechRetopall,1558611664,,0,1
1421,2019-5-23,2019,5,23,21,bs24ay,Simulating A/B tests offline using counterfactual inference,https://www.reddit.com/r/MachineLearning/comments/bs24ay/simulating_ab_tests_offline_using_counterfactual/,Arnie0426,1558613146,,3,25
1422,2019-5-23,2019,5,23,21,bs2l2b,PyTorch implementation of our SEM-PCYC model for zero-shot sketch-based image retrieval,https://www.reddit.com/r/MachineLearning/comments/bs2l2b/pytorch_implementation_of_our_sempcyc_model_for/,AnjanDutta,1558616002,[removed],0,1
1423,2019-5-23,2019,5,23,22,bs33n2,Controlling word limit on GPT-2,https://www.reddit.com/r/MachineLearning/comments/bs33n2/controlling_word_limit_on_gpt2/,rish-16,1558618957,[removed],0,1
1424,2019-5-23,2019,5,23,23,bs3b0v,How do I get started with machine learning?,https://www.reddit.com/r/MachineLearning/comments/bs3b0v/how_do_i_get_started_with_machine_learning/,Sou27,1558620066,[removed],0,1
1425,2019-5-23,2019,5,23,23,bs3d9v,Teaching AI to play Flappy Bird,https://www.reddit.com/r/MachineLearning/comments/bs3d9v/teaching_ai_to_play_flappy_bird/,Risse,1558620384,,0,1
1426,2019-5-23,2019,5,23,23,bs3haa,Looking for a Speech to Emotion API,https://www.reddit.com/r/MachineLearning/comments/bs3haa/looking_for_a_speech_to_emotion_api/,l2reg,1558620952,[removed],0,1
1427,2019-5-23,2019,5,23,23,bs3sbh,[Research] Want to Know the Trick to Achieve Robust Winograd Schema Challenge Results?,https://www.reddit.com/r/MachineLearning/comments/bs3sbh/research_want_to_know_the_trick_to_achieve_robust/,cdossman,1558622540," [https://medium.com/ai%C2%B3-theory-practice-business/want-to-know-the-trick-to-achieve-robust-winograd-schema-challenge-results-4569a3bbd](https://medium.com/ai%C2%B3-theory-practice-business/want-to-know-the-trick-to-achieve-robust-winograd-schema-challenge-results-4569a3bbd) 

 In this paper, we show that the performance of three language models on WSC273 strongly improves when fine-tuned on a similar pronoun disambiguation problem dataset (denoted WSCR). We additionally generate a large unsupervised WSC-like dataset. By fine-tuning the BERT language model both on the introduced and on the WSCR dataset, we achieve overall accuracies of 72.2% and 71.9% on WSC273 and WNLI, improving the previous state-of-the-art solutions by 8.5% and 6.8%, respectively",2,7
1428,2019-5-24,2019,5,24,0,bs42pf,Choosing Python for your ML project: TOP-5 reasons,https://www.reddit.com/r/MachineLearning/comments/bs42pf/choosing_python_for_your_ml_project_top5_reasons/,Celadon_soft,1558624004,,0,1
1429,2019-5-24,2019,5,24,0,bs46zs,Intuition behind Bias-Variace Tradeoff in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bs46zs/intuition_behind_biasvariace_tradeoff_in_machine/,prakhar21121,1558624625,[removed],0,1
1430,2019-5-24,2019,5,24,0,bs4fp1,"ACM Announces Best Doctoral Paper, Learning to Learn with Gradients",https://www.reddit.com/r/MachineLearning/comments/bs4fp1/acm_announces_best_doctoral_paper_learning_to/,Yuqing7,1558625876,,0,1
1431,2019-5-24,2019,5,24,0,bs4nyi,[R] Generative Neural Visual Artist task for image generation from text,https://www.reddit.com/r/MachineLearning/comments/bs4nyi/r_generative_neural_visual_artist_task_for_image/,gan_man,1558627025,,1,1
1432,2019-5-24,2019,5,24,1,bs4xt8,Samsung AI lab develops tech that can animate highly realistic heads using only a few -or in some cases - only one starter image.,https://www.reddit.com/r/MachineLearning/comments/bs4xt8/samsung_ai_lab_develops_tech_that_can_animate/,Acujl,1558628360,,0,1
1433,2019-5-24,2019,5,24,1,bs54cf,[P] PyTorch implementation of SEM-PCYC model for zero-shot sketch-based image retrieval,https://www.reddit.com/r/MachineLearning/comments/bs54cf/p_pytorch_implementation_of_sempcyc_model_for/,AnjanDutta,1558629254,,1,1
1434,2019-5-24,2019,5,24,2,bs5izb,Where can I find Purchase orders/invoices dataset for OCR?,https://www.reddit.com/r/MachineLearning/comments/bs5izb/where_can_i_find_purchase_ordersinvoices_dataset/,uchiha_indra,1558631304,[removed],0,1
1435,2019-5-24,2019,5,24,2,bs5juz,Simplifying AI with the new automated machine learning UI,https://www.reddit.com/r/MachineLearning/comments/bs5juz/simplifying_ai_with_the_new_automated_machine/,brminnick,1558631421,,0,1
1436,2019-5-24,2019,5,24,2,bs5kk5,"Moving Camera, Moving People: A Deep Learning Approach to Depth Prediction",https://www.reddit.com/r/MachineLearning/comments/bs5kk5/moving_camera_moving_people_a_deep_learning/,sjoerdapp,1558631525,,0,1
1437,2019-5-24,2019,5,24,2,bs5n3s,Few-Shot Adversarial Learning of Realistic Neural Talking Head Models,https://www.reddit.com/r/MachineLearning/comments/bs5n3s/fewshot_adversarial_learning_of_realistic_neural/,praveenscience,1558631883,,0,1
1438,2019-5-24,2019,5,24,2,bs5o7z,Take your machine learning models to production with new MLOps capabilities,https://www.reddit.com/r/MachineLearning/comments/bs5o7z/take_your_machine_learning_models_to_production/,brminnick,1558632038,,0,1
1439,2019-5-24,2019,5,24,2,bs5zd7,[1902.04615] Gauge Equivariant Convolutional Networks and the Icosahedral CNN,https://www.reddit.com/r/MachineLearning/comments/bs5zd7/190204615_gauge_equivariant_convolutional/,for_all_eps,1558633607,,34,72
1440,2019-5-24,2019,5,24,2,bs60s4,How to protect against model extraction attacks?,https://www.reddit.com/r/MachineLearning/comments/bs60s4/how_to_protect_against_model_extraction_attacks/,machinelearningwiz1,1558633811,"I'm building a ML model and serving it via an API. How do I prevent people from stealing my model through an extraction attack \[1\]?

  
One thing I'm thinking about is limiting the number of requests someone can make. They can only make 100 requests per hour. 

&amp;#x200B;

Are there any ideas on how to prevent these types of attacks?

&amp;#x200B;

\[1\] [https://arxiv.org/pdf/1705.08504.pdf](https://arxiv.org/pdf/1705.08504.pdf)",0,1
1441,2019-5-24,2019,5,24,2,bs63la,Political party matcher not matching the way you'd like? Let's try some data sciencing!,https://www.reddit.com/r/MachineLearning/comments/bs63la/political_party_matcher_not_matching_the_way_youd/,egpbos,1558634199,,0,2
1442,2019-5-24,2019,5,24,3,bs65uj,Samsung AI lab develops tech that can animate highly realistic heads using only a few -or in some cases - only one starter image.,https://www.reddit.com/r/MachineLearning/comments/bs65uj/samsung_ai_lab_develops_tech_that_can_animate/,VermillionBlu,1558634513,,0,1
1443,2019-5-24,2019,5,24,3,bs6d4v,Deep Learning based Image Classification in PyTorch,https://www.reddit.com/r/MachineLearning/comments/bs6d4v/deep_learning_based_image_classification_in/,spmallick,1558635546,,1,1
1444,2019-5-24,2019,5,24,3,bs6lqd,[P] Teaching a Car to Drive with PyTorch and Trajectory Optimization,https://www.reddit.com/r/MachineLearning/comments/bs6lqd/p_teaching_a_car_to_drive_with_pytorch_and/,ishmandoo,1558636804,"A little project I did optimizing the path of a steering car using automatic differentiation in PyTorch. 

[http://blog.benwiener.com/programming/2019/05/14/steering-car.html](http://blog.benwiener.com/programming/2019/05/14/steering-car.html) 

I'd love to know what you think!",7,13
1445,2019-5-24,2019,5,24,3,bs6s9y,[N] Samsung AI lab develops tech that can animate highly realistic heads using only a few - or in some cases - only one starter image.,https://www.reddit.com/r/MachineLearning/comments/bs6s9y/n_samsung_ai_lab_develops_tech_that_can_animate/,Natsu6767,1558637725,,0,1
1446,2019-5-24,2019,5,24,4,bs6vwd,Question About the Coursera Courses,https://www.reddit.com/r/MachineLearning/comments/bs6vwd/question_about_the_coursera_courses/,kymedcs,1558638246,"Hey guys,

I'm a CS student and I thought this would be an okay place to ask this question.

&amp;#x200B;

I'm currently enrolled under scholarship, the Stanford Machine Learning course by Andrew Ng. I was planning to take the deep learning specialization by [deeplearning.ai](https://deeplearning.ai) right afterwards. This plan was because I assumed the stanford content was beneficial theory, and/or prerequisite knowledge. Though it seems to be that some concepts in the Stanford course are in the [deeplearning.ai](https://deeplearning.ai) course.

&amp;#x200B;

Is it worth doing the Stanford one? Or should I skip it and jump right into the modern deep learning stuff?

  
Thanks :)",0,1
1447,2019-5-24,2019,5,24,4,bs76vy,How to samples,https://www.reddit.com/r/MachineLearning/comments/bs76vy/how_to_samples/,XxXJosephB87XxX,1558639843,[removed],0,1
1448,2019-5-24,2019,5,24,4,bs7bh8,"[R] State of the art- Latest from Google AI: Moving Camera, Moving People: A Deep Learning Approach to Depth Prediction",https://www.reddit.com/r/MachineLearning/comments/bs7bh8/r_state_of_the_art_latest_from_google_ai_moving/,himanshuragtah1,1558640508,"paper: [**Learning the Depths of Moving People by Watching Frozen People**](https://www.profillic.com/paper/arxiv:1904.11111)  

&amp;#x200B;

Goal: Predict depth when both camera and person are moving.

These guys show improvement over **state-of-the-art** monocular depth prediction methods",3,0
1449,2019-5-24,2019,5,24,5,bs7mf4,External GPU in 2019 - Yes or No?,https://www.reddit.com/r/MachineLearning/comments/bs7mf4/external_gpu_in_2019_yes_or_no/,Knl_Kolhe,1558642074,"Are external GPu's like rtx 2070 in the aorus eGPU case good for deep learning with the thunderbolt 3?

I am a student and cannot have 1 desktop and 1 laptop for portability. I wanted to buy a new laptop and I want to get some advice about buying a thin and light laptop w/o GPU + eGPU because this option seems the best to me. I am trying to avoid a heavier laptop because I currently own one and it is not fun to carry it around. (But if I need to I will)",0,1
1450,2019-5-24,2019,5,24,5,bs7v0o,SingularityNET Supervisory Council,https://www.reddit.com/r/MachineLearning/comments/bs7v0o/singularitynet_supervisory_council/,_w1kke_,1558643293,,0,2
1451,2019-5-24,2019,5,24,5,bs7zp1,SingularityNET-X Lab,https://www.reddit.com/r/MachineLearning/comments/bs7zp1/singularitynetx_lab/,_w1kke_,1558643948,,0,2
1452,2019-5-24,2019,5,24,6,bs8hrg,Deploy a model,https://www.reddit.com/r/MachineLearning/comments/bs8hrg/deploy_a_model/,drmayorga,1558646559,[removed],0,1
1453,2019-5-24,2019,5,24,6,bs8j32,[P] Linear Regression: all you need to know before an interview,https://www.reddit.com/r/MachineLearning/comments/bs8j32/p_linear_regression_all_you_need_to_know_before/,Nathan-toubiana,1558646760,"Hey all! Just sharing this article on linear regression theory - all that's needed before a data science interview. Hope it helps!

[https://towardsdatascience.com/all-you-need-to-know-about-linear-regression-to-be-interview-ready-fc58a00a0b8c](https://towardsdatascience.com/all-you-need-to-know-about-linear-regression-to-be-interview-ready-fc58a00a0b8c)",67,200
1454,2019-5-24,2019,5,24,6,bs8tqy,Time series with larges sizes differences,https://www.reddit.com/r/MachineLearning/comments/bs8tqy/time_series_with_larges_sizes_differences/,FelipwMarcelino,1558648415,[removed],0,1
1455,2019-5-24,2019,5,24,7,bs9amh,Samsung AI Makes the Mona Lisa Speak,https://www.reddit.com/r/MachineLearning/comments/bs9amh/samsung_ai_makes_the_mona_lisa_speak/,Yuqing7,1558651035,,0,1
1456,2019-5-24,2019,5,24,8,bs9qbs,from matplotlib.pyplot import here_we_go_again,https://www.reddit.com/r/MachineLearning/comments/bs9qbs/from_matplotlibpyplot_import_here_we_go_again/,NoteToBear,1558653608,,0,1
1457,2019-5-24,2019,5,24,9,bsaon2,Machine learning in investment firms,https://www.reddit.com/r/MachineLearning/comments/bsaon2/machine_learning_in_investment_firms/,lppier,1558659548,[removed],0,1
1458,2019-5-24,2019,5,24,10,bsayaa,"Hello! I am Mauricio Costa, and I have just started a small show on Youtube where I create softwares from famous series and movies.",https://www.reddit.com/r/MachineLearning/comments/bsayaa/hello_i_am_mauricio_costa_and_i_have_just_started/,mauricecost,1558661222,[removed],0,1
1459,2019-5-24,2019,5,24,10,bsb4d1,[D] Multivariate series with large different sizes,https://www.reddit.com/r/MachineLearning/comments/bsb4d1/d_multivariate_series_with_large_different_sizes/,FelipwMarcelino,1558662272,"Hello Guys, I need hel with some data. I have a multivariate time series that have 6k columns and samples with different size. The data are collected per second. However I have samples with 5 hours and others samples with 10 minutos. I already thought about padding, but is this case, the padding part is too big that can downgrade accuracy of RNN. There are articles using another type of solution for this situation?",10,1
1460,2019-5-24,2019,5,24,10,bsb60y,"Large Deviation, Optimal Transport and Machine Learning References",https://www.reddit.com/r/MachineLearning/comments/bsb60y/large_deviation_optimal_transport_and_machine/,its-trivial,1558662576,[removed],0,1
1461,2019-5-24,2019,5,24,11,bsbjh3,Double Major in EE and Applied Math + MSCS for Machine Learning? Good combo?,https://www.reddit.com/r/MachineLearning/comments/bsbjh3/double_major_in_ee_and_applied_math_mscs_for/,doctereaves,1558664981,[removed],0,0
1462,2019-5-24,2019,5,24,11,bsbk9i,"[D] Industry ""postdoc""?",https://www.reddit.com/r/MachineLearning/comments/bsbk9i/d_industry_postdoc/,postdoct222,1558665129,"Throwaway for obvious reasons. 

&amp;#x200B;

Background: I am currently finishing up a PhD in physics and got into ML two years ago. I have been lucky enough to collaborate with some ML researchers at my institution and have published a few (\~4) papers at top conferences. Two of them are first author papers, but none have really gotten that much attention (I do have several hundred citations but mostly for my physics publications).

&amp;#x200B;

I have spent the last few months looking for research scientist positions at industry labs. Unfortunately I was unable to secure a research scientist position, but was recently offered a ""postdoc"" position at a well-known lab. Basically they said that that they want to hire me in this position for a year and then potentially promote me to a ""research scientist"" position depending on my performance.

&amp;#x200B;

I was surprised since I didn't know postdoc positions even existed in industry (but they told me that this type of position is typically offered to students who are taking a year off before becoming faculty). While the salary is not too bad (\~130K), they are giving me no stock. My ML PhD friends have told me that you can typically expect 100K+ in stock that vests over a few years when you join as a reserach scientist with a industy research lab.  So I am not too happy in terms of total compensation.

&amp;#x200B;

I do have another job lined up (quant finance) that pays substantially more (300K+), but it is not a research position. Any advice on what to do? Is there a chance that the industry lab would consider giving stock to a ""postdoc""? Anyone else have taken such a postdoc position?",8,8
1463,2019-5-24,2019,5,24,11,bsbmy4,"Resource: Automated Machine Learning: Methods, Systems and Challenges",https://www.reddit.com/r/MachineLearning/comments/bsbmy4/resource_automated_machine_learning_methods/,harry_comp_16,1558665598,"Started going through this very thorough and well put together resource on automated machine learning 

https://link.springer.com/book/10.1007/978-3-030-05318-5

Folks know of other great resources on this topic? I know that the references inside are quite useful!",0,1
1464,2019-5-24,2019,5,24,11,bsbqku,Let's check how many papers are submitted to Neurips2019!!,https://www.reddit.com/r/MachineLearning/comments/bsbqku/lets_check_how_many_papers_are_submitted_to/,Mannershin,1558666235,[removed],1,1
1465,2019-5-24,2019,5,24,12,bsbuqn,Forward algorithm and Viterbi algorithm,https://www.reddit.com/r/MachineLearning/comments/bsbuqn/forward_algorithm_and_viterbi_algorithm/,peter_nguyenanh,1558666981,Could the results from the forward algorithm be used for Viterbi algorithm?,0,1
1466,2019-5-24,2019,5,24,12,bsbvn6,What questions would you ask a 'genuine artificial intelligence'?,https://www.reddit.com/r/MachineLearning/comments/bsbvn6/what_questions_would_you_ask_a_genuine_artificial/,p_bogdan,1558667138,[removed],0,1
1467,2019-5-24,2019,5,24,12,bsbzk1,[D] What questions would you ask a 'genuine artificial intelligence'?,https://www.reddit.com/r/MachineLearning/comments/bsbzk1/d_what_questions_would_you_ask_a_genuine/,p_bogdan,1558667820,"I have just woke up (literally its 4:45am here) from a dream where I have built a ""genuine AI"" based on optoelectronic hardware. I call it genuine because 1) it had some kind of personality, 2) it trained itself from scratch and in unsupervised manner.

\- What is your name?  
 \- Sania

Sometimes this AI provided less articulate answers:

\- What is my name?   
 \- Math fails to give a definitive answer

Which I suppose was an equivalent to ""I don't know"". 

Which other things do you think I have asked?

P. S. Depending on its 'mood', this AI sometimes called itself Donnie",24,0
1468,2019-5-24,2019,5,24,12,bsc6kf,Integration of Machine Learning in Life Sciences,https://www.reddit.com/r/MachineLearning/comments/bsc6kf/integration_of_machine_learning_in_life_sciences/,analyticsinsight,1558669107,,0,1
1469,2019-5-24,2019,5,24,12,bsc9nz,[R] Parallel Neural Text-to-Speech,https://www.reddit.com/r/MachineLearning/comments/bsc9nz/r_parallel_neural_texttospeech/,iyaja,1558669699,,1,8
1470,2019-5-24,2019,5,24,13,bscf4a,Machine learning courses offered in ECE vs CSE departments,https://www.reddit.com/r/MachineLearning/comments/bscf4a/machine_learning_courses_offered_in_ece_vs_cse/,priyacbooklover,1558670763,[removed],0,1
1471,2019-5-24,2019,5,24,14,bscyye,[R] AttentionRNN: A Structured Spatial Attention Mechanism,https://www.reddit.com/r/MachineLearning/comments/bscyye/r_attentionrnn_a_structured_spatial_attention/,xternalz,1558674822,,1,5
1472,2019-5-24,2019,5,24,14,bsd2nx,[Research] The Convolutional Tsetlin Machine peaks at 99.51% accuracy on MNIST with a single layer of interpretable filters in propositional logic.,https://www.reddit.com/r/MachineLearning/comments/bsd2nx/research_the_convolutional_tsetlin_machine_peaks/,olegranmo,1558675672,"Finally completed the Convolutional Tsetlin Machine. It turned out that the simplest design worked best. I use the  conjunctive clauses of the Tsetlin Machine as filters, and  the OR operation to calculate the output of each convolution. [https://arxiv.org/abs/1905.09688](https://arxiv.org/abs/1905.09688)

https://i.redd.it/jvj55zqhg3031.png",34,68
1473,2019-5-24,2019,5,24,15,bsdec9,[P] Exploring a Pixel-Maze with Evolution Strategies,https://www.reddit.com/r/MachineLearning/comments/bsdec9/p_exploring_a_pixelmaze_with_evolution_strategies/,Matumio,1558678299,"[https://log2.ch/2019/exploring-a-pixel-maze-with-evolution-strategies/](https://log2.ch/2019/exploring-a-pixel-maze-with-evolution-strategies/)

This was a fun experiment, training a small NN with CMA-ES.

I've written an article about the results, with links to inspirational papers and posts.

https://i.redd.it/bvk5vtg7n3031.png",4,9
1474,2019-5-24,2019,5,24,16,bsdwo4,Is machine learning basically just linear (or quadratic) regression mixed with PID?,https://www.reddit.com/r/MachineLearning/comments/bsdwo4/is_machine_learning_basically_just_linear_or/,cyphrrr,1558682546,[removed],0,1
1475,2019-5-24,2019,5,24,16,bse25u,[P] Illustrated Artificial Intelligence cheatsheets covering Stanford's CS 221 class,https://www.reddit.com/r/MachineLearning/comments/bse25u/p_illustrated_artificial_intelligence_cheatsheets/,shervinea,1558683920,"Set of animated Artificial Intelligence cheatsheets covering the content of Stanford's CS 221 class:

* Reflex-based: [https://stanford.edu/\~shervine/teaching/cs-221/cheatsheet-reflex-models](https://stanford.edu/~shervine/teaching/cs-221/cheatsheet-reflex-models)
* States-based: [https://stanford.edu/\~shervine/teaching/cs-221/cheatsheet-states-models](https://stanford.edu/~shervine/teaching/cs-221/cheatsheet-states-models)
* Variables-based: [https://stanford.edu/\~shervine/teaching/cs-221/cheatsheet-variables-models](https://stanford.edu/~shervine/teaching/cs-221/cheatsheet-variables-models)
* Logic-based: [https://stanford.edu/\~shervine/teaching/cs-221/cheatsheet-logic-models](https://stanford.edu/~shervine/teaching/cs-221/cheatsheet-logic-models)

&amp;#x200B;

https://i.redd.it/aet4o7el44031.png

&amp;#x200B;

All the above in PDF format: [https://github.com/afshinea/stanford-cs-221-artificial-intelligence](https://github.com/afshinea/stanford-cs-221-artificial-intelligence)

https://i.redd.it/5kfhjwcu54031.png",17,477
1476,2019-5-24,2019,5,24,17,bse5rp,[N] Teslas Deep Learning at Scale: Using Billions of Miles to Train Neural Networks,https://www.reddit.com/r/MachineLearning/comments/bse5rp/n_teslas_deep_learning_at_scale_using_billions_of/,vadhavaniyafaijan,1558684841,,0,1
1477,2019-5-24,2019,5,24,17,bse86r,[D] Automated Machine Learning is broken,https://www.reddit.com/r/MachineLearning/comments/bse86r/d_automated_machine_learning_is_broken/,pp314159,1558685433,"I'm working on my AutoML startup since 2016. Recently, I've come to the conclusion that AutoML is broken. AutoML should make deploying ML easier and faster, I think it is the opposite. What is more, I think it can do more harm than good.

The benefits of AutoML:

1. AutoML can train (some) model for provided input data. Nice!
2. AutoML can be used for computing batch predictions for provided test data.
3. AutoML can be used for deploying the model behind REST API. User can access model with REST requests. Nice :)

The boring code - hidden part needed to make AutoML work:

1. In real-life someone needs to create the input dataset. There is a need to write some code which will fetch data from multiple sources, merge and create a data frame. This code needs to be well written, so all software engineering good practices should be applied.
2. If the final ML model will be used in production, then more code needs to be created, for preparing data for predictions. Whether you are going to use batch predictions or use REST API,  you need to add more code.
3. Most of the AutoML solutions I know, do not provide detailed model maintenance mode. There should be monitoring of model and monitoring for input data quality. Based on this monitoring, there should be an automated decision when to update/retrain the model. The feedback of model decisions should be stored somewhere. If you go to production with AutoML, probably there will be a need to write additional code for maintenance and monitoring.

I think there is a lot of boring-code needed to use AutoML in real-life in the production.

The last thing I don't like in AutoML is complexity. When you apply ML to the problem even a simple model should bring you huge improvement (ROI). I like simple models because they are easy to understand. Applying a complex model as a first iteration for solving a business case with ML is unnecessary over-complication. 

If you want to read more, I wrote a post about my thoughts about broken AutoML:
https://pplonski.github.io/automatic-machine-learning-is-broken/",4,0
1478,2019-5-24,2019,5,24,19,bsf3t8,Samsung AI lab develops tech that can animate highly realistic heads using only a few -or in some cases - only one starter image.,https://www.reddit.com/r/MachineLearning/comments/bsf3t8/samsung_ai_lab_develops_tech_that_can_animate/,ankitnayak1,1558692744,,0,1
1479,2019-5-24,2019,5,24,19,bsf6zx,[D] Generating and Manipulating Faces Using Deep Learning,https://www.reddit.com/r/MachineLearning/comments/bsf6zx/d_generating_and_manipulating_faces_using_deep/,manneshiva,1558693370,"Nvidias research team proposed StyleGAN at the end of 2018, and instead of trying to create a fancy new technique to stabilize GAN training or introducing a new architecture, the paper says that their technique is orthogonal to the ongoing discussion about GAN loss functions, regularization, and hyper-parameters.  


They also discuss some of the clever techniques (Adaptive Instance Normalization, Style Mixing, Stochastic Noise, Mapping Network) that youd need to generate images that you exactly want. 

This blog covers these topics and contains all the code you need to experiment on your images (Might be interesting to see what you'd look like when you grow old): [https://blog.nanonets.com/stylegan-got](https://blog.nanonets.com/stylegan-got/#stylegan)

 

*Processing gif 80h5lzfex4031...*",2,10
1480,2019-5-24,2019,5,24,19,bsf82l,How to compare Accuracy of two RandomForest models? (Chi-Test or Cohen's H?),https://www.reddit.com/r/MachineLearning/comments/bsf82l/how_to_compare_accuracy_of_two_randomforest/,Durzo_92,1558693599,[removed],0,1
1481,2019-5-24,2019,5,24,19,bsfa2c,[D] Generative model for whisper speech,https://www.reddit.com/r/MachineLearning/comments/bsfa2c/d_generative_model_for_whisper_speech/,HanSatyam,1558694001,Since fundamental frequency is added to whisper speech to produce a normal speech. How is Generative model able to capture features of whisper speech and what features are captured by these models?,2,2
1482,2019-5-24,2019,5,24,19,bsfcnt,Humble Bundle now has 23 Machine Learning books available for 15$! Happy reading! :-) (3 more days),https://www.reddit.com/r/MachineLearning/comments/bsfcnt/humble_bundle_now_has_23_machine_learning_books/,DoctorN0mad,1558694518,,0,1
1483,2019-5-24,2019,5,24,20,bsfij5,[D] Pose Estimation on Android?,https://www.reddit.com/r/MachineLearning/comments/bsfij5/d_pose_estimation_on_android/,cbsudux,1558695683,"CoreML offers pose estimation on IOS. 

&amp;#x200B;

\- Are there any resources for Pose Estimation on Android?

\- Also, when is TF lite pose estimation going to be released?

&amp;#x200B;

Thanks",2,1
1484,2019-5-24,2019,5,24,20,bsfxon,[P] Dimensionality Reduction Methods for a dataset with continuous and binary features?,https://www.reddit.com/r/MachineLearning/comments/bsfxon/p_dimensionality_reduction_methods_for_a_dataset/,sap218,1558698477,"I was told to use PCA from my postdoc supervisor, but I realized it is best for continuous features. I've looked into splitting my data to do a PCA with continuous features and MCA for binary. My postdoc supervisor said he combined the results - I was wondering how? A PCA returns PC1 and PC2 with coordinates (length = dataframe size). My MCA returns dim1 and dim2 but with 24 coordinates for the binary data. I'm not sure how to proceed?

But if there is another method that someone could recommend would be much appreciated! I am using R currently. But I can use Python too.",9,0
1485,2019-5-24,2019,5,24,21,bsgfqz,NLP and ML By Example,https://www.reddit.com/r/MachineLearning/comments/bsgfqz/nlp_and_ml_by_example/,BoTreeTechnologies,1558701581,,0,1
1486,2019-5-24,2019,5,24,22,bsh1ng,DataScience Digest - Issue #17,https://www.reddit.com/r/MachineLearning/comments/bsh1ng/datascience_digest_issue_17/,flyelephant,1558705137,,0,1
1487,2019-5-24,2019,5,24,22,bsh3zv,"[N] Call for papers: RSS Workshop ""Scalable Learning for Integrated Perception and Planning""",https://www.reddit.com/r/MachineLearning/comments/bsh3zv/n_call_for_papers_rss_workshop_scalable_learning/,speyside42,1558705496,"**Workshop Website:** [https://scalableroboticlearning.github.io/](https://scalableroboticlearning.github.io/)

&amp;#x200B;

**When:** June 22, 2019

**Where:** Freiburg, Germany @[Robotics: Science and Systems (RSS)](http://www.roboticsconference.org/) Conference

**Wanted:** Extended Abstracts (2-4 pages) are due **June 7th, 2019**

&amp;#x200B;

**Topics include**

* Automatized training procedures
* Interactions between sensing, manipulation and planning
* Scalable approaches for grasping and generalizing functional grasps
* End-to-end vs. modular approaches (introspection, suitable metrics, ..)
* Automatic recovery and failure proof systems

&amp;#x200B;

**Invited Speakers**

* [**Kai Arras**](https://www.bosch.com/de/forschung/know-how/forscher/dr-kai-oliver-arras/), Bosch Robotics Research Program
* [**Michael Beetz**](http://ai.uni-bremen.de/team/michael_beetz), University Bremen
* [**Renaud Detry**](https://www-robotics.jpl.nasa.gov/people/Renaud_Detry/), NASA JPL
* [**Dieter Fox**](https://homes.cs.washington.edu/~fox/), University of Washington, as well as NVIDIA AI Robotics Research Lab
* [**Maxim Likhachev**](http://www.cs.cmu.edu/~maxim/), Robotics Institute Carnegie Mellon University
* [**Gilwoo Lee**](https://gilwoolee.github.io/), School of Computer Science &amp; Engineering at the University of Washington
* [**Peter Pastor**](https://scholar.google.com/citations?user=_ws9LLgAAAAJ&amp;hl=de), X",1,2
1488,2019-5-24,2019,5,24,22,bsh81l,[Research/Dataset] Duration-of-Stay Storage Assignment under Uncertainty,https://www.reddit.com/r/MachineLearning/comments/bsh81l/researchdataset_durationofstay_storage_assignment/,shaznl1,1558706125,[removed],0,1
1489,2019-5-24,2019,5,24,23,bshc0r,[Research] Duration-of-Stay Storage Assignment under Uncertainty,https://www.reddit.com/r/MachineLearning/comments/bshc0r/research_durationofstay_storage_assignment_under/,shaznl1,1558706725,"A new type of dataset for all of the ML fellows here to work on (inquiry by email in paper, institution + use case). Longitudinal warehousing storage dataset - first of its kind (AFAAK). Try to predict how long it will stay in the warehouse! (Spoiler: You may first want to properly define what ""how long"" means first)

Not to mention the company transports 30% of American's frozen food, so....see if what you eat is in there!

[https://arxiv.org/abs/1903.05063](https://arxiv.org/abs/1903.05063)",0,2
1490,2019-5-24,2019,5,24,23,bshimv,[R] Invitation to a Competition for AI under Uncertainty: Reconnaissance Blind Chess (NeurIPS 2019),https://www.reddit.com/r/MachineLearning/comments/bshimv/r_invitation_to_a_competition_for_ai_under/,rwgardner,1558707694,"**Summary:** Create a bot that can play chess blind but with the ability to sense a 3x3 square of the board each turn, as part of a fun, online AI competition of NeurIPS 2019!

Participants do not need to attend the NeurIPS conference and there is no cost to participate.  ([Play now](http://rbc.jhuapl.edu).)

&amp;#x200B;

&amp;#x200B;

**Slightly Longer Story:** All are invited to participate in a computer science competition that will be part of the 2019 Conference on Neural Information Processing Systems (NeurIPS, [https://nips.cc/](https://nips.cc/)), Reconnaissance Blind Chess.

&amp;#x200B;

Many of the favorite studied games in artificial intelligence (AI) such as checkers, chess, and Go lack something that is common and critical in real-life decision making, uncertainty.

&amp;#x200B;

This is a competition with a simple but powerful twist on what may be considered the most classic game in AI history, chess. Reconnaissance Blind Chess (RBC) is like chess except a player cannot see where her opponent's pieces are a priori. Rather, she learns partial information about them with the ability to sense a 3x3 square of the board each turn and from the results of moves.

&amp;#x200B;

In comparison to poker, which seems to be the most popularly studied game of imperfect information, RBC includes a critical component of long-term planning. Compared to phantom games like Kriegspiel, in RBC players have much more ability to manage their uncertainty, which we believe makes the game more interesting from an AI perspective and more realistic for most scenarios; players are not completely blind, but rather, metaphorically, they simply cannot look everywhere at once.

&amp;#x200B;

For more information on the NeurIPS competition, the game itself, or the API, or to play the game to get a feel for it, visit our website below.

&amp;#x200B;

All are welcome to create the best RBC bot they can, and see how well it can play against other bots in the tournament starting on October 21, 2019!

&amp;#x200B;

[http://rbc.jhuapl.edu](http://rbc.jhuapl.edu)",0,10
1491,2019-5-24,2019,5,24,23,bshpvc,[P] A lightweight sparse implementation of GraRep (WWW 15),https://www.reddit.com/r/MachineLearning/comments/bshpvc/p_a_lightweight_sparse_implementation_of_grarep/,benitorosenberg,1558708785,"&amp;#x200B;

https://i.redd.it/sop8gops76031.png

Github: [https://github.com/benedekrozemberczki/GraRep](https://github.com/benedekrozemberczki/GraRep)

Paper:  [https://www.researchgate.net/publication/301417811\_GraRep](https://www.researchgate.net/publication/301417811_GraRep)

Abstract:

In this paper, we present GraRep, a novel model for learning vertex  representations of weighted graphs. This model learns low dimensional  vectors to represent vertices appearing in a graph and, unlike existing  work, integrates global structural information of the graph into the  learning process. We also formally analyze the connections between our  work and several previous research efforts, including the DeepWalk model  of Perozzi et al. as well as the skip-gram model with negative sampling  of Mikolov et al. We conduct experiments on a language network, a  social network as well as a citation network and show that our learned  global representations can be effectively used as features in tasks such  as clustering, classification and visualization. Empirical results  demonstrate that our representation significantly outperforms other  state-of-the-art methods in such tasks.",0,1
1492,2019-5-24,2019,5,24,23,bshrmg,Looking for a good book on machine learning in Python,https://www.reddit.com/r/MachineLearning/comments/bshrmg/looking_for_a_good_book_on_machine_learning_in/,PatOnTheBackEh,1558709055,[removed],0,1
1493,2019-5-24,2019,5,24,23,bshs6w,[P] Build a Transformer Chatbot with TensorFlow 2.0,https://www.reddit.com/r/MachineLearning/comments/bshs6w/p_build_a_transformer_chatbot_with_tensorflow_20/,ZWF0cHVzc3k,1558709137,,0,1
1494,2019-5-25,2019,5,25,0,bsi4me,America is turning against facial-recognition software,https://www.reddit.com/r/MachineLearning/comments/bsi4me/america_is_turning_against_facialrecognition/,j_orshman,1558710920,,0,1
1495,2019-5-25,2019,5,25,0,bsi4xf,Why do we apply bias in the hidden layer?,https://www.reddit.com/r/MachineLearning/comments/bsi4xf/why_do_we_apply_bias_in_the_hidden_layer/,MiyamotoKnows,1558710960,[removed],0,1
1496,2019-5-25,2019,5,25,0,bsi938,Deploy a model,https://www.reddit.com/r/MachineLearning/comments/bsi938/deploy_a_model/,drmayorga,1558711560,"Hi, finished a prediction model in R but don't know how is the best way to deploy a model. Would like to receive any recommendation. Thanks",0,1
1497,2019-5-25,2019,5,25,0,bsie95,"After using azure notebook-vm I can't open local notebooks, Anyone else experience or solve this?",https://www.reddit.com/r/MachineLearning/comments/bsie95/after_using_azure_notebookvm_i_cant_open_local/,Zayba,1558712313,[removed],0,1
1498,2019-5-25,2019,5,25,0,bsif1x,Sam Altman on OpenAI's Business model,https://www.reddit.com/r/MachineLearning/comments/bsif1x/sam_altman_on_openais_business_model/,thebusinessmodel,1558712421,[removed],0,1
1499,2019-5-25,2019,5,25,0,bsiixo,[D] An easy guide to gauge equivariant convolutional networks,https://www.reddit.com/r/MachineLearning/comments/bsiixo/d_an_easy_guide_to_gauge_equivariant/,kayzaks,1558712979,"After reading ""[Gauge Equivariant Convolutional Networks and the Icosahedral CNN](https://arxiv.org/abs/1902.04615)"" by Cohen et. al. posted [here](https://www.reddit.com/r/MachineLearning/) yesterday, I thought I'd share a non-math summary for everyone who doesn't have the time to read up on an entire field of physics, just to read a paper. I really think the fundamental idea of the paper is really cool for everyone doing deep learning.

[https://medium.com/@kayzaks/an-easy-guide-to-gauge-equivariant-convolutional-networks-9366fb600b70](https://medium.com/@kayzaks/an-easy-guide-to-gauge-equivariant-convolutional-networks-9366fb600b70)",3,68
1500,2019-5-25,2019,5,25,0,bsij7c,[News] Sam Altman on OpenAI's Business model,https://www.reddit.com/r/MachineLearning/comments/bsij7c/news_sam_altman_on_openais_business_model/,thebusinessmodel,1558713020,"Here is Sam Altman, CEO of OpenAI, talking about their business model (31:26 - 32:13)

[https://youtu.be/TzcJlKg2Rc0?t=1886](https://youtu.be/TzcJlKg2Rc0?t=1886)

&amp;#x200B;

When asked about how the business model is going to work, he replies:

""The honest answer is that we have no idea. We have never made any revenue. We have no current plans to make revenue. We have no idea about how we may one day generate revenue. We have made a soft promise to investors that once we build this sort-of generally intelligent system, basically we will ask it to figure out a way to generate an investment return.""",47,77
1501,2019-5-25,2019,5,25,1,bsj2t8,"Semantically Tied Paired Cycle Consistency for Zero-Shot Sketch-based Image Retrieval, Anjan Dutta and Zeynep Akata, CVPR 2019.",https://www.reddit.com/r/MachineLearning/comments/bsj2t8/semantically_tied_paired_cycle_consistency_for/,AnjanDutta,1558715759,,1,1
1502,2019-5-25,2019,5,25,1,bsj99o,"Machine Learning Engineers, What are you currently working on?",https://www.reddit.com/r/MachineLearning/comments/bsj99o/machine_learning_engineers_what_are_you_currently/,rawrtherapy,1558716668,,0,1
1503,2019-5-25,2019,5,25,2,bsjfwp,"Moving Camera, Moving People: Google AIs Deep Learning Approach to Depth Prediction",https://www.reddit.com/r/MachineLearning/comments/bsjfwp/moving_camera_moving_people_google_ais_deep/,Yuqing7,1558717593,,0,1
1504,2019-5-25,2019,5,25,2,bsjxx2,Machine Learning for Vehicle Routing Problem,https://www.reddit.com/r/MachineLearning/comments/bsjxx2/machine_learning_for_vehicle_routing_problem/,saloumalrg,1558720132,[removed],0,1
1505,2019-5-25,2019,5,25,3,bska9b,Strong ML departments UK,https://www.reddit.com/r/MachineLearning/comments/bska9b/strong_ml_departments_uk/,buttholebolt,1558721869,[removed],0,1
1506,2019-5-25,2019,5,25,3,bskndd,Creating a Neural Network Game a la Code Bullet,https://www.reddit.com/r/MachineLearning/comments/bskndd/creating_a_neural_network_game_a_la_code_bullet/,Popup4t4,1558723712,[removed],0,1
1507,2019-5-25,2019,5,25,4,bskuxo,Paper Submissions Break NeurIPS 2019 Paper Submission System,https://www.reddit.com/r/MachineLearning/comments/bskuxo/paper_submissions_break_neurips_2019_paper/,Yuqing7,1558724775,,0,1
1508,2019-5-25,2019,5,25,5,bslttc,Serverless Machine Learning on AWS Lambda with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/bslttc/serverless_machine_learning_on_aws_lambda_with/,The_Real_Slim_Shady_,1558729743,,0,1
1509,2019-5-25,2019,5,25,5,bsm09u,[Discussion] Do any data science practitioners develop their own MCMC models?,https://www.reddit.com/r/MachineLearning/comments/bsm09u/discussion_do_any_data_science_practitioners/,AlexSnakeKing,1558730626,"I gave a tutorial on MCMC methods to some interns last week. And it struck me that although the math is absolutely fascinating, the usefulness of knowing the detailed workings of MCMC was very limited. After all, unless you are part of the PyTorch or TensorFlow dev teams, you really don't need to know how SGD or ADAM works. 

Similarly, you just need to know that your model uses MCMC, but do you really need to know the details of Metropolis-Hastings and or how the cooling schedule works in Simulating Annealing? You would just really on whatever framework you were using (TFP, PyStan, etc...) to do all that for you, and worry about the high level functional aspects of your model. 

&amp;#x200B;

Have any of you had to write your own MCMC code in a modeling context, not a software development context?",15,0
1510,2019-5-25,2019,5,25,6,bsmsl7,[Project] Looking for an advanced-beginner/intermediate NLP challenge?,https://www.reddit.com/r/MachineLearning/comments/bsmsl7/project_looking_for_an/,AlexSnakeKing,1558734540,"I'm trying to learn NLP. Most of the available material is ""spoon fed"" tutorials where you don't really learn anything, cause you are just doing a by the numbers rehashing of the code. Is there any NLP challenge or data set which is challenging enough that one can actually learn from it, but still suitable to a beginner in the topic (I know other ML methods, especially times series, it's just NLP that I don't have any experience in)?",5,3
1511,2019-5-25,2019,5,25,7,bsng8x,"Will AI preserve humanity? Further, will the desire to preserve our ecosystem and lifeforms emerge in AI?",https://www.reddit.com/r/MachineLearning/comments/bsng8x/will_ai_preserve_humanity_further_will_the_desire/,FD435,1558738048,,0,1
1512,2019-5-25,2019,5,25,8,bsnwpi,Few-Shot Adversarial Learning of Realistic Neural Talking Head Models,https://www.reddit.com/r/MachineLearning/comments/bsnwpi/fewshot_adversarial_learning_of_realistic_neural/,CptVifen,1558740727,,20,112
1513,2019-5-25,2019,5,25,9,bsojvu,"[P] Explanation/PyTorch implementation of ""The Curious Case of Neural Text Degeneration.""",https://www.reddit.com/r/MachineLearning/comments/bsojvu/p_explanationpytorch_implementation_of_the/,yaroslavvb,1558744795,[removed],0,1
1514,2019-5-25,2019,5,25,10,bsox0u,Using Progressive Growing of GANs Official Tensorflow implementation,https://www.reddit.com/r/MachineLearning/comments/bsox0u/using_progressive_growing_of_gans_official/,sjzchan,1558747186,[removed],0,1
1515,2019-5-25,2019,5,25,10,bsp3mq,Mona Lisa frown: Machine learning brings old paintings and photos to life  TechCrunch,https://www.reddit.com/r/MachineLearning/comments/bsp3mq/mona_lisa_frown_machine_learning_brings_old/,rutul96,1558748397,,0,1
1516,2019-5-25,2019,5,25,11,bspcdc,The Mathematics of Learning  discovering the math behind the learning,https://www.reddit.com/r/MachineLearning/comments/bspcdc/the_mathematics_of_learning_discovering_the_math/,rajesh_d24,1558749977,,1,1
1517,2019-5-25,2019,5,25,12,bsq755,[D] Paper Digest: ICML 2019 Highlights,https://www.reddit.com/r/MachineLearning/comments/bsq755/d_paper_digest_icml_2019_highlights/,biandangou,1558755812,"[paperdigest.org](http://paperdigest.org/) just  released ICML 2019 paper digest. It associates every single paper with a one-sentence highlight. 774 in total. :-)

[https://www.paperdigest.org/2019/05/icml-2019-highlights/](https://www.paperdigest.org/2019/05/icml-2019-highlights/)",0,1
1518,2019-5-25,2019,5,25,12,bsq7xy,[D] Any tips to crack the DeepMind RS onsite interviews?,https://www.reddit.com/r/MachineLearning/comments/bsq7xy/d_any_tips_to_crack_the_deepmind_rs_onsite/,deepmind_interviewee,1558755971,"Hi all, 

&amp;#x200B;

I'm interviewing for a Research Scientist position at DeepMind. I cracked the 2 hr pop quiz stage and now have a day of onsite interviews. Just wondering whether anyone with experience can share what the onsite interviews typically cover?

Thanks in advance for your help.

&amp;#x200B;

Thanks,

Subhrajit",3,1
1519,2019-5-25,2019,5,25,12,bsq80c,Can anyone think of mobile-first machine learning companies? I'm trying to understand all the companies who either 1) utilize this technology in their core offering (industry agnostic) or 2) are developing an improved model for mobile machine learning? Bonus if anyone knows a travel use case,https://www.reddit.com/r/MachineLearning/comments/bsq80c/can_anyone_think_of_mobilefirst_machine_learning/,Ddude488,1558755981,[removed],0,1
1520,2019-5-25,2019,5,25,12,bsqay5,[Research] Mobile-First Machine Learning Companies,https://www.reddit.com/r/MachineLearning/comments/bsqay5/research_mobilefirst_machine_learning_companies/,Ddude488,1558756582,Can anyone think of mobile-first machine learning companies? I'm trying to understand all the companies who either 1) utilize this technology in their core product (industry agnostic) or 2) are developing an improved model for mobile machine learning? Bonus if anyone knows travel focused machine learning companies,13,0
1521,2019-5-25,2019,5,25,13,bsqcdu,How much coding skills are required for Data Science as a fresher?,https://www.reddit.com/r/MachineLearning/comments/bsqcdu/how_much_coding_skills_are_required_for_data/,Academic_Help,1558756873,[removed],0,2
1522,2019-5-25,2019,5,25,13,bsqjmx,Why should I (not) use ML for time-series forecasting?,https://www.reddit.com/r/MachineLearning/comments/bsqjmx/why_should_i_not_use_ml_for_timeseries_forecasting/,Archa3opt3ryx,1558758303,[removed],0,1
1523,2019-5-25,2019,5,25,14,bsr2bj,Encountered problems when implementing the gradient algorithm in a paper,https://www.reddit.com/r/MachineLearning/comments/bsr2bj/encountered_problems_when_implementing_the/,LLjiang,1558762299,"&amp;#x200B;

[W,b  are parameters of a neural networkepsilon is a noise parameter D is a diagonal matrix which is](https://i.redd.it/eujphq0pja031.png)

used as an activate function.  The value on the diagonal of D is determined by l, u.

&amp;#x200B;

[the gradient algorithm ](https://i.redd.it/7o77oca1ka031.png)

&amp;#x200B;

I'm confused by the shape of gD.  Suppose we have a 4-layers fully connected neural network \[2,16,32,10\].  I think the shape of gD is \[16,2\], but I get errors when I calculated the gM.  Can someone help me find the error? I am really confused about the gradient calculation.",0,1
1524,2019-5-25,2019,5,25,14,bsr7ai,Dataset of purchase orders/invoices for OCR,https://www.reddit.com/r/MachineLearning/comments/bsr7ai/dataset_of_purchase_ordersinvoices_for_ocr/,maykulkarni,1558763457,[removed],0,1
1525,2019-5-25,2019,5,25,18,bssrqo,Intuition behind Bias &amp; Variance,https://www.reddit.com/r/MachineLearning/comments/bssrqo/intuition_behind_bias_variance/,prakhar21,1558777459,[removed],0,1
1526,2019-5-25,2019,5,25,20,bstig2,using non-free open source code and models,https://www.reddit.com/r/MachineLearning/comments/bstig2/using_nonfree_open_source_code_and_models/,tyrellxelliot,1558783980,[removed],0,1
1527,2019-5-25,2019,5,25,21,bstx3j,How to find a good Machine Learning Engineer job?,https://www.reddit.com/r/MachineLearning/comments/bstx3j/how_to_find_a_good_machine_learning_engineer_job/,seekerEU,1558787130,,1,1
1528,2019-5-25,2019,5,25,22,bsuqvq,"If someone made a very powerful AI, how would he/she approach using it to benefit humanity?",https://www.reddit.com/r/MachineLearning/comments/bsuqvq/if_someone_made_a_very_powerful_ai_how_would/,Pekraab,1558792457,,0,1
1529,2019-5-25,2019,5,25,23,bsv964,"[P] Meet Barbara, the talking AI Camera",https://www.reddit.com/r/MachineLearning/comments/bsv964/p_meet_barbara_the_talking_ai_camera/,8BitsAndAByte,1558795393,,1,1
1530,2019-5-26,2019,5,26,0,bsvoj3,Looking for Mid/Senior ML Engineers in the Bay Area,https://www.reddit.com/r/MachineLearning/comments/bsvoj3/looking_for_midsenior_ml_engineers_in_the_bay_area/,manu_ai,1558797787,[removed],0,1
1531,2019-5-26,2019,5,26,0,bsw29p,Check out these cool and neat machinery that are super easy and interesting to use and watch.,https://www.reddit.com/r/MachineLearning/comments/bsw29p/check_out_these_cool_and_neat_machinery_that_are/,Shanii101,1558799895,,0,1
1532,2019-5-26,2019,5,26,1,bswdt1,Churn Prediction with XGBoost Classifier,https://www.reddit.com/r/MachineLearning/comments/bswdt1/churn_prediction_with_xgboost_classifier/,karamanbk,1558801548,,0,1
1533,2019-5-26,2019,5,26,1,bswgmb,How long should it take to train an FCNN for image segmentation?,https://www.reddit.com/r/MachineLearning/comments/bswgmb/how_long_should_it_take_to_train_an_fcnn_for/,Devin_White,1558801943,[removed],0,1
1534,2019-5-26,2019,5,26,2,bswysl,Help: Understanding Fine Stride Max Pooling in OverFeat,https://www.reddit.com/r/MachineLearning/comments/bswysl/help_understanding_fine_stride_max_pooling_in/,draliacshun,1558804595,[removed],0,1
1535,2019-5-26,2019,5,26,2,bsxekj,[N] Bunch of books on Machine Learning are discounted for 48h more,https://www.reddit.com/r/MachineLearning/comments/bsxekj/n_bunch_of_books_on_machine_learning_are/,KristineDevore,1558806961,"It's all within one bundle and these books are high-quality ones with bunch of good reviews. Also it expires in 48hours so it's worth checking.

&amp;#x200B;

See them here:",3,0
1536,2019-5-26,2019,5,26,3,bsxgg5,Learn 16 Machine Learning Algorithms in a Fun and Easy along with Practical Python Labs using Keras,https://www.reddit.com/r/MachineLearning/comments/bsxgg5/learn_16_machine_learning_algorithms_in_a_fun_and/,udemudailydeal,1558807242,,0,1
1537,2019-5-26,2019,5,26,4,bsyqnh,Published my first Article on machine learning!,https://www.reddit.com/r/MachineLearning/comments/bsyqnh/published_my_first_article_on_machine_learning/,brokemybackagain,1558813846,"[https://medium.com/@ashwinbhat2906/deep-learning-notes-stargan-f5506c2ce833](https://medium.com/@ashwinbhat2906/deep-learning-notes-stargan-f5506c2ce833)

Please Give it a read and let me know how and where I can improve! It is about StarGAN's",0,1
1538,2019-5-26,2019,5,26,4,bsyu3p,https://www.reddit.com/r/LanguageTechnology/comments/bsyqgc/gutenberg_books_cleaner_python_package_sudo_pip/,https://www.reddit.com/r/MachineLearning/comments/bsyu3p/httpswwwredditcomrlanguagetechnologycommentsbsyqgc/,kiasari,1558814309,[removed],0,1
1539,2019-5-26,2019,5,26,5,bsyv1i,Gutenberg books cleaner python package ([sudo] pip install gutenberg-cleaner),https://www.reddit.com/r/MachineLearning/comments/bsyv1i/gutenberg_books_cleaner_python_package_sudo_pip/,kiasari,1558814436,[removed],0,1
1540,2019-5-26,2019,5,26,5,bsz1e7,Does Machine Learning require extreme Mathematics knowledge ?,https://www.reddit.com/r/MachineLearning/comments/bsz1e7/does_machine_learning_require_extreme_mathematics/,CSRequiem,1558815301,[removed],0,1
1541,2019-5-26,2019,5,26,5,bsz45s,[Project] How To Beat Kaggle (the Easy Way),https://www.reddit.com/r/MachineLearning/comments/bsz45s/project_how_to_beat_kaggle_the_easy_way/,KappaClosed,1558815708,,1,1
1542,2019-5-26,2019,5,26,5,bszco8,Thresholding method for imbalanced classification - GitHub Gist,https://www.reddit.com/r/MachineLearning/comments/bszco8/thresholding_method_for_imbalanced_classification/,ketsok,1558816946,,0,1
1543,2019-5-26,2019,5,26,6,bszngr,What path i should take to achieve my goal of making a program that can mark essays?,https://www.reddit.com/r/MachineLearning/comments/bszngr/what_path_i_should_take_to_achieve_my_goal_of/,hassanjamilyeet,1558818559,[removed],0,1
1544,2019-5-26,2019,5,26,6,bsznp3,Network Topology: Insights from Biology and Complexity Science toward G.A.I,https://www.reddit.com/r/MachineLearning/comments/bsznp3/network_topology_insights_from_biology_and/,Flimflamm,1558818595,"""General A.I"" has been roughly defined as an intelligence that is capable of learning to solve many different task types (without *forgetting* previous learning) and also under some definitions it would include the ability to abstract solutions to new types of problems on the first try. We have been making advancements toward better *multi-purpose* network architectures (networks which can work in a wide range of task types, not networks which can work *between* them), but we still have not made much progress toward the kind of AI that might begin to resemble the sentient minds that biological brains produce, not have we replicated their abstract thinking skills.

Given most of our previous advancements have been biology inspired, why stop now?

Biological brains are vastly more complex than their machine counterparts, but this complexity does not exist solely at the basic level of *number of neural connections*. Brains are more like *groups of neural networks* which exist within vertical and horizontal hierarchies of partially discrete networks. At the bottom level of this hierarchy we can only assume that *specific skills,* which are organically learned, must physically exist within a local or discrete network in the same way that an individual trained *perceptron* identifies and encodes relevant relationships from input data + feedback. We might think of a perceptron as the most fundamentally basic version of intelligence (and a necessary component in more complex intelligent structures).

One or more levels up from this array of *learned specific skills*, there must be another network which actually operates them or identifies the *skill* or *combination of skills* required to perform an incoming task so that it can activate the correct grouping of sub-skills. Whether by some horizontal connectivity between specific skilled networks at the bottom level (established, somehow, in organic training), or by a higher level network which can learn which combination of sub-skills are required to complete a given task, this seems like a plausible means of achieving the ability to combine specific skills in creative or abstract ways.

Biological brains have a superstructure that goes beyond individual neural connections (there are coherent parts of the brain, which themselves are composed of sub-networks, and which as a whole connect to other parts of the brain in peculiar (evolutionarily designed) ways. One thing about neurons in the brain is that they have a varying numbers of relatively short and long range connections. Neurons that are physically closer together are more likely to form a random connection with each-other, and are less likely to form distant connections with more distant neurons. If we visualize this, we might come out with the idea that any **given local group of neurons may constitute a somewhat separated network from more distantly separated regions simply because there are more local interconnections.** I believe that this aspect of neural topology is a necessary component of capacity of the human brain to produce an exotic *general* intelligence, specifically, our ability to solve *abstract* problems.

Some studies examining the neural topology (and associated cognitive profiles) of Autism Spectrum (Disorder) have found that individuals who are on the Autism spectrum tend to have a greater ratio of shorter range neural connections to longer range connections than the average population (they have more shorter range connections, or fewer longer range connections), and many studies have also found that Autism is associated often with an increase in cognitive/learning capacity in specific task types (such as spatial reasoning), but it is also associated with decreased learning capacity in areas such as language. In other words, the average population performs averagely across the main fields of human cognitive ability, but individuals with Autism often have an increased capacity in specific areas of learning, and reduced capacity in others (typically language, communication and social skills).

I'm definitely not making the statement that anyone who is on the Autism spectrum is less intelligent than anyone else. In fact, it seems to me that having an intelligence geared toward a very specific field of study is now in high demand, and anecdotally I can say with certainty that individuals with Autism have been making endless contributions to the advancement of science and human knowledge. In the primitive setting, human beings have always needed to perform a very general set of tasks that often changed with the seasons and the weather (we needed to be generally intelligent to adapt and survive), but once civilization started to take off we essentially began to require specific-field-experts in addition to general laymen, and this might actually be one of the strongest selective forces that might help us explain the rise of Autism in contemporary society. Nobody knows for sure why Autism has been on the rise beyond an increase in diagnosis opportunity, but the insights we get from network topology and my assumptions about general and specific intelligence actually gives us an explanatory model *and* the selective mechanism which can help explain the rise of autism.

Autism is a very interesting case study because it seems to actually relate to abstract thinking in humans. Schizophrenia, which is sometimes described as the symptom of *too much abstract thinking* does correlate with having a greater number of long range connections than the average person (there's evidence they are diametric conditions). If we can export any of these insights into the quest for G.A.I, that would be amazing. I'm only an initiate into machine learning (but have long explored subjects like complexity science, biology, evolution, philosophy, and more, which has I think given me these rather unique insights). I know this is a very messy post, but as a recovering perfectionist who never finishes anything, and as someone not wishing to write the 9000 minimum required words to explore the theses, I just had to get the gist of it out. Please share your feedback! (how else can we learn?)",0,1
1545,2019-5-26,2019,5,26,6,bszrxh,Is it time to change how Machine Learning products are built?,https://www.reddit.com/r/MachineLearning/comments/bszrxh/is_it_time_to_change_how_machine_learning/,Lordobba,1558819237,[removed],0,1
1546,2019-5-26,2019,5,26,6,bszykd,I'm building a subreddit recommender system and I need some tips,https://www.reddit.com/r/MachineLearning/comments/bszykd/im_building_a_subreddit_recommender_system_and_i/,EmptyResearch,1558820239,[removed],0,1
1547,2019-5-26,2019,5,26,7,bt0aib,"Learn Machine Learning with Data Science in Kukatpally, Hyderabad",https://www.reddit.com/r/MachineLearning/comments/bt0aib/learn_machine_learning_with_data_science_in/,aditidigital,1558822077,"[Aditi Digital Solutions](https://aditidigitalsolutions.com/data-science-training-hyderabad/) offers best [data science training in kukatpally](https://aditidigitalsolutions.com/data-science-training-hyderabad/), Hyderabad. The course curriculum has been designed as per MNC requirements. Data Scientists in this institute are deployed as faculty with 100% placement assistance and job support. Get the globally accepted certificate from us and rule the world of data science. We provide 60 days of classroom training sessions with [Python](https://aditidigitalsolutions.com/data-science-training-hyderabad/), [Machine learning](https://aditidigitalsolutions.com/data-science-training-hyderabad/) and Statistics by 18+ Years Experienced Corporate Trainer. 

[Data Science Training in Hyderabad](https://aditidigitalsolutions.com/data-science-training-hyderabad/),  [Data Science Course in Hyderabad](https://aditidigitalsolutions.com/data-science-training-hyderabad/), [Data Science Course in Kukatpally](https://aditidigitalsolutions.com/data-science-training-hyderabad/), [Top data science training institute in Hyderabad](https://aditidigitalsolutions.com/data-science-training-hyderabad/), [data science training institute in Hyderabad](https://aditidigitalsolutions.com/data-science-training-hyderabad/)

&amp;#x200B;

Location:   

[https://goo.gl/maps/r5gFksZyPknUoLcm9](https://goo.gl/maps/r5gFksZyPknUoLcm9)

&amp;#x200B;

[https://goo.gl/maps/EJdpztnFBXzijNhk9](https://goo.gl/maps/EJdpztnFBXzijNhk9)

&amp;#x200B;

![video](77wq46mkkf031 ""Data Science Training Institute in Hyderabad"")",0,1
1548,2019-5-26,2019,5,26,7,bt0m8v,[D] To finish or to master out?,https://www.reddit.com/r/MachineLearning/comments/bt0m8v/d_to_finish_or_to_master_out/,throwaway_ml14,1558823978,"Im sure nearly every ML PhD student feels this way at least once or twice throughout their program but as I finish my third year Im seriously considering going to industry.

Im happy with my research group, but the pace is slow and I dont feel like Im learning much anymore. Ive spent the past three summers at internships (one at a big industrial research lab, the other two at smaller startups doing research engineer-style work) and I enjoyed the startup environment a lot more than pure research. Im more much a fan of building systems that do things in the real world than coming up with an architecture that does 0.5% better on CIFAR100. 

I would much rather do researchy engineering than data science or generic software engineering but I dont think I have a good shot at one of the research labs-best case is that I graduate in 6 years and then likely end up in one of these research engineer roles that only require an MS. On the one hand I feel like Id regret not getting a PhD and close some doors, but on the other hand Im forgoing 3 years of a good salary and spending the remainder of my 20s in a much more interesting location (my university is in a boring college town).

Im seeing my friends from undergrad grow in their careers (and in their skillsets as engineers) while I sit in the lab 24/7 trying to crank out another paper. The 

Those of you who strongly considered leaving (or left) shortly after they got their MS in their program, do you regret it (or do you regret not leaving)?",93,213
1549,2019-5-26,2019,5,26,9,bt1fbw,Detect if animal has increased in size and track it's growth rate,https://www.reddit.com/r/MachineLearning/comments/bt1fbw/detect_if_animal_has_increased_in_size_and_track/,Fuerzacode,1558828942,[removed],0,1
1550,2019-5-26,2019,5,26,9,bt1m3m,Mindsdb deleted their github ?!,https://www.reddit.com/r/MachineLearning/comments/bt1m3m/mindsdb_deleted_their_github/,onetwosex,1558830081,[removed],0,1
1551,2019-5-26,2019,5,26,9,bt1n8j,The Best (...and 100% Free) Machine Learning Courses on the Internet.,https://www.reddit.com/r/MachineLearning/comments/bt1n8j/the_best_and_100_free_machine_learning_courses_on/,skj8,1558830282,,0,1
1552,2019-5-26,2019,5,26,10,bt24x0,Implementation of Machine Learning Algorithms using only Numpy,https://www.reddit.com/r/MachineLearning/comments/bt24x0/implementation_of_machine_learning_algorithms/,aditya1702,1558833399,[removed],0,1
1553,2019-5-26,2019,5,26,11,bt2q3b,How do I try and machine learn cryptocurrency?,https://www.reddit.com/r/MachineLearning/comments/bt2q3b/how_do_i_try_and_machine_learn_cryptocurrency/,wealthjustin,1558837195,[removed],0,1
1554,2019-5-26,2019,5,26,11,bt2t4d,PERCEPTRN EN MACHINE LEARNING,https://www.reddit.com/r/MachineLearning/comments/bt2t4d/perceptrn_en_machine_learning/,jeffry_30,1558837764,[removed],0,1
1555,2019-5-26,2019,5,26,13,bt3rhx,Mona Lisa frown: Machine learning brings old paintings and photos to life,https://www.reddit.com/r/MachineLearning/comments/bt3rhx/mona_lisa_frown_machine_learning_brings_old/,jonfla,1558844100,,0,1
1556,2019-5-26,2019,5,26,16,bt5013,Not understanding the chain rule of derivative of a matrix-matrix product,https://www.reddit.com/r/MachineLearning/comments/bt5013/not_understanding_the_chain_rule_of_derivative_of/,LLjiang,1558854092,[removed],0,1
1557,2019-5-26,2019,5,26,16,bt509o,Machine Learning A To Z Complete Course by Andrew Ng,https://www.reddit.com/r/MachineLearning/comments/bt509o/machine_learning_a_to_z_complete_course_by_andrew/,mhdibu,1558854147,[removed],0,1
1558,2019-5-26,2019,5,26,16,bt54j4,Optimization of data transfer,https://www.reddit.com/r/MachineLearning/comments/bt54j4/optimization_of_data_transfer/,hoanglinh96nl,1558855189,[removed],0,1
1559,2019-5-26,2019,5,26,16,bt56ky,"[1905.09550] Revisiting Graph Neural Networks: All We Have is Low-Pass Filters ""Our results indicate that graph neural networks only perform low-pass filtering on feature vectors""",https://www.reddit.com/r/MachineLearning/comments/bt56ky/190509550_revisiting_graph_neural_networks_all_we/,downtownslim,1558855724,,5,57
1560,2019-5-26,2019,5,26,17,bt5n28,Validation rules in super-resolution models,https://www.reddit.com/r/MachineLearning/comments/bt5n28/validation_rules_in_superresolution_models/,falmasri,1558859835,[removed],0,1
1561,2019-5-26,2019,5,26,19,bt6lz7,Why isn't there a league of legend AI yet?,https://www.reddit.com/r/MachineLearning/comments/bt6lz7/why_isnt_there_a_league_of_legend_ai_yet/,Kenivia,1558868087,[removed],0,1
1562,2019-5-26,2019,5,26,20,bt6y9c,[Project] This video goes over a breast cancer diagnosis model that uses neural networks (implemented in python),https://www.reddit.com/r/MachineLearning/comments/bt6y9c/project_this_video_goes_over_a_breast_cancer/,antaloaalonso,1558870781,,0,1
1563,2019-5-26,2019,5,26,20,bt6zhp,"I want to learn machine learning, where to start with?",https://www.reddit.com/r/MachineLearning/comments/bt6zhp/i_want_to_learn_machine_learning_where_to_start/,kchaitanya39,1558871053,[removed],0,1
1564,2019-5-26,2019,5,26,21,bt7dog,Advanced Regression Optimization with Neural Networks RMSProp - Neural ...,https://www.reddit.com/r/MachineLearning/comments/bt7dog/advanced_regression_optimization_with_neural/,DevTechRetopall,1558873833,,0,1
1565,2019-5-26,2019,5,26,22,bt7wre,Using Logistic Regression to Classify Text,https://www.reddit.com/r/MachineLearning/comments/bt7wre/using_logistic_regression_to_classify_text/,RacerRex9727,1558877283,,0,1
1566,2019-5-26,2019,5,26,23,bt8bz5,Which Deep learning paper to implement ?,https://www.reddit.com/r/MachineLearning/comments/bt8bz5/which_deep_learning_paper_to_implement/,ashutosj,1558879816,[removed],0,1
1567,2019-5-26,2019,5,26,23,bt8jrt,[D] is Huawei's Matebook D a good laptop for Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/bt8jrt/d_is_huaweis_matebook_d_a_good_laptop_for_machine/,leocus4,1558881038,"I will use it for things like Kaggle's challenges and/or ML projects.
Does the ROCm framework allow discrete performances (comparable to a laptop with similar price and NVIDIA GPU)?",16,0
1568,2019-5-26,2019,5,26,23,bt8sap,[P] Implementations of basic RL algorithms with minimal codes!,https://www.reddit.com/r/MachineLearning/comments/bt8sap/p_implementations_of_basic_rl_algorithms_with/,seungeun07,1558882347,"Hi,

for those of you who are interested in RL, 

&amp;#x200B;

I recently implemented basic RL algorithms such as 

REINFORCE, vanilla actor-critic, DDPG, A3C, DQN and PPO with PyTorch.

&amp;#x200B;

Characteristics are as follows :

* Each algorithm is complete within a single file.
* Length of each algorithm is up to 100\~150 lines of codes.
* Every algorithm can be trained within 30 seconds, even without GPU.
* Envs are fixed to ""CartPole-v1"". You can just focus on the implementations.

&amp;#x200B;

As you can see in the name of the repository, 

I tried to make the code as  brief and intuitive as possible. 

&amp;#x200B;

Hope you enjoy :)

Thank you.

&amp;#x200B;

 [https://github.com/seungeunrho/minimalRL](https://github.com/seungeunrho/minimalRL)",41,281
1569,2019-5-27,2019,5,27,0,bt8xi1,Help for choose a machine learning algorithm,https://www.reddit.com/r/MachineLearning/comments/bt8xi1/help_for_choose_a_machine_learning_algorithm/,Yann-F,1558883125,"(Machine Learning is new for me)

&amp;#x200B;

I want to detect if a manuscrite word is good or not.

&amp;#x200B;

I have some characters (get with OCR no problem with that), with accuracy by char.

For exemple : \[\['A', 0.81\], \['8', 0.3\], \['C', 0.83\], \['D', 0.98\]\]

&amp;#x200B;

And I have the good word to detect, for exemple : ""ABCD"". (the size is always the same)

&amp;#x200B;

I build a data like that : All good word chars, all OCR chars, all accuracy OCR chars, then if is good  or not (boolean).

For exemple : \[\['A', 'B', 'C', 'D', 'A', '8', 'C', 'D', 0.81, 0.3, 0.83, 0.98, 1\], ...\]

(I use LabelEncoder for the 8 firsts fields)

&amp;#x200B;

My answer is : what is the best machine learning algorithm for my predict ?

If you think I am not in the good way, dont be afraid to say to me, I am a beginner and I accept that.

&amp;#x200B;

Thanks for read.",0,1
1570,2019-5-27,2019,5,27,1,bt9krc,AI!,https://www.reddit.com/r/MachineLearning/comments/bt9krc/ai/,ingloreous_wetard,1558886502,,0,1
1571,2019-5-27,2019,5,27,1,bta7it,Optimizer unit tests,https://www.reddit.com/r/MachineLearning/comments/bta7it/optimizer_unit_tests/,Lemax0,1558889711,[removed],0,1
1572,2019-5-27,2019,5,27,1,bta7m2,Importance of AVX512 for ML workloads?,https://www.reddit.com/r/MachineLearning/comments/bta7m2/importance_of_avx512_for_ml_workloads/,SmugEskim0,1558889722,[removed],0,1
1573,2019-5-27,2019,5,27,2,btaj7o,[Project] Implementation of ML Algorithms using only Numpy and Python,https://www.reddit.com/r/MachineLearning/comments/btaj7o/project_implementation_of_ml_algorithms_using/,aditya1702,1558891373,"Features of this project:

1. No external library used - sklearn, tensorflow, pytorch etc...
2. Uses a fit() and predict() function approach - just like sklearn. This makes any model very easy to plug and use.
3. I have utilised these implementations in my own pet projects on real datasets and they work without any errors. Obviously, not optimised for speed but they are more for learning the concept and math.

&amp;#x200B;

List of the algorithms:

* **Supervised Learning**

&amp;#x200B;

* Regression Algorithms
   * [Linear Regression](https://github.com/aditya1702/Machine-Learning-and-Data-Science/blob/master/Implementation%20of%20Machine%20Learning%20Algorithms/Supervised/Regression/linear_regression.py)
   * [Ridge Regression](https://github.com/aditya1702/Machine-Learning-and-Data-Science/blob/master/Implementation%20of%20Machine%20Learning%20Algorithms/Supervised/Regression/ridge_regression.py)
   * [Lasso Regression](https://github.com/aditya1702/Machine-Learning-and-Data-Science/blob/master/Implementation%20of%20Machine%20Learning%20Algorithms/Supervised/Regression/lasso_regression.py)
   * [Decision Tree Regressor](https://github.com/aditya1702/Machine-Learning-and-Data-Science/blob/master/Implementation%20of%20Machine%20Learning%20Algorithms/Supervised/Regression/decision_tree_regressor.py)
   * [K-Nearest Neighbors Regressor](https://github.com/aditya1702/Machine-Learning-and-Data-Science/blob/master/Implementation%20of%20Machine%20Learning%20Algorithms/Supervised/Regression/knn.py)
   * [Neural Network Regressor](https://github.com/aditya1702/Machine-Learning-and-Data-Science/blob/master/Implementation%20of%20Machine%20Learning%20Algorithms/Supervised/Regression/neural_network_regressor.py)

&amp;#x200B;

* Classification Algrithms
   * [Binary Logistic Regression](https://github.com/aditya1702/Machine-Learning-and-Data-Science/blob/master/Implementation%20of%20Machine%20Learning%20Algorithms/Supervised/Classification/logistic_regression.py)
   * [Softmax Regression](https://github.com/aditya1702/Machine-Learning-and-Data-Science/blob/master/Implementation%20of%20Machine%20Learning%20Algorithms/Supervised/Classification/softmax_regression.py)
   * [Decision Tree Classifier](https://github.com/aditya1702/Machine-Learning-and-Data-Science/blob/master/Implementation%20of%20Machine%20Learning%20Algorithms/Supervised/Classification/decision_tree_classifier.py)
   * [Adaboost Binary Classifier](https://github.com/aditya1702/Machine-Learning-and-Data-Science/blob/master/Implementation%20of%20Machine%20Learning%20Algorithms/Supervised/Classification/adaboost_classifier.py)
   * [K-Nearest Neighbors Classifier](https://github.com/aditya1702/Machine-Learning-and-Data-Science/blob/master/Implementation%20of%20Machine%20Learning%20Algorithms/Supervised/Classification/knn.py)
   * [Neural Network Classifier](https://github.com/aditya1702/Machine-Learning-and-Data-Science/blob/master/Implementation%20of%20Machine%20Learning%20Algorithms/Supervised/Classification/neural_network_classifier.py)

&amp;#x200B;

* **Unsupervised Learning**
   * [K-Means Clustering](https://github.com/aditya1702/Machine-Learning-and-Data-Science/blob/master/Implementation%20of%20Machine%20Learning%20Algorithms/Unsupervised/kmeans.py)
   * [Gaussian Mixture Models](https://github.com/aditya1702/Machine-Learning-and-Data-Science/blob/master/Implementation%20of%20Machine%20Learning%20Algorithms/Unsupervised/gaussian_mixture.py)
   * [Variational Auto-Encoders (In Progress)](https://github.com/aditya1702/Machine-Learning-and-Data-Science/blob/master/Implementation%20of%20Machine%20Learning%20Algorithms/Unsupervised/vae.py)

&amp;#x200B;

Feel free to check them out. You can also check out my other projects (ML, AI and data science):

Github Link - [https://github.com/aditya1702/Machine-Learning-and-Data-Science](https://github.com/aditya1702/Machine-Learning-and-Data-Science)

Online Portfolio - [https://adityavyas17.com](http://adityavyas17.com/)",4,40
1574,2019-5-27,2019,5,27,2,btaoyl,[D] What doubts do you have about the way you do Data Science at work?,https://www.reddit.com/r/MachineLearning/comments/btaoyl/d_what_doubts_do_you_have_about_the_way_you_do/,kite_and_code,1558892193,"Hi, sometimes I am in a blue mood, and I am thinking about the following questions:

&amp;#x200B;

Will I be automated away in the future?

Is it all just hype?

Am I doing it correctly?

Are others faster than me? How can I become faster?

Why is the process sometimes so tedious?

How can I become a faster/better/more valuable Data Scientist?

Why do easy things sometimes take so long?

&amp;#x200B;

I am wondering, which thoughts do you have about the way you do Data Science at work?

And what do you do about it? e.g. learn new skills or libraries, go to meetups, ...",8,14
1575,2019-5-27,2019,5,27,3,btb3zq,A day in a Data Scientists life,https://www.reddit.com/r/MachineLearning/comments/btb3zq/a_day_in_a_data_scientists_life/,Tyrion_Lannister7,1558894321,[removed],0,1
1576,2019-5-27,2019,5,27,3,btb5ep,Am I doing it right?,https://www.reddit.com/r/MachineLearning/comments/btb5ep/am_i_doing_it_right/,MLforYay,1558894523,[removed],0,1
1577,2019-5-27,2019,5,27,3,btb6an,A clever use of Mannequin challenge videos by Google,https://www.reddit.com/r/MachineLearning/comments/btb6an/a_clever_use_of_mannequin_challenge_videos_by/,asprasan,1558894638,,0,1
1578,2019-5-27,2019,5,27,3,btbita,Decentralized AI marketplace on its way,https://www.reddit.com/r/MachineLearning/comments/btbita/decentralized_ai_marketplace_on_its_way/,Daimen1,1558896336,,0,2
1579,2019-5-27,2019,5,27,3,btbjao,[D] Has anyone seen an implementation of this paper by EA? (Imitation Learning with Concurrent Actions in 3D Games),https://www.reddit.com/r/MachineLearning/comments/btbjao/d_has_anyone_seen_an_implementation_of_this_paper/,acelis,1558896403,"Hi everyone,

I'm looking for an implementation of a specific paper or something similar, can someone point me in the good direction?

They use a modified A3C, but to be honest I'm mostly interested in the algorithm because I need to use imitation learning with multiple discrete actions.

Paper: https://arxiv.org/abs/1803.05402
Blog post: https://www.ea.com/seed/news/seed-imitation-learning-concurrent-actions

I hope I'm posting in the appropriate subreddit.

Thanks!",0,3
1580,2019-5-27,2019,5,27,4,btbrrj,[D] [NLP] Reconstructing input sentence of VAE instead of using auto-regressive,https://www.reddit.com/r/MachineLearning/comments/btbrrj/d_nlp_reconstructing_input_sentence_of_vae/,speedcell4,1558897550,"I am training a VAE on a NLP task, I found that the reconstruction part is really memory consuming. Just wonder if there is any other method to reconstruct the input but without using auto-regressive.

I have thought about 
1. Negative sampling, like what word2vec does, then we dont need to normalize on the whole vocabulary 
2. Bag of words, just simply averaging the word vectors and then use it as sentence vector, then reconstruct this sentence vector by minimizing MSE. Since this method reconstruct only one vector for one sentence instead every words, this will be fast and memory friendly. But I am not sure if this can work, or if the neural network will learn some trivial representation instead, e.g. all zeros

Any other idea? Thanks~",3,0
1581,2019-5-27,2019,5,27,4,btc9xw,Which laptop should I buy?,https://www.reddit.com/r/MachineLearning/comments/btc9xw/which_laptop_should_i_buy/,Lqgongora,1558900018,[removed],0,1
1582,2019-5-27,2019,5,27,4,btcadz,Excellent and Practical Guide to Obtaining Machine Learning Internships,https://www.reddit.com/r/MachineLearning/comments/btcadz/excellent_and_practical_guide_to_obtaining/,MammothCost,1558900076,,0,1
1583,2019-5-27,2019,5,27,5,btchgu,SingularityNET X-Lab Blockchain AI Accelerator,https://www.reddit.com/r/MachineLearning/comments/btchgu/singularitynet_xlab_blockchain_ai_accelerator/,Daimen1,1558901044,,0,7
1584,2019-5-27,2019,5,27,5,btcmdw,Self-Taught Series: How to Learn Math for Data Science,https://www.reddit.com/r/MachineLearning/comments/btcmdw/selftaught_series_how_to_learn_math_for_data/,hisham_elamir,1558901703,,0,1
1585,2019-5-27,2019,5,27,5,btcp5r,[D] DL in industry,https://www.reddit.com/r/MachineLearning/comments/btcp5r/d_dl_in_industry/,dramanautica,1558902064,"Outside of the large industry labs (FAIR, Brain etc.) and various start ups, what kind of companies are using DL in industry (ideally in Europe)?

Sorry if this isnt the best place to ask this question but Id imagine its most relevant here.",12,7
1586,2019-5-27,2019,5,27,6,btda2o,Resources for Bayesian Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/btda2o/resources_for_bayesian_deep_learning/,schrodingershit,1558904943,,0,1
1587,2019-5-27,2019,5,27,6,btde09,"I know its not machine learning, but could you teach a machine how to do this without explicitly having it do something when it senses movement?",https://www.reddit.com/r/MachineLearning/comments/btde09/i_know_its_not_machine_learning_but_could_you/,314har,1558905502,,0,0
1588,2019-5-27,2019,5,27,6,btdoh6,[R] Single Cortical Neurons as Deep Artificial Neural Networks,https://www.reddit.com/r/MachineLearning/comments/btdoh6/r_single_cortical_neurons_as_deep_artificial/,phobrain,1558907030,,0,1
1589,2019-5-27,2019,5,27,6,btdtwu,Machine Learning Datasets and Projects,https://www.reddit.com/r/MachineLearning/comments/btdtwu/machine_learning_datasets_and_projects/,hisham_elamir,1558907808,[removed],0,1
1590,2019-5-27,2019,5,27,9,btfg57,Implementing K-Means Clustering From Scratch: Simply Explained,https://www.reddit.com/r/MachineLearning/comments/btfg57/implementing_kmeans_clustering_from_scratch/,jimscott1232,1558917236,,0,1
1591,2019-5-27,2019,5,27,9,btflve,Is Fast.ai a good place for noobs to learn the basics of Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/btflve/is_fastai_a_good_place_for_noobs_to_learn_the/,07_ghostface,1558918208,[removed],0,1
1592,2019-5-27,2019,5,27,10,btg172,[R] Training decision trees as replacement for convolution layers,https://www.reddit.com/r/MachineLearning/comments/btg172/r_training_decision_trees_as_replacement_for/,xternalz,1558920819,,15,35
1593,2019-5-27,2019,5,27,10,btg2cr,[D] Calculating the confidence interval for RMSE,https://www.reddit.com/r/MachineLearning/comments/btg2cr/d_calculating_the_confidence_interval_for_rmse/,this_nicholas,1558921023," 

I'm reading a book on machine learning where the author uses the Random Forest Regression model to fit a dataset. The confidence interval for the root mean squared error is then computed using the following code.

Does anyone know why the code works? Under what assumptions does the sum of squared errors follow a generalized t distribution (unintuitive to me, I feel like it should follow a  ${\\chi}\^2$ distribution?

    from scipy import stats
        confidence = 0.95
        squared_errors = (final_predictions - y_test) ** 2 #y_test is real values, final_predictions is predicted values of y
        ci = np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, loc=squared_errors.mean(), scale=stats.sem(squared_errors)))",1,1
1594,2019-5-27,2019,5,27,11,btgk5c,[D],https://www.reddit.com/r/MachineLearning/comments/btgk5c/d/,Ayakalam,1558924114,"Hi all, 

I came across some stuff from Vicarious-AI online, (somewhat recently, eg, see here: [https://science.sciencemag.org/content/358/6368/eaag2612](https://science.sciencemag.org/content/358/6368/eaag2612))

&amp;#x200B;

My question is simple: Why haven't RCN's received more attention?... Dis-regarding the Vicarious-AI itself, the technology does seem to be interesting, right? The fact that they are trying to re-think how we should design AI systems / or at least partition the problem seems to hold some water I would say.... so I was curious why this piece of \_technology\_ hasn't received more attention?...",2,0
1595,2019-5-27,2019,5,27,11,btglxb,[N] I've been working on video series on the key insights behind Deep Learning/ML for 16 months (Art of the Problem),https://www.reddit.com/r/MachineLearning/comments/btglxb/n_ive_been_working_on_video_series_on_the_key/,britcruise,1558924440,,0,1
1596,2019-5-27,2019,5,27,11,btgn1y,Combining Neural Networks and Genetic Algorithms,https://www.reddit.com/r/MachineLearning/comments/btgn1y/combining_neural_networks_and_genetic_algorithms/,Popup4t4,1558924640,[removed],0,1
1597,2019-5-27,2019,5,27,11,btgrnn,[R] Almost Unsupervised Text to Speech and Automatic Speech Recognition,https://www.reddit.com/r/MachineLearning/comments/btgrnn/r_almost_unsupervised_text_to_speech_and/,rayeren,1558925501,,4,0
1598,2019-5-27,2019,5,27,11,btgsdu,"[R] ""Training language GANs from Scratch"", de Masson d'Autume et al 2019 {DM} [sequence GANs on par with MLE]",https://www.reddit.com/r/MachineLearning/comments/btgsdu/r_training_language_gans_from_scratch_de_masson/,gwern,1558925641,,15,26
1599,2019-5-27,2019,5,27,11,btgsw8,[N] Exceptionally high quality video series on Deep Learning by Art of the Problem (focused on key insights),https://www.reddit.com/r/MachineLearning/comments/btgsw8/n_exceptionally_high_quality_video_series_on_deep/,puppers90,1558925736,,0,1
1600,2019-5-27,2019,5,27,12,btgxw3,Does anyone write about how interpretability is *not* necessary in machine learning?,https://www.reddit.com/r/MachineLearning/comments/btgxw3/does_anyone_write_about_how_interpretability_is/,pretysmitty,1558926666,[removed],0,1
1601,2019-5-27,2019,5,27,12,btgzmu,[D] Why are RCN's not a thing?,https://www.reddit.com/r/MachineLearning/comments/btgzmu/d_why_are_rcns_not_a_thing/,Ayakalam,1558927007,"Hi all,

I came across some stuff from Vicarious-AI online, (somewhat recently, eg, see here: [https://science.sciencemag.org/content/358/6368/eaag2612](https://science.sciencemag.org/content/358/6368/eaag2612))

My question is simple: Why haven't RCN's received more attention?... Dis-regarding the Vicarious-AI itself, the technology does seem to be interesting, right? The fact that they are trying to re-think how we should design AI systems / or at least partition the problem seems to hold some water I would say.... so I was curious why this piece of \_technology\_ hasn't received more attention?...

&amp;#x200B;

Edit: Also - I realized that I had forgotten to add a title, so re-made the post. Thank you!",12,4
1602,2019-5-27,2019,5,27,12,bth52y,Problem creating CNN,https://www.reddit.com/r/MachineLearning/comments/bth52y/problem_creating_cnn/,I_Want_Answer,1558928017,[removed],0,1
1603,2019-5-27,2019,5,27,13,bthdg7,"[N] Arxiv introduces Bibex, get a list of references and citations for each paper. Though the feature seems to be controversial among some ML people.",https://www.reddit.com/r/MachineLearning/comments/bthdg7/n_arxiv_introduces_bibex_get_a_list_of_references/,BatmantoshReturns,1558929609,"If you go on arxiv, you can activate a feature called Bibex, which shows references and citations for each paper. 

Here's a description
https://labs.arxiv.org/projects/bibexplorer

offtopic, but I didn't even know arxiv had labs 
https://labs.arxiv.org/

Anyways, some people do not like the new feature

https://twitter.com/zacharylipton/status/1132479367425093634

I can't say one way or another, but as someone who analyzes research papers, this data will be great to analyze and train on.",12,12
1604,2019-5-27,2019,5,27,13,bthryz,[P] ICLR optimization papers I: Fluctuation-Dissipation relations for SGD,https://www.reddit.com/r/MachineLearning/comments/bthryz/p_iclr_optimization_papers_i/,DLamikins,1558932380,,0,1
1605,2019-5-27,2019,5,27,13,bthtzw,[Discussion] How are papers validated if they lack the code &amp; weights ?,https://www.reddit.com/r/MachineLearning/comments/bthtzw/discussion_how_are_papers_validated_if_they_lack/,elcric_krej,1558932771,"I'm not part of academia, but I've noticed a trend of ML papers containing no source code for creating the models, training on the dataset and no weights for the actual trained model on which the experiments were ran.

&amp;#x200B;

How would one go about validating the research in these papers with the lacking source code ? 

&amp;#x200B;

For one re-constructing the model exactly as the original in some cases could be hard, since the paper might only mention the generic architecture for some block, but not the exact variation they went for.

&amp;#x200B;

Secondly, it seems it would be a rather tedious task to reverse engineer the model and the training code just to validate simple things about what the researchers did (e.g. that they didn't make some error in the sampling of train/test/validation data and as such got the wrong numbers in the paper for a specific dataset)

&amp;#x200B;

Do people just email the authors and ask for the source code ? Thus keeping it constrained within the specific academic circles the author desire ? If so, why make the paper open to being with, since for a lot of them a pretty good replica of the model can be constructed from the paper alone.",73,211
1606,2019-5-27,2019,5,27,14,bti2lq,awesome-RecSys,https://www.reddit.com/r/MachineLearning/comments/bti2lq/awesomerecsys/,data-chef,1558934432,[removed],0,1
1607,2019-5-27,2019,5,27,14,btidln,[P] ICLR optimization papers I: Fluctuation-Dissipation relations for SGD,https://www.reddit.com/r/MachineLearning/comments/btidln/p_iclr_optimization_papers_i/,DLamikins,1558936667,"A technical overview of Fluctuation-dissipation relations for stochastic gradient descent published at ICLR 2019:

[https://medium.com/@yaroslavvb/iclr-optimization-papers-i-fluctuation-dissipation-relations-for-sgd-a638ad9964cc](https://medium.com/@yaroslavvb/iclr-optimization-papers-i-fluctuation-dissipation-relations-for-sgd-a638ad9964cc)",1,2
1608,2019-5-27,2019,5,27,15,btihdl,"Github Repo of ""Recommender System"" - awesome-Recsys",https://www.reddit.com/r/MachineLearning/comments/btihdl/github_repo_of_recommender_system_awesomerecsys/,data-chef,1558937426,[removed],0,1
1609,2019-5-27,2019,5,27,15,btiig5,[R] Gradient boosting research papers from the last 25 years,https://www.reddit.com/r/MachineLearning/comments/btiig5/r_gradient_boosting_research_papers_from_the_last/,benitorosenberg,1558937644,"## [https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers)

A curated list of gradient boosting research papers with implementations from the following conferences.

Machine learning:

1. NeurIPS
2. ICML
3. ICLR

Computer vision:

1. CVPR
2. ICCV
3. ECCV

Natural language processing:

1. ACL
2. NAACL
3. EMNLP

Data Mining:

1. KDD
2. ICDM
3. CIKM
4. WWW

Artificial intelligence:

1. AAAI
2. IJCAI
3. UAI
4. AISTATS",0,0
1610,2019-5-27,2019,5,27,16,btixl0,Machine Learning Book Club,https://www.reddit.com/r/MachineLearning/comments/btixl0/machine_learning_book_club/,hisham_elamir,1558940930,[removed],1,1
1611,2019-5-27,2019,5,27,16,btj0yc,"Updates on ""Professor Forcing""",https://www.reddit.com/r/MachineLearning/comments/btj0yc/updates_on_professor_forcing/,marchinidavide,1558941657,[removed],0,1
1612,2019-5-27,2019,5,27,17,btjexq,How do I overcome the accuracy quantum jumping when training neural network?,https://www.reddit.com/r/MachineLearning/comments/btjexq/how_do_i_overcome_the_accuracy_quantum_jumping/,inyukwo1,1558944705,[removed],0,1
1613,2019-5-27,2019,5,27,18,btjuye,[R] Gradient boosting research papers from the last 25 years,https://www.reddit.com/r/MachineLearning/comments/btjuye/r_gradient_boosting_research_papers_from_the_last/,benitorosenberg,1558948377,"##  [https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers)

&amp;#x200B;

A curated list of gradient boosting research papers with implementations from the following conferences.

Machine learning:

1. NeurIPS
2. ICML
3. ICLR

Computer vision:

1. CVPR
2. ICCV
3. ECCV

Natural language processing:

1. ACL
2. NAACL
3. EMNLP

Data Mining:

1. KDD
2. ICDM
3. CIKM
4. WWW

Artificial intelligence:

1. AAAI
2. IJCAI
3. UAI
4. AISTATS",13,81
1614,2019-5-27,2019,5,27,18,btjx68,[N] Weekly newsletter for reinforcement learning,https://www.reddit.com/r/MachineLearning/comments/btjx68/n_weekly_newsletter_for_reinforcement_learning/,seungjaeryanlee,1558948847,"Hello /r/machinelearning ! 

&amp;#x200B;

I have been writing a weekly newsletter sharing interesting new papers, projects, or issues in reinforcement learning. If you are interested in reinforcement learning but find it hard to keep up with it, my newsletter might be able to help! You can subscribe and check past issues here: [https://www.endtoend.ai/tags/rl-weekly/](https://www.endtoend.ai/tags/rl-weekly/). The 19th issue was sent just a few minutes ago.

&amp;#x200B;

Cheers :)",0,11
1615,2019-5-27,2019,5,27,18,btk0kj,Unstable training results CNN,https://www.reddit.com/r/MachineLearning/comments/btk0kj/unstable_training_results_cnn/,-Rizhiy-,1558949623,[removed],0,1
1616,2019-5-27,2019,5,27,18,btk10p,[R] Multi-relational Poincar Graph Embeddings,https://www.reddit.com/r/MachineLearning/comments/btk10p/r_multirelational_poincar_graph_embeddings/,ibalazevic,1558949724,"&amp;#x200B;

https://i.redd.it/eonsgsm74q031.png

&amp;#x200B;

**Paper:** [https://arxiv.org/abs/1905.09791](https://arxiv.org/abs/1905.09791)

**PyTorch Code:** [https://github.com/ibalazevic/multirelational-poincare](https://github.com/ibalazevic/multirelational-poincare)

&amp;#x200B;

**Key contributions:**

* proposing MuRP, a theoretically inspired method to embed **hierarchical multi-relational data** in the **Poincar ball** model of hyperbolic space which achieves **state-of-the-art** results on the hierarchical WN18RR knowledge graph dataset;
* showing that our Poincar embeddings require far **fewer dimensions** than Euclidean embeddings to achieve comparable performance; and
* visualizing the learned embeddings and analyzing the properties of the Poincar model compared to its Euclidean analogue.

**Abstract:**

Hyperbolic embeddings have recently gained attention in machine learning due to their ability to represent hierarchical data more accurately and succinctly than their Euclidean analogues. However, multi-relational knowledge graphs often exhibit multiple simultaneous hierarchies, which current hyperbolic models do not capture. To address this, we propose a model that embeds multi-relational graph data in the Poincar ball model of hyperbolic space. Our Multi-Relational Poincar model (MuRP) learns relation-specific parameters to transform entity embeddings by Mbius matrix-vector multiplication and Mbius addition. Experiments on the hierarchical WN18RR knowledge graph show that our multi-relational Poincar embeddings outperform their Euclidean counterpart and existing embedding methods on the link prediction task, particularly at lower dimensionality.",0,24
1617,2019-5-27,2019,5,27,18,btk1u1,Chatbot Development Tutorial Using Python | Part 1,https://www.reddit.com/r/MachineLearning/comments/btk1u1/chatbot_development_tutorial_using_python_part_1/,slywolf97,1558949919,[removed],0,1
1618,2019-5-27,2019,5,27,18,btk4bw,"Machine Learning Book Club, Data Is Utopia",https://www.reddit.com/r/MachineLearning/comments/btk4bw/machine_learning_book_club_data_is_utopia/,hisham_elamir,1558950496,,0,1
1619,2019-5-27,2019,5,27,19,btk7tm,#discussion# is it worthy to train a model for some competition to get the reward as coral dev board?,https://www.reddit.com/r/MachineLearning/comments/btk7tm/discussion_is_it_worthy_to_train_a_model_for_some/,makereven,1558951278,[removed],0,1
1620,2019-5-27,2019,5,27,19,btkkb5,Video 2 comics,https://www.reddit.com/r/MachineLearning/comments/btkkb5/video_2_comics/,sergeevii123,1558954074,[removed],0,1
1621,2019-5-27,2019,5,27,20,btkpkc,Any ideas final year projects?,https://www.reddit.com/r/MachineLearning/comments/btkpkc/any_ideas_final_year_projects/,thefuhrer004,1558955181,[removed],0,1
1622,2019-5-27,2019,5,27,20,btkvyn,"[Discussion] Do you experience GPU ""shutdown"" on servers with multiple GPUs?",https://www.reddit.com/r/MachineLearning/comments/btkvyn/discussion_do_you_experience_gpu_shutdown_on/,wingtales,1558956392,"We have several servers with four RTX 2080TI cards, used for research. We are noticing that there is a tendency for one GPU to unexpectedly ""die"" from time to time.

This happens with a frequency in the range of several times a week to several times a month and requires a server restart to get back online.

The GPU load varies, but there is no clear correlation between periods (days) of high load and GPUs dying.

Do other groups experience this? Is this expected behaviour? And has anyone found a way to avoid this?",13,11
1623,2019-5-27,2019,5,27,20,btl5ht,[D] Papers on real-time Time Series Classification,https://www.reddit.com/r/MachineLearning/comments/btl5ht/d_papers_on_realtime_time_series_classification/,cbsudux,1558958160,"Hey guys,

&amp;#x200B;

Looking for papers and code on real-time TSC.

&amp;#x200B;

Cheers",0,1
1624,2019-5-27,2019,5,27,20,btl5sj,[D] Papers on real-time Time Series Classification?,https://www.reddit.com/r/MachineLearning/comments/btl5sj/d_papers_on_realtime_time_series_classification/,cbsudux,1558958209,"Hey guys, Looking for code and papers on real-time TSC.

&amp;#x200B;

Cheers",4,4
1625,2019-5-27,2019,5,27,21,btlbix,Zero-shot Knowledge Transfer via Adversarial Belief Matching,https://www.reddit.com/r/MachineLearning/comments/btlbix/zeroshot_knowledge_transfer_via_adversarial/,polo555,1558959206,[removed],0,1
1626,2019-5-27,2019,5,27,21,btlc5p,Learn Machine Learning and Python For Data Science,https://www.reddit.com/r/MachineLearning/comments/btlc5p/learn_machine_learning_and_python_for_data_science/,aditidigisol,1558959317,"[Aditi Digital Solutions](https://aditidigitalsolutions.com/) offers best [data science training in Hyderabad](https://aditidigitalsolutions.com/data-science-training-hyderabad/) from 18+ Years Experienced Corporate Trainer. Only 10 Students Per Batch are allowed. Curriculum is attached below.

&amp;#x200B;

**MACHINE LEARNING**

**INTRODUCTION TO MACHINE LEARNING ** What is Machine Learning? What is the Challenge? Introduction to Supervised Learning, Unsupervised Learning, what is Reinforcement Learning?

**LINEAR REGRESSION** \-Introduction to Linear Regression, Linear Regression with Multiple Variables, Disadvantage of Linear Models, Interpretation of Model Outputs, Understanding Co variance and Co linearity, Understanding Heteroscedasticity, Case study on Application of Linear Regression for housing price prediction

**LOGISTIC REGRESSION**  Why Logistic Regression, Introduction to classification Cost function for logistic regression, Application of logistic regression to multi-class classification, Confusion Matrix, Odds Ratio and ROC Curve, Advantages and Disadvantages of Logistic Regression, Case study on To classify an email as spam or not spam using logistic Regression.

**DECISION TREES AND SUPERVISED LEARNING**  Decision Tree, data set, How to build decision tree? Understanding Kart Model, Classification Rules- Over fitting Problem, Stopping Criteria And Pruning, How to find final size of Trees? Model a decision Tree, Naive Bayes, Random Forests and Support Vector, Machines, Interpretation of Model Outputs, Business Case Study for Kart Model, Business Case Study for Random Forest, and Business Case Study for SVM

**UNSUPERVISED LEARNING ** Hierarchical Clustering, k-Means algorithm for  
clustering, groupings of unlabeled data points, Principal Component Analysis (PCA), Independent components analysis(ICA), Anomaly Detection, Recommender System-collaborative filtering algorithm, Case study on Recommendation Engine for E-commerce/retail chain.

**DEEP LEARNING ** Neural Network, Understanding Neural Network Model, Understanding Tuning of Neural Network, Case study using Neural Network

**NATURAL LANGUAGE PROCESSING ** Introduction to Natural Language Processing (NLP), Word Frequency Algorithms for NLP Sentiment Analysis, Case Study on Twitter data analysis using NLP.

**PYTHON FOR DATA SCIENCE AND MACHINE LEARNING**

**PYTHON PROGRAMMING BASICS**  Installing Jupyter Notebooks, Python Overview, Python 2.7 vs Python 3, Python Identifiers, Various Operators and Operators Precedence, Getting input from User, Comments and Multi line Comments.

**MAKING DECISIONS AND LOOP CONTROL ** Simple if Statement, if-else  
Statement, if-else-if Statement, Introduction to while Loops, Introduction to For Loops, Using continue and break.

**PYTHON DATA TYPES : LIST, TUPLES AND DICTIONARIES ** Python Lists, Tuples, Dictionaries, Accessing Values, Basic Operations, Indexing, Slicing, and Matrices, Built-in Functions \&amp;amp; Methods, Exercises on List, Tuples and Dictionary.

**FUNCTIONS AND MODULES** \-Introduction to Functions, Why Defining Functions? Calling Functions Functions with Multiple Arguments, Anonymous Functions  Lambda Using Built-In Modules, User-Defined Modules, Module Namespaces, Iterators and Generators

**FILE I/O AND EXCEPTIONAL HANDLING**  Opening and Closing Files, Open Function, File Object Attributes, Close Method , Read, Write, Seek. Exception Handling, the try-finally Clause, Raising an Exceptions, User-Defined Exceptions Regular Expression- Search and Replace, Regular Expression Modifiers, Regular Expression Patterns and Re module

**NUMPY**  Introduction to Numpy, Array Creation, Printing Arrays, Basic Operations- Indexing, Slicing and Iterating Shape Manipulation  Changing shape, stacking and splitting of array Vector stacking

**PANDAS ** Introduction to Pandas, Importing data into Python, Pandas Data Frames, Indexing Data Frames, Basic Operations With Data frame, Renaming Columns, Subletting and Filtering a data frame.

**MATPLOTLIB **Introduction, Plot, Controlling Line Properties, Working with Multiple Figures and Histograms.

&amp;#x200B;

[Data Science Training in Kukatpally](https://aditidigitalsolutions.com/data-science-training-hyderabad/), [Data science course in Hyderabad](https://aditidigitalsolutions.com/data-science-training-hyderabad/), [Data science course in Kukatpally](https://aditidigitalsolutions.com/data-science-training-hyderabad/), [data science course in kphb](https://aditidigitalsolutions.com/data-science-training-hyderabad/), [data science training in kphb](https://aditidigitalsolutions.com/data-science-training-hyderabad/), [data science training institute in Hyderabad](https://aditidigitalsolutions.com/data-science-training-hyderabad/), [Top data science training institute in Hyderabad](https://aditidigitalsolutions.com/data-science-training-hyderabad/), [data science training institutes in Hyderabad](https://aditidigitalsolutions.com/data-science-training-hyderabad/)

&amp;#x200B;

![video](7t4owy6lwq031)

[https://goo.gl/maps/r5gFksZyPknUoLcm9](https://goo.gl/maps/r5gFksZyPknUoLcm9)

 

[https://goo.gl/maps/EJdpztnFBXzijNhk9](https://goo.gl/maps/EJdpztnFBXzijNhk9)",0,1
1627,2019-5-27,2019,5,27,21,btldk0,Open Image By Matplotlib and OpenCV | Python,https://www.reddit.com/r/MachineLearning/comments/btldk0/open_image_by_matplotlib_and_opencv_python/,spokhrel,1558959553,By this simple code you can open any images using Matplotlib or OpenCV. Matplotlib and OpenCv are both python library used in computer vision and many more applications.,0,1
1628,2019-5-27,2019,5,27,21,btlgl7,Code for this paper!,https://www.reddit.com/r/MachineLearning/comments/btlgl7/code_for_this_paper/,ak96,1558960089,[removed],0,1
1629,2019-5-27,2019,5,27,21,btlj1k,[R] Zero-shot Knowledge Transfer via Adversarial Belief Matching,https://www.reddit.com/r/MachineLearning/comments/btlj1k/r_zeroshot_knowledge_transfer_via_adversarial/,polo555,1558960524,"&amp;#x200B;

![gif](w2ldw0o30r031)

**TLDR:** Our task is to compress a large neural network (teacher) into a smaller one (student), but we assume that the data used to train the teacher is not available anymore. Our solution is to generate pseudo points adversarially (yellow markers above) and use those to match the student (right) to the teacher (left).

&amp;#x200B;

**Paper (with PyTorch code)**:  [https://arxiv.org/abs/1905.09768](https://arxiv.org/abs/1905.09768)",4,33
1630,2019-5-27,2019,5,27,21,btll8t,[D] Why do I keep getting this error with tensorflow gpu?,https://www.reddit.com/r/MachineLearning/comments/btll8t/d_why_do_i_keep_getting_this_error_with/,YeeOfficer,1558961005,"I have installed all the cuda things necessary for it, but why will it not work? It gives me this error:

    C:\Users\pc\Documents\Ai Test\Text Test AI\TestBench3 Rap&gt;train.py
    Traceback (most recent call last):
      File ""C:\Users\pc\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in &lt;module&gt;
        from tensorflow.python.pywrap_tensorflow_internal import *
      File ""C:\Users\pc\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in &lt;module&gt;
        _pywrap_tensorflow_internal = swig_import_helper()
      File ""C:\Users\pc\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
        _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
      File ""C:\Users\pc\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
        return load_dynamic(name, filename, file)
      File ""C:\Users\pc\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
        return _load(spec)
    ImportError: DLL load failed: The specified module could not be found.
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""C:\Users\pc\Documents\Ai Test\Text Test AI\TestBench3 Rap\train.py"", line 3, in &lt;module&gt;
        import tensorflow as tf
      File ""C:\Users\pc\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in &lt;module&gt;
        from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
      File ""C:\Users\pc\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in &lt;module&gt;
        from tensorflow.python import pywrap_tensorflow
      File ""C:\Users\pc\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in &lt;module&gt;
        raise ImportError(msg)
    ImportError: Traceback (most recent call last):
      File ""C:\Users\pc\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in &lt;module&gt;
        from tensorflow.python.pywrap_tensorflow_internal import *
      File ""C:\Users\pc\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in &lt;module&gt;
        _pywrap_tensorflow_internal = swig_import_helper()
      File ""C:\Users\pc\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
        _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
      File ""C:\Users\pc\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
        return load_dynamic(name, filename, file)
      File ""C:\Users\pc\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
        return _load(spec)
    ImportError: DLL load failed: The specified module could not be found.
    
    
    Failed to load the native TensorFlow runtime.
    
    See https://www.tensorflow.org/install/errors
    
    for some common reasons and solutions.  Include the entire stack trace
    above this error message when asking for help.

Thank you",16,0
1631,2019-5-27,2019,5,27,21,btlqk0,Real-time Streaming for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/btlqk0/realtime_streaming_for_machine_learning/,IguazioDani,1558961956,"**This tutorial demonstrates the availability of streaming data in a data science environment**, which is useful for working with real-time and fresh datasets.

First, we collect data from an existing Kafka stream into an Iguazio time series table. Next, we visualize the stream with a Grafana dashboard; and finally, we access the data in a Jupyter notebook using Python code.

We use a [Nuclio serverless function](https://github.com/nuclio/nuclio) to listen to a Kafka stream and then ingest its events into our time series table. Iguazio gets you started with a template for Kafka to time series.

We visualize the data with Grafana and work with time series data using Python code in Jupyter. Data scientist easily access both historical and real-time data in a full Python environment for exploration and training with Iguazio.

Stream data into your Jupyter notebook on Iguazios Data Science Platform by [signing up for a free trial](https://www.iguazio.com/lp/14-day-free-trial-in-the-cloud/).

![video](0k044bxk4r031)",0,1
1632,2019-5-27,2019,5,27,22,btm06w,[D] Machine Learning Engineer looking to contribute to papers,https://www.reddit.com/r/MachineLearning/comments/btm06w/d_machine_learning_engineer_looking_to_contribute/,theLastQuestionLTBL,1558963761,"Hi,

I'm currently employed as a Machine Learning Engineer at a startup performing Computer Vision tasks (currently Object Detection). I've never produced papers and don't have the experience or time at work to do so. I'd love to understand the process of creating papers and produce them in topics I find interesting in the future. Hence to gain this experience I'd like to offer my time to any current PhD's/post-docs/other researchers, free of charge, in return for co-authorship in papers and a learning experience for me (and probably both parties).

Qualifications: BENg + MSc (Distinction) in Engineering from top 100 world university/top 20 UK university (Russell Group). 

Skills: Good understanding of a large amount of ML literature and methods (especially CNN related) and strong capacity to learn, Python, OpenCV, Pandas, Scikit-Learn, Tensorflow, Keras, AWS, WEKA 

Multiple Previous Projects in: Image Classification, Object Detection, Time Series Analysis and Prediction

If needed I have a laptop with a reasonable GPU for running models too.


Please comment on this post or drop me a message with any advice you may have or opportunities to collaborate, thanks.",14,50
1633,2019-5-27,2019,5,27,23,btmeqt,"[N] CVPR 2019 Accepted Papers (Oral, Poster)",https://www.reddit.com/r/MachineLearning/comments/btmeqt/n_cvpr_2019_accepted_papers_oral_poster/,skrish13,1558966274,"For people who dont want to wait till the list comes on CVF OpenAccess or IEEE Explore, here is the list.

Oral -- https://github.com/hoya012/CVPR-2019-Paper-Statistics/blob/master/2019_cvpr/cvpr_2019_oral.csv

Poster (all papers are posters) -- https://github.com/hoya012/CVPR-2019-Paper-Statistics/blob/master/2019_cvpr/cvpr_2019_poster.csv",2,13
1634,2019-5-27,2019,5,27,23,btmgpa,[R] Perturbative estimation of stochastic gradients and training of models with discrete weights,https://www.reddit.com/r/MachineLearning/comments/btmgpa/r_perturbative_estimation_of_stochastic_gradients/,LucaAmbrogioni,1558966565,,5,9
1635,2019-5-27,2019,5,27,23,btmo35,Fake results using deep learning,https://www.reddit.com/r/MachineLearning/comments/btmo35/fake_results_using_deep_learning/,luciolis,1558967657,[removed],0,1
1636,2019-5-27,2019,5,27,23,btmpz0,perforation machinery,https://www.reddit.com/r/MachineLearning/comments/btmpz0/perforation_machinery/,habmkloganjt,1558967930,,0,1
1637,2019-5-27,2019,5,27,23,btmshl,[D] Falsified results with Deep learning,https://www.reddit.com/r/MachineLearning/comments/btmshl/d_falsified_results_with_deep_learning/,luciolis,1558968280,"Hi everyone !

I am looking for papers that would show that deep learning can ""invent"" things, especially in the domain of super-resolution/upsampling. I only find papers that boasts the merits of their method, but not papers that show that their network can mistake high frequency features and noise or stuff like that...

&amp;#x200B;

Anyone?

Thanks!",6,15
1638,2019-5-27,2019,5,27,23,btmvcr,The Basics of Machine Learning (ebooks),https://www.reddit.com/r/MachineLearning/comments/btmvcr/the_basics_of_machine_learning_ebooks/,meghala,1558968739,,0,1
1639,2019-5-28,2019,5,28,0,btnbre,Improving quality of old historical images.,https://www.reddit.com/r/MachineLearning/comments/btnbre/improving_quality_of_old_historical_images/,1f1nas,1558971053,[removed],0,1
1640,2019-5-28,2019,5,28,0,btncnh,Managing ML project lifecycle,https://www.reddit.com/r/MachineLearning/comments/btncnh/managing_ml_project_lifecycle/,mattiadmr,1558971190,[removed],0,1
1641,2019-5-28,2019,5,28,0,btnj4s,[R] Learning to learn by Self-Critique,https://www.reddit.com/r/MachineLearning/comments/btnj4s/r_learning_to_learn_by_selfcritique/,AntreasAntoniou,1558972087,"Dear r/MachineLearning friends,

I just wanted to present my latest work to you. We, humans, have the ability to not only learn quickly from a few labelled examples, but we can also adjust our notions in light of new unlabelled data. I found this fascinating and wondered whether meta-learning an intrinsic loss function to do the same in the few-shot learning would, in fact, improve generalization performance. Turns out, it does. Any and all feedback is welcomed. You can be as harsh as you want. You can't top reviewer #2 anyway.

Regards, Antreas

Paper: https://arxiv.org/abs/1905.10295
Code: Soon to follow (should be up before 29/05/2019)",14,107
1642,2019-5-28,2019,5,28,0,btnkyt,Open Image By Matplotlib and OpenCV | Python,https://www.reddit.com/r/MachineLearning/comments/btnkyt/open_image_by_matplotlib_and_opencv_python/,spokhrel,1558972348,,0,1
1643,2019-5-28,2019,5,28,1,btnoen,"[D] ""Keras is a minimalist, highly modular neural network library in the spirit of Torch""",https://www.reddit.com/r/MachineLearning/comments/btnoen/d_keras_is_a_minimalist_highly_modular_neural/,Senior_Tough,1558972839,"Given the recent controversy of who inspired who: [https://twitter.com/fchollet/status/1132713291757711361](https://twitter.com/fchollet/status/1132713291757711361)

&amp;#x200B;

Let's look at one of the early README's of Keras itself: [https://web.archive.org/web/20150522055317/https://github.com/fchollet/keras](https://web.archive.org/web/20150522055317/https://github.com/fchollet/keras)

\&gt; Keras is a minimalist, highly modular neural network library **in the spirit of Torch**, written in Python / Theano so as not to have to deal with the dearth of ecosystem in Lua. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.

&amp;#x200B;

Some people forget the history of torch, one of the original deep learning libraries which inspired many libraries thereafter.",8,0
1644,2019-5-28,2019,5,28,1,btnrmy,Train a DNN with Bfloat16 in PyTorch,https://www.reddit.com/r/MachineLearning/comments/btnrmy/train_a_dnn_with_bfloat16_in_pytorch/,CArchGuy,1558973288,[removed],0,1
1645,2019-5-28,2019,5,28,1,bto0ou,[R] From Here to There: Video Inbetweening Using Direct 3D Convolutions,https://www.reddit.com/r/MachineLearning/comments/bto0ou/r_from_here_to_there_video_inbetweening_using/,AsherahTrial,1558974540,,2,1
1646,2019-5-28,2019,5,28,1,bto11u,UNet on MNIST,https://www.reddit.com/r/MachineLearning/comments/bto11u/unet_on_mnist/,Geeks_sid,1558974588,"Hey,

&amp;#x200B;

I wanted to know has anyone tried a simple 3-layer UNet on MNIST for segmentation of digits.

Although the problem sounds fairly simple, What we do is take all pixels which are not the minimum as segmentation and try to train a very small U-Net. (Literally in terms of a few MB). Let me know thoughts and suggestions as begin this journey of stupidity and triumph.

&amp;#x200B;

Tips and suggestions are happily accepted.",0,1
1647,2019-5-28,2019,5,28,1,bto9ky,[Project] New workshop to trial alternative publication model,https://www.reddit.com/r/MachineLearning/comments/bto9ky/project_new_workshop_to_trial_alternative/,often_worried,1558975781,"We are trialing the preregistration protocol with a brand new ICCV workshop.  


In a nutshell:

1) Authors write a proposal paper illustrating their idea and the experimental protocol that will be used to evaluate it.

2) Paper is submitted to OpenReview and goes through an efficient two-phases double-blind peer review process.

3) After the paper gets accepted, experiments are conducted and results are reported.

  
\--&gt; The overall paper gets published irrespective of the results achieved, focus is on soundness/originality of ideas and scientific rigor of the experimental protocol.

  
**Website**: [https://preregister.vision/](https://preregister.vision/)",4,39
1648,2019-5-28,2019,5,28,2,btojkm,Difference between generative and discriminative models?,https://www.reddit.com/r/MachineLearning/comments/btojkm/difference_between_generative_and_discriminative/,nickponline,1558977133,[removed],0,1
1649,2019-5-28,2019,5,28,2,btosn2,Linear Regression in Python with Large Dataset Example,https://www.reddit.com/r/MachineLearning/comments/btosn2/linear_regression_in_python_with_large_dataset/,codefrk,1558978390,,0,1
1650,2019-5-28,2019,5,28,2,btp3nu,[Research] Practical Classes for Robots,https://www.reddit.com/r/MachineLearning/comments/btp3nu/research_practical_classes_for_robots/,cdossman,1558979950," [https://medium.com/ai%C2%B3-theory-practice-business/practical-classes-for-robots-8a0ee915e2b8](https://medium.com/ai%C2%B3-theory-practice-business/practical-classes-for-robots-8a0ee915e2b8) 

 Robots need to learn behaviors in intuitive and practical ways for widespread deployment in human environments. To learn a robot behavior end-to-end, we train a variant of the ResNet that maps eye-in-hand camera images to end-effector velocities. In our setup, a human teacher demonstrates the task via joystick. We show that a simple servoing task can be learned in less than an hour including data collection, model training and deployment time. Moreover, 16 minutes of demonstrations were enough for the robot to learn the task.",2,0
1651,2019-5-28,2019,5,28,3,btpisb,comparing AutoML tools: the Google Cloud AutoML Tables update,https://www.reddit.com/r/MachineLearning/comments/btpisb/comparing_automl_tools_the_google_cloud_automl/,SoggyTee,1558981976,[removed],0,1
1652,2019-5-28,2019,5,28,3,btpt79,ML &amp; DL for autonomous robots,https://www.reddit.com/r/MachineLearning/comments/btpt79/ml_dl_for_autonomous_robots/,DrDumbenstein,1558983371,[removed],0,1
1653,2019-5-28,2019,5,28,4,btq50n,Does anyone created a formal database for word meaning?,https://www.reddit.com/r/MachineLearning/comments/btq50n/does_anyone_created_a_formal_database_for_word/,moshestv,1558984915,[removed],0,1
1654,2019-5-28,2019,5,28,4,btq8mc,Best book to learn about the recommender systems.,https://www.reddit.com/r/MachineLearning/comments/btq8mc/best_book_to_learn_about_the_recommender_systems/,person01234567,1558985406,[removed],0,1
1655,2019-5-28,2019,5,28,4,btqasd,Which machine learning algorithm should use?,https://www.reddit.com/r/MachineLearning/comments/btqasd/which_machine_learning_algorithm_should_use/,seemingly_omniscient,1558985698,[removed],0,1
1656,2019-5-28,2019,5,28,4,btqc58,[1905.10027] Neural Temporal-Difference Learning Converges to Global Optima,https://www.reddit.com/r/MachineLearning/comments/btqc58/190510027_neural_temporaldifference_learning/,banananach,1558985875,[removed],0,4
1657,2019-5-28,2019,5,28,4,btqe57,Introduction to Machine Learning for Data Science,https://www.reddit.com/r/MachineLearning/comments/btqe57/introduction_to_machine_learning_for_data_science/,HannahHumphreys,1558986151,[removed],0,1
1658,2019-5-28,2019,5,28,4,btqkpy,Understanding the behavior of Wasserstein loss,https://www.reddit.com/r/MachineLearning/comments/btqkpy/understanding_the_behavior_of_wasserstein_loss/,AbduallahM,1558987074,"Hi,   
I'm training a GAN model using  [Wasserstein ](https://myurasov.github.io/2017/09/24/wasserstein-gan-keras.html)loss with [Gradient penalty](https://arxiv.org/pdf/1704.00028.pdf) (WASS\_GP), the Discriminator and Generator loss behaves as in the coming figure. 

&amp;#x200B;

Any ideas of what might be the problem? The output quality is rubbish.   


https://i.redd.it/7qck5ooq6t031.png",0,1
1659,2019-5-28,2019,5,28,5,btqm9b,Data Science Bootcamp: Pay only after you get a data science job.,https://www.reddit.com/r/MachineLearning/comments/btqm9b/data_science_bootcamp_pay_only_after_you_get_a/,HannahHumphreys,1558987296,[removed],0,1
1660,2019-5-28,2019,5,28,5,btqnvd,Modern Language,https://www.reddit.com/r/MachineLearning/comments/btqnvd/modern_language/,ingloreous_wetard,1558987518,,0,1
1661,2019-5-28,2019,5,28,5,btqpp9,X-Lab Incubator Launches to Provide Support to Promising AI Startups,https://www.reddit.com/r/MachineLearning/comments/btqpp9/xlab_incubator_launches_to_provide_support_to/,strangeattractors,1558987771,,0,2
1662,2019-5-28,2019,5,28,5,btr4ij,[D] Which Machine Learning algorithm should use?,https://www.reddit.com/r/MachineLearning/comments/btr4ij/d_which_machine_learning_algorithm_should_use/,seemingly_omniscient,1558989886,"A simplified cheat sheet.  

![img](3x4aiworft031)

 [More AI / ML Slides](https://www.aisoma.de/10-useful-ai-ml-slides/)",6,0
1663,2019-5-28,2019,5,28,5,btr4tb,What is the life of a Machine Learning Engineer and what skills do you need to do the job?,https://www.reddit.com/r/MachineLearning/comments/btr4tb/what_is_the_life_of_a_machine_learning_engineer/,ryuhphino,1558989927,"Wanting to transition to Machine Learning Engineer eventually, and was curious what skills are needed. I currently do backend software engineering and am working on creating services and learning to get them to production (lower level developer). Was curious what skills are required for ML engineers so I can verify if Im on the right track. Doing a ton of personal dev for MLing and have a background in it. 

Im also aware the skills required will vary by location since the title itself can be used differently in some locations. Just looking for some general information.",0,1
1664,2019-5-28,2019,5,28,6,btreky,Hiring anyone that can write about common ML processes,https://www.reddit.com/r/MachineLearning/comments/btreky/hiring_anyone_that_can_write_about_common_ml/,apachemilo,1558991310,[removed],0,1
1665,2019-5-28,2019,5,28,6,btrfrl,101 Slides on Machine Learning,https://www.reddit.com/r/MachineLearning/comments/btrfrl/101_slides_on_machine_learning/,ConfidentMushroom,1558991476,,0,1
1666,2019-5-28,2019,5,28,7,btsf2a,Impressions from Embedded Vision Summit 2019,https://www.reddit.com/r/MachineLearning/comments/btsf2a/impressions_from_embedded_vision_summit_2019/,alghar,1558996923,"I had an opportunity to visit the showcase of the Embedded Vision Summit (held this week in Santa Clara Convention Center). The event was certainly bigger and more colorful than the previous ones. Aside from the usual vendors, I came across many new names offering various products, services, and tools covering the gamut in the field of computer vision. Clearly AI-based vision processing and automotive overshadowed the traditional computer vision technologies and use cases. As expected, there were number of new companies announcing their new vision-optimized AI accelerator chips and Intellectual properties. (I must say that it is becoming increasingly difficult to keep track of all the players in this domain and I might be forced to add a new tab to my excel sheet. Reducing the font size does not cut it anymore).

I was struck by one notable trend and that is vertical integration. There were several vendors that had offerings covering the entire spectrum for the automotive application. They offered autonomous driving software stack, ADAS (Advanced Driver Assistance Systems) hardware, AI accelerator ASICs, and in some cases even their IP.

While there were many promising companies with breakthrough technologies, I was particularly impressed by the technology and achievements of a few that are most relevant to this forum. Below is a brief summary:

&amp;#x200B;

Mythic

Mythic ([www.mythic-ai.com](https://www.mythic-ai.com)) is a fabless semiconductor company based in Austin and Redwood City building AI edge accelerator chips. Their initial target applications are smart speakers, drones, battery-powered video monitors specialized smartphones for certain vertical markets. Their implementation of deep neural networks is based on flash-based analog technology. This approach reduces the need to retrieve network weights from memory leading to a significant power savings (10x). 

I felt they have a very clear and focused mission and were able to effectively convey their differentiation (10x power savings). They showed various reference boards with different form factors. One in particular caught my eye and that was a PCIe card hosting eight of their chips. I am guessing they are targeting power-restricted edge servers. Keep in mind all server companies are building edge servers that are only slightly bigger and thicker than an iPad, with hardened enclosure and having a fan is not an option.

While the power advantage of their analog approach is undisputable, this advantage becomes less pronounced when the competitors opt to use finer process nodes. Despite such a competitive threat, I have never come across a startup with innovative technology that has been able to maintain their lead without reinvention. My guess is that Mythic will continue to innovate.

&amp;#x200B;

[Xnor.ai](https://Xnor.ai)

[Xnor.ai](https://Xnor.ai) ([www.xnor.ai](https://www.xnor.ai)) is a company based in Seattle that has developed very low footprint and optimized Deep Learning CNN models for computer vision tasks. These models can run on low-power SoCs with embedded Arm CPUs or dedicated computer vision hardware. Their initial target markets are Home Security, Home Automation, and Smart Appliances. They seem to have a close partnership with Ambarella. The beauty of their story is that they can enable legacy hardware to tackle complex vision processing tasks such as object, or face identification as well as more complex tasks dealing with live video feeds. As an example, large video surveillance camera companies can add AI-based vision processing features without major hardware rework.

I would guess their technology is a godsent for traditional MCU companies (the likes of Microchip, On Semi., Silicon Labs, and ST) that are yet to have MCUs with dedicated AI acceleration hardware.

Like any other upstart, they also have their challenges. Computer vision models are getting bigger and more complex and there are limits on what optimization and compression can achieve.

&amp;#x200B;

&amp;#x200B;

Hailo

Hailo ([www.hailo.ai](https://www.hailo.ai)) is an Israeli company that has been able to accomplish a tremendous amount with relatively limited funding (\~$25M I think?). I saw live demos of their Hailo-8 chip and was impressed by its performance metrics. Tiny chip with no heat sink. Impressive peak performance of 26 TOPs and 2.8 TOPs/W (for an AI edge inference accelerator).

Although they are targeting most edge applications (autonomous vehicles, smart cameras, smartphones, drones, AR/VR), I got the sense that they have a special affinity toward the automotive segment. This makes sense since their performance is among the highest that I have seen (among edge inference chips) and no other edge application values performance more than the automotive segment. Alternatively, they may have been inspired by Intels acquisition of Mobileye for $15B. These are merely my guesses and both reasons are perfectly valid in my book.

Aside from the technical mumbo jumbo, their unwavering focus, infectious optimism, and determination impressed me the most.

&amp;#x200B;

Al Gharakhanian

Check out prior posts [here](https://aihardwareupdate.com)",0,1
1667,2019-5-28,2019,5,28,8,btt23y,Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/btt23y/ordered_neurons_integrating_tree_structures_into/,vochicong,1559000735,,5,35
1668,2019-5-28,2019,5,28,9,bttg2n,"Call for papers: The 10th International Conference on Learning Analytics &amp; Knowledge (LAK20), March 23-27, 2020, Frankfurt, Germany.",https://www.reddit.com/r/MachineLearning/comments/bttg2n/call_for_papers_the_10th_international_conference/,vkovanovic,1559003115,,0,1
1669,2019-5-28,2019,5,28,10,btty8p,Supervised VS Unsupervised Machine Learning - Which Works Better For Your Company?,https://www.reddit.com/r/MachineLearning/comments/btty8p/supervised_vs_unsupervised_machine_learning_which/,datayes123,1559006578,,0,1
1670,2019-5-28,2019,5,28,10,btu3n3,Feature selection for unbalanced data set?,https://www.reddit.com/r/MachineLearning/comments/btu3n3/feature_selection_for_unbalanced_data_set/,I_haven-t_reddit,1559007590,[removed],0,1
1671,2019-5-28,2019,5,28,10,btu59n,[P] GAN Music Generator,https://www.reddit.com/r/MachineLearning/comments/btu59n/p_gan_music_generator/,scriptcoder43,1559007879,"Hi MachineLearning,

I have also created a GAN music generator (seen a few on here in the past few months)

Link: [https://hookgen.com](https://hookgen.com)

It is basically a GAN in TensorFlow r1.13, however, I have two separate discriminative networks, i.e. one network feeds the piano score, while the other the duration the song was played on the website in milliseconds

Please let me know if you have any questions

I will deploy the code tomorrow on GitHub if you have any interest",46,138
1672,2019-5-28,2019,5,28,11,btun5c,"Call for papers: The 10th International Conference on Learning Analytics &amp; Knowledge (LAK20), March 23-27, 2020, Frankfurt, Germany",https://www.reddit.com/r/MachineLearning/comments/btun5c/call_for_papers_the_10th_international_conference/,VitomirKovanovic,1559011048,,0,1
1673,2019-5-28,2019,5,28,11,btuuqa,[1905.09381] Learning to Prove Theorems via Interacting with Proof Assistants,https://www.reddit.com/r/MachineLearning/comments/btuuqa/190509381_learning_to_prove_theorems_via/,zhamisen,1559012335,,11,5
1674,2019-5-28,2019,5,28,12,btv4ic,Algorithm idea,https://www.reddit.com/r/MachineLearning/comments/btv4ic/algorithm_idea/,Yashbahl,1559014039,[removed],0,1
1675,2019-5-28,2019,5,28,13,btvv2d,"Robocrop can pick 25,000 raspberries a day",https://www.reddit.com/r/MachineLearning/comments/btvv2d/robocrop_can_pick_25000_raspberries_a_day/,finphil,1559019162,,0,1
1676,2019-5-28,2019,5,28,14,btwdug,What are some comprehensive lists of successful ML startups in 2019?,https://www.reddit.com/r/MachineLearning/comments/btwdug/what_are_some_comprehensive_lists_of_successful/,rodrigonader,1559023039,[removed],0,1
1677,2019-5-28,2019,5,28,15,btwi4f,The gradient of matrix w.r.t. vector,https://www.reddit.com/r/MachineLearning/comments/btwi4f/the_gradient_of_matrix_wrt_vector/,LLjiang,1559023950,[removed],0,1
1678,2019-5-28,2019,5,28,16,btwyb7,AI in industrial robotics,https://www.reddit.com/r/MachineLearning/comments/btwyb7/ai_in_industrial_robotics/,Imsuri9,1559027544,[removed],0,1
1679,2019-5-28,2019,5,28,16,btx02k,Is text generation through GAN possible if so how can someone guide me to some resource?,https://www.reddit.com/r/MachineLearning/comments/btx02k/is_text_generation_through_gan_possible_if_so_how/,aviknayak,1559027952,[removed],0,1
1680,2019-5-28,2019,5,28,16,btx0l4,Telescopic Forks for Automated Warehouse AS/RS | Automated AS/RS Systems,https://www.reddit.com/r/MachineLearning/comments/btx0l4/telescopic_forks_for_automated_warehouse_asrs/,lhd121,1559028069,,0,1
1681,2019-5-28,2019,5,28,16,btx1n2,AI in industrial Robotics,https://www.reddit.com/r/MachineLearning/comments/btx1n2/ai_in_industrial_robotics/,Imsuri9,1559028306,[removed],0,1
1682,2019-5-28,2019,5,28,16,btx3zu,Telescopic Forks for Automated Warehouse AS/RS | Automated AS/RS Systems,https://www.reddit.com/r/MachineLearning/comments/btx3zu/telescopic_forks_for_automated_warehouse_asrs/,lhd121,1559028856,[removed],0,1
1683,2019-5-28,2019,5,28,16,btx5dh,[R] Triplet loss for image retrieval,https://www.reddit.com/r/MachineLearning/comments/btx5dh/r_triplet_loss_for_image_retrieval/,kdhht2334,1559029188,"Hi, there!

  
This is an example of image retrieval based on MNIST and fashion-MNIST!  

&amp;#x200B;

After learning by using typical triplet loss (a type of metric loss), we checked the retrieval results for the query image.  

Despite simple learning, the results show fairly accurate retrieval results. 

We have also shown the embedding space for the learned network in various ways.  

If you are interested in doing research or metric learning, you might want to refer to it.

[https://github.com/kdhht2334/Triplet\_loss\_for\_image\_retrieval](https://github.com/kdhht2334/Triplet_loss_for_image_retrieval)",0,4
1684,2019-5-28,2019,5,28,16,btx8uk,"[P] Demo of ""Arbitrary Style Transfer with Style-Attentional Network""",https://www.reddit.com/r/MachineLearning/comments/btx8uk/p_demo_of_arbitrary_style_transfer_with/,likebullet86,1559030019,"Hi MachineLearning

I'll introduce awsome style transfer demo site.

this demo site implementation of  ""Arbitrary Style Transfer with Style-Attentional Network"" (CVPR 2019).

\- project page: [https://dypark86.github.io/SANET/](https://dypark86.github.io/SANET/)

\- Fine Tuning Version Demo Site: [http://style.airi.kr/demo/](http://style.airi.kr/demo/?fbclid=IwAR30BqHYkg3iPKFvhNwN_5xqrFhHHUYfHgdmN15GlNZ1p-tqHi23P9FYWQY)

\- Paper Version Demo Site: [http://style.airi.kr/ori\_demo/](http://style.airi.kr/demo/?fbclid=IwAR30BqHYkg3iPKFvhNwN_5xqrFhHHUYfHgdmN15GlNZ1p-tqHi23P9FYWQY)

\- Paper Arxiv:  [https://arxiv.org/pdf/1812.02342.pdf](https://arxiv.org/pdf/1812.02342.pdf?fbclid=IwAR3I33jqbvN7e5aiy0odBpa8WJ--DzyEmLCH8cUlDKNYrdPSAs0WQodfhPg)

If you were interested, please share these post.

Thanks!

&amp;#x200B;

![img](xoajj8cuow031)",11,23
1685,2019-5-28,2019,5,28,17,btxhtt,[D] Does anyone created a formal database for word meaning?,https://www.reddit.com/r/MachineLearning/comments/btxhtt/d_does_anyone_created_a_formal_database_for_word/,moshestv,1559032164,"I'm looking for a database that has sufficient information to mathematically prove things like: ""\[young man\] == \[a boy\] == \[male child\]"".

This database should have basic information such as ""\[a king\] is \[male\], \[person\], \[sovereign\], \[ruler\], etc.."" but also strict definitions like ""\[quick\] == \[moving fast\] or \[in short time\]"".

Any thoughts?",5,0
1686,2019-5-28,2019,5,28,17,btxkwc,What are some things that you wish you knew before starting out in the domain of Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/btxkwc/what_are_some_things_that_you_wish_you_knew/,prateekarma,1559032888,[removed],0,1
1687,2019-5-28,2019,5,28,17,btxmg7,Request and sell data on our new Data Market,https://www.reddit.com/r/MachineLearning/comments/btxmg7/request_and_sell_data_on_our_new_data_market/,apachemilo,1559033244,"We've run a community for anyone interested in tech with a focus on making money, and if you want to sell data you've gathered and cleaned up, or if you're looking for someone to mine a specific data for you, you can create a listing on our new data market.

&amp;#x200B;

The first listing on our market has been a dataset of over 5,000 cryptocurrency ICO, STO and IEO's, and we take listings and requests for data relating to fields such as AI, blockchain, virtual and augmented reality, 3d printing and drones.

&amp;#x200B;

PM for a link to the market and our community (I don't want to spam a link publicly and have the posts removed).",0,1
1688,2019-5-28,2019,5,28,18,btxrpw,Advances in Machine Learning Research,https://www.reddit.com/r/MachineLearning/comments/btxrpw/advances_in_machine_learning_research/,andrea_manero,1559034450,[removed],0,1
1689,2019-5-28,2019,5,28,19,bty7fr,LEDNet Architecture implementation,https://www.reddit.com/r/MachineLearning/comments/bty7fr/lednet_architecture_implementation/,aniket_agarwal,1559037869,[removed],0,1
1690,2019-5-28,2019,5,28,19,btyg9n,[D]HELP in machine learning,https://www.reddit.com/r/MachineLearning/comments/btyg9n/dhelp_in_machine_learning/,Vaibhav_37,1559039711,I am so much confused and tired for looking online courses...Idk where to start and what to do..i tried udacity intro to machine learning but they use python 2 and it gets very hectic to change to python 3 all time ... Neither the teachers teach well ...Pls guide me ....im a beginner,3,0
1691,2019-5-28,2019,5,28,19,btyhfq,Looking for NN/ML c++ library recomendations for classifier,https://www.reddit.com/r/MachineLearning/comments/btyhfq/looking_for_nnml_c_library_recomendations_for/,dashingstag,1559039956,[removed],0,1
1692,2019-5-28,2019,5,28,19,btyhig,Machine Learning Training from Industry Expert,https://www.reddit.com/r/MachineLearning/comments/btyhig/machine_learning_training_from_industry_expert/,hpsdua,1559039972,"  

Hands-On **#Machine Learning #Online** **#Training** by **#IndustryExperts**. Experience the difference.

==============================

**#training** **#classroomtraining** **#onlinetraining** **#projectwork** in **#emergingtechnologies** :

==============================

**#datascience** **#businessanalytics** **#predictiveanalytics** **#machinelearning** **#naturallanguageprocessing** **#textmining** **#artificialintelligence** **#deeplearning**

**#datavisualisation** **#statistical** **#modelling** **#textmining** **#analytics #R #Python #advanceExcel #SQL**

BDS Research ( The Analytics People )

\+91-9815204025

data.bdsr@gmail.com",0,1
1693,2019-5-28,2019,5,28,19,btyj35,Is a PhD in Evolutionary Computation worth it?,https://www.reddit.com/r/MachineLearning/comments/btyj35/is_a_phd_in_evolutionary_computation_worth_it/,actuallynotcanadian,1559040286,[removed],0,1
1694,2019-5-28,2019,5,28,19,btyl6z,[R] [OC] Intrinsic Multi-scale Evaluation of Generative Models,https://www.reddit.com/r/MachineLearning/comments/btyl6z/r_oc_intrinsic_multiscale_evaluation_of/,olBaa,1559040711,"Generative models are often used to sample high-dimensional data points from a manifold with small intrinsic dimension. Existing techniques for comparing generative models focus on global data properties such as mean and covariance; in that sense, they are extrinsic and uni-scale. We develop the first, to our knowledge, intrinsic and multi-scale method for characterizing and comparing underlying data manifolds, based on comparing all data moments by lower-bounding the spectral notion of the Gromov-Wasserstein distance between manifolds. In a thorough experimental study, we demonstrate that our method effectively evaluates the quality of generative models; further, we showcase its efficacy in  discerning the disentanglement process in neural networks.

TL;DR: We introduce a metric based on heat kernels that is able to compare data manifolds in unaligned spaces. We test it on GAN evaluation (it works), and on tracking the disentanglement and training progress of NNs (also works!).

ArXiv page: https://arxiv.org/abs/1905.11141
Code is available here: https://github.com/xgfs/msid
I am here to answer your questions about the paper! :)

Extra shoutout to /u/augustushimself who wrote a brilliant GAN post, and the discussion that followed on this sub. It gave a lot of inspiration for writing this.",0,24
1695,2019-5-28,2019,5,28,20,btywjp,Searching for a bounding box creator(not classifier),https://www.reddit.com/r/MachineLearning/comments/btywjp/searching_for_a_bounding_box_creatornot_classifier/,ZiePeregrine,1559042969,[removed],0,1
1696,2019-5-28,2019,5,28,20,btywr2,The Future of Machine Learning Under Hoods of Google Is Ingenious | Analytics Insight,https://www.reddit.com/r/MachineLearning/comments/btywr2/the_future_of_machine_learning_under_hoods_of/,analyticsinsight,1559043010,,0,1
1697,2019-5-28,2019,5,28,20,btz406,Reconstructing Faces from Voices,https://www.reddit.com/r/MachineLearning/comments/btz406/reconstructing_faces_from_voices/,taurish,1559044366,[removed],0,1
1698,2019-5-28,2019,5,28,21,btz94z,"Came across a post where /u/_hypervguy asked why a user loathed windows development. I wanted to respond, but it's too late.",https://www.reddit.com/r/MachineLearning/comments/btz94z/came_across_a_post_where_u_hypervguy_asked_why_a/,halfassadmin,1559045297,[removed],0,1
1699,2019-5-28,2019,5,28,21,btza53,[P] Intro to word2vec recommenders and how Airbnb and Alibaba use them [video],https://www.reddit.com/r/MachineLearning/comments/btza53/p_intro_to_word2vec_recommenders_and_how_airbnb/,nortab,1559045471,"Hello r/machinelearning,

&amp;#x200B;

This talk is my best attempt to strip down the core concepts of word2vec, and explain how the algorithm is being to power recommendation engines. The power of recent NLP models is a testament to how far we've come in extracting patterns from sequential data. This application of the algorithm treats other sequences of data (e.g. website click sessions, songs in user-created playlists) as sentences leading us to create embeddings (for items in an ecommerce store, or songs/artists in a music service) that we can use for similarity and recommendation. My \[Illustrated Word2vec\]([https://jalammar.github.io/illustrated-word2vec/](https://jalammar.github.io/illustrated-word2vec/)) post built on the materials I created for this talk. I hope you find it useful:

&amp;#x200B;

[https://www.infoq.com/presentations/nlp-word-embedding](https://www.infoq.com/presentations/nlp-word-embedding)",5,36
1700,2019-5-28,2019,5,28,21,btzbmw,Emphasis Regularisation by Gradient Rescaling for Training Deep Neural Networks with Noisy Labels,https://www.reddit.com/r/MachineLearning/comments/btzbmw/emphasis_regularisation_by_gradient_rescaling_for/,XinshaoWang,1559045745,[removed],2,1
1701,2019-5-28,2019,5,28,21,btzemq,[R] What the Vec? Towards Probabilistically Grounded Embeddings,https://www.reddit.com/r/MachineLearning/comments/btzemq/r_what_the_vec_towards_probabilistically_grounded/,ibalazevic,1559046294,"&amp;#x200B;

https://i.redd.it/dlku03uq2y031.png

**TL;DR: This is why word2vec works.**

&amp;#x200B;

**Paper:** [https://arxiv.org/pdf/1805.12164.pdf](https://arxiv.org/pdf/1805.12164.pdf)

&amp;#x200B;

**Abstract:**

Word2Vec (W2V) and Glove are popular word embedding algorithms that perform well on a variety of natural language processing tasks. The algorithms are fast, efficient and their embeddings widely used. Moreover, the W2V algorithm has recently been adopted in the field of graph embedding, where it underpins several leading algorithms. However, despite their ubiquity and the relative simplicity of their common architecture, *what* the embedding parameters of W2V and Glove learn and *why* that it useful in downstream tasks largely remains a mystery. We show that different interactions of *PMI vectors* encode semantic properties that can be captured in low dimensional word embeddings by suitable projection, theoretically explaining why the embeddings of W2V and Glove work, and, in turn, revealing an interesting mathematical interconnection between the semantic relationships of relatedness, similarity, paraphrase and analogy.

&amp;#x200B;

**Key contributions:**

* to show that semantic similarity is captured by high dimensional PMI vectors and, by considering geometric and probabilistic aspects of such vectors and their domain, to establish a hierarchical mathematical interrelationship between relatedness, similarity, paraphrases and analogies;
* to show that these semantic properties arise through additive interactions and so are best captured in low dimensional word embeddings by linear projection, thus explaining, by comparison of their loss functions, the presence of semantic properties in the embeddings of W2V and Glove;
* to derive a relationship between learned embedding matrices, proving that they necessarily differ (in the real domain), justifying the heuristic use of their mean, showing that different interactions are required to extract different semantic information, and enabling popular embedding comparisons, such as cosine similarity, to be semantically interpreted.",9,41
1702,2019-5-28,2019,5,28,21,btzewo,Looking for an Apple/Orange type data set,https://www.reddit.com/r/MachineLearning/comments/btzewo/looking_for_an_appleorange_type_data_set/,lucidmath,1559046341,[removed],0,1
1703,2019-5-28,2019,5,28,21,btzf8h,Improving MAE against CCE under Label Noise,https://www.reddit.com/r/MachineLearning/comments/btzf8h/improving_mae_against_cce_under_label_noise/,XinshaoWang,1559046405,[removed],2,1
1704,2019-5-28,2019,5,28,21,btzmgi,Ranked List Loss for Deep Metric Learning,https://www.reddit.com/r/MachineLearning/comments/btzmgi/ranked_list_loss_for_deep_metric_learning/,XinshaoWang,1559047649,[removed],0,1
1705,2019-5-28,2019,5,28,21,btzqda,Deep Metric Learning by Online Soft Mining and Class-Aware Attention,https://www.reddit.com/r/MachineLearning/comments/btzqda/deep_metric_learning_by_online_soft_mining_and/,XinshaoWang,1559048334,[removed],0,1
1706,2019-5-28,2019,5,28,22,btzr4a,Adadelta and Adegrad on Keras,https://www.reddit.com/r/MachineLearning/comments/btzr4a/adadelta_and_adegrad_on_keras/,SOUINnnn,1559048456,[removed],0,1
1707,2019-5-28,2019,5,28,22,btzre0,Paper recommendations for image recognition with lots of classes?,https://www.reddit.com/r/MachineLearning/comments/btzre0/paper_recommendations_for_image_recognition_with/,rowsiearruba,1559048508,[removed],0,1
1708,2019-5-28,2019,5,28,22,bu04ee,The Whole Data Science World in Your Hands,https://www.reddit.com/r/MachineLearning/comments/bu04ee/the_whole_data_science_world_in_your_hands/,faviovaz,1559050622,,0,1
1709,2019-5-28,2019,5,28,22,bu05ua,[P] Playing SuperMario Bros. without knowing any scores using flow-based curiosity method,https://www.reddit.com/r/MachineLearning/comments/bu05ua/p_playing_supermario_bros_without_knowing_any/,Kanahei,1559050858,"Hi MachineLearning

&amp;#x200B;

I'll introduce a new method to play SuperMario Bros. using RL agent and without knowing the scores from the environment (pure exploration). We employ optical flow for evaluating the novelty of states to guide the RL agent.

&amp;#x200B;

I hope you find it interesting:

&amp;#x200B;

Here are some links:

\- Demo video: [https://www.youtube.com/watch?v=w-a6akKpWT0](https://www.youtube.com/watch?v=w-a6akKpWT0)

\- Github: [https://github.com/hellochick/MarioO\_O-flow-curioisty](https://github.com/hellochick/MarioO_O-flow-curioisty)

\- Arxiv: [https://arxiv.org/abs/1905.10071](https://arxiv.org/abs/1905.10071)",20,135
1710,2019-5-28,2019,5,28,23,bu0g3s,Can't pickle generator objects,https://www.reddit.com/r/MachineLearning/comments/bu0g3s/cant_pickle_generator_objects/,Competitive_Mongoose,1559052440,[removed],0,1
1711,2019-5-28,2019,5,28,23,bu0jxb,Generative Grading: Neural Approximate Parsing for Automated Student Feedback,https://www.reddit.com/r/MachineLearning/comments/bu0jxb/generative_grading_neural_approximate_parsing_for/,SkiddyX,1559053002,,0,1
1712,2019-5-28,2019,5,28,23,bu0k5o,[R] Generative Grading: Neural Approximate Parsing for Automated Student Feedback,https://www.reddit.com/r/MachineLearning/comments/bu0k5o/r_generative_grading_neural_approximate_parsing/,SkiddyX,1559053036,,1,2
1713,2019-5-28,2019,5,28,23,bu0oe1,What does ethics mean to artificial intelligence? | Would love some professional opinions,https://www.reddit.com/r/MachineLearning/comments/bu0oe1/what_does_ethics_mean_to_artificial_intelligence/,klyndonlee,1559053702,,1,2
1714,2019-5-28,2019,5,28,23,bu0x39,[Research] Towards Safety-Aware Computing System Design in Autonomous Vehicles,https://www.reddit.com/r/MachineLearning/comments/bu0x39/research_towards_safetyaware_computing_system/,cdossman,1559054969," [https://medium.com/ai%C2%B3-theory-practice-business/ai-scholar-building-safety-aware-computing-system-design-in-autonomous-vehicles-d7065f4239fe](https://medium.com/ai%C2%B3-theory-practice-business/ai-scholar-building-safety-aware-computing-system-design-in-autonomous-vehicles-d7065f4239fe) 

Abstract:  Recently, autonomous driving development ignited competition among car makers and technical corporations. Low-level automation cars are already commercially available. But high automated vehicles where the vehicle drives by itself without human monitoring is still at infancy. Such autonomous vehicles (AVs) rely on the computing system in the car to interpret the environment and make driving decisions. Therefore, computing system design is essential particularly in enhancing the attainment of driving safety. However, to our knowledge, no clear guideline exists so far regarding safety-aware AV computing system and architecture design. To understand the safety requirement of AV computing system, we performed a field study by running industrial Level-4 autonomous driving fleets in various locations, road conditions, and traffic patterns. The field study indicates that traditional computing system performance metrics, such as tail latency, average latency, maximum latency, and timeout, cannot fully satisfy the safety requirement for AV computing system design. To address this issue, we propose a safety score as a primary metric for measuring the level of safety in AV computing system design. Furthermore, we propose a perception latency model, which helps architects estimate the safety score of given architecture and system design without physically testing them in an AV. We demonstrate the use of our safety score and latency model, by developing and evaluating a safety-aware AV computing system computation hardware resource management scheme.",0,2
1715,2019-5-28,2019,5,28,23,bu0zbg,Learning Deep Learning framework,https://www.reddit.com/r/MachineLearning/comments/bu0zbg/learning_deep_learning_framework/,bikanation,1559055301,[removed],0,1
1716,2019-5-29,2019,5,29,0,bu14ci,"Live demo of a dataset (""NSFW"" ML trained model) shared/monetized through Ethereum",https://www.reddit.com/r/MachineLearning/comments/bu14ci/live_demo_of_a_dataset_nsfw_ml_trained_model/,jbrg,1559055981,,0,1
1717,2019-5-29,2019,5,29,0,bu1bpt,Recommend a text book,https://www.reddit.com/r/MachineLearning/comments/bu1bpt/recommend_a_text_book/,drmayorga,1559057039,[removed],0,1
1718,2019-5-29,2019,5,29,0,bu1cby,[D] Self normalizing weight and activations,https://www.reddit.com/r/MachineLearning/comments/bu1cby/d_self_normalizing_weight_and_activations/,kosongsatunorm,1559057123,"I need to train a classifier and use the last linear layer as embedding for other stuff. If possible I want the weights always constrained to -1 and 1 during training.

Is there paper that shows method to update weights so that all weights have 0 mean and 1 variance? Does the function weight_norm in pytorch actually does that?

I read paper Self Normalizing Neural Networks but only activations are normalized to N(0, 1)

And speaking of self normalizing outputs, does anyone made a network that explicitly output values already approximately close to softmax without calculating softmax with logits?

Thanks",5,2
1719,2019-5-29,2019,5,29,0,bu1dyc,"OReilly give away: Achieving Real Business Outcomes from Artificial Intelligence by Atif Kureishy, Chad Meley &amp; Ben Mackenzie",https://www.reddit.com/r/MachineLearning/comments/bu1dyc/oreilly_give_away_achieving_real_business/,dafrogspeaks,1559057356,,0,1
1720,2019-5-29,2019,5,29,1,bu1xqs,[P] TensorWatch: A tool for debugging and visualization of any tensor stream. [Microsoft Research],https://www.reddit.com/r/MachineLearning/comments/bu1xqs/p_tensorwatch_a_tool_for_debugging_and/,violentdeli8,1559060170, [https://github.com/microsoft/tensorwatch](https://github.com/microsoft/tensorwatch),0,17
1721,2019-5-29,2019,5,29,1,bu223l,ML for blood tests?,https://www.reddit.com/r/MachineLearning/comments/bu223l/ml_for_blood_tests/,rca06d,1559060755,[removed],0,1
1722,2019-5-29,2019,5,29,1,bu23gv,DeepMind Proposes a Novel Way to Improve GANs Using Gradient Information,https://www.reddit.com/r/MachineLearning/comments/bu23gv/deepmind_proposes_a_novel_way_to_improve_gans/,Yuqing7,1559060938,,0,1
1723,2019-5-29,2019,5,29,1,bu25wl,Looking for explainability techniques for decision trees/forests,https://www.reddit.com/r/MachineLearning/comments/bu25wl/looking_for_explainability_techniques_for/,vexillology-nerd,1559061266,[removed],0,1
1724,2019-5-29,2019,5,29,1,bu283q,Does JMLR receive short library implementation papers?,https://www.reddit.com/r/MachineLearning/comments/bu283q/does_jmlr_receive_short_library_implementation/,Red-Portal,1559061557,[removed],0,1
1725,2019-5-29,2019,5,29,1,bu2ez7,[R] Training decision trees as replacement for convolution layers: 98.41% on CIFAR10 without data augmentation,https://www.reddit.com/r/MachineLearning/comments/bu2ez7/r_training_decision_trees_as_replacement_for/,downtownslim,1559062478,,12,41
1726,2019-5-29,2019,5,29,2,bu2kqz,Project Idea - Smart Billiards Score Counter with Smart Cameras,https://www.reddit.com/r/MachineLearning/comments/bu2kqz/project_idea_smart_billiards_score_counter_with/,Omgfunsies,1559063236,[removed],0,1
1727,2019-5-29,2019,5,29,2,bu2lzg,MeanSum: A Neural Model for Unsupervised Multi-document Abstractive Summarization,https://www.reddit.com/r/MachineLearning/comments/bu2lzg/meansum_a_neural_model_for_unsupervised/,backprop13,1559063391,[https://arxiv.org/abs/1810.05739](https://arxiv.org/abs/1810.05739),0,1
1728,2019-5-29,2019,5,29,2,bu2owh,[News] Google announces tensorflow graphics library for unsupervised deep learning,https://www.reddit.com/r/MachineLearning/comments/bu2owh/news_google_announces_tensorflow_graphics_library/,_guru007,1559063772,,0,1
1729,2019-5-29,2019,5,29,2,bu2s0r,[P] Emotional Game with Game of Thrones (with face detection and emotion recognition),https://www.reddit.com/r/MachineLearning/comments/bu2s0r/p_emotional_game_with_game_of_thrones_with_face/,agasitko,1559064192,,1,2
1730,2019-5-29,2019,5,29,2,bu2uli,[R] Discrete Flows: Invertible Generative Models of Discrete Data,https://www.reddit.com/r/MachineLearning/comments/bu2uli/r_discrete_flows_invertible_generative_models_of/,____jelly_time____,1559064531,,1,7
1731,2019-5-29,2019,5,29,2,bu32bv,[R] Levenshtein Transformer - flexible autoregressive model for sequence generation,https://www.reddit.com/r/MachineLearning/comments/bu32bv/r_levenshtein_transformer_flexible_autoregressive/,vackosar,1559065569,,4,6
1732,2019-5-29,2019,5,29,3,bu3iua,Stop hating on Siraj,https://www.reddit.com/r/MachineLearning/comments/bu3iua/stop_hating_on_siraj/,codininja1337,1559067805,[removed],0,1
1733,2019-5-29,2019,5,29,3,bu3mn2,Machine Learning Algorithms Cheat Sheet,https://www.reddit.com/r/MachineLearning/comments/bu3mn2/machine_learning_algorithms_cheat_sheet/,ai-lover,1559068322,,0,1
1734,2019-5-29,2019,5,29,3,bu3n0f,AI in industrial robotics,https://www.reddit.com/r/MachineLearning/comments/bu3n0f/ai_in_industrial_robotics/,Imsuri9,1559068374,"I'm looking for project or paper where industrial robots like kuka,Fanuc can perform auto path planning using computer vision and ML algorithm.
TIA",0,1
1735,2019-5-29,2019,5,29,4,bu4a20,[D] K-FAC for Transformer?,https://www.reddit.com/r/MachineLearning/comments/bu4a20/d_kfac_for_transformer/,HigherTopoi,1559071436,"K-FAC is very efficient on training on Imagenet ([link](https://mc.ai/introducing-k-fac-and-its-application-for-large-scale-deep-learning/)). There is also a paper in which K-FAC outperforms Adam on LSTM LM. So, has anyone tried K-FAC on Transformer?",0,3
1736,2019-5-29,2019,5,29,4,bu4bfl,Python Tutorial for Beginners | Python Full Course From Scratch | Python Programming,https://www.reddit.com/r/MachineLearning/comments/bu4bfl/python_tutorial_for_beginners_python_full_course/,dafynrl,1559071626,,0,1
1737,2019-5-29,2019,5,29,4,bu4jd3,[D] Andrew Ng on how much silicon valley ML engineers know,https://www.reddit.com/r/MachineLearning/comments/bu4jd3/d_andrew_ng_on_how_much_silicon_valley_ml/,snapp55,1559072680,"In Ng's course on Machine Learning he said something that I was very shocked by. 

After the logistic regression modules, which I think is module 3 or 4, he said something along the lines of ""If you understand everything up until this point then you know as much as most of the ML engineers making lots of money in silicon valley."" 

Then I come on here and most everyone has PhD's and knows well beyond what Ng's course teaches yet I see posts about how hard of a time you are having getting a job in the ML career field. 

What am I missing, and what is the average academic background of the people in this sub?

&amp;#x200B;

Thanks",129,394
1738,2019-5-29,2019,5,29,4,bu4jo3,DataOps Principles: How Startups Do Data The Right Way - Retina.ai,https://www.reddit.com/r/MachineLearning/comments/bu4jo3/dataops_principles_how_startups_do_data_the_right/,phlogisticfugu,1559072719,,0,1
1739,2019-5-29,2019,5,29,4,bu4oum,List of Data Science and Machine Learning GitHub Repositories to Try in 2019,https://www.reddit.com/r/MachineLearning/comments/bu4oum/list_of_data_science_and_machine_learning_github/,ai-lover,1559073414,[removed],0,1
1740,2019-5-29,2019,5,29,5,bu4rd9,[R] MicroNet Challenge @ NeurIPS 2019,https://www.reddit.com/r/MachineLearning/comments/bu4rd9/r_micronet_challenge_neurips_2019/,tjgale,1559073738,"The neural networks we build are designed for the hardware we have, and the hardware we build is designed for the neural networks we have. Help break the wheel  build networks that influence the design of future hardware. 

[https://micronet-challenge.github.io/](https://micronet-challenge.github.io/)

&amp;#x200B;

We have Google Cloud TPU credits available for the first 25 participants that submit a proposal! 1-page limit. Describe what youd like to do for your entry, and email your proposals to [micronet.challenge@gmail.com](mailto:micronet.challenge@gmail.com).

&amp;#x200B;

Follow us on twitter ([@unet\_challenge](https://twitter.com/unet_challenge)) for updates as the competition progresses!",4,32
1741,2019-5-29,2019,5,29,5,bu4yf4,Intro to ML: A great video that explains the basics of how this stuff works!,https://www.reddit.com/r/MachineLearning/comments/bu4yf4/intro_to_ml_a_great_video_that_explains_the/,mostarz93,1559074689,,0,1
1742,2019-5-29,2019,5,29,5,bu4zfp,My 4 tutorial series for creating your very own neural network with Keras that learns to play a game is complete! You can try it on your own PC as well.,https://www.reddit.com/r/MachineLearning/comments/bu4zfp/my_4_tutorial_series_for_creating_your_very_own/,theguy2108,1559074828,[removed],0,1
1743,2019-5-29,2019,5,29,7,bu6syt,"[R] AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence",https://www.reddit.com/r/MachineLearning/comments/bu6syt/r_aigas_aigenerating_algorithms_an_alternate/,wei_jok,1559084051,,6,4
1744,2019-5-29,2019,5,29,8,bu7gmh,Regarding Sentiment Analysis: Is it practical or possibly helpful to train a ML model by manually assigning sentiment to short phrases and evaluating the results against them?,https://www.reddit.com/r/MachineLearning/comments/bu7gmh/regarding_sentiment_analysis_is_it_practical_or/,sentiment_analysis1,1559087540,[removed],0,1
1745,2019-5-29,2019,5,29,9,bu7nvu,[D]Apply machine learning in database design,https://www.reddit.com/r/MachineLearning/comments/bu7nvu/dapply_machine_learning_in_database_design/,GeForceKawaiiyo,1559088646,"Hello, I've met some problems recently. Here it is: 
Given a database, and there are a few tables in it. And also data dictionaries, comments, data types and so on are given. What can we do to automatically or semi-automatically extract E-R diagram (or table relationships) from these tables using ML or NLP methods?
Tables can also be as input since your method  can detect some attributes as 
I read some papers and found that not really many people are doing researchs in these fields. Many of these papers are really old. Because I'm really new to NLP, I 'm not sure how such problems are handled in NLP areas. Can anyone share some insights? Much appreciated.",0,1
1746,2019-5-29,2019,5,29,9,bu7y3l,[P] PiciT: A Powerful Face Crop Tool(Android),https://www.reddit.com/r/MachineLearning/comments/bu7y3l/p_picit_a_powerful_face_crop_toolandroid/,abhi_uno,1559090221,,0,1
1747,2019-5-29,2019,5,29,10,bu8drz,"[R] Network Deconvolution (a strong alternative to BatchNorm, with code)",https://www.reddit.com/r/MachineLearning/comments/bu8drz/r_network_deconvolution_a_strong_alternative_to/,xternalz,1559092736,,6,9
1748,2019-5-29,2019,5,29,10,bu8gis,Build system for data science pipelines for faster iteration and reproducibility,https://www.reddit.com/r/MachineLearning/comments/bu8gis/build_system_for_data_science_pipelines_for/,gsvigruha,1559093181,,0,1
1749,2019-5-29,2019,5,29,10,bu8kp4,"[P] Build system for ML pipelines for faster iteration, testing and reproducibility",https://www.reddit.com/r/MachineLearning/comments/bu8kp4/p_build_system_for_ml_pipelines_for_faster/,gsvigruha,1559093853,,0,1
1750,2019-5-29,2019,5,29,10,bu8pnx,Intro to Artificial NN's Video For High School Math Class,https://www.reddit.com/r/MachineLearning/comments/bu8pnx/intro_to_artificial_nns_video_for_high_school/,kylexy32,1559094688,,0,1
1751,2019-5-29,2019,5,29,11,bu99mi,Training a model,https://www.reddit.com/r/MachineLearning/comments/bu99mi/training_a_model/,kanybekasanbekov,1559098022,[removed],0,1
1752,2019-5-29,2019,5,29,11,bu9aic,Extending a neural network to classify new objects,https://www.reddit.com/r/MachineLearning/comments/bu9aic/extending_a_neural_network_to_classify_new_objects/,brandojazz,1559098175,,0,1
1753,2019-5-29,2019,5,29,12,bu9oex,Strawberry Picking Robots - Traptic,https://www.reddit.com/r/MachineLearning/comments/bu9oex/strawberry_picking_robots_traptic/,lewisdotai,1559100512,[removed],0,1
1754,2019-5-29,2019,5,29,12,bu9r5n,Brand New to Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bu9r5n/brand_new_to_machine_learning/,rudyorre,1559100951,[removed],0,1
1755,2019-5-29,2019,5,29,12,bu9swi,Explanation of how BERT architecture gets 110M parameters,https://www.reddit.com/r/MachineLearning/comments/bu9swi/explanation_of_how_bert_architecture_gets_110m/,pascaltuna,1559101254,,0,1
1756,2019-5-29,2019,5,29,12,bu9vry,Dataset for ticks,https://www.reddit.com/r/MachineLearning/comments/bu9vry/dataset_for_ticks/,vreten,1559101756,[removed],0,1
1757,2019-5-29,2019,5,29,13,buacee,An introduction to Machine Learning for Physicists,https://www.reddit.com/r/MachineLearning/comments/buacee/an_introduction_to_machine_learning_for_physicists/,aiforworld2,1559104840,[removed],0,2
1758,2019-5-29,2019,5,29,13,buafzb,[D]: Grapheme-to-Phoneme (G2P) refinement ideas.,https://www.reddit.com/r/MachineLearning/comments/buafzb/d_graphemetophoneme_g2p_refinement_ideas/,chapter544,1559105552,"Hi all,
I am doing a project relating to G2P. I am looking at CMU seq2seq G2P project as a starting point. Basically, for the world ""Hello"", G2P gives ""HH EH L OW"", which is almost what I want. I want additional separation between distinct sounds, more like, ""HH EH - L OW"" (i.e, the additional ""-"" separator), or ""machine"" --&gt; ""M AH - SH IY N"" (instead of just ""M AH SH IY N""). Re-labeling the CMU training dictionary is one way, but I wonder if there is some simpler methods? Thank you!",1,1
1759,2019-5-29,2019,5,29,14,buaj0f,How difficult would it be to make an AI that optimizes the drag and downforce of a form? Or other aerodynamic properties,https://www.reddit.com/r/MachineLearning/comments/buaj0f/how_difficult_would_it_be_to_make_an_ai_that/,anjkkm,1559106185,,0,1
1760,2019-5-29,2019,5,29,14,buamx2,Has anyone used the Tesseract OCR to extract a table from a document?,https://www.reddit.com/r/MachineLearning/comments/buamx2/has_anyone_used_the_tesseract_ocr_to_extract_a/,bananaskywalker,1559106957,[removed],0,1
1761,2019-5-29,2019,5,29,14,buayci,NER from Excel data,https://www.reddit.com/r/MachineLearning/comments/buayci/ner_from_excel_data/,tarunn2799,1559109363,,0,1
1762,2019-5-29,2019,5,29,14,buaywd,Best Language for Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/buaywd/best_language_for_machine_learning/,rudyorre,1559109484,[removed],0,1
1763,2019-5-29,2019,5,29,15,bub7ew,Pattern based chess engine,https://www.reddit.com/r/MachineLearning/comments/bub7ew/pattern_based_chess_engine/,lifeinsrndpt,1559111275,[removed],0,1
1764,2019-5-29,2019,5,29,15,bubaov,How to handle multiple string features in dataset,https://www.reddit.com/r/MachineLearning/comments/bubaov/how_to_handle_multiple_string_features_in_dataset/,mitml,1559112008,[removed],0,1
1765,2019-5-29,2019,5,29,15,bubdpy,Looking for a nice LaTex CV template,https://www.reddit.com/r/MachineLearning/comments/bubdpy/looking_for_a_nice_latex_cv_template/,patty_nelsonn,1559112676,[removed],0,1
1766,2019-5-29,2019,5,29,16,bublbu,[R] Object Discovery with a Copy-Pasting GAN,https://www.reddit.com/r/MachineLearning/comments/bublbu/r_object_discovery_with_a_copypasting_gan/,iyaja,1559114360,,2,8
1767,2019-5-29,2019,5,29,16,bubmpl,Fair is Better than Sensational:Man is to Doctor as Woman is to Doctor,https://www.reddit.com/r/MachineLearning/comments/bubmpl/fair_is_better_than_sensationalman_is_to_doctor/,iyaja,1559114692,,44,15
1768,2019-5-29,2019,5,29,16,bubqbp,[D] The Rise of DataOps (from the ashes of Data Governance) Legacy Data Governance is broken in the ML era,https://www.reddit.com/r/MachineLearning/comments/bubqbp/d_the_rise_of_dataops_from_the_ashes_of_data/,thumbsdrivesmecrazy,1559115567,"With adding a consistent version system across all of the code the art of coding moved from craft to engineering - the same thing will happen to data governance: [https://towardsdatascience.com/the-rise-of-dataops-from-the-ashes-of-data-governance-da3e0c3ac2c4](https://towardsdatascience.com/the-rise-of-dataops-from-the-ashes-of-data-governance-da3e0c3ac2c4) (full article)

Currently, data governance teams attempt to apply manual control at various points to control the consistency and quality of the data. The introduction of Data Version Control (DVC) version tracking would allow data governance and engineering teams to engineer the data together, filing bugs against data versions, applying quality control checks to the data compilers, etc.

Platforms like [Palantir Foundry](https://www.palantir.com/palantir-foundry/) already treat the management of data in much the same way as versioning of code. Within data versioning platforms datasets can be versioned, branched, acted upon by versioned code to create new data sets. This enables data driven testing, where the data itself is tested in much the same way as that the code that modifies it.

There also some open source options:

\* [Data Version Control](https://dvc.org/) project is focused on data scientist users.

\* [Delta Lake](https://docs.databricks.com/delta/index.html) project is a DataBricks' version control system for data lakes with big data workloads.",1,0
1769,2019-5-29,2019,5,29,16,bubr2k,Document/Sequence ranking using multi head self attetntion,https://www.reddit.com/r/MachineLearning/comments/bubr2k/documentsequence_ranking_using_multi_head_self/,666BlackJesus666,1559115750,[removed],0,1
1770,2019-5-29,2019,5,29,16,bubr47,Challenges or hot topic in Machine learning and deep learning,https://www.reddit.com/r/MachineLearning/comments/bubr47/challenges_or_hot_topic_in_machine_learning_and/,milad_farzalizadeh,1559115763,[removed],0,1
1771,2019-5-29,2019,5,29,16,bubsu1,IEEE ban Huawei employees being reviewers and editors.,https://www.reddit.com/r/MachineLearning/comments/bubsu1/ieee_ban_huawei_employees_being_reviewers_and/,WeeklyTraining,1559116184," I always believe that **IEEE** belongs to all human beings, What do you think about this?

&amp;#x200B;

https://i.redd.it/1isni30cv3131.jpg",0,1
1772,2019-5-29,2019,5,29,16,bubtu9,Which machine learning algorithm should I use?,https://www.reddit.com/r/MachineLearning/comments/bubtu9/which_machine_learning_algorithm_should_i_use/,milad_farzalizadeh,1559116421,[removed],0,1
1773,2019-5-29,2019,5,29,18,buch1q,Huawei is banned by IEEE? Is it true?,https://www.reddit.com/r/MachineLearning/comments/buch1q/huawei_is_banned_by_ieee_is_it_true/,thyrix,1559122066,,1,1
1774,2019-5-29,2019,5,29,18,bucog1,Having a hard time with CentOS7 and TF installation,https://www.reddit.com/r/MachineLearning/comments/bucog1/having_a_hard_time_with_centos7_and_tf/,MagicElyas,1559123737,[removed],0,1
1775,2019-5-29,2019,5,29,19,bucwcb,Help required!,https://www.reddit.com/r/MachineLearning/comments/bucwcb/help_required/,notthecommonwealth,1559125469,[removed],0,1
1776,2019-5-29,2019,5,29,19,buczf9,"Oracle opens Riyadh tech hub: ""The Californian tech company Oracle has opened its first innovation hub in Saudi Arabia to drive the implementation of artificial intelligence, Internet of Things and other technology in the Kingdoms public and private sectors.""",https://www.reddit.com/r/MachineLearning/comments/buczf9/oracle_opens_riyadh_tech_hub_the_californian_tech/,craigbrownphd,1559126073,,0,1
1777,2019-5-29,2019,5,29,19,buczuh,AutoML E-Book,https://www.reddit.com/r/MachineLearning/comments/buczuh/automl_ebook/,aiforworld2,1559126160,[removed],0,1
1778,2019-5-29,2019,5,29,20,bud7d3,"Trying to do linear regression with TensorFlow, but all output is NaNs!",https://www.reddit.com/r/MachineLearning/comments/bud7d3/trying_to_do_linear_regression_with_tensorflow/,StarkGuy1234,1559127631,"I'm trying to do the below linear regression in TensorFlow, but my output is all Inf and NaNs.

My input dataset has to be Y=0.5*X + 2 + Noise; where X is a normal distribution of size(1000) and Noise is Gaussian with (mu=0.0 and sigma=50)

Output:

loss= 82662.945 W= 15974.369 b 24.379812

loss= 81293050000000.0 W= -508895600.0 b -775064.06

loss= 8.250697e+22 W= 16212403000000.0 b 24692003000.0

loss= 8.373905e+31 W= -5.1649487e+17 b -786638100000000.0

loss= inf W= 1.6454498e+22 b 2.5060722e+19

loss= inf W= -5.2420755e+26 b -7.9838474e+23

loss= inf W= 1.6700204e+31 b 2.543495e+28

loss= inf W= -5.320352e+35 b -8.1030665e+32

loss= inf W= inf b inf

loss= inf W= nan b nan

loss= nan W= nan b nan

loss= nan W= nan b nan

loss= nan W= nan b nan

	import tensorflow as tf
	from sklearn.model_selection import train_test_split
	import numpy as np
	import pandas as pd
	from matplotlib import pyplot as plt


	noise=np.random.normal(0.0,50,1000)#.astype(np.float32)
	x_data=np.random.uniform(0,1000,1000)#.astype(np.float32)
	y_data=0.5*x_data+2+noise#.astype(np.float32)

	plt.scatter(x_data,y_data,s=0.1)
	plt.show()


	X=tf.placeholder(shape=(1000,),dtype=tf.float32)
	Y=tf.placeholder(shape=(1000,),dtype=tf.float32)

	#Learning W and b over the epochs
	W=tf.get_variable(name='Weight',dtype=tf.float32,shape(),initializer=tf.zeros_initializer())
	b=tf.get_variable(name='Bias',dtype=tf.float32,shape=(),initializer=tf.zeros_initializer())

	Y_pred= tf.add(tf.multiply(X, W),b)
	loss = tf.reduce_mean(tf.square(Y_pred - Y))



	optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(loss)

	epochs=100
	with tf.Session() as sess:
	init=tf.global_variables_initializer()
	sess.run(init)
	for e in range(epochs):
		_,c=sess.run([optimizer,loss],feed_dict={X: x_data,Y: y_data})
		print('loss=',c,'W=',sess.run(W),'b',sess.run(b))

	#plt.scatter(x_data, y_data, 'ro', label='Original data')
	plt.plot(x_data, sess.run(W) * x_data + sess.run(b), label='Fitted line')
	plt.legend()
	plt.show()",1,1
1779,2019-5-29,2019,5,29,20,bud94j,Data Science Certification in Pune,https://www.reddit.com/r/MachineLearning/comments/bud94j/data_science_certification_in_pune/,dwivediabhinav,1559127930,"  

ExcelR is a global leader in technical and management training catering the training needs of the professionals in more than 27 countries with offices in USA, Malaysia, India etc. with over 21 branches across the globe.Learn data science Course in Excelr with Top-notch faculties.

&amp;#x200B;

  

[https://www.excelr.com/data-science-course-training-in-pune/](https://www.excelr.com/data-science-course-training-in-pune/)",0,1
1780,2019-5-29,2019,5,29,20,budd4u,Transformer for continuous space?,https://www.reddit.com/r/MachineLearning/comments/budd4u/transformer_for_continuous_space/,claverru,1559128666,[removed],0,1
1781,2019-5-29,2019,5,29,20,budlex,Chi-Square test for feature selection in ML,https://www.reddit.com/r/MachineLearning/comments/budlex/chisquare_test_for_feature_selection_in_ml/,prakhar21,1559130115,"Checkout my learning on categorical feature selection based on it's dependence and independence using Chi-Squared test.  [https://prakhartechviz.blogspot.com/2019/05/chi-square-test-for-feature-selection.html](https://prakhartechviz.blogspot.com/2019/05/chi-square-test-for-feature-selection.html)

Please share your knowledge on the same.

Thanks",0,1
1782,2019-5-29,2019,5,29,20,budoyb,"[D] IEEE bans Huawei employees from reviewing or handling papers for IEEE journals, some people resign from IEEE editorial board as a result",https://www.reddit.com/r/MachineLearning/comments/budoyb/d_ieee_bans_huawei_employees_from_reviewing_or/,mln000b,1559130728,"The news broke here: [https://twitter.com/qian\_junhui/status/1133595554905124869](https://twitter.com/qian_junhui/status/1133595554905124869)

&amp;#x200B;

Here is Prof. Zhang's (from Peking University) resignation letter from IEEE NANO: [https://twitter.com/qian\_junhui/status/1133657229561802752](https://twitter.com/qian_junhui/status/1133657229561802752)",209,607
1783,2019-5-29,2019,5,29,21,budyhc,Deep Learning Series (Part 1): Introduction,https://www.reddit.com/r/MachineLearning/comments/budyhc/deep_learning_series_part_1_introduction/,hisham_elamir,1559132324,,0,1
1784,2019-5-29,2019,5,29,21,budz4p,Implementing Batch Normalization in this implementation?,https://www.reddit.com/r/MachineLearning/comments/budz4p/implementing_batch_normalization_in_this/,Jandevries101,1559132441,[removed],0,1
1785,2019-5-29,2019,5,29,21,bue1w9,Deep NLP Series (Part 1): Introduction,https://www.reddit.com/r/MachineLearning/comments/bue1w9/deep_nlp_series_part_1_introduction/,hisham_elamir,1559132922,,0,1
1786,2019-5-29,2019,5,29,21,bue27u,[D] Would you adjust the prediction after re-transformation /back transformation?,https://www.reddit.com/r/MachineLearning/comments/bue27u/d_would_you_adjust_the_prediction_after/,leo_000,1559132979,"In regression problem, one may transform the dependent variable to log space before fitting the model.  And in some cases we would want to transform the prediction back to a normal scale.

&amp;#x200B;

According to [this discussion](https://stats.stackexchange.com/a/58077), it seems right to adjust the prediction by Duan's Smearing Estimator or a [general smearing adjustment](https://stats.stackexchange.com/a/361632) after taking `exp`. But in practice, I do not see a lot of example which do such adjustment. Is the effect so small that people normally neglect it?",0,1
1787,2019-5-29,2019,5,29,21,bue2ju,ML Tools - MSys Training,https://www.reddit.com/r/MachineLearning/comments/bue2ju/ml_tools_msys_training/,msystrainingllc,1559133035,[removed],0,1
1788,2019-5-29,2019,5,29,21,bue6mz,[D] Which tasks in machine learning requires exact decimal multiplication?,https://www.reddit.com/r/MachineLearning/comments/bue6mz/d_which_tasks_in_machine_learning_requires_exact/,alrojo,1559133700,"Exact addition, subtraction, multiplication, and division of real numbers is a challenge for machine learning models.   
Neural networks can approximate complex functions. However, when extrapolating to out-of-distribution samples on arithmetic operations neural networks often fail. I would like to gather a set of applications requiring exact arithmetic of real numbers - do you have any ideas?",1,1
1789,2019-5-29,2019,5,29,22,bueiy7,ML applications - MSys Training,https://www.reddit.com/r/MachineLearning/comments/bueiy7/ml_applications_msys_training/,msystrainingllc,1559135623,[removed],0,1
1790,2019-5-29,2019,5,29,22,buelv8,Occasional missing feature values time series classification,https://www.reddit.com/r/MachineLearning/comments/buelv8/occasional_missing_feature_values_time_series/,dluther93,1559136075,[removed],0,1
1791,2019-5-29,2019,5,29,22,bueww1,Question about Batch Normalization,https://www.reddit.com/r/MachineLearning/comments/bueww1/question_about_batch_normalization/,Echoes1996,1559137756,[removed],0,1
1792,2019-5-29,2019,5,29,22,buex93,Should I cite the CVPR 2019 papers as an Arxiv paper?,https://www.reddit.com/r/MachineLearning/comments/buex93/should_i_cite_the_cvpr_2019_papers_as_an_arxiv/,salihkaragoz,1559137820,"We are writing an article for the 3DV conference. I will cite some papers which will publish at CVPR 2019. I got this information from the author' personal website or paper GitHub repo. They also show their work' BibTeX as CVPR paper. But CVPR Committee just only published the ids of accepted papers that means we can't know the which papers will publish CVPR officially. 

It seems like, the correct way is that cite papers as an Arxiv paper. What is your experiments and thoughts on this situation? Thanks in advance.",0,1
1793,2019-5-29,2019,5,29,23,buf83m,Cyber Security or AI-ML? Which one should I pursue for higher studies?,https://www.reddit.com/r/MachineLearning/comments/buf83m/cyber_security_or_aiml_which_one_should_i_pursue/,the_notorious_beast,1559139457,[removed],0,1
1794,2019-5-29,2019,5,29,23,bufaia,"SUNCG dataset no longer available, authors not responding",https://www.reddit.com/r/MachineLearning/comments/bufaia/suncg_dataset_no_longer_available_authors_not/,NaughtyCranberry,1559139823,[removed],0,1
1795,2019-5-29,2019,5,29,23,bufbep,[D] Should I cite the CVPR 2019 papers as an Arxiv paper?,https://www.reddit.com/r/MachineLearning/comments/bufbep/d_should_i_cite_the_cvpr_2019_papers_as_an_arxiv/,salihkaragoz,1559139952,"We are writing an article for the 3DV conference. I will cite some papers which will publish at CVPR 2019. I got this information from the author' personal website or paper GitHub repo. They also show their work' BibTeX as CVPR paper. But CVPR Committee just only published the ids of accepted papers that means we can't know the which papers will publish CVPR officially.

It seems like, the correct way is that cite papers as an Arxiv paper. What is your experiments and thoughts on this situation? Thanks in advance.",4,2
1796,2019-5-29,2019,5,29,23,bufdxy,Cold Case: The Lost MNIST Digits,https://www.reddit.com/r/MachineLearning/comments/bufdxy/cold_case_the_lost_mnist_digits/,iyaja,1559140309,,6,27
1797,2019-5-29,2019,5,29,23,bufhe8,"Free Webinar: How to use continual learning in your ML models June 19th, 12pm EST",https://www.reddit.com/r/MachineLearning/comments/bufhe8/free_webinar_how_to_use_continual_learning_in/,Mayalittlepony,1559140791,[removed],0,2
1798,2019-5-30,2019,5,30,0,bufy5o,Machine Learning Certification Course - Msys Training,https://www.reddit.com/r/MachineLearning/comments/bufy5o/machine_learning_certification_course_msys/,msystrainingllc,1559143133,,0,1
1799,2019-5-30,2019,5,30,0,bufy8r,Tsinghua University and Huawei Noahs ERNIE Tackles Google BERT's Problem,https://www.reddit.com/r/MachineLearning/comments/bufy8r/tsinghua_university_and_huawei_noahs_ernie/,Yuqing7,1559143145,,0,1
1800,2019-5-30,2019,5,30,0,bug0wi,Where to start?,https://www.reddit.com/r/MachineLearning/comments/bug0wi/where_to_start/,Entellex,1559143516,[removed],0,1
1801,2019-5-30,2019,5,30,0,bug54o,[R] Representing Theorems as Graphs Improves State-of-the-Art by 50% in Deep Automated Theorem Proving,https://www.reddit.com/r/MachineLearning/comments/bug54o/r_representing_theorems_as_graphs_improves/,GrandmasterMochizuki,1559144094,,13,46
1802,2019-5-30,2019,5,30,0,bug6dc,[P] I am making a rock sorter. Should I train the network on what are big/small rocks or should I train the network by watching someone operate the sorting machine?,https://www.reddit.com/r/MachineLearning/comments/bug6dc/p_i_am_making_a_rock_sorter_should_i_train_the/,bolom_sounga,1559144275,"I am doing a project on automating the task to sort rocks on a conveyor belt. My initial idea was to setup a camera and train a model (using TensorFlow) on what is a small rock, big rock, quartz rock. And so far my tabletop demo is successful. My goal is to then send the coordinates of the rock to pick out to a robotic arm that will pick out the rock at a given coordinate. Lets assume that the rock picking process is trivial.

My problem is when vendors approached us, they convinced my boss that they know better (usual vendor logic) and they say that its better if we train the machine by observing an actual operator do the work. They didnt go into much detail but when i asked them how they would do that they had vague answers that made no sense, such as ""its an AI that learns which rocks to pick, by wtaching the operator it will learn""

They showed us demos of games being played like racing cars around artificial tracks, super mario etc. But from my understanding, wouldnt it be better if we could actually train the algorithm to detect what is a quartz rock and how to detect it in a camera frame and then proceed to remove the rock? I think its much harder to train the algorithm with a weird camera angle watching someone do the job and then determining which rocks to pick.

Recently i saw a user post a video on here showing a generation based car racing track. (https://www.youtube.com/watch?v=wL7tSgUpy8w) Wouldnt that be a generative model and is maybe what's causing confusion to the vendors?

Im wondering what would be your thoughts",17,12
1803,2019-5-30,2019,5,30,0,bugcqq,"Simple Questions Thread May 29, 2019",https://www.reddit.com/r/MachineLearning/comments/bugcqq/simple_questions_thread_may_29_2019/,AutoModerator,1559145176,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",0,1
1804,2019-5-30,2019,5,30,1,buggh8,Free Machine Learning Course for Physicists 2017,https://www.reddit.com/r/MachineLearning/comments/buggh8/free_machine_learning_course_for_physicists_2017/,aiforworld2,1559145677,"(12 Video/Audio Lectures)

By Prof. Dr. Florian Marquardt,
Institute of Theoretical Physics,
Chair of Theoretical Physics,
Friedrich-Alexander-Universitt Erlangen-Nrnberg (FAU)

Faculty: https://lnkd.in/fbBHqNW

Course: https://lnkd.in/fhFXRrS",0,1
1805,2019-5-30,2019,5,30,1,bugn6r,Best way to measure effect of news articles on Twitter,https://www.reddit.com/r/MachineLearning/comments/bugn6r/best_way_to_measure_effect_of_news_articles_on/,entercaspa,1559146561,[removed],0,1
1806,2019-5-30,2019,5,30,1,bugro8,Compiling Tesseract Directly,https://www.reddit.com/r/MachineLearning/comments/bugro8/compiling_tesseract_directly/,angelkosa,1559147145,[removed],0,1
1807,2019-5-30,2019,5,30,1,bugw2o,[R] Evaluating and Calibrating Uncertainty Prediction in Regression Tasks,https://www.reddit.com/r/MachineLearning/comments/bugw2o/r_evaluating_and_calibrating_uncertainty/,ethanfetaya,1559147721,,1,7
1808,2019-5-30,2019,5,30,1,bugy5j,[R] SGD on Neural Networks Learns Functions of Increasing Complexity,https://www.reddit.com/r/MachineLearning/comments/bugy5j/r_sgd_on_neural_networks_learns_functions_of/,preetum,1559148005,,6,9
1809,2019-5-30,2019,5,30,1,buh0a3,[D] What is the effect of training a system with jumbled up feature vectors?,https://www.reddit.com/r/MachineLearning/comments/buh0a3/d_what_is_the_effect_of_training_a_system_with/,General_Example,1559148297,"An idea has been bouncing around my head today, and since I'm not very knowledgeable about ML I want to know if there's any literature (or common sense from experts) about it.

Lets say we have training examples consisting of (say) two features, color (`c`) and size (`s`), such that `v1 = [c1, s1]`, `v2 = [c2, s2]`, and `vn = [cn, sn]`.

What is the effect of training a system with ""jumbled"" inputs `vx`, and `vy`, such that `vx = [c1, s2]` and `vy = [c2, s1]`?

My immediate thought is that you can't really give labels to jumbled training examples (it can't be a cat if it has a horse's head and a pig's tail), but perhaps the system could learn a probability distribution of the labels based on the features included?

Anyway, can jumbled training examples produce a model that is useful in any way? Is there any literature exploring this?",3,1
1810,2019-5-30,2019,5,30,2,buhenn,"What are some good public data sets/algorithm pairings that are good for an advanced beginner, but represent more production/business use cases?",https://www.reddit.com/r/MachineLearning/comments/buhenn/what_are_some_good_public_data_setsalgorithm/,ezeeetm,1559150159,,0,1
1811,2019-5-30,2019,5,30,2,buhi7c,[D] UK Masters VS US Masters for American,https://www.reddit.com/r/MachineLearning/comments/buhi7c/d_uk_masters_vs_us_masters_for_american/,Neilson5,1559150637,"Hello, I have seen some form of this question before on this subreddit, but not quite any answers specific to my question.

I have known I wanted to get a masters for quite some time. I am currently a Data Scientist in the DC area, however want to move towards machine learning applied for engineering applications as opposed to business/ enterprise applications.

For a while I have been looking at schools in the United States, such as Maryland, Austin, UIL, Michigan etc. However, the idea of going abroad has been in the back of mind for some time. I studied abroad at Edinburgh, which is where I first studied and acquired a passion for machine learning.

Alongside the cheaper cost of school there, on a more personal level I am anxious about missing out on a chance to experience living in a foreign country again. So I am considering applying to programs at UCL, Imperial and Edinburgh.  My one concern is how thst will Impact career prospects in my future.

Some more background, I graduated relatively recently. I had a 3.5 GPA with a degree in applied stats and computer science from a fairly well known school (albeit not well known in CS). I have research experience from an internship and expect to be published at some point. I   have some long term professional goals, but they may just be pipe dreams, of continuing my growth at one of the well known residency programs but I understand my performance during a masters will impact that likelihood more than the school itself. Also a Ph. D is not off the table, but I think masters would help with finding a program. 

Tl;Dr will a masters in the UK  hinder my chances to return to the US for a career in advanced AI and ML?",2,6
1812,2019-5-30,2019,5,30,2,buhlq4,[D] If you use pandas: which tasks are the hardest for data cleaning and manipulation?,https://www.reddit.com/r/MachineLearning/comments/buhlq4/d_if_you_use_pandas_which_tasks_are_the_hardest/,kite_and_code,1559151102,"Hi,

I am obsessed with making Data Science in Python faster and many people told me that data cleaning and manipulation are the most tedious tasks in their daily work.

Which are the exact tasks where you spend/lose most of your time when performing data cleaning/manipulation in pandas?

1. reading in datasets (finding the right separator, dataformat, ...)
2. adjusting the data types of the columns - eg parse datetime, parse to numeric or categoric, others?
3. removing missing values
4. finding and removing duplicate values
5. parsing columns and removing invalid strings?
6. concatenating datasets
7. joining multiple tables
8. creating groupbys and aggregations
9. filtering and selecting subsets
10. creating new columns/feature engineering
11. visualizing the dataset and exploring it
12. **Something else? Did I miss something?**

I am planning to collect the best libraries for the tasks (or maybe write a library on my own to fill the missing gaps) in order to make the working process much faster.

I would be grateful for any input

Best,

Florian",24,10
1813,2019-5-30,2019,5,30,2,buhtnq,[D] Why are arithmetic operations of latent variables meaningful?,https://www.reddit.com/r/MachineLearning/comments/buhtnq/d_why_are_arithmetic_operations_of_latent/,TheRedSphinx,1559152207,"I've noticed that in a lot of latent variable models, a lot of authors will perform arithmetic operations on the latent space and show that they have meaning e.g. 'king - men + woman = queen' in word2vec, the idea of attribute vectors for VAE, and even linear interpolation for VAEs. 

&amp;#x200B;

What part of training makes this happen? For concreteness, let's look at VAEs for the time being, with the usual Gaussian prior. It would seem like linear interpolation in this case could yield bad results, since there's a good chance that at some point in the interpolation we could pass by a vector of smalll norm, which would be very unlikely to be sampled from a Gaussian in the latent space has high dimension. In fact, some papers even make references to this and use things like SLERP. Nevertheless, the results clearly work. Is there a theoretical justification for why these operations have meaning? Why should we even expect a properly-trained VAE to exhibit these properties?",10,11
1814,2019-5-30,2019,5,30,3,buhyir,[D] How do you measure the change in dependent variable by a unit change in the independent variable?,https://www.reddit.com/r/MachineLearning/comments/buhyir/d_how_do_you_measure_the_change_in_dependent/,amil123123,1559152866,"Hey all,

Firstly I know it's a basic question but there are a few constraints that make it interesting. 

The problem is as follows - I have an equation of the following form

a1 \* b1 \* c1...f1 = x/y

here 

x/y is a ratio which can range from 0 to +inf.

a1, b1....f1 are features that are all binary. 

I want to measure the change in ratio( x/y )  due to the presence of a1 feature and similarly for other features as well.

I applied LOG transformation of the equation to convert into sums and applied Linear Regression ( sticking to basics ).  The learned weights by the model actually denote the change my output value which can be reduced back by taking Exponent of the weights. However, the results were not good. 

I needed help with the following - 

* Are there any other methods by which I can measure such change, like using weights of Linear Regression, etc?
* Since ratio tends to be from 0 to +inf, applying, is there some other transformation I should prefer?
* All my features are categorical which were one hot encoded, is there some other encoding that might be preferred in such use case?",4,2
1815,2019-5-30,2019,5,30,3,buhysw,[D] Neural Network Implementation,https://www.reddit.com/r/MachineLearning/comments/buhysw/d_neural_network_implementation/,cronoz30,1559152903,"Hello reddit,

I'm following this lecture series: [https://www.youtube.com/watch?v=SGZ6BttHMPw&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH](https://www.youtube.com/watch?v=SGZ6BttHMPw&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH) and am trying to implement a neural network from scratch.

This is my forward pass:

`res_1 =` [`np.dot`](https://np.dot/)`(W_1.T, x) + b1`

`res_1_activation = sigmoid(res_1)`

`output =`  
 [`np.dot`](https://np.dot/)`(W_2.T, res_1_activation) + b2`

`output_activation = sigmoid(output)`

This is my backward pass:

`grad_pre_output = -(y[i] - output_activation)`

`grad_W_2 =` [`np.dot`](https://np.dot/)`(np.expand_dims(res_1_activation, axis=1),np.expand_dims(grad_pre_output, 1).T)`

`grad_b2 = np.expand_dims(grad_pre_output, 1)`

`grad_post_1 =` [`np.dot`](https://np.dot/)`(W_2, np.expand_dims(grad_pre_output, axis=1))`

`grad_pre_1 =np.multiply(grad_post_1.T, d_sigmoid(res_1))`

`grad_W_1 =` [`np.dot`](https://np.dot/)`(np.expand_dims(x, axis=1), grad_pre_1)`

`grad_b1 = grad_pre_1.T`

`W_1 = np.add(W_1, lr * ((grad_W_1) - lamda * 2 * W_1))`

`W_2 = np.add(W_2, lr * ((grad_W_2) - lamda * 2 * W_2))`

`b1 = np.add(b1, np.reshape(grad_b1, (grad_b1.shape[0],)))`

`b2 = np.add(b2, np.reshape(grad_b2, (grad_b2.shape[0],)))`

I think this implementation should work, but when I run it for &gt;2 epochs all the output neurons get saturated and always return a value of 1. I tried using L2 regularization but it still saturates. Can anyone please tell me what I am doing wrong?

Any help would be appreciated.

Thank you!",5,0
1816,2019-5-30,2019,5,30,3,bui0a5,Heya Fellas Look at my work inspired to colorize Old Black and White Images.,https://www.reddit.com/r/MachineLearning/comments/bui0a5/heya_fellas_look_at_my_work_inspired_to_colorize/,01raven01,1559153097,,0,1
1817,2019-5-30,2019,5,30,3,bui111,Need datasets for multimodal sentiment analysis.,https://www.reddit.com/r/MachineLearning/comments/bui111/need_datasets_for_multimodal_sentiment_analysis/,flabbychicken,1559153192,"I am currently working on an architecture that focuses on multimodal sentiment analysis, I wanted to know where i could find it, or from where should i crawl it?  

Pre-made datasets would be better.",0,1
1818,2019-5-30,2019,5,30,3,bui7qd,[P] We analyzed 18086 frames of reaction videos on YouTube with computer vision and created an emotional ranking of characters in Game of Thrones. Emotion recognition was just the tip of the iceberg ;) What do you think?,https://www.reddit.com/r/MachineLearning/comments/bui7qd/p_we_analyzed_18086_frames_of_reaction_videos_on/,agasitko,1559154083,,0,1
1819,2019-5-30,2019,5,30,3,buia2u,Human Brain vs Machine Learning - A Lost Battle?,https://www.reddit.com/r/MachineLearning/comments/buia2u/human_brain_vs_machine_learning_a_lost_battle/,andrea_manero,1559154409,[removed],0,1
1820,2019-5-30,2019,5,30,4,buj1um,Best free place to learn machine learning?,https://www.reddit.com/r/MachineLearning/comments/buj1um/best_free_place_to_learn_machine_learning/,Acujl,1559158334,[removed],0,1
1821,2019-5-30,2019,5,30,4,buja86,[D] Undergrad Student Stuck in NLP Forever?,https://www.reddit.com/r/MachineLearning/comments/buja86/d_undergrad_student_stuck_in_nlp_forever/,AnonMLstudent,1559159546,"Basically am an undergrad student doing a lot of research work with professors in NLP. Recently had some interviews with companies focused on other things such as robotics and CV and got instantly rejected because ""my strengths do not fit what they are looking for"".

This has got me thinking that maybe it was a mistake to focus all my undergrad projects on one field such as NLP? It's a bit late to diversify as grad school applications are soon. Since I have to choose a supervisor for grad I feel like they will have to be NLP focused as well to accept me, and hence will I be stuck in NLP for masters then eventually PhD and forever?

Honestly I'm just kind of upset because it appears many companies and jobs are focusing on other areas of ML and AI such as robotics and CV (e.g. autonomous vehicles, etc.) whereas my focus on NLP is significantly less desired.",3,0
1822,2019-5-30,2019,5,30,5,bujkol,In need of advice,https://www.reddit.com/r/MachineLearning/comments/bujkol/in_need_of_advice/,westminsterrealm,1559160991,[removed],0,1
1823,2019-5-30,2019,5,30,5,bujv0k,State of Smart Question Generators?,https://www.reddit.com/r/MachineLearning/comments/bujv0k/state_of_smart_question_generators/,jdalcin,1559162365,[removed],0,0
1824,2019-5-30,2019,5,30,5,buk454,AI Learnt That Buzzfeed is Bad,https://www.reddit.com/r/MachineLearning/comments/buk454/ai_learnt_that_buzzfeed_is_bad/,adidotdash,1559163555,,0,1
1825,2019-5-30,2019,5,30,6,bukd5l,EfficientNet: Improving Accuracy and Efficiency through AutoML and Model Scaling,https://www.reddit.com/r/MachineLearning/comments/bukd5l/efficientnet_improving_accuracy_and_efficiency/,sjoerdapp,1559164755,,0,1
1826,2019-5-30,2019,5,30,6,bukhm0,[P] PyCM 2.2 released: Machine learning library for confusion matrix statistical analysis,https://www.reddit.com/r/MachineLearning/comments/bukhm0/p_pycm_22_released_machine_learning_library_for/,sepandhaghighi,1559165347,"[http://www.pycm.ir](http://www.pycm.ir/)  

[https://github.com/sepandhaghighi/pycm](https://github.com/sepandhaghighi/pycm)

&amp;#x200B;

* Negative likelihood ratio interpretation (NLRI) added
* Cramer's benchmark (SOA5) added
* Matthews correlation coefficient interpretation (MCCI) added [\#204](https://github.com/sepandhaghighi/pycm/issues/204)
* Matthews's benchmark (SOA6) added [\#204](https://github.com/sepandhaghighi/pycm/issues/204)
* F1 macro added
* F1 micro added
* Accuracy macro added [\#205](https://github.com/sepandhaghighi/pycm/issues/205)
* Compare class score calculation modified
* Parameters recommendation for multi-class dataset modified
* Parameters recommendation for imbalance dataset modified
* [README.md](https://README.md) modified
* Document modified
* Logo updated",3,10
1827,2019-5-30,2019,5,30,6,bukkgq,Is a PhD on Evolutionary Algorithms worth it?,https://www.reddit.com/r/MachineLearning/comments/bukkgq/is_a_phd_on_evolutionary_algorithms_worth_it/,actuallynotcanadian,1559165737,[removed],0,1
1828,2019-5-30,2019,5,30,6,bukmqc,ML Community Raises Inclusivity Concerns After IEEE Bars Huawei Paper Reviewers,https://www.reddit.com/r/MachineLearning/comments/bukmqc/ml_community_raises_inclusivity_concerns_after/,Yuqing7,1559166048,,0,1
1829,2019-5-30,2019,5,30,6,bukuas,Classic NLP Analogies from Embeddings are hard coded not to return original input,https://www.reddit.com/r/MachineLearning/comments/bukuas/classic_nlp_analogies_from_embeddings_are_hard/,modx07,1559167080,,0,1
1830,2019-5-30,2019,5,30,7,bul4k8,[R] EfficientNet: Improving Accuracy and Efficiency through AutoML and Model Scaling,https://www.reddit.com/r/MachineLearning/comments/bul4k8/r_efficientnet_improving_accuracy_and_efficiency/,madebyollin,1559168489,,1,1
1831,2019-5-30,2019,5,30,7,bul9yd,Best IDE for Kaggle,https://www.reddit.com/r/MachineLearning/comments/bul9yd/best_ide_for_kaggle/,DanorRon,1559169267,[removed],0,1
1832,2019-5-30,2019,5,30,7,buldvo,[R] Classification Accuracy Score for Conditional Generative Models,https://www.reddit.com/r/MachineLearning/comments/buldvo/r_classification_accuracy_score_for_conditional/,wall-eeeee,1559169856,,2,1
1833,2019-5-30,2019,5,30,7,buleba,tensorflow/text is a new github repo by tensorflow,https://www.reddit.com/r/MachineLearning/comments/buleba/tensorflowtext_is_a_new_github_repo_by_tensorflow/,sjoerdapp,1559169925,,0,1
1834,2019-5-30,2019,5,30,7,bulja9,[D] Interpreting the dimensions in spectral embedding,https://www.reddit.com/r/MachineLearning/comments/bulja9/d_interpreting_the_dimensions_in_spectral/,pfd1986,1559170655,"Hi all, I've been pondering about how to interpret the new dimensions created by spectral embeddings. More specifically [this question](https://stats.stackexchange.com/questions/410741/spectral-embedding-interpretation-of-new-dimensions):

Does anyone here have an intuition to what's going on?",4,5
1835,2019-5-30,2019,5,30,8,buls08,Has Tensorflow 2.0 bridged the gap between PyTorch and Tensorflow in ease of use?,https://www.reddit.com/r/MachineLearning/comments/buls08/has_tensorflow_20_bridged_the_gap_between_pytorch/,insanelylogical,1559171938,,0,1
1836,2019-5-30,2019,5,30,8,bulyla,Using SVMlight,https://www.reddit.com/r/MachineLearning/comments/bulyla/using_svmlight/,Strange_Flatworm,1559172927,[removed],0,1
1837,2019-5-30,2019,5,30,9,bumjdc,[R] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/bumjdc/r_efficientnet_rethinking_model_scaling_for/,hardmaru,1559176068,,60,301
1838,2019-5-30,2019,5,30,10,bun7i4,Style transfer-based image synthesis as an efficient regularization technique in deep learning,https://www.reddit.com/r/MachineLearning/comments/bun7i4/style_transferbased_image_synthesis_as_an/,Chitoyo,1559179888,,3,1
1839,2019-5-30,2019,5,30,11,buniur,"[R] Given N samples, we generate NxN samples for training.",https://www.reddit.com/r/MachineLearning/comments/buniur/r_given_n_samples_we_generate_nxn_samples_for/,zhedongzheng,1559181797,"Training data is one of the keys to deep learning. Given N images, our cvpr19 oral paper could generate O(NxN) high-fidelity images for training. The variants in generated samples are from the original dataset instead of random vectors.

Papaer Link: [https://arxiv.org/abs/1904.07223](https://arxiv.org/abs/1904.07223)

Video Demo: [https://www.youtube.com/watch?v=ubCrEAIpQs4](https://www.youtube.com/watch?v=ubCrEAIpQs4)",16,45
1840,2019-5-30,2019,5,30,11,bunw8o,Best course on Machine learning for a complete beginner?,https://www.reddit.com/r/MachineLearning/comments/bunw8o/best_course_on_machine_learning_for_a_complete/,Geek4lyf,1559184056,,0,1
1841,2019-5-30,2019,5,30,12,buo4kc,papers on real-world application of pure Machine learning techiniques (not deep learning),https://www.reddit.com/r/MachineLearning/comments/buo4kc/papers_on_realworld_application_of_pure_machine/,preetham_salehundam,1559185497,need pointers to some research papers on real-world application of a machine learning algorithm.,0,1
1842,2019-5-30,2019,5,30,12,buofe8,Video sentiment analysis,https://www.reddit.com/r/MachineLearning/comments/buofe8/video_sentiment_analysis/,order_chaos_in,1559187438,"Hello All,
Looking for advice on how to perform sentiment analysis on videos with or without audio? Any pointers to existing thread, project, discussion will be helpful. Even your thoughts on how to preprocess videos, appropriate model architecture would be great. 

My apologies for repost if this has been discussed already. However, as suggested by mod I have googled it online and searched this thread before posting.

Thanks for your time",0,1
1843,2019-5-30,2019,5,30,12,buojya,"[R] Using local plasticity rules to train recurrent neural networks -- Owen Marschall, Kyunghyun Cho, Cristina Savin",https://www.reddit.com/r/MachineLearning/comments/buojya/r_using_local_plasticity_rules_to_train_recurrent/,enverx,1559188281,,1,3
1844,2019-5-30,2019,5,30,13,bup47t,Important Use Cases of NLP,https://www.reddit.com/r/MachineLearning/comments/bup47t/important_use_cases_of_nlp/,chetnatripathi19,1559192230,,0,1
1845,2019-5-30,2019,5,30,14,bup9sn,What is the current SOTA image classification technique,https://www.reddit.com/r/MachineLearning/comments/bup9sn/what_is_the_current_sota_image_classification/,revant_t,1559193348,[removed],0,1
1846,2019-5-30,2019,5,30,14,bupj2y,[R] Speech2Face: Learning the Face Behind a Voice,https://www.reddit.com/r/MachineLearning/comments/bupj2y/r_speech2face_learning_the_face_behind_a_voice/,astrange,1559195378,,3,16
1847,2019-5-30,2019,5,30,15,bupo8q,Wondering if my understanding of YOLO V1 is correct.,https://www.reddit.com/r/MachineLearning/comments/bupo8q/wondering_if_my_understanding_of_yolo_v1_is/,UnfazedButDazed,1559196497,[removed],0,1
1848,2019-5-30,2019,5,30,15,bupp8i,EfficientNet: Improving Accuracy and Efficiency through AutoML and Model Scaling,https://www.reddit.com/r/MachineLearning/comments/bupp8i/efficientnet_improving_accuracy_and_efficiency/,ml-engineer,1559196713,,0,1
1849,2019-5-30,2019,5,30,15,bupwhx,(First) Comprehensive study of using BFLOAT16 for DL training,https://www.reddit.com/r/MachineLearning/comments/bupwhx/first_comprehensive_study_of_using_bfloat16_for/,fulcrum_xyz,1559198348,,1,4
1850,2019-5-30,2019,5,30,15,bupxjv,Run kmeans many times on several CPU cores in R,https://www.reddit.com/r/MachineLearning/comments/bupxjv/run_kmeans_many_times_on_several_cpu_cores_in_r/,Elrondsen,1559198577,,0,1
1851,2019-5-30,2019,5,30,16,buq3j1,Will the Coral Dev Board Replace Raspberry Pi?,https://www.reddit.com/r/MachineLearning/comments/buq3j1/will_the_coral_dev_board_replace_raspberry_pi/,makereven,1559199904,"Single-board computers (SBCs) are revolutionary devices. The most famous SBC is undoubtedly the Raspberry Pi. Howeverthe Coral Dev board is an SBC with Googles custom Mendel operating system, designed for use with the TensorFlow Lite neural network.",0,1
1852,2019-5-30,2019,5,30,16,buq5ft,Raw data for speech recognition,https://www.reddit.com/r/MachineLearning/comments/buq5ft/raw_data_for_speech_recognition/,adityawastaken,1559200326,"I am currently building a new model for speech recognition. That is why I am trying to research about how is data converted from raw amplitude information into frequency information using fourier transforms.

I have looked at using a CNN for processing the raw sound data and recognizing the patterns in that information. However through my research i found that it is not as accurate. 

I wanted to know why is raw sound data not used in the speech recognition process and if it is used what would be the benefits of using it.

Thank you!",0,1
1853,2019-5-30,2019,5,30,16,buq5ku,"[D] Advice for getting back into ML and Data Science after a significant ""absence"" from the field?",https://www.reddit.com/r/MachineLearning/comments/buq5ku/d_advice_for_getting_back_into_ml_and_data/,SpockTriesToReturn,1559200354,"I got my Ph.D in Machine Learning in 2008. Worked on the intersection of physics and supervised learning methods.  

Wasn't happy as a postdoc and decided to join the industry instead. 

Joined a mid-level software company that developed analytics software for various businesses, hoping to  utilize my ML and stats knowledge, but quickly found that at the time (2009\~2010) domain knowledge and implementation expertise were much more valued than mathematical and modeling skills (again *at that time* \- things seems to have changed radically since then - I remember hearing more than once while interviewing in 2009 hiring managers say that they'd rather hire a domain expert and train them on the science than hire a scientist and train them on the domain).   

Within a year, I switched to a TPM/Architect type of role, going to client sites and doing implementations or assisting with project management and tech support, and worked for various consulting firms in that capacity, as there seemed to be more money and opportunity there. 

In a way, I was never that far removed from data science, since the products I was peddling always involved an analytics/predictive function. But none of the roles I had involved any explicit science work, other than having to explain to clients and business stake holders the math that was behind some of the algorithms used in the software I worked with. 

&amp;#x200B;

Then in 2014, I realized that DS and ML were fashionable, and that experience with neural networks (my old passion) was considered a hot skill on the market, so I decided to get back into data science. I started learning R and Python (in my grad school days I was mostly a Matlab person) and catching up on the latest developments in the field, frequenting Kaggle discussions and other forums, Leetcode, etc...

I thought the combination of business knowledge / consulting experience + Ph.D level understanding of the models and math involved would make me a golden candidate for various DS/ML roles. 

&amp;#x200B;

Instead, over the last 5 years, I have found it very difficult to break into the field, with recruiters and hiring managers rarely paying attention to me, and when they do, I can't seem to get passed the technical screens or initial interviews because of my lack of real world experience with things like Spark, Flask, etc... and my mostly theoretical (very deep, but still purely theoretical) knowledge of how models other than time series and linear regression work. 

Even more frustrating is that more and more people with less qualifications than me seem to transition at will into data science roles (business analyst or SDE completes a Coursera certificate in machine learning, gets promoted to data science role the next month). 

To top it all off: I'm nearing forty now (which I gather is ancient in machine learning years) and I feel like my chances of breaking into the filed based on age alone are decreasing exponentially every year.   

&amp;#x200B;

Am I hopeless? Any advice for an aging data science has been/wanna be?",29,19
1854,2019-5-30,2019,5,30,16,buq9y1,Head Pose Estimation using OpenCV and Dlib,https://www.reddit.com/r/MachineLearning/comments/buq9y1/head_pose_estimation_using_opencv_and_dlib/,spmallick,1559201405,,1,1
1855,2019-5-30,2019,5,30,16,buqfaw,[R] Brain-inspired reverse adversarial examples,https://www.reddit.com/r/MachineLearning/comments/buqfaw/r_braininspired_reverse_adversarial_examples/,yeshaokai,1559202716,"Paper: [https://arxiv.org/abs/1905.12171](https://arxiv.org/abs/1905.12171)

&amp;#x200B;

Abstract: A human does not have to see all elephants to recognize an animal as an elephant. On contrast, current state-of-the-art deep learning approaches heavily depend on the variety of training samples and the capacity of the network. In practice, the size of network is always limited and it is impossible to access all the data samples. Under this circumstance, deep learning models are extremely fragile to human-imperceivable adversarial examples, which impose threats to all safety critical systems. Inspired by the association and attention mechanisms of the human brain, we propose reverse adversarial examples method that can greatly improve models' robustness on unseen data. Experiments show that our reverse adversarial method can improve accuracy on average 19.02% on ResNet18, MobileNet, and VGG16 on unseen data transformation. Besides, the proposed method is also applicable to compressed models and shows potential to compensate the robustness drop brought by model quantization - an absolute 30.78% accuracy improvement.

&amp;#x200B;

Hi, I am one of the authors on this paper. 

&amp;#x200B;

We like to collect opinions on this paper and answer questions.",2,9
1856,2019-5-30,2019,5,30,16,buqfq7,"Call for Book Chapter on ""Practical Applications and Implementations of Machine Learning Techniques-IGI Global Inc.",https://www.reddit.com/r/MachineLearning/comments/buqfq7/call_for_book_chapter_on_practical_applications/,sathyait2003,1559202820,[removed],2,1
1857,2019-5-30,2019,5,30,17,buqj8g,"Call for Book Chapter on ""Practical Applications and Implementations of Machine Learning Techniques""-IGI GLobal Inc",https://www.reddit.com/r/MachineLearning/comments/buqj8g/call_for_book_chapter_on_practical_applications/,sathyait2003,1559203668,,0,1
1858,2019-5-30,2019,5,30,17,buqkuq,RNN/LSTM Models and Sequences with uninitialized variables or semi structured data?,https://www.reddit.com/r/MachineLearning/comments/buqkuq/rnnlstm_models_and_sequences_with_uninitialized/,zoombapup,1559204077,[removed],0,1
1859,2019-5-30,2019,5,30,17,buqmpt,[R] A massive collection of fraud detection research papers,https://www.reddit.com/r/MachineLearning/comments/buqmpt/r_a_massive_collection_of_fraud_detection/,benitorosenberg,1559204563,"&amp;#x200B;

https://i.redd.it/4rijby336b131.png

# [https://github.com/benedekrozemberczki/awesome-fraud-detection-papers](https://github.com/benedekrozemberczki/awesome-fraud-detection-papers)

A curated list of papers about fraud detection from the following conferences:

* Network Science  

   * [ASONAM](http://asonam.cpsc.ucalgary.ca/2019/)
* Natural Language Processing  

   * [ACL](http://www.acl2019.org/EN/index.xhtml)
* Data Mining  

   * [KDD](https://www.kdd.org/)
   * [ICDM](http://icdm2019.bigke.org/)
   * [SDM](http://icdm2019.bigke.org/)
   * [WWW](https://www2019.thewebconf.org/)
* Artificial Intelligence  

   * [AAAI](https://www.aaai.org/)
   * [AISTATS](http://www.auai.org/)
   * [IJCAI](https://www.ijcai.org/)
   * [UAI](http://www.auai.org/)

Similar collections about [graph embedding](https://github.com/benedekrozemberczki/awesome-graph-embedding), [classification/regression tree](https://github.com/benedekrozemberczki/awesome-decision-tree-papers), [gradient boosting](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers) and [community detection](https://github.com/benedekrozemberczki/awesome-community-detection) papers with implementations.",3,15
1860,2019-5-30,2019,5,30,17,buqp21,Cornell's entire Machine Learning class (CS 4780) is now entirely on You Tube. Taught by one of the funniest and best professors from UCornell,https://www.reddit.com/r/MachineLearning/comments/buqp21/cornells_entire_machine_learning_class_cs_4780_is/,joyanta180199,1559205179,,0,1
1861,2019-5-30,2019,5,30,17,buqr98,"Combined Telescopic forks for load handling devices. Telescopic Forks, Single deep telescopic fork",https://www.reddit.com/r/MachineLearning/comments/buqr98/combined_telescopic_forks_for_load_handling/,lhd121,1559205735,,0,1
1862,2019-5-30,2019,5,30,17,buqs8y,artificial intelligence app for android|artificial intelligence app for Mobile App Development,https://www.reddit.com/r/MachineLearning/comments/buqs8y/artificial_intelligence_app_for_androidartificial/,clarke2106,1559205991,[removed],0,1
1863,2019-5-30,2019,5,30,17,buqucv,What kind of neural network can predict a set of data based on another one?,https://www.reddit.com/r/MachineLearning/comments/buqucv/what_kind_of_neural_network_can_predict_a_set_of/,Competitive_Mongoose,1559206545,[removed],0,1
1864,2019-5-30,2019,5,30,18,buqwiv,Is a PhD on Evolutionary Algorithms worth it?,https://www.reddit.com/r/MachineLearning/comments/buqwiv/is_a_phd_on_evolutionary_algorithms_worth_it/,actuallynotcanadian,1559207099,[removed],0,1
1865,2019-5-30,2019,5,30,18,buqyeq,AI with Image Recognition: Benefits that it can Provide to Your Business,https://www.reddit.com/r/MachineLearning/comments/buqyeq/ai_with_image_recognition_benefits_that_it_can/,quytech1,1559207545,,0,1
1866,2019-5-30,2019,5,30,18,bur4dy,How to stay afloat with the latest Machine learning/Data science knowledge?,https://www.reddit.com/r/MachineLearning/comments/bur4dy/how_to_stay_afloat_with_the_latest_machine/,Mrpega,1559208933,[removed],0,1
1867,2019-5-30,2019,5,30,19,burgq3,[R] Nonlinear Approximation via Compositions,https://www.reddit.com/r/MachineLearning/comments/burgq3/r_nonlinear_approximation_via_compositions/,anonymousTestPoster,1559211731,,3,3
1868,2019-5-30,2019,5,30,19,burm7m,"""IEEE is actually trying its best to save Huawei under US pressure"" IEEE member-at-large Yu Yuan. (in Chinese)",https://www.reddit.com/r/MachineLearning/comments/burm7m/ieee_is_actually_trying_its_best_to_save_huawei/,DiogLin,1559212893,,0,1
1869,2019-5-30,2019,5,30,20,burt34,Cross-Domain Transferability of Adversarial Perturbations,https://www.reddit.com/r/MachineLearning/comments/burt34/crossdomain_transferability_of_adversarial/,muzammal-naseer,1559214312,,1,1
1870,2019-5-30,2019,5,30,20,bus5pt,Pytorch Implementation of GCPN paper,https://www.reddit.com/r/MachineLearning/comments/bus5pt/pytorch_implementation_of_gcpn_paper/,yashkhem,1559216786,[removed],0,1
1871,2019-5-30,2019,5,30,20,bus841,[N] Call For Speakers &amp; Call For Tutors for Data Science Conference in Belgrade,https://www.reddit.com/r/MachineLearning/comments/bus841/n_call_for_speakers_call_for_tutors_for_data/,NezirDov,1559217255,"Hi all!

Im Nezir, representative of the **Institute of Contemporary Sciences**, an NGO in Serbia whose aim is to popularize and advance Data Science and all related fields of science. Our main activity that we work on is **Data Science Conference**, which aims to bring data scientists from all over the world to exchange ideas and discuss the hottest topics in data science.

[Data Science Conference](https://datasciconference.com), one of the **biggest conferences dedicated to data science in Europe**, will be held for **5th annual time** on **19-20 November** in Belgrade.
We are looking for inspirational individuals who would like to become a part of our Speakers and Tutors team. For this year we have planned to organize 5 Workshops, 5 parallel tracks, 8 Data discussion, 60 Talks &amp; 6 Keynote talks and over 70 Tech Tutorials for over 1500 attendees. 

If you are interested to become part of the Conference apply and join us! Application is open until 17 June.

**Call For Speakers**
If you have an interesting story you would like to share with our attendees - [apply](https://datasciconference.com/call-for-speakers/application/) to become a part of Speakers team. 
You would be able to choose between **10** topics. Topics we are going to cover this year are:  Applicative ML, Data Monetization, Deep Learning &amp; AI, Business Intelligence, Big Data &amp; Data Engineering, Framework for AI &amp; ML modeling, How to start Career in Data Science, Data Science in 4th Industrial revolution, Open Data, Data Privacy &amp; Data for good, Data Science Startups &amp; Digital transformation.
More information you can find on our [website](https://datasciconference.com/call-for-speakers/).

**Call For Tutors**
We are looking for experts who would like to share their knowledge with attendees of Data Science Conference as a part of our Tutors team. 
You would be able to choose between more than **15** topics, **6** languages, **18** platforms and tools you would like to present to attendees of the Conference. 
More information you can find at our [website](https://datasciconference.com/call-for-tutors/).

Tutors and Speakers who reside outside of Republic of Serbia will be accommodated in Belgrade city center, while the cost of the stay will be covered by the organizers. All selected speakers &amp; tutors will get a free pass for the Conference and invitation to participate in Data Science Summit (3rd networking day of the Conference).

For any additional questions, you may contact us in comments or through private messages.",1,2
1872,2019-5-30,2019,5,30,20,bus8fc,Machine learning and how does machine learning works,https://www.reddit.com/r/MachineLearning/comments/bus8fc/machine_learning_and_how_does_machine_learning/,shoutopia,1559217317,,0,0
1873,2019-5-30,2019,5,30,20,bus9uu,CycleGAN output quality peaks before first epoch is completedhelp needed!,https://www.reddit.com/r/MachineLearning/comments/bus9uu/cyclegan_output_quality_peaks_before_first_epoch/,lujkuj,1559217584,[removed],0,1
1874,2019-5-30,2019,5,30,21,buskuu,[P] Combining numerical and text features in (deep) neural networks in keras,https://www.reddit.com/r/MachineLearning/comments/buskuu/p_combining_numerical_and_text_features_in_deep/,ixeption,1559219483,"Hi folks,

&amp;#x200B;

A lot of people ask how to combine NLP based features (or in general sequence embeddings) with standart features. In keras it pretty easy with a multiple input modell:

    nlp_input = Input(shape=(seq_length,), name='nlp_input')
    meta_input = Input(shape=(10,), name='meta_input')
    emb = Embedding(output_dim=embedding_size, input_dim=100, input_length=seq_length)(nlp_input)
    nlp_out = Bidirectional(LSTM(128, dropout=0.3, recurrent_dropout=0.3, kernel_regularizer=regularizers.l2(0.01)))(emb)
    x = concatenate([nlp_out, meta_input])
    x = Dense(classifier_neurons, activation='relu')(x)
    x = Dense(1, activation='sigmoid')(x)
    model = Model(inputs=[nlp_input , meta_input], outputs=[x])

[Here is a link, where it more detailed.](http://digital-thinking.de/deep-learning-combining-numerical-and-text-features-in-deep-neural-networks/)

&amp;#x200B;

Cheers",0,2
1875,2019-5-30,2019,5,30,21,buslrf,[D] Does anyone know of papers that encode motion information using an auto-encoder with a biologically inspired architecture?,https://www.reddit.com/r/MachineLearning/comments/buslrf/d_does_anyone_know_of_papers_that_encode_motion/,Edelsonc,1559219634,"Some cursory googling didnt yield anything, so I was wondering if anyone here had come across anything?",9,5
1876,2019-5-30,2019,5,30,21,busmgy,[P] Can Neural Networks learn temporal contexts in time series?,https://www.reddit.com/r/MachineLearning/comments/busmgy/p_can_neural_networks_learn_temporal_contexts_in/,doyuplee,1559219760,"Yes, our recent work shows that.

Out proposed model (TGNet, temporal-guided network) learns temporal contexts in time series data explicitly.

Paper link: [https://arxiv.org/abs/1905.10709](https://arxiv.org/abs/1905.10709)

Github: [https://github.com/LeeDoYup/TGNet-keras](https://github.com/LeeDoYup/TGNet-keras) 

https://i.redd.it/bxp74fgcfc131.png",18,37
1877,2019-5-30,2019,5,30,22,busywh,Yolov3 loss function,https://www.reddit.com/r/MachineLearning/comments/busywh/yolov3_loss_function/,deluded_soul,1559221773,[removed],0,1
1878,2019-5-30,2019,5,30,22,but4ir,Writing billion songs with C# and Deep Learning,https://www.reddit.com/r/MachineLearning/comments/but4ir/writing_billion_songs_with_c_and_deep_learning/,atomlib_com,1559222643,,0,1
1879,2019-5-30,2019,5,30,22,but75z,[D] CycleGAN performance peaks before first epoch is finished--help needed!,https://www.reddit.com/r/MachineLearning/comments/but75z/d_cyclegan_performance_peaks_before_first_epoch/,lujkuj,1559223062,"Hi,

I have been trying to use an unpaired image-to-image style transfer GAN to convert shoes to dresses. After trying a few different algorithms (including DiscoGAN, StarGAN, and MUNIT), I settled on CycleGAN as it has given me the best results.

I have collected 99,500 and 114,000 images of dresses and shoes, respectively. All of the shoes and dresses are the same orientation and scale (within category). Compared to other projects, I think my dataset quality is quite good.

I am using an implementation by [aitorzip](https://github.com/aitorzip/PyTorch-CycleGAN) ([my fork](https://github.com/lpupp/PyTorch-CycleGAN)). I have tried a few different configurations but havent been able to get anything better than the default configurations.

The quality of the generated images initially progresses well (until around 1/3 of an epoch30,000 steps on a batch size of 1). After that, even after 32,000, the results worsen. The mode seems to collapse after the first epoch has been completed.

&amp;#x200B;

[Image 1: progression of generator's outputs and reconstruction over time for shoes2dresses task](https://i.redd.it/a8r08leroc131.jpg)

The results generated by the model after 30,000 steps are still very checkered. I tried replacing the transposed convolutions with upsamples (see [here](https://distill.pub/2016/deconv-checkerboard/ 2)) but it resulted in a mode collapse very quickly.

I am not sure if I have already reached the best performance I can expect with such small images (64x64) but its hard to believe since this would imply that the best results are achieved before the GAN has seen the whole training set. I am unable to get anything comparable with larger images (128x128 or 256x256).

Additionally, I feel like I am battling many issues at the same time. First, I am finding it difficult to get high-quality outputs. Then, the GAN is very unstable and collapses frequently. Finally, it is often the case the even though the GAN produces the same output for any input, it is able to reconstruct the input image (see image 2)

&amp;#x200B;

[Image 2: reconstructs output of mode collapse generator well](https://i.redd.it/y40069tvoc131.jpg)

I have already played around with many of the training parameters (batch size, learning rate, decay rate, loss function composition, added feature matching, Wasserstein loss, added noise to the input images, added extra layers to the discriminator and generator, used upsample operations in place of transposed convolutions, played around with the buffer size, also attempted to keep the kernel size proportional to the 64x64 image ratio when using larger images). However, since there are so many things to change, my experiments were not exhaustive. In particular, I feel like I could try playing around the decay rates and composition of the loss function in more detail. Things that are on my list but that I havent tried are mini-batch discrimination, packing (from PACGAN), different normalization (currently, I am using InstanceNorm), and logit loss (from LOGAN).

Any advice would be greatly appreciated as I have been spent many hours on this, and, without positive feedback, my personal learning is not so high.

Id be happy to give more info.",2,7
1880,2019-5-30,2019,5,30,22,butdgg,"Grad student specialising in ML and DS, wondering if I should all my eggs in one basket - CV?",https://www.reddit.com/r/MachineLearning/comments/butdgg/grad_student_specialising_in_ml_and_ds_wondering/,clavamxr,1559224028,"Hi, I'm a grad student pursuing Ml and DS from UCSD. I've done courses in Deep Learning, Statistical Learning, Sensing and Estimation,  Image processing and Recommender Systems - and got a few projects. I'm good at Python, and using PyTorch for developing CNNs ( familiar with TF and Keras, but not very fluent). I'm at the end of my first year, and I've got my summer ahead of me, after which I want to start hunting for jobs. One wrong decision I made was to take up a project under a Prof in the summer on Data Science, which I think has made my work too diversified- basically a little bit of everything but not enough depth in one. I think this was a bad idea because as a grad student I'm expected to me really knowledgeable in one field. Of all the things I've done, I'm interested in CV the most, as it looks the most exciting and I've had fun doing projects on it. I am planning to do a classical CV course from the CSE department next quarter. I was wondering if I'm making mistake by deciding to give all my energies to CV. Are there enough job opportunities out there in CV to a Master's student to pursue? Or should I dabble a bit everywhere - a few projects on NLP etc, so that I have a shot at any ML related opening?

TL;DR - Masters student trying to choose between focusing on CV or doing a bit of everything, for ML related software jobs.

Thank you! 

P.S - I've previous work ex as a Modem Engineer (1 year) in Qualcomm, before I went to grad school, but nothing in the SW industry or ML field.",0,1
1881,2019-5-30,2019,5,30,22,butf9h,Open Image By Matplotlib and OpenCV | Python,https://www.reddit.com/r/MachineLearning/comments/butf9h/open_image_by_matplotlib_and_opencv_python/,spokhrel,1559224309,,0,1
1882,2019-5-30,2019,5,30,22,butigr,Applications of the p-bit to stochastic machine learning,https://www.reddit.com/r/MachineLearning/comments/butigr/applications_of_the_pbit_to_stochastic_machine/,throwa2934,1559224783,,1,1
1883,2019-5-30,2019,5,30,23,butygl,INTRODUCCIN AL DEEP LEARNING,https://www.reddit.com/r/MachineLearning/comments/butygl/introduccin_al_deep_learning/,jeffry_30,1559227134,[removed],0,1
1884,2019-5-30,2019,5,30,23,buu0bl,Q-Learning.!,https://www.reddit.com/r/MachineLearning/comments/buu0bl/qlearning/,indi0508,1559227411,"i made a basic RL agent using Q-learning.! :)

The agent has to avoid red blocks and pass through Green blocks.

![video](lllzjrph1d131 ""CAPO(V3)"")",0,1
1885,2019-5-30,2019,5,30,23,buu27v,"[WIP Book] Deep Learning for Programmers: An Interactive Tutorial with CUDA, OpenCL, MKL-DNN, Java, and Clojure",https://www.reddit.com/r/MachineLearning/comments/buu27v/wip_book_deep_learning_for_programmers_an/,dragandj,1559227690,,0,1
1886,2019-5-31,2019,5,31,0,buu7m8,"[P] (WIP Book) Deep Learning for Programmers: An Interactive Tutorial with CUDA, OpenCL, MKL-DNN, Java, and Clojure",https://www.reddit.com/r/MachineLearning/comments/buu7m8/p_wip_book_deep_learning_for_programmers_an/,dragandj,1559228472,"My in-progress book is aimed at explaining efficient implementation techniques useful in machine learning to programmers. It is on topic of Deep Learning as an example, but tries to be applicable to anyone that needs to use high performance computing from high level languages to implement or integrate machine learning algorithms into broader software infrastructure.

The link (including downloads of the samples):
http://aiprobook.com/deep-learning-for-programmers/",6,9
1887,2019-5-31,2019,5,31,0,buubcl,Modulating David Silver's voice in his Reinforcement Learning Lectures -- help needed,https://www.reddit.com/r/MachineLearning/comments/buubcl/modulating_david_silvers_voice_in_his/,username-cs231n,1559228994,I was going through David Silver's Reinforcement Learning Lectures on Youtube and realised that the volume of his voice fluctuates a lot due to him moving towards and away from mic repeatedly. Could someone write a program that would use face detection to track his face and than modulate his voice accordingly.Would be a great help to this community.,0,1
1888,2019-5-31,2019,5,31,0,buucg0,RuneScape&amp;reg; Bot - RuneScape&amp;reg; Mobile Bot - OSRS Bot - TRiBot,https://www.reddit.com/r/MachineLearning/comments/buucg0/runescapereg_bot_runescapereg_mobile_bot_osrs_bot/,janelladonaghej,1559229149,,0,1
1889,2019-5-31,2019,5,31,0,buucku,DeepMind's new neural network model beats AlexNet with 13 images per class,https://www.reddit.com/r/MachineLearning/comments/buucku/deepminds_new_neural_network_model_beats_alexnet/,keurigg,1559229169,,39,434
1890,2019-5-31,2019,5,31,0,buutg6,[D] Patterns of Self-Supervised Learning,https://www.reddit.com/r/MachineLearning/comments/buutg6/d_patterns_of_selfsupervised_learning/,ekshaks,1559231632,"I wrote up a [short article](https://towardsdatascience.com/self-supervised-learning-78bdd989c88b) introducing self-supervised learning and noting down common recurring patterns that I've observed in several self-supervised problem setups.  Feedback welcome. In general, one has to be quite creative setting up the right 'input' and 'output' for learning a particular object's representation.

Are there other common patterns that others have observed?

How do we compare the representations learned from two different self-supervised setups for the same object type, e.g., rotation vs patch-based, BERT-like masked loss vs word vectors?",2,0
1891,2019-5-31,2019,5,31,0,buuukn,[R] Adversarially robust models on ImageNet,https://www.reddit.com/r/MachineLearning/comments/buuukn/r_adversarially_robust_models_on_imagenet/,VaryRobust,1559231786,[removed],0,1
1892,2019-5-31,2019,5,31,1,buuznx,Is my understanding of YOLO correct?,https://www.reddit.com/r/MachineLearning/comments/buuznx/is_my_understanding_of_yolo_correct/,UnfazedButDazed,1559232493,[removed],0,1
1893,2019-5-31,2019,5,31,1,buvadz,[D] Hands-on Graph Neural Networks with PyTorch &amp; PyTorch Geometric,https://www.reddit.com/r/MachineLearning/comments/buvadz/d_handson_graph_neural_networks_with_pytorch/,steeveHuang,1559234010,"PyTorch Geometric is one of the fastest Graph Neural Networks frameworks in the world. In this article, I talked about the basic usage of PyTorch Geometric and how to use it on real-world data. 

[https://medium.com/@huangkh19951228/hands-on-graph-neural-networks-with-pytorch-pytorch-geometric-359487e221a8](https://medium.com/@huangkh19951228/hands-on-graph-neural-networks-with-pytorch-pytorch-geometric-359487e221a8)",2,10
1894,2019-5-31,2019,5,31,1,buvbs5,Advice for beginners in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/buvbs5/advice_for_beginners_in_machine_learning/,MoistSyrupp,1559234200,"Hi, I'm a few months into learning machine learning and I'm just wondering if anyone has any advice for a beginner that you wish you knew when you started or something that helped you advance fast in the field. I use python3 and I'll be going to college next year to learn CS. Just trying to see if I can find some people that I can take a look at their projects, learn a thing or two, and learn ML.",0,1
1895,2019-5-31,2019,5,31,1,buvcox,Must read papers for beginners?,https://www.reddit.com/r/MachineLearning/comments/buvcox/must_read_papers_for_beginners/,lkit57a03,1559234322,[removed],0,1
1896,2019-5-31,2019,5,31,1,buvdae,AI Restores Photos of 90s Hong Kong Film Stars,https://www.reddit.com/r/MachineLearning/comments/buvdae/ai_restores_photos_of_90s_hong_kong_film_stars/,Yuqing7,1559234404,,0,1
1897,2019-5-31,2019,5,31,1,buvi62,CommunityPilot,https://www.reddit.com/r/MachineLearning/comments/buvi62/communitypilot/,icmma,1559235069,"Hey we are CommunityPilot!

We are open source community based on [comma.ai](https://comma.ai/)'s openpilot

[Comma.ai](https://comma.ai/) is working on consumer friendly product where we had to split to the community since we had different vision towards the project which is building L3 system except for just staying with L2.

We are going to open source the driving data, If anyone wants to get involved it will be awesome.

I have shared blueprints of the project on [github](https://github.com/communitypilot/blueprints/blob/master/README.md)

You can join our [discord](https://discord.gg/UBnEmYp)

I would love to your feedback on the project thank you:)

Elliot",0,1
1898,2019-5-31,2019,5,31,2,buvxqy,[R] Learning to learn by Self-Critique Github Repo,https://www.reddit.com/r/MachineLearning/comments/buvxqy/r_learning_to_learn_by_selfcritique_github_repo/,AntreasAntoniou,1559237169,"Dear r/MachineLearning friends,
In my previous [post](https://old.reddit.com/r/MachineLearning/comments/btnj4s/r_learning_to_learn_by_selfcritique/) I shared my latest work on *Learning to learn via Self-Critique*. In that post, I also stated that code would soon follow. Well, today I published a refactored version of the code I used to produce the results in my paper. Relevant links can be found below. 


Regards, Antreas

Paper: https://arxiv.org/abs/1905.10295

Code: https://github.com/AntreasAntoniou/Learning_to_learn_via_Self-Critique",0,14
1899,2019-5-31,2019,5,31,2,buvznn,"[D] Flow-Based Generative Models, Bijective Transforms and Neural Lossless Compression",https://www.reddit.com/r/MachineLearning/comments/buvznn/d_flowbased_generative_models_bijective/,ArmenAg,1559237431,New [blog post](https://armenag.com/2019/05/30/flow-based-generative-models-bijective-transforms-and-neural-lossless-compression/) discussing flow-based generative models and the various coupling transforms that exist. We touch on the recent paper on Integer Discrete Flows toward the end.,4,24
1900,2019-5-31,2019,5,31,3,buwo4x,Machine Learning for Astronomy,https://www.reddit.com/r/MachineLearning/comments/buwo4x/machine_learning_for_astronomy/,thearkamitra,1559240821,[removed],0,1
1901,2019-5-31,2019,5,31,3,buwrgo,How Business Can Benefit from Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/buwrgo/how_business_can_benefit_from_recurrent_neural/,LemmyChildish,1559241276,,0,1
1902,2019-5-31,2019,5,31,4,bux4k2, Capture the Flag: the emergence of complex cooperative agents,https://www.reddit.com/r/MachineLearning/comments/bux4k2/capture_the_flag_the_emergence_of_complex/,sjoerdapp,1559243120,,0,1
1903,2019-5-31,2019,5,31,5,buxtwd,why model prediction error will decrease with the model complexity,https://www.reddit.com/r/MachineLearning/comments/buxtwd/why_model_prediction_error_will_decrease_with_the/,xihajun,1559246645,"why model prediction error will decrease with the model complexity

for example Y = X1A1 + e1

Y = X1A1+X2A2 +e2

can we prove that E(e2) is less than e1?",0,1
1904,2019-5-31,2019,5,31,5,buyenq,Embedding techniques for an integer input to RNN model,https://www.reddit.com/r/MachineLearning/comments/buyenq/embedding_techniques_for_an_integer_input_to_rnn/,abinashsinha330,1559249622,"My input to each block of RNN is an integer and I have to get embedding vector of each of these inputs. The input integer values are restricted between 0 to 2\^56.

&amp;#x200B;

Can I consider this an n-gram natural language model and use standard NLP embedding techniques like word2vec to get embedding vectors of each of the integer input?",0,1
1905,2019-5-31,2019,5,31,6,buyk4n,Here is the list of commonly used machine learning algorithms. The code is provided in both #R and #Python. [Download Link],https://www.reddit.com/r/MachineLearning/comments/buyk4n/here_is_the_list_of_commonly_used_machine/,ai-lover,1559250398,,0,1
1906,2019-5-31,2019,5,31,6,buylr7,"[D] How much of an effect, if any, does batch size have when doing hyperparameter optimization?",https://www.reddit.com/r/MachineLearning/comments/buylr7/d_how_much_of_an_effect_if_any_does_batch_size/,Gkg14,1559250634,"I have been using sci-kit optimize to do hyperparameter search (using gp_minimize specifically) for a neural network. I am working on a binary classification problem with a significant class imbalance. I have been using a batch size of 10, but just came across a [tweet](https://twitter.com/fchollet/status/1133425339370074112) and [notebook](https://colab.research.google.com/drive/1xL2jSdY-MGlN60gGuSH_L30P7kxxwUfM) by Francois Chollet where he recommended using a high batch size in class imbalance problems in order so that each batch contains at least a few positive examples.

My question is can I just take the networks with the best network architectures I found via my hyperparameter search where I used a batch size of 10, but just retrain them using the same hyperparameters but using a higher batch size? 

Or, would batch size have a significant effect on hyperparameter optimization, and I would be better off just redoing hyperparameter optimization but this time with a larger batch size?

Going off of that, any recommendations for how to select batch size? My data contains between 400,000 - 500,000 samples, and I'm feeding in 7 features to the network.

I hope my question makes sense, thanks for any help!",5,7
1907,2019-5-31,2019,5,31,6,buyppy,Capture the Flag: the emergence of complex cooperative agents,https://www.reddit.com/r/MachineLearning/comments/buyppy/capture_the_flag_the_emergence_of_complex/,ml-engineer,1559251222,,0,1
1908,2019-5-31,2019,5,31,6,buyuj5,Update on Mushroom: Reinforcement Learning library,https://www.reddit.com/r/MachineLearning/comments/buyuj5/update_on_mushroom_reinforcement_learning_library/,carloderamo,1559251911,[removed],0,1
1909,2019-5-31,2019,5,31,6,buz12y,Human-level performance in 3D multiplayer games with reinforcement learning,https://www.reddit.com/r/MachineLearning/comments/buz12y/humanlevel_performance_in_3d_multiplayer_games/,ml-engineer,1559252829,,0,2
1910,2019-5-31,2019,5,31,7,buz7lr,https://tvm.ai/2019/05/30/pytorch-frontend,https://www.reddit.com/r/MachineLearning/comments/buz7lr/httpstvmai20190530pytorchfrontend/,bwasti,1559253819,,0,1
1911,2019-5-31,2019,5,31,7,buzkfj,Integrating TVM into PyTorch,https://www.reddit.com/r/MachineLearning/comments/buzkfj/integrating_tvm_into_pytorch/,bwasti,1559255803,,0,1
1912,2019-5-31,2019,5,31,7,buzqkf,Not so sure about that one Marky ,https://www.reddit.com/r/MachineLearning/comments/buzqkf/not_so_sure_about_that_one_marky/,noob10,1559256763,,0,1
1913,2019-5-31,2019,5,31,8,bv02sp,Is a PhD on the following topic worth it?,https://www.reddit.com/r/MachineLearning/comments/bv02sp/is_a_phd_on_the_following_topic_worth_it/,actuallynotcanadian,1559258667,[removed],0,1
1914,2019-5-31,2019,5,31,8,bv03ok,[D] Changing Fields,https://www.reddit.com/r/MachineLearning/comments/bv03ok/d_changing_fields/,A27_97,1559258805,,0,1
1915,2019-5-31,2019,5,31,8,bv04ig,Learning voice changer?,https://www.reddit.com/r/MachineLearning/comments/bv04ig/learning_voice_changer/,TPCallie,1559258938,[removed],0,1
1916,2019-5-31,2019,5,31,8,bv07qa,The difference between Machine Learning and D3eep Learning.,https://www.reddit.com/r/MachineLearning/comments/bv07qa/the_difference_between_machine_learning_and_d3eep/,mudassirmandviwala,1559259435,,0,1
1917,2019-5-31,2019,5,31,8,bv0959,Is my understanding of YOLO correct here?,https://www.reddit.com/r/MachineLearning/comments/bv0959/is_my_understanding_of_yolo_correct_here/,UnfazedButDazed,1559259649,[removed],0,1
1918,2019-5-31,2019,5,31,9,bv0tzc,Razer blade 2019 - RTX 2060 - How to install nvidia drivers to make it work on this laptop?,https://www.reddit.com/r/MachineLearning/comments/bv0tzc/razer_blade_2019_rtx_2060_how_to_install_nvidia/,gimel1213,1559262944,"Hi, I recently bought a razer laptop to make some development on the go since it has the rtx 2060 but i have cannot install the nvidia drivers for it. 

Any advise you can give me guys will be really helpful.

Thanks",0,1
1919,2019-5-31,2019,5,31,10,bv198t,Bias value,https://www.reddit.com/r/MachineLearning/comments/bv198t/bias_value/,Ole_72444,1559265475,[removed],0,1
1920,2019-5-31,2019,5,31,10,bv1h3v,[D] What is the consensus on the noise input for GANs?,https://www.reddit.com/r/MachineLearning/comments/bv1h3v/d_what_is_the_consensus_on_the_noise_input_for/,Statistical_Incline,1559266771,"Is it standard to save the noise distribution your model trained on to use and use the same values for transfer learning / fine tuning / inference?

&amp;#x200B;

Or do people typically create a new distribution of noise for the model to learn at each new application?

&amp;#x200B;

I'm assuming that during one entire training session you create the noise distribution once so that the model is learning the function of turning that noise into features of the images right?

&amp;#x200B;

Thanks in advance!",4,5
1921,2019-5-31,2019,5,31,10,bv1j99,my first project using a GAN,https://www.reddit.com/r/MachineLearning/comments/bv1j99/my_first_project_using_a_gan/,tuseroni,1559267105,[removed],0,1
1922,2019-5-31,2019,5,31,11,bv236d,[R] On Network Design Spaces for Visual Recognition,https://www.reddit.com/r/MachineLearning/comments/bv236d/r_on_network_design_spaces_for_visual_recognition/,xternalz,1559270369,,1,9
1923,2019-5-31,2019,5,31,11,bv24g0,Why Down vote the truth?,https://www.reddit.com/r/MachineLearning/comments/bv24g0/why_down_vote_the_truth/,best1400au,1559270589,[removed],1,1
1924,2019-5-31,2019,5,31,12,bv2c74,ECOSISTEMA Y COMPONENTES DE BIG DATA,https://www.reddit.com/r/MachineLearning/comments/bv2c74/ecosistema_y_componentes_de_big_data/,jeffry_30,1559271878,[removed],0,1
1925,2019-5-31,2019,5,31,13,bv2zg9,Best sources to learn PCA,https://www.reddit.com/r/MachineLearning/comments/bv2zg9/best_sources_to_learn_pca/,alpaca1331,1559276113,[removed],0,1
1926,2019-5-31,2019,5,31,13,bv30nn,[D] How to find volunteer opportunities in machine learning?,https://www.reddit.com/r/MachineLearning/comments/bv30nn/d_how_to_find_volunteer_opportunities_in_machine/,DisastrousProgrammer,1559276322,"The most obvious ones are probably academic groups, but perhaps there are some non-profits I could identify?",5,10
1927,2019-5-31,2019,5,31,13,bv3172,Fully Supervised Speaker Diarization,https://www.reddit.com/r/MachineLearning/comments/bv3172/fully_supervised_speaker_diarization/,viennafairfax,1559276421,,2,3
1928,2019-5-31,2019,5,31,15,bv3wej,New to ML,https://www.reddit.com/r/MachineLearning/comments/bv3wej/new_to_ml/,RodYT,1559282536,[removed],0,1
1929,2019-5-31,2019,5,31,15,bv46cz,How Machine Learning is Enhancing Mobile Gadgets and Applications,https://www.reddit.com/r/MachineLearning/comments/bv46cz/how_machine_learning_is_enhancing_mobile_gadgets/,analyticsinsight,1559284681,,0,1
1930,2019-5-31,2019,5,31,16,bv4cb6,What's the best resource to learn about making your very own self driving car ?,https://www.reddit.com/r/MachineLearning/comments/bv4cb6/whats_the_best_resource_to_learn_about_making/,masterRJ2404,1559286011,[removed],0,1
1931,2019-5-31,2019,5,31,16,bv4ihu,"How to approach ""Price prediction"" Machine Learning problem on time series data?",https://www.reddit.com/r/MachineLearning/comments/bv4ihu/how_to_approach_price_prediction_machine_learning/,pktippa,1559287354,[removed],0,1
1932,2019-5-31,2019,5,31,16,bv4iy0,GradCAM for heat/Saliency maps,https://www.reddit.com/r/MachineLearning/comments/bv4iy0/gradcam_for_heatsaliency_maps/,Doraguniru92,1559287461,[removed],0,1
1933,2019-5-31,2019,5,31,16,bv4j7w,Learning and Solving Sudoku via SATNet,https://www.reddit.com/r/MachineLearning/comments/bv4j7w/learning_and_solving_sudoku_via_satnet/,ml-engineer,1559287525,,0,1
1934,2019-5-31,2019,5,31,17,bv4txl,[R] CIFAR-10 Classes Extracted from Imagenet?,https://www.reddit.com/r/MachineLearning/comments/bv4txl/r_cifar10_classes_extracted_from_imagenet/,alexmlamb,1559290099,"Hello, 

&amp;#x200B;

I'm interested in a labeled dataset with the same 10 classes from CIFAR-10, but with the images extracted from imagenet (thus much higher quality and higher resolution) and with some reasonable class matching.  

&amp;#x200B;

Do you know if this dataset has been produced or is out there somewhere?",4,2
1935,2019-5-31,2019,5,31,17,bv4vu4,is there any paper for the workflow of the labeling process for medical images by doctors?,https://www.reddit.com/r/MachineLearning/comments/bv4vu4/is_there_any_paper_for_the_workflow_of_the/,lunasdejavu,1559290547,[removed],0,1
1936,2019-5-31,2019,5,31,17,bv532q,Using a networks predictions as it's own training data?,https://www.reddit.com/r/MachineLearning/comments/bv532q/using_a_networks_predictions_as_its_own_training/,jewnicorn27,1559292369,[removed],0,1
1937,2019-5-31,2019,5,31,19,bv5pv5,[D] CycleGAN implementation just learning identity mapping,https://www.reddit.com/r/MachineLearning/comments/bv5pv5/d_cyclegan_implementation_just_learning_identity/,nottodaymrdick,1559297517,"Hi, don't know where else to ask but I just don't know what else I could try out with my code. 

&amp;#x200B;

I'm trying to reimplement CycleGAN in a Jupyter notbook and (for me) the code looks good, but somehow my generators just learn to map an input to itself (so what I put into it comes out at the other end). I'm testing my implementation with the horse2zebra dataset.

*Processing img nehc7wksti131...*

*Processing img th5sljd1ui131...*

What's odd is that the GAN loss is going up, which is probably why the generators don't learn anything meaningful other than the identity mapping. I also got the feeling that my discriminators just learn to distinguish fake from real images, but nothing about horses or zebras.

&amp;#x200B;

Here's a link to the notebook:  [https://github.com/kiwiwa/GANs-from-scratch/blob/master/cyclegan/cyclegan.ipynb](https://github.com/kiwiwa/GANs-from-scratch/blob/master/cyclegan/cyclegan.ipynb)

I would be so happy if somebody could give me a hint. The discriminator/generator architectures should be fine, probably the training process?",8,2
1938,2019-5-31,2019,5,31,19,bv5r7o,"[D] How to approach ""Price prediction"" Machine Learning problem on time series data?",https://www.reddit.com/r/MachineLearning/comments/bv5r7o/d_how_to_approach_price_prediction_machine/,pktippa,1559297786,"My initial thoughts

I got a time series transaction data of purchase logs of different products.

Ex: Product named ""Y"" purchased for ""M"" amount, on Date ""D"" at Location/Place ""P"" and 5-10 other features.

Now i need to predict, how much we need to pay for purchasing Product named ""Y"" on new Date ""D1"" at Location/Place ""P1"" and same 5-10 other features.

Need inputs on choosing

1. Machine Learning library - I have TensorFlow 2.0 in my mind.

2. ML Model / Neural network arch need to choose - I have Linear Regression in my mind.

3. Training strategy - using TF 2.0

4. Inference/Prediction strategy - using TF 2.0

5. Model saving, loading, versioning. - Saving to / Loading from / versioning using AWS S3.

6. Machine Configuration - Dataset is small so i am Okay with Running on CPU.",11,5
1939,2019-5-31,2019,5,31,19,bv5v7b,What is the decision boundary achieved by a classification algorithm?,https://www.reddit.com/r/MachineLearning/comments/bv5v7b/what_is_the_decision_boundary_achieved_by_a/,josesil,1559298604,[removed],0,1
1940,2019-5-31,2019,5,31,20,bv6d46,How to manually label centroids for detection,https://www.reddit.com/r/MachineLearning/comments/bv6d46/how_to_manually_label_centroids_for_detection/,QuantJr,1559302180,[removed],0,1
1941,2019-5-31,2019,5,31,21,bv6n3h,IOT and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bv6n3h/iot_and_machine_learning/,wowinfotechdm,1559304031,[removed],0,1
1942,2019-5-31,2019,5,31,21,bv6p27,What Is Cross Validation Technique in Machine Learning and its Process?,https://www.reddit.com/r/MachineLearning/comments/bv6p27/what_is_cross_validation_technique_in_machine/,digvijaykrtiwari,1559304353,,0,1
1943,2019-5-31,2019,5,31,21,bv6q2t,"""Defending Against Neural Fake News"" (Check out the generated samples in the appendix- amazing)",https://www.reddit.com/r/MachineLearning/comments/bv6q2t/defending_against_neural_fake_news_check_out_the/,no_bear_so_low,1559304539,,92,230
1944,2019-5-31,2019,5,31,21,bv6ta3,Performance Measures: Cohen's Kappa Statistic,https://www.reddit.com/r/MachineLearning/comments/bv6ta3/performance_measures_cohens_kappa_statistic/,TheTesseractAcademy,1559305090,,0,1
1945,2019-5-31,2019,5,31,21,bv6u1x,[D] Deploying deep learning models into production without HTTP overhead,https://www.reddit.com/r/MachineLearning/comments/bv6u1x/d_deploying_deep_learning_models_into_production/,ixeption,1559305229,"Hi folks,

when it comes to deployment and integration of deep learning models, it seems like there are a lot of solutions out there, but the most are kind of complex or depend on third party solutions. While it often seems to be a good idea to containerize models and deploy them the modern microservices way, I think this approach is kind of heavy and needs a lot of infrastructure, therefore is not the best idea as small or medium sized company.

I want to share an alternative approach, which can be used easily by just utilizing what python and e.g. the Java ecosystem offers out-of-the-box. [Link](http://digital-thinking.de/deploying-deep-learning-models-into-production-without-famework-overhead/)

What about you, how do you productionize deep neural networks?

Cheers",9,0
1946,2019-5-31,2019,5,31,22,bv7j2x,Papers on the intersection between signal processing and NLP,https://www.reddit.com/r/MachineLearning/comments/bv7j2x/papers_on_the_intersection_between_signal/,BlandBiryani,1559309350,[removed],0,1
1947,2019-5-31,2019,5,31,23,bv81s4,"Open Questions in Model-based RL (Sutton, 2019)",https://www.reddit.com/r/MachineLearning/comments/bv81s4/open_questions_in_modelbased_rl_sutton_2019/,yazriel0,1559312136,,1,1
1948,2019-5-31,2019,5,31,23,bv84i6,"[D] Open Questions in Model-based RL, Sutton 2019",https://www.reddit.com/r/MachineLearning/comments/bv84i6/d_open_questions_in_modelbased_rl_sutton_2019/,yazriel0,1559312539,,1,1
1949,2019-5-31,2019,5,31,23,bv87oy,[D] Reinforcement Learning with Policy Mixture Model for Temporal Point Processes Clustering,https://www.reddit.com/r/MachineLearning/comments/bv87oy/d_reinforcement_learning_with_policy_mixture/,mellow54,1559313023,,4,16
1950,2019-5-31,2019,5,31,23,bv8b3t,OpenAI's Christine Payne on the Mind The Machines podcast (MuseNet + AI ethics),https://www.reddit.com/r/MachineLearning/comments/bv8b3t/openais_christine_payne_on_the_mind_the_machines/,jeremie-harris,1559313488,,0,1
