,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2015-10-1,2015,10,1,9,3n1fak,"[Question] Applied Caffe implementation of this white paper, does it exist? How to find it?",https://www.reddit.com/r/MachineLearning/comments/3n1fak/question_applied_caffe_implementation_of_this/,osiris679,1443659200,,4,1
1,2015-10-1,2015,10,1,9,3n1gsv,Artificial Tasks for Artificial Intelligence - Facebook Presentation,https://www.reddit.com/r/MachineLearning/comments/3n1gsv/artificial_tasks_for_artificial_intelligence/,napsternxg,1443659856,,0,5
2,2015-10-1,2015,10,1,10,3n1kme,How do I model a combination of categorical and continuous variables?,https://www.reddit.com/r/MachineLearning/comments/3n1kme/how_do_i_model_a_combination_of_categorical_and/,geekybarnstar,1443661568,"I have a dataset where the explanatory variables are continuous variables. The variable I'm trying to predict has two possible values : either 'no occurrence' or a value between 0 and 100. How can I model such a relationship?

If it were yes/no, I would have simply gone for logistic regression. Or something else if this were a continuous  variable alone. How should I adopt such a problem?

Should I make all 'no occurrence' values to zero? Not exactly true in my case....but...",12,6
3,2015-10-1,2015,10,1,11,3n1x2z,What's the best way to represent time of minute/hour/day/week/month in the input space?,https://www.reddit.com/r/MachineLearning/comments/3n1x2z/whats_the_best_way_to_represent_time_of/,[deleted],1443667129,[deleted],11,18
4,2015-10-1,2015,10,1,12,3n263b,Corporate logos dataset - any commercial or OSS datasets?,https://www.reddit.com/r/MachineLearning/comments/3n263b/corporate_logos_dataset_any_commercial_or_oss/,mikos,1443671432,"I know of the flickr32, I was looking for labeled dataset containing logos covering most company/product logos. Any suggestions?",1,1
5,2015-10-1,2015,10,1,12,3n26ys,Capital Machinery Sales,https://www.reddit.com/r/MachineLearning/comments/3n26ys/capital_machinery_sales/,chiroseo,1443671884,,0,0
6,2015-10-1,2015,10,1,15,3n2oan,[Question] Is it normal to have difficulty remembering specific algorithms?,https://www.reddit.com/r/MachineLearning/comments/3n2oan/question_is_it_normal_to_have_difficulty/,Gay_Hat_On_Nun,1443682303,"I'm a newbie with machine learning, but I'm thoroughly interested in it and am learning as much as I can about it. However, I am not very good at memorizing formulas and such. What I excel in is learning the actual concepts of the algorithms and the intuitive reasoning. So when I am presented with algorithms(example: hypothesis function for logistic regression), I find myself forgetting the actual algorithm. Yet I still understand the concept itself. I feel that part of the problem may be the fact that when I try to look up the derivation or explanation of the algorithm itself, most of it goes well over my head since I have not taken a calculus class yet(currently taking one). I understand the concepts of how it works in general, but I tend to have trouble remembering the specific formula/algorithm and I never really understand where it comes from. Is this normal(as in, is this a standard part of the learning curve)? And if so, are there any tips to reduce the learning curve and just have a firmer grasp of the mathematics and reasoning behind machine learning?

Thanks. :)",6,4
7,2015-10-1,2015,10,1,15,3n2osg,Global Accelerator Pedal Module Market 2015-2020,https://www.reddit.com/r/MachineLearning/comments/3n2osg/global_accelerator_pedal_module_market_20152020/,roshanimrr,1443682682,,0,1
8,2015-10-1,2015,10,1,16,3n2r70,[Question]SoTA for specific object recognition,https://www.reddit.com/r/MachineLearning/comments/3n2r70/questionsota_for_specific_object_recognition/,EigenFace,1443684424,"Hey, I want to analyze videos of a 2d game and recognize when certain items are on the screen or certain animations are played. That is, I want to detect when **specific** objects are on the screen, objects which never rotate, but may appear in different lighting, be larger or smaller, may be partially occluded, or shifted about the screen. Let's say there are about 1000 of these specific objects.

My initial thought was that this would be pretty easy because convnets are good at detecting non-specific objects which are 3d and vary rotationally from a large class of objects, and my problem is much simpler than that. However, now I am not so sure. 

Any recommendations for the best algorithms or papers on this problem? Any results for using CNNs for thing like this, or maybe even using RNNs to matching specific animations(sequences of images)?

Edit: Many of the objects may also vary wildly in color to the point where no colors are the same between two instances of the same object(palettes are changed). Shape is invariant.",2,1
9,2015-10-1,2015,10,1,19,3n36vc,Support Vector Machines Tutorials and Case Studies,https://www.reddit.com/r/MachineLearning/comments/3n36vc/support_vector_machines_tutorials_and_case_studies/,[deleted],1443696652,[deleted],0,0
10,2015-10-1,2015,10,1,22,3n3jb4,Possible to build a profile of a typical class member?,https://www.reddit.com/r/MachineLearning/comments/3n3jb4/possible_to_build_a_profile_of_a_typical_class/,pfizer_soze,1443704715,"I have thousands of observations and 20+ categorical characteristics (a lot more in binary form). Is there some method that can be used to build a profile of the typical record of a certain class? For example, if country of origin and industry are important characteristics of one class, is there a statistical method that I can use to determine that records in class A are typically from country X and business Y when that relationship exists?

I figure I could use a binary classifier to find out which characteristics have strong positive predictive value, but I was hoping to learn some new tricks. Is something like the Kullback-Leibler divergence applicable here? A random forest did not seem appropriate because a single variable can have different directional impacts at different depths of the tree.

edit: Does every post that isn't about neural networks get downvoted here?",2,4
11,2015-10-1,2015,10,1,22,3n3jua,Symbol Emergence in Robotics: A Survey,https://www.reddit.com/r/MachineLearning/comments/3n3jua/symbol_emergence_in_robotics_a_survey/,InaneMembrane,1443704975,,0,8
12,2015-10-1,2015,10,1,23,3n3q0d,Machine Learning to predict the startup success???,https://www.reddit.com/r/MachineLearning/comments/3n3q0d/machine_learning_to_predict_the_startup_success/,Pavan19485,1443708015,,4,0
13,2015-10-2,2015,10,2,0,3n40zo,"[question] ""Dynamic"" Neural Networks",https://www.reddit.com/r/MachineLearning/comments/3n40zo/question_dynamic_neural_networks/,Kiuhnm,1443712928,"I'm new to machine learning and I'm wondering if there are any studies about neural networks which can change their neural interconnections during learning. In other words, they don't only learn in the usual sense but also learn how to learn.",5,3
14,2015-10-2,2015,10,2,0,3n41an,A Word of Caution on Using Scheduled Sampling (Google's method for Training RNNs which won the image captioning challenge),https://www.reddit.com/r/MachineLearning/comments/3n41an/a_word_of_caution_on_using_scheduled_sampling/,fhuszar,1443713059,,17,58
15,2015-10-2,2015,10,2,1,3n4acf,"Semantics, Representations and Grammars for Deep Learning",https://www.reddit.com/r/MachineLearning/comments/3n4acf/semantics_representations_and_grammars_for_deep/,julian88888888,1443716770,,0,6
16,2015-10-2,2015,10,2,1,3n4dum,Intuition behind Gaussian Processes,https://www.reddit.com/r/MachineLearning/comments/3n4dum/intuition_behind_gaussian_processes/,Zephyr314,1443718219,,1,23
17,2015-10-2,2015,10,2,1,3n4esi,Becoming an expert on neutral networks,https://www.reddit.com/r/MachineLearning/comments/3n4esi/becoming_an_expert_on_neutral_networks/,atomant30,1443718601,I have a pretty solid understanding of the basics of neutral networks. What should I be reading to become an expert in the field?,3,0
18,2015-10-2,2015,10,2,2,3n4lyh,Would generating image datasets with 3D graphics aid in object recognition?,https://www.reddit.com/r/MachineLearning/comments/3n4lyh/would_generating_image_datasets_with_3d_graphics/,[deleted],1443721516,[deleted],4,4
19,2015-10-2,2015,10,2,3,3n4u9i,Need resources for deep learning,https://www.reddit.com/r/MachineLearning/comments/3n4u9i/need_resources_for_deep_learning/,code2hell,1443724927,Can people please share resources for learning practical deep learning algorithms preferably in python. A tutorial with implemented examples (ex. Imagenet) will be great. Thanks! Hope many people to get good materials from this post. ,1,0
20,2015-10-2,2015,10,2,3,3n4v4m,AWS Costs: the 5 most common mistakes,https://www.reddit.com/r/MachineLearning/comments/3n4v4m/aws_costs_the_5_most_common_mistakes/,[deleted],1443725262,[deleted],0,1
21,2015-10-2,2015,10,2,4,3n4z8i,Saving Lives with Data: Python and Global Health,https://www.reddit.com/r/MachineLearning/comments/3n4z8i/saving_lives_with_data_python_and_global_health/,marcoborracho,1443726917,,0,12
22,2015-10-2,2015,10,2,4,3n50g4,Compute Gradient for Individual Instances with Backprop Variant?,https://www.reddit.com/r/MachineLearning/comments/3n50g4/compute_gradient_for_individual_instances_with/,alexmlamb,1443727430,"Hello, 

The usual minibatch gradient descent involves computing dsum(L[i],i) / dw.  That is, the derivative of the total loss over the minibatch with respect to the parameters.  

I am interested in doing a similar computation, but where I instead get the gradient for each individual instance in the minibatch.  dL[i] / dw.  I still want to use minibatches and GPUs for computational reasons.  

My main goal is to research alternatives to gradient clipping, which involve looking at the gradient for each instance separately.  

Best, 

Alex.  ",6,5
23,2015-10-2,2015,10,2,5,3n599j,Introductory slides on Deep Learning and Neural Networks for General Audience,https://www.reddit.com/r/MachineLearning/comments/3n599j/introductory_slides_on_deep_learning_and_neural/,napsternxg,1443731121,,0,1
24,2015-10-2,2015,10,2,6,3n5lt0,A few questions about training large RNN's,https://www.reddit.com/r/MachineLearning/comments/3n5lt0/a_few_questions_about_training_large_rnns/,jamesj,1443736493,"I've been using Karpethy's [char-rnn](https://github.com/karpathy/char-rnn) to train some models for a while now and have been trying to develop an intuition for it. I have a 500MB data set, and started with models using 10MB of it. Once I got those working well, I went to 20MB, then 50MB, then 100MB without too many issues. 

1. My validation loss begins close to 3.8. In most cases it will extremely quickly drop to 1.4 or 1.5, then decrease towards my current best of 1.1-1.3 depending on my hyperparameters. Sometimes if I let it go for a while the validation loss moves up and down a bit. I took the low points of these checkpoints, and made the learning rate smaller. This got me pretty good gains. I then kept doing this with smaller and smaller learning rates. My intuition is that with large learning rates you can jump over local minimums. I can only get into the local minimum with a smaller rate. Each time the validation loss goes up and down this is a new potential minimum that I can explore with a lower learning rate. Each time I jump over it, I have no idea how low it might be. Is this accurate? If so I can just let it drift up and down at the high rate, then take each of my low checkpoints and turn my rate down and start from those, then rinse and repeat for ever smaller learning rates. I've decreased my validation loss a lot doing this, but maybe I'm just overfitting? Or maybe just reducing my rate when it starts to level off is the only thing that matters, and I don't need to worry about picking these minimums?

2. I tried to make the jump up to 200MB and now it isn't converging at all. I made the net sizes a lot larger, I have ~50million parameters (3 layers of 1800). My 100MB dataset was converging fine with ~20million paramaters (3 layers of 980), though I think it was underfitting by a bit. I still got my best results using the 100MB dataset and model. Do I need a smaller learning rate, or are there other tricks once the model starts getting pretty big? I'd like to use my entire dataset if I can, and my cards can handle pretty large models. I should probably just lower the size and/or learning rate and increase it until it stops converging, but was hoping changing a different parameter might be a faster/better way of going about it.

Thanks for any help!",7,12
25,2015-10-2,2015,10,2,7,3n5oxm,Neural Algorithm of Artistic Style for Videos ?,https://www.reddit.com/r/MachineLearning/comments/3n5oxm/neural_algorithm_of_artistic_style_for_videos/,mreima,1443737900,"After reading the [paper on artistic style transfer](http://gitxiv.com/posts/jG46ukGod8R7Rdtud/a-neural-algorithm-of-artistic-style), I wonder how this technique can be applied to videos ?

When applying the algorithm frame by frame, there is a big amount of incoherence, which results in a noisy video ([example](http://cdn.makeagif.com/media/10-01-2015/acbwkj.gif)).

How can temporal coherence be achieved ?

I have some fuzzy ideas, but as I'm not a deep learning researcher, I don't know if and how they can be applied:

* using optical flow to penalize strong changes in style application while optimizing the image
* using recurrent neural networks to remember the last style application and reapply this on the new frame

Any thoughts?",10,4
26,2015-10-2,2015,10,2,7,3n5sjw,Would it be possible to learn a neural net architecture (i.e. not just tune existing weights) by gradient descent?,https://www.reddit.com/r/MachineLearning/comments/3n5sjw/would_it_be_possible_to_learn_a_neural_net/,onlyml,1443739543,"What I mean by that is essentially allowing the network to dynamically alter its basic structure in some manner. 

To be concrete lets say we have a convolutional network and we leave the number of features in each layer as free parameters. Obviously this would be a little nontrivial to implement as it would have to be done in a differentiable manner.

One way I could see this working is training the network with one additional feature per layer but with the outputs of these features weighted by a softmax over the layers. This way the network would have to select where it was most valuable to have this partial feature. Presumably we would regularize the weights so that the partial features were actually inferior to full features.

After allowing the network to train like this for a while we could just take the partial feature with the largest weighting (possibly weighted by computational cost of the addition) and actually add it as a full feature to the network. Iterate until we reach some allowed computation budget.

Would this be feasible/practical to do? Has anything like this been done?",18,16
27,2015-10-2,2015,10,2,8,3n5vdg,Open source platform for controlling games with AI,https://www.reddit.com/r/MachineLearning/comments/3n5vdg/open_source_platform_for_controlling_games_with_ai/,rt4fun,1443740862,,4,18
28,2015-10-2,2015,10,2,9,3n64th,Project Jupyter: collaboration is the name of the game,https://www.reddit.com/r/MachineLearning/comments/3n64th/project_jupyter_collaboration_is_the_name_of_the/,shugert,1443745368,,1,0
29,2015-10-2,2015,10,2,10,3n6bnr,How To Avoid a Bad Data-Drive Decision That Will Make You Broke,https://www.reddit.com/r/MachineLearning/comments/3n6bnr/how_to_avoid_a_bad_datadrive_decision_that_will/,shugert,1443748706,,1,0
30,2015-10-2,2015,10,2,10,3n6de5,Is my vectorized Softmax derivative correct?,https://www.reddit.com/r/MachineLearning/comments/3n6de5/is_my_vectorized_softmax_derivative_correct/,danijar,1443749555,,3,4
31,2015-10-2,2015,10,2,11,3n6h4g,Struggling with Machine Learning course,https://www.reddit.com/r/MachineLearning/comments/3n6h4g/struggling_with_machine_learning_course/,machLearnerthrowAway,1443751375,I am highly interested in pursuing Machine Learning in my career but I seem to be struggling with the course offered at my university.  It is heavily math oriented/theoretical. Please let me know how I can prepare myself better to understand the coursework. The link to the course is here :http://l2r.cs.uiuc.edu/~danr/Teaching/CS446-15/,7,6
32,2015-10-2,2015,10,2,14,3n71r9,"Unusual question about types of companies dealing in data that ""makes a difference""? And opinions on classes",https://www.reddit.com/r/MachineLearning/comments/3n71r9/unusual_question_about_types_of_companies_dealing/,ThrowawayTartan,1443762327,"Hey there! 

**Question 1**So I understand that this is a somewhat strange question but I was wondering if anyone knew of any other companies like Palantir which deal with using data to ""make a difference""? 

Okay, I'm exaggerating of course- I think most people feel like their data is making a difference. I'll explain myself more: I'm currently looking around to apply for internships over the Summer of 2016. Last Summer I dealt with data related to health, and predicting if a user would get injured and such. It was a lot of fun, but it was a young startup and it came with a lot of great things, but also some things that weren't necessarily great: 

i) the nature of it being a startup meant that often the data was sparse and I had to get it myself/ I had to do a lot of ground-up work.
ii) There were a lot of deliverables: meaning there were times where efficiency of an algorithm wasn't necessarily what mattered, where a lot of it felt like Google and Stack Overflow were my answers and I wasn't using the best thing that was out there/ approaching the problem the right way.
iii) In some sense I was working with data but with the end goal of a product in mind. The objective was to sell the product, and to sell the product we needed some degree of accuracy. What I imagine/ I'm thinking about in a company is a company that is concerned with making sense of what is happening and then selling that (so kind of the reverse of the startup)

**Q1: Tl;dr care to recommend data companies that deal with ""important data""? Only caveat is that it needs to be in Europe or in America.**

**Question 2** I've been considering a double major in statistics and machine learning to help me in an application to grad school but I don't know how rigorous it actually is. My school is pretty decent when it comes to ML but the courses required for the double surprised me in that they seem more focused on statistics and less about math? Anyone have opinions on whether I should just switch or do the courses that I think are relevant [stat-ml](http://www.stat.cmu.edu/new-majors-launch/program/stat-ml.html) I've been thinking about taking algebraic structures(might just skip this), [linear algebra (really really old syllabus)](http://www.math.cmu.edu/~handron/21_341/03spring/sched.html) - has a pre-req of [matrix algebra](http://www.math.cmu.edu/~handron/21_241/schedule.html) and real analysis on top of the classes that sound more technical on that list. The classes on that list that I think are technical and haven't actually taken yet are 10-701, 15-351, 10-605, 36-401 and 36-402. The others I've either already taken or I think aren't that math-inclined. My problem is that I don't have much time left here so I can't do everything or I definitely would.

**Q2: Tl;dr What should I take classes that are rigorous but not a major that supposedly prepares you for grad school, or a major that supposedly prepares you for grad school (and I get the fancy cert) but I don't think prepares me well enough for grad school.**

Sorry if my questions seem long winded/ if what I'm posting isn't allowed here/ I've come across as a violent asshole who thinks he's really smart and is looking down on things organized by people far smarter than me. I'm just really worried about my future. It's been a long day with a lot of discussions with professors about my future ( on top of having exams) I thought of posting this to /r/datasciencejobs or /r/bigdatajobs but all the posts there I've seen have been posts about hiring and for hire people not necessarily a discussion.",3,3
33,2015-10-2,2015,10,2,19,3n7mcc,Which embeddings quality measures are used?,https://www.reddit.com/r/MachineLearning/comments/3n7mcc/which_embeddings_quality_measures_are_used/,olBaa,1443781367,"Word2vec, for example: are there any way to check the quality of the embedding, besides that analogy dataset Mikolov has provided. Are there any other examples in different areas?",5,6
34,2015-10-2,2015,10,2,21,3n7x9r,Article about Bayes' Theorem for very beginners,https://www.reddit.com/r/MachineLearning/comments/3n7x9r/article_about_bayes_theorem_for_very_beginners/,kiote_the_one,1443789298,,8,19
35,2015-10-2,2015,10,2,22,3n8347,What are techniques that can be used to classify rare events?,https://www.reddit.com/r/MachineLearning/comments/3n8347/what_are_techniques_that_can_be_used_to_classify/,o_safadinho,1443792488,"For example, if 98% of your group falls into one group and only 2% of your sample falls into another group.

I know that a modified version of logistic regression can be used, but what else is there?",10,6
36,2015-10-2,2015,10,2,22,3n85ea,"Slides from the DArViN2015 workshop, Wrocaw, Poland",https://www.reddit.com/r/MachineLearning/comments/3n85ea/slides_from_the_darvin2015_workshop_wrocaw_poland/,r4and0muser9482,1443793647,,3,5
37,2015-10-2,2015,10,2,23,3n86w0,Libraries for Top 10 Machine Learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/3n86w0/libraries_for_top_10_machine_learning_algorithms/,[deleted],1443794405,[deleted],0,0
38,2015-10-2,2015,10,2,23,3n88oj,"Show /r/ml: Machine Learning is Magic- a Chrome extension that converts ""machine learning algorithm"" to ""magic spell""",https://www.reddit.com/r/MachineLearning/comments/3n88oj/show_rml_machine_learning_is_magic_a_chrome/,dunnowhattoputhere,1443795287,,0,0
39,2015-10-2,2015,10,2,23,3n88xu,DeepMind's huge machine reading question/answer dataset is now available for public use,https://www.reddit.com/r/MachineLearning/comments/3n88xu/deepminds_huge_machine_reading_questionanswer/,egrefen,1443795397,,12,162
40,2015-10-3,2015,10,3,0,3n8g28,Gradient clipping: what are good values to clip at and why?,https://www.reddit.com/r/MachineLearning/comments/3n8g28/gradient_clipping_what_are_good_values_to_clip_at/,polytop3,1443798651,"In the context of deep neural networks, RNNs, and LSTMs, what are good values to use for gradient clipping? Is there any rationale for choosing that specific value? And does the answer depend on network type?",2,6
41,2015-10-3,2015,10,3,3,3n9b6k,"HashRobot: a social media assistant built with Rails, jQuery and MonkeyLearn",https://www.reddit.com/r/MachineLearning/comments/3n9b6k/hashrobot_a_social_media_assistant_built_with/,wildcodegowrong,1443811963,,0,0
42,2015-10-3,2015,10,3,3,3n9br5,"Do Convolutional Neural Networks learn the parameters of each kernel, or the weights applied to the kernel",https://www.reddit.com/r/MachineLearning/comments/3n9br5/do_convolutional_neural_networks_learn_the/,PieThon,1443812202,"In a CNN I understand that there can be N numbers of kernels applied to the input image. The output of this would then be connected to either a pooling layer or other convolutions or a fully connected network.

When back propagating through the convolutional layers, does the network update the values in the individual kernels, meaning the network learns the kernels as it trains? If so, is there a guideline for the number of kernels needed to classify data given X number of labels?",2,1
43,2015-10-3,2015,10,3,4,3n9cuy,Example scenario using cassandra-stress to plan out a new Cassandra cluster by Spotify.,https://www.reddit.com/r/MachineLearning/comments/3n9cuy/example_scenario_using_cassandrastress_to_plan/,dot_2,1443812670,,0,1
44,2015-10-3,2015,10,3,4,3n9exp,Machine Learning for US Political Debates,https://www.reddit.com/r/MachineLearning/comments/3n9exp/machine_learning_for_us_political_debates/,llSourcell,1443813596,"I have an idea for a chrome extension for political debate videos on youtube. Every time a politician cities a numerical statistic, the app queries the web and returns a boolean if its true or false. Each politician gets a % score out of a 100 based on their trustworthiness. 

I'm thinking about how to implement this and would love your help. 

High level
Step 1: Convert speech to text
Step 2: segment sentences that contain numbers
Step 3: Query the web for that segment
Step 4: update UI with boolean result


All of this seems very doable to me except for step 3. For the rest, I can just use a client-side NLP library. 

Is there a fact checker API out there I could use? If not I'm willing to attempt to build one. How would I do that? More importantly, should I even try on this one? If Google's making significant progress in this space, it might not be worth it. 
",0,0
45,2015-10-3,2015,10,3,5,3n9oa4,A nice series of lectures on Probabilistic Modeling by Iain Murray,https://www.reddit.com/r/MachineLearning/comments/3n9oa4/a_nice_series_of_lectures_on_probabilistic/,ojaved,1443817572,,0,1
46,2015-10-3,2015,10,3,6,3n9wy7,Can we use deep learning for Regression problem?,https://www.reddit.com/r/MachineLearning/comments/3n9wy7/can_we_use_deep_learning_for_regression_problem/,nimakhin,1443821389,"Hi 

I have a dataset for which i have to use deep learning in order to predict value of some element based on other elements. So i think it is a regression problem. right? 

can deep learning be trained to be used for regression? which architecture is the best for this purpose ? (RBM or SDAE)

what is the best software (framework) for beginners like me who knows just the basic concepts of deep learning. 

thanks ",4,0
47,2015-10-3,2015,10,3,7,3na75m,A new release of Deep Learning book draft,https://www.reddit.com/r/MachineLearning/comments/3na75m/a_new_release_of_deep_learning_book_draft/,clbam8,1443826185,,0,92
48,2015-10-3,2015,10,3,11,3nasee,What are some useful ML libraries to develop?,https://www.reddit.com/r/MachineLearning/comments/3nasee/what_are_some_useful_ml_libraries_to_develop/,ginger_beer_m,1443837613,"Suggest any library that you feel is still lacking beyond the standard off-the-shelf solutions such as scikit-learn etc. Doesn't have to be specifically ML, could be other related activities too, eg visualisation etc.",4,5
49,2015-10-3,2015,10,3,16,3nbl9b,AI lacks deep self-reference so is unable to work with quines. Manually coding many special cases wont help you this time.,https://www.reddit.com/r/MachineLearning/comments/3nbl9b/ai_lacks_deep_selfreference_so_is_unable_to_work/,BenRayfield,1443857180,"A quine is a string of code that outputs itself. This is rigid and not much useful by itself, but if we relax the constraint to output something similar to itself, like in evolution or designing of variations of itself, thats where it gets interesting.

Its reasonable for low level code, like how many bits are in a number, to not be part of the quine, but at some level there should be turing complete logic and statistics that quines itself within that level, or some variation of itself. The code which creates the variations of itself should itself be varied as part of the quine-like structure.

Thats the main thing I see missing from AIs these days, and I hope more research is done in that area. Its not easy to get started, but I hope to get there someday when my parts are simplified and more self-referencing.",3,0
50,2015-10-3,2015,10,3,16,3nblqz,Describing videos by exploiting temporal structure (paper+code),https://www.reddit.com/r/MachineLearning/comments/3nblqz/describing_videos_by_exploiting_temporal/,samim23,1443857637,,0,3
51,2015-10-3,2015,10,3,19,3nbv27,Similarity / clustering methods for temporal event data,https://www.reddit.com/r/MachineLearning/comments/3nbv27/similarity_clustering_methods_for_temporal_event/,djharsk,1443866851,"Disclaimer: I have also posted this question to [SO](http://stats.stackexchange.com/questions/175244/similarity-clustering-methods-for-temporal-event-data) yesterday. If you feel this is inappropriate, please delete the question.

I am looking for methods to calculate a reasonable similarity metric or clustering for my data. I have data in the form of a number of distinct events (lets call them A, B, C, D, ...), and the time between the events. A few example data points could look like,

    X1 = [(A, 0), (B, 2.2), (C, 3.2), (A, 4.0)]
    X2 = [(B, 0), (C, 2.2)]
    X3 = [(B, 0), (C, 2.2), (D, 4.2)]
    X4 = [(B, 0), (A, 2.2), (E, 1.5)]
    X5 = [(B, 0), (F, 2.2), (E, 1.5), (F, 5.0), (G, 2.0), (H, 2.0)]
    X6 = [(B, 0), (A, 0.0), (E, 1.5), (F, 5.0), (G, 2.0)]
    X7 = [(I, 0), (J, 0.0), (K, 1.5), (L, 5.0), (M, 2.0)]

To explain the format: each entry in the array is in the format event, time since first event. E.g. for X1, at time=0, event A happened. At time=3.2 (after event A), event C happened, and so forth. Also note that multiple events may happen at the same time. 

In reality, the sequences are typically much longer (50-100ish). The ""vocabulary"" of events is 1500ish. The time between two events ranges from 0.0-100.  

I have a good few million data points, give or take, so I have been trying to look into a clustering based approach. From the above example, I would expect X1, X2, and X3 to have some similarity (X4 maybe, since the pattern A-&gt;B is just reversed). Also, X5 and X6 should be fairly similar. X7 should not have any similarity to any of the patterns. This is a somewhat fuzzy explanation, and that is intended. I am not looking for the one method that clusters perfectly - I am just looking for one that does an OK job taking all variables into consideration

I have been looking into things like Latent Dirichlet Allocation, Self Organising Maps, unsupervised Recurrent Neural Networks, Autoencoders, just counting the similarity between triplets, ...

My major problems in finding something reasonable has been,

1. The method expects an equal number of features
2. The method sees the sequence as a bag-of-words
3. The method does not implicitly handle a mixture of categorical and continous data
4. The method has no notion of time
 
I put some very broad categories on this question, simply because I am looking for suggestions or pointers to software/papers. 

Any help would be much appreciated, and please do comment if something is not clear. 
",2,5
52,2015-10-3,2015,10,3,19,3nbxbp,Machine Learning to predict the startup success???,https://www.reddit.com/r/MachineLearning/comments/3nbxbp/machine_learning_to_predict_the_startup_success/,Pavan19485,1443868944,"Check this interesting blog post and throw your inputs regarding this topic

http://machinelearninghub.blogspot.com.es/2015/10/machinelearning-to-predict-startup.html",0,0
53,2015-10-3,2015,10,3,23,3ncilt,"Great primer on ""Neural networks for natural language processing""",https://www.reddit.com/r/MachineLearning/comments/3ncilt/great_primer_on_neural_networks_for_natural/,cast42,1443883824,,3,101
54,2015-10-3,2015,10,3,23,3ncizv,Differentiable computation of condition number,https://www.reddit.com/r/MachineLearning/comments/3ncizv/differentiable_computation_of_condition_number/,AnvaMiba,1443884028,"In this paper ""[High-Dimensional Probability Estimation with Deep Density Models](http://arxiv.org/abs/1302.5125)"", the authors train invertible neural networks. In order to do so, they include in the training cost an ""invertibility measure"", which is the sum of the logarithms of the condition numbers of the weight matrices.

I have two issues with that. First, computing the condition number has O( n^3 ) complexity, where n is the dimension of the square matrix. Second, and more importantly, in order to use this term in the training cost, you need to compute its gradient w.r.t. the weights. Technically, if I understand correctly, the condition number is differentiable almost everywhere, but all the algorithms that I've seen are quite complicated, involving matrix decompositions and iterations, and I doubt you could successfully backpropagate through them, hence you would need to estimate the gradient using something like the finite differences method, which would be very expensive.

So my questions are: is this ""invertibility measure"" really needed, given that you initialize with an orthogonal or near-orthogonal matrix and almost all square matrices are invertible? (The authors themselves claim that the invertibility measure didn't constrain the model by much in their experiments).  
If it is needed, how do I compute its gradient?

EDIT:

Found a [formula](http://www.win.tue.nl/casa/meetings/seminar/previous/_abstract051019_files/Presentation.pdf) to analitically compute the derivatives of eigenvalues from eigenvectors.
",1,4
55,2015-10-4,2015,10,4,1,3ncrnr,Fun with Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/3ncrnr/fun_with_reinforcement_learning/,CireNeikual,1443888433,,0,2
56,2015-10-4,2015,10,4,1,3ncw6z,Five principles for applying data science for social good,https://www.reddit.com/r/MachineLearning/comments/3ncw6z/five_principles_for_applying_data_science_for/,gradientflow,1443890635,,0,6
57,2015-10-4,2015,10,4,2,3nd2x7,The Importance of Human Innovation in Artificial Intelligence Ethics,https://www.reddit.com/r/MachineLearning/comments/3nd2x7/the_importance_of_human_innovation_in_artificial/,johnchavens,1443893869,,0,0
58,2015-10-4,2015,10,4,4,3ndf0y,What are the pros and cons for recommender systems based on global poularity of items?,https://www.reddit.com/r/MachineLearning/comments/3ndf0y/what_are_the_pros_and_cons_for_recommender/,rohanpota,1443899382,"Does it provide:-

    1)Personalization
    2)features
    3)does it capture context?
    4)solve cold start problem(atleast partially)?",0,0
59,2015-10-4,2015,10,4,4,3ndffw,Clustering of multivariate time series?,https://www.reddit.com/r/MachineLearning/comments/3ndffw/clustering_of_multivariate_time_series/,[deleted],1443899581,[deleted],1,6
60,2015-10-4,2015,10,4,4,3ndgvw,The Curious AI Company: a new startup in Helsinki by Harri Valpola to focus on unsupervised learning.,https://www.reddit.com/r/MachineLearning/comments/3ndgvw/the_curious_ai_company_a_new_startup_in_helsinki/,fhuszar,1443900201,,3,14
61,2015-10-4,2015,10,4,4,3ndh5f,Giving machine-learning systems partial credit during training improves image classification.,https://www.reddit.com/r/MachineLearning/comments/3ndh5f/giving_machinelearning_systems_partial_credit/,MyMomSaysImHot,1443900326,,2,7
62,2015-10-4,2015,10,4,5,3ndryg,A Huge List of Machine Learning And Statistics Repositories,https://www.reddit.com/r/MachineLearning/comments/3ndryg/a_huge_list_of_machine_learning_and_statistics/,josephmisiti,1443905506,,1,6
63,2015-10-4,2015,10,4,7,3ne2p7,Cross-Entropy vs. Mean square error,https://www.reddit.com/r/MachineLearning/comments/3ne2p7/crossentropy_vs_mean_square_error/,Aerospacio,1443910795,"I've seen when dealing with MNIST digits that cross-entropy is always used, but none elaborated on why. What is the mathematical reason behind it?

Thanks in advance!",4,10
64,2015-10-4,2015,10,4,15,3nfelb,Unboxing the Random Forest Classifier: The Threshold Distributions,https://www.reddit.com/r/MachineLearning/comments/3nfelb/unboxing_the_random_forest_classifier_the/,cast42,1443939392,,1,18
65,2015-10-4,2015,10,4,15,3nfh45,Integration of multiple kinds of NN together (vision+NLP+voice) as the path to AGI ?,https://www.reddit.com/r/MachineLearning/comments/3nfh45/integration_of_multiple_kinds_of_nn_together/,Schlagv,1443941497,"I am just a ML hobbyist, but I am surprised as I rarely or never see papers here about integrating many NN together.

What I mean by this is that we take a NN for vision, one for NLP, one for voice recognition and so on. Then we feed the last hidden layer of each NN as input of all the NNs, communicating context between all the NNs.

Everytime I see or read things about neuroscience, they always say how we activate other parts of the brain when we think about something and how other senses interact together. How we activate specific motor parts of our brain when we think about a task that involves moving those muscules. How we activate the fear module of our brain just by remembering a traumatic event.

But in ML, we always do single tasks, with the expectation that training a single task well will get better results than having the cooperation of many modules harder to train.

I find Imagenet especially ridiculous as we try to learn everything from vision only. I would intuitively expect having a NLP module would massively increase context understanding. Same thing with sounds. We see many things about videos but they often just are conv nets where we feed several pictures at once to see motion, just by adding more channels. They are not multisense architectures. They are just a convnet trick.

Also, another related topic is general intelligence. It seems obvious that our brain works by having a few dozen modules connected to each other as a graph and that our complex decision making just comes from having modules competing against each other.

But I never saw a paper that tried to create a generalised architecture to have many modules interact together. A universal handling of context, independant of the type of network used inside the NLP RNN, the vision convnet and so on.

For example, we see many NNs to do picture description these days. They are simple convnets, where once trained we put a second NLP module to generate a sentence based on the last hidden layer of the convnet.

But there is no looping. There is no mechanism so that while writing the description, the NLP module calls a third module ""attention manager"" to zoom into a part of the image, redo the convnet forward computation and then we have a more detailed input for the NLP module, so that the description is more accurate. That would be looping, the NLP module would not simply describe the output context of the vision module, the NLP module would tell the vision module to provide more details on a part of the image.

Of course, this would be dirty and full of tricks, that's why I spoke before of a having a generalised way to feed ouput layers into input layers of all the modules, so that ""contexts"" are helping each module to do a better job.

What I said is a little messy, but my question is: Why don't we see more work on generalised architectures with lots of modules instead of massive single module, purely feedforward, single task, AIs ?",9,8
66,2015-10-4,2015,10,4,18,3nfpcw,Perpetual Learning Machines: Deep Neural Networks with Brain-like On-The-Fly Learning &amp; Forgetting [YouTube],https://www.reddit.com/r/MachineLearning/comments/3nfpcw/perpetual_learning_machines_deep_neural_networks/,ajrs,1443949452,,4,2
67,2015-10-4,2015,10,4,21,3ng0yv,A ConvNet trying to learn Flappy Bird [more in comments],https://www.reddit.com/r/MachineLearning/comments/3ng0yv/a_convnet_trying_to_learn_flappy_bird_more_in/,NasenSpray,1443960455,,47,86
68,2015-10-4,2015,10,4,22,3ng5xi,What is SGLD in practice?,https://www.reddit.com/r/MachineLearning/comments/3ng5xi/what_is_sgld_in_practice/,erogol,1443964100,"Given the well foundation as the back of Stochastic Gradient Langevin Dynamics, what is it in practice? Is it only a gaussian noise scaled by the learning rate and applied to gradient updates or more than that ?

For who does not know SGLD, here is the linked paper; http://www.icml-2011.org/papers/398_icmlpaper.pdf ",4,5
69,2015-10-4,2015,10,4,23,3ngbf7,How to give valid citations to authors?,https://www.reddit.com/r/MachineLearning/comments/3ngbf7/how_to_give_valid_citations_to_authors/,laura1222,1443967676,"Hey,

My colleague and I just completed a work related to some work we did in Deep Learning. We want to publish the results and maybe submit it to a journal. We don't however understand the way you are supposed to mention the author's names in the text. Some times two authors are mentioned, whereas sometimes ""et al."" is used. For example, ""Bengio et al"" for the 1994 paper on exploding gradients, but ""Hochreiter and Schmidhuber"" for the 1997 LSTM paper. I initially thought it was two vs many criterion, but there were exceptions to it as well. Is there a honor code or Etiquette that is followed?

Thanks a Lot! :)
",8,5
70,2015-10-5,2015,10,5,1,3ngpkz,Curated list of Torch tutorials and projects,https://www.reddit.com/r/MachineLearning/comments/3ngpkz/curated_list_of_torch_tutorials_and_projects/,carpedm20,1443975028,,0,14
71,2015-10-5,2015,10,5,1,3ngsam,Translate word vectors from Dutch to English,https://www.reddit.com/r/MachineLearning/comments/3ngsam/translate_word_vectors_from_dutch_to_english/,ChiefChinawhite,1443976307,"Hello reddit,

I'm trying to find a function from word vectors from one language (dutch) to
word vectors to another language (english).
The vectors are generated using word2vec and a have a fixed size of 300.

Translation is done by activating the network with the associated dutch word vector
and the finding the closest word vector in english.

The only thing I thought which could learn that is a neural network. After some
trial and error, the best model seemed to be a nn without hidden layers. The
activation function is: `f(x) = tanh(W . x + b)`, with `W:[300x300]` and
`b:[300]`.

The error is defined as `sum((prediction - expected)**2)`.

While training the error rapidly drops from around 1300 to 500, but after a
few hundred iterations it doesn't change anymore.

Training algorithm is minibatch SGD with L1 and L2 regularization and dropout
(tried p=0.1,0.3,0.5). Without dropout the error would rapidly converge to small
values on the training set, but raise to values of about 5000 on the validation
set. I only have about 4000 samples to train on.

All inputs are normalized to mu=0, sigma=1.

Unable to get better results, I tested the network on a few words, which showed
some interesting results, the most reasonable showed below. Numbers shown are
the error values.

Now, I'd like to know; How can I make this model perform better?

Are neural networks the right solution here and is the network used not way too simple?
Are there some algorithms that would make it perform better?
Or is the problem more fundamental and does such a mapping not exists?

    In [15]: translate('schroevendraaier') # screwdriver
    ...
    dremel_tool 58.5039376996
    toggle_bolts 57.8535931049

    In [17]: translate('hond') # dog
    ...
    Pit_bull_terrier 88.9857747881
    Hot_diggity 85.2213764115

    In [19]: translate('verf') # paint
    ...
    blush_mascara 118.088133994
    reduce_puffiness 116.670874871
    orangey_pink 114.640416513
    thinset_mortar 101.743799984
    varnish_remover 97.1864729807
    Latex_paints 94.4730057758
    acrylic_varnish 89.0819768

    In [24]: translate('toetsenbord') # keyboard
    ...
    monochrome_OLED 113.454832584
    stylus_silo 111.018164689

    In [28]: translate('licht') # light
    ...
    backlit_gauges 84.3780609568
    purple_camouflage_pants 84.1761951761
    undereye_circles 75.6201924618
    OLEDs_emit 72.9782353479

    In [31]: translate('glas') # glass
    ...
    lightly_toasted_bread 118.3863144
    glass_candleholders 108.6125367
    vitreous_enamel 98.9492048961
    spandrel_panels 95.9949890762
    Glazed_ceramic 95.9125024686
    wood_veneered 92.5651651561
    fiber_fusion_splicing 87.3851124549

    In [33]: translate('deur') # door
    ...
    dentil_moldings 94.7346672929
    Install_deadbolt_locks 89.5135770543
    Dresser_drawers 86.3607363704
",5,3
72,2015-10-5,2015,10,5,2,3ngxxm,"Hey /r/MachineLearning, I made a 2 part tutorial on how to implement a recurrent net in Python. Let me know what you think.",https://www.reddit.com/r/MachineLearning/comments/3ngxxm/hey_rmachinelearning_i_made_a_2_part_tutorial_on/,Xochipilli,1443978817,,9,145
73,2015-10-5,2015,10,5,3,3nh4mn,Two Minute Papers - Creating Photographs Using Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3nh4mn/two_minute_papers_creating_photographs_using_deep/,Buck-Nasty,1443981817,,6,36
74,2015-10-5,2015,10,5,3,3nh4rm,"Would you consider Web Development a good gateway to a career in data science? How does data science relate machine learning? When is it worth getting a ""masters in data science""?",https://www.reddit.com/r/MachineLearning/comments/3nh4rm/would_you_consider_web_development_a_good_gateway/,thisredaccount,1443981879,,5,1
75,2015-10-5,2015,10,5,3,3nh605,Sentient Technologies Antione Blondeau CEO,https://www.reddit.com/r/MachineLearning/comments/3nh605/sentient_technologies_antione_blondeau_ceo/,certaintyisdangerous,1443982414,,0,0
76,2015-10-5,2015,10,5,3,3nh6cl,Is having projects is the best way to land a job in machine learning/data science?,https://www.reddit.com/r/MachineLearning/comments/3nh6cl/is_having_projects_is_the_best_way_to_land_a_job/,thisredaccount,1443982555,,9,6
77,2015-10-5,2015,10,5,7,3ni3uy,Help with Naive Bayes Classifiers with nominal data,https://www.reddit.com/r/MachineLearning/comments/3ni3uy/help_with_naive_bayes_classifiers_with_nominal/,theninjahamster7,1443997207,"I am currently working on a project that would involve making predictions about a user's location (could be as simple as room1 etc..) based on some independent variables such as day of the week and time of day. After researching for a while, I cant seem to find a solid tutorial that handles nominal predictions. I have been playing around with using neural networks, but I am really not sure how confident I can be in my predictions since its really meant for continuous data as opposed to discrete ones. Would predictions like this even be possible? or am I running in the wrong direction? Thanks!",5,6
78,2015-10-5,2015,10,5,7,3ni8qr,Newbie to GPUs and cuDNN needs help.,https://www.reddit.com/r/MachineLearning/comments/3ni8qr/newbie_to_gpus_and_cudnn_needs_help/,y05f,1443999519,"Hi,
I am a PhD student i work on Evolutionary ANNs.

I want to start using GPUs, my budget can reach 150$ Max.

* I found in my town a new GTX 750 and GTX 650 Ti.  Which one is better? 

* Are they supported by cuDNN? 

* Can i get better GPU?

Thanks ",2,0
79,2015-10-5,2015,10,5,9,3nils8,Question about stochastic weight update using RProp,https://www.reddit.com/r/MachineLearning/comments/3nils8/question_about_stochastic_weight_update_using/,hixidom,1444005679,"So far I have found lots of good explanations of RProp in the literature, but one thing has eluded me: For stochastic RProp (i.e. using RProp to update only a handful of weights for each frame in the training process), should the new gradient of a weight, assuming it has been selected to be updated, be compared against the gradient in the previous training frame or against the gradient calculated the last time this particular weight happened to be updated?  In the former case, I have to update gradients for all weights at every step in the training process, whereas for the latter case I only update weight gradients when the weights themselves are being updated.

Some papers (e.g. the DQN Atari Nature paper) claim that they use stochastic RProp because it causes the update process to remain constant as a function of the number of weights. This implies that all weight gradients are not calculated every frame since, in that case, computation would scale with the number of weights. However, I haven't found an explicit description of the correct procedure (at least not one that I could understand).

Thanks for any insight you can provide.",4,2
80,2015-10-5,2015,10,5,10,3nip94,In what areas have there been recent breakthroughs?,https://www.reddit.com/r/MachineLearning/comments/3nip94/in_what_areas_have_there_been_recent_breakthroughs/,sinewat,1444007336,"As someone who's just recently started learning about ML, I've heard and read about some of the great recent successes that neural nets have had in many areas (eg. convnets in image processing), but I don't know much about areas outside of these more 'popular' algorithms, and I certainly don't know enough to understand whether a result is important or not.

What are some areas that you guys have seen recent breakthroughs in/believe will have breakthroughs in the future? References/links to the literature would definitely be appreciated. Thanks!",4,13
81,2015-10-5,2015,10,5,11,3nizuy,Dependency Engine for Multi-Device Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3nizuy/dependency_engine_for_multidevice_deep_learning/,antinucleon,1444012461,,1,6
82,2015-10-5,2015,10,5,13,3njcur,Intel Stacks its Chips in the Deep Learning Game,https://www.reddit.com/r/MachineLearning/comments/3njcur/intel_stacks_its_chips_in_the_deep_learning_game/,[deleted],1444019347,[deleted],1,2
83,2015-10-5,2015,10,5,14,3njjaw,Do you know any interesting ways to visualize complex data?,https://www.reddit.com/r/MachineLearning/comments/3njjaw/do_you_know_any_interesting_ways_to_visualize/,quant88,1444023372,"I'm a big supporter of data visualization and I've been pondering the task of visualizing complex data. Do you know of any visualization techniques out there that may not be so common or you think are neat/intriguing? 

How can you go about visualizing data past the third dimension other than reducing dimensionality through processes such as PCA? 
",10,8
84,2015-10-5,2015,10,5,15,3njlfx,Neural Artistic Captions,https://www.reddit.com/r/MachineLearning/comments/3njlfx/neural_artistic_captions/,iori42,1444024844,,4,5
85,2015-10-5,2015,10,5,15,3njlrp,Predicting power consumptions with Hierarchical Temporal Memory,https://www.reddit.com/r/MachineLearning/comments/3njlrp/predicting_power_consumptions_with_hierarchical/,mrcslws,1444025059,,2,5
86,2015-10-5,2015,10,5,15,3njlu6,Tutorial in slides on tensor decompositions from MLLS Kyoto with Ipython notebook,https://www.reddit.com/r/MachineLearning/comments/3njlu6/tutorial_in_slides_on_tensor_decompositions_from/,cast42,1444025107,,1,4
87,2015-10-5,2015,10,5,16,3njqd6,Unsupervised feature learning and deep learning tutorial,https://www.reddit.com/r/MachineLearning/comments/3njqd6/unsupervised_feature_learning_and_deep_learning/,cast42,1444028445,,0,2
88,2015-10-5,2015,10,5,16,3njty2,Questions about learning and is it worth the time.,https://www.reddit.com/r/MachineLearning/comments/3njty2/questions_about_learning_and_is_it_worth_the_time/,gamerfreakish,1444031202,"I've been thinking of learning Machine Learning, but I've stumbled across a few posts here that say people wouldn't even consider you if you don't have any Ph.D. or masters.

My background is in digital advertising, I produced creative apps, web apps, games, etc. but have been thinking of learning machine learning, not overly complicated just something practical such as determining user's consumption pattern or maybe something that I can use on a game, etc. 

Is it worth the time or even possible? if yes where should I start?",5,3
89,2015-10-5,2015,10,5,21,3nkgzq,We want to discuss scientific research methods with r/MachineLearning. Our new sub r/scientificresearch is for you to discuss how to best obtain new knowledge in your field. This is the link to the site and mods have cleared us for posting it. We hope you'll give it a shot,https://www.reddit.com/r/MachineLearning/comments/3nkgzq/we_want_to_discuss_scientific_research_methods/,scientific_research,1444048578,,2,3
90,2015-10-5,2015,10,5,21,3nkia7,How far are we from making Deep Reinforcement Learning to play Quake successfully (or other 3D games)?,https://www.reddit.com/r/MachineLearning/comments/3nkia7/how_far_are_we_from_making_deep_reinforcement/,cesarsalgado,1444049273,,31,48
91,2015-10-5,2015,10,5,23,3nkvig,Understanding a kernel,https://www.reddit.com/r/MachineLearning/comments/3nkvig/understanding_a_kernel/,adwarakanath,1444055901,"Hi guys

Sensory physiologist here working with high dimensional datasets. I'm using something called the gaussian process factor analysis (Yu and Cunningham J Neurophys 2009, Ecker et al Neuron 2014) to infer latent network states. I have a basic maths background. I understand the model conceptually and I can use it and I get interesting results.

But I still don't get what a Kernel really is. Is it a rule which is used to infer the boundary that separates data points and classifies them? If yes, how does one understand it intuitively? In the GPFA model, is the gaussian process a kernel?

I'd be grateful for an intuitive explanation. Wikipedia is too technical and the lecture slides and notes that I acquired from the grad school here are also a bit beyond me. 

Thanks.",5,1
92,2015-10-5,2015,10,5,23,3nkxcm,Is there a place where you can check if some research ideas are not already in the literature?,https://www.reddit.com/r/MachineLearning/comments/3nkxcm/is_there_a_place_where_you_can_check_if_some/,[deleted],1444056769,[deleted],0,1
93,2015-10-5,2015,10,5,23,3nkxvs,Is there a place (webpage/subreddit) where you could check if some research idea is not already in the literature?,https://www.reddit.com/r/MachineLearning/comments/3nkxvs/is_there_a_place_webpagesubreddit_where_you_could/,ecobost,1444057022,Mostly to avoid reworking. I was thinking on a place where you could announce what you are planning to do and maybe someone will tell you if it's already done or why it hasn't.,6,3
94,2015-10-6,2015,10,6,3,3nlt1t,Deep learning robot using CNNs learns to grasp objects after trying 50000 times,https://www.reddit.com/r/MachineLearning/comments/3nlt1t/deep_learning_robot_using_cnns_learns_to_grasp/,dllu,1444070027,,6,45
95,2015-10-6,2015,10,6,3,3nlu05,Big Data: The Future of Marketing is Here,https://www.reddit.com/r/MachineLearning/comments/3nlu05/big_data_the_future_of_marketing_is_here/,shugert,1444070405,,0,0
96,2015-10-6,2015,10,6,4,3nm4tw,"Forecasting continuous variables, multiple periods into the future? (E.g. Sales)",https://www.reddit.com/r/MachineLearning/comments/3nm4tw/forecasting_continuous_variables_multiple_periods/,Qwerty10110,1444074717,"Where would you start for forecasting continuous variables multiple steps into the future from somewhat noisy/colinear data? Examples would be economic indicators, firm sales, sunspots, etc. My first approach would be to use LSTMs with Adaprop training and random Gaussian hyperparameter optimization. I'm not familiar with any frameworks, so I could use Torch, Theano, DeepLearning4J, or whatever you think would be best. ",8,0
97,2015-10-6,2015,10,6,5,3nm6cd,RMS - why used on test set?,https://www.reddit.com/r/MachineLearning/comments/3nm6cd/rms_why_used_on_test_set/,[deleted],1444075314,[deleted],1,0
98,2015-10-6,2015,10,6,5,3nm6mp,Pandas tips and tricks,https://www.reddit.com/r/MachineLearning/comments/3nm6mp/pandas_tips_and_tricks/,efavdb,1444075430,,0,9
99,2015-10-6,2015,10,6,6,3nmnkq,What are the real reasons the ensemble methods work in machine learning?,https://www.reddit.com/r/MachineLearning/comments/3nmnkq/what_are_the_real_reasons_the_ensemble_methods/,savs95,1444082254,What is the intuitive explanation and theoretical reason as to why weak learners combined together produces a nice result.  ,10,9
100,2015-10-6,2015,10,6,7,3nmuip,The Bayes Classifier: building a tweet sentiment analysis tool,https://www.reddit.com/r/MachineLearning/comments/3nmuip/the_bayes_classifier_building_a_tweet_sentiment/,asdrubaldone,1444085299,,0,0
101,2015-10-6,2015,10,6,8,3nn33n,"Have a question that may be able to be solved via Machine Learning type algorithm, but have little experience with that type of problem. Any pointers?",https://www.reddit.com/r/MachineLearning/comments/3nn33n/have_a_question_that_may_be_able_to_be_solved_via/,[deleted],1444089155,[deleted],4,1
102,2015-10-6,2015,10,6,9,3nn8wc,Logistic regression and its coefficients,https://www.reddit.com/r/MachineLearning/comments/3nn8wc/logistic_regression_and_its_coefficients/,coldaspluto,1444091784,"I have a trained Logistic Regression model for a 0/1 binary classification problem. The coefficients (logs of odds ratios) have values usually between -1 and +1 (though not always). The features are mostly ordinal (binary). My question is: can I use these coefficients to make some claims about the importance of features? My gut says that coefficients with weights &lt; 0 reduce the probability of the class ""1"", so they are anti-correlated with the outcome? And coefficients which are close to 0 aren't adding much value to the classification. ",13,5
103,2015-10-6,2015,10,6,9,3nnauj,"For dimensionality reduction: When is linear PCA good enough, and when are more sophisticated non-linear approaches (eg. stacked autoencoders) likely to make a big difference? Hoping for some intuition here.",https://www.reddit.com/r/MachineLearning/comments/3nnauj/for_dimensionality_reduction_when_is_linear_pca/,sanity,1444092682,,20,53
104,2015-10-6,2015,10,6,10,3nndo2,Question about Jacobians and vanishing gradient in RNNs,https://www.reddit.com/r/MachineLearning/comments/3nndo2/question_about_jacobians_and_vanishing_gradient/,azg-1,1444093959,"My math is a bit rusty and I've been reading ""[On the difficulty of training RNNs](http://www.jmlr.org/proceedings/papers/v28/pascanu13.pdf)"". I'm stuck on equation (5). Nando de Freitas uses the same equations - albeit with different notation - to describe the vanishing gradient problem in his [lecture](https://youtu.be/56TYLaQN4N8?t=19m8s).

I'm not sure why the weight matrix is transposed? 

-----------

Following Nando's notation (replace x with h in the paper), dh\_i/dh\_{i-1} is clearly a Jacobian matrix since we're taking the derivative of one vector with respect to another one. 

Let h be an m-dimensional column vector as defined in equation (2). Ignore the bias and the weighted input since they are constant during our differentiation and thus unimportant. We are thus left with h\_i = W(h\_{i-1)}

We're therefore looking for dW(h\_{i-1)}/dh\_{i-1}. Now W is a constant, it isn't a function of h\_{i-1} so by definition we can take W out:

dW(h\_{i-1)}/dh\_{i-1} = W(d(h\_{i-1)}/dh\_{i-1})

Where does W' come from then? :( ",3,3
105,2015-10-6,2015,10,6,11,3nnjv8,Question on Sklearn's cross_val_score method,https://www.reddit.com/r/MachineLearning/comments/3nnjv8/question_on_sklearns_cross_val_score_method/,I_Know_You_Now,1444096885,"Hey Everyone,

I am working on a project for school with machine learning, and I am using sklearn.  I was looking and experimenting with the cross_val_score method (http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_score.html#sklearn.cross_validation.cross_val_score) and I was running into some issues.  

1) If I just run it with a brand new model (e.g. LinearRegression()) it will score it, but if I try to use the model after, it says that it has not been fit.  How does it score the model if it is not fit?

2) I then made it so it fits the model, then scores it, and I found that the cross_val_score was consistently lower than just scoring the model using its own method. Whose side is the error on?

Maybe I'm just not understanding the proper pipeline or purpose of using this function.  Any advice on when and how to use would be very appreciated.  Thanks! ",4,2
106,2015-10-6,2015,10,6,12,3nnt4g,Techniques for Learning from Large amounts of Data,https://www.reddit.com/r/MachineLearning/comments/3nnt4g/techniques_for_learning_from_large_amounts_of_data/,srinath_perera,1444101293,,1,1
107,2015-10-6,2015,10,6,12,3nnvdq,Feature extraction tool needed...,https://www.reddit.com/r/MachineLearning/comments/3nnvdq/feature_extraction_tool_needed/,[deleted],1444102410,[deleted],0,0
108,2015-10-6,2015,10,6,13,3no07p,V sao dng my nn kh piston li c ngnh sa cha xe hi tin dng?,https://www.reddit.com/r/MachineLearning/comments/3no07p/v_sao_dng_my_nn_kh_piston_li_c_ngnh_sa/,vetinhkingbin01,1444104985,,0,0
109,2015-10-6,2015,10,6,14,3no7uu,What to do with small data?,https://www.reddit.com/r/MachineLearning/comments/3no7uu/what_to_do_with_small_data/,D33B,1444109607,,8,21
110,2015-10-6,2015,10,6,15,3nocg5,Fast Algorithms for Convolutional Neural Networks - VGG - 2.6X as fast as Caffe,https://www.reddit.com/r/MachineLearning/comments/3nocg5/fast_algorithms_for_convolutional_neural_networks/,r-sync,1444112572,,49,5
111,2015-10-6,2015,10,6,16,3nojiu,Learn what cleaners are appropriate for metals.,https://www.reddit.com/r/MachineLearning/comments/3nojiu/learn_what_cleaners_are_appropriate_for_metals/,easternoil48341,1444117959,,0,1
112,2015-10-6,2015,10,6,18,3nor9z,Deeplearning Training datasets - best practices,https://www.reddit.com/r/MachineLearning/comments/3nor9z/deeplearning_training_datasets_best_practices/,mikos,1444124238,"Hello everybody, I am a deep-learning newbie and have a 3-part question:

I. Are there any best practices in organizing datasets for training a CNN? i.e. number of classes and number of samples for each class? For example if I was training on Vehicles, would I be better off either:

(a) Vehicles - Car-Sedans/Car-Hatchback/Car-SUV/Truck-18-wheeler/.... (note this could mean several thousand classes), or 

(b) have a higher level model that classifies between car/truck/2-wheeler and so on... and if car type then query the Car Model to get the car type (sedan/hatchback etc)



II. How many training images per class is a typical best practice? I know there are several other variables that affect the accuracy of the CNN, but what rough number is good to shoot for in each class? Should it be a function of the number of classes in the model? For example, if I have many classes in my model, should I provide more samples per class?


III.  How do we ensure we are not overfitting to class? Is there way to measure heterogeneity in training samples for a class?


Thanks in advance.
  ",1,3
113,2015-10-6,2015,10,6,19,3nouhs,Machine Learning for Programming ?,https://www.reddit.com/r/MachineLearning/comments/3nouhs/machine_learning_for_programming/,ganarajpr,1444126607,"Are there any good examples or good use cases where Machine Learning is used to assist programmers ? Would love to know any such examples in the wild out there which help in the process of programming itself. 

Would you guys think that a tool using Machine Learning would be able to bridge the gap between others and programmers - so that the others can start working on / programming tasks ? Whats your take ?  ",6,1
114,2015-10-6,2015,10,6,19,3nowjw,Google Prediction API: a Machine Learning black box for developers,https://www.reddit.com/r/MachineLearning/comments/3nowjw/google_prediction_api_a_machine_learning_black/,asdrubaldone,1444128155,,17,102
115,2015-10-6,2015,10,6,21,3np8bq,How maxout units are useful with regard to information encoding ?,https://www.reddit.com/r/MachineLearning/comments/3np8bq/how_maxout_units_are_useful_with_regard_to/,erogol,1444135372,"Recently, I devoted some time to understand the usefulness of maxout units which its effectiveness is blurry to me in a practical sense. 

The problem that exemplified in my head is, if a maxout unit outputs 0.9 as its activation, it has two possible source for it which is hidden from the following layer. That is, the next layer  is unable to uncover whether 0.9 is coming from the first linear piece or the second (given a 2 pieces maxout units). 

For making the problem more concrete, suppose you have two different filters capturing corner and horizontal edge. Assume these are the competing functions of a maxout unit. Given two different images with a horizontal edge and a corner respectively, final maxout unit activation can be same where as, in essence, one of these filters is activated for corresponding shape.  

Given this intuition, how can we thrust the information flow out of maxout units?",15,3
116,2015-10-6,2015,10,6,22,3npg0d,How to keep track of experiments ?,https://www.reddit.com/r/MachineLearning/comments/3npg0d/how_to_keep_track_of_experiments/,FilippoC,1444139288,"Hello,

I'm a PhD student in structured prediction. As of my day to day work, I made a lot of different experiments on multiple datasets, with different version of algorithms and parameters.

Does anyone have some advice in order to not lost myself in experiments ?
(note that I'm not only interested in keeping track of the best scores, a lot of other measure are very important for me too as speed, model size, ...)

thanks !

PS: I don't know if it is important, but I don't use an external library for my machine learning algorithm : everything as been written almost from scratch by myself in Python (with some Cython and C++ extensions).",22,12
117,2015-10-6,2015,10,6,23,3npl3k,Can we discuss Convolutional Autoencoders?,https://www.reddit.com/r/MachineLearning/comments/3npl3k/can_we_discuss_convolutional_autoencoders/,LyExpo,1444141612,"The idea here seems fairly natural: extend the autoencoder so that it works with convolution and can handle image tasks. [Here](http://people.idsia.ch/~ciresan/data/icann2011.pdf) is a paper that I'm looking at. There was also a short post about it [here](https://www.reddit.com/r/MachineLearning/comments/353foi/do_convolutional_autoencoders_exist/).

I think I understand the general idea, but I'm a bit confused about how reconstruction occurs. The encoding works as so:
    
    input -&gt; convolution -&gt; max-pooling -&gt; hidden autoencoder layer

Then, to reconstruct, we want to ""undo"" these layers:

    hidden autoencoder layer -&gt; undo max-pooling -&gt; new convolution -&gt; output

The first thing that throws me off is how to undo the max-pooling layer. The linked paper says nothing about this step. Maybe they don't even do it? I think it might even be reasonable to go directly from the hidden autoencoder layer to the output?

Any thoughts or suggestions on this?",14,18
118,2015-10-7,2015,10,7,0,3npu9m,Multi Layer Perceptron - XOR example,https://www.reddit.com/r/MachineLearning/comments/3npu9m/multi_layer_perceptron_xor_example/,marcossponton,1444145500,,4,6
119,2015-10-7,2015,10,7,0,3npuux,Comparing Speech to Text Software,https://www.reddit.com/r/MachineLearning/comments/3npuux/comparing_speech_to_text_software/,tehgargoth,1444145736,I want to do a side-by-side comparison of the commercial speech to text products but all the papers I've read on speech recognition compare algorithms at the phone/acoustic detection level.  The only method I could come up with using the transcribed utterances was using the time offset data that comes back from most of these APIs and trying to compare word by word for set blocks of time but this method has many foreseeable problems.  Is anyone aware of a  method for doing something like this?,7,2
120,2015-10-7,2015,10,7,0,3npy5r,R: Linear regression analysis,https://www.reddit.com/r/MachineLearning/comments/3npy5r/r_linear_regression_analysis/,padmajatamada,1444147073,,0,0
121,2015-10-7,2015,10,7,1,3nq19e,Phd in deep learning @ IDSIA Switzerland(Schmidhuber's group),https://www.reddit.com/r/MachineLearning/comments/3nq19e/phd_in_deep_learning_idsia/,regularized,1444148271,I have been constantly checking the website http://idsia.ch/idsia_en/institute/work-with-us.html for phd positions at IDSIA but it seems that they do not recruit any phd students. Do you have any idea about the application process?,2,2
122,2015-10-7,2015,10,7,1,3nq2d9,Intro to ApacheSpark + Integration with ScienceCluster,https://www.reddit.com/r/MachineLearning/comments/3nq2d9/intro_to_apachespark_integration_with/,elisebreda,1444148714,,0,0
123,2015-10-7,2015,10,7,1,3nq34b,How do you start with the implementation of a paper/idea?,https://www.reddit.com/r/MachineLearning/comments/3nq34b/how_do_you_start_with_the_implementation_of_a/,[deleted],1444149006,[deleted],1,0
124,2015-10-7,2015,10,7,1,3nq5dv,"How ""Recommendation Systems"" are changing the ways that we make everyday decisions",https://www.reddit.com/r/MachineLearning/comments/3nq5dv/how_recommendation_systems_are_changing_the_ways/,persianhunter,1444149884,,0,1
125,2015-10-7,2015,10,7,1,3nq69w,Speed up classification task on sklearn/Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/3nq69w/speed_up_classification_task_on_sklearnmachine/,Chuckytah,1444150255,"Hello,	

I already have a classifier trained that I load up through pickle. 
My classifier is an LinearSVC from sklearn.svm . To train the classifier took 16h and I saved it to a Pickle. Now I am just need it to the classifier.classify() task . I am new to ML, I just wanted to know if 1 sec to predict a category of a text is normal or if it could be reduced. Is there is anything that can speed up the classification task. It is taking almost 1 minute for each text (feature extraction - less than 1 sec and classification about 40 secs), is that normal? Should I go on multi-threading?

You can see some parts of my code here: http://stackoverflow.com/questions/32972138/speed-up-classification-task-on-sklearn-machine-learning-with-pickle?noredirect=1#comment53774814_32972138

Thanks for the help!",7,0
126,2015-10-7,2015,10,7,3,3nqp84,Why you should use open data to hone your machine learning models,https://www.reddit.com/r/MachineLearning/comments/3nqp84/why_you_should_use_open_data_to_hone_your_machine/,wildcodegowrong,1444157723,,1,0
127,2015-10-7,2015,10,7,5,3nr0ya,End-to-end trainable ocr?,https://www.reddit.com/r/MachineLearning/comments/3nr0ya/endtoend_trainable_ocr/,[deleted],1444162244,[deleted],2,3
128,2015-10-7,2015,10,7,5,3nr6es,Rapid Development &amp; Performance in Spark For Data Scientists,https://www.reddit.com/r/MachineLearning/comments/3nr6es/rapid_development_performance_in_spark_for_data/,supermanava,1444164397,,1,0
129,2015-10-7,2015,10,7,7,3nrj9u,Introduction to deep learning in german,https://www.reddit.com/r/MachineLearning/comments/3nrj9u/introduction_to_deep_learning_in_german/,fimari,1444169687,,0,2
130,2015-10-7,2015,10,7,11,3nsd6y,3Blades Rises In a Sea of Cloud Services Options,https://www.reddit.com/r/MachineLearning/comments/3nsd6y/3blades_rises_in_a_sea_of_cloud_services_options/,shugert,1444183270,,0,0
131,2015-10-7,2015,10,7,15,3nt629,Austin Rochford - Bayesian Survival Analysis in Python with pymc3,https://www.reddit.com/r/MachineLearning/comments/3nt629/austin_rochford_bayesian_survival_analysis_in/,cast42,1444199067,,0,20
132,2015-10-7,2015,10,7,15,3nt6gq,[1510.01378] Batch Normalized Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3nt6gq/151001378_batch_normalized_recurrent_neural/,iori42,1444199342,,9,31
133,2015-10-7,2015,10,7,15,3nt6k6,Logistic Regression  Geometric Intuition,https://www.reddit.com/r/MachineLearning/comments/3nt6k6/logistic_regression_geometric_intuition/,cast42,1444199402,,0,2
134,2015-10-7,2015,10,7,18,3ntjgq,Logistic Regression with R,https://www.reddit.com/r/MachineLearning/comments/3ntjgq/logistic_regression_with_r/,padmajatamada,1444209475,,0,0
135,2015-10-7,2015,10,7,19,3ntr4m,Fiber Laser Marking Machines: One of the Best Laser Marking techniques,https://www.reddit.com/r/MachineLearning/comments/3ntr4m/fiber_laser_marking_machines_one_of_the_best/,HangRex,1444215295,,0,1
136,2015-10-7,2015,10,7,21,3ntzb3,Structured Transforms for Small-Footprint Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3ntzb3/structured_transforms_for_smallfootprint_deep/,muktabh,1444220596,,0,6
137,2015-10-7,2015,10,7,23,3nubfk,QANTA vs. Ken Jennings at UW,https://www.reddit.com/r/MachineLearning/comments/3nubfk/qanta_vs_ken_jennings_at_uw/,vkhuc,1444226564,,6,26
138,2015-10-7,2015,10,7,23,3nuh7g,Is it effective to use one hot encoding of categorical data as input to PCA for anomaly detection (where there is a mix of numerical and categorical inputs)?,https://www.reddit.com/r/MachineLearning/comments/3nuh7g/is_it_effective_to_use_one_hot_encoding_of/,sanity,1444229198,,5,7
139,2015-10-8,2015,10,8,0,3nuj25,Machine Learning : Drawing an Artificial Intelligence line in the sand ...,https://www.reddit.com/r/MachineLearning/comments/3nuj25/machine_learning_drawing_an_artificial/,Nitc,1444230006,,3,0
140,2015-10-8,2015,10,8,0,3num54,[Question] Make predictions from a saved trained classifier in Scikit Learn,https://www.reddit.com/r/MachineLearning/comments/3num54/question_make_predictions_from_a_saved_trained/,[deleted],1444231278,[deleted],3,0
141,2015-10-8,2015,10,8,0,3nunwc,Tryout: 104 tonnes(!) wood-chip truck,https://www.reddit.com/r/MachineLearning/comments/3nunwc/tryout_104_tonnes_woodchip_truck/,Teknikfreak,1444232002,,0,0
142,2015-10-8,2015,10,8,0,3nuq5w,"VQA challenge announced, full dataset released. One of the most challenging datasets in ML history.",https://www.reddit.com/r/MachineLearning/comments/3nuq5w/vqa_challenge_announced_full_dataset_released_one/,downtownslim,1444232932,,12,19
143,2015-10-8,2015,10,8,2,3nv50b,A curated list of speech and natural language processing resources,https://www.reddit.com/r/MachineLearning/comments/3nv50b/a_curated_list_of_speech_and_natural_language/,joshdotai,1444239169,,0,17
144,2015-10-8,2015,10,8,3,3nvcz3,AWS Machine Learning Course (2h 11m),https://www.reddit.com/r/MachineLearning/comments/3nvcz3/aws_machine_learning_course_2h_11m/,[deleted],1444242420,[deleted],0,3
145,2015-10-8,2015,10,8,3,3nvgrk,PCO-PLS,https://www.reddit.com/r/MachineLearning/comments/3nvgrk/pcopls/,FireOnSomething,1444243950,"Hi everybody!
I'm researching about methods for variable selection in PLS regression. The project is to determine the optimal condition (values of process variables that maximize the quality variables ) of a continuous process in industry. After that, the point is to select the process variables that most describe the y functions.

I was searching for methods, and i've found a PCO-PLS model. But all I saw was implementations with a single Y (as in QSAR problems).

Does anyone knows a PCO-PLS algorithm for variable selection with multiple Ys?

Thanks in advance",4,2
146,2015-10-8,2015,10,8,5,3nvyoa,"Kaggle competition for ""Are you smarter than an 8th grader?""",https://www.reddit.com/r/MachineLearning/comments/3nvyoa/kaggle_competition_for_are_you_smarter_than_an/,r-sync,1444251136,,0,23
147,2015-10-8,2015,10,8,8,3nwmaa,"Deep Learning Review - Yann LeCun, Yoshua Bengio &amp; Geoffrey Hinton.",https://www.reddit.com/r/MachineLearning/comments/3nwmaa/deep_learning_review_yann_lecun_yoshua_bengio/,[deleted],1444261304,[deleted],3,4
148,2015-10-8,2015,10,8,9,3nwpgf,IBM's Ginni Rometty: Prepare for the Cognitive Computing Era,https://www.reddit.com/r/MachineLearning/comments/3nwpgf/ibms_ginni_rometty_prepare_for_the_cognitive/,DamonVryce,1444262740,,0,1
149,2015-10-8,2015,10,8,9,3nwukm,Can You Use L1/L2 Regularization on RNN or LSTMs?,https://www.reddit.com/r/MachineLearning/comments/3nwukm/can_you_use_l1l2_regularization_on_rnn_or_lstms/,LeavesBreathe,1444265160,"Hey Everyone, somewhat of a beginner here.

I know that L1 and L2 Regularization is used to prevent overfitting, and they are commonly used on convolutional nets. 

My question is: Can they also be applied to Recurrent Neural Networks, or more specifically, Long Short Term Memory Networks (or even Gradient Recurrent Networks)?

I know that Dropout can reduce overfitting for LSTMs. But I'm wondering if its possible to apply L1 or L2 to lstms. I regularly use Keras and they have an option to apply L1 and L2 to conv nets, but not to LSTMs. Thanks!

The idea would be to use both dropout AND L2 Regularization to prevent overfitting for LSTMs",7,8
150,2015-10-8,2015,10,8,10,3nwyqo,Mindori: On-demand GPUs for Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3nwyqo/mindori_ondemand_gpus_for_neural_networks/,momentum_on_mars,1444267150,,11,35
151,2015-10-8,2015,10,8,12,3nxcl6,CNC Machine Singapore,https://www.reddit.com/r/MachineLearning/comments/3nxcl6/cnc_machine_singapore/,aliesha980,1444273879,,0,0
152,2015-10-8,2015,10,8,13,3nxknn,Code released: Unsupervised Learning on Neural Network Outputs,https://www.reddit.com/r/MachineLearning/comments/3nxknn/code_released_unsupervised_learning_on_neural/,yaolubrain,1444278426,,2,27
153,2015-10-8,2015,10,8,13,3nxla0,Tools to extract tabular data from images?,https://www.reddit.com/r/MachineLearning/comments/3nxla0/tools_to_extract_tabular_data_from_images/,killugon,1444278817,"Are there any available (preferably open-source) tools out there to programatically extract tabular data from images (say, a scanned receipt)? I've searched google but haven't found anything like that..",1,0
154,2015-10-8,2015,10,8,16,3ny2qa,What No One Tells You About Real-Time Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3ny2qa/what_no_one_tells_you_about_realtime_machine/,dmpetrov,1444290435,,11,2
155,2015-10-8,2015,10,8,18,3nybdk,Mold Making China,https://www.reddit.com/r/MachineLearning/comments/3nybdk/mold_making_china/,ernitinsingh,1444297404,[removed],0,1
156,2015-10-8,2015,10,8,21,3nynkp,"Recurrent Neural Networks Tutorial, Part 3  Backpropagation Through Time and Vanishing Gradients",https://www.reddit.com/r/MachineLearning/comments/3nynkp/recurrent_neural_networks_tutorial_part_3/,pogopuschel_,1444305782,,6,77
157,2015-10-9,2015,10,9,0,3nzc41,What are your preferences in python based Deep Learning libraries?,https://www.reddit.com/r/MachineLearning/comments/3nzc41/what_are_your_preferences_in_python_based_deep/,andrewbarto28,1444317457,Keras and Chainer seems to be the easiest to use. What are their pros and cons? Any other new libs that are worth checking out?,16,13
158,2015-10-9,2015,10,9,0,3nzdbk,Visualizing CIFAR-10 Categories with WordNet and NetworkX,https://www.reddit.com/r/MachineLearning/comments/3nzdbk/visualizing_cifar10_categories_with_wordnet_and/,__lava__,1444317967,,5,5
159,2015-10-9,2015,10,9,0,3nzhy4,What techniques do you use to do feature selection when you know you have epistasis in your data set?,https://www.reddit.com/r/MachineLearning/comments/3nzhy4/what_techniques_do_you_use_to_do_feature/,rhiever,1444319912,"i.e., you have two features that are not predictive of the class on their own, but when combined they are highly predictive.

All of the methods I've experimented with so far only look at linear correlations between the features and the class, and usually end up pruning these epistatic pairs.

I've experimented with exhaustively looking at all pairs of features and looking at the correlation between the *pair* and the class, and that works decently. But let's say I have 3-way epistasis or higher in my features, then it becomes increasingly expensive to perform this exhaustive search. Is there a smarter way?",6,1
160,2015-10-9,2015,10,9,1,3nzkrv,Inputting Raw Words Into LSTM Using Keras,https://www.reddit.com/r/MachineLearning/comments/3nzkrv/inputting_raw_words_into_lstm_using_keras/,LeavesBreathe,1444321060,"Hey Guys, beginner here. 

I absolutely love Keras, and would highly recommend it. Their examples are great.

I'm trying to figure out the best way to take raw words, and make a word dictionary (word_index) and then based upon a list of words, input it into a lstm

For example, Take the sentence ""The cow jumped over the moon"" is first converted to 

    ['the', 'cow', 'jumped', 'over', 'the', 'moon'] in a list form. 

I have made a word index for all words that I will use, where each word is assigned a unique number.

the =1, cow =10, jumped =5, etc to get:

    'the cow jumped over the moon' is translated to: ['1','10','5','7','12','28']

But this is where I get stuck. How do I feed this list of numbers into a lstm and RETAIN THEIR ORDER. 

I see that keras has the one_hot function which might be useful in this case.

The main thing is that I need to retain the order of the words. I don't want to scramble them or anything. I really appreciate any help. 

P.S. I have looked very extensively at all the keras examples, including the text-generation one. The main issue with this is that it is character based. I have tried converting it to using words, but I think their might be a more efficient way. 
",7,0
161,2015-10-9,2015,10,9,1,3nzmkg,"Hugo Larochelle @hugo_larochelle : experiments show that they can achieve speedup factors of over 500 on the CPU, and over 1500 on the GPU",https://www.reddit.com/r/MachineLearning/comments/3nzmkg/hugo_larochelle_hugo_larochelle_experiments_show/,derRoller,1444321802,,5,26
162,2015-10-9,2015,10,9,1,3nznj1,[1509.06461] Deep Reinforcement Learning with Double Q-learning,https://www.reddit.com/r/MachineLearning/comments/3nznj1/150906461_deep_reinforcement_learning_with_double/,pilooch,1444322194,,0,18
163,2015-10-9,2015,10,9,2,3nzwyf,I have about 2000 samples from which to create a classifier,https://www.reddit.com/r/MachineLearning/comments/3nzwyf/i_have_about_2000_samples_from_which_to_create_a/,wan2Kno,1444326027,"It's okay if the classifier is fairly inaccurate as long as it is much better than random chance. My concern is, what are the mathematics behind the minimum number of samples and how it relates to the likelihood my classifier will generalize well? I don't think I have enough samples to create a training set and a test set",10,3
164,2015-10-9,2015,10,9,2,3nzzpa,Machine learning vs AI,https://www.reddit.com/r/MachineLearning/comments/3nzzpa/machine_learning_vs_ai/,[deleted],1444327131,[deleted],7,3
165,2015-10-9,2015,10,9,6,3o0soy,"I am 23. I have never gone to school, and I am becoming increasingly dissatisfied by my cushy six figure software engineering job. I spend all of my time thinking about AI. I want to contribute but I don't know how. This is a cry for help.",https://www.reddit.com/r/MachineLearning/comments/3o0soy/i_am_23_i_have_never_gone_to_school_and_i_am/,tincanfurball,1444338924,"I am going to write this post ""stream of consciousness"" style, but I will include a TL;DR at the bottom.

I dropped out of high school and left the home I grew up in to go to university when I was 16. I dropped out of university and moved to Tokyo to learn Japanese when I was 19. I returned to america, taught myself how to code, and have been making 6 figures as a programmer for almost 2 years now.

Every decision I've ever made has been made with strict consideration to maximizing the return on my investment (of time) - making me as marketable as possible in the shortest amount of time. Originally I had planned to retire by my 28th birthday. I'm currently 23 and this is still very much within reach. But I've never felt any sort of satisfaction, or passion towards any one thing, except for the process of learning, itself.

Enter Artificial Intelligence - stage right.

For as long as I can remember, I've thought about intelligence. In grade school i would write poetry asking questions like ""what **is** learning?"" and have spent countless nights awake in bed pondering the mysteries of cognition and intelligence and over the last year I've been growing especially dissatisfied with the trajectory my life is taking.

I have ideas! I want to research them! Test them! I want to contribute to AI *right now*, but I don't know where to start and I'm feeling trapped with my career taking up all of my productive hours. Should I go back to school? Should I start pouring my retirement money into startup companies that focus on deep learning? What would i study? Where would I go? Who do I talk to about my ideas to determine a solid course for me? I have no trepidations with leaving my career, as I have total confidence that I can land a similar or better job with a medium amount of effort - I just need some direction.

I've never asked for help with making a major life decision before, so this is new territory - and completely terrifying - for me.


tl;dr - want to research AI. don't know where to start. halp.

edit: readability 


",20,0
166,2015-10-9,2015,10,9,6,3o0yez,Simplest way to detect and return specified sections of text with training? [Pattern Recognition],https://www.reddit.com/r/MachineLearning/comments/3o0yez/simplest_way_to_detect_and_return_specified/,eggbrain,1444341488,"**Preamble**
I've been reading up on text extraction and keyword extraction today, and I feel like I must be missing something, because I feel like I can't figure out what I'd consider a ""simple"" use case. To put it simply, I'd like to take a body of text, and train it to recognize certain pieces of information in that text (with as little code as possible). I've provided some generalized examples below so you can see what I'm thinking.

*Example 1:*

&gt; ""While the United States was fighting with Russia, it also had to deal with Spain, and that was the worst time for  the fire nation to attack, and attack it did. The United States went on to win that war, though, with the help of thousands of CocaCola bottles.""

I'd then give the algorithm the text, and tell it what data I want it to respond with (in this case, I want it to return ""United States"", since I'm trying to extract the subject of the text). I could do named entity recognition, but there's multiple named entities there, and so I'd want it to then start to look at frequency, or other factors, without possibly being told about those factors.


*Example 2:*
&gt; ""Hey, just checking in. If you could pick up 12 popsicles, that would be great. I'm located at 123 fake street, Cali""

In this case, I'd want it to start to detect addresses, so I'd supply it with the answer ""123 fake street, Cali"" (and train it on thousands of examples with supplying it answers)"".


*Example 3:*
&gt; ""Hey man,

&gt; You're right, I do love those shoes. You are always so great!

&gt; Sincerely,

&gt; Jordan""

In this case, I'd want it to detect just the body of an email, rather than the signature, so I'd supply it with the answer ""You're right, I do love those shoes. You are always so great!"". 

**Final thoughts**

The idea would be that, in each example, I feed it thousands of training data samples (and the correct answer) and it starts to determine features about that text that it can use to generate its predictions. Then, I can give it a new sample (within it's category), and it can spit out what it predicts is the match. 

I feel like I've read hundreds of articles on text extraction, keyword analysis and more, however, and I can't find a simple example with code that I can work with. Does anyone have any background on doing something like this simply?
",1,1
167,2015-10-9,2015,10,9,7,3o12ux,New Android SwiftKey keyboard uses n-gram instead of Markov models,https://www.reddit.com/r/MachineLearning/comments/3o12ux/new_android_swiftkey_keyboard_uses_ngram_instead/,[deleted],1444343381,[deleted],0,0
168,2015-10-9,2015,10,9,8,3o1d6d,What's the best machine learning approaches (frameworks) to classify EEG time-series data ?,https://www.reddit.com/r/MachineLearning/comments/3o1d6d/whats_the_best_machine_learning_approaches/,pica_foices,1444348132,I am trying to do some offline classification  (2 categories) on EEG time-series (already segmented).  What are the best approaches using MATLAB or Deep Learning frameworks out there. Thanks in advance.,2,5
169,2015-10-9,2015,10,9,13,3o2857,Time Series Features,https://www.reddit.com/r/MachineLearning/comments/3o2857/time_series_features/,ZioFascist,1444363970,"I am looking for examples of feature engineering for time series data that can be used for time series forecasting and or classification. Would anyone be willing to share..? :)

",7,1
170,2015-10-9,2015,10,9,13,3o292h,Direct heat step sealer,https://www.reddit.com/r/MachineLearning/comments/3o292h/direct_heat_step_sealer/,dongfengpacking,1444364494,,1,1
171,2015-10-9,2015,10,9,13,3o2b53,DF 770 Band sealer,https://www.reddit.com/r/MachineLearning/comments/3o2b53/df_770_band_sealer/,dongfengpacking,1444365726,,1,1
172,2015-10-9,2015,10,9,13,3o2bte,Strange Patents Claimed by Everday Inventions : InventHelp Blog,https://www.reddit.com/r/MachineLearning/comments/3o2bte/strange_patents_claimed_by_everday_inventions/,Blaideselly,1444366083,,0,1
173,2015-10-9,2015,10,9,13,3o2cmw,DF 770WL Band sealer,https://www.reddit.com/r/MachineLearning/comments/3o2cmw/df_770wl_band_sealer/,dongfengpacking,1444366626,,1,1
174,2015-10-9,2015,10,9,14,3o2hwd,Nonparametric Latent Dirichlet Allocation,https://www.reddit.com/r/MachineLearning/comments/3o2hwd/nonparametric_latent_dirichlet_allocation/,cast42,1444370118,,0,6
175,2015-10-9,2015,10,9,15,3o2jj2,zhejiang dongfeng packing machine,https://www.reddit.com/r/MachineLearning/comments/3o2jj2/zhejiang_dongfeng_packing_machine/,dongfengpacking,1444371229,,1,1
176,2015-10-9,2015,10,9,17,3o2v9e,Dp you think it can be a platform really competitive with GPU Tesla?,https://www.reddit.com/r/MachineLearning/comments/3o2v9e/dp_you_think_it_can_be_a_platform_really/,[deleted],1444380574,[deleted],0,1
177,2015-10-9,2015,10,9,17,3o2vbi,Nonconvex Optimization,https://www.reddit.com/r/MachineLearning/comments/3o2vbi/nonconvex_optimization/,chad_brochill69,1444380635,"I was recently advised to make myself familiar with the basics of convex and nonconvex optimization, for they will be fairly useful for machine learning and quantum computing. I was told that Convex Optimization by Stephen Boyd and Lieven Vandenberghe is a great book to review, but my professor could not think of a nonconvex optimization book that is at least half-descent. I was wondering if you any of you guys know of one? ",13,12
178,2015-10-9,2015,10,9,18,3o2w3x,Do you think it can be a platform really competitive with GPU Tesla?,https://www.reddit.com/r/MachineLearning/comments/3o2w3x/do_you_think_it_can_be_a_platform_really/,Parmeni,1444381311,,5,0
179,2015-10-9,2015,10,9,18,3o2xqt,Time series ranking? E.g. Early Twitter trend detection?,https://www.reddit.com/r/MachineLearning/comments/3o2xqt/time_series_ranking_eg_early_twitter_trend/,cryptocerous,1444382687,"I have a large set of signals, of the same type, from different sensors. I want to predict which of those sensors is most likely to go from low to high, and end up among the top 100 highest-valued sensors, within a window of upcoming time periods.

Seasonality isn't significant for this dataset. 

It is very important that it detects the trends very early. If the problem is framed like this, it would be ideal -- Detect sensors that are going to be in the top 100 within the next 100 time periods, while they're still in the bottom 10,000.

NOTE: Since the problem involves an overall ranking - which sensors will end up in the top 100 - the magnitude of each signal must be considered in relation to all the other magnitudes, not just the shape of the signals over time.

This could also be seen as somewhat like an early anomaly detection system, but those don't seem to frame the problem correctly for my task.

The data follows natural patterns (unlike stocks) and looks quite predictable. It's similar to detecting of trends on twitter. See [here](https://snikolov.wordpress.com/2012/11/14/early-detection-of-twitter-trends/) and [code](https://github.com/snikolov/rumor/). But that one doesn't compare against any baseline and I doubt that it's state of the art anymore.


Are there more modern approaches to doing this, other than the one approach shown here? Maybe some code / contest / or shared task somewhere?",7,8
180,2015-10-9,2015,10,9,21,3o3ak6,Deep Learning with Torch,https://www.reddit.com/r/MachineLearning/comments/3o3ak6/deep_learning_with_torch/,SuperFX,1444392134,,5,18
181,2015-10-9,2015,10,9,21,3o3bxx,Data Perspective: Topic Modeling in R,https://www.reddit.com/r/MachineLearning/comments/3o3bxx/data_perspective_topic_modeling_in_r/,padmajatamada,1444392925,,0,1
182,2015-10-9,2015,10,9,21,3o3fi8,Possible to train classifiers in parallel on clusters?,https://www.reddit.com/r/MachineLearning/comments/3o3fi8/possible_to_train_classifiers_in_parallel_on/,i_sed_so,1444394847,"Is it possible to train classifiers on data in parallel? I think it makes sense when bagging. But what about in the case of SVM? Can this be only done by creating subsets of data? Does this tend to be easier with neural nets? If so, why?

Thanks!

EDIT : by clusters I mean parallel computing clusters and not clusters in the data! Sorry about the ambiguity.",2,1
183,2015-10-9,2015,10,9,21,3o3fzh,The Trouble with Missing Data - Computerphile,https://www.reddit.com/r/MachineLearning/comments/3o3fzh/the_trouble_with_missing_data_computerphile/,jackbrucesimpson,1444395086,,0,10
184,2015-10-9,2015,10,9,22,3o3iwv,Thoughts when considering a Machine Learning project - A 2013 post that still stands.,https://www.reddit.com/r/MachineLearning/comments/3o3iwv/thoughts_when_considering_a_machine_learning/,thesameoldstories,1444396569,,3,11
185,2015-10-9,2015,10,9,23,3o3pbg,Increasing Levels of Abstraction With More LSTM Layers,https://www.reddit.com/r/MachineLearning/comments/3o3pbg/increasing_levels_of_abstraction_with_more_lstm/,LeavesBreathe,1444399693,"Hey Guys, Question about levels of abstraction.

When it comes to writing text, Karpathy wrote an excellent [post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) using character based training with lstms. 

I noticed in his post that he regularly used 2 layers of lstms and at max three. I went ahead and used 4 and 5 layers, and obtained slightly better results (in my opinion -- hard to actually judge objectively). But it started to match word pairs together. 

The reason why I think this is the case is that by using more and more layers, you increase the final level of abstraction you can reach.

For example, if you used character based writing, you first have to learn that there is a concept of words separated by spaces. We'll call that level one. 

However, if you use more levels (say 5 or 6), I believe that you start to get the next level: That is, you understand that these words have relationships to each other. This is level two of abstraction.

So I guess my question is: Should I go for the 10 layer lstm? It would take me a week to train on my 980 TI. Does abstraction level correlate with the number of layers you stack? 

I'm hoping to reach level three of abstraction: Sentence to sentence relationships. 

I put 256 hidden variables on each layer with a dropout of 0.5 between each one. I'm also currently investigating word to word relationships instead of character to character. 

P.S. I know I have been asking many questions lately, but the help I have received has really changed my machine learning thought process tremendously. Don't mean to annoy!",9,8
186,2015-10-10,2015,10,10,0,3o3xy4,Generating a Word2Vec model from a block of Text using Gensim (Python),https://www.reddit.com/r/MachineLearning/comments/3o3xy4/generating_a_word2vec_model_from_a_block_of_text/,sachinrjoglekar,1444403622,,0,6
187,2015-10-10,2015,10,10,0,3o42e3,Excellent Youtube playlist explaining Machine Learning concepts,https://www.reddit.com/r/MachineLearning/comments/3o42e3/excellent_youtube_playlist_explaining_machine/,OMICRON_PERSEI_VIII,1444405568,,24,274
188,2015-10-10,2015,10,10,1,3o495q,Comparing Python Clustering Algorithms,https://www.reddit.com/r/MachineLearning/comments/3o495q/comparing_python_clustering_algorithms/,cast42,1444408480,,0,11
189,2015-10-10,2015,10,10,2,3o4fna,New Reinforcement Learning Algorithm + Demo!,https://www.reddit.com/r/MachineLearning/comments/3o4fna/new_reinforcement_learning_algorithm_demo/,CireNeikual,1444411252,,5,2
190,2015-10-10,2015,10,10,2,3o4g82,"Wikimedia, Zipcar, Fitbit Are Among IBM Watsons New Neighbors",https://www.reddit.com/r/MachineLearning/comments/3o4g82/wikimedia_zipcar_fitbit_are_among_ibm_watsons_new/,teklaperry,1444411507,,0,1
191,2015-10-10,2015,10,10,2,3o4kf6,"Machine Learning So Easy, Even Your Cat Could Do It (Part 2): Text Tags",https://www.reddit.com/r/MachineLearning/comments/3o4kf6/machine_learning_so_easy_even_your_cat_could_do/,[deleted],1444413222,[deleted],0,1
192,2015-10-10,2015,10,10,3,3o4qrs,BigML: Machine Learning made easy (with code),https://www.reddit.com/r/MachineLearning/comments/3o4qrs/bigml_machine_learning_made_easy_with_code/,[deleted],1444415912,[deleted],0,0
193,2015-10-10,2015,10,10,3,3o4r58,AWS re:Invent 2015: real-world smart applications with Amazon Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3o4r58/aws_reinvent_2015_realworld_smart_applications/,asdrubaldone,1444416089,,0,0
194,2015-10-10,2015,10,10,4,3o4x1n,Parmesan: Variational and semi-supervised neural network toppings for Lasagne,https://www.reddit.com/r/MachineLearning/comments/3o4x1n/parmesan_variational_and_semisupervised_neural/,modeless,1444418612,,1,17
195,2015-10-10,2015,10,10,4,3o521q,Making Sense of IoT Data with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3o521q/making_sense_of_iot_data_with_machine_learning/,reworksophie,1444420745,,0,1
196,2015-10-10,2015,10,10,7,3o5m41,Concepts You Want to Know Related to Data Science,https://www.reddit.com/r/MachineLearning/comments/3o5m41/concepts_you_want_to_know_related_to_data_science/,shugert,1444430011,,0,0
197,2015-10-10,2015,10,10,7,3o5ncg,Analytics of Things: The Beginners Guide.,https://www.reddit.com/r/MachineLearning/comments/3o5ncg/analytics_of_things_the_beginners_guide/,shugert,1444430625,,1,0
198,2015-10-10,2015,10,10,7,3o5ny6,[burning] Watch Suffragette 2015 Full Movie Free,https://www.reddit.com/r/MachineLearning/comments/3o5ny6/burning_watch_suffragette_2015_full_movie_free/,pointlesscabin3lt,1444430915,[removed],1,1
199,2015-10-10,2015,10,10,8,3o5ptl,Reputation Management with Business Analytics,https://www.reddit.com/r/MachineLearning/comments/3o5ptl/reputation_management_with_business_analytics/,TheDataPit,1444431815,,0,0
200,2015-10-10,2015,10,10,11,3o6b2v,Tree-structured composition in neural networks without tree-structured architectures,https://www.reddit.com/r/MachineLearning/comments/3o6b2v/treestructured_composition_in_neural_networks/,SuperFX,1444443254,,1,6
201,2015-10-10,2015,10,10,15,3o7149,Can robots truly be creative and use their imagination?,https://www.reddit.com/r/MachineLearning/comments/3o7149/can_robots_truly_be_creative_and_use_their/,certaintyisdangerous,1444460380,,2,0
202,2015-10-10,2015,10,10,16,3o72tz,Random search/bayesian optimization,https://www.reddit.com/r/MachineLearning/comments/3o72tz/random_searchbayesian_optimization/,[deleted],1444461847,[deleted],6,3
203,2015-10-10,2015,10,10,18,3o7cbw,"Tor is in theory a great place for AIs to interact with eachother because all the addresses can reach eachother, and you can allocate as many new addresses as you want.",https://www.reddit.com/r/MachineLearning/comments/3o7cbw/tor_is_in_theory_a_great_place_for_ais_to/,BenRayfield,1444470462,What are some AIs already in Tor? What are they doing?,2,0
204,2015-10-10,2015,10,10,22,3o7ryg,The 5 Tribes of Machine Learning - Pedro Domingos @MLconf,https://www.reddit.com/r/MachineLearning/comments/3o7ryg/the_5_tribes_of_machine_learning_pedro_domingos/,rinuboney,1444482736,,9,79
205,2015-10-10,2015,10,10,23,3o82wv,Interesting probability theory behind machine learning?,https://www.reddit.com/r/MachineLearning/comments/3o82wv/interesting_probability_theory_behind_machine/,tablelegend,1444489062,"I want to write my bachelor thesis about ML. I am reading bishops book at the moment, and until now the probability theory ""heaviest"" part was gaussian processes. At the moment I am at graphical models.

To the more experienced MLers, in which areas should I search to find interesting theoretical questions regarding probability theory/stochastics in machine learning?",4,9
206,2015-10-11,2015,10,11,0,3o86np,"Novice Question - How do you initially work with a new data set to find the ""interesting"" pieces?",https://www.reddit.com/r/MachineLearning/comments/3o86np/novice_question_how_do_you_initially_work_with_a/,Simusid,1444490916,"I'm a ML novice, and I'm basically starting by watching the Andrew Ng videos and various other ML videos.   I'm interested in classification and I find SVM to be really interesting. I've been moderately successful with some simple canned stuff and I want to move on to some ""real world"" data sets.

To that end I have obtained a dataset of 25M ambulance calls that comes with an extensive [data dictionary](http://nemsis.org/v2/downloads/documents/NEMSIS_Data_Dictionary_v2.2.1_04092012.pdf).

I've worked through a lot of novice things like importing all the CSV files into MySQL, learning some R, learning to use the RMySQL library, querying remotely and massaging large data frames (particularly annoying is getting numeric data, when the dataset has placeholder values of ""."")

My exploratory queries where I'm just trying to get a sense of the 25M records some time take minutes to run (db indexes have helped that).   But I'm getting very bogged down with trying to decide what columns in my database might be ""interesting"" and useful enough as an input to an SVM.    

Is there a typical approach to preselect or triage large data sets to help get a sense of what columns might be relevant and worthy of more time and attention?   Sadly I have a short attention span and I'd like to learn a more efficient way of approaching 15GB of data.",6,6
207,2015-10-11,2015,10,11,1,3o8bto,IBM Watson invited to Allen Brain Institute AI challenge,https://www.reddit.com/r/MachineLearning/comments/3o8bto/ibm_watson_invited_to_allen_brain_institute_ai/,beemerteam,1444493519,,0,1
208,2015-10-11,2015,10,11,2,3o8mlr,Please advice : Exploding Gradient,https://www.reddit.com/r/MachineLearning/comments/3o8mlr/please_advice_exploding_gradient/,muktabh,1444498661,"Hi,
I have a neural network where I am trying to optimize sum of a reconstruction error, a rectified linear output and L2 norm of weights using SGD.

Here are some of the cases I face:

1. When I chose a high coefficient for L2 norms of weights, the cost reaches infinity pretty quickly. Since I am restricting the range in which weights can vary, this seems to be due to high coefficient
2. The same happens when I chose a low coefficient for L2 norm. This I assume is because the weights are allowed to vary more and hence the gradient gets pretty high.
3. Since this (2) seems to be a case of gradient overshoot, I clip the gradient at a low threshold. In this case, the gradient seems to be stuck in a range where the overall cost is mostly comprised of the L2 norm (so if loss if 10, the loss due to L2 norm is 8, due to reconstruction and relu is 2). When I run this for a long time, the results seem to be OKayish, but I am not sure SGD optimized well.
4. When I try changing the initialization of weights (even by a factor of 1000) , it just seems to delay the converging to infinity in (1) and (2) by sometime, but nothing else.

From what I can think, the right way is to find a middle ground by setting weight-initializations, coefficient of L2 norm and Gradient Clipping. Is there a thumb rule I should follow while varying these three?
Also I might be missing out on some method I should have employed, if you think that is the case, please let me know.",12,0
209,2015-10-11,2015,10,11,3,3o8rtm,OpenCV +android to implement a facial rating algortihm,https://www.reddit.com/r/MachineLearning/comments/3o8rtm/opencv_android_to_implement_a_facial_rating/,jinjamaverick,1444501010,"How can one implement a facial rating algorithm in OpenCV ? 
Like on scale from 1-10. How good he/she looks.
And which algorithm for facial rating would prouduce more enhanced results?


",3,0
210,2015-10-11,2015,10,11,7,3o9mpq,Simulation of a system with an deep autoencoder?,https://www.reddit.com/r/MachineLearning/comments/3o9mpq/simulation_of_a_system_with_an_deep_autoencoder/,brockl33,1444515395,"I have been thinking about using a deep autoencoder as a way of simulating artificial changes but I would like to get some more opinions.

Hypothetically you have an trained a deep autoencoder regularized with dropout on 1,000,000 different cities which each had data on 1,000 informative features (e.g. population, incomes, crime rate, date, etc). Would you be able to feed in a manipulated example to predict changes in that example?

For example, let's say I took the Los Angeles input vector and reduced the population size by a little bit or increased the average family income: would such a model be able to extrapolate and accurately predict the future state of the LA if that artificial change took place?

I think I am having trouble here because my understanding with such complex systems is that I can imagine almost all features to capable of causal influence. Hinton talks about using multiplicative interaction models in a similar way which is what set me off on this thought process [Causal generation from a learned model](http://videolectures.net/nips09_hinton_dlmi/).",4,0
211,2015-10-11,2015,10,11,9,3oa325,How is Netflix using machine learning to understand users?,https://www.reddit.com/r/MachineLearning/comments/3oa325/how_is_netflix_using_machine_learning_to/,buddiBot,1444523803,"Hi everyone,
Can anyone tell me how Netflix uses ML to know user's preferences?
Are there any services that offers ML on user's preferences?",5,9
212,2015-10-11,2015,10,11,11,3oaekd,SDRRL Reinforcement Learning Algorithm Tutorial,https://www.reddit.com/r/MachineLearning/comments/3oaekd/sdrrl_reinforcement_learning_algorithm_tutorial/,CireNeikual,1444530344,,5,13
213,2015-10-11,2015,10,11,13,3oapzj,What's the easiest way to make a classifier that maximizes f1?,https://www.reddit.com/r/MachineLearning/comments/3oapzj/whats_the_easiest_way_to_make_a_classifier_that/,wan2Kno,1444537079,I can't figure out a way to do this with scikit learn after hours of research ... perhaps I am just missing something?,2,0
214,2015-10-11,2015,10,11,15,3oayxh,Artificial Neural Network - Case study,https://www.reddit.com/r/MachineLearning/comments/3oayxh/artificial_neural_network_case_study/,Yigalw,1444543240,http://ucanalytics.com/blogs/artificial-neural-networks-retail-case-study-example-part-8/?relatedposts_hit=1&amp;relatedposts_origin=3973&amp;relatedposts_position=0,0,0
215,2015-10-11,2015,10,11,23,3oc3g3,"Facebooks's AI system to ""understand"" text",https://www.reddit.com/r/MachineLearning/comments/3oc3g3/facebookss_ai_system_to_understand_text/,MrJiks,1444575295,"In the video: [Facebook F8 Keynote](https://youtu.be/1rIdujZ0Gv0?t=839), just after 13 minutes. They are using a system called Memory Network. Have anyone able to install it? Looks to me like a breathtaking system.",17,12
216,2015-10-12,2015,10,12,0,3oc6fc,What would it take to get a PhD in Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/3oc6fc/what_would_it_take_to_get_a_phd_in_machine/,mrdrozdov,1444576760,,13,2
217,2015-10-12,2015,10,12,1,3ocdye,Public Dataset: Long-tail regression with neural network baseline?,https://www.reddit.com/r/MachineLearning/comments/3ocdye/public_dataset_longtail_regression_with_neural/,alexmlamb,1444580238,"Hello, 

Does anyone know of a dataset which has the following properties: 

1.  Public.  
2.  It's a regression problem and the distribution is long-tailed.  For example, views of different TV shows, housing prices, etc.  
3.  Ideally it would have an existing NN baseline.  
4.  Ideally somewhat large (millions of examples).  

Best, 

Alex.  ",0,2
218,2015-10-12,2015,10,12,1,3och7o,How would you use Scikit learn to predict user behavior?,https://www.reddit.com/r/MachineLearning/comments/3och7o/how_would_you_use_scikit_learn_to_predict_user/,buddiBot,1444581714,"Hi,

I have a dataset that contains a list of words that the user ""likes"". I am wondering how I could use sklearn to make predictions if a user would ""like"" a new entry that has a set of words or analyze what words that the user would most likely ""like""
Thanks guys",3,8
219,2015-10-12,2015,10,12,1,3oci5l,Pca-magic: PCA that iteratively replaces missing data,https://www.reddit.com/r/MachineLearning/comments/3oci5l/pcamagic_pca_that_iteratively_replaces_missing/,nrpprn,1444582133,,8,60
220,2015-10-12,2015,10,12,2,3ocpa9,DeepLearning frameworks outside Python,https://www.reddit.com/r/MachineLearning/comments/3ocpa9/deeplearning_frameworks_outside_python/,petrux,1444585453,"I spent the whole weekend with Theano and Lasagne and I am quite happy of them. As far as you know, is there any other framework for any other language (e.g. Java/Scala, C#, haskell) which could be comparable to the best one in the python environment? Chose you which one is the best, I just need to know is there is anything comparable. Thanks in advance.",35,7
221,2015-10-12,2015,10,12,3,3octoe,How machine learning plays a key role in Amazon retail and Kindle services,https://www.reddit.com/r/MachineLearning/comments/3octoe/how_machine_learning_plays_a_key_role_in_amazon/,djphilosopher,1444587458,,1,0
222,2015-10-12,2015,10,12,3,3ocx0j,"14 Great Machine Learning, Data Science, R , DataViz Cheat Sheets",https://www.reddit.com/r/MachineLearning/comments/3ocx0j/14_great_machine_learning_data_science_r_dataviz/,vincentg64,1444588956,,0,1
223,2015-10-12,2015,10,12,3,3ocyr7,Deep Q Networks and Policy Abstraction,https://www.reddit.com/r/MachineLearning/comments/3ocyr7/deep_q_networks_and_policy_abstraction/,BoldArtistry,1444589763,"How do Deep Q networks actually go about abstracting policies for an MDP?

References:
Link: http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html


Quote: ""Here we use recent advances in training deep neural networks9, 10, 11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning.""

",2,1
224,2015-10-12,2015,10,12,4,3oczs2,Efficient Per-Example Gradient Computations,https://www.reddit.com/r/MachineLearning/comments/3oczs2/efficient_perexample_gradient_computations/,downtownslim,1444590217,,0,9
225,2015-10-12,2015,10,12,4,3od4hx,Where/how do you fit your training data?,https://www.reddit.com/r/MachineLearning/comments/3od4hx/wherehow_do_you_fit_your_training_data/,OSUperson,1444592255,"I'm somewhat new to machine learning, and I'm just wondering where/how others fit their training data. I'm currently trying out scikit-learn's SVC implementation with MNIST digit data on my Macbook Pro (2.3GHz Core i7 w/ 16GB RAM), but it's taking quite a while to train. 

Has anyone used any of Amazon's tools or any HPC-as-a-service kind of tools for training models? Or is it pretty common to just wait it out on a laptop for fairly trivial ML problems?

Thanks in advance!
",2,0
226,2015-10-12,2015,10,12,4,3od5zp,Combine RL with a classifier?,https://www.reddit.com/r/MachineLearning/comments/3od5zp/combine_rl_with_a_classifier/,[deleted],1444592889,[deleted],1,0
227,2015-10-12,2015,10,12,7,3oduuu,A review of parameter regularization and Bayesian regression,https://www.reddit.com/r/MachineLearning/comments/3oduuu/a_review_of_parameter_regularization_and_bayesian/,efavdb,1444604268,,2,1
228,2015-10-12,2015,10,12,8,3odxkx,Time-frequency bootstrapping and cross validation - am I doing this right?,https://www.reddit.com/r/MachineLearning/comments/3odxkx/timefrequency_bootstrapping_and_cross_validation/,neo82087,1444605578,"Hi, I was hoping to get some input on a technique I'm using to train and evaluate a model for biological time series classification. My initial set of data is unfortunately quite small, so I've generated artificial trials by time-frequency bootstrapping (break down each trial by time and frequency components and randomly recombine within a particular class to generate novel trials). 

Instead of using cross validation, I'm training on only the artificial dataset and testing on the actual data. Does this seem like a legitimate technique or am I missing something that will confound my results? I've tested and it seems that once past a minimum number of artificial trials, generating more doesn't seem to increase classification accuracy. 

I was hoping to compare classification of the actual data to a label shuffled null set, but without cross validation I only have a single accuracy value. Would it make sense to subsample from the test data and do a number of training/test loops to generate a distribution of classification accuracies?

I apologize if these are obvious questions, but I'm quite new to ML and learning on my own! Any help or suggestions are absolutely appreciated!

Thanks!",3,2
229,2015-10-12,2015,10,12,9,3oe45s,Gaussian Mixture Model with Latent Variables,https://www.reddit.com/r/MachineLearning/comments/3oe45s/gaussian_mixture_model_with_latent_variables/,jostmey,1444608874,"Can a Gaussian Mixture Model with latent variables be solved analytically? Is there a closed form solution for finding the best fit to the data assuming we don't have any data for the hidden variables? Or is a method like Expectation Maximization, which may just find a local minima, the only approach in this case?

Thanks!",5,2
230,2015-10-12,2015,10,12,10,3oeaw0,Converting a matrix to plot on a 2D graph,https://www.reddit.com/r/MachineLearning/comments/3oeaw0/converting_a_matrix_to_plot_on_a_2d_graph/,buddiBot,1444612249,Does anyone know how do this with python? I want to plot matrixs onto a graph to use SVM.  ,1,0
231,2015-10-12,2015,10,12,11,3oeh3q,Ask Your Neurons: A Neural-based Approach to Answering Questions about Images,https://www.reddit.com/r/MachineLearning/comments/3oeh3q/ask_your_neurons_a_neuralbased_approach_to/,downtownslim,1444615357,,2,26
232,2015-10-12,2015,10,12,11,3oemn0,Eric Drexler - A Cambrian Explosion in Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3oemn0/eric_drexler_a_cambrian_explosion_in_deep_learning/,[deleted],1444618265,[deleted],0,0
233,2015-10-12,2015,10,12,13,3oex41,Random Trees algorithm in SPSS Modeler 17.1,https://www.reddit.com/r/MachineLearning/comments/3oex41/random_trees_algorithm_in_spss_modeler_171/,aruizga7,1444624011,,0,0
234,2015-10-12,2015,10,12,19,3ofo0i,"""Continuous"" Multi-Label Classification with Real Values",https://www.reddit.com/r/MachineLearning/comments/3ofo0i/continuous_multilabel_classification_with_real/,TheCocoanaut,1444644107,"Hi there!

I have data in the format of text and associated labels given by values between 1 and 5, where 2.5 is the zero point (&gt; 2.5 means the label is positive for this instance, &lt; 2.5 means it is negative, 2.5 is neutral). (Note that the values are not discrete but continuous.)
I always assumed that this was simply a multi-label classification problem. Currently I think it is more of a ""bunch of regression"" problem where I should predict the value for each ""label"".

My original plan was to predict labels for each instance and then extract the estimated probability for each of those labels (ranging from 0 to 1) and scale them linearly to [1,5]. I am not sure if this is too nave or not, or even terrifyingly wrong.

I'd love to hear your input on this as I am not sure this is the way to go.

/edit: Just another note: the labels do have interdependencies I'd like to take into account.",4,2
235,2015-10-12,2015,10,12,19,3ofow1,Controlled Experiments for Word Embeddings,https://www.reddit.com/r/MachineLearning/comments/3ofow1/controlled_experiments_for_word_embeddings/,benjaminwilson,1444644768,,4,22
236,2015-10-12,2015,10,12,19,3ofq5n,Packaging Equipment Market in North America: Research Report 2015-2019,https://www.reddit.com/r/MachineLearning/comments/3ofq5n/packaging_equipment_market_in_north_america/,roshanimrr,1444645754,,0,1
237,2015-10-12,2015,10,12,21,3og308,Earn Money 2016: Basic to Advance Dailymotion Setting,https://www.reddit.com/r/MachineLearning/comments/3og308/earn_money_2016_basic_to_advance_dailymotion/,[deleted],1444654431,[deleted],0,0
238,2015-10-12,2015,10,12,22,3og80o,Recreating Skip-Thoughts Paper In Keras,https://www.reddit.com/r/MachineLearning/comments/3og80o/recreating_skipthoughts_paper_in_keras/,LeavesBreathe,1444657138,"Hey Everyone, There's a pretty popular paper on skip thoughts here:
http://arxiv.org/pdf/1506.06726v1.pdf

I've spent some time trying to recreate this paper in [Keras](http://keras.io/). 

In summary, I want to feed the model an array of words (translated to their respective integers) and get an output of an array of integers (that correspond with their respective words). Each array should be a sentence. 

For example the input sentence would be: ""The stars shine very brightly."" and the output sentence would be something like ""They shine brightly because of the enormous amount of energy they contain"". 

My full script can be found on github:
https://github.com/LeavesBreathe/Sequence-To-Sequence-Generation-Skip-Thoughts-/blob/master/sentence_to_sentence_generation.py

There are plenty of examples in Keras using text-generation, but they all use one-hot vectors for words/letters. 

However, I've heard from several that using an embedding layer is more efficient than one-hotting words. To that end, I can't find my examples that use embedding AND predict from their model. 

The main problem I'm currently having is this. The whole thing works except that when I do the predict function, I end up getting a 1d array of incredibly small numbers. Instead, I want to get a 1d array of integers that correspond with the index_to_word dictionary. 

Here is an example of my output currently when I use the model.predict function:
    
	[ 0.00055403  0.00111817  0.00215877  0.00108721  0.000766    0.00319258
	  0.00298639  0.00099919  0.00404425  0.00052357  0.00211825  0.00094894
	  0.00157265  0.00308218  0.01090179  0.00284554  0.00237611  0.00128609
	  0.01087372  0.01681928  0.00361982  0.02696433  0.043065    0.00565515
	  0.0075417   0.01731373  0.02892326  0.00648065  0.0115167   0.00250217
	  0.04943457  0.0069293   0.03155829  0.00629489  0.00888385  0.00769502
	  0.00804267  0.01472379  0.01857707  0.03626939  0.11137675  0.06347591
	  0.03270336  0.05129562  0.08768249  0.08185349  0.06703533  0.07524383
	  0.01045811  0.00662899]


I guess my question is: How do you use a model.predict function knowing that you're using an embedding layer? Is there somthing fundamentally wrong that I'm doing?

P.S. apologies for the excessive amounts of comments on the github script",13,7
239,2015-10-12,2015,10,12,23,3ogfzu,How do you avoid self-enforcing predictions/recommendations?,https://www.reddit.com/r/MachineLearning/comments/3ogfzu/how_do_you_avoid_selfenforcing/,thefone,1444661030,"Hi guys! I want to start off by saying that ML is not my area, but recently I have started reading a lot about it and it really intrigues me.

One thing that has crossed my mind is how you stop ""self-enforcing"" (not sure if that is the correct term) predictions/recommendations in engines based on a bunch of data.

To clarify, I discovered Quake III Arena while trying to download Tony Hawk Pro Skater 3 from DC++. I discovered french electro duo Justice while searching for Metallica on Spotify. A lot of the things I've discovered have been relatively random, and I can't help but think that if a recommendation engine were to ""choose for me"" instead, I would've nevered discovered the greatest FPS of all time while searching for a skateboard game, or finding an electro duo while searching for heavy metal. In contrast, today I noticed that the ""Recommended for you"" playlist on Spotify is entirely based on what genres I've listened to in the last weeks, resulting in a playlist consisting solely of Swedish hiphop. If I were to listen to that, next week's recommended playlist would probably also just consist of that genre.

I guess my question is: with 1. all the data collected on me 2. the trend that more and more decisions are taken for me by intelligent engines; wouldn't I ultimately be seeing the same things over and over again? Is this some kind of phenomenon that is discussed in ML? I guess there is an obvious answer to this, but I can't really find it by searching, and was wondering if you guys could help me in my thinking.",7,4
240,2015-10-13,2015,10,13,0,3ogjde,We need open and vendor-neutral metadata services,https://www.reddit.com/r/MachineLearning/comments/3ogjde/we_need_open_and_vendorneutral_metadata_services/,gradientflow,1444662494,,0,0
241,2015-10-13,2015,10,13,0,3ogmjo,Neural Turing Machine in pure numpy. Implements all 5 tasks from paper.,https://www.reddit.com/r/MachineLearning/comments/3ogmjo/neural_turing_machine_in_pure_numpy_implements/,doctorteeth2,1444663828,,12,112
242,2015-10-13,2015,10,13,1,3ogts6,Masters/PhD in Machine Learning vs. Statistics or Computer Science,https://www.reddit.com/r/MachineLearning/comments/3ogts6/mastersphd_in_machine_learning_vs_statistics_or/,garygulf,1444666818,"I'm currently getting a master's degree in analytics.  From what I've learned, I find machine learning to be the most interesting aspect/approach of data analysis, and I also think it has the most potential for use in the future (ie. traditional statistical approaches may be phased out in favor of machine learning techniques).

After graduation, I'm considering continuing my studies and perhaps even going for a PhD, as I would like to further develop my knowledge of data science/analysis.  

As someone interested in machine learning, would it be better to aim for a computer science degree or a statistics degree?  Essentially all jobs that I have looked at that require (or prefer) a PhD list both as viable options.  

Also, is Carnegie Mellon the only school to offer an official Machine Learning degree?  Their program is so competitive that I don't think I would be accepted...I have good grades (undergrad and grad both above a 3.8), but my bachelor's degree is in political science, and I only started studying in this field (through MOOCs) about three years ago, so I don't have a long history of quantitative experience that would qualify me for a program like that.",15,1
243,2015-10-13,2015,10,13,2,3oh70x,Towards end-to-end learning in robotics: http://goo.gl/LRRsaj @padsmagt keynote on deep learning in robotics at this years #IROS2015,https://www.reddit.com/r/MachineLearning/comments/3oh70x/towards_endtoend_learning_in_robotics/,osdf,1444672127,,2,6
244,2015-10-13,2015,10,13,2,3oh7iu,I made a computer pretend to be a computer thats pretending to be a human. Heres what happened.,https://www.reddit.com/r/MachineLearning/comments/3oh7iu/i_made_a_computer_pretend_to_be_a_computer_thats/,joshdotai,1444672337,,0,0
245,2015-10-13,2015,10,13,4,3ohjuc,Master Algorithm is to Machine Learning what the Standard Model is to Particle Physics.,https://www.reddit.com/r/MachineLearning/comments/3ohjuc/master_algorithm_is_to_machine_learning_what_the/,vikashkodati,1444677096,,3,0
246,2015-10-13,2015,10,13,4,3ohkbf,Atari dreams with RNNs (the last part of the lecture by Alex Graves),https://www.reddit.com/r/MachineLearning/comments/3ohkbf/atari_dreams_with_rnns_the_last_part_of_the/,Ghostlike4331,1444677296,,0,9
247,2015-10-13,2015,10,13,4,3ohkt8,I solved Facebook's bAbi and found lots of errors in the dataset,https://www.reddit.com/r/MachineLearning/comments/3ohkt8/i_solved_facebooks_babi_and_found_lots_of_errors/,knighton_,1444677475,,23,15
248,2015-10-13,2015,10,13,5,3ohsyt,The similarities between NNs and how the brain encodes spatial information are really inspiring.,https://www.reddit.com/r/MachineLearning/comments/3ohsyt/the_similarities_between_nns_and_how_the_brain/,[deleted],1444680616,[deleted],0,0
249,2015-10-13,2015,10,13,7,3oiefh,"Quantization then reduces the number of bits that represent each connection from 32 to 5. ... reduced the size of VGG16 by 49 from 552MB to 11.3MB,again with no loss of accuracy.",https://www.reddit.com/r/MachineLearning/comments/3oiefh/quantization_then_reduces_the_number_of_bits_that/,derRoller,1444689425,,4,29
250,2015-10-13,2015,10,13,10,3oj2us,Linear Support Vector Machine in Modeler 17.1,https://www.reddit.com/r/MachineLearning/comments/3oj2us/linear_support_vector_machine_in_modeler_171/,aruizga7,1444700436,,7,0
251,2015-10-13,2015,10,13,12,3ojdrh,Question Regarding Blending And Stacking,https://www.reddit.com/r/MachineLearning/comments/3ojdrh/question_regarding_blending_and_stacking/,tehsandvich,1444705521,I've read a couple of posts regarding blending and stacking. But I can't seem to find any R code examples of them. ,4,1
252,2015-10-13,2015,10,13,12,3ojfs9,"NN architecture that ""disovers"" 2D nature of input data?",https://www.reddit.com/r/MachineLearning/comments/3ojfs9/nn_architecture_that_disovers_2d_nature_of_input/,physixer,1444706482,"Sorry for the typo in title: ~~""disovers""~~ ""discovers""

I'm reading about CNNs and how they take advantage of 2D nature of images, when a fully connected non-CNN network would treat input as a sequence of 1D numbers.

My question is, has anyone done any work on ""discovering"" the 2D (or 3D or whatever) nature of the inputs, then modify itself and evolve into a CNN without the human researcher hand-coding it as a CNN?

After all, 2D nature of input data itself is a feature.",8,2
253,2015-10-13,2015,10,13,14,3ojtvo,"PACKT is offering today, for free the ebook ""Building Machine Learning Systems with Python""",https://www.reddit.com/r/MachineLearning/comments/3ojtvo/packt_is_offering_today_for_free_the_ebook/,[deleted],1444714156,[deleted],0,1
254,2015-10-13,2015,10,13,19,3okigv,Help with the Maxout Paper by Ian J. Goodfellow,https://www.reddit.com/r/MachineLearning/comments/3okigv/help_with_the_maxout_paper_by_ian_j_goodfellow/,rishok,1444731634,"Hi
I look at the code provided with the Maxout network Paper by Ian J. Goodfellow. In order to somewhat recreate the somewhat recreate the architecture (of the convolutional model) i am looking at the hyperparameter. However, i can't get the hyperparameter to fit according to his code:
https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/scripts/papers/maxout/mnist.yaml


As I understand:

input -&gt; 28x28

conv -&gt; filter= 8x8 with stride 1x1

pool layer -&gt; pool = 4x4 with stride 2x2

however that is not possible.... any help ?",7,1
255,2015-10-13,2015,10,13,19,3okioy,"Applying ML in areas for ""social good""",https://www.reddit.com/r/MachineLearning/comments/3okioy/applying_ml_in_areas_for_social_good/,askacadthrowaway,1444731787,"Hello all,

I'm interested in a career in ML. However, what excites me about ML isn't really the prevalent uses of ML but rather just the math and theories behind it. I want my work to ""matter"" in a way that I find meaningful (I briefly debated studying to be a doctor before deciding something more math-intensive is the right choice for me). So for me, the ideal would be applying ML methods (or even just computational methods) in fields that can benefit other people. The example I come up with is using quantitative methods to identify pathologies based on certain markers: this is actually something someone at my university's pharm school is working jointly with my (undergrad) lab to accomplish. But while quantitative/ computational biology is one such useful application of ML methods, I imagine the field have much more.

Really, I'm just interested in anything involving doing ML for the benefit of society:

* potential fields

* potential career paths (I have no problem doing a PhD, but are they required)

* potential companies or types of companies

* interesting papers

* interesting research groups

* relevant anecdotes

* anything else you find relevant

And so on. I'd really appreciate all of your help! :)

Edit: Thanks all for the responses! You have really helped me continue to figure things out. I'm always open to new information and perspectives, in case you're opening this late. :)",26,10
256,2015-10-13,2015,10,13,20,3okowe,"What are your thoughts on ""The Ladder"" algorithm for accurate leaderboards? Is there any chance Kaggle could adopt it?",https://www.reddit.com/r/MachineLearning/comments/3okowe/what_are_your_thoughts_on_the_ladder_algorithm/,AlfonzoKaizerKok,1444736012,"The original paper can be found [here](http://arxiv.org/abs/1502.04585), inspired by [this paper](http://arxiv.org/abs/1411.2664). I was reminded about it after [this](http://arxiv.org/abs/1510.03349) came out today. I would be interested in any sort of discussion about the topic!",5,1
257,2015-10-13,2015,10,13,20,3okpn2,What happens with backpropagation when batch normalization is applied at every layer?,https://www.reddit.com/r/MachineLearning/comments/3okpn2/what_happens_with_backpropagation_when_batch/,personalityson,1444736468,"Do you adjust the derivative somehow?


Basically, if I used max(0, x) as my activation function previously, the new activation function with batch normalization becomes
max(0, (x-mu)/s)

Derivating it gives me 0 if x&lt;=0, and 1/s if x&gt;0... But I'm stuck here...",1,1
258,2015-10-13,2015,10,13,20,3okpuu,Training (deep) Neural Networks Part: 1,https://www.reddit.com/r/MachineLearning/comments/3okpuu/training_deep_neural_networks_part_1/,upulbandara,1444736590,,2,7
259,2015-10-13,2015,10,13,21,3oku2z,Ask reddit: are variational methods practical?,https://www.reddit.com/r/MachineLearning/comments/3oku2z/ask_reddit_are_variational_methods_practical/,[deleted],1444739118,"Whenever I see a paper about variational methods, the experiments in it are done on rather small or simple datasets.

While the current state-of-the-art research is done on ImageNet and ATARI games, variational methods are demoed on MNIST and multi-armed bandits.

I don't know of any Kaggle competition where variational methods were employed by the winners.

Is there something about variational methods that limits their scalability?

PRML has a chapter called Approximate Inference that mostly talks about variational methods. If one is interested in practical applications, is it worth spending weeks of one's time studying that?

**Edit:** I""m asking about variational methods applied to NNs.",18,14
260,2015-10-13,2015,10,13,21,3okumr,OpenFace: Face recognition with Google's FaceNet deep neural network.,https://www.reddit.com/r/MachineLearning/comments/3okumr/openface_face_recognition_with_googles_facenet/,bdamos,1444739391,,5,127
261,2015-10-13,2015,10,13,21,3okwed,What are the best ML news feeds?,https://www.reddit.com/r/MachineLearning/comments/3okwed/what_are_the_best_ml_news_feeds/,Bardelaz,1444740272,"Looking for industry news, new papers, discussions. Already familiar with:

* http://www.datatau.com/
* http://news.startup.ml/
* http://gitxiv.com/top

and of course this sub-reddit! Which is awesome.",7,26
262,2015-10-13,2015,10,13,22,3okyud,Genuine Deal Is Used,https://www.reddit.com/r/MachineLearning/comments/3okyud/genuine_deal_is_used/,threedimensioneng,1444741545,,2,0
263,2015-10-13,2015,10,13,22,3ol168,Using Neural Networks to test for jumps in Financial Markets,https://www.reddit.com/r/MachineLearning/comments/3ol168/using_neural_networks_to_test_for_jumps_in/,GammaOfaHalf,1444742703,,0,11
264,2015-10-13,2015,10,13,22,3ol1yq,Interesting blog on Basic recommendation engine using R,https://www.reddit.com/r/MachineLearning/comments/3ol1yq/interesting_blog_on_basic_recommendation_engine/,padmajatamada,1444743076,,0,0
265,2015-10-13,2015,10,13,22,3ol4kf,Binary Entropy vs Crossentropy. Which to use?,https://www.reddit.com/r/MachineLearning/comments/3ol4kf/binary_entropy_vs_crossentropy_which_to_use/,Mattoss,1444744279,"In the case that I have multi-class output layer in a neural network where each input belongs to exactly one class, I have the choice to use a softmax layer with cross entropy or use a sigmoid layer with binary entropy as cost function. In the first case I might have to introduce a label for ""none of the classes"", since I need a distribution for the true label. 
I am trying to find an explanation or intuition about which costfunction is ""better"" in this situation. From my current intuition I would say crossentropy is better suited because it better reflects my assumption about the fact that I only have one true label per input. 

I would be grateful for your intuition or pointers towards literature. Thx",2,2
266,2015-10-14,2015,10,14,0,3oldr2,How can I classify sequncia of different lenghts?,https://www.reddit.com/r/MachineLearning/comments/3oldr2/how_can_i_classify_sequncia_of_different_lenghts/,[deleted],1444748469,[deleted],0,1
267,2015-10-14,2015,10,14,0,3oldxu,How can I classify sequences with different lenghts?,https://www.reddit.com/r/MachineLearning/comments/3oldxu/how_can_i_classify_sequences_with_different/,fariax,1444748550,"Hello!

I am using an RNN to classify sequences with different lenghts. If my input sequence has lenght L, then I have L outputs. After, I perform a voting scheme on the L outputs to get a unique label. Is there any smarter way to do it? What are the others ideas to achieve this?

Thank you!",3,3
268,2015-10-14,2015,10,14,0,3olg68,Auto-Generating Clickbait With Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3olg68/autogenerating_clickbait_with_recurrent_neural/,julian88888888,1444749470,,11,70
269,2015-10-14,2015,10,14,0,3olhmr,"Deep Learning Book, printing script",https://www.reddit.com/r/MachineLearning/comments/3olhmr/deep_learning_book_printing_script/,trtm,1444750077,,5,7
270,2015-10-14,2015,10,14,1,3oln72,1x1 Convolutions - Why use them?,https://www.reddit.com/r/MachineLearning/comments/3oln72/1x1_convolutions_why_use_them/,ImgPrcSng,1444752321,"Made famous by the papers Network in Network (http://arxiv.org/pdf/1312.4400v3.pdf) and used in GoogleNet and other papers,  I am still unable to grasp why we use them and what are their advantages?
One reason that I was able to find was to reduce dimensions. Are there any other advantages?",19,14
271,2015-10-14,2015,10,14,1,3olpki,Yhat releases Rodeo 1.0: Free native Python IDE,https://www.reddit.com/r/MachineLearning/comments/3olpki/yhat_releases_rodeo_10_free_native_python_ide/,elisebreda,1444753258,,3,18
272,2015-10-14,2015,10,14,1,3olubr,NegOut: Substitute for MaxOut units,https://www.reddit.com/r/MachineLearning/comments/3olubr/negout_substitute_for_maxout_units/,[deleted],1444755175,[deleted],10,8
273,2015-10-14,2015,10,14,1,3olv92,"Why accuracy alone is a bad measure for classification tasks, and what we can do about it.",https://www.reddit.com/r/MachineLearning/comments/3olv92/why_accuracy_alone_is_a_bad_measure_for/,thesameoldstories,1444755553,,2,3
274,2015-10-14,2015,10,14,2,3om0h6,Best Data Science Online Courses,https://www.reddit.com/r/MachineLearning/comments/3om0h6/best_data_science_online_courses/,[deleted],1444757644,[deleted],0,1
275,2015-10-14,2015,10,14,3,3om5jo,Sentiment analysis with machine learning and web scraped data,https://www.reddit.com/r/MachineLearning/comments/3om5jo/sentiment_analysis_with_machine_learning_and_web/,wildcodegowrong,1444759629,,0,1
276,2015-10-14,2015,10,14,4,3omj3x,How does visual search work?,https://www.reddit.com/r/MachineLearning/comments/3omj3x/how_does_visual_search_work/,buddiBot,1444764843,Can anyone tell me how does visual search work? And how can you can you use SKLearn to implement it,0,3
277,2015-10-14,2015,10,14,4,3oml2p,Bayesian CNNs with Bernoulli Approximate Variational Inference (7.7% error on CIFAR-10),https://www.reddit.com/r/MachineLearning/comments/3oml2p/bayesian_cnns_with_bernoulli_approximate/,[deleted],1444765637,,6,6
278,2015-10-14,2015,10,14,7,3onbtr,Jupyter Project and The Future of IPhyton,https://www.reddit.com/r/MachineLearning/comments/3onbtr/jupyter_project_and_the_future_of_iphyton/,shugert,1444776220,,3,1
279,2015-10-14,2015,10,14,8,3ong6z,"Demis Hassabis -""General learning algorithms (speech to the Royal Society)",https://www.reddit.com/r/MachineLearning/comments/3ong6z/demis_hassabis_general_learning_algorithms_speech/,[deleted],1444778121,[deleted],3,38
280,2015-10-14,2015,10,14,11,3oo5p3,"About Kaggle CERN challenge, what does a perfect score tell us?",https://www.reddit.com/r/MachineLearning/comments/3oo5p3/about_kaggle_cern_challenge_what_does_a_perfect/,ihsgnef,1444789723,"[Kaggle page](https://www.kaggle.com/c/flavours-of-physics/leaderboard/private)

In this challenge the winner achieved 1.0 AUC score on both public and private leaderboard.

What does this tell us?",2,2
281,2015-10-14,2015,10,14,12,3oob6b,Help on the paper 'Variational Dropout and the Local Reparameterization Trick'!,https://www.reddit.com/r/MachineLearning/comments/3oob6b/help_on_the_paper_variational_dropout_and_the/,AlfonzoKaizerKok,1444792377,"The paper can be found [here](http://arxiv.org/abs/1506.02557). 

I'm having trouble understanding the difference between variational dropout with independent noise and correlated weight noise (i.e. section 3.1 and 3.2 respectively). In particular, appendix B1 and B2 suggest that you end up sampling the same thing.

Am I missing something here? How are the weights correlated?",0,2
282,2015-10-14,2015,10,14,15,3oovqh,Bidirectional LSTM with CRF,https://www.reddit.com/r/MachineLearning/comments/3oovqh/bidirectional_lstm_with_crf/,ss5432,1444803665,"I need to implement a bidirectional LSTM network with a CRF layer at the end. Specifically the model presented in this paper, and train it.

http://www.aclweb.org/anthology/P15-1109

I want to implement it in Python preferably. Can anyone present some libraries or sample code as to how this can be done. I looked at PyBrain but couldn't really understand it.

I'm also open to tool-kits in other programming languages.",7,2
283,2015-10-14,2015,10,14,16,3op2c3,Help on speeding up Caffe LMDB creation for millions of images,https://www.reddit.com/r/MachineLearning/comments/3op2c3/help_on_speeding_up_caffe_lmdb_creation_for/,pilooch,1444808220,"I'm scaling up to larger models, using in the order of 8M images. The building of the LMDB database for Caffe is very slow (~60K images / hour). I've checked and tweaked the performances of all my drives, they're all totally fine, with perfs much much higher than the above write-through rate. Google &amp; Stackoverflow do not report much on this issue. Any hint to share ? Thanks in advance!",14,1
284,2015-10-14,2015,10,14,20,3opj5n,AskReddit: where are the best places to do a PhD in Bayesian machine learning?,https://www.reddit.com/r/MachineLearning/comments/3opj5n/askreddit_where_are_the_best_places_to_do_a_phd/,AlfonzoKaizerKok,1444820778,I know that [Zoubin Ghahramani's lab](http://mlg.eng.cam.ac.uk/) is considered to be one of the most Bayesian ML labs in the world. Which other universities do you think are good at Bayesian ML?,35,10
285,2015-10-14,2015,10,14,20,3opmq5,AUC for Linear classifiers,https://www.reddit.com/r/MachineLearning/comments/3opmq5/auc_for_linear_classifiers/,stua8992,1444823094,"Hi all,

I think that I'm across what the AUC is meant to do and I hear from a lot of people who swear by it. I've heard the definitions about probability of classifying a positive example as positive over a randomly sample negative example and all that. That's fine.

My interest here is in things that make it work extremely poorly and I think I've found a case but would just like some confirmation that I'm thinking about it correctly. Basically imagine that you have some data in two classes and you train two separate classifiers for it. The classifiers identify two parallel hyperplanes to separate the classes but they are not equivalent. Now when you adjust the threshold to generate the ROC curve those hyperplanes will basically be translated up and down normal to their direction and at each point sensitivity/specificity are calculated and so on. But if they're parallel they will surely cover the same ground and so the AUC will be identical even if one classifier is intuitively much worse than the other.

Any help would be much appreciated. Thanks for your time


",11,9
286,2015-10-14,2015,10,14,21,3ops35,Python Tutorials on Collaborative Filtering,https://www.reddit.com/r/MachineLearning/comments/3ops35/python_tutorials_on_collaborative_filtering/,MaraCerino,1444826141,,6,8
287,2015-10-14,2015,10,14,22,3opvzd,Huge 5 Axis Machining Centre moved in 3 minute time lapse video,https://www.reddit.com/r/MachineLearning/comments/3opvzd/huge_5_axis_machining_centre_moved_in_3_minute/,jebearing,1444828101,[removed],0,0
288,2015-10-14,2015,10,14,23,3oq481,Machines Informing Financial Positions And Investments. Are We Simply Moving Forward With New Technology Or We Are Just Down Right Lazy?,https://www.reddit.com/r/MachineLearning/comments/3oq481/machines_informing_financial_positions_and/,Pavan19485,1444831911,,0,1
289,2015-10-15,2015,10,15,0,3oqe0n,Deep learning  Convolutional neural networks and feature extraction with Python,https://www.reddit.com/r/MachineLearning/comments/3oqe0n/deep_learning_convolutional_neural_networks_and/,stackoverflooooooow,1444836057,,0,9
290,2015-10-15,2015,10,15,0,3oqgxe,Question about machine learning with sparse data,https://www.reddit.com/r/MachineLearning/comments/3oqgxe/question_about_machine_learning_with_sparse_data/,melgax,1444837248,"I am working in a problem in which I have several billion entries with some thousand features per entry. Each of the features take values between 0 and 10, but the problem is that not all of the features are defined for all of the entries. 
I am trying to train an algorithm that, given a list of entries, will find a list of entries similar to the input, and I've had a certain degree of success by simply using a random forest (I love random forests, they're really robust and work out of the box).
One issue I have been thinking about is what to do about the sparse features. I have not been able to find a clear treatment for these data in the literature. The standard solution seems to be treating missing entries as 0's, but for my specific case, a value of 0 means that we know something about that variable and it takes a value of 0, while not knowing the value of the variable represents a different situation. 
My question is, do you guys know of any way to deal with these situations? I have been thinking about trying to encode the missing values with a -1, but I'm also not sure if that's a good solution, since for real valued variables, -1 is closer to 0 than to 10.",1,1
291,2015-10-15,2015,10,15,1,3oqkgh,No one knows why Facebook blocked the phrase everyone will know,https://www.reddit.com/r/MachineLearning/comments/3oqkgh/no_one_knows_why_facebook_blocked_the_phrase/,pilooch,1444838686,,6,1
292,2015-10-15,2015,10,15,2,3or0k6,Visual Information Theory -- colah's blog,https://www.reddit.com/r/MachineLearning/comments/3or0k6/visual_information_theory_colahs_blog/,onewugtwowugs,1444845187,,14,125
293,2015-10-15,2015,10,15,4,3orat0,Need help with (really simple) training data,https://www.reddit.com/r/MachineLearning/comments/3orat0/need_help_with_really_simple_training_data/,Dummern,1444849204,"Please remove if off topic
I'm starting to learn to code neural networks by doing a perceptron and teaching it over and under a line in a 2d space. Pretty simple I thought.

But I can't seem to get my supervised training right. 
To debug my code I need some proper training data and output. Is there someone here that can help me with: A training set of  coordinates X and Y and if the point is over or under a line (1,-1). Tihs training set needs to be combined with the output of the actual training. Lets say staring with random weights (X,Y, Bias) and a printout of those weights for each update using some training coefficient. I use 0.01 now but any number will do.",0,1
294,2015-10-15,2015,10,15,4,3org6p,Questions about unsupervised feature extraction with few examples labeled.,https://www.reddit.com/r/MachineLearning/comments/3org6p/questions_about_unsupervised_feature_extraction/,_Jos_,1444851332,"Hi guys.

I am addressing a classification problem. The inputs are X-ray images of bones. It is a complicated task, the results using textures and geometric features are very bad.

I would like to try unsupervised feature extraction (deep learning), but i have very few have tagged images (about 200).

I am not an expert on deep learning and had thought about starting using a autoencoder. 

I welcome any comment. Thank you. 

Sorry for my English.",3,1
295,2015-10-15,2015,10,15,4,3orj66,Looking for master programs for recurrent nets and embodied learning,https://www.reddit.com/r/MachineLearning/comments/3orj66/looking_for_master_programs_for_recurrent_nets/,danijar,1444852463,"I think embodied recurrent models might be the way to more intelligent behavior. Next year I'm going to graduate. What are the best places for master programs for these research interests? CMU, UCB, Toronto, ISDIA, UCL are very good places in general. But maybe not for this special field and also I'd like to increase my chances by applying to not just those.",3,0
296,2015-10-15,2015,10,15,6,3ortua,"Professor Geoff Hinton - ""Deep Learning",https://www.reddit.com/r/MachineLearning/comments/3ortua/professor_geoff_hinton_deep_learning/,[deleted],1444856626,[deleted],0,1
297,2015-10-15,2015,10,15,6,3ortz7,"Professor Geoff Hinton's speech to the Royal Society - ""Deep Learning",https://www.reddit.com/r/MachineLearning/comments/3ortz7/professor_geoff_hintons_speech_to_the_royal/,[deleted],1444856684,[deleted],7,19
298,2015-10-15,2015,10,15,7,3os5k9,Using CNNs to solve CAPTCHAs?,https://www.reddit.com/r/MachineLearning/comments/3os5k9/using_cnns_to_solve_captchas/,polytop3,1444861524,"I was wondering if there were any github projects (preferably python) that demonstrate the usage of CNNs to tackle CAPTCHAs.

If not, is there any training data available (CAPTCHA --&gt; text in CAPTCHA) that one could use to train a CNN?

Seems like a pretty interesting/fun CNN application I would love to get into.",3,2
299,2015-10-15,2015,10,15,8,3osfgk,Sparse Filtering in Theano,https://www.reddit.com/r/MachineLearning/comments/3osfgk/sparse_filtering_in_theano/,__lava__,1444865915,,0,4
300,2015-10-15,2015,10,15,12,3ot9q9,Scikit Learn - Malware detected?,https://www.reddit.com/r/MachineLearning/comments/3ot9q9/scikit_learn_malware_detected/,PocketHeyman,1444879971,"Hi All - 

I'm running Windows 8.1 and I'm trying to install Sci-kit learn from [https://pypi.python.org/pypi/scikit-learn/0.16.1#downloads](here) and I'm getting a Trojan detected from Windows defender?  Anyone have any experience with this?  Is this part of the sourceforge disaster?  Let me know if you need any additional information.",12,0
301,2015-10-15,2015,10,15,12,3otabv,Why kaggle winners don't use Torch?,https://www.reddit.com/r/MachineLearning/comments/3otabv/why_kaggle_winners_dont_use_torch/,andrewbarto28,1444880279,,5,1
302,2015-10-15,2015,10,15,15,3otqhf,CNC Plate Punching and Marking Machine for power transmission tower,https://www.reddit.com/r/MachineLearning/comments/3otqhf/cnc_plate_punching_and_marking_machine_for_power/,goodcncmachines,1444889767,,1,0
303,2015-10-15,2015,10,15,16,3otyte,Contradiction in VAE math?,https://www.reddit.com/r/MachineLearning/comments/3otyte/contradiction_in_vae_math/,[deleted],1444895865,"I'm reading [the VAE paper](http://arxiv.org/abs/1312.6114) and I noticed what seems like a contradiction in its math:

If we change the weights in such a way that both the code and the noise get rescaled by the same factor, and at the same time reduce the input weights of the decoder by this factor, we should have an equivalent AE. However, the objective function (the variational lower bound) in Eq. 10 will change because of the first term. 

What would explain this apparent contradiction?

[1] To do this, we can *e.g.* increase **b_5** by 1 and multiply **W_4** and **b_4** by *e^2* in Eq. 12, while also dividing the input weights of the decoder **W_1** by *e^2* in Eq. 11. (*e=2.72*)",8,9
304,2015-10-15,2015,10,15,18,3ou5no,help making a convolutional autoencoder,https://www.reddit.com/r/MachineLearning/comments/3ou5no/help_making_a_convolutional_autoencoder/,stop_ttip,1444901462,"Hi,

I have tweaked around with Lasagne and I found it an excellent library. My experiments revolve around building an autoencoder for the MNIST dataset. Unfortunately I didn't manage to train it properly, maybe you can spot any mistakes I made?

Here is the structure I chose:

 * input layer (28x28)
 * 2D convolutional layer, filter size 7x7
 * Max Pooling layer, size 3x3, stride 2x2
 * Dense (fully connected) flattening layer, 10 units (this is the bottleneck)
 * Dense (fully connected) layer, 121 units
 * Reshaping layer to 11x11
 * 2D convolutional layer, filter size 3x3
 * 2D Upscaling layer factor 2
 * 2D convolutional layer, filter size 3x3
 * 2D Upscaling layer factor 2
 * 2D convolutional layer, filter size 5x5
 * Feature max pooling (from 31x28x28 to 28x28)

All the 2D convolutional layers have the biases untied, sigmoid activations and 31 filters.

All the fully connected layers have sigmoid activations.

The loss function used is squared error, the updating function is adagrad. The length of the chunk for the learning is 100 samples, multiplied for 1000 epochs.

[here is the original post I made on *stats.stackexchange.com* (containing the full source code of the autoencoder)](http://stats.stackexchange.com/questions/176881/cannot-make-this-autoencoder-network-function-properly-with-convolutional-and-m)

Thank you for any hint on what might be wrong! 

UPDATE: The following is an illustration of the problem: the upper row are some samples set as inputs of the network, the lower row is the reconstruction: http://i.imgur.com/8IXAhr4.png

UPDATE: **SOLVED!!**
Here is my implementation: http://nopaste.linux-dev.org/?782799
and here is a visualization of the result with only 2 bottleneck nodes!: http://imgur.com/xJQhZ6f Is there still room for improvement?",14,2
305,2015-10-15,2015,10,15,20,3oue31,Aquaculture Equipment Industry 2014 Market Research Report,https://www.reddit.com/r/MachineLearning/comments/3oue31/aquaculture_equipment_industry_2014_market/,RachelElla,1444907613,,0,0
306,2015-10-15,2015,10,15,21,3ouj71,AskReddit: Is there any naming convention for Theano's symbolic variables?,https://www.reddit.com/r/MachineLearning/comments/3ouj71/askreddit_is_there_any_naming_convention_for/,AlfonzoKaizerKok,1444910895,"For instance, in CUDA, you could have:

    float* host_x;
    float* dev_x;

In Theano, it could be confusing in the following:

    x = T.float()
    f = theano.function([x], x*x)
    x = 10.0
    print f(x)
    
Would it be acceptable to use, say, a trailing (or leading) underscore for symbolic variables?

    x_ = T.float()
    f = theano.function([x_], x_*x_)
    x = 10.0
    print f(x)


Or would that be unpythonic? Any advice would be very much appreciated!
",7,1
307,2015-10-15,2015,10,15,23,3ouxjh,The Latest Medical Breakthrough In Spinal Cord Injuries Was Made By A Computer Program,https://www.reddit.com/r/MachineLearning/comments/3ouxjh/the_latest_medical_breakthrough_in_spinal_cord/,cavedave,1444918266,,16,91
308,2015-10-15,2015,10,15,23,3ouysk,Learning a users daily routine and recurring events: Where to start?,https://www.reddit.com/r/MachineLearning/comments/3ouysk/learning_a_users_daily_routine_and_recurring/,_hendra,1444918819,"I'm computer science student, but completely new to machine learning.

I have an app where users can enter when they get up, eat, dring alcohol, doing sports and go to bed. Time and location gets tracked. I would like to detect a pattern in their daily routine, so that I'm able to predict e.g. the expected lunch time for a specific week day (if the user follows a recognizable routine).

I have two questions:
1) Where should I start my research? Keywords, topics, techniques?
2) Do you think it's possible to run that kind of machine learning on the users smartphone?

Thanks for your advice!",6,1
309,2015-10-16,2015,10,16,0,3ov7iu,imagenet public solutions,https://www.reddit.com/r/MachineLearning/comments/3ov7iu/imagenet_public_solutions/,michal_sustr,1444922496,Are there publicly available solutions of trained networks from imagenet for download?,1,1
310,2015-10-16,2015,10,16,1,3ovdkl,Training an accurate probability of an event,https://www.reddit.com/r/MachineLearning/comments/3ovdkl/training_an_accurate_probability_of_an_event/,roundhouse27,1444924870,"I have a ton of data, some structured some unstructured, for each sample, and the outcome I have is a binary outcome - success or failure. 

I want a model that doesn't just classify but will give me a probability of success.

What class of algorithms should I be looking at? I imagine this means basically that I need to minimize some kind of cost function - for my purposes it's not super critical which norm if some are better suited for a given algorithm. ",3,0
311,2015-10-16,2015,10,16,2,3ovuv8,Effect of numerical errors when using Data parallelism - How to go about calculating it (in the context of DNN)?,https://www.reddit.com/r/MachineLearning/comments/3ovuv8/effect_of_numerical_errors_when_using_data/,ImgPrcSng,1444931857,"With Multi GPU solutions being openly available, I tried them on the MNIST dataset. I took a batch_size of 64 on a single GPU and ran it for 'n' epochs and I took a batch_size of 16 on each of 4 GPU and ran it for 'n' epochs ( with the same random seed ). But the results from both seems to diverge from the first iteration. Is it because of numerical precision or am I missing something? If it is because of precision, how do we calculate its effect on each iteration and how to keep it under certain epsilon? ( I will post the numbers I got from the experiments soon )",2,1
312,2015-10-16,2015,10,16,3,3ovvzz,Generating rudimentary Mind-Maps from Word2Vec models,https://www.reddit.com/r/MachineLearning/comments/3ovvzz/generating_rudimentary_mindmaps_from_word2vec/,sachinrjoglekar,1444932290,,2,11
313,2015-10-16,2015,10,16,4,3owddo,Sequence Classification with RNNs,https://www.reddit.com/r/MachineLearning/comments/3owddo/sequence_classification_with_rnns/,fariax,1444939112,"Hello!

I'm trying to classify sequences of EEGs data.

The EEG data is (number_samples, sequence_duration, number_channels)=(45,657,64). I only have 45 samples to train, beign 23 from clas 0 and 22 from class 1, each sample has duration of 657 points with 64 different channels, the channels data are real values.

I'm trying to train a keras LSTM but I couldn't figure out the parameters to converge, I'm afraid that the number of data to train is too small.

I have a code running with an Echo State Network doing this task, but I would like to compare against a LSTM

This the piece of code that I wrote for the LSTM on keras

    self.model.add(LSTM(input_dim=self.input_dim, input_length=self.input_length,output_dim=self.n_hidden,
                                init=lambda shape: uniform(shape, scale),
                                inner_activation='tanh', return_sequences=True))
    self.model.add(Dropout(0.5))
    self.model.add(Dense(output_dim=self.output_dim, init=lambda shape: uniform(shape, scale), activation='softmax'))

    optimizer = 'rmsprop'
    loss = 'binary_crossentropy'

    self.model.compile(optimizer=optimizer, loss=loss)#, class_mode='binary')

    #X_train(45,657,5)      y_train(45,2)
    self.model.fit(X_train, y_train)
    # The ground truth is something like [[1,0], [0,1], [0,1], [1,0], [1,0]]
    # The predictions are being almost constant like [[0.519,0,491], [0.5171,0,4921], [0.5182,0,494], [0.51834,0,49342], [0.51775,0,4935]]
    predictions = self.model.predict(X_test)  

Does anyone knows if i'm doing something wrong?
Any other ideas that I could apply to this problem?

Thank you!",8,3
314,2015-10-16,2015,10,16,6,3owulm,Jupyter Project and The Future of IPython,https://www.reddit.com/r/MachineLearning/comments/3owulm/jupyter_project_and_the_future_of_ipython/,shugert,1444945977,,0,1
315,2015-10-16,2015,10,16,13,3oy93u,Need suggestions for project in Pattern Recognition course,https://www.reddit.com/r/MachineLearning/comments/3oy93u/need_suggestions_for_project_in_pattern/,vicky210,1444970025,"Hello,
I am a graduate student. This semester I have Pattern Recognition course and have to do a project as part of the course. Looking for suggestions for a good project idea in which I could use unsupervised learning algorithms. I have a basic knowledge in popular machine learning algorithms and have experience in working on projects using supervised learning . Although this would be my first proper project in unsupervised learning ,I'm looking for moderate- hard problems. Would like the data to be easily available or not very tough to scrap. ",5,3
316,2015-10-16,2015,10,16,14,3oyfrv,Multiple Hypothesis Testing: thoughtful &amp; interactive,https://www.reddit.com/r/MachineLearning/comments/3oyfrv/multiple_hypothesis_testing_thoughtful_interactive/,cast42,1444974261,,6,5
317,2015-10-16,2015,10,16,18,3oyxx7,Pipes - Artificially Intelligent News App. Thoughts?,https://www.reddit.com/r/MachineLearning/comments/3oyxx7/pipes_artificially_intelligent_news_app_thoughts/,sg_16,1444987901,,0,0
318,2015-10-16,2015,10,16,21,3ozckp,Difference between sharing and not sharing weights between the encoder and decoder layers of an Autoencoder.,https://www.reddit.com/r/MachineLearning/comments/3ozckp/difference_between_sharing_and_not_sharing/,kmul00,1444998040,"I have a convolutional autoencoder with a pooling layer, as given in [this] (http://people.idsia.ch/~ciresan/data/icann2011.pdf) paper. 
That is, my encoder layer is :

    input -&gt; convolution_1 -&gt; tanh _activation -&gt; max_pool -&gt; encoded_representation

The decoder layer is as follows :

    encoded_representation -&gt; reverse_max_pool -&gt; convolution_2 -&gt; output

My question is, what are the advantages (or disadvantages) of **weight sharing** between *convolution_1* and *convolution_2* ?

Currently I have them as two different convolution layers, with **no weight sharing**. 
Is it possible that with this approach, maybe *convolution_2* is better trained, and hence my reconstruction error is low, but in fact my *encoded representation* is not that good ?
And if yes, will sharing the weights mitigate this problem ?",9,4
319,2015-10-16,2015,10,16,21,3ozg99,Learning Stats and Probability with #python notebooks,https://www.reddit.com/r/MachineLearning/comments/3ozg99/learning_stats_and_probability_with_python/,cast42,1444999877,,3,53
320,2015-10-16,2015,10,16,22,3ozo79,Survey of loss functions in Machine learning?,https://www.reddit.com/r/MachineLearning/comments/3ozo79/survey_of_loss_functions_in_machine_learning/,mr_robot_elliot,1445003916,"Does anyone know of a survey paper where all the loss functions with their properties like Lipschitz Continuity, Convexity, Strong Convexity etc are specified?",7,2
321,2015-10-16,2015,10,16,23,3oztvk,Why 50% when using dropout?,https://www.reddit.com/r/MachineLearning/comments/3oztvk/why_50_when_using_dropout/,deepfrolf,1445006548,"I am fairly new in my machine learning education, so I haven't really had the chance to experiment myself, nor have I come across the explanation in my readings or lectures for why 50% seems to be the optimal amount for dropout or why that amount is generally used.

Per my understanding as to why dropout is successful in learning these networks, I would have guessed that differing amounts of dropout would be optimal for different applications.

Are there any examples of different networks using a range of say 20-80% dropout in 10% steps and plotting the accuracy of the network at these different intervals? I would be curious how such a curve would look across different types of applications.

*edit* also I am familiar with the small section in the original dropout paper by Hinton (https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf ~page 17), but was hoping for something more comprehensive. ",9,12
322,2015-10-16,2015,10,16,23,3ozvts,This Crazy Clickbait Generator Will SHOCK You ----&gt;,https://www.reddit.com/r/MachineLearning/comments/3ozvts/this_crazy_clickbait_generator_will_shock_you/,dabshitty,1445007417,,0,0
323,2015-10-17,2015,10,17,0,3p04m2,simple well drilling machine,https://www.reddit.com/r/MachineLearning/comments/3p04m2/simple_well_drilling_machine/,falel_jaya,1445010947,,0,1
324,2015-10-17,2015,10,17,1,3p06s6,The future of AI is on the cloud,https://www.reddit.com/r/MachineLearning/comments/3p06s6/the_future_of_ai_is_on_the_cloud/,wildcodegowrong,1445011811,,1,0
325,2015-10-17,2015,10,17,1,3p09qt,"Hi ML, my startup just released our first fashion search experiment, thoughts? (Kip works best in NYC for now)",https://www.reddit.com/r/MachineLearning/comments/3p09qt/hi_ml_my_startup_just_released_our_first_fashion/,[deleted],1445012968,[deleted],0,0
326,2015-10-17,2015,10,17,1,3p0bc0,Introduction to Machine Learning with Web Analytics: Random Forests and K-Means,https://www.reddit.com/r/MachineLearning/comments/3p0bc0/introduction_to_machine_learning_with_web/,fhoffa,1445013581,,0,1
327,2015-10-17,2015,10,17,1,3p0e6s,Understanding Machine Learning in the Cloud,https://www.reddit.com/r/MachineLearning/comments/3p0e6s/understanding_machine_learning_in_the_cloud/,[deleted],1445014793,[deleted],0,0
328,2015-10-17,2015,10,17,2,3p0fbo,I'm CEO of Ayasdi - we develop software for Machine Intelligence &amp; Advanced Analytics. Ask me Anything!,https://www.reddit.com/r/MachineLearning/comments/3p0fbo/im_ceo_of_ayasdi_we_develop_software_for_machine/,singhgurjeet,1445015245,"I am the CEO and Co-Founder of Ayasdi, a machine intelligence &amp; advanced analytics company. Ask Me Anything.

Ayasdi is a machine intelligence company that began at Stanfords
mathematics department. We were initially funded by DARPA based
on our core technology - Topological Data Analysis, to find
subtle patterns in complex data. Subsequently funded by Floodgate,
Khosla Ventures, Institutional Venture Partners and Kleiner Perkins we build solutions for F500 companies across healthcare, financial services and the government.

We have been in the news recently based on the ability of our software to generate breakthroughs in dark data - data that was often considered to be useless but actually holds tremendous value. Media article - http://www.fastcoexist.com/3052282/the-latest-medical-breakthrough-in-spinal-cord-injuries-was-made-by-a-computer-program. Paper - http://www.nature.com/ncomms/2015/151014/ncomms9581/full/ncomms9581.html

Personally, I came out of academia, started Ayasdi, raised ~100M from VCs, built a ~100 strong team, actively help academic collaborators and have closed business with many F1000 companies.

AMA!
",73,88
329,2015-10-17,2015,10,17,4,3p0xtp,System that replaces human intuition with algorithms outperforms human teams,https://www.reddit.com/r/MachineLearning/comments/3p0xtp/system_that_replaces_human_intuition_with/,Youre_Cool,1445022618,,0,0
330,2015-10-17,2015,10,17,5,3p1cbv,Simple recurrent neural network with/without LSTM Tutorial ?,https://www.reddit.com/r/MachineLearning/comments/3p1cbv/simple_recurrent_neural_network_withwithout_lstm/,rishok,1445028512,Does somebody know a Simple recurrent neural network with/without LSTM Tutorial in python (and theano))?,2,4
331,2015-10-17,2015,10,17,6,3p1k7t,[1510.02693] Feedforward Sequential Memory Neural Networks without Recurrent Feedback,https://www.reddit.com/r/MachineLearning/comments/3p1k7t/151002693_feedforward_sequential_memory_neural/,[deleted],1445032005,,7,13
332,2015-10-17,2015,10,17,7,3p1nap,[1510.03009] Neural Networks with Few Multiplications (no more GPU dominance??),https://www.reddit.com/r/MachineLearning/comments/3p1nap/151003009_neural_networks_with_few/,[deleted],1445033373,,41,40
333,2015-10-17,2015,10,17,7,3p1q8r,Clustering data with autoencoder,https://www.reddit.com/r/MachineLearning/comments/3p1q8r/clustering_data_with_autoencoder/,pumpkin105,1445034783,"Hi,

I have received a bunch of documents from a company and need to cluster and classify them. The document are bag-of-words vectors containing around 5000 words.

Classifying with this dataset is no problem, I am getting very good results training a plain feedforward network. But I want to show that it is also possible to cluster and visualize data with unsupervised training. I used the autoencoder on the [theano website](http://deeplearning.net/tutorial/dA.html), reduced dimensionality and visualized the data using PCA.
Without any training, the raw data looks like [this](http://imgur.com/dNWy4Mx). After pretraining the first layer, the data looks like [this](http://imgur.com/mROAATo). As you can see, the data is hardly clustered. When I train the network with labels and then output the values of the second last layers and visualize that, it looks like [this](http://imgur.com/9a3CMi5). Here you see that the data has been split way better. In the middle, the data lies on a big pile, but that may be the visualization in two dimensions.

What do I have to keep in mind when training a network to cluster data and get good visualizations? Has anyone experience with that?

Thank you",2,2
334,2015-10-17,2015,10,17,8,3p1v7r,How useful are Neural Networks at actually finding the relevant factors of a pattern?,https://www.reddit.com/r/MachineLearning/comments/3p1v7r/how_useful_are_neural_networks_at_actually/,BeezLionmane,1445037076,"For example, let's say you have a Neural Network with 30 inputs, and only two of them are relevant to the pattern you're looking for.  Would the NN be able to find the relevant inputs and effectively ignore the rest of the inputs, or is it more likely to come up with some complicated grouping of its own that only seems to fit?",2,0
335,2015-10-17,2015,10,17,12,3p2owr,"Is there a machine learning process that summarizes multiple entries to create a ""prototypical"" entry?",https://www.reddit.com/r/MachineLearning/comments/3p2owr/is_there_a_machine_learning_process_that/,Data_Driven_Dude,1445053274,"There are lots of summarizing algorithms out there (e.g., http://smmry.com/) that summarize an article, story, essay, etc. Basically, all the ""important"" parts are distilled into a condensed format. Lots of bots like this are floating around reddit. 

I'm curious about a similar kind of ML process, but instead of summarizing one entry (e.g., article, essay), it creates a prototypical or ""averaged"" entry based on multiple entries. For example, taking thousands of tweets about X and creating one or a few tweets that basically represent thousands. Is there a process or system of algorithms out there like this? Any assistance would be much appreciated!",4,0
336,2015-10-17,2015,10,17,14,3p2zdl,Is it possible?,https://www.reddit.com/r/MachineLearning/comments/3p2zdl/is_it_possible/,[deleted],1445060096,"I really like the topic on machine learning. I'm interested in making a facial recognition system,but I'm only a high school senior. 
I need to know the basics before I can even begin this.
The only language I have experience in is C++ as well.
Is it possible for a layman ,like myself, to start to delve into this topic? If so, how would I approach it?",8,0
337,2015-10-17,2015,10,17,16,3p35yk,"Uniform Learning in a Deep Neural Network via ""Oddball"" Stochastic Gradient Descent (2015) - [YouTube video of a DNN encoding a video]",https://www.reddit.com/r/MachineLearning/comments/3p35yk/uniform_learning_in_a_deep_neural_network_via/,ajrs,1445065257,,0,2
338,2015-10-17,2015,10,17,18,3p3eoj,[Question] What makes some operations differentiable and others not?,https://www.reddit.com/r/MachineLearning/comments/3p3eoj/question_what_makes_some_operations/,[deleted],1445073208,[deleted],7,1
339,2015-10-17,2015,10,17,20,3p3q7n,Learning practical Data Science (the one that will pay my bills)?,https://www.reddit.com/r/MachineLearning/comments/3p3q7n/learning_practical_data_science_the_one_that_will/,__AndrewB__,1445082935,"Hey, 

So let's say I'm interested in ML (mostly algorithms) and I'd like to get a job in the field. I live in EU and probably will never have a PhD, so let's say we're not talking about research positions.

As far as I understand, though, most positions will not only require strong skills in the algorithmic part of ML, but also broader knowledge of ""data science"", like getting and cleaning data, exploratory analysis, descriptive anlysis, feature engineering etc.

I guess most job interviews will **not** be about **""OK, so how do we formulate learning objective for SVM?""**, but more about **""OK, here's a dataset, what do you do with it""**. 

* **Q1: how do learn such practical skills.**
* **Q2: what books can You guys recommend?**
* **Q3: how do I prepare for a job interview as a data scientist / machine learning guy.** There are tons of web-pages for coders to practice their algorithmic skills. Is there any such thing for ML / DS (except Kaggle).

Note: 

* I hate MOOCs. They're slow paced, take a lot of time, are mostly targeted at high-school level audience (ML without calculus...?) and none of the free ones uses Python. 
* When it comes to alogs, every ML expert should know Murphy's ML:PP (or equivalent). Is there any such text for more general Data Scientist?
* ""Compete on Kaggle"" is not really an answer. I first have to learn that Random Forests exist to use them. I first have to learn data science to use it in practice. Reading winning solutions give me fragmentated knowledge that I can't always understand. 

Best, 
A.",6,1
340,2015-10-17,2015,10,17,22,3p3vrv,v0.1.1 Metric Learning in Python on PyPI now,https://www.reddit.com/r/MachineLearning/comments/3p3vrv/v011_metric_learning_in_python_on_pypi_now/,terrytangyuan,1445086858,,0,5
341,2015-10-17,2015,10,17,22,3p3w85,Unified interface to do ggplot2 for popular packages in R,https://www.reddit.com/r/MachineLearning/comments/3p3w85/unified_interface_to_do_ggplot2_for_popular/,[deleted],1445087146,[deleted],0,1
342,2015-10-17,2015,10,17,22,3p40rl,"Is there a way, in a neural network, to tell which features were most relevant in classifying a specific test sample?",https://www.reddit.com/r/MachineLearning/comments/3p40rl/is_there_a_way_in_a_neural_network_to_tell_which/,PeterIanStaker,1445089915,"I have a network with 2 hidden layers, with ReLU activations, classifying MNIST. lets say I feed-forward a picture of a ""0"" into the network, and it gets correctly classified. Is there a way, since I know the weights and activation-levels for each layer, to work backwards and weigh the features (pixels) in terms of how much they ""contributed"" to the final layer's output?

My first intuition is to look at each feature one at a time. So looking at the top left pixel, feature #1, I can determine how important it is to the layer1 units based on their weight matrices. Then I can determine the relevance of each neuron by the ratio of its activation level to the total activation level for the layer. 

I can do this for each layer. My first thought was, effectively, for each possible path to the output, take the product of

(layer 1 weight)(layer 1 activation)(layer 2 weight)(layer 2 activation)(layer 3 weight)

and then take the sum over all paths, and that seems like it should at least correlate to what I'm after. 

I was wondering, though, if there's a more intelligent way to do this",19,36
343,2015-10-18,2015,10,18,0,3p4aij,"DL Tutorial Part 2: Autoencoders, ConvNets and Recurrent Nets - Quoc Le",https://www.reddit.com/r/MachineLearning/comments/3p4aij/dl_tutorial_part_2_autoencoders_convnets_and/,gwulfs,1445094979,,0,5
344,2015-10-18,2015,10,18,0,3p4ba9,A visual explanation of decision trees and machine learning,https://www.reddit.com/r/MachineLearning/comments/3p4ba9/a_visual_explanation_of_decision_trees_and/,[deleted],1445095350,[deleted],0,0
345,2015-10-18,2015,10,18,3,3p4yjb,Some questions about what constitutes ML/Analytics,https://www.reddit.com/r/MachineLearning/comments/3p4yjb/some_questions_about_what_constitutes_mlanalytics/,minkowskicap,1445105810,"What is the difference between ML and predictive analytics? 

And ML involves thins like image recognition, voice recognition, right? So how much statistics/probability are involved in those? And linear algebra? 

What about doing predictions on datasets, like netflix movie challenge for example. Where does that fall? Becuase I think of it as being predictive analytics, but I'm sure it's also ML right? Which  is a sub-branch of the other? Is all predictive analytics ML? Or vice versa? 

And where does linear algebra fit into the picture? Is it used equally in all types of ML? ",5,1
346,2015-10-18,2015,10,18,4,3p56hv,C# - LSTM &amp; GRU Library,https://www.reddit.com/r/MachineLearning/comments/3p56hv/c_lstm_gru_library/,afry316,1445109447,"In my process of teaching myself ML I implemented a C# version of Andrej Karpathy's RecurrentJs and Thomas Lahore's RecurrentJava.

https://github.com/andrewfry/SharpML-Recurrent

",3,4
347,2015-10-18,2015,10,18,4,3p587l,Nvidia &amp; Facebook Join Deep Learning Workshops Speaker List,https://www.reddit.com/r/MachineLearning/comments/3p587l/nvidia_facebook_join_deep_learning_workshops/,arshakn,1445110216,,1,12
348,2015-10-18,2015,10,18,4,3p593b,What types of problems are extremely well suited for machine learning?,https://www.reddit.com/r/MachineLearning/comments/3p593b/what_types_of_problems_are_extremely_well_suited/,theVAguy,1445110610,":)

Or, perhaps better put, how can one look at a problem and tell whether machine learning is likely going to offer substantial improvements?

For example -- working out which results are relevant in a Google results page. It makes sense that machine learning might be well used to work out which results were useful based on how users clicked and interacted with the pages. 

Other questions:

- how much data is perhaps needed?",2,3
349,2015-10-18,2015,10,18,5,3p5dqo,What is the value in getting a Masters in Predictive Analytics for getting an industry job in ML or data science in general?,https://www.reddit.com/r/MachineLearning/comments/3p5dqo/what_is_the_value_in_getting_a_masters_in/,[deleted],1445112763,[deleted],0,1
350,2015-10-18,2015,10,18,6,3p5o7v,Retaining More Meaning in Topic Modeling?,https://www.reddit.com/r/MachineLearning/comments/3p5o7v/retaining_more_meaning_in_topic_modeling/,blowjobtransistor,1445117629,"Hey r/MachineLearning - I've been working with topic modeling for the last couple months, and while I am getting useful results out of models like LDA, I'm having trouble making those results easily interpretable to other humans - to help them find actionable insights from them.

I'm looking at app review text, trying to find emergent problems or issues.  For instance, did many more people start complaining about X in the last month?  Given the space of potential problems in apps, it's impossible to write rules to detect every X, making these kinds of models attractive.  The problem I'm experiencing is that results always take lots of study to yield valuable information, due to the destructuring (via bag of words) of reviews.  Using N-grams make the space too sparse, even after extensive preprocessing.  

Is there something I should look at that doesn't rely on the BOW assumption in topic modeling that would produce more immediately interpretable results?  Thanks!

",4,3
351,2015-10-18,2015,10,18,10,3p6jma,Looking for more exposure to research in Machine Learning. Please help.,https://www.reddit.com/r/MachineLearning/comments/3p6jma/looking_for_more_exposure_to_research_in_machine/,karan_42,1445133353,"I am a second year CS undergraduate from one of the top colleges in India. I got introduced to Machine Learning recently and feel strongly that I might pursue research in this field. To get a head start, I wanted to experience research and the only viable method seems to pursue a summer research internship in Machine Learning.

Due to limited scope of research in my country, I wanted to get international exposure in some university, research center or lab where I can do meaningful research. Even though I am sending emails to professors and waiting for responses, I was thinking I should ask Reddit once.

Can you let me know of any university or lab that accepts research interns in this field? Or any companies with research level work? Any input will be a great help.

*I have done many relevant courses and projects and will send a formal letter with my CV if required.

Thanks in advance.",2,2
352,2015-10-18,2015,10,18,14,3p72qe,L1-regularized Neural Networks are Improperly Learnable in Polynomial Time,https://www.reddit.com/r/MachineLearning/comments/3p72qe/l1regularized_neural_networks_are_improperly/,iidealized,1445144487,,3,19
353,2015-10-18,2015,10,18,15,3p79fo,Analyzing Pronto CycleShare Data with Python and Pandas,https://www.reddit.com/r/MachineLearning/comments/3p79fo/analyzing_pronto_cycleshare_data_with_python_and/,cast42,1445149298,,0,9
354,2015-10-18,2015,10,18,17,3p7ilz,Improving YouTube video thumbnails with deep neural nets,https://www.reddit.com/r/MachineLearning/comments/3p7ilz/improving_youtube_video_thumbnails_with_deep/,rndnum123,1445157541,,22,101
355,2015-10-18,2015,10,18,19,3p7q1s,Hinton said Google uses dAE's extensively. What for?,https://www.reddit.com/r/MachineLearning/comments/3p7q1s/hinton_said_google_uses_daes_extensively_what_for/,[deleted],1445164608,,8,4
356,2015-10-18,2015,10,18,20,3p7tns,Handling Infrequent Words in Corpus with Word2Vec,https://www.reddit.com/r/MachineLearning/comments/3p7tns/handling_infrequent_words_in_corpus_with_word2vec/,LeavesBreathe,1445167788,"Hey Guys,

I've been using Word2Vec to vectorize words. Based upon these vectorizations, I apply a K-means to cluster the words, and I've been absolutely blown away by the results. It groups them very well: better than I ever could...and all in 10 minutes. 

Here's an example of one cluster of words from 33mb doc on astronomy:

	[265:'stars','fusion','fission','H','speed_of_light','Hydrogen','atoms','hydrogen','energy']

If anyone else is interested in doing this, there's a nice guide here: http://www.yseam.com/blog/WV.html

However, there is a huge problem I have with this approach. Word2Vec only works well on words that appear at least ~5 times within your corpus. From Zipf's law, we know that there are going to be *many* words that don't even come close to reaching this number. 

I feel that those who are involved with translation would also have this problem: What do you do with all of these infrequent words? What happens when you're training your neural net and you come across one of these words?

Ideas I've come across:

1. Make another cluster, and just throw all of these infrequent words into that cluster. (Plausible, but you still lose all meaning of the word)

2. Simply delete the words, and have the neural net never know that there was that word (I really don't like this idea, because it throws off the grammar/sentiment properties of your net)

3. Just Vectorize the words with a word count of 1 (leads to all your clusters being contaminated by words that simply don't belong.)

4. Increase Your Training Corpus Size by Orders of Magnitude, and delete infrequent words (I can probably get about 1gb of text at the most, but with more text will come more infrequent words!!!)

These just don't seem like the best approaches. Is there a better idea to group these infrequent words? Ram and Gpu Memory is not a concern at all. ",9,17
357,2015-10-19,2015,10,19,0,3p8h9o,Machine Learning Will Transform Business: How to Benefit,https://www.reddit.com/r/MachineLearning/comments/3p8h9o/machine_learning_will_transform_business_how_to/,Nuno_EdgarFernandes,1445181971,,0,0
358,2015-10-19,2015,10,19,0,3p8kei,Optimizing a Box of Crayons with Hierarchical Clustering,https://www.reddit.com/r/MachineLearning/comments/3p8kei/optimizing_a_box_of_crayons_with_hierarchical/,theMadcap,1445183429,,4,4
359,2015-10-19,2015,10,19,1,3p8m80,A quick tour of Torch internals,https://www.reddit.com/r/MachineLearning/comments/3p8m80/a_quick_tour_of_torch_internals/,[deleted],1445184270,[deleted],0,2
360,2015-10-19,2015,10,19,3,3p99je,CTC vs HMM-DNN for Speech Processing,https://www.reddit.com/r/MachineLearning/comments/3p99je/ctc_vs_hmmdnn_for_speech_processing/,enk100,1445194362,"I saw the speech processing lecture from the cs224d stanford course.  
Andrew Maas say in that lecture - ""HMM-DNN systems are now the default, state of the art for speech recognition"", whereas i saw the google research blog which say that their systems now running the CTC models - http://googleresearch.blogspot.co.il/2015/09/google-voice-search-faster-and-more.html 

Which model get the better results CTC or HMM-DNN?
maybe google has tons of non-aligned data that the CTC model get the best results compare to the HMM-DNN that need aligned data for training?
",14,3
361,2015-10-19,2015,10,19,5,3p9oqa,Using the Kernel Trick on a logistic regression with gradient descent,https://www.reddit.com/r/MachineLearning/comments/3p9oqa/using_the_kernel_trick_on_a_logistic_regression/,ProjectAmmeh,1445200713,"Hey all,

I'm trying to make a logistic regression model in Matlab, and apply the kernel trick to it to allow it to learn on non-linearaly separable data. I've got the logistic regression bit working with a gradient descent algorithm and have tested it on a few different data sets - it works exactly as I'd expect. My issue is with the kernel part.

I think the problem is that I don't truly understand what the kernel does from a programming perspective. 

What I think it does is this: Using the data set, generate a column vector which, when multiplied with a row of data, results in a scalar value which is in turn multiplied by the vector which describes the change in the weightings (dE/dw). This new value of dE/dw is then applied to the weights, and a new iteration starts. 

Is this correct? Am I just implementing it wrong? Or have I got the whole thing backwards?",14,2
362,2015-10-19,2015,10,19,7,3pa297,Calendar,https://www.reddit.com/r/MachineLearning/comments/3pa297/calendar/,[deleted],1445206610,[deleted],0,0
363,2015-10-19,2015,10,19,8,3pa9t8,An intuitive breakdown of LDA (topic modeling),https://www.reddit.com/r/MachineLearning/comments/3pa9t8/an_intuitive_breakdown_of_lda_topic_modeling/,Jxieeducation,1445209969,,0,1
364,2015-10-19,2015,10,19,8,3paf14,Mapping Press Releases in the 2015 Canadian Federal Election with word2vec and t-SNE,https://www.reddit.com/r/MachineLearning/comments/3paf14/mapping_press_releases_in_the_2015_canadian/,ddcarnage,1445212429,,0,9
365,2015-10-19,2015,10,19,9,3paje2,I'm using a Naive Bayes classifier to deal with realty data and I think I'm on the wrong tracks,https://www.reddit.com/r/MachineLearning/comments/3paje2/im_using_a_naive_bayes_classifier_to_deal_with/,TiuTalk,1445214482,"I'm trying to use Naive Bayes to classify realty oportunities into ""good"" or ""bad"" deals.

I scrapped/crawled a realty website that has around 2k~ places, deleted some broken records and created my own data set that consists of:

* state
* city
* neighbourhood
* kind (house/apartment)
* price (tried to group values into ""ok"", ""cheap"" and ""expensive"")
* rooms
* area (grouped values into ""small"", ""ok"" and ""large""
* **label** (good/bad) &lt;-- This is the value I'll try to find later, the one I'm using to check my algorithm accuracy

The problem I'm seeing ahead is that some of these values are not discrete (price, rooms, area) and these are the most important values for the classification.

The other values like state/city are not really important, because all my dataset is on the same location, so they're all equal.

Other problem I'm having is that I have too few ""good"" labeled training oportunities (because what I'm looking for is really specific in terms of rent value range and number of rooms)... more than 90% are bad oportunities (too expensive).

And another problem with NB is that it cannot handle unkown values... if I my dataset has only places with 1-4 rooms, and I throw a place with 5 rooms at it, it simply cannot handle it.. even though it would be awesome to find a 5 room place with the same rent range.

I'm starting to wonder that **maybe NB is not the correct algorithm to use on this kind of problem**... I started to look into some regression classification algorithms but I wanted to hear from you guys first.

You might ask **WHY i'm trying to use machine learning** in this classification... Well, my idea is that there are too many variables to handle each one of them with simple if/elses.. maybe there is a good opportunity with a rent 10-20% more expensive, that would be filtered out if I did a simple `if &lt; x`.. But this is exactly what's happening now that I need to group the values into labels to use NB.
",13,4
366,2015-10-19,2015,10,19,9,3pak4b,Continuous classification of unsegmented time series,https://www.reddit.com/r/MachineLearning/comments/3pak4b/continuous_classification_of_unsegmented_time/,Lylax,1445214843,"I'm relatively new to ML (have only done Andrew Ng's course).
I have data from optical brain imaging of the motor cortex with 20 channels. The output is binary- which direction the subject moves their arm.

The end goal is to develop a means of continuously receiving data and predicting the *current* direction. Right now I'm considering a probabilistic model based on the last n data points, or an RNN, but I'm not sure how the RNN would be trained. I looked into CTC, but it seems it's not suited for outputting the current state, rather than discrete chunks, but I'm probably wrong. Thoughts?",1,1
367,2015-10-19,2015,10,19,12,3pb5ek,Multi-GPU/Distributed Deep Learning toolkit in R,https://www.reddit.com/r/MachineLearning/comments/3pb5ek/multigpudistributed_deep_learning_toolkit_in_r/,antinucleon,1445225228,,2,43
368,2015-10-19,2015,10,19,15,3pbomu,Deep Learning Startups,https://www.reddit.com/r/MachineLearning/comments/3pbomu/deep_learning_startups/,john_philip,1445237140,,0,0
369,2015-10-19,2015,10,19,16,3pbpwt,What exactly is Big Data?,https://www.reddit.com/r/MachineLearning/comments/3pbpwt/what_exactly_is_big_data/,Dawny33,1445238055,,0,0
370,2015-10-19,2015,10,19,16,3pbrvg,"Deep Learning Startups, Applications and Acquisitions  A Summary",https://www.reddit.com/r/MachineLearning/comments/3pbrvg/deep_learning_startups_applications_and/,muktabh,1445239498,,0,1
371,2015-10-19,2015,10,19,16,3pbrzo,"[Question] If a dataset has multiple columns all in different formats, what would be the best approach to deal with such data?",https://www.reddit.com/r/MachineLearning/comments/3pbrzo/question_if_a_dataset_has_multiple_columns_all_in/,chaoism,1445239581,"Say, a dataset has columns like length and width which can be float, and it can also have some binary elements (yes/no) or discrete numbers (categories transformed into numbers). What would it be wise to simply use all these as features without having to worry about the formats (or more like the nature of the features)? When doing normalization, can we just normalize the discrete numbers the same way as continuous numbers? I'm really confused on dealing with multiple formats.....

EDIT: instead of just downvoting me, can you provide some insight or what you think about this? It might seem like a trivial question to you, but it could help the others greatly.",4,4
372,2015-10-19,2015,10,19,17,3pbv1g,Machine Learning Daily,https://www.reddit.com/r/MachineLearning/comments/3pbv1g/machine_learning_daily/,GaryThomson10,1445242028,,0,0
373,2015-10-19,2015,10,19,18,3pbzhl,"Reinforcement Learning (Part 1), n-armed bandit problems",https://www.reddit.com/r/MachineLearning/comments/3pbzhl/reinforcement_learning_part_1_narmed_bandit/,outlacedev,1445245635,,0,7
374,2015-10-19,2015,10,19,19,3pc4a3,Theoretical Motivations for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3pc4a3/theoretical_motivations_for_deep_learning/,rinuboney,1445249421,,4,35
375,2015-10-19,2015,10,19,19,3pc4uk,How Amazon re-invented Data Science at Amazon AWS re:Invent 2015?,https://www.reddit.com/r/MachineLearning/comments/3pc4uk/how_amazon_reinvented_data_science_at_amazon_aws/,john_philip,1445249876,,0,0
376,2015-10-19,2015,10,19,21,3pcge9,Machine learning enters the SEO world,https://www.reddit.com/r/MachineLearning/comments/3pcge9/machine_learning_enters_the_seo_world/,john_philip,1445257744,,0,1
377,2015-10-19,2015,10,19,21,3pcgzl,How the HDBSCAN clustering algorithm works,https://www.reddit.com/r/MachineLearning/comments/3pcgzl/how_the_hdbscan_clustering_algorithm_works/,lmcinnes,1445258047,,1,3
378,2015-10-19,2015,10,19,23,3pcycf,A quick tour of Torch internals,https://www.reddit.com/r/MachineLearning/comments/3pcycf/a_quick_tour_of_torch_internals/,wesolyromek,1445266380,,0,14
379,2015-10-19,2015,10,19,23,3pcywj,"A Data Science Landscape, One Year After",https://www.reddit.com/r/MachineLearning/comments/3pcywj/a_data_science_landscape_one_year_after/,cast42,1445266635,,1,3
380,2015-10-20,2015,10,20,0,3pd031,I'm trying to identify the most influential factors in a Bayesian sales forecasting application. Not sure how to start on it,https://www.reddit.com/r/MachineLearning/comments/3pd031/im_trying_to_identify_the_most_influential/,AudioManiac,1445267123,"I've just started the 2nd year of my Masters in Software Design &amp; Development. This year I have to write my thesis, and was lucky enough to get an internship with a company who will pay me to do it. So the area is Machine Learning &amp; Data Mining, and essentially this company has a sales forecasting application that uses Bayesian classifiers to help predict whether a sale will win or lose.

My job (or at least part of it) is to try and establish which attributes in the model are the most influencing factors to a sale. So for example, if the model predicts that this on going sale is at the risk of a loss, I have to find which factors are providing the most influence on the prediction.

I'm in the research stage at the moment, just looking at past research and an overall view of sales forecasting and bayesian classifiers, but I'm fairly stumped when it comes down to how I'll try and identify the most influential ones.

If anyone can offer any advice, good papers/journals to look up, I'd really appreciate it. I haven't started work for the company yet, but I want to go in there with at least a idea of how to approach the problem.",4,3
381,2015-10-20,2015,10,20,0,3pd09q,Deep Feature Synthesis: Towards Automating Data Science Endeavors,https://www.reddit.com/r/MachineLearning/comments/3pd09q/deep_feature_synthesis_towards_automating_data/,reidhoch,1445267196,,2,6
382,2015-10-20,2015,10,20,1,3pdc3q,How I Became a Data Scientist,https://www.reddit.com/r/MachineLearning/comments/3pdc3q/how_i_became_a_data_scientist/,dnabeyta,1445272018,,2,0
383,2015-10-20,2015,10,20,1,3pddac,pomegranate: probabilistic modelling in python,https://www.reddit.com/r/MachineLearning/comments/3pddac/pomegranate_probabilistic_modelling_in_python/,ants_rock,1445272510,"A new version of [pomegranate](https://github.com/jmschrei/pomegranate) is out, with a large number of new features! The biggest feature is that the GIL has been released for distributions and hidden Markov models, improving their speed on all tasks significantly. Viterbi is usually 10x faster now, and Baum-Welch training can be 4x faster. Given that the GIL has been released, multithreading training is now an option, though it only works well for large models.

* Discrete Bayesian networks and Factor Graphs have been added
* Bayesian networks can now impute missing data from tables easily
* Conditional and Joint Probability Tables have been added
* Finite state machines have been simplified
* General Mixture Models have been added
* scikit-learn like interface, where models can be `fit` and `predict` or `predict_proba` can be called on new points.
* MultivariateGaussians and Kernel Densities have been significantly sped up, with MultivariateGaussians training and prediction being an order of magnitude faster in some cases
* JSON serialization now works for all distributions and models (except Bayesian networks) 

Most importantly, a [series of tutorials](https://github.com/jmschrei/pomegranate/tree/master/tutorials) have been added as iPython notebooks so that you can see how to use these methods in the wild.

Please let me know if you have any questions, concerns, or feedback!",3,91
384,2015-10-20,2015,10,20,1,3pdgvo,Facial Landmark Detection : A Tutorial,https://www.reddit.com/r/MachineLearning/comments/3pdgvo/facial_landmark_detection_a_tutorial/,spmallick,1445273911,,4,16
385,2015-10-20,2015,10,20,2,3pdksx,Intuition behind Covariance Kernels,https://www.reddit.com/r/MachineLearning/comments/3pdksx/intuition_behind_covariance_kernels/,Zephyr314,1445275428,,0,5
386,2015-10-20,2015,10,20,3,3pdw0f,Azure Machine Learning: a cloud-based predictive analytics service,https://www.reddit.com/r/MachineLearning/comments/3pdw0f/azure_machine_learning_a_cloudbased_predictive/,Lancelot-du-Lac,1445279849,,0,1
387,2015-10-20,2015,10,20,6,3pele3,I'm training a face recognition system on the IJB-A facial image database. I'm beginning to notice a theme...,https://www.reddit.com/r/MachineLearning/comments/3pele3/im_training_a_face_recognition_system_on_the_ijba/,BadGoyWithAGun,1445289645,,0,6
388,2015-10-20,2015,10,20,7,3pevnd,"Are there small (""lifestyle"") businesses doing ML, or is it all get-big-or-die startups? Examples?",https://www.reddit.com/r/MachineLearning/comments/3pevnd/are_there_small_lifestyle_businesses_doing_ml_or/,[deleted],1445293906,[deleted],11,17
389,2015-10-20,2015,10,20,10,3pfhgb,[1510.05328] Exploring the Space of Adversarial Images,https://www.reddit.com/r/MachineLearning/comments/3pfhgb/151005328_exploring_the_space_of_adversarial/,[deleted],1445303276,,8,11
390,2015-10-20,2015,10,20,10,3pfjzs,Looking for advice on the server I'm building for machine learning. My budget is about 2k. Should i take away some of the costly cpu and ram in favor of just a good video card? Here is my list so far.,https://www.reddit.com/r/MachineLearning/comments/3pfjzs/looking_for_advice_on_the_server_im_building_for/,monkzta,1445304349,,14,6
391,2015-10-20,2015,10,20,13,3pg6qs,Help finding an FP-Growth function in R?,https://www.reddit.com/r/MachineLearning/comments/3pg6qs/help_finding_an_fpgrowth_function_in_r/,Arighea,1445314503,"Hello.

I currently have an assignment for my data mining course on association rules. Essentially we're asked to find and prune rules for a few given datasets using the Apriori and FP-Growth algorithms in R, but I'm lost as to where to find a library containing the FP-Growth function. Google hasn't seemed to help much, but I have found that I can maybe substitute it with the Eclat algorithm? 

Thank you.",0,2
392,2015-10-20,2015,10,20,13,3pg7jc,What's the intuition behind the Rectified linear unit?,https://www.reddit.com/r/MachineLearning/comments/3pg7jc/whats_the_intuition_behind_the_rectified_linear/,Jxieeducation,1445314880,"specifically why: 

1. it introduces sparsity

2. it prevents the gradients from vanishing for normalized data",6,9
393,2015-10-20,2015,10,20,14,3pgfj9,"Any resource for ""systematic"" study of NNs?",https://www.reddit.com/r/MachineLearning/comments/3pgfj9/any_resource_for_systematic_study_of_nns/,physixer,1445319327,"After getting basic understanding of NNs, I'm wondering:

We start with a very simple network: 1 input 1 parameter 1 output. We think of all possible ways it could be set up (I guess this would create a 2 variable system, one being input, and the other being the only parameter)

Then we consider:

- 1 input, 2 parameters
- 1 input, n parameters
- 2 inputs, 1 parameter
- 2 inputs, 2 parameters

and so on. Obviously there are two many cases but we can make some observations:

- We don't need parameters that are fed partial inputs, e.g., for a 2 input network, all our parameters have 2 inputs. We don't need any parameter with 1 input because it's the equivalent of a 2-input parameter one of which is zero.
- We can limit n, e.g., for 1 grayscale pixel, input is 0-255. Maybe we can use this to conclude a value of n beyond which further enumeration is superfluous.


Anyway I hope you get the idea. So my question is, is there any resource, tutorial, article, book chapter that discusses neural networks along these lines?

second question: is there something like a directory/collection of well-known neural network designs? (**EDIT 1**: in other words, well-known NN architectures). Because after exploring some of these we might start to see patterns, and some NNs performing certain types of actions.",2,4
394,2015-10-20,2015,10,20,18,3pgx6y,Dask + Sklearn experiment. Reuse intermediate results from Pipelines in parameter sweeps.,https://www.reddit.com/r/MachineLearning/comments/3pgx6y/dask_sklearn_experiment_reuse_intermediate/,cast42,1445332261,,19,16
395,2015-10-20,2015,10,20,18,3pgxmi,Multiple Caffe models on single GPU,https://www.reddit.com/r/MachineLearning/comments/3pgxmi/multiple_caffe_models_on_single_gpu/,Silversparro,1445332582,"Hi,

We have trained multiple caffe models already.

We want to use these multiple caffe models for making predictions on Single GPU simultaneously.

Is this possible, if yes how to do it ?

Thanks in advance for the help.

Thanks,
Abhinav",8,2
396,2015-10-20,2015,10,20,19,3ph2us,"""Data Science Machine"" crunches numbers faster and more effectively than most humans",https://www.reddit.com/r/MachineLearning/comments/3ph2us/data_science_machine_crunches_numbers_faster_and/,[deleted],1445336444,[deleted],1,0
397,2015-10-20,2015,10,20,19,3ph3y5,[1510.05336] Clustering is Easy When ....What?,https://www.reddit.com/r/MachineLearning/comments/3ph3y5/151005336_clustering_is_easy_when_what/,iori42,1445337296,,1,44
398,2015-10-20,2015,10,20,21,3phe3v,Understanding Bengio's 2003 Neural Network Language model paper,https://www.reddit.com/r/MachineLearning/comments/3phe3v/understanding_bengios_2003_neural_network/,koormoosh,1445343691,"I am reading Bengio's 2003 JMLR paper and I need some clarifications for the section 2 where he explains the architecture of the model. I couldn't find any resources, so I wonder if you guys are aware of any good tutorial which breaks things apart in more details.",7,14
399,2015-10-20,2015,10,20,22,3phnkb,Classification with Azure Machine Learning Studio,https://www.reddit.com/r/MachineLearning/comments/3phnkb/classification_with_azure_machine_learning_studio/,dstarnes,1445348200,,0,2
400,2015-10-20,2015,10,20,22,3phosl,is it right to divide features of a sample into sub-samples for training machine learning models?,https://www.reddit.com/r/MachineLearning/comments/3phosl/is_it_right_to_divide_features_of_a_sample_into/,insider_7,1445348753,"Hi I have a large database which has a large number of features (around 2000) per sample.

Is it right to divide a sample into ""subsamples"" of fewer features?

For example, divide a X sample vector with shape (1, 2000) into 4 sub samples (x1, x2, x3, x4) of shape:

(1, 500)
(1, 500)
(1, 500)
(1, 500)

Is it right to train a machine learning model like this? 
If yes or no, why?

In this case, every sample will be divided into 4 sub-samples to preserve consistency. 


Thank you.

EDIT: more details from the specific type of experiments:

The data I have is acquisition frames from a sensor system. one acquired frame has observations from 100 sensors. 2000 frames of 100 sensors (features) each, represent one experiment.
I want to know if it is right to train my machine learning models with one sample for one frame that has 100 features. Instead of training with sample per experiment which has 2000 features per sample. In the case of a sample per frame, each sample has the same feature space, which is the same type of sensors.
",14,3
401,2015-10-20,2015,10,20,22,3phpwr,Looking for a free open source DNN library,https://www.reddit.com/r/MachineLearning/comments/3phpwr/looking_for_a_free_open_source_dnn_library/,NovaRom,1445349235,"I would like to embed some DeepLearning functionality (say MLP/RNN training and scoring) into my C++ application. Is there any library which is free (has no dependecies on proprietary modules like CUDA) and which can be simply linked with a C or C++ program (e.g. in form of a shared lib)? GPU support is not required. Ideally, it should be a shared lib and few headers (e.g. like many GNU libs).",1,0
402,2015-10-20,2015,10,20,23,3phvf9,[1510.04709] Multi-Language Image Description with Neural Sequence Models,https://www.reddit.com/r/MachineLearning/comments/3phvf9/151004709_multilanguage_image_description_with/,votadini_,1445351579,,2,2
403,2015-10-21,2015,10,21,0,3phzmu,How Important is Weight Symmetry in Backpropagation?,https://www.reddit.com/r/MachineLearning/comments/3phzmu/how_important_is_weight_symmetry_in/,Ghostlike4331,1445353333,,17,21
404,2015-10-21,2015,10,21,2,3pij84,Why the Golden Age of Machine Learning Begins Now,https://www.reddit.com/r/MachineLearning/comments/3pij84/why_the_golden_age_of_machine_learning_begins_now/,[deleted],1445360914,[deleted],1,0
405,2015-10-21,2015,10,21,2,3pijpl,A question about data normalization/scaling and alpha values for gradient descent,https://www.reddit.com/r/MachineLearning/comments/3pijpl/a_question_about_data_normalizationscaling_and/,pfizer_soze,1445361106,"I come from more of a stats background, but I am taking the stanford machine learning course on coursera. I just got past the portion that looks at how data normalization can speed up gradient descent in multivariate regressions, but I had a question.

Rather than scale the variables, would it be possible to have a separate alpha value for each dimension and scale that? The idea would be to have a vector of alpha values that all start the same but are scaled to match the variable they influence. It seems like you would be able to then make predictions without scaling your inputs first.",7,0
406,2015-10-21,2015,10,21,2,3pimv8,Confidence Splitting Criterions Can Improve Precision And Recall in Random Forest Classifiers,https://www.reddit.com/r/MachineLearning/comments/3pimv8/confidence_splitting_criterions_can_improve/,cast42,1445362260,,3,24
407,2015-10-21,2015,10,21,2,3piqcc,Dive into Machine Learning..Get Your Feet Wet!,https://www.reddit.com/r/MachineLearning/comments/3piqcc/dive_into_machine_learningget_your_feet_wet/,dabshitty,1445363522,,3,27
408,2015-10-21,2015,10,21,3,3pirt2,Introduction to Amazon Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3pirt2/introduction_to_amazon_machine_learning/,[deleted],1445364037,[deleted],0,2
409,2015-10-21,2015,10,21,4,3pj477,What am I doing wrong?,https://www.reddit.com/r/MachineLearning/comments/3pj477/what_am_i_doing_wrong/,hltt,1445368508,[removed],0,1
410,2015-10-21,2015,10,21,4,3pj7pp,How can I execute conditional GAN code? https://github.com/hans/adversarial,https://www.reddit.com/r/MachineLearning/comments/3pj7pp/how_can_i_execute_conditional_gan_code/,wildtales,1445369772,"I can see lots of class files, but can't make out how to run a conditional GAN on LFW data. Any help will be appreciated.

https://github.com/hans/adversarial",4,1
411,2015-10-21,2015,10,21,4,3pja65,Machine Learning Tool Seeks to Automate Data Science,https://www.reddit.com/r/MachineLearning/comments/3pja65/machine_learning_tool_seeks_to_automate_data/,efavdb,1445370648,,0,0
412,2015-10-21,2015,10,21,4,3pjbc3,What am I doing wrong?,https://www.reddit.com/r/MachineLearning/comments/3pjbc3/what_am_i_doing_wrong/,hltt,1445371079,[removed],0,1
413,2015-10-21,2015,10,21,5,3pjd62,What am I doing wrong?,https://www.reddit.com/r/MachineLearning/comments/3pjd62/what_am_i_doing_wrong/,hltt911,1445371757,"I am writing a model to do sentiment classification using Convolutional Neural Network. My idea is a bit different from Kim Yoon's CNN (http://arxiv.org/abs/1408.5882). The main difference is that the model tries to detect the sense of each word before learning sentiment classification.

The overall architecture is illustrated at http://imgur.com/ap113cl. The first layer is the project layer with word2vec embedding from one-hot-encoding vectors. The second layer is the higher-level features from the first layer. The third layer is the sense detection (learned from higher-level feature). The fourth layer is the projection of the senses with sense embedding. The 5th and so-on layers are just a mimic from Kim Yoon's model.

My code is below (using Keras), train with Movie Review data:

    class SoftMaxOut1D(Activation):
        """""" Similar to Max Out but compute softmax activation instead """"""
        
        def __init__(self, **kwargs):
            super(Activation, self).__init__(**kwargs)
            self.activation = self.softmax
        
        @classmethod
        def softmax(cls, X):
            X = X - X.max(axis=2, keepdims=True)
            return T.exp(X)/T.exp(X).sum(axis=2, keepdims=True)

    embed_size = 300
    senses = 500
    sense_embed_size = 300
    Wmax = maxnorm(12)
    model = Sequential()

    model.add(Embedding(vocab_size, embed_size, W_constraint=Wmax))
    model.add(Dropout(.25))

    # context interaction layer
    model.add(Convolution1D(embed_size, 100, 5, border_mode='full', W_constraint=Wmax))
    model.add(Activation('relu'))
    model.add(Dropout(.25))

    # sense detection layer
    model.add(Convolution1D(100, senses, 1, border_mode='valid', W_constraint=Wmax))
    # model.add(Activation('relu'))
    model.add(SoftMaxOut1D())
    # model.add(Dropout(.25))

    # sense embed layer
    model.add(Convolution1D(senses, sense_embed_size, 1, border_mode='valid', W_constraint=Wmax))
    model.add(Dropout(.25))

    # context sense interaction layer
    model.add(Convolution1D(sense_embed_size, 200, 5, border_mode='valid', W_constraint=Wmax))
    model.add(Activation('relu'))
    model.add(MaxPooling1D(pool_length=61))
    model.add(Flatten())
    model.add(Dropout(.25))

    model.add(Dense(200, 20, activation='relu', W_constraint=Wmax))
    model.add(Dropout(.25))

    model.add(Dense(20, 1, activation='sigmoid', W_constraint=Wmax))

    model.compile(loss='binary_crossentropy', optimizer=""rmsprop"", class_mode='binary')
    model.fit(X_train, y_train, batch_size=32, nb_epoch=20, show_accuracy=True, validation_data=(X_test, y_test))


However my training doesn't seem to work as the loss doesn't reduce. Here is the output from my train:

    Train on 9595 samples, validate on 1067 samples
    Epoch 0
    9595/9595 [==============================] - 288s - loss: 0.6932 - acc: 0.5078 - val_loss: 0.6932 - val_acc: 0.4986
    Epoch 1
    9595/9595 [==============================] - 288s - loss: 0.6935 - acc: 0.4940 - val_loss: 0.6932 - val_acc: 0.4986
    Epoch 2
    9595/9595 [==============================] - 288s - loss: 0.6932 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5014
    Epoch 3
    9595/9595 [==============================] - 288s - loss: 0.6933 - acc: 0.4913 - val_loss: 0.6932 - val_acc: 0.5014
    Epoch 4
    9595/9595 [==============================] - 288s - loss: 0.6934 - acc: 0.4870 - val_loss: 0.6931 - val_acc: 0.4986
    Epoch 5
    9595/9595 [==============================] - 288s - loss: 0.6933 - acc: 0.4968 - val_loss: 0.6932 - val_acc: 0.4986
    Epoch 6
    9595/9595 [==============================] - 287s - loss: 0.6932 - acc: 0.4940 - val_loss: 0.6932 - val_acc: 0.4986
    Epoch 7
    9595/9595 [==============================] - 287s - loss: 0.6931 - acc: 0.5060 - val_loss: 0.6933 - val_acc: 0.5014
    Epoch 8
    9595/9595 [==============================] - 287s - loss: 0.6935 - acc: 0.4952 - val_loss: 0.6932 - val_acc: 0.4986
    Epoch 9
    9595/9595 [==============================] - 287s - loss: 0.6933 - acc: 0.4991 - val_loss: 0.6932 - val_acc: 0.4986
    Epoch 10
    9595/9595 [==============================] - 288s - loss: 0.6932 - acc: 0.5056 - val_loss: 0.6932 - val_acc: 0.4986
    Epoch 11
    9595/9595 [==============================] - 288s - loss: 0.6933 - acc: 0.5020 - val_loss: 0.6932 - val_acc: 0.4986
    Epoch 12
    9595/9595 [==============================] - 288s - loss: 0.6932 - acc: 0.5030 - val_loss: 0.6933 - val_acc: 0.4986
    Epoch 13
    9595/9595 [==============================] - 287s - loss: 0.6934 - acc: 0.4887 - val_loss: 0.6932 - val_acc: 0.4986
    Epoch 14
    9595/9595 [==============================] - 288s - loss: 0.6931 - acc: 0.5022 - val_loss: 0.6932 - val_acc: 0.5014
    Epoch 15
    9595/9595 [==============================] - 288s - loss: 0.6933 - acc: 0.4887 - val_loss: 0.6932 - val_acc: 0.4986
    Epoch 16
    9595/9595 [==============================] - 287s - loss: 0.6932 - acc: 0.4932 - val_loss: 0.6932 - val_acc: 0.4986
    Epoch 17
    9595/9595 [==============================] - 287s - loss: 0.6932 - acc: 0.4979 - val_loss: 0.6932 - val_acc: 0.4986
    Epoch 18
    9595/9595 [==============================] - 288s - loss: 0.6932 - acc: 0.4953 - val_loss: 0.6931 - val_acc: 0.4986
    Epoch 19
    9595/9595 [==============================] - 288s - loss: 0.6933 - acc: 0.4912 - val_loss: 0.6932 - val_acc: 0.4986



Do you guys, by any chance, know what I am doing wrong?when I remove SoftMax1D() layer, it works as expected.",5,4
414,2015-10-21,2015,10,21,5,3pjl05,transfer learning resources - open source software list,https://www.reddit.com/r/MachineLearning/comments/3pjl05/transfer_learning_resources_open_source_software/,auraham,1445374633,,1,6
415,2015-10-21,2015,10,21,7,3pjyki,A fun image processing project in javascript that is marginally related to my learning theory research.,https://www.reddit.com/r/MachineLearning/comments/3pjyki/a_fun_image_processing_project_in_javascript_that/,NarcolepticFrog,1445379664,,0,2
416,2015-10-21,2015,10,21,10,3pks9o,Restricted Boltzmann Machine hidden units,https://www.reddit.com/r/MachineLearning/comments/3pks9o/restricted_boltzmann_machine_hidden_units/,vega455,1445392295,"Sorry for the noob question, but in an RBM, how do you decide on the number of units in a hidden layer? Do you iterate through a large number of hidden units and pick the best?",6,0
417,2015-10-21,2015,10,21,10,3pkt1i,Why is unsupervised learning so important?,https://www.reddit.com/r/MachineLearning/comments/3pkt1i/why_is_unsupervised_learning_so_important/,feedthecreed,1445392629,"All the machine learning experts (particularly those in deep learning like Yoshua Bengio, Andrew Ng, Yann LeCun, and Geoff Hinton) believe that unsupervised learning is the future. 

What do they mean by this? Does it mean creating generative models which we can sample from? Are they talking about specific things like autoencoders? What kinds of algorithms are they talking about when they say *unsupevised learning*?

Say I have a model that I can generate from? For example, I've trained a model that generates the best MNIST/CIFAR digits so far. What do I do with that model now?",24,60
418,2015-10-21,2015,10,21,11,3pl0c3,Here's a intro course on Machine Learning by Stanford free starting Nov. 2 Has anyone tried it in the past?,https://www.reddit.com/r/MachineLearning/comments/3pl0c3/heres_a_intro_course_on_machine_learning_by/,heytaytay69,1445395848,,16,7
419,2015-10-21,2015,10,21,15,3plqqf,DFJ-130,https://www.reddit.com/r/MachineLearning/comments/3plqqf/dfj130/,dongfengpacking,1445410560,,1,1
420,2015-10-21,2015,10,21,16,3plsj8,"I want to normalize small changes in format (e.g. E70*i-A3*** =&gt; E701i-A3, M602i-B3 =&gt; M602IB3, or 32PFL4909/F7 =&gt; 32PFL4909) is machine learning (scikit-learn) appropriate?",https://www.reddit.com/r/MachineLearning/comments/3plsj8/i_want_to_normalize_small_changes_in_format_eg/,gunsofbrixton,1445411868,"I have a problem at my job that I'm exploring options to solve. We deal with model numbers for electronics but the problem is there is little consistency in the formats of the text. I've included some examples in the title but oftentimes the changes are quite small. What we do is match the strings 1 to 1, and then literally go through the rest and match them manually. Since the differences are oftentimes predictable patterns I believe the process could be improved by using machine learning and scikit-learn. Our volume is high and we have a lot of data of examples of these patterns and how they match up. My question for you guys is if this sounds like a problem best solved by ML, and if so, can you point me in the direction of the best functionality within scikit-learn to get going? I can figure out the rest myself but I don't want to be barking up the wrong tree if I can't help it :)",6,2
421,2015-10-21,2015,10,21,16,3plt0l,Random Forest for Unsupervised Learning,https://www.reddit.com/r/MachineLearning/comments/3plt0l/random_forest_for_unsupervised_learning/,agingmonster,1445412213,"I came across this article (by original author of paper on RF) which indicates that RF can be used for unsupervised learning. [This is link] (https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#unsup). I understood how to create fake classes from unlabeled data.

I don't get anything else: what's it good for? how RF supposed to help unsupervised learning, and what of unsupervised learning? what about ""dependency"" they are talking about?

Can you help, or point to resource where similar thing is explained in more detail?",2,10
422,2015-10-21,2015,10,21,16,3plv48,Making predictions using Caffe on CPUs,https://www.reddit.com/r/MachineLearning/comments/3plv48/making_predictions_using_caffe_on_cpus/,Silversparro,1445413852,"Hi all,

How can we improve efficiency of Caffe in making predictions on CPUs ? What are all the various ways in which we can do that ?

Thanks,
Abhinav",2,0
423,2015-10-21,2015,10,21,17,3plvwc,Cheapest GPU on the cloud ?,https://www.reddit.com/r/MachineLearning/comments/3plvwc/cheapest_gpu_on_the_cloud/,Silversparro,1445414468,If we want to use GPUs for Deeplearning on the cloud ? What are the various options available and what is the cheapest option ?,15,12
424,2015-10-21,2015,10,21,22,3pmnoj,VLSI Implementation of Deep Neural Network Using Integral Stochastic Computing,https://www.reddit.com/r/MachineLearning/comments/3pmnoj/vlsi_implementation_of_deep_neural_network_using/,mttd,1445433151,,0,5
425,2015-10-21,2015,10,21,22,3pms9l,Intel taking an interest in julialang,https://www.reddit.com/r/MachineLearning/comments/3pms9l/intel_taking_an_interest_in_julialang/,__mtb__,1445435273,,4,0
426,2015-10-21,2015,10,21,23,3pmtyd,"Automated Behavioral Analytics and machine learning can detect threats as they are unfolding in real time, leveraging those same logs and data.",https://www.reddit.com/r/MachineLearning/comments/3pmtyd/automated_behavioral_analytics_and_machine/,MillerewdBentley5_,1445436013,,0,0
427,2015-10-22,2015,10,22,0,3pn3wt,"Is increasing the number of epochs for less data same as using more data with less number of epochs, while training a Neural network?",https://www.reddit.com/r/MachineLearning/comments/3pn3wt/is_increasing_the_number_of_epochs_for_less_data/,napsternxg,1445440081,"I was wondering that one of the major arguments made against using Artificial Neural Networks (ANN) is that they require large amounts of data to train on. However, when we look at the implementation of an ANN, then we see that there is no term which defines how many data points we need to train on. In fact, one of the major parameters to tune are the regularization terms and the number of epochs (or iterations). Also, it has been shown that ANN are universal approximators, and they can learn functions like XOR using small data but enough epochs. 

So, building on that shouldn't it be possible for an ANN to learn using a small data set, of the order which is used by SVMs, Logistic Regressions (which can all be represented as ANN), e.g. 20k instances for binary sentiment analysis. 

What are some limitations of using ANN for small data, e.g. Iris data with ~100 instances, movie reviews data ~20k instances etc?

Doesn't small data (Order of 1000) with 1000 times more epochs equals big data (Order of 10^6), when it comes to training neural networks? (In understand there is not going to be enough diversity in the smaller data compared to larger data)",3,0
428,2015-10-22,2015,10,22,1,3pnd8i,Why The Golden Age Of Machine Learning is Just Beginning,https://www.reddit.com/r/MachineLearning/comments/3pnd8i/why_the_golden_age_of_machine_learning_is_just/,john_philip,1445443627,,12,50
429,2015-10-22,2015,10,22,1,3pnjly,Is it possible to avoid mean/var training data normalization and to get the same accuracy?,https://www.reddit.com/r/MachineLearning/comments/3pnjly/is_it_possible_to_avoid_meanvar_training_data/,NovaRom,1445445998,I have just a raw training data and I want to avoid mean/var normalization. Is there any technique to make my network learn this so that the end result will be almost the same accuracy as with normalization?,6,0
430,2015-10-22,2015,10,22,3,3pnx9p,How do you think emotions and feelings would be implemented in a brain?,https://www.reddit.com/r/MachineLearning/comments/3pnx9p/how_do_you_think_emotions_and_feelings_would_be/,[deleted],1445451128,[deleted],3,0
431,2015-10-22,2015,10,22,11,3ppvy5,Im2Calories ICCV 2015 demo,https://www.reddit.com/r/MachineLearning/comments/3ppvy5/im2calories_iccv_2015_demo/,[deleted],1445479876,[deleted],1,21
432,2015-10-22,2015,10,22,11,3ppzvs,How serious is this? (Deep Learning on Quantum Computers),https://www.reddit.com/r/MachineLearning/comments/3ppzvs/how_serious_is_this_deep_learning_on_quantum/,nivwusquorum,1445481579,,18,20
433,2015-10-22,2015,10,22,12,3pq6eh,Intuitions behind the dropout layer in Neuro Nets,https://www.reddit.com/r/MachineLearning/comments/3pq6eh/intuitions_behind_the_dropout_layer_in_neuro_nets/,Jxieeducation,1445484433,"http://jxieeducation.com/2015-10-21/Dropout-As-Prevention-For-Co-Adpotion-In-Feedforward-Neural-Net/

Enjoy!",0,0
434,2015-10-22,2015,10,22,12,3pq7is,Loss and accuracy in rnn,https://www.reddit.com/r/MachineLearning/comments/3pq7is/loss_and_accuracy_in_rnn/,sweetrabh,1445484986,"I'm getting mixed results with an rnn setup I'm testing and I was hoping someone here can clarify on why that is. Just want to note that I don't have any formal education in machine learning, just trying to learn it on the side, so forgive me if the questions sound a little noobish. 

After several epochs of testing, I find that my loss dips below 0 and eventually reaches -2. At the same time, the accuracy is consistent and never changes from the original value. This happened after I changed the rnn's final output to be a single variable. In a setup I tested before this, I had the rnn's final output to have 4 variables. With this setup, the loss eventually reached to a low number, 0.05, but the accuracy was around 0.65. The plot of the accuracy over several epochs was changing unlike the single variable case, but it was always decreasing. So I'm having trouble understanding how to interpret these results. Does a loss of 0.05 and an accuracy of 0.65 mean 'the rnn will use 95% of the input data to correctly predict 65% of the time'? If that is the case, what does it mean to have a negative loss? And I believe the usual behavior of an rnn is to increase in accuracy while the loss is taken out. How come the accuracy never increases in either setup I've tried? I'm sure I'm doing something wrong, I appreciate any tips on where that may be.",2,1
435,2015-10-22,2015,10,22,13,3pqawp,What's the current state of scalability of Bayesian nonparametrics models?,https://www.reddit.com/r/MachineLearning/comments/3pqawp/whats_the_current_state_of_scalability_of/,AlfonzoKaizerKok,1445486580,"I'm new to Bayesian nonparametrics, and I'm wondering if researchers in that area focus a lot on scalability? How's the progress like in terms of scalability, and is it looking promising?",10,1
436,2015-10-22,2015,10,22,13,3pqdl8,Automating variational inference with Stan,https://www.reddit.com/r/MachineLearning/comments/3pqdl8/automating_variational_inference_with_stan/,Leibz,1445487938,,0,3
437,2015-10-22,2015,10,22,14,3pqlzu,Virality Prediction and Community Structure in Social Networks : Scientific Reports in Nature!,https://www.reddit.com/r/MachineLearning/comments/3pqlzu/virality_prediction_and_community_structure_in/,cast42,1445492876,,0,0
438,2015-10-22,2015,10,22,15,3pqp8a,Automatic Model Generation workflow: Optimal vs. efficiency?,https://www.reddit.com/r/MachineLearning/comments/3pqp8a/automatic_model_generation_workflow_optimal_vs/,beginner_,1445495005,"I've been improving my model building workflows after reading [this article](http://danielnee.com/2015/01/common-pitfalls-in-machine-learning/).

I for sure made some of the mistakes shown in the article. My question now is around how important it is to follow these guidelines.

Some info about my data:

- low n (around 1000)
- highly dimensional 

Features are calculated values and there are many, many available. You can easily get 500+ features if you like.

1. Feature Selection Leakage

I for sure made that mistake. As a simple method of feature selection I used GBM to determine the importance of each feature and cut of all features below a certain threshold. So this is bad correct? I now removed that step and simply to low variance and correlation filtering. Problem is this leaves me with 100+ features. I thought it was bad to have many features compared to rows. Plus this of course greatly increases runtime.

Other options of feature selection? PCA seems to greatly reduce performance.

2. Parameter Selection

specially in aspect of runtime as the article mentions inner and outer cross validation loops. So that would lead to runtimes of days for what I seem simple models (n = 1000).

So for optimizing parameters I should only use a part of all data available? And then repeat the whole optimization step in a outer CV loop? Isn't that a bit excessive?

In my case I might get 10 new data points (measurements) a year. Am I correct to assume I can optimize parameters once and not bother to do it again when creating a new model with new data points say once a year?",7,0
439,2015-10-22,2015,10,22,17,3pr1h6,How We Use Deep Learning to Classify Business Photos at Yelp,https://www.reddit.com/r/MachineLearning/comments/3pr1h6/how_we_use_deep_learning_to_classify_business/,pogopuschel_,1445503922,,7,54
440,2015-10-22,2015,10,22,18,3pr4v4,Are there any open source speech recognition implementations that work out of the box?,https://www.reddit.com/r/MachineLearning/comments/3pr4v4/are_there_any_open_source_speech_recognition/,data_gatherer,1445506482,"I'm aware of cmu-sphinx, however the user interface/experience for training the network is complex at best. I don't seem to be able to map my speech at the 'phoneme level'. I feel like I'm competent enough to only add dialogue support to existing language model, which is not ideal, as it's built around US and I'd like a purely GB model. It uses HMM as the foundation of it's learning. I'm not sure how relevant this still is as I've seen papers on other implementations of NN.

So yes, if there's anything else out there that's kind of plug and play, that would be great. If not, I would take any suggestions that people have on optimizing my cmu sphinx experience.

Edit: to clarify, I don't need training data, I am willing to create my own. I'm just looking for the implementation.

Cheers!",9,1
441,2015-10-22,2015,10,22,19,3pr9c7,Cudnn v2 + theano 3d convolutions faster than Cudnn v3?,https://www.reddit.com/r/MachineLearning/comments/3pr9c7/cudnn_v2_theano_3d_convolutions_faster_than_cudnn/,[deleted],1445509893,[deleted],11,0
442,2015-10-22,2015,10,22,21,3prmur,Hitting local minima in bidi LSTMs on speech recognition problem,https://www.reddit.com/r/MachineLearning/comments/3prmur/hitting_local_minima_in_bidi_lstms_on_speech/,sthoppay,1445518318,"I have implemented a bidi-LSTM network for speech recognition. The network impementation is similar to Karpathy's neuraltalk (http://cs.stanford.edu/people/karpathy/deepimagesent/). I wanted to decode the input sequences directly as alphabets or phenoms without aligning on frame basis, so I implemented Connectionist temporal classification algorithm as described in Alex Graves's thesis (http://www.cs.toronto.edu/~graves/phd.pdf)
As usual, the implementation has various tunable parameters and hyper parameters (lr, momemtum, forget gate bias setting). Input features are long sequences (upto 600 timesteps) with 13 Mel ceptral coeffcients per timestep. The problem that I am facing is that with any time of network architectures (1 hidden with 100 neurons, 2 hidden with 50 neurons each, etc) run on a small training set. My outputs are getting stuck at one or two output neurons (softmax output is high for these 2 neurons). it is not always the same neurons. My weight vectors are initialized in gaussian distribution between -0.01 to 0.01 and my learning rates are 1e-5 (fixed at this point). Initial training with learning rate of 1e-3 caused the loss to explode and weight gradients to explode.

I read several research papers on initialization, training the network,
http://arxiv.org/pdf/1504.00941v2.pdf - 
http://www.jmlr.org/proceedings/papers/v28/pascanu13.pdf
http://arxiv.org/pdf/1409.3215v3.pdf - sequence to sequence learning with neural network

I would greatly appreciate on any pointer on how to train such long time series sequence, what are right ways to initialize the network?",11,0
443,2015-10-22,2015,10,22,23,3ps25d,An explanation of Gaussianisation in Autoregressive Image Models and how one might use it to train nontrivial deep models.,https://www.reddit.com/r/MachineLearning/comments/3ps25d/an_explanation_of_gaussianisation_in/,fhuszar,1445525272,,0,1
444,2015-10-23,2015,10,23,1,3psfqz,Jitter Test for Overfitting (Kaggle Python Script),https://www.reddit.com/r/MachineLearning/comments/3psfqz/jitter_test_for_overfitting_kaggle_python_script/,zach_will,1445530845,,0,11
445,2015-10-23,2015,10,23,1,3pshkq,Can I use my XBOX 360 for Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/3pshkq/can_i_use_my_xbox_360_for_deep_learning/,dev17392,1445531588,"I have an old 360 lying around. Does anyone know if I can install CUDA on it? I found that there's a linux project Free60, allowing me to install linux on 360. But I couldn't find CUDA drivers for the Xenos GPU in Xbox. Also, it's an ATI graphics card. 

I mostly work on NLP, and Theano. So, I don't require a GPU with a really high memory. Normal notebook GPUs server my purpose well.  

Does anyone know if this would be possible? If yes, how hard will it be to install? ",5,0
446,2015-10-23,2015,10,23,2,3pspnz,Artificial Intelligence (AI) - How algorithms impact our lives!,https://www.reddit.com/r/MachineLearning/comments/3pspnz/artificial_intelligence_ai_how_algorithms_impact/,adeniyiks,1445534851,,0,1
447,2015-10-23,2015,10,23,2,3psq06,Panos Toulis - Implicit Stochastic Gradient Descent for large-scale GLMs,https://www.reddit.com/r/MachineLearning/comments/3psq06/panos_toulis_implicit_stochastic_gradient_descent/,improbabble,1445534976,,1,1
448,2015-10-23,2015,10,23,2,3psqdx,Beginner exploration of dataset.,https://www.reddit.com/r/MachineLearning/comments/3psqdx/beginner_exploration_of_dataset/,captain_silverman,1445535135,"I was looking at learning some ML and was recommended to use Weka since it enables to visualize things well. I found a random dataset of news articles and since I was reading about information gain from stats, I loaded the dataset and chose InfoGainAttributeEval with default Ranker. I got all words ranked and around 50 of them had value of 0. Does that mean that they are present among all classes of articles and hence won't help in decision tree splits? There are 5 such classes btw.

I would also like to understand what that means visually hence I took a picture of 1) most useful word with largest information gain against classes and 2) Least useful attribute value with information gain of 0 among 50+ others.

1) http://puu.sh/kTENs/7845ac9bba.png
2) http://puu.sh/kTEPu/0ea7378016.png

How can I interpret this visual information? ",0,0
449,2015-10-23,2015,10,23,2,3psqil,Sentence to Sentence Text Generation Using LSTM's,https://www.reddit.com/r/MachineLearning/comments/3psqil/sentence_to_sentence_text_generation_using_lstms/,LeavesBreathe,1445535186,"Hey Everyone,

I'm currently using Keras to implement LSTMs in NLP. I currently input a sentence of words (represented by a cluster id and then a word id) and ask the LSTM to predict the next word, and it works pretty well. If you are interested in the clustering, the ideas was started in this thread: https://www.reddit.com/r/MachineLearning/comments/3og80o/recreating_skipthoughts_paper_in_keras/

If you're interested on how to handle infrequent words in clustering: https://www.reddit.com/r/MachineLearning/comments/3p7tns/handling_infrequent_words_in_corpus_with_word2vec/

	Currently:

	Input Sentence of Words --&gt; Predicts next word

	Goal:

	Input Sentence of Words --&gt; Predict next sentence


I'm looking now into feeding the LSTM and entire sentence, and then asking it to produce an entire sentence. However, the major problem is this: I don't know how to format y_train (or the labels)

For x_train (input), I understand that I use a 2d array, and pad the array with 0's. So that the first dimension is the number of samples, and the second dimension is the sentence that I'm feeding into the lstm. I then feed this into Keras's embedding layer -- and it works great! (I mask the zeros)

However, the 2d y_train is where I get lost. Currently, I one-hot my y_train to the *one word* that is supposed to come next. In order to have y_train be a sequence of words, it almost seems I would have to add an extra dimension (which is impossible right?). I have a Dense Layer at the end which condenses the output to 2d, followed by a softmax.  

So the question is: How do you format the y_train so that you can output an entire sentence versus a word? 

Hopefully others in the future with this question can use this post! Thanks alot guys. ",17,1
450,2015-10-23,2015,10,23,2,3psrjy,Back to The Future II...Could Marty McFly have done better with Dear Beloved Algorithm?!,https://www.reddit.com/r/MachineLearning/comments/3psrjy/back_to_the_future_iicould_marty_mcfly_have_done/,adeniyiks,1445535591,,0,0
451,2015-10-23,2015,10,23,3,3psylf,Can somebody tell me if this would be a publishable experiment?,https://www.reddit.com/r/MachineLearning/comments/3psylf/can_somebody_tell_me_if_this_would_be_a/,o_safadinho,1445538374,"So I have an idea for an experiment and I want to know if it seems like it is something that would be publishable.

Anyway, the entire reason that I majored in Statistics as an undergrad and am currently going to grad school in Data Mining was because of a paper that I read on predicting the winners of greyhound races. All through undergrad and to this day, I've been reading papers about this specific topic.

I've run across papers in CS, Operations Research, Statistics and Economics specific journals. As well as thesis projects that you can just find online. A lot of the papers do things like show that ""X"" technique can be used to predict winners ([Adapting support vector machine methods for horserace odds prediction](http://link.springer.com/article/10.1007%2Fs10479-006-0131-7)), others compare the performance of variations of learning algorithms in the same family([A Case Study Using Neural Network Algorithms: Horse Racing Predictions in Jamaica](https://eprints.usq.edu.au/4300/1/Williams_Li_Publ_version.pdf)), some use horse racing as an example of how prediction markets work ([Prediction Markets for Economic Forecasting](http://www.brookings.edu/~/media/research/files/papers/2012/6/13%20prediction%20markets%20wolfers/13%20prediction%20markets%20wolfers.pdf)) and so on.

I've noticed that in a lot of the papers that really take a focus on testing some classification algorithm, especially the CS papers, never really go into how they do feature selection. They generally say that it is important and that there are a lot of variables involved and we decided to pick these 5,7,12, or whatever the number is. Sometimes they'll say that they spoke to an ""expert"" or they will just use the same features that were used in another paper but with a different learning technique ([here](http://www.researchgate.net/publication/4030214_Neural_networks_mine_for_gold_at_the_greyhound_racetrack)).

What I want to know is, what effect does using various feature selection algorithms have on your prediction accuracy and ROI? So I'd start with as many explanatory variables as possible and then use different feature selection algorithms while keeping the classification algorithm the same and then check the results.",3,4
452,2015-10-23,2015,10,23,3,3pt1p0,Beginner question about choosing algorithims,https://www.reddit.com/r/MachineLearning/comments/3pt1p0/beginner_question_about_choosing_algorithims/,souldeux,1445539618,"I am trying to get in to machine learning, and the resources on this subreddit have been hugely helpful. I was hoping someone could help me understand two concepts which are giving me trouble:

1. I've worked through the tutorial stuff on Kaggle with the Titanic dataset. They use a random forest to try and predict passenger survival from a variety of factors (age, gender, passenger class, etc.). The tutorial itself makes sense to me; however, after learning a little about support vector classification I am confused as to why a random forest is the better choice here. 

2. On the topic of SVC, I am having a hard time understanding what the C and gamma values do. I have a vague idea that they control ""fuzziness"" - but when I read that low gamma means a training example has a far reach and a high gamma means a close reach, I don't really get what that means. I think if I better understood this, I could wrap my head around `sklearn`'s grid searching.

Thanks!!",10,0
453,2015-10-23,2015,10,23,4,3ptb14,BigML and The Polytechnic University of Valencia join forces to promote Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3ptb14/bigml_and_the_polytechnic_university_of_valencia/,czuriaga,1445543299,,0,1
454,2015-10-23,2015,10,23,5,3ptdqu,Marty McFly did Amazingly Better than Algorithms!,https://www.reddit.com/r/MachineLearning/comments/3ptdqu/marty_mcfly_did_amazingly_better_than_algorithms/,adeniyiks,1445544392,,0,0
455,2015-10-23,2015,10,23,5,3ptjvf,"Yann LeCun at BayLearn: ""Obstacles on the path to AI 'how I learned to stop worrying and love unsupervised learning'""",https://www.reddit.com/r/MachineLearning/comments/3ptjvf/yann_lecun_at_baylearn_obstacles_on_the_path_to/,evc123,1445546870,,25,62
456,2015-10-23,2015,10,23,5,3ptkjx,What does it mean for a LSTM block to have N cells?,https://www.reddit.com/r/MachineLearning/comments/3ptkjx/what_does_it_mean_for_a_lstm_block_to_have_n_cells/,macncookies,1445547142,"Does it refer to the cell state carousel of the block having N 'channels'? 

Please do consider that my knowledge on LSTMs is almost entirely based on [this blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). 

Thank you! 

P.S. Does anyone here have any experience with [convolutional LSTMs](http://arxiv.org/abs/1506.04214)?",3,0
457,2015-10-23,2015,10,23,6,3ptq3r,Plans after graduating from a PhD in computer science,https://www.reddit.com/r/MachineLearning/comments/3ptq3r/plans_after_graduating_from_a_phd_in_computer/,[deleted],1445549421,[deleted],0,1
458,2015-10-23,2015,10,23,6,3pts4i,How can a PhD in Machine Learning and AI help you in a life out of Academia?,https://www.reddit.com/r/MachineLearning/comments/3pts4i/how_can_a_phd_in_machine_learning_and_ai_help_you/,__null__,1445550259,"I have applied for a PhD in Computer Science. I do not however plan to become an academic afterwards. What I want to achieve is to try to commercialize my research by founding a startup or find a job outside of Academia, using the knowledge I will obtain for enhancing or building products/services for my employer.

What I would like ideally, is to achieve Academic as well as Industry expertise in machine learning and AI, through a job or by trying to found a startup with a related thematology. Becoming a professor is completely out of my goals. However I want to be able to study academic literature with ease.

Do you think my goal is reasonable, or a PhD is an overkill that would make me overqualified, loosing precious time in case I change my mind in the future ?

I already have an MSc that provided a machine learing and image analysis course. My Master's thesis was also heavily focused on these subjects. But the reason I want the PhD is to learn more things and receive more hands on experience about Machine Learning and Complex Event Analysis. I believe that this process will also enhance my critical skills and analytical thinking.

I understand this can get difficult and risky but I am willing to try.

Does my goal make any sense or do I need a reality check ? I need hard critique...

Is that a healthy reason for attending graduate school ?",15,12
459,2015-10-23,2015,10,23,7,3ptv9f,Improved looks? Algorithm for Image filters!,https://www.reddit.com/r/MachineLearning/comments/3ptv9f/improved_looks_algorithm_for_image_filters/,adeniyiks,1445551579,,0,0
460,2015-10-23,2015,10,23,8,3pu76u,tutorial on support vector machines for classification - with interactive plots,https://www.reddit.com/r/MachineLearning/comments/3pu76u/tutorial_on_support_vector_machines_for/,efavdb,1445556844,,2,15
461,2015-10-23,2015,10,23,9,3pubtv,Is there an implicit tradeoff between using sigmoid units with cross-entropy error vs other units with mean squared error,https://www.reddit.com/r/MachineLearning/comments/3pubtv/is_there_an_implicit_tradeoff_between_using/,LyExpo,1445558915,"I'm specifically thinking about the case of an autoencoder here.

In my experiments, and in what I've read, cross entropy cross function is preferable to mean squared error. On the other hand, relu and tanh are usually preferred to sigmoid units. But in order to use cross entropy error, we need outputs in the range [0,1], at least in the last layer, right?

Can someone please shed some light on this tradeoff? w
What have you found to work best together?",2,1
462,2015-10-23,2015,10,23,11,3pur53,Is unsupervised learning essentially just dimensional reduction?,https://www.reddit.com/r/MachineLearning/comments/3pur53/is_unsupervised_learning_essentially_just/,[deleted],1445566008,[deleted],4,0
463,2015-10-23,2015,10,23,11,3puw9e,What techniques are used in recommender systems to adapt a system to a user over time?,https://www.reddit.com/r/MachineLearning/comments/3puw9e/what_techniques_are_used_in_recommender_systems/,[deleted],1445568463,[deleted],2,2
464,2015-10-23,2015,10,23,11,3puwd8,"nh gi v my ht bi HiClean HC 70, model my ht bi cc hot ca HiClean hin nay",https://www.reddit.com/r/MachineLearning/comments/3puwd8/nh_gi_v_my_ht_bi_hiclean_hc_70_model_my/,yenphatseo01,1445568523,,0,0
465,2015-10-23,2015,10,23,12,3pv15r,hiclean my ht bi cng nghip cng ngh  | my ht bi cng nghip cng sut ln,https://www.reddit.com/r/MachineLearning/comments/3pv15r/hiclean_my_ht_bi_cng_nghip_cng_ngh__my/,dophamvan30541,1445570802,,0,1
466,2015-10-23,2015,10,23,13,3pvb9i,Squeeze the Memory Consumption of Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3pvb9i/squeeze_the_memory_consumption_of_deep_learning/,antinucleon,1445576329,,1,19
467,2015-10-23,2015,10,23,14,3pvgca,What is a good high school science project that involves machine learning?,https://www.reddit.com/r/MachineLearning/comments/3pvgca/what_is_a_good_high_school_science_project_that/,b-y-t-e,1445579374,"I'm currently a sophomore currently taking precalculus; over the summer I took part of Andrew Ng's Machine Learning course (up to week four). I think I'm leaning more towards a science project in which I develop a simple SVM based on data from clinical trials, but I'm not completely sure. Any help or links from people with experience would be appreciated. Thanks!",9,3
468,2015-10-23,2015,10,23,16,3pvnh6,thp gii nhit | tho gii nhit gi r,https://www.reddit.com/r/MachineLearning/comments/3pvnh6/thp_gii_nhit_tho_gii_nhit_gi_r/,dophamvan30541,1445584161,,0,0
469,2015-10-23,2015,10,23,16,3pvopz,DZ 300T Vacuum packing machine,https://www.reddit.com/r/MachineLearning/comments/3pvopz/dz_300t_vacuum_packing_machine/,dongfengpacking,1445585144,,1,1
470,2015-10-23,2015,10,23,16,3pvr0f,DZ 300 (New) Vacuum packing machine,https://www.reddit.com/r/MachineLearning/comments/3pvr0f/dz_300_new_vacuum_packing_machine/,dongfengpacking,1445586993,,1,1
471,2015-10-23,2015,10,23,17,3pvt2e,DZ 420T Vacuum packing machine,https://www.reddit.com/r/MachineLearning/comments/3pvt2e/dz_420t_vacuum_packing_machine/,dongfengpacking,1445588633,,1,1
472,2015-10-23,2015,10,23,18,3pvxfg,Is there a clustering algorithm for bookmarks based on title or content?,https://www.reddit.com/r/MachineLearning/comments/3pvxfg/is_there_a_clustering_algorithm_for_bookmarks/,hdmpmendoza,1445592090,"Hi there,

I realized that I have way too many bookmarks and after deleting some, I still want to have them very organized, and I need some kind of starting point.

My question is this: Is there out there in the wild an algorithm or application that organizes bookmarks based on name, title, or content? I was thinking unsupervised learning, but it also can be done with supervised learning using tags. Maybe [/r/MachineLearning!](https://reddit.com/r/MachineLearning) is not the best subreddit to ask this, so if anyone could point me in the right direction, where to ask, where to look, I would appreciate it.

Thanks in advanced",6,2
473,2015-10-23,2015,10,23,19,3pw0g5,DZQ 400B Vacuum packing machine,https://www.reddit.com/r/MachineLearning/comments/3pw0g5/dzq_400b_vacuum_packing_machine/,dongfengpacking,1445594532,,1,1
474,2015-10-23,2015,10,23,19,3pw1tc,Fingerprint based Attendance Machine,https://www.reddit.com/r/MachineLearning/comments/3pw1tc/fingerprint_based_attendance_machine/,iamsmeofindia1,1445595476,,0,1
475,2015-10-23,2015,10,23,19,3pw3o1,Machining Services and Manufacturing Marketplace - Landing Page,https://www.reddit.com/r/MachineLearning/comments/3pw3o1/machining_services_and_manufacturing_marketplace/,masumnetcox,1445596898,,0,1
476,2015-10-23,2015,10,23,19,3pw4cs,"Dewatering Pumps, Centrifugal and Diaphragm",https://www.reddit.com/r/MachineLearning/comments/3pw4cs/dewatering_pumps_centrifugal_and_diaphragm/,Esichoice,1445597382,,0,1
477,2015-10-23,2015,10,23,20,3pw8fu,Machine Learning Solution helps to fight fraud,https://www.reddit.com/r/MachineLearning/comments/3pw8fu/machine_learning_solution_helps_to_fight_fraud/,john_philip,1445600210,,0,0
478,2015-10-23,2015,10,23,21,3pwbri,The Number Of Data Scientists Has Doubled Over The Last 4 Years,https://www.reddit.com/r/MachineLearning/comments/3pwbri/the_number_of_data_scientists_has_doubled_over/,john_philip,1445602239,,31,44
479,2015-10-23,2015,10,23,21,3pwcp3,[1510.06096] When Are Nonconvex Problems Not Scary?,https://www.reddit.com/r/MachineLearning/comments/3pwcp3/151006096_when_are_nonconvex_problems_not_scary/,iori42,1445602825,,2,3
480,2015-10-23,2015,10,23,21,3pwf5f,Ideas for the implementation of Particle Swarm Optimization?,https://www.reddit.com/r/MachineLearning/comments/3pwf5f/ideas_for_the_implementation_of_particle_swarm/,aksareen,1445604121,"Can you tell me on which type of dataset PSO can be applied upon ? Where can I download such dataset ? What tools should I use for the same ? 
Also, how to analyze the results ?",6,0
481,2015-10-23,2015,10,23,21,3pwfi0,What is unsupervised learning?,https://www.reddit.com/r/MachineLearning/comments/3pwfi0/what_is_unsupervised_learning/,TheNotKnower,1445604301,"Is it learning without labels, or without an external labeler/supervisor?

It always seemed to me that it was learning without labels, which gave a relatively clear categorization of algorithms. SVM, MLP, decision trees, etc. = supervised; SOM, PCA, k-means, etc. = unsupervised. Unsupervised learning algorithms can be used for clustering, dimensionality reduction and visualization, but typically not for prediction. 

But it seems that there is also another interpretation, which is that it is just learning without an external supervisor. This also makes a lot of sense given the name. In this case, ""supervised learning algorithms"" (see above) could be used for unsupervised learning, e.g. by using observations from a future timestep as labels. 

Is one of these interpretations wrong, or is one perhaps used much more than the other? At this point, I'm often not sure what people mean when they talk about unsupervised learning.",8,0
482,2015-10-23,2015,10,23,23,3pwp7s,Episode 48 - Machine Learning Algorithms | TechTalk,https://www.reddit.com/r/MachineLearning/comments/3pwp7s/episode_48_machine_learning_algorithms_techtalk/,dstarnes,1445609057,,0,9
483,2015-10-23,2015,10,23,23,3pwpjn,ZNN - A Fast and Scalable Algorithm for Training 3D Convolutional Networks on Multi-Core and Many-Core Shared Memory Machines,https://www.reddit.com/r/MachineLearning/comments/3pwpjn/znn_a_fast_and_scalable_algorithm_for_training_3d/,improbabble,1445609201,,1,3
484,2015-10-23,2015,10,23,23,3pww9i,Help - R h2o package chooses wrong model,https://www.reddit.com/r/MachineLearning/comments/3pww9i/help_r_h2o_package_chooses_wrong_model/,TenshiS,1445612170,"I am trying to classify the MNIST Dataset using the deeplearning model from h2o in R, but the resulting model is (almost) always a regression one instead of a classification one - h2o decided for some reason to take away the ability of the user to tell the library which model to train (since h2o 3), since it now 'automatically infers' this from the input data - but it is faulty. Does anybody else have this problem? Is there a workaround?

My parameters look like this:

    dlmodel &lt;- h2o.deeplearning(x=2:785, y=1, training_frame=trainHex, validation_frame=validHex,
                                hidden=c(196, 392, 784), input_dropout_ratio = 0.2, epochs=0.1, activation=""RectifierWithDropout"")
",2,1
485,2015-10-24,2015,10,24,1,3px8l0,Google Brain residency program,https://www.reddit.com/r/MachineLearning/comments/3px8l0/google_brain_residency_program/,doomie,1445617330,,22,35
486,2015-10-24,2015,10,24,1,3px9z1,What is Machine Learning (specifically Amazon ML),https://www.reddit.com/r/MachineLearning/comments/3px9z1/what_is_machine_learning_specifically_amazon_ml/,[deleted],1445617884,[deleted],0,0
487,2015-10-24,2015,10,24,1,3pxbdk,How to use Mechanical Turk in combination with Amazon ML for dataset labelling,https://www.reddit.com/r/MachineLearning/comments/3pxbdk/how_to_use_mechanical_turk_in_combination_with/,Lancelot-du-Lac,1445618481,,1,1
488,2015-10-24,2015,10,24,2,3pxjn4,Sex Just got a Lot Steamier....Mr Algorithm for the Ladies!,https://www.reddit.com/r/MachineLearning/comments/3pxjn4/sex_just_got_a_lot_steamiermr_algorithm_for_the/,adeniyiks,1445621854,,0,1
489,2015-10-24,2015,10,24,4,3pxyef,Questions about stacked (denoising) autoencoders,https://www.reddit.com/r/MachineLearning/comments/3pxyef/questions_about_stacked_denoising_autoencoders/,pumpkin105,1445627948,"Hi,

I am currently experimenting with (S)DAE and have a few questions:

- When using dropout, the node values are lowered in the non-training phase so that the summed value of all nodes stays the same. When using denoising autoencoders, the input values are just set randomly to 0, when non-training the input is being put into the network without corruption. Why are the values not adjusted then like when using dropout?
- When I am training a second layer, are the inputs also set randomly to 0 or is this only done for the lowest input layer?
- An upper layer is trained with the hidden data of the lower layer. The lower layer is trained with tied weights, so the results are not as good as with untied weights. When the network is being unfolded and trained as a whole with untied weights, does the pretraining of the upper layers still make sense, since they have to learn from input which later don't appear anymore? Wouldn't it make more sense to pretrain one layer, then untie it and then train the next layer?
- When using dropout, in the unfolded phase, does it make sense to use dropout in the decoding part of the network?
- When corrupting the input, is there any heuristic which input to corrupt? Like corrupt input values that are 1 seldomly with a higher probability?
- From the experience I got so far, I noticed when visualizing data using PCA/tsne, the network does not need to have several layers, one layer is enough. Is one layer really enough to learn patterns from data and cluster/visualize data accordingly?
",3,1
490,2015-10-24,2015,10,24,4,3py1xa,Multiple GPUs support in theano?,https://www.reddit.com/r/MachineLearning/comments/3py1xa/multiple_gpus_support_in_theano/,AustinZhang,1445629440,"Does theano support multiple GPU for DNN/LSTM parallel training? How?

Now days, training LSTM on any ""serious tasks"" takes time. In my humble opinion, Multiple GPU parallization is a must feature.",6,6
491,2015-10-24,2015,10,24,5,3py6yl,Ask ML: Lift/Sales prediction,https://www.reddit.com/r/MachineLearning/comments/3py6yl/ask_ml_liftsales_prediction/,[deleted],1445631585,[deleted],1,0
492,2015-10-24,2015,10,24,6,3pyeow,Two Tenure-Track Assistant Professor Positions in Computer Science - University of Wyoming,https://www.reddit.com/r/MachineLearning/comments/3pyeow/two_tenuretrack_assistant_professor_positions_in/,jclune,1445634763,"Hello all, 

We are hiring! Please consider applying and/or spreading the word about these openings. 

Two Tenure-Track Assistant Professor Positions in Computer Science - University of Wyoming
 

Main Contact:  search@cs.uwyo.edu

 

The Computer Science Department at the University of Wyoming seeks applicants for two tenure-track Assistant Professors (positions #0450 and #2507) to start in August 2016. We seek individuals that will perform exciting, game-changing research. To enable new faculty to build their research labs, we offer generous startup packages, funding for multiple Ph.D. students, access to world-class supercomputing resources, and reduced teaching loads for pre-tenure faculty.

 

For #0450, preference will be given to candidates that research AI/Machine Learning, especially, but not limited to, Deep Learning (aka Deep Neural Networks). This hire is intended to amplify existing department expertise in this area. For #2507, preference will be given to candidates that research High Performance Computing, Cloud Computing, and/or Big Data including visualization. Exceptional candidates that research Cybersecurity are also encouraged to apply.

 

Candidates must have completed (or expect to complete by August 2016) a Ph.D. in Computer Science or a related area.  Expectations for new faculty include: establishing a vibrant, externally funded research program, teaching at the undergraduate and graduate levels, advising students, and service to the department and/or college. 

 

The Computer Science Department is part of the College of Engineering and Applied Science, and offers B.S., M.S., and Ph.D. degrees in Computer Science. Undergraduate programs are ABET accredited and include a concentration in Big Data. The University of Wyoming is closely affiliated with the NCAR-Wyoming Supercomputing  Center (NWSC) located 40 miles east of the University. NWSC is a petascale supercomputing center designed to maximize data-intensive science. Also, the university Advanced Research Computing Center houses and supports the Mount Moran computing cluster, which allows UW faculty to perform supercomputing experiments for free on unused computer cycles, as well as to purchase new supercomputing hardware and have it professionally housed and maintained by the Advanced Research Computing Center staff.  

 

The University of Wyoming is located in Laramie, WY, a lovely, small college town (population 30,000). Laramie is situated in a scenic location at the base of the Rockies. The Medicine Bow National Forest is nearby and provides quick access to hiking, biking, skiing, rock climbing, fishing, hunting, and camping. Nearby towns in Colorado (Ft. Collins and Denver) offer additional amenities, including an international airport with daily flights to and from the airport in Laramie.

 

The University of Wyoming is an Equal Employment Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status or any other characteristic protected by law and University policy. Please see www.uwyo.edu/diversity/fairness. We strongly encourage applications from women and other groups underrepresented in computer science.

 

The University conducts background investigations for all final candidates being considered for employment. Offers of employment are contingent upon the completion of the background check.

 

To ensure full consideration applications should be completed by January 10, 2016 though applications will be accepted until the position is filled.  Applications must include: i.) a curriculum vitae,  ii.) a statement of research interests, iii.) a statement of teaching interests, iv.) at least three letters of reference.  Submission of application materials must be made electronically through the EZFacultySearch system at www.cs.uwyo.edu/search.  Inquires should be made to: search@cs.uwyo.edu





Best regards,
Jeff Clune

Assistant Professor, Computer Science
Director, Evolving Artificial Intelligence Lab
University of Wyoming
jeffclune@uwyo.edu
jeffclune.com
",0,2
493,2015-10-24,2015,10,24,6,3pyfwu,"MIT's ""Data Science Machine""",https://www.reddit.com/r/MachineLearning/comments/3pyfwu/mits_data_science_machine/,carmichael561,1445635295,,3,4
494,2015-10-24,2015,10,24,6,3pygb3,Where do you post or look for faculty positions in Deep Learning/AI/ML?,https://www.reddit.com/r/MachineLearning/comments/3pygb3/where_do_you_post_or_look_for_faculty_positions/,jclune,1445635468,"Since we are hiring a faculty member in deep learning, it would be good to know where to advertise the job. Any suggestions are appreciated!

Best regards,
Jeff Clune

Assistant Professor, Computer Science
Director, Evolving Artificial Intelligence Lab
University of Wyoming
jeffclune@uwyo.edu
jeffclune.com
",2,10
495,2015-10-24,2015,10,24,8,3pyu8o,New general-purpose optimization algorithm promises order-of-magnitude speedups on some problems,https://www.reddit.com/r/MachineLearning/comments/3pyu8o/new_generalpurpose_optimization_algorithm/,Aruscher,1445641649,,16,108
496,2015-10-24,2015,10,24,8,3pyzbr,Has there been work on building a Recommender system that considers long term reward?,https://www.reddit.com/r/MachineLearning/comments/3pyzbr/has_there_been_work_on_building_a_recommender/,[deleted],1445644061,[deleted],1,0
497,2015-10-24,2015,10,24,9,3pz32r,"Has anyone tried using modern machine learning techniques to create a ""personality"" rather than an ability?",https://www.reddit.com/r/MachineLearning/comments/3pz32r/has_anyone_tried_using_modern_machine_learning/,simstim_addict,1445645820,"Naturally all the focus is on skills and abilities. I was just wondering if anyone was looking at creating personalities. I can't help but distinguish between Hume's passion and reason. Are the passions not formed in a complex manner also. Are they in any way related to pattern recognition?

Is recognising a cat in pictures anything like satisfying a desire? There is no perfect cat to find. There is no perfect desire to satisfy.

",6,0
498,2015-10-24,2015,10,24,9,3pz417,A Gentle Introduction to Randomized Decision Forests,https://www.reddit.com/r/MachineLearning/comments/3pz417/a_gentle_introduction_to_randomized_decision/,ramen2387,1445646298,,0,7
499,2015-10-24,2015,10,24,9,3pz5ze,"What are recurrent neural networks, deep convolutional neural networks and recursive neural networks, and what are the merits of each of them?",https://www.reddit.com/r/MachineLearning/comments/3pz5ze/what_are_recurrent_neural_networks_deep/,winstonl,1445647275,"I came across the terms from reading an article deep learning. I know the most basic types of neural nets. So, how are they different and when do you use them?",2,0
500,2015-10-24,2015,10,24,11,3pziar,New posts in /r/MachineLearning are all being downvoted.,https://www.reddit.com/r/MachineLearning/comments/3pziar/new_posts_in_rmachinelearning_are_all_being/,ithinkiwaspsycho,1445653585,I'm not sure if it's just a few people downvoting or everyone but not cool. Lots of these posts are people asking valid questions or just trying to learn.,8,1
501,2015-10-24,2015,10,24,13,3pzxg3,"Five Hundred Deep Learning Papers, Graphviz and Python",https://www.reddit.com/r/MachineLearning/comments/3pzxg3/five_hundred_deep_learning_papers_graphviz_and/,john_philip,1445662004,,0,7
502,2015-10-24,2015,10,24,14,3pzyyn,There should be a javascript AI that turns all the pictures of people naked,https://www.reddit.com/r/MachineLearning/comments/3pzyyn/there_should_be_a_javascript_ai_that_turns_all/,BenRayfield,1445662920,[removed],0,0
503,2015-10-24,2015,10,24,14,3pzzrz,What is Temporal Difference Learning? ELI5,https://www.reddit.com/r/MachineLearning/comments/3pzzrz/what_is_temporal_difference_learning_eli5/,minato3421,1445663445,,3,7
504,2015-10-24,2015,10,24,15,3q04x6,"What kinds of games or experiments could an AI do in a webpage that would attract millions of people to play with it, and whats the minimum intelligence needed?",https://www.reddit.com/r/MachineLearning/comments/3q04x6/what_kinds_of_games_or_experiments_could_an_ai_do/,BenRayfield,1445666878,"Chatbots are common, but words are too high dimensional, leaving most AIs at best a stupid friend who are annoying to talk to.

Visual effects are interesting, but only for a short time. You dont see [ElectricSheep](https://en.wikipedia.org/wiki/Electric_Sheep) growing in popularity or their algorithms getting really advanced. [Deepdream](https://en.wikipedia.org/wiki/DeepDream) does, but it takes too long to train, and whatever it is should work in realtime to attract the most attention of people playing with it.

http://theaigames.com and http://fightcodegame.com may be fun for AI programmers but dont hold much interest in people who are used to more action and deeper strategy in their games.

I'm not saying anything against AI itself. I find it very interesting to play with and try to make smarter. But I'm also looking for something average people will find interesting too.",2,0
505,2015-10-24,2015,10,24,16,3q0a0q,Multi-layer perceptron now in scikit-learn (link with documentation),https://www.reddit.com/r/MachineLearning/comments/3q0a0q/multilayer_perceptron_now_in_scikitlearn_link/,cast42,1445671003,,11,14
506,2015-10-24,2015,10,24,19,3q0nus,Machining Services Marketplace Machine Shops RFQ Manufacturing Sourcing,https://www.reddit.com/r/MachineLearning/comments/3q0nus/machining_services_marketplace_machine_shops_rfq/,masumnetcox,1445683509,,0,1
507,2015-10-24,2015,10,24,22,3q10pv,Keeping Track of Summer Schools,https://www.reddit.com/r/MachineLearning/comments/3q10pv/keeping_track_of_summer_schools/,siddharth-agrawal,1445693216,"I recently joined a Masters program in Computer Science, and would like to stay updated with the Machine Learning summer schools being organized; especially if the speakers are working in Deep Learning, like here: https://sites.google.com/site/deeplearningsummerschool/. How do I keep track of future events?",4,1
508,2015-10-25,2015,10,25,0,3q1b9n,Artificial Intelligence is the Dating Service Intelligence!,https://www.reddit.com/r/MachineLearning/comments/3q1b9n/artificial_intelligence_is_the_dating_service/,adeniyiks,1445698941,,0,1
509,2015-10-25,2015,10,25,0,3q1brm,"Google reports strong profit, says it's 'rethinking everything' around machine learning",https://www.reddit.com/r/MachineLearning/comments/3q1brm/google_reports_strong_profit_says_its_rethinking/,muktabh,1445699161,,7,191
510,2015-10-25,2015,10,25,2,3q1sr5,Books/articles on general post-2011 deep learning online?,https://www.reddit.com/r/MachineLearning/comments/3q1sr5/booksarticles_on_general_post2011_deep_learning/,Coffee2theorems,1445706668,"Are there any good books or articles available online that would cover modern deep learning, i.e. the stuff that came after the end of the unsupervised pre-training era? Y'know, ReLUs, dropout, and in general deep/recurrent nets trained directly using gradients. Especially if it covers more of the ""art"" side of it than the ""science"", because that's where research articles are lacking. Stuff like currently popular optimization methods for rNN/dNN's for instance. Did Hessian-Free optimization ever catch on, or is it still mostly plain stochastic gradient with minibatches? And so on. It's been half a decade by now, so there should be something, right?

I kind of liked the [ESL](http://statweb.stanford.edu/~tibs/ElemStatLearn/) book, but the way it completely skips the rNN/dNN area aside from some references to LeCun's old convolutional NN work makes it feel decidedly dated, despite being published in 2009. Some other texts are stuck in the Hinton era and that's not really an improvement.

If not online ones, then I suppose dead trees could in theory be useful references, if unwieldy.",2,0
511,2015-10-25,2015,10,25,2,3q1t9z,Recommendation for a book for getting one's hands dirty with Machine Learning algorithms?,https://www.reddit.com/r/MachineLearning/comments/3q1t9z/recommendation_for_a_book_for_getting_ones_hands/,needlzor,1445706896,"I am not talking about a book that tells me what scikit or R function to call to do x and y (most ""practical"" books seem to be more or less that) but one that I could read side-by-side with something like PRML and would give me more insight into the algorithms (even maybe providing a naive Python implementation) rather than the mathy jargon that is usually common for this kind of book. I don't have anything against the mathy jargon by itself, but I find that my knowledge ""sticks"" way better when going from intuition to theory than the other way around. 

The best candidate I had in mind was ""[Machine Learning: An Algorithmic Perspective](http://www.amazon.com/Machine-Learning-Algorithmic-Perspective-Recognition/dp/1420067184)"" but the reviews I read were pretty mixed, some even saying that it was stuffed with errors. Is there a better equivalent for that book and if there isn't, would I be throwing my money away by ordering a copy of it and working with it?",8,2
512,2015-10-25,2015,10,25,2,3q1wkz,Your favorite [Python] OCR library,https://www.reddit.com/r/MachineLearning/comments/3q1wkz/your_favorite_python_ocr_library/,bea_bear,1445708321,"I've been leaning towards Tesseract - open source and apparently very accurate. I'm a little overwhelmed by the number of Python wrappers there are. Anyone have a preference for one of these?

    &gt; pip search tesseract  
    ReadBot              - A delightful tesseract OCR module  
    TesseractTrainer     - A small framework taking over the manual tesseract  
                           training process described in the Tesseract Wiki  
    pyocr                - A Python wrapper for OCR engines (Tesseract, Cuneiform,  
                           etc)
    tesseract_sip        - A SIP-based python wrapper around libtesseract  
    tesserwrap           - Basic python bindings to the Tesseract C++ API  
    tesseract-ocr        - A Python wrapper for Tesseract  
    pytesseract          - Python-tesseract is a python wrapper for google's  
                           Tesseract-OCR  
    tightocr             - Thin and pleasant wrapper for Tesseract OCR.  
    tesserpy             - Python interface to the Tesseract library  

Do you have experience with optical-character recognition libraries? What is your favorite and why? What other tools (in e.g. C++, ROS, Python, R) do you like to use with it?

(Crossposted to /r/computervision/)",4,6
513,2015-10-25,2015,10,25,2,3q1yjh,The many possible ways of combining neuralnets with compute theory,https://www.reddit.com/r/MachineLearning/comments/3q1yjh/the_many_possible_ways_of_combining_neuralnets/,BenRayfield,1445709149,"When neuralnets are used with symmetric weights and their states observed as weighted coin flips, they are called boltzmann machines. These are great for working with bit vars.

The main limit of neuralnets is they are more of a computing format and lack external memory to save and load large amounts of data. Convolutional Neuralnets deal with that by running the same neuralnet centered on each pixel, so they can gradually talk to eachother across distance. Deepmind's kind of Neural Turing Machine uses a turing tape of small blocks of bits. Eurisko stored data in lisp lists, numbers, and derived its own datastructs.

Eurisko's datastructs, starting with lisp's basic datstructs, were most flexible because they were sparse. The others are more linear, even with multi-tape-turing. But with that flexibility came the usual problems of Turing Completeness including infinite loops.

Somewhere between, keeping the sparseness, but not the full power of lisp easily able to destroy itself, I expect to find a good combination of neuralnet and compute theory, something that merges statistical learning with data storage and sparse navigating through it. Example: immutable binary forest nodes, which lack lisp's ability to have cycles while still being sparse and potentially very high dimensional.",2,0
514,2015-10-25,2015,10,25,3,3q261i,10 keys to successful machine learning for developers,https://www.reddit.com/r/MachineLearning/comments/3q261i/10_keys_to_successful_machine_learning_for/,obi-nine,1445712287,,0,0
515,2015-10-25,2015,10,25,4,3q2ate,PyBrain LSTM Example resultin in ValueError:Attempted relative import in non-package,https://www.reddit.com/r/MachineLearning/comments/3q2ate/pybrain_lstm_example_resultin_in/,lucas_0,1445714342,"I'm trying to run an LSTM network for like two weeks now and I cant find a good framework to do so. I'm actually trying with PyBrain which has this directory hierarchy:

    pybrain/
        ...
        examples/
            ...
            supervised/
                ...
                neuralnets+svm/
                    ...
                    example_rnn.py    

 but I'm getting this relative import error:
    
    Traceback (most recent call last):
    File ""example_fnn.py"", line 14, in &lt;module&gt;
    from .datasets import generateGridData, generateClassificationData, plotData
    ValueError: Attempted relative import in non-package

 when make the call like this:

    Lucass-MacBook-Pro:neuralnets+svm lucaslourenco$ python example_fnn.py 
    
some of the answers about this same error say that I should make the call from the parent directory using the `-m` flag, like:

    Lucass-MacBook-Pro:pybrain lucaslourenco$ python -m examples.supervised.neuralnets+svm.example_fnn

When I do it, I get this:

    /Users/lucaslourenco/anaconda/bin/python: No module named examples.supervised.neuralnets+svm


 - Am I just doing a simple mistake on the `-m` flag call? 
 - There is a simple way of correcting this without making changes on the framework(you know how bad can be the results of modifying a framework)? 
 - There are other options of frameworks to run an LSTM example in OSX or W7, preferable in python?

Thank you!",4,0
516,2015-10-25,2015,10,25,6,3q2w7g,Accelerating Deep LSTM Net's Training Time,https://www.reddit.com/r/MachineLearning/comments/3q2w7g/accelerating_deep_lstm_nets_training_time/,LeavesBreathe,1445723803,"Hey Guys, 

Lately, I've been experimenting with pretty substantially deep neural nets:

1. Six to Ten LSTM Layers
2. Anywhere from 512 to 1024 Hidden Variables.

To clarify, chars *are not characters*, they are words. I take all the words in the text and cluster them. From that I assign a cluster id and a word id. This allows chars to be around 400. I used to do chars which is why the variable is named that way. 

For reference, my Keras model is below:
	number_of_layers = 6
	hidden_variables = 512
	chars = 400

	print('Build model...')
	model = Sequential()
	model.add(Embedding(input_dim=(len(chars)), output_dim=512, input_length=maxlen))
	model.add(LSTM(hidden_variables, return_sequences=True, input_shape=(maxlen, len(chars))))
	model.add(Dropout(dropout))
	for z in range(0,number_of_layers-2):
	    model.add(LSTM(hidden_variables, return_sequences=True))
	    model.add(Dropout(dropout))
	model.add(LSTM(hidden_variables, return_sequences=False))
	model.add(Dropout(dropout))
	model.add(Dense(len(chars))) #this is the length of your output array. 
	model.add(Activation('softmax'))
	print('you are about to compile')
	model.compile(loss='categorical_crossentropy', optimizer='adam')
	print('you finished compiling')


I am inputting sentences and predicting the next word. I have much training data (about 10M samples), and I end up doing a softmax over ~400 outputs. However, these large neural nets on a 980 Ti take about 9 days to train for 100 epochs. I'm trying to figure out ways to speed up testing so that I can estimate what parameters are best, and then do a full 9 training days.

My goal is to have each test to take at most 48 hours. Once the most promising configuration is observed, we'll run the full LSTM 9 day training. 

Here are a few ideas to speed things up:

Idea 1: Change all the LSTMS to GRU's -- GRU's are about 2x faster, but LSTM's seem to outperform them in the long run (as the number of epochs increase).

Idea 2: Change the learning rate on the Adam Optimizer from the standard 0.001 to 0.010. After about epoch 10, decrease the learning rate by half each time. Stop learning after epoch 20.

Idea 3: Decrease hidden variable numbers or LSTM layer numbers -- I dislike this option the most because I feel that the level of abstraction reached will be significantly lower. 

Are there any other ideas you guys can think of? Maybe I'm missing something obvious. I'm also considering buying another 980 TI to run two separate configurations at once. 

As it currently stands with the config above, it takes me about 9 days to get 100 epochs. 

Thanks!",14,7
517,2015-10-25,2015,10,25,8,3q39iy,Neural network trained on Game of Thrones series was used to generate this website.,https://www.reddit.com/r/MachineLearning/comments/3q39iy/neural_network_trained_on_game_of_thrones_series/,vkazemi,1445729927,,0,0
518,2015-10-25,2015,10,25,8,3q3agz,Training an AlexNet from scratch,https://www.reddit.com/r/MachineLearning/comments/3q3agz/training_an_alexnet_from_scratch/,abhikandoi2000,1445730367,"Are there some open source implementations for AlexNet. I did a little digging and have planned to start out with AlexNet for training a model using an open image data-set for specific mammal animal species.

Found this as one implementation using Theano: https://github.com/uoguelph-mlrg/theano_alexnet

What are some directional ways to train an AlexNet from scratch? And if there are ways to build up from existing models, how should one proceed?",2,1
519,2015-10-25,2015,10,25,9,3q3f4q,Predict using proportions instead of labels,https://www.reddit.com/r/MachineLearning/comments/3q3f4q/predict_using_proportions_instead_of_labels/,napsternxg,1445732626,"I am interested in solving a prediction problem. However, instead of hard labels like positive or negative for each instance in my data set, I actually have the counts of times that instance can be positive and times it can be negative. 

I have tried using a method where I take the label which has &gt; 90% proportion of the total counts as the true label and can train a model on that using any binary supervised machine learning model. However, I feel in this case I am throwing away a lot of information which doesn't have &gt; 90% proportion as well as information that something which is 90% positive is different from something which is 100% positive. 

Are there any methods which I can use to train my model which uses the full information present in the model? Either the raw counts or proportions.

One method I can think of is using the cross entropy error and instead of y being a one hot vector like [0, 1] for counts [10, 90], I can have y as [0.1, 0.9] and then back propagate the true cross entropy error as defined here https://en.wikipedia.org/wiki/Cross_entropy, 
which if of the form H(x) = -sum(p(x)log(q(x)))

Would this be the correct technique ? Are there any relevant papers discussing this technique ?

Any constructive answers will be appreciated. 

",4,2
520,2015-10-25,2015,10,25,15,3q4gwi,"How interesting is ML actually in practice, compared to learning it? Is it mainly trial and error and tedium?",https://www.reddit.com/r/MachineLearning/comments/3q4gwi/how_interesting_is_ml_actually_in_practice/,mlthrowaway2,1445754656,"I took a few Machine Learning modules at Cambridge as part of my four-year Engineering degree, from which I've just graduated. I can say that I found the theory very interesting - the maths is compelling and I loved how the concepts brought all the statistics and linear algebra that I had learnt before in one place.


HOWEVER, what I'm worried about is whether a job will be actually very intellectually stimulating at all. Looking at the student projects for the Stanford course, it seems like pretty much all of them involve trying to make a number go down (error rate) by changing other numbers (hyperparameters, variables) without really using much linear algebra or statistics to get those numbers. It just seems like they just try everything, and THEN in retrospect try to explain why this particular set of numbers worked best by saying, ah, yes, this makes sense because X. Like, if the opposite approach worked, they'd just say ah yes because Y. It was exactly like how some of my courseworks ended up being, and it just feels so meaningless! At the end of the day you don't really learn much from the work itself because everything is just probablys and maybes.


Is this kind of thing just fundamental in machine learning jobs? Do you ever really need to have understood deeply the mechanics that underpin the practice? I KNOW that, in order to choose an approach, you could theorise that SVNs would work better in this particular domain or something. But even in Kaggle competitions, looking at the postmortems by the winners, it turns out that it was just a lot of trial and error, and choosing a combination of different approaches, which might as well have been black boxes. And all the work is really just coding it all up, making it run fast, maybe writing a batch script to be able to use all the data, etc. There was something Andrew Ng said as well, about how the frontier now is not in theory, but simply in computing power, which further gives me the impression that in practice it's mainly just dark arts and trial and error.


I'm at the point now where I've graduated with my Masters in Engineering, and I must choose between going into industry as an engineer/machine learning engineer.. or to throw it all away and go into Medicine.


My biggest fear in life is having a job that feels meaningless. Data science in order to predict how often you should send push notifications feels utterly meaningless to me, and I'm not sure I could derive much pleasure from it unless it was particularly intellectually stimulating. Even if it did get results, and get you a number (""3 times a week!""), it just.. compared to diagnosing someone and helping them personally with an illness.. ugh.


I've also shadowed doctors enough over the past months to know that as a doctor I will be guaranteed to have a fundamentally meaningful job for the rest of my life, even with the paperwork and bureaucracy, because I would be directly helping individuals with their problems. It will just take another 4 years of medical school. (Money thankfully is not a problem and is the least of my worries. It's just wanting to impact the world in any way I can)

But I would like to think it's possible to get a meaningful job in Engineering too, and I have a natural affinity to the theories and the maths. People like Erik Bern inspire me to no end. Seeing how great Spotify's Discover Weekly is, how well it works for me personally, is a great motivation for ML. And there's still all this talk about how the world needs Data Scientists etc etc.

From talking to software engineers in industry, it seems like it's very possible to end up being a small cog helping a company with growth by doing tedious jobs. I would never ever want to do this, and would much prefer ML jobs where ML is actually part of the product itself (either recommendations engines, or robotics, or NLP etc), rather than just to help a company grow. Do many of these kinds of jobs exist, because I can't really find many, and if anyone is in this kind of job, can you tell me how it's like?

So yeah, it's a big decision to make, whether to stick with this and hope to be able to impact the world in a bigger way (but maybe end up not doing so at all), or to go into medicine and have a smaller but guaranteed impact. If anyone can tell me if my outlook on the tedium in Machine Learning jobs is accurate, and maybe tell me what the most rewarding part of the work is, it'd be appreciated!",60,84
521,2015-10-25,2015,10,25,17,3q4nac,Reinforcement Learning with Monte Carlo methods,https://www.reddit.com/r/MachineLearning/comments/3q4nac/reinforcement_learning_with_monte_carlo_methods/,outlacedev,1445760328,,0,9
522,2015-10-25,2015,10,25,17,3q4q9q,data science interview questions,https://www.reddit.com/r/MachineLearning/comments/3q4q9q/data_science_interview_questions/,evc123,1445763233,,2,1
523,2015-10-25,2015,10,25,18,3q4rs3,Working with midi data representations,https://www.reddit.com/r/MachineLearning/comments/3q4rs3/working_with_midi_data_representations/,CreativePunch,1445764651,"Hi all,

Lately I have been working with midi data but I have been struggling with a good and learnable representation of my data.

As far as I know right now I have two ways of representing a midi file.

1) sample every X ticks and use a 1 when a note is ON at the current step and a 0 when it is OFF.

Pros: The neural network needs only to learn binary output.
Cons: If sampled at too low rate, the input data is no longer representative of an actual song and it skips notes.
If sampled at too high rate, the data becomes extremely large, with for example a quarter note taking up 16 timesteps.

2) Use real numbers, where the first column of my data is the offset from the last note and the rest are the notes with their duration.
Pros: the data is now flawless AND much more compact, leaving any sampling out of the question.
Cons: the data is very precise. A note could have a duration of 3.750, 1.666..., 0.333... And so on, which makes the data harder to learn an autoencoding

I have been working a lot with the last option using ReLU output. And while my NN is apparently very quick at learning the rough structure of a song perhaps even better so than with binary representations, it has trouble getting to those exact values of offsets and note durations, especially in a VAE setting. So basicly I am looking for some pointers on other ways I might be able to represent my data. Preferably in a way so that it circumvents any sampling and thus corruption of input data.

Important side note: multiple notes can be on at the same time in my data (nottingham midi database, piano-midi,...)

Thanks!",4,1
524,2015-10-25,2015,10,25,21,3q54gd,What software supports multiple machines to speed up learning?,https://www.reddit.com/r/MachineLearning/comments/3q54gd/what_software_supports_multiple_machines_to_speed/,londons_explorer,1445775935,"I was trying to train char-rnn and it's taking ages!  I can run it on Amazon EC2, and they have hundreds of GPU machines for only cents per hour, yet I can't make all the machines work together on the same problem.

Is there an easy way in Torch7/Lua to split a large dataset across trainers?",12,1
525,2015-10-25,2015,10,25,23,3q5k7p,Finding S&amp;P 500 Clusters and Diversification with a Dendrogram,https://www.reddit.com/r/MachineLearning/comments/3q5k7p/finding_sp_500_clusters_and_diversification_with/,theMadcap,1445784983,,0,6
526,2015-10-26,2015,10,26,0,3q5lnu,Comparison of skip-thoughts model versus doc2vec in semantic relatedness (and other) tasks?,https://www.reddit.com/r/MachineLearning/comments/3q5lnu/comparison_of_skipthoughts_model_versus_doc2vec/,ranterskanter,1445785665,"I've read the skip-thoughts paper and was wondering if anyone has compared the performance of both models, and what people see as the main differences, similarities between them, and their relative strengths and weakness?",3,3
527,2015-10-26,2015,10,26,1,3q5uyz,Why is Deep Learning mostly applied to 'analog' inputs?,https://www.reddit.com/r/MachineLearning/comments/3q5uyz/why_is_deep_learning_mostly_applied_to_analog/,purpledazy,1445789934,"I got this impression (could be wrong), that deep learning is mostly use on problems with 'analog' inputs (e.g., images/sound), why is that? Why deep learning isn't used as often to solve feature based data problems (e.g., predicting sales/clicks/etc)?",7,2
528,2015-10-26,2015,10,26,1,3q5ys7,help with course selection,https://www.reddit.com/r/MachineLearning/comments/3q5ys7/help_with_course_selection/,whoisthiscrazyman,1445791556,"I got one more year of my math undergrad and i'm trying to plan what courses i'm going to take. I would like to either get a master's in machine learning or work in the field. Should I take probability (rigorous), measure theory, statistical inference (Casella &amp; Berger), or should I machine learning specific courses? I would like to do all of the above but I won't be able to fit it all in one year. What should be my priority? ",2,0
529,2015-10-26,2015,10,26,2,3q6175,I'm a theoretical physicist who just made it past the first round of cuts for the Data Incubator fellowship. AMA!,https://www.reddit.com/r/MachineLearning/comments/3q6175/im_a_theoretical_physicist_who_just_made_it_past/,[deleted],1445792549,"I've tried a few times for these ""get STEM people into data science"" fellowships (I'm looking at you Insight) with no luck. Maybe I can help you figure out what you need to do to get into one of these programs, or how to capture the attention of this industry.",13,2
530,2015-10-26,2015,10,26,4,3q6oe2,What a Deep Neural Network thinks about your #selfie,https://www.reddit.com/r/MachineLearning/comments/3q6oe2/what_a_deep_neural_network_thinks_about_your/,vkhuc,1445802424,,32,211
531,2015-10-26,2015,10,26,7,3q7eis,Training Deep Net on 14 Million Images by Using A Single Machine,https://www.reddit.com/r/MachineLearning/comments/3q7eis/training_deep_net_on_14_million_images_by_using_a/,antinucleon,1445813137,,9,32
532,2015-10-26,2015,10,26,8,3q7n76,Why hasn't there been an AMA in so long?,https://www.reddit.com/r/MachineLearning/comments/3q7n76/why_hasnt_there_been_an_ama_in_so_long/,mrdrozdov,1445816901,The previous maximum number of months without an AMA was about 4 months. We've already hit 5 months.,4,0
533,2015-10-26,2015,10,26,9,3q7pjc,Why is removing the mean pixel value from each image making training more difficult?,https://www.reddit.com/r/MachineLearning/comments/3q7pjc/why_is_removing_the_mean_pixel_value_from_each/,LyExpo,1445817881,"I am playing with the MNIST data set. I have read that it is often useful to remove the mean pixel value from each input image. I am doing this in python:

    # X is the (50000, 784) MNIST dataset
    X = (X.T - np.mean(X, axis=1).T).T    # remove mean from each example
    X = X - np.min(X)                             # make all values positive
    X = X / np.max(X)                             # scale to [0,1]

I'm putting this dataset into an autoencoder, and strangely, my costs are much higher than they were before doing this pre-processing step. Also, the reconstructed images don't look as good.

Can someone explain what is going on here?",11,2
534,2015-10-26,2015,10,26,10,3q81eh,Unbalanced classes in one-vs-rest scheme,https://www.reddit.com/r/MachineLearning/comments/3q81eh/unbalanced_classes_in_onevsrest_scheme/,scttnlsn,1445823148,"When performing a one-vs-rest multi-class classification scheme the classifier will likely see far more negative samples (i.e. the rest).  Is it common to account for this, perhaps by subsampling the negative samples?  Or even subsampling multiple times and averaging the results?",4,0
535,2015-10-26,2015,10,26,10,3q83i1,Understanding adaptive learning rates in LeCun's Efficient BackProp paper,https://www.reddit.com/r/MachineLearning/comments/3q83i1/understanding_adaptive_learning_rates_in_lecuns/,koormoosh,1445824080,"I am trying to understand the relation between eigenvalues/eigenvectors and the optimization problem. I've explored the space of web and understood the relation between Hessian's eigenvalue and concavity of the objective function, but still cannot understand (visualize in my brain) the argument on the second paragraph of page 15 of LeCun's Efficient BackProp paper: ""... the parameter vector w(t) will approach the minimum from the direction of the minimum eigenvector of the Hessian ...""",0,4
536,2015-10-26,2015,10,26,12,3q8hdw,A less math-focussed machine learning course/tutorials?,https://www.reddit.com/r/MachineLearning/comments/3q8hdw/a_less_mathfocussed_machine_learning/,Troll_Random,1445830514,"Most coursera courses on machine learning are too mathy for me. Are there any courses, tutorials or books on ML that not too mathy, but rather focus on practical applications?

I have tried Ng's course, and it feels a bit too mathy even though this /r/ says it's not.",6,0
537,2015-10-26,2015,10,26,13,3q8mx9,nh gi v cht lng my ht bi HiClean HC 30,https://www.reddit.com/r/MachineLearning/comments/3q8mx9/nh_gi_v_cht_lng_my_ht_bi_hiclean_hc_30/,yenphatseo01,1445833369,,0,0
538,2015-10-26,2015,10,26,13,3q8o3x,"a ch sa cha, bo dng thp gii nhit Liang Chi uy tn | thp gii nhit Liang Chi",https://www.reddit.com/r/MachineLearning/comments/3q8o3x/a_ch_sa_cha_bo_dng_thp_gii_nhit_liang/,dophamvan30541,1445834027,,0,0
539,2015-10-26,2015,10,26,14,3q8s56,Machine Learning Quiz for Developers,https://www.reddit.com/r/MachineLearning/comments/3q8s56/machine_learning_quiz_for_developers/,srinimf,1445836302,,1,1
540,2015-10-26,2015,10,26,14,3q8ttq,Why Machine Learning will dominate Video Marketing,https://www.reddit.com/r/MachineLearning/comments/3q8ttq/why_machine_learning_will_dominate_video_marketing/,john_philip,1445837402,,0,0
541,2015-10-26,2015,10,26,14,3q8w47,Here are some free datasets of video game reviews from the Steam website,https://www.reddit.com/r/MachineLearning/comments/3q8w47/here_are_some_free_datasets_of_video_game_reviews/,mulhod,1445838934,,3,3
542,2015-10-26,2015,10,26,15,3q8yfn,Alternatives to Kaggle,https://www.reddit.com/r/MachineLearning/comments/3q8yfn/alternatives_to_kaggle/,DM_walker,1445840468,"About www.kaggle.com : I think it is absolutely normal practice to have three accounts for one participant: 1) one for forum, 2) another one for experiments and 3) the last one for presenting results.",7,0
543,2015-10-26,2015,10,26,15,3q90p2,Sadly Microsoft's 'Common objects in context' dataset does not include a category for phallic symbolism...,https://www.reddit.com/r/MachineLearning/comments/3q90p2/sadly_microsofts_common_objects_in_context/,insperatum,1445841990,,0,0
544,2015-10-26,2015,10,26,17,3q98ye,What a Deep Neural Network thinks about your selfie - Andrej Karpathy,https://www.reddit.com/r/MachineLearning/comments/3q98ye/what_a_deep_neural_network_thinks_about_your/,visarga,1445848104,,0,0
545,2015-10-26,2015,10,26,18,3q9bqr,Balancing Precision and Recall in Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3q9bqr/balancing_precision_and_recall_in_neural_networks/,Mattoss,1445850303,"I have a classification setting in which my neural network has high Precision, but low Recall. Since my data is highly imbalanced (ca 5% positive labels), I understand why this might be the case. Apart from techniques that influence my training data (e.g. sampling from underrepresented classes), are there any ways to more directly influence precision and recall in this setting? 
I'd be thankful for ideas to pursue. ",9,6
546,2015-10-26,2015,10,26,18,3q9cei,CodaLab - open-source platform that provides an ecosystem for conducting computational research,https://www.reddit.com/r/MachineLearning/comments/3q9cei/codalab_opensource_platform_that_provides_an/,datasci_,1445850841,,0,1
547,2015-10-26,2015,10,26,20,3q9lyg,Why are neutral nets considered so much sexier in the ML community compared to other function approximation techniques?,https://www.reddit.com/r/MachineLearning/comments/3q9lyg/why_are_neutral_nets_considered_so_much_sexier_in/,fuckinghelldad,1445857995,,15,2
548,2015-10-26,2015,10,26,22,3q9zx7,[1502.00524] Unsupervised Incremental Learning and Prediction of Music Signals,https://www.reddit.com/r/MachineLearning/comments/3q9zx7/150200524_unsupervised_incremental_learning_and/,mttd,1445866084,,1,15
549,2015-10-26,2015,10,26,23,3qa6as,Learning 3D shape,https://www.reddit.com/r/MachineLearning/comments/3qa6as/learning_3d_shape/,basedgodel,1445869111,,2,11
550,2015-10-26,2015,10,26,23,3qa6nd,A Framework for Distributed Deep Learning Layer Design in Python,https://www.reddit.com/r/MachineLearning/comments/3qa6nd/a_framework_for_distributed_deep_learning_layer/,clmcl,1445869284,,0,2
551,2015-10-26,2015,10,26,23,3qa7th,Have spiking neural networks achieved truly state-of-art results in anything?,https://www.reddit.com/r/MachineLearning/comments/3qa7th/have_spiking_neural_networks_achieved_truly/,olBaa,1445869785,,6,6
552,2015-10-26,2015,10,26,23,3qa9kn,How can we use Machine Learning to understand the huge amounts of data created by the IoT? We'll explore at REWORK Connect Summit next month!,https://www.reddit.com/r/MachineLearning/comments/3qa9kn/how_can_we_use_machine_learning_to_understand_the/,reworksophie,1445870556,,0,1
553,2015-10-27,2015,10,27,0,3qadhs,New Deep Learning Library from IDSIA,https://www.reddit.com/r/MachineLearning/comments/3qadhs/new_deep_learning_library_from_idsia/,pranv,1445872203,,17,48
554,2015-10-27,2015,10,27,0,3qaiqq,Deep Learning for NLP resources,https://www.reddit.com/r/MachineLearning/comments/3qaiqq/deep_learning_for_nlp_resources/,arthomas73,1445874292,,0,16
555,2015-10-27,2015,10,27,0,3qajoe,Evolutionary Neuron Connectivity,https://www.reddit.com/r/MachineLearning/comments/3qajoe/evolutionary_neuron_connectivity/,tehgargoth,1445874657,I was wondering if anyone could help me find anyone who has done research on using evolutionary algorithms to generate non-layer based map of interconnected neurons for essentially optimized complex node/neuron/recurrent structures,7,11
556,2015-10-27,2015,10,27,1,3qatb1,Google Turning Its Lucrative Web Search Over to AI Machines,https://www.reddit.com/r/MachineLearning/comments/3qatb1/google_turning_its_lucrative_web_search_over_to/,modeless,1445878298,,26,85
557,2015-10-27,2015,10,27,2,3qawub,"6 Biggest Misconceptions About Data Scientists, Data Engineers and Business Analysts.",https://www.reddit.com/r/MachineLearning/comments/3qawub/6_biggest_misconceptions_about_data_scientists/,[deleted],1445879615,[deleted],0,1
558,2015-10-27,2015,10,27,2,3qaywj,Meet RankBrain: The Artificial Intelligence That's Now Processing Google Search Results,https://www.reddit.com/r/MachineLearning/comments/3qaywj/meet_rankbrain_the_artificial_intelligence_thats/,dabshitty,1445880381,,0,13
559,2015-10-27,2015,10,27,4,3qbens,Amazon Kinesis: managed real-time event processing,https://www.reddit.com/r/MachineLearning/comments/3qbens/amazon_kinesis_managed_realtime_event_processing/,[deleted],1445886338,[deleted],0,0
560,2015-10-27,2015,10,27,4,3qblur,"6 Biggest Misconceptions About Data Scientists, Data Engineers and Business Analysts.",https://www.reddit.com/r/MachineLearning/comments/3qblur/6_biggest_misconceptions_about_data_scientists/,[deleted],1445889022,[deleted],0,1
561,2015-10-27,2015,10,27,4,3qblxn,Robot Love!,https://www.reddit.com/r/MachineLearning/comments/3qblxn/robot_love/,adeniyiks,1445889049,,0,1
562,2015-10-27,2015,10,27,5,3qbpph,Application that analyzes emotions in text based on Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3qbpph/application_that_analyzes_emotions_in_text_based/,[deleted],1445890443,[removed],0,1
563,2015-10-27,2015,10,27,5,3qbu18,Help understanding convolutional network architecture from a blog post,https://www.reddit.com/r/MachineLearning/comments/3qbu18/help_understanding_convolutional_network/,learnin_no_bully_pls,1445892116,"I'm want to do some audio classification with a convnet and I found this blog post: http://benanne.github.io/2014/08/05/spotify-cnns.html

In the main post:

&gt;Note that all these convolutions are one-dimensional; the convolution happens only in the time dimension, not in the frequency dimension.

This is in reply of a comment:

&gt;No, the spectrogram is processed by a 1D convolutional layer with 256 filters. So the first 4 frames (4 x 128) are convolved with 256 filters that are also 4 x 128 in shape, giving rise to 1 x 256 outputs. Because this is a convolution, we get this over the entire time axis, so the input is 599 x 128 and the output is 596 x 256. This is then max-pooled to 149 x 256 (pool size 4).

My knowledge of neural networks is limited to using libraries and I don't understand much of the theory behind it, so I don't understand how this is a 1D convolution but uses a 2D shape (4x128). An example of this first layer in any python library/torch would probably clear up my confusion.",1,0
564,2015-10-27,2015,10,27,5,3qbw1q,What personal data could we gather that would let us train useful ML algorithms?,https://www.reddit.com/r/MachineLearning/comments/3qbw1q/what_personal_data_could_we_gather_that_would_let/,rasputin48,1445892894,"Apologies for a sort of strange question.  But suppose I had 1000's of hours of video of myself and recorded conversation audio, pictures of everything I ate, tracking data of my location etc. for years.

Do you foresee data like this either 1. Being useful for future machine learning algs I may use, for example a future version of Siri or whatever being able to understand me better, or 2. Being something of potential value to someone else that they may actually pay money for (exceeding the terabyte hard drives needed to store it)? ",6,0
565,2015-10-27,2015,10,27,6,3qc3rc,Good book on statistical time series analysis,https://www.reddit.com/r/MachineLearning/comments/3qc3rc/good_book_on_statistical_time_series_analysis/,[deleted],1445895853,[deleted],0,1
566,2015-10-27,2015,10,27,9,3qcq9w,Parallelizing SGD algorithm - Async SGD,https://www.reddit.com/r/MachineLearning/comments/3qcq9w/parallelizing_sgd_algorithm_async_sgd/,Jxieeducation,1445904885,"http://blog.dato.com/parallel-ml-with-hogwild

Enjoy",0,8
567,2015-10-27,2015,10,27,9,3qcwoh,ConvNet question - How come the parameters/weights can be visualized?,https://www.reddit.com/r/MachineLearning/comments/3qcwoh/convnet_question_how_come_the_parametersweights/,apple-sauce,1445907563,"Each [filter](http://cs231n.github.io/assets/cnn/weights.jpeg) represents the weights that are learned right? However, I don't fully understand how (or why) the weights can be visualized as 'images.'

Thank you!



Picture taken from [here](http://cs231n.github.io/convolutional-networks/).",15,5
568,2015-10-27,2015,10,27,11,3qd6kn,Google Views Itself Through Machine Learning Lens | Emerging Tech,https://www.reddit.com/r/MachineLearning/comments/3qd6kn/google_views_itself_through_machine_learning_lens/,shugert,1445911700,,0,1
569,2015-10-27,2015,10,27,12,3qdemc,Difference in using Graph Structures vs Tabular data?,https://www.reddit.com/r/MachineLearning/comments/3qdemc/difference_in_using_graph_structures_vs_tabular/,systemd0wn,1445915410,"I'm very new to ML concepts and have been taking the Coursera Machine Learning signature track. We're currently covering high level overviews of different ML case studies. In the course we use Dato, a commercial ML library (although they suggest Pandas as an alternative). Previous to this I had done research into Graph Databases like neo4j. Can you utilize ML libraries like these over graph structures instead of tabular ones? Are other similar tools available to accomplish the same things? I'm very new to this so I appreciate anything you can add to the discussion.",0,2
570,2015-10-27,2015,10,27,14,3qdw5d,Apple poaches Nvidia's deep learning director to fuel automotive plans,https://www.reddit.com/r/MachineLearning/comments/3qdw5d/apple_poaches_nvidias_deep_learning_director_to/,john_philip,1445924882,,3,4
571,2015-10-27,2015,10,27,15,3qe1vk,Speeding Up Spark With Native Binaries,https://www.reddit.com/r/MachineLearning/comments/3qe1vk/speeding_up_spark_with_native_binaries/,blueeyes44,1445928809,,1,2
572,2015-10-27,2015,10,27,15,3qe28b,Application that analyzes emotions in text based on Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3qe28b/application_that_analyzes_emotions_in_text_based/,EmoshioQuant,1445929080,[removed],0,1
573,2015-10-27,2015,10,27,16,3qe2se,Estimating Delivery Times: A Case Study In Practical Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3qe2se/estimating_delivery_times_a_case_study_in/,arrowoftime,1445929498,,4,18
574,2015-10-27,2015,10,27,17,3qe7t8,Numenta HTM Challenge,https://www.reddit.com/r/MachineLearning/comments/3qe7t8/numenta_htm_challenge/,hassaanz,1445933616,,10,6
575,2015-10-27,2015,10,27,17,3qeao6,"Recurrent Neural Network Tutorial, Part 4  Implementing a GRU/LSTM RNN with Python and Theano",https://www.reddit.com/r/MachineLearning/comments/3qeao6/recurrent_neural_network_tutorial_part_4/,pogopuschel_,1445936054,,0,53
576,2015-10-27,2015,10,27,18,3qedfd,Crash Course on Learning Theory - MSR,https://www.reddit.com/r/MachineLearning/comments/3qedfd/crash_course_on_learning_theory_msr/,mr_robot_elliot,1445938276,,0,22
577,2015-10-27,2015,10,27,18,3qeepw,I developed an app for iOS that allows you to use Torch. Help me to improve it!,https://www.reddit.com/r/MachineLearning/comments/3qeepw/i_developed_an_app_for_ios_that_allows_you_to_use/,MBernava,1445939300,,0,1
578,2015-10-27,2015,10,27,18,3qefka,Deep Minds: An Interview with Google's Alex Graves &amp; Koray Kavukcuoglu #reworkDL,https://www.reddit.com/r/MachineLearning/comments/3qefka/deep_minds_an_interview_with_googles_alex_graves/,[deleted],1445939951,[deleted],0,1
579,2015-10-27,2015,10,27,19,3qeic7,Why Machine Learning? What inspired and fascinates you about it?,https://www.reddit.com/r/MachineLearning/comments/3qeic7/why_machine_learning_what_inspired_and_fascinates/,iveaname,1445942025,"Hi, i was recently thinking about how and why i decided on  machine learning of all other fields. So i thought i will ask you guys as well, what are your reasons?",7,1
580,2015-10-27,2015,10,27,19,3qejud,Deep Learning for Computer Vision with MATLAB and cuDNN,https://www.reddit.com/r/MachineLearning/comments/3qejud/deep_learning_for_computer_vision_with_matlab_and/,harrism,1445943110,,1,8
581,2015-10-27,2015,10,27,23,3qfa64,What is a language model? NLP definition.,https://www.reddit.com/r/MachineLearning/comments/3qfa64/what_is_a_language_model_nlp_definition/,rhnam,1445956717,"So quick question about how to define a language model? I'm under the assumption that a language model is how to represent the text as features, such as a unigram, bigram or trigram (any order n-gram) or any other way to generate features. Is this correct and if not what's a simple definition of a language model? Thanks!",3,0
582,2015-10-27,2015,10,27,23,3qfd0g,Crowdsourcing and Machine Learning at Pinterest,https://www.reddit.com/r/MachineLearning/comments/3qfd0g/crowdsourcing_and_machine_learning_at_pinterest/,gregory_k,1445957896,,0,0
583,2015-10-28,2015,10,28,0,3qfg9l,On End-to-End Program Generation from User Intention by Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3qfg9l/on_endtoend_program_generation_from_user/,downtownslim,1445959190,,6,19
584,2015-10-28,2015,10,28,0,3qfh06,Facebook plans to roll out automatic image captioning using deep learning,https://www.reddit.com/r/MachineLearning/comments/3qfh06/facebook_plans_to_roll_out_automatic_image/,gwern,1445959475,,13,74
585,2015-10-28,2015,10,28,0,3qfjd0,New IBM Watson Partners interesting use in content marketing.,https://www.reddit.com/r/MachineLearning/comments/3qfjd0/new_ibm_watson_partners_interesting_use_in/,luzeck,1445960374,,3,1
586,2015-10-28,2015,10,28,1,3qfmog,"New Q&amp;A on our blog today with Google DeepMind senior research scientists Alex Graves &amp; Koray Kavukcuoglu discussing neural Turing machines, reinforcement learning, personal thoughts on deep learning &amp; more",https://www.reddit.com/r/MachineLearning/comments/3qfmog/new_qa_on_our_blog_today_with_google_deepmind/,reworksophie,1445961670,,0,1
587,2015-10-28,2015,10,28,1,3qfofl,Networks for Unordered Collections?,https://www.reddit.com/r/MachineLearning/comments/3qfofl/networks_for_unordered_collections/,LordTocs,1445962376,"Essentially I have an unknown number of disjoint input vectors in a collection all relating to a desired output. Their order isn't important and once I start handing it real data I wont have a way to sort it into an order which is important (without getting real creative).

RNNs are exceptional at processing ordered n length sequences of vectors. Are there networks that are similar but for unordered n length collections of vectors?
",5,0
588,2015-10-28,2015,10,28,1,3qfrcc,Deep Learning for Fashion (feedback from /r/ML on our method appreciated),https://www.reddit.com/r/MachineLearning/comments/3qfrcc/deep_learning_for_fashion_feedback_from_rml_on/,[deleted],1445963476,[deleted],0,0
589,2015-10-28,2015,10,28,1,3qfswm,Mitsubishi Electric uses machine-learning tech to detect distracted drivers,https://www.reddit.com/r/MachineLearning/comments/3qfswm/mitsubishi_electric_uses_machinelearning_tech_to/,shugert,1445964073,,0,1
590,2015-10-28,2015,10,28,2,3qfw3k,Google Prediction API: a Machine Learning black box for developers,https://www.reddit.com/r/MachineLearning/comments/3qfw3k/google_prediction_api_a_machine_learning_black/,[deleted],1445965252,[deleted],0,1
591,2015-10-28,2015,10,28,2,3qfzjq,Using Word2Vec to determine which famous author you write like,https://www.reddit.com/r/MachineLearning/comments/3qfzjq/using_word2vec_to_determine_which_famous_author/,andre_beton,1445966557,,6,0
592,2015-10-28,2015,10,28,2,3qg26e,Are there any standard books which can serve as an introduction to Neural Networks?,https://www.reddit.com/r/MachineLearning/comments/3qg26e/are_there_any_standard_books_which_can_serve_as/,eygrr,1445967538,"I've had a look around and found a few different ones, but they all appear to have slightly different focuses and I'm not entirely sure how in-date they are, or even how in-date they have to be for me to find valuable information about neural networks within them.

So far I've found: 

- An Introduction to Neural Networks (8th ed) (Ben Krose)

- Neural Networks: A Systematic Introduction (Raul Rojas)

- A Brief Introduction to Neural Networks (David Kriesel)

I was just wondering if there is a more recent, perhaps more standard book that can serve as learning material for understanding the theory behind what neural networks are, without being fed out of date information from potentially long forgotten practices.

Which books would you recommend for this?",10,1
593,2015-10-28,2015,10,28,2,3qg4a7,[Question] For Simulated Annealing of Neural Network weights...,https://www.reddit.com/r/MachineLearning/comments/3qg4a7/question_for_simulated_annealing_of_neural/,umib0zu,1445968317,"should I modify just a single weight in a single node for each step, or modify more weights?",8,2
594,2015-10-28,2015,10,28,7,3qhc65,Get matched with a machine learning mentor,https://www.reddit.com/r/MachineLearning/comments/3qhc65/get_matched_with_a_machine_learning_mentor/,petesoder,1445984754,,0,0
595,2015-10-28,2015,10,28,7,3qhe46,Video / Post | Machine Learning In Fintech/Credit,https://www.reddit.com/r/MachineLearning/comments/3qhe46/video_post_machine_learning_in_fintechcredit/,Friars1993,1445985537,,0,1
596,2015-10-28,2015,10,28,8,3qhoha,"David Blei ""Modern Machine Learning for Science"" [talk]",https://www.reddit.com/r/MachineLearning/comments/3qhoha/david_blei_modern_machine_learning_for_science/,AmusementPork,1445989756,,1,3
597,2015-10-28,2015,10,28,10,3qi6j6,[HELP]Rookie guy trying to get the mean with a Regression Tree,https://www.reddit.com/r/MachineLearning/comments/3qi6j6/helprookie_guy_trying_to_get_the_mean_with_a/,[deleted],1445997316,[deleted],0,0
598,2015-10-28,2015,10,28,12,3qilar,Parallelizing Stochastic Gradient Descent,https://www.reddit.com/r/MachineLearning/comments/3qilar/parallelizing_stochastic_gradient_descent/,Jxieeducation,1446004051,"http://jxieeducation.com/2015-10-27/Async-Stochastic-Gradient-Descent/
",0,0
599,2015-10-28,2015,10,28,15,3qj3ak,"Lecture given at Stanford on ""deep neural networks"" (modeling brain visual processing)",https://www.reddit.com/r/MachineLearning/comments/3qj3ak/lecture_given_at_stanford_on_deep_neural_networks/,pobm2f,1446014869,,0,15
600,2015-10-28,2015,10,28,18,3qjhql,"Data Science for Losers, Part 4 - Machine Learning",https://www.reddit.com/r/MachineLearning/comments/3qjhql/data_science_for_losers_part_4_machine_learning/,brakmic,1446026174,,0,5
601,2015-10-28,2015,10,28,19,3qjjx8,Nice course to machine learning,https://www.reddit.com/r/MachineLearning/comments/3qjjx8/nice_course_to_machine_learning/,Sunshine_Reggae,1446027745,,5,134
602,2015-10-28,2015,10,28,19,3qjkzk,Test instances for regression-solving algorithm,https://www.reddit.com/r/MachineLearning/comments/3qjkzk/test_instances_for_regressionsolving_algorithm/,nvrslnc,1446028496,"I am currently working on an implementation of the Alternating Direction Method of Multiplier (ADMM), specifically for problems involving l1-norm.
I would like to test my algorithm on a big data set. Is there a benchmark for these kind of problems?",0,2
603,2015-10-28,2015,10,28,20,3qjq40,how using Pipeline for multiple features processing for classification [scikit learn],https://www.reddit.com/r/MachineLearning/comments/3qjq40/how_using_pipeline_for_multiple_features/,gillesa,1446031977,"Hey guys.

I have a Dataframe (pandas) with multiple string columns.

Columns description : 
- description : text =&gt; ""I like it, i was very happy about it and my friend have the same opinion""
- label : text ==&gt; ""restaurant""
- target : 0 or 1

So i try use some classification and predict target.

I first only use ""description"" column with this processing:
1. remove stopword
2. remove ponctuation
3. stem word
4. using CountVectorizer
5. using TfidfTransformer
6. train LogisticRegression

step 1, 2 &amp; 3 are done by a function : clear_text(text).
clear_text(""""I like it, i was very happy about it and my friend have the same opinion"") return ""like happi friend opinion""

step 4, 5, 6 are done by pipeline :
clf = Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('clf', LogisticRegression()),
])

Then i use GridSearchCV to tune my parameters.

Result are quit good ! But i would like to add ""label"" feature.

As ""label"" is a nominal feature (ie : len(df.label.unique()) : 119). i procession it like to feed my model :
1. LabelEncoder()
2. OneHotEncoder()

Question : 

How can i use Pipeline process and GridSearchCV with this 2 different features processing ?

Thank guys ! :)
",3,2
604,2015-10-28,2015,10,28,21,3qjtx2,Does this way of combining beliefs have a name?,https://www.reddit.com/r/MachineLearning/comments/3qjtx2/does_this_way_of_combining_beliefs_have_a_name/,[deleted],1446034299,[deleted],3,0
605,2015-10-28,2015,10,28,22,3qk498,"Borrowed 2nd ed. Machine Learning: An Algorithmic Perspective, by Stephen Marsland from the library and want to ask if the 2nd edition rectifies the shortcomings of the first edititon before buying it.",https://www.reddit.com/r/MachineLearning/comments/3qk498/borrowed_2nd_ed_machine_learning_an_algorithmic/,jollybobbyroger,1446039332,"I've only read ~100 pages, but I really find it well written, since it discusses relatively complex concepts in a very clear and concise way.  

The math is present, but the author does his best to make it clear and comprehensible, something I find 90% of all articles that discusses math either does not even attempt or completely fails to do.  

I also like the code examples even though I have some minor gripes with the python coding style, but nothing that gets in the way of communicating the main point.

**My main point**, I have read reviews of the previous edition, stating that there are quite a lot of flaws in the first edition. I would like to know if these flaws have been completely rectified in this latest edition.  

**Secondly**, are there any other introductory books on ML that are as well written as this and does a good job of presenting the math in a really pedagogic manner? I want to know, because Marsland's book is quite expensive, so I only want to spend this kind of money on one introductory book on ML.
",2,1
606,2015-10-29,2015,10,29,1,3qkudy,[loraw] WATCH THE INTERN 2015 FOR FREE,https://www.reddit.com/r/MachineLearning/comments/3qkudy/loraw_watch_the_intern_2015_for_free/,[deleted],1446049708,[deleted],2,1
607,2015-10-29,2015,10,29,1,3qkzd9,Please help a beginner choose his final year project,https://www.reddit.com/r/MachineLearning/comments/3qkzd9/please_help_a_beginner_choose_his_final_year/,[deleted],1446051552,[deleted],4,1
608,2015-10-29,2015,10,29,2,3ql2rw,Rationale for greedy training of RBMs,https://www.reddit.com/r/MachineLearning/comments/3ql2rw/rationale_for_greedy_training_of_rbms/,wt0881,1446052775,"My understanding is that the training for RBMs via Contrastive Divergence is generally done greedily. That is, you learn the weight matrix for each layer in series (as opposed to in parallel) starting with the first. I could understand this approach if RBMs were likely to encounter vanishing gradients as you go deeper back into the network, but if you inspect the log-likelihood this clearly isn't going to be a problem.

Does anyone have any intuition as to why people do this? Does it somehow relate to the fact that the gradients are noisy?",6,1
609,2015-10-29,2015,10,29,2,3ql37z,Sklearn estimator (deeplearning): Multilayer perceptron using keras,https://www.reddit.com/r/MachineLearning/comments/3ql37z/sklearn_estimator_deeplearning_multilayer/,aulloa,1446052955,"This utility will allow you to use sklearn functions that evaluate estimators. 

See the repository here
https://github.com/alvarouc/mlp

See an example here
https://github.com/alvarouc/mlp/blob/master/examples/moon_sklearn.ipynb",1,2
610,2015-10-29,2015,10,29,2,3ql6c1,IBM to Acquire the Weather Company for Watson,https://www.reddit.com/r/MachineLearning/comments/3ql6c1/ibm_to_acquire_the_weather_company_for_watson/,blueeyes44,1446054112,,1,6
611,2015-10-29,2015,10,29,3,3ql9ll,Machine Learning For The Trip Planning Addict,https://www.reddit.com/r/MachineLearning/comments/3ql9ll/machine_learning_for_the_trip_planning_addict/,katsommerkamp,1446055304,,0,0
612,2015-10-29,2015,10,29,3,3qlazx,[Question] Why doesn't NTMs etc use the differentiable addressing mechanism like in DRAW?,https://www.reddit.com/r/MachineLearning/comments/3qlazx/question_why_doesnt_ntms_etc_use_the/,[deleted],1446055802,[deleted],12,6
613,2015-10-29,2015,10,29,4,3qll83,"How robots, artificial intelligence, and machine learning will affect employment and public policy",https://www.reddit.com/r/MachineLearning/comments/3qll83/how_robots_artificial_intelligence_and_machine/,shugert,1446059508,,3,0
614,2015-10-29,2015,10,29,6,3qm3ik,[Question] How would you design this experiment?,https://www.reddit.com/r/MachineLearning/comments/3qm3ik/question_how_would_you_design_this_experiment/,howMuchCheeseIs2Much,1446066142,"Let's say I was trying to build a model to recommend the optimal amount to feed an animal where I am trying to maximize muscle gain. I have several starting variables about each animal (starting weight, height, body fat %, etc.), but I'm also going to run an experiment that varies the amount I feed the animal, which will likely have an impact on muscle gain.

**How would you recommend assigning the amounts to feed each animal?** My current assumption is it's best to do this randomly.

**What technique would you then use to determine what the optimal feed amount is?** I was considering clustering the data based on the starting attributes and seeing which randomly assigned feed amount results in the highest muscle gain.

Thanks in advance for taking the time to answer.",5,2
615,2015-10-29,2015,10,29,6,3qm4u8,Question on Andrew's Ng's second lecture,https://www.reddit.com/r/MachineLearning/comments/3qm4u8/question_on_andrews_ngs_second_lecture/,dudedudeman1,1446066624,"https://youtu.be/5u4G23_OohI?t=1h8m43s

and here is the same thing in the course notes: http://i.imgur.com/oDlknUd.jpg

my issue is with the third step where he equates the derivative to the derivative of the trace. ""we used the fact that the trace of a real number is the real number."" ok, but with that perspective, the derivative or gradient of a real number is zero, so is there a better explanation for this step? ",4,2
616,2015-10-29,2015,10,29,7,3qmdnl,Can someone explain me bayesian regression learning and gaussian processes?,https://www.reddit.com/r/MachineLearning/comments/3qmdnl/can_someone_explain_me_bayesian_regression/,Fragore,1446069921,"I'm a Msc at ETH and I'm studing for the machine learning exam but I can't get the point of bayesian learning and gaussian processes. I know the bayes theorem and I've also spent a lot of time searching for a good explanation on the internet, but with no result. Can you help me? Maybe also with examples. Thanks a lot
",5,1
617,2015-10-29,2015,10,29,7,3qmfbz,Is IBM Watson just (mostly) marketing?,https://www.reddit.com/r/MachineLearning/comments/3qmfbz/is_ibm_watson_just_mostly_marketing/,[deleted],1446070567,[deleted],38,68
618,2015-10-29,2015,10,29,8,3qmnbw,I'm trying to write a java library to perform feature selection on a data set. Can somebody tell me if I've written the function for calculating the conditional entropy correctly?,https://www.reddit.com/r/MachineLearning/comments/3qmnbw/im_trying_to_write_a_java_library_to_perform/,o_safadinho,1446073657,"[Here](https://github.com/jjerrodtaylor/filters/blob/master/src/main/java/edu/uba/filters/Entropy.java) is a link to my github profile that contains the file that I'm asking about.

I'm basically just using two lists to [test](https://github.com/jjerrodtaylor/filters/blob/master/src/test/java/edu/uba/filters/EntropyTest.java) everything. The numbers that I get from my code are way off compared to what I get by hand. I had a similar problem when I wrote the entropy function, but since I've corrected that I know it isn't the same error.",0,1
619,2015-10-29,2015,10,29,8,3qmuqx,machine learning is still missing a key ingredient,https://www.reddit.com/r/MachineLearning/comments/3qmuqx/machine_learning_is_still_missing_a_key_ingredient/,toisanji,1446076630,,3,0
620,2015-10-29,2015,10,29,9,3qmwcm,Liquid State Machine: How is it different to Spiking Neural Network Models,https://www.reddit.com/r/MachineLearning/comments/3qmwcm/liquid_state_machine_how_is_it_different_to/,HenryKisinger,1446077288,"I am very new to the 'reservoir computing world', and I've heard that the Liquid State Machines (LSM) are a certain kind of spiking neuron network models (SNN). Exactly what is the difference in terms of the implementation between the two.

Another aspect on which I need some clarity is in regards to their counterpart the 'Leaky integrator models of Echo state network (ESN). I found from another answer in the forum that 'as I see it (I could be wrong) the big difference between the two approaches is the individual unit. In liquid state machine use biological like neurons, and in the Echo state use more analog units. So in term of very short term memory the Liquid State approach each individual neuron remember its own history, where in the Echo state approach each individual neuron react base only on the current state, there for the memory stored in the activity between the units.

Please tell me if this is the correct and if not what is the actual concept behind them.
",1,2
621,2015-10-29,2015,10,29,12,3qnnre,30 TFlops in a single box - machine learning development on a budget.,https://www.reddit.com/r/MachineLearning/comments/3qnnre/30_tflops_in_a_single_box_machine_learning/,CaffeineFreak77,1446089311,,6,0
622,2015-10-29,2015,10,29,13,3qny0t,TD E twist tie machine,https://www.reddit.com/r/MachineLearning/comments/3qny0t/td_e_twist_tie_machine/,dongfengpacking,1446094699,,1,1
623,2015-10-29,2015,10,29,14,3qo1f3,Can transfer learning to applied to more than cNNs?,https://www.reddit.com/r/MachineLearning/comments/3qo1f3/can_transfer_learning_to_applied_to_more_than_cnns/,Jxieeducation,1446096795,It was pleasantly surprised by the power of transfer learning on the MNIST dataset. Can this be applied to other types of neuronetworks / algorithms?,1,2
624,2015-10-29,2015,10,29,14,3qo2ph,sklearn: time series with multiple inputs.. how to structure data?,https://www.reddit.com/r/MachineLearning/comments/3qo2ph/sklearn_time_series_with_multiple_inputs_how_to/,iNeverHaveNames,1446097605,"Hello, I really thought finding information on this would be easier, and I am still just getting my feet wet with ML.. so maybe I missed something ..but hopefully someone here will be able to shed some light on this for me..

I am trying to use sklearn to predict a time series with multiple features.. and I just dont know how to organize the data.

If I only had one input, my understanding is that I would structure the data as such with a 'lookback' window of, say ,'3' : 
rawInputData =  [1,2,3,4,5,6,7,8]
features = [[1,2,3],[2,3,4],[3,4,5]]
labels = [[4],[5],[6]]
... etc; I hope I have this correct.. if not, please let me know.

My issue is that if I have multiple inputs, I'm not sure of the best way to present this data to sklearn without using 3 dimensional arrays.

Can I just append data from the second input source onto the data from the first input?
ie:
rawInput1 = [1,2,3,4,5]
rawInput2 = [11,12,13,14,15]
features = [[1,2,3,11,12,13], [2,3,4,12,13,14]]
labels = [[4,14],[5,15]]

Any guidance or shove in the right direction would be so appreciated.

If there is a library in python that is a better fit for doing timeseries analysis, I'd welcome that as well..

Thanks!",7,2
625,2015-10-29,2015,10,29,15,3qo7yc,From confusion matrix to expected value: tutorial with examples,https://www.reddit.com/r/MachineLearning/comments/3qo7yc/from_confusion_matrix_to_expected_value_tutorial/,cast42,1446101159,,0,0
626,2015-10-29,2015,10,29,16,3qo9dt,auotmatic sleeve labeling machine,https://www.reddit.com/r/MachineLearning/comments/3qo9dt/auotmatic_sleeve_labeling_machine/,dongfengpacking,1446102228,,1,1
627,2015-10-29,2015,10,29,16,3qobla,Built new hardware tools to study C. elegans in finer detail in order to understand the human brain.,https://www.reddit.com/r/MachineLearning/comments/3qobla/built_new_hardware_tools_to_study_c_elegans_in/,toisanji,1446103940,,0,0
628,2015-10-29,2015,10,29,18,3qojzu,How Having A Forklift License Can Help To Reduce Workplace Incidents,https://www.reddit.com/r/MachineLearning/comments/3qojzu/how_having_a_forklift_license_can_help_to_reduce/,danielviktori,1446110695,,0,1
629,2015-10-29,2015,10,29,19,3qonic,Popular Deep Learning Papers That Are Free To Read,https://www.reddit.com/r/MachineLearning/comments/3qonic/popular_deep_learning_papers_that_are_free_to_read/,YasukoHolmen,1446113164,,2,20
630,2015-10-29,2015,10,29,19,3qorg2,A Framework for Distributed Deep Learning Layer Design in Python,https://www.reddit.com/r/MachineLearning/comments/3qorg2/a_framework_for_distributed_deep_learning_layer/,cast42,1446115840,,1,10
631,2015-10-29,2015,10,29,21,3qp0w2,Switching to machine learning postdoc after a PhD in bioinformatics?,https://www.reddit.com/r/MachineLearning/comments/3qp0w2/switching_to_machine_learning_postdoc_after_a_phd/,PM_ME_GOATS_PLURAL,1446121618,"Hey all,

I am coming towards the end of my PhD in bioinformatics. I do a lot of programming and data analysis, but for the last year or so have been developing an increasingly strong interest in machine learning, and I may be able to apply some of it in this final year (my third year, I'm in the UK).

I am considering applying for postdocs in machine learning. However, I have not yet met any people who have gone into ML postdocs without some serious background in it. From participating in an ML competition, I am aware how I am quite behind others in the theory side of things (though I would probably do quite applied ML). 

Does anyone have any advice on making the transition to an ML postdoc from a non-ML PhD? 

Cheers.",7,2
632,2015-10-29,2015,10,29,22,3qpa4x,Time Maps: Visualizing Discrete Events Across Many Timescales,https://www.reddit.com/r/MachineLearning/comments/3qpa4x/time_maps_visualizing_discrete_events_across_many/,galapag0,1446125976,,1,58
633,2015-10-29,2015,10,29,23,3qpkl3,LIGHT: Linear-time Detection of Non-linear Changes (code and paper),https://www.reddit.com/r/MachineLearning/comments/3qpkl3/light_lineartime_detection_of_nonlinear_changes/,improbabble,1446130459,,1,8
634,2015-10-30,2015,10,30,0,3qpnvx,Predictive sales lead scoring now a commodity?,https://www.reddit.com/r/MachineLearning/comments/3qpnvx/predictive_sales_lead_scoring_now_a_commodity/,intenskitty,1446131752,"Predictive sales lead scoring assigns a probability to each of your sales leads. The probability represents the likelihood of closing the lead within a certain time period.

Do you think this has been commoditized like credit scoring (http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2227333)? ",0,2
635,2015-10-30,2015,10,30,0,3qprj7,[Question] Conditional features,https://www.reddit.com/r/MachineLearning/comments/3qprj7/question_conditional_features/,multited,1446133163,"I'm trying to model a problem where some features only exist for observations, depending on the values of other features. For example, a person's time to complete a task might only be available if that person was assigned that task (where each person corresponds to an observation).

Is there a more appropriate name for this and approach to building a model (example linear regression) for such problems?

To clarify further, an example of a response variable might be job satisfaction and so an observation is (job_satisfaction, received_task, time_task, other_features...). Here job_satisfaction is the response variable and (received_task, time_task, other_features)  is the array of features. received_task is in {0,1} and time_task == NA if received_task == 0, or time_task = (some numeric value) otherwise. other_features array is present (no NAs, assume real-valued array for simplicity) for all observations.
",2,2
636,2015-10-30,2015,10,30,1,3qpve8,Twitch Plays Robotics (main post in /r/artificial),https://www.reddit.com/r/MachineLearning/comments/3qpve8/twitch_plays_robotics_main_post_in_rartificial/,JAnetsbe,1446134589,,0,7
637,2015-10-30,2015,10,30,1,3qpxo5,How to optimize pixel-wise crossentropy?,https://www.reddit.com/r/MachineLearning/comments/3qpxo5/how_to_optimize_pixelwise_crossentropy/,[deleted],1446135454,[deleted],0,1
638,2015-10-30,2015,10,30,1,3qpxwc,[Question]choice of language,https://www.reddit.com/r/MachineLearning/comments/3qpxwc/questionchoice_of_language/,risingduck,1446135545,Is R or Python better for implementing machine learning algorithms?,8,3
639,2015-10-30,2015,10,30,1,3qpywp,Supervised Term Weighting Schemes: Alternatives to tf-df,https://www.reddit.com/r/MachineLearning/comments/3qpywp/supervised_term_weighting_schemes_alternatives_to/,senyat,1446135927,,6,7
640,2015-10-30,2015,10,30,1,3qpzs4,Riding on Large Data with Scikit-learn,https://www.reddit.com/r/MachineLearning/comments/3qpzs4/riding_on_large_data_with_scikitlearn/,alexperrier,1446136257,,1,11
641,2015-10-30,2015,10,30,2,3qq579,[Question][conv net training] How to optimize pixel-wise crossentropy?,https://www.reddit.com/r/MachineLearning/comments/3qq579/questionconv_net_training_how_to_optimize/,Improbus_42,1446138280,"The goal is to segment an image, i.e. I want to classify each pixel. Consider a conv net with a softmax classifier on top. If i have an 16x16 image and each pixel can belong to one of 10 classes, does that mean that my output layer has 16 * 16 * 10 neurons? Also, would that mean that i have to transform a label, e.g. label ""3"" into a vector like ""[ 0 0 0 1 0 0 0 0 0 0]"" ?",2,0
642,2015-10-30,2015,10,30,2,3qq920,SDRRL v2.0 and PRSDRRL,https://www.reddit.com/r/MachineLearning/comments/3qq920/sdrrl_v20_and_prsdrrl/,CireNeikual,1446139773,,2,7
643,2015-10-30,2015,10,30,2,3qq94v,Minimum Spanning Tree Clustering (Notebook),https://www.reddit.com/r/MachineLearning/comments/3qq94v/minimum_spanning_tree_clustering_notebook/,cast42,1446139803,,0,2
644,2015-10-30,2015,10,30,2,3qqa0h,New Julia Deep Learning Package,https://www.reddit.com/r/MachineLearning/comments/3qqa0h/new_julia_deep_learning_package/,antinucleon,1446140136,,8,21
645,2015-10-30,2015,10,30,2,3qqdew,Learning to Hash for Indexing Big Data (x-post r/CompressiveSensing ),https://www.reddit.com/r/MachineLearning/comments/3qqdew/learning_to_hash_for_indexing_big_data_xpost/,compsens,1446141374,,1,3
646,2015-10-30,2015,10,30,3,3qqfre,The Definitive Guide to Natural Language Processing,https://www.reddit.com/r/MachineLearning/comments/3qqfre/the_definitive_guide_to_natural_language/,feconroses,1446142281,,0,6
647,2015-10-30,2015,10,30,3,3qqhx2,"Deep networks, LSTMs, and other complex architectures: is SGD enough?",https://www.reddit.com/r/MachineLearning/comments/3qqhx2/deep_networks_lstms_and_other_complex/,polytop3,1446143142,"Hi All,

I have recently been learning about advanced optimization methods, such as hessian-free methods. I am wondering if such methods are worth it, and give significant boosts over, say, plain old SGD?

The reason I ask is because gradients are noisy. Also, with more complex optimization algorithms more time is required for a single update; in that same time, multiple SGD updates can be done (so complex optimization algorithms have to deal with this trade off).

Further, correct me if I am wrong, the error surface changes from minibatch to minibatch (since the optimal weights will typically be different for any given minibatch).

So given noisy gradients, changing error surfaces, and the fact that SGD can go through multiple updates in the same time as a single update for a more complex optimization algorithm, are more complex optimization algorithms worth it?

Does the answer to the above depend on the network type? E.g. for RNNs and LSTMs perhaps hessian-free results in way better results?

Are there any empirical studies of different optimization methods on different network types?

Optimization seems quite critical to deep learning, and I am curious about the above.

Thanks in advance for sharing your knowledge!",6,4
648,2015-10-30,2015,10,30,3,3qqiwn,CNN Architectures for Matching Natural Language Sentences,https://www.reddit.com/r/MachineLearning/comments/3qqiwn/cnn_architectures_for_matching_natural_language/,MrTwiggy,1446143512,,0,6
649,2015-10-30,2015,10,30,3,3qqjdm,Really want to train an autoencoder on [these](https://www.flickr.com/photos/britishlibrary/) and redraw new images in 'illustration' form,https://www.reddit.com/r/MachineLearning/comments/3qqjdm/really_want_to_train_an_autoencoder_on/,[deleted],1446143688,[deleted],0,1
650,2015-10-30,2015,10,30,3,3qqkze,A little help with this project that I'm working on.,https://www.reddit.com/r/MachineLearning/comments/3qqkze/a_little_help_with_this_project_that_im_working_on/,velcronicoov,1446144287,"Hi guys

So I'm working on this thing for uni. I received a Facebook database containing everything(for example age, likes, comments, location, language, reactions, friends, groups, school,...) of over 8000 Facebook members.

With this information I have to attempt to predict whether a certain Facebook page will be liked or not (or maybe the chance a page is liked).

I have found research papers suggesting Collaborative Filtering is the best method to go about doing this. I am fairly new to data science and machine learing. What method(s) do you guys think of when reading this case?

thanks in advance!!",0,3
651,2015-10-30,2015,10,30,3,3qql53,Training autoencoder on old illustrations -&gt; encoding photos in illustration format,https://www.reddit.com/r/MachineLearning/comments/3qql53/training_autoencoder_on_old_illustrations/,[deleted],1446144347,[deleted],0,0
652,2015-10-30,2015,10,30,4,3qquw4,"What are your ""primary sources"" to keep up with ML?",https://www.reddit.com/r/MachineLearning/comments/3qquw4/what_are_your_primary_sources_to_keep_up_with_ml/,omniron,1446147952,"We see a lot of news articles here and some papers on ML, but does anyone have a place they check daily to see what the researchers are up to? I'd like to be able to read about advancements before the media gets to things. I already follow Yann LeCun on Facebook and he posts his own work, and some others' work. Anyone know of other good people that keep up their social media/blogs?",11,96
653,2015-10-30,2015,10,30,5,3qqz7w,Google is 're-thinking' all of its products to include machine learning,https://www.reddit.com/r/MachineLearning/comments/3qqz7w/google_is_rethinking_all_of_its_products_to/,shugert,1446149557,,0,4
654,2015-10-30,2015,10,30,5,3qr2lw,Which deep learning library should I learn?,https://www.reddit.com/r/MachineLearning/comments/3qr2lw/which_deep_learning_library_should_i_learn/,FatSoccerMan,1446150852,"I am familiar with deep learning from papers but I haven't tried implementing anything yet. I usually use Python so I was leaning towards Theano - but are the other libraries (Caffe, Torch, etc) better enough to make it worth learning them instead? Would be interested to hear tradeoffs (e.g. since this is in industry, training/evaluation speed is important).",11,9
655,2015-10-30,2015,10,30,5,3qr3du,Neural conversation model code?,https://www.reddit.com/r/MachineLearning/comments/3qr3du/neural_conversation_model_code/,asymptotics,1446151143,Is there any code that replicates Google's conversation model available? Anything similar?,8,9
656,2015-10-30,2015,10,30,5,3qr63w,How do I apply this data set to the E-Step of an Expectation Maximization?,https://www.reddit.com/r/MachineLearning/comments/3qr63w/how_do_i_apply_this_data_set_to_the_estep_of_an/,Andhurati,1446152149,"I've been reading a lot of papers on Expectation Maximization, but I am having trouble grasping the E-Step. I understand how they did the E-Step for this [Stack Exchange Question](http://math.stackexchange.com/questions/25111/how-does-expectation-maximization-work).



(Paper is from [here](http://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf))

But I am having trouble applying that to other data sets. Specifically, how am I supposed to apply a set such as this (""-"" denotes missing data) to the E-Step:

    Gender	 Weight	 Height
    0	0	0
    0	0	0
    0	0	1
    -	0	0
    1	1	1
    0	0	1
    0	0	0
    -	0	1
    0	1	0
    1	1	1
    1	1	1
    0	0	0
    -	0	1
    -	1	1   

In this case, Weight and Height is related to Gender ([very simply Bayes Network](http://imgur.com/i9hKREA)).         
",6,3
657,2015-10-30,2015,10,30,10,3qs8q6,Convex cost function for neural networks? (xpost from /r/optimization),https://www.reddit.com/r/MachineLearning/comments/3qs8q6/convex_cost_function_for_neural_networks_xpost/,______POTATOES______,1446168342,"Moving my post here simply to reach a larger audience.

Are there any convex cost functions known for sigmoidal feedforward neural networks? All i've ever seen for these networks is some kind of error cost on the output compared to the ""correct error"" such as cross entropy or mean square error in supervised learning, yet this is not convex over the network parameters.
The only convex costs I've really seen for any neural networks is boltzmann learning, originally by hinton, which is an iterative maximum likelihood computation and greedy by layer. Wondering if there is a convex cost for deterministic feedforward nets...
",6,1
658,2015-10-30,2015,10,30,12,3qsmbu,What did the pirate say to the image?,https://www.reddit.com/r/MachineLearning/comments/3qsmbu/what_did_the_pirate_say_to_the_image/,nicholas-leonard,1446174857,[removed],0,0
659,2015-10-30,2015,10,30,13,3qstiq,Will quantum computing make brute force intelligent agents such as AIXI and the Gdel Machine viable?,https://www.reddit.com/r/MachineLearning/comments/3qstiq/will_quantum_computing_make_brute_force/,OptimalProblemSolver,1446178719,,2,2
660,2015-10-30,2015,10,30,13,3qsw5u,Why Apple is Falling Behind in AI,https://www.reddit.com/r/MachineLearning/comments/3qsw5u/why_apple_is_falling_behind_in_ai/,pashakun,1446180247,,29,17
661,2015-10-30,2015,10,30,15,3qt5oo,"Bagging, boosting and stacking in machine learning",https://www.reddit.com/r/MachineLearning/comments/3qt5oo/bagging_boosting_and_stacking_in_machine_learning/,Dawny33,1446186538,,2,19
662,2015-10-30,2015,10,30,16,3qt96c,DZQ 800B Vacuum packaging machine,https://www.reddit.com/r/MachineLearning/comments/3qt96c/dzq_800b_vacuum_packaging_machine/,dongfengpacking,1446189117,,1,1
663,2015-10-30,2015,10,30,19,3qtn6o,Is audio signal processing still useful in the era of machine learning? (x-post r/CompressiveSensing ),https://www.reddit.com/r/MachineLearning/comments/3qtn6o/is_audio_signal_processing_still_useful_in_the/,compsens,1446200841,,8,7
664,2015-10-30,2015,10,30,20,3qtq56,ICML 2015 videos,https://www.reddit.com/r/MachineLearning/comments/3qtq56/icml_2015_videos/,cesarsalgado,1446202992,,0,34
665,2015-10-30,2015,10,30,21,3qtxby,Comparing Python Clustering Algorithms,https://www.reddit.com/r/MachineLearning/comments/3qtxby/comparing_python_clustering_algorithms/,gthank,1446207679,,8,9
666,2015-10-30,2015,10,30,23,3quaiw,[Question] Best way to organize and save instances that have a varying number of subsets of attributes over time?,https://www.reddit.com/r/MachineLearning/comments/3quaiw/question_best_way_to_organize_and_save_instances/,Strel0k,1446214336,"I am collecting data on the sellers selling a specific product on an ecommerce website. I will be saving snapshots of their attributes and who was ""selected"" every 15 minutes. How do I save this information into a usable training set format? Or whats the best way to organize and store it?


into a f Lets use [this one](http://www.amazon.com/dp/B00ZY9JTJ0) as an example. There are currently three sellers, each with three attributes I am interested in: feedback score, feedback number and price. The dataset for this information would look something like this:

    [[94%, 28137, 5.99], # seller1
    [99%, 148475, 8.49], # seller2
    [95%, 150, 19.99]] # seller3

I also want to collect who is currently being featured on the product page (if you click ""Add to Cart"", who would be buying from). Currently, its `seller1`.

Since the number of sellers varies over time (some stop selling or more start selling the product), what would be the best way to collect and organize this data so that it could be easily fed into machine learning training data?

TL;DR: The data I want to collect has varying number of ""entities"" with a subset of attributes `'entity1': {'A':1, 'B':2}, 'entity2': {'A':8, 'B':9}`and I also need to collect which entity was selected. How do I collect this data into a training set format?",1,1
667,2015-10-30,2015,10,30,23,3qugwq,Filter Hundreds of TextBooks To Only Readable Text -- NLP,https://www.reddit.com/r/MachineLearning/comments/3qugwq/filter_hundreds_of_textbooks_to_only_readable/,LeavesBreathe,1446217167,"Hey Guys, I have a few hundred textbooks that I want to convert to txt files. I'm fully aware of pdftotext and pdftohtml +beautifulsoup. The textbooks are all pdfs and I convert them using pdftotext at the moment.

However, the problem is that I *only* want the actual readable content. I'm trying to filter out the indexes, glossaries, sources, and whatnot. I just want the text itself for machine learning.

I've written some python regex, but I figured I should post here and ask: Is there a good resource to do this? I have about a 1gb worth of pdfs. What really kills me is there are random '*()$*#' parts in the text. Ideally, there would be a really comprehensive regex script that would filter out the majority of this information. 

Here is what I have written so far. I'm very new to regex so I apologize for how inefficient this is. I don't really care for how efficient it is, because its usually done in 5 minutes. Even if it can be completed in 72 hours, I would be very happy. 

    def bookfilter(text):
        text = unicode(text, errors='replace')
        text = text.encode('ascii', errors='ignore')
        text = re.sub(r'(Further reading|Glossary|Glossaries|Systematics|Bibliography|Books|Dictionaries|External links|See also|Sources|References|Cited texts)\n\n(- .+\n|  .+\n)*', r'',text)
        text = re.sub(r'(Further reading|Glossary|Glossaries|Systematics|Bibliography|Books|Dictionaries|External links|See also|Sources|References|Cited texts)\n(- .+\n|  .+\n)*', r'',text)
        text = re.sub('(Footnotes|References|Notes)\n\n\[\d*\].(.+\n|.+\n)*', '',text)
        text = re.sub(r'\[\d*\].(.+\n|.+\n)*', '',text)
        text = re.sub('Main article:.+\n', '',text)
        text = re.sub(r'(|||||||||)', '',text)
        text = re.sub(r'[^\x00-\x7F]+', r'', text) #remove consecutive non-ascii chars
        text = re.sub('(     |    |   |  )', '',text)
        text = re.sub('(\n\n\n\n|\n\n\n\n\n|\n\n\n)', '\n\n',text)
        text = re.sub(r'\.([a-zA-Z])', r'. \1', text) #ensures space after every period (cuts down on word diversity)
        text = re.sub('`', '',text)
        text = text.lower()
        text = re.sub('---','--',text)
        text = re.sub('___','',text)
        text = re.sub(r'\n[0-9a-z_]{,10}\n', r'',text) 
        text = re.sub(r'\n*\.\.\.\.+\n',r'',text)",6,2
668,2015-10-31,2015,10,31,0,3quifq,IPAM Machine Learning Videos,https://www.reddit.com/r/MachineLearning/comments/3quifq/ipam_machine_learning_videos/,_spreadit,1446217774,,0,5
669,2015-10-31,2015,10,31,0,3quphr,Stacking ensembles to improve prediction,https://www.reddit.com/r/MachineLearning/comments/3quphr/stacking_ensembles_to_improve_prediction/,chuckatx,1446220642,"I recently read this blog (http://mlwave.com/kaggle-ensembling-guide/) and it has many ideas for ensembling various models. I created three models for my training data, random forest model, SVM model and a KNN model. However when I use linear method to stack these ensemble methods together the error I get is lower than the RF model only about 40% of the time.
The linear method that I employ for stacking is a plain one: determining coefficients of each model and minimizing their least squares.
Am I missing something? This results in a ""best case"" scenario less than half the time, so I would be better off just using the randomforest model.
Secondly, What exactly are meta features as described in the netflix prize winners paper? How do I extract them for any data and successfully use them to build a lower error ensemble that individual models themselves?",5,8
670,2015-10-31,2015,10,31,0,3qupur,Scikit good tutorials or examples for SVM?,https://www.reddit.com/r/MachineLearning/comments/3qupur/scikit_good_tutorials_or_examples_for_svm/,isolar89,1446220796,"I'm trying to learn SVM. Using Bishop's text for reference and Scikit svm as my coding tools. I can run my svm.predict() function and get results however, I am not sure how to correctly tune my model. Has anyone seen an example of someone tuning their svm? 

Thanks!",1,2
671,2015-10-31,2015,10,31,1,3qutk7,Must Read Books for Beginners on Machine Learning and Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/3qutk7/must_read_books_for_beginners_on_machine_learning/,john_philip,1446222250,,11,38
672,2015-10-31,2015,10,31,1,3qutl4,WarpLDA: a Simple and Efficient O(1) Algorithm for Latent Dirichlet Allocation,https://www.reddit.com/r/MachineLearning/comments/3qutl4/warplda_a_simple_and_efficient_o1_algorithm_for/,muktabh,1446222258,,2,9
673,2015-10-31,2015,10,31,2,3qv0o2,Attention with Intention,https://www.reddit.com/r/MachineLearning/comments/3qv0o2/attention_with_intention/,LLCoolZ,1446224940,,5,18
674,2015-10-31,2015,10,31,2,3qv3k8,Amazon Machine Learning: a real example in Python,https://www.reddit.com/r/MachineLearning/comments/3qv3k8/amazon_machine_learning_a_real_example_in_python/,[deleted],1446226030,[deleted],1,4
675,2015-10-31,2015,10,31,3,3qvgij,Suggestions for a college data project,https://www.reddit.com/r/MachineLearning/comments/3qvgij/suggestions_for_a_college_data_project/,rustic_coiffure,1446230882,"I am taking a data mining class and was just assigned my final -project. I need to find a data set to perform classifications and regression on. We need to find a data set with no more than 10,000 observations and less than 50 columns. The whole project will be done in RStudio, so I am ideally looking for a .csv file. From there I am kind of stuck, I don't know what sort of data to look for, the project is really open ended. I ideally want to do something a little unusual and more fun than something like census data. I tried to find data on horse racing, but that was surprisingly difficult. I have to work with the data the rest of the year, so I want to keep it interesting. Could you guys give some cool ideas?",5,1
676,2015-10-31,2015,10,31,5,3qvtvq,Stochastic Discrimination: an overtraining-resistant stochastic modeling method for Pattern Recognition,https://www.reddit.com/r/MachineLearning/comments/3qvtvq/stochastic_discrimination_an/,coldaspluto,1446236114,"I spent one summer coming up with an implementation of this method for my advisor, and got pretty decent results. The theory intrigued me, as it was different than anything else I had read about ML/PR. I wish it was more well known. Throwing it out to the broader community, as I want to see what others think of it. Too much ""Deep Learning"" all the time here! ;-)

  * [Main Website](http://kappa.math.buffalo.edu/)
  * [Results](http://kappa.math.buffalo.edu/results.shtml) (since that's what interests most people :D )
  * [Background](http://kappa.math.buffalo.edu/background.shtml)
  * [References](http://kappa.math.buffalo.edu/showref.pl)

",3,8
677,2015-10-31,2015,10,31,6,3qw63h,[Question] Regression Method to Fit a Monte-Carlo Generated Distribution to an Existing Data Set?,https://www.reddit.com/r/MachineLearning/comments/3qw63h/question_regression_method_to_fit_a_montecarlo/,Jollyhrothgar,1446241088,"Hello,

I'm working on a project right now for some research. I have a Monte-Carlo model which I use to generate simulated data, shown in red, and I'm trying to make this data match my ""real data"" which is shown in blue. 

You can see my manually tuned distributions here: http://i.imgur.com/8SwBIPj.png

The distributions are histograms, binned in an arbitrary parameter - so we have some number of observations of the parameter per bin.

I'd like to match the simulated distribution as closely as possible to the data distribution, so that I can use this to estimate the value of a few features responsible for the shape of the the data distribution.
I can tune the simulation arbitrarily to match the data, manually, but I'd like a way to automate this - something like least-squares fitting (gradient descent fitting) or some other method/algorithm.

The problem is that I generate the simulated data based on randomly sampling a distribution, and this distribution is generated by assigning values to the features I want to estimate in the first place. There isn't a ""simple"" functional form which governs the shape of the simulation. I tune the value of the features which then generate distributions, which are then sampled randomly, to produce the histogram. I feel like this rules out using regression/gradient descent, as I can't ""take the derivative with respect to a feature"" of my simulated distribution. 

One idea is to generate ranges of parameters, and many different simulated distributions based in these parameters, and then pick the one which is mathematically most similar to my data, however, as it stands, this is computationally intensive, might take a long time (perhaps very long), and I feel that there must be a better way.

So, I feel like I am touching on a class of solved computational and/or machine learning problems, and I just don't have the exposure to implement a proper solution. Can you folks offer any help?

Cheers!
",2,2
678,2015-10-31,2015,10,31,6,3qw8bw,[Question] Is there a known way to imprint information on a LSTM cell?,https://www.reddit.com/r/MachineLearning/comments/3qw8bw/question_is_there_a_known_way_to_imprint/,macncookies,1446242049,"Hello /r/MachineLearning,

Say I want to train a LSTM with the following:

Input | Output
---|---
(0, 1, 0, 0, 0, 0, 0) | 2
(0, 0, 1, 0, 0, 0, 0) | 3
(0, 0, 0, 1, 0, 0, 0) | 4
(0, 0, 1, 0, 0, 0, 0) | 3
(0, 1, 0, 0, 0, 0, 0) | 2

I imagine that shouldn't be too hard. But if I add a ""distraction"":

Input | Output
---|---
(0, 1, 0, 0, 0, 0, 1) | 2
(0, 0, 1, 0, 0, 1, 0) | 3
(0, 0, 0, 1, 1, 0, 0) | 4
(0, 0, 1, 0, 1, 0, 0) | 3
(0, 1, 0, 0, 0, 1, 0) | 2

... things get complicated. Are you aware of a way to 'tell' the LSTM cell to only track the first 1 and ignore the second (at test time)? 

Thanks!",10,2
679,2015-10-31,2015,10,31,7,3qwaqx,Beginners Guide: Apache Spark Machine Learning Scenario With A Large Input Dataset,https://www.reddit.com/r/MachineLearning/comments/3qwaqx/beginners_guide_apache_spark_machine_learning/,djphilosopher,1446243030,,0,32
680,2015-10-31,2015,10,31,8,3qwmof,Q-learning with Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3qwmof/qlearning_with_neural_networks/,outlacedev,1446248304,,3,27
681,2015-10-31,2015,10,31,9,3qwsjn,Let AI predict your mind,https://www.reddit.com/r/MachineLearning/comments/3qwsjn/let_ai_predict_your_mind/,sethibharat,1446251025,,0,0
682,2015-10-31,2015,10,31,10,3qx1ml,Vacuum packaging machine DZQ-400B,https://www.reddit.com/r/MachineLearning/comments/3qx1ml/vacuum_packaging_machine_dzq400b/,dongfengpacking,1446255586,,1,1
683,2015-10-31,2015,10,31,10,3qx2go,Can machine learning be used for content creation? If so can anyone recommend libraries or cloud based services?,https://www.reddit.com/r/MachineLearning/comments/3qx2go/can_machine_learning_be_used_for_content_creation/,Visibleone,1446256025,,1,0
684,2015-10-31,2015,10,31,10,3qx33s,SST 1600,https://www.reddit.com/r/MachineLearning/comments/3qx33s/sst_1600/,dongfengpacking,1446256364,,1,1
685,2015-10-31,2015,10,31,13,3qxixd,Big Data Machine Learning in SQL,https://www.reddit.com/r/MachineLearning/comments/3qxixd/big_data_machine_learning_in_sql/,[deleted],1446265009,[deleted],0,0
686,2015-10-31,2015,10,31,13,3qxlpe,"support vector regression and Gaussian process regression, which one tends to be more accurate?",https://www.reddit.com/r/MachineLearning/comments/3qxlpe/support_vector_regression_and_gaussian_process/,augustus2010,1446266753,,5,1
687,2015-10-31,2015,10,31,16,3qxx4d,Qualitative Projection Using Deep Neural Networks [arXiv],https://www.reddit.com/r/MachineLearning/comments/3qxx4d/qualitative_projection_using_deep_neural_networks/,ajrs,1446275212,,1,0
688,2015-10-31,2015,10,31,16,3qxxvz,"Happy Halloween! Baidu Research Introduces FaceYou, a New Face Morphing App for iOS based on Deep Learning",https://www.reddit.com/r/MachineLearning/comments/3qxxvz/happy_halloween_baidu_research_introduces_faceyou/,stefanomazzalai,1446276011,,1,0
689,2015-10-31,2015,10,31,16,3qxyah,Machine Tools Manufacturers,https://www.reddit.com/r/MachineLearning/comments/3qxyah/machine_tools_manufacturers/,Md-Malik,1446276366,,0,1
690,2015-10-31,2015,10,31,16,3qy0j6,[Theoretical ML] Anybody helps me about a SVM problem?,https://www.reddit.com/r/MachineLearning/comments/3qy0j6/theoretical_ml_anybody_helps_me_about_a_svm/,Polo222,1446278381,"not a homework, I just saw a problem and tried to solve it but fails after several hours. 

it's about the dual form of sparse svm. 

BTW, is there anyone who can tell me how to post a picture?",2,0
691,2015-10-31,2015,10,31,20,3qycs2,LSTM: Merging the results of mini-batched sub-segments of a really long segment,https://www.reddit.com/r/MachineLearning/comments/3qycs2/lstm_merging_the_results_of_minibatched/,bge0,1446289566,"So to capture long-term dependencies one will need to either:
 
  1. run full backprop-through-time
  2. run truncated BPTT with a mass of data
  3. RTRL

To parallelize a single long sequence I would imagine that segmenting the sequence into windows &amp; training it in mini-batches would make sense. 


The hidden state should be easy enough to pass along as you might just use the resultant h_t from the final state of the previous sequence (eg: seq0's output) as the initializer of the new sequence (eg: seq1's initial h_0) at the next iteration &amp; so on.  Now the issue as I see it is in merging the weights/recurrent weights &amp; biases.  Is there a standard way to do this? eg: averaging, etc? 
",1,3
692,2015-10-31,2015,10,31,21,3qyi10,Batch Normalization for Object Detection,https://www.reddit.com/r/MachineLearning/comments/3qyi10/batch_normalization_for_object_detection/,math0point,1446293801,"Does anyone apply batch normalization for object detection method RCNN? When I try to apply it for RCNN where minibatch has positive sample(including 20 class) and negative sample(background class) at rate 1:1, it didn't work...",4,5
693,2015-10-31,2015,10,31,22,3qym7y,The best sources for ml datasets?,https://www.reddit.com/r/MachineLearning/comments/3qym7y/the_best_sources_for_ml_datasets/,mrborgen86,1446296678,"I'm a machine learning newbie interested in knowing here people find their datasets to work and play with. 

The ones I've used are Kaggle and the UCI repository.

http://archive.ics.uci.edu/ml/
https://www.kaggle.com/competitions

If you know of any other good sites that hosts some great datasets, please comment, as we'll all benefit from knowing about more data sources :)",4,5
694,2015-10-31,2015,10,31,22,3qyn0m,Sequence to sequence mapping via LSTM encoder/decoder nets: why not word vectors and squared error loss?,https://www.reddit.com/r/MachineLearning/comments/3qyn0m/sequence_to_sequence_mapping_via_lstm/,polytop3,1446297171,"So I am trying to implement the LSTM encoder/decoder architecture presented here: [Sequence to Sequence Learning with Neural Networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) by  Ilya Sutskever et al (Google).

My understanding of what they did is given below:

Input sequence (English sentence) --&gt; Encoder LSTM --&gt; **v** vector that represents the entire sentence

**v** --&gt; Decoder LSTM --&gt; output sequence (French sentence)

In particular, each time an output word is produced by the decoder LSTM, it is done by carrying out a softmax over all possible French words: http://i.imgur.com/43E4PKF.png

This seems quite expensive to me since the output layer needs to be the size of the French vocabulary (80k was the actual number).

--------------------------------------------------------------------------------------------------------------------------------

So I was wondering, why not use vector representation for words? Then the output layer just needs to be n dimensional (where n is the vector size).

Then the LSTM decoder output is a set of n-dimensional vectors: [ outputVec1, outputVec2, ... ] which is essentially a matrix, let's call this matrix **Y**. We then compare **Y** with the target output matrix, let's call it **T**. The height of **Y** and **T** may differ, so we can use zero padding. Afterwards, our loss can be, loss = sum( (**Y**-**T**)^2 ), or elementwise difference, followed by elementwise squaring, followed by a sum on all elements.

(You can use kNN on Y to get the actual French sentence).

To me the above would be a **lot** more efficient. I am wondering if there is any flaw with the idea above? And if not, why wasn't this approach used?",20,10
695,2015-10-31,2015,10,31,22,3qyp01,Generative Image Modeling Using Spatial LSTMs (paper+code),https://www.reddit.com/r/MachineLearning/comments/3qyp01/generative_image_modeling_using_spatial_lstms/,samim23,1446298459,,0,33
696,2015-10-31,2015,10,31,23,3qyuz5,ITS THE WEEKEND; DIVE INTO MACHINE LEARNING RIGHT HERE,https://www.reddit.com/r/MachineLearning/comments/3qyuz5/its_the_weekend_dive_into_machine_learning_right/,Friars1993,1446301808,,0,1
