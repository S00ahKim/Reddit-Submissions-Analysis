,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2015-11-1,2015,11,1,11,3r1fca,Regarding Google's announcement of large-scale use of word-vector models in its search engine (RankBrain),https://www.reddit.com/r/MachineLearning/comments/3r1fca/regarding_googles_announcement_of_largescale_use/,ranterskanter,1446344577,"I was heavily downvoted by some at this sub just a few months ago for suggesting that Google was probably using word vectors at scale in its products.

I wonder if any of those downvoters would care to speak their minds about this development, and if they feel they have learned something now.",10,0
1,2015-11-1,2015,11,1,16,3r26k8,"Apache SINGA, A Distributed Deep Learning Platform",https://www.reddit.com/r/MachineLearning/comments/3r26k8/apache_singa_a_distributed_deep_learning_platform/,pilooch,1446364252,,13,55
2,2015-11-1,2015,11,1,18,3r2bnn,Anyone have experience with TensorFlow? Google's upcoming neural network library?,https://www.reddit.com/r/MachineLearning/comments/3r2bnn/anyone_have_experience_with_tensorflow_googles/,tensorflow,1446369454,"Does anyone have any information on how TensorFlow compares with existing libraries like Theano and Torch? 

I've heard from multiple independent sources that Google will be releasing it soon. Just curious if it brings anything new to the table than what already exists.

",15,13
3,2015-11-1,2015,11,1,21,3r2och,Is any cluster algorithms suitable for this situation?,https://www.reddit.com/r/MachineLearning/comments/3r2och/is_any_cluster_algorithms_suitable_for_this/,Neuer93,1446381626,"I encountered a clustering problem recently: I have some nodes needed to be clustered and I know the distance (similarity) between each nodes. But there are lots of noisy nodes which dont belong to any clusters. I want to obtain clusters and discard noisy nodes. 
I am not familiar to clustering algorithms (just know some simple algorithm like KNN, K-Means), so I want to ask if there are some mature clustering algorithms suitable this situation?
",7,10
4,2015-11-2,2015,11,2,0,3r38i2,Shockingly Effective Trick To Maximize your Margin!!!,https://www.reddit.com/r/MachineLearning/comments/3r38i2/shockingly_effective_trick_to_maximize_your_margin/,[deleted],1446392876,[deleted],2,0
5,2015-11-2,2015,11,2,1,3r3dc8,Machine Learning To Save Distracted Drivers,https://www.reddit.com/r/MachineLearning/comments/3r3dc8/machine_learning_to_save_distracted_drivers/,[deleted],1446395004,[deleted],0,1
6,2015-11-2,2015,11,2,2,3r3nqj,Classifier Technology and the Illusion of Progress,https://www.reddit.com/r/MachineLearning/comments/3r3nqj/classifier_technology_and_the_illusion_of_progress/,mttd,1446399348,,3,5
7,2015-11-2,2015,11,2,3,3r3tpq,Is there any library in python with proper documentation and tutorial for speech recognition?,https://www.reddit.com/r/MachineLearning/comments/3r3tpq/is_there_any_library_in_python_with_proper/,rohanpota,1446401730,"One option is Caffe Deep Learning Framework but I can't seem to find any tutorials(related to speech recognition) on it.If someone has any links regarding this,please share them.",4,3
8,2015-11-2,2015,11,2,3,3r3y3a,Where can I get the latest dataset for a network intrusion detection system?,https://www.reddit.com/r/MachineLearning/comments/3r3y3a/where_can_i_get_the_latest_dataset_for_a_network/,rohanpota,1446403521,"I am using machine learning for network intrusion detection(Signature based).
Except kddcup99.
",4,3
9,2015-11-2,2015,11,2,4,3r40v6,Mondrian forests: Efficient random forests for streaming data (Python Code and lecture),https://www.reddit.com/r/MachineLearning/comments/3r40v6/mondrian_forests_efficient_random_forests_for/,[deleted],1446404650,[deleted],2,23
10,2015-11-2,2015,11,2,4,3r425h,Machine Learning Trick of the Day (4): Reparameterisation Tricks,https://www.reddit.com/r/MachineLearning/comments/3r425h/machine_learning_trick_of_the_day_4/,skrza,1446405168,,0,41
11,2015-11-2,2015,11,2,5,3r4a97,Best Material for Learning RNNs,https://www.reddit.com/r/MachineLearning/comments/3r4a97/best_material_for_learning_rnns/,atomant30,1446408393,"I have a good understanding of feed-forward neutral networks and I want to learn about RNNs. What are some of the best resources available?

My short-term goal is to implement LSTMs in Torch, but I don't feel like I have a solid enough understanding of RNNs in general. ",8,10
12,2015-11-2,2015,11,2,5,3r4b63,Preprocessing convnet RGB filters for visualization?,https://www.reddit.com/r/MachineLearning/comments/3r4b63/preprocessing_convnet_rgb_filters_for/,LyExpo,1446408758,"I have so far only experimented with convnets using greyscale images. I have successfully visualized the first layer filters by simply plotting the values of the weights. 

This method doesn't seem to working for me when I change over to rgb images. In this setting, plotting using `ax.imshow(...)` in matplotlib is giving me filters that look like colorful noise. I'm wondering if there is some sort of preprocessing I should do to the first layer weights before plotting them. In particular, it might be possible that the r,g, and b weights are in totally different ranges, in which case simply plotting them as is might not make sense.

Can someone share their experiences?",5,2
13,2015-11-2,2015,11,2,7,3r4rga,Low dimensional time series.,https://www.reddit.com/r/MachineLearning/comments/3r4rga/low_dimensional_time_series/,[deleted],1446415288,[deleted],6,4
14,2015-11-2,2015,11,2,7,3r4y9u,Questions about karpathy's char-rnn code,https://www.reddit.com/r/MachineLearning/comments/3r4y9u/questions_about_karpathys_charrnn_code/,Allanqunzi,1446418005,"In the LSTM model (https://github.com/karpathy/char-rnn/blob/master/model/LSTM.lua#L34) at line 34 and 35, code is like below

local reshaped = nn.Reshape(4, rnn_size)(all_input_sums) --line 34

local n1, n2, n3, n4 = nn.SplitTable(2)(reshaped):split(4)  -- line 35


According to my understanding, ""reshaped"" at line 34 is a 4 by rnn_size matrix, 

and at line 35, calling ""nn.SplitTable(2)(reshaped)"" yields a table, and this table has rnn_size elements(since it is splitting along the dimension 2), and each element is a 4-element vector.

But I don't know how "":split(4)"" works? 

Any one can explain how "":split(4)"" splits the results of ""nn.SplitTable(2)(reshaped)""? Thanks!
",2,2
15,2015-11-2,2015,11,2,11,3r5qw1,"Python text sentiment analysis server, what do you think?",https://www.reddit.com/r/MachineLearning/comments/3r5qw1/python_text_sentiment_analysis_server_what_do_you/,linux1975,1446430433,,16,36
16,2015-11-2,2015,11,2,13,3r6b18,Good practices for data mining?,https://www.reddit.com/r/MachineLearning/comments/3r6b18/good_practices_for_data_mining/,code_kansas,1446440305,"Hi, I am doing some web scraping to build a data set to do some distributional semantics analysis. My code is basically a python script that does a breadth-first search on all the links found on a particular page that belong to a particular domain (in this case, Huffington Post, because I wanted blog text). The thing is, I am now having to go back through and clean a bunch of it up because my code isn't very good at identifying what is ""blog"" text and what isn't. I was wondering, does anyone have any suggestions on how to go about this well? Are there any resources out there for people like me who would like to do this? Also, if I wanted to make the data available, is there a good way to structure it?",1,1
17,2015-11-2,2015,11,2,18,3r6xtw,Variational Recurrent Neural Network (paper+code),https://www.reddit.com/r/MachineLearning/comments/3r6xtw/variational_recurrent_neural_network_papercode/,samim23,1446456315,,0,6
18,2015-11-2,2015,11,2,19,3r71ft,Fast Randomized SVD | Facebook Blog + Code,https://www.reddit.com/r/MachineLearning/comments/3r71ft/fast_randomized_svd_facebook_blog_code/,pilooch,1446459318,"This fast randomized SVD / PCA python lib by Facebook just allowed me to scale up some clustering very easily, since I had never heard of it, I thought I would share:

- code: https://github.com/facebook/fbpca
- blog post: https://research.facebook.com/blog/294071574113354/fast-randomized-svd/",8,13
19,2015-11-2,2015,11,2,19,3r73nl,Machine Learning To Save Distracted Drivers,https://www.reddit.com/r/MachineLearning/comments/3r73nl/machine_learning_to_save_distracted_drivers/,[deleted],1446461182,[deleted],0,1
20,2015-11-2,2015,11,2,20,3r76rm,Texture Synthesis Using Convolutional Neural Networks (paper+code),https://www.reddit.com/r/MachineLearning/comments/3r76rm/texture_synthesis_using_convolutional_neural/,samim23,1446463743,,9,31
21,2015-11-2,2015,11,2,20,3r77mb,Poisson distribution for recommender system(by LDA creator David Blei),https://www.reddit.com/r/MachineLearning/comments/3r77mb/poisson_distribution_for_recommender_systemby_lda/,yobichi,1446464366,"Recently David Blei has published a paper talking about using poisson distribution for recommender system. Given the observed y_ui, which equal one if user u consume item i otherwise it is zero. And y_ui is modeled with a poisson distribution:  y_ui ~ Poisson(theta_u * beta_i)
Anyone can answer me the question that why he can use the poisson model here? How can the recommending scenario fulfil the requirement of poisson distribution? 
The paper is here: http://auai.org/uai2015/proceedings/papers/208.pdf",11,2
22,2015-11-2,2015,11,2,20,3r78n0,Vacuum packaging machine VS 600,https://www.reddit.com/r/MachineLearning/comments/3r78n0/vacuum_packaging_machine_vs_600/,dongfengpacking,1446465100,,1,1
23,2015-11-2,2015,11,2,21,3r79hz,Vacuum packing machine DZ 325,https://www.reddit.com/r/MachineLearning/comments/3r79hz/vacuum_packing_machine_dz_325/,dongfengpacking,1446465708,,1,1
24,2015-11-2,2015,11,2,21,3r7am6,Double chamber vacuum packing machine,https://www.reddit.com/r/MachineLearning/comments/3r7am6/double_chamber_vacuum_packing_machine/,dongfengpacking,1446466487,,1,1
25,2015-11-2,2015,11,2,21,3r7ejw,Machine Learning for indentify the author of an email,https://www.reddit.com/r/MachineLearning/comments/3r7ejw/machine_learning_for_indentify_the_author_of_an/,raulgzm,1446469058,,0,0
26,2015-11-2,2015,11,2,23,3r7ldj,"Deep Learning Summer School,Montreal 2015 - Videolectures.net google drive link!",https://www.reddit.com/r/MachineLearning/comments/3r7ldj/deep_learning_summer_schoolmontreal_2015/,code2hell,1446472904,"As videolectures.net  provide download option to a few lectures(they do have a valid reason to do so explained [here!](http://videolectures.net/faq/)
and also the fact that its a streaming video, thereby making it hard for people to download it. I've downloaded and uploaded the lectures in google drive and the link for the same
[here!](https://drive.google.com/folderview?id=0B6EE3Sw2jmZWfnRIcm5rRkNVTDdPTlc3NUFIQU04MmRuVXFUM2JRYURmSVVBQmhqbFBxM00&amp;usp=sharing)

Please do check out the course here:
http://videolectures.net/deeplearning2015_montreal/?q=deep%20learning

Im not much sure of the citations that I have to provide for the material. Please do suggest me about citations in the comments! Thanks!",6,31
27,2015-11-3,2015,11,3,0,3r7x8q,Scoring for Unsupervised Anomaly Detection,https://www.reddit.com/r/MachineLearning/comments/3r7x8q/scoring_for_unsupervised_anomaly_detection/,DataOmbudsman,1446478315,"Hey all,

I'm in a need of any input regarding my current project so let me ask you a question.

I'm working on unsupervised anomaly detection. I use an ensemble of algorithms for detecting anomalies in the same data. One algorithm is based on frequent pattern mining, another is on PCA etc.

Each of these algorithms outputs an outlier score for each item in my data set; let's call these 'raw scores'. These raw scores could be on various scales; e.g., the raw scores of algorithm #1 are between [0,1], of algorithm #2 between [-inf, 0]. Even if two algorithms score on the same scale, the interpretation of the scores could be different (one is purely probability, the other is not).

My goal is to calibrate these scores and put the raw scores from all algorithms on [0,1] with the following requirements:

- the same calibrated score has the same interpretation across all of my algorithms so that I can compare the scores of different algorithms,

- the same calibrated score has the same interpretation across all of my data sets so that I can compare the scores of different data sets.

So far, my best approach was to score the training data set with my algorithms and somehow use the distribution of the raw scores to calibrate. For example, I used a sigmoid function to convert the normalized raw scores to scores between 0-1. But then, I was a bit disappointed because that forced me in a way such that a fixed portion of calibrated scores went beyond the threshold of anomalousness (let's say, 0.5) now matter how normal my data were (and I don't want normal data to have high scores).

It might be that my requirements are contradicting but I couldn't prove it so far.

Can you point me in the right direction? What should I look into? I'm aware there is a whole area of calibrating the raw scores of supervised algorithms into probabilities (Platt scaling, isotonic regression) but those methods need labels which I don't have.

Thanks for all ideas!",4,1
28,2015-11-3,2015,11,3,0,3r80gu,"Tutorial for Visual Question Answering using CNN and LSTMs, code in Python+Keras(Theano)",https://www.reddit.com/r/MachineLearning/comments/3r80gu/tutorial_for_visual_question_answering_using_cnn/,[deleted],1446479648,[deleted],0,1
29,2015-11-3,2015,11,3,1,3r8a4w,How to measure the similarity between two weight spaces in an ANN,https://www.reddit.com/r/MachineLearning/comments/3r8a4w/how_to_measure_the_similarity_between_two_weight/,joeyglasgow,1446483431,"Hi,

I'm trying to get a metric for the distance between to sets of weights for neural networks in an ensemble to test how different training schemes affect diversity. Does anyone know what kind of measures may be useful?",18,5
30,2015-11-3,2015,11,3,2,3r8jn5,How Klout Leverages Machine Learning To Calculate Influence,https://www.reddit.com/r/MachineLearning/comments/3r8jn5/how_klout_leverages_machine_learning_to_calculate/,shugert,1446486902,,0,0
31,2015-11-3,2015,11,3,2,3r8jwt,"Are models like AlexNet, VGGNet, GoogleNet available in theano/theano-based libraries?",https://www.reddit.com/r/MachineLearning/comments/3r8jwt/are_models_like_alexnet_vggnet_googlenet/,feedthecreed,1446486997,"I want to fine-tune an entire pre-trained model on a new dataset, but I can't find source code that creates the theano expression for these pre-trained models. Does anyone know if something like this exists anywhere? Is theano simply too slow to run networks this larger?",4,1
32,2015-11-3,2015,11,3,3,3r8lju,NIPS 2015 thread,https://www.reddit.com/r/MachineLearning/comments/3r8lju/nips_2015_thread/,[deleted],1446487603,[removed],0,1
33,2015-11-3,2015,11,3,3,3r8the,Machine learning and immigration,https://www.reddit.com/r/MachineLearning/comments/3r8the/machine_learning_and_immigration/,__null__,1446490497,Are there any research projects  or papers related to machine learning and immigration ?,6,0
34,2015-11-3,2015,11,3,4,3r8uxo,Amazon EMR: five ways to improve the way you use Hadoop,https://www.reddit.com/r/MachineLearning/comments/3r8uxo/amazon_emr_five_ways_to_improve_the_way_you_use/,leonardodotio,1446491028,,0,3
35,2015-11-3,2015,11,3,4,3r927m,"""Considering a fullyconnected neural network layer with d input nodes, and d output nodes, this method improves the time complexity from O(d^2) to O(d log d) and space complexity from O(d^2) to O(d)""",https://www.reddit.com/r/MachineLearning/comments/3r927m/considering_a_fullyconnected_neural_network_layer/,downtownslim,1446493674,,16,12
36,2015-11-3,2015,11,3,5,3r94eg,word2vec: how to choose embedding space dimensions,https://www.reddit.com/r/MachineLearning/comments/3r94eg/word2vec_how_to_choose_embedding_space_dimensions/,solololol,1446494507,"I am training word2vec on 50GB of English scientific papers with gensim. I created a model with 100 dimensions and one with 300. I've seen people use up to 500 with word2vec.

My impressionistic assessment is that my 100-dim and 300-dim models are quite different. I see no obvious winner though.

Could anyone explain what changing dimension captures, and whether you'd recommend some specific dimension for 50GB of English text?

Here is one sample, notice the important differences:

    model-100-dim.most_similar('scratch')
    [('mar', 0.6890989542007446),
     ('abrasion', 0.6360374093055725),
     ('sticky', 0.560904860496521),
     ('bit-addressable', 0.5505107641220093),
     ('wear', 0.5437108874320984),
     ('corrosion', 0.5321375131607056),
     ('pre-fetch/post-store', 0.5250539779663086),
     ('coating,f.', 0.5158470273017883),
     ('wrinkle', 0.5154947638511658),
     ('good', 0.5135785341262817)

and:

    model-300-dim.most_similar('scratch')
    [('abrasion', 0.5975580811500549),
     ('mar', 0.5910640954971313),
     ('wear', 0.49950164556503296),
     ('scratch,', 0.46674802899360657),
     ('corrosion', 0.462272971868515),
     ('coating,f.', 0.4538571834564209),
     ('stain', 0.4521702826023102),
     ('tintable,', 0.45050448179244995),
     ('scuff', 0.44467946887016296),
     ('abrasion,', 0.4431385099887848)",6,2
37,2015-11-3,2015,11,3,5,3r9b4s,"6 Biggest Misconceptions About Data Scientists, Data Engineers and Business Analysts.",https://www.reddit.com/r/MachineLearning/comments/3r9b4s/6_biggest_misconceptions_about_data_scientists/,shugert,1446496924,,0,0
38,2015-11-3,2015,11,3,5,3r9dao,Tutorial for Visual Question Answering using CNN and LSTM with Python code,https://www.reddit.com/r/MachineLearning/comments/3r9dao/tutorial_for_visual_question_answering_using_cnn/,zapata999,1446497718,,2,4
39,2015-11-3,2015,11,3,5,3r9dt2,The Rules of Data Visualization Get an Update,https://www.reddit.com/r/MachineLearning/comments/3r9dt2/the_rules_of_data_visualization_get_an_update/,shugert,1446497915,,2,9
40,2015-11-3,2015,11,3,6,3r9hvk,Artificial-intelligence institute launches free science search engine,https://www.reddit.com/r/MachineLearning/comments/3r9hvk/artificialintelligence_institute_launches_free/,bakarr,1446499418,,0,1
41,2015-11-3,2015,11,3,7,3r9oys,I have a question in regards to video game design.,https://www.reddit.com/r/MachineLearning/comments/3r9oys/i_have_a_question_in_regards_to_video_game_design/,[deleted],1446502022,"I am asking about this because I want to make sure I understand machine learning correctly in how it would be utilized in the specific situation below. 

Problem:
I am tasked with generating all of the art assets for a video game.

I have a large database of art assets from already existing games. Lets say for arguments sake all of these assets are organized in similar groups.

In the case of utilizing machine learning algorithm I would tell the program to analyze the assets then generate 100 similar assets. I would then pick the top 10 and repeat the process until I got assets I felt were usable for what I currently needed.

Is this correct? 

Side bonus question:
Does anything like this already exist?",4,1
42,2015-11-3,2015,11,3,7,3r9rvr,Machine Learning To Save Distracted Drivers,https://www.reddit.com/r/MachineLearning/comments/3r9rvr/machine_learning_to_save_distracted_drivers/,Pavan19485,1446503105,,0,1
43,2015-11-3,2015,11,3,8,3r9y1j,Use evolutionary algorithms instead of gridsearch in scikit-learn. This allows you to exponentially reduce the time required to find the best parameters for your estimator.,https://www.reddit.com/r/MachineLearning/comments/3r9y1j/use_evolutionary_algorithms_instead_of_gridsearch/,rsteca,1446505497,,48,61
44,2015-11-3,2015,11,3,9,3ra8bn,A Brain Trust for Data Science: NSF announces awards for four regional big data innovation hubs.,https://www.reddit.com/r/MachineLearning/comments/3ra8bn/a_brain_trust_for_data_science_nsf_announces/,Mexicorn,1446509726,,0,7
45,2015-11-3,2015,11,3,10,3raivl,Code to Calculate Confidence Interval for Linear Regression (Sklearn)?,https://www.reddit.com/r/MachineLearning/comments/3raivl/code_to_calculate_confidence_interval_for_linear/,srt19170,1446514149,"Can anyone provide a pointer to existing code I can use to calculate the confidence interval for a new observation (and its prediction) given a fitted Scikit Learn linear regression, as (for example) described [here](http://www.utd.edu/~ammann/stat3360/node35.html)?  I'm specifically interested in the case of calculating the interval for a new prediction.  Any help would be appreciated!",2,1
46,2015-11-3,2015,11,3,10,3ralkr,What are the most interesting recent developments in penalization since structured sparsity?,https://www.reddit.com/r/MachineLearning/comments/3ralkr/what_are_the_most_interesting_recent_developments/,gabjuasfijwee,1446515343,?,0,6
47,2015-11-3,2015,11,3,11,3rau4s,Good book for applications of topology in ML?,https://www.reddit.com/r/MachineLearning/comments/3rau4s/good_book_for_applications_of_topology_in_ml/,paulkon,1446518960,A DeepMind researcher recently wrote a blog post about the application of topology in machine learning. Was looking for related books to get a better picture of how it can be used.,5,2
48,2015-11-3,2015,11,3,13,3rb75w,Deep Learning Companies,https://www.reddit.com/r/MachineLearning/comments/3rb75w/deep_learning_companies/,brotherrain,1446524665,,0,3
49,2015-11-3,2015,11,3,15,3rbij0,Asynchronous SGD: Averaging Parameters vs. Averaging Gradients?,https://www.reddit.com/r/MachineLearning/comments/3rbij0/asynchronous_sgd_averaging_parameters_vs/,alexmlamb,1446530400,"For asynchronous sgd (i.e. with a parameter server), has anyone investigated whether it's better to average the gradients from the workers or average their parameter values.  If the model replicas stored on each worker diverge enough, then the two approaches can be quite different.  I can think of a few advantages / disadvantages either way.  ",1,3
50,2015-11-3,2015,11,3,15,3rbils,RE.WORK - Deep Minds: An Interview with Google's Alex Graves &amp; Koray Kavukcuoglu,https://www.reddit.com/r/MachineLearning/comments/3rbils/rework_deep_minds_an_interview_with_googles_alex/,Buck-Nasty,1446530448,,0,8
51,2015-11-3,2015,11,3,17,3rbv22,Intergenerational Knowledge Transfer,https://www.reddit.com/r/MachineLearning/comments/3rbv22/intergenerational_knowledge_transfer/,[deleted],1446538439,[deleted],2,0
52,2015-11-3,2015,11,3,17,3rbxyw,"What's the difference between momentum based gradient descent, and Nesterov's accelerated gradient descent?",https://www.reddit.com/r/MachineLearning/comments/3rbxyw/whats_the_difference_between_momentum_based/,polytop3,1446540728,"Momentum based gradient descent works as follows:

v = self.momentum * m - lr * g

where m is the previous weight update, and g is the current gradient with respect to the parameters p, lr is the learning rate, self.momentum is a constant, and v is velocity.

new_p = p + v = p + self.momentum * m - lr * g

and Nesterov's accelerated gradient descent works as follows:

new_p = p + self.momentum * v - lr * g

which is equivalent to:

new_p = p + self.momentum * (self.momentum * m - lr * g ) - lr * g

or

new_p = p + self.momentum^2 * m - (1 + self.momentum) * lr * g

source: https://github.com/fchollet/keras/blob/master/keras/optimizers.py

So to me it seems Nesterov's accelerated gradient descent just gives more weight to the lr * g term over the pervious weight change term m (compared to plain old momentum). Is this interpretation correct?",6,1
53,2015-11-3,2015,11,3,18,3rc09r,Yann LeCun keynote: Obstacles on the path to AI,https://www.reddit.com/r/MachineLearning/comments/3rc09r/yann_lecun_keynote_obstacles_on_the_path_to_ai/,modeless,1446542550,,1,40
54,2015-11-3,2015,11,3,19,3rc4gr,SVM with polynomial kernel visualization,https://www.reddit.com/r/MachineLearning/comments/3rc4gr/svm_with_polynomial_kernel_visualization/,Dawny33,1446545787,,0,14
55,2015-11-3,2015,11,3,20,3rc877,Question: how to formulate this as a machine learning problem.,https://www.reddit.com/r/MachineLearning/comments/3rc877/question_how_to_formulate_this_as_a_machine/,MLKmakesmyday,1446548567,"So I have a set of sequences of frames, with data for each frame.


I also have a set of much, much, larger sequences of frames, also with data for each frame.

I want to generate, from the latter set, a set of sub-sequences of those frames that are similar to the sequences from the former set.

How can I do this?

also sorry if I worded this poorly or if I'm an idiot or if these types of posts aren't allowed here.",8,3
56,2015-11-3,2015,11,3,21,3rch6q,Curated list of new research papers?,https://www.reddit.com/r/MachineLearning/comments/3rch6q/curated_list_of_new_research_papers/,thegoz,1446554811,"Hi, does anybody know of such a list? Just so that one can have an idea of what kind of research different research groups are doing. I usually check out individual research groups of different universities, but I tend to overlook the research that are happening in less famous univiersities/research centres. I have a feeling someone in here might have such a list. thanks.",3,3
57,2015-11-3,2015,11,3,22,3rcj5a,Video recommendation by dynamic content analysis approach,https://www.reddit.com/r/MachineLearning/comments/3rcj5a/video_recommendation_by_dynamic_content_analysis/,marceloboeira,1446555984,,1,7
58,2015-11-3,2015,11,3,22,3rcnfl,Overfitting in word2vec,https://www.reddit.com/r/MachineLearning/comments/3rcnfl/overfitting_in_word2vec/,elsonidoq,1446558283,"I've been wondering: how would you measure overfitting on a word2vec model?
 
The only thing I can think of is having word vectors with huge norms, but other than I can not think of how an overfitted word2vec model should behave.

Any ideas?
Thanks!
Pablo",13,7
59,2015-11-4,2015,11,4,0,3rcxdc,Deeper nets: MSR style weight initialization,https://www.reddit.com/r/MachineLearning/comments/3rcxdc/deeper_nets_msr_style_weight_initialization/,derk22,1446562801,"Following the CS231N course on deep learning for computer vision I am trying to obtain high accuracy on CIFAR 10 (see https://github.com/dmus/convnet/blob/master/cifar10_five_layer.ipynb). When making my convolutional net and going up from 3 to 4 to 5 layers I noticed weight initialization becomes more and more difficult.  Weights are initialized by random
weights drawn from Gaussian distributions and the network seems to be very sensitive to the weight scale:
 
    for i in [1, 2, 3, 4, 5]:
      model['W%d' % i] *= weight_scale.

# a weight_scale of 0.05 results in a network that learns, with other values the network learns nothing

So I looked to the MSR style of weight initialization (http://arxiv.org/pdf/1502.01852v1.pdf and http://torch.ch/blog/2015/07/30/cifar.html) and tried to implement this to get rid of the weight scale hyperparameter. I implemented this as 
    
    model['W1'] *= np.sqrt(2.0/(3*filter_sizes[0]**2)) # first drawn from gaussians with model['W5'] = np.random.randn(FC, num_classes) etc
    model['W2'] *= np.sqrt(2.0/(F1*filter_sizes[1]**2))  
    model['W3'] *= np.sqrt(2.0/(F2*filter_sizes[2]**2))
    model['W4'] *= np.sqrt(2.0/(H * W * F3 / 64))
    model['W5'] *= np.sqrt(2.0/(FC * num_classes))

But unfortunately the network does not seem to converge when trying to overfit a small portion of the training data. Is this a correct style of implementing the MSR style?

Is it similar to this code in Torch (from http://torch.ch/blog/2015/07/30/cifar.html)?
  
    -- initialization from MSR
    local function MSRinit(net)
      local function init(name)
        for k,v in pairs(net:findModules(name)) do
          local n = v.kW*v.kH*v.nOutputPlane
          v.weight:normal(0,math.sqrt(2/n))
          v.bias:zero()
        end
      end
      init'nn.SpatialConvolution'
    end

Especially for the first layer the sigma becomes 0.27 which is quite large compared to the other layers (approximately 0.08, 0.08, 0.04, 0.04) but logical when considering the small amount of inputs (3x3x3=9). Filter size is 3x3 and 3 channels",3,3
60,2015-11-4,2015,11,4,0,3rcxpo,Machine Learning To Save Distracted Drivers,https://www.reddit.com/r/MachineLearning/comments/3rcxpo/machine_learning_to_save_distracted_drivers/,Pavan19485,1446562954,,0,1
61,2015-11-4,2015,11,4,1,3rd80d,How to create a two way mapping between an input space and a higher dimensional sparse constrained space?,https://www.reddit.com/r/MachineLearning/comments/3rd80d/how_to_create_a_two_way_mapping_between_an_input/,[deleted],1446567051,[deleted],0,2
62,2015-11-4,2015,11,4,1,3rd9zz,Facebook Breathes Some Common Sense into Artificial-Intelligence Systems | MIT Technology Review,https://www.reddit.com/r/MachineLearning/comments/3rd9zz/facebook_breathes_some_common_sense_into/,code2hell,1446567807,,6,21
63,2015-11-4,2015,11,4,2,3rdiz3,"""Smart Reply"" to be released for Gmail",https://www.reddit.com/r/MachineLearning/comments/3rdiz3/smart_reply_to_be_released_for_gmail/,[deleted],1446571087,[deleted],0,3
64,2015-11-4,2015,11,4,2,3rdlsd,Google to productise automatic email response generation based on the sequence-to-sequence framework. This is very cool.,https://www.reddit.com/r/MachineLearning/comments/3rdlsd/google_to_productise_automatic_email_response/,fhuszar,1446572105,,20,140
65,2015-11-4,2015,11,4,2,3rdlzk,What happened to OpenMT15 evaluation. They have promised to release results long time ago. Have I missed it?,https://www.reddit.com/r/MachineLearning/comments/3rdlzk/what_happened_to_openmt15_evaluation_they_have/,[deleted],1446572169,[deleted],0,2
66,2015-11-4,2015,11,4,2,3rdor6,From Big Blues Predictive Analytics to Machine Learning with BigML,https://www.reddit.com/r/MachineLearning/comments/3rdor6/from_big_blues_predictive_analytics_to_machine/,czuriaga,1446573187,,0,2
67,2015-11-4,2015,11,4,2,3rdozo,"Computer, respond to this email. - Google using deep neural nets to draft replies to emails for you.",https://www.reddit.com/r/MachineLearning/comments/3rdozo/computer_respond_to_this_email_google_using_deep/,[deleted],1446573273,[deleted],0,1
68,2015-11-4,2015,11,4,2,3rdp7i,Differential Privacy Mini-series from Win-Vector,https://www.reddit.com/r/MachineLearning/comments/3rdp7i/differential_privacy_miniseries_from_winvector/,normee,1446573357,,0,3
69,2015-11-4,2015,11,4,3,3rdqep,Smart Reply - a deep neural network that writes email,https://www.reddit.com/r/MachineLearning/comments/3rdqep/smart_reply_a_deep_neural_network_that_writes/,linuxjava,1446573785,,1,1
70,2015-11-4,2015,11,4,3,3rdqtd,Python module to apply several classifiers to your data. Good for baseline results,https://www.reddit.com/r/MachineLearning/comments/3rdqtd/python_module_to_apply_several_classifiers_to/,aulloa,1446573940,"This module will run several classifiers including: Decision tree, Linear SVM, RBF SVM, Logistic regression, naive bayes, Nearest neighbours, and random forest. 

Please feel free to provide any comments, or even better, fork it. 

See example here https://github.com/alvarouc/polyssifier/blob/master/sample/example.ipynb",2,2
71,2015-11-4,2015,11,4,4,3re0rm,Experimental Facebook App Can Answer Questions About the Content of Photos | MIT Technology Review,https://www.reddit.com/r/MachineLearning/comments/3re0rm/experimental_facebook_app_can_answer_questions/,[deleted],1446577605,[deleted],0,1
72,2015-11-4,2015,11,4,5,3reie4,Practical applications of Machine Learning Meetup (NYC),https://www.reddit.com/r/MachineLearning/comments/3reie4/practical_applications_of_machine_learning_meetup/,osiris679,1446584200,,0,3
73,2015-11-4,2015,11,4,7,3reui9,Machine Learning: Making Sense of a Messy World,https://www.reddit.com/r/MachineLearning/comments/3reui9/machine_learning_making_sense_of_a_messy_world/,2Punx2Furious,1446588924,,0,1
74,2015-11-4,2015,11,4,7,3reuyq,Machine Learning To Save Distracted Drivers,https://www.reddit.com/r/MachineLearning/comments/3reuyq/machine_learning_to_save_distracted_drivers/,Pavan194,1446589096,,0,0
75,2015-11-4,2015,11,4,8,3rf8ac,Play Impressionist: crowd-sourced learning of mesh saliency,https://www.reddit.com/r/MachineLearning/comments/3rf8ac/play_impressionist_crowdsourced_learning_of_mesh/,Thors_Son,1446594573,"Hi r/MachineLearning!

Play [here](http://impressionist.herokuapp.com)!

[Bull for reference](http://imgur.com/rwzL7He)

Our lab is seeking suggestions and participants for a 3D peekaboo game inspired by Dr. Luis Von Ahn's ideas in human computation (see here for example: https://www.youtube.com/watch?v=-Ht4qiDRZE8). Our game ""impressionist"" (http://impressionist.heroku.com, http://github.com/antantantant/impressionist) challenges players to quickly identify 3D objects through collaboration.

It's a bit like 3D pictionary. The game will either match you up with an available human player, or a computer with one of the best ""saliency"" algorithms available. As a human, you will either be a drawer or a guesser: if you see a 3D object, hold S and drag the mouse over parts of the object you think will most help the other player gues what it is. As a guesser, you will see the object start to take shape, and you need to guess as quickly as possible what object it is. 

 The game data contributed by players will help us to better understand the relationship between geometries and human perception. Potential applications include saliency-preserving mesh reduction, more efficient 3D object identification, 3D printing and scanning, and product design.

Thanks!

(This project is conducted by the Design Informatics Lab at Arizona State University. You can visit us at: http://www.public.asu.edu/~yren32/)",0,5
76,2015-11-4,2015,11,4,8,3rf9b7,Is clustering a special case of allocation problem?,https://www.reddit.com/r/MachineLearning/comments/3rf9b7/is_clustering_a_special_case_of_allocation_problem/,FireOnSomething,1446594996,"The allocation problem usually is an optimization type of problem which tries to allocate resources to maximize or minimize an objective function.

The clustering problems are used to allocate objects into groups, trying to minimize the variance within groups, and maximize the variance between groups.

But for me, it's just a special case of allocation problem. Is it true? or how are those related? Are they into a same ""umbrela"" of knowledge?

Thanks in advance!",2,0
77,2015-11-4,2015,11,4,9,3rffhw,Career/Grad School Advice,https://www.reddit.com/r/MachineLearning/comments/3rffhw/careergrad_school_advice/,uniform_convergence,1446597612,[removed],10,0
78,2015-11-4,2015,11,4,12,3rg0ba,npy : Bayesian nonparametric machine learning for python,https://www.reddit.com/r/MachineLearning/comments/3rg0ba/npy_bayesian_nonparametric_machine_learning_for/,[deleted],1446606624,[deleted],0,1
79,2015-11-4,2015,11,4,12,3rg0ds,bnpy : Bayesian nonparametric machine learning for python,https://www.reddit.com/r/MachineLearning/comments/3rg0ds/bnpy_bayesian_nonparametric_machine_learning_for/,lakando,1446606651,,0,4
80,2015-11-4,2015,11,4,12,3rg0hz,Any recommendations for cheap cloud instances with GPU for students?,https://www.reddit.com/r/MachineLearning/comments/3rg0hz/any_recommendations_for_cheap_cloud_instances/,nevermindtheusernam,1446606698,I was looking for a single GPU instance to train a neural network. Are there any cheap options for students? ,7,6
81,2015-11-4,2015,11,4,12,3rg6ib,Introduction to machine learning with scikit-learn video series,https://www.reddit.com/r/MachineLearning/comments/3rg6ib/introduction_to_machine_learning_with_scikitlearn/,mmmayo13,1446609462,,0,17
82,2015-11-4,2015,11,4,13,3rg7iu,NEED HELP FOR PROJECT! (BIRD SPECIES RECOGONITION),https://www.reddit.com/r/MachineLearning/comments/3rg7iu/need_help_for_project_bird_species_recogonition/,sujaysomaiah,1446609932,[removed],1,0
83,2015-11-4,2015,11,4,13,3rgbta,NIPS 2015 Schedule Posted Today,https://www.reddit.com/r/MachineLearning/comments/3rgbta/nips_2015_schedule_posted_today/,sonaut,1446612108,"Early registration is ending soon, and they appear to have the schedule pretty sorted.",2,8
84,2015-11-4,2015,11,4,13,3rgdv1,Jeff Dean's slides show TensorFlow with code samples (slide 48 to 63),https://www.reddit.com/r/MachineLearning/comments/3rgdv1/jeff_deans_slides_show_tensorflow_with_code/,r-sync,1446613148,,13,24
85,2015-11-4,2015,11,4,16,3rgrvd,Just curious,https://www.reddit.com/r/MachineLearning/comments/3rgrvd/just_curious/,[deleted],1446621277,[deleted],0,0
86,2015-11-4,2015,11,4,16,3rgshy,Machine Learning Cartoons,https://www.reddit.com/r/MachineLearning/comments/3rgshy/machine_learning_cartoons/,yaelkoppenhaver,1446621709,,2,5
87,2015-11-4,2015,11,4,17,3rgzaq,Google &amp; Facebook upload AI PR videos on the same day,https://www.reddit.com/r/MachineLearning/comments/3rgzaq/google_facebook_upload_ai_pr_videos_on_the_same/,[deleted],1446626832,[deleted],0,1
88,2015-11-4,2015,11,4,17,3rgzfu,Google &amp; Facebook upload AI PR videos on the same day,https://www.reddit.com/r/MachineLearning/comments/3rgzfu/google_facebook_upload_ai_pr_videos_on_the_same/,evc123,1446626947,"Is it just coincidence?

https://www.facebook.com/zuck/videos/10102456212502251/?pnref=story &amp;
https://www.youtube.com/watch?v=l95h4alXfAA",3,5
89,2015-11-4,2015,11,4,20,3rhbli,Training Neural Network on speech database,https://www.reddit.com/r/MachineLearning/comments/3rhbli/training_neural_network_on_speech_database/,utkarshsimha,1446636471,"I'm working on training Neural Networks with the AN4 database. I have tried a lot of methods but the training and testing accuracy isn't crossing 60%. 
I have currently tried : Autoencoders, Sparse Autoencoders, Learning rate decay, dropout, momentum, using different cost/activation functions, mini-batching.
I'm using a Neural Network architecture of 440-700-102 (Adding more layers messes it up). Could anyone suggest more techniques to try out?
Thank you!",6,3
90,2015-11-4,2015,11,4,21,3rhjf6,GPU Server for extra low price,https://www.reddit.com/r/MachineLearning/comments/3rhjf6/gpu_server_for_extra_low_price/,jacke111,1446641613,[removed],2,0
91,2015-11-4,2015,11,4,22,3rhkei,Facebook Aims Its AI at the Game No Computer Can Crack,https://www.reddit.com/r/MachineLearning/comments/3rhkei/facebook_aims_its_ai_at_the_game_no_computer_can/,sdiepend,1446642205,,14,58
92,2015-11-4,2015,11,4,23,3rhuvv,Learn Deep Learning &amp; Neural Networks For Free,https://www.reddit.com/r/MachineLearning/comments/3rhuvv/learn_deep_learning_neural_networks_for_free/,john_philip,1446647405,,0,1
93,2015-11-5,2015,11,5,1,3ri9v8,Engaging And Retaining Mobile Users: Why Data And Machine Learning Are Your Best Friends,https://www.reddit.com/r/MachineLearning/comments/3ri9v8/engaging_and_retaining_mobile_users_why_data_and/,shugert,1446653755,,0,1
94,2015-11-5,2015,11,5,1,3riakl,Simple classification question - sorry if this isn't the right place,https://www.reddit.com/r/MachineLearning/comments/3riakl/simple_classification_question_sorry_if_this_isnt/,wintermute93,1446654032,"Hi all. If this subreddit isn't the place for questions like this I apologize, let me know where it belongs and I'll delete this thread. 

Anyway, I have a task I'd like to do some straightforward supervised classification on, but instead of a bunch of feature vectors that need to be labeled, I have a bunch of matrices that need to be labeled. Is there a standard way of dealing with 2-d ""features""? 

Of course I could always just reshape the matrices into vectors and use regular old logistic regression and gradient descent, but there's important information in their spatial structure that would be lost. (If it matters, the matrices are essentially arrays of correlated time series data, with each row being a time series of readings from some sensor and each column being a snapshot of all sensors at a given instant.) 

Am I going to have to pretend the matrices are image data and use something typically reserved for image recognition like a CNN, or is there an easier way?",3,2
95,2015-11-5,2015,11,5,1,3rifh9,Categorizing Stores?,https://www.reddit.com/r/MachineLearning/comments/3rifh9/categorizing_stores/,buddiBot,1446656028,"Hey guys,

How do you use machine learning to categorize transactions? I am assuming that I would to collect some data about the retail store and classify that. I'm hoping that through this classification that any other similar retail stores may be put into the same category.

Thanks",1,1
96,2015-11-5,2015,11,5,2,3rih71,Maintained List of ML Libraries?,https://www.reddit.com/r/MachineLearning/comments/3rih71/maintained_list_of_ml_libraries/,[deleted],1446656686,[deleted],2,1
97,2015-11-5,2015,11,5,3,3riskf,Apache MADlib | Big Data Machine Learning in SQL,https://www.reddit.com/r/MachineLearning/comments/3riskf/apache_madlib_big_data_machine_learning_in_sql/,pilooch,1446660945,,0,0
98,2015-11-5,2015,11,5,3,3riuis,What are some problems with the way data science contests are run on sites like Kaggle?,https://www.reddit.com/r/MachineLearning/comments/3riuis/what_are_some_problems_with_the_way_data_science/,sergeyfeldman,1446661690,"I'm interested in both methodological and conceptual problems, as well as potential solutions.",24,11
99,2015-11-5,2015,11,5,4,3rj6wa,DNN training software optimized for Intel/AMD GPUs,https://www.reddit.com/r/MachineLearning/comments/3rj6wa/dnn_training_software_optimized_for_intelamd_gpus/,NovaRom,1446666272,Many DNN training frameworks have CUDA optimizations available. I am looking for one with OpenCL support to use with Intel or AMD accelerators.,4,5
100,2015-11-5,2015,11,5,5,3rjdus,Deep Learning in a Nutshell: Core Concepts,https://www.reddit.com/r/MachineLearning/comments/3rjdus/deep_learning_in_a_nutshell_core_concepts/,harrism,1446668979,,5,87
101,2015-11-5,2015,11,5,5,3rjif3,How to get started without being intimidated?,https://www.reddit.com/r/MachineLearning/comments/3rjif3/how_to_get_started_without_being_intimidated/,seventythree37,1446670764,"**Background**: Freshman in college getting a B.S. in mathematical science. I have very little knowledge in programming. I'm barely starting a course in python. I have the attention span of a goldfish and the reading level of a fifth grader. 

&amp;nbsp;


**Question**: What are some good introductory resources for someone of my level? Where should I get started so I can understand the concepts and the material being passed around on this subreddit? And how long do you guys think it will take me to be a substantial member of the machine learning community? ",7,1
102,2015-11-5,2015,11,5,7,3rjtdk,Calling All Data Scientists! Win 2 Tickets to See Big Data in San Francisco,https://www.reddit.com/r/MachineLearning/comments/3rjtdk/calling_all_data_scientists_win_2_tickets_to_see/,dnabeyta,1446675163,,0,1
103,2015-11-5,2015,11,5,8,3rk6tc,Why type of local database (not cloud) is best for natural language processing data storage?,https://www.reddit.com/r/MachineLearning/comments/3rk6tc/why_type_of_local_database_not_cloud_is_best_for/,[deleted],1446680703,[deleted],4,0
104,2015-11-5,2015,11,5,10,3rknzi,"MLconf is in SF on 11/13 with talks from IBM Watson, Quora, Kaggle, CMU, Google and many more.",https://www.reddit.com/r/MachineLearning/comments/3rknzi/mlconf_is_in_sf_on_1113_with_talks_from_ibm/,shonburton,1446688359,,3,21
105,2015-11-5,2015,11,5,11,3rkv6u,"I wrote a short piece on Word2Vec. Would appreciate a second set of eyes checking the concepts, lest I mislead someone.",https://www.reddit.com/r/MachineLearning/comments/3rkv6u/i_wrote_a_short_piece_on_word2vec_would/,omgitsjo,1446691613,,12,17
106,2015-11-5,2015,11,5,12,3rl0z3,Would anyone like to have a community for sharing models / techniques online?,https://www.reddit.com/r/MachineLearning/comments/3rl0z3/would_anyone_like_to_have_a_community_for_sharing/,Jxieeducation,1446694319,Kind of like what Caffe has,5,2
107,2015-11-5,2015,11,5,12,3rl156,"Yann LeCun: Teaching Machines to Understand Us (Nov, 2)",https://www.reddit.com/r/MachineLearning/comments/3rl156/yann_lecun_teaching_machines_to_understand_us_nov/,[deleted],1446694398,[deleted],8,35
108,2015-11-5,2015,11,5,14,3rlclc,"How Machine Learning Works, As Explained By Google",https://www.reddit.com/r/MachineLearning/comments/3rlclc/how_machine_learning_works_as_explained_by_google/,john_philip,1446700206,,0,5
109,2015-11-5,2015,11,5,15,3rljs5,Are there efforts in this direction : A ML system which can scan a text corpus and identify statements or dialogues or snippets which are analogous to secrets people reveal on secret messaging services like YikYak/ Whisper apps ?,https://www.reddit.com/r/MachineLearning/comments/3rljs5/are_there_efforts_in_this_direction_a_ml_system/,singham,1446704415,"I think this problem belongs to the field of [Sentimental Analysis](https://en.wikipedia.org/wiki/Sentiment_analysis). Best way to test this would be to post the identified statements on those services after manual filtration.
It could run on large sets of data to find interesting information, whether it's Hillary's emails or the wikileaks dataset.
What are your thoughts? ",0,1
110,2015-11-5,2015,11,5,15,3rlk5c,Semi-Automatic Liquid Filling machine,https://www.reddit.com/r/MachineLearning/comments/3rlk5c/semiautomatic_liquid_filling_machine/,dongfengpacking,1446704634,,1,1
111,2015-11-5,2015,11,5,15,3rlliu,Semi-Automatic Double heads Liquid Filling machine,https://www.reddit.com/r/MachineLearning/comments/3rlliu/semiautomatic_double_heads_liquid_filling_machine/,dongfengpacking,1446705508,,1,1
112,2015-11-5,2015,11,5,15,3rlnfq,Semi-Automatic Paste Filling machine,https://www.reddit.com/r/MachineLearning/comments/3rlnfq/semiautomatic_paste_filling_machine/,dongfengpacking,1446706667,,1,1
113,2015-11-5,2015,11,5,16,3rls3m,Journal articles to print?,https://www.reddit.com/r/MachineLearning/comments/3rls3m/journal_articles_to_print/,canttouchmypingas,1446709933,"Hello, I am in college and I've switched my major to MS in Artificial Intelligence because I find it fascinating. I've seen many lectures and videos online about neural networks, deep learning, and various other videos that interest me greatly. I've skimmed a few papers and have read the results of many others. I recently purchased 25 journal notebooks so I could print out relevant journals and read them not on the computer. They can roughly hold 100 pages each. 

For a student getting a BS in Cognitive Sci going to get an MS in AI (and PhD later), what are some journals I could read? I'm going to make a cluster soon and hopefully buy a Jetson TK1 so I can experiment with different nets enough that I can try and build my own, which I have no idea how to begin to do (I know how everything works, but I've only passively watched, I need to work hands on).",1,2
114,2015-11-5,2015,11,5,18,3rly9x,.BSPIRIT: watch online the martian,https://www.reddit.com/r/MachineLearning/comments/3rly9x/bspirit_watch_online_the_martian/,jumpypredestinauO,1446714781,[removed],1,1
115,2015-11-5,2015,11,5,18,3rm0yb,"Why Neurons Have Thousands of Synapses, A Theory of Sequence Memory in Neocortex",https://www.reddit.com/r/MachineLearning/comments/3rm0yb/why_neurons_have_thousands_of_synapses_a_theory/,Atupis,1446716970,,14,18
116,2015-11-5,2015,11,5,19,3rm3es,Aluminum Die Casting Company,https://www.reddit.com/r/MachineLearning/comments/3rm3es/aluminum_die_casting_company/,gmarketing247,1446718883,[removed],0,1
117,2015-11-5,2015,11,5,20,3rmabw,Generate images styled to your favorite artist,https://www.reddit.com/r/MachineLearning/comments/3rmabw/generate_images_styled_to_your_favorite_artist/,Wiremask,1446724305,,14,55
118,2015-11-5,2015,11,5,23,3rmqxd,Determing if RNN Model Is Underfitting Vs. Overfitting,https://www.reddit.com/r/MachineLearning/comments/3rmqxd/determing_if_rnn_model_is_underfitting_vs/,LeavesBreathe,1446733728,"Hey Guys, did a few google searches and can't get a straight answer to this. I understand what underfitting and over fitting is, but I don't know how to determine if my model is underfitting. 

I've read in several places that if your validation loss is slightly above than your training loss (2.2 vs 2.1), you're doing really well. You're right on the sweetspot. 

If, on the other hand, your validation loss is far above your training loss (3.4 vs 2.3), you have vastly overfitted. 

So my main question is: Are you underfitting when you val loss is far below your training loss? (2.2 vs 4.3)",16,4
119,2015-11-6,2015,11,6,1,3rn7px,"Solving Problems From Space With Remote Sensing, IoT &amp; AI",https://www.reddit.com/r/MachineLearning/comments/3rn7px/solving_problems_from_space_with_remote_sensing/,reworksophie,1446740790,,0,1
120,2015-11-6,2015,11,6,3,3rnvl5,Having trouble picking between two machine learning products (LiftIgniter and Seldon). Would love some insight and advice on what the ideal choice would be!,https://www.reddit.com/r/MachineLearning/comments/3rnvl5/having_trouble_picking_between_two_machine/,lovestartups,1446749991,"I'm looking at using machine learning for a side project that was given to me at work. We'd like to make retail item recommendations based on what our customers see/do in our physical retail stores. After evaluating different products we narrowed it down to two finalists:


**LiftIgniter**


I've looked at LiftIgniter and they fit the bill in terms of recommendations, but their system seems very geared to standard e-commerce sites. They use actions such as page views and conversions to calculate and perform recommendations, which isn't something that our project will have. We've spoken to them about our product and they said that we can still make it work with their system. In order to do that we would have to submit our data by essentially emulating what would be a page view/conversion/whatever with whatever event we have.


**Seldon**


Seldon is an open-source solution, so it's very customizable and has a lot of features to offer. There might be a little extra effort involved in setting things up, but after that's done we would have more flexibility in terms of what we can do and tweak, not to mention that we would send our data in our way. I think the fact that we can change anything we want is great, but we haven't even started gathering information so I'm not sure if maybe I'm getting way too ahead of myself.

My question is: When getting started with these kinds of systems, is it very important to have that kind of flexibility available from the beginning? Or is it better to get things started quickly, gather data, and port it over to a more robust &amp; flexible solution later on?",3,0
121,2015-11-6,2015,11,6,4,3rnyyb,Using differential privacy to reuse training data,https://www.reddit.com/r/MachineLearning/comments/3rnyyb/using_differential_privacy_to_reuse_training_data/,alexeyr,1446751253,,1,7
122,2015-11-6,2015,11,6,4,3ro3ca,Volunteering/getting additional research experience?,https://www.reddit.com/r/MachineLearning/comments/3ro3ca/volunteeringgetting_additional_research_experience/,rrrrme,1446752938,[removed],3,0
123,2015-11-6,2015,11,6,5,3ro8db,What Counting Jelly Beans Can Teach Us About Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3ro8db/what_counting_jelly_beans_can_teach_us_about/,dnabeyta,1446754877,,0,0
124,2015-11-6,2015,11,6,5,3roc5i,Maybe - simple machine learning API,https://www.reddit.com/r/MachineLearning/comments/3roc5i/maybe_simple_machine_learning_api/,datazoo-maybe,1446756360,"Hi guys,

We are two Computer Science students in the Netherlands with a passion for data. Recently we thought it would be cool to have a machine learning API for simple projects on low budget/little time. In the past days we have been hacking around and made an initial design http://datazoo.club/maybe/ref=rml. As a first step we decided to support classification, but other tasks could be added shortly. We really aim for simplicity and rapid implementation with good enough results, e.g. a best guess prediction to be used for non-critical functionality. 

We are just curious if this is something that could make your life easier. What do you think? 

If you need any help or would like to get in touch, just reply here, send us an email at maybe@datazoo.club, or reach us at irc://irc.foonetic.net/datazoo-maybe-support

http://datazoo.club/maybe/ref=rml
",2,0
125,2015-11-6,2015,11,6,7,3roqjn,Introduction to Machine Learning Theory and Its Applications - A Visual Tutorial with Examples,https://www.reddit.com/r/MachineLearning/comments/3roqjn/introduction_to_machine_learning_theory_and_its/,vincentg64,1446762085,,0,1
126,2015-11-6,2015,11,6,7,3rore8,[Question] Comparing documents using word2vec,https://www.reddit.com/r/MachineLearning/comments/3rore8/question_comparing_documents_using_word2vec/,gbenne,1446762455,"Can I generate word2vec models for different documents (their vocab may vary) then using words that are in all documents to calculate their similarities? (e.g sum the vectors from one model and then calculate the distance to the sum of vectors from another model, the sum is only to words that are in both documents)

I know there are other (and better) ways of doing this, but I'm curious if this would work.
",5,1
127,2015-11-6,2015,11,6,7,3rotbf,Just found this: Google now uses a DNN for automatic Youtube video thumbnail selection.,https://www.reddit.com/r/MachineLearning/comments/3rotbf/just_found_this_google_now_uses_a_dnn_for/,sirkloda,1446763249,,2,59
128,2015-11-6,2015,11,6,9,3rp7a5,A New Cortical Column Model,https://www.reddit.com/r/MachineLearning/comments/3rp7a5/a_new_cortical_column_model/,CireNeikual,1446769444,,1,2
129,2015-11-6,2015,11,6,10,3rpjzi,Machine learning peoples. Tell me about when you got started.,https://www.reddit.com/r/MachineLearning/comments/3rpjzi/machine_learning_peoples_tell_me_about_when_you/,DancesWithPizzas,1446775179,"Hi everyone.  So I'm really into machine learning and all things 'data sciency'. I've got a handful of years working in the public sector doing data analysis/research kind of stuff (nothing like apply ML techniques though). I'm also in grad school part-time in the NYC area, in a non-CS department though the majority of my classes have been CS/Stats related (think ML, NLP, algorithms, distributed computing, etc.). My school program is kinda like machine learning for the social sciences. I'm thinking about switching jobs into a company that does more ML kinda of stuff. But when I look at these job posting I see a lot of requirements about pure CS backgrounds, etc. Which is kinda of troubling since I don't have that structured CS training (rather I learn it as I go or when necessary).

For the folks who work in a ML role or at a company as such, does this background matter? Do you look for people with really solid training a CS? Also, what was is like when you started after school? Was the work load completely different than what you saw in school? More difficult, less difficult? Thanks in advance.


ps sorry for the cross post with hacker new.",5,0
130,2015-11-6,2015,11,6,11,3rpmy3,Dry standard of Kiln dried wood,https://www.reddit.com/r/MachineLearning/comments/3rpmy3/dry_standard_of_kiln_dried_wood/,xinan2015,1446776643,[removed],0,1
131,2015-11-6,2015,11,6,11,3rpofx,A note on the evaluation of generative models,https://www.reddit.com/r/MachineLearning/comments/3rpofx/a_note_on_the_evaluation_of_generative_models/,emansim,1446777345,,3,12
132,2015-11-6,2015,11,6,13,3rq2ut,Neural Network Libraries In Java,https://www.reddit.com/r/MachineLearning/comments/3rq2ut/neural_network_libraries_in_java/,donginafuku,1446784062,,0,0
133,2015-11-6,2015,11,6,13,3rq5b6,[Question] Recursive neural network with variable-length output,https://www.reddit.com/r/MachineLearning/comments/3rq5b6/question_recursive_neural_network_with/,outlacedev,1446785357,"(I originally posted in /r/MLQuestions but looks like that place is pretty inactive so re-posting here)

I made this https://github.com/outlace/Machine-Learning-Experiments/blob/master/VariableOutput.ipynb
It's a simple 3 layer neural network (2x3x2 [not counting biases]) where one of the output nodes controls whether or not to recursively call the neural network again. The other output node produces the actual output data. This allows it to produce variable-length output vectors, or it could even learn to not produce any output at all given an input.

Has there been any use for this type of a network? I did some research but it seems people only use recurrent neural networks for variable ins/outs.",3,3
134,2015-11-6,2015,11,6,14,3rq91z,5 Best Machine Learning APIs for Data Science,https://www.reddit.com/r/MachineLearning/comments/3rq91z/5_best_machine_learning_apis_for_data_science/,margarettecrystal,1446787365,,0,1
135,2015-11-6,2015,11,6,14,3rq9l1,Are there any best practices while using the PCA algorithm on a dataset?,https://www.reddit.com/r/MachineLearning/comments/3rq9l1/are_there_any_best_practices_while_using_the_pca/,Dawny33,1446787651,,1,3
136,2015-11-6,2015,11,6,15,3rqg1n,"Scikit-learn 0.17 is out: LDA topic models, faster NMF, faster trees, faster T-SNE, ...",https://www.reddit.com/r/MachineLearning/comments/3rqg1n/scikitlearn_017_is_out_lda_topic_models_faster/,cast42,1446791463,,10,40
137,2015-11-6,2015,11,6,15,3rqgvp,Confused about RNN batch sizes,https://www.reddit.com/r/MachineLearning/comments/3rqgvp/confused_about_rnn_batch_sizes/,DrCrypto,1446791999,"Hello everyone,

Apologies for the long wall of text, I promise I tried to keep it concise :/

I am going to start by presenting an analogy of the problem I am trying to solve.

&gt; Suppose I am monitoring the commercial activity at a shopping mall, and that my goal is to . Each customer **i** that enters the mall is monitored until he leaves it, for a duration of **n_duration(i)** minutes. At each minute, I collect **n_param** parameters on the visitor's activity. Let's say I am trying to predict **n_target** parameters (e.g., number of seconds not moving, distance to the book store over that minute), based on **n_input** parameters (e.g., number of steps walked / number of words spoken during that minute), with **n_input + n_target = n_param**. I collect this data for every customer that comes to the shopping mall after 2pm and leaves before 5pm and end up with a collection of **n_customers** input and target arrays, of respective sizes **n_duration(i)**x**n_input** and **n_duration(i)**x**n_target**.

Note: the dummy parameters presented here don't necessarily make sense, but let's suppose we can have reasonable confidence they are linked somehow.

I previously tried to train a vanilla neural network (MLP) to solve my problem, and got relatively satisfying results. One of my main problems was that the MLP could not capture the continuity in the input parameters I fed it (for instance, it is clear that the *distance to the book store* parameter shouldn't vary widely between time steps due to space-time travel limitations).

I have then tried solving my problem using recurrent neural networks with Torch. While most tutorials I found deal with classification on image / text data, I am trying to do regression over scalar quantities, therefore I had a bit of trouble troubleshooting what was wrong in my problem formulation, but hey, at least I learnt some stuff :)

Anyway, I tried training RNNs/LSTMs over my dataset, and I found that I had to deal with a batch size parameter, and I am having trouble figuring out what it does exactly.
On my old regular MLP, I would just feed it each input sample with the corresponding target, one after the other, but now I have to split my full dataset into smaller batches.

**My first question is: what exactly are these batches?**

My first understanding was that they regroup samples into sequences that are temporally linked, which I got from the description of the corresponding field in [this example](https://github.com/Element-Research/rnn/blob/master/examples/recurrent-language-model.lua#L19):

    cmd:option('--batchSize', 32, 'number of examples per batch')

However, in [https://github.com/karpathy/char-rnn/blob/master/train.lua#L48](this example), the description is as follows:

    cmd:option('-batch_size',50,'number of sequences to train on in parallel')

I'm a bit confused now, as in the first case, I had in mind that the RNN processed each example after the other and learnt the time dependency that way, but the second description appears to me like a way to accelerate the training (as in parallel programming), without time consideration. In this second description, time would be taken into account through the ""sequences"" mentioned. In other words:

* Description 1 seems to say that a batch is a sequence (successive samples)

* Description 2 seems to say that a batch is a collection of sequences processed in parallel for computational efficiency

I guess both make sense on their own, but is one of these definitions more prevalent than the other? I think this particularly makes it difficult to find the right resources to learn about it (I'd be very grateful for some pointers on this matter).

**My second question is: how should the batch size influence the way I feed my dataset to the RNN?**

I used the first definition, as I based my code on this tutorial, but my understanding of it may also be flawed. In the toy example I described, each batch would correspond to a dataset relative to a given customer. However, the batch size apparently needs to be unique, which is problematic here, as customers can have very different visit durations. For instance, let's simplify and say I have three customers, that respectively stay 60, 90, and 110 minutes. Ideally, I would want to train my RNN with the first sequence of 60 samples, then 90m then 110. However, the RNN implementation requires as input a (torch.Tensor) 3D matrix of fixed size **n_batches**x**batch_size**x**n_input**, and I cannot have different sizes per batch. I tried two things:

* taking batch_size = max(sequence_sizes), here 110, and padding the shorter sequences with zeros: unsatisfying results as some sequences require adding a lot of zeros

* finding a smaller batch size that could ""make sense"", e.g. each customer goes to the bathroom every 15 minutes, but I couldn't observe anything vaguely periodic.

* deciding on an arbitrary batch size. For instance, with a batch size of 20 minutes, the first customer's 60 minutes then appear like three customers visiting for 20 minutes.

For now, I settled on the third option, as I think it was the lesser evil among these options. While I have relatively fine results after training my RNN with my dataset split like that, I am still not satisfied about my understanding of it.

My concerns are as follows:

* I lose some information relative to similar patterns across customers: for instance, each visit starts and ends in the vicinity of the entrance/exits, which does not appear anymore when splitting the dataset into smaller sequences.

* I am not sure about this one, but could some temporal discontinuity appear in the RNN's predictions? When trying to predict the behaviour of a customer staying 80 minutes, I currently have to split this dataset into four batches of 20 samples. It is not clearly visible on the resulting predicted data, but the way I see it, if the four batches are processed independently, couldn't they be discontinuous with each other? For the prediction phase, I would rather try to send a single batch of 80 samples at once in order to preserve continuity, but I get size mismatch errors when trying to do so. Are RNNs tied to a specific batch size once they're trained, or am I just doing something wrong in my code?

I realise my message is very long, so thank you very much if you've read this far.
Thank you in advance for your help!",2,3
138,2015-11-6,2015,11,6,16,3rqkzz,ryankiros/neural-storyteller - A recurrent neural network for generating little stories about images,https://www.reddit.com/r/MachineLearning/comments/3rqkzz/ryankirosneuralstoryteller_a_recurrent_neural/,_spreadit,1446794749,,14,118
139,2015-11-6,2015,11,6,18,3rqumf,The X10 Programming Language for computer cluster and GPGPU,https://www.reddit.com/r/MachineLearning/comments/3rqumf/the_x10_programming_language_for_computer_cluster/,ttt72,1446802346,,6,8
140,2015-11-6,2015,11,6,19,3rqyri,An experiment in generating stories about images (incl. videos &amp; taylor swift),https://www.reddit.com/r/MachineLearning/comments/3rqyri/an_experiment_in_generating_stories_about_images/,samim23,1446805758,,0,3
141,2015-11-6,2015,11,6,20,3rr4pv,"FORGET BIG DATA, THINK MACHINE LEARNING Big Data Week Conference - For a limited time only, you get access to one conference track of your choice for a massive 55% off the regular ticket. Hurry up, offer ends tonight.",https://www.reddit.com/r/MachineLearning/comments/3rr4pv/forget_big_data_think_machine_learning_big_data/,BigDataLady,1446810426,,0,1
142,2015-11-6,2015,11,6,21,3rraah,Can I use Python's ML libraries while working on Scala in Spark?,https://www.reddit.com/r/MachineLearning/comments/3rraah/can_i_use_pythons_ml_libraries_while_working_on/,Dawny33,1446814250,,0,2
143,2015-11-6,2015,11,6,22,3rre2r,Made a video explaining backprop without formal calculus. Feedback?,https://www.reddit.com/r/MachineLearning/comments/3rre2r/made_a_video_explaining_backprop_without_formal/,algomanic,1446816547,,0,1
144,2015-11-7,2015,11,7,0,3rrphn,Thoughts on Massively Scalable Gaussian Processes (paper with a major breakthrough: almost exact kernel methods scaling linearly in training time),https://www.reddit.com/r/MachineLearning/comments/3rrphn/thoughts_on_massively_scalable_gaussian_processes/,cast42,1446822108,,8,54
145,2015-11-7,2015,11,7,0,3rrt5m,Accuracy and kappa of 1.0,https://www.reddit.com/r/MachineLearning/comments/3rrt5m/accuracy_and_kappa_of_10/,hellomachinelearning,1446823718,"Hello,
I have a dataset with many variables and a lot of them have little data in them. I ran a multinomial neuralnet on data because y variable is categorial with 6 discrete outcomes. When I measured accuracy I got back 1.0 in accuracy and 1.0 in kappa.
This means that the model is overfitting correct?",7,0
146,2015-11-7,2015,11,7,0,3rrx4j,I quickly need your help: Is it possible to implement the Softmax derivative independent from any loss function?,https://www.reddit.com/r/MachineLearning/comments/3rrx4j/i_quickly_need_your_help_is_it_possible_to/,danijar,1446825396,,6,0
147,2015-11-7,2015,11,7,0,3rrxim,What is the minimum number of samples needed to use a deep learning algorithm?,https://www.reddit.com/r/MachineLearning/comments/3rrxim/what_is_the_minimum_number_of_samples_needed_to/,sleepicat,1446825550,Is it simply a factor of the number of classes that you're trying to identify with the algorithm?,8,0
148,2015-11-7,2015,11,7,2,3rsavm,Alternative cost functions,https://www.reddit.com/r/MachineLearning/comments/3rsavm/alternative_cost_functions/,rudyl313,1446830913,"Usually a binary classifier will use a cost function like:

number of incorrect predictions / total number of predictions

But, I was working on a binary classifier where I thought to myself: ""I'd like to model to be able to say: I don't know"". This lead me to think that a better cost function would be based on a percentage prediction (0% if certainly 0, 100% if certainly 1) by the model. If the model predicts 50% then it's uncertain if it will be 1 or 0. Then I could make the cost function:

sum(abs(correct value - percent prediction)) or sum(sqrt((correct value - percent prediction) ^ 2))

I'm not sure how convex this cost function would be, but I could implement a linear classifier (using the optim function in R or equivalent) using any cost function I like, but I don't think I'd be able to do it for other non-linear models like random forests, GBMs or neural networks.

Any advice?  ",5,1
149,2015-11-7,2015,11,7,3,3rsk94,Great twitter follows?,https://www.reddit.com/r/MachineLearning/comments/3rsk94/great_twitter_follows/,blowjobtransistor,1446834628,"Alright, we had great news sources to follow - who are some great people to follow on Twitter?",4,8
150,2015-11-7,2015,11,7,4,3rsor8,What are the best ipython notebooks for learning/teaching machine learning and applied stats?,https://www.reddit.com/r/MachineLearning/comments/3rsor8/what_are_the_best_ipython_notebooks_for/,holdie,1446836415,"I'm helping to organize a course in statistics/data analysis. We'll have about 4 weeks to talk about machine learning in general, and I'm trying to find some good resources (ideally IPython Notebooks) to illustrate the principles of machine learning. Not just a ""here's how to use package X and function Y"", but some of the general principles of linear algebra, optimization, etc.

E.g., what's a hyperplane, how does it relate to classifying, how does it differ for different types of classifiers?

Anyone know of something that could be useful?",4,12
151,2015-11-7,2015,11,7,5,3rt13m,"Jobs USA: Business Analyst- Natural Language Processing (NLP) Job Houston, TX",https://www.reddit.com/r/MachineLearning/comments/3rt13m/jobs_usa_business_analyst_natural_language/,Jobs_USA_reddit,1446841300,,1,1
152,2015-11-7,2015,11,7,5,3rt24f,Anyone had troubles with theano and keras on the gpu? It computes the loss as NaN.,https://www.reddit.com/r/MachineLearning/comments/3rt24f/anyone_had_troubles_with_theano_and_keras_on_the/,nick_ok,1446841704,"Just wondering if anyone else has seen this problem. I can't seem to find a fix for it online. Thanks :)

edit: the problem seems to have been caused by 'optimizer=fast_compile' just remove that from .theanorc and it works fine :)",4,0
153,2015-11-7,2015,11,7,6,3rt6tk,Deepbeat - Machine Learning to Generate Rap Lyrics,https://www.reddit.com/r/MachineLearning/comments/3rt6tk/deepbeat_machine_learning_to_generate_rap_lyrics/,dabshitty,1446843614,,0,5
154,2015-11-7,2015,11,7,6,3rt9j5,Autograd for Torch,https://www.reddit.com/r/MachineLearning/comments/3rt9j5/autograd_for_torch/,kswerve,1446844718,,7,24
155,2015-11-7,2015,11,7,6,3rtbfa,[Question] Learning Both Numeric and Binary Features,https://www.reddit.com/r/MachineLearning/comments/3rtbfa/question_learning_both_numeric_and_binary_features/,darklinggg,1446845489,"Hello,

I have a dataset with binary features, numeric features, and categorical features. For testing purposes, I am going to ignore the categorical features right now. I was wondering if it's possible to apply a linear regression algorithm like Gradient Descent to a feature vector with both binary and numeric features? Or am I restricted to one data type or the other? Any help would be greatly appreciated!",0,0
156,2015-11-7,2015,11,7,6,3rtcj6,Using machine learning to predict gender,https://www.reddit.com/r/MachineLearning/comments/3rtcj6/using_machine_learning_to_predict_gender/,linuxwil,1446845935,,9,18
157,2015-11-7,2015,11,7,7,3rtl3b,Beginners Guide: Apache Spark Machine Learning with Large Data,https://www.reddit.com/r/MachineLearning/comments/3rtl3b/beginners_guide_apache_spark_machine_learning/,djphilosopher,1446849627,,0,27
158,2015-11-7,2015,11,7,8,3rtq3l,"See your HTM run -- NuPIC's behavior, plotted across time",https://www.reddit.com/r/MachineLearning/comments/3rtq3l/see_your_htm_run_nupics_behavior_plotted_across/,mrcslws,1446851911,,0,4
159,2015-11-7,2015,11,7,11,3rudcr,Torch or Nervana for new code?,https://www.reddit.com/r/MachineLearning/comments/3rudcr/torch_or_nervana_for_new_code/,[deleted],1446862994,"**Torch or Nervana for new code?**

I'm looking at them, and I don't know what the right choice is for new code.

**Torch:**

All the cool and BIG kids are using it (Google, DeepMind, FB). Some online ML courses are even teaching it.

**Nervana or Neon:**

Seems to run faster. Python-only, no need to learn Lua or get used to 1-based indexing! Supported by a start-up. Future objectively less certain than that of Google and FB.

**So I'm wondering if others faced the same question and have any thoughts that might help me here**

",37,25
160,2015-11-7,2015,11,7,11,3ruhib,Efficiently determine if two files are similar,https://www.reddit.com/r/MachineLearning/comments/3ruhib/efficiently_determine_if_two_files_are_similar/,FutureIsMine,1446865192,Here is my scenario: I have a stream of job posts and what I would like to do is  determine how similar are two job posts. Where Im having trouble is figuring out a way to store a lot of these documents and be able to examine how similar a given post is to posts already in a database without having to go through all the entries in a database for each new addition. What is a good approach to these kind of situations? ,7,6
161,2015-11-7,2015,11,7,12,3rukqz,"Face vs. Non-Face Neural network Example in Theano, or any other Image binary classifier on real world images?",https://www.reddit.com/r/MachineLearning/comments/3rukqz/face_vs_nonface_neural_network_example_in_theano/,deep_learner,1446866941,"Hi,
I am looking for a theano example on binary classification on images e.g. Face vs. Non-Face or any other binary classification problem. 

Though i found a keypoint detector on daniel nouri's blog, its not exactly what i want. Does anyone know of a github repo that might have made something of this sort available?  

Thanks!",3,1
162,2015-11-7,2015,11,7,13,3ruttk,Intelligent Learning: Similarity Control and Knowledge Transfer (Prof. Vladimir Vapnik),https://www.reddit.com/r/MachineLearning/comments/3ruttk/intelligent_learning_similarity_control_and/,saigeco,1446872070,,0,3
163,2015-11-7,2015,11,7,14,3ruw4m,Time Attendance System,https://www.reddit.com/r/MachineLearning/comments/3ruw4m/time_attendance_system/,matrixsecusol,1446873425,,0,0
164,2015-11-7,2015,11,7,17,3rva8g,How to invest with Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/3rva8g/how_to_invest_with_artificial_intelligence/,adeniyiks,1446883568,,0,1
165,2015-11-7,2015,11,7,18,3rvf3y,"[Question] Dimension of each element x_i of input sequence X = (x_1, ..., x_T) in LSTM architecture.",https://www.reddit.com/r/MachineLearning/comments/3rvf3y/question_dimension_of_each_element_x_i_of_input/,eunjilee,1446888005,"In [Beyond Short Snippets: Deep Networks for Video Classification](http://arxiv.org/pdf/1503.08909v2.pdf), what is the dimension of x_i in an input sequence X(3.2 LSTM architecture)?
Since x_i represents a feature at frame T, I guess it has 4096-dim or something like that. But it looks like that x_i is scalar(1-dim) as I look into some codes(ex. Keras).
I'm doing a research on video classification and this issue is so confusing to me. Please help.",2,2
166,2015-11-7,2015,11,7,18,3rvgx0,Establishing Objective to Manufacture Best of Pharmaceutical Products - Anchor Mark,https://www.reddit.com/r/MachineLearning/comments/3rvgx0/establishing_objective_to_manufacture_best_of/,Anchormark1,1446889667,,0,1
167,2015-11-7,2015,11,7,18,3rvh2i,How do I lay a solid foundation for the practical side of machine learning?,https://www.reddit.com/r/MachineLearning/comments/3rvh2i/how_do_i_lay_a_solid_foundation_for_the_practical/,V1ncam,1446889810,[removed],6,0
168,2015-11-7,2015,11,7,22,3rvxy7,Looking for a dataset of political speeches to train a rnn. Is this the right place to look for help?,https://www.reddit.com/r/MachineLearning/comments/3rvxy7/looking_for_a_dataset_of_political_speeches_to/,GPU-Brain,1446903646,"Me and my friend think it will be quite a lot of fun to compare the speeches a rnn generates for various current and historic political figures (Trump anyone?). The problem is where to find the dataset? The sources I found are not enough to get even a 2MB file. 

Reddit detectives?",4,3
169,2015-11-7,2015,11,7,22,3rvzgc,Understanding Convolutional Neural Networks for NLP,https://www.reddit.com/r/MachineLearning/comments/3rvzgc/understanding_convolutional_neural_networks_for/,pogopuschel_,1446904666,,11,94
170,2015-11-8,2015,11,8,2,3rwpo7,"BinaryConnect Update: ""We obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN""",https://www.reddit.com/r/MachineLearning/comments/3rwpo7/binaryconnect_update_we_obtain_near_stateoftheart/,downtownslim,1446917810,,8,24
171,2015-11-8,2015,11,8,3,3rwzyg,"Finding the probability of an event occurring, without knowing the number of trials, given the number of occurrences.",https://www.reddit.com/r/MachineLearning/comments/3rwzyg/finding_the_probability_of_an_event_occurring/,johnryan465,1446922285,"I wish to find the probability of an event concurring without knowing the number of trials. The number of occurrences is the number of vehicles that pass a point on a given road. I was thinking that you would model the problem as two separate functions perform regression on each and then multiply the results of each function to get the number of expected outcomes and the difference between the number of actual occurrences would be the error function. How would the model tell which function should have regression applied at any given time, due to the fact that either function could be the problem? The feature vector of each function contain some overlapping items if that helps. I have a few data-sets with the number of occurrences recorded but I have less that 30 of these and my total data-set is over 200000 items, also these data sets are slight outliers. Any help in the best way to model this problem would be greatly appreciated appreciated. ",4,4
172,2015-11-8,2015,11,8,4,3rx5r8,Announcing CGT,https://www.reddit.com/r/MachineLearning/comments/3rx5r8/announcing_cgt/,[deleted],1446924810,,14,20
173,2015-11-8,2015,11,8,4,3rx7zb,KDD 2015: 0-Bit Consistent Weighted Sampling,https://www.reddit.com/r/MachineLearning/comments/3rx7zb/kdd_2015_0bit_consistent_weighted_sampling/,mttd,1446925772,,0,2
174,2015-11-8,2015,11,8,5,3rxb9z,When you interview a data scientist,https://www.reddit.com/r/MachineLearning/comments/3rxb9z/when_you_interview_a_data_scientist/,HappyDataScientist,1446927212,,0,1
175,2015-11-8,2015,11,8,11,3ryk9n,Is randomized input data less prone to the exploding gradient problem?,https://www.reddit.com/r/MachineLearning/comments/3ryk9n/is_randomized_input_data_less_prone_to_the/,jstaker7,1446948037,[removed],1,0
176,2015-11-8,2015,11,8,11,3rykzi,Metric Learning in Python v0.2.0 available! Now support both python 3 and python 2!,https://www.reddit.com/r/MachineLearning/comments/3rykzi/metric_learning_in_python_v020_available_now/,terrytangyuan,1446948424,,0,11
177,2015-11-8,2015,11,8,11,3rynlh,Demonstration of Facebook's AI by Yann LeCun,https://www.reddit.com/r/MachineLearning/comments/3rynlh/demonstration_of_facebooks_ai_by_yann_lecun/,Lyle_Cantor,1446949799,,46,146
178,2015-11-8,2015,11,8,20,3rzwwh,Need help in audio signal processing,https://www.reddit.com/r/MachineLearning/comments/3rzwwh/need_help_in_audio_signal_processing/,arvindkumar422,1446983218,"So i am implementing a project on determining a birds species from its song (wav file) I figured [this method](http://hearinghealthmatters.org/waynesworld/2012/animal-vocalization-analysis-the-gplp/) can be tried out...
I need help on implementing it on python... 
Any help would be appreciated",4,0
179,2015-11-8,2015,11,8,21,3s00yk,MIT EmTech Videos,https://www.reddit.com/r/MachineLearning/comments/3s00yk/mit_emtech_videos/,JackieForien,1446986824,,0,3
180,2015-11-8,2015,11,8,23,3s09bo,What tool can I use to draw a Convolutional Neural Network for my report?,https://www.reddit.com/r/MachineLearning/comments/3s09bo/what_tool_can_i_use_to_draw_a_convolutional/,wildtales,1446992516,[removed],3,2
181,2015-11-9,2015,11,9,1,3s0lrv,More people would get into AI if we called it simulated brainporn,https://www.reddit.com/r/MachineLearning/comments/3s0lrv/more_people_would_get_into_ai_if_we_called_it/,[deleted],1446999119,[deleted],0,0
182,2015-11-9,2015,11,9,1,3s0op5,Interactive t-SNE visualization demo with tsne-js,https://www.reddit.com/r/MachineLearning/comments/3s0op5/interactive_tsne_visualization_demo_with_tsnejs/,transcranial,1447000514,,1,7
183,2015-11-9,2015,11,9,2,3s0t7c,Evan Shelhamer: Working with Caffe [Slides for CS231n: CNN for Visual Recognition],https://www.reddit.com/r/MachineLearning/comments/3s0t7c/evan_shelhamer_working_with_caffe_slides_for/,kunjaan,1447002471,,0,1
184,2015-11-9,2015,11,9,2,3s0wgo,[Discuss] Generative models don't seem so general after all.,https://www.reddit.com/r/MachineLearning/comments/3s0wgo/discuss_generative_models_dont_seem_so_general/,gengenmodels,1447003796,"Since this paper came out: ""A note on the evaluation of generative models"" http://arxiv.org/abs/1511.01844, it seems like there's no reliable metric for assessing the quality of a generative model. It seems to be wholly application specific. This is contrary to belief that once you have a good generative model of your data, you have something much more powerful than a discriminative model. You have a model that 'understands' the data distribution completely.

But this paper implies that when you have a generative model, you might have nothing more than something that gets a high number with one particular metric. And these metrics do not generalize very well.

What does this mean for researchers who work on these kinds of models?

What is considered 'progress' in the space of generative models?",19,31
185,2015-11-9,2015,11,9,2,3s0yf2,Next Please?,https://www.reddit.com/r/MachineLearning/comments/3s0yf2/next_please/,sethibharat,1447004640,,0,1
186,2015-11-9,2015,11,9,4,3s1fqn,I want to build a service that blocks all content related to a specific keyword. The articles/news feeds that are in any way related to the Kardshians or Miley Cyrus would just not show up unless I search for it specifically.,https://www.reddit.com/r/MachineLearning/comments/3s1fqn/i_want_to_build_a_service_that_blocks_all_content/,[deleted],1447011782,[deleted],1,0
187,2015-11-9,2015,11,9,4,3s1g3w,Need help with Data Mining,https://www.reddit.com/r/MachineLearning/comments/3s1g3w/need_help_with_data_mining/,ilw50848foxjacom,1447011922,"I'm a beginner in Data Mining. I have an imbalanced data of 2k+features and 14k+ samples classified into 100 class. I don't have test data, just the train data, and I have only 2 chances to submit the output over test data. How should I approach it? What are the methods I can use to remove class imbalance, classify almost correctly.

Some points I think are,
1. SVM : Not useful, 2048 features are too much for 14k+ samples to be used with SVM.
2. Decision Tree : Maybe
3. Naive Bayes : No way
4. Artificial Neural Network : Slow, most probably, not good enough.

Please help. As I said, I'm no expert, so I can be wrong. I want to use everything I can use, bootstrapping, boosting or whatever, if it can be applied, and will help in getting better prediction.",2,0
188,2015-11-9,2015,11,9,5,3s1jpb,Statsmodels for large data?,https://www.reddit.com/r/MachineLearning/comments/3s1jpb/statsmodels_for_large_data/,napsternxg,1447013388,"I like using statsmodels package for training my linear models in python. One of the major reason for using this package is the nice output format where it lists the coefficient significance as well as std. error for all features. Also the fact that I can use the formula feature to specify my model makes it my go to choice to doing any kind of data analysis.

However, recently I have started working with large data sets with 22M rows and 100+ features. With the data of this scale, I feel the need for using a faster and more memory efficient model which can give me the same results.

Is there a way to efficiently use statsmodelsor a similar python module which can give me linear model results using the same format and in less time (probably using parallel processing) and less memory overhead?

I have also started using pyspark recently and am impressed with its parallel computing abilities and am aware of its Mlib module but couldn't find anything which provides the same functionality in terms of the descriptive outputs given by statsmodels.",2,1
189,2015-11-9,2015,11,9,7,3s2657,dmlc/mxnet,https://www.reddit.com/r/MachineLearning/comments/3s2657/dmlcmxnet/,[deleted],1447022741,,4,2
190,2015-11-9,2015,11,9,8,3s2a4e,"[Question] Part-Time (professional) Data Engineers/Data Scientists, Where &amp; How did you start work on/find projects ?",https://www.reddit.com/r/MachineLearning/comments/3s2a4e/question_parttime_professional_data_engineersdata/,__sirius,1447024429,"For those who work on part-time jobs as contractors on freelancing projects. How can someone with professional experience to get started and find both consulting &amp; software freelancing projects ?. 

I have some free time, and I would like to know how can I start finding these jobs. 

I tried using ODesk, but unfortunately, the level of projects submitted by most clients is vague (without clear milestones), in addition to that the level of price competition is very low (for someone living in expensive city). I tried also ""who is hiring"", but most of companies look for full-time employees, not seasoned data engineerS/data scientists, who can work on part-time projects (approximately: 5-4 hours a day).    

Please, share your experience with us ?
",9,17
191,2015-11-9,2015,11,9,8,3s2cw4,Favourite conferences out of some of the big ones?,https://www.reddit.com/r/MachineLearning/comments/3s2cw4/favourite_conferences_out_of_some_of_the_big_ones/,nsfy33,1447025654,"I usually get to go to one with work and a lot are in NA this year (SIAM, ICDM, ICML, KDD). Wondering which ones people prefer - I've only gone to internal government ones so far.

If it helps, my job is mostly applied data mining/machine learning so I like things I can use rather than cutting edge results",4,2
192,2015-11-9,2015,11,9,10,3s2pwz,Need a hit?...check this bit,https://www.reddit.com/r/MachineLearning/comments/3s2pwz/need_a_hitcheck_this_bit/,sethibharat,1447031546,,0,0
193,2015-11-9,2015,11,9,10,3s2rgn,State of the art on permutation invariant SVHN?,https://www.reddit.com/r/MachineLearning/comments/3s2rgn/state_of_the_art_on_permutation_invariant_svhn/,alexmlamb,1447032217,"I.e. the best result that can be achieved without considering the image-structure of the data.  

The baselines here seem to use convnets: 

http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#5356484e

This paper gives a 9.3% error rate as their ""control"" for fully connected SVHN, but I don't think that they claim that this result is state of the art.  

http://arxiv.org/pdf/1312.4461v4.pdf

If you're wondering why I care, my goal is to have a dataset to evaluate fully connected NN on, which is less trivial than MNIST.  ",2,3
194,2015-11-9,2015,11,9,11,3s30wd,"I answered a whole bunch of general ML questions which I just compiled to a FAQ, looking for feedback!",https://www.reddit.com/r/MachineLearning/comments/3s30wd/i_answered_a_whole_bunch_of_general_ml_questions/,rasbt,1447036565,,14,67
195,2015-11-9,2015,11,9,16,3s3vnt,POLYESTER FLAKES PELLEIZING MACHINE_PET BOTTLE FLAKES GUANULATING LINE_F...,https://www.reddit.com/r/MachineLearning/comments/3s3vnt/polyester_flakes_pelleizing_machine_pet_bottle/,cisscomachine,1447053225,,1,0
196,2015-11-9,2015,11,9,18,3s44rr,[Question] Recommendations for literature on using neural network for information extraction from structured data?,https://www.reddit.com/r/MachineLearning/comments/3s44rr/question_recommendations_for_literature_on_using/,mimighost,1447060360,"Hi, I am looking into a potentially interesting topic recently. There are a lot of tools/library to help you extract a *cleaner* and more *readable* view from a html page, like the one below:
https://github.com/mozilla/readability

However, those tools/library often runs on the client side, so most of them uses hand-tuned rules or simple scoring system for picking important element, it is inevitable sometimes the result is not quite good. 

Now suppose we can label some of the output from those rule-based systems as good, others as bad, is there any existing machine learning method, particularly neural network powered ones can help to improve the performance? ",1,1
197,2015-11-9,2015,11,9,18,3s457t,POLYESTER ZIPPER MONOFILAMENT MAKING MACHINE_POLYPROPYLENE ZIPPER FILAME...,https://www.reddit.com/r/MachineLearning/comments/3s457t/polyester_zipper_monofilament_making_machine/,cisscomachine,1447060752,,0,0
198,2015-11-9,2015,11,9,18,3s45bm,Gmail AI: What's Privacy concerns about Smart Reply?,https://www.reddit.com/r/MachineLearning/comments/3s45bm/gmail_ai_whats_privacy_concerns_about_smart_reply/,Questechie,1447060830,,1,1
199,2015-11-9,2015,11,9,19,3s4cbr,Regressing Images: 24 Hours in New Orleans (incl. video),https://www.reddit.com/r/MachineLearning/comments/3s4cbr/regressing_images_24_hours_in_new_orleans_incl/,samim23,1447066651,,0,5
200,2015-11-9,2015,11,9,20,3s4gbu,Tips to deal with class imbalance,https://www.reddit.com/r/MachineLearning/comments/3s4gbu/tips_to_deal_with_class_imbalance/,arclite123,1447069689,,0,0
201,2015-11-9,2015,11,9,22,3s4qpm,Google Tensorflow released,https://www.reddit.com/r/MachineLearning/comments/3s4qpm/google_tensorflow_released/,samim23,1447076147,,152,679
202,2015-11-9,2015,11,9,23,3s4yaf,[Question] Want to know more about OCR guidance needed.,https://www.reddit.com/r/MachineLearning/comments/3s4yaf/question_want_to_know_more_about_ocr_guidance/,tazonis,1447079914,"I want to create an OCR app for android for Greek characters. Am thinking of using a multi layer percepron network for the core but am fairly new to this. I know how to create the network but I have some holes to fill in. I read that for the training process I need to feed it with with some training images witch I made and match the exit nodes of the network, so for all Greek letters upper case and lower, numbers and characters I need to create some binarized training images, and if I want more than one font then I multiply by the number of fonts I want it to support. This method I think is called back propagation? After I train the network I save the values of the weights to use for the network I will include in the app. In the app i want to take a picture and then crop it where i want to read, so for example if I take a picture of a menu I will crop the image where there is something I want to read to avoid unnecessary pixels. After the image is croped I know that you need to preprocess it somehow. You need to scale it down to black and white, correct the orientation and somehow crop each character out of it to start the recognition. Where I need help is to how to make my training images, I know you need to use some binarize method to make the data understandable for the neurons. Do you know any library on android that does image processing? And what size must the train images must be? Also I have no idea how to crop the images of the characters from an image, do they have to be same size as the training images?. Anything could help, even general guidelines am not seeking a programming answer more than an answer that helps me learn more about the OCR in general. 

Thank you for your time,
best regards.",2,1
203,2015-11-10,2015,11,10,0,3s58fu,Right skewed vars and applying Log to them.,https://www.reddit.com/r/MachineLearning/comments/3s58fu/right_skewed_vars_and_applying_log_to_them/,hellomachinelearning,1447084408,"Hello,
I have a dataset with a lot of variables/predictors with right skewed distributions. When I apply log to them it fixes it and they become normal distributed but since dataset has a lot of 0 values they become -Inf and most algorithms do not accept that. How can I fix this?",2,0
204,2015-11-10,2015,11,10,1,3s59lg,[1511.02222] Deep Kernel Learning,https://www.reddit.com/r/MachineLearning/comments/3s59lg/151102222_deep_kernel_learning/,iori42,1447084898,,1,15
205,2015-11-10,2015,11,10,1,3s59pz,Visualization of decision tree leaf nodes in Random Forests,https://www.reddit.com/r/MachineLearning/comments/3s59pz/visualization_of_decision_tree_leaf_nodes_in/,senyat,1447084948,,0,9
206,2015-11-10,2015,11,10,1,3s5bx5,"TensorFlow - Googles latest machine learning system, open sourced for everyone",https://www.reddit.com/r/MachineLearning/comments/3s5bx5/tensorflow_googles_latest_machine_learning_system/,[deleted],1447085800,[deleted],0,1
207,2015-11-10,2015,11,10,2,3s5j8o,Google Just Open Sourced the Artificial Intelligence Engine at the Heart of Its Online Empire,https://www.reddit.com/r/MachineLearning/comments/3s5j8o/google_just_open_sourced_the_artificial/,cast42,1447088668,,0,2
208,2015-11-10,2015,11,10,2,3s5lta,"Using Empirical Bayes to approximate posteriors for large ""black box"" estimators",https://www.reddit.com/r/MachineLearning/comments/3s5lta/using_empirical_bayes_to_approximate_posteriors/,carmichael561,1447089659,,0,1
209,2015-11-10,2015,11,10,2,3s5nzq,"What does TensorFlow mean for Keras, Lasagne, Block, Nervana?",https://www.reddit.com/r/MachineLearning/comments/3s5nzq/what_does_tensorflow_mean_for_keras_lasagne_block/,solololol,1447090443,The title is self explanatory. I'm a noob and would like a perspective from the kind people on this site.,39,30
210,2015-11-10,2015,11,10,4,3s64b0,Unsupervised Learning (Specifically ART),https://www.reddit.com/r/MachineLearning/comments/3s64b0/unsupervised_learning_specifically_art/,g_wm,1447096613,"I understand the basics of supervised learning in neural networks in relation to classification, but can't get my head around how unsupervised learning works, specifically adaptive resonance theory. Can anyone link me to any good resources (preferably ones which are noob friendly) or briefly explain how it works?",0,1
211,2015-11-10,2015,11,10,4,3s65x8,"[TensorFlow] relu6 = min(max(features, 0), 6)",https://www.reddit.com/r/MachineLearning/comments/3s65x8/tensorflow_relu6_minmaxfeatures_0_6/,[deleted],1447097244,,8,1
212,2015-11-10,2015,11,10,4,3s691e,Big Data in the Cloud: Predictive Analytics Driving Efficiency,https://www.reddit.com/r/MachineLearning/comments/3s691e/big_data_in_the_cloud_predictive_analytics/,aaron_shugert,1447098427,,0,1
213,2015-11-10,2015,11,10,4,3s69n2,H2O.ai Raises $20M For Its Open Source Machine Learning Platform,https://www.reddit.com/r/MachineLearning/comments/3s69n2/h2oai_raises_20m_for_its_open_source_machine/,smoothjazzradiohits,1447098660,,5,3
214,2015-11-10,2015,11,10,5,3s6e5m,"Thanks for your feedback reddit!, here is the improved polyssifier (python module for running various classifiers) that now includes the brand new Voting classifier from sklearn.",https://www.reddit.com/r/MachineLearning/comments/3s6e5m/thanks_for_your_feedback_reddit_here_is_the/,aulloa,1447100373,,0,4
215,2015-11-10,2015,11,10,5,3s6fpe,Algorithm for learning with multiple Hierarchical labels?,https://www.reddit.com/r/MachineLearning/comments/3s6fpe/algorithm_for_learning_with_multiple_hierarchical/,napsternxg,1447100935,"I have some text data in which each instance has multiple labels and these labels are also part of a hierarchy where each label can belong to multiple branches. 

1. Each instance can have multiple labels
2. Each of the labels are part of a hierarchy. E.g. Labels are as follows: R.0.0, R.1.0, R.2.0, R.0.1, R.1.1, where R is the root node. 

Example row will be:
&lt;features&gt; &lt;L1, L2, L3&gt;

Where,
L1 = R.0.0 and R.1.0
L2 = R.1.1
L3 = R.2.3.1 and R.1.4.3

I can imagine that one way to train this model would be to use a softmax classifier for each possible label, however, in that case I will loose the information provided by the hierarchy of the labels. 

Using a softmax for all possible paths in the tree is also an option but then I loose information that 1 label can point to 2 different paths in the hierarchy (see Labels L1 and L3)

Is there any literature describing a way to train a neural network or any other machine learning algorithm to train a prediction model using this kind of data?",1,4
216,2015-11-10,2015,11,10,6,3s6ldu,[Lasagne] Simple LSTM failing due undefined input dimension,https://www.reddit.com/r/MachineLearning/comments/3s6ldu/lasagne_simple_lstm_failing_due_undefined_input/,lucas_0,1447103174,"I'm trying to build an LSTM model for classifying the ATIS dataset.

From a sentence of undefined size N, I generate a context window word embedding matrix. That's what I need to feed on my model, but I can't figure out how to make it so.

When I define my input layer as:

    def build_lstm(input_var=None):
        l_in = lasagne.layers.InputLayer(shape=(None, 1, None, None), input_var=input_var)
        l_hid = l_lstm = lasagne.layers.LSTMLayer(l_in, num_units=300)
        l_out = lasagne.layers.DenseLayer(l_hid, num_units=127, nonlinearity=lasagne.nonlinearities.softmax)
    return l_out

I get:

    TypeError: unsupported operand type(s) for *: 'NoneType' and 'NoneType'

While if I define the input shape in the l_in declaration it works, for example:

    l_in = lasagne.layers.InputLayer(shape=(None, 1, 30, 30), input_var=input_var)

The point is that each sentence has a different size, thus resulting in a context window word embedding matrix of different shape. What can I do?",2,0
217,2015-11-10,2015,11,10,6,3s6r25,7 Free Machine Learning Courses,https://www.reddit.com/r/MachineLearning/comments/3s6r25/7_free_machine_learning_courses/,vincentg64,1447105323,,0,1
218,2015-11-10,2015,11,10,6,3s6t6l,Data Mining/Machine Learning for advertising products to customers.,https://www.reddit.com/r/MachineLearning/comments/3s6t6l/data_miningmachine_learning_for_advertising/,Vespco,1447106108,"So, I am launching an online marketplace and I like the idea of using something that can look at all of the customers behavior and data and from that learn what other products to advertise to them - and get feedback of it's ad success rate so that it can learn from them and improve. 

Is this possible, or remotely practical to do this? Where would I even begin?
Seems like it would be hugely helpful. 

I could imagine various scenarios where if someone purchased agar + a pressure cooker, vs pectin + a pressure cooker, it means an awful lot from just those two data points. 
One is making sterile agar, and likely doing something with bacteria and fungi/mushroom growing - while the other is most likely just making a jam or jelly and is into food preservation. 

This would be helpful to know what to advertise to them next, petri dishes, or mason jar lids.  

With more customer data and ad feedback you could get even better at finding their niche interests and advertise relevant products. 

I'm no expert in any of this, so maybe it's easy or insanely difficult. I'd love to explore this option if it is possible. 

",4,0
219,2015-11-10,2015,11,10,7,3s6v24,[Question] How to detrend time series for clustering?,https://www.reddit.com/r/MachineLearning/comments/3s6v24/question_how_to_detrend_time_series_for_clustering/,rutherfordofman,1447106817,"I have a bunch (~300) different time series that I would like to cluster together based on some simple similarity measure e.g. Pearson coefficient. The problem is they all share a couple of large peaks which dominate the behaviour leading to uniformly high correlations (see figures [here](http://imgur.com/mgbL4aL) and [here](http://imgur.com/jEdv2cV)). I'm familiar with detrending a linear trend from time series, but I would like to know if there is some established way to 'remove' the affect of the large peaks.",5,1
220,2015-11-10,2015,11,10,8,3s7728,When is cloning in torch necessary and why?,https://www.reddit.com/r/MachineLearning/comments/3s7728/when_is_cloning_in_torch_necessary_and_why/,logrech,1447111569,"As of late, I've found myself needing a deeper understanding of how torch and nn, and so I've come to you guys. 

[Karpathy's char rnn](https://github.com/karpathy/char-rnn/blob/master/train.lua) has been the means with which I've picked up torch on a beginner level. 

As I understand it, the nn library does not at any point modify inputs sent to the forward function or the backward function. Those inputs are used to update values and gradients for various modules (values that are internal to the module), but at no point are the inputs themselves modified. 

As a question that I have, at these two places: [1](https://github.com/karpathy/char-rnn/blob/master/train.lua#L247), [2](https://github.com/karpathy/char-rnn/blob/master/train.lua#L272), a clone is performed. A clone allocates an entirely new space in memory. Why is this necessary?

So I understand why [3](https://github.com/karpathy/char-rnn/blob/master/train.lua#L192) is necessary. At each timestep, the model has different activation/gradient values and therefore you need a clone for each timestep. That makes sense. But are these clones sharing the same weights and parameters? How does torch then accumulate gradients across all the timesteps? 

But going back to the previous example, why can you not pass in the same h_init for the first call to forward in any pass of a batch? Is this necessary for cuda? 

I guess if someone could just give a run down of what I'm missing here, and how torch and nn and nngraph actually does what it does specifically in this context that would be awesome. ",2,3
221,2015-11-10,2015,11,10,9,3s7har,how much machine learning can you do on a desktop?,https://www.reddit.com/r/MachineLearning/comments/3s7har/how_much_machine_learning_can_you_do_on_a_desktop/,machinestupidity,1447115869,"

Ive got an 8 core 4.0Ghz system with 32 gig of ram and one enthusiast graphics card (4gb of onboard ram). How much could I do with this hardware? Could I drive a car? Could I run Deepdream real time? Could I create a virtual assistant? Could I pass the turing test? Could I run a simulation of a self balancing robot? How much exactly can you do with a high end desktop these days?",3,0
222,2015-11-10,2015,11,10,14,3s8i87,Jeff Dean explains TensorFlow [video],https://www.reddit.com/r/MachineLearning/comments/3s8i87/jeff_dean_explains_tensorflow_video/,TDaltonC,1447132119,,11,179
223,2015-11-10,2015,11,10,16,3s8ul0,POLYESTER SOLID SHEET PRODUCTION LINE_PET PANEL MAKING MACHINE_PET BOARD...,https://www.reddit.com/r/MachineLearning/comments/3s8ul0/polyester_solid_sheet_production_line_pet_panel/,cisscomachine,1447139003,,0,1
224,2015-11-10,2015,11,10,16,3s8vqq,Veles - deep learning platform from Samsung,https://www.reddit.com/r/MachineLearning/comments/3s8vqq/veles_deep_learning_platform_from_samsung/,outlacedev,1447139765,,13,28
225,2015-11-10,2015,11,10,16,3s8wm5,"Big science problems, big data solutions",https://www.reddit.com/r/MachineLearning/comments/3s8wm5/big_science_problems_big_data_solutions/,gradientflow,1447140356,,0,1
226,2015-11-10,2015,11,10,17,3s90iv,"TensorFlow and the Apache 2.0 License, what does it mean for startups contributing and using TensorFlow?",https://www.reddit.com/r/MachineLearning/comments/3s90iv/tensorflow_and_the_apache_20_license_what_does_it/,tensorflow,1447143200,What liabilities do startups and companies outside of Google have to worry about when using TensorFlow?,7,10
227,2015-11-10,2015,11,10,17,3s91ty,The Science Behind Money Making (Investment),https://www.reddit.com/r/MachineLearning/comments/3s91ty/the_science_behind_money_making_investment/,adeniyiks,1447144202,,0,1
228,2015-11-10,2015,11,10,17,3s92f9,"So, should I scrap theano, torch, caffe, and dive into TesorFlow?",https://www.reddit.com/r/MachineLearning/comments/3s92f9/so_should_i_scrap_theano_torch_caffe_and_dive/,polytop3,1447144686,"Hi All,

So as we are all aware, big news today with google's release of TensorFlow. I have been skimming their docs, and it looks brilliant!

It can be quite expensive to switch from one system to another; but going forward into the future, which do you think is the best learning investment: theano, torch, caffe, or TensorFlow?

Currently I am quite comfortable with theano, but TensorFlow looks very interesting. I don't want to use multiple systems for deep learning, so I am thinking about scrapping theano for TensorFlow, as going forward I think we can expect TensorFlow to overtake many of the alternatives (if it hasn't already upon initial release).

What are your thoughts?",16,18
229,2015-11-10,2015,11,10,19,3s9b9b,Best scalable cloud option for playing with TensorFlow?,https://www.reddit.com/r/MachineLearning/comments/3s9b9b/best_scalable_cloud_option_for_playing_with/,Tenoke,1447151673,"I want to start playing with TensorFlow, and have no idea what I'll really get to doing, so I am looking for something that is cost-effective and easily scalable based on how much resources I need that day/month.

As a web developer, I'll usually spin something on DigitalOcean or AWS, but it doesn't seem like they will be very good options for Machine Learning.",7,6
230,2015-11-10,2015,11,10,20,3s9g00,Automatic data visualization instrument?,https://www.reddit.com/r/MachineLearning/comments/3s9g00/automatic_data_visualization_instrument/,Fortyq,1447155369,"Hi, I wonder if there is any simple instrument for automatic data visualization. In python or standalone app. I need something like: upload csv, do aggregation and plotting by choosing columns, with scaling and some other perks.",0,3
231,2015-11-10,2015,11,10,21,3s9i9j,Microsoft Distributed Machine Learning Toolkit (DMTK),https://www.reddit.com/r/MachineLearning/comments/3s9i9j/microsoft_distributed_machine_learning_toolkit/,BillZheng,1447157011,,7,34
232,2015-11-10,2015,11,10,22,3s9pa8,Using RNN to turn variable-length series of vectors to a single vector where the order of the input vectors does not matter?,https://www.reddit.com/r/MachineLearning/comments/3s9pa8/using_rnn_to_turn_variablelength_series_of/,adagradlace,1447161538,"Recurrent NNs are used to process sequences of vectors and turn them into a single vector representation.

But can they also be used to turn a set of input vectors (like a bag-of-words, only that the words are vectors) into a single output vector, where the order of the set is not important? Can I somehow make the output single vector representation invariant to the input order?

If you happen to know literature references or experience on that I would be very grateful, as I am just starting to learn about (R)NNs",4,1
233,2015-11-10,2015,11,10,22,3s9qj8,[4fun] build a dataset for translating across programming languages with seq2seq,https://www.reddit.com/r/MachineLearning/comments/3s9qj8/4fun_build_a_dataset_for_translating_across/,pilooch,1447162211,"Now that there's a nice / simple to use seq2seq implementation available within the newly released TensorFlow (TF), I'm wondering what would be the best data source for translation across programming languages.

I would be very glad to team up with whoever is interested in building a proof of concept. I'm an experienced deep / machine learnist, just too much stuff on my plate workwise / lifewise to consider playing with this one alone. I see this as a potential good way to put TF to work and get a verdict from the practice side.",4,2
234,2015-11-11,2015,11,11,0,3sa3om,Why deep learning?,https://www.reddit.com/r/MachineLearning/comments/3sa3om/why_deep_learning/,thenerdstation,1447168350,So I'm taking a class in machine learning at my college and my professor showed us that you can implement any function to arbitrary precision with only a 3 layer neural network. If this is true (or atleast my understanding of it is true) then why was there such a big push recently about deep learning?,16,12
235,2015-11-11,2015,11,11,0,3sa5rv,A Wider Net is Cast for Deep Learning on GPUs,https://www.reddit.com/r/MachineLearning/comments/3sa5rv/a_wider_net_is_cast_for_deep_learning_on_gpus/,[deleted],1447169225,[deleted],0,7
236,2015-11-11,2015,11,11,0,3sa7gq,Facebook M  The Anti-Turing Test,https://www.reddit.com/r/MachineLearning/comments/3sa7gq/facebook_m_the_antituring_test/,Lyle_Cantor,1447169918,,33,114
237,2015-11-11,2015,11,11,1,3saakq,Using CDF (or ECDF) for feature scaling,https://www.reddit.com/r/MachineLearning/comments/3saakq/using_cdf_or_ecdf_for_feature_scaling/,justanotherta5,1447171214,"I want my normalized values to lie in the interval [0, 1], so I propose using the empirical CDF of the feature values to scale the raw values into the normalized ones. The problem is that [this turns an arbitrary distribution into a uniform one](http://math.stackexchange.com/questions/868400/showing-that-y-has-a-uniform-distribution-if-y-fx-where-f-is-the-cdf-of-x). Does that make this form of scaling invalid for general feature scaling?",7,5
238,2015-11-11,2015,11,11,1,3safll,Generating Images from Captions with Attention,https://www.reddit.com/r/MachineLearning/comments/3safll/generating_images_from_captions_with_attention/,emansim,1447173218,,7,16
239,2015-11-11,2015,11,11,1,3sago9,Is python good for ML ?,https://www.reddit.com/r/MachineLearning/comments/3sago9/is_python_good_for_ml/,GreenTea001,1447173643,"I am completely new to this, and i was seaching for some advice about in which language should I start (my inicial thought was to go ahead with python), and also if there are some kind of *start-from-zero* book about ML and IA. Thanks!",4,0
240,2015-11-11,2015,11,11,2,3sapkd,Learning to generate spectra based on two inputs,https://www.reddit.com/r/MachineLearning/comments/3sapkd/learning_to_generate_spectra_based_on_two_inputs/,Bigchrome,1447177089,"Apologies in advance, as I do not know how to adequately characterize this problem. I'm attempting to find and parameterize a spectrum that matches the average spectrum of ocean conditions at a site for any sea state, characterized by Hm0 (wave height) and T02 (wave period). Essentially I want to train a model using two input parameters and a spectral plot, and be able to accurately reproduce the spectral plot using these two input parameters. I understand what to do to train a model to produce a single point output using scikit learn, but how can I make the step of producing a full spectral plot? 

At the moment I simply iterate through variations of an (idealised) Bretschneider spectrum until I achieve the best fit with my real spectrum, then save the result. I'd like to either devise the parameters for this using a real machine learning approach, or produce a model that will fully generate the spectrum.

All input appreciated (and sorely needed)
",0,1
241,2015-11-11,2015,11,11,2,3sas8o,Semi-supervised Sequence Learning,https://www.reddit.com/r/MachineLearning/comments/3sas8o/semisupervised_sequence_learning/,arthomas73,1447178129,,1,6
242,2015-11-11,2015,11,11,3,3sb1y9,Gist of different lisences,https://www.reddit.com/r/MachineLearning/comments/3sb1y9/gist_of_different_lisences/,reddit_tl,1447181823,"I've seen a few mentions of lisence issues regarding different open source packages. I googled around and it's quite time consuming to get hold of the gist of each different one. 

Can someone explain in plain language?",2,0
243,2015-11-11,2015,11,11,4,3sb4t3,Yo! Live webinar Nov 19th on how data analysts are breaking the rules (with the assistance of some machine learning)...,https://www.reddit.com/r/MachineLearning/comments/3sb4t3/yo_live_webinar_nov_19th_on_how_data_analysts_are/,[deleted],1447182896,[deleted],0,0
244,2015-11-11,2015,11,11,4,3sb5zo,Anomaly detection benchmark from Numenta (NAB),https://www.reddit.com/r/MachineLearning/comments/3sb5zo/anomaly_detection_benchmark_from_numenta_nab/,tabacof,1447183345,"Numenta finally released an open-source benchmark for their anomaly detection software: https://github.com/numenta/NAB

They already compared NuPIC to competing algorithms from Twitter and Etsy, but we must take this with a grain of salt unless the other algorithms were tuned competitively.

The benchmark seems to consist of a lot of distinct univariate time-series. Each time-series has a few ""anomaly periods"". The learning must be unsupervised and online. The sooner the anomaly is detected, the better, and further detections on the same period are not penalized. False positives are heavily penalized. The full description of the evaluation criterion is given in this paper: http://arxiv.org/abs/1510.03336.

Is anyone here interested in trying to beat the benchmark with a different algorithm? I believe this to be a low-hanging fruit right now. The online and unsupervised learning requirements narrow the algorithmic choices, but I feel a Bayesian approach could be promising. Gaussian processes for time-series, perhaps?",1,0
245,2015-11-11,2015,11,11,5,3sbd6v,Python Density Based Clustering (DeBaCl) Toolbox,https://www.reddit.com/r/MachineLearning/comments/3sbd6v/python_density_based_clustering_debacl_toolbox/,lakando,1447186068,,1,1
246,2015-11-11,2015,11,11,5,3sbkyh,SharedTensor: An easy way to distribute your Torch training across many machines. (or AWS instances),https://www.reddit.com/r/MachineLearning/comments/3sbkyh/sharedtensor_an_easy_way_to_distribute_your_torch/,londons_explorer,1447188990,"See [SharedTensor](https://github.com/Hello1024/shared-tensor) here, added to [Karpathy's Char-RNN LSTM demo](https://github.com/Hello1024/char-rnn) here, which you can run on AWS [like this](https://github.com/Hello1024/char-rnn#running-on-multiple-machines).

Overall, I got an 8.4x speedup on my toy problem with 10 instances, although tuning of various parameters seems necessary, and I'm sure there are lots of improvements that could be made to the library to make it better.

The basic idea is you have a magical tensor which will sync across machines.   If you add some value to it on one machine, the change will very quickly propagate, approximately at first, then accurately, to other machines.   That makes these tensors great for storing parameters for data parallelism or any other intermediate values for model parallelism.

Reddit - what use can you make of this?",2,18
247,2015-11-11,2015,11,11,6,3sbmqe,Recorded stream of Tensor Flow installation and tutorial,https://www.reddit.com/r/MachineLearning/comments/3sbmqe/recorded_stream_of_tensor_flow_installation_and/,vanboxel,1447189663,,0,0
248,2015-11-11,2015,11,11,6,3sbsnq,LSTM vs IPredictiveRSDR duel: LSTM champion needed!,https://www.reddit.com/r/MachineLearning/comments/3sbsnq/lstm_vs_ipredictiversdr_duel_lstm_champion_needed/,CireNeikual,1447191967,"Hello everyone!

I wrote an algorithm that performs in a similar domain that is occupied by LSTMs, so I would like to compare the two. So, I started by trying PyBrain's LSTM implementation (since I do not have an Nvidia GPU), but got really poor performance. So, I would like someone to optimize LSTM competitively to my algorithm to see which one wins. We should test it on several datasets, and compare on an agreed error measure.

Anyone feeling up to it? I would greatly appreciate it!

About my algorithm: IPredictiveRSDR is sort of like HTM, but geared towards high-performance. It uses spiking neurons to solve for hierarchical spatio-temporal sparse codes which it then uses to predict.",21,5
249,2015-11-11,2015,11,11,6,3sbtg5,"NVIDIA Hyperscale accelerators with support for Mesos, Docker, RESTful GPU Engine, Deep Learning",https://www.reddit.com/r/MachineLearning/comments/3sbtg5/nvidia_hyperscale_accelerators_with_support_for/,harrism,1447192242,,0,1
250,2015-11-11,2015,11,11,7,3sbx1t,Approximation of Data (and why it matters for optimization),https://www.reddit.com/r/MachineLearning/comments/3sbx1t/approximation_of_data_and_why_it_matters_for/,Zephyr314,1447193660,,0,1
251,2015-11-11,2015,11,11,7,3sbzia,Push me in the right direction,https://www.reddit.com/r/MachineLearning/comments/3sbzia/push_me_in_the_right_direction/,julian88888888,1447194634,"
Sethbling's mario videos inspired me to want to learn machine learning, specifically genetic algorithms (and neural network?) for games:

* https://www.youtube.com/watch?v=qv6UVOQ0F44
* https://www.youtube.com/watch?v=S9Y_I9vY8Qw
* Here's his source code: http://pastebin.com/ZZmSNaHX

##I want to do that, but how?

My goal in the next few months is to do that with a super small game, like tic-tac-toe, and progressively try more complex games, like agar.io. I have a background in user experience and web design, but not programming


I tried Andrew Ng's ML coursera class, but after the first week [I felt really overwhelmed with the math](http://i.imgur.com/1N6CjXC.jpg) which was disheartening.


I'm currently 2 weeks into this other ML Coursera Class:
[Machine Learning Foundations: A Case Study Approach](https://www.coursera.org/learn/ml-foundations). It's still hard for me, but I haven't gotten blocked yet. So far I've done some simple feature modeling on a housing prices data set using Dato's graphLab and python.

Because I don't have a programming background, I've spent the last few months learning python on Codeacademy. I'm enrolled in the [Programming for Everybody (getting started with Python)](https://www.coursera.org/learn/python/home/welcome), but so far it seems a little basic. SethBling (the mario cart ML video guy), uses  Lua.

####Future projects/homework:
1. [Try n-armed bandit problem](http://outlace.com/Reinforcement-Learning-Part-1/)
2. Finish codecademy python courses (40% remaining)
3. Try [A Neural Network in 11 lines of Python](https://iamtrask.github.io/2015/07/12/basic-python-network/)

4. [Read Reinforcement Learning by Richard S. Sutton and Andrew G. Barto](http://outlace.com/Reinforcement-Learning-Part-1/)
seems a bit over my head, I'm wondering how beneficial this actually is

5. Tic-tac-toe with a genetic algorithm.

##What are your thoughts of what I'm doing, bad, good, and what do you recommend I do instead?",4,0
252,2015-11-11,2015,11,11,8,3sc5ar,The Big List of Deep Learning Toolkits,https://www.reddit.com/r/MachineLearning/comments/3sc5ar/the_big_list_of_deep_learning_toolkits/,kcimc,1447196967,,4,17
253,2015-11-11,2015,11,11,8,3sc5bz,What is the best language for deep learning?,https://www.reddit.com/r/MachineLearning/comments/3sc5bz/what_is_the_best_language_for_deep_learning/,geekybarnstar,1447196977,"Hi, so I have worked in neural networks before in Matlab and I have decent proficiency in Python, R and Matlab; R being my favorite. For sometime, I have been interested in trying deep learning techniques on some of my existing datasets. But not sure which language has the best support in terms of libraries etc.

Could you suggest which one would be best? And which libraries I should look at? If possible, I would like to avoid Matlab as I am trying to reduce dependence on paid software.

P.S. I have put a question here: http://stats.stackexchange.com/questions/181177/how-can-i-predict-using-deep-learning",9,0
254,2015-11-11,2015,11,11,8,3sc8p9,TensorFlow Examples,https://www.reddit.com/r/MachineLearning/comments/3sc8p9/tensorflow_examples/,nlintz,1447198360,"Hi r/machinelearning, I made some simple TensorFlow examples which show how to write logistic regression, DNNs, CNNs and more. Check it out here https://github.com/nlintz/TensorFlow-Tutorials",25,260
255,2015-11-11,2015,11,11,9,3sck14,"NVIDIA launches Jetson TX1 system-on-a-module for robots, drones, other low-power autonomous PCs",https://www.reddit.com/r/MachineLearning/comments/3sck14/nvidia_launches_jetson_tx1_systemonamodule_for/,cudoer,1447203133,,0,1
256,2015-11-11,2015,11,11,11,3scu5l,What to (junior) machine learning scientists at companies in the SF Bay area earn?,https://www.reddit.com/r/MachineLearning/comments/3scu5l/what_to_junior_machine_learning_scientists_at/,short_of_good_length,1447207829,"Not sure if this is the right forum for questions like this. I am a postdoc in ML from a US university who is on the job market, and I have an offer from a tech company in the SF bay area. I do not have other offers as of now, but I want to know if the offer I have in hand is reasonable. 

What do new hires in ML research earn typically? Base + bonus etc?",2,0
257,2015-11-11,2015,11,11,11,3scvqb,Training a CNN with 2000 classes,https://www.reddit.com/r/MachineLearning/comments/3scvqb/training_a_cnn_with_2000_classes/,mikos,1447208595,"I need to classify images into one of 2000 classes? I am using the Nvidia DIGITS + caffe and provided 10K samples per class (so a whopping 20 million images (~1Tb data!). But the data prep (""create db"") task is being estimated to be 102 days and I shudder to think what the actual training time will be if that estimate is correct.

What is the best way to approach this challenge? should I break up dataset into 3-4 models? and use them separately? Use a smaller dataset and risk less accuracy? something else?

Thanks for helping out a newbie.",6,6
258,2015-11-11,2015,11,11,11,3scxcr,Using ML to make sense of a large IT environment,https://www.reddit.com/r/MachineLearning/comments/3scxcr/using_ml_to_make_sense_of_a_large_it_environment/,gingersaurusmaximus,1447209351,"Hey folks, I'm a complete ML neophyte, but have been interested in the idea of learning it for some time.  In practice I've found that it's easiest to learn things in the context of another objective, and I've recently found a possible candidate.

I recently joined a firm with a fairly large number of hosts (~30k) running various flavors of Linux (mostly Ubuntu, some CentOS).  I'd like to get some of the configuration under control, and am wondering if it would make sense to use ML against the corpus of configuration data to identify patterns that could become standardized templates.  This could include config files in /etc, running processes, mounted filesystems, firewall rules, etc.

Does this make sense?  Any recommendations where I should start?  I've been tinkering with Weka a bit, primarily because it has lots of examples and the built in charting might help me understand what i'm looking at.

Thanks

",1,1
259,2015-11-11,2015,11,11,11,3sczew,How does the Bengio's NNLM compare with the Mikolov CBOW?,https://www.reddit.com/r/MachineLearning/comments/3sczew/how_does_the_bengios_nnlm_compare_with_the/,koormoosh,1447210328,"I find it not clear how the two works are compared with each other. Specially if we only assume a classic feed forward NN, and assume softmax output layer, what is the difference between CBOW of Mikolov and Bengio's early work? Is it just the way the projection layer is created? In Mikolov, the project layer is an average, while in Bengio's work it's the concatenation of input words vector?",2,3
260,2015-11-11,2015,11,11,12,3sd2kv,"Shape of Data: Neural networks, linear transformations and word embeddings",https://www.reddit.com/r/MachineLearning/comments/3sd2kv/shape_of_data_neural_networks_linear/,jejomath,1447211843,,0,3
261,2015-11-11,2015,11,11,12,3sd666,The Data Science Industry: Who Does What (Infographic),https://www.reddit.com/r/MachineLearning/comments/3sd666/the_data_science_industry_who_does_what/,DataPoints,1447213593,,0,2
262,2015-11-11,2015,11,11,12,3sd6if,Looking to upgrade my Laptop - what could I do with my old one to support ML Tasks?,https://www.reddit.com/r/MachineLearning/comments/3sd6if/looking_to_upgrade_my_laptop_what_could_i_do_with/,gary_feesher,1447213777,"I am looking to upgrade my current laptop (Intel(R) Core(TM) i3 CPU, with 4GB RAM), but I still want to utilize it in some manner to support machine learning tasks.

What would you all reccommend that I do with my old laptop?  Should I wipe it clean and run Linux on it?  Should I turn it into a server?  I'd be looking for something that will serve as another node in machine learning processes.

Any resources or tutorials on how to retrofit an old laptop into a ML machine would be great!
",9,0
263,2015-11-11,2015,11,11,13,3sdaj1,Some of useful machine learning resources from beginner to intermediate.,https://www.reddit.com/r/MachineLearning/comments/3sdaj1/some_of_useful_machine_learning_resources_from/,Dawny33,1447215798,,0,20
264,2015-11-11,2015,11,11,13,3sdbz1,What do companies need from sentiment analysis/opinion mining?,https://www.reddit.com/r/MachineLearning/comments/3sdbz1/what_do_companies_need_from_sentiment/,AlOrozco53,1447216531,"Hi everyone!

Recently some friends and I were discussing about the real needs that the enterprise world require from machine learning. But when it comes to sentiment analysis, we were wondering about what most companies need to know about their customers' (and potential customers') feelings towards their products/services.

Could you guys help us with some ideas from your experience on this issue? Is there anything not yet tackled by any sentiment analysis app that you consider important for a company?

Thanks a lot in advance!!",0,1
265,2015-11-11,2015,11,11,14,3sdgzw,"[ELI5] Reasoning, attention and memory in deep networks",https://www.reddit.com/r/MachineLearning/comments/3sdgzw/eli5_reasoning_attention_and_memory_in_deep/,spidey-fan,1447219284,[removed],0,1
266,2015-11-11,2015,11,11,14,3sdh5j,Marvin : A minimal hackable Deep Learning framework from Princeton University,https://www.reddit.com/r/MachineLearning/comments/3sdh5j/marvin_a_minimal_hackable_deep_learning_framework/,muktabh,1447219377,,0,5
267,2015-11-11,2015,11,11,14,3sdk3g,TensorFlow and ImageNet,https://www.reddit.com/r/MachineLearning/comments/3sdk3g/tensorflow_and_imagenet/,JogglingDroid,1447221178,"Did any of you manage to import the ImageNet pretrained models (these ones: https://github.com/BVLC/caffe/wiki/Model-Zoo) in TensorFlow? Or: is anyone trying?

Given how well the whole library is presented, it would be GREAT to have such a possibility. ",2,4
268,2015-11-11,2015,11,11,15,3sdkxa,Deep Learning in a Single File for Smart Devices,https://www.reddit.com/r/MachineLearning/comments/3sdkxa/deep_learning_in_a_single_file_for_smart_devices/,antinucleon,1447221718,,0,12
269,2015-11-11,2015,11,11,15,3sdn8a,Ordering tags for a paragraph.,https://www.reddit.com/r/MachineLearning/comments/3sdn8a/ordering_tags_for_a_paragraph/,gomerry,1447223134,"I have some paragraphs and list of tags. now Each paragraph is associated with one or more tags, and in case of more than one tag they have been rank ordered manually by some priority. Now I am supposed to come up with a ML algorithm which can predict order of tags for a paragraph(I have a classifier already which can predict if a tag is associated with a paragraph of not). Can it be done using ML and if yes how to formulate this problem as classification or something else. any pointers in this direction ?",0,1
270,2015-11-11,2015,11,11,17,3sdvgj,Which SQL course do I do?,https://www.reddit.com/r/MachineLearning/comments/3sdvgj/which_sql_course_do_i_do/,Chemicalcharlie,1447229379,"I'm an honours grad in statistics. I've 3 years experience with R and I've got a knack for programming (I think), but no experience with C++ or SQL. These languages seem to be pretty important for jobs. I've heard about SQL courses with Oracle, can you suggest one? Some general advice would also be useful. Thanks! ",3,2
271,2015-11-11,2015,11,11,17,3sdwgo,Am I the only one intimidated by TensorFlow's sheer size?,https://www.reddit.com/r/MachineLearning/comments/3sdwgo/am_i_the_only_one_intimidated_by_tensorflows/,[deleted],1447230278,[deleted],0,1
272,2015-11-11,2015,11,11,17,3sdxbs,Do you know Marvin? (Deep Learning in N Dimensions),https://www.reddit.com/r/MachineLearning/comments/3sdxbs/do_you_know_marvin_deep_learning_in_n_dimensions/,[deleted],1447230931,[deleted],0,1
273,2015-11-11,2015,11,11,17,3sdy6i,Soumith's benchmarks TensorFlow's CNN performance. Disappointing for now at least.,https://www.reddit.com/r/MachineLearning/comments/3sdy6i/soumiths_benchmarks_tensorflows_cnn_performance/,[deleted],1447231580,[deleted],0,1
274,2015-11-11,2015,11,11,17,3sdy90,"Soumith benchmarks TensorFlow's CNN performance. For now at least, it's disappointing.",https://www.reddit.com/r/MachineLearning/comments/3sdy90/soumith_benchmarks_tensorflows_cnn_performance/,bluecoffee,1447231640,,28,102
275,2015-11-11,2015,11,11,19,3se3to,Kind of a bit lost in understanding LSTM,https://www.reddit.com/r/MachineLearning/comments/3se3to/kind_of_a_bit_lost_in_understanding_lstm/,tunoat,1447236726,"Hi, i'm quite new to machine learning so this question might look a little bit dump but it would be good if somebody can answer me in a more plain detail with an example.

so what i knew so far is that RNN is the one suffer most from Vanishing Gradient/Exploding Gradient due to their BPTT because the linear transition when you integral back.

LSTM is the most popular way to deal with this problem by adding Cell, Forget Gate, Input Gate and Output Gate

To the best of my knowledge, cell is a storage of errors which normally in FNN it can be computed when the inputs of system have pass through the network til you see the outputs of it and comparing it with target outputs.
The cell keep the error not to change until Forget Gate is set to zero which will be exactly the same time that Input Gate is set to one and writing something down.

What confusing me the most is that... the equation below:

i_t = sigmoid(W_i*[h_t-1,x_t] + b_i)

C_hat_t = tanh(W_c*[h_t-1,x_t] + b_c)


C_t = f_t * C_t-1 + i_t * C_hat_t


Why the write gate wrote to the cell directly instead of comparing it with the target output first to see the errors?",6,1
276,2015-11-11,2015,11,11,20,3se7uj,KNIME and R Integration with Unbalanced Classes in Test and Train Partitions,https://www.reddit.com/r/MachineLearning/comments/3se7uj/knime_and_r_integration_with_unbalanced_classes/,InformationEntropy,1447240470,,0,1
277,2015-11-11,2015,11,11,21,3sebf0,Udacity Machine Learning Engineer Nanodegree,https://www.reddit.com/r/MachineLearning/comments/3sebf0/udacity_machine_learning_engineer_nanodegree/,bradam,1447243411,,26,20
278,2015-11-11,2015,11,11,21,3sedta,What you need to know about Google's Open Source AI Platform - TensorFlow,https://www.reddit.com/r/MachineLearning/comments/3sedta/what_you_need_to_know_about_googles_open_source/,Questechie,1447245248,,1,1
279,2015-11-12,2015,11,12,0,3sewxy,[Help] Regularized Linear Regression,https://www.reddit.com/r/MachineLearning/comments/3sewxy/help_regularized_linear_regression/,Kiuhnm,1447255649,"Hi everyone,
I'm reading [ml](http://goodfeli.github.io/dlbook/contents/ml.html) and at the bottom of page 128 it says
&gt; Comparing Eqs. 5.32 and 5.33, we see that the MAP estimate amounts to a weighted combination of the prior maximum probability value, \mu_0, and the ML estimate.

Well, I don't see it! Any help?

P.S. If this is not the right forum to ask these questions, please recommend another forum to me. I know about /r/MLQuestions but it has too little traffic.",4,1
280,2015-11-12,2015,11,12,0,3sexmi,Transfering Hidden States in Sequence To Sequence Learning,https://www.reddit.com/r/MachineLearning/comments/3sexmi/transfering_hidden_states_in_sequence_to_sequence/,LeavesBreathe,1447255955,"Hey Guys,

Lately, I've been trying to predict the next sentence given the past two sentences. This is done by sequence to sequence learning as Google did [here](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf).

So far, I've had a pretty basic model with this set up in Keras:

    model = Sequential()    
    model.add(LSTM(hidden_variables_encoding, return_sequences = True)) #LSTM layer 1
    model.add(LSTM(hidden_variables_encoding, return_sequences = False)) #LSTM layer 2
    model.add(Dropout(dropout))
    model.add(Dense(hidden_variables_encoding))
    model.add(Activation('relu'))
    model.add(RepeatVector(y_sent_len))
    model.add(LSTM(hidden_variables_decoding, return_sequences = True)) #LSTM layer 3
    model.add(LSTM(hidden_variables_decoding, return_sequences = True)) #LSTM layer 4
    model.add(Dropout(dropout))
    model.add(TimeDistributedDense(y_matrix_axis, activation = 'softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam') 

However, I'm now looking into *transfering hidden states* in between LSTM layers. This would be such that LSTM layer 1 transfer its hidden state to LSTM layer 2, layer 2 transfer to layer 3, and layer 3 transfer to layer 4. 

My question in all of this is: **Why does transferring hidden states help the neural network learn?** Why is it advantageous to transfer hidden states as opposed to just initializing each LSTM layer normally? 

In addition, I have another question. What is the difference between:

    model.add(LSTM(512)) #LSTM with 512 memory cells
    model.add(LSTM(512)) #LSTM with 512 memory cells

and

    model.add(LSTM(1024)) #single LSTM with 1024 cells

Which one performs better for better tasks and more importantly: why? I apologize if these are basic questions, but I'm just having a hard time understanding these concepts. Thanks!",17,2
281,2015-11-12,2015,11,12,2,3sfcbu,Can't understand shape(output) = (shape(value) - ksize + 1) / strides in TensorFlow docs,https://www.reddit.com/r/MachineLearning/comments/3sfcbu/cant_understand_shapeoutput_shapevalue_ksize_1/,cesarsalgado,1447262078,"In the following extract from http://tensorflow.org/api_docs/python/nn.md#pooling

""shape(output) = (shape(value) - ksize + 1) / strides

where the rounding direction depends on padding:

    padding = 'SAME': Round down (only full size windows are considered).
    padding = 'VALID': Round up (partial windows are included).""

I can't understand the formula above. I am familiar with following formula: shape(out) = (shape(value) - ksize + 2*pad)/strides+1.

Are the two equivalent?

For example, let's say shape(value) = 9, ksize = 3, strides = 2, and padding = 'SAME'.

In the first formula the shape(output) will be (9-3+1)/2 = 7/2 = 3.5 and rounding down results in 3.

In the second formula the shape(output) will be (9-3+2*1)/2 + 1 = 5

It doesn't seem to be the same formula. Even if I round up the first one, the result will be 4.

In addition to that, the padding definition seems to be inverted. Isn't the 'SAME' padding that includes partial windows?

",1,2
282,2015-11-12,2015,11,12,2,3sff10,Just-add-water AI,https://www.reddit.com/r/MachineLearning/comments/3sff10/justaddwater_ai/,[deleted],1447263194,[deleted],7,0
283,2015-11-12,2015,11,12,3,3sfo8c,TensorFlow on AWS,https://www.reddit.com/r/MachineLearning/comments/3sfo8c/tensorflow_on_aws/,edmarferreira,1447266851,[removed],0,1
284,2015-11-12,2015,11,12,4,3sfvca,Beginners Guide: Apache Spark Python  Machine Learning Scenario With A Large Input Dataset,https://www.reddit.com/r/MachineLearning/comments/3sfvca/beginners_guide_apache_spark_python_machine/,dmpetrov,1447269676,,0,7
285,2015-11-12,2015,11,12,4,3sg0fx,Petuum - A distributed ML framework (from CMU),https://www.reddit.com/r/MachineLearning/comments/3sg0fx/petuum_a_distributed_ml_framework_from_cmu/,vikkamath,1447271645,,1,2
286,2015-11-12,2015,11,12,5,3sg40o,Convolutional Autoencoder: why do filters continue to improve after predictions have settled?,https://www.reddit.com/r/MachineLearning/comments/3sg40o/convolutional_autoencoder_why_do_filters_continue/,LyExpo,1447273059,"I have noticed that during training a very simple convolutional autoencoder on mnist that the predictions become ""good"" very quickly, say within the first epoch. At this early stage, filters look mostly like noise. 

Allowing the training to continue, I see that the filters slowly look more and more ""nice"", but the predictions don't seem to change much, at least to the human eye.

Can anyone explain this behavior?",23,6
287,2015-11-12,2015,11,12,5,3sg6gj,Guide for figuring out receptive field in convnet?,https://www.reddit.com/r/MachineLearning/comments/3sg6gj/guide_for_figuring_out_receptive_field_in_convnet/,zmjjmz,1447273984,"I've been trying to figure out how to calculate the receptive field size for a unit at a given depth for a convnet, assuming there's nothing more complicated than typcial convulotions and pooling layers (and 'same' padding). 

For a stride of 1 it seems like you should just add the filter size - 1 at each layer, but I don't think this generalizes to other strides...

Does anyone know of a good way to calculate this? ",4,1
288,2015-11-12,2015,11,12,5,3sg706,Two+ Minute Papers - How Does Deep Learning Work?,https://www.reddit.com/r/MachineLearning/comments/3sg706/two_minute_papers_how_does_deep_learning_work/,matiskay,1447274194,,4,28
289,2015-11-12,2015,11,12,6,3sgeq3,kixAzzML Machine-Learning tool (by Eugenio Culurciello of Teradeep),https://www.reddit.com/r/MachineLearning/comments/3sgeq3/kixazzml_machinelearning_tool_by_eugenio/,bboyjkang,1447277257,,2,1
290,2015-11-12,2015,11,12,6,3sggk7,NVIDIA Jetson TX1 Supercomputer-on-Module Drives Next Wave of Autonomous Machines,https://www.reddit.com/r/MachineLearning/comments/3sggk7/nvidia_jetson_tx1_supercomputeronmodule_drives/,harrism,1447277975,,14,65
291,2015-11-12,2015,11,12,7,3sgpvz,CNN/LSTM Image Captioning frameworks with GPU support?,https://www.reddit.com/r/MachineLearning/comments/3sgpvz/cnnlstm_image_captioning_frameworks_with_gpu/,osiris679,1447281740,"I've been using Neuraltalk https://github.com/karpathy/neuraltalk for image caption training on new images but this platform doesn't utilize GPUs (outside of the caffe image feature extraction). 

The only framework I've found that supports CNN/LSTM with GPU is neon: http://neon.nervanasys.com/docs/latest/_modules/neon/data/imagecaption.html#ImageCaption

I would prefer a Torch or Theano implementation with GPU support. Does it exist or anything similar? Or has anyone had success with the Neon ImageCaption Module? (there isn't much documentation about this feature). ",3,2
292,2015-11-12,2015,11,12,8,3sgv5j,"TensorFlow App Idea Exchange: Have an idea, want to collaborate? Share here.",https://www.reddit.com/r/MachineLearning/comments/3sgv5j/tensorflow_app_idea_exchange_have_an_idea_want_to/,PoliticizeThis,1447284053,"Hey guys,

In the spirit of this week's big TensorFlow news I thought I'd open this tread for ML enthusiasts to share their application ideas and, who knows, find others that want to help make them a reality. I'll start off with an example(not saying it's good, but it's something well suited for ML):

Input audio from loved one speaking. This trains a neural net to produce new sentences using the voice of the person they were trained on. Can be used for reading text. I.e. A mom used to read to her kid every night but she's dying of cancer. She records her voice while reading a variety of stories where her emotion is annotated(I.e. Surprise) for the neural network. On new stories the text is provided with *needed* emotion annotated.",10,2
293,2015-11-12,2015,11,12,9,3sh64a,should I choose statistics or electrical engineering form my second degree?,https://www.reddit.com/r/MachineLearning/comments/3sh64a/should_i_choose_statistics_or_electrical/,tristochief,1447288518,"I am going to uni next year and I am literally pulling my hair out on trying to decide what degrees to choose to get into AI.

I specifically love robots and autonomous systems, and designing advanced AI that will control these robots.

I am interested in both hardware and software but much more software than hardware.

In Auz, the only mathematics degrees around is called a 'mathematics and statistics' I am guessing that this means that the degree encompasses statistics and maths in general. If the degree isn't purely statistics, will it at all help me in my computer science degree? Computer science degree already has maths courses, am I being silly stacking up on extra maths I probably wont need?

On the other hand will electrical engineering, major in computer engineering, help me with artificial intelligence on robots? I have hard that too many computer science graduates dont know enough about the hardware these days, and take the hardware for granted, and therefore produce bad machine learning apps. Is this true? Will an electrical engineering help me at all in computer science?

Thanks",4,4
294,2015-11-12,2015,11,12,10,3sh9xh,"Tensorflow, now working on AWS GPU's",https://www.reddit.com/r/MachineLearning/comments/3sh9xh/tensorflow_now_working_on_aws_gpus/,londons_explorer,1447290273,,10,60
295,2015-11-12,2015,11,12,10,3shdjc,Jeremy Howard - Big Data &amp; Machine Learning (He talks a bit about TensorFlow),https://www.reddit.com/r/MachineLearning/comments/3shdjc/jeremy_howard_big_data_machine_learning_he_talks/,[deleted],1447292009,[deleted],0,5
296,2015-11-12,2015,11,12,10,3shflv,Tensorflows poor performance means they don't use it like we do internally.,https://www.reddit.com/r/MachineLearning/comments/3shflv/tensorflows_poor_performance_means_they_dont_use/,ml_lover,1447292953,"[This](https://www.reddit.com/r/MachineLearning/comments/3sdy90/soumith_benchmarks_tensorflows_cnn_performance/) thread shows TensorFlows poor performance on GPU's.

Since Google presumably cares a *lot* about performance, and I can't believe they wouldn't notice most opensource tools outperforming their baby, that must mean one of:

1. Google has so many spare GPU's that getting only half the performance out of each is fine.
1. Google doesn't use TensorFlow internally.
1. Google uses AMD GPU's or some other GPU's/FPGA's.
1. Google has a separate fork of the TensorFlow code which performs well and never benchmarked the fork they open-sourced.


To me, they all are pretty significant.",16,25
297,2015-11-12,2015,11,12,13,3shypb,Easy text analysis API - NLP/Natural language processing documentation,https://www.reddit.com/r/MachineLearning/comments/3shypb/easy_text_analysis_api_nlpnatural_language/,thatneedle,1447302548,,0,0
298,2015-11-12,2015,11,12,13,3shzuk,Suggestions for papers on identifying topic trends over time in document corpora,https://www.reddit.com/r/MachineLearning/comments/3shzuk/suggestions_for_papers_on_identifying_topic/,pappypapaya,1447303172,"Has there been much work on finding 'high-level trends' (for a lack of a better phrase) in time-stamped corpus of documents? Please excuse my vagueness in defining the problem. My guess is there might be some of this kind of work in applications of machine learning to the digital humanities, but perhaps there's also other areas that I haven't thought of. I'm curious as to where to begin in the literature.",1,1
299,2015-11-12,2015,11,12,14,3si4uc,Why are NLP and Machine Learning communities interested in deep learning?,https://www.reddit.com/r/MachineLearning/comments/3si4uc/why_are_nlp_and_machine_learning_communities/,Dawny33,1447306116,,0,0
300,2015-11-12,2015,11,12,14,3si5yp,[Help] Need Snap twitter data set for college project,https://www.reddit.com/r/MachineLearning/comments/3si5yp/help_need_snap_twitter_data_set_for_college/,arbabu123,1447306772,I was looking at https://snap.stanford.edu/data/twitter7.html for getting a sufficiently large twitter dataset. But it seems due to twitter policy changes it has been removed. Could someone share the data or point to someone who can help? Thanks!,2,1
301,2015-11-12,2015,11,12,17,3sihru,TensorFlow Tutorials and Examples for beginners,https://www.reddit.com/r/MachineLearning/comments/3sihru/tensorflow_tutorials_and_examples_for_beginners/,mela1029,1447315619,,3,16
302,2015-11-12,2015,11,12,18,3simds,MY CA CD T NG,https://www.reddit.com/r/MachineLearning/comments/3simds/my_ca_cd_t_ng/,hoa-posts,1447319617,,1,0
303,2015-11-12,2015,11,12,18,3simhx,"Sit Back, and enjoy the ride sir",https://www.reddit.com/r/MachineLearning/comments/3simhx/sit_back_and_enjoy_the_ride_sir/,sethibharat,1447319739,,0,0
304,2015-11-12,2015,11,12,18,3sin7y,Getting into Machine Learning Summer School,https://www.reddit.com/r/MachineLearning/comments/3sin7y/getting_into_machine_learning_summer_school/,PMMEYOURVCDIMENSION,1447320433,"Hello fellow redditors

Have anyone here participated in [machine learning summer schools](http://mlss.cc/)? How competitive is the selection? What can I do to increase my chances of acceptance?",1,5
305,2015-11-12,2015,11,12,20,3sitok,Inference: The Next Step in GPU-Accelerated Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3sitok/inference_the_next_step_in_gpuaccelerated_deep/,harrism,1447326338,,0,1
306,2015-11-12,2015,11,12,20,3siwdx,What other Machine Learning can or could be done with TensorFlow?,https://www.reddit.com/r/MachineLearning/comments/3siwdx/what_other_machine_learning_can_or_could_be_done/,boxstabber,1447328596,Obviously it's used for all sorts of Neural Networks. But they hinted that it is supposed to be a general ML library. Could it do Bayesian Inference? Could it replace scikit-learn?,3,1
307,2015-11-12,2015,11,12,21,3sj0j5,Book on ML,https://www.reddit.com/r/MachineLearning/comments/3sj0j5/book_on_ml/,Kiuhnm,1447331767,"Everyone recommends the books by Bishop and Murphy, but what about [Machine Learning: A Bayesian and Optimization Perspective](http://www.amazon.com/Machine-Learning-Optimization-Perspective-Developers/dp/0128015225/) by Theodoridis?

Has anyone read this book?",8,2
308,2015-11-12,2015,11,12,22,3sj4rc,"After Google's TensorFlow, now Microsoft has released a Distributed Machine Learning Toolkit",https://www.reddit.com/r/MachineLearning/comments/3sj4rc/after_googles_tensorflow_now_microsoft_has/,Elendar42,1447334526,,16,136
309,2015-11-12,2015,11,12,23,3sjbvg,10 Must Watch Movies on Machine Learning and Data Science,https://www.reddit.com/r/MachineLearning/comments/3sjbvg/10_must_watch_movies_on_machine_learning_and_data/,john_philip,1447338554,,1,0
310,2015-11-13,2015,11,13,0,3sjl7o,TensorFlow vs. Theano vs. Torch comparison,https://www.reddit.com/r/MachineLearning/comments/3sjl7o/tensorflow_vs_theano_vs_torch_comparison/,cast42,1447342950,,25,55
311,2015-11-13,2015,11,13,1,3sjnvn,"""This open source release supports single machines and mobile devices."" and the danger of pseudo-opensource releases.",https://www.reddit.com/r/MachineLearning/comments/3sjnvn/this_open_source_release_supports_single_machines/,ragipy,1447344103,"I appreciate the release and all the work they put in. Kudos for the effort.  However, the disclaimers on what they are open sourcing is very worrying and with the performance benchmarks coming in I think this worry has some justification. 

A few words about our experience. A university lab. We have been developing our models on torch until now, and we are on the language understanding domain. With LSTMs and other weird architectures that work in practice but have no sound theorethical justification, we have to try different architectures all the time. Then a flow graph makes a lot of sense, so we have been getting by with nngraph etc. For us, TensorFlow is a godsend. 

Google's power, all the know-how. I was so excited. 

What is to do though? Switch to a pseudo-commitment which becomes irrelevant when you go beyond the capacity of a single machine? Deep learning only works at scale though, no? What do I do if my model doesn't fit 4-GPUs. Email it to Google so they can run it in their real infrastructure/non open source software?

Overall I am worried. TensorFlow sounds like a plan by a company (a great one, you guys rock), to increase the pool of talented engineers that understand their software to ease their hiring. 

I am fine with that but we should demand more in return.  













",13,2
312,2015-11-13,2015,11,13,1,3sjqdg,MCMC sampling with pymc3,https://www.reddit.com/r/MachineLearning/comments/3sjqdg/mcmc_sampling_with_pymc3/,gwulfs,1447345173,,0,12
313,2015-11-13,2015,11,13,2,3sjwgt,AskReddit: How do I implement a learning algorithm (e.g. AdaDelta) in TensorFlow?,https://www.reddit.com/r/MachineLearning/comments/3sjwgt/askreddit_how_do_i_implement_a_learning_algorithm/,AlfonzoKaizerKok,1447347674,"Hi Reddit. I really need help trying to implement a custom learning algorithm in TF.

I've had a look at the [optimizer class](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/optimizer.py) and its [adagrad subclass](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/adagrad.py), but I honestly just get lost in the source code. It seems like I need to define an _apply_dense and an _apply_sparse method, but I am lost in the imports, and cannot find training_ops.apply_adagrad and training_ops.sparse_apply_adagrad.

An alternative would be to do tf.assign on a variable and its update to make it more Theano-like. I wonder if this is the best option though - I feel like I should be sub-classing from the optimizer class.

I would really appreciate any advice/tips/guidance I can get! Thank you!",6,3
314,2015-11-13,2015,11,13,2,3sjxxy,What kind of naive Bayes classifier should I use for ternary features?,https://www.reddit.com/r/MachineLearning/comments/3sjxxy/what_kind_of_naive_bayes_classifier_should_i_use/,Rangi42,1447348289,"I'm supposed to implement a naive Bayes classifier and train it on a certain set of data. This data set uses ternary features: for each feature vector **X**, *x_i* can be 0, 1, or 2.

This is not continuous data, so Gaussian NB is not appropriate. But multinomial and Bernoulli NB deal with binary features. I could convert each ternary feature *x_i* into two binary ones *x_i1* and *x_i2*, but they would obviously be dependent (they'd both be false if *x_i* was 0, but they can't both be true).

What formula should I be using for P(*x_i* | class)? (In case it's relevant, the classes are 0-9, and the features are pixels in digit images which can be black, gray, or white.)

**Update:** Okay, I think I would assume a [categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution) (""generalized Bernoulli"", or ""multinoulli"") with 3 discrete values.",2,2
315,2015-11-13,2015,11,13,2,3sk15m,The Human Kernel,https://www.reddit.com/r/MachineLearning/comments/3sk15m/the_human_kernel/,compsens,1447349637,,2,9
316,2015-11-13,2015,11,13,3,3sk9jj,Tensorflow tutorials converted to jupyter notebooks!,https://www.reddit.com/r/MachineLearning/comments/3sk9jj/tensorflow_tutorials_converted_to_jupyter/,suki907,1447353096,"I just ran all the official tensorflow tutorial `.md` files through ""[notedown](https://github.com/aaren/notedown)"", to produce [jupyter notebooks](http://nbviewer.ipython.org/github/MarkDaoust/tensorflow/blob/notebooks/tensorflow/g3doc/tutorials/index.ipynb).

The conversion isn't perfect, but quite usable.

The notebooks link to eachother properly; I wrote a [quick script](https://github.com/MarkDaoust/tensorflow/blob/notebooks/tensorflow/g3doc/md2ipynb.py) that does the `os.walk`, and replaces links to `.md` files with links to `.ipynb` files.

`python md2ipynb.py tutorials`",1,8
317,2015-11-13,2015,11,13,3,3skd69,Regression MLP in Lasagne,https://www.reddit.com/r/MachineLearning/comments/3skd69/regression_mlp_in_lasagne/,Muirbequ,1447354608,"Has anyone managed to get a multilayer perception working in Lasagne? Changing the loss function to squared error (along with changing the output to a float tensor) results in dimension mismatches. There are little to no examples on anything outside classification just for Lasagne; it seems almost all examples are in Nolearn or plain Theano.

Thanks

EDIT: example code below


    #!/usr/bin/env python
    from __future__ import print_function

    import sys
    import os
    import time

    import numpy as np
    import theano
    import theano.tensor as T
    import lasagne
    from lasagne import layers
    from lasagne import nonlinearities
    from PreprocessData import load_dataset

    def build_mlp(input_var=None):
        l_in = lasagne.layers.InputLayer(shape=(None, 1, 30), input_var=input_var)
        l_hid1 = lasagne.layers.DenseLayer(
                l_in, num_units=50,
                nonlinearity=lasagne.nonlinearities.rectify,
                W=lasagne.init.GlorotUniform())
        l_hid2 = lasagne.layers.DenseLayer(
                l_hid1, num_units=40,
                nonlinearity=lasagne.nonlinearities.rectify)
        l_hid3 = lasagne.layers.DenseLayer(
                l_hid2, num_units=30,
                nonlinearity=lasagne.nonlinearities.rectify)
        l_out = lasagne.layers.DenseLayer(
                l_hid3, num_units=2,
                nonlinearity=None)
        return l_out
    
    
    def iterate_minibatches(inputs, targets, batchsize, shuffle=False):
        assert len(inputs) == len(targets)
        if shuffle:
            indices = np.arange(len(inputs))
            np.random.shuffle(indices)
        for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):
            if shuffle:
                excerpt = indices[start_idx:start_idx + batchsize]
            else:
                excerpt = slice(start_idx, start_idx + batchsize)
            yield inputs[excerpt], targets[excerpt]
    
    def main(num_epochs=60000):
        # Load the dataset
        print(""Loading data..."")
        #dataset is rows of 30 features with 1 label.
        X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()
    
        #data processing
        X_train = X_train.reshape(-1, 1, 30).astype(np.float32)
        y_train = y_train.reshape(-1).astype(np.float32)
        X_val = X_val.reshape(-1, 1, 30).astype(np.float32)
        y_val = y_val.reshape(-1).astype(np.float32)
        X_test = X_test.reshape(-1, 1, 30).astype(np.float32)
        y_test = y_test.reshape(-1).astype(np.float32)
    
        # Prepare Theano variables for inputs and targets
        input_var = T.ftensor3('inputs')
        target_var = T.fvector('targets')
    
        print(""Building model and compiling functions..."")
        network = build_mlp(input_var)
    
        prediction = lasagne.layers.get_output(network)
        loss = lasagne.objectives.squared_error(prediction, target_var)
        loss = lasagne.objectives.aggregate(loss, mode = 'mean')
    
        params = lasagne.layers.get_all_params(network, trainable=True)
        updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=0.000000001, momentum=0.9)
    
        test_prediction = lasagne.layers.get_output(network)
        test_loss = lasagne.objectives.squared_error(test_prediction, target_var)
        test_loss = lasagne.objectives.aggregate(test_loss, mode = 'mean')
    
        # As a bonus, also create an expression for the classification accuracy:
        test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),
                          dtype=theano.config.floatX)
    
        # Compile a function performing a training step on a mini-batch (by giving
        # the updates dictionary) and returning the corresponding training loss:
        train_fn = theano.function([input_var, target_var], loss, updates=updates)
    
        # Compile a second function computing the validation loss and accuracy:
        val_fn = theano.function([input_var, target_var], [test_loss, test_acc])
    
        # Finally, launch the training loop.
        print(""Starting training..."")
        # We iterate over epochs:
        for epoch in range(num_epochs):
            # In each epoch, we do a full pass over the training data:
            train_err = 0
            train_batches = 0
            start_time = time.time()
            for batch in iterate_minibatches(X_train, y_train, 50, shuffle=True):
                inputs, targets = batch
                train_loss = train_fn(inputs, targets)
                train_err += train_loss 
                train_batches += 1
            val_err = 0
            val_acc = 0
            val_batches = 0
            for batch in iterate_minibatches(X_val, y_val, 50, shuffle=False):
                inputs, targets = batch
                err, acc = val_fn(inputs, targets)
                val_err += err
                val_acc += acc
                val_batches += 1
            # Then we print the results for this epoch:
            print(""Epoch {} of {} took {:.3f}s"".format(
                epoch + 1, num_epochs, time.time() - start_time))
            print(""  training loss:\t\t{:.6f}"".format(train_err / train_batches))
            print(""  validation loss:\t\t{:.6f}"".format(val_err / val_batches))
            print(""  validation accuracy:\t\t{:.2f} %"".format(
                val_acc / val_batches * 100))",10,2
318,2015-11-13,2015,11,13,4,3skdpz,Tensorflow with cuda/cudnn Docker container,https://www.reddit.com/r/MachineLearning/comments/3skdpz/tensorflow_with_cudacudnn_docker_container/,BrodyHuval,1447354835,,2,9
319,2015-11-13,2015,11,13,4,3skfqp,Is this nanodegree any good and worth the money?,https://www.reddit.com/r/MachineLearning/comments/3skfqp/is_this_nanodegree_any_good_and_worth_the_money/,[deleted],1447355680,[deleted],0,1
320,2015-11-13,2015,11,13,4,3sklse,Is this nanodegree in ML any good and worth the $200/month for 10 months?,https://www.reddit.com/r/MachineLearning/comments/3sklse/is_this_nanodegree_in_ml_any_good_and_worth_the/,trashyguitar,1447358195,,5,3
321,2015-11-13,2015,11,13,5,3sknex,What's the best way to go about transitioning to a ML career? Is it even realistic for someone with my background?,https://www.reddit.com/r/MachineLearning/comments/3sknex/whats_the_best_way_to_go_about_transitioning_to_a/,Cant_Find_Library,1447358819,"Background:

I was drawn to programming since I was very young.  I taught myself Python in eighth grade, and continued to work on small personal projects throughout high school and college.  I never got any formal CS education, and haven't ever felt competent enough to publish code online.  I think there might be a lot of gaps in my basic CS knowledge.  However, over the past 6 months I've been coding and studying a lot (good OOP practices, good general programming principles, etc...) and feel that I've managed to fill in at least some of those gaps.

I graduated from a good school (nowhere close to top 10, but it's a private school that you've probably heard of) with 3.8 GPA and a B.S. in Math.  I then did flight school and flew for the military for a few years.  I like aviation, but it's not something I really chose for myself and I would hate to do it as a career for the rest of my life.  So I'm 26 and trying to figure out where to go from here.

What I really want to do is something with ML.  I've coded some basics recently (basic 2-layer feed-forward NN's, using genetic algorithms and an artificial life simulation to learn and evolve, all coded from scratch, even the physics engine!), but I'm not sure where to go from here.  I visited CMU recently, hoping to learn what I needed to do to give myself a good shot at getting into their ML Master's program, but was very disappointed to learn that one of the largest programs in the country has a 3% acceptance rate.  It's something I may still try for in a year, but I now know I shouldn't base my life plans solely on my chances of getting into one of the few ML programs around (Berkeley, Stanford, CMU, MIT, etc...).

Some good things:

* I am self motivated and can learn on my own.
* I have the time and the savings to sustain myself to take a year and just self-study.  I know there are a lot of good resources online to learn ML concepts, and I can use those.
* I love coding.
* My good GPA will work for me if I go in a direction where it's relevant (e.g. apply for decent, but not top, CS MS programs).

Some bad things:

* My statistics background is weak.  Somehow I just never took the classes, despite my degree.  I can fill that gap in with self-study, but my transcript won't reflect that.
* My CS background is all but zero.  No formal education and no work experience.  Just a lot of personal projects and experience in C++, Java, and Python.
* I have work experience, but it's completely irrelevant to CS/ML.

My options as I see them:

* Self-study for a while and hope I get into a ML grad program.  Maybe try to find a CS/ML research opportunity at my local state university to increase my chances.  (It seems to me, especially after my CMU tour, that I probably wouldn't get in because of my lack of CS/stats background, and I would end up just wasting a year or two self-studying and end up in trouble, with my savings running low and no good plan).
* Self-study for a while, put code out online, and hope someone eventually hires me because of it?  (This option is even scarier than the first...what if I delude myself into thinking I'm making progress, when I'm really just mucking about at an undergraduate level, and then eventually my savings dry up and I have no plan and no options?)
* Lower my ambitions a little bit, try to get into a normal CS M.S. Program, and then maybe try to get into ML later.  This seems like an unattractive option to me, because I definitely don't want to be a non-specialized coder.  I spent enough time on Slashdot when I was younger to know that being a generalist code-monkey is not something I want!
* Recognize that it's too late for this dramatic of a career transition and resign myself to a career as a pilot.

I'm lost and could really use some guidance and advice.  A lot of people in my life are telling me to just give in and fly for career, and I'm starting to fear they are right.  Where do I go from here?  Thanks in advance for the time, knowledge, and wisdom shared!

**Edit: Thanks so much for all the help!  I will continue asking questions below and PM'ing some of you for a few days if you all don't mind...there are a few things I'm still going to respond to!  It seems that a combination of options 1 and 3, shoot for grad programs, are my best choices.  Apparently I was underrating a general CS degree: I just need to pick the program carefully and make sure I can do some ML things.  I still have a lot to figure out over the next year before I apply (I likely will need some classes to fill in some CS/stats gaps if I want to get into a really good program) but I'm motivated now and know where I'm headed.  Thanks so much for all the shared knowledge, you all had a real positive influence on my life today.  I hope future searchers will get something out of this thread as well!**",29,24
322,2015-11-13,2015,11,13,5,3skred,[1511.02954] Reducing the Training Time of Neural Networks by Partitioning,https://www.reddit.com/r/MachineLearning/comments/3skred/151102954_reducing_the_training_time_of_neural/,[deleted],1447360426,[deleted],0,1
323,2015-11-13,2015,11,13,6,3skvto,Needed: More women in data science,https://www.reddit.com/r/MachineLearning/comments/3skvto/needed_more_women_in_data_science/,shugert,1447362222,,7,0
324,2015-11-13,2015,11,13,6,3skxg4,2016 Predictions: What's Next for Data Scientists?,https://www.reddit.com/r/MachineLearning/comments/3skxg4/2016_predictions_whats_next_for_data_scientists/,shugert,1447362880,,0,1
325,2015-11-13,2015,11,13,6,3sl0bg,Automatically Tuning Text Classifiers,https://www.reddit.com/r/MachineLearning/comments/3sl0bg/automatically_tuning_text_classifiers/,Zephyr314,1447364067,,2,8
326,2015-11-13,2015,11,13,7,3sl7wf,"Reducing the Training Time of Neural Networks by Partitioning (Conrado S. Miranda, Fernando J. Von Zuben)",https://www.reddit.com/r/MachineLearning/comments/3sl7wf/reducing_the_training_time_of_neural_networks_by/,[deleted],1447367160,,1,4
327,2015-11-13,2015,11,13,7,3sl9c8,How to perform adagrad SGD for word2vec?,https://www.reddit.com/r/MachineLearning/comments/3sl9c8/how_to_perform_adagrad_sgd_for_word2vec/,Jxieeducation,1447367747,"AdaGrad is an enhanced SGD that automatically determines a per-parameter learning rate.

However, in word2vec, there's no clear ""parameter"" to perform adagrad on. So what's the closest algorithm to adagrad for word2vec?",3,2
328,2015-11-13,2015,11,13,7,3sl9f1,Skype Group For Seq2Seq Learning In TensorFlow,https://www.reddit.com/r/MachineLearning/comments/3sl9f1/skype_group_for_seq2seq_learning_in_tensorflow/,LeavesBreathe,1447367781,"Hey guys, I'll make this short: Just wondering if there is any interest in a skype group for discussing the seq2seq designs TensorFlow introduces. 

It would be meant for discussing things like transferring hidden states, attention mechanisms, optimal memory cell and layer numbers, and more. All of this would be in the context of TensorFlow specifically. 

Edit: People suggested we use Slack instead of Skype as it allows for more functionality and organization. So, we are NOT using the skype group. Instead, we are using slack: https://seq2seqtensorflow.slack.com/

If you don't have an account, its free to join, but I think the functionality beats Skype's by a longshot. ",2,1
329,2015-11-13,2015,11,13,7,3sl9lh,ANNABELL chatbot,https://www.reddit.com/r/MachineLearning/comments/3sl9lh/annabell_chatbot/,Yax42,1447367850,,4,8
330,2015-11-13,2015,11,13,8,3slfgy,A basic TensorFlow example?,https://www.reddit.com/r/MachineLearning/comments/3slfgy/a_basic_tensorflow_example/,thingsPC,1447370362,"I would like to experiment with Tensorflow but I don't know how to begin with it. I had been experimenting with neural networks years ago but it was much simpler. We basically had an array of input variables and one or more output variables. The learning was about altering the network so that the results were improving.

With TensorFlow (or Theano) I am missing some very basic examples like comparing two random inputs and the network has to decide which input is greater.

Is TensorFlow completely different from what I am thinking or am I just not understanding the concept?",2,9
331,2015-11-13,2015,11,13,8,3slhun,AI Ethics: Its not my problem  but shouldnt it be?,https://www.reddit.com/r/MachineLearning/comments/3slhun/ai_ethics_its_not_my_problem_but_shouldnt_it_be/,Nitc,1447371387,,0,1
332,2015-11-13,2015,11,13,8,3slimg,Predicting path from root to leaf for tree structured response,https://www.reddit.com/r/MachineLearning/comments/3slimg/predicting_path_from_root_to_leaf_for_tree/,beamsearch,1447371717,"I have data where the response has the following structure. 

A -&gt; {B,C}
B -&gt; {D,E}
C -&gt; {F,G}

A is the root node and the interpretation is D is a B is a A. Using a some feature vector x, the goal is to predict which class y in {D,E,F,G} is most likely. 

The naive thing to do would be softmax regression over the classes {D,E,F,G}, but that throws away a lot of information. For instance, if the label is D, I'd like to penalize a prediction of E less than a prediction of F or G. In general, I would like to predict the probability of each path from the root to each of the leaf nodes. 

I'm sure this has been studied before but I'm having trouble finding the relevant literature. Any pointers would be appreciated. 

",2,0
333,2015-11-13,2015,11,13,8,3slk3f,Potential non-racist American taxi coming within the next decade,https://www.reddit.com/r/MachineLearning/comments/3slk3f/potential_nonracist_american_taxi_coming_within/,Nitc,1447372324,,0,1
334,2015-11-13,2015,11,13,9,3slng7,Master of Artificial Intelligence Application Critique,https://www.reddit.com/r/MachineLearning/comments/3slng7/master_of_artificial_intelligence_application/,cole-maclean,1447373692,"Hi all, I'm currently working on applications for a Master's degree in Artificial Intelligence. Looking for feedback on my [Statement of Intent](https://drive.google.com/file/d/0B8XlYTkW7of2Y0xfRHlKVzBJbXM/view) and [Resume](https://drive.google.com/file/d/0B8XlYTkW7of2NzVCSWNtT2VCSlU/view) as pieces to this application. Any thoughts would be greatly appreciated.

Thanks!  
-Cole",5,0
335,2015-11-13,2015,11,13,9,3slt1t,Tensorflow feed_dict question,https://www.reddit.com/r/MachineLearning/comments/3slt1t/tensorflow_feed_dict_question/,dee_roy,1447376221,"Hi,
I'm trying to pass a list into feed_dict, however I'm having trouble doing so. Say I have:

    inputs = 10 * [tf.placeholder(tf.float32, shape=(batch_size, input_size))]

where inputs is fed into some function ""outputs"" that I want to compute. So to run this in tensorflow, I created a session and ran the following:

    sess.run(outputs, feed_dict = {inputs: data}) 
    #data is my list of inputs, which is also of length 10

but I get an error, TypeError: unhashable type: 'list'. However, I'm able to pass the data element-wise like so:

    sess.run(outputs, feed_dict = {inputs[0]: data[0], ..., inputs[9]: data[9]}) 

So I'm wondering if there's a way I can solve this issue. I've also tried to construct a dictionary(using a for loop), which gave me the same error. 
I also wasn't sure where to post this question, there doesn't seem to be an established google group at the moment.

Edit: So the issue was with:
         inputs = 10 * [tf.placeholder(tf.float32, shape=(batch_size, input_size))]
Instead it should be implemented as:
         inputs = [tf.placeholder(...) for i in range(10)]
Thanks for the help/suggestions. 

A link to the solution/discussion on stackexchange: 
http://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow/33685256#33685256







",2,1
336,2015-11-13,2015,11,13,14,3smo1u,Distributed machine learning platform from Samsung,https://www.reddit.com/r/MachineLearning/comments/3smo1u/distributed_machine_learning_platform_from_samsung/,Dawny33,1447391596,,5,23
337,2015-11-13,2015,11,13,15,3smvdx,Keras Multilayer Perceptron for scikit-learn,https://www.reddit.com/r/MachineLearning/comments/3smvdx/keras_multilayer_perceptron_for_scikitlearn/,[deleted],1447396362,[deleted],0,0
338,2015-11-13,2015,11,13,15,3smxlt,Does mean centering or feature scaling affect a Principal Component Analysis?,https://www.reddit.com/r/MachineLearning/comments/3smxlt/does_mean_centering_or_feature_scaling_affect_a/,cast42,1447397938,,0,2
339,2015-11-13,2015,11,13,16,3sn0ps,Promising Benefits of a Sound Weed and Termite Control Program,https://www.reddit.com/r/MachineLearning/comments/3sn0ps/promising_benefits_of_a_sound_weed_and_termite/,megatrax,1447400280,,0,1
340,2015-11-13,2015,11,13,19,3sndta,TensorFlow with Multi-GPUs,https://www.reddit.com/r/MachineLearning/comments/3sndta/tensorflow_with_multigpus/,malleus17,1447411514,,1,23
341,2015-11-13,2015,11,13,22,3snokp,A simple explanation of ML and Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3snokp/a_simple_explanation_of_ml_and_deep_learning/,[deleted],1447419905,[deleted],2,1
342,2015-11-13,2015,11,13,22,3snqip,What is the difference between full AD (e.g. Autograd) and computational graph-based approaches (e.g. Theano and TF)?,https://www.reddit.com/r/MachineLearning/comments/3snqip/what_is_the_difference_between_full_ad_eg/,SuperFX,1447421128,"Can someone explain the fundamental difference between auto-diff approaches that work directly on native code (e.g. [Autograd](https://github.com/HIPS/autograd) and Twitter Cortex's recently released [Autograd for Torch](https://github.com/twitter/torch-autograd)) and frameworks that use a DSL to construct a computational graph (like Theano and TensorFlow)? Is the difference ""only"" that the former approach analyzes the execution trace of native code and constructs, effectively, a computation graph on the fly that is then auto-differentiated, while the latter forces the user to explicitly construct that computation graph? Or is there some additional magic in what I'm calling full AD approaches that the graph-based approaches lack? Much thanks!",4,23
343,2015-11-13,2015,11,13,22,3sntd8,A simple explanation of ML and Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3sntd8/a_simple_explanation_of_ml_and_deep_learning/,[deleted],1447422621,[deleted],0,0
344,2015-11-13,2015,11,13,22,3snu8q,Microsoft open sources Distributed Machine Learning Toolkit for more efficient big data research,https://www.reddit.com/r/MachineLearning/comments/3snu8q/microsoft_open_sources_distributed_machine/,PoliticizeThis,1447423106,,1,0
345,2015-11-13,2015,11,13,23,3sny6g,Play Go Against a Deep Neural Net,https://www.reddit.com/r/MachineLearning/comments/3sny6g/play_go_against_a_deep_neural_net/,iori42,1447425178,,16,79
346,2015-11-13,2015,11,13,23,3so1fz,Deep Neural Decision Forests,https://www.reddit.com/r/MachineLearning/comments/3so1fz/deep_neural_decision_forests/,iori42,1447426717,,10,28
347,2015-11-14,2015,11,14,1,3soa0d,Distributed Machine Learning Toolkit,https://www.reddit.com/r/MachineLearning/comments/3soa0d/distributed_machine_learning_toolkit/,dabshitty,1447430545,,0,0
348,2015-11-14,2015,11,14,1,3sobff,is it normal for a convnet to produce redundant filters?,https://www.reddit.com/r/MachineLearning/comments/3sobff/is_it_normal_for_a_convnet_to_produce_redundant/,PeterIanStaker,1447431178,"So I'm trying to train up a convnet on pretty limited data. About 3000 samples (6000 if you count l-r mirror images). Through carefully shrinking the size of the network, playing with L2 regularizers, and a lot of synthetic data, I've managed to get it to the point where it's producing plausible looking filters.

[Here's a picture](http://i.imgur.com/SLdrnap.png)

My main question is, what am I to make of the fact that some of those filters are almost perfectly identical to others? I thought the whole point of relu's and regularization was to avoid multiple neurons doing the  same job. Should I consider harsher regularization? An even smaller network? Fewer filters? Or is this just something that happens?",4,1
349,2015-11-14,2015,11,14,1,3sohl9,"LIVE NOW: The Machine Learning Conference with talks from Google, IBM Watson, Quora, CMU and more!",https://www.reddit.com/r/MachineLearning/comments/3sohl9/live_now_the_machine_learning_conference_with/,shonburton,1447433767,,1,1
350,2015-11-14,2015,11,14,2,3sok8k,Tensorflow basic RNN example with 'variable length' sequences,https://www.reddit.com/r/MachineLearning/comments/3sok8k/tensorflow_basic_rnn_example_with_variable_length/,siblbombs,1447434901,"I've finally gotten a chance to look at recurrence in tensorflow, the documentation examples are a bit complicated for understanding the bare bones of what is happening. This is the basic example I've come up with for just passing some data through a LSTM with no learning going on, its useful for understanding how to set things up.

My takeaways from writing this are:

 * Getting the inputs in is a little weird, since the recurrence loop is built with a python loop. Because of this I had to define the input, then use tf.split to break it into discrete timesteps. Split also keeps the dimension you split on, so there is a reshape in there as well. If you aren't comfortable with list comprehension, it feels like something you will want to bone up on for TF.

 * Variable initialization is an operation you run, not a function you call. This threw me off (coming from theano), I assumed tf.initialize_all_variables() was what I needed, but you have to actually pass that into the session. Makes sense in hindsight.

 * Conditionals aren't documented at all on the tensorflow website, but are in the library. This is how we 'bail' from the recurrent loop for variable length sequences. check out rnn.py for how it is used in action. Same idea as theano's ifelse.

 * For variable length sequences you will need to build the graph out to the maximum length you want to support, then exit early during runtime. You can pass in the bail point for your sequence at each .run() call, since the conditional check is in tensorflow and not python. 

 * You are going to need to pad your input to the maximum size of the loop. I didn't play with tf.pad enough to figure out if you can actually pass in variable length sequences to the .run() call, but the inputs you pass to the rnn when constructing it needs to be the maximum length so I had to make the placeholder that long. Worst case is you will need to pad your data before passing it into .run(), I assume the pain of this is lessened with the Queue setup that is available.

I couldn't get the code block working in the post, so the code is in the comments.",18,44
351,2015-11-14,2015,11,14,3,3sor0r,Any recommended methods/resources for clustering time series data by features?,https://www.reddit.com/r/MachineLearning/comments/3sor0r/any_recommended_methodsresources_for_clustering/,LemurPrime,1447437730,I have data in which the shape of the curve is more informative than the numerical values at an exact time. I'm looking for a good way to cluster millions of time series that's robust to random shifting and scaling. Any suggestions? ,6,2
352,2015-11-14,2015,11,14,3,3sotjm,Advice on a tensorflow mlp example not using MNIST data?,https://www.reddit.com/r/MachineLearning/comments/3sotjm/advice_on_a_tensorflow_mlp_example_not_using/,[deleted],1447438804,[deleted],0,1
353,2015-11-14,2015,11,14,3,3soub7,What Python (evolutionary) neural network library allows for inputting the result of an objective function?,https://www.reddit.com/r/MachineLearning/comments/3soub7/what_python_evolutionary_neural_network_library/,[deleted],1447439116,[deleted],0,1
354,2015-11-14,2015,11,14,4,3sp28d,Help with mlp example not using MNIST data?,https://www.reddit.com/r/MachineLearning/comments/3sp28d/help_with_mlp_example_not_using_mnist_data/,aquajets1,1447442271,"Hello!

So Ive been experimenting with tensorflow, specifically modifying americdamiens examples (https://github.com/aymericdamien/TensorFlow-Examples) to try to create a mlp to solve multi float input, single float output problems (like XOR as a basic example).  Ive had problems getting it to converge, Im wondering if anyone can tell if its obviously an issue with my cost function, or my chosen optimizer, or something else? 

Thanks!

(code, i was having trouble getting it to format right)
https://gist.github.com/dpcollin/fef642bfc684380bfbb5",10,0
355,2015-11-14,2015,11,14,4,3sp45c,Transfer learning with CNN (Incremental learning)?,https://www.reddit.com/r/MachineLearning/comments/3sp45c/transfer_learning_with_cnn_incremental_learning/,mikos,1447443039,"Hi all, I trained a CNN for about 2000 classes with a few hundred samples each (""Model-1""). I used pre-trained model Imagenet for transfer learning.

For **incremental** training, should I add new samples into the folder and:

1. Use imagenet again as pre-trained model? or 
1. Should I point to  the previous model (""model 1"")? 

what is the more advisable route? I am using DIGITS/Caffe  and generating GoogLeNet.

Note: I am not adding new classes, just augmenting the training samples in existing classes.

Thanks all for your fantastic help in helping out a newbie.

",0,0
356,2015-11-14,2015,11,14,5,3spee6,The Real Trouble with Cognitive Computing,https://www.reddit.com/r/MachineLearning/comments/3spee6/the_real_trouble_with_cognitive_computing/,[deleted],1447447236,[deleted],0,0
357,2015-11-14,2015,11,14,6,3spm6d,Multi-label (Convolutional) Neural Networks w\ Label Interdependence,https://www.reddit.com/r/MachineLearning/comments/3spm6d/multilabel_convolutional_neural_networks_w_label/,TheCocoanaut,1447450530,"Hey there,

I'm currently looking into utilising a CNN learned on text (several different architectures[1][2][3][4]) for multi-label text classification. I found this report (https://cs224d.stanford.edu/reports/BergerMark.pdf) but it was missing some informations and I am not sure it is correct (it's just a students report, not a paper):

We then add a fully connected output layer of size |L| with a sigmoid activation function, which produces a probability for each of our potential labels. y =  (Uc' + b^o)

Is `U` supposed to be the dropout matrix? I don't know. Also from my understanding one can't model multivariate dependent probabilities with the sigmoid function, or is it only dependent on the error function I use? I'd need to do some thresholding on the dependencies (summing to one) to say this is positively associated with the input x^i.

I didn't found that much (read: almost none) papers on this topic. Especially using CNN/RNN/DBN/DBM/RBMs/ANN etc.

Some other possibilities I've found or came up with include using 2^n softmax outputs modelling a label power set (I think it should be ok, because n = 8 in my specific case, untested yet). Also I wanted to have a look at cross-entropy as an error function for this case[5], but not sure yet (well, mostly because I have to do some reading about this).


I'd love some clarification or hints on my thoughts. I was not able to find papers on the basics (sigmoid/logsoftmax/cross-entropy etc for multi label) and thus cannot verify any of this. If you can provide any resources I'd be glad if you could refer me to them! I'm looking forward to your input.


TL;DR: Can I use sigmoid/log softmax for multi-label? Does it model label interdependence? If not, can I incorporate this without label power set?


[1]: Effective Use of Word Order for Text Categorization with Convolutional Neural Networks, Rie &amp; Zhang

[2]: Convolutional Neural Networks for Sentence Classification, Kim

[3]: A Convolutional Neural Network for Modelling Sentences, Kalchbrenner, Grefenstette, Blunsom

[4]: Character-level Convolutional Networks for Text Classification, Zhang, Zhao, LeCun (not sure if this is multi-label already to be honest, uses log soft max as output)

[5]: Large-scale Multi-label Text Classification  Revisiting Neural Networks, Nam et al.",3,4
358,2015-11-14,2015,11,14,7,3sps3p,Is variance reduction SGD used in training deep neural networks?,https://www.reddit.com/r/MachineLearning/comments/3sps3p/is_variance_reduction_sgd_used_in_training_deep/,george_redit,1447453075,Anyone has experience using variance reduction methods for training deep neural networks?,6,0
359,2015-11-14,2015,11,14,7,3spslo,Simple Tensor Flow demo: XOR learning,https://www.reddit.com/r/MachineLearning/comments/3spslo/simple_tensor_flow_demo_xor_learning/,nivwusquorum,1447453298,,0,2
360,2015-11-14,2015,11,14,7,3sptmo,Strong scaling DNN training on commodity g2.2xlarge instances,https://www.reddit.com/r/MachineLearning/comments/3sptmo/strong_scaling_dnn_training_on_commodity/,XeonPhitanium,1447453768,,3,5
361,2015-11-14,2015,11,14,7,3spulp,Pylearn2 is no longer developed?,https://www.reddit.com/r/MachineLearning/comments/3spulp/pylearn2_is_no_longer_developed/,SamyMel,1447454207,"While I was asking about different deepLearning Frameworks, some people (from another framework community other than pytlearn2's) told me that pylearn2 was no longer very maintained.
Is it true? is there a reason why? And what do you guys (python programmers) use for deepLearning? (my work will be mainly to implement and try out different ConvNets configs, no RNN)",8,5
362,2015-11-14,2015,11,14,12,3sqqvv,Discussion of Juergen Schmidhuber's 'Brainstorm' package?,https://www.reddit.com/r/MachineLearning/comments/3sqqvv/discussion_of_juergen_schmidhubers_brainstorm/,discgolfingQnet,1447472461,"As found here: http://people.idsia.ch/~juergen/brainstorm.html 

I was trying to find some general overviews, analysis, or reviews outside of the documentation and have been unable to do so.

Any thoughts on Brainstorm or what Juergen is up to these days?",6,4
363,2015-11-14,2015,11,14,13,3sqvl3,SPFC 0202,https://www.reddit.com/r/MachineLearning/comments/3sqvl3/spfc_0202/,dongfengpacking,1447475188,,1,1
364,2015-11-14,2015,11,14,15,3sr9h4,10 more lessons learned from building real-life machine learning systems @quora (and the master algorithm),https://www.reddit.com/r/MachineLearning/comments/3sr9h4/10_more_lessons_learned_from_building_reallife/,cast42,1447484130,,0,0
365,2015-11-14,2015,11,14,17,3srhqn,Is using PHP alot helpful for ML/data science?,https://www.reddit.com/r/MachineLearning/comments/3srhqn/is_using_php_alot_helpful_for_mldata_science/,[deleted],1447490940,[deleted],4,0
366,2015-11-14,2015,11,14,18,3sriww,"ShhhDont touch me, the TV is watching!",https://www.reddit.com/r/MachineLearning/comments/3sriww/shhhdont_touch_me_the_tv_is_watching/,adeniyiks,1447492087,,0,1
367,2015-11-14,2015,11,14,19,3srnzr,Classification-aware regression loss function?,https://www.reddit.com/r/MachineLearning/comments/3srnzr/classificationaware_regression_loss_function/,ekerazha,1447496699,"Suppose I use the MSE loss function for a regression problem.

Let's say my ideal value is ""+10"" and my output/predicted value is ""-5"". Now, let's say my output/predicted value is ""+100"".

Obviously, -5 is closer to +10 than +100, so the squared error for -5 is lower.

However, if we classify the value as sign(value), then the +100 prediction got the classification right (because +100 and +10 have the same sign), while -5 is a classification error.

It would be nice to have a regression loss function which also penalizes a discordant sign. So, maybe, when my ideal value is ""+10"", then ""-5"" isn't much better than ""+100"", because it is closer to ""+10"" but it has the wrong sign.

Is there a well-known regression loss function which is also ""classification-aware""?",10,2
368,2015-11-14,2015,11,14,21,3srxf3,Symphonies from Synapses - The Brain as Universal Dynamical Systems Computer,https://www.reddit.com/r/MachineLearning/comments/3srxf3/symphonies_from_synapses_the_brain_as_universal/,fergbyrne,1447504809,,10,0
369,2015-11-14,2015,11,14,21,3srxgp,Reverse classification using ANN?,https://www.reddit.com/r/MachineLearning/comments/3srxgp/reverse_classification_using_ann/,JonnyRobbie,1447504834,The classic approach with ANNs is to have some input (ie. images) and the output is a classification vector. Are there any examples/papers reversing the problem? When you have a classification vectors as an input and the output would be generated image?,3,0
370,2015-11-14,2015,11,14,22,3ss2hl,Online MIT Lectures Covering Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/3ss2hl/online_mit_lectures_covering_artificial/,RANDOMDBZ,1447508446,,0,0
371,2015-11-15,2015,11,15,1,3ssh6r,Reinforcement Learning By David Silver Google Drive Link!,https://www.reddit.com/r/MachineLearning/comments/3ssh6r/reinforcement_learning_by_david_silver_google/,code2hell,1447516874,"I found this course very useful and informative for learning RL.
Thought it would be helpful to others to so Im sharing the link [here](https://drive.google.com/folderview?id=0B6EE3Sw2jmZWNTZXNW9ucHhHS2M&amp;usp=sharing)",10,25
372,2015-11-15,2015,11,15,2,3ssphx,Speaker Slides from MLconf San Francisco 2015,https://www.reddit.com/r/MachineLearning/comments/3ssphx/speaker_slides_from_mlconf_san_francisco_2015/,dafcok,1447520770,,0,1
373,2015-11-15,2015,11,15,2,3sstt2,K-Means Clustering with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/3sstt2/kmeans_clustering_with_tensorflow/,sachinrjoglekar,1447522721,,5,25
374,2015-11-15,2015,11,15,5,3stfo1,Neat problem about building better predictors from simpler ones,https://www.reddit.com/r/MachineLearning/comments/3stfo1/neat_problem_about_building_better_predictors/,horia141,1447532438,,0,0
375,2015-11-15,2015,11,15,6,3stoh7,Awesome oxford deep learning lectures,https://www.reddit.com/r/MachineLearning/comments/3stoh7/awesome_oxford_deep_learning_lectures/,swentso,1447536409,,14,151
376,2015-11-15,2015,11,15,7,3su091,Google's New AI System Could Be 'Machine Learning' Breakthrough,https://www.reddit.com/r/MachineLearning/comments/3su091/googles_new_ai_system_could_be_machine_learning/,shugert,1447541952,,0,0
377,2015-11-15,2015,11,15,8,3su2xf,Neo/RL - a GPU neocortex simulation library,https://www.reddit.com/r/MachineLearning/comments/3su2xf/neorl_a_gpu_neocortex_simulation_library/,CireNeikual,1447543195,,18,5
378,2015-11-15,2015,11,15,8,3su569,How versatile is the autoencoder method?,https://www.reddit.com/r/MachineLearning/comments/3su569/how_versatile_is_the_autoencoder_method/,[deleted],1447544288,[deleted],5,2
379,2015-11-15,2015,11,15,9,3su83r,TensorFlow differentiating matrix determinant,https://www.reddit.com/r/MachineLearning/comments/3su83r/tensorflow_differentiating_matrix_determinant/,bridgebywaterfall,1447545649,"I am interested in computing the derivative of a matrix determinant using TensorFlow. I can see from experimentation that TensorFlow has not implemented a method of differentiating through a determinant:

    LookupError: No gradient defined for operation 'MatrixDeterminant' 
    (op type: MatrixDeterminant)

A little further investigation revealed that it is actually possible to compute the derivative; see for example [Jacobi's formula](https://en.wikipedia.org/wiki/Jacobi%27s_formula). I determined that in order to implement this means of differentiating through a determinant that I need to use the function decorator,

    @tf.RegisterGradient(""MatrixDeterminant"")
    def _sub_grad(op, grad):
        ...

However, I am not familiar enough with tensor flow to understand how this can be accomplished. Does anyone have any insight on this matter?

Here's an example problem where I run into this issue:

    x = tf.Variable(tf.ones(shape=[1]))
    y = tf.Variable(tf.ones(shape=[1]))
    
    A = tf.reshape(
        tf.pack([tf.sin(x), tf.zeros([1, ]), tf.zeros([1, ]), tf.cos(y)]), (2,2)
    )
    loss = tf.square(tf.matrix_determinant(A))
    
    
    optimizer = tf.train.GradientDescentOptimizer(0.001)
    train = optimizer.minimize(loss)
    
    init = tf.initialize_all_variables()
    sess = tf.Session()
    sess.run(init)
    

    for step in xrange(100):
        sess.run(train)
        print sess.run(x)",1,2
380,2015-11-15,2015,11,15,9,3suf8d,IBM SystemML: Declarative machine learning that works no matter the environment.,https://www.reddit.com/r/MachineLearning/comments/3suf8d/ibm_systemml_declarative_machine_learning_that/,hjoel5,1447549121,"SystemML is a flexible, scalable machine learning (ML) language written in Java. SystemML's distinguishing characteristics are: (1) algorithm customizability, (2) multiple execution modes, including Standalone, Hadoop Batch, and Spark Batch, and (3) automatic optimization.

The latest documentation can be found at the SystemML Documentation web site.

https://github.com/sparktc/systemml

",1,0
381,2015-11-15,2015,11,15,12,3suu9i,Unsupervised Learning of Video Representations using LSTMs (paper+code),https://www.reddit.com/r/MachineLearning/comments/3suu9i/unsupervised_learning_of_video_representations/,samim23,1447557008,,0,5
382,2015-11-15,2015,11,15,14,3sv7yz,"Question about combinatorial/non-noisy learning (e.g., Chess)",https://www.reddit.com/r/MachineLearning/comments/3sv7yz/question_about_combinatorialnonnoisy_learning_eg/,physixer,1447564884,"I have a question about learning decisions in a state-space, e.g., the space of chess moves.

- Let's say my program is sitting with the start state of a chess board, and doesn't know any tactics/strategies, it only knows the rules of chess, and that own piece elimination or own loss is bad, and opponent's piece elimination, and opponent's loss is good.
- It knows it has to make a move decision out of the 20 (or whatever) possible first moves.
- Since it doesn't know any move being better than any other move, all moves are ""equally valuable"".
- ""Equally valuable"" gives the first clue that we should perhaps assign ""probability"" to each move. So 1/20 to each move. And that the learning process should adjust these probabilities so that the next time the program starts a new game, not all moves are equally valued (have equal move probability) and either the move with highest prob. is picked, or a move is picked by sampling the probability distribution of moves.

So my question is, what kind of learning algorithm helps with updating these probabilities? how is it different if the probability is updated every time a piece (own or opponent's) is eliminated, vs, updating probabilities at the end of a win or loss, vs, a combination of those? finally, how can a ""hierarchical"" probability model is built such that, there are probabilities assigned to, not only each move, but to each tactic (which themselves are learned not fed), as well as each strategy (which also is learned).

I guess it would be a combination of ""reinforcement learning"", ""hierarchical bayesian modeling"", and so on. But I'd appreciate, if anyone can point out specific resource for this kind of approach (not necessarily for chess, but preferably for a combinatorial problem, not things like images, signals, etc).

Also please note that I'm somewhat aware of typical approaches for chess programs (graph search, hard coding openings, end-games etc), I'm not looking for those approaches. (I guess I'm looking for a method that would most mimic the way a human chess newbie learns and improves. Though part of chess training includes ""explicit"" learning of many tactics, strategies, openings, and end games; let's say I want to ignore that for this simple learning approach).

Thanks in advance.",2,0
383,2015-11-15,2015,11,15,14,3sv8hi,ML tools?,https://www.reddit.com/r/MachineLearning/comments/3sv8hi/ml_tools/,bigd8a,1447565240,"What (easy to use) tools would you recommend for -
1. Collaborative Filtering (recommendations)
2. K-means clustering
3. Multi-class classifier",2,0
384,2015-11-15,2015,11,15,14,3svacj,Pedro Domingos attempts to predict the future in Wall Street Journal.,https://www.reddit.com/r/MachineLearning/comments/3svacj/pedro_domingos_attempts_to_predict_the_future_in/,lozj,1447566445,,0,0
385,2015-11-15,2015,11,15,16,3svk85,Deep Neural Decision Forests,https://www.reddit.com/r/MachineLearning/comments/3svk85/deep_neural_decision_forests/,Atupis,1447573361,,1,0
386,2015-11-15,2015,11,15,16,3svlbq,Python libraries for Deep Learning with Sequences (by hawflake 2015-11-11),https://www.reddit.com/r/MachineLearning/comments/3svlbq/python_libraries_for_deep_learning_with_sequences/,cast42,1447574304,,0,1
387,2015-11-15,2015,11,15,16,3svlfe,Bayesian Modelling in Python (PYMC3 Tutorial),https://www.reddit.com/r/MachineLearning/comments/3svlfe/bayesian_modelling_in_python_pymc3_tutorial/,cast42,1447574397,,0,11
388,2015-11-15,2015,11,15,17,3svndc,Anyone know of a good database for music-classification?,https://www.reddit.com/r/MachineLearning/comments/3svndc/anyone_know_of_a_good_database_for/,burgeoning_philosoph,1447576061,"Hi all, I've recently become acquainted with neural networks and I would like to apply them to music classification. The problem i'm trying to solve is to build a classifier like Shazam that works on a composition regardless of it's stylistic properties like instrumentation, bpm, and key, such as being able to recognize a heavy-metal, jazz, or electronic arrangement of a well-known classical piece like Beethoven's 5th. (If this has been done before, great, I'm just using this as a learning experience with the possibility of extending functionality later) For training, I'm looking for a source for classical compositions performed in these different styles.  Does anyone know where I could find such a database? Thanks.",2,5
389,2015-11-15,2015,11,15,17,3svoo2,DeepQ learning using tensorflow,https://www.reddit.com/r/MachineLearning/comments/3svoo2/deepq_learning_using_tensorflow/,nivwusquorum,1447577129,,12,71
390,2015-11-15,2015,11,15,18,3svqx7,A short introduction about LSTM and corresponding applications,https://www.reddit.com/r/MachineLearning/comments/3svqx7/a_short_introduction_about_lstm_and_corresponding/,neutronest,1447579289,"http://www.neutronest.moe/2015-11-15-LSTM-survey.html

Hi, friends. These days I keep surveying the LSTM for a long time.. Therefore I make a summary about LSTM and its applications involved in ACL, EMNLP, ICML... hope it can give you some help if you are interested into LSTM and deep learning!

The blog's url is at the above. I'm sure that there are many mistakes therefore I will keep modifying my article... 

Thank you very much.",3,8
391,2015-11-15,2015,11,15,18,3svtbn,Ensemble methods: Bayes' Broadsword,https://www.reddit.com/r/MachineLearning/comments/3svtbn/ensemble_methods_bayes_broadsword/,kaj_sotala,1447581496,,0,9
392,2015-11-15,2015,11,15,21,3sw2uh,1D convolutional network ?,https://www.reddit.com/r/MachineLearning/comments/3sw2uh/1d_convolutional_network/,fifnir,1447590297,"Hello all,

I'm a machine learning amateur who got very excited with Tensorflow and I am now trying to wrap my head around the first two tutorials. 


My ultimate goal is to use this on genomics data, so as a first step I thought I'd rebuild the second tutorial, from a 2d neural network in a 1D network that will handle the image data in a 1d vector, just like tutorial 1 did. It then occured to me that the convolution function on which the whole ""network"" concept is based on, is strictly 2d.    

So here's my question:  
Is it silly to try to try and build a 1d convolutional network? 
Do they only work in 2d? Could I be doing something simpler that will be just as good for 1d data ?

Any relevant resources would be very appreciated!",10,1
393,2015-11-15,2015,11,15,21,3sw2ys,What is the best recommendation algorithm for book?,https://www.reddit.com/r/MachineLearning/comments/3sw2ys/what_is_the_best_recommendation_algorithm_for_book/,[deleted],1447590417,[deleted],0,0
394,2015-11-16,2015,11,16,0,3swkg9,Can someone explain in simple english how the margin ranking loss algorithm works?,https://www.reddit.com/r/MachineLearning/comments/3swkg9/can_someone_explain_in_simple_english_how_the/,ilija139,1447602241," It is used for training in Memory Networks http://arxiv.org/abs/1410.3916. I believe the algorithm is described in http://research.microsoft.com/apps/pubs/default.aspx?id=65610 but it requires some background (which I don't have) to understand it.  
",3,3
395,2015-11-16,2015,11,16,1,3swpz3,Want an open-source deep learning framework? Take your pick,https://www.reddit.com/r/MachineLearning/comments/3swpz3/want_an_opensource_deep_learning_framework_take/,Elendar42,1447605050,,2,23
396,2015-11-16,2015,11,16,1,3swr18,Keras shape mismatch,https://www.reddit.com/r/MachineLearning/comments/3swr18/keras_shape_mismatch/,jacques_lefont,1447605555,"I'm playing with keras (python ML library) and trying to deploy a regression model, but I'm stuck. During the fitting of the model, it throws me a ""shape mismatch"" error. My code goes like this:
input_matrix = np.load(r'../INPUT_MATRIX.npy')
np.random.shuffle(input_matrix)

# 70/30 split
n_rows = np.shape(input_matrix)[0]
train = input_matrix[0:1000, :]
test = input_matrix[1000:, :]

# make sure that type == float
train = train.astype(float)
test = test.astype(float)

# Train/test split
X_train = train[:, :-1]
y_train = train[:, -1]
X_test = test[:, :-1]
y_test = test[:, -1]

# Model
model = Sequential()
model.add(Dense(input_dim=np.shape(train)[1], output_dim=256, init='uniform'))
model.add(Activation('tanh'))
model.add(Dropout(0.2))
model.add(Dense(input_dim=np.shape(train)[1], output_dim=256, init='uniform'))
model.add(Activation('tanh'))
model.add(Dropout(0.5))
model.add(Dense(input_dim=np.shape(train)[1], output_dim=1, init='uniform'))
model.compile(loss='mean_squared_error', optimizer='rmsprop')

# Fitting
model.fit(X_train, y_train, batch_size=32, nb_epoch=30, verbose=2, validation_split=0.15)

It throws out this:
ValueError: Shape mismatch: x has 29617 cols (and 32 rows) but y has 29618 rows (and 256 cols)

Can't see what I'm doing wrong...
",7,0
397,2015-11-16,2015,11,16,2,3swvsj,Is there a place that keeps track of state-of-the-art results for all the common public datasets?,https://www.reddit.com/r/MachineLearning/comments/3swvsj/is_there_a_place_that_keeps_track_of/,[deleted],1447607635,[deleted],4,12
398,2015-11-16,2015,11,16,2,3swzew,Machine learning and 3D games.,https://www.reddit.com/r/MachineLearning/comments/3swzew/machine_learning_and_3d_games/,Cortez2015,1447609219,"Imagine AI/ML figure out by itself how to beat games like *Super Mario 64* or *Legend of Zelda: Ocarina of time* without supervised learning.

What's your thoughs?",3,3
399,2015-11-16,2015,11,16,3,3sx331,Microsoft's Emotion Recognition,https://www.reddit.com/r/MachineLearning/comments/3sx331/microsofts_emotion_recognition/,swentso,1447610743,"https://www.projectoxford.ai/demo/emotion#detection
Nice (for once, coming from Microsoft :p) isn't it ?
",5,6
400,2015-11-16,2015,11,16,6,3sxt18,Computational Linguistics and Deep Learning-Christopher Manning,https://www.reddit.com/r/MachineLearning/comments/3sxt18/computational_linguistics_and_deep/,sidsig,1447621793,,6,26
401,2015-11-16,2015,11,16,7,3sy1ao,NVIDIA Deep Learning Course,https://www.reddit.com/r/MachineLearning/comments/3sy1ao/nvidia_deep_learning_course/,swentso,1447625265,,8,103
402,2015-11-16,2015,11,16,8,3sy9ev,tpot - A Python tool that automatically creates and optimizes Machine Learning pipelines using genetic programming,https://www.reddit.com/r/MachineLearning/comments/3sy9ev/tpot_a_python_tool_that_automatically_creates_and/,Kalendos,1447628840,,23,23
403,2015-11-16,2015,11,16,8,3syahd,Help me build a recommendation engine with restaurants! (or other stuff -- I have a lot of user review data and I don't know what to do with it),https://www.reddit.com/r/MachineLearning/comments/3syahd/help_me_build_a_recommendation_engine_with/,sharpchicity,1447629335,"I was watching the new Aziz Ansari show Master of None and it got me thinking about how much time people waste pouring through yelp reviews trying to get more information about the restaurant they're looking at. When you click on a restaurant, you're probably thinking ""What is different in this 4 star pizza place compared to the other?"" or ""What are some of the popular dishes that users have tried?""

Well I have built a database of ~1700 Chicago restaurants with information from Yelp including rating, cost, location, etc as well as up to 500 user reviews for that restaurant. What I want to do with it is still semi-up in the air, but I was thinking of building a recommendation engine based on a user's previous restaurant visits, tastes, budget, food choices (would theoretically need menu info as well), etc.


That being said, while I know how to use R, Python, SQL, etc, and am into the whole ""data science"" stuff, I'm not too familiar with doing / building Natural Language Processing, recommendation engines, website building, nor anything else that goes into this kind of work. I was wondering if anyone would be interested in working with me on this project.

Let me know!
",3,0
404,2015-11-16,2015,11,16,9,3sykap,Can somebody tell me if I'm using bootstrap resampling correctly?,https://www.reddit.com/r/MachineLearning/comments/3sykap/can_somebody_tell_me_if_im_using_bootstrap/,o_safadinho,1447633837,"I'm trying to use a relatively large csv file, 573.8 MB, to train a classification model. The problem is that I can't read the entire file into memory AND process it. I thought that I could use bootstrap resampling to approximate the value that I'm really interested in since I can't perform the calculations that I want on the entire data set. I plan on ranking the variables by their information gain values in descending order and then taking the top 10 percent.

I wrote a function that I can use to read in the entire file. From there I perform random sampling with replacement 90,000.

I know that I have to I have to resample from my sampled data set, i.e. take samples from the 90,000 that were sampled from the original set. The questions that I want to know is, is the mean information gain value that is taken from the resampled subsets the value that I use for the information gain ranking?",0,3
405,2015-11-16,2015,11,16,12,3sz746,Anyone Can Learn to Code an LSTM-RNN (Part 1: RNN),https://www.reddit.com/r/MachineLearning/comments/3sz746/anyone_can_learn_to_code_an_lstmrnn_part_1_rnn/,[deleted],1447644738,[deleted],1,0
406,2015-11-16,2015,11,16,12,3sz904,Handling missing data,https://www.reddit.com/r/MachineLearning/comments/3sz904/handling_missing_data/,gntc,1447645631,"I'm relatively new to this so I apologize I'll apologize up front.  I have data that has a fair number of missing values (in total about 20%, some features are missing up to 90%).  What is the best way of dealing with this?  I have been looking at imputation methods, but there must be algorithms that can build models around the missing values.  I'm just wondering what is out there and what normally done in this situation.

Thanks",2,1
407,2015-11-16,2015,11,16,13,3sze01,Python distributed machine learning,https://www.reddit.com/r/MachineLearning/comments/3sze01/python_distributed_machine_learning/,nem2k87,1447648235,"I occasionally train neural nets for my research, and they usually take quite a long time to run (especially when I'm working on my laptop).

I'm looking for a way to build the model on any computer and send it up to a server for training and have it return the graphs/accuracies/weights etc. I know there are paid solutions for this but I'm looking for a distributed solution I can run myself.

I have a server set up at home which is about to get a CPU and GPU upgrade. I'd like to be able to set it up so that when I'm working on the LAN, or when I'm working remotely on my laptop, I can send code to the server and have it train the model and return to me the results (or save the results if the sender machine is switched off)

Are there any existing solutions to accomplish something like this? I'm not tied to any specific library, but would prefer to stick with Python if possible",1,0
408,2015-11-16,2015,11,16,13,3szfsn,Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN),https://www.reddit.com/r/MachineLearning/comments/3szfsn/anyone_can_learn_to_code_an_lstmrnn_in_python/,Dawny33,1447649196,,11,85
409,2015-11-16,2015,11,16,13,3szh7w,A new work on topic generation:HLSM review wanted,https://www.reddit.com/r/MachineLearning/comments/3szh7w/a_new_work_on_topic_generationhlsm_review_wanted/,GuoruiZhou,1447649976," Abstract:
    Much of information sits in an unprecedented amount of text data. Managing allocation of these large scale text data is an important problem for many areas. Topic modeling performs well in this problem. The traditional generative models (PLSA,LDA) are the state-of-the-art approaches in topic modeling and most recent research on topic generation has been focusing on improving or extending these models. However, results of traditional generative models are sensitive to the number of topics K, which must be specified manually. The problem of generating topics from corpus resembles community detection in networks. Many effective algorithms can automatically detect communities from networks without a manually specified number of the communities. Inspired by these algorithms, in this paper, we propose a novel method named Hierarchical Latent Semantic Mapping (HLSM), which automatically generates topics from corpus. HLSM calculates the association between each pair of words in the latent topic space, then constructs a unipartite network of words with this association and hierarchically generates topics from this network. We apply HLSM to several document collections and the experimental comparisons against several state-of-the-art approaches demonstrate the promising performance.

full article: http://arxiv.org/abs/1511.03546

Any comments will be welcome",4,2
410,2015-11-16,2015,11,16,14,3szlsb,Karpathy short story on reinforcement learning,https://www.reddit.com/r/MachineLearning/comments/3szlsb/karpathy_short_story_on_reinforcement_learning/,[deleted],1447652492,[deleted],1,1
411,2015-11-16,2015,11,16,14,3szmpm,Asking Reddit: The recently developed Deep Learning powered Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/3szmpm/asking_reddit_the_recently_developed_deep/,I_ai_AI,1447653023,I am new to this topic. Can somebody help me answer the two questions: 1. Do the recent surprising performance of RL mainly come from the improved perception ability of DL? 2. Are there some new techniques of RL developed (e.g. the learning algorithm) because of introducing DL into RL?,8,7
412,2015-11-16,2015,11,16,15,3szqcf,Action Recognition in video using Visual Attention (paper+code),https://www.reddit.com/r/MachineLearning/comments/3szqcf/action_recognition_in_video_using_visual/,samim23,1447655134,,4,6
413,2015-11-16,2015,11,16,15,3szr4g,NeoRL: How it works,https://www.reddit.com/r/MachineLearning/comments/3szr4g/neorl_how_it_works/,CireNeikual,1447655660,,9,7
414,2015-11-16,2015,11,16,18,3t03wg,Does anyone have a slide deck about LHC using Random Forests?,https://www.reddit.com/r/MachineLearning/comments/3t03wg/does_anyone_have_a_slide_deck_about_lhc_using/,srkiboy83,1447665152,"Hi everyone, 

I read in a Hacker News thread today (https://news.ycombinator.com/item?id=10572678) that the LHC used Random Forests to some extent to detect the Higgs Boson. Apparently, there's a slide deck about it.

Would anyone happen to have said deck on hand?",2,1
415,2015-11-16,2015,11,16,18,3t04li,plastic injection molding machine,https://www.reddit.com/r/MachineLearning/comments/3t04li/plastic_injection_molding_machine/,torchofficesystem,1447665729,,0,0
416,2015-11-16,2015,11,16,19,3t0b5c,Try Tensor Flow in this online environment,https://www.reddit.com/r/MachineLearning/comments/3t0b5c/try_tensor_flow_in_this_online_environment/,ahmed_ajaali,1447670966,,14,17
417,2015-11-16,2015,11,16,20,3t0eg2,TensorFlow simple examples questions,https://www.reddit.com/r/MachineLearning/comments/3t0eg2/tensorflow_simple_examples_questions/,mooh78,1447673532,I am learning how to use tensorflow but I am a beginner and I didn't understand much the official tutorials. So I found those more simple examples: https://github.com/aymericdamien/TensorFlow-Examples but I can't figure out how to use my own data instead of mnsit inside the logistic_regression.py script,2,1
418,2015-11-16,2015,11,16,23,3t0vmw,How to make the outputs of different LR models comparable?,https://www.reddit.com/r/MachineLearning/comments/3t0vmw/how_to_make_the_outputs_of_different_lr_models/,[deleted],1447684330,[deleted],1,0
419,2015-11-17,2015,11,17,1,3t1887,Use Word Embeddings in Tensor Flow's Seq 2 Seq Model,https://www.reddit.com/r/MachineLearning/comments/3t1887/use_word_embeddings_in_tensor_flows_seq_2_seq/,LeavesBreathe,1447690268,"Hey Guys,

I really like Tensor Flow, but one aspect has confused me: Why do all the seq2seq models only accept 2D tensor input?

It seems to me that you are forced to assign *one* number to each word, allowing you to make a 2D tensor for your input.

However, in all the seq2seq papers, they input their words in word-embedding format where each word has 100 or even 1000 length vectors. 

So I'm confused: Why does TensorFlow's Seq2Seq models only allow you to 2D inputs? Wouldn't this make the seq2seq model drastically worse because you can't essentially use word embeddings? Thanks alot! ",8,3
420,2015-11-17,2015,11,17,1,3t190f,Has anybody got experience with using DL4J for NLP?,https://www.reddit.com/r/MachineLearning/comments/3t190f/has_anybody_got_experience_with_using_dl4j_for_nlp/,WeeShirtOn,1447690610,"I'm working on sarcasm detection using neural nets. I've got some questions about preparing the data - normalization, word embeddings etc. Does anybody have relevant experience and willing to help me out? Thanks.",7,2
421,2015-11-17,2015,11,17,1,3t1d8x,Generating artificial images for training (Deep learning),https://www.reddit.com/r/MachineLearning/comments/3t1d8x/generating_artificial_images_for_training_deep/,mikos,1447692376,"I am trying to identify different types of vehicles and logos etc. As with most efforts in this domain the challenge has been a paucity of training images. 

Has anyone tried generating artificial images, say by ""drawing"" the logo atop (say) a vehicle hood, vehicle  rear and performing distortions to make it realistic. Could anyone share their experiences with this approach? ",5,5
422,2015-11-17,2015,11,17,1,3t1di3,Resources that teach machine learning/AI?,https://www.reddit.com/r/MachineLearning/comments/3t1di3/resources_that_teach_machine_learningai/,aexolthum,1447692482,"Hello, I'm looking for possibly codecademy type resources, but maybe even just videos that also provide projects/hw.
My learning (as I'm sure is true of many programmers) is very project based.

I'm in an intro to AI class now that's gone over search problems np complete problems, bays nets and now is going to machine learning, but I dont like the class has reinforced the material well and am not yet comfortable with it.

Any help appreciated!",3,0
423,2015-11-17,2015,11,17,2,3t1fbg,AMD @ SC15: AMD will somewhat support CUDA source code compilation to native AMD GPU binaries.,https://www.reddit.com/r/MachineLearning/comments/3t1fbg/amd_sc15_amd_will_somewhat_support_cuda_source/,rndnum123,1447693211,,11,34
424,2015-11-17,2015,11,17,2,3t1kme,Question: Stand alone name detection?,https://www.reddit.com/r/MachineLearning/comments/3t1kme/question_stand_alone_name_detection/,yyttr3,1447695369,"I'm trying to write a program to automatically extract ""hashtags"" from a photo for a work project (digitizing some books). One of the steps requires me to do Name Entity Recognition after I have extracted the text. I know this can be done with some accuracy with nltk, but from what I understand only on structured text.

The names I would be extracting would be of the form (an example):

    . Vice-Presid ent

    . Secretary and Treasurer
    . . . . Sponsor

    LILLIAN ALLISON
    JOHNIE HAMILTON
    WAVER HAIRSTON

    LOIS ARMSTRONG
    jENNIE McCANN
    DAISY DEENE CAMPBELL
    RUTH COOPER
I've done some basic text classification with bayesian classifiers and my first thought was maybe I could do the same with a large name corpora.

I just want someone to point me to some, relatively understandable, research on the topic or direct me in the right direction. I'd rather not waste a lot of time on a wrong technique and I have never really done anything like this.

Thank you.",1,0
425,2015-11-17,2015,11,17,2,3t1lca,"Notes on ""Speed learning on the fly"" -- ""learning the learning rate"" of SGD",https://www.reddit.com/r/MachineLearning/comments/3t1lca/notes_on_speed_learning_on_the_fly_learning_the/,mttd,1447695648,,4,6
426,2015-11-17,2015,11,17,3,3t1t9o,To all the wanna-be rappers out there... Raporythm is coming! - How an algorithm writes your rap lyrics!,https://www.reddit.com/r/MachineLearning/comments/3t1t9o/to_all_the_wannabe_rappers_out_there_raporythm_is/,Fezi22,1447698744,,0,0
427,2015-11-17,2015,11,17,3,3t1u8s,Adaptive Learning Rate Algorithms - Yoni Iny @ Upsolver (Eng),https://www.reddit.com/r/MachineLearning/comments/3t1u8s/adaptive_learning_rate_algorithms_yoni_iny/,mttd,1447699126,,0,1
428,2015-11-17,2015,11,17,4,3t1ypo,Generating Faces with Torch,https://www.reddit.com/r/MachineLearning/comments/3t1ypo/generating_faces_with_torch/,modeless,1447700851,,5,37
429,2015-11-17,2015,11,17,5,3t2cup,VIS+LSTM model for Visual Question Answering,https://www.reddit.com/r/MachineLearning/comments/3t2cup/vislstm_model_for_visual_question_answering/,abhshkdz,1447706296,,2,11
430,2015-11-17,2015,11,17,7,3t2s5a,Single Artificial Neuron Taught to Recognize Hundreds of Patterns,https://www.reddit.com/r/MachineLearning/comments/3t2s5a/single_artificial_neuron_taught_to_recognize/,cybrbeast,1447712372,,37,80
431,2015-11-17,2015,11,17,7,3t2ter,Jeremy Howard: Deep Learning AI Better Than Your Doctor at Finding Cancer,https://www.reddit.com/r/MachineLearning/comments/3t2ter/jeremy_howard_deep_learning_ai_better_than_your/,cybrbeast,1447712890,,2,0
432,2015-11-17,2015,11,17,9,3t3cn2,Variational inference in 5 minutes,https://www.reddit.com/r/MachineLearning/comments/3t3cn2/variational_inference_in_5_minutes/,davmre,1447720937,,9,37
433,2015-11-17,2015,11,17,10,3t3mrg,[1511.04834] Neural Programmer: Inducing Latent Programs with Gradient Descent,https://www.reddit.com/r/MachineLearning/comments/3t3mrg/151104834_neural_programmer_inducing_latent/,beamsearch,1447725376,,5,30
434,2015-11-17,2015,11,17,12,3t3vl8,Deep Kalman Filters,https://www.reddit.com/r/MachineLearning/comments/3t3vl8/deep_kalman_filters/,[deleted],1447729370,[deleted],1,1
435,2015-11-17,2015,11,17,13,3t4468,Tensorflow CLA: why no reciprocal license?,https://www.reddit.com/r/MachineLearning/comments/3t4468/tensorflow_cla_why_no_reciprocal_license/,jyegerlehner,1447733377,"Google has pledged to only uses patents in a defensive manner:

http://www.google.com/patents/opnpledge/pledge/

~~I observe that their CLA (to which one must agree before they will accept any pull requests into Tensorflow) requires one to give them a perpetual irrevocable license on any patents that one owns. They do not reciprocate and grant contributors any license.  Further, It occurs to me that if their intention was purely defensive, there wouldn't be any reason they would withhold a grant of a patent license to contributors. The only reason to withhold it is if they wish to retain the option of prosecuting their patents against others.~~

~~In other words, under the CLA, all their patents belong to Google and all your patents belong to Google.~~

~~The pledge says that they intend to prosecute their patents only if someone else goes after them first. But under the terms of the CLA, a contributor has already granted them a license. So why is there any reason not to grant a contributor a license? Will their pledge be legally binding if they later change their mind? Is a pledge posted on their website as good as a clause in a contract?~~

~~I don't want to be an ingrate and fail to recognize that they have made a wonderful tool available to us in open source fashion. But let's not forget they have deep learning related patents, and who knows what future patents they may obtain.  It seems there's a very real possibility of stupendously valuable advances in machine learning being contributed ultimately to Tensorflow source code base (from academia or other commercial entities) and Google having a monopoly on their commercial application.~~

Edit:
Oops. I now see in the Apache license it does explicitly grant a patent license. Sorry to waste everyone's time with a false alarm.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

At first I was thinking I should delete this post, but it's equally interesting that by using TensorFlow one is granted license to Google's patent estate (and that of other contributors). It makes sense that one has to agree to the CLA (before they can accept one's pull requests), otherwise they wouldn't be able to grant the patent license in the software license. TensorFlow is unique in this regard as far as I know, amongst ML libraries. ",4,3
436,2015-11-17,2015,11,17,18,3t4xi6,How Pneumatic Pumps Help Reduce the Operating Costs?,https://www.reddit.com/r/MachineLearning/comments/3t4xi6/how_pneumatic_pumps_help_reduce_the_operating/,jackerfrinandis,1447750933,,0,1
437,2015-11-17,2015,11,17,19,3t55i3,Telemonitoring of patients with Parkinsons disease,https://www.reddit.com/r/MachineLearning/comments/3t55i3/telemonitoring_of_patients_with_parkinsons_disease/,datapablo,1447756824,,0,0
438,2015-11-17,2015,11,17,20,3t5bsy,Many new machine learning libraries popping up what do you guys think?,https://www.reddit.com/r/MachineLearning/comments/3t5bsy/many_new_machine_learning_libraries_popping_up/,bbsome,1447761473,"So I'm a veteran Theano user, but was aware for a while now of both Torch and caffee as the main competitors as well as providing both similar performance and capabilities. However, recently it seems that after Tensor Flow came out there have been even more similar libraries popping up here and there. For instance CGT(http://rll.berkeley.edu/cgt/) ArrayFire-ML (https://github.com/arrayfire/arrayfire-ml) Leaf in Rust (https://github.com/autumnai/leaf) MXNet (https://github.com/dmlc/mxnet) , Chainer (http://chainer.org/) and probably a few more which I'm missing. I do appreciate the fact that people are trying to solve the problem in different ways, however I'm starting to think that this is starting to get a bit counter productive. Rather than people coming together to improve a lot a smaller subset of these, having so many of them would lead to potentially making it harder for people to easily port others solution. Everyone will know his own ""dialect"" and if you want to repeat someone else experiment you either have to learn they tool as well or recode everything in whatever you know. Additionally, as a PhD student I always care about performance, but there is not strict benchmark on different tasks, comparing compile and run times etc, etc and I just get the inner grudge that maybe my choice is suboptimal (I guess its coming a bit from my nature to want to use the best...). I just wandered what does other people think or am I alone in this opinion. ",11,5
439,2015-11-18,2015,11,18,0,3t5y9j,Why does word2vec generates good representative of possible factorizations?,https://www.reddit.com/r/MachineLearning/comments/3t5y9j/why_does_word2vec_generates_good_representative/,mihaild,1447773503,"It is known that word2vec with skip-gram model and negative sampling training just factorizes shifted PMI matrix (https://levyomer.files.wordpress.com/2014/09/neural-word-embeddings-as-implicit-matrix-factorization.pdf). So, word2vec finds two matrices, W and C, that WC ~ PMI - logK.

But, if WC is a factorization, then (W A)(A^-1 C), where A is arbitrary invertible matrix, is also a factorization with the same quality.

We then use rows of matrix W as vectors for words - for example, cosine similarity between this vectors is large for similar words and small for not similar (the same is true for PMI matrix). But multiplication by arbitrary matrix doesn't preserve this property in general.

The question is why from all possible factorizations, word2vec chooses one from the small class where cosine similarity of rows of W is similar to cosine similarity of rows of (shifted) PMI matrix?

(the same question is applicable to all of word2vec models and algorighms, as they all generates two matrices, but evaluates only their product)",5,3
440,2015-11-18,2015,11,18,0,3t5z0m,How to Train your Generative Models? And why does Adversarial Training work so well? (ICLR 2016 submission),https://www.reddit.com/r/MachineLearning/comments/3t5z0m/how_to_train_your_generative_models_and_why_does/,fhuszar,1447773830,,26,22
441,2015-11-18,2015,11,18,0,3t61ki,Would it be possible to teach an NN to read images out of my mind?,https://www.reddit.com/r/MachineLearning/comments/3t61ki/would_it_be_possible_to_teach_an_nn_to_read/,Yax42,1447774912,"So, I had this idea that I couldn't get rid of. I'm a total newbie so, pardon me if it's an absurd idea.

So first I need two things:

- A neural helmet to record with a decent precision the electromagnetic field that my brain generates

- A GoPro camera

 

Then, I stroll around for a couple of days while wearing them both.

The data I have after doing so are images associated with a representation of the state of my brain when I'm approximately seeing this image.

Then I design an NN that takes as input the state of my brain, and as output the image supposedly corresponding. And I run it.

And that's it. If everything goes well, I can now have a film of what I am currently seeing just from the record from the neural helmet. And with some random luck, I can also record and replay the images out my dreams...

Is it any close to being doable?",12,5
442,2015-11-18,2015,11,18,0,3t61v7,[Help] What are the prerequisites for Reinforcement Learning and what are some good resources to get started?,https://www.reddit.com/r/MachineLearning/comments/3t61v7/help_what_are_the_prerequisites_for_reinforcement/,metacurse,1447775033,"Hi Reddit,

I have been given a quite big research project and after some research, it's clear that Reinforcement Learning methods could offer a good solution to my problem. Unfortunately I can't reveal any details of the project itself. What are the things one has to know to get started with RL? I know multivariate calclus, and basic probability theory with good experience programming complex scientific models.

Also, what would be a good place to start learning about RL?

Thanks,",7,4
443,2015-11-18,2015,11,18,1,3t66ct,Popular Deep Learning Tools,https://www.reddit.com/r/MachineLearning/comments/3t66ct/popular_deep_learning_tools/,martinanalytics,1447776803,,0,1
444,2015-11-18,2015,11,18,1,3t69z6,"Why would a Data Scientist, looking for a job, not apply for this opportunity?",https://www.reddit.com/r/MachineLearning/comments/3t69z6/why_would_a_data_scientist_looking_for_a_job_not/,BlackSushi11,1447778208,,7,0
445,2015-11-18,2015,11,18,1,3t6ai6,Fancy Addressing for Neural Turing Machines (with code),https://www.reddit.com/r/MachineLearning/comments/3t6ai6/fancy_addressing_for_neural_turing_machines_with/,doctorteeth2,1447778402,,7,14
446,2015-11-18,2015,11,18,1,3t6dah,Tuesday = (Monday + Wednesday) / 2,https://www.reddit.com/r/MachineLearning/comments/3t6dah/tuesday_monday_wednesday_2/,AlanZucconi,1447779458,,33,385
447,2015-11-18,2015,11,18,2,3t6g66,How would I approach analyzing mice protein expression?,https://www.reddit.com/r/MachineLearning/comments/3t6g66/how_would_i_approach_analyzing_mice_protein/,[deleted],1447780571,[deleted],2,0
448,2015-11-18,2015,11,18,2,3t6l3a,TensorFlow Tutorial,https://www.reddit.com/r/MachineLearning/comments/3t6l3a/tensorflow_tutorial/,upulbandara,1447782450,,0,0
449,2015-11-18,2015,11,18,3,3t6o0o,indico's take on tensorflow,https://www.reddit.com/r/MachineLearning/comments/3t6o0o/indicos_take_on_tensorflow/,[deleted],1447783540,[deleted],0,1
450,2015-11-18,2015,11,18,3,3t6u64,Machine Learning and Technical Debt with D. Sculley,https://www.reddit.com/r/MachineLearning/comments/3t6u64/machine_learning_and_technical_debt_with_d_sculley/,softwaredaily,1447785827,,0,1
451,2015-11-18,2015,11,18,4,3t6xvq,Fully Convolutional Inference without subsampling in Caffe just using the python interface.,https://www.reddit.com/r/MachineLearning/comments/3t6xvq/fully_convolutional_inference_without_subsampling/,cesarsalgado,1447787157,"I want to use a trained convnet to predict labels for every pixel of an image. I have already seen this notebook example: http://nbviewer.ipython.org/github/BVLC/caffe/blob/master/examples/net_surgery.ipynb

The problem with this notebook example is that it doesn't generate an output for every window. It just gives an 8x8 prediction map instead of 451x451 prediction map.

I am also aware of FCN in the model zoo: https://github.com/longjon/caffe/tree/future

But I was looking for a simpler solution than to try to figure out how to use that branch. Besides, I don't want a code to train. I just want to do inference.

Is there a way to run a trained convnet densely in an image in a way faster than the naive approach*, ideally just using the python interface and just using the caffe master branch?

A related question: Is there any code to do fully convolutional inference without subsampling in other frameworks besides caffe? In theano, torch, chainer, tensorflow, brainstorm, for instance?

* The naive approach is to get every overlapping window in the image and copy them in the mini-batches and feed them to the CNN. Clearly there will be redundant computations doing this way.",3,3
452,2015-11-18,2015,11,18,4,3t71lp,Try out our real-time object detector!,https://www.reddit.com/r/MachineLearning/comments/3t71lp/try_out_our_realtime_object_detector/,pjreddie,1447788537,"We recently added webcam support to YOLO, our real-time object detection system, and we would love it if people wanted to try it out and give us their feedback!

Instructions to run the demo are here:

[http://pjreddie.com/darknet/yolo/#webcam](http://pjreddie.com/darknet/yolo/#webcam)

Installation should be easy, especially if you already have CUDA and OpenCV installed as many of you probably do. You will need a pretty good GPU (GTX 980+) to get real-time performance with the normal YOLO model.

Real-time object detection can be pretty fun and interactive, here are some videos we made of the same system where we pointed the webcam at YouTube videos and ran detection on them:

[https://www.youtube.com/watch?v=J3_aJI2S-n0](https://www.youtube.com/watch?v=J3_aJI2S-n0)

[https://www.youtube.com/watch?v=r6ZzopHEO1U](https://www.youtube.com/watch?v=r6ZzopHEO1U)

I hope some of you get the system running and can give us your feedback, let me know if you have any questions!",4,22
453,2015-11-18,2015,11,18,4,3t74cr,TensorFlow with cuda 7.5? (i.e. arch linux),https://www.reddit.com/r/MachineLearning/comments/3t74cr/tensorflow_with_cuda_75_ie_arch_linux/,spurious_recollectio,1447789578,"Has anyone had any luck getting tensorflow to work with cuda 7.5 -- the version on arch linux.  Obviously I mean the GPU version. Also any benchmarks on TensorFlow for RNNs (I've seen the CNN benchmarks).

Thanks",6,3
454,2015-11-18,2015,11,18,4,3t74r5,Dynd: Next generation multidimensional array library for pydata python,https://www.reddit.com/r/MachineLearning/comments/3t74r5/dynd_next_generation_multidimensional_array/,lakando,1447789725,,1,8
455,2015-11-18,2015,11,18,5,3t7dfs,Unifying distillation and privileged information [ICLR 2016 submission],https://www.reddit.com/r/MachineLearning/comments/3t7dfs/unifying_distillation_and_privileged_information/,[deleted],1447793033,,0,4
456,2015-11-18,2015,11,18,9,3t89zo,Best Optimizer for Text Seq2Seq Models,https://www.reddit.com/r/MachineLearning/comments/3t89zo/best_optimizer_for_text_seq2seq_models/,[deleted],1447806068,[deleted],0,1
457,2015-11-18,2015,11,18,9,3t8caa,Microsoft to world: We've got open source machine learning too,https://www.reddit.com/r/MachineLearning/comments/3t8caa/microsoft_to_world_weve_got_open_source_machine/,shugert,1447807057,,2,0
458,2015-11-18,2015,11,18,10,3t8hr6,"Smart Programs Read Shakespeare [audio, 8 min]",https://www.reddit.com/r/MachineLearning/comments/3t8hr6/smart_programs_read_shakespeare_audio_8_min/,fstorino,1447809490,,0,0
459,2015-11-18,2015,11,18,11,3t8phj,Highly abstract formulation of a classifier,https://www.reddit.com/r/MachineLearning/comments/3t8phj/highly_abstract_formulation_of_a_classifier/,[deleted],1447813054,[deleted],2,0
460,2015-11-18,2015,11,18,12,3t8vx1,How does one learn machine learning quickly?,https://www.reddit.com/r/MachineLearning/comments/3t8vx1/how_does_one_learn_machine_learning_quickly/,sleepicat,1447816107,"It seems that there are some core bits that one needs to know inside and out, and then there are a lot of superficial bits that are nice to know.  

What are the few core pieces that one should focus on to build a good foundational level of understanding of machine learning and be up-to-date with the technology of the last &lt;3 years?  

TL;DR: I'm looking for a map of the cat.",15,8
461,2015-11-18,2015,11,18,12,3t8w6v,neuronetworks that don't use backpropagation,https://www.reddit.com/r/MachineLearning/comments/3t8w6v/neuronetworks_that_dont_use_backpropagation/,Jxieeducation,1447816239,An example is Cortical Learning Algorithm or HTM. What are some other examples?,9,5
462,2015-11-18,2015,11,18,13,3t97fg,How Do you Think of TensorFlow?,https://www.reddit.com/r/MachineLearning/comments/3t97fg/how_do_you_think_of_tensorflow/,wxyyxc1992,1447821863,"Few days ago, Google presents tensorflow ,someone speak highly of it while someone are disappointed",2,0
463,2015-11-18,2015,11,18,17,3t9r0w,Artificial Intelligence? 97.4 Percent Of Computers Say They Still Us Need Humans,https://www.reddit.com/r/MachineLearning/comments/3t9r0w/artificial_intelligence_974_percent_of_computers/,john_philip,1447833866,,1,0
464,2015-11-18,2015,11,18,17,3t9thj,[1511.05392] Infinite Dimensional Word Embeddings,https://www.reddit.com/r/MachineLearning/comments/3t9thj/151105392_infinite_dimensional_word_embeddings/,iori42,1447835776,,2,31
465,2015-11-18,2015,11,18,18,3t9ywz,Why would a Data Scientist looking for a new job pass this opportunity?,https://www.reddit.com/r/MachineLearning/comments/3t9ywz/why_would_a_data_scientist_looking_for_a_new_job/,[deleted],1447840064,[deleted],2,0
466,2015-11-18,2015,11,18,19,3ta27b,TensorFlow Tutorial with notebooks and code examples,https://www.reddit.com/r/MachineLearning/comments/3ta27b/tensorflow_tutorial_with_notebooks_and_code/,dpmms,1447842523,,7,159
467,2015-11-18,2015,11,18,20,3ta9bg,Silly mistake I made today while looking at data,https://www.reddit.com/r/MachineLearning/comments/3ta9bg/silly_mistake_i_made_today_while_looking_at_data/,TangerineX,1447847492,"I had some fresh data today that I wanted to look at for a project of mine. Obvious first step is to PCA the data and then simply take a look at what you've got. 

I throw the data into Matlab, do a PCA and biplot and [bam, this pops out](http://imgur.com/gmrUTec).

I say to myself, ""damn, my data is totally separable! Machine learning this should be easy""

But I stopped and paused for a second. It can't be THIS easy. I looked back on my data and noticed that I had 7 dimmensions instead of 6. Turns out, column 1 was a timestamp.

""Hehe,"" I thought to myself. ""Rookie mistake"".

Now my data looks like [this](http://imgur.com/y1dcPpO) and the fun begins

Moral of this story is to always check what your columns are before you throw them into machine learning algorithms. ",20,32
468,2015-11-18,2015,11,18,23,3tana3,"Kaggle: The ML Social Hub. Jobs, Competitions, Knowledge, and More",https://www.reddit.com/r/MachineLearning/comments/3tana3/kaggle_the_ml_social_hub_jobs_competitions/,Synthint,1447855666,,0,0
469,2015-11-18,2015,11,18,23,3tanja,variable frequency drives,https://www.reddit.com/r/MachineLearning/comments/3tanja/variable_frequency_drives/,mashety,1447855786,,0,0
470,2015-11-18,2015,11,18,23,3taqzd,Learning the Architecture of Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3taqzd/learning_the_architecture_of_deep_neural_networks/,suurajsrinivas,1447857454,,3,7
471,2015-11-19,2015,11,19,0,3tav06,How was the OpenCV default People Detector trained ?,https://www.reddit.com/r/MachineLearning/comments/3tav06/how_was_the_opencv_default_people_detector_trained/,nex_jeb,1447859258,,1,5
472,2015-11-19,2015,11,19,0,3tb1tr,soft starters,https://www.reddit.com/r/MachineLearning/comments/3tb1tr/soft_starters/,mashety,1447862247,,0,0
473,2015-11-19,2015,11,19,1,3tb9qn,Neural Programmer: Inducing Latent Programs with Gradient Descent,https://www.reddit.com/r/MachineLearning/comments/3tb9qn/neural_programmer_inducing_latent_programs_with/,[deleted],1447865336,[deleted],0,1
474,2015-11-19,2015,11,19,4,3tbuhm,Shai Shalev-Shwartz and Shai Ben-David - Understanding Machine Learning: From Theory to Algorithms,https://www.reddit.com/r/MachineLearning/comments/3tbuhm/shai_shalevshwartz_and_shai_bendavid/,mtrn,1447873226,,5,12
475,2015-11-19,2015,11,19,4,3tc176,The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations,https://www.reddit.com/r/MachineLearning/comments/3tc176/the_goldilocks_principle_reading_childrens_books/,[deleted],1447875832,[deleted],0,7
476,2015-11-19,2015,11,19,5,3tc7ce,Methods for annotating features from other features?,https://www.reddit.com/r/MachineLearning/comments/3tc7ce/methods_for_annotating_features_from_other/,mmNinersmmm,1447878138,"Given a set of features measured across a set of samples (here gene expression in patient samples, more features than samples), what are the best methods to model each feature based on some unknown underlying signals in the data? My boss wants me to cluster the data, and then regress the cluster centroids against every feature in the dataset.  This intuitively feels very wrong to me, mostly because there is no good reason to assume that the cluster centroids represent important underlying processes vs mixtures of them.  ICA seems potentially appropriate, but I don't have enough samples for it to perform well.  Does latent dirichlet allocation make sense?  Does someone smarter than me have a good idea, or at least the right place to look in the literature?",5,1
477,2015-11-19,2015,11,19,6,3tche7,Using pigeons to do medical image classification: Pigeons (Columba livia) as Trainable Observers of Pathology and Radiology Breast Cancer Images,https://www.reddit.com/r/MachineLearning/comments/3tche7/using_pigeons_to_do_medical_image_classification/,iownaredball,1447881977,,3,13
478,2015-11-19,2015,11,19,8,3tcxs6,"Something does not work for me, I get ""No module named tensorflow"" after calling ""python"" and ""import tensorflow as tf"".",https://www.reddit.com/r/MachineLearning/comments/3tcxs6/something_does_not_work_for_me_i_get_no_module/,DMassenet,1447888525,"This topic has been solved but yet, it doesn't work for me.
I've got Mac OS X10.10.",11,0
479,2015-11-19,2015,11,19,8,3tcyie,Trying to find clustered points in an image,https://www.reddit.com/r/MachineLearning/comments/3tcyie/trying_to_find_clustered_points_in_an_image/,JoseMendez22,1447888819,"Hey! I'm trying to do some complex analysis on this image: 

https://www.dropbox.com/s/izwhj1s3qbqj9uc/Screenshot%202015-11-18%2018.02.30.png?dl=0

I've already got it doing pretty much what I want, which is detecting the circular answer bubbles: https://www.dropbox.com/s/gvtmxmx1img64k7/Screenshot%202015-11-18%2018.02.48.png?dl=0

This is done with OpenCV (actually, JavaCV which is the java native binding to OpenCV).

Now, I'm trying to find each of the groups of four questions, and I thought the best way to do that would be to perform basic machine learning (clustering) on the centers of the circles (instead of involving any more computer vision stuff).

https://commons.apache.org/proper/commons-math/userguide/ml.html

I'm using this library for my clustering math (because I'm awful at math) and I cannot get any productive results out of any of the methods.

What would be an appropriate method of finding the groups of four bubbles, given the center of every bubble in the image? Note, that I wanted to make it so that the bubbles could be along the Y axis, which would make a clustering algorithm likely the best choice)

Here's my code https://www.dropbox.com/s/mhzxoaprhbpp30x/Screenshot%202015-11-18%2018.18.03.png?dl=0

I chose a DBSCANClusterer because it gave the closest thing to useful results. This is what I get https://www.dropbox.com/s/4v9j1gqcoxfll8z/Screenshot%202015-11-18%2018.18.51.png?dl=0

As I change the input variables (which are specified in the DBSCANClusterer constructor, and are eps and minPts respectively) I either get an insane amount of clustering in all directions, one large box, or no clusters detected. Nothing seems to hit a perfect balance.

Would appreciate any help :)",0,1
480,2015-11-19,2015,11,19,8,3tczeh,Stream Processing with Apache Flink,https://www.reddit.com/r/MachineLearning/comments/3tczeh/stream_processing_with_apache_flink/,[deleted],1447889195,[deleted],0,1
481,2015-11-19,2015,11,19,8,3tczfj,Wikitract - crowdsourced wikipedia for papers/patents (e.g. deep learning),https://www.reddit.com/r/MachineLearning/comments/3tczfj/wikitract_crowdsourced_wikipedia_for/,[deleted],1447889207,[deleted],0,1
482,2015-11-19,2015,11,19,8,3td04a,Music Generation Using Stacked Denoising Autoencoder and LSTM model in Keras,https://www.reddit.com/r/MachineLearning/comments/3td04a/music_generation_using_stacked_denoising/,ThatWillNeverShake,1447889505,"I was able to generate music by training a NN model over Joanna Newsom's song ""Sapokanikan"" (https://www.youtube.com/watch?v=ky9Ro9pP2gc). A 45 second sample from the song is overlappingly sliced into 2000 samples and then FFT'ed (essentially STFT without window), and use the frequency phase and magnitude as input. This means we have K * 2000 dimension data, where K = N/2000*L (N is the total number of samples, and L is overlap size). Then a 14 layer Denoising Autoencoder is trained with softplus activation for the encoder layers and linear activation for the decoder layers. Each Autoencoder layer reduces the dimension by 200, which means the resulting encoded data has ~600 dimension. I then train the LSTM model with non-overlapping (i.e. discarding data points that overlap with others) encoded data (refer to Aran Nayebi and Matt Vitelli LSTM model for training and generation https://github.com/MattVitelli/GRUV). To generate new data, I feed the encoded data into the LSTM, save the last predicted data, pad the new data onto the data point and remove the first data point, and feed it back to the model and repeat. After LSTM is done generating the new data, I feed that data onto the Autoencoder decoder layers.

Results:

Generative model with LSTM sequence length of 75 (This generation might have been overfitted?):


https://youtu.be/RXbEhvT9-Ws

Generative model with LSTM sequence length of 150:

https://youtu.be/L7pqnlT3v-I


Generative model with LSTM sequence length of 300:

https://youtu.be/OvHN-QUgQho


Generative model with LSTM sequence length of 600:

https://youtu.be/QTW7ixuBJVs


Generative model with FFT Autoencoder (i.e. non-overlapping), LSTM sequence length of 150:

https://youtu.be/0_l00Z5xXeo


(If you really think about it, it actually make sense why it's better to do STFT: the more samples the decoder has, the more variety the LSTM can produce and be decoded properly. However, if the overlapping size is too big, the Autoencoder's lower bound loss would also increase, decreasing the end fidelity of the music).


It should be pointed out that this is still far from perfect. The reason why the sample dimension is soo small is because i'm using a consumer level GPU (geForce 750 Ti 2GB). However, I was able to shorten the training time significantly by using Autoencoder. Also, Autoencoder reduces overfitting (I think?). Another problem is the clicking noise that arises from STFT process. I tried windowing the STFT function but it significantly impairs Autoencoder's reconstruction process.

I'll upload the code to github soon if ya'll want it. I used a slightly modified version of Keras.",14,24
483,2015-11-19,2015,11,19,10,3tdcbs,[help] L1 Regularization,https://www.reddit.com/r/MachineLearning/comments/3tdcbs/help_l1_regularization/,Kiuhnm,1447894832,"Hi, I'm reading [this](http://goodfeli.github.io/dlbook/contents/regularization.html) and I don't understand where the two formulas after (7.13) on page 208 come from.

Any help would be greatly appreciated!",3,2
484,2015-11-19,2015,11,19,10,3tdd3r,Implementation of algorithms,https://www.reddit.com/r/MachineLearning/comments/3tdd3r/implementation_of_algorithms/,newtonlp,1447895174,"I'm a physics graduate and I've gone through the entire Shalev-Shwartz textbook and solved most of the problems in that. I am, however, barely experienced when it comes to writing code or implementing an algorithm for which I've seen the pseudocode. What is the best way for me to learn to write my own Naive Bayes classifier or SVM?",7,4
485,2015-11-19,2015,11,19,13,3te11p,[1511.05641] Net2Net: Accelerating Learning via Knowledge Transfer,https://www.reddit.com/r/MachineLearning/comments/3te11p/151105641_net2net_accelerating_learning_via/,antinucleon,1447906283,,18,33
486,2015-11-19,2015,11,19,14,3te9vm,Understanding Machine Learning: From Theory to Algorithms,https://www.reddit.com/r/MachineLearning/comments/3te9vm/understanding_machine_learning_from_theory_to/,pradeep_sinngh,1447910782,,1,19
487,2015-11-19,2015,11,19,14,3tecxy,Help with my machine learning paper?,https://www.reddit.com/r/MachineLearning/comments/3tecxy/help_with_my_machine_learning_paper/,[deleted],1447912435,[deleted],0,0
488,2015-11-19,2015,11,19,15,3tee57,Picking the right algorithm for this job?,https://www.reddit.com/r/MachineLearning/comments/3tee57/picking_the_right_algorithm_for_this_job/,Gay_Hat_On_Nun,1447913085,"Hello, I have a dataset with approximately 1000 instances and 70 attributes each. The attributes are all decimal numbers between 0 and 6, and I would like to create a predictive model where given some of the attributes, the rest can be determined. (i.e. I input an example with 30 attributes, and the remaining 40 are predicted, along with a percent accuracy) I would like to have it so a varying number of attributes can be inputted, so instead of feeding 30 attributes to the algorithm, I could perhaps instead feed 50 and have the algorithm only predict the remaining 20, most likely resulting in a higher accuracy. What algorithm/approach would best fit this? (sorry if this is a stupid question, I'm a noob)",11,0
489,2015-11-19,2015,11,19,15,3teidu,"What sort of research is one expected to do, before grad school, for someone with low gpa ?",https://www.reddit.com/r/MachineLearning/comments/3teidu/what_sort_of_research_is_one_expected_to_do/,deanSheep,1447915577,"Hi, I want to take a masters specializing in ML. This [post](https://www.reddit.com/r/gradadmissions/comments/25gmig/a_somewhat_notgentle_guide_to_getting_into_grad/) in /r/gradadmissions say that relevant research is very valuable for getting an admission. I would like to get into a university which is strong in ML. 

This being the case I have low gpa and no publications. I have a bachelors in CS. I'm currently working and my work has nothing to with ML, but I have the luxury that there is not much work. So I have ample time both in office and home. I'm taking Andrew Ng's excellent machine learning course and have also taken Trevor Hastie &amp; Rob Tibshirani's ISLR course. 
But I'm not sure if I have time to publish a paper. I'm currently targeting for 2017 fall, so I have about a year.

How can I best use my time to improve my profile, and my knowledge for this subject ? Any help would be much appreciated.

Edit 1: I'm also passionate about open source, will me contributing to established projects or my own personal projects be seen as 'research' or does research have to culminate with a paper ?

Edit 2: It would also help if someone can tell me the workings of a research group, seeing that many here have interacted or been in one. I would like to know 
1) if someone can collaborate over internet and hence not physically be there ?
2) if someone can continue working their day job and collaborate for the research group during their free time ? 

in short work similarly to an open source project",9,9
490,2015-11-19,2015,11,19,16,3tejsp,Where does India stand as machines become intelligent?,https://www.reddit.com/r/MachineLearning/comments/3tejsp/where_does_india_stand_as_machines_become/,varaggarwal,1447916401,,4,0
491,2015-11-19,2015,11,19,16,3tels0,[1502.02791] Learning Transferable Features with Deep Adaptation Networks,https://www.reddit.com/r/MachineLearning/comments/3tels0/150202791_learning_transferable_features_with/,downtownslim,1447917657,,0,4
492,2015-11-19,2015,11,19,17,3tetcq,Advice by a Data Scientist to Your Younger Self,https://www.reddit.com/r/MachineLearning/comments/3tetcq/advice_by_a_data_scientist_to_your_younger_self/,john_philip,1447923102,"Hi
If you are a data scientist, what is that one piece of advice or words of wisdom that you would like to give to your younger self ? 
P.S - Advice should be related to data science only",11,7
493,2015-11-19,2015,11,19,18,3teuu0,Clustering of images using CNN - review this approach?,https://www.reddit.com/r/MachineLearning/comments/3teuu0/clustering_of_images_using_cnn_review_this/,akshayxyz,1447924205,"Roughly - I am considering using a deep CNN based autoencoder to reduce the dimensions of images from a given catalog (that is they are not as randomized as Imagnet stuff), and then use KNN or some clustering method on these features (or augmented with text features) to cluster the similar looking images.

Questions:

a) Has any one tried something like this. I could not find some prior work on google scholar. To be honest, I have spent only few hours with limited combinations of the keywords.

b) How do I ensure that the learnt features capture the relevant information to differentiate based on shapes.

c) If this setup looks like working, I am going to use it on different set of images, each set having some different number of 'clusters'..e.g. (apples, oranges), (shirt, pants, shoes). How do I figure out # of clusters for each set? I am assuming something like - plotting some internal metric, or using something like mean-shift clustering.

d) Any suggestions are welcome.


Edit:

1) Thanks for the responses, will update with the progress/results.
",10,1
494,2015-11-19,2015,11,19,20,3tf4l8,An article about Item Based Collaborative Filtering Recommender Systems in R,https://www.reddit.com/r/MachineLearning/comments/3tf4l8/an_article_about_item_based_collaborative/,padmajatamada,1447931332,,0,1
495,2015-11-19,2015,11,19,21,3tfa0e,EIT Digital Master School for ML ?,https://www.reddit.com/r/MachineLearning/comments/3tfa0e/eit_digital_master_school_for_ml/,YanisLupmc,1447935080,"Hello,

I'm a french student with a double major in maths and mechanics and I would like to apply for a master or a school in ML, i found two masters (well, in fact others but I have questions for these two masters) : 


http://www.masterschool.eitdigital.eu/programmes/hcid/

http://www.masterschool.eitdigital.eu/programmes/dsc/

The thing is, I did not find informations on linkedin, reddit or else about the recognition from industry etc..But the universities are good...

So, is it a trap ? ",2,0
496,2015-11-19,2015,11,19,21,3tfaci,Skflow: scikit interface to tensorflow,https://www.reddit.com/r/MachineLearning/comments/3tfaci/skflow_scikit_interface_to_tensorflow/,gwulfs,1447935301,,32,151
497,2015-11-19,2015,11,19,21,3tfcbm,"The ""Compression Ratio"" of a Neural Net",https://www.reddit.com/r/MachineLearning/comments/3tfcbm/the_compression_ratio_of_a_neural_net/,kcimc,1447936525,"What would you see if you plotted every neural net used for MNIST as a point on two axes: test error % and number of parameters?

The extrema are clear: with very few parameters the error is very high, and with enough parameters the error is zero. But I imagine that there's some point where adding more parameters stops doing as much to help minimize the error. That there's some kind of ""sweet spot"" given the training techniques and kinds of architectures we know now that accomplishes the most for the given number of parameters.

I had this thought while reading about compression ratios and looking at some leaderboards for MNIST results:

http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354
http://yann.lecun.com/exdb/mnist/

Has anyone seen a plot comparing architectures this way, or is there other research that elaborates on this topic in more detail?",3,2
498,2015-11-19,2015,11,19,22,3tfij0,Google Offers Free Software in Bid to Gain Edge in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3tfij0/google_offers_free_software_in_bid_to_gain_edge/,stormforce7916,1447940007,,0,0
499,2015-11-19,2015,11,19,23,3tflwn,Any API that will give me related terms?,https://www.reddit.com/r/MachineLearning/comments/3tflwn/any_api_that_will_give_me_related_terms/,backpackerzee,1447941679,"I am looking for a tool/api which can give me related terms like ""Bruce Wayne"", ""Gotham"", ""DC Comics"", ""Superman"", ""Comics"" if the search query is ""Batman""

",5,0
500,2015-11-19,2015,11,19,23,3tfmyj,How to solve this ANN question faster in the exam?,https://www.reddit.com/r/MachineLearning/comments/3tfmyj/how_to_solve_this_ann_question_faster_in_the_exam/,machinegaze,1447942164,"Link : http://imgur.com/jIw5oML

We are given only a non-programmable calculator. Calculating the dot product, putting in the logistic function and then calculating the weight updates 10 times, takes a LOT of time. Is there any way to do this faster?",9,1
501,2015-11-20,2015,11,20,1,3tg5o0,"Video: Machine learning with a data-unfriendly stack (a presentation from Wrangle Conference by Michael Manapat, Stripe)",https://www.reddit.com/r/MachineLearning/comments/3tg5o0/video_machine_learning_with_a_dataunfriendly/,robdoo,1447950123,,0,3
502,2015-11-20,2015,11,20,2,3tgkeo,Paris Attacks: A reminder of A.I. in War,https://www.reddit.com/r/MachineLearning/comments/3tgkeo/paris_attacks_a_reminder_of_ai_in_war/,[deleted],1447955820,[deleted],1,0
503,2015-11-20,2015,11,20,3,3tgmtb,"Machine learning with Python meta-tutorial, start to finish",https://www.reddit.com/r/MachineLearning/comments/3tgmtb/machine_learning_with_python_metatutorial_start/,mmmayo13,1447956735,,1,2
504,2015-11-20,2015,11,20,4,3tgy9b,What are some interesting German universities that offer (in English) Machine Learning Masters?,https://www.reddit.com/r/MachineLearning/comments/3tgy9b/what_are_some_interesting_german_universities/,swentso,1447960980,"Since Study is free even for foreigners, I'm thinking about enrolling in a Master in Machine Learning in Germany. Do you guys know any good university that offers that in English?",15,0
505,2015-11-20,2015,11,20,6,3thfgz,NIPS,https://www.reddit.com/r/MachineLearning/comments/3thfgz/nips/,LyExpo,1447967522,"I will be attending NIPS for the first time in December and am a bit confused as to how it works. The scheduling looks very different from other conferences I have been to (not that many). For example, many events seem to overlap.

Could someone give me some guidlines as to how this generally works? Tutorials? Symposiums? Workshops? Spotlights? Ahhhh!

I'm also curious to know what other people are most looking forward to, since it seems I will not be able to attend everything.",13,16
506,2015-11-20,2015,11,20,7,3thpvj,LSTM Classifying all the words as the same class,https://www.reddit.com/r/MachineLearning/comments/3thpvj/lstm_classifying_all_the_words_as_the_same_class/,lucas_0,1447971713,"I've used Lasagne to build a LSTM model to classify words with the IOB-tags. About 25-40% of the training words classes is O, thus receiving the same int32 class number 126.

The words go through a context window method, in order to increase the number of features, and be influenced by the neighboring words.

After that, the words(with their context window) go through a word embedding process, before being fed to the model.

At the first training words, my model classify the words with different classes, then it starts classifying a lot of words with the same class: 

    [ 54   9 119  41  77   1   1  96  96  84  84  96  96  96  96  45  74  34   34  34  34  34  34  34  34  34  34  34  34  34  34  34  34  34  34  34  34  34  34  34  34  34  34  34  34  34]

    [ 54  85   7 119  22   7 115  84  62  62  71  71  71  71  71  71  71  71  71  71  71  71  71  71  71  71  71  71  71  71  71  71  71  71  71  71  71  71  71  71  71  71  71  71  71  71]

    [ 85   1  83 113  13  36  82  58 126   2   2  17  19 117  25  25  25  25  25  25  25  25  25  25  25  25  25  25  25  25  25  25  25  25  25  25  25  25  25  25  25  25  25  25  25  25]

After some training, it starts classifying every word as 126, the index of the O class.

It looks as a hyper-paramether configuration problem, but I don't have a clue of how to fix it. I've tried grad_clipping, increasing units on the lstm layer, adding a second layer, learning_rate and momentum, but the results are the same. Can someone give me a hint? Thank you.",4,1
507,2015-11-20,2015,11,20,7,3thrmb,MNIST equivalent for seq2seq,https://www.reddit.com/r/MachineLearning/comments/3thrmb/mnist_equivalent_for_seq2seq/,n_mca,1447972426,"Hey, I'm keen to implement seq2seq myself to make sure I understand it properly. Can anyone recommend a dataset (or how to generate a dataset) that will be easily learnt by a small LSTM seq2seq model so I can train locally on my laptop?",3,8
508,2015-11-20,2015,11,20,7,3thtoy,Potential reason for why Deepmind uses Torch?,https://www.reddit.com/r/MachineLearning/comments/3thtoy/potential_reason_for_why_deepmind_uses_torch/,cerberusd,1447973315,"Like many others, it caught my attention that Deepmind is using Torch even though it had access to Tensorflow. 

Now this is all conjecture, but it occurred to me that a possible explanation is that when Google and Deepmind were in talks for acquisition, maybe Deepmind insisted that they be able to publish their code to the public. From interviews of Demis Hassabis he makes it very clear that he would like to steer the direction of Deepmind is to scientific research and publication. Maybe at the time (2014) using time would have made this agreement difficult for both sides.

Does this make sense or no? Is there another reason why Deepmind wouldn't use Tensorflow?",6,1
509,2015-11-20,2015,11,20,7,3thuws,"Me, My Family &amp; Jibo!!!!",https://www.reddit.com/r/MachineLearning/comments/3thuws/me_my_family_jibo/,sethibharat,1447973850,,1,0
510,2015-11-20,2015,11,20,8,3thvvi,"Me, My Family and Jibo!!!!! - my new robot friend!",https://www.reddit.com/r/MachineLearning/comments/3thvvi/me_my_family_and_jibo_my_new_robot_friend/,Fezi22,1447974279,,0,0
511,2015-11-20,2015,11,20,8,3ti0fp,Loss function: must it be convex?,https://www.reddit.com/r/MachineLearning/comments/3ti0fp/loss_function_must_it_be_convex/,despardesi,1447976295,"Does the loss function have to be convex (or differentiable) for learning techniques like GD or SGD to work?

Here's a hypothetical scenario: suppose you are trying to predict the winner of NFL games. Sometimes, teams win by the slimmest of margins (or due to [officiating screwups](http://www.nfl.com/news/story/0ap3000000581718/article/nfl-officiating-error-cost-ravens-on-final-play-vs-jags) ). An error by your model in such a situation is not the same as an error when there's a blowout.

You want to train a binary classifier, say, using logistic regression. But for your loss function, instead of making it a simple function of y (actual outcome, 0/1) and p (predicted, 0/1 again), you throw in the margin of victory too: so, if y == 1 and p =~ 0, then the loss won't be 1 (as current systems do it), but a function of the margin of victory too: so, the loss may be 1 when margin &gt; 21, and the loss could be, say, ~0.5 when the team loses by &lt; 3 points. ",10,1
512,2015-11-20,2015,11,20,12,3titrm,Linear capping machine LCM 2000,https://www.reddit.com/r/MachineLearning/comments/3titrm/linear_capping_machine_lcm_2000/,dongfengpacking,1447989929,,1,1
513,2015-11-20,2015,11,20,12,3tivdv,NeuralTalk2 RNN Image Captioning code in Torch,https://www.reddit.com/r/MachineLearning/comments/3tivdv/neuraltalk2_rnn_image_captioning_code_in_torch/,clbam8,1447990716,,1,15
514,2015-11-20,2015,11,20,13,3tj204,A TON of new interesting papers today (because of the ICLR deadline),https://www.reddit.com/r/MachineLearning/comments/3tj204/a_ton_of_new_interesting_papers_today_because_of/,[deleted],1447994133,[deleted],0,1
515,2015-11-20,2015,11,20,13,3tj2te,ALAN Robot kit-when your machine can literally tell you what it has learned.,https://www.reddit.com/r/MachineLearning/comments/3tj2te/alan_robot_kitwhen_your_machine_can_literally/,joyrperez,1447994575,,0,0
516,2015-11-20,2015,11,20,13,3tj3kq,Linear separability,https://www.reddit.com/r/MachineLearning/comments/3tj3kq/linear_separability/,marsyred,1447994979,,7,75
517,2015-11-20,2015,11,20,14,3tj5sn,Help me understand the difference between Neural Turing Machines and LSTMs,https://www.reddit.com/r/MachineLearning/comments/3tj5sn/help_me_understand_the_difference_between_neural/,dil8,1447996195,Are these two concepts of long term dependence and coupling a NN to an external memory somewhat related?  Any help or resources appreciated.,4,5
518,2015-11-20,2015,11,20,14,3tj5so,Awesome-RL. A curated list of resources dedicated to reinforcement learning.,https://www.reddit.com/r/MachineLearning/comments/3tj5so/awesomerl_a_curated_list_of_resources_dedicated/,hsk90,1447996196,,4,33
519,2015-11-20,2015,11,20,15,3tje9c,Machine learning a super opportunity for India,https://www.reddit.com/r/MachineLearning/comments/3tje9c/machine_learning_a_super_opportunity_for_india/,varaggarwal,1448001407,[removed],2,0
520,2015-11-20,2015,11,20,16,3tjgm0,Net2Net: Accelerating Learning via Knowledge Transfer (paper+code),https://www.reddit.com/r/MachineLearning/comments/3tjgm0/net2net_accelerating_learning_via_knowledge/,[deleted],1448002912,[deleted],0,1
521,2015-11-20,2015,11,20,16,3tjhg0,[1511.06068] Reducing Overfitting in Deep Networks by Decorrelating Representations,https://www.reddit.com/r/MachineLearning/comments/3tjhg0/151106068_reducing_overfitting_in_deep_networks/,iori42,1448003510,,13,17
522,2015-11-20,2015,11,20,16,3tjhof,[1511.04586] Character-based Neural Machine Translation,https://www.reddit.com/r/MachineLearning/comments/3tjhof/151104586_characterbased_neural_machine/,iori42,1448003696,,0,15
523,2015-11-20,2015,11,20,18,3tjrvn,Machine Learning methods used in a major breakthrough in Nutrition Science,https://www.reddit.com/r/MachineLearning/comments/3tjrvn/machine_learning_methods_used_in_a_major/,NovaRom,1448011703,,3,19
524,2015-11-20,2015,11,20,18,3tjtoy,Deep Learning on Disassembly [pdf],https://www.reddit.com/r/MachineLearning/comments/3tjtoy/deep_learning_on_disassembly_pdf/,galapag0,1448013146,,3,3
525,2015-11-20,2015,11,20,19,3tjup9,"Demis Hassabis alludes to ""Quite a big surprise"" from Google DeepMind in the coming months regarding the board game Go",https://www.reddit.com/r/MachineLearning/comments/3tjup9/demis_hassabis_alludes_to_quite_a_big_surprise/,[deleted],1448013913,[deleted],0,1
526,2015-11-20,2015,11,20,20,3tk3or,"Demis Hassabis alludes to ""Quite a big surprise"" from Google DeepMind in the coming months regarding the board game Go",https://www.reddit.com/r/MachineLearning/comments/3tk3or/demis_hassabis_alludes_to_quite_a_big_surprise/,Buck-Nasty,1448020716,,45,34
527,2015-11-20,2015,11,20,21,3tk82e,When you interview a data scientist ...,https://www.reddit.com/r/MachineLearning/comments/3tk82e/when_you_interview_a_data_scientist/,HappyDataScientist,1448023745,,0,1
528,2015-11-20,2015,11,20,22,3tk9cz,Multitask Sequence to Sequence Learning,https://www.reddit.com/r/MachineLearning/comments/3tk9cz/multitask_sequence_to_sequence_learning/,personanongrata,1448024537,,3,11
529,2015-11-20,2015,11,20,22,3tkehf,Fine-tune VGG or AlexNet for non-square inputs,https://www.reddit.com/r/MachineLearning/comments/3tkehf/finetune_vgg_or_alexnet_for_nonsquare_inputs/,Registerml,1448027517,"VGG and AlexNet, amongst others, require a fixed image input of square dimensions (H == W). How can one fine-tune or otherwise perform net surgery such that non-square inputs can be provided? 

For your reference, I'm using Caffe and intend to extract FC7 features for non-square image inputs.",3,0
530,2015-11-21,2015,11,21,0,3tkrjh,Can auto-encoders really learn the identity map?,https://www.reddit.com/r/MachineLearning/comments/3tkrjh/can_autoencoders_really_learn_the_identity_map/,XalosXandrez,1448033715,"A friend was playing around with auto-encoders some time ago. He reported that he couldn't get it to learn the identity map. While it is possible that he didn't ""do it properly"", I wanted to ask whether someone has indeed seen a weight matrix being approximately identity. 

I'm aware of denoising autoencoders and the motivation behind them. Even here, an identity map could be learnt in principle, with the model attributing noise to just ""noise in the data"". In fact, this would be simplest explanation. (Occam's razor, anyone?)

The [Neural Turing Machine](http://arxiv.org/abs/1410.5401) paper reports that LSTMs can't learn the copy operation. Are the two observations related? Or am I missing something fundamental?",7,0
531,2015-11-21,2015,11,21,0,3tktgf,Laptop with a Graphics Card?,https://www.reddit.com/r/MachineLearning/comments/3tktgf/laptop_with_a_graphics_card/,badhri,1448034557,"Hi. I'm a Computer Science student and I'm going to study Machine Learning next semester. I plan to follow it up with Deep Learning (and other stuff beyond the curriculum). I have to buy a new laptop and I was wondering whether it is a must to buy one with a decent graphics card(purely from ML perspective, I do not play games).

If yes, then which one(graphics card) should I choose?(Company that makes them, Capacity, Model, anything).

Thanks in advance.

P.S. I know this is kind of a noob question, but my google searches did not provide a satisfactory answer and I thought that you guys could help me out.",32,8
532,2015-11-21,2015,11,21,2,3tl6ml,Recursively copying elements from one Graph to another in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/3tl6ml/recursively_copying_elements_from_one_graph_to/,sachinrjoglekar,1448040004,,0,0
533,2015-11-21,2015,11,21,3,3tlekh,Loading a TensorFlow graph with the C++ API,https://www.reddit.com/r/MachineLearning/comments/3tlekh/loading_a_tensorflow_graph_with_the_c_api/,jimfleming,1448043234,,0,11
534,2015-11-21,2015,11,21,4,3tlrmk,Deep Learning on AWS -- anyone trying it?,https://www.reddit.com/r/MachineLearning/comments/3tlrmk/deep_learning_on_aws_anyone_trying_it/,IIIIIBarea,1448048545,"Does anyone have experience running theano/torch etc on an amazon GPU node, and have a good estimate of the cost and difficulty?  I'm having a hell of a time using our out of date cluster, and thinking about paying out of pocket to get some work done. Also, anyone know how difficult/expensive it is to upload large datasets to AWS?",28,64
535,2015-11-21,2015,11,21,5,3tlyh9,Finding the True Error Rate in Seq2Seq Text Generation,https://www.reddit.com/r/MachineLearning/comments/3tlyh9/finding_the_true_error_rate_in_seq2seq_text/,LeavesBreathe,1448051301,"Hey Guys,

So lately, I've been trying to write the next sentence given the previous sentence using seq2seq.

I was using my validation loss to compare models, but I realize that this probably not a good way, especially if you're using different optimizers, so I thought I'd ask you guys:

**In Text Generation, what is the best way to measure your error?**

Here are a few ideas I had:

1. Use Google's Doc2Vec to Vectorize your output sentence. You then compare your generated sentence's vector with the target sentence's vector. Your aim is to minimize the cosine distance between the two. (Do a Root Mean Square)

2. Compare the percentage of words that similar between target sentence and generated sentence. 

3. I've been doing some reading about adversarial models, but I'm not quite sure how they would fit in here.

The main problem with these ideas is that your generated sentence could be a correct, logical one (which you want!), but it is not the target sentence. Thoughts on this?",10,3
536,2015-11-21,2015,11,21,5,3tm2kr,"""Exploratory Analysis: Why Do We Need Particular Caution?"" For those of you interested in biomedical applications, this is talk absolutely crucial",https://www.reddit.com/r/MachineLearning/comments/3tm2kr/exploratory_analysis_why_do_we_need_particular/,gabjuasfijwee,1448052967,,1,1
537,2015-11-21,2015,11,21,5,3tm2sr,Help interpreting feature importance values?,https://www.reddit.com/r/MachineLearning/comments/3tm2sr/help_interpreting_feature_importance_values/,souldeux,1448053050,"I am a noob when it comes to machine learning, and I'm having trouble interpreting some of the results I'm getting from my first program. Here's the setup:

I have a dataset of book reviews. These books can be tagged with any number of qualifiers from a set of about 1600. The people reviewing these books can also tag themselves with these qualifiers to indicate that they like to read things with that tag. 

The dataset has a column for each qualifier. For every review, if a given qualifier is used to tag both the book and the reviewer a value of 1 is recorded. If there is not a ""match"" for a given qualifier on a given review, a value of 0 is recorded. 

There is also a ""Score"" column, which holds an integer 1-5 for each review (the ""star rating"" of that review). My goal is to determine what features are most important to getting a high score.

[Here's the code I have right now.](https://gist.github.com/souldeux/99f71087c712c48e50b7) I am successfully generating a plot, but I am honestly not sure what the plot means. As I understand it, this is showing me how strongly any given feature impacts the score variable. But, and I realize this must be a stupid question, how do I know if the impact is positive or negative? 
",1,0
538,2015-11-21,2015,11,21,7,3tmc1m,"Geoffrey Hinton: ""Does the Brain do Inverse Graphics?""",https://www.reddit.com/r/MachineLearning/comments/3tmc1m/geoffrey_hinton_does_the_brain_do_inverse_graphics/,_spreadit,1448057017,,2,19
539,2015-11-21,2015,11,21,8,3tmmpx,/r/evolutionarycomp: The Subreddit for Evolutionary Computation: The Other Side of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3tmmpx/revolutionarycomp_the_subreddit_for_evolutionary/,Synthint,1448061654,,0,0
540,2015-11-21,2015,11,21,8,3tmqd6,An Introduction to K-means Cluster Analysis,https://www.reddit.com/r/MachineLearning/comments/3tmqd6/an_introduction_to_kmeans_cluster_analysis/,dnabeyta,1448063242,,1,0
541,2015-11-21,2015,11,21,8,3tmrlk,[1511.06314] Why M Heads are Better than One: Training a Diverse Ensemble of Deep Networks,https://www.reddit.com/r/MachineLearning/comments/3tmrlk/151106314_why_m_heads_are_better_than_one/,[deleted],1448063808,[deleted],0,2
542,2015-11-21,2015,11,21,9,3tmtd1,Build a Recommendation Engine With Deeplearning4j,https://www.reddit.com/r/MachineLearning/comments/3tmtd1/build_a_recommendation_engine_with_deeplearning4j/,vonnik,1448064632,,0,0
543,2015-11-21,2015,11,21,9,3tmxy2,Short explanation of word2vec,https://www.reddit.com/r/MachineLearning/comments/3tmxy2/short_explanation_of_word2vec/,hduongtrong,1448066796,,1,8
544,2015-11-21,2015,11,21,9,3tmytu,Open discussion of #ICLR2016 submissions is now open,https://www.reddit.com/r/MachineLearning/comments/3tmytu/open_discussion_of_iclr2016_submissions_is_now/,evc123,1448067258,,5,6
545,2015-11-21,2015,11,21,10,3tn4k8,Staleness-aware Async-SGD for Distributed Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3tn4k8/stalenessaware_asyncsgd_for_distributed_deep/,virtualpotential,1448070231,,0,4
546,2015-11-21,2015,11,21,10,3tn4vd,2016 Conference Deadlines are Passed or Coming Quickly! Anyone Submit?,https://www.reddit.com/r/MachineLearning/comments/3tn4vd/2016_conference_deadlines_are_passed_or_coming/,Synthint,1448070406,"Anyone here submit to any cool conferences? If so, what was it on? Post the paper if you can! Let's see what /r/machinelearning has to show :).

Personally, I have not submitted this year for 2016 BUT will be for 2017.",0,0
547,2015-11-21,2015,11,21,13,3tno0x,WT [1511.06279] Neural Programmer-Interpreters (Google DeepMind),https://www.reddit.com/r/MachineLearning/comments/3tno0x/wt_151106279_neural_programmerinterpreters_google/,[deleted],1448080659,[deleted],0,1
548,2015-11-21,2015,11,21,14,3tntsq,WT of [1511.06279] Neural Programmer-Interpreters (Google DeepMind),https://www.reddit.com/r/MachineLearning/comments/3tntsq/wt_of_151106279_neural_programmerinterpreters/,[deleted],1448084066,[deleted],0,1
549,2015-11-21,2015,11,21,15,3tnzcs,[1511.04868] An Online Sequence-to-Sequence Model Using Partial Conditioning,https://www.reddit.com/r/MachineLearning/comments/3tnzcs/151104868_an_online_sequencetosequence_model/,downtownslim,1448087567,,0,16
550,2015-11-21,2015,11,21,16,3to5zh,"Machine Learning Summer School 2016 in Cdiz, Spain",https://www.reddit.com/r/MachineLearning/comments/3to5zh/machine_learning_summer_school_2016_in_cdiz_spain/,musically_ut,1448092388,,0,5
551,2015-11-21,2015,11,21,18,3toawu,Few Reasons For Your Motor Bearing Failure,https://www.reddit.com/r/MachineLearning/comments/3toawu/few_reasons_for_your_motor_bearing_failure/,jackerfrinandis,1448096476,,0,1
552,2015-11-21,2015,11,21,19,3togqn,How Pneumatic Pumps are Beneficial to use for Domestic and Commercial Applications?,https://www.reddit.com/r/MachineLearning/comments/3togqn/how_pneumatic_pumps_are_beneficial_to_use_for/,jackerfrinandis,1448101381,,0,1
553,2015-11-21,2015,11,21,22,3totee,When all you have is pigeons,https://www.reddit.com/r/MachineLearning/comments/3totee/when_all_you_have_is_pigeons/,wwxxcc,1448111493,,25,68
554,2015-11-21,2015,11,21,22,3tox8e,Maching Learning Libraries in Java?,https://www.reddit.com/r/MachineLearning/comments/3tox8e/maching_learning_libraries_in_java/,bleeeeghh,1448113812,"I am looking for a good machine learning library written in Java. A library similar to Pythons scikit-learn. I prefer an offline library.

I used to use Weka for this but it seems to be quite old. What are you guys using?",22,21
555,2015-11-22,2015,11,22,1,3tpecn,Neural Programmer-Interpreters:: a recurrent and compositional NN that learns to represent and execute programs (submitted to ICLR 2016),https://www.reddit.com/r/MachineLearning/comments/3tpecn/neural_programmerinterpreters_a_recurrent_and/,cast42,1448122785,,8,26
556,2015-11-22,2015,11,22,2,3tpok7,Reusing data for supervised and unsupervised learning in neural networks,https://www.reddit.com/r/MachineLearning/comments/3tpok7/reusing_data_for_supervised_and_unsupervised/,gaussian_dvorak,1448127270,"Let's say we are given a set of data {(x_i , y_i)}, where the x_i are feature vectors and the y_i are class labels. Would it make any sense  to, for example, first train an autoencoder (unsupervised) with the feature vectors x_i only, and then fine tune our neural network (supervised) with {(x_i , y_i)}? In other words, the same feature vectors x_i are being used twice-- in an unsupervised and a supervised context.",5,5
557,2015-11-22,2015,11,22,4,3tq8xb,Introducing Swift AI - a Machine Learning library written entirely in Swift! It's a work in progress and I'd love to hear your feedback.,https://www.reddit.com/r/MachineLearning/comments/3tq8xb/introducing_swift_ai_a_machine_learning_library/,hundley10,1448135980,,4,8
558,2015-11-22,2015,11,22,8,3tr1ny,[1511.06295] Policy Distillation: A new paper from Deep Mind for knowledge transferring among Atari-playing deep learning agents and building agents that can play multiple games.,https://www.reddit.com/r/MachineLearning/comments/3tr1ny/151106295_policy_distillation_a_new_paper_from/,personanongrata,1448148606,,7,58
559,2015-11-22,2015,11,22,8,3tr2c4,It's Never Only a Game,https://www.reddit.com/r/MachineLearning/comments/3tr2c4/its_never_only_a_game/,adeniyiks,1448148902,,0,1
560,2015-11-22,2015,11,22,8,3tr2qe,Does anyone know of classification task papers that presented new insights of the task itself?,https://www.reddit.com/r/MachineLearning/comments/3tr2qe/does_anyone_know_of_classification_task_papers/,onewugtwowugs,1448149083,"This is me trying to start caring about classification tasks once more. Basically, I've started to feel like most machine learning applicationshave become less and less about the task itself, and more about turning nobs on algorithms in hope of getting a few more points of performance out of it. I mean, sure, this is fine if you are mostly interested in learning about the limitations of a certain model, but I rarely feel like I have become wiser regarding why a certain task is hard after reading papers on the issue.

Since I'm coming from an NLP background, preferably papers on classification problems where they gained new linguistic insights.",3,5
561,2015-11-22,2015,11,22,8,3tr3wh,"Seldon machine learning pipeline: scikit-learn, Vowpal Wabbit, XGBoost and Keras",https://www.reddit.com/r/MachineLearning/comments/3tr3wh/seldon_machine_learning_pipeline_scikitlearn/,ahousley,1448149624,,0,14
562,2015-11-22,2015,11,22,10,3trep5,"Weights for simple NN of the MNIST hand-writing recognition dataset. Red = Likely, Blue = Unlikely.",https://www.reddit.com/r/MachineLearning/comments/3trep5/weights_for_simple_nn_of_the_mnist_handwriting/,dinosaur_noises,1448154749,,11,4
563,2015-11-22,2015,11,22,11,3trntb,K-means clustering with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/3trntb/kmeans_clustering_with_tensorflow/,narphorium,1448159349,,0,0
564,2015-11-22,2015,11,22,13,3ts2kc,Insulation Blowing Machines &amp; Insulation Blowing Equipment,https://www.reddit.com/r/MachineLearning/comments/3ts2kc/insulation_blowing_machines_insulation_blowing/,ScottKScott,1448167403,,0,0
565,2015-11-22,2015,11,22,14,3ts64z,Unexpected results with low info gain features,https://www.reddit.com/r/MachineLearning/comments/3ts64z/unexpected_results_with_low_info_gain_features/,rePAN6517,1448169485,"I have a 3-class classification problem where the 3 classes are basically Yes, No, and Draw.  So what I'm really aiming for are high levels of accuracy on the Yes and No classes.  Draws I don't care about as much but I would prefer not having an excessive number of Draws at the expense of having enough Yes &amp; No's.  I have over 100 potential features I've been testing and I'm a little confused because I am seeing the best Yes &amp; No classifications (Naive Bayes Classifier) when I use a subset of features that are low info gain.  The low info gain features do result in lower receiver operator characteristic scores when compared to high info gain features though.  I'm not an expert and I'm not completely sure what to make of this.

I ran all my features through a GainRatio, InfoGain, and ChiSquared attribute evaluators to rank the info gain of each potential feature.   I also tried a Random Forest algorithm which does actually perform better on subsets of high info-gain features.",0,4
566,2015-11-22,2015,11,22,14,3ts8jw,Describing Videos by Exploiting Temporal Structure (paper + code),https://www.reddit.com/r/MachineLearning/comments/3ts8jw/describing_videos_by_exploiting_temporal/,samim23,1448171039,,0,1
567,2015-11-22,2015,11,22,20,3tsygj,Cross-Validation when training NNs,https://www.reddit.com/r/MachineLearning/comments/3tsygj/crossvalidation_when_training_nns/,[deleted],1448192451,[deleted],0,0
568,2015-11-22,2015,11,22,21,3tt0dg,Data Science conferences,https://www.reddit.com/r/MachineLearning/comments/3tt0dg/data_science_conferences/,Dawny33,1448194040,,0,2
569,2015-11-22,2015,11,22,21,3tt12a,Docker Swarm as an alternative(near) to GPU,https://www.reddit.com/r/MachineLearning/comments/3tt12a/docker_swarm_as_an_alternativenear_to_gpu/,harshjv,1448194617,"Hello,

I have 4 computers, each with 4 GB memory and have Core i3 CPU &amp; Intel 4000 HD graphics card, but without any dedicated GPU.

Is there any possibility that I can use Docker Swarm for ML on these computers to aid unavailability of GPUs? Like faster results?

For example, this project https://github.com/karpathy/neuraltalk2

Can this be done using clusters without GPU as good as with-GPU system?

Anyone tried something as an efficient cluster alternative to GPUs for CPU?",8,0
570,2015-11-22,2015,11,22,21,3tt49i,Big data and machine learning,https://www.reddit.com/r/MachineLearning/comments/3tt49i/big_data_and_machine_learning/,based2,1448197157,,1,7
571,2015-11-23,2015,11,23,0,3tti81,Has deepmind's ai system solved go?,https://www.reddit.com/r/MachineLearning/comments/3tti81/has_deepminds_ai_system_solved_go/,misgite,1448205778,,3,0
572,2015-11-23,2015,11,23,0,3ttkoq,Neuroevolution: The Development of Complex Neural Networks and Getting Rid of Hand Engineering,https://www.reddit.com/r/MachineLearning/comments/3ttkoq/neuroevolution_the_development_of_complex_neural/,Synthint,1448207042,"I'm interested in seeing who here has any experience with neuroevolution. This is the majority of my work in the lab; evolving deep neural networks (not much literature out there with deep nets but certainly a lot with large/wide nets (some with even millions of connections [8 million to be exact]).
For those who'd like a short intro: Neuroevolution is a machine learning technique that applies evolutionary algorithms to construct artificial neural networks, taking inspiration from the evolution of biological nervous systems in nature. Source: http://www.scholarpedia.org/article/Neuroevolution

Also, there's a subreddit /r/evolutionarycomp for this kind of stuff (evolutionary computation) y'all should check out if interested. ",30,26
573,2015-11-23,2015,11,23,2,3ttvna,Choosing R or Python for data analysis? An infographic,https://www.reddit.com/r/MachineLearning/comments/3ttvna/choosing_r_or_python_for_data_analysis_an/,Synthint,1448212124,,8,0
574,2015-11-23,2015,11,23,3,3tu876,Convolutional Autoencoder,https://www.reddit.com/r/MachineLearning/comments/3tu876/convolutional_autoencoder/,pumpkin105,1448217260,"Hi,

I'm trying to implement a convolutional autoencoder. Doing that, I'm struggling with training more than one layer. My structure looks like this:

- Input Noise
- Conv (1-&gt;32, (4,4)), Maxpooling (2,2)
- Conv (32-&gt;64, (4,4)), Maxpooling (2,2)
- Conv (64-&gt;32, (4,4)), Upsampling (2,2)
- Conv (32-&gt;1, (4,4)), Upsampling (2,2)

When using one layer (i.e. Conv 1-&gt;32, Maxpooling, Upsampling, Conv 32-&gt;1), the results and the reconstrutec pictures look nice. With the mentioned two layered structure, the values are growing and growing, the last layer quickly has values &lt; -1000 after a few iterations, thus the reconstructed images are just black. Has anyone made experience with ConvAEs?

A more general question: When feature maps are created using the input image, the kernel slides across the image and creates its feature map. When doing the second convolution, there are n feature maps, depending on the number of kernels. Are the next kernels using all the feature maps? If so, are they avering their results? It's hard to find any detailed explanation on this..",17,8
575,2015-11-23,2015,11,23,3,3tu8sh,[Matlab]Help with CNN implementation,https://www.reddit.com/r/MachineLearning/comments/3tu8sh/matlabhelp_with_cnn_implementation/,MarcoROG-SG,1448217511,"Hello!
I'm relatively new to machine learning, and i'm trying to implement a CNN using MatLAB as part of this tutorial:
http://ufldl.stanford.edu/tutorial/supervised/ExerciseConvolutionalNeuralNetwork/

I have tried implementing the backpropagation of the network but i'm having some issues and i'm not really sure if i'm doing it right.
The gradients determined analitically don't match up with the ones backpropagated and i would really appreciate if someone could take a look to my cnnConvolve, cnnPool and cnnCost files:
https://github.com/MarcoROG/ConcurrentNeuralNetwork
You can find it in the cnn folder.

EDIT: Pool and Convolve are OK, so the problem must be in the backProp part of cnnCost

Thanks a lot for your help!",7,0
576,2015-11-23,2015,11,23,3,3tu9qz,Machine Learning Trick of the Day (5): Log Derivative Trick  The Spectator,https://www.reddit.com/r/MachineLearning/comments/3tu9qz/machine_learning_trick_of_the_day_5_log/,mttd,1448217923,,7,79
577,2015-11-23,2015,11,23,4,3tug6m,"""Neural net descriptions generated in realtime during a brief walk around Amsterdam""",https://www.reddit.com/r/MachineLearning/comments/3tug6m/neural_net_descriptions_generated_in_realtime/,s3ma,1448220686,,8,67
578,2015-11-23,2015,11,23,5,3tusk9,How to cluster large data set with k-means using silhouettes?,https://www.reddit.com/r/MachineLearning/comments/3tusk9/how_to_cluster_large_data_set_with_kmeans_using/,aprstar,1448225726,[removed],0,1
579,2015-11-23,2015,11,23,6,3tuxja,Feeding Seq2Seq Decoder with Mean and Last Hidden State,https://www.reddit.com/r/MachineLearning/comments/3tuxja/feeding_seq2seq_decoder_with_mean_and_last_hidden/,LeavesBreathe,1448227694,"Hey Guys,

kknaster helped me greatly with this post discussing hidden states: https://www.reddit.com/r/MachineLearning/comments/3sexmi/transfering_hidden_states_in_sequence_to_sequence/

Currently, I'm using Google's TensorFlow Model for predicting the next sentence given the previous sentence.

My main problem right now is that the predicted sentences are **very repetitive** and go something like this:

	""the stars in the sky are the stars.""
	""the most common planets are the most common planets.""
	""stars are common in the galaxy in the galaxy.""

I think what is going on is that the network is only seeing ""galaxy"", and then predicting ""in"" then ""the"" then ""galaxy""

This has led me to this idea: Using both the *mean* of the hidden states **and** the last hidden state:

 	#decoder_states refers to previous states produced by the encoder AND decoder. 
    mean_decoder_states = np.mean(decoder_states)
  	final_decoder_state = (0.5*decoder_states[-1])+0.5*mean_decoder_states

Where half of the state reflects the last hidden state and the other half represents what has already been said. This seems too obvious that no one has reported on it, so I'm thinking there must be a problem with this idea. I should clarify: that this combined mean and last hidden state average would be for **both training and testing**

Thoughts? Thanks! Open to any ideas.",12,2
580,2015-11-23,2015,11,23,7,3tv2fl,Are there R packages to create a bigram/trigram frequency matrix for a naive bayes classifier?,https://www.reddit.com/r/MachineLearning/comments/3tv2fl/are_there_r_packages_to_create_a_bigramtrigram/,[deleted],1448229676,[deleted],0,0
581,2015-11-23,2015,11,23,12,3tw74n,sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings,https://www.reddit.com/r/MachineLearning/comments/3tw74n/sense2vec_a_fast_and_accurate_method_for_word/,[deleted],1448248050,[deleted],0,1
582,2015-11-23,2015,11,23,13,3twdp4,5 Ways Machine Learning Is Reshaping Our World (by Bernard Marr),https://www.reddit.com/r/MachineLearning/comments/3twdp4/5_ways_machine_learning_is_reshaping_our_world_by/,vincentg64,1448251211,,0,1
583,2015-11-23,2015,11,23,14,3twqol,IBM's SystemML machine learning system becomes Apache Incubator project,https://www.reddit.com/r/MachineLearning/comments/3twqol/ibms_systemml_machine_learning_system_becomes/,KeponeFactory,1448258047,,1,7
584,2015-11-23,2015,11,23,15,3twsh3,Training Neural Networks on Spark : SparkNet,https://www.reddit.com/r/MachineLearning/comments/3twsh3/training_neural_networks_on_spark_sparknet/,muktabh,1448259042,,3,12
585,2015-11-23,2015,11,23,15,3twtdj,Auotmatic Tray Sealing Machine,https://www.reddit.com/r/MachineLearning/comments/3twtdj/auotmatic_tray_sealing_machine/,dongfengpacking,1448259553,,1,1
586,2015-11-23,2015,11,23,15,3twwwh,Let's discuss: is arXiv always good?,https://www.reddit.com/r/MachineLearning/comments/3twwwh/lets_discuss_is_arxiv_always_good/,citeordie,1448261743,"Machine learning researcher here. It seems like our field is moving towards 'publish-on-arXiv-ASAP' model of research. This is obviously good for distributing ideas fast, but I am somewhat frustrated by the flag-planting culture that's being implicitly encouraged by arXiv. I've been going through the recent crop of ICLR papers and while some are paragons of good/innovative science, many are obvious rough drafts (just using ICLR as an example. I am well aware that ICLR submissions are currently being reviewed and considered non-archival.)

I've been trying to battle this by citing ""best-executed"" papers (usually conference publications) versus ""first"" papers (usually technical reports on arXiv) when I have to choose one citation (due to space), but I've had several reviewers telling me to cite the ""first"" paper instead. 

Interested to hear what other ML researchers think of this.
",19,26
587,2015-11-23,2015,11,23,16,3twy86,Deepwalk algorithm to learn distributed embeddings of Nodes in social networks implemented using Keras and igraph,https://www.reddit.com/r/MachineLearning/comments/3twy86/deepwalk_algorithm_to_learn_distributed/,napsternxg,1448262599,,2,3
588,2015-11-23,2015,11,23,16,3twyev,The hardest parts of data science,https://www.reddit.com/r/MachineLearning/comments/3twyev/the_hardest_parts_of_data_science/,cast42,1448262718,,1,10
589,2015-11-23,2015,11,23,16,3twys1,FC 5035,https://www.reddit.com/r/MachineLearning/comments/3twys1/fc_5035/,dongfengpacking,1448262953,,1,1
590,2015-11-23,2015,11,23,17,3tx3j1,[1511.06391] Order Matters: Sequence to sequence for sets,https://www.reddit.com/r/MachineLearning/comments/3tx3j1/151106391_order_matters_sequence_to_sequence_for/,iori42,1448266066,,0,3
591,2015-11-23,2015,11,23,17,3tx44w,WT of [1511.06114] Multi-task Sequence to Sequence Learning (Google team),https://www.reddit.com/r/MachineLearning/comments/3tx44w/wt_of_151106114_multitask_sequence_to_sequence/,[deleted],1448266507,[deleted],2,0
592,2015-11-23,2015,11,23,17,3tx4kj,Worldwide Agriculture M2M Market Research and Analysis Report 2019 | Business,https://www.reddit.com/r/MachineLearning/comments/3tx4kj/worldwide_agriculture_m2m_market_research_and/,priyalambani,1448266811,,0,1
593,2015-11-23,2015,11,23,20,3txip3,Do you really need a PhD to do machine learning? I think this is no longer the case.,https://www.reddit.com/r/MachineLearning/comments/3txip3/do_you_really_need_a_phd_to_do_machine_learning_i/,smith2008,1448277464,"I keep hearing ( reading ) the way to learn how to build things with ML is through obtaining a PhD on a related field. I have a master degree in AI and it helps a lot but I wouldn't say it is a must have to start building cool and useful stuff with AI. 

It could've been the case when ML was mostly science oriented field and not so much of an engineering one. But just like with building software it become a lot easier with rising of powerful frameworks and cheap GPU / cloud solutions.

I think we should not promote the idea that ML is really hard core science stuff only. Well, it is, but there is so much more there. IMO this could slow down the development of the field in a long term.

What do you think?

EDIT: it seems there is a lot of controversy and confusion about my statement. Just wanted to make it clear that I am not saying a PhD in ML is not a good thing. Quite the opposite. If you can get one you should.

To try to explain what I really mean I will give an example with rocket science. Imagine you are really passionate about rockets, you want to build them, to experiment with different ideas and approaches. You want to create amazing rockets. Well that's great but you cannot do a lot if you are not part of some a research lab. Well, probably you can build a small rocket model if you put a couple of grands and a lot of passion but nothing even close to what NASA or SpaceX are doing. Now imagine you have that passion for AI and you want to build your AI ""rocket"". Could you build something close to what big companies are building? Well I can relate to plenty of small startups who are doing it.

So, all I am trying to say is that ML is a really an amazing and powerful approach to AI and it is getting a lot more accessible than it use to be.",63,43
594,2015-11-23,2015,11,23,20,3txjm5,Refinery -- user-friendly interface to do NLP topic modelling with LDA,https://www.reddit.com/r/MachineLearning/comments/3txjm5/refinery_userfriendly_interface_to_do_nlp_topic/,cast42,1448278159,,0,0
595,2015-11-23,2015,11,23,20,3txlvu,Deep Learning using object recognition (project question),https://www.reddit.com/r/MachineLearning/comments/3txlvu/deep_learning_using_object_recognition_project/,buhburkz,1448279877,"Hello , I'm a third year comp sci student. I have recently been given a project on Deep learning as the title says. It's more of a introduction project whereas I'd have to learn about it and produce a demo for the object recognition part. I've found many resources for the theoretical side of things , but does anyone have any resources on implementation in code ? I've seen most people use Python(Theano), Torch , R etc.  I was leaning towards Python with the libraries myself.  

Thanks in advance for your help.",0,0
596,2015-11-23,2015,11,23,21,3txm3r,"Remember FB 20 Q&amp;A tasks? This paper ""achieve near-perfect accuracy on all categories, including positional reasoning and pathfinding that have proved difficult for all previous approaches"".",https://www.reddit.com/r/MachineLearning/comments/3txm3r/remember_fb_20_qa_tasks_this_paper_achieve/,derRoller,1448280048,,3,7
597,2015-11-23,2015,11,23,21,3txngw,Ken Stanley: Discovery Without Objectives,https://www.reddit.com/r/MachineLearning/comments/3txngw/ken_stanley_discovery_without_objectives/,Ghostlike4331,1448280977,,6,4
598,2015-11-23,2015,11,23,21,3txql2,Optimization of Expectation,https://www.reddit.com/r/MachineLearning/comments/3txql2/optimization_of_expectation/,Kiuhnm,1448283105,"I'm not a mathematician so I'm learning the math I need along the way.

If you go at page 216 of [this pdf](http://goodfeli.github.io/dlbook/contents/regularization.html) you'll see an objective function (7.19) to be minimized w.r.t. a function.
First of all, I think the authors forgot a trace operator. Indeed, the first expectation should be a scalar.
I asked on [math.stackexchange.com](http://math.stackexchange.com/questions/1542039/minimization-of-expected-value) but so far no one answered my question. Can you help me in any way?
I could just move on, but I think optimization is important.",0,0
599,2015-11-23,2015,11,23,22,3txwfp,EuroSciPy 2015: European Scientific Computing with Python Conference,https://www.reddit.com/r/MachineLearning/comments/3txwfp/euroscipy_2015_european_scientific_computing_with/,ealx10,1448286492,,0,2
600,2015-11-23,2015,11,23,23,3txzfh,"Is there a type of ""reverse collocation"" function for NLTK type programs?; a function that displays words that are the most unused together?",https://www.reddit.com/r/MachineLearning/comments/3txzfh/is_there_a_type_of_reverse_collocation_function/,Rllrllrrlrrl,1448288103,"Sorry if this is a dumb question, I'm new to this and an amateur.",3,0
601,2015-11-24,2015,11,24,0,3ty5nf,Sense2vec  A Fast and Accurate Method for Word Sense Disambiguation,https://www.reddit.com/r/MachineLearning/comments/3ty5nf/sense2vec_a_fast_and_accurate_method_for_word/,iamtrask,1448291260,,2,5
602,2015-11-24,2015,11,24,0,3ty6j9,File Overlap in Big Datasets,https://www.reddit.com/r/MachineLearning/comments/3ty6j9/file_overlap_in_big_datasets/,wxyyxc1992,1448291687,[removed],0,0
603,2015-11-24,2015,11,24,1,3tydtp,A website for hosting and sharing Machine Learning trained models,https://www.reddit.com/r/MachineLearning/comments/3tydtp/a_website_for_hosting_and_sharing_machine/,_cr_55_,1448294900,"Hi all,


We are building a platform where you can upload and showcase your trained Machine Learning models.


The idea is that you can focus on the Machine Learning algorithms and easily deploy your models without the need of coding UIs, configuring servers, etc.


It is an early prototype and for now it works only with Python code. Here you can play with two classic examples based on scikit-learn (it may take a couple of seconds to load the models):

[https://predictors.ai/#/p/Sales_demo_predictor_probabilities](https://predictors.ai/#/p/Sales_demo_predictor_probabilities)

[https://predictors.ai/#/p/Iris_flower_classifier](https://predictors.ai/#/p/Iris_flower_classifier)


And these are two more recent examples based on Keras and Lasagne:

[https://predictors.ai/index.html#/p/bAbI_size_reasoning_predictor](https://predictors.ai/index.html#/p/bAbI_size_reasoning_predictor)

[https://predictors.ai/#/p/Image_Labeler_VGG_S](https://predictors.ai/#/p/Image_Labeler_VGG_S)


In order to upload a model to the platform you just have to implement an additional method in your code and describe the model with a json file.


Public predictors are hosted for free. You can even deploy a model directly from GitHub and a link to its source code will be displayed. This is a very convenient way of sharing both the source code and an interactive demo of the model, and it also enables people to easily fork existing online models.


We have a long To-Do list full of improvements and new features, but if anyone would like to start uploading models to this beta version and get a profile on the website, send me a PM or an email and we will gladly grant you access.",3,20
604,2015-11-24,2015,11,24,1,3tykrw,Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/3tykrw/unsupervised_representation_learning_with_deep/,r-sync,1448297774,,32,174
605,2015-11-24,2015,11,24,2,3tynwl,David Silver (Google DeepMind) - Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/3tynwl/david_silver_google_deepmind_deep_reinforcement/,[deleted],1448298977,[deleted],0,0
606,2015-11-24,2015,11,24,2,3tyrlh,"Highlights from the IEEE International Conference on Data Mining, November 2015",https://www.reddit.com/r/MachineLearning/comments/3tyrlh/highlights_from_the_ieee_international_conference/,thvasilo,1448300366,"I had the chance to attend ICDM last week, so I did a small write-up where I included some of the interesting work I saw there, as well as summary of M.I. Jordan's keynote address.

You can read the whole thing here: http://tvas.me/conferences/2015/11/23/ICDM-2015-Highlights.html

If anyone else here attended I would love to hear your impressions, and specifically if someone attended Lada Adamic's keynote I'd like to see a summary, since I had to miss that.",2,2
607,2015-11-24,2015,11,24,4,3tz4c3,Input on rigorous online course,https://www.reddit.com/r/MachineLearning/comments/3tz4c3/input_on_rigorous_online_course/,justanotherta5,1448305306,"I find myself in the happy position of being in grad school, needing to take machine learning for my research, but the ML course is full and the elitist comp sci department won't add new slots for non-comp sci students. So my advisor has suggested that I enroll in a couple of research credits and teach myself through an online class. Hooray.

So I want something equally rigorous. I am looking at [Ng's Stanford lectures](https://www.youtube.com/watch?v=UzxYlbK2c7E&amp;list=PLA89DCFA6ADACE599), the Georgia Tech online computer science masters program machine learning course ([here on Udacity](https://www.udacity.com/course/machine-learning-supervised-learning--ud675)), [the CMU 10-701 course lectures taught by Tom Mitchell](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx?#folderID=%2285e1b6bf-6ac9-4a92-a0de-aaf8c2dd2418%22), or the [Caltech CS 156 course by Professor Yaser Abu-Mostafa](http://work.caltech.edu/telecourse.html).

I have a strong background in linear algebra, calculus (including matrix calculus), probability theory, and some topics particularly useful for ML like reproducing kernel hilbert spaces, so e.g. the Ng coursera course would be too shallow. What do you guys think?",7,1
608,2015-11-24,2015,11,24,4,3tz5u4,What are some good strategies for implementing a Trained Machine Learning Model into a Web App?,https://www.reddit.com/r/MachineLearning/comments/3tz5u4/what_are_some_good_strategies_for_implementing_a/,polyglotdev,1448305859,"I've trained some models and wanted to build them into a production application. 

What're some best practices? Is it better to leave everything on the server side and use API Calls from the fronted(this would be the obvious choice for something like KNN)? Or does it ever make since to write the prediction function in Javascript and then pass it the current set of weights to execute on the Client Side (if you're doing a single layer Neural Network)? And for Convolutional Neural Networks are those run ""offline"" and just added to the data model? 

Just wondering what are some strategies used in practice? ",1,1
609,2015-11-24,2015,11,24,4,3tz633,Uncovering Temporal Context for Video Question and Answering,https://www.reddit.com/r/MachineLearning/comments/3tz633/uncovering_temporal_context_for_video_question/,ffmpbgrnn,1448305968,,0,2
610,2015-11-24,2015,11,24,4,3tz7ep,Anyone interested in a study group for Andrew Ng's Machine Learning Coursera / Stanford class?,https://www.reddit.com/r/MachineLearning/comments/3tz7ep/anyone_interested_in_a_study_group_for_andrew_ngs/,sjforman,1448306476,[removed],5,0
611,2015-11-24,2015,11,24,4,3tzczw,A Statistical View of Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3tzczw/a_statistical_view_of_deep_learning/,alexeyr,1448308588,,0,4
612,2015-11-24,2015,11,24,5,3tzf70,Image classification using R and MXnet (MXNetR),https://www.reddit.com/r/MachineLearning/comments/3tzf70/image_classification_using_r_and_mxnet_mxnetr/,phunter_lau,1448309415,"A Shiny app for image classification using R (trust me, it is pretty fast, and yes this deep learning model runs in R) https://github.com/thirdwing/mxnet_shiny

To run it, the first step is installing MXnet https://github.com/dmlc/mxnet by following the installation guide http://mxnet.readthedocs.org/en/latest/build.html and the second step is, install MXnetR and mxnet_shiny like this http://imgur.com/rpDwtHo That is it!

After setup everything, the shiny app should look like this http://imgur.com/9QEzUbU Enjoy your R deep learning shiny app for classifying images. It is good at classifying types of dogs/cats (well, like other deep learning models), as well as other interesting stuff. 

Any suggestions please comment, and/or submit issues to https://github.com/dmlc/mxnet ",0,0
613,2015-11-24,2015,11,24,5,3tzfve,Machine Learning for Finance [Universit Panthon-Assas],https://www.reddit.com/r/MachineLearning/comments/3tzfve/machine_learning_for_finance_universit/,gwulfs,1448309686,,1,6
614,2015-11-24,2015,11,24,7,3u00l7,Can someone guide me with PyBrain?,https://www.reddit.com/r/MachineLearning/comments/3u00l7/can_someone_guide_me_with_pybrain/,Robotronic777,1448317583,[removed],17,1
615,2015-11-24,2015,11,24,8,3u08q4,Mining of Massive Data Sets - Solutions Manual? [TLDR],https://www.reddit.com/r/MachineLearning/comments/3u08q4/mining_of_massive_data_sets_solutions_manual_tldr/,Hexzor2204,1448320905,"TLDR: need information on solution manual for data mining textbook.

I've been taking a course in data mining/machine learning and we have been using the free textbook from the stanford university courses described here.
http://www.mmds.org/
Its a great textbook with corresponding slides. However, I am stumped on one of the Exercises and the website alludes to a solutions manual but through all of my searching has turned up nothing. Does anyone have any idea where I could find it or have any other relevant info on the topic?

PS: My apologies for a not so interesting Machine Learning post...",0,0
616,2015-11-24,2015,11,24,8,3u0bbf,Decision Tree question,https://www.reddit.com/r/MachineLearning/comments/3u0bbf/decision_tree_question/,AlteKartoffel,1448321951,"Dear Data Scientists,

I have a question regarding decision trees. 

I have a dataset of 477 people with several features. Each person is either classified Class A or Class B, which is known.

Unfortunately only 55 out of those 477 are classified as class B.

I used the tree package of R to create a decision tree, to predict the Class of people via their feature vector.
In this application it is way more expensive if a Class B person is misclassified. Misclassifying Class A people is cheap.

The problem is, that the decision tree classifies Class A with high accuracy and Class B with low accuracy. Because the majority of the dataset is class A, the overall accuracy is high, but the result is not very useful.

Is there a way to include a cost function into the optimization algorithm of the decision tree, so it focuses more on classifying Class B correctly (while being overall less accurate)?

Thank you very much for your help.
",5,1
617,2015-11-24,2015,11,24,8,3u0cbv,Ideas/Help with best type of learning algorithm for this dataset.,https://www.reddit.com/r/MachineLearning/comments/3u0cbv/ideashelp_with_best_type_of_learning_algorithm/,zZJollyGreenZz,1448322381,"Imagine a dataset of images where each image contains a random number of mnist digits that are randomly spread around the image.


I could generate this data set with an algorithm, so for training and testing, I would have an unlimited data set.


I would like to build an algorithm that can effectively place a bounding box around each digit.


I was thinking about trying to do this as a simple reinforcement learning problem.  It seems like for each image, I would want a policy that can search the image and place boxes on digits.  I could build a convnet that knows the difference between a digit and background, so this could be part of the reward.  Placing boxes on empty background could result in negative reward or no reward and placing boxes on digits could result in higher reward.


I could imagine the controls being something like move left/right/up/down and place box.


Maybe reinforcement learning is overkill for this idea or completely the wrong way to go about it.  Any ideas or assistance?
",4,1
618,2015-11-24,2015,11,24,10,3u0mq9,Deeplearning4j or Theano,https://www.reddit.com/r/MachineLearning/comments/3u0mq9/deeplearning4j_or_theano/,koormoosh,1448326967,"I am very new to deep learning research and am interested in doing some experiments by (trying to) replicating Bengio/Mikolov's language models to deepen my knowledge about what I've read. I wonder if you could tell me your opinion on which framework I need to use for programming: Python (Theano), or Java (deeplearning4j)? In particular I am thinking to benefit from parallel computing either on CPUs or GPUs, and am not sure if Theano is good for that purpose or it will bite my down the road. And at the same time I am not sure if deeplearning4j has rich libraries to offer what Theano has been offerring to the research community. So, I appreciate it if you make a comparison based on:

- Library richness (i.e. matrix operations, optimization libraries, etc)
- Parallel capabilities (i.e. CPUs for example using running under Spark, or GPUs)",8,1
619,2015-11-24,2015,11,24,10,3u0n4m,WT of [1511.06440] Towards Principled Unsupervised Learning (Google team),https://www.reddit.com/r/MachineLearning/comments/3u0n4m/wt_of_151106440_towards_principled_unsupervised/,[deleted],1448327157,[deleted],2,0
620,2015-11-24,2015,11,24,10,3u0otc,Psy Tower - An RPG battle game controlled solely by programmable bots,https://www.reddit.com/r/MachineLearning/comments/3u0otc/psy_tower_an_rpg_battle_game_controlled_solely_by/,rt4fun,1448327911,,0,1
621,2015-11-24,2015,11,24,10,3u0psc,"Short summary of (Coordinate, Stochastic) Gradient Descent Convergence Rate",https://www.reddit.com/r/MachineLearning/comments/3u0psc/short_summary_of_coordinate_stochastic_gradient/,hduongtrong,1448328361,,2,6
622,2015-11-24,2015,11,24,12,3u1643,New to machine learning? This blog post covers the very basics of a linear regression in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/3u1643/new_to_machine_learning_this_blog_post_covers_the/,donovan28,1448335982,,0,0
623,2015-11-24,2015,11,24,13,3u1akq,"NN with a genetic algorithm used for an initial phase for a starting point in a conjugate gradient algorithm. I have a few questions, does someone have some time to help?",https://www.reddit.com/r/MachineLearning/comments/3u1akq/nn_with_a_genetic_algorithm_used_for_an_initial/,reactionforceatA,1448338112,"I'm a Structural Engineering grad student and I'm doing a research paper for a Structural Reliability class  base on [this](http://www.sciencedirect.com/science/article/pii/S0965997807001007) paper. I understand how the neural network works but I'm having some trouble with the optimization using the genetic algorithm and the conjugate gradient algorithm. This portion of the paper falls between equations (7) and (8).

Would someone mind ELI5 this process using first iteration of these algorithms with actual numbers, as used in this paper? These algorithms are beyond the scope of this class and I've been trying to find examples but have not had any luck.  ",2,0
624,2015-11-24,2015,11,24,15,3u1r79,Is there a multi-targets linear SVR implementation,https://www.reddit.com/r/MachineLearning/comments/3u1r79/is_there_a_multitargets_linear_svr_implementation/,leewardx,1448346722,"For *multi-targets*, I mean the response *y* is two dimensions, i.e., *n_samples x n_targets*.

The famouse [liblinear](https://www.csie.ntu.edu.tw/~cjlin/liblinear/) only supports 1D *y*. So the [scikit Linear SVR](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR) is also not support.

Any language implementation is welcome.",6,0
625,2015-11-24,2015,11,24,15,3u1r8q,Impulse sealer,https://www.reddit.com/r/MachineLearning/comments/3u1r8q/impulse_sealer/,dongfengpacking,1448346748,,1,1
626,2015-11-24,2015,11,24,15,3u1sb3,Do people make web apps with machine learning?,https://www.reddit.com/r/MachineLearning/comments/3u1sb3/do_people_make_web_apps_with_machine_learning/,NewArithmetic,1448347367,"I'm currently an undergrad studying CS and have found the field of machine learning incredibly intriguing. I've self-studied quite a bit now, and I would love to apply what I know and make something with it. Though, every idea feels like it's already been done, or it'll take a crazy amount of time. Any ideas on what the best way is I could apply machine learning as a solo dev?

Thanks!",4,1
627,2015-11-24,2015,11,24,15,3u1ssk,Neural Style in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/3u1ssk/neural_style_in_tensorflow/,anishathalye,1448347625,,13,30
628,2015-11-24,2015,11,24,16,3u1unc,Impulse sealer plastic body,https://www.reddit.com/r/MachineLearning/comments/3u1unc/impulse_sealer_plastic_body/,dongfengpacking,1448348826,,1,1
629,2015-11-24,2015,11,24,18,3u27i0,"8 Pneumatic System Applications, You Didnt Know Existed Around You",https://www.reddit.com/r/MachineLearning/comments/3u27i0/8_pneumatic_system_applications_you_didnt_know/,jackerfrinandis,1448358231,,0,1
630,2015-11-24,2015,11,24,20,3u2eam,Data Science Podcasts?,https://www.reddit.com/r/MachineLearning/comments/3u2eam/data_science_podcasts/,Dawny33,1448363538,,12,55
631,2015-11-24,2015,11,24,20,3u2fgi,Arte tooling newsletter,https://www.reddit.com/r/MachineLearning/comments/3u2fgi/arte_tooling_newsletter/,artetooling,1448364395,,0,0
632,2015-11-24,2015,11,24,20,3u2hp0,deconvolutional network in semantic segmentation,https://www.reddit.com/r/MachineLearning/comments/3u2hp0/deconvolutional_network_in_semantic_segmentation/,[deleted],1448366045,[deleted],0,0
633,2015-11-24,2015,11,24,23,3u2uuj,Neural Art in tensorflow (also a demo of loading various Caffe models),https://www.reddit.com/r/MachineLearning/comments/3u2uuj/neural_art_in_tensorflow_also_a_demo_of_loading/,Sanavoir,1448374095,,3,32
634,2015-11-24,2015,11,24,23,3u2vc9,How does vectorization work for backpropogation through time?,https://www.reddit.com/r/MachineLearning/comments/3u2vc9/how_does_vectorization_work_for_backpropogation/,c0wpig,1448374360,"I'm trying to implement an LSTM in Tensorflow (but without using their built-in RNN class), and am having trouble understanding how to vectorize the backpropogation through time algorithm. The example in their tutorials seems to break up data into finite-sized non-overlapping chunks. 

Here is what I believe I understand from what I have read so far:

For a character model the sentence ""a cat jumps"" might be broken up like this, if the number of steps before clipping is 3:

[(""a c"", "" ca""), (""at "", ""t j""), (""jum"", ""ump"") ...]

I think that batches consist of different phrases or sentences whose character sequences are independent of one another.

However, there are two things I don't understand about this approach.

- What happens when different batches have different lengths?

- Won't this training strategy miss out on learning the dependencies between character pairs that happen to fall on the border between segments?

Any intuition here would be much appreciated!",10,6
635,2015-11-25,2015,11,25,3,3u3v4f,"Help! We are Running a Survey on ""Dirty"" Data.",https://www.reddit.com/r/MachineLearning/comments/3u3v4f/help_we_are_running_a_survey_on_dirty_data/,datascience101,1448388995,,1,0
636,2015-11-25,2015,11,25,3,3u3xma,TensorFlow: Open source machine learning,https://www.reddit.com/r/MachineLearning/comments/3u3xma/tensorflow_open_source_machine_learning/,alexandredvolpi,1448389921,,2,0
637,2015-11-25,2015,11,25,6,3u4rig,"Deep Learning by Ian Goodfellow, Aaron Courville, and Yoshua Bengio",https://www.reddit.com/r/MachineLearning/comments/3u4rig/deep_learning_by_ian_goodfellow_aaron_courville/,reidhoch,1448401054,,1,18
638,2015-11-25,2015,11,25,7,3u4v36,Deep Learning applied to ranking/information retrieval?,https://www.reddit.com/r/MachineLearning/comments/3u4v36/deep_learning_applied_to_rankinginformation/,futrawo,1448402455,"There is a huge amount of literature concerning the application of Deep Learning approaches to things like image recognition, speech recognition etc but I was hoping to play around with it in the context of a toy version of a web ranking algorithm (just as a learning exercise). 

So far I haven't seen much in this area - does anyone know of any implementations or descriptions of approaches that might be useful as a starting point?

Thanks.",2,1
639,2015-11-25,2015,11,25,7,3u51c5,"WT of [1511.06807] Adding Gradient Noise Improves Learning for Very Deep Networks (Google team + UMass Amherst, U Toronto)",https://www.reddit.com/r/MachineLearning/comments/3u51c5/wt_of_151106807_adding_gradient_noise_improves/,[deleted],1448404931,[deleted],10,12
640,2015-11-25,2015,11,25,9,3u5fhp,Neural Random-Access Machines,https://www.reddit.com/r/MachineLearning/comments/3u5fhp/neural_randomaccess_machines/,siblbombs,1448410947,,10,27
641,2015-11-25,2015,11,25,10,3u5q5n,Evolution Algorithm Question,https://www.reddit.com/r/MachineLearning/comments/3u5q5n/evolution_algorithm_question/,jamarshon,1448415842,"So I was reading this really interesting paper about evolution algorithm for the Rubik's Cube ( http://www.genetic-programming.org/hc2010/7-Borschbach/Borschbach-Evo-App-2010-Paper.pdf ) and I was slightly confused by some concepts.

1) Does selection always occur after evaluating fitness regardless whether phase transition happens? Also, would the selection methods be the same (ie the selected individual [a b c] becomes the new population [a a b b c c]?

2) Does the mutation method allow the same rotation to be picked more than once?

3) In the diagram, what does ""phase i &gt; 0"" or ""phase i = 0"" mean? Shouldn't the only thing that determines phase transition be x &gt; u?

4) Going from G2 to G3, what actually occurs? What does ""putting edges in orbit"", ""adjacent corners in a circuit"" and ""(x) number of wrong colored facelets"" mean?

Thanks
 ",2,1
642,2015-11-25,2015,11,25,11,3u5t15,BBS 4525 Shrink packing system,https://www.reddit.com/r/MachineLearning/comments/3u5t15/bbs_4525_shrink_packing_system/,dongfengpacking,1448417209,,1,1
643,2015-11-25,2015,11,25,13,3u6abh,[torch-autograd] Joining last outputs of multiple LSTM,https://www.reddit.com/r/MachineLearning/comments/3u6abh/torchautograd_joining_last_outputs_of_multiple/,yfei,1448425923,"Hello all,

I'm very new to torch, just saw this library release by twitter and was trying it out and then hit this problem. There aren't many documentation around, so I thought reddit is a good place to get help.

I referred to it's LSTM example and wanted to do things slightly differently. I had 3 LSTM all returns the last output, and then I want to combine the outputs into a Tensor. Here's the snippet.

    local lstm1,params = model.RecurrentLSTMNetwork({
       inputFeatures = opt.wordSize,
       hiddenFeatures = opt.hiddens,
       outputType = 'last',
    })

    local h1, newState1 = lstm1(params[1], regularize(inputs[1], dropout), prevState)
    local h2, newState2 = lstm1(params[1], regularize(inputs[2], dropout), prevState)
    local c = d.nn.JoinTable(1)({h1,h2})

I got the error below:

    ... autograd/nnwrapper.lua:110: attempt to call method 'type' (a nil value)



Thanks.",3,0
644,2015-11-25,2015,11,25,14,3u6eco,How good are deep learning networks really?,https://www.reddit.com/r/MachineLearning/comments/3u6eco/how_good_are_deep_learning_networks_really/,[deleted],1448428099,[deleted],22,7
645,2015-11-25,2015,11,25,14,3u6go0,Specializing in ML?,https://www.reddit.com/r/MachineLearning/comments/3u6go0/specializing_in_ml/,MusicIsLife1995,1448429274,"I'm a Junior in college and pretty interested in Machine Learning. So far I've gone through tons of ML books, pdfs, videos over the past 7 months. Is a PHD required to get good at this field? I'm always self learning at school and I don't really see the point of me getting a PHD except for prestige. ",12,1
646,2015-11-25,2015,11,25,15,3u6ntp,carton sealing machine FXJ 5050,https://www.reddit.com/r/MachineLearning/comments/3u6ntp/carton_sealing_machine_fxj_5050/,dongfengpacking,1448433121,,1,1
647,2015-11-25,2015,11,25,15,3u6ppw,"Exponential Linear Units, ""yielded the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging""",https://www.reddit.com/r/MachineLearning/comments/3u6ppw/exponential_linear_units_yielded_the_best/,anyonetriedthis,1448434218,,45,65
648,2015-11-25,2015,11,25,16,3u6qyh,"WT of [1511.06789] The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition (Stanford, Google)",https://www.reddit.com/r/MachineLearning/comments/3u6qyh/wt_of_151106789_the_unreasonable_effectiveness_of/,[deleted],1448434969,[deleted],1,2
649,2015-11-25,2015,11,25,16,3u6svw,Can we open up flair selection?,https://www.reddit.com/r/MachineLearning/comments/3u6svw/can_we_open_up_flair_selection/,RiemannZero,1448436180,Would be good for us to post our backgrounds to make conversations smoother.,8,9
650,2015-11-25,2015,11,25,17,3u6wc5,DFJ 130 forming-filling -sealing machine,https://www.reddit.com/r/MachineLearning/comments/3u6wc5/dfj_130_formingfilling_sealing_machine/,dongfengpacking,1448438434,,1,1
651,2015-11-25,2015,11,25,17,3u6xyu,Shrink tunnel BS 300 400,https://www.reddit.com/r/MachineLearning/comments/3u6xyu/shrink_tunnel_bs_300_400/,dongfengpacking,1448439581,,1,1
652,2015-11-25,2015,11,25,18,3u71cv,can trees be used for deep learning?,https://www.reddit.com/r/MachineLearning/comments/3u71cv/can_trees_be_used_for_deep_learning/,godspeed_china,1448442076,"The basic thought is that since GLM and decision tree are two basic models. And nowadays stacked GLM becomes popular deep learning architecture. I feel some ""conjugation"" between GLM and DT. So is there a way to do DT based deep learning? Can we stack DT in some way?",5,0
653,2015-11-25,2015,11,25,18,3u72za,How do summary bots like smmry work?,https://www.reddit.com/r/MachineLearning/comments/3u72za/how_do_summary_bots_like_smmry_work/,rms_returns,1448443249,"Often times, I'm left wondered at the succinct summary prepared by those tldr bots that begin with `This is the best tldr; I could make` while reading a news or blog post.

And those tldrs are so much to the point that in most cases is almost comparable to human efforts. Call me a noob but even being a programmer I can't fathom the logic of how such a bot might work? I know a bit about the python `nltk` library for natural language processing. I know it can break down a sentence or paragraph and assign tags to it. But how does `tagging` magically lead to a tldr; summary, can someone care to explain me?",1,2
654,2015-11-25,2015,11,25,19,3u792k,Adopt To the Healthy Lubrication Practices For Prolonged Bearings Life,https://www.reddit.com/r/MachineLearning/comments/3u792k/adopt_to_the_healthy_lubrication_practices_for/,jackerfrinandis,1448447711,,0,1
655,2015-11-25,2015,11,25,21,3u7i9m,"Starch Paste Kettle, R&amp;D Equipments - Anchor Mark",https://www.reddit.com/r/MachineLearning/comments/3u7i9m/starch_paste_kettle_rd_equipments_anchor_mark/,Anchormark1,1448453999,,0,1
656,2015-11-25,2015,11,25,22,3u7o9z,IBM open-sources its SystemML machine learning tech,https://www.reddit.com/r/MachineLearning/comments/3u7o9z/ibm_opensources_its_systemml_machine_learning_tech/,Elendar42,1448457654,,5,24
657,2015-11-25,2015,11,25,23,3u7u0o,A tricky question about Kernel Functions,https://www.reddit.com/r/MachineLearning/comments/3u7u0o/a_tricky_question_about_kernel_functions/,Lopelh,1448460571,http://stats.stackexchange.com/questions/183513/implicit-feature-space-of-power-kernel,1,5
658,2015-11-25,2015,11,25,23,3u7wvs,archive.ics.uci.edu is down - Who to contact?,https://www.reddit.com/r/MachineLearning/comments/3u7wvs/archiveicsuciedu_is_down_who_to_contact/,[deleted],1448461976,[deleted],2,0
659,2015-11-25,2015,11,25,23,3u7z9q,Mixture Density Networks with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/3u7z9q/mixture_density_networks_with_tensorflow/,gwulfs,1448463029,,1,16
660,2015-11-25,2015,11,25,23,3u7zjs,Aprendizado de mquina com Microsoft Azure #2,https://www.reddit.com/r/MachineLearning/comments/3u7zjs/aprendizado_de_mquina_com_microsoft_azure_2/,EgonKirchof,1448463155,,0,0
661,2015-11-26,2015,11,26,0,3u85gl,Machine Learning and CBT,https://www.reddit.com/r/MachineLearning/comments/3u85gl/machine_learning_and_cbt/,byedit,1448465750,[removed],0,1
662,2015-11-26,2015,11,26,1,3u8f7z,Is there any hardware technology to watch for machine learning in the relatively near future?,https://www.reddit.com/r/MachineLearning/comments/3u8f7z/is_there_any_hardware_technology_to_watch_for/,onlyml,1448469815,"How long do you think GPUs will dominate the industry? What other tech, either in development or already existing, do you think could provide a competitive advantage going forward? I'm wondering about both generally useful stuff (in the sense a GPU is) and application specific hardware (e.g. I've heard FPGA are somewhat useful for evaluation, but useless for learning).",18,2
663,2015-11-26,2015,11,26,2,3u8k0q,"Get Started Building Intelligent, Serverless Apps Using AWS Lambda and Algorithmia",https://www.reddit.com/r/MachineLearning/comments/3u8k0q/get_started_building_intelligent_serverless_apps/,3eyedravens,1448471711,,0,0
664,2015-11-26,2015,11,26,2,3u8m3z,Multilayer Perceptrons and Event Classification with data from CODEC using Scilab and Weka,https://www.reddit.com/r/MachineLearning/comments/3u8m3z/multilayer_perceptrons_and_event_classification/,chtef,1448472576,,0,0
665,2015-11-26,2015,11,26,2,3u8n4c,Can anyone recommend some medical datasets for my machine learning group project?,https://www.reddit.com/r/MachineLearning/comments/3u8n4c/can_anyone_recommend_some_medical_datasets_for_my/,[deleted],1448472955,[deleted],5,0
666,2015-11-26,2015,11,26,2,3u8nga,"[1509.09308] Fast Algorithms for Convolutional Neural Networks ""10 Effective TFLOPS on an NVIDIA Titan X""",https://www.reddit.com/r/MachineLearning/comments/3u8nga/150909308_fast_algorithms_for_convolutional/,[deleted],1448473092,[deleted],0,1
667,2015-11-26,2015,11,26,2,3u8np6,"[Update] Fast Algorithms for Convolutional Neural Networks ""10 Effective TFLOPS on an NVIDIA Titan X""",https://www.reddit.com/r/MachineLearning/comments/3u8np6/update_fast_algorithms_for_convolutional_neural/,anyonetriedthis,1448473192,,4,13
668,2015-11-26,2015,11,26,5,3u9cdw,"Object Detection in the ""Wild""",https://www.reddit.com/r/MachineLearning/comments/3u9cdw/object_detection_in_the_wild/,pjreddie,1448482784,,34,110
669,2015-11-26,2015,11,26,5,3u9ge8,Where to find terabyte-size dataset for machine learning,https://www.reddit.com/r/MachineLearning/comments/3u9ge8/where_to_find_terabytesize_dataset_for_machine/,mthemove,1448484437,,4,15
670,2015-11-26,2015,11,26,6,3u9lf7,Realistic hand pose estimation without a GPU 40fps+,https://www.reddit.com/r/MachineLearning/comments/3u9lf7/realistic_hand_pose_estimation_without_a_gpu_40fps/,kingkdex,1448486570,,1,16
671,2015-11-26,2015,11,26,6,3u9lts,Rotation Forest - A Classifier Ensemble based on on feature extraction,https://www.reddit.com/r/MachineLearning/comments/3u9lts/rotation_forest_a_classifier_ensemble_based_on_on/,cast42,1448486753,,0,1
672,2015-11-26,2015,11,26,6,3u9p27,"Hi, guys. I'm a senior in high school and I was looking if anyone knows of any summer programs or internships on machine learning?",https://www.reddit.com/r/MachineLearning/comments/3u9p27/hi_guys_im_a_senior_in_high_school_and_i_was/,abs6969,1448488092,"I don't have a strong background in statistics or python, but I am proficient in Java and advanced calculus. I am also willing to learn stats and python before the summer of 2016, which is when I hope to start an unpaid internship. Please help!",5,0
673,2015-11-26,2015,11,26,7,3u9sai,What books are best as an introduction to machine learning for a math major?,https://www.reddit.com/r/MachineLearning/comments/3u9sai/what_books_are_best_as_an_introduction_to_machine/,fd9573f5x0,1448489448,"I know which books are recommended for people who may not know too much math, but I'm studying math at a UK university and (as far as I'm aware) I know all the main areas of mathematics needed for machine learning.

I've heard that ""Machine Learning: a Probabilistic Perspective"" is good, but is it suitable for a beginner? Would it be better to start with an easier book such as ""Introduction to Statistical Learning""?",8,12
674,2015-11-26,2015,11,26,8,3u9zhc,Cat Swarm Optimization,https://www.reddit.com/r/MachineLearning/comments/3u9zhc/cat_swarm_optimization/,machinegaze,1448492570,,3,4
675,2015-11-26,2015,11,26,9,3uaamu,PhD in Probability/Stochastic Processes and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3uaamu/phd_in_probabilitystochastic_processes_and/,topiary320,1448497772,"I'm thinking about doing a PhD in a probability related field and from a little bit of reading on the subject, it seems probability and machine learning go hand in hand. I just wanted to find out a bit more information about how one might go about linking the two fields. I am currently doing a masters in Financial Maths so I guess I'd want to apply a PhD to finance related topics, and machine learning could definitely be incorporated into this mix.

Could anyone give me some information about how researchers use probability to develop machine learning techniques? I'd want to focus more on the probability side of the field because I would definitely not consider myself a computer scientist (although I can programme and probably will learn more about the field in the coming year) . A browse on the internet has just mentioned the areas of probability that are used, not any real techniques or applications. Can anyone explain how areas like Brownian Motion or Martingale Theory are used in machine learning? Anyone know of any concrete links with quantitative finance?

Websites/papers/article recommendations would be great! thanks",1,1
676,2015-11-26,2015,11,26,9,3uablk,WT of [1511.06392] Neural Random-Access Machines (Google),https://www.reddit.com/r/MachineLearning/comments/3uablk/wt_of_151106392_neural_randomaccess_machines/,[deleted],1448498236,[deleted],0,1
677,2015-11-26,2015,11,26,9,3uacea,Google TensorFlow &amp; Machine Learning Will Kill Search Engine Keyword Tools For SEO,https://www.reddit.com/r/MachineLearning/comments/3uacea/google_tensorflow_machine_learning_will_kill/,eric55,1448498637,,2,0
678,2015-11-26,2015,11,26,9,3uacrq,WT of [1511.06392] Neural Random-Access Machines (Google),https://www.reddit.com/r/MachineLearning/comments/3uacrq/wt_of_151106392_neural_randomaccess_machines/,[deleted],1448498834,[deleted],0,0
679,2015-11-26,2015,11,26,11,3uarys,Lecture Note for &lt;NLP with Distributed Representation&gt; on arXiv Now,https://www.reddit.com/r/MachineLearning/comments/3uarys/lecture_note_for_nlp_with_distributed/,matiskay,1448506447,,0,6
680,2015-11-26,2015,11,26,13,3uazks,Face rocognition open source code using deep learning,https://www.reddit.com/r/MachineLearning/comments/3uazks/face_rocognition_open_source_code_using_deep/,manhquang144,1448510645,I am studying about deep learning. Does anyone have a open source code using deep learning for Face recognition task. Please give it to me. Thanks a lot,2,0
681,2015-11-26,2015,11,26,13,3ub4n1,Legality Of AI,https://www.reddit.com/r/MachineLearning/comments/3ub4n1/legality_of_ai/,PollyWantAToilet,1448513478,"Okay, I imagine this is going to be difficult question for anyone to answer but i decieded to post here instead of r/law for several reasons, if no one answers ill probally post there too, but i was wonder if a machine saw that it would be in its best interest to do something illegal and did it, would the creator of the program or user who executed be imprisioned or what?",8,0
682,2015-11-26,2015,11,26,15,3ubfkr,Weka: how to add an attribute through GUI?,https://www.reddit.com/r/MachineLearning/comments/3ubfkr/weka_how_to_add_an_attribute_through_gui/,csInterviewQ,1448519632,"Hi guys,

I'm using the WEKA GUI to do some text classification. I'm not that experienced with Machine Learning, but I understand it at a high level. 

Right now I'm testing a set of attributes for binary classification of text. I want to add another attribute, specifically mean word concreteness, to the classifier. How do I go about doing this, preferably through the GUI?

If I can't do it through the GUI then:

Can I convert the ARFF to a CSV file, write the values onto another column in the CSV file, re-convert to ARFF and run the classifier with a new column of attributes?

Or do I need to write Java code to add an attribute? Can I add just concreteness or would I have to write code for all of the attributes I'm interested in? How would I go about programming a script to do this?

Is there any change that I need to make to the actual classifier?

Again, I'm kind of new at this so please correct any misconceptions I have. I would also appreciate some beginner tutorials on creating attributes, preprocessing, and Weka in general.",0,0
683,2015-11-26,2015,11,26,16,3ubkfy,I've been training a K-Means classifier for 40 hrs; how long will it take to finish?,https://www.reddit.com/r/MachineLearning/comments/3ubkfy/ive_been_training_a_kmeans_classifier_for_40_hrs/,mkgrean,1448522672,"I'm a newbie on machine learning. I'm training a kmeans classifier for bag of visual words purposes. I followed the Zisserman's approach (~1000 clusters).

It started to train the classifier 40 hrs ago, still going on. I was wondering how long will it take to finish?

Max iterations = 300

Dimensions = 128

\# of inputs = 2047506

\# of clusters = 1000

BTW, it runs on a Windows Server with a Intel Xeon E5-1650 @ 3.50 GHz, 16 GB RAM. Using scikit-learn library of Python to classify.

Another question: I don't know if each of the 2047506 inputs is unique. Should I eliminate the re-occurring SIFT descriptors beforehand to decrease the number of input?",16,0
684,2015-11-26,2015,11,26,17,3ubqst,"WT of [1511.08130] A Roadmap towards Machine Intelligence (Facebook, U Trento)",https://www.reddit.com/r/MachineLearning/comments/3ubqst/wt_of_151108130_a_roadmap_towards_machine/,[deleted],1448527014,[deleted],2,0
685,2015-11-26,2015,11,26,19,3ubxte,Advice on string comparison,https://www.reddit.com/r/MachineLearning/comments/3ubxte/advice_on_string_comparison/,personalityson,1448532004,"Hi,
I want to try to use machine learning in record linkage: Two databases with names of companies and addresses, which are maintained separately, need to be linked together. The address part is more or less easy to compare, but to give a credible likeness of two names is a little more difficult. Raw string comparison does not pick on synonymous words. I have already linked data to train on (matched by humans), and non-matches are easy to generate.

Each name is no more than 40 characters long.


What would you have done?

 
Some examples



Kreisklinik Alt/Neutting -- vs -- Kreiskrankenhaus Alt/Neutting

Wohn- und Pflegeheim Stephanihof -- vs -- Senioren-u. Pflegeheim Stephanihof	

Universittsklinikum Bonn - Verwaltung Abt. 02.2 -- vs -- Uni-Klinikum Bonn",6,0
686,2015-11-26,2015,11,26,19,3uby5l,"A Roadmap towards Machine Intelligence (Mikolov, Joulin, Baroni)",https://www.reddit.com/r/MachineLearning/comments/3uby5l/a_roadmap_towards_machine_intelligence_mikolov/,onewugtwowugs,1448532224,,4,35
687,2015-11-26,2015,11,26,20,3uc2un,Recognizing Functions in Binaries with Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3uc2un/recognizing_functions_in_binaries_with_neural/,galapag0,1448535680,,0,16
688,2015-11-26,2015,11,26,21,3uc8rq,What Happens When The Bearing Is Lubricated With The Running Motor?,https://www.reddit.com/r/MachineLearning/comments/3uc8rq/what_happens_when_the_bearing_is_lubricated_with/,jackerfrinandis,1448539840,,0,1
689,2015-11-26,2015,11,26,22,3ucdfi,Jumping Jacks on Construction Sites,https://www.reddit.com/r/MachineLearning/comments/3ucdfi/jumping_jacks_on_construction_sites/,Esichoice,1448543030,,0,1
690,2015-11-26,2015,11,26,22,3ucfa6,Data prep question,https://www.reddit.com/r/MachineLearning/comments/3ucfa6/data_prep_question/,nomi2k,1448544188,"Hi,

i'm relative new to that field and my questions is maybe not that smart but i want to make sure I'm correct here.

If I have two lists for example:

List 1: 
Name, age, gender, destination

List 2: 
Destination, distance to destination (from us)

To get better results i want to integrated the list2 in list1 so i have
Name, age, gender, destination, distance

Thats for my train set.
In my test set i can't do that because i want to predict the destination. 
My test set would only be Name, age, gender,

Should the results be better if i add the informations from the second list to my train set? So the system learns that a 85 years old male don't travel to Germany because its too faraway.
And then in the test set the systems don't predict Germany because the user is 85 years old and male.
Is that the correct way?

(sorry for my bad english)
(fyi its for kaggle airbnb comp)

",3,0
691,2015-11-26,2015,11,26,22,3ucj46,Machine Learning with R Online Certification,https://www.reddit.com/r/MachineLearning/comments/3ucj46/machine_learning_with_r_online_certification/,jatingoel,1448546285,,0,1
692,2015-11-26,2015,11,26,23,3uco2d,Detecting In-App Purchase Fraud with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3uco2d/detecting_inapp_purchase_fraud_with_machine/,gurdotan,1448548922,,0,19
693,2015-11-27,2015,11,27,0,3ucvji,Distributed Machine Learning Libraries,https://www.reddit.com/r/MachineLearning/comments/3ucvji/distributed_machine_learning_libraries/,doublebyte1,1448552649,"I am reviewing available scalable ML libraries (preferentially with an Open-Source-* license) and I see the concept of scalable is a bit tricky. Sometimes people identify scalable with fast (as in ""you can scale up easily, without compromising efficiency"") and point to languages that allow you a strict control of the memory (such as C++) and a programming style that focus on efficiency (such as templates). Other times I see scalable referred on the context of a single-machine, as scaling to multiple cores. All this claims are relevant, but what I am actually looking for are libraries that can scale horizontally at machine level; i.e.: to a cluster. I believe that after ""exhausting the other artefacts"" - efficient programming style and appropriated use of machine cores - this would be the ultimate path.
IMHO, to go this path you need to implement or rely on a third-party file management system; this is because if you are going to care about distributed analysis, the chances are you have a lot of data and you also want to have distributed storage (and storage management).
The libraries that I found, that comply with these requirements are: MLib, Mahout (till a certain extent), H2O, Flink, Mahout and Deeplearning4j (Deep Learning only).
I was wondering if someone could point me to any other libraries that should be on this list.",7,1
694,2015-11-27,2015,11,27,1,3ud2m2,Where can I find an implementation of GPU friendly sparse linear batches for torch.,https://www.reddit.com/r/MachineLearning/comments/3ud2m2/where_can_i_find_an_implementation_of_gpu/,shrimpMasta,1448555805,"I am training a language model, the size of the vocabulary is about 420k unique tokens. I'm planning to use a sparse linear input layer and a hierarchical softmax (HS) output layer to be able to train. I could find and implementation of HS in the fbcunn package, I've also found https://github.com/fstrub95/nnsparse for sparse linear batches, but it seems there is no GPU support yet. Any idea where to find that, or any advice on how people are training model with large vocabulary size in literature ? 

",1,1
695,2015-11-27,2015,11,27,2,3uddgg,GPU Cluster recommendations,https://www.reddit.com/r/MachineLearning/comments/3uddgg/gpu_cluster_recommendations/,cruvadom,1448560457,"Hi,
I am interested in buying a machine to handle 8 * nvidia K80 GPUs for my research team in university, where we conduct research in Deep Learning. Unfortunately, we are no experts in hardware, and servers in particular. Can someone please give me some general guidelines about minimal and recommended requirements that such machine should have? I would highly appreciate it. In case you can guide me a little bit, you might want to refer to: supporting CPUs, cooling system, minimum RAM requirements, minimum network requirements, what should be the price range, and so on. 

Thanks!",16,7
696,2015-11-27,2015,11,27,3,3udgbx,Machine learning in home automation [x-post /r/homeautomation],https://www.reddit.com/r/MachineLearning/comments/3udgbx/machine_learning_in_home_automation_xpost/,blpst,1448561736,"Hello,

I would like to know if anyone has experience in machine learning or trend detection algorithms used in home automation who would point me in the right direction to learning about these.

Idealy what I would like to create is a system which learns based on the users behavior. Starting example case: Motion is detected in the bathroom every morning around 7, the system should learn that and an action is associated ex: the heating should turn on as well as the lights.

I can think of some algorithms based on frequency, but there must be a more robust way with machine learning.

If someone could point me into the right direction on which algorithms could be used or stuff I should look at that'd be appreciated!




",4,5
697,2015-11-27,2015,11,27,3,3udlsv,Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/3udlsv/actormimic_deep_multitask_and_transfer/,vonnik,1448564197,,0,14
698,2015-11-27,2015,11,27,4,3udmfh,CIFAR-100 Result with Exponential Linear Units and Randomized ReLU,https://www.reddit.com/r/MachineLearning/comments/3udmfh/cifar100_result_with_exponential_linear_units_and/,antinucleon,1448564473,,29,37
699,2015-11-27,2015,11,27,5,3udv97,Any multi-GPU LSTM frameworks?,https://www.reddit.com/r/MachineLearning/comments/3udv97/any_multigpu_lstm_frameworks/,anonDogeLover,1448568385,Does anyone know any deep learning frameworks that can train LSTMs using multiple GPUs?,19,3
700,2015-11-27,2015,11,27,5,3ue0dm,[1511.06303] Alternative structures for character-level RNNs,https://www.reddit.com/r/MachineLearning/comments/3ue0dm/151106303_alternative_structures_for/,downtownslim,1448570748,,6,14
701,2015-11-27,2015,11,27,5,3ue1ol,What happened to OpenMT'15 results? Have winners been announced/papers made publically available?,https://www.reddit.com/r/MachineLearning/comments/3ue1ol/what_happened_to_openmt15_results_have_winners/,bbb2bbb,1448571382,,3,12
702,2015-11-27,2015,11,27,7,3ueeog,IBMs Apache SystemML (incubating),https://www.reddit.com/r/MachineLearning/comments/3ueeog/ibms_apache_systemml_incubating/,based2,1448577877,,0,1
703,2015-11-27,2015,11,27,7,3ueezl,How is the L* algorithm applied in practice?,https://www.reddit.com/r/MachineLearning/comments/3ueezl/how_is_the_l_algorithm_applied_in_practice/,SarcasmMonster,1448578046,"I'm referring to the paper ""Learning Regular Sets from Queries and Counterexamples"" by Dana Angluin in 1987. The paper is widely influential but I have hard time understanding how one would go about implementing the oracle required by the algorithm?",0,0
704,2015-11-27,2015,11,27,8,3uegic,Darpa Alchemy: Open Source AI,https://www.reddit.com/r/MachineLearning/comments/3uegic/darpa_alchemy_open_source_ai/,based2,1448578846,,4,48
705,2015-11-27,2015,11,27,11,3uf02i,Shrink tunnel BS4525 4535,https://www.reddit.com/r/MachineLearning/comments/3uf02i/shrink_tunnel_bs4525_4535/,dongfengpacking,1448589999,,1,1
706,2015-11-27,2015,11,27,11,3uf38b,Shrink tunnel BS 550 650,https://www.reddit.com/r/MachineLearning/comments/3uf38b/shrink_tunnel_bs_550_650/,dongfengpacking,1448591795,,1,1
707,2015-11-27,2015,11,27,11,3uf529,Tong sealer with transformer,https://www.reddit.com/r/MachineLearning/comments/3uf529/tong_sealer_with_transformer/,dongfengpacking,1448592833,,1,1
708,2015-11-27,2015,11,27,12,3uf91p,Grian filling machine,https://www.reddit.com/r/MachineLearning/comments/3uf91p/grian_filling_machine/,dongfengpacking,1448595026,,1,1
709,2015-11-27,2015,11,27,14,3ufitb,Where to start,https://www.reddit.com/r/MachineLearning/comments/3ufitb/where_to_start/,blackwoodstone,1448600626,"I have access to all of the code review data at my company. I want to try and use the data to answer interesting questions things such as determining developer skill. I can get the data into any layout and I have been playing with R, weka and excel to try and get meaningful answers to my questions but have had not a lot of luck.

For example one thing I am trying to learn is if the number of times a developer comments on other developers reviews is and indicator of their skill. My thought is that a more skilled developer would comment more on his peers reviews. To show if a developer has higher skill I think their reviews would have less iterations, generate less comments from others among other things.

Where do I start? What I've done is queried the data in SQL server into csv files. For example I made a file that has two columns. Column one is the amount of comments the developer comments on others and column two is how many times a others comment on that developers review. When I put it in a scatter plot all I see is the data mind of smooshed together. I think the reason is that many reviews are for massive uptake type code changes which do not get many comments. I am not sure what the best way to trim the data is. Is there any specific types of data mining algorithms that I should learn to answer these types of questions? 

When you come to a new set of data like this, what steps do you take to answer questions on it?",0,0
710,2015-11-27,2015,11,27,15,3ufp9s, RecruitmentMachine Learning Expert,https://www.reddit.com/r/MachineLearning/comments/3ufp9s/recruitmentmachine_learning_expert/,Le_Sucre,1448604341,[removed],0,0
711,2015-11-27,2015,11,27,15,3ufqk5,Ensembling using Logistic Regression,https://www.reddit.com/r/MachineLearning/comments/3ufqk5/ensembling_using_logistic_regression/,tehsandvich,1448605123,"I'm having trouble understanding how to do ensemble models using Logistic regression. 
Here is my understanding please correct me if I am wrong.

You split the data into partitions.
You train models on one partition of the data. 
Then predict the classes on the second partition of the data for each of the model.
After which you combine the predicted values from the models and  the actual values.  Then run logistic regression on this dataset and use the coefficients to calculate the weights for the algorithm.
Is this correct?",3,1
712,2015-11-27,2015,11,27,16,3ufvp9,ELI5: Why should I care about deep generative models?,https://www.reddit.com/r/MachineLearning/comments/3ufvp9/eli5_why_should_i_care_about_deep_generative/,XalosXandrez,1448608361,Why not simply stick to vanilla Autoencoders (for unsupervised) and convnets (for supervised)? ,9,32
713,2015-11-27,2015,11,27,16,3ufxyj,How important are Vibratory Plate Compactors?,https://www.reddit.com/r/MachineLearning/comments/3ufxyj/how_important_are_vibratory_plate_compactors/,Esichoice,1448609877,,0,1
714,2015-11-27,2015,11,27,17,3ug3om,How An Expert's Advice Matters When Finding the Oil Lubricator?,https://www.reddit.com/r/MachineLearning/comments/3ug3om/how_an_experts_advice_matters_when_finding_the/,jackerfrinandis,1448614040,,0,1
715,2015-11-27,2015,11,27,17,3ug3qr,How hard would it be to perform image recognition on Lego pieces?,https://www.reddit.com/r/MachineLearning/comments/3ug3qr/how_hard_would_it_be_to_perform_image_recognition/,castigatio,1448614085,"I had a thought that it would be cool if there were a way of digitising the content of a typical ""big box of various lego"" - to catalogue the pieces and allow one to use one of the online databases to identify what you can build with the pieces you have. In my mind I imagine using a phone camera to ""scan"" multiple lego pieces laid out on a surface by passing it over them. Recognising shape and colour would be issues.. ",3,0
716,2015-11-27,2015,11,27,17,3ug45q,What's the deal with RNNs? Are they as OP as they sound?,https://www.reddit.com/r/MachineLearning/comments/3ug45q/whats_the_deal_with_rnns_are_they_as_op_as_they/,rag2rich,1448614406,"I'm still wrapping my head around what's been going in in deep learning the past several years. I've read that RNNs are Turing Complete, so I take this to mean they can learn any algorithm (given enough resources and the right techniques for configuring the weights and topologies).

1. Is this correct? And if so...

2. Is this the main deal with them? 

3. Is this as significant as it seems? 

4. Should I be investing my time learning about how to implement and/or configure them?",12,0
717,2015-11-27,2015,11,27,18,3ug7dy,Why do most LSTM implementations keep multiple copies of same RNN?,https://www.reddit.com/r/MachineLearning/comments/3ug7dy/why_do_most_lstm_implementations_keep_multiple/,wind_of_amazingness,1448616934,"I've been going through some examples of LSTM implementation in Torch (like [this](https://github.com/wojciechz/learning_to_execute/blob/master/main.lua#L148) and [this](https://github.com/karpathy/char-rnn/blob/master/train.lua#L284)) and it puzzles me why the LSTM network is cloned multiple times to get seq_length clones (sharing all weights and grad_params) instead of just using single LSTM and do multiple passes through the same network?

Here is example backward pass from ""learning to execute"":
 
    for i = params.seq_length, 1, -1 do
        state.pos = state.pos - 1
        local tmp = model.rnns[i]:backward({state.data.x[state.pos],     &lt;&lt;&lt;&lt;&lt; rnns[i] are clones of same LSTM
                                                    state.data.y[state.pos + 1],
                                                    model.s[i - 1]},
                                                    { tmp_val, model.ds})[3]
        copy_table(model.ds, tmp)

As I understand forward pass through the network doesn't change anything in it and backward pass only accumulates gradInput values in each node.
Since all params and gradParams are shared across clones it seems having multiple RNNs to be an overkill.

Does anyone understand a reason behing it?",3,11
718,2015-11-27,2015,11,27,18,3ug8iq,"WT of [1511.04119] Action Recognition using Visual Attention (U Toronto, w/GitHub code)",https://www.reddit.com/r/MachineLearning/comments/3ug8iq/wt_of_151104119_action_recognition_using_visual/,[deleted],1448617821,[deleted],1,0
719,2015-11-27,2015,11,27,22,3ugpe1,Convergent Learning: Do different neural networks learn the same representations?,https://www.reddit.com/r/MachineLearning/comments/3ugpe1/convergent_learning_do_different_neural_networks/,Rob,1448630575,,0,26
720,2015-11-28,2015,11,28,1,3uh6sw,A newbie's question about preparation of data for machine learning,https://www.reddit.com/r/MachineLearning/comments/3uh6sw/a_newbies_question_about_preparation_of_data_for/,rammak,1448640302,"So I've been studying about deep learning lately and I have decided to do a project on convolutional neural network using TensorFlow. I have a bunch of jpg images with RGB channels and fixed resolution along with a csv file of labels. 


My problem is that there are many tutorials about how to model and run a network on TensorFlow but there are very little information about how to prepare data which can be plugged into the input of the network to train it. All they have is a MNIST database and CIFAR10 for CNN. In both cases, in tutorials they just import pre-processed data into Python using import at the start. I tried to learn from [cifar10.py](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/image/cifar10/cifar10.py) and [cifar10_input.py](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/image/cifar10/cifar10_input.py) and what I have learned so far is that there are binary files provided in batches and cifar10_input.py slices that data into batches and returns dataset which is suitable for the network. [This](https://www.cs.toronto.edu/~kriz/cifar.html) page has details about internal structure of those binary files but converting jpg images and labels to that format is a daunting task for a newcomer.


I have tried to Google it but I couldn't find much about this topic. Is there any script that I can use to prepare such type of datasets from images and labels file? If not then is there any tutorial which explains these things step by step?




",3,6
721,2015-11-28,2015,11,28,1,3uhdn4,"I (will) have tens (hundreds, maybe) of hours of recording of just myself talking. Can deep learning help me transcribe them semi automatically?",https://www.reddit.com/r/MachineLearning/comments/3uhdn4/i_will_have_tens_hundreds_maybe_of_hours_of/,learnin_no_bully_pls,1448643528,"Like the title says, I'm going to be collecting a couple of hours/day of me talking, every day, at least for a year, all in Italian.


1) I'm guessing that having just my voice to deal with makes the task easier, provided I have a big amount of data like hundreds of hours, is that correct?

2) What is a reasonably simple architecture with example on the web (any framework in python or lua is fine) that I can modify to do this with reasonable accuracy?

I can code the dataset annotation tool, various glue code and other simple stuff (at least to get something rough going on), but I don't know enough math and machine learning to actually implement the state of the art model(tm) from scratch.

Thoughts? Ideas?",15,2
722,2015-11-28,2015,11,28,2,3uhh0k,Interview with deep learning expert Andrej Karpathy,https://www.reddit.com/r/MachineLearning/comments/3uhh0k/interview_with_deep_learning_expert_andrej/,reworksophie,1448645031,,0,1
723,2015-11-28,2015,11,28,2,3uhi5p,(xPost from /r/ComputerVision) Finding grasp points on a new object,https://www.reddit.com/r/MachineLearning/comments/3uhi5p/xpost_from_rcomputervision_finding_grasp_points/,deathlymonkey,1448645556,"Hi! I've been working on trying to implement an algorithm to find grasping points for a robotic arm and I have some questions. I won't ask you to solve the problem for me as my goal is to learn, but I would definitely appreciate if you could set me on the right track! Please keep in mind that I have never really done any computer vision at all (Only in my 2nd year of uni) and I have only been programming for a bit more than a year now.


In this article about finding grasp points for a robotic arm: http://www-cs.stanford.edu/~quocle/icra2010-LeKammKaraNg.pdf


From what I understand, they find the grasp points by finding the gradients at different points and then checking if the vector from a point to the other forms a lign with the gradients. What I'm not sure I understand: - In part III (and briefly V.B), they mention that they used SVM to classify good grasps vs bad grasps. However, they also say that their algorithm can be used on new, unknown objects. How exactly is that possible with SVM? My guess is that the SVM only checks the gradients/angle of the vector between points when it is trying to classify the grasps? If I wanted to implement such a SVM, would I need different images for my learning set or would a single image with multiple grasp points be enough? Also, what kind of object should I be using? A block, a cup, etc?


My second question(quick question):
I'm not sure I understand how to select contact points for the calculation of the gradients (parc IV - A)(They say they compute 10x0 patches around the contact points. Do they simply take every point that corresponds to an edge on the image and use it as a contact point? I feel like doing this could create a lot of error?


Thanks a lot if you can find some time to answer my questions, it would really help me out a lot!",0,1
724,2015-11-28,2015,11,28,4,3uhyy6,Implementing custom loss functions with automatic differentiation. (This is a lot easier than it sounds).,https://www.reddit.com/r/MachineLearning/comments/3uhyy6/implementing_custom_loss_functions_with_automatic/,cavaunpeu,1448653062,,0,6
725,2015-11-28,2015,11,28,4,3ui11j,Applying deep learning to regression task,https://www.reddit.com/r/MachineLearning/comments/3ui11j/applying_deep_learning_to_regression_task/,coskunh,1448653995,"I have model that get images as an input and produces real valued outputs. My outputs are ranging from -5 to 5.(These boundaries are not definite, so i can not scale it )) 

These are ideas i found while doing research, if you have more ideas or you think that it is wrong please let me know.

* Applying dropout not useful especially just before output layer.
* If your output values below than zero, you shouldn't apply Relu(I'm trying pRelu)
* Mean_squared_error better comparing to the root mean squared error and 
mean absolute error.",6,0
726,2015-11-28,2015,11,28,5,3ui5cr,80+ PCI 3.0 Lanes for 3 or more GPU's,https://www.reddit.com/r/MachineLearning/comments/3ui5cr/80_pci_30_lanes_for_3_or_more_gpus/,LeavesBreathe,1448655888,"Hey Guys quick question here.

I'm looking to upgrade my system with four 980 TI's.

The problem with this is the pci lanes. Most CPU's have 40 lanes, meaning that if you have 3 or more GPU's you can only assign 8 lanes per GPU. Naturally, this severely limits the bandwidth of your gpu's. 

So, I'm trying to find a cpu with at least 48 lanes that isn't too pricey ($300 to 500 max). 

I've heard that there are a group of intel cpu's that have 80 lanes, but I simply can't find them! What cpu's would you recommend? I don't plan on using this system for anything but machine learning. So having many CPU cores is not really a benefit. Thanks!

Also what motherboard would you recommend with the cpu?",21,2
727,2015-11-28,2015,11,28,5,3ui5o7,Chatbots and automated online assistants - ML applied,https://www.reddit.com/r/MachineLearning/comments/3ui5o7/chatbots_and_automated_online_assistants_ml/,thesameoldstories,1448656024,,0,1
728,2015-11-28,2015,11,28,6,3uib4m,How hard is it to get research positions at Google/Facebook/Microsoft for AI/ML?,https://www.reddit.com/r/MachineLearning/comments/3uib4m/how_hard_is_it_to_get_research_positions_at/,gfmmfg,1448658589,"I've heard people say that getting research scientist positions at the above places are as difficult as getting tenure-track positions at good (top 25) universities (US), because they are super competitive, providing all the benefits of academic research (and more pay) without the hassles of being in a university, such as teaching, grant-writing, etc.

I somehow find that a little hard to believe, but how true is this? Do people turn down top academic jobs to go work at these companies? ",29,63
729,2015-11-28,2015,11,28,9,3uixax,Can't get a simple keras program to build.,https://www.reddit.com/r/MachineLearning/comments/3uixax/cant_get_a_simple_keras_program_to_build/,realfuzzhead,1448669303,"Edit -- nvm, wasn't using numpy tensors as data input",0,0
730,2015-11-28,2015,11,28,11,3ujbel,[Case Study] Analyzing Job &amp; Skills Trends using Data Science (Web-Scraping &amp; Text-Mining),https://www.reddit.com/r/MachineLearning/comments/3ujbel/case_study_analyzing_job_skills_trends_using_data/,lawrencejones617,1448676837,,0,0
731,2015-11-28,2015,11,28,13,3uju9s,Machine Learning libraries in C#,https://www.reddit.com/r/MachineLearning/comments/3uju9s/machine_learning_libraries_in_c/,vijayrajanna,1448686646,"What are the best possible ways to use ML algorithms in C# ?

1) Is there any well known c# ML library?
2) Should I use Python scikit-learn from C#? How?
3) Or, using Weka from C#

If you have already used one or more of the above-listed methods, please share your experience.
",8,0
732,2015-11-28,2015,11,28,13,3uju9u,Heat transfer printing machine,https://www.reddit.com/r/MachineLearning/comments/3uju9u/heat_transfer_printing_machine/,dongfengpacking,1448686647,,1,1
733,2015-11-28,2015,11,28,15,3uk1qd,Stock Market Forecasting using deep learning ?,https://www.reddit.com/r/MachineLearning/comments/3uk1qd/stock_market_forecasting_using_deep_learning/,m_alzantot,1448690908,"I wonder what models of deep learning can be successful in forecasting future stock market returns from past data. For example, can the LSTM perform well on this task ?? Do you have any examples ?",19,0
734,2015-11-28,2015,11,28,15,3uk2q5,"[1511.06464] Unitary Evolution Recurrent Neural Networks, proposed architecture generally outperforms LSTMs",https://www.reddit.com/r/MachineLearning/comments/3uk2q5/151106464_unitary_evolution_recurrent_neural/,downtownslim,1448691500,,68,43
735,2015-11-28,2015,11,28,15,3uk5mn,Sentence Composition through averaging pre-trained (paraphrastic) Word Vectors outperform LSTMs across tasks (DL),https://www.reddit.com/r/MachineLearning/comments/3uk5mn/sentence_composition_through_averaging_pretrained/,dafcok,1448693389,,0,7
736,2015-11-28,2015,11,28,18,3ukhm2,"Can a Bachelors degree holder directly get into a PhD program in AI at a top lab [Montreal, IDSIA etc]",https://www.reddit.com/r/MachineLearning/comments/3ukhm2/can_a_bachelors_degree_holder_directly_get_into_a/,roschpm,1448702488,"Hello,

Is it even possible? If yes, what are the requirements/constraints? Good Publication record to some extent, programming skills, passion.. Etc 

Thanks",14,0
737,2015-11-28,2015,11,28,18,3ukijg,Data ScienceTech Institute partners with Amazon AWS and launches Spring 2016 entry for its MSc,https://www.reddit.com/r/MachineLearning/comments/3ukijg/data_sciencetech_institute_partners_with_amazon/,datasciencetech,1448703246,,0,1
738,2015-11-28,2015,11,28,20,3ukq3a,Order Matters - Sequence to sequence for sets (Google) - at Wikitract,https://www.reddit.com/r/MachineLearning/comments/3ukq3a/order_matters_sequence_to_sequence_for_sets/,[deleted],1448709293,[deleted],1,0
739,2015-11-28,2015,11,28,21,3ukvc6,Datasets of one to one conversations?,https://www.reddit.com/r/MachineLearning/comments/3ukvc6/datasets_of_one_to_one_conversations/,hapliniste,1448713433,"Hi,

I'm currently playing with Keras and Tensorflow, trying to understand machine learning. I would like to create a chatbot (with Word2Vec and sequence to sequence model).

The problem is that I can't find any ""one to one"" conversations dataset (only datasets of chat with multiple actors).

Do you have some datasets you would recommand me? Or web sources for minning data?

Thanks!",3,1
740,2015-11-28,2015,11,28,23,3ul8ko,Self-Organizing Maps with Googles TensorFlow,https://www.reddit.com/r/MachineLearning/comments/3ul8ko/selforganizing_maps_with_googles_tensorflow/,sachinrjoglekar,1448722073,,9,116
741,2015-11-29,2015,11,29,1,3ullmv,laplacian-modified naive bayesian classifier used in ligand-based virtual screen in combination with ensemble docking with data fusion discovers novel multitarget inhibitor for cancer,https://www.reddit.com/r/MachineLearning/comments/3ullmv/laplacianmodified_naive_bayesian_classifier_used/,allenbk,1448728506,,2,2
742,2015-11-29,2015,11,29,3,3ulz13,notes on Infinite Dimensional Word Embeddings,https://www.reddit.com/r/MachineLearning/comments/3ulz13/notes_on_infinite_dimensional_word_embeddings/,[deleted],1448734271,[deleted],0,0
743,2015-11-29,2015,11,29,3,3um1ob,[1511.06807] Adding Gradient Noise Improves Learning for Very Deep Networks,https://www.reddit.com/r/MachineLearning/comments/3um1ob/151106807_adding_gradient_noise_improves_learning/,downtownslim,1448735352,,6,5
744,2015-11-29,2015,11,29,3,3um4to,char-rnn in tensorflow,https://www.reddit.com/r/MachineLearning/comments/3um4to/charrnn_in_tensorflow/,sherjilozair,1448736644,,2,13
745,2015-11-29,2015,11,29,4,3um6h2,How to 'force' a sparse activation in conv layer.,https://www.reddit.com/r/MachineLearning/comments/3um6h2/how_to_force_a_sparse_activation_in_conv_layer/,mlaway,1448737340,"Say I have a signal which I know is a binary vector convolved with some filter. I want to learn the filter and to do so I want the activation in the convolutional layer to be sparse. Is there a way to encourage the activation to be sparse and in turn make the net learn the underlying filter? I've thought about having binary units, but from what I gather that would make it really hard to learn anything at all.

Also, mildly related, /r/fitness has a thread every monday for stupid questions. Would people be interested in having something similar on /r/machinelearning? Considering that this is a smaller community, maybe have a stickied post for beginner questions?",11,1
746,2015-11-29,2015,11,29,5,3umjiy,"Pedro Domingos gives talk at Google on his book, ""the Master Algorithm""",https://www.reddit.com/r/MachineLearning/comments/3umjiy/pedro_domingos_gives_talk_at_google_on_his_book/,evc123,1448742625,,1,28
747,2015-11-29,2015,11,29,5,3umkvy,Anyone here have any experience with the Actor-Critic method for Reinforcement Learning? If so I'd like to pick your brain,https://www.reddit.com/r/MachineLearning/comments/3umkvy/anyone_here_have_any_experience_with_the/,internet_ham,1448743164,"Hello all, 

I'm currently trying to implement Actor-Critic/RL for a project of mine. My environment/critic is a group of agents who's behavior depends on the actor/critic (another agent in the system). I'm using two ANNs for the actor and critic, and my critic is managing to roughly estimate the system, but I can't seem to find the right cost function to generate the right behavior in the actor. I am trying to teach the actor how to move the agents into a certain position via its own movements. 

My question is mainly about the form of cost function for this kind of problem, but also about discussing the AC approach in general in this context (tips, pitfalls, etc) ",2,0
748,2015-11-29,2015,11,29,5,3umli1,Question regarding inversion of softmax function,https://www.reddit.com/r/MachineLearning/comments/3umli1/question_regarding_inversion_of_softmax_function/,[deleted],1448743407,[deleted],0,1
749,2015-11-29,2015,11,29,8,3un60a,Decision tree learning - why we use entropy/gini over the classification error to grow a tree,https://www.reddit.com/r/MachineLearning/comments/3un60a/decision_tree_learning_why_we_use_entropygini/,[deleted],1448752016,[deleted],0,3
750,2015-11-29,2015,11,29,8,3un6qv,Fitting a model via closed-form equations vs. Gradient Descent vs Stochastic Gradient Descent vs Mini-Batch Learning. What is the difference?,https://www.reddit.com/r/MachineLearning/comments/3un6qv/fitting_a_model_via_closedform_equations_vs/,[deleted],1448752322,[deleted],0,1
751,2015-11-29,2015,11,29,9,3unf8i,What is the best tool/program to draw deep neural networks?,https://www.reddit.com/r/MachineLearning/comments/3unf8i/what_is_the_best_toolprogram_to_draw_deep_neural/,pedromnasc,1448756080,"What is the best tool/program to draw deep neural networks? 
Is there a tool/program that is used for almost everyone?

Thanks!",9,8
752,2015-11-29,2015,11,29,9,3unftv,"Pointer Networks (Google, UC Berkley) - at Wikitract",https://www.reddit.com/r/MachineLearning/comments/3unftv/pointer_networks_google_uc_berkley_at_wikitract/,[deleted],1448756351,[deleted],1,0
753,2015-11-29,2015,11,29,9,3unjdb,Conversational Seq2Seq Networks - Deep Vs Shallow,https://www.reddit.com/r/MachineLearning/comments/3unjdb/conversational_seq2seq_networks_deep_vs_shallow/,LeavesBreathe,1448757930,"Hey Guys,

I noticed that in many Vinyals' papers, the models tend to be shallow, yet have many memory cells.

[Neural Conversational Model Paper](http://arxiv.org/abs/1506.05869): 

2 Layer LSTM, 4096 cells each

[Translation English to French](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)

4 Layer LSTM, 1000 cells each

My main question is: **why are they choosing these shallow types of networks?** Why not go with a 10 layer LSTM, 512 cells each? 

Vinyals, I don't mean to call you out or anything. Its just you happen to be authored in these great papers, so if you see this post, would be happy to hear your thoughts. 

My theory is that they want more cells to remember more of the conversation that occurred in the past. For me, these shallow networks do improve performance versus the 10 layer, 512 cell networks.",2,11
754,2015-11-29,2015,11,29,10,3unn4i,Pipelined TensorFlow in R (WIP),https://www.reddit.com/r/MachineLearning/comments/3unn4i/pipelined_tensorflow_in_r_wip/,terrytangyuan,1448759692,,2,27
755,2015-11-29,2015,11,29,14,3uoiro,VGG feature visualizer in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/3uoiro/vgg_feature_visualizer_in_tensorflow/,[deleted],1448775628,[deleted],1,1
756,2015-11-29,2015,11,29,21,3upe5k,impoetance of the ECC feature of a GPU for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3upe5k/impoetance_of_the_ecc_feature_of_a_gpu_for_deep/,cruvadom,1448798538,"Hi all,
I am about to buy a server machine with 8 GPUs on it, and I am thinking about Titan X GPUs. The machine will be used for Deep Learning and other machine learning methods. I heard somewhere that on a server machine, the lack of the ECC feature in the Titan X GPUs can be a problem. Do you know anything about the importance of ECC for Deep Learning?",10,3
757,2015-11-29,2015,11,29,21,3upfo8,Tesla M40 GPU,https://www.reddit.com/r/MachineLearning/comments/3upfo8/tesla_m40_gpu/,cruvadom,1448799720,"Hi all, 
Nvidia is about to release the Tesla M40 GPU which is sort of dedicated for Deep Learning, though I could not find too many details on the web about it. Anyone knows something about one of the following?
a) When will it be released
b) What will be the price
c) Performance with Deep Learning, compared to Titan X or K40
d) Assume i want to buy some GPUs in the next two months, should I wait for the M40 or just get the best thing I can get now? ",3,2
758,2015-11-29,2015,11,29,22,3upmss,[Question] Is there a formal equivalence between clustering of items represented as a network through community detection and traditional clustering e.g. k-means?,https://www.reddit.com/r/MachineLearning/comments/3upmss/question_is_there_a_formal_equivalence_between/,rutherfordofman,1448804838,"I'm wondering if there are any circumstances under which these two methods are equivalent? The network view and associated algorithms are all generally physics-inspired whereas regular clustering is very typical of information retrieval and machine learning.

In the network approach, we have single number which defines pairwise 'similarity' or 'distance'. Based on this we generally maximise the modularity so that nodes in the same cluster have more links between themselves than to others outside the cluster (see [Louvain](https://perso.uclouvain.be/vincent.blondel/research/louvain.html) and [Infomap](http://mapequation.org/code.html) methods).

On the other hand, the clustering picture tends to give us items with a position in some n-dimensional space from which we can calculate the pairwise distance as above. Then in k-means for example, the clusters mid-points are defined to partition the items in that space into well separated groups.",1,1
759,2015-11-29,2015,11,29,22,3upneb,Creating An Open Source Dataset Sharing Platform,https://www.reddit.com/r/MachineLearning/comments/3upneb/creating_an_open_source_dataset_sharing_platform/,mrborgen86,1448805244,"Hello there.

My name is Per Borgen, I'm a norwegian developer and machine learning hobbyist.

PROBLEM:

Missing the right dataset is often what stops me from continuing with my machine learning ideas. I suspect other data scientist/hobbyists have felt this pain too. The raw data is out there somewhere, and in some cases it might also exist as a clean dataset. It's just out of reach.

SOLUTION:

So what if there was an open source platform where you could easily share and request datasets? Perhaps even pay people to gather the data you want.
This should be a site that is fun to browse through. And it should be extremely easy to view all available datasets in your domain of interest. Over time it could contain all the worlds publicly available datasets.

NEXT STEP:

So I've built a tiny MVP in order to see try and confirm my hypothesis, and now I'd like people to test it.

The alpha version of the product can be found here: 
https://frozen-ocean-7041.herokuapp.com/

BE AWARE: 

It's not ready to be 'launched', it has CRAPPY design, and has only got a few dummy datasets which aren't documented properly. But as the url was requested I might as well share it.

If you are interested in testing my service I would be forever grateful. Please send me a PM, add a comment or mail me at perhborgen@gmail.com.

Cheers",22,37
760,2015-11-29,2015,11,29,23,3upodo,Using Keras LSTM RNN for variable length sequence prediction,https://www.reddit.com/r/MachineLearning/comments/3upodo/using_keras_lstm_rnn_for_variable_length_sequence/,curryage,1448805862,"I have a set of sequences S_1,S_2 ... S_M which form my training set. Each sequence is the form S_i = {(x1,y1),(x2,y2)} where x's are real valued numbers and y's are corresponding labels from a fixed alphabet. *The length of each sequence S_i may be different*.  Suppose I have a test sequence S = {x1,x2 ...xN}. I would like to use the training set to predict the best label sequence {y1,y2 ...yN}. How can I use Keras LSTM for this ?

Most examples/posts seem to be on sentence generation/word prediction. Is it possible to use Keras LSTM functionality to predict an output sequence ? The work on [sequence-to-sequence learning](http://arxiv.org/abs/1409.3215) seems related. Assuming that to be the case, my problem is a specialized version : the length of input and output sequences is the same.
",5,3
761,2015-11-30,2015,11,30,0,3upxbm,Best Library or Way to Implement Collaborative Filtering?,https://www.reddit.com/r/MachineLearning/comments/3upxbm/best_library_or_way_to_implement_collaborative/,FR_STARMER,1448810722,"I have a backend that will be receiving whether or not a user likes a certain product from the frontend mobile application. This information is stored in a user profile that will be used to be able to predict what products the user likes and thus display those to them in the future.

I know that collaborative filtering is the best tool to accomplish this, but I'm having a hard time finding a framework that is able to handle this. What's the best resource out there?",3,1
762,2015-11-30,2015,11,30,1,3uq213,Tutorial: create a machine learning pipeline for Iris classification,https://www.reddit.com/r/MachineLearning/comments/3uq213/tutorial_create_a_machine_learning_pipeline_for/,ahousley,1448812954,,0,1
763,2015-11-30,2015,11,30,2,3uqazs,Populating Hyperspace: how to properly generate points in high dimensions,https://www.reddit.com/r/MachineLearning/comments/3uqazs/populating_hyperspace_how_to_properly_generate/,TheSwitchBlade,1448816666,,3,13
764,2015-11-30,2015,11,30,2,3uqgzj,Question regarding inversion of softmax function (x-post from /r/MLQuestions),https://www.reddit.com/r/MachineLearning/comments/3uqgzj/question_regarding_inversion_of_softmax_function/,medakk,1448819035,"I've been reading [Michael Nielsen's online book](http://neuralnetworksanddeeplearning.com/) on neural networks and deep learning and I've been struggling over [this problem](http://neuralnetworksanddeeplearning.com/chap3.html#problem_905066) for the past few days.

Given the activation for a neuron 'a', I have to arrive at an expression for the weighted input 'z' in the form of

    z = ln(a) + C

where C is a constant independent of the neuron being considered. (I've omitted the 'j' and 'L' notation which represent the neuron number and the final layer of neurons respectively)

The closest I can derive is


    z = ln(a) + ln(e^z + C)


by working from the definition of the softmax function, but that is obviously not what the problem in question requires.

I'm stumped by this and would appreciate any form of help. I've posted this on /r/MLQuestions, but that sub doesn't seem to have much activity.",8,1
765,2015-11-30,2015,11,30,3,3uqm3d,A range of Caffe-compatible Deep Neural Net Models for Image Classification,https://www.reddit.com/r/MachineLearning/comments/3uqm3d/a_range_of_caffecompatible_deep_neural_net_models/,pilooch,1448820923,"I've seen the request for image classification models surfacing here a few times. I've put a few models out, ready for download. They range from clothing to furnitures, buildings and planes: http://www.deepdetect.com/applications/model/

The models are from an automated pipeline mostly (and free even for commercial use). As such they are not too costly to produce and I can accomodate a few more, this is open to discussion.

It should be fairly easy to convert the models to TF, Torch and others (mxnet?), so I've put the links to some code I know to do so in the post.",5,14
766,2015-11-30,2015,11,30,3,3uqq0n,[1511.06015] Active Object Localization with Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/3uqq0n/151106015_active_object_localization_with_deep/,zZJollyGreenZz,1448822467,,2,14
767,2015-11-30,2015,11,30,6,3urk11,Machine Learning book recommendation,https://www.reddit.com/r/MachineLearning/comments/3urk11/machine_learning_book_recommendation/,[deleted],1448834265,[deleted],4,0
768,2015-11-30,2015,11,30,7,3urkvj,Graduating soon. Looking for advice and criticism.,https://www.reddit.com/r/MachineLearning/comments/3urkvj/graduating_soon_looking_for_advice_and_criticism/,AfraidOfToasters,1448834585,"I'm getting my bachelor's in CS soon and want to eventually become a data scientist. I've taken a lot of courses which were graduate/undergraduate combined (machine learning, Artificial intelligence, Neural modeling, Neural net Mathematics, Intelligent Systems analysis, *Intelligent Systems Design). Most of my experience is on paper and not in code (i.e. deriving weight update rules designing and analyzing functions). I'm planning on trying to implement an idea I have over the winter break so I have something to show people but it seems like there aren't many entry level positions in the field. How do I get my foot in the door? I've seen some talk about kaggle and I'll see how well I can do but it seems like an indirect path. I would consider going to graduate school but it doesn't seem to be an option at the moment and I'm not sure I would be learning much more than what I can learn on my own at this point. 


This is my current plan:

Patch any holes left in my calculus

Do a project that shows my ability to design an intelligent system

Gain some more experience with Python and R (not really sure what I should do but these seem common)


I'm really looking for any advice or criticism on what I should do.
",13,0
769,2015-11-30,2015,11,30,9,3us3nv,5 Common Misconceptions About Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3us3nv/5_common_misconceptions_about_deep_learning/,clbam8,1448842098,,0,0
770,2015-11-30,2015,11,30,9,3us4qv,"[1511.05641] Net2Net: Accelerating Learning via Knowledge Transfer (Google, U Wash - Source Code) - at Wikitract",https://www.reddit.com/r/MachineLearning/comments/3us4qv/151105641_net2net_accelerating_learning_via/,[deleted],1448842547,[deleted],0,0
771,2015-11-30,2015,11,30,10,3ushuq,Specific examples of algorithm design?,https://www.reddit.com/r/MachineLearning/comments/3ushuq/specific_examples_of_algorithm_design/,VerticesII,1448847887,"I'm an undergrad majoring in Data Science, and I have a machine learning/data mining unit next year, I've been looking into it and it seems like a fascinating area! The problem is I've only really been able to find conceptual descriptions, e.g. that recursion can play a big part. 

I'm looking for some examples, (pseudo or otherwise) that show the specific structure these algorithms. I assume there's more than one way to do it, but if there's a general overarching structure I'd be keen to see some good examples. 

Thanks!",1,0
772,2015-11-30,2015,11,30,10,3usizj,[1511.08400] Regularizing RNNs by Stabilizing Activations,https://www.reddit.com/r/MachineLearning/comments/3usizj/151108400_regularizing_rnns_by_stabilizing/,alecradford,1448848359,,26,27
773,2015-11-30,2015,11,30,10,3usj9b,Could DeepDream or DeepStyle be applied to audio as well?,https://www.reddit.com/r/MachineLearning/comments/3usj9b/could_deepdream_or_deepstyle_be_applied_to_audio/,Chondriac,1448848474,"Most of you are probably familiar with Google's ""Inceptionism: Going Deeper With Convolutions"" paper and the resulting phenomenon people are calling DeepDream. There's also been a method established that uses a similar design to ""paint"" an image in the style of another image, as seen in [this sub](https://www.reddit.com/r/deepstyle/).

My question is, would something like this be possible using audio, eg. morph one song into the style of another? I imagine you would have to use a recurrent model rather than convolutions but is this possible in theory and if so has it been done?",7,1
774,2015-11-30,2015,11,30,11,3usovq,Why don't we put RNN hidden states in the main memory?,https://www.reddit.com/r/MachineLearning/comments/3usovq/why_dont_we_put_rnn_hidden_states_in_the_main/,[deleted],1448850754,"Storing RNN hidden state values takes O(MNT) space, where M is the minibatch size, N is hidden state vector dimension and T is the unrolling factor/number of timesteps. For a vanilla RNN of M = 128, N = 1024 and T = 1024 (RNNs with attention, LSTMs and uRNNs can easily model dependencies this long) with no inputs and using fp32, hidden state alone occupies 0.5 GB. Taking account into input state, RNN stacking and more complex modules like LSTM/GRU, this number could easily blow up to be several times bigger than that to the point GPU memory becomes a limiting factor when training RNNs.

Here's how storing RNN hidden states in the main memory works. In the forward pass, RNN states are computed and subsequently transferred D2H to the main memory via PCIe [in an overlapping manner](http://devblogs.nvidia.com/parallelforall/how-overlap-data-transfers-cuda-cc/), while in the mirrored backward pass, RNN states are transferred H2D and used to propagate error backwards and update weights. The only things GPU stores consistently throughout the training process are the weight matrices, which are in the manageable order of O(N\^2).

This would only work efficiently if the computation on the GPU is not much faster than the data transfer. Here in a ballpark estimate we assume we have a Titan X operating at 1250 MHz under GPU boost. It would reach a theoretical peak speed of 3072 cores * 1250 MHz * 2 ops/FMA = 7.68 TFLOPS which is a very generous estimate considering [you'll need Nervana's GEMM to get a performance even close to that](http://svail.github.io/). With the above vanilla RNN example, computing one timestep on one minibatch requires 2 * N * N * M = 268.4 Mops and it will take 0.03495 ms to do so ignoring all copy costs and overhead. Assuming a measured-by-myself 11 GB/s transfer rate over PCIe 3.0 x16 with pinned host memory (theoretical peak is 16 GB/s), transferring one timestep minibatch worth of hidden state vector of size N * M * 4 bytes = 524.3 kB takes 0.04439 ms which would allow the GPU to work at an efficiency of 78%! In reality transferring such small amount of data will incur heavy overhead, but you can batch together several timesteps to alleviate that.

Of course, when you use LSTM/GRU and factor in input etc. things get much more complicated, but there's one crucial thing we can take away from this: computation cost grows quadratically w.r.t. N, while transfer cost only grows linearly. This mean we would actually BENEFIT from choosing a larger hidden state if we put RNN states in the main memory. Again with the above example, assuming we now transfer over PCIe 3.0 x8, we would only need a hidden state size of 2602 (for a computationally ""round"" number, use 2560) for the transfer speed to break even with the computation speed.",11,23
775,2015-11-30,2015,11,30,12,3ut130,X-Posted to HN: I want to make an email based machine learning tool for high value items,https://www.reddit.com/r/MachineLearning/comments/3ut130/xposted_to_hn_i_want_to_make_an_email_based/,[deleted],1448855773,[deleted],0,0
776,2015-11-30,2015,11,30,13,3ut28i,"Looking for a recent paper on deep learning; it used signedness and bitshifts for inference, ideal for embedded neural nets",https://www.reddit.com/r/MachineLearning/comments/3ut28i/looking_for_a_recent_paper_on_deep_learning_it/,qsucvatz,1448856250,"I first saw it on Hacker news within the last two months, and searching for these ideas has turned up empty.",5,3
777,2015-11-30,2015,11,30,13,3ut2nf,How has the symbolic AI approach contributed to ML?,https://www.reddit.com/r/MachineLearning/comments/3ut2nf/how_has_the_symbolic_ai_approach_contributed_to_ml/,[deleted],1448856427,[deleted],10,13
778,2015-11-30,2015,11,30,15,3utjkc,Food vacuum packing machine DF-250,https://www.reddit.com/r/MachineLearning/comments/3utjkc/food_vacuum_packing_machine_df250/,dongfengpacking,1448865262,,1,1
779,2015-11-30,2015,11,30,15,3utkvf,Vacuum packing machine DZ 300T,https://www.reddit.com/r/MachineLearning/comments/3utkvf/vacuum_packing_machine_dz_300t/,dongfengpacking,1448866011,,1,1
780,2015-11-30,2015,11,30,16,3utrc9,Cross-Lingual Plagiarism Detection with Scikit-Learn,https://www.reddit.com/r/MachineLearning/comments/3utrc9/crosslingual_plagiarism_detection_with_scikitlearn/,blowjobtransistor,1448870018,,0,0
781,2015-11-30,2015,11,30,17,3utuq4,Bidirectional Helmholtz Machines (paper+code),https://www.reddit.com/r/MachineLearning/comments/3utuq4/bidirectional_helmholtz_machines_papercode/,samim23,1448872435,,2,11
782,2015-11-30,2015,11,30,18,3utxjn,Beginner in machinelearning and tensorflow has a noob question,https://www.reddit.com/r/MachineLearning/comments/3utxjn/beginner_in_machinelearning_and_tensorflow_has_a/,tegna,1448874467,"So I just began with machinelearning, reading up on stuff, looking on youtube for informational videos. But there's one thing I don't see that's really being covered (or i'm just not looking at the right places). How do you actually give an unlabeled image to your model to give you the probability what it is.
For example I'm doing the tutorials on Tensorflow, I'm now working with Convolutional Neural Networks tutorial. Teaching the model the mnist dataset. But how can I take a picture of a number and give it to the model so it can give me the probability of what that number could be. I can't find any code examples do this. Could you guys point me in the right direction?",5,0
783,2015-11-30,2015,11,30,23,3uuqau,BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies (Million Word vocabulary can be learned on a single Machine in a week),https://www.reddit.com/r/MachineLearning/comments/3uuqau/blackout_speeding_up_recurrent_neural_network/,muktabh,1448892536,,7,26
784,2015-11-30,2015,11,30,23,3uuu3c,Non-gradient based pre-training?,https://www.reddit.com/r/MachineLearning/comments/3uuu3c/nongradient_based_pretraining/,ekerazha,1448894361,"Because of the local minima problem in neural networks, wouldn't it make sense to pre-train NNs using a non-gradient based algorithm such as PSO? So we could find a good starting point in the very large space of solutions (explored by the PSO algorithm), and then fine tune it using a gradient descent method. It isn't a common practice, so maybe it's useless... what do you think about this?",6,1
