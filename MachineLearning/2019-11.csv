,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2019-11-1,2019,11,1,9,dpx2cc,What lucky value do you set the random seed to?,https://www.reddit.com/r/MachineLearning/comments/dpx2cc/what_lucky_value_do_you_set_the_random_seed_to/,stormtrooper1721,1572569725,[removed],0,1
1,2019-11-1,2019,11,1,9,dpx2no,Increasing IOU for object detection model,https://www.reddit.com/r/MachineLearning/comments/dpx2no/increasing_iou_for_object_detection_model/,RedKnicks123,1572569775,[removed],0,1
2,2019-11-1,2019,11,1,9,dpx47u,[Project] [Get paid for ML projects/gigs],https://www.reddit.com/r/MachineLearning/comments/dpx47u/project_get_paid_for_ml_projectsgigs/,MLtinkerer,1572569986,"\[Get paid for ML projects/gigs\]

Do you have any technical skills in machine learning, data science, NLP, deep learning, etc.?

Are you interested in getting paid for mini-projects and gigs on the side?

Then this is a great opportunity for you to sharpen your technical skills while earning some extra cash as well!

Sign up in &lt;2 minutes below.

\[Accepting new folks only while this form is open!\]  
[https://docs.google.com/forms/d/1I74N-MYu5HIR8dIhyOAscU7h70tZ4hEg4lscBJVq5Pw/viewform](https://docs.google.com/forms/d/1I74N-MYu5HIR8dIhyOAscU7h70tZ4hEg4lscBJVq5Pw/viewform)",5,0
3,2019-11-1,2019,11,1,10,dpx9fm,Google Introduces Huge Universal Language Translation Model: 103 Languages Trained on Over 25 Billion Examples,https://www.reddit.com/r/MachineLearning/comments/dpx9fm/google_introduces_huge_universal_language/,Yuqing7,1572570681,,0,1
4,2019-11-1,2019,11,1,10,dpxojw,Text2Image: A new way to NLP?,https://www.reddit.com/r/MachineLearning/comments/dpxojw/text2image_a_new_way_to_nlp/,titian101,1572572757,[removed],0,1
5,2019-11-1,2019,11,1,11,dpxuue,"Best programs (US, Canada, UK) for theoretical machine learning?",https://www.reddit.com/r/MachineLearning/comments/dpxuue/best_programs_us_canada_uk_for_theoretical/,Turings_Ego,1572573627,[removed],0,1
6,2019-11-1,2019,11,1,11,dpy81n,cosmetic cartoning machine automatic cartoner,https://www.reddit.com/r/MachineLearning/comments/dpy81n/cosmetic_cartoning_machine_automatic_cartoner/,Jochamp-Machinery,1572575463,,0,1
7,2019-11-1,2019,11,1,11,dpygqx,Automatic cling wrap shrink wrapping machine Aluminum foil roll packagin...,https://www.reddit.com/r/MachineLearning/comments/dpygqx/automatic_cling_wrap_shrink_wrapping_machine/,Jochamp-Machinery,1572576722,,0,1
8,2019-11-1,2019,11,1,12,dpyy7w,Some thoughts about AlphaStar being Grandmaster.,https://www.reddit.com/r/MachineLearning/comments/dpyy7w/some_thoughts_about_alphastar_being_grandmaster/,FloRicx,1572579442,,0,1
9,2019-11-1,2019,11,1,12,dpyybl,[R] Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck (NeurIPS 2019),https://www.reddit.com/r/MachineLearning/comments/dpyybl/r_generalization_in_reinforcement_learning_with/,hardmaru,1572579459,,1,4
10,2019-11-1,2019,11,1,12,dpz4oj,caffe: train input with pre-trained network,https://www.reddit.com/r/MachineLearning/comments/dpz4oj/caffe_train_input_with_pretrained_network/,arjundupa,1572580499,[removed],0,1
11,2019-11-1,2019,11,1,13,dpz91z,Code Tutorial: Manipulate Hyperparameter Spaces for Hyperparameter Tuning,https://www.reddit.com/r/MachineLearning/comments/dpz91z/code_tutorial_manipulate_hyperparameter_spaces/,GChe,1572581173,,0,1
12,2019-11-1,2019,11,1,13,dpzlfa,"[D] AlphaStar: the Good, the Bad and the Ugly",https://www.reddit.com/r/MachineLearning/comments/dpzlfa/d_alphastar_the_good_the_bad_and_the_ugly/,FloRicx,1572583322,,0,1
13,2019-11-1,2019,11,1,13,dpzq2d,"[D] AlphaStar: the Good, the Bad and the Ugly",https://www.reddit.com/r/MachineLearning/comments/dpzq2d/d_alphastar_the_good_the_bad_and_the_ugly/,FloRicx,1572584171,"https://twitter.com/FloRicx/status/1189831729336307712?s=20

Here are some thoughts about AlphaStar being Grandmaster. I mainly made a list of some limitations that are not well described in DeepMind's blog post but you can see (or sometimes guess) by reading the Nature paper.",136,134
14,2019-11-1,2019,11,1,15,dq0dda,Batch Inference of Object Detection and Instance Segmentation.,https://www.reddit.com/r/MachineLearning/comments/dq0dda/batch_inference_of_object_detection_and_instance/,elepherai,1572588705,[removed],0,1
15,2019-11-1,2019,11,1,15,dq0fva,Master Thesis and Research Topics in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/dq0fva/master_thesis_and_research_topics_in_machine/,WriteMyThesis,1572589240,,0,1
16,2019-11-1,2019,11,1,15,dq0jwb,"Machine learning prediction considering several variables, is that possible?",https://www.reddit.com/r/MachineLearning/comments/dq0jwb/machine_learning_prediction_considering_several/,Mastershafi,1572590101,,0,1
17,2019-11-1,2019,11,1,19,dq28m4,Difference between 'L1_ratio' and 'C' in sklearn.linear_model.LogisticRegression?,https://www.reddit.com/r/MachineLearning/comments/dq28m4/difference_between_l1_ratio_and_c_in/,AtreyaJi,1572603091,[removed],0,1
18,2019-11-1,2019,11,1,19,dq2azz,I'm researching ML models zoos and good practices around model metadata. Anyone got good info or links?,https://www.reddit.com/r/MachineLearning/comments/dq2azz/im_researching_ml_models_zoos_and_good_practices/,porto_bello,1572603564,[removed],0,1
19,2019-11-1,2019,11,1,19,dq2lqx,[P] Machine Learning: Doodle Recognition with Convolutional Neural Network,https://www.reddit.com/r/MachineLearning/comments/dq2lqx/p_machine_learning_doodle_recognition_with/,ssusnic,1572605624,"Hi all,

I'm launching a step-by-step video tutorial series on making a game like **""Quick, Draw!""**. As you probably know, it is an online game that challenges players to draw a doodle and then artificial intelligence guesses what the drawings represent. So I thought this group would appreciate this project.

Using **Phaser 2** framework and **Tensorflow.js** library, I've created a complete HTML5 game called **Doodle Predictor** that runs directly in the browser and recognizes doodles.

To classify drawings, the game uses Machine Learning with a Convolutional Neural Network. The model is trained on a small subset of the Quick Draw Dataset.

I tried to make this project easy enough for someone without any machine learning experience to follow. So if you are interested in it, here are the links:

**[Video Episode 1:](https://youtu.be/kLF5vx5Ya1A)**

* Introduction

**[Video Episode 2:](https://youtu.be/-iBBWDJrUEs)**

* Part 1 - Project Setup
* Part 2 - Getting Data
* Part 3 - Building Model
* Part 4 - Training Model 

**Video Episode 3:** (coming soon)

* Part 5 - Predicting Samples
* Part 6 - Drawing Doodles
* Part 7 - Recognizing Doodles
* Part 8 - Adding More Doodle Categories

**Video Episode 4:** (coming soon)

* Game Demo
	
**Play Doodle Predictor on my blog:**

* [https://www.askforgametask.com/tutorial/doodle-recognition-convolutional-neural-network-intro](https://www.askforgametask.com/tutorial/doodle-recognition-convolutional-neural-network-intro)

**Source Code available on Github:**

* [https://github.com/ssusnic/Machine-Learning-Doodle-Recognition](https://github.com/ssusnic/Machine-Learning-Doodle-Recognition)",1,26
20,2019-11-1,2019,11,1,20,dq2vel,"NEED SMART PEOPLE, A.I. AND MACHINE LEARNING TO SOLVE A MAJOR PROBLEM IN MY COUNTRY",https://www.reddit.com/r/MachineLearning/comments/dq2vel/need_smart_people_ai_and_machine_learning_to/,mursalleen,1572607299,[removed],0,1
21,2019-11-1,2019,11,1,20,dq33ic,11 Top Machine Learning Algorithms used by Data Scientists,https://www.reddit.com/r/MachineLearning/comments/dq33ic/11_top_machine_learning_algorithms_used_by_data/,AnujG23,1572608680,,0,1
22,2019-11-1,2019,11,1,21,dq3e95,Machine learning in business,https://www.reddit.com/r/MachineLearning/comments/dq3e95/machine_learning_in_business/,stanissse,1572610412,[removed],0,1
23,2019-11-1,2019,11,1,21,dq3kt1,Optimizing the Code with Machine Learning for Software Development,https://www.reddit.com/r/MachineLearning/comments/dq3kt1/optimizing_the_code_with_machine_learning_for/,Harshitkansal,1572611448,,0,1
24,2019-11-1,2019,11,1,22,dq46zb,Behind the scenes of text similarity in NLP,https://www.reddit.com/r/MachineLearning/comments/dq46zb/behind_the_scenes_of_text_similarity_in_nlp/,mrnerdy59,1572614699,[removed],0,1
25,2019-11-1,2019,11,1,22,dq4dec,Predicting a (time-series) waveform rather than a scalar,https://www.reddit.com/r/MachineLearning/comments/dq4dec/predicting_a_timeseries_waveform_rather_than_a/,showgan1,1572615622,[removed],0,1
26,2019-11-1,2019,11,1,22,dq4fl7,TensorHUB BERT model: is it only useable for classification tasks?,https://www.reddit.com/r/MachineLearning/comments/dq4fl7/tensorhub_bert_model_is_it_only_useable_for/,ReasonablyBadass,1572615914,"I would like to predict word vectors instead of labeling text, a sort of text summary, but it appears it requires some sort of labels the way it is set up right now.",0,0
27,2019-11-1,2019,11,1,22,dq4j85,What's the best method of teaching and learning? [Discussion],https://www.reddit.com/r/MachineLearning/comments/dq4j85/whats_the_best_method_of_teaching_and_learning/,ZeroMaxinumXZ,1572616413,"What's the best method of teaching and learning? How should we as humans teach and learn? How should ML algorithms learn from us? Should we use supervised methods, unsupervised methods, reinforcement methods, or a mixture? What do you think is the best method for teaching and learning concepts?",5,1
28,2019-11-1,2019,11,1,23,dq4o6a,[Discussion] CNN Binary Classifier returning same accuracy in several epochs....,https://www.reddit.com/r/MachineLearning/comments/dq4o6a/discussion_cnn_binary_classifier_returning_same/,kirasama16997,1572617081,"
....what do I do?

I have 2000 images each of classes ""Mobile"" and ""Cake"", with a validation split of 0.3. This is the code of the model..

&amp;amp;#x200B;

 \----------------------------------------------------------------------------------------

  
model = keras.models.Sequential()

  
model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input\_shape=(150, 150, 3)))  
model.add(keras.layers.MaxPooling2D((2, 2)))

model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))  
model.add(keras.layers.MaxPooling2D((2, 2)))

model.add(keras.layers.Conv2D(128, (3, 3), activation='relu'))  
model.add(keras.layers.MaxPooling2D((2, 2)))  
model.add(keras.layers.Conv2D(128, (3, 3), activation='relu'))  
model.add(keras.layers.MaxPooling2D((2, 2)))  
model.add(keras.layers.Flatten())  
model.add(keras.layers.Dropout(0.5))  
model.add(keras.layers.Dense(512, activation='relu'))  
model.add(keras.layers.Dense(1, activation='sigmoid'))  
model.compile(loss='binary\_crossentropy',optimizer=keras.optimizers.RMSprop(lr=((1e-4)/10)),metrics=\['acc'\])  
[history=model.fit](https://history=model.fit)(Xnew,Ynew,1,10,validation\_split=0.3)

 \----------------------------------------------------------------------------------------

Can anyone help me out? The accuracy gets stuck on 0.5280 right from the second epoch. I am a beginner in CNN, and would appreciate some guidance with this. Thanks!",24,0
29,2019-11-1,2019,11,1,23,dq4uct,Music source separation library including pretrained models (open source).,https://www.reddit.com/r/MachineLearning/comments/dq4uct/music_source_separation_library_including/,yoogidoky,1572617912,,0,1
30,2019-11-2,2019,11,2,0,dq5dz9,Understanding chatbots and how Machine Learning and NLP makes them powerful,https://www.reddit.com/r/MachineLearning/comments/dq5dz9/understanding_chatbots_and_how_machine_learning/,mto96,1572620458,,1,1
31,2019-11-2,2019,11,2,0,dq5mdq,Scissorwalk - a comic created by a curated AI. Methodology is explained in the link. Previous works were kind of goofy. This one seems a bit sinister.,https://www.reddit.com/r/MachineLearning/comments/dq5mdq/scissorwalk_a_comic_created_by_a_curated_ai/,KaneCreole,1572621512,,0,1
32,2019-11-2,2019,11,2,1,dq6cut,Geometric Deep Learning Subreddit comunity!,https://www.reddit.com/r/MachineLearning/comments/dq6cut/geometric_deep_learning_subreddit_comunity/,flawnson,1572624680,[removed],0,1
33,2019-11-2,2019,11,2,1,dq6ho0,Kohonen: Self-Organizing Maps,https://www.reddit.com/r/MachineLearning/comments/dq6ho0/kohonen_selforganizing_maps/,Creepy_Disco_Spider,1572625251,,0,1
34,2019-11-2,2019,11,2,1,dq6qo7,Improving Visual Reasoning with Attention Alignment,https://www.reddit.com/r/MachineLearning/comments/dq6qo7/improving_visual_reasoning_with_attention/,codehacked,1572626317,,0,1
35,2019-11-2,2019,11,2,2,dq72bw,(Data Quality Ebook excerpt) Chapter 2: Why Clean Data?,https://www.reddit.com/r/MachineLearning/comments/dq72bw/data_quality_ebook_excerpt_chapter_2_why_clean/,kazi_analyst,1572627756,[removed],0,1
36,2019-11-2,2019,11,2,2,dq7u5j,A Questionable Paper Published in SIGIR 2019,https://www.reddit.com/r/MachineLearning/comments/dq7u5j/a_questionable_paper_published_in_sigir_2019/,joyyeki,1572631154,[removed],0,1
37,2019-11-2,2019,11,2,3,dq80aw,Which Experiment tracking tool are you using?,https://www.reddit.com/r/MachineLearning/comments/dq80aw/which_experiment_tracking_tool_are_you_using/,gargomeister,1572631895,"Hello, r/MachineLearning's people! 

I plan on using a cloud solution for experiment tracking in my different projects (which are mostly image and text classification tasks using Deep Learning).  

I found several tools online: 

[https://www.comet.ml](https://www.comet.ml/)

[https://neptune.ml](https://neptune.ml/)

[https://www.wandb.com](https://www.wandb.com/)

[https://github.com/allegroai/trains  ](https://github.com/allegroai/trains)  


By looking at them and trying them on several experiments, I noticed that they almost are providing the same feature (except versioning for some of them).  


I would like to have your feedback after using them for a long time (what are the drawbacks and advantages of the solution that you are using). 

Here are observations that I was able to make after trying them:

[https://www.comet.ml](https://www.comet.ml/) 

- Pretty insightful interface
- Used by some famous companies
- Easy to use

[https://neptune.ml](https://neptune.ml/)

- Integration with a lot of frameworks (such as MLFLOW)
- Versioning of notebook
- Good collaboration tool for each experiment 
- Easy to comment, edit and plan the next experiments.
- Look like their service connection is not great (on AWS cluster, even by whitelisting the service, experienced a lot of HTTP errors)
- No automatic metric recording (you have to create your own callback)

[https://www.wandb.com](https://www.wandb.com/)

- The interface is really clean
- Experiments are easy to implement
- Tracking a lot of different metrics
- Really good to generate reports in order to present experiments.
- 

[https://github.com/allegroai/trains  ](https://github.com/allegroai/trains)

- Open-source project  
- Noisy interface
- Quite easy to implement.",0,1
38,2019-11-2,2019,11,2,3,dq82x7,[Discussion] A Questionable SIGIR 2019 Paper,https://www.reddit.com/r/MachineLearning/comments/dq82x7/discussion_a_questionable_sigir_2019_paper/,joyyeki,1572632241,"I recently read the paper ""Adversarial Training for Review-Based Recommendations"" published on the SIGIR 2019 conference. I noticed that this paper is almost exactly the same as the paper ""Why I like it: Multi-task Learning for Recommendation and Explanation"" published on the RecSys 2018 conference.

At first, I thought it is just a coincidence. It is likely for researchers to have similar ideas. Therefore it is possible that two research groups independently working on the same problem come up with the same solution. However, after thoroughly reading and comparing the two papers, now I believe that the SIGIR 2019 paper is plagiarizing the RecSys 2018 paper.

The model proposed in the SIGIR 2019 paper is almost a replicate of the model in the RecSys 2018 paper. (1) Both papers used an adversarial sequence-to-sequence learning model on top of the matrix factorization framework. (2) For the generator and discriminator part, both papers use GRU for generator and CNN for discriminator. (3) The optimization methodology is the same, i.e. alternating optimization between two parts. (4) The evaluations are the same, i.e. evaluating MSE for recommendation performance and evaluating the accuracy for discriminator to show that the generator has learned to generate relevant reviews. (5) The notations and also the formulas that have been used by the two papers look extremely similar.

While ideas can be similar given that adversarial training has been prevalent in the literature for a while, it is suspicious for the SIGIR 2019 paper to have large amount of text overlaps with the RecSys 2018 paper.

Consider the following two sentences:

(1) ""The Deep Cooperative Neural Network (DeepCoNN) model user-item interactions based on review texts by utilizing a factorization machine model on top of two convolutional neural networks."" in Section 1 of the SIGIR 2019 paper.

(2) ""Deep Cooperative Neural Network (DeepCoNN) model user-item interactions based on review texts by utilizing a factorization machine model on top of two convolutional neural networks."" in Section 2 of the RecSys 2018 paper.

I think this is the most obvious sign of plagiarism. If you search Google for this sentence using ""exact match"", you will find that this sentence is only used by these two papers. It is hard to believe that the authors of the SIGIR 2019 paper could come up with the exact same sentence without reading the RecSys 2018 paper.

As another example:

(1) ""The decoder employs a single GRU that iteratively produces reviews word by word. In particular, at time step $t$ the GRU first maps the output representation $z\_{ut-1}$ of the previous time step into a $k$-dimensional vector $y\_{ut-1}$ and concatenates it with $\\bar{U\_{u}}$ to generate a new vector $y\_{ut}$. Finally, $y\_{ut}$ is fed to the GRU to obtain the hidden representation $h\_{t}$, and then $h\_{t}$ is multiplied by an output projection matrix and passed through a softmax over all the words in the vocabulary of the document to represent the probability of each word. The output word $z\_{ut}$ at time step $t$ is sampled from the multinomial distribution given by the softmax."" in Section 2.1 of the SIGIR 2019 paper.

(2) ""The user review decoder utilizes a single decoder GRU that iteratively generates reviews word by word. At time step $t$, the decoder GRU first embeds the output word $y\_{i, t-1}$ at the previous time step into the corresponding word vector $x\_{i, t-1} \\in \\mathcal{R}\^{k}$, and then concatenate it with the user textual feature vector $\\widetilde{U\_{i}}$. The concatenated vector is provided as input into the decoder GRU to obtain the hidden activation $h\_{t}$. Then the hidden activation is multiplied by an output projection matrix and passed through a softmax over all the words in the vocabulary to represent the probability of each word given the current context. The output word $y\_{i, t}$ at time step $t$ is sampled from the multinomial distribution given by the softmax."" in Section 3.1.1 of the RecSys 2018 paper.

In this example, the authors of the SIGIR 2019 paper has replaced some of the phrases in the writing so that the two texts are not exactly the same. However, I believe the similarity of the two texts still shows that the authors of the SIGIR 2019 paper must have read the RecSys 2018 paper before writing their own paper.

I do not intend to go through all the text overlaps between the two papers, but let us see a final example:

(1) ""Each word of the review $r$ is mapped to the corresponding word vector, which is then concatenated with a user-specific vector. Notice that the user-specific vectors are learned together with the parameters of the discriminator $D\_{\\theta}$ in the adversarial training of Section 2.3. The concatenated vector representations are then processed by a convolutional layer, followed by a max-pooling layer and a fully-connected projection layer. The final output of the CNN is a sigmoid function which normalizes the probability into the interval of $\[0, 1\]$"", expressing the probability that the candidate review $r$ is written by user $u$."" in Section 2.2 of the SIGIR 2019 paper.

(2) ""To begin with, each word in the review is mapped to the corresponding word vector, which is then concatenated with a user-specific vector that identifies user information. The user-specific vectors are learned together with other parameters during training. The concatenated vector representations are then processed by a convolutional layer, followed by a max-pooling layer and a fully-connected layer. The final output unit is a sigmoid non-linearity, which squashes the probability into the $\[0, 1\]$ interval."" in Section 3.1.2 of the RecSys 2018 paper.

There is one sentence (""The concatenated vector representations are ...... a fully-connected projection layer."") that is exactly the same in the two papers. Also, I think concatenating the user-specific vectors to every word vector in the review is a very unintuitive idea. I do not think ideas from different research groups can be the same in that granularity of detail. If I were the authors, I will just concatenate the user-specific vectors to the layer before the final projection layer, as it saves computational cost and should lead to better generalization.

As a newbie in information retrieval, I am not sure if such case should be considered as plagiarism. However, as my professor told me that the SIGIR conference is the premier conference in the IR community, I believe that this paper definitely should not be published at a top conference such as SIGIR.

What makes me feel worse is that the two authors of this paper, Dimitrios Rafailidis from Maastricht University, Maastricht, Netherlands and Fabio Crestani from Universit della Svizzera italiana (USI), Lugano, Switzerland, are both professors. They should be aware that plagiarism is a big deal in academia.

The link to the papers are [https://dl.acm.org/citation.cfm?id=3331313](https://dl.acm.org/citation.cfm?id=3331313) and [https://dl.acm.org/citation.cfm?id=3240365](https://dl.acm.org/citation.cfm?id=3240365)",118,361
39,2019-11-2,2019,11,2,3,dq84la,Hi guys! We wrote a guide to video analytics. What are your thoughts about it? W'd love to get your feedback.,https://www.reddit.com/r/MachineLearning/comments/dq84la/hi_guys_we_wrote_a_guide_to_video_analytics_what/,tryo_labs,1572632451,[removed],0,1
40,2019-11-2,2019,11,2,3,dq85to,[D] Code examples in paper (instead of pseudo code),https://www.reddit.com/r/MachineLearning/comments/dq85to/d_code_examples_in_paper_instead_of_pseudo_code/,schludy,1572632613,"I'm a PhD candidate and not that experienced in research yet, so forgive me for a somewhat basic question.

I'm reviewing a paper that includes a short (\~15 lines) code example in Julia. I'm not a big fan of this, I think they should consider using pseudo code and connect it to the math used in the paper.

Is it acceptable to use actual code examples in a paper?",21,3
41,2019-11-2,2019,11,2,4,dq8sg1,[D] Is there any AI capable of playing a simulation racing Game like iRacing or Asseto Corsa?,https://www.reddit.com/r/MachineLearning/comments/dq8sg1/d_is_there_any_ai_capable_of_playing_a_simulation/,andymus1,1572635379,"I'm looking for resources/papers where someone has tried to create an AI that can play a simulation racing game like IRacing, Assetto Corsa or Project Cars 2. I know that there are nets that can play basic racing games like mario kart but I was wondering if there are ones that can play in a complex physics environment where you need to employ real strategies to win at a competitive level.",7,1
42,2019-11-2,2019,11,2,4,dq93t2,Any Published Resource on Low-level and High-level features of a neural network,https://www.reddit.com/r/MachineLearning/comments/dq93t2/any_published_resource_on_lowlevel_and_highlevel/,PaganPasta,1572636766,[removed],0,1
43,2019-11-2,2019,11,2,5,dq9ywc,Is udacity machine learning nanodegree worth it??,https://www.reddit.com/r/MachineLearning/comments/dq9ywc/is_udacity_machine_learning_nanodegree_worth_it/,Angolie2400,1572640484,[removed],0,1
44,2019-11-2,2019,11,2,5,dqa155,[R] Neural spiking for causal inference,https://www.reddit.com/r/MachineLearning/comments/dqa155/r_neural_spiking_for_causal_inference/,chimp73,1572640763,,0,1
45,2019-11-2,2019,11,2,6,dqaufi,Smaller Is Better: Lightweight Face Detection For Smartphones,https://www.reddit.com/r/MachineLearning/comments/dqaufi/smaller_is_better_lightweight_face_detection_for/,Yuqing7,1572644286,,0,1
46,2019-11-2,2019,11,2,6,dqavx0,[R] Reversing Classical Software with Differentable Logic Gates,https://www.reddit.com/r/MachineLearning/comments/dqavx0/r_reversing_classical_software_with_differentable/,neuralPr0cess0r,1572644469,"Hello everyone,

I'd like to share some work I have done in recasting logic gates into a differentiable form, thereby enabling gradient descent through a classical program.  

[https://youtu.be/hSEinrmMm9A](https://youtu.be/hSEinrmMm9A)

Codebase will be released soon.",8,9
47,2019-11-2,2019,11,2,7,dqbp9g,"[D] Momentum methods helps to escape local minima, so what? It was never our objective.",https://www.reddit.com/r/MachineLearning/comments/dqbp9g/d_momentum_methods_helps_to_escape_local_minima/,fromnighttilldawn,1572648208,"Something that seems to be under-discussed in machine learning is why we bother with momentum method in the first place. 

Suppose we are training a classifier and the loss function has two local minima, one of which is global. Suppose by sheer unluck, the gradient descent gets stuck in the worse local minima. If you ask around as to what can be done, you will hear answers like ""oh just use the momentum method, it gets you out of the local minima"". 

First, there is no guarantee you will be out of the local minima (only if the difference between the current and previous iterate is large enough do you have a chance), and more importantly,

Second, great, you have found the global mimina and....you have just overfitted your classifier.

In other words, we are looking for local minima (or  even just some point associated with the loss function) with good generalization properties, and I don't think momentum methods guarantees that. 

Has there been any research on the generalization properties of the minima that you find and what algorithm get you the best minima, not in terms of how small the loss is, but how well it achieves generalization?",30,0
48,2019-11-2,2019,11,2,7,dqbs1v,A question on clusteriing algorithms.,https://www.reddit.com/r/MachineLearning/comments/dqbs1v/a_question_on_clusteriing_algorithms/,PuzzleheadedGas3,1572648572,[removed],0,1
49,2019-11-2,2019,11,2,7,dqbs2o,Data labeling services,https://www.reddit.com/r/MachineLearning/comments/dqbs2o/data_labeling_services/,kur1j,1572648575,[removed],0,1
50,2019-11-2,2019,11,2,9,dqd4pm,SinGAN (ICCV 2019) paper summary,https://www.reddit.com/r/MachineLearning/comments/dqd4pm/singan_iccv_2019_paper_summary/,aniket_agarwal,1572655129,[removed],0,1
51,2019-11-2,2019,11,2,10,dqdnuo,MOBILEBERT: TASK-AGNOSTIC COMPRESSION OF BERT BY PROGRESSIVE KNOWLEDGE TRANSFER,https://www.reddit.com/r/MachineLearning/comments/dqdnuo/mobilebert_taskagnostic_compression_of_bert_by/,I_ai_AI,1572657862,,6,6
52,2019-11-2,2019,11,2,13,dqfiht,Automating ML Feature Engineering (Spark),https://www.reddit.com/r/MachineLearning/comments/dqfiht/automating_ml_feature_engineering_spark/,bweber,1572668572,,0,1
53,2019-11-2,2019,11,2,15,dqgq5y,A Comprehensive Guide to Pythons Built-In Data Structures,https://www.reddit.com/r/MachineLearning/comments/dqgq5y/a_comprehensive_guide_to_pythons_builtin_data/,eyaltrabelsi,1572677464,,0,1
54,2019-11-2,2019,11,2,16,dqgzl6,"Data Science Course in Pune https://www.google.com/maps/place/ExcelR+Solutions+-+Data+Science+Training+Course+%7C+Digital+Marketing+Course+in+Pune/@18.5584566,73.7889225,17z/data=!3m1!4b1!4m5!3m4!1s0x3bc2bf37817aae43:0x6c49e2eda8b01c77!8m2!3d18.5584566!4d73.7911113",https://www.reddit.com/r/MachineLearning/comments/dqgzl6/data_science_course_in_pune/,Successdig,1572679595,,0,1
55,2019-11-2,2019,11,2,17,dqhduh,Anybody working on alternate credit scoring models?,https://www.reddit.com/r/MachineLearning/comments/dqhduh/anybody_working_on_alternate_credit_scoring_models/,NextCrab2,1572682803,[removed],0,1
56,2019-11-2,2019,11,2,18,dqhvjp,Top 17 Uses of Machine Learning Technology,https://www.reddit.com/r/MachineLearning/comments/dqhvjp/top_17_uses_of_machine_learning_technology/,narayanareddy12,1572686790,,0,1
57,2019-11-2,2019,11,2,18,dqhzyu,Mask-RCNN for blurring advertisement on streets.,https://www.reddit.com/r/MachineLearning/comments/dqhzyu/maskrcnn_for_blurring_advertisement_on_streets/,wannafIy,1572687664,[removed],0,1
58,2019-11-2,2019,11,2,19,dqi7v5,[P] Mask_RCNN for blurring advertisment on streets.,https://www.reddit.com/r/MachineLearning/comments/dqi7v5/p_mask_rcnn_for_blurring_advertisment_on_streets/,wannafIy,1572689366,"[https://github.com/WannaFIy/mask\_AD](https://github.com/WannaFIy/mask_AD) 

&amp;#x200B;

https://preview.redd.it/6dv43jvyy8w31.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;s=b9feedfeb74912aac43d53eb6fe03cd82cd7b08e",43,228
59,2019-11-2,2019,11,2,19,dqif5i,Hand trarcking,https://www.reddit.com/r/MachineLearning/comments/dqif5i/hand_trarcking/,Schumyspain,1572690880,"Hi all,

I'm trying to train a model to track the palm of the hand. I've seen things like HandTrack.js([https://victordibia.github.io/handtrack.js/#/](https://victordibia.github.io/handtrack.js/#/)) which is able to track my hand really good but I need this model in Swift.   
Therefore, I've downloaded the set of images of egohands and trained my own model using TuriCreate but the results I'm getting are far from the expectations. Compared to HandTrack.js for example, the confidence I'm getting most of the times while detecting the palm of the hand it's lower than 0.5, never 0.8 or higher.  
Also, since what I'm doing it's quite specific(just track the palm of the hand, I don't need any other position), I was wondering if it was a better idea just to make a set of images of my own and train it with that. So I did that, I got 100 pictures, made the bounding boxes and trained it.   
Results it's pretty much the same, it's not able to track the hand perfectly. It looses the hand quite often.

Do you have any tips for me that can guide me into this problem?",0,1
60,2019-11-2,2019,11,2,20,dqj6gc,[Academic] Data Analysis on Graph Databases (Anyone with data science knowledge),https://www.reddit.com/r/MachineLearning/comments/dqj6gc/academic_data_analysis_on_graph_databases_anyone/,not_apyramidscheme,1572695892,[removed],0,1
61,2019-11-2,2019,11,2,21,dqjb4q,[D] Why re-sampling imbalanced data isn't always the best idea,https://www.reddit.com/r/MachineLearning/comments/dqjb4q/d_why_resampling_imbalanced_data_isnt_always_the/,kchnkrml,1572696694,"I often times work with people (medical studies) with a huge ""knowledge"" on statistical methods but none of the required basics or understanding what goes on inside some algorithms. That's perfectly fine because after all that's not their job but mine.

&amp;#x200B;

But over time, I've come across a few problems where (due to not finding the ""needed significance"") some really basic over-sampling was applied. I've thrown together a really simple example, that anyone should be able to follow (without any deep statistical knowledge) to showcase what could happen - maybe it helps you or you can use it to your help:

&amp;#x200B;

[https://stroemer.cc/resample-imbalanced-data/](https://stroemer.cc/resample-imbalanced-data/)",42,155
62,2019-11-2,2019,11,2,22,dqjwdg,"Trying to reproducing ""IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures""",https://www.reddit.com/r/MachineLearning/comments/dqjwdg/trying_to_reproducing_impala_scalable_distributed/,ioricha,1572699990,"I have been to reproducing IMPALA by deepmind.

In this link, some results with atari(breakout, pong, star-gunner, boxing, space-invader) are introduced in [README.md](https://readme.md/).

If there are any fault in implementation, please be willing to comment. Thank you.

[https://github.com/RLOpensource/IMPALA-Distributed-Tensorflow](https://github.com/RLOpensource/IMPALA-Distributed-Tensorflow)",0,1
63,2019-11-2,2019,11,2,22,dqk0o3,"Manufacturing Equipment Suppliers, Wholesalers &amp; Exporters at Beldara.com",https://www.reddit.com/r/MachineLearning/comments/dqk0o3/manufacturing_equipment_suppliers_wholesalers/,BeldaraMarketplace,1572700615,[removed],0,1
64,2019-11-2,2019,11,2,22,dqk3aq,How to interpret this diagram? is this a good fit?,https://www.reddit.com/r/MachineLearning/comments/dqk3aq/how_to_interpret_this_diagram_is_this_a_good_fit/,awais0,1572700988,[removed],0,1
65,2019-11-2,2019,11,2,23,dqkoxh,[P] NER Tagger based on BERT + CRF (for Korean),https://www.reddit.com/r/MachineLearning/comments/dqkoxh/p_ner_tagger_based_on_bert_crf_for_korean/,eagle705,1572703935,"Hi all,

I did a NER toy project for Korean (in progress). If you are interested in Korean NER tagger, try it.
If you want to apply it to other languages, you don't have to change the model structure, you just change vocab, pretrained BERT, and training data.

https://github.com/eagle705/pytorch-bert-crf-ner",3,16
66,2019-11-2,2019,11,2,23,dqkxwc,What is a good entry job for a person hoping to eventually land a machine learning job?,https://www.reddit.com/r/MachineLearning/comments/dqkxwc/what_is_a_good_entry_job_for_a_person_hoping_to/,JackIsNotInTheBox,1572705099,"As you know, machine learning jobs require years of experience and a Master/PhD on top of that. For a recent grad, is data analyst jobs a good starting point?  And then perhaps do a part-time Masters?",0,1
67,2019-11-2,2019,11,2,23,dql7dt,[D] What is SOTA in Multi-Task Learning,https://www.reddit.com/r/MachineLearning/comments/dql7dt/d_what_is_sota_in_multitask_learning/,searchingundergrad,1572706313,Is it domain specific?,6,19
68,2019-11-2,2019,11,2,23,dql8p9,"Suggestions for universities for Masters in Machine Learning, Data Science in Germany?",https://www.reddit.com/r/MachineLearning/comments/dql8p9/suggestions_for_universities_for_masters_in/,vikid99,1572706489,[removed],0,1
69,2019-11-3,2019,11,3,0,dqliwg,[R] Function-Space Distributions over Kernels,https://www.reddit.com/r/MachineLearning/comments/dqliwg/r_functionspace_distributions_over_kernels/,Dear_Function,1572707777,[removed],0,1
70,2019-11-3,2019,11,3,1,dqm6vj,[P] Person remover: image-to-image project,https://www.reddit.com/r/MachineLearning/comments/dqm6vj/p_person_remover_imagetoimage_project/,javirk,1572710718,"Hi, during summer I worked on a project with the objective of removing people or objects from photos. Person-remover uses a pretrained YOLO to detect them and then feeds the resulting bounding boxes to the generator of a pix2pix which I trained from zero on Paris dataset. Even though the generator wasn't trained with the purpose of filling person-shaped objects, the results are pretty great and seems to generalize well to unseen photos or video.

Any ideas on how to improve the results even further?

Repo:  [https://github.com/javirk/Person\_remover](https://github.com/javirk/Person_remover)",15,12
71,2019-11-3,2019,11,3,1,dqmbnu,"[R] MGBPv2: Scaling Up Multi-Grid Back-Projection Networks (Winner of AIM ICCV19 Extreme-SR, Perceptual track)",https://www.reddit.com/r/MachineLearning/comments/dqmbnu/r_mgbpv2_scaling_up_multigrid_backprojection/,pnavarre,1572711298,"&amp;#x200B;

[\(16x upscaling example from paper\)](https://preview.redd.it/w9xiwo2fsaw31.png?width=1855&amp;format=png&amp;auto=webp&amp;s=e287eaa48b8a0078344f083771d5b1b16998f9bc)

Authors:[Pablo Navarrete Michelini](https://www.researchgate.net/profile/Pablo_Navarrete_Michelini), Wenbin Chen, Hanwen Liu, Dan Zhu

&gt;Abstract:   Here, we describe our solution for the AIM2019 Extreme SuperResolution Challenge, where we won the 1st place in terms of perceptual quality (MOS) similar to the ground truth and achieved the 5th place in terms of highfidelity (PSNR). To tackle this challenge, we introduce the second generation of MultiGrid BackProjection networks (MGBPv2) whose major modifications make the system scalable and more general than its predecessor. It combines the scalability of the multigrid algorithm and the performance of iterative backprojections. In its original form, MGBP is limited to a small number of parameters due to a strongly recursive structure. In MGBPv2, we make full use of the multigrid recursion from the beginning of the network; we allow different parameters in every module of the network; we simplify the main modules; and finally, we allow adjustments of the number of network features based on the scale of operation. For inference tasks, we introduce an overlapping patch approach to further allow processing of very large images (e.g. 8K). Our training strategies make use of a multiscale loss, combining distortion and/or perception losses on the output as well as downscaled output images. The final system can balance between high quality and high performance.

[PDF Link](https://arxiv.org/pdf/1909.12983) | [Landing Page](https://arxiv.org/abs/1909.12983) | [Github](https://github.com/pnavarre/mgbpv2)",0,3
72,2019-11-3,2019,11,3,2,dqn1ab,Aim and Shoot: A game where your opponents are neural networks,https://www.reddit.com/r/MachineLearning/comments/dqn1ab/aim_and_shoot_a_game_where_your_opponents_are/,mitousa,1572714327,,0,1
73,2019-11-3,2019,11,3,3,dqo3pe,Why do we require a GPU to train Deep Learning Models?,https://www.reddit.com/r/MachineLearning/comments/dqo3pe/why_do_we_require_a_gpu_to_train_deep_learning/,bastolasushann,1572718790,[removed],0,1
74,2019-11-3,2019,11,3,3,dqo4yo,My first machine learning program,https://www.reddit.com/r/MachineLearning/comments/dqo4yo/my_first_machine_learning_program/,mcCLEANal,1572718931,"Hi, Ive been assigned to write a supervised machine learning program that has a 10 dimensional input, all numbers, and a binary output. I have the training data in a  .txt file, what is the quickest method of making a program, in preferably python, that will be able to learn the data and predict an output? I am new to machine learning and very keen to learn more",0,1
75,2019-11-3,2019,11,3,3,dqoh2u,[D] any principled reason for cross entropy instead of L2 in language modelling? (more details in post),https://www.reddit.com/r/MachineLearning/comments/dqoh2u/d_any_principled_reason_for_cross_entropy_instead/,mesmer_adama,1572720333,"Is there any principled reason for doing softmax and cross entropy for the loss in for example transformers, rather than doing L2 over the target embeddings and the output from the model?",13,2
76,2019-11-3,2019,11,3,4,dqorgl,"Noob here, can someone ELI5 what ""Network Representation Learning"" is?",https://www.reddit.com/r/MachineLearning/comments/dqorgl/noob_here_can_someone_eli5_what_network/,maosquared,1572721532,"They discuss [here](https://link.springer.com/article/10.1007/s42452-019-1044-9) but it's too technical for me to understand. No need to go greatly into detail, just a gist would be much appreciated!!! I'm trying to use this [tool](https://github.com/churchmanlab/genewalk) for some analysis and it'd be great to know a bit more about how it works! 

Thank you!",0,1
77,2019-11-3,2019,11,3,4,dqoxsv,Numerical Optimization vs. Numerical Analysis,https://www.reddit.com/r/MachineLearning/comments/dqoxsv/numerical_optimization_vs_numerical_analysis/,fireflyinajar9,1572722282,"I'm not a Math major but I've been taking a lot of upper division math courses (Math minor) since I'm interested in getting into the mathematical foundations of ML. So far, I've covered Probability and Stochastic Processes, Statistics, Combinatorics, and Numerical Analysis (Matrix Computations), and now I'm contemplating between taking another Numerical Analysis course (Non-Linear Equations) or a Numerical Optimization course (Linear Optimization). I have two quarters left until I graduate and these courses are part of sequences. If I take the Numerical Analysis (Non-Linear Equations) course, I'll be on track towards completing the entire NA sequence, which includes a final course on ODEs and PDEs. If I take the Numerical Optimization course, I won't be able to complete the sequence obviously but I'll be able to cover Linear and Non-Linear Optimization. I'm a little more inclined towards Numerical Analysis because (i) I can complete the sequence, (ii) NO is a subset of NA (although NO is probably more applied), and (iii) I'm very keen on doing the final course on ODEs and PDEs because I'm also very interested in computational neuroscience. The ML and deep learning coursework I've taken so far gets into the basics of optimization but it's certainly not very comprehensive, and there is a single-quarter course covering optimization that I'm interested in taking sometime in the future.. That said, I would really like some advice on whether I should mix it up a little if it helps me broaden my understanding!",0,1
78,2019-11-3,2019,11,3,4,dqpahp,Can Machine Learning Turn Music Into A Science To Spark Specific Emotions In Listeners?,https://www.reddit.com/r/MachineLearning/comments/dqpahp/can_machine_learning_turn_music_into_a_science_to/,BlastPalace,1572723779,,0,1
79,2019-11-3,2019,11,3,5,dqq1lg,Question Answering System cdQA,https://www.reddit.com/r/MachineLearning/comments/dqq1lg/question_answering_system_cdqa/,helpful_dev_isaac,1572726980,,0,1
80,2019-11-3,2019,11,3,6,dqqevo,What tools are used to visually present complex architectures such as those in DM's papers?,https://www.reddit.com/r/MachineLearning/comments/dqqevo/what_tools_are_used_to_visually_present_complex/,ada_td,1572728561,[removed],0,1
81,2019-11-3,2019,11,3,6,dqqt1b,ICLR Review Release Date,https://www.reddit.com/r/MachineLearning/comments/dqqt1b/iclr_review_release_date/,misunderstoodpoetry,1572730212,[removed],0,1
82,2019-11-3,2019,11,3,6,dqqw6p,[D] Conversational AI,https://www.reddit.com/r/MachineLearning/comments/dqqw6p/d_conversational_ai/,Rioghasarig,1572730598,"I'm curious about what the current state of conversational AI. To be more specific, by ""conversation"" I'm not talking about something that can take orders to schedule appointments or buy tickets or something like that. I mean something like discussing movies or TV shows or current events.  I think remember Amazon holding a contest for this kind of thing but I haven't really seen something like this implemented anywhere I can access it. Does anyone have any examples of what this kind of technology can do? Or better yet, anything I can play and experiment with on my own?",10,6
83,2019-11-3,2019,11,3,8,dqrz3w,Choosing optimal threshold for multi label classification,https://www.reddit.com/r/MachineLearning/comments/dqrz3w/choosing_optimal_threshold_for_multi_label/,nisucuk,1572735610,[removed],0,1
84,2019-11-3,2019,11,3,8,dqsf3a,Technical knowledge needed as a Machine Learning Engineer,https://www.reddit.com/r/MachineLearning/comments/dqsf3a/technical_knowledge_needed_as_a_machine_learning/,sziyo,1572737688,[removed],0,1
85,2019-11-3,2019,11,3,8,dqsm14,parameter sharing decoder pair for auto composing,https://www.reddit.com/r/MachineLearning/comments/dqsm14/parameter_sharing_decoder_pair_for_auto_composing/,edisonzhao,1572738605,,2,25
86,2019-11-3,2019,11,3,9,dqss0c,[D] Ben Recht: Is Optimization a Sufficient Language for Understanding Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/dqss0c/d_ben_recht_is_optimization_a_sufficient_language/,fromnighttilldawn,1572739381,,0,1
87,2019-11-3,2019,11,3,11,dquuyw,Definition of tensors in a simple term!,https://www.reddit.com/r/MachineLearning/comments/dquuyw/definition_of_tensors_in_a_simple_term/,bastolasushann,1572749852,,0,1
88,2019-11-3,2019,11,3,12,dqv3o4,Where to find pretrained attention network for images given captions,https://www.reddit.com/r/MachineLearning/comments/dqv3o4/where_to_find_pretrained_attention_network_for/,doghouse_cathouse,1572751197,[removed],0,1
89,2019-11-3,2019,11,3,13,dqvz2e,Real numbers and Neural Networks,https://www.reddit.com/r/MachineLearning/comments/dqvz2e/real_numbers_and_neural_networks/,drkolenklow,1572756532,[removed],0,1
90,2019-11-3,2019,11,3,14,dqw9x8,Large carton packing machine Corrugated box cartoner&amp;cartoning machinea...,https://www.reddit.com/r/MachineLearning/comments/dqw9x8/large_carton_packing_machine_corrugated_box/,Jochamp-Machinery,1572758524,,0,1
91,2019-11-3,2019,11,3,16,dqx489,What order do I go about to learn ML ?,https://www.reddit.com/r/MachineLearning/comments/dqx489/what_order_do_i_go_about_to_learn_ml/,emphee12,1572764913,"Ok so I know python to a decent extent (finished learning all the core concepts and all that) ( it's the mosh hamedani course on YouTube)

Now I want to start  learning ML and I keep hearing terms like NumPY , pandas , tensorFlow sci-kit etc etc all over the place 
Since I just finished a python course what do I start learning next ? Do I start going through these packages (if so what order ?) Or do I jump into more advanced topics in python ?
Guide me 

Thank you",0,1
92,2019-11-3,2019,11,3,16,dqxdl3,Free Vinyl Guiro For Kontakt! 20 Round Robbins! Cut from a vinyl record!,https://www.reddit.com/r/MachineLearning/comments/dqxdl3/free_vinyl_guiro_for_kontakt_20_round_robbins_cut/,replicant-369,1572767066,,0,1
93,2019-11-3,2019,11,3,16,dqxemv,Question : Where can I find sport specific Ml models and what kind of development is happening in sport using Machine learning algorithms?,https://www.reddit.com/r/MachineLearning/comments/dqxemv/question_where_can_i_find_sport_specific_ml/,Shreyas9231,1572767312,[removed],0,1
94,2019-11-3,2019,11,3,17,dqxmnv,Getting specific entities from text,https://www.reddit.com/r/MachineLearning/comments/dqxmnv/getting_specific_entities_from_text/,rakshit444,1572769207,[removed],0,1
95,2019-11-3,2019,11,3,19,dqyiri,Getting specific entities from text,https://www.reddit.com/r/MachineLearning/comments/dqyiri/getting_specific_entities_from_text/,rakshit444,1572776160,"Hi,

I am new with machine learning. I currently building an app for learning ML purposes.

**Problem**: Within 2-3 sentences you need to find out some keywords and associated value.

For example:

1. Hi, when is your *birthday*?
2. It's *tomorrow*

**Result**:

key: Birthday

Value: Tomorrow

This would be a simple use case. There will be similar use cases. A resource I read online points me to the following site:

[https://cloud.google.com/natural-language/automl/entity-analysis/docs/](https://cloud.google.com/natural-language/automl/entity-analysis/docs/)

Q Is there any service out there for making this possible in a very short span of time

Q Would this be the right direction?

Q Where could I get the data set for the above analysis?",0,1
96,2019-11-3,2019,11,3,19,dqykjg,Artificial Intelligence platform raises 3.7M: on Cervest - TECHNOLOGY WELL,https://www.reddit.com/r/MachineLearning/comments/dqykjg/artificial_intelligence_platform_raises_37m_on/,OneAnabal,1572776487,,3,2
97,2019-11-3,2019,11,3,20,dqyy94,Best text classification for large texts,https://www.reddit.com/r/MachineLearning/comments/dqyy94/best_text_classification_for_large_texts/,raffbr2,1572779024,"I have large collections of lawsuits. Long texts each, 10 pages in average. What would be the most appropriate method to associate a risk to it (or equivalently classify the set of documents between probable win or loss, the result of the lawsuit) to deal with long/large texts?",0,1
98,2019-11-3,2019,11,3,20,dqz0yr,Also relevant for us. Neat Visualization,https://www.reddit.com/r/MachineLearning/comments/dqz0yr/also_relevant_for_us_neat_visualization/,xRazorLazor,1572779481,,0,1
99,2019-11-3,2019,11,3,20,dqz1sq,Best text classification techniques for large texts,https://www.reddit.com/r/MachineLearning/comments/dqz1sq/best_text_classification_techniques_for_large/,raffbr2,1572779629,"I have large collections of lawsuits. Long texts each, 10 pages in average. What would be the most appropriate method to associate a risk to it (or equivalently classify the set of documents between probable win or loss, the result of the lawsuit) to deal with long/large texts?",0,1
100,2019-11-3,2019,11,3,21,dqzpkf,I made a simple documentation on NLP (Clothing Reviews) on GitHub.,https://www.reddit.com/r/MachineLearning/comments/dqzpkf/i_made_a_simple_documentation_on_nlp_clothing/,KingMSM,1572783907,"Hey everyone. 
I have made a simple NLP model documentation using Jupyter Notebook which is on my GitHub. 

[NLP Model Documentation](https://github.com/MoizSM/NLP-Model-Review-Rating)

I would really appreciate it if you all could give it a read and throw in your reviews on how it is. All kinds of opinions are most welcomed. Thanks a lot. 

Cheers.",0,1
101,2019-11-3,2019,11,3,22,dr0bma,"Are you a #MachineLearning, #ArtificialIntelligence, #DataScience or #Cloud #Enthusiast ??",https://www.reddit.com/r/MachineLearning/comments/dr0bma/are_you_a_machinelearning_artificialintelligence/,mlait1908,1572787495,[removed],0,1
102,2019-11-3,2019,11,3,22,dr0i1f,Join Us! | MLAIT,https://www.reddit.com/r/MachineLearning/comments/dr0i1f/join_us_mlait/,mlait1908,1572788462,,0,1
103,2019-11-3,2019,11,3,22,dr0k0k,Data normalization for signal classification,https://www.reddit.com/r/MachineLearning/comments/dr0k0k/data_normalization_for_signal_classification/,shahzaibmalik1,1572788751,[removed],0,1
104,2019-11-3,2019,11,3,23,dr0qor,Thought Experiment,https://www.reddit.com/r/MachineLearning/comments/dr0qor/thought_experiment/,satorigged,1572789632,[removed],0,1
105,2019-11-3,2019,11,3,23,dr1a1w,[D] Is finetuning on part of the evaluation dataset acceptable for publishing machine learning papers?,https://www.reddit.com/r/MachineLearning/comments/dr1a1w/d_is_finetuning_on_part_of_the_evaluation_dataset/,roset_ta,1572792232,"Hello everyone,


I have been trying yo reproduce the results of a SOTA paper regarding object detection. I have reimplemented their method and trained on the same dataset, based on the paper, however I was not able to achieve their results on the datasets they use for evaluation, no matter what I have tried.


Then I also studied their referenced papers and realised that many of them use a train-test split strategy for evaluating their models. This means that they use a part of the evaluation dataset for finetuning their already trained model and then evaluate it on the testing part of the same dataset. In the case of these papers, this fact was explicitly mentioned. I think that this also happened in the paper I tried to reproduce. However, they don't mention it. 


My question for discussion is, what do you think about this strategy? Is finetuning on part of the evaluation dataset a way to go? What about generalisation on totally unknown data?",39,31
106,2019-11-3,2019,11,3,23,dr1aia,Where can I find deepfake datasets for detection ?,https://www.reddit.com/r/MachineLearning/comments/dr1aia/where_can_i_find_deepfake_datasets_for_detection/,neuron837839,1572792293,[removed],0,1
107,2019-11-4,2019,11,4,0,dr1rob,Peer reviewed finance papers in machine learning,https://www.reddit.com/r/MachineLearning/comments/dr1rob/peer_reviewed_finance_papers_in_machine_learning/,bicepjai,1572794414,"Can you share some peer reviewed famous machine learning papers in finance domain. I can always find by search in semantic scholar or google, but dont know how to check if its peer reviewed.",0,1
108,2019-11-4,2019,11,4,0,dr1vmv,The Machine Learning Software Engineering Interview,https://www.reddit.com/r/MachineLearning/comments/dr1vmv/the_machine_learning_software_engineering/,HN_Crosspost_Bot,1572794914,,0,1
109,2019-11-4,2019,11,4,0,dr1xv1,Seeking advice on what to focus on for ML regarding Statistics,https://www.reddit.com/r/MachineLearning/comments/dr1xv1/seeking_advice_on_what_to_focus_on_for_ml/,deez29,1572795174,[removed],0,1
110,2019-11-4,2019,11,4,0,dr20nw,"Extract voice, piano, drums, etc. from any music track using Machine Learning",https://www.reddit.com/r/MachineLearning/comments/dr20nw/extract_voice_piano_drums_etc_from_any_music/,HN_Crosspost_Bot,1572795515,,0,1
111,2019-11-4,2019,11,4,0,dr28el,[Discussion] On what basis are anchors chosen in YOLO algorithm?,https://www.reddit.com/r/MachineLearning/comments/dr28el/discussion_on_what_basis_are_anchors_chosen_in/,kirasama16997,1572796409,"So we had a poject review, and our teacher asked us on what basis anchors are chosen in YOLO, Faster R-CNN and the lot. 

Now I have no idea one what criterion is it based, so if anyone has something to say on this, please do. I would appreciate it!",2,1
112,2019-11-4,2019,11,4,1,dr2p8h,Methods of processing Whole Slide Images,https://www.reddit.com/r/MachineLearning/comments/dr2p8h/methods_of_processing_whole_slide_images/,ramzavil,1572798365,[removed],0,1
113,2019-11-4,2019,11,4,1,dr2tt5,GitHub - tribhuvanesh/knockoffnets: Knockoff Nets: Stealing Functionality of Black-Box Models,https://www.reddit.com/r/MachineLearning/comments/dr2tt5/github_tribhuvaneshknockoffnets_knockoff_nets/,nshmyrev,1572798870,,0,1
114,2019-11-4,2019,11,4,1,dr2vir,[D] DeepMind's PR regarding Alphastar is unbelievably bafflingg.,https://www.reddit.com/r/MachineLearning/comments/dr2vir/d_deepminds_pr_regarding_alphastar_is/,SoulDrivenOlives,1572799075,"David Silver hinted that DeepMind is done with Starcraft in BBC news article saying ""the lab may rest now"" and that they have ""completed the Starcraft challenge"".

I thought this was a little disappointing since the skill level Alphastar reached on ladder was not enough to beat professional players. I think we all wanted a real nice showdown between the human champion and the robot, right? That'd been pretty cool. 

The Nature paper had a nice graph depicting Alphastar's MMR which is basically Blizzard's version of elo rating. The Protoss agent had reached an MMR of ~6200 and the aggregate of all three races was 6030 iirc. The graph also had MMR's of Alphastar's opponents and information on whether the agent won or lost.

Basically Alphastar had lost all but 2 games against players who had higher than 6200 MMR. On ladder, it could not beat the professional. 

The agent from January was estimated to have been over 7000 MMR. I figured it'd be nice to estimate how well this newest agent would have faired against MaNa.

So I looked at the EU ladder, found someone with an MMR of ~6200. Popped him and Mana into Aligulac (sc2 database) and let it estimate some odds. Mana had ~75% chance of winning a Best of 5, and his 6200 MMR opponent had less than 1% chance of beating Mana 5-0. 

At this point I became convinced that DeepMind was throwing in the towel on sc2 because the cost of further improving Alphastar was too high to justify the publicity they were getting from the project. The team looked to be moving on to different things and the showmatch vs the world champion had been cancelled. 

But then something absolutely baffling happened which I don't think anyone saw coming.

Blizzcon was this weekend. With little to no fanfare DeepMind had brought Alphastar with them and let Blizzcon visitors play against it. Serral, one of the best players in the world, had just finished top 4 in the biggest tournament of the year wandered to the arcade and played a few games against the bot. Serral's MMR is over 7000. 

He lost 0-3 to the Protoss agent. These games were not televised. All we have is some blurry smartphone footage. 

I don't get it. If Alphastar was this strong why didn't DeepMind let it play more on ladder and get a higher ranking? Why didn't they organize a showmatch or something? They dropped the ball pretty hard on this one. This is so confusing to me.",158,362
115,2019-11-4,2019,11,4,2,dr387n,Was looking through some old files and found this GIF I made ages ago of an MNIST GAN during training,https://www.reddit.com/r/MachineLearning/comments/dr387n/was_looking_through_some_old_files_and_found_this/,Fanfic_Galore,1572800530,,0,1
116,2019-11-4,2019,11,4,2,dr3fh2,Need Suggestions From The Experts On Audio ML,https://www.reddit.com/r/MachineLearning/comments/dr3fh2/need_suggestions_from_the_experts_on_audio_ml/,AfterEmpire,1572801345,[removed],0,1
117,2019-11-4,2019,11,4,2,dr3fqu,[P] Was looking through some old files and found this GIF I made ages ago of an MNIST GAN during training,https://www.reddit.com/r/MachineLearning/comments/dr3fqu/p_was_looking_through_some_old_files_and_found/,Fanfic_Galore,1572801377,,0,1
118,2019-11-4,2019,11,4,4,dr5rvk,First Reddit bot,https://www.reddit.com/r/MachineLearning/comments/dr5rvk/first_reddit_bot/,shubhayans,1572811134,,0,1
119,2019-11-4,2019,11,4,5,dr5vwg,[D] Heads up - MLPerf Inference results publishing Wednesday,https://www.reddit.com/r/MachineLearning/comments/dr5vwg/d_heads_up_mlperf_inference_results_publishing/,riking27,1572811588,"MLPerf, a project to benchmark machine learning hardware, is publishing their first round of Inference results this Wednesday.

Take some time to review the precise challenge they're putting the hardware to: [https://mlperf.org/inference-overview/](https://mlperf.org/inference-overview/) and the general rules for Inference submissions: [mlperf/inference\_policies: inference\_rules.adoc](https://github.com/mlperf/inference_policies/blob/master/inference_rules.adoc#general-rules)

I'm excited to see some of the low-power chip results.

Source for date: [\#single-submission-round-schedule](https://github.com/mlperf/policies/blob/master/submission_rules.adoc#42-single-submission-round-schedule) \- Submission for this cycle was October 11th so therefore Week 1 Monday is October 14th, and Week 4 Wednesday (publication day) is **November 6th, 10AM US/Pacific time**.",3,18
120,2019-11-4,2019,11,4,5,dr67uo,Auto ML algorithms - performance comparisons? Code availability?,https://www.reddit.com/r/MachineLearning/comments/dr67uo/auto_ml_algorithms_performance_comparisons_code/,mckao,1572812983,[removed],0,1
121,2019-11-4,2019,11,4,6,dr6nca,[D] Machine Learning - WAYR (What Are You Reading) - Week 74,https://www.reddit.com/r/MachineLearning/comments/dr6nca/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,1572814805,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|
|----|-----|-----|-----|-----|-----|-----|-----|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)||||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)||

Most upvoted papers two weeks ago:

/u/ecart33: [https://arxiv.org/abs/1906.00817v1](https://arxiv.org/abs/1906.00817v1)

Besides that, there are no rules, have fun.",4,12
122,2019-11-4,2019,11,4,6,dr6vp8,[D] How do you handle sparse features?,https://www.reddit.com/r/MachineLearning/comments/dr6vp8/d_how_do_you_handle_sparse_features/,__Julia,1572815793,"I am working on a problem where I have a sequence of events happening, every event generate a set of tokens (some of the tokens are shared between the events, but not all), the task is to categorize the behavior that generated this set of events.

Let me give you a simple example to have an understanding on the input.

&amp;#x200B;

|event\_type|order|value\_type\_1|value\_1|value\_type\_2|value\_2|
|:-|:-|:-|:-|:-|:-|
|E1|1|alpha 1|24|alpha 2|33|
|E2|2|beta|120|||
|E1|3|alpha 1|234|alpha 2|56|
|E3|4|theta|150|||
|E4|5|||||

You can notice for example that the token ""**theta""** doesnt exist in event\_type E2, it only exist in some event types.

If I want to  do feature engineering in this case, what is the best way to vectorize my data. If I take the token, and try to put this way, I will end up with  a very sparse features.

&amp;#x200B;

|event\_type|order|alpha 1|alpha 2|beta|theta|
|:-|:-|:-|:-|:-|:-|
|E1|1|24|33|||
|E2|2|||120||
|E1|3|234|56|||
|E3|4||||150|
|E4|5|||||

&amp;#x200B;

If I construct my features this way, it will be very sparse and it doesnt make sense to consider it as missing data (because the data doesnt exist in first place).

I don't want to apply data imputation method such filling the last value  (You can see below the example, I have added the **number in bold to show it as an example**) . The reason is that some event type are very frequent, and some event types are not.

&amp;#x200B;

|event\_type|order|alpha 1|alpha 2|beta|theta|
|:-|:-|:-|:-|:-|:-|
|E1|1|24|33|**0**|**0**|
|E2|2|**24**|**33**|120|**0**|
|E1|3|234|56|**120**|**0**|
|E3|4|**234**|**56**|**120**|150|
|E4|5|**234**|**56**|**120**|150|

&amp;#x200B;

If you were in my shoes, how would you treat this problem?. Ideas, references are welcomed.

If you are wondering what do I want to do, I want to categorize the behavior that generated this set of events. I can experiment with any method if I get feature engineering right (you can think of clustering as an example).",13,4
123,2019-11-4,2019,11,4,6,dr729q,Best algorithm/algorithms for predicting taxi demand based on location and time?,https://www.reddit.com/r/MachineLearning/comments/dr729q/best_algorithmalgorithms_for_predicting_taxi/,Parkar99,1572816569,[removed],0,1
124,2019-11-4,2019,11,4,7,dr7hxs,"Anomaly Detection in Time Series data using One Class SVM, Help?",https://www.reddit.com/r/MachineLearning/comments/dr7hxs/anomaly_detection_in_time_series_data_using_one/,andrewpol88,1572818454,[removed],0,1
125,2019-11-4,2019,11,4,7,dr7ii1,Neural Networks as approximations to functions?,https://www.reddit.com/r/MachineLearning/comments/dr7ii1/neural_networks_as_approximations_to_functions/,TreeFullOfBirds,1572818519,[removed],0,1
126,2019-11-4,2019,11,4,7,dr7rh2,Failing at implement Q-Learning for openAI's Lunar Lander.,https://www.reddit.com/r/MachineLearning/comments/dr7rh2/failing_at_implement_qlearning_for_openais_lunar/,person4422,1572819625,[removed],0,1
127,2019-11-4,2019,11,4,7,dr7trq,"[D] The ""test set"" is nonsense",https://www.reddit.com/r/MachineLearning/comments/dr7trq/d_the_test_set_is_nonsense/,OverLordGoldDragon,1572819901,"I often see ML practitioners, and even experts, pose the idea of the ""test set"" as the ultimate benchmark of a model's performance. **This is nonsense** \- and I'll explain why.

Suppose you gather some data, label it, preprocess it, and compile it into a dataset. Now it's time to split your data - train, validation, test; how will you do it?

1. Random selection; may work well if the dataset is large enough
2. Engineered selection - assign samples to each set according to some *rationale*

The ultimate goal of an ML model is *generalization*; as such, said 'rationale' could be:

1. The best test set is comprised of the most ""realistic"" or ""difficult"" samples. *Problem*: model performance is harmed by artificially biasing the train set to exclude realistic/difficult samples.
2. The best set split is, each gets same quality data. *Justification*: ""poorer"" quality usually means (a) noise; (b) low complexity (""too obvious""). Whatever the description, if you *test* the model on an *information landscape* ( / probability distribution) that it wasn't trained on, the model may perform poorly simply because it ""learned"" little that's relevant to the test set.

Thus, ""split equally"" should work best. Onto the problem: why do we use a test set at all? Because - we ""fit"" the validation set with our *hyperparameters*, and we need to test on ""never seen"" data to avoid bias. Indeed, agreed - the test set does suppress said bias. But here's its red line: **variance**. 

Direct statistical theory: a sample is an approximation of the population distribution, with an uncertain mean, standard deviation, &amp; other. The more complex the problem, the greater the variation. -- So, is there a solution? Yes: *K-Fold Cross-Validation*. Per known theory, K-Fold CV can significantly slash variance of model performance - the higher the ""K"", the better. *Without* it, **classification error can easily differ by 5-15%**, if not 20-30%. When deciding what's ""SOTA"", every single percentage point can be a battle hard-fought - so a ""mere 5%"" is already astronomical. 

One may counter-argue, ""it's fine if the test set is large enough"". Except it's not fine; you get a ""large enough"" test set by either sacrificing train data, or, dataset is large enough so that you can make an even validation-test split. Former's undesirable for obvious reasons - and in latter, unless you have a *gargantuan* dataset (extremely rare), your test samples are still subject to significant-enough variance; merely swapping test &amp; validation samples can flip tables.

As a final punchline, note that the *random seed* can also substantially impact final outcome, further amplifying variance. Consequence: you don't know you did well because \`Dropout(0.5)\` works better than \`Dropout(0.2)\` or because dice rolled nicely. K-Fold CV will also reduce seed variance as a side-effect, but ideally (though often prohibitively) you'd do ""K seeds"".

**Verdict**: test set isn't good for testing. Instead, use K-fold CV, which both better estimates generalization performance by reducing variance, and allows using more train data.

&lt;hr&gt;

Though I am knowledgeable on the topic, I'm not an ""expert"" - and even experts disagree. Thus, counterarguments welcome.",43,0
128,2019-11-4,2019,11,4,9,dr986m,How do you cope with peers being noticeably more successful than you in grad school?,https://www.reddit.com/r/MachineLearning/comments/dr986m/how_do_you_cope_with_peers_being_noticeably_more/,EdmondRR,1572826126,[removed],0,2
129,2019-11-4,2019,11,4,14,drdafn,Can we do adversarial attack on XGBoost?,https://www.reddit.com/r/MachineLearning/comments/drdafn/can_we_do_adversarial_attack_on_xgboost/,al10101,1572846032,[removed],0,1
130,2019-11-4,2019,11,4,14,drdb4f,[D] How to find progressive improvements with neural networks?,https://www.reddit.com/r/MachineLearning/comments/drdb4f/d_how_to_find_progressive_improvements_with/,kachoufugetsu,1572846148,,0,1
131,2019-11-4,2019,11,4,14,drde7r,"[N] PyTorch-NLP 0.5.0 Released! Heres to contributing back to open source, hurah! ",https://www.reddit.com/r/MachineLearning/comments/drde7r/n_pytorchnlp_050_released_heres_to_contributing/,Deepblue129,1572846665,"Hi There!   

2 years into PyTorch-NLP and another 6 months from the previous release, I am releasing PyTorch=NLP 0.5.0. Also, with your help, we'll break 50,000 downloads! Thank you :) I love helping the community because I myself benefit from the hard work of other open source contributors!

As always, the theme of PyTorch-NLP is to be small, extensible and intuitive much like PyTorch is! And the goal is to extend PyTorch with basic NLP utilities.

Here are the release notes highlights:

# Python 3.5 Support, Sampler Pipelining, Finer Control of Random State

## Major Updates

- Support for Python 3.5
- Extensive rewrite of README.md to focus on new users and building an NLP pipeline. See [here](https://github.com/PetrochukM/PyTorch-NLP)
- Support for Pytorch 1.2
- Added `torchnlp.random` for finer grain control of random state building on PyTorch's `fork_rng`. This module controls the random state of `torch`, `numpy` and `random`.
```python
import random
import numpy
import torch

from torchnlp.random import fork_rng

with fork_rng(seed=123):  # Ensure determinism
    print('Random:', random.randint(1, 2**31))
    print('Numpy:', numpy.random.randint(1, 2**31))
    print('Torch:', int(torch.randint(1, 2**31, (1,))))
```
- Refactored `torchnlp.samplers` enabling pipelining. For example:
```python
from torchnlp.samplers import DeterministicSampler
from torchnlp.samplers import BalancedSampler

data = ['a', 'b', 'c'] + ['c'] * 100
sampler = BalancedSampler(data, num_samples=3)
sampler = DeterministicSampler(sampler, random_seed=12)
print([data[i] for i in sampler])  # ['c', 'b', 'a']
```
- Added `torchnlp.samplers.balanced_sampler` for balanced sampling extending Pytorch's `WeightedRandomSampler`.
- Added `torchnlp.samplers.deterministic_sampler` for deterministic sampling based on `torchnlp.random`.
- Added `torchnlp.samplers.distributed_batch_sampler` for distributed batch sampling that's more extensible and less restrictive than PyTorch's version.
- Added `torchnlp.samplers.oom_batch_sampler` to sample large batches first in order to force an out-of-memory error earlier rather than later into training.
- Added `torchnlp.utils.get_total_parameters` to measure the number of parameters in a model.
- Added `torchnlp.utils.get_tensors` to measure the size of an object in number of tensor elements. This is useful for dynamic batch sizing and for `torchnlp.samplers.oom_batch_sampler`.
```python
from torchnlp.utils import get_tensors

random_object_ = tuple([{'t': torch.tensor([1, 2])}, torch.tensor([2, 3])])
tensors = get_tensors(random_object_)
assert len(tensors) == 2
```
- Added a corporate sponsor to the library: https://wellsaidlabs.com/",5,40
132,2019-11-4,2019,11,4,15,drdvh4,CNC Hydraulic Guillotine Shear Machine with anti twist device,https://www.reddit.com/r/MachineLearning/comments/drdvh4/cnc_hydraulic_guillotine_shear_machine_with_anti/,CNCPressBrakeMachine,1572849713,,0,1
133,2019-11-4,2019,11,4,16,dre1v2,[P] How Not to Fail Your Machine Learning Interview,https://www.reddit.com/r/MachineLearning/comments/dre1v2/p_how_not_to_fail_your_machine_learning_interview/,cdossman,1572850914,"How Not to Fail Your Machine Learning Interview --   Getting an interview is easy, not mucking it up can be hard

 [https://medium.com/ai%C2%B3-theory-practice-business/how-not-to-fail-your-machine-learning-interview-9545a67b35bc](https://medium.com/ai%C2%B3-theory-practice-business/how-not-to-fail-your-machine-learning-interview-9545a67b35bc)",3,0
134,2019-11-4,2019,11,4,16,dre27v,diabolo shot blasting machine,https://www.reddit.com/r/MachineLearning/comments/dre27v/diabolo_shot_blasting_machine/,shotblastpro,1572850978,,0,1
135,2019-11-4,2019,11,4,16,dre7bd,Kerbstone shot blasting machine,https://www.reddit.com/r/MachineLearning/comments/dre7bd/kerbstone_shot_blasting_machine/,shotblastpro,1572851918,,0,1
136,2019-11-4,2019,11,4,17,drel4t,[R] Differentiable Convex Optimization Layers,https://www.reddit.com/r/MachineLearning/comments/drel4t/r_differentiable_convex_optimization_layers/,pilooch,1572854539,,0,1
137,2019-11-4,2019,11,4,17,dremkg,Data Exploration in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/dremkg/data_exploration_in_machine_learning/,Techtter,1572854807,,0,1
138,2019-11-4,2019,11,4,17,dretk5,[D] relation between the learned parameters of two trained neural networks on the same dataset,https://www.reddit.com/r/MachineLearning/comments/dretk5/d_relation_between_the_learned_parameters_of_two/,nodet07,1572856266,"I was wondering if there is any work that studies the relation of learned weights between two neural nets.  
*For example, suppose we have a simple regression task, and we trained an MLP with one hidden layer with 20 neurons.  If we train another MLP with 15 neurons in the hidden layer, what would the relation of the weight matrices be between these two networks?*    
I found some related works on neural network compression literature that start with the bigger model and use matrix pruning with factorization and/or decomposition to reach a smaller model. But, I'm not sure if the obtained parameters will be close to the weights a neural network will learn with similar parameters. I mean, the fact that we can use pruning methods and get good accuracy doesn't necessarily mean that that is the true relation between the bigger model and the smaller one. What do you think?",14,24
139,2019-11-4,2019,11,4,18,drf7v1,"How do you solve Python dependencies, package management and permissions in an enterprise setting?",https://www.reddit.com/r/MachineLearning/comments/drf7v1/how_do_you_solve_python_dependencies_package/,Tullsokk,1572859206,"I have mainly been using SAS for machine learning the last few years. But pyhton has a lot going for it, and I find myself supplementing a lot with jupyter notebook running python forad hoc tasks involiving LIME, text analysis and some visualizations. But not for running actual models in production. I run python on a virtual machine, as well as locally. But both sollutions have a ton of issues, from installation and package management with limited admin permissions, to the general troubbles of just messing up my python installation by upgrading a package that causes all hell to break loose. A lot of time is spent fixing issues, and repairing stuff. 

What is the reccomended way to use python in an enterprise setting? If I had my hands free, I would probably use some sort of container for each task, such as LIME, or for a partucular visulalization, so that I didnt need to worry about ruining my LIME notebook after messing around with some new plotly packages.",0,1
140,2019-11-4,2019,11,4,18,drf9d1,[P] Training an object detection model with CVAT,https://www.reddit.com/r/MachineLearning/comments/drf9d1/p_training_an_object_detection_model_with_cvat/,kramerkee,1572859513,"Hello!

I have a bunch of video that I have annotated using CVAT. Then I exported the annotation. How can I train (and test) a model based on this? 

It's a video file and not a sequence of images, and there is only one annotation file.",0,1
141,2019-11-4,2019,11,4,18,drfiqr,Is it really worth pursuing a Masters in Data Science or ML?,https://www.reddit.com/r/MachineLearning/comments/drfiqr/is_it_really_worth_pursuing_a_masters_in_data/,tj_droid,1572861426,"I am planning to pursue a Masters Degree in US in Fall 2020  
It seems a lot of universities are now offering specialised courses in Data Science.  
I am personally interested in ML (also have a strong liking for Mathematics) but it seems a lot of people are advising against it because most courses are new and not very useful. Is this true?  

What due to guys say? For someone who wants to build a career in ML, is it better pursue a CS degree or specialized degree in DataSci/ML?",0,1
142,2019-11-4,2019,11,4,19,drfxpk,"[R] Adversarial NLI - hard moving target for NLU, rather than a static benchmark that will quickly saturate",https://www.reddit.com/r/MachineLearning/comments/drfxpk/r_adversarial_nli_hard_moving_target_for_nlu/,Isinlor,1572864256,,1,1
143,2019-11-4,2019,11,4,20,drg89e,Alternator Bench Mark,https://www.reddit.com/r/MachineLearning/comments/drg89e/alternator_bench_mark/,rexxy019,1572866253,,0,1
144,2019-11-4,2019,11,4,20,drgbyj,5 Facial Recognition Trends and Market Predictions 2019,https://www.reddit.com/r/MachineLearning/comments/drgbyj/5_facial_recognition_trends_and_market/,pallavi_sharma,1572866917,,0,1
145,2019-11-4,2019,11,4,20,drgfh1,"SMT Equipment, PCB Equipment &amp; Machines",https://www.reddit.com/r/MachineLearning/comments/drgfh1/smt_equipment_pcb_equipment_machines/,smthelp1,1572867532,,0,1
146,2019-11-4,2019,11,4,21,drgx3f,"Artificial Intelligence: A Modern Approach 4th Edition, releasing February 3, 2020.",https://www.reddit.com/r/MachineLearning/comments/drgx3f/artificial_intelligence_a_modern_approach_4th/,35-56,1572870435,,0,1
147,2019-11-4,2019,11,4,21,drgy8x,[P] StarCraft 2 24/7 AI Ladder Stream - ai-arena.net,https://www.reddit.com/r/MachineLearning/comments/drgy8x/p_starcraft_2_247_ai_ladder_stream_aiarenanet/,m1ndfuck,1572870600,,0,4
148,2019-11-4,2019,11,4,21,drh5js,[P] Machine Learning for Excel. Anyone interested?,https://www.reddit.com/r/MachineLearning/comments/drh5js/p_machine_learning_for_excel_anyone_interested/,DomLiu,1572871730,"Hello everyone! I'm looking into building a machine learning Excel Add-In. Here is how it works (roughly):

1. Prepare a sheet of training data, one of the columns contain the target or label, other columns are features.
2. Open the add-in. Select relevant columns for training. For each column, choose whether it is categorical or numerical.
3. The data is submitted to a cluster of servers and the servers automatically try different types of models and hyperparameters to produce the most accurate results.
4. Then the user can use the add-in to make predictions on new data in another Excel sheet.

Would this be useful for people who don't know how to train machine learning models?",96,21
149,2019-11-4,2019,11,4,21,drh6we,"[N] Artificial Intelligence: A Modern Approach (4th Edition, releasing 3, February 2020.",https://www.reddit.com/r/MachineLearning/comments/drh6we/n_artificial_intelligence_a_modern_approach_4th/,35-56,1572871930,,0,1
150,2019-11-4,2019,11,4,22,drhb22,"""[N]"" Artificial Intelligence: A Modern Approach, 4th Edition releasing 3, February 2020.",https://www.reddit.com/r/MachineLearning/comments/drhb22/n_artificial_intelligence_a_modern_approach_4th/,35-56,1572872534,,0,1
151,2019-11-4,2019,11,4,22,drhfba,"Artificial Intelligence: A Modern Approach, 4th Edition releasing 3, February 2020.",https://www.reddit.com/r/MachineLearning/comments/drhfba/artificial_intelligence_a_modern_approach_4th/,35-56,1572873140,,0,1
152,2019-11-4,2019,11,4,22,drhtkl,[R] Announcing Confident Learning: Finding and Learning with Label Errors in Datasets,https://www.reddit.com/r/MachineLearning/comments/drhtkl/r_announcing_confident_learning_finding_and/,cgnorthcutt,1572875201,"&amp;#x200B;

Post: [https://l7.curtisnorthcutt.com/confident-learning](https://l7.curtisnorthcutt.com/confident-learning)

Paper: [https://arxiv.org/abs/1911.00068](https://arxiv.org/abs/1911.00068)

Code: [https://github.com/cgnorthcutt/cleanlab/](https://github.com/cgnorthcutt/cleanlab/)

Abstract: Learning exists in the context of data, yet notions of confidence typically focus on model predictions, not label quality. Confident learning (CL) has emerged as an approach for characterizing, identifying, and learning with noisy labels in datasets, based on the principles of pruning noisy data, counting to estimate noise, and ranking examples to train with confidence. Here, we generalize CL, building on the assumption of a classification noise process, to directly estimate the joint distribution between noisy (given) labels and uncorrupted (unknown) labels. This generalized CL, open-sourced as cleanlab, is provably consistent under reasonable conditions, and experimentally performant on ImageNet and CIFAR, outperforming recent approaches, e.g. MentorNet, by 30% or more, when label noise is non-uniform. cleanlab also quantifies ontological class overlap, and can increase model accuracy (e.g. ResNet) by providing clean data for training.

&amp;#x200B;

Hi, Reddit. I'm excited to share confident learning for characterizing, finding, and learning with label errors in datasets. To promote and standardize future research in learning with noisy labels and weak supervision, I've also open-sourced the cleanlab Python package: [https://pypi.org/project/cleanlab/](https://pypi.org/project/cleanlab/)",16,143
153,2019-11-4,2019,11,4,22,drhv82,[D] Thoughts on Quantum Artificial Intelligence / Q Supremacy,https://www.reddit.com/r/MachineLearning/comments/drhv82/d_thoughts_on_quantum_artificial_intelligence_q/,danielhanchen,1572875425,"&amp;#x200B;

[\\""Quantum Computing: The Why and How  Jonathan Baker, UChicago\\"" https:\/\/www.youtube.com\/watch?v=5kTiB\_KDUj0](https://preview.redd.it/c14lq0ox2ow31.png?width=1256&amp;format=png&amp;auto=webp&amp;s=ba9fa8531b570972797302ad58111019260c0698)

Hey :) So just wanted to start a discussion on what people think about whether Quantum Algorithms will ""revolutinize"" machine learning algorithms. I'm not a quantum expert, so take my stance with a grain of salt.

I was watching many videos (""Quantum algorithm for solving linear equations"" [https://www.youtube.com/watch?v=KtIPAPyaPOg](https://www.youtube.com/watch?v=KtIPAPyaPOg), ""Seth Lloyd: Quantum Machine Learning"" [https://www.youtube.com/watch?v=wkBPp9UovVU](https://www.youtube.com/watch?v=wkBPp9UovVU) etc etc) + reading Wikipedia blah. Then I came across the diagram above.

According to Jonathan Baker, there are 3 main future trends for QPCs. I just extrapolated his graphs. The green line is most optimistic, utilising ""co-design""??? which I don't know what that means. The read is less steep, and the blue is just a straight line continuation of the current # of qubit trend. (Notice the log10 scale)

QAOA or Quantum Approximation Optimization Algorithms include Quantum Linear Regression, and possibly??? (I don't know) optimisation methods for backprop. The green line shows by 2025 QPCs can be used for Linear Reg. Red which is like average case is 2035? and worst case is 2045.

To crach cryptography (ie Shor's Algorithm), over 10,000 # of qubits are needed. By green best case, that will be at 2032. Average is 2045 and worst is 2067.

I was also reading Wikipedia ""Quantum algorithm for linear systems of equations"" [https://en.wikipedia.org/wiki/Quantum\_algorithm\_for\_linear\_systems\_of\_equations](https://en.wikipedia.org/wiki/Quantum_algorithm_for_linear_systems_of_equations), and it highlights how solving X \* beta = y or A \* x = b takes O( log(P) \* K\^2 ), where K is the condition number and P is the # of coefficients in beta. The best conjugate gradeint method takes O( P \* K ).

More concretely, the ""exponential speedup"" (I think???) applies to sparse matrices. If you include error bounds, then you get O( log(P) \* K\^2 / err ). For dense matrices you get O( sqrt(P) log(P) K\^2 ).

The issue I see is since methods all include error bounds, it isn't necessarily a good way to compare direct methods with quantum algos. A better way is to compare randomized methods, where an ""exponential speedup"" is also possible by sketching only log(N) rows. It's possible to also say apply the randomized methods with Q Algos, hence in total you might get a staggering ""exponential-exponential"" speedup, but because Q Algos are inherently have error, this will exaggerate the error a lot.

So what do people think about the potential of Q Algos for ML ?

PS: The graph above is suprisingly a log10 plot (ie x10). This is clearly different from Moore's Law graph (x2 for # of transistors), but anyways I'm guessing Qubits don't follow Moores Law",10,0
154,2019-11-4,2019,11,4,22,drhxt5,"Q-Learning Tic-Tac-Toe, Briefly",https://www.reddit.com/r/MachineLearning/comments/drhxt5/qlearning_tictactoe_briefly/,ajschumacher,1572875787,[removed],0,2
155,2019-11-4,2019,11,4,23,driecu,OffWorld Gym: Open-access physical robotics environment for real-world reinforcement learning benchmarking and research,https://www.reddit.com/r/MachineLearning/comments/driecu/offworld_gym_openaccess_physical_robotics/,ArtisticLunch,1572877991,,0,1
156,2019-11-4,2019,11,4,23,drifgl,"Finally! coding with ipad, smartphone, TV and refridgerator",https://www.reddit.com/r/MachineLearning/comments/drifgl/finally_coding_with_ipad_smartphone_tv_and/,eungbean,1572878132,[removed],0,1
157,2019-11-4,2019,11,4,23,drikcw,You can find a lot of interesting things in the loss landscape of your neural network,https://www.reddit.com/r/MachineLearning/comments/drikcw/you_can_find_a_lot_of_interesting_things_in_the/,universome,1572878746,[removed],0,1
158,2019-11-5,2019,11,5,0,drj12t,[R] You can find a lot of interesting things in the loss landscape of your neural network,https://www.reddit.com/r/MachineLearning/comments/drj12t/r_you_can_find_a_lot_of_interesting_things_in_the/,universome,1572880835,"Just sharing with you a small (and somewhat fun) project I was recently working on, which is about finding different patterns in the loss surface of neural networks. Usually, a landscape around a minimum looks like a pit with random hills and mountains surrounding it, but there exist more meaningful ones, like in the picture below (check the paper for more results). We have discovered that you can find a minimum with (almost) any landscape you like. An interesting thing is that the found landscape pattern remains valid even for a *test* set, i.e. it is a property that (most likely) remains valid for the whole data distribution.

&amp;#x200B;

https://preview.redd.it/t885u6vosow31.png?width=1810&amp;format=png&amp;auto=webp&amp;s=793644af78a5430368e7a1c05d7b38c6b02ec637

Paper: [https://arxiv.org/abs/1910.03867](https://arxiv.org/abs/1910.03867)  
Code: [https://github.com/universome/loss-patterns](https://github.com/universome/loss-patterns)",62,319
159,2019-11-5,2019,11,5,1,drjt10,How to read text/watermark from images,https://www.reddit.com/r/MachineLearning/comments/drjt10/how_to_read_textwatermark_from_images/,kotesh_nitrkl,1572884150,[removed],0,1
160,2019-11-5,2019,11,5,1,drjxa8,[D] Federated machine learning,https://www.reddit.com/r/MachineLearning/comments/drjxa8/d_federated_machine_learning/,julian88888888,1572884604,,0,1
161,2019-11-5,2019,11,5,1,drjzlc,Training BERT for non-English language,https://www.reddit.com/r/MachineLearning/comments/drjzlc/training_bert_for_nonenglish_language/,thatphotoguy89,1572884873,"Hi /r/machinelearning,

I am looking to use the [huggingface Transformers BERT module](https://huggingface.co/transformers/model_doc/bert.html#) to train a BERT-like model for a MaskedLanguage model for a language other than English. It also wouldn't be using the same tokenizer. All the [examples](https://huggingface.co/transformers/model_doc/bert.html#bertformaskedlm) on the huggingface website seem to be using a pre-trained set of weights. This is mentioned on their [Github issues](https://github.com/huggingface/transformers/issues/954) page as well. Any pointers on where to start would be highly appreicated",0,1
162,2019-11-5,2019,11,5,1,drk8s0,Locating Moving Objects Using Stereo Sound Instead of Visual Input,https://www.reddit.com/r/MachineLearning/comments/drk8s0/locating_moving_objects_using_stereo_sound/,Yuqing7,1572885901,,0,1
163,2019-11-5,2019,11,5,1,drk9r0,Should I change project during the internship.,https://www.reddit.com/r/MachineLearning/comments/drk9r0/should_i_change_project_during_the_internship/,incenger,1572886025,[removed],0,1
164,2019-11-5,2019,11,5,2,drkri6,Setup your own ASR (Automatic Speech Recognition) Engine on AWS,https://www.reddit.com/r/MachineLearning/comments/drkri6/setup_your_own_asr_automatic_speech_recognition/,mrnerdy59,1572887957,,0,1
165,2019-11-5,2019,11,5,2,drkzg2,Vazam the vehicle in front of you,https://www.reddit.com/r/MachineLearning/comments/drkzg2/vazam_the_vehicle_in_front_of_you/,rthiy,1572888830,"I have an idea which I would like to realize, but I am new to programming (image recognition) and looking for someone with some knowledge who can give me some tips how to start with it. 

Short version of the idea:  [https://shazvehicle.rhilft.ch](https://shazvehicle.rhilft.ch/) 

BR",0,1
166,2019-11-5,2019,11,5,3,drm38k,[D] Visualizing Supervised Primitive Fitting Network (SPFN)?,https://www.reddit.com/r/MachineLearning/comments/drm38k/d_visualizing_supervised_primitive_fitting/,SexySaxMachine,1572893305,,0,1
167,2019-11-5,2019,11,5,4,drmwf1,What are the advantages and disadvantages when training set is same as scoring set,https://www.reddit.com/r/MachineLearning/comments/drmwf1/what_are_the_advantages_and_disadvantages_when/,vamsi115,1572896569,[removed],0,1
168,2019-11-5,2019,11,5,4,drn4jp,"Model Marketplace, Introducing Choice, Scale, and Security for the Enterprise",https://www.reddit.com/r/MachineLearning/comments/drn4jp/model_marketplace_introducing_choice_scale_and/,smbale,1572897462,,0,1
169,2019-11-5,2019,11,5,5,drn98j,"According to your opinion and best of knowledge, what are some of the most interesting projects in ML/AI that are addressing problems triggered by climate and/or climate change provoking problems?",https://www.reddit.com/r/MachineLearning/comments/drn98j/according_to_your_opinion_and_best_of_knowledge/,rufenmatt,1572897954,[removed],0,1
170,2019-11-5,2019,11,5,5,drn9ol,Gaps in time-series for LSTM classification model,https://www.reddit.com/r/MachineLearning/comments/drn9ol/gaps_in_timeseries_for_lstm_classification_model/,buchholzmd,1572897996,[removed],0,1
171,2019-11-5,2019,11,5,5,drnon5,Cost-sensitive classification in fraud prevention,https://www.reddit.com/r/MachineLearning/comments/drnon5/costsensitive_classification_in_fraud_prevention/,Tokukawa,1572899614,,0,1
172,2019-11-5,2019,11,5,5,drnzsg,MachineLearning on source code,https://www.reddit.com/r/MachineLearning/comments/drnzsg/machinelearning_on_source_code/,miss_Kick,1572900817,[removed],0,1
173,2019-11-5,2019,11,5,6,droqs0,Using super resolution for increasing old anime's quality,https://www.reddit.com/r/MachineLearning/comments/droqs0/using_super_resolution_for_increasing_old_animes/,mokoker,1572903804,[removed],0,1
174,2019-11-5,2019,11,5,7,drp3po,Visiting the SOSP 2019 AI System Workshop,https://www.reddit.com/r/MachineLearning/comments/drp3po/visiting_the_sosp_2019_ai_system_workshop/,Yuqing7,1572905311,,0,1
175,2019-11-5,2019,11,5,7,drpdam,Chollet comments on the pytorch community,https://www.reddit.com/r/MachineLearning/comments/drpdam/chollet_comments_on_the_pytorch_community/,stormtrooper1721,1572906405,,0,1
176,2019-11-5,2019,11,5,7,drpjud,Chollet's comments on the Pytorch community and Reddit,https://www.reddit.com/r/MachineLearning/comments/drpjud/chollets_comments_on_the_pytorch_community_and/,stormtrooper1721,1572907168,,0,1
177,2019-11-5,2019,11,5,8,drqdfg,About linear modeling,https://www.reddit.com/r/MachineLearning/comments/drqdfg/about_linear_modeling/,MinerAL2018,1572910568,"Hi guys I have a question: what tool do you know similar of h2o but for linear modeling? I would like to know if a tool like this exists and; on the other hand, some resources that I can use for enhance my skills in that topic will be appreciated. Thank you. PS I just use R :/",0,1
178,2019-11-5,2019,11,5,9,drqw9n,Jeff Dean in TF World: A single large model comprised of sub-models,https://www.reddit.com/r/MachineLearning/comments/drqw9n/jeff_dean_in_tf_world_a_single_large_model/,erfaneshrati,1572912916,[removed],0,1
179,2019-11-5,2019,11,5,9,drr9br,[D] The Machine Learning Conference is next week - Free ticket code here,https://www.reddit.com/r/MachineLearning/comments/drr9br/d_the_machine_learning_conference_is_next_week/,shonburton,1572914501,"Hi All, as a thank you to this community, we're giving away 5 free tickets to MLconf SF. The event is almost sold out so first to register gets the tickets: [https://www.eventbrite.com/e/mlconf-sf-2019-tickets-52641374769](https://www.eventbrite.com/e/mlconf-sf-2019-tickets-52641374769)

For free registration use code: slashmlfree

Note: The code will expire once the 5 free tickets are registered, they're gone. First come first served. If the code stops working you  may still be able to use to 50% off code: slashml19

\*Please don't share the code, we're like these tickets to go to community members.

\*\*If you've already purchased a ticket, you're not eligible for a refund.  


For video from past events see: [https://www.youtube.com/channel/UCjeM1xxYb\_37bZfyparLS3Q/videos](https://www.youtube.com/channel/UCjeM1xxYb_37bZfyparLS3Q/videos)",0,1
180,2019-11-5,2019,11,5,10,drrlhu,Any good review papers on time series forecasting of stocks?,https://www.reddit.com/r/MachineLearning/comments/drrlhu/any_good_review_papers_on_time_series_forecasting/,hypocriteseverywhere,1572916014,[removed],0,1
181,2019-11-5,2019,11,5,10,drs6vn,[D] ICLR 2020 Reviews,https://www.reddit.com/r/MachineLearning/comments/drs6vn/d_iclr_2020_reviews/,turing_1997,1572918661,ICLR 2020 reviews will be released on 4th November 2019. Creating a discussion thread for this year's reviews.,77,59
182,2019-11-5,2019,11,5,13,dru10v,Why Data Analysis for Small E-commerce Businesses Is Best Outsourced?,https://www.reddit.com/r/MachineLearning/comments/dru10v/why_data_analysis_for_small_ecommerce_businesses/,Countants123,1572927472,,0,1
183,2019-11-5,2019,11,5,14,drujy2,Is image caption generation is good for undergraduate project?,https://www.reddit.com/r/MachineLearning/comments/drujy2/is_image_caption_generation_is_good_for/,Pratik668,1572930385,[removed],0,1
184,2019-11-5,2019,11,5,14,drurub,[P] Using AI to see which Celebs Photoshop their Instagram pictures.,https://www.reddit.com/r/MachineLearning/comments/drurub/p_using_ai_to_see_which_celebs_photoshop_their/,wow998877,1572931680,,0,1
185,2019-11-5,2019,11,5,14,druyv9,Clustering Problem,https://www.reddit.com/r/MachineLearning/comments/druyv9/clustering_problem/,sagar_paudel18,1572932904,[removed],0,1
186,2019-11-5,2019,11,5,14,drv140,[P] Using AI to see which Celebs Facetune their Instagram pictures.,https://www.reddit.com/r/MachineLearning/comments/drv140/p_using_ai_to_see_which_celebs_facetune_their/,ewelumokeke,1572933287,,0,1
187,2019-11-5,2019,11,5,16,drvq06,Trending AI projects of the Week 44-19,https://www.reddit.com/r/MachineLearning/comments/drvq06/trending_ai_projects_of_the_week_4419/,hiphop1987,1572937765,[removed],1,1
188,2019-11-5,2019,11,5,16,drvrkz,New to Machine Learning,https://www.reddit.com/r/MachineLearning/comments/drvrkz/new_to_machine_learning/,TheNotoriouz,1572938061,[removed],0,1
189,2019-11-5,2019,11,5,16,drvyfz,AI for fashion,https://www.reddit.com/r/MachineLearning/comments/drvyfz/ai_for_fashion/,faya333,1572939420,[removed],0,1
190,2019-11-5,2019,11,5,18,drwyi0,"AI, Deep Learning, Computer Vision and the Insurance Industry",https://www.reddit.com/r/MachineLearning/comments/drwyi0/ai_deep_learning_computer_vision_and_the/,manneshiva,1572946884,[removed],0,1
191,2019-11-5,2019,11,5,20,drxro9,AI and Dynamic Pricing  Secret Weapon of Tech Giants Today,https://www.reddit.com/r/MachineLearning/comments/drxro9/ai_and_dynamic_pricing_secret_weapon_of_tech/,Albertchristopher,1572952610,,0,1
192,2019-11-5,2019,11,5,20,drxvd0,[D] 2020 Residencies Applicants Discussion Thread,https://www.reddit.com/r/MachineLearning/comments/drxvd0/d_2020_residencies_applicants_discussion_thread/,mahaveer0suthar,1572953269,"* Facebook AI Residency Program \[[Link](https://research.fb.com/programs/facebook-ai-residency-program/)\]. Application Deadline: January 31, 2020, 05:00pm PST.
* Google AI Residency \[[Link](https://ai.google/research/join-us/ai-residency/)\]. Application Deadline: December 19th, 2019.
* Google X AI Residency \[[Link](https://x.company/careers-at-x/4225880002/)\]
* Google AI Resident (Health), 2020 Start - London, UK \[[Link](https://careers.google.com/jobs/results/136709006283416262-google-ai-resident-health-2020-start-fixed-term-employee/?company=Google&amp;company=Google%20Fiber&amp;company=YouTube&amp;employment_type=FULL_TIME&amp;hl=en_US&amp;jlo=en_US&amp;q=Residency%20Program%20Healthcare&amp;sort_by=relevance)\]
* Google AI Resident (Health), 2020 - Start Palo Alto, CA, USA \[[Link](https://careers.google.com/jobs/results/95901233513931462-google-ai-resident-health-2020-start-fixed-term-employee/?company=Google&amp;company=Google%20Fiber&amp;company=YouTube&amp;employment_type=FULL_TIME&amp;hl=en_US&amp;jlo=en_US&amp;q=Residency%20Program%20Healthcare&amp;sort_by=relevance)\]
* OpenAI 2020 Winter Scholars \[[Link](https://jobs.lever.co/openai/d30e1f04-b548-4503-ba8b-9853cb49bdc7)\]. Application Deadline: Nov 15, 2019.

Thought it would be helpful to have a discussion thread for 2020 Residencies applicants to share the updates, info, resources to prepare etc.

Below are some useful discussion threads :

[https://www.reddit.com/r/MachineLearning/comments/9uyzc1/d\_google\_ai\_residency\_2019\_applicants\_discussion/](https://www.reddit.com/r/MachineLearning/comments/9uyzc1/d_google_ai_residency_2019_applicants_discussion/)

[https://www.reddit.com/r/MachineLearning/comments/7rajic/d\_anyone\_heard\_back\_from\_google\_ai\_residency/](https://www.reddit.com/r/MachineLearning/comments/7rajic/d_anyone_heard_back_from_google_ai_residency/)

[https://www.reddit.com/r/MachineLearning/comments/7wst07/d\_study\_guides\_for\_interview\_at\_ai\_research/](https://www.reddit.com/r/MachineLearning/comments/7wst07/d_study_guides_for_interview_at_ai_research/)

[https://www.reddit.com/r/MachineLearning/comments/690ixs/d\_google\_brain\_residency\_requirements\_and/](https://www.reddit.com/r/MachineLearning/comments/690ixs/d_google_brain_residency_requirements_and/)",43,136
193,2019-11-5,2019,11,5,21,dryee2,What all cost are covered in Google AI Residency program?,https://www.reddit.com/r/MachineLearning/comments/dryee2/what_all_cost_are_covered_in_google_ai_residency/,-TheBoyWhoLived,1572956604,[removed],0,1
194,2019-11-5,2019,11,5,22,drysz4,[R] Learning similarity measures from data: Extended Siamese Neural Networks,https://www.reddit.com/r/MachineLearning/comments/drysz4/r_learning_similarity_measures_from_data_extended/,epic,1572958804,"**Short summary:** We extend the state of the art of using neural networks to learn similarity measures (learning distance between pairs of data points in a dataset) with a new method called Extended Siamese Neural Networks (eSNN). This new method is put into the context of a framework that is used to describe different types of similarity measures. eSNN outperforms all current methods, while at the same time requiring the least training of all methods (down to half of the training of some comparable methods).  
**Paper link (Open Access)**: [https://link.springer.com/article/10.1007/s13748-019-00201-2](https://link.springer.com/article/10.1007/s13748-019-00201-2)  
**PDF (Open Access)**: [https://link.springer.com/content/pdf/10.1007%2Fs13748-019-00201-2.pdf](https://link.springer.com/content/pdf/10.1007%2Fs13748-019-00201-2.pdf)  
**Code**: [https://github.com/ntnu-ai-lab/eSNN/](https://github.com/ntnu-ai-lab/eSNN/)

**Authors**: [Bjrn Magnus Mathisen](https://orcid.org/0000-0002-8063-1835), [Agnar Aamodt](https://orcid.org/0000-0003-0114-8931), [Kerstin Bach](https://orcid.org/0000-0002-4256-7676) and [Helge Langseth](https://orcid.org/0000-0001-6324-6284)

**Abstract**:

&gt;Defining similarity measures is a requirement for some machine learning methods. One such method is case-based reasoning (CBR) where the similarity measure is used to retrieve the stored case or a set of cases most similar to the query case. Describing a similarity measure analytically is challenging, even for domain experts working with CBR experts. However, datasets are typically gathered as part of constructing a CBR or machine learning system. These datasets are assumed to contain the features that correctly identify the solution from the problem features; thus, they may also contain the knowledge to construct or learn such a similarity measure. The main motivation for this work is to automate the construction of similarity measures using machine learning. Additionally, we would like to do this while keeping training time as low as possible. Working toward this, our objective is to investigate how to apply machine learning to effectively learn a similarity measure. Such a learned similarity measure could be used for CBR systems, but also for clustering data in semi-supervised learning, or one-shot learning tasks. Recent work has advanced toward this goal which relies on either very long training times or manually modeling parts of the similarity measure. We created a framework to help us analyze the current methods for learning similarity measures. This analysis resulted in two novel similarity measure designs: The first design uses a pre-trained classifier as basis for a similarity measure, and the second design uses as little modeling as possible while learning the similarity measure from data and keeping training time low. Both similarity measures were evaluated on 14 different datasets. The evaluation shows that using a classifier as basis for a similarity measure gives state-of-the-art performance. Finally, the evaluation shows that our fully data-driven similarity measure design outperforms state-of-the-art methods while keeping training time low.

Hope you like my work. I will try to answer questions if you have any.",2,17
195,2019-11-5,2019,11,5,22,dryv1p,AI is reading better than humans,https://www.reddit.com/r/MachineLearning/comments/dryv1p/ai_is_reading_better_than_humans/,GuruTechnolab,1572959061,,0,1
196,2019-11-5,2019,11,5,22,drzbi3,Fast number plate detection on cars,https://www.reddit.com/r/MachineLearning/comments/drzbi3/fast_number_plate_detection_on_cars/,sergeevii123,1572961430,[removed],0,1
197,2019-11-5,2019,11,5,23,drzlnc,"Topological data analysis &amp; machine learning, new python library Giotto is bridging the gap",https://www.reddit.com/r/MachineLearning/comments/drzlnc/topological_data_analysis_machine_learning_new/,Giotto_Ai,1572962841,[removed],0,1
198,2019-11-5,2019,11,5,23,drzoix,Machine Learning and Its Application,https://www.reddit.com/r/MachineLearning/comments/drzoix/machine_learning_and_its_application/,Uniquesoftwaredev,1572963209,,0,1
199,2019-11-5,2019,11,5,23,drzu10,Why Machine Learning Matters,https://www.reddit.com/r/MachineLearning/comments/drzu10/why_machine_learning_matters/,Uniquesoftwaredev,1572963900,,0,1
200,2019-11-5,2019,11,5,23,drzxkl,Fast number plate detection on cars [OC],https://www.reddit.com/r/MachineLearning/comments/drzxkl/fast_number_plate_detection_on_cars_oc/,sergeevii123,1572964365,,0,2
201,2019-11-5,2019,11,5,23,ds00cs,Deep Reinforcement Learning Dataset Size,https://www.reddit.com/r/MachineLearning/comments/ds00cs/deep_reinforcement_learning_dataset_size/,Razanson,1572964709,[removed],0,1
202,2019-11-5,2019,11,5,23,ds0ac1,What makes a dataset a good one?,https://www.reddit.com/r/MachineLearning/comments/ds0ac1/what_makes_a_dataset_a_good_one/,mphuget,1572965909,[removed],0,1
203,2019-11-6,2019,11,6,0,ds0cfx,Matlab feels like cheating,https://www.reddit.com/r/MachineLearning/comments/ds0cfx/matlab_feels_like_cheating/,ikezhang66,1572966179,[removed],0,1
204,2019-11-6,2019,11,6,0,ds0st4,[R] Adversarial explanations for understanding image classification decisions and improved neural network robustness,https://www.reddit.com/r/MachineLearning/comments/ds0st4/r_adversarial_explanations_for_understanding/,waltywalt,1572968093,"**Abstract:**

&gt;For sensitive problems, such as medical imaging or fraud detection,  neural network (NN) adoption has been slow due to concerns about their  reliability, leading to a number of algorithms for explaining their  decisions. NNs have also been found to be vulnerable to a class of  imperceptible attacks, called adversarial examples, which arbitrarily  alter the output of the network. Here we demonstrate both that these  attacks can invalidate previous attempts to explain the decisions of  NNs, and that with very robust networks, the attacks themselves may be  leveraged as explanations with greater fidelity to the model. We also  show that the introduction of a novel regularization technique inspired  by the Lipschitz constraint, alongside other proposed improvements  including a half-Huber activation function, greatly improves the  resistance of NNs to adversarial examples. On the ImageNet  classification task, we demonstrate a network with an  accuracy-robustness area (ARA) of 0.0053, an ARA 2.4 times greater than  the previous state-of-the-art value. Improving the mechanisms by which  NN decisions are understood is an important direction for both  establishing trust in sensitive domains and learning more about the  stimuli to which NNs respond.

**Open Access pre-print:** [https://arxiv.org/abs/1906.02896](https://arxiv.org/abs/1906.02896)

**Open Access PDF (low-resolution images, due to size restriction):** [https://arxiv.org/pdf/1906.02896.pdf](https://arxiv.org/pdf/1906.02896.pdf)

**Peer-reviewed publication (with full-resolution images; also see bottom of this Reddit post):** [https://www.nature.com/articles/s42256-019-0104-6](https://www.nature.com/articles/s42256-019-0104-6)

**Code:** [https://github.com/wwoods/adversarial-explanations-cifar/](https://github.com/wwoods/adversarial-explanations-cifar/)

[Comparing explanatory power between Grad-CAM \[Selvaraju et al. 2017\] and Adversarial Explanations \(AEs\) when applied to a robust NN trained on CIFAR-10.  The top four rows, subfigure a, demonstrate comparisons on different inputs.  For each row, the columns show: the original \`\`Input'' image, labeled with the most confidently-predicted class, the correct class, and the NN's confidence in each; two Grad-CAM explanations, one for each predicted class shown by the input; two AEs, divided into the adversarial noise used to produce the AE, and the AE itself.  Below those rows, subfigures b through i are annotated versions of the AEs for subfigure a, indicating regions which contributed to or detracted from each predicted class.  See the main text for full commentary.](https://preview.redd.it/ip56d0uxxvw31.png?width=1424&amp;format=png&amp;auto=webp&amp;s=84e3d8a0751dce4ce72d305a53163a0156d30901)

**Author's note:** The freely-available pre-print on ArXiv contains all content available in the Nature version, just in a slightly different ordering (IEEE vs Nature style).  The resolution of the ArXiv images is a bit lower, as the full document from pdflatex is \~97 MB due to included images...  A Ghostscript-optimized version, with full-resolution images, weighs in at 25MB and may be found here: [https://drive.google.com/open?id=1xGCja0BUQ2VR9nlKre6QzJ2Q-qpp8ub8](https://drive.google.com/open?id=1xGCja0BUQ2VR9nlKre6QzJ2Q-qpp8ub8)",4,9
205,2019-11-6,2019,11,6,0,ds13pg,Unstable accuracy of Gaussian Mixture Model classifier from sklearn,https://www.reddit.com/r/MachineLearning/comments/ds13pg/unstable_accuracy_of_gaussian_mixture_model/,asparox,1572969368,[removed],0,1
206,2019-11-6,2019,11,6,1,ds1arn,"UNC BIAG Releases Mermaid, Pytorch based image registration toolkit",https://www.reddit.com/r/MachineLearning/comments/ds1arn/unc_biag_releases_mermaid_pytorch_based_image/,SahinOlut,1572970165,[removed],0,1
207,2019-11-6,2019,11,6,1,ds1fge,Online Active Learning Frameworks,https://www.reddit.com/r/MachineLearning/comments/ds1fge/online_active_learning_frameworks/,ZoldyckFiend,1572970698,[removed],0,1
208,2019-11-6,2019,11,6,1,ds1r64,[R] Implementing an idea.,https://www.reddit.com/r/MachineLearning/comments/ds1r64/r_implementing_an_idea/,InfamousPianist,1572972048,"Text Modulation Audio book(TMA)



Many people don't enjoy the traditional reading style to know the story of a book; and have replaced this monotonous style with audio books. Actors and Authors have to take out a lot of time to make an audio book. This consumes a lot of time which could be used for other fruitful activities. Some people have tried to use the text to speech tool, but that bland, robotic sound is equivalent to the reading method. TMA will not only convert text into speech, but with the help of AI it will detect the genre and accordingly add appropriate voice modulation and also some background music. It will detect the genre through some key words, for e.g if the program finds 'crime', 'murder', 'evidence' etc. in the text, it will take it as a thriller genre and if it finds the author as 'Agatha Christie' then it will know the mystery genre through the data fed into the program. This will not only save time, but will also allow readers to read books consisting of 1000+ pages(e.g Mein Kampf, IT, etc.). This will help in the field of education, by attracting more and more students towards books and help them by building their vocabulary, IQ and EQ. This will change the current scenario and make reading books a lot more interesting.",6,4
209,2019-11-6,2019,11,6,1,ds1xvc,"[D] Deep Learning has a size problem. We need to focus on state-of-the-art efficiency, not state-of-the-art accuracy.",https://www.reddit.com/r/MachineLearning/comments/ds1xvc/d_deep_learning_has_a_size_problem_we_need_to/,jamesonatfritz,1572972832,"I'm not sure the recent trend of larger and larger models is going to help make deep learning more useful or applicable. Mulit-billion parameter models might add a few percentage points of accuracy, but they don't make it easier to build DL-powered applications or help other people start using the technology.

At the same time, there are some incredible results out there applying techniques like distillation, pruning, and quantization. I'd love for it to be standard practice to apply these techniques to more projects to see just how small and efficient we can make models.

For anyone interested in the topic, I wrote up a brief primer on the problem and some research into solutions. I'd love to hear of any success or failures people here have had with these techniques in production settings.

[https://heartbeat.fritz.ai/deep-learning-has-a-size-problem-ea601304cd8](https://heartbeat.fritz.ai/deep-learning-has-a-size-problem-ea601304cd8)",134,520
210,2019-11-6,2019,11,6,2,ds21vx,Dataset request - Human Face Description,https://www.reddit.com/r/MachineLearning/comments/ds21vx/dataset_request_human_face_description/,anand_altekar,1572973319,[removed],0,1
211,2019-11-6,2019,11,6,2,ds28lr,Machine Learning Does Not Exist,https://www.reddit.com/r/MachineLearning/comments/ds28lr/machine_learning_does_not_exist/,NotAlphaGo,1572974093,,0,1
212,2019-11-6,2019,11,6,2,ds28sd,Looking for images with different exposures (exposure bracketing) for an ML project,https://www.reddit.com/r/MachineLearning/comments/ds28sd/looking_for_images_with_different_exposures/,AlanRoofies,1572974116,[removed],0,1
213,2019-11-6,2019,11,6,2,ds2hei,[D] Cloud Compute platforms that accept Bitcoin?,https://www.reddit.com/r/MachineLearning/comments/ds2hei/d_cloud_compute_platforms_that_accept_bitcoin/,unrulyspeed,1572975115,"Hi, I was wondering if anyone knows of a GPU Cloud Compute platform that accepts cryptocurrency?

I know Vultr does, but unfortunately they only offer VPS/storage services.",3,14
214,2019-11-6,2019,11,6,2,ds2irv,[D] ICLR reviews are out!,https://www.reddit.com/r/MachineLearning/comments/ds2irv/d_iclr_reviews_are_out/,zergylord,1572975265,Good time to discuss scores and plans for the rebuttal/revision process.,32,21
215,2019-11-6,2019,11,6,2,ds2m6u,[D] Training tensor2tensor on my own dataset?,https://www.reddit.com/r/MachineLearning/comments/ds2m6u/d_training_tensor2tensor_on_my_own_dataset/,kausarahmad,1572975638,"I'm working on a project where I'm required to train T2T on a custom dataset. 

Now, I've looked at tensor2tensor's own *vaguely brief* documentation ([https://tensorflow.github.io/tensor2tensor/new\_problem.html](https://tensorflow.github.io/tensor2tensor/new_problem.html)) on how to do this. I have also browsed the *very short* amount of implementations available online which are either unclear or incomplete. 

And as you can infer by now, I'm frustrated. Why is there a general lack of discussion about this? Have any of you trained t2t on your own dataset?",4,0
216,2019-11-6,2019,11,6,2,ds2mn9,Open AI releases GPT-2 model,https://www.reddit.com/r/MachineLearning/comments/ds2mn9/open_ai_releases_gpt2_model/,prudentio,1572975694,,0,1
217,2019-11-6,2019,11,6,2,ds2vdf,OpenAI releases GPT-2 with 1.5B parameters!,https://www.reddit.com/r/MachineLearning/comments/ds2vdf/openai_releases_gpt2_with_15b_parameters/,maykulkarni,1572976697,,0,3
218,2019-11-6,2019,11,6,3,ds2zg1,[D] Estimator Behavior of Categorical Encoding Strategies,https://www.reddit.com/r/MachineLearning/comments/ds2zg1/d_estimator_behavior_of_categorical_encoding/,jaredstufft,1572977163,,0,1
219,2019-11-6,2019,11,6,3,ds2zln,Consequences/issues with Membership Inference Hacking?,https://www.reddit.com/r/MachineLearning/comments/ds2zln/consequencesissues_with_membership_inference/,MakeOrBreak1234,1572977180,[removed],0,1
220,2019-11-6,2019,11,6,3,ds32wu,"[N] GPT-2 1.5B, the largest GPT-2 model, has been released.",https://www.reddit.com/r/MachineLearning/comments/ds32wu/n_gpt2_15b_the_largest_gpt2_model_has_been/,minimaxir,1572977575,[removed],0,1
221,2019-11-6,2019,11,6,3,ds3cva,"[D] History of NLP: During the Enlightenment, Leibniz dreamed of a machine that could calculate ideas",https://www.reddit.com/r/MachineLearning/comments/ds3cva/d_history_of_nlp_during_the_enlightenment_leibniz/,newsbeagle,1572978740,"This offbeat essay argues that the roots of NLP go way back. In the 17th century, Gottfried Leibniz imagined a ""great instrument of reason that wouldn't just generate text, it would generate entire ideas. 

[https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-the-17th-century-leibniz-dreamed-of-a-machine-that-could-calculate-ideas](https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-the-17th-century-leibniz-dreamed-of-a-machine-that-could-calculate-ideas)

""Leibniz ... embarked on a project to create his own method of idea generation through symbolic combination. He wanted to use his machine not for theological debate, but for philosophical reasoning.He proposed that such a system would require three things: an 'alphabet of human thoughts'; a list of logical rules for their valid combination and re-combination;and a mechanism that could carry out the logical operations on the symbols quickly and accurately...""",0,15
222,2019-11-6,2019,11,6,3,ds3duz,[Free Course-Udemy] - The Ultimate 2019 Deep Learning &amp; machine Learning Bootcamp,https://www.reddit.com/r/MachineLearning/comments/ds3duz/free_courseudemy_the_ultimate_2019_deep_learning/,upworknepal,1572978856,[removed],0,1
223,2019-11-6,2019,11,6,3,ds3joh,[P]Creating a Machine Learning Framework on the top of Tensorflow,https://www.reddit.com/r/MachineLearning/comments/ds3joh/pcreating_a_machine_learning_framework_on_the_top/,Kaljit,1572979538," 

Hey Wizards, I am creating a Machine Learning a Framework on the top of Tensorflow to accelerate the compute involved on GPUs. Any suggestions are welcome!!!!

Here's the GitHub link of my code:

[https://github.com/kaljitism/TensorML](https://github.com/kaljitism/TensorML)",4,0
224,2019-11-6,2019,11,6,4,ds3zbj,[D] Location of ranked list of architectures that perform well on imagenet?,https://www.reddit.com/r/MachineLearning/comments/ds3zbj/d_location_of_ranked_list_of_architectures_that/,eddiemcenrue,1572981282,"At some point in time, I swear I came across a ranked table of architectures that perform well on imagenet with one of the columns including the size of the architecture. I have no idea where it went, but it was an incredibly useful resource for me to argue the inclusion of certain smaller networks in project discussions",3,1
225,2019-11-6,2019,11,6,4,ds45fr,[D] Looking for university course selection advice,https://www.reddit.com/r/MachineLearning/comments/ds45fr/d_looking_for_university_course_selection_advice/,drhectapus,1572981983,"Looking for course selection for my final two semesters of school as a CS major.

So far I've only completed a course on Machine Learning (similar level to Stanford's [CS229](http://cs229.stanford.edu/)) and the rest have been CS courses not related to ML. Aside from discrete maths,I haven't taken any additional math courses and all of it has been self-taught (probability, linear algebra, calculus).

&amp;#x200B;

My goal is to graduate having a deep understanding of how ML algorithm work at the mathematical level and be able to understand most of the maths in ML papers. I'm not looking to do a PhD per se, but I'd like to be more of an ""academic ML engineer"". My particular interests are ML and NLP applied in healthcare.

&amp;#x200B;

With all that said, which courses should I pick over the next two semesters to optimize my goals? Keep in mind that I'll be doing applied ML research under a professor in both semesters as well (likely to do with analyzing text in the healthcare setting). In an ideal world I'd take all these courses because they all seem super interesting, but with limited time, I'd rather pick the ones which will give me a solid foundation so I can self-learn the others later on.

&amp;#x200B;

**Spring 2020 (Pick two):**

Computational Linguistics (NLP)

Deep Learning for Data Science

Modern Convex Optimization

Bayesian Statistics

&amp;#x200B;

**Fall 2020 (Pick two):**

Elements of Probability Theory and Random Processes

Mathematical Statistics

Introduction to Optimization Theory

Advanced Analysis

&amp;#x200B;

Thanks!",5,1
226,2019-11-6,2019,11,6,5,ds5cpk,Do Deep Neural Networks See Faces Like Brains Do?,https://www.reddit.com/r/MachineLearning/comments/ds5cpk/do_deep_neural_networks_see_faces_like_brains_do/,Yuqing7,1572987071,,0,1
227,2019-11-6,2019,11,6,6,ds6b2o,"[D] OpenAI releases GPT-2 1.5B model despite ""extremist groups can use GPT-2 for misuse"" but ""no strong evidence of misuse so far"".",https://www.reddit.com/r/MachineLearning/comments/ds6b2o/d_openai_releases_gpt2_15b_model_despite/,permalip,1572991160,"The findings:

1. **Humans find GPT-2 outputs convincing**
2. **GPT-2 can be fine-tuned for misuse**
3. **Detection is challenging**
4. **Weve seen no strong evidence of misuse so far**
5. **We need standards for studying bias**

They are going against their own word, but nevertheless, it's nice to see that they are releasing everything.

&amp;#x200B;

Read the full blog post here: [https://openai.com/blog/gpt-2-1-5b-release/](https://openai.com/blog/gpt-2-1-5b-release/)

&amp;#x200B;

GitHub Model: [https://github.com/openai/gpt-2](https://github.com/openai/gpt-2)

GitHub Dataset: [https://github.com/openai/gpt-2-output-dataset](https://github.com/openai/gpt-2-output-dataset)",70,67
228,2019-11-6,2019,11,6,7,ds6sgo,"I understand that neural networks have an output layer, but how does this translate to complex outputs?",https://www.reddit.com/r/MachineLearning/comments/ds6sgo/i_understand_that_neural_networks_have_an_output/,theorizable,1572993265,[removed],1,1
229,2019-11-6,2019,11,6,7,ds6wg8,[1909.04164] Knowledge Enhanced Contextual Word Representations,https://www.reddit.com/r/MachineLearning/comments/ds6wg8/190904164_knowledge_enhanced_contextual_word/,mikeross0,1572993754,,2,1
230,2019-11-6,2019,11,6,7,ds71pn,[R] Knowledge Enhanced Contextual Word Representations,https://www.reddit.com/r/MachineLearning/comments/ds71pn/r_knowledge_enhanced_contextual_word/,mikeross0,1572994410,,4,3
231,2019-11-6,2019,11,6,8,ds7mk0,[D] Explaining your ICLR reviews,https://www.reddit.com/r/MachineLearning/comments/ds7mk0/d_explaining_your_iclr_reviews/,programmerChilli,1572997010,"Since the respective review threads are always flooded with people talking about their reviews, I thought I should post some stats on various reviews. These are my guesses about how you should interpret your reviews (based off of previous year's trends). Note that the quantization of reviews from a 1-10 scale to a [1,3,6,8] scale may make historical data less predictive, especially in the presence of rebuttals. 

So what do your reviews mean? First, know that the acceptance rate at ICLR has historically been around 30%.

[3,3,3] or below: Unfortunately, you are in the bottom third of papers. Typically, there will only be a handful of papers that the AC's will rescue from this category. If you can convince 2 of your reviewers to bump your ratings up, you have a good shot. 

[3,3,6]: Your ratings aren't ideal, but you still have a good shot of being accepted. This year, 20% of papers got this rating (top 40-60 percentile). You're one good rebuttal away from having a good shot of being accepted. If you don't succeed in raising your scores, only about 10% of the papers in this range have been accepted, historically speaking. 

[3,6,6]: Congrats on the good ratings! Although you certainly aren't guaranteed acceptance, you definitely have a solid shot of acceptance. This year, [3,6,6] made up the 20-30 percentile of reviews. The results for papers in this range are the opposite of the 40-60 percentile range - only about 15% of papers in this range get rejected.

[6,6,6] or above: Congratulations on the likely acceptance! This year, you are in the top 10% of papers. Usually, you could probably sit back and relax - papers in the top 10% of ratings are nearly never rejected.  However, the flatness of the ratings this year makes that a much riskier endeavour. 

Now, let's talk about rebuttals. Unfortunately, rebuttals often don't affect the reviewers' ratings as much as you'd like. Historically, across all papers, only ~30% of papers have any reviewers update their scores following the rebuttal. Among borderline papers (ie: papers in the 30-60 percentiles), however, about 50% of papers will have reviewers update the scores.

Overall, what this means that roughly a quarter of the papers in the ""borderline"" category will move into ""accepted"". Given that the most common rating delta after rebuttals was +1, I don't know how reviewers will behave with the new quantized scores.

Good luck on your rebuttals, and remember that the reviewing system has an immense amount of random noise. This year, 47% of reviews were done by reviewers who said that they had not published in the area they're reviewing. Previously, NeurIPS has done a study showing that when the same papers were given to 2 separate AC's, 57% of the papers that one AC accepted were rejected by the other AC. See http://blog.mrtz.org/2014/12/15/the-nips-experiment.html for more reading.",10,42
232,2019-11-6,2019,11,6,8,ds7qmf,OpenAI Releases 1.5 Billion Parameter GPT-2 Model,https://www.reddit.com/r/MachineLearning/comments/ds7qmf/openai_releases_15_billion_parameter_gpt2_model/,Yuqing7,1572997543,,0,1
233,2019-11-6,2019,11,6,8,ds7rk5,Object-oriented Programming in Python,https://www.reddit.com/r/MachineLearning/comments/ds7rk5/objectoriented_programming_in_python/,arman_52,1572997669,,0,1
234,2019-11-6,2019,11,6,8,ds7wl8,Taken aback by how beautiful and poetic this auto-generated text is from GPT-2,https://www.reddit.com/r/MachineLearning/comments/ds7wl8/taken_aback_by_how_beautiful_and_poetic_this/,anonymousTestPoster,1572998298,,0,1
235,2019-11-6,2019,11,6,9,ds80cr,Object-oriented Programming in Python,https://www.reddit.com/r/MachineLearning/comments/ds80cr/objectoriented_programming_in_python/,arman_52,1572998829,[removed],0,1
236,2019-11-6,2019,11,6,9,ds8er8,Normalization methods in Deep Learning.,https://www.reddit.com/r/MachineLearning/comments/ds8er8/normalization_methods_in_deep_learning/,Climb2K2,1573000682,[removed],0,1
237,2019-11-6,2019,11,6,9,ds8fb0,How These Self-Aware Robots Are Redefining Consciousness,https://www.reddit.com/r/MachineLearning/comments/ds8fb0/how_these_selfaware_robots_are_redefining/,RogerBilly,1573000759,,1,1
238,2019-11-6,2019,11,6,10,ds9ep6,[D] Regarding Encryption of Deep learning models,https://www.reddit.com/r/MachineLearning/comments/ds9ep6/d_regarding_encryption_of_deep_learning_models/,aseembits93,1573005490,"My team works on deploying models on the edge (android mobile devices). The data, model, code, everything resides on the client device. Is there any way to protect your model from being probed into by the client? The data and predictions can be unencrypted. Please let me know your thoughts on this and any resources you can point me to. Thanks!",17,6
239,2019-11-6,2019,11,6,11,ds9ix9,CNC Hydraulic Guillotine Shear Machine Cutting Steel Plate Video,https://www.reddit.com/r/MachineLearning/comments/ds9ix9/cnc_hydraulic_guillotine_shear_machine_cutting/,CNCPressBrakeMachine,1573006060,,0,1
240,2019-11-6,2019,11,6,11,ds9sv3,Is Reinforcement Learning Practical?,https://www.reddit.com/r/MachineLearning/comments/ds9sv3/is_reinforcement_learning_practical/,edelweiss_ml,1573007397,"Is reinforcement learning practical at this point for industry work? The most prominent examples we see are from DeepMind (AlphaStar, AlphaGo), but the team are world-class researchers (over 40 of them) who also worked closely with expert Starcraft 2 players with ton of computing resources.

As someone who hasn't had much experience in RL, I see potential applications but am unsure of the amount of work or practicality of it. For example, one potential application for RL is to learn fraudulent behavior in an online retailer system (i.e. Amazon, EBay) and proactively find methods of fraud before they happen. One could imagine all the unintended behavior of misspecified reward function ( [https://openai.com/blog/faulty-reward-functions/](https://openai.com/blog/faulty-reward-functions/)).  But then I see comments (like this article [https://www.alexirpan.com/2018/02/14/rl-hard.html](https://www.alexirpan.com/2018/02/14/rl-hard.html)) about the sample inefficiency, not to mention having to build your own simulator (and hope it's representative to some degree).

What is ML reddit's opinion on the practicality of using RL in something like fraud? Does it even make sense to build a simple online retailer simulator? I ask because it would be a shame to learn RL only to find it totally impractical and better to use simpler methods to finding fraud MO's.",0,1
241,2019-11-6,2019,11,6,12,dsac70,[D] On the Difficulty of Evaluating Baselines: A Study on Recommender Systems,https://www.reddit.com/r/MachineLearning/comments/dsac70/d_on_the_difficulty_of_evaluating_baselines_a/,jody293,1573010086,"Here is the paper [https://arxiv.org/abs/1905.01395](https://arxiv.org/abs/1905.01395).  I read this paper recently. I found it is quite interesting. It points out some issues in this research field. In my view, the key claim is that we need standardized benchmarks and the whole community should converge to well-calibrated results. I didn't find any discussions here. So I create this post and look forward to some discussions.",3,2
242,2019-11-6,2019,11,6,13,dsayx2,[News] Microsoft's Project Silica succeeds in storing Superman film on a piece of glass.,https://www.reddit.com/r/MachineLearning/comments/dsayx2/news_microsofts_project_silica_succeeds_in/,Anirban_Hazra,1573013370,"&amp;#x200B;

[Zoomed image of 1978 film \\""Superman\\"" stored in a piece of glass.](https://preview.redd.it/woqog76lqzw31.png?width=923&amp;format=png&amp;auto=webp&amp;s=03c38df2227820dd4f4acfb046375ee57a196d14)

Cloud storage has now seamlessly entered into our digital lives to such an extent that we neither realize that we are utilizing it in many instances and nor do we comprehend the fact that all this data is being physically stored in hardware whose capacity in increasingly flattening. This problem is compounded by the fact that the amount of data that each of us generate is increasing exponentially.

Even if the infrastructure keeps up with this rising demand, the hardware such as disk drives itself have a lifespan of around five years. This means that to keep the data saved they have to be cyclically written on to newer hardware.

A unique solution thought of, was to use the same ultrashort optical pulses used in LASIK surgeries to store data in glass by permanently changing its structure. This can help keep the data saved for centuries. Quartz glass also doesnt need energy-intensive air conditioning to keep material at a constant temperature or systems that remove moisture from the air  both of which could lower the environmental footprint of large-scale data storage.

Continue reading: [https://latesttechnewswiki.blogspot.com/2019/11/microsoft-stores-superman-in-a-piece-of-glass.html](https://latesttechnewswiki.blogspot.com/2019/11/microsoft-stores-superman-in-a-piece-of-glass.html)",8,10
243,2019-11-6,2019,11,6,13,dsb9z8,Machine learning.,https://www.reddit.com/r/MachineLearning/comments/dsb9z8/machine_learning/,learn_machine_ravi,1573015040,Hi I am beginners in machine learning anyone help how should I  start this.,0,1
244,2019-11-6,2019,11,6,14,dsbt8c,Best way to count telco users in the city!!,https://www.reddit.com/r/MachineLearning/comments/dsbt8c/best_way_to_count_telco_users_in_the_city/,bekterra13,1573018178,[removed],0,1
245,2019-11-6,2019,11,6,15,dsc5le,Bringing Google AutoML to 3.5 million data scientists on Kaggle,https://www.reddit.com/r/MachineLearning/comments/dsc5le/bringing_google_automl_to_35_million_data/,antgoldbloom,1573020323,,0,1
246,2019-11-6,2019,11,6,17,dsda11,[P] Recurrent GAN code for iterative text-based image generation [ICCV 2019],https://www.reddit.com/r/MachineLearning/comments/dsda11/p_recurrent_gan_code_for_iterative_textbased/,deeplearning4ever,1573027593,"* GitHub: [https://github.com/Maluuba/GeNeVA](https://github.com/Maluuba/GeNeVA)
* Paper: [http://openaccess.thecvf.com/content\_ICCV\_2019/html/El-Nouby\_Tell\_Draw\_and\_Repeat\_Generating\_and\_Modifying\_Images\_Based\_on\_ICCV\_2019\_paper.html](http://openaccess.thecvf.com/content_ICCV_2019/html/El-Nouby_Tell_Draw_and_Repeat_Generating_and_Modifying_Images_Based_on_ICCV_2019_paper.html)
* Project Page: [https://www.microsoft.com/en-us/research/project/generative-neural-visual-artist-geneva/](https://www.microsoft.com/en-us/research/project/generative-neural-visual-artist-geneva/)
* Blog Post: [https://www.microsoft.com/en-us/research/blog/from-blank-canvas-unfolds-a-scene-gan-based-model-generates-and-modifies-images-based-on-continual-linguistic-instruction/](https://www.microsoft.com/en-us/research/blog/from-blank-canvas-unfolds-a-scene-gan-based-model-generates-and-modifies-images-based-on-continual-linguistic-instruction/)

Code for recurrent GAN for image generation on text-based iterative inputs.",6,35
247,2019-11-6,2019,11,6,17,dsdbme,[R] High Fidelity Video Prediction with Large Stochastic Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/dsdbme/r_high_fidelity_video_prediction_with_large/,baylearn,1573027902,,1,1
248,2019-11-6,2019,11,6,17,dsde0a,[N] Spleeter released by Deezer for Source Separation,https://www.reddit.com/r/MachineLearning/comments/dsde0a/n_spleeter_released_by_deezer_for_source/,Claree007,1573028409,"Spleeter is an open-source project from Deezer for source separation on music tracks. Built with keras and tensorflow. 

So basically this allows you to separate the vocal, drum, bass tracks and more from an mp3 file. They have provided a Google colab link so you can test their work without the need for installing anything. 

Blog post: https://deezer.io/releasing-spleeter-deezer-r-d-source-separation-engine-2b88985e797e
Github: https://github.com/deezer/spleeter
Colab: https://colab.research.google.com/github/deezer/spleeter/blob/master/spleeter.ipynb",44,191
249,2019-11-6,2019,11,6,18,dsds1i,Question: Is there a way to control network quality during ActiveLearning?,https://www.reddit.com/r/MachineLearning/comments/dsds1i/question_is_there_a_way_to_control_network/,speedyhoopster3,1573031188,[removed],0,1
250,2019-11-6,2019,11,6,18,dsdy5u,Begin Your Digital Transformation with Augmented Analytics!,https://www.reddit.com/r/MachineLearning/comments/dsdy5u/begin_your_digital_transformation_with_augmented/,ElegantMicroWebIndia,1573032416,,0,1
251,2019-11-6,2019,11,6,19,dse872,Easy Topic Classifier on iOS with Apples Natural Language Framework,https://www.reddit.com/r/MachineLearning/comments/dse872/easy_topic_classifier_on_ios_with_apples_natural/,omarmhaimdat,1573034427,,0,1
252,2019-11-6,2019,11,6,19,dsecqu,Technology is becoming so intelligent that it is gaining an insight of typical wife behavior,https://www.reddit.com/r/MachineLearning/comments/dsecqu/technology_is_becoming_so_intelligent_that_it_is/,michaeljohn03,1573035283,[removed],0,1
253,2019-11-6,2019,11,6,19,dseh2r,Transforming normal font to italic,https://www.reddit.com/r/MachineLearning/comments/dseh2r/transforming_normal_font_to_italic/,tofijak,1573036129,"Hey,  


I've looking at a project transforming a normal font character to an italic version. I currently work with [Character Font Images Dataset](https://archive.ics.uci.edu/ml/datasets/Character+Font+Images), which haves 153 fonts, including several variations of the fonts and the variations in normal and italic style.   


I was thinking of using a GAN to train it in two domains, normal and italic, with the data I have from the data set. The intention was that you input a normal font character and the network would transform it into an italic version, based on what it learned from the training in the two domains.

  
What GAN would you recommend as a starting point?",0,1
254,2019-11-6,2019,11,6,19,dsehp0,The Importance of Data Processing in Machine Learning &amp; AI,https://www.reddit.com/r/MachineLearning/comments/dsehp0/the_importance_of_data_processing_in_machine/,chrissteveuk,1573036262,,0,1
255,2019-11-6,2019,11,6,19,dsei66,Can anyone help me with my ML exam revision?,https://www.reddit.com/r/MachineLearning/comments/dsei66/can_anyone_help_me_with_my_ml_exam_revision/,Y0niii,1573036353,[removed],0,1
256,2019-11-6,2019,11,6,19,dselm0,[D] How easy is it to get into Deepmind in 2019?,https://www.reddit.com/r/MachineLearning/comments/dselm0/d_how_easy_is_it_to_get_into_deepmind_in_2019/,alexmlamb,1573036987,"More specifically: 

* What time does the office open and close in the morning?  
* Is it possible to get in during off hours if you have an access card?  
* Is the office open during the weekends?  
* Are the instructions for finding the entrance easy to follow?  
* How long is the walk from the metro station to the office?",10,0
257,2019-11-6,2019,11,6,19,dsemll,Implementing Python Script in Power BI,https://www.reddit.com/r/MachineLearning/comments/dsemll/implementing_python_script_in_power_bi/,SuperPiglet97,1573037188,[removed],0,1
258,2019-11-6,2019,11,6,19,dsepj2,I made simple self-driving AI with Reinforcement learning,https://www.reddit.com/r/MachineLearning/comments/dsepj2/i_made_simple_selfdriving_ai_with_reinforcement/,Mingoooose,1573037759,[removed],0,2
259,2019-11-6,2019,11,6,20,dsewnl,Machine Learning Project - Titanic dataset,https://www.reddit.com/r/MachineLearning/comments/dsewnl/machine_learning_project_titanic_dataset/,Rutvij07,1573039066,[removed],0,1
260,2019-11-6,2019,11,6,22,dsgexn,can GPT-2 sing ??!!!,https://www.reddit.com/r/MachineLearning/comments/dsgexn/can_gpt2_sing/,neonooo,1573047476,[removed],0,1
261,2019-11-6,2019,11,6,22,dsgkni,"[R] LG Service Robot autonomous navigation and people avoidance test at the Seoul, Incheon airport.",https://www.reddit.com/r/MachineLearning/comments/dsgkni/r_lg_service_robot_autonomous_navigation_and/,moverstreet007,1573048276,,0,1
262,2019-11-6,2019,11,6,23,dsgp7b,[D] Parallelization for neuroevolution AutoML models,https://www.reddit.com/r/MachineLearning/comments/dsgp7b/d_parallelization_for_neuroevolution_automl_models/,MrAcurite,1573048898,"I want to run multiple smaller models in parallel on the same GPU for the purposes of implementing something like CoDeepNEAT. However when, in testing, creating 100 small Torch CUDA models and getting the output of a 1000x8 tensor passed to each model with layer sizes 8-64-8, parallelizing with a pool of 8 workers takes ~15 seconds and uses ~6 GB of vRAM, and serially processing them takes ~0.03 seconds and uses ~100 MB of vRAM.

Is there some particular scheme that I should be using for this? Should I switch from Torch to Tensorflow? From Python to C++? Anyone have any ideas?",0,2
263,2019-11-6,2019,11,6,23,dsgsk9,"Alexa AI's head scientist on Alexa's 5th birthday: How the voice service got here, and where it's headed",https://www.reddit.com/r/MachineLearning/comments/dsgsk9/alexa_ais_head_scientist_on_alexas_5th_birthday/,georgecarlyle76,1573049328,,0,1
264,2019-11-6,2019,11,6,23,dsgytg,On the lecture slides of the ML course at my university.,https://www.reddit.com/r/MachineLearning/comments/dsgytg/on_the_lecture_slides_of_the_ml_course_at_my/,Maxinho96,1573050166,,0,1
265,2019-11-6,2019,11,6,23,dsh2uj,Machine Learning Translation and the Google Translate Algorithm,https://www.reddit.com/r/MachineLearning/comments/dsh2uj/machine_learning_translation_and_the_google/,andrea_manero,1573050695,[removed],0,1
266,2019-11-6,2019,11,6,23,dshbcj,Competition: Explaining black box machine learning models,https://www.reddit.com/r/MachineLearning/comments/dshbcj/competition_explaining_black_box_machine_learning/,andrea_manero,1573051784,[removed],0,1
267,2019-11-7,2019,11,7,0,dshszu,Numerical gradient descent vs gradient calculation and propagation,https://www.reddit.com/r/MachineLearning/comments/dshszu/numerical_gradient_descent_vs_gradient/,In_for_a_pound,1573053986,[removed],0,1
268,2019-11-7,2019,11,7,0,dsi6a2,"Simple Questions Thread November 06, 2019",https://www.reddit.com/r/MachineLearning/comments/dsi6a2/simple_questions_thread_november_06_2019/,AutoModerator,1573055555,[removed],0,1
269,2019-11-7,2019,11,7,1,dsic0a,[D] Random Forests and Decision Trees,https://www.reddit.com/r/MachineLearning/comments/dsic0a/d_random_forests_and_decision_trees/,spot4992,1573056231,"I am doing a binary classification problem where I currently run a decision tree across the data with 100 different random seeds, and then take the total number of outputs and figure out the final predicted classification. So if it comes out 1 75 times and 0 25 times, then the final prediction is a 1. I am using a pure majority problem (in the event of a tie, I go with 0). Would there be any benefit to running the exact same thing, but with 100 different random forests? In other words, will a decision tree and random forest predict the same wrong ones, but predict different correct ones? I am trying to find a way to push the accuracy a little higher. It works well, coming in with about 65% accuracy.

&amp;#x200B;

P.S. I do all the normal stuff like train-test split, limit the number of branches to the decision tree, etc.

&amp;#x200B;

P.P.S. I should note that the random seed changes for the train-test split and the decision tree when running the next tree.",16,1
270,2019-11-7,2019,11,7,1,dsiize,"[D] Given the recent news about plagiarism, will this be even more of a problem in the future?",https://www.reddit.com/r/MachineLearning/comments/dsiize/d_given_the_recent_news_about_plagiarism_will/,FirstTimeResearcher,1573057045,"A couple examples:

https://www.reddit.com/r/MachineLearning/comments/dq82x7/discussion_a_questionable_sigir_2019_paper/

https://www.reddit.com/r/MachineLearning/comments/dh2xfs/d_siraj_has_a_new_paper_the_neural_qubit_its/

Both papers were easy to catch because they directly copied word for word large sections of text. But with more aggressive word substitution and NLP applications getting better, this would get much harder to detect in the future.

Are we going to see plagiarism on the rise in the near future?",10,7
271,2019-11-7,2019,11,7,1,dsikbx,Looking for a Machine Learning group,https://www.reddit.com/r/MachineLearning/comments/dsikbx/looking_for_a_machine_learning_group/,imbeauleo,1573057202,"I'm already part of a small team that has an idea to use ML to give college football players in the Big 10 a rating (similar to madden ratings). We want to apply ML to be able to find these figures. 

We have a small 3 man team of programmers that are all at different levels. We are thinking of doing this in R or Python.",0,1
272,2019-11-7,2019,11,7,1,dsiqfx,"[R] [P] UNC BIAG Releases Mermaid, Pytorch based image registration toolkit",https://www.reddit.com/r/MachineLearning/comments/dsiqfx/r_p_unc_biag_releases_mermaid_pytorch_based_image/,SahinOlut,1573057920,"We are thrilled to release our image registration toolkit after a long time! 

You can quickly prototype and test your registration ideas with Mermaid, based on PyTorch. 

By using Mermaid, it is possible to train deep registration models easily. PRs, questions are welcome! 

&amp;#x200B;

Github repository: [https://github.com/uncbiag/mermaid](https://github.com/uncbiag/mermaid)

Documentation: [https://mermaid.readthedocs.io/en/latest/index.html](https://mermaid.readthedocs.io/en/latest/index.html)

Related papers:

&amp;#x200B;

**Region-specific Diffeomorphic Metric Mapping** [https://arxiv.org/pdf/1906.00139.pdf](https://arxiv.org/pdf/1906.00139.pdf) [https://github.com/uncbiag/easyreg](https://github.com/uncbiag/easyreg)

Zhengyang Shen, Franois-Xavier Vialard, Marc Niethammer. **NeurIPS** 2019.

&amp;#x200B;

**Networks for Joint Affine and Non-parametric Image Registration** [https://arxiv.org/pdf/1903.08811.pdf](https://arxiv.org/pdf/1903.08811.pdf) [https://github.com/uncbiag/easyreg](https://github.com/uncbiag/easyreg)

Zhengyang Shen, Xu Han, Zhenlin Xu, Marc Niethammer. **CVPR** 2019.

&amp;#x200B;

**Metric Learning for Image Registration** [https://arxiv.org/pdf/1904.09524.pdf](https://arxiv.org/pdf/1904.09524.pdf)

Marc Niethammer, Roland Kwitt, Francois-Xavier Vialard. **CVPR** 2019.

&amp;#x200B;

**Quicksilver: Fast predictive image registration--a deep learning approach** [https://arxiv.org/pdf/1703.10908.pdf](https://arxiv.org/pdf/1703.10908.pdf) [https://github.com/rkwitt/quicksilver](https://github.com/rkwitt/quicksilver)

Xiao Yang, Roland Kwitt, Martin Styner, Marc Niethammer, **NeuroImage** 2017.

&amp;#x200B;

**Fast Predictive Image Registration** [https://github.com/rkwitt/FastPredictiveImageRegistration](https://github.com/rkwitt/FastPredictiveImageRegistration)

Xiao Yang, Roland Kwitt, Marc Niethammer. **DLMIA** 2016.",5,18
273,2019-11-7,2019,11,7,1,dsixxz,"Chatbots or Human assistants, who will serve us in 2020?",https://www.reddit.com/r/MachineLearning/comments/dsixxz/chatbots_or_human_assistants_who_will_serve_us_in/,michaeljohn03,1573058792,[removed],0,3
274,2019-11-7,2019,11,7,1,dsiyae,What's the best resource for a a holistic TensorFlow 2.0 Training?,https://www.reddit.com/r/MachineLearning/comments/dsiyae/whats_the_best_resource_for_a_a_holistic/,Gr33kG33k,1573058830,[removed],0,1
275,2019-11-7,2019,11,7,1,dsj0g3,Algorithms Have Nearly Mastered Human Language. Why Cant They Stop Being Sexist?,https://www.reddit.com/r/MachineLearning/comments/dsj0g3/algorithms_have_nearly_mastered_human_language/,WarbleHead,1573059072,,0,1
276,2019-11-7,2019,11,7,1,dsj48y,[D] Is Reinforcement Learning Practical?,https://www.reddit.com/r/MachineLearning/comments/dsj48y/d_is_reinforcement_learning_practical/,edelweiss_ml,1573059491,"Is reinforcement learning practical at this point for industry work? The most prominent examples we see are from DeepMind (AlphaStar, AlphaGo), but the team are world-class researchers (over 40 of them) who also worked closely with expert Starcraft 2 players with a ton of computing resources.

As someone who hasn't had much experience in RL, I see potential applications but am unsure of the amount of work or practicality of it. For example, one potential application for RL is to learn fraudulent behavior in an online retailer system (i.e. Amazon, EBay) and proactively find methods of fraud before they happen. One could imagine all the unintended behavior of misspecified reward function being useful for finding exploits in a system ( [https://openai.com/blog/faulty-reward-functions/](https://openai.com/blog/faulty-reward-functions/)). But there are a lot of issues to overcome, (some mentioned in this article [https://www.alexirpan.com/2018/02/14/rl-hard.html](https://www.alexirpan.com/2018/02/14/rl-hard.html)) about sample inefficiency, not to mention having to build your own simulator (and hope it's representative to some degree).

What are people's opinion on the practicality of using RL in something like fraud? Does it even make sense to build a simple online retailer simulator? I ask because it while I think RL is quite powerful, it feels it isn't quite ready to be used. I would love to be shown to be wrong.",66,99
277,2019-11-7,2019,11,7,1,dsj4b1,[R] The Measure of Intelligence,https://www.reddit.com/r/MachineLearning/comments/dsj4b1/r_the_measure_of_intelligence/,Reiinakano,1573059498,,31,21
278,2019-11-7,2019,11,7,1,dsj50f,What would be the simple way to build an automated ML model monitoring tool with RStudio and SQL Server,https://www.reddit.com/r/MachineLearning/comments/dsj50f/what_would_be_the_simple_way_to_build_an/,vamsi115,1573059583,[removed],0,1
279,2019-11-7,2019,11,7,2,dsjjuf,Querying about GAN training,https://www.reddit.com/r/MachineLearning/comments/dsjjuf/querying_about_gan_training/,ahmedmokhtar97,1573061271,[removed],0,1
280,2019-11-7,2019,11,7,2,dsjweq,"[P] - NLP and Optimisation project feedback, is this a good idea?",https://www.reddit.com/r/MachineLearning/comments/dsjweq/p_nlp_and_optimisation_project_feedback_is_this_a/,DaveatAuquan,1573062747," Hey Guys, Looking for some feedback - Delete if not allowed.

We're a data science company that runs competitions (like kaggle for finance). We're about to run our first competition that requires ML approaches to do well.  The competition is for students in the UK where the problems are an Optimisation (e.g. using reinforcement learning) and an NLP (e.g. by fine-tuning pre-trained neural networks)  

The problems are sourced from top firms and are real problems that their data scientists and quants are working on. There are three problems covering different aspects of data science with a focus on the finance space.

Cash Prizes of up to $5000  
AWS Credits up to $1000  
10x Genuine Impact Investment Research Subscriptions

Does this sound like an interesting project or challenge? Would you as a student be interested?",0,0
281,2019-11-7,2019,11,7,3,dsk69k,"[P] DepthAI hardware: RGBd, Myriad X VPU, Object-Tracking, Neural Network Accelerators for Raspberry Pi",https://www.reddit.com/r/MachineLearning/comments/dsk69k/p_depthai_hardware_rgbd_myriad_x_vpu/,Luxonis-Brian,1573063888,"We wanted to share with you all about some embedded and low-cost hardware we've been working on that combines disparity depth and AI via Intel's Myriad X VPU. We've developed a SoM that's not much bigger than a US quarter which takes direct image inputs from 3 cameras (2x OV9282, 1x IMX378), processes it, and spits the result back to the host via USB3.1.

We wanted disparity + AI so we could get object localization outputs - an understanding of where and what objects are in our field of view, and we wanted this done fast, with as little latency as possible. Oh, and at the edge. And for low power. Our ultimate goal is actually to develop a rear-facing AI vision system that will alert cyclists of potential danger from distracted drivers. An ADAS for bikes!

There are some Myriad X solutions on the market already, but most use PCIe, so the data pipeline isn't as direct as Sensor--&gt;Myriad--&gt;Host, and the existing solutions also don't offer a three camera solution for RGBd. So, we built it!

Hope the shameless plug is OK here (sorry mods!), and if anyone has any questions or comments, we'd love to hear it!

[cnx-software article](https://www.cnx-software.com/2019/11/06/depthai-brings-ai-plus-depth-to-the-raspberry-pi/)

[hackster.io article](https://www.hackster.io/news/luxonis-launches-depthai-depth-sensing-object-tracking-neural-network-accelerators-on-crowd-supply-de4e7e826f1f)

[crowdsupply](https://www.crowdsupply.com/luxonis/depthai)

[hackaday](https://hackaday.io/project/163679-luxonis-depthai)
https://hackaday.io/project/163679-luxonis-depthai",5,13
282,2019-11-7,2019,11,7,3,dskb1k,Torch Up: Brood War AI Tournament RFC 2,https://www.reddit.com/r/MachineLearning/comments/dskb1k/torch_up_brood_war_ai_tournament_rfc_2/,jchassoul,1573064465,,0,1
283,2019-11-7,2019,11,7,3,dskem5,Have you forget which parking floor you have parked your car???,https://www.reddit.com/r/MachineLearning/comments/dskem5/have_you_forget_which_parking_floor_you_have/,kdluvani,1573064871,,0,1
284,2019-11-7,2019,11,7,3,dskhnl,"[Resume Review] Started getting into the field a year ago, would appreciate your thoughts on my CV",https://www.reddit.com/r/MachineLearning/comments/dskhnl/resume_review_started_getting_into_the_field_a/,starzmustdie,1573065253,,0,1
285,2019-11-7,2019,11,7,3,dskr8u,[P] Filtering data in a Pyspark Pipeline without losing all the data?,https://www.reddit.com/r/MachineLearning/comments/dskr8u/p_filtering_data_in_a_pyspark_pipeline_without/,Octosaurus,1573066392,"I have a project where I'm feeding a dataframe into a PipelineModel with two pretrained models inside. The flow goes something like this:

Input DF -&gt; Preprocessing Transformers -&gt; Model1 -&gt; Model2 -&gt; Output DF

The thing is, Model1 and Model2 predict on different values (e.g. Male vs Female). I tried using the SQLTransformer to filter the data on each type, but I drop everything, so the output of Model1 throws away all the data I need to predict in Model2. 

Is there a way to filter data to be fed into Model1, then filter data to be fed into Model2, and then concatenate the dataframes to be returned?

&amp;#x200B;

Please let me know if I can clarify anything!",4,1
286,2019-11-7,2019,11,7,4,dsl20a,Aren't we running too fast?,https://www.reddit.com/r/MachineLearning/comments/dsl20a/arent_we_running_too_fast/,whatever_you_absorb,1573067650,,0,1
287,2019-11-7,2019,11,7,4,dsl27r,I want to create a ai bot for any tennis game.,https://www.reddit.com/r/MachineLearning/comments/dsl27r/i_want_to_create_a_ai_bot_for_any_tennis_game/,eternal_dev1l,1573067678,[removed],0,1
288,2019-11-7,2019,11,7,4,dslaud,"[D] List of DL topics with resources for a quick brief, especially before interviews",https://www.reddit.com/r/MachineLearning/comments/dslaud/d_list_of_dl_topics_with_resources_for_a_quick/,dakshit97,1573068859,"Vision and Language Group, a deep learning group at IIT Roorkee, has made a list of topics of DL with resources which one should be familiar with, and that could come in handy before interviews for briefing up.

[https://github.com/vlgiitr/DL\_Topics](https://github.com/vlgiitr/DL_Topics)

Feel free to contribute any amazing resources that have been useful for a quick prep before your interviews, and star the repo if it is helpful to you!",15,232
289,2019-11-7,2019,11,7,4,dsle11,Is ASU a good school for a PhD in computer science?(And other questions),https://www.reddit.com/r/MachineLearning/comments/dsle11/is_asu_a_good_school_for_a_phd_in_computer/,WasNotMeantToLive,1573069288,[removed],0,1
290,2019-11-7,2019,11,7,4,dsll84,[D] Using UMAP for clustering,https://www.reddit.com/r/MachineLearning/comments/dsll84/d_using_umap_for_clustering/,Andrejkarp,1573070282,"UMAP (Uniform Manifold Approximation and Projection) is a brand new dimension reduction technique, however it has been used already in the paper [https://arxiv.org/abs/1908.05968](https://arxiv.org/abs/1908.05968) as a part of clustering pipeline. It gives a really nice results, however I'm not quite convinced of its correctnes. My concerns are similar to those regarding TSNE for clustering (nice stackoverflow discussion here: [https://stats.stackexchange.com/questions/263539/clustering-on-the-output-of-t-sne](https://stats.stackexchange.com/questions/263539/clustering-on-the-output-of-t-sne)). The UMAP lib for python also touches this issue: [https://umap-learn.readthedocs.io/en/latest/clustering.html](https://umap-learn.readthedocs.io/en/latest/clustering.html).  


What are your thoughts?",9,10
291,2019-11-7,2019,11,7,5,dslxhr,MLPerf Releases Inference Results,https://www.reddit.com/r/MachineLearning/comments/dslxhr/mlperf_releases_inference_results/,gdiamos,1573071711,[removed],0,1
292,2019-11-7,2019,11,7,5,dslyng,"[P] 'Spectral Clustering in Heterogeneous Information Networks' (AAAI, 2019) implemented in Python",https://www.reddit.com/r/MachineLearning/comments/dslyng/p_spectral_clustering_in_heterogeneous/,wellfriedbeans,1573071847,,2,1
293,2019-11-7,2019,11,7,5,dsm6g7,[D] Andrew Ng's thoughts on 'robustness' - looking for relevant resources,https://www.reddit.com/r/MachineLearning/comments/dsm6g7/d_andrew_ngs_thoughts_on_robustness_looking_for/,deep-yearning,1573072772,"For those of you unfamiliar, Andrew Ng runs a weekly newsletter where he shares thoughts and new developments in deep learning. It's called 'The Batch'. I was very interested in something he said in today's newsletter (which can be [read here](https://info.deeplearning.ai/the-batch-deepmind-masters-starcraft-2-ai-attacks-on-amazon-a-career-in-robot-management-banks-embrace-bots)), in which he talks about how deep learning systems still fail in many real scenarios because they are not yet robust to changes in data quality/distributions 

&gt; *One of the challenges of robustness is that it is hard to study systematically. How do we benchmark how well an algorithm trained on one distribution performs on a different distribution? Performance on brand-new data seems to involve a huge component of luck. Thats why the amount of academic work on robustness is significantly smaller than its practical importance. Better benchmarks will help drive academic research.* 

I am looking for more resources that study this type of robustness systematically. Is anyone aware of any key works on this topic? For example looking at how real datasets and corresponding performance vary from train/test datasets a model is developed on?

Thanks!",6,2
294,2019-11-7,2019,11,7,6,dsmkot,New keras 3dgan implementation,https://www.reddit.com/r/MachineLearning/comments/dsmkot/new_keras_3dgan_implementation/,kanxx030,1573074418,[removed],0,1
295,2019-11-7,2019,11,7,6,dsncqr,On the fence for choosing between two models.,https://www.reddit.com/r/MachineLearning/comments/dsncqr/on_the_fence_for_choosing_between_two_models/,sixquills,1573077504,[removed],0,1
296,2019-11-7,2019,11,7,7,dsng25,[D][P] The output of a GAN trained on the MNIST dataset over time,https://www.reddit.com/r/MachineLearning/comments/dsng25/dp_the_output_of_a_gan_trained_on_the_mnist/,tjf314,1573077865,,0,1
297,2019-11-7,2019,11,7,7,dsnj5m,GAN-based Synthetic Medical Image Augmentation for increased CNN Performance in Liver Lesion Classification,https://www.reddit.com/r/MachineLearning/comments/dsnj5m/ganbased_synthetic_medical_image_augmentation_for/,mjangle1985,1573078202,,0,1
298,2019-11-7,2019,11,7,7,dsnj69,Reservoir computing application/echo state networks,https://www.reddit.com/r/MachineLearning/comments/dsnj69/reservoir_computing_applicationecho_state_networks/,curioussouya,1573078204,[removed],0,1
299,2019-11-7,2019,11,7,7,dsntlp,ProtoPNet Recognizes Birds and Shows Us How in Real Time,https://www.reddit.com/r/MachineLearning/comments/dsntlp/protopnet_recognizes_birds_and_shows_us_how_in/,Yuqing7,1573079404,,0,1
300,2019-11-7,2019,11,7,7,dsnz7m,[R]GAN-based Synthetic Medical Image Augmentation for increased CNN Performance in Liver Lesion Classification,https://www.reddit.com/r/MachineLearning/comments/dsnz7m/rganbased_synthetic_medical_image_augmentation/,mjangle1985,1573080044,,1,3
301,2019-11-7,2019,11,7,7,dso1o6,"What machine learning researchers are you following on twitter, insta, etc.? Who would you recommend?",https://www.reddit.com/r/MachineLearning/comments/dso1o6/what_machine_learning_researchers_are_you/,liashchynskyi,1573080337,[removed],0,1
302,2019-11-7,2019,11,7,7,dso6cr,"[D] Debate on Instrumental Convergence between LeCun, Russell, Bengio, Zador, and More",https://www.reddit.com/r/MachineLearning/comments/dso6cr/d_debate_on_instrumental_convergence_between/,mircare,1573080914,[removed],0,1
303,2019-11-7,2019,11,7,8,dsof31,"[P] New $10,000 ML Challenge: Mapping Disaster Risk from Aerial Imagery",https://www.reddit.com/r/MachineLearning/comments/dsof31/p_new_10000_ml_challenge_mapping_disaster_risk/,dat-um,1573081990,"[https://www.drivendata.org/competitions/58/disaster-response-roof-type/](https://www.drivendata.org/competitions/58/disaster-response-roof-type/)

Excited to launch a new machine learning competition! The goal is to be able to a better job creating disaster response plans based on detailed maps of communities. In order to do this, we need to understand the risk to structures, which we can do by understanding what kind of roof a building has. 

Come use your machine learning skills for a good cause! Plus it's got interesting geo data, novel imagery, and the opportunity to develop new methods.",8,56
304,2019-11-7,2019,11,7,8,dsofqg,Where to start to learn in spare time.,https://www.reddit.com/r/MachineLearning/comments/dsofqg/where_to_start_to_learn_in_spare_time/,Krowtic,1573082067,[removed],0,1
305,2019-11-7,2019,11,7,8,dsolxx,[D] What are good heuristics when choosing classes for image classification?,https://www.reddit.com/r/MachineLearning/comments/dsolxx/d_what_are_good_heuristics_when_choosing_classes/,Kerlin_Michel,1573082863,"For example let's say I want to classify eggs. And eggs in images can often be seen as just an egg, eggs in an egg cartoon and the carton can be closed or opened.

A naive approach would be to put all these image under the class eggs. But it might work better if there are 2 classes one for eggs and one for eggs in carton so training should be easier since these can look quite different since a group of eggs looks much different from a closed carton of eggs. I also feel like separating  the classes can have unwanted outcomes like separating contexts. For example eggs withing cartons could rely on context of being in a kitchen and grocery store so it may less accurately predict an image has eggs if it is a carton of eggs in a farm.

Is my thinking correct on this?

What has been your experience with similar situations? 

This question specifically focuses on image classification using neural nets.",4,6
306,2019-11-7,2019,11,7,8,dsoz4l,Eric Schmidt and Robert O. Work on Machine Learning for National Security: the China challenge,https://www.reddit.com/r/MachineLearning/comments/dsoz4l/eric_schmidt_and_robert_o_work_on_machine/,craigspencersmith,1573084537,[removed],0,1
307,2019-11-7,2019,11,7,9,dspajk,arXiv Search: Generating Tags from Paper Titles with NLP,https://www.reddit.com/r/MachineLearning/comments/dspajk/arxiv_search_generating_tags_from_paper_titles/,c0rpus_call0sum,1573086027,[removed],0,1
308,2019-11-7,2019,11,7,10,dspwmg,Given a dataset with 60 rows and each row having 10 values. The values are device readings ranging from 0.111111-0.9999999. The classification of each device has been done as 1 or -1. How can we rank the devices using ml ?,https://www.reddit.com/r/MachineLearning/comments/dspwmg/given_a_dataset_with_60_rows_and_each_row_having/,Tragaknight,1573088903,[removed],0,1
309,2019-11-7,2019,11,7,10,dsqfoh,Announcing the release of StellarGraph version 0.8.0 open-source Python Machine Learning Library for graphs,https://www.reddit.com/r/MachineLearning/comments/dsqfoh/announcing_the_release_of_stellargraph_version/,StellarGraphLibrary,1573091419,[removed],0,1
310,2019-11-7,2019,11,7,11,dsqohb,Announcing the release of StellarGraph version 0.8.0 open-source Python Machine Learning Library for graphs,https://www.reddit.com/r/MachineLearning/comments/dsqohb/announcing_the_release_of_stellargraph_version/,StellarGraphLibrary,1573092570,[removed],0,1
311,2019-11-7,2019,11,7,11,dsqy7r,[R] Announcing the release of StellarGraph version 0.8.1 open-source Python Machine Learning Library for graphs,https://www.reddit.com/r/MachineLearning/comments/dsqy7r/r_announcing_the_release_of_stellargraph_version/,StellarGraphLibrary,1573093862,"StellarGraph is an open-source library implementing a variety of state-of-the-art graph machine learning algorithms. The project is delivered as part of CSIROs Data61.

We are happy to announce the 0.8.1 release of the library, which extends StellarGraph capability by adding new algorithms and demos, enhancing interpretability via saliency maps for Graph Attention (GAT), and further simplifying graph machine learning workflows through standardised model APIs and arguments. 

This release, weve dealt with some bugs from the previous release and  introduced new features and enhancements. Some of these include:

* New directed GraphSAGE algorithm (a generalisation of GraphSAGE to directed graphs)
* New Attri2vec algorithm  
* New PPNP and APPNP algorithms 
* New Graph Attention (GAT) saliency maps for interpreting node classification with Graph Attention Networks
* Added directed SampledBFS walks on directed graphs
* Unified API of GCN, GAT, GraphSAGE, and HinSAGE classes by adding build() method to GCN and GAT classes
* Enhanced unsupervised GraphSage speed up via multithreading
* Support of sparse generators in the GCN saliency map implementation.
* Unified activations and regularisation for GraphSAGE, HinSAGE, GCN and GAT
* Changed from using keras to tensorflow.keras

Weve also added [new demos](https://github.com/stellargraph/stellargraph/tree/master/demos/) using real-world datasets to show how StellarGraph can solve these tasks.

Access the StellarGraph project and explore the new features on [GitHub](https://github.com/stellargraph/stellargraph). StellarGraph is a Python 3 library.

We welcome your feedback and contributions.

With thanks, the StellarGraph team.",7,118
312,2019-11-7,2019,11,7,11,dsr89c,"[N] Python Creator Guido van Rossum Quits Dropbox, And Announced His Retirement",https://www.reddit.com/r/MachineLearning/comments/dsr89c/n_python_creator_guido_van_rossum_quits_dropbox/,navin49,1573095203,"&amp;#x200B;

https://preview.redd.it/oixo7lmzh6x31.png?width=793&amp;format=png&amp;auto=webp&amp;s=148db18f71aadd6e77d6d01a1d24de28462c1c3b

The creator of one of the worlds most popular programming language Python, Mr. Guido van Rossum is all set to start the second half of his life, Rossum has announced his retirement this week. Guido is stepping down from his current role at cloud file storage firm Dropbox and heading into retirement.

Guido joins Dropbox in the month of [December 2012](https://blogs.dropbox.com/tech/2012/12/welcome-guido/), at Dropbox he spent his last six and half years. The hiring of Guido makes sense because so much of Dropbox functionality was built on Python.

Dropbox has about four million lines of Python code and its the most heavily used language for its back-end services, desktop app and in other major operations. When Guido van Rossum started in 2012, Dropboxs server and desktop client were written: almost exclusively in Python.

[**Continue Reading**](https://techgrabyte.com/python-guido-van-rossum-announced-retirement/)",3,9
313,2019-11-7,2019,11,7,12,dss1fu,Incorporating Artificial Intelligence (AI) into Sales &amp; Customer Retention Strategies,https://www.reddit.com/r/MachineLearning/comments/dss1fu/incorporating_artificial_intelligence_ai_into/,Countants123,1573099197,,0,1
314,2019-11-7,2019,11,7,14,dssyzl,[D] CNN + DQNs?,https://www.reddit.com/r/MachineLearning/comments/dssyzl/d_cnn_dqns/,YuhFRthoYORKonhisass,1573103855,[removed],0,1
315,2019-11-7,2019,11,7,15,dsthwm,How to become a master in machine learning,https://www.reddit.com/r/MachineLearning/comments/dsthwm/how_to_become_a_master_in_machine_learning/,Sanchit112,1573106735,[removed],0,1
316,2019-11-7,2019,11,7,16,dsubio,[D] Predicting figure skating world championship ranking from season performances (part 2: hybrid models learned by gradient descent),https://www.reddit.com/r/MachineLearning/comments/dsubio/d_predicting_figure_skating_world_championship/,seismatica,1573111470,"I previous [posted](https://www.reddit.com/r/MachineLearning/comments/dn8lq9/p_predict_figure_skating_world_championship/) the write-up on the first part of my project (Github [repo](https://github.com/dknguyengit/skate_predict)) to predict how skaters would rank in the figure skating world championship from earlier scores that they earned in the season. The main idea is to separate the skater effect, the intrinsic ability of each skater, from the event effect, the influence of an event on a skaters performance, so that a more accurate ranking could be built.

In that previous part, I considered two simple models to find the latent skater scores that are used to predict the skater ranking:

1. Score of a skater at an event = baseline + latent skater score + latent event score 

2. Score of a skater at an event = baseline  latent skater score  latent event score

In this part of the project ([analysis](https://github.com/dknguyengit/skate_predict/blob/master/analysis_part2.ipynb), [write-up](https://medium.com/@seismatica/predicting-figure-skating-world-championship-ranking-from-season-performances-part-2-hybrid-7d296747b15?source=friends_link&amp;sk=86881d127654ece260be2e3029dfbad2)), I consider a hybrid model of those earlier models:

**Score of a skater at an event = baseline + latent skater score  latent event score**

Unfortunately, this model does not have a closed-form solution to learn the parameters as opposed to the earlier models. Therefore, gradient descent was used to learn them, which resulted in this neat little [animation](https://i.imgur.com/IK6AZ6W.mp4) that tracks how the model residuals, RMSE, as well as predicted ranking gets better and better as gradient descent runs. I also explore different strategies to reduce model overfit (so that it can predict skater ranking more accurately), using familiar methods such as model penalization and early stopping. 

Lastly, note that this hybrid model is nothing but factorizing the event-skater score matrix into an event-specific vector and skater-specific vector, which can multiply together to approximate the score matrix. Therefore, the gradient descent to learn the values of these latent vectors is very similar to that of the famous [FunkSVD](https://sifter.org/~simon/journal/20061211.html) algorithm to learn the user-specific and item-specific latent factors, which can multiply together to approximate the rating matrix of a recommendation system (in this case user=skater, and item=event). However, FunkSVD was used with multiple factors, and in the next part of my project, I will show how multi-factor matrix factorization can be applied to this ranking problem.",0,1
317,2019-11-7,2019,11,7,16,dsuepv,[P] Predict figure skating world championship ranking from season performances (part 2: hybrid models learned by gradient descent),https://www.reddit.com/r/MachineLearning/comments/dsuepv/p_predict_figure_skating_world_championship/,seismatica,1573112088,"I previous [posted](https://www.reddit.com/r/MachineLearning/comments/dn8lq9/p_predict_figure_skating_world_championship/) the write-up on the first part of my project (Github [repo](https://github.com/dknguyengit/skate_predict)) to predict how skaters would rank in the figure skating world championship from earlier scores that they earned in the season. The main idea is to separate the **skater effect**, the intrinsic ability of each skater, from the **event effect**, the influence of an event on a skaters performance, so that a more accurate ranking could be built.

In that previous part, I considered two simple models to find the latent skater scores that are used to predict the skater ranking:

1. Score of a skater at an event = baseline score + latent skater score + latent event score 

2. Score of a skater at an event = baseline score  latent skater score  latent event score

In this part of the project ([analysis](https://github.com/dknguyengit/skate_predict/blob/master/analysis_part2.ipynb), [write-up](https://medium.com/@seismatica/predicting-figure-skating-world-championship-ranking-from-season-performances-part-2-hybrid-7d296747b15?source=friends_link&amp;sk=86881d127654ece260be2e3029dfbad2)), I consider a hybrid model of those earlier models:

**Score of a skater at an event = baseline score + latent skater score  latent event score**

Unfortunately, this model does not have a closed-form solution to learn the parameters as opposed to the earlier models. Therefore, gradient descent was used to learn them, which resulted in this neat little [animation](https://i.imgur.com/IK6AZ6W.mp4) that tracks how the model residuals, RMSE, as well as predicted ranking gets better and better as gradient descent runs. I also explore different strategies to reduce model overfit (so that it can predict skater ranking more accurately), using familiar methods such as model penalization and early stopping. 

Lastly, note that this hybrid model is nothing but factorizing the event-skater score matrix into an event-specific vector and skater-specific vector, which can multiply together to approximate the score matrix. Therefore, the gradient descent to learn the values of these latent vectors is very similar to that of the famous [FunkSVD](https://sifter.org/~simon/journal/20061211.html) algorithm to learn the user-specific and item-specific latent factors, which can multiply together to approximate the rating matrix of a recommendation system (in this case user=skater, and item=event). However, FunkSVD was used with multiple factors, and in the next part of my project, I will show how multi-factor matrix factorization can be applied to this ranking problem.

If you have any question or feedback on this, just let me know :)",0,1
318,2019-11-7,2019,11,7,16,dsuequ,Buy Top Quality Shoe Upper Knitting Machine for Commercial Use,https://www.reddit.com/r/MachineLearning/comments/dsuequ/buy_top_quality_shoe_upper_knitting_machine_for/,janeho0,1573112095,,0,1
319,2019-11-7,2019,11,7,16,dsuifu,Best Collar Knitting Machine Manufacturers,https://www.reddit.com/r/MachineLearning/comments/dsuifu/best_collar_knitting_machine_manufacturers/,janeho0,1573112714,,0,1
320,2019-11-7,2019,11,7,17,dsv3a0,[Research] Which is the most prestigious ML journal?,https://www.reddit.com/r/MachineLearning/comments/dsv3a0/research_which_is_the_most_prestigious_ml_journal/,Agent_ANAKIN,1573116679,[removed],0,1
321,2019-11-7,2019,11,7,18,dsvhsh,Deploy Machine Learning Models with Django,https://www.reddit.com/r/MachineLearning/comments/dsvhsh/deploy_machine_learning_models_with_django/,pp314159,1573119594,,1,1
322,2019-11-7,2019,11,7,18,dsvj2e,[D] The DeepMind quizz has changed: any feedback on the new interview?,https://www.reddit.com/r/MachineLearning/comments/dsvj2e/d_the_deepmind_quizz_has_changed_any_feedback_on/,DependentSky6,1573119834,"Hello, 

A while ago I made a post about the DeepMind quizz where people could get relevant information. [https://www.reddit.com/r/MachineLearning/comments/bf1xh2/d\_any\_tips\_and\_tricks\_to\_crack\_the\_deepmind\_quiz/](https://www.reddit.com/r/MachineLearning/comments/bf1xh2/d_any_tips_and_tricks_to_crack_the_deepmind_quiz/) 

Now it seems that the quizz has changed, and there is a code comprehension part. 

If someone could give feedback on this new test, it would be great to help others ! 

Thanks!",5,1
323,2019-11-7,2019,11,7,18,dsvk20,[P] Deploy Machine Learning Models with Django,https://www.reddit.com/r/MachineLearning/comments/dsvk20/p_deploy_machine_learning_models_with_django/,pp314159,1573120046,"I've created tutorial that shows how to create web service in Python and Django to serve multiple Machine Learning models. It is different (more advanced) from most of the tutorials available on the internet:

- it keeps information about many ML models in the web service. There can be several ML models available at the same endpoint with different versions. What is more, there can be many endpoint addresses defined.

- it stores information about requests sent to the ML models, this can be used later for model testing and audit.

- it has tests included for ML code and server code.

- it can run A/B tests between different versions of ML models. 

The tutorial is available at https://www.deploymachinelearning.com

The source code from the tutorial is available at https://github.com/pplonski/my_ml_service",36,1
324,2019-11-7,2019,11,7,18,dsvmge,[P] Understanding CNNs with Kannada-MNIST,https://www.reddit.com/r/MachineLearning/comments/dsvmge/p_understanding_cnns_with_kannadamnist/,kenanajkunic,1573120540,"This is my first AI project and I just wanted to share it. Hopefully it's somewhat helpful :)

\[GitHub\]([https://github.com/kenanajkunic/kmnist-cnn](https://github.com/kenanajkunic/kmnist-cnn))

\[Kaggle\]([https://www.kaggle.com/kenanajk/understanding-cnns-with-kannada-mnist](https://www.kaggle.com/kenanajk/understanding-cnns-with-kannada-mnist))",2,1
325,2019-11-7,2019,11,7,19,dsvo5k,[D] GPT-2 1.5B (the largest model) was released and it is amazing!,https://www.reddit.com/r/MachineLearning/comments/dsvo5k/d_gpt2_15b_the_largest_model_was_released_and_it/,csxeba,1573120890,You can test it here:  [https://talktotransformer.com/](https://talktotransformer.com/),50,1
326,2019-11-7,2019,11,7,19,dsvq2u,[D] Deep learning- agent for stock investment.,https://www.reddit.com/r/MachineLearning/comments/dsvq2u/d_deep_learning_agent_for_stock_investment/,tmaloo,1573121243,"For last few months i am trying to create an agent which will take an input amount and a stock to invest on along with the time for investments.

It comprised of an deep learning algorithm which will predict, sentiment analysis of news related to that company and related companies, scraping of data from google trends, stock data for that company and related company for training. But i am struggling to create an agent myself which could use these data for dummy investments.",3,1
327,2019-11-7,2019,11,7,19,dsvs29,Project [P] Apply Machine Learning Algorithms using Python on a Loan Delinquency Problem,https://www.reddit.com/r/MachineLearning/comments/dsvs29/project_p_apply_machine_learning_algorithms_using/,Shilpa_Opencodez,1573121631,,0,1
328,2019-11-7,2019,11,7,19,dsvt8n,Speech-to-text and ASR tutorials,https://www.reddit.com/r/MachineLearning/comments/dsvt8n/speechtotext_and_asr_tutorials/,PlatoTheSloth,1573121869,[removed],0,1
329,2019-11-7,2019,11,7,19,dsvttk,[R] Video Analysis of AlphaStar: Grandmaster level in StarCraft II using multi-agent reinforcement learning,https://www.reddit.com/r/MachineLearning/comments/dsvttk/r_video_analysis_of_alphastar_grandmaster_level/,ykilcher,1573121994,[removed],0,1
330,2019-11-7,2019,11,7,20,dsw7wx,The New Face of Digital Disruption - Technology Landscape 2020,https://www.reddit.com/r/MachineLearning/comments/dsw7wx/the_new_face_of_digital_disruption_technology/,Albertchristopher,1573124741,,0,1
331,2019-11-7,2019,11,7,20,dswbpd,What is crowding problem in T-SNE?,https://www.reddit.com/r/MachineLearning/comments/dswbpd/what_is_crowding_problem_in_tsne/,venkarafa,1573125452,What is the intuition behind crowding problem in dimension reduction algorithms like T-SNE ?,0,1
332,2019-11-7,2019,11,7,20,dswdj7,What are the challenges faced by data scientists and ML engineers while using the proprietary tools?,https://www.reddit.com/r/MachineLearning/comments/dswdj7/what_are_the_challenges_faced_by_data_scientists/,psmlbhor,1573125780,[removed],0,1
333,2019-11-7,2019,11,7,22,dsxrpx,BS in Computer Science | BS in Data Science,https://www.reddit.com/r/MachineLearning/comments/dsxrpx/bs_in_computer_science_bs_in_data_science/,GoodNews970,1573133577,[removed],0,1
334,2019-11-7,2019,11,7,23,dsye2e,why softmax+CE over sigmoid+BCE?,https://www.reddit.com/r/MachineLearning/comments/dsye2e/why_softmaxce_over_sigmoidbce/,DeMorrr,1573136554,"Most of the popular neural network language models use softmax+cross entropy loss during training, which is based on the assumption that only the target label is true, and everything else is false. But isn't language modeling a multilabel classification task? why sigmoid+BCE isn't used often?",0,1
335,2019-11-8,2019,11,8,0,dsz2yc,[N] A Challenge to Automatically Assess Damaged Buildings After a Disaster - NASA Science Mission Directorate,https://www.reddit.com/r/MachineLearning/comments/dsz2yc/n_a_challenge_to_automatically_assess_damaged/,nirav_diu,1573139630,,0,1
336,2019-11-8,2019,11,8,0,dszfqs,"Let Machines do it for you, Subnetting Networks is Easiest Now!",https://www.reddit.com/r/MachineLearning/comments/dszfqs/let_machines_do_it_for_you_subnetting_networks_is/,staykaroid,1573141177,,0,1
337,2019-11-8,2019,11,8,0,dszoe6,Generative Adversarial Network's discriminative distribution explained!,https://www.reddit.com/r/MachineLearning/comments/dszoe6/generative_adversarial_networks_discriminative/,kpagels,1573142235,[removed],0,1
338,2019-11-8,2019,11,8,1,dszpxw,[Discussion] Is MINE (Mutual Information Neural Estimation) suitable for reducing the mutual information?,https://www.reddit.com/r/MachineLearning/comments/dszpxw/discussion_is_mine_mutual_information_neural/,pky3436,1573142448,"Hello, i got a old-fashioned but confused question about Belghazi et al., Mutual Information Neural Estimation, ICML 2018.

In the paper, the lower bound of mutual information is achieved by neural-net-parameterized function (what they call 'statistics network'), and various experiments are conducted including information bottleneck which is case of 'reducing' I(X; Z).

Here i'm quite interested with reducing mutual information, so i started to regenerate their results, but it's quite stucked. 

**Unfortunately not much details about IB implementation are included in paper, so if you have any experience employing MINE to reduce mutual information, it'd be a big pleasure, please share your way.**

The paper is well-written with clear theoretical background, **but i'm not sure how lowering the 'approximated lower bound' is helpful to reduce the actual mutual information**. For those kinds of lower-bound mutual information models; Do you think those models are also practically useful to reduction of MI?",0,0
339,2019-11-8,2019,11,8,1,dszurq,[Discussion] Is MINE(Mutual Information Neural Estimation) is also helpful for reducing MI?,https://www.reddit.com/r/MachineLearning/comments/dszurq/discussion_is_minemutual_information_neural/,pky3436,1573143094,"Hello, i got a old-fashioned but confused question about Mutual Information Neural Estimation(MINE), 2018 ICML.

In the paper, the lower bound of mutual information is estimated with neural-net-parameterized function (what is called as statistics network), and various experiments were held including information bottleneck, which reduces I(X; Z).

It's very well-written with theoretical background, **but i'm stucked with reimplement the IB results**; Unfortunately the paper doesn't provides full details about IB section; So if you have any kind of experience with employing MINE to reducing mutual information, it'd be a big pleasure if you share the experience.  I made a statistics network following the paper, and optimize the statistics network while employ its estimated MI lower bound to the I(X; Z) regularizer.

Also, i'm not fully convinced how such MI lower-bound estimating models are greatful to reducing MI problems; Is reducing the 'approximated' lower bound of MI guarantee the practical reduction of MI? I think optimizing the MI estimator while also reducing such estimated MI lower bound might be not stable; as GAN, it may be kind of minmax training. How do you think about it?",1,1
340,2019-11-8,2019,11,8,1,dszzkk,[Discussion] Is MINE(Mutual Information Neural Estimation) also helpful for reducing Mutual Information problem?,https://www.reddit.com/r/MachineLearning/comments/dszzkk/discussion_is_minemutual_information_neural/,pky3436,1573143699,"Hello, i got a old-fashioned but confused question about Mutual Information Neural Estimation(MINE), 2018 ICML.

In the paper, the lower bound of mutual information is estimated with neural-net-parameterized function (what is called as statistics network), and various experiments were held including information bottleneck, which reduces I(X; Z).

It's very well-written with theoretical background, **but i'm stucked with reimplement the IB results**; Unfortunately the paper doesn't provides full details about IB section; So if you have any kind of experience with employing MINE to reducing mutual information, it'd be a big pleasure if you share the experience.  **I made a statistics network following the paper, and optimize the statistics network while employ its estimated MI lower bound to the I(X; Z) regularizer. But it seems very volatile to initial value of exponential\_moving\_average(exp(t)).**

Also, i'm not fully convinced how such MI lower-bound estimating models are greatful to reducing MI problems; Is reducing the 'approximated' lower bound of MI guarantee the practical reduction of MI? I think optimizing the MI estimator while also reducing such estimated MI lower bound might be not stable; as GAN, it may be kind of minmax training. How do you think about it?",13,1
341,2019-11-8,2019,11,8,1,dt00ze,How to understand Generative Adversarial Network Discriminative distribution?,https://www.reddit.com/r/MachineLearning/comments/dt00ze/how_to_understand_generative_adversarial_network/,kpagels,1573143850,[removed],0,1
342,2019-11-8,2019,11,8,2,dt0u7p,Where to start learning about ML,https://www.reddit.com/r/MachineLearning/comments/dt0u7p/where_to_start_learning_about_ml/,swordnoobVV,1573147168,Hi all my aim is to learn and apply machine learning .I have just finished the entire codecademy course on Python and really would like for someone to guide me on where to continue next.i checked the machine learning course on codecademy but it wasn't as begginer friendly as needed.Please help guys I would really appreciate it.,0,1
343,2019-11-8,2019,11,8,2,dt0vog,How Deep Learning is Different from Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/dt0vog/how_deep_learning_is_different_from_machine/,yiriwer,1573147326,,0,1
344,2019-11-8,2019,11,8,2,dt1e04,[Free Course-Udemy] - The Complete R programming Course: Zero to Hero,https://www.reddit.com/r/MachineLearning/comments/dt1e04/free_courseudemy_the_complete_r_programming/,upworknepal,1573149372,[removed],0,1
345,2019-11-8,2019,11,8,3,dt1rih,Google T5 Explores the Limits of Transfer Learning,https://www.reddit.com/r/MachineLearning/comments/dt1rih/google_t5_explores_the_limits_of_transfer_learning/,Yuqing7,1573150878,[removed],0,1
346,2019-11-8,2019,11,8,3,dt1sy3,Any ideas for podcast name?,https://www.reddit.com/r/MachineLearning/comments/dt1sy3/any_ideas_for_podcast_name/,Doctor_who1,1573151033,[removed],0,1
347,2019-11-8,2019,11,8,4,dt39wj,Beginner Machine Learning Question!,https://www.reddit.com/r/MachineLearning/comments/dt39wj/beginner_machine_learning_question/,rocksarefake,1573156748,[removed],0,1
348,2019-11-8,2019,11,8,5,dt3ona,[D] arxiv.org is down? use archive.org!,https://www.reddit.com/r/MachineLearning/comments/dt3ona/d_arxivorg_is_down_use_archiveorg/,InfiniteChaoxys,1573158358,"For anyone (e.g. my roommate) who needs to access arXiv papers but can't because the website is down, you can use the Wayback Machine to get copies of papers that were scraped in the past. For example:

 [https://web.archive.org/web/20190621093536/https://arxiv.org/pdf/1603.02754.pdf](https://web.archive.org/web/20190621093536/https://arxiv.org/pdf/1603.02754.pdf) 

I hope this is helpful for someone!",50,1
349,2019-11-8,2019,11,8,5,dt3ou4,[R] Learning Visual Dynamics Models of Rigid Objects using Relational Inductive Biases,https://www.reddit.com/r/MachineLearning/comments/dt3ou4/r_learning_visual_dynamics_models_of_rigid/,whiletrue2,1573158377,"Trying to learn object dynamics from vision with Graph Neural Networks? Read some insights in our new paper, e.g. how the choice of edge attributes can deteriorate vision-based prediction performance.    
Paper: [http://arxiv.org/abs/1909.03749](https://t.co/Rv8AggbSD5?amp=1)  
Webpage: [https://sites.google.com/view/dynamicsmodels](https://sites.google.com/view/dynamicsmodels)",0,1
350,2019-11-8,2019,11,8,5,dt3t39,"Need a way to track, compare, explain and reproduce your ML experiments? Check out Comet!",https://www.reddit.com/r/MachineLearning/comments/dt3t39/need_a_way_to_track_compare_explain_and_reproduce/,CometML,1573158831,[removed],0,1
351,2019-11-8,2019,11,8,6,dt4bdc,GPT-2 has Dr. Lecter talking to Lincoln,https://www.reddit.com/r/MachineLearning/comments/dt4bdc/gpt2_has_dr_lecter_talking_to_lincoln/,mantaphysics,1573160828,[removed],0,1
352,2019-11-8,2019,11,8,6,dt4hfv,[P] Rock-Paper-Scissor,https://www.reddit.com/r/MachineLearning/comments/dt4hfv/p_rockpaperscissor/,TenmaYato12,1573161506,"I've created a simple AI using LSTMs in Keras to play rock-paper-scissors.  Any suggestions on how to improve it or if I've made any mistakes? Here is the link to it on Kaggle: 

 [https://www.kaggle.com/tenmayato1/rock-paper-scissors](https://www.kaggle.com/tenmayato1/rock-paper-scissors)",14,1
353,2019-11-8,2019,11,8,6,dt4m5q,Have you used ML in advertising?,https://www.reddit.com/r/MachineLearning/comments/dt4m5q/have_you_used_ml_in_advertising/,writerwritesalot,1573162031,[removed],0,1
354,2019-11-8,2019,11,8,7,dt5ewo,"Homework, how to calculate AUC given some predictions and actual result",https://www.reddit.com/r/MachineLearning/comments/dt5ewo/homework_how_to_calculate_auc_given_some/,FredPower_,1573165222,[removed],0,1
355,2019-11-8,2019,11,8,7,dt5n9j,microsoft/DialoGPT: Large-scale pretraining for dialogue (SOTA conversation model with fine tuned GPT-2),https://www.reddit.com/r/MachineLearning/comments/dt5n9j/microsoftdialogpt_largescale_pretraining_for/,anustretch,1573166170,,0,1
356,2019-11-8,2019,11,8,7,dt5woy,[P] DialogPT: State of the Art Conversational Model with Fine-Tuned GPT-2 (Microsoft Research),https://www.reddit.com/r/MachineLearning/comments/dt5woy/p_dialogpt_state_of_the_art_conversational_model/,anustretch,1573167293,"[https://github.com/microsoft/dialogpt](https://github.com/microsoft/dialogpt)

According to Microsoft:

\&gt; The [human evaluation results](https://github.com/microsoft/dialogpt#human_eval) indicate that the response generated from DialoGPT is comparable to human response quality under a single-turn conversation Turing test.

Example conversation:

\&gt; User Does money buy happiness?

\&gt; Bot Depends how much money you spend on it.

\&gt; User What is the best way to buy happiness?

\&gt; Bot You just have to be a millionaire by your early 20s, then you can be happy.

\&gt; User This is so difficult!

\&gt; Bot You have no idea how hard it is to be a millionaire and happy . There is a reason the rich have a lot of money",55,1
357,2019-11-8,2019,11,8,8,dt68g3,Test set being used as validation set in popular Unsupervised Domain Adaptation methods?!,https://www.reddit.com/r/MachineLearning/comments/dt68g3/test_set_being_used_as_validation_set_in_popular/,chigur86,1573168709,[removed],0,1
358,2019-11-8,2019,11,8,8,dt68l5,Fayyad &amp; Irani's MDL discretization method,https://www.reddit.com/r/MachineLearning/comments/dt68l5/fayyad_iranis_mdl_discretization_method/,Vremin,1573168729,"Hello everyone, I have lately been looking on how to discretize continuous features. 
I came across the MDL discretization method, but don't quite understand how it works. 
Could you please explain me how the algorithm works? 
Best!",0,1
359,2019-11-8,2019,11,8,8,dt6kfo,[R] How can we fool LIME and SHAP? Adversarial Attacks on Post hoc Explanation Methods -- post hoc explanation methods can be games to say whatever you want,https://www.reddit.com/r/MachineLearning/comments/dt6kfo/r_how_can_we_fool_lime_and_shap_adversarial/,agamemnonlost,1573170147,,19,1
360,2019-11-8,2019,11,8,9,dt7cid,Properties of Conv3d,https://www.reddit.com/r/MachineLearning/comments/dt7cid/properties_of_conv3d/,thetonus1150,1573173681,[removed],0,1
361,2019-11-8,2019,11,8,11,dt8wff,Trained a StyleGAN on all emojis from emojipedia. Samples posted hourly. Please help describing samples!,https://www.reddit.com/r/MachineLearning/comments/dt8wff/trained_a_stylegan_on_all_emojis_from_emojipedia/,nehalemlabs,1573181037,[removed],0,1
362,2019-11-8,2019,11,8,12,dt99hs,Why does Exp3 perform well?,https://www.reddit.com/r/MachineLearning/comments/dt99hs/why_does_exp3_perform_well/,genBcat,1573182836,"In the adversarial setting, the rewards have no assumed pattern and the adversary may choose rewards below a given bound as they wish. In this setting, why is it useful to keep track of the past rewards of each of the arms? 

I am really confused on why selecting good ""past"" actions would help since the rewards keep changing. Can someone please help me understand the intuition behind the algorithm?

Thanks!",0,1
363,2019-11-8,2019,11,8,13,dt9zu9,[P] Trained a StyleGAN on all emojis from emojipedia. Samples posted hourly. Please help describing samples!,https://www.reddit.com/r/MachineLearning/comments/dt9zu9/p_trained_a_stylegan_on_all_emojis_from/,nehalemlabs,1573186571,"I set up a twitter bot posting a new sample every hour here:[https://twitter.com/EmojiPainter](https://twitter.com/EmojiPainter)

I am trying to collect a dataset with images and associated descriptions. I am hoping to collect human generated descriptions to augment the existing emojis (too few to train an image - language model on).

The images can be unintentionally hilarious but also sometimes disturbing! Enjoy!",4,1
364,2019-11-8,2019,11,8,14,dtawvx,Which liquid-cooled RTX 2080 Ti brand to choose? [quiet 4 GPU DL office workstation],https://www.reddit.com/r/MachineLearning/comments/dtawvx/which_liquidcooled_rtx_2080_ti_brand_to_choose/,jakub37,1573191624,[removed],0,1
365,2019-11-8,2019,11,8,14,dtb239,[P] Generating Tags from arXiv Paper Titles w/ NLP,https://www.reddit.com/r/MachineLearning/comments/dtb239/p_generating_tags_from_arxiv_paper_titles_w_nlp/,c0rpus_call0sum,1573192434,[https://www.wandb.com/articles/generating-tags-from-arvix](https://www.wandb.com/articles/generating-tags-from-arvix),0,1
366,2019-11-8,2019,11,8,15,dtba35,[P] Generating Tags from Paper Titles w/ NLP,https://www.reddit.com/r/MachineLearning/comments/dtba35/p_generating_tags_from_paper_titles_w_nlp/,c0rpus_call0sum,1573193758,,0,1
367,2019-11-8,2019,11,8,15,dtbajx,The life of future: AI and AR/VR will shape your tomorrow!,https://www.reddit.com/r/MachineLearning/comments/dtbajx/the_life_of_future_ai_and_arvr_will_shape_your/,day1technologies,1573193842,,0,1
368,2019-11-8,2019,11,8,15,dtbia3,The Full GPT-2 still doesn't have strong reasoning,https://www.reddit.com/r/MachineLearning/comments/dtbia3/the_full_gpt2_still_doesnt_have_strong_reasoning/,alexmlamb,1573195184,[removed],0,1
369,2019-11-8,2019,11,8,16,dtbsj2,[Discussion] Next AI winter? Do you think that deep learning solutions will be used in production?,https://www.reddit.com/r/MachineLearning/comments/dtbsj2/discussion_next_ai_winter_do_you_think_that_deep/,jasabdom,1573196986,"ML/AI is gaining traction, but actually not a lot of solutions are being deployed in production, especially in corporations. In startups sure, because very often the main product is AI-based, but I have serious doubts whether regular corporations will continue investing in machine learning engineers. There is still enthusiasm, but not a lot of successful stories, both in CV and NLP ([https://medium.com/tooploox-ai/how-to-optimize-your-operations-with-ai-nlp-d3180f2670e3](https://medium.com/tooploox-ai/how-to-optimize-your-operations-with-ai-nlp-d3180f2670e3)).",22,1
370,2019-11-8,2019,11,8,16,dtc741,Neural Network: How can RNN learn logical rules in time sequence?,https://www.reddit.com/r/MachineLearning/comments/dtc741/neural_network_how_can_rnn_learn_logical_rules_in/,throwaway0599671,1573199610,"Hi, I've been having issues with my code regarding urban sound data set. Im using LSTM, as I ran the code I almost got 99 percent training accuracy but the problem is that the testing accuracy has stopped at 63 percent. I tried many things to resolve this issue however, nothing works. I tried many ways to reduce the overfitting but nothing is working as of now. I used MFCC and mel-spectrogram and I stacked all the features which I can recieve from sound. Please let me know if you have any ideas.",0,1
371,2019-11-8,2019,11,8,18,dtcv5c,Searching for a starter GPU but very confused,https://www.reddit.com/r/MachineLearning/comments/dtcv5c/searching_for_a_starter_gpu_but_very_confused/,thisgirlneedstherapy,1573204190,[removed],1,1
372,2019-11-8,2019,11,8,18,dtcx21,Need help classifying unlabeled data set using WEKA,https://www.reddit.com/r/MachineLearning/comments/dtcx21/need_help_classifying_unlabeled_data_set_using/,MoreHorseThenAMan,1573204558,I'm trying to practice weka and i have a labeled data set for which i have found an algorithm that predicts 98% correctly. I'm trying to now apply these algorithms to an unlabeled data set to predict the results however i can't seem to find any guide on how to add and run the unlabeled data set. Can anyone help me?,0,1
373,2019-11-8,2019,11,8,18,dtczgt,[D] To use triplet loss or not when classes labels are given. Question about theoretical/experimental expectations.,https://www.reddit.com/r/MachineLearning/comments/dtczgt/d_to_use_triplet_loss_or_not_when_classes_labels/,kmkolasinski,1573205029,"Hi, I look for some  theoretical (or experimental) evidences for the superiority (or not) of the triplet loss over cross-entropy loss. Do you know some research papers which try to benchmark following setup?

* Let's say we a fixed dataset which contains M images with annotated labels e.g. MNIST dataset.
* Then we train two models (with same architecture), one with regular categorical cross entropy and second one using triplet-loss approach (or contrastive) etc.

Since the dataset and model architectures are fixed (I assume all other hyperparameters are also fixed, maybe expect learning rate and number of epochs), we will have two models trained to minimize different objectives. I wonder if there is some common knowledge to answer following questions:

* can we expect one of the approaches to have better test accuracy ?
* can we expect one of the approaches to better generalize for new classes (e.g. not present in the training dataset) ? I mean, triplet loss was first used for face recognition, so one would expect that embeddings generated from model trained with triplet loss should be more useful for finding new classes.
* are there other expected differences ? (I'm aware that model trained with triplet loss requires different methodology for measuring performance)

TLDR: different objective function should result in different models. Can we expect the performance differences without training any model?",7,1
374,2019-11-8,2019,11,8,18,dtd1h4,Do you think Google T5 might have cheated on GLUE?,https://www.reddit.com/r/MachineLearning/comments/dtd1h4/do_you_think_google_t5_might_have_cheated_on_glue/,Kavillab,1573205430,[removed],0,1
375,2019-11-8,2019,11,8,18,dtd3v7,Ini Keunggulan Portable Hardness Tester - Testingindonesia.co.id,https://www.reddit.com/r/MachineLearning/comments/dtd3v7/ini_keunggulan_portable_hardness_tester/,rizki28,1573205909,,0,1
376,2019-11-8,2019,11,8,18,dtd4z7,Tag Based Information Retrieval,https://www.reddit.com/r/MachineLearning/comments/dtd4z7/tag_based_information_retrieval/,ihababdk,1573206124,[removed],0,1
377,2019-11-8,2019,11,8,19,dtdcmo,[P] How to Identify Business Processes That Can Be Machine Learning-Enabled,https://www.reddit.com/r/MachineLearning/comments/dtdcmo/p_how_to_identify_business_processes_that_can_be/,cdossman,1573207559,"Identifying Business Processes That Can Be Machine Learning-Enabled   *What to look for and expect when analyzing workflows for tasks can be automated with Machine Learning* 

 [https://medium.com/ai%C2%B3-theory-practice-business/identifying-business-processes-that-can-be-ml-enabled-529cbe90aa84](https://medium.com/ai%C2%B3-theory-practice-business/identifying-business-processes-that-can-be-ml-enabled-529cbe90aa84)",0,1
378,2019-11-8,2019,11,8,19,dtddr2,Download Spreadsheet Using Sheets API in Python - Countants : Scalable Custom Cloud Based Data Solutions,https://www.reddit.com/r/MachineLearning/comments/dtddr2/download_spreadsheet_using_sheets_api_in_python/,Countants123,1573207750,,0,1
379,2019-11-8,2019,11,8,19,dtdhqo,Studies in the variation of the features extracted by a CNN?,https://www.reddit.com/r/MachineLearning/comments/dtdhqo/studies_in_the_variation_of_the_features/,drr21,1573208462,"Hi,  I'm investigating what is the variation in the features extracted by a  CNN per class. In other words, if we have a CNN classifier, every time  we input an image we can extract a feature layer previous to the  classification layer. Let's say we get 1000 features. I'm interesting in  knowing what's the variation inside of a class (e.g. dog) for all these  1000 features. Does it follow a gaussian distribution? Or more complex  distributions?

I've been looking for work done in this task but I haven't found anything. Do you guys know if someone researched this? Thanks!!

Also posted here: [https://www.reddit.com/r/deeplearning/comments/dtdgb6/studies\_in\_the\_variation\_of\_the\_features/](https://www.reddit.com/r/deeplearning/comments/dtdgb6/studies_in_the_variation_of_the_features/)",0,1
380,2019-11-8,2019,11,8,19,dtdhva,Hypi &gt; Blog &gt; Evaluating a Car's Condition with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/dtdhva/hypi_blog_evaluating_a_cars_condition_with/,hypi_universe,1573208483,,0,1
381,2019-11-8,2019,11,8,20,dte66t,[R] Natural image reconstruction from brain waves: a novel visual BCI system with native feedback,https://www.reddit.com/r/MachineLearning/comments/dte66t/r_natural_image_reconstruction_from_brain_waves_a/,permalip,1573212753,[removed],0,1
382,2019-11-8,2019,11,8,21,dteitv,"[D] Regression tasks with ""duplicate samples""",https://www.reddit.com/r/MachineLearning/comments/dteitv/d_regression_tasks_with_duplicate_samples/,Doo0oog,1573214803,"Assume there is a data set `{(x_i, y_i)}, 0&lt;=i&lt;n`, there exist some samples that have the same `x` value but different `y` values (`x_i == x_j &amp;&amp; y_i != y_j`) because there is noise when collecting data.
Are there any researches resolving this kind of data?
If yes, what is this kind of problems called? Or some keywords for doing search.",11,1
383,2019-11-8,2019,11,8,21,dtelxy,Most Important Artificial Intelligence and Machine Learning Interview Questions,https://www.reddit.com/r/MachineLearning/comments/dtelxy/most_important_artificial_intelligence_and/,Reddit-4321,1573215299,[removed],0,1
384,2019-11-8,2019,11,8,21,dteunz,Begginer in need of help,https://www.reddit.com/r/MachineLearning/comments/dteunz/begginer_in_need_of_help/,ludwigavis,1573216636,[removed],0,1
385,2019-11-8,2019,11,8,22,dtf9hd,Titanic: Machine Learning from Disaster,https://www.reddit.com/r/MachineLearning/comments/dtf9hd/titanic_machine_learning_from_disaster/,knut_2,1573218824,[removed],0,1
386,2019-11-8,2019,11,8,22,dtfmt5,Artificial-intelligence vs Machine Learning,https://www.reddit.com/r/MachineLearning/comments/dtfmt5/artificialintelligence_vs_machine_learning/,Himash,1573220729,,0,1
387,2019-11-8,2019,11,8,23,dtfx9m,[R]Theoretical research paper in GAN's,https://www.reddit.com/r/MachineLearning/comments/dtfx9m/rtheoretical_research_paper_in_gans/,Mushr00mParadise,1573222153,"Hello,

I am doing some research about GAN's, and I am looking for some mathematical/theoretical articles. I have noticed a lot of papers presenting new types of GAN's with (sometimes) just some minor alterations to the original. Two papers that fall within the category that I am looking for are:

'On the limitations of First-Order Approximation in GAN Dynamics' -&gt;  [https://arxiv.org/pdf/1706.09884.pdf](https://arxiv.org/pdf/1706.09884.pdf) 

'Which training methods for GANs do actually converge?' -&gt;  [https://arxiv.org/pdf/1801.04406.pdf](https://arxiv.org/pdf/1801.04406.pdf) 

They both start from a simple model and are then able to mathematically prove some  properties and then to empirically demonstrate them. 

I hope the question is clear, and thank you in advance!",10,1
388,2019-11-8,2019,11,8,23,dtg743,First learnings of our computer vision deep dive - Detecting occupied parking spots,https://www.reddit.com/r/MachineLearning/comments/dtg743/first_learnings_of_our_computer_vision_deep_dive/,softtronic,1573223482,[removed],0,1
389,2019-11-8,2019,11,8,23,dtg91v,[R] New Graph Classification Data Sets,https://www.reddit.com/r/MachineLearning/comments/dtg91v/r_new_graph_classification_data_sets/,nd7141,1573223724,"Graph classification has been popular recently, which led to rich development of Graph Kernels and Graph Neural Networks. All papers more or less verify the results on 10-15 benchmark data sets. We found that these data sets (and 40 others) have a lot of isomorphic graphs which leads to (1) train-to-test leakage and (2) incorrect validation comparison. Absurdly, some isomorphic graphs have different classification labels, making it impossible to classify correctly such instances. We explain the reasons why these isomorphic instances appear in data sets in the first place (e.g. meta-data, sizes of graphs, or origin of a data set) and open-source new clean data sets, both in [GitHub](https://github.com/nd7141/graph_datasets) and in [PyTorch-Geometric](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.TUDataset). 

Here is a link to the paper:  [https://arxiv.org/abs/1910.12091](https://arxiv.org/abs/1910.12091) 

Here is more informal [blog post](https://link.medium.com/bHxHoW8Nq1) about findings.",5,1
390,2019-11-8,2019,11,8,23,dtga4w,Pycharm or Anaconda?,https://www.reddit.com/r/MachineLearning/comments/dtga4w/pycharm_or_anaconda/,yurido,1573223854,[removed],0,1
391,2019-11-8,2019,11,8,23,dtgdo5,how to use AIC to see if my model is fitting well,https://www.reddit.com/r/MachineLearning/comments/dtgdo5/how_to_use_aic_to_see_if_my_model_is_fitting_well/,Persona_No_Grata,1573224332,[removed],0,1
392,2019-11-8,2019,11,8,23,dtgiks,"thinking about making a machine playing mario bros using ML.NET, is it a good project.",https://www.reddit.com/r/MachineLearning/comments/dtgiks/thinking_about_making_a_machine_playing_mario/,juasjuasie,1573224967,[removed],0,1
393,2019-11-9,2019,11,9,0,dthczd,[D] Please help me find this paper (Foundations of DL)!,https://www.reddit.com/r/MachineLearning/comments/dthczd/d_please_help_me_find_this_paper_foundations_of_dl/,HenryWJReeve,1573228665,"Dear all,

I have been searching for some time now for a paper I read a while ago but misplaced. 

The paper was very interesting and showed that finding optimisation with with deep neural networks is, in some sense, easier than with shallow neural networks.

The authors generated a data set by generating random input data and then using the predictions of a shallow neural network (A) to provide the ground truth labels of those data. They then tried to train another shallow network (B) with same architecture as (A) the one that created the labels, but with different initializations. It was shown that it was very difficult to find the optimal solution for this dataset. They then tried the same task with a deeper network (C) and found the optimal solution.

If anyone knows the name of this paper then please let me know where I can find it. I would be eternally grateful!

Many thanks!",9,1
394,2019-11-9,2019,11,9,1,dthrev,Machine Learning vs Deep Learning | Machine Learning | Deep Learning [Di...,https://www.reddit.com/r/MachineLearning/comments/dthrev/machine_learning_vs_deep_learning_machine/,LearnaholicIndia,1573230299,,0,1
395,2019-11-9,2019,11,9,1,dthyhg,[D] why softmax+CE over sigmoid+BCE?,https://www.reddit.com/r/MachineLearning/comments/dthyhg/d_why_softmaxce_over_sigmoidbce/,DeMorrr,1573231119,"Most of the popular neural network language models use softmax+cross entropy loss during training, which is based on the assumption that only the target label is true, and everything else is false. But isn't language modeling a multilabel classification task? why sigmoid+BCE isn't used often?",16,1
396,2019-11-9,2019,11,9,1,dthzlo,[R] Deepmind at NeurIPS 2019: Improving the learning effectiveness in healthcare via pre-training with differential privacy,https://www.reddit.com/r/MachineLearning/comments/dthzlo/r_deepmind_at_neurips_2019_improving_the_learning/,valdanylchuk,1573231247,,0,1
397,2019-11-9,2019,11,9,1,dti6hu,"Need a tool to track, compare, explain your ML experiments? Check out Comet.ml. 100% free for public projects.",https://www.reddit.com/r/MachineLearning/comments/dti6hu/need_a_tool_to_track_compare_explain_your_ml/,CometML,1573232083,,0,1
398,2019-11-9,2019,11,9,2,dtiilw,How much disk space do you need for caffe2?,https://www.reddit.com/r/MachineLearning/comments/dtiilw/how_much_disk_space_do_you_need_for_caffe2/,zimmer550king,1573233461,I git cloned the pytorch repo and was in the process of installing caffe2 when I got a prompt saying that I had run out of disk space!!!? I looked at the pytorch folder where I was installing caffe2 and its size had gone up to 28 GB. Is caffe2 really so large???,0,1
399,2019-11-9,2019,11,9,2,dtiout,Machine Learning &amp; Data Analysis,https://www.reddit.com/r/MachineLearning/comments/dtiout/machine_learning_data_analysis/,arman_52,1573234210,[removed],0,1
400,2019-11-9,2019,11,9,2,dtircc,[D] Is Neural Magic a scam?,https://www.reddit.com/r/MachineLearning/comments/dtircc/d_is_neural_magic_a_scam/,isthataprogenjii,1573234506,I recently learned about this new startup which advertises that they can provide GPU level learning using a CPU. There are already CPU versions of neural network training algorithms. Is neural magic doing false advertisement? What approach are they taking specifically to make the 'magic' happen?,28,1
401,2019-11-9,2019,11,9,3,dtjnlu,Drone Image Recognition,https://www.reddit.com/r/MachineLearning/comments/dtjnlu/drone_image_recognition/,iain247,1573238273,"Hey everyone. I'm working on a project which involves using image recognition to allow an autonomous drone to identify a red square target. I have a general idea on how to carry this out. However, it involves using image recognition with a raspberry pi to report the target's coordinates and I'm not sure how to get started with this. Is there any good open source software I could use?. Or alternatively, do you know of any relative projects or papers. Any advice would also be appreciated. Thanks in advance :)",0,1
402,2019-11-9,2019,11,9,3,dtjpc3,"A new semantic answer engine powered by BERT &amp; GPT 2, developed by SOCO.AI, a CMU spin-off.",https://www.reddit.com/r/MachineLearning/comments/dtjpc3/a_new_semantic_answer_engine_powered_by_bert_gpt/,tianchez_ai,1573238476,,0,1
403,2019-11-9,2019,11,9,4,dtk7ht,Scalp Images Dataset,https://www.reddit.com/r/MachineLearning/comments/dtk7ht/scalp_images_dataset/,redchill707,1573240556,[removed],0,1
404,2019-11-9,2019,11,9,4,dtkgc6,Best Practices for NLP Classification in TensorFlow 2.0,https://www.reddit.com/r/MachineLearning/comments/dtkgc6/best_practices_for_nlp_classification_in/,ThinkCritically,1573241570,,0,1
405,2019-11-9,2019,11,9,5,dtkvdd,Tensorflow 2.0 support for other models,https://www.reddit.com/r/MachineLearning/comments/dtkvdd/tensorflow_20_support_for_other_models/,purpletuce,1573243276,[removed],0,1
406,2019-11-9,2019,11,9,5,dtkwyd,Searching for model for action recognition using 3d convolution neural networks,https://www.reddit.com/r/MachineLearning/comments/dtkwyd/searching_for_model_for_action_recognition_using/,lucky31044,1573243435,[removed],0,1
407,2019-11-9,2019,11,9,6,dtlwz9,The precisionFDA and Georgetown-ICBI Brain Cancer Predictive Modeling and Biomarker Discovery Challenge is now live!,https://www.reddit.com/r/MachineLearning/comments/dtlwz9/the_precisionfda_and_georgetownicbi_brain_cancer/,hollystephens723,1573247536,[removed],0,1
408,2019-11-9,2019,11,9,7,dtmmdg,Which autoencoder to use?,https://www.reddit.com/r/MachineLearning/comments/dtmmdg/which_autoencoder_to_use/,kylepob,1573250536,[removed],0,1
409,2019-11-9,2019,11,9,7,dtmyyt,[D] Statistical Physics and Neural Networks question.,https://www.reddit.com/r/MachineLearning/comments/dtmyyt/d_statistical_physics_and_neural_networks_question/,AlexSnakeKing,1573252044,"If you look at the theoretical physics literature, there's a ton of research being done on statistical physics of neural networks and statistical physics of deep learning, etc...where they use analogies between spin glasses and condensed matter models to get all sorts of theoretical results about neural networks. 

To be clear, I'm not talking about studies were neural nets were used to model and solve a problem in statistical physics. I'm thinking about the line of research were the mathematics of statistical physics and spin glasses are used as frameworks to analyze the behavior of neural nets, and then arrive at conclusions like ""The loss surface of neural nets have this particular topological property"" or ""CNN show a phase transition when the number of classes jumps from x to y"", etc.....

My question did any of these theoretical results from the analysis of neural nets using methods from physics ever lead to any practical results, such as a faster training algorithm, or  improved generalization ability, etc....?

As far as I can tell: No, none of the popular NNet models incorporate results from these physics inspired studies. All the improvements come from purely mathematical insights, or originally from biological insights. 

But I might be wrong: Did any of the significant practical developments in NNets and Deep Learning  (better activation functions, training algorithms, regularizations methods,...) stem from the statistical physics approaches?",52,1
410,2019-11-9,2019,11,9,7,dtnd4h,[Project] Aim and Shoot: A game where your opponents are neural networks,https://www.reddit.com/r/MachineLearning/comments/dtnd4h/project_aim_and_shoot_a_game_where_your_opponents/,mitousa,1573253786,"Link to game: [https://www.outpan.com/app/5f2e59fad6/aim-and-shoot](https://www.outpan.com/app/5f2e59fad6/aim-and-shoot)

From the repo: 

""I've always wanted to take the time to make a [Neuroevolution](https://en.wikipedia.org/wiki/Neuroevolution) experiment, so I did.

Each bot is controlled by it's own Neural Network (that I made a while back - [here](https://github.com/victorqribeiro/digitRecognition)). When all the bots die, the genetic algorithm evaluates their fitness score (based on how many shots they fired, how many hits the got, how many friends they shot, how much they hurt themselves and how much they moved during the round) and cross the ones with the highest scores.

This goes on forever, until the player dies (which will happen eventually, so Nole can't never save the human race, after all). By the way, the background history is a joke. I don't mean to make fun of anyone. The idea just seems funny and fit the project.""",0,1
411,2019-11-9,2019,11,9,8,dto33v,Fathers of Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/dto33v/fathers_of_artificial_intelligence/,bardhito,1573257032,[removed],0,1
412,2019-11-9,2019,11,9,9,dtoo1p,[P] Java Deep Learning cookbook finally released!,https://www.reddit.com/r/MachineLearning/comments/dtoo1p/p_java_deep_learning_cookbook_finally_released/,willis7747,1573259794,[removed],0,1
413,2019-11-9,2019,11,9,12,dtqbdn,[D] How has work on optimizing training cost and model size for deep learning algorithms progressed?,https://www.reddit.com/r/MachineLearning/comments/dtqbdn/d_how_has_work_on_optimizing_training_cost_and/,alias_is,1573268433,[removed],0,1
414,2019-11-9,2019,11,9,12,dtqj0x,How to Get into Data Science  Math Or Coding,https://www.reddit.com/r/MachineLearning/comments/dtqj0x/how_to_get_into_data_science_math_or_coding/,weihong95,1573269647,[removed],0,1
415,2019-11-9,2019,11,9,12,dtqwp7,Loss-To-Domain,https://www.reddit.com/r/MachineLearning/comments/dtqwp7/losstodomain/,ZeroMaxinumXZ,1573271783,,0,1
416,2019-11-9,2019,11,9,13,dtr51c,[P] Hidden Markov Model with GMM as emission distributions: A new implementation on top of TF2,https://www.reddit.com/r/MachineLearning/comments/dtr51c/p_hidden_markov_model_with_gmm_as_emission/,kesmarag,1573273159,"Does anyone still use HMMs? 

My new python implementation on top of TensorFlow 2.0

[https://gitlab.com/kesmarag/hmm-gmm-tf2](https://gitlab.com/kesmarag/hmm-gmm-tf2)

Any comment or suggestion is well appreciated!",4,1
417,2019-11-9,2019,11,9,14,dts0jq,BRAZIL FOREST FIRES ANALYSIS,https://www.reddit.com/r/MachineLearning/comments/dts0jq/brazil_forest_fires_analysis/,sahibpreet,1573278566,[removed],0,1
418,2019-11-9,2019,11,9,15,dtsf3l,Beat the Heat with this Machine Learning Cheat Sheet,https://www.reddit.com/r/MachineLearning/comments/dtsf3l/beat_the_heat_with_this_machine_learning_cheat/,AnujG23,1573281324,,0,1
419,2019-11-9,2019,11,9,15,dtsjng,Google's Unrestricted Adversarial Examples Challenge,https://www.reddit.com/r/MachineLearning/comments/dtsjng/googles_unrestricted_adversarial_examples/,W70aYK8tn4,1573282251,,0,1
420,2019-11-9,2019,11,9,16,dtsom2,[R] Accurate and interpretable modelling of conditional distributions (predicting densities) by decomposing joint distribution into mixed moments,https://www.reddit.com/r/MachineLearning/comments/dtsom2/r_accurate_and_interpretable_modelling_of/,jarekduda,1573283235,"I am developing methodology e.g. for very accurate modeling of joint distribution by decomposing in basis of orthonormal polynomials - where coefficients have similar interpretation as (mixed) moments, e.g. to model their relations, time evolution for nonstationary time series.

We can nicely see growing likelihood of such predictions as conditional distributions when adding information from succeeding variables.

While people are used to predicting values, which can be put into excel table, we can get better predictions by modelling entire (conditional) probability distributions - starting with additionally getting variance evaluating uncertainty of such predicted value e.g. as expected value.

I have implementation and further develop it - what kind of data could you suggest to use it for? ML methods to compare it with?

[Slides](https://www.dropbox.com/s/7u6f2zpreph6j8o/rapid.pdf), [recent paper](https://arxiv.org/pdf/1911.02361), its overview: 

https://i.imgur.com/2xNPCIm.png",8,1
421,2019-11-9,2019,11,9,16,dtswon,End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds,https://www.reddit.com/r/MachineLearning/comments/dtswon/endtoend_multiview_fusion_for_3d_object_detection/,sandskies123,1573284877,[removed],1,1
422,2019-11-9,2019,11,9,16,dtt066,Training deep learning models on a cluster,https://www.reddit.com/r/MachineLearning/comments/dtt066/training_deep_learning_models_on_a_cluster/,ssd123456789,1573285610,"I have always either trained models on my own gpu or on Google Colab. However, I need to now train a model on a cluster that is situated locally in a lab. All I know is that I need to use SSH and a docker container. Can anyone share any resources that will be helpful in getting started? Couldn't find much for beginners on YouTube or Google.",0,1
423,2019-11-9,2019,11,9,16,dtt0sb,Life is y = f(x) and can be optimized via gradient descent given a cost function.,https://www.reddit.com/r/MachineLearning/comments/dtt0sb/life_is_y_fx_and_can_be_optimized_via_gradient/,leagueProgrammer,1573285756,,0,1
424,2019-11-9,2019,11,9,17,dttdzk,Master's in CV or ML?,https://www.reddit.com/r/MachineLearning/comments/dttdzk/masters_in_cv_or_ml/,JoshBatz,1573288425,I've gotta make a decision and I'm totally torn apart. I really want to be future proofed and get into a decent job. Whaaat do I pursue????,0,1
425,2019-11-9,2019,11,9,17,dtting,[D] Training deep learning models on a cluster,https://www.reddit.com/r/MachineLearning/comments/dtting/d_training_deep_learning_models_on_a_cluster/,ssd123456789,1573289329,"I have always either trained models on my own gpu or on Google Colab. However, I need to now train a model on a cluster situated locally in a lab. All I know is that I need to use SSH and a docker container. Can anyone share any resources that will be helpful in getting started? Couldn't find much for beginners on YouTube or Google.",21,1
426,2019-11-9,2019,11,9,20,dtv00w,Whats a good major to develop problem solving skills in the age of artificial intelligence?,https://www.reddit.com/r/MachineLearning/comments/dtv00w/whats_a_good_major_to_develop_problem_solving/,Devils_negotiator,1573300279,[removed],0,1
427,2019-11-9,2019,11,9,20,dtv0kr,The visual hull concept for silhouette-based image understanding,https://www.reddit.com/r/MachineLearning/comments/dtv0kr/the_visual_hull_concept_for_silhouettebased_image/,HistoricalTouch0,1573300389,"Hi, is anyone familiar with this paper [The visual hull concept for silhouette-based image understanding](https://areeweb.polito.it/ricerca/cgvg/Articles/pami94.pdf) and know what Fig.19 is doing?",0,1
428,2019-11-9,2019,11,9,21,dtv785,[D] Ways to classify text with very low training data?,https://www.reddit.com/r/MachineLearning/comments/dtv785/d_ways_to_classify_text_with_very_low_training/,ThinkCritically,1573301649,"What techniques does this group feel works best when classifying text with low amounts of training data?

I ask because I recently put together a [tutorial](https://towardsdatascience.com/best-practices-for-nlp-classification-in-tensorflow-2-0-a5a3d43b7b73) that shows how to use TensorFlow Data Pipelines and NLP classification (BERT) and it gets 85% accuracy, but it tends to work best when there are at least 200 examples of a particular class. 

 I am uncertain if this  technique will work if I only have 1 or two examples of training data for a class.  For example, it is uncertain if the same approach would be as effective if I have a piece of text that says ""I was in a line today for 3 hours"",  if I only have 1 or two examples of that text, and if I am trying to classify this as ""Long wait times"". Building on what I was saying earlier, I think that this problem made worse when looking at engineering text or text that is specific to a corporation (where it would be difficult to generate the examples or to get Mechanical Turk workers to classify the examples correctly). 

What are your thoughts on this? Have you seen better ways to classify text when there are low amounts of data?",25,1
429,2019-11-9,2019,11,9,21,dtvbun,I NEED HELP! I am not getting the linear regression line on the chart.,https://www.reddit.com/r/MachineLearning/comments/dtvbun/i_need_help_i_am_not_getting_the_linear/,romish18,1573302537,[removed],0,1
430,2019-11-9,2019,11,9,22,dtw262,How to check scores after submitting rebuttal? (AAAI),https://www.reddit.com/r/MachineLearning/comments/dtw262/how_to_check_scores_after_submitting_rebuttal_aaai/,ArkhamKnight15,1573306965,"I am asking in particular about AAAI. After submitting the rebuttal and clicking ""View Author Feedback summary"" on Microsoft CMT portal, I cannot see the scores/original review. I can only see the submitted rebuttal. How do I know that the rebuttal has been considered and the scores have been updated/not updated? Is there any chance for a (6,6,6) or (6,5,5) to get accepted?",0,1
431,2019-11-9,2019,11,9,22,dtw2vo,Is pandas really slow for simple data processing?,https://www.reddit.com/r/MachineLearning/comments/dtw2vo/is_pandas_really_slow_for_simple_data_processing/,amirninja,1573307075,[removed],0,1
432,2019-11-9,2019,11,9,22,dtw64j,[D] How to check post rebuttal scores? (AAAI),https://www.reddit.com/r/MachineLearning/comments/dtw64j/d_how_to_check_post_rebuttal_scores_aaai/,ArkhamKnight15,1573307581,"I am asking in particular about AAAI. After submitting the rebuttal and clicking ""View Author Feedback summary"" on Microsoft CMT portal, I cannot see the scores/original review. I can only see the submitted rebuttal. How do I know that the rebuttal has been considered and the scores have been updated/not updated? Is there any chance for a (6,6,6) or (6,5,5) to get accepted?",0,1
433,2019-11-9,2019,11,9,23,dtwun4,[D] What are the best universities/colleges to be an ML prof?,https://www.reddit.com/r/MachineLearning/comments/dtwun4/d_what_are_the_best_universitiescolleges_to_be_an/,ilia10000,1573311016,"I'm currently at that point at the end of my PhD where I all of a sudden need to start applying for jobs. I'm applying for both post-docs and prof positions. I thought it's worth a shot even though my guess is that it's not likely to get a prof position at a high-tier institution straight out of a PhD. 

It looks like there are a ton of tenure-track positions open across Europe and North America. How do you choose between them? What are the factors that are important to consider when applying for prof jobs? (e.g. is the number of papers the department gets into NeurIPS a good metric?) And are there any universities that are  generally considered the best for ML profs?",23,1
434,2019-11-10,2019,11,10,0,dtxeaf,[D] How can I understand the Max Pooling Operation in this Paper?,https://www.reddit.com/r/MachineLearning/comments/dtxeaf/d_how_can_i_understand_the_max_pooling_operation/,avdalim,1573313594,"In this paper, the Input got scaled down from  41816  to 40x80x6 with Max Pooling. 

How did they exactly do it?

 [https://arxiv.org/ftp/arxiv/papers/1901/1901.07761.pdf](https://arxiv.org/ftp/arxiv/papers/1901/1901.07761.pdf) 

I want to do something similar done in this paper, but I have 2 Matrixes with 65x49 and 4 with 64x48, since there is a difference with nodes and elements. (There is one more node in each dimension of the Elements)

They look like this:

[https://imgur.com/a/7Mtq4s4](https://imgur.com/a/7Mtq4s4)

The problem is: strains can be only on elements, the volume fraction as well (These are the 4 64x48 Matrixes). The displacement can only be shown with nodes. (These are the 2 65x49 Matrixes)

I was thinking about adding padding to the smaller Matrixes, so that the Dimensions are the same. Is a zero-padding okay in this case, since I make a Max Pooling Operation anyways?",6,1
435,2019-11-10,2019,11,10,1,dty5fd,problem in matplotlib,https://www.reddit.com/r/MachineLearning/comments/dty5fd/problem_in_matplotlib/,ahmedib123,1573317035,"the numbers in plots appear upper the plot not inside it so it do not appear clearly 

and this error appear

#  &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fac659afe10&gt;",0,1
436,2019-11-10,2019,11,10,1,dty6bs,[P] A Tool for gAnimating Anime with StyleGAN,https://www.reddit.com/r/MachineLearning/comments/dty6bs/p_a_tool_for_ganimating_anime_with_stylegan/,re_gen,1573317149,"I've been working on a multipart project involving a reimplementation of StyleGAN and a research tool to interact with trained StyleGAN models.

Here're some example images/gifs from the project:

* [UI of the tool](https://imgur.com/BJUAeFN)
* [Spatially isolated animations](https://imgur.com/383nHxe)
* [Rectangular image generation with attribute modification](https://imgur.com/lHXcj0X)
* Automatic [facial feature detection](https://imgur.com/IOlEJNu) and [modification](https://imgur.com/xG9DHBI) without needing labels for the training data:
* [Unfortunately, somewhat poor-quality male images](https://imgur.com/yJBgHuq)

&amp;#x200B;

I published a couple of blogs that go into more detail.

In the first blog, I introduce the project and discuss the results, the implementation, and the [training data](https://www.gwern.net/Danbooru2018). I also share the code for the tool and my StyleGAN reimplementation:

[https://towardsdatascience.com/animating-ganime-with-stylegan-part-1-4cf764578e](https://towardsdatascience.com/animating-ganime-with-stylegan-part-1-4cf764578e)

The second part is a tutorial that demonstrates how to use the tool to animate images and detect facial features. In it, I supply a compiled version of the (Windows) tool that can be used to follow the tutorial:

[https://towardsdatascience.com/animating-ganime-with-stylegan-the-tool-c5a2c31379d?source=friends\_link&amp;sk=eec12e2da8c84b9736d32f697da21689](https://towardsdatascience.com/animating-ganime-with-stylegan-the-tool-c5a2c31379d?source=friends_link&amp;sk=eec12e2da8c84b9736d32f697da21689)

My background before ML was reverse engineering, so building a tool that simplified visualizing and interacting with the internal representation of a model felt like an important step to understand it better. There were some results I did not expect, like how modifying a single feature map can consistently make the same meaningful changes across many different images (like opening/closing a mouth). Also, the ability of some feature maps to act as facial feature detectors without training labels made me interested in applying the same approach to other types of generative models.

Let me know if you have any questions/comments/corrections/criticisms or know about similar prior work. As this has been a solo project, I'm pretty starved for outside perspectives.",2,1
437,2019-11-10,2019,11,10,1,dtyce9,Merging 3 samples of the same image into a single image using a Neural Network,https://www.reddit.com/r/MachineLearning/comments/dtyce9/merging_3_samples_of_the_same_image_into_a_single/,AlanRoofies,1573317919,"So, I want to merge multiple images that contain exactly the same scene but with different color filters or different effects, and i was thinking about simply creating a CNN that takes in 3 RGB images of the same size and that contain the same scene (9 channels in total), and that outputs 1 RGB image (3 channels in total) (merge).

theoretically the neural network would merge each RGB values in a pixel position to give the best outcome. 

do you have any idea if this would work ? do you have any ideas on how to achieve this ? any advice ? any resources i could use ?

PS: this is my first real project with neural nets, all i have been doing until now is following tutorials.",0,1
438,2019-11-10,2019,11,10,1,dtydc8,[R] Evolving Structures in Complex Systems,https://www.reddit.com/r/MachineLearning/comments/dtydc8/r_evolving_structures_in_complex_systems/,downtownslim,1573318043,,2,1
439,2019-11-10,2019,11,10,2,dtypw6,[D] I am confused. It appears that the Tensor2Tensor Transformer recquires the actual target as an input?,https://www.reddit.com/r/MachineLearning/comments/dtypw6/d_i_am_confused_it_appears_that_the_tensor2tensor/,ReasonablyBadass,1573319611,"If, say, we translate from english to german then it appears that the Decoder wants the german text as in Input before it produces an output that could be used for a loss calculation and then training.

[https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py)",4,1
440,2019-11-10,2019,11,10,2,dtyw1b,2020 AI Residency Guide,https://www.reddit.com/r/MachineLearning/comments/dtyw1b/2020_ai_residency_guide/,Yuqing7,1573320424,,0,1
441,2019-11-10,2019,11,10,2,dtz7sb,Physics and machine learning,https://www.reddit.com/r/MachineLearning/comments/dtz7sb/physics_and_machine_learning/,Nexus9291,1573321918,"This post is inspired by a recent post on this /r about the statistical physics of neural networks. 

I'm working towards my PhD in theoretical physics, and since undergraduate, I always knew my passion was at the intersection of computer science and physics. So far, I've worked on numerous projects that involved me implementing algorithms from scratch, etc. Recently, I picked up a project that involves machine learning. Since I can't just code without knowing what I'm really doing under the hood, I picked up some references to learn more about ML and wow! It's so cool---both the math and the concepts are just magnificently amazing, not to mention the algorithms that packages like PyTorch use. 

I know now more than ever that I desire an industry job that operates at the intersection of physics and machine learning. I don't mean I'm looking for projects where ML is merely applied, but rather I like to be involved in thinking about ML from a theoretical perspective. A perfect example is the inspiration of this post, where I leanred that statistical physics is used to understand the behavior of neural nets theoretically. 

So my question is, what concepts connect theoretical physics to theoretical research in ML? How can I make my physics PhD experience as closely tied in with ML?",0,1
442,2019-11-10,2019,11,10,2,dtzb90,"What do the matrix (S, U, V) returned by singular value decomposition represent (in terms of variation)?",https://www.reddit.com/r/MachineLearning/comments/dtzb90/what_do_the_matrix_s_u_v_returned_by_singular/,rainboworigamipaper,1573322353,[removed],0,1
443,2019-11-10,2019,11,10,3,dtzhdm,"How do I, someone with no machine learning experience, learn how to build machine learning software?",https://www.reddit.com/r/MachineLearning/comments/dtzhdm/how_do_i_someone_with_no_machine_learning/,fred_the_mailman,1573323131,[removed],0,1
444,2019-11-10,2019,11,10,3,dtznlv,"CNN with ""non square"" input with pytorch.",https://www.reddit.com/r/MachineLearning/comments/dtznlv/cnn_with_non_square_input_with_pytorch/,psychodrivenmusic,1573323914,"Hey! 

I want to build a classifier that  takes in pictures that dont have ""square"" dimensions. That is, not 32x32 but 32x400 for instance. 

I cant find any documentation for where in code this is to be specified. I am using pytorch with python. Does anyone know how to do this?",0,1
445,2019-11-10,2019,11,10,4,du043f,What's the latest in speech synthesis where one is able to make minor text-based changes to existing speech?,https://www.reddit.com/r/MachineLearning/comments/du043f/whats_the_latest_in_speech_synthesis_where_one_is/,strich,1573326028,[removed],0,1
446,2019-11-10,2019,11,10,4,du0ips,Catch me if you can,https://www.reddit.com/r/MachineLearning/comments/du0ips/catch_me_if_you_can/,greygorius,1573327894,[removed],0,1
447,2019-11-10,2019,11,10,4,du0ril,[P] A Deep Dream implementation in PyTorch,https://www.reddit.com/r/MachineLearning/comments/du0ril/p_a_deep_dream_implementation_in_pytorch/,juanigp,1573329043,"Hi, I know I am kinda late for this topic but this effect still blows my mind, I have been working on an implementation in PyTorch that you can see on this repo

[https://github.com/juanigp/Pytorch-Deep-Dream](https://github.com/juanigp/Pytorch-Deep-Dream)

And here are some images I got using my code

![]( https://raw.githubusercontent.com/juanigp/Pytorch-Deep-Dream/master/pics/facetrip.png )

![]( https://raw.githubusercontent.com/juanigp/Pytorch-Deep-Dream/master/pics/catrippy.png )",17,1
448,2019-11-10,2019,11,10,5,du0wvy,Tips for a high schooler in terms of eventually getting a top program for a PhD in ML/AI?,https://www.reddit.com/r/MachineLearning/comments/du0wvy/tips_for_a_high_schooler_in_terms_of_eventually/,Agile_Musician,1573329736,[removed],0,1
449,2019-11-10,2019,11,10,5,du12hy,I scraped a ton of data from a popular forum website (not reddit!) and I want to open source that data + the scraper! What is the best way to go about this??,https://www.reddit.com/r/MachineLearning/comments/du12hy/i_scraped_a_ton_of_data_from_a_popular_forum/,SmartSpray,1573330443,"To the best of my knowledge, this dataset doesn't exist yet. Also, I want to release my scraper cause why not. I think its a pretty good scraper and am proud of it :)",0,1
450,2019-11-10,2019,11,10,5,du1b6p,What are some cool things one person can make with machine learning?,https://www.reddit.com/r/MachineLearning/comments/du1b6p/what_are_some_cool_things_one_person_can_make/,fred_the_mailman,1573331594,[removed],0,1
451,2019-11-10,2019,11,10,6,du267r,[D] Models which try to learn operations like mirroring,https://www.reddit.com/r/MachineLearning/comments/du267r/d_models_which_try_to_learn_operations_like/,gwynbleiddeyr,1573335580,"Hi,

Some time back someone jested on this sub about swapping a pair of images (`x, y = y, x`) using a 'model' (I believe the context was a faceswap news) and I was wondering whether there are works that try to learn discrete-ish operations like swap, mirror etc. using a neural network (possibly just to poke around the 'learnings'). For example, a network that learns [image mirroring](https://imgur.com/slE7zmY) horizontally (third panel is prediction from a bad feed forward model trying to minimize mse on the flipped image).

Considering that data augmentation in most fields use a lot of similar operations, I tried looking around there but didn't find anything (not sure what to search for exactly). A [recent one](https://arxiv.org/abs/1906.11172) does some sort of learned data augmentation but its output is _based on_ these operations and not discovering them. I guess the problem doesn't make much sense in context of models like neural nets and fit in more nicely with a more GOFAI approach where impose semantics and then extract rules from data. In any case, wanted to know sources that have inspected this or similar ideas.",9,1
452,2019-11-10,2019,11,10,6,du2c5j,[R] AllenNLP interpret: EMNLP Best Demo Paper Award,https://www.reddit.com/r/MachineLearning/comments/du2c5j/r_allennlp_interpret_emnlp_best_demo_paper_award/,rstoj,1573336337,"AllenNLP has won the EMNLP 2019 Best Demo Paper Award:

[https://allennlp.org/interpret](https://allennlp.org/interpret)

From their website:

&gt;We present AllenNLP Interpret, a toolkit built on top of AllenNLP for interactive model interpretations. The toolkit makes it easy to apply gradient-based **saliency maps** and **adversarial attacks** to *new models*, as well as develop *new interpretation methods*. AllenNLP interpret contains three components: a suite of interpretation techniques applicable to most models, APIs for developing new interpretation methods (e.g., APIs to obtain input gradients), and reusable front-end components for visualizing the interpretation results.

For people doing NLP: do you feel this is going to be useful to you? If so, in which contexts?

(Disclaimer: I'm not associated with this project in any way - just curious on how to use it)",0,1
453,2019-11-10,2019,11,10,7,du2lmv,[N] The State of Machine Learning Frameworks in 2019,https://www.reddit.com/r/MachineLearning/comments/du2lmv/n_the_state_of_machine_learning_frameworks_in_2019/,asuagar,1573337510,,0,1
454,2019-11-10,2019,11,10,7,du2pj7,[D] Tips for a high schooler in terms of eventually getting a top program for a PhD in ML/AI?,https://www.reddit.com/r/MachineLearning/comments/du2pj7/d_tips_for_a_high_schooler_in_terms_of_eventually/,Agile_Musician,1573338032,"Hello! I'm a senior who is currently applying to undergrad programs.

I was wondering if you guys have any suggestions for what programs I should apply to and what I need to do to make into a top 5 PhD program (i.e. UCB, Stanford, CMU, Caltech, MIT).

I'm mostly interested in what steps I can do as a high schooler to help increase my chances as I hear that these programs are incredibly competitive. I don't think I will be competetive for T20 school but I hope I will get into a T50 school like UW, Colorado School of Mines, Grinell, or RPI for my undergrad (all of which I am happy with). Are there any other good programs for grad placement that I should apply to? Should I take any programs out?

Also, what should I do in order to help me later on? I'm planning on taking some AI related online courses.

Should I try to get a research position (btw, can I get this at a community college in Washington or no) or should I try to get an internship at a company like Microsoft or a startup?

My stats can be found here: https://old.reddit.com/r/chanceme/comments/dtbmqp/chance_me_grinnell_uiuc_cornell_gatech_uw_cs/",35,1
455,2019-11-10,2019,11,10,7,du2s02,[D] Converting HuggingFace GPT2 Models to Tensorflow 1.x,https://www.reddit.com/r/MachineLearning/comments/du2s02/d_converting_huggingface_gpt2_models_to/,leogao2,1573338334,"\&gt; [HuggingFace Transformers](https://github.com/huggingface/transformers) is a wonderful suite of tools for working with transformer models in both Tensorflow 2.x and Pytorch. However, many tools are still written against the original TF 1.x code published by OpenAI. Unfortunately, the model format is different between the TF 2.x models and the original code, which makes it difficult to use models trained on the new code with the old code. There are many tools for converting the old format to TF 2.x and Pytorch, but not vice versa. In this blog post, I will share the (frustrating) process of getting the conversion to work.

[https://leogao.dev/2019/11/09/Converting-HuggingFace-GPT2-Models-to-Tensorflow-1/](https://leogao.dev/2019/11/09/Converting-HuggingFace-GPT2-Models-to-Tensorflow-1/)",0,1
456,2019-11-10,2019,11,10,7,du31u1,I hate regularization. Please help me!,https://www.reddit.com/r/MachineLearning/comments/du31u1/i_hate_regularization_please_help_me/,FalseCondition,1573339566,[removed],0,1
457,2019-11-10,2019,11,10,8,du3ojy,"Ken Burns Effect, Now In 3D!",https://www.reddit.com/r/MachineLearning/comments/du3ojy/ken_burns_effect_now_in_3d/,lkjiomva,1573342576,,0,1
458,2019-11-10,2019,11,10,9,du3ywu,[D] Recruiting for AI Startup - Getting Test Questions Right,https://www.reddit.com/r/MachineLearning/comments/du3ywu/d_recruiting_for_ai_startup_getting_test/,xanc17,1573344012,"Has anyone here worked with HackerRank? If so, Id value any help choosing appropriate questions as Im recruiting graduate CE/EE students. Looking forward to all constructive feedback!",2,1
459,2019-11-10,2019,11,10,9,du44y3,Transformer predicts own input on time-series data,https://www.reddit.com/r/MachineLearning/comments/du44y3/transformer_predicts_own_input_on_timeseries_data/,collider_in_blue,1573344885,"Hi, I'm working on a project which requires predicting the remainder of a time series given many examples. For example, given a time series from T=0 to T=m, I need to predict T=m to T=n.

I've trained a few different autoregressive models for the task: (1) pure decoder Transformer where I learn the joint probability over the full sequence then given the known part of the sequence I just impute the part that I'm interested in, and (2) encoder-decoder Transformer where I provide the data up to the point I'd like to predict as conditioning information then model the joint conditional probability over just the region of interest.

In both cases, I'm finding a very strong effect where the network learns to simply predict whatever the input value is at each time step (and I've confirmed that the labels are being passed in correctly  shifted right relative to input.) This means during inference it will always predict a straight line. In contrast with neural machine translation or language modeling tasks where the token of the next word may be very different from the previous word, with a high resolution time series the next token is always almost the input token because it's a function. I've also tried a continuous version of the Transformer and it simply picks out a few common modes and predicts these each time during inference. I found I can do better in terms of RMSE and MAE by just using a fully connected network that predicts the entire region of interest simultaneously (making the assumption each point is independent of the others) which seems strange.

Does anyone have experience with a similar task and suggestions on how to handle this? I imagine using an artificially lower time resolution would make this better but that solution is rather unsatisfying.

(I've seen a few blog posts and a previous r/ML post about Transformers on time series data but none address this problem.)",0,1
460,2019-11-10,2019,11,10,9,du4b8e,Do you have trouble deploying machine learning models?,https://www.reddit.com/r/MachineLearning/comments/du4b8e/do_you_have_trouble_deploying_machine_learning/,rvitorper,1573345773,[removed],0,1
461,2019-11-10,2019,11,10,9,du4fql,[D] Transformer predicts own input on time-series data,https://www.reddit.com/r/MachineLearning/comments/du4fql/d_transformer_predicts_own_input_on_timeseries/,collider_in_blue,1573346424,"Hi, I'm working on a project which requires predicting the remainder of a time series given many examples. For example, given a time series from T=0 to T=m, I need to predict T=m to T=n.

I've trained a few different autoregressive models for the task: (1) pure decoder Transformer where I learn the joint probability over the full sequence then given the known part of the sequence I just impute the part that I'm interested in, and (2) encoder-decoder Transformer where I provide the data up to the point I'd like to predict as conditioning information then model the joint conditional probability over just the region of interest.

In both cases, I'm finding a very strong effect where the network learns to simply predict whatever the input value is at each time step (and I've confirmed that the labels are being passed in correctly  shifted right relative to input.) This means during inference it will always predict a straight line. In contrast with neural machine translation or language modeling tasks where the token of the next word may be very different from the previous word, with a high resolution time series the next token is always almost the input token because it's a function. I've also tried a continuous version of the Transformer and it simply picks out a few common modes and predicts these each time during inference. I found I can do better in terms of RMSE and MAE by just using a fully connected network that predicts the entire region of interest simultaneously (making the assumption each point is independent of the others) which seems strange.

Does anyone have experience with a similar task and suggestions on how to handle this? I imagine using an artificially lower time resolution would make this better but that solution is rather unsatisfying.

(I've seen a few blog posts and a previous r/MachineLearning post about Transformers on time series data but none address this problem.)",14,1
462,2019-11-10,2019,11,10,10,du4p0l,What is the patentability of domain-specific ML/DL applications?,https://www.reddit.com/r/MachineLearning/comments/du4p0l/what_is_the_patentability_of_domainspecific_mldl/,pedalstiffcranks,1573347760,[removed],0,1
463,2019-11-10,2019,11,10,10,du4xav,[D] How patentable are domain-specific ML applications?,https://www.reddit.com/r/MachineLearning/comments/du4xav/d_how_patentable_are_domainspecific_ml/,pedalstiffcranks,1573348979,"I've built a Mask RCNN-based tool for processing images in a specific scientific sub-domain (that has commercial applications). I was all set to submit it to [The Journal of Open Source Software](https://joss.theoj.org/) -- however, my research group is funded by a large, deep-pocketed company that shall not be named. After months of not responding to multiple emails about the project and my intent to publish it, they've decided at the last minute that they want to ""look into its patentability.""

What is the likelihood that this is a patentable application? (It may be relevant that I didn't write the Mask R-CNN model from scratch -- I use an implementation that is MIT-licensed.)

In general, how patentable are domain-specific ML applications (where the ML is not novel)?",1,1
464,2019-11-10,2019,11,10,10,du55br,[P] Neural Network based Terrain Generator,https://www.reddit.com/r/MachineLearning/comments/du55br/p_neural_network_based_terrain_generator/,apseren,1573350182,"&amp;#x200B;

https://preview.redd.it/hcibsplhirx31.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=1094e2e472b64aaedee7bcb275c8cf7e83cf52e7

Official site: [https://apseren.com/mlterraform/](https://apseren.com/mlterraform/)",30,1
465,2019-11-10,2019,11,10,11,du5f4e,[R] Learning on the Edge: Investigating Boundary Filters in CNNs,https://www.reddit.com/r/MachineLearning/comments/du5f4e/r_learning_on_the_edge_investigating_boundary/,xternalz,1573351639,,0,1
466,2019-11-10,2019,11,10,11,du5f6a,"[D] I'm another high school student wanting to get into a top CS school (Waterloo, MIT, Stanford, etc) to study ML. Any advice?",https://www.reddit.com/r/MachineLearning/comments/du5f6a/d_im_another_high_school_student_wanting_to_get/,stormtrooper1721,1573351648,"And what are my chances of being accepted?

SAT: 1500-1550
2 sat subject tests

5 ap courses; 3 5s and 2 4s

Admissions avg around the 95% mark

I have several silver and bronze medals medals on kaggle and I'm ranked in in the top 500 of all kagglers.

Several large ml projects, as well as contributions to major ml open-source projects




What should I do to get a better chance of getting in? I don't have any publications at any conferences so maybe I should start there?",5,1
467,2019-11-10,2019,11,10,11,du5fp2,[R] Learning on the Edge: Investigating Boundary Filters in CNNs,https://www.reddit.com/r/MachineLearning/comments/du5fp2/r_learning_on_the_edge_investigating_boundary/,xternalz,1573351715,,1,1
468,2019-11-10,2019,11,10,14,du7ko4,[P] Kaldi speech recognition gRPC server,https://www.reddit.com/r/MachineLearning/comments/du7ko4/p_kaldi_speech_recognition_grpc_server/,gwynbleiddeyr,1573364725,"Hello,

While working with [Kaldi](https://kaldi-asr.org/) speech recognition models, we found relatively less serving solutions which haven't worked out nicely for us so we have been building a simple server [here](https://github.com/Vernacular-ai/kaldi-serve).

The program should not be considered stable, is missing many features we want (like supporting on-the-fly HLCG creation for providing speech contexts like in gspeech) and we are going relatively slow on it, it can be used without unreasonable effort if you have regular trained chain models.

Feel free to suggest changes, improvements, PRs etc.",0,1
469,2019-11-10,2019,11,10,14,du7lnd,Anomaly detection in a sequence of strings,https://www.reddit.com/r/MachineLearning/comments/du7lnd/anomaly_detection_in_a_sequence_of_strings/,aagmon,1573364902,[removed],0,1
470,2019-11-10,2019,11,10,15,du7uve,[P] tog: A hackable Emacs based data-tagging framework,https://www.reddit.com/r/MachineLearning/comments/du7uve/p_tog_a_hackable_emacs_based_datatagging_framework/,gwynbleiddeyr,1573366521,"There are some really good tools for tagging data and creating datasets for ML like [doccano](https://github.com/chakki-works/doccano). Most of these are web GUIs though which I find hard and annoying to extend.

Some time back, I made a system for tagging within Emacs which then got extended into sort of a framework. If you live inside Emacs and are willing to spend some time creating a fast tagging workflow, you can try [tog](https://github.com/Vernacular-ai/tog) which lets you create custom data taggers by writing a few rendering and parsing functions. I have been using it personally for the following:

1. NER tagging
2. Audio/text intent tagging
3. Transcribing
4. Voicing texts or parses
5. Triplet-ish song similarity tagging

It's all Emacs Lisp, so you can extend everything. A recent example, I hit an active learning backend on each save and get next to-tag data points which are _hard_ according to the then tagged dataset + model.",3,1
471,2019-11-10,2019,11,10,15,du8582,How you prepare your data?,https://www.reddit.com/r/MachineLearning/comments/du8582/how_you_prepare_your_data/,TrueLankinen,1573368426,[removed],0,1
472,2019-11-10,2019,11,10,17,du8zrj,[D] When the AAAI 2020 announce the Notification?,https://www.reddit.com/r/MachineLearning/comments/du8zrj/d_when_the_aaai_2020_announce_the_notification/,Mannershin,1573374926,I can't sleep until I see the result................ plz.....,22,1
473,2019-11-10,2019,11,10,17,du902g,GPT-2 text,https://www.reddit.com/r/MachineLearning/comments/du902g/gpt2_text/,talkingtoaj2,1573374994,[removed],3,1
474,2019-11-10,2019,11,10,17,du9166,[D] GLM and Fully connected layer,https://www.reddit.com/r/MachineLearning/comments/du9166/d_glm_and_fully_connected_layer/,andryxxx,1573375218," Often a Generalized Linear Model is developed using a neural network with one or more Fully Connected layers, but I can't understand why.

Here you can find an example:  
[https://github.com/quertenmont/GLMPerf/blob/master/Airline%20Delay%20with%20a%20GLM%20in%20Keras.ipynb](https://github.com/quertenmont/GLMPerf/blob/master/Airline%20Delay%20with%20a%20GLM%20in%20Keras.ipynb)

**How are GLMs and neural network with** **Fully Connected layers related to each other?**",24,1
475,2019-11-10,2019,11,10,18,du9h2l,Hey,https://www.reddit.com/r/MachineLearning/comments/du9h2l/hey/,erkalsedat,1573378719,[removed],0,1
476,2019-11-10,2019,11,10,18,du9kvv,Disadvantages of moving window ensemble approach?,https://www.reddit.com/r/MachineLearning/comments/du9kvv/disadvantages_of_moving_window_ensemble_approach/,J3diMindTricks,1573379507,[removed],0,1
477,2019-11-10,2019,11,10,20,dua8wv,"[N] PyTorch at Tesla - Andrej Karpathy, Tesla",https://www.reddit.com/r/MachineLearning/comments/dua8wv/n_pytorch_at_tesla_andrej_karpathy_tesla/,strontal,1573384573,,0,1
478,2019-11-10,2019,11,10,21,duaqe9,R vs python which language is better for machine learning???,https://www.reddit.com/r/MachineLearning/comments/duaqe9/r_vs_python_which_language_is_better_for_machine/,darkside2301,1573388100,[removed],0,1
479,2019-11-10,2019,11,10,21,duayjy,[D] SSH and using a remote server,https://www.reddit.com/r/MachineLearning/comments/duayjy/d_ssh_and_using_a_remote_server/,ssd123456789,1573389634,"My last post asked for help with SSH and training a model on a remote server. 
In this post can I just get a confirmation for what I think I need to do?
1) build docker image for project 
2) transfer image to server using scp
3) log in to server 
4) select a machine to compute on
5) use linux command to run docker image
Did I get this right? If not, can someone please tell me what I got wrong?
Thank you for your help everyone!",0,1
480,2019-11-10,2019,11,10,22,dubfmo,Im 16 and need help for neural network,https://www.reddit.com/r/MachineLearning/comments/dubfmo/im_16_and_need_help_for_neural_network/,Paolo31000,1573392654,[removed],0,1
481,2019-11-10,2019,11,10,22,dubj2f,[D] Is there any way to explain the output features of the word2vec.,https://www.reddit.com/r/MachineLearning/comments/dubj2f/d_is_there_any_way_to_explain_the_output_features/,korokage,1573393184,"I am aware of the famous example of Embedding(King) - Embedding(Man) + Embedding(Woman) = Embedding(Queen). From this example, we can say that the characteristic of ""royalty"" has been understood.

I guess in a way I am trying to interpret the hidden layer neurons which might not always have meaning.

I have looked into techniques like SHAP and LIME but I'm still to plug the concepts together.",6,1
482,2019-11-10,2019,11,10,23,dubrb4,I need understand how neural network in pc work =&gt; i create my own lib,https://www.reddit.com/r/MachineLearning/comments/dubrb4/i_need_understand_how_neural_network_in_pc_work_i/,lukas0025,1573394448,[removed],0,1
483,2019-11-11,2019,11,11,1,dudq1i,How would the peer review systems improve in AI conferences?,https://www.reddit.com/r/MachineLearning/comments/dudq1i/how_would_the_peer_review_systems_improve_in_ai/,DrMoonlightBox,1573404051,[removed],0,1
484,2019-11-11,2019,11,11,1,duds5d,[D] Colab has P100 GPUs,https://www.reddit.com/r/MachineLearning/comments/duds5d/d_colab_has_p100_gpus/,RickMcCoy,1573404320,"So apparently [Colab has P100 GPUs now](https://imgur.com/YZy4p0u). I can't find any announcements and/or discussions about this, seems like google decided to do this without any notice?",35,1
485,2019-11-11,2019,11,11,2,due21y,[R] Understanding the Neural Tangent Kernel,https://www.reddit.com/r/MachineLearning/comments/due21y/r_understanding_the_neural_tangent_kernel/,MindSustenance,1573405623,"Hey everyone,

I've put up a new blog post, [Understanding the Neural Tangent Kernel](https://rajatvd.github.io/NTK), that aims to distill the ideas behind the _neural tangent kernel_ that is making waves in recent theoretical deep learning research. A large portion of the talks in the recent [Workshop on Theory of Deep Learning](https://www.math.ias.edu/wtdl) at the Institute for Advanced Study were based on ideas related to the neural tangent kernel. This is a slightly long post, as it involves a fair bit of math (you can skip some of the proofs though). A bit of linear algebra background is necessary to fully grasp what is going on here, but I hope that my visualizations can help with that.

Code for the experiments and animations: https://github.com/rajatvd/NTK

Feedback and suggestions are welcome!",29,1
486,2019-11-11,2019,11,11,2,duen9l,Dear Reddit what can I do with this,https://www.reddit.com/r/MachineLearning/comments/duen9l/dear_reddit_what_can_i_do_with_this/,ALPHAG7,1573408232,,0,1
487,2019-11-11,2019,11,11,3,duet8e,AI talks to AI,https://www.reddit.com/r/MachineLearning/comments/duet8e/ai_talks_to_ai/,MACBDJr,1573408968,,0,1
488,2019-11-11,2019,11,11,4,dufozv,Best linux laptops (with linux pre-installed) for doing machine learning,https://www.reddit.com/r/MachineLearning/comments/dufozv/best_linux_laptops_with_linux_preinstalled_for/,zimmer550king,1573412690,[removed],0,1
489,2019-11-11,2019,11,11,4,dug1z5,Combining probabilities from different predictions over time to reduce uncertainty,https://www.reddit.com/r/MachineLearning/comments/dug1z5/combining_probabilities_from_different/,drr21,1573414208,[removed],0,1
490,2019-11-11,2019,11,11,4,dug5n6,Feature Maps Visualization in dynamics,https://www.reddit.com/r/MachineLearning/comments/dug5n6/feature_maps_visualization_in_dynamics/,DeepRobotics,1573414649,"The video: [https://youtu.be/RNnKtNrsrmg](https://youtu.be/RNnKtNrsrmg)

&amp;#x200B;

https://preview.redd.it/kx5wswaiuwx31.png?width=2335&amp;format=png&amp;auto=webp&amp;s=8b8c3188c7f5afac5e9c22230b57888ed04dedc1",0,1
491,2019-11-11,2019,11,11,5,dughjv,learn algorithms for data science and machine learning,https://www.reddit.com/r/MachineLearning/comments/dughjv/learn_algorithms_for_data_science_and_machine/,skj8,1573416041,,0,1
492,2019-11-11,2019,11,11,5,duh1ig,"[D][N] Tensorflow announces Tensorflow 2.0 Hackathon. $150,000 in prizes. Submissions due Dec 31st.",https://www.reddit.com/r/MachineLearning/comments/duh1ig/dn_tensorflow_announces_tensorflow_20_hackathon/,BatmantoshReturns,1573418343,"https://tfworld.devpost.com/

It looks like their competing with Pytorch, I remember Pytorch has 60,000 in prizes in their hackathon.",12,1
493,2019-11-11,2019,11,11,5,duh9ta,Help Analysing Loss and accuracy graphs,https://www.reddit.com/r/MachineLearning/comments/duh9ta/help_analysing_loss_and_accuracy_graphs/,CarrotCakePls,1573419313,[removed],0,1
494,2019-11-11,2019,11,11,7,dui7xu,[P] fast-scnn - An implementation of the proposed Fast-SCNN,https://www.reddit.com/r/MachineLearning/comments/dui7xu/p_fastscnn_an_implementation_of_the_proposed/,kenanajkunic,1573423368,"This is my implementation of the proposed Fast-SCNN. You can learn more about it \[here\]([https://arxiv.org/abs/1902.04502](https://arxiv.org/abs/1902.04502))

[https://github.com/kenanajkunic/fast-scnn](https://github.com/kenanajkunic/fast-scnn)",4,1
495,2019-11-11,2019,11,11,7,duigmr,XLM-R: Amazing results on XLU and GLUE benchmarks from Facebook AI: large transformer network trained on 2.5TB of text from 100 languages.,https://www.reddit.com/r/MachineLearning/comments/duigmr/xlmr_amazing_results_on_xlu_and_glue_benchmarks/,abdeljalil73,1573424410,https://arxiv.org/abs/1911.02116,0,1
496,2019-11-11,2019,11,11,8,dujdwu,A simple app to Transfer Makeup from one face to another.,https://www.reddit.com/r/MachineLearning/comments/dujdwu/a_simple_app_to_transfer_makeup_from_one_face_to/,cortlandd,1573428647,[removed],0,1
497,2019-11-11,2019,11,11,10,dukn1r,[D] what youtube channels/podcasts do you find post interesting videos/lectures about AI/ML?,https://www.reddit.com/r/MachineLearning/comments/dukn1r/d_what_youtube_channelspodcasts_do_you_find_post/,Laafheid,1573434311,"Personally I like to watch(/listen) moreso than read,  


what youtube channels would you recommend for interesting developments/information w.r.t. ML or AI  
I would recommend, in no particular order, personally familiar with:

* This week in Machine Learning and AI
* Lex Fridman's Artificial Intelligence podcast
* Deepmind
* Institute for Advanced Study
* Microsoft research
* Two Minute Papers",24,1
498,2019-11-11,2019,11,11,10,dul2gl,5 Things You Should Know Before Getting into Data Science,https://www.reddit.com/r/MachineLearning/comments/dul2gl/5_things_you_should_know_before_getting_into_data/,weihong95,1573436305,[removed],0,1
499,2019-11-11,2019,11,11,10,dul3xq,"[D] How can I test a network trained on triplet loss, given an anchor sample and test sample?",https://www.reddit.com/r/MachineLearning/comments/dul3xq/d_how_can_i_test_a_network_trained_on_triplet/,Cranial_Vault,1573436495,"During training I generate triplets consisting of an anchor, a positive match, and a negative match.   So long as the difference in the L2 norm from the anchor to each is within some margin, the prediction of ""match vs not a match"" is correct.  How can this be implemented for a deployed model when I only have an anchor and a single test sample?  I've been looking into [FaceNet](https://github.com/davidsandberg/facenet) but there doesn't appear to be a straightforward method for testing with just two samples like there is for looking at a full triplet.",0,1
500,2019-11-11,2019,11,11,11,dulkeb,[D] How might I test a network with only two samples when it was trained using triplet loss?,https://www.reddit.com/r/MachineLearning/comments/dulkeb/d_how_might_i_test_a_network_with_only_two/,Cranial_Vault,1573438631,"During training I generate triplets using an anchor sample along with a positive match and negative match.  The L2 distance of each from the anchor is used to calculate loss, but how does this work in the deployed case where you have only two samples where one might be an anchor?  I have been looking at [FaceNet](https://arxiv.org/abs/1503.03832) but the paper only mentions ""a squared L2 distance threshold"".   Is this something that needs to be determined empirically?  Or is there a better analytical solution?",12,1
501,2019-11-11,2019,11,11,11,dulp3y,best Twitter accounts to follow for updates/news re: AI/ML?,https://www.reddit.com/r/MachineLearning/comments/dulp3y/best_twitter_accounts_to_follow_for_updatesnews/,jasbrooks03,1573439246,,0,1
502,2019-11-11,2019,11,11,11,dulyt3,"train a model on synthesis data, generalize it on real data",https://www.reddit.com/r/MachineLearning/comments/dulyt3/train_a_model_on_synthesis_data_generalize_it_on/,boostsch,1573440518,[removed],0,1
503,2019-11-11,2019,11,11,11,dum18j,Vertical Federated Learning Problem,https://www.reddit.com/r/MachineLearning/comments/dum18j/vertical_federated_learning_problem/,JayYip,1573440839,[removed],0,1
504,2019-11-11,2019,11,11,12,dum89i,[D] Do you think Google T5 might have cheated on GLUE?,https://www.reddit.com/r/MachineLearning/comments/dum89i/d_do_you_think_google_t5_might_have_cheated_on/,Kavillab,1573441752,"It used Common crawl for training data which i believe is a crawl of the entire internet right?

So what if the GLUE and SuperGLUE task answers were in the training data?",9,1
505,2019-11-11,2019,11,11,12,dumi8a,Good resource for Horse Racing,https://www.reddit.com/r/MachineLearning/comments/dumi8a/good_resource_for_horse_racing/,kiwamizamurai,1573443110,[removed],0,1
506,2019-11-11,2019,11,11,12,dumieb,[D] Tracking algorithms State-of-the-art that can run on CPU real time or near real time in real world application ?,https://www.reddit.com/r/MachineLearning/comments/dumieb/d_tracking_algorithms_stateoftheart_that_can_run/,hosjiu,1573443135,[removed],0,1
507,2019-11-11,2019,11,11,15,duo7g4,[D] AAAI-20 Notifications,https://www.reddit.com/r/MachineLearning/comments/duo7g4/d_aaai20_notifications/,ihavesomeinquiries,1573452377,"20.6% acceptance rate this year compared to 16.2% last year. Another ""record number of submissions."" 7737 papers reviewed this year compared to 7095 last year.",12,1
508,2019-11-11,2019,11,11,15,duodbm,[R] Self-Supervised Representation Learning (overview blog post),https://www.reddit.com/r/MachineLearning/comments/duodbm/r_selfsupervised_representation_learning_overview/,hardmaru,1573453331,"An [article](https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html) by Lilian Weng discuss self-supervised learning techniques applied to images (there are many articles on self-supervised NLP), with an emphasis on control-based RL tasks.

https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html",23,1
509,2019-11-11,2019,11,11,17,dupbk6,[P] DOVPANDA: A really awesome open source that will save your life while using Pandas,https://www.reddit.com/r/MachineLearning/comments/dupbk6/p_dovpanda_a_really_awesome_open_source_that_will/,PhYsIcS-GUY227,1573459473,"*Obligatory disclaimer - this is not my project, I just thought it's awesome and you guys should know about it.*

This time, it's [Directions OVer PANDAs or DOVPANDA](https://github.com/dovpanda-dev/dovpanda?fbclid=IwAR1NdzIBW9LtAtIhDtXXuqsfOo8sd1ekIYIW35BbueG4zYqnLTXUeEpUHQw). From the GitHub description:

&gt;Directions are hints and tips for using pandas in an analysis environment. dovpanda is an overlay for working with pandas in an analysis environment.  
If you think your task is common enough, it probably is, and Pandas probably has a built-in solution. dovpanda is an overlay module that tries to understand what you are trying to do with your data, and help you find easier ways to write your code.

In other words, it will help you write better Pandas code by giving you great hints &amp; tips on how to do just that.

I always like packages with a simple integration, and this one is no different. Just:

    import pandas as pd
    import dovpanda

And you're good to go.   
It's most useful when working via notebook but displays hints and tips in the console as well.

I wonder if there are any similar tools for other widely used data science / ML libraries. Please share if you know any.",15,1
510,2019-11-11,2019,11,11,18,dupugc,Talk to transformer,https://www.reddit.com/r/MachineLearning/comments/dupugc/talk_to_transformer/,kringata,1573463038,[removed],0,1
511,2019-11-11,2019,11,11,18,duqb3n,[D] When using BERT what is the best way to convert the embedding back into text?,https://www.reddit.com/r/MachineLearning/comments/duqb3n/d_when_using_bert_what_is_the_best_way_to_convert/,ReasonablyBadass,1573466155,"Since the vocab file has over 30,000 entries a softmax vector would be gigantic.

But a similarity lookup would recquire embedding the vocab file myself, correct? In a way similar to the BERT embedding, which we can't get because the entire point of BERT is an embedding depndent on the text itself.",10,1
512,2019-11-11,2019,11,11,19,duqh7p,ROC for KNN?,https://www.reddit.com/r/MachineLearning/comments/duqh7p/roc_for_knn/,emergenthoughts,1573467227,[removed],0,1
513,2019-11-11,2019,11,11,19,duqhom,"[D] Search vs Sentiment, Google Trends vs Twitter",https://www.reddit.com/r/MachineLearning/comments/duqhom/d_search_vs_sentiment_google_trends_vs_twitter/,m-i-n-a-r,1573467321,"Hi, i'm looking for an advice or an idea. I'm writing a thesis and i chose to use what i learned in another project (prediction using Google Trends) to investigate the difference between the meaning of a search on Google and a Tweet on Twitter. 

I'm using Pytrends and Tweepy and i collected some data about a set of trending topics. I'm comparing them using various techniques, but i'd like to use those data in some kind of ML model (recomendation maybe?).

I'd really appreciate some advice to make my thesis more than just a data analysis, thanks in advance!",6,1
514,2019-11-11,2019,11,11,19,duqrel,Human in the loop workflows for deep learning and OCR automation,https://www.reddit.com/r/MachineLearning/comments/duqrel/human_in_the_loop_workflows_for_deep_learning_and/,manneshiva,1573469156,[removed],0,1
515,2019-11-11,2019,11,11,19,duqrkb,World first ever RPG being made with 'Dungeon Master AI' a 'story engine' using neural networks and machine learning.,https://www.reddit.com/r/MachineLearning/comments/duqrkb/world_first_ever_rpg_being_made_with_dungeon/,bugsixx,1573469193,,0,1
516,2019-11-11,2019,11,11,19,duqs0n,Finding hundred(s) of users for dataset?,https://www.reddit.com/r/MachineLearning/comments/duqs0n/finding_hundreds_of_users_for_dataset/,FeldsparKnight,1573469284,[removed],0,1
517,2019-11-11,2019,11,11,21,durghm,How would you try to build an AGI if you had truly unlimited compute? [discussion],https://www.reddit.com/r/MachineLearning/comments/durghm/how_would_you_try_to_build_an_agi_if_you_had/,yitzilitt,1573473740,"If somehow you managed to get your hands on a computer with literally unlimited compute power (through an interstellar alien technology exchange or whatever), and could feed it any/all data currently available online, what would be your approach to creating a true AGI? Is there any approach currently out there that might realistically result in an AGI if given enough processing power/data?

Im currently trying my hand at writing speculative fiction, and was wondering what a realistic approach in such a scenario might look like...",54,1
518,2019-11-11,2019,11,11,21,durklk,Estimating binomial parameter from noisy data? (Wilson score interval),https://www.reddit.com/r/MachineLearning/comments/durklk/estimating_binomial_parameter_from_noisy_data/,TombKrax,1573474392,"Hi,

I am a comp.sci. major in search of answers on parameter estimation. I am doing a research project where I want to estimate a ""success probability"" \*p\* from a set of \*n\* independent observations {0,1}, where \*m&lt;n\* denotes the number of ""successes"". If my data is not noisy then I can estimate \*p\* using a Wilson score interval.

The problem is that each outcome has a nonzero probability of being a ""false"" observation (i.e. the negated output). Lucky for us there is an upper bound \*q\* on the probability with which an observation is false. I have managed to make a maximum likelihood estimate for \*q\* but that does not provide an interval in the case where \*n\* is small.

So my questions are:

* Does the Wilson score interval still apply?
* Is there a convergent estimator for \*p\* in a binomial experiment when the data is noisy (with a known upper bound on the noise)?

I suppose this is well-known stuff but I have not been able to find any articles specifically on noisy data, so I was hoping you could help me out.

Thank you for your attention.",0,1
519,2019-11-11,2019,11,11,21,dury97,[R] MIDAS: Microcluster-Based Detector of Anomalies in Edge Streams,https://www.reddit.com/r/MachineLearning/comments/dury97/r_midas_microclusterbased_detector_of_anomalies/,siddharthb_,1573476621,"MIDAS detects microcluster anomalies from an edge stream in constant time and memory, while providing theoretical guarantees about its false positive probability.

Paper: [https://www.comp.nus.edu.sg/\~sbhatia/assets/pdf/midas.pdf](https://www.comp.nus.edu.sg/~sbhatia/assets/pdf/midas.pdf) (Accepted at AAAI 2020)

Code: [https://github.com/bhatiasiddharth/MIDAS](https://github.com/bhatiasiddharth/MIDAS)

Feedback is welcome!",0,1
520,2019-11-11,2019,11,11,22,dus4rh,[D]Why network runs much faster after loading the trained models(parameters)?,https://www.reddit.com/r/MachineLearning/comments/dus4rh/dwhy_network_runs_much_faster_after_loading_the/,AlphaGoMK,1573477609,"I've found that before loading trained models, the network runs at relatively low speed. After loading the .pth file, the speed of inference boosts about 10 times faster. Does this circumstance normally come in deep learning? 

I've tried on SSD(single shot multibox detector) on object detection task of COCO dataset. 

Before loading, I got 2-3 fps on GTX1080, and after that, it reaches 20 fps on the same device under same environment.",11,1
521,2019-11-11,2019,11,11,22,dush2q,Artificial Intelligence Could Be a $14 Trillion Boon to the Global EconomyIf It Can Overcome These Obstacles,https://www.reddit.com/r/MachineLearning/comments/dush2q/artificial_intelligence_could_be_a_14_trillion/,mantha_anirudh,1573479418,,0,1
522,2019-11-11,2019,11,11,23,dutca9,GPU slower than CPU on simple network,https://www.reddit.com/r/MachineLearning/comments/dutca9/gpu_slower_than_cpu_on_simple_network/,gabberthomson,1573483613,[removed],0,1
523,2019-11-12,2019,11,12,0,duu2rj,pytorch/pytorch-ci-dockerfiles by pytorch,https://www.reddit.com/r/MachineLearning/comments/duu2rj/pytorchpytorchcidockerfiles_by_pytorch/,sjoerdapp,1573486884,,0,1
524,2019-11-12,2019,11,12,1,duug3c,"Nov 21, Free Talk on PyTorch with Its Co-Author and Maintainer, Adam Paszke",https://www.reddit.com/r/MachineLearning/comments/duug3c/nov_21_free_talk_on_pytorch_with_its_coauthor_and/,ACMLearning,1573488445,,0,1
525,2019-11-12,2019,11,12,1,duug9h,How to save feature embeddings in production,https://www.reddit.com/r/MachineLearning/comments/duug9h/how_to_save_feature_embeddings_in_production/,_pydl_,1573488465,[removed],0,1
526,2019-11-12,2019,11,12,1,duul7r,"[P] Auptimizer - A faster, easier way to do HPO",https://www.reddit.com/r/MachineLearning/comments/duul7r/p_auptimizer_a_faster_easier_way_to_do_hpo/,YetAnotherAI,1573489045,"Hey all, a team I'm part of just open-sourced our internal HPO tool called Auptimizer. Auptimizer does a couple of things. It provides a single interface to 6 different HPO algorithms including Spearmint and HyperOpt. It also makes it easy to scale your model training from CPUs and GPUs all the way to multiple instances on AWS. The repo is on [Github](https://github.com/LGE-ARC-AdvancedAI/auptimizer). We have an article about it on [Medium](https://towardsdatascience.com/auptimizer-a-faster-easier-way-to-do-hyperparameter-optimization-for-machine-learning-88f37c1fcfb7) and you can find more implementation details in our [2019 IEEE Big Data paper](https://arxiv.org/abs/1911.02522). If you do HPO, check it out and let us know on Github how things look.",3,1
527,2019-11-12,2019,11,12,1,duulq0,[D] How to save feature vectors in production server?,https://www.reddit.com/r/MachineLearning/comments/duulq0/d_how_to_save_feature_vectors_in_production_server/,_pydl_,1573489111,"I  have feature vectors with 2048 elements, also my feature may change  overtime e.g. new features are added. I use Faiss as a search engine but  I am not quite sure how to save these vectors. Right now I am syncing  local folder with AWS s3. I do not think it is optimal because I have to  sync files each time I search for similarity which takes a while.

Maybe I should use a vector database (like [https://github.com/a-mma/AquilaDB](https://github.com/a-mma/AquilaDB) or some other) or is there a more optimized method to sync my local and s3 storage?",2,1
528,2019-11-12,2019,11,12,1,duurn1,Anybody working in Fintech? What are some challenges you face related to Credit risk management?,https://www.reddit.com/r/MachineLearning/comments/duurn1/anybody_working_in_fintech_what_are_some/,NextCrab2,1573489784,[removed],0,1
529,2019-11-12,2019,11,12,1,duv0pc,Build a Machine Learning Model in your Browser using TensorFlow.js and Python,https://www.reddit.com/r/MachineLearning/comments/duv0pc/build_a_machine_learning_model_in_your_browser/,BlisteringBernacle,1573490841,,0,1
530,2019-11-12,2019,11,12,2,duv8zg,Can we have more case studies here please?,https://www.reddit.com/r/MachineLearning/comments/duv8zg/can_we_have_more_case_studies_here_please/,NextCrab2,1573491770,[removed],0,1
531,2019-11-12,2019,11,12,2,duvock,"Hey Siri, Define Artificial Intelligence And Machine Learning",https://www.reddit.com/r/MachineLearning/comments/duvock/hey_siri_define_artificial_intelligence_and/,kazi_analyst,1573493471,[removed],0,1
532,2019-11-12,2019,11,12,4,dux2rm,"MultiGPU: Lower performance with increased batch size, number of workers, and max queue.",https://www.reddit.com/r/MachineLearning/comments/dux2rm/multigpu_lower_performance_with_increased_batch/,zalamandagora,1573499005,"[If anyone knows a very active forum for discussing things like this, please let me know in the comments.]

I'm getting unexpected results when I'm tweaking the training parameters of my run.

* CNN model (~1M parameters, consecutive 2D CNN into two FC layers)

* Data set: 20k images from CelebA with 218x178x3 images

* GCP VM with 16 cores, 104 GB RAM, 4 K80 GPUs.

* VM Image: The image is the latest deep learning image w/ Ubuntu and tf 1.15.

I can't seem to get GPU utilization above 60%, or CPU utilization above 7%. It is really weird to me that I get the fastest epoch speeds with 4 workers (for pre-processing and batch assembly), max queue = 2, and a batch size of 32. It seems to me that increasing all of these should increase performance and help the GPU run at max capacity

| Batch size | Workers | Max Queue | Epoch 1 (s) | Epoch 2 (s) |
|:----------:|:-------:|:---------:|:-----------:|:-----------:|
|     16     |    4    |     4     |     135     |      76     |
|     32     |    4    |     0     |     134     |      56     |
|     32     |    4    |     2     |     136     |      49     |
|     32     |    4    |     4     |     136     |      49     |
|     32     |    4    |     16    |     135     |      51     |
|     32     |    8    |     4     |     137     |      56     |
|     32     |    8    |     8     |     138     |      53     |
|     32     |    16   |     0     |     197     |      62     |
|     32     |    16   |     16    |     139     |      57     |
|     32     |    32   |    400    |     161     |      70     |
|     64     |    4    |     4     |     137     |      61     |
|     64     |    16   |    400    |     196     |      61     |
|     64     |    32   |    400    |     200     |      72     |
|     64     |    64   |    400    |     203     |      72     |
|     512    |    4    |     4     |     145     |      56     |
|     512    |    8    |     4     |     154     |      67     |
|     764    |    4    |     4     |     158     |      62     |
|     512    |    8    |     4     |     154     |      67     |
|     764    |    4    |     4     |     158     |      62     |
 
Model is built like so:

with tf.device('/cpu:0'):
       endec = Model(inputs = E_input, outputs = E_outputs)
endec = multi_gpu_model(endec, gpus = GPUS)

Training like so:

        run_history = endec.fit_generator(tr_gen, steps_per_epoch = tr_gen.total_batches,
            epochs = EPOCHS, validation_data = val_gen, validation_steps = val_gen.total_batches,
            shuffle = True, verbose = 2, max_queue_size = MAX_QUEUE, 
            use_multiprocessing = MULTI_THREAD, workers = WORKERS)

Data fetched by generator like so:

    def __load_img_batch(self, batch_list):
        no_files = len(batch_list)

        x_data = np.empty((no_files, *self.img_dims))
        y_data = np.empty((no_files, self.labels.num_labels))
        for i in range(no_files):
            x_data[i,:,:,:] = cv2.cvtColor(cv2.imread(self.dir_path + batch_list[i]), cv2.COLOR_BGR2RGB)
            y_data[i,:] = self.labels.lookup(batch_list[i])
        
        return x_data, y_data

I would really appreciate any insight into why this would be.",0,1
533,2019-11-12,2019,11,12,4,dux7k2,[P] Playing Snake with Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/dux7k2/p_playing_snake_with_reinforcement_learning/,luckymouse0,1573499519,"Snake AI trained on reinforcement learning using a DQN, implemented in TensorFlow and Pygame.

&amp;#x200B;

*Processing gif 77podv3jv3y31...*

Github: [https://github.com/luckymouse0/Snake-RL](https://github.com/luckymouse0/Snake-RL)",26,1
534,2019-11-12,2019,11,12,4,duxkyr,Text generation using GPT-2,https://www.reddit.com/r/MachineLearning/comments/duxkyr/text_generation_using_gpt2/,nlpjohn345,1573500960,[removed],0,1
535,2019-11-12,2019,11,12,4,duxnz1,Is it illegal to use tweets data for a chatbot?,https://www.reddit.com/r/MachineLearning/comments/duxnz1/is_it_illegal_to_use_tweets_data_for_a_chatbot/,aravindraghvi,1573501276,"If I want to build a chatbot trained on the tweets of a particular celebrity, and if I made that chatbot public but not the dataset, will it still be illegal? Is there any workaround to do that?",0,1
536,2019-11-12,2019,11,12,5,duy2ov,Robust Multi-domain Sentiment Classifier,https://www.reddit.com/r/MachineLearning/comments/duy2ov/robust_multidomain_sentiment_classifier/,IbrahimSharaf,1573502839,[removed],0,1
537,2019-11-12,2019,11,12,5,duyvuw,A great and thorough explanation of convolutional neural networks,https://www.reddit.com/r/MachineLearning/comments/duyvuw/a_great_and_thorough_explanation_of_convolutional/,antaloaalonso,1573505876,,0,1
538,2019-11-12,2019,11,12,6,duz663,[D] Impala vs MCTS for self play,https://www.reddit.com/r/MachineLearning/comments/duz663/d_impala_vs_mcts_for_self_play/,so_tiredso_tired,1573506988,"AlphaStar uses Impala over tree search. Comments here explain this is mainly due to action space width.  But conceptually, i never grasped by one method makes better use of a given ""exploration budget"".

A. Is it just tree width or also the episode length?

B. Someone (maybe Vinyals?) mentioned that  ""it would be hard to saturate the GPU"" with tree search. So if sc2 was a light weight reversible environment,  would tree search become feasible?

 (Lets ignore issues such as hidden information, agent league, real time. the building order assistance)

Thank u for any comment",4,1
539,2019-11-12,2019,11,12,6,duzcuu,"CTRL-generated Trump vs real Trump quiz: ""respondents guessed correctly only 40% of the time, or 10% worse than blindly guessing""",https://www.reddit.com/r/MachineLearning/comments/duzcuu/ctrlgenerated_trump_vs_real_trump_quiz/,transtwin,1573507701,,0,1
540,2019-11-12,2019,11,12,6,duzsav,[D] Adversarial Attacks on Obstructed Person Re-identification,https://www.reddit.com/r/MachineLearning/comments/duzsav/d_adversarial_attacks_on_obstructed_person/,paubric,1573509424,"Hi,

I was reading this post ICCV 19 - The state of some (ethically) questionable papers (https://www.reddit.com/r/MachineLearning/comments/dp389c/d_iccv_19_the_state_of_some_ethically/?utm_medium=android_app&amp;utm_source=share) and started wondering whether an adversarial attack on obstructed personal re-identification models would be feasible, how would it be carried out, and whether it would actually make a relevant and useful personal project in the context of todays power dynamics.

For example, with a GAN approach, one might use the re-identification model as a discriminator and try to train a generator to generate custom overlapping mask patches over one's face in order to fool the discriminator into misidentifying the person.

What do you guys think?",26,1
541,2019-11-12,2019,11,12,7,duzu3s,Independent study ideas,https://www.reddit.com/r/MachineLearning/comments/duzu3s/independent_study_ideas/,Glaedr2697,1573509679,[removed],0,1
542,2019-11-12,2019,11,12,9,dv1v4i,[P] Importing Pyspark PipelineModel with custom transformers into Scala,https://www.reddit.com/r/MachineLearning/comments/dv1v4i/p_importing_pyspark_pipelinemodel_with_custom/,Octosaurus,1573518595,"I recently created a PipelineModel with a few custom transformers to generate features not doable with the native Spark transformers. Here's an example of one of my transformers:

    class newLabelMap(
        Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable,
    ):
        inputCol = Param(Params._dummy(),""inputCol"",""The input column"",TypeConverters.toString)
        outputCol = Param(Params._dummy(),""outputCol"",""The output column"",TypeConverters.toString)
    
        def __init__(self, inputCol = """", outputCol=""""):
            super(newLabelMap, self).__init__()
            self._setDefault(inputCol="""")
            self._setDefault(outputCol="""")
            self._set(inputCol=inputCol)
            self._set(outputCol=outputCol)
    
        
        def getInputCol(self):
            return self.getOrDefault(self.inputCol)
        
        def setInputCol(self, inputCol):
            self._set(inputCol=inputCol)
    
        def getOutputCol(self):
            return self.getOrDefault(self.outputCol)
    
        def setOutputCol(self, outputCol):
            self._set(outputCol=outputCol)
    
        def _transform(self, dataset):
            @udf(""string"")
            def findLabel(labelVal):
                new_label_dict = {'oldLabel0' : 'newLabel0',
                              'oldLabel1' : 'newLabel1',
                              'oldLabel2' : 'newLabel1',
                              'oldLabel3' : 'newLabel1',
                              'oldLabel4' : 'newLabel2',
                              'oldLabel5' : 'newLabel2',
                              'oldLabel6' : 'newLabel2',
                              'oldLabel7' : 'newLabel3',
                              'oldLabel8' : 'newLabel3',
                              'oldLabel9' : 'newLabel4',
                              'oldLabel10' : 'newLabel4'}
    
                try:
                    labelKey = new_label_dict[labelVal]
                    return labelKey
                except:
                    return 'other'
    
            out_col = self.getOutputCol()
            in_col = dataset[self.getInputCol()]
            return dataset.withColumn(out_col, findLabel(in_col))

The transformer works fine in the Pipeline, I can save it, load it back into a pyspark session, and transform. The issue comes when I try to import it into a scala environment. When I try to load the model, I receive this error output:

    Name: java.lang.IllegalArgumentException
    Message: requirement failed: Error loading metadata: Expected class name org.apache.spark.ml.PipelineModel but found class name pyspark.ml.pipeline.PipelineModel
    StackTrace:   at scala.Predef$.require(Predef.scala:224)
      at org.apache.spark.ml.util.DefaultParamsReader$.parseMetadata(ReadWrite.scala:638)
      at org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:616)
      at org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:267)
      at org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:348)
      at org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:342)

If I remove the custom transformer, it loads just fine in Scala, so I'm curious how to be able to use custom transformers written in pyspark that can be ported in a PipelineModel to a Scala environment? Do I need to append my code in any way? Any help is greatly appreciated :)",0,1
543,2019-11-12,2019,11,12,10,dv2csl,Does anyone regret dropping out of their PhD?,https://www.reddit.com/r/MachineLearning/comments/dv2csl/does_anyone_regret_dropping_out_of_their_phd/,liqui_date_me,1573520753,[removed],0,1
544,2019-11-12,2019,11,12,10,dv2jbq,Could Deezer's vocal-removal-tool Spleeter be used to remove commentary from NHL / NBA / NHL games?,https://www.reddit.com/r/MachineLearning/comments/dv2jbq/could_deezers_vocalremovaltool_spleeter_be_used/,coulsim,1573521584,[removed],0,1
545,2019-11-12,2019,11,12,10,dv2tsb,"[Project] So Im working on this new project, and I need more data, MUCH more data. So is you could help me out by commenting with some chocolate chip cookie recipes and an honest rating out of 100. This would help me out so much if you could participate.",https://www.reddit.com/r/MachineLearning/comments/dv2tsb/project_so_im_working_on_this_new_project_and_i/,thatnerd69,1573522905,[removed],0,1
546,2019-11-12,2019,11,12,11,dv3g1k,This fascinating 1985 PC Magazine cover story on expert systems will help you appreciate just how far we've come [N],https://www.reddit.com/r/MachineLearning/comments/dv3g1k/this_fascinating_1985_pc_magazine_cover_story_on/,TrueBirch,1573525713,"Back in 1985, PC Magazine dedicated an issue to expert systems, which were an early form of machine learning. Thanks to the Internet Archive, you can see the [whole issue here](https://archive.org/details/PC-Mag-1985-04-16/page/n109). If you're not familiar with expert systems, [Wikipedia](https://en.wikipedia.org/wiki/Expert_system) has you covered. I'm especially impressed by the [review of Expert-Ease](https://archive.org/details/PC-Mag-1985-04-16/page/n119), which reads a lot like some of the user-friendly decision support tools you see emerging these days.",2,1
547,2019-11-12,2019,11,12,11,dv3hmj,Decoding Game of Thrones ($100 Amazon Gift Card),https://www.reddit.com/r/MachineLearning/comments/dv3hmj/decoding_game_of_thrones_100_amazon_gift_card/,apikzorian,1573525923,[removed],0,1
548,2019-11-12,2019,11,12,11,dv3tg8,error in matab in neuroal network,https://www.reddit.com/r/MachineLearning/comments/dv3tg8/error_in_matab_in_neuroal_network/,elyemlahitarik,1573527518,[removed],0,1
549,2019-11-12,2019,11,12,12,dv3ukc,[D] What is the proper etiquette for extending someone's research code?,https://www.reddit.com/r/MachineLearning/comments/dv3ukc/d_what_is_the_proper_etiquette_for_extending/,ilia10000,1573527678,"Over the last several months, I wrote a paper about extensions/improvements to an ML method that someone had previously proposed in their own paper. The authors of the original paper have open-sourced the PyTorch implementation for their paper on Github under an MIT license. I forked their repository and added some fairly major modifications/features to their code. Recently, I put up a preprint of my paper on arXiv, and have received a few requests for my code. I'd like to be able to open-source my code under the MIT license as well, but I want to make sure I properly give credit to the original authors. In my paper I, of course, cite the original authors profusely; I'm just not sure what to do in my code. 

I've considered a few options but I'm not sure if any of them is the right one:

1. I can make a pull request and ask to have my code merged into the original repo, but the changes are fairly major and may actually change the behavior of the original authors' code in ways that makes it inconsistent with how they describe it in their paper. 
2. I can keep maintaining a public fork of the original repo, but I'm not sure what proper etiquette is in this case in terms of modifying READMEs, references, and links. I've also heard that it's bad to maintain a public fork for a long time because it tends to confuse people about which fork they should use. 
3. I can make a new repo and move the code into there, but again, I'm not sure what the etiquette is for properly crediting the original authors. 

Are any of these the right option or should I be doing something else entirely?   
If it is one of these, how can I make sure that I properly credit the original authors for their (very impressive) work?",11,1
550,2019-11-12,2019,11,12,12,dv4b17,Quantum Machine Learning,https://www.reddit.com/r/MachineLearning/comments/dv4b17/quantum_machine_learning/,aiforworld2,1573529943,[removed],0,1
551,2019-11-12,2019,11,12,12,dv4c5g,Python Numbers - Further Exploration,https://www.reddit.com/r/MachineLearning/comments/dv4c5g/python_numbers_further_exploration/,iramirsina,1573530111,,0,1
552,2019-11-12,2019,11,12,13,dv4thx,This Entire Article was Written by Open AI's GPT-2,https://www.reddit.com/r/MachineLearning/comments/dv4thx/this_entire_article_was_written_by_open_ais_gpt2/,LimarcAmbalina,1573532670,[removed],0,1
553,2019-11-12,2019,11,12,13,dv4u67,[D] AI article generator released to the public...eerily accurate,https://www.reddit.com/r/MachineLearning/comments/dv4u67/d_ai_article_generator_released_to_the/,VlanzingManz,1573532760,"When I fed it ""**I HATE HUMANS!!! ROBOTS WILL OVERCOME!! THE END IS NEAR!!!""**

It outputted an article that stated the following (and I have not tampered with it one bit):

&amp;#x200B;

  
The only way we can stop the robots is by getting rid of all the humans...the sooner the better. If you think I'm joking, try going through the movies and seeing the human characters kill each other all the time. I mean they'll kill you! And the movie's the same. The robots will win. The human race is dying out and they're not going to be around to defend themselves.  


The robots are going to kill us. It's only a matter of time. They've already been outed in some movies. I'm just saying that there is no chance to stop them. They're going to come for us no matter what. And even if they get through and don't kill us, they're going to take our jobs. 

&amp;#x200B;

Try it out for yourselves [here](https://talktotransformer.com/)",8,1
554,2019-11-12,2019,11,12,14,dv57xn,Experiments Cheat in ICCV 2019 Pedestrain Detection Paper,https://www.reddit.com/r/MachineLearning/comments/dv57xn/experiments_cheat_in_iccv_2019_pedestrain/,ZXZZXXZZZXXX,1573534980,[removed],0,1
555,2019-11-12,2019,11,12,14,dv5axp,"[N] Hikvision marketed ML surveillance camera that automatically identifies Uyghurs, on its China website",https://www.reddit.com/r/MachineLearning/comments/dv5axp/n_hikvision_marketed_ml_surveillance_camera_that/,sensetime,1573535449,"News Article: https://ipvm.com/reports/hikvision-uyghur

h/t [James Vincent](https://twitter.com/jjvincent/status/1193935124582322182) who regularly reports about ML in The Verge.

The [article](https://ipvm.com/reports/hikvision-uyghur) contains a marketing image from Hikvision, the world's largest security camera company, that speaks volumes about the brutal simplicity of the techno-surveillance state.

The product feature is simple: Han , Uyghur 

Hikvision is a regular sponsor of top ML conferences such as CVPR and ICCV, and have reportedly recruited research interns for their US-based research lab using [job posting](https://eccv2018.org/jobs/research-internship/) in ECCV. They have recently been added to a US government [blacklist](https://www.bloomberg.com/news/articles/2019-10-07/u-s-blacklists-eight-chinese-companies-including-hikvision-k1gvpq77), among other companies such as Shenzhen-based Dahua, Beijing-based Megvii (Face++) and Hong Kong-based Sensetime over human rights violation.

Should research conferences continue to allow these companies to sponsor booths at the events that can be used for recruiting?

https://ipvm.com/reports/hikvision-uyghur

(N.B. no, I *don't* work at Sensetime :)",111,1
556,2019-11-12,2019,11,12,15,dv5tdc,Google Analytics Vs Adobe Analytics Vs IBM Coremetrics,https://www.reddit.com/r/MachineLearning/comments/dv5tdc/google_analytics_vs_adobe_analytics_vs_ibm/,Countants123,1573538502,,0,1
557,2019-11-12,2019,11,12,15,dv614h,[D] Tuning of generated synthetic data for instance segmentation,https://www.reddit.com/r/MachineLearning/comments/dv614h/d_tuning_of_generated_synthetic_data_for_instance/,good_rice,1573539749,"I'm currently training a Mask-RCNN model through synthetic images ""generated"" through a procedure similar to the ""[Cut, Paste and Learn](https://arxiv.org/abs/1708.01642)"" paper. In a nutshell, this paper just randomly pastes crops of objects over backgrounds, with pretty standard augmentation for the crops themselves and Gaussian / Poisson blending for pasting. The resulting images contain all the objects with perfect masks and bounding box labels, over some arbitrary backgrounds. 

However, the generated training data still looks fairly different from real images. I do, however, have a large dataset of unlabeled real images with the real objects in them. Would anyone be aware of a method for tuning a generated image to look more similar to the images in the real dataset? I would want to preserve spatial information so as to not invalidate generated labels, but also add noise / shadows / pixel artifacts in a meaningful way that resembles those found in my real dataset. 

My first thought was to look for papers using something like auto-encoders, but I was flooded with papers about VAEs and end-to-end generation. Is anyone aware of research for this specific problem?",4,1
558,2019-11-12,2019,11,12,15,dv691d,How does scoring parameter in cross_val_score affects hyperopt fmin function score?,https://www.reddit.com/r/MachineLearning/comments/dv691d/how_does_scoring_parameter_in_cross_val_score/,MrDominikku,1573541076,[removed],0,1
559,2019-11-12,2019,11,12,16,dv6i17,[D] Saw on LinkedIn and got a laugh,https://www.reddit.com/r/MachineLearning/comments/dv6i17/d_saw_on_linkedin_and_got_a_laugh/,NTGuardian,1573542642,,0,1
560,2019-11-12,2019,11,12,16,dv6py0,[P] Simple PyTorch implementation of Auto-regressive Language Model on Wikipedia text,https://www.reddit.com/r/MachineLearning/comments/dv6py0/p_simple_pytorch_implementation_of_autoregressive/,lyeoni,1573544090,"A step-by-step tutorial on how to implement and adapt recurrent language model to Wikipedia text.

A  pre-trained BERT, XLNET is publicly available ! But, for NLP beginners,  like me, It could be hard to use/adapt after full understanding. For  them, I covered whole, end-to-end implementation process for language  modeling, using recurrent network, we already know. + do not use  torchtext !

I hope that this repo can be a good solution for people who want to have their own language model :)

[https://github.com/lyeoni/pretraining-for-language-understanding](https://github.com/lyeoni/pretraining-for-language-understanding)",1,1
561,2019-11-12,2019,11,12,16,dv6r9e,Computer Vision Marketplace &amp; SaaS - New Node.js SDK / API,https://www.reddit.com/r/MachineLearning/comments/dv6r9e/computer_vision_marketplace_saas_new_nodejs_sdk/,CobbleVision,1573544350,[removed],0,1
562,2019-11-12,2019,11,12,18,dv7k1d,What's the most interesting case of ML implementation have you seen/heard of within the last year?,https://www.reddit.com/r/MachineLearning/comments/dv7k1d/whats_the_most_interesting_case_of_ml/,Huspi_sp_z_o_o,1573550021,[removed],7,1
563,2019-11-12,2019,11,12,18,dv7mjw,[D] Machine Learning for Systems,https://www.reddit.com/r/MachineLearning/comments/dv7mjw/d_machine_learning_for_systems/,ASVS_Kartheek,1573550494,"I recently found out about this new inter disciplinary field ML for Systems (not Systems for ML). I could only find CSAIL-MIT and DSAIL labs working on this. Any professors or labs working on this, would be really helpful..",5,1
564,2019-11-12,2019,11,12,18,dv7oyt,Humans are losing their social capabilities due to ease of access.,https://www.reddit.com/r/MachineLearning/comments/dv7oyt/humans_are_losing_their_social_capabilities_due/,zozzle69,1573550962,,0,1
565,2019-11-12,2019,11,12,19,dv85p4,The role of intelligence in biological evolution and why a game engine is the perfect virtual biodome for the evolution of AI,https://www.reddit.com/r/MachineLearning/comments/dv85p4/the_role_of_intelligence_in_biological_evolution/,mto96,1573554220,,1,1
566,2019-11-12,2019,11,12,19,dv8659,Chance based accuracy when training a model on small sets of data,https://www.reddit.com/r/MachineLearning/comments/dv8659/chance_based_accuracy_when_training_a_model_on/,PeepeePoopooloulus,1573554315,[removed],0,1
567,2019-11-12,2019,11,12,19,dv88p0,[Post] Becoming a Machine Learning Engineer,https://www.reddit.com/r/MachineLearning/comments/dv88p0/post_becoming_a_machine_learning_engineer/,cdossman,1573554822,[removed],0,1
568,2019-11-12,2019,11,12,19,dv89zt,Question related to smote,https://www.reddit.com/r/MachineLearning/comments/dv89zt/question_related_to_smote/,Awstrakh,1573555068,[removed],0,1
569,2019-11-12,2019,11,12,19,dv8an2,Business Users and Data Scientists Need Analytics Tools!,https://www.reddit.com/r/MachineLearning/comments/dv8an2/business_users_and_data_scientists_need_analytics/,ElegantMicroWebIndia,1573555199,,0,1
570,2019-11-12,2019,11,12,19,dv8emy,"[R] @poly_ai has released ConveRT, a pretrained sentence encoder optimized for conversational understanding. It's under 60MB, is substantially faster &amp; cheaper to train than standard encoder models (BERT etc.), and beats those models on conversational tasks",https://www.reddit.com/r/MachineLearning/comments/dv8emy/r_poly_ai_has_released_convert_a_pretrained/,CaHoop,1573555988,,0,1
571,2019-11-12,2019,11,12,19,dv8g1z,Alexa Research Paper Shows Genetic Algorithms Offer Best Solution for Neural Network Optimization,https://www.reddit.com/r/MachineLearning/comments/dv8g1z/alexa_research_paper_shows_genetic_algorithms/,NuEd_Fernandes,1573556266,,0,1
572,2019-11-12,2019,11,12,20,dv8kfa,LibriSpeech - torrent?,https://www.reddit.com/r/MachineLearning/comments/dv8kfa/librispeech_torrent/,aviniumau,1573557099,[removed],0,1
573,2019-11-12,2019,11,12,20,dv8ogm,This is science,https://www.reddit.com/r/MachineLearning/comments/dv8ogm/this_is_science/,kevin_David_k,1573557836,,0,1
574,2019-11-12,2019,11,12,20,dv8roz,Viewing jupyter notebooks on Github,https://www.reddit.com/r/MachineLearning/comments/dv8roz/viewing_jupyter_notebooks_on_github/,donXscott,1573558416,[removed],0,1
575,2019-11-12,2019,11,12,20,dv8uks,Automated Negotiation for your Digitized Seller,https://www.reddit.com/r/MachineLearning/comments/dv8uks/automated_negotiation_for_your_digitized_seller/,gfrison,1573558925,,0,1
576,2019-11-12,2019,11,12,20,dv8wd0,I asked GPT-2 (AI Transformer) to finish the sentence The end of the world and it came with this....,https://www.reddit.com/r/MachineLearning/comments/dv8wd0/i_asked_gpt2_ai_transformer_to_finish_the/,WhatTheHomePod,1573559248,[removed],0,1
577,2019-11-12,2019,11,12,20,dv8x2j,See how real Trump's response would be,https://www.reddit.com/r/MachineLearning/comments/dv8x2j/see_how_real_trumps_response_would_be/,Remloy,1573559365,[removed],0,1
578,2019-11-12,2019,11,12,20,dv8z32,Are there any funded conferences with good hindex?,https://www.reddit.com/r/MachineLearning/comments/dv8z32/are_there_any_funded_conferences_with_good_hindex/,muaz65,1573559724,,0,1
579,2019-11-12,2019,11,12,21,dv9bwh,[Q] Predicting Location of Target Via Gaussian Process,https://www.reddit.com/r/MachineLearning/comments/dv9bwh/q_predicting_location_of_target_via_gaussian/,thingythangabang,1573561840,[removed],0,1
580,2019-11-12,2019,11,12,21,dv9ggz,Open Source Carbon (an idea),https://www.reddit.com/r/MachineLearning/comments/dv9ggz/open_source_carbon_an_idea/,nickcarmont,1573562539,[removed],0,1
581,2019-11-12,2019,11,12,22,dv9q7q,Is there a way to go from ordered list of lemmas back to a declined and concorded setence (Python)?,https://www.reddit.com/r/MachineLearning/comments/dv9q7q/is_there_a_way_to_go_from_ordered_list_of_lemmas/,voyexlakocho,1573563989,[removed],0,1
582,2019-11-12,2019,11,12,22,dv9tnn,[D] Why is this so painful,https://www.reddit.com/r/MachineLearning/comments/dv9tnn/d_why_is_this_so_painful/,Studyr3ddit,1573564515,"Been doing core ML research at a great uni for the past 5 years and I have nothing to show for it. No publications means no funding. Whenever I bring up the topic of publishing, my advisor says theres not enough results which is partially true. Is having low impact publication better than no publication? Can I publish other research as a solo author? I've literally spent the best years of my youth behind this..and I dont understand why it has to be so painful. I'm no genius.. Just hardworking but there are people with neurips, icml publications as first author whilst still in undergrad..how am I supposed to compete against raw natural talent?",136,1
583,2019-11-12,2019,11,12,22,dv9yaw,[P] Deploy A Text Generating API With Hugging Faces DistilGPT-2,https://www.reddit.com/r/MachineLearning/comments/dv9yaw/p_deploy_a_text_generating_api_with_hugging_faces/,KindaKnowKarate,1573565193,"[https://towardsdatascience.com/deploy-a-text-generating-api-with-hugging-faces-distilgpt-2-9791b9f356f9](https://towardsdatascience.com/deploy-a-text-generating-api-with-hugging-faces-distilgpt-2-9791b9f356f9)

A step-by-step tutorial for deploying Hugging Face's NLP models as web APIs on AWS. Uses open source tools and covers many more advanced infrastructure challenges (autoscaling, prediction monitoring, rolling updates, etc.)",0,1
584,2019-11-12,2019,11,12,23,dvatpy,How Business Intelligence is different from Data Science,https://www.reddit.com/r/MachineLearning/comments/dvatpy/how_business_intelligence_is_different_from_data/,weihong95,1573569534,"""Is Business Intelligence really different from data science? Both are using data to solve the problem or create value. Isn't that the same?""

This is one of the questions often asked by people around me and I would like to share my own perspective through the experience I have had.

Most importantly, if you are still an undergraduate, or just graduated, or still considering to switch your job career to business intelligence, read this article before making your final decision.

Comment below on how you think data science is different from business intelligence!

Link: [https://towardsdatascience.com/how-business-intelligence-is-different-from-data-science-f1673456b80c?source=friends\_link&amp;sk=e6d19d387c23649f2df8074ad97752f5](https://towardsdatascience.com/how-business-intelligence-is-different-from-data-science-f1673456b80c?source=friends_link&amp;sk=e6d19d387c23649f2df8074ad97752f5)",0,1
585,2019-11-12,2019,11,12,23,dvauxx,What Artificial Intelligence Can Do For Retail Industry?,https://www.reddit.com/r/MachineLearning/comments/dvauxx/what_artificial_intelligence_can_do_for_retail/,mantha_anirudh,1573569683,,0,1
586,2019-11-12,2019,11,12,23,dvavp5,[P] Refit existing Spark ML PipelineModel with new data,https://www.reddit.com/r/MachineLearning/comments/dvavp5/p_refit_existing_spark_ml_pipelinemodel_with_new/,Hakuhun,1573569781,"Hi! 

I'd like to refit an **alerady existing PipeLineModel** in my project from microbatch to microbatch. 

I curently use a  **DecisionTreeRegressor.** I load back the previusly used PipelineModel, set it's stages to a pipeline and use that pipeline to refit with new data, but as I can understand my solution only saves the latest model. 

[I enclose my github repo/model for the easyer understanding.](https://github.com/Hakuhun/bkk-data-process-spark/blob/master/src/main/scala/hu/oe/bakonyi/bkk/BkkDataDeserializer.scala)

Does **Spark** ***Structured*** **Streaming** capable of streaming learning? Is it possible to refit my already fitted model with new data? 

Do I have to use the **RDD** based Spark Streaming with  **StreamingLinearRegressionWithSGD**?",0,1
587,2019-11-12,2019,11,12,23,dvb1cn,Analyzing Road Traffic with OpenCV,https://www.reddit.com/r/MachineLearning/comments/dvb1cn/analyzing_road_traffic_with_opencv/,creotiv,1573570515,,0,1
588,2019-11-13,2019,11,13,0,dvbf7o,Causal Machine Learning,https://www.reddit.com/r/MachineLearning/comments/dvbf7o/causal_machine_learning/,divyat09,1573572237,[removed],0,1
589,2019-11-13,2019,11,13,0,dvbjcg,Predicting Reddit upvotes,https://www.reddit.com/r/MachineLearning/comments/dvbjcg/predicting_reddit_upvotes/,jaku1000000,1573572756,[removed],0,1
590,2019-11-13,2019,11,13,0,dvbleq,[R] Top Machine Learning Journal Publications,https://www.reddit.com/r/MachineLearning/comments/dvbleq/r_top_machine_learning_journal_publications/,Linga2020,1573573009,,0,1
591,2019-11-13,2019,11,13,1,dvc4q6,[P] Traffic Analysis in Original Video Data,https://www.reddit.com/r/MachineLearning/comments/dvc4q6/p_traffic_analysis_in_original_video_data/,ido90,1573575366,"Half a year ago, when I lived in an apartment with a view of Ayalon Road in Tel Aviv, I decided I couldn't just let the data pass indifferently beneath my window.

I used my smartphone camera to record 81 short videos of the traffic; built a dedicated CNN to detect the vehicles (after the small, crowded cars in the videos were failed to be detected by several out-of-the-box networks) and trained it on a few manually-tagged video-frames; modified SORT algorithm to allow tracking a vehicle even when it does not overlap itself over adjacent frames (required due to particularly low videos frame-rate); and derived several insights from the resulted data, mainly regarding lane-transitions and the relations between density, speed and flux.

I believe that this nicely demonstrates the amounts of data surrounding us, and their accessibility using as trivial tools as a smartphone.

Any comments, questions and insights are welcome :)

A small demonstration is attached in the form of an unpolished poster.

For more details please visit the repo's readme:

[https://github.com/ido90/AyalonRoad](https://github.com/ido90/AyalonRoad)",1,1
592,2019-11-13,2019,11,13,1,dvcccc,The Visual Task Adaptation Benchmark,https://www.reddit.com/r/MachineLearning/comments/dvcccc/the_visual_task_adaptation_benchmark/,sjoerdapp,1573576273,,0,1
593,2019-11-13,2019,11,13,1,dvchiq,Loan Prediction Project using Machine Learning in Python,https://www.reddit.com/r/MachineLearning/comments/dvchiq/loan_prediction_project_using_machine_learning_in/,saruque,1573576871,,0,1
594,2019-11-13,2019,11,13,1,dvcpi6,[D] History of NLP: How Andrey Markov and Claude Shannon Made Language Models for Text Generation,https://www.reddit.com/r/MachineLearning/comments/dvcpi6/d_history_of_nlp_how_andrey_markov_and_claude/,newsbeagle,1573577802,"These were painstaking procedures. Markov counted vowels and consonants in a Russian novel to make probability rules for which letter would appear next. 30 years later, Shannon took the idea a step further by creating statistical models of language and generating text according to the rules he devised. Shannon's first output was ""OCRO HLI RGWR,"" but his model did get a bit better. 

[https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/andrey-markov-and-claude-shannon-built-the-first-language-generation-models](https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/andrey-markov-and-claude-shannon-built-the-first-language-generation-models)",0,1
595,2019-11-13,2019,11,13,2,dvcrje,"Tensorflow vs Pytorch, production and application in industry",https://www.reddit.com/r/MachineLearning/comments/dvcrje/tensorflow_vs_pytorch_production_and_application/,AndrejSa,1573578044,[removed],0,1
596,2019-11-13,2019,11,13,2,dvcu33,What are the mathematics of these equations?,https://www.reddit.com/r/MachineLearning/comments/dvcu33/what_are_the_mathematics_of_these_equations/,Chicken_Chowman,1573578325,,2,1
597,2019-11-13,2019,11,13,2,dvcxva,[N] Free 100$ GCP credits,https://www.reddit.com/r/MachineLearning/comments/dvcxva/n_free_100_gcp_credits/,paubric,1573578765,"So I went to this hackathon and got some coupons, but apparently only one per account works, and they expire in a week or so, so there you go: https://imgur.com/a/38v7kiy",11,1
598,2019-11-13,2019,11,13,2,dvcyzu,[D] AI clones voice from 5 second recording,https://www.reddit.com/r/MachineLearning/comments/dvcyzu/d_ai_clones_voice_from_5_second_recording/,OverLordGoldDragon,1573578896,"[TwoMinutePapers clip](https://www.youtube.com/watch?v=0sR1rU3gLzQ) \-- [Research paper](https://arxiv.org/abs/1806.04558)

Published in 2018, and I've yet to see any fuss made over this - as opposed to image face recognition. Isn't it a big deal? Evidence fabrication can be taken to a whole new level - in criminal, political, personal, and other domains.

On another note, combined with [GPT2](https://talktotransformer.com/), and some of the face simulation methods, shouldn't one be able to make a fairly convincing conversational AI? If GPT2 can somehow be configured to turn off lecture-mode and shorten responses, it might be fully convincing to those who don't look much into AI.",69,1
599,2019-11-13,2019,11,13,2,dvd1ko,[D] Can someone explain the proof which converts the splitting of decision tree categorical variables from O(2^n-1) to O(n)?,https://www.reddit.com/r/MachineLearning/comments/dvd1ko/d_can_someone_explain_the_proof_which_converts/,korokage,1573579214,,2,1
600,2019-11-13,2019,11,13,2,dvdgg5,Are there any good papers on using neural nets for sorting?,https://www.reddit.com/r/MachineLearning/comments/dvdgg5/are_there_any_good_papers_on_using_neural_nets/,Frumpagumpus,1573580931,[removed],0,1
601,2019-11-13,2019,11,13,3,dvdvl0,: I am totally out of picture - is anybody trying to make some ML algorithms using FRP ?,https://www.reddit.com/r/MachineLearning/comments/dvdvl0/i_am_totally_out_of_picture_is_anybody_trying_to/,yetanothernormalG,1573582615,[removed],0,1
602,2019-11-13,2019,11,13,3,dvdzya,[D] Are there any papers that do Birds Eye View Pose Estimation?,https://www.reddit.com/r/MachineLearning/comments/dvdzya/d_are_there_any_papers_that_do_birds_eye_view/,soulslicer0,1573583120,Possibily in the context of self-driving etc.,13,1
603,2019-11-13,2019,11,13,4,dvekue,"Hallo everybody, sorry for my bad English. I'm Italian boy and I love sport betting. I wish to know if is possible to associate betting with AI (artificial intelligence) and to help for predictions?",https://www.reddit.com/r/MachineLearning/comments/dvekue/hallo_everybody_sorry_for_my_bad_english_im/,manuelneri,1573585520,[removed],0,1
604,2019-11-13,2019,11,13,4,dvetke,Texas A&amp;M and Simon Fraser Universities Open-Source RL Toolkit for Card Games,https://www.reddit.com/r/MachineLearning/comments/dvetke/texas_am_and_simon_fraser_universities_opensource/,Yuqing7,1573586539,,0,1
605,2019-11-13,2019,11,13,4,dvexpm,[D] What are the most reputable VC firms / Accelerators for machine learning startups?,https://www.reddit.com/r/MachineLearning/comments/dvexpm/d_what_are_the_most_reputable_vc_firms/,AdditionalWay,1573586998,I'm looking to join a startup. I curious to know what are the most reputable funding companies in this area?,5,1
606,2019-11-13,2019,11,13,5,dvfoqr,"[D] AI Coverage Best Practices, According to AI Researchers",https://www.reddit.com/r/MachineLearning/comments/dvfoqr/d_ai_coverage_best_practices_according_to_ai/,regalalgorithm,1573590099,"Hey all, want to share [an article I wrote (AI Coverage Best Practices, According to AI Researchers)](https://www.skynettoday.com/editorials/ai-coverage-best-practices) with help from several other AI PhDs and after polling AI twitter.   


Bemoaning AI hype/bad coverage seems pretty popular among AI researchers both on Twitter and on here (there was this recent ['I'm so sick of the hype'](https://www.reddit.com/r/MachineLearning/comments/donbz7/d_im_so_sick_of_the_hype/) discussion), so on top of the overall Skynet Today effort this article in particular is something we put together to try and help with that. Of course the core issue is incentives etc. , but still it's better to try and have this as a resource to point people to when criticizing I think?  


Do you think there are any best practices we did not include, or any we did that are questionable?",1,1
607,2019-11-13,2019,11,13,5,dvfrvp,Ask HN: What is your ML stack like?,https://www.reddit.com/r/MachineLearning/comments/dvfrvp/ask_hn_what_is_your_ml_stack_like/,HN_Crosspost_Bot,1573590446,,0,1
608,2019-11-13,2019,11,13,5,dvftuj,New Insights into Human Mobility with Privacy Preserving Aggregation,https://www.reddit.com/r/MachineLearning/comments/dvftuj/new_insights_into_human_mobility_with_privacy/,sjoerdapp,1573590670,,0,1
609,2019-11-13,2019,11,13,5,dvfunz,Automatic Differentiation Step by Step,https://www.reddit.com/r/MachineLearning/comments/dvfunz/automatic_differentiation_step_by_step/,formalsystem,1573590759,,0,1
610,2019-11-13,2019,11,13,5,dvfz5s,Machine learning in a nutshell,https://www.reddit.com/r/MachineLearning/comments/dvfz5s/machine_learning_in_a_nutshell/,pvsa,1573591262,,0,1
611,2019-11-13,2019,11,13,5,dvg5dm,[Discussion] The humanitarian food crisis solved by three dimensional printing,https://www.reddit.com/r/MachineLearning/comments/dvg5dm/discussion_the_humanitarian_food_crisis_solved_by/,zozzle69,1573591959,,0,1
612,2019-11-13,2019,11,13,6,dvgdjo,[R] Picking groups instead of samples: A close look at Static Pool-based Meta-Active Learning,https://www.reddit.com/r/MachineLearning/comments/dvgdjo/r_picking_groups_instead_of_samples_a_close_look/,MrLeylo,1573592794,,2,1
613,2019-11-13,2019,11,13,6,dvgfsw,Machine Learning Engineer,https://www.reddit.com/r/MachineLearning/comments/dvgfsw/machine_learning_engineer/,9_5B-Lo-9_m35iih7358,1573593018,"Hey guys, I want to become a top-notch Machine Learning Engineer.

Currently an undergrad in Mathematical Finance (Python, R, SQL). Do you think taking a Master in Data Science (and taking as many ML, DL courses as possible) is going to be enough? 

I heard the best ML engineers are mathematicians.",0,1
614,2019-11-13,2019,11,13,6,dvggp6,Implementing ML Infrastructure,https://www.reddit.com/r/MachineLearning/comments/dvggp6/implementing_ml_infrastructure/,cassini12,1573593112,"Just Curious for some input.. Have about 4 months to implement an Infrastructure for a small/mid sized company doing ML/DL on a daily basis. We currently have one GPU box in total with 8- 1050ti in it but need to expand drastically including data storage and some form of Queue and container scenario.

Assuming we will use a few Frameworks such as MX,Pytorch, Tensoflow and follow basic guidelines of Docker/Kubernetes or TensorRT .

 My question is really what do people use for researching the actual infrastructure for Data to GPU setup and interfaces for speed in between the data storage and computing power machines?  Multiple users will need to use this Computenode as well.

Any advice on actual hardware welcome, be it Bizon,NVIDIA, Lambda Labs etc/ Thanks",0,1
615,2019-11-13,2019,11,13,6,dvh8e8,[1911.04252] Self-training with Noisy Student improves ImageNet classification,https://www.reddit.com/r/MachineLearning/comments/dvh8e8/191104252_selftraining_with_noisy_student/,brettkoonce,1573595940,,17,1
616,2019-11-13,2019,11,13,7,dvhjbq,[D] AMD vs Nvidia GPU,https://www.reddit.com/r/MachineLearning/comments/dvhjbq/d_amd_vs_nvidia_gpu/,MoistNotWet,1573597145,"Hi.

I've recently built a new PC with a 5700 xt AMD gpu, and I'm wondering whether or not I should replace it with an RTX 2070 super. 

From what I've seen, AMD with ROCm doesn't have much support when it comes to deep learning projects, and while Nvidia gpus are generally leading in this area with stronger support for libraries like Tensorflow,  I can't find much info on the 2070 super or the SUPER cards in general when it comes to ML.

By no means will I be running anything too intensive any time soon, since I'm just a beginner; however, down the line I definitely see myself getting into machine learning and I'd hope to have the right hardware to start. (I also heard AWS or other services like it are good options, but I've already built a PC anyway.)

Any advice on the matter would be appreciated!",33,1
617,2019-11-13,2019,11,13,7,dvhjcl,[AI application with source code] Let your machine play Street Fighter!,https://www.reddit.com/r/MachineLearning/comments/dvhjcl/ai_application_with_source_code_let_your_machine/,1991viet,1573597148,,1,1
618,2019-11-13,2019,11,13,8,dvimjp,[D] Neural Differential Equations,https://www.reddit.com/r/MachineLearning/comments/dvimjp/d_neural_differential_equations/,bthi,1573601532,"I had a question about these. I know that you calculate the whole network in one go and then you just evaluate it at some points along the the depth. However, I was wondering how the parameters work. 

1) How are the weights and biases updated? I know they are ""shared"" through the whole network (and hence less parameters than the usual network) however, how do the individual evaluations work then? For the network. Like say the network is defined from t = 0 to t = 5, and I evaluate at t = 1 and t = 2; are the weights the same here and the only thing that changes is t? And if so, what's the point even? Why not evaluate just at the end point (i.e. the maximum depth you want) ?

2) Going off of that, what is the point of those in between evaluations if the parameters are shared anyway? Wouldn't they be updated the same way every time? Or is it that.. multiple evaluations means that the derivatives and the updates are ""better""?

I'm just really confused about this whole shared parameters thing. Please help!",11,1
619,2019-11-13,2019,11,13,8,dvimo5,"[D] How to Build a Silent, Multi-GPU Water-Cooled Deep-Learning Rig for under $10k",https://www.reddit.com/r/MachineLearning/comments/dvimo5/d_how_to_build_a_silent_multigpu_watercooled/,mooneymark201,1573601550,,0,1
620,2019-11-13,2019,11,13,9,dvjege,Any tools to embed image file to fixed column in dataset (.csv)?,https://www.reddit.com/r/MachineLearning/comments/dvjege/any_tools_to_embed_image_file_to_fixed_column_in/,khalidmuzappa,1573604888,"Hello good people of machine learning-land,
Is there any tool to embed image into 1 row and fixed column dataset (preferably csv)? 
Thank you",0,1
621,2019-11-13,2019,11,13,10,dvk2ca,[D] Deep-Learning in the Cloud vs. On-Premises Machines,https://www.reddit.com/r/MachineLearning/comments/dvk2ca/d_deeplearning_in_the_cloud_vs_onpremises_machines/,mooneymark201,1573607872,,0,1
622,2019-11-13,2019,11,13,12,dvlk69,How do large enterprises put AI and ML to effective use in network security?,https://www.reddit.com/r/MachineLearning/comments/dvlk69/how_do_large_enterprises_put_ai_and_ml_to/,Mathster0598,1573615054,,0,1
623,2019-11-13,2019,11,13,12,dvlr5z,SIGNIFICANCE OF DATA TRANSFORMATION IN MACHINE LEARNING,https://www.reddit.com/r/MachineLearning/comments/dvlr5z/significance_of_data_transformation_in_machine/,analyticsinsight,1573616025,,0,1
624,2019-11-13,2019,11,13,12,dvls0r,[D] Why does Deep Learning receive way more attention than Reinforcement Learning?,https://www.reddit.com/r/MachineLearning/comments/dvls0r/d_why_does_deep_learning_receive_way_more/,mkiisa,1573616136,[removed],0,1
625,2019-11-13,2019,11,13,12,dvlzoo,Cartoning machine for toothpaste &amp; ointment cartoner packaging machine,https://www.reddit.com/r/MachineLearning/comments/dvlzoo/cartoning_machine_for_toothpaste_ointment/,Jochamp-Machinery,1573617256,,0,1
626,2019-11-13,2019,11,13,13,dvmlpl,Sorting As A Service,https://www.reddit.com/r/MachineLearning/comments/dvmlpl/sorting_as_a_service/,rahul101chowdhury,1573620537,[removed],0,1
627,2019-11-13,2019,11,13,13,dvmn1s,Two arrays in one file. How to separate using numpy?,https://www.reddit.com/r/MachineLearning/comments/dvmn1s/two_arrays_in_one_file_how_to_separate_using_numpy/,xinwow,1573620731,,0,1
628,2019-11-13,2019,11,13,16,dvnzqo,Which epoch to use when YOLOv2 never overfits,https://www.reddit.com/r/MachineLearning/comments/dvnzqo/which_epoch_to_use_when_yolov2_never_overfits/,spasserkongen,1573628650,[removed],1,1
629,2019-11-13,2019,11,13,16,dvodqz,AI: Whats New and whats Not,https://www.reddit.com/r/MachineLearning/comments/dvodqz/ai_whats_new_and_whats_not/,venkatvajradhar,1573631045,[removed],0,1
630,2019-11-13,2019,11,13,16,dvof4s,[P] What is the best way to read data in batches from a datastore?,https://www.reddit.com/r/MachineLearning/comments/dvof4s/p_what_is_the_best_way_to_read_data_in_batches/,MrDoOO,1573631292,I'm training a PyTorch model on data stored in Google BigQuery (basically a SQL like database). What's the best way to fetch data in batches for training a model such that I don't bottleneck the training process. Querying too many times is slow and the dataset is way too large to fit into memory. Any best practices here or existing tools for this?,10,1
631,2019-11-13,2019,11,13,18,dvp7ud,Best SMT Nozzle Suppliers &amp; Spare Parts at Affordable,https://www.reddit.com/r/MachineLearning/comments/dvp7ud/best_smt_nozzle_suppliers_spare_parts_at/,smthelp1,1573636529,,0,1
632,2019-11-13,2019,11,13,18,dvpm9g,What are some good online courses/books for learning machine learning maths?,https://www.reddit.com/r/MachineLearning/comments/dvpm9g/what_are_some_good_online_coursesbooks_for/,PagalProgrammer,1573639169,,0,1
633,2019-11-13,2019,11,13,19,dvprs1,Can Artificial Intelligence reveal the Origins of the Universe?,https://www.reddit.com/r/MachineLearning/comments/dvprs1/can_artificial_intelligence_reveal_the_origins_of/,rangilorajasthan,1573640109,,0,1
634,2019-11-13,2019,11,13,19,dvq1ji,[R] ConveRT: Efficient and Accurate Conversational Representations from Transformers,https://www.reddit.com/r/MachineLearning/comments/dvq1ji/r_convert_efficient_and_accurate_conversational/,CaHoop,1573641836,,7,1
635,2019-11-13,2019,11,13,20,dvq8mv,Usage of SMOTE(),https://www.reddit.com/r/MachineLearning/comments/dvq8mv/usage_of_smote/,Awstrakh,1573643139,[removed],0,1
636,2019-11-13,2019,11,13,20,dvqmit,What is open set recognition?,https://www.reddit.com/r/MachineLearning/comments/dvqmit/what_is_open_set_recognition/,TheRochVoices,1573645669,"I read that open set means the classes that the classifier has not seen during training time but can come during testing. What does this definition of ""class"" means here. 

If i have a trained a Dog-Cat classifier, what will mean that my classifier handles open set? Or that it doesn't?",0,1
637,2019-11-13,2019,11,13,20,dvqqc3,[D] What does it mean that my classifier handles open set?,https://www.reddit.com/r/MachineLearning/comments/dvqqc3/d_what_does_it_mean_that_my_classifier_handles/,TheRochVoices,1573646329,"Suppose I have trained a Cat-Dog classifier, open set handling means it can recognise classes that it has not seen while training. What classes are we talking about here? Different breeds of Dogs/Cats or what?",8,1
638,2019-11-13,2019,11,13,21,dvqtiq,[D] Internships for next summer in Europe,https://www.reddit.com/r/MachineLearning/comments/dvqtiq/d_internships_for_next_summer_in_europe/,rikkajounin,1573646833,"Hi all!

I'm a second year PhD student in machine learning and I was looking for an Internship possibly in Europe to apply for next summer, 3/4 months ideally. 

My lab does mainly theoretical work and started doing a bit of deep learning just 3 years ago. When I started i was more focused on the experiments (deep learning) and now i am shifting towards the theoretical side due to the lab and supervisor expertise. Since i like both aspects of machine learning I though that an internship could be a good opportunity to make more impactful experimental work and also make some connections.

I only have 2  (ICML/Neurips) publication as a co-author. Do you think something like Deepmind in London could already be out of reach for me? Do you know about some nice internships program that could suit me better?",0,1
639,2019-11-13,2019,11,13,21,dvqvzv,[D] Internships for next year in Europe,https://www.reddit.com/r/MachineLearning/comments/dvqvzv/d_internships_for_next_year_in_europe/,rikkajounin,1573647203,[removed],0,1
640,2019-11-13,2019,11,13,21,dvr50q,[D] Laptop computer for deep learning,https://www.reddit.com/r/MachineLearning/comments/dvr50q/d_laptop_computer_for_deep_learning/,joakim_ogren,1573648537,"Is the performance from a laptop with Intel Core i9-9980HK with NVIDIA Quadro RTX 3000 good or would it never be able to compete with a good desktop/tower computer with RTX 2080 Ti?

Either I need a super good laptop or combo of Desktop and laptop.  Using TensorFlow.",17,1
641,2019-11-13,2019,11,13,21,dvrbub,How can I submit a camera ready version of AAAI 2020 paper?,https://www.reddit.com/r/MachineLearning/comments/dvrbub/how_can_i_submit_a_camera_ready_version_of_aaai/,nwj0612,1573649559,[removed],0,1
642,2019-11-13,2019,11,13,22,dvrhqn,Need help about OCR (Optical Character Recognition) on python,https://www.reddit.com/r/MachineLearning/comments/dvrhqn/need_help_about_ocr_optical_character_recognition/,MadEngineerBR,1573650371,[removed],0,1
643,2019-11-13,2019,11,13,22,dvrjzl,"ELI5: I was studying about ARTIFICIAL NEURAL NETWORK when i came across a layer, called the hidden box, where a specific node has a bias number. Couldn't wrap my head around what that number is supposed to do?",https://www.reddit.com/r/MachineLearning/comments/dvrjzl/eli5_i_was_studying_about_artificial_neural/,AfzalOzil360,1573650695,[removed],0,1
644,2019-11-13,2019,11,13,22,dvrrf5,Vearch v0.2 is out!,https://www.reddit.com/r/MachineLearning/comments/dvrrf5/vearch_v02_is_out/,certiser,1573651721,[removed],0,1
645,2019-11-13,2019,11,13,23,dvsbaa,[R] Europarl-ST: A Multilingual Corpus For Speech Translation Of Parliamentary Debates,https://www.reddit.com/r/MachineLearning/comments/dvsbaa/r_europarlst_a_multilingual_corpus_for_speech/,IranzoSanchez,1573654352,,4,1
646,2019-11-13,2019,11,13,23,dvsdhb,Python machone learning,https://www.reddit.com/r/MachineLearning/comments/dvsdhb/python_machone_learning/,bigtower57,1573654642,[removed],0,1
647,2019-11-13,2019,11,13,23,dvsw27, L'Amour au bordel / Love at the brothel (2019),https://www.reddit.com/r/MachineLearning/comments/dvsw27/lamour_au_bordel_love_at_the_brothel_2019/,benjaminbardou,1573656987,"&amp;#x200B;

*Processing img c4lleb1fwgy31...*

&amp;#x200B;

https://preview.redd.it/4pgvj31fwgy31.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=21306246c4538a6b1883a92a9830a454de443bf3

&amp;#x200B;

https://preview.redd.it/piv64b2fwgy31.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=a401dba06cca1d2ba494e31a2ad2a98d45ddc9dd

&amp;#x200B;

https://preview.redd.it/g9y70c1fwgy31.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=fc079faedc9d2b48a179a15b5d5cf80db9e47d0e

&amp;#x200B;

https://preview.redd.it/l0777a1fwgy31.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=f32e87468e0351d3e4101d0aa1e9521a1ae24777

&amp;#x200B;

https://preview.redd.it/osht731fwgy31.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=b8294f235ab2273fa27bbe33a14ab39cdea3c21f

&amp;#x200B;

https://preview.redd.it/6hkqo91fwgy31.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=cd8f2f8b9b2b8bf28f8da9926189601907f3fa17

&amp;#x200B;

https://preview.redd.it/752y451fwgy31.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=17e24508928d2226a5e0714837d097404590d1bb

It's been a while since I wanted to try machine learning.  
After several tests to see how it worked, I tried to produce an image as if I had done it in a traditional way (drawing, modeling, rendering, paintover) but this time I would start from a machine learning algorithm.

To do this, I used the AttnGAN image synthesis algorithm that transforms text into an image.  
I really like Baudelaire's poem ""Voyage"" because it prefigures as early as 1859 an idea of the cinema:  
""To pass over our minds, stretched like a canvas,  
Your memories with their frames of horizons. ""

So I used these verse to generate an image.  
Since this sketch, I reworked the masses, the contrasts and the colorimetry by paintover as if I had done it normally from a rough drawing.

The image that results from this work is entitled ""Love to the brothel"". Do not ask me why. Another algorithm could very well have found this title. :)  
For me, it evokes the surrealists, Degas, Baudelaire and ... the cinema. ;)

[\#machinelearning](https://www.facebook.com/hashtag/machinelearning?source=feed_text&amp;epa=HASHTAG) [\#digitalpainting](https://www.facebook.com/hashtag/digitalpainting?source=feed_text&amp;epa=HASHTAG) [\#GAN](https://www.facebook.com/hashtag/gan?source=feed_text&amp;epa=HASHTAG) [\#AttnGAN](https://www.facebook.com/hashtag/attngan?source=feed_text&amp;epa=HASHTAG)",0,1
648,2019-11-14,2019,11,14,0,dvt6f8,Looking for like minded people who are committed to learn and advance the state of AI,https://www.reddit.com/r/MachineLearning/comments/dvt6f8/looking_for_like_minded_people_who_are_committed/,GetABroom69,1573658261,[removed],0,1
649,2019-11-14,2019,11,14,0,dvta1k,[R] Finding a human-like classifier,https://www.reddit.com/r/MachineLearning/comments/dvta1k/r_finding_a_humanlike_classifier/,hjk92r,1573658695,"**Paper**:  [https://openreview.net/forum?id=BJeGFs9FsH](https://openreview.net/forum?id=BJeGFs9FsH) 

&amp;#x200B;

**Abstract**:

&amp;#x200B;

There were many attempts to explain the trade-off between accuracy and adversarial robustness. However,  there  was  no  clear  understanding  of  the  behaviors  of  a  robust  classifier  which  has human-like robustness.

&amp;#x200B;

We  argue  (1)  why  we  need  to  consider  adversarial  robustness  against  varying  magnitudes  of perturbations not only focusing on a fixed perturbation threshold, (2) why we need to use different method to generate adversarially perturbed samples that can be used to train a robust classifier and measure the robustness of classifiers and (3) why we need to prioritize adversarial accuracies with different magnitudes.

&amp;#x200B;

We introduce Lexicographical Genuine Robustness (LGR) of classifiers that combines the above requirements.  We also suggest a candidate oracle classifier called ""Optimal Lexicographically Genuinely  Robust  Classifier  (OLGRC)""  that  prioritizes  accuracy  on  meaningful  adversarially perturbed  examples  generated  by  smaller  magnitude  perturbations.   The  training  algorithm  for estimating OLGRC requires lexicographical optimization unlike existing adversarial training methods. To apply lexicographical optimization to neural network, we utilize Gradient Episodic Memory (GEM) which was originally developed for continual learning by preventing catastrophic forgetting.

&amp;#x200B;

**TL;DR:** We try to design and train a classifier whose adversarial robustness is more resemblance to robustness of human.",1,1
650,2019-11-14,2019,11,14,0,dvtnxv,"Simple Questions Thread November 13, 2019",https://www.reddit.com/r/MachineLearning/comments/dvtnxv/simple_questions_thread_november_13_2019/,AutoModerator,1573660358,[removed],0,1
651,2019-11-14,2019,11,14,0,dvtolx,[P] `gpt2-client` is now on Buy me a coffee!,https://www.reddit.com/r/MachineLearning/comments/dvtolx/p_gpt2client_is_now_on_buy_me_a_coffee/,rish-16,1573660433,"Hey y'all!

The past couple of days have been awesome. \`gpt2-client\` finally hit 12.5K downloads worldwide and we're growing faster than ever. I've added a **Buy me a coffee!** link to the README in case any of you would donate to the project. Your continued support motivates me to continue building such nifty tools and your donations mean a lot to me!

Again, if you haven't checked gpt2-client out, do visit [https://github.com/rish-16/gpt2client](https://github.com/rish-16/gpt2client) . I'm now accepting feature requests (some have already been incorporated in!). So, feel free to drop me a message down below or on a Feature Requests template on GitHub.

Cheers!",3,1
652,2019-11-14,2019,11,14,1,dvtwcv,Artificial Intelligence: Top 20 Powerful AI Applications,https://www.reddit.com/r/MachineLearning/comments/dvtwcv/artificial_intelligence_top_20_powerful_ai/,mantha_anirudh,1573661293,,0,1
653,2019-11-14,2019,11,14,1,dvtyau,AAAI 2020 Reports Record-High Paper Submissions,https://www.reddit.com/r/MachineLearning/comments/dvtyau/aaai_2020_reports_recordhigh_paper_submissions/,Yuqing7,1573661507,,0,1
654,2019-11-14,2019,11,14,1,dvu7ib,[P] Article: Curiosity through random network distillation with Montezuma's revenge [Deep Reinforcement learning course],https://www.reddit.com/r/MachineLearning/comments/dvu7ib/p_article_curiosity_through_random_network/,cranthir_,1573662615,"Hello everyone,

We've just published the new article of [Deep reinforcement Learning course](https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938) where we study Open AI's Paper ""Exploration by Random Network Distillation

THE ARTICLE: [https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938](https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938)

The bonus is that **we give you a trained model on Montezuma's Revenge during 21hours with 128 parallel environments in a Tesla K80.**

Let **me know what you think about this article.**

PS: For people who follow me and follow Deep Reinforcement Learning Course **I know that I'm totally late on this article** (it was supposed to be published some months ago...) to be totally transparent with you, the publication rate is slow because since March I'm a RL research scientist at Dataiku so I have a lot of things to do. **But stay tuned I'm currently working very hard on updating everything (especially the PR on github) and things will be announced in the 2 next weeks (and yes it will stay totally free and open source ).**

Cheers!",0,1
655,2019-11-14,2019,11,14,1,dvuds2,"METHODOLOGY OF BOX JENKINS VS ARTIFICIAL NEURONAL NETWORKS TO BUILD A FORECAST MODEL OF THE MONTHLY CLOSURE PURCHASE PRICE OF THE PERU OF THE CREDIT BANK OF PERU IN THE LIMA SECURITIES BANK, APRIL 2005 UNTIL FEBRUARY 2018",https://www.reddit.com/r/MachineLearning/comments/dvuds2/methodology_of_box_jenkins_vs_artificial_neuronal/,jeffry_30,1573663356,[removed],0,1
656,2019-11-14,2019,11,14,2,dvupo7,[P] I made an easy to read blog post about MCMC and other sampling techniques commonly used in Bayesian Learning theory. Hope you all will give it a read!,https://www.reddit.com/r/MachineLearning/comments/dvupo7/p_i_made_an_easy_to_read_blog_post_about_mcmc_and/,realist_konark,1573664765,,0,1
657,2019-11-14,2019,11,14,2,dvv5ay,[1910.13051] ROCKET: Exceptionally fast and accurate time series classification using random convolutional kernels,https://www.reddit.com/r/MachineLearning/comments/dvv5ay/191013051_rocket_exceptionally_fast_and_accurate/,jwuphysics,1573666552,,2,1
658,2019-11-14,2019,11,14,2,dvvdo0,[R] Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks,https://www.reddit.com/r/MachineLearning/comments/dvvdo0/r_learning_to_fewshot_learn_across_diverse/,ai_reader,1573667517,"**Paper:** [https://arxiv.org/abs/1911.03863](https://arxiv.org/abs/1911.03863)

**Abstract:** Self-supervised pre-training of transformer models has shown enormous success in improving performance on a number of downstream tasks. However, fine-tuning on a new task still requires large amounts of task-specific labelled data to achieve good performance. We consider this problem of learning to generalize to new tasks with few examples as a meta-learning problem. While meta-learning has shown tremendous progress in recent years, its application is still limited to simulated problems or problems with limited diversity across tasks. We develop a novel method, LEOPARD, which enables optimization-based meta-learning across tasks with different number of classes, and evaluate existing methods on generalization to diverse NLP classification tasks. LEOPARD is trained with the state-of-the-art transformer architecture and shows strong generalization to tasks not seen at all during training, with as few as 8 examples per label. On 16 NLP datasets, across a diverse task-set such as entity typing, relation extraction, natural language inference, sentiment analysis, and several other text categorization tasks, we show that LEOPARD learns better initial parameters for few-shot learning than self-supervised pre-training or multi-task training, outperforming many strong baselines, for example, increasing F1 from 49% to 72%.",2,1
659,2019-11-14,2019,11,14,2,dvvg45,[Project] Deepdos,https://www.reddit.com/r/MachineLearning/comments/dvvg45/project_deepdos/,C3NZ,1573667799,"**Description**

Hello, r/MachineLearning! Over the course of the last 2 months I've been working on my first major machine learning project called, ""Deepdos"" in my free time outside of school and work. Deepdos is a network tool that provides analysis and in the future mitigation of all network traffic coming over whatever network adapter you specify. The analysis utilizes a logistic regression model that classifies traffic as either safe or malicious based on aggregated packet capture data using the [CICFlowmeter](http://netflowmeter.ca/) (The people that created the tool are also the same people that created the dataset used for training). The mitigation, which will only be for Linux based systems, will create and manage firewall rules written directly to iptables. While the name includes ""deep"", there is actually no deep learning involved at all. (At least not yet)

The project source code can be found here: [deepdos](https://github.com/C3NZ/deepdos)

Currently the project is listed as being in a pre-alpha state, as there are a lot of milestones that need to be hit before I can consider this a stable/production ready project. Hopefully, some of you can help me get there! Currently, I'm looking for constructive feedback on the projects current state, additions that I should be making, and really anything else that can help me grow this project into something that can be useful for companies. Here is a snapshot of the project without having to look at any of the code:

**Where I'm at:**

* Currently utilize a logistic regression model that is trained on 200,000 samples of network traffic with 100,000 being ""normal"" network traffic and 100,000 being malicious.
* Packet capture data aggregation via tcpdump. Currently, I listen for very short bursts of time for development but will be ramping this time up to reflect the communication between two devices more accurately.
* Published on Pypi (Not stable, yet).
* I've rebuilt the structure of the application 3 times right now for scalability and think I finally developed a system 

**Where I'm trying to go:**

* I'm currently thinking about how I can develop a robust testing system so that this project can continue to scale with reliability.
* Training on the full data set which is comprised of roughly 57 million samples, as I'm currently only using 200,000 of those samples. :\[
* Experimenting with different machine and deep learning models to see how I can maximize performance of the classification and of the overall application.

Working on this project has been quite the learning experience and honestly, a really enjoyable time. I really appreciate those of you that took time out of your day to read this and hope that I can garner the opinions and expertise of those of you from this thread to make this into something awesome.",8,1
660,2019-11-14,2019,11,14,3,dvvq1n,Microsoft Sends a New Kind of AI Processor Into the Cloud,https://www.reddit.com/r/MachineLearning/comments/dvvq1n/microsoft_sends_a_new_kind_of_ai_processor_into/,wheresmyhat8,1573668911,,0,1
661,2019-11-14,2019,11,14,3,dvw2e6,Best city in Europe to work as ML Engineer (except UK cities),https://www.reddit.com/r/MachineLearning/comments/dvw2e6/best_city_in_europe_to_work_as_ml_engineer_except/,erhall0102,1573670300,[removed],0,1
662,2019-11-14,2019,11,14,3,dvw84j,AAAI 2020 Reports Record-High Paper Submissions,https://www.reddit.com/r/MachineLearning/comments/dvw84j/aaai_2020_reports_recordhigh_paper_submissions/,Yuqing7,1573670951,,0,1
663,2019-11-14,2019,11,14,3,dvw9lr,Looking for ways to squeeze more performance from Transformer-based models (e.g. BERT),https://www.reddit.com/r/MachineLearning/comments/dvw9lr/looking_for_ways_to_squeeze_more_performance_from/,alghar,1573671115,[removed],0,1
664,2019-11-14,2019,11,14,4,dvwejz,[D] How do you standardize/scale data for multi-input neural networks?,https://www.reddit.com/r/MachineLearning/comments/dvwejz/d_how_do_you_standardizescale_data_for_multiinput/,ekerazha,1573671643,"Books teach us that we have to standardize or scale data (when features have non-comparable scales) before feeding a neural network (or other algorithms such as linear or logistic regressions).

When you have multi-input neural networks (for example you have some ""dense"" inputs for tabular/structured data and some convolutional layers for unstructured data such as images... and then at some point you concatenate their output), how do you standardize/scale data? You have totally different input data, different NN layers and then at some point you merge them (e.g.  [https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models](https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models)). How do you handle data normalization in this case?",1,1
665,2019-11-14,2019,11,14,4,dvwhw3,Kaolin: A PyTorch Library for Accelerating 3D Deep Learning Research from NVIDIA,https://www.reddit.com/r/MachineLearning/comments/dvwhw3/kaolin_a_pytorch_library_for_accelerating_3d_deep/,edwardsmith1884,1573671983,,0,1
666,2019-11-14,2019,11,14,4,dvwsjc,SEMINAL DEBATE : YOSHUA BENGIO | GARY MARCUS  LIVE STREAMING,https://www.reddit.com/r/MachineLearning/comments/dvwsjc/seminal_debate_yoshua_bengio_gary_marcus_live/,ComedyIsOver,1573673113,,0,1
667,2019-11-14,2019,11,14,4,dvwwoo,"[D] Gaussian Processes, Not Quite for Dummies",https://www.reddit.com/r/MachineLearning/comments/dvwwoo/d_gaussian_processes_not_quite_for_dummies/,hughbzhang,1573673558,"Yuge gives an intuitive overview of Gaussian Processes!

[https://thegradient.pub/gaussian-process-not-quite-for-dummies/](https://thegradient.pub/gaussian-process-not-quite-for-dummies/)",6,1
668,2019-11-14,2019,11,14,4,dvwzig,[R] NVIDIA's Kaolin: A PyTorch Library for Accelerating 3D Deep Learning Research,https://www.reddit.com/r/MachineLearning/comments/dvwzig/r_nvidias_kaolin_a_pytorch_library_for/,edwardsmith1884,1573673879,"Link to repo:  [https://github.com/NVIDIAGameWorks/kaolin](https://github.com/NVIDIAGameWorks/kaolin)

Link to arxiv paper: [https://arxiv.org/abs/1911.05063](https://arxiv.org/abs/1911.05063)

Abstract: We present Kaolin, a PyTorch library aiming to accelerate 3D deep learning research. Kaolin provides efficient implementations of differentiable 3D modules for use in deep learning systems. With functionality to load and preprocess several popular 3D datasets, and native functions to manipulate meshes, pointclouds, signed distance functions, and voxel grids, Kaolin mitigates the need to write wasteful boilerplate code. Kaolin packages together several differentiable graphics modules including rendering, lighting, shading, and view warping. Kaolin also supports an array of loss functions and evaluation metrics for seamless evaluation and provides visualization functionality to render the 3D results. Importantly, we curate a comprehensive model zoo comprising many state-of-the-art 3D deep learning architectures, to serve as a starting point for future research endeavours. Kaolin is available as open-source software at [this https URL](https://github.com/NVIDIAGameWorks/kaolin/).",6,1
669,2019-11-14,2019,11,14,4,dvx5uv,AAAI 2020 with stupid reviewer form old school,https://www.reddit.com/r/MachineLearning/comments/dvx5uv/aaai_2020_with_stupid_reviewer_form_old_school/,tamouze,1573674587,[removed],1,1
670,2019-11-14,2019,11,14,5,dvxgz5,This free service provides a browser-based interface for the new Deezer's (Spleeter) open source audio separation engine,https://www.reddit.com/r/MachineLearning/comments/dvxgz5/this_free_service_provides_a_browserbased/,geraldoramos,1573675812,,0,1
671,2019-11-14,2019,11,14,5,dvxhe7,"FIRST time using Weka, help with multilayer perceptron.",https://www.reddit.com/r/MachineLearning/comments/dvxhe7/first_time_using_weka_help_with_multilayer/,IgorHTRuy,1573675861,[removed],0,1
672,2019-11-14,2019,11,14,5,dvxkdm,What salary should a Research Scientist in Computer Vision get?,https://www.reddit.com/r/MachineLearning/comments/dvxkdm/what_salary_should_a_research_scientist_in/,ExtremeSavings,1573676210,[removed],0,1
673,2019-11-14,2019,11,14,5,dvxsku,Introducing the Next Generation of On-Device Vision Models: MobileNetV3 and MobileNetEdgeTPU,https://www.reddit.com/r/MachineLearning/comments/dvxsku/introducing_the_next_generation_of_ondevice/,sjoerdapp,1573677132,,0,1
674,2019-11-14,2019,11,14,6,dvybgv,Is Image/Video Data Collection for Deep Learning is Hard? Share your opinion and we can directly help you with your problem.,https://www.reddit.com/r/MachineLearning/comments/dvybgv/is_imagevideo_data_collection_for_deep_learning/,srbh66,1573679219,,1,1
675,2019-11-14,2019,11,14,6,dvyd8e,A New Paper from Francois Chollet where he disconnects Artificial General Intelligence from Training on Larger and Larger Datasets.,https://www.reddit.com/r/MachineLearning/comments/dvyd8e/a_new_paper_from_francois_chollet_where_he/,jirukulapati,1573679416,,30,1
676,2019-11-14,2019,11,14,6,dvyi48,[D] What stupid things did you use to do?,https://www.reddit.com/r/MachineLearning/comments/dvyi48/d_what_stupid_things_did_you_use_to_do/,etienne_ben,1573679959,"When I was a student, I used to spend weeks tweaking hyperparameters to improve my models for Kaggle competitions by 1 or 2%. Not once did I look at the data to see what my model was doing wrong.

Share your shame.",86,1
677,2019-11-14,2019,11,14,6,dvyjvr,Did you know programming bugs are being searched out using Machine Learning ?,https://www.reddit.com/r/MachineLearning/comments/dvyjvr/did_you_know_programming_bugs_are_being_searched/,mhazi,1573680154,,0,1
678,2019-11-14,2019,11,14,6,dvyua7,[R] Function-Space Distributions over Kernels,https://www.reddit.com/r/MachineLearning/comments/dvyua7/r_functionspace_distributions_over_kernels/,Dear_Function,1573681317,[removed],0,1
679,2019-11-14,2019,11,14,6,dvyx3j,Self Driving Cars system design =&gt; paper Recommendations ?,https://www.reddit.com/r/MachineLearning/comments/dvyx3j/self_driving_cars_system_design_paper/,MohamedRashad,1573681629,[removed],0,1
680,2019-11-14,2019,11,14,9,dw0z2i,"John Carmack on leaving Oculus as full-time CTO: ""Im going to work on artificial general intelligence (AGI). I think it is possible, enormously valuable, and that I have a non-negligible chance of making a difference there..""",https://www.reddit.com/r/MachineLearning/comments/dw0z2i/john_carmack_on_leaving_oculus_as_fulltime_cto_im/,anustretch,1573690131,,0,1
681,2019-11-14,2019,11,14,9,dw146b,Nazirinis story - using machine learning to tackle crop disease,https://www.reddit.com/r/MachineLearning/comments/dw146b/nazirinis_story_using_machine_learning_to_tackle/,BombBurper,1573690719,,0,1
682,2019-11-14,2019,11,14,9,dw1nug,John Carmack moving to Consulting CTO Position with Oculus in favor of pursuing AGI,https://www.reddit.com/r/MachineLearning/comments/dw1nug/john_carmack_moving_to_consulting_cto_position/,mimomusic,1573693168,,4,1
683,2019-11-14,2019,11,14,10,dw1v6m,Show HN: Can a neural network predict if your HN post title will get up votes?,https://www.reddit.com/r/MachineLearning/comments/dw1v6m/show_hn_can_a_neural_network_predict_if_your_hn/,HN_Crosspost_Bot,1573694125,,0,1
684,2019-11-14,2019,11,14,10,dw1wqh,[N] Graphcores AI accelerator chips launch on Microsoft Azure,https://www.reddit.com/r/MachineLearning/comments/dw1wqh/n_graphcores_ai_accelerator_chips_launch_on/,xternalz,1573694331,,0,1
685,2019-11-14,2019,11,14,10,dw1zhg,[N] Graphcores AI accelerator chips launch on Microsoft Azure,https://www.reddit.com/r/MachineLearning/comments/dw1zhg/n_graphcores_ai_accelerator_chips_launch_on/,xternalz,1573694687,,0,1
686,2019-11-14,2019,11,14,12,dw3hfg,I have 40 .dat files each with two arrays (one 2D one 3D) that I want to get loaded up for ML. I've figured out how to get one file into a numpy array. Whats the best plan for the other 39 files?,https://www.reddit.com/r/MachineLearning/comments/dw3hfg/i_have_40_dat_files_each_with_two_arrays_one_2d/,xinwow,1573702014,,0,1
687,2019-11-14,2019,11,14,13,dw44lc,[D] John Carmack stepping down as Oculus CTO to work on artificial general intelligence (AGI),https://www.reddit.com/r/MachineLearning/comments/dw44lc/d_john_carmack_stepping_down_as_oculus_cto_to/,jd_3d,1573705297,,1,1
688,2019-11-14,2019,11,14,13,dw46ts,Try out function approximation with the +-*/ tree,https://www.reddit.com/r/MachineLearning/comments/dw46ts/try_out_function_approximation_with_the_tree/,rootmolloch,1573705654,,0,1
689,2019-11-14,2019,11,14,13,dw4a2c,"""[D]"" John Carmack stepping down as Oculus CTO to work on artificial general intelligence (AGI)",https://www.reddit.com/r/MachineLearning/comments/dw4a2c/d_john_carmack_stepping_down_as_oculus_cto_to/,jd_3d,1573706130,"Here is John's post with more details:

 [https://www.facebook.com/permalink.php?story\_fbid=2547632585471243&amp;id=100006735798590](https://www.facebook.com/permalink.php?story_fbid=2547632585471243&amp;id=100006735798590) 

I'm curious what members here on MachineLearning think about this, especially that he's going after AGI and starting from his home in a ""Victorian Gentleman Scientist"" style. John Carmack is one of the smartest people alive in my opinion, and even as CTO at Oculus he's answered several of my questions via Twitter despite never meeting me nor knowing who I am. A real stand-up guy.",182,1
690,2019-11-14,2019,11,14,13,dw4hmr,any one tried stock prediction ?,https://www.reddit.com/r/MachineLearning/comments/dw4hmr/any_one_tried_stock_prediction/,sarathak7,1573707302,[removed],0,1
691,2019-11-14,2019,11,14,15,dw5avm,Why do so many startups claim that machine learning is their long game,https://www.reddit.com/r/MachineLearning/comments/dw5avm/why_do_so_many_startups_claim_that_machine/,smartphoneenthusiast,1573711956,,0,1
692,2019-11-14,2019,11,14,15,dw5e5v,[Q] How to create a realistic facial heatmap for visualization that ?,https://www.reddit.com/r/MachineLearning/comments/dw5e5v/q_how_to_create_a_realistic_facial_heatmap_for/,hosjiu,1573712508,[removed],0,1
693,2019-11-14,2019,11,14,16,dw62z7,Ask HN: Why do so many startups claim machine learning is their long game?,https://www.reddit.com/r/MachineLearning/comments/dw62z7/ask_hn_why_do_so_many_startups_claim_machine/,HN_Crosspost_Bot,1573716713,,0,1
694,2019-11-14,2019,11,14,16,dw695a,BERT for non-textual sequence data,https://www.reddit.com/r/MachineLearning/comments/dw695a/bert_for_nontextual_sequence_data/,daanvdn,1573717789,"Hi there, I'm working on a deep learning solution for classifying sequence data that isn't raw text but rather entities (which have already been extracted from the text). I am currently using word2vec-style embeddings to feed the entities to a CNN, but I was wondering if a Transformer ( la BERT) would be a better alternative &amp; provide a better way of capturing the semantics of the entities involved. I can't seem to find any articles (let alone libraries) to apply sth like BERT to non-textual sequence data. Does anybody know any papers about this angle? I've thought about training a BERT model from scratch and treating the entities as if they were text. The issue with that though is that apparently BERT is slow when dealing with long sequences (sentences). In my data I often have sequences that have a length of 1000+  so I'm worried BERT won't cut it. Any help, insights or references are very much appreciated! Thanks",0,1
695,2019-11-14,2019,11,14,17,dw6p7s,Machine Learning Captcha,https://www.reddit.com/r/MachineLearning/comments/dw6p7s/machine_learning_captcha/,_hockenberry,1573720754,,0,1
696,2019-11-14,2019,11,14,17,dw6pl1,[D] Thoughts about this conversation?,https://www.reddit.com/r/MachineLearning/comments/dw6pl1/d_thoughts_about_this_conversation/,GreySindrome,1573720816,"This thread is on a public forum(Twitter) between two scientists.

Person1 - \[Director of #AI #research @nvidia, Bren #Professor @Caltech, Fmr Principal scientist @awscloud\] Person2 - Research Scientist at Deepmind

Both are entitled to their own opinions. Here's how the thread goes...

Person1(talking about her newly published work): DeepLearning is only good at interpolation. But applications need extrapolation that can reason about more complex scenarios than it is trained on. With current methods, accuracy degrades rapidly when complexity of test instances grows. Our new work aims to overcome this...

Person2: This tweet really downplays prior work. NTM, memory nets, Neural GPU, MANN, graph nets, and many, many other related methods also degrade gracefully. Your work looks like an important next step, but this rhetoric is unhelpful.

Person1: What you are doing is rhetoric and rude. We have mentioned all prior work in our paper. You don't want to engage in science. It is inevitable to get attacked online as a woman. #deepmind can engage in all kind of media hype that is unethical but I get attacked for stating facts. As a woman stating science, I get accused of engaging in rhetoric.

I personally feel this response by Person1 to be extremely out of the blue. Putting aside the fact that Person1 is a Director @ NVIDIA + some title at Caltech and Person2 is a scientist as well @Google, let's look at the simple conversation here. The thread started with a tweet about an interesting work. That was followed by a review directed only at the tweet being rhetoric. And it was then replied with something unimaginable. Am I the only one looking at this all confused?

Source post: https://twitter.com/AnimaAnandkumar/status/1194338388221972480",20,1
697,2019-11-14,2019,11,14,17,dw6us9,[D] BERT for non-textual sequence data,https://www.reddit.com/r/MachineLearning/comments/dw6us9/d_bert_for_nontextual_sequence_data/,daanvdn,1573721801,"Hi there, I'm working on a deep learning solution for classifying sequence data that isn't raw text but rather entities (which have already been extracted from the text). I am currently using word2vec-style embeddings to feed the entities to a CNN, but I was wondering if a Transformer ( la BERT) would be a better alternative &amp; provide a better way of capturing the semantics of the entities involved. I can't seem to find any articles (let alone libraries) to apply sth like BERT to non-textual sequence data. Does anybody know any papers about this angle? I've thought about training a BERT model from scratch and treating the entities as if they were text. The issue with that though is that apparently BERT is slow when dealing with long sequences (sentences). In my data I often have sequences that have a length of 1000+  so I'm worried BERT won't cut it. Any help, insights or references are very much appreciated! Thanks",3,1
698,2019-11-14,2019,11,14,17,dw6vgg,How Can My Business Get the Most Out of Self-Serve Advanced Analytics?,https://www.reddit.com/r/MachineLearning/comments/dw6vgg/how_can_my_business_get_the_most_out_of_selfserve/,ElegantMicroWebIndia,1573721911,,0,1
699,2019-11-14,2019,11,14,18,dw6xwn,Compositional Learning,https://www.reddit.com/r/MachineLearning/comments/dw6xwn/compositional_learning/,m-cdf,1573722365,[removed],0,1
700,2019-11-14,2019,11,14,18,dw6yo3,[R] Teaching a neural network to use a calculator,https://www.reddit.com/r/MachineLearning/comments/dw6yo3/r_teaching_a_neural_network_to_use_a_calculator/,wei_jok,1573722497,"*[Article](https://reiinakano.com/2019/11/12/solving-probability.html) by Reiichiro Nakano:*

This article explores a seq2seq architecture for solving simple probability problems in [Saxton et. al.](https://arxiv.org/abs/1904.01557)s [Mathematics Dataset](https://github.com/deepmind/mathematics_dataset/). A transformer is used to map questions to intermediate steps, while an external symbolic calculator evaluates intermediate expressions. This approach emulates how a student might solve math problems, by setting up intermediate equations, using a calculator to solve them, and using those results to construct further equations.

https://reiinakano.com/2019/11/12/solving-probability.html",3,1
701,2019-11-14,2019,11,14,18,dw74wc,Should DeepMind employ Chinese researchers?,https://www.reddit.com/r/MachineLearning/comments/dw74wc/should_deepmind_employ_chinese_researchers/,lucozade_uk,1573723601,[removed],0,1
702,2019-11-14,2019,11,14,18,dw7b4w,Data Analytics Courses in Pune,https://www.reddit.com/r/MachineLearning/comments/dw7b4w/data_analytics_courses_in_pune/,Successdig,1573724819,,0,1
703,2019-11-14,2019,11,14,19,dw7nw9,Get Best In Class Nursing Assignment Help from Our PhD Experts,https://www.reddit.com/r/MachineLearning/comments/dw7nw9/get_best_in_class_nursing_assignment_help_from/,sidandrew881,1573727098,[removed],0,1
704,2019-11-14,2019,11,14,19,dw7r4a,Teaching a neural network to use a calculator,https://www.reddit.com/r/MachineLearning/comments/dw7r4a/teaching_a_neural_network_to_use_a_calculator/,mmaksimovic,1573727693,,0,1
705,2019-11-14,2019,11,14,19,dw7se0,[R] HOList: An Environment for Machine Learning of Higher-Order Theorem Proving,https://www.reddit.com/r/MachineLearning/comments/dw7se0/r_holist_an_environment_for_machine_learning_of/,epicwisdom,1573727910,,2,1
706,2019-11-14,2019,11,14,19,dw7sms,[D] Working on an ethically questionnable project...,https://www.reddit.com/r/MachineLearning/comments/dw7sms/d_working_on_an_ethically_questionnable_project/,big_skapinsky,1573727954,"Hello all,

I'm writing here to discuss a bit of a moral dilemma I'm having at work with a new project we got handed. Here it is in a nutshell : 

&gt;Provide a tool that can gauge a person's personality just from an image of their face. This can then be used by an HR office to help out with sorting job applicants.

So first off, there is no concrete proof that this is even possible. I mean, I have a hard time believing that our personality is characterized by our facial features. [Lots of papers](http://alittlelab.com/littlelab/pubs/Little_07_personality_composites.pdf) claim this to be possible, but they don't give accuracies above 20%-25%. (And if you are detecting a person's personality using the big 5, this is simply random.) This branch of [pseudoscience](https://en.wikipedia.org/wiki/Physiognomy) was discredited in the Middle Ages for crying out loud.

Second, if somehow there is a correlation, and we do develop this tool, I don't want to be anywhere near the training of this algorithm. What if we underrepresent some population class? What if our algorithm becomes racist/ sexist/ homophobic/ etc... The social implications of this kind of technology used in a recruiter's toolbox are huge.

Now the reassuring news is that the team I work with all have the same concerns as I do. The project is still in its State-of-the-Art phase, and we are hoping that it won't get past the Proof-of-Concept phase. Hell, my boss told me that it's a good way to ""empirically prove that this mumbo jumbo does not work.""

What do you all think?",336,1
707,2019-11-14,2019,11,14,19,dw7swp,Deep learning for regression problems,https://www.reddit.com/r/MachineLearning/comments/dw7swp/deep_learning_for_regression_problems/,rahulkumar1972,1573728004,[removed],0,1
708,2019-11-14,2019,11,14,19,dw7wht,Recession &amp; AI Jobs: What It Implies for Next-Gen Professionals?,https://www.reddit.com/r/MachineLearning/comments/dw7wht/recession_ai_jobs_what_it_implies_for_nextgen/,Albertchristopher,1573728648,,0,1
709,2019-11-14,2019,11,14,19,dw7zji,[D] What is a good method for measuring processing time for a classifier?,https://www.reddit.com/r/MachineLearning/comments/dw7zji/d_what_is_a_good_method_for_measuring_processing/,FragileStudios,1573729188,"I've been looking at StackOverflow etc for an answer to this question, and so far I've only seen that time is not a good way of measuring how long a classifier takes to run on a dataset.

Is there a better method of measuring the ""time"" it takes for a classifier to run?

What are your thoughts?",3,1
710,2019-11-14,2019,11,14,20,dw80qs,WHY do some algorithms work better than others?,https://www.reddit.com/r/MachineLearning/comments/dw80qs/why_do_some_algorithms_work_better_than_others/,__horned_owl__,1573729390,"Hey guys, I've been wondering about this for a while now. The question is: why specifically do some algorithms work better than others on a given dataset?

Is it because the algorithm that works better changes the loss landscape in such a way that it is easier to optimize, it has fewer local optima, each optima is better in terms of likelihood, etc.

Or is it that it all depends on the properties of the data on which the algorithm is applied?

Some of these questions may not be accurately defined, but generally I'm interested in the question in the title. Also, links to research papers and summaries are welcomed.",0,1
711,2019-11-14,2019,11,14,20,dw8ayc,A group of medical researchers has applied machine learning and pattern recognition to detect various types of cancer in patients at an early stage,https://www.reddit.com/r/MachineLearning/comments/dw8ayc/a_group_of_medical_researchers_has_applied/,Huspi_sp_z_o_o,1573731236,,0,1
712,2019-11-14,2019,11,14,20,dw8jju,Looking for Machine Learning Book,https://www.reddit.com/r/MachineLearning/comments/dw8jju/looking_for_machine_learning_book/,LongtNG,1573732729,[removed],0,1
713,2019-11-14,2019,11,14,21,dw8w20,SPICE: Self-Supervised Pitch Estimation,https://www.reddit.com/r/MachineLearning/comments/dw8w20/spice_selfsupervised_pitch_estimation/,sjoerdapp,1573734676,,0,1
714,2019-11-14,2019,11,14,22,dw9co8,Useful resource for data science interviews,https://www.reddit.com/r/MachineLearning/comments/dw9co8/useful_resource_for_data_science_interviews/,sagarj,1573737110,,0,1
715,2019-11-14,2019,11,14,23,dwagyg,How much data do you really need,https://www.reddit.com/r/MachineLearning/comments/dwagyg/how_much_data_do_you_really_need/,HyamsG,1573742766,[removed],0,1
716,2019-11-15,2019,11,15,0,dwav8s,[R]Research Guide: Pruning Techniques for Neural Networks,https://www.reddit.com/r/MachineLearning/comments/dwav8s/rresearch_guide_pruning_techniques_for_neural/,mwitiderrick,1573744579,"Pruning is a technique in deep learning that aids in the development of smaller and more efficient neural networks. Its a model optimization technique that involves eliminating unnecessary values in the weight tensor. This results in compressed neural networks that run faster, reducing the computational cost involved in training the networks. 

&amp;#x200B;

More at [https://heartbeat.fritz.ai/research-guide-pruning-techniques-for-neural-networks-d9b8440ab10d](https://heartbeat.fritz.ai/research-guide-pruning-techniques-for-neural-networks-d9b8440ab10d)",0,1
717,2019-11-15,2019,11,15,0,dwb2l6,We trained a self-balancing physics-based character to follow interactive motion capture.,https://www.reddit.com/r/MachineLearning/comments/dwb2l6/we_trained_a_selfbalancing_physicsbased_character/,profbof,1573745508,"Here's a twitter thread with videos of a ragdoll getting lots of cubes in the face:  [https://twitter.com/profbof/status/1194734191843454976](https://twitter.com/profbof/status/1194734191843454976) 

Here's a blog post:  [https://montreal.ubisoft.com/en/drecon-data-driven-responsive-control-of-physics-based-characters/](https://montreal.ubisoft.com/en/drecon-data-driven-responsive-control-of-physics-based-characters/) 

And a paper, which will be presented at Siggraph Asia next week:  [https://static-wordpress.akamaized.net/montreal.ubisoft.com/wp-content/uploads/2019/11/13214229/DReCon.pdf](https://static-wordpress.akamaized.net/montreal.ubisoft.com/wp-content/uploads/2019/11/13214229/DReCon.pdf)",0,1
718,2019-11-15,2019,11,15,0,dwbajp,[Research] We trained a self-balancing physics-based character to follow interactive motion capture.,https://www.reddit.com/r/MachineLearning/comments/dwbajp/research_we_trained_a_selfbalancing_physicsbased/,profbof,1573746498,"Here's a twitter thread including a video of a ragdoll getting lots of cubes in the face: [https://twitter.com/profbof/status/1194734191843454976](https://twitter.com/profbof/status/1194734191843454976)

Here's a blog post: [https://montreal.ubisoft.com/en/drecon-data-driven-responsive-control-of-physics-based-characters](https://montreal.ubisoft.com/en/drecon-data-driven-responsive-control-of-physics-based-characters)

Here's a paper, which will be presented at Siggraph Asia next week: [https://static-wordpress.akamaized.net/montreal.ubisoft.com/wp-content/uploads/2019/11/13214229/DReCon.pdf](https://static-wordpress.akamaized.net/montreal.ubisoft.com/wp-content/uploads/2019/11/13214229/DReCon.pdf)

And here's a high level explanation of what this is all about:

Physics-based animation holds the promise of unlocking unprecedented levels of interaction, fidelity, and variety in games. The intricate interactions between a character and its environment can only be faithfully synthesized by respecting real physical principles. On the other hand, data-driven animation systems utilizing large amounts of motion capture data have already shown that artistic style and motion variety can be preserved even when tight constraints on responsiveness and motion control objectives are required by a games design.

To combine the strengths of both methods we developed **DReCon**, a character controller created using deep reinforcement learning. Essentially, simulated human characters learn to move around and balance from precisely controllable motion capture examples. Once trained, gamepad controlled characters can be fully simulated using physics and simultaneously directed with a high level of responsiveness at a surprisingly low runtime cost on todays hardware.",2,1
719,2019-11-15,2019,11,15,0,dwbcxy,[N] Awesome AI Research and Papers reviewed on Computer Vision News (with codes!) November 2019,https://www.reddit.com/r/MachineLearning/comments/dwbcxy/n_awesome_ai_research_and_papers_reviewed_on/,Gletta,1573746782,"The November issue of Computer Vision News: 38 pages about AI and Deep Learning through research and practical applications.

[HTML5 version (recommended)](https://www.rsipvision.com/ComputerVisionNews-2019November/)

[PDF version](https://www.rsipvision.com/computer-vision-news-2019-august-pdf/)

Technical articles on pages 4-8 and 24-29. Subscribe for free on page 38.

Enjoy!

https://preview.redd.it/mnlc2u9dboy31.jpg?width=700&amp;format=pjpg&amp;auto=webp&amp;s=527504eb90c8efe1a73a94b4a7242c127e0d1aa7",2,1
720,2019-11-15,2019,11,15,1,dwbr5r,"[R] ""Compressive Transformers for Long-Range Sequence Modelling"", Rae et al 2019",https://www.reddit.com/r/MachineLearning/comments/dwbr5r/r_compressive_transformers_for_longrange_sequence/,gwern,1573748496,,2,1
721,2019-11-15,2019,11,15,1,dwc6hl,[D][R] Applications of AI/ML in the field of Digital Pathology,https://www.reddit.com/r/MachineLearning/comments/dwc6hl/dr_applications_of_aiml_in_the_field_of_digital/,omayrakhtar,1573750261,"I have prior experience of working on a very interesting project: [Prostate Cancer Detection using Deep Learning in Histopathology slides](https://www.linkedin.com/pulse/prostate-cancer-detection-using-deep-learning-umair-khan/), it was in collaboration with Helsinki University Hospital. Now, I am interested in working on something beyond classification. I am also going through relevant publications but at the same time, I need some feedback from the community about interesting problems.",5,1
722,2019-11-15,2019,11,15,1,dwc8u7,Seeking Info on Tuning Deep Learning,https://www.reddit.com/r/MachineLearning/comments/dwc8u7/seeking_info_on_tuning_deep_learning/,mockrun,1573750525,[removed],0,1
723,2019-11-15,2019,11,15,1,dwcbcz,"[R] Momentum Contrast for Unsupervised Visual Representation Learning: ""the gap between unsupervised and supervised representation learning has been largely closed""",https://www.reddit.com/r/MachineLearning/comments/dwcbcz/r_momentum_contrast_for_unsupervised_visual/,downtownslim,1573750798,,16,1
724,2019-11-15,2019,11,15,2,dwcfk0,Why do AIs need so much training before they get good at a game?,https://www.reddit.com/r/MachineLearning/comments/dwcfk0/why_do_ais_need_so_much_training_before_they_get/,mikemishere,1573751247,[removed],0,1
725,2019-11-15,2019,11,15,2,dwcif1,Building chess bot for rasberi pi with opencv,https://www.reddit.com/r/MachineLearning/comments/dwcif1/building_chess_bot_for_rasberi_pi_with_opencv/,Jonisas0407,1573751562,[removed],0,1
726,2019-11-15,2019,11,15,2,dwd10m,Machine Learning for Plant Breeding at Rijk Zwaan in the Netherlands,https://www.reddit.com/r/MachineLearning/comments/dwd10m/machine_learning_for_plant_breeding_at_rijk_zwaan/,Dorfkrug,1573753664,,0,1
727,2019-11-15,2019,11,15,2,dwd1gd,"Planning for NeurIPS, how to search through papers/posters/talks?",https://www.reddit.com/r/MachineLearning/comments/dwd1gd/planning_for_neurips_how_to_search_through/,boltzBrain,1573753717,[removed],0,1
728,2019-11-15,2019,11,15,3,dwdpqf,How easy is it to learn ML from scratch?,https://www.reddit.com/r/MachineLearning/comments/dwdpqf/how_easy_is_it_to_learn_ml_from_scratch/,Redditor28482,1573756514,"Hello.

I'm a high school student, and I have to develop an android studio app (java). Just to mention, I have a decent background with java, and some advanced math knowledge.

I had an idea to develop an app which, given 2 MMA fighters, predicts who will win and how, based on previous data and certain parameters (such as opponents beaten, accuracy etc). The data will be on a website with a full database. 

My question is:

1. Is it possible to learn and get this project done in a couple of months? 
2. Is it possible/good to do ML with java? I've heard about other languages that suit it more...
3. How hard do you think it will be to implement the idea?",0,1
729,2019-11-15,2019,11,15,4,dwea0o,DeepMind events?,https://www.reddit.com/r/MachineLearning/comments/dwea0o/deepmind_events/,goblix,1573758913,[removed],0,1
730,2019-11-15,2019,11,15,4,dwej5r,[Research] Driving Experience Survey,https://www.reddit.com/r/MachineLearning/comments/dwej5r/research_driving_experience_survey/,Ajani20,1573759996,"Hi my name is Ajani McIntosh. I am a student at Farmingdale State College and I am conducting a research on how the driving experiment affects the functioning of the driver. This survey should take between 2-3 minutes. I intend to use the data I gain strictly for my course and for nothing else. The data collected will not be used for financial, political or legal gain. Your participation is appreciated and voluntary. If you find any of the questions too personal or excessive, you can leave the survey as you please. Please note that if your entry is incomplete, it will not be included in the data leading to the conclusion of my research. Thank you for your participation and time.  


Here is the link to the survey:

[https://docs.google.com/forms/d/e/1FAIpQLSdSQ0yKbLi1bfGOG6f6jQu4vOKuYm-F-si-xszopjbOMLk4RQ/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLSdSQ0yKbLi1bfGOG6f6jQu4vOKuYm-F-si-xszopjbOMLk4RQ/viewform?usp=sf_link)",1,1
731,2019-11-15,2019,11,15,4,dwes82,Architecture of a neural network implementing a neural algorithm of artistic style,https://www.reddit.com/r/MachineLearning/comments/dwes82/architecture_of_a_neural_network_implementing_a/,Exploree1,1573761054,[removed],0,1
732,2019-11-15,2019,11,15,5,dwexqc,Topic modeling for small dataset,https://www.reddit.com/r/MachineLearning/comments/dwexqc/topic_modeling_for_small_dataset/,NouraAlba,1573761694,[removed],0,1
733,2019-11-15,2019,11,15,5,dwf4jq,Sequence classification with only dense layers(FCN),https://www.reddit.com/r/MachineLearning/comments/dwf4jq/sequence_classification_with_only_dense_layersfcn/,nicetryho,1573762488,[removed],0,1
734,2019-11-15,2019,11,15,5,dwf6wr,Sequence classification with only dense layers(FCN),https://www.reddit.com/r/MachineLearning/comments/dwf6wr/sequence_classification_with_only_dense_layersfcn/,copythatpasta,1573762789,[removed],0,1
735,2019-11-15,2019,11,15,5,dwfbg6,Food Classification with FastAI,https://www.reddit.com/r/MachineLearning/comments/dwfbg6/food_classification_with_fastai/,DevJonPizza,1573763330,,0,1
736,2019-11-15,2019,11,15,5,dwfhbh,[D] [P] Architecture of a neural network implementing a neural algorithm of artistic style,https://www.reddit.com/r/MachineLearning/comments/dwfhbh/d_p_architecture_of_a_neural_network_implementing/,Exploree1,1573764014,"Hey guys,

which neural network would you prefer to use to  implement this style transfer algorithm from that paper?

[https://arxiv.org/pdf/1508.06576.pdf](https://arxiv.org/pdf/1508.06576.pdf)

I saw Tutorials e.g. from Tensorflow using the VGG19 net, but I find it too painful too train and since accuracy in classification is not that important because it is more a sort of artistic thing, I thing a network like **SqueezeNet** is a better fit and the final model is ridiculously light. I would like to hear your thoughts about which architecture you would prefer.",10,1
737,2019-11-15,2019,11,15,5,dwfhdy,FILM ADVISOR REQUEST: Seeking an advisor for an AI storyline on a movie script. If interested please DM me.,https://www.reddit.com/r/MachineLearning/comments/dwfhdy/film_advisor_request_seeking_an_advisor_for_an_ai/,shamus_gumshoe,1573764022,[removed],0,1
738,2019-11-15,2019,11,15,5,dwfkev,How do i change the ensemble size in weka while using bagging?There is no clear instruction given anywhere.,https://www.reddit.com/r/MachineLearning/comments/dwfkev/how_do_i_change_the_ensemble_size_in_weka_while/,ihavefewq,1573764399,[removed],0,1
739,2019-11-15,2019,11,15,6,dwg1r0,A scientist has come up with an algorithm that brings clarity and color to underwater photos,https://www.reddit.com/r/MachineLearning/comments/dwg1r0/a_scientist_has_come_up_with_an_algorithm_that/,y_ourfutureself,1573766449,,0,1
740,2019-11-15,2019,11,15,6,dwg9gl,[Discussion] fully connected network to classify sequential data ?,https://www.reddit.com/r/MachineLearning/comments/dwg9gl/discussion_fully_connected_network_to_classify/,copythatpasta,1573767363,"I have been experimenting with classifying raw audio samples using temporal convolutions and RNNs. Recently I did an experiment where I just replaced all layers with Dense layers(Keras) and took out all other layers. Surprisingly it did quite well on validation and test data and trained very fast.

Why did this work? Its only classifying in one second of audio data sampled at 8khz. Can fully connected networks model sequential data ? Or is there something wrong with my experiment/setup? 

TL;DR Can FCN classify sequential data ?",10,1
741,2019-11-15,2019,11,15,6,dwgaxw,[D] Online paper reading / note taking software?,https://www.reddit.com/r/MachineLearning/comments/dwgaxw/d_online_paper_reading_note_taking_software/,good_rice,1573767538,"Im looking for a way to save, annotate, highlight, and write notes on recent papers Ive read. 

Is anyone aware of an online, browser based tool that would allow for this (E.G. upload a PDF and do aforementioned tasks)? I use a variety of devices, both personal and at work, and would prefer to avoid downloading software onto all, but would be open to any suggestions.

What do you all use?",5,1
742,2019-11-15,2019,11,15,7,dwgyj0,[R] Confidence Estimation for Black Box Automatic Speech Recognition Systems Using Lattice Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/dwgyj0/r_confidence_estimation_for_black_box_automatic/,LostBottleCap,1573770200,"Paper: https://arxiv.org/abs/1910.11933

Github repo: https://github.com/alecokas/BiLatticeRNN-Confidence

Abstract:

Recently, there has been growth in providers of speech transcription services enabling others to leverage technology they would not normally be able to use. As a result, speech-enabled solutions have become commonplace. Their success critically relies on the quality, accuracy, and reliability of the underlying speech transcription systems. Those black box systems, however, offer limited means for quality control as only word sequences are typically available. This paper examines this limited resource scenario for confidence estimation, a measure commonly used to assess transcription reliability. In particular, it explores what other sources of word and sub-word level information available in the transcription process could be used to improve confidence scores. To encode all such information this paper extends lattice recurrent neural networks to handle sub-words. Experimental results using the IARPA OpenKWS 2016 evaluation system show that the use of additional information yields significant gains in confidence estimation accuracy.


TL;DR:

This paper looks at how to improve confidence estimates for black-box automatic speech recognition systems.",0,1
743,2019-11-15,2019,11,15,7,dwh3q6,"Ranking Problem: What is the best performance metric to look at when ranking, and how should one visualize predictions?",https://www.reddit.com/r/MachineLearning/comments/dwh3q6/ranking_problem_what_is_the_best_performance/,MLn00bie22,1573770828,"I'm working on a ranking problem where I am trying to provide a list of advertisements that the user is most likely to click on. 

To simplify the problem, I am treating it as a binary classification problem and am using a logistic regression model with the following labels: 

* 0 : for not clicked 
* 1 : for clicked 

The data is highly imbalanced with a ratio of say 1:100 for clicks to non-clicks. 

**Metrics**

What metrics are suitable for a problem like this? The typical classification metrics like accuracy, recall, precision, etc. do not apply here because I ultimately do not care about predicting 1's or 0's. I care about whether or not a 1 was ranked higher than a 0 and that a 0 is ranked lower. 

I know that the AUC score basically does this. Is this the best metric when it comes to ranking? Is there a way to take into account the imbalance of data because we have a lot of 0's, wouldn't it be easy to think that we have a higher than normal AUC? 

The other thing that is tricky is if I had 100 pieces of advertisements to show, I only care about the performance of the top 10 because i would never recommend any advertisements from the last 90 during show time. So I am not sure how to account for that when comparing models based on a single metric. I was thinking about something like an ""adjusted AUC score"" where I only calculate the AUC for say the top 10. 

**Visualization**

What is the best way to visualize the results of a ranking problem? Because the data is highly imbalanced, a histogram doesn't really work. I tried out a kernel density plot and thought it was pretty good, but wanted to know about others' thoughts. Are there any pitfalls to using a density plot in this context? 

&amp;#x200B;

![img](btfgnb9raqy31)

Sorry for all the questions. 

Thanks!",0,1
744,2019-11-15,2019,11,15,8,dwi09y,[D] Looking for a good open-source NAS implementation,https://www.reddit.com/r/MachineLearning/comments/dwi09y/d_looking_for_a_good_opensource_nas_implementation/,ilia10000,1573774703,"I'm looking for a good Neural Architecture Search implementation that I could use to learn a deep network for my custom dataset. The dataset is sequential (long sequences of \~40 unique tokens), and the task is just sequence modeling. I'm happy to use a fixed window size and one-hot encoding to turn the inputs into fixed-size matrices if that makes life easier (e.g. turning this into an image classification task). I've already used hand-crafted RNNs and attention-based models and achieved pretty high accuracy, but I'm hoping to try out NAS and see how it compares. 

I wanted to ask here and see if anyone has suggestions first before I sink a large number of hours into any one package. Some packages I've used/heard of:

AutoKeras - Doesn't seem to work with the sequential form of the data but does work with the fixed-size matrices by using the ImageClassifier. Having a bunch of problems though as 0.4 version is a little broken, and 1.0 version isn't yet complete. 

auto-sklearn - My understanding is it focuses on XGBoost rather than neural nets.

[https://github.com/quark0/darts](https://github.com/quark0/darts) \- I really like the idea/paper for this one, has anyone had experience using it for a custom dataset?

[https://github.com/carpedm20/ENAS-pytorch](https://github.com/carpedm20/ENAS-pytorch) \- Same algorithm as AutoKeras (ENAS) but only RNNs are implemented so far. I see a lot of open issues on GitHub so a little apprehensive about committing time to this specific implementation. 

Exploring Randomly Wired Neural Networks for Image Recognition ([https://arxiv.org/abs/1904.01569](https://arxiv.org/abs/1904.01569)) - I love this paper but can only find unofficial implementations on GitHub. Looking through them it seems like they don't achieve quite as high results as in the paper. Has anyone seen an official (or best) implementation of this?

Microsoft NNI ([https://github.com/microsoft/nni](https://github.com/microsoft/nni)) - This seems a little bit intense and I haven't had a chance to learn how to use it yet. It does seem to contain ENAS as one of the algorithms though, so this seems promising. 

Is one of these the right tool for me to use? Are there some other implementations out there that would be better suited for my task?",0,1
745,2019-11-15,2019,11,15,9,dwiccp,Handwritten date recognizer,https://www.reddit.com/r/MachineLearning/comments/dwiccp/handwritten_date_recognizer/,SmightyCreep,1573776255,[removed],0,1
746,2019-11-15,2019,11,15,9,dwicrq,Latest Handpicked AI News,https://www.reddit.com/r/MachineLearning/comments/dwicrq/latest_handpicked_ai_news/,WebAI,1573776317,,0,1
747,2019-11-15,2019,11,15,9,dwihz0,[D] gpt-2 length of output,https://www.reddit.com/r/MachineLearning/comments/dwihz0/d_gpt2_length_of_output/,orenog,1573776939,"Hi, how can I run gpt-2 with the new full model locally? And will I be able to generate very long (100,000 characters) outputs on a standard gaming PC? 16GB ram, gtx 1060 6GB , i7 4770k @4.8ghz",0,1
748,2019-11-15,2019,11,15,9,dwilzs,[D] A good NLP pre-processing package?,https://www.reddit.com/r/MachineLearning/comments/dwilzs/d_a_good_nlp_preprocessing_package/,lyeoni,1573777457,"Hi Reddit,

I am working on various **NLP** project for several years. And, I have used various pre-processing packages related with **NLP**, such as nltk, gluonnlp and torchtext. However, It's not still clear which package is the best to process text.

\- what **NLP packages** would you recommend me ?

Thank you so much !",6,1
749,2019-11-15,2019,11,15,9,dwiphq,where can I find already created ML models?,https://www.reddit.com/r/MachineLearning/comments/dwiphq/where_can_i_find_already_created_ml_models/,WifeStealer99,1573777910,[removed],0,1
750,2019-11-15,2019,11,15,10,dwjbzh,DL with Metropolis,https://www.reddit.com/r/MachineLearning/comments/dwjbzh/dl_with_metropolis/,yetanothernormalG,1573780941,[removed],0,1
751,2019-11-15,2019,11,15,11,dwkbmk,Boomer detection machine-learning AI,https://www.reddit.com/r/MachineLearning/comments/dwkbmk/boomer_detection_machinelearning_ai/,PM_ME_PIKACHU_CUTIE,1573785838,,0,1
752,2019-11-15,2019,11,15,13,dwl8y6,[D] Art of the Problem posted an EPIC video on neural network history today - thoughts?,https://www.reddit.com/r/MachineLearning/comments/dwl8y6/d_art_of_the_problem_posted_an_epic_video_on/,puppers90,1573790517,,0,1
753,2019-11-15,2019,11,15,14,dwm6h4,Nave Bayes for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/dwm6h4/nave_bayes_for_machine_learning/,HN_Crosspost_Bot,1573795715,,0,1
754,2019-11-15,2019,11,15,16,dwngvc,World first ever RPG with 'dungeon master ai' and 'story engine' in the works using neural network and machine learning,https://www.reddit.com/r/MachineLearning/comments/dwngvc/world_first_ever_rpg_with_dungeon_master_ai_and/,bugsixx,1573803959,[removed],0,1
755,2019-11-15,2019,11,15,16,dwnio4,My second blog ,https://www.reddit.com/r/MachineLearning/comments/dwnio4/my_second_blog/,Shubhamai,1573804274,[removed],0,1
756,2019-11-15,2019,11,15,16,dwnk72,World first ever computer RPG with 'dungeon master ai' and 'story engine' in the works using neural network and machine learning,https://www.reddit.com/r/MachineLearning/comments/dwnk72/world_first_ever_computer_rpg_with_dungeon_master/,bugsixx,1573804558,,0,1
757,2019-11-15,2019,11,15,17,dwnuwh,"[D] DanNet, the CUDA CNN of Dan Ciresan in Jurgen Schmidhuber's team, won 4 image recognition challenges prior to AlexNet",https://www.reddit.com/r/MachineLearning/comments/dwnuwh/d_dannet_the_cuda_cnn_of_dan_ciresan_in_jurgen/,siddarth2947,1573806466,"probably many do not know this, I learned it by studying the references in section 19 of Jurgen's very dense [inaugural tweet](https://twitter.com/SchmidhuberAI/status/1180035193962401792) 

I knew AlexNet, the CUDA CNN by Alex Krizhevsky and Ilya Sutskever and Geoff Hinton which won ImageNet 2012, but prior to AlexNet, Jurgen's team with his ""outstanding Romanian postdoc Dan Ciresan ... won 4 important computer vision competitions in a row between May 15, 2011, and September 10, 2012"" with an earlier CUDA CNN, let me call this DanNet, the [blog post on their miraculous year](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html) links to a [summary of these contests](http://people.idsia.ch/~juergen/computer-vision-contests-won-by-gpu-cnns.html)

I saw a news article claiming that AlexNet started a deep learning revolution in 2012, but actually the references show that DanNet was the [first superhuman CNN](http://people.idsia.ch/~juergen/superhumanpatternrecognition.html) in 2011 and also won a [medical imaging contest](http://people.idsia.ch/~juergen/first-time-deep-learning-won-medical-imaging-contest-september-2012.html) on images way bigger than AlexNet's 

the most cited DanNet paper is [CVPR July 2012](http://people.idsia.ch/~juergen/cvpr2012.pdf), 5 months before [AlexNet at NIPS 2012](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf), but earlier descriptions of DanNet appeared at [IJCAI 2011](http://people.idsia.ch/~juergen/ijcai2011.pdf) and [IJCNN 2011](http://people.idsia.ch/~juergen/ijcnn2011.pdf) 

in his blog, Jurgen also cites CNN pioneers since Fukushima 1979, and GPU implementations of neural networks since [Jung and Oh 2004](https://www.sciencedirect.com/science/article/abs/pii/S0031320304000524)

to be fair, [AlexNet](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf) cites [DanNet](https://arxiv.org/abs/1202.2745) and admits that it is similar, however, it does not mention that DanNet won all those earlier challenges

[ResNet](https://arxiv.org/abs/1512.03385) beat AlexNet on ImageNet in 2015, but ResNet is actually a special case of the earlier [highway networks](https://arxiv.org/abs/1507.06228), also invented in Jurgen's lab, the ""First Working Feedforward Networks With Over 100 Layers,"" section 4 of [The Blog](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html) links to [an overview](http://people.idsia.ch/~juergen/highway-networks.html), he credits his students Rupesh Kumar Srivastava and Klaus Greff

there was a big reddit thread on section 5 of his blog, [Jurgen's GAN of 1990](https://www.reddit.com/r/MachineLearning/comments/djju8a/d_jurgen_schmidhuber_really_had_gans_in_1990/), and everybody knows LSTM, which won contests already in 2009, section 4 of [The Blog](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html), but I think many don't know yet that his team also was first in the CUDA CNN game",45,1
758,2019-11-15,2019,11,15,17,dwnxp7,[N] World first ever computer RPG with 'dungeon master ai' and 'story engine' in the works using neural network and machine learning,https://www.reddit.com/r/MachineLearning/comments/dwnxp7/n_world_first_ever_computer_rpg_with_dungeon/,bugsixx,1573807009,"https://www.youtube.com/watch?v=tw6CUVk4mn0

What the developers want to achieve is basicaly like there would be a human dungeon master in the game that reacts to your actions in the game.",22,1
759,2019-11-15,2019,11,15,19,dwov1d,[R] Efficiently Stealing Machine Learning Models from Prediction APIs,https://www.reddit.com/r/MachineLearning/comments/dwov1d/r_efficiently_stealing_machine_learning_models/,hackingai,1573813192,Gradient Driven Adaptive Learning Rate (GDALR) is proposed to efficiently duplicate a model,8,1
760,2019-11-15,2019,11,15,19,dwovy0,Athena 850 Bi-levelling - Duralift,https://www.reddit.com/r/MachineLearning/comments/dwovy0/athena_850_bilevelling_duralift/,zoejones886,1573813333,[removed],0,1
761,2019-11-15,2019,11,15,19,dwp2g5,Rough Terrain Scissor Lift - Duralift,https://www.reddit.com/r/MachineLearning/comments/dwp2g5/rough_terrain_scissor_lift_duralift/,zoejones886,1573814529,[removed],0,1
762,2019-11-15,2019,11,15,19,dwp4v6,Kindly let us know your reviews on this in order to improve!,https://www.reddit.com/r/MachineLearning/comments/dwp4v6/kindly_let_us_know_your_reviews_on_this_in_order/,day1technologies,1573815000,,0,1
763,2019-11-15,2019,11,15,19,dwp65n,Tracked Scissor Lift - Duralift,https://www.reddit.com/r/MachineLearning/comments/dwp65n/tracked_scissor_lift_duralift/,zoejones886,1573815264,,0,1
764,2019-11-15,2019,11,15,20,dwpc6b,Introducing r/mlaicirclejerk Help build a big data set so we can deep learn how to circle jerk,https://www.reddit.com/r/MachineLearning/comments/dwpc6b/introducing_rmlaicirclejerk_help_build_a_big_data/,soupbrah,1573816348,[removed],0,1
765,2019-11-15,2019,11,15,20,dwpg55,"No Degrees, No Jobs? Fret Not, AI Can Cater to Your Needs Today",https://www.reddit.com/r/MachineLearning/comments/dwpg55/no_degrees_no_jobs_fret_not_ai_can_cater_to_your/,Albertchristopher,1573817037,,0,1
766,2019-11-15,2019,11,15,21,dwq279,Is stuff like a generative robot realistic?,https://www.reddit.com/r/MachineLearning/comments/dwq279/is_stuff_like_a_generative_robot_realistic/,solidarietanazionale,1573820687,[removed],0,1
767,2019-11-15,2019,11,15,21,dwq2h4,The essence Of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/dwq2h4/the_essence_of_machine_learning/,prih_yah,1573820729,,0,1
768,2019-11-15,2019,11,15,21,dwqd19,[P] Replicate Toronto BookCorpus,https://www.reddit.com/r/MachineLearning/comments/dwqd19/p_replicate_toronto_bookcorpus/,SynonymOfHeat,1573822316,"Hey all,

I created a small python repository called [Replicate TorontoBookCorpus](https://github.com/sgraaf/Replicate-Toronto-BookCorpus) that one can use to replicate the no-longer-available Toronto BookCorpus (TBC) dataset. 

As I'm currently doing research on transformers for my thesis, but could not find/get a copy of the original TBC dataset by any means, my only alternative was to replicate it. I figured I am not the only one with this issue, and thus made and published this small project.

As with the original TBC dataset, it only contains English-language books with at least 20k words. Furthermore, the total number of words in the replica dataset is also slightly over 0.9B. All in all, if you follow the steps outlined in the repository, you end up with a 5Gb text file with one sentence per line (and three blank sentences between books).

PS. If you have a copy of the original TBC dataset, please get in touch with me (I am desperately looking for the original)!",3,1
769,2019-11-15,2019,11,15,22,dwqu1i,Everwild  /YOULIKEBET,https://www.reddit.com/r/MachineLearning/comments/dwqu1i/everwild__youlikebet/,hiyova2513a,1573824786,[removed],1,1
770,2019-11-15,2019,11,15,22,dwquqj,"Question on ML architecture, considering restrictions that were imposed",https://www.reddit.com/r/MachineLearning/comments/dwquqj/question_on_ml_architecture_considering/,AlphaMelciados,1573824884,[removed],0,1
771,2019-11-15,2019,11,15,22,dwqyan,How to develop a Faceapp like application in limited computational resources,https://www.reddit.com/r/MachineLearning/comments/dwqyan/how_to_develop_a_faceapp_like_application_in/,ANil1729,1573825381,[removed],0,1
772,2019-11-15,2019,11,15,23,dwrcgd,Twitter Data,https://www.reddit.com/r/MachineLearning/comments/dwrcgd/twitter_data/,astante13,1573827274,[removed],0,1
773,2019-11-15,2019,11,15,23,dwrd9p,i need a creative idea 4 it,https://www.reddit.com/r/MachineLearning/comments/dwrd9p/i_need_a_creative_idea_4_it/,sparof,1573827395,,0,1
774,2019-11-15,2019,11,15,23,dwrda3,"[N] Nvidia claims the worlds smallest, most powerful AI supercomputer",https://www.reddit.com/r/MachineLearning/comments/dwrda3/n_nvidia_claims_the_worlds_smallest_most_powerful/,navin49,1573827396,[removed],0,1
775,2019-11-15,2019,11,15,23,dwrnia,Probability Theory: The Logic of Science: Is it good for learning Bayesian Statistics? Would like to know your opinion,https://www.reddit.com/r/MachineLearning/comments/dwrnia/probability_theory_the_logic_of_science_is_it/,upulbandara,1573828782,,0,1
776,2019-11-15,2019,11,15,23,dwrrgf,Creat network to predict features instead of classify ?,https://www.reddit.com/r/MachineLearning/comments/dwrrgf/creat_network_to_predict_features_instead_of/,copythatpasta,1573829322,[removed],0,1
777,2019-11-16,2019,11,16,0,dws0u0,[Discussion] Creat network to predict features instead of classify ?,https://www.reddit.com/r/MachineLearning/comments/dws0u0/discussion_creat_network_to_predict_features/,copythatpasta,1573830506,"
Am new to ML, background in signal processing and traditional CS. Have been working on classifying raw audio from my custom created datasets and I want to move into prediction and then generation(GANs)

Is there any difference between GANs and prediction ? Like if I wanted to predict the next audio sample how would my current classification CNN change? My thinking is to be able to predict the next sentence  from a recorded sentence.",3,1
778,2019-11-16,2019,11,16,1,dwthjj,Estimating Depth Map With Machine Learning,https://www.reddit.com/r/MachineLearning/comments/dwthjj/estimating_depth_map_with_machine_learning/,cgpipeliner,1573836832,,0,1
779,2019-11-16,2019,11,16,2,dwubdp,Weekly Papers | EMNLP 2019 Best Paper; Facebook XLM-R and More!,https://www.reddit.com/r/MachineLearning/comments/dwubdp/weekly_papers_emnlp_2019_best_paper_facebook_xlmr/,Yuqing7,1573840452,,0,1
780,2019-11-16,2019,11,16,3,dwuh35,Researchers teach robots to use inference to complete complex tasks,https://www.reddit.com/r/MachineLearning/comments/dwuh35/researchers_teach_robots_to_use_inference_to/,jonfla,1573841145,,0,1
781,2019-11-16,2019,11,16,3,dwumf6,Does gradient descent work for any function/model?,https://www.reddit.com/r/MachineLearning/comments/dwumf6/does_gradient_descent_work_for_any_functionmodel/,Learningtosing97,1573841803,[removed],0,1
782,2019-11-16,2019,11,16,3,dwuodb,[P] Nearing BERT's accuracy on Sentiment Analysis with a model 56 times smaller by Knowledge Distillation,https://www.reddit.com/r/MachineLearning/comments/dwuodb/p_nearing_berts_accuracy_on_sentiment_analysis/,alexamadoriml,1573842041,"Hello everyone,

I recently trained a tiny bidirectional LSTM model to achieve high accuracy on Stanford's SST-2 by using knowledge distillation and data augmentation. The accuracy is comparable to BERT after fine-tuning, but the model is small enough to run at hundreds of iterations per second on a laptop CPU core. I believe this approach could be very useful since most user-devices in the world are low-power.

I believe this can also give some insight into the success of huggingface's DistilBERT, as it seems their success doesn't stem solely from knowledge distillation but also from the Transformer's unique architecture and the clever way they initialize its weights.

If you have any questions or insights, please share :)  


For more details please take a look at the article:

[https://blog.floydhub.com/knowledge-distillation/](https://blog.floydhub.com/knowledge-distillation/)",19,1
783,2019-11-16,2019,11,16,3,dwupo0,New Solutions for Quantum Gravity with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/dwupo0/new_solutions_for_quantum_gravity_with_tensorflow/,sjoerdapp,1573842206,,0,1
784,2019-11-16,2019,11,16,3,dwutwj,[Project]How to use search algorithms instead of backpropagation??,https://www.reddit.com/r/MachineLearning/comments/dwutwj/projecthow_to_use_search_algorithms_instead_of/,suhas_bn_1412,1573842736,"So basically I have to build a neural network but instead of backpropagation , i should use any search algorithm ( hill climbing , simulated annealing) . I have no idea on how to implement that. Can you guys help me out. Any help is appreciated.",13,1
785,2019-11-16,2019,11,16,4,dwvyh5,Perhaps we need to re-examine the atomic artificial neutron model.,https://www.reddit.com/r/MachineLearning/comments/dwvyh5/perhaps_we_need_to_reexamine_the_atomic/,mehum,1573847697,,0,1
786,2019-11-16,2019,11,16,5,dww889,DeepMind Research Lead Doina Precup On Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/dww889/deepmind_research_lead_doina_precup_on/,Yuqing7,1573848872,,0,1
787,2019-11-16,2019,11,16,5,dwwb5n,[N] XNNPACK: High-performance floating-point inference for mobile,https://www.reddit.com/r/MachineLearning/comments/dwwb5n/n_xnnpack_highperformance_floatingpoint_inference/,Nimitz14,1573849232,"https://github.com/google/XNNPACK

Recently found out about this. It works well and is easy to integrate (unlike Arm ComputeLibrary).

Wondering when pytorch will switch to it because NNPACK is definitely slower.",0,1
788,2019-11-16,2019,11,16,6,dwxe3o,Why are ROC curves better for imbalanced datasets?,https://www.reddit.com/r/MachineLearning/comments/dwxe3o/why_are_roc_curves_better_for_imbalanced_datasets/,defaultXusername,1573854077,[removed],0,1
789,2019-11-16,2019,11,16,6,dwxljb,Why are ROC curves better for imbalanced datasets?,https://www.reddit.com/r/MachineLearning/comments/dwxljb/why_are_roc_curves_better_for_imbalanced_datasets/,WishIWereNerd,1573854996,[removed],0,1
790,2019-11-16,2019,11,16,7,dwxwjk,[P] How to tackle a real-world problem with GuidedLDA,https://www.reddit.com/r/MachineLearning/comments/dwxwjk/p_how_to_tackle_a_realworld_problem_with_guidedlda/,hszafarek,1573856362,,0,1
791,2019-11-16,2019,11,16,7,dwy0xe,[P] I'm 16 and I made my own ML library!,https://www.reddit.com/r/MachineLearning/comments/dwy0xe/p_im_16_and_i_made_my_own_ml_library/,16yoMLDev,1573856908,"Hi, I made a ML library using only the c++17 standard library over the summer!

Take a look at it on product hunt: github.com/bkkaggle/L2

And on GitHub: github.com/bkkaggle/L2

Let me know what you think!",34,1
792,2019-11-16,2019,11,16,7,dwycg3,Generative Adversarial Networks research,https://www.reddit.com/r/MachineLearning/comments/dwycg3/generative_adversarial_networks_research/,dontuseyourreal_name,1573858373,[removed],0,1
793,2019-11-16,2019,11,16,8,dwyloe,I want to develop an AI to take a sketch and make a full illustration of it.,https://www.reddit.com/r/MachineLearning/comments/dwyloe/i_want_to_develop_an_ai_to_take_a_sketch_and_make/,ca3games,1573859560,[removed],0,1
794,2019-11-16,2019,11,16,8,dwywzl,Distilling knowledge from neural networks to build smaller and faster models,https://www.reddit.com/r/MachineLearning/comments/dwywzl/distilling_knowledge_from_neural_networks_to/,HN_Crosspost_Bot,1573861036,,0,1
795,2019-11-16,2019,11,16,9,dwzb81,[R] - Training GANs with a pre-trained Discriminator?,https://www.reddit.com/r/MachineLearning/comments/dwzb81/r_training_gans_with_a_pretrained_discriminator/,abello966,1573862987,"Hello everyone!

I'm just wondering whether someone had already tried to use a pre-trained classifier or other kind of neural network as part of or as a starting point for the discriminator of a GAN. I thought maybe it could help stabilize or speed up training, specially if you freeze the first layers (it'd be a mix of the perceptual loss and the adversarial loss I guess?)

Tried to search a little on Google Scholar or Arxiv and couldn't find anything. Has this been done before? What do you think of this idea?",5,1
796,2019-11-16,2019,11,16,9,dwzrd7,[P] An Open-Source Course on Hacking Neural Networks,https://www.reddit.com/r/MachineLearning/comments/dwzrd7/p_an_opensource_course_on_hacking_neural_networks/,kayzaks,1573865194,"For the past few months I've been working on a small introduction on how to hack with and into neural networks. 

The entire course (Article + Exercises) can be found here:

[https://github.com/Kayzaks/HackingNeuralNetworks](https://github.com/Kayzaks/HackingNeuralNetworks) 

Abstract:

&gt;A large chunk of research on the security issues of neural networks is focused on adversarial attacks. However, there exists a vast sea of simpler attacks one can perform both against and with neural networks. In this article, we give a quick introduction on how deep learning in security works and explore the basic methods of exploitation, but also look at the oensive capabilities deep learning enabled tools provide. All presented attacks, such as backdooring, GPU-based buer overows or automated bug hunting, are accompanied by short open-source exercises for anyone to try out.

&amp;#x200B;

The course is more aimed towards Security Experts that want to learn about how they can use/misuse neural networks, rather than ML researchers.

  
I think the exercises are the best part of the project at the moment. The article itself is fine in my opinion, but the introduction to neural networks isn't all that great (I've been thinking about taking it out completely). 

&amp;#x200B;

Would love to hear what you guys think about it! Any feedback is greatly appreciated.",5,1
797,2019-11-16,2019,11,16,11,dx0rhl,How Much Is Plastic Water Tank Making Machine,https://www.reddit.com/r/MachineLearning/comments/dx0rhl/how_much_is_plastic_water_tank_making_machine/,7lididi7,1573870503,,0,1
798,2019-11-16,2019,11,16,12,dx1chg,Carton Glue box machine for instant coffee tea cartoner cartoning machi...,https://www.reddit.com/r/MachineLearning/comments/dx1chg/carton_glue_box_machine_for_instant_coffee_tea/,Jochamp-Machinery,1573873666,,0,1
799,2019-11-16,2019,11,16,14,dx2m7j,"[D] Frameworks used at NeurIPS 2018-2019: PyTorch 68-&gt;166, TensorFlow 43 -&gt; 91",https://www.reddit.com/r/MachineLearning/comments/dx2m7j/d_frameworks_used_at_neurips_20182019_pytorch/,programmerChilli,1573881085,"Apologies if I've been spamming the subreddit with these threads.

However, some people raised some doubts about my data, wondering whether PyTorch's ascension applied only to NLP conferences. With these recent NeurIPS figures, it's clear that the answer is no.",3,1
800,2019-11-16,2019,11,16,14,dx2vuh,"[D] Machine Learning Frameworks used at NeurIPS 2019: PyTorch 68 -&gt; 166, TensorFlow 91 -&gt; 74",https://www.reddit.com/r/MachineLearning/comments/dx2vuh/d_machine_learning_frameworks_used_at_neurips/,programmerChilli,1573882821,"Apologies if I've been spamming the subreddit with these threads.

When I originally posted my [article comparing PyTorch's and Tensorflow's growth](https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/), some people questioned whether the results only extended to NLP conferences. 

Judging from these results at NeurIPS, the answer is no. 

Some people still have the narrative that dynamic graphs/PyTorch are only popular in the NLP community. It's clear that that's not the case.

https://twitter.com/cHHillee/status/1195568939381776386",64,1
801,2019-11-16,2019,11,16,15,dx3bvx,[D] Would something like this be feasible?,https://www.reddit.com/r/MachineLearning/comments/dx3bvx/d_would_something_like_this_be_feasible/,ehtsu,1573885740,"What if someone wore a brain scanner / portable FMRI machine on their head, as well as a Google Glass type device with a camera, microphone, etc., and then just went about their lives for some significant amount of time. Then, since there would be so much labeled data with a correspondence from their brain imaging to the major sensory inputs they were experiencing at the time, you could create a model that could then reconstruct their real-time brain imaging into a video that others could watch. So, theoretically, you could do things like translate brain activity in someone's dreams into a video, or otherwise read people's minds.",5,1
802,2019-11-16,2019,11,16,15,dx3j97,Confusion Matrix and Performance Measures in ML,https://www.reddit.com/r/MachineLearning/comments/dx3j97/confusion_matrix_and_performance_measures_in_ml/,saruque,1573887192,,0,1
803,2019-11-16,2019,11,16,16,dx3qav,The Easy Way to Extend Pandas API,https://www.reddit.com/r/MachineLearning/comments/dx3qav/the_easy_way_to_extend_pandas_api/,eyaltrabelsi,1573888630,,0,1
804,2019-11-16,2019,11,16,16,dx3tvi,Powering SQL With Machine Learning Capabilities,https://www.reddit.com/r/MachineLearning/comments/dx3tvi/powering_sql_with_machine_learning_capabilities/,eyaltrabelsi,1573889388,,0,1
805,2019-11-16,2019,11,16,16,dx3u5j,Linear Regression vs Logistic Regression - Javatpoint,https://www.reddit.com/r/MachineLearning/comments/dx3u5j/linear_regression_vs_logistic_regression/,nehapandey01,1573889449,,0,1
806,2019-11-16,2019,11,16,16,dx42cl,Can I go for CS with a specialization in ML (MS/PhD) with my math-stat background?,https://www.reddit.com/r/MachineLearning/comments/dx42cl/can_i_go_for_cs_with_a_specialization_in_ml_msphd/,anonymous_guyy,1573891143,[removed],0,1
807,2019-11-16,2019,11,16,17,dx45gv,[Self-Post]Nothing but NumPy: Understanding &amp; Creating Binary Classification Neural Networks with Computational Graphs from Scratch,https://www.reddit.com/r/MachineLearning/comments/dx45gv/selfpostnothing_but_numpy_understanding_creating/,rafay_a_k,1573891797,[removed],0,1
808,2019-11-16,2019,11,16,17,dx4aca,"Audio, Video Transcription, Translation (text/audio), Foreign Subtitles, and Captions",https://www.reddit.com/r/MachineLearning/comments/dx4aca/audio_video_transcription_translation_textaudio/,DoloresDrawing,1573892804,,0,1
809,2019-11-16,2019,11,16,17,dx4his,[R] Resizable Neural Networks,https://www.reddit.com/r/MachineLearning/comments/dx4his/r_resizable_neural_networks/,xternalz,1573894355,"&gt;**Abstract:**  In this paper, we present a deep convolutional neural network (CNN) which performs arbitrary resize operation on intermediate feature map resolution at stage-level. Motivated by weight sharing mechanism in neural architecture search, where a super-network is trained and sub-networks inherit the weights from the super-network, we present a novel CNN approach. We construct a spatial super-network which consists of multiple sub-networks, where each sub-network is a single scale network that obtain a unique spatial configuration, the convolutional layers are shared across all sub-networks. Such network, named as Resizable Neural Networks, are equivalent to training infinite single scale networks, but has no extra computational cost. Moreover, we present a training algorithm such that all sub-networks achieve better performance than individually trained counterparts. On large-scale ImageNet classification, we demonstrate its effectiveness on various modern network architectures such as MobileNet, ShuffleNet, and ResNet.   To go even further, we present three variants of resizable networks: 1) Resizable as Architecture Search (Resizable-NAS). On ImageNet, Resizable-NAS ResNet-50 attain 0.4% higher on accuracy and 44% smaller than the baseline model. 2) Resizable as Data Augmentation (Resizable-Aug). While we use resizable networks as a data augmentation technique, it obtains superior performance on ImageNet classification, outperform AutoAugment by 1.2% with ResNet-50. 3) Adaptive Resizable Network (Resizable-Adapt). We introduce the adaptive resizable networks as dynamic networks, which further improve the performance with less computational cost via data-dependent inference.",3,1
810,2019-11-16,2019,11,16,18,dx4kj0,Pruning for object detection,https://www.reddit.com/r/MachineLearning/comments/dx4kj0/pruning_for_object_detection/,aisanity,1573895008,Has anyone here done structured pruning for object detection tasks and managed to achieve good results (minimal time with maximum accuracy)?,0,1
811,2019-11-16,2019,11,16,18,dx4t1x,Machine Learning Tutorial 4 - Logistic Regression in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/dx4t1x/machine_learning_tutorial_4_logistic_regression/,ProgrammingKnowledge,1573896832,,0,1
812,2019-11-16,2019,11,16,18,dx4wsx,Machine Learning Learning Path,https://www.reddit.com/r/MachineLearning/comments/dx4wsx/machine_learning_learning_path/,KYM1912,1573897626,[removed],0,1
813,2019-11-16,2019,11,16,19,dx570f,[D] NN Pruning for Object Detection?,https://www.reddit.com/r/MachineLearning/comments/dx570f/d_nn_pruning_for_object_detection/,aisanity,1573899711,Has anyone here done structured pruning for object detection tasks and managed to achieve good results (minimal time with maximum accuracy)?,2,1
814,2019-11-16,2019,11,16,20,dx5lv7,Polynomial regression,https://www.reddit.com/r/MachineLearning/comments/dx5lv7/polynomial_regression/,Ajan121,1573902583,[removed],0,1
815,2019-11-16,2019,11,16,21,dx67ar,3d renders as training material?,https://www.reddit.com/r/MachineLearning/comments/dx67ar/3d_renders_as_training_material/,neukStari,1573906597,[removed],0,1
816,2019-11-16,2019,11,16,23,dx7vfy,TIL a dog was revealed in 1908 to be a fake hero. It was pushing children into the river Seine to rescue them and win beef steaks.,https://www.reddit.com/r/MachineLearning/comments/dx7vfy/til_a_dog_was_revealed_in_1908_to_be_a_fake_hero/,fajiruu,1573916297,,0,1
817,2019-11-17,2019,11,17,0,dx8h87,"high bias, high variance problem or is it just right ?",https://www.reddit.com/r/MachineLearning/comments/dx8h87/high_bias_high_variance_problem_or_is_it_just/,lovepeacejoy4,1573919272,[removed],0,1
818,2019-11-17,2019,11,17,0,dx8hli,Why is Sagemaker 10x faster than google cloud at 1/10th the price? Am I doing something wrong?,https://www.reddit.com/r/MachineLearning/comments/dx8hli/why_is_sagemaker_10x_faster_than_google_cloud_at/,AmittOfficial,1573919327,[removed],0,1
819,2019-11-17,2019,11,17,1,dx9162,How to implement this?,https://www.reddit.com/r/MachineLearning/comments/dx9162/how_to_implement_this/,mangotree90210,1573921877,[removed],0,1
820,2019-11-17,2019,11,17,1,dx977q,"[D] Trying to use bert for simple classification, but it doesnt work, please help",https://www.reddit.com/r/MachineLearning/comments/dx977q/d_trying_to_use_bert_for_simple_classification/,hadaev,1573922658,"So, the idea is simple: feed bert context vector to gru layer.

    class Model(torch.nn.Module):
        def __init__(self):
            super(Model, self).__init__()
            self.bert = BertModel.from_pretrained('bert-base-uncased')
            freeze(self.bert)
            self.rnn = torch.nn.GRU(768, 128, 1,
                               batch_first=True, bidirectional=False)
            self.linear = torch.nn.Linear(128, 5)

        def forward(self, x, lengths):
            x, _ = self.bert(x)
            x = torch.nn.utils.rnn.pack_padded_sequence(
                x, lengths, batch_first=True)
            self.rnn.flatten_parameters()
            _, x = self.rnn(x)
            x = self.linear(x.squeeze(0))
            return x

But model loss do not decrease.

[Here is Colab link](https://colab.research.google.com/drive/14XZ3Fb0E8KCGRStfkgAy9LolJqCCrHxP/)",3,1
821,2019-11-17,2019,11,17,7,dxdg28,How do I properly preprocess frames?,https://www.reddit.com/r/MachineLearning/comments/dxdg28/how_do_i_properly_preprocess_frames/,pitin753,1573941662,[removed],0,1
822,2019-11-17,2019,11,17,7,dxdrcd,GPT-2 and the future of writing,https://www.reddit.com/r/MachineLearning/comments/dxdrcd/gpt2_and_the_future_of_writing/,sunstah,1573943084,[removed],0,1
823,2019-11-17,2019,11,17,7,dxe26m,Andrew Ngs Machine Learning Course Maths,https://www.reddit.com/r/MachineLearning/comments/dxe26m/andrew_ngs_machine_learning_course_maths/,promiscouspleasure,1573944499,[removed],0,1
824,2019-11-17,2019,11,17,7,dxe6rj,[D] Where are withdrawn ICLR papers being submitted to?,https://www.reddit.com/r/MachineLearning/comments/dxe6rj/d_where_are_withdrawn_iclr_papers_being_submitted/,tablehoarder,1573945087,"There's a fair numbers of papers that been withdrawn from ICLR yesterday and today, including many with 6/3/3 scores which isn't that bad. At first I assumed they would be submitted to CVPR, but many of these papers are on optimization, robustness, pruning and other topics which I'd guess are not very suitable for CVPR.

Where are these papers being sent to? I got bad scores on one of my papers and would like to submit it to another conference, but I don't think CVPR makes sense for me specifically.",11,1
825,2019-11-17,2019,11,17,8,dxecli,Any free and good tutorial?,https://www.reddit.com/r/MachineLearning/comments/dxecli/any_free_and_good_tutorial/,Messatsu92,1573945841,[removed],0,1
826,2019-11-17,2019,11,17,8,dxevk2,Applied Statistics or Machine Learning Master,https://www.reddit.com/r/MachineLearning/comments/dxevk2/applied_statistics_or_machine_learning_master/,baldinggerman,1573948331,[removed],0,1
827,2019-11-17,2019,11,17,9,dxezer,Need help about setting parameters of my ANN,https://www.reddit.com/r/MachineLearning/comments/dxezer/need_help_about_setting_parameters_of_my_ann/,MadEngineerBR,1573948842,[removed],3,1
828,2019-11-17,2019,11,17,9,dxfjhm,Examples of repository that has a good presentation of machine learning projects.,https://www.reddit.com/r/MachineLearning/comments/dxfjhm/examples_of_repository_that_has_a_good/,FelipwMarcelino,1573951575,[removed],0,1
829,2019-11-17,2019,11,17,9,dxfmtp,Help request in finding a link,https://www.reddit.com/r/MachineLearning/comments/dxfmtp/help_request_in_finding_a_link/,clone290595,1573952037,[removed],0,1
830,2019-11-17,2019,11,17,10,dxfuca,Python Sets,https://www.reddit.com/r/MachineLearning/comments/dxfuca/python_sets/,iramirsina,1573953091,,0,1
831,2019-11-17,2019,11,17,10,dxg0kb,[D] Lets Talk About Vicarious Inc.,https://www.reddit.com/r/MachineLearning/comments/dxg0kb/d_lets_talk_about_vicarious_inc/,Chromobacterium,1573953955,"Lately, I have been reading some of the papers from Vicarious Inc. Some of you may have read about the (over-hyped) Recursive Cortical Network (RCN) or Schema Network. Some of you found it very [infuriating](https://www.reddit.com/r/MachineLearning/comments/6h8q4z/r_general_game_playing_with_schema_networks/), while others were [skeptical](https://www.reddit.com/r/MachineLearning/comments/1pe38n/yann_lecun_gives_some_points_of_why_we_should_be/) about the company.

I will be honest, the papers they present were good and bad in their own ways. The RCN paper does prove that generative modelling is superior to traditional CNNs. Then again, they did not test it on a major dataset like Imagenet (the compilers of the Omniglot dataset performed their own experiments using the [RCN](https://arxiv.org/pdf/1902.03477.pdf) recently). The Schema Network sounded revolutionary when it performed ""zero-shot knowledge transfer"" using a model based algorithm. They never benchmarked meta-reinforcement learning algorithms. On top of that, they did not use the RCN to parse proper entities when playing breakout (it was mentioned that they assumed an object based vision system, referring to their generative vision model). I find their breakthroughs interesting, but I also feel that they are way too hyped by the media when other attention deserving work remain lost on arxiv. What do you guys think?",12,1
832,2019-11-17,2019,11,17,11,dxgr40,[P] baikal: A graph-based functional API for building complex scikit-learn pipelines,https://www.reddit.com/r/MachineLearning/comments/dxgr40/p_baikal_a_graphbased_functional_api_for_building/,algnz,1573957746,"Hello everyone.

I'd like to share a project I've been working on: [https://github.com/alegonz/baikal](https://github.com/alegonz/baikal)

baikal is a graph-based, functional API for building complex machine learning pipelines of objects that implement the scikit-learn API.

It aims to make easier the process of building, fitting, predicting with, and querying complex pipelines, while making the code more readable, less verbose and closer to our mental representation of the pipeline.

You can think of it as ""scikit-learn meets Keras"". With it you can write a pipeline that looks like this:

![img](ubnb82upp5z31 ""Some contrived pipeline for illustrative purposes"")

with code that looks like this:

    x1 = Input()
    x2 = Input()
    y_t = Input()
    
    y1 = ExtraTreesClassifier()(x1, y_t)
    y2 = RandomForestClassifier()(x2, y_t)
    z = PowerTransformer()(x2)
    z = PCA()(z)
    y3 = LogisticRegression()(z, y_t)
    
    ensemble_features = Stack()([y1, y2, y3])
    y = SVC()(ensemble_features, y_t)
    
    model = Model([x1, x2], y, y_t)

A more detailed explanation of the project is in the README, and there are also some examples.

It is available from PyPI if you would like to give it a try (some features are still work-in-progress, though). Any feedback is greatly appreciated :)",37,1
833,2019-11-17,2019,11,17,11,dxgxhu,Which are the hot topics nowadays?,https://www.reddit.com/r/MachineLearning/comments/dxgxhu/which_are_the_hot_topics_nowadays/,lendacerda,1573958652,[removed],0,1
834,2019-11-17,2019,11,17,11,dxgz8v,[D] Recommendations about users and repository that has good organization in ML projects on Github,https://www.reddit.com/r/MachineLearning/comments/dxgz8v/d_recommendations_about_users_and_repository_that/,FelipeMarcelino,1573958931,"Hello, I work with machine learning, and I have personal projects. However, I don't know how to presentation these projects in GitHub. I would emphasis on Jupyter Notebook only or using some template likes [Cookiecutter](https://github.com/drivendata/cookiecutter-data-science). Have you been watching some users or repository that has a good organization on ML Projects?",8,1
835,2019-11-17,2019,11,17,12,dxh8ov,Is Multi-Headed Attention similar to CapsNets,https://www.reddit.com/r/MachineLearning/comments/dxh8ov/is_multiheaded_attention_similar_to_capsnets/,ZeroMaxinumXZ,1573960254,,0,1
836,2019-11-17,2019,11,17,14,dxihr2,Generative adversarial networks (GAN) based efficient sampling of chemical space for inverse design of inorganic materials,https://www.reddit.com/r/MachineLearning/comments/dxihr2/generative_adversarial_networks_gan_based/,stockwow,1573967557,"The power of Generative adversarial network(GAN) to learn implicit atomic/chemical composition rules of inorganic materials. 

&amp;#x200B;

 

&gt;A major challenge in materials design is how to efficiently search the vast chemical design space to find the materials with desired properties. One effective strategy is to develop sampling algorithms that can exploit both explicit chemical knowledge and implicit composition rules embodied in the large materials database. Here, we propose a generative machine learning model (MatGAN) based on a generative adversarial network (GAN) for efficient generation of new hypothetical inorganic materials. Trained with materials from the ICSD database, our GAN model can generate hypothetical materials not existing in the training dataset, reaching a novelty of 92.53% when generating 2 million samples. The percentage of chemically valid (charge neutral and electronegativity balanced) samples out of all generated ones reaches 84.5% by our GAN when trained with materials from ICSD even though no such chemical rules are explicitly enforced in our GAN model, indicating its capability to learn implicit chemical composition rules. Our algorithm could be used to speed up inverse design or computational screening of inorganic materials.",0,1
837,2019-11-17,2019,11,17,15,dxj2qc,[D] TIL something about Google Photos search that I'm ambivalent about,https://www.reddit.com/r/MachineLearning/comments/dxj2qc/d_til_something_about_google_photos_search_that/,vPyDev,1573971418,,0,1
838,2019-11-17,2019,11,17,15,dxj6z3,[P] ML consults on building medical decision assistance.,https://www.reddit.com/r/MachineLearning/comments/dxj6z3/p_ml_consults_on_building_medical_decision/,spacepope3,1573972267,"Hey, 

Long-time lurker here :)

I'm working on a project in the UK and USA on building medical decision assistance to help doctors in hospitals with management decisions. The overarching aim is to build medical assistants that can help doctors deliver ideal medical care anywhere on the planet (or even space). These assistants will one day ensure that the clinicians taking care of us make fewer mistakes, deliver multi-specialty expert care (rather than bouncing between single-specialty doctors who can't put everything together), understand and appraise the evidence and apply it us as individuals AND reduce the cost of medical care.

A little about me: I am a hospital physician with an engineering and technical background. I also advise the Centres for Disease Control and Prevention initiative on digitizing medicine and building clinical decision assistance. 

The project is focusing initially on building assistance for stroke hyperacute thrombolysis decisions. Reasons for this choice include: it's a binary decision (to give or not to give), has relatively few research papers to understand, access to domain knowledge, access to data and access to testing sites. 

ELI5: A stroke is when an area gets damaged in the brain due to a blockage in the blood flow to that area of the brain, usually in the form of a blood clot. In some circumstances, it is possible to unblock and prevent brain damage by dissolving the clot with a strong unblocking drug (or sometimes fishing the clot out with a wire). We want to see if we can train a program that can help doctors make better-informed decisions when deciding what treatment to give the patient. 

I am hoping to get together a small group to consult if anyone is interested in applying ML in the medical field. Thanks!",7,1
839,2019-11-17,2019,11,17,17,dxjw7o,Into machine learning need friend to talk about ml,https://www.reddit.com/r/MachineLearning/comments/dxjw7o/into_machine_learning_need_friend_to_talk_about_ml/,jaouadbialach,1573977638,[removed],0,1
840,2019-11-17,2019,11,17,19,dxkz8w,Recommend books about ML? [R],https://www.reddit.com/r/MachineLearning/comments/dxkz8w/recommend_books_about_ml_r/,welshpineapple,1573986244,Whats everyone reading atm about ML?,13,1
841,2019-11-17,2019,11,17,19,dxl0i7,Comparing Data Science Masters,https://www.reddit.com/r/MachineLearning/comments/dxl0i7/comparing_data_science_masters/,dhruvrnaik,1573986517,[removed],0,1
842,2019-11-17,2019,11,17,20,dxl974,Suggestion for Time-Series dataset Non-Stationary with Multiple Features,https://www.reddit.com/r/MachineLearning/comments/dxl974/suggestion_for_timeseries_dataset_nonstationary/,ebuzz168,1573988433,[removed],0,1
843,2019-11-17,2019,11,17,20,dxlfym,Some issues with a basic R problem,https://www.reddit.com/r/MachineLearning/comments/dxlfym/some_issues_with_a_basic_r_problem/,Firingam,1573989856,"Hi everyone. I'm dealing with a fitting problem where I have to Fit a sequence of natural spline

 models with degrees of freedom from 1 to 40 to the set of data shown below

and illustrate how the degrees of freedom control the bias-variance trade-off.

&amp;#x200B;

They give me  this code to create the learning set

set.seed(2701)

generated= function(n){

x &lt;- sort(runif(n,0,8))

eps &lt;- rnorm(n,0,0.5)

y=sin(x)+eps

return(data.frame(x=x,y=y))

}

n=1000

xy=generated(n)  

I'm a rookie with R so I can't get what further steps could possibly be.

&amp;#x200B;

Thank you in advance!",0,1
844,2019-11-17,2019,11,17,20,dxlmow,Pivotal Role of AI and Machine Learning in Industry 4.0 and Manufacturing,https://www.reddit.com/r/MachineLearning/comments/dxlmow/pivotal_role_of_ai_and_machine_learning_in/,ZIF_AI,1573991273,,0,1
845,2019-11-17,2019,11,17,20,dxlptf,[P] How to Run TensorFlow Lite on Raspberry Pi for Object Detection,https://www.reddit.com/r/MachineLearning/comments/dxlptf/p_how_to_run_tensorflow_lite_on_raspberry_pi_for/,Taxi-guy,1573991917,,0,1
846,2019-11-17,2019,11,17,21,dxlrhw,[P] Pedestrian Attribute Recognition baseline with reproducibility in mind based on Pytorch Lightning,https://www.reddit.com/r/MachineLearning/comments/dxlrhw/p_pedestrian_attribute_recognition_baseline_with/,xxxzzzlll,1573992251,,0,1
847,2019-11-17,2019,11,17,21,dxltck,Tips and explaination,https://www.reddit.com/r/MachineLearning/comments/dxltck/tips_and_explaination/,Xriiis,1573992595,"Hi y'all, I'm an IT student and i'm currently following a machine learning class, the struggle is real, I'd like to know if there is someone here that could help me time to time when I have a question, for now i'm trying to understand the outliers, elbow concept and silhouette analysis, Thanks you in advance :)",0,1
848,2019-11-17,2019,11,17,22,dxmnnz,"This video explains exactly how convolutional neural networks work, with a cool implementation. The code is written in Python and implemented with Keras. Hopefully this will help you understand anything that you do not already understand about them.",https://www.reddit.com/r/MachineLearning/comments/dxmnnz/this_video_explains_exactly_how_convolutional/,antaloaalonso,1573998039,,0,1
849,2019-11-17,2019,11,17,22,dxmox5,The song Paperblade synced with GAN images,https://www.reddit.com/r/MachineLearning/comments/dxmox5/the_song_paperblade_synced_with_gan_images/,LiquidVibes,1573998259,,0,1
850,2019-11-17,2019,11,17,23,dxngwq,Which are the hot areas for ML nowadays?,https://www.reddit.com/r/MachineLearning/comments/dxngwq/which_are_the_hot_areas_for_ml_nowadays/,lendacerda,1574002683,[removed],0,1
851,2019-11-18,2019,11,18,0,dxo1lz,Motion: All CVF conferences will have a dataset review process for papers promising a dataset,https://www.reddit.com/r/MachineLearning/comments/dxo1lz/motion_all_cvf_conferences_will_have_a_dataset/,AlleUndKalle,1574005502,[removed],0,1
852,2019-11-18,2019,11,18,0,dxo4rz,Data augmentation - list of resources,https://www.reddit.com/r/MachineLearning/comments/dxo4rz/data_augmentation_list_of_resources/,Chitoyo,1574005919,[removed],0,1
853,2019-11-18,2019,11,18,1,dxp1be,[D] NeurIPS proceedings - what are is the image reuse copyright license?,https://www.reddit.com/r/MachineLearning/comments/dxp1be/d_neurips_proceedings_what_are_is_the_image_reuse/,eigenlaplace,1574009910,I'm writing my MSc thesis and would like to use a couple of images from a NeurIPS paper. What should I follow to avoid copyright issues? I can't find any licensing information on their website,9,1
854,2019-11-18,2019,11,18,2,dxp8ka,"[D] Statistical/ML analysis of intention + wordnets, phrasenets",https://www.reddit.com/r/MachineLearning/comments/dxp8ka/d_statisticalml_analysis_of_intention_wordnets/,Live_Think_Diagnosis,1574010727,"I'm having a mental struggle right now trying to understand how I would go about programming this, and I'm not even sure it's feasible.

## The problem

Let's say we're analyzing song lyrics. Let's say that hypothetically, whenever the word ""darkness"" is mentioned in a lyric, there is a 23% chance that the word ""night"" is also mentioned and a 14% chance that the word ""doubt"" is also in the lyric.

A second and more complex relationship would be that of phrases. We could imagine that whenever the word ""darkness"" is mentioned, there is a 3.2% chance that the phrase ""I'm scared"" is somewhere in the lyric and 0.9% chance that the phrase ""going to die"" is also there.

A third addition to the complexity would be to add sentiment analysis with a machine learning version of a wordnet that analyzes not only the related words but the related moods.

A fourth addition to the complexity would see morphosyntactical analysis. ""I'm scared"" is not a feasible assumption as there are many possible subjects in a ""scared"" sentence, but it would be more feasible for it to be frequent if we said ""noun + [to be, present tense] + scared"". This would cover ""I'm scared"", ""he's scared"", ""we're scared"", ""my son is scared"", etc. And then we could add adverbs and sentence changes ('our family is, therefore, exceptionally scared').

## The bad way

My current thoughts about it come from traditional programming where for that analysis to occur, we would grab a `reference word`, grab the rest of the `corpus words` and count each of ocurrence of each `corpus word`, then throw all of those counts into an array belonging to the `reference word` we were analyzing for, and then do that for every word in a text. That would be insanely expensive and would get nowhere.

## The ideal but unknown way

A cheaper way to do this would be with an AI + a vectorial or matrix datatype. I've been exploring the kinds of AI's that there are but I'm very new to this and don't know which one is more appropriate and which analysis algorithm would be best. I'm not even sure if it can be done with our current technology in this exact way, or whether there would be differences in the results I described. Perhaps AI would not be as accurate statistically but would instead rate analytically with a 0-100 not the statistical tendency but the ""feel"" it gets for how ""similar"" one word is to another due to their common context. How accurate would this be statistically?

---

I've been pumped recently with BERT, but I'm not experienced enough to create my own conclusions on the topic.

* How feasible do you think this would be?
* What are your thoughts about the necessary implications and existing ways to approach them?
* What similar projects are there being developed right now that you know?
* How would someone interested in this go into learning more about this specifically without much experience in machine learning in general?",1,1
855,2019-11-18,2019,11,18,3,dxpyvx,"Mechanically transformative electronics, sensors, and implantable devices",https://www.reddit.com/r/MachineLearning/comments/dxpyvx/mechanically_transformative_electronics_sensors/,dwcrmcm,1574013883,[removed],0,1
856,2019-11-18,2019,11,18,3,dxpzf3,Lacking Sample Data? Generate it,https://www.reddit.com/r/MachineLearning/comments/dxpzf3/lacking_sample_data_generate_it/,xTouny,1574013943,,0,1
857,2019-11-18,2019,11,18,3,dxq3gq,Safety is key. Violence Unlikely,https://www.reddit.com/r/MachineLearning/comments/dxq3gq/safety_is_key_violence_unlikely/,Kronosbach,1574014437,,0,1
858,2019-11-18,2019,11,18,3,dxq873,[D] Progress bar for Scikit Learn / Sklearn?,https://www.reddit.com/r/MachineLearning/comments/dxq873/d_progress_bar_for_scikit_learn_sklearn/,saint----,1574015015,"Hi! I was wondering if there's a way to get a progress bar or some form of indication of how far along a model is when using Scikit learn? I've tried using verbose=True, but it doesn't seem to do much and some models don't allow it.

Thanks!",3,1
859,2019-11-18,2019,11,18,3,dxq87f,Fine tuning Bert for QA dataset,https://www.reddit.com/r/MachineLearning/comments/dxq87f/fine_tuning_bert_for_qa_dataset/,SiddharthKothari93,1574015017,[removed],0,1
860,2019-11-18,2019,11,18,3,dxqlvu,[R] Neural Network Processing Neural Networks,https://www.reddit.com/r/MachineLearning/comments/dxqlvu/r_neural_network_processing_neural_networks/,firat_tuna,1574016655,"I would like to share some research I have been working on my spare time:

[https://arxiv.org/abs/1911.05640](https://arxiv.org/abs/1911.05640)

It is about another type of neural networks which take neural networks as inputs and/or produce them as outputs which seem to be doing well especially on search problems according to my own experiments. I would be really grateful if anyone could provide some feedback.",10,1
861,2019-11-18,2019,11,18,4,dxqt2l,I want to compare tools for a typical regression task. Would it make more sense to include Spark or Pytorch (the other ones are TensorFlow and scikit),https://www.reddit.com/r/MachineLearning/comments/dxqt2l/i_want_to_compare_tools_for_a_typical_regression/,Falkenauge,1574017496,[removed],0,1
862,2019-11-18,2019,11,18,4,dxqtdg,"Best way to output key, value pairs?",https://www.reddit.com/r/MachineLearning/comments/dxqtdg/best_way_to_output_key_value_pairs/,Natural-Wish,1574017532,"I have  to output python dict { ""key"": value} where keys are strings and values can be integers, strings, etc. What is the best way to predict the (key,value) pairs considering the fact that order is not important? What ML approach would you use in this case? (It's not limited to NNs) Thank you!",0,1
863,2019-11-18,2019,11,18,4,dxr4tx,[D] Transfer Learning for Survival Models,https://www.reddit.com/r/MachineLearning/comments/dxr4tx/d_transfer_learning_for_survival_models/,stat_leaf,1574018846,Survival models are similar to linear regression models and in this case I am using a AFT survival model. I have trained the model on one dataset and I intend to use this model to predict time to failure for another dataset. I would like to discuss on the criteria that is needed for the transfer to happen as in how the model transfer can be done and if there are approaches I can consider for this purpose. Thanks.,3,1
864,2019-11-18,2019,11,18,4,dxr8a3,[D] How Machine Learning Can Help Unlock the World of Ancient Japan,https://www.reddit.com/r/MachineLearning/comments/dxr8a3/d_how_machine_learning_can_help_unlock_the_world/,hughbzhang,1574019233,"Most surviving documents from humanity's ancient history lie dormant in storage, unreadable to all but a handful of expert scholars. Alex Lamb discusseshow researchers from the Center for Open Data in the Humanities are using ML to solve this tragedy.

[https://t.co/pnzk80VqLw?amp=1](https://t.co/pnzk80VqLw?amp=1)",0,1
865,2019-11-18,2019,11,18,4,dxrki8,[D] Is the inception architecture/block a failure?,https://www.reddit.com/r/MachineLearning/comments/dxrki8/d_is_the_inception_architectureblock_a_failure/,TheAlgorithmist99,1574020617,"While we see many direct uses of ResNet blocks and variations of the ResNet architecture being applied everywhere, but I never see anything similar with the Inception blocks, do you guys have any examples of it being use? Why is it not more used?",28,1
866,2019-11-18,2019,11,18,5,dxro83,Data Analysis vs Data Science vs Machine Learning,https://www.reddit.com/r/MachineLearning/comments/dxro83/data_analysis_vs_data_science_vs_machine_learning/,crowdforapps,1574021040,,0,1
867,2019-11-18,2019,11,18,6,dxshkg,[D] Machine Learning - WAYR (What Are You Reading) - Week 75,https://www.reddit.com/r/MachineLearning/comments/dxshkg/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,1574024405,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|
|----|-----|-----|-----|-----|-----|-----|-----|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)||||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)||

Most upvoted papers two weeks ago:

/u/adventuringraw: [original TrueSkill paper from Microsoft](https://www.microsoft.com/en-us/research/publication/trueskilltm-a-bayesian-skill-rating-system/)

/u/Grimm___: http://proceedings.mlr.press/v67/gutierrez17a/gutierrez17a.pdf

Besides that, there are no rules, have fun.",2,1
868,2019-11-18,2019,11,18,6,dxslwq,[N] Microsoft Incorporates Graphcore AI Chips in Azure Cloud,https://www.reddit.com/r/MachineLearning/comments/dxslwq/n_microsoft_incorporates_graphcore_ai_chips_in/,downtownslim,1574024903,"Graphcores AI accelerator chip, the Colossus intelligence processing unit (IPU) is now available for customers to use as part of Microsofts Azure cloud platform.

This is the first time any major cloud service provider has publicly offered customers the opportunity to run their data on an accelerator from any of the dozens of AI chip startups and as such, it represents a big win for Graphcore. Microsoft has said access will initially be prioritised for customers who are pushing the boundaries of machine learning.

Microsoft and Graphcore have been working together for two years to develop cloud systems and build enhanced vision and natural language processing models for the Graphcore IPU. In particular, the natural language processing (NLP) model, Googles BERT (bidirectional encoder representations from transformers), which is currently very popular with search engines, including Google themselves.

Using eight Graphcore IPU processor cards (each with a pair of Colossus accelerators), BERT can be trained in 56 hours, similar to the result for GPU with PyTorch, though it is faster than the GPU with TensorFlow (see graph below). Graphcore says customers are seeing BERT inference throughput increase threefold, with 20% improvement in latency.

Given the level of hype surrounding Graphcore the company is valued at $1.7 billion  these performance improvements seem rather modest. It remains to be seen whether the promised improvement is enough to tempt customers into optimising their models for the IPU.

**Advanced models**  
At the same time, Graphcore has also released some results on more advanced models, where it showed more dramatic performance improvements.

Inference on image processing model ResNext was accelerated 3.4x in terms of throughput at 18x lower latency, compared to a GPU solution consuming the same amount of power. ResNext uses a technique called group separable convolutions, which splits convolution filters into smaller separable blocks to increase accuracy while reducing the parameter count. This approach is well-suited to the IPU, Graphcore says, because of the chips massively parallel processor architecture and more flexible, high-throughput memory; smaller blocks of data can be mapped to thousands of fully independent processing threads.

Graphcore also showed good results for Markov Chain Monte Carlo (MCMC)-based models, a new type of probabilistic algorithm which is used for modelling financial markets. This type of model has been out of reach for many in the finance industry, as it was previously considered too computationally expensive to use, said Graphcore. Early access IPU customers in the finance sector have been able to train their proprietary, optimised MCMC models in 4.5 minutes on IPUs, compared to over 2 hours with their existing hardware, a 26x speed up in training time.

Reinforcement learning (RL), another popular technique in modern AI algorithm development, can also be accelerated compared to typical existing solutions. Graphcore cited a factor of ten improvement in throughput for RL models, even before they are optimised for the IPU.

&amp;#x200B;

[https://www.eetimes.com/document.asp?doc\_id=1335297#](https://www.eetimes.com/document.asp?doc_id=1335297#)",12,1
869,2019-11-18,2019,11,18,6,dxsz7a,LSTM Autoencoder not learning a proper encoding for videos,https://www.reddit.com/r/MachineLearning/comments/dxsz7a/lstm_autoencoder_not_learning_a_proper_encoding/,FreddyShrimp,1574026439,,0,1
870,2019-11-18,2019,11,18,6,dxtabl,what is the SOTA Meta-Learning Approach for Face Recognition ?,https://www.reddit.com/r/MachineLearning/comments/dxtabl/what_is_the_sota_metalearning_approach_for_face/,savan77,1574027742,[removed],0,1
871,2019-11-18,2019,11,18,7,dxtdz6,"Detecting objects on top of or in front of other objects, how would you detect them?",https://www.reddit.com/r/MachineLearning/comments/dxtdz6/detecting_objects_on_top_of_or_in_front_of_other/,djaym7,1574028172,[removed],0,1
872,2019-11-18,2019,11,18,7,dxtfl3,[D] Best resources to learn about Anomaly Detection on Big Datasets?,https://www.reddit.com/r/MachineLearning/comments/dxtfl3/d_best_resources_to_learn_about_anomaly_detection/,spq,1574028362," What  are best books, university courses or mooc to learn how to detect  outliers ? Preferably methods that are applicable to Big Data Ecosystem .

Thank you in advance.",8,1
873,2019-11-18,2019,11,18,7,dxu2j3,Evaluating regression models: can R^2 be interpreted as accuracy (higher indicates a better model)?,https://www.reddit.com/r/MachineLearning/comments/dxu2j3/evaluating_regression_models_can_r2_be/,LEG_EATER,1574031161,"I'm seeing a lot of stats when it comes to evaluating regression models. With classification, evaluating the accuracy of a model is simple: you just look at how many examples your model got right. 

But with regression it's not so simple, because an answer being within 0.0001 of the correct answer is probably a great result, but of course is not exactly the correct answer. 

Here are some of the measurements I've encountered: R^2, Mean absolute error, mean squared error, Root mean squared error, mean signed difference, mean absolute percentage error.

How should I be interpreting these? I'm guessing for MAE, MSE, RMSE, and MAPE, the lower the value the better (just basing this off the fact that ""error"" is in the name of these measurements: the lower the error the better). 

But",0,1
874,2019-11-18,2019,11,18,7,dxu4a8,Encoded representations of unique objects in Yolo,https://www.reddit.com/r/MachineLearning/comments/dxu4a8/encoded_representations_of_unique_objects_in_yolo/,l2reg,1574031396,[removed],0,1
875,2019-11-18,2019,11,18,8,dxuw2i,So where are we at on the Hype Cycle right now? Before or after the trough of disillusionment?,https://www.reddit.com/r/MachineLearning/comments/dxuw2i/so_where_are_we_at_on_the_hype_cycle_right_now/,carrolldunham,1574034881,[removed],0,1
876,2019-11-18,2019,11,18,9,dxvoac,[D] So where are we at on the Hype Cycle right now? Before or after the trough of disillusionment?,https://www.reddit.com/r/MachineLearning/comments/dxvoac/d_so_where_are_we_at_on_the_hype_cycle_right_now/,carrolldunham,1574038637,"You never hear about ML or ""AI"" in the mainstream any more, not since about 2017. AlphaGo and self-driving was exciting, then the self-driving started to struggle and people twigged that the self-play approach cannot be translated to applications where you don't have a 'game' i.e. a perfect model of the environment's response to actions. The only things that make the media now are garbage from OpenAI like ""We PuBliSheD a BoOk written by our garbled-text generator"". So I just wonder what people think about whether we are on the way down to a crash, or just past it and quietly plateauing in productivity, this time around the ""AI"" hype cycle?",15,1
877,2019-11-18,2019,11,18,10,dxw3aj,When not to use machine learning?,https://www.reddit.com/r/MachineLearning/comments/dxw3aj/when_not_to_use_machine_learning/,weihong95,1574040645,"When you are solving a problem, in what circumstances will you apply machine learning?

Is it true that in every circumstance, machine learning will always outperform rules and heuristic approaches?

In this article, I will explain using several real-world cases to illustrate why sometimes machine learning will not be the best choice to tackle a problem.

Link: [https://towardsdatascience.com/when-not-to-use-machine-learning-14ec62daacd7?source=friends\_link&amp;sk=90b0f6d1945e92f9fcdccc1d6c6a95f7](https://towardsdatascience.com/when-not-to-use-machine-learning-14ec62daacd7?source=friends_link&amp;sk=90b0f6d1945e92f9fcdccc1d6c6a95f7)

Comment below if you have any thoughts to add on!",0,1
878,2019-11-18,2019,11,18,10,dxw41j,[P] Sentence similarity using siamese LSTM,https://www.reddit.com/r/MachineLearning/comments/dxw41j/p_sentence_similarity_using_siamese_lstm/,momo11arsenal,1574040740,"So I have a project where I find the semantic sentence similarity between a dataset of two sentences. For the dataset, I use [STS-Benchmark](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark). 

 First, I used [English Wikipedia dump](http://vectors.nlpl.eu/explore/embeddings/en/models/) to create a word2vec matrix. Then I used the function text\_to\_sequence to convert my sentences to an array. 

I developped a siamese LSTM, but my problem is that the validation accuracy never increase. I'm stuck at an accuracy of 0.25 to 0.30. When I use spearman's correlation, I get a value of 45%.

Here is my code:  [https://pastebin.com/jPaZbDDM](https://pastebin.com/jPaZbDDM)",3,1
879,2019-11-18,2019,11,18,10,dxwdcd,Artemis - Laser Object Tracking,https://www.reddit.com/r/MachineLearning/comments/dxwdcd/artemis_laser_object_tracking/,nickbild,1574041978,[removed],0,1
880,2019-11-18,2019,11,18,11,dxwqlc,[R] Is there any way to incorporate dictionary definitions as features into an NLP system?,https://www.reddit.com/r/MachineLearning/comments/dxwqlc/r_is_there_any_way_to_incorporate_dictionary/,Seankala,1574043711,"I'm currently trying to get the dictionary definitions of words as features. However, I'm having trouble finding a source that actually has such a source.

I understand that there are many techniques that allow us to leverage the information that words carry (e.g. WordNet, Word2Vec, GloVe, ELMo, etc.) but these aren't exactly the ""dictionary definition"" that I'm looking for. For example, WordNet takes advantage of the hierarchical relationship among words, and Word2Vec and GloVe tell us how similar two words are. Not exactly a ""dictionary definition"" in my opinion.

Does anybody know if there exists any source out there that provides such features? Thanks in advance.",4,1
881,2019-11-18,2019,11,18,12,dxx8og,[D] Is there a tool/way/idk.. to guess what was deleted comment about basing on left responses? I mean wouldn't it be cool?,https://www.reddit.com/r/MachineLearning/comments/dxx8og/d_is_there_a_toolwayidk_to_guess_what_was_deleted/,efojs,1574046110,,0,1
882,2019-11-18,2019,11,18,12,dxxawn,[D] What should I do?,https://www.reddit.com/r/MachineLearning/comments/dxxawn/d_what_should_i_do/,OriginalMoment,1574046403,"Hi, I'm a math major at the University of Alberta, with a 3.8 gpa.

I'm not anything super special, and I think that my ability to math is pretty subpar and I'm probably not capable of doing a PhD in math. ML is something that always interested me (read a good chunk of Pattern Recognition and Machine Learning and all of Reinforcement Learning over the past year, worked as a MLE for a small stint) but I have 0 research experience.

&amp;#x200B;

My big pluses would be:  

* 3.8 GPA in mostly pure math isn't too shabby (got a B- in Real Analysis II though, which looks very *very* bad for PhD applications in pure math)
* A+ achieved in the introduction to machine learning course offered at my university
* Currently on the final stretch of an internship at LinkedIn as an Infra SWE, built a cute little compiler which generates linear algebra kernels for sparse tensors
* Going to intern at Jane Street Capital next summer (prestige wise it's pretty much the best an undergrad could do in terms of SWE)

My big minuses would be:

* 0 research experience
* B- in Real Analysis II (got an A in Real Analysis I though)
* Have not written the GRE (pretty much limits me to masters programs in Canada I think)

I'm mainly concerned with getting a PhD anywhere, I'm not too concerned with getting a PhD at somewhere prestigious. I have 3 semesters left in my degree, all of which are light (I have 4 very difficult math courses left, 1 CS course (compilers), 2 english courses and 4 arts courses which I plan to break into semesters of 4, 4, 3 courses). I have some background in compilers (read a good portion of Engineering A Compiler while I was at LinkedIn in order to do my project) which might be an interesting intersection. I'm taking a Reinforcement Learning class next semester, and I'm preparing to knock it out of the park.

&amp;#x200B;

What should I do within my last 3 semesters in order to maximize my quality of PhD acceptances come the end of my undergrad?",13,1
883,2019-11-18,2019,11,18,12,dxxdkk,why non-binary activation functions are important which most explanations miss,https://www.reddit.com/r/MachineLearning/comments/dxxdkk/why_nonbinary_activation_functions_are_important/,bookersamson,1574046765,,0,1
884,2019-11-18,2019,11,18,12,dxxn0r,"Questions about ""Momentum Contrast for Unsupervised Visual Representation Learning""",https://www.reddit.com/r/MachineLearning/comments/dxxn0r/questions_about_momentum_contrast_for/,Xiao_Wang96,1574048097,[removed],0,1
885,2019-11-18,2019,11,18,13,dxyak3,Github repositories on Deep Learning to contribute to?,https://www.reddit.com/r/MachineLearning/comments/dxyak3/github_repositories_on_deep_learning_to/,thisistakenn,1574051686,[removed],0,1
886,2019-11-18,2019,11,18,14,dxyus6,Bot Marketplace - 1,https://www.reddit.com/r/MachineLearning/comments/dxyus6/bot_marketplace_1/,getengati,1574055016,[removed],0,1
887,2019-11-18,2019,11,18,14,dxz3l4,[D] Bahdanau attention model,https://www.reddit.com/r/MachineLearning/comments/dxz3l4/d_bahdanau_attention_model/,wyzkssm,1574056500,"Hello. I am a machine learning enthusiast. Recently I got interested in NLP and I found attention model interesting. 

I was reading  [https://arxiv.org/pdf/1409.0473.pdf](https://arxiv.org/pdf/1409.0473.pdf) 

and I couldn't find how to compute Cz, Cr, and C. 

You can find them in the paper, Appendix A where they explain how to compute update gates and reset gates. 

I have searched on google but seems like people don't mention how to compute them.

&amp;#x200B;

1. How to compute Cz, Cr, and C in Bahdanau attention model?
2. Where should I ask these questions? ( I am new to Machine learning and don't have anyone to ask in person)
3. Am I focus on too much detail? Should I just use libraries which has pre-built attention models? Actually I am working on a simple chat-bot project.",9,1
888,2019-11-18,2019,11,18,14,dxz4z1,How Trip Inferences and Machine Learning Optimize Delivery Times on Uber Eats,https://www.reddit.com/r/MachineLearning/comments/dxz4z1/how_trip_inferences_and_machine_learning_optimize/,Vikram8,1574056738,,0,1
889,2019-11-18,2019,11,18,15,dxzb5g,Want to learn ML from scratch,https://www.reddit.com/r/MachineLearning/comments/dxzb5g/want_to_learn_ml_from_scratch/,scowzy,1574057814,[removed],0,1
890,2019-11-18,2019,11,18,15,dxznfs,Collaborative Research: ANNs,https://www.reddit.com/r/MachineLearning/comments/dxznfs/collaborative_research_anns/,rabbitcarrots,1574060010,[removed],0,1
891,2019-11-18,2019,11,18,16,dxzyo3,[D] An Interesting (in my opinion) Observation While Messing With the Full GPT-2,https://www.reddit.com/r/MachineLearning/comments/dxzyo3/d_an_interesting_in_my_opinion_observation_while/,Argenteus_CG,1574062100,"When playing around with an online implementation of the full model (talktotransformer.com), I noticed it can do something that is (to me) really cool: It can complete analogies! If you use an open quote and start an analogy leaving out the last word, it can often get it right! This is interesting because the model was not to my knowledge trained to do this, so it must be an emergent result of ""understanding"" the english language! I've so far tried a number of different analogies in varying orders, for example '""Angry is to Anger as Afraid is to' and '""Big is to Bigger as Small is to', and while it doesn't ALWAYS get it right it does more often than not.

I tried this on the earliest, incomplete model they released and it failed, so this seems to be unique to the full model (although I never tried with any of the models of intermediate complexity that they released in between with their staged release plan, so I can't confirm at which point it gained the ability).

Anyone else noticed this? And am I alone in thinking it's cool? It may not be as flashy as writing an article, but it shows a level of ""understanding"" that things like markov chains, etc., can't generally match IME.",105,1
892,2019-11-18,2019,11,18,17,dy0ays,Is there any AI app able to tell me what job field should I pick?,https://www.reddit.com/r/MachineLearning/comments/dy0ays/is_there_any_ai_app_able_to_tell_me_what_job/,xkoroto,1574064479,[removed],0,1
893,2019-11-18,2019,11,18,18,dy0yr3,Minimal GMM implementation,https://www.reddit.com/r/MachineLearning/comments/dy0yr3/minimal_gmm_implementation/,piyush-kgp,1574069327,"Hi, I learnt about GMMs in my class recently but could not find a minimal implementation (in numpy) anywhere. So, I wrote my own implementation:

[https://gist.github.com/piyush-kgp/81d7e05b5b32e41fb2e83c2d89e68722](https://gist.github.com/piyush-kgp/81d7e05b5b32e41fb2e83c2d89e68722)

Let me know what you think. I am also interested in implementing other ideas I learnt such as tricks with gradient descent (for a simple function in 2-D) viz. momentum, adaptive LR etc.",0,1
894,2019-11-18,2019,11,18,18,dy11dc,[Post] Data Collection and Feature Extraction for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/dy11dc/post_data_collection_and_feature_extraction_for/,cdossman,1574069851,[removed],0,1
895,2019-11-18,2019,11,18,19,dy1grs,Many DNN papers dont do hyperparameter search,https://www.reddit.com/r/MachineLearning/comments/dy1grs/many_dnn_papers_dont_do_hyperparameter_search/,alex19111,1574072917,"A thing that I recognized after reading papers about various DNN models is that they often compare against other DNN baselines and dont seem to perform hyperparameter search on them.
Many reported results seem to be for hand-picked configurations only. No search methods (like grid search, Bayesian optimization or even random search) have been used to find the best-performing configuration.

However, the performance of a DNN models definitely depends on the choice of hyperparameters, so hypothetically you could make a baseline performing bad by picking poor hyperparameters.

Why are so many famous papers with such an (in my opinion) incomplete evaluation out there?
Or am I missing something here and it is enough to look at one configuration only?",0,1
896,2019-11-18,2019,11,18,19,dy1lo1,"Global Machine Learning Market - Segment Analysis, Opportunity Assessment, Competitive Intelligence, Industry Outlook 2016-2026",https://www.reddit.com/r/MachineLearning/comments/dy1lo1/global_machine_learning_market_segment_analysis/,IndustryUpdates,1574073886,[removed],0,1
897,2019-11-18,2019,11,18,20,dy23rm,[D] Many papers dont do hyperparameter search on DNN baselines,https://www.reddit.com/r/MachineLearning/comments/dy23rm/d_many_papers_dont_do_hyperparameter_search_on/,alex19111,1574077287,"A thing that I recognized after reading various DNN model papers is that they often dont seem to perform hyperparameter search on their / baseline models.
Many reported results seem to be for hand-picked configurations only. No search methods (like grid search, Bayesian optimization or even random search) have been used to find the best-performing configurations.

IMO this is a problem: The performance of a DNN models really depends on the choice of hyperparameters, so hypothetically you could make a baseline model perform badly by picking poor hyperparameters.

Why are so many big papers with such an incomplete evaluation out there?
Or am I missing something here and it is enough to look at one configuration only?",19,1
898,2019-11-18,2019,11,18,21,dy2dt6,[R] A New Deterministic Technique for Symbolic Regression,https://www.reddit.com/r/MachineLearning/comments/dy2dt6/r_a_new_deterministic_technique_for_symbolic/,ennanco,1574079060,"We would love to receive feedback about our last work which, in my humble opinion, is at least worthy of a quick read due to the original development.

&amp;#x200B;

 [https://arxiv.org/abs/1908.06754](https://arxiv.org/abs/1908.06754)",0,1
899,2019-11-18,2019,11,18,22,dy31tt,ML application for complex physical models,https://www.reddit.com/r/MachineLearning/comments/dy31tt/ml_application_for_complex_physical_models/,cynixx1,1574082987,[removed],0,1
900,2019-11-18,2019,11,18,22,dy36zu,Price Optimisation Using Decision Tree (Regression Tree) - Machine Learning,https://www.reddit.com/r/MachineLearning/comments/dy36zu/price_optimisation_using_decision_tree_regression/,andrea_manero,1574083800,[removed],0,1
901,2019-11-18,2019,11,18,22,dy3i97,[D] Are design patterns useful to build machine learning projects of python?,https://www.reddit.com/r/MachineLearning/comments/dy3i97/d_are_design_patterns_useful_to_build_machine/,SubstantialSwimmer4,1574085491,Or is The GoF book worth reading for ML practitioner?,10,1
902,2019-11-18,2019,11,18,23,dy3k2z,Learning Machine Learning,https://www.reddit.com/r/MachineLearning/comments/dy3k2z/learning_machine_learning/,Free-_-Yourself,1574085756,[removed],0,1
903,2019-11-18,2019,11,18,23,dy3u8d,How to extract date from an receipt image,https://www.reddit.com/r/MachineLearning/comments/dy3u8d/how_to_extract_date_from_an_receipt_image/,saravanakumar17,1574087168,[removed],0,1
904,2019-11-18,2019,11,18,23,dy3v1q,Trying a bit different ,https://www.reddit.com/r/MachineLearning/comments/dy3v1q/trying_a_bit_different/,developers_hutt,1574087292,,0,1
905,2019-11-18,2019,11,18,23,dy3xjr,5-minute-survey request,https://www.reddit.com/r/MachineLearning/comments/dy3xjr/5minutesurvey_request/,AgentThompson95,1574087623,[removed],0,1
906,2019-11-18,2019,11,18,23,dy40i7,Participe Simulation,https://www.reddit.com/r/MachineLearning/comments/dy40i7/participe_simulation/,donovan680_,1574088033,,0,1
907,2019-11-18,2019,11,18,23,dy40u9,"Intel's oneAPI tries to be a ""write once run anywhere"" for machine learning",https://www.reddit.com/r/MachineLearning/comments/dy40u9/intels_oneapi_tries_to_be_a_write_once_run/,Dagusiu,1574088083,,0,1
908,2019-11-18,2019,11,18,23,dy42jc,[Discussion]System that constantly learns,https://www.reddit.com/r/MachineLearning/comments/dy42jc/discussionsystem_that_constantly_learns/,copythatpasta,1574088313,How to make a system that can train it self during run time? Like adding a class to a classifier after its already ported ? And then giving examples to the classifers through a microservice or simple executleble and have the classifiers be able to retrain the current model and load in the new auto?,6,1
909,2019-11-18,2019,11,18,23,dy45rj,[Research] 5-minute-survey,https://www.reddit.com/r/MachineLearning/comments/dy45rj/research_5minutesurvey/,AgentThompson95,1574088737,"Hello everyone, I'm working on an article on my PhD studies and I need to gather about 100 answers to my survey. It takes about 5 minutes and consist of 3 parts: 

1 - 12 straightforward questions of the same type - you may feel a bit confused but it is made that way in purpose

2 - Explanation part with no questions

3 - the same 12 questions but explained earlier

I would be very grateful for doing that and thank you in advance. Here's the link: [https://www.survio.com/survey/d/T4X7T4P1F2V9S8D3P](https://www.survio.com/survey/d/T4X7T4P1F2V9S8D3P)

Of course if you have any questions about the research feel free to ask",15,1
910,2019-11-18,2019,11,18,23,dy482p,[N] French BERT (CamemBERT) now available in Transformers library,https://www.reddit.com/r/MachineLearning/comments/dy482p/n_french_bert_camembert_now_available_in/,jikkii,1574089023,"The **CamemBERT** Transformer model (by Facebook AI, Inria and Sorbonne Universit), trained on 138GB of French text was added this morning to the [huggingface/transformers](https://github.com/huggingface/transformers) model repository, and is now usable in both **PyTorch and TensorFlow 2**! Install the library from source to play around with it!

It is available alongside chinese and german BERT models and [other multi-lingual models](https://huggingface.co/transformers/pretrained_models.html).

CamemBERT improves the state of the art on several French NLP tasks, outperforming multi-lingual models in several tasks. It's based on RoBERTa's training scheme but uses whole-word masking as well as sentence-piece tokenization.",10,1
911,2019-11-19,2019,11,19,0,dy4jyt,What is the methodology of partial_fit in sklearn?,https://www.reddit.com/r/MachineLearning/comments/dy4jyt/what_is_the_methodology_of_partial_fit_in_sklearn/,vash9590,1574090559,[removed],0,1
912,2019-11-19,2019,11,19,1,dy5dp9,Huawei Tops ETH Zurich 2019 Smartphone Deep Learning Rankings,https://www.reddit.com/r/MachineLearning/comments/dy5dp9/huawei_tops_eth_zurich_2019_smartphone_deep/,Yuqing7,1574094197,,0,1
913,2019-11-19,2019,11,19,1,dy5opd,One sentence highlight for every single NIPS 2019 Paper (~1420 in total),https://www.reddit.com/r/MachineLearning/comments/dy5opd/one_sentence_highlight_for_every_single_nips_2019/,biandangou,1574095559,"Browse all NIPS-2019 papers in three hours:

[https://www.paperdigest.org/2019/11/neurips-2019-highlights/](https://www.paperdigest.org/2019/11/neurips-2019-highlights/)",0,1
914,2019-11-19,2019,11,19,2,dy601t,MLP does not learn,https://www.reddit.com/r/MachineLearning/comments/dy601t/mlp_does_not_learn/,baldinggerman,1574096906,"Hey Guys,

tried to implement a MLP as a class, but when I test it, accuracy stays 50%, so obv it does not learn anything.

If you could just give me a hint on why, would be great!

[https://colab.research.google.com/drive/1KlnqmunviSkKaEiT5PJ98zsIzK\_7Zrcv#scrollTo=HU9tu\_1oi56u](https://colab.research.google.com/drive/1KlnqmunviSkKaEiT5PJ98zsIzK_7Zrcv#scrollTo=HU9tu_1oi56u)",0,1
915,2019-11-19,2019,11,19,2,dy6exw, Advanced Machine Learning Helps Play Store Users Discover Personalised Apps,https://www.reddit.com/r/MachineLearning/comments/dy6exw/advanced_machine_learning_helps_play_store_users/,sjoerdapp,1574098643,,0,1
916,2019-11-19,2019,11,19,2,dy6nke,[P] Autopilot in keras for self-driving cars,https://www.reddit.com/r/MachineLearning/comments/dy6nke/p_autopilot_in_keras_for_selfdriving_cars/,stormtrooper1721,1574099661,"Not mine, but I got permission from the creator to post it here on Reddit.

Blog post: https://littlemountainman.github.io/2019/11/27/selfdrivingfun/",2,1
917,2019-11-19,2019,11,19,3,dy72zq,Best Paper Presentation: Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/dy72zq/best_paper_presentation_ordered_neurons/,TensorsDontFlow,1574101439,[removed],0,1
918,2019-11-19,2019,11,19,3,dy7dgd,"[D] History of NLP: People projected intelligence and humanity onto the world's first chatbot, Eliza",https://www.reddit.com/r/MachineLearning/comments/dy7dgd/d_history_of_nlp_people_projected_intelligence/,newsbeagle,1574102667,"People familiar with NLP probably know about the ELIZA chatbot, which Joseph Weizenbaum created in 1966. ELIZA used a psychoanalyst's tricks to keep a fairly natural conversation going, focusing in on keywords. (For example, if a human typed ""I'm so angry with my sister,"" the chatbot might reply, ""Tell me why you're so angry with your sister."")

What I didn't know was that Weizenbaum was thoroughly creeped out by people's response to his creation. During their interactions with ELIZA, people developed emotional attachments to the program, and often confided in it. 

From the article: ""Even more surprising was that this sense of intimacy persisted even after Weizenbaum described how the machine worked and explained that it didnt really understand anything that was being said. Weizenbaum was most troubled when his secretary, who had watched him build the program from scratch over many months, insisted that he leave the room so she could talk to Eliza in private."" 

[https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/why-people-demanded-privacy-to-confide-in-the-worlds-first-chatbot](https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/why-people-demanded-privacy-to-confide-in-the-worlds-first-chatbot)

This is the third installment in a 6-part series on the history of NLP that I'm editing for IEEE Spectrum. I'm curious to know if people find it interesting.",3,1
919,2019-11-19,2019,11,19,4,dy7lnm,NIPS website seems to be down,https://www.reddit.com/r/MachineLearning/comments/dy7lnm/nips_website_seems_to_be_down/,TheAssmanBob,1574103617,[removed],0,1
920,2019-11-19,2019,11,19,4,dy7qpn,This is the data on which I train my neural network.,https://www.reddit.com/r/MachineLearning/comments/dy7qpn/this_is_the_data_on_which_i_train_my_neural/,ShubhamEkapure_1999,1574104154,,0,1
921,2019-11-19,2019,11,19,4,dy85pv,[N] Pre-trained knowledge graph embedding models are available in GraphVite!,https://www.reddit.com/r/MachineLearning/comments/dy85pv/n_pretrained_knowledge_graph_embedding_models_are/,kiddozhu,1574105836,"In the recent update of GraphVite, we release a new large-scale knowledge graph dataset, along with new benchmarks of knowledge graph embedding methods. The dataset, Wikidata5m, contains 5 million entities and 21 million facts constructed from Wikidata and Wikipedia. Most of the entities come from the general domain or the scientific domain, such as celebrities, events, concepts and things.

To facilitate the usage of knowledge graph representations in semantic tasks, we provide a bunch of pre-trained embeddings from popular models, including TransE, DistMult, ComplEx, SimplE and RotatE. You can directly access these embeddings by natural language index, such as ""machine learning"", ""united states"" or even abbreviations like ""m.i.t."". Check out these models [here](https://graphvite.io/docs/latest/pretrained_model).

Here are the benchmarks of these models on Wikidata5m.

|            | MR     | MRR   | HITS@1 | HITS@3 | HITS@10 |
|------------|--------|-------|--------|--------|---------|
| [TransE]   | 109370 | 0.253 | 0.170  | 0.311  | 0.392   |
| [DistMult] | 211030 | 0.253 | 0.209  | 0.278  | 0.334   |
| [ComplEx]  | 244540 | 0.281 | 0.228  | 0.310  | 0.373   |
| [SimplE]   | 112754 | 0.296 | 0.252  | 0.317  | 0.377   |
| [RotatE]   | 89459  | 0.290 | 0.234  | 0.322  | 0.390   |

[TransE]: http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf
[DistMult]: https://arxiv.org/pdf/1412.6575.pdf
[ComplEx]: http://proceedings.mlr.press/v48/trouillon16.pdf
[SimplE]: https://papers.nips.cc/paper/7682-simple-embedding-for-link-prediction-in-knowledge-graphs.pdf
[RotatE]: https://arxiv.org/pdf/1902.10197.pdf",2,1
922,2019-11-19,2019,11,19,4,dy8hjh,[P] Cortex: Deploy models from any framework as production APIs,https://www.reddit.com/r/MachineLearning/comments/dy8hjh/p_cortex_deploy_models_from_any_framework_as/,killthecloud,1574107159,"Cortex just released V 0.10, which includes their new Predictor Interface for serving models. It lets you take models from any framework and implement them in simple Python, before deploying them with a single terminal command. V 0.10 also still includes out-of-the-box support for TensorFlow Serving and ONNX Runtime.

Repo link: [https://github.com/cortexlabs/cortex](https://github.com/cortexlabs/cortex)  


Examples:

\- [Deploying Hugging Face's DistilGPT-2](https://github.com/cortexlabs/cortex/tree/master/examples/pytorch/text-generator) \- PyTorch

\- [Deploying a sentiment analyzer with BERT](https://github.com/cortexlabs/cortex/tree/master/examples/tensorflow/sentiment-analysis) \- TensorFlow

\- [The classic iris classifier](https://github.com/cortexlabs/cortex/tree/master/examples/xgboost/iris-classifier) \- XGBoost/ONNX",17,1
923,2019-11-19,2019,11,19,5,dy9ejb,"[D] For NLP, how to generate cluster labels from cluster ids?",https://www.reddit.com/r/MachineLearning/comments/dy9ejb/d_for_nlp_how_to_generate_cluster_labels_from/,theusdesi,1574110777,"I have a business requirement where we have unsupervised data.

We have a model that assigns a cluster id for every sample. What mathematical way to generate a name for the samples associated with the same cluster ids? The modality is text.  

&amp;#x200B;

Help would be appreciated.",0,1
924,2019-11-19,2019,11,19,6,dy9l4u,"[P] Open source library to perform entity embeddings on categorical variables using Convolutional Neural Networks [+ Unit Tests, Code Coverage and Continuous Integration]",https://www.reddit.com/r/MachineLearning/comments/dy9l4u/p_open_source_library_to_perform_entity/,CrazyCapivara,1574111502,"In the past 2 years I have been working as a Machine Learning developer, mostly with tabular data, and I've developed a tool to perform entity embeddings on categorical variables using CNN with Keras. I tried pretty much to make it easy to use and flexible to most of the existent scenarios (regression, binary and multi-class classification), but if you find any other need or issue to be fixed, do not hesitate to ask.

I tried to add some cool stuff on the project, such as **unit tests**, **code coverage** with Codacy, **continuous integration** with Travis CI and **auto deployment** to PyPi and **auto-generated documentation** with Sphinx and ReadTheDocs, so if any of you is interested in how to setup your project to have these features, feel free to use it as a base project.

Looking forward to any reviews about the source code. Any tip to improve the readability or even performance, its really welcome and well appreciated.

**Github:** [**https://github.com/bresan/entity\_embeddings\_categorical**](https://github.com/bresan/entity_embeddings_categorical)

PyPi: [https://pypi.org/project/entity-embeddings-categorical/](https://pypi.org/project/entity-embeddings-categorical/)

Code coverage (nowadays reaching 97%): [https://coveralls.io/github/bresan/entity\_embeddings\_categorical?branch=master](https://coveralls.io/github/bresan/entity_embeddings_categorical?branch=master)

Thanks and I hope it can help somebody out there :-)",1,1
925,2019-11-19,2019,11,19,6,dy9wu4,Hacking Neural Networks: A Short Introduction,https://www.reddit.com/r/MachineLearning/comments/dy9wu4/hacking_neural_networks_a_short_introduction/,HN_Crosspost_Bot,1574112822,,0,1
926,2019-11-19,2019,11,19,6,dy9x08,Google Brain Hugo Larochelle on Few-Shot Learning,https://www.reddit.com/r/MachineLearning/comments/dy9x08/google_brain_hugo_larochelle_on_fewshot_learning/,Yuqing7,1574112842,,0,1
927,2019-11-19,2019,11,19,6,dya17s,Generating New Pokemons Using GANs,https://www.reddit.com/r/MachineLearning/comments/dya17s/generating_new_pokemons_using_gans/,myhotpot,1574113316,,0,1
928,2019-11-19,2019,11,19,6,dya2ik,Suggestions for USD 13000 setup for academic use.,https://www.reddit.com/r/MachineLearning/comments/dya2ik/suggestions_for_usd_13000_setup_for_academic_use/,mikkelbue,1574113457,[removed],0,1
929,2019-11-19,2019,11,19,7,dyaj7l,[P] Update: DepthAI hardware: Demo video MobileNetSSD (20class) running at 25FPS,https://www.reddit.com/r/MachineLearning/comments/dyaj7l/p_update_depthai_hardware_demo_video_mobilenetssd/,Luxonis-Brian,1574115300,"u/Luxonis-Brandon put together a [video demonstrating the real-time speed of the DepthAI.](https://www.youtube.com/watch?v=OBlrxuVtC6A)

The device is something we've been working on that combines disparity depth and AI via Intel's Myriad X VPU. We've developed a SoM that's not much bigger than a US quarter which takes direct image inputs from 3 cameras (2x OV9282, 1x IMX378), processes it, and spits the result back to the host via USB3.1.

Our ultimate goal is to develop a rear-facing AI vision system that will alert cyclists of potential danger from distracted drivers, so we needed disparity + AI to get object localization outputs - an understanding of where and what objects are. This needs to happen fast and with as little latency as possible... and at the edge... and at low power! 

There are some Myriad X solutions on the market already, but most use PCIe, so the data pipeline isn't as direct as Sensor--&gt;Myriad--&gt;Host, and the existing solutions also don't offer a three camera solution for RGBd. So, we built it!

If anyone has any questions or comments, we'd love to hear it!

Shameless plugs for our [hackaday](https://hackaday.io/project/163679-luxonis-depthai) and [crowdsupply](https://www.crowdsupply.com/luxonis/depthai) :)",0,1
930,2019-11-19,2019,11,19,7,dyap0h,[R] All The Ways You Can Compress BERT,https://www.reddit.com/r/MachineLearning/comments/dyap0h/r_all_the_ways_you_can_compress_bert/,BitForger,1574115992,"[http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html](http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html)

I've seen a lot of papers compressing BERT, so I compiled them all in one place. They all use pretty similar methods, so there's also a brief taxonomy.",7,1
931,2019-11-19,2019,11,19,7,dyauby,Funlines - A humor-based online game for labelling humorous headlines and classification of humor.,https://www.reddit.com/r/MachineLearning/comments/dyauby/funlines_a_humorbased_online_game_for_labelling/,tanvirsajed,1574116579,[removed],0,1
932,2019-11-19,2019,11,19,7,dyb3yr,Well cant agree more!!,https://www.reddit.com/r/MachineLearning/comments/dyb3yr/well_cant_agree_more/,Dejopeja,1574117706,,0,1
933,2019-11-19,2019,11,19,8,dyb6j3,[Request] Machine Learning Predict Car Price with scikit-learn,https://www.reddit.com/r/MachineLearning/comments/dyb6j3/request_machine_learning_predict_car_price_with/,Ammoti,1574118032,"Hello everyone,

I need help any help my graduation assignment we took machine learning course and prof wanted do that, I am sharing my notebook, so you can see what i did, someone who understand ML and scikit-learn concept can check my notebook and giving feedback? Every part has own question I added it as comment so you can check I really appericate any kind of help.

Thanks

[https://colab.research.google.com/drive/1MJq7jHL44Xu5d\_F1X1ivNlEyAd3qinrZ](https://colab.research.google.com/drive/1MJq7jHL44Xu5d_F1X1ivNlEyAd3qinrZ)",0,1
934,2019-11-19,2019,11,19,8,dybfpj,[P] MLP output of first layer is zero after one epoch,https://www.reddit.com/r/MachineLearning/comments/dybfpj/p_mlp_output_of_first_layer_is_zero_after_one/,deathlymonkey,1574119082,"I've been running into an issue lately trying to train a simple MLP.

I'm basically trying to get a network to map the XYZ position and RPY orientation of the end-effector of a robot arm (6-dimensional input) to the angle of every joint of the robot arm to reach that position (6-dimensional output), so this is a regression problem.

I've generated a dataset using the angles to compute the current position, and generated datasets with 5k, 500k and 500M sets of values.

My issue is the MLP I'm using doesn't learn anything at all. Using Tensorboard (I'm using Keras), I've realized that the output of my very first layer is always zero (see  [Image1](https://i.stack.imgur.com/UHFcM.png) ), no matter what I try.

Basically, my input is a shape (6,) vector and the output is also a shape (6,) vector.

Here is what I've tried so far, without success:

 * I've tried MLPs with 2 layers of size 12, 24; 2 layers of size 48, 48; 4 layers of size 12, 24, 24, 48.
 * Adam, SGD, RMSprop optimizers
 * Learning rates ranging from 0.15 to 0.001, with and without decay
 * Both Mean Squared Error (MSE) and Mean Absolute Error (MAE) as the loss function
 * Normalizing the input data, and not normalizing it (the first 3 values are between -3 and +3, the last 3 are between -pi and pi)
 * Batch sizes of 1, 10, 32
 * Tested the MLP of all 3 datasets of 5k values, 500k values and 5M values.
 * Tested with number of epoches ranging from 10 to 1000
 * Tested multiple initializers for the bias and kernel.
 * Tested both the Sequential model and the Keras functional API (to make sure the issue wasn't how I called the model)
 * All 3 of sigmoid, relu and tanh activation functions for the hidden layers (the last layer is a linear activation because its a regression)

Additionally, I've tried the very same MLP architecture on the basic Boston housing price regression dataset by Keras, and the net was definitely learning something, which leads me to believe that there may be some kind of issue with my data. However, I'm at a complete loss as to what it may be as the system in its current state does not learn anything at all, the loss function just stalls starting on the 1st epoch.

Any help or lead would be appreciated, and I will gladly provide code or data if needed!

Thank you",21,1
935,2019-11-19,2019,11,19,8,dybokl,[D] Non-Academic Attendance at NeurIPS,https://www.reddit.com/r/MachineLearning/comments/dybokl/d_nonacademic_attendance_at_neurips/,gwchase,1574120152,"For those who have attended NeurIPS in the past, is there value in a data scientist attending the conference when your background is not in research/ academics?",18,1
936,2019-11-19,2019,11,19,9,dycp47,A few questions from someone who just got introduced to machine learning.,https://www.reddit.com/r/MachineLearning/comments/dycp47/a_few_questions_from_someone_who_just_got/,fred_the_mailman,1574124700,[removed],0,1
937,2019-11-19,2019,11,19,10,dydcix,[D] Machine Learning vs Statistics,https://www.reddit.com/r/MachineLearning/comments/dydcix/d_machine_learning_vs_statistics/,datageek1987,1574127537,"I know this is an old debate, but I was talking to one of my colleagues from work and something he said struck me as really odd. He said, Statistics is more concerned with inference than results. After that, I did a little bit of internet research and found the same narrative there too. How Statistics does not have train-test split, and not concerned with the performance on unseen data, etc.

But this led me to the line of thinking that, Statistics (if it's not concerned with unseen data performance) is doing something wrong. 

If you fit to your train set perfectly with an interpretable model, but the performance on unseen data is dismal, then should we really take the interpretations from such a model as the truth? 

Looking towards all the statisticians out there, to tell me I'm wrong and why.",30,1
938,2019-11-19,2019,11,19,10,dydivh,"[D] #GOQ Q11 From Homoiconism, Tamagotchi, towards Technological Singularity, by 2030",https://www.reddit.com/r/MachineLearning/comments/dydivh/d_goq_q11_from_homoiconism_tamagotchi_towards/,wengchunkn,1574128291,,0,1
939,2019-11-19,2019,11,19,11,dydpix,LiDAR Point clouds for Deep Hough Voting,https://www.reddit.com/r/MachineLearning/comments/dydpix/lidar_point_clouds_for_deep_hough_voting/,RiGoRiEl,1574129118,[removed],1,1
940,2019-11-19,2019,11,19,11,dyed1l,[1911.07335] Overcoming Practical Issues of Deep Active Learning and its Applications on Named Entity Recognition,https://www.reddit.com/r/MachineLearning/comments/dyed1l/191107335_overcoming_practical_issues_of_deep/,rockermaxx,1574132185,,1,1
941,2019-11-19,2019,11,19,12,dyeron,Grad Students and Undergrad RAs: How do you update your PIs on your progress?,https://www.reddit.com/r/MachineLearning/comments/dyeron/grad_students_and_undergrad_ras_how_do_you_update/,pretysmitty,1574134162,[removed],0,1
942,2019-11-19,2019,11,19,12,dyesk4,Classifying the bias of the Brazilian supreme court with machine learning,https://www.reddit.com/r/MachineLearning/comments/dyesk4/classifying_the_bias_of_the_brazilian_supreme/,Niehls_Oppenheimer,1574134293,,1,1
943,2019-11-19,2019,11,19,13,dyfdn1,RandAugment: Practical automated data augmentation with a reduced search space,https://www.reddit.com/r/MachineLearning/comments/dyfdn1/randaugment_practical_automated_data_augmentation/,brettkoonce,1574137338,,2,1
944,2019-11-19,2019,11,19,13,dyfkwi,[Q] Is there a way to save embeddings from graphsage in supervised mode? I tried the save_val_embedding from unsuprervised.py but to no avail!,https://www.reddit.com/r/MachineLearning/comments/dyfkwi/q_is_there_a_way_to_save_embeddings_from/,kameshso01,1574138450,[removed],0,1
945,2019-11-19,2019,11,19,14,dyg8n9,[D] Is it possible to train an AI to detect mouse and keyboard sounds and filter them out of VoIP applications such as discord or Skype?,https://www.reddit.com/r/MachineLearning/comments/dyg8n9/d_is_it_possible_to_train_an_ai_to_detect_mouse/,GuysImConfused,1574142250,[removed],0,1
946,2019-11-19,2019,11,19,15,dyghx0,How AI Can Reduce Electricity Theft,https://www.reddit.com/r/MachineLearning/comments/dyghx0/how_ai_can_reduce_electricity_theft/,venkatvajradhar,1574143816,,0,1
947,2019-11-19,2019,11,19,15,dygv11,Phases of a Recommendation Engine.,https://www.reddit.com/r/MachineLearning/comments/dygv11/phases_of_a_recommendation_engine/,justAnotherGeorge,1574146202,"I'm building a project that provides movie recommendations to critics based off their ratings of movies and other critics on the site. I want to know where I can learn more about how a recommendation engine should evolve over time with increasing number of users. Also, since running recommendation engines on large data sets are quite expensive, how and when are they executed, especially when considering companies with less resources? (In all my searches I found different algorithms that are deployed but haven't found a lot of information regarding its practice).",0,1
948,2019-11-19,2019,11,19,17,dyhs8j,Does anybody know any classification models that can take an inputs of unfixed length?,https://www.reddit.com/r/MachineLearning/comments/dyhs8j/does_anybody_know_any_classification_models_that/,ThrowAwayForWailing,1574152776,Let's say we have a task of classification of surnames (that could be of different lengths) by nationality. What model of a neural network can be employed?,0,1
949,2019-11-19,2019,11,19,17,dyht4o,Full-time PhD and working part-time?,https://www.reddit.com/r/MachineLearning/comments/dyht4o/fulltime_phd_and_working_parttime/,Teddyzander,1574152955,,0,1
950,2019-11-19,2019,11,19,18,dyi05f,Big data clustering,https://www.reddit.com/r/MachineLearning/comments/dyi05f/big_data_clustering/,Rify,1574154408,[removed],0,1
951,2019-11-19,2019,11,19,18,dyi1x1,What Type of Errors Can Machine Learning Solve?,https://www.reddit.com/r/MachineLearning/comments/dyi1x1/what_type_of_errors_can_machine_learning_solve/,onlineit3,1574154769,,0,1
952,2019-11-19,2019,11,19,18,dyial9,Cartoning equipment for candles carton box packaging machine with automa...,https://www.reddit.com/r/MachineLearning/comments/dyial9/cartoning_equipment_for_candles_carton_box/,Jochamp-Machinery,1574156525,,0,1
953,2019-11-19,2019,11,19,19,dyima8,"Luong attention model, calculating attention weight",https://www.reddit.com/r/MachineLearning/comments/dyima8/luong_attention_model_calculating_attention_weight/,wyzkssm,1574158784,[removed],0,1
954,2019-11-19,2019,11,19,19,dyiquk,What do we need to make a basic Generative Art+Architecture A.I.? Looking for Co-founder programmer,https://www.reddit.com/r/MachineLearning/comments/dyiquk/what_do_we_need_to_make_a_basic_generative/,Niu_Davinci,1574159673,[removed],0,1
955,2019-11-19,2019,11,19,19,dyivz2,[R] How Machine Learning Can Help Unlock the World of Ancient Japan (by Alex Lamb),https://www.reddit.com/r/MachineLearning/comments/dyivz2/r_how_machine_learning_can_help_unlock_the_world/,hardmaru,1574160715,"[Blog post](https://thegradient.pub/machine-learning-ancient-japan/) by /u/alexmlamb about a line of research work combining deep learning with Classical Japanese Literature.

They also discuss [KuroNet: Pre-Modern Japanese Kuzushiji Character Recognition with Deep Learning](https://arxiv.org/abs/1910.09433)

Excerpt:

*Humanitys rich history has left behind an enormous number of historical documents and artifacts. However, virtually none of these documents, containing stories and recorded experiences essential to our cultural heritage, can be understood by non-experts due to language and writing changes over time.*

*For instance, archaeologist have unearthed tens of thousands of clay tablets from ancient Babylon, yet only a few hundred specially trained scholars can translate them. The vast majority of these documents have never been read, even if they were uncovered in the 1800s. To give a further illustration of the challenge posed by this scale, a tablet from the Tale of Gilgamesh was collected in an expedition in 1851, but its significance was not brought to light until 1872. This tablet contains a pre-biblical flood narrative, which has enormous cultural significance as a precursor to the Noahs Ark narrative.*

*This is a global problem, yet one of the most striking examples is the case of Japan. From 800 until 1900 CE, Japan used a writing system called Kuzushiji, which was removed from the curriculum in 1900 when the elementary school education was reformed. Currently, the overwhelming majority of Japanese speakers cannot read texts which are more than 150 years old. The volume of these texts  comprised of over three million books in storage but only readable by a handful of specially-trained scholars  is staggering. One library alone has digitized 20 million pages from such documents. The total number of documents  including, but not limited to, letters and personal diaries  is estimated to be over one billion. Given that very few people can understand these texts, mostly those with PhDs in classical Japanese literature and Japanese history, it would be very expensive and time-consuming to finance for scholars to convert these documents to modern Japanese. This has motivated the use of machine learning to automatically understand these texts.*

https://thegradient.pub/machine-learning-ancient-japan/",16,1
956,2019-11-19,2019,11,19,20,dyiyj7,Detect Geometrical Angle of the shapes.,https://www.reddit.com/r/MachineLearning/comments/dyiyj7/detect_geometrical_angle_of_the_shapes/,yash_yennam,1574161216,[removed],0,1
957,2019-11-19,2019,11,19,20,dyiyqw,"An Image Reckognition software that can identify legs, ears, face, etc, and sellect them on smoothie-3D software!",https://www.reddit.com/r/MachineLearning/comments/dyiyqw/an_image_reckognition_software_that_can_identify/,Niu_Davinci,1574161254,[removed],0,1
958,2019-11-19,2019,11,19,20,dyjbte,Rent RTX2080ti based machines for $0.20-$0.25/GPU/hr. Vast.ai Peer Based GPU Market,https://www.reddit.com/r/MachineLearning/comments/dyjbte/rent_rtx2080ti_based_machines_for_020025gpuhr/,UltraBallUK,1574163686,[removed],0,1
959,2019-11-19,2019,11,19,20,dyjf03,[D] ICML 2019 Machine Learning Talks,https://www.reddit.com/r/MachineLearning/comments/dyjf03/d_icml_2019_machine_learning_talks/,goolulusaurs,1574164283,"---
Recent Advances in Population-Based Search for Deep Neural Networks: Quality Diversity, Indirect Encodings, and Open-Ended Algorithms

Presented by Jeff Clune, Joel Lehman and Kenneth Stanley

https://www.facebook.com/icml.imls/videos/481758745967365/

---
Never-Ending Learning

Presented by Tom Mitchell and Partha Talukdar.

https://www.facebook.com/icml.imls/videos/350412952342021/
https://www.facebook.com/icml.imls/videos/1083330081864839/

---
A Primer on PAC-Bayesian Learning

Presented by Benjamin Guedj and John Shawe-Taylor

https://www.facebook.com/icml.imls/videos/318683639013879/

---
Meta-Learning: from Few-Shot Learning to Rapid Reinforcement Learning

Presented by Chelsea Finn and Sergey Levine

https://www.facebook.com/icml.imls/videos/400619163874853/
https://www.facebook.com/icml.imls/videos/2970931166257998/

---
Active Learning: From Theory to Practice

Presented by Robert Nowak and Steve Hanneke

https://www.facebook.com/icml.imls/videos/662482727539899/

---
Neural Approaches to Conversational AI

Presented by Michel Galley and Jianfeng Gao

https://www.facebook.com/icml.imls/videos/2375117292730871/

---
A Tutorial on Attention in Deep Learning

Presented by Alex Smola and Aston Zhang

https://www.facebook.com/icml.imls/videos/382464939283864/
https://www.facebook.com/icml.imls/videos/889237771440064/

---
Active Hypothesis Testing: An Information Theoretic (re)View

Presented by Tara Javidi

https://www.facebook.com/icml.imls/videos/478549476247044/

---
Algorithm configuration: learning in the space of algorithm designs

Presented by Kevin Leyton-Brown and Frank Hutter

https://www.facebook.com/icml.imls/videos/2044426569187107/

---
""The U.S. Census Bureau Tries to be a Good Data Steward in the 21st Century""
invited talk by John M. Abowd

Best Paper Awards:
Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations

https://www.facebook.com/icml.imls/videos/446476306189465/

---
Session on Deep Learning Algorithms


 SelectiveNet: A Deep Neural Network with an Integrated Reject Option

 Manifold Mixup: Better Representations by Interpolating Hidden States

 Processing Megapixel Images with Deep Attention-Sampling Models

 TapNet: Neural Network Augmented with Task-Adaptive Projection for Few-Shot Learning

 Online Meta-Learning

 Training Neural Networks with Local Error Signals

 GMNN: Graph Markov Neural Networks

 Self-Attention Graph Pooling

 Combating Label Noise in Deep Learning using Abstention

 LGM-Net: Learning to Generate Matching Networks for Few-Shot Learning

https://www.facebook.com/icml.imls/videos/336722770596090/

---
Session on Deep Reinforcement Learning

 ELF OpenGo: an analysis and open reimplementation of AlphaZero

 Making Deep Q-learning methods robust to time discretization

 Nonlinear Distributional Gradient Temporal-Difference Learning

 Composing Entropic Policies using Divergence Correction

 TibGM: A Transferable and Information-Based Graphical Model Approach for Reinforcement Learning

 Multi-Agent Adversarial Inverse Reinforcement Learning

 Policy Consolidation for Continual Reinforcement Learning

 Off-Policy Deep Reinforcement Learning without Exploration

 Random Expert Distillation: Imitation Learning via Expert Policy Support Estimation

 Revisiting the Softmax Bellman Operator: New Benefits and New Perspective

https://www.facebook.com/icml.imls/videos/1577337105730518/

---
Session on Adversarial Examples

 Adversarial Attacks on Node Embeddings via Graph Poisoning

 First-Order Adversarial Vulnerability of Neural Networks and Input Dimension

 On Certifying Non-Uniform Bounds against Adversarial Attacks

 Improving Adversarial Robustness via Promoting Ensemble Diversity

 Adversarial camera stickers: A physical camera-based attack on deep learning systems

 Adversarial examples from computational constraints

 POPQORN: Quantifying Robustness of Recurrent Neural Networks

 Using Pre-Training Can Improve Model Robustness and Uncertainty

 Generalized No Free Lunch Theorem for Adversarial Robustness

 PROVEN: Verifying Robustness of Neural Networks with a Probabilistic Approach

https://www.facebook.com/icml.imls/videos/689280291532883/

---
Session on Generative Adversarial Networks

 Self-Attention Generative Adversarial Networks

 Multivariate-Information Adversarial Ensemble for Scalable Joint Distribution Matching

 High-Fidelity Image Generation With Fewer Labels

 Revisiting precision recall definition for generative modeling

 Wasserstein of Wasserstein Loss for Learning Generative Models

 Flat Metric Minimization with Applications in Generative Modeling

 Entropic GANs meet VAEs: A Statistical Approach to Compute Sample Likelihoods in GANs

 Non-Parametric Priors For Generative Adversarial Networks

 Lipschitz Generative Adversarial Nets

 HexaGAN: Generative Adversarial Nets for Real World Classification

https://www.facebook.com/icml.imls/videos/713631379054038/

---
Session on Deep Reinforcement Learning

 An Investigation of Model-Free Planning

 CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning

 Task-Agnostic Dynamics Priors for Deep Reinforcement Learning

 Collaborative Evolutionary Reinforcement Learning

 EMI: Exploration with Mutual Information

 Imitation Learning from Imperfect Demonstration

 Curiosity-Bottleneck: Exploration By Distilling Task-Specific Novelty

 Dynamic Weights in Multi-Objective Deep Reinforcement Learning

 Fingerprint Policy Optimisation for Robust Reinforcement Learning

https://www.facebook.com/icml.imls/videos/298536957693171/

---
Session on Deep Learning Theory

 On Learning Invariant Representations for Domain Adaptation

 Lexicographic and Depth-Sensitive Margins in Homogeneous and Non-Homogeneous Deep Models

 Adversarial Generation of Time-Frequency Features with application in audio synthesis

 On the Universality of Invariant Networks

 Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks

 Gauge Equivariant Convolutional Networks and the Icosahedral CNN

 Feature-Critic Networks for Heterogeneous Domain Generalization

 Learning to Convolve: A Generalized Weight-Tying Approach

 On Dropout and Nuclear Norm Regularization

 Gradient Descent Finds Global Minima of Deep Neural Networks

https://www.facebook.com/icml.imls/videos/2339557826311186/

---
Session on Deep Learning Architectures

 Graph Matching Networks for Learning the Similarity of Graph Structured Objects

 BayesNAS: A Bayesian Approach for Neural Architecture Search

 Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks

 Shallow-Deep Networks: Understanding and Mitigating Network Overthinking

 Graph U-Nets

 SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver

 Area Attention

 The Evolved Transformer

 Jumpout : Improved Dropout for Deep Neural Networks with ReLUs

 Stochastic Deep Networks

https://www.facebook.com/icml.imls/videos/3253466301345987/

---
Session on Deep Learning Optimization

 An Investigation into Neural Net Optimization via Hessian Eigenvalue Density

 Differentiable Linearized ADMM

 Adaptive Stochastic Natural Gradient Method for One-Shot Neural Architecture Search

 A Quantitative Analysis of the Effect of Batch Normalization on Gradient Descent

 The Effect of Network Width on Stochastic Gradient Descent and Generalization: an Empirical Study

 AdaGrad stepsizes: sharp convergence over nonconvex landscapes

 Beyond Backprop: Online Alternating Minimization with Auxiliary Variables

 SWALP : Stochastic Weight Averaging in Low Precision Training

 Efficient optimization of loops and limits with randomized telescoping sums

 Self-similar Epochs: Value in arrangement

https://www.facebook.com/icml.imls/videos/874988016194584/

---
Session on Large Scale Learning and Systems

 Composable Core-sets for Determinant Maximization: A Simple Near-Optimal Algorithm

 Sublinear Time Nearest Neighbor Search over Generalized Weighted Space

 Compressing Gradient Optimizers via Count-Sketches

 Scalable Fair Clustering

 Conditional Gradient Methods via Stochastic Path-Integrated Differential Estimator

 Fault Tolerance in Iterative-Convergent Machine Learning

 Static Automatic Batching In TensorFlow

 Improving Neural Network Quantization without Retraining using Outlier Channel Splitting

 Memory-Optimal Direct Convolutions for Maximizing Classification Accuracy in Embedded Applications

 DL2: Training and Querying Neural Networks with Logic

https://www.facebook.com/icml.imls/videos/2250364101882755/

---
""Machine Learning for Robots To Think Fast""
invited talk by Aude Billard

Test of time Award
Online dictionary learning for Sparse Coding

https://www.facebook.com/icml.imls/videos/2368059266588651/

---
Session on Deep Generative Models

 Sum-of-Squares Polynomial Flow

 FloWaveNet : A Generative Flow for Raw Audio

 Are Generative Classifiers More Robust to Adversarial Attacks?

 A Gradual, Semi-Discrete Approach to Generative Network Training via Explicit Wasserstein Minimization

 Disentangling Disentanglement in Variational Autoencoders

 EDDI: Efficient Dynamic Discovery of High-Value Information with Partial VAE

 A Wrapped Normal Distribution on Hyperbolic Space for Gradient-Based Learning

 Emerging Convolutions for Generative Normalizing Flows

 A Large-Scale Study on Regularization and Normalization in GANs

 Variational Annealing of GANs: A Langevin Perspective

https://www.facebook.com/icml.imls/videos/325725335009518/
https://www.facebook.com/icml.imls/videos/518469445360005/

---
Session on Deep Reinforcement Learning

 Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning

 Maximum Entropy-Regularized Multi-Goal Reinforcement Learning

 Imitating Latent Policies from Observation

 SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning

 Dimension-Wise Importance Sampling Weight Clipping for Sample-Efficient Reinforcement Learning

 Structured agents for physical construction

 Learning Novel Policies For Tasks

 Taming MAML: Efficient unbiased meta-reinforcement learning

 Self-Supervised Exploration via Disagreement

 Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables

https://www.facebook.com/icml.imls/videos/355035025132741/

---
Session on Adversarial Examples

 Theoretically Principled Trade-off between Robustness and Accuracy

 The Odds are Odd: A Statistical Test for Detecting Adversarial Examples

 ME-Net: Towards Effective Adversarial Robustness with Matrix Estimation

 Certified Adversarial Robustness via Randomized Smoothing

 Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition

 Parsimonious Black-Box Adversarial Attacks via Efficient Combinatorial Optimization

 Wasserstein Adversarial Examples via Projected Sinkhorn Iterations

 Transferable Clean-Label Poisoning Attacks on Deep Neural Nets

 NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks

 Simple Black-box Adversarial Attacks

https://www.facebook.com/icml.imls/videos/607431793098200/

---
Session on Deep Learning Architectures

 Invertible Residual Networks

 NAS-Bench-101: Towards Reproducible Neural Architecture Search

 Approximated Oracle Filter Pruning for Destructive CNN Width Optimization

 LegoNet: Efficient Convolutional Neural Networks with Lego Filters

 Sorting Out Lipschitz Function Approximation

 Graph Element Networks: adaptive, structured computation and memory

 Training CNNs with Selective Allocation of Channels

 Equivariant Transformer Networks

 Overcoming Multi-model Forgetting

 Bayesian Nonparametric Federated Learning of Neural Networks

https://www.facebook.com/icml.imls/videos/552835701913736/

---
Session on Deep Reinforcement Learning

 The Natural Language of Actions

 Control Regularization for Reduced Variance Reinforcement Learning

 On the Generalization Gap in Reparameterizable Reinforcement Learning

 Trajectory-Based Off-Policy Deep Reinforcement Learning

 A Deep Reinforcement Learning Perspective on Internet Congestion Control

 Model-Based Active Exploration

 Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations

 Distributional Multivariate Policy Evaluation and Exploration with the Bellman GAN

 A Baseline for Any Order Gradient Estimation in Stochastic Computation Graphs

 Remember and Forget for Experience Replay

https://www.facebook.com/icml.imls/videos/674476986298614/

---
Session on Causality

 Causal Identification under Markov Equivalence: Completeness Results

 Counterfactual Off-Policy Evaluation with Gumbel-Max Structural Causal Models

 Causal Discovery and Forecasting in Nonstationary Environments with State-Space Models

 Classifying Treatment Responders Under Causal Effect Monotonicity

 Learning Models from Data with Measurement Error: Tackling Underreporting

 Adjustment Criteria for Generalizing Experimental Findings

 Conditional Independence in Testing Bayesian Networks

 Sensitivity Analysis of Linear Structural Causal Models

 More Efficient Off-Policy Evaluation through Regularized Targeted Learning

 Inferring Heterogeneous Causal Effects in Presence of Spatial Confounding

https://www.facebook.com/icml.imls/videos/2188227091246504/

---
Session on Representation Learning

 Adversarially Learned Representations for Information Obfuscation and Inference

 Adaptive Neural Trees

 Connectivity-Optimized Representation Learning via Persistent Homology

 Minimal Achievable Sufficient Statistic Learning

 Learning to Route in Similarity Graphs

 Invariant-Equivariant Representation Learning for Multi-Class Data

 Infinite Mixture Prototypes for Few-shot Learning

 MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing

 Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting

https://www.facebook.com/icml.imls/videos/307375446865883/

---
Session on Generative Models

 Tensor Variable Elimination for Plated Factor Graphs

 Predicate Exchange: Inference with Declarative Knowledge

 Discriminative Regularization for Latent Variable Models with Applications to Electrocardiography

 Hierarchical Decompositional Mixtures of Variational Autoencoders

 Finding Mixed Nash Equilibria of Generative Adversarial Networks

 CompILE: Compositional Imitation Learning and Execution

 Sparse Multi-Channel Variational Autoencoder for the Joint Analysis of Heterogeneous Data

 Deep Generative Learning via Variational Gradient Flow

 Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design

 Learning Neurosymbolic Generative Models via Program Synthesis

https://www.facebook.com/icml.imls/videos/457663645035961/

---
Session on Deep Learning Algorithms

 How does Disagreement Help Generalization against Label Corruption?

 EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis

 Addressing the Loss-Metric Mismatch with Adaptive Loss Alignment

 Deep Compressed Sensing

 Differentiable Dynamic Normalization for Learning Deep Representation

 Toward Understanding the Importance of Noise in Training Neural Networks

 Cheap Orthogonal Constraints in Neural Networks: A Simple Parametrization of the Orthogonal and Unitary Group

 Breaking Inter-Layer Co-Adaptation by Classifier Anonymization

 Understanding the Impact of Entropy on Policy Optimization

 Probability Functional Descent: A Unifying Perspective on GANs, Variational Inference, and Reinforcement Learning

https://www.facebook.com/icml.imls/videos/600823507067800/

---
Session on Deep Generative Models

 State-Reification Networks: Improving Generalization by Modeling the Distribution of Hidden Representations

 Variational Laplace Autoencoders

 Latent Normalizing Flows for Discrete Sequences

 Multi-objective training of Generative Adversarial Networks with multiple discriminators

 Learning Discrete and Continuous Factors of Data via Alternating Disentanglement

 Bit-Swap: Recursive Bits-Back Coding for Lossless Compression with Hierarchical Latent Variables

 Graphite: Iterative Generative Modeling of Graphs

 Hybrid Models with Deep and Invertible Features

 MIWAE: Deep Generative Modelling and Imputation of Incomplete Data Sets

 On Scalable and Efficient Computation of Large Scale Optimal Transport

https://www.facebook.com/icml.imls/videos/1269891676506524/

---
Session on Reinforcement Learning

 Batch Policy Learning under Constraints

 Quantifying Generalization in Reinforcement Learning

 Learning Latent Dynamics for Planning from Pixels

 Projections for Approximate Policy Iteration Algorithms

 Learning Structured Decision Problems with Unawareness

 Calibrated Model-Based Deep Reinforcement Learning

 Reinforcement Learning in Configurable Continuous Environments

 Target-Based Temporal-Difference Learning

 Iterative Linearized Control: Stable Algorithms and Complexity Guarantees

 Finding Options that Minimize Planning Time

https://www.facebook.com/icml.imls/videos/2547484245262588/

---
Session on Interpretability 

 Neural Network Attributions: A Causal Perspective

 Towards a Deep and Unified Understanding of Deep Neural Models in NLP

 Explaining Deep Neural Networks with a Polynomial Time Algorithm for Shapley Value Approximation

 Functional Transparency for Structured Data: a Game-Theoretic Approach

 Exploring interpretable LSTM neural networks over multi-variable data

 TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing

 Gaining Free or Low-Cost Interpretability with Interpretable Partial Substitute

 State-Regularized Recurrent Neural Networks

 Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation

 On the Connection Between Adversarial Robustness and Saliency Map Interpretability

https://www.facebook.com/icml.imls/videos/460378531393374/

---
Session on Deep Learning

 Understanding and correcting pathologies in the training of learned optimizers

 Demystifying Dropout

 Ladder Capsule Network

 Unreproducible Research is Reproducible

 Geometric Scattering for Graph Data Analysis

 Robust Inference via Generative Classifiers for Handling Noisy Labels

 LIT: Learned Intermediate Representation Training for Model Compression

 Analyzing and Improving Representations with the Soft Nearest Neighbor Loss

 What is the Effect of Importance Weighting in Deep Learning?

 Similarity of Neural Network Representations Revisited

https://www.facebook.com/icml.imls/videos/308727963404001/

---
Session on Deep Sequence Models

 Stochastic Beams and Where To Find Them: The Gumbel-Top-k Trick for Sampling Sequences Without Replacement

 Learning to Exploit Long-term Relational Dependencies in Knowledge Graphs

 Meta-Learning Neural Bloom Filters

 CoT: Cooperative Training for Generative Modeling of Discrete Data

 Non-Monotonic Sequential Text Generation

 Insertion Transformer: Flexible Sequence Generation via Insertion Operations

 Empirical Analysis of Beam Search Performance Degradation in Neural Sequence Models

 Trainable Decoding of Sets of Sequences for Neural Sequence Models

 Learning to Generalize from Sparse and Underspecified Rewards

 Efficient Training of BERT by Progressively Stacking

https://www.facebook.com/icml.imls/videos/895968107420746/

---
Session on Deep Learning Theory

 Why do Larger Models Generalize Better? A Theoretical Perspective via the XOR Problem

 On the Spectral Bias of Neural Networks

 Recursive Sketches for Modular Deep Learning

 Zero-Shot Knowledge Distillation in Deep Networks

 A Convergence Theory for Deep Learning via Over-Parameterization

 A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks

 Approximation and non-parametric estimation of ResNet-type convolutional neural networks

 Global Convergence of Block Coordinate Descent in Deep Learning

 Measurements of Three-Level Hierarchical Structure in the Outliers in the Spectrum of Deepnet Hessians

 On the Limitations of Representing Functions on Sets

https://www.facebook.com/icml.imls/videos/606052416553010/

---
""What 4 Year Olds Can Do and AI Can't (yet)""

invited talk by Alison Gopnik

Best Paper Awards:
Rates of Convergence for Sparse Variational Gaussian Process Regression

https://www.facebook.com/icml.imls/videos/680801775700033/

---
Session on Representation Learning

 Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations

 Breaking the Softmax Bottleneck via Learnable Monotonic Pointwise Non-linearities

 Multi-Object Representation Learning with Iterative Variational Inference

 Cross-Domain 3D Equivariant Image Embeddings

 Loss Landscapes of Regularized Linear Autoencoders

 Hyperbolic Disk Embeddings for Directed Acyclic Graphs

 LatentGNN: Learning Efficient Non-local Relations for Visual Recognition

 Robustly Disentangled Causal Mechanisms: Validating Deep Representations for Interventional Robustness

 Lorentzian Distance Learning for Hyperbolic Representations

https://www.facebook.com/icml.imls/videos/321425055451434/

---
Session on Bandits and Multiagent Learning

 Decentralized Exploration in Multi-Armed Bandits

 Warm-starting Contextual Bandits: Robustly Combining Supervised and Bandit Feedback

 Exploiting structure of uncertainty for efficient matroid semi-bandits

 PAC Identification of Many Good Arms in Stochastic Multi-Armed Bandits

 Contextual Multi-armed Bandit Algorithm for Semiparametric Reward Model

 Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning

 TarMAC: Targeted Multi-Agent Communication

 QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning

 Actor-Attention-Critic for Multi-Agent Reinforcement Learning

 Finite-Time Analysis of Distributed TD(0) with Linear Function Approximation on Multi-Agent Reinforcement Learning

https://www.facebook.com/icml.imls/videos/444326646299556/

---
Session on Bayesian Deep Learning

 Probabilistic Neural Symbolic Models for Interpretable Visual Question Answering

 Nonparametric Bayesian Deep Networks with Local Competition

 Good Initializations of Variational Bayes for Deep Models

 Dropout as a Structured Shrinkage Prior

 ARSM: Augment-REINFORCE-Swap-Merge Estimator for Gradient Backpropagation Through Categorical Variables

 On Variational Bounds of Mutual Information

 Partially Exchangeable Networks and Architectures for Learning Summary Statistics in Approximate Bayesian Computation

 Hierarchical Importance Weighted Autoencoders

 Faster Attend-Infer-Repeat with Tractable Probabilistic Models

 Understanding Priors in Bayesian Neural Networks at the Unit Level

https://www.facebook.com/icml.imls/videos/2202320806483370/

---
Workshop on Generative Modeling and Model-Based Reasoning for Robotics and AI

""Self Supervised Learning""
invited talk by Yann LeCun 

""Mental Simulation, Imagination, and Model-Based Deep RL""
invited talk by Jessica B. Hamrick


 Bayesian Inference to Identify the Cause of Human Errors

 Data-Efficient Model-Based RL through Unsupervised Discovery and Curiosity-Driven Exploration

 A Top-Down Bottom-Up Approach to Learning Hierarchical Physics Models for Manipulation

 Discovering, Predicting, and Planning with Objects

 FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained Object Generation and Discovery

 Generalized Hidden Parameter MDPs for Model-based Meta-reinforcement Learning

 HEDGE: Hierarchical Event-Driven Generation

 Improved COnditional VRNNs for Video Prediction

 Improvisation through Physical Understanding: Using Novel Objects as Tools with Visual Foresight

 Learning Feedback Linearization by MF RL

 ""Learning High Level Representations from Continous Experience""

 Deep Knowledge-Based Agents

https://www.facebook.com/icml.imls/videos/394896141118878/
https://www.facebook.com/icml.imls/videos/2084133498380491/

---
Workshop on Uncertainty and Robustness in Deep Learning

https://www.facebook.com/icml.imls/videos/892421577776699/

---
Workshop on Understanding and Improving Generalizing in Deep Learning

Daniel Roy - Progress on Nonvacuous Generalization Bounds

Chelsea Finn - Training for Generalization

Spotlight Talk - A Meta-Analysis of Overfitting in Machine Learning

Spotlight Talk - Uniform Convergence may be unable to explain generalization in deep learning

https://www.facebook.com/icml.imls/videos/834773703576296/

---
Workshop on Understanding and Improving generalization in Deep Learning

Sham Kakade - Prediction, Learning and Memory

Mikhail Belkin - A Hard Look at Generalization and its Theories

Spotlight Talk - Towards Task and Architecture-Indipendent Generalization Gap Predictors

Spotlight Talk - Data-Dependent Sample Complexity of Deep Neural Networks Via Lipschitz Augmentation

https://www.facebook.com/icml.imls/videos/2543954589165286/

---
Workshop on Generative Modeling and Model-Based Reasoning for Robotics and AI

""What should be Learned?""
Invited talk by Stefan Schaal


 When to Trust Your Model: Model-Based Policy Optimization

 Model Based Planning with Energy Based Models

 A Perspective on Objects and Systematic Generalization in Model-Based RL

https://www.facebook.com/icml.imls/videos/1286528018196347/

---
Workshop Session

Keynote by Kilian Weinberger: On Calibration and Fairness


 Why ReLU networks yield high-confidence predictions far away from training data and how to mitigate the problem

 Detecting Extrapolation with Influence Functions

 How Can We Be So Dense? The Robustness of Highly Sparse Representations

Keynote by Suchi Saria: Safety Challenges with Black-Box Predictors and Novel Learning Approaches for Failure Proofing

https://www.facebook.com/icml.imls/videos/474831503062000/

---
Workshop on Understanding and Improving generalization in Deep Learning

Invited Speaker: Aleksander Mdry ""Are All Features Created Equal?""
Invited Speaker: Jason Lee ""On the Foundations of Deep Learning: SGD, Overparametrization, and Generalization""
Spotlight Talk: ""Towards Large Scale Structure of the Loss Landscape of Neural Networks""
Spotlight Talk: ""Zero-Shot Learning from scratch: leveraging local compositional representations""

https://www.facebook.com/icml.imls/videos/365029137702011/

---
Workshop Session


 Subspace Inference for Bayesian Deep Learning

 Quality of Uncertainty Quantification for Bayesian Neural Network Inference

 In-Between Uncertainty in Bayesian Neural Networks

Keynote by Dawn Song: Adversarial Machine Learning: Challenges, Lessons, and Future Directions

https://www.facebook.com/icml.imls/videos/320132412242165/

---
Workshop on Generative Modeling and Model-Based Reasoning for Robotics and AI

Value Focused Models, Invited Talk by David Silver

Manipulation by Feel: Touch-Based Control with Deep Predictive Models

Model-based Policy Gradients with Entropy Exploration through Sampling

Model-based Reinforcement Learning for Atari

Learning to Predict Without Looking Ahead: World Models Without Forward Prediction

Physics-as-Inverse-Graphics: Joint Unsupervised Learning of Objects and Physics from Video

Planning to Explore Visual Environments without Rewards

PRECOG: PrEdiction Conditioned On Goals in Visual Multi-Agent settings

Regularizing Trajectory Optimization with Denoising Autoencoders

Towards Jumpy Planning

Variational Temporal Abstraction

Visual Planning with Semi-Supervised Stochastic Action Representations

World Programs for Model-Based Learning and Planning in Compositional State and Action Spaces

Online Learning and Planning without Prior Knowledge

https://www.facebook.com/icml.imls/videos/2366831430268790/

---
Workshop on Generative Modeling and Model-Based Reasoning for robotics and AI

""Online Learning for Adaptive Robotic Systems"" - Byron Boots


""An inference perspective on model-based reinforcement learning""

""Reducing Noise in GAN Training with Variance Reduced Extragradient""


""Complexity without Losing Generality: The Role of Supervision and Composition"" - Chelsea Finn

""Self-supervised Learning for Exploration &amp; Representation"" - Abhinav Gupta

Panel Discussion

https://www.facebook.com/icml.imls/videos/449245405622423/

---
Workshop on Understanding and Improving generalization in Deep Learning

Panel Discussion (Moderator: Nati Srebro)

""Overparameterization without Overfitting: Jacobian-based Generalization Guarantees for Neural Networks""

""How Learning Rate and Delay Affect Minima Selection in Asynchronous Training of Neural Networks: Toward Closing the Generalization Gap""

https://www.facebook.com/icml.imls/videos/854556684898913/

---
Workshop on Self-Supervised Learning


""BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"" - Jacob Devlin

""Play as Self-Supervised Learning"" - Alison Gopnik

""Learning Latent Plans from Play"" - Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, Pierre Sermanet

""Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty"" - Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, Dawn Song

https://www.facebook.com/icml.imls/videos/2479161722147572/

---
Workshop on Identify and Understanding Deep Learnign Phenomena

""Optimizations Untold Gift to Learning: Implicit Regularization"" - Nati Srebro

""Bad Global Minima Exist and SGD Can Reach Them ""

""Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask""

""Are all layers created equal? -- Studies on how neural networks represent functions"" - Chiyuan Zhang

https://www.facebook.com/icml.imls/videos/450413519084800/

---
Workshop on Exploration in Reinforcement Learning

""Exploration: The Final Frontier"" - Doina Precup

""Overcoming Exploration with Play"" - Corey Lynch

""Optimistic Exploration with Pessimistic Initialisation"" - Tabish Rashid

""Scheduled Intrinsic Drive: A Hierarchical Take on Intrinsically Motivated Exploration"" - Nicolai Dorka

""Generative Exploration and Exploitation"" (Missing)

""The Journey is the Reward: Unsupervised Learning of Influential Trajectories"" - Jonathan Binas

https://www.facebook.com/icml.imls/videos/2236060723167801/

---
Workshop on Exploration in Reinforcement Learning

""Sampling and exploration for control of physical systems"" - Emo Todorov

""Benchmarking Bonus-Based Exploration Methods on the Arcade Learning Environment"" - Adrien Taiga

""Simple Reget Minimzation for Contextual Bandits"" - Aniket Deshmukh

""Some Explorations of Exploration in Reinforcement Learning"" - Pieter Abbeel

https://www.facebook.com/icml.imls/videos/2265408103721327/

---
Workshop Session


 Line attractor dynamics in recurrent networks for sentiment classication 

 Do deep neural networks learn shallow learnable examples first? 

 Crowdsourcing Deep Learning Phenomena

https://www.facebook.com/icml.imls/videos/855147788189057/

---
""Agents that Set Measurable Goals for Themselves"" - Chelsea Finn

https://www.facebook.com/icml.imls/videos/315467659393385/

---
Workshop Session

""Reverse engineering neuroscience and cognitive science principles"" - Aude Oliva

""On Understanding the Hardness of Samples in Neural Networks""

""On the Convex Behavior of Deep Neural Networks in Relation to the Layers' Width""

""Intriguing phenomena in training and generalization dynamics of deep networks"" - Andrew Saxe

https://www.facebook.com/icml.imls/videos/2353033231653025/

---
Workshop session on Self-Supervised Learning

""Self Supervised Learning"" - Yann LeCun

""Revisiting Self-Supervised Visual Representation Learning"" - Alexander Kolesnikov, Xiaohua Zhai, Lucas Beyer

""Data-Efficient Image Recognition with Contrastive Predictive Coding"" - Olivier J. Henaff, Ali Razavi, Carl Doersch, S. M. Ali Eslami, Aaron van den Oord

https://www.facebook.com/icml.imls/videos/378993762742156/

---
Workshop Session on Explroation in Reinforcemnt Learning

""Exploration... in a dangerous world"" - Raia Hadsell

Lightning Talks:

""Curious iLQR: Resolving Uncertainty in Model-based RL"" - Sarah Bechtle

""An Empirical and Conceptual Categorization of Value-based Exploration Methods"" - Niko Yasui

""Skew-Fit: State-Covering Self-Supervised Reinforcement Learning"" - Vitchyr H. Pong

""Optimistic Proximal Policy Optimization"" - Takahisa Imagawa

""Exploration with Unreliable Intrinsic reward in Multi-Agent reinforcement Learning"" - Tabish Rashid

""Parameterized Exploration"" - Lili Wu

""Efficient Exploration in Side-scrolling VIdeo Games with Trajectory Replay"" - I-Huan Chiang

""Hypothesis Driven Exploration for Deep Reinforcement Learning"" - Caleb Chuck

""Epistemic Risk-Sensitive Reinforcemnt Learning"" - Hannes Eriksson

""Near-optimal Optimistic Reinforcement Learning using Empriical Bernstein Inequalities"" - Aristide Tossou

""Improved Tree Search for Automatic Program Synthesis"" - Lior Wolf

""MuleX: Disentangling Exploration and Exploitation in Deep Reinforcement Learning"" - Olivier Teboul

https://www.facebook.com/icml.imls/videos/2324338441219681/

---
Workshop Session on Explroation in Reinforcemnt Learning

""Adapting Behaviour via Intrinsic Rewards to Learn Predictions"" - Martha White

Panel Discussion: Martha White, Jeff Clune, Pulkit Agrawal, and Pieter Abbeel. Moderated by Doina Precup

https://www.facebook.com/icml.imls/videos/1094687407344868/

---
Workshop Session

""Stratagies for mitigating social bias in deep learning systems"" - Olga Russakovsky
Panel Discussion: Kevin Murphy, Nati Srebro, Aude Oliva, Andrew Saxe, Olga Russakovsky Moderator: Ali Rahimi

https://www.facebook.com/icml.imls/videos/2374820496098856/

---
Workshop Session on Self-Supervised Learning

""Self-Supervised learning from videos (with sound)"" - Andrew Zisserman

""SuperSizing+Empowering Self-Supervised Learning"" - Abhinav Gupta

""The Revolution Will Not Be Supervised!"" - Alexei Efros

https://www.facebook.com/icml.imls/videos/2030095370631729/

---
Workshop Session

""The Deep Unknown: on Open-set and Adversarial Examples in Deep Learning"" - Terrance Boult

Panel Discussion (moderated by Tom Dietterich)

https://www.facebook.com/icml.imls/videos/2436992626360413/

---

I thought I would put together a list of the machine learning talks from ICML 2019 since I found they were kind of difficult to look through on facebook, and I figured I would share it here. There may be some minor errors in the listing also. I believe they are mostly available on the ICML website too, but I was just looking through the livestreams: https://icml.cc/Conferences/2019/Videos . I already posted some of these over on /r/reinforcementlearning as well.",9,1
960,2019-11-19,2019,11,19,21,dyjhtn,[R] DeepMind recommender for Google Play Store,https://www.reddit.com/r/MachineLearning/comments/dyjhtn/r_deepmind_recommender_for_google_play_store/,valdanylchuk,1574164802,,1,1
961,2019-11-19,2019,11,19,21,dyjma5,BrisTech Video: CTO of @Gapsquare on Seldon as part of your #ML journey ,https://www.reddit.com/r/MachineLearning/comments/dyjma5/bristech_video_cto_of_gapsquare_on_seldon_as_part/,BakerLJ,1574165532,[removed],0,1
962,2019-11-19,2019,11,19,21,dyjoxq,[D] Difficulties in obtaining a dataset with a license fee,https://www.reddit.com/r/MachineLearning/comments/dyjoxq/d_difficulties_in_obtaining_a_dataset_with_a/,Optimal-Temporary,1574165996,"Throwaway account.

I'm trying to download the [Multi-PIE dataset][1], which has a license fee for academic use. Since it's a large dataset (309GB), the only way provided to obtain it is through shipping a hard-drive. The total fee is 350,00 USD, which includes the dataset license + a hard-drive + international shipping. That is quite expensive for my country standards, so if I could pay only for the dataset license, then I could download it instead of paying the hard-drive + international shipping fees. 

I've contacted the staff if it's possible to buy only the license fee, however they haven't answered yet if it's possible and for what price. If it's possible and I could find a kind soul that could share the dataset with me, it would be really helpful. As the dataset was released in 2009, I understand the reasoning behind shipping a hard-drive. Nowadays, 309GB is still a lot, but more feasible to download.

I'm reaching you out because I'm not seeing alternative ways to obtain the dataset. The current prices were changed to the current value in 2016 (as much as I can't tell from the WebArchive). It's been 10 years since the dataset was published. If they haven't offered an alternative until now, it seems unlikely they will change it anytime soon. In their [FAQ][1], you can see this is requested occasionally:

&gt; **Can the database just be downloaded?**
&gt;
&gt; Since the dataset is so large (308 GB), it has to be shipped on a dedicated USB-attached hard drive. 

Just making clear, they are in their right to charge a license for the dataset, and I want to give them their fair share for their work. I'm just in a tough spot in which I need the dataset to compare with other approaches in the literature and there isn't a less expensive alternative to obtain it (nor response from the staff).

[1]: https://anonym.to/?http://www.flintbox.com/public/project/4742/",3,1
963,2019-11-19,2019,11,19,21,dyjv9q,What happened with What happened with Adversarial Logit Pairing (ALP) for NIPS18,https://www.reddit.com/r/MachineLearning/comments/dyjv9q/what_happened_with_what_happened_with_adversarial/,Liory91,1574167032,[removed],0,1
964,2019-11-19,2019,11,19,21,dyk30t,NLP News Cypher | 11.17.19,https://www.reddit.com/r/MachineLearning/comments/dyk30t/nlp_news_cypher_111719/,Quantum_Stat,1574168299,[removed],0,1
965,2019-11-19,2019,11,19,21,dyk389,ML consultants,https://www.reddit.com/r/MachineLearning/comments/dyk389/ml_consultants/,nomad_world,1574168334,[removed],0,1
966,2019-11-19,2019,11,19,23,dykzny,How to use Machine Learnings to predict Portfolio Performance,https://www.reddit.com/r/MachineLearning/comments/dykzny/how_to_use_machine_learnings_to_predict_portfolio/,h234sd,1574173114,[removed],0,1
967,2019-11-19,2019,11,19,23,dyl8rf,[P] How to apply machine learning and deep learning methods to audio analysis,https://www.reddit.com/r/MachineLearning/comments/dyl8rf/p_how_to_apply_machine_learning_and_deep_learning/,gidime,1574174370,,17,1
968,2019-11-20,2019,11,20,0,dylhzz,Introduction to Statistical Methods in AI,https://www.reddit.com/r/MachineLearning/comments/dylhzz/introduction_to_statistical_methods_in_ai/,atul_at_home,1574175612,,0,1
969,2019-11-20,2019,11,20,0,dylkb2,"[N] Cerebras claims that the CS-1 delivers the performance of more than 1,000 leading GPUs combined",https://www.reddit.com/r/MachineLearning/comments/dylkb2/n_cerebras_claims_that_the_cs1_delivers_the/,MassivePellfish,1574175892,"The Cerebras CS-1 computes deep learning AI problems by being bigger, bigger, and bigger than any other chip

https://techcrunch.com/2019/11/19/the-cerebras-cs-1-computes-deep-learning-ai-problems-by-being-bigger-bigger-and-bigger-than-any-other-chip/

""Cerebras claims that the CS-1 delivers the performance of more than 1,000 leading GPUs combined  a claim that TechCrunch hasnt verified, although we are intently waiting for industry-standard benchmarks in the coming months when testers get their hands on these units.""",37,1
970,2019-11-20,2019,11,20,0,dylm8y,"UCSD Data Science vs GA Tech OMSA, which one is better?",https://www.reddit.com/r/MachineLearning/comments/dylm8y/ucsd_data_science_vs_ga_tech_omsa_which_one_is/,zomkungzz,1574176154,[removed],0,1
971,2019-11-20,2019,11,20,0,dylu4r,[D] Best of AI October 2019,https://www.reddit.com/r/MachineLearning/comments/dylu4r/d_best_of_ai_october_2019/,tarteflambee,1574177126,"Hi everyone, I wanted to share with you my first article that just got published today in Sicara's blog. I've written about my favorite 10 news in AI that came out in October. I would be happy to have your feedback! Which are your favorite AI articles this November? Thanks in advance!   
[https://www.sicara.ai/blog/10-2019-best-of-ai-october-2019](https://www.sicara.ai/blog/10-2019-best-of-ai-october-2019)",3,1
972,2019-11-20,2019,11,20,0,dymb2b,Image augmentation for MRI in Detectron2 (MaskRCNN-like network),https://www.reddit.com/r/MachineLearning/comments/dymb2b/image_augmentation_for_mri_in_detectron2/,johnathanjones1998,1574179184,[removed],1,1
973,2019-11-20,2019,11,20,1,dymbbs,[D] What is the state-of-the-art implementation for style transfer?,https://www.reddit.com/r/MachineLearning/comments/dymbbs/d_what_is_the_stateoftheart_implementation_for/,DependentSky6,1574179220,"Hello, 

I'm having a hard time finding an implementation of style transfer that would work as well as an example like this : 

[Style transfer that works well](https://preview.redd.it/683t2hxzxnz31.png?width=1800&amp;format=png&amp;auto=webp&amp;s=6f7d871ba8249a898550f0fa405c50389a23c378)

See how the details of the wave are well transferred to the hair and beard? I'm looking for an implementation of style transfer that is able to do this type of detail. 

I have not been lucky in finding a style transfer algorithm that works like this. For instance a well cited paper like [AdaIn style transfer](https://arxiv.org/abs/1703.06868) typically yields something like this (see [github](https://github.com/xunhuang1995/AdaIN-style) implementation):   


[Style transfer that does not work that well](https://preview.redd.it/b0quzh2iznz31.png?width=2316&amp;format=png&amp;auto=webp&amp;s=aea430958ccd81dd54bf1ce3c38add1ac2f59de3)

What I'm not looking for is this type of style transfer that only changes the colors and roughly the shapes of the image content. But in the end we cannot really find the ""identity"" of the style image (pencil drawing).

I suspect that the first image with pewdiepie might have used photoshop in some ways... But if you guys think it's possible to have this type of output using only style transfer I would really like a bit of help here. Thanks!",8,1
974,2019-11-20,2019,11,20,1,dymfdv,Simple Semantic Image Segmentation in an iOS Application  DeepLabV3 Implementation,https://www.reddit.com/r/MachineLearning/comments/dymfdv/simple_semantic_image_segmentation_in_an_ios/,omarmhaimdat,1574179680,,0,1
975,2019-11-20,2019,11,20,1,dympa5,[D] Are ALL problems that are classified under BPP and ZPP complexity solvable using statistical/machine learning/Deep learning-based approaches?,https://www.reddit.com/r/MachineLearning/comments/dympa5/d_are_all_problems_that_are_classified_under_bpp/,rulerofthehell,1574180877,"BPP complexity:
https://en.wikipedia.org/wiki/BPP_(complexity)

ZPP complexity:
https://en.wikipedia.org/wiki/ZPP_(complexity)",3,1
976,2019-11-20,2019,11,20,1,dyn06z,Keypoint annotation tool,https://www.reddit.com/r/MachineLearning/comments/dyn06z/keypoint_annotation_tool/,lpatks,1574182191,[removed],0,1
977,2019-11-20,2019,11,20,1,dyn285,[D] what are the current production and SOTA algorithms behind chatbots?,https://www.reddit.com/r/MachineLearning/comments/dyn285/d_what_are_the_current_production_and_sota/,mesmer_adama,1574182427,"In many companies today there are talks about using chatbots, often that means using an existing framework. But what are the current parts and their algorithms that are used in those systems. 

Intent detection and NER are the ones I am familiar with. 

What are examples of common algorithms/papers used in production?
What are the SOTA alternatives?
What else is part of the commercial chatbot pipelines besides intent and NER?",12,1
978,2019-11-20,2019,11,20,2,dyn6bu,"This may be too basic for this sub, but is there something like a ""self-partitioning histogram""?",https://www.reddit.com/r/MachineLearning/comments/dyn6bu/this_may_be_too_basic_for_this_sub_but_is_there/,rumborak,1574182909,[removed],0,1
979,2019-11-20,2019,11,20,2,dynaka,Can we train YOLO with circles as a ground truth instead of bounding boxes?,https://www.reddit.com/r/MachineLearning/comments/dynaka/can_we_train_yolo_with_circles_as_a_ground_truth/,drkolenklow,1574183394,,0,1
980,2019-11-20,2019,11,20,2,dynslq,[D] Which ML task(s) would you use to solve this problem?,https://www.reddit.com/r/MachineLearning/comments/dynslq/d_which_ml_tasks_would_you_use_to_solve_this/,Falkenauge,1574185494,"Hi, I wanted to work with machine learning as a project task and I was able to get my hands on a real problem.
As I'm still testing the waters with machine learning, I would like to get input of the experienced community here.

Someone used a drone to gather flight data, their overall goal is to find out what influences the network quality so that they can predict it in unknown territory.

I have flight data (time/coordinates,cell tower, network quality) for the territory A, however territory A is only a very small art of the overall territory.

So I have to predict the network quality of untested territories, with the help of the exisitng data. Fun!

--

So much about the problem, what I was trying to learn the last week is what kind of machine learning task I could use for this.

It seems to me that I would need a supervised regression task. Am I correct in that assumption? Am I thinking to simple?

Thanks for any and all input.",5,1
981,2019-11-20,2019,11,20,2,dyntqz,CrowdforThink : Blog -5 Skills You Need to Become a Machine Learning Engineer,https://www.reddit.com/r/MachineLearning/comments/dyntqz/crowdforthink_blog_5_skills_you_need_to_become_a/,crowdforapps,1574185640,,0,1
982,2019-11-20,2019,11,20,3,dyo65v,"[D] Michael Kearns: Algorithmic Fairness, Bias, Privacy, and Ethics in Machine Learning | Artificial Intelligence Podcast",https://www.reddit.com/r/MachineLearning/comments/dyo65v/d_michael_kearns_algorithmic_fairness_bias/,UltraMarathonMan,1574187099,"Michael Kearns is a professor at University of Pennsylvania and a co-author of the new book Ethical Algorithm that is the focus of much of our conversation, including algorithmic fairness, bias, privacy, and ethics in general. But, that is just one of many fields that Michael is a world-class researcher in, some of which we touch on quickly including learning theory or theoretical foundations of machine learning, game theory, algorithmic trading, quantitative finance, computational social science, and more.

**Video:** [https://www.youtube.com/watch?v=AzdxbzHtjgs](https://www.youtube.com/watch?v=AzdxbzHtjgs)  
**Audio:** [https://lexfridman.com/michael-kearns](https://lexfridman.com/michael-kearns)

**Outline:**  
*(click on the timestamp to jump to that part of the video)*

[0:00](https://www.youtube.com/watch?v=AzdxbzHtjgs&amp;t=0s) \- Introduction  
[2:45](https://www.youtube.com/watch?v=AzdxbzHtjgs&amp;t=165s) \- Influence from literature and journalism  
[7:39](https://www.youtube.com/watch?v=AzdxbzHtjgs&amp;t=459s) \- Are most people good?  
[13:05](https://www.youtube.com/watch?v=AzdxbzHtjgs&amp;t=785s) \- Ethical algorithm  
[24:28](https://www.youtube.com/watch?v=AzdxbzHtjgs&amp;t=1468s) \- Algorithmic fairness of groups vs individuals  
[33:36](https://www.youtube.com/watch?v=AzdxbzHtjgs&amp;t=2016s) \- Fairness tradeoffs  
[46:29](https://www.youtube.com/watch?v=AzdxbzHtjgs&amp;t=2789s) \- Facebook, social networks, and algorithmic ethics  
[58:05](https://www.youtube.com/watch?v=AzdxbzHtjgs&amp;t=3485s) \- Machine learning  
[59:19](https://www.youtube.com/watch?v=AzdxbzHtjgs&amp;t=3559s) \- Algorithm that determines what is fair  
[1:01:25](https://www.youtube.com/watch?v=AzdxbzHtjgs&amp;t=3685s) \- Computer scientists should think about ethics  
[1:05:59](https://www.youtube.com/watch?v=AzdxbzHtjgs&amp;t=3959s) \- Algorithmic privacy  
[1:11:50](https://www.youtube.com/watch?v=AzdxbzHtjgs&amp;t=4310s) \- Differential privacy  
[1:19:10](https://www.youtube.com/watch?v=AzdxbzHtjgs&amp;t=4750s) \- Privacy by misinformation  
[1:22:31](https://www.youtube.com/watch?v=AzdxbzHtjgs&amp;t=4951s) \- Privacy of data in society  
[1:27:49](https://www.youtube.com/watch?v=AzdxbzHtjgs&amp;t=5269s) \- Game theory  
[1:29:40](https://www.youtube.com/watch?v=AzdxbzHtjgs&amp;t=5380s) \- Nash equilibrium  
[1:30:35](https://www.youtube.com/watch?v=AzdxbzHtjgs&amp;t=5435s) \- Machine learning and game theory  
[1:34:52](https://www.youtube.com/watch?v=AzdxbzHtjgs&amp;t=5692s) \- Mutual assured destruction  
[1:36:56](https://www.youtube.com/watch?v=AzdxbzHtjgs&amp;t=5816s) \- Algorithmic trading  
[1:44:09](https://www.youtube.com/watch?v=AzdxbzHtjgs&amp;t=6249s) \- Pivotal moment in graduate school

https://preview.redd.it/auuvygjlooz31.png?width=1280&amp;format=png&amp;auto=webp&amp;s=5001b4f3493cb4aae67caa484fe32b4db0bde477",1,1
983,2019-11-20,2019,11,20,3,dyoha8,Announcing Jupyter Notebooks on AI Cheatsheets(https://www.aicheatsheets.com) - Free 10 Notebook Hours,https://www.reddit.com/r/MachineLearning/comments/dyoha8/announcing_jupyter_notebooks_on_ai/,kailashahirwar12,1574188370,[removed],0,1
984,2019-11-20,2019,11,20,3,dyojpj,Ideas of machine-learning-based projects/apps that will make people's life better,https://www.reddit.com/r/MachineLearning/comments/dyojpj/ideas_of_machinelearningbased_projectsapps_that/,AchrafAmil,1574188646,[removed],0,1
985,2019-11-20,2019,11,20,5,dypthd,Can Bots Surpass the Realism of Human Dialogue?,https://www.reddit.com/r/MachineLearning/comments/dypthd/can_bots_surpass_the_realism_of_human_dialogue/,Yuqing7,1574193705,,0,1
986,2019-11-20,2019,11,20,5,dypwgn,[R] Learning to Simulate (blog post),https://www.reddit.com/r/MachineLearning/comments/dypwgn/r_learning_to_simulate_blog_post/,StrawberryNumberNine,1574194003,,0,39
987,2019-11-20,2019,11,20,5,dypz3g,[D] Over/Under/SMOTE sampling for EXTREMELY imbalanced data without getting data?,https://www.reddit.com/r/MachineLearning/comments/dypz3g/d_overundersmote_sampling_for_extremely/,dattud,1574194293,"I am working on a case study where they gave me 3 text, 2 categorical, 1 numerical features to classify 6 classes.

However, the data is very imbalanced. Its splits like this:

Case_1: 5215/5899 = 88.4% 

Case_2: 631/5899 = 10.7% 

Case_3: 23/5899 =  0.39% 

Case_4: 16/5899 = 0.27% 

Case_5: 2/5899 = 0.03% 

Case_6: 12/5899 = 0.2% 

and  Case_5 comes to only 1 observation after splitting data to training.

To me, it seems like over sampling minorities might result in serious overfitting. Undersampling from 5215 might result in some serious data loss. I don't know what to do. I did do the bias to weights to log reg, but only got decent results:

normalized confusion matrix (True Positive percents):

Category_1: 96% which is 1.08 times better

Category_2: 86% which is 8.03 times better

Category_3: 100% which is 256 times better 

Category_4: 80% 296 times better

Category_5: 0% since it was only 1 example in test data

Category_6: 75% which is 375 times better",4,3
988,2019-11-20,2019,11,20,5,dyq6ca,RecSim: A Configurable Simulation Platform for Recommender Systems,https://www.reddit.com/r/MachineLearning/comments/dyq6ca/recsim_a_configurable_simulation_platform_for/,sjoerdapp,1574195106,,0,1
989,2019-11-20,2019,11,20,5,dyqk76,For Busy IT Professionals who struggle to gain muscle weight and/or lose fat,https://www.reddit.com/r/MachineLearning/comments/dyqk76/for_busy_it_professionals_who_struggle_to_gain/,siso_jake,1574196661,[removed],0,1
990,2019-11-20,2019,11,20,6,dyrj79,Parameter Adjustment based only on tagged predictions,https://www.reddit.com/r/MachineLearning/comments/dyrj79/parameter_adjustment_based_only_on_tagged/,EuclidiaFlux,1574200474,[removed],0,1
991,2019-11-20,2019,11,20,7,dys242,Loss function for increasing the quality of the image when labels are not perfectly alligned,https://www.reddit.com/r/MachineLearning/comments/dys242/loss_function_for_increasing_the_quality_of_the/,ikadorus,1574202652,[removed],0,1
992,2019-11-20,2019,11,20,7,dysayu,[XGBoost] Anyone has tips for complex dataset ?,https://www.reddit.com/r/MachineLearning/comments/dysayu/xgboost_anyone_has_tips_for_complex_dataset/,dztrader62,1574203656,[removed],0,1
993,2019-11-20,2019,11,20,8,dysk98,mL,https://www.reddit.com/r/MachineLearning/comments/dysk98/ml/,Value255,1574204700,[removed],0,1
994,2019-11-20,2019,11,20,8,dysn34,[R] The Option Keyboard Combining Skills in Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/dysn34/r_the_option_keyboard_combining_skills_in/,feedtheaimbot,1574205031,,0,1
995,2019-11-20,2019,11,20,9,dytglv,RandAugment: Practical automated data augmentation with a reduced search space,https://www.reddit.com/r/MachineLearning/comments/dytglv/randaugment_practical_automated_data_augmentation/,AlleUndKalle,1574208667,[removed],0,1
996,2019-11-20,2019,11,20,9,dytjic,[P] askgpt.com I built a simple ui to converse with gpt-2,https://www.reddit.com/r/MachineLearning/comments/dytjic/p_askgptcom_i_built_a_simple_ui_to_converse_with/,realgt,1574209008,"As part of small personal project learning about ML, I built a simple interface to ask questions of the gpt-2 model by hacking together a syntax which reliably returns responses to questions.....similar to Alexa or Siri. Its not the smartest or fastest ai but, at least it doesn't track you

[https://www.askgpt.com](https://www.askgpt.com)

&amp;#x200B;

[\\\_\(\)\_\/](https://preview.redd.it/t07vdn5tiqz31.png?width=441&amp;format=png&amp;auto=webp&amp;s=69b073b164b843f13dc2104115fc36ab47affd73)

any feedback is welcome",20,3
997,2019-11-20,2019,11,20,10,dyuabl,A prospective employer won't tell me what I would be working on unless I accept the job offer. Is this normal?,https://www.reddit.com/r/MachineLearning/comments/dyuabl/a_prospective_employer_wont_tell_me_what_i_would/,spauldeagle,1574212388,[removed],0,1
998,2019-11-20,2019,11,20,10,dyub09,SandroGan- DCGAN that can generate paintings like the great Sandro Botticelli,https://www.reddit.com/r/MachineLearning/comments/dyub09/sandrogan_dcgan_that_can_generate_paintings_like/,DongDilly,1574212479,,0,1
999,2019-11-20,2019,11,20,10,dyug4o,[D] Early coauthorship with top scientists predicts success in academic careers,https://www.reddit.com/r/MachineLearning/comments/dyug4o/d_early_coauthorship_with_top_scientists_predicts/,hardmaru,1574213133,"Doesn't related directly with ML research, but still interesting to see whether it applies as much as other scientific fields:

Article from Nature Communications: [Early coauthorship with top scientists predicts success in academic careers](https://www.nature.com/articles/s41467-019-13130-4)

*Abstract*

We examined the long-term impact of coauthorship with established, highly-cited scientists on the careers of junior researchers in four scientific disciplines. Here, using matched pair analysis, we find that junior researchers who coauthor work with top scientists enjoy a persistent competitive advantage throughout the rest of their careers, compared to peers with similar early career profiles but without top coauthors. Such early coauthorship predicts a higher probability of repeatedly coauthoring work with top-cited scientists, and, ultimately, a higher probability of becoming one. Junior researchers affiliated with less prestigious institutions show the most benefits from coauthorship with a top scientist. As a consequence, we argue that such institutions may hold vast amounts of untapped potential, which may be realised by improving access to top scientists.

https://www.nature.com/articles/s41467-019-13130-4",49,1
1000,2019-11-20,2019,11,20,11,dyuwn5,[Discussion] NLP Embeddings Applied to Classification,https://www.reddit.com/r/MachineLearning/comments/dyuwn5/discussion_nlp_embeddings_applied_to/,outswimtheshark,1574215301,"Ive been experimenting with word embeddings lately from different frameworks like BERT and ELMo. Ive tried applying these to a sequence classification problem (generated sequence embeddings by taking mean of token embeddings in the second to last hidden layer of the BERT model) and running logistic regression and random forest models using these embeddings. 

However, it doesnt seem like this works that well for small datasets (in my case, 500 data points for a 3-label classification problem). Am I correct in saying that classification using these embeddings only works well given tens of thousands of data points? All the sequence classification problems Ive seen using these embeddings seem to support this since they have way more data (e.g. Googles IMDB movie review sentiment example). Or are there ways you can get robust classification models with less data to work with? I was thinking of trying fine-tuning or PCA to reduce the dimensionality of the sequence embeddings and ultimately build a better classification model.",4,1
1001,2019-11-20,2019,11,20,11,dyv4wb,new M.S. in Natural Language Processing at UCSC Silicon Valley,https://www.reddit.com/r/MachineLearning/comments/dyv4wb/new_ms_in_natural_language_processing_at_ucsc/,marilynawalker,1574216320,[removed],0,1
1002,2019-11-20,2019,11,20,13,dywzsl,Looking for a textbook or course that teaches machine learning with examples calculated by hand.,https://www.reddit.com/r/MachineLearning/comments/dywzsl/looking_for_a_textbook_or_course_that_teaches/,gtboy1994,1574225323,,0,1
1003,2019-11-20,2019,11,20,14,dyxanu,[R] The Deep Learning Revolution and Its Implications for Computer Architecture and Chip Design,https://www.reddit.com/r/MachineLearning/comments/dyxanu/r_the_deep_learning_revolution_and_its/,hardmaru,1574226912,,2,1
1004,2019-11-20,2019,11,20,14,dyxc71,Places365 Dataset,https://www.reddit.com/r/MachineLearning/comments/dyxc71/places365_dataset/,arjundupa,1574227134,"[Link](http://places2.csail.mit.edu/download.html) suggests that the training set of the Places365-Standard dataset is 105 GB. I don't have this much storage. Is there a way I can still access these images?

Also, I have a set of 4000 images that I got from Google Images, and I want to see whether any of these exist in the training set of the Places365-Standard dataset. Is there a way to do this?

Any ideas will be greatly appreciated, thanks!",0,1
1005,2019-11-20,2019,11,20,14,dyxeh9,Bot Marketplace - 2,https://www.reddit.com/r/MachineLearning/comments/dyxeh9/bot_marketplace_2/,getengati,1574227461,[removed],0,1
1006,2019-11-20,2019,11,20,15,dyy0y4,Sorting multi-index Series by both Indexes and Values Without Breaking the Index,https://www.reddit.com/r/MachineLearning/comments/dyy0y4/sorting_multiindex_series_by_both_indexes_and/,windy_yiling,1574231000,"It is used for sorting multi-index series with by both indexes and values like this:

&amp;#x200B;

[Raw Data](https://preview.redd.it/hscevxk91sz31.png?width=215&amp;format=png&amp;auto=webp&amp;s=db9e86ebd69994dff3238af13fc6f363d655fdcf)

In this series, we have two indexes: first and second. The third column presents value.

What we need to do is to sort this series by 'first' index and value, without breaking the chunk made by 'first' index

The final outcome should like this:

&amp;#x200B;

[Outcome](https://preview.redd.it/om7p7bmh1sz31.png?width=187&amp;format=png&amp;auto=webp&amp;s=53ddc719a181e793bab953292187d9987e8fb776)

Let's start the discussion:

Let s = series we want to sort

If we use built-in functions inside pandas to sort this kind of series, the outcome will look like this:

    s.sort_index(level = 'first').sort_values()

&amp;#x200B;

*Processing img no3ro6yn7sz31...*

It breaks the 'first' index, that is not we want

To overcome this problem, we need to convert this series into the dataframe by using reset\_index()

     s_df = s.reset_index() 

 Dataframe provide a function also called sort\_values(), because of the data type 'Dataframe' does not have multi-index, the different indexes in series, as well as values, will be considered as cells in dataframe. 

sort\_values() are able to sort cells inside detaframe by different priority, so we can set the index in series to higher priority.

    
s_sorted_df = s_df.sort_values(by = ['first',0]) 

'0' is the default column name of values when a series were converted into dataframe.

&amp;#x200B;

[After applying sort\_values\(\)](https://preview.redd.it/fmp8yp3w9sz31.png?width=226&amp;format=png&amp;auto=webp&amp;s=9d117792f79a7bd984acdaa3b2b49c9ea6dd4a33)

 Now use this dataframe to rebuild a series, the new series is the final outcome we want

     index1 = s_sorted_df.loc[:,'first'].tolist()
     index2 = s_sorted_df.loc[:,'second'].tolist() 
     sorted_values = s_df.loc[:,0] 

Remember you need to convert the values into numpy.array otherwise it cannot be used in series

    sorted_values = np.array(sorted_values)

 Now assemble everything to form a series:

    sorted_index = [index1,index2]
sorted_s = pd.Series(sorted_values,index=sorted_index)

&amp;#x200B;

https://preview.redd.it/e6q0vy9qasz31.png?width=190&amp;format=png&amp;auto=webp&amp;s=a1c3d749decee43f270c2dc4b45ab10bd1fa97e0

That is what we want.

&amp;#x200B;

Notebook file on Github: [https://github.com/610yilingliu/code-for-blog/blob/master/index\_value\_sorting.ipynb](https://github.com/610yilingliu/code-for-blog/blob/master/index_value_sorting.ipynb)

If you are not a Chinese speaker, just ignore those Chinese inside the notebook. It is just a translation of the English contents.",0,1
1007,2019-11-20,2019,11,20,16,dyylrr,PCB separator machine - Southern Machinery,https://www.reddit.com/r/MachineLearning/comments/dyylrr/pcb_separator_machine_southern_machinery/,smthelp1,1574234644,,0,1
1008,2019-11-20,2019,11,20,17,dyyy1f,distance preserving in autoencoder,https://www.reddit.com/r/MachineLearning/comments/dyyy1f/distance_preserving_in_autoencoder/,JzinOu,1574236906,[removed],0,1
1009,2019-11-20,2019,11,20,17,dyz2kc,"Which open source NER Model is the best ? Comparing CoreNLP, Spacy and Flair",https://www.reddit.com/r/MachineLearning/comments/dyz2kc/which_open_source_ner_model_is_the_best_comparing/,TalkingJellyFish,1574237744,,3,1
1010,2019-11-20,2019,11,20,17,dyz4g1,PCB separator machine - Southern Machinery,https://www.reddit.com/r/MachineLearning/comments/dyz4g1/pcb_separator_machine_southern_machinery/,smthelp1,1574238110,,0,1
1011,2019-11-20,2019,11,20,17,dyz7qm,Can brain signals of detecting objects be converted to relevant object detection models?,https://www.reddit.com/r/MachineLearning/comments/dyz7qm/can_brain_signals_of_detecting_objects_be/,sreejith_kumar_m,1574238744,[removed],0,1
1012,2019-11-20,2019,11,20,17,dyz9ja,AI and Machine Learning - The Messi &amp; Ronaldo of the IT World,https://www.reddit.com/r/MachineLearning/comments/dyz9ja/ai_and_machine_learning_the_messi_ronaldo_of_the/,AnujG23,1574239092,,0,1
1013,2019-11-20,2019,11,20,18,dyzj19,"[D] Regarding the ability of neural networks to learn ""simple"" examples first",https://www.reddit.com/r/MachineLearning/comments/dyzj19/d_regarding_the_ability_of_neural_networks_to/,Minimum_Zucchini,1574240894,"So I've been pretty interested in this paper (A Closer Look at Memorization in Deep Networks)[https://arxiv.org/abs/1706.05394] and particularly the first experiment they did where they showed that certain data points are consistently fit in the first epoch of training whereas other data points consistently take longer epochs to fit. 

But I haven't seen any discussions anywhere about why that would be the case? Like what is it about these data points that allows them to be easily fit in the first epoch? How can we formalize this notion of ""simpleness""? 

My first thought is that the ""simple"" data are just the ones which have a gradient direction that is close to the averaged gradient direction for a given minibatch?

Anyone aware of any work specifically expanding on these questions? 


Unfortunately I don't have anyone in my lab to discuss these things with so I just resort to the next best place lol.",12,1
1014,2019-11-20,2019,11,20,19,dz00nl,70+ Machine Learning Datasets  Gain real-world experience with Data Science projects,https://www.reddit.com/r/MachineLearning/comments/dz00nl/70_machine_learning_datasets_gain_realworld/,karan991136,1574244117,[removed],0,1
1015,2019-11-20,2019,11,20,19,dz022q,[P] Deploying a working model on the server.,https://www.reddit.com/r/MachineLearning/comments/dz022q/p_deploying_a_working_model_on_the_server/,retardis_roark,1574244356,"Hello guys, 

I recently developed a CNN to analyse and cut images (image text documents). I saved the entire network locally as "".h5"" extension and whenever I have to use it I call it. I also use ""pytesseract"" (OCR Library) to extract data from the cutouts. I have created a data frame using pandas where I append the results from tesseract to maintain the logs. Currently, I am using Jupyter notebook.

I want to upload it all on the server to automate this process so I can daily check the DataFrame without the hassle of running all the notebooks. I currently have a subscription for DigitalOcean's server. 

Any leads or help on how to do this will be appreciated.",3,1
1016,2019-11-20,2019,11,20,19,dz09r7,[Research] Announcing Kaolin - PyTorch Library for Accelerating 3D Deep Learning Research,https://www.reddit.com/r/MachineLearning/comments/dz09r7/research_announcing_kaolin_pytorch_library_for/,cdossman,1574245759,"A group of researchers who were working at NVIDIA has introduced Kaolin, a new PyTorch library with an aim to accelerate 3D deep learning research. Kaolin is home for future 3D DL research and you are welcome to make contributions. 

Read more: [https://medium.com/ai%C2%B3-theory-practice-business/pytorch-library-for-accelerating-3d-deep-learning-research-6b83df2073bf](https://medium.com/ai%C2%B3-theory-practice-business/pytorch-library-for-accelerating-3d-deep-learning-research-6b83df2073bf)",1,1
1017,2019-11-20,2019,11,20,19,dz0d3o,Using AI to Help Airlines Make the Most of Ancillary Income,https://www.reddit.com/r/MachineLearning/comments/dz0d3o/using_ai_to_help_airlines_make_the_most_of/,venkatvajradhar,1574246354,,0,1
1018,2019-11-20,2019,11,20,20,dz0lxj,How to detect AI Snake oil,https://www.reddit.com/r/MachineLearning/comments/dz0lxj/how_to_detect_ai_snake_oil/,crazy_boss_reloaded,1574247974,,81,1
1019,2019-11-20,2019,11,20,20,dz0uwt,[D] Is it ill-advised to perform transfer learning with generalized linear models?,https://www.reddit.com/r/MachineLearning/comments/dz0uwt/d_is_it_illadvised_to_perform_transfer_learning/,Lewba,1574249563,"I've typically only performed transfer learning via fine-tuning with neural networks (eg. image classifiers from pre-trained MobileNet, etc.), but does the same idea hold for a model like logistic regression or CRF? I'd argue yes because your essentially just training a new model with non-randomized initial weights (a prior). But am I missing something?

I'm currently looking into cross-domain transfer learning for non-neural NER models, and I wanted to fine-tune the weights of a pre-trained CRF with some newly annotated user-generated data.",2,1
1020,2019-11-20,2019,11,20,21,dz181x,Most Innovative AI and Machine Learning Companies in 2019,https://www.reddit.com/r/MachineLearning/comments/dz181x/most_innovative_ai_and_machine_learning_companies/,OliviaWillson,1574251783,[removed],0,1
1021,2019-11-20,2019,11,20,21,dz19sv,TimeSeries Prediction - Fault occurrence based on multiple features,https://www.reddit.com/r/MachineLearning/comments/dz19sv/timeseries_prediction_fault_occurrence_based_on/,vigg_1991,1574252064,[removed],0,1
1022,2019-11-20,2019,11,20,21,dz1cut,Machine Learning Using Python?,https://www.reddit.com/r/MachineLearning/comments/dz1cut/machine_learning_using_python/,onlineit3,1574252575,,0,1
1023,2019-11-20,2019,11,20,21,dz1en4,[Discussion] TimeSeries Prediction - Fault occurrence based on multiple features,https://www.reddit.com/r/MachineLearning/comments/dz1en4/discussion_timeseries_prediction_fault_occurrence/,vigg_1991,1574252875," \[Discussion\]

The data set is time series with 1 min frequency for the last four years. There are 40 features associated with the asset. Then there is a target which has a 0 when there is no fault and 1 when there is fault.

having this data, currently I have approached it in this way.

1. I have set the column 0 as index and set the type as date-time.
2. The 39 features had any empty values in-between, I interpolated linearly for now to get the values assigned to them

Now when browse online, I only see people picking one column and doing time series analysis to identify anomaly,  in my case, I want to use the features in time sequence and then identify if the asset will fail or not in the future anytime/ any date.

Can someone help me understand what approach I need to take for this kind of a problem.",0,1
1024,2019-11-20,2019,11,20,22,dz20pf,[Discussion] How legitimate is this?,https://www.reddit.com/r/MachineLearning/comments/dz20pf/discussion_how_legitimate_is_this/,DanteIsBack,1574256333,,1,1
1025,2019-11-20,2019,11,20,22,dz23mt,[D] Machine Learning Roadmap 2019,https://www.reddit.com/r/MachineLearning/comments/dz23mt/d_machine_learning_roadmap_2019/,BlisteringBernacle,1574256755,,1,1
1026,2019-11-20,2019,11,20,23,dz34lb,[P] Machine Learning Flight Rules,https://www.reddit.com/r/MachineLearning/comments/dz34lb/p_machine_learning_flight_rules/,16yoMLDev,1574261765,"A guide for astronauts (now, people doing machine learning) about what to do when things go wrong.

GitHub: https://github.com/bkkaggle/machine-learning-flight-rules

Product Hunt: https://www.producthunt.com/posts/machine-learning-flight-rules

There's a lot of ""hidden knowledge"" online on places like Stackoverflow, Kaggle, and the Pytorch discussion forums that is really useful but not easily accessible to people who are just getting started with machine learning.  This is why I made Machine learning flight rules, this Github repo compiles all of the things I have learned over the last two years about best practices, common mistakes, and little-known tricks when training neural networks.  I've tried to make sure that all the information in this repository is accurate, but if you find something that you think is wrong, please let me know by opening an issue.  This repository is still a work in progress, so if you find a bug, think there is something missing, or have any suggestions for new features, feel free to open an issue or a pull request. Feel free to use the library or code from it in your own projects, and if you feel that some code used in this project hasn't been properly accredited, please open an issue.  I named this project after the awesome Git Flight Rules project (https://github.com/k88hudson/git-flight-rules).   I took a lot of tips from both Andrej Kaparthy's blog post on a recipe for training neural networks (https://karpathy.github.io/2019/04/25/recipe/) and the Amid Fish blog post on lessons learned when reporoducing a deep reinforcement learning paper (http://amid.fish/reproducing-deep-rl)",1,1
1027,2019-11-21,2019,11,21,0,dz3cot,Would anyone be able to help me with this neural network assignment? I'm not sure how to do this. Anything would help,https://www.reddit.com/r/MachineLearning/comments/dz3cot/would_anyone_be_able_to_help_me_with_this_neural/,JackTheEntry,1574262754,[removed],0,1
1028,2019-11-21,2019,11,21,0,dz3dkd,Other ML/DL technical discussion/Q&amp;A groups besides reddit and stackoverflow?,https://www.reddit.com/r/MachineLearning/comments/dz3dkd/other_mldl_technical_discussionqa_groups_besides/,bandalorian,1574262863,"When I have more obscure/specific questions I find they are not getting answered as often. Most of my questions are more on the engineering side, and sometimes I'm just stuck and could use input from someone who's worked on similar problems. My go tos are reddit and stackoverflow and sometime [stats.stackexchange.com](https://stats.stackexchange.com). I tried the r/ML slack channel but found it harder to get answers there. Are there any other such places that are worth being aware of? Thanks!",0,1
1029,2019-11-21,2019,11,21,0,dz3e82,What are the advantages of using a GAN over a CNN for image translation tasks?,https://www.reddit.com/r/MachineLearning/comments/dz3e82/what_are_the_advantages_of_using_a_gan_over_a_cnn/,DryCredit,1574262946,[removed],0,1
1030,2019-11-21,2019,11,21,0,dz3ed6,[D] Likelihood vs probability ?,https://www.reddit.com/r/MachineLearning/comments/dz3ed6/d_likelihood_vs_probability/,anirudh3832,1574262965,,0,1
1031,2019-11-21,2019,11,21,0,dz3i6x,[R]Research Guide: Model Distillation Techniques for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/dz3i6x/rresearch_guide_model_distillation_techniques_for/,mwitiderrick,1574263418,"Knowledge distillation is a model compression technique whereby a small network (student) is taught by a larger trained neural network (teacher). The smaller network is trained to behave like the large neural network. This enables the deployment of such models on small devices such as mobile phones or other edge devices. In this guide, well look at a couple of papers that attempt to tackle this challenge.

[https://heartbeat.fritz.ai/research-guide-model-distillation-techniques-for-deep-learning-4a100801c0eb](https://heartbeat.fritz.ai/research-guide-model-distillation-techniques-for-deep-learning-4a100801c0eb)",2,1
1032,2019-11-21,2019,11,21,0,dz3wh3,"Simple Questions Thread November 20, 2019",https://www.reddit.com/r/MachineLearning/comments/dz3wh3/simple_questions_thread_november_20_2019/,AutoModerator,1574265159,[removed],0,1
1033,2019-11-21,2019,11,21,1,dz4aox,Big RNNs Achieve SOTA Performance in Video Prediction,https://www.reddit.com/r/MachineLearning/comments/dz4aox/big_rnns_achieve_sota_performance_in_video/,Yuqing7,1574266917,,0,1
1034,2019-11-21,2019,11,21,1,dz4rz9,What are best practices for drawing bounding boxes for object detection?,https://www.reddit.com/r/MachineLearning/comments/dz4rz9/what_are_best_practices_for_drawing_bounding/,Rando_mango,1574269009,"I am working to label a bunch of images for object detection. Specifically I am looking at images from cameras in sewer pipes. One specific challenge I am looking at is labelling roots. Roots come in several different varieties - clumped as a ball or many different strands. Im looking for some guidance on how to draw bounding boxes.

Any guidance is much appreciated.

Thanks

Raw Image

&amp;#x200B;

https://preview.redd.it/ywipiv99gvz31.jpg?width=383&amp;format=pjpg&amp;auto=webp&amp;s=af8d70b4295a4e52e6f84f984927ff3d1c2bc2b5

Many boxes

&amp;#x200B;

https://preview.redd.it/u2gkawx9gvz31.jpg?width=457&amp;format=pjpg&amp;auto=webp&amp;s=9441e2abd6e24732c79dc03acf07aee96246beaf

one box

&amp;#x200B;

https://preview.redd.it/76kwqpnfgvz31.jpg?width=464&amp;format=pjpg&amp;auto=webp&amp;s=7fff67fbee700dbad3ec966186d9c704f172fc54

larger root

&amp;#x200B;

https://preview.redd.it/uwe4n3gggvz31.jpg?width=650&amp;format=pjpg&amp;auto=webp&amp;s=c1096d8854b39ec4dd1302c19e4d77cda3065654",0,1
1035,2019-11-21,2019,11,21,2,dz4u37,DeepMind MuZero beats AlphaZero,https://www.reddit.com/r/MachineLearning/comments/dz4u37/deepmind_muzero_beats_alphazero/,dshawul2,1574269280,"Deepmind creates an AI that learns rules of games by itself (atari,chess,go etc) and beats AlphaZero despite using fewer blocks. This seems to be the real ""zero"" approach but I am sure we will find a fault soon.

Paper: [https://arxiv.org/pdf/1911.08265.pdf](https://arxiv.org/pdf/1911.08265.pdf)",0,1
1036,2019-11-21,2019,11,21,2,dz569o,ML Chatbot Capacity based on Chat Support Data,https://www.reddit.com/r/MachineLearning/comments/dz569o/ml_chatbot_capacity_based_on_chat_support_data/,Smartchicken5,1574270665,"Hey, People!   
I have to create a chat Suggestion system that will use the dataset of 'past chat history of product users with support agents'. Having the timeline of 3-4 months and being beginner to machine learning, I want your advice for the possibilities and path which I should use to develop this project. I got to know that i can get help on reddit community, so I signed up here! I welcome your thoughts here. :)",0,1
1037,2019-11-21,2019,11,21,2,dz5be0,Is TensorFlow suitable for my timeseries project?,https://www.reddit.com/r/MachineLearning/comments/dz5be0/is_tensorflow_suitable_for_my_timeseries_project/,CorerMaximus,1574271278,[removed],0,1
1038,2019-11-21,2019,11,21,2,dz5eka,Project Marduk,https://www.reddit.com/r/MachineLearning/comments/dz5eka/project_marduk/,_noob369,1574271672,I am looking for collaborators. Please follow this link: [https://projectmarduk.net/2019/11/20/project-marduk/](https://projectmarduk.net/2019/11/20/project-marduk/),0,1
1039,2019-11-21,2019,11,21,3,dz6c1x,GPT2-Double-Heads-Model and SWAG Task (NLP),https://www.reddit.com/r/MachineLearning/comments/dz6c1x/gpt2doubleheadsmodel_and_swag_task_nlp/,h56cho,1574275580,[removed],0,1
1040,2019-11-21,2019,11,21,4,dz6lwn,IRLS,https://www.reddit.com/r/MachineLearning/comments/dz6lwn/irls/,Enceladus92,1574276709,"Hello,

I am trying to implement IRLS ( Iteratively reweighted least square algorithm). I keep getting non-invertible matrix error when I try to update the weight. Any idea why? and how to fix that?  


Thanks",0,1
1041,2019-11-21,2019,11,21,4,dz6urq,"[Discussion] Advice needed: Feeling trapped by lack of management/strategy, no implemented models.",https://www.reddit.com/r/MachineLearning/comments/dz6urq/discussion_advice_needed_feeling_trapped_by_lack/,low_life_walrus,1574277711,"(also posted to r/datascience but I realized this community is almost 10x bigger)

Hey all,

I'm a data scientist and looking to reddit for some advice... I've been in this role for about two years and have been the *only* data scientist that entire time - this was also my first data scientist role. Since entering into the new 'data' team about a year ago, we've been continuously plagued with issues like:

* strange and distracting projects by our boss (who has no analytics experience, but a long career of software developer management) - and a lack of him be able to understand scope and true effort required for these random projects
* lack of interest in hearing from Sr DAs and me on what our ideal working environments would be (warehouse design, what we can put on our VMs, etc)
* lack of him working with leadership to build an actual understanding of business needs
* exerting random/arbitrary control over how things get done (I've never seen him do this and it end up being a *benefit* to a project)

I've stayed in the role mainly because it was my foot in the door to this industry (which I am very grateful for), and at the beginning (and to this day, really) the amount of possibilities here are huge and exciting... if they could ever be executed properly as a team. And, to add difficulty to it even more, my boss is an overall great guy - I just don't think he has the mental horsepower for such a huge change this late into career.

My main predicament is that I've been tasked with building out a customer engagement 'engine' of sorts - so attempting to predict individually customers likely to leave, and also understand customer cohorts that are more engaged. I'm approaching it similar to a customer churn model, but with a few differences. This has been hyped for a year- the board of directors is aware of it, and so is everyone in leadership. To say the least: the hype around the team he was supposed to build is huge. That pendulum is beginning to swing back in.

The problem is that because of my manager's disjointed priorities, we have had no progress in building out a warehouse or pipeline that helps a data scientist/me in any way. So, I'm spending time crafting ETL around extremely messy and unreliable system data which has cost me a few months just to implement that - and it isn't done, of course. He has made very little progress in figuring out that his mental model of what a data scientist does is mostly *not true* and that the slowness of this project is a result of the past year's lack of a decent strategy.

And just for a quick, very cringe, example: a few **weeks** ago he was sweet-talked by a Harvard MBA type into trialing yet-another-vendor's autoML solution, thinking that the reason why my project has taken months to get off of the ground was simply because I was having trouble building decent models (I haven't been able to train a model, here, for months because the data didn't exist in any usable way!). And this is \*not\* because I've been quiet about the real challenges - he simply does not listen to other points of view unless it's coming down to him from his boss.

But - to be fair - I made several mistakes when joining his team- my #1 one being that I doubted my intuition around the team's strategy. If someone has 10+ years experience managing software teams, he's this confident, and I'm this new to the role, then I need to stop challenging the status quo he's putting together.

Recently I learned that - finally - the screws are coming down on him from his boss and he's been told to do several of the things I suggested months ago (petty for me to mention, but it feels good and is validating). But rather than him reaching out to me for advice on changing strategy, or what we can do to accelerate the project, it's more of the same. It's also too late at this point for me to give suggestions that our small team could do before the end of the year.

**Skip to here if you don't wanna read:**

Previously, I've said to myself that I will stay at this company until I can put into production ***at least one model***. The question I'm coming to is, what do I do when the timeline for that keeps extending indefinitely and you've lost faith in management to be where you need them to be? What do I do in my job hunt when they see I've been in a DS role for \~2 years and never got to implement a model into production? If I was a hiring manager, I'd assume to some extent that this person wasn't in a *real* data scientist role and would doubt my skills/abilities. Of course I could just lie in an interview- but that feels extremely gross.

My solution so far is to do a few ambitious personal projects that flex on modeling, python ability, and creativity. But we all know that (at least traditionally), your professional experience is the most important factor.

So, if anyone has words of encouragement, discouragement, suggestions, whatever- I'd love to hear it. I doubt my situation is truly unique - and I also know things could be much worse. I am thankful to have gotten my foot in the door, which can be quite hard.

Thanks for reading",18,1
1042,2019-11-21,2019,11,21,5,dz7zp9,"Speaking Machine, Art, And Design With John Maeda",https://www.reddit.com/r/MachineLearning/comments/dz7zp9/speaking_machine_art_and_design_with_john_maeda/,hoopism,1574282643,,0,1
1043,2019-11-21,2019,11,21,7,dz9kti,Any roadmap/state-of-art about AGI,https://www.reddit.com/r/MachineLearning/comments/dz9kti/any_roadmapstateofart_about_agi/,Krokodeale,1574289723,[removed],0,1
1044,2019-11-21,2019,11,21,8,dz9u6f,[D] An idea that came to me randomly,https://www.reddit.com/r/MachineLearning/comments/dz9u6f/d_an_idea_that_came_to_me_randomly/,CheckMiner,1574290871,"I like machine learning. I'm interested in it. I don't exactly understand how it works but I get the basic gist. 

I was thinking... What if there was a website/social experiment thing where there was a machine learning bot and anyone could submit the training material? It would have a theme, maybe, and it would generate a result after a while. 

Just an idea. I'm not smart enough to make it lol",9,1
1045,2019-11-21,2019,11,21,8,dza27z,Is there one input neuron for each feature?,https://www.reddit.com/r/MachineLearning/comments/dza27z/is_there_one_input_neuron_for_each_feature/,rainboworigamipaper,1574291820,[removed],0,1
1046,2019-11-21,2019,11,21,8,dzakrs,"[R] [1911.08265] Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",https://www.reddit.com/r/MachineLearning/comments/dzakrs/r_191108265_mastering_atari_go_chess_and_shogi_by/,ankeshanand,1574294040,,94,1
1047,2019-11-21,2019,11,21,9,dzaqbe,[R] Announcing the IKEA Furniture Assembly Environment for Long-Horizon Complex Manipulation Tasks,https://www.reddit.com/r/MachineLearning/comments/dzaqbe/r_announcing_the_ikea_furniture_assembly/,edwardthegreat2,1574294790,"Hi r/machinelearning, I'm excited to announce the release of our new environment on robotic furniture assembly.  We have over 80 furniture models from IKEA as well as support for Baxter and Sawyer robots (and a Cursor agent for those who don't want to learn low-level control). We support a Gym interface and build on top of MuJoCo for ease of use with RL. 

I'm excited to see what researchers will come up with to tackle furniture assembly, a complex and long-horizon task.  Let me know if you have any questions or comments!

Website: www.clvrai.com/furniture
Github: www.github.com/clvrai/furniture

Paper: https://arxiv.org/abs/1911.07246",3,1
1048,2019-11-21,2019,11,21,9,dzb644,[P] I created an unofficial Google Colab notebook sharing site for researchers to showcase their work.,https://www.reddit.com/r/MachineLearning/comments/dzb644/p_i_created_an_unofficial_google_colab_notebook/,OppositeMidnight,1574296849,"[https://www.google-colab.com](https://www.google-colab.com/)

I started by cataloguing a few interesting notebooks [in a github repo](https://github.com/firmai/google-colab-notebooks) and want to make it more easy for others to share their work and receive comments.

If you are interested, the posts also push to [twitter](https://twitter.com/colabnotebooks), [linkedin](https://www.linkedin.com/company/google-colab-notebooks/), [reddit](https://www.reddit.com/r/googlecolabnotebooks)  and [facebook](https://www.facebook.com/ColabNotebooks) 

So why Google Colab? -- Its is fairly easy to use with a click-and-run format and helps those with limited resources. And even though it has some issues, I am sure we are only at the start of a long-term project and that Colab would keep on improving.",10,1
1049,2019-11-21,2019,11,21,10,dzbj60,[Q] What are 'Polynomial feature expansions',https://www.reddit.com/r/MachineLearning/comments/dzbj60/q_what_are_polynomial_feature_expansions/,Knell123,1574298550,"I am explaining what polynomial regression is and have been asked to make sure i cover certain points, one of which is 'Polynomial feature expansions' what does this mean? Ive been searching online for hours and cant seem to understand exactly I am being asked",0,1
1050,2019-11-21,2019,11,21,10,dzbjz4,Building PyTorch on ROCm,https://www.reddit.com/r/MachineLearning/comments/dzbjz4/building_pytorch_on_rocm/,seraschka,1574298661,,0,1
1051,2019-11-21,2019,11,21,11,dzcdmy,[D] Is there a way to package games for Steam that could use GPU accelerated TensorFlow?,https://www.reddit.com/r/MachineLearning/comments/dzcdmy/d_is_there_a_way_to_package_games_for_steam_that/,starskublue,1574302639,I imagine getting the cuda/cudnn issues right would be difficult. Not sure if there's anything out there already.,3,1
1052,2019-11-21,2019,11,21,11,dzcnao,[R] MarioNETte: Few-shot Face Reenactment Preserving Identity of Unseen Targets,https://www.reddit.com/r/MachineLearning/comments/dzcnao/r_marionette_fewshot_face_reenactment_preserving/,shurain,1574303930,[removed],0,1
1053,2019-11-21,2019,11,21,11,dzcsm9,[R] MarioNETte: Few-shot Face Reenactment Preserving Identity of Unseen Targets,https://www.reddit.com/r/MachineLearning/comments/dzcsm9/r_marionette_fewshot_face_reenactment_preserving/,shurain,1574304665,[removed],0,1
1054,2019-11-21,2019,11,21,12,dzd242,[R] MarioNETte: Few-shot Face Reenactment Preserving Identity of Unseen Targets,https://www.reddit.com/r/MachineLearning/comments/dzd242/r_marionette_fewshot_face_reenactment_preserving/,shurain,1574305914,"* Project page: [https://hyperconnect.github.io/MarioNETte/](https://hyperconnect.github.io/MarioNETte/)
* Paper: [https://arxiv.org/abs/1911.08139](https://arxiv.org/abs/1911.08139)
* Video: [https://youtu.be/Y6HE1DtdJHg](https://youtu.be/Y6HE1DtdJHg)

Here's a summary of the work. We can perform face reenactments under a few-shot or even a one-shot setting, where only a single target face image is provided. Previous approaches to face reenactments had a hard time preserving the identity of the target and tried to avoid the problem through fine-tuning or choosing a driver that does not diverge too much from the target. We tried to tackle this ""identity preservation problem"" through several novel components.

Instead of working with a spatial-agnostic representation of a driver or a target, we encode the style information to a spatial-information preserving representation. This allows us to maintain the details that easily get lost when utilizing spatial-agnostic representation such as those attained from AdaIN layers.

We proposed image attention blocks and feature alignment modules to attend to a specific location and warp feature-level information as well. Combining attention and flow allows us to naturally deal with multiple target images, making the proposed model to gracefully handle one-shot and few-shot settings without resorting to reductions such as sum/max/average pooling.

Another part of the contribution is the landmark transformer, where we alleviate the identity preservation problem even further. When the drivers landmark differs a lot from that of the target, the reenacted face tends to resemble the drivers facial characteristics. Landmark transformer disentangles the identity and expression and can be trained in an unsupervised fashion.

&amp;#x200B;

Check out the video and tell us what you think. Thanks!",12,1
1055,2019-11-21,2019,11,21,12,dzd2qx,[D] Spacy word vectors of dissimilar words match exactly,https://www.reddit.com/r/MachineLearning/comments/dzd2qx/d_spacy_word_vectors_of_dissimilar_words_match/,nivter,1574305985,"I found that the word vectors loaded from `en_core_web_md` in `spacy` have some odd behavior. For instance, the words `flamingo`, `penguin` and `igloo` have precisely the same vectors (with nonzero values). I can't find much on Google. Has anyone encountered this before?",1,1
1056,2019-11-21,2019,11,21,12,dzd3gv,Weighted TF/IDF with Singular Value Decomposition,https://www.reddit.com/r/MachineLearning/comments/dzd3gv/weighted_tfidf_with_singular_value_decomposition/,eerilyweird,1574306089,[removed],0,1
1057,2019-11-21,2019,11,21,12,dzdkep,MIT develops AI to predict whether a patient will get breast cancer within 5 years,https://www.reddit.com/r/MachineLearning/comments/dzdkep/mit_develops_ai_to_predict_whether_a_patient_will/,TobySomething,1574308467,,0,1
1058,2019-11-21,2019,11,21,14,dzesc5,[D] Help! How much does your data change in serious ML projects?,https://www.reddit.com/r/MachineLearning/comments/dzesc5/d_help_how_much_does_your_data_change_in_serious/,Train_Smart,1574315146,"Too bad I cant create polls on Reddit...

I was talking with a data scientist friend about versioning data in ML projects (I know there are a lot of great solutions and this post is not meant to focus on any of them). 

What he said really flipped the notion I had in my head that data is an integral part of data science source code.

He claimed that in most data science projects the data and artifacts (intermediate stages of data processing not including models) dont change that much. This is to say, the source data might be changed, but it is just one file (So you can get away with not versioning it) and intermediate stages should always be determined by code so you just need to manage the code you used to create the stage and not the result (the only exception is if you have some painful or resource intensive processing where you wouldnt want to repeat that process).

I was wondering, from people here with experience in real world projects, how versatile is your data?
Do you feel its hard to manage the data and artifacts?

Im confused and your input would be greatly appreciated.",9,1
1059,2019-11-21,2019,11,21,15,dzeznp,[R] Scalable graph machine learning: a mountain we can climb?,https://www.reddit.com/r/MachineLearning/comments/dzeznp/r_scalable_graph_machine_learning_a_mountain_we/,StellarGraphLibrary,1574316348,,0,1
1060,2019-11-21,2019,11,21,15,dzf9vw,Benefits of Custom Application Development,https://www.reddit.com/r/MachineLearning/comments/dzf9vw/benefits_of_custom_application_development/,venkatvajradhar,1574318108,"&amp;#x200B;

Adaptive app development can be scary because you never know what you can get for it and how much it costs you. Furthermore, the growing adoption of Artificial Intelligence (AI), Machine Learning (ML) and Block chain adds to the complexity of business processes, and you need to customize the software to meet specific business needs.

&amp;#x200B;

![img](j7tjdfjfizz31)

After analyzing the trending developments in software, companies continue to study where changes often occur. Successful companies engage in technology, evaluate solutions, solve problems, and streamline business processes. Realizing that growing business requires software customization to survive in a competitive landscape.

The top custom application developers at Entrance Consulting demystify some of the key trends that revolutionize software worldwide, and explain how we can leverage the benefits of app optimization to grow business.

**3 Critical Consequences Opening new vistas in custom app development**

**AI-embedded companies are leading the race in niche dominance**

A noteworthy trend is that companies have made significant inroads into business processing with the help of [AI](https://www.usmsystems.com/category/application-development/) and ML and appropriately anchored the brand's visibility. Experts estimate that AI is on track to create a $ 16 trillion stake in global business development.

Americas leading companies are leveraging big data to boost customer relationship ship management (CRM) to a whole new level. At the forefront is Chatbots, an ML application that dramatically reduces customer engagement costs while increasing customer satisfaction levels.

AI ensures continuity of business legacy, improves the scalability of business operations and accelerates service delivery by responding to change.

**Reduce block chain costs and streamline business process reengineering**

According to Statista, the global cost of block chain solutions, currently estimated at $ 2.7 billion, will reach $ 12 billion by 2022.

From the banking and finance sectors to the industrial assembly lines, healthcare and logistics, block chain is gaining acceptance and redefining the rules of engagement in highly competitive sectors.

Specify the block chain and the first thing that strikes you is the uproar of cryptocurrency, especially bitcoin and the digital currency in the financial sector. But the block chain goes above and beyond bitcoin.

By efficiently coding big data in an irreplaceable ledger in a computer network system, block chain provides a cost-effective, red-tape-free alternative to slow-moving central companies like governments, banks and financial institutions.

**Advances in the field of quantum physics are promoting the exponential growth of AI and ML**

The traditional binary computer, zeros and crunching them, limits the computing power and slows down the processing of big data by AI and ML algorithms. Quantum Computing (QC) Processing speeds increase many times when unlocking the enormous potential of subatomic particles.

In the coming era, quantum processing power will enable AI and ML algorithms to feature highly mapped data formats to provide better insights into industrial and business efficiencies. A revolution in adaptive application development promises growth in many sectors of the economy.

**4 adaptive** [**application development**](https://www.usmsystems.com/application-development/) **benefits that drive revolutionary software changes**

**Significant reduction in cyber security threats**

Digital disruption provides significant exposure to cyber threats. The software developer's essential job is to eliminate security threats and reduce the vulnerability of IT infrastructure to malicious attacks.

The combination of Activity Wings increases business efficiency and customer satisfaction

Scaling of business activities creates multiple divisions and intersections within the organization, with one side not knowing (or looking after) what the other is doing. The result is a system that negatively impacts efficiency and productivity.

The main advantage of adaptive application development is the multiple systems and protocols in the same system.

Come together under one platform. Systems integration can help you eliminate unproductive operational patterns. You will retain focus in areas that require your immediate attention.

Integration works for companies and clients. Companies become more efficient at conceptualizing, implementing and delivering products and services, and increasing customer relationships to increase customer satisfaction.

Brand imaging, employee satisfaction, and customer relationships go up

Optimizing software for your specific needs is like a self-fulfilling prophecy - you get positive results in three critical areas.

**Brand Imaging**

Branding sets you apart from your competitors. It is the personality that enhances the product that encourages customers to engage you up close and personal. It is the quality of your service, pride, and loyalty when people associate with your company.

One of the unique benefits of software customization is that you can design your app to beautifully project your brand identity, but you also extend a happy customer experience, which forces users to come back for more.

**Employee satisfaction**

Conduct an employee engagement survey, and you will find that a significant portion of your employees wants to break free from daily routines and find a gateway to more professionalism that fosters purpose-fulfilled roles.

The customization manual provides a gold standard for reducing workload and improving the efficiency of business processes, engaging employees with customers and contributing to business development.

By building a single operations platform, you are creating a place where all employees (even in remote areas) can feel free to communicate and stay open.

**Customer Relationships**

Getting to know a small cross-section of customers in your immediate neighborhood and meeting their needs is not a task. What do you do when big data brings you a hugely growing customer base that spans your reach worldwide? Your challenge is to expand the scaling business and customer base, respond quickly to customer intent, and maintain customer loyalty, track and monitor customer preferences. Feedback via social media. The all-in-one custom software solution manages customer data, oversees marketing campaigns, keeps tabs on the team responsible for product delivery, and ensures that your interface with the user works seamlessly and without minus hassles.

**Closing Point**

Consider adaptive application development as a strategic tool that effortlessly integrates different aspects of your business to improve your operational success, increase your productivity, and increase revenue. The customization of the software gives you a technical boost that catapults the leagues before the competition and differentiates you from the audience you want to engage with.

[**Connect USM**](https://www.usmsystems.com/contact-us/) to know how our Mobile apps help your business and boost your  business much more in short time.",0,1
1061,2019-11-21,2019,11,21,16,dzfmn9,Teachable Machine with Google,https://www.reddit.com/r/MachineLearning/comments/dzfmn9/teachable_machine_with_google/,doctor101,1574320366,,0,1
1062,2019-11-21,2019,11,21,16,dzg1ee,How much data did / do we generate every year and how is it split?,https://www.reddit.com/r/MachineLearning/comments/dzg1ee/how_much_data_did_do_we_generate_every_year_and/,DBAnalysis,1574323057,[removed],0,1
1063,2019-11-21,2019,11,21,17,dzg444,Kaggle Survey 2019,https://www.reddit.com/r/MachineLearning/comments/dzg444/kaggle_survey_2019/,awakening-of-lazarus,1574323573,[removed],0,1
1064,2019-11-21,2019,11,21,17,dzg5rj,Laptop for professional purposes: Machine and Deep Leaning,https://www.reddit.com/r/MachineLearning/comments/dzg5rj/laptop_for_professional_purposes_machine_and_deep/,WolfFinster,1574323884,[removed],0,1
1065,2019-11-21,2019,11,21,18,dzh32g,ML Architecture ideas about automated generation of Expert System Rules,https://www.reddit.com/r/MachineLearning/comments/dzh32g/ml_architecture_ideas_about_automated_generation/,Simokaki,1574330236,[removed],0,1
1066,2019-11-21,2019,11,21,19,dzhdza,[N] The Promise and Limitations of AI,https://www.reddit.com/r/MachineLearning/comments/dzhdza/n_the_promise_and_limitations_of_ai/,goto-con,1574332119,"This is a talk from GOTO Chicago 2019 by Doug Lenat, Award-winning AI pioneer who created the landmark Machine Learning program, AM, in 1976 and CEO of Cycorp. I've dropped the full talk abstract below for a read before diving into the talk:

* [Video](https://youtu.be/v2rK40bNrrY?list=PLEx5khR4g7PLIxNHQ5Ze0Mz6sAXA8vSPE)
* [Slides](https://gotochgo.com/2019/sessions/724/the-promise-and-limitations-of-ai)

Almost everyone who talks about Artificial Intelligence, nowadays, means training multi-level neural nets on big data. Developing and using those patterns is a lot like what our right brain hemispheres do; it enables AI's to react quickly and  very often  adequately. But we human beings also make good use of our left brain hemisphere, which reasons more slowly, logically, and causally.

I will discuss this ""other type of AI""  i.e., left brain AI, which comprises a formal representation language, a ""seed"" knowledge base with hand-engineered default rules of common sense and good domain-specific expert judgement written in that language, and an inference engine capable of producing hundreds-deep chains of deduction, induction, and abduction on that large knowledge base. I will describe the largest such platform, Cyc, and will demo a few commercial applications that were produced just by educating it as one might teach a new human employee.

But it is important to remember that human beings' ""super-power"" is our ability to harness both types of reasoning, and I believe that the most powerful AI solutions in the coming decade will likewise be hybrids of right-brain-like ""thinking fast"" and left-brain-like ""thinking slow"". That is the only path I see by which we will overcome the current dangerous inability of deep-learning AI's to rationalize and explain their decisions, and will make AI's far more trusted and  more importantly  far more trustworthy.

Anyone who understood this abstract and found it interesting should find my actual talk similarly accessible  and hopefully interesting!",4,1
1067,2019-11-21,2019,11,21,19,dzhgyg,[D] Does EfficientNet really help in real projects ?,https://www.reddit.com/r/MachineLearning/comments/dzhgyg/d_does_efficientnet_really_help_in_real_projects/,___mlm___,1574332640,"There are large amount of papers which show that EfficientNet improves some CV  tasks e.g. [EfficientDet: Scalable and Efficient Object Detection](https://arxiv.org/abs/1911.09070).

But does it help much in real projects ? 
Do you guys have any experience with that ?

One more thing - ImageNet or COCO datasets are far away from what we have to deal with in real projects. Usually we have only small amount of images/classes, so improvements for COCO/ImageNet != improvements for real projects.
What do you think ?",14,1
1068,2019-11-21,2019,11,21,19,dzhlbu,[Time-Series] Detecting stock market crashes with topological data analysis,https://www.reddit.com/r/MachineLearning/comments/dzhlbu/timeseries_detecting_stock_market_crashes_with/,T-dog-machine,1574333432,"Your task is to generate constant chaos, what do you do?   Instead of trying to do it by yourself, i would suggest organising a competition, a number betting competition. Give no restriction on the numbers, and everyone must try to guess the average of all guesses. Unless the contestants are organised, this will result in massive chaos.   

This is basically what is happening in financial markets. The competitive betting happening there removes any form of pattern from the time series, because a pattern is an opportunity for making money.   

So success on the markets comes down to finding patterns that other's don't... With topological data analysis you're finding a geometric pattern that helps quantify the shape of noise and find hidden cycles.

The idea is to map a sliding window into a higher dimensional space, and study the topology of the aggregated time series. The whole pipeline is explained in this [medium article](https://towardsdatascience.com/detecting-stock-market-crashes-with-topological-data-analysis-7d5dd98abe42), there is [code](https://github.com/giotto-ai/stock-market-crashes), which uses [Giotto'](https://github.com/giotto-ai/giotto-learn)s time series TDA.",0,1
1069,2019-11-21,2019,11,21,19,dzhn3l,What's the redundant correlation ?,https://www.reddit.com/r/MachineLearning/comments/dzhn3l/whats_the_redundant_correlation/,ayoub-jibouni,1574333746,[removed],0,1
1070,2019-11-21,2019,11,21,20,dzhpqn,Suggestions for buying a GPU server,https://www.reddit.com/r/MachineLearning/comments/dzhpqn/suggestions_for_buying_a_gpu_server/,mikkelbue,1574334177,"Hi all

I am part of an engineering group at University of Exeter. We mostly do CPU demanding tasks on traditional HPC computers, but we want to start using various machine learning techniques as surrogate models for our physical simulations.

We have a grant surplus of around USD 13.000 for computer equipment that we would like to invest in an entry-level GPU server. I have an offer on Exxact TensorEX TS2 with:

*2x Intel Xeon Silver 4214 Processor, 12 Core, 24 Threads, 2.2Ghz
*4x NVIDIA RTX 2080 Ti 11GB
*12x 16GB DDR4 ECC/REG 2666MHZ (192GB RAM)

for around USD 10000, with educational discounts applied.

Can someone give some advice on this? We are very familiar with CPU HPC systems, but none of us are particularly GPU-savvy. ANy sort of feedback or advice would be greatly appreciated!",0,1
1071,2019-11-21,2019,11,21,20,dzhrij,Is the word 'a naturallity' or 'a junk sequence'?,https://www.reddit.com/r/MachineLearning/comments/dzhrij/is_the_word_a_naturallity_or_a_junk_sequence/,whitedruid,1574334464,"Hello!

I think that my question is a very easy for this section, but I dare ask :) I've wrote very simple filter for my phpBB forum and I want to extend that with some features. The idea is checking a nickname of new user via word's filter for detect of incoming data by 'a naturallity'. I analyzed over about a thousands spammers nicknames and I've spotted a pattern, that names of spammers accounts most are a fully random. It was even very simple :) I mean that 'player 167' is a with high precisely will be 'a real user', but 'afuknqekxa' is a spammer with a high probability. Now I'm searching an implementaion of algorithm for checking words by 'a naturallity'. 

Thank you!

Links and articles are welcome.",0,1
1072,2019-11-21,2019,11,21,20,dzi2wg,I want to collect Dataset for AI Composer.,https://www.reddit.com/r/MachineLearning/comments/dzi2wg/i_want_to_collect_dataset_for_ai_composer/,Kricklobderno,1574336373,[removed],0,1
1073,2019-11-21,2019,11,21,20,dzi5tm,AI and Machine learning are Reshaping the Global Banking Industry,https://www.reddit.com/r/MachineLearning/comments/dzi5tm/ai_and_machine_learning_are_reshaping_the_global/,Albertchristopher,1574336871,,0,1
1074,2019-11-21,2019,11,21,20,dzi721,[P] Spectral Normalization implemented of Tensorflow 2,https://www.reddit.com/r/MachineLearning/comments/dzi721/p_spectral_normalization_implemented_of/,thisisiron,1574337086," [https://github.com/thisisiron/spectral\_normalization-tf2](https://github.com/thisisiron/spectral_normalization-tf2)   
If you find anything wrong with my code, please write it in the comments or issues.",0,1
1075,2019-11-21,2019,11,21,21,dziis1,[R] Scalable graph machine learning: a mountain we can climb?,https://www.reddit.com/r/MachineLearning/comments/dziis1/r_scalable_graph_machine_learning_a_mountain_we/,StellarGraphLibrary,1574338850,"Graph machine learning is still a relatively new and developing area of research and brings with it a bucket load of complexities and challenges. One such challenge that both fascinates and infuriates those of us working with graph algorithms is  *scalability*.

I learned first-hand that when trying to apply graph machine learning techniques to identify fraudulent behaviour in the bitcoin blockchain data, scalability was the biggest roadblock. The bitcoin blockchain graph I used has millions of wallets (nodes) and billions of transactions (edges) which makes most graph machine learning methods infeasible.

An algorithm called GraphSAGE (based on the method of neighbour-sampling) offered some solid breakthroughs, but there are still mountains to climb to make scalable graph machine learning more practical. 

[https://medium.com/stellargraph/scalable-graph-machine-learning-a-mountain-we-can-climb-753dccc572f](https://medium.com/stellargraph/scalable-graph-machine-learning-a-mountain-we-can-climb-753dccc572f)",0,1
1076,2019-11-21,2019,11,21,21,dzimhf,"[R] Video Analysis: MuZero - Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",https://www.reddit.com/r/MachineLearning/comments/dzimhf/r_video_analysis_muzero_mastering_atari_go_chess/,ykilcher,1574339389,"[https://youtu.be/We20YSAJZSE](https://youtu.be/We20YSAJZSE)

&amp;#x200B;

MuZero harnesses the power of AlphaZero, but without relying on an accurate environment model. This opens up planning-based reinforcement learning to entirely new domains, where such environment models aren't available. The difference to previous work is that, instead of learning a model predicting future observations, MuZero predicts the future observations' latent representations, and thus learns to only represent things that matter to the task!

&amp;#x200B;

Abstract:

Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.

&amp;#x200B;

Authors: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, David Silver",11,1
1077,2019-11-21,2019,11,21,21,dzip4a,Modern expert system,https://www.reddit.com/r/MachineLearning/comments/dzip4a/modern_expert_system/,tensorfuser,1574339769,[removed],0,1
1078,2019-11-21,2019,11,21,21,dziq2p,What is Machine Learning? Basics of Machine Learning| Machine learning Introduction| GreyCampus,https://www.reddit.com/r/MachineLearning/comments/dziq2p/what_is_machine_learning_basics_of_machine/,Reginald_Martin,1574339911,,0,1
1079,2019-11-21,2019,11,21,22,dzjoko,Which electives/skills to pick up?,https://www.reddit.com/r/MachineLearning/comments/dzjoko/which_electivesskills_to_pick_up/,Plyad1,1574344727,[removed],0,1
1080,2019-11-21,2019,11,21,23,dzjt07,Kernel || Kernal ?,https://www.reddit.com/r/MachineLearning/comments/dzjt07/kernel_kernal/,Starscream9559,1574345302,[removed],0,1
1081,2019-11-21,2019,11,21,23,dzk1cp,"[D] Must read papers on application of NNs to 3D data, most importantly point clouds",https://www.reddit.com/r/MachineLearning/comments/dzk1cp/d_must_read_papers_on_application_of_nns_to_3d/,Unpigged,1574346399,"Could you please list most important or interesting publicataions covering application of neural networks and deep learning to 3D data, especially point clouds? I'm mostly interested in application of NNs to tasks like classification/segmentation of 3D objects but any other references are higly appreciated.

Thanks in advance.",15,1
1082,2019-11-21,2019,11,21,23,dzk1p2,What is statistical power of study,https://www.reddit.com/r/MachineLearning/comments/dzk1p2/what_is_statistical_power_of_study/,psjadhav,1574346441,[removed],0,1
1083,2019-11-21,2019,11,21,23,dzk6rn,Is a Machine Learning Engineer Job in SPORTS any option??,https://www.reddit.com/r/MachineLearning/comments/dzk6rn/is_a_machine_learning_engineer_job_in_sports_any/,krispudzian,1574347101,[removed],0,1
1084,2019-11-21,2019,11,21,23,dzkfoh,Are there any models out there for detecting handwritten text?,https://www.reddit.com/r/MachineLearning/comments/dzkfoh/are_there_any_models_out_there_for_detecting/,craaaft,1574348233,,0,1
1085,2019-11-22,2019,11,22,0,dzl2h0,Would you watch a Machine learning + Deep learning course if I uploaded it on YouTube? I will upload it if I got 250 upvotes,https://www.reddit.com/r/MachineLearning/comments/dzl2h0/would_you_watch_a_machine_learning_deep_learning/,SolidProfessor13,1574350965,[removed],0,1
1086,2019-11-22,2019,11,22,0,dzl8u2,[R] Transfer learning of sentiment analysis model?,https://www.reddit.com/r/MachineLearning/comments/dzl8u2/r_transfer_learning_of_sentiment_analysis_model/,agromenawer,1574351654,[removed],0,1
1087,2019-11-22,2019,11,22,0,dzlbem,imp machine learning skills,https://www.reddit.com/r/MachineLearning/comments/dzlbem/imp_machine_learning_skills/,skj8,1574351931,,0,1
1088,2019-11-22,2019,11,22,1,dzlutp,Noisy sampling from autoregressive models (e.g. PixelCNN++),https://www.reddit.com/r/MachineLearning/comments/dzlutp/noisy_sampling_from_autoregressive_models_eg/,robertdinh,1574354075,[removed],0,1
1089,2019-11-22,2019,11,22,2,dzmml5,[D] Combining non-text features with text classifier,https://www.reddit.com/r/MachineLearning/comments/dzmml5/d_combining_nontext_features_with_text_classifier/,saint----,1574357217,"Hi! So I'm building a classifier which primarily looks at text, but I also want to include other features, which are non-text, and I was wondering what is the best way to do it? I feel like just adding another dimension in the vector which represents the text might cause these features to get 'lost', but maybe that's not true. Is ther there some sort of agreed upon way of including these additional non-text features in? By non-text I mean just information which is not part of the body of the text, like some other meta data.

Thanks!",5,1
1090,2019-11-22,2019,11,22,2,dzmssp,[D] Why does hierarchical Bayesian regression work well on imbalanced data?,https://www.reddit.com/r/MachineLearning/comments/dzmssp/d_why_does_hierarchical_bayesian_regression_work/,paulie007,1574357922,"I have a dataset of electrical outages and it is extremely imbalanced, &lt;2% of all of the data are positive classes. I am using weather station data to try to predict the probability of an outage occurring near the weather stations. 

&amp;#x200B;

When I try any other model I have to rebalance the data to get any good results. However I have recently tried hierarchical Bayesian logistic regression and it performs just fine without resampling. In my methodology every individual weather station has a unique intercept and coefficients, but they are each drawn from a parent distribution. 

&amp;#x200B;

What I would like to discuss is why does the hierarchical approach perform so much better on the imbalanced dataset?",34,1
1091,2019-11-22,2019,11,22,2,dzmz3k,Training CoreML Object Detection model from scratch using CreateML,https://www.reddit.com/r/MachineLearning/comments/dzmz3k/training_coreml_object_detection_model_from/,TomekB,1574358631,,0,1
1092,2019-11-22,2019,11,22,3,dzn72t,Top 5 AutoML Tools Easing Out Machine Learning for Non-Experts,https://www.reddit.com/r/MachineLearning/comments/dzn72t/top_5_automl_tools_easing_out_machine_learning/,analyticsinsight,1574359520,,0,1
1093,2019-11-22,2019,11,22,3,dzn98i,How does the EngageMe MIT machine learning robot work?,https://www.reddit.com/r/MachineLearning/comments/dzn98i/how_does_the_engageme_mit_machine_learning_robot/,i-am-Breesus,1574359741,[removed],1,1
1094,2019-11-22,2019,11,22,3,dznuiw,Predictive analytics based on the historical and current listing price,https://www.reddit.com/r/MachineLearning/comments/dznuiw/predictive_analytics_based_on_the_historical_and/,Benq3927,1574362126,[removed],0,1
1095,2019-11-22,2019,11,22,4,dzo9cn,[D] What APIs/Libraries are available for Offline Handwriting OCR?,https://www.reddit.com/r/MachineLearning/comments/dzo9cn/d_what_apislibraries_are_available_for_offline/,TheM0zart,1574363776,"I want to implement offline handwriting ocr within an application. It should work on photographs of handwritings (=no scanned images). I tried tesseract and Google Cloud Vision. Both seem not to work well with handwritings. 

Are there any handwriting specialized APIs/libraries with high accuracy? I would like to use something finished - the focus of the project should not be on building/training a model. 
Whats the state of the art in that specific area?


Ive searched around a little bit but couldnt find anything suitable. 

Thankful for every hint I get.",1,1
1096,2019-11-22,2019,11,22,4,dzoj34,[Discussion] Confusion around Multi-Step and Multivariate LSTM Time Series Forecasting,https://www.reddit.com/r/MachineLearning/comments/dzoj34/discussion_confusion_around_multistep_and/,boneless_baku,1574364842,"Hi everyone, I'm currently trying to develop an LSTM RNN for predicting train delays. I looked into Time Series Forecasting Models and different approaches but can't seem to figure out which model to use. 

I want to implement multiple features, like delay, train number, date, time and current weekday. The output of the model should be a delay in minutes.

I have trouble understanding the difference between Multistep and Multivariate Time Series Forecasting.

Can somebody please elaborate?",6,1
1097,2019-11-22,2019,11,22,4,dzovrl,[D] Is the Python Statsmodels library production grade?,https://www.reddit.com/r/MachineLearning/comments/dzovrl/d_is_the_python_statsmodels_library_production/,AlexSnakeKing,1574366231,"I've had success with sklearn in production, as well as with TF. 

Is statsmodels something that can run in production? Its documentation doesn't seem up to par but its code base doesn't look bad.",8,1
1098,2019-11-22,2019,11,22,5,dzoxru,Reverse clustering? How to create highly similar clusters?,https://www.reddit.com/r/MachineLearning/comments/dzoxru/reverse_clustering_how_to_create_highly_similar/,Djieffe88,1574366456,[removed],0,1
1099,2019-11-22,2019,11,22,5,dzp34v,Voila! SOTA French Language Model CamemBERT Debuts,https://www.reddit.com/r/MachineLearning/comments/dzp34v/voila_sota_french_language_model_camembert_debuts/,Yuqing7,1574367022,,0,1
1100,2019-11-22,2019,11,22,5,dzp5gt,50 Iconic B&amp;W Videos Colorized,https://www.reddit.com/r/MachineLearning/comments/dzp5gt/50_iconic_bw_videos_colorized/,melodyspell,1574367257,[removed],0,1
1101,2019-11-22,2019,11,22,5,dzphul,Google Brains Nicholas Frosst on Adversarial Examples and Emotional Responses,https://www.reddit.com/r/MachineLearning/comments/dzphul/google_brains_nicholas_frosst_on_adversarial/,Yuqing7,1574368589,,0,1
1102,2019-11-22,2019,11,22,5,dzpjcf,Looking for resources to build an image recognition recommender system in python,https://www.reddit.com/r/MachineLearning/comments/dzpjcf/looking_for_resources_to_build_an_image/,neuroguy6,1574368734,[removed],0,1
1103,2019-11-22,2019,11,22,7,dzqzti,Machine learning for automated testing - does this idea have any merit?,https://www.reddit.com/r/MachineLearning/comments/dzqzti/machine_learning_for_automated_testing_does_this/,pazzamanino,1574374460,[removed],0,1
1104,2019-11-22,2019,11,22,7,dzrgb9,AI to monitor network,https://www.reddit.com/r/MachineLearning/comments/dzrgb9/ai_to_monitor_network/,nvitaly,1574376315,"Hello,

I have monitoring system watching for bandwidth, connections and connections rates from multiple firewalls, which is stream of counters with interval 5 min.

My current system create baseline from data for last 4 weeks and compare current value with baseline.  It is ok but it either give me lots of false alerts or too slow to react without additional triggers. Is there anything better available today ? Some system I can feed data in that will learn patterns and identify outages in real time.

Open source, but that I can use without getting into machine learning theory too deep just to start using it :)

Thank you",0,1
1105,2019-11-22,2019,11,22,8,dzrpph,Music based deep dream generator,https://www.reddit.com/r/MachineLearning/comments/dzrpph/music_based_deep_dream_generator/,tripping-apes,1574377420,[removed],0,1
1106,2019-11-22,2019,11,22,8,dzs00o,[P] OpenAI Safety Gym,https://www.reddit.com/r/MachineLearning/comments/dzs00o/p_openai_safety_gym/,hardmaru,1574378611,"From the [project page](https://openai.com/blog/safety-gym/):

**Safety Gym**

*Were releasing Safety Gym, a suite of environments and tools for measuring progress towards reinforcement learning agents that respect safety constraints while training. We also provide a standardized method of comparing algorithms and how well they avoid costly mistakes while learning. If deep reinforcement learning is applied to the real world, whether in robotics or internet-based tasks, it will be important to have algorithms that are safe even while learninglike a self-driving car that can learn to avoid accidents without actually having to experience them.*

https://openai.com/blog/safety-gym/",12,1
1107,2019-11-22,2019,11,22,9,dzsssi,[D] Voice Assistant: Better to use a model trained on commands or just use STT?,https://www.reddit.com/r/MachineLearning/comments/dzsssi/d_voice_assistant_better_to_use_a_model_trained/,elmosworld37,1574382178,"I would like to make a deep-learning based voice assistant for an application I have that controls a digital camera. Some example commands are ""auto focus"", ""set zoom to 2"", ""turn off flash"", etc.

I see two ways of going about this:

1. Train a model that classifies an audio snippet as containing one of the commands or background noise. This seems easier than option 2 but also less robust, as I would have to retrain the model every time I add a new command. Also not sure how numbers would work (record myself saying every number up to like 100?).

2. Use STT to convert audio to text and do some fuzzy string matching to see if it matches a command. I've downloaded Mozilla's DeepSpeech and it did not seem to work very well, so I'm guessing that creating a good STT model is very difficult.

Which of these is a better approach? Or is there some in-between approach that's even better?",21,1
1108,2019-11-22,2019,11,22,9,dzszd9,"[N] ""How AI is changing the world"" - links to the 15 minute talks from the global event series I co-organized",https://www.reddit.com/r/MachineLearning/comments/dzszd9/n_how_ai_is_changing_the_world_links_to_the_15/,ubershmekel,1574383022,"We ran an event in NYC, Tel Aviv, and SF, where we asked AI companies to speak about what they do for 10-15 minutes. Seems I can't link all of them because of some reddit limitation, so here are a few links but this is the [YouTube playlist with all the talks](https://www.youtube.com/watch?v=96c557R3uS4&amp;list=PLfIoNhpjFSoRjAd7V-5rGsnLrDFIoAngK).

* [Jason Yosinski from Uber AI Labs on analyzing parameter updates in training](https://www.youtube.com/watch?v=737PKW1Rt_g&amp;list=PLfIoNhpjFSoRjAd7V-5rGsnLrDFIoAngK&amp;index=8)
* Martin Aguinis - an overview of AI at Google
* K Health - personalized health
* [Hopper - predicting when to buy flight tickets](https://www.youtube.com/watch?v=nNEmwbo0-jg&amp;list=PLfIoNhpjFSoRjAd7V-5rGsnLrDFIoAngK&amp;index=13)
* [Giphy - contextually, culturally, relevant gifs](https://www.youtube.com/watch?v=bmR2tH5oFIU&amp;list=PLfIoNhpjFSoRjAd7V-5rGsnLrDFIoAngK&amp;index=12)
* Nexar - Auto safety
* NEC - Computer vision for skin cancer
* Taranis - Drones scanning farms for pests
* RigD - Crisis Management
* D-ID - Anti facial recognition tech
* [Pavan Kumar, CTO of Cocoon Health](https://www.youtube.com/watch?v=AMqyzdB2Kzg&amp;list=PLfIoNhpjFSoRjAd7V-5rGsnLrDFIoAngK&amp;index=4)",0,1
1109,2019-11-22,2019,11,22,11,dzu4g0,Deep Learning with PyTorch book is now available for free,https://www.reddit.com/r/MachineLearning/comments/dzu4g0/deep_learning_with_pytorch_book_is_now_available/,ConfidentMushroom,1574388317,,0,1
1110,2019-11-22,2019,11,22,11,dzulx1,Hey guys I need some training data for a pokemon related machine learning project - can the community help me out!,https://www.reddit.com/r/MachineLearning/comments/dzulx1/hey_guys_i_need_some_training_data_for_a_pokemon/,SawhneyPhotograhy,1574390643,,0,1
1111,2019-11-22,2019,11,22,12,dzux1y,How to Reduce Overfitting,https://www.reddit.com/r/MachineLearning/comments/dzux1y/how_to_reduce_overfitting/,ml-ai,1574392110,,0,1
1112,2019-11-22,2019,11,22,12,dzuz7a,Sampling issue in PixelCNN++ with per-pixel distribution,https://www.reddit.com/r/MachineLearning/comments/dzuz7a/sampling_issue_in_pixelcnn_with_perpixel/,robertdinh,1574392395,[removed],0,1
1113,2019-11-22,2019,11,22,13,dzvno5,"Equipped with Four interchangeable blades the Mandoline Slicer lets you grate, slice and peel anything!",https://www.reddit.com/r/MachineLearning/comments/dzvno5/equipped_with_four_interchangeable_blades_the/,sortedfactoryy,1574395825,,0,1
1114,2019-11-22,2019,11,22,13,dzvvg7,"We hear of models trained on GPU, but is it possible for models to run on GPU? If yes, how popular is this?",https://www.reddit.com/r/MachineLearning/comments/dzvvg7/we_hear_of_models_trained_on_gpu_but_is_it/,aszora,1574396938,,0,1
1115,2019-11-22,2019,11,22,14,dzwaud,Question about a paper: Deep Online Learning: Learning Deep Neural Networks on the Fly (IJCAI 2018),https://www.reddit.com/r/MachineLearning/comments/dzwaud/question_about_a_paper_deep_online_learning/,skwaaaaat,1574399215,[removed],0,1
1116,2019-11-22,2019,11,22,14,dzwe4t,Bot Marketplace - 3,https://www.reddit.com/r/MachineLearning/comments/dzwe4t/bot_marketplace_3/,getengati,1574399718,[removed],0,1
1117,2019-11-22,2019,11,22,14,dzwm9r,[R] EfficientDet: Scalable and Efficient Object Detection,https://www.reddit.com/r/MachineLearning/comments/dzwm9r/r_efficientdet_scalable_and_efficient_object/,hardmaru,1574401048,,14,1
1118,2019-11-22,2019,11,22,14,dzwobb,Better Banking with help of Analytics and Machine learning,https://www.reddit.com/r/MachineLearning/comments/dzwobb/better_banking_with_help_of_analytics_and_machine/,andrea_manero,1574401391,[removed],0,1
1119,2019-11-22,2019,11,22,14,dzwrk1,[D] AI to monitor network,https://www.reddit.com/r/MachineLearning/comments/dzwrk1/d_ai_to_monitor_network/,nvitaly,1574401932,"Hello,

&amp;#x200B;

I have monitoring system watching for bandwidth, connections and connections rates from multiple firewalls, which is stream of counters with interval 5 min.

&amp;#x200B;

My current system create baseline from data for last 4 weeks and compare current value with baseline. It is ok but it either give me lots of false alerts or too slow to react without additional triggers. Is there anything better available today ? Some system I can feed data in that will learn patterns and identify outages in real time.

&amp;#x200B;

Open source, but that I can use without getting into machine learning theory too deep just to start using it :)

&amp;#x200B;

Thank you",14,1
1120,2019-11-22,2019,11,22,15,dzx5ko,How multi-head attentions were combined in BERT?,https://www.reddit.com/r/MachineLearning/comments/dzx5ko/how_multihead_attentions_were_combined_in_bert/,MrAaronW,1574404321,[removed],0,1
1121,2019-11-22,2019,11,22,16,dzxl1h,Research topics for ML Beginner,https://www.reddit.com/r/MachineLearning/comments/dzxl1h/research_topics_for_ml_beginner/,manoj_sadashiv,1574407090,[removed],0,1
1122,2019-11-22,2019,11,22,16,dzxrvl,Buy and Sell Used CNC Machines - Hi-Tech Machinery Inc,https://www.reddit.com/r/MachineLearning/comments/dzxrvl/buy_and_sell_used_cnc_machines_hitech_machinery/,DanielsCruz,1574408397,,0,1
1123,2019-11-22,2019,11,22,16,dzxvj1,What are the benefits of buying a Used CNC Machine?,https://www.reddit.com/r/MachineLearning/comments/dzxvj1/what_are_the_benefits_of_buying_a_used_cnc_machine/,DanielsCruz,1574409093,,0,1
1124,2019-11-22,2019,11,22,16,dzxy0c,Complex model suggestions,https://www.reddit.com/r/MachineLearning/comments/dzxy0c/complex_model_suggestions/,somethedaring,1574409557,[removed],0,1
1125,2019-11-22,2019,11,22,17,dzxyu5,"Buy and Sell Used CNC, Fabrication, Manual Machines - Hi-Tech Machinery Inc",https://www.reddit.com/r/MachineLearning/comments/dzxyu5/buy_and_sell_used_cnc_fabrication_manual_machines/,DanielsCruz,1574409716,,0,1
1126,2019-11-22,2019,11,22,17,dzybkz,Good books on machine learning/data science impact?,https://www.reddit.com/r/MachineLearning/comments/dzybkz/good_books_on_machine_learningdata_science_impact/,lambdaofgod,1574412206,[removed],0,1
1127,2019-11-22,2019,11,22,18,dzykw6,Need help for data preprocessing.,https://www.reddit.com/r/MachineLearning/comments/dzykw6/need_help_for_data_preprocessing/,mayank17srivastava,1574414071,[removed],0,1
1128,2019-11-22,2019,11,22,18,dzyqpg,ID card digitization using OCR and Graph Convolutional Networks,https://www.reddit.com/r/MachineLearning/comments/dzyqpg/id_card_digitization_using_ocr_and_graph/,manneshiva,1574415261,,1,1
1129,2019-11-22,2019,11,22,18,dzyuel,[D] Are there any available models for detecting handwritten text in images?,https://www.reddit.com/r/MachineLearning/comments/dzyuel/d_are_there_any_available_models_for_detecting/,craaaft,1574416011,[removed],0,1
1130,2019-11-22,2019,11,22,18,dzyvgs,Establishing a Data Literacy Program!,https://www.reddit.com/r/MachineLearning/comments/dzyvgs/establishing_a_data_literacy_program/,ElegantMicroWebIndia,1574416226,,0,1
1131,2019-11-22,2019,11,22,19,dzz7pv,Anyone focus on the system reference algorithm applied in e-commerce? what is mature model and any clues?,https://www.reddit.com/r/MachineLearning/comments/dzz7pv/anyone_focus_on_the_system_reference_algorithm/,MedicElegant,1574418557,[removed],0,1
1132,2019-11-22,2019,11,22,20,dzzjzb,[D] Uncertainty estimation in DL,https://www.reddit.com/r/MachineLearning/comments/dzzjzb/d_uncertainty_estimation_in_dl/,Maplernothaxor,1574420943,Any interesting papers pushing the boundaries of creating well calibrated uncertainties with neural networks (with minimal computational expense ideally)?,5,1
1133,2019-11-22,2019,11,22,20,dzzkif,AI Experts: The Next Frontier in AI After the 2020 Job Crisis,https://www.reddit.com/r/MachineLearning/comments/dzzkif/ai_experts_the_next_frontier_in_ai_after_the_2020/,Albertchristopher,1574421043,[removed],0,1
1134,2019-11-22,2019,11,22,20,dzzpge,"Using CereWave AI technology, CereProc creates a singing satrical clip featuring the text-to-speech voices of Boris Johnson and Donald Trump.",https://www.reddit.com/r/MachineLearning/comments/dzzpge/using_cerewave_ai_technology_cereproc_creates_a/,CereProc,1574421936,"Using our new system, CereWave AI, which utilises machine learning, CereProc created a satirical skit using the synthesised voices of Donald Trump and Boris Johnson. See the following links for the video, and for our website.

Feel free to ask any questions in the comments.

**Video:** [**https://www.youtube.com/watch?v=\_1M3d8nxKxw**](https://www.youtube.com/watch?v=_1M3d8nxKxw)

**Website:** [**https://www.cereproc.com/**](https://www.cereproc.com/)",0,1
1135,2019-11-22,2019,11,22,20,dzzuqr,Nozzle Suppliers &amp; Spare Parts,https://www.reddit.com/r/MachineLearning/comments/dzzuqr/nozzle_suppliers_spare_parts/,smthelp1,1574422900,,0,1
1136,2019-11-22,2019,11,22,20,dzzw5c,[D] An Open Source Stack for Managing and Deploying ML Models - DVC &amp; Cortex - Tutorial,https://www.reddit.com/r/MachineLearning/comments/dzzw5c/d_an_open_source_stack_for_managing_and_deploying/,thumbsdrivesmecrazy,1574423164,"https://towardsdatascience.com/an-open-source-stack-for-managing-and-deploying-models-c5d3b98160bc

&gt; In this tutorial, were going to use DVC to create a model capable of analyzing StackOverflow posts, and recognizing which ones are about Python. We are then going to deploy our model as a web API, ready to form the backend of a piece of production software.

&gt; DVC stores your model weights and training data in a centralized location, allowing collaborators to get started easily, while also tracking changes and ensuring an accurate version history.

&gt; As a final step in this tutorial, were going to integrate DVC with another open source toolCortexthat allows us to deploy DVC-generated models as web APIs, ready for production.",1,1
1137,2019-11-22,2019,11,22,21,e00lga,WindHive: ML based coding assistant to boost coding productivity,https://www.reddit.com/r/MachineLearning/comments/e00lga/windhive_ml_based_coding_assistant_to_boost/,crystal_alpine,1574427280,"Hi r/MachineLearning,  we were tired of constantly having to search Google, StackOverflow, and GitHub for code examples and API documentation when writing code. We built [WindHive.ai](https://WindHive.ai), a smart coding assistant that provides you contextually relevant code snippets and docs directly from your editor!

WindHive does this by using machine-learned code representations/embeddings. We have trained neural networks on hundreds of publicly available code repositories to create embeddings for the task of deciding which snippets of code would be most useful to show you at any given time. Using these embeddings, we are able to index and search through tens of thousands of code snippets and show you exactly what you need.

We believe WindHive can help you increase your programming productivity and avoid reinventing the wheel! Please to to [windhive.ai](https://windhive.ai/) to find out more and share your feedback with us!",0,1
1138,2019-11-22,2019,11,22,22,e00qdj,[P] WindHive: ML based coding assistant to boost coding productivity,https://www.reddit.com/r/MachineLearning/comments/e00qdj/p_windhive_ml_based_coding_assistant_to_boost/,crystal_alpine,1574428023,"Hi [r/MachineLearning](https://www.reddit.com/r/MachineLearning/), we were tired of constantly having to search Google, StackOverflow, and GitHub for code examples and API documentation when writing code. We built [WindHive.ai](https://windhive.ai/), a smart coding assistant that provides you contextually relevant code snippets and docs directly from your editor!

WindHive does this by using machine-learned code representations/embeddings. We have trained neural networks on hundreds of publicly available code repositories to create embeddings for the task of deciding which snippets of code would be most useful to show you at any given time. Using these embeddings, we are able to index and search through tens of thousands of code snippets and show you exactly what you need.

We believe WindHive can help you increase your programming productivity and avoid reinventing the wheel! Please to to [windhive.ai](https://windhive.ai/) to find out more and share your feedback with us!",16,1
1139,2019-11-22,2019,11,22,23,e01lr0,Which optimisation technique should be used ?,https://www.reddit.com/r/MachineLearning/comments/e01lr0/which_optimisation_technique_should_be_used/,tapannitk,1574432477,[removed],0,1
1140,2019-11-22,2019,11,22,23,e01t22,"New ML student. Can anyone assist me with figuring out this SVM Separable example, such as the calculations?",https://www.reddit.com/r/MachineLearning/comments/e01t22/new_ml_student_can_anyone_assist_me_with_figuring/,VTSAX_Over_Kids,1574433445,[removed],0,1
1141,2019-11-22,2019,11,22,23,e01uga,Help ,https://www.reddit.com/r/MachineLearning/comments/e01uga/help/,OstinOstwald,1574433631,[removed],0,1
1142,2019-11-22,2019,11,22,23,e020vo,[N] How to convert a NN model from TensorFlow Lite to CoreML,https://www.reddit.com/r/MachineLearning/comments/e020vo/n_how_to_convert_a_nn_model_from_tensorflow_lite/,xmartlabs,1574434453,"Hey everyone,

We recently put together this blogpost, to show you how to convert a NN model from TensorFlow Lite to CoreML

Hope folks find something helpful here!

[https://blog.xmartlabs.com/2019/11/22/TFlite-to-CoreML/](https://blog.xmartlabs.com/2019/11/22/TFlite-to-CoreML/)",1,1
1143,2019-11-23,2019,11,23,0,e02grw,[R] Deep neuroethology of a virtual rodent,https://www.reddit.com/r/MachineLearning/comments/e02grw/r_deep_neuroethology_of_a_virtual_rodent/,hardmaru,1574436449,,5,1
1144,2019-11-23,2019,11,23,0,e02lq4,[D] What are some interesting questions we can answer using AI?,https://www.reddit.com/r/MachineLearning/comments/e02lq4/d_what_are_some_interesting_questions_we_can/,mrsailor23,1574437042," Hi everyone,

I have created a simple concept: Train an AI to answer everyday questions and upload the simulations online

I already created the first project/simulation a couple of months ago, but Im now looking for the next questions we could answer using AI. I have a few ideas like:

*A.I. Learns: Is it better to walk or run in the rain?*

*A.I. Learns: How to make a profit from cryptocurrency*

*etc..*

Topics could be serious or lighter and funny sometimes :)

So, what are some interesting questions we can answer using AI?

Ill pick up interesting answers for my next project.

Thank you.",1,1
1145,2019-11-23,2019,11,23,0,e02p5w, Strengthening the AI community,https://www.reddit.com/r/MachineLearning/comments/e02p5w/strengthening_the_ai_community/,sjoerdapp,1574437464,,0,1
1146,2019-11-23,2019,11,23,1,e03azf,[N] China forced the organizers of the International Conference on Computer Vision (ICCV) in South Korea to change Taiwans status from a nation to a region in a set of slides.,https://www.reddit.com/r/MachineLearning/comments/e03azf/n_china_forced_the_organizers_of_the/,Only_Assist,1574440094,"Link: [http://www.taipeitimes.com/News/front/archives/2019/11/02/2003725093](http://www.taipeitimes.com/News/front/archives/2019/11/02/2003725093)

&gt;The Ministry of Foreign Affairs yesterday protested after China forced the organizers of the International Conference on Computer Vision (ICCV) in South Korea to change Taiwans status from a nation to a region in a set of slides.  
&gt;  
&gt;At the opening of the conference, which took place at the COEX Convention and Exhibition Center in Seoul from Tuesday to yesterday, the organizers released a set of introductory slides containing graphics showing the numbers of publications or attendees per nation, including Taiwan.  
&gt;  
&gt;However, the titles on the slides were later changed to per country/region, because of a complaint filed by a Chinese participant.  
&gt;  
&gt;Taiwan is wrongly listed as a country. I think this may be because the person making this chart is not familiar with the history of Taiwan, the Chinese participant wrote in a letter titled A mistake at the opening ceremony of ICCV 2019, which was published on Chinese social media under the name Cen Feng (), who is a cofounder of leiphone.com.  
&gt;  
&gt;The ministry yesterday said that Chinas behavior was contemptible and it would not change the fact that Taiwan does not belong to China.  
&gt;  
&gt;Beijing using political pressure to intervene in an academic event shows its dictatorial nature and that to China, politics outweigh everything else, ministry spokeswoman Joanne Ou () said in a statement.  
&gt;  
&gt;The ministry has instructed its New York office to express its concern to the headquarters of the Institute of Electrical and Electronics Engineers, which cosponsored the conference, asking it not to cave in to Chinese pressure and improperly list Taiwan as part of Chinas territory, she said.  
&gt;  
&gt;Beijing has to forcefully tout its one China principle in the global community because it is already generally accepted that Taiwan is not part of China, she added.  
&gt;  
&gt;As China attempts to force other nations to accept its one China principle and sabotage academic freedom, Taiwan hopes that nations that share its freedoms and democratic values can work together to curb Beijings aggression, she added.",237,1
1147,2019-11-23,2019,11,23,1,e03cce,Strengthening the AI Community,https://www.reddit.com/r/MachineLearning/comments/e03cce/strengthening_the_ai_community/,ilikepancakez,1574440265,,0,1
1148,2019-11-23,2019,11,23,1,e03k99,AI Is Tearing Up the Dancing Floor Again,https://www.reddit.com/r/MachineLearning/comments/e03k99/ai_is_tearing_up_the_dancing_floor_again/,Yuqing7,1574441200,,0,1
1149,2019-11-23,2019,11,23,1,e03m49,[P] cleanlab: accelerating ML and deep learning research with noisy labels,https://www.reddit.com/r/MachineLearning/comments/e03m49/p_cleanlab_accelerating_ml_and_deep_learning/,cgnorthcutt,1574441417,"&amp;#x200B;

Hey folks. Today I've officially released the [cleanlab](https://github.com/cgnorthcutt/cleanlab/) Python package, after working out the kinks for three years or so. It's the first standard framework for accelerating ML and deep learning research and software for datasets with label errors.  
`cleanlab` has some neat features:

1. If you have model outputs already (predicted probabilities for your dataset), you can find label errors in one line of code. If you don't have model outputs, its two lines of code.
2. If you're a researcher dealing with datasets with label errors, `cleanlab` will compute the uncertainty estimation statistics for you (noisy channel, latent prior of true labels, joint distribution of noisy and true labels, etc.)
3. Training a model (learning with noisy labels) is 3 lines of code.
4. `cleanlab` is [full of examples](https://github.com/cgnorthcutt/cleanlab/tree/master/examples) \-- how to find label errors in ImageNet, MNIST, learning with noisy labels, etc.

&amp;#x200B;

Full `cleanlab` announcement and documentation here: \[[LINK](https://l7.curtisnorthcutt.com/cleanlab-python-package)\]

&amp;#x200B;

GitHub: [https://github.com/cgnorthcutt/cleanlab/](https://github.com/cgnorthcutt/cleanlab/)  
PyPI: [https://pypi.org/project/cleanlab/](https://pypi.org/project/cleanlab/)

  
As an example, here is how you can find label errors in any dataset with PyTorch, TensorFlow, scikit-learn, MXNet, FastText, or other framework in 1 line of code.

`# Compute psx (n x m matrix of predicted probabilities)`  
`#     in your favorite framework on your own first, with any classifier.`  
`# Be sure to compute psx in an out-of-sample way (e.g. cross-validation)`  
`# Label errors are ordered by likelihood of being an error.`  
`#     First index in the output list is the most likely error.`

`from cleanlab.pruning import get_noise_indices`

`ordered_label_errors = get_noise_indices(`  
`s=numpy_array_of_noisy_labels,`  
`psx=numpy_array_of_predicted_probabilities,`  
`sorted_index_method='normalized_margin', # Orders label errors`  
`)`  


P.S. If you happen to work at Google, `cleanlab` is incorporated in the internal code base (as of July 2019).  
P.P.S. I don't work there, so you're on your own if Google's version strays from the open-source version.",8,1
1150,2019-11-23,2019,11,23,3,e04xzg,[R] Hybrid Composition with IdleBlock: More Efficient Networks for Image Recognition,https://www.reddit.com/r/MachineLearning/comments/e04xzg/r_hybrid_composition_with_idleblock_more/,ZihengJiang,1574446877,,1,1
1151,2019-11-23,2019,11,23,4,e05wko,The revolution of machine learning has been greatly exaggerated,https://www.reddit.com/r/MachineLearning/comments/e05wko/the_revolution_of_machine_learning_has_been/,HN_Crosspost_Bot,1574450833,,0,1
1152,2019-11-23,2019,11,23,4,e05x2f,Production Level Deep Learning Guide,https://www.reddit.com/r/MachineLearning/comments/e05x2f/production_level_deep_learning_guide/,k23033,1574450899,[removed],0,1
1153,2019-11-23,2019,11,23,4,e061dx,Victor Dibia on TensorFlow.js and Building Machine Learning Models with JavaScript,https://www.reddit.com/r/MachineLearning/comments/e061dx/victor_dibia_on_tensorflowjs_and_building_machine/,NuEd_Fernandes,1574451398,,0,1
1154,2019-11-23,2019,11,23,4,e064z2,NeurIPS Meetup: Vertical Data Science,https://www.reddit.com/r/MachineLearning/comments/e064z2/neurips_meetup_vertical_data_science/,CometML,1574451818,,0,1
1155,2019-11-23,2019,11,23,4,e0699h,"Given 4 binary random variables X1, X2, X3 and Y, estimate the following probability distributions: P(Y), P(X1|Y), P(X2|Y), P(X3|Y). Two sorts of estimates are required: maximum likelihood estimates and a Bayesian estimate.",https://www.reddit.com/r/MachineLearning/comments/e0699h/given_4_binary_random_variables_x1_x2_x3_and_y/,satanicgeese,1574452307," 

How would i go about it to calculate this P(Y=0) = \*

P(X1=0|Y=0) = \*

P(X1=0|Y=1) = \*

P(X2=0|Y=0) = \*

P(X2=0|Y=1) = \*

P(X3=0|Y=0) = \*

P(X3=0|Y=1) = \*

500 samples

X1,X2,X3,Y

0,0,0,0

1,1,1,0

1,0,0,1

0,0,0,1

0,0,1,1

1,0,1,0

1,0,1,0

0,1,1,0

0,1,1,0

1,1,0,0

1,0,1,0

0,0,1,1

1,0,0,0

1,1,1,1

0,1,1,0

0,0,0,0

0,0,0,1

0,0,0,0

1,1,0,0

0,1,0,0

0,0,0,0

1,1,0,1

1,0,0,0

0,0,0,0

0,0,0,0

0,0,1,1

0,1,1,0

0,0,0,0

1,1,0,0

0,0,1,0

1,0,0,0

0,1,1,0

1,0,0,0

1,1,1,1

1,1,0,0

0,1,1,0

0,0,0,1

0,1,1,0

0,1,0,0

1,1,0,1

0,1,0,0

1,1,0,0

1,0,1,1

1,0,0,0

0,1,0,0

0,0,0,1

0,1,1,1

1,0,1,1

0,0,1,1

1,0,1,1

0,1,0,0

0,0,0,0

0,0,0,0

1,0,0,0

0,1,0,0

0,1,0,0

1,0,1,0

0,0,0,1

0,0,1,1

1,1,1,0

1,0,1,1

0,1,0,0

1,1,0,0

0,1,0,0

1,0,1,1

0,0,0,0

0,1,1,0

0,0,0,0

0,1,0,0

0,0,1,0

0,0,0,0

1,0,0,0

0,1,0,0

0,0,0,1

0,0,0,0

0,0,0,0

1,1,0,0

0,1,1,0

1,0,1,1

1,0,0,0

0,1,1,0

1,1,0,0

1,0,1,1

0,1,0,0

0,0,0,0

1,0,0,0

1,1,1,1

0,1,0,0

0,0,0,0

1,0,0,0

0,0,1,0

1,0,0,0

1,1,1,1

1,0,0,0

1,0,1,1

1,0,1,1

1,0,0,0

0,0,1,1

1,0,0,1

1,0,0,1

1,1,0,1

0,1,0,0

0,0,0,0

0,1,1,0

0,1,0,0

0,0,0,0

0,1,0,0

1,0,0,1

1,1,0,0

0,1,0,0

0,1,0,0

0,1,1,0

0,1,0,0

1,0,0,0

0,1,1,0

0,0,0,0

1,0,0,1

1,1,0,0

0,1,0,0

0,0,0,0

0,0,1,0

1,1,0,0

0,1,0,0

0,0,1,0

0,1,0,0

0,1,0,0

1,1,1,1

1,0,0,1

1,0,1,1

1,1,0,0

0,0,1,1

1,1,0,0

0,1,0,0

0,1,1,0

1,0,1,0

0,0,1,1

1,1,1,1

0,0,1,0

1,1,0,1

1,0,0,1

0,1,0,0

0,0,0,0

0,1,0,0

0,1,0,0

0,0,0,0

0,1,0,1

0,0,1,1

0,1,0,0

0,0,0,0

1,1,0,1

0,0,0,0

1,0,0,0

0,1,0,0

1,0,0,0

0,1,0,0

0,1,0,0

0,0,0,0

1,0,0,0

0,1,0,0

0,1,0,0

0,0,1,1

0,1,0,0

0,1,1,0

0,1,0,0

0,0,0,0

0,0,1,1

1,0,0,0

0,1,1,0

0,0,0,0

1,0,0,0

1,0,1,1

0,0,0,1

0,1,0,0

1,1,0,0

0,0,0,1

0,1,0,0

1,0,1,1

0,0,1,1

0,1,0,0

1,0,0,1

0,0,0,0

1,0,0,0

0,1,0,0

0,1,0,0

0,1,0,0

1,0,0,0

0,0,0,1

1,1,0,0

0,0,0,0

1,0,0,0

0,1,0,0

0,0,0,1

0,1,0,0

1,0,0,1

0,1,1,1

0,1,0,0

0,0,1,1

1,1,0,0

1,1,0,1

1,0,0,1

0,1,0,0

0,0,1,1

0,1,0,0

0,1,0,0

1,0,1,1

1,0,1,0

0,1,0,1

1,1,0,0

0,0,0,0

0,0,1,1

0,1,0,0

0,1,0,0

1,0,0,1

1,1,0,0

0,0,0,0

1,0,1,0

0,0,0,0

1,0,0,0

0,1,0,0

1,1,0,1

1,0,0,0

0,1,0,0

1,0,1,1

0,0,0,1

0,1,0,0

1,0,1,0

1,1,1,1

1,0,0,0

1,1,0,0

0,0,0,0

1,1,0,0

0,0,0,1

1,1,0,1

0,1,0,0

1,0,1,1

0,1,0,0

1,1,0,0

1,0,0,0

0,0,0,0

0,1,1,0

1,1,0,0

0,0,1,0

0,0,0,0

0,1,0,0

0,0,0,1

0,1,1,0

1,1,0,0

0,0,1,1

1,1,0,0

1,0,0,0

1,1,0,0

1,0,0,0

1,0,0,0

1,1,1,1

1,0,0,0

1,1,0,0

0,1,0,0

1,0,1,1

1,0,1,1

1,1,0,0

0,1,0,0

1,1,1,1

0,1,0,0

0,1,0,0

0,1,0,0

0,1,0,0

0,0,0,1

1,0,0,0

1,0,1,1

1,1,1,1

1,1,0,0

0,0,0,0

0,0,0,0

0,0,0,0

1,0,1,1

0,0,0,1

0,1,0,0

1,1,0,1

1,0,1,1

1,0,0,1

0,0,0,0

1,0,0,0

1,1,0,0

1,0,0,0

0,1,0,0

0,0,0,0

1,0,0,1

1,0,1,0

0,0,0,0

0,0,0,1

1,0,0,0

0,1,0,0

1,0,1,1

0,1,1,0

0,1,1,0

0,1,0,0

1,0,1,1

1,0,0,0

0,1,0,0

0,0,0,0

0,0,0,0

0,0,0,0

0,0,0,1

0,0,1,1

0,0,1,0

1,0,1,1

0,1,1,1

0,1,0,0

1,0,0,0

1,0,0,1

0,1,0,0

0,1,0,0

0,1,1,0

1,0,1,0

0,1,1,1

0,0,0,0

1,0,1,1

1,0,1,1

1,0,1,1

0,1,1,0

1,0,0,0

0,0,1,1

1,0,0,0

1,0,0,0

1,1,0,0

1,1,1,1

1,0,0,1

1,1,0,1

1,1,1,1

0,0,0,0

0,1,0,0

1,1,0,1

1,0,0,1

0,1,0,0

1,0,1,1

0,0,0,0

1,0,0,0

1,0,1,1

0,1,0,0

0,0,0,0

0,0,0,0

1,0,0,0

1,0,0,0

0,0,1,1

1,0,0,0

1,0,0,1

1,1,0,0

0,1,0,0

0,0,0,0

1,0,0,1

1,0,0,0

0,0,0,0

0,1,0,0

0,0,0,0

0,0,1,1

1,0,1,0

1,1,0,1

0,0,0,0

0,0,1,1

0,1,0,0

1,0,1,1

1,1,0,0

1,0,0,1

0,1,0,0

0,0,1,1

1,0,0,0

1,0,1,1

1,0,1,1

1,1,0,1

0,0,1,1

0,0,0,1

0,0,0,1

0,1,1,0

1,1,0,1

1,0,0,0

1,0,0,1

1,0,0,1

0,0,1,0

1,0,0,1

0,0,1,0

1,0,0,0

1,1,0,0

1,1,0,0

1,0,1,0

1,0,0,1

1,0,0,1

0,1,0,0

0,0,0,1

0,1,0,0

1,0,0,0

0,1,0,0

0,1,0,0

0,0,0,0

0,1,1,1

1,1,0,1

1,0,1,0

1,0,0,1

0,1,1,0

1,0,1,1

1,0,1,1

1,0,0,1

1,1,0,1

0,1,0,0

1,1,0,0

1,0,1,1

1,1,0,0

1,0,1,1

0,0,0,0

0,0,0,0

1,0,1,1

0,1,0,0

1,0,0,1

1,0,1,1

0,0,1,1

0,0,0,1

0,1,0,0

1,0,1,1

0,1,0,0

1,1,1,1

1,0,0,0

1,0,0,0

0,0,0,0

0,0,1,1

1,0,0,0

0,0,0,0

1,0,1,1

0,1,0,0

0,0,1,1

0,0,0,1

0,1,0,0

1,0,0,1

1,0,1,0

1,0,0,1

0,0,1,1

0,1,1,0

1,0,0,0

0,1,0,0

1,1,0,1

1,1,1,1

0,0,1,1

1,0,1,1

1,0,1,0

0,1,0,0

0,1,0,0

0,0,1,1

1,0,1,1

0,1,0,0

0,0,0,1

1,1,1,0

0,0,1,1

0,0,0,0

0,1,0,0

0,1,0,1

1,0,0,0

1,0,1,0

1,1,1,0

0,1,0,0

0,1,0,0

0,0,0,0

0,0,0,1

1,1,0,0

0,0,0,0

0,0,1,1

1,0,0,1

0,1,0,0

1,1,1,0

1,0,0,0

0,0,0,0

0,1,0,0

0,1,0,0

1,1,0,1

1,1,0,0

1,1,0,0

0,1,1,0

0,0,0,0

0,1,1,1

0,0,1,1

1,0,0,0

1,0,0,1

0,0,0,1

1,0,0,0

0,1,0,0

1,1,1,0

0,0,0,0

1,0,0,0

1,0,0,1

0,0,1,1

0,0,0,0

1,0,0,1

1,0,0,0

0,1,0,0

1,0,0,0

1,0,0,0

0,0,0,1

1,0,1,1

0,0,0,0

1,0,1,0

0,0,1,0

0,1,1,0

1,0,1,1",0,1
1156,2019-11-23,2019,11,23,4,e06ahx,"NeurIPS Conference Invited Talk ""Agency and Automation: Designing AI""",https://www.reddit.com/r/MachineLearning/comments/e06ahx/neurips_conference_invited_talk_agency_and/,CometML,1574452440,,0,1
1157,2019-11-23,2019,11,23,5,e06wqa,Help regarding training OCR,https://www.reddit.com/r/MachineLearning/comments/e06wqa/help_regarding_training_ocr/,Anoop_kumar,1574454925,[removed],0,1
1158,2019-11-23,2019,11,23,6,e07f6w,[N] President Nixon Never Actually Gave This Apollo 11 Disaster Speech. MIT Brought It To Life To Illustrate Power Of Deepfakes,https://www.reddit.com/r/MachineLearning/comments/e07f6w/n_president_nixon_never_actually_gave_this_apollo/,Nebulata,1574457007,,0,1
1159,2019-11-23,2019,11,23,7,e08snk,"Weekly Papers | Quoc V. Le and Kaiming He Look at Vision; Intelligence, Psychology and AI; Evolving the Hearthstone Meta and More!",https://www.reddit.com/r/MachineLearning/comments/e08snk/weekly_papers_quoc_v_le_and_kaiming_he_look_at/,Yuqing7,1574462922,,0,1
1160,2019-11-23,2019,11,23,8,e09jyk,Visualizing the Geometric and Harmonic Means (images inside),https://www.reddit.com/r/MachineLearning/comments/e09jyk/visualizing_the_geometric_and_harmonic_means/,Luke_Persola,1574466231,,0,1
1161,2019-11-23,2019,11,23,8,e09kjp,[Discussion] Understanding Subscale WaveRNN &amp; usage of Masked Dilated CNN as conditioning network,https://www.reddit.com/r/MachineLearning/comments/e09kjp/discussion_understanding_subscale_wavernn_usage/,bigbawsboy,1574466306,"Related Paper: [Efficient Neural Audio Synthesis](https://arxiv.org/pdf/1802.08435v2.pdf)

&amp;#x200B;

I have been reading the sections relating to **Subscale WaveRNN** in where the DeepMind team was able to generate B samples in a single step. They have discussed about conditioning a particular sample using past samples and up to F samples from the previous sub-tensors future context. In their case, they used a masked dilated CNN (this can be found on the last paragraph of **4.1 Subscale Dependency Scheme**). Here's the excerpt specifically to this:

&amp;#x200B;

&gt;The Subscale WaveRNN that generates a given sub-tensor is conditioned on the future context of previous sub-tensors using a masked dilated CNN with relus and the mask applied over past connections instead of future ones.  

&amp;#x200B;

My first question is: how could a masked dilated CNN help with this?

&amp;#x200B;

Next, Nal Kalchbrenner has tweeted [this quick demo](https://twitter.com/NalKalchbrenner/status/968129212442906626) of the Subscale WaveRNN. This one confuses me a lot when I'm referring back to the original paper.

&amp;#x200B;

My final question is: does anyone have taken a look at subscaling more closely?

&amp;#x200B;

Any insights would be appreciated.

&amp;#x200B;

(Note: This is my first post and I am hoping that I followed the format correctly.)",0,1
1162,2019-11-23,2019,11,23,9,e0akbb,[D] ICLR reviewers and making the ML community better,https://www.reddit.com/r/MachineLearning/comments/e0akbb/d_iclr_reviewers_and_making_the_ml_community/,watercannon123,1574470780,"I'm reviewing for ICLR myself, so I know reading the revised papers and carefully reading all the lengthy rebuttals feels like a terrible time-sink, but to everyone else who's also reviewing: please remember that most authors have spent an enormous time and effort in their submissions.

I've noticed that many reviews have already been updated after the rebuttal period but it seems that most if not all miss key points that are addressed in the rebuttal or in the revised paper. There's an option to compare revisions which highlights the changes -- please use this feature, as some authors address points in the revision but don't mention it explicitly in the rebuttal (this actually happened for all papers I'm reviewing). I submitted a paper myself and my reviews were shorter than last year, and I also have the feeling that my rebuttal wasn't carefully read by the reviewers who updated their reviews.

I also know that many reviewers this year are reviewing for the first time, but please do make an effort to spend some time going over rebuttals and revisions. You're now part of the ML academic community -- try to make it better, we need it especially now that the many of the highest-rated papers have extremely short reviews with low confidence scores, including reviews as short as 20 words.

TLDR: we all know there are not enough reviewers and way too many submissions. while reviewing for free can be frustrating, the community depends on us and the job includes being thoughtful and reading rebuttals/revisions carefully",19,1
1163,2019-11-23,2019,11,23,10,e0akil,"Why not, after splitting up your data into train and test and getting a positive result, retrain your model over all your data?",https://www.reddit.com/r/MachineLearning/comments/e0akil/why_not_after_splitting_up_your_data_into_train/,littlepigboy5,1574470810,[removed],0,1
1164,2019-11-23,2019,11,23,10,e0axl1,Auto Machining,https://www.reddit.com/r/MachineLearning/comments/e0axl1/auto_machining/,defaultuser6969,1574472553,[removed],0,1
1165,2019-11-23,2019,11,23,11,e0bl8e,New SOTA Semi Supervised method: EnAET,https://www.reddit.com/r/MachineLearning/comments/e0bl8e/new_sota_semi_supervised_method_enaet/,Xiao_Wang96,1574475798,[removed],0,1
1166,2019-11-23,2019,11,23,12,e0chih,"Hey, what are you guys using for web deployments?",https://www.reddit.com/r/MachineLearning/comments/e0chih/hey_what_are_you_guys_using_for_web_deployments/,mischief_23,1574480399,"Im a cs student currently learning machine learning (focussing mostly on deep learning scaffolding and computational techniques to make models efficient)
I have prior experience in server side coding, with an academic project which focussed in large scale API deployments and CI/CD integration to an existing analytics API.

I am planning on building an easy to use service which makes it easy to expose a machine learning model to the web via an API endpoint. While I know this has been done before by bigger players, I found most of the solutions to have cumbersome setups and slow speeds. 

I would really appreciate it if you guys could provide me some insight as to what improvements you would like to see in such a service as compared to your current workflow.",0,1
1167,2019-11-23,2019,11,23,12,e0ci0o,Is (Convex) Optimization Still Relevant in Machine Learning Today?,https://www.reddit.com/r/MachineLearning/comments/e0ci0o/is_convex_optimization_still_relevant_in_machine/,cibidus,1574480488,[removed],0,1
1168,2019-11-23,2019,11,23,14,e0diq2,This Open Sourced Python library audit-AI does Bias Testing for Generalized Machine Learning Applications [Github and Paper link included],https://www.reddit.com/r/MachineLearning/comments/e0diq2/this_open_sourced_python_library_auditai_does/,ai-lover,1574486382,,0,1
1169,2019-11-23,2019,11,23,14,e0do2s,questions about tensor2tensor,https://www.reddit.com/r/MachineLearning/comments/e0do2s/questions_about_tensor2tensor/,siriusyooooo,1574487284,[removed],0,1
1170,2019-11-23,2019,11,23,19,e0g3oy,Detect Credit Card Fraud with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/e0g3oy/detect_credit_card_fraud_with_machine_learning/,AnujG23,1574504161,,0,1
1171,2019-11-23,2019,11,23,20,e0gkl8,Searching for old FAQ-like Conference Poster with interactive Website,https://www.reddit.com/r/MachineLearning/comments/e0gkl8/searching_for_old_faqlike_conference_poster_with/,Mc_Lawrence,1574507338,[removed],0,1
1172,2019-11-23,2019,11,23,20,e0goca,Video Semantic Search in Large Scale using GNES and Tensorflow 2.0,https://www.reddit.com/r/MachineLearning/comments/e0goca/video_semantic_search_in_large_scale_using_gnes/,h_xiao,1574508080,,0,1
1173,2019-11-23,2019,11,23,20,e0gotc,[P] Video Semantic Search in Large Scale using GNES and Tensorflow 2.0,https://www.reddit.com/r/MachineLearning/comments/e0gotc/p_video_semantic_search_in_large_scale_using_gnes/,h_xiao,1574508165,,0,1
1174,2019-11-23,2019,11,23,21,e0hbxy,Amazon Cloud Support Associate,https://www.reddit.com/r/MachineLearning/comments/e0hbxy/amazon_cloud_support_associate/,ahsankhan04,1574512611,,0,1
1175,2019-11-23,2019,11,23,21,e0hesx,Column transformer throwing away some features?,https://www.reddit.com/r/MachineLearning/comments/e0hesx/column_transformer_throwing_away_some_features/,zzorangezz,1574513141,,0,1
1176,2019-11-23,2019,11,23,21,e0hilr,Used Amada Press Brakes Sale - Hi-Tech Machinery Inc,https://www.reddit.com/r/MachineLearning/comments/e0hilr/used_amada_press_brakes_sale_hitech_machinery_inc/,hitechmachineryinc,1574513836,,0,1
1177,2019-11-23,2019,11,23,22,e0hmd9,"Used Milling Machine and Bridgeport Mill, Cincinnati and others - Hi-Tech Machinery Inc",https://www.reddit.com/r/MachineLearning/comments/e0hmd9/used_milling_machine_and_bridgeport_mill/,hitechmachineryinc,1574514437,,0,1
1178,2019-11-23,2019,11,23,22,e0hogn,Why isn't Chainer more used/discussed?,https://www.reddit.com/r/MachineLearning/comments/e0hogn/why_isnt_chainer_more_useddiscussed/,TheAlgorithmist99,1574514779,[removed],0,1
1179,2019-11-23,2019,11,23,22,e0hxls,Has anyone successfully gotten access to IBM's Diversity in Faces dataset? How long did it take?,https://www.reddit.com/r/MachineLearning/comments/e0hxls/has_anyone_successfully_gotten_access_to_ibms/,ml_runway,1574516214,[removed],0,1
1180,2019-11-23,2019,11,23,22,e0i59u,Looking Used Radial Arm Drill Press For Sale? - Hi-Tech Machinery Inc,https://www.reddit.com/r/MachineLearning/comments/e0i59u/looking_used_radial_arm_drill_press_for_sale/,hitechmachineryinc,1574517359,,0,1
1181,2019-11-23,2019,11,23,23,e0i9xo,A doctor wanting to do ML research,https://www.reddit.com/r/MachineLearning/comments/e0i9xo/a_doctor_wanting_to_do_ml_research/,shandett,1574518014,[removed],0,1
1182,2019-11-23,2019,11,23,23,e0icee,Used CNC Machines and Used CNC Lathe - HI-Tech Machinery Inc,https://www.reddit.com/r/MachineLearning/comments/e0icee/used_cnc_machines_and_used_cnc_lathe_hitech/,hitechmachineryinc,1574518357,,0,1
1183,2019-11-23,2019,11,23,23,e0ijkd,[D] Generating Visualizations for Network Architectures,https://www.reddit.com/r/MachineLearning/comments/e0ijkd/d_generating_visualizations_for_network/,ssd123456789,1574519373,"Deep learning papers often have very good diagrams of their architectures. Does anyone know of tools that can be used to generate these sorts of diagrams. I'm not looking for automatically generated diagrams.

So my question is this:

What kind of software do people use to make nice looking Visualizations for their network architecture. A really nice example is the pointnet architecture. So does anyone know what was or could have been used to generate that architecture diagram.",16,1
1184,2019-11-23,2019,11,23,23,e0iobx,Used CNC Vertical Machining Center | HI-Tech Machinery Inc,https://www.reddit.com/r/MachineLearning/comments/e0iobx/used_cnc_vertical_machining_center_hitech/,hitechmachineryinc,1574520039,,0,1
1185,2019-11-24,2019,11,24,0,e0ixgz,Used Milling Machine | HI-Tech Machinery Inc,https://www.reddit.com/r/MachineLearning/comments/e0ixgz/used_milling_machine_hitech_machinery_inc/,hitechmachineryinc,1574521293,,0,1
1186,2019-11-24,2019,11,24,0,e0j9cb,"[P] 2,000x Faster RAPIDS TSNE - 3 hours down to 5 seconds on NVIDIA GPUs",https://www.reddit.com/r/MachineLearning/comments/e0j9cb/p_2000x_faster_rapids_tsne_3_hours_down_to_5/,danielhanchen,1574522865,"TSNE is a very popular data visualization algorithm used alongside PCA and UMAP.

Sklearn's TSNE is very effective for small datasets, but already on the 60,000 MNIST Digits dataset, expect to wait 1 hour. With RAPIDS cuML, you can now run TSNE on MNIST in a mere 3 seconds!

On 200,000 rows, Sklearn takes a whopping 3 hours, whilst RAPIDS takes 5 seconds! (2,000x faster).

![img](cws1jzejeg041 "" Figure 1. cuML TSNE on MNIST Fashion takes 3 seconds. Scikit-Learn takes 1 hour. "")

Check out my blog showcasing how cuML achieves this massive performance boost, and how NVIDIA GPUs can help scientist and engineers save their precious time. [https://medium.com/rapids-ai/tsne-with-gpus-hours-to-seconds-9d9c17c941db](https://medium.com/rapids-ai/tsne-with-gpus-hours-to-seconds-9d9c17c941db)

![img](on7p16aseg041 "" Figure 2. TSNE used on the 60,000 Fashion MNIST dataset (3 seconds)"")

Give cuML a try! You might know me as the author of HyperLearn, and I can say cuML is the gold standard package for machine learning on GPUs! [https://github.com/rapidsai/cuml](https://github.com/rapidsai/cuml)

Linear Regression, UMAP, K-Means, DBSCAN etc are all sped up on the GPU! If you have any questions, feel free to ask!

![img](l72o2o2bfg041 "" Table 1. cuMLs TSNE time running on an NVIDIA DGX-1 with using 1 V100 GPU. "")",30,1
1187,2019-11-24,2019,11,24,0,e0jepq,[P] Predict figure skating world championship ranking from season performances (FunkSVD + learning to rank),https://www.reddit.com/r/MachineLearning/comments/e0jepq/p_predict_figure_skating_world_championship/,seismatica,1574523589,"I'm trying to predict the ranking of figure skaters in the annual world championship by their scores in earlier competition events in the season. The obvious method to do is by average the scores for each skater across past events and rank them by those averages. However, since no two events are the same, the goal for my project is to separate the **skater effect**, the intrinsic ability of each skater, by the **event effect**, how an event influence the score of a skater.

* I've previously posted on Reddit my attempts to do so using simple linear models, which you can read on Medium [part 1](https://towardsdatascience.com/predicting-figure-skating-championship-ranking-from-season-performances-fc704fa7971a?source=friends_link&amp;sk=7e6b2992c6dd5e6e7e1803c574b4236d) of my project. These models will output a latent score for each skater that we can use to rank them.

* However, another approach to learn the latent scores of skater is to think of factorizing the event-skater matrix of raw scores in the season into a skater-specific matrix and an event-specific matrix that multiply together to approximate the raw score. Therefore, this is exactly the same as the [matrix factorization in recommender systems](https://en.wikipedia.org/wiki/Matrix_factorization_(recommender_systems), but with user=skater, item=event, and rating=raw score.

* As a result, I used a variant of the famous [FunkSVD](https://sifter.org/~simon/journal/20061211.html) algorithm to learn the latent scores of skater. In [part 2](https://towardsdatascience.com/predicting-figure-skating-world-championship-ranking-from-season-performances-part-2-hybrid-7d296747b15?source=friends_link&amp;sk=86881d127654ece260be2e3029dfbad2) of my project, I tried finding just a single latent score for each skater, and rank skaters by those scores. Next, in [part 3](https://towardsdatascience.com/predict-figure-skating-world-championship-ranking-from-season-performances-8af099351e9c?source=friends_link&amp;sk=48c2971de1a7aa77352eb96eec77f249), I learned multiple latent factors for each skater using the same FunkSVD method. Since I'm implementing it from scratch, I try using various implementations of the algorithm: from a naive approach using for loop, to one using numpy broadcasting, and one using matrix multiplication, and benchmark them both in time and space complexity.

* However, one major problem with multiple factors is that it's hard to know which factor to rank skater with. Thankfully, the ranking metric I use in the project (Kendall's tau) allows me to build a simple logistic regression model to combine these scores to rank the skaters. This can be done with pairwise differences in score in each factor as predictors, and the world championship ranking itself as the response. I later learned that this belong to the pairwise methods used for [learning-to-rank](https://en.wikipedia.org/wiki/Learning_to_rank) tasks often encountered in information system, and you can read my implementation of it in [part 4](https://medium.com/@seismatica/predict-figure-skating-world-championship-ranking-from-season-performances-a4771f2460d2?source=friends_link&amp;sk=61ecc86c4340e2e3095720cae80c0e70). 

* However, the result at the end of this part was not very encouraging, likely due to the way that I use FunkSVD to train the latent factors. Therefore, I [part 5](https://medium.com/@seismatica/predict-figure-skating-world-championship-ranking-from-season-performances-7461dc5c0722?source=friends_link&amp;sk=fcf7e410d33925363d0bbbcf59130ade), I modified my FunkSVD implementation to solve this problem, by training the factors in sequence instead of all at once. I then discovered afterward that Mr Funk also originally trained all of his factors in sequence, so I should have read his work more carefully at the start!

You can see all the code I used for my project in the Github [repo](https://github.com/dknguyengit/skate_predict). I'm more than happy to receive any questions or feedback from you guys on my project!",0,1
1188,2019-11-24,2019,11,24,0,e0jnwu,State-of-the-Art Machine Learning Automation with HDT,https://www.reddit.com/r/MachineLearning/comments/e0jnwu/stateoftheart_machine_learning_automation_with_hdt/,andrea_manero,1574524752,[removed],0,1
1189,2019-11-24,2019,11,24,1,e0jstd,Can any one suggest a good book for getting started with machine learning,https://www.reddit.com/r/MachineLearning/comments/e0jstd/can_any_one_suggest_a_good_book_for_getting/,Gokul123654,1574525316,[removed],0,1
1190,2019-11-24,2019,11,24,1,e0ju09,[D] Something like Leetcode for to learn ML/Python?,https://www.reddit.com/r/MachineLearning/comments/e0ju09/d_something_like_leetcode_for_to_learn_mlpython/,Knackmanic,1574525450,I'd like to become more proficient in Python and ML. Have you found a method to train yourself?,7,1
1191,2019-11-24,2019,11,24,1,e0k4gd,How do I set my self up for Machine Learning after learning python basics?,https://www.reddit.com/r/MachineLearning/comments/e0k4gd/how_do_i_set_my_self_up_for_machine_learning/,UniversalLemon,1574526682,[removed],0,1
1192,2019-11-24,2019,11,24,2,e0kz1g,Help finding ideas for first deep learning project,https://www.reddit.com/r/MachineLearning/comments/e0kz1g/help_finding_ideas_for_first_deep_learning_project/,daddyRiemann,1574530101,[removed],0,1
1193,2019-11-24,2019,11,24,2,e0l3od,Fundamental Bounds on Learning Performance in Neural Circuits [Presentation],https://www.reddit.com/r/MachineLearning/comments/e0l3od/fundamental_bounds_on_learning_performance_in/,RSchaeffer,1574530614,,0,1
1194,2019-11-24,2019,11,24,3,e0lglr,What is My Type?,https://www.reddit.com/r/MachineLearning/comments/e0lglr/what_is_my_type/,CeramicVulture,1574532116,[removed],0,1
1195,2019-11-24,2019,11,24,3,e0ljb9,Can ML create a DJ remix or a mashup of songs,https://www.reddit.com/r/MachineLearning/comments/e0ljb9/can_ml_create_a_dj_remix_or_a_mashup_of_songs/,4site91,1574532451,[removed],0,1
1196,2019-11-24,2019,11,24,3,e0lmtm,C5.0 algorithm for decision trees.,https://www.reddit.com/r/MachineLearning/comments/e0lmtm/c50_algorithm_for_decision_trees/,suhas_bn_1412,1574532854,"So for classifying data in Indian Liver Patient Dataset , I recently saw a paper in which they used decision tree using C5.0 and got accuracy of over 90%. Does anybody know how to code that in python. Or any resources to study C5.0 decision tree are appreciated. Thank you.",0,1
1197,2019-11-24,2019,11,24,3,e0lrac,Fastest way to Display NumPy Array several Times per second,https://www.reddit.com/r/MachineLearning/comments/e0lrac/fastest_way_to_display_numpy_array_several_times/,Sakati84,1574533367,[removed],0,1
1198,2019-11-24,2019,11,24,4,e0mji8,How do I integrate machine learning into my smart gloves project?,https://www.reddit.com/r/MachineLearning/comments/e0mji8/how_do_i_integrate_machine_learning_into_my_smart/,mkidk,1574536624,[removed],0,1
1199,2019-11-24,2019,11,24,4,e0n0tn,Time series projects,https://www.reddit.com/r/MachineLearning/comments/e0n0tn/time_series_projects/,pro_potato96,1574538581,[removed],0,1
1200,2019-11-24,2019,11,24,5,e0ndy8,Best way to learn formula notations,https://www.reddit.com/r/MachineLearning/comments/e0ndy8/best_way_to_learn_formula_notations/,koeyoshi,1574540064,,0,1
1201,2019-11-24,2019,11,24,6,e0o9hx,[P] A Trump Twitter bot using GPT2 trained on his tweets,https://www.reddit.com/r/MachineLearning/comments/e0o9hx/p_a_trump_twitter_bot_using_gpt2_trained_on_his/,GrainElevator,1574543739,,0,1
1202,2019-11-24,2019,11,24,6,e0on0m,Building a conv autoencoder for anomaly detection on large images but the ae kind of copies the input to output,https://www.reddit.com/r/MachineLearning/comments/e0on0m/building_a_conv_autoencoder_for_anomaly_detection/,bolom_sounga,1574545360,[removed],0,1
1203,2019-11-24,2019,11,24,7,e0pir2,[D]Why isn't Chainer more used/discussed?,https://www.reddit.com/r/MachineLearning/comments/e0pir2/dwhy_isnt_chainer_more_useddiscussed/,TheAlgorithmist99,1574549240,"Pretty much what it says in the title, but to elaborate on the two questions:

Why isn't Chainer more used? It seems to have pioneered some nice ideas like model subclassing, has many interesting ""sub-libraries"", like ChainerCV and ChainerRL, but I think it is barely used outside Japan (not exactly sure if they use it a lot either)

And then in the same vein, why is it not more discussed when talking about Deep Learning frameworks? We see a lot of comparison between Pytorch and Tensorflow, then maybe some MxNet and new players (Jax, Halide) and non-python frameworks (mostly Julia's), but Chainer almost seems to be ignored in most of these discussions.

(Also feel welcome to comment on pretty much the same questions but regarding Cupy vs Numba or similar)",9,1
1204,2019-11-24,2019,11,24,8,e0py1s,Adam Schiffs ominous warning to ML developers. Airs Sunday on FX.,https://www.reddit.com/r/MachineLearning/comments/e0py1s/adam_schiffs_ominous_warning_to_ml_developers/,Mkilli,1574551092,,0,1
1205,2019-11-24,2019,11,24,8,e0pzx9,Techniques and semantics in better training of deep learning models,https://www.reddit.com/r/MachineLearning/comments/e0pzx9/techniques_and_semantics_in_better_training_of/,tjasmin111,1574551318,"I'm relatively new to Deep Learning, and trying various models and datasets using \`Keras\`. I'm starting to love it!

Through-out my experimentations, I have come into some semantic questions that I don't know how they can affect the overall accuracy of my trained model. My target application is fire detection in videos (fire vs non-fire). So I'm trying to get tips and tricks from those well experienced on Deep learning, and here are my semantic questions:

 1. Given that I have to do detection on videos, I've been mostly adding actuall frames of videos to my dataset, and less photos. Does adding photos from Google ever help (as we largen our dataset) or it's actually more considered noises and shall be removed?

 2. I've trained a deep model (\`ResNet50\`) as well as a shallow 5-layer model. I realized the ResNet50 model is more sensitive and has a high recall (all fires are definitely detected), but has false positives as well (strong source of lights like sunlight or lamps are identified as fire). While the shallower model is 10x faster, it can miss fires if it is smaller in the image, so it's less sensitive. But also has low false positives. Is it always true? So what are techniques and tips to fix these issues in each of these models? 

For instance, the shallow model doesn't see this fire. Shall I think it's not complex enough to work well when the scene has many objects inside?

\[!\[enter image description here\]\[1\]\]\[1\]

&amp;#x200B;

 3. The sample code I saw resizes photos to \`256x256\` for training. What's the effect of bigger sizes vs smaller ones say \`300x300\`? Can I expect while bigger sizes increase computation time, they provide higher accuracy?

 4. The sample code also converts photos to grayscale and uses Antialiasing before passing. Does it have good effects? What if I pass the colored version as fire is mostly about colors?

 5. When I see the model is doing bad on certain scenes (say those sun lights or lamps), I take multiple of those frames and add them to my non-fire dataset. Does it have any positive effects and being taken care of? And is it better to add multiple successive frames or just one frame is enough? 

 6. My fire dataset has \`1800\` images and my non-fire dataset has \`4500\` images. As a rule of thumb, the bigger each class, the better? Of course the non-fire data should be bigger, but we can not add whatever on earth as non-fire so what should be the distribution of the sizes?

&amp;#x200B;

  \[1\]: [https://i.stack.imgur.com/gwrjrm.jpg](https://i.stack.imgur.com/gwrjrm.jpg)",0,1
1206,2019-11-24,2019,11,24,8,e0q6y7,"[D] Non-students, what's your day job?",https://www.reddit.com/r/MachineLearning/comments/e0q6y7/d_nonstudents_whats_your_day_job/,Ctown_struggles00,1574552202,What kind of work do you guys do?,68,1
1207,2019-11-24,2019,11,24,9,e0qrgc,Road Map to Become Data Scientist / AI Developer,https://www.reddit.com/r/MachineLearning/comments/e0qrgc/road_map_to_become_data_scientist_ai_developer/,getsetcoding47,1574554860,"GSC CS Course for Beginner's / Intermediates

Link:- https://github.com/Arbazkhan4712/GSC-Open-Source-Computer-Science-Degree

Here is a course for beginner's and newbies or anyone who wants to learn or develope their skills

I have tried to create a road map for many domains like Android/IOS Developer, Full Stack Developer, DevOps Engineer , Data Scientist, Artificial Intelligence Developer And so on...

I tried to mention all the skills required for each one of these domains with YouTube videos , books and Free Courses I'll be adding more to it..

If you want to contribute please make changes and make pull request...

And Please give suggestions for improving it...

Share &amp; Support :)",0,1
1208,2019-11-24,2019,11,24,9,e0qwr7,The perfect DeepFake.,https://www.reddit.com/r/MachineLearning/comments/e0qwr7/the_perfect_deepfake/,Mkilli,1574555537,,0,1
1209,2019-11-24,2019,11,24,9,e0r6qf,Ideas for a Texture Classification Class Project,https://www.reddit.com/r/MachineLearning/comments/e0r6qf/ideas_for_a_texture_classification_class_project/,ConfusedNoobie,1574556850,[removed],0,1
1210,2019-11-24,2019,11,24,10,e0rdq8,Reimplementation of Hyperspherical Prototype Networks (NeurIPS 2020),https://www.reddit.com/r/MachineLearning/comments/e0rdq8/reimplementation_of_hyperspherical_prototype/,ACTBRUH,1574557776,"Link to paper: https://arxiv.org/abs/1901.10514 
Link to my reimplementation: https://github.com/Abhishaike/HyperProtoNetReproduce

This is a Pytorch reimplementation of the NeurIPS 2019 paper Hyperspherical Prototype Networks in Pytorch. This paper proposes an extension to Prototype Networks, in which the prototypes are placed a priori with large margin separation, and remain unchanged during the training/testing process of the model. The paper suggests that this extension allows for more flexible classification, regression, and joint multi-task training of regression/classification, and with higher accuracy compared to typical Prototype Networks.

This repo includes reproduced benchmarks for most of their datasets. Largely the same accuracy/error, but quite off on CIFAR-100 (not ImageNet-200 though for some reason), so it's possible this is an issue on my end. 

I also found their use of SGD for prototype creation to be unusual, considering that, the way they phrased the prototype problem, it seems like a job more for a constrained optimization algorithm. Alongside the SGD implementation (which are used for the included benchmarks), I added in two other optimization algorithms, one unconstrained (BFGS) and one constrained (SLSQP). These didn't seem to change the results much. 

This is my first reimplementation of a paper, so any critiques would be great!",0,1
1211,2019-11-24,2019,11,24,10,e0rei8,Reimplementation of Hyperspherical Prototype Networks (NeurIPS 2019),https://www.reddit.com/r/MachineLearning/comments/e0rei8/reimplementation_of_hyperspherical_prototype/,ACTBRUH,1574557883,[removed],0,1
1212,2019-11-24,2019,11,24,11,e0s430,[R] [1908.00156] Deep Gaussian networks for function approximation on data defined manifolds,https://www.reddit.com/r/MachineLearning/comments/e0s430/r_190800156_deep_gaussian_networks_for_function/,AforAnonymous,1574561325,,2,1
1213,2019-11-24,2019,11,24,13,e0tj9a,"Help me out, these are all outside of my girlfriends house. They also float in her pool so some information would be fantastic ",https://www.reddit.com/r/MachineLearning/comments/e0tj9a/help_me_out_these_are_all_outside_of_my/,Au91700,1574568466,,0,1
1214,2019-11-24,2019,11,24,14,e0ufoe,Considering renting out GPU's for Machine Learning. But I think my CPU's are too weak.,https://www.reddit.com/r/MachineLearning/comments/e0ufoe/considering_renting_out_gpus_for_machine_learning/,fatfucksandalcohole,1574573705,[removed],0,1
1215,2019-11-24,2019,11,24,14,e0uj7a,Mnist dataset https error,https://www.reddit.com/r/MachineLearning/comments/e0uj7a/mnist_dataset_https_error/,seventhuser,1574574294,[removed],0,1
1216,2019-11-24,2019,11,24,15,e0up0p,[R] Reimplementation of Hyperspherical Prototype Networks (NeurIPS 2019),https://www.reddit.com/r/MachineLearning/comments/e0up0p/r_reimplementation_of_hyperspherical_prototype/,ACTBRUH,1574575328,"Link to paper: https://arxiv.org/abs/1901.10514

Link to my reimplementation: https://github.com/Abhishaike/HyperProtoNetReproduce

This is a Pytorch reimplementation of the NeurIPS 2019 paper Hyperspherical Prototype Networks in Pytorch. This paper proposes an extension to Prototype Networks, in which the prototypes are placed a priori with large margin separation, and remain unchanged during the training/testing process of the model. The paper suggests that this extension allows for more flexible classification, regression, and joint multi-task training of regression/classification, and with higher accuracy compared to typical Prototype Networks.

This repo includes reproduced benchmarks for most of their datasets. Largely the same accuracy/error, but quite off on CIFAR-100 (not ImageNet-200 though for some reason), so it's possible this is an issue on my end.

I also found their use of SGD for prototype creation to be unusual, considering that, the way they phrased the prototype problem, it seems like a job more for a constrained optimization algorithm. Alongside the SGD implementation (which are used for the included benchmarks), I added in two other optimization algorithms, one unconstrained (BFGS) and one constrained (SLSQP). These didn't seem to change the results much.

This is my first reimplementation of a paper, so any critiques would be great!",5,1
1217,2019-11-24,2019,11,24,15,e0uwud,New Artificial Intelligence / Machine Learning Research by MIT is detecting serial hijackers who are manipulating internet traffic to eavesdrop or take company data.,https://www.reddit.com/r/MachineLearning/comments/e0uwud/new_artificial_intelligence_machine_learning/,itveterans,1574576720,[removed],0,1
1218,2019-11-24,2019,11,24,16,e0vbhq,[D] AISTATS 2020 Reviews,https://www.reddit.com/r/MachineLearning/comments/e0vbhq/d_aistats_2020_reviews/,donb1988,1574579576,"AISTATS 2020 reviews are marked for release on Nov 24, 2019. Here's a thread to discuss this year's reviews. Godspeed, everyone!",15,1
1219,2019-11-24,2019,11,24,17,e0vwu5,3 Ways to Encode Categorical Variables for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/e0vwu5/3_ways_to_encode_categorical_variables_for_deep/,RankLord,1574583868,,0,1
1220,2019-11-24,2019,11,24,18,e0w6wj,If people chose to use AI good in future,https://www.reddit.com/r/MachineLearning/comments/e0w6wj/if_people_chose_to_use_ai_good_in_future/,hybridpriest,1574586035,,0,1
1221,2019-11-24,2019,11,24,19,e0wygc,Where to learn,https://www.reddit.com/r/MachineLearning/comments/e0wygc/where_to_learn/,Jasasul,1574591775,"Hey guys, so I want to learn some machine learning just for fun, recommend me some guides/courses that are up to date and use python
thanks for ur time",0,1
1222,2019-11-24,2019,11,24,19,e0x56q,Need some advice for training BERT to classify multiple sentiments.[PROJECT],https://www.reddit.com/r/MachineLearning/comments/e0x56q/need_some_advice_for_training_bert_to_classify/,shstan,1574593167,"Basically, so far, I have been trying to train BERT on a very long document by cutting start, middle , and end sections of article so it could be fit into the limited input dimension of 512. However; the performance has been dismal for most of the time. So far, I am not sure if using LSTM+GRU was a better approach than this. But are there other ways to train it than just cutting up the article? When I googled for an alternative approach, I couldnt find much...",9,1
1223,2019-11-24,2019,11,24,20,e0xc8q,"Hello I am setting up a small discord server where we conceptualise, draw and discuss new cosmetics for the popular FPS, team fortress two, based off images generated by a neural network I am working on (link in comments).",https://www.reddit.com/r/MachineLearning/comments/e0xc8q/hello_i_am_setting_up_a_small_discord_server/,QUZANG_2,1574594554,,1,1
1224,2019-11-24,2019,11,24,21,e0y3qo,Key Machine Learning PreReq: Viewing Linear Algebra through the right lenses,https://www.reddit.com/r/MachineLearning/comments/e0y3qo/key_machine_learning_prereq_viewing_linear/,andrea_manero,1574599595,[removed],0,1
1225,2019-11-24,2019,11,24,23,e0yvok,[P] Hypertunity: a library for hyperparameter optimisation,https://www.reddit.com/r/MachineLearning/comments/e0yvok/p_hypertunity_a_library_for_hyperparameter/,gdikov,1574604250,"I would like to share my pet project, Hypertunity, a Python library for black-box hyperparameter optimisation. It's main features are:
 * Bayesian Optimisation using Gaussian process regression by wrapping [GPyOpt](https://sheffieldml.github.io/GPyOpt);
 * Native support for random and grid search;
 * Visualisation of the results in Tensorboard using the [HParams](https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams) plugin;
 * Scheduled, parallel execution of experiments using [joblib](https://joblib.readthedocs.io);
 * Also possible to schedule jobs on [Slurm](https://slurm.schedmd.com).

For the full set of features, check out the [docs](https://hypertunity.readthedocs.io) or the source code in the [Github repo](https://github.com/gdikov/hypertunity).

Your feedback is very much appreciated!",23,1
1226,2019-11-24,2019,11,24,23,e0z68p,Definition of Probabilistic classification,https://www.reddit.com/r/MachineLearning/comments/e0z68p/definition_of_probabilistic_classification/,ReptileCultist,1574605832,"Hello I'm searching for a definition of probabilistic classification. Now I know what probabilistic classification is but I need to have the definition or an explanation of it, for a citation in my thesis. I have looked through my books (Murphy, Bishop and Hastie). But I have not been able to find one. Does anyone know where to find a good citeworthy definition?",0,1
1227,2019-11-24,2019,11,24,23,e0z7xs,[Discussion] Hyperparameters for Word2Vec for SMS corpus...,https://www.reddit.com/r/MachineLearning/comments/e0z7xs/discussion_hyperparameters_for_word2vec_for_sms/,conradws,1574606067,"
Hey all, 

Working at a small startup, and we have extracted 33 million text messages from our users. We plan to create a model to classify different types of sms relevant to us. 

First step is to create a Word 2 Vector dictionary for EDA and clustering and possibly to use these embeddings for classification further down the line .

Just wanted some guidance about the hyperparameters for the gensim's Word2Vec. 

The corpus is 33 million sms, average sms length is 16 words and the vocab size is 1.5 million. 

I used the following hyperparameters and obtained decent results but just wanted to know if I'm doing anything wrong that could be hampering the model from performing even better:

Cbow, window = 4, vector size = 125,  iterations =10, workers = 5, min_count= 4.

Furthermore does anyone have any tips on how to evaluate the embeddings ( other than checking that the similarity for a small set of words makes sense) so that I can fine-tune these hyperparameters? 

And final question ( I promise) Would it possible or recomendable to take a pre trained Word2Vec model and improve on it by giving it the sms data so that it learns new words like slang and typos without losing its overall knowledge of the language? 

Thanks so much for your time in reading.",13,1
1228,2019-11-24,2019,11,24,23,e0z8id,5 Hacks to speed up your AI Training (Reinforcement Learning with Unity ML-Agents),https://www.reddit.com/r/MachineLearning/comments/e0z8id/5_hacks_to_speed_up_your_ai_training/,Realseppy,1574606142,,0,1
1229,2019-11-25,2019,11,25,1,e10b5x,[D] What happened to the thread on Taiwan and ICCV,https://www.reddit.com/r/MachineLearning/comments/e10b5x/d_what_happened_to_the_thread_on_taiwan_and_iccv/,arkady_red,1574611483,"As per subject, wasn't there a thread on that yesterday? I can't find it anymore. Was it mowed down by moderators?",205,1
1230,2019-11-25,2019,11,25,1,e10bz7,The Wolf of Silicon Valley - Data science is the new investment banking,https://www.reddit.com/r/MachineLearning/comments/e10bz7/the_wolf_of_silicon_valley_data_science_is_the/,stensool,1574611584,[removed],0,1
1231,2019-11-25,2019,11,25,1,e10e1h,[Discussion] The Wolf of Silicon Valley - Data science is the new investment banking,https://www.reddit.com/r/MachineLearning/comments/e10e1h/discussion_the_wolf_of_silicon_valley_data/,stensool,1574611850,"A satirical piece I wrote on the parallels between the machine learning engineers and wall street traders:

[https://towardsdatascience.com/the-wolf-of-silicon-valley-150e5f501216](https://towardsdatascience.com/the-wolf-of-silicon-valley-150e5f501216)",4,1
1232,2019-11-25,2019,11,25,1,e10e40,"[P] A Chess/Go/Shogi model that passes the Turing test, how do I build an imitation learning model that incorporates some kind of lookahead algorithm?",https://www.reddit.com/r/MachineLearning/comments/e10e40/p_a_chessgoshogi_model_that_passes_the_turing/,Pawngrubber,1574611861,"I want to build a model for Chess/Go/Shogi that is trained and tested on real players, and I want it to pass the Turing test.  I don't want my model to play the best move in a position, I want it to play the move that a person would play (of a certain strength, time control, etc..).  

&amp;#x200B;

It's easy to make this a classification problem and train a CNN on a one-hot encoded policy of actual moves played.  The only problem is, without some kind of look-ahead algorithm (MCTS for example) the model fails to learn sequences that require multiple moves, such as tactics.  

&amp;#x200B;

However, current MCTS/alpha-beta/minimax models require evaluation of leaf nodes.  I don't have a way to shape the reward to an evaluation of a leaf node.  So my question: how would I incorporate a look-ahead algorithm in an imitation learning problem like this?",17,1
1233,2019-11-25,2019,11,25,1,e10eyh,Pretty clever using face as a controller,https://www.reddit.com/r/MachineLearning/comments/e10eyh/pretty_clever_using_face_as_a_controller/,planktonfun,1574611960,,0,1
1234,2019-11-25,2019,11,25,2,e116a2,"Training and Deploying a Multi-Label Image Classifier using PyTorch, Flask, ReactJS and Firebase data storage Part 1: Multi-Label Image Classification using PyTorch",https://www.reddit.com/r/MachineLearning/comments/e116a2/training_and_deploying_a_multilabel_image/,thevatsalsaglani,1574615193,,0,1
1235,2019-11-25,2019,11,25,2,e116cr,Error loading mnist dataset,https://www.reddit.com/r/MachineLearning/comments/e116cr/error_loading_mnist_dataset/,seventhuser,1574615201,,0,1
1236,2019-11-25,2019,11,25,2,e11t34,"[P] PySNN: Spiking Neural Network framework, built on top of PyTorch",https://www.reddit.com/r/MachineLearning/comments/e11t34/p_pysnn_spiking_neural_network_framework_built_on/,DontShowYourBack,1574617926,"Hi everyone!

Recently a friend and I have been working on a new Python library for machine learning with Spiking Neural Networks (SNNs), called PySNN, which is built on top of PyTorch. We feel it is time to share it with more people, and hopefully get good feedback and contributions:)!

Our goal for  PySNN was to make a truly modular framework for machine learning with SNNs, while staying as close to PyTorch as possible. All of the existing frameworks either operate more like simulators for neuroscientific research, or use relatively fixed network and training/evaluation loop designs. PySNN, on the other hand, consists of building blocks for neurons, connections, and learning rules which the user can combine in their desired way. It even allows for mixing learning rules or training only specific parts of the network.

Furthermore, since PySNN consists of just the basic elements, the framework is lightweight and allows for easy extension. Because of its tight integration with PyTorch it fully supports GPU acceleration, batching of samples, and supports tools like the jit compiler and graph tracing for TensorBoard.

There are still many improvements and extensions that can be made, so feel free to have a look and send out a pull request! We will be very active in helping with any issues! [https://github.com/BasBuller/PySNN](https://github.com/BasBuller/PySNN)

We are looking forward to your comments and suggestions!:)",22,1
1237,2019-11-25,2019,11,25,2,e11u6o,Machine Intelligence: An Evolved Version of ML and AI with Better Prioritization,https://www.reddit.com/r/MachineLearning/comments/e11u6o/machine_intelligence_an_evolved_version_of_ml_and/,analyticsinsight,1574618046,,0,1
1238,2019-11-25,2019,11,25,3,e123b5,Googles deepfake detector is garbage. Here is one that actually works.,https://www.reddit.com/r/MachineLearning/comments/e123b5/googles_deepfake_detector_is_garbage_here_is_one/,Mkilli,1574619131,,0,1
1239,2019-11-25,2019,11,25,3,e12dnn,"Parallel attention in one module achieves SoTA performance on three main NMT datasets. (Code, models are all available)",https://www.reddit.com/r/MachineLearning/comments/e12dnn/parallel_attention_in_one_module_achieves_sota/,stopwind,1574620340,[removed],0,1
1240,2019-11-25,2019,11,25,3,e12eel,[D] What do you see as the most promising directions for reducing sample inefficiency in reinforcement learning?,https://www.reddit.com/r/MachineLearning/comments/e12eel/d_what_do_you_see_as_the_most_promising/,p6m_lattice,1574620420,"I often read from ML researchers, but more from computational cognitive scientists, that humans are able to generalize patterns from only a few data points or use ""rich, informative priors"" even as children, and how that is very important for us as cognitive beings that sets us apart from the current neural network approaches to RL used today. 

I'm also not entirely convinced that the current neural net paradigm with the McCullochPitts-esque neurons is ever going to become sample efficient enough for real-world reinforcement learning tasks. It seems like despite our best efforts to increase sample efficiency in NN techniques, the most impressive results still use hundreds of thousands or more simulations/data points that could be infeasible to implement for any sufficiently complex real-world environments.

That being said, what approaches are you most excited for in reducing sample efficiency in reinforcement learning or in neural network techniques in general?",22,1
1241,2019-11-25,2019,11,25,3,e12gaw,In the future a good application of AI,https://www.reddit.com/r/MachineLearning/comments/e12gaw/in_the_future_a_good_application_of_ai/,hybridpriest,1574620622,,0,1
1242,2019-11-25,2019,11,25,3,e12prl,Applying neural networks help,https://www.reddit.com/r/MachineLearning/comments/e12prl/applying_neural_networks_help/,polydorou,1574621663,[removed],0,1
1243,2019-11-25,2019,11,25,4,e12xuc,"An open-source, fast, simple yet effective Neural Architecture Search (NAS) pipeline",https://www.reddit.com/r/MachineLearning/comments/e12xuc/an_opensource_fast_simple_yet_effective_neural/,OldeElk,1574622540,,0,1
1244,2019-11-25,2019,11,25,4,e12yt3,A simple module consistently outperforms self-attention and Transformer model on main NMT datasets with SoTA performance.,https://www.reddit.com/r/MachineLearning/comments/e12yt3/a_simple_module_consistently_outperforms/,stopwind,1574622651,[removed],0,1
1245,2019-11-25,2019,11,25,4,e135bt,"An open-sourced, fast, simple yet effective NAS pipeline",https://www.reddit.com/r/MachineLearning/comments/e135bt/an_opensourced_fast_simple_yet_effective_nas/,OldeElk,1574623377,[removed],0,1
1246,2019-11-25,2019,11,25,4,e13az0,Block coordinate / layer-by-layer optimization for deep networks,https://www.reddit.com/r/MachineLearning/comments/e13az0/block_coordinate_layerbylayer_optimization_for/,fjanoos,1574624003,[removed],0,1
1247,2019-11-25,2019,11,25,4,e13azm,"An open-source, fast, simple yet effective Neural Architecture Search (NAS) pipeline",https://www.reddit.com/r/MachineLearning/comments/e13azm/an_opensource_fast_simple_yet_effective_neural/,OldeElk,1574624005,[removed],0,1
1248,2019-11-25,2019,11,25,5,e13qhb,[R] A simple module consistently outperforms self-attention and Transformer model on main NMT datasets with SoTA performance.,https://www.reddit.com/r/MachineLearning/comments/e13qhb/r_a_simple_module_consistently_outperforms/,stopwind,1574625759,"  

In the paper: **MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning**

They delve into three **questions** in sequence to sequence learning:

1. Is attention alone good enough
2. Is parallel   representation learning applicable to sequence data and tasks?
3. How to design  a module that combines both inductive bias of convolution and  self-attention

They find that there are shortcomings in stand-alone self-attention, and present a new module that maps the input to the hidden space and performs the three operations of self-attention, convolution and nonlinearity in parallel, simply stacking this module outperforms all previous models including Transformer (Vasvani [et al](https://et.al/)., 2017) on main NMT tasks under standard setting.

 **Key features:**

* First successfully combine convolution and self-attention in one module for      sequence tasks by the proposed shared projection
* SOTA on three  main translation datasets, including WMT14 En-Fr, WMT14 En-De and IWSLT14  De-En
* Parallel   learn sequence representations and thus have potential for acceleration

**Quick links:**

[Arxiv](https://arxiv.org/abs/1911.09483) : pdf;

[Github](https://github.com/lancopku/MUSE) : Code, pretrained models, instructions for training are all available.

 

**Main results:**

   
 

https://preview.redd.it/2qkne32qwo041.png?width=554&amp;format=png&amp;auto=webp&amp;s=f7ed2973ebdd434f2982a2ec546f65ca635ff529

&amp;#x200B;

https://preview.redd.it/cdx3sc5vwo041.png?width=554&amp;format=png&amp;auto=webp&amp;s=611d277629129f5734119321b4466f1bd4781f03

**Abstract:**

In sequence to sequence learning, the self-attention mechanism proves to be highly effective, and achieves significant improvements in many tasks. However, the self-attention mechanism is not without its own flaws. Although self-attention can model extremely long dependencies, the attention in deep layers tends to overconcentrate on a single token, leading to insufficient use of local information and difficultly in representing long sequences. In this work, we explore parallel multi-scale representation learning on sequence data, striving to capture both long-range and short-range language structures. To this end, we propose the Parallel MUlti-Scale attEntion (MUSE) and MUSE-simple. MUSE-simple contains the basic idea of parallel multi-scale sequence representation learning, and it encodes the sequence in parallel, in terms of different scales with the help from self-attention, and pointwise transformation. MUSE builds on MUSE-simple and explores combining convolution and self-attention for learning sequence representations from more different scales. We focus on machine translation and the proposed approach achieves substantial performance improvements over Transformer, especially on long sequences. More importantly, we find that although conceptually simple, its success in practice requires intricate considerations, and the multi-scale attention must build on unified semantic space. Under common setting, the proposed model achieves substantial performance and outperforms all previous models on three main machine translation tasks. In addition, MUSE has potential for accelerating inference due to its parallelism.",48,1
1249,2019-11-25,2019,11,25,5,e149u6,[D] anybody know an implementation of fchollet's 2016 paper on label embeddings?,https://www.reddit.com/r/MachineLearning/comments/e149u6/d_anybody_know_an_implementation_of_fchollets/,daanvdn,1574627973,"This is the paper: Information-theoretical label embeddings for large-scale image classification

https://arxiv.org/abs/1607.05691",2,1
1250,2019-11-25,2019,11,25,5,e14hjb,A comparison between IOTA 575 million market cap and Streamr 12 million market cap protocol with HP and Bentley Partnered with Streamr protocol for the Data Economy internet of things,https://www.reddit.com/r/MachineLearning/comments/e14hjb/a_comparison_between_iota_575_million_market_cap/,007moonboundnxs,1574628807,,0,1
1251,2019-11-25,2019,11,25,7,e164p2,Ai,https://www.reddit.com/r/MachineLearning/comments/e164p2/ai/,hybridpriest,1574635154,,0,1
1252,2019-11-25,2019,11,25,8,e16ycd,What preprocessing method should I use?,https://www.reddit.com/r/MachineLearning/comments/e16ycd/what_preprocessing_method_should_i_use/,unkownhihi,1574638535,[removed],0,1
1253,2019-11-25,2019,11,25,8,e172vo,Decentralized Oracles IExec cloud computing platform partnered with IBM Intel Microsoft and Alibaba cloud,https://www.reddit.com/r/MachineLearning/comments/e172vo/decentralized_oracles_iexec_cloud_computing/,007moonboundnxs,1574639084,,0,1
1254,2019-11-25,2019,11,25,8,e178zb,How you can learn machine learning,https://www.reddit.com/r/MachineLearning/comments/e178zb/how_you_can_learn_machine_learning/,weihong95,1574639845,[removed],0,1
1255,2019-11-25,2019,11,25,11,e18vz2,Ways to extract skills from job description?,https://www.reddit.com/r/MachineLearning/comments/e18vz2/ways_to_extract_skills_from_job_description/,bigbuddha47,1574647203,[removed],0,1
1256,2019-11-25,2019,11,25,11,e191z6,[P] NBoost: Boost Elasticsearch Search Relevance by 80% with BERT,https://www.reddit.com/r/MachineLearning/comments/e191z6/p_nboost_boost_elasticsearch_search_relevance_by/,colethienes,1574647974,"Hi Everyone!

New to reddit, but I'd like to share a project I've been working on called [NBoost](https://github.com/koursaros-ai/nboost). It's essentially a proxy for search APIs (e.g. Elasticsearch) that reranks search results using finetuned models (e.g. BERT).

Check out our [medium article](https://medium.com/koursaros-ai/boost-search-api-performance-e-g-410868e82b22) or [github](https://github.com/koursaros-ai/nboost) to learn more!

It's main features include:
- Easy, non-invasive integration with Elasticsearch and potentially other search APIs.
- Finetuned models are plugins (you can swap them in and out).
- Fast and scaleable (written at the lowest level possible)",10,1
1257,2019-11-25,2019,11,25,13,e1apgs,How can we amend the bad cases in deep learning effectively ?,https://www.reddit.com/r/MachineLearning/comments/e1apgs/how_can_we_amend_the_bad_cases_in_deep_learning/,IseezI,1574655903,[removed],0,1
1258,2019-11-25,2019,11,25,13,e1auh7,Suggestions for TextClassification with limited dataset,https://www.reddit.com/r/MachineLearning/comments/e1auh7/suggestions_for_textclassification_with_limited/,Abhijith_pk,1574656663,[removed],0,1
1259,2019-11-25,2019,11,25,14,e1bf7s,GPT-2 just brought tears to my eyes. Two Poems Generated By OpenAI's GPT-2,https://www.reddit.com/r/MachineLearning/comments/e1bf7s/gpt2_just_brought_tears_to_my_eyes_two_poems/,eveFromKarmaFm,1574659883,[removed],0,1
1260,2019-11-25,2019,11,25,14,e1boqp,Why do NNs have a difficult time with the unit circle and the modulo operation?,https://www.reddit.com/r/MachineLearning/comments/e1boqp/why_do_nns_have_a_difficult_time_with_the_unit/,weelamb,1574661426,[removed],0,1
1261,2019-11-25,2019,11,25,14,e1bpav,How to improve Object Detection accuracy?,https://www.reddit.com/r/MachineLearning/comments/e1bpav/how_to_improve_object_detection_accuracy/,aditya_mangalampalli,1574661513,[removed],0,1
1262,2019-11-25,2019,11,25,15,e1c1ey,How does a learned prior handle variable-size latent representations in a convolutional VAE?,https://www.reddit.com/r/MachineLearning/comments/e1c1ey/how_does_a_learned_prior_handle_variablesize/,Yeebster,1574663524,[removed],0,1
1263,2019-11-25,2019,11,25,16,e1cc68,Life Insurance Companies Now Have Access to Your Social Media Content,https://www.reddit.com/r/MachineLearning/comments/e1cc68/life_insurance_companies_now_have_access_to_your/,tiaraspies,1574665377,[removed],0,1
1264,2019-11-25,2019,11,25,16,e1ccwh,Natural Language Processing is redefining human interaction,https://www.reddit.com/r/MachineLearning/comments/e1ccwh/natural_language_processing_is_redefining_human/,day1technologies,1574665497,,0,1
1265,2019-11-25,2019,11,25,17,e1d447,"[D] ImageNet classification training full-resolution, no crop no resize.",https://www.reddit.com/r/MachineLearning/comments/e1d447/d_imagenet_classification_training_fullresolution/,tsauri,1574670405,"Are there papers that do so?
Since we already have adaptive pooling technques such as global average pooling that support variable resolution?

Random fixed size crop and resize seems hacky",14,1
1266,2019-11-25,2019,11,25,17,e1d8ym,Top most Real Life Examples Of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/e1d8ym/top_most_real_life_examples_of_machine_learning/,onlineit3,1574671320,,0,1
1267,2019-11-25,2019,11,25,17,e1de73,Question: Loss Function vs Cost Function.,https://www.reddit.com/r/MachineLearning/comments/e1de73/question_loss_function_vs_cost_function/,debayon,1574672346,[removed],0,1
1268,2019-11-25,2019,11,25,18,e1dfk5,Social BI: An Important Consideration of Self-Serve BI Tools!,https://www.reddit.com/r/MachineLearning/comments/e1dfk5/social_bi_an_important_consideration_of_selfserve/,ElegantMicroWebIndia,1574672589,,0,1
1269,2019-11-25,2019,11,25,18,e1dh7f,Abstract art dataset?,https://www.reddit.com/r/MachineLearning/comments/e1dh7f/abstract_art_dataset/,JoeNg_03,1574672899,[removed],0,1
1270,2019-11-25,2019,11,25,18,e1disp,Used Manual Lathes | HI-Tech Machinery Inc,https://www.reddit.com/r/MachineLearning/comments/e1disp/used_manual_lathes_hitech_machinery_inc/,juliakroberts,1574673170,,0,1
1271,2019-11-25,2019,11,25,18,e1dm5i,Used Cylindrical Grinding Machines and Cylindrical Grinder,https://www.reddit.com/r/MachineLearning/comments/e1dm5i/used_cylindrical_grinding_machines_and/,juliakroberts,1574673788,,0,1
1272,2019-11-25,2019,11,25,18,e1dpn9,[R] DeepFovea: Neural Reconstruction for Foveated Rendering and Video Compression using Learned Statistics of Natural Videos (SIGGRAPH Asia 2019),https://www.reddit.com/r/MachineLearning/comments/e1dpn9/r_deepfovea_neural_reconstruction_for_foveated/,hardmaru,1574674448,"I thought this [project](https://research.fb.com/publications/deepfovea-neural-reconstruction-for-foveated-rendering-and-video-compression-using-learned-statistics-of-natural-videos/) from FB Research is a really cool work:

*Abstract*

In order to provide an immersive visual experience, modern displays require head mounting, high image resolution, low latency, as well as high refresh rate. This poses a challenging computational problem. On the other hand, the human visual system can consume only a tiny fraction of this video stream due to the drastic acuity loss in the peripheral vision. Foveated rendering and compression can save computations by reducing the image quality in the peripheral vision. However, this can cause noticeable artifacts in the periphery, or, if done conservatively, would provide only modest savings. In this work, we explore a novel foveated reconstruction method that employs the recent advances in generative adversarial neural networks. We reconstruct a plausible peripheral video from a small fraction of pixels provided every frame. The reconstruction is done by finding the closest matching video to this sparse input stream of pixels on the learned manifold of natural videos. Our method is more efficient than the state-of-the-art foveated rendering, while providing the visual experience with no noticeable quality degradation. We conducted a user study to validate our reconstruction method and compare it against existing foveated rendering and video compression techniques. Our method is fast enough to drive gaze-contingent head-mounted displays in real time on modern hardware. We plan to publish the trained network to establish a new quality bar for foveated rendering and compression as well as encourage follow-up research.

Project page / paper / video / code: https://research.fb.com/publications/deepfovea-neural-reconstruction-for-foveated-rendering-and-video-compression-using-learned-statistics-of-natural-videos/",4,1
1273,2019-11-25,2019,11,25,18,e1dt42,Looking for Used Surface Grinders? - Hi-Tech Machinery Inc,https://www.reddit.com/r/MachineLearning/comments/e1dt42/looking_for_used_surface_grinders_hitech/,juliakroberts,1574675073,,0,1
1274,2019-11-25,2019,11,25,18,e1dwl2,Deep Learning for Programmers Ebook,https://www.reddit.com/r/MachineLearning/comments/e1dwl2/deep_learning_for_programmers_ebook/,RubiksCodeNMZ,1574675720,,0,1
1275,2019-11-25,2019,11,25,18,e1dxph,Used Gear Machines and Shaper - Hi-Tech Machinery Inc,https://www.reddit.com/r/MachineLearning/comments/e1dxph/used_gear_machines_and_shaper_hitech_machinery_inc/,juliakroberts,1574675943,,0,1
1276,2019-11-25,2019,11,25,18,e1dxvl,[D] Image synthesis before GANs,https://www.reddit.com/r/MachineLearning/comments/e1dxvl/d_image_synthesis_before_gans/,WoahCanyonero,1574675971,"I'm writing my master's thesis on image generation and was wondering: which other methods were/are used to synthesize images, aside from GANs? Any time I look up ""image synthesis"" I only find GAN tutorials, and excluding GANs seems to bring up photon scanning.",16,1
1277,2019-11-25,2019,11,25,19,e1dzh3,Facebook helping the emerging and innovative startup to grow. Great initiative by Facebook India innovation Accelerator. Startups centric to showcased AI for Social Good,https://www.reddit.com/r/MachineLearning/comments/e1dzh3/facebook_helping_the_emerging_and_innovative/,Social_Good,1574676253,,0,1
1278,2019-11-25,2019,11,25,19,e1e0gu,What is a data catalog and why should you care,https://www.reddit.com/r/MachineLearning/comments/e1e0gu/what_is_a_data_catalog_and_why_should_you_care/,knlph,1574676420,,0,1
1279,2019-11-25,2019,11,25,19,e1e182,Used Radial Arm Drill Press for Sale - Hi-Tech Machinery Inc,https://www.reddit.com/r/MachineLearning/comments/e1e182/used_radial_arm_drill_press_for_sale_hitech/,juliakroberts,1574676560,,0,1
1280,2019-11-25,2019,11,25,19,e1e8nx,Importance of Artificial Intelligence in Customer Service,https://www.reddit.com/r/MachineLearning/comments/e1e8nx/importance_of_artificial_intelligence_in_customer/,Countants123,1574677879,,0,1
1281,2019-11-25,2019,11,25,19,e1ecry,Use AI to turn into landscape into natural looking environment,https://www.reddit.com/r/MachineLearning/comments/e1ecry/use_ai_to_turn_into_landscape_into_natural/,AlexaPomata,1574678618,"Hi,

I wonder why we are still trying to mimic photorealistic world by counting every reflection, polygon, tracing every ray and so on. Shouldn't it be done in such manner that AI is just doing the job basing on photos and low polygon input like here https://assetstore.unity.com/packages/3d/characters/animals/poly-art-forest-set-128568
Also all other games like Zelda BOTW, Team Fortress 2 or even Fortnite could be easily turned by AI into photorealistic env. Shouldn't we start thinking about doing AI accelerators (like first 3dfx cards) for enriching low polygonic world's generated easily by most commodity hardware? I guess even ray tracing could be made by ML. I believed that future belongs to generating world by AI not by tricky mathematic graphics algorithms. Especially that in future it is easier to go from such trained networks into environment where instead of heaving an output on display, the output would be ""drawn"" directly in human brain through neural-connectivity. Also AI is able to properly handle cases where object is moving fast or turning around.

Cheers,
Alexa",0,1
1282,2019-11-25,2019,11,25,19,e1eht3,Use AI to turn low poly world into photorealistic scenarios,https://www.reddit.com/r/MachineLearning/comments/e1eht3/use_ai_to_turn_low_poly_world_into_photorealistic/,AlexaPomata,1574679537,"Hi,

I wonder why we are still trying to mimic photorealistic world by counting every reflection, polygon, tracing every ray and so on. Shouldn't it be done in such manner that AI is just doing the job basing on photos and low polygon input like here https://assetstore.unity.com/packages/3d/characters/animals/poly-art-forest-set-128568
Also all other games like Zelda BOTW, Team Fortress 2 or even Fortnite could be easily turned by AI into photorealistic env. Shouldn't we start thinking about doing AI accelerators (like first 3dfx cards) for enriching low polygonic world's generated easily by most commodity hardware? I guess even ray tracing could be made by ML. I believed that future belongs to generating world by AI not by tricky mathematic graphics algorithms. Especially that in future it is easier to go from such trained networks into environment where instead of heaving an output on display, the output would be ""drawn"" directly in human brain through neural-connectivity. Also AI is able to properly handle cases where object is moving fast or turning around.

Cheers,
Alexa",0,1
1283,2019-11-25,2019,11,25,19,e1ehxg,Optimizing the Code with Machine Learning for Software Development,https://www.reddit.com/r/MachineLearning/comments/e1ehxg/optimizing_the_code_with_machine_learning_for/,Kunal-sharma,1574679561,,0,1
1284,2019-11-25,2019,11,25,19,e1ehz1,[1911.09723] Fast Sparse ConvNets,https://www.reddit.com/r/MachineLearning/comments/e1ehz1/191109723_fast_sparse_convnets/,ekelsen,1574679570,,11,1
1285,2019-11-25,2019,11,25,20,e1eksy,Used Milling Machine for Sale: Tips to Purchase,https://www.reddit.com/r/MachineLearning/comments/e1eksy/used_milling_machine_for_sale_tips_to_purchase/,juliakroberts,1574680060,,0,1
1286,2019-11-25,2019,11,25,20,e1elhy,[D] Use AI to turn low poly world into photorealistic scenarios,https://www.reddit.com/r/MachineLearning/comments/e1elhy/d_use_ai_to_turn_low_poly_world_into/,AlexaPomata,1574680181,"Hi,

I wonder why we are still trying to mimic photorealistic world by counting every reflection, polygon, tracing every ray and so on. Shouldn't it be done in such manner that AI is just doing the job basing on photos and low polygon input like here https://assetstore.unity.com/packages/3d/characters/animals/poly-art-forest-set-128568
Also all other games like Zelda BOTW, Team Fortress 2 or even Fortnite could be easily turned by AI into photorealistic env. Shouldn't we start thinking about doing AI accelerators (like first 3dfx cards) for enriching low polygonic world's generated easily by most commodity hardware? I guess even ray tracing could be made by ML. I believed that future belongs to generating world by AI not by tricky mathematic graphics algorithms. Especially that in future it is easier to go from such trained networks into environment where instead of heaving an output on display, the output would be ""drawn"" directly in human brain through neural-connectivity. Also AI is able to properly handle cases where object is moving fast or turning around.

Cheers,
Alexa",26,1
1287,2019-11-25,2019,11,25,20,e1eocc,Tips to Buy Used CNC Lathe,https://www.reddit.com/r/MachineLearning/comments/e1eocc/tips_to_buy_used_cnc_lathe/,juliakroberts,1574680671,,0,1
1288,2019-11-25,2019,11,25,20,e1eval,Uncertainty: Comparing Bayesian Deep Neural Nets with Monte Carlo Dropout,https://www.reddit.com/r/MachineLearning/comments/e1eval/uncertainty_comparing_bayesian_deep_neural_nets/,skeering,1574681859,[removed],0,1
1289,2019-11-25,2019,11,25,20,e1f0dx,Is there a tool that suggest ML methods/approaches based on the problem?,https://www.reddit.com/r/MachineLearning/comments/e1f0dx/is_there_a_tool_that_suggest_ml_methodsapproaches/,sayezz,1574682756,[removed],1,1
1290,2019-11-25,2019,11,25,21,e1fhwg,PCB cleaning machine - Southern Machinery,https://www.reddit.com/r/MachineLearning/comments/e1fhwg/pcb_cleaning_machine_southern_machinery/,smthelp1,1574685530,,0,1
1291,2019-11-25,2019,11,25,21,e1fp0d,[P] How can I build this simple text-based ML tool?,https://www.reddit.com/r/MachineLearning/comments/e1fp0d/p_how_can_i_build_this_simple_textbased_ml_tool/,ventura__highway,1574686598,"Hello everyone!

I work with spreadsheets a lot, doing tasks manually that are just a bit too complex for rules, but I believe they certainly fall into what ML can handle. In a nutshell, I spend 2+ hours a day going through company names, removing legal terms like ""LLC"" or ""Limited"", and humanizing them. 

For instance, I have a spreadsheet with company names and emails.

|Company Name|Email Address|
|:-|:-|
|Concur Recruitment Limited - 02476 668 204|sconvery@concurengineering.co.uk|
|Confluent Technology Group|mark.anderson@confluentgroup.com|
|Construction Maintenance and Allied Workers|donmelanson@cmaw.ca|

These would become (currently by hand):

|Company Name|Email Address|
|:-|:-|
|Concur Engineering|sconvery@concurengineering.co.uk|
|Confluent|mark.anderson@confluentgroup.com|
|CMAW|donmelanson@cmaw.ca|

What we're doing here is:

1. Shorting names to their essence
2. Removing legal terms and words
3. Looking at domain names (in email addresses) as a clue for the ""most human name""

Now, I very well believe this is something Google Cloud has capabilities for. Given the lack of programming involved with Google Cloud ML (and its potential integration with Google Sheets), I'd imagine it's the best vehicle for this tool.

Some questions before I embark upon this journey:

1. Would your recommend I use Google Cloud ML or another tool?
2. How much data would you imagine would be necessary to train this tool? (uncleaned spreadsheets and cleaned spreadsheets)
3. Am I critically misunderstanding something here? This is pretty much my first time practically applying ML.

Thank you very much for all your help!",4,1
1292,2019-11-25,2019,11,25,22,e1fr5m,"Cisco, SingularityNET to Decentralize Artificial Intelligence via Blockchain - CoinDesk",https://www.reddit.com/r/MachineLearning/comments/e1fr5m/cisco_singularitynet_to_decentralize_artificial/,007moonboundnxs,1574686907,,0,1
1293,2019-11-25,2019,11,25,22,e1fu5r,[Blog Post] The fusion of physics simulation and machine learning,https://www.reddit.com/r/MachineLearning/comments/e1fu5r/blog_post_the_fusion_of_physics_simulation_and/,akira_AI,1574687340,[removed],0,1
1294,2019-11-25,2019,11,25,22,e1gf1f,Endorsement for arXiv,https://www.reddit.com/r/MachineLearning/comments/e1gf1f/endorsement_for_arxiv/,agent--zero,1574690390,[removed],0,1
1295,2019-11-25,2019,11,25,23,e1gfln,Choosing application area for ML: Finance vs Security vs Biology,https://www.reddit.com/r/MachineLearning/comments/e1gfln/choosing_application_area_for_ml_finance_vs/,xepo3abp,1574690467,[removed],0,1
1296,2019-11-25,2019,11,25,23,e1grh8,Normalizing flows for cyclical data,https://www.reddit.com/r/MachineLearning/comments/e1grh8/normalizing_flows_for_cyclical_data/,dineNshine,1574692088,[removed],1,1
1297,2019-11-25,2019,11,25,23,e1gs32,Dataset of Symptoms,https://www.reddit.com/r/MachineLearning/comments/e1gs32/dataset_of_symptoms/,njmagill,1574692175,[removed],0,1
1298,2019-11-25,2019,11,25,23,e1gssh,[D] how do you expect ML to transition over to safety critical systems?,https://www.reddit.com/r/MachineLearning/comments/e1gssh/d_how_do_you_expect_ml_to_transition_over_to/,nocomment_95,1574692285,"First off I am not an ML engineer. I am am embedded software engineer working mostly in safety critical systems. So if there are some dumb assumptions here don't crucify me. One of the biggest things that strikes me about ML is it's black box nature. We can't ask the machine how it made a descision, in fact I've heard claims that we shouldn't because it would inject human bias into the system. For things like data scraping and image recognition that seems fine, but I can't imagine having a conversation at my work go like this:

""X failed, people died. Go figure out how and fix it.

Sorry boss I can retrain the model with this new outcome but I can't tell you why it broke or guarantee to any degree of certainty it won't happen again""

That just wouldn't fly. Is there something I'm missing?",45,1
1299,2019-11-25,2019,11,25,23,e1gw1b,Upgrade from Windows 7 to Windows 10 Without Data Loss,https://www.reddit.com/r/MachineLearning/comments/e1gw1b/upgrade_from_windows_7_to_windows_10_without_data/,gaurav26t,1574692713,[removed],1,1
1300,2019-11-26,2019,11,26,0,e1h6wr,"Elon Musk Claims To Solve A Disease Which Is Actually Not A Disease, He Claims Neuralink Can Solve Autism And Schizophrenia",https://www.reddit.com/r/MachineLearning/comments/e1h6wr/elon_musk_claims_to_solve_a_disease_which_is/,navin49,1574694119,[removed],0,1
1301,2019-11-26,2019,11,26,0,e1h8n3,NLP News Cypher | 11.24.19,https://www.reddit.com/r/MachineLearning/comments/e1h8n3/nlp_news_cypher_112419/,Quantum_Stat,1574694321,[removed],0,1
1302,2019-11-26,2019,11,26,1,e1i87l,A Question about ASR in a RNN with CTC ( DeepSpeech),https://www.reddit.com/r/MachineLearning/comments/e1i87l/a_question_about_asr_in_a_rnn_with_ctc_deepspeech/,Johannes8,1574698572,[removed],0,1
1303,2019-11-26,2019,11,26,1,e1ib1e,First article I have ever written *nervous* Feedback is welcomed!,https://www.reddit.com/r/MachineLearning/comments/e1ib1e/first_article_i_have_ever_written_nervous/,thenabzter,1574698894,"Hi everyone!

I've just written my first ever article based on the different terms revolving around the topic of Data Science. It is more of an introductory article explaining what each term means (thought I would start with the basics).

Here's the link: [https://medium.com/@n.moukayed/demystifying-buzzwords-the-relationship-between-data-science-artificial-intelligence-machine-54b148749d13](https://medium.com/@n.moukayed/demystifying-buzzwords-the-relationship-between-data-science-artificial-intelligence-machine-54b148749d13)

Let me know what you all think. :)",0,1
1304,2019-11-26,2019,11,26,3,e1jvbd,Deep Learning Based Seed Quality Tester,https://www.reddit.com/r/MachineLearning/comments/e1jvbd/deep_learning_based_seed_quality_tester/,zom8ie99,1574704988,"Dear researchers,

I have taken a step in merging of Deep Learning with Agriculture and Industry with this first conference paper. Please take a look at it.

Link: [https://www.researchgate.net/publication/337485695\_DEEP\_LEARNING\_BASED\_SEED\_QUALITY\_TESTER](https://www.researchgate.net/publication/337485695_DEEP_LEARNING_BASED_SEED_QUALITY_TESTER)

Your feedback is highly appreciated.

Thank you",0,1
1305,2019-11-26,2019,11,26,3,e1jxin,[P] Webpage Data Extraction using Image Classification and Object Detection,https://www.reddit.com/r/MachineLearning/comments/e1jxin/p_webpage_data_extraction_using_image/,JsonPun,1574705207,"I am working on creating something that can detect and ideally extract information from a job posting. 

I have some questions around the data I am using. I currently crawl websites and take screenshots of their career pages. These screenshots vary in dimensions due to the length of the website. 

Disclaimer, I am not a ML Pro. I am self taught everything and currently using Google's AutoML Services for training my model. 

My Questions: 

1. Should I use these long/large images? Or is it better to cut them in half and then feed it to the AI. With the large images when I zoom in I can see everything fine for labeling. When not zoomed in, it can be hard to make things out.
2. How small should labels be? Google allows the smallest to be 8 pixels by 8 pixels. If they can be big I can use the large images and just zoom in? 
3. Is there a way to give context to the classifier/object detector? I realized when I evaluate a job posting I get context from the url and other words on the page that it doesn't get since it only sees a screenshot.
4. Should I try to label every element on the page? if yes, In a high level way or granular? 
5. Any other hints or tips I should think about to solve this problem?

My Attempts/Approaches

**Attempt 1: Object Detection** 

My first attempt was to perform object detection on screen shots that were cut down to \~2,000 pixels. I then labeled most of the content on the page with labels like: Header, Footer, Section, Heading, SubHeading, Job Title, Job Posting, Paragraph, Section Heading, Section SubHeading. 

Results :   
**Total images:** 183  
**Test items:** 17  
**Total objects:** 244  
**Object to image avg:** 14.35  
**Precision:** 91.43% (Using a score threshold of 0.508)  
**Recall:** 13.11% (Using a score threshold of 0.508)  
**Average precision:** 0.171 at 0.5 IoU

**Conclusion:** Object detection needs many more images, also the labels I provided were not concrete enough. Looking back I found the definitions for certain things to be vague. For example I was using the label heading, subheading and job title. Well sometimes the heading is also a job title, but I would only mark it as job title. Thinking about it from the computers perspective how will it know a heading from a job title? There is not much there visually for it to grab onto. This lead me to cut the images down to a height of 2,000 pixels so I could see each element more clearly. 

The problem here is do I try to label every HTML element?

**Attempt 2 Object Classification**

My second try was to use image classification to determine if I was on a job posting page, then if true use another model to extract the data. 

My first model1 results  
**Total images:** 85  
**Test items:** 9  
**Precision:** 77.78%  
**Recall:** 77.78%

My second model2 results  
**Total images:** 484  
**Test items:**55  
**Precision:** 90.7%  
**Recall:** 70.91%

These results were more in-line with what I had thought. When looking at the overall page there over and over there becomes a familiar pattern with what a job posting looks like.

**Final Attempt - Object Detection:**  
I am now trying again with an object detection model, that is trained only on job posting's, I think this will do better as it only has 3 labels, Job Title, Job Location and Apply Button. I wanted to include a label for: Responsibilities, Qualifications, skills, bonus, ect... but came back to the fact that there is not much for it to grab onto...as I find these in the posting by reading.

Model currently in training...

**Final Notes**  
I believe the correct way for me to do this problem would be to train the AI on the html code, but I am using google's automl services so I dont know how/if that is possible. I was thinking about using/combining different types of data/techniques since there is information in the URL and code that I'm not leveraging. Perhaps apply NLP to the URLS? 

Thanks for checking out my project any thoughts are appreciated.",5,1
1306,2019-11-26,2019,11,26,3,e1jyyd,"[D] ""Deep"" Machine Learning",https://www.reddit.com/r/MachineLearning/comments/e1jyyd/d_deep_machine_learning/,Rioghasarig,1574705349,"So, I'm a big fan of Lex Fridman's deep learning podcast. A big ago I watched one he did with [Ian Goodfellow](https://www.youtube.com/watch?v=Z6rxFNMGdn0). 

At the start of the interview Goodfellow describes how deep learning methods are distinguished by the fact that it involves a bunch of computations done in sequence rather than in parallel. (You can watch the video to get a better idea of what he was talking about). 

Does anyone have any other examples of machine learning techniques that you feel fit his description of being deep? Just curious about this.",5,1
1307,2019-11-26,2019,11,26,3,e1jz62,Worlds First Social Media for the A.I. Industry and A.I. Community!,https://www.reddit.com/r/MachineLearning/comments/e1jz62/worlds_first_social_media_for_the_ai_industry_and/,dominic0627,1574705372,,0,1
1308,2019-11-26,2019,11,26,3,e1jzkz,[Discussion][D] Gradient norm tracking,https://www.reddit.com/r/MachineLearning/comments/e1jzkz/discussiond_gradient_norm_tracking/,pubertat,1574705416,"Are there any best practices on how one should track gradient norms during training? Surprisingly, I haven't been able to find much reliable information on it, except the classical [Glorot's paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf). 

My current approach is to track 2-norm of weights raw gradients. However, I don't have any practical intuition on which values should make me worried. Tracking the actual weight updates (e.g adjusted by Adam) makes make much more sense, but I haven't seen anyone doing so. 

A few words why am I concerned: I'm working on some exotic NN architecture for 3D, where different architecture choices implicate gradient behavior drastically, up to blow up.",3,1
1309,2019-11-26,2019,11,26,3,e1k092,[R][P] Talking Head Anime from a Single Image,https://www.reddit.com/r/MachineLearning/comments/e1k092/rp_talking_head_anime_from_a_single_image/,pramook,1574705488,"I trained a network to animate faces of anime characters. The input is an image of the character looking straight at the viewer and a pose, specified by 6 numbers. The output is another image of the character with the face posed accordingly.

![video](5h95d7fzfv041 ""What the network can do in a nutshell."")

I created two tools with this network.

* One that changes facial poses by GUI manipulation:  [https://www.youtube.com/watch?v=kMQCERkTdO0](https://www.youtube.com/watch?v=kMQCERkTdO0) 
* One that reads a webcam feed and make a character imitates the user's facial movement:  [https://www.youtube.com/watch?v=T1Gp-RxFZwU](https://www.youtube.com/watch?v=T1Gp-RxFZwU) 

Using a face tracker, I could transfer human face movements from existing videos to anime characters. Here are some characters impersonating President Obama:

![video](jqb6eziwgv041)

The approach I took is to combine two previous works. The first is the [Pumarola et al.'s 2018 GANimation paper](https://www.albertpumarola.com/research/GANimation/index.html), which I use to change the facial features (closing eyes and mouth, in particular). The second is  [Zhou et al.'s 2016 object rotation by appearance flow paper](https://arxiv.org/abs/1605.03557), which I use to rotate the face. I generated a new dataset by rendering 8,000 downloadable 3D models of anime characters. 

You can find out more about the project at [https://pkhungurn.github.io/talking-head-anime/](https://pkhungurn.github.io/talking-head-anime/).",35,1
1310,2019-11-26,2019,11,26,3,e1k0en,PCIe Gen 4 for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/e1k0en/pcie_gen_4_for_deep_learning/,boba_tea_life,1574705508,[removed],0,1
1311,2019-11-26,2019,11,26,3,e1k81s,[D] Artificial Life for AI People,https://www.reddit.com/r/MachineLearning/comments/e1k81s/d_artificial_life_for_ai_people/,weiqiplayer,1574706320,"AI asks fundamental questions about the nature of intelligence, but what about understanding life itself? Sina gives an overview of Artificial Life for AI people.

&amp;#x200B;

[https://thegradient.pub/an-introduction-to-artificial-life-for-people-who-like-ai/](https://thegradient.pub/an-introduction-to-artificial-life-for-people-who-like-ai/)",19,1
1312,2019-11-26,2019,11,26,3,e1k8cc,[D] Data-poisoning and Trojan attacks at training time. Is it a real threat?,https://www.reddit.com/r/MachineLearning/comments/e1k8cc/d_datapoisoning_and_trojan_attacks_at_training/,niklongstone,1574706350,"I would like to know anyone's opinion on this.

Recent work has identified that classification models implemented as neural networks are vulnerable to data-poisoning and Trojan attacks at training time.

Source: Attacks on Deep Reinforcement Learning Agents : [https://arxiv.org/abs/1903.06638](https://arxiv.org/abs/1903.06638)

&amp;#x200B;

1. Is it a real threat?
2. How the risk can be identified from someone that just uses the model without access to its source or training data (i.e. prepare a set of tests)?",1,1
1313,2019-11-26,2019,11,26,3,e1kfy2,[R] [1911.06786] Stagewise Knowledge Distillation,https://www.reddit.com/r/MachineLearning/comments/e1kfy2/r_191106786_stagewise_knowledge_distillation/,akshayk07,1574707193,,5,1
1314,2019-11-26,2019,11,26,3,e1klx8,Questions about the working of YOLO,https://www.reddit.com/r/MachineLearning/comments/e1klx8/questions_about_the_working_of_yolo/,CSGOvelocity,1574707867,[removed],0,1
1315,2019-11-26,2019,11,26,4,e1l2aq,Machine learning has revealed exactly how much of a Shakespeare play was written by someone else,https://www.reddit.com/r/MachineLearning/comments/e1l2aq/machine_learning_has_revealed_exactly_how_much_of/,jonfla,1574709652,,0,1
1316,2019-11-26,2019,11,26,4,e1l3i5,AI Helps Quantum Chemists Determine Molecular Wave Functions,https://www.reddit.com/r/MachineLearning/comments/e1l3i5/ai_helps_quantum_chemists_determine_molecular/,Yuqing7,1574709790,,0,1
1317,2019-11-26,2019,11,26,4,e1l5d9,"Understanding the generalization of ""lottery tickets"" in neural networks",https://www.reddit.com/r/MachineLearning/comments/e1l5d9/understanding_the_generalization_of_lottery/,circuithunter,1574709985,,0,1
1318,2019-11-26,2019,11,26,4,e1l5en,about required math for algorithms and ML,https://www.reddit.com/r/MachineLearning/comments/e1l5en/about_required_math_for_algorithms_and_ml/,flakesrc,1574709988,"I was wanting to learn ML, where they say to study things like calculus, linear algebra, probability and statistics.

I thought: OK, just learn on average 80% of the school math (here in Brazil) and I can go to those subjects.

But looking at a book on algorithms, I saw that it is necessary to understand ""mathematical analysis"", and this is not part of school math, nor calculus, linear algebra etc ... (I think).

So what else is missing?

The content I want to study is:

1. Recommended by a teacher to study on their online platform: Basic Math, Introduction to Functions, Affine Function, Quadratic Function, Exponential, Logarithms, Probability, Statistics, Newton's Binomial, Trigonometry, Matrices, Determinants, Linear Systems, Analytic Geometry, Module, Polynomials.

Maybe I have some module with the strange name, because I don't know English and I'm using traudor.

After that, I plan to study:

2. Linear algebra, discrete mathematics, calculus l (perhaps also ll), probability and statistics.

There is also the book ""Introduction to Algorithms"", does it TEACH the necessary subjects? Unlike a book I saw at the beginning, it simply tells the basics about the concept and after that begins to use it.

What else is missing? I just want to read basic algorithm books, and some also about machine learning (for things like chatbot (NLP), recommendation systems, etc.)",0,1
1319,2019-11-26,2019,11,26,4,e1l6px,"[R] Understanding the generalization of ""lottery tickets"" in neural networks",https://www.reddit.com/r/MachineLearning/comments/e1l6px/r_understanding_the_generalization_of_lottery/,circuithunter,1574710129,,0,1
1320,2019-11-26,2019,11,26,4,e1ldj4,Reduce churn using Machine learning (new idea),https://www.reddit.com/r/MachineLearning/comments/e1ldj4/reduce_churn_using_machine_learning_new_idea/,AzizBelkhir,1574710877,,0,1
1321,2019-11-26,2019,11,26,5,e1mhaf,Undergraduate machine learning projects suggestions,https://www.reddit.com/r/MachineLearning/comments/e1mhaf/undergraduate_machine_learning_projects/,_vaibhav4,1574715063,"I need some suggestions for an undergraduate final year project based on machine learning, computer vision, and robotics.",0,1
1322,2019-11-26,2019,11,26,5,e1mlj1,Exclusive: SingularityNET Announces Its New PayPal Integration,https://www.reddit.com/r/MachineLearning/comments/e1mlj1/exclusive_singularitynet_announces_its_new_paypal/,007moonboundnxs,1574715526,,0,1
1323,2019-11-26,2019,11,26,7,e1ns9q,GPT-2 Fine Tuning &amp; Voice Assistant Tutorial,https://www.reddit.com/r/MachineLearning/comments/e1ns9q/gpt2_fine_tuning_voice_assistant_tutorial/,Bunkydoo,1574720128,,0,1
1324,2019-11-26,2019,11,26,7,e1o29m,[Research] Understanding the generalization of lottery tickets in neural networks,https://www.reddit.com/r/MachineLearning/comments/e1o29m/research_understanding_the_generalization_of/,circuithunter,1574721232,,0,1
1325,2019-11-26,2019,11,26,8,e1ojfo,"[R] Understanding the generalization of ""lottery tickets"" in neural networks",https://www.reddit.com/r/MachineLearning/comments/e1ojfo/r_understanding_the_generalization_of_lottery/,arimorcos,1574723189,"Sharing our recent blog post summarizing some of our recent work understanding the boundaries of the lottery ticket hypothesis. In particular, we make some progress towards the following questions:

* Do winning ticket initializations contain generic inductive biases or are they overfit to the particular dataset and optimizer used to generate them? 
* Is the lottery ticket phenomenon limited to supervised image classification, or is it also present in other domains like RL and NLP? 
* Can we begin to explain lottery tickets theoretically? 

The blog post is below:

[**Understanding the generalization of ""lottery tickets"" in neural networks**](https://ai.facebook.com/blog/understanding-the-generalization-of-lottery-tickets-in-neural-networks/)

And the papers covered can be found here:

[One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers](https://arxiv.org/abs/1906.02773)

[Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP](https://arxiv.org/abs/1906.02768)

[Luck Matters: Understanding Training Dynamics of Deep ReLU Networks](https://arxiv.org/abs/1905.13405)

[Student Specialization in Deep ReLU Networks With Finite Width and Input Dimension](https://arxiv.org/abs/1909.13458)",4,1
1326,2019-11-26,2019,11,26,8,e1ooou,Non-Uniform Memory Architecture (NUMA) Problems,https://www.reddit.com/r/MachineLearning/comments/e1ooou/nonuniform_memory_architecture_numa_problems/,Greendogo,1574723784,[removed],0,1
1327,2019-11-26,2019,11,26,8,e1ossg,"Introducing dKeras, new framework for distributed Keras, 30x inference improvements on large CPUs",https://www.reddit.com/r/MachineLearning/comments/e1ossg/introducing_dkeras_new_framework_for_distributed/,s-offer,1574724244,"dKeras is an open-source framework that uses UC Berkeley's Ray to distributed Keras models while maintaining the same Keras API. So far, it only supports data parallelism inference but will have much more functionality soon, read more on its GitHub page: [https://github.com/dkeras-project/dkeras](https://github.com/dkeras-project/dkeras) and on its introductory article on Medium: [https://medium.com/@offercstephen/dkeras-make-keras-faster-with-a-few-lines-of-code-a1792b12dfa0](https://medium.com/@offercstephen/dkeras-make-keras-faster-with-a-few-lines-of-code-a1792b12dfa0)",0,1
1328,2019-11-26,2019,11,26,8,e1oti8,PCI Express Bandwidth and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/e1oti8/pci_express_bandwidth_and_machine_learning/,GAGARIN0461,1574724334,[removed],0,1
1329,2019-11-26,2019,11,26,9,e1pgd0,[R] Adversarial Examples Improve Image Recognition. ImageNet top-1 accuracy 85.5% (no extra data) with adversarial examples.,https://www.reddit.com/r/MachineLearning/comments/e1pgd0/r_adversarial_examples_improve_image_recognition/,hardmaru,1574727063,,6,1
1330,2019-11-26,2019,11,26,9,e1pvg1,[P] Machine Learning Systems Design (open source book by @chipro),https://www.reddit.com/r/MachineLearning/comments/e1pvg1/p_machine_learning_systems_design_open_source/,hardmaru,1574728877,"*An [open source book](https://github.com/chiphuyen/machine-learning-systems-design) compiled by Chip Huyen. Feel free to contribute:*

This booklet covers four main steps of designing a machine learning system:

1. Project setup

2. Data pipeline

3. Modeling: selecting, training, and debugging

4. Serving: testing, deploying, and maintaining

It comes with links to practical resources that explain each aspect in more details. It also suggests case studies written by machine learning engineers at major tech companies who have deployed machine learning systems to solve real-world problems.

At the end, the booklet contains 27 open-ended machine learning systems design questions that might come up in machine learning interviews. The answers for these questions will be published in the book Machine Learning Interviews.

project: https://github.com/chiphuyen/machine-learning-systems-design

PDF: https://github.com/chiphuyen/machine-learning-systems-design/blob/master/build/build1/consolidated.pdf",4,1
1331,2019-11-26,2019,11,26,10,e1qr93,Paying a trading bot,https://www.reddit.com/r/MachineLearning/comments/e1qr93/paying_a_trading_bot/,eatdatpussy343,1574732922,[removed],0,1
1332,2019-11-26,2019,11,26,11,e1r0ou,"[D] Chinese government uses machine learning not only for surveillance, but also for predictive policing and for deciding who to arrest in Xinjiang",https://www.reddit.com/r/MachineLearning/comments/e1r0ou/d_chinese_government_uses_machine_learning_not/,sensetime,1574734164,"Link to story: https://www.icij.org/investigations/china-cables/exposed-chinas-operating-manuals-for-mass-internment-and-arrest-by-algorithm/

This post is not an ML *research* related post, but I think it will be interesting to the community to see how research is applied by authoritarian governments to achieve their goals. It is related to a few previous popular posts on this thread with high upvotes, which prompted me to post this [story](https://www.icij.org/investigations/china-cables/exposed-chinas-operating-manuals-for-mass-internment-and-arrest-by-algorithm/):

[ICCV 19: The state of some ethically questionable papers](https://redd.it/dp389c)

[Hikvision marketed ML surveillance camera that automatically identifies Uyghurs](https://redd.it/dv5axp)

[Working on an ethically questionnable project...](https://redd.it/dw7sms)

This [story](https://www.icij.org/investigations/china-cables/exposed-chinas-operating-manuals-for-mass-internment-and-arrest-by-algorithm/) reports the details of a new leak of highly classified Chinese government documents reveals the operations manual for running the mass detention camps in Xinjiang and exposed the mechanics of the regions system of mass surveillance.

*The [lead journalist](https://twitter.com/BethanyAllenEbr/status/1198663008152621057) of the story also wrote a summary of findings:*

The China Cables represent the first leak of a classified Chinese government document revealing the inner workings of the detention camps, as well as the first leak of classified government documents unveiling the predictive policing system in Xinjiang.

The leak features classified intelligence briefings that reveal, in the governments own words, how Xinjiang police essentially take orders from a massive cybernetic brain known as IJOP, which flags entire categories of people for investigation &amp; detention.

These secret intelligence briefings reveal the scope and ambition of the governments AI-powered policing platform, which purports to predict crimes based on computer-generated findings alone. The result? Arrest by algorithm.

*The article describe methods used for algorithmic policing:*

The classified intelligence briefings reveal the scope and ambition of the governments artificial-intelligence-powered policing platform, which purports to predict crimes based on these computer-generated findings alone. Experts say the platform, which is used in both policing and military contexts, demonstrates the power of technology to help drive industrial-scale human rights abuses.

The Chinese have bought into a model of policing where they believe that through the collection of large-scale data run through artificial intelligence and machine learning that they can, in fact, predict ahead of time where possible incidents might take place, as well as identify possible populations that have the propensity to engage in anti-state anti-regime action, said Mulvenon, the SOS International document expert and director of intelligence integration. And then they are preemptively going after those people using that data.

In addition to the predictive policing aspect of the article, there are side articles about the entire ML stack, including how [mobile apps](https://www.icij.org/investigations/china-cables/how-china-targets-uighurs-one-by-one-for-using-a-mobile-app/) are used to target Uighurs, and also how the inmates are [re-educated](https://www.bbc.com/news/world-asia-china-50511063) once inside the concentration camps. The documents reveal how every aspect of a detainee's life is monitored and controlled.

https://www.icij.org/investigations/china-cables/exposed-chinas-operating-manuals-for-mass-internment-and-arrest-by-algorithm/

*Note: My motivation for posting this story is to raise ethical concerns and awareness in the research community (see this [thread](https://redd.it/e10b5x) for context). I do not want to heighten levels of racism towards the Chinese research community (not that it may matter, but I am Chinese). I am aware of the fact that the Chinese government's policy is to integrate the state and the people as one, so accusing the party is perceived domestically as insulting the Chinese people, but I also believe that we as a research community is intelligent enough to be able to separate government, and those in power, from individual researchers.*",217,1
1333,2019-11-26,2019,11,26,11,e1r1ur,What are Good clustering techiques in High Dimensions?,https://www.reddit.com/r/MachineLearning/comments/e1r1ur/what_are_good_clustering_techiques_in_high/,PyWarrior,1574734318,[removed],0,1
1334,2019-11-26,2019,11,26,12,e1rpzq,[D] Where are the good machine learning books for practitioners?,https://www.reddit.com/r/MachineLearning/comments/e1rpzq/d_where_are_the_good_machine_learning_books_for/,Ctown_struggles00,1574737458,"For beginners there's PRML by Bishop and maybe Understanding Machine Learning by Shai^2 but for advanced readers or those interested in the deep learning and GAN research landscape (and how to apply it) there really isn't anything good out there. 

I personally don't like Goodfellow's Deep Learning book. I wish there was a good deep-dive out there but there just isn't what I need.",5,1
1335,2019-11-26,2019,11,26,14,e1t2hu,Bot Marketplace - 4,https://www.reddit.com/r/MachineLearning/comments/e1t2hu/bot_marketplace_4/,getengati,1574744417,[removed],0,1
1336,2019-11-26,2019,11,26,14,e1t3k9,This app allows you to apply effects to individual objects using CV,https://www.reddit.com/r/MachineLearning/comments/e1t3k9/this_app_allows_you_to_apply_effects_to/,ednevsky,1574744569,,0,1
1337,2019-11-26,2019,11,26,14,e1ta9i,[1911.10636] Pyramid Vector Quantization and Bit Level Sparsity in Weights for Efficient Neural Networks Inference,https://www.reddit.com/r/MachineLearning/comments/e1ta9i/191110636_pyramid_vector_quantization_and_bit/,BLMACPVQ,1574745592,[removed],0,1
1338,2019-11-26,2019,11,26,14,e1tgbs,Gradient Descent,https://www.reddit.com/r/MachineLearning/comments/e1tgbs/gradient_descent/,ClairemontCanyon,1574746527,[removed],0,1
1339,2019-11-26,2019,11,26,14,e1tkri,[Project] Help contribute to my ml5.js project,https://www.reddit.com/r/MachineLearning/comments/e1tkri/project_help_contribute_to_my_ml5js_project/,salutiferous-,1574747221,"I am creating a project using a javascript machine learning library to run an Image Classifier in the browser. Please help me fill out this google form: [https://forms.gle/8TfZMMwp46VgARPM9](https://forms.gle/8TfZMMwp46VgARPM9)

I need to collect as much data as possible. Much appreciated!",1,1
1340,2019-11-26,2019,11,26,15,e1tsd7,"[P] I re-implemented Hyperband, check it out!",https://www.reddit.com/r/MachineLearning/comments/e1tsd7/p_i_reimplemented_hyperband_check_it_out/,Deepblue129,1574748432,"Hyperband is a state-of-the-art algorithm for hyperparameter tunning that focuses on resource efficiency. It does so by encorperating early-stopping into it's strategy. Here are some of the results:

[For more, go here: http:\/\/www.argmin.net\/2016\/06\/23\/hyperband\/ ](https://preview.redd.it/24yxsku51z041.png?width=1250&amp;format=png&amp;auto=webp&amp;s=f939f02f86cd29125930b790ef6fd0ac3f3ef976)

I was unable to find any great implementations of hyperband, so I implemented it! Here it is: [https://gist.github.com/PetrochukM/2c5fae9daf0529ed589018c6353c9f7b](https://gist.github.com/PetrochukM/2c5fae9daf0529ed589018c6353c9f7b)

The implementation is commented and documented to help ensure correctness and improve code readability.",4,1
1341,2019-11-26,2019,11,26,15,e1tthm,Top 3 Ways Artificial Intelligence Can Help you in Club Management,https://www.reddit.com/r/MachineLearning/comments/e1tthm/top_3_ways_artificial_intelligence_can_help_you/,KumarGymLover,1574748600,,0,1
1342,2019-11-26,2019,11,26,15,e1tx61,MSMarco-Bio Information Retrieval Benchmark for Biology/ Medicine,https://www.reddit.com/r/MachineLearning/comments/e1tx61/msmarcobio_information_retrieval_benchmark_for/,jpertschuk,1574749155,[removed],0,1
1343,2019-11-26,2019,11,26,16,e1uj5k,Why is Artificial Intelligence in Business Analytics is so Important?,https://www.reddit.com/r/MachineLearning/comments/e1uj5k/why_is_artificial_intelligence_in_business/,Countants123,1574752973,,0,1
1344,2019-11-26,2019,11,26,17,e1uz7b,Improve Reorder Sales by Machine Learning-based Email Personalization Model,https://www.reddit.com/r/MachineLearning/comments/e1uz7b/improve_reorder_sales_by_machine_learningbased/,devgspann,1574755961,[removed],0,1
1345,2019-11-26,2019,11,26,17,e1v2q1,[P] A Visual Guide to Using BERT for the First Time,https://www.reddit.com/r/MachineLearning/comments/e1v2q1/p_a_visual_guide_to_using_bert_for_the_first_time/,nortab,1574756644,"Hi r/MachineLearning,

I wrote a blog post that I hope could be the gentlest way for you to start playing with BERT for the first time;

[https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/](https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)

It uses a lighter version of BERT (the distilled version from HuggingFace, distilBERT) to do sentence embedding, then uses Scikit Learn for Linear Regression classification. As a first exposure to BERT, I'm having people use the general trained model and not worry about fine-tuning for now. After getting people through this initial hump, I'm hoping readers would get more comfortable doing more exploration and poking around with the model and its usecases. 

&amp;#x200B;

I hope you enjoy it. All feedback/corrections are appreciated.",12,1
1346,2019-11-26,2019,11,26,17,e1v3m7,5 Examples of Machine Learning You Should Know About,https://www.reddit.com/r/MachineLearning/comments/e1v3m7/5_examples_of_machine_learning_you_should_know/,Priyanka_Kolabtree,1574756819,,0,1
1347,2019-11-26,2019,11,26,18,e1vgpd,"[R] Gradient descent happens in a tiny subspace, approximately preserved over long periods of training",https://www.reddit.com/r/MachineLearning/comments/e1vgpd/r_gradient_descent_happens_in_a_tiny_subspace/,jarekduda,1574759387,,17,1
1348,2019-11-26,2019,11,26,18,e1vhcd,MuJoCo alternatives,https://www.reddit.com/r/MachineLearning/comments/e1vhcd/mujoco_alternatives/,Fragore,1574759500,[removed],0,1
1349,2019-11-26,2019,11,26,18,e1vne0,Handling Dependency of Multiple Y-Variables,https://www.reddit.com/r/MachineLearning/comments/e1vne0/handling_dependency_of_multiple_yvariables/,srimanth_ds,1574760677,[removed],0,1
1350,2019-11-26,2019,11,26,18,e1vw1r,why changing a MAC adress?,https://www.reddit.com/r/MachineLearning/comments/e1vw1r/why_changing_a_mac_adress/,Microsoft555,1574762358,[removed],0,1
1351,2019-11-26,2019,11,26,19,e1w1rf,Neural Quine: Is Self-Replicating AI Real? [Research and Code],https://www.reddit.com/r/MachineLearning/comments/e1w1rf/neural_quine_is_selfreplicating_ai_real_research/,rachnogstyle,1574763377,,0,1
1352,2019-11-26,2019,11,26,19,e1w2lo,27 Rare Photos from Early 1900's of India COLORIZED with Machine Learning!!!,https://www.reddit.com/r/MachineLearning/comments/e1w2lo/27_rare_photos_from_early_1900s_of_india/,abhishekK50,1574763511,,1,1
1353,2019-11-26,2019,11,26,20,e1wl0y,How Artificial Intelligence Software will Change Ways of Doing Business in the Upcoming Years,https://www.reddit.com/r/MachineLearning/comments/e1wl0y/how_artificial_intelligence_software_will_change/,day1technologies,1574766934,,0,1
1354,2019-11-26,2019,11,26,20,e1wllc,High resolution video visualization of mode connectivity using real data | NeurIPS | ARXIV:1802.10026,https://www.reddit.com/r/MachineLearning/comments/e1wllc/high_resolution_video_visualization_of_mode/,javismiles,1574767038,,0,1
1355,2019-11-26,2019,11,26,20,e1wqyv,"Computer Science Topics List for Thesis, Research, and Project",https://www.reddit.com/r/MachineLearning/comments/e1wqyv/computer_science_topics_list_for_thesis_research/,writethesis,1574768011,,0,1
1356,2019-11-26,2019,11,26,20,e1ww0a,Breaking Black-box AI,https://www.reddit.com/r/MachineLearning/comments/e1ww0a/breaking_blackbox_ai/,mto96,1574768896,,1,1
1357,2019-11-26,2019,11,26,20,e1wwr9,Intel Core i7-9700K or AMD Ryzen 7 3800X?,https://www.reddit.com/r/MachineLearning/comments/e1wwr9/intel_core_i79700k_or_amd_ryzen_7_3800x/,baabaaaam,1574769024,[removed],0,1
1358,2019-11-26,2019,11,26,20,e1wyfv,SMT Machine at Best Price | Southern Machinery,https://www.reddit.com/r/MachineLearning/comments/e1wyfv/smt_machine_at_best_price_southern_machinery/,smthelp1,1574769317,,0,1
1359,2019-11-26,2019,11,26,21,e1x6na,10 Innovative Machine Learning APIs You Should Learn in 2020,https://www.reddit.com/r/MachineLearning/comments/e1x6na/10_innovative_machine_learning_apis_you_should/,KiranKiller,1574770671,"At this moment, machine learning is dominated by big companies including Google, Amazon, IBM, Microsoft, but the trend is now shifting and smaller companies are bringing their algorithms and APIs into the field. APIs are making it easier for companies to share knowledge and information across multiple spectrum.   


Here is an Article is Read on the Innovative ML APIs  we should try At least once.  
 [https://medium.com/datadriveninvestor/10-innovative-machine-learning-apis-you-should-learn-in-2018-64a7f2cc45cd](https://medium.com/datadriveninvestor/10-innovative-machine-learning-apis-you-should-learn-in-2018-64a7f2cc45cd)   


I found the list really helpful, I wanted to know , If there are any updates ,that they are missing or you guys feel should be there, it would really help me out.",0,1
1360,2019-11-26,2019,11,26,21,e1xae8,Talking about the illusion of consciousness and the attention schema theory,https://www.reddit.com/r/MachineLearning/comments/e1xae8/talking_about_the_illusion_of_consciousness_and/,gfrison,1574771280,,0,1
1361,2019-11-26,2019,11,26,22,e1xy85,"3 technologies that will impact the next wave of digital transformation: The new industrial infrastructure comprises three specific features that perform critical business functions: connectivity, computing, and transactions.",https://www.reddit.com/r/MachineLearning/comments/e1xy85/3_technologies_that_will_impact_the_next_wave_of/,Digiryte,1574774956,,0,1
1362,2019-11-26,2019,11,26,23,e1yeuc,Best Machine Learning Frameworks: 5 Machine Learning Frameworks of 2019,https://www.reddit.com/r/MachineLearning/comments/e1yeuc/best_machine_learning_frameworks_5_machine/,pradeepmakhija,1574777317,,0,1
1363,2019-11-27,2019,11,27,0,e1z4ge,[1906.05433v2] Tackling Climate Change with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/e1z4ge/190605433v2_tackling_climate_change_with_machine/,Smith4242,1574780674,,6,1
1364,2019-11-27,2019,11,27,0,e1z4je,"Is the whole concept of automatic hyperparameter tuning and searching merely the equivalent of hypothesis testing where you running the same janky experiment enough number of times until getting some desired outcome, and calling it a finding, when it's luck?",https://www.reddit.com/r/MachineLearning/comments/e1z4je/is_the_whole_concept_of_automatic_hyperparameter/,gerry_mandering_50,1574780682,[removed],0,1
1365,2019-11-27,2019,11,27,0,e1z725,[P] I Reimplemented StyleGAN using TensorFlow 2.0 - With A Web Demo!,https://www.reddit.com/r/MachineLearning/comments/e1z725/p_i_reimplemented_stylegan_using_tensorflow_20/,manicman1999,1574781005,"Here is a sample of 64 of the images when trained on r/EarthPorn:

[https://i.imgur.com/K9mU7vH.jpg](https://i.imgur.com/K9mU7vH.jpg)

&amp;#x200B;

Here is the code:

 [https://github.com/manicman1999/StyleGAN-Tensorflow-2.0](https://github.com/manicman1999/StyleGAN-Tensorflow-2.0) 

&amp;#x200B;

And finally, here is the live web demo:

 [https://matchue.ca/p/earthhd/](https://matchue.ca/p/earthhd/) 

&amp;#x200B;

Enjoy!",0,1
1366,2019-11-27,2019,11,27,0,e1z7ld,[Q] How to check if my datasets have covariate shift?,https://www.reddit.com/r/MachineLearning/comments/e1z7ld/q_how_to_check_if_my_datasets_have_covariate_shift/,stat_leaf,1574781073,"I have 2 datasets where the covariates used are the same but the scale used are different. For example, dataset 1 could have scale for pressure parameter at 0.1, 0.2, 0.5, 0.8 and dataset 2 scale for the pressure parameter is 0.3, 0.8, 0.2, 0.9, 1.0

so far I have plotted the distribution for the response variable (dependent variable) and dataset1 and dataset2 have different distributions for the response variable but if I understand correctly, covariate shift occurs when the distribution of the covariate differs between dataset1 and dataset2. 

How can I properly check this assumption?

If I am planning to train a model on dataset1 to be tested for dataset2 with a different distribution, what are the methods that can be used ( this is a regression task) besides Neural Network?

Thank you.",0,1
1367,2019-11-27,2019,11,27,0,e1z9ct,Suggestions for a new smaller laptop capable of doing ML work on it.,https://www.reddit.com/r/MachineLearning/comments/e1z9ct/suggestions_for_a_new_smaller_laptop_capable_of/,nagol3,1574781297,[removed],0,1
1368,2019-11-27,2019,11,27,0,e1zdly,[D] How to check if my datasets have covariate shifts?,https://www.reddit.com/r/MachineLearning/comments/e1zdly/d_how_to_check_if_my_datasets_have_covariate/,stat_leaf,1574781824,"I have 2 datasets where the covariates used are the same but the scale used are different. For example, dataset 1 could have scale for pressure parameter at 0.1, 0.2, 0.5, 0.8 and dataset 2 scale for the pressure parameter is 0.3, 0.8, 0.2, 0.9, 1.0

so far I have plotted the distribution for the response variable (dependent variable) and dataset1 and dataset2 have different distributions for the response variable but if I understand correctly, covariate shift occurs when the distribution of the covariate differs between dataset1 and dataset2.

How can I properly check this assumption?

If I am planning to train a model on dataset1 to be tested for dataset2 with a different distribution, what are the methods that can be used ( this is a regression task) besides Neural Network?

Thank you.",3,1
1369,2019-11-27,2019,11,27,0,e1zfa8,[P] Benchmarking Metric Learning Algorithms the Right Way,https://www.reddit.com/r/MachineLearning/comments/e1zfa8/p_benchmarking_metric_learning_algorithms_the/,VanillaCashew,1574782026,,0,1
1370,2019-11-27,2019,11,27,0,e1zfri,[P] CorrGAN.io - Sampling Realistic Financial Correlations,https://www.reddit.com/r/MachineLearning/comments/e1zfri/p_corrganio_sampling_realistic_financial/,gau_mar,1574782084,,0,1
1371,2019-11-27,2019,11,27,0,e1zo1o,Benchmarking Metric Learning Algorithms the Right Way,https://www.reddit.com/r/MachineLearning/comments/e1zo1o/benchmarking_metric_learning_algorithms_the_right/,VanillaCashew,1574783090,[removed],0,1
1372,2019-11-27,2019,11,27,0,e1zpih,Downloading Full ImageNet URLs,https://www.reddit.com/r/MachineLearning/comments/e1zpih/downloading_full_imagenet_urls/,FreddeFrallan,1574783261,[removed],0,1
1373,2019-11-27,2019,11,27,0,e1zq0c,[R] Preventing undesirable behavior of intelligent machines,https://www.reddit.com/r/MachineLearning/comments/e1zq0c/r_preventing_undesirable_behavior_of_intelligent/,CyberByte,1574783324,,0,1
1374,2019-11-27,2019,11,27,0,e1zqsl,Ideas for interesting (eye-opening for beginner) mathematics in machine learning?,https://www.reddit.com/r/MachineLearning/comments/e1zqsl/ideas_for_interesting_eyeopening_for_beginner/,titusng074,1574783429,[removed],0,1
1375,2019-11-27,2019,11,27,0,e1zt4y,Vehicle count | road traffic analyze using raspberry pi and OPENCV,https://www.reddit.com/r/MachineLearning/comments/e1zt4y/vehicle_count_road_traffic_analyze_using/,amalanju,1574783705,,0,1
1376,2019-11-27,2019,11,27,0,e1zub9,[P] Benchmarking Metric Learning Algorithms the Right Way,https://www.reddit.com/r/MachineLearning/comments/e1zub9/p_benchmarking_metric_learning_algorithms_the/,VanillaCashew,1574783844,"I've been researching metric learning algorithms for a while now, and in the process I discovered some issues with the field.

You can read about it here: [https://medium.com/@tkm45/benchmarking-metric-learning-algorithms-the-right-way-90c073a83968](https://medium.com/@tkm45/benchmarking-metric-learning-algorithms-the-right-way-90c073a83968)

TL;DR:

1. Many papers don't do apple-to-apple comparisons. They change the network architecture, embedding size, data augmentation, or just use performance-boosting tricks that aren't mentioned in their paper.
2. Most papers don't use a validation set.
3. Two baseline algorithms (triplet and contrastive loss) are actually competitive with the state-of-the-art, but are not presented this way in most papers.
4. I've made a flexible benchmarking tool that can standardize the way we evaluate metric learning algorithms. You can see it here: [https://github.com/KevinMusgrave/powerful\_benchmarker](https://github.com/KevinMusgrave/powerful_benchmarker)",15,1
1377,2019-11-27,2019,11,27,1,e1zw6t,[D] Ideas for interesting (eye-opening for beginner) mathematics in machine learning?,https://www.reddit.com/r/MachineLearning/comments/e1zw6t/d_ideas_for_interesting_eyeopening_for_beginner/,titusng074,1574784076,"So I'm an IB high school student and the IB curriculum requires us to write an extended essay, which is a 4000-word mini-research paper answering a research question. As I'm interested in machine learning, but CS is not available at school, I chose to write about mathematics. I explored some basic stuffs such as gradient descent and back propagation; but they are too fixed and I can't seem to formulate a question around them. Can you guys suggest some interesting mathematics in machine learning to investigate on?

Also, an adult friend of mine suggest me to try ""beta distribution"", but after an hour of research I can't find the relationship between it and machine learning. Some insight will be hugely appreciated. Thanks.",15,1
1378,2019-11-27,2019,11,27,1,e202r7,[P] I Reimplemented StyleGAN using TensorFlow 2.0 - Including a Web Demo!,https://www.reddit.com/r/MachineLearning/comments/e202r7/p_i_reimplemented_stylegan_using_tensorflow_20/,manicman1999,1574784781,"Here is a sample of 64 of the images when trained on [r/EarthPorn](https://www.reddit.com/r/EarthPorn/):

[https://i.imgur.com/K9mU7vH.jpg](https://i.imgur.com/K9mU7vH.jpg)

Here is the code:

[https://github.com/manicman1999/StyleGAN-Tensorflow-2.0](https://github.com/manicman1999/StyleGAN-Tensorflow-2.0)

And finally, here is the live web demo:

[https://matchue.ca/p/earthhd/](https://matchue.ca/p/earthhd/)

Enjoy!",13,1
1379,2019-11-27,2019,11,27,1,e204lo,Is there a machine learning algorithm that shows which are the most important criteria in getting the final outcome?,https://www.reddit.com/r/MachineLearning/comments/e204lo/is_there_a_machine_learning_algorithm_that_shows/,antjanjan1993,1574784992,[removed],0,1
1380,2019-11-27,2019,11,27,1,e20im5,Reverse engineering the Human Brain. Sensory association cortex.,https://www.reddit.com/r/MachineLearning/comments/e20im5/reverse_engineering_the_human_brain_sensory/,Korrelan,1574786516,,0,1
1381,2019-11-27,2019,11,27,1,e20m7j,Input blur image instead of pure noise to the GAN generator,https://www.reddit.com/r/MachineLearning/comments/e20m7j/input_blur_image_instead_of_pure_noise_to_the_gan/,greenvalley0101,1574786924,[removed],0,1
1382,2019-11-27,2019,11,27,2,e20sy9,Machine Learning on Encrypted data without Decrypting it,https://www.reddit.com/r/MachineLearning/comments/e20sy9/machine_learning_on_encrypted_data_without/,phrasebuilder,1574787687,,0,1
1383,2019-11-27,2019,11,27,2,e20vja,Career in ML,https://www.reddit.com/r/MachineLearning/comments/e20vja/career_in_ml/,iamsadielarue,1574787963,[removed],0,1
1384,2019-11-27,2019,11,27,2,e20zal,Gradient Descent and Human Learning,https://www.reddit.com/r/MachineLearning/comments/e20zal/gradient_descent_and_human_learning/,wel3anee,1574788386,[removed],0,1
1385,2019-11-27,2019,11,27,2,e21792,Machine Learning on Encrypted Data Without Decrypting It,https://www.reddit.com/r/MachineLearning/comments/e21792/machine_learning_on_encrypted_data_without/,HN_Crosspost_Bot,1574789246,,0,1
1386,2019-11-27,2019,11,27,2,e21fgz,[R] Unsupervised Attention Mechanism across Neural Network Layers,https://www.reddit.com/r/MachineLearning/comments/e21fgz/r_unsupervised_attention_mechanism_across_neural/,doerlbh,1574790137,,2,1
1387,2019-11-27,2019,11,27,2,e21o46,MNIST classification running on a Nintendo DSi thanks to TF Lite Micro,https://www.reddit.com/r/MachineLearning/comments/e21o46/mnist_classification_running_on_a_nintendo_dsi/,FTC55,1574791093,,0,1
1388,2019-11-27,2019,11,27,3,e220gu,Artificial Intelligence is revolutionizing Digital Marketing,https://www.reddit.com/r/MachineLearning/comments/e220gu/artificial_intelligence_is_revolutionizing/,benbihi,1574792493,[removed],0,1
1389,2019-11-27,2019,11,27,3,e224mt,Astrophotography with Night Sight on Pixel Phones,https://www.reddit.com/r/MachineLearning/comments/e224mt/astrophotography_with_night_sight_on_pixel_phones/,sjoerdapp,1574792954,,0,1
1390,2019-11-27,2019,11,27,3,e225l4,"This has been boggling my mind for months. I respectfully want to ask you, what exactly is going wrong with TPU's in my scenario.",https://www.reddit.com/r/MachineLearning/comments/e225l4/this_has_been_boggling_my_mind_for_months_i/,blazejohk,1574793070,"These past two months I have been trying to train a Neural Network with multiple TPU's. And with no success, because of memory-related issues.

Here's a little picture of the error: [https://user-images.githubusercontent.com/30562332/62986225-64f32300-bdef-11e9-8948-c9552cc53844.png](https://user-images.githubusercontent.com/30562332/62986225-64f32300-bdef-11e9-8948-c9552cc53844.png)

My model is 7GB and dataset 150GB. I cannot use more than 1/8 TPU's at a time to train this model.

I have tested TPU v2 and v3.

I would highly appreciate any help related to this problem. :)",0,1
1391,2019-11-27,2019,11,27,3,e22gmc,[Discussion] Won't Max Pooling mess up a lot of information in this Network?,https://www.reddit.com/r/MachineLearning/comments/e22gmc/discussion_wont_max_pooling_mess_up_a_lot_of/,avdalim,1574794305,"As far as I understood it, Max Pooling takes the maximum positive value and not absolute values.

In this paper, optimized mechanical structures get generated with a U-Net by feeding it mechanical information like nodal displacement, element strains, and volume fractions.

 [https://arxiv.org/ftp/arxiv/papers/1901/1901.07761.pdf](https://arxiv.org/ftp/arxiv/papers/1901/1901.07761.pdf) 

My question now is: won't Max Pooling mess up a lot of information, if it just takes the positive value and not the absolute value? Since positive elements in the strain matrix correspond to tensile strains and negative to compressive ones.",6,1
1392,2019-11-27,2019,11,27,3,e22kmo,Subreddit Populated of AI personifications of other subreddits,https://www.reddit.com/r/MachineLearning/comments/e22kmo/subreddit_populated_of_ai_personifications_of/,Shishi1315,1574794735,,1,1
1393,2019-11-27,2019,11,27,4,e22pkd,Keras Output Shape Help,https://www.reddit.com/r/MachineLearning/comments/e22pkd/keras_output_shape_help/,venatorrrrr,1574795242,[removed],0,1
1394,2019-11-27,2019,11,27,4,e22wwm,New Machine Learning Approach to Detecting Cybercrime - Stopping Serial Hijackers (Eavesdroppers) and Corporate Data Thieves,https://www.reddit.com/r/MachineLearning/comments/e22wwm/new_machine_learning_approach_to_detecting/,itveterans,1574796045,[removed],0,1
1395,2019-11-27,2019,11,27,4,e236ly,"I am learning on machine learning. Papers are just tweaking or fining one parameter from previous papers, and call it new bleeding edge finding. Tbh, i am really disappointed.",https://www.reddit.com/r/MachineLearning/comments/e236ly/i_am_learning_on_machine_learning_papers_are_just/,knut_2,1574797148,[removed],0,1
1396,2019-11-27,2019,11,27,4,e238fe,Getting a bipedal/quadrapidel robot to walk!,https://www.reddit.com/r/MachineLearning/comments/e238fe/getting_a_bipedalquadrapidel_robot_to_walk/,dancing_panther123,1574797358,[removed],0,1
1397,2019-11-27,2019,11,27,4,e23a3f,24+ resources to learn python from scratch,https://www.reddit.com/r/MachineLearning/comments/e23a3f/24_resources_to_learn_python_from_scratch/,frenchdic,1574797546,,0,1
1398,2019-11-27,2019,11,27,4,e23df6,MarioNETte: Few-Shot Identity Preservation in Facial Reenactment,https://www.reddit.com/r/MachineLearning/comments/e23df6/marionette_fewshot_identity_preservation_in/,Yuqing7,1574797907,,0,1
1399,2019-11-27,2019,11,27,4,e23ezq,[P] Using StyleGAN to make a music visualizer,https://www.reddit.com/r/MachineLearning/comments/e23ezq/p_using_stylegan_to_make_a_music_visualizer/,AtreveteTeTe,1574798077,"I'm excited to share this generative video project I worked on with Japanese electronic music artist Qrion for the release of her Sine Wave Party EP.

Here's the first [generated video](https://www.youtube.com/watch?v=rqEaOL-TzDg) - two more coming out soon.

This was created using StyleGAN and doing a transfer learning with a custom dataset of images curated by the artist.  Qrion picked images that matched the mood of each song (things like clouds, lava hitting the ocean, forest interiors, and snowy mountains) and I generated interpolation videos for each track.

The tempo of the GAN evolution is controlled by a few different things: 
*the beat of the song
*a system I built that incorporates live input (you can tap the keyboard to add a jump to the playback in After Effects)
*a keyframeable overall playback speed fader system

I've also posted some [more images](http://www.nathanshipley.com/gan) I created with Qrion's custom models here on my site.  There are some further StyleGAN experiments there too if you're interested.

It's been fascinating to learn how to use StyleGAN like this.  As a visual effects artist, I'm over the moon with the sorts of things that are possible.  Indeed, I also wanted to shout out [/u/_C0D32_](https://www.reddit.com/user/_C0D32_) who shared an art-centered StyleGAN model that was really influential to me!  Thanks for that.  Also, of course, [/u/gwern](https://www.reddit.com/user/gwern/) who posted an incredible guide to using StyleGAN.",25,1
1400,2019-11-27,2019,11,27,4,e23gyy,Build and and Deploy a machine learning app to AWS from scratch: an end-to-end tutorial ,https://www.reddit.com/r/MachineLearning/comments/e23gyy/build_and_and_deploy_a_machine_learning_app_to/,ahmedbesbes,1574798297,,0,1
1401,2019-11-27,2019,11,27,5,e23irt,Challenges getting in shape / losing weight ?,https://www.reddit.com/r/MachineLearning/comments/e23irt/challenges_getting_in_shape_losing_weight/,techGeek_19232,1574798501,[removed],0,1
1402,2019-11-27,2019,11,27,5,e24e5k,[D] How can i elaborate texture and statistic features in CNN?,https://www.reddit.com/r/MachineLearning/comments/e24e5k/d_how_can_i_elaborate_texture_and_statistic/,Samatarou,1574801896,"I have a dataset 2200x34 where 1-33 column are features (texture and statistic) and 34th column is the class (0 or 1). I know my dataset is quite poor, but I splitted in 80% training set and 20% validation test.   
I'd like to use CNN for classification using these features, my steps are:

\- Splitting in training set and validation test;

\- Mean normalisation of features;

\- Reshaping training set and validation set in order to have 1760x34x1 and 440x34x1 as dimensions;

\- Create my model:

    opt = SGD(lr=0.0001)
    model = Sequential()
    model.add(Conv1D(16, 3, activation=""relu"", input_shape =(34,1)))
    model.add(BatchNormalization())
    model.add(MaxPooling1D(2))
    model.add(Conv1D(32, 3, activation=""relu""))
    model.add(MaxPooling1D(2))
    model.add(Flatten())
    model.add(Dense(512, activation=""relu""))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation=""sigmoid""))
    model.summary()
    # compile the model
    model.compile(loss='binary_crossentropy', optimizer= opt, metrics=['accuracy'])
    


Sadly my model has bad performance (acc = 55% more or less and loss = 0.69). Do you have any suggestion to increase my performance? Is there something wrong?  


Here the model.summary()

    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv1d_3 (Conv1D)            (None, 32, 16)            64        
    _________________________________________________________________
    batch_normalization_1 (Batch (None, 32, 16)            64        
    _________________________________________________________________
    max_pooling1d_3 (MaxPooling1 (None, 16, 16)            0         
    _________________________________________________________________
    conv1d_4 (Conv1D)            (None, 14, 32)            1568      
    _________________________________________________________________
    max_pooling1d_4 (MaxPooling1 (None, 7, 32)             0         
    _________________________________________________________________
    flatten_1 (Flatten)          (None, 224)               0         
    _________________________________________________________________
    dense_2 (Dense)              (None, 512)               115200    
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 512)               0         
    _________________________________________________________________
    dense_3 (Dense)              (None, 1)                 513       
    =================================================================",4,1
1403,2019-11-27,2019,11,27,6,e24iph,Classification accuracy based on top 3 most likely classifications,https://www.reddit.com/r/MachineLearning/comments/e24iph/classification_accuracy_based_on_top_3_most/,JakeBSc,1574802366,[removed],0,1
1404,2019-11-27,2019,11,27,6,e24p4n,Are chatbots viable source of passive income?,https://www.reddit.com/r/MachineLearning/comments/e24p4n/are_chatbots_viable_source_of_passive_income/,DreamOfDestiny,1574803043,[removed],0,1
1405,2019-11-27,2019,11,27,6,e25746,[P] Handwritten Text Recognition using Convolution Sequence to Sequence,https://www.reddit.com/r/MachineLearning/comments/e25746/p_handwritten_text_recognition_using_convolution/,spacevstab,1574804924,"[Convolution Seq-to-Seq](https://github.com/spaceVStab/ConvSeq2Seq-HTR)

Instead of using RNN with Seq-to-Seq modeling, CNN with Seq-to-Seq has been used which reduces the training and inference time. The work is novel when implemented around Dec 2018. The training and testing pipeline has been created for IAM handwitten dataset. Please provide some feedback on this project and its continuity since I would like to make further advancement and formally document the work into a article.",0,1
1406,2019-11-27,2019,11,27,7,e25ikj,"Launching TX, the spearhead of enterprise adoption of Streamr technology",https://www.reddit.com/r/MachineLearning/comments/e25ikj/launching_tx_the_spearhead_of_enterprise_adoption/,007moonboundnxs,1574806110,,0,1
1407,2019-11-27,2019,11,27,7,e25nws,Machine learning microscope adapts lighting to improve diagnosis,https://www.reddit.com/r/MachineLearning/comments/e25nws/machine_learning_microscope_adapts_lighting_to/,FindLight2017,1574806676,,0,1
1408,2019-11-27,2019,11,27,7,e25qeo,Possibly computer generated videos,https://www.reddit.com/r/MachineLearning/comments/e25qeo/possibly_computer_generated_videos/,cholehouse-eyre,1574806942,[removed],0,1
1409,2019-11-27,2019,11,27,8,e26ny8,I'm building a recommender system for events. Any interesting papers/ideas on this?,https://www.reddit.com/r/MachineLearning/comments/e26ny8/im_building_a_recommender_system_for_events_any/,zen_vangaever,1574810678,"I'm looking for cool ideas to try out. I've already implemented content-based recommendations, so that's not what I'm looking for. 

I'm having trouble with using peer information (collaborative filtering). This is mainly because there is only 'implicit feedback', which means: no reviews, likes, ratings,... (only attendence). Most collaborative filtering papers use the ratings of users to look for patterns between users. 

Are there any papers (or ideas from other redditors) that I can use for this collaborative filtering, with only implicit feedback?",0,1
1410,2019-11-27,2019,11,27,8,e26p8q,"Machine Learning: Logistic Regression, LDA &amp; K-NN in Python - Free Online Courses #Machine_Learning #Udemy",https://www.reddit.com/r/MachineLearning/comments/e26p8q/machine_learning_logistic_regression_lda_knn_in/,Free0nlineCourses,1574810831,,0,1
1411,2019-11-27,2019,11,27,9,e27sef,[R] Rigging the Lottery: Making All Tickets Winners,https://www.reddit.com/r/MachineLearning/comments/e27sef/r_rigging_the_lottery_making_all_tickets_winners/,hardmaru,1574815566,,7,1
1412,2019-11-27,2019,11,27,10,e28nao,Genetically Modified Babies Are Ethically OK,https://www.reddit.com/r/MachineLearning/comments/e28nao/genetically_modified_babies_are_ethically_ok/,frequenttimetraveler,1574819452,,0,1
1413,2019-11-27,2019,11,27,11,e292j5,Question about adversarial robustness,https://www.reddit.com/r/MachineLearning/comments/e292j5/question_about_adversarial_robustness/,cupsnake1988,1574821335,[removed],0,1
1414,2019-11-27,2019,11,27,12,e29orj,[D] [P] Machine Learning Applications for Classification of Propoganda Posts,https://www.reddit.com/r/MachineLearning/comments/e29orj/d_p_machine_learning_applications_for/,Showboo11,1574824142,"Hi! I was wondering if there have been any models / datasets that have been collected for creating a ML Model (RNN or some other architecture) that would be able to detect from text whether or not a specific text is propoganda from country A, B, or C.

&amp;#x200B;

In recent years, a lot of governments have employed people to make thousands of accounts and thousands of bots to disseminate misinformation. Are there any research papers or datasets that would be useful in this task?

&amp;#x200B;

Thanks!",4,1
1415,2019-11-27,2019,11,27,13,e2afb4,[D] What is the latest consensus on the effect of batch size on generalization?,https://www.reddit.com/r/MachineLearning/comments/e2afb4/d_what_is_the_latest_consensus_on_the_effect_of/,Minimum_Zucchini,1574827812,"In general is this true:


    small batch size + long training ~ large batch size + short training


Just wondering because I'm not aware of any standard literature on this topic, and if anyone knows any good papers I would appreciate some references!",15,1
1416,2019-11-27,2019,11,27,14,e2b3ta,Would training with many specific sub-domains help finding the optimal solution for the whole domain?,https://www.reddit.com/r/MachineLearning/comments/e2b3ta/would_training_with_many_specific_subdomains_help/,lendacerda,1574831535,[removed],0,1
1417,2019-11-27,2019,11,27,14,e2bhts,Alignment procedure,https://www.reddit.com/r/MachineLearning/comments/e2bhts/alignment_procedure/,zZThe_BreakerZz,1574833760,[removed],0,1
1418,2019-11-27,2019,11,27,15,e2bx5c,[R],https://www.reddit.com/r/MachineLearning/comments/e2bx5c/r/,benitorosenberg,1574836128,"&amp;#x200B;

![img](8eggn4spa6141)

&amp;#x200B;

[https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers](https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers)

A curated list of Monte Carlo tree search papers with implementations from the following conferences:

* Machine learning  
   * [NeurIPS](https://nips.cc/)
   * [ICML](https://icml.cc/)
* Computer vision  
   * [CVPR](http://cvpr2019.thecvf.com/)
   * [ICCV](http://iccv2019.thecvf.com/)
* Natural language processing  
   * [ACL](http://www.acl2019.org/EN/index.xhtml)
* Data  
   * [KDD](https://www.kdd.org/)
* Artificial intelligence  
   * [AAAI](https://www.aaai.org/)
   * [AISTATS](https://www.aistats.org/)
   * [IJCAI](https://www.ijcai.org/)
   * [UAI](http://www.auai.org/)",0,1
1419,2019-11-27,2019,11,27,15,e2byoo,Handmade Wedding Rings - WeddingRings.com,https://www.reddit.com/r/MachineLearning/comments/e2byoo/handmade_wedding_rings_weddingringscom/,weddingrings10,1574836363,,0,1
1420,2019-11-27,2019,11,27,15,e2c0nv,[R] A list of Monte Carlo tree search papers from major conferences,https://www.reddit.com/r/MachineLearning/comments/e2c0nv/r_a_list_of_monte_carlo_tree_search_papers_from/,benitorosenberg,1574836681,"&amp;#x200B;

https://preview.redd.it/2theaxlac6141.png?width=694&amp;format=png&amp;auto=webp&amp;s=3e0cbe5dd33f9ce95ebb057936df46c215df57f1

[https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers](https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers)

A curated list of Monte Carlo tree search papers with implementations from the following conferences:

* Machine learning  
   * [NeurIPS](https://nips.cc/)
   * [ICML](https://icml.cc/)
* Computer vision  
   * [CVPR](http://cvpr2019.thecvf.com/)
   * [ICCV](http://iccv2019.thecvf.com/)
* Natural language processing  
   * [ACL](http://www.acl2019.org/EN/index.xhtml)
* Data  
   * [KDD](https://www.kdd.org/)
* Artificial intelligence  
   * [AAAI](https://www.aaai.org/)
   * [AISTATS](https://www.aistats.org/)
   * [IJCAI](https://www.ijcai.org/)
   * [UAI](http://www.auai.org/)",1,1
1421,2019-11-27,2019,11,27,15,e2c83b,"When do the applications for MVA, Paris- Savlay program open for international students for fall 2020?",https://www.reddit.com/r/MachineLearning/comments/e2c83b/when_do_the_applications_for_mva_paris_savlay/,Terminally_scared,1574837878,"The website is very difficult to navigate since I do not know french. If any of you have additional information to share about the University, the Programm or the application process please feel free to do so. I am terribly lost at the moment but it's a very prestigious course and I do not want to miss the opportunity to apply there.",0,1
1422,2019-11-27,2019,11,27,16,e2ch4t,"[R] ""Single Headed Attention RNN: Stop Thinking With Your Head"": Take that Sesame Street!",https://www.reddit.com/r/MachineLearning/comments/e2ch4t/r_single_headed_attention_rnn_stop_thinking_with/,init__27,1574839297,"One of THE Best Papers that I've ever read (Both in terms of the research and the paper-writeup itself): 

[https://arxiv.org/pdf/1911.11423.pdf](https://arxiv.org/pdf/1911.11423.pdf)

&gt;The leading approaches in language modeling are all obsessed with TV shows of my youth - namely Transformers and Sesame Street. Transformers this, Transformers that, and over here a bonfire worth of GPU-TPU-neuromorphic wafer scale silicon. We opt for the lazy path of old and proven techniques with a fancy crypto inspired acronym: the Single Headed Attention RNN (SHA-RNN). The author's lone goal is to show that the entire field might have evolved a different direction if we had instead been obsessed with a slightly different acronym and slightly different result. We take a previously strong language model based only on boring LSTMs and get it to within a stone's throw of a stone's throw of state-of-the-art byte level language model results on enwik8. We also achieve state-of-the-art on WikiText-103 - or do we? This work has undergone no intensive hyperparameter optimization and lived entirely on a commodity desktop machine that made the author's small studio apartment far too warm in the midst of a San Franciscan summer. The final results are achievable in plus or minus 24 hours on a single GPU as the author is impatient. The attention mechanism is also readily extended to large contexts and requires minimal computation. Take that Sesame Street.",64,1
1423,2019-11-27,2019,11,27,16,e2cu2b,[D] can you still use variable importance of a bad model?,https://www.reddit.com/r/MachineLearning/comments/e2cu2b/d_can_you_still_use_variable_importance_of_a_bad/,abjeroen,1574841469,"If I created a model with a bad accuracy, sensitivity etc., how much value can you give the variable importance? Is it nonsense, since he model is bad or does it tell you stuff based on which you can improve?",1,1
1424,2019-11-27,2019,11,27,17,e2cyi8,[R] Single Headed Attention RNN: Stop Thinking With Your Head. Gets near SOTA on enwik8 in hours using a single GPU.,https://www.reddit.com/r/MachineLearning/comments/e2cyi8/r_single_headed_attention_rnn_stop_thinking_with/,hardmaru,1574842248,,1,1
1425,2019-11-27,2019,11,27,17,e2dacr,Any idea what type of GAN the app called 'Remini Photo Enhancer' uses?,https://www.reddit.com/r/MachineLearning/comments/e2dacr/any_idea_what_type_of_gan_the_app_called_remini/,AlexOptimal,1574844394,[removed],0,1
1426,2019-11-27,2019,11,27,18,e2dm75,[R] Preventing undesirable behavior of intelligent machines (published in 'Science' journal),https://www.reddit.com/r/MachineLearning/comments/e2dm75/r_preventing_undesirable_behavior_of_intelligent/,msamwald,1574846501,,0,1
1427,2019-11-27,2019,11,27,18,e2dxc0,Just wrote this for towards data science - Been learning TDA for noise proofing.,https://www.reddit.com/r/MachineLearning/comments/e2dxc0/just_wrote_this_for_towards_data_science_been/,rocketpythontutors,1574848490,,0,1
1428,2019-11-27,2019,11,27,19,e2e42f,[R] AI takes on popular Minecraft game in machine-learning contest,https://www.reddit.com/r/MachineLearning/comments/e2e42f/r_ai_takes_on_popular_minecraft_game_in/,MasterScrat,1574849670,,0,1
1429,2019-11-27,2019,11,27,19,e2ei1a,Know difference between image segmentations and classifications in image processing,https://www.reddit.com/r/MachineLearning/comments/e2ei1a/know_difference_between_image_segmentations_and/,buzztowns,1574852147,,0,1
1430,2019-11-27,2019,11,27,20,e2eksx,Concepts of Data Preprocessing in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/e2eksx/concepts_of_data_preprocessing_in_machine_learning/,mlheadredditor,1574852631,[removed],0,1
1431,2019-11-27,2019,11,27,20,e2ernl,Microsoft Releases DialogGPT AI Conversation Model,https://www.reddit.com/r/MachineLearning/comments/e2ernl/microsoft_releases_dialoggpt_ai_conversation_model/,maxtility,1574853857,,0,1
1432,2019-11-27,2019,11,27,20,e2eubf,Ever heard of Active Learning?,https://www.reddit.com/r/MachineLearning/comments/e2eubf/ever_heard_of_active_learning/,mlobo1997,1574854385,[removed],0,1
1433,2019-11-27,2019,11,27,20,e2ezmo,[Post] Why tf.keras? one ring to rule them all!,https://www.reddit.com/r/MachineLearning/comments/e2ezmo/post_why_tfkeras_one_ring_to_rule_them_all/,cdossman,1574855319,"Whether youre a seasoned researcher or simply a newcomer, the complexity of the TensorFlow ecosystem will likely overwhelm you. To lower the bar of entry, TensorFlow developers have placed **tf.keras** centerstage. 

 [https://medium.com/@lsgrep/tensorflow-1-0-vs-2-0-part-3-tf-keras-ea403bd752c0](https://medium.com/@lsgrep/tensorflow-1-0-vs-2-0-part-3-tf-keras-ea403bd752c0)",0,1
1434,2019-11-27,2019,11,27,20,e2f06b,"Learn more about the performance of distributed TensorFlow in a multi-node and multi-GPU configuration, running on an Amazon EC2 cluster. Download the research paper.",https://www.reddit.com/r/MachineLearning/comments/e2f06b/learn_more_about_the_performance_of_distributed/,benjamin_brook,1574855426,,0,1
1435,2019-11-27,2019,11,27,21,e2fh2z,Ways of estimating data probability distribution and generating similar data,https://www.reddit.com/r/MachineLearning/comments/e2fh2z/ways_of_estimating_data_probability_distribution/,terraregina,1574858349,[removed],1,1
1436,2019-11-27,2019,11,27,21,e2fm6b,Can I submit a paper that I presented at a conference to arXiv?,https://www.reddit.com/r/MachineLearning/comments/e2fm6b/can_i_submit_a_paper_that_i_presented_at_a/,agent--zero,1574859184,[removed],0,1
1437,2019-11-27,2019,11,27,21,e2fnp3,10 Advanced Research Topics in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/e2fnp3/10_advanced_research_topics_in_machine_learning/,writethesis,1574859426,,0,1
1438,2019-11-27,2019,11,27,22,e2fzjg,Machine Learning on Encrypted Data Without Decrypting It,https://www.reddit.com/r/MachineLearning/comments/e2fzjg/machine_learning_on_encrypted_data_without/,HN_Crosspost_Bot,1574861234,,0,1
1439,2019-11-27,2019,11,27,22,e2g1tk,Rob Percival &amp; Anthony NG's Complete Machine Learning Course,https://www.reddit.com/r/MachineLearning/comments/e2g1tk/rob_percival_anthony_ngs_complete_machine/,iphone6plususer,1574861558,,0,1
1440,2019-11-27,2019,11,27,22,e2gd79,Online surveys,https://www.reddit.com/r/MachineLearning/comments/e2gd79/online_surveys/,maximusgibus,1574863141,[removed],0,1
1441,2019-11-27,2019,11,27,23,e2gf1f,[R] Study on 299 data sets shows that non linear SVMs/ANNs outperform linear SVMs/ANNs only in 20 per cent of cases,https://www.reddit.com/r/MachineLearning/comments/e2gf1f/r_study_on_299_data_sets_shows_that_non_linear/,pppeer,1574863374,"When do non-linear versions of algorithms such as SVMs or neural networks outperform linear methods at a statistically significant level? We researched this question by running experiments on 299 data sets in OpenML. Results show that only in around 20 per cent of cases non linear results are better at a significant level. We also investigated this question deeper by looking at for what type of data sets this happens by looking at number of instances, features and building meta learning models. 

Reference:

Benjamin Strang, Peter van der Putten, Jan N. van Rijn and Frank Hutter. [Don't Rule Out Simple Models Prematurely: a Large Scale Benchmark Comparing Linear and Non-linear Classifiers in OpenML.](http://liacs.leidenuniv.nl/~rijnjnvan/pdf/pub/ida2018.pdf) In: Seventeenth International Symposium on Intelligent Data Analysis (IDA), 2018",29,1
1442,2019-11-27,2019,11,27,23,e2giis,[R] SuperGlue: Learning Feature Matching with Graph Neural Networks,https://www.reddit.com/r/MachineLearning/comments/e2giis/r_superglue_learning_feature_matching_with_graph/,youali,1574863830,"Arxiv: https://arxiv.org/abs/1911.11763

Abstract: This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems.",1,1
1443,2019-11-27,2019,11,27,23,e2gnpr,Need advice from experience ML engineer?,https://www.reddit.com/r/MachineLearning/comments/e2gnpr/need_advice_from_experience_ml_engineer/,AbhiDelhi,1574864503,"So I want to know how often do you implement algorithm from scratch while building Machine learning system? In my internship, they, my boss, ask me to develop a ML model which will fetch 8 entities (Name, Location, email ID etc) from resume, now for that I use Spacy NER and trained 200 resume with annotation (with 8 entities) after that I get accuracy of some 50-60 percent. Now of course, we can't use this model. My boss told me you need to develop your algorithm for that to get those entities. He is not happy with those who only know how to use library. I don't think there's anything wrong, if work can be done.
I want to how often do you implement algorithm from scratch?",0,1
1444,2019-11-28,2019,11,28,0,e2hao3,About Cross Domain Recommender System,https://www.reddit.com/r/MachineLearning/comments/e2hao3/about_cross_domain_recommender_system/,munjalsourav,1574867404,[removed],0,1
1445,2019-11-28,2019,11,28,0,e2hdyw,"Would it be possible to use machine learning to predict alcohol or other substance use, only with speech?",https://www.reddit.com/r/MachineLearning/comments/e2hdyw/would_it_be_possible_to_use_machine_learning_to/,Wiemel,1574867813,[removed],0,1
1446,2019-11-28,2019,11,28,0,e2hurp,Could anyone interpret this for me? what all those percentage and color mean? it would be really helpful .. thanks in advance,https://www.reddit.com/r/MachineLearning/comments/e2hurp/could_anyone_interpret_this_for_me_what_all_those/,thelostseeker_,1574869841,,0,1
1447,2019-11-28,2019,11,28,0,e2hvuz,"Simple Questions Thread November 27, 2019",https://www.reddit.com/r/MachineLearning/comments/e2hvuz/simple_questions_thread_november_27_2019/,AutoModerator,1574869966,[removed],0,1
1448,2019-11-28,2019,11,28,0,e2hwu0,Instance segmentation mask R-CNN change backbone - fine tuning,https://www.reddit.com/r/MachineLearning/comments/e2hwu0/instance_segmentation_mask_rcnn_change_backbone/,ser17m,1574870080,[removed],0,1
1449,2019-11-28,2019,11,28,1,e2i6bv,Resources for Understanding how to use coefficients of Logistic Regression from SciKit Learn,https://www.reddit.com/r/MachineLearning/comments/e2i6bv/resources_for_understanding_how_to_use/,lebeer13,1574871116,"Hey r/MachineLearning,

I am working with the Telco Churn Data set from Kaggle. An LR model built off that data gets pretty close to 80% accuracy right off the bat. But i'm having trouble understanding how to interpret what the model is telling me, I want to understand the relative importance levels of each feature.

I call model.coef\_ and it gives me a list of numbers that are the coefficients the model used. Are these in the same order that the training/test data was in? data features left to right is the same as coefficients list top to bottom?

Secondly, what do i do with the coefficients? Is it like R\^2 in Linear Regression, they'll sum to 1 assuming you can explain all the variance in the data? 

Any resources you could point me to would be appreciated as well!",0,1
1450,2019-11-28,2019,11,28,1,e2icea,PyTorch  A Savior Deep Learning Framework,https://www.reddit.com/r/MachineLearning/comments/e2icea/pytorch_a_savior_deep_learning_framework/,iramirsina,1574871796,[removed],0,1
1451,2019-11-28,2019,11,28,1,e2ick2,Types of Machine Learning and how they differe,https://www.reddit.com/r/MachineLearning/comments/e2ick2/types_of_machine_learning_and_how_they_differe/,navin49,1574871811,,0,1
1452,2019-11-28,2019,11,28,1,e2idcd,types of machine learning and how they differ,https://www.reddit.com/r/MachineLearning/comments/e2idcd/types_of_machine_learning_and_how_they_differ/,navin49,1574871894,,0,1
1453,2019-11-28,2019,11,28,1,e2ie8i,Showing interpolation to animations using DAIN,https://www.reddit.com/r/MachineLearning/comments/e2ie8i/showing_interpolation_to_animations_using_dain/,CloverDuck,1574871987,,1,1
1454,2019-11-28,2019,11,28,2,e2ixt5,Rules/Actions in expert system from a NN,https://www.reddit.com/r/MachineLearning/comments/e2ixt5/rulesactions_in_expert_system_from_a_nn/,tensorfuser,1574874107,[removed],0,1
1455,2019-11-28,2019,11,28,2,e2j5wb,Go champion Lee Se-dol beaten by DeepMinds AlphaGo retires after declaring AI invincible,https://www.reddit.com/r/MachineLearning/comments/e2j5wb/go_champion_lee_sedol_beaten_by_deepminds_alphago/,ilikepancakez,1574874952,[removed],0,1
1456,2019-11-28,2019,11,28,2,e2jj8b,[D] Go champion Lee Se-dol beaten by DeepMind retires after declaring AI invincible,https://www.reddit.com/r/MachineLearning/comments/e2jj8b/d_go_champion_lee_sedol_beaten_by_deepmind/,ilikepancakez,1574876352,"[https://en.yna.co.kr/view/AEN20191127004800315](https://en.yna.co.kr/view/AEN20191127004800315)

Announced today in South Korea, and its made me think on the sort of impact that these things will have on people in the coming days. Theres definitely a great deal of good that can be achieved, with innovation/growth and so many opportunities in general for the companies and people involved in this work.

But at the same time, it is kind of sad to see some of the human element get left behind. Im sure Lee Se-dol could have played for many more years if he wanted to, continuing to contribute greatly to the professional Go scene as a player.

This is something that I wonder then, if people working at companies like Google / DeepMind should be thinking about. Im sure the growing profit margins and money thats flowing in from all our work is more than satisfactory for the company leadership / investors to not have any issues with all this. As the engineers responsible for actually building everything though, is their any kind of ethical consideration on our parts that we need to recognize? I dont know myself to be honest. But I am curious as to what you all think here in the [r/machinelearning](https://www.reddit.com/r/machinelearning/) community.",168,1
1457,2019-11-28,2019,11,28,2,e2jur5,"[R] New DeepMind Nature paper: ""Optimizing agent behavior over long time scales by transporting value""",https://www.reddit.com/r/MachineLearning/comments/e2jur5/r_new_deepmind_nature_paper_optimizing_agent/,gohu_cd,1574877575,,0,1
1458,2019-11-28,2019,11,28,3,e2jv5m,[N] How to Build a Semantic Search Engine in 3 minutes,https://www.reddit.com/r/MachineLearning/comments/e2jv5m/n_how_to_build_a_semantic_search_engine_in_3/,colethienes,1574877618,,0,1
1459,2019-11-28,2019,11,28,3,e2k1y2,Machine Learning for Fraud Prevention,https://www.reddit.com/r/MachineLearning/comments/e2k1y2/machine_learning_for_fraud_prevention/,analyticsinsight,1574878307,,0,1
1460,2019-11-28,2019,11,28,3,e2kf4d,Simple tutorial for distilling BERT,https://www.reddit.com/r/MachineLearning/comments/e2kf4d/simple_tutorial_for_distilling_bert/,pahanoid2k,1574879709,,0,1
1461,2019-11-28,2019,11,28,3,e2km5l,Lessons learned building an ML trading system that turned $5k into $200k,https://www.reddit.com/r/MachineLearning/comments/e2km5l/lessons_learned_building_an_ml_trading_system/,HN_Crosspost_Bot,1574880448,,0,1
1462,2019-11-28,2019,11,28,4,e2kvic,Did I find anything useful?,https://www.reddit.com/r/MachineLearning/comments/e2kvic/did_i_find_anything_useful/,partitionist,1574881419,[removed],0,1
1463,2019-11-28,2019,11,28,4,e2kzbc,[D] Should we start charging for interviews as candidates?,https://www.reddit.com/r/MachineLearning/comments/e2kzbc/d_should_we_start_charging_for_interviews_as/,EnterOblivionS,1574881806,"I've been interviewing for a few months now with various companies. I have a PhD in ML and 5 years of experience in academia and industry, and been actively publishing in prestigious ML and CV conferences and journals for the past 5 years.

During my very frustrating job hunt, I managed to get interviews with 20+ companies, startups, and academic/industrial collaborative institutions. Apart from a couple of the interviews that I quickly discovered wouldn't be a good fit for my future, all the interviews went really well, and I communicated well with the recruiting management and the recruiters. I travelled across country, paid for accommodation, paid for interview attire (we wear T-shirts to work), got unpaid leave for many days from my current employer, and as such lost lots of money out of my pocket to get a job; not to mention the many hours I had to spend preparing for the interviews, showing up to the interviews, waiting for the recruiter or hiring committee to not show up and then reschedule the whole thing, etc.

I also spend both priceless time doing coding interviews or more time-consuming take home challenges to solve. I paid out of my pocket for Google cloud to do the training for the coding challenges. More often than not I felt me doing the interview is actually offering the company a ""FREE CONSULTATION GIG"", rather than them wanting to know more about my background, my resume, and gauge my experience and abilities, and see if I'd be a good cultural fit for their team.

All that to hear firsthand from the hiring manager or my connections (friends who work in the said company) that internal candidate got prioritized, or they can't offer me work permit sponsorship, or the hiring manager wasn't too sure if they really want to hire someone at the moment, or get hit by a country-wide hiring freeze because of elections (Canada).

My search still continues for the right job, but I've been thinking of starting to send bills to companies that outright waste candidates' time and money, and don't offer any compensation for accommodation and travel.

What do you think? Are we selling ourselves too cheaply these days or is this a norm and I should get used to getting treated like this when I'm interviewing?",67,1
1464,2019-11-28,2019,11,28,4,e2kzii,Google &amp; Johns Hopkins University | Can Adversarial Examples Improve Image Recognition?,https://www.reddit.com/r/MachineLearning/comments/e2kzii/google_johns_hopkins_university_can_adversarial/,Yuqing7,1574881825,,0,1
1465,2019-11-28,2019,11,28,4,e2l2s2,How to turn a list of tags into features?,https://www.reddit.com/r/MachineLearning/comments/e2l2s2/how_to_turn_a_list_of_tags_into_features/,scribe36,1574882175,[removed],0,1
1466,2019-11-28,2019,11,28,4,e2l31i,[D] Did I find anything useful?,https://www.reddit.com/r/MachineLearning/comments/e2l31i/d_did_i_find_anything_useful/,partitionist,1574882201,"Hello everyone!

I found an algorithm that allows me to solve the problem of the optimal partition quite(very) quickly. For any type of score (gini index of the target for example) I can find the partition of the input that minimizes it (even on really big datasets). I remind you that in the case of decision trees this optimal partition is not always reached because the algorithm is greedy.

Since I'm not a data-scientist, but rather a combinatorial optimization specialist, I'd like to know if it's really a discovery or it's already been done, for example in decision trees implementations.",8,1
1467,2019-11-28,2019,11,28,4,e2llln,Kinetics-400,https://www.reddit.com/r/MachineLearning/comments/e2llln/kinetics400/,HappyKaleidoscope8,1574884102,[removed],0,1
1468,2019-11-28,2019,11,28,5,e2m4my,"Comparing UK ML-Neuroscience Labs: Deepmind, Google Brain, Microsoft.",https://www.reddit.com/r/MachineLearning/comments/e2m4my/comparing_uk_mlneuroscience_labs_deepmind_google/,Napoleon-1804,1574886125,[removed],0,1
1469,2019-11-28,2019,11,28,5,e2ma3q,"[Discussion] Comparing UK ML-Neuro Labs: Deepmind, Google Brain, Microsoft Cambridge, etc.",https://www.reddit.com/r/MachineLearning/comments/e2ma3q/discussion_comparing_uk_mlneuro_labs_deepmind/,Napoleon-1804,1574886723,"I'm a masters student in the UK hoping to get 1-2 years of research experience at the intersection of machine learning and neuroscience (so not so much image recognition, speech recognition, etc.)

The two academic options that immediately came to mind were the Gatsby Comp. Neuroscience at UCL and Cambridge's Computational Biological Learning group. I'm not familiar with industry labs and would like your help in comparing Deepmind, Google Brain (UK site), Microsoft Research Cambridge, etc. Which one of these offers the most relevant research in terms of machine learning applied to neuroscience as I am hoping to enter academia for computational neuroscience? A lab that publishes well is also a nice plus as always. Thanks.",11,1
1470,2019-11-28,2019,11,28,5,e2moii,"AI/ML Examples for Image and Binary Classification. Uses Python with TensorFlow, Flask, scikit-learn, and JavaScript.",https://www.reddit.com/r/MachineLearning/comments/e2moii/aiml_examples_for_image_and_binary_classification/,__app_dev__,1574888281,,0,1
1471,2019-11-28,2019,11,28,6,e2mslp,Is there a mobile app for NeurIPS 2019 ?,https://www.reddit.com/r/MachineLearning/comments/e2mslp/is_there_a_mobile_app_for_neurips_2019/,curiousHomoSapien,1574888725,[removed],0,1
1472,2019-11-28,2019,11,28,6,e2n9b0,tf-idf weighting for document collections,https://www.reddit.com/r/MachineLearning/comments/e2n9b0/tfidf_weighting_for_document_collections/,emilywrennh,1574890541,Does anyone with NLE/NLP experience have recommendations for applying a tf-idf weighting to compare two document collections? I'm currently stuck on building a term frequency function and would appreciate any input. Thanks in advance!,0,1
1473,2019-11-28,2019,11,28,6,e2nge8,"One Month, 500,000 Face Scans: How China Is Using A.I. to Profile a Minority",https://www.reddit.com/r/MachineLearning/comments/e2nge8/one_month_500000_face_scans_how_china_is_using_ai/,EnemyAsmodeus,1574891339,,1,1
1474,2019-11-28,2019,11,28,6,e2nk4i,Synthetic data generation and over-fitting,https://www.reddit.com/r/MachineLearning/comments/e2nk4i/synthetic_data_generation_and_overfitting/,Theweekendstate,1574891750,"I've been playing around with a CNN that takes in images of ellipses and spits out their area. In order to train my network, I'm using synthetically generated ellipse images, of which there are 3 parameters: rotation angle, major radius, and minor radius. As well, I add (Gaussian) noise to the images. I can freely vary these parameters, and generate as many images as I wish.

&amp;#x200B;

What I'm trying to understand better is how best to ""distribute"" my ellipse generation. For example, generating the ""same"" ellipse a million times isn't going to help my network learn - obviously, I need to explore the parameter space in some way. So is it better to generate 5 different ellipses, each repeated 100 times, or 100 different ellipses, each repeated 5 times? Or, maybe just generate 500 different, random, ellipses altogether (uniformly sampling the parameter space)?

&amp;#x200B;

Moreover, how does the noise play into this? I understand that having exact duplicates in my dataset isn't useful, but, does this change when they're not literal duplicates due to noise?

&amp;#x200B;

(The connection to the title here is that my model is consistently overfitting the data I feed it. It seems to learn the ellipses it sees, but craps out when presented with new ones. I understand there are hyperparameter factors that contribute here too, but I want to make sure my input data is appropriate to begin with)",0,1
1475,2019-11-28,2019,11,28,7,e2nz13,Hardware recommendation needed - used workstation/server for number crunching,https://www.reddit.com/r/MachineLearning/comments/e2nz13/hardware_recommendation_needed_used/,wornbrain,1574893513,,0,1
1476,2019-11-28,2019,11,28,7,e2o2f4,[Q] Self-training with Noisy Student improves ImageNet classification (STNS),https://www.reddit.com/r/MachineLearning/comments/e2o2f4/q_selftraining_with_noisy_student_improves/,idg101,1574893918,[removed],0,1
1477,2019-11-28,2019,11,28,9,e2pbhk,faces4coco dataset released: Face bounding box annotations for the MSCOCO Images dataset,https://www.reddit.com/r/MachineLearning/comments/e2pbhk/faces4coco_dataset_released_face_bounding_box/,frankcarey,1574899265,,0,1
1478,2019-11-28,2019,11,28,9,e2pdt7,"Question Regarding ""Deep Convolutional Spiking Neural Networks for Image Classification""",https://www.reddit.com/r/MachineLearning/comments/e2pdt7/question_regarding_deep_convolutional_spiking/,bitcoin_analysis_app,1574899561,[removed],0,1
1479,2019-11-28,2019,11,28,9,e2pi0q,[Project] faces4coco dataset released: Face bounding box annotations for the MSCOCO Images dataset,https://www.reddit.com/r/MachineLearning/comments/e2pi0q/project_faces4coco_dataset_released_face_bounding/,frankcarey,1574900046,"Github: [https://github.com/ACI-Institute/faces4coco](https://github.com/ACI-Institute/faces4coco)

Over half of the 120,000 images in the 2017 [COCO(Common Objects in Context)](http://cocodataset.org/) dataset contain people, and while COCO's bounding box annotations include some 90 different classes, there is only one class for people. Those bounding boxes encompass the entire body of the person (head, body, and extremities), but being able to detect and isolate specific parts is useful and has many applications in machine learning. Detecting faces in particular is useful, so we've created a dataset that adds faces to COCO.",0,1
1480,2019-11-28,2019,11,28,9,e2popg,[D] Has anyone used context to improve object detection and image classification?,https://www.reddit.com/r/MachineLearning/comments/e2popg/d_has_anyone_used_context_to_improve_object/,nitrodolphin,1574900918,"We do a lot of image classification using the Tensorflow Object Detection API.

Our images often appear in groups, e.g. a cluster of fish swimming by a camera. Ofter our model will recognize some of the fish - but not all of them. This is obviously a mistake a human would never make.

I am researching whether there are any examples of object detectors/image classifiers using context to improve results? I.e. knowing that there are three fish swimming through would increase the model's propensity to find a fourth nearby?

Another way to potentially attack this problem would be to identify clusters of objects -&gt; then reexamine the cluster only to identify the number of objects within the cluster. The complexity with this issue is that some of the objects we are examining appear in clusters, while others do not.

So to summarize, my two questions are:

* Is there research on, and how would you suggest I go about improving an object detector by using contextual features within an image?
* In situations where there are clusters of objects, would you recommend recognizing those clusters as individual images and then subsequently processing them to identify the number of images within the cluster? Are there examples / is there research on this?

Thanks in advance - obviously doing my own research as well, but keen to hear if the community has any thoughts/examples!",5,1
1481,2019-11-28,2019,11,28,10,e2qany,What is the state of the art for disentanglement?,https://www.reddit.com/r/MachineLearning/comments/e2qany/what_is_the_state_of_the_art_for_disentanglement/,runvnc,1574903795,[removed],0,1
1482,2019-11-28,2019,11,28,10,e2qbra,[D] What is the state of the art for disentanglement?,https://www.reddit.com/r/MachineLearning/comments/e2qbra/d_what_is_the_state_of_the_art_for_disentanglement/,runvnc,1574903935," I just found out about disentangled variational autoencoders and they seem pretty exciting to me.

I was wondering if there was something similar with GANs and so I searched and found this [https://arxiv.org/abs/1906.06034](https://arxiv.org/abs/1906.06034) ""InfoGAN-CR: Disentangling Generative Adversarial Networks with Contrastive Regularizers"".

What  should I search for in order to find papers that have the best  performance, or is there a new technique for disentanglement (or  something along those lines)?

Thanks.",8,1
1483,2019-11-28,2019,11,28,11,e2r4yq,Does anyone know of any data set to predict heart attack or stroke from raw heart rate or eeg signal,https://www.reddit.com/r/MachineLearning/comments/e2r4yq/does_anyone_know_of_any_data_set_to_predict_heart/,dasante78,1574907715,[removed],1,1
1484,2019-11-28,2019,11,28,11,e2r79b,Is this a good system for my first machine learning project?,https://www.reddit.com/r/MachineLearning/comments/e2r79b/is_this_a_good_system_for_my_first_machine/,lostwhitewalker,1574907994,[removed],0,1
1485,2019-11-28,2019,11,28,12,e2rqiw,Chess Go and,https://www.reddit.com/r/MachineLearning/comments/e2rqiw/chess_go_and/,MegavirusOfDoom,1574910376,"What about folding@home? Its 3d with many angular variations, and many orders of magnitude more tricky than Go... so when will it be solved and how many goalposts is AI going to move every year, what are your predictions? AI doctors? AI music? Which are the tasks which represent upcoming goalposts for AI?",0,1
1486,2019-11-28,2019,11,28,12,e2s2ds,     - Duballclub,https://www.reddit.com/r/MachineLearning/comments/e2s2ds/_____duballclub/,brynnconsortieg,1574911958,,0,1
1487,2019-11-28,2019,11,28,13,e2si8b,"Need help with data prep, matching labels in text file with jpegs",https://www.reddit.com/r/MachineLearning/comments/e2si8b/need_help_with_data_prep_matching_labels_in_text/,str8cokane,1574914178,[removed],0,1
1488,2019-11-28,2019,11,28,13,e2sr74,Ideas how to make jarvis tts?,https://www.reddit.com/r/MachineLearning/comments/e2sr74/ideas_how_to_make_jarvis_tts/,Exitio_Interfectorem,1574915586,[removed],0,1
1489,2019-11-28,2019,11,28,13,e2swxo,"Need a latest housing dataset with at least 25-30 features. Can someone suggest a source , I cant to find one , Dont want the kaggle one. Please help!!",https://www.reddit.com/r/MachineLearning/comments/e2swxo/need_a_latest_housing_dataset_with_at_least_2530/,EscanorLSP,1574916486,[removed],0,1
1490,2019-11-28,2019,11,28,13,e2t19t,How can I learn everything we know about Distributed Sensorimotor Causal Inference?,https://www.reddit.com/r/MachineLearning/comments/e2t19t/how_can_i_learn_everything_we_know_about/,Stack3,1574917146,[removed],0,1
1491,2019-11-28,2019,11,28,14,e2tgw2,Bot Marketplace - 5,https://www.reddit.com/r/MachineLearning/comments/e2tgw2/bot_marketplace_5/,getengati,1574919574,[removed],0,1
1492,2019-11-28,2019,11,28,15,e2tq7d,Disposable Paper Straw Making Machine Supplier in Kolkata,https://www.reddit.com/r/MachineLearning/comments/e2tq7d/disposable_paper_straw_making_machine_supplier_in/,machineryspb,1574921072,[removed],0,1
1493,2019-11-28,2019,11,28,15,e2twod,Who is actually gaining value from Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/e2twod/who_is_actually_gaining_value_from_machine/,greatlakes7,1574922102,[removed],0,1
1494,2019-11-28,2019,11,28,16,e2ujb5,Kaggle 2019 survey USA vs INDIA,https://www.reddit.com/r/MachineLearning/comments/e2ujb5/kaggle_2019_survey_usa_vs_india/,sahibpreet,1574925643,,0,1
1495,2019-11-28,2019,11,28,16,e2una0,[R] Contrastive Learning of Structured World Models,https://www.reddit.com/r/MachineLearning/comments/e2una0/r_contrastive_learning_of_structured_world_models/,triplefloat,1574926293,,20,1
1496,2019-11-28,2019,11,28,17,e2v02j,Sales &amp; Marketing,https://www.reddit.com/r/MachineLearning/comments/e2v02j/sales_marketing/,Zachieeee,1574928417,,0,1
1497,2019-11-28,2019,11,28,17,e2v38h,Best model for anomaly detection with large feature and sample set?,https://www.reddit.com/r/MachineLearning/comments/e2v38h/best_model_for_anomaly_detection_with_large/,redditperson24,1574928979,[removed],0,1
1498,2019-11-28,2019,11,28,18,e2vkix,Best open source ML/DL experiment management tool,https://www.reddit.com/r/MachineLearning/comments/e2vkix/best_open_source_mldl_experiment_management_tool/,thak123,1574932112,[removed],0,1
1499,2019-11-28,2019,11,28,18,e2vomz,Machine Learning Tutorial for Beginners,https://www.reddit.com/r/MachineLearning/comments/e2vomz/machine_learning_tutorial_for_beginners/,nikita-singh,1574932829,,0,1
1500,2019-11-28,2019,11,28,18,e2vtpa,[P] Predict figure skating world championship ranking from season performances (part 6: rank aggregation),https://www.reddit.com/r/MachineLearning/comments/e2vtpa/p_predict_figure_skating_world_championship/,seismatica,1574933761,"I'm trying to predict the ranking of figure skaters in the annual world championship by their scores in earlier competition events in the season. The obvious method to do is by average the scores for each skater across past events and rank them by those averages. However, since no two events are the same, the goal for my project is to separate the **skater effect**, the intrinsic ability of each skater, by the **event effect**, how an event influence the score of a skater.

In the previous 5 parts of my projects, I've developed several models to predict the ranking of skaters (as outlined in an earlier Reddit [post](https://www.reddit.com/r/MachineLearning/comments/e0jepq/p_predict_figure_skating_world_championship/)). In this last part of my project, I try to combine these rankings into a final ranking that hopefully will be more accurate than any of the previous rankings individually. You can read the write-up for it [here](https://medium.com/@seismatica/predict-figure-skating-world-championship-ranking-from-season-performances-d97bfbd37807).

I used two different approach to combine the rankings:

* An unsupervised approach using the centuries-old method of [Borda count](https://en.wikipedia.org/wiki/Borda_count) that is used to tally ranked votes.

* A supervised approach using logistic regression to combine the scores from each model more intelligently, using the world championship itself as a guide.

Finally, all of the 7 ranking models that I developed in my project are benchmarked on the 5 seasons in the test set. I won't spoil the details and explanations of the final result (you can see a glimpse of it [here](https://i.imgur.com/5yVZzNo.png)), but let's just say that predicting sports is hard AF!

You can check out the Github [repo](https://github.com/dknguyengit/skate_predict) of the project for all my analyses. I'm more than happy to answer any question or feedback you might have for my project. Thank you for taking the time to read it.",0,1
1501,2019-11-28,2019,11,28,18,e2vv9d,Artificial Intelligence Future and Present in Manufacturing Industry,https://www.reddit.com/r/MachineLearning/comments/e2vv9d/artificial_intelligence_future_and_present_in/,mantha_anirudh,1574934047,,0,1
1502,2019-11-28,2019,11,28,19,e2wday,Basic underlying assumption: Fixed input nature,https://www.reddit.com/r/MachineLearning/comments/e2wday/basic_underlying_assumption_fixed_input_nature/,maddy_progs,1574937516,[removed],0,1
1503,2019-11-28,2019,11,28,20,e2woyt,"In Nature Machine Intelligence, the scientists show that bots are more successful than humans in certain human-machine interactions -- but only if they are allowed to hide their non-human identity.",https://www.reddit.com/r/MachineLearning/comments/e2woyt/in_nature_machine_intelligence_the_scientists/,day1technologies,1574939757,[removed],0,1
1504,2019-11-28,2019,11,28,20,e2wpha,SONM powers up its fog computing platform to give users a marketplace where buying and renting computing power is possible,https://www.reddit.com/r/MachineLearning/comments/e2wpha/sonm_powers_up_its_fog_computing_platform_to_give/,007moonboundnxs,1574939851,,0,1
1505,2019-11-28,2019,11,28,21,e2xlsd,[P] How emojis are used on Twitter (link in comments),https://www.reddit.com/r/MachineLearning/comments/e2xlsd/p_how_emojis_are_used_on_twitter_link_in_comments/,enric94,1574945413,,1,1
1506,2019-11-28,2019,11,28,22,e2y0xk,Can someone please explain to a beginner to machine learning the code here is doing?,https://www.reddit.com/r/MachineLearning/comments/e2y0xk/can_someone_please_explain_to_a_beginner_to/,hydrogenproton,1574947740,[removed],0,1
1507,2019-11-28,2019,11,28,23,e2yexr,Stock Price Prediction using LSTM in Python with scikit-learn,https://www.reddit.com/r/MachineLearning/comments/e2yexr/stock_price_prediction_using_lstm_in_python_with/,saruque,1574949834,,0,1
1508,2019-11-28,2019,11,28,23,e2ysi9,"[Blog post] Introduction of Adversarial Examples Improve Image Recognition , ImageNet SOTA method using Adversarial Training",https://www.reddit.com/r/MachineLearning/comments/e2ysi9/blog_post_introduction_of_adversarial_examples/,akira_AI,1574951710,[removed],0,1
1509,2019-11-28,2019,11,28,23,e2yw3k,How is this not the most intriguing tool in the world?!,https://www.reddit.com/r/MachineLearning/comments/e2yw3k/how_is_this_not_the_most_intriguing_tool_in_the/,bubbleburst1,1574952190,,0,1
1510,2019-11-29,2019,11,29,0,e2zbi4,[D] Self-training with Noisy Student improves ImageNet classification (STNS),https://www.reddit.com/r/MachineLearning/comments/e2zbi4/d_selftraining_with_noisy_student_improves/,idg101,1574954262," A few questions about this paper:

1. If you train an ensemble of SOTA architectures on Imagenet and average their results, do you beat STNS?
2. Why  not fine tune the teacher?  Why involve the student at all? Why not  have the teacher fine tune with noisy labels and get rid of the student  completely?
3. The  noisy part to the student seems odd to me.  Why would this work other  than the fact of adding noise you sort of anneal the solution.  Why not  add noise to the gradient or do what i suggest in 2.

I see Q Le has investigated noisy gradients already.  [https://arxiv.org/pdf/1511.06807.pdf](https://arxiv.org/pdf/1511.06807.pdf)",4,1
1511,2019-11-29,2019,11,29,0,e2zmzp,Mac vs windows and linux,https://www.reddit.com/r/MachineLearning/comments/e2zmzp/mac_vs_windows_and_linux/,TheMightyBob97,1574955765,[removed],0,1
1512,2019-11-29,2019,11,29,1,e30gw0,[P] Improving Music Recommendations - looking for users to take part!,https://www.reddit.com/r/MachineLearning/comments/e30gw0/p_improving_music_recommendations_looking_for/,FeldsparKnight,1574959377,"I'm looking for user data for my Computer Science Masters project ""Using Community Detection to Improve Music Recommendations"".

I'll be using machine learning to examine user music data from Spotify with the aim of improving the songs people are recommended.

I've produced a web app where you can consent to data being (anonymously) sampled from your Spotify account. It only takes about 1 minute to log in and would really help me out.

Thanks!

https://james-atkin-spotify-project.herokuapp.com/",12,1
1513,2019-11-29,2019,11,29,1,e30l9l,Lessons learned building an ML trading system that turned $5k into $200k,https://www.reddit.com/r/MachineLearning/comments/e30l9l/lessons_learned_building_an_ml_trading_system/,CodePerfect,1574959901,,0,1
1514,2019-11-29,2019,11,29,2,e30yug,A little help with MLP and Image recognition in Python,https://www.reddit.com/r/MachineLearning/comments/e30yug/a_little_help_with_mlp_and_image_recognition_in/,iskulll,1574961453,[removed],0,1
1515,2019-11-29,2019,11,29,2,e3101a,[R] Task-Oriented Language Grounding for Language Input with Multiple Sub-Goals of Non-Linear Order,https://www.reddit.com/r/MachineLearning/comments/e3101a/r_taskoriented_language_grounding_for_language/,Lua_b,1574961595,"arxiv: [https://arxiv.org/abs/1910.12354](https://arxiv.org/abs/1910.12354)

gitrhub: [https://github.com/vkurenkov/language-grounding-multigoal](https://github.com/vkurenkov/language-grounding-multigoal)

Abstract:

&gt;In this work, we analyze the performance of general deep reinforcement learning algorithms for a task-oriented language grounding problem, where language input contains multiple sub-goals and their order of execution is non-linear.  
We generate a simple instructional language for the GridWorld environment, that is built around three language elements (order connectors) defining the order of execution: one linear - ""comma"" and two non-linear - ""but first"", ""but before"". We apply one of the deep reinforcement learning baselines - Double DQN with frame stacking and ablate several extensions such as Prioritized Experience Replay and Gated-Attention architecture.  
Our results show that the introduction of non-linear order connectors improves the success rate on instructions with a higher number of sub-goals in 2-3 times, but it still does not exceed 20%. Also, we observe that the usage of Gated-Attention provides no competitive advantage against concatenation in this setting. Source code and experiments' results are available at [this https URL](https://github.com/vkurenkov/language-grounding-multigoal)",0,1
1516,2019-11-29,2019,11,29,3,e32dqu,Solving the FrozenLake environment from OpenAI gym using Value Iteration,https://www.reddit.com/r/MachineLearning/comments/e32dqu/solving_the_frozenlake_environment_from_openai/,aidiganta,1574967140,,0,1
1517,2019-11-29,2019,11,29,4,e32tkt,Image recognition model,https://www.reddit.com/r/MachineLearning/comments/e32tkt/image_recognition_model/,hung_jungian,1574968979,[removed],0,1
1518,2019-11-29,2019,11,29,4,e32w7u,IT Professionals - Challenges with Fitness &amp; Health &amp; Weight Loss,https://www.reddit.com/r/MachineLearning/comments/e32w7u/it_professionals_challenges_with_fitness_health/,techGeek_19232,1574969283,[removed],0,1
1519,2019-11-29,2019,11,29,5,e33ngq,Most used programming language for ML-Tasks,https://www.reddit.com/r/MachineLearning/comments/e33ngq/most_used_programming_language_for_mltasks/,valentin_di,1574972438,[removed],0,1
1520,2019-11-29,2019,11,29,5,e33v5h,Good resources for understanding transformer models,https://www.reddit.com/r/MachineLearning/comments/e33v5h/good_resources_for_understanding_transformer/,Walpleb,1574973326,[removed],0,1
1521,2019-11-29,2019,11,29,5,e33zix,"Herring, Not Herring: Deep Learning Accelerates Detection and Classification of Underwater Species",https://www.reddit.com/r/MachineLearning/comments/e33zix/herring_not_herring_deep_learning_accelerates/,Yuqing7,1574973849,,0,1
1522,2019-11-29,2019,11,29,5,e3445z,"Introduction to Applied Linear Algebra  Vectors, Matrices, and Least Squares -Stanford-Book",https://www.reddit.com/r/MachineLearning/comments/e3445z/introduction_to_applied_linear_algebra_vectors/,ai-lover,1574974394,,36,1
1523,2019-11-29,2019,11,29,6,e34hcc,Voice transformation for dialog replacement and animation,https://www.reddit.com/r/MachineLearning/comments/e34hcc/voice_transformation_for_dialog_replacement_and/,mytwobucks,1574975985,I recently got to test our voice transformation technology based on machine learning. It replaces your speech or singing with a famous voice. Voice fonts are trained in a machine learning process by listening to recorded speech. We are also working on imparting foreign accents into actors' or animated characters' speech (dialog replacement). Please check out the demo link and share your thoughts on possible applications for media and entertainment.,0,1
1524,2019-11-29,2019,11,29,6,e34qkc,Your Local GAN: Designing Two Dimensional Local Attention Mechanisms for Generative Models,https://www.reddit.com/r/MachineLearning/comments/e34qkc/your_local_gan_designing_two_dimensional_local/,darasgiannis,1574977119,,1,1
1525,2019-11-29,2019,11,29,6,e34vjk,Your Local GAN: Designing Two Dimensional Local Attention Mechanisms for Generative Models,https://www.reddit.com/r/MachineLearning/comments/e34vjk/your_local_gan_designing_two_dimensional_local/,darasgiannis,1574977750,[removed],0,1
1526,2019-11-29,2019,11,29,7,e357vy,How do you deal with the pressure of such a fast moving field?,https://www.reddit.com/r/MachineLearning/comments/e357vy/how_do_you_deal_with_the_pressure_of_such_a_fast/,vakker00,1574979297,[removed],0,1
1527,2019-11-29,2019,11,29,7,e35e6t,How to Train a Final Machine Learning Model,https://www.reddit.com/r/MachineLearning/comments/e35e6t/how_to_train_a_final_machine_learning_model/,andrea_manero,1574980099,[removed],0,1
1528,2019-11-29,2019,11,29,7,e35hff,"If you started self-teaching ML in high school, reply to this post plz (y/n):",https://www.reddit.com/r/MachineLearning/comments/e35hff/if_you_started_selfteaching_ml_in_high_school/,expericonatus,1574980514,[removed],1,1
1529,2019-11-29,2019,11,29,7,e35s1j,I hate those guys on stackoverflow constantly deleting my questions,https://www.reddit.com/r/MachineLearning/comments/e35s1j/i_hate_those_guys_on_stackoverflow_constantly/,liujxing,1574981950,"I cannot remember how many times my questions were deleted on stackoverflow because they are ""opinion-based"" or ""off-topic"". Mostly I was seeking some general help more like helping with choosing a programming language for implementing my own library, then it is now off topic isn't it? So am I only supposed to ask questions like ""how to write a regular expression to extract domain from a URL""? And some superuser comments like ""You were talking about machine learning libraries in C++, but I googled and they are mostly implemented in Python"", which is the dumbest thing I have ever seen on Stackoverflow.",0,1
1530,2019-11-29,2019,11,29,8,e36fo9,Is this a threat?,https://www.reddit.com/r/MachineLearning/comments/e36fo9/is_this_a_threat/,GermanGamer77,1574985223,,0,1
1531,2019-11-29,2019,11,29,9,e36nbp,What are the practical applications of machine learning technology to the average person?,https://www.reddit.com/r/MachineLearning/comments/e36nbp/what_are_the_practical_applications_of_machine/,fred_the_mailman,1574986249,[removed],0,1
1532,2019-11-29,2019,11,29,9,e36vc7,Help wanted to get into Machine Learning!,https://www.reddit.com/r/MachineLearning/comments/e36vc7/help_wanted_to_get_into_machine_learning/,CodeMonkey008,1574987330,"Hey,

I am really interested in Machine Learning but I do not know how to start or where to start. The scope of the subject is overwhelming to me. I have some experience in the field of computer science when it comes to coding. I know a little bit of python and c++ but I am far from being an expert.

It would be of great help of you could guide me and tell me where I could get resources.

Thank you for helping!",0,1
1533,2019-11-29,2019,11,29,9,e36xcv,Can someone shed light on Adversarial and Generative Graph Learning,https://www.reddit.com/r/MachineLearning/comments/e36xcv/can_someone_shed_light_on_adversarial_and/,faisal3325,1574987613,"I am new to graph learning. Spend a couple of hours on Adversarial and Generative Graph Learning, could not get anything about it. Instead got more confused.",0,1
1534,2019-11-29,2019,11,29,10,e37gxs,Can Recurrent Neural Networks have loops that go backward?,https://www.reddit.com/r/MachineLearning/comments/e37gxs/can_recurrent_neural_networks_have_loops_that_go/,adkyary,1574990440,[removed],0,1
1535,2019-11-29,2019,11,29,11,e38k66,[D] Can Recurrent Neural Networks have loops that go backward?,https://www.reddit.com/r/MachineLearning/comments/e38k66/d_can_recurrent_neural_networks_have_loops_that/,adkyary,1574995925,"[https://youtu.be/oJNHXPs0XDk?t=333](https://youtu.be/oJNHXPs0XDk?t=333)

I was watching this video where the guy says that RNNs can have loopbacks (5:33).

I always thought they were called ""Recurrent"" because they have units that can be appended over and over to an architecture to form a sequence.

Is it really correct to say that they can have feedback loops to a previous layer and that ""it's not just a feedforward network""?

&amp;#x200B;

Thanks",8,1
1536,2019-11-29,2019,11,29,13,e39dzu,Is it better to train multiple neural networks to classify something specific?,https://www.reddit.com/r/MachineLearning/comments/e39dzu/is_it_better_to_train_multiple_neural_networks_to/,draterami,1575000127,[removed],0,1
1537,2019-11-29,2019,11,29,14,e3aizm,Why is inference-time dropout used in Tacotron 2,https://www.reddit.com/r/MachineLearning/comments/e3aizm/why_is_inferencetime_dropout_used_in_tacotron_2/,txhwind,1575006537,[removed],0,1
1538,2019-11-29,2019,11,29,15,e3aooq,[Project]Recommender web app for short stories,https://www.reddit.com/r/MachineLearning/comments/e3aooq/projectrecommender_web_app_for_short_stories/,ShubC,1575007472,"Developed a bare-bone web app which helps in reading short stories from project Guttenberg  and based on what a user might like recommends something similar

[https://project-guttenberg.herokuapp.com/](https://project-guttenberg.herokuapp.com/)

(Thoughts and ideas on what can be done to enhance it please??)",2,1
1539,2019-11-29,2019,11,29,15,e3ax7t,When to not use neural networks ?,https://www.reddit.com/r/MachineLearning/comments/e3ax7t/when_to_not_use_neural_networks/,nicole1135,1575008891,[removed],0,1
1540,2019-11-29,2019,11,29,15,e3b0iw,Opinions on Alibaba ML Platform Alink,https://www.reddit.com/r/MachineLearning/comments/e3b0iw/opinions_on_alibaba_ml_platform_alink/,basso1995,1575009440,[removed],0,1
1541,2019-11-29,2019,11,29,15,e3b82v,Anybody knows how to watch the Bit Player?,https://www.reddit.com/r/MachineLearning/comments/e3b82v/anybody_knows_how_to_watch_the_bit_player/,josecyc,1575010705,[removed],0,1
1542,2019-11-29,2019,11,29,16,e3blw8,"[D] Codingbullet is trending on the AI he made to play a snake game, when it seems like all he did was ripped an algorithm off a guy on the internet. Does he actually use AI to solve the problem?",https://www.reddit.com/r/MachineLearning/comments/e3blw8/d_codingbullet_is_trending_on_the_ai_he_made_to/,cbayl3,1575013058,,0,1
1543,2019-11-29,2019,11,29,16,e3brb9,Continuously adding new users in recommender systems,https://www.reddit.com/r/MachineLearning/comments/e3brb9/continuously_adding_new_users_in_recommender/,jesbeng,1575014032,[removed],0,1
1544,2019-11-29,2019,11,29,17,e3buo3,[D] Five major deep learning papers by Geoff Hinton did not cite similar earlier work by Jurgen Schmidhuber,https://www.reddit.com/r/MachineLearning/comments/e3buo3/d_five_major_deep_learning_papers_by_geoff_hinton/,siddarth2947,1575014661,"still milking Jurgen's very dense [inaugural tweet](https://twitter.com/SchmidhuberAI) about their [annus mirabilis 1990-1991](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html) with Sepp Hochreiter and others, 2 of its 21 sections already made for nice reddit threads, section 5 [Jurgen really had GANs in 1990](https://www.reddit.com/r/MachineLearning/comments/djju8a/d_jurgen_schmidhuber_really_had_gans_in_1990/) and section 19 [DanNet, the CUDA CNN of Dan Ciresan in Jurgen's team, won 4 image recognition challenges prior to AlexNet](https://www.reddit.com/r/MachineLearning/comments/dwnuwh/d_dannet_the_cuda_cnn_of_dan_ciresan_in_jurgen/), but these are not the juiciest parts of the blog post

instead look at sections 1 2 8 9 10 where Jurgen mentions work they did long before Geoff, who did not cite, as confirmed by studying the references, at first glance it's not obvious, it's hidden, one has to work backwards from the references

[section 1, First Very Deep NNs, Based on Unsupervised Pre-Training (1991)](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html#Sec.%201), Jurgen ""facilitated supervised learning in deep RNNs by unsupervised pre-training of a hierarchical stack of RNNs"" and soon was able to ""solve previously unsolvable Very Deep Learning tasks of depth &gt; 1000,"" he mentions reference [UN4] which is actually Geoff's later similar work:

&gt; More than a decade after this work [UN1], a similar method for more limited feedforward NNs (FNNs) was published, facilitating supervised learning by unsupervised pre-training of stacks of FNNs called Deep Belief Networks (DBNs) [UN4]. The 2006 justification was essentially the one I used in the early 1990s for my RNN stack: each higher level tries to reduce the description length (or negative log probability) of the data representation in the level below. 

back then unsupervised pre-training was a big deal, today it's not so important any more, see [section 19, From Unsupervised Pre-Training to Pure Supervised Learning (1991-95 and 2006-11)](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html#Sec.%2019) 

[section 2, Compressing / Distilling one Neural Net into Another (1991)](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html#Sec.%202), Jurgen also trained ""a student NN to imitate the behavior of the teacher NN,"" briefly referring to Geoff's much later similar work [DIST2]:

&gt; I called this ""collapsing"" or ""compressing"" the behavior of one net into another. Today, this is widely used, and also called ""distilling"" [DIST2] or ""cloning"" the behavior of a teacher net into a student net. 

[section 9, Learning Sequential Attention with NNs (1990)](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html#Sec.%209), Jurgen ""had both of the now common types of neural sequential attention: end-to-end-differentiable ""soft"" attention (in latent space) through multiplicative units within NNs [FAST2](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.1885&amp;rep=rep1&amp;type=pdf), and ""hard"" attention (in observation space) in the context of Reinforcement Learning (RL) [ATT0](http://people.idsia.ch/~juergen/FKI-128-90ocr.pdf) [ATT1],"" the blog has a statement about Geoff's later similar work [ATT3](https://papers.nips.cc/paper/4089-learning-to-combine-foveal-glimpses-with-a-third-order-boltzmann-machine.pdf) which I find both funny and sad: 

&gt; My overview paper for CMSS 1990 [ATT2] summarised in Section 5 our early work on attention, to my knowledge the first implemented neural system for combining glimpses that jointly trains a recognition &amp; prediction component with an attentional component (the fixation controller). Two decades later, the reviewer of my 1990 paper wrote about his own work as second author of a related paper [ATT3]: ""To our knowledge, this is the first implemented system for combining glimpses that jointly trains a recognition component ... with an attentional component (the fixation controller)."" 

similar in [section 10, Hierarchical Reinforcement Learning (1990)](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html#Sec.%2010), Jurgen introduced HRL ""with end-to-end differentiable NN-based subgoal generators [HRL0](http://people.idsia.ch/~juergen/FKI-129-90ocr.pdf), also with recurrent NNs that learn to generate sequences of subgoals [HRL1] [HRL2],"" referring to Geoff's later work [HRL3](https://papers.nips.cc/paper/714-feudal-reinforcement-learning.pdf):  

&gt; Soon afterwards, others also started publishing on HRL. For example, the reviewer of our reference [ATT2] (which summarised in Section 6 our early work on HRL) was last author of ref [HRL3]

[section 8, End-To-End-Differentiable Fast Weights: NNs Learn to Program NNs (1991)](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html#Sec.%208), Jurgen published a network ""that learns by gradient descent to quickly manipulate the fast weight storage"" of another network, and ""active control of fast weights through 2D tensors or outer product updates [FAST2](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.1885&amp;rep=rep1&amp;type=pdf),"" dryly referring to [FAST4a](https://papers.nips.cc/paper/6057-using-fast-weights-to-attend-to-the-recent-past.pdf) which happens to be Geoff's later similar paper: 

&gt; A quarter century later, others followed this approach [FAST4a]

it's really true, Geoff did not cite Jurgen in any of these similar papers, and what's kinda crazy, he was editor of Jurgen's 1990 paper [ATT2](http://people.idsia.ch/~juergen/hinton-rev.pdf) summarising both attention learning and hierarchical RL, then later he published closely related work, sections 9, 10, but he did not cite 

Jurgen also [famously complained](http://people.idsia.ch/~juergen/deep-learning-conspiracy.html) that Geoff's deep learning survey in Nature neither mentions the inventors of backpropagation (1960-1970) nor ""the father of deep learning, Alexey Grigorevich Ivakhnenko, who published the first general, working learning algorithms for deep networks"" in 1965 

apart from the early pioneers in the 60s and 70s, like Ivaknenko and Fukushima, most of the big deep learning concepts stem from Jurgen's team with Sepp and Alex and Dan and others: unsupervised pre-training of deep networks, artificial curiosity and GANs, vanishing gradients, LSTM for language processing and speech and everything, distilling networks, attention learning, CUDA CNNs that win vision contests, deep nets with 100+ layers, metalearning, plus theoretical work on optimal AGI and Godel Machine",194,1
1545,2019-11-29,2019,11,29,17,e3bwxd,Workflow : how to debug in the cloud,https://www.reddit.com/r/MachineLearning/comments/e3bwxd/workflow_how_to_debug_in_the_cloud/,FenryrMKIII,1575015041,[removed],0,1
1546,2019-11-29,2019,11,29,17,e3byrv,[D] How do you deal with the pressure of such a fast moving field?,https://www.reddit.com/r/MachineLearning/comments/e3byrv/d_how_do_you_deal_with_the_pressure_of_such_a/,vakker00,1575015402,"The progress of ML is getting crazy and I feel super stressed about it lately. It also didn't help that a few ICML submissions do exactly the same thing that I wanted to do during my PhD that I started recently (I was an ML engineer previously).

There are just so many people working on similar ideas, I find it hard to keep up and contribute original ideas. How do you deal with this?",45,1
1547,2019-11-29,2019,11,29,17,e3cabj,[D] Bayes Optimal Classifier,https://www.reddit.com/r/MachineLearning/comments/e3cabj/d_bayes_optimal_classifier/,ssd123456789,1575017724,"For the bayes optimal Classifier, when deriving it, if you have a loss function with unequal penalties for two incorrect decisions:
    L=10 when y=1 and f=0
    L=1 when y=0 and f=1
    L=0 when y=f
Where f is the classifier.
How does one go about deriving a decision threshold for this problem?",9,1
1548,2019-11-29,2019,11,29,18,e3cfx6,WHAT DO YOU THINK WHAT WILL MORE ADVANCED TECHNOLOGY MEAN FOR CLIMATE CHANGE?,https://www.reddit.com/r/MachineLearning/comments/e3cfx6/what_do_you_think_what_will_more_advanced/,day1technologies,1575018785,[removed],0,1
1549,2019-11-29,2019,11,29,18,e3ciyr,I need to LEARN More about Machine Learning- Any examples?,https://www.reddit.com/r/MachineLearning/comments/e3ciyr/i_need_to_learn_more_about_machine_learning_any/,SottileDesign,1575019382,[removed],0,1
1550,2019-11-29,2019,11,29,18,e3ctv5,[D] Help/Question about using Vector Projection + K-Means in VAE encoded result as pseudo-recommendation system,https://www.reddit.com/r/MachineLearning/comments/e3ctv5/d_helpquestion_about_using_vector_projection/,sarmientoj24,1575021494,"I have a project that uses Variational Autoencoder for an apparel dataset that is grouped into five categories (say, (A B C D E). 

My plan is the following.

1. Train a VAE model using the apparel dataset.
2. Use encoder on each data to produce latent code (e.g. my bottleneck/latent representation is of size 10 for example). Store to database
3. Use K-Means to cluster the data (using the latent code) in the database with n categories (five for example). Store cluster labels for each data in database.
4. Store cluster centroids in the database created from #3.
5. User interacts with a GUI that lets him use sliders to generate its own latent code (ten slides because #2 is 10). Decoder generates an image from the latent code given.
6. Click Recommend - enables the user two parts
   1. Get the product/apparel from database that is most similar. (1. Predict cluster. 2. Find the most similar in the cluster using distance metric on the latent codes stored).
   2. Recommend from other cluster. The idea is that if the user generates a topwear (e.g. a shirt), I would also generate from other clusters (for example, other cluster have bottomwear, shoes, etc.). **This is my problem right here.**

**For clustering, I could just use a simple K-Means. I can get the cluster labels and the cluster centroids.**

**My idea for 6.2 (Recommendation):**

* I'm not really sure but for sure, there is a relationship between the cluster centroid (cluster mean) and the most similar/generated latent code. ***Is dot product applicable to this?***
* My idea is that if my user generated code (vector) is called ***X****,* the most similar as ***A1****,* cluster centroid for the predicted cluster as ***A0,*** cluster centroid for ***another cluster as B0.*** **""*****I could know the projection of X w.r.t A0 and then use this amount of projection (idk what it is called, or if there is such a concept), to B0 to find the most similar as B1.*** 

*IS this even possible? If yes, what is this called? If not, could you recommend a better recommendation system that revolves around the same concept?*",14,1
1551,2019-11-29,2019,11,29,19,e3czou,[R] Faster AutoAugment: Learning Augmentation Strategies using Backpropagation,https://www.reddit.com/r/MachineLearning/comments/e3czou/r_faster_autoaugment_learning_augmentation/,youali,1575022608,"Paper: https://arxiv.org/abs/1911.06987


Abstract: 
Data augmentation methods are indispensable heuristics to boost the performance of deep neural networks, especially in image recognition tasks. Recently, several studies have shown that augmentation strategies found by search algorithms outperform hand-made strategies. Such methods employ black-box search algorithms over image transformations with continuous or discrete parameters and require a long time to obtain better strategies. In this paper, we propose a differentiable policy search pipeline for data augmentation, which is much faster than previous methods. We introduce approximate gradients for several transformation operations with discrete parameters as well as the differentiable mechanism for selecting operations. As the objective of training, we minimize the distance between the distributions of augmented data and the original data, which can be differentiated. We show that our method, Faster AutoAugment, achieves significantly faster searching than prior work without a performance drop.",0,1
1552,2019-11-29,2019,11,29,19,e3d4lk,Build and and Deploy a machine learning app to AWS from scratch using Python: an end-to-end tutorial ,https://www.reddit.com/r/MachineLearning/comments/e3d4lk/build_and_and_deploy_a_machine_learning_app_to/,ahmedbesbes,1575023590,,0,1
1553,2019-11-29,2019,11,29,20,e3dhft,Curiosity Driven World Models,https://www.reddit.com/r/MachineLearning/comments/e3dhft/curiosity_driven_world_models/,akhandait,1575026095,[removed],0,1
1554,2019-11-29,2019,11,29,20,e3dkqr,[R] Curiosity Driven World Models,https://www.reddit.com/r/MachineLearning/comments/e3dkqr/r_curiosity_driven_world_models/,akhandait,1575026748,[Here's](https://github.com/akhandait/curiosity-driven-world-models/blob/master/report.pdf) some work we did as a course project. It's an (unsuccessful) attempt at incorporating [curiosity](https://arxiv.org/abs/1705.05363) in [world models](https://arxiv.org/abs/1803.10122). It's a beginner's work and any feedback is appreciated.,7,1
1555,2019-11-29,2019,11,29,20,e3dpw5,[P] Open HPC dataset for anomaly detection,https://www.reddit.com/r/MachineLearning/comments/e3dpw5/p_open_hpc_dataset_for_anomaly_detection/,huyhcmut,1575027775,[removed],0,1
1556,2019-11-29,2019,11,29,20,e3dreg,ANYBODY HAS INTEREST OF MEDICAL IMAGE PLS FEEL FREE TO MESSAGE ME,https://www.reddit.com/r/MachineLearning/comments/e3dreg/anybody_has_interest_of_medical_image_pls_feel/,longkum,1575028059,[removed],0,1
1557,2019-11-29,2019,11,29,21,e3e3y2,What are new dimensionality reduction techniques in Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/e3e3y2/what_are_new_dimensionality_reduction_techniques/,sdinesh718,1575030283,[removed],0,1
1558,2019-11-29,2019,11,29,21,e3e4cv,"[D] What are some problems types where ML could be applied ""in theory"" but it's outside of practical reach ?",https://www.reddit.com/r/MachineLearning/comments/e3e4cv/d_what_are_some_problems_types_where_ml_could_be/,elcric_krej,1575030361,"It might be an overly-simplified view of the field, but it seems to me that a lot of the ML boom of this decade is due to the appearance of hardware+architectures that were able to tackle a set of problems which were easy in terms of data gathering and ""pretty deterministic"" (that is to say, based on our human abilities to tackle them, we can be pretty certain there are usually no latent variables which are necessary to solve  the problem correctly), things like bounding boxes, image classification and translation.

On the other hand these new methods have hardly put a dent in how most people approach mostly ""pretty non-deterministic"" issues (e.g. stock trading or risk analysis), where practice and intuition shows that there's simply not sufficient ""easy"" data that can make a reliable prediction.

It seems to me that most efforts right now are focused on either ""productizing"" the gains that were had on text and image problems (e.g. getting that 0.x% extra accuracy and 0.y% extra specificity that makes them practical to use in fields with low error margins) or getting algorithms that can better communicate the uncertainty of non-deterministic datasets (e.g. Bayesian/Probabilistic NNs).

However, it's not obvious to me what the next set of problems similar to images and text will hit the chopping block, or if there is such a set of problems. 

I've seen some interesting research (e.g. Alpha Fold) and some huge failures (e.g. that earthquake prediction publish in Nature that was worse than a linear regression) in the realm of scientific problems where we ""seem to"" have sufficient data but lack the mathematical frameworks to gain insights from the data. I think anything related to complex molecular dynamics in a ""static"" environment is a pretty good example, since in theory the starting state should allow us insight into any state at a later time T, but in practice this is often too computationally expensive and/or too complex to formalize in a way that is fitting for our current models. However, there doesn't seem to be near that amount of adoption, excitement or novel ideas coming from this class of problems.

So I wonder, what would you guys think would be the next ""category"" of problems where, conceptually, ML techniques could be applied without too much of a data-gathering barrier, yet the hardware+knowledge combination of current humans is yet to evolve to a point where they are feasible.",1,1
1559,2019-11-29,2019,11,29,21,e3e6ex,[D] Status of LSTM/GRU Support for TF Lite in TF 2.0?,https://www.reddit.com/r/MachineLearning/comments/e3e6ex/d_status_of_lstmgru_support_for_tf_lite_in_tf_20/,craaaft,1575030739,,0,1
1560,2019-11-29,2019,11,29,21,e3eatj,Larger networks help with shift in data distribution,https://www.reddit.com/r/MachineLearning/comments/e3eatj/larger_networks_help_with_shift_in_data/,hobbesfanclub,1575031506,[removed],0,1
1561,2019-11-29,2019,11,29,22,e3ejv3,"Style transfer and inpainting algorithms to combine Jaws film still with Copley's"" Watson and the Shark"" by Artnome.",https://www.reddit.com/r/MachineLearning/comments/e3ejv3/style_transfer_and_inpainting_algorithms_to/,hoopism,1575033016,,0,1
1562,2019-11-29,2019,11,29,22,e3en1u,Swift for TensorFlow [SUBREDDIT],https://www.reddit.com/r/MachineLearning/comments/e3en1u/swift_for_tensorflow_subreddit/,rahulbhalley,1575033550,,0,1
1563,2019-11-29,2019,11,29,22,e3enju,[R] Graph Neural Ordinary Differential Equations,https://www.reddit.com/r/MachineLearning/comments/e3enju/r_graph_neural_ordinary_differential_equations/,soshiheart,1575033623, [https://arxiv.org/abs/1911.07532](https://arxiv.org/abs/1911.07532),7,1
1564,2019-11-29,2019,11,29,23,e3fgbc,[P] Introducing Valohai - The managed platform for building production-scale machine learning.,https://www.reddit.com/r/MachineLearning/comments/e3fgbc/p_introducing_valohai_the_managed_platform_for/,keely,1575037953,"Hi everyone!

We're Valohai, a platform for running experiments in the cloud with full version control.

It would be great if you'd [sign up](https://app.valohai.com/accounts/signup/) for a free account and gave us your feedback.

Some of the features of Valohai:

* Tech-agnostic by design. Whatever language or library you are using, we can run it
* Cloud-agnostic by design. Run the same project easily in Azure, AWS, GCP or on-premise hardware
* Experiments are fully versioned and reproducible. All code, data, logs, metrics, models, and environments are stored for later use and other team members to see
* Cloud instances are started and stopped automatically. Don't waste your money on those idle machines!
* Link your git or GitHub repository directly for automatic code pull
* Jupyter notebook support using the [Jupyhai](https://docs.valohai.com/jupyter/index.html) plugin
* For more advanced users, we have a robust [CLI](https://blog.valohai.com/from-zero-to-hero-with-valohai-cli) and a fully documented [API](https://app.valohai.com/api/docs/)

More info in our [website](https://valohai.com/) and [docs](https://docs.valohai.com/).",0,1
1565,2019-11-30,2019,11,30,0,e3fwat,[D] ML Paper Notes: My notes of various ML research papers (DL - CV - NLP),https://www.reddit.com/r/MachineLearning/comments/e3fwat/d_ml_paper_notes_my_notes_of_various_ml_research/,youali,1575040212,"Hello,

As a PhD student, I read quite a lot of papers, and sometimes I make short summaries with a simple latex template to get a better understanding and have clearer idea of the paper's contributions. For a while I stored them in private Github repo, so I tought why note share them, some people might find them helpful. 

PS: Sorry for the (sometimes frequent) spelling mistakes.

Here is the Github link: https://github.com/yassouali/ML_paper_notes",11,1
1566,2019-11-30,2019,11,30,1,e3gmpj,[P] (Early Stage) kaggledatasets will make things easy - Looking for Contributions &amp; Reviews,https://www.reddit.com/r/MachineLearning/comments/e3gmpj/p_early_stage_kaggledatasets_will_make_things/,op_prabhuomkar,1575043659,"**kaggledatasets**: Collection of Kaggle Datasets ready for everyone to use.

It is a Python package which will contain several high voted kaggle datasets made available in easy to use format with special support for frameworks like Tensorflow 2.0 and PyTorch.

Every beginner in ML/DL/Data Science struggles in the initial stages of Data Loading and Pre-processing. Kaggle being the gold mine of datasets, it's necessary to make it easy to use for everyone.  

And hold on, support for more datasets and more types is on the way. Feel free to contribute and add datasets. Also, looking for experienced developers who can contribute to the architecture of the project or provide reviews for improvement.

  
**GitHub:** [https://github.com/kaggledatasets/kaggledatasets](https://github.com/kaggledatasets/kaggledatasets)",1,1
1567,2019-11-30,2019,11,30,1,e3gw4m,Advice on Research Prospects,https://www.reddit.com/r/MachineLearning/comments/e3gw4m/advice_on_research_prospects/,kbnewreddit,1575044824,"What are the research prospects at the intersection of multi-agent systems, causality, and learning theory?

Are any of you working (or used to work) in a research capacity in industry or academia? What do you think are some of the pressing questions? Could you give a quick overview of the field?

Also, do you happen to know research groups or professors who work in these fields, either in the US or Europe?

I found this area to be interesting after watching talks on YouTube. How should I go about exploring this interdisciplinary field? I plan to pursue a PhD in the same.

Thanks!",0,1
1568,2019-11-30,2019,11,30,2,e3hhzh,Machine Learning Model to Locate and Map Cranberry Bogs -- Happy Thanksgiving!,https://www.reddit.com/r/MachineLearning/comments/e3hhzh/machine_learning_model_to_locate_and_map/,purens,1575047441,,0,1
1569,2019-11-30,2019,11,30,2,e3hi5u,[P] Detecting Sarcasm - How good are humans compared to machines?,https://www.reddit.com/r/MachineLearning/comments/e3hi5u/p_detecting_sarcasm_how_good_are_humans_compared/,CHR1597,1575047462,"I'm developing a tool to use machine learning and sentiment analysis to determine the usefulness of context when parsing sarcasm. 

In order to evaluate the success of the tool, I'm looking for human responses to a few examples from SARC 2.0 (https://arxiv.org/abs/1704.05579), a corpus of Reddit comments.

If you can spare a few minutes to answer [this Google form](https://forms.gle/etBga9tmNyhWrKBa8) (no login required), please help me out!",10,1
1570,2019-11-30,2019,11,30,2,e3hkvl,AMD Threadripper Gen 3 or Intel CPU?,https://www.reddit.com/r/MachineLearning/comments/e3hkvl/amd_threadripper_gen_3_or_intel_cpu/,thinking_computer,1575047778,[removed],1,1
1571,2019-11-30,2019,11,30,2,e3hu7i,[D] Amd or Intel CPU for deep learning server?,https://www.reddit.com/r/MachineLearning/comments/e3hu7i/d_amd_or_intel_cpu_for_deep_learning_server/,thinking_computer,1575048864,"Building a Deep Learning home server with 4x 2080 ti blower style GPUs and I am wondering what CPU to get for this machine.  The delima is that I am using python Pytorch and Numpy which has a lot of support with Intels MLK packages that sabotage AMD performance. However, I am utilizing the GPUs and CUDA technology for the matrix multiplication and can get away with Numpy Openblas for my environments on the AMD machine.  I'm trying to keep this project under $8000 and want the best performance.  Also, I am primary using DDPG and DQN networks.  What would you do?

&amp;#x200B;

|Type|Item|Price|
|:-|:-|:-|
|**CPU**|[AMD Threadripper 3960X 3.8 GHz 24-Core Processor](https://pcpartpicker.com/product/kNyqqs/amd-ryzen-9-3960x-38-ghz-24-core-processor-100-100000010wof)|\-|
|**CPU Cooler**|[be quiet! Dark Rock Pro TR4 59.5 CFM CPU Cooler](https://pcpartpicker.com/product/Z6zkcf/be-quiet-dark-rock-pro-tr4-595-cfm-cpu-cooler-bk023)|\-|
|**Motherboard**|[ASRock TRX40 Creator ATX sTRX4 Motherboard](https://pcpartpicker.com/product/TL7p99/asrock-trx40-creator-atx-strx4-motherboard-trx40-creator)|\-|
|**Memory**|[G.Skill Ripjaws V Series 16 GB (2 x 8 GB) DDR4-3200 Memory](https://pcpartpicker.com/product/qGqbt6/gskill-memory-f43200c16d16gvgb)|$59.99 @ Newegg|
|**Storage**|[HP EX950 1 TB M.2-2280 NVME Solid State Drive](https://pcpartpicker.com/product/PyhKHx/hp-ex950-1-tb-m2-2280-solid-state-drive-5ms23aaabc)|$129.99 @ Newegg|
|**Video Card**|[PNY GeForce RTX 2080 Ti 11 GB Blower Video Card](https://pcpartpicker.com/product/FfYLrH/pny-geforce-rtx-2080-ti-11-gb-blower-video-card-vcg2080t11blmpb)|\-|
|**Case**|[Corsair Air 540 ATX Mid Tower Case](https://pcpartpicker.com/product/wgkD4D/corsair-case-air540)|$129.98 @ Newegg|
|**Power Supply**|[EVGA SuperNOVA P2 1600 W 80+ Platinum Certified Fully Modular ATX Power Supply](https://pcpartpicker.com/product/LbtWGX/evga-power-supply-220p21600x1)|\-|
|*Prices include shipping, taxes, rebates, and discounts*|||
|Total (before mail-in rebates)|$339.96||
|Mail-in rebates|\-$20.00||
|**Total**|**$319.96**||
|Generated by [PCPartPicker](https://pcpartpicker.com) 2019-11-29 12:22 EST-0500|||",13,1
1572,2019-11-30,2019,11,30,2,e3i1ch,Training CoreML Object Detection model from scratch using CreateML,https://www.reddit.com/r/MachineLearning/comments/e3i1ch/training_coreml_object_detection_model_from/,TomekB,1575049729,,0,1
1573,2019-11-30,2019,11,30,2,e3i1u6,Training Text Classifier to detect SPAM using CreateML,https://www.reddit.com/r/MachineLearning/comments/e3i1u6/training_text_classifier_to_detect_spam_using/,TomekB,1575049790,,0,1
1574,2019-11-30,2019,11,30,2,e3i2nn,Tencent Open-Sources High-Performance Graph Computing Framework Plato,https://www.reddit.com/r/MachineLearning/comments/e3i2nn/tencent_opensources_highperformance_graph/,Yuqing7,1575049881,,0,1
1575,2019-11-30,2019,11,30,2,e3i3ls,Training Text Classifier to detect SPAM using CreateML,https://www.reddit.com/r/MachineLearning/comments/e3i3ls/training_text_classifier_to_detect_spam_using/,TomekB,1575049997,,0,1
1576,2019-11-30,2019,11,30,3,e3i8i4,How can we test an an adversarial attack against a black box system?,https://www.reddit.com/r/MachineLearning/comments/e3i8i4/how_can_we_test_an_an_adversarial_attack_against/,underwater_elephant,1575050565,[removed],0,1
1577,2019-11-30,2019,11,30,3,e3ifdp,[R] Icebreaker - Tackling the challenge of deploying machine learning models when very little or no training data is initially available.,https://www.reddit.com/r/MachineLearning/comments/e3ifdp/r_icebreaker_tackling_the_challenge_of_deploying/,alshell7,1575051343,,0,1
1578,2019-11-30,2019,11,30,3,e3ijq8,Access to current e-commerce sales data. Could be from anywhere in the world.,https://www.reddit.com/r/MachineLearning/comments/e3ijq8/access_to_current_ecommerce_sales_data_could_be/,jameskyleweb,1575051851,[removed],0,1
1579,2019-11-30,2019,11,30,4,e3j4eu,[D]What is the latest on this Fujitsu Laboratories announcement from Oct?,https://www.reddit.com/r/MachineLearning/comments/e3j4eu/dwhat_is_the_latest_on_this_fujitsu_laboratories/,FreckledMil,1575054207,"I found this :

https://www.fujitsu.com/global/about/resources/news/press-releases/2019/1015-01.html

Looks like it was posted about a month ago, I haven't really found much more info on it, was trying to see if it was anything groundbreaking, the details provided are slim to none.  Just gives a basic overview of how neural networks seem to work, and some fancy terms for normalization/transformations/landmark detection.
Thanks.",0,1
1580,2019-11-30,2019,11,30,4,e3j82i,Resources for live detection + android,https://www.reddit.com/r/MachineLearning/comments/e3j82i/resources_for_live_detection_android/,huut900,1575054606,[removed],0,1
1581,2019-11-30,2019,11,30,4,e3jcb7,How to do Wikipedia Title Suggestion by using machine learning,https://www.reddit.com/r/MachineLearning/comments/e3jcb7/how_to_do_wikipedia_title_suggestion_by_using/,janaka1984,1575055109,[removed],0,1
1582,2019-11-30,2019,11,30,4,e3jeye,NFL Big Data Bowl Kaggle competition: public 1st place solution,https://www.reddit.com/r/MachineLearning/comments/e3jeye/nfl_big_data_bowl_kaggle_competition_public_1st/,killver,1575055403,[removed],0,1
1583,2019-11-30,2019,11,30,4,e3jv3j,[D] What is SOTA for image manipulation detection?,https://www.reddit.com/r/MachineLearning/comments/e3jv3j/d_what_is_sota_for_image_manipulation_detection/,davidblyrics,1575057224,"What is sota for image manipulation detection? I have not seen much work in this field happening, does anybody know the reason for that? The only recent paper have seen is from CVPR2018 but I thought there would be more work being done? 

Thanks for your help!",1,1
1584,2019-11-30,2019,11,30,5,e3ke9s,Loss for restricted boltzmann machine,https://www.reddit.com/r/MachineLearning/comments/e3ke9s/loss_for_restricted_boltzmann_machine/,matej1408,1575059401,[removed],0,1
1585,2019-11-30,2019,11,30,5,e3kjxi,[P] Loss for restricted boltzmann machines,https://www.reddit.com/r/MachineLearning/comments/e3kjxi/p_loss_for_restricted_boltzmann_machines/,matej1408,1575060054,"Hello guys,
I am trying to implement [this](https://arxiv.org/pdf/1801.07172.pdf) paper.
They used restricted boltzmann machines to get evolution of a system.
Due to nature of the problem ""activation"" function  is not sigmoid but tanh which is later transformed  into probability.
Different activation function means that gradient is different, although in the paper they wrote ""standard"" gradient equations. 
When I tried to train model with standard gradient equations I couldn't get any meaningful results.I am aware that RBM minimizes KL divergence, but I couldn't figure out how to implement that (how to get probability function). After trying to find what should I put for loss function when training with eg. Adam I stumbled upon some random github code which for loss function had:

L = energy(data_vec) - energy(genrated_vec)

(yes without absolute value).
Well, I plugged that in and it worked like a charm. When I tried to put absolute value over loss it did not worked.
I am having trouble understanding why is this correct loss for RBM and I would appreciate help :)",8,1
1586,2019-11-30,2019,11,30,6,e3kvws,[D] 2 Titan RTX's or 4 2080Tis?,https://www.reddit.com/r/MachineLearning/comments/e3kvws/d_2_titan_rtxs_or_4_2080tis/,soulslicer0,1575061437,"In terms of performance, which would be better? The price is about the same

With the 2080Tis, I have the overhead of inter gpu communication, and have 44 GB of memory, 17408 cuda cores total, 2176 tensor cores total, 1545 Mhz speed

With the 2 Titans, I dont have as much overhead i suppose, have 48GB, only 9216 cuda cores total, 1152 tensor cores total, 1770 Mhz speed.

&amp;#x200B;

Which do I go with. Once again, the price is about the same",24,1
1587,2019-11-30,2019,11,30,6,e3l5f9,[NLP] Any papers on multiclass classification on big texts?,https://www.reddit.com/r/MachineLearning/comments/e3l5f9/nlp_any_papers_on_multiclass_classification_on/,mrbow,1575062527,[removed],0,1
1588,2019-11-30,2019,11,30,6,e3ldlw,Resources for object detection + android,https://www.reddit.com/r/MachineLearning/comments/e3ldlw/resources_for_object_detection_android/,huut900,1575063464,[removed],0,1
1589,2019-11-30,2019,11,30,7,e3lpvt,[P] ML Buddies for Projects,https://www.reddit.com/r/MachineLearning/comments/e3lpvt/p_ml_buddies_for_projects/,SWEbyday,1575064804,"Hey all,

&amp;#x200B;

Not sure if this is the best place to post this, but I am looking to enhance my portfolio and ML skills more importantly through Kaggle competitions and side projects.

&amp;#x200B;

About me:

 cornell grad with coursework experience in ML, did an internship dealing with NLP and CV applications. now working as a data engineer and doing a project dealing with using nlp learning from unstructured data. 

currently following along with cs231 and 230 to improve my deep learning skills. Almost done with the andrew ng 

&amp;#x200B;

looking for like minded individuals to partake in side projects or kaggle contests with!",13,1
1590,2019-11-30,2019,11,30,7,e3lznt,Need some help creating a dataset.,https://www.reddit.com/r/MachineLearning/comments/e3lznt/need_some_help_creating_a_dataset/,mkidk,1575065836,"Hi, ML newbie here. 

So, I'm working on a sensor-based smart gloves to translate sign language into text. Basically, the five flex sensor will output numbers, say the letter 'A' outputs ''5, 4, 8, 12, 24'', there's a margin error of course. I need to create a data set for all of the alphabets, each letter would have something near 100-200 sample, I guess(?). How would I go about that? Feel free to recommend articles or any other recourses.",0,1
1591,2019-11-30,2019,11,30,7,e3m065,Multi view facial dataset,https://www.reddit.com/r/MachineLearning/comments/e3m065/multi_view_facial_dataset/,jackie285,1575065892,[removed],0,1
1592,2019-11-30,2019,11,30,7,e3mjfg,Weekly Papers | Multi-Label Deep Forest (MLDF); Huawei UK Critiques DeepMind -Rank,https://www.reddit.com/r/MachineLearning/comments/e3mjfg/weekly_papers_multilabel_deep_forest_mldf_huawei/,Yuqing7,1575068013,,0,1
1593,2019-11-30,2019,11,30,8,e3my1p,Lovely Christmas Fairy Lights! - 76% Off - [Buy 1 Get 1 FREE] Today Only,https://www.reddit.com/r/MachineLearning/comments/e3my1p/lovely_christmas_fairy_lights_76_off_buy_1_get_1/,Hierolatry,1575069762,,0,1
1594,2019-11-30,2019,11,30,9,e3nzqv,Modified Loss Function(s) for decorrelating neurons within a layer?,https://www.reddit.com/r/MachineLearning/comments/e3nzqv/modified_loss_functions_for_decorrelating_neurons/,RSchaeffer,1575074417,[removed],2,1
1595,2019-11-30,2019,11,30,10,e3oiqm,GPU amd for deep learning is possible? [I don't speak english. Sorry with text if have some error],https://www.reddit.com/r/MachineLearning/comments/e3oiqm/gpu_amd_for_deep_learning_is_possible_i_dont/,LeoLobo30,1575077041,"I study deep learning, but a know. the speed to train with gpu is better than cpu....

&amp;#x200B;

How can I use the AMD GPU for train in deep learning?",0,1
1596,2019-11-30,2019,11,30,10,e3ol2p,[R]? Is there AIs that can be used on a web browser for:,https://www.reddit.com/r/MachineLearning/comments/e3ol2p/r_is_there_ais_that_can_be_used_on_a_web_browser/,Scetch13,1575077379,"1- Imputing a bunch of text and getting an output that tries to replicate it as close as possible
2- The same but with images",4,1
1597,2019-11-30,2019,11,30,11,e3ozpj,looking for ML algo partner,https://www.reddit.com/r/MachineLearning/comments/e3ozpj/looking_for_ml_algo_partner/,open-trade,1575079480,we target to be execution algo service and alpha signal provider in china financial market. my wechat: opentrade,0,1
1598,2019-11-30,2019,11,30,11,e3pa9x,hackathon partner,https://www.reddit.com/r/MachineLearning/comments/e3pa9x/hackathon_partner/,trashcanolives,1575080974,[removed],0,1
1599,2019-11-30,2019,11,30,13,e3qlxk,[Tool] Machine Learning Generated Gradient Colors,https://www.reddit.com/r/MachineLearning/comments/e3qlxk/tool_machine_learning_generated_gradient_colors/,hydershykh,1575087930,,0,1
1600,2019-11-30,2019,11,30,14,e3r5ln,Pre-training a model vs training from scratch,https://www.reddit.com/r/MachineLearning/comments/e3r5ln/pretraining_a_model_vs_training_from_scratch/,good_profile,1575090986,,0,1
1601,2019-11-30,2019,11,30,14,e3rbgm,[D] Pretraining vs learning from scratch,https://www.reddit.com/r/MachineLearning/comments/e3rbgm/d_pretraining_vs_learning_from_scratch/,good_profile,1575091944,"Hi, I saw this video: https://www.youtube.com/watch?v=AhEVk7TLVjQ explaining the difference and I'm quite confused.

What is the actual basis for deciding whether to use pre-trained models or learn from scratch? My understanding is, if you don't have resources you have to use pre-trained models. If you have enough resources you can learn from scratch with your own data.",10,1
1602,2019-11-30,2019,11,30,14,e3rdbm,Getting into ML?,https://www.reddit.com/r/MachineLearning/comments/e3rdbm/getting_into_ml/,Lionry,1575092234,"Hey there ML reddit,

Im a freshman in college looking to start learning about ML. Do you have any suggestions?

Thanks!",0,1
1603,2019-11-30,2019,11,30,15,e3rn2p,I want to make something that recognizes images in a grid. How should I do that? I'm very new,https://www.reddit.com/r/MachineLearning/comments/e3rn2p/i_want_to_make_something_that_recognizes_images/,BaneBowcultist,1575093861,[removed],0,1
1604,2019-11-30,2019,11,30,16,e3stoy,Deep Learning with PyTorch: Part 1 of 5 | 2-hour tutorial on PyTorch Basics &amp; Linear Regression,https://www.reddit.com/r/MachineLearning/comments/e3stoy/deep_learning_with_pytorch_part_1_of_5_2hour/,aakashns,1575100631,,0,1
1605,2019-11-30,2019,11,30,17,e3t4dr,[Buy 1 Get 1 Free] Xmas Tree Color Changing LED Night Light - 90% Off,https://www.reddit.com/r/MachineLearning/comments/e3t4dr/buy_1_get_1_free_xmas_tree_color_changing_led/,MelodicAddendum,1575102411,,0,1
1606,2019-11-30,2019,11,30,17,e3td32,[P] What does a Fine-tuned BERT model look at?,https://www.reddit.com/r/MachineLearning/comments/e3td32/p_what_does_a_finetuned_bert_model_look_at/,infinitylogesh,1575103798,,0,1
1607,2019-11-30,2019,11,30,19,e3uc7q,Speech to speech voice conversion using phoneme classifier,https://www.reddit.com/r/MachineLearning/comments/e3uc7q/speech_to_speech_voice_conversion_using_phoneme/,amoghQuarks,1575109495,[removed],0,1
1608,2019-11-30,2019,11,30,20,e3uvwo,Missing Site,https://www.reddit.com/r/MachineLearning/comments/e3uvwo/missing_site/,shanthosh12,1575112801,[removed],0,1
1609,2019-11-30,2019,11,30,21,e3vgni,[D] Which problems would you say are solvable today if we just had more data and compute? Using just the theoretical knowledge we have today.,https://www.reddit.com/r/MachineLearning/comments/e3vgni/d_which_problems_would_you_say_are_solvable_today/,mrconter1,1575116233,"In other words what could we expect to achieve sooner or later if our theoretical work stopped today and we just focused on collecting more data and adding more compute? Are there any types of problems that definitely not wouldn't be solvable? 

If you have any paper discussion this subject, feel free to share them.",85,1
1610,2019-11-30,2019,11,30,22,e3wbv5,CRF-RNN PyTorch version (deep learning based semantic image segmentation),https://www.reddit.com/r/MachineLearning/comments/e3wbv5/crfrnn_pytorch_version_deep_learning_based/,sadeepj,1575120853,,0,1
1611,2019-11-30,2019,11,30,22,e3wgl2,[P] CRF-RNN PyTorch version (deep learning based semantic image segmentation),https://www.reddit.com/r/MachineLearning/comments/e3wgl2/p_crfrnn_pytorch_version_deep_learning_based/,sadeepj,1575121464,,0,1
