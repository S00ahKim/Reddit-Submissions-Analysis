,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2019-6-1,2019,6,1,9,bvf0jq,[D] Pertubative Neural Network vs Convolutional?,https://www.reddit.com/r/MachineLearning/comments/bvf0jq/d_pertubative_neural_network_vs_convolutional/,BC_HE_SPOKE_TRUTH,1559349123,"I read this paper

http://hal.cse.msu.edu/assets/pdfs/papers/2018-cvpr-pnn.pdf

But I don't understand much of it, I am mostly a beginner ML engineer. Can anyone explain to me the big differences/similarities between these two modes of computation?",5,2
1,2019-6-1,2019,6,1,10,bvfel3,My startup won't be able to consume all the free Google Cloud credits so I am offering cheap (for commercial use) or free (for cool non-profit) Deep Learning Instances. Contact me for more info.,https://www.reddit.com/r/MachineLearning/comments/bvfel3/my_startup_wont_be_able_to_consume_all_the_free/,carlos_argueta,1559351658,"&amp;#x200B;

![img](i6bwcb70bn131)",0,1
2,2019-6-1,2019,6,1,10,bvfpow,"How to automatically identify startpoints and endpoints of an ""event"" on a video?",https://www.reddit.com/r/MachineLearning/comments/bvfpow/how_to_automatically_identify_startpoints_and/,Brokoba,1559353762,[removed],0,1
3,2019-6-1,2019,6,1,10,bvfskh,[D] Was there a point during an ML job interview at which you just wondered why the hell are they asking me this question?,https://www.reddit.com/r/MachineLearning/comments/bvfskh/d_was_there_a_point_during_an_ml_job_interview_at/,GeekMonolith,1559354315,"I've had my own fair bit of surprises at ML job interviews, with some rookie interviewers just asking me completely off-topic, basic, irrelevant, or just outright ridiculous questions; that is, after I've explained to them in detail what my background is, and what I know, etc.

I always wondered, is there a point at which you're supposed to stop the interviewer and ask them why they are asking irrelevant questions, or questions that evaluate a fish's ability to climb a tree? Also, how the hell does a singular irrelevant question determine your ability to perform at the job in question? I've always wanted to say something along the line of: ""dude, I have no idea what the answer to that question is, and I can't come up with an answer quickly for you right now; but, there's a ton of other stuff that I've accumulated in my brain, that really truly interests me. Do you wanna maybe discuss those instead?""",69,96
4,2019-6-1,2019,6,1,11,bvg728,A letter from the PAMI TC and CVPR 2019 organizers,https://www.reddit.com/r/MachineLearning/comments/bvg728/a_letter_from_the_pami_tc_and_cvpr_2019_organizers/,shellehs,1559357095,,0,1
5,2019-6-1,2019,6,1,12,bvgee2,MNIST digits generated with a variational autoencoder,https://www.reddit.com/r/MachineLearning/comments/bvgee2/mnist_digits_generated_with_a_variational/,DanielAPO,1559358538,,0,1
6,2019-6-1,2019,6,1,13,bvgyim,is it impossible or possible forecast time series for next value using LSTM in Keras?,https://www.reddit.com/r/MachineLearning/comments/bvgyim/is_it_impossible_or_possible_forecast_time_series/,GoBacksIn,1559362635,[removed],0,1
7,2019-6-1,2019,6,1,14,bvhg38,[D] TensorFlow 1.14 Release Candidate,https://www.reddit.com/r/MachineLearning/comments/bvhg38/d_tensorflow_114_release_candidate/,milaworld,1559366539,"Im not sure if this is their last release of TF1 before supporting mainly TF2

From the release notes:

*This is the first 1.x release containing the compat.v2 module. This module is required to allow libraries to publish code which works in both 1.x and 2.x. After this release, no backwards incompatible changes are allowed in the 2.0 Python API.*

https://github.com/tensorflow/tensorflow/releases/tag/v1.14.0-rc0",1,10
8,2019-6-1,2019,6,1,14,bvhmex,Get enrolled for Free Data Science (online webinar) on 9th June @ 11 AM,https://www.reddit.com/r/MachineLearning/comments/bvhmex/get_enrolled_for_free_data_science_online_webinar/,skillanalytica105,1559367985,"Make the best career movie by attending for the Free Data Science (online Webinar) scheduled On 9th June . The event is For 1 hour including Q&amp;A Session. 

 REGISTRATION IS MUST  
 REGISTER HERE: http://bit.ly/2JuFKG1  

Cheers,

See you at the event.

![img](i0i2m79vno131)",0,1
9,2019-6-1,2019,6,1,15,bvhypm,Machine Learning with Google Cloud Platform,https://www.reddit.com/r/MachineLearning/comments/bvhypm/machine_learning_with_google_cloud_platform/,rohitgupta010,1559370945,,0,1
10,2019-6-1,2019,6,1,16,bvi72s,Explaining Machine Learning to a 5 year old,https://www.reddit.com/r/MachineLearning/comments/bvi72s/explaining_machine_learning_to_a_5_year_old/,navulepavan,1559373098,,0,1
11,2019-6-1,2019,6,1,16,bvi8o4,[D] What are the best Scene boundary detection papers out there? (preferably with code),https://www.reddit.com/r/MachineLearning/comments/bvi8o4/d_what_are_the_best_scene_boundary_detection/,Eoncarry,1559373515,Scene-boundary detection. Not shot boundary detection. I could find barely 2 papers for scene boundary detection [https://ieeexplore.ieee.org/document/7177476](https://ieeexplore.ieee.org/document/7177476) and [A Deep Siamese Network for Scene Detection in Broadcast Videos](https://arxiv.org/pdf/1510.08893.pdf) but neither of them have available code. Any help?,7,25
12,2019-6-1,2019,6,1,16,bvibcj,[Discussion] The 6 types of data scientist:,https://www.reddit.com/r/MachineLearning/comments/bvibcj/discussion_the_6_types_of_data_scientist/,AlexSnakeKing,1559374223,"*(Inspired by a bunch of recent blog posts I've come across with titles like: ""The Next Generation Data Scientist"" and ""The 3rd Wave Data Scientist"")* 

&amp;#x200B;

**Type I - the olympian deity:** Works as part of the core ML or research team of a FAANG or for an academic team at a major university. Has multiple papers at NeurIPS or similar status conference or publication. Wouldn't be able to recognize domain knowledge even if it punched her/him in the face. Envied by all others in the data science / machine learning community, but is actually miserable because she/he hasn't received the Turing award or gotten a theorem or class of neural networks named after them. Is this closing to give up and becoming a tenured lecturer for linear algebra 101 at a community college.

**Type II - the demigod:** Works for an applied ML/product team at a FAANG or a an up and coming very promising startup. Actually knows how to program in C++, understands dynamic polymorphism, and can solve Leetcode hard problems in their sleep. Also the envy of all others inthe data science / machine learning community, but is actually miserable because they have to work 85 hoursa week, and the awesome framework they contributed major chunks to 2 years ago is no longer fashionable and has been superseded by another framework. Miserable because they could have worked for a startup and been billionaires at the age of 32, but instead are making a measly 250K a year in a city where a suburban one bedroom costs 1.5M.

**Type III - the grumpy old hand:** In their late 40s/50s, was around during the previous Neural Networks hype cycle, and worked on them before the Tensorflow core dev team was even born. Has survived at least one A.I. Winter. Doesn't know what StackOverflow is, and doesn't need it since she/he learned how to code back when people had those huge ""The Java Bible"" and ""The Unix Bible"" reference volumes on their desk. Is almost as good a coder as the demigod, and almost as good a mathematician as the olympian, and beats them both in domain knowledge. But she/he has no idea what GitHub is, never had to spin up a docker pod or run something on a GPU, and haven't done an interview in 27 years. The last time they did, interviews were 45 minute affairs conducted over the phone. They are not really the envy of anyone in the data science / machine learning community, except for the fact that they have accrued 7 weeks of PTO by this point in their career. They are miserable because they know they are stuck in their current role, ageism is a definitely a thing, and they will likely be the first to go come the next economic downturn.

**Type IV - the hipster data scientist**: young, very likely a millennial or even Gen Z, although might be a Gen Xer who is in very good shape and has tattoos. Recent college graduate or somehow managed to transition from a marketing or sales role into data science after completing a couple of classes on Udemy. Has a huge social media presence, and their LinkedIn profile picture is one of them at a podium with a mic or giving a TEDx talk. Has produced several blog posts and/or podcasts. Doesn't know any languages besides R. Doesn't know what a partial derivative is, and freaks out whenever they see an integral, but is still very good at explaining and simplifying concepts, hence always has the attention of the business stakeholders. Says things like ""Cross-Validation is fun"", ""And I love boosting"" with a straight face. Usually works for a small to mid-level company, but occasionally manages to land a role at a FAANG, after which they develop weapons grade levels of obnoxiousness. Not so much the envy of the of all others inthe data science / machine learning community. More like the object of lust of all others inthe data science / machine learning community. Is miserable because they are not Mark Zuckerberg.

**Type V - the overseas data scientist:** As good as the olympian, the demigod, and the grumpy old hand combined, but nobody takes them seriously because of their skin color and very thick accent. Their career is additionally hampered by their cultural aversion to self-promotion and BS artistry which comes so naturally to many Westerners. Can do EDA, prototyping, production deployment, A/B testing, performance testing, and devOps all in one day, yet still somehow manages to be out of the office by 16:45  (they also don't show up until 9:45 in the morning). Is the reason why the hipster data scientist is able to get away with so little real work. Is the envy of all others in the data science / machine learning community, because they know that the overseas data scientist will be the last one to be let go in case of a $#!tst0rm, since they do the majority of the work on the team despite having the lowest salary. Is also the envy of everyone else, because at 35, they already own a home and have two teenage kids who are getting straight A+s in school. Is none the less miserable because their H1b might get revoked any day now.  

**Type VI - the stealth data scientist:** Very smart dev or TPM, who never actually used the title 'data scientist' or even 'machine learning engineer' (they might even sneer at those titles). A mix of the hipster, the demigod and the overseas data scientist in terms of personality and demographics, who works mainly on infrastructure or platform stuff, or maybe on the web portal team, but has also prototyped and deployed more than one regression or clustering model to production, without ever having taking a single machine learning or stats class. Has no idea who Andrew Ng is, and thinks A.I. is mainly about robotics and the Turing test. Masters Shell, Java, Scala, Kotlin, Node.js and PL/SQL, and can shift between on-prem and cloud systems at will. Thinks Python is a joke, and is floored every time somebody says they are a Python expert even though they don't know OOP. Is not so much envied, as they are feared by the rest of the data science and machine learning community, because the stealth is unphased by any of the Deep Learning hype, and once the AutoML frameworks finally mature, they know that the stealth will make the rest of them redundant. Is not miserable at all, because the stealth knows that they will be around long after the whole DS/ML hype died down, and don't really care because they will be Sr. Director or VP of Engineering by then anyway.",97,142
13,2019-6-1,2019,6,1,17,bvik3y,Returning after long term illness,https://www.reddit.com/r/MachineLearning/comments/bvik3y/returning_after_long_term_illness/,Deanodirector,1559376562,[removed],0,1
14,2019-6-1,2019,6,1,17,bvip5q,"I don't know if this has been posted here, but this is cool",https://www.reddit.com/r/MachineLearning/comments/bvip5q/i_dont_know_if_this_has_been_posted_here_but_this/,HodeMann,1559377943,,0,1
15,2019-6-1,2019,6,1,17,bvit1e,Telescopic forks for Automotive,https://www.reddit.com/r/MachineLearning/comments/bvit1e/telescopic_forks_for_automotive/,lhd121,1559379087,[removed],0,1
16,2019-6-1,2019,6,1,17,bvit3v,[AI application] Let your machine play Super Mario Bros!,https://www.reddit.com/r/MachineLearning/comments/bvit3v/ai_application_let_your_machine_play_super_mario/,1991viet,1559379106,,1,1
17,2019-6-1,2019,6,1,18,bvizke,Help: tenworflow 2.0 : tf.keras.experimental.export_saved_model error: __init__() missing 1 required positional argument: 'feature_columns',https://www.reddit.com/r/MachineLearning/comments/bvizke/help_tenworflow_20_tfkerasexperimentalexport/,nothingveryserious,1559380934,[removed],0,1
18,2019-6-1,2019,6,1,18,bvj5rt,Review paper on Deep Learning (possibly limited to computer vision),https://www.reddit.com/r/MachineLearning/comments/bvj5rt/review_paper_on_deep_learning_possibly_limited_to/,IborkedyourGPU,1559382580,[removed],0,1
19,2019-6-1,2019,6,1,19,bvjanr,[Discussion] How big is the problem of gathering and labeling pictures?,https://www.reddit.com/r/MachineLearning/comments/bvjanr/discussion_how_big_is_the_problem_of_gathering/,msmialko,1559383877,"I wonder how much of the problem for companies who are using machine learning in their products is to gather and label enough data. 

Im wondering if those companies would be interested in the idea that they post a need for data (eg we need pictures of flowers) and then people could take and upload photos and get paid for that. 
What do you think of that?",16,9
20,2019-6-1,2019,6,1,19,bvjghk,Wasserstein Autoencoder: pros &amp; cons,https://www.reddit.com/r/MachineLearning/comments/bvjghk/wasserstein_autoencoder_pros_cons/,IborkedyourGPU,1559385247,[removed],0,1
21,2019-6-1,2019,6,1,19,bvjmbf,[D] How would Grover or GPT-2 manage with writing code?,https://www.reddit.com/r/MachineLearning/comments/bvjmbf/d_how_would_grover_or_gpt2_manage_with_writing/,mrconter1,1559386492,,8,4
22,2019-6-1,2019,6,1,19,bvjn67,"Data Science Training In Tnagar, Chennai | Data Science Course in Tnagar",https://www.reddit.com/r/MachineLearning/comments/bvjn67/data_science_training_in_tnagar_chennai_data/,livewireindia,1559386684,,0,1
23,2019-6-1,2019,6,1,20,bvjv34,Accelerating Gradient Boosting Machine,https://www.reddit.com/r/MachineLearning/comments/bvjv34/accelerating_gradient_boosting_machine/,IborkedyourGPU,1559388542,,4,28
24,2019-6-1,2019,6,1,20,bvjwf3,[Project] I trained the NN to classify people's mentions by context. Here is the result with source code and online demo.,https://www.reddit.com/r/MachineLearning/comments/bvjwf3/project_i_trained_the_nn_to_classify_peoples/,generall93,1559388866,,0,2
25,2019-6-1,2019,6,1,20,bvjyid,[Project] Data Visualization for Exploration,https://www.reddit.com/r/MachineLearning/comments/bvjyid/project_data_visualization_for_exploration/,cstorm3000,1559389402,"I've compiled a notebook on visualization tricks I usually use for data exploration. Would love to have your feedback.

 What I intend to do:

* Understand as much as possible about the data as a human

What I DO NOT intend to do:

* Make beautiful graphs
* Feature engineer input for models

Kaggle kernel (Internet needed): [https://www.kaggle.com/cstorm3000/data-visualization-for-exploration](https://www.kaggle.com/cstorm3000/data-visualization-for-exploration) 

Github:  [https://github.com/cstorm125/viztech](https://github.com/cstorm125/viztech)",0,16
26,2019-6-1,2019,6,1,20,bvk2iw,[P] EfficientNet in PyTorch,https://www.reddit.com/r/MachineLearning/comments/bvk2iw/p_efficientnet_in_pytorch/,LukeAndGeorge,1559390382,"TL;DR: Implemented EfficientNet in PyTorch: [https://github.com/lukemelas/EfficientNet-PyTorch](https://github.com/lukemelas/EfficientNet-PyTorch)

&amp;#x200B;

I found [EfficientNet](https://arxiv.org/abs/1905.11946) to be quite an exciting paper, so I implemented it in PyTorch over the past two days. The implementation makes it simple to load pretrained models and integrate them into your own projects/research/models. GitHub: [https://github.com/lukemelas/EfficientNet-PyTorch](https://github.com/lukemelas/EfficientNet-PyTorch)

&amp;#x200B;

There are multiple examples in the GitHub repo and [here](https://colab.research.google.com/drive/1Jw28xZ1NJq4Cja4jLe6tJ6_F5lCzElb4) is one on Colab. It's as quick as 

    from efficientnet_pytorch import EfficientNet 
    model = EfficientNet.from_pretrained('efficientnet-b0')

And you can install it via pip if you would like: 

    pip install efficientnet_pytorch

Finally, there are scripts to evaluate on ImageNet (with training scripts coming soon) and there's functionality to easily extract image features. 

&amp;#x200B;

Let me know (either here or on GitHub) if you have any comments or find any bugs. I hope some of you find it useful!",19,177
27,2019-6-1,2019,6,1,21,bvkbu1,[R] Image Classification using context information,https://www.reddit.com/r/MachineLearning/comments/bvkbu1/r_image_classification_using_context_information/,uniqueUserName1527,1559392376,Hello everyone! Could someone recommend a good state-of-the-art paper and/or dataset for image classification using context information or metadata in general? I have been trying to get into the topic but there isn't much relevant research in that particular field yet and I am still looking for a good entry point.,3,2
28,2019-6-1,2019,6,1,21,bvkdb2,[D] An Explicitly Relational Neural Network Architecture,https://www.reddit.com/r/MachineLearning/comments/bvkdb2/d_an_explicitly_relational_neural_network/,mellow54,1559392683,,4,16
29,2019-6-1,2019,6,1,22,bvko71,A Visual Introduction to Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bvko71/a_visual_introduction_to_machine_learning/,andrea_manero,1559394889,[removed],0,1
30,2019-6-1,2019,6,1,23,bvl93e,C++ Neural Network library?,https://www.reddit.com/r/MachineLearning/comments/bvl93e/c_neural_network_library/,kaztale,1559398610,[removed],0,1
31,2019-6-2,2019,6,2,0,bvlqel,[D] Can we improve AI using Reinforcement Learning?,https://www.reddit.com/r/MachineLearning/comments/bvlqel/d_can_we_improve_ai_using_reinforcement_learning/,saadmrb,1559401540,,2,0
32,2019-6-2,2019,6,2,0,bvm4em,When will analog compute replace GPUs for machine learning?,https://www.reddit.com/r/MachineLearning/comments/bvm4em/when_will_analog_compute_replace_gpus_for_machine/,Randall172,1559403804,[removed],0,1
33,2019-6-2,2019,6,2,0,bvm4xx,DeepMind's AI Beats Humans At Quake III Arena,https://www.reddit.com/r/MachineLearning/comments/bvm4xx/deepminds_ai_beats_humans_at_quake_iii_arena/,FriendOfOrder,1559403893,,0,1
34,2019-6-2,2019,6,2,2,bvnif6,A simplified PyTorch implementation of GANsynth,https://www.reddit.com/r/MachineLearning/comments/bvnif6/a_simplified_pytorch_implementation_of_gansynth/,cloud60138,1559411502,[removed],0,1
35,2019-6-2,2019,6,2,3,bvnmaf,[P] A simplified PyTorch implementation of GANsynth,https://www.reddit.com/r/MachineLearning/comments/bvnmaf/p_a_simplified_pytorch_implementation_of_gansynth/,cloud60138,1559412116,"Hi everyone, the Github link below is my PyTorch implementation of GANsynth paper from magenta which published at ICLR'19. Now the repo only supports the best setting of the original paper. I will finish the rest of the code in a few weeks.

Link: [https://github.com/ss12f32v/GANsynth-pytorch](https://github.com/ss12f32v/GANsynth-pytorch)",0,20
36,2019-6-2,2019,6,2,3,bvnp2n,Data science bootcamp thoughts,https://www.reddit.com/r/MachineLearning/comments/bvnp2n/data_science_bootcamp_thoughts/,ComputerNerd16,1559412551,,0,1
37,2019-6-2,2019,6,2,3,bvo3xj,[D] An Introduction to Virtual Adversarial Training,https://www.reddit.com/r/MachineLearning/comments/bvo3xj/d_an_introduction_to_virtual_adversarial_training/,svufzafa,1559414920,"Virtual Adversarial Training is an effective regularization technique which has given good results in supervised learning, semi-supervised learning, and unsupervised clustering. In this article, I give an overview of the technique and also dive into the implementation. 

[https://divamgupta.com/unsupervised-learning/semi-supervised-learning/2019/05/31/introduction-to-virtual-adversarial-training.html](https://divamgupta.com/unsupervised-learning/semi-supervised-learning/2019/05/31/introduction-to-virtual-adversarial-training.html)

&amp;#x200B;

Link to my jupyter notebook:

[https://gist.github.com/divamgupta/c778c17459c1f162e789560d5e0b2f0b](https://gist.github.com/divamgupta/c778c17459c1f162e789560d5e0b2f0b)",6,108
38,2019-6-2,2019,6,2,4,bvodup,How can i do IoT to Machine Learning Data Transfer?,https://www.reddit.com/r/MachineLearning/comments/bvodup/how_can_i_do_iot_to_machine_learning_data_transfer/,psandeep777,1559416529,[removed],0,1
39,2019-6-2,2019,6,2,4,bvooft,a question regarding choosing compatible device,https://www.reddit.com/r/MachineLearning/comments/bvooft/a_question_regarding_choosing_compatible_device/,elena_eng,1559418193," I want to do detection and tracking through video. I will train YOLO model. I am planning to get a GPU enabled desktop and/or build a GPU enabled desktop.  
I really need an expert to tell me what to buy that it meets my need. I would appreciate if you could tell me what is the best to buy?  
I am doing this as an inspection surveillance project and I need to implement one in the real world. I heard that I would need a raspberry pi or other tools. But I also heard that raspberry pi is not powerful enough to run YOLO in real time.  
What would be your suggestion for choosing devices for my project?",0,1
40,2019-6-2,2019,6,2,5,bvov70,"A Deeper Diver into UBI, Automation and the Economy of Tomorrow | Martin Ford",https://www.reddit.com/r/MachineLearning/comments/bvov70/a_deeper_diver_into_ubi_automation_and_the/,The_Syndicate_VC,1559419257,[removed],0,1
41,2019-6-2,2019,6,2,6,bvq4nz,Can anyone tell me what I should be searching/asking for on fiverr - I have this machine learning job. thanks,https://www.reddit.com/r/MachineLearning/comments/bvq4nz/can_anyone_tell_me_what_i_should_be/,harrymanders,1559426305,[removed],0,1
42,2019-6-2,2019,6,2,7,bvq6hz,How do Keras LSTMs work?,https://www.reddit.com/r/MachineLearning/comments/bvq6hz/how_do_keras_lstms_work/,userjoinedyourchanel,1559426574,[removed],0,1
43,2019-6-2,2019,6,2,7,bvq82p,Do you know what Babble Labs means?,https://www.reddit.com/r/MachineLearning/comments/bvq82p/do_you_know_what_babble_labs_means/,Gabyleon2019,1559426819,[removed],0,1
44,2019-6-2,2019,6,2,8,bvqs62,"Is it possible to automate the game ""AMAZE!""?",https://www.reddit.com/r/MachineLearning/comments/bvqs62/is_it_possible_to_automate_the_game_amaze/,ColdPresent,1559430088,[removed],0,1
45,2019-6-2,2019,6,2,8,bvqugi,Need a bit of help,https://www.reddit.com/r/MachineLearning/comments/bvqugi/need_a_bit_of_help/,HippieKittie,1559430468,[removed],1,1
46,2019-6-2,2019,6,2,10,bvrzah,How do I use Google's BERT with Keras?,https://www.reddit.com/r/MachineLearning/comments/bvrzah/how_do_i_use_googles_bert_with_keras/,moonman239-redditbot,1559437590,[removed],0,1
47,2019-6-2,2019,6,2,10,bvs249,Deep Unsupervised Learning using Nonequilibrium Thermodynamics (2015) - Is this the same as flow-based generative models using neural differential equations?,https://www.reddit.com/r/MachineLearning/comments/bvs249/deep_unsupervised_learning_using_nonequilibrium/,HumanSpinach2,1559438099,[removed],0,1
48,2019-6-2,2019,6,2,10,bvs7q8,[R] Crystal Graph Neural Networks for Data Mining in Materials Science,https://www.reddit.com/r/MachineLearning/comments/bvs7q8/r_crystal_graph_neural_networks_for_data_mining/,TonyY_RIMCS,1559439128,"This study introduces scale-invariant crystal graphs to build machine learning models based only on topological information of crystalline materials. The CGNN models trained on a 561k [OQMD](http://oqmd.org) dataset gave much less errors.

[View PDF](https://storage.googleapis.com/rimcs_cgnn/cgnn_matsci_May_27_2019.pdf)

&amp;#x200B;

A PyTorch implementation of CGNN used in this study was open-sourced:

[GitHub Pages site](https://tony-y.github.io/cgnn/)

[GitHub repo](https://github.com/Tony-Y/cgnn)

[A crystal graph of SiO2 \(left\) and the CGNN architecture \(right\) ](https://i.redd.it/4s3lqgbzfu131.png)",3,19
49,2019-6-2,2019,6,2,10,bvscrd,Remember Recurrent Networks,https://www.reddit.com/r/MachineLearning/comments/bvscrd/remember_recurrent_networks/,MusingEtMachina,1559440024,,0,1
50,2019-6-2,2019,6,2,11,bvsom0,Deep Unsupervised Learning using Nonequilibrium Thermodynamics (2015) - Is this the same as flow-based generative models using neural differential equations?,https://www.reddit.com/r/MachineLearning/comments/bvsom0/deep_unsupervised_learning_using_nonequilibrium/,HumanSpinach2,1559442231,[removed],0,1
51,2019-6-2,2019,6,2,11,bvsvv7,"[D] I'm writing a full C++ wrapper for Tensorflow, is anyone at all interested?",https://www.reddit.com/r/MachineLearning/comments/bvsvv7/d_im_writing_a_full_c_wrapper_for_tensorflow_is/,memento87,1559443598,"I've asked this question around long before I began working on this project and here's a (non-exhaustive) list of answers I got:

1) Building ML models in Python is faster and easier

2) There's absolutely no use-case where you might need to train in C++

3) If performance is what you're after, why not train in Python then export your model for inference in C++

4) If you insist on C++, why not use caffe, mxnet or pytorch

And my answers are the following:

1) It's easier if you're comfortable with Python. Personally, I hate Python and I am never comfortable working with untyped languages. I may be old school but I have 15+ years of C++ experience and that makes it easier and faster for me.

2) Here's a few use cases off the top of my head:
- Using the library to perform tensor calculations on the GPU, for non machine-learning uses (such as Audio DSPs, Ray Tracing, etc...) while still benefitting from TF's optimizations and distributed graph computation capabilities
- Training Unsupervised ML models with data read from physical sensors in realtime
- Training models that require some lengthy data preprocessing or postprocessing that need to be done on CPU
- Online-Training models on devices that are memory or battery constrained where having a Python interpreter and a webserver to serve the inference model would be wasteful

3) See #2 There are many cases where this assumption doesn't hold

4) I tried mxnet for a year before eventually giving up. The library is so unstable and buggy it's barely usable. Also tensorflow is truly remarkable when it comes to its distributed graph computation capabilities and seems to be the most evolved in terms of portability. It works across OSs, GPUs, TPUs, different CPU arch, etc... Not to mention the large community and active Google support

With all that being said, I'm interested to know what everyone thinks. I plan on Open-Sourcing the wrapper eventually but that would require some extra work on my behalf, proper documentation and working examples, etc... I'd push that further if I know there's absolutely no interest in the project.

What are your thoughts?",63,143
52,2019-6-2,2019,6,2,14,bvuc0n,One of the BEST MachineLearning Glossary by Google,https://www.reddit.com/r/MachineLearning/comments/bvuc0n/one_of_the_best_machinelearning_glossary_by_google/,ai-lover,1559454466,,0,1
53,2019-6-2,2019,6,2,14,bvucn8,How Do Machines Learn?,https://www.reddit.com/r/MachineLearning/comments/bvucn8/how_do_machines_learn/,Oleguardian,1559454625,,0,1
54,2019-6-2,2019,6,2,15,bvug5b,Is anyone actively working on a neural net to replicate something of a common sense or moral compass decision outcome based on a dataset of moral decisions of the general population?,https://www.reddit.com/r/MachineLearning/comments/bvug5b/is_anyone_actively_working_on_a_neural_net_to/,cduke921,1559455468,[removed],0,1
55,2019-6-2,2019,6,2,15,bvumis,[P] (WIP Book) Java Deep Learning cookbook (Recipe Based practical approach),https://www.reddit.com/r/MachineLearning/comments/bvumis/p_wip_book_java_deep_learning_cookbook_recipe/,willis7747,1559457137,"My In-progress book aimed at implementation of deep learning use-cases using deeplearning4j, a JVM based deep learning library. If you wonder why Java or any JVM languages for this, you may read more about it [here](https://skymind.ai/wiki/java-ai).  Codebase is already setup here:  [https://github.com/rahul-raj/Java-Deep-Learning-Cookbook](https://github.com/rahul-raj/Java-Deep-Learning-Cookbook)    
Codebase gets updated as the book progress. We will add helpful comments in the code and push further optimizations before we release the book on this September. We're welcoming feedback and still room for lots of improvements. Feel free to make a clone, run on your local and have fun :)",4,20
56,2019-6-2,2019,6,2,15,bvupfa,[D] Approach to taking variable-length player action history as model input?,https://www.reddit.com/r/MachineLearning/comments/bvupfa/d_approach_to_taking_variablelength_player_action/,oops_ur_dead,1559457925,"I'm trying to build a deep learning model for playing a particular game where previous action history is crucial in deciding future actions. I'm wondering if any of you have any tips for how best to encode this history of actions; for this example lets say that there are a set of actions that each player can perform, some of which have another player as a target, and some of which don't. I was thinking about doing one-hot encoding for action type along with a static player ID number, then feeding the entire history into an LSTM. Are there any better ways?",2,8
57,2019-6-2,2019,6,2,18,bvvjuq,Application of 2D keypoints from pose estimation in field of fitness ?,https://www.reddit.com/r/MachineLearning/comments/bvvjuq/application_of_2d_keypoints_from_pose_estimation/,VisionBasics17,1559466538,,0,1
58,2019-6-2,2019,6,2,18,bvvk6i,[Discussion] Moving to ML after long term illness,https://www.reddit.com/r/MachineLearning/comments/bvvk6i/discussion_moving_to_ml_after_long_term_illness/,Deanodirector,1559466625,[removed],0,1
59,2019-6-2,2019,6,2,18,bvvkx1,Advice on a Career transition into ML/DL Engineer,https://www.reddit.com/r/MachineLearning/comments/bvvkx1/advice_on_a_career_transition_into_mldl_engineer/,praveen369,1559466824,[removed],0,1
60,2019-6-2,2019,6,2,18,bvvq90,[D] Are normalizing flows dead?,https://www.reddit.com/r/MachineLearning/comments/bvvq90/d_are_normalizing_flows_dead/,mellow54,1559468292,"I seldom hear the triumphs and accomplishments of normalizing flows that I hear about from GANs, VAEs and even autoregressive generative models (like PixelCNN). Are the benefits of flows understated? What's their future?",19,10
61,2019-6-2,2019,6,2,18,bvvqml,"[P] Simple Tensorflow implementation of Semantic Image Synthesis with Spatially-Adaptive Normalization (CVPR 2019 Oral) a.k.a. SPADE, GauGAN",https://www.reddit.com/r/MachineLearning/comments/bvvqml/p_simple_tensorflow_implementation_of_semantic/,taki0112,1559468394,,0,1
62,2019-6-2,2019,6,2,18,bvvrp8,"[P] Simple Tensorflow implementation of GauGAN (SPADE, CVPR 2019 Oral)",https://www.reddit.com/r/MachineLearning/comments/bvvrp8/p_simple_tensorflow_implementation_of_gaugan/,taki0112,1559468687,"&amp;#x200B;

[Style Manipulation of women](https://i.redd.it/p524sb1yyw131.png)

&amp;#x200B;

[Style Manipulation of men](https://i.redd.it/q8sfn5d2zw131.png)",8,31
63,2019-6-2,2019,6,2,19,bvw5l6,[D] Confusions about Entity/Categorical embeddings,https://www.reddit.com/r/MachineLearning/comments/bvw5l6/d_confusions_about_entitycategorical_embeddings/,KindYAK,1559472405,"Hello r/Machine Learning,

I have a couple of questions regarding entity/categircal embeddings that I hope we could clear out:
1) Can Entity/Categorical embeddings be used for an output layer (for multi-class classification task)?
2) Can I use entity/categorical embeddings if I a variable can store several categories (for example I have three classes {A, B, C} and an object can be {A and B})? Is there a python implementation for it?
3) Is Keras Embedding layer is actually performing entity embedding? The documentation doesn't seem to be clear in this regard

Thank you :)",1,5
64,2019-6-2,2019,6,2,20,bvwgbb,"Is your country even aware of *this*? And if so, what's it doing about it?",https://www.reddit.com/r/MachineLearning/comments/bvwgbb/is_your_country_even_aware_of_this_and_if_so/,CrashCourseHEMA,1559475165,[removed],0,1
65,2019-6-2,2019,6,2,21,bvwocv,Goodharts Law: Are Academic Metrics Being Gamed?,https://www.reddit.com/r/MachineLearning/comments/bvwocv/goodharts_law_are_academic_metrics_being_gamed/,hughbzhang,1559477079,[removed],0,1
66,2019-6-2,2019,6,2,21,bvwol4,[D] Goodharts Law: Are Academic Metrics Being Gamed?,https://www.reddit.com/r/MachineLearning/comments/bvwol4/d_goodharts_law_are_academic_metrics_being_gamed/,hughbzhang,1559477132,"[https://thegradient.pub/over-optimization-of-academic-publishing-metrics/](https://thegradient.pub/over-optimization-of-academic-publishing-metrics/)

&amp;#x200B;

Michael Fire claims that authors in machine learning (and also in general academia) are gaming their paper and citation counts.",11,11
67,2019-6-2,2019,6,2,21,bvwvoo,[P] AI Against Humanity: Play Cards Against Humanity with GPT-2-generated cards,https://www.reddit.com/r/MachineLearning/comments/bvwvoo/p_ai_against_humanity_play_cards_against_humanity/,cpury,1559478724,"I was toying around with GPT-2 and found it can actually generate some pretty messed up / funny CAH cards! So I built a little game around it: [https://www.aiagainsthumanity.app/](https://www.aiagainsthumanity.app/)

Right now, you can select your favorite answer out of five choices. I log all decisions made in order to train an AI-opponent in the future. Some features that are planned:

* Play against an AI opponent
* Invite your friends
* More cards, also questions with more than one gap
* Share your favorite card combos with friends
* Level up
* Info page for each card where you can discuss and vote on them

Let me know what you think!",60,274
68,2019-6-2,2019,6,2,22,bvx88s,Small projects &amp; datasets for learning GANs &amp; Style Transfer,https://www.reddit.com/r/MachineLearning/comments/bvx88s/small_projects_datasets_for_learning_gans_style/,PyWarrior,1559481254,[removed],0,1
69,2019-6-2,2019,6,2,22,bvxacy,[R] Segmentation is All You Need,https://www.reddit.com/r/MachineLearning/comments/bvxacy/r_segmentation_is_all_you_need/,xternalz,1559481641,,8,3
70,2019-6-2,2019,6,2,22,bvxb6q,Deep learning; shallow understanding.,https://www.reddit.com/r/MachineLearning/comments/bvxb6q/deep_learning_shallow_understanding/,GoodStrat,1559481800,,1,1
71,2019-6-2,2019,6,2,22,bvxjsy,current state of the arte for GENs for image generation,https://www.reddit.com/r/MachineLearning/comments/bvxjsy/current_state_of_the_arte_for_gens_for_image/,XMasterDE,1559483423,[removed],0,1
72,2019-6-2,2019,6,2,23,bvxnqb,Does anybody use paperspace? It seems so promising but it's so messy,https://www.reddit.com/r/MachineLearning/comments/bvxnqb/does_anybody_use_paperspace_it_seems_so_promising/,Tramagust,1559484146,[removed],0,1
73,2019-6-2,2019,6,2,23,bvxrxw,[D] Does anybody use paperspace? It seems so promising but it's so messy (,https://www.reddit.com/r/MachineLearning/comments/bvxrxw/d_does_anybody_use_paperspace_it_seems_so/,Tramagust,1559484879,"I was attracted away from AWS by paperspace claiming it's so fast and easy with ML in a box. Well it seems fast and easy but in reality it's a different kind of painful. Don't get me wrong getting nvidia drivers and docker to work in AWS is quite the challenge sometimes.

Paperspace offers their own ""web"" terminal doesn't seem to support copy/paste, their inbuilt remote desktop connection is very laggy and keeps dropping out and to use my own SSH connection I have to pay a 3 dollar a month fee for a public IP?!?

Honestly what hurts me the most is the incosistent copy paste. It barely works in their native app and their native terminal. It's unbelievable how buggy their connection software is. On top of that the password they provide when you create a machine doesn't seem to be the root password. I can sudo just fine but there doesn't seem to be a way to become root at all. I'm a little in disbelief.

Has anyone had any good experience with their system? I think there's no way their system is this bad an I'm just using it wrong.",16,11
74,2019-6-2,2019,6,2,23,bvxvru,So techniques like MAML or Reptile meta-learning learns a good initialization?,https://www.reddit.com/r/MachineLearning/comments/bvxvru/so_techniques_like_maml_or_reptile_metalearning/,qudcjf7928,1559485532,[removed],0,1
75,2019-6-3,2019,6,3,0,bvyc9z,[P] nn_builder - a new package that builds neural networks in 1 line,https://www.reddit.com/r/MachineLearning/comments/bvyc9z/p_nn_builder_a_new_package_that_builds_neural/,__data_science__,1559488340,"nn\_builder is a new package that lets you build neural networks in 1 line using PyTorch or TensorFlow 2.0 that lots of you might find useful!

Let me know what you think and if you'd like to contribute[https://github.com/p-christ/nn\_builder](https://github.com/p-christ/nn_builder)",0,0
76,2019-6-3,2019,6,3,0,bvydor,[D] Differences between ML conferences,https://www.reddit.com/r/MachineLearning/comments/bvydor/d_differences_between_ml_conferences/,dramanautica,1559488567,Could someone give me a run down of what kind of papers should be submitted to each ML conference (I.e. theoretical ML to NeurIPS etc)?,9,14
77,2019-6-3,2019,6,3,1,bvyz3e,Very basic digit recognition real time . Ignore the silly mistakes. I wanted to see whether it would work or not that's it.,https://www.reddit.com/r/MachineLearning/comments/bvyz3e/very_basic_digit_recognition_real_time_ignore_the/,b14cksh4d0w369,1559491863,,0,1
78,2019-6-3,2019,6,3,1,bvz20m,[R] [1905.11169] Physics-as-Inverse-Graphics: Joint Unsupervised Learning of Objects and Physics from Video,https://www.reddit.com/r/MachineLearning/comments/bvz20m/r_190511169_physicsasinversegraphics_joint/,PeterPrinciplePro,1559492313,,1,7
79,2019-6-3,2019,6,3,1,bvz2oh,"[R] [1905.11940] Cerberus: A Multi-headed Derenderer (Deng, Kornblith, Hinton)",https://www.reddit.com/r/MachineLearning/comments/bvz2oh/r_190511940_cerberus_a_multiheaded_derenderer/,PeterPrinciplePro,1559492413,,1,2
80,2019-6-3,2019,6,3,1,bvzc7w,[D] Has anyone noticed a lot of ML research into facial recognition of Uyghur people lately?,https://www.reddit.com/r/MachineLearning/comments/bvzc7w/d_has_anyone_noticed_a_lot_of_ml_research_into/,Kickuchiyo,1559493858,"[https://i.imgur.com/7lCmYQt.jpg](https://i.imgur.com/7lCmYQt.jpg)
[https://i.imgur.com/KSSVkGT.jpg](https://i.imgur.com/KSSVkGT.jpg)

This popped up on my feed this morning and I thought it was interesting/horrifying.",246,999
81,2019-6-3,2019,6,3,2,bvzmtl,Would it be possible to use Generative models in football analytics?,https://www.reddit.com/r/MachineLearning/comments/bvzmtl/would_it_be_possible_to_use_generative_models_in/,veranceftw,1559495400,"I was wondering if we could apply generative models in football analysis. Imagine taking event data from every match of a specific team and try to generate a synthetic match in order to preview or simulate different plays.   
However, it doesn't seem feasible; teams change accordingly to their manager and match itself. I don't think that we can use data from 5 seasons and expect your results to be reliable. Even if we only take the pass map into account and create a network with that, we would never have enough data to generate a whole match based on the lineup, manager, opponent, local (away/home), (...)

&amp;#x200B;

Have you ever thought about that? What is the use of Machine Learning in football at this moment? Geometric Deep Learning seems promising to study the team and its mechanincs and the interrelational behaviour.",0,1
82,2019-6-3,2019,6,3,2,bvznoc,[P] kubeflow tutorial,https://www.reddit.com/r/MachineLearning/comments/bvznoc/p_kubeflow_tutorial/,nlkey2022,1559495522,"Hello.

This is project a guideline for basic use and installation of kubeflow in AWS.

 Kubeflow is a Cloud Native platform for machine learning based on Googles internal machine learning pipelines to ml-serving, Devops, distributed training, etc.

Thanks

&amp;#x200B;

 [https://github.com/graykode/aws-kubeflow](https://github.com/graykode/aws-kubeflow)",0,4
83,2019-6-3,2019,6,3,3,bw0fve,Ian Goodfellow's Deep Learning book problems,https://www.reddit.com/r/MachineLearning/comments/bw0fve/ian_goodfellows_deep_learning_book_problems/,andwhata,1559499693,[removed],0,1
84,2019-6-3,2019,6,3,3,bw0ro0,Hypothesis testing visualized,https://www.reddit.com/r/MachineLearning/comments/bw0ro0/hypothesis_testing_visualized/,rohitpandey576,1559501444,,0,1
85,2019-6-3,2019,6,3,4,bw0xzx,Bayes Theorem: A Primer,https://www.reddit.com/r/MachineLearning/comments/bw0xzx/bayes_theorem_a_primer/,0_marauders_0,1559502393,[removed],0,1
86,2019-6-3,2019,6,3,4,bw0z5h,"Help finding research on image classification tasks for which the output class depends on the spatial location of objects, rather than the category of objects.",https://www.reddit.com/r/MachineLearning/comments/bw0z5h/help_finding_research_on_image_classification/,gangstergattuso,1559502561,"For example, an image with a cat in the center would have a different class compared to an image with a cat in the bottom-left.",0,1
87,2019-6-3,2019,6,3,4,bw15ps,[D] How to estimate fine-grained labels from coarse labels + rules ?,https://www.reddit.com/r/MachineLearning/comments/bw15ps/d_how_to_estimate_finegrained_labels_from_coarse/,farmingvillein,1559503524,"Set up:

I have a large number of free-text documents that have coarse labels and would like to get approximated fine-grained labels.

(For the sake of this setup, let's say that that getting sufficient labeled fine-grained is to do a full-supervised set up is too costly.)

* The relation between the coarse &amp; fine-grained labels can be described deterministically (i.e., if a given set of fine-grained labels are true, we can write a rule that states what the coarse label will be).  But this relationship is many fine : one coarse (i.e., there are multiple sets of fine-grained labels which can describe a given coarse label).

* There is overlap between what fine-grained labels drive which coarse results ({""furry"", ""whiskers""} =&gt; ""cat"", {""furry"", ""barks""} =&gt; ""dog"").  

I'm trying to figure out a good way to try unsupervised techniques to approximate these coarse-grained labels, leveraging the domain rules on-hand (but lacking any explicit distributions for fine-grained labels and between fine-grained &amp; coarse).  

Conceptually, this seems like this could be analogous to a clustering problem, where we are trying to simultaneously cluster the docs on the different fine-grained label dimensions, and use the domain rules to enforce relative relations between the labels clusters (""if doc X is clustered next to label Y1, then it can't be clustered next to fine-grained label B1, because B1+Y1=&gt;wrong coarse label"").

But I'm having trouble finding any references relevant or similar to what I'm trying to accomplish (very possibly because I'm not using the write language to search).

Of course, welcome alternate suggestions.  I also considered something like simply randomly labeling each doc w/ fine-grained labels pulled from the valid sets (given the coarse docs), and then seeing whether the predictions (trained on these very noisy labeling) stabilize on something that seems ""reasonable"".  This seems...speculative...however, particularly because it is explicitly putting a prior in place on the distribution of the fine-grained labels which is almost certainly fairly incorrect.

For overall context/goals here--the fine-grained label accuracy doesn't have to be perfect.  The business issue here is that the coarse labels are predicted and then hard to understand (on correctness and on interpretability), without a lot of domain work to back into where the coarse label should have been derived from and whether it was correct.  Also predicting fine-grained labels--even if somewhat noisy--will help humans ""eyeball"" the outputs on the coarse labels to understand system accuracy.",0,1
88,2019-6-3,2019,6,3,4,bw1ibm,The MLOps NYC call for papers is officially open!,https://www.reddit.com/r/MachineLearning/comments/bw1ibm/the_mlops_nyc_call_for_papers_is_officially_open/,IguazioDani,1559505404,"Submit your abstract through the site - anything about machine learning pipelines, open source AI, serverless automation, multi-model data access and datascience at scale is most welcome [https://www.mlopsnyc.com/](https://www.mlopsnyc.com/).",0,1
89,2019-6-3,2019,6,3,5,bw1jm7,[D] Machine Learning - WAYR (What Are You Reading) - Week 64,https://www.reddit.com/r/MachineLearning/comments/bw1jm7/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,1559505605,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|
|----|-----|-----|-----|-----|-----|-----|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)||

Most upvoted papers two weeks ago:

/u/mowrilow: [This website](http://geometricdeeplearning.com/)

Besides that, there are no rules, have fun.",4,6
90,2019-6-3,2019,6,3,5,bw1jxb,"[x-post r/machinelearning] China found to be doing a shit ton of research into facial recognition lately, particularly of ethnic minority Uyghurs.",https://www.reddit.com/r/MachineLearning/comments/bw1jxb/xpost_rmachinelearning_china_found_to_be_doing_a/,Zermutt,1559505646,,0,1
91,2019-6-3,2019,6,3,5,bw1jxm,[D] ML Terminology in Clinical Prediction as well as the consequence of certain predictors being weighted the most important,https://www.reddit.com/r/MachineLearning/comments/bw1jxm/d_ml_terminology_in_clinical_prediction_as_well/,slimuser98,1559505647,"I was recently reading this paper:

&amp;#x200B;

[Predicting suicide attempts in adolescents with longitudinal clinical data and machine learning](https://sci-hub.tw/https://doi.org/10.1111/jcpp.12916)

&amp;#x200B;

First thing I wanted to discuss was the use of the term control. They had 3 ""control groups"" (OSI, Depressed, General) and they had a group of cases. In terms of model construction and validation they say:

&amp;#x200B;

\&gt;A total of 1,470 adolescents with ICD codes for suicide and self-inflicted injury (i.e. E950E959) were identified for model development and validation.

&amp;#x200B;

So to my understanding, they built the model using cases and OSI, then when testing discrimination, they mix in the cases with the control group of interest to see how well they can identify them? Or based on the general aim of their paper, are they trying to detect suicide attempts within the controls. I don't necessarily see this because they don't state how many true cases are within each group. I'm used to the use of controls in terms of measuring an effect of a treatment and less so in terms of prediction models (I'm more used to training vs testing datasets or people not even doing that).

&amp;#x200B;

Another confusing issue, was the fact that they used their control (OSI) as part of the model development and validation. Generally speaking, shouldn't it be separate (i.e. you have training and testing). I was also confused by their Figures 4A-B in terms of ""depressed control comparison"" (4A) vs ""general control comparison"" (4B). I thought the only thing being compared is cases to controls. 

&amp;#x200B;

Finally, I'm not sure how much of a concern this is, but I found it interesting (see charts) that BMI and Age were generally some of the best predictors. In a sense, while the algorithms don't care and will just choose what best discriminates, I find it concerning that these kind of superficial predictors, which in some ways (more so BMI than age) have little to do with suicide attempts, are considered the most powerful. I am not sure if this kind of information implies anything about the generalizability of the model where one wouldn't be able to find such systematic difference in terms of BMI and Age.",0,1
92,2019-6-3,2019,6,3,5,bw1u10,Ask Reddit: How to interview a ML candidates if I'm not an ML expert?,https://www.reddit.com/r/MachineLearning/comments/bw1u10/ask_reddit_how_to_interview_a_ml_candidates_if_im/,hazard02,1559507141,[removed],0,1
93,2019-6-3,2019,6,3,6,bw2c5u,Better approximations than greedy?,https://www.reddit.com/r/MachineLearning/comments/bw2c5u/better_approximations_than_greedy/,t4YWqYUUgDDpShW2,1559509848,[removed],0,1
94,2019-6-3,2019,6,3,6,bw2jpg,[D] Ask Reddit: How to interview a ML candidates if I'm not an ML expert?,https://www.reddit.com/r/MachineLearning/comments/bw2jpg/d_ask_reddit_how_to_interview_a_ml_candidates_if/,hazard02,1559511009,"I've been assigned by my company to interview some ML research &amp; engineer candidates. However, I myself am not an ML expert, although I'm fairly familiar with ML at the ""hobbyist"" level. For instance, I've implemented RL algorithms in Pytorch and I'm comfortable reading many ML papers, but I'm still struggling to understand Transformer. Outside of ML I would consider myself fairly strong in C++/Python, probably more so than the average ML candidate.

How can a non-ML-expert effectively interview ML candidates? What questions would you ask? If you were applying, what questions would you want someone who isn't an ML expert to ask you?",10,1
95,2019-6-3,2019,6,3,6,bw2oh5,Machine Learning Algorithm Identifies Tweets Sent Under the Influence of Alcohol,https://www.reddit.com/r/MachineLearning/comments/bw2oh5/machine_learning_algorithm_identifies_tweets_sent/,andrea_manero,1559511752,[removed],0,1
96,2019-6-3,2019,6,3,7,bw2zsr,[P] A reimplementation of EfficientNet,https://www.reddit.com/r/MachineLearning/comments/bw2zsr/p_a_reimplementation_of_efficientnet/,efficientnets,1559513492,Code on GitHub: https://github.com/qubvel/efficientnet,2,6
97,2019-6-3,2019,6,3,7,bw3b1w,No App for ICML or CVPR 2019,https://www.reddit.com/r/MachineLearning/comments/bw3b1w/no_app_for_icml_or_cvpr_2019/,kittykatkitkatkitty,1559515293,[removed],0,1
98,2019-6-3,2019,6,3,7,bw3h3r,[D] Adding more data will make the model perform worse ?,https://www.reddit.com/r/MachineLearning/comments/bw3h3r/d_adding_more_data_will_make_the_model_perform/,Bigdey,1559516275,"Hi, I am using XGboost regressor for a personal project. Initially I used a data set with measurements from 01.Jan.2016 to 24.Dec.2018 and I got those results : MAE =  2.332 , MSE =  7.764  for testing data. I recently got from the same source, the same data set but with measurements from 01.Jan.2016 up to 14.May.2019 and  for testing data I got those results : MAE =  2.729 , MSE =  12.002. I have tuned the  hyperparameters, in both cases, using the same method through cv. I tried to adjust the parameters more for the second data set but I did not get better results. Even if the differences are not very high, the fact that I used a larger data set could have affected the performance or is something I have overlooked?",8,3
99,2019-6-3,2019,6,3,8,bw3pxz,What is an area of machine learning which is ripe for a major breakthrough in next 3-6 years?,https://www.reddit.com/r/MachineLearning/comments/bw3pxz/what_is_an_area_of_machine_learning_which_is_ripe/,namuradAulad,1559517706,[removed],0,1
100,2019-6-3,2019,6,3,8,bw3v07,Machine Learning and Urbanism Game,https://www.reddit.com/r/MachineLearning/comments/bw3v07/machine_learning_and_urbanism_game/,tiramirez,1559518537,[removed],0,1
101,2019-6-3,2019,6,3,9,bw44yh,[R] Finite-time Analysis of Approximate Policy Iteration for the Linear Quadratic Regulator,https://www.reddit.com/r/MachineLearning/comments/bw44yh/r_finitetime_analysis_of_approximate_policy/,karilex,1559520277,"We recently released a preprint analyzing policy iteration on LQR. Here's the arxiv paper:
https://arxiv.org/abs/1905.12842

And a tl;dr explanation on Twitter:
https://twitter.com/Krauth/status/1134630118930997249",0,3
102,2019-6-3,2019,6,3,9,bw4epf,World Wide Technology AI R&amp;D Program - white papers,https://www.reddit.com/r/MachineLearning/comments/bw4epf/world_wide_technology_ai_rd_program_white_papers/,wwtAI-Research,1559521950,"This is our first post to reddit from [World Wide Technology's](https://www.wwt.com/) Business and Analytics Advisors team, and this thread seemed like the right place to start. We would love to get everyone's thoughts on our AI R&amp;D program we started about a year and half ago, in particular the white papers we have been publishing on our website. Our data science and engineering teams have been working diligently on topics we know our customers are only thinking about now (or will be thinking about soon), and will probably be a reality in 3-5 years. We want to stay ahead of the market and make sure we are learning and sharing our findings across a number of areas. [This](https://www.wwt.com/all-blog/uncovering-ai-research-development/) is a blog post talking about why we are doing R&amp;D and at the bottom are links to the abstracts of the 6 papers we have published so far. Please kindly register for the website to download the papers. We promise we won't bombard you with spam!

We look forward to your thoughts!

[https://www.wwt.com/all-blog/uncovering-ai-research-development/](https://www.wwt.com/all-blog/uncovering-ai-research-development/)",0,1
103,2019-6-3,2019,6,3,9,bw4mc7,How to write a custom objective for XGBoost?,https://www.reddit.com/r/MachineLearning/comments/bw4mc7/how_to_write_a_custom_objective_for_xgboost/,mydogissnoring,1559523272,[removed],0,1
104,2019-6-3,2019,6,3,10,bw4qf7,[D] XGBoost Custom Objective,https://www.reddit.com/r/MachineLearning/comments/bw4qf7/d_xgboost_custom_objective/,mydogissnoring,1559523996,"I have seen that for xgboost you can write your own loss function, and have even seen the example on the xgboost github. What I am wondering is there an example somewhere about how to go about developing that code?

In other words, say I have some metric (other than say squared loss) that I want my model to optimize and use to determine the weights. It looks like I need to determine the gradient and hessian, but I'm not sure how to figure this out.",10,7
105,2019-6-3,2019,6,3,10,bw4rx9,[R] World Wide Technology's AI R&amp;D Program - White Papers,https://www.reddit.com/r/MachineLearning/comments/bw4rx9/r_world_wide_technologys_ai_rd_program_white/,wwtAI-Research,1559524259,"This is our first post to reddit from [World Wide Technology's](https://www.wwt.com/) Business and Analytics Advisors team, and this thread seemed like the right place to start. We would love to get everyone's thoughts on our AI R&amp;D program we started about a year and half ago, in particular the white papers we have been publishing on our website. Our data science and engineering teams have been working diligently on topics we know our customers are only thinking about now (or will be thinking about soon), and will probably be a reality in 3-5 years. We want to stay ahead of the market and make sure we are learning and sharing our findings across a number of areas. [This](https://www.wwt.com/all-blog/uncovering-ai-research-development/) is a blog post talking about why we are doing R&amp;D and at the bottom are links to the abstracts of the 6 papers we have published so far. Please kindly register for the website to download the papers. We promise we won't bombard you with spam!

We look forward to your thoughts!

[https://www.wwt.com/all-blog/uncovering-ai-research-development/](https://www.wwt.com/all-blog/uncovering-ai-research-development/)",1,0
106,2019-6-3,2019,6,3,10,bw50ye,"Recommender System GitHub Repository - ""awesome-RecSys""",https://www.reddit.com/r/MachineLearning/comments/bw50ye/recommender_system_github_repository_awesomerecsys/,data-chef,1559525831,[removed],0,1
107,2019-6-3,2019,6,3,11,bw5j5w,[D] Reading group substitute,https://www.reddit.com/r/MachineLearning/comments/bw5j5w/d_reading_group_substitute/,Maplernothaxor,1559529019,"While at university I used paper reading groups to keep on top of current developments in ML. However, now that Im in industry (in a non-research position), how do I maintain my knowledge of cutting edge ML progress? There isnt enough time for me to read through every paper that is on here/arxivsanity.


Any tips?",11,8
108,2019-6-3,2019,6,3,11,bw5sob,IEEE lifts its ban on Huawei employees,https://www.reddit.com/r/MachineLearning/comments/bw5sob/ieee_lifts_its_ban_on_huawei_employees/,shannoncoin,1559530732,[removed],0,1
109,2019-6-3,2019,6,3,12,bw5xqt,[N] IEEE lifts its ban on Huawei employees,https://www.reddit.com/r/MachineLearning/comments/bw5xqt/n_ieee_lifts_its_ban_on_huawei_employees/,shannoncoin,1559531639,"An email sent to IEEE members:

&gt; Last week the U.S. government issued export controls on Huawei Technologies Ltd and 68 of its affiliated companies. In response, IEEE issued guidance on actions required to comply with these controls. We acted promptly because we wanted to protect our volunteers and members from potential legal risk that could have involved significant penalties. As a non-political, not-for-profit organization registered in New York, IEEE must comply with its legal obligations under the laws of the United States and other jurisdictions. We also engaged the U.S. government to seek clarification on the extent to which these export control restrictions were applicable to IEEE activities.

&gt; I am pleased to report that this engagement was successful and we have revised our guidance to remove any restriction on the participation of the employees of these companies as editors or peer reviewers in the IEEE publication process. To reemphasize, all IEEE members can continue to participate in the open and public activities of the IEEE, including our scientific and technical publications.

&gt; Many members expressed apprehension with respect to IEEEs initial guidance and its impact on editors and peer reviewers based on their employer affiliation. I understand the concern this raised for many of you and appreciate the feedback that IEEE leaders and I received.

&gt; As an international organization operating in 160 countries, IEEE supports the free and open exchange of scholarly and academic work and the global advancement of science and technology. IEEE is committed to enabling an environment of international cooperation and the sharing of our members wealth of knowledge to drive innovation.

&gt; We appreciate the patience of our members and volunteers as we worked through a legally complex situation. If you have any comments, questions, or concerns, please contact me at president@ieee.org.

&gt; For more information, please visit www.ieee.org.

&gt; Thank you for supporting IEEE in our mission to advance technology for humanity.

&gt; Jos M. F. Moura 
&gt; 2019 IEEE President and CEO",11,53
110,2019-6-3,2019,6,3,12,bw65pr,KLINIK ABORSI AMAN TEMPAT KURET LEGAL KLINIK ABORSI JAKARTA,https://www.reddit.com/r/MachineLearning/comments/bw65pr/klinik_aborsi_aman_tempat_kuret_legal_klinik/,kathlenepistill,1559533069,,0,1
111,2019-6-3,2019,6,3,13,bw6dt5,Grouping customers using Kmeans (clustering) on Azure SQL Database Machine Learning based on their purchase and return history,https://www.reddit.com/r/MachineLearning/comments/bw6dt5/grouping_customers_using_kmeans_clustering_on/,albertomorillo,1559534573,,0,1
112,2019-6-3,2019,6,3,13,bw6jrj,[D] How to draw the model's architecture?,https://www.reddit.com/r/MachineLearning/comments/bw6jrj/d_how_to_draw_the_models_architecture/,hadaev,1559535738,"[I like this and also have resnet, but 1d](https://neurohive.io/wp-content/uploads/2019/01/resnet-architecture-3.png)
Are here pre-made apps or something?",11,9
113,2019-6-3,2019,6,3,15,bw7d62,How well would GPT-2 do at generating images or audio instead of text?,https://www.reddit.com/r/MachineLearning/comments/bw7d62/how_well_would_gpt2_do_at_generating_images_or/,monsieurpooh,1559542051,,0,1
114,2019-6-3,2019,6,3,15,bw7f5r,"ANIME, Manga, and Video Game Datasets",https://www.reddit.com/r/MachineLearning/comments/bw7f5r/anime_manga_and_video_game_datasets/,LimarcAmbalina,1559542491,[removed],0,1
115,2019-6-3,2019,6,3,15,bw7k1r,[P] GAN-based Recommender System Papers,https://www.reddit.com/r/MachineLearning/comments/bw7k1r/p_ganbased_recommender_system_papers/,data-chef,1559543599,[removed],0,1
116,2019-6-3,2019,6,3,16,bw7xkz,[R]Text Classification  RNNs or CNNs?,https://www.reddit.com/r/MachineLearning/comments/bw7xkz/rtext_classification_rnns_or_cnns/,Oleguardian,1559546895,,0,1
117,2019-6-3,2019,6,3,16,bw7y6k,"If I have a pre-trained model in the pickle format, is it possible to convert it to hdf5 format compatible with tensorflow?",https://www.reddit.com/r/MachineLearning/comments/bw7y6k/if_i_have_a_pretrained_model_in_the_pickle_format/,bananaskywalker,1559547052,[removed],0,1
118,2019-6-3,2019,6,3,16,bw7zkw,Guide to Exploratory Data Analysis with Python,https://www.reddit.com/r/MachineLearning/comments/bw7zkw/guide_to_exploratory_data_analysis_with_python/,RubiksCodeNMZ,1559547405,,0,1
119,2019-6-3,2019,6,3,16,bw7zyw,[R]Parameters in Machine Learning algorithms,https://www.reddit.com/r/MachineLearning/comments/bw7zyw/rparameters_in_machine_learning_algorithms/,Oleguardian,1559547500,,0,1
120,2019-6-3,2019,6,3,16,bw82mp,[R]Predicting Next Purchase Day,https://www.reddit.com/r/MachineLearning/comments/bw82mp/rpredicting_next_purchase_day/,Oleguardian,1559548216,,0,1
121,2019-6-3,2019,6,3,19,bw8xzc,What are the various Types of Data Sets used in Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/bw8xzc/what_are_the_various_types_of_data_sets_used_in/,trainingdata,1559556224,,0,1
122,2019-6-3,2019,6,3,19,bw90o9,Machine Learning 101-The Basic Underpinnings,https://www.reddit.com/r/MachineLearning/comments/bw90o9/machine_learning_101the_basic_underpinnings/,mishra39,1559556871,,0,1
123,2019-6-3,2019,6,3,19,bw93im,Integrating RNN/LSTM into GAN?,https://www.reddit.com/r/MachineLearning/comments/bw93im/integrating_rnnlstm_into_gan/,zoombapup,1559557544,"Hi all,

I'm in need of some advice on papers etc relating to a specific requirement if anyone can think of any.

I'm trying to build a model that generates new shot content for a virtual cinematic based off of an ensemble analysis of previous content (specific small sections of the TV show Breaking Bad in this case).

So I have as data, a bunch of per-frame data in a variety of forms, things like bounding boxes of characters, character pose, face and head pose, face recognition values etc. You get the picture, there are a whole bunch of different data items that may or may not appear per-frame. I also have a set of shot boundaries so that I can determine when camera changes happen etc.

Now my problem is, I want to take this structured-but-not-always-relevant data and transform it into a generative prediction model (like a GAN) that predicts similar data for a ""similar"" sequence of shots in a virtual environment.

Now I have an urge to use a RNN/LSTM approach, because I can get sequence predictions from it.. But that doesn't really work, because even though my data is sequential, it doesn't reduce to a single label (i.e. some of the data is relevant and some isn't for a given frame).

What I need, is a model that takes temporal structure for varied data and outputs probabilities and/or sequences of similar structured data for each ""aspect"" of the data I guess. Which means I could likely train an ensemble of individual RNN/LSTM models and then use those as Generator for a GAN?. But I want to train them all together so that the data interactions that are happening in the sequence of frames is maintained.

So I guess I'm asking, does there exist a type of model that 1) preserves temporal information 2) allows for optional structured data 3) outputs similar structured data probabilities and 4) allows for generative adversarial improvements?

I guess that is a lot to ask :)",0,1
124,2019-6-3,2019,6,3,19,bw93w7,Doubts/Questions regarding a Ph.D. in ML and CV,https://www.reddit.com/r/MachineLearning/comments/bw93w7/doubtsquestions_regarding_a_phd_in_ml_and_cv/,sainatarajan7,1559557632,[removed],0,1
125,2019-6-3,2019,6,3,19,bw9c3y,Has anybody used the Tencent ML dataset to make a multi-label classifier?,https://www.reddit.com/r/MachineLearning/comments/bw9c3y/has_anybody_used_the_tencent_ml_dataset_to_make_a/,sahebqaran,1559559472,[removed],0,1
126,2019-6-3,2019,6,3,20,bw9h05,Open Standards for Machine Learning Deployment,https://www.reddit.com/r/MachineLearning/comments/bw9h05/open_standards_for_machine_learning_deployment/,newthinkingevents,1559560481,[removed],1,1
127,2019-6-3,2019,6,3,20,bw9p47,[D] Terminator with Stallone face,https://www.reddit.com/r/MachineLearning/comments/bw9p47/d_terminator_with_stallone_face/,cmillionaire9,1559562130," DeepFake 

https://youtu.be/CWaVva2ZGyc",26,41
128,2019-6-3,2019,6,3,20,bw9t2a,HELP: deep learning API,https://www.reddit.com/r/MachineLearning/comments/bw9t2a/help_deep_learning_api/,defudger,1559562942,[removed],0,1
129,2019-6-3,2019,6,3,21,bw9xhm,How to make a book cover scanning algorithm similar to Goodreads?,https://www.reddit.com/r/MachineLearning/comments/bw9xhm/how_to_make_a_book_cover_scanning_algorithm/,oneAJ,1559563774,"Hi,

&amp;#x200B;

I'd like to know if its trivial to implement an image scanner in an app that can scan book covers and then correctly identify the book like this feature in Goodreads: [https://www.goodreads.com/blog/show/913-goodreads-hack-scan-a-book-cover](https://www.goodreads.com/blog/show/913-goodreads-hack-scan-a-book-cover)

&amp;#x200B;

I have some experience in ML and know a little about CNNs which I assume would be the right path for this task but I'd just like to know whether this is trivial to make or extremely difficult - I would be needed an accuracy of 99%",0,1
130,2019-6-3,2019,6,3,21,bwa3ki,[P] Keras implementation of Depthwise 3D Convolutions,https://www.reddit.com/r/MachineLearning/comments/bwa3ki/p_keras_implementation_of_depthwise_3d/,Alex_Stergiou,1559564914,"Due to lack of an official TensorFlow implementation for separable 3D Convolutions, I have created a Keras custom layer implemented with TensorFlow. 

&amp;#x200B;

(The repository is a work in progress)

&amp;#x200B;

Link to github repo: [https://github.com/alexandrosstergiou/keras-DepthwiseConv3D](https://github.com/alexandrosstergiou/keras-DepthwiseConv3D)",7,47
131,2019-6-3,2019,6,3,21,bwaclw,Data Science Jobs - how to guide to get hired quickly,https://www.reddit.com/r/MachineLearning/comments/bwaclw/data_science_jobs_how_to_guide_to_get_hired/,Anurajaram,1559566572,,0,1
132,2019-6-3,2019,6,3,22,bwanbz,Guys is Germany good place for Ms in Machine learning?,https://www.reddit.com/r/MachineLearning/comments/bwanbz/guys_is_germany_good_place_for_ms_in_machine/,008karan,1559568435,[removed],0,1
133,2019-6-3,2019,6,3,22,bwaxkh,Why RPA is the Wave of The Future for Customer Service Automation,https://www.reddit.com/r/MachineLearning/comments/bwaxkh/why_rpa_is_the_wave_of_the_future_for_customer/,Victor_Stakh,1559570178,,0,1
134,2019-6-3,2019,6,3,23,bwb0ix,[P] An introduction to SVD and its widely used applications,https://www.reddit.com/r/MachineLearning/comments/bwb0ix/p_an_introduction_to_svd_and_its_widely_used/,Nathan-toubiana,1559570650,"Hey all! just sharing this article on SVD. Would love to get your feedback!

[https://towardsdatascience.com/an-introduction-to-svd-and-its-widely-used-applications-f5b8f19cb6cb](https://towardsdatascience.com/an-introduction-to-svd-and-its-widely-used-applications-f5b8f19cb6cb)",3,0
135,2019-6-3,2019,6,3,23,bwb2qa,Understanding NLP Problem based on a scenario,https://www.reddit.com/r/MachineLearning/comments/bwb2qa/understanding_nlp_problem_based_on_a_scenario/,bvy007,1559571002,[removed],0,1
136,2019-6-3,2019,6,3,23,bwb3gd,IEEE Reverses Huawei Paper Review Restrictions,https://www.reddit.com/r/MachineLearning/comments/bwb3gd/ieee_reverses_huawei_paper_review_restrictions/,Yuqing7,1559571113,,0,1
137,2019-6-3,2019,6,3,23,bwb7f4,"[N] Meet the computer scientist using artificial intelligence to help 140,000 paying customers plan the perfect Disney vacation",https://www.reddit.com/r/MachineLearning/comments/bwb7f4/n_meet_the_computer_scientist_using_artificial/,amw5gster,1559571737,"[https://amp.businessinsider.com/touringplans-disney-world-len-testa-interview-2019-5](https://amp.businessinsider.com/touringplans-disney-world-len-testa-interview-2019-5) 

From Business Insider, so not a technical piece, by any means.  But an interesting (to me) application of data science &amp; ML.",2,0
138,2019-6-3,2019,6,3,23,bwb87n,[D] Make BERT model smaller,https://www.reddit.com/r/MachineLearning/comments/bwb87n/d_make_bert_model_smaller/,sudo_su_,1559571850,"We used BERT in one of the tasks we work on in our company. It worked incredibly well, and we want to try it in many other tasks. 

The issue is that BERT is a huge model and requires a GPU both in training and inference. We want to find a way to utilize BERT without using GPU everywhere. 

Is there a way to make BERT smaller or build some approximation model?

What we thought for now is:

1. For some task, train a model using BERT on a small amount of data (what we currently have). If the results are good, ""tag"" a lot of data using this model and the train another, much smaller, model on the large artificially tagged data.

2. Use part of the BERT layers, for example, take only 2-3 first attention layers out of 12 and fine-tune them.

&amp;#x200B;

Thanks.",15,31
139,2019-6-3,2019,6,3,23,bwbbvy,Is is possible to train on non-optimal data?,https://www.reddit.com/r/MachineLearning/comments/bwbbvy/is_is_possible_to_train_on_nonoptimal_data/,RslWlsn3,1559572396,[removed],0,1
140,2019-6-3,2019,6,3,23,bwbdyk,"[D] PyTorch 101 Part 1: Understanding Graphs, Automatic Differentiation and Autograd",https://www.reddit.com/r/MachineLearning/comments/bwbdyk/d_pytorch_101_part_1_understanding_graphs/,AutomaticTomato9,1559572709,,0,1
141,2019-6-3,2019,6,3,23,bwbj93,Getting hired in DataScience/ML roles - book link,https://www.reddit.com/r/MachineLearning/comments/bwbj93/getting_hired_in_datascienceml_roles_book_link/,Anurajaram,1559573514,[removed],0,1
142,2019-6-4,2019,6,4,0,bwbspv,Language Detector for social media Language Detect - [Google's Compact Language Detector 3] in Kotlin,https://www.reddit.com/r/MachineLearning/comments/bwbspv/language_detector_for_social_media_language/,hash_t,1559574909,[removed],0,1
143,2019-6-4,2019,6,4,0,bwbtw5,[R] Distributional concavity regularization for GANs (ICLR2019),https://www.reddit.com/r/MachineLearning/comments/bwbtw5/r_distributional_concavity_regularization_for/,Darkwhiter,1559575082,"Openreview [thread](https://openreview.net/forum?id=SklEEnC5tQ) and [paper](https://openreview.net/pdf?id=SklEEnC5tQ) and [poster](https://twitter.com/guguchi_yama/status/1125776072618979330).

Spectral normalization was published by what seems to be roughly the same research group and has been one of few GAN modifications that improves GAN performance, reliably and significantly. This paper proposes a regularization term in the cost function that encourages the generator to maximize its entropy, in order to prevent mode collapse, which looks very promising.

However, I really can't wrap my head around what they are actually doing, i.e. how to implement the regularization, the theory section is rather dense and I can't find any implementations. Any pointers towards code or rough pseudocode would be very much appreciated.",3,10
144,2019-6-4,2019,6,4,0,bwc0bh,Training Machine Learning Models Using Noisy Medical Data,https://www.reddit.com/r/MachineLearning/comments/bwc0bh/training_machine_learning_models_using_noisy/,taion,1559576026,,0,1
145,2019-6-4,2019,6,4,0,bwc2k7,"AMA: We are IBM researchers, scientists and developers working on data science, machine learning and AI. Start asking your questions now and we'll answer them on Tuesday the 4th of June at 1-3 PM ET / 5-7 PM UTC",https://www.reddit.com/r/MachineLearning/comments/bwc2k7/ama_we_are_ibm_researchers_scientists_and/,alexa_y,1559576342,,0,1
146,2019-6-4,2019,6,4,0,bwc6qm,Examples of AI in Healthcare 2019: Can Intelligent Machines Make Humans Healthier?,https://www.reddit.com/r/MachineLearning/comments/bwc6qm/examples_of_ai_in_healthcare_2019_can_intelligent/,MilaKyryliuk,1559576934,,0,1
147,2019-6-4,2019,6,4,0,bwc7f1,[R] Training Machine Learning Models Using Noisy Medical Data,https://www.reddit.com/r/MachineLearning/comments/bwc7f1/r_training_machine_learning_models_using_noisy/,taion,1559577034,,0,1
148,2019-6-4,2019,6,4,1,bwcg9o,High Frequency Component Helps Explain the Generalization of Convolutional Neural Networks (Adversarial examples due to HF components?),https://www.reddit.com/r/MachineLearning/comments/bwcg9o/high_frequency_component_helps_explain_the/,vackosar,1559578279,,8,14
149,2019-6-4,2019,6,4,1,bwcj9a,[D] Rajat Monga: TensorFlow | Artificial Intelligence Podcast,https://www.reddit.com/r/MachineLearning/comments/bwcj9a/d_rajat_monga_tensorflow_artificial_intelligence/,UltraMarathonMan,1559578705,"Rajat Monga is an Engineering Director at Google, leading the TensorFlow team.

**Video:**  [https://www.youtube.com/watch?v=NERNE4UThHU](https://www.youtube.com/watch?v=NERNE4UThHU)

https://i.redd.it/rh3wbmgk26231.png

**Outline:**

0:00 - Introduction

1:17 - Google Brain early days

4:47 - TensorFlow early days - open sourcing, etc

12:53 - TensorFlow growth

22:00 - Keras

26:24 - TensorFlow project management

37:10 - Competition and PyTorch

39:48 - TensorFlow 2.0

51:20 - Building a good software engineering team

1:03:48 - Search ads and paying for content

1:08:43 - Using TensorFlow on a budget

1:10:16 - How to get started with TensorFlow",5,123
150,2019-6-4,2019,6,4,1,bwcqix,Newbie looking for some guidance,https://www.reddit.com/r/MachineLearning/comments/bwcqix/newbie_looking_for_some_guidance/,NentialG,1559579714,"Hola Amigos , 

&amp;#x200B;

I am almost completely new to this scene, and am currently looking to apply to university as a back-up plan regarding my career progression. I am limited to select from universities within Europe, if you have any recommendations to send my way, please do so. 

&amp;#x200B;

Ideally they would be universities that still take applicants for 2019 September.

&amp;#x200B;

Regarding what I am looking for in the syllabus, some interests of mine are Reinforcement Learning as well as Genetic Algorithms. Very sparse reading on the these topics so far and will be delving deeper into those areas. 

&amp;#x200B;

If you can suggest some universities to look at, I would appreciate that. 

Having said this, I also recognize that self-learning a lot of this is possible and is what I would like to pursue, the university applications are a just-incase scenario, where i am not making headway. As well as to satisfy my parents concerns going forward... 

&amp;#x200B;

My background is -  

Economics Undergraduate ( Econometrics focus for thesis) 

Worked as a fullstack web developer with Javascript for 5 months 

Programming for 1 year in Javascript.

 

&amp;#x200B;

Some reading material I am going to be checking out are 

&amp;#x200B;

The Quest for Artificial Intelligence - Nils J.Nilson

&amp;#x200B;

Deep Learning - Ian Goodfellow, Joshua Bengio &amp; Aaron Courville 

&amp;#x200B;

Reinforcement Learning - Richard Sutton &amp; Andrew Barto 

&amp;#x200B;

Also for clarification I am more interested in applications of Machine learning with reference to Artificial Intelligence and will hoping to work/create in that space rather than in Data Science. People seem to mix these up quite often. 

&amp;#x200B;

Last but not least, if you have read up to this point. Thank you for taking the time. 

&amp;#x200B;

I look forward to hear from you.",0,1
151,2019-6-4,2019,6,4,1,bwcqo4,RNN - recurrent layer weights,https://www.reddit.com/r/MachineLearning/comments/bwcqo4/rnn_recurrent_layer_weights/,vladesomo,1559579728,[removed],0,1
152,2019-6-4,2019,6,4,2,bwd8qo,Learn skills for Real-world ML and DL,https://www.reddit.com/r/MachineLearning/comments/bwd8qo/learn_skills_for_realworld_ml_and_dl/,Best_Approximation,1559582248,[removed],0,1
153,2019-6-4,2019,6,4,2,bwdjrm,Unsupervised Anomaly Detection on big dataset,https://www.reddit.com/r/MachineLearning/comments/bwdjrm/unsupervised_anomaly_detection_on_big_dataset/,NightKing_GOT,1559583844,[removed],0,1
154,2019-6-4,2019,6,4,2,bwdq29,"DeepMinds plans to make AI systems robust &amp; reliable, why its a core issue in AI design, and how to succeed at AI research (Podcast)",https://www.reddit.com/r/MachineLearning/comments/bwdq29/deepminds_plans_to_make_ai_systems_robust/,robwiblin,1559584707,[removed],0,1
155,2019-6-4,2019,6,4,3,bwdzn5,Me and my friends started a community where python and machine learning beginners help each other out and learn together like a group study ? Wanna join our community ? Dm and Ill link you to our discord group chat!,https://www.reddit.com/r/MachineLearning/comments/bwdzn5/me_and_my_friends_started_a_community_where/,FaizRahim,1559585993,[removed],0,1
156,2019-6-4,2019,6,4,3,bwe2yj,Searching for dark matter in CERN's Large Hadron Collider dataset,https://www.reddit.com/r/MachineLearning/comments/bwe2yj/searching_for_dark_matter_in_cerns_large_hadron/,0_marauders_0,1559586446,,0,1
157,2019-6-4,2019,6,4,3,bwe630,Great Greater iGrater,https://www.reddit.com/r/MachineLearning/comments/bwe630/great_greater_igrater/,TimScherer94,1559586877,,0,1
158,2019-6-4,2019,6,4,4,bweivs,"NumPyro, Probabilistic Programming backed by Numpy and JAX (CPU/GPU)",https://www.reddit.com/r/MachineLearning/comments/bweivs/numpyro_probabilistic_programming_backed_by_numpy/,maiybe,1559588636,,1,1
159,2019-6-4,2019,6,4,4,bwesuy,What type of Style Transfer is ostagram.ru using?,https://www.reddit.com/r/MachineLearning/comments/bwesuy/what_type_of_style_transfer_is_ostagramru_using/,alvisanovari,1559590008,[removed],0,1
160,2019-6-4,2019,6,4,4,bwesw2,[p] Tutorial on the Convolutional Tsetlin Machine,https://www.reddit.com/r/MachineLearning/comments/bwesw2/p_tutorial_on_the_convolutional_tsetlin_machine/,olegranmo,1559590013,"I have made a tutorial that hopefully will illuminate the intuition and rationale behind the Convolutional Tsetlin Machine, with the help of figures and examples. Let me know if any part of the tutorial needs further clarification.

[https://github.com/cair/convolutional-tsetlin-machine](https://github.com/cair/convolutional-tsetlin-machine)

[Clause composed by a team of Tsetlin Automata](https://i.redd.it/7xz147g8z6231.png)",0,12
161,2019-6-4,2019,6,4,4,bweukz,"No google wtf, not what I meant",https://www.reddit.com/r/MachineLearning/comments/bweukz/no_google_wtf_not_what_i_meant/,nucses,1559590243,,0,1
162,2019-6-4,2019,6,4,4,bwev4a,UC Berkeleys RL-Powered SOLAR Accelerates Robotic Learning,https://www.reddit.com/r/MachineLearning/comments/bwev4a/uc_berkeleys_rlpowered_solar_accelerates_robotic/,Yuqing7,1559590317,,0,1
163,2019-6-4,2019,6,4,5,bwf8yt,[Research] Angry AI - Learning To Walk // Live Stream,https://www.reddit.com/r/MachineLearning/comments/bwf8yt/research_angry_ai_learning_to_walk_live_stream/,Drone_Mesh,1559592119,,0,1
164,2019-6-4,2019,6,4,5,bwf9uy,This doctor is recruiting an army of medical experts to drown out fake health news on Instagram and Twitter (crosspost),https://www.reddit.com/r/MachineLearning/comments/bwf9uy/this_doctor_is_recruiting_an_army_of_medical/,motionSymmetry,1559592234,,1,1
165,2019-6-4,2019,6,4,5,bwfh2n,Selecting the Best Architecture for Artificial Neural Networks,https://www.reddit.com/r/MachineLearning/comments/bwfh2n/selecting_the_best_architecture_for_artificial/,AhmedGadFCIT,1559593210,,0,1
166,2019-6-4,2019,6,4,5,bwflgr,[P] Selecting the Best Architecture for ANNs,https://www.reddit.com/r/MachineLearning/comments/bwflgr/p_selecting_the_best_architecture_for_anns/,AhmedGadFCIT,1559593812,[removed],0,1
167,2019-6-4,2019,6,4,5,bwfurb,[D] What do people think is the best way to predict wickets in cricket? Dataset provided if needed.,https://www.reddit.com/r/MachineLearning/comments/bwfurb/d_what_do_people_think_is_the_best_way_to_predict/,DaveatAuquan,1559595100,"Hey guys, my company is running an open data science competition for the Cricket World Cup and I'm trying to have a go myself. I've done some research on some possible approaches but wondered if anyone else could think of another approach.

  
 The approach I've tried (paper linked below):

  
\- I created new attributes for things like form and consistency (as described in the paper)  
\- Then I've had a go at using Nave  bayes,  random  forest, multiclass  SVM  and  decision  tree  classifiers to predict if a ball is a wicket. (but made a hash of it so far)  
[https://www.researchgate.net/publication/323611656\_Predicting\_Players'\_Performance\_in\_One\_Day\_International\_Cricket\_Matches\_Using\_Machine\_Learning](https://www.researchgate.net/publication/323611656_Predicting_Players'_Performance_in_One_Day_International_Cricket_Matches_Using_Machine_Learning))  
The dataset is about 1m points, 29 factors and about 4000 matches (recorded ball by ball), if people want to see I can post a link to the competition.",5,3
168,2019-6-4,2019,6,4,6,bwgddc,[P] Using Neural Networks (CycleGAN) to Generate Pokemon as Different Elemental Types,https://www.reddit.com/r/MachineLearning/comments/bwgddc/p_using_neural_networks_cyclegan_to_generate/,carouselderby,1559597721,,0,1
169,2019-6-4,2019,6,4,6,bwghn1,[Project] Using Neural Networks (CycleGAN) to Generate Pokemon as Different Elemental Types,https://www.reddit.com/r/MachineLearning/comments/bwghn1/project_using_neural_networks_cyclegan_to/,carouselderby,1559598323,"Blog post here: [pokemon2pokemon: Using Neural Networks to Generate Pokemon as Different Elemental Types](https://www.rileynwong.com/blog/2019/5/22/pokemon2pokemon-using-cyclegan-to-generate-pokemon-as-different-elemental-types)

I trained CycleGAN to generate Pokemon as different elemental types. e.g. [Gyarados as fire type](https://images.squarespace-cdn.com/content/v1/52c62b4be4b0ca96fb3cfcba/1559166105167-03Q8M6L340LKN4YILNKO/ke17ZwdGBToddI8pDm48kHhlTY0to_qtyxq77jLiHTtZw-zPPgdn4jUwVcJE1ZvWhcwhEtWJXoshNdA9f1qD7T-j82ScS_xjTqFYGqFrT72qZ_E0ELtHpOZiWcSG1QwIMeEVreGuQ8F95X5MZTW1Jw/gyarados_fake_B.png)",24,288
170,2019-6-4,2019,6,4,6,bwgjn3,Nuts and bolts of kmeans,https://www.reddit.com/r/MachineLearning/comments/bwgjn3/nuts_and_bolts_of_kmeans/,satyajit_,1559598615,,0,1
171,2019-6-4,2019,6,4,7,bwgvah,Everything you know about word2vec is wrong.,https://www.reddit.com/r/MachineLearning/comments/bwgvah/everything_you_know_about_word2vec_is_wrong/,Bollu,1559600274,,0,2
172,2019-6-4,2019,6,4,7,bwgye0,AI Launchpad,https://www.reddit.com/r/MachineLearning/comments/bwgye0/ai_launchpad/,AILaunchpad,1559600720,[removed],0,1
173,2019-6-4,2019,6,4,7,bwh05e,"GluonTS, probabilistic time series modeling in Python",https://www.reddit.com/r/MachineLearning/comments/bwh05e/gluonts_probabilistic_time_series_modeling_in/,lostella,1559600997,,0,1
174,2019-6-4,2019,6,4,8,bwhoax,[D] What are some of the best publicly available voice cloning libraries/repos?,https://www.reddit.com/r/MachineLearning/comments/bwhoax/d_what_are_some_of_the_best_publicly_available/,chirau,1559604802,I want to take a stab at voice cloning but I want to start from something that already exists and then maybe learn from there or if it suffices just use whatever is available. Any link to good resources?,0,3
175,2019-6-4,2019,6,4,10,bwil1o,[R] Scalable and transferable learning of algorithms via graph embedding for multi-robot reward collection,https://www.reddit.com/r/MachineLearning/comments/bwil1o/r_scalable_and_transferable_learning_of/,hywkkang,1559610192,"We recently released a preprint solving multi-robot/machine scheduling problem with stochastic task completion time and time-dependent rewards. Here's the Arxiv page:  [https://arxiv.org/abs/1905.12204](https://arxiv.org/abs/1905.12204)  

Github repo will soon be uploaded.",0,2
176,2019-6-4,2019,6,4,10,bwio51,[R] Deep learning + bio-mechanics: Non-invasive diagnosis of breast cancer based on its mechanical properties,https://www.reddit.com/r/MachineLearning/comments/bwio51/r_deep_learning_biomechanics_noninvasive/,DhruvVPatel,1559610702,"Current cancer diagnosis and treatment requires patient to undergo an invasive biopsy process. This treatment is not only painful and can cause further [complications](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5127875/), but is also [emotionally and financially draining](https://www.ncbi.nlm.nih.gov/pubmed/2003712) process for the patient and his/her family.

&amp;#x200B;

https://i.redd.it/wq19fw93p8231.png

In recent years, researchers have developed a non-invasive process, called [elastography](https://en.wikipedia.org/wiki/Elastography) or [elasticity imaging](https://en.wikipedia.org/wiki/Elastography), which  maps the mechanical properties (like Young's modulus) of tissue from ultrasound and/or MRI data and use this map of (mechanical) properties to diagnose the tumor more accurately. Thus, circumventing the need of undergoing an invasive biopsy.

&amp;#x200B;

While valuable and promising, elasticity imaging requires solution of an inverse elasticity problem in modulus reconstruction step. This step is complex (doesn't admit unique solution), computationally expensive, and is time consuming (this could be critical in medical diagnosis). **To circumvent these challenges, in this paper, we propose a novel deep-learning based workflow to non-invasively diagnose breast lesions from ultrasound data, while preserving the underlying mechanics.**

https://i.redd.it/0pxxkm25p8231.png

&amp;#x200B;

In the process, we also demonstrate how physics-based modeling can felicitate transfer learning/domain randomization in data-scarce applications like medical imaging. By analyzing learned filters in physical and Fourier space, and interpreting them as a discrete differential operator, we also find interesting connection between this learning-based approach to elasticity imaging and traditional strain-imaging based approaches.

&amp;#x200B;

Apart from medical diagnosis, the proposed framework can also find application in many areas of science and engineering like design optimization, non-destructive testing etc., where the goal is to make decision based on quantities of interest inferred from measurements.

&amp;#x200B;

**Paper :**   [https://doi.org/10.1016/j.cma.2019.04.045](https://doi.org/10.1016/j.cma.2019.04.045)  
 

&amp;#x200B;

Comments and feedback are welcome!",0,3
177,2019-6-4,2019,6,4,10,bwixhd,"[D] A detailed explanation of how PyTorchs Automatic Differentiation Engine, Autograd works",https://www.reddit.com/r/MachineLearning/comments/bwixhd/d_a_detailed_explanation_of_how_pytorchs/,AutomaticTomato9,1559612248,[removed],0,1
178,2019-6-4,2019,6,4,11,bwj6aj,Data Science Bootcamp thoughts?,https://www.reddit.com/r/MachineLearning/comments/bwj6aj/data_science_bootcamp_thoughts/,ComputerNerd16,1559613720,[removed],0,2
179,2019-6-4,2019,6,4,11,bwj9v6,Trending NLP Projects,https://www.reddit.com/r/MachineLearning/comments/bwj9v6/trending_nlp_projects/,PyWarrior,1559614310,[removed],0,1
180,2019-6-4,2019,6,4,11,bwjq0t,[P] A tutorial series on PyTorch: Understanding how Autograd works,https://www.reddit.com/r/MachineLearning/comments/bwjq0t/p_a_tutorial_series_on_pytorch_understanding_how/,AutomaticTomato9,1559617027,[removed],0,1
181,2019-6-4,2019,6,4,12,bwjt4p,[R] Applying Machine Learning and Discrete Choice Modeling to understand the quality of urban landscape,https://www.reddit.com/r/MachineLearning/comments/bwjt4p/r_applying_machine_learning_and_discrete_choice/,tiramirez,1559617568,"#  Wekun ([wekun.ing.puc.cl](http://wekun.ing.puc.cl)) is a game that seeks to understand how people perceive public space and, thus, understand what determines the quality of these spaces. 

We ask people to chose between images to measure their preferences. Then, the information collected is processed with Machine Learning algorithms and discrete choice models, in order to understand the role played by different elements of the built environment and nature in the preferences of people. The methodology is not new (links below), but we have incorporated a section to register sociodemographic information aiming to find heterogeneity among observers. We use Discrete Choice Modelling as a benchmark to Machine Learning Algorithms, typically referred to as black boxes, to overcome the explainability problems involved with them.

https://i.redd.it/xamzwiv0a9231.jpg

**Please comment on the following subjects to help us!**

1. Any recommendation of semantic segmentation algorithms? or Object detection?
2. For the success of this research, we need your help evaluating photos of public spaces and sharing this message to have the opinion of more people!

**Some references:**

Rossetti, T., Lobel, H., Rocco, V., &amp; Hurtubia, R. (2019). Explaining subjective perceptions of public spaces as a function of the built environment: A massive data approach. Landscape and urban planning, 181, 169-178. ([link](https://www.sciencedirect.com/science/article/pii/S0169204618310260))

 Salesses, P., Schechtner, K., &amp; Hidalgo, C. A. (2013). The collaborative image of the city: mapping the inequality of urban perception. *PloS one*, *8*(7), e68400. ([link](http://pulse.media.mit.edu/papers/))

Dubey, A., Naik, N., Parikh, D., Raskar, R., &amp; Hidalgo, C. A. (2016, October). Deep learning the city: Quantifying urban perception at a global scale. In *European conference on computer vision* (pp. 196-212). Springer, Cham.  ([link](http://pulse.media.mit.edu/papers/))",0,7
182,2019-6-4,2019,6,4,13,bwkc5z,Is Optimization a Sufficient Language for Understanding Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/bwkc5z/is_optimization_a_sufficient_language_for/,hosjiu,1559620984,,0,1
183,2019-6-4,2019,6,4,13,bwkfmw,Interviewed an IT company. Haven't got interview reimbursement over 3 months. Recruiter keep on saying no update. What can I do?,https://www.reddit.com/r/MachineLearning/comments/bwkfmw/interviewed_an_it_company_havent_got_interview/,mic0815,1559621621,[removed],0,1
184,2019-6-4,2019,6,4,13,bwkkfl,Resolving Conflicting statements in the Focal Loss Paper [D],https://www.reddit.com/r/MachineLearning/comments/bwkkfl/resolving_conflicting_statements_in_the_focal/,depressed_hooloovoo,1559622542,"I recently read [this](https://arxiv.org/abs/1708.02002) paper on the focal loss and there seem to be some contradictions in the paper.  The first sentence of the conclusion is ""In this work, we identify class imbalance as the primary obstacle preventing one-stage object detectors from surpassing top-performing, two-stage methods."" 
 
However, when they used a simple weighting mechanism to balance the background statements, the results were not significantly improved from using no weighting (alpha = 0.5), and when they employ their focal loss, they actually upweight background samples (alpha = 0.25).  Is it not a misinterpretation of their results to say that their improvement comes from resolving a class imbalance?",2,13
185,2019-6-4,2019,6,4,14,bwl3im,Machine learning model to find if a geo-spatial point is within a radius.,https://www.reddit.com/r/MachineLearning/comments/bwl3im/machine_learning_model_to_find_if_a_geospatial/,rogerganga,1559626444,[removed],0,1
186,2019-6-4,2019,6,4,15,bwlaop,Provably efficient reinforcement learning with rich observations,https://www.reddit.com/r/MachineLearning/comments/bwlaop/provably_efficient_reinforcement_learning_with/,violentdeli8,1559628018,,0,1
187,2019-6-4,2019,6,4,16,bwm209,ML training preferences - local or remote hardware? (Radiology DICOM images),https://www.reddit.com/r/MachineLearning/comments/bwm209/ml_training_preferences_local_or_remote_hardware/,wkoszek,1559634312,[removed],0,1
188,2019-6-4,2019,6,4,16,bwm5gc,[D] iOS 13 Safari enables the possibility to use Google Colab easily on iOS/iPad OS devices. What kind of projects will you plan to do on your iDevice?,https://www.reddit.com/r/MachineLearning/comments/bwm5gc/d_ios_13_safari_enables_the_possibility_to_use/,OPMaster494,1559635175,,1,1
189,2019-6-4,2019,6,4,17,bwm6ed,Google DeepMind AI Beats Human Players in Quake III: What Comes Next?,https://www.reddit.com/r/MachineLearning/comments/bwm6ed/google_deepmind_ai_beats_human_players_in_quake/,crypt0sparta,1559635407,,0,3
190,2019-6-4,2019,6,4,17,bwmbpd,[P] Porting a Face Detector Written in C to WebAssembly,https://www.reddit.com/r/MachineLearning/comments/bwmbpd/p_porting_a_face_detector_written_in_c_to/,histoire_guy,1559636833,"The article share the method used by PixLab to port the real-time face detection runtime written in pure C of the SOD Computer Vision library to WebAssembly to achieve real-time face detection in the browser.

Link: https://sod.pixlab.io/articles/porting-c-face-detector-webassembly.html",2,82
191,2019-6-4,2019,6,4,17,bwmcrf,[R] Generating Diverse High-Fidelity Images with VQ-VAE-2,https://www.reddit.com/r/MachineLearning/comments/bwmcrf/r_generating_diverse_highfidelity_images_with/,sensetime,1559637114,,24,38
192,2019-6-4,2019,6,4,18,bwmjzs,[R] Human-level performance in 3D multiplayer games with population-based reinforcement learning,https://www.reddit.com/r/MachineLearning/comments/bwmjzs/r_humanlevel_performance_in_3d_multiplayer_games/,jakn,1559639041,,0,1
193,2019-6-4,2019,6,4,18,bwmk3e,How to implement deep sort on YOLO,https://www.reddit.com/r/MachineLearning/comments/bwmk3e/how_to_implement_deep_sort_on_yolo/,CSGOvelocity,1559639061,[removed],0,1
194,2019-6-4,2019,6,4,18,bwmk52,"""Arxiv Sanity Preserver"" vs ""GroundAI""",https://www.reddit.com/r/MachineLearning/comments/bwmk52/arxiv_sanity_preserver_vs_groundai/,gt_tugsuu,1559639072,[removed],0,1
195,2019-6-4,2019,6,4,18,bwmkbl,[D] Any updates from MICCAI-19?,https://www.reddit.com/r/MachineLearning/comments/bwmkbl/d_any_updates_from_miccai19/,redlow0992,1559639120,"Does anyone have news about what is happening with MICCAI-19 reviews? 

The organizers sent an email about the delay of rebuttal period (and reviews) 12 days ago. Currently even the rebuttal timeline in the website ([https://www.miccai2019.org/information/information-conference\_timeline/](https://www.miccai2019.org/information/information-conference_timeline/)) is updated to TBD (It was previously 27-30 May, then updated to 3-7 June, and now TBD).",16,12
196,2019-6-4,2019,6,4,18,bwmovm,Something simple to determine if image contains predefined text,https://www.reddit.com/r/MachineLearning/comments/bwmovm/something_simple_to_determine_if_image_contains/,imprezobus,1559640271,[removed],0,1
197,2019-6-4,2019,6,4,19,bwn9rn,"Friend just graduated with a masters in info systems, told him I'd post this here. Never forget to train the most important model.",https://www.reddit.com/r/MachineLearning/comments/bwn9rn/friend_just_graduated_with_a_masters_in_info/,TyrannosaurusFlex92,1559645127,,0,1
198,2019-6-4,2019,6,4,19,bwncpj,[P] Catalogue of Python Based Machine Learning Applications in Various Industries V1,https://www.reddit.com/r/MachineLearning/comments/bwncpj/p_catalogue_of_python_based_machine_learning/,OppositeMidnight,1559645786,"If anyone is a subject expert or simply want to help with the project please send me a pull request or get in contact with me at d.snow\\atsymbolcomeshere\\jbs.cam.ac.uk. Any help on this project would be greatly appreciated.

Its still very fresh so any ideas/feedback are welcome and certainly appreciated. See below for the industries/areas currently covered.

Link: https://github.com/firmai/industry-machine-learning

&amp;#x200B;

|                          |                       |                           |
| -------------------------------- | -------------------------------- | --------------------------------- |
| [Accommodation &amp; Food](https://github.com/firmai/industry-machine-learning#accommodation)             | [Agriculture](https://github.com/firmai/industry-machine-learning#agriculture)           | [Banking &amp; Insurance](https://github.com/firmai/industry-machine-learning#bankfin)               |
| [Biotechnological &amp; Life Sciences](https://github.com/firmai/industry-machine-learning#biotech) | [Construction &amp; Engineering](https://github.com/firmai/industry-machine-learning#construction)       | [Education &amp; Research](https://github.com/firmai/industry-machine-learning#education)              |
| [Emergency &amp; Relief](https://github.com/firmai/industry-machine-learning#emergency)               | [Finance](https://github.com/firmai/industry-machine-learning#finance) | [Manufacturing](#manufacturing)             |
| [Government and Public Works](https://github.com/firmai/industry-machine-learning#public)      | [Healthcare](https://github.com/firmai/industry-machine-learning#healthcare)  | [Media &amp; Publishing](https://github.com/firmai/industry-machine-learning#media)                |
| [Justice, Law and Regulations](https://github.com/firmai/industry-machine-learning#legal)      | [Miscellaneous](https://github.com/firmai/industry-machine-learning#miscellaneous)                    | [Accounting](https://github.com/firmai/industry-machine-learning#accounting) |
| [Real Estate, Rental &amp; Leasing](https://github.com/firmai/industry-machine-learning#realestate)    | [Utilities](https://github.com/firmai/industry-machine-learning#utilities)              | [Wholesale &amp; Retail](https://github.com/firmai/industry-machine-learning#wholesale)                  |",2,43
199,2019-6-4,2019,6,4,20,bwngn7,[D] what do you think are some of the most important advancements in NLP in the past couple of years?,https://www.reddit.com/r/MachineLearning/comments/bwngn7/d_what_do_you_think_are_some_of_the_most/,tdls_to,1559646605,"NLP has been moving so fast in the past few years that it makes it so difficult to keep up with all the details of the progress. We will be hosting a lunch and learn session to review some of these methods tomorrow (link), and are looking for questions and topics that we should cover. what do you think are some of the most important advancements in NLP in the past couple of years and why? 

[https://www.eventbrite.ca/e/state-of-nlp-in-2019-aisc-lunch-learn-tickets-62899715743](https://www.eventbrite.ca/e/state-of-nlp-in-2019-aisc-lunch-learn-tickets-62899715743)",39,31
200,2019-6-4,2019,6,4,20,bwniac,Hyper-parameter tuning of neural network: do we always need it? and how to do it efficiently?,https://www.reddit.com/r/MachineLearning/comments/bwniac/hyperparameter_tuning_of_neural_network_do_we/,osm3000,1559646952,[removed],0,1
201,2019-6-4,2019,6,4,20,bwnix7,Combining open source serverless with deep learning,https://www.reddit.com/r/MachineLearning/comments/bwnix7/combining_open_source_serverless_with_deep/,selrok,1559647087,[removed],0,1
202,2019-6-4,2019,6,4,20,bwnjch,"Twitter acquires Fabula AI, a machine learning startup that helps spot fake news",https://www.reddit.com/r/MachineLearning/comments/bwnjch/twitter_acquires_fabula_ai_a_machine_learning/,TheTesseractAcademy,1559647175,,0,1
203,2019-6-4,2019,6,4,20,bwns8f,[R] Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff (ICML'19 long oral),https://www.reddit.com/r/MachineLearning/comments/bwns8f/r_rethinking_lossy_compression_the/,YocB,1559648959,"This paper mathematically proves that optimizing for bit-rate and distortion when compressing perceptual data such as images, video and audio is not the right thing to do. Optimizing for bit-rate and distortion can lead to unnatural low-quality outputs, as perceived by humans. Instead, compression algorithms should directly optimize for perceptual quality, yet this will always come at the cost of increased rate or distortion.

&amp;#x200B;

Link to PDF: [http://proceedings.mlr.press/v97/blau19a/blau19a.pdf](http://proceedings.mlr.press/v97/blau19a/blau19a.pdf)",24,15
204,2019-6-4,2019,6,4,21,bwnz0m,NLP Series (Part 1): Tokenizing Text &amp;raquo; Data Is Utopia,https://www.reddit.com/r/MachineLearning/comments/bwnz0m/nlp_series_part_1_tokenizing_text_raquo_data_is/,hisham_elamir,1559650239,,0,1
205,2019-6-4,2019,6,4,21,bwo1x1,Have you heard of our Machine Learning services?,https://www.reddit.com/r/MachineLearning/comments/bwo1x1/have_you_heard_of_our_machine_learning_services/,HemenAshodia,1559650789,"F(x) Data Labs Pvt Ltd is a research-based Lab that has become the go-to IT-firm for cutting edge technologies. We are the pioneers of H+tree (300% Faster Index Technology) and also one of the few companies in India to provide IaaS which is powered by OpenStack along with novel machine learning algorithms. 

Our core specialty is **Machine Learning** and **Novel Algorithms**. By scoring and clustering algorithms, we can develop the recommendation systems that will offer the most relevant solutions to you.

Key Service offerings:

* Machine Learning
* Data Science
* Deep Learning
* Neural Network/ANN/DNN
* Natural Language Processing
* Computer Vision
* Novel Algorithm
* Enterprise Resource Planning (ERP) System
* Full Stack Web Development
* Cloud Technology
* Custom Software Development

&amp;#x200B;

Hop in the cockpit, we'll be your co-pilot

For more information on the company and its services please contact us on +91 (9727-999-595) ||admin@htree.plus",0,1
206,2019-6-4,2019,6,4,21,bwo6dw,An open source python framework for automated feature engineering,https://www.reddit.com/r/MachineLearning/comments/bwo6dw/an_open_source_python_framework_for_automated/,_quanttrader_,1559651635,,0,1
207,2019-6-4,2019,6,4,21,bwo6gi,[P] nn_builder - a new package that removes the need for boilerplate code when building neural networks,https://www.reddit.com/r/MachineLearning/comments/bwo6gi/p_nn_builder_a_new_package_that_removes_the_need/,__data_science__,1559651647,"nn\_builder is a new package that lets you build neural networks without the boilerplate code using PyTorch or TensorFlow 2.0 that some of you might find useful

Let me know what you think and if you'd like to contribute[https://github.com/p-christ/nn\_builder](https://github.com/p-christ/nn_builder)

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/6vstgpmh3c231.png",5,0
208,2019-6-4,2019,6,4,21,bwo96t,The intuition behind Word2Vec,https://www.reddit.com/r/MachineLearning/comments/bwo96t/the_intuition_behind_word2vec/,pmuens,1559652165,,0,1
209,2019-6-4,2019,6,4,21,bwoa9d,[R] Understanding and Controlling Memory in Recurrent Neural Networks (ICML'19 oral),https://www.reddit.com/r/MachineLearning/comments/bwoa9d/r_understanding_and_controlling_memory_in/,DoronHaviv12,1559652358,"This paper shows that RNNs are able to form long-term memories despite being trained only for short-term with a limited amount of timesteps, but that not all memories are created equal. The authors find that each memory is correlated with a dynamical object in the hidden-state phase space and that the objects properties can quantitatively predict long term effectiveness. By regularizing the dynamical object, the long-term functionality of the RNN is significantly improved, while not adding to the computational complexity of training.  


Link to PDF: [http://proceedings.mlr.press/v97/haviv19a/haviv19a.pdf](http://proceedings.mlr.press/v97/haviv19a/haviv19a.pdf)

Oral: Tue Jun 11th 03:10 PM @ Room 201

Poster: Tue Jun 11th 06:30 PM @ Pacific Ballroom #258",13,120
210,2019-6-4,2019,6,4,22,bwofm2,4 Ways AI Technology Will Be Used in Autonomous Drones,https://www.reddit.com/r/MachineLearning/comments/bwofm2/4_ways_ai_technology_will_be_used_in_autonomous/,S_paddy,1559653312,,0,1
211,2019-6-4,2019,6,4,22,bwopml,Artificial neural networks explained in simple words,https://www.reddit.com/r/MachineLearning/comments/bwopml/artificial_neural_networks_explained_in_simple/,atomlib_com,1559654974,,0,1
212,2019-6-4,2019,6,4,23,bwp7gr,[D] Handling Lag Features for different time frames.,https://www.reddit.com/r/MachineLearning/comments/bwp7gr/d_handling_lag_features_for_different_time_frames/,babuunn,1559657815,"Hi,

&amp;#x200B;

I'm currently working on a project which involves a sort of time series problem which I transformed to a classification problem for more detailed prediction, i.e. rather than having an aggregated figure at the end of the day in the time series  modeling of the problem, I rather classify single instances which eventually depict the figure of the time series modeling when aggregated.

&amp;#x200B;

Anyway, I try to train two different models which should be used at two different ponts in time. One is basically a 24h model which should predict instances scheduled for the next day and one model which should predict the very same instances one week beforehand. Below, I tried to illustrate the problem on a time line, hope this helps. 

&amp;#x200B;

&amp;#x200B;

![img](c4p09z2rkc231)

I started with the former model which seems a bit easier, as all information that can be available, are available at prediction point for this model. I did some feature engineering which mostly includes lag features that is over the last instances that are recorded. Since the lag features seems to contribute quite well to the model's performance, I actually wanted to re-use them in the 'one-week' model. However, I face the problem that I don't know how to calculate them accurately (if that makes even sense in this case).

&amp;#x200B;

As you can see in the second time line, between the prediction point and the time where the instance is scheduled I'd like to predict, there's a gap of a week where there could potentially be more scheduled instances. I'm not sure how to deal with it. If I was ignoring the gap week completely and keep on calculating the lag features in the same sense as in the 24h model, I feel that this will not work out quite well (although I haven't tried it yet). 

&amp;#x200B;

Unfortunately, I couldn't find any literature on this problem or some sort of kaggle competition where this problem was also faced. Therefore, I don't have any ideas how to handle it and would appreciate any kind of ideas from you guys.

&amp;#x200B;

Thanks very much!",2,3
213,2019-6-4,2019,6,4,23,bwpdg4,[R] Genome-wide Variants of Eurasian Facial Shape Differentiation and a prospective model of DNA based Face Prediction (Tested on Uighurs),https://www.reddit.com/r/MachineLearning/comments/bwpdg4/r_genomewide_variants_of_eurasian_facial_shape/,jivatman,1559658701,,0,1
214,2019-6-4,2019,6,4,23,bwphdo,"CX_DB8 - A state of the art, biasable, word-level extractive summarizer",https://www.reddit.com/r/MachineLearning/comments/bwphdo/cx_db8_a_state_of_the_art_biasable_wordlevel/,Best_Mord_Brazil,1559659283,,1,1
215,2019-6-4,2019,6,4,23,bwpiox,[P] Predicting Hotel Cancellations,https://www.reddit.com/r/MachineLearning/comments/bwpiox/p_predicting_hotel_cancellations/,plentyofnodes,1559659482,"I've recently been working on a project to predict whether a customer will cancel a hotel booking based on numerous factors.

&amp;#x200B;

The blog can be found below, any feedback is greatly appreciated!

&amp;#x200B;

Blog: [Using ExtraTrees and Logistic Regression to Predict Hotel Cancellations](https://www.michael-grogan.com/hotel-cancellations/)",7,9
216,2019-6-4,2019,6,4,23,bwpja3,PU learning state of the art,https://www.reddit.com/r/MachineLearning/comments/bwpja3/pu_learning_state_of_the_art/,jintoku,1559659571,[removed],0,1
217,2019-6-5,2019,6,5,0,bwpx1f,"AMA: We are IBM researchers, scientists and developers working on data science, machine learning and AI. Start asking your questions now and we'll answer them on Tuesday the 4th of June at 1-3 PM ET / 5-7 PM UTC",https://www.reddit.com/r/MachineLearning/comments/bwpx1f/ama_we_are_ibm_researchers_scientists_and/,alexa_y,1559661602,,0,1
218,2019-6-5,2019,6,5,0,bwpyr2,[D] Velocity control for autonomous vehicles,https://www.reddit.com/r/MachineLearning/comments/bwpyr2/d_velocity_control_for_autonomous_vehicles/,figglesfiggles,1559661846,"I've been reading a lot lately about autonomous vehicles (self-driving cars), and see lots of different algorithms people use to try to regulate what speed the car should be going.  What I don't find many articles on lately (mostly articles from the 90s) are how to actually regulate the car's current parameters (speed, acceleration, steering) compared to the desired outcome (i.e. what your algorithm told you to adjust your parameters to).

My initial thought (and this is what cruise control uses, at least in the past, for example) is that velocity and steering are controlled by PID controllers.  Is that true now, even on vehicles whose desired velocity/steering inputs are determined from machine learning algorithms?  If not, can anyone elaborate or point me in a direction where I can learn more about this?  For example, if you train a vehicle to drive based off of driving data, suppose your algorithm tells you to speed up.  When you increase the throttle, if for some unseen reason your acceleration isn't at what the algorithm wants you to be at, what happens, and what is in charge of how to handle these differences?",5,5
219,2019-6-5,2019,6,5,0,bwq0ga,Text clustering in same thematic corpus ?,https://www.reddit.com/r/MachineLearning/comments/bwq0ga/text_clustering_in_same_thematic_corpus/,Wats0ns,1559662076,[removed],0,1
220,2019-6-5,2019,6,5,0,bwq3k9,Python Package for Text Cleaning,https://www.reddit.com/r/MachineLearning/comments/bwq3k9/python_package_for_text_cleaning/,filt_er,1559662513,,0,1
221,2019-6-5,2019,6,5,0,bwq3pm,"AMA: We are IBM researchers, scientists and developers working on data science, machine learning and AI. Start asking your questions now and we'll answer them on Tuesday the 4th of June at 1-3 PM ET / 5-7 PM UTC",https://www.reddit.com/r/MachineLearning/comments/bwq3pm/ama_we_are_ibm_researchers_scientists_and/,kmh4321,1559662534,,0,1
222,2019-6-5,2019,6,5,0,bwq8a8,Artificial Intelligence Is Headed to the Automotive Industry,https://www.reddit.com/r/MachineLearning/comments/bwq8a8/artificial_intelligence_is_headed_to_the/,S_paddy,1559663193,,0,1
223,2019-6-5,2019,6,5,1,bwqn3z,Grouping already known objects on an Image.,https://www.reddit.com/r/MachineLearning/comments/bwqn3z/grouping_already_known_objects_on_an_image/,ankudini,1559665310,[removed],0,1
224,2019-6-5,2019,6,5,1,bwqsh7,Why They Shut Down Facebook's AI Wing?,https://www.reddit.com/r/MachineLearning/comments/bwqsh7/why_they_shut_down_facebooks_ai_wing/,AshishKhuraishy,1559666062,,0,1
225,2019-6-5,2019,6,5,1,bwqtqr,QuantumBlack aims to tame the unruliness of data scientists with a new open source tool called Kedro,https://www.reddit.com/r/MachineLearning/comments/bwqtqr/quantumblack_aims_to_tame_the_unruliness_of_data/,stichbury,1559666246,,0,2
226,2019-6-5,2019,6,5,1,bwqztc,Paper list on adversarial machine learning,https://www.reddit.com/r/MachineLearning/comments/bwqztc/paper_list_on_adversarial_machine_learning/,rararandom0,1559667109,[removed],0,1
227,2019-6-5,2019,6,5,2,bwr61x,IBM AMA on AI for enterprise going on now,https://www.reddit.com/r/MachineLearning/comments/bwr61x/ibm_ama_on_ai_for_enterprise_going_on_now/,[deleted],1559667985,,0,1
228,2019-6-5,2019,6,5,2,bwraum,[N] IBM AMA on AI right now,https://www.reddit.com/r/MachineLearning/comments/bwraum/n_ibm_ama_on_ai_right_now/,sumitg,1559668655,there is a IBM AMA on AI for the enterprise today at 10am PST [https://www.reddit.com/r/artificial/comments/bvbgw9/ama\_we\_are\_ibm\_researchers\_scientists\_and/?sort=new](https://www.reddit.com/r/artificial/comments/bvbgw9/ama_we_are_ibm_researchers_scientists_and/?sort=new),2,4
229,2019-6-5,2019,6,5,2,bwrjys,[R] Learning Perceptually-Aligned Representations via Adversarial Robustness,https://www.reddit.com/r/MachineLearning/comments/bwrjys/r_learning_perceptuallyaligned_representations/,andrew_ilyas,1559669939,"Blog Post: [http://gradientscience.org/robust\_reps](http://gradientscience.org/robust_reps) 

Paper: [https://arxiv.org/abs/1906.00945](https://arxiv.org/abs/1906.00945)

&amp;#x200B;

TL;DR: Representation layers of standard networks are really useful for, e.g. transfer learning, but are extremely brittle and known to sort of ""break down"" when it comes to manipulating them or visualizing them in natural ways. We propose robust optimization (adversarial training) as a way to enforce priors on models' learned features. The resulting models (just lp-robust classifiers)  are amenable to all sorts of ""natural"" manipulation that follow exactly from our idealization of representations as high-level features, but are impossible with standard networks. This suggests that robustness might be more broadly useful than just protection against adversarial examples.",8,6
230,2019-6-5,2019,6,5,2,bwrn7g,Is it possible to convert model to code,https://www.reddit.com/r/MachineLearning/comments/bwrn7g/is_it_possible_to_convert_model_to_code/,Baynez1103,1559670384,[removed],0,1
231,2019-6-5,2019,6,5,2,bwrsgm,[N] Audio Clips Generated by MachineLearning,https://www.reddit.com/r/MachineLearning/comments/bwrsgm/n_audio_clips_generated_by_machinelearning/,vadhavaniyafaijan,1559671117,,0,1
232,2019-6-5,2019,6,5,3,bwrwc1,CVPR 2019 Noise-Tolerant Training work `Learning to Learn from Noisy Labeled Data 'https://arxiv.org/pdf/1812.05214.pdf,https://www.reddit.com/r/MachineLearning/comments/bwrwc1/cvpr_2019_noisetolerant_training_work_learning_to/,[deleted],1559671636,,0,1
233,2019-6-5,2019,6,5,3,bws5iv,[R] CVPR 2019 Noise-Tolerant Training work `Learning to Learn from Noisy Labeled Data ',https://www.reddit.com/r/MachineLearning/comments/bws5iv/r_cvpr_2019_noisetolerant_training_work_learning/,XinshaoWang,1559672925,"[https://arxiv.org/pdf/1812.05214.pdf](https://arxiv.org/pdf/1812.05214.pdf)

This work achieves promising results with meta-learning. Our result on Clothing 1M is comparable with theirs. However, their modelling via meta-learning seems extremely complex in practice.

&amp;#x200B;

Too many hyper-parameters shown in their Algorithm 1 and implementation section 4.2:

1. The number of synthetic mini-batches (meta-training iterations) M;
2. Meta-training step size \\alpha;
3. Meta-learning rate \\eta;
4. Student learning rate \\beta;
5. Exponential moving average (EMA) decay \\gamma;
6. The threshold for data filtering \\tau;
7. The number of samples with label replacement, \\rho;

&amp;#x200B;

The strategies of iterative training together with iterative data filtering/cleaning, reusing last-round best model as mentor, etc., make it difficult to handle in practice.

&amp;#x200B;

However, the ideas are interesting and novel:

1. Oracle/Mentor (Consistency loss): To make meta-test reliable, the teacher/mentor model should be reliable and robust to real noisy examples. Therefore, they apply iterative training and iterative data cleaning to make the meta-test consistency loss reliable and an optimisation oracle against real noise.
2. Unaffected by synthetic noise: The meta-training sees synthetic noisy training examples. After training on them, the meta-testing evaluates its consistency with oracle and aims to maximise the consistency, i.e., making it unaffected after seeing synthetic noise.

&amp;#x200B;

Quetions arise:

Is meta-learning really a good solution in practice with such many configurations?

Or could we simplfiy its modelling to make it easier in practice?",1,1
234,2019-6-5,2019,6,5,3,bws9c4,CapsAttacks: Testing Adversarial Attacks on Capsule Networks,https://www.reddit.com/r/MachineLearning/comments/bws9c4/capsattacks_testing_adversarial_attacks_on/,Yuqing7,1559673437,,0,1
235,2019-6-5,2019,6,5,3,bwsao7,[R]Emphasis Regularisation by Gradient Rescaling for Training Deep Neural Networks with Noisy Labels,https://www.reddit.com/r/MachineLearning/comments/bwsao7/remphasis_regularisation_by_gradient_rescaling/,XinshaoWang,1559673623,[removed],1,1
236,2019-6-5,2019,6,5,3,bwsffu,[R] Improving MAE against CCE under Label Noise,https://www.reddit.com/r/MachineLearning/comments/bwsffu/r_improving_mae_against_cce_under_label_noise/,XinshaoWang,1559674294,"**Why does MAE work much worse than CCE although it is noise-robust?**

**How to improve MAE against CCE to embrace noise-robustness and high generalisation performance?**

&amp;#x200B;

To find an answer, please check and feel free to contact:

[https://arxiv.org/abs/1903.12141](https://arxiv.org/abs/1903.12141) (Xinshao Wang, Elyor Kodirov, Yang Hua, Neil M. Robertson)

&amp;#x200B;

Abstract:

Label noise is inherent in many deep learning tasks when the training set becomes large. A typical approach to tackle noisy labels is using robust loss functions. Categorical cross entropy (CCE) is a successful loss function in many applications. However, CCE is also notorious for fitting samples with corrupted labels easily. In contrast, mean absolute error (MAE) is noise-tolerant theoretically, but it generally works much worse than CCE in practice. In this work, we have three main points. First, to explain why MAE generally performs much worse than CCE, we introduce a new understanding of them fundamentally by exposing their intrinsic sample weighting schemes from the perspective of every sample's gradient magnitude with respect to logit vector. Consequently, we find that MAE's differentiation degree over training examples is too small so that informative ones cannot contribute enough against the non-informative during training. Therefore, MAE generally underfits training data when noise rate is high. Second, based on our finding, we propose an improved MAE (IMAE), which inherits MAE's good noise-robustness. Moreover, the differentiation degree over training data points is controllable so that IMAE addresses the underfitting problem of MAE. Third, the effectiveness of IMAE against CCE and MAE is evaluated empirically with extensive experiments, which focus on image classification under synthetic corrupted labels and video retrieval under real noisy labels.",1,0
237,2019-6-5,2019,6,5,4,bwsltn,[1905.13678v2] Learning Sparse Networks Using Targeted Dropout,https://www.reddit.com/r/MachineLearning/comments/bwsltn/190513678v2_learning_sparse_networks_using/,sidsig,1559675162,,1,8
238,2019-6-5,2019,6,5,4,bwsneu,"[D] Friend just graduated with a masters in info systems, told him I'd post this here. Never forget to train the most important model.",https://www.reddit.com/r/MachineLearning/comments/bwsneu/d_friend_just_graduated_with_a_masters_in_info/,[deleted],1559675373,[deleted],0,1
239,2019-6-5,2019,6,5,4,bwsrdu,"Introducing TensorNetwork, an Open Source Library for Efficient Tensor Calculations",https://www.reddit.com/r/MachineLearning/comments/bwsrdu/introducing_tensornetwork_an_open_source_library/,sjoerdapp,1559675900,,0,1
240,2019-6-5,2019,6,5,4,bwssdx,Finding it hard to understand and code research paper as im a newbie in deep learning .,https://www.reddit.com/r/MachineLearning/comments/bwssdx/finding_it_hard_to_understand_and_code_research/,b14cksh4d0w369,1559676043,[removed],0,1
241,2019-6-5,2019,6,5,4,bwssep,"[Discussion] Friend just graduated with a masters in info systems, told him I'd post this here. Never forget to train the most important model.",https://www.reddit.com/r/MachineLearning/comments/bwssep/discussion_friend_just_graduated_with_a_masters/,TyrannosaurusFlex92,1559676046,,0,1
242,2019-6-5,2019,6,5,4,bwsvn5,[R] Emphasis Regularisation by Gradient Rescaling for Training Deep Neural Networks with Noisy Labels,https://www.reddit.com/r/MachineLearning/comments/bwsvn5/r_emphasis_regularisation_by_gradient_rescaling/,XinshaoWang,1559676479,[removed],1,1
243,2019-6-5,2019,6,5,4,bwt0ej,"""[R]"" Emphasis Regularisation by Gradient Rescaling for Training Deep Neural Networks with Noisy Labels",https://www.reddit.com/r/MachineLearning/comments/bwt0ej/r_emphasis_regularisation_by_gradient_rescaling/,XinshaoWang,1559677082,[removed],0,1
244,2019-6-5,2019,6,5,5,bwtc7p,Padding LSTM - Is this the right move?,https://www.reddit.com/r/MachineLearning/comments/bwtc7p/padding_lstm_is_this_the_right_move/,zoombapup,1559678624,[removed],0,1
245,2019-6-5,2019,6,5,6,bwucq9,KVQA: Knowledge-aware Visual Question Answering,https://www.reddit.com/r/MachineLearning/comments/bwucq9/kvqa_knowledgeaware_visual_question_answering/,IcemanLove,1559683755,[removed],0,1
246,2019-6-5,2019,6,5,6,bwudwi,[R] RNAsamba: coding potential assessment using ORF and whole transcript sequence information,https://www.reddit.com/r/MachineLearning/comments/bwudwi/r_rnasamba_coding_potential_assessment_using_orf/,redna11,1559683915,"Landing page: [https://www.biorxiv.org/content/10.1101/620880v1](https://www.biorxiv.org/content/10.1101/620880v1)

Paper: [https://www.biorxiv.org/content/biorxiv/early/2019/04/28/620880.full.pdf](https://www.biorxiv.org/content/biorxiv/early/2019/04/28/620880.full.pdf)

&amp;#x200B;

In this paper, the IGLOO structure is used as an alternative to RNNs for the biological task of predicting whether a RNA sequence codes for proteines or not. This approach improves upon standard methods. Contrary to GRUs and LSTMs, IGLOO can deal with a large number of time steps, i.e. more than 25,000 which is particularly suitable for biological data.",0,1
247,2019-6-5,2019,6,5,6,bwufgy,Computer Vision and Image Processing Online Course,https://www.reddit.com/r/MachineLearning/comments/bwufgy/computer_vision_and_image_processing_online_course/,anurag_pandey_,1559684127,[removed],0,1
248,2019-6-5,2019,6,5,6,bwuhk6,[R] RNAsamba: coding potential assessment using ORF and whole transcript sequence information,https://www.reddit.com/r/MachineLearning/comments/bwuhk6/r_rnasamba_coding_potential_assessment_using_orf/,redna11,1559684415,"Landing page: [https://www.biorxiv.org/content/10.1101/620880v1](https://www.biorxiv.org/content/10.1101/620880v1)

Paper: [https://www.biorxiv.org/content/biorxiv/early/2019/04/28/620880.full.pdf](https://www.biorxiv.org/content/biorxiv/early/2019/04/28/620880.full.pdf)

&amp;#x200B;

In this paper, the IGLOO structure is used as an alternative to GRUs and LSTMs to build a model used to predict whether a RNA sequence will code for proteines or not. IGLOO is particularly suited for long sequences (25,000++ times steps). This method improves on standard approaches.",0,1
249,2019-6-5,2019,6,5,6,bwujxu,[D] What is the absolute minimal computational power required for an artificial neural network?,https://www.reddit.com/r/MachineLearning/comments/bwujxu/d_what_is_the_absolute_minimal_computational/,RealBrofessor,1559684754,"I know this question might sound stupid, but let's consider we have infinite amount of time for a simple classification task. What would be the minimal computational device able to perform such task? Has a net this ""enefficient"" been ever built?",14,1
250,2019-6-5,2019,6,5,6,bwuotn,[D] How to train for obtaining contextualized word embeddings,https://www.reddit.com/r/MachineLearning/comments/bwuotn/d_how_to_train_for_obtaining_contextualized_word/,JanssonsFrestelse,1559685449,"I am a little confused about how the actual training of models like ELMo and BERT can be achieved. In ELMo, the model predicts a representation of a word given its backward and forward context, while in BERT the encoder of the transformer model uses the attention mechanism over other words in the input to determine the representation for the masked word(s). In both models the representation is fed to a softmax layer over the vocabulary, correct? So say we have the two sentences ""the bank of the river"" and ""the central bank of Germany"". The word ""bank"" should get different representations in the sentences because of the different contexts. However, if this representation is sent to the softmax layer, both would like the output to have the highest probability for the index of the word ""bank"" in the vocabulary. How is this achieved if the two representations are different? How can we condition to learn to create different contextual representation if we, in the end, still want to end up with the same word in the vocabulary from the softmax output? Should this not result in all representations being conditioned on the same thing regardless of context, i.e. the softmax output having the highest probability for the true target word?",6,0
251,2019-6-5,2019,6,5,7,bwv19o,StyleGAN encoder with new features,https://www.reddit.com/r/MachineLearning/comments/bwv19o/stylegan_encoder_with_new_features/,albertoce,1559687230,[removed],0,1
252,2019-6-5,2019,6,5,7,bwvbmc,[P] StyleGAN encoder with player prolificacy,https://www.reddit.com/r/MachineLearning/comments/bwvbmc/p_stylegan_encoder_with_player_prolificacy/,albertoce,1559688820,"Building on StyleGAN, I generated a set of fake basketball players, starting from pics of NBA players. Now I would like to map the latent space using the number of points each player made in the last season. My idea is to adapt what [Puzer](https://github.com/Puzer/stylegan-encoder) did, mapping the latent space using the prolificacy of the different players.

&amp;#x200B;

How should I proceed after labeling the players images? I have no landmark model I can build on.",4,0
253,2019-6-5,2019,6,5,7,bwvckp,[Discussion] Text generation and Discrete latent space models,https://www.reddit.com/r/MachineLearning/comments/bwvckp/discussion_text_generation_and_discrete_latent/,vikigenius,1559688972,"After looking at this paper and the image generation quality : [Generating Diverse High-Fidelity Images with VQ-VAE-2](https://arxiv.org/abs/1906.00446), I wondered if such models can be used for text generation. And I noticed that most of the similar papers discussing discrete latent space models, don't talk about text at all, all the experiments seem to be done on images or audio domain.

What gives ? Intuitively I would think it makes sense for discrete latent space models to be quite useful for text considering it's discrete nature. Can anyone explain this or provide links to papers that successfully generate quality text from a discrete latent space?",2,3
254,2019-6-5,2019,6,5,8,bwvj0x,[D] ML Research without supervisor support,https://www.reddit.com/r/MachineLearning/comments/bwvj0x/d_ml_research_without_supervisor_support/,dramanautica,1559689994,"Im currently working toward my thesis in ML (Graph NN to be specific) however my supervisor isnt very knowledgeable in ML techniques as Im in an adjacent field. 

Does anyone have any advice on how I can make the most of this situation? I currently attend paper reading groups but most of the time I am working alone and have no one to consult or bounce ideas off.",11,19
255,2019-6-5,2019,6,5,9,bwwdh6,Doing a PhD is not worth it (with very limited exceptions),https://www.reddit.com/r/MachineLearning/comments/bwwdh6/doing_a_phd_is_not_worth_it_with_very_limited/,FutureWatch4,1559694988,[removed],0,1
256,2019-6-5,2019,6,5,9,bwwla8,Poetry and Prose from Rowan Zellers' Grover,https://www.reddit.com/r/MachineLearning/comments/bwwla8/poetry_and_prose_from_rowan_zellers_grover/,Kuzefra,1559696342,,2,1
257,2019-6-5,2019,6,5,9,bwwldh,[D] Attending or taking advantage of machine learning summer schools and conferences,https://www.reddit.com/r/MachineLearning/comments/bwwldh/d_attending_or_taking_advantage_of_machine/,dragoph,1559696357,"I have seen some conferences that I would like to attend or be a part and various deep learning summer schools. The issue is that most of these can be quite expensive, require you to travel and require you to apply to attend (i'm not sure if I would make the requirements to attend) and i'm only an undergrad student. Is it worth trying to attend or such things when you'r not an expert in the field or what can be an alternate if you're not able to physically attend but would still like to learn from?",2,1
258,2019-6-5,2019,6,5,10,bwwmh9,[D] Doing a PhD is not worth it unless exception circumstances exist,https://www.reddit.com/r/MachineLearning/comments/bwwmh9/d_doing_a_phd_is_not_worth_it_unless_exception/,FutureWatch4,1559696535,"This is a throwaway account since I don't wish to reveal my affiliation.

I am currently a PhD student at one of the top 10 groups in ML in the USA and am loving my studies and really wouldn't want to be doing anything else at this current moment. Given this, its an extremely specific set of circumstances that led me to do a PhD. To be blunt, outside of top tier programs, I dont see any benefits of doing a PhD. I have interned at a top company and every single employee I worked with had come from the same set of about 50 labs, with many publications in the same top 3 conferences. I have realized that there is a very small set of (extremely) high paying research jobs for a very small selected amount of outstanding PhD graduates. Publications in theoretical ML are also dominated by a very small amount of universities and groups and if you are not in one of these groups you are unlikely to ever come close to these conferences.

If you are offered a PhD in a group that does not have consistent excellence in the top tier ML conferences, you do not have a shot at these research scientist roles (where we hear about the 300k+ salaries in ML research). Everyone else I have seen has ended up in very traditional data scientist roles that could have been easily gained out of a masters or undergraduate degree. These roles do not require the research skills that we spend 4-7 years learning during a PhD and these research skills could even be a detriment to most commercial applied ML roles. Work experience is also extremely valuable in these roles and can present opportunities for raises that double your salary in the equivalent time to complete a PhD.",207,0
259,2019-6-5,2019,6,5,11,bwxdb7,I have a question!,https://www.reddit.com/r/MachineLearning/comments/bwxdb7/i_have_a_question/,justastudent32,1559701163,[removed],0,1
260,2019-6-5,2019,6,5,11,bwxfj0,Revolutionizing Medical Diagnosis with Machine Learning | Ankit Gupta | TED Talk,https://www.reddit.com/r/MachineLearning/comments/bwxfj0/revolutionizing_medical_diagnosis_with_machine/,jimscott1232,1559701550,,0,1
261,2019-6-5,2019,6,5,11,bwxk7k,Speech transformer for Mandarin,https://www.reddit.com/r/MachineLearning/comments/bwxk7k/speech_transformer_for_mandarin/,lyngenchan,1559702373,,0,1
262,2019-6-5,2019,6,5,11,bwxldp,[D] Xavier glotrot initialization intuition,https://www.reddit.com/r/MachineLearning/comments/bwxldp/d_xavier_glotrot_initialization_intuition/,__BetterAgent__,1559702579,"https://i.ytimg.com/vi/OAb_p-SXSeM/maxresdefault.jpg

If input/output layers get bigger, the range [-x,x] and standard deviation decreases, wouldn't this make the initialized nodes be more similar? Wouldn't it make sense to spread out the initialization of many nodes instead of making them less spread out?",2,4
263,2019-6-5,2019,6,5,11,bwxngt,"[D] 1,000 patent claims by GPT-2",https://www.reddit.com/r/MachineLearning/comments/bwxngt/d_1000_patent_claims_by_gpt2/,js_lee,1559702951,,0,1
264,2019-6-5,2019,6,5,13,bwyg5j,IPad for Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/bwyg5j/ipad_for_machine_learning/,LadyAeya,1559708351,[removed],0,1
265,2019-6-5,2019,6,5,13,bwyi28,PDF file,https://www.reddit.com/r/MachineLearning/comments/bwyi28/pdf_file/,adarshraj_nandu,1559708733,[removed],0,1
266,2019-6-5,2019,6,5,14,bwysnz,Drones for Disaster Management,https://www.reddit.com/r/MachineLearning/comments/bwysnz/drones_for_disaster_management/,chauhanpiyushIITP,1559710963,[removed],0,1
267,2019-6-5,2019,6,5,14,bwysu1,A short film written by an AI!,https://www.reddit.com/r/MachineLearning/comments/bwysu1/a_short_film_written_by_an_ai/,AncientSuntzu,1559710993,,0,1
268,2019-6-5,2019,6,5,14,bwz0dy,[R] Evaluating Scalable Bayesian Deep Learning Methods for Robust Computer Vision,https://www.reddit.com/r/MachineLearning/comments/bwz0dy/r_evaluating_scalable_bayesian_deep_learning/,dirac-hatt,1559712636,"We propose an evaluation framework for predictive uncertainty estimation that is specifically designed to test the robustness required in real-world computer vision applications. Using the proposed framework, we perform an extensive comparison of the popular ensembling and MC-dropout methods on the tasks of depth completion and street-scene semantic segmentation. Our comparison suggests that ensembling consistently provides more reliable uncertainty estimates.

arXiv: https://arxiv.org/abs/1906.01620
Code: https://github.com/fregu856/evaluating_bdl
Video: https://youtu.be/CabPVqtzsOI
Project page: http://www.fregu856.com/publication/evaluating_bdl/",20,54
269,2019-6-5,2019,6,5,14,bwz12g,How to Make Machine Learning Models for Beginners | Blog,https://www.reddit.com/r/MachineLearning/comments/bwz12g/how_to_make_machine_learning_models_for_beginners/,divya2018,1559712785,,0,1
270,2019-6-5,2019,6,5,15,bwzdg6,Reputation of less-known ML conferences,https://www.reddit.com/r/MachineLearning/comments/bwzdg6/reputation_of_lessknown_ml_conferences/,angulated_tortoise,1559715533,[removed],1,1
271,2019-6-5,2019,6,5,15,bwzegj,Machine Learning Yearning Draft by Andrew Ng [Download Link],https://www.reddit.com/r/MachineLearning/comments/bwzegj/machine_learning_yearning_draft_by_andrew_ng/,ai-lover,1559715762,[removed],0,1
272,2019-6-5,2019,6,5,16,bwzs68,The Machine Learning Crash Course  Part 1,https://www.reddit.com/r/MachineLearning/comments/bwzs68/the_machine_learning_crash_course_part_1/,BercoviciAdrian,1559718975,[removed],0,1
273,2019-6-5,2019,6,5,16,bwzyk7,Optimal Unsupervised Domain Translation,https://www.reddit.com/r/MachineLearning/comments/bwzyk7/optimal_unsupervised_domain_translation/,emmnll,1559720581,"Domain Translation is the problem of finding a meaningful correspondence between two domains. Since in a majority of settings paired supervision is not available, much work focuses on Unsupervised Domain Translation (UDT) where data samples from each domain are unpaired. Following the seminal work of CycleGAN for UDT, many variants and extensions of this model have been proposed. However, there is still little theoretical understanding behind their success. We observe that these methods yield solutions which are approximately minimal w.r.t. a given transportation cost, leading us to reformulate the problem in the Optimal Transport (OT) framework. This viewpoint gives us a new perspective on Unsupervised Domain Translation and allows us to prove the existence and uniqueness of the retrieved mapping, given a large family of transport costs. We then propose a novel framework to efficiently compute optimal mappings in a dynamical setting. We show that it generalizes previous methods and enables a more explicit control over the computed optimal mapping. It also provides smooth interpolations between the two domains. Experiments on toy and real world datasets illustrate the behavior of our method.",0,1
274,2019-6-5,2019,6,5,16,bwzzms,[D] Deep Learning and Chemical Physics - research oriented companies and startups?,https://www.reddit.com/r/MachineLearning/comments/bwzzms/d_deep_learning_and_chemical_physics_research/,konasj,1559720848,"Dear community,

I am right now pursuing a PhD at the intersection of molecular science and machine learning during which I am researching new advanced sampling techniques and generative models for molecular structures respecting physical constraints. 

While there are plenty of opportunities to just stay in academia I m curious about companies who do applied research in related fields, like searching for new materials using computational methods (in particular modern machine learning techniques) or designing functional proteins using such techniques.

I am personally very interested to eventually work on something ""real"" that exists in the physical world (e.g. designing a new catalysis membrane for waste decomposition vs building new image filters for Snapchat). But besides current endeavors in rather pure research e.g. as done by DeepMind (AlphaFold) or Google AS I don't know smaller/less known companies with maybe much more focus on real applications. There are obviously pharmaceutical firms also starting to work in this field, but right now I am more curious about possible other applications.

Do you have any ideas or suggestions what companies might be worth to look at? 

Thanks a lot!",14,3
275,2019-6-5,2019,6,5,16,bx01e9,[R] Are Disentangled Representations Helpful for Abstract Visual Reasoning?,https://www.reddit.com/r/MachineLearning/comments/bx01e9/r_are_disentangled_representations_helpful_for/,milaworld,1559721315,,1,18
276,2019-6-5,2019,6,5,17,bx04gw,Machine learning mastery with weka,https://www.reddit.com/r/MachineLearning/comments/bx04gw/machine_learning_mastery_with_weka/,pasco1111,1559722125,"Hey every one,

Can anyone help me out to get this book: 

&amp;#x200B;

Machine learning mastery with weka, by Jason Brownlee

thank you",0,1
277,2019-6-5,2019,6,5,17,bx04ti,Combining Distributed Computing with Deep Learing,https://www.reddit.com/r/MachineLearning/comments/bx04ti/combining_distributed_computing_with_deep_learing/,selrok,1559722224,"Are there any open source projects (or free products) that I can use to combine Deep Learning with Distributed Computing? Because I recently discovered what serverless computing is and I realized that I needed a cloud provider, so the first thing that I thought of was Apache's products, like MXNet, Hadoop and Spark but I don't know if they're as useful as serverless computing, given that serverless uses a more dynamic approach.",0,1
278,2019-6-5,2019,6,5,17,bx05gx,[R] Learning Sparse Networks Using Targeted Dropout,https://www.reddit.com/r/MachineLearning/comments/bx05gx/r_learning_sparse_networks_using_targeted_dropout/,sensetime,1559722402,,3,30
279,2019-6-5,2019,6,5,17,bx05s3,[R] Combining Distributed Computing with Deep Learing,https://www.reddit.com/r/MachineLearning/comments/bx05s3/r_combining_distributed_computing_with_deep/,selrok,1559722488," Are  there any open source projects (or free products) that I can use to  combine Deep Learning with Distributed Computing? Because I recently  discovered what serverless computing is and I realized that I needed a  cloud provider, so the first thing that I thought of was Apache's  products, like MXNet, Hadoop and Spark but I don't know if they're as  useful as serverless computing, given that serverless uses a more  dynamic approach.

P.S I'm aiming to built one for an Image Recognition algorithm",0,2
280,2019-6-5,2019,6,5,17,bx0apm,[D] How do you manage your machine learning experiments?,https://www.reddit.com/r/MachineLearning/comments/bx0apm/d_how_do_you_manage_your_machine_learning/,pigdogsheep,1559723805,"For a long time, I have been using old style spreadsheets to log the results of my experiments with columns like ""dataset-version"", ""git commit"" ""PARAMS"" ""results""...etc

However, I find that this became a pain to consistently fill and update those spreadsheets especially when performing hyper-param search.

There are many frameworks built to manage your ML experiments. I have collected a list below (thanks to the comment [here](http://akosiorek.github.io/ml/2018/11/28/forge.html)). I would like to know if you have a favourite of the ones below or another 

&amp;#x200B;

SACRED [https://github.com/IDSIA/sacred](https://github.com/IDSIA/sacred) 

Studio[https://github.com/studioml/studio](https://github.com/studioml/studio)  
Datmo[https://github.com/datmo/datmo](https://github.com/datmo/datmo)  
Lore[https://github.com/instacart/lore](https://github.com/instacart/lore)

FORGE [https://github.com/akosiorek/forge](https://github.com/akosiorek/forge)  
Sumatra [https://pythonhosted.org/Sumatra/](https://pythonhosted.org/Sumatra/)  
RandOpt[https://github.com/seba-1511/randopt](https://github.com/seba-1511/randopt)  
Pachyderm[https://github.com/pachyderm/pachyderm](https://github.com/pachyderm/pachyderm)  
feature Forge[https://github.com/machinalis/featureforge](https://github.com/machinalis/featureforge)  
Model Chimp[https://github.com/ModelChimp/modelchimp](https://github.com/ModelChimp/modelchimp)  
PolyAxon[https://github.com/polyaxon/polyaxon](https://github.com/polyaxon/polyaxon)  
Kubeflow[https://github.com/kubeflow/kubeflow](https://github.com/kubeflow/kubeflow)  
Weights and Biases[https://www.wandb.com/](https://www.wandb.com/)  


&amp;#x200B;

ps: This maybe a repost but it is worth revisiting this topic since new frameworks are out.",127,205
281,2019-6-5,2019,6,5,18,bx0ih5,Explanation of Neural Networks working without Matrices Jargon,https://www.reddit.com/r/MachineLearning/comments/bx0ih5/explanation_of_neural_networks_working_without/,ibadia,1559725921,,0,1
282,2019-6-5,2019,6,5,18,bx0pav,"Smarten Advanced Analytics is a Silver Sponsor for Gartner Data &amp; Analytics Summit, June 2019 in Mumbai, India",https://www.reddit.com/r/MachineLearning/comments/bx0pav/smarten_advanced_analytics_is_a_silver_sponsor/,ElegantMicroWebIndia,1559727655,,0,1
283,2019-6-5,2019,6,5,19,bx0za8,Info-graphic: Artificial Intelligence and Benefits Businesses can Reap From it.,https://www.reddit.com/r/MachineLearning/comments/bx0za8/infographic_artificial_intelligence_and_benefits/,marak_technologies,1559730027,[removed],0,1
284,2019-6-5,2019,6,5,19,bx13ta,What is the right way to build a recommender system for a startup?,https://www.reddit.com/r/MachineLearning/comments/bx13ta/what_is_the_right_way_to_build_a_recommender/,TheTesseractAcademy,1559731098,,0,1
285,2019-6-5,2019,6,5,20,bx1f2q,Propose of the neural net architecture for RL algorithm with possibility using of several operations at one moment,https://www.reddit.com/r/MachineLearning/comments/bx1f2q/propose_of_the_neural_net_architecture_for_rl/,klizardin,1559733611,,0,1
286,2019-6-5,2019,6,5,20,bx1h27,"Don't know if it has already been published, but I found it interesting the way this presentation allows to easily understand machine learning with simple parameters",https://www.reddit.com/r/MachineLearning/comments/bx1h27/dont_know_if_it_has_already_been_published_but_i/,danclimpertec,1559734037,,0,1
287,2019-6-5,2019,6,5,20,bx1hx8,GAN implementation issue,https://www.reddit.com/r/MachineLearning/comments/bx1hx8/gan_implementation_issue/,b14cksh4d0w369,1559734225,[removed],0,1
288,2019-6-5,2019,6,5,20,bx1qd9,[D] Is the new Mac Pro viable/reasonable choice for deep learning research?,https://www.reddit.com/r/MachineLearning/comments/bx1qd9/d_is_the_new_mac_pro_viablereasonable_choice_for/,IliumFuit,1559735986,"Basic specs are (up to) 28 CPU cores @ 2.5GHz, 1.5TB memory, and dual Radeon Pro Vega IIs.

I haven't seen a price listed yet, so I guess considering it based on the appropriateness of the hardware rather than value.",6,0
289,2019-6-5,2019,6,5,21,bx22rm,"[D] 1,000 patent claims by GPT-2",https://www.reddit.com/r/MachineLearning/comments/bx22rm/d_1000_patent_claims_by_gpt2/,js_lee,1559738320,"Hi,

Does anybody know whether the 40G WebText for GPT-2 contains lots of patents? As early as the 36th step of fine-tuning, GPT-2 can start generating patent-like text correctly with three special tags (""&lt;|startoftext|&gt;"", ""&lt;|endoftext|&gt;"", ""@@@"") in our training data. It is really unreasonably effective. Anybody in similar situation during fine-tuning? 

[Available on web: (1) the first 100 steps of fine-tuning, (2) 1000 generated patent claims.](https://aipatent.wordpress.com/research/)",12,17
290,2019-6-5,2019,6,5,21,bx25mm,Variational and Adversarial autoencoders for Feature Extraction/Dimensionality reduction (and not Generation),https://www.reddit.com/r/MachineLearning/comments/bx25mm/variational_and_adversarial_autoencoders_for/,ajmyk,1559738839,[removed],0,1
291,2019-6-5,2019,6,5,22,bx2mok,Having trouble understanding some parts of AlphaZero,https://www.reddit.com/r/MachineLearning/comments/bx2mok/having_trouble_understanding_some_parts_of/,Fossana,1559741738,[removed],0,1
292,2019-6-5,2019,6,5,22,bx2qmw,[D] Having trouble understanding some parts of AlphaZero,https://www.reddit.com/r/MachineLearning/comments/bx2qmw/d_having_trouble_understanding_some_parts_of/,Fossana,1559742371,"These are my questions:

1. 25,000 games are played against itself. After each game, is the MCTS reset for the next game, or is it kept?

2. Does each neural network consist of 1000 batches of 2048 game positions, or can it have more than 2,048,000 inputs?

3. After a new neural network is chosen, is the MCTS reset (thrown away) or kept? Are parts of it reset? Like all of the nodes in the MCTS have P's from the prior neural network, so do these get recalculated? Are the W's kept the same even though they were all calculated using V's from the previous neural network?

Thanks.",6,2
293,2019-6-5,2019,6,5,23,bx2yrq,Is KNN the right algorithm to use for lat/lon groupings?,https://www.reddit.com/r/MachineLearning/comments/bx2yrq/is_knn_the_right_algorithm_to_use_for_latlon/,chubaklava,1559743652,[removed],0,1
294,2019-6-5,2019,6,5,23,bx32t2,Why did Fast Classification Neural Network based on Kak's CC4 classification not popular,https://www.reddit.com/r/MachineLearning/comments/bx32t2/why_did_fast_classification_neural_network_based/,Abhishek_nair_1303,1559744284,[removed],0,1
295,2019-6-5,2019,6,5,23,bx35dq,Three new machine learning courses by Google,https://www.reddit.com/r/MachineLearning/comments/bx35dq/three_new_machine_learning_courses_by_google/,ConfidentMushroom,1559744674,,0,1
296,2019-6-5,2019,6,5,23,bx36yx,[R]Why did Fast Classification Neural Network based on Kak's CC4 classification not popular,https://www.reddit.com/r/MachineLearning/comments/bx36yx/rwhy_did_fast_classification_neural_network_based/,Abhishek_nair_1303,1559744924,"[https://link.springer.com/article/10.1007%2Fs00034-002-2007-7](https://link.springer.com/article/10.1007%2Fs00034-002-2007-7)

It seems a faster way of learning or training models.

This can be used for IoT applications where online learning can be challenging in terms of power and speed (performance).

I understand that this has limitations when scaled up. But can anyone tell me more about demerits of this classification algorithm.",0,6
297,2019-6-5,2019,6,5,23,bx38r6,"[D] I created dilated convolution seq2seq and I want to publish it. I do not have any degree, then how?",https://www.reddit.com/r/MachineLearning/comments/bx38r6/d_i_created_dilated_convolution_seq2seq_and_i/,huseinzol05,1559745192,"I improve https://arxiv.org/abs/1705.03122 architecture to use dilated convolution based and change the attention mechanism. In small dataset, I beat that model in term of word position accuracy, never tested on bigger dataset, I would like to if I can get some guide to publish the results on research paper.

Accuracy after 20 epoch on convolution seq2seq,
```
epoch: 20, avg loss: 5.096660, avg accuracy: 0.177135
```

Accuracy after 20 epoch on dilated convolution seq2seq,
```
epoch: 20, avg loss: 0.009440, avg accuracy: 1.023615
```

Pardon about the accuracy more than 1.0, that one just a bug from my code, can fix it less than a second.

Again, I never tested on bigger dataset.",16,0
298,2019-6-5,2019,6,5,23,bx39ui,[D] Understanding Gradient Boosting as a gradient descent,https://www.reddit.com/r/MachineLearning/comments/bx39ui/d_understanding_gradient_boosting_as_a_gradient/,Niourf,1559745357,,0,2
299,2019-6-5,2019,6,5,23,bx3a8o,"[R] New release of Microsoft Recommenders: new algorithms, notebooks and utilities (LigthGBM, Riemannian Low-rank Matrix Completion and others)",https://www.reddit.com/r/MachineLearning/comments/bx3a8o/r_new_release_of_microsoft_recommenders_new/,hoaphumanoid,1559745415,,0,1
300,2019-6-5,2019,6,5,23,bx3a94,Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations,https://www.reddit.com/r/MachineLearning/comments/bx3a94/scene_representation_networks_continuous/,3d_is_key,1559745418,[removed],1,1
301,2019-6-5,2019,6,5,23,bx3cx1,Decision Tees in ML,https://www.reddit.com/r/MachineLearning/comments/bx3cx1/decision_tees_in_ml/,ranjiraj,1559745832,,0,1
302,2019-6-5,2019,6,5,23,bx3db8,I have to create a voice classification NN. need some help.,https://www.reddit.com/r/MachineLearning/comments/bx3db8/i_have_to_create_a_voice_classification_nn_need/,GetMeSomeDownVotes,1559745883,[removed],0,1
303,2019-6-6,2019,6,6,0,bx3nr6,INTRODUCCIN AL MACHINE LEARNING,https://www.reddit.com/r/MachineLearning/comments/bx3nr6/introduccin_al_machine_learning/,jeffry_30,1559747442,[removed],0,1
304,2019-6-6,2019,6,6,0,bx3pfe,"[D] When asked what not-yet-existing tool they wish they had, the IBM AI team said tools to automate and generate training data. Is there a subfield for ML models to clean and format training data?",https://www.reddit.com/r/MachineLearning/comments/bx3pfe/d_when_asked_what_notyetexisting_tool_they_wish/,BatmantoshReturns,1559747681,"&gt; JS - A tool to automatically generate all the training data that we need...the problem is, **however, development of this tool will likely need training data as well.**
&gt; 
&gt; FT - Agree with all of the above. But in particular for Conversational AI (where I spend most of my time with Watson Assistant) - any automation tool that could take a clients data and automatically build out intent/entity recognition AND the dialog.
&gt; 
&gt; RP - Once you are in the trenches, you realize, it all starts from data. I wish we had a tool that takes noisy data and makes it clear for AI - all automatically. Enterprises soon realize, they spend most of the time in getting data ready for AI, from different formats, in different places with different permission, with tons of noise. An automation tool to make that ""look ma - no hands"" will be great!

I am thinking, what sort of model would you use for this? What training data would you feed it? How would you generate that data?",16,4
305,2019-6-6,2019,6,6,0,bx453n,"Simple Questions Thread June 05, 2019",https://www.reddit.com/r/MachineLearning/comments/bx453n/simple_questions_thread_june_05_2019/,AutoModerator,1559749971,[removed],0,1
306,2019-6-6,2019,6,6,0,bx467g,12 Best AI Chatbots For Android,https://www.reddit.com/r/MachineLearning/comments/bx467g/12_best_ai_chatbots_for_android/,S_paddy,1559750132,,0,1
307,2019-6-6,2019,6,6,0,bx471d,"Free Machine Learning Courses: Stanford, Harvard, Udacity and Google...",https://www.reddit.com/r/MachineLearning/comments/bx471d/free_machine_learning_courses_stanford_harvard/,sak1m,1559750261,,0,1
308,2019-6-6,2019,6,6,1,bx492w,"Interview with Christian Szegedy, inventor of Inception, BatchNorm",https://www.reddit.com/r/MachineLearning/comments/bx492w/interview_with_christian_szegedy_inventor_of/,connaissance,1559750539,,0,1
309,2019-6-6,2019,6,6,1,bx4a5d,Group on Telegram For Who Have a Passion For:,https://www.reddit.com/r/MachineLearning/comments/bx4a5d/group_on_telegram_for_who_have_a_passion_for/,farzadhs,1559750687,[removed],0,1
310,2019-6-6,2019,6,6,1,bx4ejg,"[D] Interview with Christian Szegedy, inventor of Inception, BatchNorm",https://www.reddit.com/r/MachineLearning/comments/bx4ejg/d_interview_with_christian_szegedy_inventor_of/,connaissance,1559751302,,0,1
311,2019-6-6,2019,6,6,1,bx4few,Google AI EfficientNets Improve CNN Scaling,https://www.reddit.com/r/MachineLearning/comments/bx4few/google_ai_efficientnets_improve_cnn_scaling/,Yuqing7,1559751420,,0,1
312,2019-6-6,2019,6,6,1,bx4mb6,Jewelry got Smart with this Bangle!,https://www.reddit.com/r/MachineLearning/comments/bx4mb6/jewelry_got_smart_with_this_bangle/,Meee-Services,1559752359,[removed],0,1
313,2019-6-6,2019,6,6,1,bx4qje,On the state of Deep Learning outside of CUDAs walled garden,https://www.reddit.com/r/MachineLearning/comments/bx4qje/on_the_state_of_deep_learning_outside_of_cudas/,marijnfs,1559752949,,0,1
314,2019-6-6,2019,6,6,1,bx4w8z,[P] From Y=X to Building a Complete Artificial Neural Network,https://www.reddit.com/r/MachineLearning/comments/bx4w8z/p_from_yx_to_building_a_complete_artificial/,ahmedfgad,1559753754,,0,2
315,2019-6-6,2019,6,6,1,bx4xlb,[D] How to tokenize noisy text data properly?,https://www.reddit.com/r/MachineLearning/comments/bx4xlb/d_how_to_tokenize_noisy_text_data_properly/,svufzafa,1559753941,"Hi,

&amp;#x200B;

I have a noisy text corpus from twitter. I want to tokenize it efficiently to train language models ( e.g. GPT ).

Most of the sentences have :

1) Spelling errors

2) Emojis 

3) Slang spellings ( eg great -&gt; gr8 )

4)  All sorts of weird stuff 

&amp;#x200B;

Any script/tool which takes care of all kinds of cases and works for all kinds of English text.

&amp;#x200B;

Thank you",4,5
316,2019-6-6,2019,6,6,1,bx4xvh,It's a bit messy but: I _really_ like Visdom,https://www.reddit.com/r/MachineLearning/comments/bx4xvh/its_a_bit_messy_but_i_really_like_visdom/,bluecoffee,1559753979,,1,1
317,2019-6-6,2019,6,6,2,bx53va,[D] It's a bit messy but: I _really_ like Visdom,https://www.reddit.com/r/MachineLearning/comments/bx53va/d_its_a_bit_messy_but_i_really_like_visdom/,bluecoffee,1559754809,,1,2
318,2019-6-6,2019,6,6,2,bx5a17,An Inside Look at Google Earth Timelapse,https://www.reddit.com/r/MachineLearning/comments/bx5a17/an_inside_look_at_google_earth_timelapse/,sjoerdapp,1559755685,,0,1
319,2019-6-6,2019,6,6,2,bx5bt7,[D] I was assigned to create voice classification NN. need some advices.,https://www.reddit.com/r/MachineLearning/comments/bx5bt7/d_i_was_assigned_to_create_voice_classification/,GetMeSomeDownVotes,1559755922,"For my end semester final project, Professor asked our team to develop a voice classification neural network. I dont have any idea where should I start from. Classification types are laughing , stammering, crying , angry voice , gender of  the speaker,  age group, etc. 

please give me some advices.   
Where should I start from ?   
Any existing projects or services ? (So far I havent found any).   
Any similar projects to refer ?",4,0
320,2019-6-6,2019,6,6,2,bx5id3,How to Use Machine Learning to Scale Data Quality,https://www.reddit.com/r/MachineLearning/comments/bx5id3/how_to_use_machine_learning_to_scale_data_quality/,raj11113,1559756842,,0,1
321,2019-6-6,2019,6,6,2,bx5jzt,INTRODUCCIN AL APRENDIZAJE AUTOMTICO,https://www.reddit.com/r/MachineLearning/comments/bx5jzt/introduccin_al_aprendizaje_automtico/,jeffry_30,1559757064,[removed],0,1
322,2019-6-6,2019,6,6,3,bx6ak0,Printing a poster for ICML?,https://www.reddit.com/r/MachineLearning/comments/bx6ak0/printing_a_poster_for_icml/,helpmeprintmyposter,1559760683,[removed],0,1
323,2019-6-6,2019,6,6,3,bx6c50,Using Machine Learning to predict unobservable/lost demand,https://www.reddit.com/r/MachineLearning/comments/bx6c50/using_machine_learning_to_predict/,downthemlrabbithole,1559760893,[removed],0,1
324,2019-6-6,2019,6,6,4,bx6jfr,Where AI Creates Winners  and Losers - Techonomy,https://www.reddit.com/r/MachineLearning/comments/bx6jfr/where_ai_creates_winners_and_losers_techonomy/,kingeurythmic,1559761871,,0,1
325,2019-6-6,2019,6,6,4,bx6kbj,[R] Optimal Unsupervised Domain Translation,https://www.reddit.com/r/MachineLearning/comments/bx6kbj/r_optimal_unsupervised_domain_translation/,emmnll,1559761991,"Domain Translation is the problem of finding a meaningful correspondence between two domains. Since in a majority of settings paired supervision is not available, much work focuses on Unsupervised Domain Translation (UDT) where data samples from each domain are unpaired. Following the seminal work of CycleGAN for UDT, many variants and extensions of this model have been proposed. However, there is still little theoretical understanding behind their success. We observe that these methods yield solutions which are approximately minimal w.r.t. a given transportation cost, leading us to reformulate the problem in the Optimal Transport (OT) framework. This viewpoint gives us a new perspective on Unsupervised Domain Translation and allows us to prove the existence and uniqueness of the retrieved mapping, given a large family of transport costs. We then propose a novel framework to efficiently compute optimal mappings in a dynamical setting. We show that it generalizes previous methods and enables a more explicit control over the computed optimal mapping. It also provides smooth interpolations between the two domains. Experiments on toy and real world datasets illustrate the behavior of our method.

&amp;#x200B;

arxiv: [https://arxiv.org/abs/1906.01292](https://arxiv.org/abs/1906.01292)",2,15
326,2019-6-6,2019,6,6,5,bx76xb,How do I get started in Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/bx76xb/how_do_i_get_started_in_machine_learning/,Foesa,1559765115,[removed],0,1
327,2019-6-6,2019,6,6,5,bx7msb,[P] Complete Python project is needed,https://www.reddit.com/r/MachineLearning/comments/bx7msb/p_complete_python_project_is_needed/,vitalker,1559767303,"Hello.

&amp;#x200B;

Is there someone, who can share his/her project with me? The project (code) should not be publicly available (found by search engines).

Dataset should contain numbers only. The projest should be for beginners: evalutaion, validation, SVM, kNN, random forest, feature selection/generation, xgb, neural networks, unsupervised learning, regularization.

Thanks in advance.",2,0
328,2019-6-6,2019,6,6,6,bx8aey,Suggestions for any Reinforcement learning project for a beginner,https://www.reddit.com/r/MachineLearning/comments/bx8aey/suggestions_for_any_reinforcement_learning/,tiwari504,1559770662,[removed],1,1
329,2019-6-6,2019,6,6,6,bx8csq,[D] When should I archive a paper?,https://www.reddit.com/r/MachineLearning/comments/bx8csq/d_when_should_i_archive_a_paper/,SlowPhilosopherData,1559771012,"Hey everyone! I am a n00b here, A quick question. 

We plan to submit results to a conference that due in 4 months. Our study currently have good results, in case of other labs are working on the same problem, we would like to archive it on arXiv as soon as possible. However, there is possibilities that other research groups can look into our archived paper, improve upon, and submit to same conference in between. If that is the case, can the reviewers of the conference simply reject our paper and prefer the new one? 

&amp;#x200B;

Thanks in advance!",5,1
330,2019-6-6,2019,6,6,6,bx8dzw,New SOTA for 3D Object Detection,https://www.reddit.com/r/MachineLearning/comments/bx8dzw/new_sota_for_3d_object_detection/,Hypernion,1559771177,[removed],0,1
331,2019-6-6,2019,6,6,7,bx8jhz,[R] New SOTA for 3D Object Detection,https://www.reddit.com/r/MachineLearning/comments/bx8jhz/r_new_sota_for_3d_object_detection/,Hypernion,1559772003,"A new efficient, anchor-free and end-to-end trainable 3D object detection method that achieves SOTA results on ScanNet and S3DIS and is approx. 10x faster than most existing methods...

Paper and code: [https://arxiv.org/pdf/1906.01140.pdf](https://arxiv.org/pdf/1906.01140.pdf)",16,100
332,2019-6-6,2019,6,6,8,bx9juf,[D] Superdatascience Pod: Achieving Data Science Maturity,https://www.reddit.com/r/MachineLearning/comments/bx9juf/d_superdatascience_pod_achieving_data_science/,ShantanuJoshi,1559777541,,0,1
333,2019-6-6,2019,6,6,9,bxa613,Where to start?,https://www.reddit.com/r/MachineLearning/comments/bxa613/where_to_start/,jrell4,1559781214,[removed],0,1
334,2019-6-6,2019,6,6,9,bxa6z5,Image processing guidance,https://www.reddit.com/r/MachineLearning/comments/bxa6z5/image_processing_guidance/,ActualRealBuckshot,1559781368,Where would you start with learning image processing and analysis?,0,1
335,2019-6-6,2019,6,6,10,bxb116,How effective are current methods for classifying hate speech/other media that platforms like YT are getting bad press for?,https://www.reddit.com/r/MachineLearning/comments/bxb116/how_effective_are_current_methods_for_classifying/,pretysmitty,1559786299,[removed],0,1
336,2019-6-6,2019,6,6,11,bxbera,[P] Creating an algorithm that can play Battleship using neural networks,https://www.reddit.com/r/MachineLearning/comments/bxbera/p_creating_an_algorithm_that_can_play_battleship/,I_am_roundar,1559788561,"Hi,

I am a student who is researching the topic of neural networks. I am posting a video of a number of games played by the neural network versus human.

&amp;#x200B;

Please observe the video and answer which board (A or B) is being played by the human.

&amp;#x200B;

Link: [https://youtu.be/1tJvdbX0efo](https://youtu.be/1tJvdbX0efo)",2,6
337,2019-6-6,2019,6,6,12,bxbxlr,[D] Thoughts on this PC for moderate-level machine learning tasks?,https://www.reddit.com/r/MachineLearning/comments/bxbxlr/d_thoughts_on_this_pc_for_moderatelevel_machine/,decimated_napkin,1559791932,"Here is the link, best deal I could find for a deep learning pc for under $1000  [https://www.amazon.com/SkyTech-Azure-Computer-Desktop-GeForce/dp/B07NV1YJK9/ref=cm\_cr\_arp\_d\_product\_top?ie=UTF8](https://www.amazon.com/SkyTech-Azure-Computer-Desktop-GeForce/dp/B07NV1YJK9/ref=cm_cr_arp_d_product_top?ie=UTF8) 

Since I have been playing around with CNNs quite a bit I was really hoping to find a cheap RTX 2060, and this is by far the cheapest I've been able to find. The 16GB of RAM is solid and the 500G NVMe PCI e SSD should be all I need for awhile. The benchmarks on the Ryzen 5 1600 processor seem very good as well. Am I missing something or is this just a really good bargain for a consumer-grade machine learning pc?",3,0
338,2019-6-6,2019,6,6,12,bxbyey,using tfidf to create page list,https://www.reddit.com/r/MachineLearning/comments/bxbyey/using_tfidf_to_create_page_list/,ajaykpaul,1559792081,"Can I create a page list of arrays out of any document(doc, pdf, text, html) using tfidf in python ?",0,1
339,2019-6-6,2019,6,6,12,bxbzno,Classifier returning absolute 0/1 scores,https://www.reddit.com/r/MachineLearning/comments/bxbzno/classifier_returning_absolute_01_scores/,biryani1996,1559792306,[removed],0,1
340,2019-6-6,2019,6,6,13,bxc9p1,[P] Official Pytorch Implementation of Invertible Residual Networks (ICML19 Long Oral),https://www.reddit.com/r/MachineLearning/comments/bxc9p1/p_official_pytorch_implementation_of_invertible/,jhjac,1559794174,[removed],0,1
341,2019-6-6,2019,6,6,13,bxc9pl,"Minimal Machine Learning Cheat Sheet: A cleaner version of the Scikit-learn algorithm flow chart or cheat sheet, originally made by Andrew Mueller",https://www.reddit.com/r/MachineLearning/comments/bxc9pl/minimal_machine_learning_cheat_sheet_a_cleaner/,ai-lover,1559794177,[removed],0,1
342,2019-6-6,2019,6,6,13,bxc9vb,[R] Learning on the Edge: Explicit Boundary Handling in CNNs,https://www.reddit.com/r/MachineLearning/comments/bxc9vb/r_learning_on_the_edge_explicit_boundary_handling/,xternalz,1559794210,,0,1
343,2019-6-6,2019,6,6,13,bxcao8,[R] Learning on the Edge: Explicit Boundary Handling in CNNs,https://www.reddit.com/r/MachineLearning/comments/bxcao8/r_learning_on_the_edge_explicit_boundary_handling/,xternalz,1559794379,,0,1
344,2019-6-6,2019,6,6,13,bxcb4u,[R] Learning on the Edge: Explicit Boundary Handling in CNNs,https://www.reddit.com/r/MachineLearning/comments/bxcb4u/r_learning_on_the_edge_explicit_boundary_handling/,xternalz,1559794469,,1,8
345,2019-6-6,2019,6,6,13,bxclz9,[P] Pytorch Implementation of Invertible Residual Networks (ICML19 Long Oral),https://www.reddit.com/r/MachineLearning/comments/bxclz9/p_pytorch_implementation_of_invertible_residual/,jhjac,1559796525,,0,1
346,2019-6-6,2019,6,6,13,bxcnxy,[R] MelNet: A Generative Model for Audio in the Frequency Domain,https://www.reddit.com/r/MachineLearning/comments/bxcnxy/r_melnet_a_generative_model_for_audio_in_the/,sjv-,1559796915,,1,1
347,2019-6-6,2019,6,6,14,bxcuyu,Performing Streaming Speech Recognition on LOCAL FILE with IBM Watson,https://www.reddit.com/r/MachineLearning/comments/bxcuyu/performing_streaming_speech_recognition_on_local/,soominjung,1559798317,[removed],0,1
348,2019-6-6,2019,6,6,14,bxcwl1,[R] MelNet: A Generative Model for Audio in the Frequency Domain,https://www.reddit.com/r/MachineLearning/comments/bxcwl1/r_melnet_a_generative_model_for_audio_in_the/,sjv-,1559798644,"**Abstract:** Capturing high-level structure in audio waveforms is challenging because a single second of audio spans tens of thousands of timesteps. While long-range dependencies are difficult to model directly in the time domain, we show that they can be more tractably modelled in two-dimensional time-frequency representations such as spectrograms. By leveraging this representational advantage, in conjunction with a highly expressive probabilistic model and a multiscale generation procedure, we design a model capable of generating high-fidelity audio samples which capture structure at timescales that time-domain models have yet to achieve. We apply our model to a variety of audio generation tasks, including unconditional speech generation, music generation, and text-to-speech synthesis---showing improvements over previous approaches in both density estimates and human judgments.

**Paper:** [https://arxiv.org/abs/1906.01083](https://arxiv.org/abs/1906.01083)

**Blog (with samples):** [https://sjvasquez.github.io/blog/melnet/](https://sjvasquez.github.io/blog/melnet/)

**Many more samples:** [https://audio-samples.github.io/](https://audio-samples.github.io/)",41,65
349,2019-6-6,2019,6,6,14,bxcyjh,MLFlow as a model repository in your CI/CD workflow,https://www.reddit.com/r/MachineLearning/comments/bxcyjh/mlflow_as_a_model_repository_in_your_cicd_workflow/,manojlds,1559799050,,0,1
350,2019-6-6,2019,6,6,14,bxd21p,Performing Streaming Speech Recognition on LOCAL FILE with IBM Watson,https://www.reddit.com/r/MachineLearning/comments/bxd21p/performing_streaming_speech_recognition_on_local/,soominjung,1559799783,[removed],0,1
351,2019-6-6,2019,6,6,16,bxdsiu,What is The Hathaway Effect? | ML Theories,https://www.reddit.com/r/MachineLearning/comments/bxdsiu/what_is_the_hathaway_effect_ml_theories/,AshishKhuraishy,1559805435,,0,1
352,2019-6-6,2019,6,6,16,bxe1vn,"We are from TomTom Autonomous Driving, and are making HD maps. Ask us Anything!",https://www.reddit.com/r/MachineLearning/comments/bxe1vn/we_are_from_tomtom_autonomous_driving_and_are/,TomTom_developers,1559807619,[removed],0,1
353,2019-6-6,2019,6,6,16,bxe23e,AI Replaces Human Appraisers stardate 2019.420,https://www.reddit.com/r/MachineLearning/comments/bxe23e/ai_replaces_human_appraisers_stardate_2019420/,bentaylordata,1559807676,[removed],0,1
354,2019-6-6,2019,6,6,17,bxe4g9,"[D] We are from TomTom Autonomous Driving, and are making HD maps. Ask us Anything!",https://www.reddit.com/r/MachineLearning/comments/bxe4g9/d_we_are_from_tomtom_autonomous_driving_and_are/,TomTom_developers,1559808224,"Hi everyone,

We are from TomTom Autonomous Driving from Amsterdam and Berlin, and we are working on HD map making for automated driving and navigation.

Being able to build a highly accurate map is essential for precise localization and navigation for an automated vehicle.

To create maps that are precise and detailed enough, we need to efficiently analyse incoming data from diverse on-car sensors such as LiDAR and cameras using state of the art machine learning techniques.

We use a lot of deep learning especially pertaining to computer vision. Published papers from our group include work on structured prediction for lane detection, as well as domain adaptation:

* [EL-GAN: Embedding Loss Driven Generative Adversarial Networks for Lane Detection](http://openaccess.thecvf.com/content_eccv_2018_workshops/w2/html/Ghafoorian_EL-GAN_Embedding_Loss_Driven_Generative_Adversarial_Networks_for_Lane_Detection_ECCVW_2018_paper.html)
* [Dynamic Adaptation on Non-Stationary Visual Domains](http://openaccess.thecvf.com/content_eccv_2018_workshops/w8/html/Shkodrani_Dynamic_Adaptation_on_Non-Stationary_Visual_Domains_ECCVW_2018_paper.html)

The following users will be answering your questions from 10am-12pm CEST, and from 4pm-6pm CEST:

/u/kmhofmann, /u/pierluigi_tomtom, /u/AlessioColombo, /u/fkariminejadasl, /u/jvvugt, /u/ml_steve

So Ask us Anything!",183,223
355,2019-6-6,2019,6,6,17,bxe70b,"The mostly complete chart of Neural Networks, explained",https://www.reddit.com/r/MachineLearning/comments/bxe70b/the_mostly_complete_chart_of_neural_networks/,EmielSteerneman,1559808864,,0,1
356,2019-6-6,2019,6,6,18,bxempb,AI Replaces Human Appraisers stardate 2019.420,https://www.reddit.com/r/MachineLearning/comments/bxempb/ai_replaces_human_appraisers_stardate_2019420/,bentaylordata,1559812707,,0,1
357,2019-6-6,2019,6,6,19,bxf0d4,Enjoy the Advantages of Advanced Analytics!,https://www.reddit.com/r/MachineLearning/comments/bxf0d4/enjoy_the_advantages_of_advanced_analytics/,ElegantMicroWebIndia,1559815858,,0,1
358,2019-6-6,2019,6,6,19,bxf2w8,Must do Courses at DataCamp for a beginner in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bxf2w8/must_do_courses_at_datacamp_for_a_beginner_in/,swaroop_2000,1559816426,[removed],0,1
359,2019-6-6,2019,6,6,19,bxf4ul,"[Q] Why do some experts say that we should stop comparing ANNs and CNNs to the neural networks of the human brain, while others say that the human brain is the foundation for said networks?",https://www.reddit.com/r/MachineLearning/comments/bxf4ul/q_why_do_some_experts_say_that_we_should_stop/,albertaso,1559816856,"I've read in a couple of articles that we should stop comparing ANNs and CNNs to the neural networks of the human brain. But while reading the the convolutional chapter of deeplearningbook.org there's a section describing how neuroscience is widely used as basis for neural network algorithms.

This confuses me a great deal and I don't know what to think. What are these different experts disagreeing about and how come?",0,1
360,2019-6-6,2019,6,6,19,bxf5jz,Naive Bayes Classifier - ML,https://www.reddit.com/r/MachineLearning/comments/bxf5jz/naive_bayes_classifier_ml/,ranjiraj,1559817006,,0,1
361,2019-6-6,2019,6,6,20,bxffs0,Question on machine learning algorithms for bidding simulation,https://www.reddit.com/r/MachineLearning/comments/bxffs0/question_on_machine_learning_algorithms_for/,dsax7,1559819151,[removed],0,1
362,2019-6-6,2019,6,6,20,bxfrm4,Machine Learning Mobile App Development With Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/bxfrm4/machine_learning_mobile_app_development_with/,haniskaroy,1559821499,,0,1
363,2019-6-6,2019,6,6,20,bxft88,are there some tricks for RNN train?,https://www.reddit.com/r/MachineLearning/comments/bxft88/are_there_some_tricks_for_rnn_train/,googcheng,1559821812,[removed],0,1
364,2019-6-6,2019,6,6,21,bxg7md,[D] Expectations working in a research lab as a new researcher,https://www.reddit.com/r/MachineLearning/comments/bxg7md/d_expectations_working_in_a_research_lab_as_a_new/,Minimum_Zucchini,1559824476,"So I've got a little issue and I'm looking for some advice please.

I'm new as a researcher in the field, I am a masters student with zero publications so far and I've been accepted for an internship at another research lab to work on more theoretical things in ML. My thesis work has been entirely applied, where I worked on some clinical applications of ML, and I was hoping this internship now will help me to broaden my knowledge by working with other experts. 


When I signed up I was asked what my research plan there was, and I gave a vague statement that I am interested in the theme of their research with nothing specific in mind. The issue came about when after I joined the team, I was discussing with some of the post-docs what their own projects were, and one post-doc seemed excited to share his projects and have me help out with various aspects. Then, this other post-doc, who seemed a little annoyed at me, tells me I need to find my own project and not to ask others for a project. Ok.. fine.. I was taken aback honestly, because I don't know what research culture is like, and how people collaborate. From my perspective, there is an insane amount of publications in every niche of ML, so for me as a masters student to just waltz in and be expected to have my own specific idea ready to research it, seems quite silly and a waste of time, but maybe that is the norm? I honestly don't know.

Sorry for the long read, its more of a vent. But my questions are:

1) Is it out of line to join a research team and expect to be given a problem statement or at least some specific ideas?

2) If yes then, how the hell is it possible for someone who has not worked in the field for a long time to come up with an original problem statement? Is this expectation realistic at all??

Anyone with similar experiences, stories, discussion, advice is appreciated!",7,12
365,2019-6-6,2019,6,6,21,bxgduj,Chatbot Development Company,https://www.reddit.com/r/MachineLearning/comments/bxgduj/chatbot_development_company/,clarke2106,1559825604,[removed],0,1
366,2019-6-6,2019,6,6,22,bxgkgj,Can AI make an Art Introduction?,https://www.reddit.com/r/MachineLearning/comments/bxgkgj/can_ai_make_an_art_introduction/,betaaz,1559826734,,0,1
367,2019-6-6,2019,6,6,22,bxgkx3,[D] Besides decaying learning rate and increasing batchsize: Decay momentum? Decay droprate? Increase L2 regularization?,https://www.reddit.com/r/MachineLearning/comments/bxgkx3/d_besides_decaying_learning_rate_and_increasing/,thntk,1559826810,"Decaying learning rate is a popular practice even for adaptive optimizers such as Adam. Increasing batchsize was also shown to have the same effect.  
But there are other hyperparameters with similar nature.  
- Does it make sense to decay/increase them?  
- Have anyone tried decaying momentum, or decaying droprate, or increasing L2 regularization?  
- Are there other hyperparameters that need tuning like this?",13,8
368,2019-6-6,2019,6,6,22,bxgznn,[R] Invertible Residual Networks Paper + Code (ICML19 Long Oral),https://www.reddit.com/r/MachineLearning/comments/bxgznn/r_invertible_residual_networks_paper_code_icml19/,jhjac,1559829284,"**Paper:** [http://proceedings.mlr.press/v97/behrmann19a.html](http://proceedings.mlr.press/v97/behrmann19a.html)  
**Code:** [https://github.com/jhjacobsen/invertible-resnet](https://github.com/jhjacobsen/invertible-resnet)  


Abstract:  
We show that standard ResNet architectures can be made invertible, allowing the same model to be used for classification, density estimation, and generation. Typically, enforcing invertibility requires partitioning dimensions or restricting network architectures. In contrast, our approach only requires adding a simple normalization step during training, already available in standard frameworks. Invertible ResNets define a generative model which can be trained by maximum likelihood on unlabeled data. To compute likelihoods, we introduce a tractable approximation to the Jacobian log-determinant of a residual block. Our empirical evaluation shows that invertible ResNets perform competitively with both state-of-the-art image classifiers and flow-based generative models, something that has not been previously achieved with a single architecture.",0,24
369,2019-6-6,2019,6,6,22,bxgzwh,[N] Rodney Brooks and Gary Marcus launch new startup robust.ai,https://www.reddit.com/r/MachineLearning/comments/bxgzwh/n_rodney_brooks_and_gary_marcus_launch_new/,chisai_mikan,1559829328,"Their new startup robust.ai aims to create a new foundation for the future of robotics.

From their press:

The team behind the new startup, Robust.AI, is firmly in the second camp.

- One co-founder is Gary Marcus, an NYU psychologist and AI expert who carries the banner for scientists who don't believe AI can learn how to navigate through the world without some level of prior knowledge about how it works.

- Another is Rodney Brooks, a legendary MIT roboticist who previously built Rethink Robotics, which sold factory robots meant to work alongside humans. Rethink folded last year.
No robot today can deliver a package all the way to any doorstep, or take care of an elderly person in their home. ""For those kinds of situations, you need robots that can actually think for themselves  robots that can deal with an ever-changing world,"" Marcus says.

- He argues that deep learning  a reigning AI technique that teaches machines patterns without any hard rules  can't do the job on its own.

- ""In order for these machines to reason and operate with more humanlike priors and a deeper understanding of the world, just brute-forcing deep learning is not going to get you there,"" says Peter Barrett, co-founder of VC firm Playground Global, which led the seed-round investment in Robust.AI.
Bringing back ideas from the era of symbolic AI  a focus on ground rules that died out in the 1980s  is a potential way forward, Barrett says. ""I see it as absolutely necessary if we really want to close the gap between the tour de force mechanical capabilities of today's robots and their rather limited intellectual capacities.""

More information: https://www.axios.com/newsletters/axios-future-5f95c639-9837-4b66-96e0-3914c4a114e0.html?chunk=2#story2",10,3
370,2019-6-6,2019,6,6,23,bxh27t,What is the current state of the art approach for unsupervised semantic image segmentation?,https://www.reddit.com/r/MachineLearning/comments/bxh27t/what_is_the_current_state_of_the_art_approach_for/,Statistical_Incline,1559829698,[removed],0,1
371,2019-6-6,2019,6,6,23,bxh30x,[D] What is the current state of the art approach for unsupervised/no ground truth semantic image segmentation? (June 5th 2019),https://www.reddit.com/r/MachineLearning/comments/bxh30x/d_what_is_the_current_state_of_the_art_approach/,probably_likely_mayb,1559829819,,5,2
372,2019-6-6,2019,6,6,23,bxh3d1,Samsung Few-Shot Deepfake AI implementation,https://www.reddit.com/r/MachineLearning/comments/bxh3d1/samsung_fewshot_deepfake_ai_implementation/,MrCaracara,1559829868,[removed],0,1
373,2019-6-6,2019,6,6,23,bxh4pw,Some questions regarding a pc build for machine learning,https://www.reddit.com/r/MachineLearning/comments/bxh4pw/some_questions_regarding_a_pc_build_for_machine/,Asurada11,1559830083,[removed],0,1
374,2019-6-6,2019,6,6,23,bxh8xj,[P] Samsung Talking Heads AI Implementation,https://www.reddit.com/r/MachineLearning/comments/bxh8xj/p_samsung_talking_heads_ai_implementation/,MrCaracara,1559830730,"Hi Reddit,

&amp;#x200B;

Recently, some peeps from the Samsung AI Center in Moscow published the paper [Few-Shot Adversarial Learning of Realistic Neural Talking Head Models](https://arxiv.org/pdf/1905.08233.pdf), which quickly attracted a lot of attention due to the seemingly impressive results it can produce. However, they did not release any official implementation nor trained models, which pushed to create my own, driven by my fascination with this network.

Now, I don't have much experience with Deep Learning myself, and the paper omits a lot of very important details, so I have been struggling to get it to work properly, even if I have been working on it from day 1. Another obstacle is the computing power necessary to run such a model for as long as it is necessary to generate results of the same quality as they show in their demo.

So, I've decided to release my own attempt at an implementation of this model, with the intention of finding other people with the same fascination in this project, so that we could work together and hopefully make it work just as well as in their demo.

So far I've gotten to the point where I have been able to produce results like these:

&amp;#x200B;

But we can still improve it much more!!!

&amp;#x200B;

Here's the link to the repo. Feel free to contribute, improve the algorithm, and upload your own trained models if you make it work! I will keep working hard on it myself!

[https://github.com/grey-eye/talking-heads](https://github.com/grey-eye/talking-heads)",11,16
375,2019-6-6,2019,6,6,23,bxhgok,Double Depth Telescopic Forks - LHD S.p.A.,https://www.reddit.com/r/MachineLearning/comments/bxhgok/double_depth_telescopic_forks_lhd_spa/,lhd121,1559831921,,0,1
376,2019-6-6,2019,6,6,23,bxhh4e,"Company requested a program that takes a collection of 70 songs and creates variations of them, any idea on where to start?",https://www.reddit.com/r/MachineLearning/comments/bxhh4e/company_requested_a_program_that_takes_a/,Nick-Conner,1559831992,[removed],0,1
377,2019-6-6,2019,6,6,23,bxhm95,Saw Google made a new AI assistant. Is this the type of thing that's possible with only LSTM?,https://www.reddit.com/r/MachineLearning/comments/bxhm95/saw_google_made_a_new_ai_assistant_is_this_the/,CJP-2019,1559832758,,0,1
378,2019-6-7,2019,6,7,0,bxhpyk,[P] Generative Neural Visual Artist (GeNeVA) task: recurrent image generation from text,https://www.reddit.com/r/MachineLearning/comments/bxhpyk/p_generative_neural_visual_artist_geneva_task/,gan_man,1559833304,,1,1
379,2019-6-7,2019,6,7,0,bxhymy,- :        ,https://www.reddit.com/r/MachineLearning/comments/bxhymy/_______/,QianaJonesrj3,1559834547,,0,1
380,2019-6-7,2019,6,7,0,bxi8io,Impact of Artificial Intelligence on Image Recognition Techniques,https://www.reddit.com/r/MachineLearning/comments/bxi8io/impact_of_artificial_intelligence_on_image/,S_paddy,1559835970,,0,1
381,2019-6-7,2019,6,7,0,bxic5m,[D] Deepfake swapping Alexios with his Voice Actor's face [Assassin's Creed Odyssey],https://www.reddit.com/r/MachineLearning/comments/bxic5m/d_deepfake_swapping_alexios_with_his_voice_actors/,UploAdore,1559836503,"Deepfakes in gaming sounds like an interesting slippery slope, so I decided to give it my best shot:

[https://youtu.be/isRRyb0xdeA](https://youtu.be/isRRyb0xdeA)

&amp;#x200B;

To what extent do you guys think Deepfakes in the production of face animation is going to change the gaming industry?",4,6
382,2019-6-7,2019,6,7,0,bxicqm,Maths behind training RNN networks,https://www.reddit.com/r/MachineLearning/comments/bxicqm/maths_behind_training_rnn_networks/,prakhar21,1559836592,[removed],0,1
383,2019-6-7,2019,6,7,1,bxihfc,What software engineering skills are required/desired for a ML engineer?,https://www.reddit.com/r/MachineLearning/comments/bxihfc/what_software_engineering_skills_are/,HubbyBear,1559837248,[removed],0,1
384,2019-6-7,2019,6,7,1,bxik0y,[P] Can AI create an Art Introduction?,https://www.reddit.com/r/MachineLearning/comments/bxik0y/p_can_ai_create_an_art_introduction/,betaaz,1559837618," Our take on building an AI to create exhibition statements based on the Museum of Modern Art. 

 [https://medium.com/ymedialabs-innovation/can-ai-make-an-art-introduction-842d20b96b06](https://medium.com/ymedialabs-innovation/can-ai-make-an-art-introduction-842d20b96b06)",0,1
385,2019-6-7,2019,6,7,1,bxinxu,Email Responder in Python,https://www.reddit.com/r/MachineLearning/comments/bxinxu/email_responder_in_python/,n0man4ever,1559838176,[removed],0,1
386,2019-6-7,2019,6,7,1,bxiynn,"[P] ""Replicating GPT21.5B"" in Tensorflow on GCP TPUs --Conor Leahy",https://www.reddit.com/r/MachineLearning/comments/bxiynn/p_replicating_gpt215b_in_tensorflow_on_gcp_tpus/,gwern,1559839693,,0,2
387,2019-6-7,2019,6,7,2,bxj43d,[D] Generative Neural Visual Artist (GeNeVA),https://www.reddit.com/r/MachineLearning/comments/bxj43d/d_generative_neural_visual_artist_geneva/,gan_man,1559840451,"**The Generative Neural Visual Artist (GeNeVA) task**

Link:  [https://www.microsoft.com/en-us/research/project/generative-neural-visual-artist-geneva/](https://www.microsoft.com/en-us/research/project/generative-neural-visual-artist-geneva/)   
Paper:  [https://arxiv.org/abs/1811.09845](https://arxiv.org/abs/1811.09845) 

The GeNeVA task involves a *Teller* giving a sequence of linguistic instructions to a *Drawer* for the ultimate goal of image generation.

The *Teller* is able to gauge progress through visual feedback of the generated image. This is a challenging task because the *Drawer* needs to learn how to map complex linguistic instructions to realistic objects on a canvas, maintaining not only object properties but relationships between objects (e.g., relative location). The *Drawer* also needs to modify the existing drawing in a manner consistent with previous images and instructions, so it needs to remember previous instructions. All of these involve understanding a complex relationship between objects in the scene and how those relationships are expressed in the image in a way that is consistent with all instructions given.

\------------

What thoughts does the community have about generating images conditioned on captions iteratively instead of doing generation in one go? Most papers do not seem to be doing this iterartively but some recent papers have appeared which seems to be a good idea to me.",1,10
388,2019-6-7,2019,6,7,2,bxjbwk,[D] Was OpenAI right not to release GPT-2 fully?,https://www.reddit.com/r/MachineLearning/comments/bxjbwk/d_was_openai_right_not_to_release_gpt2_fully/,Fearless_Share,1559841530,"Now that we've had time to see what can happen with really good generative text models, can we retrospectively say whether OpenAI did the right thing not releasing GPT-2?

&amp;#x200B;

Some recent work based off GPT-2:

[https://www.reddit.com/r/MachineLearning/comments/bv6q2t/defending\_against\_neural\_fake\_news\_check\_out\_the/](https://www.reddit.com/r/MachineLearning/comments/bv6q2t/defending_against_neural_fake_news_check_out_the/)

[https://www.reddit.com/r/SubSimulatorGPT2/comments/bwxrbt/there\_is\_nothing\_wrong\_with\_buying\_a\_used\_car/](https://www.reddit.com/r/SubSimulatorGPT2/comments/bwxrbt/there_is_nothing_wrong_with_buying_a_used_car/)",51,55
389,2019-6-7,2019,6,7,2,bxje4t,[Beginner] Need help in understanding Count Vectorizer,https://www.reddit.com/r/MachineLearning/comments/bxje4t/beginner_need_help_in_understanding_count/,Revanthmk23200,1559841842,[removed],0,1
390,2019-6-7,2019,6,7,2,bxjfl4,[P] Create deep learning models with flowpoints,https://www.reddit.com/r/MachineLearning/comments/bxjfl4/p_create_deep_learning_models_with_flowpoints/,mariusbrataas,1559842051,"&amp;#x200B;

https://i.redd.it/my2ek0j7pr231.png

[Flowpoints](https://mariusbrataas.github.io/flowpoints_ml/?p=9fehu18ra4ty) makes it possible to create deep learning models in a flowchart kind of manner.

&amp;#x200B;

Simply create some nodes, connect them however you like, and copy the automatically written code! Models can be created with either TensorFlow or PyTorch.

&amp;#x200B;

With link sharing it's easy to share models with others, and with a graphical representation of your model it becomes much easier to explain your machine learning model to pretty much anyone:)

&amp;#x200B;

Check out the [readme](https://github.com/mariusbrataas/flowpoints_ml#readme) or [this medium post](https://towardsdatascience.com/create-deep-learning-models-with-flowpoints-d9b675d0e5af) for more info.

&amp;#x200B;

https://i.redd.it/rub1btzrqr231.png

To begin with, I created this tool for my own use. Soon after, I started using it a whole lot for keeping track of model architectures, explaining to project managers and friends how the model worked, and it enabled me to create models waay quicker than I had before.

Now I hope it can be useful for others as well:)

&amp;#x200B;

I've open-sourced this project, and would love some help maintaining the code or adding functionality!",0,4
391,2019-6-7,2019,6,7,2,bxjk4o,Going Beyond GAN? New DeepMind VAE Model Generates High Fidelity Human Faces,https://www.reddit.com/r/MachineLearning/comments/bxjk4o/going_beyond_gan_new_deepmind_vae_model_generates/,Yuqing7,1559842708,,0,1
392,2019-6-7,2019,6,7,2,bxjnyp,Extension of universal approximation theorem to nested deep sets and their cartesian products,https://www.reddit.com/r/MachineLearning/comments/bxjnyp/extension_of_universal_approximation_theorem_to/,pevnak,1559843236,[removed],0,1
393,2019-6-7,2019,6,7,4,bxkq6u,TF-IDF for tabular data featurization &amp; classification,https://www.reddit.com/r/MachineLearning/comments/bxkq6u/tfidf_for_tabular_data_featurization/,i_eat_pasta_with_jam,1559848583,[removed],0,1
394,2019-6-7,2019,6,7,4,bxkycc,"Running classifiers with train, test and validation sets",https://www.reddit.com/r/MachineLearning/comments/bxkycc/running_classifiers_with_train_test_and/,BubblyResponsibility,1559849731,[removed],0,1
395,2019-6-7,2019,6,7,4,bxkzbe,[Question] BERT embedding - how to measure similarity?,https://www.reddit.com/r/MachineLearning/comments/bxkzbe/question_bert_embedding_how_to_measure_similarity/,Fredbull,1559849859,[removed],0,1
396,2019-6-7,2019,6,7,4,bxl6kl,[P] Creating an AI algorithm that can play Battleship using neural networks,https://www.reddit.com/r/MachineLearning/comments/bxl6kl/p_creating_an_ai_algorithm_that_can_play/,I_am_roundar,1559850901,"Hi,

&amp;#x200B;

I am a student who is researching the topic of neural networks. I am posting a Google Forms survey containing 12 videos, each containing a game being played by the neural network versus the human player in Battleship, and require your help in this project.

&amp;#x200B;

Please observe the video and answer which board (A or B) is being played by the AI for each of the 12 games.

&amp;#x200B;

Link: [https://docs.google.com/forms/d/e/1FAIpQLSfvQWt2RIu1mfq62OP9X\_ieM8dlwoPsuBu-exgW4rPrGdyWmg/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLSfvQWt2RIu1mfq62OP9X_ieM8dlwoPsuBu-exgW4rPrGdyWmg/viewform?usp=sf_link)",0,1
397,2019-6-7,2019,6,7,5,bxlaxd,[P] Creating an AI algorithm that can play Battleship using neural networks (Reupload),https://www.reddit.com/r/MachineLearning/comments/bxlaxd/p_creating_an_ai_algorithm_that_can_play/,I_am_roundar,1559851500,"Hi,

&amp;#x200B;

I am a student who is researching the topic of neural networks. I am posting a Google Forms survey containing 12 videos, each containing a game being played by the neural network versus the human player in Battleship, and require your help in this project.

&amp;#x200B;

Please observe the videos and answer which board (A or B) is being played by the AI for each of the 12 games.

&amp;#x200B;

Link: [https://docs.google.com/forms/d/e/1FAIpQLSfvQWt2RIu1mfq62OP9X\_ieM8dlwoPsuBu-exgW4rPrGdyWmg/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLSfvQWt2RIu1mfq62OP9X_ieM8dlwoPsuBu-exgW4rPrGdyWmg/viewform?usp=sf_link)",5,10
398,2019-6-7,2019,6,7,5,bxlndw,DeepMind AI Reaches Human-level Performance in Quake III Arena,https://www.reddit.com/r/MachineLearning/comments/bxlndw/deepmind_ai_reaches_humanlevel_performance_in/,Yuqing7,1559853363,,0,1
399,2019-6-7,2019,6,7,5,bxlsdv,I'm leaving my job at Facebook to create a collaboration platform for Data Science &amp; ML,https://www.reddit.com/r/MachineLearning/comments/bxlsdv/im_leaving_my_job_at_facebook_to_create_a/,afriggeri,1559854093,,0,1
400,2019-6-7,2019,6,7,6,bxm2n2,Is it possible to generate new 3d models inspired by existing 3d models? What kind of technologies do I need to look at to achieve this?,https://www.reddit.com/r/MachineLearning/comments/bxm2n2/is_it_possible_to_generate_new_3d_models_inspired/,yoloswek,1559855581,"I want to try to generate new designs for furniture, which are inspired by existing furniture which I'll feed to the neural network. Something like the following video: [https://www.youtube.com/watch?v=M22NyJW8Hfo](https://www.youtube.com/watch?v=M22NyJW8Hfo)",0,1
401,2019-6-7,2019,6,7,6,bxmfi8,"Response from the publisher of ""Facial feature discovery for ethnicity recognition"" paper targeting Uyghur, Tibetan, and Korean ethnic minorities.",https://www.reddit.com/r/MachineLearning/comments/bxmfi8/response_from_the_publisher_of_facial_feature/,Isinlor,1559857422,,0,1
402,2019-6-7,2019,6,7,7,bxmw9y,What happened to Andrew Ng's company?,https://www.reddit.com/r/MachineLearning/comments/bxmw9y/what_happened_to_andrew_ngs_company/,aviniumau,1559859874,"Last I recall, Andrew Ng left Baidu to start a company in CV (self-driving cars?) - but the name escapes me and I can't find any more information. This must have been a couple of years ago.

Anyone know if this went anywhere, or did it just die on the vine?",0,1
403,2019-6-7,2019,6,7,7,bxn5lb,Towards Lossless Encoding of Sentences,https://www.reddit.com/r/MachineLearning/comments/bxn5lb/towards_lossless_encoding_of_sentences/,gabprato,1559861353,[removed],0,1
404,2019-6-7,2019,6,7,8,bxnp9s,[D] Having trouble with Deep Q-learning on the OpenAI Gym Lunar Lander.,https://www.reddit.com/r/MachineLearning/comments/bxnp9s/d_having_trouble_with_deep_qlearning_on_the/,Buttons840,1559864542,"A few months ago I spent some time trying to learn deep reinforcement learning, and became obsessed with the [OpenAI Gym Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) environment. I ended up doing KNN on memory (as in, ""memory replay""), and I got some intelligent behavior out of the lander, but it was far from perfect (and yes, I know KNN is not ""deep learning"", but I used what I understood). Recently I took another shot at it using deep Q-learning with neural networks, but I'm having even less success than before.

I wanted to ask about, what I believe to be, a major contributor to my difficulties.

In Deep Q-Learning you have a neural networks that will be used to approximate `Q(s, a)` which is the value of action `a` in state `s`, or rather, the value you can expect to obtain in the long run after taking action `a` in state `s`. We can also say that the value of a state is `V(s) = Q(s, maximizing_a)`, meaning the value of a state is the value of choosing the optimal action in that state.

As far as I understand, deep Q-learning revolves around the `Q(s, a) = r + V(s')` equation, meaning the long term expected value of a state and action are simply the immediate reward, plus the expected value of the next state. In deep Q-learning you basically turn this equation into training data and train your neural network on it.

**My problem is** that `Q` predicts action values like this: `Float32[-32.5629, -32.8037, -32.6016, -32.5938]` (There are 4 possible actions at each step in the lunar lander environment.)

`Q` seems to understand \[correctly\] that it really doesn't matter much what I do for a single step in the lander environment. A single action barely changes the situation at all, so the expected values of all actions are very very close. **I don't think my neural network is able to make such a subtle distinction between which action is best, because any single action has a very very small effect.** This is especially troublesome because, again, `Q` is correct, there really isn't much difference between the actions at a single step, so I can't just ""fix"" `Q`, because it's already working, but it will never be 100% correct since it's just a function *approximator*.

So what do I do? Do I throw more parameters at it? Do I train it another way? Any suggestions?",16,9
405,2019-6-7,2019,6,7,8,bxnpqy,[D] How I increased Tensorflow's speed by over 100x with 1 line of code,https://www.reddit.com/r/MachineLearning/comments/bxnpqy/d_how_i_increased_tensorflows_speed_by_over_100x/,__BetterAgent__,1559864624,"put use_multiprocessing=True, workers=1000 in kera's fit_generator. I am surprised a lot of people don't know about this",4,0
406,2019-6-7,2019,6,7,9,bxoatv,[R] Performing Streaming Speech Recognition on LOCAL FILE with IBM Watson,https://www.reddit.com/r/MachineLearning/comments/bxoatv/r_performing_streaming_speech_recognition_on/,soominjung,1559868179,"For comparison test of the performance of streaming STT from various companies, I had to make sure I input the same audio data each time.
Thus, I modified watson-streaming-stt (with mic) to work on LOCAL FILE.

In case anyone of you wants to do the same comparison test, I post the link here.
I will update codes and README to make it easy to use and understand soon.


[Modified](https://github.com/soominjung/watson-streaming-localfile-stt)
[Original](https://github.com/ibm-dev/watson-streaming-stt)",4,44
407,2019-6-7,2019,6,7,10,bxokh3,"Positive Externalities (Stories of Labelers, from Scale)",https://www.reddit.com/r/MachineLearning/comments/bxokh3/positive_externalities_stories_of_labelers_from/,shariq_scale,1559869836,,0,1
408,2019-6-7,2019,6,7,10,bxotix,[R] Limitations of the Empirical Fisher Approximation,https://www.reddit.com/r/MachineLearning/comments/bxotix/r_limitations_of_the_empirical_fisher/,iidealized,1559871395,"Paper: [https://arxiv.org/abs/1905.12558](https://arxiv.org/abs/1905.12558)

Blog-post: [https://www.inference.vc/on-empirical-fisher-information/](https://www.inference.vc/on-empirical-fisher-information/)

TLDR: empirical Fisher information may be so wrong it can be useless in practice.",6,68
409,2019-6-7,2019,6,7,10,bxoumi,[R] On Value Functions and the Agent-Environment Boundary (DeepRL built on shaky foundations),https://www.reddit.com/r/MachineLearning/comments/bxoumi/r_on_value_functions_and_the_agentenvironment/,baylearn,1559871599,,7,23
410,2019-6-7,2019,6,7,10,bxozxo,[R] DeepMDP: Learning Continuous Latent Space Models for Representation Learning,https://www.reddit.com/r/MachineLearning/comments/bxozxo/r_deepmdp_learning_continuous_latent_space_models/,hardmaru,1559872549,,1,5
411,2019-6-7,2019,6,7,11,bxp62n,Speech recognition API beyond words,https://www.reddit.com/r/MachineLearning/comments/bxp62n/speech_recognition_api_beyond_words/,Nott_taken,1559873609,[removed],0,1
412,2019-6-7,2019,6,7,11,bxpd7m,[D] What is the best metric to evaluate a model?,https://www.reddit.com/r/MachineLearning/comments/bxpd7m/d_what_is_the_best_metric_to_evaluate_a_model/,__BetterAgent__,1559874873,,6,0
413,2019-6-7,2019,6,7,11,bxpjmm,New to ML. What algorithms/tools do I best for this search problem?,https://www.reddit.com/r/MachineLearning/comments/bxpjmm/new_to_ml_what_algorithmstools_do_i_best_for_this/,memetologizt,1559875968,[removed],0,1
414,2019-6-7,2019,6,7,11,bxpk19,This new paper shows how to use #machinelearning to steal pins and passwords using only the sound you make when typing them on your phone or tablet.,https://www.reddit.com/r/MachineLearning/comments/bxpk19/this_new_paper_shows_how_to_use_machinelearning/,ai-lover,1559876036,[removed],0,1
415,2019-6-7,2019,6,7,12,bxq1rs,My dog.,https://www.reddit.com/r/MachineLearning/comments/bxq1rs/my_dog/,lazycnt,1559879206,,0,1
416,2019-6-7,2019,6,7,13,bxqoa6,Introduction to Genetic Algorithms,https://www.reddit.com/r/MachineLearning/comments/bxqoa6/introduction_to_genetic_algorithms/,atomlib_com,1559883477,,0,1
417,2019-6-7,2019,6,7,14,bxquxk,Feature Selection Techniques in Regression Model,https://www.reddit.com/r/MachineLearning/comments/bxquxk/feature_selection_techniques_in_regression_model/,chetnatripathi19,1559884846,,0,1
418,2019-6-7,2019,6,7,14,bxr112,Stanford Image-Paragraph-Captioning Dataset Confusion,https://www.reddit.com/r/MachineLearning/comments/bxr112/stanford_imageparagraphcaptioning_dataset/,arjundupa,1559886089,[removed],0,1
419,2019-6-7,2019,6,7,14,bxr2yy,How can we use AI to Predict the Stock Market?  Interview with Data Science Researcher Oscar Javier Hernandez,https://www.reddit.com/r/MachineLearning/comments/bxr2yy/how_can_we_use_ai_to_predict_the_stock_market/,LimarcAmbalina,1559886485,[removed],0,1
420,2019-6-7,2019,6,7,14,bxr3yb,Intec 2019 Invitation - Ultramax Hydrojet,https://www.reddit.com/r/MachineLearning/comments/bxr3yb/intec_2019_invitation_ultramax_hydrojet/,Ultramaxhydrojet,1559886677,,0,1
421,2019-6-7,2019,6,7,14,bxr5ww,Federated Learning,https://www.reddit.com/r/MachineLearning/comments/bxr5ww/federated_learning/,nasser_holovac,1559887061,,0,1
422,2019-6-7,2019,6,7,15,bxr8t0,How do I start learning about using AI in the field of Robotics ?,https://www.reddit.com/r/MachineLearning/comments/bxr8t0/how_do_i_start_learning_about_using_ai_in_the/,masterRJ2404,1559888926,[removed],0,1
423,2019-6-7,2019,6,7,16,bxroe7,Google Sets Aside $2.6 Billion for the Purchase of Analytics Software Company Looker,https://www.reddit.com/r/MachineLearning/comments/bxroe7/google_sets_aside_26_billion_for_the_purchase_of/,cryptokunbo,1559892412,,0,1
424,2019-6-7,2019,6,7,18,bxsen7,Call for collaboration in horse racing,https://www.reddit.com/r/MachineLearning/comments/bxsen7/call_for_collaboration_in_horse_racing/,agataich,1559898730,[removed],0,1
425,2019-6-7,2019,6,7,18,bxsnf4,Lets discuss about some technologies that business owners must utilize to leverage their profits and reach!,https://www.reddit.com/r/MachineLearning/comments/bxsnf4/lets_discuss_about_some_technologies_that/,AppcodeTechnologies,1559900726,[removed],0,1
426,2019-6-7,2019,6,7,19,bxsrvd,[Beginner] Game Bots,https://www.reddit.com/r/MachineLearning/comments/bxsrvd/beginner_game_bots/,Revanthmk23200,1559901666,[removed],0,1
427,2019-6-7,2019,6,7,20,bxtiui,Papers/Works on scoring authentication,https://www.reddit.com/r/MachineLearning/comments/bxtiui/papersworks_on_scoring_authentication/,chandiramouli,1559907159,"Suggestions to calculate a risk score associated with user login based on the features extracted from previous logins  including location,ip,logintime etc.",0,1
428,2019-6-7,2019,6,7,20,bxtnmr,[D] (on-policy) exploration when adding new actions,https://www.reddit.com/r/MachineLearning/comments/bxtnmr/d_onpolicy_exploration_when_adding_new_actions/,so_tiredso_tired,1559908095,"I am using policy gradient DRL with on-policy exploration in a discrete domain. 

&amp;#x200B;

After some-time, with significant exploration, with a decent network performance, I have to handle newly discovered actions. I  can ""widen"" and initialize the network to handle these actions. 

&amp;#x200B;

is there recommendation for increasing the exploration rate, and specifically  ""over-exploring"" these new actions?

The data domain itself is structured/tabular/wide.",2,8
429,2019-6-7,2019,6,7,21,bxtvbr,Data Engineering 101 resources,https://www.reddit.com/r/MachineLearning/comments/bxtvbr/data_engineering_101_resources/,bubble_chart,1559909485,[removed],0,1
430,2019-6-7,2019,6,7,22,bxud9t,Telescopic Forks from L.H.D S.p.A,https://www.reddit.com/r/MachineLearning/comments/bxud9t/telescopic_forks_from_lhd_spa/,lhd121,1559912618,[removed],0,1
431,2019-6-7,2019,6,7,22,bxulah,A good IB Extended Essay topic on ML,https://www.reddit.com/r/MachineLearning/comments/bxulah/a_good_ib_extended_essay_topic_on_ml/,lollocat3,1559913941,"Hi everyone, 

&amp;#x200B;

I would like to write my IB Extended Essay on machine (or deep) learning, but I still haven't decided exactly what I would like to explore. 

Ideally, the research question should be in the form 'To what extent does A compare to B when trying to do C?'. I thought about comparing different learning algorithms (stochastic gradient descent against L-BFGS, for instance) or alternative gradient computation algorithms (like backprop) but still am unconvinced. 

Any advice on possible topics and tasks to perform would be greatly appreciated.",0,1
432,2019-6-7,2019,6,7,22,bxusnh,Teaching Alexa to Follow Conversations : Alexa Blogs,https://www.reddit.com/r/MachineLearning/comments/bxusnh/teaching_alexa_to_follow_conversations_alexa_blogs/,georgecarlyle76,1559915140,,0,1
433,2019-6-7,2019,6,7,22,bxuw77,I wonder what you think of this?,https://www.reddit.com/r/MachineLearning/comments/bxuw77/i_wonder_what_you_think_of_this/,Jirokoh,1559915716,,0,1
434,2019-6-7,2019,6,7,23,bxv249,Avoid overfitting with feature importance boosting,https://www.reddit.com/r/MachineLearning/comments/bxv249/avoid_overfitting_with_feature_importance_boosting/,OnlySquareCookies,1559916625,[removed],0,1
435,2019-6-7,2019,6,7,23,bxv35x,[P] Yet another pytorch implementation of neural style transfer,https://www.reddit.com/r/MachineLearning/comments/bxv35x/p_yet_another_pytorch_implementation_of_neural/,cztheday25,1559916779,,0,2
436,2019-6-7,2019,6,7,23,bxvgis,[D] I am in my final year of Computer Engineering course from University of Mumbai.,https://www.reddit.com/r/MachineLearning/comments/bxvgis/d_i_am_in_my_final_year_of_computer_engineering/,JatJaw,1559918822,"We need to complete a project by the end of the academic year on any topic. Being interested in ML, I want to do something related to it. Could you guys suggest some interesting topics, please?",4,0
437,2019-6-7,2019,6,7,23,bxvh7q,[D] Training a single AI model can emit as much carbon as five cars in their lifetimes,https://www.reddit.com/r/MachineLearning/comments/bxvh7q/d_training_a_single_ai_model_can_emit_as_much/,EmielSteerneman,1559918925,"Do you think about the environment when training your models? If so, how?

&amp;#x200B;

[https://www.technologyreview.com/s/613630/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes](https://www.technologyreview.com/s/613630/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes)",145,235
438,2019-6-7,2019,6,7,23,bxvkdm,REGRESIN LINEAL COMO TCNICA DEL MACHINE LEARNING,https://www.reddit.com/r/MachineLearning/comments/bxvkdm/regresin_lineal_como_tcnica_del_machine_learning/,jeffry_30,1559919417,[removed],0,1
439,2019-6-8,2019,6,8,0,bxvtwh,"NAIVE BAYES, UNA HERRAMIENTA PARA EL APRENDIZAJE AUTOMTICO",https://www.reddit.com/r/MachineLearning/comments/bxvtwh/naive_bayes_una_herramienta_para_el_aprendizaje/,jeffry_30,1559920762,[removed],0,1
440,2019-6-8,2019,6,8,0,bxw5en,[Project] Yet another PyTorch implementation of Neural Style Transfer,https://www.reddit.com/r/MachineLearning/comments/bxw5en/project_yet_another_pytorch_implementation_of/,cztheday25,1559922427,,0,1
441,2019-6-8,2019,6,8,0,bxw66q,[D] Is it OK to arxiv my own paper if I've only tested on MNIST?,https://www.reddit.com/r/MachineLearning/comments/bxw66q/d_is_it_ok_to_arxiv_my_own_paper_if_ive_only/,needausername333,1559922542,"Hello. I have a novel method that I would like to write a paper about. I have extensively tested the method on a synthetic dataset and MNIST and the results are statistically significant. Due to circumstances, I would have to wait for a nearly two months before my new hardware come so I can test on bigger datasets. In the mean time, I'm afraid that somebody else might publish the same thing on arxiv. Would it be ethical to put the version with just MNIST experiments onto arxiv?",19,9
442,2019-6-8,2019,6,8,1,bxwbqf,What happened to M?,https://www.reddit.com/r/MachineLearning/comments/bxwbqf/what_happened_to_m/,josne,1559923342,"A few years ago, Facebook Messenger launched a bot, M, in beta. Some aspect of M's behaviour (inconsistency, typos, delays in responses, handling of impressively complex requests) caused the community to speculate about it being partially human-powered, and launched for the purpose of data collection. That's a bold, if expensive, approach.

It's been a while now since M was shut down, - do we know anything about how that experiment went?",0,1
443,2019-6-8,2019,6,8,1,bxwme7,[P] Computer Science Summarization Dataset,https://www.reddit.com/r/MachineLearning/comments/bxwme7/p_computer_science_summarization_dataset/,BatmantoshReturns,1559924862,"This is a dataset of 5.6 million title / abstract data points, about 75% of which are from computer science papers (I tried my best to filter all non-CS papers (perhaps the non-CS papers add a bit of a ""regularization"" effect . . . ?) ) . 

Title/Abstract pairs have been used to train biomedical summarizers [https://arxiv.org/pdf/1804.08875.pdf] , but I am doing a project on CS/ML papers so I made my own. 

The dataset is basically a filtered version of the Semantic Scholar Corpus https://api.semanticscholar.org/corpus/

But it took some effort to produce it and I figure I may save some people time if they wanted the same. 

This is a zip file containing 12 parquet files
https://drive.google.com/open?id=1WEdf-_au3vg2EzmWhawmW9xsYaHAE7iV
it's ~2.5 gb zipped, I think like 6 something gigs unzipped

This is the sqlite database version, 1 file
https://drive.google.com/open?id=1IhIaBD98BEseteAUi1S_f_SfIaUI8V4D
it's 2.5 gb zipped, 7.5 gb unzipped

If anyone is interested, this a part of an ongoing project to use deep learning models to better search through research papers, started with ML/CS papers. If anyone is interested in being involved, feel free to reach out. We also have a public page if anyone wants to keep updated.

https://github.com/Santosh-Gupta/Arxiv-Manatee-PublicUpdates

https://snag.gy/cwnUGB.jpg",0,12
444,2019-6-8,2019,6,8,1,bxwv20,[R] Stanford AI system can change what people said.,https://www.reddit.com/r/MachineLearning/comments/bxwv20/r_stanford_ai_system_can_change_what_people_said/,NicoleK1993,1559926107,,0,1
445,2019-6-8,2019,6,8,2,bxx241,Introducing Google Research Football: A Novel Reinforcement Learning Environment,https://www.reddit.com/r/MachineLearning/comments/bxx241/introducing_google_research_football_a_novel/,sjoerdapp,1559927107,,0,1
446,2019-6-8,2019,6,8,2,bxx2y0,Tensorflow 2.0.0-beta Released,https://www.reddit.com/r/MachineLearning/comments/bxx2y0/tensorflow_200beta_released/,tensorflower,1559927227,[removed],0,1
447,2019-6-8,2019,6,8,2,bxx3h5,[N] TensorFlow 2.0.0-beta0 Release,https://www.reddit.com/r/MachineLearning/comments/bxx3h5/n_tensorflow_200beta0_release/,tensorflower,1559927297,"https://github.com/tensorflow/tensorflow/releases/tag/v2.0.0-beta0

RIP tf.contrib.

Has anyone successfully migrated from 'traditional' tf to the new Keras namespaces? Is it worth it?",13,31
448,2019-6-8,2019,6,8,2,bxx8oa,VisualData: A Search Engine for Computer Vision Datasets,https://www.reddit.com/r/MachineLearning/comments/bxx8oa/visualdata_a_search_engine_for_computer_vision/,Yuqing7,1559928044,,0,1
449,2019-6-8,2019,6,8,2,bxx9j7,"Head of Google AI, Jeff Dean, lays out his vision for the future of machine learning",https://www.reddit.com/r/MachineLearning/comments/bxx9j7/head_of_google_ai_jeff_dean_lays_out_his_vision/,hiphipj0rge,1559928174,,0,1
450,2019-6-8,2019,6,8,2,bxxecl,[P] Writing a Backend Compiler for PyTorch,https://www.reddit.com/r/MachineLearning/comments/bxxecl/p_writing_a_backend_compiler_for_pytorch/,bwasti,1559928886,,0,1
451,2019-6-8,2019,6,8,2,bxxhrd,"Looking for video (or article?) about theory, that basic learning unit of cortex is not 1 neuron, but group of pyramidal neurons. I swear it was there somewhere...",https://www.reddit.com/r/MachineLearning/comments/bxxhrd/looking_for_video_or_article_about_theory_that/,existentialcarrot,1559929392,[removed],0,1
452,2019-6-8,2019,6,8,2,bxxlme,What are the advantages/disadvantages of the XGBoost Classification model?,https://www.reddit.com/r/MachineLearning/comments/bxxlme/what_are_the_advantagesdisadvantages_of_the/,ManHuman,1559929867,,0,1
453,2019-6-8,2019,6,8,3,bxxpuq,Program Synthesis Meets Machine Learning - Talks at MSR,https://www.reddit.com/r/MachineLearning/comments/bxxpuq/program_synthesis_meets_machine_learning_talks_at/,EveryDay-NormalGuy,1559930471,,0,1
454,2019-6-8,2019,6,8,3,bxy1da,[R] MNIST-C: A Robustness Benchmark for Computer Vision,https://www.reddit.com/r/MachineLearning/comments/bxy1da/r_mnistc_a_robustness_benchmark_for_computer/,normanmu,1559932085,"We apply simple corruptions to MNIST to create a new dataset for the purpose of measuring non-adversarial robustness in computer vision models. We then evaluate various models on MNIST-C and find that a simple CNN outperforms various adversarial defenses and alternative architectures by a wide margin.

&amp;#x200B;

Paper: [https://arxiv.org/abs/1906.02337](https://arxiv.org/abs/1906.02337)

Code: [https://github.com/google-research/mnist-c](https://github.com/google-research/mnist-c)",5,13
455,2019-6-8,2019,6,8,3,bxydk2,Help in CNN,https://www.reddit.com/r/MachineLearning/comments/bxydk2/help_in_cnn/,karima1999,1559933839,[removed],0,1
456,2019-6-8,2019,6,8,4,bxylqm,[N] TensorFlow 2.0 (Beta) is Here!,https://www.reddit.com/r/MachineLearning/comments/bxylqm/n_tensorflow_20_beta_is_here/,iyaja,1559934984,"TensorFlow 2.0 Beta is officially out. Here are a few links of relevance:

* [Official documentation](https://www.tensorflow.org/beta)
* [Official announcement on Twitter](https://twitter.com/TensorFlow/status/1137036975893037056)
* [Official demo notebook](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/keras/basic_classification.ipynb)
* [Official announcement article on Medium](https://medium.com/tensorflow/announcing-tensorflow-2-0-beta-abb24bbfbe3d)

&amp;#x200B;

Also, I received a notification via email. Here's one section of it:

&gt;What's new in beta?  
&gt;  
&gt;In this beta release you'll find a final API surface, also available as part of the v2 compatibility module inside the [TensorFlow1.14release](https://www.google.com/appserve/mkt/p/AFnwnKUVEjyg64gUhpACfyB68g9HFnHC5OdT9coHKPzTiDgu9fs7chpRuzjQTJtDh0VHA6t1_cyoPp3N_j4z748PknT2g2wnEkt09GcD6ub_GAP3J3TLSM8YxGZsy__RItNxwrnU8f0BQzucZVIsKFWF1A). We have also added 2.0 support for Keras features like model subclassing, simplified the API for custom training loops, added [distributionstrategy](https://www.google.com/appserve/mkt/p/AFnwnKWa-RZ1_Jx2ySZELbk5S3oHGjXvmkEbKw1_mk0ldCzHcCGf_-pRE6wNGZtKF-w6_z4Tlmrk1EECDHOLNjy-HfKgYYNb9qjQ-FSmQ7ZwzEk1-y6wOEFQMxG9tswkBJmAelwovK19XPqC) support for most kinds of hardware, and lots more. You can see a list of all symbol changes [here](https://www.google.com/appserve/mkt/p/AFnwnKWXX1hF_6McU3EDtno7oU-_KlckUD9Iw4i25XrcxkcdAoiYMEgiAdtmWuYC4hxW83fICVK2UiT5JGH-bTWOxCQeDQrgq-tId3359KVkUdBa7pHPNbtijQiof8KkC4Gh-ehn8ueG8Ry27xykHBSl7XixDIMoneqfHXoXSRcB2N8), and check out the link below for a collection of tutorials and getting startedguides.  
&gt;  
&gt;  
&gt;  
&gt;[See documentation](https://www.google.com/appserve/mkt/p/AFnwnKUGhbe99UXrUPTCSzjpKft4pCr3rSkVXrQuE-8D7TswneoRUTLxnb8c2ssyi-YyWwl1DW4lSqp-uwMCCqBTiGO8rpx6eHjrr6pmchyXIaA)",0,8
457,2019-6-8,2019,6,8,4,bxyqki,"Language, trees, and geometry in neural networks [R]",https://www.reddit.com/r/MachineLearning/comments/bxyqki/language_trees_and_geometry_in_neural_networks_r/,1wheel,1559935692,,0,1
458,2019-6-8,2019,6,8,5,bxzc6i,Google Announces TensorFlow 2.0 Beta,https://www.reddit.com/r/MachineLearning/comments/bxzc6i/google_announces_tensorflow_20_beta/,Yuqing7,1559938841,,0,1
459,2019-6-8,2019,6,8,5,bxzero,StyleGAN on TPUS?,https://www.reddit.com/r/MachineLearning/comments/bxzero/stylegan_on_tpus/,alvisanovari,1559939238,[removed],0,1
460,2019-6-8,2019,6,8,5,bxzmuu,[D] DeepTweets: Generating fake tweets with GPT-2 fine-tuned on individual twitter accounts,https://www.reddit.com/r/MachineLearning/comments/bxzmuu/d_deeptweets_generating_fake_tweets_with_gpt2/,UltraMarathonMan,1559940432,"I fine-tuned the [GPT-2 language model](https://github.com/openai/gpt-2) (345 million parameters) on tweets from peoples Twitter accounts to create AI versions of them, and then had the bots rewrite real tweets. Ive generated hundreds of the following images and picked my favorites. All are surprisingly representative of their corresponding real Twitter accounts. Some are funny, some are profound, some are dark in a way that gives me pause. Heres a real [tweet about tunnels from Elon Musk](https://twitter.com/elonmusk/status/1132011864949383168) rewritten by AI versions of Justin Bieber, Kanye West, and Katy Perry: 

https://i.redd.it/h2d7o3vnxz231.png

So far Ive trained AI versions of the following people (listed below). If you have more suggestions for who/what you would like to see, let me know. Ill release the models, code, and more tweet bot rewrites and conversations when I have time on [https://lexfridman.com/deeptweets/](https://lexfridman.com/deeptweets/). Everything together took \~4 hours of programming time and \~2 weeks neural network training time. List of fine-tuned language models (in alphabetical order) Ive trained so far:

Barack Obama  
Bernie Sanders  
Conan OBrien  
Deepak Chopra  
Donald Trump  
Dwayne The Rock Johnson  
Ellen DeGeneres  
Elon Musk  
Hillary Clinton  
Jimmy Fallon  
Joe Rogan  
Jordan Peterson  
Justin Bieber  
Kanye West  
Katy Perry  
Kevin Hart  
Lex Fridman  
Neil deGrasse Tyson  
Richard Dawkins  
Ricky Gervais  
Sam Harris",4,3
461,2019-6-8,2019,6,8,6,bxztxs,"Accelerating Endpoint Inferencing: Machine learning, with the correct hardware infrastructure, may soon reach endpoints",https://www.reddit.com/r/MachineLearning/comments/bxztxs/accelerating_endpoint_inferencing_machine/,Chipdoc,1559941486,,0,1
462,2019-6-8,2019,6,8,6,bxzug7,AI Saved Life during Emerge Conference '19,https://www.reddit.com/r/MachineLearning/comments/bxzug7/ai_saved_life_during_emerge_conference_19/,nya-yo,1559941564,[removed],0,1
463,2019-6-8,2019,6,8,6,by0148,Innovation Competition - bring some intelligence to the TBM aircraft - MANY REWARDS,https://www.reddit.com/r/MachineLearning/comments/by0148/innovation_competition_bring_some_intelligence_to/,TheGeneralAviathon,1559942565,[removed],0,1
464,2019-6-8,2019,6,8,7,by0rbj,[R] Text-based Editing of Talking-head Video,https://www.reddit.com/r/MachineLearning/comments/by0rbj/r_textbased_editing_of_talkinghead_video/,hanyuqn,1559946552,"Paper: https://www.ohadf.com/projects/text-based-editing/data/text-based-editing.pdf

Video: https://www.youtube.com/watch?v=0ybLCfVeFL4

Project page: https://www.ohadf.com/projects/text-based-editing/",3,12
465,2019-6-8,2019,6,8,7,by0ski,AI-Enabled Embedded Systems / EfficientNet: Compound Model Scaling and More,https://www.reddit.com/r/MachineLearning/comments/by0ski/aienabled_embedded_systems_efficientnet_compound/,alghar,1559946746,[removed],0,1
466,2019-6-8,2019,6,8,7,by1136,"Machine Learning Use Case in Google, Facebook, Amazon, Microsoft, Kaggle, General Electric, and Cornerstone By Bernard Maar",https://www.reddit.com/r/MachineLearning/comments/by1136/machine_learning_use_case_in_google_facebook/,ai-lover,1559948129,[removed],0,1
467,2019-6-8,2019,6,8,8,by1fxi,[D] Anyone try to implement Ganfit?,https://www.reddit.com/r/MachineLearning/comments/by1fxi/d_anyone_try_to_implement_ganfit/,away1023912094,1559950560,"This paper [https://github.com/barisgecer/GANFit](https://github.com/barisgecer/GANFit) has been out for a couple months now and is very interesting, was wondering if anyone tried to implement their own version and what types of results they have been getting?",0,3
468,2019-6-8,2019,6,8,8,by1n1f,[D] How does the typical day differ from academic research vs research at a company?,https://www.reddit.com/r/MachineLearning/comments/by1n1f/d_how_does_the_typical_day_differ_from_academic/,dragoph,1559951786,"For those of you who have had some experience in both academia and industry as researchers, what are some of the differences between academia vs industry, some of the different responsibilities, time investments, freedom to do the research you want, pros and cons of one or the other, day in a life like, etc",29,123
469,2019-6-8,2019,6,8,8,by1n9a,Paper highlights for ICML?,https://www.reddit.com/r/MachineLearning/comments/by1n9a/paper_highlights_for_icml/,iamquah,1559951828,[removed],0,1
470,2019-6-8,2019,6,8,9,by1p5r,GPT paper disappointingly simple,https://www.reddit.com/r/MachineLearning/comments/by1p5r/gpt_paper_disappointingly_simple/,canttouchmypingas,1559952160,[removed],0,1
471,2019-6-8,2019,6,8,9,by1sok,Yolo hand gesture recognition dataset,https://www.reddit.com/r/MachineLearning/comments/by1sok/yolo_hand_gesture_recognition_dataset/,RumboYT,1559952782,[removed],0,1
472,2019-6-8,2019,6,8,9,by1sor,[D] GPT paper disappointingly simple,https://www.reddit.com/r/MachineLearning/comments/by1sor/d_gpt_paper_disappointingly_simple/,canttouchmypingas,1559952783,"Not in a bad way, of course. I don't know how else to put it. I read through the whole thing and cross referenced their old models and techniques they cited, and while there's a lot of preliminary preprocessing and clever things going on in terms of how the text is compressed, the actual machine learning model, all they did was rearrange where they did layer normalization and added an extra one. I got curious about how the attention function was formulated in the 'Attention is all you need' and it reminded me of looking at an lstm, just a seemingly random sequence of matrix transformations.

Part of my intuition is just telling me ""why don't you just collect a set of 30 or so symbols representing all the functions that can be performed on the input as its currently formatted at this step, randomly pick a few, and see what happens?"" Surely a lot of these models can be produced, trained to a small amount of epochs, and compared against each other. Perhaps a better method of doing scalar dot product attention can be attained like this, especially considering its only 4 operations. This paragraph is more of a ramble, because I haven't been able to get this feeling out of my mind as I read some papers and look at these models.

It almost seems as if how the initial data is organized and represented is almost more important than the model itself in some cases. 

Can anyone shed some more light so I can have more intuition on the beauty of the matrix math here, rather than thinking that I can just randomly cherry pick some random sequence of transformations, fine tune it, call it attention or something buzz wordy, and call it a day?",4,14
473,2019-6-8,2019,6,8,9,by1w0m,Yolo inaccurate object coordinates,https://www.reddit.com/r/MachineLearning/comments/by1w0m/yolo_inaccurate_object_coordinates/,RumboYT,1559953380,[removed],0,1
474,2019-6-8,2019,6,8,9,by288m,PREPARACIN DE DATOS PARA EL APRENDIZAJE AUTOMTICO,https://www.reddit.com/r/MachineLearning/comments/by288m/preparacin_de_datos_para_el_aprendizaje/,jeffry_30,1559955596,[removed],0,1
475,2019-6-8,2019,6,8,10,by2kwp,"Google levels up on machine learning, introduces TensorNetwork library",https://www.reddit.com/r/MachineLearning/comments/by2kwp/google_levels_up_on_machine_learning_introduces/,CodePerfect,1559958023,,0,1
476,2019-6-8,2019,6,8,11,by2x5l,(Spectral) Multigraph Networks for Discovering and Fusing Relationships in Molecules (and other graph data),https://www.reddit.com/r/MachineLearning/comments/by2x5l/spectral_multigraph_networks_for_discovering_and/,bknyazev,1559960317,[removed],0,1
477,2019-6-8,2019,6,8,11,by2z5e,[D] Why do so many USA technology companies use GLMs?,https://www.reddit.com/r/MachineLearning/comments/by2z5e/d_why_do_so_many_usa_technology_companies_use_glms/,FastTruth,1559960705,Why are GLMs so popular? Why not Deep Learning?,1,0
478,2019-6-8,2019,6,8,11,by3171,dual path network with other networks?,https://www.reddit.com/r/MachineLearning/comments/by3171/dual_path_network_with_other_networks/,invoker66,1559961090,[removed],0,1
479,2019-6-8,2019,6,8,12,by3dtf,Does anyone know if Google has an API or code or something for what they used to create these lip readings? Does anyone know what methods they might have used?,https://www.reddit.com/r/MachineLearning/comments/by3dtf/does_anyone_know_if_google_has_an_api_or_code_or/,LaMasterShredder,1559963527,,0,1
480,2019-6-8,2019,6,8,12,by3goj,"[R] Language, trees, and geometry in neural networks",https://www.reddit.com/r/MachineLearning/comments/by3goj/r_language_trees_and_geometry_in_neural_networks/,tensorflower,1559964073,"https://pair-code.github.io/interpretability/bert-tree/

Corresponding paper: https://arxiv.org/abs/1906.02715",5,7
481,2019-6-8,2019,6,8,12,by3iee,[D] Does anyone know what methods Google might have used to create these lip readings? Is there an API or code for that?,https://www.reddit.com/r/MachineLearning/comments/by3iee/d_does_anyone_know_what_methods_google_might_have/,LaMasterShredder,1559964415,,0,1
482,2019-6-8,2019,6,8,12,by3pdl,The Machine Learning Crash Course  Part 2: Linear Regression,https://www.reddit.com/r/MachineLearning/comments/by3pdl/the_machine_learning_crash_course_part_2_linear/,CharlesPolley,1559965785,,0,1
483,2019-6-8,2019,6,8,13,by3zb4,[D] Google Co-Lab to Google drive file transfer fails.,https://www.reddit.com/r/MachineLearning/comments/by3zb4/d_google_colab_to_google_drive_file_transfer_fails/,GetMeSomeDownVotes,1559967692,"I was working with large number of files in google colab. Its about 125GB &amp;140000 files. After preprocessing the data-set before the session gets expire I was  trying to transfer all this files  into google driver, which I was already mounted into google colab .   

&amp;#x200B;

I made an archive file (.tag.gz) before transfer it. then I execute the command to copy it to google drive. It was seem like coping but it didn't work. I don't remember the exact error message but it says ""fail. not enough space"" , But my google drive has 1TB storage space &amp; more than 800GB free space.   

&amp;#x200B;

Day before this happens I have copied 100GB files into google drive without any problem. Only different is that time I didn't make a archive file. And it took a lot of time to transfer completely.  

&amp;#x200B;

Do anyone has any idea about this? please help me.. sorry about my English writings.",0,1
484,2019-6-8,2019,6,8,14,by4n8p,[P] Student with access to TPU credits reproduced GPT2-1.5B and plan to release model,https://www.reddit.com/r/MachineLearning/comments/by4n8p/p_student_with_access_to_tpu_credits_reproduced/,milaworld,1559971790,"From the author Connor Leahy:

*Hey OpenAI, I've replicated GPT2-1.5B in full and plan on releasing it to the public on July 1st. I sent you an email with the model. For my reasoning why, please read my post:*

https://medium.com/@NPCollapse/gpt2-counting-consciousness-and-the-curious-hacker-323c6639a3a8

Excerpt from post:

*TL;DR: Im a student that replicated OpenAIs GPT21.5B. I plan on releasing it on the 1st of July. Before criticizing my decision to do so, please read my arguments below. If you still think Im wrong, contact me on Twitter @NPCollapse or by email (thecurioushacker@outlook.com) and convince me. For code and technical details, see this [post](https://medium.com/@NPCollapse/replicating-gpt2-1-5b-86454a7f26af).*",55,89
485,2019-6-8,2019,6,8,15,by53a0,"Guys I've created CNN-LSTM based r/RoastMe Bot ,that roasts given the image,how can I improve further that model",https://www.reddit.com/r/MachineLearning/comments/by53a0/guys_ive_created_cnnlstm_based_rroastme_bot_that/,saurabh241,1559975370,,0,1
486,2019-6-8,2019,6,8,16,by5kpg,eGPU or gaming Laptop for ML/DL abroad?,https://www.reddit.com/r/MachineLearning/comments/by5kpg/egpu_or_gaming_laptop_for_mldl_abroad/,code4meplz,1559979711,"Hey fellas,

I am in need of advice and opinions, so heres the deal:

I am currently a comp. sci masters student from europe and I am going to china (Chengdu) for an internship and my masters thesis. This might be up to a year. I started getting into ML in my free time, since my university does not offer specialized degrees in this field. SO far I did Andres DL course, [fast.ai](https://fast.ai), a few university projects on self driving and wather prediction and I am currently working with the carla simulator on intelligent agents. At home i have my TR / 32gb ram / 1080ti workstation, but i definitely want to be able to continue learning/training and then starting to focus on kaggle challenges while I am abroad. Now my budget for hardware is somewhere around 2000+-300 Euros.

&amp;#x200B;

This leaves me with 2 options: (ultra)book + eGPU or a gaming Laptop:

(Iam excluding cloud solutions here because Iam not sure how this is gonna work out in china)

&amp;#x200B;

For the first option Iam looking at a [Laptop](https://www.tuxedocomputers.com/en/Linux-Hardware/Linux-Notebooks/15-6-inch/TUXEDO-InfinityBook-Pro-15-v4-RED-Edition-15-6-non-glare-Full-HD-IPS-aluminium-chassis-max-Intel-Core-i7-Quad-Core-max-64GB-RAM-max-two-HDD/SSD-max-10h-battery-USB-C-Thunderbolt-3.tuxedo#) from Tuxedo Computers:

for around 1250 euros I am getting:

\-i7 8565U

\-32 GB DDR4 (2\*16)

\-full 4 PCIe lanes TB3 

as eGPU iam currently looking at the Aorus Gaming Box with an RTX 2070 for around 750 euros. 

&amp;#x200B;

As for a gaming Laptop in this price-range I am usually getting such as Asus ROG Strix series or HP Omen for example:

Intel Core i7-9 9750H

16GB DDR4

RTX 2070 8GB

&amp;#x200B;

\-What Iam mostly worried about is that those gaming Laptop wont cope with training DL models for hours and hours, whereas in the gaming box the graphics card has its own enclosure

\-Also getting 32GB of RAM (replay mem for RL can easily be above 16 gigs) on a gaming Laptop is somewhat too expensive.

\- I feel like I pay a lot of money for useless tech on gaming laptops like 144+ Hz, G-sync, useless RGB. Gsync even makes one lose optimus, so worse battery life when in uni.

\-Using eGPU gives somewhat more flexibility.

\-performancewise I imagine both setups to be somewhat equal (when ram is no bottelneck) because laptop 2070 (assuming non maxq) is somehwat slower than desktop, but eGPU is limited by TB3, has anyone experience with that?

&amp;#x200B;

Has anyone been in a similar situation or regularly training models on Laptops? Iam happy for any input and discussion :)",0,1
487,2019-6-8,2019,6,8,16,by5mmb,Masters' in Machine Learning at the University of Tubingen,https://www.reddit.com/r/MachineLearning/comments/by5mmb/masters_in_machine_learning_at_the_university_of/,srmsoumya,1559980225,[removed],0,1
488,2019-6-8,2019,6,8,17,by5pbi,[D] Returning the hidden state in keras RNNs with return_state,https://www.reddit.com/r/MachineLearning/comments/by5pbi/d_returning_the_hidden_state_in_keras_rnns_with/,ixeption,1559980944,,0,1
489,2019-6-8,2019,6,8,17,by5syd,[D] Returning the hidden state in keras RNNs with return_state,https://www.reddit.com/r/MachineLearning/comments/by5syd/d_returning_the_hidden_state_in_keras_rnns_with/,ixeption,1559981888,"Hi folks,

In keras it's quite confusing, what ***return\_sequences***, ***return\_state*** and stateful RNNs does and how to use the output tensors. If you have ever thought about what retun\_state is doing in keras, [here](http://digital-thinking.de/keras-returning-hidden-state-in-rnns/) is some explanation for it. 

Any additional information is welcome and will be included.

Cheers",0,0
490,2019-6-8,2019,6,8,17,by5z21,Telescopic forks for Automotive | Telescopic Forks | Automated Warehouse AS/RS,https://www.reddit.com/r/MachineLearning/comments/by5z21/telescopic_forks_for_automotive_telescopic_forks/,lhd121,1559983572,,0,1
491,2019-6-8,2019,6,8,19,by6lpq,"""Hey OpenAI, I've replicated GPT2-1.5B in full and plan on releasing it to the public on July 1st.""",https://www.reddit.com/r/MachineLearning/comments/by6lpq/hey_openai_ive_replicated_gpt215b_in_full_and/,moultano,1559989274,,0,1
492,2019-6-8,2019,6,8,19,by6q4e,"If all you're doing is copy/pasting someone else's blog/tutorial/stackoverflow and making minor adjustments, please do not create another frigging Medium article. It's just worthless noise.",https://www.reddit.com/r/MachineLearning/comments/by6q4e/if_all_youre_doing_is_copypasting_someone_elses/,halfassadmin,1559990365,[removed],0,1
493,2019-6-8,2019,6,8,19,by6qvq,What is Linear Regression? Part:2,https://www.reddit.com/r/MachineLearning/comments/by6qvq/what_is_linear_regression_part2/,TecTunnel,1559990572,,0,1
494,2019-6-8,2019,6,8,19,by6sbn,"[D] If all you're doing is copy/pasting someone else's blog/tutorial/stackoverflow and making minor adjustments, please do not create another frigging Medium article. It's just worthless noise.",https://www.reddit.com/r/MachineLearning/comments/by6sbn/d_if_all_youre_doing_is_copypasting_someone_elses/,halfassadmin,1559990964,"So  many ""blogs"" are just someone linking back to a stack overflow post I  already found. The reason I found their ""blog post"" is because I was  looking for more context into something that was in the stackoverflow  post.

I don't need 20 iterations of the tensorflow object detection api tutorial where they just copy paste someone else's code into the post. If you want to build an online presence go do something meaningful.",156,983
495,2019-6-8,2019,6,8,20,by6yb9,"[D] Activation masking (pruning), then how to calculate pruned weights (zeroed weights due to zero activations)",https://www.reddit.com/r/MachineLearning/comments/by6yb9/d_activation_masking_pruning_then_how_to/,tsauri,1559992493,"Calculating pruned weights is easy, just nonzero\_params/all\_params \* 100%  


Anyone here know how to calculate pruned weights due to pruned activations (masked activations)?  
Unlike ordinary weight pruning, it involves going through zeros through matmul, conv, with strides and kernel size, etc.   
If there are papers on how to do the calculations, do share here, thanks",0,2
496,2019-6-8,2019,6,8,21,by7an7,[R] Deep Residual Learning in Pomegranate Networks,https://www.reddit.com/r/MachineLearning/comments/by7an7/r_deep_residual_learning_in_pomegranate_networks/,lazycnt,1559995384,,6,4
497,2019-6-8,2019,6,8,21,by7bv0,[D] Does anyone know what methods Google might have used to create these lip readings? Is there an API or code for that?,https://www.reddit.com/r/MachineLearning/comments/by7bv0/d_does_anyone_know_what_methods_google_might_have/,LaMasterShredder,1559995638,,0,1
498,2019-6-8,2019,6,8,22,by7w5j,[D] Does anyone know what methods Google might have used to create these lip readings? Is there an API or code for that?,https://www.reddit.com/r/MachineLearning/comments/by7w5j/d_does_anyone_know_what_methods_google_might_have/,LaMasterShredder,1559999943,,0,1
499,2019-6-8,2019,6,8,22,by81m9,[D] Does anyone know what methods Google might have used to create these lip readings? Is there an API or code for that?,https://www.reddit.com/r/MachineLearning/comments/by81m9/d_does_anyone_know_what_methods_google_might_have/,EnterOblivionS,1560001033,,0,1
500,2019-6-8,2019,6,8,23,by89n8,[P] AI Art Shop,https://www.reddit.com/r/MachineLearning/comments/by89n8/p_ai_art_shop/,neurokinetikz,1560002527,,1,1
501,2019-6-8,2019,6,8,23,by8b7y,"[D] Patent Claims based on ""Deep learning for brushing teeth"" by GPT-2",https://www.reddit.com/r/MachineLearning/comments/by8b7y/d_patent_claims_based_on_deep_learning_for/,js_lee,1560002809,"[15 samples generated after fine-tuning GPT-2](https://aipatent.wordpress.com/deep-learning-for-brushing-teeth/) and no cherry-picking. Some might be interesting to read and some don't make sense at all. All of them are far from patenting a product like [Oclean Air Electric Toothbrush](https://www.essentialhomeandgarden.com/oclean-air-review/). 
Our motivation is trying to see whether GPT-2 can generate new ideas for innovation.",2,4
502,2019-6-8,2019,6,8,23,by8eza,[D] What is the software used to draw nice CNN models?,https://www.reddit.com/r/MachineLearning/comments/by8eza/d_what_is_the_software_used_to_draw_nice_cnn/,FastTruth,1560003492,"like 

https://www.researchgate.net/profile/Mehmet_Hacibeyoglu/publication/328405250/figure/fig1/AS:683792754806784@1540040305691/The-basic-structure-of-convolution-neural-networks.jpg",4,8
503,2019-6-8,2019,6,8,23,by8ijo,The legality of using and sharing images found on google search.,https://www.reddit.com/r/MachineLearning/comments/by8ijo/the_legality_of_using_and_sharing_images_found_on/,vincent_rodriguezz,1560004136,[removed],0,1
504,2019-6-8,2019,6,8,23,by8o4w,Predicting age (or skill level) of a website,https://www.reddit.com/r/MachineLearning/comments/by8o4w/predicting_age_or_skill_level_of_a_website/,venuv,1560005134,[removed],0,1
505,2019-6-8,2019,6,8,23,by8rly,What are some research ideas that can be done by a final year student using machine learning,https://www.reddit.com/r/MachineLearning/comments/by8rly/what_are_some_research_ideas_that_can_be_done_by/,FluidDecision,1560005776,[removed],0,1
506,2019-6-9,2019,6,9,0,by90uz,Analyzing actors with facial recognition in Tarantino films,https://www.reddit.com/r/MachineLearning/comments/by90uz/analyzing_actors_with_facial_recognition_in/,rememberlennydotcom,1560007344,,0,1
507,2019-6-9,2019,6,9,0,by93to,"Object detection, classification and tracking models",https://www.reddit.com/r/MachineLearning/comments/by93to/object_detection_classification_and_tracking/,lloydrayner,1560007838,[removed],0,1
508,2019-6-9,2019,6,9,0,by9aqf,Neuroevolution and Deep Learning Comparison,https://www.reddit.com/r/MachineLearning/comments/by9aqf/neuroevolution_and_deep_learning_comparison/,8756314039380142,1560008958,[removed],0,1
509,2019-6-9,2019,6,9,1,by9rul,[D] Neuroevolution and Deep Learning Comparison,https://www.reddit.com/r/MachineLearning/comments/by9rul/d_neuroevolution_and_deep_learning_comparison/,8756314039380142,1560011710,Does anyone have a benchmark/comparison of a neuroevolution algorithm for machine learning such as NEAT and a more traditional deep learning approach? Preferably an interactive one that features many different kinds of tasks and tracks the performance of different sizes of models as they train. I have done some experiments with genetic algorithms in the past and would like to see if they provide any long-term advantages over gradient-descent based methods at any type of problem. Any help would be appreciated. (Repost because forgot to tag post the first time.),4,5
510,2019-6-9,2019,6,9,2,bya2at,Car Accident Event Detection With Smartphone Application #Python #UnlebaledData #NEED_sampleCode/tutorial,https://www.reddit.com/r/MachineLearning/comments/bya2at/car_accident_event_detection_with_smartphone/,glassAlloy,1560013336,[removed],0,1
511,2019-6-9,2019,6,9,2,byaaal,Can we evolve 0 and 1?,https://www.reddit.com/r/MachineLearning/comments/byaaal/can_we_evolve_0_and_1/,dendrite12,1560014593,[removed],0,1
512,2019-6-9,2019,6,9,2,byafwk,Using cluster-algorithms on time series data,https://www.reddit.com/r/MachineLearning/comments/byafwk/using_clusteralgorithms_on_time_series_data/,Unlistedd,1560015486,[removed],0,1
513,2019-6-9,2019,6,9,2,byam05,[D] What research is there in end to end normalization and whitening for time series deep learning models?,https://www.reddit.com/r/MachineLearning/comments/byam05/d_what_research_is_there_in_end_to_end/,iamiamwhoami,1560016440,So Im looking for work that studies how normalization and whitening can be included in the input layer for a time series model. I know batch normalization is related but I dont think that explicitly looks at time series data. Anyone have recommendations?,3,6
514,2019-6-9,2019,6,9,3,byaoai,[P] Dilated Convolution Seq2Seq,https://www.reddit.com/r/MachineLearning/comments/byaoai/p_dilated_convolution_seq2seq/,huseinzol05,1560016805,"I implemented dilated convolution Seq2Seq, based architecture from [Convolution Seq2Seq](https://arxiv.org/abs/1705.03122), tested on [100k English-Malay translation dataset](https://github.com/huseinzol05/Malaya-Dataset/tree/master/english-malay), and I beat that model in term of word position. 80% to train, 20% to test.

This result after 20 epochs only,
1. Attention is All you need, train accuracy 19.09% test accuracy 20.38%
2. BiRNN Seq2Seq Luong Attention, Beam decoder, train accuracy 45.2% test accuracy 37.26%
3. Convolution Encoder Decoder, train accuracy 35.89% test accuracy 30.65%
4. Dilated Convolution Encoder Decoder, train accuracy 82.3% test accuracy 56.72%
5. Dilated Convolution Encoder Decoder Self-Attention, train accuracy 60.76% test accuracy 36.59%

[Source code here](https://github.com/huseinzol05/Bahasa-NLP-Tensorflow#english-malay-translation)

Feel free to use it for future research, and let me know if got better or bad results!",4,5
515,2019-6-9,2019,6,9,3,byawnj,Wasserstein GAN in Swift for TensorFlow,https://www.reddit.com/r/MachineLearning/comments/byawnj/wasserstein_gan_in_swift_for_tensorflow/,rahulbhalley,1560018115,,0,1
516,2019-6-9,2019,6,9,3,byb4ea,Interesting AI research that requires little compute,https://www.reddit.com/r/MachineLearning/comments/byb4ea/interesting_ai_research_that_requires_little/,LeonDeHill,1560019370,,0,1
517,2019-6-9,2019,6,9,3,bybakp,[D] Interesting AI research that requires little compute. More ideas ?,https://www.reddit.com/r/MachineLearning/comments/bybakp/d_interesting_ai_research_that_requires_little/,LeonDeHill,1560020379,,0,1
518,2019-6-9,2019,6,9,4,bybdy7,I created two demos using google colab,https://www.reddit.com/r/MachineLearning/comments/bybdy7/i_created_two_demos_using_google_colab/,Foulgaz3,1560020917,[removed],0,1
519,2019-6-9,2019,6,9,4,bybfln,"[P] I created two demos using Google Colab, one to explain Gradient Descent, one a Shakespeare Char-RNN; I would appreciate some feedback",https://www.reddit.com/r/MachineLearning/comments/bybfln/p_i_created_two_demos_using_google_colab_one_to/,Foulgaz3,1560021188,[removed],0,1
520,2019-6-9,2019,6,9,4,bybmyk,Entrenamiento de un modelo de Aprendizaje automtico,https://www.reddit.com/r/MachineLearning/comments/bybmyk/entrenamiento_de_un_modelo_de_aprendizaje/,jeffry_30,1560022386,[removed],0,1
521,2019-6-9,2019,6,9,5,bybyrv,How to version compare and share notebooks?,https://www.reddit.com/r/MachineLearning/comments/bybyrv/how_to_version_compare_and_share_notebooks/,ai_yoda,1560024328,[removed],0,1
522,2019-6-9,2019,6,9,5,byc459,"How to version, compare and share your notebooks?",https://www.reddit.com/r/MachineLearning/comments/byc459/how_to_version_compare_and_share_your_notebooks/,ai_yoda,1560025202,"We have just created a Jupyter notebook extension that lets you ""commit"" notebook snapshots and then

do diffs on code and outputs and share it with other people.

You can get more info [here](https://community.neptune.ml/t/launched-notebooks-comparison/524) or simply check it out [in the app](https://ui.neptune.ml/o/neptune-ml/org/credit-default-prediction/compare-notebooks?sourceNotebookId=ac75c237-1630-4109-b532-dd125badec0e&amp;sourceCheckpointId=69b6a2b6-0672-468c-8658-58b9e60ae305&amp;targetNotebookId=ac75c237-1630-4109-b532-dd125badec0e&amp;targetCheckpointId=c526d0db-1e35-4e1c-ad70-2cc64bf9680c).  

I would absolutely love to hear your feedback on it.

https://i.redd.it/qt1014x1y6331.png",0,1
523,2019-6-9,2019,6,9,5,byc4lk,"[R]: Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates",https://www.reddit.com/r/MachineLearning/comments/byc4lk/r_painless_stochastic_gradient_interpolation/,sinsecticide,1560025281,"The authors use a classic Armijo line-search approach in the context of SGD to automatically tune the line search parameter in training the neural networks. They're also able to prove convergence results on minimizing convex and non-convex objective functions satisfying certain growth conditions. An aside, but as an optimization-head myself, it's nice to see some of the traditional optimization ideas make their way into an ML context.

&amp;#x200B;

[https://arxiv.org/pdf/1905.09997.pdf](https://arxiv.org/pdf/1905.09997.pdf)",1,14
524,2019-6-9,2019,6,9,5,bycesa,What are some of techniques for loading big datasets (100m+ observations &amp; 30 features) in python.,https://www.reddit.com/r/MachineLearning/comments/bycesa/what_are_some_of_techniques_for_loading_big/,Pimp_Fada,1560026993,"Hi everyone, I have just been given access to a 100m+ rows of data housed in a MySQL database on AWS.

I tried loading the database just as s test with pandas by choosing a chunk size of 1,000,000 and the connection broke after a long time trying to load.

This is my first taste of a really ""big dataset"" and I'm out of ideas how to even load the data. 

Any pointers for me?",0,1
525,2019-6-9,2019,6,9,6,byd1aj,Im trying to use Neuroevolution with SSD or Yolo and the only thing Im currently struggling with is how to do variation over weights. Any ideas?,https://www.reddit.com/r/MachineLearning/comments/byd1aj/im_trying_to_use_neuroevolution_with_ssd_or_yolo/,kartinko28,1560030693,,0,1
526,2019-6-9,2019,6,9,7,byd6t2,[D] Im trying to use Neuroevolution with SSD or Yolo and the thing Im struggling with currently is how to do variation over the weights. Any ideas?,https://www.reddit.com/r/MachineLearning/comments/byd6t2/d_im_trying_to_use_neuroevolution_with_ssd_or/,kartinko28,1560031578,,4,1
527,2019-6-9,2019,6,9,7,byddns,"Finally did it! Hey everyone! My name is Maurice, new to reddit, and I currently gather feedback here on reddit about the software and applications that I create inspired in movies and TV shows :)",https://www.reddit.com/r/MachineLearning/comments/byddns/finally_did_it_hey_everyone_my_name_is_maurice/,mauricecost,1560032749," I started learning face recognition and the OpenCV library for Python a few weeks ago, and it has been an amazing journey! 

BTW, If you do not know me yet, my name is Maurice, and I started a small show on Youtube, where I recreate software and applications from movies and TV shows. 

I strongly believe that every victory should be celebrated, and a few days ago, I managed to wrap my head around some concepts of face recognition and computer vision, and implement it in Python. I feel really good, and I would like to share it with all of you! (Don't give up on your ideas, push through and you'll eventually make it).

It's been an awesome experience to get feedback and constructive criticism from a lot of people here on reddit. For the past a few days, I've been working on the third episode of this show, where I recreated the face recognition from CSI (Crime Scene Investigation).

PS: If you are interested in seeing what I did, it is right here: [https://youtu.be/eTvtUkce4IE](https://youtu.be/eTvtUkce4IE)

Ask me anything :D",0,1
528,2019-6-9,2019,6,9,8,bye818,https://www.kaggle.com/lavanyashukla01/searching-for-dark-matter,https://www.reddit.com/r/MachineLearning/comments/bye818/httpswwwkagglecomlavanyashukla01searchingfordarkma/,beeblebrox_9,1560038141,[removed],0,1
529,2019-6-9,2019,6,9,9,byebek,How-To: search for particles/dark matter in CERN's LHC dataset,https://www.reddit.com/r/MachineLearning/comments/byebek/howto_search_for_particlesdark_matter_in_cerns/,0_marauders_0,1560038758,,0,1
530,2019-6-9,2019,6,9,9,byeegt,EVALUACIN DE UN MODELO DE APRENDIZAJE AUTOMTICO,https://www.reddit.com/r/MachineLearning/comments/byeegt/evaluacin_de_un_modelo_de_aprendizaje_automtico/,jeffry_30,1560039295,[removed],0,1
531,2019-6-9,2019,6,9,9,byehnw,[Q] Are their easy ways to combine trained models?,https://www.reddit.com/r/MachineLearning/comments/byehnw/q_are_their_easy_ways_to_combine_trained_models/,FruityWelsh,1560039856,[removed],0,1
532,2019-6-9,2019,6,9,9,byeseo,PyTorch Linear Layer 2D Input,https://www.reddit.com/r/MachineLearning/comments/byeseo/pytorch_linear_layer_2d_input/,arjundupa,1560041788,[removed],0,1
533,2019-6-9,2019,6,9,10,byezuc,Which library should I learn to create models? (I have been using sci-kit learn),https://www.reddit.com/r/MachineLearning/comments/byezuc/which_library_should_i_learn_to_create_models_i/,dattud,1560043111,[removed],0,1
534,2019-6-9,2019,6,9,10,byf58d,PyTorch Model _forward() Parameters,https://www.reddit.com/r/MachineLearning/comments/byf58d/pytorch_model_forward_parameters/,arjundupa,1560044109,[removed],0,1
535,2019-6-9,2019,6,9,10,byf7ed,Butterfly Transform: An Efficient FFT Based Neural Architecture Design,https://www.reddit.com/r/MachineLearning/comments/byf7ed/butterfly_transform_an_efficient_fft_based_neural/,iyaja,1560044522,,15,66
536,2019-6-9,2019,6,9,12,byfza0,DeepMind &amp; Google Brain Open Source HLE Framework for Hanabi,https://www.reddit.com/r/MachineLearning/comments/byfza0/deepmind_google_brain_open_source_hle_framework/,Yuqing7,1560049993,,0,1
537,2019-6-9,2019,6,9,13,bygjt9,Will we ever reach brain level intelligence?,https://www.reddit.com/r/MachineLearning/comments/bygjt9/will_we_ever_reach_brain_level_intelligence/,push_limits__13,1560054247,[removed],0,1
538,2019-6-9,2019,6,9,13,bygssq,Whats the difference between Low Rank Approximation and Principal Component Analysis?,https://www.reddit.com/r/MachineLearning/comments/bygssq/whats_the_difference_between_low_rank/,Makashi,1560056159,They both look like problems of finding the top eigenvectors. What am I missing?,0,1
539,2019-6-9,2019,6,9,14,bygvut,[D] Whats the difference between Low Rank Approximation and Principal Component Analysis?,https://www.reddit.com/r/MachineLearning/comments/bygvut/d_whats_the_difference_between_low_rank/,Makashi,1560056844,They both look like problems of finding the top eigenvectors. What am I missing?,16,37
540,2019-6-9,2019,6,9,14,byh8eg,Question for anyone who can help me find the right tool for something.,https://www.reddit.com/r/MachineLearning/comments/byh8eg/question_for_anyone_who_can_help_me_find_the/,explain_it_please,1560059749,[removed],1,1
541,2019-6-9,2019,6,9,15,byhdm3,MSc Machine Learning at UCL,https://www.reddit.com/r/MachineLearning/comments/byhdm3/msc_machine_learning_at_ucl/,vavantoo,1560061012,[removed],0,1
542,2019-6-9,2019,6,9,16,byhp15,[P] Mario Reinforcement Learning - YouTube,https://www.reddit.com/r/MachineLearning/comments/byhp15/p_mario_reinforcement_learning_youtube/,themathstudent,1560064028,,0,1
543,2019-6-9,2019,6,9,16,byhrrk,Which is most relevant in Machine Learning/Data Science? Statistics or Maths or Programming?,https://www.reddit.com/r/MachineLearning/comments/byhrrk/which_is_most_relevant_in_machine_learningdata/,cooper_pair_,1560064755,[removed],0,1
544,2019-6-9,2019,6,9,17,byi3dh,"Microsoft Launches First AI Hub in Louisville, Kentucky.",https://www.reddit.com/r/MachineLearning/comments/byi3dh/microsoft_launches_first_ai_hub_in_louisville/,crypt0sparta,1560068053,,0,2
545,2019-6-9,2019,6,9,17,byia8s,[D] Reasons for an increasing loss while having a steady accuracy?,https://www.reddit.com/r/MachineLearning/comments/byia8s/d_reasons_for_an_increasing_loss_while_having_a/,Spenhouet,1560069991,"I have this phenomenon that I don't understand.My loss is the cross entropy of the softmax prediction with one-hot encoded labels.The accuracy is a simple argmax comparison.While the accuracy stays the same, the loss is heavily increasing.I would guess the reason is that all outputs get more equal but the correct label stays the highest values.But what could be the reason for such a behavior? Why is that happening? What can I do about it?",12,23
546,2019-6-9,2019,6,9,17,byic3s,What is the cutting edge Text Classification Approach?,https://www.reddit.com/r/MachineLearning/comments/byic3s/what_is_the_cutting_edge_text_classification/,SEFDStuff,1560070572,[removed],0,1
547,2019-6-9,2019,6,9,18,byifj3,Which direction is the most important when you train yourself to be a data scientist from a strong (PhD level) academic background?,https://www.reddit.com/r/MachineLearning/comments/byifj3/which_direction_is_the_most_important_when_you/,Change_ML,1560071510,[removed],0,1
548,2019-6-9,2019,6,9,19,byiwh2,Data Science. Correlation,https://www.reddit.com/r/MachineLearning/comments/byiwh2/data_science_correlation/,luminoumen,1560076081,,0,1
549,2019-6-9,2019,6,9,20,byjbh7,1st ML challenge,https://www.reddit.com/r/MachineLearning/comments/byjbh7/1st_ml_challenge/,TrusterZero,1560079870,[removed],0,1
550,2019-6-9,2019,6,9,20,byjddf,How to turn a Machine Learning project into a research opportunity?,https://www.reddit.com/r/MachineLearning/comments/byjddf/how_to_turn_a_machine_learning_project_into_a/,arismission,1560080337,[removed],0,1
551,2019-6-9,2019,6,9,22,byk0tv,Training a modest machine-learning model uses more carbon than the manufacturing and lifetime use of five automobiles,https://www.reddit.com/r/MachineLearning/comments/byk0tv/training_a_modest_machinelearning_model_uses_more/,wildnux,1560085593,,0,1
552,2019-6-9,2019,6,9,22,byk4hf,Training a modest machine-learning model uses more carbon than the manufacturing and lifetime use of five automobiles [Research],https://www.reddit.com/r/MachineLearning/comments/byk4hf/training_a_modest_machinelearning_model_uses_more/,wildnux,1560086323,,0,1
553,2019-6-9,2019,6,9,22,bykacx,YOLO architecture,https://www.reddit.com/r/MachineLearning/comments/bykacx/yolo_architecture/,promach,1560087485,[removed],0,1
554,2019-6-9,2019,6,9,22,bykbly,Predicting Sales with LSTM,https://www.reddit.com/r/MachineLearning/comments/bykbly/predicting_sales_with_lstm/,karamanbk,1560087727,,0,1
555,2019-6-9,2019,6,9,22,bykdh8,A few questions from a behavioral scientist on reinforcement learning...,https://www.reddit.com/r/MachineLearning/comments/bykdh8/a_few_questions_from_a_behavioral_scientist_on/,massimosclaw2,1560088087,[removed],0,1
556,2019-6-9,2019,6,9,22,bykgpc,[D] A few questions from a behavioral scientist on reinforcement learning...,https://www.reddit.com/r/MachineLearning/comments/bykgpc/d_a_few_questions_from_a_behavioral_scientist_on/,massimosclaw2,1560088699,"I've recently started getting interested in AI. As far as I know, from my friend who is more familiar with AI, in reinforcement learning only 2 concepts are applied: reinforcement (layman's term: reward), and punishment.

However, in behavioral science there are many more elements that may be useful to implement in an AI.

1. There are concepts such as discriminative stimuli, or stimulus generalization.
2. There are also other concepts such as continuous and discrete stimulus/response fields.
3. There are ""schedules of reinforcement"" which change the frequency of an organisms response (such as variable ratio, fixed ratio, variable interval, fixed interval)
4. etc.

I'm not familiar with any AI that uses these concepts and others from behavior analysis. Is anyone familiar with any AI that tries to implement these?

It seems to me a concept such as stimulus generalization may be a useful step in artificial general intelligence for 'one shot learning' - to give an example from a study: if you reinforce a pigeon with food everytime it pecks a key under a 550 milimicron (color wavelength) light, then you stop reinforcement (giving it food if it pecks), and you change the color of the light slightly to 560 or 570, the pigeon still pecks the key but fewer times. Therefore 'stimulus generalization' occurs. In other words, the pigeon responds similarly to similar stimuli (but with a reduced frequency). The more you change the color - to say 580, then 590, the less responses (or pecks on the key). This also occurs with punishment.

Is anyone familiar with this being applied to AI? Or anything additional from behavioral science for that matter? If only reinforcement and punishment are applied I fear that's really limiting the great potential of AI.",20,78
557,2019-6-9,2019,6,9,23,bykt1o,This student claims that has replicated the full 1'5B GPT2 model and plans to release it 1st of July. Thoughts?,https://www.reddit.com/r/MachineLearning/comments/bykt1o/this_student_claims_that_has_replicated_the_full/,adriacabeza,1560090927,,0,1
558,2019-6-9,2019,6,9,23,byku9u,Help with YOLO + Tensorflow records,https://www.reddit.com/r/MachineLearning/comments/byku9u/help_with_yolo_tensorflow_records/,andrelan,1560091137,[removed],0,1
559,2019-6-9,2019,6,9,23,bykypy,I am completly lost with with my thesis/life topic related to ML,https://www.reddit.com/r/MachineLearning/comments/bykypy/i_am_completly_lost_with_with_my_thesislife_topic/,BeamerAT5,1560091866,[removed],0,1
560,2019-6-10,2019,6,10,0,byl2mv,MineRL Competition on Reinforcement Learning in Minecraft Launched!,https://www.reddit.com/r/MachineLearning/comments/byl2mv/minerl_competition_on_reinforcement_learning_in/,MadcowD,1560092498,[removed],0,2
561,2019-6-10,2019,6,10,2,bymmft,How I made top 0.3% on Kaggle,https://www.reddit.com/r/MachineLearning/comments/bymmft/how_i_made_top_03_on_kaggle/,0_marauders_0,1560101113,,0,1
562,2019-6-10,2019,6,10,3,byn3v4,AI for strategy,https://www.reddit.com/r/MachineLearning/comments/byn3v4/ai_for_strategy/,AIforEarth,1560103658,"Hey, have you ever crossed any AI applications for business strategic expansions?

I.e. = Bombora, identifying best opportunities and keywords searched on the Web of the interested location.

Just an example came in my mind, I'm looking for something a Lil bit smarter :D",0,1
563,2019-6-10,2019,6,10,3,byn5rw,best ML news sources,https://www.reddit.com/r/MachineLearning/comments/byn5rw/best_ml_news_sources/,Swartzcenter,1560103944,"Hey everyone,

Im fairly new to this sub. Does anyone have recommendations for ML related news sources? I.e. sites, daily emails, etc...

Thanks!",0,1
564,2019-6-10,2019,6,10,3,bynaqy,INTRODUCCIN A LAS REDES NEURONALES ARTIFICIALES,https://www.reddit.com/r/MachineLearning/comments/bynaqy/introduccin_a_las_redes_neuronales_artificiales/,jeffry_30,1560104705,[removed],0,1
565,2019-6-10,2019,6,10,3,bynef8,Machine learning from scratch with python,https://www.reddit.com/r/MachineLearning/comments/bynef8/machine_learning_from_scratch_with_python/,codingislife496,1560105243,,0,1
566,2019-6-10,2019,6,10,3,bynmzf,What Does the Future Hold For AI Salaries? - BlockDelta,https://www.reddit.com/r/MachineLearning/comments/bynmzf/what_does_the_future_hold_for_ai_salaries/,BlockDelta,1560106496,,0,1
567,2019-6-10,2019,6,10,4,bynsz8,[D] what would be the best model to use for forecasting sales of a new product?,https://www.reddit.com/r/MachineLearning/comments/bynsz8/d_what_would_be_the_best_model_to_use_for/,whiteferrari17,1560107372,I have this case study for forecasting the sales for a new mobile phone. I have sales and order quantity per week for two predecessor phones ( 2 years worth of data). I am currently looking at some time series models but was wondering if there were any other machine learning models that would be better to use.,5,0
568,2019-6-10,2019,6,10,4,bynx6l,New Trends in Artificial Intelligence &amp; Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bynx6l/new_trends_in_artificial_intelligence_machine/,andrea_manero,1560107983,[removed],0,1
569,2019-6-10,2019,6,10,4,byo092,[D] Design skills in ML research,https://www.reddit.com/r/MachineLearning/comments/byo092/d_design_skills_in_ml_research/,harmonium1,1560108435,"Some of the papers/repos I admire the most dont necessarily have the most important methods but have the nicest tables, layout and cleanest code and clearest APIs. 

I know Im conflating several notions of design here but Ive noticed that some researchers seem to excel at both writing beautiful papers and code as well as maintaining visually nice and well-written blogs. There seems to be a pattern in the sense that care is afforded to the presentation of the work, and this quality becomes a part of the brand of the researcher.  

Meanwhile, some of the code that has been the most useful for my research has been appallingly written and some of the papers that Ive found the most useful have been barely understandable and riddled with grammatical and even spelling mistakes. 

Obviously one should strive to present their work in the best way possible, so how does a PhD student who writes average quality research code and creates average Inkscape figures level up in terms of research and code presentation?",20,145
570,2019-6-10,2019,6,10,4,byo3ki,[D] An Overview of Deep Learning Based Clustering Techniques,https://www.reddit.com/r/MachineLearning/comments/byo3ki/d_an_overview_of_deep_learning_based_clustering/,svufzafa,1560108910,"Hi, Ive written a post describing the start of the art clustering algorithms. They all use deep learning!
On MNIST, some models can get 97%+ accuracy without using any label.

https://towardsdatascience.com/an-overview-of-deep-learning-based-clustering-techniques-ff640f108b5d",1,1
571,2019-6-10,2019,6,10,5,byoz5s,[R] RandomOut: Using a convolutional gradient norm to rescue convolutional filters,https://www.reddit.com/r/MachineLearning/comments/byoz5s/r_randomout_using_a_convolutional_gradient_norm/,ieee8023,1560113610,,1,14
572,2019-6-10,2019,6,10,7,bypu9y,[D] Using GANS with image and non-image data.,https://www.reddit.com/r/MachineLearning/comments/bypu9y/d_using_gans_with_image_and_nonimage_data/,machiavellian1,1560118297,Is there a way to combine both image and non-image while training  GAN's. So far most papers I've read seem to use one or the other but not both.,7,1
573,2019-6-10,2019,6,10,7,byq4ai,Help needed to understand some computer vision concepts,https://www.reddit.com/r/MachineLearning/comments/byq4ai/help_needed_to_understand_some_computer_vision/,vanhoutens,1560119895,[removed],0,1
574,2019-6-10,2019,6,10,7,byq7i3,"I am very interested in AI and will soon be entering the field through the military. Before I leave for my tech school I want to learn as much as possible about AI in general, if you have any good resources that could help me out please let me know.",https://www.reddit.com/r/MachineLearning/comments/byq7i3/i_am_very_interested_in_ai_and_will_soon_be/,awesomepickle21,1560120401,[removed],0,1
575,2019-6-10,2019,6,10,7,byq9ru,How to Install Python on Windows 10,https://www.reddit.com/r/MachineLearning/comments/byq9ru/how_to_install_python_on_windows_10/,softlect,1560120772,,0,1
576,2019-6-10,2019,6,10,8,byqet7,What happens when you train a GAN on pixel art,https://www.reddit.com/r/MachineLearning/comments/byqet7/what_happens_when_you_train_a_gan_on_pixel_art/,jalapenoses,1560121601,,0,1
577,2019-6-10,2019,6,10,8,byqpr0,Cannot find a paper I stumbled upon regarding graph embeddings and deep neural nets,https://www.reddit.com/r/MachineLearning/comments/byqpr0/cannot_find_a_paper_i_stumbled_upon_regarding/,anterak13,1560123345,[removed],1,1
578,2019-6-10,2019,6,10,9,byqzuf,Use (-1)*loss as reward function for RL??,https://www.reddit.com/r/MachineLearning/comments/byqzuf/use_1loss_as_reward_function_for_rl/,charlie44z,1560125069,[removed],0,1
579,2019-6-10,2019,6,10,10,byrloj,Beginner looking for advice using Python or R,https://www.reddit.com/r/MachineLearning/comments/byrloj/beginner_looking_for_advice_using_python_or_r/,pitin753,1560128894,[removed],0,1
580,2019-6-10,2019,6,10,10,byrrl2,Deep Learning vs. Traditional Computer Vision - major tradeoffs?,https://www.reddit.com/r/MachineLearning/comments/byrrl2/deep_learning_vs_traditional_computer_vision/,the_floom_room,1560129931,[removed],0,1
581,2019-6-10,2019,6,10,10,byrucw,Seeking Job Advice: I'm going to start my first job [junior data scientist at health care company] in a few days - any advice to prepare?,https://www.reddit.com/r/MachineLearning/comments/byrucw/seeking_job_advice_im_going_to_start_my_first_job/,Scatterbrain191,1560130428,"Hello, I am seriously nervous about my first day on the job. I feel like I know nothing, and yet I've devoted a ton of classwork to basic linear model theory, took a tutorial on machine learning (learned and applied some things very generally - basically learn the intuition of the model, write some basic R program to run it), and spent at least three years inconsistently working in R (took other courses in strict comp sci in Java and C++ in between).   


And I am having serious impostor syndrome! I feel like I have a ton to learn and I'm not ready to jump into a big project, and I'm afraid I'll be stumbling as I recall a lot of things that I might've forgotten in the past few weeks since I finished school. The job is catered to new undergrads, and makes that clear that the job is open to non-traditional majors (I myself am not a comp sci or stats major), but I'm afraid I'm gonna look like an idiot still. How do I look professional and ready to my new team? Should I read any nice books to ""look smart""?",0,1
582,2019-6-10,2019,6,10,10,byrwkp,Any able to discuss one on one the potential applications of AI/ML within agriculture.,https://www.reddit.com/r/MachineLearning/comments/byrwkp/any_able_to_discuss_one_on_one_the_potential/,DowntownPackage,1560130833,[removed],0,1
583,2019-6-10,2019,6,10,11,bys53u,[R] Selfie: Self-supervised Pretraining for Image Embedding,https://www.reddit.com/r/MachineLearning/comments/bys53u/r_selfie_selfsupervised_pretraining_for_image/,hardmaru,1560132291,,4,13
584,2019-6-10,2019,6,10,11,bys9wu,[R] Selfie: Self-supervised Pretraining for Image Embedding,https://www.reddit.com/r/MachineLearning/comments/bys9wu/r_selfie_selfsupervised_pretraining_for_image/,xternalz,1560133129,,1,1
585,2019-6-10,2019,6,10,12,bysuic,[D] Biggest roadblocks on ML projects,https://www.reddit.com/r/MachineLearning/comments/bysuic/d_biggest_roadblocks_on_ml_projects/,iocuydi,1560136803,"Hi all, I recently had to abandon a couple personal projects due to problems with data and training cost. This got me thinking about whether many others in the community share these issues, of if they were unique to my niche projects. If you've spent much time on ML projects lately, I'd appreciate if you could take this 5-10 minute survey. Thanks!!

[https://forms.gle/S6LM74gkh5uoMTKK8](https://forms.gle/S6LM74gkh5uoMTKK8)

&amp;#x200B;

Also, if you've run into other problems not addressed in the survey, I'd love to hear about them

&amp;#x200B;

Let me know if this breaks the rules or should go elsewhere",6,13
586,2019-6-10,2019,6,10,12,bysvon,[D] Is there any reason besides theory not to use binary cross-entropy for each class in a multi-class classification problem?,https://www.reddit.com/r/MachineLearning/comments/bysvon/d_is_there_any_reason_besides_theory_not_to_use/,lmericle,1560137013,"I know it's technically wrong, but I'm interested in whether research has been done with a serious investigation into the differences in outcomes between the two.

Categorical cross-entropy is correct for multinomial problems because it corresponds directly to the log-likelihood of the multinomial distribution. Optimizing it corresponds to optimizing the Kullback-Leibler divergence between the true and predicted distributions over the classes.

But something irks me. When performing backpropagation, the gradient is only nonzero at one terminal node. This seems to me suboptimal, especially as the number of classes grows.

Consider instead computing the binary cross-entropy at each terminal node. Then there is gradient information at every terminal node and a more solid update signal. Plus, it's not exactly a stretch to consider each class label as independent *a priori*.

I liken this to the difference between multinomial and one-vs-rest (OVR) logistic regression. In OVR logistic regression, the predicted label is simply the arg max of all the prediction probabilities. The same procedure can be performed with neural networks almost trivially.

For those unfamiliar, [here](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_multinomial.html) is an example in the Scikit-Learn documentation that demonstrates graphically the difference between the two to get a better understanding. That example shows that on the synthetic dataset, OVR logistic regression performs worse. But is that often the case? I'm not sure.",16,1
587,2019-6-10,2019,6,10,13,bytbx1,Data Science Training in Bengaluru,https://www.reddit.com/r/MachineLearning/comments/bytbx1/data_science_training_in_bengaluru/,venkatkrishna3560,1560140020,[removed],0,1
588,2019-6-10,2019,6,10,13,bytlhh,Convolutional Neural Networks Benefits. 7 Examples of Business Applications,https://www.reddit.com/r/MachineLearning/comments/bytlhh/convolutional_neural_networks_benefits_7_examples/,LemmyChildish,1560141880,,0,1
589,2019-6-10,2019,6,10,14,bytxto,What interesting projects have you made with the Google Coral Dev Board?,https://www.reddit.com/r/MachineLearning/comments/bytxto/what_interesting_projects_have_you_made_with_the/,makereven,1560144293,[removed],0,1
590,2019-6-10,2019,6,10,15,byu7ps,Adaptive Authentication,https://www.reddit.com/r/MachineLearning/comments/byu7ps/adaptive_authentication/,chandiramouli,1560146409,[removed],0,1
591,2019-6-10,2019,6,10,15,byu7x9,Artificial neural networks,https://www.reddit.com/r/MachineLearning/comments/byu7x9/artificial_neural_networks/,amrutapokhare,1560146453,[removed],0,1
592,2019-6-10,2019,6,10,15,byumc0,Projected Gradient Descent for Max and Min Eigenpairs - Proof of Convergence,https://www.reddit.com/r/MachineLearning/comments/byumc0/projected_gradient_descent_for_max_and_min/,sudeepraja,1560149552,,0,1
593,2019-6-10,2019,6,10,16,byv05v,How Artificial Intelligence Can Enhance Your Marketing Strategy To Grow Your Business,https://www.reddit.com/r/MachineLearning/comments/byv05v/how_artificial_intelligence_can_enhance_your/,quytech1,1560152565,,0,1
594,2019-6-10,2019,6,10,16,byv29v,Has anyone tried using GPT-2 to make a text-based game or just a really amazing chatbot?,https://www.reddit.com/r/MachineLearning/comments/byv29v/has_anyone_tried_using_gpt2_to_make_a_textbased/,monsieurpooh,1560153074,[removed],0,1
595,2019-6-10,2019,6,10,16,byv2t6,Gotchas with Clustering,https://www.reddit.com/r/MachineLearning/comments/byv2t6/gotchas_with_clustering/,delta1epsilon,1560153200,,0,1
596,2019-6-10,2019,6,10,17,byvbee,Machine learning jokes,https://www.reddit.com/r/MachineLearning/comments/byvbee/machine_learning_jokes/,syed_imam,1560155218,,0,1
597,2019-6-10,2019,6,10,17,byvdu6,I am sort of beginning to learn Machine Learning. How should I proceed?,https://www.reddit.com/r/MachineLearning/comments/byvdu6/i_am_sort_of_beginning_to_learn_machine_learning/,abhishek-31,1560155826,"I had planned to do machine learning in this summer and so I need what all courses I should take up so that I become comfortable in making newer projects along with knowing the concepts well enough.

I have the following course suggestions from my friends:

1. Andrew Ng. COuse on Machine Learning and further his deep learning specialization.
2. University of Washington course on Coursera as well.
3. Udacity's course on Machine Learning.
4. Machine Learning hands-on from A to Z on Udemy.
5. Any other resources?

&amp;#x200B;

Actually, I had bought the course on Eduonix regarding the projects on machine learning so basically, once I have completed the course on machine learning, I could focus on building projects. I was planning to these courses in as less rate as possible so I was thinking of moving on with Coursera's course because there one could apply for financial aid and will get it for sure. On the other hand, if I move with Udacity's course on machine learning, it is a paid course.

So any help will be appreciated.

Also, I can spend like 10-12 hours daily on this and I need to watch results in a month or two. So please suggest a method that is best suited for shorter time periods. I am pursuing bachelor's and I need not go into details much.",0,1
598,2019-6-10,2019,6,10,18,byvrww,Which network to use for image segmentation?,https://www.reddit.com/r/MachineLearning/comments/byvrww/which_network_to_use_for_image_segmentation/,hp2304,1560159202,"I am working on binary image segmentation of grayscale images. I have just labeled my dataset (approx. 2376 images). And there is too much class imbalance in my dataset and also boundaries are complex too. I have so many questions.

\- Should I use transfer learning?

\- Which model to use? (U-net, deeplabv3, fcn, etc)

\- Which loss function to use to tackle the class imbalance?

Thanks in advance.",0,1
599,2019-6-10,2019,6,10,19,bywg6e,Multiple-action policy (RL),https://www.reddit.com/r/MachineLearning/comments/bywg6e/multipleaction_policy_rl/,jaromiru,1560164327,[removed],0,1
600,2019-6-10,2019,6,10,20,bywh6h,CogX Full Pass for sale - Offers,https://www.reddit.com/r/MachineLearning/comments/bywh6h/cogx_full_pass_for_sale_offers/,mrbearatlas,1560164518,"Hi I won a CogX ticket in a raffle but can no longer make it hence putting it up for sale. Full 3 day pass is worth 2000. 
Location is Kings Cross in London.

I'm open to offers as we are almost half way through day 1! PM me :) 

Payment can be in crypto or PayPal. 

I can confirm this is noooot a scam",0,1
601,2019-6-10,2019,6,10,20,bywlx8,Hey y' all! Wanna improve your data science and machine learning skills?,https://www.reddit.com/r/MachineLearning/comments/bywlx8/hey_y_all_wanna_improve_your_data_science_and/,moosend,1560165441,[removed],0,1
602,2019-6-10,2019,6,10,20,bywqxp,Hey y' all! What are you reading on Machine Learning these days?,https://www.reddit.com/r/MachineLearning/comments/bywqxp/hey_y_all_what_are_you_reading_on_machine/,moosend,1560166416,[removed],0,1
603,2019-6-10,2019,6,10,21,byx253,How to add your dataset on the iExec Data Store?,https://www.reddit.com/r/MachineLearning/comments/byx253/how_to_add_your_dataset_on_the_iexec_data_store/,jbrg,1560168472,,0,1
604,2019-6-10,2019,6,10,21,byx3ie,[R] Multiple-action policy (RL),https://www.reddit.com/r/MachineLearning/comments/byx3ie/r_multipleaction_policy_rl/,jaromiru,1560168718,"In the following work, authors propose a simple trick to sample multiple actions in *linear time* in every step and train the model with policy gradient. It is proposed ""by the way"", but to me, it is a very important contribution to the RL itself. See pages 4 and 5 of [https://arxiv.org/abs/1905.12916](https://arxiv.org/abs/1905.12916) (Chen et al., Effective Medical Test Suggestions Using Deep Reinforcement Learning, 2019).

The main trick is, instead of outputing softmax probability distribution directly, to output sigmoid values (0-1) and sample this Bernoulli distribution. The authors then show how to make a proper probability distribution of this exponential action-space and subsequently do the policy gradient (both is very simple).",5,3
605,2019-6-10,2019,6,10,21,byx3yn,eGPU + T480 vs PC,https://www.reddit.com/r/MachineLearning/comments/byx3yn/egpu_t480_vs_pc/,orekrogue,1560168798," Hi everyone,  
I am at a point where I need to upgrade my machine(it's 9 years old) in order to cut waiting time as much as possible, and I was gonna go for PC with rtx2070, but then I realized...I don't game at all. So wouldn't it be better to go for egpu rtx2070 with some used ThinkPad t470/480 i would use for work and college?? Price would be very comparable, I might even be able to save some selling my old ThinkPad X1 2nd gen.  


My only concern is - how much performance would I lose? How much of a bottleneck in thunderbolt/U-series processors could I expect - would it still be worth it?  


Thanks in advance",0,1
606,2019-6-10,2019,6,10,21,byxdtt,ICML 2019 live stream?,https://www.reddit.com/r/MachineLearning/comments/byxdtt/icml_2019_live_stream/,AhmedElGazzar,1560170563,[removed],0,1
607,2019-6-10,2019,6,10,22,byxoww,[N] Feel at CVPR as if you were at CVPR! How to receive the CVPR Daily magazine,https://www.reddit.com/r/MachineLearning/comments/byxoww/n_feel_at_cvpr_as_if_you_were_at_cvpr_how_to/,Gletta,1560172362,"**CVPR 2019**, the Computer Vision and Pattern Recognition conference, will take place in Long Beach, CA in just a week.

If you are not going to CVPR, you can still receive every day the **CVPR Daily** in your mailbox. We will send you the link to the fresh magazine every morning and you will know the highlights of what is going on at the conference.

[**Register here, it's free!**](https://www.rsipvision.com/feel-at-cvpr-without-being-at-cvpr/)

You can also subscribe colleagues and friends who would like to receive great technology and community updates from CVPR 2019.

Enjoy!

&amp;#x200B;

https://i.redd.it/u0aju9ue3j331.jpg",0,5
608,2019-6-10,2019,6,10,22,byxsbl,Double Depth Telescopic Forks for load handling devices or Telescopic Forks,https://www.reddit.com/r/MachineLearning/comments/byxsbl/double_depth_telescopic_forks_for_load_handling/,lhd121,1560172921,,0,1
609,2019-6-10,2019,6,10,22,byy078,Is there a foolproof way to reproduce results in RNN using Keras?,https://www.reddit.com/r/MachineLearning/comments/byy078/is_there_a_foolproof_way_to_reproduce_results_in/,tiredofquoraguy,1560174133,"So I am doing univariate time series analysis using RNN LSTM on Keras. However, for same product, the results are coming different for different iterations. Is there any way to get constant results?
Thanks a lot in advance :)",0,1
610,2019-6-10,2019,6,10,23,byyey3,Information on Private vs. Public ML Projects,https://www.reddit.com/r/MachineLearning/comments/byyey3/information_on_private_vs_public_ml_projects/,EquityDarling,1560176392,"I'm looking for some information about how much machine learning research and innovation is carried out by private companies/researchers, as opposed to research organisations (public universities, etc.) in Europe (US data could also be interesting). Does anyone have some sources on this? 

Also, I was wondering whether someone knows about an interesting project using ML by an independent researcher not linked to a company or an institution.",0,1
611,2019-6-11,2019,6,11,0,byz0p9,BatchNorm + Dropout = DNN Success!,https://www.reddit.com/r/MachineLearning/comments/byz0p9/batchnorm_dropout_dnn_success/,Yuqing7,1560179545,,0,1
612,2019-6-11,2019,6,11,0,byz85e,[D] Find text in long audio clip.,https://www.reddit.com/r/MachineLearning/comments/byz85e/d_find_text_in_long_audio_clip/,marksbren,1560180623,Are there any ML models or tools that take a sentence (text) and 1-hour audio clip (mp3/wav) as input and output the time span(s) where that sentence is said in the audio?,6,2
613,2019-6-11,2019,6,11,0,byz871,[D] eGPU + T480 vs PC,https://www.reddit.com/r/MachineLearning/comments/byz871/d_egpu_t480_vs_pc/,orekrogue,1560180629,"Hi everyone,  
I am at a point where I need to upgrade my machine(it's 9 years old) in order to cut waiting time as much as possible, and I was gonna go for PC with rtx2070, but then I realized...I don't game at all and PC is not portable at all. So wouldn't it be better to go for egpu rtx2070 with some used ThinkPad t470/480 i would use for work and college?? Price would be very comparable, I might even be able to save some money selling my old ThinkPad X1 2nd gen.

My only concern is - how much performance would I lose? How much of a bottleneck in thunderbolt/U-series processors/thermals could I expect - would it still be worth it?

Thanks in advance",7,2
614,2019-6-11,2019,6,11,0,byzb9m,"What to optimize first in Deep Learning, sampling or architecture?",https://www.reddit.com/r/MachineLearning/comments/byzb9m/what_to_optimize_first_in_deep_learning_sampling/,tonifuc3m,1560181071,[removed],0,1
615,2019-6-11,2019,6,11,0,byzbo6,[P] Marketing Science: One of the ways we use our ML scores to optimize marketing campaigns,https://www.reddit.com/r/MachineLearning/comments/byzbo6/p_marketing_science_one_of_the_ways_we_use_our_ml/,datadem,1560181129,,0,1
616,2019-6-11,2019,6,11,0,byzgkm,[P] Marketing Science: How we use our ML scores to optimize marketing campaigns,https://www.reddit.com/r/MachineLearning/comments/byzgkm/p_marketing_science_how_we_use_our_ml_scores_to/,datadem,1560181821,"I have [previously](https://old.reddit.com/r/MachineLearning/comments/ae7nd7/p_building_thousands_of_reproducible_ml_models/) posted about our internal Machine Learning pipeline, pipe, and its technical infrastructure. We, as data/marketing scientists at my company, also spend time on making sure that the ML scores are used in interventions. I found there to be many posts about how to build ML models yet very few about how to set up experiments and what optimization actually looks like.

So I [wrote](https://data.blog/2019/06/10/using-ml-for-campaign-optimization-our-journey-to-marketing-science-at-automattic/) one for a simple setup.

Also, feel free to AMA about working as a remote data scientist.",25,151
617,2019-6-11,2019,6,11,0,byzhs3,Google at ICML 2019,https://www.reddit.com/r/MachineLearning/comments/byzhs3/google_at_icml_2019/,sjoerdapp,1560181998,,0,1
618,2019-6-11,2019,6,11,1,byzna9,Find someone to learn R together,https://www.reddit.com/r/MachineLearning/comments/byzna9/find_someone_to_learn_r_together/,Questceque_cest,1560182767,I'm going to do a master in 2 months and before it begins I'd love to add R to my toolkit. I believe that it would be much more productive if I can find someone to learn together with and maybe also team up to do some kaggle competitions. Anyone interested?,0,1
619,2019-6-11,2019,6,11,1,byznbi,[P] TF-IDF for tabular data featurization &amp; classification [Project],https://www.reddit.com/r/MachineLearning/comments/byznbi/p_tfidf_for_tabular_data_featurization/,bdilday-enigma,1560182771,This links to a post on some experimentation we did with tf-idf for classifying data in columns of a table. [https://medium.com/enigma-engineering/tf-idf-for-tabular-data-featurization-classification-4cd5b034ce62](https://medium.com/enigma-engineering/tf-idf-for-tabular-data-featurization-classification-4cd5b034ce62),0,10
620,2019-6-11,2019,6,11,1,byzrfo,[D] Effect of chaining multiple transformers (attention),https://www.reddit.com/r/MachineLearning/comments/byzrfo/d_effect_of_chaining_multiple_transformers/,mellow54,1560183333,"For recurrent neural networks (RNNs) increasing the number of units allows the network to (better) model a relationship over more distant inputs in an input sequence.

***However what's the effect of increasing the number of layers in a transformer?*** Since the transformer looks at multiple inputs of the sequence simultaneously at each layer - it doesn't have an analogue with RNNs.",4,2
621,2019-6-11,2019,6,11,1,byzwx0,ICML Live Stream URL,https://www.reddit.com/r/MachineLearning/comments/byzwx0/icml_live_stream_url/,shashankrajput,1560184080,[removed],0,1
622,2019-6-11,2019,6,11,1,bz0122,[D] Help with YOLO + TensorFlow records,https://www.reddit.com/r/MachineLearning/comments/bz0122/d_help_with_yolo_tensorflow_records/,andrelan,1560184656,"Hello everyone,  
I apologize for my incorrect English, I hope you understand my problem anyway.  
I  state that I am new to the machine learning sector, being a university  student. I'm looking for some repository or someone to explain to me  step by step (in short, something already ready) how to use my .record  (Tensorflow records) with any YOLO algorithm in Object Detection.  
I  tried almost any Git repository found on the web regarding this problem  but 99% do not use Tensorflow records and those few who used them had  errors in the execution of the various phases of training, detection etc  ...  
My dataset, as I have already heard, is formed by three .record  (train, validation, test), where we have for each Bounding Box each  image: Xmin, Xmax, Ymin, Ymax, Width, Height and a total of 3 classes  (low, medium, high).  
I ask you this because otherwise I would be  forced to create xml files for each image and then use the classic  Pascal VOC notation for training.  
I can use Google Colab without problems.  
I hope I was clear, thank you in advance !!",5,1
623,2019-6-11,2019,6,11,2,bz0cow,How can memory networks perform well in lists/set type tasks?,https://www.reddit.com/r/MachineLearning/comments/bz0cow/how_can_memory_networks_perform_well_in_listsset/,aasgr,1560186247,[removed],0,1
624,2019-6-11,2019,6,11,2,bz0xnv,semantic similarity of articles with neural networks,https://www.reddit.com/r/MachineLearning/comments/bz0xnv/semantic_similarity_of_articles_with_neural/,razum-bak,1560189051,[removed],1,1
625,2019-6-11,2019,6,11,2,bz0zx1,Network inference on large images,https://www.reddit.com/r/MachineLearning/comments/bz0zx1/network_inference_on_large_images/,Alex-S-S,1560189367,"I have a question about applying trained models on large images. Typically, neural networks are trained with small input sizes like 128x128 ... 512x512, and they rarely go above that. How do you use a trained network on a larger image than the input size? For example: how do you a network trained on 256x256 on a 4k image?",0,1
626,2019-6-11,2019,6,11,3,bz13cr,[Project] An introductory guide to using machine learning to predict sports outcomes (cricket),https://www.reddit.com/r/MachineLearning/comments/bz13cr/project_an_introductory_guide_to_using_machine/,DaveatAuquan,1560189831,"Hey guys,

I've written a guide to a basic implementation of machine learning to predict if a ball will be a wicket in a cricket match. In case anyone here is a Moneyball fan who wants to give it a go, I'll post here. Details about how to get the dataset etc are in the article.

https://medium.com/auquan/a-guided-approach-to-using-machine-learning-for-cricket-wicket-prediction-5ff0e0e2313d?postPublishedType=initial",2,0
627,2019-6-11,2019,6,11,3,bz1eax,How to predict user from session log dataset ?,https://www.reddit.com/r/MachineLearning/comments/bz1eax/how_to_predict_user_from_session_log_dataset/,ashutosj,1560191287,[removed],0,1
628,2019-6-11,2019,6,11,3,bz1hfr,How would i go about building a machine learning tool to automate Amazon Pay Per Click advertising?,https://www.reddit.com/r/MachineLearning/comments/bz1hfr/how_would_i_go_about_building_a_machine_learning/,rawrtherapy,1560191700,[removed],0,1
629,2019-6-11,2019,6,11,3,bz1rob,[P] PyTorch Hub: Towards Reproducible Research,https://www.reddit.com/r/MachineLearning/comments/bz1rob/p_pytorch_hub_towards_reproducible_research/,rosstaylor90,1560193092,"[https://pytorch.org/blog/towards-reproducible-research-with-pytorch-hub/](https://pytorch.org/blog/towards-reproducible-research-with-pytorch-hub/)

&amp;#x200B;

""Reproducibility is an essential requirement for many fields of research including those based on machine learning techniques. However, many machine learning publications are either not reproducible or are difficult to reproduce. With the continued growth in the number of research publications, including tens of thousands of papers now hosted on arXiv and submissions to conferences at an all time high, research reproducibility is more important than ever. While many of these publications are accompanied by code as well as trained models which is helpful but still leaves a number of steps for users to figure out for themselves.

We are excited to announce the availability of PyTorch Hub, a simple API and workflow that provides the basic building blocks for improving machine learning research reproducibility. PyTorch Hub consists of a pre-trained model repository designed specifically to facilitate research reproducibility and enable new research. It also has built-in support for Colab, integration with Papers With Code and currently contains a broad set of models that include Classification and Segmentation, Generative, Transformers, and more.""",3,68
630,2019-6-11,2019,6,11,3,bz1s4n,How to build an effective multi class image classification?,https://www.reddit.com/r/MachineLearning/comments/bz1s4n/how_to_build_an_effective_multi_class_image/,sprx7767,1560193154,[removed],0,1
631,2019-6-11,2019,6,11,4,bz1zlx,Question about amazon lex chat bot.,https://www.reddit.com/r/MachineLearning/comments/bz1zlx/question_about_amazon_lex_chat_bot/,EpicCorpseMan,1560194150,[removed],0,1
632,2019-6-11,2019,6,11,4,bz21ih,"Introduction to Statistical Machine Learning Notes by Andreas Lindholm, Uppsala University",https://www.reddit.com/r/MachineLearning/comments/bz21ih/introduction_to_statistical_machine_learning/,ai-lover,1560194400,"The course is focusing on supervised learning, i.e, classification, and regression. The course will cover a range of methods used in machine learning and data science, including:  
Linear regression (including ridge regression and the Lasso)

Classification via logistic regression and k nearest neighbor

Linear and quadratic discriminant analysisRegression and classification trees (including bagging and random forests)

Boosting

  
Credit to Uppsalla University

# [DOWNLOAD LINK](https://lookaside.fbsbx.com/file/Introduction%20to%20Statistical%20Machine%20Learning-%20Andreas.pdf?token=AWwz7uXHUR1nRcduAnZ2Mkl8YE7adOVX2Ibgd4g7pz43V1KJvnmbVom3Z5KY64fU5Qqi8h7vF9PK8FOq5XOREuQ2cGNv2Jovpppjt1boDqchEphrlGKovyLiuiSfSvkW1V0vQBn0hLrZ52Iq1BsXLowMSguhRh8g08YdoWzRzr-jCA)

&amp;#x200B;

https://i.redd.it/nx97rligxk331.png",0,1
633,2019-6-11,2019,6,11,4,bz243i,[D] Gavin Miller: Adobe Research | Artificial Intelligence Podcast,https://www.reddit.com/r/MachineLearning/comments/bz243i/d_gavin_miller_adobe_research_artificial/,UltraMarathonMan,1560194748,"Gavin Miller is the Head of Adobe Research. Adobe have empowered artists, designers, and creative minds from all professions working in the digital medium for over 30 years with software such as Photoshop, Illustrator, Premiere, After Effects, InDesign, Audition that work with images, video, and audio. Adobe Research is working to define the future evolution of these products in a way that makes the life of creatives easier, automates the tedious tasks, and gives more &amp; more time to operate in the idea space instead of pixel space. This is where the cutting-edge deep learning methods of the past decade can shine more than perhaps any other application. Gavin is the embodiment of combing tech and creativity. Outside of Adobe Research, he writes poetry &amp; builds robots. 

**Video:** https://www.youtube.com/watch?v=q0mokx-iiws 

https://i.redd.it/9adlhbz7yk331.png

**Outline:**

0:00 - Introduction

1:11 - Poetry &amp; crossover to creative work

6:35 - Turning one medium into another

7:45 - Creative process in both the space pixels and ideas

10:00 - Improving workflow in Adobe tools with AI

14:31 - Taking ideas from prototype to product

16:22 - Learning how to use Adobe tools

21:13 - Applications of deep learning

28:46 - Improving user experience from data

34:30 - Augmented reality and virtual reality

39:57 - Resistance to change

43:40 - Poem - Today I Left My Phone at Home

44:17 - Illusion of beauty in digital space

49:17 - Secret to a thriving research lab

55:27 - Future ideas in Adobe Research

58:13 - Robotics and animation in the physical world

1:08:01 - Poem - Cast My Ashes Wide and Far",1,24
634,2019-6-11,2019,6,11,4,bz25sa,Need help with a multi class image classifier with tensorflow.,https://www.reddit.com/r/MachineLearning/comments/bz25sa/need_help_with_a_multi_class_image_classifier/,sprx7767,1560194977,[removed],0,1
635,2019-6-11,2019,6,11,4,bz26va,[D] Using same model in both implementation and evaluation,https://www.reddit.com/r/MachineLearning/comments/bz26va/d_using_same_model_in_both_implementation_and/,AnonMLstudent,1560195130,"Let's say for my research project I use a model in part of my implementation (e.g. To calculate semantic similarities between sentences) since it is the best suited for the particular task at hand. Then in the evaluation of my implementation and those of my competitors, can I use the same model (e.g. To calculate semantic similarities between output and input) or would this be inappropriate since it was used in my own implementation? It wouldn't make much sense to use a different model for the evaluation since it is the ""best suited"" both for implementation and evaluation. What should the approach be here?",0,0
636,2019-6-11,2019,6,11,4,bz2bku,"No one knows where Americas helipads are, except this neural network",https://www.reddit.com/r/MachineLearning/comments/bz2bku/no_one_knows_where_americas_helipads_are_except/,ddrum001,1560195794,[removed],0,1
637,2019-6-11,2019,6,11,5,bz2iwa,[R] Computer Vision with a Single Robust Classifier,https://www.reddit.com/r/MachineLearning/comments/bz2iwa/r_computer_vision_with_a_single_robust_classifier/,andrew_ilyas,1560196810,"Blog Post: [http://gradientscience.org/robust\_apps/](http://gradientscience.org/robust_apps/)

Paper: [http://gradientscience.org/robust-apps.pdf](http://gradientscience.org/robust-apps.pdf)

TL;DR: Bunch of Computer Vision applications (generation, superresolution, inpainting, etc.) with just a single robustly trained classifier, straightforwardly scales to (1K-class, 224px) ImageNet.

&amp;#x200B;

We show that a single classifier trained on a standard dataset can be leveraged for diverse computer vision applications. Using \*only\* an adversarially trained classifier (no generative architecture, just a standard ResNet trained with cross-entropy loss), we show that we can perform image generation, super resolution, inpainting, and interactive editing. The approach shows no instability and trivially scales to full (224x224) ImageNet. Our results suggest the robust classification framework as a viable alternative to more complex or task-specific approaches.",23,124
638,2019-6-11,2019,6,11,6,bz3u7a,[D] STOA Hand Pose detection model?,https://www.reddit.com/r/MachineLearning/comments/bz3u7a/d_stoa_hand_pose_detection_model/,push_limits__13,1560203413,"I wanted to detect hand pose from on a video, on RGB images. What models perform the best? 

Any links to code/ datasets that are used for hand pose detection would be great! 

Thanks a lot!",2,0
639,2019-6-11,2019,6,11,8,bz4x30,Using personal labelling tool with Amazon Mechanical Turk (AMT),https://www.reddit.com/r/MachineLearning/comments/bz4x30/using_personal_labelling_tool_with_amazon/,ruderr,1560209090,[removed],0,1
640,2019-6-11,2019,6,11,8,bz4zwi,[QUESTION] Stacking models,https://www.reddit.com/r/MachineLearning/comments/bz4zwi/question_stacking_models/,4govanism,1560209515,[removed],0,1
641,2019-6-11,2019,6,11,8,bz590g,How do you actually handle text and dates?,https://www.reddit.com/r/MachineLearning/comments/bz590g/how_do_you_actually_handle_text_and_dates/,editreddit2,1560210871,[removed],0,1
642,2019-6-11,2019,6,11,9,bz5un0,[D] Should I do a journal review as a 2nd year PhD student?,https://www.reddit.com/r/MachineLearning/comments/bz5un0/d_should_i_do_a_journal_review_as_a_2nd_year_phd/,schludy,1560214308,"I got a request for review for Journal of Machine Learning Research. I just finished my 3rd semester, I wrote some conference reviews which my advisor said were quite good. My advisor also said that reviewing a journal paper might take me a full week... Is it worth to do it? Can I get into trouble if the review is qualitatively not great?",11,10
643,2019-6-11,2019,6,11,10,bz62lu,[P] RUNN (game),https://www.reddit.com/r/MachineLearning/comments/bz62lu/p_runn_game/,hardmaru,1560215614,"Someone made an interactive browser-based scroller game where the levels are based on music generated by a recurrent neural network model.

game: https://vibertthio.com/runn/

(I tested it on Chrome- seems to have issues with other browsers)

code: https://medium.com/@vibertthio/i-built-2-games-to-make-machine-learning-fun-3668ef871eae

blog: https://medium.com/@vibertthio/i-built-2-games-to-make-machine-learning-fun-3668ef871eae",4,21
644,2019-6-11,2019,6,11,10,bz6hsi,[R] An Efficient Graph Convolutional Network Technique for the Travelling Salesman Problem,https://www.reddit.com/r/MachineLearning/comments/bz6hsi/r_an_efficient_graph_convolutional_network/,chaitjo,1560218163,,9,23
645,2019-6-11,2019,6,11,11,bz6o6y,INTRODUCCIN AL #BIGDATA,https://www.reddit.com/r/MachineLearning/comments/bz6o6y/introduccin_al_bigdata/,jeffry_30,1560219199," Big Data naci con el objetivo de cubrir unas necesidades no satisfechas por las tecnologas existentes, como es el almacenamiento y tratamiento de grandes volmenes de datos que poseen unas caractersticas muy concretas definidas como las tres Vs (puede haber ms.

Les presento un alcance sobre el tema muy didctico, espero les sirva:

[Resumen de big data](https://aidst.blogspot.com/2019/03/introduccion-al-big-data.html)",0,1
646,2019-6-11,2019,6,11,11,bz6q53,Node embedding generation in graphs,https://www.reddit.com/r/MachineLearning/comments/bz6q53/node_embedding_generation_in_graphs/,preetham_salehundam,1560219519,[removed],0,1
647,2019-6-11,2019,6,11,11,bz6yvx,[P] Reinforcement Learning in Non-Stationary Environments,https://www.reddit.com/r/MachineLearning/comments/bz6yvx/p_reinforcement_learning_in_nonstationary/,iamiamwhoami,1560220925,"https://arxiv.org/abs/1905.03970

I just started reading it. Wanted to get some other people's thoughts. Seems like it's attempting to solve a pretty significant problem.",0,1
648,2019-6-11,2019,6,11,11,bz6zkh,[R] Reinforcement Learning in Non-Stationary Environments,https://www.reddit.com/r/MachineLearning/comments/bz6zkh/r_reinforcement_learning_in_nonstationary/,iamiamwhoami,1560221038,"https://arxiv.org/abs/1905.03970

I just started reading it and I wanted to get other people's thoughts. Seems like it's solving a pretty significant problem.",17,30
649,2019-6-11,2019,6,11,12,bz77eo,"[D] Does AMD have now or plan to have any product in the GPU-for-Machine-Learning space, or is nVidia the only game in town?",https://www.reddit.com/r/MachineLearning/comments/bz77eo/d_does_amd_have_now_or_plan_to_have_any_product/,JoeRogansPituitary,1560222337,,58,88
650,2019-6-11,2019,6,11,12,bz7axw,Interactive visualization of the iris data set [Project],https://www.reddit.com/r/MachineLearning/comments/bz7axw/interactive_visualization_of_the_iris_data_set/,hageldave,1560222927,,1,1
651,2019-6-11,2019,6,11,12,bz7ov4,"[N] Daily, De-Duplicated arXiv RSS Updates",https://www.reddit.com/r/MachineLearning/comments/bz7ov4/n_daily_deduplicated_arxiv_rss_updates/,vstuart,1560225505,"Hi everyone.  In case this is useful to anyone here, I wrote a BASH script for automated downloading of selected arXiv RSS feeds.

That content is parsed into two documents:

  * keyword-matched articles of interest;
  * the remaining articles.

The script can be scheduled to run daily via crontab, or manually executed.

By example, today among five arXiv RSS feeds my RSS reader provided 690 entries (including duplicated, cross-posted entries) while my script returned 344 de-duplicated (unique) articles.

* script: https://persagen.com/files/misc/arxiv-rss.sh
* accompanying research blog post: https://persagen.com/2019/06/10/arxiv-rss.html",1,24
652,2019-6-11,2019,6,11,13,bz7rwj,[P] A Gentle Guide to Starting Your NLP Project with AllenNLP,https://www.reddit.com/r/MachineLearning/comments/bz7rwj/p_a_gentle_guide_to_starting_your_nlp_project/,yasufumy,1560226028,,0,1
653,2019-6-11,2019,6,11,13,bz83kf,Reinforcement Learning On Road Traffic System,https://www.reddit.com/r/MachineLearning/comments/bz83kf/reinforcement_learning_on_road_traffic_system/,amogh_13,1560228156,[removed],0,1
654,2019-6-11,2019,6,11,14,bz8bi9,Teaching a drone to fly by itself project,https://www.reddit.com/r/MachineLearning/comments/bz8bi9/teaching_a_drone_to_fly_by_itself_project/,kingslayyer,1560229644,[removed],0,1
655,2019-6-11,2019,6,11,14,bz8j7y,"[1906.03352] Watch, Try, Learn: Meta-Learning from Demonstrations and Reward",https://www.reddit.com/r/MachineLearning/comments/bz8j7y/190603352_watch_try_learn_metalearning_from/,sensetime,1560231171,,1,7
656,2019-6-11,2019,6,11,14,bz8nhj,[D] Is there a way for a Neural Net (or other approach) to handle interchangeable features explicitly?,https://www.reddit.com/r/MachineLearning/comments/bz8nhj/d_is_there_a_way_for_a_neural_net_or_other/,HomieSapien,1560232018,"Let's say I have 3 features describing some value of 3 objects of importance. The 3 objects are identical. 

In my current model, the three objects are basically randomly assigned to be the features object\_0, object\_1, object\_2.

It doesn't matter for the real life outcome which object is 0, 1 , or 2. Of course, the neural net will learn that these are interchangeable in theory, but is there a way people have been doing this in a more explicit way?   
Would it improve training to, for example, always order the values from largest to smallest, 0-2? Or some manipulation like that?

If this is a terrible question or you have corrections about the way I asked it, feel free to let me know. I am still learning.

:)

Thanks",4,12
657,2019-6-11,2019,6,11,14,bz8o1d,Car Color Detection,https://www.reddit.com/r/MachineLearning/comments/bz8o1d/car_color_detection/,adityak2920,1560232134,[removed],0,1
658,2019-6-11,2019,6,11,15,bz8sak,[R] Metric learning and person re-identification in CVPR 2019,https://www.reddit.com/r/MachineLearning/comments/bz8sak/r_metric_learning_and_person_reidentification_in/,kdhht2334,1560233020,"Hi, guys!

From now on, **CVPR 2019 open access** is accessible!

In addition to advanced metric learning research, many researches have been conducted on the real-world application such as **person re-identification** and **image registration**.

Through the following links, we can look at the work of CVPR 2019 in addition to the existing metric learning studies.

[https://github.com/kdhht2334/Survey\_of\_Deep\_Metric\_Learning](https://github.com/kdhht2334/Survey_of_Deep_Metric_Learning)",0,5
659,2019-6-11,2019,6,11,15,bz8xrd,"Dear voice conversion experts, would this simplified task be feasible with existing open source tools?",https://www.reddit.com/r/MachineLearning/comments/bz8xrd/dear_voice_conversion_experts_would_this/,dualwieldranger,1560234178,[removed],0,1
660,2019-6-11,2019,6,11,15,bz92iv,How can I install Vizdoom (Doom Environment) for reinforcement learning on Windows?,https://www.reddit.com/r/MachineLearning/comments/bz92iv/how_can_i_install_vizdoom_doom_environment_for/,masterRJ2404,1560235181,"Currently I am studying reinforcement learning and I am planning to make an AI agent which can play Doom. But after enough of googling I was not able to find out any resources related to installation of Doom Environment except this

 https://www.michaelkrax.com/blog/how-to-run-vizdoom-on-windows-10/

which I think is pretty much complex. I have already installed ""openai-gym"" and ""openai-retro""  in my laptop so is there any way that Doom environment is already available in it. If it is available them how can I use it or is there any less complex procedure to install vizdoom.",0,1
661,2019-6-11,2019,6,11,16,bz98f7,Clustering before training RNN?,https://www.reddit.com/r/MachineLearning/comments/bz98f7/clustering_before_training_rnn/,Sonderskov,1560236458,[removed],0,1
662,2019-6-11,2019,6,11,16,bz9meo,How to build trigger word dataset?,https://www.reddit.com/r/MachineLearning/comments/bz9meo/how_to_build_trigger_word_dataset/,jonyfromdablock,1560239563,[removed],0,1
663,2019-6-11,2019,6,11,16,bz9n4k,Making AWS Sagemaker to work properly,https://www.reddit.com/r/MachineLearning/comments/bz9n4k/making_aws_sagemaker_to_work_properly/,rbmsingh,1560239728,[removed],0,1
664,2019-6-11,2019,6,11,18,bzac2m,Single depth Telescopic forks for Load Handling Devices,https://www.reddit.com/r/MachineLearning/comments/bzac2m/single_depth_telescopic_forks_for_load_handling/,lhd121,1560245664,[removed],0,1
665,2019-6-11,2019,6,11,18,bzaczx,Document(text) similarity for Coding Interview Questions?,https://www.reddit.com/r/MachineLearning/comments/bzaczx/documenttext_similarity_for_coding_interview/,JakubJancto,1560245880,[removed],0,1
666,2019-6-11,2019,6,11,19,bzasix,[D] Deep Learning with Python - Francois Chollet and other book recommendations/ reviews,https://www.reddit.com/r/MachineLearning/comments/bzasix/d_deep_learning_with_python_francois_chollet_and/,ssd123456789,1560249152,"Hi everyone! 

I am looking to get the book by Francois Chollet and I was just wondering if it is worth buying or not. Also, how detailed is it and what level does it start at? 

I know the basics of deep learning and keras and I'd say that I can get things done in python comfortably. Would you recommend it for someone at this level? 

Also I'm open to other suggestions. I'm looking for something that goes into a fair bit of detail on deep learning. 

Thank you!",5,2
667,2019-6-11,2019,6,11,19,bzay6f,[P] Learn Evolutionary Algorithms in 10 minutes,https://www.reddit.com/r/MachineLearning/comments/bzay6f/p_learn_evolutionary_algorithms_in_10_minutes/,AI_with_demons,1560250341,,0,1
668,2019-6-11,2019,6,11,19,bzayap,CNN to detect number of Polygonvertices,https://www.reddit.com/r/MachineLearning/comments/bzayap/cnn_to_detect_number_of_polygonvertices/,Centauri24,1560250366,[removed],0,1
669,2019-6-11,2019,6,11,20,bzb8yr,How do i select an CNN architecture suiting my specific usecase?,https://www.reddit.com/r/MachineLearning/comments/bzb8yr/how_do_i_select_an_cnn_architecture_suiting_my/,2ringo,1560252368,[removed],0,1
670,2019-6-11,2019,6,11,20,bzbiwq,[P] Using tf-idf to analyse economic documents,https://www.reddit.com/r/MachineLearning/comments/bzbiwq/p_using_tfidf_to_analyse_economic_documents/,plentyofnodes,1560254137,"This is a recent analysis I conducted on economic bulletins from the ECB in pdf format. pdf2txt was used to convert into text format, the text was appropriately processed in Python, and then tf-idf was used to rank terms which were then incorporated into a word cloud. The intention behind this is to extract key terms from a document quickly, e.g. **tariffs, downturn, debt**, etc.

Would appreciate your opinions!

Link: [Summarizing Economic Bulletin Documents with tf-idf](https://www.michael-grogan.com/nlp-economics/)",9,24
671,2019-6-11,2019,6,11,20,bzbjla,Business Intelligence Services and Solutions for All!,https://www.reddit.com/r/MachineLearning/comments/bzbjla/business_intelligence_services_and_solutions_for/,ElegantMicroWebIndia,1560254256,,0,1
672,2019-6-11,2019,6,11,21,bzbky1,[R] A Survey of Reinforcement Learning Informed by Natural Language,https://www.reddit.com/r/MachineLearning/comments/bzbky1/r_a_survey_of_reinforcement_learning_informed_by/,_rockt,1560254489,,1,92
673,2019-6-11,2019,6,11,21,bzbnig,[P] Interactive visualization of the iris data set,https://www.reddit.com/r/MachineLearning/comments/bzbnig/p_interactive_visualization_of_the_iris_data_set/,hageldave,1560254906,,1,1
674,2019-6-11,2019,6,11,21,bzbs90,[P] Interactive visualization of the iris data set,https://www.reddit.com/r/MachineLearning/comments/bzbs90/p_interactive_visualization_of_the_iris_data_set/,hageldave,1560255718,,1,1
675,2019-6-11,2019,6,11,21,bzbstu,Research in AI,https://www.reddit.com/r/MachineLearning/comments/bzbstu/research_in_ai/,itgirl007,1560255822,[removed],0,1
676,2019-6-11,2019,6,11,21,bzc0dv,Documentations or researches to learn about AI powered Logo generator?,https://www.reddit.com/r/MachineLearning/comments/bzc0dv/documentations_or_researches_to_learn_about_ai/,ilyesH,1560257060,[removed],0,1
677,2019-6-11,2019,6,11,21,bzc0pj,[R] Stupid question about training/test data to check feature importance.,https://www.reddit.com/r/MachineLearning/comments/bzc0pj/r_stupid_question_about_trainingtest_data_to/,Frogad,1560257112,"Hi, I'm fairly new to this and planning on using random forests to try and see which variables most affect the outcome of another variable.

In this case, I can't exactly see what the training data would be used for.

https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e

I don't really understand what splitting my data would achieve, can someone explain?",3,4
678,2019-6-11,2019,6,11,22,bzcf43,"[P] Simple Tensorflow implementation of ""Image-to-Image Translation via Group-wise Deep Whitening-and-Coloring Transformation"" (CVPR 2019 Oral)",https://www.reddit.com/r/MachineLearning/comments/bzcf43/p_simple_tensorflow_implementation_of/,taki0112,1560259348,"&amp;#x200B;

[Comparison with baselines on Artworks dataset](https://i.redd.it/lzc50cxv9q331.png)

&amp;#x200B;

[Comparison with baselines on CelebA dataset](https://i.redd.it/zycen7rx9q331.png)",1,1
679,2019-6-11,2019,6,11,23,bzd7fx,[P] understanding how to output steering angles,https://www.reddit.com/r/MachineLearning/comments/bzd7fx/p_understanding_how_to_output_steering_angles/,theThinker6969,1560263442,"Hey all,
im following the [Nvidia 2016 paper](https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf)

I have created a similar model architecture they are using 

i have a dataset of images and steering_angles obtained from CARLA itself 
(carla is a self-driving car simulator)

Do you all have any tips on how they output a steering angle?


Thank you very much for your help",1,3
680,2019-6-11,2019,6,11,23,bzda7v,"[P] Simple Tensorflow implementation of ""Image-to-Image Translation via Group-wise Deep Whitening-and-Coloring Transformation"" (CVPR 2019 Oral)",https://www.reddit.com/r/MachineLearning/comments/bzda7v/p_simple_tensorflow_implementation_of/,taki0112,1560263837,"&amp;#x200B;

[Comparison with baselines on Artworks dataset](https://i.redd.it/tviqprisnq331.png)

&amp;#x200B;

[Comparison with baselines on CelebA dataset](https://i.redd.it/f27rpydvnq331.png)",0,1
681,2019-6-12,2019,6,12,0,bzdk42,BlockSwap: Fisher guided block substitution for network compression,https://www.reddit.com/r/MachineLearning/comments/bzdk42/blockswap_fisher_guided_block_substitution_for/,jw-turner,1560265224,"&amp;#x200B;

![img](jimx1507rq331 ""Many networks are composed of blocks. For compression, Moonshine [1] proposed substituting all blocks for a single type of substitute. We propose a method (BlockSwap) for choosing mixed block-type configurations."")

&amp;#x200B;

**Paper:** [https://arxiv.org/abs/1906.04113](https://arxiv.org/abs/1906.04113)

**PyTorch Code:** [https://github.com/BayesWatch/pytorch-blockswap](https://github.com/BayesWatch/pytorch-blockswap)

&amp;#x200B;

TL;DR: Compress overparameterised networks using Fisher information to rank randomly proposed alternatives.   

&amp;#x200B;

**Abstract:**

The desire to run neural networks on low-capacity edge devices has led to the development of a wealth of compression techniques. Moonshine is a simple and powerful example of this: one takes a large pre-trained network and substitutes each of its convolutional blocks with a selected cheap alternative block, then distills the resultant network with the original. However, not all blocks are created equally; for a required parameter budget there may exist a potent combination of many different cheap blocks. In this work, we find these by developing BlockSwap: an algorithm for choosing networks with interleaved block types by passing a single minibatch of training data through randomly initialised networks and gauging their Fisher potential. We show that block-wise cheapening yields more accurate networks than single block-type networks across a spectrum of parameter budgets.

&amp;#x200B;

\[1\] Crowley, Elliot J., Gavin Gray, and Amos J. Storkey. ""Moonshine: Distilling with cheap convolutions."" *Advances in Neural Information Processing Systems*. 2018.",0,1
682,2019-6-12,2019,6,12,0,bzdolg,[R] BlockSwap: Fisher guided block substitution for network compression,https://www.reddit.com/r/MachineLearning/comments/bzdolg/r_blockswap_fisher_guided_block_substitution_for/,jw-turner,1560265835,"&amp;#x200B;

[Many networks are composed of blocks. For compression, Moonshine \[1\] proposed substituting all blocks for a single type of substitute. We propose a method \(BlockSwap\) for choosing mixed block-type configurations.](https://i.redd.it/guyfo9aqtq331.png)

&amp;#x200B;

**Paper:** [https://arxiv.org/abs/1906.04113](https://arxiv.org/abs/1906.04113)

**PyTorch Code:** [https://github.com/BayesWatch/pytorch-blockswap](https://github.com/BayesWatch/pytorch-blockswap)

&amp;#x200B;

TL;DR: Compress overparameterised networks using Fisher information to rank randomly proposed alternatives.

&amp;#x200B;

**Abstract:**

The desire to run neural networks on low-capacity edge devices has led to the development of a wealth of compression techniques. Moonshine is a simple and powerful example of this: one takes a large pre-trained network and substitutes each of its convolutional blocks with a selected cheap alternative block, then distills the resultant network with the original. However, not all blocks are created equally; for a required parameter budget there may exist a potent combination of many different cheap blocks. In this work, we find these by developing BlockSwap: an algorithm for choosing networks with interleaved block types by passing a single minibatch of training data through randomly initialised networks and gauging their Fisher potential. We show that block-wise cheapening yields more accurate networks than single block-type networks across a spectrum of parameter budgets.

\[1\] Crowley, Elliot J., Gavin Gray, and Amos J. Storkey. ""Moonshine: Distilling with cheap convolutions."" *Advances in Neural Information Processing Systems*. 2018.",0,11
683,2019-6-12,2019,6,12,0,bzdvs1,Is anyone well informed of studies involving Incentive Mechanisms?,https://www.reddit.com/r/MachineLearning/comments/bzdvs1/is_anyone_well_informed_of_studies_involving/,kindnesd99,1560266840,[removed],0,1
684,2019-6-12,2019,6,12,0,bze5be,[P] Tensorflow implementation of WaveGlow with VQVAE,https://www.reddit.com/r/MachineLearning/comments/bze5be/p_tensorflow_implementation_of_waveglow_with_vqvae/,jaywalnut-310,1560268145,"code and samples: [https://github.com/jaywalnut310/waveglow-vqvae](https://github.com/jaywalnut310/waveglow-vqvae) 

&amp;#x200B;

Hi, I am newbie in here.

Anyway, I am currently working on combining VQVAE and WaveGlow.

WaveGlow is a great model to synthesize speech in a parallel way.

VQVAE is known to good at disentangling speaker identity and linguistic features from raw audio.

&amp;#x200B;

As I want to make an efficient multi-speaker voice synthesizer, I have been trying combining those two models.

There are a lot of remaining works though.

&amp;#x200B;

So far, What I found from my implementation is

\- For single speaker, it works quite well

\- For multi speakers, it doesn't seem to disentangle speaker identity and linguistic features.

I am trying to solve this issue at now, So if you have any idea, please let me know.

&amp;#x200B;

Additionally I slightly modified pure VQVAE method with Soft-EM like gradient descent method.

For now, it seems work quite well avoiding hyper parameter tuning and index collapse.

&amp;#x200B;

For more information, please see my repository

and if you're interested, please give me critic comments !",2,18
685,2019-6-12,2019,6,12,0,bze79g,Learning Individual Styles of Conversational Gesture,https://www.reddit.com/r/MachineLearning/comments/bze79g/learning_individual_styles_of_conversational/,amirbar,1560268428,"New CVPR paper on speech-to-gesture prediction! Human speech is often accompanied by hand and arm gestures. Given audio speech input, we generate plausible gestures to go along with the sound and synthesize a corresponding video of the speaker. We also release our full dataset and will make the code available.

project page:
http://people.eecs.berkeley.edu/~shiry/projects/speech2gesture/index.html",0,1
686,2019-6-12,2019,6,12,1,bzeb7u,What's your take on the information bottleneck theory for deep neural networks?,https://www.reddit.com/r/MachineLearning/comments/bzeb7u/whats_your_take_on_the_information_bottleneck/,elfungi_,1560268943,"I have been reading the papers by [Tishby and Shwartz](https://arxiv.org/abs/1703.00810) that claim that deep neural network training using SGD consists of 2 phases: one where the mutual information of the layers with Y increases relatively fast and one (which takes up most of the epochs) in which the network is actually compressing information and reducing the mutual information with the input. They claim that this compression is the reason for good generalization.

[Saxe](http://decisions.psy.ox.ac.uk/people/saxe_site/papers/Saxe%20et%20al.%20-%202018%20-%20On%20the%20Information%20Bottleneck%20Theory%20of%20Deep%20Learning.pdf), on the other hand, claims to disprove this and [Noshad](https://arxiv.org/abs/1801.09125) again claims to disprove Saxe since he seems to have calculated mutual information the wrong way.

I was looking through all their repos on Github and their experiments are not easily and 100% reproducible (e.g. data is missing) even with extensive work on the code. Who do you think is correct? Why? I think the theory in general sounds very compelling but there seems to be no clear consense about it.",0,1
687,2019-6-12,2019,6,12,1,bzeee7,New Facebook PyTorch Hub Facilitates Reproducibility Testing,https://www.reddit.com/r/MachineLearning/comments/bzeee7/new_facebook_pytorch_hub_facilitates/,Yuqing7,1560269391,,0,1
688,2019-6-12,2019,6,12,1,bzehge,How does a GMM-VAE work (Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders)?,https://www.reddit.com/r/MachineLearning/comments/bzehge/how_does_a_gmmvae_work_deep_unsupervised/,theonlyQuan,1560269799,"I am reading [Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders](https://arxiv.org/abs/1611.02648) , and I cannot grasp the idea presented in the paper, especially on how the KL loss is calculated.

&amp;#x200B;

As far as I understand (correct me if I am wrong, pretty sure I got some of these ideas wrong), the encoder outputs K sets of means and variances, and the probabilities for each set of distribution. We then sample z from the mixture distribution like a normal VAE, z has a shape of (batch\_size, latent\_size). The decoder then reconstructs the encoder input using z.

&amp;#x200B;

How is the KL loss computed here? In a usual VAE, we optimize the KL divergence between the distribution of z and a normal distribution with mean 0 and standard deviation 1. Since there are now K sets of means and variances, what is the prior? How is the mixing probabilities optimized?",0,1
689,2019-6-12,2019,6,12,1,bzevic,apricot: submodular selection for machine learning in Python,https://www.reddit.com/r/MachineLearning/comments/bzevic/apricot_submodular_selection_for_machine_learning/,ants_rock,1560271666,[removed],0,1
690,2019-6-12,2019,6,12,1,bzf0cg,SOTA for symbolic ML,https://www.reddit.com/r/MachineLearning/comments/bzf0cg/sota_for_symbolic_ml/,LeanderKu,1560272244,[removed],0,1
691,2019-6-12,2019,6,12,2,bzf3fw,"[P] I trained a YOLO v3 model to identify playing cards using 50,000 synthetically generated images. I'm using the model to count cards at the blackjack table!",https://www.reddit.com/r/MachineLearning/comments/bzf3fw/p_i_trained_a_yolo_v3_model_to_identify_playing/,Taxi-guy,1560272600,,0,1
692,2019-6-12,2019,6,12,2,bzf6g4,[P] apricot: submodular selection for machine learning in Python,https://www.reddit.com/r/MachineLearning/comments/bzf6g4/p_apricot_submodular_selection_for_machine/,ants_rock,1560272956,"Hello everyone!

I just posted a preprint of our overview of apricot, a Python package that implements submodular selection for machine learning. You can find it here: https://arxiv.org/abs/1906.03543

While submodular optimization is a very broad field, when applied to large data sets it can be used to select representative subsets that are useful for training machine learning models. Because these subsets are selected specifically to be non-redundant, you can frequent get comparable model accuracy with only a small fraction of the number of examples. A natural application of submodular selection in this setting is to remove correlated examples. For example, when applied to a video, submodular selection will frequently select frames that capture very different scenes.

I've worked hard to make apricot both easy to use and very fast. It has the API of a scikit-learn transformer, meaning that it can be dropped in to most current ML pipelines (including the literal sklearn pipeline object!) and can summarize massive data sets in only a few minutes.

The GitHub repo is here: https://github.com/jmschrei/apricot You can get it using pip install apricot-select.

I give an overview of some of the major features with some pretty pictures in this thread here: https://twitter.com/jmschreiber91/status/1138286268503085056
Would love to get any feedback.",7,21
693,2019-6-12,2019,6,12,2,bzfc7s,AQRs Problem With Machine Learning: Cats Morph Into Dogs,https://www.reddit.com/r/MachineLearning/comments/bzfc7s/aqrs_problem_with_machine_learning_cats_morph/,_quanttrader_,1560273650,,0,1
694,2019-6-12,2019,6,12,2,bzfdo9,Learning Individual Styles of Conversational Gesture,https://www.reddit.com/r/MachineLearning/comments/bzfdo9/learning_individual_styles_of_conversational/,amirbar,1560273823,,3,9
695,2019-6-12,2019,6,12,2,bzfhxr,"Using Tensor Flow to Recognize Cross Track Error, Unique Boundaries (Haystacks)",https://www.reddit.com/r/MachineLearning/comments/bzfhxr/using_tensor_flow_to_recognize_cross_track_error/,csapidus,1560274350,[removed],0,1
696,2019-6-12,2019,6,12,2,bzfo37,[D] TensorFlow Without Tears,https://www.reddit.com/r/MachineLearning/comments/bzfo37/d_tensorflow_without_tears/,KappaClosed,1560275105,,1,1
697,2019-6-12,2019,6,12,2,bzfsp5,[R] [1906.00820] One-Way Prototypical Networks,https://www.reddit.com/r/MachineLearning/comments/bzfsp5/r_190600820_oneway_prototypical_networks/,PeterPrinciplePro,1560275688,,1,8
698,2019-6-12,2019,6,12,2,bzft9m,"[Video] Sergey Levine (UC Berkeley): ""Robots That Learn",https://www.reddit.com/r/MachineLearning/comments/bzft9m/video_sergey_levine_uc_berkeley_robots_that_learn/,PeterPrinciplePro,1560275758,,0,1
699,2019-6-12,2019,6,12,2,bzfuod,[Read] How To Learn Machine Learning Quickly?,https://www.reddit.com/r/MachineLearning/comments/bzfuod/read_how_to_learn_machine_learning_quickly/,gauravlogical,1560275929,"If anyone new to machine learning , follow these steps for quick self learning.

[https://www.kaggle.com/learn-forum/95326](https://www.kaggle.com/learn-forum/95326)",0,1
700,2019-6-12,2019,6,12,2,bzfv14,1000x Faster Data Augmentation,https://www.reddit.com/r/MachineLearning/comments/bzfv14/1000x_faster_data_augmentation/,rayspear,1560275976,,2,5
701,2019-6-12,2019,6,12,3,bzfzht,Margin Elo (MELO)  new ML algorithm for pairwise comparison,https://www.reddit.com/r/MachineLearning/comments/bzfzht/margin_elo_melo_new_ml_algorithm_for_pairwise/,AgainstTheSpreads,1560276506,"Been working on a new ML algorithm called Margin Elo (MELO) that generalizes the [Bradley-Terry](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model) paired comparison model beyond binary outcomes to include margin-of-victory information. It does this by redefining what it means to win using a variable handicap to shift the threshold of paired comparison. The framework is general and has numerous applications in ranking, estimation, and time series prediction.

[https://moreland.dev/projects/melo](https://moreland.dev/projects/melo)  


I just released the package to PyPi and wanted to share!",0,1
702,2019-6-12,2019,6,12,3,bzg3n3,Machine learning model improves public transit ETA predictions by 15%,https://www.reddit.com/r/MachineLearning/comments/bzg3n3/machine_learning_model_improves_public_transit/,itsdonkeyjote,1560277026,,0,1
703,2019-6-12,2019,6,12,3,bzg6sb,[R] Capacitron: Effective Use of Variational Embedding Capacity in Expressive End-to-End Speech Synthesis,https://www.reddit.com/r/MachineLearning/comments/bzg6sb/r_capacitron_effective_use_of_variational/,animus144,1560277432,"**Paper**: [https://arxiv.org/abs/1906.03402](https://arxiv.org/abs/1906.03402)

**Audio Examples**: [https://google.github.io/tacotron/publications/capacitron](https://google.github.io/tacotron/publications/capacitron)  (best consumed in conjunction with reading the paper).

Capacitron is the Tacotron team's most recent contribution to the world of expressive end-to-end speech synthesis (e.g., transfer and control of prosody and speaking style). Our previous Style Tokens and prosody transfer work implicitly controls reference embedding capacity by modifying the encoder architecture, thereby targeting a trade-off between text-specific transfer fidelity and text-agnostic style generality. Capacitron treats embedding capacity as a first class citizen by targeting a specific value for the representational mutual information via a variational information bottleneck.

We also show that by modifying the stochastic reference encoder to match the form of the true latent posterior, we can achieve high-fidelity prosody transfer, text-agnostic style transfer, and natural-sounding prior samples in the same model. The modified encoder also addresses the pitch range preservation problems we observed during inter-speaker transfer in our past work.

Lastly, we show the capacity of the embedding can be decomposed hierarchically, allowing us to control the amount of sample-to-sample variation for transfer use cases.

To appreciate the results fully, we recommend listening to the audio examples in conjunction with reading the paper.",13,58
704,2019-6-12,2019,6,12,3,bzg7zd,[R] ICML 2019 Tutorials - an Easy to Follow version,https://www.reddit.com/r/MachineLearning/comments/bzg7zd/r_icml_2019_tutorials_an_easy_to_follow_version/,tigerneil,1560277584,,0,1
705,2019-6-12,2019,6,12,3,bzg9t4,How we used machine learning to improve public transit ETAs by 15%,https://www.reddit.com/r/MachineLearning/comments/bzg9t4/how_we_used_machine_learning_to_improve_public/,transitapp,1560277822,[removed],0,1
706,2019-6-12,2019,6,12,3,bzgl87,ICCV reviews are out,https://www.reddit.com/r/MachineLearning/comments/bzgl87/iccv_reviews_are_out/,lifeadvicesponge,1560279313,[removed],0,1
707,2019-6-12,2019,6,12,4,bzh5ky,[R] A Topology Layer for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bzh5ky/r_a_topology_layer_for_machine_learning/,__arch__,1560282076,,11,27
708,2019-6-12,2019,6,12,4,bzha96,nVidia not a sponsor at ICML?,https://www.reddit.com/r/MachineLearning/comments/bzha96/nvidia_not_a_sponsor_at_icml/,Aargau,1560282729,[removed],0,1
709,2019-6-12,2019,6,12,5,bzhvht,[D] is meta-learning the holy grail?,https://www.reddit.com/r/MachineLearning/comments/bzhvht/d_is_metalearning_the_holy_grail/,cryptonewsguy,1560285650,"I have been reading up on meta-learning the past few days and I read a study (sorry I lost the link) but in the study they were able to decrease computational resource needed to build machine learning models by 90-99% in some cases. I believe they used a simply genetic algorithm to design different ML models.

It seems that this could be the holy grail. If you can figure out how to effectively implement meta-learning that you could exponentially increase the desire results of a model. 

I'm currently expirementing with a genetic algorithm for model design in keras on the fashion mnist dataset. But I have yet to come up with a more effective model than the one given on the tensorflow website for this task. Although I'm not finished my experiment yet... I can find sparse information on the topic other than in academic literature. But it seems like it should almost be the primary focus if you are design model architectures from scratch no?

Here is a video on the topic if you are unaware https://www.youtube.com/watch?v=2z0ofe2lpz4",1,0
710,2019-6-12,2019,6,12,6,bzidlq,L1/2 Regularization?,https://www.reddit.com/r/MachineLearning/comments/bzidlq/l12_regularization/,ultronthedestroyer,1560288183,[removed],0,1
711,2019-6-12,2019,6,12,6,bziftv,New DeepMind Unsupervised Image Model Challenges AlexNet,https://www.reddit.com/r/MachineLearning/comments/bziftv/new_deepmind_unsupervised_image_model_challenges/,Yuqing7,1560288501,,0,1
712,2019-6-12,2019,6,12,6,bzijb4,"Tips on ""unsupervised"" custom object detection?",https://www.reddit.com/r/MachineLearning/comments/bzijb4/tips_on_unsupervised_custom_object_detection/,obl-sci,1560289003,[removed],0,1
713,2019-6-12,2019,6,12,6,bzimjj,Arxives - knowledge portal on arXiv,https://www.reddit.com/r/MachineLearning/comments/bzimjj/arxives_knowledge_portal_on_arxiv/,arxives,1560289478,[removed],0,1
714,2019-6-12,2019,6,12,6,bziniy,Probabilistic Programming with Variational Inference: Under the Hood,https://www.reddit.com/r/MachineLearning/comments/bziniy/probabilistic_programming_with_variational/,entoros,1560289626,,0,1
715,2019-6-12,2019,6,12,6,bzio0e,What is the state of the art approach for face segmentation?,https://www.reddit.com/r/MachineLearning/comments/bzio0e/what_is_the_state_of_the_art_approach_for_face/,sota1997,1560289696,[removed],0,1
716,2019-6-12,2019,6,12,6,bzipiy,What are the alternatives to GP in Bayesian Optimization?,https://www.reddit.com/r/MachineLearning/comments/bzipiy/what_are_the_alternatives_to_gp_in_bayesian/,sepehrakhavan,1560289917,"While GP is the most common (an intuitive) way of predicting a surrogate model for the ""expensive"" objective function in Bayesian Optimization, I was wondering what else have been proposed and used in the literature? I see things like [this](https://arxiv.org/pdf/1502.05700.pdf) where a NN is being used. I can see things like random forest or regression are possible (perhaps not the best options).",0,1
717,2019-6-12,2019,6,12,6,bzipoi,"[D] Tips on ""unsupervised"" custom object detection?",https://www.reddit.com/r/MachineLearning/comments/bzipoi/d_tips_on_unsupervised_custom_object_detection/,obl-sci,1560289936,"Is there any way to automatically label objects when training an object detection model library from scratch?  I've begun working through tutorials, eg TensorFlow's post [here](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html), but common methods require labelled training data.

Perhaps there are established techniques to partially label training data at scale, or common methods to adapt existing libraries?  Please forgive my ignorance, I just don't know where to start googling.",12,9
718,2019-6-12,2019,6,12,7,bziw0x,"[D] research papers related to tesla autonomy day's ""data engine""",https://www.reddit.com/r/MachineLearning/comments/bziw0x/d_research_papers_related_to_tesla_autonomy_days/,CartPole,1560290849,"I'm a grad student interested in the process described by Andrej at tesla's autonomy day. Below I've put together some of my brief notes with links. So far the most specific published research I could find that seemed to heavily inspire their work was the NeurIPS 2017 paper [Decoupling when to update from how to update](https://papers.nips.cc/paper/6697-decoupling-when-to-update-from-how-to-update.pdf). 

&amp;#x200B;

Does anyone else have other related papers to suggest? For example, I'm guessing measuring distance via the L2 is a bad idea.

&amp;#x200B;

[Generic Object Detection improvement](https://youtu.be/Ucp0TTmvqOE?t=7549) 

If you know a specific problem you have: take that specific problem and use it to find similar examples to pull into a training set 

I'm guessing they embed every image with a generic imagenet model and then find similar images based on L2 distance between embedded vectors

  
[training pipeline](https://youtu.be/Ucp0TTmvqOE?t=7716) 

start training with a uniformly sampled dataset and select new images for training if: 

1. detect uncertainties in the network predictions 
   1. I'm guessing 2 networks disagreeing with each other(similar to: [decoupling what to update from how to update](https://papers.nips.cc/paper/6697-decoupling-when-to-update-from-how-to-update.pdf))
2. driver intervention 

Too fix either of (1) or (2) use the process described in generic object detection",1,5
719,2019-6-12,2019,6,12,7,bzj7cu,[D] Does training data have to be randomly sampled/representative of the population?,https://www.reddit.com/r/MachineLearning/comments/bzj7cu/d_does_training_data_have_to_be_randomly/,Adamworks,1560292564,"I am a survey statistician by profession, but I am incorporating more and more machine learning techniques into my work and I am curious how survey sampling may affect the results of various machine learning models (e.g., DT, Random Forest, SVM, GBT, K-NN, etc.)

What I mean by survey sampling is that we often employ sampling techniques on a target population that on its own will return data that looks nothing like the population we are trying to study. 

In the more simple situations, it could be an over-sample of a rare minority population that we would never get naturally through simple random samples. Or potentially more complex sampling, either sampling proportional to a variables of interest (e.g., revenue of a company) or sampling area clusters to reduce costs of in-person data collection (e.g., cheaper to survey 100 people in 3 states, rather than surveying 100 people in 50 states...) or a combination of any number of sampling techniques.

In survey statistics we usually use special procedures in combination with case weights and sample design variables to ""fix"" the known imbalances due to sampling so that the results look like the population of interest and we also take special care to properly inflate the variances (e.g., confidence intervals) of all of our estimates due to non-random sampling (e.g., a non random sample of 99 men &amp; 1 woman, is not the same as a random sample that returns 50 men &amp; 50 women, even though the total is n = 100 in both cases). 

But I am not sure how that would influence various machine learning algorithms since their goals are slightly different and have less focus on p-values and confidence intervals.",6,1
720,2019-6-12,2019,6,12,9,bzk9so,And voil!  Jupyter Blog,https://www.reddit.com/r/MachineLearning/comments/bzk9so/and_voil_jupyter_blog/,_quanttrader_,1560298593,,0,1
721,2019-6-12,2019,6,12,9,bzka5r,[R] Weight Agnostic Neural Networks,https://www.reddit.com/r/MachineLearning/comments/bzka5r/r_weight_agnostic_neural_networks/,baylearn,1560298651,"[Weight Agnostic Neural Networks](https://weightagnostic.github.io)

**Abstract**

Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights.

*interactive article:* https://weightagnostic.github.io

*arxiv version:* https://arxiv.org/abs/1906.04358",60,215
722,2019-6-12,2019,6,12,9,bzkj00,Reinforcement Learning in production?,https://www.reddit.com/r/MachineLearning/comments/bzkj00/reinforcement_learning_in_production/,aviniumau,1560300144,[removed],0,1
723,2019-6-12,2019,6,12,10,bzkqps,Any tutorials on BERT with pytorch,https://www.reddit.com/r/MachineLearning/comments/bzkqps/any_tutorials_on_bert_with_pytorch/,saravanakumar17,1560301462,"Hi guys, I've been trying to learn Google's BERT with pytorch for sometime I've tried huggingface GitHub repo and few other sources, but I couldn't quite figure out how to implement it in an end to end project with pytorch. If you folks knew any resources of this kind, please let me know. Thanks in advance.",0,1
724,2019-6-12,2019,6,12,10,bzl9n1,[R] Deep Reinforcement Learning Zoo for TensorFlow 2.0,https://www.reddit.com/r/MachineLearning/comments/bzl9n1/r_deep_reinforcement_learning_zoo_for_tensorflow/,zsdh123,1560304686,,0,1
725,2019-6-12,2019,6,12,11,bzldq4,"[R] [1906.04493] Unsupervised Minimax: Adversarial Curiosity, Generative Adversarial Networks, and Predictability Minimization (Schmidhuber)",https://www.reddit.com/r/MachineLearning/comments/bzldq4/r_190604493_unsupervised_minimax_adversarial/,hardmaru,1560305365,,37,98
726,2019-6-12,2019,6,12,12,bzm270,[D] Multi-level classification,https://www.reddit.com/r/MachineLearning/comments/bzm270/d_multilevel_classification/,__BetterAgent__,1560309601,"I want to make a CNN to classify first by race (black, white, Hispanic etc.) and then by gender (m/f) to give outputs like Black-Male, Hispanic-Female etc. How can I set up the structure? Is this even possible with 1 CNN?",3,1
727,2019-6-12,2019,6,12,13,bzmggb,"[R] OpenAI Five @RedisConf19 | ""Reinforcement Learning on Hundreds of Thousands of Cores""",https://www.reddit.com/r/MachineLearning/comments/bzmggb/r_openai_five_redisconf19_reinforcement_learning/,YoshML,1560312215,"[https://www.youtube.com/watch?v=ui4F\_A46wN0](https://www.youtube.com/watch?v=ui4F_A46wN0)

Speaker: Henrique Ponde de Oliveira Pinto

Talk about how OpenAI Five training was orchestrated, using Redis.

Cool stuff!",1,10
728,2019-6-12,2019,6,12,13,bzmqar,Analysis of user content for sentiment analysis using NLP,https://www.reddit.com/r/MachineLearning/comments/bzmqar/analysis_of_user_content_for_sentiment_analysis/,eulerslab,1560314077,,0,1
729,2019-6-12,2019,6,12,14,bzn0k2,funny project building with coral dev board!,https://www.reddit.com/r/MachineLearning/comments/bzn0k2/funny_project_building_with_coral_dev_board/,makereven,1560316099,,0,1
730,2019-6-12,2019,6,12,14,bzndb5,"""[P]"" An AI-Powered Domain Name Generator",https://www.reddit.com/r/MachineLearning/comments/bzndb5/p_an_aipowered_domain_name_generator/,Refeb,1560318720,"Hey there, Saeed here from [DeepNamer.com](https://DeepNamer.com), we are glad that we can share our platform with you today:

DeepNamer is an AI-powered domain name generator and deep brainstorm platform that can help you find a catchy and creative domain name for your business for free. DeepNamer is built based on a deep sequence-to-sequence (i.e., keywords-to-domain) architecture, which utilizes the most recent natural language processing algorithms such as dynamic recurrent neural networks.

Note that we find our name DeepNamer via our AI algorithm and our platform inspired by the way startups names their businesses.

**We would be happy to share our platform (**[**DeepNamer.com**](https://DeepNamer.com)**) with you and any comment, feedback or suggestion would be appreciated.**",5,0
731,2019-6-12,2019,6,12,15,bznj4s,Deploying pytorch based Gan model,https://www.reddit.com/r/MachineLearning/comments/bznj4s/deploying_pytorch_based_gan_model/,shbnm,1560319918,[removed],0,1
732,2019-6-12,2019,6,12,15,bznm4k,Leela Chess Zero,https://www.reddit.com/r/MachineLearning/comments/bznm4k/leela_chess_zero/,tiwari504,1560320548,[removed],0,1
733,2019-6-12,2019,6,12,16,bzo0lc,CREDIT CARD FRAUD DETECTION THROUGH MACHINE LEARNING!!!,https://www.reddit.com/r/MachineLearning/comments/bzo0lc/credit_card_fraud_detection_through_machine/,aasthaarora,1560323568,,0,1
734,2019-6-12,2019,6,12,16,bzo0o1,"building an automated training tool - how do i detect ""userid"" ?",https://www.reddit.com/r/MachineLearning/comments/bzo0o1/building_an_automated_training_tool_how_do_i/,sandys1,1560323584,[removed],0,1
735,2019-6-12,2019,6,12,16,bzo25m,[D] Some takeaways from 1st day ICML 2019,https://www.reddit.com/r/MachineLearning/comments/bzo25m/d_some_takeaways_from_1st_day_icml_2019/,gau_mar,1560323887,,0,1
736,2019-6-12,2019,6,12,16,bzo6fq,Machine Learning &amp; Artificial Intelligence Tools,https://www.reddit.com/r/MachineLearning/comments/bzo6fq/machine_learning_artificial_intelligence_tools/,aasthaarora,1560324809,"Two of the most popular technological trends of this age are AI and machine learning. It is because we have just started to scratch the surface of AI and machine learning and the results have been stunning. Also, with many companies investing heavily in AI and machine learning, the trend is only expected soar through the roof in the coming years. 

&amp;#x200B;

So, for an AI and machine learning enthusiast like me, I wanted to learn more about these two fascinating concepts. One of the subsets of learning AI and machine learning are to know some of its essential tools. One of these tools is Matplotlib, which is a Python 2D plotting library. It produces publication quality figures in a plethora of hard-copy formats as well as interactive environments across platforms. It is used in web application servers, Python scripts, the Jupyter notebook, the Python and IPython shells, web application servers, and four graphical user interface toolkits. 

&amp;#x200B;

While there are many courses that promises to teach about Matplotlib, most of them are hard to grasp. So, after a lot of glancing over other courses, I found this online tutorial. The course explains the essentials of Matplotlib with all the terminologies and also giving a lot of examples which made me understand the concept much easily compared to other tutorials. The tutorial also goes onto explain some of the most vital sections of Matplotlib such as bar plots, box plots and subplotting. I learned how to use boxplot in Matplotlib and also make a timeseries plot along with many other features of it. 

&amp;#x200B;

Furthermore, I was able to build a graph from a real data set along with learning many other statistical tricks. 

&amp;#x200B;

The tutor teaching the course is an expert on machine learning and the tutorial will also gave me several examples which helped me understand the concepts in a better way. Upon the completion of the course, I was also awarded with a certificate of completion.I have also found a short link to the video that i really recommend to watch so that you can have a better undertsanding of the course!

&amp;#x200B;

&amp;#x200B;

[https://www.youtube.com/playlist?list=PLDmvslp\_VR0y7dipg4qASNS-pg7mPth7M](https://www.youtube.com/playlist?list=PLDmvslp_VR0y7dipg4qASNS-pg7mPth7M)",0,1
737,2019-6-12,2019,6,12,17,bzofit,join the project,https://www.reddit.com/r/MachineLearning/comments/bzofit/join_the_project/,A_Samoshkin,1560326868,"Hi everyone!  
My name is Artem and I have 5 years experience in ML developing:

* Software development for server and system applications with computer vision technologies/libraries and machine learning algorithms
* Developed an applications for processing images and videos
* Developed a middleware for streaming with FFmpeg
* Handled responsibility of deploying and testing applications
* Profiled python code for optimization
* Used a different architecture of neural network
* Research and Development
* Assigned to estimate tasks  


I'm writing here only to understand is it possible for me and my team 

to join some half-a-year project in ML or AI field.

We are professionals and want to grow up with a new practise.  
So, If You have some problems in startup or during long-term project

We would be excited to help You.

&amp;#x200B;

Best wishes,  
Artem",0,1
738,2019-6-12,2019,6,12,17,bzokof,[D] Text detection - recognition - extraction,https://www.reddit.com/r/MachineLearning/comments/bzokof/d_text_detection_recognition_extraction/,cashshots,1560328065,"For a project, I need to get all the text off an image, in a structured format (sentences, paragraphs, etc.), and have it be accurate. 

Most of my experiments have dealt with scene detection, which usually just detects text being there in a non structured. The out of the box OCR engines dont seem to be accurate, as Im hoping to run some NLP on top of the extracted data. 

An idea I had was detecting sentences and paragraphs of text, cropping and OCRing the data until there is no more text on the page, but I found that text recognition isnt that far along yet. 

Im looking for any help going forward, and hopefully come up with an end to end solution for this.",4,2
739,2019-6-12,2019,6,12,17,bzolpi,Machine Learning Model for ambient sound recognition,https://www.reddit.com/r/MachineLearning/comments/bzolpi/machine_learning_model_for_ambient_sound/,mansoorkhansitar,1560328307,[removed],0,1
740,2019-6-12,2019,6,12,17,bzom8t,Unbounded Table Detection,https://www.reddit.com/r/MachineLearning/comments/bzom8t/unbounded_table_detection/,data_autopsy,1560328430,,0,1
741,2019-6-12,2019,6,12,17,bzond7,Machine learning to allocate a text string to a category?,https://www.reddit.com/r/MachineLearning/comments/bzond7/machine_learning_to_allocate_a_text_string_to_a/,Gazpage,1560328680,[removed],0,1
742,2019-6-12,2019,6,12,18,bzothp,[R] Practical Deep Learning with Bayesian Principles,https://www.reddit.com/r/MachineLearning/comments/bzothp/r_practical_deep_learning_with_bayesian_principles/,ednops52,1560330085,"Bayesian methods promise to fix many shortcomings of deep learning, but they are impractical and rarely match the performance of standard methods, let alone improve them. In this paper, we demonstrate practical training of deep networks with natural-gradient variational inference. By applying techniques such as batch normalisation, data augmentation, and distributed training, we achieve similar performance in about the same number of epochs as the Adam optimiser, even on large datasets such as ImageNet. Importantly, the benefits of Bayesian principles are preserved: predictive probabilities are well-calibrated and uncertainties on out-of-distribution data are improved. This work enables practical deep learning while preserving benefits of Bayesian principles. A PyTorch implementation will be available as a plug-and-play optimiser.

Arxiv: [https://arxiv.org/abs/1906.02506](https://arxiv.org/abs/1906.02506)",2,30
743,2019-6-12,2019,6,12,18,bzovx9,Neural ODEs: another deep learning breakthrough,https://www.reddit.com/r/MachineLearning/comments/bzovx9/neural_odes_another_deep_learning_breakthrough/,rachnogstyle,1560330627,,0,1
744,2019-6-12,2019,6,12,18,bzozv7,[Suggestions] Parsing out frames in 2d pose estimation,https://www.reddit.com/r/MachineLearning/comments/bzozv7/suggestions_parsing_out_frames_in_2d_pose/,theSamuraiMonk17,1560331488,,0,1
745,2019-6-12,2019,6,12,19,bzpg5u,"[P] Simple Tensorflow implementation of ""Image-to-Image Translation via Group-wise Deep Whitening-and-Coloring Transformation"" (CVPR 2019 Oral)",https://www.reddit.com/r/MachineLearning/comments/bzpg5u/p_simple_tensorflow_implementation_of/,taki0112,1560334782,"&amp;#x200B;

[Comparison with baselines on Artworks dataset](https://i.redd.it/upn54l9uiw331.png)

&amp;#x200B;

[Comparison with baselines on CelebA dataset](https://i.redd.it/1nuxnn5viw331.png)",3,19
746,2019-6-12,2019,6,12,19,bzpi73,Telescopic Forks for Automated Warehouse AS/RS | Automated AS/RS Systems,https://www.reddit.com/r/MachineLearning/comments/bzpi73/telescopic_forks_for_automated_warehouse_asrs/,lhd121,1560335203,,0,1
747,2019-6-12,2019,6,12,19,bzpifq,Pythia (Facebook) Greek god doing Deep learning,https://www.reddit.com/r/MachineLearning/comments/bzpifq/pythia_facebook_greek_god_doing_deep_learning/,whitezl0,1560335258,,0,1
748,2019-6-12,2019,6,12,19,bzpock,Using VAR with statsmodels - questions,https://www.reddit.com/r/MachineLearning/comments/bzpock/using_var_with_statsmodels_questions/,lppier,1560336450,[removed],0,1
749,2019-6-12,2019,6,12,20,bzpsqj,Quick Question on reinforced learning,https://www.reddit.com/r/MachineLearning/comments/bzpsqj/quick_question_on_reinforced_learning/,BritterDayzz,1560337286,[removed],0,1
750,2019-6-12,2019,6,12,20,bzptvr,How do they do it? - Companies solving art forgery with AI/ML and Blockchain,https://www.reddit.com/r/MachineLearning/comments/bzptvr/how_do_they_do_it_companies_solving_art_forgery/,HamburgersNHeroin,1560337499,[removed],0,1
751,2019-6-12,2019,6,12,20,bzpy8j,[D] Creating a Product Code Picker - Azure ML,https://www.reddit.com/r/MachineLearning/comments/bzpy8j/d_creating_a_product_code_picker_azure_ml/,MLDummy,1560338332,"I'm looking for a ML solution to what I thought was a simple problem. I have say 20K product codes with descriptions and I'm trying to create a ML Model that will allow me to input a description and the model picks the most suitable product code.  I've attempted this in Azure Machine Learning Studio , transform data &gt; selecting columns &gt; edit meta data &gt; process text &gt; Train Model ( Multiclass Neural Network) &gt; Score model .  My question is this the right methodology ?  Is there a better way about things ?

&amp;#x200B;

I'm very much a ML dummy so please forgive my ignorance.",0,1
752,2019-6-12,2019,6,12,20,bzpybe,Artificial Intelligence in Industry: New Services and Applications,https://www.reddit.com/r/MachineLearning/comments/bzpybe/artificial_intelligence_in_industry_new_services/,mhonrubia,1560338347,[removed],0,1
753,2019-6-12,2019,6,12,21,bzqfwc,[D] What are some useful ML packages others should know about?,https://www.reddit.com/r/MachineLearning/comments/bzqfwc/d_what_are_some_useful_ml_packages_others_should/,FastTruth,1560341483,"Tensorboard, liveplotloss etc",29,66
754,2019-6-12,2019,6,12,21,bzqm2o,Adaptive authentication,https://www.reddit.com/r/MachineLearning/comments/bzqm2o/adaptive_authentication/,chandiramouli,1560342505,[removed],0,1
755,2019-6-12,2019,6,12,21,bzquua,Webinar: How to use continual learning in your ML models,https://www.reddit.com/r/MachineLearning/comments/bzquua/webinar_how_to_use_continual_learning_in_your_ml/,Mayalittlepony,1560343988,[removed],0,1
756,2019-6-12,2019,6,12,22,bzr8su,The Role of AI and Machine Learning in Data Quality,https://www.reddit.com/r/MachineLearning/comments/bzr8su/the_role_of_ai_and_machine_learning_in_data/,rohit1221qq,1560346164,,0,1
757,2019-6-12,2019,6,12,22,bzrdha,An exploration of AI-facilitated or -focused US federal grants,https://www.reddit.com/r/MachineLearning/comments/bzrdha/an_exploration_of_aifacilitated_or_focused_us/,ashendruk,1560346877,[removed],0,1
758,2019-6-12,2019,6,12,22,bzrj5y,Questions about a project.,https://www.reddit.com/r/MachineLearning/comments/bzrj5y/questions_about_a_project/,JimmyCroissant,1560347766,"I want to do an NLP project but i don't know if it's doable or not as i have no experience or knowledge in NLP or ML yet.

The idea is as follows: Let's say we have a story (in text) in English that has 10 characters, Can we define them, their characteristics, whole sentences they said, and then analyze emotions within those sentences ?

After that is it possible to generate an audio version of the story where: the text in general is narrated by one voice, each individual character's sentences are read in a different voice generated specifically for that character, finally is it possible to make the tones of the characters voices change depending on the emotions detected in their sentences ?",0,1
759,2019-6-12,2019,6,12,23,bzrsks,ANN that controls hyperparameters of another ANN,https://www.reddit.com/r/MachineLearning/comments/bzrsks/ann_that_controls_hyperparameters_of_another_ann/,mikehawk1988,1560349103,[removed],0,1
760,2019-6-12,2019,6,12,23,bzs5r9,[N] Keras Tuner (official hyper-parameter tuning library ),https://www.reddit.com/r/MachineLearning/comments/bzs5r9/n_keras_tuner_official_hyperparameter_tuning/,tlkh,1560351042,"Official hyper-parameter tuning library for Keras just dropped:

[https://github.com/keras-team/keras-tuner](https://github.com/keras-team/keras-tuner)

Early days, but it provides a simple way to do hyper-parameter tuning in Keras.

Teaser Slides from Google I/O 2019:

[https://elie.net/static/files/cutting-edge-tensorflow-keras-tuner-hypertuning-for-humans/cutting-edge-tensorflow-keras-tuner-hypertuning-for-humans-slides.pdf](https://elie.net/static/files/cutting-edge-tensorflow-keras-tuner-hypertuning-for-humans/cutting-edge-tensorflow-keras-tuner-hypertuning-for-humans-slides.pdf)",25,221
761,2019-6-12,2019,6,12,23,bzs5zi,[Discussion] What is the minimum amount of cloud computing knowledge necessary for a data scientist?,https://www.reddit.com/r/MachineLearning/comments/bzs5zi/discussion_what_is_the_minimum_amount_of_cloud/,AlexSnakeKing,1560351075,"Hi All. 

&amp;#x200B;

I am a data scientist with pretty deep domain knowledge and a broad and comprehensive knowledge of statistics and ML algorithms. I can go toe-to-toe with the best over when to use neural networks and when to use logistic regression, why the BIC is sometimes better than the AIC, which optimization algorithm is best suited for a specific optimization problem etc....

&amp;#x200B;

Unfortunately, my tech stack has been all on-prem and ERP tools so far. My knowledge of cloud computing amounts to knowing how to log into a cloud environment and then doing whatever I would do on a on-prem server on the cloud instead (e.g. running SQL queries in Redshit or BigQuery the same way I would run them on any Oracle or SQL Server tool, running Python scripts the same way I run them on my local machine or on a Linux box, S3 and GS are just a bunch of remote folders that you scp to and from, etc....)

&amp;#x200B;

I feel that I need to learn more.  Most people keep telling me that I am fine, as long as I am sticking to the role of data scientist and not trying to do ML engineer or data engineer work instead, but I have a nagging feeling that there is a minimum amount of cloud computing knowledge that I should have and that I am missing. 

&amp;#x200B;

Can anybody give me some pointers?",17,10
762,2019-6-12,2019,6,12,23,bzs87u,"This just in--It was definitely Juergen Schmidhuber who invented GANs in the '90s, not that phoney Ian Goodfellow in 2014",https://www.reddit.com/r/MachineLearning/comments/bzs87u/this_just_init_was_definitely_juergen_schmidhuber/,AyEhEigh,1560351412,,1,1
763,2019-6-13,2019,6,13,0,bzshls,How Artificial Intelligence is Impacting Clinical Trials,https://www.reddit.com/r/MachineLearning/comments/bzshls/how_artificial_intelligence_is_impacting_clinical/,S_paddy,1560352736,,0,1
764,2019-6-13,2019,6,13,0,bzsvvm,"Simple Questions Thread June 12, 2019",https://www.reddit.com/r/MachineLearning/comments/bzsvvm/simple_questions_thread_june_12_2019/,AutoModerator,1560354791,[removed],0,1
765,2019-6-13,2019,6,13,1,bzsz61,Want to build a large # core (dual CPU?) dual GPU machine learning server-- anybody built one recently?,https://www.reddit.com/r/MachineLearning/comments/bzsz61/want_to_build_a_large_core_dual_cpu_dual_gpu/,SpicyBroseph,1560355241,[removed],0,1
766,2019-6-13,2019,6,13,1,bzt98a,Detect Typical Customer Mistakes in the Shopping Cart,https://www.reddit.com/r/MachineLearning/comments/bzt98a/detect_typical_customer_mistakes_in_the_shopping/,roma-glushko,1560356639,,0,1
767,2019-6-13,2019,6,13,1,bztkvo,[P] Real-Time Voice Cloning (code + pretrained models + toolbox),https://www.reddit.com/r/MachineLearning/comments/bztkvo/p_realtime_voice_cloning_code_pretrained_models/,Valiox,1560358162,,1,1
768,2019-6-13,2019,6,13,1,bztod8,[D] Best practices for interpreting 1-D convolutions,https://www.reddit.com/r/MachineLearning/comments/bztod8/d_best_practices_for_interpreting_1d_convolutions/,satsatsat,1560358630,"1D convolutions are often applied at the earliest layers when doing deep learning on dependent (time-series, dynamical, audio e.g. WaveNet, etc) data. Are there any best practices for interpreting the learned convolution weights? Can anyone point me to examples of good papers where the representations learned by stacked 1D-convolutions were interpreted? A signal-processing (or other established mathematical framework's) viewpoint  would be especially interesting. Thanks!",9,3
769,2019-6-13,2019,6,13,2,bztyzg,[D] Costs of Deep Learning,https://www.reddit.com/r/MachineLearning/comments/bztyzg/d_costs_of_deep_learning/,muwnd,1560360065,"Latest DL language models such as Groover ( https://arxiv.org/abs/1905.12616 ) costs up to 600k USD just for training the final model. At the same time, training a single AI model can emit as much carbon as five cars in their lifetimes ( https://www.technologyreview.com/s/613630/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/ ).

Is this a sustainable trend especially for research labs in academia which lack funding?",5,0
770,2019-6-13,2019,6,13,2,bzu0xf,Permuting RV order in stacked Auto-regressive Flows for density estimation,https://www.reddit.com/r/MachineLearning/comments/bzu0xf/permuting_rv_order_in_stacked_autoregressive/,AgEcyentist,1560360328,[removed],0,1
771,2019-6-13,2019,6,13,2,bzu2ow,"ICML 2019 | Google, ETH Zurich, MPI-IS, Cambridge &amp; PROWLER.io Share Best Paper Honours",https://www.reddit.com/r/MachineLearning/comments/bzu2ow/icml_2019_google_eth_zurich_mpiis_cambridge/,Yuqing7,1560360578,,0,1
772,2019-6-13,2019,6,13,2,bzu87u,Picture of a Person -&gt; Anime,https://www.reddit.com/r/MachineLearning/comments/bzu87u/picture_of_a_person_anime/,rand0mstring,1560361301,"I'm trying to find a model to run to convert a real picture of a person into an anime-ified version.

&amp;#x200B;

I was checking out [TwinGAN](https://zhuanlan.zhihu.com/p/35777788) but the results appear almost completely independent of the source image. 

&amp;#x200B;

Anyone know point me in a better direction?",0,1
773,2019-6-13,2019,6,13,2,bzu8ol,"Debugging a run job (Linux, python3, cython)",https://www.reddit.com/r/MachineLearning/comments/bzu8ol/debugging_a_run_job_linux_python3_cython/,phycem,1560361361,[removed],0,1
774,2019-6-13,2019,6,13,3,bzurja,Meditations on Machine Learning Intepretability - with LIME Creator Marco Ribeiro,https://www.reddit.com/r/MachineLearning/comments/bzurja/meditations_on_machine_learning_intepretability/,timscarfe,1560363917,[removed],0,1
775,2019-6-13,2019,6,13,3,bzv4a2,Tiling and Randomizing Images as a Stopgap for Visual Data Shortage [D],https://www.reddit.com/r/MachineLearning/comments/bzv4a2/tiling_and_randomizing_images_as_a_stopgap_for/,LFOwen,1560365667,"Hi there,

This is my first post and, as a beginner, I don't yet have the vocabulary to totally articulate my thoughts on this and I apologize in advance for that. 

I'm currently interested in training a convolutional neural network to recognize images. However, a problem I'm encountering is a lack of source images so I thought, why not throw the few images I have into some program that could cut them up into tiles and shuffle them around randomly?

Upon further reading, it seems like this is - or is at least similar to - a preestablished method for synthesizing new visual data called Structured Domain Randomization.

My question: is this a valid method for training my neural network? 

I'm worried that the algorithm wouldn't actually be learning anything new since the structures present within each image will remain the same, only their location within the image will change.

Thank you!",5,2
776,2019-6-13,2019,6,13,4,bzvbov,RingNet | 3D Face Estimation Without 3D Supervision,https://www.reddit.com/r/MachineLearning/comments/bzvbov/ringnet_3d_face_estimation_without_3d_supervision/,Yuqing7,1560366662,,0,1
777,2019-6-13,2019,6,13,4,bzvga1,[P] Real-Time Voice Cloning (code + pretrained models + toolbox),https://www.reddit.com/r/MachineLearning/comments/bzvga1/p_realtime_voice_cloning_code_pretrained_models/,Valiox,1560367278,,1,1
778,2019-6-13,2019,6,13,4,bzvtjm,Is there any way to access the contents of this UC Davis Course?,https://www.reddit.com/r/MachineLearning/comments/bzvtjm/is_there_any_way_to_access_the_contents_of_this/,AffectionateEmu8,1560369054,[removed],0,1
779,2019-6-13,2019,6,13,5,bzvxyp,Converting text to images,https://www.reddit.com/r/MachineLearning/comments/bzvxyp/converting_text_to_images/,LolSalaam,1560369643,[removed],0,1
780,2019-6-13,2019,6,13,5,bzwgr9,Adversarial training Papers,https://www.reddit.com/r/MachineLearning/comments/bzwgr9/adversarial_training_papers/,aniket_agarwal,1560372203,"Hi, can anyone suggest me some good papers on crafting of  adversarial examples, various techniques used to defend from such an attack and also techniques to benchmark such techniques.",0,1
781,2019-6-13,2019,6,13,5,bzwgsk,Difference between ROC and Envelope curves,https://www.reddit.com/r/MachineLearning/comments/bzwgsk/difference_between_roc_and_envelope_curves/,shanahmedshaffi,1560372208,Are ROC curves a type of envelope curve?,0,1
782,2019-6-13,2019,6,13,5,bzwmqs,[D] Are there open source projects for Video Dialogue Replacement?,https://www.reddit.com/r/MachineLearning/comments/bzwmqs/d_are_there_open_source_projects_for_video/,TalosAI,1560373011,"Going off of the article linked below, I'm curious about the VDR tech CannyAI created. Does anyone know if there are open source projects that accomplish something similar? Thanks.

[https://www.vice.com/en\_us/article/ywyxex/deepfake-of-mark-zuckerberg-facebook-fake-video-policy](https://www.vice.com/en_us/article/ywyxex/deepfake-of-mark-zuckerberg-facebook-fake-video-policy)",0,1
783,2019-6-13,2019,6,13,5,bzwnri,Whats the best practise for learning the maths for machine learning?,https://www.reddit.com/r/MachineLearning/comments/bzwnri/whats_the_best_practise_for_learning_the_maths/,Ghjjj4433,1560373154,"Machine learning consists of calculus,linear algebra and probability &amp; statistics. Should I cover each subject from a textbook front to back to understand the maths in machine learning.Or should I learn enough of each subject and then go in depth when I dont get a concept in a particular Algorithm that consists of a mathematical concept I dont understand.",0,1
784,2019-6-13,2019,6,13,6,bzws7g,Neural Networks with non-smooth loss?,https://www.reddit.com/r/MachineLearning/comments/bzws7g/neural_networks_with_nonsmooth_loss/,groovyJesus,1560373761,"I'm a student researcher looking for literature on neural network parameter optimazation where the objective loss is nonsmooth. Meaning that that the typical gradient based methods are rulled out and something like proximal gradient methods are employed. Preferably in the context of regression. This seems uncommon in practice.

1. Are non difderentiable losses avoided in NN's?
2. Is there a need for this kind of work from a non theoretical point of view? That is, smoothness conditions are violated, but gradient methods still find empirical success.

I have a lot of questions and any direction or content would be helpful! Thanks!",0,1
785,2019-6-13,2019,6,13,6,bzx2o7,Companies with high quality Python on-boarding training?,https://www.reddit.com/r/MachineLearning/comments/bzx2o7/companies_with_high_quality_python_onboarding/,Cbrum11,1560375198,,0,1
786,2019-6-13,2019,6,13,6,bzx94d,Need help with Image Captioning,https://www.reddit.com/r/MachineLearning/comments/bzx94d/need_help_with_image_captioning/,plmlp1,1560376102,[removed],0,1
787,2019-6-13,2019,6,13,7,bzxn9u,[P] Need help with Image Captioning,https://www.reddit.com/r/MachineLearning/comments/bzxn9u/p_need_help_with_image_captioning/,plmlp1,1560378160,"Hello I'm trying to learn CNNs and I've hit a deadend with an Image Captioning project I was working on for fun.

Dataset: 10k images from [Google Conceptual Captions](https://ai.google.com/research/ConceptualCaptions/download)  

Tutorial I'm mostly following: [Automatic Image Captioning](https://github.com/hlamba28/Automatic-Image-Captioning)  


One difference between my dataset and the Flicker8k dataset in the tutorial is that my dataset only has one caption per image but latter has five captions per image.

The problem is that I am getting the same caption for nearly all images. I have tried to use:
- LSTM instead of GRU cells
- 50 and 200 Glove word embeddings. I even tried to create my own embeddings using all captions in the dataset
- beam search and greedy search to get a prediction

What do I do?",3,1
788,2019-6-13,2019,6,13,8,bzydgr,"[D] Model compression for generative models (GAN, VAE, Autoregressive, etc)",https://www.reddit.com/r/MachineLearning/comments/bzydgr/d_model_compression_for_generative_models_gan_vae/,tsauri,1560382048,"Are there works that investigate VAE and GAN quality, disentanglement, latent variables under model compression constraints? (activations and weights are pruned/sparsed, quantization, distillation, etc)  


I can't seem to find one that apply compression on GANs and VAEs (Exceptions include Parallel WaveNet which do use distillation, but that is because otherwise they can't get convergence)  
Last time we got denoising autoencoders and sparse autoencoders...",3,10
789,2019-6-13,2019,6,13,9,bzysdj,"Colin Cantrell Nexus / NXS Interview- Space Machine Learning AI , 3D blockchain, Cryptography, &amp; the Future",https://www.reddit.com/r/MachineLearning/comments/bzysdj/colin_cantrell_nexus_nxs_interview_space_machine/,007moonboundnxs,1560384455,,0,1
790,2019-6-13,2019,6,13,9,bzz2ca,A start-up funded by Eric Schmidt doing enterprise AI research!,https://www.reddit.com/r/MachineLearning/comments/bzz2ca/a_startup_funded_by_eric_schmidt_doing_enterprise/,realitydistortion1,1560386097,,0,1
791,2019-6-13,2019,6,13,10,bzzcf6,[R] Learning to Route in Similarity Graphs,https://www.reddit.com/r/MachineLearning/comments/bzzcf6/r_learning_to_route_in_similarity_graphs/,justheuristic,1560387786,"# Learning to Route in Similarity Graphs [(arxiv)](https://arxiv.org/abs/1905.10987)

The paper improves Similarity Graphs for large-scale Nearest Neighbor Search by training an agent to efficiently navigate the graph with deep imitation learning. Put simply, these guys train the search engine to better navigate the graph of all images so as to find the nearest neighbours. Basically Deep Imitation Learning meets Graph Convolutional Networks meets Web/Image Search and other fancy large-scale applications.

[Toy example. Each node represents one data point \(e.g. image\). Given the query \\""q\\"", the algorithm navigates the graph from \\""start\\"" vertex to find the nearest neighbour \\""gt\\"" for the query. The yellow path follows the oririginal search procedure, the orange path corresponds to the learned agent.](https://i.redd.it/gk29y0fhu0431.png)

&amp;#x200B;

Read the [paper (arxiv)](https://icml.cc/Conferences/2019/Schedule?showEvent=4084) , browse the [code (github)](https://github.com/dbaranchuk/learning-to-route) or [talk to authors](https://icml.cc/Conferences/2019/Schedule?showEvent=4084) at ICML right about now if you're attending :)

*(source: saw the paper at icml, acquainted with the authors)*",0,15
792,2019-6-13,2019,6,13,10,bzznyy,"5 Million Faces: Free Image Datasets for Facial Recognition (totaling over 5,000,000 images and video frames)",https://www.reddit.com/r/MachineLearning/comments/bzznyy/5_million_faces_free_image_datasets_for_facial/,LimarcAmbalina,1560389721,[removed],0,1
793,2019-6-13,2019,6,13,11,c004tw,Machine learning for optimizing online traffic distribution,https://www.reddit.com/r/MachineLearning/comments/c004tw/machine_learning_for_optimizing_online_traffic/,jimjames888,1560392603,[removed],1,1
794,2019-6-13,2019,6,13,11,c00cea,SciHive: a free open-source platform for collaboratively reading and discovering papers on AI/CS/ML,https://www.reddit.com/r/MachineLearning/comments/c00cea/scihive_a_free_opensource_platform_for/,SciHive,1560393865,"Hi, we just launched an open source project called [SciHive](https://www.scihive.org/), which is a collaborative platform for reading and discovering new papers. We only just started, but it already supports multiple features, including  
 Ranking of trending papers on Twitter  
 Commenting and asking questions on papers with the SciHive community  
 Collaboration within a research group  
 Saving private notes  
 Table of contents navigation  
 Preview of references  
 Acronyms resolver  


Feedback is most welcome, this is only the beginning!

&amp;#x200B;

![video](j6khw4cbe1431)",0,1
795,2019-6-13,2019,6,13,11,c00eis,[R] Neural Networks with non-smooth loss?,https://www.reddit.com/r/MachineLearning/comments/c00eis/r_neural_networks_with_nonsmooth_loss/,groovyJesus,1560394216,"I'm a student researcher looking for literature on neural network parameter optimization where the objective loss is non-smooth. Meaning that that the typical gradient based methods are ruled out and something like proximal gradient methods are employed. Preferably in the context of regression. This seems uncommon in practice.

1. Are non differentialable losses avoided in NN's?
2. Is there a need for this kind of work from a non theoretical point of view? That is, smoothness conditions are violated, but gradient methods still find empirical success?

I have many more questions, but really any direction or content would be helpful! Thanks!",12,2
796,2019-6-13,2019,6,13,12,c00uaw,X-Ray Dataset for Research,https://www.reddit.com/r/MachineLearning/comments/c00uaw/xray_dataset_for_research/,Redhatchamp00,1560396919,[removed],0,1
797,2019-6-13,2019,6,13,12,c00us9,What determines Neural Network capacity?,https://www.reddit.com/r/MachineLearning/comments/c00us9/what_determines_neural_network_capacity/,lyy1780,1560397005,[removed],0,1
798,2019-6-13,2019,6,13,12,c0102z,[D] About to apply for masters - need help selecting the branch. PLEASE.,https://www.reddit.com/r/MachineLearning/comments/c0102z/d_about_to_apply_for_masters_need_help_selecting/,Akainu18448,1560397986,"**Background**: 

I got introduced to this field around 7 months back, circumstances were that I had no internship opportunities in mechanical engineering and a friend of mine was working on his data analyst profile. I have an upcoming undergrad project under a professor in the field of reinforcement learning. My current intern revolves around deep learning and computer vision. Pretty academically oriented - CGPA 8.5 on a 10 point scale which translates to 3.75+ in most colleges.

&amp;#x200B;

**Current situation**:

I mentioned I was a really academically focused student. During my intern here, I have to implement certain computer vision models and stuff related to disparity maps. Issue is, I don't have the necessary background, coming as a mechanical engineer. I researched and found out statistics is an essential part and a lot of CS background is required as well. I plan on going full ahead to pursue a PhD in this field - it doesn't tire me, I enjoy it and in the 3 years of my college life I have never wanted to learn more about a field. Definitely determined to go for a PhD.

&amp;#x200B;

So, my undergrad is unrelated. I have 3 options in front of me now:

* **MSc in Statistics** : I heard this is a good option, but a lot of what they teach isn't applied to Data Science which focused on computational statistics. My knowledge in Statistics is limited to the high school probability class, permutations and combinations and I touched a little on the z-scores and p-scores thing casually. Definitely seems like I need formal education in statistics but I'm worried about going too deep into this when it's not required in Data Science. 
* **MSc in Computer Science** : One of the most sought after degrees right now. I talked to a few batch-mates doing undergrad in CS though, they're also learning of Operating Systems, Database management and hardware. I don't think this will be required AT ALL in the field of my interest. But I'd definitely love an opinion from someone with experience in this regard.
* **MSc in Data Science** : Now this is a rather new branch! I have seen few colleges offering this but I'm skeptical because I read reviews that the colleges are just brushing the topics superficially and not going in depth. If I plan on going for a research I'll need good foundation - that's can't be compromised. It also seems to be a Jack of all trades and a master of none thing. I would have the basic idea of everything - CS and Stats, but a good grasp in neither. Again, would love to hear from someone who is pursuing this.

&amp;#x200B;

Appreciate any support. I have been researching this for days and I was about to write emails to professors/PhD students directly for guidance but thought of asking here first.",13,0
799,2019-6-13,2019,6,13,13,c01am0,Blockchain and Artificial Intelligence: how are they shaping the future?,https://www.reddit.com/r/MachineLearning/comments/c01am0/blockchain_and_artificial_intelligence_how_are/,davidsmith3865,1560399896,,0,1
800,2019-6-13,2019,6,13,13,c01hwx,Career Choices as Electrical Engineering Graduate Student: Computer Vision Engineer vs Hardware Electronics Engineer vs Embedded/Firmware Developer?,https://www.reddit.com/r/MachineLearning/comments/c01hwx/career_choices_as_electrical_engineering_graduate/,half-timesListener,1560401255,[removed],0,1
801,2019-6-13,2019,6,13,14,c01z68,Single depth Telescopic forks for Load Handling Devices,https://www.reddit.com/r/MachineLearning/comments/c01z68/single_depth_telescopic_forks_for_load_handling/,lhd121,1560404695,[removed],0,1
802,2019-6-13,2019,6,13,15,c024e0,DRL Model Zoo for TensorFlow 2,https://www.reddit.com/r/MachineLearning/comments/c024e0/drl_model_zoo_for_tensorflow_2/,zsdh123,1560405748,,0,1
803,2019-6-13,2019,6,13,15,c029dm,Graphics Designing Course Training in Delhi - WizCrafter,https://www.reddit.com/r/MachineLearning/comments/c029dm/graphics_designing_course_training_in_delhi/,Wizcrafter,1560406801,,0,1
804,2019-6-13,2019,6,13,15,c02dl1,AutoCAD Mechanical Institute in Delhi - WizCrafter,https://www.reddit.com/r/MachineLearning/comments/c02dl1/autocad_mechanical_institute_in_delhi_wizcrafter/,Wizcrafter,1560407683,,0,1
805,2019-6-13,2019,6,13,15,c02imb,How to win Kaggle competition?,https://www.reddit.com/r/MachineLearning/comments/c02imb/how_to_win_kaggle_competition/,ajif86,1560408761,,0,1
806,2019-6-13,2019,6,13,15,c02ixa,Basic ML algorithms implementation in Python,https://www.reddit.com/r/MachineLearning/comments/c02ixa/basic_ml_algorithms_implementation_in_python/,yugaank_kalia,1560408833,"https://github.com/Yugaank-Kalia/machine-learning-python

P.S : Check it out and star or for if you like, will be adding more notebooks in the near future

Thanks, bye",0,1
807,2019-6-13,2019,6,13,17,c03aym,Self-Serve Data Preparation for Business Users!,https://www.reddit.com/r/MachineLearning/comments/c03aym/selfserve_data_preparation_for_business_users/,ElegantMicroWebIndia,1560415241,,0,1
808,2019-6-13,2019,6,13,17,c03c78,Need advice for practical LSTM model for video analysis,https://www.reddit.com/r/MachineLearning/comments/c03c78/need_advice_for_practical_lstm_model_for_video/,zoombapup,1560415538,[removed],0,1
809,2019-6-13,2019,6,13,18,c03kz2,[P] A PyTorch implementation of Robust Universal Neural Vocoding.,https://www.reddit.com/r/MachineLearning/comments/c03kz2/p_a_pytorch_implementation_of_robust_universal/,b-shall,1560417522,"Just wanted to share my PyTorch implementation of Amazon's paper [Robust Universal Neural Vocoding](https://arxiv.org/abs/1811.06292)

**Repo:** [https://github.com/bshall/UniversalVocoding](https://github.com/bshall/UniversalVocoding)

**Samples:** [https://bshall.github.io/UniversalVocoding/](https://bshall.github.io/UniversalVocoding/)

**Pretrained Models:** [https://github.com/bshall/UniversalVocoding/releases/tag/v0.1](https://github.com/bshall/UniversalVocoding/releases/tag/v0.1)

&amp;#x200B;

I found that the model trains relatively quickly with intelligible audio after only 20k steps and decent results after 100k steps. Also seems to work well on out of domain speakers (as advertised in the paper).",3,9
810,2019-6-13,2019,6,13,18,c03pxw,[D] How common is it to teach neural networks in stages?,https://www.reddit.com/r/MachineLearning/comments/c03pxw/d_how_common_is_it_to_teach_neural_networks_in/,JoelMahon,1560418681,"For example, a human learns to crawl, then stand, then walk, then run. Learning to run directly would probably be slower.

I've seen plenty of ""neural network learns to run"" youtube videos, but none show a stand training stage.

Is there a formal name for this kind of training? Is it even used in any meaningful way?",39,106
811,2019-6-13,2019,6,13,18,c03tdj,Did I accidentally invent something?,https://www.reddit.com/r/MachineLearning/comments/c03tdj/did_i_accidentally_invent_something/,FlorianDietz,1560419437,"I just noticed that I have been successfully using a regularization technique for Gradient Boosting Trees for years, but its wikipedia page doesn't list it ([https://en.wikipedia.org/wiki/Gradient\_boosting](https://en.wikipedia.org/wiki/Gradient_boosting)) and I can't find it on Google.

&amp;#x200B;

Can someone tell me if this is already known, but under a name I didn't search for, or if it's actually new?

&amp;#x200B;

The idea is this:

&amp;#x200B;

Before training a set of Gradient Boosted Trees, enhance your input data by adding a bunch of new features with completely random values. Since these new features are random, there is no real correlation between them and the output.

&amp;#x200B;

When you train the system with gradient boosting, it will sometimes create Trees that make use of these random features. Whenever this happens, you know for sure that this is a case of overfitting, since those features are completely random. So you can just go ahead and delete that Tree and stop the gradient boosting.

&amp;#x200B;

This technique can also be adapted for Random Forests.

&amp;#x200B;

You can also adapt it for any other machine learning models that allow you to quantify how large the impact of a feature is: Any time that one of the random features has a larger impact than one of the actual features, you can assume you are overfitting. Unlike decision trees it's sometimes non-trivial to remove the dummy features from the trained model, though, which you need to do to make it usable during testing.",0,1
812,2019-6-13,2019,6,13,19,c03z66,[N] Awesome papers and engineering reviews on Computer Vision News of June (with codes!). Links for free reading!,https://www.reddit.com/r/MachineLearning/comments/c03z66/n_awesome_papers_and_engineering_reviews_on/,Gletta,1560420674,"RSIP Vision has just published the June issue of **Computer Vision News**. Here it is for you to read online.

42 pages with exclusive articles on **AI, computer vision and deep learning**.

Subscribe for free on page 42. **Important message about CVPR on page 9!**

[**HTML5 version (recommended)**](https://www.rsipvision.com/ComputerVisionNews-2019June/)

[**PDF version**](https://www.rsipvision.com/computer-vision-news-2019-june-pdf/)

Enjoy!

&amp;#x200B;

https://i.redd.it/c5km6t56m3431.jpg",3,25
813,2019-6-13,2019,6,13,20,c04jpg,[D] How to approach a project with several sets(representations) of features?,https://www.reddit.com/r/MachineLearning/comments/c04jpg/d_how_to_approach_a_project_with_several/,xk86,1560424692,"I am working on a supervised regression problem and I have several different representations of essentially the same input features but with varying formulations and of varying length (model\_i). I also have a list of properties that influence the target variable (props), which I am thinking of adding as a stacked layer. Basically I want to proceed in the following way:

&amp;#x200B;

model\_i --&gt; predict props --&gt; predict target variable, 

&amp;#x200B;

where I compute for each separate model ""i"" and just choose either the best performing result, or take the average over all initial models. Is there a better way to approach this problem? I was thinking of using decision tree methods with this approach. I am not too familiar with neural networks but it seems that they are usually ideal for image/video/audio tasks. Can someone point me in the right direction please. Thank you.",3,6
814,2019-6-13,2019,6,13,20,c04n3e,"Telegram group about deep learning ,machine learning , Artificial Intelligence ,....",https://www.reddit.com/r/MachineLearning/comments/c04n3e/telegram_group_about_deep_learning_machine/,Doctor_who1,1560425325,[removed],0,1
815,2019-6-13,2019,6,13,20,c04oye,Clarification on the thought process around Outlier/Anomaly detection,https://www.reddit.com/r/MachineLearning/comments/c04oye/clarification_on_the_thought_process_around/,Pillus,1560425651,[removed],0,1
816,2019-6-13,2019,6,13,20,c04v7d,[Newcomer] I need correct terminology and tutorials on how to teach AI how to create stuff from examples [Using Tensorflow Keras],https://www.reddit.com/r/MachineLearning/comments/c04v7d/newcomer_i_need_correct_terminology_and_tutorials/,giltine528,1560426799,[removed],0,1
817,2019-6-13,2019,6,13,20,c04x18,Is there a library in python through which i can figure out similarity or relatedness between 2 phrases?,https://www.reddit.com/r/MachineLearning/comments/c04x18/is_there_a_library_in_python_through_which_i_can/,flabbychicken,1560427125,[removed],0,1
818,2019-6-13,2019,6,13,21,c059tx,"Professor Shai Shen-Orr PhD., Associate Professor at Technion - Israel Institute of Technology, and Founder and Chief Scientist CytoReason, Discussing Machine Learning and Speeding Up Drug Discovery",https://www.reddit.com/r/MachineLearning/comments/c059tx/professor_shai_shenorr_phd_associate_professor_at/,bioquarkceo,1560429321,,0,1
819,2019-6-13,2019,6,13,21,c05gm2,How to use CNN-LSTM architecture for video classification?,https://www.reddit.com/r/MachineLearning/comments/c05gm2/how_to_use_cnnlstm_architecture_for_video/,ajeenkkya,1560430456,[removed],0,1
820,2019-6-13,2019,6,13,21,c05huy,Accuracy decrease in production after adding additional input datas,https://www.reddit.com/r/MachineLearning/comments/c05huy/accuracy_decrease_in_production_after_adding/,ManInmoon2000,1560430663,[removed],0,1
821,2019-6-13,2019,6,13,21,c05ias,"[D] This might be better suited for ""learnmachinelearning"" or stack overflow, but I feel Im a step past that. Any tips on increasing numpy to tensor performance?",https://www.reddit.com/r/MachineLearning/comments/c05ias/d_this_might_be_better_suited_for/,halfassadmin,1560430730,"So I've used serpent, mss, pil, custom buffer transfer from Windows to numpy and no matter what you're topping out at 60ish fps. So we have about 60 fps capture running in its own thread. Then I have yolov3 tiny which when capturing from a on disk video can process at 10-20ms per frame. Cool. Also about 60 fps. 

I think im losing an additional 15ms when I do expand, and I think it's resize. So you have an image that is a numpy array and you expand it (which I think just adds a dimension), then you resize it for processing by the model. Those 2 lines are killing me. 

For people that are doing things like real time game play, how are you handling your pipeline in?

Edit: too, I've read pytorch is faster (I'm dying reimplementing this thing so many times though) would anyone agree?",1,0
822,2019-6-13,2019,6,13,22,c05jco,AI FOR YOUR BUSINESS,https://www.reddit.com/r/MachineLearning/comments/c05jco/ai_for_your_business/,clarke2106,1560430893,[removed],0,1
823,2019-6-13,2019,6,13,22,c05r7j,[R] Fast Task Inference with Variational Intrinsic Successor Features,https://www.reddit.com/r/MachineLearning/comments/c05r7j/r_fast_task_inference_with_variational_intrinsic/,zergylord,1560432114,"It has been established that diverse behaviors spanning the controllable subspace of an Markov decision process can be trained by rewarding a policy for being distinguishable from other policies. However, one limitation of this formulation is generalizing behaviors beyond the finite set being explicitly learned, as is needed for use on subsequent tasks. Successor features provide an appealing solution to this generalization problem, but require defining the reward function as linear in some grounded feature space. In this paper, we show that these two techniques can be combined, and that each method solves the other's primary limitation. To do so we introduce Variational Intrinsic Successor FeatuRes (VISR), a novel algorithm which learns controllable features that can be leveraged to provide enhanced generalization and fast task inference through the successor feature framework. We empirically validate VISR on the full Atari suite, in a novel setup wherein the rewards are only exposed briefly after a long unsupervised phase. Achieving human-level performance on 14 games and beating all baselines, we believe VISR represents a step towards agents that rapidly learn from limited feedback.

[https://arxiv.org/abs/1906.05030](https://arxiv.org/abs/1906.05030)",9,19
824,2019-6-13,2019,6,13,22,c05xkk,"How Alexa does ""active learning"", or automatically selecting high-value training examples for machine learning",https://www.reddit.com/r/MachineLearning/comments/c05xkk/how_alexa_does_active_learning_or_automatically/,georgecarlyle76,1560433113,,0,1
825,2019-6-13,2019,6,13,22,c061bb,[D] What are your favorite ML articles of May 2019?,https://www.reddit.com/r/MachineLearning/comments/c061bb/d_what_are_your_favorite_ml_articles_of_may_2019/,BastouBab,1560433687,,0,3
826,2019-6-13,2019,6,13,22,c062i0,[D] Using embeddings to improve upon NER,https://www.reddit.com/r/MachineLearning/comments/c062i0/d_using_embeddings_to_improve_upon_ner/,throwawat2312,1560433875,"I'm currently using Spacy (CRF-based) and updating/training it with my own tags for a data set. However, the results aren't too great. I was wondering if there was a simple way use embeddings like word2vec or the like and just improve the approach.



Thanks!",12,6
827,2019-6-13,2019,6,13,22,c0656j,"I have 4 months of free time until i start college, how can i effectively use this spare time to learn Machine Learning with Python? (Very *little* knowledge of python, but my math knowledge is solid)",https://www.reddit.com/r/MachineLearning/comments/c0656j/i_have_4_months_of_free_time_until_i_start/,VagaScot,1560434274,"So as the title says, i have 4 months, i plan to study around \~12 hours a day until i start college (and continue studying) Machine Learning with Python, without just starting a random Udemy course, what would you guys recommend? 

&amp;#x200B;

I saw a picture when i was having a look:

* 8 months learning **Python Core**
* 6 months learning **Pandas**
* 2 months learning **Matplotlib**
* 6 months learning **SciKit-Learn**
* 6 months learning **Keras**

But i was hoping in the 4 months to maybe spread out the learning, i know a **little** of python core (roughly 2-3 weeks) like..

    time = 1200
    if time == 1200:
      print(""The time is: {}"".format(time))

&amp;#x200B;

    words = [""one"", ""two"", ""three""]
    
    for word in words:
        print(word)

&amp;#x200B;

    double_word = ['one', 'two', 'two', 'three']
    count_double = {}
    
    for word in double_word:
        if word not in double_word:
            double_word[word] = 1
        else:
            double_word[word] += 1
    print(double_word)

These are just off the top of my head after roughly 3 weeks of checking out python core.

&amp;#x200B;

So what do you guys think? Just spend a month each learning Pandas, Matplotlib, SciKit-Learn and then Keras? or any advice?",0,1
828,2019-6-13,2019,6,13,23,c06q0b,ml.lib 1.0.2 release announcement,https://www.reddit.com/r/MachineLearning/comments/c06q0b/mllib_102_release_announcement/,NiccoloGranieri,1560437264,[removed],0,1
829,2019-6-13,2019,6,13,23,c06r54,[P] CURL: How to learn better sentence embeddings,https://www.reddit.com/r/MachineLearning/comments/c06r54/p_curl_how_to_learn_better_sentence_embeddings/,kingcai,1560437430,"Hi there, 

I've been working on a project about SOTA methods in learning sentence embeddings. In particular I took a look at [Quickthoughts](https://arxiv.org/pdf/1803.02893.pdf), which uses a word2vec-like objective, seeking to identify ""related"" sentences.

As such I've produced an open PyTorch implementation of QuickThoughts here: https://github.com/jcaip/quickthoughts

In adddition I tried to use the theoretical framework described by [Arora et al](https://www.offconvex.org/2019/03/19/CURL/). about contrastive unsupervised representation learning, to examine the effect of changing the context_size of Quickthoughts, and offer a possible modification that should increase performance. Unfortunately this was unsuccessful, but hopefully you'll still find the work interesting. 

Post: https://jcaip.github.io/Quickthoughts/

Please lmk if you have any questions/comments, thanks for reading!",5,1
830,2019-6-14,2019,6,14,0,c06z8j,Very Cheap GPU Dedicated Servers for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/c06z8j/very_cheap_gpu_dedicated_servers_for_machine/,render_rapidly,1560438557,[removed],0,1
831,2019-6-14,2019,6,14,0,c079cc,[D] Best Of AI articles of May 2019,https://www.reddit.com/r/MachineLearning/comments/c079cc/d_best_of_ai_articles_of_may_2019/,__yannickw__,1560439929,I just came across a Best Of AI articles of May 2019. What do you think of this selection? What are your favorite AI articles of the past month?,0,1
832,2019-6-14,2019,6,14,0,c07fd3,[P] Scaling DeepSpeech using Mixed Precision and KubeFlow,https://www.reddit.com/r/MachineLearning/comments/c07fd3/p_scaling_deepspeech_using_mixed_precision_and/,Stormfreek,1560440781,"An overview of changes we've made to our PyTorch ASR training to integrate KubeFlow and mixed-precision to speed up and scale our ASR training pipeline:

https://medium.com/@seannaren/scaling-deepspeech-using-mixed-precision-and-kubeflow-b83965e79173

Code: https://github.com/SeanNaren/deepspeech.pytorch

KubeFlow branch coming very soon (need to update our scripts since KubeFlow has improved since).

Would love to hear any feedback or questions!",4,20
833,2019-6-14,2019,6,14,0,c07gw5,Hugging Face has released a new demo where you can type texts with GPT-2 helping you,https://www.reddit.com/r/MachineLearning/comments/c07gw5/hugging_face_has_released_a_new_demo_where_you/,jikkii,1560440998,[removed],0,1
834,2019-6-14,2019,6,14,1,c07mm6,[D] Best Of AI articles of May 2019,https://www.reddit.com/r/MachineLearning/comments/c07mm6/d_best_of_ai_articles_of_may_2019/,__yannickw__,1560441738,I just came across a [Best Of AI articles of May 2019](https://blog.sicara.com/05-2019-best-ai-new-articles-this-month-4a202e173e5a). What do you think of this selection? What are your favorite AI articles of the past month?,2,0
835,2019-6-14,2019,6,14,1,c07s68,Are Weights Really Important to Neural Networks?,https://www.reddit.com/r/MachineLearning/comments/c07s68/are_weights_really_important_to_neural_networks/,Yuqing7,1560442466,,0,1
836,2019-6-14,2019,6,14,1,c089lf,Sparse multilayer perceptrons: converting CNNs to MLPs,https://www.reddit.com/r/MachineLearning/comments/c089lf/sparse_multilayer_perceptrons_converting_cnns_to/,aul12,1560444812,,0,1
837,2019-6-14,2019,6,14,1,c08ana,[D] Quoc Le is silently killing it in 2019,https://www.reddit.com/r/MachineLearning/comments/c08ana/d_quoc_le_is_silently_killing_it_in_2019/,thatguydr,1560444953,"Over the past 6 months, he's been an author on

* EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
* Attention Augmented Convolutional Networks
* Transformer-XL: Attentive language models beyond a fixed-length context
* The Effect of Network Width on Stochastic Gradient Descent and Generalization
* Unsupervised Data Augmentation
* Soft Conditional Computation
* Selfie: Self-supervised Pretraining for Image Embedding
* NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection
* Specaugment: A simple data augmentation method for automatic speech recognition
* Using videos to evaluate image model robustness
* Searching for MobileNetV3
* The Evolved Transformer

I know he always does great research, but is there anyone else who's had this much impact recently? That list of papers is impressive, and for them all to have come out in a six month period with contributions from the same author is almost ridiculous.

I figured he should get a shout out. If there's discussion to be had, I'd love to know who else has been moving the field (maybe in other areas like RL or NLP) as much recently.",3,2
838,2019-6-14,2019,6,14,1,c08beb,[N] Deep Graph Library v0.3 release,https://www.reddit.com/r/MachineLearning/comments/c08beb/n_deep_graph_library_v03_release/,jermainewang,1560445050,"Graph Neural Network has become the new fashion in many graph-based learning problems. Deep Graph Library (DGL) is a Python package built for easy implementation of graph neural network model family, on top of existing DL frameworks (e.g. PyTorch, MXNet, Gluon etc.). As the team behind this library, we want to share with you the new release of DGL (v0.3) that is much faster (up to 19x faster) and more scalable for training GNNs on large graphs (up to 8x larger). Checkout our full release note here: [https://www.dgl.ai/release/2019/06/12/release.html](https://www.dgl.ai/release/2019/06/12/release.html) .

&amp;#x200B;

For whom have never heard of DGL or Graph Neural Network, maybe it is worth to take a look at this new trend of geometric deep learning. Some links here:

* Checkout this 10-minute tutorial about how to use Graph Neural Network to predict community membership ([https://docs.dgl.ai/tutorials/basics/1\_first.html](https://docs.dgl.ai/tutorials/basics/1_first.html)).
* Checkout more about how a variety of models can be unified under the message passing framework and can be implemented in DGL ([https://docs.dgl.ai/tutorials/models/index.html](https://docs.dgl.ai/tutorials/models/index.html)).
* Our github repo: [https://github.com/dmlc/dgl](https://github.com/dmlc/dgl)
* Our project site: [https://www.dgl.ai/](https://www.dgl.ai/) . We publish many blogs about the new findings in this area.",6,48
839,2019-6-14,2019,6,14,2,c08m5e,[P] Residual Convolution Recurrent Network for ECG Classification,https://www.reddit.com/r/MachineLearning/comments/c08m5e/p_residual_convolution_recurrent_network_for_ecg/,hadaev,1560446478,,1,1
840,2019-6-14,2019,6,14,2,c08p4v,Deep learning for metal alloy design,https://www.reddit.com/r/MachineLearning/comments/c08p4v/deep_learning_for_metal_alloy_design/,IborkedyourGPU,1560446863,[removed],0,1
841,2019-6-14,2019,6,14,2,c08sar,Machine Learning Tutorial Part 7 | Machine Learning For Beginners - 5-Fold Cross Validation,https://www.reddit.com/r/MachineLearning/comments/c08sar/machine_learning_tutorial_part_7_machine_learning/,SquareTechAcademy,1560447281,,0,1
842,2019-6-14,2019,6,14,2,c08unk,Machine learning,https://www.reddit.com/r/MachineLearning/comments/c08unk/machine_learning/,majd0507,1560447603,[removed],0,1
843,2019-6-14,2019,6,14,2,c08yoi,How do facial recognition algos differentiate between real-life and photos/computer screen faces?,https://www.reddit.com/r/MachineLearning/comments/c08yoi/how_do_facial_recognition_algos_differentiate/,bigDATAbig,1560448113,[removed],0,1
844,2019-6-14,2019,6,14,2,c091de,K Nearest Neighbor,https://www.reddit.com/r/MachineLearning/comments/c091de/k_nearest_neighbor/,AMehtoliya,1560448469,[removed],0,1
845,2019-6-14,2019,6,14,2,c0935q,"Best Telegram Groups about machine learning , deeep learning , ai ,..",https://www.reddit.com/r/MachineLearning/comments/c0935q/best_telegram_groups_about_machine_learning_deeep/,Doctor_who1,1560448705,[removed],0,1
846,2019-6-14,2019,6,14,3,c09e38,NVIDIA GauGAN beta available as a web service,https://www.reddit.com/r/MachineLearning/comments/c09e38/nvidia_gaugan_beta_available_as_a_web_service/,mingyuliutw,1560450126,,0,1
847,2019-6-14,2019,6,14,3,c09gqt,[R] NVIDIA GauGAN beta available as a web service,https://www.reddit.com/r/MachineLearning/comments/c09gqt/r_nvidia_gaugan_beta_available_as_a_web_service/,mingyuliutw,1560450482,,0,1
848,2019-6-14,2019,6,14,3,c09kmy,Google TensorNetwork Library Dramatically Accelerates ML &amp; Physics Tasks,https://www.reddit.com/r/MachineLearning/comments/c09kmy/google_tensornetwork_library_dramatically/,Yuqing7,1560450991,,0,1
849,2019-6-14,2019,6,14,4,c09ytj,[R] NVIDIA GauGAN is available online.,https://www.reddit.com/r/MachineLearning/comments/c09ytj/r_nvidia_gaugan_is_available_online/,mingyuliutw,1560452898,,1,1
850,2019-6-14,2019,6,14,4,c0a2uo,[P] Interactive demo - Write with Transformer,https://www.reddit.com/r/MachineLearning/comments/c0a2uo/p_interactive_demo_write_with_transformer/,Thomjazz,1560453441,[removed],0,1
851,2019-6-14,2019,6,14,4,c0aaw6,Looking for CVPR2019 ticket,https://www.reddit.com/r/MachineLearning/comments/c0aaw6/looking_for_cvpr2019_ticket/,santacruzmab,1560454528,[removed],0,1
852,2019-6-14,2019,6,14,4,c0af0h,How to implement LSTM in Tensorflow?,https://www.reddit.com/r/MachineLearning/comments/c0af0h/how_to_implement_lstm_in_tensorflow/,Jandevries101,1560455105,[removed],0,1
853,2019-6-14,2019,6,14,4,c0agk4,U-Net for brain MRI segmentation on PyTorch Hub,https://www.reddit.com/r/MachineLearning/comments/c0agk4/unet_for_brain_mri_segmentation_on_pytorch_hub/,ketsok,1560455320,,0,1
854,2019-6-14,2019,6,14,4,c0ahhu,[P]How to scrape the ImageNet,https://www.reddit.com/r/MachineLearning/comments/c0ahhu/phow_to_scrape_the_imagenet/,MartinSpartanX,1560455450,,0,1
855,2019-6-14,2019,6,14,4,c0aj1x,This conversation on deep learning and open source on the Debian Science mailing list is getting really interesting,https://www.reddit.com/r/MachineLearning/comments/c0aj1x/this_conversation_on_deep_learning_and_open/,[deleted],1560455659,[deleted],1,1
856,2019-6-14,2019,6,14,4,c0ajmr,[D] This conversation about machine learning and open source on the Debian Science mailing list is getting really interesting,https://www.reddit.com/r/MachineLearning/comments/c0ajmr/d_this_conversation_about_machine_learning_and/,radarsat1,1560455737,,0,1
857,2019-6-14,2019,6,14,5,c0amku,[P] Write with Transformer,https://www.reddit.com/r/MachineLearning/comments/c0amku/p_write_with_transformer/,Thomjazz,1560456126,"HuggingFace has published a new web interface for the small (117M parameter) and medium (345M parameter) versions of GPT-2 that let you interact directly with the model, by writing a prompt, asking for several completions using a customizable decoder, editing the completions if needed and asking further completions.

It's a great way to prob how the model reacts to various probing patterns and content, what kind of common sense is stored in the model and to test future interfaces of creative writing in which a human and a language model could collaborate together.

It can be accessed here: https://transformer.huggingface.co",14,89
858,2019-6-14,2019,6,14,5,c0b06w,"Current SOTA techniques to handle word variations (plurals, verb conjugations, hyphens, etc.) in NLP?",https://www.reddit.com/r/MachineLearning/comments/c0b06w/current_sota_techniques_to_handle_word_variations/,pamessina,1560457976,[removed],0,1
859,2019-6-14,2019,6,14,5,c0b2a9,[R] NVIDIA GauGAN demo online (https://nvda.ws/2WsY2cM),https://www.reddit.com/r/MachineLearning/comments/c0b2a9/r_nvidia_gaugan_demo_online_httpsnvdaws2wsy2cm/,mingyuliutw,1560458264,,0,1
860,2019-6-14,2019,6,14,5,c0b3q8,[R] [1906.01815] Towards Multimodal Sarcasm Detection (An _Obviously_ Perfect Paper),https://www.reddit.com/r/MachineLearning/comments/c0b3q8/r_190601815_towards_multimodal_sarcasm_detection/,bryant1410,1560458451,,3,9
861,2019-6-14,2019,6,14,5,c0b9c8,[P] How I wrote a tool for creating datasets from ImageNet,https://www.reddit.com/r/MachineLearning/comments/c0b9c8/p_how_i_wrote_a_tool_for_creating_datasets_from/,MartinSpartanX,1560459231,"It started as a need for a dataset with images and turned into writing a tool which can download parts of Imagenet using its API. I also created a little analysis of the current state of the ImageNet URLs from its API which helped to significantly improve the downloader.  

Here I wrote about the process: [https://mf1024.github.io/2019/06/09/how-to-scrape-the-imagenet/](https://mf1024.github.io/2019/06/09/how-to-scrape-the-imagenet/)

I hope it's helpful to others :)",0,0
862,2019-6-14,2019,6,14,5,c0ba59,[R] When to use parametric models in reinforcement learning?,https://www.reddit.com/r/MachineLearning/comments/c0ba59/r_when_to_use_parametric_models_in_reinforcement/,jbmlres,1560459347,"**Paper**: [https://arxiv.org/abs/1906.05243](https://arxiv.org/abs/1906.05243)

**Abstract**: We examine the question of when and how parametric models are most useful in reinforcement learning. In particular, we look at commonalities and differences between parametric models and experience replay. Replay-based learning algorithms share important traits with model-based approaches, including the ability to plan: to use more computation without additional data to improve predictions and behaviour. We discuss when to expect benefits from either approach, and interpret prior work in this context. We hypothesise that, under suitable conditions, replay-based algorithms should be competitive to or better than model-based algorithms if the model is used only to generate fictional transitions from observed states for an update rule that is otherwise model-free. We validated this hypothesis on Atari 2600 video games. The replay-based algorithm attained state-of-the-art data efficiency, improving over prior results with parametric models.",2,11
863,2019-6-14,2019,6,14,6,c0bjdz,[P] How I wrote a tool for creating datasets from ImageNet,https://www.reddit.com/r/MachineLearning/comments/c0bjdz/p_how_i_wrote_a_tool_for_creating_datasets_from/,MartinSpartanX,1560460673,"It started as a need for a dataset with images and turned into writing a tool which can download parts of Imagenet using its API. I also created a little analysis of the current state of the ImageNet URLs from its API which helped to significantly improve the downloader.

Here I wrote about the process: [https://mf1024.github.io/2019/06/09/how-to-scrape-the-imagenet/](https://mf1024.github.io/2019/06/09/how-to-scrape-the-imagenet/)

I hope it's helpful to others and comments are appreciated.",1,3
864,2019-6-14,2019,6,14,6,c0btc4,TensorNetwork: A Library for Physics and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/c0btc4/tensornetwork_a_library_for_physics_and_machine/,ai-lover,1560462045,[removed],0,1
865,2019-6-14,2019,6,14,7,c0ckrm,Why does a company like google choose to open source and release research papers?,https://www.reddit.com/r/MachineLearning/comments/c0ckrm/why_does_a_company_like_google_choose_to_open/,kcusefil,1560466079,"A company like google, which has so much data and invests so much money in research has released many research findings in the field of machine learning. These findings introduce ideas that other competitors are able to use. So what benefit does Google have to release their findings rather than to just keep it secret so they can improve their own performance, building up a monopoly. 

If any of my understanding is wrong, please correct me. Thanks!",0,1
866,2019-6-14,2019,6,14,7,c0cl1t,"[D] Is one year of master's ""sufficient""?",https://www.reddit.com/r/MachineLearning/comments/c0cl1t/d_is_one_year_of_masters_sufficient/,diditi,1560466124,"Hello,

Throughout my search for promising master's programs in Europe (and the UK :) ), I've seen the trend that some of the top programs (mostly located in the UK) offer only one year of study. Since I'm interested in pursuing a PhD, I feel like my master's should mostly be spent on branching out my knowledge in various ML domains as much as possible to better guide me to an interesting research area. However, with only two semesters of study, where the second one also includes a master's thesis, this goes against my notion.  Moreover, I would like to join a research group while doing master's, and I feel like having everything in a single year could become overwhelming.

&amp;#x200B;

Is my understanding ""wrong""? There are many great institutions in the UK offering a 1-year master's program and I would really like to like this shorter-term master's idea. Ideally, I would like to do PhD in the same institution as I did my master's and thus I don't want to miss out.

&amp;#x200B;

So, is one year of master's sufficient?

&amp;#x200B;

Thank you!",14,0
867,2019-6-14,2019,6,14,7,c0cnka,Which institutions (education/corporate) are doing a lot of research on NLP?,https://www.reddit.com/r/MachineLearning/comments/c0cnka/which_institutions_educationcorporate_are_doing_a/,ndo3,1560466524,[removed],0,1
868,2019-6-14,2019,6,14,7,c0cp5p,Why does Google choose to open source and release research findings?,https://www.reddit.com/r/MachineLearning/comments/c0cp5p/why_does_google_choose_to_open_source_and_release/,greterness,1560466786,[removed],0,1
869,2019-6-14,2019,6,14,8,c0d07d,[D] Update on the GPT2 replication claim,https://www.reddit.com/r/MachineLearning/comments/c0d07d/d_update_on_the_gpt2_replication_claim/,__arch__,1560468512,,0,2
870,2019-6-14,2019,6,14,8,c0d90b,[P] GauGAN - Turn scribbles into masterpieces with GANs,https://www.reddit.com/r/MachineLearning/comments/c0d90b/p_gaugan_turn_scribbles_into_masterpieces_with/,an_ace_of_spades,1560470021,,0,1
871,2019-6-14,2019,6,14,9,c0deua,"What are the SOTA techniques in NLP to handle morphological variations in words (e.g. plurals, verb conjugations, hyphens, etc.)?",https://www.reddit.com/r/MachineLearning/comments/c0deua/what_are_the_sota_techniques_in_nlp_to_handle/,pamessina,1560471025,"I need to process natural language sentences in which words can appear with morphological variations: car -&gt; cars; play -&gt; playing, played; etc. There might be hyphens also, e.g. ""dog-friendly hotel"", ""load-bearing walls"", ""rock-hard cake"", etc. What do current SOTA techniques in NLP do to handle this variability in natural language? Would it be a good idea to use ""character embeddings"" (instead of word embeddings) and use any model (e.g. Transformer, biLSTM) on top of that?

Motivating context: I'm working on VQA (visual question answering), and I'm still trying to figure out how to best implement the NLP branch that processes the questions. I would like it to be robust to morphological variations in words, which happen all the time in natural language.",0,1
872,2019-6-14,2019,6,14,9,c0dg2a,2019 Google Gravity Games - Autonomously Steering a Gravity-Powered Soapbox Car,https://www.reddit.com/r/MachineLearning/comments/c0dg2a/2019_google_gravity_games_autonomously_steering_a/,csapidus,1560471214,[removed],0,1
873,2019-6-14,2019,6,14,9,c0dhxh,Can an expert answer some questions I have about decision trees?,https://www.reddit.com/r/MachineLearning/comments/c0dhxh/can_an_expert_answer_some_questions_i_have_about/,mydogissnoring,1560471529,[removed],0,1
874,2019-6-14,2019,6,14,9,c0dmiv,[P] Convolutional Neural Networks: an Introduction (TensorFlow Eager API),https://www.reddit.com/r/MachineLearning/comments/c0dmiv/p_convolutional_neural_networks_an_introduction/,strikingLoo,1560472286,,0,1
875,2019-6-14,2019,6,14,9,c0dnv7,Stphane Mallat and Naftali Tishby,https://www.reddit.com/r/MachineLearning/comments/c0dnv7/stphane_mallat_and_naftali_tishby/,4m0rf4t1,1560472524,[removed],0,1
876,2019-6-14,2019,6,14,10,c0eel6,[Research] Tackling Climate Change with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/c0eel6/research_tackling_climate_change_with_machine/,hardmaru,1560477285,,7,13
877,2019-6-14,2019,6,14,11,c0ehcr,How to increase OpenAI's 345M model's character limit?,https://www.reddit.com/r/MachineLearning/comments/c0ehcr/how_to_increase_openais_345m_models_character/,Cloudcry,1560477764,[removed],0,1
878,2019-6-14,2019,6,14,11,c0ewxg,Anyone using Spark for feature extraction and TF/PyTorch for DL?,https://www.reddit.com/r/MachineLearning/comments/c0ewxg/anyone_using_spark_for_feature_extraction_and/,CrossEntropyLoss,1560480601,[removed],0,1
879,2019-6-14,2019,6,14,12,c0f5pw,[P] Techniques to Tackle Overfitting and Achieve Robustness for Donkey Car Neural Network Self-Driving Agent,https://www.reddit.com/r/MachineLearning/comments/c0f5pw/p_techniques_to_tackle_overfitting_and_achieve/,baylearn,1560482203,,0,1
880,2019-6-14,2019,6,14,12,c0fcs2,[R] Training Neural Networks for and by Interpolation,https://www.reddit.com/r/MachineLearning/comments/c0fcs2/r_training_neural_networks_for_and_by/,xternalz,1560483588,,3,7
881,2019-6-14,2019,6,14,12,c0fee5,Dataset for Raman Spectroscopy,https://www.reddit.com/r/MachineLearning/comments/c0fee5/dataset_for_raman_spectroscopy/,SaashaJoshi,1560483900,[removed],0,1
882,2019-6-14,2019,6,14,13,c0fsni,[P] Tutorial to build a complete project in credit card fraud detection with Machine Learning!,https://www.reddit.com/r/MachineLearning/comments/c0fsni/p_tutorial_to_build_a_complete_project_in_credit/,Slight_Role,1560486363,"Machine Learning has numerous applications in todays technological world. One of its key applications comes in detecting credit card frauds. Credit card fraud has become quite common with time. Detecting it can often prove to be a strenuous task and sometimes it can take a long time after the act is committed. But with machine learning, the process becomes faster and more accurate. 

There are many tutorials that explain how machine learning helps in detecting credit card fraud. But this video, explains machine learning with a project. 

Checkout [**https://youtu.be/9cXeEwOXWU8**](https://youtu.be/9cXeEwOXWU8) to create project in credit card fraud detection.",2,0
883,2019-6-14,2019,6,14,13,c0fsu6,"[D] Google Translate ""happy to see hong kong become China""",https://www.reddit.com/r/MachineLearning/comments/c0fsu6/d_google_translate_happy_to_see_hong_kong_become/,bulc381,1560486395,"Just saw some discussions about Google translating the English sentence ""so sad to see hong kong become China"" into ""so **happy** to see hong kong become China"" in Chinese.

Video: [https://www.youtube.com/watch?v=Lf5\_u4x-rw4](https://www.youtube.com/watch?v=Lf5_u4x-rw4)

Seems that people are reporting this error so that specific sentence was corrected right now. Is it an attempt to attack Google Translate by providing false examples?",39,300
884,2019-6-14,2019,6,14,13,c0ftw8,"Is ""A Course in Machine Learning"" by Daume a readable book?",https://www.reddit.com/r/MachineLearning/comments/c0ftw8/is_a_course_in_machine_learning_by_daume_a/,DelverOfSeacrest,1560486591,[removed],0,1
885,2019-6-14,2019,6,14,13,c0fwqk,Here are some steps for you to setup Jupyter Notebook as a Windows Service running its own Python 3 virtual environment.,https://www.reddit.com/r/MachineLearning/comments/c0fwqk/here_are_some_steps_for_you_to_setup_jupyter/,TechSpreader,1560487087,,0,1
886,2019-6-14,2019,6,14,14,c0gdbe,[D] ML for mathematicians,https://www.reddit.com/r/MachineLearning/comments/c0gdbe/d_ml_for_mathematicians/,oops513,1560490301,"Various versions of this questions have been asked, but a lot of what I've seen focuses on people coming from statistics/analysis/etc backgrounds in math. My focus is in algebraic geometry and homotopy theory, so it's very far removed from most of those areas that appear directly in typical ML papers. Does anyone have any reading/resource recommendations that draw from the more algebraic side of math, or that would be most interesting to someone in AG/homotopy theory? Thanks!",8,8
887,2019-6-14,2019,6,14,15,c0h1gx,Pretty 2019 CVPR Papers: https://mattdeitke.github.io/CVPR-2019/,https://www.reddit.com/r/MachineLearning/comments/c0h1gx/pretty_2019_cvpr_papers/,mattdeitke,1560495418,[removed],0,1
888,2019-6-14,2019,6,14,16,c0h2jv,My 1st Machine learning challenge,https://www.reddit.com/r/MachineLearning/comments/c0h2jv/my_1st_machine_learning_challenge/,TrusterZero,1560495657,[removed],0,1
889,2019-6-14,2019,6,14,16,c0h5fn,[P] My first ML project,https://www.reddit.com/r/MachineLearning/comments/c0h5fn/p_my_first_ml_project/,TrusterZero,1560496300,"Hi everyone working on my 1st ML project and I'm looking for some tips. 

I want to predict the income of a store (clothing store for example) by comparing multiple statistics that might influence the income (rain, temperature, day in month etc.) first question will be if this is even possible at all and where to start. I think I can get 3 years of income details of a clothing store that would be the 1st step but where do I go from there?

Pre-thanks guys and gals",6,0
890,2019-6-14,2019,6,14,16,c0h5mu,NLP EVALUATION??,https://www.reddit.com/r/MachineLearning/comments/c0h5mu/nlp_evaluation/,FoCDoT,1560496345,Is there any way to evaluate unsupervised NLP task if I use k means or LSA??,0,1
891,2019-6-14,2019,6,14,16,c0h7s0,"AI is improving our daily life, even without noticing it, it is known that social media use it, but what about our city? What advantages does it have? Let's find out!",https://www.reddit.com/r/MachineLearning/comments/c0h7s0/ai_is_improving_our_daily_life_even_without/,evadeltor,1560496834,,0,1
892,2019-6-14,2019,6,14,16,c0h8py,[N] Pretty 2019 CVPR Papers: https://mattdeitke.github.io/CVPR-2019/,https://www.reddit.com/r/MachineLearning/comments/c0h8py/n_pretty_2019_cvpr_papers/,mattdeitke,1560497050,"Hello, everyone! I made a page that displays the 2019 CVPR Accepted Papers, in a way that is more parsable and easier to sort through. The primary page display is below, which loads in the 1294 accepted papers for this year. In particular, you can easily access the PDFs, view glimpses of the paper through the thumbnail, sort by LDA topics, sort by similar papers, copy and display the BibTeX, and show (or hide) each abstract and LDA topics. The page is available at [https://mattdeitke.github.io/CVPR-2019/](https://mattdeitke.github.io/CVPR-2019/).

[Display of the page for 2019 CVPR Accepted Papers](https://i.redd.it/qrsfnr48x9431.jpg)

This is in **comparison to what CVPR Open Access** adds to their website for the published papers, which is shown below.

[How CVPR displays their papers on the website](https://i.redd.it/9uk1tj6bx9431.png)

I've also **open sourced the scripts** used to generate this page, many of which are thanks to Andrej Karpathy's NeurIPS guide and ArXiV Sanity Preserver, on my **GitHub page** at [**https://github.com/mattdeitke/CVPR2019**](https://github.com/mattdeitke/CVPR2019).

Let me know if you have any suggestions for improvement!",2,27
893,2019-6-14,2019,6,14,17,c0hjzu,Deep learning telegram group and car learning with the collaboration of Kursera website and world experts and researchers including andrew ng,https://www.reddit.com/r/MachineLearning/comments/c0hjzu/deep_learning_telegram_group_and_car_learning/,Doctor_who1,1560499851,[removed],0,1
894,2019-6-14,2019,6,14,17,c0hmmj,"Telegram group about deep learning ,machine learning , Artificial Intelligence ,.... ....",https://www.reddit.com/r/MachineLearning/comments/c0hmmj/telegram_group_about_deep_learning_machine/,Doctor_who1,1560500532,[removed],0,1
895,2019-6-14,2019,6,14,17,c0ho4h,[D] Learning the rotation of 2d images with a CNN,https://www.reddit.com/r/MachineLearning/comments/c0ho4h/d_learning_the_rotation_of_2d_images_with_a_cnn/,the_best_broccoli,1560500920,"Hi all,
this is my first post here so I hope I do this right...

I'm currently trying to get a CNN to learn the rotation angle of 2D images. I tried to make a post on stackoverflow first, because I didn't know if the question fits into this subreddit, but so far that thread didn't gain any traction... [see here](https://stackoverflow.com/questions/56462202/cnn-for-2d-image-rotation-estimation-angle-regression).

I hope it's ok when I just quote the question from there:


&gt; I am trying to build a CNN (in Keras) that can estimate the rotation of an image (or a 2d object). So basically, the input is an image and the output should be its rotation.
&gt; 
&gt; My first experiment is to estimate the rotation of MIST digits (starting with only one digit ""class"", let's say the ""3""). So what I did was extracting all 3s from the MNIST set, and then building a ""rotated 3s"" dataset, by randomly rotating these images multiple times, and storing the rotated images together with their rotation angles as ground truth labels.
&gt; 
&gt; So my first problem was that a 2d rotation is cyclic and I didn't know how to model this behavior. Therefore, I encoded the angle as y=sin(ang), x = cos(ang). This gives me my dataset (the rotated 3s images) and the corresponding labels (x and y values).
&gt; 
&gt; For the CNN, as a start, i just took the keras MNIST CNN example (https://keras.io/examples/mnist_cnn/) and replaced the last dense layer (that had 10 outputs and a softmax activation) with a dense layer that has 2 outputs (x and y) and a tanh activation (since y=sin(ang), x = cos(ang) are within [-1,1]).
&gt; 
&gt; The last thing i had to decide was the loss function, where i basically want to have a distance measurement for angles. Therefore i thought ""cosine_proximity"" is the way to go.
&gt; 
&gt; When training the network I can see that the loss is decreasing and converging to a certain point. However when I then check the predictions vs the ground truth I observe a (for me) fairly surprising behavior. Almost all x and y predictions tend towards 0 or +/-1. And since the ""decoding"" of my rotation is ang=atan2(y,x) the predictions are usually either +/- 0, 45, 90, 135 or 180. However, my training and test data has only angles of 0, 20, 40, ... 360. This doesn't really change if I change the complexity of the network. I also played around with the optimizer parameters without any success.
&gt; 
&gt; Is there anything wrong with the assumptions:
&gt;
&gt; * x,y encoding for angle
&gt;
&gt; * tanh activation to have values in [-1,1]
&gt; 
&gt; * cosine_proximity as loss function
&gt; 
&gt; Thanks in advance for any advice, tips or pointing me towards a possible mistake i made!

If this is the wrong place for this question I'm sorry and would be happy if someone could point me to the right forum subreddit!",16,3
896,2019-6-14,2019,6,14,17,c0hsph,Python examples of popular machine learning algorithms with interactive Jupyter demos and math being explained,https://www.reddit.com/r/MachineLearning/comments/c0hsph/python_examples_of_popular_machine_learning/,yanina_s,1560502129,[removed],0,0
897,2019-6-14,2019,6,14,18,c0i8dc,Downfall of RNN and LSTM,https://www.reddit.com/r/MachineLearning/comments/c0i8dc/downfall_of_rnn_and_lstm/,dittercane,1560505850,"Why did the RNN technique are not improving anymore? Majority of people have abandoned it, why? is it not as good anymore?",0,1
898,2019-6-14,2019,6,14,19,c0igxd,Cricket World Cup 2019 Winner Prediction Using Machine Learning,https://www.reddit.com/r/MachineLearning/comments/c0igxd/cricket_world_cup_2019_winner_prediction_using/,MainBuilder,1560507729,,0,1
899,2019-6-14,2019,6,14,20,c0j78q,Cpu optimised tensorflow wheel packages,https://www.reddit.com/r/MachineLearning/comments/c0j78q/cpu_optimised_tensorflow_wheel_packages/,deathholes,1560513156,,0,1
900,2019-6-14,2019,6,14,21,c0jiaw,How to do crossed columns in keras?,https://www.reddit.com/r/MachineLearning/comments/c0jiaw/how_to_do_crossed_columns_in_keras/,nothingveryserious,1560515229,[removed],0,1
901,2019-6-14,2019,6,14,21,c0jmas,Fight Overfitting with Dropout,https://www.reddit.com/r/MachineLearning/comments/c0jmas/fight_overfitting_with_dropout/,dima_kop,1560515963,[removed],0,1
902,2019-6-14,2019,6,14,23,c0kfc3,Did you know the evolution of chatbot?,https://www.reddit.com/r/MachineLearning/comments/c0kfc3/did_you_know_the_evolution_of_chatbot/,evincedevin,1560520851,,0,1
903,2019-6-14,2019,6,14,23,c0khi8,Confusion Matrix looks skewed,https://www.reddit.com/r/MachineLearning/comments/c0khi8/confusion_matrix_looks_skewed/,JBall1997,1560521176,[removed],0,1
904,2019-6-14,2019,6,14,23,c0kla4,Autoencoders vs VAE for semi-supervised learning,https://www.reddit.com/r/MachineLearning/comments/c0kla4/autoencoders_vs_vae_for_semisupervised_learning/,question_hour,1560521748,[removed],0,1
905,2019-6-14,2019,6,14,23,c0kmvt,"best Telegram group about deep learning ,machine learning , Artificial Intelligence ,.... .... collaboration of coursera , udacity ,... website and world experts and researchers including Andrej Karpathy , ian goodflow",https://www.reddit.com/r/MachineLearning/comments/c0kmvt/best_telegram_group_about_deep_learning_machine/,Doctor_who1,1560521985,[removed],0,1
906,2019-6-14,2019,6,14,23,c0krnv,[D] Autoencoders vs VAE for semi-supervised learning,https://www.reddit.com/r/MachineLearning/comments/c0krnv/d_autoencoders_vs_vae_for_semisupervised_learning/,question_hour,1560522669,"Hello,

I hope to clear a few doubts regarding Variational Autoencoders.

https://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf

According to the above paper, the representations learned by VAEs are used for semi-supervised learning. However, in my experiments I was getting better accuracy results with regular autoencoders than VAEs. I do know that we can generate new samples using a VAE but is there a reason why VAEs are used in the paper instead of regular autoencoders? What can be the advantages of the representations of VAE compared to that of an autoencoder w.r.t. semi-supervised learning?

Any help is appreciated?",11,9
907,2019-6-14,2019,6,14,23,c0kwz3,[D] Can someone answer some questions based on decision trees?,https://www.reddit.com/r/MachineLearning/comments/c0kwz3/d_can_someone_answer_some_questions_based_on/,mydogissnoring,1560523458,"So I understand that you don't need to scale features for a decision tree, since it will just find the right place to split on anyway. Do outliers need to be handled though?
Does it matter if I have some features that have a way higher variance within it compared to others?
Lastly, for categorical variables, I have my observations labeled in one of 5 labels (this is a feature, not the target). Will it adversely affect the results if the labels are imbalanced? I have one label that makes up roughly 60% of all the observations. Should I try to relabel things so it's a little more balanced (I can collapse some of the other labels)
Btw these questions are for either classification or regression trees.",1,1
908,2019-6-15,2019,6,15,0,c0l7qv,Peeking Inside DNNs With Information Theory,https://www.reddit.com/r/MachineLearning/comments/c0l7qv/peeking_inside_dnns_with_information_theory/,Yuqing7,1560525014,,0,1
909,2019-6-15,2019,6,15,1,c0luuc,[Project] pyTsetlinMachine released. High-level Tsetlin Machine Python API with fast C-extensions.,https://www.reddit.com/r/MachineLearning/comments/c0luuc/project_pytsetlinmachine_released_highlevel/,olegranmo,1560528366,"I have made a Python library for the Tsetlin Machine. You can now set up, train and evaluate Tsetlin Machines in just three lines of code. I have used C extensions for speed, wrapped in Python. Currently, the Multi-class and Convolutional Tsetlin Machines are available. The Regression Tsetlin Machine follows soon. Will also add more demos and support functions (e.g. binarization).  
[https://github.com/cair/pyTsetlinMachine](https://github.com/cair/pyTsetlinMachine?fbclid=IwAR2oCiknG-ZMkiVHck9V_jhYiOCQtTLdJE79EF2eaDj7JQ7OuzzpuhciJUw)

https://i.redd.it/s04lp2nyhc431.png",6,32
910,2019-6-15,2019,6,15,1,c0m1s3,Trying to create a tool that tracks and predicts the path of a blob,https://www.reddit.com/r/MachineLearning/comments/c0m1s3/trying_to_create_a_tool_that_tracks_and_predicts/,planetzephyr,1560529350,[removed],0,1
911,2019-6-15,2019,6,15,2,c0mj7p,Deep Learning vs Neural Nets,https://www.reddit.com/r/MachineLearning/comments/c0mj7p/deep_learning_vs_neural_nets/,ceriolie,1560531819,[removed],0,1
912,2019-6-15,2019,6,15,2,c0mjgz,Applying AutoML to Transformer Architectures,https://www.reddit.com/r/MachineLearning/comments/c0mjgz/applying_automl_to_transformer_architectures/,sjoerdapp,1560531856,,0,1
913,2019-6-15,2019,6,15,2,c0n5a1,[P] [UPDATE] AI Against Humanity: Now with an AI opponent! Running in the browser with TFJS,https://www.reddit.com/r/MachineLearning/comments/c0n5a1/p_update_ai_against_humanity_now_with_an_ai/,cpury,1560534988,"Original post here: [https://www.reddit.com/r/MachineLearning/comments/bvwvoo/p\_ai\_against\_humanity\_play\_cards\_against\_humanity/](https://www.reddit.com/r/MachineLearning/comments/bvwvoo/p_ai_against_humanity_play_cards_against_humanity/)

Thanks for your positive feedback in my last post! :) I kept working on the project, adding new features and cards, and collecting all user choices to train a simple AI you can play against!

Try it out: [https://www.aiagainsthumanity.app/](https://www.aiagainsthumanity.app/)

I tried to be super frugal and everything works 100% out of your browser via a static website. A blog post detailing the process is in the works, but in short: I generated BERT embeddings for each of the cards, then applied PCA to reduce the dimension to something a browser / mobile device can easily handle. Then I trained a small neural network that takes a question-answer pair and outputs a probability. This then gets loaded in the browser via TensorFlow.js.

Please let me know what you think or if you run into any trouble. Ideas are welcome as well! And yes, I know multiplayer would be amazing, but then I'd have to set up a real server and - god forbid - actually pay hosting costs ",48,197
914,2019-6-15,2019,6,15,3,c0ncn9,I think i broke talk-to-transformer,https://www.reddit.com/r/MachineLearning/comments/c0ncn9/i_think_i_broke_talktotransformer/,Spoodermanultra,1560536016,,0,1
915,2019-6-15,2019,6,15,3,c0noc5,[R] Applying AutoML to Transformer Architectures,https://www.reddit.com/r/MachineLearning/comments/c0noc5/r_applying_automl_to_transformer_architectures/,alxndrkalinin,1560537710,,0,1
916,2019-6-15,2019,6,15,4,c0o6ra,Apache Spark Ecosystem Timeline,https://www.reddit.com/r/MachineLearning/comments/c0o6ra/apache_spark_ecosystem_timeline/,faviovaz,1560540386,,0,1
917,2019-6-15,2019,6,15,4,c0ocwg,[P] Clickstream based user intent prediction with LSTMs and CNNs,https://www.reddit.com/r/MachineLearning/comments/c0ocwg/p_clickstream_based_user_intent_prediction_with/,ixeption,1560541277,"Hi folks, 

I recently worked on prediction of event streams to improve the customer experience on web sites. I compared simple LSTMs, GRUs and 1D convolution models and found that for my data convNets perform better, but also with a different characteristics.

You can find the article [here.](http://digital-thinking.de/deep-learning-clickstream-based-user-intent-prediction-with-anns/)

Cheers",15,61
918,2019-6-15,2019,6,15,5,c0omb4,An inte,https://www.reddit.com/r/MachineLearning/comments/c0omb4/an_inte/,karenactionsitafaal,1560542655,,0,1
919,2019-6-15,2019,6,15,5,c0orcp,Adobes prototype AI tool automatically spots Photoshopped faces,https://www.reddit.com/r/MachineLearning/comments/c0orcp/adobes_prototype_ai_tool_automatically_spots/,Jarochomocho,1560543414,,0,1
920,2019-6-15,2019,6,15,7,c0qfiq,[P] VisualSearch app (OSX),https://www.reddit.com/r/MachineLearning/comments/c0qfiq/p_visualsearch_app_osx/,tanreb,1560552903,"Hi, 

Is there any app/software that analyses thousands of images and has an interface to browse the collection via content-based related images?

I'm trying to find a tutorial to achieve something like this, [https://github.com/machinebox/visualsearch](https://github.com/machinebox/visualsearch)  but locally and OpenSource

&amp;#x200B;

Goal: have a local ""google-photos"", that self-organizes my photo archive and I can browse and I get ""related suggestions"" per each image.",13,3
921,2019-6-15,2019,6,15,8,c0qk8q,Machine Learning From Scratch With Python,https://www.reddit.com/r/MachineLearning/comments/c0qk8q/machine_learning_from_scratch_with_python/,codingislife496,1560553676,[removed],0,1
922,2019-6-15,2019,6,15,8,c0qksw,ML training preferences - local or remote hardware for big datasets?,https://www.reddit.com/r/MachineLearning/comments/c0qksw/ml_training_preferences_local_or_remote_hardware/,wkoszek,1560553771,"I wonder if you guys have any preferences for training remotely on AWS / working locally on a PC with a decent graphics card? My gut feeling is that cloud is fine and more convenient, with an ability to just purchase the GPU time when you need it. But I also know that ML parameters are often picked experimentally, and re-training and validation takes time... So are local PCs more popular to cloud for training?

We have a need to do some training on DICOM files which can run on anything from 0.5MB to 500MB, and wondering where to do it in the most cost efficient and convenient way.",0,1
923,2019-6-15,2019,6,15,8,c0qv9p,"CUDA, HMM or Tensorflow?",https://www.reddit.com/r/MachineLearning/comments/c0qv9p/cuda_hmm_or_tensorflow/,LucasBR96,1560555522,[removed],0,1
924,2019-6-15,2019,6,15,8,c0qz19,[D] What are some good papers to read on machine learning approaches to work on data with low Signal to Noise ratio?,https://www.reddit.com/r/MachineLearning/comments/c0qz19/d_what_are_some_good_papers_to_read_on_machine/,rulerofthehell,1560556190,"This question isn't just asking for neural network or RL related approaches, open to everything! Thanks!",15,24
925,2019-6-15,2019,6,15,8,c0r2g6,[D] Update on the GPT2 replication claim,https://www.reddit.com/r/MachineLearning/comments/c0r2g6/d_update_on_the_gpt2_replication_claim/,__arch__,1560556770,"The author has decided not to release the model. Read here: [https://medium.com/@NPCollapse/the-hacker-learns-to-trust-62f3c1490f51](https://medium.com/@NPCollapse/the-hacker-learns-to-trust-62f3c1490f51)

&amp;#x200B;

Excerpt from the post:

&gt;*I have decided to not release my model, and explain why below. I have also written a* *small addendum* *answering some questions about my model and its quality. I would like to thank every single person that engaged with me and helped me come to these conclusions. I have learned so much and could never have done it on my own. Thank you.*",26,19
926,2019-6-15,2019,6,15,8,c0r2j0,[Research] Generalized Information - A Straightforward Method for Judging Machine Learning Models,https://www.reddit.com/r/MachineLearning/comments/c0r2j0/research_generalized_information_a/,johnnyb_61820,1560556783,,1,1
927,2019-6-15,2019,6,15,9,c0reul,"If we could use and generate DeepFakes for a positive purpose, what could it be ?",https://www.reddit.com/r/MachineLearning/comments/c0reul/if_we_could_use_and_generate_deepfakes_for_a/,lotfai,1560558987,[removed],0,1
928,2019-6-15,2019,6,15,9,c0rlzs,How to create an e-commerce application with recommendation system?,https://www.reddit.com/r/MachineLearning/comments/c0rlzs/how_to_create_an_ecommerce_application_with/,depressedcompiler,1560560325,[removed],0,1
929,2019-6-15,2019,6,15,13,c0t9wx,[D] Comparing topics of texts,https://www.reddit.com/r/MachineLearning/comments/c0t9wx/d_comparing_topics_of_texts/,Nowado,1560571836,,0,1
930,2019-6-15,2019,6,15,14,c0ttu7,Object detection with text recognition,https://www.reddit.com/r/MachineLearning/comments/c0ttu7/object_detection_with_text_recognition/,RedKnicks123,1560575972,[removed],0,1
931,2019-6-15,2019,6,15,15,c0uf5p,[D] ML intern with BS in Bio med engineering. Very basic knowledge of ML. How can I increase my chances to get into AI residency programs?,https://www.reddit.com/r/MachineLearning/comments/c0uf5p/d_ml_intern_with_bs_in_bio_med_engineering_very/,GetStuffTogether,1560580518,,6,0
932,2019-6-15,2019,6,15,19,c0w2m1,Industrial Component Cleaning Machine Manufacturer,https://www.reddit.com/r/MachineLearning/comments/c0w2m1/industrial_component_cleaning_machine_manufacturer/,Ultramaxhydrojet,1560595922,[removed],0,1
933,2019-6-15,2019,6,15,20,c0we41,"Our AI R&amp;D team is focusing this year on our AI operations rather than purely algorithms. We want to start taking a more intelligent approach to code/data versioning, containerization, CI/CD, etc. Does anyone know any good literature on transferring these ideas from traditional app dev to AI dev?",https://www.reddit.com/r/MachineLearning/comments/c0we41/our_ai_rd_team_is_focusing_this_year_on_our_ai/,wwtAI-Research,1560598672,,0,1
934,2019-6-15,2019,6,15,20,c0wgvt,Can someone explain to me why Machine Learning is not heavily used to 'play' the stock-market?,https://www.reddit.com/r/MachineLearning/comments/c0wgvt/can_someone_explain_to_me_why_machine_learning_is/,rs10rs10,1560599325,[removed],0,1
935,2019-6-15,2019,6,15,22,c0xfuu,How do I generate AI speeches?,https://www.reddit.com/r/MachineLearning/comments/c0xfuu/how_do_i_generate_ai_speeches/,Darth_Zerstorer,1560606317,[removed],0,1
936,2019-6-15,2019,6,15,22,c0xig6,[D] AI undergrad looking for a young community,https://www.reddit.com/r/MachineLearning/comments/c0xig6/d_ai_undergrad_looking_for_a_young_community/,brain-trainer,1560606790,I currently am at university studying for a bachelors in CS. However the field of AI research and application is so young and with such a limited pool of talent that I cant find peers to collaborate with. Ive spoken to multiple professors who are excited to work with me in the future but Ive struggled to find undergrad students who have the necessary skills to do so. I know I would find undergrads full of AI talent if I went to a school like Stanford or MIT. Maybe Im looking in the wrong places but I sincerely have yet to meet another undergrad student who knows how to use TensorFlow or even Keras. Does anyone else in their bachelors feel this way? Do you know of any online communities that I would be a good fit for?,13,0
937,2019-6-16,2019,6,16,0,c0ydk9,[D] Want to public 12 CVPR papers in two years? Attention and Saliency Is All You Need!,https://www.reddit.com/r/MachineLearning/comments/c0ydk9/d_want_to_public_12_cvpr_papers_in_two_years/,i_love_rl,1560611895,"For five years I have been playing with reinforcement learning and some computer graphic, but recently I decided to give up all my previous researches and begin my journey on attention and saliency. It seems that it is easy to make massive CVPR publications every year by combining modules, for example:

&amp;#x200B;

(please pay attention to the paper title and the first (or join-first) author W. Wang)

&amp;#x200B;

CVPR 2019: 

Salient object detection with pyramid attention and salient edges

W. Wang\*, S. Zhao\*, S. Hoi, J. Shen, and A. Borji. 

&amp;#x200B;

CVPR 2019: 

An iterative and cooperative top-down and bottom-up inference network for salient object detection

W. Wang, J. Shen, M.-M. Cheng, and L. Shao.

&amp;#x200B;

CVPR 2019:

Shifting more attention to video salient object detection

D. Fan, W. Wang, M.-M. Cheng, and J. Shen

&amp;#x200B;

CVPR 2019:

See more, know more: Unsupervised video object segmentation with co-attention siamese networks

X. Lu\*, W. Wang\*, C. Ma, J. Shen, L. Shao and F. Porikli

&amp;#x200B;

CVPR 2019:

Learning unsupervised video object segmentation through visual attention

W. Wang\*, H. Song\*, S. Zhao, J. Shen, S. Zhao, S. Hoi, and H. Ling

&amp;#x200B;

CVPR 2019:

Reasoning visual dialogs with structural and partial observations

Z. Zheng\*, W. Wang\*, S. Qi\*, and S.-C. Zhu

&amp;#x200B;

CVPR 2018:

Inferring shared attention in social scene videos

L. Fan, Y. Chen, P. Wei, W. Wang

&amp;#x200B;

CVPR 2018:

Salient object detection driven by fixation prediction

W. Wang, J. Shen and A. Borji

&amp;#x200B;

CVPR 2018:

Revisiting video saliency: A large-scale benchmark and a new model

W. Wang, J. Shen, F. Guo, M.-M. Cheng, and A. Borji

&amp;#x200B;

CVPR 2018:

Attentive fashion grammar network for fashion landmark detection and clothing category classification

W. Wang\*, Y. Xu\*, J. Shen, and S.-C. Zhu

&amp;#x200B;

...much more hidden

&amp;#x200B;

I think I HAVE to give up all my previous researches and start my works on attention and saliency, otherwise all my job and teaching opportunities will disappear because these people are so talented.",29,27
938,2019-6-16,2019,6,16,0,c0yjml,[P] Meme Classifier Using CNNs in Keras,https://www.reddit.com/r/MachineLearning/comments/c0yjml/p_meme_classifier_using_cnns_in_keras/,brain-trainer,1560612847,,0,1
939,2019-6-16,2019,6,16,1,c0yyuc,[P] Brancher: A user-friendly PyTorch module for deep probabilistic inference,https://www.reddit.com/r/MachineLearning/comments/c0yyuc/p_brancher_a_userfriendly_pytorch_module_for_deep/,LucaAmbrogioni,1560615174,,0,1
940,2019-6-16,2019,6,16,1,c0z6co,[P] Brancher: A user-friendly PyTorch module for deep probabilistic inference,https://www.reddit.com/r/MachineLearning/comments/c0z6co/p_brancher_a_userfriendly_pytorch_module_for_deep/,LucaAmbrogioni,1560616316,[removed],0,1
941,2019-6-16,2019,6,16,1,c0z6v5,ML Newbie Bootcamp Training,https://www.reddit.com/r/MachineLearning/comments/c0z6v5/ml_newbie_bootcamp_training/,bacon666666,1560616399,[removed],0,1
942,2019-6-16,2019,6,16,1,c0z7ty,Major Machine Learning Types Explained in 11 Pages,https://www.reddit.com/r/MachineLearning/comments/c0z7ty/major_machine_learning_types_explained_in_11_pages/,ai-lover,1560616545,[removed],0,1
943,2019-6-16,2019,6,16,2,c0ztv9,AAAI Student Abstract Acceptance Rate,https://www.reddit.com/r/MachineLearning/comments/c0ztv9/aaai_student_abstract_acceptance_rate/,arvind1096,1560619901,[removed],0,1
944,2019-6-16,2019,6,16,2,c0zw42,Data Science Bootcamp: Pay only after you get a data science job.,https://www.reddit.com/r/MachineLearning/comments/c0zw42/data_science_bootcamp_pay_only_after_you_get_a/,HannahHumphreys,1560620247,[removed],0,1
945,2019-6-16,2019,6,16,2,c102a2,OATML/bdl-benchmarks,https://www.reddit.com/r/MachineLearning/comments/c102a2/oatmlbdlbenchmarks/,_quanttrader_,1560621171,,0,1
946,2019-6-16,2019,6,16,3,c10mam,FAKTA: An Automatic End-to-End Fact Checking System,https://www.reddit.com/r/MachineLearning/comments/c10mam/fakta_an_automatic_endtoend_fact_checking_system/,moinnadeem,1560624199,,0,1
947,2019-6-16,2019,6,16,3,c10ncu,Deep Learning From Scratch With Python,https://www.reddit.com/r/MachineLearning/comments/c10ncu/deep_learning_from_scratch_with_python/,codingislife496,1560624362,[removed],0,1
948,2019-6-16,2019,6,16,3,c10qfz,[R] FAKTA: An Automatic End-to-End Fact Checking System,https://www.reddit.com/r/MachineLearning/comments/c10qfz/r_fakta_an_automatic_endtoend_fact_checking_system/,moinnadeem,1560624838,,0,2
949,2019-6-16,2019,6,16,3,c10r1r,How do you tag your data?,https://www.reddit.com/r/MachineLearning/comments/c10r1r/how_do_you_tag_your_data/,abhiksark,1560624927,[removed],0,1
950,2019-6-16,2019,6,16,4,c114we,Good online course for Masters application,https://www.reddit.com/r/MachineLearning/comments/c114we/good_online_course_for_masters_application/,alexz1993,1560627033,[removed],0,1
951,2019-6-16,2019,6,16,4,c11892,Multiple implementations for text summarization to run on google colab,https://www.reddit.com/r/MachineLearning/comments/c11892/multiple_implementations_for_text_summarization/,theamrzaki,1560627553,[removed],0,1
952,2019-6-16,2019,6,16,4,c119nv,short 2-3y PhD program in machine learning?,https://www.reddit.com/r/MachineLearning/comments/c119nv/short_23y_phd_program_in_machine_learning/,throwawaylalallal,1560627768,"hi, I know i ve got wrong motivation for jumping into ML and I will probably struggle cuz of not having 'advanced' math thinking as 'true' PhD guys. But HRs ask and look for advanced degree. I believe it's cuz they dont have clear criteria for how to select good employees. 

PhD/ Masters seem like this filter , aka **serious commitment - if you are in AI seriously, you will easily** be in research.

I m not research oriented. I know it's horrible to go for money reasons, but i admit - i dont wanna be web dev anymore, and data science + big data seems the next big thing for a decade, plus the salaries are higher

Where can i do 2y PhD degree in ml?",0,1
953,2019-6-16,2019,6,16,6,c1272x,[R] Disentangling Disentanglement in Variational Autoencoders,https://www.reddit.com/r/MachineLearning/comments/c1272x/r_disentangling_disentanglement_in_variational/,mellow54,1560633162,,5,21
954,2019-6-16,2019,6,16,6,c12atz,"Y'all ""Machine Learning / AI enthusiasts"" got to chill out.",https://www.reddit.com/r/MachineLearning/comments/c12atz/yall_machine_learning_ai_enthusiasts_got_to_chill/,tempstem5,1560633769,,0,1
955,2019-6-16,2019,6,16,6,c12lkk,[Project] Using ML to create a cat door that automatically locks when a cat has prey in its mouth,https://www.reddit.com/r/MachineLearning/comments/c12lkk/project_using_ml_to_create_a_cat_door_that/,dasblog,1560635566,"Full disclosure, this isn't my project. I volunteer for a Seattle based nonprofit that puts on speaking events.

One of our speakers did a 5 minute talk about a cat door he created using machine learning.

His cat was bringing in dead animals to the house, so he created a cat door that will automatically lock for 15 minutes when the cat has something in its mouth. To do this he connected a camera to the cat door and used machine learning to check the image of the cat and whether the door should be locked or not.

Presentation is here if I've piqued your interest: [https://youtu.be/1A-Nf3QIJjM](https://youtu.be/1A-Nf3QIJjM)",46,397
956,2019-6-16,2019,6,16,6,c12m84,How do you learn about new techniques or approaches to things?,https://www.reddit.com/r/MachineLearning/comments/c12m84/how_do_you_learn_about_new_techniques_or/,Radon-Nikodym,1560635667,[removed],0,1
957,2019-6-16,2019,6,16,7,c138t5,[Project] An open-source chatbot built with a NN,https://www.reddit.com/r/MachineLearning/comments/c138t5/project_an_opensource_chatbot_built_with_a_nn/,ananagame,1560639500,,0,1
958,2019-6-16,2019,6,16,9,c13y2e,What kind of algorithm should I use?,https://www.reddit.com/r/MachineLearning/comments/c13y2e/what_kind_of_algorithm_should_i_use/,bdrayne,1560643878,[removed],0,1
959,2019-6-16,2019,6,16,9,c145ha,[R] Online GauGAN/SPADE demo from NVIDIA https://nvlabs.github.io/SPADE/demo.html,https://www.reddit.com/r/MachineLearning/comments/c145ha/r_online_gauganspade_demo_from_nvidia/,mingyuliutw,1560645204,,0,1
960,2019-6-16,2019,6,16,10,c14gsi,[R] Cognitive Model Priors for Predicting Human Decisions,https://www.reddit.com/r/MachineLearning/comments/c14gsi/r_cognitive_model_priors_for_predicting_human/,joshuacpeterson,1560647253,,1,5
961,2019-6-16,2019,6,16,10,c14i1u,Easy-to-use cloud service for GPU training?,https://www.reddit.com/r/MachineLearning/comments/c14i1u/easytouse_cloud_service_for_gpu_training/,edon581,1560647489,[removed],0,1
962,2019-6-16,2019,6,16,10,c14ofw,Data sets showing biases of different cultures,https://www.reddit.com/r/MachineLearning/comments/c14ofw/data_sets_showing_biases_of_different_cultures/,StellaAthena,1560648691,[removed],0,1
963,2019-6-16,2019,6,16,10,c14v8d,[N] ICML 2019 Livestream Archives (Talk Videos),https://www.reddit.com/r/MachineLearning/comments/c14v8d/n_icml_2019_livestream_archives_talk_videos/,joshuacpeterson,1560650032,"https://slideslive.com/icml
https://facebook.com/pg/icml.imls/videos/",4,19
964,2019-6-16,2019,6,16,10,c14wg8,AudioStellar: Open source data-driven musical instrument for latent sound structure discovery and music experimentation,https://www.reddit.com/r/MachineLearning/comments/c14wg8/audiostellar_open_source_datadriven_musical/,macramole,1560650256,[removed],0,1
965,2019-6-16,2019,6,16,11,c157mz,How do I start to do Machine Learning on training a model to play the game Clash Royale?,https://www.reddit.com/r/MachineLearning/comments/c157mz/how_do_i_start_to_do_machine_learning_on_training/,NomeDU,1560652468,[removed],0,1
966,2019-6-16,2019,6,16,12,c15g7e,inventory management..and perhaps revenue prediction,https://www.reddit.com/r/MachineLearning/comments/c15g7e/inventory_managementand_perhaps_revenue_prediction/,snip3r77,1560654172,[removed],0,1
967,2019-6-16,2019,6,16,13,c16cw0,I tried to reproduce results from a NIPS 2018 (Spotlight) paper but I figured something was wrong,https://www.reddit.com/r/MachineLearning/comments/c16cw0/i_tried_to_reproduce_results_from_a_nips_2018/,real_mehran_kh,1560660886,[removed],0,1
968,2019-6-16,2019,6,16,14,c16g9v,[N] The whisper of schizophrenia: Machine learning finds sound words predict psychosis,https://www.reddit.com/r/MachineLearning/comments/c16g9v/n_the_whisper_of_schizophrenia_machine_learning/,phobrain,1560661623,,1,1
969,2019-6-16,2019,6,16,14,c16ozi,What is stepAIC in R?,https://www.reddit.com/r/MachineLearning/comments/c16ozi/what_is_stepaic_in_r/,TecTunnel,1560663605,,0,1
970,2019-6-16,2019,6,16,14,c16s6u,Is there any paper/progress on sentence similarity detection since BERT is released?,https://www.reddit.com/r/MachineLearning/comments/c16s6u/is_there_any_paperprogress_on_sentence_similarity/,seanbayarea,1560664352,[removed],0,1
971,2019-6-16,2019,6,16,16,c17k78,[P] Olivia your new best friend built with an artifical NN,https://www.reddit.com/r/MachineLearning/comments/c17k78/p_olivia_your_new_best_friend_built_with_an/,ananagame,1560671518,,0,2
972,2019-6-16,2019,6,16,18,c188qc,[R] Differentiable Dynamic Normalization for Learning Deep Representation,https://www.reddit.com/r/MachineLearning/comments/c188qc/r_differentiable_dynamic_normalization_for/,xternalz,1560678187,,0,1
973,2019-6-16,2019,6,16,19,c18dzm,[P] Lab: Organize ML Experiments,https://www.reddit.com/r/MachineLearning/comments/c18dzm/p_lab_organize_ml_experiments/,mlvpj,1560679586,"This is a simple project I started a few months ago to organize machine learning projects. I've recently added PyTorch support to it and updated the documentations.

[https://github.com/vpj/lab](https://github.com/vpj/lab)

It helps you organize experiment results, checkpoints and summaries, and takes care of console outputs.

I'm working on cleaning up the API a little more.",17,21
974,2019-6-16,2019,6,16,19,c18q45,Collection of ML and DL courses,https://www.reddit.com/r/MachineLearning/comments/c18q45/collection_of_ml_and_dl_courses/,Msadat97,1560682721,[removed],0,1
975,2019-6-16,2019,6,16,20,c1921v,Giving a presentation on AI education and democratisation,https://www.reddit.com/r/MachineLearning/comments/c1921v/giving_a_presentation_on_ai_education_and/,rish-16,1560685598,"Hey y'all!

I am a Machine Learning Engineer and Researcher and am invited to give a talk on the Future of AI, democratizing AI Education, how students can start learning about ML, how teachers can start teaching it in schools at a fundamental level, resources to learn ML, etc.

&amp;#x200B;

I'm currently creating my slide deck for the night and am finding it difficult to come up with insightful and engaging content for the audience (mainly comprised of teachers and educators). I was hoping to ask if there is anything I can talk about or include in the slides because brainstorming sure is hard :P

&amp;#x200B;

Any help or suggestions would be wildly appreciated!

&amp;#x200B;

Cheers!",1,1
976,2019-6-16,2019,6,16,21,c19g2l,how much time does it take you to write and produce an ML paper?,https://www.reddit.com/r/MachineLearning/comments/c19g2l/how_much_time_does_it_take_you_to_write_and/,throwawaylalallal,1560688684,[removed],0,1
977,2019-6-16,2019,6,16,22,c1a35s,Midi Corrector,https://www.reddit.com/r/MachineLearning/comments/c1a35s/midi_corrector/,author31,1560693034,[removed],0,1
978,2019-6-16,2019,6,16,23,c1a6kf,Machine Learning and Big Data,https://www.reddit.com/r/MachineLearning/comments/c1a6kf/machine_learning_and_big_data/,kalkaseer,1560693625,,0,1
979,2019-6-16,2019,6,16,23,c1asql,Beginners's guide to NLP using spaCy,https://www.reddit.com/r/MachineLearning/comments/c1asql/beginnerss_guide_to_nlp_using_spacy/,theainerd,1560697121,,0,1
980,2019-6-17,2019,6,17,0,c1b0yn,[P] Brancher: A user-friendly PyTorch module for deep probabilistic inference,https://www.reddit.com/r/MachineLearning/comments/c1b0yn/p_brancher_a_userfriendly_pytorch_module_for_deep/,kseeliger,1560698403,"Finally, after almost two years of development, we are excited to release our toolbox for deep probabilistic inference!   


[https://brancher.org](https://www.google.com/url?q=https://brancher.org&amp;sa=D&amp;source=hangouts&amp;ust=1560784490644000&amp;usg=AFQjCNHu1qQYuEVcZl-RmTzZJ6ju09AbSA)  


Brancher is designed to make the integration between Bayesian statistics and deep learning easy and intuitive. We have prepared tutorials and examples in Google Colab:   [https://brancher.org/#examples](https://www.google.com/url?q=https://brancher.org/%23examples&amp;sa=D&amp;source=hangouts&amp;ust=1560784490644000&amp;usg=AFQjCNHxBZ_DSL0GIEVkxHgLrBuNIt6cKg)  |  [https://brancher.org/#tutorials](https://www.google.com/url?q=https://brancher.org/%23tutorials&amp;sa=D&amp;source=hangouts&amp;ust=1560784490644000&amp;usg=AFQjCNHDEP-DzNiq0VUZqjxgKIj4GoJCkg)  


We are curious about your feedback! Either here or on Twitter:  [@pybrancher](https://twitter.com/pybrancher)",20,137
981,2019-6-17,2019,6,17,0,c1b69b,[N] Top 5 Career Paths To Pick In The World Of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/c1b69b/n_top_5_career_paths_to_pick_in_the_world_of/,vadhavaniyafaijan,1560699232,,0,1
982,2019-6-17,2019,6,17,0,c1b8u6,"best Telegram group about deep learning ,machine learning , Artificial Intelligence ,.including andrew ng , Andrej Karpathy",https://www.reddit.com/r/MachineLearning/comments/c1b8u6/best_telegram_group_about_deep_learning_machine/,Doctor_who1,1560699628,[removed],0,1
983,2019-6-17,2019,6,17,2,c1cp5q,[R][ICML 2019 Best Paper] Disentangling Disentanglement in Variational Autoencoders,https://www.reddit.com/r/MachineLearning/comments/c1cp5q/ricml_2019_best_paper_disentangling/,downtownslim,1560707397,,1,1
984,2019-6-17,2019,6,17,4,c1e5xn,Simons Institute on Youtube channel provides the lecture/seminar/workshop of the latest practice/research,https://www.reddit.com/r/MachineLearning/comments/c1e5xn/simons_institute_on_youtube_channel_provides_the/,Ikuyas,1560715000,"I looked up if any posts have mentioned [**Simons Institute**](https://www.youtube.com/channel/UCW1C2xOfXsIzPgjXyuhkw9g) youtube channel, and only found 3 results. The seminar seems to provide the very valuable knowledge in a concise matter than reading a whole paper. Each presenter is very talented from essentially prestigious institutions and speaks very well. 

But I wonder why I haven't seen the discussion on it here.",0,1
985,2019-6-17,2019,6,17,5,c1eeeg,[D]Simons Institute on Youtube channel provides the lecture/seminar/workshop of the latest practice/research,https://www.reddit.com/r/MachineLearning/comments/c1eeeg/dsimons_institute_on_youtube_channel_provides_the/,Ikuyas,1560716229,"I looked up if any posts have mentioned [**Simons Institute**](https://www.youtube.com/channel/UCW1C2xOfXsIzPgjXyuhkw9g) youtube channel, and only found 3 results. The seminar seems to provide the very valuable knowledge in a concise matter than reading a whole paper. Each presenter is very talented from essentially prestigious institutions and speaks very well.

But I wonder why I haven't seen the discussion on it here.",7,52
986,2019-6-17,2019,6,17,5,c1eqvi,[R] Research roadblock. Help with Extreme Multi-Label Classification,https://www.reddit.com/r/MachineLearning/comments/c1eqvi/r_research_roadblock_help_with_extreme_multilabel/,atif_hassan,1560718059,"Hey guys!

I have been working on the [BioASQ challenge, Task A](http://bioasq.org/) which is the large scale semantic indexing of PubMed abstracts. It is supposed to be my Master's thesis but I have hit a roadblock.

The current state-of-the-art results, that is if we concern ourselves with just micro-f score, is 68.8% while I can't seem to get past the 60% mark. I am currently using pre-trained bio-medical FastText word vectors with a bidirectional GRU the output of which branches out into two parts. The first part computes a document vector using attention mechanism while the second part applies a CNN and then k-max pooling to get yet another document representation. Both vectors are merged along with some additional hand-crafted features which are then finally fed to the output layer which is of size 28,472 (the total number of labels) with sigmoid activation and binary cross entropy loss. Upon training this architecture on 3 million abstracts, I am getting a micro-f score of 58.2%.

I have tried a number of other methods and architectures but none are working. It is extremely frustrating since I have made absolutely no progress for the entirety of this month and I am growing anxious with every passing day as my deliverable deadline keeps coming closer. It would be of immense help if anyone could point me in the right direction on how to proceed further. What to read, what to change, etc. I did read about Label wise attention networks but cannot understand how to implement that in Keras. A small hint or some pseudocode would be of great help.",22,5
987,2019-6-17,2019,6,17,5,c1es1v,VAR tutorial,https://www.reddit.com/r/MachineLearning/comments/c1es1v/var_tutorial/,zen-otter,1560718236,"dear all,

I am actually really interested in autoencoders! I really like the architecture and the concept behind it.

I would like to start taking a look to Variational Autoencoders, do you know what is a book/tutorial/article/paper/blog that describes these architectures and eventually a not too simple tutorial or example to find in order to get a little bit of ideas behind the code?

&amp;#x200B;

thank you very much!",0,1
988,2019-6-17,2019,6,17,6,c1fciz,Is there a discord dedicated to machine learning?,https://www.reddit.com/r/MachineLearning/comments/c1fciz/is_there_a_discord_dedicated_to_machine_learning/,Nick-Conner,1560721278,,0,1
989,2019-6-17,2019,6,17,7,c1fm4t,I need financial support for my research. Please click on ads. I'm student.,https://www.reddit.com/r/MachineLearning/comments/c1fm4t/i_need_financial_support_for_my_research_please/,codingislife496,1560722734,[removed],0,1
990,2019-6-17,2019,6,17,7,c1fr1o,Using high-level ML Libraries vs. writing ML algorithms from Scratch,https://www.reddit.com/r/MachineLearning/comments/c1fr1o/using_highlevel_ml_libraries_vs_writing_ml/,_Zer0_Cool_,1560723500,[removed],0,1
991,2019-6-17,2019,6,17,7,c1fz0p,[Project] Fixed input and variable output neural network,https://www.reddit.com/r/MachineLearning/comments/c1fz0p/project_fixed_input_and_variable_output_neural/,Pilo290,1560724784,"Hey everyone,

&amp;#x200B;

I need some advice on choosing a neural network type which is suitable for the application described below.

&amp;#x200B;

I have a data set with 39600 samples/entries, each sample has an image and a corresponding vector of variable length.

&amp;#x200B;

I want to create a neural network capable of predicting the vector associated with image based solely on the image.

&amp;#x200B;

So, I need a neural network which accepts a fixed length input (the image) and outputs a vector of variable length.

&amp;#x200B;

How can this be achieved?

&amp;#x200B;

Thank you.",3,1
992,2019-6-17,2019,6,17,8,c1gb4t,"[D] Do (or have) you ever work on a project for months, to almost abandon it and finally get decent results in the end?",https://www.reddit.com/r/MachineLearning/comments/c1gb4t/d_do_or_have_you_ever_work_on_a_project_for/,xHipster,1560726777,"As the title says: Do or have you ever worked on a project for months straight, to almost abandon it because e.g. models do not converge, to finally get lucky and get decent results in the end?

&amp;#x200B;

The above just happened to me, where i have been working full-time on something for the past 4 months.  
Without getting decent results and getting completely out of options, almost thinking the whole project would have failed.   
To finally find out that in the end, due to some 'luck', the model turned out quite well with results where i'm happy with. 

&amp;#x200B;

How often does this happen in the field?",10,13
993,2019-6-17,2019,6,17,8,c1ghd7,[D] Can we minimize counting cost function for perceptron algorithm?,https://www.reddit.com/r/MachineLearning/comments/c1ghd7/d_can_we_minimize_counting_cost_function_for/,this_nicholas,1560727838,The [question](https://stats.stackexchange.com/questions/413330/can-we-minimize-counting-cost-function-for-perceptron-algorithm) is posted on Cross Validated (haven't figured out how to write formulas in reddit),2,0
994,2019-6-17,2019,6,17,9,c1h2ht,Machine Learning postgraduate programs,https://www.reddit.com/r/MachineLearning/comments/c1h2ht/machine_learning_postgraduate_programs/,robergas,1560731438,"Hi everybody, hope everyone is doing well. I am a third year university student and I am starting to look for postgraduate programs. Me degree is in Genomic Sciences; meaning I have been taught biology (mostly celular and molecular biology), Bioinformatics, Linear Algebra, Statistics, etc. 

I was also taught some basic principles of Machine Learning and I even worked on a project in NLP using ML. My knowledge mostly extends to knowing the basic principles behind the most basic algorithms, knowing how to use them with Scikit-learn and a bit of Deep Learning theory (I have created some DNNs for test problems).

For my postgraduate studies I would love to work with a group and/or program that analyze biological data using ML/DL. My first question is:

Do you think it is worth studying a Master's in ML/Data-Analysis before attempting a PhD?

Me second question is:
Do you know of any postgraduate programs or research groups that use ML/DL in genomics or evolution? I am specially interested in antibiotic resistance but I am open to everything.

I know these things can be case-specific. But I'd really be looking for a place where I can learn a lot and grow as a researcher. 


Thank you very much guys!",0,1
995,2019-6-17,2019,6,17,9,c1h727,[R] How can I improve my material segmentations?,https://www.reddit.com/r/MachineLearning/comments/c1h727/r_how_can_i_improve_my_material_segmentations/,EmielBoss,1560732242,"I am trying to perform material segmentation (essentially semantic segmentation with respect to materials) on street-view imagery. My datasets only has ground truth for select regions, so not all pixels have a label, and I calculate loss and metrics only within these ground truth regions. I use \[Semantic FPN\]([https://arxiv.org/pdf/1901.02446.pdf](https://arxiv.org/pdf/1901.02446.pdf)) (with the ResNet-50 backbone pre-trained on ImageNet), a learning rate of 0.001, momentum of 0.8, and learning rate is divided by 4 if there is no validations loss improvement after three epochs. My loss function is a per-pixel multiclass cross-entropy loss.  

  

My dataset is extremely limited. Not only are not all pixels classified, I also only have 700 images and a severe class imbalance. I tried tackling this imbalance through loss class weighting (based on the number of ground truth pixels for each respective class, i.e. their area sizes), but it barely helps. I also possess, for every image, a depth map, which I (can) supply as a fourth channel to the input layer.  

  

\[A table of results\]([https://imgur.com/a/v7VFgnk](https://imgur.com/a/v7VFgnk))  

\[Visualizations of images trained only on RGB\]([https://imgur.com/a/oiSrLeM](https://imgur.com/a/oiSrLeM))

\[Visualizations of images trained on RGBD\]([https://imgur.com/a/LDiTQGK](https://imgur.com/a/LDiTQGK))

\[Visualizations of images trained only on RGB, but with class loss weighting\]([https://imgur.com/a/6Jv6mtu](https://imgur.com/a/6Jv6mtu))

\[Visualizations of images trained on RGBD, and with class loss weighting\]([https://imgur.com/a/oc2asRj](https://imgur.com/a/oc2asRj))

&amp;#x200B;

Performance is pretty crappy. What's more, there is very little difference between results of my four experiments. Why is this? I would expect that the addition of depth information (which encodes surface normals and perhaps texture information; pretty discriminitive information). Besides the overall metrics being rather low, the predictions are very messy, and the networks rarely, if ever, predicts ""small"" classes (in terms of area size), e.g. plastic or gravel. This is to be expected with such a small amount of data, but I was wondering if there are any ""performance hacks"" that can boost my network, or if I am missing any obvious stuff? Or is data likely the only bottleneck here? Any suggestions are greatly appreciated!

&amp;#x200B;

PS. I also tried a simple ResNet-50 FCN (I simply upsample ResNet's output until I have the same resolution; there aren't even skip connections), and the results are worse, but at least they are \[smooth\]([https://imgur.com/a/nV9vHGl](https://imgur.com/a/nV9vHGl)). Why are these more smooth?",8,2
996,2019-6-17,2019,6,17,10,c1hsar,Video prediction and understanding: State of the art?,https://www.reddit.com/r/MachineLearning/comments/c1hsar/video_prediction_and_understanding_state_of_the/,reddit_tl,1560735961,[removed],0,1
997,2019-6-17,2019,6,17,12,c1iidb,[R] Detailed ICML 2019 Notes from David Abel,https://www.reddit.com/r/MachineLearning/comments/c1iidb/r_detailed_icml_2019_notes_from_david_abel/,hardmaru,1560740603,"Here's the [link](https://david-abel.github.io/notes/icml_2019.pdf) to the pdf.

I'm amazed at the ability of a small minority of people who can typeset LaTeX for taking notes during a presentation or lecture, and grateful that they made their notes available.",29,231
998,2019-6-17,2019,6,17,12,c1j026,Id like to learn just the mathematics behind ML,https://www.reddit.com/r/MachineLearning/comments/c1j026/id_like_to_learn_just_the_mathematics_behind_ml/,n0lifeismylife,1560743926,"I've got a very strong Math basis in Linear Algebra and undergraduate Statistics and of course the entirety of the calculus series.

I'd like to write basic nueral nets in various languages for fun and practice just so I can understand what's going on behind the scenes, I'd rather not have to resort to Tensor flow or other Python libraries.


Is there a good resource that explains just the mathematics of different Neural Nets so I can write some small stuff myself?",0,1
999,2019-6-17,2019,6,17,15,c1k9ew,Looking for Recommendation,https://www.reddit.com/r/MachineLearning/comments/c1k9ew/looking_for_recommendation/,Lium001,1560753221,[removed],0,1
1000,2019-6-17,2019,6,17,15,c1kdjk,Could someone help me with my depth estimation model ?,https://www.reddit.com/r/MachineLearning/comments/c1kdjk/could_someone_help_me_with_my_depth_estimation/,UpstairsCurrency,1560754153,[removed],0,1
1001,2019-6-17,2019,6,17,15,c1kfsi,Learning how machines learn [discussions],https://www.reddit.com/r/MachineLearning/comments/c1kfsi/learning_how_machines_learn_discussions/,talha1010,1560754643,[removed],0,1
1002,2019-6-17,2019,6,17,16,c1kln4,[P] TensorFlow 2.0 Model Zoo,https://www.reddit.com/r/MachineLearning/comments/c1kln4/p_tensorflow_20_model_zoo/,zsdh123,1560755955,,0,1
1003,2019-6-17,2019,6,17,16,c1km6o,[P] TF 2.0 DRL Resources,https://www.reddit.com/r/MachineLearning/comments/c1km6o/p_tf_20_drl_resources/,zsdh123,1560756083,,0,1
1004,2019-6-17,2019,6,17,16,c1kosq,Classification of image style using deep learning with Python - Custom Web Development Blog,https://www.reddit.com/r/MachineLearning/comments/c1kosq/classification_of_image_style_using_deep_learning/,issart,1560756705,[removed],0,1
1005,2019-6-17,2019,6,17,18,c1ld0s,What is Natural Language Processing &amp; How Does it Benefit a Business? (Part 3 of 3 articles),https://www.reddit.com/r/MachineLearning/comments/c1ld0s/what_is_natural_language_processing_how_does_it/,ElegantMicroWebIndia,1560762587,,0,1
1006,2019-6-17,2019,6,17,19,c1lytj,[R] Model ideas for UAV (unmanned aerial vehicle) landing,https://www.reddit.com/r/MachineLearning/comments/c1lytj/r_model_ideas_for_uav_unmanned_aerial_vehicle/,Boysenberry_Tart,1560767554,"From a lot of browsing, most of the models I've found are related to target detection and identification. I am currently stuck on how to start. I do have some tools for it ([a kit on hand](https://developer.nvidia.com/embedded/jetson-agx-xavier-developer-kit), a raspi and a camera), but I'm really stuck on how to best utilise it. Any ideas and guidance would really be much appreciated!",3,1
1007,2019-6-17,2019,6,17,20,c1mcrw,Component Cleaning Machine &amp; Bin Cleaning Machine manufacturer,https://www.reddit.com/r/MachineLearning/comments/c1mcrw/component_cleaning_machine_bin_cleaning_machine/,Ultramaxhydrojet,1560770552,[removed],1,1
1008,2019-6-17,2019,6,17,20,c1mdna,Need help with plotting the weights and activations of each layer in a TF model,https://www.reddit.com/r/MachineLearning/comments/c1mdna/need_help_with_plotting_the_weights_and/,thirstypoet,1560770731,[removed],0,1
1009,2019-6-17,2019,6,17,20,c1me6w,[N] Hindsight Experience Replay (HER) with SAC/DDPG/DQN support + Evolution Strategy bridge | Stable Baselines v2.6.0,https://www.reddit.com/r/MachineLearning/comments/c1me6w/n_hindsight_experience_replay_her_with_sacddpgdqn/,araffin2,1560770844,"Stable Baselines 2.6.0 was just released. It comes with a bunch of new features and improvements:

&amp;#x200B;

\- a performance tested Hindsight Experience Replay (HER) re-implementation with SAC, DDPG and DQN support included (only custom DDPG was supported in the original OpenAI Baselines)

\- you can now mix Reinforcement Learning (RL) and Evolution Strategies (ES) in few lines of code, thanks to the new get/load parameters method. (see example below with A2C + CMAES)

\- a guide was added in the documentation to deal wth NaNs and Infs: [https://stable-baselines.readthedocs.io/en/master/guide/checking\_nan.html](https://stable-baselines.readthedocs.io/en/master/guide/checking_nan.html)

&amp;#x200B;

&amp;#x200B;

Gist (for an example of mixing ES and RL):  [https://gist.github.com/araffin/404ef9625a4a78d42396c5292e465337](https://gist.github.com/araffin/404ef9625a4a78d42396c5292e465337)

Colab Notebook (for testing HER): [https://colab.research.google.com/drive/1VDD0uLi8wjUXIqAdLKiK15XaEe0z2FOc#scrollTo=qPg7pyvK\_Emi](https://colab.research.google.com/drive/1VDD0uLi8wjUXIqAdLKiK15XaEe0z2FOc#scrollTo=qPg7pyvK_Emi)

Documentation: [https://stable-baselines.readthedocs.io/en/master/modules/her.html](https://stable-baselines.readthedocs.io/en/master/modules/her.html)

Full changelog: [https://github.com/hill-a/stable-baselines/releases](https://github.com/hill-a/stable-baselines/releases)",1,14
1010,2019-6-17,2019,6,17,20,c1mfid,tf.keras performance worse than keras?,https://www.reddit.com/r/MachineLearning/comments/c1mfid/tfkeras_performance_worse_than_keras/,Botekin,1560771095,[removed],0,1
1011,2019-6-17,2019,6,17,20,c1mhgu,BG for Data Science,https://www.reddit.com/r/MachineLearning/comments/c1mhgu/bg_for_data_science/,thunder6776,1560771486,[removed],0,1
1012,2019-6-17,2019,6,17,21,c1ms4g,research topic in geometric deep learning?,https://www.reddit.com/r/MachineLearning/comments/c1ms4g/research_topic_in_geometric_deep_learning/,Kaits929,1560773539,[removed],0,1
1013,2019-6-17,2019,6,17,21,c1msdg,BG for Data Science,https://www.reddit.com/r/MachineLearning/comments/c1msdg/bg_for_data_science/,thunder6776,1560773580,So I'm really interested in Data Science and Analytics  and im considering applying to get a post grad degree in Data Engineering and Analytics from The Technical University of Munich. However im not sure about my background since i am pursuing Mechanical Engineering as my under grad. Im Decent at coding in java and in the process of learning python and my Mathematics is pretty strong ( topics like linear algebra and probablity). Is it an option i should consider or am i punching way above my weight here?,0,1
1014,2019-6-17,2019,6,17,21,c1n1d8,Temporal Action Localization SOTA?,https://www.reddit.com/r/MachineLearning/comments/c1n1d8/temporal_action_localization_sota/,cinjon,1560775261,[removed],0,1
1015,2019-6-17,2019,6,17,22,c1nnm2,Pharmacy,https://www.reddit.com/r/MachineLearning/comments/c1nnm2/pharmacy/,Dradzo,1560778886,,0,1
1016,2019-6-17,2019,6,17,23,c1nunl,Idea for research topic in geometric deep learning,https://www.reddit.com/r/MachineLearning/comments/c1nunl/idea_for_research_topic_in_geometric_deep_learning/,Kaits929,1560780012,"Hi, I'm in my 4th year in my undergraduate studies and looking for a  research topic about geometric deep learning especially related to the  graph theory such as graph signal processing or graph neural networks.  But I'm still deciding what should I do first. I mean what can I do as  undergraduate student. what is the right topics for me? And I want to  know what kind of topics there are. So please tell me related works and  recommend me something! Thank you in advance.",0,1
1017,2019-6-17,2019,6,17,23,c1nvi0,[D] NVIDIA has released a demo for GauGAN,https://www.reddit.com/r/MachineLearning/comments/c1nvi0/d_nvidia_has_released_a_demo_for_gaugan/,Nick_Pyth,1560780132,,0,1
1018,2019-6-17,2019,6,17,23,c1nzxw,[R] Meta-Learning surrogate models for sequential decision making (DeepMind),https://www.reddit.com/r/MachineLearning/comments/c1nzxw/r_metalearning_surrogate_models_for_sequential/,Forsaken_Scientist,1560780817,,0,1
1019,2019-6-17,2019,6,17,23,c1o2lo,Data Cleaning Beginner to Advanced,https://www.reddit.com/r/MachineLearning/comments/c1o2lo/data_cleaning_beginner_to_advanced/,hisham_elamir,1560781212,[removed],0,1
1020,2019-6-17,2019,6,17,23,c1o5fa,Jupyter Notebook memory leak?,https://www.reddit.com/r/MachineLearning/comments/c1o5fa/jupyter_notebook_memory_leak/,hansn,1560781626,[removed],0,1
1021,2019-6-18,2019,6,18,0,c1oknx,Free cloud GPU credits for deep learning,https://www.reddit.com/r/MachineLearning/comments/c1oknx/free_cloud_gpu_credits_for_deep_learning/,whitezl0,1560783771,"Hi, I am offering free credits for 1080Ti GPU instances for deep learning purposes  more than 12hrs for free

I am working on https://www.tensorpad.com/  developing cloud infrastructure for machine learning.

Part of our computational capacity is idle; hence, were offering credits at a free and discounted rate, so that data scientists can benefit from the resources available, and work on neural networks.

Specs:  60GB of RAM, 4 CPUs, 1080Ti GPU  JupyterLab environment with access to the terminal  Pre-installed Tensorflow, Keras, and other ML frameworks

You can access the free credits by signing up (https://dashboard.tensorpad.com/ and redeeming ""promo450"" promo code in the Billing tab (https://dashboard.tensorpad.com/billing).

For any questions, please contact us here, through support@tensorpad.com, or the Intercom on the site.",0,1
1022,2019-6-18,2019,6,18,0,c1omb1,CLUSTERING INSIDE CLUSTERING,https://www.reddit.com/r/MachineLearning/comments/c1omb1/clustering_inside_clustering/,ares_no_tebnin,1560784004,[removed],0,1
1023,2019-6-18,2019,6,18,0,c1p2qo,Question:,https://www.reddit.com/r/MachineLearning/comments/c1p2qo/question/,Yetric,1560786284,[removed],0,1
1024,2019-6-18,2019,6,18,1,c1pavo,I became ill during my degree. I froze it for 8 months. I want to study courses independently at home and I have difficulties. Would appreciate help.,https://www.reddit.com/r/MachineLearning/comments/c1pavo/i_became_ill_during_my_degree_i_froze_it_for_8/,yeshetbeshet,1560787381,"I'm a third year  CS student at the open university of my country. I finished most of my courses.

Programming languages I had experience with are C and Java.

I now have time to learn new things on my own.

It's very hard for me to know where to begin.

All I can do to help you help me is to say what areas I'm really interested in:

Biological Computing, AI, Machine Learning, VR, Autonomous Systems.

I realize that some of the areas I wrote contained in some other area I wrote.

These subjects are very interesting to me. I don't know what path to take and also where the path should start.

I chose to put this post here because I think that Machine Learning is something common to all the areas I wrote above.",0,1
1025,2019-6-18,2019,6,18,1,c1pbey,[D] Rosalind Picard: Affective Computing | Artificial Intelligence Podcast,https://www.reddit.com/r/MachineLearning/comments/c1pbey/d_rosalind_picard_affective_computing_artificial/,UltraMarathonMan,1560787450,"Rosalind Picard is a professor at MIT, director of the Affective Computing Research Group at the MIT Media Lab, and co-founder of two companies, Affectiva and Empatica. Over two decades ago she launched the field of affective computing with her book of the same name. This book described the importance of emotion in artificial and natural intelligence, the vital role emotion communication has to relationships between people in general and in human-robot interaction. I really enjoyed talking with Roz over so many topics including emotion, ethics, privacy, wearable computing, her recent work in epilepsy, and even love and meaning.

**Video:** [https://www.youtube.com/watch?v=kq0VO1FqE6I](https://www.youtube.com/watch?v=kq0VO1FqE6I)

https://i.redd.it/rkc34eetwx431.png

**Outline:**

0:00 - Introduction

1:00 - Affective computing

2:45 - Clippy

5:03 - Diversity in computer science

5:55 - Emotion in AI

8:40 - Privacy

18:10 - Forming a connection with AI systems

30:31 - Emotion

39:05 - Measuring signals from the brain and the body

50:20 - Future AI systems

53:50 - Faith and science

56:35 - Meaning of life",0,4
1026,2019-6-18,2019,6,18,1,c1pdki,CVPR 2019 | Waymo Introduces Open Dataset to Accelerate Autonomous Driving Research,https://www.reddit.com/r/MachineLearning/comments/c1pdki/cvpr_2019_waymo_introduces_open_dataset_to/,Yuqing7,1560787726,,0,1
1027,2019-6-18,2019,6,18,1,c1pka5,Uber to Launch Self Driving Volvo | IT News,https://www.reddit.com/r/MachineLearning/comments/c1pka5/uber_to_launch_self_driving_volvo_it_news/,S_paddy,1560788582,,0,1
1028,2019-6-18,2019,6,18,1,c1ps18,Coefficient of Determination in ML,https://www.reddit.com/r/MachineLearning/comments/c1ps18/coefficient_of_determination_in_ml/,alohaproblem,1560789583,[removed],0,1
1029,2019-6-18,2019,6,18,1,c1pxkb,problem in implementation: ModuleNotFoundError: No module named 'options',https://www.reddit.com/r/MachineLearning/comments/c1pxkb/problem_in_implementation_modulenotfounderror_no/,DIVEINTO123,1560790312,[removed],0,1
1030,2019-6-18,2019,6,18,2,c1q3wj,Google at CVPR 2019,https://www.reddit.com/r/MachineLearning/comments/c1q3wj/google_at_cvpr_2019/,sjoerdapp,1560791119,,0,1
1031,2019-6-18,2019,6,18,2,c1q5wf,[Discussion] Methods to use alternative form of reconstruction objective for VAE than pixelwise error,https://www.reddit.com/r/MachineLearning/comments/c1q5wf/discussion_methods_to_use_alternative_form_of/,akanimax,1560791374,"I am currently working on a project which involves improving the reconstruction capability of the VAE perceptually. Since the basic VAE objective uses the pixelwise error for the reconstruction part, the generated images have a peculiar blurry characteristic which makes them perceptually unreal. I did keyword searches on Scholar and ResearchGate, but was not able to find works that replace this pixelwise metric with something more appropriate for images.

&amp;#x200B;

The closest I got was with the paper titled ""Autoencoding beyond pixels using a learned similarity metric"" [https://arxiv.org/pdf/1512.09300.pdf](https://arxiv.org/pdf/1512.09300.pdf). This is a great piece of work and I find the idea of combining the GAN discriminator with the VAE superb.

&amp;#x200B;

In my search, I also found the flow based papers such as GLOW and RealNVP. But these use the reversible operations because of which, the posterior probability can be easily calculated since it is a deterministic function of the prior probability. I am actually looking for the variational inference generative models which simply use a different form of reconstruction objective for better perceptual results.

&amp;#x200B;

I kindly request all the fellow redditors to please provide me with works that you are aware of. It would be a great help. Thanking you.

&amp;#x200B;

Best regards,

akanimax",6,3
1032,2019-6-18,2019,6,18,2,c1q83q,[D] Machine learning papers that look at the 2nd gradient /gradient of the gradient ?,https://www.reddit.com/r/MachineLearning/comments/c1q83q/d_machine_learning_papers_that_look_at_the_2nd/,BatmantoshReturns,1560791654,"I feel that looking at the 2nd gradient (gradient of the gradient (2nd derivative) ) may be interesting to look at but I can't seem to query this. I haven't found any papers on this. 

But I feel that somebody must have looked into this. Has research on this never been performed? Or is there a specific phrase to query ?",7,0
1033,2019-6-18,2019,6,18,2,c1q9kn,Podcast only presenting papers?,https://www.reddit.com/r/MachineLearning/comments/c1q9kn/podcast_only_presenting_papers/,EtienneT,1560791849,[removed],0,1
1034,2019-6-18,2019,6,18,2,c1qhb5,[R] Meta-Learning surrogate models for sequential decision making (DeepMind),https://www.reddit.com/r/MachineLearning/comments/c1qhb5/r_metalearning_surrogate_models_for_sequential/,petrified_piranha,1560792849,,1,27
1035,2019-6-18,2019,6,18,2,c1qko0,[D] Text character visual similarity algo?,https://www.reddit.com/r/MachineLearning/comments/c1qko0/d_text_character_visual_similarity_algo/,bigDATAbig,1560793290,"Hey, for an application I'm making I want to find the similarity between how two letters look. 

For example, o and O have a high similarity but o and K do not. 

Can someone guide me on what sort of techniques I would use (not necessarily ML, but this sounds like a DL task) in order to find a similarity between the look of two characters? It could be any character in any language that's why I can't just do it manually. 

My proposed algorithm is as follows:
1. Accept 2 letters as argument
2. Generate same size image with same sized characters placed in them
3. Compute the similarity between the two images somehow 
4. Get result 

How would I do step 3? Any guidelines are appreciated. I'm currently looking into HOG classifiers, any other information is appreciated.",5,1
1036,2019-6-18,2019,6,18,2,c1ql2a,[D] Time series forecasting using gradient boosting,https://www.reddit.com/r/MachineLearning/comments/c1ql2a/d_time_series_forecasting_using_gradient_boosting/,tiredofquoraguy,1560793344,"Hi all!
I am trying to figure out how to forecast time series data using gradient boosting? 
Have you tried this? How is the performance compared to other techniques like Arimax, RNN LSTM etc.?
Also, would really appreciate if you know of sources with examples in R. 
Thanks a lot in advance!!",8,0
1037,2019-6-18,2019,6,18,2,c1qli3,Habana Labs launches its Gaudi AI training processor  TechCrunch,https://www.reddit.com/r/MachineLearning/comments/c1qli3/habana_labs_launches_its_gaudi_ai_training/,alvisanovari,1560793397,,0,1
1038,2019-6-18,2019,6,18,3,c1qtlw,Angle prediction problem [P],https://www.reddit.com/r/MachineLearning/comments/c1qtlw/angle_prediction_problem_p/,Shinigaami7,1560794500,"Hello people, I would like to develop an algorithm to predict angles for my project. Now what I'm doing is collecting values from two gyro sensors which in the form of angles. One of these angles is the input and the other is to be predicted, the second sensor is used to check whether the predicted angle is right or wrong.
Now what we have tried - - we checked the correlation here and it came out to be 91% which was expected and we checked that these angle values can be derived from a lookup table but these mappings change when derivative or rate of change of input changes. And this is my problem I'm not able to come up with an apt algo to solve this problem also the solution needs to less memory hungry. Which again a problem. We thought of fuzzy logic but again it is difficult to form proper membership functions.
Please please people of Reddit help me!",12,0
1039,2019-6-18,2019,6,18,3,c1qw9h,[P] Implementation of VoVNet(CVPRW'19),https://www.reddit.com/r/MachineLearning/comments/c1qw9h/p_implementation_of_vovnetcvprw19/,stigma0617,1560794828,"Hi,  


I implemented [VoVNet](https://arxiv.org/abs/1904.09730) which is **efficient** backbone network presented in CVPR workshop on [CEFRL](http://www.ee.oulu.fi/~lili/CEFRLatCVPR2019.html).   


My implementations provide ImageNet classification and object detections in Detectron.

&amp;#x200B;

Highlight 

* 2x faster than DenseNet on ImageNet classification
* more accurate than ResNet, especially on the small object detection

&amp;#x200B;

ImageNet classification : [https://github.com/stigma0617/VoVNet.pytorch](https://github.com/stigma0617/VoVNet.pytorch)

Detectron : [https://github.com/stigma0617/maskrcnn-benchmark-vovnet/tree/vovnet](https://github.com/stigma0617/maskrcnn-benchmark-vovnet/tree/vovnet)",0,5
1040,2019-6-18,2019,6,18,3,c1r67j,PCA vs LSA vs TruncatedSVD for NLP,https://www.reddit.com/r/MachineLearning/comments/c1r67j/pca_vs_lsa_vs_truncatedsvd_for_nlp/,usernanimous,1560796173,[removed],0,1
1041,2019-6-18,2019,6,18,3,c1r9r4,Kaggle course,https://www.reddit.com/r/MachineLearning/comments/c1r9r4/kaggle_course/,bizzieboi,1560796645,[removed],0,1
1042,2019-6-18,2019,6,18,3,c1re3p,[P] PyTorch Basics: Understanding Autograd and Computation Graphs,https://www.reddit.com/r/MachineLearning/comments/c1re3p/p_pytorch_basics_understanding_autograd_and/,coffeepants87,1560797240,,0,1
1043,2019-6-18,2019,6,18,3,c1rehc,"Yoshua Bengio on the Turing Award, AI Trends, and Very Unfortunate US-China Tensions",https://www.reddit.com/r/MachineLearning/comments/c1rehc/yoshua_bengio_on_the_turing_award_ai_trends_and/,Yuqing7,1560797293,,0,1
1044,2019-6-18,2019,6,18,4,c1rle7,[Discussion] How do you maintain motivation and perseverance when you realize your idea has already been published.,https://www.reddit.com/r/MachineLearning/comments/c1rle7/discussion_how_do_you_maintain_motivation_and/,vikigenius,1560798206,"I am a Masters student and have been struggling with motivation for some time.

Just a few days ago, I had what I thought was a really interesting idea. I performed a few small experiments and confirmed my hypothesis. Then when I started to look for existing work/theoretical foundations I found the exact same idea has been worked on and been put up on arxiv just a week ago.

This has happened to me multiple times and is a bit demoralizing and I just give up on the idea altogether thinking, what's the point, even if i improve upon this idea, it's not unique or novel in any way.

Has this happened to anyone else, do you work on the idea further? Or do you just give up and look for different ideas ?",51,182
1045,2019-6-18,2019,6,18,4,c1rxhy,Machine learning in the energy sector,https://www.reddit.com/r/MachineLearning/comments/c1rxhy/machine_learning_in_the_energy_sector/,hackysack72,1560799796,[removed],0,1
1046,2019-6-18,2019,6,18,4,c1s2fj,a distributed Hyperband implementation on Steroids,https://www.reddit.com/r/MachineLearning/comments/c1s2fj/a_distributed_hyperband_implementation_on_steroids/,_quanttrader_,1560800473,,0,1
1047,2019-6-18,2019,6,18,4,c1s73e,"Tackling Climate Change with Machine Learning (Y. Bengio, A. Ng, D. Hassabis, et al.)",https://www.reddit.com/r/MachineLearning/comments/c1s73e/tackling_climate_change_with_machine_learning_y/,SimpleThoughts-,1560801112,[removed],0,1
1048,2019-6-18,2019,6,18,5,c1sbn4,"[News] Tackling Climate Change with Machine Learning (Y. Bengio, A. Ng, D. Hassabis, et al.)",https://www.reddit.com/r/MachineLearning/comments/c1sbn4/news_tackling_climate_change_with_machine/,SimpleThoughts-,1560801711,"97-page arXiv paper from some of the biggest names and institutions in AI, posted a few days ago :

[https://arxiv.org/abs/1906.05433](https://arxiv.org/abs/1906.05433)",1,2
1049,2019-6-18,2019,6,18,5,c1sqrb,[N] Training a single AI model can emit as much carbon as five cars in their lifetimes..,https://www.reddit.com/r/MachineLearning/comments/c1sqrb/n_training_a_single_ai_model_can_emit_as_much/,white_noise212,1560803744,,0,1
1050,2019-6-18,2019,6,18,5,c1sykv,Best libraries to train object recognition?,https://www.reddit.com/r/MachineLearning/comments/c1sykv/best_libraries_to_train_object_recognition/,bitcoin-wiz,1560804803,[removed],0,1
1051,2019-6-18,2019,6,18,6,c1t1od,"5 minute summary of chapter 1 of ""Reinforcement Learning: An Introduction""",https://www.reddit.com/r/MachineLearning/comments/c1t1od/5_minute_summary_of_chapter_1_of_reinforcement/,jdyr1729,1560805220,[removed],0,2
1052,2019-6-18,2019,6,18,6,c1tb5m,[P] Using AI to generate recipes from food images,https://www.reddit.com/r/MachineLearning/comments/c1tb5m/p_using_ai_to_generate_recipes_from_food_images/,downtownslim,1560806507,"A new approach to generating recipes directly from food images that produces more compelling recipes than retrieval-based approaches, according to human judgment. Evaluated on the large-scale Recipe1M data set, this approach improves performance with respect to previous baselines for ingredient prediction. With this work, we aim to provide access to the preparation of a meal simply by inputting a food image.

&amp;#x200B;

[https://ai.facebook.com/blog/inverse-cooking/](https://ai.facebook.com/blog/inverse-cooking/)",7,56
1053,2019-6-18,2019,6,18,6,c1tcj6,[Project] Port of the tensorflow facenet pretrained models to pytorch,https://www.reddit.com/r/MachineLearning/comments/c1tcj6/project_port_of_the_tensorflow_facenet_pretrained/,timesler,1560806692," [https://github.com/timesler/facenet-pytorch](https://github.com/timesler/facenet-pytorch)

&amp;#x200B;

Hi all, this project contains pytorch pretrained inception resnets ported from the davidsandberg/facenet github repo. Models are implemented and used according to the standard pytorch/torchvision methodology (inheritable model modules, torchvision style model zoo for downloaded/cached pretrained state dictionaries etc.). Currently, the project covers face detection using MTCNN and face recognition. MTCNN is implemented as a single stand-alone pytorch module that wraps the p-, r-, and o-net modules as well as the post-processing, making it easy to chain MTCNN and recognition resnets together in a face recognition pipeline.

&amp;#x200B;

The motivation for the project was the lack of a clean implementation in pytorch that provides the performance of the davidsandberg/facenet github repo. My aim was to build a project that could be easily used to add value existing pytorch projects without a great deal of effort.

&amp;#x200B;

Performance wise, I see similar or better inference speed on my local machine when compared to the original repo, but that one data point doesn't say a hell of a lot. Any extra testing or feedback much appreciated.",1,8
1054,2019-6-18,2019,6,18,6,c1tdcl,Very first use of tensorflow and i'm confused,https://www.reddit.com/r/MachineLearning/comments/c1tdcl/very_first_use_of_tensorflow_and_im_confused/,NanoRocket,1560806795,"Hi, 

I'm trying tensorflow for the first time, and i'm even not really used to python being more a R guy, anyway i'm a bit lost. 

&amp;#x200B;

I have been able to install Python and Anaconda this evening, import this github library: 

&amp;#x200B;

[https://github.com/NVlabs/stylegan](https://github.com/NVlabs/stylegan)

&amp;#x200B;

And ran the [pretrained\_example.py](https://github.com/NVlabs/stylegan/blob/master/pretrained_example.py) code. 

&amp;#x200B;

However i got several error, all linked to the fact i don't have a GPU, i'm using a macbook air, so i'm afraid i won't be able to run this code on my machine ? I'm currently installing Linux as apparently there is Tensorflow GPU available only for Linux and Windows, but i assume that using the Tensorflow GPU library on a computer with only a CPU won't make the code working right ? 

&amp;#x200B;

Thanks for you help",0,1
1055,2019-6-18,2019,6,18,7,c1u8o4,Overtaking maneuver with robot,https://www.reddit.com/r/MachineLearning/comments/c1u8o4/overtaking_maneuver_with_robot/,huryou,1560811044,[removed],0,1
1056,2019-6-18,2019,6,18,8,c1ulok,What's the difference between a score and a probability in a classification model?,https://www.reddit.com/r/MachineLearning/comments/c1ulok/whats_the_difference_between_a_score_and_a/,vfsilva,1560813022,I always wanted to know the difference between these two terms.,0,1
1057,2019-6-18,2019,6,18,8,c1v48i,[D] PowerPlay + Meta-learning a potential path to AGI?,https://www.reddit.com/r/MachineLearning/comments/c1v48i/d_powerplay_metalearning_a_potential_path_to_agi/,cryptonewsguy,1560815937,"So this is obviously hypothetical and entirely speculative on my part as an ML hobbyist so I'm sure I'm missing something and will get downvoted for my dumb hypothetical.


But here it goes.

# Meta-learn

Lets say you have a Neural Network (the parent network) which can design arbitrary children networks and learn optimized design patterns for a given task. Of course your parent network won't be super generalized for any type of network design, just relatively specific tasks. This is kind of why we don't have ""real"" AI or AGI. The tasks are still relatively narrow. 


Skimming the literature on meta-learning it looks like researchers have been able to get SOME generalization by training their meta-networks on multiple tasks. But of course data and identifying tasks might be a limitation for scale-ability and high levels of generalization. So I purpose a potentially more elegant way.


# PowerPlay


This is where [Jrgen Schmidhubers PowerPlay would come in.](https://arxiv.org/abs/1112.5309) The PowerPlay algorithm is split into a solver and a problem generator. The problem generator generates novel problems which the solver has to try to solve. Novel problems are problems which are unsolvable by the current solver. The created problems are just a bit more complicated than the most complicated solvable problem.



# Meta-PowerPlay

Both the problem Solver and Generator have parent Networks which continually learn to design more sophisticated Solvers and Generators until you have much more general problem solvers, or rather a neural network that can design general problem solvers.",1,3
1058,2019-6-18,2019,6,18,9,c1v6nh,"17 interviews (4 phone screens, 13 onsite, 5 different companies), all but two of them asked this one basic classification question, and I don't know the answer...",https://www.reddit.com/r/MachineLearning/comments/c1v6nh/17_interviews_4_phone_screens_13_onsite_5/,SpockTriesToReturn,1560816356,"I've been trying to get into a more ML/science based role (currently I'm more on the tech business side). Within my own specific domain, I know all of the major algorithms and have been able to shine in that particular topic (times series and regression models). When it comes to generic data science, I have been able to handle myself quite well on most fronts (probability questions, conceptual questions, what is the central mean theorem? Can you explain MLE? etc...) .  

&amp;#x200B;

One question kept coming up though, with every one of the 5 companies, including two of the biggest names in tech, asking this exact question: 

**Suppose you have a binary classifier (logistic regression, neural net, etc...) how do you handle imbalanced data sets in production?** 

&amp;#x200B;

I don't know :-( . I know that you need to be careful with which metric you use to evaluate your model, that you should look at precision and recall or the ROC, instead of just accuracy. And that your sampling strategies should change to better reflect each class. But all of this is during training. 

&amp;#x200B;

Once in production, I know that you face a catch-22 situation: 

* If you *don't skew* your training data, then you don't have enough data from the sparse class for the classifier to learn something, and it will just learn to always predict the dense class. 
* If you *do skew* your data, then now you're facing a situation where the distribution of the training data and the distribution of the production data are completely different, so your model won't predict well (at least my understanding is that different distributions in test and in prod is always a recipe for disaster). 

&amp;#x200B;

Is my assessment of the dilemma correct?  And how do you solve it? 

&amp;#x200B;

Why is this question so popular (FWIW - none of these companies were doing medical or security applications....)",0,1
1059,2019-6-18,2019,6,18,9,c1v84s,TensorFlow 2.0 Tutorial series from Lambda [P],https://www.reddit.com/r/MachineLearning/comments/c1v84s/tensorflow_20_tutorial_series_from_lambda_p/,sabalaba,1560816601,"Hey everyone, Chuan Li, our Chief Science Officer at Lambda put together a series of five TensorFlow 2 tutorials:

Basic Image Classification:
https://lambdalabs.com/blog/tensorflow-2-0-tutorial-01-image-classification-basics/

Transfer Learning:
https://lambdalabs.com/blog/tensorflow-2-0-tutorial-02-transfer-learning/

Checkpoints:
https://lambdalabs.com/blog/tensorflow-2-0-tutorial-03-saving-checkpoints/

Early Stopping:
https://lambdalabs.com/blog/tensorflow-2-0-tutorial-04-early-stopping/

Distributed Multi-node Training:
https://lambdalabs.com/blog/tensorflow-2-0-tutorial-05-distributed-training-multi-node/

Let us know what you think. Hope that you find these to be helpful!",2,16
1060,2019-6-18,2019,6,18,9,c1vjgh,[P] How to run evolution strategies on Google Kubernetes Engine,https://www.reddit.com/r/MachineLearning/comments/c1vjgh/p_how_to_run_evolution_strategies_on_google/,sensetime,1560818409,"Ran into this [blog post](https://cloud.google.com/blog/products/ai-machine-learning/how-to-run-evolution-strategies-on-google-kubernetes-engine) on Google Cloud blog. A bit surprising they are pushing evolution rather than TPUs / deep learning.

Reinforcement learning (RL) has become popular in the machine learning community as more and more people have seen its amazing performance in games, chess and robotics. In previous blog posts weve shown you how to run RL algorithms on AI Platform utilizing both Googles powerful computing infrastructure and intelligently managed training service such as Bayesian hyperparameter optimization. In this blog, we introduce Evolution Strategies (ES) and show how to run ES algorithms on Google Kubernetes Engine (GKE) ...

https://cloud.google.com/blog/products/ai-machine-learning/how-to-run-evolution-strategies-on-google-kubernetes-engine

https://github.com/lerrytang/es_on_gke",0,7
1061,2019-6-18,2019,6,18,10,c1vxoc,"[D] 17 interviews (4 phone screens, 13 onsite, 5 different companies), all but two of the interviewes asked this one basic classification question, and I still don't know the answer...",https://www.reddit.com/r/MachineLearning/comments/c1vxoc/d_17_interviews_4_phone_screens_13_onsite_5/,SpockTriesToReturn,1560820798,"I've been trying to get back into a more ML/science based role (currently I'm more on the tech business side). Within my own specific domain, I know all of the major algorithms and have been able to shine in that particular topic (times series and regression models). When it comes to generic data science, I have been able to handle myself quite well on most fronts (probability questions, conceptual questions, what is the central mean theorem? can you explain MLE? etc...) .

One topic kept coming up though, with 15 out of the 17 interviewers, across all 5 companies (including two of the biggest names in tech) asking this exact question:

**Suppose you have a binary classifier (logistic regression, neural net, etc...) how do you handle imbalanced data sets in production?**

I don't know :-( . I know that you need to be careful with which metric you use to evaluate your model, that you should look at precision and recall or the ROC, instead of just accuracy. And that your sampling strategies should change to better reflect each class. But all of this is during training.

Once in production, I know that you face a catch-22 situation:

* If you *don't skew* your training data, then you don't have enough data from the sparse class for the classifier to learn something, and it will just learn to always predict the dense class.
* If you *do skew* your data, then now you're facing a situation where the distribution of the training data and the distribution of the production data are completely different, so your model won't predict well (at least my understanding is that different distributions in test and in prod is always a recipe for disaster).

Is my assessment of the dilemma correct? And how do you solve it?

Why is this question so popular (FWIW - none of these companies were doing medical or security applications....)

&amp;#x200B;

Some follow up questions and/or hints that were given (but I still couldn't really answer the question in a satisfactory way): 

* If this is the case, but only you noticed that your binary classifier is not performing well only after you have already deployed it in production and had been scoring it for a few weeks, what do you do? (My answer, go back to training, and either re-evaluate which features you want to use, or find more data to train on) , second follow from the same person: What if I told you that you are stuck with the same model and couldn't get any more data, what do you do then (I answered: l1 or l2 regularization? but these are applicable to any data set, they aren't specific to imbalanced data. Fiddle with the K in your K-fold CV? that wouldn't work either -- by this point I felt like I was being Kobayashi Marued...) 
* Can you adjust your classifier after training, but before deploying it, so that it is adjusted to the original distribution, not the skewed (downsampled or upsampled) distribution you used during training? (Drew a blank - as far as I know, any adjustment to the model based on knowledge prior to deployment constitutes training in one form or the other....) 

With regards to the second question, I did come across \[this thread and the blog that it linked to\]([https://stats.stackexchange.com/a/403244/89649](https://stats.stackexchange.com/a/403244/89649)) . It applies only to logistic regression, not any other binary classifier as far as I can tell . What about other classifiers? (Or is it that logistic regression is the only applicable algorithm in the imbalanced case?)",120,356
1062,2019-6-18,2019,6,18,10,c1vz57,[P] Designers built an AI penis detector to protest Googles prudish doodles,https://www.reddit.com/r/MachineLearning/comments/c1vz57/p_designers_built_an_ai_penis_detector_to_protest/,milaworld,1560821047,"[Designers built an AI penis detector to protest Googles prudish doodles](https://www.theverge.com/tldr/2019/6/17/18681733/google-ai-doodle-detector-penis-protest-moniker-mozilla)

[demo](https://donotdrawapenis.com) [dataset](https://github.com/studiomoniker/quickdraw-appendix)

*From the article:*

Cast your mind back to 2016 and you might recall [Quick, Draw!](https://quickdraw.withgoogle.com)  an AI experiment from Google that guessed what users were doodling. It was basically AI Pictionary, with Google later releasing the millions of sketches it collected as an [open-source dataset](https://www.theverge.com/tldr/2017/5/19/15662784/google-quick-draw-data-dragon-artificial-intelligence).

But there was one doodle that Googles AI never recognized and that never appeared in its data: the humble penis.

It sounds childish, but its sort of a big omission. The penis is perhaps the most significant and durable doodle of all time. Its a sigil thats been scrawled on surfaces for thousands of years  everywhere from [Roman walls](https://www.militarytimes.com/off-duty/military-culture/2019/02/28/1800-year-old-roman-penis-carvings-discovered-near-hadrians-wall-some-things-never-change/) to [medieval manuscripts](https://www.atlasobscura.com/articles/medieval-marginalia-books-doodles)  and variously signifies good luck, virility, or just Im a man and I was here.

To rectify Googles mistake, the Mozilla foundation commissioned Dutch design studio Moniker to build an AI penis doodle detector. Its a bit of silly fun, but Moniker and Mozilla say theyre also making a serious point: in an age where US tech giants control so much of what we see online, should we be worried about the moral standards they get to set?

You can test out the penis detector out [here](https://donotdrawapenis.com). When you doodle a penis itll say we assume this was a mistake and erase it, warning users: Dont take individual expression too far! Draw enough of them and it will go on a mad tirade, doodling itself into a frenzy.



article: https://www.theverge.com/tldr/2019/6/17/18681733/google-ai-doodle-detector-penis-protest-moniker-mozilla

demo: https://donotdrawapenis.com

dataset: https://github.com/studiomoniker/quickdraw-appendix",0,1
1063,2019-6-18,2019,6,18,12,c1x2hq,Support vectors,https://www.reddit.com/r/MachineLearning/comments/c1x2hq/support_vectors/,prajapatiKaushal,1560827699,[removed],0,1
1064,2019-6-18,2019,6,18,12,c1x5s0,[P] Do you know SQuAD dataset? Try KorQuAD(Korean SQuAD dataset) with pre-trained BERT!,https://www.reddit.com/r/MachineLearning/comments/c1x5s0/p_do_you_know_squad_dataset_try_korquadkorean/,lyeoni,1560828271,"A simple tutorial on how to apply pre-trained BERT model to Korean QA task.

&amp;#x200B;

A pre-trained BERT model is publicly available !

[huggingface/pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT) repository contains op-for-op PyTorch reimplementations, pre-trained models and fine-tuning examples for Google's BERT model. And as a result of submission BERT fine-tuned with default hyper-parameter, it ranked **30th** **with EM= 71.47, F1= 89.71 on the KorQuAD** [leaderboard](https://korquad.github.io/).

&amp;#x200B;

So, I covered **the process of fine-tuning and submitting BERT** and result for official evaluation on KorQuAD. Once your BERT model has been evaluated officially, scores will be added to the leaderboard.

&amp;#x200B;

[https://github.com/lyeoni/KorQuAD](https://github.com/lyeoni/KorQuAD)",0,5
1065,2019-6-18,2019,6,18,12,c1xb29,Information About the Sarich Orbital Engine,https://www.reddit.com/r/MachineLearning/comments/c1xb29/information_about_the_sarich_orbital_engine/,isace1234,1560829216,[removed],0,1
1066,2019-6-18,2019,6,18,13,c1xod2,[R] One Epoch Is All You Need,https://www.reddit.com/r/MachineLearning/comments/c1xod2/r_one_epoch_is_all_you_need/,HigherTopoi,1560831668,,5,7
1067,2019-6-18,2019,6,18,13,c1y0ao,Any geometric deep learning idea for undergrad research project?,https://www.reddit.com/r/MachineLearning/comments/c1y0ao/any_geometric_deep_learning_idea_for_undergrad/,convexer,1560833922,[removed],0,1
1068,2019-6-18,2019,6,18,14,c1y9x0,"Neuroevolution - Just started, looking for advice/good practices/things to add/avoid.",https://www.reddit.com/r/MachineLearning/comments/c1y9x0/neuroevolution_just_started_looking_for/,stupidfrigginidiot,1560835840,"Hi, I dont have a degree or any actual education in this field at all. It just interests me so I looked some things up and have been playing around with my own algorithms made from some intuition/things learnt in Processing. Im not really sure how to categorise this type of machine learning, but here are the current features I have.

&amp;#x200B;

The model has a brain which holds a set of nodes (basically neurons). The nodes are kept in a 2d ArrayList where each column is a new layer of nodes. The amount of layers and number of nodes in each layer is easily variable, which leads me to want to go down a NEAT sort of route. The Nodes themselves have a list of nodes they are connected to (meaning not every node in the prior layer needs to be connected to the next layer), and any nodes connected will have a corresponding weight. These connected nodes can be added and taken away at will. There is a way to mutate weights (at the moment Im just multiplying the current weights by some multiplier, or you can generate completely new ones whcih is the same as getting a new node). The brain can then think by feeding forward the information layer by layer starting with the inputs being the values for the first nodes. The nodes carry an objective function (not sure if this is the right word) which takes in a weight and the summation of the values passed to it by the prior nodes, and feeds forward the outcome of this function to the connecting nodes.

&amp;#x200B;

To me, this seems about as basic as I can get. And being so basic, it can only accomplish basic tasks. I've tried to apply this to a 2D physics system where there a set of agents all trying to complete a common goal. The higher the fitness found (Likely 1/distance) the more likely to be bred. There is a super basic physics system that has no restraints/gravity, and I've made the outputs of the brain correspond to the x and y accelerations. These agents can learn to reach a target just fine when the area has non-moving obstacles, but as soon as the problem becomes more advanced they hit local, bad optima very quickly.

&amp;#x200B;

I'm sure I could keep experimenting, play around with the parent selection process, tweak how nodes are mutated/which nodes are mutated, put in way more and way better inputs (Like giving each agent a set of eyes which look in some amount of directions for an obstacle) , but I thought I might get your ideas as well so I dont go down some bad path. Once I am happy with the completion of this, I want to start playing around with some proper machine learning libraries and c++ graphics. Anyway, thanks for any help.",0,1
1069,2019-6-18,2019,6,18,14,c1yh2j,AI Takeover: I dipped my head in the waters,https://www.reddit.com/r/MachineLearning/comments/c1yh2j/ai_takeover_i_dipped_my_head_in_the_waters/,Oratorshub,1560837296,,0,1
1070,2019-6-18,2019,6,18,15,c1yrnp,[P] MMDetection: Open MMLab Detection Toolbox and Benchmark,https://www.reddit.com/r/MachineLearning/comments/c1yrnp/p_mmdetection_open_mmlab_detection_toolbox_and/,tkzcthu,1560839389,"We present MMDetection, an object detection toolbox that contains a rich set of object detection and instance segmentation methods as well as related components and modules.

* collaboration with 10+ research institutes
* 20+ supported methods
* 200+ model weights

Technical report: https://arxiv.org/abs/1906.07155",1,12
1071,2019-6-18,2019,6,18,15,c1ys84,"Benefits of ensembling the exact same model, differing just in seed?",https://www.reddit.com/r/MachineLearning/comments/c1ys84/benefits_of_ensembling_the_exact_same_model/,EveningAlgae,1560839503,[removed],0,1
1072,2019-6-18,2019,6,18,15,c1yusv,[D] Distilling BERT  How to achieve BERT performance using Logistic Regression,https://www.reddit.com/r/MachineLearning/comments/c1yusv/d_distilling_bert_how_to_achieve_bert_performance/,sudo_su_,1560840029,"&amp;#x200B;

Few days ago, in this [post](https://www.reddit.com/r/MachineLearning/comments/bwb87n/d_make_bert_model_smaller/), I asked about a way to make BERT smaller. I got some interesting results and found some relevant papers. The basic idea is, given a relatively small labelled dataset, and another much bigger unlabelled set:

1. Train BERT on the labeled set
2. Predict values of the unlabelled set  
3. Train a much smaller model using the now the labelled big set

&amp;#x200B;

I tried it with Logistic Regression and got some interesting results here:

[https://towardsdatascience.com/distilling-bert-how-to-achieve-bert-performance-using-logistic-regression-69a7fc14249d?source=friends\_link&amp;sk=2e62337d0b44f56409640c27277b99ce](https://towardsdatascience.com/distilling-bert-how-to-achieve-bert-performance-using-logistic-regression-69a7fc14249d?source=friends_link&amp;sk=2e62337d0b44f56409640c27277b99ce)",3,2
1073,2019-6-18,2019,6,18,15,c1yx51,[N] Best Python Libraries for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/c1yx51/n_best_python_libraries_for_machine_learning/,vadhavaniyafaijan,1560840497,,0,1
1074,2019-6-18,2019,6,18,15,c1yz3m,Machine Learning  A Growing Technology Field and A Rewarding Career!,https://www.reddit.com/r/MachineLearning/comments/c1yz3m/machine_learning_a_growing_technology_field_and_a/,multisoftmva0,1560840915,,0,1
1075,2019-6-18,2019,6,18,15,c1z0dk,Product impact on customers model,https://www.reddit.com/r/MachineLearning/comments/c1z0dk/product_impact_on_customers_model/,Raining_Day,1560841195,[removed],0,1
1076,2019-6-18,2019,6,18,16,c1z6pm,[D] Stumbled upon this interesting use of AI while doing my research,https://www.reddit.com/r/MachineLearning/comments/c1z6pm/d_stumbled_upon_this_interesting_use_of_ai_while/,sonofmoutain,1560842498,"I was actually looking to start my own project on forecasting cryptocurrency prices and stumbled upon [this](https://3piks.com/main).  
Their top AI is predicting at around 63%, i found it rather impressive for it to be predicting at such a high rate.   


Does anyone have any idea on what they are doing? Was trying to dive further but the language is quite terrible.",5,1
1077,2019-6-18,2019,6,18,16,c1zbzo,Are there any Python libraries that allow byte-pair encoding that splits on something other than space?,https://www.reddit.com/r/MachineLearning/comments/c1zbzo/are_there_any_python_libraries_that_allow/,shamoons,1560843655,[removed],0,1
1078,2019-6-18,2019,6,18,17,c1zhds,[D] Are there any Python libraries that allow byte-pair encoding that splits on something other than space?,https://www.reddit.com/r/MachineLearning/comments/c1zhds/d_are_there_any_python_libraries_that_allow/,shamoons,1560844888," I have a large corpus of source code, and space matters in certain languages (like Python). It seems that [https://github.com/rsennrich/subword-nmt](https://github.com/rsennrich/subword-nmt) splits on space. Are there other packages that will split on ''?",3,3
1079,2019-6-18,2019,6,18,17,c1zitu,9 Free Machine Learning Courses from World-Class Educators,https://www.reddit.com/r/MachineLearning/comments/c1zitu/9_free_machine_learning_courses_from_worldclass/,sanjayit38,1560845230,,0,1
1080,2019-6-18,2019,6,18,17,c1ziz9,10 Best Machine Learning Textbooks that All Data Scientists Should Read,https://www.reddit.com/r/MachineLearning/comments/c1ziz9/10_best_machine_learning_textbooks_that_all_data/,LimarcAmbalina,1560845268,[removed],0,1
1081,2019-6-18,2019,6,18,17,c1zkql,[Tutorial] How to setup MLflow in production - Get a Machine Learning model into production with MLflow in 10 minutes,https://www.reddit.com/r/MachineLearning/comments/c1zkql/tutorial_how_to_setup_mlflow_in_production_get_a/,dcanones,1560845687,,0,2
1082,2019-6-18,2019,6,18,17,c1zms7,Does someone research instrument simulation? [Discussion],https://www.reddit.com/r/MachineLearning/comments/c1zms7/does_someone_research_instrument_simulation/,fimari,1560846175,"I think of something that's modelled like a GAN but one side is a real instrument (like a midi controlled organ) as discrimination and the other network as generation.

In that way the network could learn from the real instrument, to generate perfect real sounding midi instruments.

Just an idea - some thoughts about the implementation?",1,0
1083,2019-6-18,2019,6,18,18,c1zxq5,"group Telegram about deep learning ,machine learning , Artificial Intelligence by andrew ng",https://www.reddit.com/r/MachineLearning/comments/c1zxq5/group_telegram_about_deep_learning_machine/,Doctor_who1,1560848768,[removed],0,1
1084,2019-6-18,2019,6,18,18,c1zyd6,[P] Detecting Kissing Scenes in a Database of Hollywood Films,https://www.reddit.com/r/MachineLearning/comments/c1zyd6/p_detecting_kissing_scenes_in_a_database_of/,milaworld,1560848909,,4,3
1085,2019-6-18,2019,6,18,18,c202y2,Found a chrome extension that let's you save arxiv papers to read later,https://www.reddit.com/r/MachineLearning/comments/c202y2/found_a_chrome_extension_that_lets_you_save_arxiv/,chicchoctech,1560849945,[removed],0,1
1086,2019-6-18,2019,6,18,18,c204kq,[Tutorial] How to set up MLflow in production: put a Machine Learning model into production with MLflow in 10 minutes,https://www.reddit.com/r/MachineLearning/comments/c204kq/tutorial_how_to_set_up_mlflow_in_production_put_a/,thegurus,1560850301,,0,1
1087,2019-6-18,2019,6,18,19,c20fhs,Stand-Alone Self-Attention in Vision Models,https://www.reddit.com/r/MachineLearning/comments/c20fhs/standalone_selfattention_in_vision_models/,reformed_scientist,1560852555,,4,8
1088,2019-6-18,2019,6,18,19,c20iq4,Indian English + DeepSpeech,https://www.reddit.com/r/MachineLearning/comments/c20iq4/indian_english_deepspeech/,opensourcefairy,1560853214,[removed],0,1
1089,2019-6-18,2019,6,18,19,c20irx,any ML course recommendation for paid or free.,https://www.reddit.com/r/MachineLearning/comments/c20irx/any_ml_course_recommendation_for_paid_or_free/,Gloryzy,1560853224,[removed],0,1
1090,2019-6-18,2019,6,18,19,c20kzp,Validation loss decreases but starts increasing after a certain point?,https://www.reddit.com/r/MachineLearning/comments/c20kzp/validation_loss_decreases_but_starts_increasing/,myidispg,1560853675," 

I was training the Le-Net 5-LeCun et al architecture on the Cifar-10 dataset for some tensorflow practice and I noticed that my Validation loss and accuracy starts to increase after a point and then starts to yo-yo around the point while my training loss keeps decreasing. I think that my model might be overfitting after that point. Am I right? Any suggestions to get better results?

![img](u07kt0ajc3531)

Train loss graph

&amp;#x200B;

![img](mxeu0m7kc3531)

Validation loss graph:

&amp;#x200B;

![img](755rpuukc3531)

Validation accuracy graph:

&amp;#x200B;

![img](r0zsp5glc3531)",0,1
1091,2019-6-18,2019,6,18,19,c20l98,"Telescopic Forks , Telescopic fork , Single Deep Telescopic Forks",https://www.reddit.com/r/MachineLearning/comments/c20l98/telescopic_forks_telescopic_fork_single_deep/,lhd121,1560853725,,0,1
1092,2019-6-18,2019,6,18,19,c20tor,[D] How can I encode positional information of sentences in a text in a BERT model?,https://www.reddit.com/r/MachineLearning/comments/c20tor/d_how_can_i_encode_positional_information_of/,radcapbill,1560855439,"I am trying to train a BERT model for a certain form of text classification and I realized that it might be useful to know whether a sentence is on the same line or on a newline in a pdf document. Is that any way to encode the newline distance in BERT? My idea was using different customized positional encoding but I am not sure whether it is the correct approach, and if it is, what continuous function to use. Would love to hear any suggestions on this.",7,2
1093,2019-6-18,2019,6,18,20,c20vwa,[R] Stacked Capsule Autoencoders,https://www.reddit.com/r/MachineLearning/comments/c20vwa/r_stacked_capsule_autoencoders/,hardmaru,1560855863,,7,53
1094,2019-6-18,2019,6,18,20,c20w8u,NeuroEvolution - How can I improve this program?,https://www.reddit.com/r/MachineLearning/comments/c20w8u/neuroevolution_how_can_i_improve_this_program/,stupidfrigginidiot,1560855931,[removed],0,1
1095,2019-6-18,2019,6,18,20,c20wo8,Tacotron - Text Predicted Global Style Tokens (TP GST),https://www.reddit.com/r/MachineLearning/comments/c20wo8/tacotron_text_predicted_global_style_tokens_tp_gst/,its_sandy,1560856009,"I have some doubts on the concept of Text Predicted Global Style Tokens (TP-GST) that was introduced in this paper  [https://arxiv.org/pdf/1808.01410.pdf](https://arxiv.org/pdf/1808.01410.pdf)  . Basically, why do we even need the reference encoder (except for still being able to condition generated speech on reference audio) when you can directly predict a style context vector from the text? Why should the style context vector predicted from text be made to match the vector given by the reference encoder? Can't we directly use whatever style context vector is predicted by text (which will anyway learn to model style as it will be made to minimize loss w.r.t the true mel-spectograms)?",0,1
1096,2019-6-18,2019,6,18,20,c20xyg,Visualising feature weight (importance) in Logistic Regression Model,https://www.reddit.com/r/MachineLearning/comments/c20xyg/visualising_feature_weight_importance_in_logistic/,Turtleneck_Ed,1560856271,[removed],0,1
1097,2019-6-18,2019,6,18,20,c20yja,Business Intelligence and Analytics with Data Governance!,https://www.reddit.com/r/MachineLearning/comments/c20yja/business_intelligence_and_analytics_with_data/,ElegantMicroWebIndia,1560856375,,0,1
1098,2019-6-18,2019,6,18,20,c20zmw,8 Ways You Can Succeed In a Machine Learning Career,https://www.reddit.com/r/MachineLearning/comments/c20zmw/8_ways_you_can_succeed_in_a_machine_learning/,fullstackanalytics1,1560856585,,0,1
1099,2019-6-18,2019,6,18,20,c212p8,What's the Real Cost of Cyber Attacks?,https://www.reddit.com/r/MachineLearning/comments/c212p8/whats_the_real_cost_of_cyber_attacks/,MimozaNaumovska,1560857131,,0,1
1100,2019-6-18,2019,6,18,21,c21fox,Modeling Music Modality with a Key-Class Invariant Pitch Chroma CNN,https://www.reddit.com/r/MachineLearning/comments/c21fox/modeling_music_modality_with_a_keyclass_invariant/,AElowsson,1560859605,,1,1
1101,2019-6-18,2019,6,18,21,c21ini,[R] Modeling Music Modality with a Key-Class Invariant Pitch Chroma CNN,https://www.reddit.com/r/MachineLearning/comments/c21ini/r_modeling_music_modality_with_a_keyclass/,AElowsson,1560860134,,1,1
1102,2019-6-18,2019,6,18,21,c21kgz,[R] Modeling Music Modality with a Key-Class Invariant Pitch Chroma CNN,https://www.reddit.com/r/MachineLearning/comments/c21kgz/r_modeling_music_modality_with_a_keyclass/,AElowsson,1560860430,,4,3
1103,2019-6-18,2019,6,18,22,c223yg,[R] Should I test multiple seeds for randomforests?,https://www.reddit.com/r/MachineLearning/comments/c223yg/r_should_i_test_multiple_seeds_for_randomforests/,Frogad,1560863642,"So I carried out a random forest for regression, and I have one set of results. Would it make sense to run the algorithm on different seeds, but how would I compare the data for each run?

Or is the one run enough?

Also, if its for regression and not predicting, is there much to gain from running test vs training data?",4,1
1104,2019-6-18,2019,6,18,22,c225zv,How Compliant Cloud can make the difference to businesses in a regulated sector?,https://www.reddit.com/r/MachineLearning/comments/c225zv/how_compliant_cloud_can_make_the_difference_to/,rosamarts,1560863964,,0,1
1105,2019-6-18,2019,6,18,22,c22ejl,Use of ML and NLP to detect and classify noise complaints from Online Social Networks,https://www.reddit.com/r/MachineLearning/comments/c22ejl/use_of_ml_and_nlp_to_detect_and_classify_noise/,luisgasco,1560865344,,0,1
1106,2019-6-18,2019,6,18,22,c22kzc,Facebook are releasing code for a new approach to generating recipes directly from food images. This produces more compelling recipes than retrieval-based approaches and improves performance with respect to previous baselines for ingredient prediction.,https://www.reddit.com/r/MachineLearning/comments/c22kzc/facebook_are_releasing_code_for_a_new_approach_to/,ZhiyuanWan,1560866356,,0,1
1107,2019-6-18,2019,6,18,23,c22pov,Difference Between Linear and Logistic Regression with Code Examples (Python),https://www.reddit.com/r/MachineLearning/comments/c22pov/difference_between_linear_and_logistic_regression/,studytonight,1560867028,,0,1
1108,2019-6-18,2019,6,18,23,c22q3e,Artificial Intelligence and Machine Learning in Cybersecurity,https://www.reddit.com/r/MachineLearning/comments/c22q3e/artificial_intelligence_and_machine_learning_in/,MariaMiladinovikj,1560867088,,0,1
1109,2019-6-18,2019,6,18,23,c22qd9,[R] Functional Regularisation for Continual Learning (DeepMind),https://www.reddit.com/r/MachineLearning/comments/c22qd9/r_functional_regularisation_for_continual/,petrified_piranha,1560867126,,0,1
1110,2019-6-18,2019,6,18,23,c22ry1,Please help me find an ML textbook that was mentioned on a podcast.,https://www.reddit.com/r/MachineLearning/comments/c22ry1/please_help_me_find_an_ml_textbook_that_was/,realSieh,1560867356,"Asking for a friend.

&amp;#x200B;

He mentioned he was listening to a podcast (maybe on datacamp.. not sure which) about two weeks ago and someone recommended a textbook on ML with applications in behavioral sciences. Anyone here come across it ? Sorry I don't have that much info to go with this.",0,1
1111,2019-6-18,2019,6,18,23,c22u9s,"[R] DeepMind: Neural networks suffer from catastrophic forgetting when tasks are encountered sequentially. We overcome this by Bayesian inference in function space, using inducing point sparse GP methods and by optimising over rehearsal data points",https://www.reddit.com/r/MachineLearning/comments/c22u9s/r_deepmind_neural_networks_suffer_from/,Corp-Por,1560867700,,49,332
1112,2019-6-18,2019,6,18,23,c232jx,"[N]Apple, Google, and Facebook Are Raiding Animal Research Labs",https://www.reddit.com/r/MachineLearning/comments/c232jx/napple_google_and_facebook_are_raiding_animal/,htrp,1560868938,,0,1
1113,2019-6-19,2019,6,19,0,c239x9,Safecont Tutorial: Learn how to use the SEO tool in a few minutes,https://www.reddit.com/r/MachineLearning/comments/c239x9/safecont_tutorial_learn_how_to_use_the_seo_tool/,Safecont,1560870011,,1,1
1114,2019-6-19,2019,6,19,0,c23ahu,[D] What are some examples of malicious tasks that language models can trained for?,https://www.reddit.com/r/MachineLearning/comments/c23ahu/d_what_are_some_examples_of_malicious_tasks_that/,DisastrousProgrammer,1560870095,"AI safety and ethics is a hot topic. Besides fake news generation, and toxic comment generation, what other malicious tasks could language models be trained for?",5,2
1115,2019-6-19,2019,6,19,0,c23fe5,Refining the question bank,https://www.reddit.com/r/MachineLearning/comments/c23fe5/refining_the_question_bank/,IndominusRexx7,1560870733,[removed],0,1
1116,2019-6-19,2019,6,19,0,c23jtw,Spacy ner,https://www.reddit.com/r/MachineLearning/comments/c23jtw/spacy_ner/,GrImPeAper236969,1560871356,[removed],0,1
1117,2019-6-19,2019,6,19,0,c23yrp,Violating assumptions for practicality,https://www.reddit.com/r/MachineLearning/comments/c23yrp/violating_assumptions_for_practicality/,Dreshna,1560873405,[removed],0,1
1118,2019-6-19,2019,6,19,1,c246v8,[D] Is Computer Engineering relevant in the field of ML/AI?,https://www.reddit.com/r/MachineLearning/comments/c246v8/d_is_computer_engineering_relevant_in_the_field/,sonicmachine,1560874465,"Hey guys,

I am a computer engineer(software design and hardware-software engineering) and planning to foray into ML/AI. I have studied the basics of ML/AI and wish to leverage my CE skills (memory optimization, speed optimization) in the field of ML/AI (especially deep learning). All the algorithms are based on probabilities and statistical methods (A strong generalization, please excuse me for this. Any kind of contradiction to this statement is welcome). I learn that mathematical optimization is predominant in the field of ML/AI. How can something be built in the field of ML/AI with the focus on CE skills? I was looking for a direction of research or demand for novelty in the said field.

Please help me understand how to leverage computer engineering to enhance models in the field of AI.

I will appreciate anything which helps me understand the place for computer engineer in this field. I am primarily looking for things such as; research demands, current state-of-the-art work or concept extrapolation from CE to ML/AI. Where would you start if you were me?

Thank you.",0,2
1119,2019-6-19,2019,6,19,1,c24bxn,[D] How can a computer engineer make himself/herself relevant in the field of AI?,https://www.reddit.com/r/MachineLearning/comments/c24bxn/d_how_can_a_computer_engineer_make_himselfherself/,sonicmachine,1560875180,"Hey guys,

I am a computer engineer(software design and hardware-software engineering) and planning to get into ML/AI. I have studied the basics of ML/AI and wish to leverage my CE skills (memory optimization, speed optimization) to solve problems in the field of ML/AI (especially deep learning). All the algorithms in the machine learning domain are based on probabilities and on statistical methods (A strong generalization perhaps?, please excuse me if that's the case. Any kind of contradiction to this statement is welcome). I learn that mathematical optimization is predominant in the field of ML/AI. How can something be built in the field of ML/AI with the focus on CE skills? I was looking for a direction of research or demand for novelty in the said field.

Please help me understand how to leverage computer engineering to enhance models in the field of AI.

I will appreciate anything which helps me understand the place for computer engineer in this field. I am primarily looking for things such as; research demands, current state-of-the-art work or concept extrapolation from CE to ML/AI. 

Thank you.",5,0
1120,2019-6-19,2019,6,19,1,c24eau,[Q] Architecture for Simultaneous Classification,https://www.reddit.com/r/MachineLearning/comments/c24eau/q_architecture_for_simultaneous_classification/,CircuitBeast,1560875485,[removed],0,1
1121,2019-6-19,2019,6,19,1,c24kfa,"[D] The Past, Present, and Future of AI Art",https://www.reddit.com/r/MachineLearning/comments/c24kfa/d_the_past_present_and_future_of_ai_art/,hughbzhang,1560876311,[removed],0,1
1122,2019-6-19,2019,6,19,1,c24l8y,"[D] The Past, Present, and Future of AI Art",https://www.reddit.com/r/MachineLearning/comments/c24l8y/d_the_past_present_and_future_of_ai_art/,hughbzhang,1560876426,Recent news coverage would have you believe that the computer generated art revolution has finally come. Fabian Offert argues that it has been here all along.,22,0
1123,2019-6-19,2019,6,19,2,c24s8w,[D] Skorch for BERT,https://www.reddit.com/r/MachineLearning/comments/c24s8w/d_skorch_for_bert/,dataOR,1560877343,"I'm attempting to wrap my pretrained BERT into an sklearn model with very limited success.  I've managed to load input embeddings and labels using datasets, but I hit errors trying to pass more than one X to include the input masks and segment_ids.  Any thoughts? Have people had success with this approach?",2,3
1124,2019-6-19,2019,6,19,2,c24uk4,Improved Microsoft MT-DNN Tops GLUE Rankings,https://www.reddit.com/r/MachineLearning/comments/c24uk4/improved_microsoft_mtdnn_tops_glue_rankings/,Yuqing7,1560877627,,0,1
1125,2019-6-19,2019,6,19,2,c24vyf,[P] Residual Convolution Recurrent Network for ECG Classification,https://www.reddit.com/r/MachineLearning/comments/c24vyf/p_residual_convolution_recurrent_network_for_ecg/,hadaev,1560877818,"Classification of long time series using residual convolutions and GRU layer.

Used data from physionet2017 challenge.

It seemed to me strange that participants extracted features by other methods, instead of giving this task to model itself.

A pretty simple example, but I hope someone finds it useful.

[Github with code](https://laterforreddit.com/)",0,3
1126,2019-6-19,2019,6,19,2,c252op,What is the difference between the following roles? Machine Learning Scientist vs Machine Learning Engineer.,https://www.reddit.com/r/MachineLearning/comments/c252op/what_is_the_difference_between_the_following/,7Araa,1560878685,[removed],0,1
1127,2019-6-19,2019,6,19,2,c25fuy,Training dataset of various documents?,https://www.reddit.com/r/MachineLearning/comments/c25fuy/training_dataset_of_various_documents/,saasvc,1560880431,[removed],0,1
1128,2019-6-19,2019,6,19,3,c25o80,[D] Creating a Face Verification algorithm for authentication,https://www.reddit.com/r/MachineLearning/comments/c25o80/d_creating_a_face_verification_algorithm_for/,Berdas_,1560881485,"Hello guys!  


I've just started working on a small project that involves analyzing a web cam generated image and compare it to images on a dataset folder to try and find a match.

There are some pre-trained models out there, which enables one-shot learning (like this GitHub for example:  [https://github.com/mohitwildbeast/Facial-Recognition-Using-FaceNet-Siamese-One-Shot-Learning](https://github.com/mohitwildbeast/Facial-Recognition-Using-FaceNet-Siamese-One-Shot-Learning))

However, it is not precise as I wanted, and I don't really know why.

I was looking the FaceNet model by David Sandberg ([https://github.com/davidsandberg/facenet](https://github.com/davidsandberg/facenet)) and it seems promising, however I don't know how to use it for my case.

So, I was wondering if you guys have any advice for me, any link, that might help me!  
The system should be simple, is just a proof of concept, so as long as the algorithm can compare the face it is detecting on the webcam, for example, to one on a images folder and return the embedded distance (distance between the faces, where smaller are similar faces and bigger otherwise).

  
I'm not sure if I was clear, as it is my first time writing on this sub.

&amp;#x200B;

Thank you all in advance.",2,1
1129,2019-6-19,2019,6,19,3,c25u2f,Pytorch or Tensorflow?,https://www.reddit.com/r/MachineLearning/comments/c25u2f/pytorch_or_tensorflow/,evanthebouncy,1560882251,,1,6
1130,2019-6-19,2019,6,19,3,c26040,[D] Meta-Generative Adversarial Networks for AGI,https://www.reddit.com/r/MachineLearning/comments/c26040/d_metagenerative_adversarial_networks_for_agi/,cryptonewsguy,1560883031,"So I have this idea for creating AGI based off of some things I've been reading from Jrgen Schmidhubers.

# Meta-learn

Lets say you have a Neural Network (the parent network) which can design arbitrary children networks and learn optimized design patterns for a given task. Of course your parent network won't be super generalized for any type of network design, just relatively specific tasks. This is kind of why we don't have ""real"" AI or AGI. The tasks are still relatively narrow. 


Skimming the literature on meta-learning it looks like researchers have been able to get SOME generalization by training their meta-networks on multiple tasks. But of course data and identifying tasks might be a limitation for scale-ability and high levels of generalization. So I purpose a potentially more elegant way.


# PowerPlay


This is where [Jrgen Schmidhubers PowerPlay would come in.](https://arxiv.org/abs/1112.5309) The PowerPlay algorithm is split into a solver and a problem generator. The problem generator generates novel problems which the solver has to try to solve. Novel problems are problems which are unsolvable by the current solver. The created problems are just a bit more complicated than the most complicated solvable problem. The solver has to be able to solve all previous problems the generator created plus the new one. 




# Meta-PowerPlay

Both the problem Solver and Generator have parent Networks which continually learn to design more sophisticated Solvers and Generators until you have much more general problem solvers, or rather a neural network that can design general problem solvers.


Its kind of similar how GANs work for deepfakes and image problems work, the networks try to outsmart eachother in a feedbackloop but instead of just doing a faceswap, it can generate a general purpose computational engine. Or at least one that is a lot more general than what we currently have.",8,4
1131,2019-6-19,2019,6,19,4,c26hai,Bachelor's degree holder targeting Machine Learning jobs,https://www.reddit.com/r/MachineLearning/comments/c26hai/bachelors_degree_holder_targeting_machine/,optimizedEater,1560885330,"I recently completed my bachelor's in Computer Science and have been working as a Machine Learning Engineer for the past 5 months. Now I'm looking to transition into a bigger company in a similar role.

Background:

I have a bachelor's degree in Computer Science from a mid-tier public university in the United States. During college, I participated in a summer REU in ML. I have co-authored 3 publications (two as second author) and published them at top AI conferences. 

My initial plan was to go straight for a PhD in Machine Learning, but I was too ambitious with the schools I picked, and I ended up being rejected by all of them. 

Luckily, in the meantime, I landed a job as a ML Engineer and I've been working in this position for the past 5 months. In the time that I've worked, I have realized that I like writing code and putting things into production slightly more than hardcore research. I like my job, but I'd like to transition into a bigger company with a more established Data Science/ML team. 

Here's where I'd like to hear my fellow redditors' thoughts.

I'm debating whether I should consider doing a Master's in Machine Learning, get that degree, and then target the big companies? Or can I make up for the lack of an advanced degree through work experience?

As you all probably know, most of the job postings that I see expect the candidate to have a Master's degree as a minimum, or a PhD. I understand where they come from, as I'm aware that nothing can substitute the knowledge depth you gain by going through the rigor of grad school. 

I was in the fast track master's program during my bachelor's, and I'm only 2 semesters away from getting my master's in C.S with a concentration in Data Science. The reason I didn't continue is because I knew for a fact that 1) I could learn more by working and 2) the quality of the coursework is not great. On the plus side of doing the master's, I have a good relationship with a few professors who are pretty involved in ML research, and I could do a research based master's with a thesis and boost my profile. 

&amp;#x200B;

What are your thoughts?",0,1
1132,2019-6-19,2019,6,19,4,c26knl,CVPR 2019 Attracts 9K Attendees; Best Papers Announced; ImageNet Honoured 10 Years Later,https://www.reddit.com/r/MachineLearning/comments/c26knl/cvpr_2019_attracts_9k_attendees_best_papers/,Yuqing7,1560885781,,0,1
1133,2019-6-19,2019,6,19,4,c26lt1,[D] C.S Bachelor's degree holder seeking ML career advice,https://www.reddit.com/r/MachineLearning/comments/c26lt1/d_cs_bachelors_degree_holder_seeking_ml_career/,optimizedEater,1560885933,"I recently completed my bachelor's in Computer Science and have been working as a Machine Learning Engineer for the past 5 months. Now I'm looking to transition into a bigger company in a similar role.

Background:

I have a bachelor's degree in Computer Science from a mid-tier public university in the United States. During college, I participated in a summer REU in ML. I have co-authored 3 publications (two as second author) and published them at top AI conferences.

My initial plan was to go straight for a PhD in Machine Learning, but I was too ambitious with the schools I picked, and I ended up being rejected by all of them.

Luckily, in the meantime, I landed a job as a ML Engineer and I've been working in this position for the past 5 months. In the time that I've worked, I have realized that I like writing code and putting things into production slightly more than hardcore research. I like my job, but I'd like to transition into a bigger company with a more established Data Science/ML team.

Here's where I'd like to hear my fellow redditors' thoughts.

I'm debating whether I should consider doing a Master's in Machine Learning, get that degree, and then target the big companies? Or can I make up for the lack of an advanced degree through work experience?

As you all probably know, most of the job postings that I see expect the candidate to have a Master's degree as a minimum, or a PhD. I understand where they come from, as I'm aware that nothing can substitute the knowledge depth you gain by going through the rigor of grad school.

I was in the fast track master's program during my bachelor's, and I'm only 2 semesters away from getting my master's in C.S with a concentration in Data Science. The reason I didn't continue is because I knew for a fact that 1) I could learn more by working and 2) the quality of the coursework is not great. On the plus side of doing the master's, I have a good relationship with a few professors who are pretty involved in ML research, and I could do a research based master's with a thesis and boost my profile.

&amp;#x200B;

What are your thoughts?",13,16
1134,2019-6-19,2019,6,19,4,c26o5i,The Mathematics of Machine Learning by UC Berkeley [ WRITTEN BY - GARRET THOMAS ],https://www.reddit.com/r/MachineLearning/comments/c26o5i/the_mathematics_of_machine_learning_by_uc/,ai-lover,1560886242," **Download Link:**  [https://gwthomas.github.io/docs/math4ml.pdf](https://gwthomas.github.io/docs/math4ml.pdf?fbclid=IwAR3G48y_-hwC1xglF9aEQxerTSNbiNf5jnsJ4GUIrpAlAi5tm5Q1ZypN3PQ)  

&amp;#x200B;

![img](l6gb50zn26531)",0,1
1135,2019-6-19,2019,6,19,4,c26s0b,a course to learn how to productionize(deploy?) ML in real world?,https://www.reddit.com/r/MachineLearning/comments/c26s0b/a_course_to_learn_how_to_productionizedeploy_ml/,throwawaylalallal,1560886765,"It seems there is not much covering how to bring trained ML models to real world. I ve read its called 'productionize'.

I think engineers who can both train,tune and successfully deploy new model are much more valuable.

Where to study this skill?",0,1
1136,2019-6-19,2019,6,19,5,c2759x,Perceptrons was developed around the 1950s and 1960s!,https://www.reddit.com/r/MachineLearning/comments/c2759x/perceptrons_was_developed_around_the_1950s_and/,frontnetcoin,1560888489,,0,1
1137,2019-6-19,2019,6,19,5,c279yp,ML journal club/reading group in Munich,https://www.reddit.com/r/MachineLearning/comments/c279yp/ml_journal_clubreading_group_in_munich/,lagrange_2,1560889100,"Hi everyone,

&amp;#x200B;

I'm setting up an ML journal club in Munich to read and discuss papers in a (super) casual setting.  Since I moved to industry it's definitely something I miss from academia, and I think it benefits everyone who participates.

&amp;#x200B;

If you're interested in joining, send me a message and let's get the ball rolling.  I think we'd need a critical mass of around 5 people, with 10-15 being the ideal number.

&amp;#x200B;

I'm primarily work with computer vision/perception topics, so that's my bias, but my interests are fairly wide.

&amp;#x200B;

cheers,

andy",0,1
1138,2019-6-19,2019,6,19,5,c27eaa,Clear example for BERT with Pytorch,https://www.reddit.com/r/MachineLearning/comments/c27eaa/clear_example_for_bert_with_pytorch/,saravanakumar17,1560889663,[removed],0,1
1139,2019-6-19,2019,6,19,5,c27mnd,Is there a name for this type of algorithm I want to use?,https://www.reddit.com/r/MachineLearning/comments/c27mnd/is_there_a_name_for_this_type_of_algorithm_i_want/,abm513,1560890773,[removed],0,1
1140,2019-6-19,2019,6,19,6,c282uu,Should my dataset be balanced if the distribution in real world is imbalanced?,https://www.reddit.com/r/MachineLearning/comments/c282uu/should_my_dataset_be_balanced_if_the_distribution/,jambery,1560892961,[removed],0,1
1141,2019-6-19,2019,6,19,6,c283n5,is Machine Leaning helpful for Software Development ?,https://www.reddit.com/r/MachineLearning/comments/c283n5/is_machine_leaning_helpful_for_software/,taherooo,1560893077,[removed],0,1
1142,2019-6-19,2019,6,19,6,c28742,[D] Should my dataset be balanced if the distribution in the real world is imbalanced?,https://www.reddit.com/r/MachineLearning/comments/c28742/d_should_my_dataset_be_balanced_if_the/,jambery,1560893558,"Say I am predicting smokers in my dataset, and from prior knowledge I know that 15% of adults in the U.S. are smokers. My end goal is to deploy my model into a database that has information on 200+ million adults in the U.S to find potential smokers for a marketing campaign.

For my modeling data, should I purposefully mimic the ""distribution"" of smokers in the U.S. and have 15% of the data be smokers, and 85% of the data be non-smokers? Most of my coworkers have said ""it's easier to just balance them"" but I believe this model would not be generalizable to the entire U.S. population if I keep it balanced.",8,3
1143,2019-6-19,2019,6,19,6,c289a6,Predicting wealth of banking customers,https://www.reddit.com/r/MachineLearning/comments/c289a6/predicting_wealth_of_banking_customers/,scac1041,1560893852,"Hi all!  


Are there studies on predicting the wealth of banking customers, such as   
1. those that do not hold a bank account with us,  
2. those that do but for some reason have other bank accounts too  


Share of wallet comes to mind for credit cards, but this is more on (current) accounts.",0,1
1144,2019-6-19,2019,6,19,6,c289sk,Predicting always negative target. Need Reference,https://www.reddit.com/r/MachineLearning/comments/c289sk/predicting_always_negative_target_need_reference/,turiya2,1560893920,[removed],0,1
1145,2019-6-19,2019,6,19,6,c28han,The Roadmap to Learn Generative Adversarial Networks (GANs),https://www.reddit.com/r/MachineLearning/comments/c28han/the_roadmap_to_learn_generative_adversarial/,iramirsina,1560894959,[removed],0,1
1146,2019-6-19,2019,6,19,7,c28lz2,Understand the Powerful Supervised Machine Learning in a Simple Way,https://www.reddit.com/r/MachineLearning/comments/c28lz2/understand_the_powerful_supervised_machine/,iramirsina,1560895588,[removed],0,1
1147,2019-6-19,2019,6,19,7,c28oxu,Application of Gradient Boosting in Order Book Modeling,https://www.reddit.com/r/MachineLearning/comments/c28oxu/application_of_gradient_boosting_in_order_book/,lamres,1560896008,,0,1
1148,2019-6-19,2019,6,19,7,c291tz,Deep Learning Short Course Offer,https://www.reddit.com/r/MachineLearning/comments/c291tz/deep_learning_short_course_offer/,mpcrlab,1560897924,[removed],0,1
1149,2019-6-19,2019,6,19,7,c292jl,[D] What do you do to optimize your work?,https://www.reddit.com/r/MachineLearning/comments/c292jl/d_what_do_you_do_to_optimize_your_work/,bergholma,1560898021,"We all use Jupyter notebooks, Pytorch/Tensorflow/etc.

What libraries, plugins, IDE's or other things do you use that others could find useful?",4,0
1150,2019-6-19,2019,6,19,9,c2accr,[R] Superposition of many models into one,https://www.reddit.com/r/MachineLearning/comments/c2accr/r_superposition_of_many_models_into_one/,hardmaru,1560905284,,9,48
1151,2019-6-19,2019,6,19,9,c2agk2,"Reaction to Siraj Raval's ""How to Build a Healthcare Startup"" -- will it pay?",https://www.reddit.com/r/MachineLearning/comments/c2agk2/reaction_to_siraj_ravals_how_to_build_a/,toxa26,1560905950,[removed],0,1
1152,2019-6-19,2019,6,19,10,c2alca,An overview of Adversarial Attacks and Defense at CVPR 2019,https://www.reddit.com/r/MachineLearning/comments/c2alca/an_overview_of_adversarial_attacks_and_defense_at/,bardofcodes,1560906697,[removed],0,1
1153,2019-6-19,2019,6,19,10,c2amff,Needs some resources on writing a grammar checker using machine learning.,https://www.reddit.com/r/MachineLearning/comments/c2amff/needs_some_resources_on_writing_a_grammar_checker/,ca3games,1560906888,[removed],0,1
1154,2019-6-19,2019,6,19,11,c2b4tx,How to build a user profile in a document recommender system?,https://www.reddit.com/r/MachineLearning/comments/c2b4tx/how_to_build_a_user_profile_in_a_document/,czechrepublic,1560909923,[removed],0,1
1155,2019-6-19,2019,6,19,11,c2b4yi,[D] Going to a Weeklong AI/ML Summer School,https://www.reddit.com/r/MachineLearning/comments/c2b4yi/d_going_to_a_weeklong_aiml_summer_school/,patronus816,1560909946,"Good day! This is my first time going to such an event; and I was wondering if there were any event veterans here who could offer any advice as to how can I maximize learning in the said event.   


P.S. This is the  Singapore AI School :  [https://aisummerschool.aisingapore.org/](https://aisummerschool.aisingapore.org/)",5,6
1156,2019-6-19,2019,6,19,12,c2bool,Multi-Label Binary Classification with MANY zeros: What's the state of research?,https://www.reddit.com/r/MachineLearning/comments/c2bool/multilabel_binary_classification_with_many_zeros/,sahebqaran,1560913241,"Hi there,

&amp;#x200B;

I didn't know whether to post this in Learn Machine Learning or on here, but basically, I've been looking at papers all day, and nothing I've found so far seems to be able to solve the problem of massively imbalanced datasets for image classification. The one and only thing I found was this paper on an algorithm called CoCoa, but it had only been tested for image classification on 6 classes.

How do you deal with having label sets where, the model can predict all zeros and still get a 99% accuracy?",0,1
1157,2019-6-19,2019,6,19,12,c2bqjw,Investing in stocks benefiting from Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/c2bqjw/investing_in_stocks_benefiting_from_machine/,lovejazz17,1560913568,"I did not see covered in another post.  I am interested in this communities thoughts on investing in QTUM ETF which tracks BlueStar Quantum Computing and Machine Learning Index.  Fairly small ETF given the potential of this technology.  If another sub more appropriate, please let me know.",0,1
1158,2019-6-19,2019,6,19,13,c2ccbo,You Need to Know Edge Relaxation for Shortest Paths Problem with Python,https://www.reddit.com/r/MachineLearning/comments/c2ccbo/you_need_to_know_edge_relaxation_for_shortest/,yasufumy,1560917382,,0,1
1159,2019-6-19,2019,6,19,13,c2cpv0,Is there such a thing as sexual reproduction (like in genetic algorithms) for machine learning,https://www.reddit.com/r/MachineLearning/comments/c2cpv0/is_there_such_a_thing_as_sexual_reproduction_like/,Laminationman,1560919869,[removed],0,1
1160,2019-6-19,2019,6,19,14,c2d2wh,Any ML model or algorithm for very small sample sizes?,https://www.reddit.com/r/MachineLearning/comments/c2d2wh/any_ml_model_or_algorithm_for_very_small_sample/,coolheaded_dev,1560922484,[removed],1,1
1161,2019-6-19,2019,6,19,14,c2d8ok,"Two artificial neural networks with same environment, hyper parameters and training samples, will they develop the same models?",https://www.reddit.com/r/MachineLearning/comments/c2d8ok/two_artificial_neural_networks_with_same/,bankapalli,1560923749,[removed],0,1
1162,2019-6-19,2019,6,19,15,c2dik0,Doubt regarding mean normalisation?,https://www.reddit.com/r/MachineLearning/comments/c2dik0/doubt_regarding_mean_normalisation/,shreyash77,1560925394,Why are we fine with theta getting effected by mean normalisation? If theta changes output also changes right?,0,1
1163,2019-6-19,2019,6,19,16,c2dt94,Machine Learning In logistic sector,https://www.reddit.com/r/MachineLearning/comments/c2dt94/machine_learning_in_logistic_sector/,skywalkeriit,1560927631,[removed],0,1
1164,2019-6-19,2019,6,19,16,c2dxj0,"NodeJS  The Complete Guide (incl. MVC, REST APIs, GraphQL) - Free Education Site",https://www.reddit.com/r/MachineLearning/comments/c2dxj0/nodejs_the_complete_guide_incl_mvc_rest_apis/,jackijosh,1560928496,,0,1
1165,2019-6-19,2019,6,19,16,c2e1ec,Mask Based Unsupervised Content Transfer,https://www.reddit.com/r/MachineLearning/comments/c2e1ec/mask_based_unsupervised_content_transfer/,GrepIt6,1560929326,[removed],0,1
1166,2019-6-19,2019,6,19,17,c2edl3,Multi GPU Configuration for Deep Learning Rig,https://www.reddit.com/r/MachineLearning/comments/c2edl3/multi_gpu_configuration_for_deep_learning_rig/,CSGOvelocity,1560932100,[removed],0,1
1167,2019-6-19,2019,6,19,17,c2eeab,Is 'Third Wave' AI possible?,https://www.reddit.com/r/MachineLearning/comments/c2eeab/is_third_wave_ai_possible/,TyeKon,1560932246,,0,1
1168,2019-6-19,2019,6,19,17,c2eg7t,[Book] Recommendation engine in python book recommendation,https://www.reddit.com/r/MachineLearning/comments/c2eg7t/book_recommendation_engine_in_python_book/,hisham_elamir,1560932696,"I am searching for nice books in recommendation engines(technical and in python), and currently found both of these:

*  [recommendation systems in python](https://leanpub.com/recommendation_engine/).
* [practical recommender systems](https://www.manning.com/books/practical-recommender-systems).

&amp;#x200B;

could you recommend more???",0,1
1169,2019-6-19,2019,6,19,17,c2eid0,Data Preprocessing,https://www.reddit.com/r/MachineLearning/comments/c2eid0/data_preprocessing/,AmbitiousOkra,1560933206,[removed],0,1
1170,2019-6-19,2019,6,19,17,c2ejyz,An open-source app to annotate online social network documents to be used in opinion mining,https://www.reddit.com/r/MachineLearning/comments/c2ejyz/an_opensource_app_to_annotate_online_social/,luisgasco,1560933593,,0,1
1171,2019-6-19,2019,6,19,18,c2epyq,Data Science in the Post Hadoop Era,https://www.reddit.com/r/MachineLearning/comments/c2epyq/data_science_in_the_post_hadoop_era/,IguazioDani,1560934985,,0,1
1172,2019-6-19,2019,6,19,18,c2esa2,Predict context words using center word,https://www.reddit.com/r/MachineLearning/comments/c2esa2/predict_context_words_using_center_word/,Wats0ns,1560935506,"Hello,

I can't seem to find a solution to this: I need to predict context words around a center word. To do this, I have trained the word2vec from gensim with the skip-gram model, but there is a predict\_output\_word from context method, but not a predict\_context from word method. I can't seem to find a way to develop this, does anyone knows how to do this from gensim word2vec ?

Thanks a lot",0,1
1173,2019-6-19,2019,6,19,18,c2esq2,What happened here? (NeurIPS 2018),https://www.reddit.com/r/MachineLearning/comments/c2esq2/what_happened_here_neurips_2018/,yusuf_bengio,1560935605,[removed],0,1
1174,2019-6-19,2019,6,19,18,c2eyju,Phd help,https://www.reddit.com/r/MachineLearning/comments/c2eyju/phd_help/,easysynopsis,1560936879,[removed],0,1
1175,2019-6-19,2019,6,19,18,c2f2yd,Artificial Intelligence: Mankind's Last Invention - what we know,https://www.reddit.com/r/MachineLearning/comments/c2f2yd/artificial_intelligence_mankinds_last_invention/,sherozekhan,1560937882,,0,1
1176,2019-6-19,2019,6,19,19,c2f6ar,Go Machine Learning Projects and predictive analytics applications in Go,https://www.reddit.com/r/MachineLearning/comments/c2f6ar/go_machine_learning_projects_and_predictive/,mritraloi6789,1560938573,,0,1
1177,2019-6-19,2019,6,19,19,c2f8bx,Interviewing for Google AI Residency: My ML Journey,https://www.reddit.com/r/MachineLearning/comments/c2f8bx/interviewing_for_google_ai_residency_my_ml_journey/,init__27,1560939023,[removed],0,1
1178,2019-6-19,2019,6,19,19,c2f8ef,Self Starter Way to Learn Machine Learning,https://www.reddit.com/r/MachineLearning/comments/c2f8ef/self_starter_way_to_learn_machine_learning/,boredcodeeva,1560939036,[removed],0,1
1179,2019-6-19,2019,6,19,19,c2fccd,How to add extra features after creating word vectors?,https://www.reddit.com/r/MachineLearning/comments/c2fccd/how_to_add_extra_features_after_creating_word/,HuntV1,1560939864,[removed],0,1
1180,2019-6-19,2019,6,19,19,c2fdd5,image processing-help,https://www.reddit.com/r/MachineLearning/comments/c2fdd5/image_processinghelp/,blueh8t,1560940070,[removed],0,1
1181,2019-6-19,2019,6,19,19,c2feee,[P] A serverless computing cloud with zero backend cost,https://www.reddit.com/r/MachineLearning/comments/c2feee/p_a_serverless_computing_cloud_with_zero_backend/,johnkim1010,1560940267,"Hey everyone,

It's John from Common Computer, a tech startup building serverless cloud platform best supporting AI scale solutions that helps developers deploy their machine learning codes with zero cost. We believe, just like YouTube or any other content platforms, codes should be deployed without worrying its backend cost to make the innovation, especially for the AI. Zero cost backend is possible by sharing risk as well as revenue with computing resource provider, which will be our company during the initial phase but will be open to everyone who has computing resources.

If you're interested, visit our [website](https://www.ainize.ai/) and apply to get an invitation.  Also, we'd love to hear your feedback about our product/idea!",3,0
1182,2019-6-19,2019,6,19,19,c2fj6e,Machine Learning | Complete Project In Credit Card Fraud Detection,https://www.reddit.com/r/MachineLearning/comments/c2fj6e/machine_learning_complete_project_in_credit_card/,Aisha_b,1560941247,,0,1
1183,2019-6-19,2019,6,19,19,c2fjbs,Learning or Citizen Data Scientists,https://www.reddit.com/r/MachineLearning/comments/c2fjbs/learning_or_citizen_data_scientists/,ElegantMicroWebIndia,1560941279,,0,1
1184,2019-6-19,2019,6,19,19,c2fkmu,Import just essentials packages to kickstart new DL project,https://www.reddit.com/r/MachineLearning/comments/c2fkmu/import_just_essentials_packages_to_kickstart_new/,dattran2346,1560941559,[removed],0,1
1185,2019-6-19,2019,6,19,20,c2fosz,[Research]A Quick Easy Guide to Deep Learning with Java  Deeplearaning4j / DL4J,https://www.reddit.com/r/MachineLearning/comments/c2fosz/researcha_quick_easy_guide_to_deep_learning_with/,Shilpa_Opencodez,1560942385,,0,1
1186,2019-6-19,2019,6,19,20,c2fz27,How to setup devops for machine learning?,https://www.reddit.com/r/MachineLearning/comments/c2fz27/how_to_setup_devops_for_machine_learning/,Zerotool1,1560944534,[removed],0,1
1187,2019-6-19,2019,6,19,20,c2g2dw,How could i regard y variables as ordinal or nominal values in machine learning?,https://www.reddit.com/r/MachineLearning/comments/c2g2dw/how_could_i_regard_y_variables_as_ordinal_or/,hyperdx,1560945386,[removed],0,1
1188,2019-6-19,2019,6,19,20,c2g2li,[N] There are many platforms to manage your ML models and experiments. We just open sourced ours.,https://www.reddit.com/r/MachineLearning/comments/c2g2li/n_there_are_many_platforms_to_manage_your_ml/,LSTMeow,1560945436,"Hi Everyone,

My Team just released a very cool open source tool for ML! 

This one is completely free and open source.  Since i'm not on the marketing team I am probably doing this all wrong, but  I really think the greater community could benefit from using **trains**, and want you to be the cool kids that knew about it before everyone else... ^((isn't this why we all joined reddit so many years ago?))

&amp;#x200B;

Q: Why should I click this link?

A: Because you only need to add two lines of code to your train script and you get full tracking of metrics, hyperparameters, model and git commit.

&amp;#x200B;

Anyways,

I think I have done enough damage.

Learn more, try our live demo, fork us on github! 

[https://github.com/allegroai/trains](https://github.com/allegroai/trains)",65,239
1189,2019-6-19,2019,6,19,21,c2g5hh,Artificial Intelligence for Healthcare,https://www.reddit.com/r/MachineLearning/comments/c2g5hh/artificial_intelligence_for_healthcare/,InAccer,1560946094,[removed],0,1
1190,2019-6-19,2019,6,19,21,c2gp8d,AIWAYS Previews Intelligent AI Cockpit Tech at CES Asia 2019,https://www.reddit.com/r/MachineLearning/comments/c2gp8d/aiways_previews_intelligent_ai_cockpit_tech_at/,S_paddy,1560948698,,0,1
1191,2019-6-19,2019,6,19,22,c2gve4,Telescopic Forks|Single Depth Telescopic Forks - LHD S.p.A,https://www.reddit.com/r/MachineLearning/comments/c2gve4/telescopic_forkssingle_depth_telescopic_forks_lhd/,lhd121,1560949618,[removed],0,1
1192,2019-6-19,2019,6,19,22,c2gvg8,Improving Business Communications and Human Interactions with NLP,https://www.reddit.com/r/MachineLearning/comments/c2gvg8/improving_business_communications_and_human/,Verma_RJ,1560949629,,0,1
1193,2019-6-19,2019,6,19,22,c2h11m,Uber to Test Drone Delivery Soon | IT News,https://www.reddit.com/r/MachineLearning/comments/c2h11m/uber_to_test_drone_delivery_soon_it_news/,S_paddy,1560950527,,0,1
1194,2019-6-19,2019,6,19,22,c2h5l4,Online learning for multinomial likelihood,https://www.reddit.com/r/MachineLearning/comments/c2h5l4/online_learning_for_multinomial_likelihood/,fori1to10,1560951234,[removed],0,1
1195,2019-6-19,2019,6,19,22,c2h7ur,Interviewing for Google AI Residency: My ML Journey,https://www.reddit.com/r/MachineLearning/comments/c2h7ur/interviewing_for_google_ai_residency_my_ml_journey/,init__27,1560951589,[removed],0,1
1196,2019-6-19,2019,6,19,22,c2h9sm,Need to predict revenue on promo month?,https://www.reddit.com/r/MachineLearning/comments/c2h9sm/need_to_predict_revenue_on_promo_month/,bekterra13,1560951891,"Hi folks. I need to Predict sales or revenue on promo month(prices are changed for items on promo month). I have 1 year(Jan-2011 - Dec-2011) data with 2 item columns with count, date, price. I need to feed test dataset with changed price for items and  I need to predict revenue on promo month(Jan-2012)?

Any ideas?",0,1
1197,2019-6-19,2019,6,19,22,c2heci,Machine Learning with C++ - Classification with Shogun library,https://www.reddit.com/r/MachineLearning/comments/c2heci/machine_learning_with_c_classification_with/,andrea_manero,1560952593,[removed],0,1
1198,2019-6-19,2019,6,19,23,c2hhvw,[R] Mask Based Unsupervised Content Transfer (PyTorch code and summary in comments),https://www.reddit.com/r/MachineLearning/comments/c2hhvw/r_mask_based_unsupervised_content_transfer/,RonMokady,1560953116,[removed],1,1
1199,2019-6-19,2019,6,19,23,c2hsjr,[R] Mask Based Unsupervised Content Transfer,https://www.reddit.com/r/MachineLearning/comments/c2hsjr/r_mask_based_unsupervised_content_transfer/,RonMokady,1560954693,"[https://arxiv.org/abs/1906.06558](https://arxiv.org/abs/1906.06558)

Hi All, Author here -

Given two domains where one contains some additional information compared to the other, our method disentangles the common and the seperate parts and transfers the seperate information from one image to another using a mask, while not using any supervision at train time. For example, we can transfer the specific facial hair from an image of a men with a mustache to an image of a shaved person. Using a mask enables state-of-the-art quality (seeexample [here](http://github.com/rmokady/mbu-content-tansfer/blob/master/images/mustache_grid.png)), but also, the generated mask can be used as a semantic segmentation of the seperate part. Thus our method perform weakly-supervised semantic segmentation, using only class lables as supervision, achieving state-of-the-art performance, see example [here](http://github.com/rmokady/mbu-content-tansfer/blob/master/images/mu_gl_segmentation%20(1).png).

In short, our architecture consist of two encoders, two decoders and discriminator. One encoder for encoding the common part and one to encode the separate part. The discriminator used to disentangle the encoding to the separateand common parts correctly. In training, One decoder used to decode only the common part, and the second decoder decodes only the separate part using a mask. In inference, we use only the second decoder which given the relevant encoding, adds the specific content to a new image. We also use novel regularization scheme to encourage to mask to be minimal.

Refer to the full paper for more details.

PyTorch implementation is on[GitHub](http://github.com/rmokady/mbu-content-tansfer).

&amp;#x200B;

Feel free to ask questions.",3,10
1200,2019-6-20,2019,6,20,0,c2ifgy,"MNIST Reborn, Restored and Expanded: Additional 50K Training Samples",https://www.reddit.com/r/MachineLearning/comments/c2ifgy/mnist_reborn_restored_and_expanded_additional_50k/,Yuqing7,1560957917,,0,2
1201,2019-6-20,2019,6,20,0,c2irmp,"Simple Questions Thread June 19, 2019",https://www.reddit.com/r/MachineLearning/comments/c2irmp/simple_questions_thread_june_19_2019/,AutoModerator,1560959604,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",0,1
1202,2019-6-20,2019,6,20,0,c2ite3,Graph convolutional networks,https://www.reddit.com/r/MachineLearning/comments/c2ite3/graph_convolutional_networks/,Wso333,1560959848,"Does anyone have a good explanation of how graph convolutional networks work? I looked online but it didn't really help much. I understand regular convolutional networks perfectly fine, but I don't understand how the graph networks work. 
From a few sites I could find, it seemed like they just had every node sum it's neighbors features and then pass that on to the next layer of the network. But this seems too simplistic, there doesn't seem to be any real convolution happening. 
If anyone understands these really well, I'd really appreciate the help. Or if anyone knows a good intuitive guide or basic working example I can try to look at. 
Thanks!",0,1
1203,2019-6-20,2019,6,20,1,c2iyop,Artificial intelligence is making its way to a new industry: automotive. AI (in the form of virtual service advisors) brings the next level of service to auto dealerships and repair shops and gives customers something they didn't expect.,https://www.reddit.com/r/MachineLearning/comments/c2iyop/artificial_intelligence_is_making_its_way_to_a/,openbay,1560960578,,0,1
1204,2019-6-20,2019,6,20,1,c2j4tp,How useful is Apache Zookeeper in Machine Learning and AI?,https://www.reddit.com/r/MachineLearning/comments/c2j4tp/how_useful_is_apache_zookeeper_in_machine/,vigbig,1560961423,[removed],0,1
1205,2019-6-20,2019,6,20,1,c2j5p6,Machine Learning Modules,https://www.reddit.com/r/MachineLearning/comments/c2j5p6/machine_learning_modules/,Rugani_95,1560961543,[removed],0,1
1206,2019-6-20,2019,6,20,1,c2j7qx,Transform Women in AI Awards,https://www.reddit.com/r/MachineLearning/comments/c2j7qx/transform_women_in_ai_awards/,npatrici,1560961826,,0,1
1207,2019-6-20,2019,6,20,1,c2jaod,"Weight Agnostic Neural Networks Inspired by precocial species in biology, we set out to search for neural net architectures that can already (sort of) perform various tasks even when they use random weight values.",https://www.reddit.com/r/MachineLearning/comments/c2jaod/weight_agnostic_neural_networks_inspired_by/,saadmrb,1560962217,,0,1
1208,2019-6-20,2019,6,20,1,c2jc04,[R] You can jam with ML models without having to train a thing!,https://www.reddit.com/r/MachineLearning/comments/c2jc04/r_you_can_jam_with_ml_models_without_having_to/,saadmrb,1560962395,,0,1
1209,2019-6-20,2019,6,20,2,c2jstj,AI Reading List (from Machine Learning for Humans),https://www.reddit.com/r/MachineLearning/comments/c2jstj/ai_reading_list_from_machine_learning_for_humans/,i_am_squishy,1560964634,,0,1
1210,2019-6-20,2019,6,20,2,c2jtey,[P] AI Reading List (Machine Learning for Humans),https://www.reddit.com/r/MachineLearning/comments/c2jtey/p_ai_reading_list_machine_learning_for_humans/,i_am_squishy,1560964712,,0,1
1211,2019-6-20,2019,6,20,2,c2jvh8,PHP for Beginners  Become a PHP Master  CMS Project - Free Education Site,https://www.reddit.com/r/MachineLearning/comments/c2jvh8/php_for_beginners_become_a_php_master_cms_project/,jackijosh,1560964989,,0,1
1212,2019-6-20,2019,6,20,2,c2k2lg,Adobes AI Is Capable of Learning Painting Styles and Reproducing Arts in under a Minute,https://www.reddit.com/r/MachineLearning/comments/c2k2lg/adobes_ai_is_capable_of_learning_painting_styles/,azmodeus99,1560965904,,0,1
1213,2019-6-20,2019,6,20,2,c2k4sf,TESLA and Microsoft?,https://www.reddit.com/r/MachineLearning/comments/c2k4sf/tesla_and_microsoft/,AIforEarth,1560966190,[removed],0,1
1214,2019-6-20,2019,6,20,2,c2k5v7,Advanced JavaScript Concepts - Free Education Site,https://www.reddit.com/r/MachineLearning/comments/c2k5v7/advanced_javascript_concepts_free_education_site/,jackijosh,1560966331,,0,1
1215,2019-6-20,2019,6,20,2,c2kat2,Are there word2vec models that are pre-trained on code?,https://www.reddit.com/r/MachineLearning/comments/c2kat2/are_there_word2vec_models_that_are_pretrained_on/,sorokine,1560966970,[removed],0,1
1216,2019-6-20,2019,6,20,3,c2keay,Automated Machine Learning User Interface  How Can We Simplify and Accelerate AI?,https://www.reddit.com/r/MachineLearning/comments/c2keay/automated_machine_learning_user_interface_how_can/,brunocborges,1560967420,,0,1
1217,2019-6-20,2019,6,20,3,c2kw1f,Best place for training deep RL notebooks?,https://www.reddit.com/r/MachineLearning/comments/c2kw1f/best_place_for_training_deep_rl_notebooks/,thinking_computer,1560969750,[removed],0,1
1218,2019-6-20,2019,6,20,3,c2kz16,The Web Developer Bootcamp - Free Education Site,https://www.reddit.com/r/MachineLearning/comments/c2kz16/the_web_developer_bootcamp_free_education_site/,jackijosh,1560970149,,0,1
1219,2019-6-20,2019,6,20,3,c2l0x2,Machine Learning Engineers...,https://www.reddit.com/r/MachineLearning/comments/c2l0x2/machine_learning_engineers/,EOHFA,1560970395,"What route did you guys take, from the beginning college to your career now? I am a rising junior in high school who is interested in Machine Learning. Just curious on yhe routes that you guys took. Thanks!",0,1
1220,2019-6-20,2019,6,20,3,c2l2z9,Join us to implement machine learning papers,https://www.reddit.com/r/MachineLearning/comments/c2l2z9/join_us_to_implement_machine_learning_papers/,y05r1,1560970664,[removed],0,1
1221,2019-6-20,2019,6,20,4,c2l41d,Relational reasoning tasks,https://www.reddit.com/r/MachineLearning/comments/c2l41d/relational_reasoning_tasks/,lollocat3,1560970802,"I have recently read this paper on Arxiv about recurrent relational networks used to solve relational reasoning tasks: 


https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;url=https://arxiv.org/abs/1711.08028&amp;ved=2ahUKEwicnJTjmvbiAhVKY1AKHRTKBmIQFjAAegQIBxAB&amp;usg=AOvVaw2z89rvMn-uSXBe8BArUaqp&amp;cshid=1560970177794

In this paper, the author evaluates the network on the Babi and CLEVR dataset and on Sudokus with 17 givens. I was wondering if any of you knew of other relatively complex relational reasoning tasks on which I could test the architecture on.
Thanks in advance",0,1
1222,2019-6-20,2019,6,20,4,c2l6s3,Off-Policy Classification - A New Reinforcement Learning Model Selection Method,https://www.reddit.com/r/MachineLearning/comments/c2l6s3/offpolicy_classification_a_new_reinforcement/,sjoerdapp,1560971159,,0,1
1223,2019-6-20,2019,6,20,4,c2llpm,"If Tom Brings Jerry Home, ML Locks the Cat Door",https://www.reddit.com/r/MachineLearning/comments/c2llpm/if_tom_brings_jerry_home_ml_locks_the_cat_door/,Yuqing7,1560973151,,0,1
1224,2019-6-20,2019,6,20,5,c2lveq,Siraj Raval  a Man Ahead of His Time,https://www.reddit.com/r/MachineLearning/comments/c2lveq/siraj_raval_a_man_ahead_of_his_time/,stensool,1560974423,[removed],0,1
1225,2019-6-20,2019,6,20,5,c2lxwi,[Discussion] Siraj Raval  a Man Ahead of His Time,https://www.reddit.com/r/MachineLearning/comments/c2lxwi/discussion_siraj_raval_a_man_ahead_of_his_time/,stensool,1560974719,"[https://medium.com/@stensootla/siraj-raval-a-man-ahead-of-his-time-e756778c4240](https://medium.com/@stensootla/siraj-raval-a-man-ahead-of-his-time-e756778c4240)

&amp;#x200B;

I recently watched Lex Fridman's excellent podcast with Siraj Raval, and was completely taken aback by it. I used to be a mild Siraj hater, thinking that his superficial educational videos, not to even mention the raps, are misleading and completely unnecessary. However, the podcast made me see him, and his mission, in an entirely new light. I now believe him to be one of the most inspiring persons in the AI community I've had a chance to listen to.

As an homage of sorts, I put together a longer-form article, explaining his worthwhile mission to people who don't have the time to listen to the hour-long podcast.",0,1
1226,2019-6-20,2019,6,20,5,c2lyj5,[D] Siraj Raval  a Man Ahead of His Time,https://www.reddit.com/r/MachineLearning/comments/c2lyj5/d_siraj_raval_a_man_ahead_of_his_time/,stensool,1560974809,"[https://medium.com/@stensootla/siraj-raval-a-man-ahead-of-his-time-e756778c4240](https://medium.com/@stensootla/siraj-raval-a-man-ahead-of-his-time-e756778c4240)

I recently watched Lex Fridman's excellent podcast with Siraj Raval, and was completely taken aback by it. I used to be a mild Siraj hater, thinking that his superficial educational videos, not to even mention the raps, are misleading and completely unnecessary. However, the podcast made me see him, and his mission, in an entirely new light. I now believe him to be one of the most inspiring persons in the AI community I've had a chance to listen to.

As an homage of sorts, I put together a longer-form article, explaining his worthwhile mission to people who don't have the time to listen to the hour-long podcast.",15,0
1227,2019-6-20,2019,6,20,6,c2ms3j,Where do I start coding a Text to Speech platform for a less known language?,https://www.reddit.com/r/MachineLearning/comments/c2ms3j/where_do_i_start_coding_a_text_to_speech_platform/,utccuptcttptupdtd,1560978802,[removed],0,1
1228,2019-6-20,2019,6,20,6,c2n2wq,[R] code2seq: Generating Sequences from Structured Representations of Code,https://www.reddit.com/r/MachineLearning/comments/c2n2wq/r_code2seq_generating_sequences_from_structured/,basiliskgf,1560980256,,0,1
1229,2019-6-20,2019,6,20,6,c2n7vh,AI analyzes language to predict schizophrenia,https://www.reddit.com/r/MachineLearning/comments/c2n7vh/ai_analyzes_language_to_predict_schizophrenia/,finphil,1560980922,[removed],0,1
1230,2019-6-20,2019,6,20,6,c2nafn,Alternative to hierarchical clustering on defining the classification hierarchy,https://www.reddit.com/r/MachineLearning/comments/c2nafn/alternative_to_hierarchical_clustering_on/,shogw0w,1560981261,[removed],0,2
1231,2019-6-20,2019,6,20,7,c2njas,[D] What side projects could help an undergraduate stand out when applying to graduate programs in ML?,https://www.reddit.com/r/MachineLearning/comments/c2njas/d_what_side_projects_could_help_an_undergraduate/,searchingundergrad,1560982463,"I have implemented two research papers and am trying to get more involved in research over the next year (3rd year) at my university, but I'd like to push it to compensate for some unfortunate grades? What are some side projects that could really help? Contribute to a DL framework like PyTorch or Tensorflow? Implement/replicate a lot more research papers? Thanks!",21,24
1232,2019-6-20,2019,6,20,7,c2nqt7,Improved Microsoft MT-DNN Tops GLUE Rankings,https://www.reddit.com/r/MachineLearning/comments/c2nqt7/improved_microsoft_mtdnn_tops_glue_rankings/,Yuqing7,1560983504,,0,1
1233,2019-6-20,2019,6,20,7,c2nr27,Should I do Machine Learning or Software Engineering?,https://www.reddit.com/r/MachineLearning/comments/c2nr27/should_i_do_machine_learning_or_software/,dirtymikethelegend,1560983541,[removed],0,1
1234,2019-6-20,2019,6,20,7,c2nz2i,Confidence levels,https://www.reddit.com/r/MachineLearning/comments/c2nz2i/confidence_levels/,yellowcrayon7,1560984693,"hi there,

I am working on cifar 10 dataset and have created a model to classify and plotted confusion matrix as shown in the attached figure,

I am new to this and want to know, how can I get a confidence level for each class prediction?

so for confidence matrix shown in figure, i want to know what is the confidence for predicting that airplane is ship when out of 1000, 26 are labeled as ship falsely, i want to know what was the confidence level for the false labeling

https://i.redd.it/i9az9e997e531.png",0,1
1235,2019-6-20,2019,6,20,7,c2nzqc,The Options Lab,https://www.reddit.com/r/MachineLearning/comments/c2nzqc/the_options_lab/,samjjohnson1972,1560984789,,0,1
1236,2019-6-20,2019,6,20,8,c2o84y,Modern GAN-based drawing assistance projects/papers?,https://www.reddit.com/r/MachineLearning/comments/c2o84y/modern_ganbased_drawing_assistance_projectspapers/,thewhitelynx,1560985996,[removed],0,1
1237,2019-6-20,2019,6,20,8,c2ocyb,[R] Model Compression by Entropy Penalized Reparameterization,https://www.reddit.com/r/MachineLearning/comments/c2ocyb/r_model_compression_by_entropy_penalized/,hardmaru,1560986735,,4,9
1238,2019-6-20,2019,6,20,10,c2pfgb,[D] How can you do great AI research when you don't have access to google-scale compute? By being weird.  @togelius,https://www.reddit.com/r/MachineLearning/comments/c2pfgb/d_how_can_you_do_great_ai_research_when_you_dont/,baylearn,1560992765,"*Just ran into this this interesting thread by [Julian Togelius](https://en.wikipedia.org/wiki/Julian_Togelius), who authored several papers books in the area of AI in games and procedural generation.*

[unrolled summary](https://threadreaderapp.com/thread/1088679404937625600.html):

[How can you do great AI research when you don't have access to google-scale compute?](https://twitter.com/togelius/status/1088679404937625600) By being weird. The big tech companies are obsessed with staying nimble despite being big, and some succeed to some extent. But they can't afford to be as weird as a lone looney professor.
A lone professor with a handful of students and a few computers can never win over DeepMind or FAIR in a straight competition. But we can afford to try methods that make absolutely no sense, or attack problems that nobody wants to solve as they don't look like problems.

To the extent I've done anything useful or worthwhile in my career, it's always been through trying to solve a problem nobody thought of, or trying a method that shouldn't work. Very often the useful/publishable end result was nothing like what I thought I was working towards.

So go on, be weird. Out-weird the giants. Even if they're both nimble and powerful, they cannot be as stupid and ridiculous as you. Because how would that look? To managers, investors, board members, the general public? You can afford to completely disregard such entities.

Now, I'm not saying that there's no value in throwing giant compute resources at some problem, and trying to break a long-standing benchmark. That's all good, I'm happy that there are people that do those things. But I'm happy that I don't have to do it. Because it's a bit boring

And of course the advantage of the big tech companies is not only in having many GPUs. It's also in having large teams of highly competent people working on the project non-stop without having to e.g. teach or go to faculty meetings. Still, you can do it.
Many of the best ideas still come from academia, even though the best results don't.",76,387
1239,2019-6-20,2019,6,20,11,c2q0l7,[R] OgmaNeo2 Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/c2q0l7/r_ogmaneo2_reinforcement_learning/,CireNeikual,1560996105,"**Link to the blog post:** [https://ogma.ai/2019/06/ogmaneo2-and-reinforcement-learning/](https://ogma.ai/2019/06/ogmaneo2-and-reinforcement-learning/)

Hey all,

We have finally figured out a good way of integrating reinforcement learning into our biologically plausible, fully online/incremental learning system OgmaNeo2 (implementing Sparse Predictive Hierarchies, SPH). Here we provide a few demos and some high level description, as well as links to learn more.

Included among the demos is a real-world mini-sumo robot fight, where the agents are implemented using our system. The game proceeds in episodes and automatically resets itself.

&amp;#x200B;

For those who are wondering what SPH is, we included a link to a more in-depth presentation in the blog post. Here is a quick summary though:

SPH is a fully online/incremental lifelong learning system that does not use backpropagation, and is biologically plausible. It is also extremely fast, able to run in real-time on platforms such as a Raspberry Pi Zero with learning enabled. It uses a bidirectional hierarchy of very sparse encoder-decoder pairs. It is activated in two passes (although asynchronous implementation is also possible): An up-pass followed by a down-pass. All input/output occurs at the ""bottom"" of the hierarchy. Each encoder-decoder pair forms a layer, and each layer clocks at a slower rate than the layer directly below. We call this ""exponential memory"", as it encodes information into slower and slower timescales going up the hierarchy, and decodes into faster timescales going down, allowing us to bridge exponentially large time lags with respect to the number of layers.

For reinforcement learning, we took out the original decoders which just predicted the next timestep(s) of input and replaced them with a swarm of reinforcement learning agents that all seek to locally maximize the same reward.

Let us know what you think!",2,9
1240,2019-6-20,2019,6,20,11,c2q51o,"[R] XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE) (Google Brain)",https://www.reddit.com/r/MachineLearning/comments/c2q51o/r_xlnet_a_new_pretraining_method_for_nlp_that/,chisai_mikan,1560996819,,1,1
1241,2019-6-20,2019,6,20,11,c2q5k7,"[R] XLNet: a new pretraining method for NLP that significantly improves upon BERT on 20 tasks (e.g., SQuAD, GLUE, RACE)",https://www.reddit.com/r/MachineLearning/comments/c2q5k7/r_xlnet_a_new_pretraining_method_for_nlp_that/,chisai_mikan,1560996906,,57,60
1242,2019-6-20,2019,6,20,11,c2qma7,[P] What are Alpha Zero's Inputs (Chess)?,https://www.reddit.com/r/MachineLearning/comments/c2qma7/p_what_are_alpha_zeros_inputs_chess/,MiddleStress,1560999593,"My current understanding of Alpha Zero from a high level is that it takes in the board state and outputs the probability distribution of results. I am especially confused about the anatomy of the inputs though.

Taking the example of chess, the [DeepMind arxiv paper](https://arxiv.org/pdf/1712.01815.pdf) said that the network had a total of 119 spatial planes. Of these spatial planes four types stick out to me: color, repetitions, total move count, and no-progress count.

Color

The layer feels useless because it only encodes a single number repeated. Couldn't this be optimized out because the information is already encoded within the board structure. The chess board is oriented such that the side AlphaZero is playing is on the bottom and color should be implicit with the layers that the P1 and P2 pieces exist in. Didn't the authors also say P1 is always the player to make the move? Why do we need a color map?

Repetitions

I'm not sure what this is. Does this come from fen representation? Why does this need to be counted per time-step? I believe that past a certain point the repetitions no longer count towards a draw. As well, how are these encoded? Is it just a copy of the relevant positions that would cause a three-fold repetition?

Total Move Count and No-Progress Count

Are these encoded as integers? To my understanding, activation functions perform best near 0. So, a small difference in count would not make a difference. Would these be normalized or will this happen implicitly in the network. Also not sure what are no-progress counts.

I apologize if my questions are a waste of time.",4,1
1243,2019-6-20,2019,6,20,13,c2rbk0,ONNX from python training to model deployment in C# or the browser using client-side JS,https://www.reddit.com/r/MachineLearning/comments/c2rbk0/onnx_from_python_training_to_model_deployment_in/,rmill040,1561003885,[removed],0,1
1244,2019-6-20,2019,6,20,13,c2rp2e,[R] Mila/MS: Unsupervised State Representation Learning in Atari,https://www.reddit.com/r/MachineLearning/comments/c2rp2e/r_milams_unsupervised_state_representation/,runvnc,1561006350,,1,8
1245,2019-6-20,2019,6,20,13,c2rpaa,[R] Any Research on Learning Context for Reinforcement Learning?,https://www.reddit.com/r/MachineLearning/comments/c2rpaa/r_any_research_on_learning_context_for/,ejmejm1,1561006390,"I'm looking to start a new research project on the subject of pre-training models to learn some sort of context. When applied to a new environment the goal is to have the model perform better by leveraging what it knows about the context of the environment (which it can learn from other environments or outside examples).

Example: In many games (like Mario), being reset back to your original position is a bad thing because it usually means you died and had to restart.

&amp;#x200B;

I realize this is a very broad topic and description, but I was wondering if anyone could point me in the direction of any similar research. I'm having trouble finding related research.",1,1
1246,2019-6-20,2019,6,20,14,c2runh,Why not make your customers happy?,https://www.reddit.com/r/MachineLearning/comments/c2runh/why_not_make_your_customers_happy/,getengati,1561007405,[removed],0,1
1247,2019-6-20,2019,6,20,14,c2s33h,[R] Disentangled representation learning,https://www.reddit.com/r/MachineLearning/comments/c2s33h/r_disentangled_representation_learning/,akhandait,1561009017,This is my first research project. I will appreciate any feedback to improve myself. The link to the report - [https://github.com/akhandait/disentangled-representation-learning/blob/master/report.pdf](https://github.com/akhandait/disentangled-representation-learning/blob/master/report.pdf).,8,0
1248,2019-6-20,2019,6,20,14,c2s5jv,Education Sector- 3 ways Machine Learning is Reshaping the Future,https://www.reddit.com/r/MachineLearning/comments/c2s5jv/education_sector_3_ways_machine_learning_is/,OliverSmith6244,1561009479,[removed],0,1
1249,2019-6-20,2019,6,20,14,c2s6lb,LHD S.p.A Is Worldwide Leader in designing and building Telescopic Forks | Telescopic Fork |,https://www.reddit.com/r/MachineLearning/comments/c2s6lb/lhd_spa_is_worldwide_leader_in_designing_and/,lhd121,1561009676,,0,1
1250,2019-6-20,2019,6,20,14,c2s88b,Let's build a neural net using JavaScript  Only in 7 minutes!,https://www.reddit.com/r/MachineLearning/comments/c2s88b/lets_build_a_neural_net_using_javascript_only_in/,liashchynskyi,1561010010,,0,1
1251,2019-6-20,2019,6,20,14,c2s93g,Photo-Geometric Autoencoding to Learn 3D Objects from Unlabelled Images,https://www.reddit.com/r/MachineLearning/comments/c2s93g/photogeometric_autoencoding_to_learn_3d_objects/,elliottwu,1561010178,,1,1
1252,2019-6-20,2019,6,20,15,c2scud,Photo-Geometric Autoencoding to Learn 3D Objects from Unlabelled Images,https://www.reddit.com/r/MachineLearning/comments/c2scud/photogeometric_autoencoding_to_learn_3d_objects/,elliottwu,1561010900,,1,1
1253,2019-6-20,2019,6,20,15,c2shuk,[R] Photo-Geometric Autoencoding to Learn 3D Objects from Unlabelled Images,https://www.reddit.com/r/MachineLearning/comments/c2shuk/r_photogeometric_autoencoding_to_learn_3d_objects/,elliottwu,1561011926,,1,6
1254,2019-6-20,2019,6,20,15,c2spss,[P] A list of annotation tools for building datasets,https://www.reddit.com/r/MachineLearning/comments/c2spss/p_a_list_of_annotation_tools_for_building_datasets/,UpdraftDev,1561013576,"Ive posted about [datasetlist.com](https://www.datasetlist.com) a few months back and received lots of great feedback and ideas.

Id like to share a new addition to the site: a list of annotation tools for building datasets at [datasetlist.com/tools](https://www.datasetlist.com/tools). I hope this saves someone from writing a new tool from scratch. :) Let me know what you think! I'll be keeping the list updated with new tools and features.",1,35
1255,2019-6-20,2019,6,20,16,c2szpa,Embedding Neural Networks into existing Structures,https://www.reddit.com/r/MachineLearning/comments/c2szpa/embedding_neural_networks_into_existing_structures/,sheepsy90,1561015540,[removed],0,1
1256,2019-6-20,2019,6,20,16,c2t025,[News] Monge blunts Bayes: 'Vaccine' for machine learning models developed by CSIRO's Data61,https://www.reddit.com/r/MachineLearning/comments/c2t025/news_monge_blunts_bayes_vaccine_for_machine/,GtothePtotheN,1561015610,,0,1
1257,2019-6-20,2019,6,20,17,c2t97s,[D] What were great talks in ICML 2019?,https://www.reddit.com/r/MachineLearning/comments/c2t97s/d_what_were_great_talks_in_icml_2019/,thisisareallife,1561017617,"In NeurIPS2018, [Michael Levin's keynote](https://www.facebook.com/nipsfoundation/videos/neurips-2018-invited-talk/480969442428608/) was really amazing. What were your favorite talks in ICML 2019 that you'd like to recommend? I'm particularly interested in some less task-specific and more general and inspiring ones.",3,37
1258,2019-6-20,2019,6,20,17,c2t992,Best Resources to learn the math needed for Machine Learning? (Im in 9th grade),https://www.reddit.com/r/MachineLearning/comments/c2t992/best_resources_to_learn_the_math_needed_for/,pietromarse_,1561017624,[removed],0,1
1259,2019-6-20,2019,6,20,17,c2t9mu,[R] What are some new approaches to text-to-speech and what is currently SOTA?,https://www.reddit.com/r/MachineLearning/comments/c2t9mu/r_what_are_some_new_approaches_to_texttospeech/,Craq221,1561017707,I just started researching text2speech and am interested in reading in some new novel approaches and what is considered as SOTA in text2speech. Will be thankful for any insights and recommendations.,1,2
1260,2019-6-20,2019,6,20,17,c2tk3p,A personal assistant built with a Neural Network: Olivia,https://www.reddit.com/r/MachineLearning/comments/c2tk3p/a_personal_assistant_built_with_a_neural_network/,ananagame,1561020163,[removed],0,1
1261,2019-6-20,2019,6,20,17,c2tmnd,[P] A personal assistant built with a Neural Network: Olivia,https://www.reddit.com/r/MachineLearning/comments/c2tmnd/p_a_personal_assistant_built_with_a_neural/,ananagame,1561020789," 

Hello,

Here is Olivia, your new personal assistant and best friend.  
 It is completely  written in Golang with a Neural Network.  
 Enough talking, here is a video showing can Olivia can do [https://www.youtube.com/watch?v=JmJZi9gmKvI](https://www.youtube.com/watch?v=JmJZi9gmKvI)

[https://github.com/olivia-ai/olivia](https://github.com/olivia-ai/olivia)",0,1
1262,2019-6-20,2019,6,20,18,c2u4ac,"[D] In light of Strubell et al (2019) paper on carbon emissions, what can we do as a community",https://www.reddit.com/r/MachineLearning/comments/c2u4ac/d_in_light_of_strubell_et_al_2019_paper_on_carbon/,jamesravey,1561024669,"Hey folks  - long time lurker here. I've been following [Strubell et al's paper](https://arxiv.org/pdf/1906.02243.pdf) which was some pretty decent research showing that complex neural models like BERT and neural architecture search approaches are pretty energy intensive and the mainstream media reaction which seems to have been along the lines of ""ALL AI MODELS EVAR GIVE OFF MORE CO2 THAN A CAR. BAN AI RESEARCH!"" 

I wrote a [blog post]( https://medium.com/filament-ai/how-can-ai-practitioners-reduce-our-carbon-footprint-3c47d1ef275f) proposing some steps that data scientists in industry could take to reduce carbon footprint and also as a barometer of interest in either a) making the reporting of energy consumption standard at ML conferences or b) a conference venue for ""energy efficient"" ML model submissions.

inb4 I get downvoted to hell for self-promotion of my blog post.",5,0
1263,2019-6-20,2019,6,20,18,c2u4k3,Why does normalizing an image dataset affects the accuracy of the model.,https://www.reddit.com/r/MachineLearning/comments/c2u4k3/why_does_normalizing_an_image_dataset_affects_the/,tp_taran,1561024722,[removed],0,1
1264,2019-6-20,2019,6,20,19,c2u90z,Best Artificial Intelligence development company|ai development services,https://www.reddit.com/r/MachineLearning/comments/c2u90z/best_artificial_intelligence_development/,clarke2106,1561025659,[removed],0,1
1265,2019-6-20,2019,6,20,19,c2ubke,[D] Second order gradient optimization vs ADAM/momentum,https://www.reddit.com/r/MachineLearning/comments/c2ubke/d_second_order_gradient_optimization_vs/,mellow54,1561026194,"I'm having trouble wrapping my head around how optimisers like ADAM and Momentum differ from second-order optimization methods.

The latter involves calculating/approximating the Hessian  however the momentum based optimisers adjust their gradients from past steps (which is quite similar to how higher order derivatives work).

I know that mathematically and implementation-wise these two methods are different however can anyone provide any intuition as to how they differ in practice - perhaps by giving an example of where you would expect wildly different results from these two types of optimisers.

Thanks :)",5,7
1266,2019-6-20,2019,6,20,20,c2upqw,Learn Big Data : Complete Hadoop Ecosystem with Practicals,https://www.reddit.com/r/MachineLearning/comments/c2upqw/learn_big_data_complete_hadoop_ecosystem_with/,HannahHumphreys,1561029089,[removed],0,1
1267,2019-6-20,2019,6,20,20,c2v0gr,Know How Artificial Intelligence Strengthening Cybersecurity,https://www.reddit.com/r/MachineLearning/comments/c2v0gr/know_how_artificial_intelligence_strengthening/,Verma_RJ,1561031131,,0,1
1268,2019-6-20,2019,6,20,21,c2vebl,[Project]Got some questions...,https://www.reddit.com/r/MachineLearning/comments/c2vebl/projectgot_some_questions/,AmeyaSama,1561033638," Can someone suggest any project ideas for undergrad? Any suggestions on how Machine learning could be applied in building a real world application to put it to use? A newbie here, trying to learn.",6,1
1269,2019-6-20,2019,6,20,21,c2veia,"DC-GAN output stuck as (28,28,3)",https://www.reddit.com/r/MachineLearning/comments/c2veia/dcgan_output_stuck_as_28283/,theThinker6969,1561033675,,0,1
1270,2019-6-20,2019,6,20,22,c2vype,Best way to understand Overfitting- Underfitting,https://www.reddit.com/r/MachineLearning/comments/c2vype/best_way_to_understand_overfitting_underfitting/,JayRathod3497,1561036935,,0,1
1271,2019-6-20,2019,6,20,22,c2w1q3,"You work on a data science team, but your background isn't data science. So what is your job?",https://www.reddit.com/r/MachineLearning/comments/c2w1q3/you_work_on_a_data_science_team_but_your/,jrdnmyr,1561037407,[removed],0,1
1272,2019-6-20,2019,6,20,22,c2w1sd,Visulizing CNN filters,https://www.reddit.com/r/MachineLearning/comments/c2w1sd/visulizing_cnn_filters/,PyWarrior,1561037416,[removed],0,1
1273,2019-6-20,2019,6,20,22,c2w3lp,Best way to understand Overfitting-Underfitting,https://www.reddit.com/r/MachineLearning/comments/c2w3lp/best_way_to_understand_overfittingunderfitting/,JayRathod3497,1561037708,,0,1
1274,2019-6-20,2019,6,20,22,c2w94r,[Research] Learning to Play Video Games from Audio Cues,https://www.reddit.com/r/MachineLearning/comments/c2w94r/research_learning_to_play_video_games_from_audio/,cdossman,1561038587," [https://medium.com/ai%C2%B3-theory-practice-business/teach-game-playing-agents-to-play-video-games-solely-from-audio-cues-50120a0db1b2?postPublishedType=repub](https://medium.com/ai%C2%B3-theory-practice-business/teach-game-playing-agents-to-play-video-games-solely-from-audio-cues-50120a0db1b2?postPublishedType=repub) 

 Game-playing AI research has focused for a long time on learning to play video games from visual input or symbolic information. However, humans benefit from a wider array of sensors which we utilise in order to navigate the world around us. In particular, sounds and music are key to how many of us perceive the world and influence the decisions we make. In this paper, we present initial experiments on game-playing agents learning to play video games solely from audio cues. We expand the Video Game Description Language to allow for audio specification, and the General Video Game AI framework to provide new audio games and an API for learning agents to make use of audio observations. We analyse the games and the audio game design process, include initial results with simple Q\~Learning agents, and encourage further research in this area.",0,1
1275,2019-6-20,2019,6,20,22,c2wcm9,Stuck with Facenet,https://www.reddit.com/r/MachineLearning/comments/c2wcm9/stuck_with_facenet/,510Monkey,1561039139,[removed],0,1
1276,2019-6-20,2019,6,20,23,c2wtu4,Opinion on unpublished papers ?,https://www.reddit.com/r/MachineLearning/comments/c2wtu4/opinion_on_unpublished_papers/,WERE_CAT,1561041627,[removed],0,1
1277,2019-6-20,2019,6,20,23,c2wx4e,Why Python is popular in machine learning,https://www.reddit.com/r/MachineLearning/comments/c2wx4e/why_python_is_popular_in_machine_learning/,sam217me,1561042073,,0,1
1278,2019-6-20,2019,6,20,23,c2wxva,[P] App to make AI-Generated submission titles for any Reddit subreddit using GPT-2 (+ keywords!),https://www.reddit.com/r/MachineLearning/comments/c2wxva/p_app_to_make_aigenerated_submission_titles_for/,minimaxir,1561042170,"[https://minimaxir.com/apps/gpt2-reddit/](https://minimaxir.com/apps/gpt2-reddit/)

This is a web UI for a finetuned GPT-2 model on a very large amount of Reddit submissions, but with a twist: you can specify the subreddit you want to generate from, and *keywords/keyphrases* to condition the text upon. For example, here are [examples](https://www.reddit.com/r/legaladviceofftopic/comments/bxi869/i_trained_an_ai_to_generate_the_ultimate/) of /r/legaladvice titles conditioned on **cat**, **dog**, **sue**, and **tree**, and the model typically does a good job of incorporating all the inputs!

Some other good subreddits for generating text are /r/amitheasshole, /r/confession, /r/writingprompts, /r/relationships, and of course the default /r/askreddit .

Technical notes on this Reddit model/API:

* The model is running on Google Cloud Run (via [gpt-2-cloud-run](https://github.com/minimaxir/gpt-2-cloud-run)), which means it's slower than GPU-backed GPT-2 demos, but it's very cheap and can scale up to Reddit-level traffic without any engineering effort. (and it can generate texts *in parallel* if you want to try many possibilities)
* Unlike /r/SubSimulatorGPT2, which has a separate GPT-2 345M model for each subreddit, this model uses *a single GPT-2* *(117M)* model. This has its advantages: the model is able to incorporate syntax/keywords from other subreddits for more creative output.
* The methodology I use to allow GPT-2 to incorporate arbitrary keywords/keyphrases in generation will be released at some point, but it's not ready yet.
* The subreddits used in the training set consist of every major subreddit you've heard of. The super niche subreddits may not be present, but the network does a good job at extrapolating subreddit type if there is a similar name in the input dataset. ([here is the full list](https://docs.google.com/spreadsheets/d/1BnoKX7Rug-PFCxSTBuzPevvlFOahwV-UIx4u1xdflDQ/edit?usp=sharing) of subreddits in the training set; 5000 total)
* The temperature is hardcoded at 0.7 and the top\_k at 40 because the results become *very* weird otherwise (see the [fanfiction output](https://github.com/minimaxir/gpt-2-fanfiction), which was done at temperature=1.0 and top\_p=0.9)
* Subreddits known for their informative titles work better than image-oriented subreddits, unsurprisingly.
* *Not all generated output will be good/make sense*, as is the case with any other type of text generation. Please don't comment ""wow the text generation sucks!"", it always takes a few tries. (but like the original GPT-2 model, the signal-to-noise ratio is better than RNN/Markov approaches)
* If you do huge mismatches of the keywords/prompt and the subreddit, the AI might ignore it.

I'm also thinking about creating another SubredditSimulator-type subreddit with generations from all subreddits but on a specific keyword/phrase.

I hope you have fun with it! Let me know if you make any interesting generations!",14,29
1279,2019-6-21,2019,6,21,0,c2x61t,"[Project] Tensorflow implementation of ""Zero-Shot Knowledge Distillation in Deep Networks """,https://www.reddit.com/r/MachineLearning/comments/c2x61t/project_tensorflow_implementation_of_zeroshot/,sseung0703,1561043295,[removed],0,1
1280,2019-6-21,2019,6,21,0,c2xgea,Leading Researchers Publish Climate Change + AI Document,https://www.reddit.com/r/MachineLearning/comments/c2xgea/leading_researchers_publish_climate_change_ai/,Yuqing7,1561044721,,0,1
1281,2019-6-21,2019,6,21,0,c2xgl3,Getting error doing data augmentation in keras..,https://www.reddit.com/r/MachineLearning/comments/c2xgl3/getting_error_doing_data_augmentation_in_keras/,hp2304,1561044747,[removed],0,1
1282,2019-6-21,2019,6,21,1,c2y1j1,[D] Learning Resources for Intermediate Machine Learning,https://www.reddit.com/r/MachineLearning/comments/c2y1j1/d_learning_resources_for_intermediate_machine/,NoEarlyStopping,1561047558,"I'm currently 9 months into my MASc with my thesis focusing on computer vision and human movement analysis with deep learning. Right now I'd say that I'm at an intermediate level in terms of my ML knowledge. I've completed two university courses so far on ML/DL, Andrew Ng's course, as well as a couple other online courses and readings. I also have taken a pattern classification course that gave me a pretty good background on statistics and its relations with ML from linear regression to HMMs.

I was wondering if anyone knows of any good resources that I can turn to now. Specifically in the area of computer vision or DL would be useful. I find that many websites, like towardsdatascience, end up being the same basics that I've seen many times. I'm open to any types of resources really: textbooks, papers, youtube videos, etc.

Also, I was wondering if anyone has experience with auditing classes (just sitting in and listening) during their MASc or PhD. Is it worthwhile?",6,10
1283,2019-6-21,2019,6,21,1,c2y3xw,B.Sc final project ideas,https://www.reddit.com/r/MachineLearning/comments/c2y3xw/bsc_final_project_ideas/,mohammad_sianaki,1561047889,[removed],0,1
1284,2019-6-21,2019,6,21,1,c2y55x,Transform Women in AI Awards,https://www.reddit.com/r/MachineLearning/comments/c2y55x/transform_women_in_ai_awards/,npatrici,1561048042,[removed],0,1
1285,2019-6-21,2019,6,21,1,c2y8fj,[D] B.Sc final project idea,https://www.reddit.com/r/MachineLearning/comments/c2y8fj/d_bsc_final_project_idea/,mohammad_sianaki,1561048488,"Could someone please tell some idea of final project in machine learning?
For example how to apply machine learning in healthcare, finance, etc.",3,0
1286,2019-6-21,2019,6,21,1,c2y8os,A Super Approachable Intro to K-Means,https://www.reddit.com/r/MachineLearning/comments/c2y8os/a_super_approachable_intro_to_kmeans/,rylandgold,1561048521,,0,1
1287,2019-6-21,2019,6,21,1,c2yakj,Game discuss ion,https://www.reddit.com/r/MachineLearning/comments/c2yakj/game_discuss_ion/,karenactionsitafaal,1561048779,,0,1
1288,2019-6-21,2019,6,21,1,c2yezs,[R] Principled Machine Learning for Efficient Collaboration,https://www.reddit.com/r/MachineLearning/comments/c2yezs/r_principled_machine_learning_for_efficient/,thumbsdrivesmecrazy,1561049354,"Machine learning projects are often harder than they should be. We're just running software, and the result is a trained ML model. But three months later do you remember how to rerun the software, the datasets may have changed, and therefore you might be unable to replicate the results. A lack of software tools to manage machine learning datasets is the culprit, and impede efforts to efficiently share of data with colleagues.

In our search for tools to efficiently manage machine learning projects these principles are important:

* Transparency: Inspecting every part of the ML project
* Audibility: Inspecting all intermediate results, and the final result
* Reproducibility: Ability to robustly rerun the software and associated datasets from any stage in the project
* Scalability: Ability to support ML projects containing any number of people, and to work on multiple projects at a time

The article explains implementation in ML projects and using some open source tools like MLFlow and DVC in this context: [Principled Machine Learning - DEV Community](https://dev.to/robogeek/principled-machine-learning-4eho)",9,112
1289,2019-6-21,2019,6,21,2,c2ykl8,Complex Data Solutions for Intelligent Modeling,https://www.reddit.com/r/MachineLearning/comments/c2ykl8/complex_data_solutions_for_intelligent_modeling/,ovidi_data,1561050100,,0,1
1290,2019-6-21,2019,6,21,2,c2yprz,Newegg flash sale: Nvidia GRID M40 GPU 16GB GDDR5 Accelerator Processing Card for $180,https://www.reddit.com/r/MachineLearning/comments/c2yprz/newegg_flash_sale_nvidia_grid_m40_gpu_16gb_gddr5/,po-handz,1561050790,,1,1
1291,2019-6-21,2019,6,21,2,c2ytwi,Biggest pet peeve,https://www.reddit.com/r/MachineLearning/comments/c2ytwi/biggest_pet_peeve/,karenactionsitafaal,1561051337,,0,1
1292,2019-6-21,2019,6,21,2,c2yy4p,How to deal with scale variation with CNN?,https://www.reddit.com/r/MachineLearning/comments/c2yy4p/how_to_deal_with_scale_variation_with_cnn/,nayriz,1561051902,[removed],0,1
1293,2019-6-21,2019,6,21,2,c2z4ha,How does batch norm really help optimisation?,https://www.reddit.com/r/MachineLearning/comments/c2z4ha/how_does_batch_norm_really_help_optimisation/,xku,1561052744,,1,1
1294,2019-6-21,2019,6,21,3,c2zocp,"CVPR Daily of today, Thursday 20 June - Directly from CVPR 2019 in Long Beach",https://www.reddit.com/r/MachineLearning/comments/c2zocp/cvpr_daily_of_today_thursday_20_june_directly/,Gletta,1561055434,,0,1
1295,2019-6-21,2019,6,21,3,c2zrz5,I'm looking to do my Master thesis on something related to Image Segmentation. Do you have any suggestions or datasets I could explore?,https://www.reddit.com/r/MachineLearning/comments/c2zrz5/im_looking_to_do_my_master_thesis_on_something/,sandalphone,1561055987,[removed],0,1
1296,2019-6-21,2019,6,21,4,c307i0,This model needs to be retrained,https://www.reddit.com/r/MachineLearning/comments/c307i0/this_model_needs_to_be_retrained/,LenjaminSandwich,1561058138,,0,1
1297,2019-6-21,2019,6,21,4,c307nr,[R] How does batch norm _really_ help optimisation?,https://www.reddit.com/r/MachineLearning/comments/c307nr/r_how_does_batch_norm_really_help_optimisation/,xku,1561058157,,0,1
1298,2019-6-21,2019,6,21,4,c30e8q,[R] SamsungAI: Realistic speech-driven face animation from a single still image,https://www.reddit.com/r/MachineLearning/comments/c30e8q/r_samsungai_realistic_speechdriven_face_animation/,hanrelan,1561059039,,1,1
1299,2019-6-21,2019,6,21,4,c30hm9,Phrase similarity dataset,https://www.reddit.com/r/MachineLearning/comments/c30hm9/phrase_similarity_dataset/,rodrigonader,1561059501,"Im looking for a dataset to train model on phrase/clause similarity. For example:


It is a television / Its not a television &gt; 0
Its a television / Its a TV &gt; 1

I have found some similar data sets but for full sentences, Im looking for more compact phrases/clauses.

Any ideas help!",0,1
1300,2019-6-21,2019,6,21,5,c30sxs,From Foodie Pic to Your Plate: Generating Recipes With Facebook AI,https://www.reddit.com/r/MachineLearning/comments/c30sxs/from_foodie_pic_to_your_plate_generating_recipes/,Yuqing7,1561060965,,0,1
1301,2019-6-21,2019,6,21,5,c30t5y,Getting Started with ML,https://www.reddit.com/r/MachineLearning/comments/c30t5y/getting_started_with_ml/,23onfroy,1561060992,[removed],0,1
1302,2019-6-21,2019,6,21,5,c30u9l,Stanford Machine Learning Class Notes (CS229),https://www.reddit.com/r/MachineLearning/comments/c30u9l/stanford_machine_learning_class_notes_cs229/,ai-lover,1561061135,[removed],0,1
1303,2019-6-21,2019,6,21,5,c312uh,All Hail The Mighty Translatotron!,https://www.reddit.com/r/MachineLearning/comments/c312uh/all_hail_the_mighty_translatotron/,GFX47,1561062255,,0,1
1304,2019-6-21,2019,6,21,5,c315du,[D] Could ML be used to moderate graphic content? This article contains gruesome details regarding the traumatizing work conditions for Facebook content moderators.,https://www.reddit.com/r/MachineLearning/comments/c315du/d_could_ml_be_used_to_moderate_graphic_content/,ValentinaMaria,1561062580,,1,1
1305,2019-6-21,2019,6,21,5,c317dm,[P] Weight Agnostic Neural Networks,https://www.reddit.com/r/MachineLearning/comments/c317dm/p_weight_agnostic_neural_networks/,AnarchisticPunk,1561062830,[removed],0,1
1306,2019-6-21,2019,6,21,5,c317vh,Creating an Over by Over win probability predictor for T20Is,https://www.reddit.com/r/MachineLearning/comments/c317vh/creating_an_over_by_over_win_probability/,agrahajigyasu,1561062901,[removed],1,1
1307,2019-6-21,2019,6,21,5,c31es0,Wanted to do a masters in machine learning. But got a C+ in my first statistics course.. Is it possible to do well without being amazing at statistics? And will this hurt me when I apply to grad school?,https://www.reddit.com/r/MachineLearning/comments/c31es0/wanted_to_do_a_masters_in_machine_learning_but/,hdbdidbdodbdu,1561063816,[removed],0,1
1308,2019-6-21,2019,6,21,5,c31imp,Struct2Depth,https://www.reddit.com/r/MachineLearning/comments/c31imp/struct2depth/,roboticsR,1561064330,Hey Guys ! I have been trying to run the depth prediction networks [https://github.com/tensorflow/models/tree/master/research/struct2depth](https://github.com/tensorflow/models/tree/master/research/struct2depth) as you can see to train it. It requires all reset (since there are a lot of variations of it) checkpoint but all the resent checkpoints I could find have different variable names. So If any one of you have used the same can you point me in the right direction ?,0,1
1309,2019-6-21,2019,6,21,6,c31yx3,NLP with generative models,https://www.reddit.com/r/MachineLearning/comments/c31yx3/nlp_with_generative_models/,pkms3,1561066566,"Hello everyone, I would really like to get your views on what is the state of using generative models in natural language processing and would also like to now what kind of research is happening in this area presently.",0,1
1310,2019-6-21,2019,6,21,6,c323w2,A look at the Deepbox Framework and it's pros and cons,https://www.reddit.com/r/MachineLearning/comments/c323w2/a_look_at_the_deepbox_framework_and_its_pros_and/,paddy1709,1561067249,,0,1
1311,2019-6-21,2019,6,21,7,c32g80,[D] GAN Theory - is optimizing a generator to model a low variance dataset a fundamentally different problem from modeling a high variance one?,https://www.reddit.com/r/MachineLearning/comments/c32g80/d_gan_theory_is_optimizing_a_generator_to_model_a/,toadsofbattle,1561068961,"GANs are usually trained on pretty high variance datasets (Imagenet, CIFAR, etc). Intuitively, these datasets are hard to model - they encompass a wide range of classes, and even within these classes, there's a ton of variance between and within images.

So, it seems natural to me that if I had a low variance dataset, I could take off-the-shelf GAN architectures and parameters that are able to model high-variance datasets, plug in my own data, and get nice outputs. For instance, let's say I had thousands of overhead views of a crop field, which changed in minor ways based on the season, crop quality, etc - I'm intuiting that the variance of this data would be much smaller and consequently easier to model (this is not my actual problem - it's even lower variance between+within images - but let's just use this as an example).

I'm finding that in practice, however, this isn't really true. Something about modeling this low variance dataset is proving to be a harder problem. It could be a quality of the dataset aside from the low-variance, but some other tests I've run (artificially increasing the variance of the images) have me thinking otherwise.

I'm thinking that perhaps the methods/architectures to optimize GAN training are probably different across different dataset types - specifically, low vs high variance datasets. Any thoughts?",7,11
1312,2019-6-21,2019,6,21,8,c333ye,[N] Our startup Replica created the voices in this video using AI. We are going to release our beta product soon. Check out the video!,https://www.reddit.com/r/MachineLearning/comments/c333ye/n_our_startup_replica_created_the_voices_in_this/,replica_ai,1561074361,,1,3
1313,2019-6-21,2019,6,21,8,c3343p,Natural Language Processing question,https://www.reddit.com/r/MachineLearning/comments/c3343p/natural_language_processing_question/,johnpcoder,1561074397,,0,1
1314,2019-6-21,2019,6,21,9,c33yl5,[R] A Growing Neural Gas Network Learns Topologies (NIPS 1994),https://www.reddit.com/r/MachineLearning/comments/c33yl5/r_a_growing_neural_gas_network_learns_topologies/,hardmaru,1561078564,"**Abstract**

An incremental network model is introduced which is able to learn
the important topological relations in a given set of input vectors by
means of a simple Hebb-like learning rule. In contrast to previous
approaches like the ""neural gas"" method of Martinetz and Schulten
(1991, 1994), this model has no parameters which change over time
and is able to continue learning, adding units and connections, until
a performance criterion has been met. Applications of the model
include vector quantization, clustering, and interpolation. 

PDF: http://papers.nips.cc/paper/893-a-growing-neural-gas-network-learns-topologies.pdf

Interactive demo (2017): https://www.demogng.de/",0,1
1315,2019-6-21,2019,6,21,11,c34o9l,TF 2.0 beta vs PyTorch,https://www.reddit.com/r/MachineLearning/comments/c34o9l/tf_20_beta_vs_pytorch/,snip3r77,1561082628,[removed],0,1
1316,2019-6-21,2019,6,21,11,c34z8l,Any success stories with Adversarial Autoencoders or Wasserstein Autoencoders?,https://www.reddit.com/r/MachineLearning/comments/c34z8l/any_success_stories_with_adversarial_autoencoders/,gazorpazorpazorpazor,1561084371,[removed],0,1
1317,2019-6-21,2019,6,21,11,c355go,Best Tooling for Non-traditional ML,https://www.reddit.com/r/MachineLearning/comments/c355go/best_tooling_for_nontraditional_ml/,OptoBot,1561085390,[removed],0,1
1318,2019-6-21,2019,6,21,12,c35ibh,"[P] Github repository for Recommender System - ""awesome-RecSys""",https://www.reddit.com/r/MachineLearning/comments/c35ibh/p_github_repository_for_recommender_system/,data-chef,1561087615,[removed],0,1
1319,2019-6-21,2019,6,21,13,c367tk,[R] Wasserstein Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/c367tk/r_wasserstein_reinforcement_learning/,inarrears,1561092191,,28,110
1320,2019-6-21,2019,6,21,13,c36859,Can labels in a dataset be a vector (or just anything non-primary) ?,https://www.reddit.com/r/MachineLearning/comments/c36859/can_labels_in_a_dataset_be_a_vector_or_just/,czechrepublic,1561092251,[removed],0,1
1321,2019-6-21,2019,6,21,13,c369p6,[R] Learning the Arrow of Time,https://www.reddit.com/r/MachineLearning/comments/c369p6/r_learning_the_arrow_of_time/,milaworld,1561092534,"Saw this one at the Theoretical Physics for Deep Learning workshop at ICML this year.

Abstract: We humans seem to have an innate understanding of the asymmetric progression of time, which we use to efficiently and safely perceive and manipulate our environment. Drawing inspiration from that, we address the problem of learning an arrow of time in a Markov Decision Process. We illustrate how a learned arrow of time can capture meaningful information about the environment, which in turn can be used to measure reachability, detect side-effects and to obtain an intrinsic reward signal. We show empirical results on a selection of discrete and continuous environments, and demonstrate for a class of stochastic processes that the learned arrow of time agrees reasonably well with a known notion of an arrow of time given by the celebrated *Jordan-Kinderlehrer-Otto* result.

https://openreview.net/pdf?id=SkevntbkJB",7,33
1322,2019-6-21,2019,6,21,14,c36e4p,[R] XLNet outperforms BERT on several NLP Tasks,https://www.reddit.com/r/MachineLearning/comments/c36e4p/r_xlnet_outperforms_bert_on_several_nlp_tasks/,omarsar,1561093355,,0,1
1323,2019-6-21,2019,6,21,14,c36nu6,Why are we the best?,https://www.reddit.com/r/MachineLearning/comments/c36nu6/why_are_we_the_best/,getengati,1561095183,[removed],0,1
1324,2019-6-21,2019,6,21,14,c36qbg,[Google AI] Direct speech-to-speech translation with a sequence-to-sequence model,https://www.reddit.com/r/MachineLearning/comments/c36qbg/google_ai_direct_speechtospeech_translation_with/,eigenlaplace,1561095671,"Blog post by google AI: [https://ai.googleblog.com/2019/05/introducing-translatotron-end-to-end.html](https://ai.googleblog.com/2019/05/introducing-translatotron-end-to-end.html)

Audio samples:  [https://google-research.github.io/lingvo-lab/translatotron/#conversational](https://google-research.github.io/lingvo-lab/translatotron/#conversational)

Link to paper:  [https://arxiv.org/abs/1904.06037](https://arxiv.org/abs/1904.06037)",0,1
1325,2019-6-21,2019,6,21,14,c36thp,XLNet: Generalized Autoregressive Pretraining for Language Understanding,https://www.reddit.com/r/MachineLearning/comments/c36thp/xlnet_generalized_autoregressive_pretraining_for/,cosentiyes,1561096301,[removed],0,1
1326,2019-6-21,2019,6,21,15,c374qa,[R] XLNet: Generalized Autoregressive Pretraining for Language Understanding,https://www.reddit.com/r/MachineLearning/comments/c374qa/r_xlnet_generalized_autoregressive_pretraining/,cosentiyes,1561098631,"""With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking.""

&amp;#x200B;

[https://arxiv.org/abs/1906.08237](https://arxiv.org/abs/1906.08237)",1,0
1327,2019-6-21,2019,6,21,15,c375fr,Automated Warehouse AS/RS | Telescopic Forks for Automated Warehouse AS/RS | Automated AS/RS Systems,https://www.reddit.com/r/MachineLearning/comments/c375fr/automated_warehouse_asrs_telescopic_forks_for/,lhd121,1561098773," 

## Telescopic Forks for Automated WarehouseAS/RS

LHD designs and develops telescopic forks for automated AS/RS systems. Automate warehouse operations with automated AS/RS systems.  
Our AS/RS systems are automated-controlled systems that store and retrieve products in distribution centers, warehouses or manufacturing facilities.  
Automated storage has been developing since the 1960s and is one fo the smartest and most sophisticated material handling system available. Loads are strategically delivered via automation upon demand without the use of manual labor.  
Most of AS/RS systems are used worldwide for handling pallets. AS/RS most commonconfiguration is pallet rack single deep or with multiple pallets stored one behind another (double deep depth) with narrow aisles between allows a tall masted crane to move down the center of the aisle on a raised metal rail.  
Our products reaches into the pallet location and slides under the pallet and pulls it out into the aisle onto the carriage.Here at LHD S.p.A. were able to work with every kind of system in every kind of Automated Warehouse. 

&amp;#x200B;

*Processing img 7j6ih8hjmn531...*

 

### [ARES 65](http://lhd.co.com/product/ares-65/)

* Standard length:from 400 mm to 2600 mm
* Stroke:from 445 to 2750 mm
* Load:up to 1.000 kg (on each fork)

### [LHD TRANSFER](http://lhd.co.com/product/lhd-transfer/)

* Standard length:from 1200 mm to 1550 mm
* Lifting stroke:from 75 to 150mm
* Load:up to 1000 kg

To know more, click here: [http://lhd.co.com/industry-applications/asrs/](http://lhd.co.com/industry-applications/asrs/)",0,1
1328,2019-6-21,2019,6,21,16,c37gw8,PCA kernels for data types,https://www.reddit.com/r/MachineLearning/comments/c37gw8/pca_kernels_for_data_types/,sap218,1561101101,[removed],0,1
1329,2019-6-21,2019,6,21,16,c37jih,"Why all Google models BERT , XLNET is still using Tensorflow 1.13 or lesser , why not tf-2.0?",https://www.reddit.com/r/MachineLearning/comments/c37jih/why_all_google_models_bert_xlnet_is_still_using/,s4sarath,1561101657,"Even though Google promotes tensorflow 2.0 like never before, when it comes to Google, why they choose older tensorflow versions. Is it because of speed constraints or anything else.",0,1
1330,2019-6-21,2019,6,21,16,c37kul,[R] PCA kernels for data types,https://www.reddit.com/r/MachineLearning/comments/c37kul/r_pca_kernels_for_data_types/,sap218,1561101958,"I read somewhere that kernels for kPCA can be used for different data types.

I used PCAmix (R package: classic PCA on continuous and MCA on categorical then combines) on my data set and my data doesnt split in any way - PC1 and PC2 is just a ball of coordinates. 

So I was thinking of trying two different kernels for data types then combining them? 

My supervisor isnt listening when I tell him that there is no variance in our data but he is determined to find something so Im looking into a lot of different dimension reduction methods.",4,2
1331,2019-6-21,2019,6,21,16,c37nsw,[D] Using proportion in place of 0 or 1 to binary-valued feature for classification prediction (Xgboost model),https://www.reddit.com/r/MachineLearning/comments/c37nsw/d_using_proportion_in_place_of_0_or_1_to/,Mysterious_Bit,1561102613,"Hi all,

Want to get some input. I have modeled buy propensity with search event data. One user can generate many search events. The model was XGBoost with features both real-valued and binary.

Now, if i want to predict buy propensity per user, is it possible just to aggregate the data for that user and feed that to the model? One worry is because aggregation can return proportion for binary feature that expects 0 or 1 instead. It seems like XGBoost is not like linear model that makes some assumptions about the data. But, still what do you think? Is it fine to do this?

Thank you",4,0
1332,2019-6-21,2019,6,21,16,c37oez,CSDN account for mobilenet,https://www.reddit.com/r/MachineLearning/comments/c37oez/csdn_account_for_mobilenet/,plusgarbage,1561102749,"Hello, I need someone who download for me this file from CSDN

https://download.csdn.net/download/ghw15221836342/10536729

Can you? I need for work. Thank you so much",0,1
1333,2019-6-21,2019,6,21,17,c37udm,Imbalance in dataset creating problems for training,https://www.reddit.com/r/MachineLearning/comments/c37udm/imbalance_in_dataset_creating_problems_for/,fhrkingbradley,1561104092,[removed],0,1
1334,2019-6-21,2019,6,21,17,c37w38,XLNet outperforms BERT on 20 NLP Benchmark Tasks,https://www.reddit.com/r/MachineLearning/comments/c37w38/xlnet_outperforms_bert_on_20_nlp_benchmark_tasks/,uchiha_indra,1561104476,,1,1
1335,2019-6-21,2019,6,21,17,c384jw,[D] Generating comments for a social media post based on its description.,https://www.reddit.com/r/MachineLearning/comments/c384jw/d_generating_comments_for_a_social_media_post/,roonishpower,1561106487,"I was tasked with building a model which would auto-generate comments based on the context of the description. The data I would be using is the description and comments that's scrapped from Instagram pages. I'm familiar with working with numerical data but this is my first time working on an NLP problem. 

From some research, I got to know that RNN-LSTM would be a good way to proceed to tackle this problem but I would love to hear what the community has to say about it. Any relevant papers, projects or posts would be appreciated.",1,0
1336,2019-6-21,2019,6,21,17,c385bc,[R] The Functional Neural Process,https://www.reddit.com/r/MachineLearning/comments/c385bc/r_the_functional_neural_process/,youali,1561106669,"**The Functional Neural Process**

Abstract:  We present a new family of exchangeable stochastic processes, the Functional Neural Processes (FNPs). FNPs model distributions over functions by learning a graph of dependencies on top of latent representations of the points in the given dataset. In doing so, they define a Bayesian model without explicitly positing a prior distribution over latent global parameters; they instead adopt priors over the relational structure of the given dataset, a task that is much simpler. We show how we can learn such models from data, demonstrate that they are scalable to large datasets through mini-batch optimization and describe how we can make predictions for new points via their posterior predictive distribution. We experimentally evaluate FNPs on the tasks of toy regression and image classification and show that, when compared to baselines that employ global latent parameters, they offer both competitive predictions as well as more robust uncertainty estimates. 


https://arxiv.org/abs/1906.08324",1,26
1337,2019-6-21,2019,6,21,18,c38avq,Word Embedding in Generalized Zero-shot Learning,https://www.reddit.com/r/MachineLearning/comments/c38avq/word_embedding_in_generalized_zeroshot_learning/,GANandreas,1561107922,[removed],0,1
1338,2019-6-21,2019,6,21,18,c38jzn,Is it a good way to get started with ML from deeplens/aws?,https://www.reddit.com/r/MachineLearning/comments/c38jzn/is_it_a_good_way_to_get_started_with_ml_from/,getchaAgain,1561109943,[removed],0,1
1339,2019-6-21,2019,6,21,19,c38u8d,A comprehensive intro to PyTorch,https://www.reddit.com/r/MachineLearning/comments/c38u8d/a_comprehensive_intro_to_pytorch/,whitezl0,1561112131,,0,1
1340,2019-6-21,2019,6,21,19,c3921h,[N] Course 3 of the deeplearning.ai TensorFlow Specialization is now available: TensorFlow in Practice | Coursera,https://www.reddit.com/r/MachineLearning/comments/c3921h/n_course_3_of_the_deeplearningai_tensorflow/,lopespm,1561113848,,0,1
1341,2019-6-21,2019,6,21,20,c39ewh,Different kind of machine learning: biology inspired,https://www.reddit.com/r/MachineLearning/comments/c39ewh/different_kind_of_machine_learning_biology/,opensourcesblog,1561116434,,0,1
1342,2019-6-21,2019,6,21,21,c3a2yd,Testy test,https://www.reddit.com/r/MachineLearning/comments/c3a2yd/testy_test/,arkady_red,1561120910,[removed],0,1
1343,2019-6-21,2019,6,21,21,c3a8wl,Response Modeling using Machine Learning Techniques in R,https://www.reddit.com/r/MachineLearning/comments/c3a8wl/response_modeling_using_machine_learning/,andrea_manero,1561121912,[removed],0,1
1344,2019-6-21,2019,6,21,22,c3afhe,"How does Apache Ambari play a big role in Big Data, Machine Learning and AI?",https://www.reddit.com/r/MachineLearning/comments/c3afhe/how_does_apache_ambari_play_a_big_role_in_big/,vigbig,1561122934,[removed],0,1
1345,2019-6-21,2019,6,21,22,c3ah2e,A Question Regarding XGBoost,https://www.reddit.com/r/MachineLearning/comments/c3ah2e/a_question_regarding_xgboost/,maditab,1561123190,[removed],0,1
1346,2019-6-21,2019,6,21,22,c3amid,nunchaku freestyle Fridays- meany variations of the lotus,https://www.reddit.com/r/MachineLearning/comments/c3amid/nunchaku_freestyle_fridays_meany_variations_of/,thetrickshotone,1561124073,,0,1
1347,2019-6-21,2019,6,21,22,c3amr0,"[D] When using unstructured meshes or multi-res Lattice boltzmann methods, how do we map that to a convolution layer if the resolution is not homogeneous?",https://www.reddit.com/r/MachineLearning/comments/c3amr0/d_when_using_unstructured_meshes_or_multires/,pradeep_sinngh,1561124113,,0,1
1348,2019-6-21,2019,6,21,22,c3angh,"[D] When using unstructured meshes or multi-res LBM (Lattice Boltzmann methods), how do we map that to a convolution layer if the resolution is not homogeneous?",https://www.reddit.com/r/MachineLearning/comments/c3angh/d_when_using_unstructured_meshes_or_multires_lbm/,pradeep_sinngh,1561124226,,0,1
1349,2019-6-21,2019,6,21,22,c3aozs,Machine Learning with Python,https://www.reddit.com/r/MachineLearning/comments/c3aozs/machine_learning_with_python/,monica_b1998,1561124465,,0,1
1350,2019-6-21,2019,6,21,23,c3b8dw,Artificial Intelligence Premier for Business Leaders,https://www.reddit.com/r/MachineLearning/comments/c3b8dw/artificial_intelligence_premier_for_business/,allakor,1561127275,,0,1
1351,2019-6-21,2019,6,21,23,c3bhcr,5 Must Have Skills Needed For Machine Learning Jobs,https://www.reddit.com/r/MachineLearning/comments/c3bhcr/5_must_have_skills_needed_for_machine_learning/,MealPlan,1561128457,,0,1
1352,2019-6-21,2019,6,21,23,c3bllf,Another Book on Data Science  Learn R and Python in Parallel,https://www.reddit.com/r/MachineLearning/comments/c3bllf/another_book_on_data_science_learn_r_and_python/,iamkeyur,1561129030,,0,1
1353,2019-6-22,2019,6,22,0,c3c65d,[D] Is self-supervised learning is really just denoising autoencoder?,https://www.reddit.com/r/MachineLearning/comments/c3c65d/d_is_selfsupervised_learning_is_really_just/,tsauri,1561131665,"Denoising autoencoder 2019.  
*Noise* the inputs and put original input as labels.  
Noise is unfortunately must be cleverly highly hand-engineered.  
Like crop image uncrop image, remove color channels predict colors, remove positions predict positions, rotate image reorient image, remove last word predict last word.  
Then take the learnt weights as feature extractor.   


I am waiting for someone to use them for generative purposes, if they are useful for disentanglement.

Now we wait for something like GANs for denoising, where generator make new denoising tasks, and discriminator must solve the new tasks",2,1
1354,2019-6-22,2019,6,22,0,c3c6b1,"Is research in large scale classical (""convex"") machine learning dead?",https://www.reddit.com/r/MachineLearning/comments/c3c6b1/is_research_in_large_scale_classical_convex/,berlinbrewer,1561131686,subj,0,1
1355,2019-6-22,2019,6,22,1,c3cf05,I tried to load Instagram on Desktop today and saw this.,https://www.reddit.com/r/MachineLearning/comments/c3cf05/i_tried_to_load_instagram_on_desktop_today_and/,BayBass,1561132845,[removed],0,1
1356,2019-6-22,2019,6,22,1,c3cjlf,Push deploy your Azure trained machine learning models to your iOS apps with Skafos.ai,https://www.reddit.com/r/MachineLearning/comments/c3cjlf/push_deploy_your_azure_trained_machine_learning/,heybluez,1561133385,,0,3
1357,2019-6-22,2019,6,22,1,c3cknw,[D] Self-supervised learning vs denoising autoencoder,https://www.reddit.com/r/MachineLearning/comments/c3cknw/d_selfsupervised_learning_vs_denoising_autoencoder/,tsauri,1561133519,"Both need cleverly hand-engineered noise as input.  
Labels are free, because labels are the original inputs themselves.  
Remove, change part of inputs (colors, rotation, crop, black pixels, missing words), then predict what is missing.  
Learnt embeddings become free feature extractor.  


This calls for something meta like GANs or [PowerPlay](https://arxiv.org/abs/1112.5309) to stop hand engineering noise.",4,4
1358,2019-6-22,2019,6,22,1,c3ct5y,AI residency application and interview process,https://www.reddit.com/r/MachineLearning/comments/c3ct5y/ai_residency_application_and_interview_process/,AbinavR,1561134560,[removed],0,1
1359,2019-6-22,2019,6,22,2,c3db4f,An interestin title,https://www.reddit.com/r/MachineLearning/comments/c3db4f/an_interestin_title/,karenactionsitafaal,1561136790,,1,1
1360,2019-6-22,2019,6,22,2,c3dist,Getting into Machine learning,https://www.reddit.com/r/MachineLearning/comments/c3dist/getting_into_machine_learning/,PumperNickel9583,1561137740,[removed],0,1
1361,2019-6-22,2019,6,22,2,c3e0hq,[D] Aftermath from CVPR,https://www.reddit.com/r/MachineLearning/comments/c3e0hq/d_aftermath_from_cvpr/,da_g_prof,1561139926,"Just came back from CVPR and wanted to offer some personal observations.
The good :
* lots of nice and interesting papers 
* lots of industry presence 
* expo full of companies ready to hire the best 
* large venue that managed to accommodate the almost 9000 people that didn't feel that we were that many 
* good catering considering the size 
* 

The bad :",0,1
1362,2019-6-22,2019,6,22,3,c3e4j4,[P] CNN Inference in C,https://www.reddit.com/r/MachineLearning/comments/c3e4j4/p_cnn_inference_in_c/,cnylnz,1561140420,[removed],0,1
1363,2019-6-22,2019,6,22,3,c3e9qu,"[D] Those who hire/interview for machine learning positions, what can self taught people include in their projects that would convince you they would be able to fit in and keep up with those with a more standard background ?",https://www.reddit.com/r/MachineLearning/comments/c3e9qu/d_those_who_hireinterview_for_machine_learning/,AdditionalWay,1561141075,,167,489
1364,2019-6-22,2019,6,22,3,c3ea6m,[D] Any references for deep learning on non uniform and unstructured meshes/ grids?,https://www.reddit.com/r/MachineLearning/comments/c3ea6m/d_any_references_for_deep_learning_on_non_uniform/,pradeep_sinngh,1561141134,,12,11
1365,2019-6-22,2019,6,22,3,c3ebmd,"[D] CVPR Observations (maybe good for science, definitely bad for the planet)",https://www.reddit.com/r/MachineLearning/comments/c3ebmd/d_cvpr_observations_maybe_good_for_science/,da_g_prof,1561141320,"Just came back from CVPR and wanted to offer some personal observations.

TLDR humangous conferences pack lots of science, but it feels overwhelming and may affect how science is 'shared'. For sure it affects the planet. 

The good :
* overall a smooth organization without major hickups
* lots of nice and interesting papers
* excellent presence of work across the globe but China had an impressive showing
* lots of industry presence
* expo full of companies ready to hire the best
* large venue that managed to accommodate the almost 9000 people that didn't feel that we were that many
* good catering considering the size
* hundreds of people worked behind the scenes to make it happen (catering, waiters, cleaners, security) and I am so happy to see people have work and take care of us


The bad :
* too many people made it harder to bump into people and network. The super sized venue spread across didn't help either. Time from one to the other could easily take 5 minutes.
* posters the first day it was impossible due to crowding. The second day and after became much better but still impossible to go through all 200 posters per session. 
* 5 min per oral was an interesting concept but it has some problems. It keeps every one on time via the video delivery but it makes for a dry non interactive delivery. The short time also tweet-sizes research. You get to focus on how amazing are the slides rather than the science. It was evident that those that broke down the message to the simplest possible and made a good presentation about it gave the best presentations. Some presentations appeared professionally made by a company.  Some rethinking of conference format is necessary. 
* questions after orals are obsolete. Remove them completely. 
* I found the herding by local security / catering people annoying. We were being herded always and everywhere. Too many rules...
* THE WORST : the carbon footprint. I am not talking about travel. I am talking about the amount of plastic and trash produced. By a small calculation I expect more than 100000 paper cups and plastic lids. Every lunch was in a plastic box and plastic spoon: about 50000 plus were used. Hundreds of thousands of plastic cups were used for water and drinks. Come on... Seriously. Have someone sponsor a reusable mug and hand it out. People can bring it back as souvenir. There are nice ones made by bamboo. (By the way it is possible that we can recycle some of the plastic but you all know that we are past beyond that now in the planet.)  As conference sizes sky rocket we must work together to help reduce impact to the planet. 

If any of you went there or not, please share your thoughts.",9,1
1366,2019-6-22,2019,6,22,3,c3ehtl,Build Realistic Human Speech Animations with the New VOCA Model and 4D Face Dataset - Medium,https://www.reddit.com/r/MachineLearning/comments/c3ehtl/build_realistic_human_speech_animations_with_the/,Yuqing7,1561142089,,0,1
1367,2019-6-22,2019,6,22,4,c3ewpo,High-quality machine learning book recommendations.,https://www.reddit.com/r/MachineLearning/comments/c3ewpo/highquality_machine_learning_book_recommendations/,jonksar,1561143978,[removed],0,1
1368,2019-6-22,2019,6,22,4,c3f2s1,"[Revision/Study Group] fast.ai Part 1 2019, Saturdays 4PM IST (Zoom calls)",https://www.reddit.com/r/MachineLearning/comments/c3f2s1/revisionstudy_group_fastai_part_1_2019_saturdays/,init__27,1561144742,[removed],0,1
1369,2019-6-22,2019,6,22,4,c3fgvh,If I wanted to use ML to analyze the attached file to identify unit count what resources would the community recommend?,https://www.reddit.com/r/MachineLearning/comments/c3fgvh/if_i_wanted_to_use_ml_to_analyze_the_attached/,MrGoodGlow,1561146567,,1,1
1370,2019-6-22,2019,6,22,5,c3ftv4,"[P] CNN Inference Library in C, Written for Readability",https://www.reddit.com/r/MachineLearning/comments/c3ftv4/p_cnn_inference_library_in_c_written_for/,cnylnz,1561148255,"Hello everyone, my team and I have been working on a project about CNN inference acceleration on FPGA and during this process I ended up writing a mini Convolutional Neural Network inference library in C so that we have everything laid out. I think it could help anyone wanting to learn how CNN inference works on a ""low"" level. My primary goal in writing this library was clarity and readability and I have inserted lots of comments to guide the reader. Any feedback would be much appreciated, here is a link to the repository:

[CNN-Inference-Didactic](https://github.com/canyalniz/CNN-Inference-Didactic)",1,14
1371,2019-6-22,2019,6,22,5,c3fxq7,Cool hackathon project of AI trained on climbing videos to generate a climbing animation.,https://www.reddit.com/r/MachineLearning/comments/c3fxq7/cool_hackathon_project_of_ai_trained_on_climbing/,jdv9,1561148762,,0,1
1372,2019-6-22,2019,6,22,5,c3g7kw,[D] Creating a NN to beat a NN,https://www.reddit.com/r/MachineLearning/comments/c3g7kw/d_creating_a_nn_to_beat_a_nn/,Eccentricc,1561150032,"What would happen if I created a NN to create better mazes(with restrictions like size) but a separate NN to complete the mazes as fast as possible. Who wins lol, or is this an actual method of training? Would this be applicable anywhere?",2,0
1373,2019-6-22,2019,6,22,5,c3g8od,[R] Deep Video Inpainting,https://www.reddit.com/r/MachineLearning/comments/c3g8od/r_deep_video_inpainting/,mcahny,1561150175,,0,1
1374,2019-6-22,2019,6,22,6,c3gnp7,Any research directly or indirectly related to learning the physics of an environment?,https://www.reddit.com/r/MachineLearning/comments/c3gnp7/any_research_directly_or_indirectly_related_to/,darthmeshkat,1561152157,[removed],0,1
1375,2019-6-22,2019,6,22,6,c3go91,Can You Reuse Training Data for Predictions?,https://www.reddit.com/r/MachineLearning/comments/c3go91/can_you_reuse_training_data_for_predictions/,maditab,1561152227,"I have a finite amount of customer churn data, and I am currently splitting it 80/20 for training and testing, respectively.  

I would like to regularly apply my model to active customer data to identify customers that are in danger of turning over.  Is this safe to do, or are there issues with reusing data in this context?",0,1
1376,2019-6-22,2019,6,22,6,c3h0lo,Semantic Sanity: an arXiv Sanity based research recommender,https://www.reddit.com/r/MachineLearning/comments/c3h0lo/semantic_sanity_an_arxiv_sanity_based_research/,Shooriki,1561153887,,1,1
1377,2019-6-22,2019,6,22,7,c3hkvt,[P] Semantic Sanity: an arXiv Sanity based research recommender,https://www.reddit.com/r/MachineLearning/comments/c3hkvt/p_semantic_sanity_an_arxiv_sanity_based_research/,Shooriki,1561156685,"Based on the very handy arXiv Sanity Preserver tool, this incarnation from the [Semantic Scholar](https://semanticscholar.org) team from the [allenai.org](https://allenai.org) non-profit has a few notable features to call out:

* Instantly updated recommendations, as you annotate papers you can refresh and see updated recommendations
* Currently covers *all* of arXiv CS + stat.ML
* You can create multiple, independent feeds to track different topics/areas

&amp;#x200B;

You can try Semantic Sanity out today at: [s2-sanity.apps.allenai.org/](http://s2-sanity.apps.allenai.org/) and let us know what you think!",14,17
1378,2019-6-22,2019,6,22,7,c3hp4v,[P] Building a CIFAR classifier neural network with PyTorch,https://www.reddit.com/r/MachineLearning/comments/c3hp4v/p_building_a_cifar_classifier_neural_network_with/,dkobran,1561157297,,0,1
1379,2019-6-22,2019,6,22,7,c3hqts,How to Win a Data Science Competition: Learn from Top Kagglers | Advanced Machine Learning Specialization,https://www.reddit.com/r/MachineLearning/comments/c3hqts/how_to_win_a_data_science_competition_learn_from/,_quanttrader_,1561157537,,0,1
1380,2019-6-22,2019,6,22,8,c3i11d,[D] The Dangers of Performance Metrics,https://www.reddit.com/r/MachineLearning/comments/c3i11d/d_the_dangers_of_performance_metrics/,KappaClosed,1561159017,,1,1
1381,2019-6-22,2019,6,22,10,c3jg7d,"[D] If convolution kernels encode a strong prior on spatial locality, what prior do transposed convolutions encode?",https://www.reddit.com/r/MachineLearning/comments/c3jg7d/d_if_convolution_kernels_encode_a_strong_prior_on/,toadsofbattle,1561166824,"Convolution layers are understood to encode an infinitely strong prior on spatial locality. (Ian Goodfellow's Deep Learning book, summary here https://medium.com/inveterate-learner/deep-learning-book-chapter-9-convolutional-networks-45e43bfc718d). From my understanding, this means that conv kernels are good at capturing regional patterns (e.g. a point here, a stroke there, etc).

Do strided transpose convolutions encode a prior of some kind as well? (an uneducated guess from my end - a preference for texture and pattern at a larger scale?)",3,7
1382,2019-6-22,2019,6,22,10,c3jnx9,[P] Multilabel Classifier With Closely Related Labels,https://www.reddit.com/r/MachineLearning/comments/c3jnx9/p_multilabel_classifier_with_closely_related/,MaxxBreak,1561168049,"Hey, I'm an industry outsider/hobbyist and trying to use AutoML text as a ternary classifier to prioritize incoming service request.  The thing I realized is that multi-label classification appears to be for unrelated labels, but I'm dealing with labels that sit on a line, ""low priority"", ""medium priority"", ""high priority"".  

&amp;#x200B;

I don't think this is the same problem as classifying with categorical labels such as ""car"", ""boat"", ""plane"".  The distance between low and high priority is much greater than medium, but the distance between a car, boat and plane are likely arbitrary.  Is there any way to capture this?  The only thing I've thought of so far is using a binary decision tree to check if low priority or not.  If not, then check high priority.  If not, then it's assumed to be medium priority.  Or does even that not work?  

&amp;#x200B;

Sorry, I'm still trying to learn terminology and more on this subject.",2,0
1383,2019-6-22,2019,6,22,11,c3jye8,Why tacotron2 use cnn instead of cbhg as encoder and postnet?,https://www.reddit.com/r/MachineLearning/comments/c3jye8/why_tacotron2_use_cnn_instead_of_cbhg_as_encoder/,yuenn,1561169741,[removed],0,1
1384,2019-6-22,2019,6,22,13,c3lcsi,[D] Generative Adversarial Networks - The Story So Far,https://www.reddit.com/r/MachineLearning/comments/c3lcsi/d_generative_adversarial_networks_the_story_so_far/,iyaja,1561178442,"Hi everyone. I just published a new blogpost which talks about the evolution of GANs over the last few years. You can check it out [here](https://blog.floydhub.com/gans-story-so-far/).

&amp;#x200B;

I think it's really interesting to see sample images generated from these models side by side. It really does give a sense of how fast this field has progressed. In just five years, we've gone from blurry, grayscale pixel arrays that vaguely resemble human faces to [thispersondoesnotexist](https://thispersondoesnotexist.com), which can easily fool most people on first glance.

&amp;#x200B;

Apart from image samples, I've also included links to papers, code, and other learning resources for each model. So this article could be a good place to start if you're a beginner looking to catch up with the latest GAN research.

&amp;#x200B;

Hope you enjoy it!",0,1
1385,2019-6-22,2019,6,22,13,c3ldo4,[D] Generative Adversarial Networks - The Story So Far,https://www.reddit.com/r/MachineLearning/comments/c3ldo4/d_generative_adversarial_networks_the_story_so_far/,iyaja,1561178612,"Hi everyone. I just published a new blog post which talks about the evolution of GANs over the last few years. You can check it out [here](https://blog.floydhub.com/gans-story-so-far/).

&amp;#x200B;

I think it's fascinating to see sample images generated from these models side by side. It really does give a sense of how fast this field has progressed. In just five years, we've gone from blurry, grayscale pixel arrays that vaguely resemble human faces to [thispersondoesnotexist](https://thispersondoesnotexist.com), which can easily fool most people on first glance.

&amp;#x200B;

Apart from image samples, I've also included links to papers, code, and other learning resources for each model. So this article could be an excellent place to start if you're a beginner looking to catch up with the latest GAN research.

&amp;#x200B;

I hope you enjoy it!",20,210
1386,2019-6-22,2019,6,22,15,c3mhih,LHD S.p.A Is World Leader in building Telescopic Forks for Automation Industry,https://www.reddit.com/r/MachineLearning/comments/c3mhih/lhd_spa_is_world_leader_in_building_telescopic/,lhd121,1561186238,[removed],0,1
1387,2019-6-22,2019,6,22,16,c3mkeb,[N] Top 8 Sources For Machine Learning and Analytics Datasets,https://www.reddit.com/r/MachineLearning/comments/c3mkeb/n_top_8_sources_for_machine_learning_and/,vadhavaniyafaijan,1561186821,,0,1
1388,2019-6-22,2019,6,22,16,c3mo86,LHD S.p.A Is Worldwide Leader in designing and building Telescopic Forks | Telescopic Fork |,https://www.reddit.com/r/MachineLearning/comments/c3mo86/lhd_spa_is_worldwide_leader_in_designing_and/,lhd121,1561187608,,0,1
1389,2019-6-22,2019,6,22,16,c3muu0,"I am going to start learning AI, this is my plan. Any suggestions?",https://www.reddit.com/r/MachineLearning/comments/c3muu0/i_am_going_to_start_learning_ai_this_is_my_plan/,IamAFireExtinguisher,1561189025,[removed],0,1
1390,2019-6-22,2019,6,22,18,c3nn36,Regression and Classification | Supervised Machine Learning,https://www.reddit.com/r/MachineLearning/comments/c3nn36/regression_and_classification_supervised_machine/,subhamroy021,1561195306,,0,1
1391,2019-6-22,2019,6,22,19,c3o4va,Github - Microsoft/MazeExplorer: Customisable 3D benchmark for assessing generalisation in Reinforcement Learning.,https://www.reddit.com/r/MachineLearning/comments/c3o4va/github_microsoftmazeexplorer_customisable_3d/,ForcedIndexing,1561199165,,0,1
1392,2019-6-22,2019,6,22,21,c3p3tq,[D] Non-research ML arbitrary FAANG,https://www.reddit.com/r/MachineLearning/comments/c3p3tq/d_nonresearch_ml_arbitrary_faang/,dramanautica,1561206302,"Hey, Im interested in applying for ML roles at FAANG companies for non-research positions. I know FB has an applied ML team but I dont know the equivalents in other companies. Can some one share he names of these teams and their experiences with them if possible?

Thanks!",0,0
1393,2019-6-22,2019,6,22,22,c3ppvs,Aren't Bert and XLNet training on the test sets for datasets like SQuAD?,https://www.reddit.com/r/MachineLearning/comments/c3ppvs/arent_bert_and_xlnet_training_on_the_test_sets/,uotsca,1561210216,[removed],0,1
1394,2019-6-22,2019,6,22,22,c3pun4,Request for references: Building models that infer physics.,https://www.reddit.com/r/MachineLearning/comments/c3pun4/request_for_references_building_models_that_infer/,darthmeshkat,1561211025,[removed],0,1
1395,2019-6-22,2019,6,22,22,c3pxbx,Machine Learning Approach in detecting if companies are the same,https://www.reddit.com/r/MachineLearning/comments/c3pxbx/machine_learning_approach_in_detecting_if/,sarmientoj24,1561211473,"I have a very large dataset of shipment data where the company names are not normalized (e.g. companies that are supposed to be the same are treated different, like Walmart Inc., Walmart Incorporated, Wallmart, WalmartInc.). A simple string normalization like regex would not do good on this. 

I have thought of TEXT SIMILARITY approach (Levenshtein Distance, Waro-Jinkler, etc.) which theoretically would work but would not do good in practice. One is that you should set a threshold and thresholds are different for each of them and various problems would arise.

Example: 

1. Large and Short Company Names would skew the threshold: (Walmart Inc - Walmart Inc. vs ABC Co. - In behalf of ABC Group of Co.)
2. Almost similar company names that are supposed to be different (ABC Company Thailand vs ABC Company Taiwan)

The problem for #1 is that Thresholding for text similarity ratio is tricky. 

The problem for #2 is that these companies have high ratio but are supposed to be different companies shipping different products (for example, ABC Thailand ships dresses while ABC Taiwan ships gadgets). 

I have shipping data that looks like this

&amp;#x200B;

|company name|products|company postal address|country|zip code|
|:-|:-|:-|:-|:-|
|ABC Company Thailand|1x dress pink|Bangkok Thailand|Thailand|11100|
|ABC Company Taiwan|20x Phones|Taipei, Taiwan|Taiwan|00291|
|Walmart California Inc.|100kgs banana|California|California|9929|
|In behalf of Walmart CaliforniaInc|200kgs meat|California|California|9929|

&amp;#x200B;

I am thinking of a solution that uses TEXT similarity metrics but across fields that could indicate that they are the same company (such as country, zip code, even products).

&amp;#x200B;

My proposed solution is

\- a new entry is compared to a constructed table consisting of columns that are distinguishing features (company name, zip code, country for example)

\- the new entry is only compared using the company name. the highest similarity is returned. And text similarity across different columns on new entry and selected data is produced.

\- text similarity ratio/points of these two is fed to a classifier that tells if they are similar companies or not. Basically, the input for the classifier is the text similarity ratio of the new entry and the nearest company name from the list.

&amp;#x200B;

Any easier approach? The approach should be able to tackle both an existing large data and new entry (for example, deduplication does not seem to tackle addition of new entries).",1,1
1396,2019-6-22,2019,6,22,22,c3pzco,[D] Any references on building models that infer real world physics?,https://www.reddit.com/r/MachineLearning/comments/c3pzco/d_any_references_on_building_models_that_infer/,darthmeshkat,1561211801,"Hi! I have been wondering about how to build a model that learns to infer the physics of environments from its input.

More specifically: how to build a model that takes input videos of, let's say, objects falling down ( a glass being pushed from a table, an apple falling from a tree, etc.) and then makes the inference that things tend to fall to the ground? The model does not have to figure out any mathematical formula for calculating the gravitational force. Learning the common sense physics as humans do is what I am primarily interested in.

Has anyone come across research that addresses this problem or a similar problem? I would greatly appreciate any help. Thanks in advance! :)",7,18
1397,2019-6-22,2019,6,22,23,c3q64f,The Role of AI and Machine Learning in Data Quality,https://www.reddit.com/r/MachineLearning/comments/c3q64f/the_role_of_ai_and_machine_learning_in_data/,rohit1221qq,1561212864,,0,1
1398,2019-6-22,2019,6,22,23,c3qk74,Acceptance Rate for CMU LTI program,https://www.reddit.com/r/MachineLearning/comments/c3qk74/acceptance_rate_for_cmu_lti_program/,arvind1096,1561215002,[removed],0,1
1399,2019-6-22,2019,6,22,23,c3ql9i,[P] Deep learning in Brancher.,https://www.reddit.com/r/MachineLearning/comments/c3ql9i/p_deep_learning_in_brancher/,LucaAmbrogioni,1561215178,"Brancher ([brancher.org](https://brancher.org)) is a new framework for deep probabilistic inferece based on PyTorch.

In this new tutorial, we show how to use Brancher to build stochastic deep learning models

&amp;#x200B;

[https://colab.research.google.com/drive/1YNwZpJgrsicK3Pz8igAktbdocm-gA9mV](https://colab.research.google.com/drive/1YNwZpJgrsicK3Pz8igAktbdocm-gA9mV)

&amp;#x200B;

We hope you'll enjoy it! Let us know if you have questions or suggestions for future improvements either here or on Twitter: @pybrancher",2,51
1400,2019-6-23,2019,6,23,0,c3qvrh,Another Book on Data Science,https://www.reddit.com/r/MachineLearning/comments/c3qvrh/another_book_on_data_science/,zelda_001,1561216708,,0,1
1401,2019-6-23,2019,6,23,0,c3r2og,[P] Sliding batch of training data to look at past X number of values,https://www.reddit.com/r/MachineLearning/comments/c3r2og/p_sliding_batch_of_training_data_to_look_at_past/,dabirdman360,1561217750,"Hi all. I am very new to the ML world but I have been thinking about mapping a project out using a bunch of my own blood glucose data I have and did not know where to start with a sliding batch of input data? In order for a model to predict the next few values, it would need to know what happen in the last X number of minutes/ X number of data points. Does anyone have any links or textbooks to checkout on models like this? Anything is appreciated!",1,1
1402,2019-6-23,2019,6,23,1,c3rjez,"[D] ImageNet, MNIST for regression with image input",https://www.reddit.com/r/MachineLearning/comments/c3rjez/d_imagenet_mnist_for_regression_with_image_input/,ultrakoge,1561220177,Does any one know if there exists popular benchmark in regression problem with image input such as ImageNet and MNIST for classification?,1,2
1403,2019-6-23,2019,6,23,1,c3ruah,Title,https://www.reddit.com/r/MachineLearning/comments/c3ruah/title/,karenactionsitafaal,1561221668,,0,1
1404,2019-6-23,2019,6,23,1,c3s1f6,[D] Machine Learning Approach in detecting if companies are the same,https://www.reddit.com/r/MachineLearning/comments/c3s1f6/d_machine_learning_approach_in_detecting_if/,sarmientoj24,1561222652,"I have a very large dataset of shipment data where the company names are not normalized (e.g. companies that are supposed to be the same are treated different, like Walmart Inc., Walmart Incorporated, Wallmart, WalmartInc.). A simple string normalization like regex would not do good on this.

I have thought of TEXT SIMILARITY approach (Levenshtein Distance, Waro-Jinkler, etc.) which theoretically would work but would not do good in practice. One is that you should set a threshold and thresholds are different for each of them and various problems would arise.

Example:

1. Large and Short Company Names would skew the threshold: (Walmart Inc - Walmart Inc. vs ABC Co. - In behalf of ABC Group of Co.)
2. Almost similar company names that are supposed to be different (ABC Company Thailand vs ABC Company Taiwan)

The problem for #1 is that Thresholding for text similarity ratio is tricky.

The problem for #2 is that these companies have high ratio but are supposed to be different companies shipping different products (for example, ABC Thailand ships dresses while ABC Taiwan ships gadgets).

I have shipping data that looks like this

&amp;#x200B;

|company name|products|company postal address|country|zip code|
|:-|:-|:-|:-|:-|
|ABC Company Thailand|1x dress pink|Bangkok Thailand|Thailand|11100|
|ABC Company Taiwan|20x Phones|Taipei, Taiwan|Taiwan|00291|
|Walmart California Inc.|100kgs banana|California|California|9929|
|In behalf of Walmart CaliforniaInc|200kgs meat|California|California|9929|

&amp;#x200B;

I am thinking of a solution that uses TEXT similarity metrics but across fields that could indicate that they are the same company (such as country, zip code, even products).

&amp;#x200B;

My proposed solution is

\- a new entry is compared to a constructed table consisting of columns that are distinguishing features (company name, zip code, country for example)

\- the new entry is only compared using the company name. the highest similarity is returned. And text similarity across different columns on new entry and selected data is produced.

\- text similarity ratio/points of these two is fed to a classifier that tells if they are similar companies or not. Basically, the input for the classifier is the text similarity ratio of the new entry and the nearest company name from the list.

&amp;#x200B;

Any easier approach? The approach should be able to tackle both an existing large data and new entry (for example, deduplication does not seem to tackle addition of new entries). Thanks!",18,1
1405,2019-6-23,2019,6,23,2,c3s2kw,How to build a CNN Binary Image Classifier using Tensorflow? - YouTube,https://www.reddit.com/r/MachineLearning/comments/c3s2kw/how_to_build_a_cnn_binary_image_classifier_using/,dasaradhsk,1561222807,,0,1
1406,2019-6-23,2019,6,23,2,c3s3uo,Adapters: A Compact and Extensible Transfer Learning Method for NLP,https://www.reddit.com/r/MachineLearning/comments/c3s3uo/adapters_a_compact_and_extensible_transfer/,omarsar,1561222978,,0,1
1407,2019-6-23,2019,6,23,2,c3soen,Importing data with Tensorflow or Keras,https://www.reddit.com/r/MachineLearning/comments/c3soen/importing_data_with_tensorflow_or_keras/,juliandwain,1561225894,[removed],0,1
1408,2019-6-23,2019,6,23,3,c3su1r,Object detection with Neuroevolution,https://www.reddit.com/r/MachineLearning/comments/c3su1r/object_detection_with_neuroevolution/,kartinko28,1561226681,[removed],0,1
1409,2019-6-23,2019,6,23,3,c3svu0,[D] Discord Reading groups in Theoretical Machine Learning,https://www.reddit.com/r/MachineLearning/comments/c3svu0/d_discord_reading_groups_in_theoretical_machine/,clarice_lispectacula,1561226947,"I created a discord server to organise open-ended, collaborative reading groups for theoretical machine learning. We are open to all topics and we discuss papers, books etc. I feel like I learn much better when working with other people. If you feel the same way come join us at [https://discord.gg/K2Pwm2E](https://discord.gg/K2Pwm2E)",28,48
1410,2019-6-23,2019,6,23,3,c3sw8c,[D] Online AI Courses Difference,https://www.reddit.com/r/MachineLearning/comments/c3sw8c/d_online_ai_courses_difference/,chick3234,1561227003,"Hey,

So I am taking CS231N online currently. I saw that Stanford also released the videos for the CS230 and CS224N classes. I intend to take the CS224N class but looking through CS230, I was wondering what the adding value of it is if I take CS224N and CS231N. Is there material there covered that isn't covered in either lecture series or does it simply rehash a lot of the concepts of the other two classes?",2,15
1411,2019-6-23,2019,6,23,3,c3tdh5,A question about word prediction RNNs,https://www.reddit.com/r/MachineLearning/comments/c3tdh5/a_question_about_word_prediction_rnns/,ivxnc,1561229660,[removed],0,1
1412,2019-6-23,2019,6,23,4,c3ti6u,Teach an AI to play a game by having it watch you play?,https://www.reddit.com/r/MachineLearning/comments/c3ti6u/teach_an_ai_to_play_a_game_by_having_it_watch_you/,Bushfries,1561230387,[removed],0,1
1413,2019-6-23,2019,6,23,4,c3tkmh,Final Year computer engineering project on Machine Learning,https://www.reddit.com/r/MachineLearning/comments/c3tkmh/final_year_computer_engineering_project_on/,joeljuzreddit,1561230776,"&amp;#x200B;

My team has been granted by an NGO to work on a project which helps them in any way.  One of their many tasks is that they provide leftover or extra food to the needy ones. This project will be our Final Year Project for our BTech graduation.

&amp;#x200B;

My team is interested in learning machine learning and data science and plan on incorporating it into the project. Can we receive any suggestion for Project Topics we can work on through the final year(2 semesters)?",0,1
1414,2019-6-23,2019,6,23,4,c3tsf4,[P] Chat Simulation to detect and prevent Cyberbullying - Participants needed!,https://www.reddit.com/r/MachineLearning/comments/c3tsf4/p_chat_simulation_to_detect_and_prevent/,Birnenbrinchen,1561231998,"I'm studying Information Science &amp; Computational Linguistics in Germany. We're doing a master thesis on the prevention of cyberbullying and hate speech in online communication tools and social media.In order to get real life data to test the algorithm, we implemented a chat room:   
[http://bullychat.inf-d.de](http://bullychat.inf-d.de) 

You can send messages, which will then be analyzed by our algorithm. You'll see whether the message was deemed appropriate and you can approve or disapprove the result. More instructions in the link!   
You don't have to reveal any personal information and all data will purely be used for scientific purposes and will not be made public.   


We'd be grateful for every participant! Just write about anything and try different ways to insult. Thank you!",15,9
1415,2019-6-23,2019,6,23,5,c3ulsm,What are the Benefits of Data Science and Machine Learning Bootcamp?  in Silicon Valley,https://www.reddit.com/r/MachineLearning/comments/c3ulsm/what_are_the_benefits_of_data_science_and_machine/,Magniminda,1561236665,,0,1
1416,2019-6-23,2019,6,23,7,c3vg7b,Will Machine Learning become automated?,https://www.reddit.com/r/MachineLearning/comments/c3vg7b/will_machine_learning_become_automated/,dirtymikethelegend,1561241459,[removed],0,1
1417,2019-6-23,2019,6,23,7,c3vguy,https://talktotransformer.com,https://www.reddit.com/r/MachineLearning/comments/c3vguy/httpstalktotransformercom/,DrLukeSkywalker,1561241561,,0,1
1418,2019-6-23,2019,6,23,7,c3vihz,Anyone has an idea or an approach on how to compete against Google SE or Youtube? Just like iOS vs. Android?,https://www.reddit.com/r/MachineLearning/comments/c3vihz/anyone_has_an_idea_or_an_approach_on_how_to/,facehall,1561241811,[removed],0,1
1419,2019-6-23,2019,6,23,8,c3wesd,[R] How can I improve my material segmentations? UPDATED,https://www.reddit.com/r/MachineLearning/comments/c3wesd/r_how_can_i_improve_my_material_segmentations/,EmielBoss,1561246956,"I am trying to perform material segmentation (essentially semantic segmentation with respect to materials) on street-view imagery. My datasets only has ground truth for select regions, so not all pixels have a label, and I calculate loss and metrics only within these ground truth regions. I use [Semantic FPN](https://arxiv.org/pdf/1901.02446.pdf) (with the ResNet-50 backbone pre-trained on ImageNet), a learning rate of 0.001, momentum of 0.8, and learning rate is divided by 4 if there is no validations loss improvement after three epochs. My loss function is a per-pixel multiclass cross-entropy loss.

My dataset is extremely limited. Not only are not all pixels classified, I also only have 700 images and a severe class imbalance. I tried tackling this imbalance through loss class weighting (based on the number of ground truth pixels for each respective class, i.e. their area sizes), but it barely helps. I also possess, for every image, a depth map, which I (can) supply as a fourth channel to the input layer.

[A table of results](https://imgur.com/a/v7VFgnk)

[Visualizations of images trained only on RGB](https://imgur.com/a/oiSrLeM)

[Visualizations of images trained on RGBD](https://imgur.com/a/LDiTQGK)

[Visualizations of images trained only on RGB, but with class loss weighting](https://imgur.com/a/6Jv6mtu)

[Visualizations of images trained only RGBD, and with class loss weighting](https://imgur.com/a/oc2asRj)

&amp;#x200B;

Performance is pretty crappy. What's more, there is very little difference between results of my four experiments. Why is this? I would expect that the addition of depth information (which encodes surface normals and perhaps texture information; pretty discriminitive information). Besides the overall metrics being rather low, the predictions are very messy, and the networks rarely, if ever, predicts ""small"" classes (in terms of area size), e.g. plastic or gravel. This is to be expected with such a small amount of data, but I was wondering if there are any ""performance hacks"" that can boost my network, or if I am missing any obvious stuff? Or is data likely the only bottleneck here? Any suggestions are greatly appreciated!

&amp;#x200B;

PS. I also tried a simple ResNet-50 FCN (I simply upsample ResNet's output until I have the same resolution; there aren't even skip connections), and the results are worse, but at least they are [smooth](https://imgur.com/a/nV9vHGl). Why are these more smooth?  
  
UPDATE: Last time I got the advice to use [(generalized) dice loss](https://arxiv.org/pdf/1707.03237.pdf), which is specifically designed to combat class imbalance in semantic segmentation problems. However, in my case, the [opposite happens](https://imgur.com/a/LKwKLk8). Why? I do not use the per-class weights, which in the paper is calculated as the inverse of the squared area of the class' ground truth. Even if I just use the inverse of the unsquared area, I just get a loss of 1 all the time. This is because the ratio of nominator to denominator becomes too small. I can't wrap my head around why that is. [I also posted this question more thoroughly to StackExchange](https://stats.stackexchange.com/questions/414244/why-are-weights-being-used-in-generalized-dice-loss-and-why-cant-i). I am quite at a loss at what else to do to improve my results. I thought depth of my network might be a bottleneck? I now use ResNet50, and have [trouble implementing deeper networks](https://stackoverflow.com/questions/56719817/tensorflow-keras-how-can-i-use-the-keras-applications-resnext-in-eager-executi). Any advice is greatly appreciated!",1,1
1420,2019-6-23,2019,6,23,9,c3wqro,[Discussion] Is Hamiltonian Monte Carlo just MCMC with momentum?,https://www.reddit.com/r/MachineLearning/comments/c3wqro/discussion_is_hamiltonian_monte_carlo_just_mcmc/,BanLeCun,1561248877,,13,37
1421,2019-6-23,2019,6,23,9,c3x02h,"Face recognition with OpenCV, Python, and deep learning",https://www.reddit.com/r/MachineLearning/comments/c3x02h/face_recognition_with_opencv_python_and_deep/,mikevhenderson,1561250410,,0,1
1422,2019-6-23,2019,6,23,9,c3x1e1,{Discussion} Are ML model documentations hard to understand and build on top of? or are they easy for you?,https://www.reddit.com/r/MachineLearning/comments/c3x1e1/discussion_are_ml_model_documentations_hard_to/,himanshuragtah1,1561250639,[removed],0,1
1423,2019-6-23,2019,6,23,9,c3x2jd,[Discussion] Are ML model documentations hard to understand and build on top of? or are they easy for you?,https://www.reddit.com/r/MachineLearning/comments/c3x2jd/discussion_are_ml_model_documentations_hard_to/,himanshuragtah1,1561250840,"Do model documentations ever get in the way of your work? or are they easy for you?

the typical things I try to find out when reading dense documentation

\-What is the input, output?

\-How I should train the model?

\-Will it work on my dataset?

Anything else you try to gather when understanding model documentation?",1,3
1424,2019-6-23,2019,6,23,10,c3x788,[D] - Exploring Methods of Neural Style Transfer,https://www.reddit.com/r/MachineLearning/comments/c3x788/d_exploring_methods_of_neural_style_transfer/,Nick_Pyth,1561251645,"I made a [repository](https://github.com/Nick-Morgan/neural-style-transfer) which explores 2 methods ([Gatys, 2015](https://arxiv.org/pdf/1508.06576.pdf) and [Johnson, 2016](https://arxiv.org/pdf/1603.08155.pdf)) of Neural Style Transfer. I've included a link to Google Collab in the repository, making it easy to run the code on your own images.

&amp;#x200B;

I would love feedback on the repository, as well as any suggestions for future reading. I find this topic fascinating.",11,97
1425,2019-6-23,2019,6,23,11,c3yb0u,Why is the actual reason for exploding gradients?,https://www.reddit.com/r/MachineLearning/comments/c3yb0u/why_is_the_actual_reason_for_exploding_gradients/,revolutionizescience,1561258589,"In case of vanishing gradients,
1. We know that product of 2 numbers between (0,1) gives still smaller number. 
eg: 0.1 * 0.3 = 0.03
2. So if gradients are smaller then in  case of deep network ,this effects the earlier layers. 
eg. (0.5)^10 = exponentially small. 
3. But why does gradients are smaller in the first place, because we can see that by the graph of derivative of sigmoid or tanh . The range is (0,0.25) for sigmoid and (0,1) for tanh.

So, I want to understand that how does gradients become larger, which in turn causes exploding gradient problem. Help needed.",0,1
1426,2019-6-23,2019,6,23,13,c3ywjy,[D] [US20190180165A1] Generating representations of input sequences using neural networks (Google Patents),https://www.reddit.com/r/MachineLearning/comments/c3ywjy/d_us20190180165a1_generating_representations_of/,penpatience,1561262442,,0,1
1427,2019-6-23,2019,6,23,14,c3zk1w,Detecting AI-Synthesized Speech Using Bispectral Analysis,https://www.reddit.com/r/MachineLearning/comments/c3zk1w/detecting_aisynthesized_speech_using_bispectral/,ebadawy,1561266858,,0,1
1428,2019-6-23,2019,6,23,15,c3zxmk,I am starting fast.ai videos today . Anyone who has completed it and can tell me how much time I should allocate for the same . I am planning to complete in a week both machine learning and deep learning .,https://www.reddit.com/r/MachineLearning/comments/c3zxmk/i_am_starting_fastai_videos_today_anyone_who_has/,shwetashri,1561269626,[removed],0,1
1429,2019-6-23,2019,6,23,15,c40bun,"Find Updated and Free Resources for Artificial Intelligence, Machine Learning, Data Science, Deep Learning, Mathematics, Python, and R Programming. [ COVERED COURSES: STANFORD, HARVARD, OXFORD, CORNELL, GOOGLE,ETC. ]",https://www.reddit.com/r/MachineLearning/comments/c40bun/find_updated_and_free_resources_for_artificial/,ai-lover,1561272710,,0,1
1430,2019-6-23,2019,6,23,17,c40v06,[D] How to deploy a machine learning model on AWS,https://www.reddit.com/r/MachineLearning/comments/c40v06/d_how_to_deploy_a_machine_learning_model_on_aws/,puneet_saini,1561276960,"Here's an [article](https://medium.com/@puneet29/how-i-built-and-deployed-my-first-machine-learning-project-4c75d1effe4e?source=friends_link&amp;sk=661c75cff27a77554c8292677216d449) for beginners on how I deployed my machine learning model using Flask and Gunicorn on AWS. Check out the [GitHub repo](https://github.com/puneet29/stylizeapp) for the same. I would love to receive any recommendations and reviews. Check out the web app at bit.ly/stylizeapp  

It is based on Artistic Neural Style Transfer, the paper by [Johnson](https://arxiv.org/abs/1603.08155). The [Gatys](https://arxiv.org/abs/1508.06576) implementation is given [here](https://github.com/puneet29/notebooks/blob/master/Image_Style_Transfer.ipynb).",52,153
1431,2019-6-23,2019,6,23,17,c4152j,NOOB Question - Basic Implementation of DNN in Python,https://www.reddit.com/r/MachineLearning/comments/c4152j/noob_question_basic_implementation_of_dnn_in/,nblogist,1561279095,[removed],0,1
1432,2019-6-23,2019,6,23,17,c41axw,Face Detection and Recognition Project.,https://www.reddit.com/r/MachineLearning/comments/c41axw/face_detection_and_recognition_project/,harsh_sagar,1561280101,[removed],0,1
1433,2019-6-23,2019,6,23,17,c41ch6,Top 5 Innovative Applications of Machine Learning in Personal and Professional Space | Analytics Insight,https://www.reddit.com/r/MachineLearning/comments/c41ch6/top_5_innovative_applications_of_machine_learning/,analyticsinsight,1561280349,,0,1
1434,2019-6-23,2019,6,23,18,c41ewf,[R] How can I improve my material segmentations? UPDATED,https://www.reddit.com/r/MachineLearning/comments/c41ewf/r_how_can_i_improve_my_material_segmentations/,EmielBoss,1561280690,"I am trying to perform material segmentation (essentially semantic segmentation with respect to materials) on street-view imagery. My datasets only has ground truth for select regions, so not all pixels have a label, and I calculate loss and metrics only within these ground truth regions. I use [Semantic FPN](https://arxiv.org/pdf/1901.02446.pdf) (with the ResNet-50 backbone pre-trained on ImageNet), a learning rate of 0.001, momentum of 0.8, and learning rate is divided by 4 if there is no validations loss improvement after three epochs. My loss function is a per-pixel multiclass cross-entropy loss.

My dataset is extremely limited. Not only are not all pixels classified, I also only have 700 images and a severe class imbalance. I tried tackling this imbalance through loss class weighting (based on the number of ground truth pixels for each respective class, i.e. their area sizes), but it barely helps. I also possess, for every image, a depth map, which I (can) supply as a fourth channel to the input layer.

[A table of results](https://imgur.com/a/v7VFgnk)

[Visualizations of images trained only on RGB](https://imgur.com/a/oiSrLeM)

[Visualizations of images trained on RGBD](https://imgur.com/a/LDiTQGK)

[Visualizations of images trained only on RGB, but with class loss weighting](https://imgur.com/a/6Jv6mtu)

[Visualizations of images trained only RGBD, and with class loss weighting](https://imgur.com/a/oc2asRj)

&amp;#x200B;

Performance is pretty crappy. What's more, there is very little difference between results of my four experiments. Why is this? I would expect that the addition of depth information (which encodes surface normals and perhaps texture information; pretty discriminitive information). Besides the overall metrics being rather low, the predictions are very messy, and the networks rarely, if ever, predicts ""small"" classes (in terms of area size), e.g. plastic or gravel. This is to be expected with such a small amount of data, but I was wondering if there are any ""performance hacks"" that can boost my network, or if I am missing any obvious stuff? Or is data likely the only bottleneck here? Any suggestions are greatly appreciated!

&amp;#x200B;

PS. I also tried a simple ResNet-50 FCN (I simply upsample ResNet's output until I have the same resolution; there aren't even skip connections), and the results are worse, but at least they are [smooth](https://imgur.com/a/nV9vHGl). Why are these more smooth?  
  
UPDATE: Last time I got the advice to use [(generalized) dice loss](https://arxiv.org/pdf/1707.03237.pdf), which is specifically designed to combat class imbalance in semantic segmentation problems. However, in my case, the [opposite happens](https://imgur.com/a/LKwKLk8). Why? I do not use the per-class weights, which in the paper is calculated as the inverse of the squared area of the class' ground truth. Even if I just use the inverse of the unsquared area, I just get a loss of 1 all the time. This is because the ratio of nominator to denominator becomes too small. I can't wrap my head around why that is. [I also posted this question more thoroughly to StackExchange](https://stats.stackexchange.com/questions/414244/why-are-weights-being-used-in-generalized-dice-loss-and-why-cant-i). I am quite at a loss at what else to do to improve my results. I thought depth of my network might be a bottleneck? I now use ResNet50, and have [trouble implementing deeper networks](https://stackoverflow.com/questions/56719817/tensorflow-keras-how-can-i-use-the-keras-applications-resnext-in-eager-executi). Any advice is greatly appreciated!",5,0
1435,2019-6-23,2019,6,23,18,c41ezc,Github Link,https://www.reddit.com/r/MachineLearning/comments/c41ezc/github_link/,harsh_sagar,1561280702,[removed],0,1
1436,2019-6-23,2019,6,23,18,c41owe,[D] The best way of clustering of articles for news aggregator?,https://www.reddit.com/r/MachineLearning/comments/c41owe/d_the_best_way_of_clustering_of_articles_for_news/,TastyInternet,1561282138," Here is the case, I get news from several news sources every minute. Basically, they are WordPress post, as the script we are using for news aggregator is based on Wordpress Plugin.   
Now, we are fetching those post to Laravel site via one of those Wordpress to Laravel([https://github.com/corcel/corcel](https://www.freelancer.com/users/l.php?url=https:%2F%2Fgithub.com%2Fcorcel%2Fcorcel&amp;sig=e2c6d6e80ea5d1482d6963c37185eefac83c4e8cfc8768cffea6ee0ec8ebbcb9)).   


So far, I'm using TextRank([https://github.com/DavidBelicza/PHP-Science-TextRank](https://www.freelancer.com/users/l.php?url=https:%2F%2Fgithub.com%2FDavidBelicza%2FPHP-Science-TextRank&amp;sig=32d3346761c38becb2a85ec57da92e5e10bce3b6f9d7e943802e4dda8fc030c8)), we can do following for any posts:  
Find sentences,  
Remove stopwords,  
Create integer values by find and count the matching words,  
Change the integer values by the related words' integer values,  
Normalize values to create scores,  
Order by scores  
To be more precise, we can get a bag of words from any WordPress Post.   
Now, I am looking for perfect algorithms, in this case, that will be able to cluster/ group lists of articles into the same Coverage table. Coverage can have any data, what I think is we need coverage ID field, and a field that accepts an array of post ID that is similar to each other and has the same Coverage ID.   
We also have a table called newsTag, that has the following field: postId, most important topic mentioned. You can ignore the topic mentioned because, it depends on only the topic that is a category, so if we cluster based on a topic mentioned from newsTag, we will be limiting clustering ability because in some post there is no topic mentioned.   
I've looked up a few algorithms like TF-LDF, cosine similarity, k- means, etc. But I am not sure which fits perfectly in this case, basically, a dynamic algorithm that doesn't depend on a number of articles, so we can clustering new articles in real-time.  Thank you for reading, appreciate any kind of help!",1,2
1437,2019-6-23,2019,6,23,18,c41yli,Machine Learning Bootcamp: Become a ML Engineer in 6 months. Job Guaranteed.,https://www.reddit.com/r/MachineLearning/comments/c41yli/machine_learning_bootcamp_become_a_ml_engineer_in/,HannahHumphreys,1561283552,[removed],0,1
1438,2019-6-23,2019,6,23,19,c42aoj,[R] Understanding and correcting pathologies in the training of learned optimizers,https://www.reddit.com/r/MachineLearning/comments/c42aoj/r_understanding_and_correcting_pathologies_in_the/,tsauri,1561285250,,0,1
1439,2019-6-23,2019,6,23,20,c42ux3,Google AutoML Vision,https://www.reddit.com/r/MachineLearning/comments/c42ux3/google_automl_vision/,kartinko28,1561288129,[removed],0,1
1440,2019-6-23,2019,6,23,20,c42y9q,So who's going to ICLR 2020?,https://www.reddit.com/r/MachineLearning/comments/c42y9q/so_whos_going_to_iclr_2020/,l3rahan,1561288584,,0,1
1441,2019-6-23,2019,6,23,20,c439nj,Help with Adversarial Examples,https://www.reddit.com/r/MachineLearning/comments/c439nj/help_with_adversarial_examples/,A27_97,1561290145,[removed],0,1
1442,2019-6-23,2019,6,23,21,c43il2,Looking for papers using ML/DL algorithms in optimization problems.,https://www.reddit.com/r/MachineLearning/comments/c43il2/looking_for_papers_using_mldl_algorithms_in/,BeingSorena,1561291331,[removed],0,1
1443,2019-6-23,2019,6,23,22,c449zd,[R] Looking for papers using ML/DL methods to solve an optimization problem.,https://www.reddit.com/r/MachineLearning/comments/c449zd/r_looking_for_papers_using_mldl_methods_to_solve/,BeingSorena,1561294856," Hello everyone,

I'm looking for papers/reports/theses that use ML/DL algorithms to tune hyperparameters of an optimization problem. It could even be for another ML/DL algorithm. If there's such anything, please cite them here.

Thank you, all.",0,0
1444,2019-6-23,2019,6,23,22,c44a4g,Book! Genes vs Cultures vs Consciousness,https://www.reddit.com/r/MachineLearning/comments/c44a4g/book_genes_vs_cultures_vs_consciousness/,Acampero,1561294873,"I just self-published a short book about the nature, the history and the development of the mind.

It is definitely not comprehensive, but I genuinely think it is insightful and fun.

In case you want to check it out (and help with amazon's engine), it is on paperback and also free on kindle for three days.

Thanks!!

&amp;#x200B;

Andres Campero

Brain and Cognitive Sciences Department

Computer Science and AI Lab 

MIT

&amp;#x200B;

&amp;#x200B;

*Processing img pllu00xot3631...*",0,1
1445,2019-6-23,2019,6,23,22,c44evt,LSTM subsequence to sequence prediction,https://www.reddit.com/r/MachineLearning/comments/c44evt/lstm_subsequence_to_sequence_prediction/,cptn_iglo,1561295454,[removed],0,1
1446,2019-6-23,2019,6,23,22,c44jvy,[P] Subsequence to sequence prediction LSTM / stacked LSTM,https://www.reddit.com/r/MachineLearning/comments/c44jvy/p_subsequence_to_sequence_prediction_lstm_stacked/,cptn_iglo,1561296065,"Hi :),

I'm currently on a project, where I'm trying to predict the next value of a sequence.

&amp;#x200B;

The data looks as follows:

y: the value to predict is captured ones a day.

x: there is an input sequence of around 2000 timesteps for every day

&amp;#x200B;

I would like to predict the next day's value of y, i.e. y\_{t+1}. However y\_{t+1} is assumed to be not only dependent on the values of x but also on the history of y, i.e. y\_t, y\_{t-1}, y\_{t-n}. I'm wondering how I could implement this idea in a LSTM-structure.

My idea is a network that looks like that:

&amp;#x200B;

https://i.redd.it/b9x7k87nw3631.png

Does that make sense or am I on the wrong track there?

How would you implement such a model in Keras? My idea was to make a network that looks like: x -&gt; TimeDistributed(LSTM1) -&gt;  LSTM2 -&gt; y",9,9
1447,2019-6-23,2019,6,23,22,c453fs,[D] What's the difference between an agent like Alphastar and real life animals?,https://www.reddit.com/r/MachineLearning/comments/c453fs/d_whats_the_difference_between_an_agent_like/,BatBast,1561298395,"Alphastar is a neural network that plays a real time game. 

Biological brains are made out of trillions of neurons connected to each other. Aren't animals just neural networks that are playing a game? The game is a 3d survival game that the network controls through a body. In order to win it has to learn to: Know to avoid predators, hunt food, reproduce ect.",11,0
1448,2019-6-23,2019,6,23,23,c4559p,Is purchasing a RTX 2080 laptop worth it for learning Deep Learning (June 2019)?,https://www.reddit.com/r/MachineLearning/comments/c4559p/is_purchasing_a_rtx_2080_laptop_worth_it_for/,Mihir_Gajjar,1561298601,[removed],0,1
1449,2019-6-23,2019,6,23,23,c45796,"Learn the basics of Machine Learning and various models like Artificial Neural Networks, Support Vector Machine, Bayesian Networks, Genetic Algorithms and Decision Tree",https://www.reddit.com/r/MachineLearning/comments/c45796/learn_the_basics_of_machine_learning_and_various/,AnonymousHacker789,1561298819,,0,1
1450,2019-6-23,2019,6,23,23,c45ne5,[D] Why we should focus experiments and solutions mainly on climate and healthcare issues only,https://www.reddit.com/r/MachineLearning/comments/c45ne5/d_why_we_should_focus_experiments_and_solutions/,superaromatic,1561300727,"We're globally in a perilous situation with a severe risk of runaway climate change due to [major Arctic gas leaks](http://www.bbc.com/future/story/20190612-the-poisons-released-by-melting-arctic-ice) even if we stop all human emissions today. Unless you're a ML researcher, I call upon everyone to focus their voluntary experimental efforts and solutions mainly on climate change and/or healthcare related problems only. This means forgetting about other toy problems like the Titanic which was a polluting behemoth anyway. Under no circumstances would you want to work for a firm that seriously and actively harms the climate. If you read and understand the linked article, you will acknowledge that our global civilization has never been at a bigger risk of collapse. I believe that a single person can make a big difference if you try.

As for the motivation for healthcare, it should be more obvious to you, especially if you're 30 or older. If you're younger, you will get there and will then understand.",15,0
1451,2019-6-24,2019,6,24,0,c46ptj,"DataTau (Hacker News clone for Data Science stuff) has been down for a month, so we cloned it :-)",https://www.reddit.com/r/MachineLearning/comments/c46ptj/datatau_hacker_news_clone_for_data_science_stuff/,thegurus,1561305182,,1,1
1452,2019-6-24,2019,6,24,1,c46vas,Subtract sample used in audio,https://www.reddit.com/r/MachineLearning/comments/c46vas/subtract_sample_used_in_audio/,macob12432,1561305789,[removed],0,1
1453,2019-6-24,2019,6,24,1,c477eo,Is it possible to recreate a voice from Audio and transcriptions ?,https://www.reddit.com/r/MachineLearning/comments/c477eo/is_it_possible_to_recreate_a_voice_from_audio_and/,Kintarrro,1561307158,[removed],0,1
1454,2019-6-24,2019,6,24,1,c47cd9,[R] Deep Set Prediction Networks,https://www.reddit.com/r/MachineLearning/comments/c47cd9/r_deep_set_prediction_networks/,Cyanogenoid,1561307701,,7,8
1455,2019-6-24,2019,6,24,1,c47ceq,[R] A General and Adaptive Robust Loss Function,https://www.reddit.com/r/MachineLearning/comments/c47ceq/r_a_general_and_adaptive_robust_loss_function/,jnbrrn,1561307706,"Hi /ml, I presented a paper at CVPR last week that seemed to go over well, so I thought I'd promote it beyond the vision community.

Video (much more approachable than the paper, and identical to the talk):  [https://www.youtube.com/watch?v=BmNKbnF69eY](https://www.youtube.com/watch?v=BmNKbnF69eY)

Abstract: *We present a generalization of the Cauchy/Lorentzian, Geman-McClure, Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2 loss functions. By introducing robustness as a continuous parameter, our loss function allows algorithms built around robust loss minimization to be generalized, which improves performance on basic vision tasks such as registration and clustering. Interpreting our loss as the negative log of a univariate density yields a general probability distribution that includes normal and Cauchy distributions as special cases. This probabilistic interpretation enables the training of neural networks in which the robustness of the loss automatically adapts itself during training, which improves performance on learning-based tasks such as generative image synthesis and unsupervised monocular depth estimation, without requiring any manual parameter tuning.*

Arxiv:  [https://arxiv.org/abs/1701.03077](https://arxiv.org/abs/1701.03077)

TensorFlow Code:  [https://github.com/google-research/google-research/tree/master/robust\_loss](https://github.com/google-research/google-research/tree/master/robust_loss)

PyTorch Code:  [https://github.com/jonbarron/robust\_loss\_pytorch](https://github.com/jonbarron/robust_loss_pytorch)",52,118
1456,2019-6-24,2019,6,24,2,c486g7,"[D] In your experience, how necessary is a PhD if you want to specialize in Deep Learning?",https://www.reddit.com/r/MachineLearning/comments/c486g7/d_in_your_experience_how_necessary_is_a_phd_if/,itcouldbemuchworse,1561311021,"As an undergrad student who graduates next year, I was looking for a masters to apply to, but I realized that there are no master programs specialized in deep learning, which is the topic I love and have studied more from machine learning. 

Also, if you look through the internships and job opportunities for ML in the big tech companies, almost all of them ask for a PhD as a minimum requisite.

So, in your experience, how necessary is a PhD if you want to work in the Deep Learning field? What are the pros and cons?",7,7
1457,2019-6-24,2019,6,24,2,c48hc0,What's the interview for the job like ?,https://www.reddit.com/r/MachineLearning/comments/c48hc0/whats_the_interview_for_the_job_like/,Diggu03,1561312233,[removed],0,1
1458,2019-6-24,2019,6,24,3,c48v64,"[N] Silicon Valley Hackathon with Machine Learning / Artificial Intelligence Competition. Sat, Jun 29  Sun, Jun 30. Allows remote virtual participation if one member is president at Silicon Valley (website has section for teaming up and looking for teams if you don't have one)",https://www.reddit.com/r/MachineLearning/comments/c48v64/n_silicon_valley_hackathon_with_machine_learning/,BatmantoshReturns,1561313601,"Here are the links

https://www.eventbrite.com/e/angelhack-2019-silicon-valley-tickets-58773239341

https://angelhack-2019-silicon-valley.devpost.com/

Details on the ML competition 

&gt; All attendees will receive $100 in AWS credits during the Hackathon
&gt; 
&gt; Challenge: Use any AWS Artificial Intelligence or Machine Learning service to integrate intelligence, learning, analytics or security into your project.
&gt; 
&gt; Prize:
&gt; 
&gt; The team that integrates the best AI/ML solution will win one (1) AWS Deeplens per team member
&gt; 
&gt; The runner up team will win one (1) Fire TV Cube per team member
&gt; 
&gt; The 2nd runner up team will win one (1) Adabox per team member

I emailed the organizer and ask if my remote teammates can participate virtually if I'm there in person, and they said yes. 

There's a page for people looking for teammates, you can connect with them there

https://angelhack-2019-silicon-valley.devpost.com/participants?search%5Bonly_looking_for_teammates%5D=1

My team also has some slots open. Feel free to PM to share backgrounds.",1,0
1459,2019-6-24,2019,6,24,3,c4947c,[D] How much time/energy do you spend keeping abreast of new research?,https://www.reddit.com/r/MachineLearning/comments/c4947c/d_how_much_timeenergy_do_you_spend_keeping/,researchthrowaway01,1561314467,"I love working the machine learning field. It is so exciting to be working on what literally feels like the future of humanity and technology every day. 

The flip side to that, though, is this constant feeling of pressure from knowing that I'm not quite fully in the know of the latest developments or trends at any given time. I've been feeling this more and more lately. As the field is expanding, there seems to be a total deluge of new ideas and findings and developments at any given time. A rational part of me says that this is just part of working in a dynamic field. Probably best to just focus on a few areas of expertise and accept that while ideas develop quickly, the everyday work is fairly predictable and controllable anyway. And I might not even feel this way if I could just stay off twitter and hackernews! But a deeper part of me can't help but feel unsatisfied by the fact that I have to make compromises when it comes to how much research I can do relative to how much actual work and experimentation I can do. And granted, I work in industry. I feel like it would be even harder if I were in an academic setting. I wonder if any of you feel the same. 

Part of the reason I'm asking this question is because I've begun fantasizing about building and providing some sort of service to help reduce this drowning feeling in regards to the deluge combination of arxiv, twitter, this subreddit, hackernews, etc. etc. Of course that would be premised on the idea that I'm not the only one who feels this way, which is part of what I'm curious about in posting this here. The main problem is that as I've thought about it more, I realize that this would have to be a paid service. Arxiv-sanity, semantic scholar etc. get us somewhat in the direction of reducing the burden, but I think they show the natural limits of what an automatic, freely-provided feed can offer us to this end at the moment. I think so much more can be done, by way of summarizing, connecting, and organizing papers as they come out, but that such features would require full time human curatorial and engineering work to be feasible. (Well, I don't know if it being paid would actually an issue or not, but I could see certain members of the community scoffing at the idea. We're all happily accustomed to freely available content, so it would have to involve some sort of compromise in which the content was free, but some other feature was paid.)

So I'm looking for a sanity check: would something addressing this actually be helpful? Or am I unique in experiencing the field in this way? I would appreciate any inputs or thoughts on the topic.",14,12
1460,2019-6-24,2019,6,24,3,c49glg,[P] That time I used a simple machine learning algorithm to classify deforested areas,https://www.reddit.com/r/MachineLearning/comments/c49glg/p_that_time_i_used_a_simple_machine_learning/,atum47,1561315656,"&amp;#x200B;

[Final result.](https://i.redd.it/vfqu5lzdj5631.png)

&amp;#x200B;

This project was developed as an assignment for a machine learning class, where we were given a pictures from the Amazon forest and we should write a machine learning algorithm to identify possibles deforested areas, including farming areas.

I then gather some training data, generated a histogram for every image and analysed the final image with a simple KNN (k=7) I wrote.

I was very happy with the results I got given the little code I wrote (this project is from 2017).

&amp;#x200B;

Link is here: [https://github.com/victorqribeiro/deforestation](https://github.com/victorqribeiro/deforestation)",25,153
1461,2019-6-24,2019,6,24,4,c49p56,Best online courses for introduction to predictive ML?,https://www.reddit.com/r/MachineLearning/comments/c49p56/best_online_courses_for_introduction_to/,poolguy8,1561316452,[removed],0,1
1462,2019-6-24,2019,6,24,4,c49uzt,"What's the best way to approach a regression problem with a lot of features (over 300), many of which are categorical?",https://www.reddit.com/r/MachineLearning/comments/c49uzt/whats_the_best_way_to_approach_a_regression/,shamoons,1561317008,"I have 20,000 training examples of various candidate attributes (highest level of education, country, year of formal training completion, etc). My output (prediction) will be for \`job\_performance\`, which is measured between 700 and 4,200 (in my training data). Initially, I was thinking about putting together a fully connected neural network, but I attempted a similar problem in a [Kaggle competition](https://www.kaggle.com/c/petfinder-adoption-prediction), but that didn't produce great results. What method(s) would you use to start?",0,1
1463,2019-6-24,2019,6,24,4,c49z17,How do you handle a cloud provider outage?,https://www.reddit.com/r/MachineLearning/comments/c49z17/how_do_you_handle_a_cloud_provider_outage/,powerforward1,1561317403,[removed],0,1
1464,2019-6-24,2019,6,24,4,c4a20d,[P] Learn to walk --- Deep Reinforcement Learning challenge @ NeurIPS 2019,https://www.reddit.com/r/MachineLearning/comments/c4a20d/p_learn_to_walk_deep_reinforcement_learning/,kidzik,1561317687,"Hi, we are launching the third challenge in a row, to advance understanding of human motor control, using reinforcement learning: [https://www.aicrowd.com/challenges/neurips-2019-learn-to-move-walk-around](https://www.aicrowd.com/challenges/neurips-2019-learn-to-move-walk-around)",0,6
1465,2019-6-24,2019,6,24,5,c4awh8,[Discussion] what to do with error in competing paper,https://www.reddit.com/r/MachineLearning/comments/c4awh8/discussion_what_to_do_with_error_in_competing/,jalapenjos,1561320583,"Without disclosing the details for the sake of anonymity: I've found a severe mistake in a competing paper, making it harder to publish my own. The performance metrics of the competing paper are computed incorrectly, meaning that their actual error rates are much higher than what the authors claim to have.

The competing paper has already been published in the conference proceedings of AAAI. My question is how I should deal with this situation.
- Should I report on that mistake in my own work?
- Should I inform the AAAI organization about this? I've tried this using their contact form, but got no response whatsoever.
- Anything else I can do with regard to the competing work to increase the odds of getting my paper published?

I'm 99 percent certain that it really is a mistake as I managed to run and debug the author's code to confirm my suspicion.",7,4
1466,2019-6-24,2019,6,24,5,c4b6eb,[R] When and Why does King - Man + Woman = Queen?,https://www.reddit.com/r/MachineLearning/comments/c4b6eb/r_when_and_why_does_king_man_woman_queen/,youali,1561321532,"**Towards Understanding Linear Word Analogies**

Kawin Ethayarajh, David Duvenaud, Graeme Hirst

Abstract: A surprising property of word vectors is that word analogies can often be solved with vector arithmetic. However, it is unclear why arithmetic operators correspond to non-linear embedding models such as skip-gram with negative sampling (SGNS). We provide a formal explanation of this phenomenon without making the strong assumptions that past theories have made about the vector space and word distribution. Our theory has several implications. Past work has conjectured that linear substructures exist in vector spaces because relations can be represented as ratios; we prove that this holds for SGNS. We provide novel justification for the addition of SGNS word vectors by showing that it automatically down-weights the more frequent word, as weighting schemes do ad hoc. Lastly, we offer an information theoretic interpretation of Euclidean distance in vector spaces, justifying its use in capturing word dissimilarity. 

**Blog post:** https://kawine.github.io/blog/nlp/2019/06/21/word-analogies.html

**Paper:** https://arxiv.org/abs/1810.04882",1,15
1467,2019-6-24,2019,6,24,5,c4bcew,[R] MelNet model all types of audio unconditionally,https://www.reddit.com/r/MachineLearning/comments/c4bcew/r_melnet_model_all_types_of_audio_unconditionally/,cryptonewsguy,1561322098,,0,1
1468,2019-6-24,2019,6,24,5,c4blvx,[D] Active audio noise cancelation/reduction,https://www.reddit.com/r/MachineLearning/comments/c4blvx/d_active_audio_noise_cancelationreduction/,VividFee,1561322986,"Hi all,  
I just saw these two video demonstrations about audio noise cancelation/reduction. It looks much like those noise cancelation features from Bose and Sony headphones, but all through the use of machine learning.

1. [Stationary vs non-stationary noises with Krisp App](https://www.youtube.com/watch?v=d3sgYvQCSAw)
2. [2Hz demo](https://www.youtube.com/watch?v=1I1GrWL40IQ)

I want to do something like this on my own. Can someone give me more information on the (most likely) used neural network and overall setup to achieve audio noise cancelation/reduction like in the videos?  
Thanks in advance!",7,3
1469,2019-6-24,2019,6,24,6,c4btif,AI Sidewalk (New AI/ML Medium Blog) - Follow for weekly articles/musings! Provide claps if you enjoy the article! Thanks!,https://www.reddit.com/r/MachineLearning/comments/c4btif/ai_sidewalk_new_aiml_medium_blog_follow_for/,sahpizzle,1561323701,,0,1
1470,2019-6-24,2019,6,24,6,c4c5dt,VGG19 fast style transfer and GANS DeepFashion on line shapes,https://www.reddit.com/r/MachineLearning/comments/c4c5dt/vgg19_fast_style_transfer_and_gans_deepfashion_on/,cherchercher2084,1561324792,"I'm doing some readings and playing with implementations on fast style transfer with VGG19 and GANS DeepFashion.

I noticed in the first example with VGG19 that style is not applied near the area of the tree(there's a fine area around the tree where the color is paler than the rest of the sky).

In the second example, the girl's backstrap is not shown on the output result, while the input result shows the strap in the front.

I am wondering what happens in both cases. To me, it seams that GANS is not very good at identifying the strap and the rest of clothes as one piece, despite them being the same color. For the VGG19 case, it seems that it separates the tree with the background pretty well....but somehow it failed to apply style near the tree. Coincidently, both cases deal with straight lines...

As an artist, I would love to have color patches around the tree similar to the rest of the sky. I would also like to get more refined/precise results for details of the girl's cloth. Any speculations? 

Thanks,

(my first reddit post)

![img](ae8qa1x556631 ""input"")

![img](x85ylk1756631 ""output fast-style-transfer VGG19"")

![img](shva5iqc76631)",0,1
1471,2019-6-24,2019,6,24,6,c4c7o6,[P] AI Sidewalk (New AI/ML Medium Blog) - Follow for weekly articles/musings! Provide claps if you enjoy the article! Thanks!,https://www.reddit.com/r/MachineLearning/comments/c4c7o6/p_ai_sidewalk_new_aiml_medium_blog_follow_for/,sahpizzle,1561325010,,0,1
1472,2019-6-24,2019,6,24,6,c4cb8z,[D] any python implementations of MelNet yet?,https://www.reddit.com/r/MachineLearning/comments/c4cb8z/d_any_python_implementations_of_melnet_yet/,cryptonewsguy,1561325349,"MelNet seems to be one of the best audio generators I've seen yet.

It can even generate unconditional music. I'm super interested in playing around with it but I lack the intellect to build it from scratch based off of the paper. 

https://audio-samples.github.io/",0,0
1473,2019-6-24,2019,6,24,6,c4co9i,[D] Does Deep RL work yet?,https://www.reddit.com/r/MachineLearning/comments/c4co9i/d_does_deep_rl_work_yet/,hazard02,1561326570,"Back in February 2018 there a blog article called [Deep Reinforcement Learning Doesn't Work Yet](https://www.alexirpan.com/2018/02/14/rl-hard.html) that basically argued that there are almost no cases of Deep RL being used in production systems (as opposed to game or research domains like AlphaGo, OpenAI Five, etc). 

&amp;#x200B;

Over a year later, has anything changed? Are there any new Deep RL success stories that are in production and making a big impact on business? I would be particularly interested in applications outside of the FAANG companies (although in-FAANG would be interesting too).",15,18
1474,2019-6-24,2019,6,24,7,c4dffy,"AI is not, and cannot be apolitical",https://www.reddit.com/r/MachineLearning/comments/c4dffy/ai_is_not_and_cannot_be_apolitical/,Laser_Plasma,1561329165,,0,1
1475,2019-6-24,2019,6,24,7,c4durn,"[R] Sun et al, ""A Survey of Optimization Methods from a Machine Learning Perspective""",https://www.reddit.com/r/MachineLearning/comments/c4durn/r_sun_et_al_a_survey_of_optimization_methods_from/,satsatsat,1561330663,,3,7
1476,2019-6-24,2019,6,24,9,c4f84v,"Just wrote a short Medium article on my research -- Deep MindRead: Using a Low-Cost Commercial EEG to Classify Brain Activity. It's a little simple/derivative from an ML perspective, but I personally felt the results were cool!",https://www.reddit.com/r/MachineLearning/comments/c4f84v/just_wrote_a_short_medium_article_on_my_research/,Weihua99,1561335618,,0,1
1477,2019-6-24,2019,6,24,9,c4fipy,A great course on optimization by Nesterov,https://www.reddit.com/r/MachineLearning/comments/c4fipy/a_great_course_on_optimization_by_nesterov/,adelrahimi,1561336695,[removed],0,1
1478,2019-6-24,2019,6,24,9,c4fuso,V2X link duration using machine learning,https://www.reddit.com/r/MachineLearning/comments/c4fuso/v2x_link_duration_using_machine_learning/,ArminBazz,1561337935,[removed],0,1
1479,2019-6-24,2019,6,24,10,c4g17j,LSTM: How to Train Neural Networks to Write Like Lovecraft,https://www.reddit.com/r/MachineLearning/comments/c4g17j/lstm_how_to_train_neural_networks_to_write_like/,strikingLoo,1561338604,,0,1
1480,2019-6-24,2019,6,24,10,c4g717,validation loss is much higher than training loss,https://www.reddit.com/r/MachineLearning/comments/c4g717/validation_loss_is_much_higher_than_training_loss/,mrsalta33,1561339256,[removed],0,1
1481,2019-6-24,2019,6,24,10,c4g7ae,Daily Fantasy Sports Ownership Projection,https://www.reddit.com/r/MachineLearning/comments/c4g7ae/daily_fantasy_sports_ownership_projection/,DYEL1105,1561339285,[removed],0,1
1482,2019-6-24,2019,6,24,10,c4geuj,[P] FB released pre-trained model on Instagram on PyTorch Hub. Gets SOTA on top-1 ImageNet after fine-tuning.,https://www.reddit.com/r/MachineLearning/comments/c4geuj/p_fb_released_pretrained_model_on_instagram_on/,sensetime,1561340118,"Link to the project page: https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/

GitHub: https://github.com/facebookresearch/WSL-Images/blob/master/hubconf.py

Colab Notebook demo: https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/facebookresearch_WSL-Images_resnext.ipynb

The released model is based on their ECCV [paper](https://arxiv.org/abs/1805.00932) from earlier:

[Exploring the Limits of Weakly Supervised Pretraining](https://arxiv.org/abs/1805.00932)

**Abstract** State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards ""small"". Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4% (97.6% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.

*Will our image models will become biased like self-centered millennial influencers!?*",24,214
1483,2019-6-24,2019,6,24,10,c4gjdn,I wrote my first blog on RNNs!,https://www.reddit.com/r/MachineLearning/comments/c4gjdn/i_wrote_my_first_blog_on_rnns/,MLnub22,1561340605,[removed],0,1
1484,2019-6-24,2019,6,24,11,c4h46c,Latent Dirichlet Allocation Question,https://www.reddit.com/r/MachineLearning/comments/c4h46c/latent_dirichlet_allocation_question/,errminator,1561343732,[removed],0,1
1485,2019-6-24,2019,6,24,12,c4hfsm,[D] About Geometric Deep Learning,https://www.reddit.com/r/MachineLearning/comments/c4hfsm/d_about_geometric_deep_learning/,davidc9320,1561345672,"I was wondering if anyone could help me getting started on the subject, indicating good resources to study from and interesting applications of this technique.

Thanks!",6,27
1486,2019-6-24,2019,6,24,13,c4ifq2,Top Courses to Learn Artificial Intelligence in 2019,https://www.reddit.com/r/MachineLearning/comments/c4ifq2/top_courses_to_learn_artificial_intelligence_in/,Majikarpp,1561351990,,0,1
1487,2019-6-24,2019,6,24,14,c4il9q,[D] Does anybody know of any conditional GAN work which encodes both understanding of object and background?,https://www.reddit.com/r/MachineLearning/comments/c4il9q/d_does_anybody_know_of_any_conditional_gan_work/,toadsofbattle,1561353021,"These days, the high fidelity GAN papers are often focused on reproducing class-conditional datasets such as ImageNet. But for use cases where it is important to not only generate ""dog"" but generate something like ""dog on grass"" or ""dog on concrete"", has any work been done to independently encode notions of object and background with a generator?

A naive approach could be to explode the amount of potential classes by separating the data into a lot more classes. But that's not very practical or interesting :P 

One thing I was thinking was perhaps to push this to the image translation domain (e.g. CycleGAN) - instead of doing something like trying to convert an object to another object (e.g. horse to zebra), the goal would be to convert a background to different sort of background. Any thoughts on this approach? 

Another more experimental/interesting thing I was thinking was perhaps to use a multi-generator approach, one of which in theory could generate ""object"", another which could do ""background/context"". But as far as I can tell, nobody's done anything like this, and I've no idea how to encode a prior on a specific type of generation so I'm just spitballing here. (miniature hypothesis that I have - you could use different architectures which have strong priors on creating different types of things, e.g. convGANs tend to do well with texture, while self attention GAN does better with structures/objects).",2,1
1488,2019-6-24,2019,6,24,14,c4iqbi,"What is fast.ai's ""learnable blur"" they mention in decrappify presentations?",https://www.reddit.com/r/MachineLearning/comments/c4iqbi/what_is_fastais_learnable_blur_they_mention_in/,HumanSpinach2,1561353976,[removed],0,1
1489,2019-6-24,2019,6,24,14,c4iri5,All-electric Trucks Market to witness huge opportunities during the forecast period from 2019 to 2023,https://www.reddit.com/r/MachineLearning/comments/c4iri5/allelectric_trucks_market_to_witness_huge/,jadhavni3,1561354183,[removed],1,1
1490,2019-6-24,2019,6,24,14,c4iu3i,Double Depth Telescopic Forks - LHD S.p.A.,https://www.reddit.com/r/MachineLearning/comments/c4iu3i/double_depth_telescopic_forks_lhd_spa/,lhd121,1561354695,,0,1
1491,2019-6-24,2019,6,24,14,c4iujl,Global All-electric Trucks Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c4iujl/global_allelectric_trucks_market_report_2019/,jadhavni3,1561354782,[removed],1,1
1492,2019-6-24,2019,6,24,14,c4iyjb,[D] Apples new iOS 13 Music highly accurate and intelligible lyrics system. How does it work?,https://www.reddit.com/r/MachineLearning/comments/c4iyjb/d_apples_new_ios_13_music_highly_accurate_and/,OPMaster494,1561355551,"When iOS 13 released, the music app had a redesign on the lyrics player, showing the lyrics like a slide show as the music played with incredible time accuracy. Plus, all the phrases were revealed in perfectly grouped of text. The lyrics player even detected when the song was in an instrumental without any singing. It was even enable to detect the quiet mumbles and unintelligible words that are covered in the background music. 

So the main question is... can this be done without ML and if it is (most likely), how can you get or train a model that can reproduce this intelligent system?

Id love to hear your thoughts!",12,0
1493,2019-6-24,2019,6,24,14,c4iz71,Growth of Refrigerated Air Dryers Market is estimated to rise with incredible Digit CAGR by 2023,https://www.reddit.com/r/MachineLearning/comments/c4iz71/growth_of_refrigerated_air_dryers_market_is/,jadhavni3,1561355683,[removed],1,1
1494,2019-6-24,2019,6,24,15,c4j5fa,Global Refrigerated Air Dryers Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c4j5fa/global_refrigerated_air_dryers_market_report_2019/,jadhavni3,1561356908,[removed],0,1
1495,2019-6-24,2019,6,24,15,c4j7j4,Understanding Bisecting K-Means Algorithms,https://www.reddit.com/r/MachineLearning/comments/c4j7j4/understanding_bisecting_kmeans_algorithms/,prakhar21,1561357352,[removed],0,1
1496,2019-6-24,2019,6,24,15,c4j8au,Night Vision Devices Market Future Forecast 20192023,https://www.reddit.com/r/MachineLearning/comments/c4j8au/night_vision_devices_market_future_forecast/,jadhavni3,1561357521,[removed],1,1
1497,2019-6-24,2019,6,24,15,c4jdos,Global Night Vision Devices Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c4jdos/global_night_vision_devices_market_report_2019/,jadhavni3,1561358664,[removed],1,1
1498,2019-6-24,2019,6,24,15,c4jgv5,Resveratrol Market Analysis By Current Trends and Future Estimations To 2023,https://www.reddit.com/r/MachineLearning/comments/c4jgv5/resveratrol_market_analysis_by_current_trends_and/,jadhavni3,1561359331,[removed],1,1
1499,2019-6-24,2019,6,24,16,c4joun,Global Resveratrol Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c4joun/global_resveratrol_market_report_2019/,jadhavni3,1561360957,[removed],1,1
1500,2019-6-24,2019,6,24,16,c4jrc8,The Pragmatic Data Scientist,https://www.reddit.com/r/MachineLearning/comments/c4jrc8/the_pragmatic_data_scientist/,mollerhoj,1561361498,,0,2
1501,2019-6-24,2019,6,24,16,c4jt2p,Global Textile Chemicals Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c4jt2p/global_textile_chemicals_market_report_2019/,jadhavni3,1561361904,[removed],1,1
1502,2019-6-24,2019,6,24,17,c4k814,Quora Insincere Questions Classification: Two notebook tutorials in PyTorch,https://www.reddit.com/r/MachineLearning/comments/c4k814/quora_insincere_questions_classification_two/,ahmedbesbes,1561365069,,0,1
1503,2019-6-24,2019,6,24,17,c4kd7e,Is Reinforcement Learning the final phase of AI?,https://www.reddit.com/r/MachineLearning/comments/c4kd7e/is_reinforcement_learning_the_final_phase_of_ai/,nit1995,1561366136,,0,1
1504,2019-6-24,2019,6,24,18,c4khm4,Global Military Communication System Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c4khm4/global_military_communication_system_market/,jadhavni3,1561366961,[removed],1,1
1505,2019-6-24,2019,6,24,18,c4kkux,"[R] A PyTorch implementation of ""Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks""",https://www.reddit.com/r/MachineLearning/comments/c4kkux/r_a_pytorch_implementation_of_clustergcn_an/,benitorosenberg,1561367593,"&amp;#x200B;

https://i.redd.it/x9tf4t4nt9631.jpg

GitHub: [https://github.com/benedekrozemberczki/ClusterGCN](https://github.com/benedekrozemberczki/ClusterGCN)

Paper: [https://arxiv.org/abs/1905.07953](https://arxiv.org/abs/1905.07953)

Abstract:

Graph convolutional network (GCN) has been successfully applied to many  graph-based applications; however, training a large-scale GCN remains  challenging. Current SGD-based algorithms suffer from either a high  computational cost that exponentially grows with number of GCN layers,  or a large space requirement for keeping the entire graph and the  embedding of each node in memory. In this paper, we propose Cluster-GCN,  a novel GCN algorithm that is suitable for SGD-based training by  exploiting the graph clustering structure. Cluster-GCN works as the  following: at each step, it samples a block of nodes that associate with  a dense subgraph identified by a graph clustering algorithm, and  restricts the neighborhood search within this subgraph. This simple but  effective strategy leads to significantly improved memory and  computational efficiency while being able to achieve comparable test  accuracy with previous algorithms. To test the scalability of our  algorithm, we create a new Amazon2M data with 2 million nodes and 61  million edges which is more than 5 times larger than the previous  largest publicly available dataset (Reddit). For training a 3-layer GCN  on this data, Cluster-GCN is faster than the previous state-of-the-art  VR-GCN (1523 seconds vs 1961 seconds) and using much less memory (2.2GB  vs 11.2GB). Furthermore, for training 4 layer GCN on this data, our  algorithm can finish in around 36 minutes while all the existing GCN  training algorithms fail to train due to the out-of-memory issue.  Furthermore, Cluster-GCN allows us to train much deeper GCN without much  time and memory overhead, which leads to improved prediction  accuracy---using a 5-layer Cluster-GCN, we achieve state-of-the-art test  F1 score 99.36 on the PPI dataset, while the previous best result was  98.71.",2,46
1506,2019-6-24,2019,6,24,18,c4kt5s,Nail Gun Market is projected to touch US $1520 Million Till 2023,https://www.reddit.com/r/MachineLearning/comments/c4kt5s/nail_gun_market_is_projected_to_touch_us_1520/,jadhavni3,1561369169,[removed],1,1
1507,2019-6-24,2019,6,24,18,c4ky9u,Global Nail Gun Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c4ky9u/global_nail_gun_market_report_2019/,jadhavni3,1561370070,[removed],1,1
1508,2019-6-24,2019,6,24,18,c4kzos,"Vision Processing Unit Market by Application (Drones, Surveillance Cameras, Autonomous Vehicles) - 2023",https://www.reddit.com/r/MachineLearning/comments/c4kzos/vision_processing_unit_market_by_application/,prashantvi,1561370311,,0,1
1509,2019-6-24,2019,6,24,19,c4l5a9,Roborace returns to Goodwood 'Festival of Speed',https://www.reddit.com/r/MachineLearning/comments/c4l5a9/roborace_returns_to_goodwood_festival_of_speed/,AIthatDrives,1561371205,,0,1
1510,2019-6-24,2019,6,24,19,c4l5qh,Global Nuclear Valves Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c4l5qh/global_nuclear_valves_market_report_2019/,jadhavni3,1561371280,[removed],1,1
1511,2019-6-24,2019,6,24,19,c4laza,Want to learn Machine learning,https://www.reddit.com/r/MachineLearning/comments/c4laza/want_to_learn_machine_learning/,kchaitanya39,1561372078,"Hello all
I am currently working as a Devops engineer and I want to learn Machine learning. Can someone tell me how difficult would be the transition and how long will it take and what are the best first steps to get into it? Kindly let me know",0,1
1512,2019-6-24,2019,6,24,19,c4lf0k,Output of current iteration as input to next iteration.,https://www.reddit.com/r/MachineLearning/comments/c4lf0k/output_of_current_iteration_as_input_to_next/,lifeinsrndpt,1561372632,[removed],0,1
1513,2019-6-24,2019,6,24,20,c4m9ct,Reinforcement learning sources,https://www.reddit.com/r/MachineLearning/comments/c4m9ct/reinforcement_learning_sources/,meechosch,1561376693,"Considering this a very current topic of research, any suggestions of books/papers that are significant in the contribution to the field would be very appreciated.",0,1
1514,2019-6-24,2019,6,24,21,c4mkcs,Phonem Recognition with TIMIT Corpus,https://www.reddit.com/r/MachineLearning/comments/c4mkcs/phonem_recognition_with_timit_corpus/,migisigi,1561378071,[removed],0,1
1515,2019-6-24,2019,6,24,21,c4mzum,[P] Quick notes on Machine Learning. Read on the go.,https://www.reddit.com/r/MachineLearning/comments/c4mzum/p_quick_notes_on_machine_learning_read_on_the_go/,pradeep_sinngh,1561379743,,0,1
1516,2019-6-24,2019,6,24,21,c4n7qx,How to get mumber of engagements and impressions from users tweets using tweepy or other tools?,https://www.reddit.com/r/MachineLearning/comments/c4n7qx/how_to_get_mumber_of_engagements_and_impressions/,LowerLaugh,1561380562,[removed],0,1
1517,2019-6-24,2019,6,24,22,c4nkxn,How to apply feature engeneering to real-world data?,https://www.reddit.com/r/MachineLearning/comments/c4nkxn/how_to_apply_feature_engeneering_to_realworld_data/,alisherAbdullaev,1561381892,[removed],0,1
1518,2019-6-24,2019,6,24,22,c4npoe,Geoffrey Hinton and Yann LeCun to Deliver Turing Lecture,https://www.reddit.com/r/MachineLearning/comments/c4npoe/geoffrey_hinton_and_yann_lecun_to_deliver_turing/,dimber-damber,1561382380,,0,1
1519,2019-6-24,2019,6,24,22,c4nyll,Mathematical relationships behind computations in FP growth and apriori algorithms.,https://www.reddit.com/r/MachineLearning/comments/c4nyll/mathematical_relationships_behind_computations_in/,CrispyChrisChicken,1561383351,[removed],0,1
1520,2019-6-24,2019,6,24,22,c4o27b,What is Digital Trust and Why Its Impact is Greater Than You Think,https://www.reddit.com/r/MachineLearning/comments/c4o27b/what_is_digital_trust_and_why_its_impact_is/,Victor_Stakh,1561383736,,0,1
1521,2019-6-24,2019,6,24,22,c4o8wz,Bye Bye Camera - an App for the Post-human Era,https://www.reddit.com/r/MachineLearning/comments/c4o8wz/bye_bye_camera_an_app_for_the_posthuman_era/,hoopism,1561384449,,0,1
1522,2019-6-24,2019,6,24,23,c4ojbn,[D] Precise detection of large number of keypoints,https://www.reddit.com/r/MachineLearning/comments/c4ojbn/d_precise_detection_of_large_number_of_keypoints/,marcopaaah,1561385514,"Keypoint detection has successfully been modelled with CNNs that outputs a heatmap tensor of size H x W x K, where K is the number of instance keypoints you want to detect and H and W the output size of the heatmaps. If you want precise detections H and W should ideally be the same size as the input image.

&amp;#x200B;

I want to detect K&gt;=300 using an input image of size 512 x 512. Due to obvious memory limitations I can't use the above naive approach that upscales to the original input size. 

&amp;#x200B;

Is anyone aware of some research that addresses this specific issue?",4,2
1523,2019-6-24,2019,6,24,23,c4oo8f,What is the best way to train a model to detect objects solely based on color?,https://www.reddit.com/r/MachineLearning/comments/c4oo8f/what_is_the_best_way_to_train_a_model_to_detect/,rbb091020,1561386026,[removed],0,1
1524,2019-6-24,2019,6,24,23,c4os1t,"Confusion on use of the words ""layer"" and ""top layers""",https://www.reddit.com/r/MachineLearning/comments/c4os1t/confusion_on_use_of_the_words_layer_and_top_layers/,NoTechBackground,1561386424,"I am trying to follow this tutorial on printing out the intermediate activations

 [https://github.com/gabrielpierobon/cnnshapes/blob/master/README.md](https://github.com/gabrielpierobon/cnnshapes/blob/master/README.md) 

An alternate link: [https://towardsdatascience.com/visualizing-intermediate-activation-in-convolutional-neural-networks-with-keras-260b36d60d0](https://towardsdatascience.com/visualizing-intermediate-activation-in-convolutional-neural-networks-with-keras-260b36d60d0)

&amp;#x200B;

1. There is mention of ""extracts the outputs of the top 12 layers"" but I don't understand where these 12 are coming from. 

 

    layer_outputs = [layer.output for layer in classifier.layers[:12]] # Extracts the outputs of the top 12 layers 
    activation_model = models.Model(inputs=classifier.input, outputs=layer_outputs) # Creates a model that will return these outputs, given the model input 

&amp;#x200B;

2. 

A few lines later in the tutorial, there is mention that we are only on our first layer. 

 

 For instance, this is the activation of the first convolution layer for the image input:

    first_layer_activation = activations[0] print(first_layer_activation.shape) 

&amp;#x200B;

For 1., how can there be 12 layers anywhere? What does that mean? It looks like in the code we only have three: 

 

    classifier.add(Conv2D(32, (3, 3), padding='same', input_shape = (28, 28, 3), activation = 'relu')) classifier.add(Conv2D(32, (3, 3), activation='relu')) classifier.add(MaxPooling2D(pool_size=(2, 2))) classifier.add(Dropout(0.5)) # antes era 0.25  # Adding a second convolutional layer classifier.add(Conv2D(64, (3, 3), padding='same', activation = 'relu')) classifier.add(Conv2D(64, (3, 3), activation='relu')) classifier.add(MaxPooling2D(pool_size=(2, 2))) classifier.add(Dropout(0.5)) # antes era 0.25  # Adding a third convolutional layer classifier.add(Conv2D(64, (3, 3), padding='same', activation = 'relu')) classifier.add(Conv2D(64, (3, 3), activation='relu')) classifier.add(MaxPooling2D(pool_size=(2, 2))) classifier.add(Dropout(0.5)) # antes era 0.25",0,1
1525,2019-6-24,2019,6,24,23,c4ox0a,"Free Book: Statistics -- New Foundations, Toolbox, and Machine Learning Recipes",https://www.reddit.com/r/MachineLearning/comments/c4ox0a/free_book_statistics_new_foundations_toolbox_and/,psangrene,1561386926,,0,1
1526,2019-6-24,2019,6,24,23,c4p2wv,Resources for interactively explaining ML?,https://www.reddit.com/r/MachineLearning/comments/c4p2wv/resources_for_interactively_explaining_ml/,DumberML,1561387517,[removed],0,1
1527,2019-6-24,2019,6,24,23,c4p81s,"Benchmarking Machine Learning on the New Raspberry Pi 4, Model B",https://www.reddit.com/r/MachineLearning/comments/c4p81s/benchmarking_machine_learning_on_the_new/,alasdairallan,1561388031,,0,1
1528,2019-6-25,2019,6,25,0,c4pczs,Advice for fossil hunting quadcopter video using AI?,https://www.reddit.com/r/MachineLearning/comments/c4pczs/advice_for_fossil_hunting_quadcopter_video_using/,MegavirusOfDoom,1561388498,[removed],0,1
1529,2019-6-25,2019,6,25,0,c4pxm2,Artificial Intelligence and Machine Learning  Hype vs Reality,https://www.reddit.com/r/MachineLearning/comments/c4pxm2/artificial_intelligence_and_machine_learning_hype/,shanemcgrawspm,1561390468,,0,1
1530,2019-6-25,2019,6,25,0,c4pz4v,[D] r/compmathneuro's first Journal Club -- an invitation,https://www.reddit.com/r/MachineLearning/comments/c4pz4v/d_rcompmathneuros_first_journal_club_an_invitation/,P4TR10T_TR41T0R,1561390616,"I'd like to spread the word about r/compmathneuro's first Journal Club, which will take place on June 27 at 9:30 (UTC+1). 

&amp;#x200B;

User u/Stereoisomer will present the paper ""Towards the neural population doctrine"" authored by Shreya Saxena and  John P. Cunningham (available through this [link](https://sci-hub.tw/https://www.sciencedirect.com/science/article/abs/pii/S0959438818300990)). The presentation will be shared through either PowerPoint or KeyNote online, while users are encouraged to join us for a live voice discussion on discord. Those not able to participate in voice chat are encouraged to join us through the discord channel #paper-sharing. Following the presentation, we'll host a short Q&amp;A.

&amp;#x200B;

The journal club should last about an hour in total and will be moderated by mod [u/mkeee2015](https://www.reddit.com/u/mkeee2015/). If you're interested, please join our discord server through this [invite](https://discord.gg/AWhAVQ). 

&amp;#x200B;

Please note that this is the first time we attempt to organize a Journal Club, and should thus be regarded as experimental. 

&amp;#x200B;

We hope to see you there!",0,2
1531,2019-6-25,2019,6,25,0,c4pz9v,How to stay updated in the field of Deep Learning,https://www.reddit.com/r/MachineLearning/comments/c4pz9v/how_to_stay_updated_in_the_field_of_deep_learning/,CSGOvelocity,1561390630,[removed],0,1
1532,2019-6-25,2019,6,25,0,c4q1ib,[R] Geoffrey Hinton and Yann LeCun Deliver Turing Lecture,https://www.reddit.com/r/MachineLearning/comments/c4q1ib/r_geoffrey_hinton_and_yann_lecun_deliver_turing/,downtownslim,1561390842,"""We are pleased to announce that Geoffrey Hinton and Yann LeCun will deliver the Turing Lecture at FCRC.  Hinton's talk, entitled, ""The Deep Learning Revolution"" and LeCun's talk, entitled, ""The Deep Learning Revolution: The Sequel,"" will be presented June 23rd from 5:15-6:30pm in Symphony Hall.""

&amp;#x200B;

[https://www.youtube.com/watch?v=VsnQf7exv5I](https://www.youtube.com/watch?v=VsnQf7exv5I)",26,163
1533,2019-6-25,2019,6,25,0,c4q1zy,T480 vs T480s and Thinkpad X1 Extreme,https://www.reddit.com/r/MachineLearning/comments/c4q1zy/t480_vs_t480s_and_thinkpad_x1_extreme/,baby_250694,1561390892,[removed],0,1
1534,2019-6-25,2019,6,25,0,c4q6fw,ICLR Reproducibility Interview #3: Alfredo and Robert,https://www.reddit.com/r/MachineLearning/comments/c4q6fw/iclr_reproducibility_interview_3_alfredo_and/,CometML,1561391327,,0,1
1535,2019-6-25,2019,6,25,0,c4qal3,[D] Speech-to-text adversarial examples to slow YouTube censorship,https://www.reddit.com/r/MachineLearning/comments/c4qal3/d_speechtotext_adversarial_examples_to_slow/,ShameSpirit,1561391723,"A Google whistleblower explained that much of the demonetization/censorship action occurring on YouTube is done through Google's speech-to-text.  If so, it seems that altering a video's audio to become an adversarial example, prior to it being uploaded, could serve to slow what's happening.  

&amp;#x200B;

Is it possible to reliably generate adversarial examples for an ai which you do not have direct access to (Google's Cloud Speech-To-Text is behind a pay wall)?  I've heard Lex  Fridman mention that adversarial examples are often effective against multiple networks, even when their structures differ.",17,11
1536,2019-6-25,2019,6,25,1,c4qjx8,MLPerf Inference v0.5 Rules finalized - results due 6 September,https://www.reddit.com/r/MachineLearning/comments/c4qjx8/mlperf_inference_v05_rules_finalized_results_due/,riking27,1561392602,,1,1
1537,2019-6-25,2019,6,25,1,c4r5jc,Multi Task learning with different sized dataset.,https://www.reddit.com/r/MachineLearning/comments/c4r5jc/multi_task_learning_with_different_sized_dataset/,randomchickibum,1561394685,[removed],0,1
1538,2019-6-25,2019,6,25,1,c4r63p,Are there entry level positions for Data analysis / Machine learning?,https://www.reddit.com/r/MachineLearning/comments/c4r63p/are_there_entry_level_positions_for_data_analysis/,Ludbrium,1561394742,[removed],0,1
1539,2019-6-25,2019,6,25,2,c4sdfg,Machine learning / data science with Java,https://www.reddit.com/r/MachineLearning/comments/c4sdfg/machine_learning_data_science_with_java/,theCodeGuyJP,1561398812,[removed],0,1
1540,2019-6-25,2019,6,25,3,c4snat,"Is there a well maintained list of good ""benchmark"" datasets for ML ?",https://www.reddit.com/r/MachineLearning/comments/c4snat/is_there_a_well_maintained_list_of_good_benchmark/,elcric_krej,1561399728,[removed],0,1
1541,2019-6-25,2019,6,25,3,c4swfe,Help! - Can you use BERT for long sequence classification?,https://www.reddit.com/r/MachineLearning/comments/c4swfe/help_can_you_use_bert_for_long_sequence/,officialpatterson,1561400595,[removed],0,1
1542,2019-6-25,2019,6,25,3,c4sxsj,Machine Learning GPU,https://www.reddit.com/r/MachineLearning/comments/c4sxsj/machine_learning_gpu/,sagun_,1561400727,,0,1
1543,2019-6-25,2019,6,25,3,c4ta3q,[P] Training Mask RCNN for recognizing objects in large images,https://www.reddit.com/r/MachineLearning/comments/c4ta3q/p_training_mask_rcnn_for_recognizing_objects_in/,WesternHarmonica,1561401893,"Hi guys! I've started a project using Facebook's [Mask R-CNN](https://github.com/matterport/Mask_RCNN). The goal is to have it recognize certain small objects from large (HD+) images.

I'm a bit worried how long it will take to train with original image size. Does it make much difference if I crop the objects out of the image and train them separately? It still should be able to work with large images though.

I am using pre-trained COCO weights and the tool I use for labeling is [VGG Image Annotator](http://www.robots.ox.ac.uk/~vgg/software/via/). Thanks in advance!",3,1
1544,2019-6-25,2019,6,25,3,c4tfpm,"[D] Optimal ML development flow/process, feedback would be helpful.",https://www.reddit.com/r/MachineLearning/comments/c4tfpm/d_optimal_ml_development_flowprocess_feedback/,feedthemartian,1561402446,"I'm a Software Engineer specializing in Data Infrastructure/Engineering and DevOps.

I've been speaking with a few colleagues who work with ML and have expressed their frustration with the lack of a consistent ""developer flow"" for ML projects, so I wanted to ask this community, what does YOUR ideal developer flow look like? 

I apologize in advance for my lack of knowledge on this subject, and if I've used any of the terms incorrectly. I'm very new and just trying to learn more about the underlying infrastructure.

&amp;#x200B;

Here's what we sketched out to be a reasonable developer flow:

**Assumptions:** 

1. Data is already available, all connections are correctly configured. You can explore it using notebooks or a sql tool like apache superset.
2. You have access to an ETL tool (eg: apache airflow) where you've built dags to aggregate data and preprocess source data to be in the input format for your ML model.
3. You have access to development machines (""devboxes"") which are configured exactly like production machines where the task/job will run - except that devboxes can only read production data but NOT write production data (can still write to dev/staging databases). These are your test environment.

**Workflow:**

1. You start a (hosted) notebook (Jupyter or Zeppelin) which has access to the data. You also have access to the pre-processed datasets mentioned in Assumptions\[2\] and you build out your models (I don't really know what happens here - i'm sorry)
2. You can also write python/scala code instead of using the notebook and test it by running it on the devbox.
3. You've built and (minimally tested) your model and want to train, deploy and productionize it. **What happens after this?**

Could someone help me understand what happens after this step?

I'm guessing you'll need to train the model, can that be done in the notebook or the python file which you can run on your devbox. Training the model in the notebook seems untrackable, so you'll probably want to train it in python/scala code which will be checked into github.

You'll probably need to re-train it periodically so the python/scala function can be deployed in an Airflow DAG which trains it daily/weekly.

**What would be the common processes of deploying it after this step?**

&amp;#x200B;

For example, for regular software projects it would be:

Code -&gt; Test Locally -&gt; Push to github (not merged yet) -&gt; CI/CD builds the new code and pushes to staging -&gt; test staging -&gt; everything looks good/no regression in other services -&gt; push to production by merging PR

&amp;#x200B;

For Data Engineering projects, the workflow is all over the place but my ideal workflow is:

Code (create a new DAG/update queries) -&gt; Test on devbox with sample data (local testing is not possible with large datasets) -&gt; Push to github (not merged yet)  -&gt; CI/CD builds the new code/DAG -&gt; new DAG runs in staging with staging data, generates staging tables to test -&gt;  everything looks good/data quality checks pass -&gt; push to production by merging PR -&gt; production jobs pick up the new queries/DAGs.

&amp;#x200B;

DISCLAIMER: I know very little about this space, I'm happy to read any documentation you provide on this. 

Thank you in advance!",2,1
1545,2019-6-25,2019,6,25,4,c4u3g2,[Discussion] Overfitting in ML,https://www.reddit.com/r/MachineLearning/comments/c4u3g2/discussion_overfitting_in_ml/,himanshuragtah1,1561404705,"Is knowing the overfitting parameter critical when trying to understand or use machine learning models?

&amp;#x200B;

If yes, what are some quick ways to find out the overfitting parameter without diving in deep?",5,0
1546,2019-6-25,2019,6,25,4,c4u58w,"[N] MLPerf announced the v0.5 inference benchmark suite (submissions due September 6th, 2019)",https://www.reddit.com/r/MachineLearning/comments/c4u58w/n_mlperf_announced_the_v05_inference_benchmark/,mllosab,1561404873,,0,1
1547,2019-6-25,2019,6,25,4,c4u9a4,60 of the most influential papers in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/c4u9a4/60_of_the_most_influential_papers_in_machine/,SquareRootsi,1561405273,[removed],1,1
1548,2019-6-25,2019,6,25,4,c4u9ik,Women in AI Award Nominations,https://www.reddit.com/r/MachineLearning/comments/c4u9ik/women_in_ai_award_nominations/,npatrici,1561405294,[removed],0,1
1549,2019-6-25,2019,6,25,4,c4uahc,InferSent results...,https://www.reddit.com/r/MachineLearning/comments/c4uahc/infersent_results/,rodrigonader,1561405384,[removed],0,1
1550,2019-6-25,2019,6,25,5,c4ur5m,Computer vision with phone images?,https://www.reddit.com/r/MachineLearning/comments/c4ur5m/computer_vision_with_phone_images/,Lunkwill_And_Fook,1561406980,[removed],0,1
1551,2019-6-25,2019,6,25,5,c4v7k8,Rant on Tensorflow 2.0,https://www.reddit.com/r/MachineLearning/comments/c4v7k8/rant_on_tensorflow_20/,HarambeTownley,1561408535,"Yesterday I decided to finally upgrade to tf2.0 from my tf1.11. Needless to say I also had reinstall CUDA and CuDNN because I use gpu version. After upgrade ALL my older code is deprecated.

&amp;#x200B;

But you might say ""oh tf provided you with convertors blah blah"" - that sounds cool and shit and for many cases the problem isn't that big but we all know machine learning is NOT about code. IT'S ABOUT MODELS. I had built a model using tf.contrib.layers which I trained for HOURS. And now you say ""we're dropping tf.contrib"" - wtf? So now my model which I created in tensorflow core but used tf.contrib.layers for Xavier Initialization is deprecated now? You say I should use keras like tf.initializers now? WHY? My goddamn model, which I saved after hours of training, was saved with tf.contrib.layers. I tried replacing tf1.contrib.layers.xavier\_initializer() with tf2.initializers.GlorotUniform() but that just causes shit ton of more errors. SO WHAT TF AM I SUPPOSED TO DO WITH THIS KERAS LAYER? I can't load my older model and run it. Am I supposed to re-train all my models?

&amp;#x200B;

I understand we have to move on, especially that newer models are dynamic so eager had to be the way forward. But why tf would you deprecate all my models and code but only provide convertors for code? Also I have so many files, Am I supposed to run tf convertor on all of them and then check which among those are working and then fix the ones that aren't?",0,1
1552,2019-6-25,2019,6,25,5,c4visk,CMU &amp; Google XLNet Tops BERT; Achieves SOTA Results on 18 NLP Tasks - Medium,https://www.reddit.com/r/MachineLearning/comments/c4visk/cmu_google_xlnet_tops_bert_achieves_sota_results/,Yuqing7,1561409626,,0,1
1553,2019-6-25,2019,6,25,5,c4vkp9,What are the practical benefits/uses of Neural Module Networks?,https://www.reddit.com/r/MachineLearning/comments/c4vkp9/what_are_the_practical_benefitsuses_of_neural/,harshsikka123,1561409807,[removed],0,1
1554,2019-6-25,2019,6,25,7,c4x5bk,@geaxart,https://www.reddit.com/r/MachineLearning/comments/c4x5bk/geaxart/,3ftdivot,1561415345,[removed],0,1
1555,2019-6-25,2019,6,25,7,c4xg13,[D] CNN on irregular grid,https://www.reddit.com/r/MachineLearning/comments/c4xg13/d_cnn_on_irregular_grid/,koobear,1561416421,"Here's what my data look like:

* There is a collection of a few thousand objects
* Each object is sampled on a grid, with multiple properties observed per point
* There are two types of objects

And the goal is to identify which of the two types of objects we have given the grid of samples. 

So this looks like a straightforward CNN project. I'm not an expert, but I have experience fitting those on RGB images using TF/keras and doing some model tuning and validation. Should be easy, right? 

The problem is that these grids are all irregular. I know how to fit CNNs when I have a collection of rectangular images all of the same size (if they're different dimensions, then it's straightforward to resize/interpolate/resample them). But how do you handle grids taken from what I can best describe as ""blobs""? To make things even harder, the sampling instrument occasionally malfunctioned, so I have occasional `NA`s for the measurements. 

My thinking is perhaps there's a way to reshape everything into rectangles of the same size, but I can't think of how. Or is there another totally different type of model that would work best for these data?",17,1
1556,2019-6-25,2019,6,25,8,c4y51t,[Discussion] What is the current status of multi-lingual NLP?,https://www.reddit.com/r/MachineLearning/comments/c4y51t/discussion_what_is_the_current_status_of/,AlexSnakeKing,1561419029,,0,1
1557,2019-6-25,2019,6,25,8,c4y8cf,When have you visualized embeddings and it was useful?,https://www.reddit.com/r/MachineLearning/comments/c4y8cf/when_have_you_visualized_embeddings_and_it_was/,random7002,1561419386,[removed],0,1
1558,2019-6-25,2019,6,25,8,c4yge0,Tiny Machine Learning on the Edge with TensorFlow Lite Running on SAMD51 - Cool demo with voice BIT from TRON :),https://www.reddit.com/r/MachineLearning/comments/c4yge0/tiny_machine_learning_on_the_edge_with_tensorflow/,blinka_friendlysnake,1561420220,,0,1
1559,2019-6-25,2019,6,25,8,c4ylga,[D] Misuse of Deep Learning in Nature Journals Earthquake Aftershock Paper,https://www.reddit.com/r/MachineLearning/comments/c4ylga/d_misuse_of_deep_learning_in_nature_journals/,milaworld,1561420756,"*Recently, I saw a [post](https://towardsdatascience.com/stand-up-for-best-practices-8a8433d3e0e8) by [Rajiv Shah](https://twitter.com/rajcs4), Chicago-based data-scientist, regarding an article published in Nature last year called [Deep learning of aftershock patterns following large earthquakes](https://www.nature.com/articles/s41586-018-0438-y), written by scientists at Harvard in collaboration with Google. Below is the article:*

**Stand Up for Best Practices:
Misuse of Deep Learning in Natures Earthquake Aftershock Paper**

*The Dangers of Machine Learning Hype*

Practitioners of AI, machine learning, predictive modeling, and data science have grown enormously over the last few years. What was once a niche field defined by its blend of knowledge is becoming a rapidly growing profession. As the excitement around AI continues to grow, the new wave of ML augmentation, automation, and GUI tools will lead to even more growth in the number of people trying to build predictive models.

But heres the rub: While it becomes easier to use the tools of predictive modeling, predictive modeling knowledge is not yet a widespread commodity. Errors can be counterintuitive and subtle, and they can easily lead you to the wrong conclusions if youre not careful.

Im a data scientist who works with dozens of expert data science teams for a living. In my day job, I see these teams striving to build high-quality models. The best teams work together to review their models to detect problems. There are many hard-to-detect-ways that lead to problematic models (say, by allowing target leakage into their training data).

Identifying issues is not fun. This requires admitting that exciting results are too good to be true or that their methods were not the right approach. In other words, *its less about the sexy data science hype that gets headlines and more about a rigorous scientific discipline.*

*Bad Methods Create Bad Results*

Almost a year ago, I read an [article](https://www.nature.com/articles/s41586-018-0438-y) in Nature that claimed unprecedented accuracy in predicting earthquake aftershocks by using deep learning. Reading the article, my internal radar became deeply suspicious of their results. *Their methods simply didnt carry many of the hallmarks of careful predicting modeling.*

I started to dig deeper. In the meantime, this article blew up and became [widely recognized](https://blog.google/technology/ai/forecasting-earthquake-aftershock-locations-ai-assisted-science/)! It was even included in the [release notes](https://medium.com/tensorflow/whats-coming-in-tensorflow-2-0-d3663832e9b8) for Tensorflow as an example of what deep learning could do. However, in my digging, I found major flaws in the paper. Namely, data leakage which leads to unrealistic accuracy scores and a lack of attention to model selection (you dont build a 6 layer neural network when a simpler model provides the same level of accuracy).

To my earlier point: these are subtle, but *incredibly basic* predictive modeling errors that can invalidate the entire results of an experiment. Data scientists are trained to recognize and avoid these issues in their work. I assumed that this was simply overlooked by the author, so I contacted her and let her know so that she could improve her analysis. Although we had previously communicated, she did not respond to my email over concerns with the paper.

*Falling On Deaf Ears*

So, what was I to do? My coworkers told me to just [tweet](https://twitter.com/rajcs4/status/1143236424738775046) [it](https://twitter.com/DataScienceLA/status/1143245342785228800) and let it go, but I wanted to stand up for good modeling practices. I thought reason and best practices would prevail, so I started a 6-month process of writing up my results and shared them with Nature.
Upon sharing my results, I received a note from Nature in January 2019 that despite serious concerns about data leakage and model selection that invalidate their experiment, they saw no need to correct the errors, because **Devries et al. are concerned primarily with using machine learning as [a] tool to extract insight into the natural world, and not with details of the algorithm design**. The authors provided a much [harsher](https://github.com/rajshah4/aftershocks_issues/blob/master/correspondence/Authors_DeVries_Response.pdf) response.

You can read the entire exchange on my [github](https://github.com/rajshah4/aftershocks_issues).

Its not enough to say that I was disappointed. This was a major paper (its **Nature**!) that bought into AI hype and published a paper despite it using flawed methods.

Then, just this week, I ran [across](https://link.springer.com/chapter/10.1007/978-3-030-20521-8_1) [articles](https://arxiv.org/abs/1904.01983) by Arnaud Mignan and Marco Broccardo on shortcomings that they found in the aftershocks article. Here are two more data scientists with expertise in earthquake analysis who also noticed flaws in the paper. I also have placed my analysis and reproducible code on [github](https://github.com/rajshah4/aftershocks_issues).

*Standing Up For Predictive Modeling Methods*

I want to make it clear: my goal is not to villainize the authors of the aftershocks paper. I dont believe that they were malicious, and I think that they would argue their goal was to just show how machine learning could be applied to aftershocks. Devries is an accomplished earthquake scientist who wanted to use the latest methods for her field of study and found exciting results from it.

But heres the problem: their insights and results were based on fundamentally flawed methods. Its not enough to say, This isnt a machine learning paper, its an earthquake paper. If you use predictive modeling, then the quality of your results are determined by the quality of your modeling. Your work becomes data science work, and you are on the hook for your scientific rigor.

There is a huge appetite for papers that use the latest technologies and approaches. It becomes very difficult to push back on these papers.

But if we allow papers or projects with fundamental issues to advance, it hurts all of us. It undermines the field of predictive modeling.

Please push back on bad data science. Report bad findings to papers. And if they dont take action, go to twitter, post about it, share your results and make noise. This type of collective action worked to raise awareness of p-values and combat the epidemic of p-hacking. We need good machine learning practices if we want our field to continue to grow and maintain credibility.

[Link to Article](https://towardsdatascience.com/stand-up-for-best-practices-8a8433d3e0e8)

[Original Nature Publication](https://www.nature.com/articles/s41586-018-0438-y) (note: paywalled)

[GitHub repo contains an attempt to reproduce Nature's paper](https://github.com/rajshah4/aftershocks_issues)

[Confrontational correspondence with authors](https://github.com/rajshah4/aftershocks_issues/blob/master/correspondence/Authors_DeVries_Response.pdf)",149,613
1560,2019-6-25,2019,6,25,10,c4zxc6,[R] A Tensorized Transformer for Language Modeling,https://www.reddit.com/r/MachineLearning/comments/c4zxc6/r_a_tensorized_transformer_for_language_modeling/,HigherTopoi,1561425947,,10,10
1561,2019-6-25,2019,6,25,11,c50nrn,[P] How to easily deploy XGBoost models in C++ production environments,https://www.reddit.com/r/MachineLearning/comments/c50nrn/p_how_to_easily_deploy_xgboost_models_in_c/,jonas_aka_guitargeek,1561428785,"Hi all!

&amp;#x200B;

I want to wish you a good evening and would like to share with you some tool I developed for my work in the particle physics community.

&amp;#x200B;

Probably we are not the only ones who like to prototype and train ML models with xgboost in Python and then have to deploy them to our high performance multithreading C++ production environment. This is exactly the step that my tool which I call ""FastForest"" wants to make easy like a breeze!

&amp;#x200B;

The mission of the library is to be:

* **Easy**: deploying your xgboost model should be as painless as it can be
* **Fast**: thanks to efficient structure-of-array data structures for storing the trees, this library goes very easy on your CPU and memory
* **Safe**: the FastForest objects are immutable, and therefore they are an excellent choice in multithreading environments
* **Portable**: FastForest has no dependency other than the C++ standard library

&amp;#x200B;

I hope that this might be of use for someone else too :) I think it might, because so far, the only solutions to this C++ deployment problem that I found on the web are either to use the sparsely documented xgboost C API or to transform your models into hardcoded C++ (which is cool as well of course). What I tried to write here is a very lightweight and clean C++ solution which can load models dynamically.

&amp;#x200B;

Check it out here: [https://github.com/guitargeek/XGBoost-FastForest](https://github.com/guitargeek/XGBoost-FastForest)",20,4
1562,2019-6-25,2019,6,25,12,c51cbl,[P] texrex web page cleaning system,https://www.reddit.com/r/MachineLearning/comments/c51cbl/p_texrex_web_page_cleaning_system/,HigherTopoi,1561431732,"Creating a gigantic dataset from Common Crawl is becoming more common recently. I found this library ""texrex"", by  to be very comprehensive and much more sophisticated than the currently popular options. I'm not the author of this library.   

[https://github.com/rsling/texrex](https://github.com/rsling/texrex)

&amp;#x200B;

[https://pdfs.semanticscholar.org/76e5/283b144ed56b60b237a503757f31140c5450.pdf](https://pdfs.semanticscholar.org/76e5/283b144ed56b60b237a503757f31140c5450.pdf) Kjetil Bugge Kristoffersen",0,1
1563,2019-6-25,2019,6,25,12,c51r45,[R] Mental Simulation with Self-Supervised Spatiotemporal Learning,https://www.reddit.com/r/MachineLearning/comments/c51r45/r_mental_simulation_with_selfsupervised/,computeisallyouneed,1561433652,"Hi all, I'd like to share my undergraduate thesis on Mental Simulation with Self-Supervised Spatiotemporal Learning. We propose that one way to understand mental simulation in humans is to approach it as a problem of video prediction. The code is based off the recent ICLR 2019 paper [Eidetic 3D LSTM: A Model for Video Prediction and Beyond](https://openreview.net/forum?id=B1lKS2AqtX). 

Paper: [https://github.com/kevinstan/video\_prediction/blob/master/paper/mental\_sim.pdf](https://github.com/kevinstan/video_prediction/blob/master/paper/mental_sim.pdf)

Code: [https://github.com/kevinstan/video\_prediction](https://github.com/kevinstan/video_prediction)

**Abstract:** Mental simulation -- the capacity to imagine objects and scenes in order to make decisions, predictions, and inferences about the world -- is a key feature of human cognition. Evidence from behavioral studies suggest that representations of visual imagery are spatial and sensitive to the causal structure of the world. Inspired by how humans anticipate future scenes, we aim to leverage state-of-the-art techniques in deep learning and computer vision to tackle the problem of spatiotemporal predictive learning in a self-supervised manner. We perform explorations across three architectural design choices: (i) the importance of 2D-convolution vs. 3D-convolution inside the cell of recurrent neural networks, (ii) the effectiveness of residual connections in stacked long short-term memory models for remembering spatial information over long time horizons, and (iii) the balance between $l\_1$ norm and $l\_2$ norm components in the objective function. Our extensive evaluations demonstrate that finetuning with residual connections achieves state-of-the-art performance on the Moving MNIST and KTH Action benchmark datasets. Potential application areas include weather forecasting, traffic flow prediction, and physical interaction simulation.

&amp;#x200B;

Comments and feedback are highly appreciated. Thanks!",2,3
1564,2019-6-25,2019,6,25,12,c51s2e,Machine Learning Bootcamp: Become a ML Engineer in 6 months. Job Guaranteed.,https://www.reddit.com/r/MachineLearning/comments/c51s2e/machine_learning_bootcamp_become_a_ml_engineer_in/,HannahHumphreys,1561433766,[removed],0,1
1565,2019-6-25,2019,6,25,13,c52ln7,Machine Learning &amp; Ethics @ Microsoft,https://www.reddit.com/r/MachineLearning/comments/c52ln7/machine_learning_ethics_microsoft/,JerryNixon,1561437712,,0,1
1566,2019-6-25,2019,6,25,13,c52q2d,Learning Specific Topics in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/c52q2d/learning_specific_topics_in_machine_learning/,d1shs0ap,1561438327,[removed],0,1
1567,2019-6-25,2019,6,25,13,c52t2k,Real-World Machine Learning Projects with Scikit-Learn,https://www.reddit.com/r/MachineLearning/comments/c52t2k/realworld_machine_learning_projects_with/,HannahHumphreys,1561438750,[removed],0,1
1568,2019-6-25,2019,6,25,14,c538ju,Automotive  LHD S.p.A.  Telescopic Forks,https://www.reddit.com/r/MachineLearning/comments/c538ju/automotive_lhd_spa_telescopic_forks/,lhd121,1561440920,[removed],0,1
1569,2019-6-25,2019,6,25,14,c53eq1,Thoughts on Visual Development,https://www.reddit.com/r/MachineLearning/comments/c53eq1/thoughts_on_visual_development/,chenglu-she,1561441827,[removed],0,1
1570,2019-6-25,2019,6,25,15,c53jr0,Automotive - LHD S.p.A. - Telescopic Forks,https://www.reddit.com/r/MachineLearning/comments/c53jr0/automotive_lhd_spa_telescopic_forks/,lhd121,1561442562,,0,1
1571,2019-6-25,2019,6,25,15,c53l7k,"[D] What is a good Data Science blog to publish in, a blog/online publication that is taken seriously by the DS community?",https://www.reddit.com/r/MachineLearning/comments/c53l7k/d_what_is_a_good_data_science_blog_to_publish_in/,AlexSnakeKing,1561442772,"I have some thoughts I'd like to share on some of the challenges I face in my day in day out work. 

No, I won't be writing another ""Gentle Introduction/ blah, blah, demystified"" type post, but what I feel will be some serious and original insights into my particular corner of the ML/DS world. 

What is a good blog or online publication that would accept my posts. I am OK with a review process, but one time I had to wait for 4 months before my post was reviewed and published (almost as long as an academic paper...), so I'm hoping a place that would publish the posts relatively quickly.

&amp;#x200B;

Any suggestions? Please do not say Towardsdatacience, the quality of that blog has plummeted in the last year.",11,7
1572,2019-6-25,2019,6,25,15,c53t6v,[D] What part of your role as a data scientist do you love the most? And what parts do you hate the most?,https://www.reddit.com/r/MachineLearning/comments/c53t6v/d_what_part_of_your_role_as_a_data_scientist_do/,narenst,1561443932,"As a data scientist, there are a ton of things you have to do every day. Which ones do you look forward to? And which tasks do you usually dread working on?",31,11
1573,2019-6-25,2019,6,25,15,c53xtd,Fine-tuning for Style Transfer with Keras,https://www.reddit.com/r/MachineLearning/comments/c53xtd/finetuning_for_style_transfer_with_keras/,ratonstubley,1561444595,[removed],0,1
1574,2019-6-25,2019,6,25,15,c53y48,Engati - Globally killing it!,https://www.reddit.com/r/MachineLearning/comments/c53y48/engati_globally_killing_it/,getengati,1561444635,[removed],0,1
1575,2019-6-25,2019,6,25,16,c547sv,The Difference Between Playbooks and Runbooks in Incident Response,https://www.reddit.com/r/MachineLearning/comments/c547sv/the_difference_between_playbooks_and_runbooks_in/,MariaMiladinovikj,1561446032,,0,1
1576,2019-6-25,2019,6,25,16,c54mm3,Best Approach for ML on Independent Sets of Tree Like Data Structures,https://www.reddit.com/r/MachineLearning/comments/c54mm3/best_approach_for_ml_on_independent_sets_of_tree/,suddenintent,1561448280,[removed],0,1
1577,2019-6-25,2019,6,25,18,c55k24,"[D] Is there a well maintained list of good ""benchmark"" datasets for ML ?",https://www.reddit.com/r/MachineLearning/comments/c55k24/d_is_there_a_well_maintained_list_of_good/,elcric_krej,1561453700,"I'm looking for up to date datasets to  benchmark various algorithms against the performance (both speed and  accuracy) of published models.

&amp;#x200B;

I've found some dataset but the main issue is that they are either:

&amp;#x200B;

a)  very old and small, e.g. most datasets hosted by UCI, which are rather  ""easy"" to ""solve"" nowadays and most papers using them came out decades  ago. Even barring that, a lot of the papers dealing with the data are not ideal for benchmarks per-say because they are not very specific in their methodology for splitting into train/test/validate.

&amp;#x200B;

OR

&amp;#x200B;

b)  They are focused on images, e.g. cifrar 100 is pretty decent, and there  are loads of high quality models with known accuracy and available  source code... but, I can't find the equivalent of cifrar 100 for, say,  financial timeseries prediction, or STT, or geospatial movement  predictions for cars... or any problem other than image classification  -\_-

&amp;#x200B;

Are  there any well maintained list of datasets that specifically have  various models benchmarked against them ? Or would it be better to just  do reverse-search on this problem, as in, look for interesting papers  that came out in the last few years and use the datasets they used.",8,12
1578,2019-6-25,2019,6,25,18,c55rml,"TRAY WASHING MACHINE, BIN CLEANING MACHINE FOR SALES CT 9444575000",https://www.reddit.com/r/MachineLearning/comments/c55rml/tray_washing_machine_bin_cleaning_machine_for/,Ultramaxhydrojet,1561454870,,0,1
1579,2019-6-25,2019,6,25,18,c561dm,Can anyone explain this ??,https://www.reddit.com/r/MachineLearning/comments/c561dm/can_anyone_explain_this/,zaid1564,1561456393,[removed],0,1
1580,2019-6-25,2019,6,25,18,c5624x,[D] Deep dive into Catboost functionalities for model interpretation,https://www.reddit.com/r/MachineLearning/comments/c5624x/d_deep_dive_into_catboost_functionalities_for/,s0ulmate,1561456514,,0,1
1581,2019-6-25,2019,6,25,18,c56357,With binocular photo..tiny sparrow's,https://www.reddit.com/r/MachineLearning/comments/c56357/with_binocular_phototiny_sparrows/,karenactionsitafaal,1561456662,,0,1
1582,2019-6-25,2019,6,25,19,c56fd1,[Question] How to offload training to a separate PC,https://www.reddit.com/r/MachineLearning/comments/c56fd1/question_how_to_offload_training_to_a_separate_pc/,absk251,1561458499,[removed],0,1
1583,2019-6-25,2019,6,25,19,c56hsx,Augmented Analytics is as Easy as 1-2-3!,https://www.reddit.com/r/MachineLearning/comments/c56hsx/augmented_analytics_is_as_easy_as_123/,ElegantMicroWebIndia,1561458860,,0,1
1584,2019-6-25,2019,6,25,19,c56mv9,Reputation of a university and applying to ML/AI jobs,https://www.reddit.com/r/MachineLearning/comments/c56mv9/reputation_of_a_university_and_applying_to_mlai/,LearnMachineMajor,1561459626,[removed],0,1
1585,2019-6-25,2019,6,25,20,c56suf,[R] Gaussian Process-Based Refinement of Dispersion Corrections,https://www.reddit.com/r/MachineLearning/comments/c56suf/r_gaussian_processbased_refinement_of_dispersion/,akira70000,1561460494,,2,2
1586,2019-6-25,2019,6,25,20,c56wt4,WHERE CAN I START ?,https://www.reddit.com/r/MachineLearning/comments/c56wt4/where_can_i_start/,Z_aki,1561461048,"hello there 

i'm a languages student and i was always obssesed with programing and AI , i got into programing in early age (16 yo) and i'm still practicing and learning  (21 yo) 

is it possible that i can learn and master machine learning knowing that im not good in math's and i always been really bad in it ! 

sorry for the bad english tho , im not a native speaker 

thank you in advance",0,1
1587,2019-6-25,2019,6,25,20,c5728n,ICC Cricket World Cup 2019 Prediction Game Using Machine Learning,https://www.reddit.com/r/MachineLearning/comments/c5728n/icc_cricket_world_cup_2019_prediction_game_using/,Rutvij07,1561461774,[removed],0,1
1588,2019-6-25,2019,6,25,20,c578w7,Join a data science crypto start up,https://www.reddit.com/r/MachineLearning/comments/c578w7/join_a_data_science_crypto_start_up/,abeecrombie,1561462661,[removed],0,1
1589,2019-6-25,2019,6,25,20,c57en3,How do I learn machine learning?,https://www.reddit.com/r/MachineLearning/comments/c57en3/how_do_i_learn_machine_learning/,AmeliaJackson22,1561463480,[removed],0,1
1590,2019-6-25,2019,6,25,21,c57ol1,How Machine Learning Within IncMan SOAR Empowers Security Analysts,https://www.reddit.com/r/MachineLearning/comments/c57ol1/how_machine_learning_within_incman_soar_empowers/,MariaMiladinovikj,1561464930,,0,1
1591,2019-6-25,2019,6,25,21,c57s3y,[R] Differentiable probabilistic models of scientific imaging with the Fourier slice theorem,https://www.reddit.com/r/MachineLearning/comments/c57s3y/r_differentiable_probabilistic_models_of/,hardmaru,1561465418,,2,7
1592,2019-6-25,2019,6,25,21,c5844t,Download Whitepaper (PDF): Using AI to Propel Your Client Engagement,https://www.reddit.com/r/MachineLearning/comments/c5844t/download_whitepaper_pdf_using_ai_to_propel_your/,S_paddy,1561467121,,0,1
1593,2019-6-25,2019,6,25,21,c584du,[R] (Replicating) Modern Neural Networks Generalize on Small Data Sets,https://www.reddit.com/r/MachineLearning/comments/c584du/r_replicating_modern_neural_networks_generalize/,biopsi,1561467158,"I have stumbled across the [NIPS paper](https://papers.nips.cc/paper/7620-modern-neural-networks-generalize-on-small-data-sets.pdf) from Olson, Wyner and Berk on decomposing deep neural networks into decorrelated sub-networks to explain how neural networks generalise well (as de facto ensembles) also on small data sets. 

Since I am trying to get more acquainted with tensorflow and pytorch, I tried to implement the decomposition strategy presented, however, am running into some issues obtaining convergence results similar to the ones presented in the paper. I am new to this subreddit and hope that maybe someone could point me to issues in my replication efforts.

&amp;#x200B;

The authors use the following network architectures they subsequently decompose into subnetworks via linear programming:

\- 10 hidden layers (with elu activation and he initialization) with 100 each

\- binary classification task

\- Adam optimizer with learning rated of 0.001 and 200 training epochs

&amp;#x200B;

I am using the following code to simulate data similar to the synthetic data set used in the paper:

&amp;#x200B;

`np.random.seed(0)`

`def classify(x):`

`if np.linalg.norm(x) &lt; 0.6:   # the paper mentions a radius of 0.3, the images look like 0.6 though`

`return 1`

`else:`

`return 1 if np.random.random() &lt; 0.15 else 0`

`x_ = np.linspace(-1, 1, 20)`

`X_train = np.asarray([(x_[i], x_[j]) for i in range(20) for j in range(20)], dtype=np.float32)`

`y_train = np.apply_along_axis(classify, 1, X_train).reshape(400, 1)`

&amp;#x200B;

Then I have set up the network architecture as follows:

&amp;#x200B;

`X_input = tf.placeholder(tf.float32, [None, 2], name='input_data')`

`y = tf.placeholder(tf.float32, [None, 1])`

&amp;#x200B;

`X = tf.layers.dense(X_input, units=M, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(), activation=tf.nn.elu, name='dense_0')`

`for i in range(1, L):`

`X = tf.layers.dense(X, units=M, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(), activation=tf.nn.elu, name='dense_' + str(int(i)))`

&amp;#x200B;

`logits = tf.layers.dense(X, units=1, activation=None, name='logits')`

`predicted = tf.nn.sigmoid(logits, name='predicted')`

`is_correct = tf.equal(tf.round(predicted), y, name='is_prediction_correct')`

`accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))`

&amp;#x200B;

`cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)`

`l = tf.reduce_mean(cross_entropy)`

`optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(l)`

&amp;#x200B;

To my understanding this should match the implementation details of the paper, however, while the paper limits the training epochs to 200 and even mentions, that 

""*In practice, we found that networks without dropout achieved 100% training accuracy after a couple dozen epochs of training.""*

my implementation only converges after 500+ epochs at best. 

Does anyone here have ideas about what I could be doing wrong?",0,5
1594,2019-6-25,2019,6,25,22,c58e34,[Discussion] Scaling a massive Deep Learning model. Opinions on the method described?,https://www.reddit.com/r/MachineLearning/comments/c58e34/discussion_scaling_a_massive_deep_learning_model/,jikkii,1561468469,"Hi, r/MachineLearning,

&amp;#x200B;

A week ago, at [Hugging Face](https://huggingface.co), we released an app which works with GPT-2 to provide a helper when writing texts. It required using GPT-2 as a backend, which is a very heavy model (the medium-sized one weighs 1.7GB).

I wrote a Medium article detailing the approach we took to scale it and to stay online for the \~10,000 users we had in the first few days. I would really like to know your opinion on the matter and if you have used other methods to take full advantage of the machines you were running your model on.

&amp;#x200B;

Here is the [Medium post](https://medium.com/huggingface/scaling-a-massive-state-of-the-art-deep-learning-model-in-production-8277c5652d5f).

&amp;#x200B;

What do you think?",0,2
1595,2019-6-25,2019,6,25,22,c58jwi,"Alexa scientist: How to do multilabel classification  classifying images by objects, texts by topics, etc.",https://www.reddit.com/r/MachineLearning/comments/c58jwi/alexa_scientist_how_to_do_multilabel/,georgecarlyle76,1561469245,,0,1
1596,2019-6-25,2019,6,25,22,c58nfs,Neural Machine Translation With Attention Mechanism: Step-by-step Guide,https://www.reddit.com/r/MachineLearning/comments/c58nfs/neural_machine_translation_with_attention/,Victor_Stakh,1561469716,,0,1
1597,2019-6-25,2019,6,25,22,c58wlh,[D] Hey Reddit! I need your help in creating a platform for AI freelancers. Could you please fill out this survey? It should take less than 8 minutes. I would be really grateful. Thank you :),https://www.reddit.com/r/MachineLearning/comments/c58wlh/d_hey_reddit_i_need_your_help_in_creating_a/,goodturtle,1561470880,,0,1
1598,2019-6-25,2019,6,25,22,c58x5g,[D] Help with the structure of LSTM networks,https://www.reddit.com/r/MachineLearning/comments/c58x5g/d_help_with_the_structure_of_lstm_networks/,SocioButt,1561470946,"I've gotten fairly interested in neural networks and machine learning, and wanted to learn more about them. So, what better way to do that than to make some yourself? I didn't want to use already existing libraries, like TensorFlow, to do it. I wanted to learn how to make one, from the ground up. So, I set out to make a library of my own in C#. Things have gone well. I've tried various types of networks, and I've gotten them to work. But, recently, I decided that I wanted to try out RNN/LSTM. This is where I've gotten a bit stuck.

I understand the concept of it, the math of it is clear to me. However, I am having some difficulties in getting the architecture of it all laid out. Most of the models I've seen of a RNN/LSTM use only one input. Or, at least, that's how I've interpreted it, and I may be wrong.

Additionally, how do you size the layers? From what I've gathered, each LSTM ""gate"" is its own layer. If you want to use more than 1 point of data, do you line up several of those gates and connect the output from gate 1 to gate 2, and add the 2nd point of input to gate 2?

Most of the research that I've found on neural networks have been pretty straightforward, and I've been able to apply it fairly easily. But, when it comes to RNN/LSTM, I haven't been able to find much. Most of the help out there is related to TensorFlow, and doesn't tell me the fundamental structure of the network. And the ones that I've found on the fundamental structure of the network all seem to contradict each other, so I'm left scratching my head.

I figured if anyone could help me out with this, it would be the good people of this subreddit. Any help would be appreciated!",17,1
1599,2019-6-25,2019,6,25,22,c58yrk,ICC Cricket World Cup 2019 Game Prediction - Using Machine Learning,https://www.reddit.com/r/MachineLearning/comments/c58yrk/icc_cricket_world_cup_2019_game_prediction_using/,Rutvij07,1561471143," [ICC Cricket](https://github.com/RutvijBhutaiya/Cricket-World-Cup-2019) [World Cup 2019](https://github.com/RutvijBhutaiya/Cricket-World-Cup-2019) Precision project is built on R with the help of Random Forest and Logistic Regression techniques. Project Execution explained in Easy Steps,

Project Link: [https://github.com/RutvijBhutaiya/Cricket-World-Cup-2019](https://github.com/RutvijBhutaiya/Cricket-World-Cup-2019)

Machine Learning techniques have been deployed to predict the match outcome, even before the game toss has been flipped.

Based on the more than last 10 years ODI cricket match dataset, machine learning is helping to predict the World Cup 2019 match result.

Interesting! Stay tuned for more details on [Github](https://github.com/RutvijBhutaiya/Cricket-World-Cup-2019) .

NOTE: Machine Learning Models has options to improve the prediction results based on current tournament results too.",0,1
1600,2019-6-25,2019,6,25,23,c5946a,"MultiNomial Classification Where Y is multiple probabilities and X is 1,000+ dummy variables",https://www.reddit.com/r/MachineLearning/comments/c5946a/multinomial_classification_where_y_is_multiple/,BruinBoy815,1561471788,"Hello All, 

&amp;#x200B;

I'm currently working on a project for school and for my capstone. So this is a fun problem I've been struggling with would love for others to chime in. Currently, I have a multinomial (ordinal) problem. 

\- I have 6 different label buckets Grade A, Grade B, Grade C, Grade D, Grade E, Grade F, where Grade A &gt; Grade B &gt; Grade C .... (This is Y or what i'm trying to predict)

\- I have 2000+ dummy variables, showing whether something is present or not. (my X, there is a very good chance that there is some high correlation between some of the X's). Some of these are powerful predictors others are not. 

\- Due to the nature of the data, the ""true value"" is masked and I do not have specific labels in my training data, but instead a probability associated for it. For example, the first observation would have Grade A \[20%\], Grade B \[20%\], Grade C \[30%\], Grade D \[30%\], Grade E \[0%\]. Very little (2%) of the training data I have has one of the grade probability of greater than 50%.

&amp;#x200B;

My objective is to predict which grade it is. I spent alot of time looking into a way to refine my knowledge of the grades, but could not. For example, knowing that something with X,Y,Z is 100% in grade C, but I could not do that and I'm stuck with this discrete probability spectrum. F

&amp;#x200B;

rom my knowledge of statistics, most methods would not take a probabilistic y as training or at least prefer some cases where there is a Perfect label on it. I don't really know much techniques that would be able to handle something like this. I feel a bayesian way to solve the system would be really efficient, but for now I feel the best way to tackle this is to use a random forest, and prune the trees. Anyone have any thoughts?",0,1
1601,2019-6-25,2019,6,25,23,c59ikz,"[R] It costs $245,000 to train the XLNet model..(512 TPU v3 chips * 2.5 days * $8 a TPU)",https://www.reddit.com/r/MachineLearning/comments/c59ikz/r_it_costs_245000_to_train_the_xlnet_model512_tpu/,MassivePellfish,1561473533,"The cost of state of the art these days.

From: https://twitter.com/eturner303/status/1143174828804857856",83,248
1602,2019-6-25,2019,6,25,23,c59pbw,"Machine Learning In Retail Sector: Benefits, Use Case Explained",https://www.reddit.com/r/MachineLearning/comments/c59pbw/machine_learning_in_retail_sector_benefits_use/,vijay2208,1561474351,,0,1
1603,2019-6-26,2019,6,26,0,c59uz2,Elon Musk- The revolutionary techie from a teenage video game to adult tag Space dive,https://www.reddit.com/r/MachineLearning/comments/c59uz2/elon_musk_the_revolutionary_techie_from_a_teenage/,tailorvikas56,1561475013,,0,1
1604,2019-6-26,2019,6,26,0,c59yto,"Supervisely June Update: Keypoints, Python Scripts, and more!",https://www.reddit.com/r/MachineLearning/comments/c59yto/supervisely_june_update_keypoints_python_scripts/,tdionis,1561475472,,0,1
1605,2019-6-26,2019,6,26,0,c59zit,"Supervisely June Update: Keypoints, Python Scripts, and more!",https://www.reddit.com/r/MachineLearning/comments/c59zit/supervisely_june_update_keypoints_python_scripts/,tdionis,1561475547,,0,1
1606,2019-6-26,2019,6,26,0,c5a0jt,What are machine learning model governance and model operations,https://www.reddit.com/r/MachineLearning/comments/c5a0jt/what_are_machine_learning_model_governance_and/,gradientflow,1561475673,,0,1
1607,2019-6-26,2019,6,26,1,c5b74h,How To Label Data,https://www.reddit.com/r/MachineLearning/comments/c5b74h/how_to_label_data/,TalkingJellyFish,1561480540,,0,5
1608,2019-6-26,2019,6,26,2,c5bnph,the daily struggle of data scientist,https://www.reddit.com/r/MachineLearning/comments/c5bnph/the_daily_struggle_of_data_scientist/,getlessreddit,1561482367,[removed],0,1
1609,2019-6-26,2019,6,26,2,c5brsr,[P] Neural Style Transfer with Adversarially Robust Classifiers,https://www.reddit.com/r/MachineLearning/comments/c5brsr/p_neural_style_transfer_with_adversarially_robust/,[deleted],1561482803,[deleted],0,1
1610,2019-6-26,2019,6,26,2,c5btsk,10 Machine Learning Methods that Every Data Scientist Should Know,https://www.reddit.com/r/MachineLearning/comments/c5btsk/10_machine_learning_methods_that_every_data/,castanan2,1561483017,,0,1
1611,2019-6-26,2019,6,26,2,c5bwzg,"[D] Blogs, Podcasts and resources for machine learning engineers and data scientists",https://www.reddit.com/r/MachineLearning/comments/c5bwzg/d_blogs_podcasts_and_resources_for_machine/,ixeption,1561483371," 

I recently posted my list of resources I use to stay up-to date and to get valuable information. I think its a good idea to   share it here too. Its a list of Data Science and Machine Learning Resources: Blogs, Podcasts, Newsletters and Twitter accounts. **Feel free to add resources you like in the comments.**

[Original article here](http://digital-thinking.de/blogs-podcasts-and-resources-for-machine-learning-engineers-and-data-scientists/)

## Newsletter

* [data sciene weekly newsletter](https://www.datascienceweekly.org/)

## Blogs

* [eBay tech](https://www.ebayinc.com/stories/blogs/tech/)
* [Google ai](https://ai.googleblog.com/)
* [Zalando tech](https://jobs.zalando.com/tech/blog/)
* [Airbnb engineering](https://medium.com/airbnb-engineering)
* [Databricks engineering](https://databricks.com/blog/category/engineering)
* [google cloud](https://cloud.google.com/blog/products/data-analytics)
* [Lyrn AI](https://www.lyrn.ai/)
* [Distill](https://distill.pub/)
* [Open AI blog](https://openai.com/blog/)
* ([My Blog](http://digital-thinking.de/))

## Podcasts

* [Coding Blocks](https://www.codingblocks.net/)
* [Data Skeptic](https://dataskeptic.com/)
* [Lex Fridman](https://lexfridman.com/ai/)

## Twitter

* [TensorFlow](https://twitter.com/tensorflow)
* [sci-kit learn](https://twitter.com/scikit_learn)
* [NeurIPS](https://twitter.com/neuripsconf)
* [ICML](https://twitter.com/icmlconf)
* [Papers with Code](https://twitter.com/paperswithcode)
* [datmoAI](https://twitter.com/datmoAI)
* [Dataiku](https://twitter.com/dataiku)
* [facebookAI](https://twitter.com/facebookai)

## Youtube

* [StatQuest](https://www.youtube.com/user/joshstarmer/videos)
* [3Blue1Brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw/featured)

## All-time favorites

* [Why Should Engineers and Scientists Be Worried About Color?](https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxiZXJuaWNlcm9nb3dpdHp8Z3g6NzIzNjZkYTY1ODYyYzk3Mw)
* [What every programmer should know](https://github.com/mtdvio/every-programmer-should-know) 
* [Kernel functions in machine learning](http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/)
* [A Recipe for Training Neural Networks](https://karpathy.github.io/2019/04/25/recipe/)
* [A visual introduction to machine learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)
* [CS 229  Machine learning cheat sheet](https://stanford.edu/~shervine/teaching/cs-229/)
* [10 roles in data science](https://www.kdnuggets.com/2018/08/top-10-roles-ai-data-science.html)",6,9
1612,2019-6-26,2019,6,26,2,c5cgtb,[P] Hands-on guide to model explainability (sklearn and XGBoost),https://www.reddit.com/r/MachineLearning/comments/c5cgtb/p_handson_guide_to_model_explainability_sklearn/,ericman93,1561485561,,0,1
1613,2019-6-26,2019,6,26,3,c5ckod,Recent meta-learning papers without updating network parameters per task?,https://www.reddit.com/r/MachineLearning/comments/c5ckod/recent_metalearning_papers_without_updating/,RSchaeffer,1561485980,[removed],0,1
1614,2019-6-26,2019,6,26,3,c5d609,Innovations in Graph Representation Learning,https://www.reddit.com/r/MachineLearning/comments/c5d609/innovations_in_graph_representation_learning/,sjoerdapp,1561488381,,0,1
1615,2019-6-26,2019,6,26,4,c5di10,"""I would start an AI company whose goal would be to teach computers how to read, so that they can absorb and understand all the written knowledge of the world - Bill Gates on what kind if company he would start today.",https://www.reddit.com/r/MachineLearning/comments/c5di10/i_would_start_an_ai_company_whose_goal_would_be/,alvisanovari,1561489727,[removed],0,1
1616,2019-6-26,2019,6,26,4,c5dsbk,Microsoft Obj-GAN Turns Words into Complex Scenes - SyncedReview - Medium,https://www.reddit.com/r/MachineLearning/comments/c5dsbk/microsoft_objgan_turns_words_into_complex_scenes/,Yuqing7,1561490916,,0,1
1617,2019-6-26,2019,6,26,4,c5e4r6,[R] Seeking model makers for a research interview (comp provided),https://www.reddit.com/r/MachineLearning/comments/c5e4r6/r_seeking_model_makers_for_a_research_interview/,BTV-Texas,1561492328,"Hello all,

&amp;#x200B;

We are working with a client who would like to better understand machine learning/deep learning ""model consumers."" We define a model consumer as someone who finds, builds on, and tweaks models made by others for his/her own needs.

&amp;#x200B;

We are seeking research participants that:

&amp;#x200B;

    Are model ""consumers"" at least half the time (as opposed to only model makers)

    Work directly with machine or deep learning technologies

    Consider themselves end users for apps or tools for building solutions using ML/DL (e.g. TensorFlow, PyTorch, Keras, etc.)

    Typically try to resolve questions on data sets via ML

    Are involved in training models, etc.

    Are applying ML/DL for profit, startup, commercial, or governmental applications (i.e. not university setting)

    Are not employees of Microsoft Azure, Google AI, Amazon Sagemaker, IBM, Data Robot, XILINX

&amp;#x200B;

We are conducting 90-minute interviews with a compensation (based on experience) for your/their time. These interviews will be done via Google Hangouts. The first 45 minutes is Q&amp;A. The last 45 minutes is getting feedback on conceptual screens. I know this seems like a long time, but deep learning requires deep discussion.

&amp;#x200B;

&amp;#x200B;

I would appreciate any leads that fit the above.

&amp;#x200B;

Thanks everyone",0,1
1618,2019-6-26,2019,6,26,5,c5e9ch,[R] Is Elmo equivalent to Fasttext+Bi-directional GRU?,https://www.reddit.com/r/MachineLearning/comments/c5e9ch/r_is_elmo_equivalent_to_fasttextbidirectional_gru/,atif_hassan,1561492868," From what I have read, Elmo uses bi-directional LSTM layers to give contextual embeddings for words in a sentence. So if I use a bi-directional LSTM/GRU layer over Fasttext representations of words, will it be the same? If not why? (I know that Fasttext works at the sub-word level while Elmo works at character level)

Also, does it make sense to use a bi-directional LSTM/GRU layer over the representations produced by Elmo?

The task that I am working on is extreme multi-label classification of documents.",2,10
1619,2019-6-26,2019,6,26,5,c5euih,So is it me or did Google just patent dropout?,https://www.reddit.com/r/MachineLearning/comments/c5euih/so_is_it_me_or_did_google_just_patent_dropout/,[deleted],1561495081,[deleted],0,1
1620,2019-6-26,2019,6,26,5,c5ewzk,[R] Developing Tech Ethically,https://www.reddit.com/r/MachineLearning/comments/c5ewzk/r_developing_tech_ethically/,clite31,1561495332,"Hey all!

Im running a tech ethics study and Id love feedback if anyone has a minute to spare!

With companies like Facebook spiraling in the media, I thought it was time to open up the floor for discussion that leads to **actual** **change**. The goal of the survey is to write an article to create more discussion around ethics, but the bigger goal is to eventually pitch Apple/Google with solutions. Which is why a diverse set of opinions is so important here.

Short survey: [https://docs.google.com/forms/d/18d5twj61AHDt8fmK1xXvIDlw4rOcsupqcpLkBaFZSlQ/edit#responses](https://docs.google.com/forms/d/18d5twj61AHDt8fmK1xXvIDlw4rOcsupqcpLkBaFZSlQ/edit?fbclid=IwAR1qvfRuGSeP3BSSrKAB6QWSIbQOEla9RVu82boSsAbmvpo7zCg6FfAEPOg#responses)

(*Results will be shared after the article's completion , or via email if you choose to submit an address.)*",2,2
1621,2019-6-26,2019,6,26,5,c5f3gm,[D] Did Google just patent dropout?,https://www.reddit.com/r/MachineLearning/comments/c5f3gm/d_did_google_just_patent_dropout/,solomondg,1561496000,,0,1
1622,2019-6-26,2019,6,26,6,c5fhmo,AI/ML Podcast,https://www.reddit.com/r/MachineLearning/comments/c5fhmo/aiml_podcast/,craigspencersmith,1561497410,[removed],0,1
1623,2019-6-26,2019,6,26,6,c5fngt,[D] How to feed variable length text data with a temporal structure?,https://www.reddit.com/r/MachineLearning/comments/c5fngt/d_how_to_feed_variable_length_text_data_with_a/,blueclover,1561498000,"I am working on a project that aims to predict stock returns using tweet data. I have been playing with an online dataset from here: [https://github.com/yumoxu/stocknet-dataset](https://github.com/yumoxu/stocknet-dataset). My aim is to feed, for example, tweets for 30 stocks in a day (variable number of tweets every day), and output a vector of stock return predictions for those 30 stocks. Since each tweet has different length, I was thinking to implement a RNN to feed in the words sequentially. It then seems to me the model will then capture the ""temporal structure"" of the text, but I am not sure how to capture the time series aspect of the data.

&amp;#x200B;

My questions can be summarised as follows:

(1) How to incorporate the time series as well as the textual temporal structure in the data I have?

(2) Or I am modelling my problem wrongly?

&amp;#x200B;

Any ideas or references will be greatly appreciated. Cheers!",7,0
1624,2019-6-26,2019,6,26,6,c5frj1,"What video editing software offers the best resolution upscaling and uses ""deep AI learning machine""",https://www.reddit.com/r/MachineLearning/comments/c5frj1/what_video_editing_software_offers_the_best/,joshem8,1561498414,[removed],0,1
1625,2019-6-26,2019,6,26,6,c5fsvi,[Discussion] Machine Learning Uses In Sociology/Psychology,https://www.reddit.com/r/MachineLearning/comments/c5fsvi/discussion_machine_learning_uses_in/,xyzxyzabc,1561498557,"Hello, i've recently stumbled upon: [Psychopathy Prediction Based on Twitter Usage](https://www.kaggle.com/c/twitter-psychopathy-prediction/overview) and i've been wondering if there any other cool applications of ml in these fields. Im the most interested in user profiling based on social media activity(papers, blog posts, kaggle competitions previous or active)

&amp;#x200B;

Thanks!",3,6
1626,2019-6-26,2019,6,26,6,c5fuih,Deep learning does not outperform simpler methods,https://www.reddit.com/r/MachineLearning/comments/c5fuih/deep_learning_does_not_outperform_simpler_methods/,sarcasticgradient,1561498723,,0,1
1627,2019-6-26,2019,6,26,6,c5g2e5,Deep Learning based recommender @dailymotion,https://www.reddit.com/r/MachineLearning/comments/c5g2e5/deep_learning_based_recommender_dailymotion/,YvesMFr,1561499538,"Hi,

For those of you interested in deep learning and recommender system, we described here how my team rebuilt our internal recommender systems :
 https://medium.com/dailymotion/building-modern-recommender-systems-when-deep-learning-meets-product-principles-c79b16375109

Would be happy to exchange around this type of works!",0,1
1628,2019-6-26,2019,6,26,6,c5g33a,Probabilistic Category-Level Pose Estimation via Segmentation and Predicted-Shape Priors,https://www.reddit.com/r/MachineLearning/comments/c5g33a/probabilistic_categorylevel_pose_estimation_via/,drx833,1561499614,,1,3
1629,2019-6-26,2019,6,26,6,c5g3k3,sort of new AI Podcast,https://www.reddit.com/r/MachineLearning/comments/c5g3k3/sort_of_new_ai_podcast/,craigspencersmith,1561499662,[removed],0,1
1630,2019-6-26,2019,6,26,7,c5ggjg,Yann Lecun interview,https://www.reddit.com/r/MachineLearning/comments/c5ggjg/yann_lecun_interview/,craigspencersmith,1561500999,[removed],0,1
1631,2019-6-26,2019,6,26,7,c5gics,Lessons Learnt by Facebook Data Scientist Brandon Rohrer,https://www.reddit.com/r/MachineLearning/comments/c5gics/lessons_learnt_by_facebook_data_scientist_brandon/,kal138,1561501183,,0,1
1632,2019-6-26,2019,6,26,8,c5hmdm,Can you tell me how to break this captcha?,https://www.reddit.com/r/MachineLearning/comments/c5hmdm/can_you_tell_me_how_to_break_this_captcha/,RyanSu98,1561505886,"## Hi, Guys
### CAPTCHA Example
![](https://s.suruifu.com/img/20190625235549.png)

### Feature
* The characters are very regular
* Random thick interference lines have been added

### Known information
* The captcha is very easy to recognize if you remove the interference wire
* If you don't get rid of the interference lines, the captcha is a disaster
* However, the interference line is very similar to the size of the characters, and no obvious characteristics of the interference line have been found
  * Is not a straight line
  * The thickness varies
  * Position uncertainty
  * And the thickness of the characters, color similar, and mutual overlay

### What I need
* Can you find the signature of this interference line? Or how do you get rid of this interference line?
* Could you recommend some relevant learning materials for my reference?

## Thank you!",0,1
1633,2019-6-26,2019,6,26,8,c5hukv,The 1.5B Grover GPT-2 model has been released,https://www.reddit.com/r/MachineLearning/comments/c5hukv/the_15b_grover_gpt2_model_has_been_released/,xplkqlkcassia,1561506998,[removed],0,1
1634,2019-6-26,2019,6,26,8,c5hvdw,[R] The 1.5B Grover GPT-2 model has been released,https://www.reddit.com/r/MachineLearning/comments/c5hvdw/r_the_15b_grover_gpt2_model_has_been_released/,xplkqlkcassia,1561507105,https://github.com/rowanz/grover,0,1
1635,2019-6-26,2019,6,26,9,c5i8nf,I think Neural Nets can help r/estoration,https://www.reddit.com/r/MachineLearning/comments/c5i8nf/i_think_neural_nets_can_help_restoration/,fuckEAandTheirGame,1561508919,[removed],1,1
1636,2019-6-26,2019,6,26,9,c5iia0,[D] I think Neural Nets can help r/estoration,https://www.reddit.com/r/MachineLearning/comments/c5iia0/d_i_think_neural_nets_can_help_restoration/,fuckEAandTheirGame,1561510218,"The idea of r/estoration is to restorate old images and remove the little specs, tears, discolorations, etc... And I think that this process could be easily automated for your average image via neural networks, I mean trying to recover data from an image as already been done (denoising autoencoder, super resolution for example), the only thing is that what is one of the most valuable thing (IMO) in Deep Learning is the dataset, plenty of autoencoder/GAN are out there but they are not worth anything if they can't be trained, hopefully someone or mutiple people (and that obviously will make things faster) reading this is/are motivated to make a dataset.

&amp;#x200B;

So I am just hoping to spread the word around, if anyone have the patience to make a dataset, maybe even if you have the resources to train a model/host a website for people to use to restore their images plenty of people will be grateful, I really think there is something to be done here.

&amp;#x200B;

^(Also some people will probably tell me ""can't you do it?"", in theory I can, in practice I don't have the patience to gather 500+ images and edit them, also I have a r9 390 so I am a bit limited hardware-side, I use plaid-ML+keras on windows 10 for those wondering, plus if I spread the word around hopefully someone with a lot more of ressources/knowledge than me can do something 10x better than what I could.)

^(Speaking of knowledge, I have thought about the architecture of the NN, I think in a first time an autoencoder would be easier (maybe a gan after, a fully convolutionnal one (so that it can accept most images sizes, with strides of 2 instead of pools, that output a 3 layer image that is then added to the input image to produce the output image, so as to reduce blurriness and maybe use dssim as loss, I will just leave that out there in case someone wants a ""guide"" instead of using a random autoencoder.)",7,1
1637,2019-6-26,2019,6,26,9,c5il8b,[R] Iterative Model-Based Reinforcement Learning Using Simulations in the Differentiable Neural Computer,https://www.reddit.com/r/MachineLearning/comments/c5il8b/r_iterative_modelbased_reinforcement_learning/,inarrears,1561510623,,1,5
1638,2019-6-26,2019,6,26,10,c5is9e,"[R] One neuron is more informative than a deep neural network for aftershock pattern forecasting (TL;DR AUC of 2 parameter model = AUC of 13,451 parameter model)",https://www.reddit.com/r/MachineLearning/comments/c5is9e/r_one_neuron_is_more_informative_than_a_deep/,sensetime,1561511569,,78,389
1639,2019-6-26,2019,6,26,10,c5j3sk,[R] Exploring Model-based Planning with Policy Networks,https://www.reddit.com/r/MachineLearning/comments/c5j3sk/r_exploring_modelbased_planning_with_policy/,baylearn,1561513111,,1,8
1640,2019-6-26,2019,6,26,11,c5jdxw,Machine Learning Bootcamp: Become a ML Engineer in 6 months. Job Guaranteed.,https://www.reddit.com/r/MachineLearning/comments/c5jdxw/machine_learning_bootcamp_become_a_ml_engineer_in/,HannahHumphreys,1561514496,[removed],0,1
1641,2019-6-26,2019,6,26,11,c5jpac,Suggestions for some good reading material on Computer Vision,https://www.reddit.com/r/MachineLearning/comments/c5jpac/suggestions_for_some_good_reading_material_on/,kkziga,1561516034,[removed],0,1
1642,2019-6-26,2019,6,26,11,c5jtea,a ch mua t bnh kem mini gi r  H Ni - My Thc Phm Trng Pht - Medium,https://www.reddit.com/r/MachineLearning/comments/c5jtea/a_ch_mua_t_bnh_kem_mini_gi_r__h_ni_my/,truongphat247,1561516590,,0,1
1643,2019-6-26,2019,6,26,12,c5k59e,Generating Question-Answer Hierarchies,https://www.reddit.com/r/MachineLearning/comments/c5k59e/generating_questionanswer_hierarchies/,rahulbhalley,1561518228,,1,1
1644,2019-6-26,2019,6,26,12,c5kgwc,Google Collaboratory GPU not working???,https://www.reddit.com/r/MachineLearning/comments/c5kgwc/google_collaboratory_gpu_not_working/,nk12312,1561519871,[removed],0,1
1645,2019-6-26,2019,6,26,13,c5l8lg,LHD S.p.A Is Worldwide Leader in designing and building Telescopic Forks | Telescopic Fork,https://www.reddit.com/r/MachineLearning/comments/c5l8lg/lhd_spa_is_worldwide_leader_in_designing_and/,lhd121,1561523911,[removed],0,1
1646,2019-6-26,2019,6,26,15,c5m8uj,Effect of batch size on training?,https://www.reddit.com/r/MachineLearning/comments/c5m8uj/effect_of_batch_size_on_training/,008karan,1561529696,[removed],0,1
1647,2019-6-26,2019,6,26,15,c5m9fu,Machine Learning with Windows 10 + NVIDIA GPU?,https://www.reddit.com/r/MachineLearning/comments/c5m9fu/machine_learning_with_windows_10_nvidia_gpu/,mgavaudan,1561529807,[removed],0,1
1648,2019-6-26,2019,6,26,15,c5mcxl,GarageBand for Mac Tutorial - Complete Beginners Guide 2. Downloading &amp; ...,https://www.reddit.com/r/MachineLearning/comments/c5mcxl/garageband_for_mac_tutorial_complete_beginners/,saanj91,1561530520,,0,1
1649,2019-6-26,2019,6,26,15,c5mdm5,[D] Google's patent on Dropout just went active today,https://www.reddit.com/r/MachineLearning/comments/c5mdm5/d_googles_patent_on_dropout_just_went_active_today/,SilentTheme,1561530673,"""System and method for addressing overfitting in a neural network""

&amp;#x200B;

2016-08-02

Application granted

2017-10-05

Assigned to GOOGLE LLC

2019-06-25

Application status is Active

&gt;A system for training a neural network. A switch is linked to feature detectors in at least some of the layers of the neural network. For each training case, the switch randomly selectively disables each of the feature detectors in accordance with a preconfigured probability. The weights from each training case are then normalized for applying the neural network to test data.

[https://patents.google.com/patent/US9406017B2/en](https://patents.google.com/patent/US9406017B2/en)",134,176
1650,2019-6-26,2019,6,26,15,c5mi70,[Discussion] Basics of Linear Algebra for Machine Learning by Jason Brownlee,https://www.reddit.com/r/MachineLearning/comments/c5mi70/discussion_basics_of_linear_algebra_for_machine/,MavSidharth,1561531657,"Has anyone read this book?

If yes I will really like to hear from you guys about how it was and who do you think this book is for.

&amp;#x200B;

Thank you!",0,1
1651,2019-6-26,2019,6,26,15,c5mjs3,How to save image after clustering using kmeans?,https://www.reddit.com/r/MachineLearning/comments/c5mjs3/how_to_save_image_after_clustering_using_kmeans/,phdsudip,1561531997,[removed],0,1
1652,2019-6-26,2019,6,26,15,c5mk7n,Context based text similarity,https://www.reddit.com/r/MachineLearning/comments/c5mk7n/context_based_text_similarity/,LolSalaam,1561532093,[removed],0,1
1653,2019-6-26,2019,6,26,16,c5mwxc,[Discussion]Basics of Linear Algebra for Machine Learning by Jason Brownlee,https://www.reddit.com/r/MachineLearning/comments/c5mwxc/discussionbasics_of_linear_algebra_for_machine/,MavSidharth,1561534713,"&amp;#x200B;

[Has anyone read this book?](https://i.redd.it/8bvfz80mln631.png)

If yes, I will really love to know what you guys think about this book. Who this book is for, what you learnt, anything about it!!

&amp;#x200B;

Thank you! :)",0,0
1654,2019-6-26,2019,6,26,17,c5n4gw,Building Recommender Systems with Machine Learning and AI,https://www.reddit.com/r/MachineLearning/comments/c5n4gw/building_recommender_systems_with_machine/,funaf2018,1561536441,,0,1
1655,2019-6-26,2019,6,26,17,c5na7q,Performance Evaluation of Apriori and FP-Growth Algorithms,https://www.reddit.com/r/MachineLearning/comments/c5na7q/performance_evaluation_of_apriori_and_fpgrowth/,CrispyChrisChicken,1561537830,[removed],0,1
1656,2019-6-26,2019,6,26,17,c5nbmk,[P] Neural Style Transfer with Adversarially Robust Classifiers,https://www.reddit.com/r/MachineLearning/comments/c5nbmk/p_neural_style_transfer_with_adversarially_robust/,chisai_mikan,1561538157,"Interest [article](https://reiinakano.com/2019/06/21/robust-neural-style-transfer.html) investigating the effect of using adversarially robust classifiers for neural style transfer, with a few examples. I felt the second set of examples is better than the first one at the beginning of the article, as it has more examples and also can compare ResNet vs VGG.

While style transfer using VGG seems to look less of an image filter than a ResNet, it seems that using a adversarially robust ResNet helps and brings it closer to VGG at least qualitatively.

[link to article](https://reiinakano.com/2019/06/21/robust-neural-style-transfer.html)

[link to colab](https://colab.research.google.com/github/reiinakano/adversarially-robust-neural-style-transfer/blob/master/Robust_Neural_Style_Transfer.ipynb)",1,19
1657,2019-6-26,2019,6,26,17,c5nc89,[P] A Visual Intro to NumPy and Data Representation,https://www.reddit.com/r/MachineLearning/comments/c5nc89/p_a_visual_intro_to_numpy_and_data_representation/,nortab,1561538284,"Hello r/machinelearning,

&amp;#x200B;

I've always been intrigued to visually map out the main parts of the NumPy API to serve as a gentle intro. Here it is:

[https://jalammar.github.io/visual-numpy/](https://jalammar.github.io/visual-numpy/)",19,221
1658,2019-6-26,2019,6,26,17,c5nhkx,New Gesture Generation Model,https://www.reddit.com/r/MachineLearning/comments/c5nhkx/new_gesture_generation_model/,Svito-zar,1561539578,[removed],0,1
1659,2019-6-26,2019,6,26,17,c5nhl6,I dont understand how an AI can play a game by a different creator?,https://www.reddit.com/r/MachineLearning/comments/c5nhl6/i_dont_understand_how_an_ai_can_play_a_game_by_a/,Dec0yyy,1561539579,[removed],0,1
1660,2019-6-26,2019,6,26,18,c5niju,Clustering embeddings based on our own chosen attributes,https://www.reddit.com/r/MachineLearning/comments/c5niju/clustering_embeddings_based_on_our_own_chosen/,neltherion,1561539773,[removed],0,1
1661,2019-6-26,2019,6,26,18,c5nj4g,[R] Monte Carlo Gradient Estimation in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/c5nj4g/r_monte_carlo_gradient_estimation_in_machine/,inarrears,1561539907,,5,37
1662,2019-6-26,2019,6,26,18,c5nmww,[D] Habits I Picked Up While Learning Machine Learning,https://www.reddit.com/r/MachineLearning/comments/c5nmww/d_habits_i_picked_up_while_learning_machine/,mlvpj,1561540697,"This is a list of habits I adopted since I started learning Deep Learning. They have helped me save time and be more effective.

[http://blog.varunajayasiri.com/practices\_learned\_while\_learning\_machine\_learning.html](http://blog.varunajayasiri.com/practices_learned_while_learning_machine_learning.html)",1,2
1663,2019-6-26,2019,6,26,18,c5nrln,Numberplate spotted in London this morning,https://www.reddit.com/r/MachineLearning/comments/c5nrln/numberplate_spotted_in_london_this_morning/,Kalessar,1561541781,,0,1
1664,2019-6-26,2019,6,26,18,c5nrv9,[R] Clustering embeddings based on our own chosen attributes,https://www.reddit.com/r/MachineLearning/comments/c5nrv9/r_clustering_embeddings_based_on_our_own_chosen/,neltherion,1561541838,"Hi, Here's an example of what I have in mind:

&amp;#x200B;

I have 1000 face images. I use a face recognition model to convert each face to a 128D embedding.

I have a hunch that each embedding has encoded special characteristics of a face such as eye color, jaw type, male or female genders and lots of other attributes.

Now I want to somehow be able to classify each face (each embedding) based on the attribute that I choose. For example I want to find all faces that have green eyes and are male ONLY based on the embeddings.

&amp;#x200B;

One way to do this is to classify the faces based on my attributes (gender, eye color, jaw type, ...) and then train the embeddings on these attributes. But this approach takes a lot of time since I have to either make a dataset of faces containing different attributes or download it from somewhere and train a model on it.

&amp;#x200B;

I was wondering if there is an unsupervised or semi-supervised approach to cluster the embeddings based on the attributes I choose (gender, eye color, jaw type, ...) by only selecting a few of the faces that have these attributes and the model/method automatically tries to cluster the faces based on my chosen attributes.

&amp;#x200B;

Simply finding the nearest neighbor of an embedding isn't enough. For example I may choose a face that has green eyes (a rare factor) and find the nearest faces of that face's embedding, but there is no guarantee that the nearest faces all have green eyes since they may have other stronger similarities that the embedding may have encoded (for example the same beards or brows).

&amp;#x200B;

One way to account for this is to average a couple of embeddings of faces which have green eyes and then try to find the nearest faces. But then again, there is no guarantee that the embeddings are even 'average friendly' meaning that averaging them would results in the attribute that is shared between them to become stronger.

&amp;#x200B;

So is this even possible and what is the fastest and highest quality approach to do it? Thanks.

&amp;#x200B;

**EDIT**: Since I may not have explained my question in full detail, if there are any questions about this post, I'd be glad to explain furthur.",4,3
1665,2019-6-26,2019,6,26,18,c5nwyi,How to print double side at same time by flexo printing machine.,https://www.reddit.com/r/MachineLearning/comments/c5nwyi/how_to_print_double_side_at_same_time_by_flexo/,samhe213,1561542969,,0,1
1666,2019-6-26,2019,6,26,19,c5o19a,[D] What's this subreddit's take on contributing personal blogs to Medium's curated journals?,https://www.reddit.com/r/MachineLearning/comments/c5o19a/d_whats_this_subreddits_take_on_contributing/,doofWario,1561543854,"Hi everyone, I've been a member here for quite a while and after discussion with some members here I started m own ML blog on Medium. 

After gaining some traction, a couple of people representing curated AI journals/websites some on Medium and some outside of Medium have been contacting me about housing my blog on their platform.  Since many of the members here have active blogs and more publishing experience than me, I wanted to ask whats the right choice? Should I keep on going solo? Contribute to one or all of the platforms? And what does it mean to have my blog get published on someone else's platform, in terms of content rights, benefits, etc?",5,1
1667,2019-6-26,2019,6,26,19,c5o1p4,[R] Shaping Belief States with Generative Environment Models for RL,https://www.reddit.com/r/MachineLearning/comments/c5o1p4/r_shaping_belief_states_with_generative/,inarrears,1561543938,,1,7
1668,2019-6-26,2019,6,26,19,c5o2ub,Google launches TensorFlow.Text library for language AI models,https://www.reddit.com/r/MachineLearning/comments/c5o2ub/google_launches_tensorflowtext_library_for/,Mayalittlepony,1561544182,,0,1
1669,2019-6-26,2019,6,26,19,c5o42z,Your favorite library for processing textual data ?,https://www.reddit.com/r/MachineLearning/comments/c5o42z/your_favorite_library_for_processing_textual_data/,lazywiing,1561544443,"Hi reddit !

I am wondering what library you would advise to handle processing of text data. More precisely, I am quite new to NLP and currently working on Deep Learning techniques for NLP tasks (I am working with PyTorch). As such, I am looking for a simple yet effective and modular way to do the following :

\- Load textual data

\- Perform pre-processing steps (tokenization, cleaning, correcting words, spell checker, handling unknown words)

\- Build vocabulary, with &lt;bos&gt;, &lt;eos&gt;, &lt;unk&gt; and &lt;pad&gt; vectors

\- Bucketing / Dynamic padding

\- Load pre-trained word vectors

I used TorchText once, but I found the doc to be incomplete and the lack of resources makes it difficult to master. Also, I encountered some issues I was not able to understand. Maybe you know some efficient ways to perform these steps or could help me find nice resources. I was told that the best solution was to write a custom script with no additional library, but I would prefer if a nice library was available.

Thank you all and have a nice day !",0,1
1670,2019-6-26,2019,6,26,19,c5o77f,Practical advice to implement on models,https://www.reddit.com/r/MachineLearning/comments/c5o77f/practical_advice_to_implement_on_models/,PayalBhatia,1561545104,[removed],0,1
1671,2019-6-26,2019,6,26,20,c5oi3n,To what extent large tech firms have interest in driving research for increasingly complex learning algorithms?,https://www.reddit.com/r/MachineLearning/comments/c5oi3n/to_what_extent_large_tech_firms_have_interest_in/,vicentethomas,1561547298,[removed],0,1
1672,2019-6-26,2019,6,26,20,c5otmt,"Machine Learning Training in Jayanagar, Bangalore | Livewire",https://www.reddit.com/r/MachineLearning/comments/c5otmt/machine_learning_training_in_jayanagar_bangalore/,livewireindia,1561549569,[removed],0,1
1673,2019-6-26,2019,6,26,21,c5oyl4,How to read and write .bmp images using tensorflow and write to disk ?,https://www.reddit.com/r/MachineLearning/comments/c5oyl4/how_to_read_and_write_bmp_images_using_tensorflow/,waterRocket8236,1561550479,[removed],0,1
1674,2019-6-26,2019,6,26,21,c5pb1j,[P] Telegram bot which rates the aesthetics of your photos,https://www.reddit.com/r/MachineLearning/comments/c5pb1j/p_telegram_bot_which_rates_the_aesthetics_of_your/,dominik_mai,1561552657,"Hi guys,

as a part of my master thesis, in which I try to predict how aesthetic an image is, I wrote a telegram bot that can rate images you send to it.

&amp;#x200B;

Actually there are two bots, you can reach the first one at [https://t.me/ImageAestheticsBot](https://t.me/ImageAestheticsBot) which will rate all kind of images. The second one is trained only with photos with people on it, so you can use that for photos like portraits. This one you can reach at [https://t.me/ImageAestheticsPeopleBot](https://t.me/ImageAestheticsPeopleBot) .

&amp;#x200B;

If you like to read something on how image aesthetics can be rated by neuronal nets [NIMA: Neural Image Assessment](https://arxiv.org/abs/1709.05424) is a good start.

&amp;#x200B;

I would really like it if you try this bot and maybe give me some feedback if you think it's useful.

&amp;#x200B;

If you have questions feel free to ask and if you know some other subreddits where this post might fit in please tell me. This is my first post here so any advice about posting, if I did something wrong, would be cool too.

&amp;#x200B;

Thanks in advance!",3,3
1675,2019-6-26,2019,6,26,21,c5pc71,What happened here? (NeurIPS 2018),https://www.reddit.com/r/MachineLearning/comments/c5pc71/what_happened_here_neurips_2018/,yusuf-bengio,1561552860,[removed],0,1
1676,2019-6-26,2019,6,26,21,c5pcv3,"RE:Wheeler experimental framework for OpenAI ""Requests for Research: HER edition""",https://www.reddit.com/r/MachineLearning/comments/c5pcv3/rewheeler_experimental_framework_for_openai/,rezer0dai,1561552981,[removed],0,1
1677,2019-6-26,2019,6,26,21,c5pdkm,[P] Blogpost about how a experiment with GDP-2 became a AI-driven browser game,https://www.reddit.com/r/MachineLearning/comments/c5pdkm/p_blogpost_about_how_a_experiment_with_gdp2/,cpury,1561553102,"I just finished writing the blog post outlining the idea and process behind AI Against Humanity. Maybe it is of interest for some of you. Let me know what you think!   
[https://cpury.github.io/ai-against-humanity/](https://cpury.github.io/ai-against-humanity/)  


Previous posts:

1. [https://www.reddit.com/r/MachineLearning/comments/bvwvoo/p\_ai\_against\_humanity\_play\_cards\_against\_humanity/](https://www.reddit.com/r/MachineLearning/comments/bvwvoo/p_ai_against_humanity_play_cards_against_humanity/)
2. [https://www.reddit.com/r/MachineLearning/comments/c0n5a1/p\_update\_ai\_against\_humanity\_now\_with\_an\_ai/](https://www.reddit.com/r/MachineLearning/comments/c0n5a1/p_update_ai_against_humanity_now_with_an_ai/)",6,16
1678,2019-6-26,2019,6,26,21,c5peko,Machine Learning Tutorial Part 7 | Machine Learning For Beginners - 5-Fold Cross Validation,https://www.reddit.com/r/MachineLearning/comments/c5peko/machine_learning_tutorial_part_7_machine_learning/,SquareTechAcademy,1561553247,,0,1
1679,2019-6-26,2019,6,26,21,c5pi9y,Machine Learning Tutorial Part 7 | Machine Learning For Beginners - 5-Fold Cross Validation,https://www.reddit.com/r/MachineLearning/comments/c5pi9y/machine_learning_tutorial_part_7_machine_learning/,lukescriptwalker,1561553876,,0,1
1680,2019-6-26,2019,6,26,22,c5pl6t,How to deal with uncertainty without using Probability theory?,https://www.reddit.com/r/MachineLearning/comments/c5pl6t/how_to_deal_with_uncertainty_without_using/,ugurbolat,1561554356,[removed],0,1
1681,2019-6-26,2019,6,26,22,c5pmkr,[D] Prior knowledge on Actor-critic / policy gradient methods for portfolio allocation,https://www.reddit.com/r/MachineLearning/comments/c5pmkr/d_prior_knowledge_on_actorcritic_policy_gradient/,tutorialboys,1561554583,"Hey guys,

&amp;#x200B;

So, I have to solve a portfolio allocation problem, which can be formulated as:

given an input (financial indicators), output a vector of weights for assets (that sum up to 1) in order to maximize a ""performance function"".

&amp;#x200B;

Translating this formulation to an RL problem seems pretty straight forward. However, I don't have much data (a couple of hundred data points). So, I was wondering if it is possible to incorporate prior knowledge in order to have a better training with fewer data.

Can I incorporate knowledge by using a ""custom"" advantage function in Actor-critic? What about using Bayesian policy gradient / Actor-critic? 

Does that make sense?

&amp;#x200B;

Thanks!",3,2
1682,2019-6-26,2019,6,26,23,c5qcjo,Aesthetically Pleasing Adversarial Attacks,https://www.reddit.com/r/MachineLearning/comments/c5qcjo/aesthetically_pleasing_adversarial_attacks/,xristos_forokolomvos,1561558574,[removed],0,1
1683,2019-6-26,2019,6,26,23,c5qeaa,Anyone have suggestions for more information on Reinforcement Learning in a negotiation setting?,https://www.reddit.com/r/MachineLearning/comments/c5qeaa/anyone_have_suggestions_for_more_information_on/,numberseed,1561558826,"&amp;#x200B;

I was inspired by the following research from Facebook AI (Links below).  I want to read more about anything related to this.

&amp;#x200B;

[https://github.com/facebookresearch/end-to-end-negotiator](https://github.com/facebookresearch/end-to-end-negotiator)

&amp;#x200B;

[https://code.fb.com/ml-applications/deal-or-no-deal-training-ai-bots-to-negotiate/](https://code.fb.com/ml-applications/deal-or-no-deal-training-ai-bots-to-negotiate/)

&amp;#x200B;

Thanks!",0,1
1684,2019-6-26,2019,6,26,23,c5qee4,Aesthetically Pleasing Adversarial Attacks,https://www.reddit.com/r/MachineLearning/comments/c5qee4/aesthetically_pleasing_adversarial_attacks/,xristos_forokolomvos,1561558842,[removed],0,1
1685,2019-6-26,2019,6,26,23,c5qhub,[D] How to deal with uncertainty without using Probability theory?,https://www.reddit.com/r/MachineLearning/comments/c5qhub/d_how_to_deal_with_uncertainty_without_using/,ugurbolat,1561559345,"Considering the definition: ""We say an environment is **uncertain** if it is not fully observable or not deterministic.""

When making predictions, Probability theory is a really useful tool under uncertainty. I was wondering if there are other ways to make predictions without necessarily using Probability theory but in a way, you take uncertainty into account.",7,1
1686,2019-6-26,2019,6,26,23,c5qnvo,"group Telegram about deep learning ,machine learning , Artificial Intelligence by andrew ng",https://www.reddit.com/r/MachineLearning/comments/c5qnvo/group_telegram_about_deep_learning_machine/,Doctor_who1,1561560223,[removed],0,1
1687,2019-6-26,2019,6,26,23,c5qph8,[R] GAN for sampling correlation matrices,https://www.reddit.com/r/MachineLearning/comments/c5qph8/r_gan_for_sampling_correlation_matrices/,gau_mar,1561560446,,0,1
1688,2019-6-26,2019,6,26,23,c5qr9z,"[N] ""Using Kubernetes for Machine Learning Frameworks"" with Arun Gupta",https://www.reddit.com/r/MachineLearning/comments/c5qr9z/n_using_kubernetes_for_machine_learning/,goto-con,1561560708,"Kubernetes provides isolation, auto-scaling, load balancing, flexibility and GPU support. These features are critical to run computationally and data intensive and hard to parallelize machine learning models. Declarative syntax of Kubernetes deployment descriptors make it easy for non-operationally focused engineers to easily train machine learning models on Kubernetes.

This talk will explain why and how Kubernetes is well suited for training and running your machine learning models in production. Specifically it will show how to setup a variety of open source machine learning frameworks such as TensorFlow, Apache MXNet and Pytorch on a Kubernetes cluster.

Attendees will learn training, massaging and inference phases of setting up a Machine Learning framework on Kubernetes. Attendees will leave with a GitHub repo of fully working samples.

* [Video](https://youtu.be/VgLxcu18bbw?list=PLEx5khR4g7PLIxNHQ5Ze0Mz6sAXA8vSPE)
* [Slides](https://gotochgo.com/2019/sessions/696)",4,16
1689,2019-6-26,2019,6,26,23,c5qt6p,group Telegram about Kaggle by andrew ng,https://www.reddit.com/r/MachineLearning/comments/c5qt6p/group_telegram_about_kaggle_by_andrew_ng/,Doctor_who1,1561560984,"The Main topics that we are going to discuss here are as below:

 Data Science

 Artificial Intelligence

 Machine Learning

 Deep Learning

Please post in English language.

Telegram group

join

[https://t.me/joinchat/CuFqkRQSxRy5M\_6KxhKrCA](https://t.me/joinchat/CuFqkRQSxRy5M_6KxhKrCA)",0,1
1690,2019-6-27,2019,6,27,0,c5qzz0,[R] Author Live Stream: Neural Models of Text Normalization for Speech Applications,https://www.reddit.com/r/MachineLearning/comments/c5qzz0/r_author_live_stream_neural_models_of_text/,tdls_to,1561561895,Live stream and paper:  https://aisc.ai.science/events/2019-06-26/,0,0
1691,2019-6-27,2019,6,27,0,c5r7u3,Speech processing audio parameters,https://www.reddit.com/r/MachineLearning/comments/c5r7u3/speech_processing_audio_parameters/,HStuart18,1561562998,[removed],0,1
1692,2019-6-27,2019,6,27,0,c5r7yl,Opinions on residual deconvolution,https://www.reddit.com/r/MachineLearning/comments/c5r7yl/opinions_on_residual_deconvolution/,HitLuca,1561563015,[removed],0,1
1693,2019-6-27,2019,6,27,0,c5rasj,Model architecture to feature hyperparameter tuning,https://www.reddit.com/r/MachineLearning/comments/c5rasj/model_architecture_to_feature_hyperparameter/,gar1t,1561563412,[removed],0,1
1694,2019-6-27,2019,6,27,0,c5re8n,"group Telegram about Women in Machine Learning and Computer Vision by Fei-Fei Li , Rachel Thomas",https://www.reddit.com/r/MachineLearning/comments/c5re8n/group_telegram_about_women_in_machine_learning/,Doctor_who1,1561563894,[removed],0,1
1695,2019-6-27,2019,6,27,0,c5rg28,[D] Model architecture to test and compare hyperparameter tuning methods,https://www.reddit.com/r/MachineLearning/comments/c5rg28/d_model_architecture_to_test_and_compare/,gar1t,1561564151,I'm looking for a model/dataset that is fast to train (e.g. comparable to logistic regression on MNIST) that can be used to test and compare various hyperparameter search methods without a GPU/accelerator. Model performance should be sensitive to 2-3 hyperparameters. My experience with tuning has been with neural networks that are very time consuming to train on non-accelerated systems.,2,2
1696,2019-6-27,2019,6,27,0,c5rgci,[P] First attempt at removing phones from the mirror selfie using Tensorflow,https://www.reddit.com/r/MachineLearning/comments/c5rgci/p_first_attempt_at_removing_phones_from_the/,abhi3188,1561564192,,0,2
1697,2019-6-27,2019,6,27,0,c5rhr6,"Simple Questions Thread June 26, 2019",https://www.reddit.com/r/MachineLearning/comments/c5rhr6/simple_questions_thread_june_26_2019/,AutoModerator,1561564387,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",0,1
1698,2019-6-27,2019,6,27,1,c5rnru,How I Built a XOR MLP Using Only Sheets Formulas,https://www.reddit.com/r/MachineLearning/comments/c5rnru/how_i_built_a_xor_mlp_using_only_sheets_formulas/,rylandgold,1561565191,,0,1
1699,2019-6-27,2019,6,27,1,c5rtli,Learn Machine Learning 2019|| From Beginner to Intermediate || video lectures of Courses To Follow || Did I Miss Something ??,https://www.reddit.com/r/MachineLearning/comments/c5rtli/learn_machine_learning_2019_from_beginner_to/,_learn_to_earn,1561565971,,0,1
1700,2019-6-27,2019,6,27,1,c5ruw5,Who is in to solve Big Social Problem with AI in a Global Community?,https://www.reddit.com/r/MachineLearning/comments/c5ruw5/who_is_in_to_solve_big_social_problem_with_ai_in/,Lordobba,1561566155,,0,1
1701,2019-6-27,2019,6,27,1,c5rx4q,"Is it possible to build a regression model for predicting worldwide gross of movies using the 'Plot', 'Music' and 'Marketing' sections on their wikipedia pages?",https://www.reddit.com/r/MachineLearning/comments/c5rx4q/is_it_possible_to_build_a_regression_model_for/,shreshths,1561566468,[removed],0,1
1702,2019-6-27,2019,6,27,1,c5s1sq,Project Help,https://www.reddit.com/r/MachineLearning/comments/c5s1sq/project_help/,jonathanbruno,1561567107,[removed],0,1
1703,2019-6-27,2019,6,27,1,c5s5zf,Roborace bridged the historical and the future in Italy!!,https://www.reddit.com/r/MachineLearning/comments/c5s5zf/roborace_bridged_the_historical_and_the_future_in/,AIthatDrives,1561567685,,1,1
1704,2019-6-27,2019,6,27,2,c5scju,"[P] Machine learning application to identify ""risky"" words",https://www.reddit.com/r/MachineLearning/comments/c5scju/p_machine_learning_application_to_identify_risky/,abdane,1561568562,"So I am doing a project to create a model that identifies words in a sentence that are related to risks. I have a large set of data (around 27k lines).

An example of words: Injury, collision, police, hit, fatal, etc...

I am doing this with Python, Sklearn library. Any suggestions on how to approach this?

So far, I have achieved to apply TFIDF on the data and print each word with its relative TFIDF score, I'm not sure if this is usefull at all.

It does output the ""risky"" words, but it also outputs all other words that I do not need. The only way I can filter the risk words out is by typing them on a seperate file, and just compare word by word, but there is no machine learning in that, and I would really like to apply some sort of machine learning (maybe naive bayes?). I am willing to label some data if it helps and make this supervised instead of being unsupervised currently.

&amp;#x200B;

Any help is appreciated :)",5,5
1705,2019-6-27,2019,6,27,2,c5sdso,Facebook Model Pretrained on Billions of Instagram Hashtags Achieves SOTA Results on Top-1 ImageNet - Medium,https://www.reddit.com/r/MachineLearning/comments/c5sdso/facebook_model_pretrained_on_billions_of/,Yuqing7,1561568718,,0,1
1706,2019-6-27,2019,6,27,2,c5se82,"[D] What's with all the Google bashing? Google gave us Transformers, BERT, AutoML, Tensorflow and so much more.",https://www.reddit.com/r/MachineLearning/comments/c5se82/d_whats_with_all_the_google_bashing_google_gave/,Relative_Register,1561568771,"This community is so ungrateful for the contributions Google has made in AI. Yet the top posts are all critiques on Google's efforts to help the field:

&amp;#x200B;

[https://www.reddit.com/r/MachineLearning/comments/c4ylga/d\_misuse\_of\_deep\_learning\_in\_nature\_journals/](https://www.reddit.com/r/MachineLearning/comments/c4ylga/d_misuse_of_deep_learning_in_nature_journals/)

[https://www.reddit.com/r/MachineLearning/comments/c5is9e/r\_one\_neuron\_is\_more\_informative\_than\_a\_deep/](https://www.reddit.com/r/MachineLearning/comments/c5is9e/r_one_neuron_is_more_informative_than_a_deep/)

[https://www.reddit.com/r/MachineLearning/comments/c59ikz/r\_it\_costs\_245000\_to\_train\_the\_xlnet\_model512\_tpu/](https://www.reddit.com/r/MachineLearning/comments/c59ikz/r_it_costs_245000_to_train_the_xlnet_model512_tpu/)

[https://www.reddit.com/r/MachineLearning/comments/c5mdm5/d\_googles\_patent\_on\_dropout\_just\_went\_active\_today/](https://www.reddit.com/r/MachineLearning/comments/c5mdm5/d_googles_patent_on_dropout_just_went_active_today/)

&amp;#x200B;

Stop this or Google will start holding back its efforts to help the community.",8,0
1707,2019-6-27,2019,6,27,2,c5sgp9,Can we use AI to hepl us predict better in gambling,https://www.reddit.com/r/MachineLearning/comments/c5sgp9/can_we_use_ai_to_hepl_us_predict_better_in/,deepsleep09,1561569091,[removed],0,1
1708,2019-6-27,2019,6,27,3,c5ta89,What is machine learning?,https://www.reddit.com/r/MachineLearning/comments/c5ta89/what_is_machine_learning/,r-qndev,1561572616,,0,1
1709,2019-6-27,2019,6,27,3,c5tamb,RecSys 2016: Paper Session 2 - Field Aware Factorization Machines for CTR Prediction,https://www.reddit.com/r/MachineLearning/comments/c5tamb/recsys_2016_paper_session_2_field_aware/,_quanttrader_,1561572660,,0,1
1710,2019-6-27,2019,6,27,3,c5ts1x,Machine Learning on Windows 10 + NVIDIA GPU,https://www.reddit.com/r/MachineLearning/comments/c5ts1x/machine_learning_on_windows_10_nvidia_gpu/,mgavaudan,1561574604,[removed],0,1
1711,2019-6-27,2019,6,27,3,c5tz0a,[R] Neural Proximal/Trust Region Policy Optimization Attains Globally Optimal Policy,https://www.reddit.com/r/MachineLearning/comments/c5tz0a/r_neural_proximaltrust_region_policy_optimization/,banananach,1561575379,"[https://arxiv.org/abs/1906.10306](https://arxiv.org/abs/1906.10306)  


Proximal policy optimization and trust region policy optimization (PPO and TRPO) with actor and critic parametrized by neural networks achieve significant empirical success in deep reinforcement learning. However, due to nonconvexity, the global convergence of PPO and TRPO remains less understood, which separates theory from practice. In this paper, we prove that a variant of PPO and TRPO equipped with overparametrized neural networks converges to the globally optimal policy at a sublinear rate. The key to our analysis is the global convergence of infinite-dimensional mirror descent under a notion of one-point monotonicity, where the gradient and iterate are instantiated by neural networks. In particular, the desirable representation power and optimization geometry induced by the overparametrization of such neural networks allow them to accurately approximate the infinite-dimensional gradient and iterate.",9,9
1712,2019-6-27,2019,6,27,3,c5tzy2,why don't any of study this self supervised learning,https://www.reddit.com/r/MachineLearning/comments/c5tzy2/why_dont_any_of_study_this_self_supervised/,loopy_fun,1561575483,,0,1
1713,2019-6-27,2019,6,27,4,c5ue6b,"#Learn how to make sure you are getting the best #predictions your model can provide. https://buff.ly/2ZFXbr4 @Experfy #MachineLearning #modeltuning #experfy #experfytraining #courses #ai For more courses, visit experfy.com/training",https://www.reddit.com/r/MachineLearning/comments/c5ue6b/learn_how_to_make_sure_you_are_getting_the_best/,tstanya77,1561577047,,0,1
1714,2019-6-27,2019,6,27,4,c5ugup,[D] Your opinions on my residual deconvolution implementation,https://www.reddit.com/r/MachineLearning/comments/c5ugup/d_your_opinions_on_my_residual_deconvolution/,HitLuca,1561577341,"I am currently looking for a solution regarding mask generation given an input image, and my approach works as follows:

\- conv + activation + max-pooling from the input image until i get to a given smaller size. Every block halves the input size, so expect max-pooling after every convolution

\- upsampling + conv + activation until I get back to the input resolution. Expect upsampling before each convolution

Bear with me, this encoder decoder architecture is required, as well as the upsampling + conv instead of transposed convolution (or deconvolution), so take it as given.

&amp;#x200B;

The model works as expected, and in order to improve it's quality I decided to go with residual connections, in particular the full pre-activation variant shown below given it's improved performance. I am not using BatchNormalization, so don't take it into account.

![img](64g76v0e4r631 ""rightmost version, no bn"")

For the encoder, I have each block defined as

\- activation of the input

\- conv

\- max-pooling

\- creation of the shortcut, defined as projection + max\_pooling of the original input

\- addition

&amp;#x200B;

Now, the interesting part: I want to build a residual deconvolutional architecture for the decoder, and I'm not entirely sure if what I ended up with is the right way of doing it:

&amp;#x200B;

\- residual encoder, up to encoded size

\- activation (as the last encoder's residual layer doesn't have it after the addition

&amp;#x200B;

Each block is defined as

\- upsampling

\- activation

\- conv

\- creation of the shortcut, defined as upsampling + projection of the original input

\- addition

&amp;#x200B;

Then I get my mask.

&amp;#x200B;

What are your thoughts regarding my approach? using the traditional approach for downsizing the shortcut using convolutions with kernel\_size of 1 and strides of 2 loses 75% of information at each step, and I would like to find a way to avoid it.

&amp;#x200B;

Thank you for your feedback!",5,3
1715,2019-6-27,2019,6,27,4,c5uhyv,July 11 Talk on Deep Learning with ACM A.M. Turing Laureate Yann LeCun,https://www.reddit.com/r/MachineLearning/comments/c5uhyv/july_11_talk_on_deep_learning_with_acm_am_turing/,ACMLearning,1561577457,,0,1
1716,2019-6-27,2019,6,27,4,c5uq5x,NN bias. Beginner question,https://www.reddit.com/r/MachineLearning/comments/c5uq5x/nn_bias_beginner_question/,Ammarock,1561578346,[removed],0,1
1717,2019-6-27,2019,6,27,5,c5v05g,[Project] Similar image search using pre-trained ResNet50 as image embeddings.,https://www.reddit.com/r/MachineLearning/comments/c5v05g/project_similar_image_search_using_pretrained/,mekass,1561579460,"Hey guys!

&amp;#x200B;

Finished my experimental project where I tried to come up with algorithm to find similar images using pre-trained ResNet50 model for image features generation and cosine similarity as distance metric. At this experiment I used DeepFashion image dataset.  


Take a look! [https://github.com/tomasrasymas/simimg](https://github.com/tomasrasymas/simimg)",21,18
1718,2019-6-27,2019,6,27,5,c5vm2a,How to select your data points when doing stochastic optimization,https://www.reddit.com/r/MachineLearning/comments/c5vm2a/how_to_select_your_data_points_when_doing/,psyyduck,1561581934,"Is there a better way than the uniform that's commonly used? Hopefully something unbiased &amp; minimum variance. I'm finding a few papers [1](https://ml.informatik.uni-freiburg.de/papers/16-ICLR-BatchSelection.pdf) [2](https://arxiv.org/pdf/1401.2753.pdf) and thought to check here before trying to implement any of them.

If it's any help, my data has 2e14 points and I can afford to do 150M of them.",0,1
1719,2019-6-27,2019,6,27,5,c5vt1t,[D] How to select your data points when doing stochastic optimization?,https://www.reddit.com/r/MachineLearning/comments/c5vt1t/d_how_to_select_your_data_points_when_doing/,psyyduck,1561582713,"Is there a better way than the uniform that's commonly used? Hopefully something unbiased &amp; minimum variance. I'm finding a few papers [1](https://ml.informatik.uni-freiburg.de/papers/16-ICLR-BatchSelection.pdf), [2](https://arxiv.org/pdf/1401.2753.pdf) and thought to check here before trying to implement any of them.

If it's relevant, my data has 2e14 points and I can afford to do 150M of them.",2,2
1720,2019-6-27,2019,6,27,6,c5vvr1,What are the Benefits of Data Science and Machine Learning Bootcamp?  in Silicon Valley,https://www.reddit.com/r/MachineLearning/comments/c5vvr1/what_are_the_benefits_of_data_science_and_machine/,Magniminda,1561583011,[removed],0,1
1721,2019-6-27,2019,6,27,6,c5wmcy,How to calculate FLOPs for a CNN model ?,https://www.reddit.com/r/MachineLearning/comments/c5wmcy/how_to_calculate_flops_for_a_cnn_model/,Greglama,1561586102,[removed],0,1
1722,2019-6-27,2019,6,27,7,c5wx1v,AI Against Humanity: Overview,https://www.reddit.com/r/MachineLearning/comments/c5wx1v/ai_against_humanity_overview/,wavelander,1561587359,,0,1
1723,2019-6-27,2019,6,27,7,c5x0ad,Ml for sales and product analysis/prediction,https://www.reddit.com/r/MachineLearning/comments/c5x0ad/ml_for_sales_and_product_analysisprediction/,duyth,1561587721,"Hi guys,
Im new to ML.
So I have 3 year data of sales (including product quantity/stock status and price adjustment logs)

I was tasked with the job to dig through order data to find how we can improve our sales performance. Specially we have identified a few goals below:

1- find products that sold well during X period in the past but are not selling well anymore this year
2- find new top sellers (sold well this year but not in the past)
3- prediction for next quarter so we can plan for stock..
4 - down the toad , we may start gathering competitive data for our analysis (competitors who are selling the same products and their price...) so we can know if competition impacts sales.

With 1 and 2, I think I can adopt excel but it is very tedious work for a small amount of data.
Also , since we need 3 and 4 anyway,  Im hoping to be able to utilise something else to automate this so we can pull data when required and start expanding the use cases further
Hope i can get some suggestions from the community.

Thank you",0,1
1724,2019-6-27,2019,6,27,7,c5xbqo,[P] PyCM 2.3 released: Machine learning library for confusion matrix statistical analysis,https://www.reddit.com/r/MachineLearning/comments/c5xbqo/p_pycm_23_released_machine_learning_library_for/,sepandhaghighi,1561589133,"  

[https://www.pycm.ir](https://www.pycm.ir/)

[https://github.com/sepandhaghighi/pycm](https://github.com/sepandhaghighi/pycm)

&amp;#x200B;

* Adjusted F-score (AGF) added [\#209](https://github.com/sepandhaghighi/pycm/issues/209)
* Overlap coefficient (OC) added [\#212](https://github.com/sepandhaghighi/pycm/issues/212)
* Otsuka-Ochiai coefficient (OOC) added [\#213](https://github.com/sepandhaghighi/pycm/issues/213)
* save\_stat and save\_vector parameters added to save\_obj method [\#210](https://github.com/sepandhaghighi/pycm/issues/210)
* Document modified [\#221](https://github.com/sepandhaghighi/pycm/issues/221)
* [README.md](https://README.md) modified
* Parameters recommendation for imbalance dataset modified
* Minor bug in Compare class fixed
* pycm\_help function modified
* Benchmarks color modified [\#221](https://github.com/sepandhaghighi/pycm/issues/221)",1,17
1725,2019-6-27,2019,6,27,8,c5xmm1,[D] Help with architecture for V2V link duration,https://www.reddit.com/r/MachineLearning/comments/c5xmm1/d_help_with_architecture_for_v2v_link_duration/,ArminBazz,1561590479,"Hey all I was wondering if you guys had any recommendations for the type of architecture I should use to estimate the link duration between two vehicles in a [VANET](https://en.wikipedia.org/wiki/Vehicular_ad-hoc_network). 

&amp;#x200B;

What I'm thinking right now is to have a MLP network that takes certain parameters (vehicle speed, lane, number of lanes, distance to intersection, traffic conditions, etc.) and feeds the output of that into an LSTM. The reason I suggest an LSTM is because the link duration, or lifetime, is constantly changing based on the parameters that I previously mentioned. To my knowledge an LSTM is good for temporal data. However, I have a feeling that my intuition may be off here and I was wondering if somebody would care to chime in.  

&amp;#x200B;

Thanks!",0,0
1726,2019-6-27,2019,6,27,8,c5xzo1,[R] Neural ODEs,https://www.reddit.com/r/MachineLearning/comments/c5xzo1/r_neural_odes/,ai_researcherr,1561592114,"We have recently developed an Adjoint based Neural ODE (ANODE) which computes unconditionally accurate gradients for Neural ODEs. This is very important as the approach presented in arxiv:1806.07366 is numerically unstable and may result in divergent training (in several cases we observed  &gt;20% accuracy degradation because of this)  


Link to Pytorch code:

[https://github.com/amirgholami/anode](https://github.com/amirgholami/anode)

&amp;#x200B;

Link to papers:

[https://arxiv.org/pdf/1902.10298.pdf](https://arxiv.org/pdf/1902.10298.pdf)

[https://arxiv.org/pdf/1906.04596.pdf](https://arxiv.org/pdf/1906.04596.pdf)

&amp;#x200B;

We hope this library would be helpful. Please let us know if you have any feedback and feel free to reach out if there was any questions",58,155
1727,2019-6-27,2019,6,27,8,c5y9ha,First Blockchain Capable of Running AI Programs: Cortex MainNet Arnold Launch,https://www.reddit.com/r/MachineLearning/comments/c5y9ha/first_blockchain_capable_of_running_ai_programs/,CTXCBlockchain,1561593571,,0,1
1728,2019-6-27,2019,6,27,9,c5ybl8,[D] Stylegan encoder mobile,https://www.reddit.com/r/MachineLearning/comments/c5ybl8/d_stylegan_encoder_mobile/,ArtisticActive,1561593929,what would be a good apporach for deploying stylegan encoder [https://github.com/Puzer/stylegan-encoder](https://github.com/Puzer/stylegan-encoder) to mobile devices? Save the model as a pb or tflite?,2,0
1729,2019-6-27,2019,6,27,10,c5z6r0,[R] The Story of Heads: Analyzing Multi-Head Self Attention,https://www.reddit.com/r/MachineLearning/comments/c5z6r0/r_the_story_of_heads_analyzing_multihead_self/,justheuristic,1561599000,[removed],0,2
1730,2019-6-27,2019,6,27,11,c5zon3,HDBScan - what sorts of data sets does it work well with?,https://www.reddit.com/r/MachineLearning/comments/c5zon3/hdbscan_what_sorts_of_data_sets_does_it_work_well/,ArkGuardian,1561601820,[removed],0,1
1731,2019-6-27,2019,6,27,12,c60dx6,...,https://www.reddit.com/r/MachineLearning/comments/c60dx6/_/,allthhatnonsense,1561605936,,0,1
1732,2019-6-27,2019,6,27,13,c60uao,Automatic Image Quality Assessment in Python,https://www.reddit.com/r/MachineLearning/comments/c60uao/automatic_image_quality_assessment_in_python/,sria91,1561608760,,0,1
1733,2019-6-27,2019,6,27,13,c60vag,What is a Hopfield network?,https://www.reddit.com/r/MachineLearning/comments/c60vag/what_is_a_hopfield_network/,suhilogy,1561608936,[removed],0,1
1734,2019-6-27,2019,6,27,14,c61ej6,What column should I drop?,https://www.reddit.com/r/MachineLearning/comments/c61ej6/what_column_should_i_drop/,NotGonnaGetWhoooshed,1561612405,[removed],0,1
1735,2019-6-27,2019,6,27,14,c61fax,Automotive - LHD S.p.A. - Telescopic Forks,https://www.reddit.com/r/MachineLearning/comments/c61fax/automotive_lhd_spa_telescopic_forks/,lhd121,1561612557,,0,1
1736,2019-6-27,2019,6,27,14,c61fwk,The European Comission Plans to Regulate AI. What do you guys think?,https://www.reddit.com/r/MachineLearning/comments/c61fwk/the_european_comission_plans_to_regulate_ai_what/,strikingLoo,1561612663,,0,1
1737,2019-6-27,2019,6,27,14,c61iek,Neural Code Search: ML-based code search using natural language queries,https://www.reddit.com/r/MachineLearning/comments/c61iek/neural_code_search_mlbased_code_search_using/,bil-sabab,1561613129,,0,1
1738,2019-6-27,2019,6,27,14,c61iwe,Telescopic forks for Automotive,https://www.reddit.com/r/MachineLearning/comments/c61iwe/telescopic_forks_for_automotive/,lhd121,1561613225,[removed],0,1
1739,2019-6-27,2019,6,27,14,c61mg4,Best Machine Learning Tools: Experts Top Picks,https://www.reddit.com/r/MachineLearning/comments/c61mg4/best_machine_learning_tools_experts_top_picks/,andrea_manero,1561613915,[removed],0,1
1740,2019-6-27,2019,6,27,15,c61xoi,[D] Dont make this big machine learning mistake: research vs application,https://www.reddit.com/r/MachineLearning/comments/c61xoi/d_dont_make_this_big_machine_learning_mistake/,vadhavaniyafaijan,1561616161,,0,1
1741,2019-6-27,2019,6,27,15,c620u5,"If not us, then who?",https://www.reddit.com/r/MachineLearning/comments/c620u5/if_not_us_then_who/,getengati,1561616800,[removed],0,1
1742,2019-6-27,2019,6,27,15,c627ea,"Pytorch implementation of ""What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?"", NIPS 2017",https://www.reddit.com/r/MachineLearning/comments/c627ea/pytorch_implementation_of_what_uncertainties_do/,imheumi,1561618107,,1,1
1743,2019-6-27,2019,6,27,16,c62b5p,How to implement a random forest algorithm,https://www.reddit.com/r/MachineLearning/comments/c62b5p/how_to_implement_a_random_forest_algorithm/,invadrvranjes,1561618862,,1,1
1744,2019-6-27,2019,6,27,16,c62m2e,How to Create Your Own State-of-the-Art Text Generation System,https://www.reddit.com/r/MachineLearning/comments/c62m2e/how_to_create_your_own_stateoftheart_text/,bil-sabab,1561621106,,0,1
1745,2019-6-27,2019,6,27,16,c62nng,Deep Cropper Notebook: https://colab.research.google.com/drive/19-lKjG_8xhaniXfuANfoOPdhvPtHvkXy,https://www.reddit.com/r/MachineLearning/comments/c62nng/deep_cropper_notebook/,alvisanovari,1561621452,,1,1
1746,2019-6-27,2019,6,27,17,c62szs,Could multi-class classification be applied for handwritten character recognition ?,https://www.reddit.com/r/MachineLearning/comments/c62szs/could_multiclass_classification_be_applied_for/,SuccessfulLeadership,1561622667,"Sorry, I know this question is naive but I searched over the internet and didn't find any answer for it. 

I'm trying to create a handwritten characters recognition with multi-class classification and struggling with high bais. 

to fix high bais  I decreased lambda to 0 (which indeed was the optimal value for lambda). But still, there's a high bais.

another way to fix high bias is to add polynomial features, but it takes forever running with me mapping to a polynomial degree since there are over 10,000 examples and 784 features in the dataset (I'm using EMNIST dataset).

I started to believe that it's almost not possible or very hard to make handwritten character recognition with multi-class classification. is it true if yes why?  does only NN works with this kind of problems?",0,1
1747,2019-6-27,2019,6,27,17,c62wgm,Google beats the records in NLP with their new xlnet,https://www.reddit.com/r/MachineLearning/comments/c62wgm/google_beats_the_records_in_nlp_with_their_new/,mollerhoj,1561623469,,0,1
1748,2019-6-27,2019,6,27,18,c63j5g,[R] Learning Causal State Representations of Partially Observable Environments,https://www.reddit.com/r/MachineLearning/comments/c63j5g/r_learning_causal_state_representations_of/,hardmaru,1561628644,,1,8
1749,2019-6-27,2019,6,27,18,c63jkr,4 Ways in Which Facial Verification Technology is Facilitating the Travel Industry,https://www.reddit.com/r/MachineLearning/comments/c63jkr/4_ways_in_which_facial_verification_technology_is/,darrenvu,1561628742,[removed],0,1
1750,2019-6-27,2019,6,27,20,c647nc,Machine Learning for Email Signature Detection with XGBoost,https://www.reddit.com/r/MachineLearning/comments/c647nc/machine_learning_for_email_signature_detection/,snapADDY,1561633766,,0,1
1751,2019-6-27,2019,6,27,20,c64n8v,[N] Do you have a bio net strategy?,https://www.reddit.com/r/MachineLearning/comments/c64n8v/n_do_you_have_a_bio_net_strategy/,phobrain,1561636757,[removed],0,1
1752,2019-6-27,2019,6,27,21,c64psg,[D] Counterfactual Explanations,https://www.reddit.com/r/MachineLearning/comments/c64psg/d_counterfactual_explanations/,Everdream13,1561637210,"I came across the paper of Wachter, Mittelstadt, et al, who propose Counterfactual Explanations as remedy against the opacity of ML decision-making  (see e.g. https://arxiv.org/abs/1711.00399 or https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3278331).

I am a lawyer myself and cannot assess the technical feasibility of this approach. How would you, Ml experts, evaluate their idea? I am really looking forward to your thoughts and insights. 

(If this was already subject to discussion elsewhere please kindly direct me to the post).",7,8
1753,2019-6-27,2019,6,27,21,c64t71,How to gain work experience in ML with 17,https://www.reddit.com/r/MachineLearning/comments/c64t71/how_to_gain_work_experience_in_ml_with_17/,Klausstaler,1561637808,[removed],0,1
1754,2019-6-27,2019,6,27,21,c64wc8,[D] How to deal with adding new data and new labels to existing models,https://www.reddit.com/r/MachineLearning/comments/c64wc8/d_how_to_deal_with_adding_new_data_and_new_labels/,mlaway,1561638380,"Say you work at a company that identifies humans. You start off with a dataset with pictures of humans and bounding boxes. The company becomes a huge success and you want to develop your product further. Now you also want to identify eyes and ears, so you make a dataset for that. You have really struck gold, the market is going wild for your product, so you decide to add a new category, indicators of whether or not the human is ill.

My question is, how do you deal with this kind of growth in ML products?

For every category you add, you have to add annotations to the dataset. This can be a tremendous amount of work and might not be feasible to backfill the data you already have with the new labels.
I have two suggestions for how to deal with this, you only annotate new data and then train the model in two phases. First phase you train the model on only detecting humans, next phase you add outputs for eyes and ears and fine tune on the rest of the data.

An other way you could do it is to train two separate models, one for humans and one for eyes/ears. Depending on your domain, you might want to have everything in one model if you have real time constraints, so multiple models might not be favorable.

Is there anywhere I can read more about how to deal with these kinds of issues? Do you guys have any experience in dealing with issues like this?",4,6
1755,2019-6-27,2019,6,27,21,c64wl4,Installing Tensorflow V2 using GPU fast and easy,https://www.reddit.com/r/MachineLearning/comments/c64wl4/installing_tensorflow_v2_using_gpu_fast_and_easy/,Telcrome,1561638421,,1,1
1756,2019-6-27,2019,6,27,21,c64zpg,How should one look to convert sentiment analysis in form of a score?,https://www.reddit.com/r/MachineLearning/comments/c64zpg/how_should_one_look_to_convert_sentiment_analysis/,roundof1995,1561638965,[removed],0,1
1757,2019-6-27,2019,6,27,21,c656zt,AI Development Company|ai development services,https://www.reddit.com/r/MachineLearning/comments/c656zt/ai_development_companyai_development_services/,clarke2106,1561640191,[removed],0,1
1758,2019-6-27,2019,6,27,22,c65bdd,3 Effective Ways Machine Learning Improves Your Bottom Line - Small Business Trends,https://www.reddit.com/r/MachineLearning/comments/c65bdd/3_effective_ways_machine_learning_improves_your/,ChrisWatney,1561640894,,0,1
1759,2019-6-27,2019,6,27,22,c65c5v,[R] Learning Explainable Models with Attribution Priors,https://www.reddit.com/r/MachineLearning/comments/c65c5v/r_learning_explainable_models_with_attribution/,gabeerion,1561641031,"Paper: [https://arxiv.org/abs/1906.10670](https://arxiv.org/abs/1906.10670)

Code: [https://github.com/suinleelab/attributionpriors](https://github.com/suinleelab/attributionpriors)

I wanted to share this paper we recently submitted to NeurIPS. TL;DR - the idea is that there has been a lot of recent research on explaining deep learning models by attributing importance to each input feature. We go one step farther and incorporate *attribution priors* \- prior beliefs about what these feature attributions should look like - into the training process. We develop a fast, differentiable new feature attribution method called *expected gradients*, and optimize differentiable functions of these feature attributions to improve performance on a variety of tasks.

Our results include: In image classification, we encourage smoothness of nearby pixel attributions to get more coherent prediction explanations and robustness to noise. In drug response prediction, we encourage similarity of attributions among features that are connected in a protein-protein interaction graph to achieve more accurate predictions whose explanations correlate better with biological pathways. Finally, with health care data, we encourage inequality in the magnitude of feature attributions to build sparser models that perform better when training data is sparse. We hope this framework will be useful to anyone who wants to incorporate prior knowledge about how a deep learning model should behave in a given setting to improve performance.",27,130
1760,2019-6-27,2019,6,27,22,c65hw6,Global Planetary Mixers Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c65hw6/global_planetary_mixers_market_report_2019/,jadhavni3,1561641958,[removed],1,1
1761,2019-6-27,2019,6,27,22,c65ikz,"[R] Pytorch implementation of ""What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?"", NIPS 2017",https://www.reddit.com/r/MachineLearning/comments/c65ikz/r_pytorch_implementation_of_what_uncertainties_do/,imheumi,1561642059,"[Github](https://github.com/Heumi/what)

Pytorch implementation of ""What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?"", NIPS 2017

\- Autoencoder network

\- Check 3 different uncertainty type (Aleatoric, Epistemic, Combined)",0,0
1762,2019-6-27,2019,6,27,22,c65n1c,GAN for fixed object manipulation and arrangement,https://www.reddit.com/r/MachineLearning/comments/c65n1c/gan_for_fixed_object_manipulation_and_arrangement/,miscmate,1561642759,[removed],0,1
1763,2019-6-27,2019,6,27,22,c65nlv,Dask Release 2.0,https://www.reddit.com/r/MachineLearning/comments/c65nlv/dask_release_20/,_quanttrader_,1561642850,,0,1
1764,2019-6-27,2019,6,27,22,c65o9m,[P] Frequency based selection vs TF-IDF score based selection,https://www.reddit.com/r/MachineLearning/comments/c65o9m/p_frequency_based_selection_vs_tfidf_score_based/,aklagoo,1561642950,"I'm working on text segmentation with novels. I plan to use 40,000 words to generate word embeddings. When filtering these words, is it okay if I merely use the most frequent words or should I use something like a TF-IDF score?",11,2
1765,2019-6-27,2019,6,27,22,c65ovq,Global Portable Water Purifiers Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c65ovq/global_portable_water_purifiers_market_report_2019/,jadhavni3,1561643051,[removed],1,1
1766,2019-6-27,2019,6,27,22,c65pxn,Can CNNs be fooled like humans?,https://www.reddit.com/r/MachineLearning/comments/c65pxn/can_cnns_be_fooled_like_humans/,baldhat,1561643209,[removed],0,1
1767,2019-6-27,2019,6,27,22,c65qwh,[P] StarAi: Deep Reinforcement Learning Course,https://www.reddit.com/r/MachineLearning/comments/c65qwh/p_starai_deep_reinforcement_learning_course/,sigmoidp,1561643365,"Way back in 2017 when Deepmind released their PySC2 interface - we thought it would be a fantastic opportunity to create a competition to help accelerate the current state of the art in ML. 

&amp;#x200B;

We thought that such a competition would need a big $ prize pool in order to attract talent to try help solve the ""Starcraft problem"". We tried to copy the model of the original [Xprize](https://en.wikipedia.org/wiki/Ansari_X_Prize) and use insurance bonds to try finance the $ prize purse. [This document,](https://github.com/star-ai/StarAi-LectureSlides/blob/master/StarAi%20Artificial%20Intelligence%20Competition2.pdf) literally bounced around to insurance brokers all around the world- but we got no takers :). Lucky for us - as we all know by now Deepmind more or less solved the Starcraft problem this year.

&amp;#x200B;

One thing we realised, early circa 2018 is that there were no bringing RL down to earth courses out there to help people get involved in the envisioned Starcraft competition. So we went ahead and made it ourselves :)

&amp;#x200B;

I know that other great resources such as OpenAi's spinning up have come out since then, but we would like to present our work and open source it to the community. We hope this content inspires someone out there to do great things!

&amp;#x200B;

[https://www.starai.io/](https://www.starai.io/)",22,51
1768,2019-6-27,2019,6,27,22,c65tms,Global Powder Puffs Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c65tms/global_powder_puffs_market_report_2019/,jadhavni3,1561643786,[removed],1,1
1769,2019-6-27,2019,6,27,23,c65ve9,F,https://www.reddit.com/r/MachineLearning/comments/c65ve9/f/,arjunvvhh,1561644070,,0,1
1770,2019-6-27,2019,6,27,23,c65yc4,Global Robot Arm Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c65yc4/global_robot_arm_market_report_2019/,jadhavni3,1561644487,[removed],1,1
1771,2019-6-27,2019,6,27,23,c6646q,Global Robot Grippers Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c6646q/global_robot_grippers_market_report_2019/,jadhavni3,1561645344,[removed],1,1
1772,2019-6-27,2019,6,27,23,c668ai,Global Anti-glare Screen Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c668ai/global_antiglare_screen_market_report_2019/,jadhavni3,1561645946,[removed],1,1
1773,2019-6-27,2019,6,27,23,c66fyv,[R] Comparing Mobile Machine Learning Frameworks,https://www.reddit.com/r/MachineLearning/comments/c66fyv/r_comparing_mobile_machine_learning_frameworks/,zsajjad,1561647043,,0,2
1774,2019-6-27,2019,6,27,23,c66i9t,What is the best metric to compare tradeoff optimisation methods ?,https://www.reddit.com/r/MachineLearning/comments/c66i9t/what_is_the_best_metric_to_compare_tradeoff/,yachenml,1561647381,[removed],0,1
1775,2019-6-28,2019,6,28,0,c671kj,[D] Has anyone used continuous RL algorithms to output the parameters of a probability distribution that actions are then sampled from,https://www.reddit.com/r/MachineLearning/comments/c671kj/d_has_anyone_used_continuous_rl_algorithms_to/,iamiamwhoami,1561650051,So I'm working on a problem where I need an agent to perform multiple different actions at each time step. A solution I have in mind is to have the agent output the mean and covariance of a gaussian distribution and then sample the actions from the Gaussian distribution. Has anyone seen anything like this? Does this seem like an immediately bad idea?,6,3
1776,2019-6-28,2019,6,28,0,c67558,"Multi-class classification using CNN over PyTorch, and the basics of CNN",https://www.reddit.com/r/MachineLearning/comments/c67558/multiclass_classification_using_cnn_over_pytorch/,thevatsalsaglani,1561650543,,1,1
1777,2019-6-28,2019,6,28,2,c681df,Understanding ROC-AUC Curves,https://www.reddit.com/r/MachineLearning/comments/c681df/understanding_rocauc_curves/,prakhar21,1561654921,[removed],0,1
1778,2019-6-28,2019,6,28,2,c682ga,Geoffrey Hintons Unsupervised Capsule Networks Achieve SOTA Results on SVHN - Medium,https://www.reddit.com/r/MachineLearning/comments/c682ga/geoffrey_hintons_unsupervised_capsule_networks/,Yuqing7,1561655060,,0,1
1779,2019-6-28,2019,6,28,2,c682q7,Decentralized Training of primitives: Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives,https://www.reddit.com/r/MachineLearning/comments/c682q7/decentralized_training_of_primitives/,Descates,1561655094,[https://arxiv.org/abs/1906.10667](https://arxiv.org/abs/1906.10667),0,1
1780,2019-6-28,2019,6,28,2,c68iha,[R] Parameter-Efficient Transfer Learning for NLP,https://www.reddit.com/r/MachineLearning/comments/c68iha/r_parameterefficient_transfer_learning_for_nlp/,blowjobtransistor,1561657246,,4,3
1781,2019-6-28,2019,6,28,2,c68m5a,[D] Has anyone here looked into evaluating multiple models on a single GPU in parallel?,https://www.reddit.com/r/MachineLearning/comments/c68m5a/d_has_anyone_here_looked_into_evaluating_multiple/,bimtuckboo,1561657739,"I'm interested in running neuroevolution algorithms on a single GPU. The idea is to combine the forward pass for many models into a single matrix multiplication per layer. To be clear, I'm not talking about weight sharing between models. All models would have their own exclusive set of weights and in many cases, different inputs as well.

Uber actually made a blog post about doing exactly this [here](https://eng.uber.com/accelerated-neuroevolution/). But they don't explain much about how they approached implementing this functionality and their provided source code quite difficult to understand (probably in large part because I'm much more experienced with pytorch than tf).

Has anyone here tried to implement anything like this or know of any other relevant projects? Or perhaps someone who understands how Uber achieved this could conceptually step through the matrix math involved in composing these conjoined networks?",19,2
1782,2019-6-28,2019,6,28,3,c68ryu,Does it make sense to enroll for ACM/ AAAI membership?,https://www.reddit.com/r/MachineLearning/comments/c68ryu/does_it_make_sense_to_enroll_for_acm_aaai/,RoboticExpression,1561658549,[removed],0,1
1783,2019-6-28,2019,6,28,3,c690q0,[D] Thoughts on combining the hidden layer of a VAE with other data sources?,https://www.reddit.com/r/MachineLearning/comments/c690q0/d_thoughts_on_combining_the_hidden_layer_of_a_vae/,oppai_suika,1561659744,"Hi all,

I'm working on a project where I've trained an autoencoder on a specific dataset (movie ratings). The hidden layer is a 256-length vector. I have another dataset of loosely related data (user demographics) that I'd like to combine with my encoding somehow to improve my model, though I'm not sure exactly how to implement this.

Do you guys know of any literature that attempts something similar? Any suggestions/criticisms would also be much appreciated.

Thanks",3,1
1784,2019-6-28,2019,6,28,3,c69bv1,[Discussion] Thinking of Opening a Small Bootcamp to Help Small Farmers via Machine Learning,https://www.reddit.com/r/MachineLearning/comments/c69bv1/discussion_thinking_of_opening_a_small_bootcamp/,botanicalai,1561661254,"Guys, I've been brewing on the bootcamp idea for a while. The reason I am posting here is to collect your input. I think that I will open the bootcamp even if you tell me that it is a terrible idea, so do not bother shooting the messenger right away.  :P On the other hand, constructive criticism is welcome. Main purpose of sharing the idea with you is for the purpose of collective thinking. Imagine that you are considering joining such a bootcamp, what will be enticing to you?

Now, without much further due:

1. Average age of a modern-day farmer is 55 (I will insert my source later :).

2. Modern farming (especially so in the US) is a large-scale chemical facility, which blasts our food and soil with known carcinogens .

3. Small farmers find it difficult to compete. However, there has been somewhat of a resurgence of local CSA (community-supported agriculture) because young people are more aware of the food we eat and the way we treat the environment.

4. Data Science, IoT, Machine Learning are sexy topics with many cerebral young professionals actively participating in developing many fields (fintech, autonomous driving, etc.).

5. Let's make agriculture sexy again (Now, Reddit, behave :) by combining Machine Learning, IoT, and Data Science and local CSAs. 

6. Imagine a bootcamp, which works on a template meant to ease the pain of growing local foods without the use of pesticides and herbicides. Real-time actionable data displaces the need to rely on chemicals.

7. Or imagine a bootcamp, which studies beehives from the inside by collecting continuous data and using machine learning techniques (DeepLabCut, for example) to identify beehive problems during early stages. 

&amp;#x200B;

Now, returning to your opinions, what would you like to see in such a bootcamp? I have a PhD in Healthcare Information Systems, and I will quit my full-time job at a research university, so I can be part of it every day. I would not want for it to be a money-making mill. I love solving complex problems with incremental improvements and collective efforts. What technologies would you like to learn how to use? What kind of equipment would you think be beneficial? What kind of people would draw you to such a bootcamp? Let me know anything that comes to mind. I will be watching and responding as much as I can (I am still working, and I will be traveling this weekend).",14,1
1785,2019-6-28,2019,6,28,4,c69iei,"Are there any implementations of machine learning in weather forecasting? If so, how are they used?",https://www.reddit.com/r/MachineLearning/comments/c69iei/are_there_any_implementations_of_machine_learning/,charmangel_,1561662148,[removed],0,1
1786,2019-6-28,2019,6,28,4,c69mgk,Project viability. Can you make a reading aloud score for kids?,https://www.reddit.com/r/MachineLearning/comments/c69mgk/project_viability_can_you_make_a_reading_aloud/,arturdaraujo,1561662673,[removed],0,1
1787,2019-6-28,2019,6,28,4,c69qmi,[D] Undress any girl ! DEEPNUDE DeepFake,https://www.reddit.com/r/MachineLearning/comments/c69qmi/d_undress_any_girl_deepnude_deepfake/,cmillionaire9,1561663227,[removed],0,1
1788,2019-6-28,2019,6,28,4,c69tei,[D] Undress any girl ! DEEPNUDEDeepFake,https://www.reddit.com/r/MachineLearning/comments/c69tei/d_undress_any_girl_deepnudedeepfake/,cmillionaire9,1561663605," https://youtu.be/xCwsRU0dv9I  
Deepnude is an application that undresses the person in the photo. The programmer has created an algorithm that ""removes"" the clothing with images of women. It uses the image of a person and creates a new one, but already without clothes. Deepnude now only supports photos of women.  Although the developer and plans to continue to teach the program ""expose"" of the male body. deepnude processes photos best in bathing suits or in fairly open clothes. So the neural network gets a more natural image.",322,0
1789,2019-6-28,2019,6,28,4,c69u7x,The Staggering Cost of Training SOTA AI Models,https://www.reddit.com/r/MachineLearning/comments/c69u7x/the_staggering_cost_of_training_sota_ai_models/,Yuqing7,1561663723,,0,1
1790,2019-6-28,2019,6,28,5,c6a9x4,Predicting Bus Delays with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/c6a9x4/predicting_bus_delays_with_machine_learning/,sjoerdapp,1561665838,,0,1
1791,2019-6-28,2019,6,28,5,c6acxy,[P] Fast cloth color identification from customer images in Go,https://www.reddit.com/r/MachineLearning/comments/c6acxy/p_fast_cloth_color_identification_from_customer/,Uriopass,1561666243,"Hi everyone,  

I'm currently working at Leboncoin, which is a French company where anyone can sell goods, a bit like Craigslist.  

I was given the task of labelling a lot of cloth items by their color, and I wrote an article on my approach and the final model.

https://medium.com/leboncoin-engineering-blog/fast-cloth-color-identification-from-customer-images-in-pure-go-bec3cc97851e

I'd be happy to hear any criticism or what you would have done in the same situation.",10,2
1792,2019-6-28,2019,6,28,5,c6afy2,Roadmap -&gt; How to Learn Machine Learning Quickly,https://www.reddit.com/r/MachineLearning/comments/c6afy2/roadmap_how_to_learn_machine_learning_quickly/,gauravlogical,1561666657,,0,1
1793,2019-6-28,2019,6,28,5,c6atra,Does anyone know of a PyTorch implementation for Loss-aware-weight-quantization?,https://www.reddit.com/r/MachineLearning/comments/c6atra/does_anyone_know_of_a_pytorch_implementation_for/,pikachuchameleon,1561668545,[removed],0,1
1794,2019-6-28,2019,6,28,6,c6b1kl,Using pre-trained weights with Keras,https://www.reddit.com/r/MachineLearning/comments/c6b1kl/using_pretrained_weights_with_keras/,thatphotoguy89,1561669570,[removed],0,1
1795,2019-6-28,2019,6,28,6,c6b6zi,A Tangent Distance Preserving Dimensionality Reduction Algorithm,https://www.reddit.com/r/MachineLearning/comments/c6b6zi/a_tangent_distance_preserving_dimensionality/,edisonzhao,1561670286,[removed],0,1
1796,2019-6-28,2019,6,28,6,c6bdfn,Deep Cropper: Using Image Segmentation to Photoshop Images,https://www.reddit.com/r/MachineLearning/comments/c6bdfn/deep_cropper_using_image_segmentation_to/,alvisanovari,1561671176,[removed],0,1
1797,2019-6-28,2019,6,28,6,c6bggb,Understanding model transferability [D],https://www.reddit.com/r/MachineLearning/comments/c6bggb/understanding_model_transferability_d/,titanandwire,1561671611,"I'm trying to reproduce and use the deep-clustering method introduced by [Guo et. al](https://xifengguo.github.io/papers/ICONIP17-DCEC.pdf) (authors [implementation](https://github.com/XifengGuo/DCEC) ) on data from a physics experiment. Reproducing the authors results on MNIST was thankfully trivial as they provide an implementation. The transfer of application has so far, not worked at all, as we seem to be unable to attain a Normalized Mutual Information (nmi) score of over 0.3 on our data, and the adjusted rand scores match that level of poor performance.

&amp;#x200B;

My question is: why would I expect that the model cannot fit the data? Or how should I scale the model s.t. it retains the same properties as it had for the MNIST dataset (I have several thoughts on this but I don't want to bias your thinking)? 

&amp;#x200B;

&amp;#x200B;

Information about the data:

|Number of images|46283 (can get more)|
|:-|:-|
|Number of classes|3|
|Class balance|2:1:2 |
|Image dimensions|80x80x1 (or 128x128x1)|

&amp;#x200B;

I've attached an image of each of the classes. They are visually very distinct, at least class 0 from the 1 and 2. I've not been able to 

[Example of class 0](https://i.redd.it/bsfpakgnuy631.png)

&amp;#x200B;

[example of class 1](https://i.redd.it/y8hwv5kzuy631.png)

[Example of class 2](https://i.redd.it/nw3fv4xwuy631.png)",0,2
1798,2019-6-28,2019,6,28,6,c6blne,BytePS: A high performance and general PS framework for distributed training,https://www.reddit.com/r/MachineLearning/comments/c6blne/byteps_a_high_performance_and_general_ps/,changlan,1561672340,[removed],0,1
1799,2019-6-28,2019,6,28,8,c6csm1,Frames classification,https://www.reddit.com/r/MachineLearning/comments/c6csm1/frames_classification/,amiths89,1561678807,"What is the appropriate algorithm and method to classify a sentence for which the classification requires a contextual sense of the sentence? The number of classes is 1000+ classes.
The classification is to identify frames from the sentences. For example the sentence ""The car engine roared again and the red car moved off but it didn't go far"" has to be classified as a 'Motion' frame.",0,1
1800,2019-6-28,2019,6,28,9,c6d7mu,[D] What do you think of this workflow?,https://www.reddit.com/r/MachineLearning/comments/c6d7mu/d_what_do_you_think_of_this_workflow/,colobas,1561681191,"I described it this Twitter thread: [https://twitter.com/colobas\_/status/1144395631588306945](https://twitter.com/colobas_/status/1144395631588306945)

Transcribing it here:

\#Jupyter &amp; #Python people of Twitter, let me know what you think of this:

* Using [Jupytext](https://github.com/mwouts/jupytext) to have my notebooks in percent format, and still be able to work on them as normal notebooks.
* In the code, I have defined classes and functions I might want to import somewhere else
* I'll also have stuff you'd have in a normal notebook: experiments, markdown, etc
* At the beginning of the notebook I have the following: ![snippet](https://pbs.twimg.com/media/D-G0VhTVAAECnEh.png)
* For every block I want to run in ""Jupyter mode"", I precede it with `if ipython is not None:`
* Every block that isn't preceded by that is considered to be in ""script/module mode"".
* I can use this to have importable stuff, in the same place I have tests and experiments that explain and test its behaviour
* Bonus: I get to use my favourite IDE to write the bulk of my notebook

Share your thoughts!",6,0
1801,2019-6-28,2019,6,28,9,c6dhgu,[P] GPT-2 full version Grover released,https://www.reddit.com/r/MachineLearning/comments/c6dhgu/p_gpt2_full_version_grover_released/,cryptonewsguy,1561682795,,0,1
1802,2019-6-28,2019,6,28,9,c6dkx4,[D] How are some people publishing 5+ first author papers a year at top conferences??,https://www.reddit.com/r/MachineLearning/comments/c6dkx4/d_how_are_some_people_publishing_5_first_author/,pomclm11,1561683367,"Yi Tay: [https://scholar.google.com.sg/citations?hl=en&amp;user=VBclY\_cAAAAJ&amp;view\_op=list\_works&amp;sortby=pubdate](https://scholar.google.com.sg/citations?hl=en&amp;user=VBclY_cAAAAJ&amp;view_op=list_works&amp;sortby=pubdate))

Ryan Cotterell: [https://scholar.google.de/citations?hl=en&amp;user=DexOqtoAAAAJ&amp;view\_op=list\_works&amp;sortby=pubdate](https://scholar.google.de/citations?hl=en&amp;user=DexOqtoAAAAJ&amp;view_op=list_works&amp;sortby=pubdate)

&amp;#x200B;

For example Yi Tay has \~15 first-author publications in 2018 at top venues (AAAI, NeurIPS, etc.). How is this even possible??",37,148
1803,2019-6-28,2019,6,28,10,c6doyc,Revealing Backdoor Attacks in CNNs - New Paper,https://www.reddit.com/r/MachineLearning/comments/c6doyc/revealing_backdoor_attacks_in_cnns_new_paper/,ani0075saha,1561684023,[removed],0,1
1804,2019-6-28,2019,6,28,12,c6f48v,Art of the Problem's new ML video unifies in a single theoretical framework organic and inorganic intelligence forms.,https://www.reddit.com/r/MachineLearning/comments/c6f48v/art_of_the_problems_new_ml_video_unifies_in_a/,britcruise,1561692559,,0,1
1805,2019-6-28,2019,6,28,13,c6fpha,#Ai in retail,https://www.reddit.com/r/MachineLearning/comments/c6fpha/ai_in_retail/,eulerslab,1561696121,,0,1
1806,2019-6-28,2019,6,28,13,c6fvst,"getting horribly large coefficients on Kaggle, but not on local machine",https://www.reddit.com/r/MachineLearning/comments/c6fvst/getting_horribly_large_coefficients_on_kaggle_but/,KishanJoshi98,1561697257,[removed],0,1
1807,2019-6-28,2019,6,28,13,c6fxo4,[R] Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics,https://www.reddit.com/r/MachineLearning/comments/c6fxo4/r_reverse_engineering_recurrent_networks_for/,hardmaru,1561697624,,4,27
1808,2019-6-28,2019,6,28,14,c6geqs,[Discussion] How can one get hands on experience with optimization problems (as opposed to supervised learning problems)?,https://www.reddit.com/r/MachineLearning/comments/c6geqs/discussion_how_can_one_get_hands_on_experience/,AlexSnakeKing,1561700798,"I would like to learn more about optimization. Right now I get to do some regression and various classification type stuff, but no real world experience with optimization (other than whatever optimizers are being run to fit the supervised learning methods I use). 

I know the basics of the theory and algorithms behind it (LP, QP, IP, Genetic Algorithms, etc...), and I've packaged optimization ERP tools, but I don't see how I can extend that knowledge to real world data sets and hands on use cases where I solve new problems. 

For supervised learning, there is Kaggle, and hundreds of other open data sets which you can practice on, but for optimization I can't find any similar competitions or data sets. Moreover the popular tools (Gurobi, Cplex, etc...) seem to be more proprietary and lack the community resources that ML and Stats open source tools have (i.e open source isn't as much of a thing for optimization as it is for ML) 

Also: Even if one had access to the right data and the right tools, how does one validate the quality of their Optimization solution? With supervised problems you have the ground truth to compare against. With clustering you have information theoretic and visual methods to examine your data. 

But with optimization and search problems, how do you evaluate your solution in a real world use case? You can try fake data such that the global optimum is known before hand, but those will always be toy examples. For real world data sets, you don't know what the global optima are, by definition, otherwise you wouldn't have to use optimization algorithms and search heuristics to solve the problem in the first place? 

&amp;#x200B;

Any advice and resources on how to get hands experience with optimization and OR problems in general?",5,2
1809,2019-6-28,2019,6,28,15,c6gmd2,[P] Gen: a general-purpose probabilistic programming system with programmable inference,https://www.reddit.com/r/MachineLearning/comments/c6gmd2/p_gen_a_generalpurpose_probabilistic_programming/,wei_jok,1561702302,"**Abstract** Although probabilistic programming is widely used for some restricted classes of statistical models, existing systems lack the flexibility and efficiency needed for practical use with more challenging models arising in fields like computer vision and robotics. This paper introduces Gen, a general-purpose probabilistic programming system that achieves modeling flexibility and inference efficiency via several novel language constructs: (i) the generative function interface for encapsulating probabilistic models; (ii) interoperable modeling languages that strike different flexibility/efficiency trade-offs; (iii) combinators that exploit common patterns of conditional independence; and (iv) an inference library that empowers users to implement efficient inference algorithms at a high level of abstraction. We show that Gen outperforms state-of-the-art probabilistic programming systems, sometimes by multiple orders of magnitude, on diverse problems including object tracking, estimating 3D body pose from a depth image, and inferring the structure of a time series.

*Project Page* https://probcomp.github.io/Gen/

*Paper* https://dl.acm.org/citation.cfm?id=3314221.3314642

*Code* https://github.com/probcomp/Gen

Article on [MIT news](http://news.mit.edu/2019/ai-programming-gen-0626) about this work. The article is a bit too hyped, but just including here for completeness, as the work looks solid on its own without this article.",9,19
1810,2019-6-28,2019,6,28,15,c6gs1q,[D] Alan Turing's Intelligent Machinery (1948),https://www.reddit.com/r/MachineLearning/comments/c6gs1q/d_alan_turings_intelligent_machinery_1948/,milaworld,1561703442,"Turing wrote a paper titled [Intelligent Machinery](https://weightagnostic.github.io/papers/turing1948.pdf) in 1948. This is a highly original work, introducing ideas such as genetic algorithms and neural networks (what he called unorganized machines) with learning capabilities, and reinforcement learning. I believe [Intelligent Machinery](https://weightagnostic.github.io/papers/turing1948.pdf) is the most detailed treatment of A.I. written before 1950. It was not published during Turings lifetime [1](https://en.wikipedia.org/wiki/Unorganized_machine). Rather than giving a detailed summary, I will just quote Turings own abstract:

**Abstract** The possible ways in which machinery might be made to show intelligent behaviour are discussed. The analogy with the human brain is used as a guiding principle. It is pointed out that the potentialities of the human intelligence can only be realised if suitable education is provided. The investigation mainly centres round an analogous teaching process applied to machines. The idea of an unorganised machine is defined, and it is suggested that the infant human cortex is of this nature. Simple examples of such machines are given, and their education by means of rewards and punishments is discussed. In one case the education process is carried through until the organisation is similar to that of an [ACE](https://en.wikipedia.org/wiki/Automatic_Computing_Engine).

Link to the paper: https://weightagnostic.github.io/papers/turing1948.pdf

h/t [hackernews](https://news.ycombinator.com/item?id=20220944)",58,327
1811,2019-6-28,2019,6,28,15,c6gvk6,Single Depth Telescopic Forks - LHD S.p.A.,https://www.reddit.com/r/MachineLearning/comments/c6gvk6/single_depth_telescopic_forks_lhd_spa/,lhd6703,1561704156,,0,1
1812,2019-6-28,2019,6,28,15,c6gylj,An Introduction to Statistical Learning (ISLR),https://www.reddit.com/r/MachineLearning/comments/c6gylj/an_introduction_to_statistical_learning_islr/,impratiksingh,1561704759,[removed],3,1
1813,2019-6-28,2019,6,28,16,c6h7cq,[Q] I am making a custom image classifier using Transfer Learning on Inception V3. I have 3 classes of images with ~6K images each. The input dimension of the network is 500X500 and the output of the network is 14X14x2048. I used global average pooling and finally got a vector of size 2048.,https://www.reddit.com/r/MachineLearning/comments/c6h7cq/q_i_am_making_a_custom_image_classifier_using/,krshna53,1561706495,[removed],0,1
1814,2019-6-28,2019,6,28,17,c6hkl9,[Q] I am making a custom image classifier using Transfer Learning on Inception V3. I have 3 classes of images with ~6K images each. The input dimension of the network is 500X500 and the output of the network is 14X14x2048. I used global average pooling and finally got a vector of size 2048.,https://www.reddit.com/r/MachineLearning/comments/c6hkl9/q_i_am_making_a_custom_image_classifier_using/,krshna53,1561709473,[removed],0,1
1815,2019-6-28,2019,6,28,17,c6hm9w,Machine Learning papers to implement,https://www.reddit.com/r/MachineLearning/comments/c6hm9w/machine_learning_papers_to_implement/,lollocat3,1561709868,Does anyone know of an innovative and interesting machine learning paper which I could go about implementing?,0,1
1816,2019-6-28,2019,6,28,17,c6hnl5,[D] A good Speech Recognition package ?,https://www.reddit.com/r/MachineLearning/comments/c6hnl5/d_a_good_speech_recognition_package/,lazywiing,1561710186,"Hi Reddit,

&amp;#x200B;

I am to work on a **Speech Recognition** project for the next few weeks/months or so. I don't have any prior knowledge on the subject, but I roughly guess a basic architecture should not be far from an encoder - decoder architecture. I have to gain insights on the field and put a model in production by the end of the year. 

&amp;#x200B;

For now, I just want to be able to transcript audio data into text. I have first to understand the basics of audio data. I guess I will have to read some papers about Fourier transforms, spectrograms, denoising, filtering and so on.

&amp;#x200B;

I have a few questions for you though.

&amp;#x200B;

\- First, do you have good **resources** (MOOC, courses, ...) to learn Speech Recognition ? I tried to look for some, and I found a Stanford course ([http://web.stanford.edu/class/cs224s/syllabus.html](http://web.stanford.edu/class/cs224s/syllabus.html)) from 2017. Given the syllabus, would you say it is a good resource to learn from ?

&amp;#x200B;

\- Then, is it worth it to implement my own **model from scratch**, or should I use a **pre-existing library** ? The audio data I want to train my model on are very task-dependent, and I don't know if a pre-trained model would be good enough to recognize specific terms. On the other hand, I won't have as much data or computational power as Google to train my own model. Given these elements, what library would you recommend ? I think the ideal solution would be to use a pre-trained model and fine-tune it on my data. Of course, any relevant resources would be much appreciated :)

&amp;#x200B;

\- Overall, what **strategy** would you recommend me to follow ? I don't know where to look and where to start.

&amp;#x200B;

Thank you so much !",23,3
1817,2019-6-28,2019,6,28,17,c6hp1i,Serverless Machine Learning model inference using GPU (preferably on AWS),https://www.reddit.com/r/MachineLearning/comments/c6hp1i/serverless_machine_learning_model_inference_using/,freshprinceofuk,1561710551,[removed],0,1
1818,2019-6-28,2019,6,28,17,c6hsjo,friday freestyle tutorials the butterfly effect,https://www.reddit.com/r/MachineLearning/comments/c6hsjo/friday_freestyle_tutorials_the_butterfly_effect/,thetrickshotone,1561711424,,0,1
1819,2019-6-28,2019,6,28,17,c6hsoe,What the theory behind the inference process of **the dual form* of **perceptron algorithm**?,https://www.reddit.com/r/MachineLearning/comments/c6hsoe/what_the_theory_behind_the_inference_process_of/,whishtLF,1561711460,[removed],0,1
1820,2019-6-28,2019,6,28,18,c6i0fw,GANs output,https://www.reddit.com/r/MachineLearning/comments/c6i0fw/gans_output/,norbaf78,1561713290,"Dear all,

I have a curiosity about GANs and what we can see around on article about this argument. In lots of article at example faces image, you are not able to understand if the image is real or is a fake generated by a GAN. My question is, this condition happen always or the fact the output face is impossible to distinguish if true or false to a real face happen only sometimes ? Hope to have been clear in what I am intend and ask.

Thanks,

Fabio",0,1
1821,2019-6-28,2019,6,28,18,c6i31r,[D] - GANs output,https://www.reddit.com/r/MachineLearning/comments/c6i31r/d_gans_output/,norbaf78,1561713894,"Dear all,

I have a curiosity about GANs and what we can see around on article about this argument. In lots of article at example faces image, you are not able to understand if the image is real or is a fake generated by a GAN. My question is, this condition happen always or the fact the output face is impossible to distinguish if true or false to a real face happen only sometimes ? Hope to have been clear in what I am intend and ask.

Thanks,

Fabio",2,0
1822,2019-6-28,2019,6,28,18,c6i5z8,"[D] In your opinion what are the best books, videos or any other resources that teach/discuss applying machine learning to EEG data?",https://www.reddit.com/r/MachineLearning/comments/c6i5z8/d_in_your_opinion_what_are_the_best_books_videos/,the_night_sun,1561714562,,1,2
1823,2019-6-28,2019,6,28,18,c6i8qy,Excellent Visual Introduction to Numpy!,https://www.reddit.com/r/MachineLearning/comments/c6i8qy/excellent_visual_introduction_to_numpy/,suryaavala,1561715184,,0,1
1824,2019-6-28,2019,6,28,19,c6ided,The Future of OEM Collaboration Defined | Episode 01 - Roborace partners with VW Data:Lab Munich and Italdesign to push the limits of machine learning through research and development.,https://www.reddit.com/r/MachineLearning/comments/c6ided/the_future_of_oem_collaboration_defined_episode/,AIthatDrives,1561716185,,0,1
1825,2019-6-28,2019,6,28,19,c6ilad,"Best group telegram about Kaggle (machine learning , ....)",https://www.reddit.com/r/MachineLearning/comments/c6ilad/best_group_telegram_about_kaggle_machine_learning/,Doctor_who1,1561717875,"Best group telegram  about  Kaggle   (machine learning , ....)

&amp;#x200B;

[https://t.me/Kaggle\_En](https://t.me/Kaggle_En)",0,1
1826,2019-6-28,2019,6,28,19,c6io9y,"[D] Implementation of ""Stand-Alone Self-Attention in Vision Models""",https://www.reddit.com/r/MachineLearning/comments/c6io9y/d_implementation_of_standalone_selfattention_in/,MerHS,1561718521,"Im implementing [https://arxiv.org/pdf/1906.05909.pdf](https://arxiv.org/pdf/1906.05909.pdf) in this repo ([https://github.com/MerHS/SASA-pytorch](https://github.com/MerHS/SASA-pytorch)), but current implementation consumes too much GPU memory. (currently x0.5 less params than ResNet-50, x10 more mem consumption)

I think some sort of \`matmul\` or \`view\` are causing this problem, hence I'm working on changing matmul to einsum. (also I am not sure that I implemented it correctly)

Could anyone guess how the authors optimized this network?",4,1
1827,2019-6-28,2019,6,28,19,c6ioz3,[D] Any possible ways to convert pytorch model to tflite or onnx to tflite?,https://www.reddit.com/r/MachineLearning/comments/c6ioz3/d_any_possible_ways_to_convert_pytorch_model_to/,cruigo93,1561718680,"Currently, I found a few solutions to convert models to onnx and keras to tflite, however, nothing is related to pytoch to tflite.",1,3
1828,2019-6-28,2019,6,28,19,c6it2k,Deep Learning in Multiple Multistep Time Series Prediction,https://www.reddit.com/r/MachineLearning/comments/c6it2k/deep_learning_in_multiple_multistep_time_series/,jerriclynsjohn,1561719557,,1,1
1829,2019-6-28,2019,6,28,20,c6ivsc,Algonory - Episode 1 - The Faraway Chair by Algo Blyton (1968) - text by GPT-2,https://www.reddit.com/r/MachineLearning/comments/c6ivsc/algonory_episode_1_the_faraway_chair_by_algo/,erocdrahs,1561720087,[removed],0,1
1830,2019-6-28,2019,6,28,20,c6j0uk,Cluster computer designed for machine learning,https://www.reddit.com/r/MachineLearning/comments/c6j0uk/cluster_computer_designed_for_machine_learning/,nexely,1561721075,,1,2
1831,2019-6-28,2019,6,28,20,c6j74k,[R] Predict the ideal price of a hotel room using Deep Learning.,https://www.reddit.com/r/MachineLearning/comments/c6j74k/r_predict_the_ideal_price_of_a_hotel_room_using/,jerriclynsjohn,1561722294,"Dataset: [Hotel Booking Demand Dataset](https://www.sciencedirect.com/science/article/pii/S2352340918315191)

The primary commodity (the hotel room) is perishable product within 24hrs and for everyday in the calendar it has a very different lead-time with its corresponding room rates.

Eg: For Room A on 25th August 2019 every hour before the booking time on 25th Aug is a lead time, which can have different pricing. (Const pricing vs Dynamic pricing) 

&amp;#x200B;

Also the pricing of the room is dependent on the number of rooms left on the booking date, pricing of competitors in the area, events in the area, seasonality, inbound flight and train patterns of the region, meteorological information of the region etc.

&amp;#x200B;

I want to know if there are people who have already worked on a problem solution, similar to this in the hotel industry? If yes what are the techniques of Data framing that you used and the models that you have tried this on?",12,5
1832,2019-6-28,2019,6,28,20,c6j9h0,Detect human behaviour with a sensor matrix,https://www.reddit.com/r/MachineLearning/comments/c6j9h0/detect_human_behaviour_with_a_sensor_matrix/,tortymo,1561722732,"  

Hello there,

Since I have no previous experience in machine learning (only in software development), I was wondering if the following approach could be feasible at all or if I have to dig much deeper:

I am currently developing a software interface (Python, running on a PC) to access sensor data that is being collected by a microcontroller and sent to the PC via USB.

The sensors are arranged in a 64x64 matrix and are placed under a piece of foil (calling the combination of it ""sensor foil""). They measure pressure created by human physical activity (e.g. a hand touching the sensor foil). 

What I want to achieve is to detect different body parts or objects placed on the foil. For example left/right hand placed flat on it, a glass being placed on it or an elbow. 

&amp;#x200B;

Is there an approach so high level that allows me to:

\- Create a fixed set of events to be detected by the system: Left/right hand, elbow and  glass placed on the sensor foil

\- Feed the live raw sensor data to a machine learning system running on the PC

\- Train the system with the events described above. This means performing the physical action like putting my left hand down and tell the system which of the previously defined events I just made occur

&amp;#x200B;

After training is finished provide the system with the live sensor data, execute one of the defined physical actions and have the system recognize the according event along with a confidence level provided

&amp;#x200B;

Is that something one of the available machine learning systems can do, considering this is a proof-of-concept project with one man behind it?

&amp;#x200B;

My apologies, if this request is being formulated too broad. I will gladly take in any recommendations on reading up on the matter.

&amp;#x200B;

Thank you!",0,1
1833,2019-6-28,2019,6,28,21,c6jcy8,[P] Detect human behaviour with sensor matrix,https://www.reddit.com/r/MachineLearning/comments/c6jcy8/p_detect_human_behaviour_with_sensor_matrix/,tortymo,1561723354," 

Hello there,

Since  I have no previous experience in machine learning (only in software  development), I was wondering if the following approach could be  feasible at all or if I have to dig much deeper:

&amp;#x200B;

I  am currently developing a software interface (Python, running on a PC)  to access sensor data that is being collected by a microcontroller and  sent to the PC via USB.

&amp;#x200B;

The sensors are  arranged in a 64x64 matrix and are placed under a piece of foil  (calling the combination of it ""sensor foil""). They measure pressure  created by human physical activity (e.g. a hand touching the sensor  foil).

&amp;#x200B;

What I want to achieve is to  detect different body parts or objects placed on the foil. For example  left/right hand placed flat on it, a glass being placed on it or an  elbow.

&amp;#x200B;

Is there an approach so high level that allows me to:

\- Create a fixed set of events to be detected by the system: Left/right hand, elbow and  glass placed on the sensor foil

\- Feed the live raw sensor data to a machine learning system running on the PC

\-  Train the system with the events described above. This means performing  the physical action like putting my left hand down and tell the system  which of the previously defined events I just made occur

&amp;#x200B;

After  training is finished provide the system with the live sensor data,  execute one of the defined physical actions and have the system  recognize the according event along with a confidence level provided

&amp;#x200B;

Is  that something one of the available machine learning systems can do,  considering this is a proof-of-concept project with one man behind it?

&amp;#x200B;

My  apologies, if this request is being formulated too broad. I will gladly  take in any recommendations on reading up on the matter.

Thank you!",6,2
1834,2019-6-28,2019,6,28,21,c6jv02,DAO.Casino Blockchain Performance Benchmark is hot on the heels after the TestNet release!,https://www.reddit.com/r/MachineLearning/comments/c6jv02/daocasino_blockchain_performance_benchmark_is_hot/,iGamblingman,1561726527,,0,1
1835,2019-6-28,2019,6,28,21,c6jv4h,"Rob Holmes, Streamr's Key Account Leader, discusses the recent TIoTA pilot, and why a more collaborative environment for data exchange is needed to power machine learning for technical advancements like autonomous vehicles at CognitionX.",https://www.reddit.com/r/MachineLearning/comments/c6jv4h/rob_holmes_streamrs_key_account_leader_discusses/,thamilton5,1561726543,,0,1
1836,2019-6-28,2019,6,28,22,c6jxog,"[P] AI Benchmark: A New Standard for ML Performance Assessment of CPUs, GPUs and TPUs",https://www.reddit.com/r/MachineLearning/comments/c6jxog/p_ai_benchmark_a_new_standard_for_ml_performance/,aiff22,1561726973,"AI Benchmark is an open source python library for evaluating AI performance of various hardware platforms, including CPUs, GPUs and TPUs. The benchmark is relying on TensorFlow machine learning library, and is providing a lightweight solution for assessing inference and training speed for key Deep Learning models, including:

&amp;#x200B;

1. MobileNet-V2
2. Inception-V3
3. Inception-V4
4. Inception-ResNet-V2
5. ResNet-V2-50
6. ResNet-V2-152
7. VGG-16
8. SRCNN 9-5-5
9. VGG-19
10. ResNet-SRGAN
11. ResNet-DPED
12. U-Net
13. Nvidia-SPADE
14. ICNet
15. PSPNet
16. DeepLab
17. Pixel-RNN
18. LSTM
19. GNMT

&amp;#x200B;

It is currently distributed as a Python pip package, installation instructions can be found at [http://ai-benchmark.com/alpha.html](http://ai-benchmark.com/alpha.html) and [https://pypi.org/project/ai-benchmark/](https://pypi.org/project/ai-benchmark/)

&amp;#x200B;

**Note:** Fast installation \[*if TensorFlow is already installed*\]:

1. `pip install ai-benchmark`
2. Run benchmark using the following python code:
   1. `from ai_benchmark import AIBenchmark`
   2. `results = AIBenchmark().run()`

&amp;#x200B;

A detailed information about test setups: [http://ai-benchmark.com/ranking\_cpus\_and\_gpus\_detailed.html](http://ai-benchmark.com/ranking_cpus_and_gpus_detailed.html)

A short preliminary ranking is available here: [http://ai-benchmark.com/ranking\_cpus\_and\_gpus.html](http://ai-benchmark.com/ranking_cpus_and_gpus.html)

&amp;#x200B;

A global ranking with the results of various hardware and software platforms, drivers / configs and TF builds should be available soon. Original post: [http://ai-benchmark.com/alpha.html](http://ai-benchmark.com/alpha.html)",6,11
1837,2019-6-28,2019,6,28,22,c6kh5s,Cross-Channel Marketing Spend Optimization using Deep Learning,https://www.reddit.com/r/MachineLearning/comments/c6kh5s/crosschannel_marketing_spend_optimization_using/,ikatsov,1561730073,,0,1
1838,2019-6-28,2019,6,28,23,c6kklc,Podcast interview with Jeremy Howard on how to study deep learning with fast.ai,https://www.reddit.com/r/MachineLearning/comments/c6kklc/podcast_interview_with_jeremy_howard_on_how_to/,wanderingtraveller,1561730609,,0,1
1839,2019-6-28,2019,6,28,23,c6koo7,[P] finished implementation XLNet with Pytorch!,https://www.reddit.com/r/MachineLearning/comments/c6koo7/p_finished_implementation_xlnet_with_pytorch/,nlkey2022,1561731227," I finished Simple XLNet implementation with Pytorch Wrapper!

You can see How XLNet Architecture work in pre-training with small batch size(=1) example.

Also I added comment in code, so you can learn XLNet Architecture more easily

[https://github.com/graykode/xlnet-Pytorch](https://github.com/graykode/xlnet-Pytorch)",0,12
1840,2019-6-28,2019,6,28,23,c6l0p7,"[D] I just posted part two in my ML series (""The Pattern Machine"" by Art of the Problem) I'd love feedback on my approach to learning via layers",https://www.reddit.com/r/MachineLearning/comments/c6l0p7/d_i_just_posted_part_two_in_my_ml_series_the/,britcruise,1561733003,,0,1
1841,2019-6-28,2019,6,28,23,c6l2zk,Combining neural NER with gazetteers?,https://www.reddit.com/r/MachineLearning/comments/c6l2zk/combining_neural_ner_with_gazetteers/,mikeross0,1561733335,[removed],1,1
1842,2019-6-29,2019,6,29,0,c6lb0n,"Uncertainty modelling by using Deep Learning for confident predictions, a research PhD topic.",https://www.reddit.com/r/MachineLearning/comments/c6lb0n/uncertainty_modelling_by_using_deep_learning_for/,4xel,1561734478,[removed],0,1
1843,2019-6-29,2019,6,29,0,c6ldhd,Local Temporal Bilinear Pooling for Fine-Grained Action Parsing,https://www.reddit.com/r/MachineLearning/comments/c6ldhd/local_temporal_bilinear_pooling_for_finegrained/,Yuqing7,1561734814,,0,1
1844,2019-6-29,2019,6,29,0,c6ls14,Thought on classification,https://www.reddit.com/r/MachineLearning/comments/c6ls14/thought_on_classification/,Loya_3005,1561736874,[removed],0,1
1845,2019-6-29,2019,6,29,0,c6lwm3,Thought on classification.,https://www.reddit.com/r/MachineLearning/comments/c6lwm3/thought_on_classification/,Loya_3005,1561737495,[removed],0,1
1846,2019-6-29,2019,6,29,1,c6lzk4,What are the most commonly used ways to standardize input size for variable length audio data on CNNs?,https://www.reddit.com/r/MachineLearning/comments/c6lzk4/what_are_the_most_commonly_used_ways_to/,ggalvao,1561737877,[removed],0,1
1847,2019-6-29,2019,6,29,1,c6mceg,"fast.ai Part 2(2019) ""Deep Learning from the Foundations"" is now available publically!",https://www.reddit.com/r/MachineLearning/comments/c6mceg/fastai_part_22019_deep_learning_from_the/,init__27,1561739638,[removed],0,1
1848,2019-6-29,2019,6,29,1,c6md7b,[D] How confident are you of your own analysis ?,https://www.reddit.com/r/MachineLearning/comments/c6md7b/d_how_confident_are_you_of_your_own_analysis/,FluidReality,1561739746,"Say you want to add something to your model that you think might improve overall performance, e.g. some feature engineering or you decide to add to your training data, some new data you acquired. How do you make sure that this is actually increasing performance and that it is not just due to the randomness of the process ? 

I kinda see how it goes for traditional ML with traditional IID assumption, fix a seed for anything that is random and just compare. But what about deep learning models ? 

For instance, say you have your neural network tuned for some past state. Wouldn't comparing past configuration with new configuration (with the added features or training data) on the same network be biased ? Maybe the feature engineering was relevant but because the network isn't large enough, it is not able to process those additional features. Or maybe adding more data changed the loss surface and the learning rate/batch size tuned to the previous configuration is not well fitted to the new configuration ? So surely setting a seed for the randomness of the network training (weights initialization, shuffle after each epoch ... ) is not enough. 

I've thought of doing some sort of autoML/gridsearch to optimize on the learning rate/batch size for several seeds on the weights initialization and do some statistical significance on the results but this would take way too much time considering how many things I need to check. I feel like a statistical study on a given network (with hyperparameters fixed) for different weights initialization might not be relevant.

I'm asking this because whenever I change something on the preprocessing side (new feature, new data, different scaling ...), or even weights initialization of the network, the ""optimal"" learning rate I find by hand tuning my network is never the same (and can differ a lot). 

Any idea is welcome!",5,2
1849,2019-6-29,2019,6,29,2,c6msgn,Announcing the YouTube-8M Segments Dataset,https://www.reddit.com/r/MachineLearning/comments/c6msgn/announcing_the_youtube8m_segments_dataset/,sjoerdapp,1561741823,,0,1
1850,2019-6-29,2019,6,29,2,c6n7gx,[R] VideoBERT: A Joint Model for Video and Language Representation Learning (from Google),https://www.reddit.com/r/MachineLearning/comments/c6n7gx/r_videobert_a_joint_model_for_video_and_language/,bobchennan,1561743876,,6,39
1851,2019-6-29,2019,6,29,2,c6n7ku,Python &amp; Ethical Hacking,https://www.reddit.com/r/MachineLearning/comments/c6n7ku/python_ethical_hacking/,funaf2018,1561743889,,0,0
1852,2019-6-29,2019,6,29,3,c6nm6e,[D] GPT-2 model answers what working at FAANG companies feels like.,https://www.reddit.com/r/MachineLearning/comments/c6nm6e/d_gpt2_model_answers_what_working_at_faang/,Murmani,1561745863,,0,1
1853,2019-6-29,2019,6,29,3,c6npej,"DeepFake Nudie App Goes Viral, Then Shuts Down",https://www.reddit.com/r/MachineLearning/comments/c6npej/deepfake_nudie_app_goes_viral_then_shuts_down/,Yuqing7,1561746291,,0,1
1854,2019-6-29,2019,6,29,3,c6nplm,XLNet Explained (Video),https://www.reddit.com/r/MachineLearning/comments/c6nplm/xlnet_explained_video/,ilokmgra,1561746318,,0,1
1855,2019-6-29,2019,6,29,3,c6nwhp,Demonstration of the largest Transformer language model released to date,https://www.reddit.com/r/MachineLearning/comments/c6nwhp/demonstration_of_the_largest_transformer_language/,lucidrains1,1561747235,,0,1
1856,2019-6-29,2019,6,29,3,c6nyq1,[D] XLNet explained (Video),https://www.reddit.com/r/MachineLearning/comments/c6nyq1/d_xlnet_explained_video/,ilokmgra,1561747541,,1,1
1857,2019-6-29,2019,6,29,3,c6o3em,[D] XLNet explained (Video),https://www.reddit.com/r/MachineLearning/comments/c6o3em/d_xlnet_explained_video/,ilokmgra,1561748154,[removed],0,1
1858,2019-6-29,2019,6,29,4,c6o725,[D] Technical job interview questions to expect for Deep Reinforcement Learning?,https://www.reddit.com/r/MachineLearning/comments/c6o725/d_technical_job_interview_questions_to_expect_for/,Naveos,1561748640,I'm nearing the end of my job interview process and I'm finally going to meet with the research lead soon. The role I am applying for is heavily involved with deep reinforcement learning R&amp;D at a prestigious company. I feel very nervous and am wondering what kind of questions I should expect / prepare for. Do you guys have any advice on what I should read up on?,24,7
1859,2019-6-29,2019,6,29,4,c6oexl,"Fantastic Data Scientists, where to find them, and how to become one",https://www.reddit.com/r/MachineLearning/comments/c6oexl/fantastic_data_scientists_where_to_find_them_and/,rachnogstyle,1561749711,,0,1
1860,2019-6-29,2019,6,29,4,c6ot8d,How to do data preprocessing with .h5 files?,https://www.reddit.com/r/MachineLearning/comments/c6ot8d/how_to_do_data_preprocessing_with_h5_files/,xuzhang5788,1561751646,[removed],0,1
1861,2019-6-29,2019,6,29,5,c6p64j,[D] Bill Gates recently mentioned he would start an AI company that could actually read and understand text. What do sentiment analysis and natural language processing frameworks need to get to this level of intelligence?,https://www.reddit.com/r/MachineLearning/comments/c6p64j/d_bill_gates_recently_mentioned_he_would_start_an/,ExilePrime,1561753314, [https://www.youtube.com/watch?v=s7O3oCWZgjE](https://www.youtube.com/watch?v=s7O3oCWZgjE),97,322
1862,2019-6-29,2019,6,29,5,c6p9hk,Machine learning engineer / researcher salary,https://www.reddit.com/r/MachineLearning/comments/c6p9hk/machine_learning_engineer_researcher_salary/,DaBobcat,1561753756,[removed],0,1
1863,2019-6-29,2019,6,29,5,c6pcv6,Reinforcement Learning - unpredictable output,https://www.reddit.com/r/MachineLearning/comments/c6pcv6/reinforcement_learning_unpredictable_output/,DanielPBak,1561754194,[removed],0,1
1864,2019-6-29,2019,6,29,5,c6pgkh,PTITION : MACRON DEVANT LA COUR PNALE INTERNATIONALE,https://www.reddit.com/r/MachineLearning/comments/c6pgkh/ptition_macron_devant_la_cour_pnale/,vorgeat,1561754665,,0,1
1865,2019-6-29,2019,6,29,6,c6pzw7,Predictive Models leak data in unexpected ways [xkcd],https://www.reddit.com/r/MachineLearning/comments/c6pzw7/predictive_models_leak_data_in_unexpected_ways/,londons_explorer,1561757255,,1,1
1866,2019-6-29,2019,6,29,7,c6qfjr,Tensorflow 2.0 &amp; Keras,https://www.reddit.com/r/MachineLearning/comments/c6qfjr/tensorflow_20_keras/,Vorphus,1561759426,[removed],0,1
1867,2019-6-29,2019,6,29,7,c6qqtk,pre-trained model for face recognition on ResNet-50 or Densenet-121 ?,https://www.reddit.com/r/MachineLearning/comments/c6qqtk/pretrained_model_for_face_recognition_on_resnet50/,Greglama,1561760988,[removed],0,1
1868,2019-6-29,2019,6,29,7,c6qszh,What course shoud I take if I want something AI related?,https://www.reddit.com/r/MachineLearning/comments/c6qszh/what_course_shoud_i_take_if_i_want_something_ai/,Acujl,1561761293,[removed],0,1
1869,2019-6-29,2019,6,29,9,c6s1ha,Bengio brothers interview,https://www.reddit.com/r/MachineLearning/comments/c6s1ha/bengio_brothers_interview/,craigspencersmith,1561767911,,0,1
1870,2019-6-29,2019,6,29,10,c6sfbs,Is Insight Fellows Program 'Mostly' Fake?,https://www.reddit.com/r/MachineLearning/comments/c6sfbs/is_insight_fellows_program_mostly_fake/,compcomp2020,1561770141,[removed],0,1
1871,2019-6-29,2019,6,29,12,c6to7l,Power over Ethernet Lighting,https://www.reddit.com/r/MachineLearning/comments/c6to7l/power_over_ethernet_lighting/,LCAssociates,1561777663,[removed],0,1
1872,2019-6-29,2019,6,29,13,c6u7td,Probabilistic Modelling and Reasoning Lecture Video Capture,https://www.reddit.com/r/MachineLearning/comments/c6u7td/probabilistic_modelling_and_reasoning_lecture/,hithere_23,1561781014,[removed],0,1
1873,2019-6-29,2019,6,29,13,c6up04,Different machine learning models give contradictory results,https://www.reddit.com/r/MachineLearning/comments/c6up04/different_machine_learning_models_give/,stat888r,1561784036,[removed],0,1
1874,2019-6-29,2019,6,29,13,c6up1h,"Best group telegram about deep learning ,machine learning ,.... including Andrew Ng , Fei-Fei Li......",https://www.reddit.com/r/MachineLearning/comments/c6up1h/best_group_telegram_about_deep_learning_machine/,Doctor_who1,1561784045,[removed],0,1
1875,2019-6-29,2019,6,29,15,c6vgur,"Best group telegram about deep learning ,machine learning ,.... including , Fei-Fei Li......",https://www.reddit.com/r/MachineLearning/comments/c6vgur/best_group_telegram_about_deep_learning_machine/,Doctor_who1,1561789308,[removed],0,1
1876,2019-6-29,2019,6,29,15,c6vn8u,LHD S.p.A Is Worldwide Leader in designing and building Telescopic Forks,https://www.reddit.com/r/MachineLearning/comments/c6vn8u/lhd_spa_is_worldwide_leader_in_designing_and/,Richa_11,1561790658,[removed],0,1
1877,2019-6-29,2019,6,29,16,c6vspu,"IMPACT IN THAI, CONCRETE SHOW TIME: 5TH TO 7TH, SEP WELCOME YOUR COMING IN THE NEAR SEPTEMBER See you soon.",https://www.reddit.com/r/MachineLearning/comments/c6vspu/impact_in_thai_concrete_show_time_5th_to_7th_sep/,ada2017,1561791795,,0,1
1878,2019-6-29,2019,6,29,17,c6wn3n,Benign Overfitting in Linear Regression,https://www.reddit.com/r/MachineLearning/comments/c6wn3n/benign_overfitting_in_linear_regression/,chiasmodon,1561798254,,6,16
1879,2019-6-29,2019,6,29,17,c6wo1r,Learn Complete Pandas Library Quickly,https://www.reddit.com/r/MachineLearning/comments/c6wo1r/learn_complete_pandas_library_quickly/,gauravlogical,1561798470,[removed],0,1
1880,2019-6-29,2019,6,29,18,c6wswz,Cluster computer designed for AI and machine learning,https://www.reddit.com/r/MachineLearning/comments/c6wswz/cluster_computer_designed_for_ai_and_machine/,haxterman,1561799560,,0,1
1881,2019-6-29,2019,6,29,18,c6wtwi,Question about taxonomy describing evolution of neural network state.,https://www.reddit.com/r/MachineLearning/comments/c6wtwi/question_about_taxonomy_describing_evolution_of/,jenya3,1561799765,[removed],0,1
1882,2019-6-29,2019,6,29,18,c6wups,Global Vacuum Tanks Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c6wups/global_vacuum_tanks_market_report_2019/,jadhavni3,1561799931,[removed],1,1
1883,2019-6-29,2019,6,29,18,c6wvue,Graph structured memory,https://www.reddit.com/r/MachineLearning/comments/c6wvue/graph_structured_memory/,Hassaan_Hashmi,1561800179,[removed],0,1
1884,2019-6-29,2019,6,29,18,c6wwbv,10 Million Complete Poker Hands Database,https://www.reddit.com/r/MachineLearning/comments/c6wwbv/10_million_complete_poker_hands_database/,OIFAGOS,1561800287,[removed],0,1
1885,2019-6-29,2019,6,29,18,c6wyf5,Global Vertical Freezers Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c6wyf5/global_vertical_freezers_market_report_2019/,jadhavni3,1561800764,[removed],1,1
1886,2019-6-29,2019,6,29,18,c6x40z,[D] State of AI report 2019,https://www.reddit.com/r/MachineLearning/comments/c6x40z/d_state_of_ai_report_2019/,phasesundaftedreverb,1561801947,"Came across the state of AI report and didn't see it posted here so thought it'd be interesting to have a discussion about the recently released report (second yearly one by Nathan Benaich and Ian Hogarth).

[https://www.slideshare.net/StateofAIReport/state-of-ai-report-2019-151804430](https://www.slideshare.net/StateofAIReport/state-of-ai-report-2019-151804430)  


What stands out for you?  
Did they miss anything important?

&amp;#x200B;

Two things stand out for me so far:

1) The 2018 prediction they made that a major AI lab would go dark actually happend (MIRI lab and OpenAI  -GPT2)

&amp;#x200B;

2) For me a as a fps CTF lover: the the Deepmind Quake III RL is wonderful to see. In 2019 they managed to include all the powerups which was a common criticism last year. (Updated blogpost here: [https://deepmind.com/blog/capture-the-flag-science/](https://deepmind.com/blog/capture-the-flag-science/))   


I'm still going through the presentation myself so if I'll see more interesting stuff I'll add it to this thread.",71,226
1887,2019-6-29,2019,6,29,18,c6x5ho,Multilevel Modelling of U.S. Home Loan Data,https://www.reddit.com/r/MachineLearning/comments/c6x5ho/multilevel_modelling_of_us_home_loan_data/,plentyofnodes,1561802256,[removed],0,1
1888,2019-6-29,2019,6,29,18,c6x5kg,Global Vertical Speed Indicators Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c6x5kg/global_vertical_speed_indicators_market_report/,jadhavni3,1561802274,[removed],1,1
1889,2019-6-29,2019,6,29,19,c6x93c,Global Vibratory Plows Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c6x93c/global_vibratory_plows_market_report_2019/,jadhavni3,1561803022,[removed],1,1
1890,2019-6-29,2019,6,29,19,c6x9ak,Understanding Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/c6x9ak/understanding_artificial_intelligence/,vaika-varma,1561803063,,0,1
1891,2019-6-29,2019,6,29,19,c6xb3s,Label Noise in rule based labelling. seq2seq,https://www.reddit.com/r/MachineLearning/comments/c6xb3s/label_noise_in_rule_based_labelling_seq2seq/,lifeinsrndpt,1561803449,[removed],0,1
1892,2019-6-29,2019,6,29,19,c6xbwr,Global Vibratory Screener Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c6xbwr/global_vibratory_screener_market_report_2019/,jadhavni3,1561803621,[removed],1,1
1893,2019-6-29,2019,6,29,19,c6xf7t,Global Walking Robots Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c6xf7t/global_walking_robots_market_report_2019/,jadhavni3,1561804315,[removed],1,1
1894,2019-6-29,2019,6,29,19,c6xklr,Global Water Cannon Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c6xklr/global_water_cannon_market_report_2019/,jadhavni3,1561805414,[removed],1,1
1895,2019-6-29,2019,6,29,20,c6xq1g,Global Water Cannon Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c6xq1g/global_water_cannon_market_report_2019/,jadhavni3,1561806574,[removed],1,1
1896,2019-6-29,2019,6,29,20,c6xt43,Global Arc Welding Robots Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c6xt43/global_arc_welding_robots_market_report_2019/,jadhavni3,1561807238,[removed],1,1
1897,2019-6-29,2019,6,29,20,c6y3l0,Global Arcade Cabinets Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c6y3l0/global_arcade_cabinets_market_report_2019/,jadhavni3,1561809347,[removed],1,1
1898,2019-6-29,2019,6,29,21,c6y5xr,Can we use convergent cross mapping for non time series data?,https://www.reddit.com/r/MachineLearning/comments/c6y5xr/can_we_use_convergent_cross_mapping_for_non_time/,maher_alzuhairi,1561809812,[removed],0,1
1899,2019-6-29,2019,6,29,21,c6y7hv,Global Automatic Pill Dispensers Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c6y7hv/global_automatic_pill_dispensers_market_report/,jadhavni3,1561810107,[removed],1,1
1900,2019-6-29,2019,6,29,21,c6y979,[R] Multilevel Modelling of U.S. Home Loan Data,https://www.reddit.com/r/MachineLearning/comments/c6y979/r_multilevel_modelling_of_us_home_loan_data/,plentyofnodes,1561810439,"This is an example of multilevel modelling in R, which is necessary when data is split across different categories or hierarchies. Specifically, we can see how this can model mortgage risk (in terms of the loan to value ratio) across different U.S. states. 

Any opinions or feedback welcome.

Link: https://www.michael-grogan.com/mortgage-multilevel/",0,4
1901,2019-6-29,2019,6,29,21,c6yhrw,Any advice or resource for a complete noob trying to learn OCR?,https://www.reddit.com/r/MachineLearning/comments/c6yhrw/any_advice_or_resource_for_a_complete_noob_trying/,PaperSense,1561812027,[removed],0,1
1902,2019-6-29,2019,6,29,22,c6yphd,"[D] DEEPNUDE &amp; DeepFake Source - App, installation and web",https://www.reddit.com/r/MachineLearning/comments/c6yphd/d_deepnude_deepfake_source_app_installation_and/,morro3,1561813402,"Hope it works for you too, links updated regularly :) 

&amp;#x200B;

Also much more on the channel :)

&amp;#x200B;

[https://discord.gg/HKMEpg](https://discord.gg/HKMEpg)

&amp;#x200B;

I hope it is the right reddit to post it as it is all based on MachineLearning :)",539,0
1903,2019-6-29,2019,6,29,22,c6yu5n,What book do you recomend for a begginer that wants to lern pratical machine lerning?,https://www.reddit.com/r/MachineLearning/comments/c6yu5n/what_book_do_you_recomend_for_a_begginer_that/,Acujl,1561814203,[removed],0,1
1904,2019-6-29,2019,6,29,22,c6z08h,The wAIting Game,https://www.reddit.com/r/MachineLearning/comments/c6z08h/the_waiting_game/,Wenderu84,1561815240,[removed],0,0
1905,2019-6-29,2019,6,29,22,c6z9fd,Global Baggage Handling Systems for Airport Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c6z9fd/global_baggage_handling_systems_for_airport/,jadhavni3,1561816741,[removed],1,1
1906,2019-6-29,2019,6,29,22,c6z9is,Different machine learning models give contradictory results,https://www.reddit.com/r/MachineLearning/comments/c6z9is/different_machine_learning_models_give/,stat888r,1561816758,[removed],0,1
1907,2019-6-29,2019,6,29,23,c6zdk5,Global Cocktail Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c6zdk5/global_cocktail_market_report_2019/,jadhavni3,1561817366,[removed],1,1
1908,2019-6-29,2019,6,29,23,c6zef6,Join Politics &amp; AI project - looking for data scientist,https://www.reddit.com/r/MachineLearning/comments/c6zef6/join_politics_ai_project_looking_for_data/,sorinadrianr,1561817497,[removed],0,0
1909,2019-6-29,2019,6,29,23,c6zwen,[Project] Use ML in Politics - make your math matter :),https://www.reddit.com/r/MachineLearning/comments/c6zwen/project_use_ml_in_politics_make_your_math_matter/,sorinadrianr,1561820316,"Tech and product team looking to start an AI based project with potential of turning into a startup. 
Looking for a data scientist partner who:
- is passionate about politics and civics ( doesnt matter if youre left or right)
 - dynamic and driven and can commit time
- ML experience a must ; NLP preferred 
- Boston or east coast based  preferably
- can work part of a team
- is fun to be around!

Let us know!!! We want to talk to you.",2,0
1910,2019-6-30,2019,6,30,0,c6zyir,How do you predict the sequence from Lorenz attractor?,https://www.reddit.com/r/MachineLearning/comments/c6zyir/how_do_you_predict_the_sequence_from_lorenz/,su_panda,1561820625,[removed],0,1
1911,2019-6-30,2019,6,30,0,c702t4,How do you model the chaotic behaviour(like the sequence from Lorenz attractor) in a stochastic sense?,https://www.reddit.com/r/MachineLearning/comments/c702t4/how_do_you_model_the_chaotic_behaviourlike_the/,su_panda,1561821247,"Recently, I encountered a difficulty of prediction Lorenz attractor by using a GRU.

(See the code from [here](https://github.com/allnightlight/LorenzAttractorPrediction).)

I think that it's inevitable since the original system, i.e. Lorenz equation, is too sensitive to the perturbation,which can cause the random behaviour and the difficulty of prediction.

On the other hand, some theories for modeling the probability of dataset by using neural networks have been developed in the Machine learning area, like GANs, VAE, etc..

&amp;#x200B;

So, let me pose this question here:

Can the chaotic behaviour(like the sequence from Lorenz attractor) be characterised by an ML technique in a stochastic sense? 

More precisely, given a sequence from Lorenz attractor: $y\_{1} \\cdots y\_{N}$, can we get the probability of $p(y\_{1}, ..., y\_{N})$ based on a neural network?",0,1
1912,2019-6-30,2019,6,30,1,c717bk,Under what conditions does it make sense to use heterogeneous data to train a classifier?,https://www.reddit.com/r/MachineLearning/comments/c717bk/under_what_conditions_does_it_make_sense_to_use/,jintoku,1561825723,"Say I wanted to train a classifier for whether someone will become a criminal, depending on the conditions in which they grow up, across different countries. There will be common predictors across countries correlating with certain crime types (say poverty might be a predictor for theft) but there will also be complicating differences across countries, say in some countries education and poverty might make theft less likely than growing up middle class and being educated. I'm really just making up examples of how different predictors might have different predictive value but the general question I have is: when does it make sense to use external data (in this example from an external country) to train a classifier to predict something in a given country, for instance in a case where there are not a lot of data points for the country in question? Is there a general rule?",0,1
1913,2019-6-30,2019,6,30,2,c71r3x,Recreating Andrew NG Intro to ML Ex 4 in Tensorflow Keras,https://www.reddit.com/r/MachineLearning/comments/c71r3x/recreating_andrew_ng_intro_to_ml_ex_4_in/,FilipAI,1561827721,[removed],0,1
1914,2019-6-30,2019,6,30,2,c71sf8,Suggestion for AI project,https://www.reddit.com/r/MachineLearning/comments/c71sf8/suggestion_for_ai_project/,Albertobagnacani,1561827897,,0,1
1915,2019-6-30,2019,6,30,2,c724gl,Distributed Machine Learning with Apache Mahout,https://www.reddit.com/r/MachineLearning/comments/c724gl/distributed_machine_learning_with_apache_mahout/,andrea_manero,1561829578,[removed],0,1
1916,2019-6-30,2019,6,30,2,c725q2,ML on various small MCUs,https://www.reddit.com/r/MachineLearning/comments/c725q2/ml_on_various_small_mcus/,dimtass,1561829750,[removed],0,1
1917,2019-6-30,2019,6,30,2,c7280a,[D] Best way to label/prepare data for full-body gesture recognition.,https://www.reddit.com/r/MachineLearning/comments/c7280a/d_best_way_to_labelprepare_data_for_fullbody/,ArsenicBismuth,1561830062,"Sorry, I'm pretty much a newbie to this kind of things. Long story short, I have a pose data obtained from Openpose and I want to recognize certain gestures using LSTM-RNN.

Training-wise it's pretty much not a problem acquiring 98% on training set and 90%+ on test set. But implementing it on my real-time data always shows how bad it's. Making me thinking it's learning the wrong kind of features.

What I've done:
- With no pre-processing at all, I separated the poses into 4 regions: NE, NW, SE, SW (as in ""north-east"", etc) which basically means the poses happening in 4 very diff locations on the image. Result: The LSTM isn't even correctly recognizing the separated region consistently, let alone the certain gestures.

- Amplifying the real distance between the 4 regions, by giving them a large offset if a certain boundary is passed. Again, same result.",0,1
1918,2019-6-30,2019,6,30,3,c72hcn,[D] Best way to label/prepare data for full-body gesture recognition.,https://www.reddit.com/r/MachineLearning/comments/c72hcn/d_best_way_to_labelprepare_data_for_fullbody/,ArsenicBismuth,1561831366,"Sorry, I'm pretty much a newbie to this kind of things. Long story short, I have a pose data obtained from Openpose and I want to recognize certain gestures using LSTM-RNN. A gesture would be 8 consecutive poses obtained from camera. Some of the gestures: walking, sweeping, idling, bringing object.

Training-wise it's pretty much not a problem acquiring 98% on training set and 90%+ on test set. Data wise also shouldn't be a problem since I have over millions of poses already (the day I realized Google Sheet cells limit is too small LOL). But implementing it on my real-time data always shows how bad it's. Making me thinking it's learning the wrong kind of features.

What I've done: 

- With no pre-processing at all, I separated the poses into 4 regions: NE, NW, SE, SW (as in ""north-east"", etc) which basically means the poses happening in 4 very diff locations on the image. Result: The LSTM isn't even correctly recognizing the separated region consistently, let alone the certain gestures.

- Amplifying the real distance between the 4 regions, by giving them a large offset if a certain boundary is passed. Again, same result.

- Normalizing all of the poses to origin. This way no translation info is present on the data (inference &amp; training). Yet, even after simplifying the data for two classes, it's still so bad.

- Normalizing only the first poses of a gesture to origin, and the last 7 would start moving from origin (to preserve translation data). Even worse that previous iteration on test set, giving 85%.


At this point, I'm afraid I'm doing something fundamentally wrong with my data. What I worried about are:

- I divide my gestures into regions instead of the direction it's going.

- The model just isn't cut for it.

- The movement from the noise is way too big compared to the actual gesture.

I humbly ask for any assistance at this point LOL.",4,0
1919,2019-6-30,2019,6,30,3,c72mnc,[D] How DeepNude dataset was created?,https://www.reddit.com/r/MachineLearning/comments/c72mnc/d_how_deepnude_dataset_was_created/,egormalyutin,1561832090,"I wonder how DeepNude dataset was created. Where the author of this program got all these 10,000 images for neural network training? I tried to find some pictures of women with clothes on/off, but they are mostly in different poses on on and off photos.",3,0
1920,2019-6-30,2019,6,30,3,c72tn9,"[D] As a Non CS undergrad looking to get internships in ML or AI in the summer of 2020, what are the skills / projects that I would need to acquire before hand",https://www.reddit.com/r/MachineLearning/comments/c72tn9/d_as_a_non_cs_undergrad_looking_to_get/,shivakanthsujit,1561833092,"At the end of the 6th semester, I'll have to do an internship as part of my coursework. I would like to get one in the field of ML or AI. 

I have attended a few courses on ML and have a grasp of the math and theoretical concepts involved. But I have not worked on any projects where I have used machine learning algorithms.  

I would prefer to work under a professor at a university for this intern and I guess I'll need to have good amount of experience in order to do so. So what kind of experience would be useful in this case?",11,0
1921,2019-6-30,2019,6,30,3,c72v0p,Teaching artificial intelligence to connect senses like vision and touch,https://www.reddit.com/r/MachineLearning/comments/c72v0p/teaching_artificial_intelligence_to_connect/,1con0clast,1561833288,,0,1
1922,2019-6-30,2019,6,30,4,c73mpe,NVIDIA DIGITS and python 2,https://www.reddit.com/r/MachineLearning/comments/c73mpe/nvidia_digits_and_python_2/,newSam111,1561837290,[removed],0,1
1923,2019-6-30,2019,6,30,4,c73q9l,Identifying features that drive cluster formation,https://www.reddit.com/r/MachineLearning/comments/c73q9l/identifying_features_that_drive_cluster_formation/,OldManufacturer,1561837802,[removed],0,1
1924,2019-6-30,2019,6,30,4,c73sdh,Custom Voice in Text to Speech,https://www.reddit.com/r/MachineLearning/comments/c73sdh/custom_voice_in_text_to_speech/,IonutRo99,1561838102,[removed],0,1
1925,2019-6-30,2019,6,30,4,c73uc4,[D] I tried to plot 6D data using pseudo dimensions in Python,https://www.reddit.com/r/MachineLearning/comments/c73uc4/d_i_tried_to_plot_6d_data_using_pseudo_dimensions/,prasadostwal,1561838376,"After many attempts to visualize and intuitively understand data, I came up with an idea to use pseudo dimensions for plotting 4D, 5D..6D data. I used different colors, size and shapes for adding pseudo dimensions. Although it isn't a true dimensional representation, it provides fairly good intuition about data.

I used plot.ly for drawing plots.

Here's article the link:
https://link.medium.com/iaHQMM8gVX

Repo:
https://github.com/ostwalprasad/PythonMultiDimensionalPlots",3,0
1926,2019-6-30,2019,6,30,6,c74v5j,Theoretical Limit to Machine Learning Accuracy,https://www.reddit.com/r/MachineLearning/comments/c74v5j/theoretical_limit_to_machine_learning_accuracy/,reactiveneon,1561843785,[removed],0,1
1927,2019-6-30,2019,6,30,6,c751f3,[Article] An account of how brain-like chips could be the future of AI,https://www.reddit.com/r/MachineLearning/comments/c751f3/article_an_account_of_how_brainlike_chips_could/,prime_007,1561844723,[removed],0,1
1928,2019-6-30,2019,6,30,7,c75ajh,"Help me understand your relationship to the environment: 2 minute survey, thanks!",https://www.reddit.com/r/MachineLearning/comments/c75ajh/help_me_understand_your_relationship_to_the/,uxrose,1561846098,,0,1
1929,2019-6-30,2019,6,30,9,c76zpr,"[N] ""Manufacturing memory means scribing silicon in a sea of sensors"" [deep learning for silicon fab quality control]",https://www.reddit.com/r/MachineLearning/comments/c76zpr/n_manufacturing_memory_means_scribing_silicon_in/,gwern,1561855908,,0,1
1930,2019-6-30,2019,6,30,10,c778tc,[P] Deepnude's application seems to be taken over and given new life.,https://www.reddit.com/r/MachineLearning/comments/c778tc/p_deepnudes_application_seems_to_be_taken_over/,OfficialYomojo,1561857446,I just stumbled upon this while reading through the Twitter of the official Deepnude and I came across DeepNude Stable https://twitter.com/DeepnudeStable I tried to join their discord server and in my opinion it looks they are giving new life to the official application. They also want to use it for different purposes which doesn't involve adult content. Might be worth to follow around?,0,1
1931,2019-6-30,2019,6,30,10,c77cwf,"scikit-learn cross validation, negative values with mean squared error",https://www.reddit.com/r/MachineLearning/comments/c77cwf/scikitlearn_cross_validation_negative_values_with/,DiligentStudio7,1561858177,[removed],0,1
1932,2019-6-30,2019,6,30,11,c77xow,PowerPlay: training an increasingly general problem solver by continually searching for the simplest still unsolvable problem,https://www.reddit.com/r/MachineLearning/comments/c77xow/powerplay_training_an_increasingly_general/,mystikaldanger,1561861779,,0,1
1933,2019-6-30,2019,6,30,11,c783ib,Updated:,https://www.reddit.com/r/MachineLearning/comments/c783ib/updated/,iyotman,1561862796,,0,1
1934,2019-6-30,2019,6,30,12,c788o3,"The ML Brothers, Yoshua and Samy Bengio",https://www.reddit.com/r/MachineLearning/comments/c788o3/the_ml_brothers_yoshua_and_samy_bengio/,craigspencersmith,1561863702,,0,1
1935,2019-6-30,2019,6,30,12,c78n61,[P] Writing mission statement for project related to machine learning for catalyst design.,https://www.reddit.com/r/MachineLearning/comments/c78n61/p_writing_mission_statement_for_project_related/,schizophrenicman123,1561866279,"Hey all!  

I am starting my first scientific job after my PhD in Physics, as a research scientist this summer in Beijing for a project related to catalyst design. I am very excited to be part of this project. I am starting to write out the scope of the project.  Let me know any tips or advice you may have. 

Many fields in Physics and Chemistry utilize the Schrodinger Equation or Density Functional Theory to compute various properties and insights into the systems under study. Any computational natural scientist will know that in practice the starting point for many of these calculations, in other words what is fed as input into the machine are the atomic numbers and geometries.  

**Two Approaches**

* (a) Can machine learning help discover new insights and physics into the systems of interest- From knowledge of just the molecular structures of the atoms are we able to acquire any insight into more efficient and less harmful combustion reactions that utilize catalysts? In a more broader sense this may give insight into the computational quantum mechanical approach.  

* (b) The inverse design problem deals with the prediction of novel undiscovered molecules.  

**Methodology**


These two problems have the opposite approaches: in case (a) we move from a known chemical space towards prediction of physical and chemical quantum properties, whereas in the second case we would like to start from a desired property to make predictions about the chemical space. 

To approach question (a) the work uses quantum mechanical data to undergo supervised learning methods which have been shown to perform generally on the same scale as models that utilize unsupervised (for e.g. convulutional neural networks).  
No theoretical methods exist to explore all combinatorially possible alloyed systems. ( for the smallest known thiolated nanocluster, Au_15 (SR)_13 there would be over 32k possibilities which presents a significant computational challenge to characterize all potential structures)[1]

The overall problem of catalyst design from a brute force machine learning method would therefore be to have stacks or layers of features and predictions starting from knowledge of just the geometry and atomic numbers of the molecular structures.  

(b) The inverse design problem revolves around finding the best chemical structures with desired properties. One could utilize invertible models from machine learning such as generative models (GANs, autoencoders).  
According to Kulik et al. [2]  only a tiny fraction (1 part in 10^50) of chemical space has ever been explored. This necessitates the need for machine learning approach rather than design each molecular system by hand (either experimentally or computationally). A great review paper that provides insight into how the inverse design problem may be approached was written by B. Sanchez-Lengeling and Alan Aspuru-Guzik [3]. One could begin unsupervised learning starting from databases of potential catalysts and 

[1] Machine-Learning Prediction CO Adsorption in Thiolated, Ag- Alloyed Au Nanoclusters, J. Am. Chem. Soc. 2018, James P. Lewis et al.

[2] Designing in the Face of Uncertainty: Exploiting Electronic Structure and Machine Learning Models for Discovery, Inorganic Chemistry, Inorg. Chem. 2019, H. Kulik et al.  

[3] Inverse Molecular Design using Machine Learning: Generative models for matter engineering, Science 2018, B.Sanchez-Lengeling and Alan Aspuru-Guzik.",9,59
1936,2019-6-30,2019,6,30,13,c795ve,IBM - Machine Learning Telephonic Interview || SyncTech,https://www.reddit.com/r/MachineLearning/comments/c795ve/ibm_machine_learning_telephonic_interview_synctech/,bairyRajeshwar,1561869701,,0,1
1937,2019-6-30,2019,6,30,13,c799ot,[R] Deep Video Inpainting,https://www.reddit.com/r/MachineLearning/comments/c799ot/r_deep_video_inpainting/,mcahny,1561870435,,1,1
1938,2019-6-30,2019,6,30,14,c79eq2,[R] Deep Video Inpainting,https://www.reddit.com/r/MachineLearning/comments/c79eq2/r_deep_video_inpainting/,mcahny,1561871422,,0,1
1939,2019-6-30,2019,6,30,14,c79sqk,XLNet language generation,https://www.reddit.com/r/MachineLearning/comments/c79sqk/xlnet_language_generation/,Professor_Entropy,1561874286,[removed],1,1
1940,2019-6-30,2019,6,30,16,c7a9xr,Cleaning Agent jobs dead,https://www.reddit.com/r/MachineLearning/comments/c7a9xr/cleaning_agent_jobs_dead/,id_Ali,1561878079,,0,1
1941,2019-6-30,2019,6,30,16,c7aaud,Ideas for Projects in Object Detection,https://www.reddit.com/r/MachineLearning/comments/c7aaud/ideas_for_projects_in_object_detection/,insomniac_dorm,1561878280,[removed],0,1
1942,2019-6-30,2019,6,30,17,c7aqok,Why is Bert masked-LM training is Unsupervised,https://www.reddit.com/r/MachineLearning/comments/c7aqok/why_is_bert_maskedlm_training_is_unsupervised/,albert1905,1561881998,[removed],0,1
1943,2019-6-30,2019,6,30,17,c7atnc,AirSim Unity reinforcement learning Quadrotor Pytorch,https://www.reddit.com/r/MachineLearning/comments/c7atnc/airsim_unity_reinforcement_learning_quadrotor/,subinlab,1561882777,[removed],0,1
1944,2019-6-30,2019,6,30,18,c7b5qv,[D] Opinions on the ACML Conference?,https://www.reddit.com/r/MachineLearning/comments/c7b5qv/d_opinions_on_the_acml_conference/,alexmlamb,1561885950,"Do you guys have an opinion on the asian conference on machine learning (ACML)?  Of course it's not as strong as ICML or Neurips but if you've submitted there or attended I'm curious if anyone has any opinions.  Are reviewers reasonably knowledgable?  Do attendees ask decent questions?  

&amp;#x200B;

[http://www.acml-conf.org/2019/](http://www.acml-conf.org/2019/)",4,35
1945,2019-6-30,2019,6,30,19,c7bhef,Machine Learning Bootcamp: Become a ML Engineer in 6 months. Job Guaranteed.,https://www.reddit.com/r/MachineLearning/comments/c7bhef/machine_learning_bootcamp_become_a_ml_engineer_in/,HannahHumphreys,1561889018,[removed],0,1
1946,2019-6-30,2019,6,30,19,c7bi2j,Learn Machine Learning 2019|| From Beginner to Intermediate || video lectures of Courses To Follow,https://www.reddit.com/r/MachineLearning/comments/c7bi2j/learn_machine_learning_2019_from_beginner_to/,_learn_to_earn,1561889209,,0,1
1947,2019-6-30,2019,6,30,22,c7ddtk,Artificial Intelligence Vs Machine Learning Vs Deep Learning,https://www.reddit.com/r/MachineLearning/comments/c7ddtk/artificial_intelligence_vs_machine_learning_vs/,meancoder,1561899834,,0,1
1948,2019-6-30,2019,6,30,22,c7dkxx,DeepNude App Down - Shows how deepfakes harm the most vulnerable,https://www.reddit.com/r/MachineLearning/comments/c7dkxx/deepnude_app_down_shows_how_deepfakes_harm_the/,sudhabhise,1561900574,,0,1
1949,2019-6-30,2019,6,30,23,c7ecxi,Image to image regression?,https://www.reddit.com/r/MachineLearning/comments/c7ecxi/image_to_image_regression/,SamPusegaonkar,1561903362,[removed],0,1
1950,2019-6-30,2019,6,30,23,c7eikh,I am looking for a remote research internship.,https://www.reddit.com/r/MachineLearning/comments/c7eikh/i_am_looking_for_a_remote_research_internship/,TheHawkGriffith,1561903914,[removed],0,1
1951,2019-6-30,2019,6,30,23,c7ezas,Python's Numpy,https://www.reddit.com/r/MachineLearning/comments/c7ezas/pythons_numpy/,Lakshmi16,1561905549,,0,1
1952,2019-6-30,2019,6,30,23,c7f125,My reviews of 8 Python Deep Learning Computer Vision Courses (LONG),https://www.reddit.com/r/MachineLearning/comments/c7f125/my_reviews_of_8_python_deep_learning_computer/,careertroubleguy,1561905715,,0,1
1953,2019-6-30,2019,6,30,23,c7f46s,what would be some good ML labs to join while doing M.Sc. in Canada?,https://www.reddit.com/r/MachineLearning/comments/c7f46s/what_would_be_some_good_ml_labs_to_join_while/,furciferX,1561906020,[removed],0,1
