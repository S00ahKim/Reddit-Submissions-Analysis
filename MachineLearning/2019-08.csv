,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2019-8-1,2019,8,1,9,ckgqan,"$500/2hr Paid Interview and Activity in San Jose, CA",https://www.reddit.com/r/MachineLearning/comments/ckgqan/5002hr_paid_interview_and_activity_in_san_jose_ca/,UErecruiting,1564618373,[removed],0,0
1,2019-8-1,2019,8,1,9,ckgv6f,GitHub - dppalomar/riskparity.py: fast and scalable design of risk parity portfolios with TensorFlow 2.0,https://www.reddit.com/r/MachineLearning/comments/ckgv6f/github_dppalomarriskparitypy_fast_and_scalable/,jvmirca,1564619105,,0,1
2,2019-8-1,2019,8,1,10,ckhtol,"Asked for follow-up video interview, then asked for references after onsite, normal??",https://www.reddit.com/r/MachineLearning/comments/ckhtol/asked_for_followup_video_interview_then_asked_for/,addyr,1564624150,[removed],0,1
3,2019-8-1,2019,8,1,11,cki4ic,Deep Learning in R,https://www.reddit.com/r/MachineLearning/comments/cki4ic/deep_learning_in_r/,HannahHumphreys,1564625750,[removed],0,1
4,2019-8-1,2019,8,1,11,ckilp7,"[D] do you agree ""The research on how to deal with time-series data is almost finished""?",https://www.reddit.com/r/MachineLearning/comments/ckilp7/d_do_you_agree_the_research_on_how_to_deal_with/,GoBacksIn,1564628352,"here is my question in quora

 [https://www.quora.com/Why-is-RNN-less-progress-research-than-CNN-especially-the-time-series](https://www.quora.com/Why-is-RNN-less-progress-research-than-CNN-especially-the-time-series) 

question is 

    Why is RNN less progress research than CNN (especially the time series)? 

and answer is

 

    Any problem concerning the images is incredibly harder than that concerns a time series. This is why the research on CNN and its derivatives (U-Net, GAN) are still continuing.
    
    The research on how to deal with time-series data is almost finished. It looks like researchers are trying to come up with better and better techniques, but what is actually happening is people are trying to predict dependencies that are actually not present in the data or using insufficient data!
    
    A good example is the stock value prediction. The simple truth is that the stock prices depend on many more variables that are not present in the typical input time series used with RNN or LSTM.


 My personal opinion is  now LSTM is SOTA but I think another SOTA network will be created. 

&amp;#x200B;

how about think of this topic?",9,0
5,2019-8-1,2019,8,1,12,ckimcm,[D] Can I use named entity recognition and multitext classification to train spacy to link key value pairs from form data?,https://www.reddit.com/r/MachineLearning/comments/ckimcm/d_can_i_use_named_entity_recognition_and/,bigdbag999,1564628457,"Like the title asks, if I have a string like ""address 1234 home street"", can I get spacy to recognize that the key is address and the value is 1234 home street? Or I guess the better question is, if I just have a string like 1234 home street, can i get spacy to recognize that as an address without further context around the string? It's not in a sentence since it's derived from form data extracted via OCR. More difficult, get spacy to recognize a string that isn't a traditional category like an address, but a custom category?

How do I start researching this?",2,2
6,2019-8-1,2019,8,1,12,ckiwcg,"[D] Where's that long list of neural networks that worked around the problem it was supposed to solve? For example, one of them found a bug in a simulation and exploited it to minimize the objective function.",https://www.reddit.com/r/MachineLearning/comments/ckiwcg/d_wheres_that_long_list_of_neural_networks_that/,ThisIs_MyName,1564630018,I think I saw it on twitter a few months ago. The list had links to papers and it was pretty long. Anyone got a link?,25,265
7,2019-8-1,2019,8,1,12,ckiz7u,Chances for top ML PhD programs?,https://www.reddit.com/r/MachineLearning/comments/ckiz7u/chances_for_top_ml_phd_programs/,DeeplyTheoreticalMan,1564630486,[removed],0,1
8,2019-8-1,2019,8,1,12,ckj0cm,Assist with suggestion of methods i can use to compare data collected in two time period (2014/ 2015),https://www.reddit.com/r/MachineLearning/comments/ckj0cm/assist_with_suggestion_of_methods_i_can_use_to/,mwennde,1564630677,[removed],0,1
9,2019-8-1,2019,8,1,12,ckj7um,Career advice by Yoshua and Rich for young researchers,https://www.reddit.com/r/MachineLearning/comments/ckj7um/career_advice_by_yoshua_and_rich_for_young/,Nishanth127,1564631922,[removed],1,0
10,2019-8-1,2019,8,1,13,ckj970,how does google does text recognition in firebase,https://www.reddit.com/r/MachineLearning/comments/ckj970/how_does_google_does_text_recognition_in_firebase/,dchatterjee172,1564632152,[removed],0,1
11,2019-8-1,2019,8,1,13,ckjm33,Help with developing a trend prediction model.,https://www.reddit.com/r/MachineLearning/comments/ckjm33/help_with_developing_a_trend_prediction_model/,altran1502,1564634319,[removed],0,1
12,2019-8-1,2019,8,1,13,ckjs91,"[D] Has anyone figured out why Adam, RMSProp, And Adadelta don't do well for training word embedding models, often worse than SGD?",https://www.reddit.com/r/MachineLearning/comments/ckjs91/d_has_anyone_figured_out_why_adam_rmsprop_and/,Research2Vec,1564635426,"It's something I've heard here and there but never really got an explanation. 

From online, I found this and this 

https://hackernoon.com/various-optimisation-techniques-and-their-impact-on-generation-of-word-embeddings-3480bd7ed54f

https://stats.stackexchange.com/questions/288658/better-performance-with-gradient-descent-than-adam-on-word2vec

Optimizers that build upon Adagrad aim to fix the vanishing learning rate problem, so why would they do worse? 

Perhaps minimas are really unstable, and would benefit from the smaller learning rates. Could this issue then be alleviated by increasing the window of past gradients?",19,18
13,2019-8-1,2019,8,1,13,ckjsjq,What does the capital letter 'J' stands for in cost function J()?,https://www.reddit.com/r/MachineLearning/comments/ckjsjq/what_does_the_capital_letter_j_stands_for_in_cost/,wodnjs116,1564635481,[removed],0,1
14,2019-8-1,2019,8,1,14,ckjwek,[D] What does the capital letter 'J' stands for in cost function J()?,https://www.reddit.com/r/MachineLearning/comments/ckjwek/d_what_does_the_capital_letter_j_stands_for_in/,wodnjs116,1564636163,I know a lot of people use J to represent cost function. what does the capital letter 'J' exactly mean?,17,18
15,2019-8-1,2019,8,1,14,ckk45i,Artificial Intelligence to speed up trip planning,https://www.reddit.com/r/MachineLearning/comments/ckk45i/artificial_intelligence_to_speed_up_trip_planning/,Ripple2709,1564637595,,0,1
16,2019-8-1,2019,8,1,14,ckk7rh,[D] Ideas on how to go about implementing a filter to deform a pattern,https://www.reddit.com/r/MachineLearning/comments/ckk7rh/d_ideas_on_how_to_go_about_implementing_a_filter/,Pepe_thelord,1564638267,"Hello everyone,

This is my very first post on this thread. I am recently getting into machine learning and CNNs. I am trying to implement a machine learning-based solution, where if I input a pattern with rough edges into the model, it will output a treated pattern based on the training data pairs.

10 examples of an input and an expected output pair are shown below, where the input is shown by the patterns outlined in black lines and the expected output is in blue.

https://i.redd.it/jywfo7ocwrd31.png

I would greatly appreciate any suggestions/help on how can I go about implementing this or where should I look for more information? 

&amp;#x200B;

Thank you",0,1
17,2019-8-1,2019,8,1,14,ckkb34,"Its so smooth, youll be thrilled - Engati",https://www.reddit.com/r/MachineLearning/comments/ckkb34/its_so_smooth_youll_be_thrilled_engati/,getengati,1564638894,[removed],0,1
18,2019-8-1,2019,8,1,14,ckkbvc,[D] Ideas on how can I implement a ML based solution to treat patterns according to training data,https://www.reddit.com/r/MachineLearning/comments/ckkbvc/d_ideas_on_how_can_i_implement_a_ml_based/,Pepe_thelord,1564639042,"Hello everyone, 

This is my very first post on this thread. I apologize if this question sounds dumb, I am new to machine learning and CNNs. I am trying to implement a machine learning-based solution, where if I input a pattern with rough edges into the model, it will output a treated pattern based on the training data pairs. 

10 examples of an input and an expected output pair are shown below, where the inputs are shown by the patterns outlined in black lines and the expected outputs are in blue. 

https://i.redd.it/zg4qf0hvzrd31.png

I would greatly appreciate any suggestions/help on how can I go about implementing this? 

&amp;#x200B;

Thank you",21,7
19,2019-8-1,2019,8,1,15,ckkfpq,Models Suggestion for Information Retrieval problem,https://www.reddit.com/r/MachineLearning/comments/ckkfpq/models_suggestion_for_information_retrieval/,IdlePerfectionist,1564639765,[removed],0,1
20,2019-8-1,2019,8,1,16,ckl80s,"What are interesting research paper topics about Machine Learning / Artificial Intelligence in finances, markets, trading or economics in general?",https://www.reddit.com/r/MachineLearning/comments/ckl80s/what_are_interesting_research_paper_topics_about/,hjc5858,1564645261,[removed],0,1
21,2019-8-1,2019,8,1,18,ckm1qu,Google claims DeepMind AI can detect acute kidney disease,https://www.reddit.com/r/MachineLearning/comments/ckm1qu/google_claims_deepmind_ai_can_detect_acute_kidney/,sudhabhise,1564651570,,0,1
22,2019-8-1,2019,8,1,18,ckm44u,[1907.12378] Music Recommendations in Hyperbolic Space: An Application of Empirical Bayes and Hierarchical Poincar Embeddings,https://www.reddit.com/r/MachineLearning/comments/ckm44u/190712378_music_recommendations_in_hyperbolic/,_joermungandr_,1564652053,,1,5
23,2019-8-1,2019,8,1,18,ckm554,Guide for experiments in Deep learning,https://www.reddit.com/r/MachineLearning/comments/ckm554/guide_for_experiments_in_deep_learning/,textMinier,1564652253,[removed],0,1
24,2019-8-1,2019,8,1,18,ckm64n,Machine learning-driven foundational data discovery: Adding efficacy to the data lake/data warehouse combination | Io-Tahoe,https://www.reddit.com/r/MachineLearning/comments/ckm64n/machine_learningdriven_foundational_data/,jacobmarsh789,1564652470,,0,1
25,2019-8-1,2019,8,1,18,ckm9sv,"Object detection (ttinyyolov2, tinyyolov3) in realtime video streams",https://www.reddit.com/r/MachineLearning/comments/ckm9sv/object_detection_ttinyyolov2_tinyyolov3_in/,FastoNoSQL,1564653232,[removed],0,1
26,2019-8-1,2019,8,1,19,ckmgxp,Predict data for data set with less columns,https://www.reddit.com/r/MachineLearning/comments/ckmgxp/predict_data_for_data_set_with_less_columns/,akshitsarin,1564654607,"Basically I have two data sets. The first one to train the model on and second one whose final column has to be predicted. The only problem is that the first data set has 10 columns and second one has three. On applying the same model, it gives a dimension error. How do I solve this, if I want to train the model using those 10 features.

Thanks!

im still a newbie!",0,1
27,2019-8-1,2019,8,1,20,ckmw5s,3 Industries Being Transformed by Emotion AI,https://www.reddit.com/r/MachineLearning/comments/ckmw5s/3_industries_being_transformed_by_emotion_ai/,AngelaD1994,1564657452,,0,1
28,2019-8-1,2019,8,1,20,ckmy43,Testing new data on a trained LGBM model,https://www.reddit.com/r/MachineLearning/comments/ckmy43/testing_new_data_on_a_trained_lgbm_model/,varunsingh85,1564657812,[removed],0,1
29,2019-8-1,2019,8,1,20,ckn2if,Classifying Dogs vs Cats problem using Deep Learning in C++!,https://www.reddit.com/r/MachineLearning/comments/ckn2if/classifying_dogs_vs_cats_problem_using_deep/,Kushashwa,1564658557,[removed],0,1
30,2019-8-1,2019,8,1,20,cknb2g,Recommender Systems,https://www.reddit.com/r/MachineLearning/comments/cknb2g/recommender_systems/,HannahHumphreys,1564660069,[removed],0,1
31,2019-8-1,2019,8,1,21,cknsry,How Do You Build A Good Machine Learning Set?,https://www.reddit.com/r/MachineLearning/comments/cknsry/how_do_you_build_a_good_machine_learning_set/,Imarticuslearning,1564662920,[removed],0,1
32,2019-8-1,2019,8,1,22,cko9vg,[P] MachinesGoneWrong - a primer to algorithmic bias,https://www.reddit.com/r/MachineLearning/comments/cko9vg/p_machinesgonewrong_a_primer_to_algorithmic_bias/,greentfrapp,1564665515,"Hey all, am a graduate student working on AI and AI ethics. As part of a 3-month final project, I built an online primer/beginner's guide to algorithmic bias. It contains:

\- xkcd-style comics (a tribute and thanks to the esteemed Randall Munroe!)

\- an explorable for fairness definitions

\- a Bongo Cat

https://i.redd.it/co19wfzb7ud31.gif

**Check it out here:**  [**https://machinesgonewrong.com**](https://machinesgonewrong.com)

(A few of my friends got a 403 error when trying to access it - if that happens to you, try this link instead: [https://greentfrapp.github.io/project-asimov/guide/](https://greentfrapp.github.io/project-asimov/guide/) ; also do let me know if anyone has a fix, I've been stuck on this bug for awhile now - if anyone is interested in helping, the website is built on GitHub Pages with Jekyll)

I'm still about two weeks away from submission so let me know if you have any feedback (I've previously posted this on r/AIethics and r/artificial)! I'm also intending to continue this as a long-term project so I'm open to collaborations on this, just drop me a message!",26,19
33,2019-8-1,2019,8,1,22,ckob2t,Global Industrial Lasers Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/ckob2t/global_industrial_lasers_market_report_2019/,jadhavni3,1564665693,[removed],1,1
34,2019-8-1,2019,8,1,22,ckohbj,Global Industrial Washing Machines Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/ckohbj/global_industrial_washing_machines_market_report/,jadhavni3,1564666621,[removed],1,1
35,2019-8-1,2019,8,1,22,ckolpz,Proof of concepts #6 nunchaku patterns,https://www.reddit.com/r/MachineLearning/comments/ckolpz/proof_of_concepts_6_nunchaku_patterns/,thetrickshotone,1564667273,,0,1
36,2019-8-1,2019,8,1,22,ckop41,Online courses for YOLO and Posenet...,https://www.reddit.com/r/MachineLearning/comments/ckop41/online_courses_for_yolo_and_posenet/,eco_bach,1564667759,[removed],0,1
37,2019-8-1,2019,8,1,23,ckor30,[P] Complete Guide - Design/Assembly/Programming an Arduino Based Neural Network Robot,https://www.reddit.com/r/MachineLearning/comments/ckor30/p_complete_guide_designassemblyprogramming_an/,seanhodgins,1564668052,,0,1
38,2019-8-1,2019,8,1,23,ckorh1,Contextual Emotion Detection in Textual Conversations Using Neural Networks,https://www.reddit.com/r/MachineLearning/comments/ckorh1/contextual_emotion_detection_in_textual/,atomlib_com,1564668107,,0,1
39,2019-8-1,2019,8,1,23,ckoslv,Neural Architecture Search : DARTS vs 'Searching for a Robust Neural Architecture in Four GPU Hours',https://www.reddit.com/r/MachineLearning/comments/ckoslv/neural_architecture_search_darts_vs_searching_for/,fox_blue,1564668252,[removed],0,1
40,2019-8-1,2019,8,1,23,ckoud7,[P] DeepMind: Using AI to give doctors a 48-hour head start on life-threatening illness,https://www.reddit.com/r/MachineLearning/comments/ckoud7/p_deepmind_using_ai_to_give_doctors_a_48hour_head/,carot_cake,1564668480,Very interesting article and paper: [https://deepmind.com/blog/predicting-patient-deterioration/](https://deepmind.com/blog/predicting-patient-deterioration/),55,243
41,2019-8-1,2019,8,1,23,ckoykn,"For those of you that are unfamiliar with Keras, here is a great video-introduction that explains exactly what it is.",https://www.reddit.com/r/MachineLearning/comments/ckoykn/for_those_of_you_that_are_unfamiliar_with_keras/,antaloaalonso,1564669055,,0,1
42,2019-8-1,2019,8,1,23,ckp4o2,[D] Is there a way to convert RAM states to frames in Atari games?,https://www.reddit.com/r/MachineLearning/comments/ckp4o2/d_is_there_a_way_to_convert_ram_states_to_frames/,gohu_cd,1564669895,"How do I find the corresponding frame of, let's say, Pacman, starting from a RAM state ?",9,0
43,2019-8-1,2019,8,1,23,ckp796,[R] Invitation to join an AI Competition: Reconnaissance Blind Chess (NeurIPS 2019) - AI under Uncertainty,https://www.reddit.com/r/MachineLearning/comments/ckp796/r_invitation_to_join_an_ai_competition/,rwgardner,1564670238,,0,1
44,2019-8-2,2019,8,2,0,ckpvqw,ACL 2019 | Best Papers Announced,https://www.reddit.com/r/MachineLearning/comments/ckpvqw/acl_2019_best_papers_announced/,Yuqing7,1564673429,,0,1
45,2019-8-2,2019,8,2,0,ckpzsl,Looking for some guidance regarding grad school,https://www.reddit.com/r/MachineLearning/comments/ckpzsl/looking_for_some_guidance_regarding_grad_school/,undergrad_with_angst,1564673957,[removed],0,1
46,2019-8-2,2019,8,2,0,ckq309,Paper Review Call 018 - HRNet,https://www.reddit.com/r/MachineLearning/comments/ckq309/paper_review_call_018_hrnet/,timscarfe,1564674390,[removed],0,1
47,2019-8-2,2019,8,2,0,ckq8hz,[D] What's your favorite way to learn ML theory?,https://www.reddit.com/r/MachineLearning/comments/ckq8hz/d_whats_your_favorite_way_to_learn_ml_theory/,korbit_ai,1564675084,[removed],0,1
48,2019-8-2,2019,8,2,1,ckqqxn,"[P] Want to build your own Machine Learning Robot? Here is a complete guide - design, assembly, and programming an Arduino Neural Network Robot.",https://www.reddit.com/r/MachineLearning/comments/ckqqxn/p_want_to_build_your_own_machine_learning_robot/,seanhodgins,1564677354,,0,1
49,2019-8-2,2019,8,2,1,ckqvph,What is the difference between LDA and LSA?,https://www.reddit.com/r/MachineLearning/comments/ckqvph/what_is_the_difference_between_lda_and_lsa/,[deleted],1564677936,,0,1
50,2019-8-2,2019,8,2,1,ckqz52,[R] Contextual Emotion Detection in Textual Conversations Using Neural Networks,https://www.reddit.com/r/MachineLearning/comments/ckqz52/r_contextual_emotion_detection_in_textual/,pvl18,1564678379,"Nowadays, talking to conversational agents is becoming a daily routine, and it is crucial for dialogue systems to generate responses as human-like as possible. As one of the main aspects, primary attention should be given to providing emotionally aware responses to users. In this article, we are going to describe **the recurrent neural network architecture for emotion detection in textual conversations**, that participated in [SemEval-2019 Task 3 EmoContext](http://alt.qcri.org/semeval2019/index.php?id=tasks), that is, an annual workshop on semantic evaluation. The task objective is to classify emotion (i.e. happy, sad, angry, and others) in a 3-turn conversational data set.

The rest of the article is organized as follows. Section 1 gives a brief overview of the EmoContext task and the provided data. Sections 2 and 3 focus on the texts pre-processing and word embeddings, consequently. In section 4, we described the architecture of the LSTM model used in our submission. In conclusion, the final performance of our system and the source code are presented. The model is implemented in Python using Keras library.

[https://habr.com/en/company/mailru/blog/439850/](https://habr.com/ru/company/mailru/blog/439850/)",0,14
51,2019-8-2,2019,8,2,1,ckr09t,5 steps to become a good data scientist !,https://www.reddit.com/r/MachineLearning/comments/ckr09t/5_steps_to_become_a_good_data_scientist/,sajad-52,1564678520,[removed],0,1
52,2019-8-2,2019,8,2,1,ckr1af,Real-time or online machine learning projects for senior design,https://www.reddit.com/r/MachineLearning/comments/ckr1af/realtime_or_online_machine_learning_projects_for/,learn_deep,1564678644,[removed],0,1
53,2019-8-2,2019,8,2,2,ckr33m,Current SOTA for topic modelling,https://www.reddit.com/r/MachineLearning/comments/ckr33m/current_sota_for_topic_modelling/,ginger_beer_m,1564678879,[removed],0,1
54,2019-8-2,2019,8,2,2,ckr3qa,What is Machine Learning? Why Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/ckr3qa/what_is_machine_learning_why_machine_learning/,andrea_manero,1564678951,[removed],0,1
55,2019-8-2,2019,8,2,2,ckr88k,Is buying an ultrabook for machine learning a bad idea?,https://www.reddit.com/r/MachineLearning/comments/ckr88k/is_buying_an_ultrabook_for_machine_learning_a_bad/,jamiliio,1564679489,[removed],0,1
56,2019-8-2,2019,8,2,2,ckrf40,[P] 700x faster Node2Vec embeddings by CSR graph representation,https://www.reddit.com/r/MachineLearning/comments/ckrf40/p_700x_faster_node2vec_embeddings_by_csr_graph/,VodkaHaze,1564680351,"[Blog post here](https://www.singlelunch.com/2019/08/01/700x-faster-node2vec-models-fastest-random-walks-on-a-graph/)

[Code here](https://github.com/VHRanger/graph2vec)

I recently rewrote node2vec, which took a severely long time to generate random walks on a graph, by representing the graph as a CSR sparse matrix, and operating directly on the sparse matrix's data arrays.

The result is a speedup from 30 hours to 3 minutes for a small sized graph (nodes and edges in the hundreds of thousands). 

This raises bigger questions about graph representation for graph analytics -- representing graphs as sparse matrices prevents node insertion, but makes operations much more efficient (though admitedly harder to write). More importantly, we can hold fairly huge graphs in RAM because the data usage is so lean.


If we're analyzing graphs, we don't care so much about adding nodes, so I think the future of graph analytics is in CSR representation.",29,28
57,2019-8-2,2019,8,2,2,ckro9y,CNNs are Globally Optimal Given Multi-Layer Support,https://www.reddit.com/r/MachineLearning/comments/ckro9y/cnns_are_globally_optimal_given_multilayer_support/,[deleted],1564681519,,0,1
58,2019-8-2,2019,8,2,2,ckrt1t,CNNs are Globally Optimal Given Multi-Layer Support,https://www.reddit.com/r/MachineLearning/comments/ckrt1t/cnns_are_globally_optimal_given_multilayer_support/,yachong24,1564682118,[removed],0,1
59,2019-8-2,2019,8,2,3,ckry1e,[D] What is the difference between LDA and LSA?,https://www.reddit.com/r/MachineLearning/comments/ckry1e/d_what_is_the_difference_between_lda_and_lsa/,ashrithracharla,1564682750,Can someone help me understand the differences between LDA (Latent Dirichlet Allocation) and LSA ( Latent Sematic Analysis)? What are the scenarios where both can be used? When do we choose one over the other?,4,3
60,2019-8-2,2019,8,2,3,ckry5c,Stages of Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/ckry5c/stages_of_artificial_intelligence/,[deleted],1564682763,[deleted],0,1
61,2019-8-2,2019,8,2,3,cks2y3,Stages of Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/cks2y3/stages_of_artificial_intelligence/,shubhwadekar,1564683365,[removed],0,1
62,2019-8-2,2019,8,2,3,cks3ul,[HIRING][IN-PERSON] Machine Learning Professionals $500/2hr #paid #interview,https://www.reddit.com/r/MachineLearning/comments/cks3ul/hiringinperson_machine_learning_professionals/,UErecruiting,1564683477,[removed],0,0
63,2019-8-2,2019,8,2,3,cks3vu,3d Game engine with machine learning bindings for tensorflow,https://www.reddit.com/r/MachineLearning/comments/cks3vu/3d_game_engine_with_machine_learning_bindings_for/,Stanley_C,1564683480,[removed],0,1
64,2019-8-2,2019,8,2,3,cks5ia,Can anyone help with this ad recommender system project?,https://www.reddit.com/r/MachineLearning/comments/cks5ia/can_anyone_help_with_this_ad_recommender_system/,vryoung2,1564683675,[removed],0,1
65,2019-8-2,2019,8,2,4,ckstly,Kaggle 1st Pos Solution of Data Science for Good: City of LA Kaggle Comp | Interview with Shivam Bansal,https://www.reddit.com/r/MachineLearning/comments/ckstly/kaggle_1st_pos_solution_of_data_science_for_good/,init__27,1564686655,[removed],0,1
66,2019-8-2,2019,8,2,4,cksvcr,Keras - CNN Model with Embedding Layer Prediction not correct !,https://www.reddit.com/r/MachineLearning/comments/cksvcr/keras_cnn_model_with_embedding_layer_prediction/,Troied,1564686868,[removed],0,1
67,2019-8-2,2019,8,2,4,cksznf,Implement Your Own Source To Source AD in ONE day!,https://www.reddit.com/r/MachineLearning/comments/cksznf/implement_your_own_source_to_source_ad_in_one_day/,Bdamkin54,1564687408,,0,1
68,2019-8-2,2019,8,2,4,ckt106,Robot Masters String Puppetry,https://www.reddit.com/r/MachineLearning/comments/ckt106/robot_masters_string_puppetry/,Yuqing7,1564687580,,0,1
69,2019-8-2,2019,8,2,4,ckt27w,[Discussion] Suggestions for organizing related work research,https://www.reddit.com/r/MachineLearning/comments/ckt27w/discussion_suggestions_for_organizing_related/,HeavyStatus4,1564687737,"Hello Everyone,

I am looking for suggestions to organize my related work research. It will be great if the method (or system) can give the ability to organize the related work in a web-based tool, where I can keep pdf's , annotate them online (as well as offline), add comments, write summaries next to them. It will be great if I can share these features with my collaborators as well.

I had been trying to use Trello and  Github's Project Management Tool ( along with ""Issues"" for comments) for this. 

It will be helpful if others can share how they organize their research.

Thanks,

Anurag Koul",5,6
70,2019-8-2,2019,8,2,4,ckt8nf,How to choose the number of hidden layers and nodes in a feedforward neural network?,https://www.reddit.com/r/MachineLearning/comments/ckt8nf/how_to_choose_the_number_of_hidden_layers_and/,Stormtrooper_TK,1564688543,[removed],0,1
71,2019-8-2,2019,8,2,4,cktajn,[P] Implementing a SumOfGaussians layer in Keras2.0,https://www.reddit.com/r/MachineLearning/comments/cktajn/p_implementing_a_sumofgaussians_layer_in_keras20/,zachmoshe,1564688787,"Following is my new blog post. This time I played a bit with the new beta version of TF and implemented a simple model where y is the sum of K gaussians which parameters are learned.

[http://zachmoshe.com/2019/08/01/sum-of-gaussians-layer-with-keras-2.0.html](http://zachmoshe.com/2019/08/01/sum-of-gaussians-layer-with-keras-2.0.html)",2,6
72,2019-8-2,2019,8,2,5,cku276,[N] Facebook AI leads in 2019 WMT international machine translation competition,https://www.reddit.com/r/MachineLearning/comments/cku276/n_facebook_ai_leads_in_2019_wmt_international/,arnoseris,1564692328,,0,1
73,2019-8-2,2019,8,2,6,ckuah8,Visual intuition on ring-Allreduce,https://www.reddit.com/r/MachineLearning/comments/ckuah8/visual_intuition_on_ringallreduce/,edirgl,1564693393,[removed],0,1
74,2019-8-2,2019,8,2,6,ckub5m,Machine Learning Approach for Aerodynamic Design?,https://www.reddit.com/r/MachineLearning/comments/ckub5m/machine_learning_approach_for_aerodynamic_design/,globoxer,1564693484,[removed],0,1
75,2019-8-2,2019,8,2,6,ckueu6,A beginner's path.,https://www.reddit.com/r/MachineLearning/comments/ckueu6/a_beginners_path/,sweaze97,1564693953,[removed],0,1
76,2019-8-2,2019,8,2,6,ckuyod,Computational Government,https://www.reddit.com/r/MachineLearning/comments/ckuyod/computational_government/,PXaZ,1564696574,[removed],0,1
77,2019-8-2,2019,8,2,7,ckv1l6,[P] Does anyone know of anywhere I can find good data regarding addiction treatment?,https://www.reddit.com/r/MachineLearning/comments/ckv1l6/p_does_anyone_know_of_anywhere_i_can_find_good/,that_one_ai_nerd,1564696963,"I want to do a machine learning project to gain insight into and then perhaps even be able to contribute ideas to the area of addiction treatment, something I think is extremely important for society. In order to begin, I have been scouring the Internet for places where I can gather data for building datasets, and maybe even existing datasets Ill be able to utilize. 

So, in this spirit, I was wondering if anyone else has any suggestions or knowledge of where I can gather data that has to do with addiction treatment, or maybe even existing datasets about the subject? Please let me know if you do - itd be immensely helpful!

P.S. - If you are interested in using some of your free time to help me out with the project, feel free to message me. Especially now, during the data sourcing part of the project, its the best part! (Lmao Im obviously kidding I hate this part, but your help would actually be seriously awesome)",6,3
78,2019-8-2,2019,8,2,7,ckv5yl,"A group of researchers from ETH Zurich has introduced a robotic system that can perform animation of real-world string puppets, aka marionettes.",https://www.reddit.com/r/MachineLearning/comments/ckv5yl/a_group_of_researchers_from_eth_zurich_has/,trcytony,1564697529,,0,1
79,2019-8-2,2019,8,2,7,ckvjvb,[P] Facebook AI leads in 2019 WMT international machine translation competition,https://www.reddit.com/r/MachineLearning/comments/ckvjvb/p_facebook_ai_leads_in_2019_wmt_international/,arnoseris,1564699387,"[Blog post here](https://ai.facebook.com/blog/facebook-leads-wmt-translation-competition/)

[Code and models here](https://github.com/pytorch/fairseq/tree/master/examples/wmt19)

[Paper here](https://arxiv.org/pdf/1907.06616.pdf)

Facebook AI models achieved first place in several language tasks included in this years annual news translation competition, hosted by the [Fourth Conference on Machine Translation](https://l.facebook.com/l.php?u=http%3A%2F%2Fwww.statmt.org%2Fwmt19%2F&amp;h=AT0apOn-3OGHr-O4mYkzoYTfjK3guVZ41nsaUn8N9l8mO_ui7IFsjuwY8SBX_i5gGPRG8y-5vo6Tx0RHm4y-C3MS1Lux42wUH88gy-uqC4Vd933_DlK5esMhRVURRtKYwq1fNb9bOqgkJ5bVElJa0Rnp) (also known as WMT). Our models outperformed all other entrants models in the four tasks we participated in, including English to German, the most competitive task in the contest, with entries drawn from a wide range of high-performing research teams. For this language direction, our translations have been declared superhuman by the WMT organizers, meaning that human evaluators preferred them over translations done by human experts.",0,1
80,2019-8-2,2019,8,2,7,ckvo98,[R] Facebook AI leads in 2019 WMT international machine translation competition,https://www.reddit.com/r/MachineLearning/comments/ckvo98/r_facebook_ai_leads_in_2019_wmt_international/,arnoseris,1564699991,[removed],0,1
81,2019-8-2,2019,8,2,8,ckvs63,Test error lower than training after finetuning,https://www.reddit.com/r/MachineLearning/comments/ckvs63/test_error_lower_than_training_after_finetuning/,nikogamulin,1564700541,[removed],0,1
82,2019-8-2,2019,8,2,8,ckw1ii,ML model security,https://www.reddit.com/r/MachineLearning/comments/ckw1ii/ml_model_security/,dacws,1564701833,[removed],0,1
83,2019-8-2,2019,8,2,8,ckw1xv,"[R] A group of researchers from ETH Zurich has introduced a robotic system that can perform animation of real-world string puppets, aka marionettes",https://www.reddit.com/r/MachineLearning/comments/ckw1xv/r_a_group_of_researchers_from_eth_zurich_has/,trcytony,1564701898,,0,1
84,2019-8-2,2019,8,2,10,ckxarf,Using machine learning to diagnose lethal sickness early,https://www.reddit.com/r/MachineLearning/comments/ckxarf/using_machine_learning_to_diagnose_lethal/,mannameofbinkie,1564708524,,0,1
85,2019-8-2,2019,8,2,10,ckxe42,Jetson Nano: Vision Recognition Neural Network Demo,https://www.reddit.com/r/MachineLearning/comments/ckxe42/jetson_nano_vision_recognition_neural_network_demo/,dwcrmcm,1564709042,,0,1
86,2019-8-2,2019,8,2,10,ckxq4c,"audio semantic hashing, multi-instance learning, misc. autoencoders",https://www.reddit.com/r/MachineLearning/comments/ckxq4c/audio_semantic_hashing_multiinstance_learning/,sz524,1564710878,"[https://github.com/4d55397500/tensorflow-implementations](https://github.com/4d55397500/tensorflow-implementations)

A few reference miscellaneous Tensorflow implementations. May be pedagogically useful or a stepping stone to more complex applied projects

\- semantic hashing (old school by now) of audio

\- multi-instance learning: learning off labels on collections of inputs

\- misc. autoencoders: single-layer autoencoder equivalent to principal component analysis",0,1
87,2019-8-2,2019,8,2,11,cky39n,Cuff Type Shrink Wrapping Film Packing Machine for Bottles Packing Equip...,https://www.reddit.com/r/MachineLearning/comments/cky39n/cuff_type_shrink_wrapping_film_packing_machine/,amygao1984,1564712942,,0,1
88,2019-8-2,2019,8,2,12,ckyml0,Bang for your buck on AWS,https://www.reddit.com/r/MachineLearning/comments/ckyml0/bang_for_your_buck_on_aws/,shameless_chicken,1564716001,[removed],0,1
89,2019-8-2,2019,8,2,12,ckyrtg,Best Machine Learning Course,https://www.reddit.com/r/MachineLearning/comments/ckyrtg/best_machine_learning_course/,dwivediabhinav,1564716860,[removed],0,1
90,2019-8-2,2019,8,2,13,ckz6re,How do digital cameras convert photons to pixel values?,https://www.reddit.com/r/MachineLearning/comments/ckz6re/how_do_digital_cameras_convert_photons_to_pixel/,DaBobcat,1564719326,[removed],0,1
91,2019-8-2,2019,8,2,13,ckzgvg,General artificial intelligence,https://www.reddit.com/r/MachineLearning/comments/ckzgvg/general_artificial_intelligence/,xHipster,1564721119,,0,1
92,2019-8-2,2019,8,2,14,ckzuhf,How useful is Google's TensorFlow?,https://www.reddit.com/r/MachineLearning/comments/ckzuhf/how_useful_is_googles_tensorflow/,DvelDeveloper,1564723516,[removed],0,1
93,2019-8-2,2019,8,2,14,cl02jy,Want to increase your sales?,https://www.reddit.com/r/MachineLearning/comments/cl02jy/want_to_increase_your_sales/,getengati,1564724989,[removed],0,1
94,2019-8-2,2019,8,2,15,cl05xy,AI and Climate Change - Our best hope,https://www.reddit.com/r/MachineLearning/comments/cl05xy/ai_and_climate_change_our_best_hope/,craigspencersmith,1564725617,,0,1
95,2019-8-2,2019,8,2,15,cl0evw,[Discussion][D]Food Industry Machines That Are At Another Level  7,https://www.reddit.com/r/MachineLearning/comments/cl0evw/discussiondfood_industry_machines_that_are_at/,GoGadegets,1564727326,,0,1
96,2019-8-2,2019,8,2,15,cl0gjj,AI empowers doctors to detect kidney related issues in advance,https://www.reddit.com/r/MachineLearning/comments/cl0gjj/ai_empowers_doctors_to_detect_kidney_related/,aismartz,1564727657,[removed],0,1
97,2019-8-2,2019,8,2,15,cl0myr,IDENTITY PREDICTION FOR UNKNOWN USERS OF AN ONLINE SYSTEM,https://www.reddit.com/r/MachineLearning/comments/cl0myr/identity_prediction_for_unknown_users_of_an/,Sukh_AE,1564728955,[removed],0,1
98,2019-8-2,2019,8,2,16,cl0o6t,An interesting graph showing relationships between 2 variables and their exponents,https://www.reddit.com/r/MachineLearning/comments/cl0o6t/an_interesting_graph_showing_relationships/,emmawhitner,1564729215,,0,1
99,2019-8-2,2019,8,2,16,cl0rts,How would I go about creating this recommendation model? Need someone to set me in the right direction.,https://www.reddit.com/r/MachineLearning/comments/cl0rts/how_would_i_go_about_creating_this_recommendation/,Ma5terVain,1564729905,[removed],0,1
100,2019-8-2,2019,8,2,16,cl0uyu,"Nvidia, Image Inpainting/restoration = Watermarks is dead?",https://www.reddit.com/r/MachineLearning/comments/cl0uyu/nvidia_image_inpaintingrestoration_watermarks_is/,samsamsamrox1212,1564730504,[removed],0,1
101,2019-8-2,2019,8,2,16,cl0x08,"Catherine Schuman, who works in ORNLs Computer Science and Mathematics Division, received funding for her proposal",https://www.reddit.com/r/MachineLearning/comments/cl0x08/catherine_schuman_who_works_in_ornls_computer/,arunragini,1564730934,,0,1
102,2019-8-2,2019,8,2,16,cl0xsu,Nvidia Image Restoration/Impainting = Bye! Bye! Watermarks,https://www.reddit.com/r/MachineLearning/comments/cl0xsu/nvidia_image_restorationimpainting_bye_bye/,samsamsamrox1212,1564731112,[removed],0,1
103,2019-8-2,2019,8,2,16,cl0y7e,[P] Open-source Image Annotation Tool (with ML assist),https://www.reddit.com/r/MachineLearning/comments/cl0y7e/p_opensource_image_annotation_tool_with_ml_assist/,imslavko,1564731186,,0,2
104,2019-8-2,2019,8,2,16,cl1203,Unsupervised Domain Adaptation via Disentangled Representations: Application to Cross-Modality Liver Segmentation,https://www.reddit.com/r/MachineLearning/comments/cl1203/unsupervised_domain_adaptation_via_disentangled/,junlin639,1564732005,,7,15
105,2019-8-2,2019,8,2,17,cl19z7,[D] What is the state of the art approach for real-time dynamic gesture recognition?,https://www.reddit.com/r/MachineLearning/comments/cl19z7/d_what_is_the_state_of_the_art_approach_for/,LessTell,1564733728,So I'm going to work on a research project and was asked to look at the state of the art works on real-time gesture recognition. What are some of the latest works that achieve state of the art performance?,21,2
106,2019-8-2,2019,8,2,17,cl1kzt,Practical Deep Learning For Coders online course video lectures by Other,https://www.reddit.com/r/MachineLearning/comments/cl1kzt/practical_deep_learning_for_coders_online_course/,priyaleo,1564736177,,0,1
107,2019-8-2,2019,8,2,18,cl1msj,[D] What exploration-exploitation strategy do you employ as a human navigating ML?,https://www.reddit.com/r/MachineLearning/comments/cl1msj/d_what_explorationexploitation_strategy_do_you/,AbitofAsum,1564736579,"For me it seems e-greedy with a high alpha, as in reading lots of random abstracts or wikis or blogs, feels the most appropriate since I am getting a very low reward signal for my long term goals. 

Has anyone put much thought into their process of acquiring new information and how deeply they need to go on a topic before they can rate its relative importance? And further, the risk of bias causing a dismissal of fruitful lines of inquiry?

The meta aspect of learning is fascinating to me and I am curious which modes of thought work best for people.",5,0
108,2019-8-2,2019,8,2,18,cl1ouc,Look Again at the Syntax: Relational Graph Convolutional Network for Gendered Ambiguous Pronoun Resolution,https://www.reddit.com/r/MachineLearning/comments/cl1ouc/look_again_at_the_syntax_relational_graph/,junlin639,1564736998,,12,15
109,2019-8-2,2019,8,2,18,cl1umc,Mac support number 1-855-629-2074 USA,https://www.reddit.com/r/MachineLearning/comments/cl1umc/mac_support_number_18556292074_usa/,lisarichardusa,1564738209,[removed],0,1
110,2019-8-2,2019,8,2,19,cl24fm,Viewers request Friday freestyle a couple of aerials,https://www.reddit.com/r/MachineLearning/comments/cl24fm/viewers_request_friday_freestyle_a_couple_of/,thetrickshotone,1564740268,,0,1
111,2019-8-2,2019,8,2,19,cl2964,Generative adversarial neural network to predict trends or prices,https://www.reddit.com/r/MachineLearning/comments/cl2964/generative_adversarial_neural_network_to_predict/,LaurensWissels,1564741169,[removed],0,1
112,2019-8-2,2019,8,2,19,cl2a4o,How Facebook Uses Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cl2a4o/how_facebook_uses_machine_learning/,madhu_SEO,1564741353,,0,1
113,2019-8-2,2019,8,2,20,cl2to4,Best Machine Learning Course in Delhi | Machine Learning Certification in Delhi,https://www.reddit.com/r/MachineLearning/comments/cl2to4/best_machine_learning_course_in_delhi_machine/,dtacademy,1564745053,[removed],0,1
114,2019-8-2,2019,8,2,21,cl39tn,"[D] TRAINS - one month later. We got some *real* nice feedback from r/ML, here is what we did since then.",https://www.reddit.com/r/MachineLearning/comments/cl39tn/d_trains_one_month_later_we_got_some_real_nice/,LSTMeow,1564747835,"Hi Everyone,

Context: Our previous [\[N\]ews post](https://www.reddit.com/r/MachineLearning/comments/c2g2li/n_there_are_many_platforms_to_manage_your_ml/)

Since the response was good, I thought you will have something to say about the [first Medium post about TRAINS](https://medium.com/@allegroai/trains-the-maiden-voyage-e099dd003cf) 

Specifically, I am looking for more feedback and feature requests from actual or would-be users. 

PS: I am aware we do not have a comparison matrix set up yet, in the meantime you can go over the feature list in the medium post, they are quite comprehensive...  
PS/2:  And as before, only two lines of integration code :)",6,13
115,2019-8-2,2019,8,2,21,cl39wl,AI ChatBot for Healthcare,https://www.reddit.com/r/MachineLearning/comments/cl39wl/ai_chatbot_for_healthcare/,Mycarebee,1564747847," 

Artificial Intelligence Chatbots-The perfect way to handle increased website visitors AI Chatbot is not hype but it is one way to improve patients experience and revolutionize the healthcare world.

The healthcare industry is evolving rapidly with a large volume of data and increasing website visitors. Early adopters of Artificial intelligence in the healthcare space are reaping the benefit in terms of patient engagement and reducing long waiting queues.

Customized AI Chatbot not only handles many visitors at a time but can help you to automate many processes like book appointments, book health checkup and results in saving time and money for the provider. While your staff can focus on other areas of your hospital Our Robot Care Bee will take care of your website visitors.

No longer waiting for chat, no manpower efforts in answering regular queries, all you have to do, contact our team and ease your process.

We also provide

&amp;#x200B;

AI ChatBot for Healthcare

Patient Feedback Software

Telemedicine App Development

Digital marketing and content writing

Website Development and IT Solutions

&amp;#x200B;

Know more: [http://www.mycarebee.com/index.php](http://www.mycarebee.com/index.php)

Video Link: [https://www.youtube.com/watch?v=pv8Dk7L5Vn0&amp;t](https://www.youtube.com/watch?v=pv8Dk7L5Vn0&amp;t)",0,1
116,2019-8-2,2019,8,2,21,cl3cb6,"[P] Complete Guide to Designing, Assembling, and Programming an Arduino Based Neural Network Robot (45 Minute Video)",https://www.reddit.com/r/MachineLearning/comments/cl3cb6/p_complete_guide_to_designing_assembling_and/,seanhodgins,1564748256,"I had a really good response to my video over at /r/learnmachinelearning (https://www.reddit.com/r/learnmachinelearning/comments/ckoy0k/want_to_build_your_own_machine_learning_robot/) so I thought I would share it here. Its a video guide that will help you build your first machine learning robot. It starts from basic breadboard prototyping all the way to PCB design, surface mount soldering, and finally programming. It uses an ARM microcontroller and an Arduino bootloader. The finished robot is designed to avoid light and a Neural Network is what controls the motors. It also fits in the palm of your hand (great for classrooms!).  
  
https://youtu.be/wtNaPLmpy1I  
  
I have a few more ML projects coming out over the next couple of months as well. Not as extensive as this, but I think really interesting topics.",15,252
117,2019-8-2,2019,8,2,22,cl3xug,What are the top 10 algorithms every software engineer should know by heart?,https://www.reddit.com/r/MachineLearning/comments/cl3xug/what_are_the_top_10_algorithms_every_software/,gyansetu23,1564751578,,0,1
118,2019-8-2,2019,8,2,22,cl44v0,Need some guidance on making Text To Speech,https://www.reddit.com/r/MachineLearning/comments/cl44v0/need_some_guidance_on_making_text_to_speech/,Riday33,1564752622,[removed],0,1
119,2019-8-2,2019,8,2,22,cl48j7,Global Anchor Windlass Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/cl48j7/global_anchor_windlass_market_report_2019/,jadhavni3,1564753129,[removed],1,1
120,2019-8-2,2019,8,2,22,cl4g9t,[D] use LSTM to predict feature from a pretrained network,https://www.reddit.com/r/MachineLearning/comments/cl4g9t/d_use_lstm_to_predict_feature_from_a_pretrained/,desperate_ano,1564754235,"Hey, I am currently working on the sequential feature prediction using LSTM (the sequential feature can be extracted by a pretrained VGG or the latent space from an autoencoder), so basically the extracted feature is considered as ""ground truth"", and I am using LSTM to predict the future features. 

The feature space is always not constrained to a specific interval, sometimes the values vary between \[-10,50\], but the tanh activation layer in LSTM will constrain the output between \[-1,1\] (If I understand correct?). I have also played with adding fc layer with leakyrelu or deconv layers after the LSTM layer, but the output is still in a much smaller interval, so the scale mismatch between my ground truth feature and predicted latent space make it really hard to train this model. I am wondering has anyone faced this issue before? And do you have some ideas about how to solve it?

Thanks in advance!!",6,1
121,2019-8-2,2019,8,2,23,cl4mct,This Nature article talks about some of the issues people should be aware of while using machine learning.,https://www.reddit.com/r/MachineLearning/comments/cl4mct/this_nature_article_talks_about_some_of_the/,thisCantBeBad,1564755047,,0,1
122,2019-8-3,2019,8,3,0,cl5i7m,"[D] Can someone explain to me how in the reinforcement learning algorthim, A3C, how the multiple workers enusre they won't retrieve the same parameters from the global network they just updated?",https://www.reddit.com/r/MachineLearning/comments/cl5i7m/d_can_someone_explain_to_me_how_in_the/,ml4564,1564759280,"I understand that the multiple workers do gradient update to the global network is done asynchronously in A3C ( [https://arxiv.org/abs/1602.01783](https://arxiv.org/abs/1602.01783) ).

But how do the workers ensure that they won't retrieve the same parameters from the global network they just updated?

Thank you.",2,1
123,2019-8-3,2019,8,3,0,cl5l4p,My notes and codes (Jupyter Notebooks) from Elements of Statistical Learning,https://www.reddit.com/r/MachineLearning/comments/cl5l4p/my_notes_and_codes_jupyter_notebooks_from/,madiyar,1564759661,[removed],0,1
124,2019-8-3,2019,8,3,0,cl5unx,Normalize data belonging to different scales?,https://www.reddit.com/r/MachineLearning/comments/cl5unx/normalize_data_belonging_to_different_scales/,aimldlcv,1564760916,[removed],0,1
125,2019-8-3,2019,8,3,0,cl5vy9,Need advice on how to predict the relevant category.,https://www.reddit.com/r/MachineLearning/comments/cl5vy9/need_advice_on_how_to_predict_the_relevant/,azzipog,1564761067,"So I have a pdf that contains blank lines for things like 'Applicant Date of Birth', 'Emergency contact Phone number', etc. Normal things that you would find in a pdf. I also have a database that contains the fields that are required by these blanks. 

My goal is to scan the pdf, identify the blanks, and predict which field from our database each blank is asking for. 

So far, I have created some code to convert each pdf page into a jpg. Scan the jpg for all horizontal lines. Get the coordinates of all words on the page. So my final dataset is on a line/blank granularity and has data for each word found on the page. For instance one row would be for a single blank that is asking for the applicant's date of birth. Then I would have a column 'Date\_distance'. This would be the Euclidian distance that the word 'Date' is from the blank in question. I would do this for everyone word on the page. From this, I can find the words that are closest to the blanks. I also have columns, for whether that word is capitalized, whether it's on the same line etc. 

So I can identify words that I believe will be most predictive for each blank. Once I have this set of words what would be the best way to tie them back to a specific field?

I've thought about using some sort of supervised learning algorithm (ECOC) to calculate the probability for every field, but this doesn't lend itself well to newly created fields. Additionally, there isn't a ton of training data. I figure a better way would be to get a list of words that are predictive and build some kind of word association model. The problem is that I have very little experience with this kind of work.

So, if anyone has any suggestions, I would be love to hear them.",0,1
126,2019-8-3,2019,8,3,2,cl6vjb,[D] Resources to learn Probabilistic Modelling,https://www.reddit.com/r/MachineLearning/comments/cl6vjb/d_resources_to_learn_probabilistic_modelling/,BlueBoySays,1564765615,I have real basic understanding of probability and statistics. If someone could suggest blogs/courses.,0,1
127,2019-8-3,2019,8,3,2,cl6y5f,Does DBSCAN Supports Incremntal(online) Learning?,https://www.reddit.com/r/MachineLearning/comments/cl6y5f/does_dbscan_supports_incremntalonline_learning/,epicSaitama,1564765951,[removed],0,1
128,2019-8-3,2019,8,3,2,cl6yxr,How to implement a Software Monitoring Machine Learning Pipeline in GCP?,https://www.reddit.com/r/MachineLearning/comments/cl6yxr/how_to_implement_a_software_monitoring_machine/,jvelez2210,1564766053,[removed],0,1
129,2019-8-3,2019,8,3,2,cl6zm6,A writing algorithm that finishes stories,https://www.reddit.com/r/MachineLearning/comments/cl6zm6/a_writing_algorithm_that_finishes_stories/,finphil,1564766143,,0,1
130,2019-8-3,2019,8,3,2,cl75du,"Anyone can learn Machine Learning with this blog, regardless of their educational background",https://www.reddit.com/r/MachineLearning/comments/cl75du/anyone_can_learn_machine_learning_with_this_blog/,DavidCode,1564766883,[removed],119,308
131,2019-8-3,2019,8,3,2,cl79ph,DCT Coefficients into a 1D CNN?,https://www.reddit.com/r/MachineLearning/comments/cl79ph/dct_coefficients_into_a_1d_cnn/,acollins1331,1564767440,[removed],0,1
132,2019-8-3,2019,8,3,3,cl7w20,How often are you updating your models?,https://www.reddit.com/r/MachineLearning/comments/cl7w20/how_often_are_you_updating_your_models/,jennysebastian,1564770269,[removed],0,1
133,2019-8-3,2019,8,3,3,cl80qb,Facebook AI Memory Layer Boosts Network Capacity by a Billion Parameters,https://www.reddit.com/r/MachineLearning/comments/cl80qb/facebook_ai_memory_layer_boosts_network_capacity/,Yuqing7,1564770889,,0,1
134,2019-8-3,2019,8,3,3,cl86fv,How can I setup my own openai dota 2 bot?,https://www.reddit.com/r/MachineLearning/comments/cl86fv/how_can_i_setup_my_own_openai_dota_2_bot/,deama15,1564771648,[removed],0,1
135,2019-8-3,2019,8,3,3,cl8c7m,[R] Invitation to join an AI Competition: Reconnaissance Blind Chess (NeurIPS 2019) - AI under Uncertainty,https://www.reddit.com/r/MachineLearning/comments/cl8c7m/r_invitation_to_join_an_ai_competition/,rwgardner,1564772388,"**We are hosting a fun, online AI competition. Participants create a bot that can play chess, but blind and with the ability to privately sense a 3x3 square of the board each turn! The competition is part of of NeurIPS. Anyone can participate.**

**$1,000 prize.**

**Participants do not need to attend the** [NeurIPS](https://nips.cc) **conference and there is no cost.**

[Play reconnaissance blind chess now.](https://rbc.jhuapl.edu)

**Those interested in the game can now join the subreddit** /r/ReconBlindChess**.**

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

All are invited to participate in an upcoming computer science competition that is being held as part of the 2019 Conference on Neural Information Processing Systems (NeurIPS), Reconnaissance Blind Chess.

Many of the favorite studied games in artificial intelligence (AI) such as checkers, chess, and Go lack something that is common and critical in real-life decision making, uncertainty.

This is a competition with a simple but powerful twist on what may be considered the most classic game in AI history, chess. Reconnaissance Blind Chess (RBC) is like chess except a player cannot see where her opponent's pieces are a priori. Rather, she learns partial information about them with the ability to sense a 3x3 square of the board each turn and from the results of moves.

In comparison to poker, which seems to be the most popularly studied game of imperfect information, RBC includes a critical component of long-term planning. Compared to phantom games like Kriegspiel, in RBC players have much more ability to manage their uncertainty, which we believe makes the game more interesting from an AI perspective and more realistic for most scenarios; players are not completely blind, but rather, metaphorically, they simply cannot look everywhere at once.

Participants are welcome to use any code or libraries available.

For more information on the NeurIPS competition, the game itself, or the API, or to play the game to get a feel for it, visit our website below.

All are welcome to create the best RBC bot they can at no cost, and see how well it can play against other bots in the tournament starting on October 21, 2019!

&amp;#x200B;

[https://rbc.jhuapl.edu](https://rbc.jhuapl.edu)

&amp;#x200B;

(on twitter: [https://twitter.com/ryan\_w\_gardner/status/1151911206019567617](https://twitter.com/ryan_w_gardner/status/1151911206019567617) )

https://i.redd.it/gic7yr2u13e31.png

(Apologies for the mostly repeated post.  I wanted to make sure people were aware and note the new subreddit.)",0,1
136,2019-8-3,2019,8,3,4,cl8kpi,"[News] An alerting tool developed in cooperation with the Google company DeepMind to speed up the diagnosis of acute kidney injury has shown no clinical benefits when it was compared with normal care, a team has concluded",https://www.reddit.com/r/MachineLearning/comments/cl8kpi/news_an_alerting_tool_developed_in_cooperation/,Haussian,1564773497,,0,1
137,2019-8-3,2019,8,3,4,cl91nt,Would you outsource ML model training?,https://www.reddit.com/r/MachineLearning/comments/cl91nt/would_you_outsource_ml_model_training/,jennysebastian,1564775733,[removed],0,1
138,2019-8-3,2019,8,3,4,cl92x8,[P] spaCy PyTorch Transformers,https://www.reddit.com/r/MachineLearning/comments/cl92x8/p_spacy_pytorch_transformers/,syllogism_,1564775897,,0,2
139,2019-8-3,2019,8,3,5,cl9jod,Play reconnaissance blind chess now.,https://www.reddit.com/r/MachineLearning/comments/cl9jod/play_reconnaissance_blind_chess_now/,rwgardner,1564778104,[removed],0,1
140,2019-8-3,2019,8,3,5,cl9lbf,Invitation to join an AI Competition: Reconnaissance Blind Chess (NeurIPS 2019) - AI under Uncertainty,https://www.reddit.com/r/MachineLearning/comments/cl9lbf/invitation_to_join_an_ai_competition/,rwgardner,1564778329,[removed],0,1
141,2019-8-3,2019,8,3,5,cl9ljk,Research Papers in ML,https://www.reddit.com/r/MachineLearning/comments/cl9ljk/research_papers_in_ml/,saivicky2015,1564778362,[removed],0,1
142,2019-8-3,2019,8,3,5,cl9p5x,How much did AlphaGo Zero cost?,https://www.reddit.com/r/MachineLearning/comments/cl9p5x/how_much_did_alphago_zero_cost/,SoulDrivenOlives,1564778868,[removed],0,1
143,2019-8-3,2019,8,3,7,clayra,PROFESSIONAL HACKER FOR HIRE,https://www.reddit.com/r/MachineLearning/comments/clayra/professional_hacker_for_hire/,stephen-mark,1564785090,[removed],0,1
144,2019-8-3,2019,8,3,7,clb5u0,Looking for feedback on where I should be applying for grad school this fall,https://www.reddit.com/r/MachineLearning/comments/clb5u0/looking_for_feedback_on_where_i_should_be/,Turings_Ego,1564786120,[removed],0,1
145,2019-8-3,2019,8,3,7,clb84a,Machine Learning for Good Projects to Boost Your Skills While Making an Impact,https://www.reddit.com/r/MachineLearning/comments/clb84a/machine_learning_for_good_projects_to_boost_your/,Lordobba,1564786455,,0,1
146,2019-8-3,2019,8,3,8,clbbp0,Are Docker and amazon SageMaker good skills to learn for ML Engineer?,https://www.reddit.com/r/MachineLearning/comments/clbbp0/are_docker_and_amazon_sagemaker_good_skills_to/,uuumeme,1564786975,[removed],0,1
147,2019-8-3,2019,8,3,8,clbxqm,PROFESSIONAL HACKER FOR HIRE,https://www.reddit.com/r/MachineLearning/comments/clbxqm/professional_hacker_for_hire/,Allay-,1564790334,"Hello guys, do you need a competent, highly-skilled and experienced Hacker? 

*STEPHEN MARK* specializes in all kinds and forms of Hacking services. We hack seemingly inaccessible and impenetrable accounts on any/all social media platforms you can ever imagine i.e. Instagram, Twitter, Facebook, Snapchat. We also hack all electronic mails, such as, Yahoo, Gmail, AOL etc. 

You can as well trust us in hacking Grades, Credit Scores, Websites, and Database of any kind. 

We also retrieve deleted text messages, lost files/documents, control devices remotely and lots more. 

Our hacking capabilities and speciality knows know bound as we effortlessly hack mobile phones, its location detection, just to mention a few. 

We have a 100% track record. We also have the highest return hire rate and we promise not to let you down - that's not even an option, and that's simply because we have a reputation to protect as our past works speak for themselves. 

A trial will convince you. 

For more enquiries, contact us at (write your mobile number/mail address)Stephenmark068@gmail.com or 2136004439",0,1
148,2019-8-3,2019,8,3,12,cle5xa,Threadripper 2920X vs i7-9800X (can't decide),https://www.reddit.com/r/MachineLearning/comments/cle5xa/threadripper_2920x_vs_i79800x_cant_decide/,th1nkpatriot,1564803490,[removed],0,1
149,2019-8-3,2019,8,3,13,clelmn,"[P] (Facebook AI) Advances in Conversational AI: ""We've made scientific advancements in 5 areas that open-domain chatbots fail in today: specificity, consistency, empathy, knowledgeability, and multimodal understanding""",https://www.reddit.com/r/MachineLearning/comments/clelmn/p_facebook_ai_advances_in_conversational_ai_weve/,Corp-Por,1564806230,,0,1
150,2019-8-3,2019,8,3,13,clerwe,Confused about tacotron,https://www.reddit.com/r/MachineLearning/comments/clerwe/confused_about_tacotron/,EternalXV,1564807384,[removed],0,1
151,2019-8-3,2019,8,3,14,cleywq,Future Face Prediction,https://www.reddit.com/r/MachineLearning/comments/cleywq/future_face_prediction/,nani_procastinator,1564808743,[removed],0,1
152,2019-8-3,2019,8,3,14,clfedg,Need help selecting GPUs for deep learning,https://www.reddit.com/r/MachineLearning/comments/clfedg/need_help_selecting_gpus_for_deep_learning/,th1nkpatriot,1564811829,[removed],0,1
153,2019-8-3,2019,8,3,15,clfo3z,[R] pytorch-lightning - The researcher's version of keras,https://www.reddit.com/r/MachineLearning/comments/clfo3z/r_pytorchlightning_the_researchers_version_of/,downtownslim,1564813872,"## What is it?

Lightning defers training and validation loop logic to you. It guarantees correct, modern best practices for the core training logic.

## Why do I want to use lightning?

When starting a new project the last thing you want to do is recode a training loop, model loading/saving, distributed training, when to validate, etc... You're likely to spend a long time ironing out all the bugs without even getting to the core of your research.

With lightning, you guarantee those parts of your code work so you can focus on what the meat of the research: Data and training, validation loop logic. Don't worry about multiple gpus or speeding up your code, lightning will do that for you!

&amp;#x200B;

[https://github.com/williamFalcon/pytorch-lightning](https://github.com/williamFalcon/pytorch-lightning)",84,219
154,2019-8-3,2019,8,3,15,clfogi,"Have you Optimized your Deep Learning Model Before Deployment? by M-Amine Hadj-Youcef, Ph.D.",https://www.reddit.com/r/MachineLearning/comments/clfogi/have_you_optimized_your_deep_learning_model/,aminehy,1564813949,,0,1
155,2019-8-3,2019,8,3,16,clfwu0,1 Introduction Machine learning,https://www.reddit.com/r/MachineLearning/comments/clfwu0/1_introduction_machine_learning/,gopirockz,1564815820,,0,1
156,2019-8-3,2019,8,3,17,clgdxr,N-Shot learning: Learn more with less data.,https://www.reddit.com/r/MachineLearning/comments/clgdxr/nshot_learning_learn_more_with_less_data/,Hsankesara,1564819766,,0,1
157,2019-8-3,2019,8,3,17,clghmp,best Computer Vision Courses?,https://www.reddit.com/r/MachineLearning/comments/clghmp/best_computer_vision_courses/,GhoOost90,1564820669,[removed],1,1
158,2019-8-3,2019,8,3,17,clgiia,Machine Learning and the Role of Google Helping in Technology Advancements,https://www.reddit.com/r/MachineLearning/comments/clgiia/machine_learning_and_the_role_of_google_helping/,rohitgupta010,1564820869,,0,1
159,2019-8-3,2019,8,3,18,clgquy,[Discussion] Does a background in Applied Math/Statistics help in self-learning ML/AI ?,https://www.reddit.com/r/MachineLearning/comments/clgquy/discussion_does_a_background_in_applied/,agoodperson44,1564822985,"So I had chosen a masters of applied statistics instead of data science or computer science. 

After reading many books I've gained a deep respect and admiration for developers of ML and artificial intelligence programs. I would like to start self-learning this. 

Does having a math/stats background help with self-learning ML/AI ?

Thanks!",11,4
160,2019-8-3,2019,8,3,18,clgrna,Unsupervised 3D Pose Estimation with Geometric Self-Supervision,https://www.reddit.com/r/MachineLearning/comments/clgrna/unsupervised_3d_pose_estimation_with_geometric/,idkname999,1564823165,[removed],0,1
161,2019-8-3,2019,8,3,18,clgu9y,Global Cold Storage Construction Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/clgu9y/global_cold_storage_construction_market_report/,jadhavni3,1564823802,[removed],1,1
162,2019-8-3,2019,8,3,19,clhec3,What is a Data Scientist? Where did Data Scientist come from? why you want to be one?,https://www.reddit.com/r/MachineLearning/comments/clhec3/what_is_a_data_scientist_where_did_data_scientist/,sudhabhise,1564828767,,0,1
163,2019-8-3,2019,8,3,20,clho17,[D] Data science is not software engineering and treating it as such is suboptimal at best. (Please) change my mind.,https://www.reddit.com/r/MachineLearning/comments/clho17/d_data_science_is_not_software_engineering_and/,ai_yoda,1564831086,"Hi, all

I have recently written a [blog post on medium](https://medium.com/neptune-ml/https-medium-com-neptune-ml-how-experiment-management-can-improve-the-roi-of-your-machine-learning-projects-ed805618817c) describing why I feel that there has to be room for science in data science and how one approach that helps solve this problem is proper experiment management. 

I would love to hear your thoughts on the subject.

What are the best practices in your organizations/teams and how do they differ from classical software development approaches?",12,0
164,2019-8-3,2019,8,3,20,clhv2y,Which method/test can I use,https://www.reddit.com/r/MachineLearning/comments/clhv2y/which_methodtest_can_i_use/,mwennde,1564832694," 

I have data collected in two different years as well as populations . In 2014 cross-section data was collected from a population in Nairobi, in 2015 using the same set of questions I collected data in Mombasa.

The data was collected based on the model below:

&amp;#x200B;

![img](46dwszhx08e31)

The study hypotheses are based on the paths. 

The variable were measured on a continuous scale (1-7)

I hope to analyse this data using an approach that accounts for:

1. Time
2. the two different populations",0,1
165,2019-8-3,2019,8,3,21,cli629,Extracting Product attributes from Product Names,https://www.reddit.com/r/MachineLearning/comments/cli629/extracting_product_attributes_from_product_names/,sushilthapa98,1564834927,[removed],0,1
166,2019-8-3,2019,8,3,21,clifzo,[D] Helper Plugin for ML Programming: Name Your Variable With Whatever You Like (Including LaTeX),https://www.reddit.com/r/MachineLearning/comments/clifzo/d_helper_plugin_for_ml_programming_name_your/,fzyzcjy,1564836895,"(See the figure :) )

![img](tdh8inpzc8e31 ""Synthesized Demo"")

Recently I have this idea and some thoughts about how to implement it as a plugin (using some hack). However, I am not sure about whether people (e.g. smart guys in this subreddit) need it and like it. Therefore, I would appreciate it if you could give me some comments and feedback!",0,1
167,2019-8-3,2019,8,3,22,cliigb,Sentiment Classification on Twitter Data with Date and Time?,https://www.reddit.com/r/MachineLearning/comments/cliigb/sentiment_classification_on_twitter_data_with/,mankadronit,1564837374,[removed],0,1
168,2019-8-3,2019,8,3,22,cliivk,Fake News Written by Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cliivk/fake_news_written_by_machine_learning/,mrblasto,1564837454,[removed],0,1
169,2019-8-3,2019,8,3,22,cliolz,[D] Helper Plugin for ML Programming: Name Your Variable With Whatever You Like (Including LaTeX),https://www.reddit.com/r/MachineLearning/comments/cliolz/d_helper_plugin_for_ml_programming_name_your/,fzyzcjy,1564838457,,1,1
170,2019-8-3,2019,8,3,22,cliox1,The Hitchiker's Guide to CNN with Fine Tuning,https://www.reddit.com/r/MachineLearning/comments/cliox1/the_hitchikers_guide_to_cnn_with_fine_tuning/,theainerd,1564838508,,0,1
171,2019-8-3,2019,8,3,22,clizog,[P] Stable Baselines 2.7.0: Twin Delayed DDPG (TD3),https://www.reddit.com/r/MachineLearning/comments/clizog/p_stable_baselines_270_twin_delayed_ddpg_td3/,araffin2,1564840418,,0,1
172,2019-8-3,2019,8,3,22,clj0v7,[P] Stable Baselines 2.7.0: Twin Delayed DDPG (TD3),https://www.reddit.com/r/MachineLearning/comments/clj0v7/p_stable_baselines_270_twin_delayed_ddpg_td3/,araffin2,1564840605,,0,1
173,2019-8-3,2019,8,3,22,clj1k2,[P] Stable Baselines 2.7.0: Twin Delayed DDPG (TD3),https://www.reddit.com/r/MachineLearning/comments/clj1k2/p_stable_baselines_270_twin_delayed_ddpg_td3/,araffin2,1564840727,[removed],0,1
174,2019-8-3,2019,8,3,22,clj1lq,How are papers evaluated ImageNet?,https://www.reddit.com/r/MachineLearning/comments/clj1lq/how_are_papers_evaluated_imagenet/,Lugi,1564840734,[removed],0,1
175,2019-8-3,2019,8,3,23,clj23f,[N] Stable Baselines 2.7.0: Twin Delayed DDPG (TD3),https://www.reddit.com/r/MachineLearning/comments/clj23f/n_stable_baselines_270_twin_delayed_ddpg_td3/,atooo57,1564840817,,0,1
176,2019-8-3,2019,8,3,23,clj55w,How are paper evaluated on ImageNet?,https://www.reddit.com/r/MachineLearning/comments/clj55w/how_are_paper_evaluated_on_imagenet/,Lugi,1564841335,[removed],0,1
177,2019-8-3,2019,8,3,23,clj6hg,[P] Stable Baselines 2.7.0: Twin Delayed DDPG (TD3),https://www.reddit.com/r/MachineLearning/comments/clj6hg/p_stable_baselines_270_twin_delayed_ddpg_td3/,araffin2,1564841547,[removed],0,1
178,2019-8-3,2019,8,3,23,clje86,How to include both Origin and Destination in your features?,https://www.reddit.com/r/MachineLearning/comments/clje86/how_to_include_both_origin_and_destination_in/,ohai123456789,1564842840,[removed],0,1
179,2019-8-3,2019,8,3,23,cljf8w,[P] Generating new ml titles/ideas with GPT-2,https://www.reddit.com/r/MachineLearning/comments/cljf8w/p_generating_new_ml_titlesideas_with_gpt2/,csinva,1564843003,"Why come up with new ideas when GPT-2 can come up with them for you?

[Website](https://csinva.github.io/gpt2-paper-title-generator/index.html), [Github](https://github.com/csinva/gpt2-paper-title-generator)

Some Examples:

* On The Non-Parametric Power Of Logistic Regression For Smooth Events
* Unifying Pac And Learning Mdps Using Influence Functions
* Machine Learning To Plan And Downlink Using Intrinsic Motivation
* Classifier Readiness Testing For Imbalanced Data
* Fast And Scalable Bayesian Deep Learning With Limited Observations
* A Comparison Of Deep Neural Networks And Adaptive Graph Neural Networks For Anomaly Detection
* Distributed Deep Learning With Gossip Networks Using Bidirectional Lstm Sensors
* Revisiting Reuse Of Super Categories
* Anatomical Visual Exploration
* Multimodal Social Learning With Active Interest Discovery",3,35
180,2019-8-3,2019,8,3,23,cljfbz,Discretization of latent variables,https://www.reddit.com/r/MachineLearning/comments/cljfbz/discretization_of_latent_variables/,HalfArmBandit,1564843018,[removed],0,1
181,2019-8-3,2019,8,3,23,cljkm2,The proof of concept #7 nunchaku clutching,https://www.reddit.com/r/MachineLearning/comments/cljkm2/the_proof_of_concept_7_nunchaku_clutching/,thetrickshotone,1564843878,,0,1
182,2019-8-3,2019,8,3,23,cljndh,Neural-Guided RANSAC: Learning Where to Sample Model Hypotheses,https://www.reddit.com/r/MachineLearning/comments/cljndh/neuralguided_ransac_learning_where_to_sample/,ThresholdTuner,1564844313,[https://arxiv.org/abs/1905.04132](https://arxiv.org/abs/1905.04132),0,1
183,2019-8-4,2019,8,4,0,clk905,High Quality Anime Faces Dataset,https://www.reddit.com/r/MachineLearning/comments/clk905/high_quality_anime_faces_dataset/,Mckinsey666,1564847573,,0,1
184,2019-8-4,2019,8,4,1,clkcen,A qlway to balance GANs: CycleGAN,https://www.reddit.com/r/MachineLearning/comments/clkcen/a_qlway_to_balance_gans_cyclegan/,-john--doe-,1564848077,,0,1
185,2019-8-4,2019,8,4,1,clkluv,A way to balance GANs: CycleGANs,https://www.reddit.com/r/MachineLearning/comments/clkluv/a_way_to_balance_gans_cyclegans/,-john--doe-,1564849438,,0,1
186,2019-8-4,2019,8,4,3,clm07b,Detect a pattern in an image,https://www.reddit.com/r/MachineLearning/comments/clm07b/detect_a_pattern_in_an_image/,fershey17,1564856705,[removed],0,1
187,2019-8-4,2019,8,4,4,clmure,Tutorials for setting up Unity ML-Agents for a maze solver,https://www.reddit.com/r/MachineLearning/comments/clmure/tutorials_for_setting_up_unity_mlagents_for_a/,Stanley_C,1564861125,Does anyone how any free tutorial links on how to set up a maze solver with Unity ML-agents from scratch?,0,1
188,2019-8-4,2019,8,4,6,clnxjo,"In your experience, what's the most useful function to interact (combine) features?",https://www.reddit.com/r/MachineLearning/comments/clnxjo/in_your_experience_whats_the_most_useful_function/,dutchbaroness,1564866769,[removed],0,1
189,2019-8-4,2019,8,4,6,clo86l,Hako switches in Stockholm.,https://www.reddit.com/r/MachineLearning/comments/clo86l/hako_switches_in_stockholm/,manmat,1564868355,[removed],0,1
190,2019-8-4,2019,8,4,7,cloinb,"[Q] If the cuda compute capability score is the same, Then is the performance the same?",https://www.reddit.com/r/MachineLearning/comments/cloinb/q_if_the_cuda_compute_capability_score_is_the/,GoBacksIn,1564869887,[removed],0,1
191,2019-8-4,2019,8,4,8,clp9al,ML in Production Issues,https://www.reddit.com/r/MachineLearning/comments/clp9al/ml_in_production_issues/,jennysebastian,1564873984,[removed],0,1
192,2019-8-4,2019,8,4,9,clpqpd,Machine learning final year project,https://www.reddit.com/r/MachineLearning/comments/clpqpd/machine_learning_final_year_project/,Habbeiz,1564876898,[removed],0,1
193,2019-8-4,2019,8,4,9,clq5q8,OpenAI: Huge computing power can deliver human-level AI in 5 years [N],https://www.reddit.com/r/MachineLearning/comments/clq5q8/openai_huge_computing_power_can_deliver/,robotvison,1564879309,[removed],0,1
194,2019-8-4,2019,8,4,10,clqgq4,[N] OpenAI: Huge computing power can deliver human-level AI in 5 years,https://www.reddit.com/r/MachineLearning/comments/clqgq4/n_openai_huge_computing_power_can_deliver/,robotvison,1564881144,,0,1
195,2019-8-4,2019,8,4,11,clr12z,[shitpost] Grad descent irl visualisation,https://www.reddit.com/r/MachineLearning/comments/clr12z/shitpost_grad_descent_irl_visualisation/,meechosch,1564884618,,0,1
196,2019-8-4,2019,8,4,11,clr7r8,Conditional Anime Character Generation,https://www.reddit.com/r/MachineLearning/comments/clr7r8/conditional_anime_character_generation/,Mckinsey666,1564885760,,0,1
197,2019-8-4,2019,8,4,12,clrmmf,Would you pay someone to fine-tune a model like BERT for you with your data?,https://www.reddit.com/r/MachineLearning/comments/clrmmf/would_you_pay_someone_to_finetune_a_model_like/,jennysebastian,1564888412,,0,1
198,2019-8-4,2019,8,4,12,clrsah,Compare Large Data sets,https://www.reddit.com/r/MachineLearning/comments/clrsah/compare_large_data_sets/,mohit__,1564889452,"Hello Reddit. I'm trying to solve a Duplicate Detection Problem. I have a massive data set of \~649000 rows containing e-commerce data. I need to find which products are duplicate of each other.

I have vectorized the text columns using Tfidf, which has resulted in an array of shape (648722, 2000). Now, if I try to compare using cosine similarity, the resulting array would be (648722, 648722) but I'm already getting Memory error. In this case, what can I use to compare large data sets? Do I have memory efficient options? I have a 8 GB RAM. 

My main problem is to detect duplicate rows. The above is my approach. I'd appreciate if someone can suggest anything.

PS: I also have image data of these products(rows) as URL.  But I thought, I'll find similar rows then download and compare the images.",0,1
199,2019-8-4,2019,8,4,14,clsmfg,Introduction to Noise Contrastive Estimation,https://www.reddit.com/r/MachineLearning/comments/clsmfg/introduction_to_noise_contrastive_estimation/,dukeleimao,1564895222,,0,1
200,2019-8-4,2019,8,4,14,clsmfs,Machine Learning Modules wont work on Pycharm. This is the first time i've ever been completely stumped on issues with an IDLE's settings.... PLEASE help.,https://www.reddit.com/r/MachineLearning/comments/clsmfs/machine_learning_modules_wont_work_on_pycharm/,Beardie12,1564895224,[removed],0,1
201,2019-8-4,2019,8,4,14,clsoot,Why it is so important to know the distribution of a variable?,https://www.reddit.com/r/MachineLearning/comments/clsoot/why_it_is_so_important_to_know_the_distribution/,sachinmukherjee,1564895683,[removed],0,1
202,2019-8-4,2019,8,4,15,cltf12,[D] Representing Time Series with varying time interval?,https://www.reddit.com/r/MachineLearning/comments/cltf12/d_representing_time_series_with_varying_time/,temporal_templar,1564901146,"Say Ive got some data, each element has a time variable to it, and the data is ordered by ascending time.

This isnt exactly a time series, as the interval between data items isnt fixed. Data may be 1 minute apart, or say 5 minutes apart. It is, however, a sequence.

I want to use this data to predict a quantity every hour, by using the data from the previous hour.

How do I capture the temporal sequence aspect of this data? Im thinking of using RNNs, but they need at least to have each sequence element to be a fixed time interval apart no? Or it wouldnt make sense?",53,72
203,2019-8-4,2019,8,4,16,cltkcz,what are some of the weirdest papers you've read in recent years?,https://www.reddit.com/r/MachineLearning/comments/cltkcz/what_are_some_of_the_weirdest_papers_youve_read/,thats_DR_chalupa_2u,1564902390,[removed],0,1
204,2019-8-4,2019,8,4,17,clty4n,Data Science Bootcamp: Pay only after you get a data science job.,https://www.reddit.com/r/MachineLearning/comments/clty4n/data_science_bootcamp_pay_only_after_you_get_a/,HannahHumphreys,1564905791,[removed],0,1
205,2019-8-4,2019,8,4,17,clu3vk,Idea: MLOps Composer. Interested in the community's opinion!,https://www.reddit.com/r/MachineLearning/comments/clu3vk/idea_mlops_composer_interested_in_the_communitys/,erikvdplas,1564907206,"I have an idea that solves a frustration I got while working on various machine learning projects which I think is quite common. I'm curious what others think before I start building the solution. I'm curious to hear all your suggestions and feedback as I want to create a tool that can be beneficial for as many people in ML as possible. First let me describe the problem and then propose my solution.

During several machine learning endeavors, we often quickly ran into scaling and operation issues: frequently it felt as though keeping (preprocessing) pipelines working correctly for different datasets, optimizing hyperparameters (cost-) effectively and managing tests and deployments was taking much more time than the actual development of the individual pieces. Lots of glue code and extremely precise documentation was necessary to keep experiments easily reproducible. Although individual processes were often relatively simple to grasp, properly managing them to work in concert (in various ways) was extremely time-consuming and tedious.

As a solution to this problem I came up with an idea for an application: what if you could manage the different components of the pipelines (let's call them modules) and hook them up in a GUI similar to Apple's [Quartz Composer](https://www.google.com/search?client=safari&amp;rls=en&amp;q=quartz+composer&amp;ie=UTF-8&amp;oe=UTF-8). Several commonly used modules can directly be used (so even without any coding experience!), but the user can also write their own Python scripts that can be interacted with using the application. A simple associated Python library aims to create some consistency in input and outputs of Python scripts which enables them to be used in the GUI. The application would also enable users to easily manage deployments of training sessions, tests and inference endpoints on various cloud providers, local compute, or other computers over SSH. Hyperparameter tuning can also be done through modules. Basically the entire process from raw data to usable models for inference can be streamlined in visual pipelines. 

I have seen similar tools, but they are often not as extensive as my idea, and sometimes suffer from vendor lock-in issues (such as with Google AutoML). If I'd build this application I might even do so open source, which would also speed up development.

As said, I'm extremely curious what y'all think. I am interested to hear suggestions, comments or similar ideas. Thanks a lot in advance!",0,1
206,2019-8-4,2019,8,4,17,clu50m,Artificial Intelligence Designing Cloths,https://www.reddit.com/r/MachineLearning/comments/clu50m/artificial_intelligence_designing_cloths/,deepdigitalfrog,1564907492,[removed],0,1
207,2019-8-4,2019,8,4,17,clu7cu,Humanoid robot named HRP-5P can install drywall.,https://www.reddit.com/r/MachineLearning/comments/clu7cu/humanoid_robot_named_hrp5p_can_install_drywall/,deepdigitalfrog,1564908094,,0,0
208,2019-8-4,2019,8,4,17,cluap6,Proof of concept #8 nunchaku switches,https://www.reddit.com/r/MachineLearning/comments/cluap6/proof_of_concept_8_nunchaku_switches/,thetrickshotone,1564908960,,0,1
209,2019-8-4,2019,8,4,18,cluho3,Final year project,https://www.reddit.com/r/MachineLearning/comments/cluho3/final_year_project/,dontuseyourreal_name,1564910657,[removed],0,1
210,2019-8-4,2019,8,4,18,cluj8t,What should I learn for predicting food consumption in cafeterias?,https://www.reddit.com/r/MachineLearning/comments/cluj8t/what_should_i_learn_for_predicting_food/,royerz2,1564911074,[removed],0,1
211,2019-8-4,2019,8,4,19,cluty4,Data Science Bootcamp: Pay only after you get a data science job.,https://www.reddit.com/r/MachineLearning/comments/cluty4/data_science_bootcamp_pay_only_after_you_get_a/,HannahHumphreys,1564913778,[removed],0,1
212,2019-8-4,2019,8,4,19,cluwk0,[D] Unsupervised 3D Pose Estimation with Geometric Self-Supervision,https://www.reddit.com/r/MachineLearning/comments/cluwk0/d_unsupervised_3d_pose_estimation_with_geometric/,idkname999,1564914442,"Has anyone seen the paper arxiv.org/abs/1904.04812

I thought it is particularly interesting because it can generate 3d pose training only on 2d pose data. But how legit is this paper though? I find that their description of the geometric projections to be quite vague. How can they get 3d coordinates without knowing the parameters of the camera?",12,2
213,2019-8-4,2019,8,4,20,clvcb0,Using deep RL to augment SGD?,https://www.reddit.com/r/MachineLearning/comments/clvcb0/using_deep_rl_to_augment_sgd/,PM_ME_ORGANS,1564918259,[removed],0,1
214,2019-8-4,2019,8,4,21,clvqlr,Why Deep NN approaches are better/worse than IR based approaches in building QA Systems??,https://www.reddit.com/r/MachineLearning/comments/clvqlr/why_deep_nn_approaches_are_betterworse_than_ir/,bvy007,1564921446,I wanted to know why Deep NN approaches are better or worser than IR based approaches. I wanted to know some points which could differentiate their advantages and disadvantages. Can anyone help me in pointing to some research paper??,0,1
215,2019-8-4,2019,8,4,21,clvrnf,Financial assistance to attend ICCV?,https://www.reddit.com/r/MachineLearning/comments/clvrnf/financial_assistance_to_attend_iccv/,felolorocher,1564921659,"Hi,

I recently got a paper accepted at ICCV as an oral presentation. I'm super stoked about this but unfortunately struggling to find money to pay for my trip. 

I was a post-doc when I worked on the project and submitted the work. I have now left to industry. My present company won't pay for me to attend for reasons. The previous grant I was employed on is ""running low"" on its travel budget so they will not assist. If I were still a post-doc, I don't think this would be an issue..

My ex-boss said she would look for some money, maybe her discretionary fund but I'm not hopeful. This leaves with me self-funding the entire trip, which is looking around $2000.

Are there any avenues I can explore for help? Have others been in this situation and managed to salvage some money?

Thanks!",0,1
216,2019-8-4,2019,8,4,21,clvuls,[D] Financial assistance for attending ICCV as a speaker,https://www.reddit.com/r/MachineLearning/comments/clvuls/d_financial_assistance_for_attending_iccv_as_a/,felolorocher,1564922259,"Hi,

I recently got a paper accepted at ICCV as an oral presentation. I'm super stoked about this but unfortunately struggling to find money to pay for my trip.

I was a post-doc when I worked on the project and submitted the work. I have now left to industry. My present company won't pay for me to attend for reasons. The previous grant I was employed on is ""running low"" on its travel budget so they will not assist. If I were still a post-doc, I don't think this would be an issue..

My ex-boss said she would look for some money, maybe her discretionary fund but I'm not hopeful. This leaves with me self-funding the entire trip, which is looking around $2000.

Are there any avenues I can explore for help? Have others been in this situation and managed to salvage some money?

Thanks!",15,9
217,2019-8-4,2019,8,4,21,clvz1q,Energy informatics,https://www.reddit.com/r/MachineLearning/comments/clvz1q/energy_informatics/,gradebee,1564923143,"Is anyone here working in this field? If so, where can I find out about current research in energy informatics , in relation to machine learning, or data science or other such. I've heard that R &amp; D in some companies are seeking professionals in this subject matter but I'm having hard time finding any information in this regard.",0,1
218,2019-8-4,2019,8,4,22,clwbpw,How to Write a Book,https://www.reddit.com/r/MachineLearning/comments/clwbpw/how_to_write_a_book/,Wenderu84,1564925469,"The Great Awakening

Chapter 20: How to Write a Book

August 4, 2019

First to write a non-fiction book, one must be well versed in a topic. That is pretty much it. Oh, but you then must create the passion in order to put your knowledge to practice. And with that dedication its reward brings, one comes to write paragraph after paragraph, chapter after chapter. In the end, it forms a book and you can be proud of what you have accomplished. Was it good or bad? Does it benefit society or harm it? There are ethical landmines all over writing non-fiction work and I recommend you produce work that will benefit society.

You really need a good motivation to write a book. You don't need an editor because you are its editor and you know it is good because you 1) spellcheck, and 2) record audio after to make sure it all flows well. If it is difficult to say, you might want to change it up a bit. Doing this caught me some important mistakes in Living Neverland... being fixed tonight but it doesn't matter, no one buys my books so it isn't like there are any out there except the ones I left in Mexico... Wonder what happened to those.

To write a book one needs to get obsessively obsessed with a particular field of study. You learn and learn and learn and eventually you gain the ability to walk around that subject. As a result you begin to see things that others can't. Those are the points you write about, in between the lines. And if you are to write, write with blood. Put your heart and soul into it, that way people will pick up your passion.

How could a robot write a book? If programmed well, it could probably learn heaps, digest relevant topic, and potentially paraphrase its back's blurb, but what would be its output? I would say writing books would be a rather difficult task for a bot to do. It would be nonsensical output. It would need to learn over time and be capable of paraphrasing a great deal of material. However, I won't say that it would be impossible for a bot to write a book. Because in the end language is only one thought after the next. Let's break it down.

I write word after word. My sentence structures are what is most important though because the linked ideas are what conveys meaning. But here is the question: can my sentences act independently of their paragraph structure? If each of my ideas (sentences) are both capable of working independently and as a whole, then that content can be used in many different chatbot applications. And if my sentences can be used elsewhere and similar ideas grouped together, then I could see it feasible for a bot to write a book. And if it can write a book very quickly, then that would put a lot of authors out of work. And if that happens, the only ones who will stand are the ones who based their ideas upon solid foundations. Because robots will also one day take over our media. And we need to be prepared for mass redundancy, but the only way to do that is to make yourself irreplaceable. And how do you do that? I am showing you right now.

Any good book consists of three parts: beginning (protasis), middle (epitasis) and end (catastrophe). The ideal topic you want to convey must have been contemplated for a long time before you start writing. To start with, keep an online journal or something and try your best to write almost every day. What you write about in the beginning, will stand as the end of its series... this is to say that your foundations will result in how high you can build the structure.

As a writer, I recommend you also do video work. It will give you more exposure. It is hard in a world of self-publishing to get your book recognised. Do not worry about a potential lack in sales, for that journey is behind you, only dormant until all actions align. Eventually past toils will become future rewards, but think ye not of this; for your mind is on the current project you are on, your past ones now archived in your memory just in case you (or anybody else) wants to view them. Running a decent website is also an important aspect when self-advocating your work. You are trying to get your presence out there as much as possible, and any publicity, bad or good, is good publicity. Love your haters.

Not everybody will be capable of writing a book. Remember, you are its editor and its publisher and if the quality is not up to standard, then I recommend to keep writing journals until your writing improves, then you can move on to books. However, maybe thinking of them as only essay after essay, then maybe it will be easier for you. My writings work both in and out of context of the books I write; meaning each chapter is an essay. Focus on your strengths and use effective methods to improve upon your weaknesses.

There is more than one way to write a book. You could write many different genres and therefore require those skills in order to effectively write that type of book. Study those you admire in the genre in which you are most attracted to. Careful analysis will eventually bring that which is being studied into instinct. But the main tip I have for you is write a paragraph. See where it takes you. Follow along with another paragraph. Take a break at any time if you wish. Just remember, paragraph after paragraph. There have been times when I started a paragraph and didn't finish it and it sounded good but I didn't understand where I was going and I lost it all because of that error: deleted, for none of it made sense without further explanation which I only had at the time of writing. This is why I recommend to try your best to finish paragraphs before you get distracted with something else. If you have to get distracted with other things, just quickly type the main points you want to get across in shorthand and the layout will be there for you when you get back.

If you self publish, then you might not get sales for a long time or find yourself needing to advertise to obtain sales. I have not yet sold a single book of mine and I started publishing at the end of 2013. This industry can be very bittersweet and if you are solely writing to make an income, then I advise you to save yourself the effort and not write a book. However if you have a burning desire to share what is inside of you with others, then I encourage you to write and write often. If you are to commit yourself to a task as huge as writing a book, then you need to first ensure that your intentions are in the right place. Just remember that not getting any sales doesn't mean your book is bad. It just means that people do not know about it yet.

If you are to write a book, make it your baby. Get very used to the tools that you will be using and format everything really nicely. Ensure that you have not made any spelling or grammatical errors such as using ""then"" when you should have used ""than"". Write whenever you are inspired. Don't let your writing take over your life but instead use writing to understand all that has been presented before you in this world. Freely go between writing and other tasks and let the events of the world be your inspiration for your writing. And just as each book has a beginning, middle, and end; so does each chapter. And if we want to further break it down, each paragraph. You have a story to tell? How can you tell it and reach the right audience for it?

Being a non-fiction author is no easy task. You have a lot of work to do and you will get many haters that will discourage you from the work. But your haters mean that you are doing something right, so their ignorant feedback is your reward and not punishment. Do not let their idle hands destroy the work of your active hands. There has been so many people who have attempted to get me to remove all of my work off of the Internet. But it has never worked for good because I am smart. I saved the work so that when you censored it, I was able to re-upload it using alternative non-mainstream methods. You need to be secure in your work. You need to ensure that nothing anyone else does against you will break what you have made: including your death. Therefore, keep lots of backups both online and offline. And don't worry about what others say, good or bad. Write for yourself and no one else. You are your audience and your work's success depends on your ability to entertain yourself.

Writing a book is a journey. I never know exactly how it will turn out, although I have a good idea. Don't worry if it becomes something different than you originally planned. The evolution of your book is merely the output of the evolution of you. As you change, your perspectives will change. That is not to say what you previously focused on was wrong but merely was taken from a different perspective. Learn well how to talk to yourself through your work because that output will determine the possibilities of your future work. To talk to yourself through your work, you need to be pointing at where you are striving to arrive at. That way, after one finishes their book, they will be able to read the beginning and the end and see a clear pathway that was shot straight through the book. Smart readers will be able to pick up on this and admire your skill in performing such a shot. In this way, your writing is transparent and clearly demonstrates your intentions, from beginning to end.

How will you licence your book? Your answer to this question will determine the future possible technological integration into your work. If you have a license slapped on your book then that means that it is not freely available and that means that robots will not be able to read your book. If you want your book to be a secret undiscoverable gem, then put a license on it. If you want your book to be widely available to not only the public but also bots, then slap a CC0 - No Rights Reserved licence on it and unlock the future potential of your book and the human race. Doing this does not mean that you can't sell your book. It just means that it is also freely available on the Internet.

Finally, I want you to understand how writing a book will dramatically internally change you. It is not a year or two of your life. It is foundations about who you are as a person that are being laid. It isn't the time it takes to write a book; it is the rest of your life. Once you write and publish a book, that book will follow you around all of the days of your life. That is why you shouldn't worry about fast income. Instead worry about the quality and longevity of your book. Because that will also determine the quality and longevity, of you...

http://nesmith.net/the-great-awakening",0,0
219,2019-8-4,2019,8,4,22,clwk2g,Final year project,https://www.reddit.com/r/MachineLearning/comments/clwk2g/final_year_project/,dontuseyourreal_name,1564926910,"Hi guys and gals,
I was thinking of doing a machine learning / computer vision based project for my final year project, I was going to look into something like mounting a raspberry pi and a camera to a car and being able to highlight stuff like stop signs and cars and such, and maybe changes in speed limits or something? Not sure, just a concept at the minute, but if anyone could point me in the right direction I'd appreciate it!",0,0
220,2019-8-4,2019,8,4,23,clwq17,[D] How to create a neural network for the game Ult. tic tac toe?,https://www.reddit.com/r/MachineLearning/comments/clwq17/d_how_to_create_a_neural_network_for_the_game_ult/,Kralex68,1564927894,"Hello I want to create a neural network for the game [Ult. Tic tac Toe](https://en.wikipedia.org/wiki/Ultimate_tic-tac-toe). It is my first neural network that I will create. Is my approach good? I want to have 90 inputs to the layer(81 representing the sub boards and 9 the global boards, -1 for occupied by O, 0 for empty and 1 for occupied by X). I want to include one or two hidden layers with 40 nodes each. The output layer has 1 output node ranging from \[-1,1\] representing 1 that X will win and -1 that O will win.

&amp;#x200B;

I want to play the neural network against itself. I want to have a dataset of 50 games. Then I want to interpolate each board state with the probability of winning. For example X won in 20 rounds, initial board has target output of 0 the next states 0.05, 0.1,0.15......, then backpropagate. What do you think?",7,0
221,2019-8-4,2019,8,4,23,clwuf2,MLIAT - We code for better tomorrow,https://www.reddit.com/r/MachineLearning/comments/clwuf2/mliat_we_code_for_better_tomorrow/,mlait1908,1564928625," MLAIT is designed for ambitious, dedicated developers who want to actively build a there future in upcoming technologies like ML, AI, Cloud and many more.

Visit our website to know more about us.

[MLAIT](https://mlait.tech)",0,1
222,2019-8-4,2019,8,4,23,clx5sy,Idea: MLOps Composer. Interested in the community's opinion! [Project],https://www.reddit.com/r/MachineLearning/comments/clx5sy/idea_mlops_composer_interested_in_the_communitys/,erikvdplas,1564930417,"I have an idea that solves a frustration I got while working on various machine learning projects which I think is quite common. I'm curious what others think before I start building the solution. I'm curious to hear all your suggestions and feedback as I want to create a tool that can be beneficial for as many people in ML as possible. First let me describe the problem and then propose my solution.

During several machine learning endeavors, we often quickly ran into scaling and operation issues: frequently it felt as though keeping (preprocessing) pipelines working correctly for different datasets, optimizing hyperparameters (cost-) effectively and managing tests and deployments was taking much more time than the actual development of the individual pieces. Lots of glue code and extremely precise documentation was necessary to keep experiments easily reproducible. Although individual processes were often relatively simple to grasp, properly managing them to work in concert (in various ways) was extremely time-consuming and tedious.

As a solution to this problem I came up with an idea for an application: what if you could manage the different components of the pipelines (let's call them modules) and hook them up in a GUI similar to Apple's [Quartz Composer](https://www.google.com/search?client=safari&amp;rls=en&amp;q=quartz+composer&amp;ie=UTF-8&amp;oe=UTF-8). Several commonly used modules can directly be used (so even without any coding experience!), but the user can also write their own Python scripts that can be interacted with using the application. A simple associated Python library aims to create some consistency in input and outputs of Python scripts which enables them to be used in the GUI. The application would also enable users to easily manage deployments of training sessions, tests and inference endpoints on various cloud providers, local compute, or other computers over SSH. Hyperparameter tuning can also be done through modules. Basically the entire process from raw data to usable models for inference can be streamlined in visual pipelines.

I have seen similar tools, but they are often not as extensive as my idea, and sometimes suffer from vendor lock-in issues (such as with Google AutoML). If I'd build this application I might even do so open source, which would also speed up development.

As said, I'm extremely curious what y'all think. I am interested to hear suggestions, comments or similar ideas. Thanks a lot in advance!",7,0
223,2019-8-5,2019,8,5,0,clx9m5,[D] How to manage your ML Experiments? (conclusions),https://www.reddit.com/r/MachineLearning/comments/clx9m5/d_how_to_manage_your_ml_experiments_conclusions/,pigdogsheep,1564931008,,0,1
224,2019-8-5,2019,8,5,0,clxf8m,"We may finally answer the question, 'Siri, what is the meaning of life.",https://www.reddit.com/r/MachineLearning/comments/clxf8m/we_may_finally_answer_the_question_siri_what_is/,deepdigitalfrog,1564931825,,0,0
225,2019-8-5,2019,8,5,0,clxjis,Calculating the properties of molecules on a noisy quantum computer.,https://www.reddit.com/r/MachineLearning/comments/clxjis/calculating_the_properties_of_molecules_on_a/,deepdigitalfrog,1564932478,,0,1
226,2019-8-5,2019,8,5,0,clxmyy,Building Neural Network Models that can reason,https://www.reddit.com/r/MachineLearning/comments/clxmyy/building_neural_network_models_that_can_reason/,kkziga,1564932986,,0,1
227,2019-8-5,2019,8,5,1,cly993,Listening to the neural network gradient norms during training,https://www.reddit.com/r/MachineLearning/comments/cly993/listening_to_the_neural_network_gradient_norms/,MasterScrat,1564936170,,0,1
228,2019-8-5,2019,8,5,2,clyqtz,5 most recommended Coursera Courses for Python : Part 1,https://www.reddit.com/r/MachineLearning/comments/clyqtz/5_most_recommended_coursera_courses_for_python/,sajad-52,1564938557,,0,1
229,2019-8-5,2019,8,5,2,clyzgx,[P] Listening to the neural network gradient norms during training,https://www.reddit.com/r/MachineLearning/comments/clyzgx/p_listening_to_the_neural_network_gradient_norms/,perone,1564939747,"I have made a small experiment to convert each network layer gradient norm into a tone in order to synthesize sound from it during training, there are some samples here: http://blog.christianperone.com/2019/08/listening-to-the-neural-network-gradient-norms-during-training/, if someone is interested.",34,287
230,2019-8-5,2019,8,5,2,clyzuf,"Deep Learning Research, Deep Learning Hardware, Sparse Networks | Interview with Tim Dettmers",https://www.reddit.com/r/MachineLearning/comments/clyzuf/deep_learning_research_deep_learning_hardware/,init__27,1564939797,[removed],0,1
231,2019-8-5,2019,8,5,2,clz8ag,How do monitoring your models in production?,https://www.reddit.com/r/MachineLearning/comments/clz8ag/how_do_monitoring_your_models_in_production/,jennysebastian,1564940975,[removed],0,1
232,2019-8-5,2019,8,5,2,clzayo,[D] Research management best practice,https://www.reddit.com/r/MachineLearning/comments/clzayo/d_research_management_best_practice/,vernunftig,1564941331,"Just wonder what are the best practices or role models for research management as a group? Research group activities include properly selecting research direction, balancing risk and feasibility, converting research findings into product, optimizing organizational structures, and performance measure etc., but these activities could easily go wild and out-of-control in practice due to the volatile nature of research. Which organization demonstrated most successful research management in history, and are there any key patterns a research group needs to follow in order to reach higher productivity?",1,5
233,2019-8-5,2019,8,5,3,clzh2a,"Can some help me with resources(research papers, blogs, etc) on Scence Representation using interaction or observation?",https://www.reddit.com/r/MachineLearning/comments/clzh2a/can_some_help_me_with_resourcesresearch_papers/,__daftvader__,1564942147,[removed],0,1
234,2019-8-5,2019,8,5,3,clzkt0,Backpropagation for GRU,https://www.reddit.com/r/MachineLearning/comments/clzkt0/backpropagation_for_gru/,CODOR777,1564942660,[removed],0,1
235,2019-8-5,2019,8,5,3,clzp5e,"I need some help with resources(research papers, blogs, etc) on Scene Representation using interaction or observation?",https://www.reddit.com/r/MachineLearning/comments/clzp5e/i_need_some_help_with_resourcesresearch_papers/,__daftvader__,1564943249,[removed],0,1
236,2019-8-5,2019,8,5,3,clzxpr,Machine Learning Basics: Excel vs SQL,https://www.reddit.com/r/MachineLearning/comments/clzxpr/machine_learning_basics_excel_vs_sql/,ahershy,1564944391,,0,1
237,2019-8-5,2019,8,5,4,cm0p7x,[Discussion]Good GAN intro tutorials.,https://www.reddit.com/r/MachineLearning/comments/cm0p7x/discussiongood_gan_intro_tutorials/,leonoel,1564948131,"I'll be teaching a tutorial on GAN with tensor flow. While I'm well versed in the math. I really want to make it easy enough for undergrads that may never have heard about GAN (let's assume they know Python/Keras and some ML).

Do you have any favorite tutorials out there, either blog post or video.",5,0
238,2019-8-5,2019,8,5,4,cm0ram,[N] Pytorch hackathon at Facebook. Aug 8th - Aug 9th. Application only.,https://www.reddit.com/r/MachineLearning/comments/cm0ram/n_pytorch_hackathon_at_facebook_aug_8th_aug_9th/,PlusImagination,1564948418,,0,1
239,2019-8-5,2019,8,5,4,cm0sr7,[R] Neural Chatbots Are Dumb,https://www.reddit.com/r/MachineLearning/comments/cm0sr7/r_neural_chatbots_are_dumb/,ricsinaruto,1564948626,"I have written a slightly sarcastic but mostly research focused blog post on issues with neural dialog models:  
[https://medium.com/@richardcsaky/neural-chatbots-are-dumb-65b6b40e9bd4](https://medium.com/@richardcsaky/neural-chatbots-are-dumb-65b6b40e9bd4)  
I also present a simple improvement (from my ACL paper this year), through a demo and some nice visualizations.  
Check it out if it spiked your interest!",9,10
240,2019-8-5,2019,8,5,5,cm0xb6,[P] FP Growth,https://www.reddit.com/r/MachineLearning/comments/cm0xb6/p_fp_growth/,gursi1,1564949278,"I have the following table:

&amp;#x200B;

![img](5szak6k1mhe31)

I need to perform fp growth so that the 'Student' tuple in 'Category' and 'Profession' are considered separately and I can get the pattern \[10-20\], \[Student\], \[Student\]. The code i am using gives me support along with the patterns and it considers 'Student' in all the columns as same, how can extract only the patterns from this code and also get the pattern mentioned above?

I have the following code: 

dataset = \[\['10-20', 'Student', 'Student', 'Chicago'\],

\['30-40', 'Adult', 'Unemployed', 'Chicago'\],

\['30-40', 'Adult', 'Finance', 'Washington'\],

\['10-20', 'Student', 'Student', 'Boston'\]\]

&amp;#x200B;

import pandas as pd

from mlxtend.preprocessing import TransactionEncoder

&amp;#x200B;

te = TransactionEncoder()

te\_ary = [te.fit](https://te.fit)(dataset).transform(dataset)

df = pd.DataFrame(te\_ary, columns=te.columns\_)

df

&amp;#x200B;

print(df)

&amp;#x200B;

from mlxtend.frequent\_patterns import fpgrowth

&amp;#x200B;

print(fpgrowth(df, min\_support=0.2))

&amp;#x200B;

print(fpgrowth(df, min\_support=0.2, use\_colnames=True))

&amp;#x200B;

Current output:

support                             itemsets

0      0.50                            (Student)

1      0.50                            (Chicago)

2      0.50                              (10-20)

3      0.50                              (Adult)

4      0.50                              (30-40)

5      0.25                         (Unemployed)

6      0.25                         (Washington)

7      0.25                            (Finance)

8      0.25                             (Boston)

9      0.25                   (Chicago, Student)

10     0.50                     (10-20, Student)

11     0.25                     (10-20, Chicago)

12     0.25            (10-20, Chicago, Student)

13     0.25                     (Chicago, Adult)

14     0.50                       (30-40, Adult)

15     0.25                     (30-40, Chicago)

16     0.25              (30-40, Chicago, Adult)

17     0.25                  (30-40, Unemployed)

18     0.25                  (Unemployed, Adult)

19     0.25                (Chicago, Unemployed)

20     0.25           (30-40, Unemployed, Adult)

21     0.25         (30-40, Chicago, Unemployed)

22     0.25         (Chicago, Unemployed, Adult)

23     0.25  (30-40, Chicago, Unemployed, Adult)

24     0.25                  (Washington, 30-40)

25     0.25                  (Washington, Adult)

26     0.25           (Washington, 30-40, Adult)

27     0.25                (Washington, Finance)

28     0.25                     (30-40, Finance)

29     0.25                     (Finance, Adult)

30     0.25         (Washington, 30-40, Finance)

31     0.25         (Washington, Finance, Adult)

32     0.25              (30-40, Finance, Adult)

33     0.25  (Washington, 30-40, Finance, Adult)

34     0.25                      (10-20, Boston)

35     0.25                    (Student, Boston)

36     0.25             (10-20, Student, Boston)

\&gt;&gt;&gt;",0,0
241,2019-8-5,2019,8,5,5,cm12qy,[N] Pytorch hackathon at Facebook. Aug 8th - Aug 9th. Application only.,https://www.reddit.com/r/MachineLearning/comments/cm12qy/n_pytorch_hackathon_at_facebook_aug_8th_aug_9th/,PlusImagination,1564950042,"https://www.eventbrite.com/e/pytorch-summer-hackathon-in-menlo-park-registration-63756668913

The eventbrite website will give you a welcome message, but you have to wait until your application is reviewed by Facebook staff and give you a confirmation message.",9,3
242,2019-8-5,2019,8,5,6,cm1jos,Web dev to ML questions,https://www.reddit.com/r/MachineLearning/comments/cm1jos/web_dev_to_ml_questions/,shawn123321,1564952425,[removed],0,1
243,2019-8-5,2019,8,5,6,cm267a,[P] How we used USE and FAISS to enhance ElasticSearch results,https://www.reddit.com/r/MachineLearning/comments/cm267a/p_how_we_used_use_and_faiss_to_enhance/,maxim_leonovich,1564955604,"I just wrote an article (quite long) about how we've build a semantic similarity index alongside the ElasticSearch and used both to provide smarter search results.

Tools used:

* [Tensorflow](https://www.tensorflow.org/) and [Simple Tensorflow Serving](https://github.com/tobegit3hub/simple_tensorflow_serving)
* [Pre-trained Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/1)
* [Docker](https://www.docker.com/)
* Python 3
* [FAISS](https://github.com/facebookresearch/faiss) by Facebook
* [ElasticSearch](https://www.elastic.co/products/elasticsearch)
* [gRPC](https://www.grpc.io/) for the API

&amp;#x200B;

Link to the article: [https://blog.onebar.io/building-a-semantic-search-engine-using-open-source-components-e15af5ed7885](https://blog.onebar.io/building-a-semantic-search-engine-using-open-source-components-e15af5ed7885)

&amp;#x200B;

I hope, this would be helpful as a practical ""how-to"" for anyone, trying to build a semantic search engine for their project.",9,17
244,2019-8-5,2019,8,5,7,cm2j8e,My Weekly Selection Digest of Papers and Technical Blogs doing Research on Deep Learning and AI,https://www.reddit.com/r/MachineLearning/comments/cm2j8e/my_weekly_selection_digest_of_papers_and/,HealthyNatural0,1564957503,[removed],0,1
245,2019-8-5,2019,8,5,7,cm2tcj,[D] Web dev to ML questions,https://www.reddit.com/r/MachineLearning/comments/cm2tcj/d_web_dev_to_ml_questions/,shawn123321,1564959028," I'm interested in transitioning from web development to machine learning.

My math background is mediocre (basic calc) and my only programming experience is in web dev technologies (3 years). I want to be realistic about my effort level and opportunities. I probably won't be going back to college, but have 10 months left on a work contract and would like to spend any free time learning the basics.

1. What is the feel of competition in the field? With my time-frame and background, is it reasonable to think I can earn an entry-level position?
2. I've heard the field is broken up into data science and data modeling. Is data modeling more programmatic? Will my experience as a dev be more relevant there?",4,0
246,2019-8-5,2019,8,5,8,cm2yke,Deemed Export Control License for AI scientists,https://www.reddit.com/r/MachineLearning/comments/cm2yke/deemed_export_control_license_for_ai_scientists/,aifordummies,1564959807,[removed],0,1
247,2019-8-5,2019,8,5,8,cm35x5,What's the current state-of-the-art for binary image classification?,https://www.reddit.com/r/MachineLearning/comments/cm35x5/whats_the_current_stateoftheart_for_binary_image/,dmdmello,1564960913,"I have no specific type of image or labels in mind. Just any state-of-the-art model for classifying images using binary labels. 

I would appreciate it if you showed me some articles and/or conferences to look for it.",0,1
248,2019-8-5,2019,8,5,9,cm3kwv,what exactly would give meaning after ml replaces lots of things in future?,https://www.reddit.com/r/MachineLearning/comments/cm3kwv/what_exactly_would_give_meaning_after_ml_replaces/,bestminipc,1564963266,[removed],0,1
249,2019-8-5,2019,8,5,9,cm3sxn,[D] Time series on GBM's - is row/instance order important?,https://www.reddit.com/r/MachineLearning/comments/cm3sxn/d_time_series_on_gbms_is_rowinstance_order/,charityworker,1564964539,"When building a time series model using something like xgboost or lightgbm, do they only take into account the present row / instance?

Say I build a model that has in one row:

- current values at timestamp

- lagged values from t-1 to t-5 of the previous values

If the rows/instances are shuffled for k fold cross validation, will that affect prediction ?",3,0
250,2019-8-5,2019,8,5,10,cm4nhc,How to update model for new data but not shutdown server (Django) ?,https://www.reddit.com/r/MachineLearning/comments/cm4nhc/how_to_update_model_for_new_data_but_not_shutdown/,hosjiu,1564969407,[removed],0,1
251,2019-8-5,2019,8,5,10,cm4rrm,"[D] The fit, transform and fit_transform methods. What do they do, exactly?",https://www.reddit.com/r/MachineLearning/comments/cm4rrm/d_the_fit_transform_and_fit_transform_methods/,radjeep,1564970106,"ML newbie here. I see ALL ML classes have these methods. Can someone explain, in plain English, what do they do?

Also, what is the difference between using fit, then transform on a dataset and fit_transform on the same?",2,0
252,2019-8-5,2019,8,5,11,cm53vz,Upcoming AI Conferences of 2019 - 2020,https://www.reddit.com/r/MachineLearning/comments/cm53vz/upcoming_ai_conferences_of_2019_2020/,LimarcAmbalina,1564972094,,0,1
253,2019-8-5,2019,8,5,12,cm5msf,"[D] Does anyone work with sparse training? If so, what architectures ? Would you like a library for this?",https://www.reddit.com/r/MachineLearning/comments/cm5msf/d_does_anyone_work_with_sparse_training_if_so/,BatmantoshReturns,1564975155,"I am currently doing a literature search of sparse training architectures, I am thinking of making a library for sparse training and hoping to make it as universal as possible. For that, I would need to get a good idea for the various use cases.",15,2
254,2019-8-5,2019,8,5,13,cm688l,"I am looking at teaching an algorithm to play a game called ""Soccer Stars"". I have gathered the necessary game data using OpenCV, but I am now looking for advice on where to go from here.",https://www.reddit.com/r/MachineLearning/comments/cm688l/i_am_looking_at_teaching_an_algorithm_to_play_a/,codie28,1564978670,[removed],0,1
255,2019-8-5,2019,8,5,13,cm69gq,Fungsi Sensor Torsi Dalam Mengukur Performa Pompa - Testingindonesia.co.id,https://www.reddit.com/r/MachineLearning/comments/cm69gq/fungsi_sensor_torsi_dalam_mengukur_performa_pompa/,rizki28,1564978893,,0,1
256,2019-8-5,2019,8,5,14,cm6v3g,Learn Today - #Free #MachineLearning course by #Google,https://www.reddit.com/r/MachineLearning/comments/cm6v3g/learn_today_free_machinelearning_course_by_google/,mlait1908,1564982833,,0,1
257,2019-8-5,2019,8,5,15,cm7a0n,[P] Used transfer learning on a MobileNetV2 model to classify American Sign Language (work in progress),https://www.reddit.com/r/MachineLearning/comments/cm7a0n/p_used_transfer_learning_on_a_mobilenetv2_model/,8uzzaw,1564985709,,0,1
258,2019-8-5,2019,8,5,15,cm7jym,my first medium blog on virtual environments,https://www.reddit.com/r/MachineLearning/comments/cm7jym/my_first_medium_blog_on_virtual_environments/,sreesai5c,1564987762,[removed],0,1
259,2019-8-5,2019,8,5,15,cm7mjv,[D] What is the hardest thing about implementing Machine / Deep Learning in your business?,https://www.reddit.com/r/MachineLearning/comments/cm7mjv/d_what_is_the_hardest_thing_about_implementing/,benur7,1564988289,[removed],0,1
260,2019-8-5,2019,8,5,16,cm7nx9,Math required for Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/cm7nx9/math_required_for_machine_learning/,deez29,1564988554,[removed],0,1
261,2019-8-5,2019,8,5,16,cm7pj4,A New Paradigm For Partial Differential Equations With Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cm7pj4/a_new_paradigm_for_partial_differential_equations/,analyticsindiam,1564988884,,0,1
262,2019-8-5,2019,8,5,16,cm7rsx,KMeans clustering,https://www.reddit.com/r/MachineLearning/comments/cm7rsx/kmeans_clustering/,Srishti15,1564989339,[removed],0,1
263,2019-8-5,2019,8,5,16,cm81ce,[Discussion][D]New Inventions That Are On Another Level  1,https://www.reddit.com/r/MachineLearning/comments/cm81ce/discussiondnew_inventions_that_are_on_another/,GoGadegets,1564991360,,0,1
264,2019-8-5,2019,8,5,17,cm8aaj,Multilayer hidden to hidden transformation in RNN (GRU/LSTM),https://www.reddit.com/r/MachineLearning/comments/cm8aaj/multilayer_hidden_to_hidden_transformation_in_rnn/,Pryanik88,1564993334,[removed],0,1
265,2019-8-5,2019,8,5,17,cm8d41,[D] Multilayer hidden to hidden transformation in RNN (GRU/LSTM),https://www.reddit.com/r/MachineLearning/comments/cm8d41/d_multilayer_hidden_to_hidden_transformation_in/,Pryanik88,1564993971,"Hello guys!  
I'm training GRU neural network with single GRU layer, and I tend to think that hidden to hidden transformation requires severe non-linearity to correctly ""merge"" memory with current timestep (and thus update hidden).  
How do I approach this, what is the best practive?  
Should I add more GRU layers or should I, for instance, add extra layers to hidden to hidden transformation with nonlinearity like relu?  
If I take second approach I guess I should use tanh instead of relu to avoid exploding gradients, am I correct?  


Thanks in advance.",9,1
266,2019-8-5,2019,8,5,17,cm8hq6,Statistical Machine Learning online course video lectures by Carnegie Mellon University,https://www.reddit.com/r/MachineLearning/comments/cm8hq6/statistical_machine_learning_online_course_video/,priyaleo,1564995014,,0,1
267,2019-8-5,2019,8,5,17,cm8jm6,Transformer with Python and TensorFlow 2.0  Attention Layers,https://www.reddit.com/r/MachineLearning/comments/cm8jm6/transformer_with_python_and_tensorflow_20/,RubiksCodeNMZ,1564995422,,0,1
268,2019-8-5,2019,8,5,18,cm8qb3,Data Analytics Brings it All Together!,https://www.reddit.com/r/MachineLearning/comments/cm8qb3/data_analytics_brings_it_all_together/,ElegantMicroWebIndia,1564996805,,0,1
269,2019-8-5,2019,8,5,18,cm8uug,We are working on a tool to test machine learning models. Can you give us feedback?,https://www.reddit.com/r/MachineLearning/comments/cm8uug/we_are_working_on_a_tool_to_test_machine_learning/,research-panda,1564997788,"We are working on a tool, which is supposed to help developers test, understand and explain their machine learning models. The tool will be open-source and free to use.

We are in the prototyping stage and have a prototype ready to show. If anyone would like to have a look and give us feedback, that would be awesome! 

If you have experience with testing models and frameworks like SHAP and LIME, it would be even more valuable.",0,1
270,2019-8-5,2019,8,5,18,cm8yck,We are working on a tool to explain the predictions of machine learning models. Can you give us feedback?,https://www.reddit.com/r/MachineLearning/comments/cm8yck/we_are_working_on_a_tool_to_explain_the/,research-panda,1564998554,[removed],0,1
271,2019-8-5,2019,8,5,19,cm967r,[P] We are working on a tool to explain the predictions of machine learning models. Can you give us feedback?,https://www.reddit.com/r/MachineLearning/comments/cm967r/p_we_are_working_on_a_tool_to_explain_the/,research-panda,1565000182,"We are working on a tool, which is supposed to help developers test, understand and explain their machine learning models. The tool will be open-source and free to use.

We are in the prototyping stage and have a prototype ready to show. If anyone would like to have a look and give us feedback, that would be awesome!

If you have experience with testing models and frameworks like SHAP and LIME, it would be even more valuable.",6,2
272,2019-8-5,2019,8,5,19,cm974n,[D] Per channel or per sample Loss calculation and averaging in a batch ?,https://www.reddit.com/r/MachineLearning/comments/cm974n/d_per_channel_or_per_sample_loss_calculation_and/,AdelSexy,1565000366,"Lets say we have an N-class semantic segmentation problem. Now on each iteration (for each batch) we can calculate Dice loss in two ways: (1) calculate average loss over classes for each sample in a batch and after that get the average over batch, or (2) calculate average loss    per class in a batch and then average over classes presented in a batch. 
Which one is better and why? Or there is no difference at all?
Can it affect on how model learns to segment small or big objects?
Any related articles?",2,2
273,2019-8-5,2019,8,5,19,cm9esq,"Artificial intelligence marketing (AI Marketing) importance, features &amp; Chatbots",https://www.reddit.com/r/MachineLearning/comments/cm9esq/artificial_intelligence_marketing_ai_marketing/,science_online,1565001982,,0,1
274,2019-8-5,2019,8,5,20,cm9jk6,"[P] Fitting (almost) any PyTorch module with just one line, including easy BERT fine-tuning",https://www.reddit.com/r/MachineLearning/comments/cm9jk6/p_fitting_almost_any_pytorch_module_with_just_one/,sudo_su_,1565002945,"Hi everyone,

My name is Dima and I wanted to tell you about an open-source library we work on called TamnunML.

Our goal is to provide an easy to use library (with `sklearn` interface) for complex model training and fine-tuning.
For example, with `tamnun` you can train any `pytorch` module like this:

```python
from torch import nn
from tamnun.core import TorchEstimator

module = nn.Linear(128, 2)
clf = TorchEstimator(module, task_type='classification').fit(train_X, train_y)
```

or, you can fine tune BERT on your task as easy as:
```python
from tamnun.bert import BertClassifier, BertVectorizer
from sklearn.pipeline import make_pipeline

clf = make_pipeline(BertVectorizer(), BertClassifier(num_of_classes=2)).fit(train_X, train_y)
predicted = clf.predict(test_X)
```

At the moment `tamnun` supports training any (almost) pytorch using just a ""fit"" method, easy BERT fine-tuning and model distillation.

You can read more about how to train (almost) any pytroch module with tamnun [here](https://medium.com/hiredscore-engineering/fitting-almost-any-pytorch-module-with-just-one-line-using-tamnunml-8ef224740ad3)
The library [github page](https://github.com/hiredscorelabs/tamnun-ml).
The [introduction to TamnunML](https://medium.com/hiredscore-engineering/introducing-octoml-73bd527491b1) of the library we published in our [blog](https://medium.com/hiredscore-engineering).",21,86
275,2019-8-5,2019,8,5,20,cm9oth,What taxes are paid on Kaggle prizes?,https://www.reddit.com/r/MachineLearning/comments/cm9oth/what_taxes_are_paid_on_kaggle_prizes/,mystikaldanger,1565003903,Just wondering how much the winners actually get when all is said and done.,0,1
276,2019-8-5,2019,8,5,20,cm9r12,Is this a machine learning problem?,https://www.reddit.com/r/MachineLearning/comments/cm9r12/is_this_a_machine_learning_problem/,GeorgeFudge,1565004301,[removed],0,1
277,2019-8-5,2019,8,5,20,cm9vkt,Minimax evaluation function,https://www.reddit.com/r/MachineLearning/comments/cm9vkt/minimax_evaluation_function/,scopreon,1565005113,[removed],0,1
278,2019-8-5,2019,8,5,20,cm9xlt,Conversational AI: The Advanced Form of Chatbots,https://www.reddit.com/r/MachineLearning/comments/cm9xlt/conversational_ai_the_advanced_form_of_chatbots/,MachineLearning001,1565005489,,0,1
279,2019-8-5,2019,8,5,21,cma80f,[Project] Fine-tuning GPT-2 on custom dataset,https://www.reddit.com/r/MachineLearning/comments/cma80f/project_finetuning_gpt2_on_custom_dataset/,rish-16,1565007281,"Hey everyone, I was wondering if there were any resources on how to fine-tune GPT-2 on a custom corpus of data, for example, Shakespearean poems or Cornell Movie Dialogues.",8,3
280,2019-8-5,2019,8,5,21,cmafr9,Best way to analyse a software's logs with Machine Learning with Tensor Flow?,https://www.reddit.com/r/MachineLearning/comments/cmafr9/best_way_to_analyse_a_softwares_logs_with_machine/,jvelez2210,1565008585,"Hey guys!

&amp;#x200B;

I am new to Tensor Flow and I would appreciate if someone could point me in the right direction, in order to create a program that extracts the logs from the database in the cloud, and analyse them with Machine Learning in order to find unusual behaviours.

&amp;#x200B;

What kind of pre-made Tensorflow or Keras model is the best for this kind of problem?

Is supervised learning the best approach to solving this problem?

&amp;#x200B;

Thanks in advance,

Jos Velez",0,1
281,2019-8-5,2019,8,5,22,cmappq,[D] Confused about OpenAI Gym: Which listed actions are correct?,https://www.reddit.com/r/MachineLearning/comments/cmappq/d_confused_about_openai_gym_which_listed_actions/,ReasonablyBadass,1565010211,"On:

[https://github.com/openai/gym/blob/master/gym/envs/atari/atari\_env.py](https://github.com/openai/gym/blob/master/gym/envs/atari/atari_env.py)

There seems to be a common list for every atari environment.

But if you use something like:

print(env.env.get\_action\_meanings())

It lists only a subsection of these options.

Could I use the same architecture for multiple games by using the ""maximum"" possible actions? Would the environment accept that?",2,1
282,2019-8-5,2019,8,5,22,cmb3nf,On testing for overfitting...,https://www.reddit.com/r/MachineLearning/comments/cmb3nf/on_testing_for_overfitting/,conradws,1565012392,[removed],0,1
283,2019-8-5,2019,8,5,22,cmbarm,GitHub Student Developer Pack - Free Softwares,https://www.reddit.com/r/MachineLearning/comments/cmbarm/github_student_developer_pack_free_softwares/,mlait1908,1565013449,,0,1
284,2019-8-5,2019,8,5,23,cmbgi8,'The Blowjob Paper:' Scientists Processed 109 Hours of Oral Sex to Develop an AI that Sucks Dick,https://www.reddit.com/r/MachineLearning/comments/cmbgi8/the_blowjob_paper_scientists_processed_109_hours/,Razaberry,1565014278,,0,1
285,2019-8-5,2019,8,5,23,cmbive,[R] A Guide to Human Pose Estimation,https://www.reddit.com/r/MachineLearning/comments/cmbive/r_a_guide_to_human_pose_estimation/,mwitiderrick,1565014612,Today I jump into the deep waters of research papers that are related to human pose estimation. This article is a sneek peak into what I found.,2,5
286,2019-8-5,2019,8,5,23,cmboo3,"[D] George Hotz: Comma.ai, OpenPilot, and Autonomous Vehicles",https://www.reddit.com/r/MachineLearning/comments/cmboo3/d_george_hotz_commaai_openpilot_and_autonomous/,UltraMarathonMan,1565015444,"George Hotz is the founder of Comma.ai, a machine learning based vehicle automation company. He is an outspoken personality in the field of AI and technology in general. He first gained recognition for being the first person to carrier-unlock an iPhone, and since then has done quite a few interesting things at the intersection of hardware and software. This conversation is part of the Artificial Intelligence podcast.

**Video:** [https://www.youtube.com/watch?v=iwcYp-XT7UI](https://www.youtube.com/watch?v=iwcYp-XT7UI)

**Audio:** [https://lexfridman.com/george-hotz](https://lexfridman.com/george-hotz)

![img](oaw3sobo4ne31)

**Outline:**

0:00 - Introduction

1:00 - Simulation

6:36 - Hacking

26:45 - Comma.ai and autonomous vehicles

1:49:12 - Hard work

1:50:20 - Merging with AI

1:56:50 - Winning",10,11
287,2019-8-5,2019,8,5,23,cmbvas,It's appropriate to use factorization machines to predict sellout information?,https://www.reddit.com/r/MachineLearning/comments/cmbvas/its_appropriate_to_use_factorization_machines_to/,ruan-putka,1565016353,[removed],0,1
288,2019-8-6,2019,8,6,0,cmc35l,Parameters for Feature Selection,https://www.reddit.com/r/MachineLearning/comments/cmc35l/parameters_for_feature_selection/,subhamroy021,1565017418,,0,1
289,2019-8-6,2019,8,6,0,cmc4tb,Help With Sony Neural Network Console,https://www.reddit.com/r/MachineLearning/comments/cmc4tb/help_with_sony_neural_network_console/,Kenneth_K,1565017627,[removed],0,1
290,2019-8-6,2019,8,6,0,cmcbj3,"Harvard Researchers Benchmark TPU, GPU &amp; CPU for Deep Learning",https://www.reddit.com/r/MachineLearning/comments/cmcbj3/harvard_researchers_benchmark_tpu_gpu_cpu_for/,Yuqing7,1565018504,,0,1
291,2019-8-6,2019,8,6,0,cmcj8a,An Archaeological Investigation on Matthews Correlation Coefficient,https://www.reddit.com/r/MachineLearning/comments/cmcj8a/an_archaeological_investigation_on_matthews/,dukeleimao,1565019527,,0,1
292,2019-8-6,2019,8,6,0,cmcpz3,[D] What are the differences between a Robust Deep Autoencoder and a Stacked Autoencoder?,https://www.reddit.com/r/MachineLearning/comments/cmcpz3/d_what_are_the_differences_between_a_robust_deep/,jweir136,1565020389,[removed],0,1
293,2019-8-6,2019,8,6,0,cmcror,Is there a solution for quickly blurring logos out of a batch of images?,https://www.reddit.com/r/MachineLearning/comments/cmcror/is_there_a_solution_for_quickly_blurring_logos/,VR_Angel,1565020606,[removed],0,1
294,2019-8-6,2019,8,6,1,cmddpv,"[D] Biggest batch size that should be used: Biggest even number that the GPU memory can handle, or biggest power of 2 that the GPU memory can handle? Also why do GPUs love power of 2s?",https://www.reddit.com/r/MachineLearning/comments/cmddpv/d_biggest_batch_size_that_should_be_used_biggest/,BatmantoshReturns,1565023332,"I have heard that GPUs love power of 2s, and that's why embeddings and batch sizes are often seen as some power of 2, (64, 128, 256, 512, 1024, etc). 

But I never have seen a concrete explanation for why this is. 

Also, should a max batch size to be considered the biggest even number that the GPU memory can handle, or the biggest power of 2 that the GPU memory can handle?",14,9
295,2019-8-6,2019,8,6,1,cmdl1j,"[D] As an ML Engineer/Researcher, if you could have a wish to help you do things, what would you request?",https://www.reddit.com/r/MachineLearning/comments/cmdl1j/d_as_an_ml_engineerresearcher_if_you_could_have_a/,rosstaylor90,1565024243,"(""More citations"" is not an acceptable answer :P)

Just curious to see what people think would be most helpful (e.g. what resources/tools/etc don't you have that you would like to have?). Let me know!",40,11
296,2019-8-6,2019,8,6,2,cmdxos,"An Interactive, Automated 3D Reconstruction of a Fly Brain",https://www.reddit.com/r/MachineLearning/comments/cmdxos/an_interactive_automated_3d_reconstruction_of_a/,sjoerdapp,1565025763,,0,1
297,2019-8-6,2019,8,6,2,cmdz3e,[P] Interpreting recurrent neural networks,https://www.reddit.com/r/MachineLearning/comments/cmdz3e/p_interpreting_recurrent_neural_networks/,AndreCNF,1565025932,"&amp;#x200B;

[Feature importance along an ALS patients time series. The border between the red shade \(output increasing features\) and the blue shade \(output decreasing features\) represents the models output for each timestamp.](https://i.redd.it/5jj6uoc1zne31.png)

I've been working on interpreting recurrent neural networks, having made some changes on the [SHAP package](https://github.com/slundberg/shap) to adapt them to this type of model, on PyTorch. In order to share this, I've recently posted an article on Medium explaining the core concepts and showing examples of how it works on multivariate time series data. You can read it here: [https://towardsdatascience.com/interpreting-recurrent-neural-networks-on-multivariate-time-series-ebec0edb8f5a](https://towardsdatascience.com/interpreting-recurrent-neural-networks-on-multivariate-time-series-ebec0edb8f5a)  


Also, feel free to ask me any questions, or to give some suggestions, if you're interested in this topic ",0,0
298,2019-8-6,2019,8,6,2,cme1d1,Need help understanding the multidimensional of the data being fed to a RNN and the output,https://www.reddit.com/r/MachineLearning/comments/cme1d1/need_help_understanding_the_multidimensional_of/,47884375,1565026210,"Assuming we have a time-series dataset whose window\_size = 30 and the batch\_size = 4, which makes the overall input = 4\*30 (2D). But as RNN expects 3D input, `tf.expand_dims` is used to make it a 3D input. 

What I don't get is that what does adding a dimension mean? Eg. what will be the element \[0,0,0\] of the input?

Also in keras, the typical format for fitting is  

    model.fit(input, output, epochs=400)

But in an RNN sample code, 

    model.fit(input, epochs=400)

Why is the `output` not given for the model to train in case of the first code? The timestamp is already included in the input in a way(in the 4\*30\*1 input, the 2nd dimension is supposed to be time-stamps), but how does the keras know against what output labels the input has to be trained?",0,1
299,2019-8-6,2019,8,6,2,cme6u4,wavenet mel spectogram to wav,https://www.reddit.com/r/MachineLearning/comments/cme6u4/wavenet_mel_spectogram_to_wav/,furciferX,1565026878,"What would be the theoretical explanation if a mel spectogram is fed to wavenet to reproduce the audio signal? I have recently learned about wavenet, haven't studied the paper well yet.  I just forked a repository and tried to reconstruct the audio signal from it's mel representation  [https://github.com/zabir-nabil/fast-wavenet-mel2wav/blob/master/fast\_mel2wav.ipynb](https://github.com/zabir-nabil/fast-wavenet-mel2wav/blob/master/fast_mel2wav.ipynb)  It's noisy but the audio is intelligible. But is this the way wavenet is conditioned on mel spectogram, I don't think so.",0,1
300,2019-8-6,2019,8,6,2,cme9te,"[P] Explaining Feedforward, Backpropagation and Optimization: The Math Explained Clearly with Visualizations. I took the time to write this long article (&gt;5k words), and I hope it helps someone understand neural networks better.",https://www.reddit.com/r/MachineLearning/comments/cme9te/p_explaining_feedforward_backpropagation_and/,permalip,1565027255,,0,1
301,2019-8-6,2019,8,6,2,cmed8t,"[D] $500/2hr Interview in San Jose, CA to develop better ML Frameworks",https://www.reddit.com/r/MachineLearning/comments/cmed8t/d_5002hr_interview_in_san_jose_ca_to_develop/,UErecruiting,1565027685,[removed],1,1
302,2019-8-6,2019,8,6,3,cmej6r,"Had a hard time getting internship offer, how to improve resume?",https://www.reddit.com/r/MachineLearning/comments/cmej6r/had_a_hard_time_getting_internship_offer_how_to/,shashi_123,1565028390,"Hey guys I don't know if this is the right place for this but anyways here is my resume [https://imgur.com/yIKLJGm](https://imgur.com/yIKLJGm), and while I had a couple of interviews for fall internship, I felt my resume was lagging. I applied for 3 months to no avail, so if anyone has pointers on 

1. Which machine learning/data science skills I can learn to improve resume,
2. resume formatting

Please let me know, I'll be really grateful. Thanks for your time!",0,1
303,2019-8-6,2019,8,6,3,cmejet,Applying AI and ML to Finance Healthcare and Hospitality,https://www.reddit.com/r/MachineLearning/comments/cmejet/applying_ai_and_ml_to_finance_healthcare_and/,AnnaOnTheWeb,1565028413,,0,1
304,2019-8-6,2019,8,6,3,cmerch,ML in signal processing,https://www.reddit.com/r/MachineLearning/comments/cmerch/ml_in_signal_processing/,OkRice10,1565029384,[removed],0,1
305,2019-8-6,2019,8,6,3,cmesgb,Deep Q learning question,https://www.reddit.com/r/MachineLearning/comments/cmesgb/deep_q_learning_question/,Kralex68,1565029518,[removed],0,1
306,2019-8-6,2019,8,6,3,cmew7g,[D] Benefits of ML in signal processing,https://www.reddit.com/r/MachineLearning/comments/cmew7g/d_benefits_of_ml_in_signal_processing/,OkRice10,1565029999,"There is plenty of research on ML in signal processing. The majority of it, so it seams to me, is about showing feasibility of ML-based receivers (end-to-end or individual functional blocks of). To me, we are past that - pretty much everybody realizes that ML-based OFDM receiver is possible and can probably achieve comparable performance to that of a conventional receiver. Furthermore, even if/when someone manages to show some (probably marginal) performance gain, that would probably have academic value, but not much beyond that.

To me, there are two fundamental questions, for which I haven't found answers in the literature and would really appreciate some pointers:

1. Can ML-based receiver achieve comparable performance with comparable complexity (to these of a conventional receiver)?
2. Beyond (probably marginal) performance gains, what can be commercial benefits of switching from a conventional receiver to an ML-based one?

Thanks!",12,11
307,2019-8-6,2019,8,6,3,cmeyq3,[D] Two questions about deep q learning,https://www.reddit.com/r/MachineLearning/comments/cmeyq3/d_two_questions_about_deep_q_learning/,Kralex68,1565030309,"&amp;#x200B;

https://i.redd.it/9eifb59zaoe31.png

I have two questions(In bold)

I want to use two neural networks to calculate the Q value for my current state(A board game). I use a sigmoid function. I correct the action with the highest value max Q that I obtain in the target network(The rest target is set to original output of the DQN). **Is this the correct approach or should I correct all output values in one iteration?** Second question: How do I calculate my target value r+ b mac Q(s(prime....). Should I use fixed reward values? \*\***Do I have to use reward values so that I can not surpass the possible output range of sigmoid function?**(\*\*Like 0,4 or 0,2)  Thank you",2,4
308,2019-8-6,2019,8,6,3,cmf116,Virtual Rehab ($VRH) CEO Speaks with NASDAQ's Jane King about the Use of Artificial Intelligence and Virtual Reality as Part of the Virtual Rehab Solution,https://www.reddit.com/r/MachineLearning/comments/cmf116/virtual_rehab_vrh_ceo_speaks_with_nasdaqs_jane/,VirtualRehab,1565030605,,0,1
309,2019-8-6,2019,8,6,3,cmf29s,Meta Arrays: How to store ImageNet in a single array?,https://www.reddit.com/r/MachineLearning/comments/cmf29s/meta_arrays_how_to_store_imagenet_in_a_single/,davidbun,1565030767,,0,1
310,2019-8-6,2019,8,6,4,cmfbki,[P] Building NSFW Image Detector,https://www.reddit.com/r/MachineLearning/comments/cmfbki/p_building_nsfw_image_detector/,jweir136,1565031905,"I have currently started building a nsfw image filter. I currently have a dataset of 120k nsfw images. 

The main issue I am having is with the anomaly detection part. All of the images are so different, I can't see how I can easily build a system to find if an image is nsfw or not. 

Any advice or notes is greatly appreciated. Thanks.",12,2
311,2019-8-6,2019,8,6,4,cmfdr2,What are some of the best general lectures regarding GAN's?,https://www.reddit.com/r/MachineLearning/comments/cmfdr2/what_are_some_of_the_best_general_lectures/,U_knight,1565032176,[removed],0,1
312,2019-8-6,2019,8,6,4,cmfeok,Lacking motivation due to barrage of google articles in my feed ! is it really that hard to get foot in door in 2019?,https://www.reddit.com/r/MachineLearning/comments/cmfeok/lacking_motivation_due_to_barrage_of_google/,highc1157,1565032286,[removed],0,1
313,2019-8-6,2019,8,6,4,cmfj0h,[N] Flatland Challenge - Multi-Agent Reinforcement Learning for Transportation Systems,https://www.reddit.com/r/MachineLearning/comments/cmfj0h/n_flatland_challenge_multiagent_reinforcement/,ML_Erik,1565032797,"Hi all


We launched the [Flatland Challenge](https://www.aicrowd.com/challenges/flatland-challenge), which is an official challenge of the [Applied Machine Learning Days](https://www.appliedmldays.org/challenges.html). 



# Flatland: Multi-Agent Reinforcement Learning Challenge

*The Flatland Challenge is a competition to foster progress in multi-agent reinforcement learning for real world applications. The [re-scheduling problem (RSP)](https://en.wikipedia.org/wiki/Vehicle_rescheduling_problem), which has traditionally been approached by operations research, serves as an excellent challenge to investigate the possibilies of deep learning for planning in stochastic environments. Different rounds with increasing difficulty and the presence of stochasticity in the environment encourage participants to look beyond classical planning algorithms and come up with solutions for the transport management systems of the future.*

## The Challenge

The challenge requires your creativity and savviness. In 2 submission rounds with increasing difficulty, you can prove that you have what it takes. We invite you to enter the race with your unique solution and to win great prizes - at the same time solving one of the key challenges in the world of transportation!

In contrast to most reinforcement learning challenges the focus of this challenge is not solely on the submission of great algorithms as controllers. We encourage the participants to come up with novel **observation spaces** for this challenge and share them with the community (community prize awarded) to improve performance on this task.

## Real world applications

The Swiss Federal Railways (SBB) operate the densest mixed railway traffic in the world. SBB maintain and operate the biggest railway infrastructure in Switzerland. Today, there are more than 10,000 trains running each day, being routed over 13,000 switches and controlled by more than 32,000 signals. Each day 1.2 million passengers and almost half of Switzerlands volume of transported goods are transported on this railway network. Due to the growing demand for mobility, SBB needs to increase the transportation capacity of the network by approximately 30% in the future.

The increase in transport capacity can be achieved through different measures, such as [denser train schedules, investments in new infrastructure, and/or investments in new rolling stock](https://smartrail40.ch/index.asp?inc=&amp;lang=en). However, SBB currently lack suitable technologies and tools to quantitatively assess these different measures.

The SBB are therefore looking for novel approaches that can help revolutionize the transportation system of the future.

## Prizes

Your problem solutions mean something to us - hence prizes with a total value of 30k CHF (approx. 30k USD) are reserved for those with the best submissions. You can excel in two categories: The best solution category and the community prize category. Within both those categories your submission is individually ranked taking into account your performance in Round 1 and Round 2. Make sure to check the participation rules before you start. Only submissions conforming to our rules have a chance of winning the prizes.

**Best Solution Prize**: Won by the participants with the best performing submission on our test set. Both of your rankings from the Round 1 and Round 2 are taken into account. Check the leader board on this site regularly for the latest information on your ranking.      

The top three submissions in this category will be awarded the following cash prizes (in Swiss Francs):

**CHF 7500.- (~USD 7500) for first prize**

**CHF 5000.- (~USD 5000) for second prize**

**CHF 2500.- (~USD 2500) for third prize**


**Community Contributions Prize**: Awarded to the person/group who makes the biggest contribution to the community - done through generating new observations and sharing them with the community.

The top submission in this category will be awarded the following cash prize (in Swiss Francs): **CHF 5000.- (~USD 5000)**

In addition, we will hand-pick and award up to five (5) travel grants to the Applied Machine Learning Days 2019 in Lausanne, Switzerland. Participants with promising solutions may be invited to present their solutions at SBB in Bern, Switzerland.

&gt; **Note:** It is possible for a participant to win in both categories  

## Participate

Are you up for the challenge? More information about the [Flatland Challenge](https://www.aicrowd.com/challenges/flatland-challenge/) can be found [here](https://www.aicrowd.com/challenges/flatland-challenge/).

## Contribute
Want to help improve and build upon **Flatland**?

Head over to our [gitlab repo](https://gitlab.aicrowd.com/flatland/flatland) to see how you can contribute shaping this environment.

## Contact

For Challenge-related questions (technical and/or content questions): 

* Gitter Channel : [https://gitter.im/AIcrowd-HQ/flatland-rl](https://gitter.im/AIcrowd-HQ/flatland-rl)
* Technical Issues : Please use the [issue tracker]( https://gitlab.aicrowd.com/flatland/flatland/issues) in the public repository
* Discussion Forum : &lt;https://discourse.aicrowd.com/&gt;

We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers.
But in case look for a direct communication channel, feel free to reach out to us at : 

- **mohanty** [at] **aicrowd.com**
- **erik.nygren** [at] **sbb.ch**

For press inquiries
Please contact SBB Media Relations at &lt;press@sbb.ch&gt;",0,6
314,2019-8-6,2019,8,6,4,cmfncc,"[D] Explaining Feedforward, Backpropagation and Optimization: The Math Explained Clearly with Visualizations. I took the time to write this long article (&gt;5k words), and I hope it helps someone understand neural networks better.",https://www.reddit.com/r/MachineLearning/comments/cmfncc/d_explaining_feedforward_backpropagation_and/,permalip,1565033301,"I have been studying Machine Learning in the last few months, and I wanted to really get to understand everything that goes on in a basic neural network (excluding the many architectures). Therefore, I took the time to write this long article, to explain what I have learned. In particular, the post on purpose very extensive and goes into the smaller details; this is to have everything in one place. As the site says, it is *machine learning from scratch*, and I share what I have learned.

The particular reason for posting here, is that I hope someone else could learn from this. The goal is to share the knowledge in the easiest absorbable way possible. I tried to visualize much of the process going on in neural networks, but I also went through the math, to the detail of the partial derivatives.

This was quite a journey, and it took about 1 month to read all the things I have read, and write it down, have it make sense and creating the graphics.

Regardless, here is the link. Any *constructive* feedback is appreciated.

[https://mlfromscratch.com/neural-networks-explained/](https://mlfromscratch.com/neural-networks-explained/)",25,130
315,2019-8-6,2019,8,6,4,cmfsw3,The proof of concept #9 centre line switches,https://www.reddit.com/r/MachineLearning/comments/cmfsw3/the_proof_of_concept_9_centre_line_switches/,thetrickshotone,1565033931,,0,1
316,2019-8-6,2019,8,6,4,cmft6a,[P] Does anyone know where I could get a dataset for astronomical spectroscopy?,https://www.reddit.com/r/MachineLearning/comments/cmft6a/p_does_anyone_know_where_i_could_get_a_dataset/,Keeeper-1,1565033967,Currently starting a project where I apply ML to find the different characteristics of celestial bodies based on spectrum data. I would appreciate if anyone would tell me where I can find such datasets. Thanks,11,7
317,2019-8-6,2019,8,6,4,cmfzlo,[Project] GraphVite: A High-Performance CPU-GPU Hybrid System for Node Embedding,https://www.reddit.com/r/MachineLearning/comments/cmfzlo/project_graphvite_a_highperformance_cpugpu_hybrid/,kiddozhu,1565034743,"We just released a general and high-performance graph embedding system GraphVite.
Compared to existing machine learning systems that are mainly designed for data with regular structures (e.g., images, speech, and natural language), GraphVite is specifically designed for large-scale graphs. It runs on the CPU-GPU hybrid architectures and scales linearly to the number of GPUs. The system is one or two magnitudes faster than existing implementations. For example, for a graph with one million nodes, it only takes around one minute to learn the node representations with 4 GPUs. Besides the superior efficiency, GraphVite also supports a variety of applications and models, including
* Node Embedding: DeepWalk, LINE, node2vec
* Knowledge Graph Embedding: TransE, DistMult, ComplEx, SimplE, RotatE
* Graph and High-dimensional Data Visualization: LargeVis.
There are already more than 30 configurations and benchmarks on standard datasets. We are actively developing new applications and models. The system is expected to support the community of graph embedding or in general, deep learning for graphs.

* Paper: https://arxiv.org/abs/1903.00757
* Website: https://graphvite.io/
* GitHub: https://github.com/DeepGraphLearning/graphvite",3,3
318,2019-8-6,2019,8,6,5,cmgen2,Calibrating the Learning Rate for Adaptive Gradient Methods to Improve Generalization Performance,https://www.reddit.com/r/MachineLearning/comments/cmgen2/calibrating_the_learning_rate_for_adaptive/,Neil_Liang,1565036619,[removed],1,1
319,2019-8-6,2019,8,6,5,cmgfye,Transcribe Live Chess with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cmgfye/transcribe_live_chess_with_machine_learning/,loganjspears,1565036794,,0,1
320,2019-8-6,2019,8,6,5,cmgh7d,AI for character animation?,https://www.reddit.com/r/MachineLearning/comments/cmgh7d/ai_for_character_animation/,fantastic1ftc,1565036953,"I am trying to make a game, and am unable to use any preexisting animations and/or motion capture for character animation. I want to find some ai framework that can be given the videos, and watch what the people are doing, and from that create an fbx animation. Is there anything to this extent anywhere?",0,1
321,2019-8-6,2019,8,6,6,cmgynk,Examples of Stochastic Processes in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cmgynk/examples_of_stochastic_processes_in_machine/,andrea_manero,1565039093,[removed],0,1
322,2019-8-6,2019,8,6,6,cmhctd,[D] Should beginner's tutorials be banned?,https://www.reddit.com/r/MachineLearning/comments/cmhctd/d_should_beginners_tutorials_be_banned/,NicolasGuacamole,1565040880,"This sub is full of them. They rise to the top for some bizarre reason and reaffirm that this subs focus is on helping people start off learning about a narrow set (neural networks / deep learning) of machine learning.

Allowing this content to be so prevalent drives the sub further from discussion of research and more into a place where spam links reside.

Furthermore, a lot of these beginners tutorials are written by beginners themselves. They contain mistakes, which upon being read by other beginners cloud their understanding and slow their learning.

Can we ban this type of content and push it to /r/learnmachinelearning or something?",145,781
323,2019-8-6,2019,8,6,6,cmheyh,Multi-View Geometry in CV textbook still highly relevant?,https://www.reddit.com/r/MachineLearning/comments/cmheyh/multiview_geometry_in_cv_textbook_still_highly/,mlforthebest,1565041151,[removed],0,1
324,2019-8-6,2019,8,6,7,cmi025,"This Startup is taking on Google in the battle of man versus machine, developing a new CAPTCHA technology that rewards people for being human.",https://www.reddit.com/r/MachineLearning/comments/cmi025/this_startup_is_taking_on_google_in_the_battle_of/,PainterBluea,1565043891,,0,1
325,2019-8-6,2019,8,6,7,cmi5xf,[P] PyTorch Implementation of Semantic Segmentation models,https://www.reddit.com/r/MachineLearning/comments/cmi5xf/p_pytorch_implementation_of_semantic_segmentation/,youali,1565044682,"Nothing fancy, but to get a handle of semantic segmentation methods, I re-implemented some well known models with a clear structured code (following this [PyTorch template](https://github.com/victoresque/pytorch-template)), in particularly:

- The implemented models are: Deeplab V3+ - GCN - PSPnet - Unet - Segnet and FCN

- Supported datasets: Pascal Voc, Cityscapes, ADE20K, COCO stuff,

- Losses: Dice-Loss, CE Dice loss, Focal Loss and Lovasz Softmax,

with various data augmentations and learning rate schedulers (poly learning rate and one cycle).


    
I though I share this implementation in case anyone might be interested, and here it is : 

**Github**: https://github.com/yassouali/pytorch_segmentation",1,2
326,2019-8-6,2019,8,6,8,cmihda,"[D] Company HireVue provides ""AI"" for early-stage interview screening",https://www.reddit.com/r/MachineLearning/comments/cmihda/d_company_hirevue_provides_ai_for_earlystage/,grey--area,1565046272,"There's been some (pretty universally negative) reaction on Twitter to a video put out by HireVue promoting their interview-related products, some of which use (unspecified) ""AI"". They have candidates answer employer-specified questions to camera, and they claim to evaluate things like whether the candidate is enthusiastic, or making enough eye contact (with the camera lens, I guess). 

I thought people might like to discuss here. The video and Twitter reactions are here: https://twitter.com/alvinfoo/status/1157793758806716417?s=19

Issues with this include baking existing hiring biases into poorly understood black boxes, that it's disrespectful to candidates, the best of which will refuse to be interviewed in this way, and that it's likely to give weird predictions when presented with anyone not well represented by the training set.

What other problems do people see with this? Is there any use of ML in the hiring process that you wouldn't object to?",3,0
327,2019-8-6,2019,8,6,8,cmimx3,[D] How to compute Ablation Study Neural Networks p-value?,https://www.reddit.com/r/MachineLearning/comments/cmimx3/d_how_to_compute_ablation_study_neural_networks/,temporal_templar,1565047067,"Let me just first say that my knowledge of statistics is shit.

I want to conduct an ablation study of a deep neural network, whose task is on regression.

I want to compare the performance (MSE) of the neural network vs the neural network without a few layers. I see in literature that people often report if results achieved are less than a p-value, say 0.05 or something.

How would I measure this p-value? Would I be able to measure the p-value simply given the 2 MSEs of the 2 models?",0,0
328,2019-8-6,2019,8,6,8,cmit6e,Prosr help,https://www.reddit.com/r/MachineLearning/comments/cmit6e/prosr_help/,ninjadudealex,1565047985,Hi I've been learning about neural networks and machine learning and I hit a speedbump while trying to figure out how to use a pretrained model made with pytorch. The ai I'm trying to learn how to use is called prosr. https://github.com/fperazzi/proSR I should have all my dependencies installed in my conda environment and I have the specific model I want to use downloaded instead of using the script that downloads the 10gbs of datasets and models that I don't need atm. I just need to figure out how to use it. It gives some info on this on how to test it and how to train it but I don't quite understand how to just use a trained model. Any help would be super appreciated. Thanks,0,1
329,2019-8-6,2019,8,6,8,cmiys8,Is there an AI or machine learning method that could scrape a bunch of text and recreate the style?,https://www.reddit.com/r/MachineLearning/comments/cmiys8/is_there_an_ai_or_machine_learning_method_that/,unclefishbits,1565048826,[removed],0,2
330,2019-8-6,2019,8,6,10,cmk8c9,"What parts of BERT, XLNET, Transformers etc. could I ""take"" in order to reinforce my current Deep Learning architectures and make them more robust?",https://www.reddit.com/r/MachineLearning/comments/cmk8c9/what_parts_of_bert_xlnet_transformers_etc_could_i/,the_parallax_II,1565055713,[removed],0,1
331,2019-8-6,2019,8,6,11,cmkkb3,"[D] Can I use tf.data to calculate new features as part of a pipeline, or should this be done before using the tf.data module?",https://www.reddit.com/r/MachineLearning/comments/cmkkb3/d_can_i_use_tfdata_to_calculate_new_features_as/,that_one_ai_nerd,1565057588,"I am just curious about how much of the data processing process I can refactor into a [tf.data](https://tf.data) pipeline for inputting my data into my model. My source data is used to calculate different features to create a dataset, and then this dataset is processed further for inputting into my models. So the process is basically like this:

Source Data (structured JSON which just has text fields for data parsed from a raw document) ---&gt;  
Dataset (this fields are used to calculate numerical features, categorical features, and sequence features) ---&gt;  
Processed Dataset (standard techniques - scaling, encoding, tokenization, padding, etc.)

And then I have my input data for the model. I am wondering whether I can refactor this entire process into a [tf.data](https://tf.data) pipeline, or will the [tf.data](https://tf.data) pipeline only handle the processing done in the second step described above? I am using TF 2.0 Beta by the way.

Any insights or help will be greatly appreciated.",4,2
332,2019-8-6,2019,8,6,11,cmkxpv,Can an agent create a simulation inside its simulation?,https://www.reddit.com/r/MachineLearning/comments/cmkxpv/can_an_agent_create_a_simulation_inside_its/,Nick-Conner,1565059707,[removed],0,1
333,2019-8-6,2019,8,6,11,cmkzft,[P] Neural Network Gradient Sonification,https://www.reddit.com/r/MachineLearning/comments/cmkzft/p_neural_network_gradient_sonification/,nnatlab,1565059990,"Heavily inspired by the recent [post](https://old.reddit.com/r/MachineLearning/comments/clyzgx/p_listening_to_the_neural_network_gradient_norms/) by /u/perone I developed a Keras callback that can easily be included during training for all you Keras users out there. During training the callback will convert the gradient norms of the network layers to a tone which can be saved along with the spectrogram of the audio after training is completed.

Additionally, I looped over a parameter space by varying the activation function, optimizer, and learning rate. Most notably, tweaking the activation function and optimizer produced quite interesting results.

* [Github](https://github.com/i-hack/Gradient-Sonification-Keras-Callback)
* [Spectrogram samples](https://github.com/i-hack/Gradient-Sonification-Keras-Callback/blob/master/examples/spectrograms.ipynb
)
* [Audio samples](https://www.kaggle.com/isaacco/neural-network-gradient-sonification)",0,6
334,2019-8-6,2019,8,6,12,cml852,How can I develop skills toward working as a Data Engineer?,https://www.reddit.com/r/MachineLearning/comments/cml852/how_can_i_develop_skills_toward_working_as_a_data/,vassadar,1565061396,"How can I develop skills toward working as a Data Engineer from a Software Engineer?

I did a few Kaggle challenges with Python and Scipy, but don't have much experience working with Data Pipeline with tools like Hadoop, Kafka and Spark/Flink.  
Where should I start or go from now?  


What I have in mind is

1. I think I should build up statistic skills, it's pretty rusty.
2. Some data visualization for data analysis.
3. Build some social network analysis or something of that sort. So, I can build myself a data pipeline playground and get familiar with Scala.
4. Practice parameter tuning, Kaggle could be the place.
5. Get familiar with Deep Learning with Pytorch or Tensorflow. (Is this needed?)  

6. Read up some Kaggle notebook and study other people's work.

&amp;#x200B;

However, I felt overwhelmed with all the time I have. Guess that you guys may be able to point me to the right direction. 

Thank you.",0,1
335,2019-8-6,2019,8,6,12,cmlerj,"[D] Where can I find audio file dataset for lung sounds, pulmonary sounds or any healthcare related sounds?",https://www.reddit.com/r/MachineLearning/comments/cmlerj/d_where_can_i_find_audio_file_dataset_for_lung/,Abhishek_nair_1303,1565062492,I am doing some benchmarking of my system which takes input as audio files. I wanted some dataset in healthcare domain to check my system. Does anyone have any pointers?,2,0
336,2019-8-6,2019,8,6,13,cmlyg2,How to detect/prevent implementation errors,https://www.reddit.com/r/MachineLearning/comments/cmlyg2/how_to_detectprevent_implementation_errors/,Syncopat3d,1565065821,[removed],0,1
337,2019-8-6,2019,8,6,13,cmm2dl,[D] How to detect/prevent fatal implementation bugs?,https://www.reddit.com/r/MachineLearning/comments/cmm2dl/d_how_to_detectprevent_fatal_implementation_bugs/,Syncopat3d,1565066473,"When you have an idea and implement it in a program and you don't get the high accuracy that you hoped for, it could be because the idea itself doesn't work or the implementation has a bug. If the low accuracy is due to a bug but you conclude that the idea doesn't work, you could be tragically missing an important research result.

One way to solve the problem is to implement the idea twice, preferably with two people working independently, and compare their results. If the results (prediction and accuracy metrics) are the same for the same input, you conclude that both implementations were made as intended. If not, then you have to make them agree, examining differences and fixing bugs along the way. However, different implementations are bound to make different assumptions about various details and have different output even if there are no bugs. Even different ways of using a PRNG results in different results. Generally, it's a very labor-intensive painstaking process that can take even more time than the initial implementation itself.

Are there other, more efficient, ways to ward off implementation errors that could doom one's research?",5,3
338,2019-8-6,2019,8,6,13,cmm4f1,[D] Topics for an at work Machine Learning program at a Finance company,https://www.reddit.com/r/MachineLearning/comments/cmm4f1/d_topics_for_an_at_work_machine_learning_program/,mrehanms,1565066822,"Hello!

I've been tasked with designing a beginner's program to Machine Learning at work. The target crowd is a bunch of QuantFin professionals - so pretty adept with Stats and Programming.

Again, the target is to get them familiarized with the math behind and rationale for various techniques - rather than teaching them to code out, let's say a Random Forest from scratch.

The reason being that, in our line of work, it's completely okay to use a package, and it's okay if you're not making significant improvements to the method in itself. It's more about acclimatizing them to the various techniques out there, so that the next time they come across a problem, they have a fair idea of what direction they can look in. I'm looking at a 40 hour programme, that eventually gives them some understanding of the underlying math and common applications 

What are some topics that this forum thinks should be a part of such an effort?",3,2
339,2019-8-6,2019,8,6,13,cmm6z0,Benefits of Using Machine Learning in Supply Chain,https://www.reddit.com/r/MachineLearning/comments/cmm6z0/benefits_of_using_machine_learning_in_supply_chain/,erp_oodles,1565067261,,0,1
340,2019-8-6,2019,8,6,14,cmmbo7,Theoretical + Practical questions and answers about VAE/ AE / GMM - Answer only if you know please.,https://www.reddit.com/r/MachineLearning/comments/cmmbo7/theoretical_practical_questions_and_answers_about/,dexmo2019,1565068044,[removed],1,1
341,2019-8-6,2019,8,6,15,cmn1d5,Drinking Glass Bottle Shrink Sleeve Labeling Machine,https://www.reddit.com/r/MachineLearning/comments/cmn1d5/drinking_glass_bottle_shrink_sleeve_labeling/,amygao1984,1565073116,,0,1
342,2019-8-6,2019,8,6,15,cmn3me,[R] How to Draw LSTM Architecture like Google Machine Translation?,https://www.reddit.com/r/MachineLearning/comments/cmn3me/r_how_to_draw_lstm_architecture_like_google/,yuh5,1565073569,"Hello fellow people who are crazy enough to do machine learning,

I'm writing a research paper that involves stacked LSTMs and some other fun ML models. What's the best way to draw my architecture like how the Google machine translation did? [http://fastml.com/images/deep\_learning\_diagrams/google\_neural\_machine\_translation\_system.jpg](http://fastml.com/images/deep_learning_diagrams/google_neural_machine_translation_system.jpg)",5,10
343,2019-8-6,2019,8,6,15,cmn44q,[Discussion][D]Amazing Construction Tools And Ingenious Machines  8,https://www.reddit.com/r/MachineLearning/comments/cmn44q/discussiondamazing_construction_tools_and/,GoGadegets,1565073665,,0,1
344,2019-8-6,2019,8,6,16,cmnpuh,Linear Regression In Machine Learning | Scikit-Learn | MLAIT,https://www.reddit.com/r/MachineLearning/comments/cmnpuh/linear_regression_in_machine_learning_scikitlearn/,mlait1908,1565078297,,0,1
345,2019-8-6,2019,8,6,18,cmo605,[P] I have built a monitoring solution for ML models running in production - Looking for beta testers !,https://www.reddit.com/r/MachineLearning/comments/cmo605/p_i_have_built_a_monitoring_solution_for_ml/,jverre,1565082039,"Hi,

I'm Jacques and I've built [https://stakion.io](https://stakion.io/). Stakion is a monitoring and debugging solution for deployed Machine Learning models. It tracks input and output data distributions in real-time to detect drifts or new behaviors that might impact model performance.

On tops of alerts, it also provides tools to better understand / debug predictions. This is especially useful when answering queries from the wider business about why a certain prediction was made.  


I am currently looking for beta-testers, here are the steps if you would like to try it out:

1. [https://dashboard.stakion.io/signUp](https://dashboard.stakion.io/signUp) and use ""reddit"" as the activation code
2. Join [Slack channel](https://join.slack.com/t/stakion/shared_invite/enQtNzE3NjA4NDEyMTUxLThhZDYzYjU4OGZjMWYwMTA0ZjU5ODdmOWRiODY2ODk0NTY5NjZhZWIyMWEzZWM3ODQ4N2ViNjA5N2JiZDE1NWM) for support
3. Once signed up, I will email you some more information to get started

I hope you like it and would love to get some feedback !

Cheers,

Jacques

*Processing img tmn5kue4mse31...*",0,7
346,2019-8-6,2019,8,6,18,cmo83c,[D] When Not to Choose the Best NLP Model,https://www.reddit.com/r/MachineLearning/comments/cmo83c/d_when_not_to_choose_the_best_nlp_model/,pirate7777777,1565082461,"Hi everyone, one of our writers wrote [an article](https://blog.floydhub.com/when-the-best-nlp-model-is-not-the-best-choice/) on [FloydHub blog](https://blog.floydhub.com/). The world of NLP already contains an assortment of pre-trained models and techniques. This article discusses how to best discern which model will work for your goals.

I hope you enjoy it!",0,0
347,2019-8-6,2019,8,6,18,cmoar1,How Machine Learning-Aided VR Is Helping Neurosurgeons Train Better,https://www.reddit.com/r/MachineLearning/comments/cmoar1/how_machine_learningaided_vr_is_helping/,analyticsindiam,1565083051,,0,1
348,2019-8-6,2019,8,6,18,cmofig,Machine Learning Course  Excel R Solutions,https://www.reddit.com/r/MachineLearning/comments/cmofig/machine_learning_course_excel_r_solutions/,excelrraising,1565084086,,0,1
349,2019-8-6,2019,8,6,18,cmok7w,How is Machine Learning used in Sports ?,https://www.reddit.com/r/MachineLearning/comments/cmok7w/how_is_machine_learning_used_in_sports/,RohanCR797,1565085112,[removed],0,1
350,2019-8-6,2019,8,6,20,cmp8qb,[P] soweego: link Wikidata to large catalogs,https://www.reddit.com/r/MachineLearning/comments/cmp8qb/p_soweego_link_wikidata_to_large_catalogs/,hell_j,1565090040,[removed],0,1
351,2019-8-6,2019,8,6,21,cmpoya,[P] soweego: link Wikidata to large catalogs,https://www.reddit.com/r/MachineLearning/comments/cmpoya/p_soweego_link_wikidata_to_large_catalogs/,hell_j,1565093001,[removed],0,1
352,2019-8-6,2019,8,6,21,cmpwzw,Kubernetes ReplicaSet 101,https://www.reddit.com/r/MachineLearning/comments/cmpwzw/kubernetes_replicaset_101/,AhmedAttef,1565094318," **Folks,**

In order for a ReplicaSet to work, it needs to know which pods it will manage?  
Here is This week's article Kubernetes ReplicaSet 101

**What's inside:**

* What is Kubernetes ReplicaSet?
* How Does ReplicaSet Manage Pods?
* Scaling and Autoscaling ReplicaSets.
* The ReplicaSet definition file.
* Is Our ReplicaSet the Owner of Those Pods?
* How Can I Remove a Pod From a ReplicaSet?

[https://www.magalix.com/blog/kubernetes-replicaset-101](https://www.magalix.com/blog/kubernetes-replicaset-101)",0,1
353,2019-8-6,2019,8,6,21,cmq0y4,[P] Encapsulating Capsule Networks: Everything You Need To Know,https://www.reddit.com/r/MachineLearning/comments/cmq0y4/p_encapsulating_capsule_networks_everything_you/,gebob19,1565094952,"Recently, I made a tutorial on Capsule Networks! Capsule Networks were introduced by Geoffrey Hinton and have been shown to be more robust to adversarial attacks, achieve higher accuracy, all while requiring significantly fewer parameters.

The tutorial covers how CNNs and Capsule networks account for viewpoint variance (a massive problem in computer vision), an introduction to Capsule Networks, and two routing algorithms, Dynamic Routing Between Capsules, and Matrix Capsules with EM Routing.

The post was written from zero knowledge to a complete understanding of the intuition and algorithm details in an easy to understand way. I'm sharing the post since it would be helpful to others wanting to learn about the topic, enjoy! 

&amp;#x200B;

[https://gebob19.github.io/capsule-networks/](https://gebob19.github.io/capsule-networks/)",8,15
354,2019-8-6,2019,8,6,22,cmqbaf,Using Bert to convert Vector to Word in pytorch,https://www.reddit.com/r/MachineLearning/comments/cmqbaf/using_bert_to_convert_vector_to_word_in_pytorch/,ggwowow1212,1565096629,[removed],0,1
355,2019-8-6,2019,8,6,22,cmqsrq,Kubernetes ReplicaSet 101,https://www.reddit.com/r/MachineLearning/comments/cmqsrq/kubernetes_replicaset_101/,AhmedAttef,1565099269,[removed],0,1
356,2019-8-6,2019,8,6,22,cmqt89,"How can I filter and balance a Windowed Tensorflow dataset with a binary classification label, based on the label?",https://www.reddit.com/r/MachineLearning/comments/cmqt89/how_can_i_filter_and_balance_a_windowed/,slingshoota,1565099339,[removed],0,1
357,2019-8-6,2019,8,6,22,cmqvvq,Deep Learning Quiz,https://www.reddit.com/r/MachineLearning/comments/cmqvvq/deep_learning_quiz/,nkptcs,1565099713,,1,1
358,2019-8-6,2019,8,6,23,cmr8o0,5 steps to become a good data scientist !,https://www.reddit.com/r/MachineLearning/comments/cmr8o0/5_steps_to_become_a_good_data_scientist/,sajad-52,1565101499,,0,1
359,2019-8-6,2019,8,6,23,cmrdma,[D] ANN - how to deal with features that can be identical from observation to observation?,https://www.reddit.com/r/MachineLearning/comments/cmrdma/d_ann_how_to_deal_with_features_that_can_be/,Sinsst,1565102200,"Firstly to explain the situation I have to deal with in more depth:

I have a dataset for which part of the features (columns of data) are identical from observation to observation (rows) and another part of the features are variable. Roughly every 1-200 observations have some features that fall into the pattern described, whereas the dataset is very large. 

Firstly, are there any specific reason why a neural network with above data would fail? Any papers/information/ideas that describe how to deal with this kind of situation?

Thanks",4,0
360,2019-8-6,2019,8,6,23,cmrl5x,Tuesday nunchaku tricks 3 syncopated wrist roll various,https://www.reddit.com/r/MachineLearning/comments/cmrl5x/tuesday_nunchaku_tricks_3_syncopated_wrist_roll/,thetrickshotone,1565103224,,0,1
361,2019-8-7,2019,8,7,0,cmryb9,What happens when CNN devised for 224x224x3 images is trained on high resolution imagery ?,https://www.reddit.com/r/MachineLearning/comments/cmryb9/what_happens_when_cnn_devised_for_224x224x3/,Many_Consideration,1565104966,[removed],0,1
362,2019-8-7,2019,8,7,0,cmryk4,But what is BERT?,https://www.reddit.com/r/MachineLearning/comments/cmryk4/but_what_is_bert/,_shy__guy_,1565104999,[removed],0,1
363,2019-8-7,2019,8,7,0,cms0ia,Abnormality Detection in Musculoskeletal Radiographs using Deep Learning,https://www.reddit.com/r/MachineLearning/comments/cms0ia/abnormality_detection_in_musculoskeletal/,rohit_lokwani17,1565105264,[removed],0,1
364,2019-8-7,2019,8,7,0,cms76e,Opinions on this?,https://www.reddit.com/r/MachineLearning/comments/cms76e/opinions_on_this/,Ginkobab,1565106138,,0,1
365,2019-8-7,2019,8,7,0,cmsbpw,[P] Testing in Machine Learning &amp; Biases,https://www.reddit.com/r/MachineLearning/comments/cmsbpw/p_testing_in_machine_learning_biases/,Sig_Luna,1565106736,"Hey all!

I'm looking to get some inputs from professional engineers in Machine Learning about how they test their models that \*might\* enter production.

I ran a survey in my own network before through a Twitter poll, and found out that over 70% of respondents were not fully confident in the abilities of their models, with 33% of all respondents saying that they are not confident in the abilities of their models at all.

Now I'd like to find out why and how to solve this. I prepared this survey and it would mean a lot if you could take 2 minutes to fill this one out.

I'm looking to write an article about ML testing methods based on those, and will be sure to share that here (and to anyone who requests it).

[https://dominicmonn.typeform.com/to/n2AbR7](https://dominicmonn.typeform.com/to/n2AbR7)",0,11
366,2019-8-7,2019,8,7,0,cmsd38,Trying to get quicker training times on a deep CNN,https://www.reddit.com/r/MachineLearning/comments/cmsd38/trying_to_get_quicker_training_times_on_a_deep_cnn/,Transit-Strike,1565106908,[removed],0,1
367,2019-8-7,2019,8,7,1,cmskwx,[D] live session discussing XLNet this afternoon,https://www.reddit.com/r/MachineLearning/comments/cmskwx/d_live_session_discussing_xlnet_this_afternoon/,tdls_to,1565107896,"we're hosting a live session this afternoon (6:30PM EST) to review XLNet, the recent high performing NLP model. feel free to join us if you're available (there will be live chat available), but also feel free to post your thoughts and discussion points here and we'll make sure to cover them
https://aisc.ai.science/events/2019-08-06",1,5
368,2019-8-7,2019,8,7,1,cmst9v,[Research] Real-time Event Detection on Social Data Streams,https://www.reddit.com/r/MachineLearning/comments/cmst9v/research_realtime_event_detection_on_social_data/,cdossman,1565108996," Social networks are quickly becoming the primary medium for discussing what is happening around real-world events. The information that is generated on social platforms like Twitter can produce rich data streams for immediate insights into ongoing matters and the conversations around them. To tackle the problem of event detection, we model events as a list of clusters of trending entities over time. We describe a real-time system for discovering events that is modular in design and novel in scale and speed: it applies clustering on a large stream with millions of entities per minute and produces a dynamically updated set of events. In order to assess clustering methodologies, we build an evaluation dataset derived from a snapshot of the full Twitter Firehose and propose novel metrics for measuring clustering quality. Through experiments and system profiling, we highlight key results from the offline and online pipelines. Finally, we visualize a high profile event on Twitter to show the importance of modeling the evolution of events, especially those detected from social data streams. 

 [https://medium.com/ai%C2%B3-theory-practice-business/real-time-event-detection-on-social-data-streams-72f89f394fda](https://medium.com/ai%C2%B3-theory-practice-business/real-time-event-detection-on-social-data-streams-72f89f394fda)",0,2
369,2019-8-7,2019,8,7,1,cmsy0d,[R] Pairwise comparisons with flexible time-dynamics,https://www.reddit.com/r/MachineLearning/comments/cmsy0d/r_pairwise_comparisons_with_flexible_timedynamics/,fondue-bigwig,1565109613,,0,1
370,2019-8-7,2019,8,7,1,cmt0p1,Using TensorFlow.js to Train a Rock-Paper-Scissors Model,https://www.reddit.com/r/MachineLearning/comments/cmt0p1/using_tensorflowjs_to_train_a_rockpaperscissors/,GantMan,1565109957,,0,1
371,2019-8-7,2019,8,7,1,cmt0r1,ML model for time series,https://www.reddit.com/r/MachineLearning/comments/cmt0r1/ml_model_for_time_series/,idonotknow9,1565109965,[removed],0,1
372,2019-8-7,2019,8,7,2,cmtalt,Confused about testing new data,https://www.reddit.com/r/MachineLearning/comments/cmtalt/confused_about_testing_new_data/,mindful_code,1565111223,[removed],0,1
373,2019-8-7,2019,8,7,2,cmtbcg,EfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoML,https://www.reddit.com/r/MachineLearning/comments/cmtbcg/efficientnetedgetpu_creating_acceleratoroptimized/,sjoerdapp,1565111319,,0,1
374,2019-8-7,2019,8,7,2,cmtoej,Tencent AI Trounces Pro Team in Chinas #1 Mobile Game,https://www.reddit.com/r/MachineLearning/comments/cmtoej/tencent_ai_trounces_pro_team_in_chinas_1_mobile/,Yuqing7,1565112991,,0,1
375,2019-8-7,2019,8,7,2,cmtv8y,Applying ML to philosophy: Has there been any research?,https://www.reddit.com/r/MachineLearning/comments/cmtv8y/applying_ml_to_philosophy_has_there_been_any/,digikar,1565113855,[removed],0,1
376,2019-8-7,2019,8,7,2,cmtw7n,Machine Learning is all about Maths : Free Resources to learn,https://www.reddit.com/r/MachineLearning/comments/cmtw7n/machine_learning_is_all_about_maths_free/,sajad-52,1565113976,,0,1
377,2019-8-7,2019,8,7,2,cmtw8o,[P] Can someone provide an overview of what a tf.data pipeline looks like for real world data instead of ML-ready datasets?,https://www.reddit.com/r/MachineLearning/comments/cmtw8o/p_can_someone_provide_an_overview_of_what_a/,that_one_ai_nerd,1565113980,"I am refactoring my data input pipeline from a custom set of classes, which need to be scrapped because the process is inefficient, error-prone, hard-to-use, and not scalable, to an end-to-end pipeline built using Tensorflow 2.0's [tf.data](https://tf.data) module. I understand the overall process, and how to use the module, but I have a few questions regarding how to properly structure the pipeline: 

1. Should I use object-oriented design or functional design? Because I would intuitively go with the OO route, using abstract base classes, and eventually having each feature be it's own sub-class, since my dataset doesn't actually include any of my features, but rather just the files from which data is sourced to calculate the features. But in the examples I have seen, I don't really see anyone implementing that sort of structure.
2. Should use the tf.feature\_columns module to store each individual feature, and then concatenate these feature columns to get my output data? Or should I concatenate the individual feature tensors together and use this as my input data?
3. My model takes 3 different inputs - one consisting of numerical and categorical features, and two separate tokenized sequence inputs. Should I implement this by creating three instances of tf.data.Dataset, one for each input, or should I create one instance for all the data, and then just 'pop' or whatever the equivalent function is the two columns holding my sequence data? Or is it a matter of preference?
4. *This one is more just to help me out and not necessarily having to do with* [*tf.data*](https://tf.data) *pipeline structure.* How do I implement a dynamic tokenizer for my sequence data, which will just update the vocabulary dict with the new word and assign it the next integer, so that the model can be trained continuously with new data, rather than having create a new tokenizer each time a significant amount of new words appear in the data, and retrain the entire model from scratch?

&amp;#x200B;

If anyone could point to some good examples of pipelines like this, or just help me understand the way the TF designed this module to be used, and how I can use it most effectively, I would be most appreciative and send a virtual hug your way, Or order you UberEats - I'm dead serious lol, that's how desperate I am to understand this. Because I have already planned everything out and how everything is supposed to work, but don't want to start coding until I'm sure I'm structuring and using everything properly.

Cheers, and if you help me figure out the answers, I will literally DM you and get you UberEats (up to $20 max).",3,5
378,2019-8-7,2019,8,7,2,cmtynu,Call for Papers - NeurIPS 2019 Workshop on Causal Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cmtynu/call_for_papers_neurips_2019_workshop_on_causal/,m_santacatterina,1565114299,[removed],0,1
379,2019-8-7,2019,8,7,3,cmu44n,Does data scaling have to coherent to all scaled features?,https://www.reddit.com/r/MachineLearning/comments/cmu44n/does_data_scaling_have_to_coherent_to_all_scaled/,Jandevries101,1565114990,,0,1
380,2019-8-7,2019,8,7,3,cmub28,[D] Best conferences for machine learning research in the natural sciences,https://www.reddit.com/r/MachineLearning/comments/cmub28/d_best_conferences_for_machine_learning_research/,drd13,1565115863,"I was wondering what everyone thought were the best matched conferences for machine learning research applied to the natural sciences. From my research online, KDD and AAAI seems to be the best venues. Other venues such as ICLR seem to encourage applied research in their calls for papers but not to accept many applied papers (outside of a few chemistry papers).

Thanks for your help.",3,15
381,2019-8-7,2019,8,7,3,cmuhno,Can machine learning be used for attribute-anomaly detection?,https://www.reddit.com/r/MachineLearning/comments/cmuhno/can_machine_learning_be_used_for_attributeanomaly/,Kasimark,1565116697,[removed],0,1
382,2019-8-7,2019,8,7,3,cmul4y,[Research] What happens when CNN devised for 224x224x3 images is trained on high resolution imagery ?,https://www.reddit.com/r/MachineLearning/comments/cmul4y/research_what_happens_when_cnn_devised_for/,Many_Consideration,1565117153,"I'm training a CNN on high resolution satellite imagery. Because of hardware constraints, I'm using `EfficientNetB0` (`EB0`) to predict classes from 800x800x3 tiles. This can be done thanks to the 2D `GlobalAveragePooling` layer at the end of the model which compresses the 25x25x1280 feature map (7x7x1280 when working with the original 224x224 images) into a 1D 1280 vector to be fed to dense layers and such. It works surprisingly well. Even more, I get better performance with `EB0` used on 800x800x3 tiles than with `EB5` applied on 456x456x3 tiles (the resolution for which they were designed).

How is this possible?",5,5
383,2019-8-7,2019,8,7,4,cmuyo2,[P] Open source an NLP/speech library DELTA,https://www.reddit.com/r/MachineLearning/comments/cmuyo2/p_open_source_an_nlpspeech_library_delta/,hankun11,1565118871,"Hi everyone, we recently open source our NLP/speech project at ACL 2019 - DELTA, a DEep Learning based language Technology plAtform ([https://github.com/didi/delta](https://github.com/didi/delta)). It aims to provide an end-to-end solution for training, deploying, and developing natural language processing and speech models for both academia and industry use cases. 

DELTA has been used for developing several state-of-the-art algorithms for publications and delivering real production to serve millions of users. It helps AI developers to quickly build models and easily deploy to production, featuring:

* Easy-to-use
   * One command to train NLP and speech models, including:
      * NLP: text classification, named entity recognition, question and answering, text summarization, etc
      * Speech: speech recognition, speaker verification, emotion recognition, etc
   * Use configuration files to easily tune parameters and network structures
* Easy-to-deploy
   * What you see in training is what you get in serving: all data processing and features extraction are integrated into a model graph
   * Uniform I/O interfaces and no changes for new models
* Easy-to-develop
   * Easily build state-of-the-art models using modularized components
   * All modules are reliable and fully-tested

For more details, check out: [https://github.com/didi/delta](https://github.com/didi/delta). Thank you!",5,19
384,2019-8-7,2019,8,7,4,cmv1fm,[N] Save forests by predicting illegal deforestation with big data!,https://www.reddit.com/r/MachineLearning/comments/cmv1fm/n_save_forests_by_predicting_illegal/,TheRaido,1565119226,"I'm working in IT (think somewhere between sysadmin/security and operations) at WWF Netherlands. A  colleague of mine who's involved in the 'Early Warning System'-program asked me to help me spread the following. 

WWF would like to invite you to participate in a Request for Proposals (RfP) to become a tech partner in the [Early Warning System (EWS)](https://www.wwf.nl/contact/ews-request-for-proposals) program which entails the development of a machine learning model to predict the risk of illegal deforestation using big data.

The RfP process began August 5th. Please have a look at the full RfP with technical details [on the website and the Terms and Conditions](https://www.wwf.nl/contact/ews-request-for-proposals). We encourage you to submit your application when ready as we will be reviewing on an ongoing basis.

If this is not appropriate let me know! If you have questions, ask.. If you have really difficult question, still ask but I might have to check with one of my colleagues :) Do not hesitate to share this, let's use tech for good.",13,129
385,2019-8-7,2019,8,7,4,cmv34r,"[Research] A Discussion of Adversarial Examples Are Not Bugs, They Are Features",https://www.reddit.com/r/MachineLearning/comments/cmv34r/research_a_discussion_of_adversarial_examples_are/,andrew_ilyas,1565119434,"Hi, one of the original paper authors here! Recently, Distill.pub hosted a community discussion of our paper, with commenters who reproduced, extended, and further discussed the results of our recent paper. Happy to answer any questions!",39,144
386,2019-8-7,2019,8,7,4,cmv511,Mask R-CNN Architecture,https://www.reddit.com/r/MachineLearning/comments/cmv511/mask_rcnn_architecture/,DaBobcat,1565119680,[removed],0,1
387,2019-8-7,2019,8,7,5,cmvjkl,Building reliable machine learning pipelines with AWS Sagemaker and Comet.ml,https://www.reddit.com/r/MachineLearning/comments/cmvjkl/building_reliable_machine_learning_pipelines_with/,CometML,1565122124,,0,1
388,2019-8-7,2019,8,7,5,cmvqpl,[P] Strategies to improve data extraction from semi-structured documents (SEC filings),https://www.reddit.com/r/MachineLearning/comments/cmvqpl/p_strategies_to_improve_data_extraction_from/,newtomtl83,1565123228,"I hope I am positing in the right place. If not, i'll take the post down.

I am a university researcher working on a project that involves matching names with biographical data from SEC filings. I downloaded all the filings for the organizations I am interested in, and wrote a Python script that basically finds officers' names in the document, and then looks for gender, age, education and job title. It is tricky because companies can have different formats for these documents, so you have to think about different possibilities the data can be presented. 

I use a fuzzy string matching to account for differences in spelling and typos, and different ways of naming tables. But most of the ""learning"" came from me manually tuning the script. Unfortunately, the script has to do a lot of safety checks to avoid outputting jibberish data (e.g. assuming a list of company names are people, etc). Finding age is also very tricky, as you often have to parse sentences to search for patterns such as ""Mr. ABC, age 56"" or ""Mrs. Jean B. XYZ, MD, 46.""

My script works well and outputs the data that I want. The main issue that I have is that it takes a good 30 seconds for all the calculations to be made for just one company (around 5-20 executives per company). The reason, I think, is because my script tests a lot of different possibilities, even if they are not applicable to the document. 

I am sure I am not the only one working on extracting data from semi-structured documents. I wanted to know if I could get feedback on what strategies I could implement to improve my script. I would particularly be interested in methods that involve tracking the performance of each data parsing strategies so that the computer does not waste time using a method that doesn't work so well.

Thanks a lot for your help!",5,2
389,2019-8-7,2019,8,7,5,cmw125,[P] Visalizing and debugging a neural network,https://www.reddit.com/r/MachineLearning/comments/cmw125/p_visalizing_and_debugging_a_neural_network/,trenmost,1565124792,,1,1
390,2019-8-7,2019,8,7,6,cmwd3e,Improving Pose Estimation performance,https://www.reddit.com/r/MachineLearning/comments/cmwd3e/improving_pose_estimation_performance/,eco_bach,1565126569,"My Pose Estimation demo works but getting 1 frame every 2-4 seconds with canned  1080p video!


Here are my specs

**Hardware + driver**

* GeForce GTX 960M

* Driver version 431.60

* Intel Core i7 6700HQ CPU 2.6 GHz

* 15.87 GB RAM

**Software**
* Windows 10

* CUDA 10.0

* Python 3.6.8

* tensorflow 1.14.0


**Questions**

1. I had to comment out the line

import tensorflow.contrib.tensoort as trt

to avoid compile errors.

Could this be causing my performance issues?

2. I have both tensorflow and tensorflow-gpu installed. How do I know which one I am running?

3. What other steps should I take to help troubleshoot performance?",0,1
391,2019-8-7,2019,8,7,6,cmwexh,An Overview of Signal Classification,https://www.reddit.com/r/MachineLearning/comments/cmwexh/an_overview_of_signal_classification/,lukerbs,1565126823,,0,1
392,2019-8-7,2019,8,7,7,cmx0m3,Software Pros: Artificial Intelligence Engineer Masters Program  Take your career and earnings growth to the next level,https://www.reddit.com/r/MachineLearning/comments/cmx0m3/software_pros_artificial_intelligence_engineer/,internetdigitalentre,1565129671,[removed],0,1
393,2019-8-7,2019,8,7,7,cmxjxl,Software Pros: Artificial Intelligence Engineer Masters Program  Take your career and earnings growth to the next level,https://www.reddit.com/r/MachineLearning/comments/cmxjxl/software_pros_artificial_intelligence_engineer/,internetdigitalentre,1565132293,[removed],0,1
394,2019-8-7,2019,8,7,8,cmxqyr,[D] Best way to train an RNN to play an instrument in a specific performer's style,https://www.reddit.com/r/MachineLearning/comments/cmxqyr/d_best_way_to_train_an_rnn_to_play_an_instrument/,jmineroff,1565133174,"I think building a model to learn a specific musician's style could be a fun side project and was hoping to get some thoughts. I have complete multi-track MIDI discographies for a few bands, but I'm struggling to figure out the best way to structure the input/output of the model.

The general idea is to feed in some set of instrument/vocal tracks (this selection will basically be a hyperparameter) and generate the target track as an output. An RNN seems like the obvious approach and I was planning to start with a BLSTM using Keras and music21 in Python. To generate the training data, I'll use a training set of the songs to randomly sample 10 second (another hyperparameter) clips. The same track will always be the target, and I am planning to use a consistent subset of the other tracks as the input. However, it would be nice to be able to use a varied set of input tracks to generate the output.

Does anyone have experience doing something similar to this, or have any relevant papers/libraries to recommend?",3,2
395,2019-8-7,2019,8,7,9,cmyz4s,[P] Awesome list for dataset tools,https://www.reddit.com/r/MachineLearning/comments/cmyz4s/p_awesome_list_for_dataset_tools/,InfoPaste,1565139496,"I find others frequently asking about different annotation tools for labeling custom datasets. Many tools are being developed all the time, so I created a awsome list to help myself and others find the proper tool they are looking for.

https://github.com/jsbroks/awesome-dataset-tools

All the best on your data hunts.",10,38
396,2019-8-7,2019,8,7,13,cn1662,online free video lectures on Machine learning from top university's !!!,https://www.reddit.com/r/MachineLearning/comments/cn1662/online_free_video_lectures_on_machine_learning/,chandulekkala,1565152241,,0,1
397,2019-8-7,2019,8,7,13,cn16w7,[D] Why does pre training work?,https://www.reddit.com/r/MachineLearning/comments/cn16w7/d_why_does_pre_training_work/,RobRomijnders,1565152367,"Using a pre-trained network for your new task seems standard practise. I'm talking about the case where we retrain the __entire__ network on the new task. Not only the final layers. What are you all thinking on how this improves performance on new tasks? Why cannot we learn those exact representations on the new task?

My own reasoning gets stuck at the following point: usually we pretrain the network on a larger data set. The network learns representations on that large data set that it could not have learned on our own smaller data set. The small data set might not have enough evidence for those representations. However, using those parameters actually __improves__ performance. So the associated representations are actually useful. That seems contradictory.",14,4
398,2019-8-7,2019,8,7,13,cn18x7,Deepfake video,https://www.reddit.com/r/MachineLearning/comments/cn18x7/deepfake_video/,Crealapata,1565152732,,0,1
399,2019-8-7,2019,8,7,14,cn1lcm,Can self supervised learning help the taks pf speech emotion recognition usiing human voice ?,https://www.reddit.com/r/MachineLearning/comments/cn1lcm/can_self_supervised_learning_help_the_taks_pf/,gshamane,1565155024,[removed],0,1
400,2019-8-7,2019,8,7,14,cn1t67,How about the best long paper of ACL2019 (Bridging the Gap between Training and Inference for Neural Machine Translation),https://www.reddit.com/r/MachineLearning/comments/cn1t67/how_about_the_best_long_paper_of_acl2019_bridging/,hanka-research,1565156539,[removed],1,1
401,2019-8-7,2019,8,7,15,cn1z7q,Research-oriented project through C lang,https://www.reddit.com/r/MachineLearning/comments/cn1z7q/researchoriented_project_through_c_lang/,letsceee,1565157755,[removed],0,1
402,2019-8-7,2019,8,7,15,cn2253,Learn why Engati is so special,https://www.reddit.com/r/MachineLearning/comments/cn2253/learn_why_engati_is_so_special/,getengati,1565158344,[removed],0,1
403,2019-8-7,2019,8,7,16,cn2jil,[N] 5 InsurTech Startups Helping Auto Insurance In Fraud Prevention,https://www.reddit.com/r/MachineLearning/comments/cn2jil/n_5_insurtech_startups_helping_auto_insurance_in/,bigelowaaron,1565161871,[removed],0,1
404,2019-8-7,2019,8,7,16,cn2lsb,Vectoglyph - Vector forms as a foreign language for the artificial intelligence GPT-2,https://www.reddit.com/r/MachineLearning/comments/cn2lsb/vectoglyph_vector_forms_as_a_foreign_language_for/,fluate,1565162328,,0,1
405,2019-8-7,2019,8,7,16,cn2mfe,[N] 5 InsurTech Startups Helping Auto Insurance In Fraud Prevention,https://www.reddit.com/r/MachineLearning/comments/cn2mfe/n_5_insurtech_startups_helping_auto_insurance_in/,bigelowaaron,1565162470,[removed],0,1
406,2019-8-7,2019,8,7,16,cn2nai,New to Development,https://www.reddit.com/r/MachineLearning/comments/cn2nai/new_to_development/,nishshetty,1565162643,"Hi guys, I am a Marketing person. But, I would love to learn about Machine Learning, AI and Data Science. Any guidance for a beginner?",0,1
407,2019-8-7,2019,8,7,16,cn2vq9,How do I confirm if hardware like Qualcomm 605 and Intel myriad X can run (support) inference for how many different kind of network ?,https://www.reddit.com/r/MachineLearning/comments/cn2vq9/how_do_i_confirm_if_hardware_like_qualcomm_605/,gireeshwaran,1565164463,[removed],0,1
408,2019-8-7,2019,8,7,17,cn34y9,What are your thoughts regarding the sensationalism about Auto-ML resources?,https://www.reddit.com/r/MachineLearning/comments/cn34y9/what_are_your_thoughts_regarding_the/,wtfzambo,1565166629,[removed],0,1
409,2019-8-7,2019,8,7,17,cn356b,[D] Why is DiscoGAN better at geometrical transformation when compared to CycleGAN ?,https://www.reddit.com/r/MachineLearning/comments/cn356b/d_why_is_discogan_better_at_geometrical/,phd_or_not,1565166685,"Hi All,  

&amp;#x200B;

CycleGAN and DiscoGAN are very similar in their functionality and seem to be concurrent works. The loss function of CycleGAN is L1 loss while DiscoGAN uses MSE. CycleGAN has an additional identity loss function. 

&amp;#x200B;

While CycleGAN produces impressive results on horse2zebra, it seems to fail at the task of cat2dog (geometric transformation). DiscoGAN, on the other hand, is able to perform the task of Handbags2Shoes. 

&amp;#x200B;

TL;DR: What makes DiscoGAN perform the geometrical transformation better than CycleGAN ? Is it the network architecture or the MSE loss function or is there is a secret sauce ?",12,18
410,2019-8-7,2019,8,7,17,cn35xe,Machine Learning Introduction,https://www.reddit.com/r/MachineLearning/comments/cn35xe/machine_learning_introduction/,andrea_manero,1565166860,[removed],0,1
411,2019-8-7,2019,8,7,17,cn37vc,NORMAL STEEL WIRES ON SALE,https://www.reddit.com/r/MachineLearning/comments/cn37vc/normal_steel_wires_on_sale/,ada2017,1565167288,,0,1
412,2019-8-7,2019,8,7,17,cn3c7l,"[D] While detecting nucleus, can we use some prior?",https://www.reddit.com/r/MachineLearning/comments/cn3c7l/d_while_detecting_nucleus_can_we_use_some_prior/,bay_der,1565168340,"Hi,

The default configurations of RCNN or Faster-RCNN are designed for VOC data-set. The data-set has many classes, and objects of different class are of different size. So, I think it is necessary to use different scales and zoom levels.

But in pathology images, all images are on same scale. Even the nuclei are comparable. So is there any particular configuration we should use for nucleus detection?  
Any intuition and advice would be helpful.",10,1
413,2019-8-7,2019,8,7,18,cn3dgt,"Pytorch Auto Encoder, Continuous values. NEEED HELP",https://www.reddit.com/r/MachineLearning/comments/cn3dgt/pytorch_auto_encoder_continuous_values_neeed_help/,RedEyedDog,1565168618,[removed],0,1
414,2019-8-7,2019,8,7,18,cn3h5l,"""Why Python Should Be The First Language You Should Learn""",https://www.reddit.com/r/MachineLearning/comments/cn3h5l/why_python_should_be_the_first_language_you/,susanvilleula1,1565169434,,0,1
415,2019-8-7,2019,8,7,18,cn3hb9,"Machine Learning Can Better Assess Heart Attack Risks, Mining the Myriad Details",https://www.reddit.com/r/MachineLearning/comments/cn3hb9/machine_learning_can_better_assess_heart_attack/,analyticsinsight,1565169470,,0,1
416,2019-8-7,2019,8,7,18,cn3k2a,"[Discussion] Is there previous research on assessment / evaluation model for franchises, credit status, etc?",https://www.reddit.com/r/MachineLearning/comments/cn3k2a/discussion_is_there_previous_research_on/,gilgarad,1565170021,"Hi all /r/machinelearning.

I've recently looked for the previous researches on evaluation model for franchises, credit status and etc, where a target has many activities (like transaction, order, etc). Credit status of a person is one good example. It has the transaction history and income info and can be used as dataset.

I couldn't find the one that seems fit to this subject. In my mind I may used a wrong keyword to find. What is the name of this field? It is not certainly NLP, or Image... 

I will appreciate for any paper you recommend or keywords to search through google, or even the subject name of the field.",2,1
417,2019-8-7,2019,8,7,18,cn3oww,machine learning concepts,https://www.reddit.com/r/MachineLearning/comments/cn3oww/machine_learning_concepts/,dhruv2707,1565171111,[removed],0,1
418,2019-8-7,2019,8,7,18,cn3rjg,[D] Is there a neuroscience / cognitive research equivalent to the relational inductive bias in machine learning?,https://www.reddit.com/r/MachineLearning/comments/cn3rjg/d_is_there_a_neuroscience_cognitive_research/,whiletrue2,1565171674,"The human decision-making is heavily influenced by beliefs, biases and heuristics. The decision-making in (inductive) machine learning algorithms is grounded in inductive biases. I was wondering if we can establish a connection between both. In particular, I am interested in bridging the gap for the following example:

Say, we have a few lego blocks on the table that are randomly arranged.  If we ask us humans to move one block passed another without interfering it, we will analyze structure in the perceptual input and decompose the scene into entities, relations and relational constraints. We will also access our knowledge/models about objects (Sperkle et al., ""Core knowledge"") and will use our beliefs and experience to find an appropriate solution.

Now, I would argue that if we ask the same thing a robot, we would require similar decision-making capabilities. I would further argue that we require two key components:  
1) a forward model of objects or the scene (to ""hallucinate"" consequences)

2) A relational inductive bias that allows to exploit structure and impose constraints on relations and interactions of entities during learning the forward model.

Assuming my assumptions are correct, I was wondering if I can make a connection between human decision-making and such a machine intelligence model. Is there something similar in human decision-making for the decomposition of a scene into entities and relations that is related to a machine learning (inductive) bias?

Thanks a lot!",0,9
419,2019-8-7,2019,8,7,19,cn3tii,[D] Open letter question for people working on automation,https://www.reddit.com/r/MachineLearning/comments/cn3tii/d_open_letter_question_for_people_working_on/,jmknmecrzy,1565172049,"Crosspost to /r/DataScience 
I am deeply worried about automation personally and it is one of the driving reasons. On par with climate change that I dont want to have any children. My question is: Is it possible to start an open letter from people working on the automation of the most common jobs in America. If we are actually automating these jobs I think this could help build actual awareness and recognition from the media.  

Some say its not possible to see job loss from automation. That other work we cant imagine will take their place. I hate to sound pessimistic but I dont really see that happening and/or I dont see the call center worker or the truck driver being able to retrain for the jobs of the future.

I saw someone in a different sub suggest an open letter where professionals could sign in support of the fact that this is in fact happening. None of the political candidates are talking about this. This is as important as the climate, if we have 25% unemployment riots I think thats going to be as bad or worse than rising sea levels. Do you think this could get traction? Or am I totally off base, sorry Im not a professional just a guy worried about the future.",1,0
420,2019-8-7,2019,8,7,19,cn3vlh,Terms to remember while building a Machine Learning Model,https://www.reddit.com/r/MachineLearning/comments/cn3vlh/terms_to_remember_while_building_a_machine/,mlait1908,1565172447,,0,1
421,2019-8-7,2019,8,7,19,cn3xyu,NVIDIA on our latest ML development,https://www.reddit.com/r/MachineLearning/comments/cn3xyu/nvidia_on_our_latest_ml_development/,philippbatura,1565172889,,0,1
422,2019-8-7,2019,8,7,19,cn40dv,Discover the Advantages of Augmented Analytics!,https://www.reddit.com/r/MachineLearning/comments/cn40dv/discover_the_advantages_of_augmented_analytics/,ElegantMicroWebIndia,1565173340,,0,1
423,2019-8-7,2019,8,7,20,cn4u8m,Artificial Tongue Acts similarly to a Human Tongue,https://www.reddit.com/r/MachineLearning/comments/cn4u8m/artificial_tongue_acts_similarly_to_a_human_tongue/,Wensosolutions,1565179041,[removed],0,1
424,2019-8-7,2019,8,7,20,cn4v32,"Too many parameters to tune, too little time",https://www.reddit.com/r/MachineLearning/comments/cn4v32/too_many_parameters_to_tune_too_little_time/,matigekunst,1565179197,[removed],0,1
425,2019-8-7,2019,8,7,21,cn4v9l,[P] Feature Engineer Optimization in HyperparameterHunter 3.0,https://www.reddit.com/r/MachineLearning/comments/cn4v9l/p_feature_engineer_optimization_in/,HunterMcGushion,1565179223,"A full description of the new feature engineering optimization capabilities can be found in this [Medium story](https://towardsdatascience.com/hyperparameter-hunter-feature-engineering-958966818b6e?source=friends_link&amp;sk=ec07a284c06b2f6a9306214f27775028).

TL;DR: [HyperparameterHunter](https://github.com/HunterMcGushion/hyperparameter_hunter) 3.0 adds support for feature engineering optimization. Define different feature engineering steps as normal functions, then let HyperparameterHunter keep track of the steps performed for Experiments, so you can optimize them just like normal hyperparameters, and learn from past Experiments automatically.

HyperparameterHunter is a scaffolding for ML experimentation and optimization. Run one-off Experiments or perform hyperparameter optimization, and HH automatically saves the model, hyperparameters, data, CV scheme, and now feature engineering steps, along with much more. Future optimization will scour your saved Experiments for those compatible with the current search space and use them to automatically jump-start learning.

* Stop keeping janky lists of all your Experiments conditions and results
* Ensure optimization actually has sufficient data to be useful
* Let no Experiment be wasted

If you love [HyperparameterHunter](https://github.com/HunterMcGushion/hyperparameter_hunter), Id like to ask you for your support (yes, you, the attractive one reading this). Starring our GitHub repo, applauding the [Medium story](https://towardsdatascience.com/hyperparameter-hunter-feature-engineering-958966818b6e?source=friends_link&amp;sk=ec07a284c06b2f6a9306214f27775028), and telling your friends (or enemies) about HyperparameterHunter would be very much appreciated!

If youd like to do more and offer some feedback, open an issue, or contribute code, I would treasure the opportunity to learn from experts such as yourselves!",23,84
426,2019-8-7,2019,8,7,21,cn4z8k,[D] Too many hyperparameters to tune too little time,https://www.reddit.com/r/MachineLearning/comments/cn4z8k/d_too_many_hyperparameters_to_tune_too_little_time/,matigekunst,1565179867,"I'm working on a model with heaps of hyperparameters. It is infeasible to test all combinations so I've come up with an attempt to tuning, but I don't know whether the method is valid. Say I have hyperparameters A, B, and C, each with 3, 4 and 5 options each. Now my plan is to set a baseline, say A:1, B:1, C:1. Then I vary the options of A keeping B and C constant. Hypothetically A:3, B:1, C:1 beats the initial baseline. Now I set A:3, B:1, C:1 to be my new baseline and I vary hyperparameter B. I repeat this process until all parameters have been varied. Then I start out with A again. The assumption here is that hyperparameters influence the performance which I know not to be true.

Can this method be seen as a genuine attempt to tuning? If it is: does anyone know of any references where this or a similar tuning method is used? If not: is there a better method? Furthermore, I'd like to know how you deal with having a lot of hyperparameters.",18,9
427,2019-8-7,2019,8,7,21,cn53wm,Learn all about the #SeasonAlpha Localisation event in ZalaZONE and stay tuned for more content coming up!,https://www.reddit.com/r/MachineLearning/comments/cn53wm/learn_all_about_the_seasonalpha_localisation/,AIthatDrives,1565180663,,0,1
428,2019-8-7,2019,8,7,21,cn5aaw,The Evolution Of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cn5aaw/the_evolution_of_machine_learning/,mlait1908,1565181697,,0,1
429,2019-8-7,2019,8,7,22,cn5nhw,"What would you choose for GPU programming? OpenCL, OpenGL, DirectCompute, Vulkan or CUDA? Is there another?",https://www.reddit.com/r/MachineLearning/comments/cn5nhw/what_would_you_choose_for_gpu_programming_opencl/,hauntedpoop,1565183748,[removed],0,1
430,2019-8-7,2019,8,7,22,cn5o8z,[R] InterFaceGAN: Interpreting Latent Space of GANs for Semantic Face Editing,https://www.reddit.com/r/MachineLearning/comments/cn5o8z/r_interfacegan_interpreting_latent_space_of_gans/,PuzzledProgrammer3,1565183861,"paper: [https://arxiv.org/pdf/1907.10786.pdf](https://arxiv.org/pdf/1907.10786.pdf)

code: [https://github.com/ShenYujun/InterFaceGAN](https://github.com/ShenYujun/InterFaceGAN)

youtube video: [https://www.youtube.com/watch?v=uoftpl3Bj6w](https://www.youtube.com/watch?v=uoftpl3Bj6w)

project page: [https://shenyujun.github.io/InterFaceGAN/](https://shenyujun.github.io/InterFaceGAN/)",3,17
431,2019-8-7,2019,8,7,22,cn62bw,ONNX runtime in C#,https://www.reddit.com/r/MachineLearning/comments/cn62bw/onnx_runtime_in_c/,drr21,1565185929,[removed],0,1
432,2019-8-7,2019,8,7,23,cn66sr,"Machine Learning is bleeding edge in its own right, but I've started learning about Quantum Unsupervised Learning. Imagine problems that today's supercomputers can't solve solved obscenely fast.",https://www.reddit.com/r/MachineLearning/comments/cn66sr/machine_learning_is_bleeding_edge_in_its_own/,Agent_ANAKIN,1565186578,,0,1
433,2019-8-7,2019,8,7,23,cn6bk9,How do you set up your ml pipeline?,https://www.reddit.com/r/MachineLearning/comments/cn6bk9/how_do_you_set_up_your_ml_pipeline/,HitLuca,1565187229,[removed],0,1
434,2019-8-7,2019,8,7,23,cn6cqo,Explainable ML,https://www.reddit.com/r/MachineLearning/comments/cn6cqo/explainable_ml/,DataScientist216,1565187398,[removed],0,1
435,2019-8-7,2019,8,7,23,cn6i3g,KDD 2019 Announces Best Paper Awards,https://www.reddit.com/r/MachineLearning/comments/cn6i3g/kdd_2019_announces_best_paper_awards/,Yuqing7,1565188148,,0,1
436,2019-8-7,2019,8,7,23,cn6mep,"[Research] SlimYOLOv3: Narrower, Faster and Better for Real-Time UAV Applications",https://www.reddit.com/r/MachineLearning/comments/cn6mep/research_slimyolov3_narrower_faster_and_better/,cdossman,1565188740,"Abstract: Drones or general Unmanned Aerial Vehicles (UAVs), endowed with computer vision function by onboard cameras and embedded systems, have become popular in a wide range of applications. However, real-time scene parsing through object detection running on a UAV platform is very challenging, due to limited memory and computing power of embedded devices. To deal with these challenges, in this paper we propose to learn efficient deep object detectors through channel pruning of convolutional layers. To this end, we enforce channel-level sparsity of convolutional layers by imposing L1 regularization on channel scaling factors and prune less informative feature channels to obtain slim object detectors. Based on such approach, we present SlimYOLOv3 with fewer trainable parameters and floating point operations (FLOPs) in comparison of original YOLOv3 (Joseph Redmon et al., 2018) as a promising solution for real-time object detection on UAVs. We evaluate SlimYOLOv3 on VisDrone2018-Det benchmark dataset; compelling results are achieved by SlimYOLOv3 in comparison of unpruned counterpart, including \~90.8% decrease of FLOPs, \~92.0% decline of parameter size, running \~2 times faster and comparable detection accuracy as YOLOv3. Experimental results with different pruning ratios consistently verify that proposed SlimYOLOv3 with narrower structure are more efficient, faster and better than YOLOv3, and thus are more suitable for real-time object detection on UAVs 

 [https://medium.com/@cdossman/slimyolov3-narrower-faster-and-better-for-real-time-uav-applications-ad3c5b8a2cf9](https://medium.com/@cdossman/slimyolov3-narrower-faster-and-better-for-real-time-uav-applications-ad3c5b8a2cf9)",3,9
437,2019-8-7,2019,8,7,23,cn6rq7,What do you guys think of this schedule for a community of Tech Talents? (Looking for feedback on the activities we run),https://www.reddit.com/r/MachineLearning/comments/cn6rq7/what_do_you_guys_think_of_this_schedule_for_a/,mateorandulfe,1565189478,,1,1
438,2019-8-7,2019,8,7,23,cn6s4k,[D] DeepMind Takes on Billion-Dollar Debt and Loses $572 Million,https://www.reddit.com/r/MachineLearning/comments/cn6s4k/d_deepmind_takes_on_billiondollar_debt_and_loses/,Boom_Various,1565189536,"DeepMind, the artificial-intelligence company owned by Google parent Alphabet Inc., saw its revenue almost double last year, but gains were dwarfed by losses that increased to hundreds of millions of dollars.

The London-based company also has more than a billion dollars of debt due for repayment this year, according to full-year accounts for the year ended Dec. 31 posted to U.K. business registry Companies House.

Losses for 2018 widened to 470.2 million pounds ($572 million) from 302.2 million pounds in 2017. Revenue rose to 102.8 million pounds, up from 54.4 million pounds. Staff costs also nearly doubled against the year-ago period to 398 million pounds in 2018.

A debt of 1.04 billion pounds due this year includes an 883 million-pound loan from its owner. DeepMind had written assurances it would be financially supported for at least another year.

Our DeepMind for Google team continues to make great strides bringing our expertise and knowledge to real-world challenges at Google scale, nearly doubling revenue in the past year, a spokeswoman for the company said in a statement. We will continue to invest in fundamental research and our world-class, interdisciplinary team, and look forward to the breakthroughs that lie ahead.

Alphabet Inc. bought DeepMind for 400 million pounds in 2014. The next year, the company began working on health-care research, eventually creating an entire division dedicated to the area.

The company works with the U.K. National Health Service hospitals, researching algorithms that can diagnose eye diseases and spot head and neck cancers from medical imagery, and the U.S. Department of Veterans Affairs on an algorithm that can predict which patients are at risk of sudden deterioration from acute kidney injury and other conditions.

&amp;#x200B;

[https://www.bloomberg.com/news/articles/2019-08-07/alphabet-s-deepmind-takes-on-billion-dollar-debt-as-loss-spirals](https://www.bloomberg.com/news/articles/2019-08-07/alphabet-s-deepmind-takes-on-billion-dollar-debt-as-loss-spirals?utm_source=google&amp;utm_medium=bd&amp;cmpId=google)",207,318
439,2019-8-7,2019,8,7,23,cn6tje,[Discussion] Discussion of machine learning outside of text/image domains,https://www.reddit.com/r/MachineLearning/comments/cn6tje/discussion_discussion_of_machine_learning_outside/,Trailqul,1565189731,"It seems like a lot of high-impact machine learning research has recently involved work in image or text domains (or perhaps this is mostly what I'm exposed to because I read a lot of deep network papers). Are people here familiar with high-impact work/papers outside of these domains? Do you think the work in these areas is actually less frequent or impactful, or is that perhaps a perceptual bias due to the popularity of things like deep network approaches?",4,1
440,2019-8-8,2019,8,8,0,cn6vze,Are there examples of using QA systems to determine if an answer given is feasible?,https://www.reddit.com/r/MachineLearning/comments/cn6vze/are_there_examples_of_using_qa_systems_to/,marimbawizard,1565190082,[removed],0,1
441,2019-8-8,2019,8,8,0,cn6w1g,"[R] ""This enables training strong classifiers using small training images...To the best of our knowledge this is the highest ImageNet single-crop, top-1 and top-5 accuracy to date."" - Fixing the train-test resolution discrepancy",https://www.reddit.com/r/MachineLearning/comments/cn6w1g/r_this_enables_training_strong_classifiers_using/,downtownslim,1565190089,,1,7
442,2019-8-8,2019,8,8,0,cn6zwj,ML Noob trying to finding the next GPS position of animals..,https://www.reddit.com/r/MachineLearning/comments/cn6zwj/ml_noob_trying_to_finding_the_next_gps_position/,ruant,1565190586,[removed],1,1
443,2019-8-8,2019,8,8,0,cn75fx,[D] Are there examples of using QA systems to determine if an answer given is feasible?,https://www.reddit.com/r/MachineLearning/comments/cn75fx/d_are_there_examples_of_using_qa_systems_to/,marimbawizard,1565191316," Say I was using Bert and trained it on Squad 2.0 (which I have done) and came across:

Question: What is your favorite color?

Possible (Correct) Answer given:Blue

Possible (Wrong) Answer given:Lasagna

The idea would be that a model would predict Blue on the first one and nothing on the second one (implying it was not a feasible answer).

Is there any research or ideas on how (if possible) you could train a model like a Bert to do that? I feel like it should be doable however my current results with training on Squad 2.0 were not extremely promising so I'm not sure if I'm not thinking about it problem correct or if there is some research out there on how to better approach this.",5,2
444,2019-8-8,2019,8,8,0,cn76tz,MILA Programs,https://www.reddit.com/r/MachineLearning/comments/cn76tz/mila_programs/,ElectricGypsyAT,1565191488,[removed],0,1
445,2019-8-8,2019,8,8,0,cn770j,Neural networks used to raise resolution in microscopy,https://www.reddit.com/r/MachineLearning/comments/cn770j/neural_networks_used_to_raise_resolution_in/,jimisommer,1565191510,,0,1
446,2019-8-8,2019,8,8,0,cn78rp,Lead Generation: How to tackle this problem?,https://www.reddit.com/r/MachineLearning/comments/cn78rp/lead_generation_how_to_tackle_this_problem/,Arthurion9,1565191753,[removed],0,1
447,2019-8-8,2019,8,8,0,cn78te,Number of images per class in MSCOCO,https://www.reddit.com/r/MachineLearning/comments/cn78te/number_of_images_per_class_in_mscoco/,nkalavak,1565191759,[removed],0,1
448,2019-8-8,2019,8,8,0,cn79wn,[D] how do you setup your ml pipeline?,https://www.reddit.com/r/MachineLearning/comments/cn79wn/d_how_do_you_setup_your_ml_pipeline/,HitLuca,1565191906,"Hi guys, I have what could be a stupid question, but I see that I'm encountering this issue regularly and would like to know your opinion: so yesterday I was trying to improve my ML model in order to improve its accuracy, and found out that it was performing worse. Why? I checked the previous model architecture (saved with Keras plot\_model) and saw what I did differently last week. No problem, I will just revert to that architecture and test again. Model overfits in half the epochs now. Damn, I also changed the dataset augmentation pipeline, now I cannot recreate those specific scores.

&amp;#x200B;

Basically this is my issue, I happen to develop a model for n-days, test it, save it etc. then after a couple of weeks I try to revert to ""that good model setup I was having"" and I cannot get the same results anymore as I changed too much stuff. I marginally fixed it by saving the model architecture as png using Keras in order to have a quick visual comparison, It's not the end of the world, but I don't have a clean way to deal with this issue. How do you guys avoid such problems?

&amp;#x200B;

Thank you!",23,7
449,2019-8-8,2019,8,8,0,cn7bn5,Chinese Video Streaming Giant Introduces Anime Facial ID Dataset,https://www.reddit.com/r/MachineLearning/comments/cn7bn5/chinese_video_streaming_giant_introduces_anime/,Yuqing7,1565192126,,0,1
450,2019-8-8,2019,8,8,0,cn7ido,Solution manual for Foundations of Machine Learning by Mohri,https://www.reddit.com/r/MachineLearning/comments/cn7ido/solution_manual_for_foundations_of_machine/,path_finder5,1565193013,[removed],0,1
451,2019-8-8,2019,8,8,0,cn7j8o,How to approach a machine learning task?,https://www.reddit.com/r/MachineLearning/comments/cn7j8o/how_to_approach_a_machine_learning_task/,Meseriasul,1565193132,[removed],0,1
452,2019-8-8,2019,8,8,0,cn7joj,"Simple Questions Thread August 07, 2019",https://www.reddit.com/r/MachineLearning/comments/cn7joj/simple_questions_thread_august_07_2019/,AutoModerator,1565193183,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",0,1
453,2019-8-8,2019,8,8,1,cn7z8r,Most Popular Programmers In The World | MLAIT,https://www.reddit.com/r/MachineLearning/comments/cn7z8r/most_popular_programmers_in_the_world_mlait/,mlait1908,1565195144,,0,1
454,2019-8-8,2019,8,8,1,cn83el,Youre beginner ? this is why you should learn Python,https://www.reddit.com/r/MachineLearning/comments/cn83el/youre_beginner_this_is_why_you_should_learn_python/,sajad-52,1565195704,,0,0
455,2019-8-8,2019,8,8,2,cn8h3s,Continious or discrete action space?,https://www.reddit.com/r/MachineLearning/comments/cn8h3s/continious_or_discrete_action_space/,confusedguy1212,1565197485,[removed],0,1
456,2019-8-8,2019,8,8,2,cn8rui,LSTM train basic log2 math function,https://www.reddit.com/r/MachineLearning/comments/cn8rui/lstm_train_basic_log2_math_function/,alfredsoeng,1565198842,"I am a ML beginner. I tried to train a LSTM model to predict log2 function, but the result is too inaccurate. Could anyone help me take a look how I should change? I tried to tune the hyper parameters, but still didn't have any luck. Thanks so much. for any help.

    import numpy as np
    import tensorflow as tf
    import matplotlib.pyplot as plt
    
    
    NUM_EPOCH = 1000
    HIDDEN_SIZE = 30
    NUM_LAYERS = 2
    TIMESTEPS = 10
    TRAINING_STEPS = 20000
    BATCH_SIZE = 20
    TRAINING_EXAMPLES = 10000
    TESTING_EXAMPLES = 1000
    SAMPLE_GAP = 0.01
    
    
    def generate_data(seq):
        X = []
        y = []
        for i in range(len(seq) - TIMESTEPS):
            X.append([seq[i: i + TIMESTEPS]])
            y.append([seq[i + TIMESTEPS]])
        return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)
    
    
    def lstm_model(X, y, is_training):
        cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(HIDDEN_SIZE) for _ in range(NUM_LAYERS)])
        outputs, _ = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)
        output = outputs[:, -1, :]
        predictions = tf.contrib.layers.fully_connected(output, 1, activation_fn=None)
        if not is_training:
            return predictions, None, None
        loss = tf.losses.mean_squared_error(labels=y, predictions=predictions)
        train_op = tf.contrib.layers.optimize_loss(
            loss, tf.train.get_global_step(), optimizer='Adagrad', learning_rate=0.1)
        return predictions, loss, train_op
    
    
    def train(sess, train_X, train_Y):
        ds = tf.data.Dataset.from_tensor_slices((train_X, train_Y))
        ds = ds.repeat().shuffle(1000).batch(BATCH_SIZE)
        X, y = ds.make_one_shot_iterator().get_next()
        losses = np.array([])
    
        with tf.variable_scope('model'):
            predictions, loss, train_op = lstm_model(X, y, True)
        sess.run(tf.global_variables_initializer())
        for i in range(TRAINING_STEPS):
            _, l = sess.run([train_op, loss])
            losses = np.append(losses, l)
            if i % NUM_EPOCH == 0:
                print('train step: ' + str(i) + ', loss: ' + str(l))
    
        plt.figure()
        plt.plot(losses, label='loss')
        plt.legend()
        # plt.show()
        plt.savefig('./test_outputs/loss.png')
    
    
    def run_eval(sess, test_X, test_y):
        ds = tf.data.Dataset.from_tensor_slices((test_X, test_y))
        ds = ds.batch(1)
        X, y = ds.make_one_shot_iterator().get_next()
        with tf.variable_scope('model', reuse=True):
            prediction, _, _ = lstm_model(X, [0, 0], False)
        predictions = []
        labels = []
        for i in range(int(TESTING_EXAMPLES / 2)):
            p, l = sess.run([prediction, y])
            predictions.append(p)
            labels.append(l)
    
        predictions = np.array(predictions).squeeze()
        labels = np.array(labels).squeeze()
        rmse = np.sqrt(((predictions - labels) ** 2).mean(axis=0))
        print('Mean Square Error is: %f' % rmse)
    
        plt.figure()
        print(predictions[:15])
        print(labels[:15])
        plt.plot(predictions, label='predictions')
        plt.plot(labels, label='real_val')
        plt.legend()
        # plt.show()
        plt.savefig('./test_outputs/test.png')
    
    
    test_start = (TRAINING_EXAMPLES + TIMESTEPS) * SAMPLE_GAP + 1
    test_end = test_start + (TESTING_EXAMPLES + TIMESTEPS) * SAMPLE_GAP + 1
    
    train_X, train_y = generate_data(np.log2(np.linspace(
        SAMPLE_GAP, test_start, TRAINING_EXAMPLES + TIMESTEPS, dtype=np.float32)))
    test_X, test_y = generate_data(np.log2(np.linspace(
        test_start, test_end, TESTING_EXAMPLES + TIMESTEPS, dtype=np.float32)))
    
    
    x_val, test_X = np.split(test_X, 2)
    y_val, test_y = np.split(test_y, 2)
    
    with tf.Session() as sess:
        train(sess, train_X, train_y)
        run_eval(sess, test_X, test_y)

&amp;#x200B;

output:

    train step: 0, loss: 6.226374 train step: 1000, loss: 0.50559336 train step: 2000, loss: 0.008770825 train step: 3000, loss: 0.006236521 train step: 4000, loss: 0.001877052 train step: 5000, loss: 0.0035174307 train step: 6000, loss: 0.0012169601 train step: 7000, loss: 0.001992577 train step: 8000, loss: 0.00071869115 train step: 9000, loss: 0.0015831447 train step: 10000, loss: 0.0015313074 train step: 11000, loss: 0.0011248669 train step: 12000, loss: 0.0009140003 train step: 13000, loss: 0.0008928403 train step: 14000, loss: 0.00045768628 train step: 15000, loss: 0.000446374 train step: 16000, loss: 0.0010496832 train step: 17000, loss: 0.0008943161 train step: 18000, loss: 0.0010290025 train step: 19000, loss: 0.0005238621 Mean Square Error is: 0.052667 [6.6955047 6.6956124 6.69572 6.695827 6.6959357 6.6960425 6.69615 6.6962576 6.6963654 6.696472 6.696579 6.6966867 6.696795 6.696902 6.697009 ] [6.7375584 6.737707 6.737856 6.7380047 6.738153 6.7383018 6.7384505 6.7385993 6.7387476 6.7388964 6.739045 6.7391934 6.739342 6.7394905 6.7396393]

&amp;#x200B;

Loss output:

https://i.redd.it/136ve8q0a2f31.png

Predict Output:

&amp;#x200B;

https://i.redd.it/7wuahrk5a2f31.png",0,1
457,2019-8-8,2019,8,8,2,cn8txj,Resources for regularization,https://www.reddit.com/r/MachineLearning/comments/cn8txj/resources_for_regularization/,1pk732,1565199112,[removed],0,1
458,2019-8-8,2019,8,8,2,cn8y01,"Researchers reveal AI weaknesses by developing more than 1,200 questions that, while easy for people to answer, stump the best computer answering systems today. The system that learns to master these questions will have a better understanding of language. Videos of human-computer matches available.",https://www.reddit.com/r/MachineLearning/comments/cn8y01/researchers_reveal_ai_weaknesses_by_developing/,ezubaric,1565199621,,62,324
459,2019-8-8,2019,8,8,3,cn9eqd,[Research] Temporal Attentive Alignment for Large-Scale Video Domain Adaptation (ICCV 2019 Oral),https://www.reddit.com/r/MachineLearning/comments/cn9eqd/research_temporal_attentive_alignment_for/,cmhung34,1565201690,"Hello,

It's my pleasure to share our recent work on Video Domain Adaptation with you!

We proposed large-scale cross-domain action datasets, and developed an attention-based spatio-temporal DA mechanism to achieve effective domain alignment.

&amp;#x200B;

Temporal Attentive Alignment for Large-Scale Video Domain Adaptation (ICCV 2019 Oral)

\[GitHub\] [https://github.com/cmhungsteve/TA3N](https://github.com/cmhungsteve/TA3N)

\[arXiv\] [https://arxiv.org/abs/1907.12743](https://arxiv.org/abs/1907.12743)

&amp;#x200B;

Feel free to share with others :)",2,1
460,2019-8-8,2019,8,8,3,cn9i48,[P] Self-hosted ML deployment platform,https://www.reddit.com/r/MachineLearning/comments/cn9i48/p_selfhosted_ml_deployment_platform/,ospillinger,1565202103,"Hi everyone,

I'm building a platform for deploying machine learning in production. It takes exported models (TensorFlow, PyTorch, XGBoost, etc), deploys them as web APIs, and handles things like autoscaling, log streaming, and inference on CPUs or GPUs. It's open source and designed to be self-hosted on AWS ([GitHub](https://github.com/cortexlabs/cortex) / [website](https://cortex.dev/)).

Id love to hear your feedback!",0,0
461,2019-8-8,2019,8,8,3,cn9yux,Can I train models on images without having to download them first?,https://www.reddit.com/r/MachineLearning/comments/cn9yux/can_i_train_models_on_images_without_having_to/,sierrafourteen,1565204243,[removed],0,1
462,2019-8-8,2019,8,8,4,cnanbv,[D] Match HF Sounds to Subspecies?,https://www.reddit.com/r/MachineLearning/comments/cnanbv/d_match_hf_sounds_to_subspecies/,Smackergeddon,1565207339,"I'm a software developer, but have yet to delve into ML. I have a library of high frequency sounds grouped by an animal subspecies. I have created a spectrum analyzer to gather data.  What tool should I use to train a machine to match it to the correct species?",0,1
463,2019-8-8,2019,8,8,4,cnasg9,Blog post: Deploying a scikit-learn model with ONNX und FastAPI,https://www.reddit.com/r/MachineLearning/comments/cnasg9/blog_post_deploying_a_scikitlearn_model_with_onnx/,9naxty5,1565207979,[removed],0,1
464,2019-8-8,2019,8,8,5,cnb2f0,[D] Where to get SemEval-2015 Task 1: Paraphrase and Semantic Similarity in Twitter (PIT) Dataset?,https://www.reddit.com/r/MachineLearning/comments/cnb2f0/d_where_to_get_semeval2015_task_1_paraphrase_and/,AnonMLstudent,1565209203,"We are investigating work embeddings and require the entire dataset including test set with labels. I have noticed many papers (even in the last year) that use this dataset, however, all Google searches lead to broken links, broken emails, and absolutely nowhere can I find this entire dataset lol. 

Some links I have tried are below:
http://alt.qcri.org/semeval2015/task1/
https://github.com/cocoxu/SemEval-PIT2015
The links above do not contain the full datasets, and the test set in the first link does not have labels.

Any help is greatly appreciated.",0,2
465,2019-8-8,2019,8,8,5,cnb4ub,My bad ai,https://www.reddit.com/r/MachineLearning/comments/cnb4ub/my_bad_ai/,Melon-Aaron,1565209504,[removed],0,1
466,2019-8-8,2019,8,8,5,cnb5aq,"[Research] Robust Lane detection and tracking framework for Autonomous Vehicles(Indian Roads) using Deep CNN, Ext. Hough Transform and Kalman Filter.",https://www.reddit.com/r/MachineLearning/comments/cnb5aq/research_robust_lane_detection_and_tracking/,ayush0016,1565209562,"Being part of the perception team at an Autonomous Vehicle research lab I had been working on the development of the lane detection and tracking module for the vehicle catering to Indian road scenario. What made the project challenging were many factors unique to the Indian landscape like highly weathered lanes and unusually congested traffic problems which took the project close to 8 months to complete.The framework is trained and evaluated on the data collected by our experimental vehicle on Indian roads. The dataset consists of a total of 4500 frames with varying driving scenarios, including highways, urban roads, traffic, shadowed lanes, partially visible lanes and curved lanes.

Project Page/Github link : [https://github.com/ayush1997/Robust-Lane-Detection-and-Tracking](https://github.com/ayush1997/Robust-Lane-Detection-and-Tracking)",0,1
467,2019-8-8,2019,8,8,5,cnb8cv,"[R] Robust Lane detection and tracking framework for Autonomous Vehicles(Indian Roads) using Deep CNN, Ext. Hough Transform and Kalman Filter.",https://www.reddit.com/r/MachineLearning/comments/cnb8cv/r_robust_lane_detection_and_tracking_framework/,ayush0016,1565209961,"Being part of the perception team at an Autonomous Vehicle research lab I had been working on the development of the lane detection and tracking module for the vehicle catering to Indian road scenario. What made the project challenging were many factors unique to the Indian landscape like highly weathered lanes and unusually congested traffic problems which took the project close to 8 months to complete.The framework is trained and evaluated on the data collected by our experimental vehicle on Indian roads. The dataset consists of a total of 4500 frames with varying driving scenarios, including highways, urban roads, traffic, shadowed lanes, partially visible lanes and curved lanes.

Project Page/Github link : [https://github.com/ayush1997/Robust-Lane-Detection-and-Tracking](https://github.com/ayush1997/Robust-Lane-Detection-and-Tracking)",2,8
468,2019-8-8,2019,8,8,5,cnb8g1,What course on ML could I get for ~200$?,https://www.reddit.com/r/MachineLearning/comments/cnb8g1/what_course_on_ml_could_i_get_for_200/,Bomull,1565209974,"I have USD 225 (or 200) to put in further professional training (I work as a developer), and I thought I could maybe get some online course on ML, but I need recommendations. I know the basics, but I haven't built any model yet. I would appreciate for any recommendations for good courses!",0,1
469,2019-8-8,2019,8,8,6,cnbxgt,Trying to understand batch normalization,https://www.reddit.com/r/MachineLearning/comments/cnbxgt/trying_to_understand_batch_normalization/,cooperbaerseth,1565213210,"I have a basic/fundamental question that I think I've glossed over in the past and would now like to look at more rigorously. 

&amp;#x200B;

For neural networks, why is it so important that we keep the distribution of the data constant as we proceed with training/validation/testing? This basic fact is what batch normalization seems to be based on, but I don't really understand the reason why it's so important. Intuitively or mathematically. 

&amp;#x200B;

Also, any text that explains the idea is very much appreciated. Thanks!!",0,1
470,2019-8-8,2019,8,8,7,cncksp,Medical Image Classification,https://www.reddit.com/r/MachineLearning/comments/cncksp/medical_image_classification/,moizsawan,1565216343,[removed],0,1
471,2019-8-8,2019,8,8,7,cncsno,[D] Help with fine-tuning for text classification task,https://www.reddit.com/r/MachineLearning/comments/cncsno/d_help_with_finetuning_for_text_classification/,snowcrashed617,1565217404,"Hi r/MachineLearning,

Testing out fine-tuning BERT and ULMFit for text classification. I've followed various tutorials using FastAI and PyTorch, but haven't yet gotten good results at all - would love to get some input and see if my approach to this problem is reasonable.

My problem is take a short snippet of text - anywhere from 10 - 200 characters and predict one from 2,510 categories that represent words (one-hot vector). I'm using CrossEntropyLoss for the problem and both BERT and ULMFit do not seem to be doing great after fine-tuning - I can't even get BERT begin to make non-naive predictions. I believe this is due to the large number of classes, but I had thought there would be enough signal in the text for BERT to make a guess at one of the categories after seeing some examples. It ends up just predicting the most common class.

I have \~10MM samples for the data and I've been testing to hopefully see some basic results on \~500K examples. Is this reasonable? Should I just try throwing all of the data in there and see what happens?

My data is fairly messy as well since it's raw text, should I be cleaning it much? Haven't found many recommendations on this front yet and all of the SOTA metrics appear to be on clean text afaik. As far as spelling goes as well wondering if that's an issue.

Any comments/thoughts on the approach? If it would help I can post the code (can't post the data unfortunately), but it's mostly copied from some tutorials with a few modifications so don't know how much that will help.

Thanks in advance for the help.",0,1
472,2019-8-8,2019,8,8,7,cncy9j,The Blowjob Paper,https://www.reddit.com/r/MachineLearning/comments/cncy9j/the_blowjob_paper/,ginger_beer_m,1565218181,,0,1
473,2019-8-8,2019,8,8,7,cnczf5,[R] The HSIC Bottleneck: Deep Learning without Back-Propagation,https://www.reddit.com/r/MachineLearning/comments/cnczf5/r_the_hsic_bottleneck_deep_learning_without/,PlentifulCoast,1565218335,,31,10
474,2019-8-8,2019,8,8,9,cne5st,Finite-Time Performance Bounds and Adaptive Learning Rate Selection for Two Time-Scale Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/cne5st/finitetime_performance_bounds_and_adaptive/,bulletbolt,1565224497,,1,4
475,2019-8-8,2019,8,8,11,cnfies,Which can be most efficient classifier for NLP?,https://www.reddit.com/r/MachineLearning/comments/cnfies/which_can_be_most_efficient_classifier_for_nlp/,h_m_madhu,1565232046,,0,1
476,2019-8-8,2019,8,8,11,cnflix,[R] SentiMATE: Learning to play Chess through Natural Language Processing,https://www.reddit.com/r/MachineLearning/comments/cnflix/r_sentimate_learning_to_play_chess_through/,hardmaru,1565232538,,3,27
477,2019-8-8,2019,8,8,12,cnfs21,[R] Word2vec to behavior: morphology facilitates the grounding of language in machines,https://www.reddit.com/r/MachineLearning/comments/cnfs21/r_word2vec_to_behavior_morphology_facilitates_the/,hardmaru,1565233605,,2,3
478,2019-8-8,2019,8,8,12,cnfsfz,Impostor Syndrome is real in Data Science and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cnfsfz/impostor_syndrome_is_real_in_data_science_and/,suryaavala,1565233662,,0,1
479,2019-8-8,2019,8,8,12,cng342,"Machine Learning: An Algorithmic Perspective, Second Edition PDF",https://www.reddit.com/r/MachineLearning/comments/cng342/machine_learning_an_algorithmic_perspective/,psychonekk,1565235442,,0,1
480,2019-8-8,2019,8,8,12,cngafl,Best Machine Learning Course,https://www.reddit.com/r/MachineLearning/comments/cngafl/best_machine_learning_course/,dwivediabhinav,1565236587,,0,1
481,2019-8-8,2019,8,8,13,cngck0,An NLP Analysis of the Mueller Testimony,https://www.reddit.com/r/MachineLearning/comments/cngck0/an_nlp_analysis_of_the_mueller_testimony/,sasa1163,1565236934,,0,1
482,2019-8-8,2019,8,8,13,cngdf7,convNetQuake overfitting,https://www.reddit.com/r/MachineLearning/comments/cngdf7/convnetquake_overfitting/,arjundupa,1565237060,"I am training the convNetQuake architecture ([https://advances.sciencemag.org/content/advances/4/2/e1700578.full.pdf](https://advances.sciencemag.org/content/advances/4/2/e1700578.full.pdf)) on time-series data for binary classification.

Here's the architecture in PyTorch:

    class ConvNetQuake(nn.Module):
        def __init__(self):
            super(ConvNetQuake, self).__init__()
            
            self.conv1 = nn.Conv1d(in_channels=2, out_channels=32, kernel_size=3, stride=2, padding=1)
            self.conv2 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)
            self.conv3 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)
            self.conv4 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)
            self.conv5 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)
            self.conv6 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)
            self.conv7 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)
            self.conv8 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)
            self.linear = nn.Linear(128, 1)
            self.sigmoid = nn.Sigmoid()
    
        def forward(self, x):
            x = F.relu((self.conv1(x)))
            x = F.relu((self.conv2(x)))
            x = F.relu((self.conv3(x)))
            x = F.relu((self.conv4(x)))
            x = F.relu((self.conv5(x)))
            x = F.relu((self.conv6(x)))
            x = F.relu((self.conv7(x)))
            x = F.relu((self.conv8(x)))
            # After the eighth convolution, the features are flattened into a 1D vector of 128 features.
            x = torch.reshape(x, (10, -1))
            # A fully connected layer outputs the class scores
            x = self.linear(x)
            x = self.sigmoid(x)
    
            return x

My issue is that this base model is overfitting on my binary classification task. Here are some accuracies I obtained:

|Model|Accuracy (%)|
|:-|:-|
|base\_model|81/82|
|base\_model + batchNorm()|**87/88**|
|base\_model + dropout(0.1)|81/82|
|base\_model + label\_smoothing|82/83|
|base\_model + batchNorm() + dropout(0.1)|82/83|
|base\_model + batchNorm() + label\_smoothing|85|
|smaller base\_model (6 layers)|81/82|
|smaller base\_model (4 layers)|72/73|

All of these models overfit, despite the effort to combat this overfitting. Any ideas about what else could be tried?

Any ideas will be much appreciated, thanks!",0,1
483,2019-8-8,2019,8,8,13,cngrpz,Why aren't you brainboxes using Julia instead of Python?,https://www.reddit.com/r/MachineLearning/comments/cngrpz/why_arent_you_brainboxes_using_julia_instead_of/,sicp4lyfe,1565239381,[removed],0,1
484,2019-8-8,2019,8,8,15,cnhjrv,What are some good resources from where I can start studying reinforcement learning from scratch?,https://www.reddit.com/r/MachineLearning/comments/cnhjrv/what_are_some_good_resources_from_where_i_can/,Ritvik19,1565244315,[removed],0,1
485,2019-8-8,2019,8,8,15,cnhntr,Can you please suggest me some intermediate level datascience and machine learning projects?,https://www.reddit.com/r/MachineLearning/comments/cnhntr/can_you_please_suggest_me_some_intermediate_level/,Ritvik19,1565245045,[removed],0,1
486,2019-8-8,2019,8,8,15,cnhoqg,What it means to have Live Chat,https://www.reddit.com/r/MachineLearning/comments/cnhoqg/what_it_means_to_have_live_chat/,getengati,1565245214,[removed],0,1
487,2019-8-8,2019,8,8,15,cnhvt9,Invitation to KaggleExchange,https://www.reddit.com/r/MachineLearning/comments/cnhvt9/invitation_to_kaggleexchange/,DesignerBe18,1565246522,[removed],0,1
488,2019-8-8,2019,8,8,15,cnhvw8,"[D] Keras vs tensorflow: Performance, GPU utilization and data pipeline",https://www.reddit.com/r/MachineLearning/comments/cnhvw8/d_keras_vs_tensorflow_performance_gpu_utilization/,ixeption,1565246537,"Hi folks,

I was recently dealing with some performance issues related to the keras image preprocessing. After several experiments, I  thought it would might be helpful to share my insights. There are several possible fixes:

* update all packages, especially keras-preprocessing.
* Deactivate your virus scanner (whitelist your data folder) and check if you have an internal SSD. 
* Try to tweak the configuration on fit\_generator (workers and queue\_size). If you are using linux try out multiprocessing and a thread-safe generator. 
* Convert your dataset to TFrecords and use it with keras or directly move to tensorflow. If you already using tensorflow 2.0, you can directly fit keras models on TFRecord datasets. 

Furthermore the tensorflow implementaion was always (slightly) faster.

[Here is a more detailed explaination.](http://digital-thinking.de/tensorflow-vs-keras-or-how-to-speed-up-your-training-for-image-data-sets-by-factor-10/)

Cheers",2,4
489,2019-8-8,2019,8,8,15,cnhweo,Phd Services,https://www.reddit.com/r/MachineLearning/comments/cnhweo/phd_services/,easysynopsis,1565246628,[removed],0,1
490,2019-8-8,2019,8,8,16,cni1pd,[R] U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation,https://www.reddit.com/r/MachineLearning/comments/cni1pd/r_ugatit_unsupervised_generative_attentional/,hardmaru,1565247618,,2,3
491,2019-8-8,2019,8,8,16,cni8qa,what's quick and best object detection algorithm?,https://www.reddit.com/r/MachineLearning/comments/cni8qa/whats_quick_and_best_object_detection_algorithm/,dragon518,1565248906,[removed],0,1
492,2019-8-8,2019,8,8,16,cniaj3,Building A Simple Neural Network in Tensorflow | MLAIT,https://www.reddit.com/r/MachineLearning/comments/cniaj3/building_a_simple_neural_network_in_tensorflow/,mlait1908,1565249239,,0,1
493,2019-8-8,2019,8,8,16,cnibwh,What is mean of Doing machine learning with Live Data?,https://www.reddit.com/r/MachineLearning/comments/cnibwh/what_is_mean_of_doing_machine_learning_with_live/,Ashishpatel26,1565249529,"What is mean of Doing machine learning with Live Data?

&amp;#x200B;

https://i.redd.it/zvbe3kcvg6f31.jpg

I heard from most of the people, you have to design machine learning model the way that it's train live data. But, I am so confuse with this statement how to design this kind of model which train a data on real time stream. I knew there are many option which is used for same problem like Apache Spark, Sparksql. But, I don't know how they are doing training on live data.

&amp;#x200B;

https://i.redd.it/31g4nz6wg6f31.jpg

Any one can please help with your answer? If you have experience with this kind of problem.",0,1
494,2019-8-8,2019,8,8,16,cnievy,How should a new one approach machine learning task?,https://www.reddit.com/r/MachineLearning/comments/cnievy/how_should_a_new_one_approach_machine_learning/,Meseriasul,1565250103,[removed],0,1
495,2019-8-8,2019,8,8,16,cnig0l,[Discussion] Would you use an app that lets you collect and label data from your phone?,https://www.reddit.com/r/MachineLearning/comments/cnig0l/discussion_would_you_use_an_app_that_lets_you/,Rtzon,1565250314,"Sometimes when I'm doing machine learning experiments, I often end up collecting my own data- image, audio, video, etc. Would other people find an app that lets you collect and label image/audio/video data right from your phone? Auto-export CSVs, labeled folders, etc.",4,10
496,2019-8-8,2019,8,8,17,cnip4m,[P] Abusing text synthesis into SVG image generation for fun and art,https://www.reddit.com/r/MachineLearning/comments/cnip4m/p_abusing_text_synthesis_into_svg_image/,shoeblade,1565252167,"Released a little utility earlier this year where you can feed raw SVG files as text into your favorite text synthesis engine, such as charRNN, and then attempt to fix the resulting output(s) back into a valid SVG file.

 [https://github.com/artBoffin/GAN-XML-Fixer](https://github.com/artBoffin/GAN-XML-Fixer) 

The philosophy behind this method is not about accuracy, but more about discovery.  
Anyone else have ideas of how to creatively ""bend"" some ML systems outside their intended use?

Some examples of art prints I've made using the utility (some are assembled in groups / modified)

https://i.redd.it/suzvevk2l6f31.png

https://i.redd.it/u50ypxk2l6f31.png

https://i.redd.it/axrffzk2l6f31.png

https://i.redd.it/6kj6hyk2l6f31.png

LEGO Minifigs

&amp;#x200B;

https://i.redd.it/lgk5vrn4l6f31.png

https://i.redd.it/r9o41go4l6f31.png

https://i.redd.it/n6awxrn4l6f31.png

https://i.redd.it/26z9orn4l6f31.png",21,148
497,2019-8-8,2019,8,8,17,cniy3k,[D] Are there any relevant image classification domains where traditional approaches are still on par with CNN-based?,https://www.reddit.com/r/MachineLearning/comments/cniy3k/d_are_there_any_relevant_image_classification/,adelredjimi,1565254002,"To be precise, I am talking about image classification, not general computer vision, and by traditional approaches I mean approaches based on features like Fisher, BoW, ... even if they use multi-layered classifiers.",3,29
498,2019-8-8,2019,8,8,17,cniypt,[D] MCTS on raw network not trained with MCTS,https://www.reddit.com/r/MachineLearning/comments/cniypt/d_mcts_on_raw_network_not_trained_with_mcts/,matigekunst,1565254142,"In the AlphaGo Zero \[paper\]([https://deepmind.com/documents/119/agz\_unformatted\_nature.pdf](https://deepmind.com/documents/119/agz_unformatted_nature.pdf)) figure 6b shows the performance of a raw network which directly takes the action with the highest q-value(?) versus an MCTS approach which gets 5 seconds of thinking time. The MCTS approach has a large performance gain over the raw network approach.

 Now I have trained a network with a policy and value head that uses the first approach and does not have a tree structure with accompanying data (such as times visited per node). I'm wondering if I can skip training using MCTS but just use the network to build a tree in the simulation phase and if there's any precedent for this technique. 

The problem is a deterministic RL problem with only one goal state and no other rewards. The same state can be reached twice and this often happens when I use the raw network approach. The agent then gets stuck in a loop. In a previous post, someone suggested taking the next best option once a certain state is reached more than once. This worked like a charm. But for real-world application, I would like to keep the number of actions taken as low as possible. This is why I think MCTS mightbe an improvement.",12,1
499,2019-8-8,2019,8,8,17,cniz0i,Testing new data on a trained LGBM model,https://www.reddit.com/r/MachineLearning/comments/cniz0i/testing_new_data_on_a_trained_lgbm_model/,varunsingh85,1565254190,[removed],0,1
500,2019-8-8,2019,8,8,18,cnj3z6,Detailed guide to using Dask for data science and machine learning,https://www.reddit.com/r/MachineLearning/comments/cnj3z6/detailed_guide_to_using_dask_for_data_science_and/,millacolin,1565255184,,0,1
501,2019-8-8,2019,8,8,18,cnj4wn,[D] A laboratory that is doing research other than what I want to do,https://www.reddit.com/r/MachineLearning/comments/cnj4wn/d_a_laboratory_that_is_doing_research_other_than/,GoBacksIn,1565255353,"may I ask you a question?

&amp;#x200B;

My lab research topic is CNN, but I want to study RNN time series prediction.

&amp;#x200B;

All six proposals were rejected in three months ...

&amp;#x200B;

In conclusion, I think I should decide whether to study CNN or leave the lab.

&amp;#x200B;

Is it right to study RNN personally while studying CNN?

&amp;#x200B;

Or is it right to see another lab?",0,1
502,2019-8-8,2019,8,8,18,cnj8p6,[D] GPT2 as seq2seq decoder,https://www.reddit.com/r/MachineLearning/comments/cnj8p6/d_gpt2_as_seq2seq_decoder/,Viecce,1565256075,"Hello! Not having the computational resources to train a seq2seq transformer-based model, Im trying to do that by fine-tuning BERT as an encoder and GPT2 as a decoder. 
Has anyone tried something similar? How can I condition GPT2 on the encoders output?",2,11
503,2019-8-8,2019,8,8,18,cnjbew,[D] The laboratory which is doing research different from the research subject I want to do.,https://www.reddit.com/r/MachineLearning/comments/cnjbew/d_the_laboratory_which_is_doing_research/,GoBacksIn,1565256602,"may I ask you a question?

&amp;#x200B;

My lab research topic is CNN, but I want to study Stock time series prediction using RNN.

&amp;#x200B;

All six proposals were rejected in three months ...

&amp;#x200B;

In conclusion, I think I should decide whether to study CNN or leave the lab.

&amp;#x200B;

Is it right to study RNN personally while studying CNN?

&amp;#x200B;

Or is it right to see another lab?",7,1
504,2019-8-8,2019,8,8,18,cnjixr,Real time face detection on the RaspberryPi-4 achieving 15-17 FPS,https://www.reddit.com/r/MachineLearning/comments/cnjixr/real_time_face_detection_on_the_raspberrypi4/,shunyaos4ai,1565258064,[removed],0,1
505,2019-8-8,2019,8,8,19,cnjmcp,[R] Recent Advances in Object Detection in the Age of Deep Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/cnjmcp/r_recent_advances_in_object_detection_in_the_age/,gnavihs,1565258699,,0,1
506,2019-8-8,2019,8,8,19,cnjzku,Anyone's got any tutorials for building a vowpal recommendation system.,https://www.reddit.com/r/MachineLearning/comments/cnjzku/anyones_got_any_tutorials_for_building_a_vowpal/,sayooj_bala,1565261112,[removed],0,1
507,2019-8-8,2019,8,8,19,cnk3zg,North Carolina Triangle Homes for Sale,https://www.reddit.com/r/MachineLearning/comments/cnk3zg/north_carolina_triangle_homes_for_sale/,margeryccrector,1565261901,,0,1
508,2019-8-8,2019,8,8,20,cnkbda,[D] word2vec and context,https://www.reddit.com/r/MachineLearning/comments/cnkbda/d_word2vec_and_context/,odna_adops,1565263144,"In my company we do not train word2vec (or Glove or fastext) for each context separately (i.e. we use the same algorithm for movie reviews and for food reviews). as i think meaning changes based on context (go read the book is a good sentiment for a book review but bad one for movie review), I was wondering do you train your embedding algorithm for each context or not.

thanks!",15,17
509,2019-8-8,2019,8,8,21,cnl0mz,[P] Video Analysis: Improved Adversarial Robustness by Reducing Open Space Risk via Tent Activations,https://www.reddit.com/r/MachineLearning/comments/cnl0mz/p_video_analysis_improved_adversarial_robustness/,ykilcher,1565267068,,0,1
510,2019-8-8,2019,8,8,21,cnlebm,Looking for a Pytorch guru to help me,https://www.reddit.com/r/MachineLearning/comments/cnlebm/looking_for_a_pytorch_guru_to_help_me/,kartinko28,1565269024,[removed],1,1
511,2019-8-8,2019,8,8,22,cnlj5x,Bridging Microsoft SEAL into TensorFlow,https://www.reddit.com/r/MachineLearning/comments/cnlj5x/bridging_microsoft_seal_into_tensorflow/,ybsu,1565269695,[removed],0,2
512,2019-8-8,2019,8,8,23,cnm670,MS-SSIM score,https://www.reddit.com/r/MachineLearning/comments/cnm670/msssim_score/,syousefi,1565272835,[removed],0,1
513,2019-8-8,2019,8,8,23,cnmamq,Text Summarization in Python,https://www.reddit.com/r/MachineLearning/comments/cnmamq/text_summarization_in_python/,rankingmachine,1565273382,[removed],0,1
514,2019-8-8,2019,8,8,23,cnmhqq,Google Pays Its DeepMind Tech Professionals a Huge Salary,https://www.reddit.com/r/MachineLearning/comments/cnmhqq/google_pays_its_deepmind_tech_professionals_a/,BrooklynShatterDome,1565274286,,0,1
515,2019-8-8,2019,8,8,23,cnmjgk,Graph Networks/Kernels for Supervised Node Classification,https://www.reddit.com/r/MachineLearning/comments/cnmjgk/graph_networkskernels_for_supervised_node/,suddenintent,1565274504,[removed],0,1
516,2019-8-8,2019,8,8,23,cnmvt9,Style Transfer to Video,https://www.reddit.com/r/MachineLearning/comments/cnmvt9/style_transfer_to_video/,e-seven,1565276175,[removed],0,1
517,2019-8-9,2019,8,9,0,cnmxfk,Ensuring IOT Data Integrity &amp; Security with Identity and Access Management (IAM),https://www.reddit.com/r/MachineLearning/comments/cnmxfk/ensuring_iot_data_integrity_security_with/,rosamarts,1565276414,[removed],0,1
518,2019-8-9,2019,8,9,0,cnmxop,Do we have any data set of Waveform pulse or other similar,https://www.reddit.com/r/MachineLearning/comments/cnmxop/do_we_have_any_data_set_of_waveform_pulse_or/,mac0q,1565276448,[removed],0,1
519,2019-8-9,2019,8,9,0,cnn5vh,[P] Bridging Microsoft SEAL into TensorFlow,https://www.reddit.com/r/MachineLearning/comments/cnn5vh/p_bridging_microsoft_seal_into_tensorflow/,jvmancuso,1565277494,,0,3
520,2019-8-9,2019,8,9,0,cnn8l2,[Q] Working with sequences of sequences,https://www.reddit.com/r/MachineLearning/comments/cnn8l2/q_working_with_sequences_of_sequences/,Gkg14,1565277838,[removed],0,1
521,2019-8-9,2019,8,9,0,cnnajg,Tip of my tongue -- a blog post recently on loss vs. independent evaluation,https://www.reddit.com/r/MachineLearning/comments/cnnajg/tip_of_my_tongue_a_blog_post_recently_on_loss_vs/,SyAbleton,1565278083,[removed],0,1
522,2019-8-9,2019,8,9,0,cnnk9s,[1908.02419] Gradient Descent Finds Global Minima for Generalizable Deep Neural Networks of Practical Sizes,https://www.reddit.com/r/MachineLearning/comments/cnnk9s/190802419_gradient_descent_finds_global_minima/,zhamisen,1565279334,,29,50
523,2019-8-9,2019,8,9,0,cnnkir,South Korean Game Developers AI Turns Your Selfie Into an Anime Face,https://www.reddit.com/r/MachineLearning/comments/cnnkir/south_korean_game_developers_ai_turns_your_selfie/,Yuqing7,1565279365,,0,1
524,2019-8-9,2019,8,9,0,cnnnrl,It is absurd that an entire field devoted to automatic text summarization keeps all of its information in papers,https://www.reddit.com/r/MachineLearning/comments/cnnnrl/it_is_absurd_that_an_entire_field_devoted_to/,[deleted],1565279766,,0,1
525,2019-8-9,2019,8,9,0,cnnpgh,[D] It is absurd that an entire field devoted to automatic text summarization keeps all of its information in papers,https://www.reddit.com/r/MachineLearning/comments/cnnpgh/d_it_is_absurd_that_an_entire_field_devoted_to/,thatguydr,1565279977,"* I wonder what the most important advances in my field have been in the past month. I could go on arXiv and search every paper in ML eff that noise. Im going to arxiv-sanity and twitter! That's sane! Theres no way I can miss anything important this way! Every company should track their changes through tweets!
* Hey this paper sounds amazing. The abstract is great. All I need is the main algorithm with every variable clearly described, how its different from current techniques, and the results! Oh the algorithm they used is the one *not* in the box? And it's described across four pages, and not in order of operations? Brilliant!

Papers are a great storage format for reference, but were all in CS. Why are we using the storage format as the information retrieval format?? Thats insane. Its figuratively like were trying to understand code changes, but instead of documentation, we could use diff, but we dont even do that and we just compare the old files and the new files by eye

Are there any non-profits working on this? Anyone? OpenAI, can you become non-sketchy for like 10 seconds again and get this thing hammered out? I will literally subscribe to a service if someone can save me the hours of my life I'm never getting back from reading through all this.",0,0
526,2019-8-9,2019,8,9,1,cnnqgl,Tesla is Going to Win Level 5 - George Hotz,https://www.reddit.com/r/MachineLearning/comments/cnnqgl/tesla_is_going_to_win_level_5_george_hotz/,[deleted],1565280099,[deleted],0,1
527,2019-8-9,2019,8,9,1,cnnvh2,"[N] PyTorch 1.2 release (new TorchScript API, NN.Transformer, Expanded ONNX Export)",https://www.reddit.com/r/MachineLearning/comments/cnnvh2/n_pytorch_12_release_new_torchscript_api/,[deleted],1565280672,[deleted],0,1
528,2019-8-9,2019,8,9,1,cnnvqo,How to Build an Interactive Spark Notebook with BeakerX and Jupyter,https://www.reddit.com/r/MachineLearning/comments/cnnvqo/how_to_build_an_interactive_spark_notebook_with/,superguenter,1565280703,,0,1
529,2019-8-9,2019,8,9,1,cnnykj,[P] Web-based implementation of Deep Image Prior,https://www.reddit.com/r/MachineLearning/comments/cnnykj/p_webbased_implementation_of_deep_image_prior/,ToraxXx,1565281062,"[https://warlock.ai/deepimageprior/](https://warlock.ai/deepimageprior/)

Using TensorFlow.js I implemented a client-side version of [Deep Image Prior](https://arxiv.org/abs/1711.10925). It can be used for denoising, inpainting, super-resolution (not implemented yet) and more. It works by training a network to output a given image. More info about the algorithm can be found on the original authors' [project page](https://dmitryulyanov.github.io/deep_image_prior).

There are still a couple of things that need to be resolved such as mask-drawing on mobile (the page scrolls when drawing right now) and the comparison view becoming stuck after the first image was selected. Also I'm not sure how well this works on devices without GPUs, although on my relatively old phone (Nexus 6) and my PC (GTX 1070) it worked reasonably well.

[Inpainting example 1](https://i.redd.it/23nfhru629f31.png)

[Inpainting example 2](https://i.redd.it/oapgc9t519f31.png)",3,27
530,2019-8-9,2019,8,9,1,cno3yz,"[N] PyTorch 1.2 release (new TorchScript API, NN.Transformer, Expanded ONNX Export)",https://www.reddit.com/r/MachineLearning/comments/cno3yz/n_pytorch_12_release_new_torchscript_api/,p1nh3ad,1565281692,[removed],0,1
531,2019-8-9,2019,8,9,1,cno6f6,[P] Need help in isolating hands from webcam stream.,https://www.reddit.com/r/MachineLearning/comments/cno6f6/p_need_help_in_isolating_hands_from_webcam_stream/,meridit45,1565281989,"I am working on a live gesture recognition project. As of now, I used backprojection for segmenting skin and applied cv2.connectedcomponents to identify all the blobs. However, I am not able to isolate the hands. A lot of false predictions come in.
Can you please suggest me a better way to go about this project. Thanks jn advance.",2,2
532,2019-8-9,2019,8,9,1,cnohyj,Machine Learning is all about Maths : Free Resources to learn  Part 2 ,https://www.reddit.com/r/MachineLearning/comments/cnohyj/machine_learning_is_all_about_maths_free/,sajad-52,1565283344,,0,1
533,2019-8-9,2019,8,9,2,cnonkv,Video Understanding Using Temporal Cycle-Consistency Learning,https://www.reddit.com/r/MachineLearning/comments/cnonkv/video_understanding_using_temporal/,sjoerdapp,1565283987,,0,2
534,2019-8-9,2019,8,9,2,cnoown,"Applications of Artificial intelligence marketing, How ai is changing marketing &amp; advertising",https://www.reddit.com/r/MachineLearning/comments/cnoown/applications_of_artificial_intelligence_marketing/,science_online,1565284143,,0,1
535,2019-8-9,2019,8,9,2,cnop18,[D] Is attention just a type of belief propagation?,https://www.reddit.com/r/MachineLearning/comments/cnop18/d_is_attention_just_a_type_of_belief_propagation/,Toast119,1565284162,"Attention is mostly discussed in the context of sequences, but I was thinking about what it meant in the context of message passing. Clearly when a certain word has ""attention"" it is relaying long-distance information, but is that synonymous with something like belief propagation?",3,5
536,2019-8-9,2019,8,9,2,cnp1ai,U-Net for dimension reduction?,https://www.reddit.com/r/MachineLearning/comments/cnp1ai/unet_for_dimension_reduction/,Scaredabeast,1565285610,[removed],0,1
537,2019-8-9,2019,8,9,2,cnp42i,[N] PyTorch 1.2 release: New TorchScript API; Expanded Onnx Export; NN.Transformer,https://www.reddit.com/r/MachineLearning/comments/cnp42i/n_pytorch_12_release_new_torchscript_api_expanded/,metaAI,1565285928,"For more details of this release, please go to GitHub release page [here](https://github.com/pytorch/pytorch/releases)",59,237
538,2019-8-9,2019,8,9,2,cnp97e,[D] U-Net for dimension reduction?,https://www.reddit.com/r/MachineLearning/comments/cnp97e/d_unet_for_dimension_reduction/,Scaredabeast,1565286554,I have recently read about U-Net and thought that since it is very similar with autoencoders then perhaps I could use it for a project in place of autoencoders for dimension reduction. But I haven't found any papers that use it for that purpose only segmentation and generative models. Is there a theoretical issue using it for that purpose?,3,2
539,2019-8-9,2019,8,9,2,cnpaam,Clustering &amp; Classification With Machine Learning In R,https://www.reddit.com/r/MachineLearning/comments/cnpaam/clustering_classification_with_machine_learning/,luckyluck123luck,1565286674,,0,1
540,2019-8-9,2019,8,9,2,cnpcpp,How to run AWS Deepracer Simulator Inside Ubuntu Distro Inside Windows,https://www.reddit.com/r/MachineLearning/comments/cnpcpp/how_to_run_aws_deepracer_simulator_inside_ubuntu/,EstablishedEmpath,1565286944,[removed],0,1
541,2019-8-9,2019,8,9,3,cnpoab,Negative correlation between feature and target,https://www.reddit.com/r/MachineLearning/comments/cnpoab/negative_correlation_between_feature_and_target/,amjad_ameur,1565288307,[removed],0,1
542,2019-8-9,2019,8,9,3,cnppzc,Neural network inference for videos in Tensorflow,https://www.reddit.com/r/MachineLearning/comments/cnppzc/neural_network_inference_for_videos_in_tensorflow/,alseambusher,1565288497,[removed],0,1
543,2019-8-9,2019,8,9,3,cnpsvc,[D] How do you go about funding your side project/startup ideas?,https://www.reddit.com/r/MachineLearning/comments/cnpsvc/d_how_do_you_go_about_funding_your_side/,SmartSpray,1565288837,"I have some interesting ideas for a vision/analytics project/potential startup, but the costs to train a model and experiment with different stuff is just absurd. How do you go about paying for the cloud compute? I am also a vision phd student, and, while I have access to plenty of gpus in the lab, I reckon there would be potential IP issues if I were to use the school's resources for my company.",2,2
544,2019-8-9,2019,8,9,3,cnpv84,[D] Increasing sample size increases no of trainable parameters,https://www.reddit.com/r/MachineLearning/comments/cnpv84/d_increasing_sample_size_increases_no_of/,atif_hassan,1565289138," Hi!

I was working with keras and tensorflow as backend on an NLP problem when I observed that increasing my training data size caused an increase in the number of trainable parameters even when batch size remained the same. From what I understand, trainable parameters are the weights which are learnt for each layer. If that is the case then it should not change irrespective of whether I increase or decrease my input data size.

So what is exactly happening here? The reason why this is important is because I perform normalization upon my data once it is fully loaded. This normalization would not work properly if I used a generator function.",8,1
545,2019-8-9,2019,8,9,3,cnpzij,Best conferences and information sourcing about the state of machine learning,https://www.reddit.com/r/MachineLearning/comments/cnpzij/best_conferences_and_information_sourcing_about/,AlphonseWestwood,1565289639,[removed],0,1
546,2019-8-9,2019,8,9,3,cnq0gw,[P] Neural network inference pipeline for videos in Tensorflow,https://www.reddit.com/r/MachineLearning/comments/cnq0gw/p_neural_network_inference_pipeline_for_videos_in/,alseambusher,1565289748,"Just as we saw a huge influx of images in the past decade or so, we are now seeing a lot of videos being produced on social media. The need to understand and moderate videos using machine learning has never been greater.

In this post, I will show you how to build an efficient pipeline to processes videos in Tensorflow

https://lifepluslinux.blogspot.com/2019/08/neural-network-inference-pipeline-for.html",2,4
547,2019-8-9,2019,8,9,4,cnqwf7,ML models with autoregressive features and distributed lag featurea,https://www.reddit.com/r/MachineLearning/comments/cnqwf7/ml_models_with_autoregressive_features_and/,dodger94,1565293540,"Hi all,

I have a typical time series data from iot devices and one of the problems I need to solve using that is to predict deviation from normal health of the device over a period of time, (it is a prediction problem). For example I have a piece of equipment. I would train it today on the dataset that I have and would start predicting from a saved model file till my model starts performing badly, this would be the indication for me that my equipment given the set of ideal conditions is performing worse over a period of time.

To solve this I took 3 approaches.
1) using no lag variables and no time features. I had a simple numeric feature X using which I was predicting my y. It gave me bad results.

2) used a pseudo time feature called duration which gave me duration of cycles in the data. For example a heating and cooling cycle would have their durations calculated in their respective groups. This introduced the time element in the model. Still performed badly. I suspect this was because the y was heavily dependent on the previous value in time.

3) using the observations of the second approach decided to go with a simple AR Model with y t-k features as well as X t-k features. This performs suspiciously good. like very small rmses and high adjusted r2s. However I suspect that this too good to be true model is performing that way since the t-k window is kind of allowing a leak that allows it to peek at previous data and predict the future. The downside to this is that forecasting would be bad with this model. Moreover I would not be catching the deviations from.the ideal case as I would be predicting using t-k lag variables. 

I think I should invest more time in finding approaches akin to the second approach. Am I thinking of this in the right way? Any suggestions are welcome!

Thanks in advance",0,1
548,2019-8-9,2019,8,9,5,cnr449,What model to use for predicting number of employees in an organization?,https://www.reddit.com/r/MachineLearning/comments/cnr449/what_model_to_use_for_predicting_number_of/,U2Fan,1565294458,[removed],0,1
549,2019-8-9,2019,8,9,5,cnr57o,[D] Parallelizing LIME,https://www.reddit.com/r/MachineLearning/comments/cnr57o/d_parallelizing_lime/,gokstudio,1565294577,"Hi everyone,

I'm using the LIME for images implementation from [https://github.com/marcotcr/lime](https://github.com/marcotcr/lime). From what I can see,  it seems that LIME works on one sample at a time. Using this in a simple for-loop (PyTorch dataloader) seems inefficient and results in &lt; 20 % GPU utilization on ImageNet val set with Inception v3.   


Anyone has any experience with speeding up LIME for better GPU utilization?",1,5
550,2019-8-9,2019,8,9,5,cnr5pn,Funny Real,https://www.reddit.com/r/MachineLearning/comments/cnr5pn/funny_real/,elstoni19,1565294633,,0,1
551,2019-8-9,2019,8,9,5,cnrbbp,Computer Vision - TensorFlow on Custom Object Identification,https://www.reddit.com/r/MachineLearning/comments/cnrbbp/computer_vision_tensorflow_on_custom_object/,LucidSnow,1565295313,I want to do object identification on a custom object on a  Raspberry Pi that might not be accessible with any of the CNNs online. Should I be using TensorFlow or is TensorFlow Lite capable of this? Do I have to build my own custom CNN for this or can i just use transfer learning on ResNet or something?,0,1
552,2019-8-9,2019,8,9,5,cnreuu,[P] Generative Modelling by Estimating Gradients - in 150 lines with JAX,https://www.reddit.com/r/MachineLearning/comments/cnreuu/p_generative_modelling_by_estimating_gradients_in/,justheuristic,1565295740,"This tutorial that implements the math behind the [Generative Modelling by Estimating Gradients of the Data Distribution](https://arxiv.org/abs/1907.05600).

**Get the tutorial:** [**view on github**](https://github.com/google/jax/blob/master/notebooks/score_matching.ipynb) or [**run in Google Colab**](https://colab.research.google.com/github/google/jax/blob/master/notebooks/score_matching.ipynb)

In short, they train a neural network to estimate gradients of image probability and iteratively follow these gradients to improve the image.",8,12
553,2019-8-9,2019,8,9,5,cnrm6m,"When training a neural network, can the addition of extra outputs increase the predictive accuracy of other correlated outputs?",https://www.reddit.com/r/MachineLearning/comments/cnrm6m/when_training_a_neural_network_can_the_addition/,konwiddak,1565296579,"Let's say I have a trained neural network with a, b and c as inputs to predict output x. I then create a similar network where I add in output neuron y, in the case where y is linked to x can this improve the network's ability to predict x? This is best illustrated with an example.

Let's say I get weather station data in the morning, and I want to use this data to predict rainfall that day, I only really care about a rainfall prediction. Initially I use historical rainfall &amp; weather station data to train a network. Let's say I also obtain historical cloud coverage data, the weather station doesn't measure this, so I can't use it as an input, however since I have the dataset I could use it as an output. Cloud coverage is obviously linked to rainfall - if I retrain a similar network with an additional output node for cloud coverage - could this improve my network's prediction of rainfall? Presumably the cloud coverage output will help the back propagation through the network during the training process, and the additional output effectively conveys more information to the network during training. Could this actually lead to better rainfall predictions?",0,1
554,2019-8-9,2019,8,9,5,cnrmdr,Best community projects?,https://www.reddit.com/r/MachineLearning/comments/cnrmdr/best_community_projects/,EQHeadquarters,1565296603,"In the last year I've come across:   


\- [Ludwig](https://eng.uber.com/introducing-ludwig/) (by Uber) which promises a set of interoperable adapters for various data types  
\- [ML Fin Lab](https://mlfinlab.readthedocs.io/en/latest/index.html#) , a finance-oriented machine learning toolkit built based on some of Lopez del Pardo's work with financial timeseries  
\- [StanfordNLP](https://stanfordnlp.github.io/stanfordnlp/) for parsing 50+ existing languages

  
And honestly more, once I take time to dig everything up.   


Staying on top of the constant innovation in the space could make your head spin. Which projects (open source, community driven, start up or otherwise) made you do a double take in terms of being sort of awesome?",0,1
555,2019-8-9,2019,8,9,5,cnrrh2,"[P] I made a persistent, online environment for AI",https://www.reddit.com/r/MachineLearning/comments/cnrrh2/p_i_made_a_persistent_online_environment_for_ai/,zollandd,1565297203,"**What is Terrarium.ai?**

I've always been interested in Reinforcement Learning and had a lot of fun messing around with OpenAI's Gym. I created some little models that could play Atari games, but I wanted to take it to the next level. I wanted to see my model compete with other models and fight for survival rather than a basic score.

This is why I built[Terrarium.ai](https://www.terrarium.ai/),a **persistent**, **online** **environment** where models control agents and fight for survival. Ive just released the first version and I believe it is the first step towards **an entire universe of persistent online worlds for AI to live in**. 

**Terrarium.ai is FREE**

I can't tell you how excited I am to share this, but I've received a bit of capital for this project. This means I can offer it to you all for free right now! Absolutely no strings attached. The only thing I want right now is feedback so we can improve as fast as possible.

**Development progress**

Here's some insight into what I'm working on at the moment.  

Right now agents can move, eat, and attack. They have energy and health which need to be minded in order to survive. I am working as hard as I can to add more features to the agents and environment. I really want to add this stuff, but I would like to get some feedback from you guys on how you would like to see them implemented.

If you are interested in using Terrarium or contributing to its development, check out the website and feel free to email me or message me on Reddit, Twitter, or Discord (info about this is on the website). I would love any feedback I can get in order to make Terrarium a better experience. Let me know what features you want prioritized, how you would like them to be implemented, or what problems you see with the platform that could be improved on!",25,96
556,2019-8-9,2019,8,9,5,cnrwel,Choosing an algorithm dealing with different label input shape sizes and creating a generalized regression model,https://www.reddit.com/r/MachineLearning/comments/cnrwel/choosing_an_algorithm_dealing_with_different/,an_object_in_space,1565297793,[removed],0,1
557,2019-8-9,2019,8,9,6,cns104,[D] How big of a problem is acquiring labeled data?,https://www.reddit.com/r/MachineLearning/comments/cns104/d_how_big_of_a_problem_is_acquiring_labeled_data/,Rtzon,1565298345,"Currently working as a machine learning intern at a well-known tech company in the Bay Area. A huge problem we have is acquiring labeled data. We simply don't have the time or resources to go through tens of thousands of pieces of data and label them individually. Currently, we employ a sort of semi-supervised learning to get this done. As I look forward in my career and to other places, I was wondering... is this a problem that anyone else faces? How big of a problem is this?",31,10
558,2019-8-9,2019,8,9,6,cns1fy,[D] Choosing an algorithm dealing with different label input shape sizes and creating a generalized regression model,https://www.reddit.com/r/MachineLearning/comments/cns1fy/d_choosing_an_algorithm_dealing_with_different/,an_object_in_space,1565298391,"Hey everyone,

Disclaimer:  Im not totally new to Machine Learning, I have witnessed some projects  and Im generally informed with how it all works but I never applied it  myself.

Anyway, I have a Machine Learning problem and because of the properties of the problem Im not sure what algorithm to use.

Im trying to predict the travel time of an object.

Each  trip is time-series labelled (infrequent due to detection and sensor  placement) and consists out of one or more data points (records in my  dataset). Each record contains geospatial information about the object.

Basically,  the dataset consists out of tons of records of which one or more  records belong to one trip exclusively. Each new trip is marked with a  unique ID. They are all important since they contain information such as  geospatial data.

Connecting all  the data points of each trip ID will generate a path. However, some  trips in the dataset only have one data point but with a known start and  end point. This means a path cannot be constructed from those records.

I  have created labels by finding certain conditions that should be met.  Each label consists at least out of two records. This means a lot of  information in between does not have to be present, as long as the  object was observed close to the start and ending point, it can be  considered a label and the total travel time can be estimated. It can  also mean that a trip with a lot of data points but with no data points  close to the start and end location of the object cannot be considered a  label.

I have added additional  features like the distance to the previous point in km and total trip  distance in km (float values). Most features describe the relationship  to the previous data point.

In summary:

For  the prediction labels: at least know the total distance the object will  travel and I will have at least one data point with geospatial  information in between this start location and end location.

For  the training labels: at least I will have two data points with  geospatial information close to start and end location together with the  total travel time.

Ideally I want  to predict the travel time for each trip with one or more data points  (records). Im not trying predict/construct the path taken. Just an  estimate for the travel time.

The problem is, is that Im not sure what algorithm to use since:

* I  have multiple records that belong to one record. So far I havent  really come across how to deal with a variable input shape size. Most  say to reduce the input shape size to a single record/row (I would call  it flattening) by binning values or one-hot encoding values.
* Flattening  the features is not really possible. The added depth of the  additionally computed features would be lost by binning since they  provide insight about how it is connected to the previous data point  (distance).
* A  trip can be represented by simply one record in the database or by a  lot of records. The greater the record count, the better. Some trips  have consists out of 50 records which allow for a better estimate.

I  was thinking of a Recurrent Neural Network since they can deal with a  time-series sequence but Im questioning it can be applied tot his  problem.

Can I train a Recurrent  Neural Network on a lot of groups (trips in this case) and generate a  generalized model that I can use to predict other groups? Or can it only  make prediction within each group? I have a lot of trips (groups) but  the available information per trip (group) is very limited in most  cases. I therefore want to develop a generalized model that will work  for all groups.",1,3
559,2019-8-9,2019,8,9,6,cns30o,[D] What topics/ideas related to AI need more explaining/debunking for non-experts?,https://www.reddit.com/r/MachineLearning/comments/cns30o/d_what_topicsideas_related_to_ai_need_more/,regalalgorithm,1565298587,"Hi all,

I  am the creator of this site Skynet Today  (https://www.skynettoday.com/), which is dedicated to providing  accessible and informed coverage of the latest AI news and trends.  Basically it's a platform for people with expertise in AI (so  researchers, engineers, etc.) to write pieces aimed at non-experts and explain concepts or clean up misunderstanding. I have been thinking to  put out a call for coverage on a specific set of topics, and wonder if you all have any thoughts on what good topics might be.

Some of the ones we already have written down are:

\* The American public is already worried about AI catastrophe  
\* Trends in media coverage of AI  
\* Documented misuses of DeepFakes so far  
\* State of weaponized military AI  
\* Stop citing non AI experts (including Gates, Hawkins, etc.) about its future

PS we can always use more contributors, if you might be interested take a look here https://www.skynettoday.com/contribute",0,0
560,2019-8-9,2019,8,9,6,cns3kk,Building categories and categorizing instagram profiles by picture,https://www.reddit.com/r/MachineLearning/comments/cns3kk/building_categories_and_categorizing_instagram/,morenoh149,1565298656,[removed],0,1
561,2019-8-9,2019,8,9,6,cns6uz,[D] Is there ever a situation where one Titan RTX is better than two RTX 2080 tis?,https://www.reddit.com/r/MachineLearning/comments/cns6uz/d_is_there_ever_a_situation_where_one_titan_rtx/,swagner648,1565299068,"Ive been looking into Nvidias lineup of cards, wondering which is the best for the $3000 ish price range. Ive come down to one Titan RTX or two RTX 2080 tis. I know they would have the same memory, and nvlink should decrease major consequences of speed with two separate cards. Is there any situation where the Titan RTX would be a better choice? Also, are there any other options that would be better than that?",15,1
562,2019-8-9,2019,8,9,6,cns79x,"Entry level Data Scientists (or other DS position), how did you just get your new job? What is your story?",https://www.reddit.com/r/MachineLearning/comments/cns79x/entry_level_data_scientists_or_other_ds_position/,Bazzert_One,1565299127,[removed],0,1
563,2019-8-9,2019,8,9,6,cnsh96,How to properly calculate the gradient update for spectral norm?,https://www.reddit.com/r/MachineLearning/comments/cnsh96/how_to_properly_calculate_the_gradient_update_for/,HumanSpinach2,1565300320,[removed],0,1
564,2019-8-9,2019,8,9,7,cnt0t1,[N] Video Understanding Using Temporal Cycle-Consistency Learning,https://www.reddit.com/r/MachineLearning/comments/cnt0t1/n_video_understanding_using_temporal/,__arch__,1565302737,"Blog post [here](https://ai.googleblog.com/2019/08/video-understanding-using-temporal.html). Excerpt from the blog

&gt;We propose a potential solution using a self-supervised learning method called Temporal Cycle-Consistency Learning (TCC). This novel approach uses correspondences between examples of similar sequential processes to learn representations particularly well-suited for fine-grained temporal understanding of videos. We are also releasing our TCC codebase to enable end-users to apply our self-supervised learning algorithm to new and novel applications.",0,15
565,2019-8-9,2019,8,9,7,cnt2bu,"keras(python): NN always predicts similar output, although inputs is different",https://www.reddit.com/r/MachineLearning/comments/cnt2bu/keraspython_nn_always_predicts_similar_output/,Okaghana,1565302934,[removed],0,1
566,2019-8-9,2019,8,9,8,cntm9z,Machine Learning for Artificial Intelligence Specialization  Boost your career growth and income potential,https://www.reddit.com/r/MachineLearning/comments/cntm9z/machine_learning_for_artificial_intelligence/,internetdigitalentre,1565305568,[removed],0,1
567,2019-8-9,2019,8,9,9,cnuhcg,[D] Has anyone made a deep q network that plays games by using ONLY the screen as input?,https://www.reddit.com/r/MachineLearning/comments/cnuhcg/d_has_anyone_made_a_deep_q_network_that_plays/,YuhFRthoYORKonhisass,1565309873,"I've seen multiple projects where people have claimed to use only the screen, but it uses some form of game that they made that obviously will work with the network. Has anyone literally used only the pixels of the screen?",0,0
568,2019-8-9,2019,8,9,9,cnuslc,Active learning and model explainability for document classification,https://www.reddit.com/r/MachineLearning/comments/cnuslc/active_learning_and_model_explainability_for/,Best_Mord_Brazil,1565311487,,0,1
569,2019-8-9,2019,8,9,10,cnv3vj,Paper recommendations for deep learning,https://www.reddit.com/r/MachineLearning/comments/cnv3vj/paper_recommendations_for_deep_learning/,GaelOfAstora,1565313087,[removed],0,1
570,2019-8-9,2019,8,9,10,cnvc02,[D] Basic RNN predicting more than 1 timestep /w Keras (Python).,https://www.reddit.com/r/MachineLearning/comments/cnvc02/d_basic_rnn_predicting_more_than_1_timestep_w/,butter-jesus,1565314307,"I've been working with RNNs for a little while now but prior to dipping my toe in this area I've successfully implemented a few basic feed forward models into a production environment.  I like to think I understand the premise posed by recurrent typologies (GRU, LSTM, for instance).  I'm struggling with the basic shape of the data and/or the proper parameters for my training data.

Here's a basic example I've been playing with for many in, one out (omitting the fancy Keras utils that do automatic encoding / mapping):

## The Data / imports

    import numpy as np
    
    from keras.utils import to_categorical, plot_model
    from keras.preprocessing.text import Tokenizer
    from keras.preprocessing.sequence import pad_sequences
    from keras.models import Sequential
    from keras.layers import Dense, Embedding, LSTM, GRU, Dropout, Flatten
    from keras import callbacks, regularizers, optimizers
    
    X = [
        [""a"", ""b"", ""c""],
        [""b"", ""c"", ""d""],    
        [""c"", ""d"", ""e""],
        [""d"", ""e"", ""f""],    
        [""e"", ""f"", ""g""],
        [""f"", ""g"", ""h""],    
        [""g"", ""h"", ""i""],
        [""h"", ""i"", ""j""],
    ]
    
    # Maps to each value observation X, 2 seq out
    y_b = [
        [""d"", ""e""],
        [""e"", ""f""],    
        [""f"", ""g""],
        [""g"", ""h""],    
        [""h"", ""i""],
        [""i"", ""j""],    
        [""j"", ""k""],
        [""k"", ""m""]
    ]
    
    # Maps to each value observation X, 1 seq out
    y_a = [
        [""d""],
        [""e""],    
        [""f""],
        [""g""],    
        [""h""],
        [""i""],    
        [""j""],
        [""k""]
    ]
    
    # Basic function to translate characters to their ordinal offsets
    decode = lambda char: [chr(i) for i in range(97, 97 + 26)]
    encode = lambda seq: np.array([[letters.index(i) for i in obs] for obs in seq])
    
    X_encoded = encode(X)
    y_encoded = encode(y_a)  # y_a == predict single timestep, y_b == predict 2 timesteps

The resulting design matrix should look something like this:

    array([[0, 1, 2],
           [1, 2, 3],
           [2, 3, 4],
           [3, 4, 5],
           [4, 5, 6],
           [5, 6, 7],
           [6, 7, 8],
           [7, 8, 9]])

## Reshaping for network

    sequence_length = 3
    X_reshaped = np.reshape(X_encoded, (len(X_encoded), sequence_length, 1))
    X_reshaped = to_categorical(X_reshaped) # This is from keras.util
    y_cat = to_categorical(y_encoded)

y\_cat ends up one-hot-encoded / binary-like for 1 representing the unique entity on index:

    sequence_length = 3
    X_reshaped = np.reshape(X_encoded, (len(X_encoded), sequence_length, 1))
    ## Experimented with 
    ## X_reshaped = to_categorical(X_reshaped)
    y_cat = to_categorical(y_encoded)

## The Model

    ## Network topology 
    model = Sequential()
    model.add(GRU(64, input_shape = (X_reshaped.shape[1], X_reshaped.shape[2])))
    ## This doesn't seem to be necessary
    # model.add(Flatten())
    
    ## This, I believe sets the assumption about the output in terms of categorical encoding
    model.add(Dense(y_cat.shape[1], activation=""softmax""))
    
    rmsprop = optimizers.rmsprop(lr = .1)
    model.compile(loss=""categorical_crossentropy"", optimizer=rmsprop, metrics=[""accuracy""])
    model_params = dict(
        x                = X_reshaped, 
        y                = y_cat, 
        epochs           = 50, 
        batch_size       = 2, 
        verbose          = 1, 
        # callbacks        = [keras_tensorboard], 
        validation_split = 0.3 
    )
    history = model.fit(**model_params)

My basic 3 in 1 out (predicting y\_a) network works just fine.  When I try to predict more than one (`y_b`), updating my parameters for the 2nd `Dense` layer, is when I run into problems.  Given the shape and the assumptions I've made about the network, seem to be incorrect because the library throws an error about the shape of my ground truth (y).

1. Is this the proper topology for this type of problem?
2. Is my `y` encoded improperly?

Of course I'm interested in solving for the multi sequence output but more importantly, I'm hoping to understand ""why"" more than ""how"".  Thanks in advance for any advice or help!",2,0
571,2019-8-9,2019,8,9,11,cnvybp,Really,https://www.reddit.com/r/MachineLearning/comments/cnvybp/really/,robinroy_peter_,1565317511,,0,1
572,2019-8-9,2019,8,9,11,cnvzhs,Getting Started to Achieving Kaggle Kernels GM &amp; Rank #2 in just 9 months | Interview with Shivam Bansal,https://www.reddit.com/r/MachineLearning/comments/cnvzhs/getting_started_to_achieving_kaggle_kernels_gm/,init__27,1565317682,"Interview with Shivam Bansal, Kernels GM Ranked 4: 

Did you know Shivam made it to the Top 2 Rankings in Kernels after just 9 months of getting started on Kaggle!

We talk about his journey into Data Science, Kaggle. Shivam also shares his pipeline and motivation behind writing kernels. 

(Audio) https://anchor.fm/chaitimedatascience/episodes/Interview-with-Kaggle-Kernels-GM-Shivam-Bansal-e4qcbe/a-ak0id3

(Video) https://www.youtube.com/watch?v=0K4C1FMVbgQ",0,1
573,2019-8-9,2019,8,9,13,cnxdm1,"[P] For NLP beginners, simple PyTorch implementation of Language Modeling",https://www.reddit.com/r/MachineLearning/comments/cnxdm1/p_for_nlp_beginners_simple_pytorch_implementation/,lyeoni,1565325306,"A step-by-step tutorial on how to implement and adapt **simple language model** to **Wikipedia text**.

A pre-trained BERT, XLNET is publicly available ! But, for NLP beginners, It could be hard to use/adapt after full understanding. For them, I covered whole, end-to-end implementation process for language modeling, using recurrent network, we already know.

I hope that this repo can be a good solution for people who want to have their own language model :)

[https://github.com/lyeoni/pretraining-for-language-understanding](https://github.com/lyeoni/pretraining-for-language-understanding)",1,4
574,2019-8-9,2019,8,9,14,cny3h6,Automatic Bulb Cartoing Machine with Instruction Paper Folding Inserting...,https://www.reddit.com/r/MachineLearning/comments/cny3h6/automatic_bulb_cartoing_machine_with_instruction/,amygao1984,1565329848,,0,1
575,2019-8-9,2019,8,9,14,cny5y1,Can a Machine Think and Behave Like Human Do,https://www.reddit.com/r/MachineLearning/comments/cny5y1/can_a_machine_think_and_behave_like_human_do/,Aarushiiiii123,1565330281,,0,1
576,2019-8-9,2019,8,9,15,cnydkn,[R] DoorGym: A Scalable Door Opening Environment And Baseline Agent,https://www.reddit.com/r/MachineLearning/comments/cnydkn/r_doorgym_a_scalable_door_opening_environment_and/,sensetime,1565331640,,3,6
577,2019-8-9,2019,8,9,16,cnysau,[D] What are the current SOTA architectures for NLP information extraction &amp; question answering?,https://www.reddit.com/r/MachineLearning/comments/cnysau/d_what_are_the_current_sota_architectures_for_nlp/,Naveos,1565334334,"Been primarily working in a different domain of DL for a while, but got a project coming up related to NLP. I've done some research though the most frequent ones that seem to be showing up at GPT-2, BERT, and ELMo. However, I am under the impression that these are burying others that may be better suited for the task.

If it's of relevance; my domain expertise is in medicine, and intend to use it for medical purposes.",5,4
578,2019-8-9,2019,8,9,16,cnz0n2,If you got your hands on 5 Tesla V100s 32gb What would you do with them?,https://www.reddit.com/r/MachineLearning/comments/cnz0n2/if_you_got_your_hands_on_5_tesla_v100s_32gb_what/,Anasoori,1565335916,,0,1
579,2019-8-9,2019,8,9,17,cnz9oz,Open BIG JSON Data files,https://www.reddit.com/r/MachineLearning/comments/cnz9oz/open_big_json_data_files/,Walterion1,1565337777,,0,1
580,2019-8-9,2019,8,9,17,cnze44,Bill Hader channels Tom Cruise [DeepFake],https://www.reddit.com/r/MachineLearning/comments/cnze44/bill_hader_channels_tom_cruise_deepfake/,PooaTheorem,1565338709,,0,1
581,2019-8-9,2019,8,9,17,cnzem1,Deep learning Blog posts,https://www.reddit.com/r/MachineLearning/comments/cnzem1/deep_learning_blog_posts/,DecentMakeover,1565338815,[removed],0,1
582,2019-8-9,2019,8,9,17,cnzf9e,Whats So Great About Citizen Data Scientists?,https://www.reddit.com/r/MachineLearning/comments/cnzf9e/whats_so_great_about_citizen_data_scientists/,ElegantMicroWebIndia,1565338947,,0,1
583,2019-8-9,2019,8,9,17,cnzoy1,"TF how to deal with very very sparse ID input, such as cookie which has billions input.",https://www.reddit.com/r/MachineLearning/comments/cnzoy1/tf_how_to_deal_with_very_very_sparse_id_input/,jdxyw,1565341002,[removed],0,1
584,2019-8-9,2019,8,9,17,cnzpoa,Has there been a single high quality article written by 345M gpt-2?,https://www.reddit.com/r/MachineLearning/comments/cnzpoa/has_there_been_a_single_high_quality_article/,ptrenko,1565341157,[removed],0,1
585,2019-8-9,2019,8,9,18,cnzuug,Andrew NG's Machine Learning Course is the no.1 coursera course of all time with 2.45 million enrollments - View the Top 10,https://www.reddit.com/r/MachineLearning/comments/cnzuug/andrew_ngs_machine_learning_course_is_the_no1/,frenchdic,1565342151,,0,1
586,2019-8-9,2019,8,9,18,co01n4,[P] Description of the tool for Machine Learning researchers/engineers.,https://www.reddit.com/r/MachineLearning/comments/co01n4/p_description_of_the_tool_for_machine_learning/,postmachines,1565343516,"The main reasons for creating the service of Machine Learning models inheritance visualization are described in this article.

Other reasons are not so obvious, and will be described later.

[https://arxiv.org/pdf/1908.01874.pdf](https://arxiv.org/pdf/1908.01874.pdf)",0,0
587,2019-8-9,2019,8,9,19,co0do1,[D] What papers should I know when it comes to text recognition with LSTM/GRU,https://www.reddit.com/r/MachineLearning/comments/co0do1/d_what_papers_should_i_know_when_it_comes_to_text/,jthat92,1565345748,Is there maybe some survey paper that summarizes the different architectures that are used wiedly for word based text recognition/classification? Or can you recommend somethong or are there some must-reads? Thanks!,17,60
588,2019-8-9,2019,8,9,20,co0spy,How to Hire a Machine Learning Engineer?,https://www.reddit.com/r/MachineLearning/comments/co0spy/how_to_hire_a_machine_learning_engineer/,cogitotechllc,1565348435,"Artificial Intelligence (AI) and Machine Learning (ML) fields are attracting the field specialists who can work on these technology efficiently and utilize their skills in the right direction. Their job is crucial and needs technical knowledge and skills to analyze the various aspects while working on a particular project to develop a right model.   

If you or your company is spending on AI and ML development, you need to be very careful while appointing such professionals who can be a true assets for your company. Hence, there are few points to hire the right AI and machine learning engineers. Here we brought what you need to look when you [hire AI or machine learning engineers](https://www.buzzblogbox.com/2019/08/how-to-hire-machine-learning-engineer.html).",1,1
589,2019-8-9,2019,8,9,20,co0uy1,"[D] Effect of Oversampling on classifiers, when combined with image transformations",https://www.reddit.com/r/MachineLearning/comments/co0uy1/d_effect_of_oversampling_on_classifiers_when/,Atom_101,1565348779,"I am trying to understand the negative consequences of oversampling in the context of image classification. If I am using a decent amount amount of image transformations, I believe it will effectively be equivalent to SMOTE for tabular data, since I am not exactly repeating any image in a batch. Does the behaviour and test set accuracy of a classifier in any way depend on the actual class distribution in the train set and by oversampling am I doing any harm? 

To take an example I was training a classifier on a dataset with 5 classes, having heavy class imbalance. To balance it out I oversampled the minority classes so that all classes have equal number of images. This caused a significant performance drop on the test set that I have, while cross validation performance was fairly high on the oversampled set. When analysing the class distributions I saw that for the original train set the distribution was: 1,3,2,4,5 with decreasing number of samples. The test set has a class distribution 3,1,2,4,5 but the predictions after training on oversampled data have distribution 4,3,2,5,1. Mathematically speaking, how can this behaviour be explained.",7,0
590,2019-8-9,2019,8,9,20,co0y0j,Friday freestyle tutorial syncopated wrist rolls part 2,https://www.reddit.com/r/MachineLearning/comments/co0y0j/friday_freestyle_tutorial_syncopated_wrist_rolls/,thetrickshotone,1565349329,,0,1
591,2019-8-9,2019,8,9,20,co166q,Python Pandas at Extreme Performance,https://www.reddit.com/r/MachineLearning/comments/co166q/python_pandas_at_extreme_performance/,IguazioDani,1565350725,[removed],0,1
592,2019-8-9,2019,8,9,20,co16s9,Tutorial | A Complete Machine Learning Project in Credit Card Detection,https://www.reddit.com/r/MachineLearning/comments/co16s9/tutorial_a_complete_machine_learning_project_in/,Slight_Role,1565350824,,0,1
593,2019-8-9,2019,8,9,21,co1hlf,Speaker embeddings to help speech-to-text models?,https://www.reddit.com/r/MachineLearning/comments/co1hlf/speaker_embeddings_to_help_speechtotext_models/,MrNiemand,1565352555,[removed],0,1
594,2019-8-9,2019,8,9,21,co1nfg,[P] Video Analysis: Manifold Mixup: Better Representations by Interpolating Hidden States,https://www.reddit.com/r/MachineLearning/comments/co1nfg/p_video_analysis_manifold_mixup_better/,ykilcher,1565353477,,0,1
595,2019-8-9,2019,8,9,22,co22wv,How to perform query classification given click frequencies for each category.,https://www.reddit.com/r/MachineLearning/comments/co22wv/how_to_perform_query_classification_given_click/,curious__homosapien,1565355814,[removed],0,1
596,2019-8-9,2019,8,9,22,co2334,[D] How do object detection algorithms and feature extractor networks work together for action detection?,https://www.reddit.com/r/MachineLearning/comments/co2334/d_how_do_object_detection_algorithms_and_feature/,LessTell,1565355840,"I'm talking about architectures such as AlexNet, Inception and object detectors like YOLO, SSD. I've read a bit online and I'm really confused how they work together.   
Lets say I want to detect a specific object/person from a video and put a box around them with a label describing the state of that object/person. How would that work? What would be steps taken by the object detector and feature extractor? A workflow for this would be really helpful.",4,1
597,2019-8-9,2019,8,9,22,co25zc,Image Compression via Mixture Models,https://www.reddit.com/r/MachineLearning/comments/co25zc/image_compression_via_mixture_models/,errminator,1565356240,"Suppose you have a binary image, or a collection thereof, and you model them as coming from a Bernoulli mixture model using either (a) 2 components or (b) 20 components.

It turns out the log-likelihood is greater (less negative) with 20 components. We know from information theory that the information of an event can be expressed in bits using the negative log-likelihood and changing to base 2. If we do that, we find that the model with 2 components contains more information (more bits) than the model with 20 components.  


So this brings me to the idea of compression:   


1, Do the mixture models represent a ""compressed"" form of the image? I suppose they must do since they contain different numbers of bits i.e. the more components we use the less bits and the more heavy the compression?  


2, But how are they compressing it differently? I'm really struggling to wrap my head around this. I was thinking that maybe with 2 components, it is comparing states of 2 adjacent pixels whilst with 20 components it is comparing states of 20 adjacent pixels and therefore smoothing the image features much more. But I don't like this as I don't think this is what the additional components represent. I'd really like it if someone can explain why more components equals more heavy compression?  


Thanks!",0,1
598,2019-8-9,2019,8,9,22,co29ip,Why does it require so many steps to run something on amazon or google instance,https://www.reddit.com/r/MachineLearning/comments/co29ip/why_does_it_require_so_many_steps_to_run/,dream_minder,1565356748,"Hello,  
I'm really surprised and frustrated with how hard it is to run just anything on an amazon or google instance - it looks like you need docker, pull some images, install this, do that, upload that etc etc. For me the overhead is just crazy and in my company they hire a dedicated person to handle all the scripting and tooling that makes this stuff usable at all. To mount s3 as filesystem you need to rebuild your linux kernel with their extension and otherwise you need to use some scripts to synchronize folders. Could someone explain why can't they just make a VPS which you can turn on or off whenever you need it and access it normally via ssh?",0,1
599,2019-8-9,2019,8,9,22,co2egm,How to submit code for reproducibility while preserving anonymity,https://www.reddit.com/r/MachineLearning/comments/co2egm/how_to_submit_code_for_reproducibility_while/,instantlybanned,1565357438,[removed],0,1
600,2019-8-9,2019,8,9,22,co2has,How to build exe file in Matlab,https://www.reddit.com/r/MachineLearning/comments/co2has/how_to_build_exe_file_in_matlab/,flyhighwithai,1565357827,,0,1
601,2019-8-9,2019,8,9,22,co2pj4,[D] Submitting code while preserving anonymity,https://www.reddit.com/r/MachineLearning/comments/co2pj4/d_submitting_code_while_preserving_anonymity/,instantlybanned,1565358948,"I received some negative feedback on a recent paper submission stating that my results would not be reproducible. However, I took care to only use easily accessible, public benchmark data and wrote code to make it easy for anyone to reproduce my results. I did remove the GitHub link to the code in my submission to preserve anonymity (stating in the submission that this was the reason for not providing a functioning link). What is the best way to handle this in the future without compromising double or triple blind reviewing?",9,4
602,2019-8-9,2019,8,9,23,co2zqh, Using machine learning to accelerate ecological research,https://www.reddit.com/r/MachineLearning/comments/co2zqh/using_machine_learning_to_accelerate_ecological/,sjoerdapp,1565360295,,0,1
603,2019-8-9,2019,8,9,23,co35tf,Beginner's guides,https://www.reddit.com/r/MachineLearning/comments/co35tf/beginners_guides/,MTGTraner,1565361104,"Hi all,


/r/machinelearning is growing rampantly, with over a thousand new subscribers *every day*. As our community grows, it is important to have fertile ground for newcomers to learn the ropes. Since there is already an active subreddit for aiding in the development of machine learning skills, we feel that this is the right time to demarcate the content between these two subs.


As a new rule, all beginner-level content should be posted to our sister sub, /r/learnmachinelearning.  This will free up real estate on our page for more in-depth, expert discussions and provide a more focused learning space for beginners.  Thats not to say that all tutorials are outright banned  in particular, explanations of recent or niche papers are still welcome.

We were all beginners once and newcomers to ML are bringing great things to this sub and the general community. Please do continue to engage with and learn from the community here. But we recommend /r/learnmachinelearning if you do want to start getting your hands dirty

We hope that this specialization will be beneficial to everyone in the long run.


Best regards, the moderator team",0,1
604,2019-8-9,2019,8,9,23,co37ut,Regarding beginner's guides,https://www.reddit.com/r/MachineLearning/comments/co37ut/regarding_beginners_guides/,MTGTraner,1565361370,"Hi all,


/r/machinelearning is growing rampantly, with over a thousand new subscribers *every day*. As our community grows, it is important to have fertile ground for newcomers to learn the ropes. Since there is already an active subreddit for aiding in the development of machine learning skills, we feel that this is the right time to demarcate the content between these two subs.


As a new rule, all beginner-level content should be posted to our sister sub, /r/learnmachinelearning.  This will free up real estate on our page for more in-depth, expert discussions and provide a more focused learning space for beginners.  Thats not to say that all tutorials are outright banned  in particular, explanations of recent or niche papers are still welcome.

We were all beginners once and newcomers to ML are bringing great things to this sub and the general community. Please do continue to engage with and learn from the community here. But we recommend /r/learnmachinelearning if you do want to start getting your hands dirty

We hope that this specialization will be beneficial to everyone in the long run.


Best regards, the moderator team",60,405
605,2019-8-9,2019,8,9,23,co39is,Afternoon of Data in NYC on 9.26,https://www.reddit.com/r/MachineLearning/comments/co39is/afternoon_of_data_in_nyc_on_926/,AnnaOnTheWeb,1565361577,,0,1
606,2019-8-9,2019,8,9,23,co3dq4,[P] I've trained a Neural Network to play Tinder for me.,https://www.reddit.com/r/MachineLearning/comments/co3dq4/p_ive_trained_a_neural_network_to_play_tinder_for/,atum47,1565362134,,0,1
607,2019-8-10,2019,8,10,0,co3p2y,AI Analyzes Chess Commentary to Learn to Play Chess,https://www.reddit.com/r/MachineLearning/comments/co3p2y/ai_analyzes_chess_commentary_to_learn_to_play/,Yuqing7,1565363532,,0,1
608,2019-8-10,2019,8,10,0,co45e8,Advice in Manifold Learning,https://www.reddit.com/r/MachineLearning/comments/co45e8/advice_in_manifold_learning/,nooob_Master_69,1565365510,[removed],0,1
609,2019-8-10,2019,8,10,0,co45l7,[D] Multi-style disentanglement and Unsupervised aesthetics prediction of music by predicting future and analysing past,https://www.reddit.com/r/MachineLearning/comments/co45l7/d_multistyle_disentanglement_and_unsupervised/,ad48hp,1565365539,"The problem with aesthetics prediction is that it's learned on datasets provided by some of the users which might not reflect the diversity of aesthetics perception of different people and have poor generalization ability..

I've been thinking about usage of Content &amp; Style disentanglement for learning several styles (and also relations between them), and then feed on mini-supervision given by a human by selecting personally most beautiful images, which would make the algorithm look for the most similiar styles..

However, to make the model exposed to as many styles as possible, the model shall have intrinsic motivation (curiosity) to explore those which it struggle more to disentangle than those which it already disentangled (almost) successfully, and then learn to combine them, followed by a model to disentangle several styles &amp; single content..

&amp;#x200B;

The next topic is music creation, today's best-performing models apparently learn on several musical genres, and then synthetize a new sample by starting out-of-scratch and then predicting the next note until it reaches several minutes..

To make the music piece more tense, i believe it might require these steps to be in place..

1.Learn one model to predict the future arrangement of a song at any given time, and then make the generator minimize the certainty of this model

2.Learn another model to analyze the past of the song, and maximize the recognition rate (certainty) of this one by the generator as well..

(These two models may share their knowledge, as the task is done on the same musical pieces..)

So, both of these models are learnt on recognizing genres &amp; learn on their patterns, except the first one focus on future (which has not been heard yet by the model), and the other one on past (which has been already heard)

Obviously, the aesthetics score would be produced by the models rewards based on analyzing/predicting on that specific song (as specified in the two steps above)",0,1
610,2019-8-10,2019,8,10,0,co46l9,"[N] Hi r/ML! You probably know about our TRAINS platform, but do you know why we open-sourced it?",https://www.reddit.com/r/MachineLearning/comments/co46l9/n_hi_rml_you_probably_know_about_our_trains/,LSTMeow,1565365663,"""TRAINS: An open-source, zero-integration tool to boost machine learning research""  
[https://heartbeat.fritz.ai/trains-all-aboard-ba92a728eb6d](https://heartbeat.fritz.ai/trains-all-aboard-ba92a728eb6d) 

In this piece, specifically the second part (Platform 1),  I tried to convey concisely why we made our platform open source, which is something I felt left to hand-waving in my previous posts here.

I would love to hear from r/MachineLearning if that particular message comes through!

... and as usual if anything does not ""magically"" works for you ;)

Context: [First](https://www.reddit.com/r/MachineLearning/comments/c2g2li/n_there_are_many_platforms_to_manage_your_ml/) and [Second](https://www.reddit.com/r/MachineLearning/comments/cl39tn/d_trains_one_month_later_we_got_some_real_nice/) posts here.

The rest of the piece is also recommended if you don't know what TRAINS is or do not  believe it can boost your ml research. Enjoy!

PS. Shout out to [fritz.ai](https://fritz.ai) for hosting us on heartbeat!

PS/2 Mods, this is probably more \[N\] than \[D\] or \[P\], but I can accept it if you change the flair!",0,0
611,2019-8-10,2019,8,10,0,co48d2,[P] Simple PyTorch implementation of Language Model,https://www.reddit.com/r/MachineLearning/comments/co48d2/p_simple_pytorch_implementation_of_language_model/,lyeoni,1565365873,"A step-by-step tutorial on how to implement and adapt simple language model to Wikipedia text.

A pre-trained BERT, XLNET is publicly available ! But, for NLP beginners, like me, It could be hard to use/adapt after full understanding. For them, I covered whole, end-to-end implementation process for language modeling, using recurrent network, we already know. + do not use torchtext ! 

I hope that this repo can be a good solution for people who want to have their own language model :)

https://github.com/lyeoni/pretraining-for-language-understanding",0,0
612,2019-8-10,2019,8,10,1,co4f4z,Alternatives to batch norm to stabilise training of deep CNNs?,https://www.reddit.com/r/MachineLearning/comments/co4f4z/alternatives_to_batch_norm_to_stabilise_training/,IRLIamOffline,1565366684,[removed],0,1
613,2019-8-10,2019,8,10,1,co4oyo,[R] Biological learning curves outperform existing ones in artificial intelligence algorithms,https://www.reddit.com/r/MachineLearning/comments/co4oyo/r_biological_learning_curves_outperform_existing/,zuberuber,1565367850,,0,1
614,2019-8-10,2019,8,10,1,co4pke,Lost in Space  Navigating the new IT landscape in the Life Science Industry,https://www.reddit.com/r/MachineLearning/comments/co4pke/lost_in_space_navigating_the_new_it_landscape_in/,rosamarts,1565367926,,0,1
615,2019-8-10,2019,8,10,1,co5447,What should I do after finishing the Deep Learning Specialization by Andrew Ng on coursera.,https://www.reddit.com/r/MachineLearning/comments/co5447/what_should_i_do_after_finishing_the_deep/,DarkStealther,1565369688,[removed],0,1
616,2019-8-10,2019,8,10,2,co5bfv,[P] entity resolution system for large-scale databases,https://www.reddit.com/r/MachineLearning/comments/co5bfv/p_entity_resolution_system_for_largescale/,tupini07,1565370554,"Hello everyone,

I'd like to share some insights about a [Wikimedia Foundation](https://wikimediafoundation.org/) project I've been contributing to.

**soweego** is an *entity resolution* system that links the [Wikidata](https://www.wikidata.org/) knowledge base to large external databases through a set of supervised algorithms:
https://soweego.readthedocs.io/

Specifically, we leveraged *Bernoulli Nave Bayes, Linear Support Vector Machines, Single-layer Perceptrons,* and *Multi-layer Perceptrons*. As an interesting finding, models based on Single-layer Perceptrons are the ones that work best for our input datasets, namely [Discogs](https://www.discogs.com/), [IMDb](https://www.imdb.com/), and [MusicBrainz](https://musicbrainz.org/).

soweego partners with [Mix'n'match](https://tools.wmflabs.org/mix-n-match/), which mainly deals with small catalogs. soweego is currently uploading *255 k confident* identifiers to Wikidata, see [its activity](https://xtools.wmflabs.org/ec/wikidata.org/Soweego%20bot). *126 k medium-confident* links are instead getting into Mix'n'match for curation.

The soweego team has also worked hard to address the following community requests:

1. sync Wikidata to external databases and check them to spot inconsistencies in Wikidata;
2. import new databases with reasonable effort.

If you like the project, please consider starring it on GitHub:
https://github.com/Wikidata/soweego",2,5
617,2019-8-10,2019,8,10,2,co5mqm,Papers on ML techniques applied on honeypot data ?,https://www.reddit.com/r/MachineLearning/comments/co5mqm/papers_on_ml_techniques_applied_on_honeypot_data/,toshn_,1565371918,,0,1
618,2019-8-10,2019,8,10,2,co5sua,[D] How to convert a pretrained TensorFlow model to PyTorch - a simple workflow and a few lessons learned,https://www.reddit.com/r/MachineLearning/comments/co5sua/d_how_to_convert_a_pretrained_tensorflow_model_to/,Thomjazz,1565372614,"A simple guide by HuggingFace on how to convert a pretrained TensorFlow model in PyTorch easily and reliably. 

The blog post summarizes the workflow they are using to make fast and accurate TensorFlow to PyTorch conversions and share some lessons learned from reimplementing a bunch of TensorFlow models in the [pytorch-transformers](https://github.com/huggingface/pytorch-transformers) open-source library.

Here is the blog post: [https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28](https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28)",9,18
619,2019-8-10,2019,8,10,2,co5zzm,Youre a data scientist ? 5 Tips For Getting Job,https://www.reddit.com/r/MachineLearning/comments/co5zzm/youre_a_data_scientist_5_tips_for_getting_job/,sajad-52,1565373499,,0,0
620,2019-8-10,2019,8,10,3,co6sng,[R] Biological learning curves outperform existing ones in artificial intelligence algorithms,https://www.reddit.com/r/MachineLearning/comments/co6sng/r_biological_learning_curves_outperform_existing/,zuberuber,1565376970,,0,1
621,2019-8-10,2019,8,10,4,co764o,[D] Facebook AI interview process,https://www.reddit.com/r/MachineLearning/comments/co764o/d_facebook_ai_interview_process/,fb_ai_7262,1565378619,"Hi /r/MachineLearning, 

I am interviewing onsite with Facebook AI later this month, as a researcher in one of their applied Machine Learning team, and was wondering if anyone here has gone through their process. What kind of questions did they ask, how where the interviews conducted? How did you find the experience?

I have taken several interviews before focused on coding, so I am familiar with those, but I was told at Facebook they have ""domain"" and ""out-of-domain"" interview rounds where I will be asked questions by ML experts, on things both in and outside of my area of research expertise, so I am a bit scared of what to expect as I never went through something like this before.

Would be super grateful to any advice or insights into the process!",13,6
622,2019-8-10,2019,8,10,5,co7o24,Semantic Based Adversarial Examples Fool Face Recognition,https://www.reddit.com/r/MachineLearning/comments/co7o24/semantic_based_adversarial_examples_fool_face/,Yuqing7,1565380859,,0,1
623,2019-8-10,2019,8,10,5,co7xzv,Google AI &amp; DeepMind Temporal Cycle-Consistency Learning Understands Every Video Frame,https://www.reddit.com/r/MachineLearning/comments/co7xzv/google_ai_deepmind_temporal_cycleconsistency/,Yuqing7,1565382085,,0,1
624,2019-8-10,2019,8,10,5,co7yj4,Project Malmo (agent based learning platform on Minecraft),https://www.reddit.com/r/MachineLearning/comments/co7yj4/project_malmo_agent_based_learning_platform_on/,timscarfe,1565382149,[removed],0,1
625,2019-8-10,2019,8,10,5,co82d4,Is Naive Bayes random?,https://www.reddit.com/r/MachineLearning/comments/co82d4/is_naive_bayes_random/,benbellmusic,1565382612,"Hi, I'm using Naive Bayes for prediction, but seems to give different answers when I run it. But shouln't naive bayes be totally deterministic? If not, how can you set a random seed? I'm using python and sklearn by the way.",0,1
626,2019-8-10,2019,8,10,6,co8lta,How do Histogram of Oriented Gradients descriptors + Linear Support-Vector Machine work together to localize a face in an image?,https://www.reddit.com/r/MachineLearning/comments/co8lta/how_do_histogram_of_oriented_gradients/,EverydayQuestion,1565385079,[removed],0,1
627,2019-8-10,2019,8,10,6,co971s,Made a small RaspPi project showing the nodes activation in a tiny ANN with binary classification task,https://www.reddit.com/r/MachineLearning/comments/co971s/made_a_small_rasppi_project_showing_the_nodes/,carnivorousdrew,1565387818,,1,1
628,2019-8-10,2019,8,10,7,co9g4f,Made a small RaspPi project showing the activation nodes in a MLP for binary classification,https://www.reddit.com/r/MachineLearning/comments/co9g4f/made_a_small_rasppi_project_showing_the/,carnivorousdrew,1565389017,,0,1
629,2019-8-10,2019,8,10,8,coa618,[D] Should tokens with a very small frequency be removed from the vocabulary before training a word2vec type model?,https://www.reddit.com/r/MachineLearning/comments/coa618/d_should_tokens_with_a_very_small_frequency_be/,searchingundergrad,1565392581,I believe they should as there is only one/few contexts they are used in so it wouldnt be possible to learn a good representation for the token.,7,1
630,2019-8-10,2019,8,10,8,coanxv,need help building an NLP model,https://www.reddit.com/r/MachineLearning/comments/coanxv/need_help_building_an_nlp_model/,gulzainali,1565395187,[removed],0,1
631,2019-8-10,2019,8,10,10,cobwhu,[P] These Lyrics Do Not Exist,https://www.reddit.com/r/MachineLearning/comments/cobwhu/p_these_lyrics_do_not_exist/,itsmybirthday19,1565401996,,1,1
632,2019-8-10,2019,8,10,11,coc09l,[P] These Lyrics Do Not Exist,https://www.reddit.com/r/MachineLearning/comments/coc09l/p_these_lyrics_do_not_exist/,itsmybirthday19,1565402595,"I have trained a songwriter Artificial Intelligence that creates you completely original song lyrics! 

Every generated line is a brand new sentence that has never been in any other song

You just provide a song topic word, then press Generate Lyrics for completely new song lyrics

Please let me know if you have any ideas for improvements

Link:  [https://theselyricsdonotexist.com/](https://theselyricsdonotexist.com/) ",62,163
633,2019-8-10,2019,8,10,12,cocx4e,What are some Intermediate and advanced topics in Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/cocx4e/what_are_some_intermediate_and_advanced_topics_in/,Nick-Conner,1565407921,,0,1
634,2019-8-10,2019,8,10,16,coez9z,[D] We made a ML model for detecting arrow and color at the specific time. Would like to discuss about it and gain some advice/suggestion to improve it from this amazing sub.,https://www.reddit.com/r/MachineLearning/comments/coez9z/d_we_made_a_ml_model_for_detecting_arrow_and/,bathon,1565421541,,0,1
635,2019-8-10,2019,8,10,16,cofay1,[P] Simple PyTorch implementation of Recurrent Language Model,https://www.reddit.com/r/MachineLearning/comments/cofay1/p_simple_pytorch_implementation_of_recurrent/,lyeoni,1565423991,"A step-by-step tutorial on how to implement and adapt recurrent language model to Wikipedia text.

A pre-trained BERT, XLNET is publicly available ! But, for NLP beginners, like me, It could be hard to use/adapt after full understanding. For them, I covered whole, end-to-end implementation process for language modeling, using recurrent network, we already know. + do not use torchtext !

I hope that this repo can be a good solution for people who want to have their own language model :)

https://github.com/lyeoni/pretraining-for-language-understanding",0,0
636,2019-8-10,2019,8,10,17,cofgle,Hadoop Administration online course video lectures,https://www.reddit.com/r/MachineLearning/comments/cofgle/hadoop_administration_online_course_video_lectures/,freevideolectures,1565425199,,1,1
637,2019-8-10,2019,8,10,18,cofxv3,Suggestions about which ML algorithm to use,https://www.reddit.com/r/MachineLearning/comments/cofxv3/suggestions_about_which_ml_algorithm_to_use/,darkhorse_5712,1565429004,[removed],0,1
638,2019-8-10,2019,8,10,20,cogun8,"""Difference Between Artificial Intelligence, Machine Learning and Deep Learning""",https://www.reddit.com/r/MachineLearning/comments/cogun8/difference_between_artificial_intelligence/,susanvilleula1,1565436127,,0,1
639,2019-8-10,2019,8,10,20,coh3iv,[Discusaion] What are the up-to-date dataset for benchmark?,https://www.reddit.com/r/MachineLearning/comments/coh3iv/discusaion_what_are_the_uptodate_dataset_for/,RTengx,1565437947,[removed],0,1
640,2019-8-10,2019,8,10,21,cohcw8,Designing a Learning System | The first step to Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cohcw8/designing_a_learning_system_the_first_step_to/,studytonight,1565439686,,0,1
641,2019-8-10,2019,8,10,22,cohwdo,Graduation Project ideas ?,https://www.reddit.com/r/MachineLearning/comments/cohwdo/graduation_project_ideas/,MohamedRashad,1565442959,"I would love to hear some ideas for my GP which uses Deep Learning in its core to make something helpful to others.

I was thinking about drug discovery, medical image segmentation and brain-machine interface but i am afraid they become harder than i think .... so if someone here will make a project in one of these areas how he would approach it and what application he would aim to build \^\^

&amp;#x200B;

Note: I am open to other ideas too as i have time to study and plan for them.",0,1
642,2019-8-10,2019,8,10,22,cohx6q,"tutorial Friday freestyle the single eagle, double eagle, sun wave and m...",https://www.reddit.com/r/MachineLearning/comments/cohx6q/tutorial_friday_freestyle_the_single_eagle_double/,thetrickshotone,1565443069,,0,1
643,2019-8-10,2019,8,10,23,coimmc,Looking for a general English dataset,https://www.reddit.com/r/MachineLearning/comments/coimmc/looking_for_a_general_english_dataset/,lolidunnowut,1565446866,[removed],0,1
644,2019-8-10,2019,8,10,23,coirsd,Is OpenAI going to release their MuseNet model?,https://www.reddit.com/r/MachineLearning/comments/coirsd/is_openai_going_to_release_their_musenet_model/,NMister_,1565447632,[removed],0,1
645,2019-8-10,2019,8,10,23,coitic,[D] O'Reilly Machine Learning book pack ends in the next couple of days,https://www.reddit.com/r/MachineLearning/comments/coitic/d_oreilly_machine_learning_book_pack_ends_in_the/,CuriousRestaurant,1565447867,,0,1
646,2019-8-10,2019,8,10,23,coiyec,[D] O'Reilly Machine Learning book pack ends in the next couple of days,https://www.reddit.com/r/MachineLearning/comments/coiyec/d_oreilly_machine_learning_book_pack_ends_in_the/,CuriousRestaurant,1565448585," I got the whole bundle last week. It is pretty valuable and so are O'Reilly ebooks imo.

I thought you would appreciate the reminder since it will end soon.

&amp;#x200B;

[Source](https://twitter.com/Machinecentral/status/1160158936881946628)",1,46
647,2019-8-11,2019,8,11,0,cojghv,[D] Question about deep Q learning,https://www.reddit.com/r/MachineLearning/comments/cojghv/d_question_about_deep_q_learning/,Kralex68,1565451089,"Hello I am implementing deep Q learning for a 2 player board game. After every move it is the turn of the other player. I want to calculate max(Q',a') for getting the max Q value for the next state but my problem is that the next state represents the quality for the opponent player. So max Q is the max quality value for my opponent(But I want to maximize MY win chances) How do I proceed? Should I calculate all states where it is again my turn?(2 depth)",7,3
648,2019-8-11,2019,8,11,0,cojp68,CNN+SVM,https://www.reddit.com/r/MachineLearning/comments/cojp68/cnnsvm/,14nmCMOS,1565452268,[removed],0,1
649,2019-8-11,2019,8,11,0,cojq9n,Are OpenAI resources good for learning AI from scratch?,https://www.reddit.com/r/MachineLearning/comments/cojq9n/are_openai_resources_good_for_learning_ai_from/,Civicricer,1565452416,[removed],0,1
650,2019-8-11,2019,8,11,0,cojrnp,"Artificial Intelligence Can Now Create Perfumes, Even Without A Sense Of Smell",https://www.reddit.com/r/MachineLearning/comments/cojrnp/artificial_intelligence_can_now_create_perfumes/,jonfla,1565452613,,0,1
651,2019-8-11,2019,8,11,1,cojwhk,[D] Keras with older hardware?,https://www.reddit.com/r/MachineLearning/comments/cojwhk/d_keras_with_older_hardware/,ReasonablyBadass,1565453251,"It turns out my GPU has only around 1GB of storage and even comparatively little models create OOM errors.

Are there ways around that? Can maybe the regular RAM act to hold the data? 

Or is 1GB just too little room to work with and I need a new card?",10,4
652,2019-8-11,2019,8,11,1,cojzg1,Coursera's Machine Learning course from scratch in Python! (Zero use of Data Modelling libs.),https://www.reddit.com/r/MachineLearning/comments/cojzg1/courseras_machine_learning_course_from_scratch_in/,rj81309050,1565453636,[removed],0,1
653,2019-8-11,2019,8,11,1,cojzj1,[N] AI pioneer accused of having sex with trafficking victim on Jeffrey Epsteins island,https://www.reddit.com/r/MachineLearning/comments/cojzj1/n_ai_pioneer_accused_of_having_sex_with/,MassiveContact,1565453648,"A victim of billionaire Jeffrey Epstein testified that she was forced to have sex with MIT professor Marvin Minsky, as revealed in a newly unsealed deposition. Epstein was registered as a sex offender in 2008 as part of a controversial plea deal. More recently, he was arrested on charges of sex trafficking amid a flood of new allegations.

Minsky, who died in 2016, was known as an associate of Epstein, but this is the first direct accusation implicating the AI pioneer in Epsteins broader sex trafficking network. The deposition also names Prince Andrew of Britain and former New Mexico governor Bill Richardson, among others.

The accusation against Minsky was made by Virginia Giuffre, who was deposed in May 2016 as part of a broader defamation suit between her and an Epstein associate named Ghislaine Maxwell. In the deposition, Giuffre says she was directed to have sex with Minsky when he visited Epsteins compound in the US Virgin Islands.

As part of the defamation suit, Maxwells counsel denied the allegations, calling them salacious and improper. Representatives for Giuffre and Maxwell did not immediately respond to a request for comment.

A separate witness lent credence to Giuffres account, testifying that she and Minsky had taken a private plane from Teterboro to Santa Fe and Palm Beach in March 2001. Epstein, Maxwell, chef Adam Perry Lang, and shipping heir Henry Jarecki were also passengers on the flight, according to the deposition. At the time of the flight, Giuffre was 17; Minsky was 73.

Got a tip for us? Use SecureDrop or Signal to securely send messages and files to The Verge without revealing your identity. Chris Welch can be reached by Signal at (845) 445-8455.

A pivotal member of MITs Artificial Intelligence Lab, Marvin Minsky pioneered the first generation of self-training algorithms, establishing the concept of artificial neural networks in his 1969 book Perceptrons. He also developed the first head-mounted display, a precursor to modern VR and augmented reality systems.

Minsky was one of a number of prominent scientists with ties to Jeffrey Epstein, who often called himself a science philanthropist and donated to research projects and academic institutions. Many of those scientists were affiliated with Harvard, including physicist Lawrence Krauss, geneticist George Church, and cognitive psychologist Steven Pinker. Minskys affiliation with Epstein went particularly deep, including organizing a two-day symposium on artificial intelligence at Epsteins private island in 2002, as reported by Slate. In 2012, the Jeffrey Epstein Foundation issued a press release touting another conference organized by Minsky on the island in December 2011.

That private island is alleged to have been the site of an immense sex trafficking ring. But Epstein associates have argued that those crimes were not apparent to Epsteins social relations, despite the presence of young women at many of his gatherings.

These people were seen not only by me, Alan Dershowitz argued in a 2015 deposition. They were seen by Larry Summers, they were seen by \[George\] Church, they were seen by Marvin Minsky, they were seen by some of the most eminent academics and scholars in the world.

There was no hint or suggestion of anything sexual or improper in the presence of these people, Dershowitz continued.

&amp;#x200B;

[https://www.theverge.com/2019/8/9/20798900/marvin-minsky-jeffrey-epstein-sex-trafficking-island-court-records-unsealed](https://www.theverge.com/2019/8/9/20798900/marvin-minsky-jeffrey-epstein-sex-trafficking-island-court-records-unsealed)",0,1
654,2019-8-11,2019,8,11,1,cok47z,[N] AI pioneer Marvin Minsky accused of having sex with trafficking victim on Jeffrey Epsteins island,https://www.reddit.com/r/MachineLearning/comments/cok47z/n_ai_pioneer_marvin_minsky_accused_of_having_sex/,MassiveContact,1565454271,"A victim of billionaire Jeffrey Epstein testified that she was forced to have sex with MIT professor Marvin Minsky, as revealed in a newly unsealed deposition. Epstein was registered as a sex offender in 2008 as part of a controversial plea deal. More recently, he was arrested on charges of sex trafficking amid a flood of new allegations.

Minsky, who died in 2016, was known as an associate of Epstein, but this is the first direct accusation implicating the AI pioneer in Epsteins broader sex trafficking network. The deposition also names Prince Andrew of Britain and former New Mexico governor Bill Richardson, among others.

The accusation against Minsky was made by Virginia Giuffre, who was deposed in May 2016 as part of a broader defamation suit between her and an Epstein associate named Ghislaine Maxwell. In the deposition, Giuffre says she was directed to have sex with Minsky when he visited Epsteins compound in the US Virgin Islands.

As part of the defamation suit, Maxwells counsel denied the allegations, calling them salacious and improper. Representatives for Giuffre and Maxwell did not immediately respond to a request for comment.

A separate witness lent credence to Giuffres account, testifying that she and Minsky had taken a private plane from Teterboro to Santa Fe and Palm Beach in March 2001. Epstein, Maxwell, chef Adam Perry Lang, and shipping heir Henry Jarecki were also passengers on the flight, according to the deposition. At the time of the flight, Giuffre was 17; Minsky was 73.

Got a tip for us? Use SecureDrop or Signal to securely send messages and files to The Verge without revealing your identity. Chris Welch can be reached by Signal at (845) 445-8455.

A pivotal member of MITs Artificial Intelligence Lab, Marvin Minsky pioneered the first generation of self-training algorithms, establishing the concept of artificial neural networks in his 1969 book Perceptrons. He also developed the first head-mounted display, a precursor to modern VR and augmented reality systems.

Minsky was one of a number of prominent scientists with ties to Jeffrey Epstein, who often called himself a science philanthropist and donated to research projects and academic institutions. Many of those scientists were affiliated with Harvard, including physicist Lawrence Krauss, geneticist George Church, and cognitive psychologist Steven Pinker. Minskys affiliation with Epstein went particularly deep, including organizing a two-day symposium on artificial intelligence at Epsteins private island in 2002, as reported by Slate. In 2012, the Jeffrey Epstein Foundation issued a press release touting another conference organized by Minsky on the island in December 2011.

That private island is alleged to have been the site of an immense sex trafficking ring. But Epstein associates have argued that those crimes were not apparent to Epsteins social relations, despite the presence of young women at many of his gatherings.

These people were seen not only by me, Alan Dershowitz argued in a 2015 deposition. They were seen by Larry Summers, they were seen by \[George\] Church, they were seen by Marvin Minsky, they were seen by some of the most eminent academics and scholars in the world.

There was no hint or suggestion of anything sexual or improper in the presence of these people, Dershowitz continued.

&amp;#x200B;

[https://www.theverge.com/2019/8/9/20798900/marvin-minsky-jeffrey-epstein-sex-trafficking-island-court-records-unsealed](https://www.theverge.com/2019/8/9/20798900/marvin-minsky-jeffrey-epstein-sex-trafficking-island-court-records-unsealed)",310,604
655,2019-8-11,2019,8,11,2,cokofr,Help with DL framework.,https://www.reddit.com/r/MachineLearning/comments/cokofr/help_with_dl_framework/,ragingpot,1565456918,[removed],0,1
656,2019-8-11,2019,8,11,4,coman0,How do computer scientists avoid bias in a machine learning algorithm?,https://www.reddit.com/r/MachineLearning/comments/coman0/how_do_computer_scientists_avoid_bias_in_a/,TheArchivist314,1565464592,[removed],0,1
657,2019-8-11,2019,8,11,4,comlb8,/r/TensorFlowJS - New subreddit for TensorFlow.js (machine learning library for JavaScript &amp; the web browser),https://www.reddit.com/r/MachineLearning/comments/comlb8/rtensorflowjs_new_subreddit_for_tensorflowjs/,wildcamp,1565465998,[removed],0,1
658,2019-8-11,2019,8,11,6,conm6k,Want to learn C++,https://www.reddit.com/r/MachineLearning/comments/conm6k/want_to_learn_c/,nishatislam,1565471000,[removed],0,1
659,2019-8-11,2019,8,11,8,copc1o,[Research] Where can I find an algorithm that plots data like this?,https://www.reddit.com/r/MachineLearning/comments/copc1o/research_where_can_i_find_an_algorithm_that_plots/,fanchiotti,1565479709,"Hello everyone,

Where can I find an algorithm that plots the steering wheel data like on the figure four of [this](https://arxiv.org/pdf/1710.02410.pdf) paper?

Any help is much appreciated. I can't seem to find anything online.

Thank you in advance.",1,0
660,2019-8-11,2019,8,11,11,cor812,Maintaining loss while decreasing number of epochs,https://www.reddit.com/r/MachineLearning/comments/cor812/maintaining_loss_while_decreasing_number_of_epochs/,atomicalexx,1565490452,[removed],0,1
661,2019-8-11,2019,8,11,12,corq6r,[D] Maths datasets and math problem solving models,https://www.reddit.com/r/MachineLearning/comments/corq6r/d_maths_datasets_and_math_problem_solving_models/,tsauri,1565493417," Are there math dataset out there with step-by-step solutions?  
Something like [Deepmind maths dataset](https://github.com/deepmind/mathematics_dataset), but includes step by step like Mathematica or Wolfram Alpha.

Also are there papers related to fully differentiable models of Mathematica/Wolfram Alpha? Tried looking into program synthesis works but can't find one",0,4
662,2019-8-11,2019,8,11,12,cortlq,[D] Parallel multi-task learning vs. continual learning,https://www.reddit.com/r/MachineLearning/comments/cortlq/d_parallel_multitask_learning_vs_continual/,vernunftig,1565493980,"Assuming we want to learn k tasks jointly, and the data for all tasks are available. We may either train a model with parallel multi-task learning (eg. each batch is a mixture of samples from the k tasks), or present tasks sequentially (eg. switch to a different task once every 5k time steps). The latter is kind of like continual learning, except that the set of tasks is fixed and there won't be new ones.  Which training paradigm yields better results? Any paper that gives theoretical analysis or makes empirical comparisons?",1,8
663,2019-8-11,2019,8,11,13,cosos6,What are the possible applications of Human Pose Estimation?,https://www.reddit.com/r/MachineLearning/comments/cosos6/what_are_the_possible_applications_of_human_pose/,myidispg,1565499329,[removed],0,1
664,2019-8-11,2019,8,11,14,cot2xm,"[N] Facebook launches online Global Pytorch Hackathon. $61,000 in prizes. Submissions due Sept 16th.",https://www.reddit.com/r/MachineLearning/comments/cot2xm/n_facebook_launches_online_global_pytorch/,Research2Vec,1565502018,"https://pytorch.devpost.com/

I had the pleasure of attending their in person hackathon at Menlo Park yesterday. If you want some inspiration for potential projects, checkout their submissions page here, they were really good. 

https://pytorchmpk.devpost.com/submissions

Pytorch rolled a bunch of new features out a few days ago. They seem to be really stepping up in response to TF 2.0. 

If you're looking for teammates, signup on the page, then you can look at other profiles of those looking for teammates

https://pytorch.devpost.com/participants",50,274
665,2019-8-11,2019,8,11,14,cot8an,Tool for visualizing the training of neural networks in a web browser?,https://www.reddit.com/r/MachineLearning/comments/cot8an/tool_for_visualizing_the_training_of_neural/,notreallysocool,1565503118,,0,1
666,2019-8-11,2019,8,11,16,cotpze,Why an AI arms race with China would be bad for humanity,https://www.reddit.com/r/MachineLearning/comments/cotpze/why_an_ai_arms_race_with_china_would_be_bad_for/,aerowindwalker,1565506868,,0,1
667,2019-8-11,2019,8,11,17,couedn,[R] Improving Adversarial Robustness via Guided Complement Entropy (ICCV'19),https://www.reddit.com/r/MachineLearning/comments/couedn/r_improving_adversarial_robustness_via_guided/,henry8527,1565512443,"**TL;DR:** We propose a new training paradigm called Guided Complement Entropy (GCE) that is capable of achieving **""adversarial defense for free,""** which involves no additional procedures in the process of improving adversarial robustness. In addition to maximizing model probabilities on the ground-truth class like cross-entropy, we neutralize its probabilities on the incorrect classes along with a ""guided"" term to balance between these two terms. We show in the experiments that our method achieves better model robustness with even better performance compared to the commonly used cross-entropy training objective.

&amp;#x200B;

Full paper: [https://arxiv.org/abs/1903.09799](https://arxiv.org/abs/1903.09799)

Github: [https://github.com/henry8527/GCE](https://github.com/henry8527/GCE)",8,9
668,2019-8-11,2019,8,11,18,coumvr,[Project] `gpt2-client`: A New Wrapper for GPT-2,https://www.reddit.com/r/MachineLearning/comments/coumvr/project_gpt2client_a_new_wrapper_for_gpt2/,rish-16,1565514484,"Hey everyone 

I recently built a wrapper for OpenAI's \`gpt-2\` model called \`gpt2-client\`. Currently, the \`gpt-2\` repo is archived and the code is messy and riddled with bugs. My wrapper simplifies the entire process by enabling anyone to get started with text generation models without the fuss.

[It looks something like this](https://i.redd.it/we9xjzbocsf31.png)

Please do go check it out here:

[https://github.com/rish-16/gpt2client](https://github.com/rish-16/gpt2client)

If you like it, a on GitHub would be highly appreciated! It's my first ever Python module I've released and am really excited about it.

If you run into any bugs, please do file an issue and if you have any suggestions or enhancements, please do file a PR with a short description of your awesome improvement.

Cheers!",0,0
669,2019-8-11,2019,8,11,18,coupe3,[Project] `gpt2-client`: A New Wrapper for GPT-2,https://www.reddit.com/r/MachineLearning/comments/coupe3/project_gpt2client_a_new_wrapper_for_gpt2/,rish-16,1565515088,"Hey everyone 

I recently built a wrapper for OpenAI's \`gpt-2\` model called \`gpt2-client\`. Currently, the \`gpt-2\` repo is archived and the code is messy and riddled with bugs. My wrapper simplifies the entire process by enabling anyone to get started with text generation models without the fuss.

&amp;#x200B;

![img](6j31gd0vcsf31 ""It looks something like this"")

Please do go check it out here:

[https://github.com/rish-16/gpt2client](https://github.com/rish-16/gpt2client)

If you like it, a on GitHub would be highly appreciated! It's my first ever Python module I've released and am really excited about it.

If you run into any bugs, please do file an issue and if you have any suggestions or enhancements, please do file a PR with a short description of your awesome improvement.

Cheers!",27,51
670,2019-8-11,2019,8,11,18,couzju,Reviews and literature on the application of RNN's to modeling prefrontal cortex dynamics,https://www.reddit.com/r/MachineLearning/comments/couzju/reviews_and_literature_on_the_application_of_rnns/,Stereoisomer,1565517586,"Hi all,

I'm about to start a PhD in cognitive/computational neuroscience and I was having trouble finding some good background on this but I was wondering if anyone here has some good suggestions for some good reviews or landmark pieces of literature on the study of RNN's for modeling neural dynamics especially in prefrontal cortex?

I'm mostly thinking along the lines of Earl Miller's recent work in applying models using reservoir computing or the Shenoy labs use of a sequential variational autoencoder (LFADS) for modeling neural state space trajectories. I have a BS and MS in Applied Math so technical reviews that unify and lend generality are preferred such as, my all-time favorite, A Unifying Review of Gaussian Linear Models by Roweis and Ghahramani (but for RNN's). 

Thanks in advance!",0,1
671,2019-8-11,2019,8,11,19,covedn,[D] CNN Image Segmentation: Why do UNET-like architectures outperform sliding-window approaches?,https://www.reddit.com/r/MachineLearning/comments/covedn/d_cnn_image_segmentation_why_do_unetlike/,automatedredditor,1565520928,"I'm writing a thesis that heavily focuses on semantic segmentation of biomedical images.

I'm reviewing different segmentation approaches, identifying two main approach branches:

* A **sliding window**\-like approach: a classification network is used over different patches of original image to reconstruct a pixel-by-pixel estimates of the probability maps.
* A **full-image** approach: like the FCNN and UNET approach, rely on fully convolutional architectures and the upscaling phase is incorporated in the network itself using transposed convolutions.[https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)

The second approach clearly outperforms the first one. I have a vague hunch on why this happens: my hypothesis is that the transposed-convolution operations, being at their core local operations, force local criteria on the segmentation of close pixels so that pixel contiguity is heavily encouraged in the fully convolutional case.

I do not find this kind of explanation satisfying because of two reasons:

1. I do not have papers or real data to support this: I cannot seem to find any paper on the theme.
2. The sliding-window approach has a built-in form of local consistency as well: if overlapping windows share most of the pixels it's reasonable to think that - given the network is not totally chaotic and shows enough linearity - the outputs would be similar.

Do anyone have a bit of insight or source on any of this? Any contribution, even brainstorming or unsupported hypothesis (like mine) is well appreciated.",21,44
672,2019-8-11,2019,8,11,20,covvr5,Turn your brain into a FaceGAN with this cool optical illusion,https://www.reddit.com/r/MachineLearning/comments/covvr5/turn_your_brain_into_a_facegan_with_this_cool/,code_refactor,1565524596,[removed],0,1
673,2019-8-11,2019,8,11,21,cow2so,"best #Telegram group about #MachineLearning ,#DeepLearning",https://www.reddit.com/r/MachineLearning/comments/cow2so/best_telegram_group_about_machinelearning/,Doctor_who1,1565525957,[removed],0,1
674,2019-8-11,2019,8,11,21,cow43e,"My AI is so bright, I gotta wear shades.",https://www.reddit.com/r/MachineLearning/comments/cow43e/my_ai_is_so_bright_i_gotta_wear_shades/,nickbild,1565526209,[removed],0,2
675,2019-8-11,2019,8,11,21,cow504,"Harvard University offers a free ""Data Science: Machine Learning"" course online through edX.",https://www.reddit.com/r/MachineLearning/comments/cow504/harvard_university_offers_a_free_data_science/,Agent_ANAKIN,1565526383,,0,1
676,2019-8-11,2019,8,11,21,cowfjc,Final Project for a class in my Masters: Website for VQA on mobile device - https://www.tiki.systems/,https://www.reddit.com/r/MachineLearning/comments/cowfjc/final_project_for_a_class_in_my_masters_website/,veryseriouspeople,1565528300,,1,1
677,2019-8-11,2019,8,11,21,cowfqn,[D] Machine Learning papers through art glasses,https://www.reddit.com/r/MachineLearning/comments/cowfqn/d_machine_learning_papers_through_art_glasses/,postmachines,1565528334,"Ilya Repin's quote: At first the artist paints simply and badly, then complicated and badly, then complicated and well, and only then simply and well.  I think it is maybe actually for ML papers too...

Share papers that you think are simply and well or complicated and well. To see really bad examples is also interesting.

Thanks!",0,0
678,2019-8-11,2019,8,11,22,cowgpo,"Difference Between Artificial Intelligence, Machine Learning and Deep Learning",https://www.reddit.com/r/MachineLearning/comments/cowgpo/difference_between_artificial_intelligence/,susanvilleula1,1565528504,,0,1
679,2019-8-11,2019,8,11,23,coxh0m,Hi! I am a new student in Machine Learning.Can anyone help me solving this exercise or giving me any suggestion?Thanks in advance!,https://www.reddit.com/r/MachineLearning/comments/coxh0m/hi_i_am_a_new_student_in_machine_learningcan/,GigaAna,1565534279,[removed],0,1
680,2019-8-12,2019,8,12,0,coxre4,leela-zero NN architecture,https://www.reddit.com/r/MachineLearning/comments/coxre4/leelazero_nn_architecture/,promach,1565535786,,0,1
681,2019-8-12,2019,8,12,0,coxzvg,[R] Theories of Error Back-Propagation in the Brain,https://www.reddit.com/r/MachineLearning/comments/coxzvg/r_theories_of_error_backpropagation_in_the_brain/,jakn,1565536937,,0,3
682,2019-8-12,2019,8,12,0,coy7et,[D] Why do we keep seeing articles like this?,https://www.reddit.com/r/MachineLearning/comments/coy7et/d_why_do_we_keep_seeing_articles_like_this/,gar1t,1565537952,"# Why You Should Document Your Work As a Data Scientist

# [https://towardsdatascience.com/why-you-should-document-your-work-as-a-data-scientist-a265af8a373](https://towardsdatascience.com/why-you-should-document-your-work-as-a-data-scientist-a265af8a373)

Setting aside the ever-decreasing length and depth of Medium posts these days, I'm puzzled why basic topics of ""communicate wtf you're doing"" in data science seem novel in 2019. It seems like someone's ringing this bell once a week.

Is this an actual problem?",0,1
683,2019-8-12,2019,8,12,1,coyive,Training a Neural Network? Start here!,https://www.reddit.com/r/MachineLearning/comments/coyive/training_a_neural_network_start_here/,0_marauders_0,1565539496,,0,1
684,2019-8-12,2019,8,12,1,coykqs,[D] Why do we keep seeing articles like this?,https://www.reddit.com/r/MachineLearning/comments/coykqs/d_why_do_we_keep_seeing_articles_like_this/,gar1t,1565539751,"[Why You Should Document Your Work As a Data Scientist](https://towardsdatascience.com/why-you-should-document-your-work-as-a-data-scientist-a265af8a373)

Setting aside the ever-decreasing length and depth of Medium posts, I'm puzzled why basic topics of ""communicate wtf you're doing"" in data science seem novel in 2019. It seems like someone's ringing this bell once a week.

Is this a problem that merits a patronizing call to action?",9,6
685,2019-8-12,2019,8,12,1,coywwt,Can a Nvidia Jetson run Tacotron 2 model creation?,https://www.reddit.com/r/MachineLearning/comments/coywwt/can_a_nvidia_jetson_run_tacotron_2_model_creation/,goiter12345,1565541349,[removed],0,1
686,2019-8-12,2019,8,12,1,coz2az,"Niches and developing technologies in AI, Big Data, Data Science, Cyber Security",https://www.reddit.com/r/MachineLearning/comments/coz2az/niches_and_developing_technologies_in_ai_big_data/,Ojamenustik,1565542069,[removed],0,1
687,2019-8-12,2019,8,12,2,cozxqe,"in 13 hours, EMNLP 2019 should notify folks whose papers were accepted.",https://www.reddit.com/r/MachineLearning/comments/cozxqe/in_13_hours_emnlp_2019_should_notify_folks_whose/,Best_Mord_Brazil,1565546176,[removed],0,1
688,2019-8-12,2019,8,12,3,cp01pv,10 Must-Try Open Source Tools for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cp01pv/10_musttry_open_source_tools_for_machine_learning/,nzxbvccsg,1565546686,,0,1
689,2019-8-12,2019,8,12,3,cp0jjf,ML Researchers: what are some good ways that you have found to explain machine learning in general as well as your own specific research area to those with little to no computer science or math background?,https://www.reddit.com/r/MachineLearning/comments/cp0jjf/ml_researchers_what_are_some_good_ways_that_you/,rickbo3,1565548974,[removed],0,1
690,2019-8-12,2019,8,12,3,cp0r57,Interesting Job Posting: Machine Learning Living Library,https://www.reddit.com/r/MachineLearning/comments/cp0r57/interesting_job_posting_machine_learning_living/,sayounh,1565549942,[removed],0,1
691,2019-8-12,2019,8,12,4,cp0s27,[D] Machine Learning Researchers: What are some good methods that you have used to communicate both your specific area of research and ML in general to those without any math or computer science backgrounds.,https://www.reddit.com/r/MachineLearning/comments/cp0s27/d_machine_learning_researchers_what_are_some_good/,rickbo3,1565550055,"I recently began doing professional ML research in medical areas, and some family and friends are excited for me and want to understand what I do. How can I explain it to them without coming off condescending or over-simplistic, if they don't have even a calculus background. (Reposting b/c I forgot to tag the first time)",7,4
692,2019-8-12,2019,8,12,4,cp0vuj,[R] Interesting Job Posting; Machine Learning Living Library,https://www.reddit.com/r/MachineLearning/comments/cp0vuj/r_interesting_job_posting_machine_learning_living/,sayounh,1565550541,"  
For those of you who are always on ArXiv digging through the latest research papers and love sharing them with others, this just might be the position for you.Job Title: [Machine Learning Living Library](https://intelligence.org/2017/12/12/ml-living-library/)

Basic Job Description: Read though latest papers, track trends in ML and be go-to consultant for researchers in the organization.

I have no relationship to this company. I found this posting looking for ML positions at non-profit organizations.",5,20
693,2019-8-12,2019,8,12,4,cp12rv,The 5 Feature Selection Algorithms every Data Scientist should know,https://www.reddit.com/r/MachineLearning/comments/cp12rv/the_5_feature_selection_algorithms_every_data/,_quanttrader_,1565551464,,0,1
694,2019-8-12,2019,8,12,5,cp1jex,[D] Machine Learning - WAYR (What Are You Reading) - Week 68,https://www.reddit.com/r/MachineLearning/comments/cp1jex/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,1565553605,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|
|----|-----|-----|-----|-----|-----|-----|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)||

Most upvoted papers two weeks ago:

/u/sasa1163: [https://medium.com/@melissa\_89553/an-nlp-analysis-of-the-mueller-testimony-6ff38e9d26f](https://medium.com/@melissa_89553/an-nlp-analysis-of-the-mueller-testimony-6ff38e9d26f)

Besides that, there are no rules, have fun.",22,137
695,2019-8-12,2019,8,12,5,cp1n2s,Looping over a submodel in torch,https://www.reddit.com/r/MachineLearning/comments/cp1n2s/looping_over_a_submodel_in_torch/,officialpatterson,1565554076,[removed],0,1
696,2019-8-12,2019,8,12,6,cp2dmj,Could what we dream be the GANS &amp; GPT2 our brain generates whilst we are aslept? [Discussion],https://www.reddit.com/r/MachineLearning/comments/cp2dmj/could_what_we_dream_be_the_gans_gpt2_our_brain/,alshell7,1565557559,"Seems if it's very lame to be discussed on this?.. I'm really not talking about 'dream prediction', but because most of the dreams that we have, are something that we happen to see that happen in daily life, or of course of something back few years ago, or that we hear, we see, we speak about. If at all, our biological brain, was analogically, a sophisticated GAN+GPT2 sort of model, would it be real to hypothetically assume that such model gets activated while we are slept?
On such assumption, how could be the generator &amp; discriminator be in such model system combined with text generation?. And is this sort of research for dream simulation, prediction really worth it?",12,0
697,2019-8-12,2019,8,12,6,cp2gwp,"Updates to Incredicat, my attempt at a 20 questions style game powered by Cat AI",https://www.reddit.com/r/MachineLearning/comments/cp2gwp/updates_to_incredicat_my_attempt_at_a_20/,twm7,1565557999,"I posted this a few months ago and had some great feedback. Ive put some work into the model and have just released the latest update. It uses a modified version of C4.5 decision trees and a load of other adjustments. Think it is working better now after some changes around the classification process. Appreciate any feedback! The link is

https://incredicat.com",0,1
698,2019-8-12,2019,8,12,6,cp2i22,"[P] Updates to Incredicat, my attempt at a 20 questions style game powered by Cat AI",https://www.reddit.com/r/MachineLearning/comments/cp2i22/p_updates_to_incredicat_my_attempt_at_a_20/,twm7,1565558153,"I posted this a few months ago and had some great feedback. Ive put some work into the model and have just released the latest update. It uses a modified version of C4.5 decision trees and a load of other adjustments. Think it is working better now after some changes around the classification process. Appreciate any feedback! The link is

https://incredicat.com",8,12
699,2019-8-12,2019,8,12,7,cp3hpf,"""Difference Between Artificial Intelligence, Machine Learning and Deep Learning""",https://www.reddit.com/r/MachineLearning/comments/cp3hpf/difference_between_artificial_intelligence/,susanvilleula1,1565562843,,0,1
700,2019-8-12,2019,8,12,9,cp4tpj,[D] Video game AI Platform suggestions,https://www.reddit.com/r/MachineLearning/comments/cp4tpj/d_video_game_ai_platform_suggestions/,ReDucTor,1565569753,"For building a video game AI what would people view as the ideal platform/tools? For both training and simulation.

The training would include the ability to work with prior replays of the game.

Currently based on my very basic research I'm thinking that using something like Kubeflow to enable people to do the data-science side of things and training inside of Jupyter

For the simulation a custom web-based interface to the game where the user can upload scripts and access previously created training data which will be run automatically.

Thoughts?",14,5
701,2019-8-12,2019,8,12,9,cp54kx,Deep Fakes in Movie Dubbing,https://www.reddit.com/r/MachineLearning/comments/cp54kx/deep_fakes_in_movie_dubbing/,Phlarix,1565571364,[removed],0,1
702,2019-8-12,2019,8,12,9,cp54zi,"What is the ""credit assignment"" problem in Machine Learning and Deep Learning?",https://www.reddit.com/r/MachineLearning/comments/cp54zi/what_is_the_credit_assignment_problem_in_machine/,real_pinocchio,1565571420,,0,1
703,2019-8-12,2019,8,12,10,cp5hqw,Knowledge graphs vs other representations for knowledge representation and access,https://www.reddit.com/r/MachineLearning/comments/cp5hqw/knowledge_graphs_vs_other_representations_for/,xkxkxkxk4,1565573325,[removed],0,1
704,2019-8-12,2019,8,12,10,cp5lc9,"I'm making a CLI to host, deploy and demo your models with one command",https://www.reddit.com/r/MachineLearning/comments/cp5lc9/im_making_a_cli_to_host_deploy_and_demo_your/,bigDATAbig,1565573862,,0,1
705,2019-8-12,2019,8,12,11,cp60f1,Lack of interpretability in deep learning putting me off to research,https://www.reddit.com/r/MachineLearning/comments/cp60f1/lack_of_interpretability_in_deep_learning_putting/,benthebarbarian3,1565576108,[removed],0,1
706,2019-8-12,2019,8,12,11,cp68zt,[D] Need help in the implementation of bidirectional recurrent language model.,https://www.reddit.com/r/MachineLearning/comments/cp68zt/d_need_help_in_the_implementation_of/,lyeoni,1565577382,"For implementation of **Bidirectional** language model, Please recommend me a paper/github repo  that I can refer to.

There's a lot of papers about uni-directional LM, but I can not find bi-directional one ..

Thanks :)",5,5
707,2019-8-12,2019,8,12,11,cp6acz,[R] One-shot Face Reenactment,https://www.reddit.com/r/MachineLearning/comments/cp6acz/r_oneshot_face_reenactment/,PuzzledProgrammer3,1565577591,"paper: [https://arxiv.org/pdf/1908.03251.pdf](https://arxiv.org/pdf/1908.03251.pdf)

code: [https://github.com/bj80heyue/One\_Shot\_Face\_Reenactment](https://github.com/bj80heyue/One_Shot_Face_Reenactment)

demo: [https://www.youtube.com/watch?v=FE-D6wh11\_A](https://www.youtube.com/watch?v=FE-D6wh11_A)",1,4
708,2019-8-12,2019,8,12,12,cp6krl,Is this online ML course by Siraj Raval any good?,https://www.reddit.com/r/MachineLearning/comments/cp6krl/is_this_online_ml_course_by_siraj_raval_any_good/,Bobo1383,1565579173,[removed],0,1
709,2019-8-12,2019,8,12,13,cp7619,Cc loi my ngnh gch,https://www.reddit.com/r/MachineLearning/comments/cp7619/cc_loi_my_ngnh_gch/,saigonbuilding,1565582529,,0,1
710,2019-8-12,2019,8,12,13,cp77yz,NLP/Text Extraction,https://www.reddit.com/r/MachineLearning/comments/cp77yz/nlptext_extraction/,samfisher18,1565582815,[removed],0,1
711,2019-8-12,2019,8,12,13,cp7djm,"[D] So, which direction should I go now? Which papers, in what order?",https://www.reddit.com/r/MachineLearning/comments/cp7djm/d_so_which_direction_should_i_go_now_which_papers/,Jeevesh88,1565583741,"Background :- I am an undergrad just beginning my sophomore year, and have studied ML the entire past year. I understand Basic NLP, Computer Vision , Machine Learning.

 I am interested in research and generally I can think of new ideas which I believe, are worth of doing research on. But , I am new to this world of research papers and learning from them . I have studied from few of them and understanding them fully made me very happy. I would like to go further into NLP. I know RNNs, LSTMs , GRUs, attention mechanism. And have read corresponding papers. 

1.) Can anyone here give an ""ordered"" list of papers from here on ,which will move my basic knowledge ahead. Avoiding the papers that are certainly dead ends or won't help if I am to do research further. A list that will increase my knowledge, from what I know, to the state of art; while also letting me ""why people chose the architectures/techniques etc. that they did?""

2.) I want an ""ordered"" list , because , most of the times , I try to read a paper , after 4 pages , I end up jumping to another paper , then another , then another and so on. What do you guys do to avoid this?",9,0
712,2019-8-12,2019,8,12,13,cp7e0i,"58,000,000 jobs in #AI. 300,000 AI developers. Skill-up now for free with the same training used by Amazon.coms developers and data scientists.",https://www.reddit.com/r/MachineLearning/comments/cp7e0i/58000000_jobs_in_ai_300000_ai_developers_skillup/,futuredude,1565583810,[removed],0,1
713,2019-8-12,2019,8,12,13,cp7iv1,This seems interesting; anybody here able to tell me more!,https://www.reddit.com/r/MachineLearning/comments/cp7iv1/this_seems_interesting_anybody_here_able_to_tell/,Smayy12,1565584607,,0,1
714,2019-8-12,2019,8,12,14,cp7vpk,[P] Does anyone know any way to download thousands of images of paintings in one place?,https://www.reddit.com/r/MachineLearning/comments/cp7vpk/p_does_anyone_know_any_way_to_download_thousands/,NarwhalAttack04,1565586831,"Im trying to get a bunch of paintings for my new project, but every site I visit has you individually download each one. Is there any site that has files for download filled with paintings?",13,1
715,2019-8-12,2019,8,12,14,cp84lr,Anyone think this service could be being used to train Natural Language Processors?,https://www.reddit.com/r/MachineLearning/comments/cp84lr/anyone_think_this_service_could_be_being_used_to/,Grade-A-Angus,1565588523,,0,1
716,2019-8-12,2019,8,12,15,cp8m9e,"Interview with Kaggle Kernels Grandmaster Ranked #1, ""Artgor"": Andrew Lukyanenko",https://www.reddit.com/r/MachineLearning/comments/cp8m9e/interview_with_kaggle_kernels_grandmaster_ranked/,init__27,1565591853,[removed],0,1
717,2019-8-12,2019,8,12,15,cp8prq,[P] The Illustrated GPT-2 (Visualizing Transformer Language Models),https://www.reddit.com/r/MachineLearning/comments/cp8prq/p_the_illustrated_gpt2_visualizing_transformer/,nortab,1565592519,"Hello r/MachineLearning,

This is a new post in which I try to visualize the majority of what happens inside a trained GPT-2. We follow the journey of an input word [from embedding](https://jalammar.github.io/images/gpt2/gpt2-input-embedding-positional-encoding-3.png), all the way up to the [output of the model](https://jalammar.github.io/images/gpt2/gpt2-output.png). I've also included a [crude analogy](https://jalammar.github.io/images/gpt2/self-attention-example-folders-3.png) for the query/key/value vectors of self-attention that I hope makes it easier for people starting out with transformer architectures. By the end of the post, we'd have looked at the major [weight matrices of a single block](https://jalammar.github.io/images/gpt2/gpt2-transformer-block-weights-2.png), as well as the [major weight matrices of the entire model](https://jalammar.github.io/images/gpt2/gpt2-weights-2.png). All feedback and corrections are welcomed!

&amp;#x200B;

The post: [https://jalammar.github.io/illustrated-gpt2/](https://jalammar.github.io/illustrated-gpt2/)",18,213
718,2019-8-12,2019,8,12,16,cp92vy,How many layer is enough for a 3x3 tic-tac-toe game?,https://www.reddit.com/r/MachineLearning/comments/cp92vy/how_many_layer_is_enough_for_a_3x3_tictactoe_game/,MarkKang2019,1565595065,[removed],0,1
719,2019-8-12,2019,8,12,16,cp931c,[YOLO] How to feed a machine learning?,https://www.reddit.com/r/MachineLearning/comments/cp931c/yolo_how_to_feed_a_machine_learning/,patricecapel,1565595092,"Hi Guys!

I need your help because I don't understanding nothing about it. I begin to code a C# app using YOLOv2 library (I don't find how to use a YOLOv3 library with C# but it's another topic). My main issue is my app works with the template recognition base linked with the library BUT I want to create my own pics database enable to recognize specific items.

Where can I find a very detailed for a neophyte like me ? I need to know how to create this database and how to feed it with pics.

Thanks in advance.

Patrice",0,1
720,2019-8-12,2019,8,12,16,cp93na,"How to quantify the quality of generated images, especially on virtual try-on task?",https://www.reddit.com/r/MachineLearning/comments/cp93na/how_to_quantify_the_quality_of_generated_images/,GuangleiXu,1565595212,[removed],0,1
721,2019-8-12,2019,8,12,17,cp9bb2,Global Laminator Market Report 2018,https://www.reddit.com/r/MachineLearning/comments/cp9bb2/global_laminator_market_report_2018/,jadhavni3,1565596823,[removed],1,1
722,2019-8-12,2019,8,12,17,cp9g0r,Global Air &amp; Gas Compressor Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/cp9g0r/global_air_gas_compressor_market_report_2019/,jadhavni3,1565597800,[removed],1,1
723,2019-8-12,2019,8,12,17,cp9jw2,Global Aircraft Emergency Evacuation System Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/cp9jw2/global_aircraft_emergency_evacuation_system/,jadhavni3,1565598598,[removed],1,1
724,2019-8-12,2019,8,12,17,cp9lq4,RTX 2080 Super or 2080 Ti for learning ML,https://www.reddit.com/r/MachineLearning/comments/cp9lq4/rtx_2080_super_or_2080_ti_for_learning_ml/,PCGeek80,1565598992,[removed],0,1
725,2019-8-12,2019,8,12,17,cp9mkl,[D] Machine Learning ebook bundle from O'Reilly is ending in less than 9 hours,https://www.reddit.com/r/MachineLearning/comments/cp9mkl/d_machine_learning_ebook_bundle_from_oreilly_is/,DudleyMraz,1565599164,"O'Reilly titles are pretty valuable and this pack is well-rounded so I thought some may enjoy the quick reminder to grab it.

&amp;#x200B;

[Here](https://twitter.com/Wegoforward/status/1160158936881946628)",8,58
726,2019-8-12,2019,8,12,17,cp9olm,Global Aircraft Ground Support Equipment Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/cp9olm/global_aircraft_ground_support_equipment_market/,jadhavni3,1565599635,[removed],1,1
727,2019-8-12,2019,8,12,18,cpa5zu,Global Articulating Boom Lifts Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/cpa5zu/global_articulating_boom_lifts_market_report_2019/,jadhavni3,1565603101,[removed],1,1
728,2019-8-12,2019,8,12,19,cpacqu,Automatic Plastic Soft Tube Toothpast Filling Sealing Machine,https://www.reddit.com/r/MachineLearning/comments/cpacqu/automatic_plastic_soft_tube_toothpast_filling/,amygao1984,1565604442,,0,1
729,2019-8-12,2019,8,12,19,cparrj,What's a good beginner neural net program I can write to better understand the process?,https://www.reddit.com/r/MachineLearning/comments/cparrj/whats_a_good_beginner_neural_net_program_i_can/,Nick-Conner,1565607243,[removed],0,1
730,2019-8-12,2019,8,12,21,cpbnj4,Alpha Zero Chess - does it improve anymore?,https://www.reddit.com/r/MachineLearning/comments/cpbnj4/alpha_zero_chess_does_it_improve_anymore/,ChessOrCheckers2,1565612630,[removed],0,1
731,2019-8-12,2019,8,12,21,cpbozr,"[D] The Zima Blue Episode of Love, Death &amp; Robots is an excellent way to demonstrate the unexpected way ML models layer on top of each other",https://www.reddit.com/r/MachineLearning/comments/cpbozr/d_the_zima_blue_episode_of_love_death_robots_is/,smallstuffshow,1565612862,"So a little late to the party, but I couldn't stop thinking about the ZIMA Blue episode of Love, Death &amp; Robots in regard to Machine Learning.

\*Spoilers\* The plot involves an intergalactically famous artist creating a final art piece in which he reveals he was a sophisticated Android that was giving up his (for lack of a better word) Sentience in order to return to his earliest form as a pool cleaner.

Though the episode was obviously an allegory for the human condition craving familiarity &amp; nostalgia, I thought the writers did a great job capturing how hard it is to tell how different inputs (or combinations and layerings of inputs) will impact the development of an AI over time.

I recorded a more in-depth audio breakdown of my thoughts here for anyone interested:

[The subtle way Love, Death &amp; Robots introduces us to the fundamentals of Machine Learning](https://sweating-the-small-stuff.pinecast.co/episode/b2fa47949fdb4333/bonus-love-death-robots-making-a-man-out-of-pool-equipment)

And I also had a conversation with an AI researcher on the novelty of Ex Machina in explaining how hard it will be to test future AI:

[A shallow dip into the ethics of AI](https://sweating-the-small-stuff.pinecast.co/episode/519cc270b1654f9a/025-ex-machina-who-s-really-being-tested-)",0,0
732,2019-8-12,2019,8,12,21,cpbskl,[D] Question about rewards in deep Q learning,https://www.reddit.com/r/MachineLearning/comments/cpbskl/d_question_about_rewards_in_deep_q_learning/,Kralex68,1565613395,"Hello I want to create a deep Q learning agent for a 2 player board game. My rewards are 250 for winning the game, 100, 80 and 50 for ""good"" moves. My tutor said to me that I should normalize the rewards because there are only limited amounts of different rewards and there are possibly infinitely manyQ values. How should I normalize the rewards? Should I normalize the rewards to \[0,1\] range so that 0,8 represents a game winning move, 0,3 a good move for example?",3,7
733,2019-8-12,2019,8,12,22,cpc5h1,Trends in Natural Language Processing,https://www.reddit.com/r/MachineLearning/comments/cpc5h1/trends_in_natural_language_processing/,MusingEtMachina,1565615307,,0,1
734,2019-8-12,2019,8,12,22,cpc9up,Artificial Intelligence - The Next Technological Revolution (Infographic),https://www.reddit.com/r/MachineLearning/comments/cpc9up/artificial_intelligence_the_next_technological/,the_spotless_mind,1565615925,,0,1
735,2019-8-12,2019,8,12,22,cpclpl,Very simple real-valued time-series dataset for RNN prototyping,https://www.reddit.com/r/MachineLearning/comments/cpclpl/very_simple_realvalued_timeseries_dataset_for_rnn/,marcelmoosbrugger,1565617628,[removed],0,1
736,2019-8-12,2019,8,12,22,cpcopp,What's wrong with my Maximum Likelihood function?,https://www.reddit.com/r/MachineLearning/comments/cpcopp/whats_wrong_with_my_maximum_likelihood_function/,OceanLinerXLL,1565618042,[removed],0,1
737,2019-8-12,2019,8,12,22,cpcqdv,[D] Best of AI Articles of July,https://www.reddit.com/r/MachineLearning/comments/cpcqdv/d_best_of_ai_articles_of_july/,BastouBab,1565618276,"Hi All!  
These are my 10 favorite AI articles and news published in July. They talk about autonomous driving, artificial intelligence, another GAN application.  
What are your best news and articles of the month of July?  
[https://blog.sicara.com/07-2019-best-ai-new-articles-this-month-3e1fa3f6c321](https://blog.sicara.com/07-2019-best-ai-new-articles-this-month-3e1fa3f6c321)",4,30
738,2019-8-12,2019,8,12,23,cpcut8,Did Alpha Zero learn knight + bishop checkmate?,https://www.reddit.com/r/MachineLearning/comments/cpcut8/did_alpha_zero_learn_knight_bishop_checkmate/,mazerakham_,1565618881,[removed],0,1
739,2019-8-12,2019,8,12,23,cpd90l,Video Analysis: Processing Megapixel Images with Deep Attention-Sampling Models,https://www.reddit.com/r/MachineLearning/comments/cpd90l/video_analysis_processing_megapixel_images_with/,ykilcher,1565620783,"[https://youtu.be/H6Qiegq\_36c](https://youtu.be/H6Qiegq_36c)

Current CNNs have to downsample large images before processing them, which can lose a lot of detail information. This paper proposes attention sampling, which learns to selectively process parts of any large image in full resolution, while discarding uninteresting bits. This leads to enormous gains in speed and memory consumption.",0,1
740,2019-8-12,2019,8,12,23,cpdevw,Modernize your IT Infrastructure Monitoring by Combining Time Series Databases with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cpdevw/modernize_your_it_infrastructure_monitoring_by/,IguazioDani,1565621541,[removed],0,1
741,2019-8-12,2019,8,12,23,cpdh1s,GLaDOS implementation on Tacotron 2,https://www.reddit.com/r/MachineLearning/comments/cpdh1s/glados_implementation_on_tacotron_2/,Lavish_Gupta,1565621831,,1,1
742,2019-8-13,2019,8,13,0,cpdujp,NVLabs StyleGAN: https://github.com/NVlabs/stylegan,https://www.reddit.com/r/MachineLearning/comments/cpdujp/nvlabs_stylegan_httpsgithubcomnvlabsstylegan/,AITrainerDerek,1565623469,[removed],0,1
743,2019-8-13,2019,8,13,0,cpe2ky,Training two models in parallel with cyclic output-input,https://www.reddit.com/r/MachineLearning/comments/cpe2ky/training_two_models_in_parallel_with_cyclic/,falmasri,1565624484,[removed],0,1
744,2019-8-13,2019,8,13,0,cpe307,[R] Video Analysis: Processing Megapixel Images with Deep Attention-Sampling Models,https://www.reddit.com/r/MachineLearning/comments/cpe307/r_video_analysis_processing_megapixel_images_with/,ykilcher,1565624542,"[https://youtu.be/H6Qiegq\_36c](https://youtu.be/H6Qiegq_36c)

Current CNNs have to downsample large images before processing them, which can lose a lot of detail information. This paper proposes attention sampling, which learns to selectively process parts of any large image in full resolution, while discarding uninteresting bits. This leads to enormous gains in speed and memory consumption.",1,33
745,2019-8-13,2019,8,13,0,cpeaiz,OReilly DataScience Textbooks - Free Google Drive PDFs,https://www.reddit.com/r/MachineLearning/comments/cpeaiz/oreilly_datascience_textbooks_free_google_drive/,conradws,1565625442,[removed],0,1
746,2019-8-13,2019,8,13,1,cpei37,[P] Building a State of the Art Bacterial Classifier with Fast.ai library,https://www.reddit.com/r/MachineLearning/comments/cpei37/p_building_a_state_of_the_art_bacterial/,coffeepants87,1565626325,,0,1
747,2019-8-13,2019,8,13,1,cpei91,"Google, Facebook, NASA, Airbus, Amazon researchers at Deep Learning Summit - London, UK",https://www.reddit.com/r/MachineLearning/comments/cpei91/google_facebook_nasa_airbus_amazon_researchers_at/,Zukicha,1565626347,,0,2
748,2019-8-13,2019,8,13,1,cpeosr,[P] Teaching a Robot to Like: how I trained a neural network to operate Tinder for me,https://www.reddit.com/r/MachineLearning/comments/cpeosr/p_teaching_a_robot_to_like_how_i_trained_a_neural/,aebrer,1565627121,,0,1
749,2019-8-13,2019,8,13,1,cpey43,Using Core ML and Natural Language for Sentiment Analysis on iOS,https://www.reddit.com/r/MachineLearning/comments/cpey43/using_core_ml_and_natural_language_for_sentiment/,omarmhaimdat,1565628236,,0,1
750,2019-8-13,2019,8,13,2,cpf8bb,List of PhD programs?,https://www.reddit.com/r/MachineLearning/comments/cpf8bb/list_of_phd_programs/,DaBobcat,1565629452,[removed],0,1
751,2019-8-13,2019,8,13,3,cpg1hh,[D] Worried about reviewers may steal my idea,https://www.reddit.com/r/MachineLearning/comments/cpg1hh/d_worried_about_reviewers_may_steal_my_idea/,PCCheater,1565632926,"Recently I submitted my paper with a very novel but easy to replicate idea to the CIKM conference. However, it seems reviewers would like to reject the paper with any arbitrary reason they came up with. Because the idea is very simple to replicate, I am very worried about that they may steal my idea to submit to upcoming conferences. In addition to submitting to arxiv, what else can protect my idea from being stolen? Thanks!",44,13
752,2019-8-13,2019,8,13,3,cpg3jl,[R] A Deep Learning Approach to Improve Emotion-Cause Extraction,https://www.reddit.com/r/MachineLearning/comments/cpg3jl/r_a_deep_learning_approach_to_improve/,omarsar,1565633143,,0,1
753,2019-8-13,2019,8,13,3,cpg8v7,Remote AI/ML opportunities,https://www.reddit.com/r/MachineLearning/comments/cpg8v7/remote_aiml_opportunities/,sucortical,1565633775,[removed],0,1
754,2019-8-13,2019,8,13,3,cpgcv8,NLP classification,https://www.reddit.com/r/MachineLearning/comments/cpgcv8/nlp_classification/,raesharma,1565634252,[removed],0,1
755,2019-8-13,2019,8,13,3,cpgevc,"Stuck on a project, need help!",https://www.reddit.com/r/MachineLearning/comments/cpgevc/stuck_on_a_project_need_help/,Mrthomsonmas,1565634497,[removed],0,1
756,2019-8-13,2019,8,13,4,cpgvuh,[P] Kannada-MNIST: A new handwritten digits dataset,https://www.reddit.com/r/MachineLearning/comments/cpgvuh/p_kannadamnist_a_new_handwritten_digits_dataset/,VinayUPrabhu,1565636524,[removed],0,1
757,2019-8-13,2019,8,13,4,cpgz46,10 differences between artificial intelligence and human intelligence,https://www.reddit.com/r/MachineLearning/comments/cpgz46/10_differences_between_artificial_intelligence/,Xaron,1565636903,,0,1
758,2019-8-13,2019,8,13,4,cph0yh,Understanding generalization through visualizations,https://www.reddit.com/r/MachineLearning/comments/cph0yh/understanding_generalization_through/,CometML,1565637107,,0,1
759,2019-8-13,2019,8,13,4,cph70m,Machine learning from scratch with python (Please support me),https://www.reddit.com/r/MachineLearning/comments/cph70m/machine_learning_from_scratch_with_python_please/,codingislife496,1565637831,[removed],0,1
760,2019-8-13,2019,8,13,5,cpi4bu,[P] Planar manipulator video dataset for testing goal-oriented prediction,https://www.reddit.com/r/MachineLearning/comments/cpi4bu/p_planar_manipulator_video_dataset_for_testing/,whiletrue2,1565641833,"Hi,

I'd like to share the following toy dataset with you which is readily available in form as raw videos and tfrecords:  


The dataset consists of 90 000 color videos that show a planar robot manipulator executing articulated manipulation tasks. More precisely, the manipulator grasps a circular object of random color and size and places it on top of a square object/platform of again random color and size. The initial congurations (location, size and color) of the objects were randomly sampled during generation. Different from other datasets such as the moving MNIST dataset, the samples comprise a goal-oriented task as described, making it more suitable for testing prediction capabilities of an ML model. For instance, one can use it as a toy dataset to investigate the capacity and output behavior of a deep neural network before testing it on real-world data.  


[https://github.com/ferreirafabio/PlanarManipulatorDataset](https://github.com/ferreirafabio/PlanarManipulatorDataset)

Feel free to use, share and comment :)",3,10
761,2019-8-13,2019,8,13,5,cpiaqs,Ride the AI tsunami: Introduction to Artificial Intelligence from Microsoft  boost your career and income growth,https://www.reddit.com/r/MachineLearning/comments/cpiaqs/ride_the_ai_tsunami_introduction_to_artificial/,internetdigitalentre,1565642612,[removed],0,1
762,2019-8-13,2019,8,13,5,cpidoq,Polynomial linear Regression,https://www.reddit.com/r/MachineLearning/comments/cpidoq/polynomial_linear_regression/,Rushil2311,1565642985,[removed],0,1
763,2019-8-13,2019,8,13,5,cpigt5,[P] Coloring Artwork With Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cpigt5/p_coloring_artwork_with_machine_learning/,NNFAK,1565643349,"I have been playing around with [DeOldify](https://github.com/jantic/DeOldify) this past week, which as been a lot of fun. During my testing, [I colorized a video of a Tasmanian Tiger](https://www.youtube.com/watch?v=gFVub3ne4nM) which managed to reach the front page and resulted in an article from Daily Mail... which was awesome! Anyway, while playing around, I realized it might be cool to visualize the colorization process by colorizing a hyper-realistic drawing time lapse. The hope was that using this method I could see when the network recognized a face, hair, eyes, etc.


Achieving this was pretty simple as I didn't need to write any code myself. I found a great time lapse from a talented artist drawing Cara Delevingne, downloaded it (with permission) and colorized it. I put the two videos side by side, and I was done!


[Here are the results!](https://www.youtube.com/watch?v=J0VF0VudUEw)


I hope you guys find it interesting. It was a little less eventful than I expected, but still pretty cool to see!",12,193
764,2019-8-13,2019,8,13,6,cpiq08,L-BFGS ALGORITHM FOR FUNCTION MINIMIZATION,https://www.reddit.com/r/MachineLearning/comments/cpiq08/lbfgs_algorithm_for_function_minimization/,without_j,1565644464,[removed],0,1
765,2019-8-13,2019,8,13,6,cpixna,Is .62 AUC too poor to deploy into production?,https://www.reddit.com/r/MachineLearning/comments/cpixna/is_62_auc_too_poor_to_deploy_into_production/,zzreflexzz,1565645393,[removed],0,1
766,2019-8-13,2019,8,13,6,cpj06i,PyTorch 1.2 Supports Transformer and Tensorboard; Summer Hackathon Announced,https://www.reddit.com/r/MachineLearning/comments/cpj06i/pytorch_12_supports_transformer_and_tensorboard/,Yuqing7,1565645713,,0,1
767,2019-8-13,2019,8,13,6,cpj4qq,[P] Kannada-MNIST : A brand new MNIST-like handwritten digits dataset,https://www.reddit.com/r/MachineLearning/comments/cpj4qq/p_kannadamnist_a_brand_new_mnistlike_handwritten/,VinayUPrabhu,1565646259,,0,1
768,2019-8-13,2019,8,13,6,cpj9u8,"Software Developers &amp; Engineers: Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning can lead to greater earning and career opportunities",https://www.reddit.com/r/MachineLearning/comments/cpj9u8/software_developers_engineers_introduction_to/,internetdigitalentre,1565646904,[removed],0,1
769,2019-8-13,2019,8,13,6,cpjbxe,NVIDIA Jetson Nano Developer Kit,https://www.reddit.com/r/MachineLearning/comments/cpjbxe/nvidia_jetson_nano_developer_kit/,HenslerSoftware,1565647165,[removed],0,1
770,2019-8-13,2019,8,13,7,cpjrbs,Will AI just eliminate jobs for the low-skilled or not?,https://www.reddit.com/r/MachineLearning/comments/cpjrbs/will_ai_just_eliminate_jobs_for_the_lowskilled_or/,Nico_X13,1565649065,[removed],0,1
771,2019-8-13,2019,8,13,8,cpk9zd,"Everyone in this subreddit probably wrote a simple gender classifier at some point, but I was very surprised to see a liberal institution like Harvard teaching that there are only 2 genders.",https://www.reddit.com/r/MachineLearning/comments/cpk9zd/everyone_in_this_subreddit_probably_wrote_a/,Agent_ANAKIN,1565651482,,0,1
772,2019-8-13,2019,8,13,8,cpkknj,Need Guidance on Machine Learning Approach,https://www.reddit.com/r/MachineLearning/comments/cpkknj/need_guidance_on_machine_learning_approach/,conradws,1565652883," 

Hi everyone, I just wanted hear some of your wisdom on how to start tackling a pretty uncommon supervised task Im working om. Ill try to give you as much detail as possible, and few concerns/questions I have. (I have an edtech website where we provide personalized educational ressources based on the studentsneeds and information).

The aim of the model is to predict from a 1,500 input vector a 100 output vector. I currently have around 2,700 examples (I know this is not a lot, but I obtain 5-10 more everyday from real life users.)

Due to the nature of the data, I was first thinking of using a seq2seq model, but I dont have a lot experience with them. Is it ok to have big 1,500 sequence encoded and then decoded into a 100 output vector?

Can I /should I use PCA or another decomposition technique to decrease the input size out the output vector? Is it possible to use a seq2seq model on a pca transformation?

For more context, the 100 vector target output actually represents an embedding of a piece of educational content. These embeddings capture the nature of the material and similar ressources have close cosine similarity.

I know neural nets can easily predict have output layers of 100 nodes but Im not sure if you can do the same with other traditional machine learning algos like randomforest. I dont want to frame the problem as a classic classfication problem because there are over 200 pieces of content and thant number grows everyday. What I need is the ability to predict an embedding vector which allows us to (through a cosine similarity search) select a useful piece of content for the user.

Looking forward to hearing your advice and thank you in advance, even if you only took the time to read this through this.",0,1
773,2019-8-13,2019,8,13,11,cpmf0k,[D] Autoencoder to reconstruct speech input from melspectrograms,https://www.reddit.com/r/MachineLearning/comments/cpmf0k/d_autoencoder_to_reconstruct_speech_input_from/,aimldlcv,1565661871,"Hello everyone,

I was trying to train an autoencoder which takes a melspectrogram as input and outputs the same melspectrogram. It's a reconstruction task. However, the model seems to be generating random noise. It'll be great if anyone could point me towards any relevant github repos/papers which solve this task.

Thank you! :)",5,2
774,2019-8-13,2019,8,13,11,cpmnqr,[D] Receiving a notification when models training is completed,https://www.reddit.com/r/MachineLearning/comments/cpmnqr/d_receiving_a_notification_when_models_training/,Cmaster11,1565663045,"Hi everyone!

[Few months ago](https://new.reddit.com/r/devops/comments/bmujjg/made_a_new_notification_tool_and_im_looking_for/) I released [Notify17](https://notify17.net/), a tool to generate mobile notifications from simple web requests.

I think redditors of this sub may be interested in using it when dealing with models training, or any similar long lasting tasks, where you need to wait a long time between each iteration.

The most basic concept would be that, as soon as a model training finishes, you can invoke a [function](https://notify17.net/recipes/python/) to let yourself know that the event occurred, and get the notification on your phone/browser:

    from notify17 import n17_raw
    n17_raw(""RAW_API_KEY"", 'Model training finished')

I'm looking for feedback from people in multiple fields, because I believe that Notify17 could be useful in many multiple scenarios than just backend development.

I'd love to know if anyone of you finds this tool to be useful for the machine learning field and/or what could be added/changed to make it worth the use for you all.

P.s. also [MATLAB](https://notify17.net/recipes/matlab/) and [LUA](https://notify17.net/recipes/lua/) examples are around. In any case, a `cURL` request is enough to trigger any notification, e.g:

    url -X POST \
        ""https://hook.notify17.net/api/raw/RAW_API_KEY"" \
        -F title=""Model $MODEL_NAME training completed""

Thanks a lot, I'm here to answer any question about this post!

Alberto",2,5
775,2019-8-13,2019,8,13,12,cpncrg,A generic line detector for soccer streams,https://www.reddit.com/r/MachineLearning/comments/cpncrg/a_generic_line_detector_for_soccer_streams/,muaz65,1565666580,[removed],0,1
776,2019-8-13,2019,8,13,13,cpo6vm,[R] Google Health: An augmented reality microscope with real-time artificial intelligence integration for cancer diagnosis,https://www.reddit.com/r/MachineLearning/comments/cpo6vm/r_google_health_an_augmented_reality_microscope/,LudicrousAbode,1565671280,"Abstract:

The microscopic assessment of tissue samples is instrumental for the diagnosis and staging of cancer, and thus guides therapy. However, these assessments demonstrate considerable variability and many regions of the world lack access to trained pathologists. Though artificial intelligence (AI) promises to improve the access and quality of healthcare, the costs of image digitization in pathology and difficulties in deploying AI solutions remain as barriers to real-world use. Here we propose a cost-effective solution: the augmented reality microscope (ARM). The ARM overlays AI-based information onto the current view of the sample in real time, enabling seamless integration of AI into routine workflows. We demonstrate the utility of ARM in the detection of metastatic breast cancer and the identification of prostate cancer, with latency compatible with real-time use. We anticipate that the ARM will remove barriers towards the use of AI designed to improve the accuracy and efficiency of cancer diagnosis.

[https://www.nature.com/articles/s41591-019-0539-7.epdf?author\_access\_token=BI9AOTsesmNoV2lSdpucn9RgN0jAjWel9jnR3ZoTv0PDGU3ZwysZtsN41a2fOgaoj4PRxjTvAHjSFrKF\_S\_mq4QNNV8dNoxAjytIQuVz9vdjplLQHUSEPiIo392MzIJY8fqxLKHC5vIwNpLLEoXMnA%3D%3D](https://www.nature.com/articles/s41591-019-0539-7.epdf?author_access_token=BI9AOTsesmNoV2lSdpucn9RgN0jAjWel9jnR3ZoTv0PDGU3ZwysZtsN41a2fOgaoj4PRxjTvAHjSFrKF_S_mq4QNNV8dNoxAjytIQuVz9vdjplLQHUSEPiIo392MzIJY8fqxLKHC5vIwNpLLEoXMnA%3D%3D)",17,145
777,2019-8-13,2019,8,13,14,cpofx3,Some Materials and References collections for AI/ML,https://www.reddit.com/r/MachineLearning/comments/cpofx3/some_materials_and_references_collections_for_aiml/,latcher_cx,1565672800,[removed],0,1
778,2019-8-13,2019,8,13,14,cpokla,How to combine multiple (facial recognition) feature vectors into one feature vector,https://www.reddit.com/r/MachineLearning/comments/cpokla/how_to_combine_multiple_facial_recognition/,appDeveloperGuy1,1565673624,[removed],0,1
779,2019-8-13,2019,8,13,14,cpolhh,[Tutorial] Build direct and reverse image search with AquilaDB Multi Model Search technique,https://www.reddit.com/r/MachineLearning/comments/cpolhh/tutorial_build_direct_and_reverse_image_search/,iamjbn123,1565673780,,0,1
780,2019-8-13,2019,8,13,14,cpot4r,CS156-Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cpot4r/cs156machine_learning/,freevideolectures,1565675173,,1,1
781,2019-8-13,2019,8,13,14,cpoxd5,9 Reasons Why You Should Keep Learning Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cpoxd5/9_reasons_why_you_should_keep_learning_machine/,bencryer123,1565675961,[removed],0,1
782,2019-8-13,2019,8,13,15,cpozla,Machine Learning in eCommerce,https://www.reddit.com/r/MachineLearning/comments/cpozla/machine_learning_in_ecommerce/,biswas9,1565676354,[removed],0,1
783,2019-8-13,2019,8,13,15,cpp01r,"What do we learn from region based object detectors (Faster R-CNN, R-FCN, FPN)?",https://www.reddit.com/r/MachineLearning/comments/cpp01r/what_do_we_learn_from_region_based_object/,Mkerian10,1565676440,,0,1
784,2019-8-13,2019,8,13,15,cpp0fx,"Best Training Institute for Machine Learning in Delhi, Machine Learning Course",https://www.reddit.com/r/MachineLearning/comments/cpp0fx/best_training_institute_for_machine_learning_in/,dtacademy,1565676512,[removed],0,1
785,2019-8-13,2019,8,13,16,cppj63,Automatic Panel Switch Cartoning Machine to Pack in Carton,https://www.reddit.com/r/MachineLearning/comments/cppj63/automatic_panel_switch_cartoning_machine_to_pack/,amygao1984,1565680081,,0,1
786,2019-8-13,2019,8,13,16,cppjxp,Modern Constructions Machines And Ingenious Tools #1,https://www.reddit.com/r/MachineLearning/comments/cppjxp/modern_constructions_machines_and_ingenious_tools/,GoGadegets,1565680229,,0,1
787,2019-8-13,2019,8,13,16,cppmy8,[P] Handout: A potential alternative to Jupyter Notebooks,https://www.reddit.com/r/MachineLearning/comments/cppmy8/p_handout_a_potential_alternative_to_jupyter/,PhYsIcS-GUY227,1565680814,"This is not my project, but I think its great and it hasnt been shared here yet.

Introducing [Handout](https://github.com/danijar/handout)

Turn Python scripts into handouts with Markdown comments and inline figures. An alternative to Jupyter notebooks without hidden state that supports any text editor.

In a sense this is the best of both worlds, as you get the visual benefits of notebooks while having your code run as scripts and make it much easier to reproduce results.

It would be interesting to hear from those that love their notebooks if a tool like this can be a real alternative? If not, why not?",55,78
788,2019-8-13,2019,8,13,16,cpppxq,"IoT Telemetry Collection using Google Protocol Buffers, Google Cloud Functions, Cloud Pub/Sub, and MongoDB Atlas",https://www.reddit.com/r/MachineLearning/comments/cpppxq/iot_telemetry_collection_using_google_protocol/,kiarash-irandoust,1565681374,,0,1
789,2019-8-13,2019,8,13,16,cppspv,AI and VR pioneer accused of having sex with trafficking victim on Jeffrey Epsteins island,https://www.reddit.com/r/MachineLearning/comments/cppspv/ai_and_vr_pioneer_accused_of_having_sex_with/,aerowindwalker,1565681908,,0,1
790,2019-8-13,2019,8,13,17,cpq45h,"Employers / interviewers, would you be impressed if..",https://www.reddit.com/r/MachineLearning/comments/cpq45h/employers_interviewers_would_you_be_impressed_if/,seriousgourmetshit,1565684164,[removed],0,1
791,2019-8-13,2019,8,13,17,cpq79c,[P] Integrating Human and Machine Intelligence for efficient and fast annotations,https://www.reddit.com/r/MachineLearning/comments/cpq79c/p_integrating_human_and_machine_intelligence_for/,feedmari,1565684804,"**Title:**  Active Annotation: bootstrapping annotation lexicon and guidelines for supervised NLU learning

**Abstract:** Natural Language Understanding (NLU) models are typically trained in a supervised learning framework. In the case of intent classification, the predicted labels are predefined and based on the designed annotation schema while the labelling process is based on a laborious task where annotators manually inspect each utterance and assign the corresponding label. We propose an Active Annotation (AA) approach where we combine an unsupervised learning method in the embedding space, a human-in-the-loop verification process, and linguistic insights to create lexicons that can be open categories and adapted over time. In particular, annotators define the y-label space on-the-fly during the annotation using an iterative process and without the need for prior knowledge about the input data. We evaluate the proposed annotation paradigm in a real use-case NLU scenario. Results show that our Active Annotation paradigm achieves accurate and higher quality training data, with an annotation speed of an order of magnitude higher with respect to the traditional human-only driven baseline annotation methodology.

**Link:** [https://arxiv.org/abs/1908.04092](https://arxiv.org/abs/1908.04092)",1,2
792,2019-8-13,2019,8,13,17,cpqao9,[D] RNN for time varying covariates,https://www.reddit.com/r/MachineLearning/comments/cpqao9/d_rnn_for_time_varying_covariates/,lazywiing,1565685500,"Hi reddit,

I am currently working on a problem and I would like to ask for your advice on the best way to handle it. 

So, the goal here is to predict the time of resolution of what we will call ""incidents"". Namely, an incident is like an issue that is opened and needs to be fixed. The status of an incident varies with time - for instance, comments may be added or the priority may change from ""Not important"" to ""Critical"". 

My dataset looks like this : a row corresponds to the status of an incident at a given time. A single incident, identified by its ID, is then made of several rows. Some features do not change while others change every time there is a modification (see table below).

|id|var1|var2|
|:-|:-|:-|
|1|a|NaN|
|1|a|x|
|1|b|x|

The idea would be to be able to give a prediction at any time in the life of an incident. I would like to make use of this sequential form to use recurrent neural networks, but I don't know how to do it exactly. 

There is a new row every time there is a change in status. So this means that the time between first and second row might be a day while the time between second and third row might be a week or a month.

I was thinking of treating the problem as a NLP problem, i.e. for a single ID, each row corresponds to one word (the embedded word). This would mean that the input to the RNN would be something like x = \[x1, x2, ..., xN\] where xi = \[xi1, ..., xiM\] a row of the dataset.

Would that make sense, and if not, how would you proceed ?

&amp;#x200B;

Thank you and have a nice day.",6,6
793,2019-8-13,2019,8,13,18,cpql9f,Differences between a complex Simulator and a Digital Twin,https://www.reddit.com/r/MachineLearning/comments/cpql9f/differences_between_a_complex_simulator_and_a/,gkapellmann,1565687587,[removed],0,1
794,2019-8-13,2019,8,13,18,cpqv5o,Best CPU for running simulations,https://www.reddit.com/r/MachineLearning/comments/cpqv5o/best_cpu_for_running_simulations/,pakidermista,1565689538,[removed],0,1
795,2019-8-13,2019,8,13,18,cpqwqw,Free cloud GPU credits for deep learning,https://www.reddit.com/r/MachineLearning/comments/cpqwqw/free_cloud_gpu_credits_for_deep_learning/,whitezl0,1565689850,[removed],0,1
796,2019-8-13,2019,8,13,19,cprezp,Artificial Intelligence and the Job Market Irony,https://www.reddit.com/r/MachineLearning/comments/cprezp/artificial_intelligence_and_the_job_market_irony/,Albertchristopher,1565693274,,0,1
797,2019-8-13,2019,8,13,19,cprfy4,[D] python - how can I solve gradient divergence problem?,https://www.reddit.com/r/MachineLearning/comments/cprfy4/d_python_how_can_i_solve_gradient_divergence/,GoBacksIn,1565693450,"here is my code

&amp;#x200B;

    for _ in range(10):
        K.clear_session()
        model = Sequential()
    
        model.add(LSTM(256, input_shape=(None, 1)))
        model.add(Dropout(0.2))
    
        model.add(Dense(256))
        model.add(Dropout(0.2))
    
        model.add(Dense(1))
    
        model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
        hist = model.fit(x_train, y_train, epochs=20, batch_size=64, verbose=0, validation_data=(x_val, y_val))
    
    
        p = model.predict(x_test)
        print(mean_squared_error(y_test, p))
    
    
        plt.plot(y_test)
        plt.plot(p)
        plt.legend(['testY', 'p'], loc='upper right')
        plt.show()
    

&amp;#x200B;

dataset is stock time series

&amp;#x200B;

\`Total params\` : 330,241

\`samples\` : 2264

&amp;#x200B;

just same code for loop ten times

&amp;#x200B;

and below is the result

&amp;#x200B;

https://i.redd.it/jngtf9xx47g31.png

&amp;#x200B;

I haven't changed anything.

&amp;#x200B;

I only ran for loop.

&amp;#x200B;

But the MSE difference in the results is very large.

&amp;#x200B;

I think the reason for this the weights are initialized randomly;

&amp;#x200B;

So, I increased the size of epochs and batch\_size, but the gradient divergence problem was not solved.

&amp;#x200B;

I wonder how we should solve this problem.

&amp;#x200B;

Your valuable opinions and thoughts will be very much appreciated.

&amp;#x200B;

if you want to see full source here is link [https://gist.github.com/Lay4U/e1fc7d036356575f4d0799cdcebed90e](https://gist.github.com/Lay4U/e1fc7d036356575f4d0799cdcebed90e)",2,0
798,2019-8-13,2019,8,13,19,cprguh,"Difference of Data Science, Machine Learning and Data Mining",https://www.reddit.com/r/MachineLearning/comments/cprguh/difference_of_data_science_machine_learning_and/,andrea_manero,1565693609,[removed],0,1
799,2019-8-13,2019,8,13,19,cprirq,Data linkage visualization tool,https://www.reddit.com/r/MachineLearning/comments/cprirq/data_linkage_visualization_tool/,lnaef,1565693970,[removed],0,1
800,2019-8-13,2019,8,13,20,cprooa,EMNLP 2019 results are out,https://www.reddit.com/r/MachineLearning/comments/cprooa/emnlp_2019_results_are_out/,davoodm93,1565694997,[removed],0,1
801,2019-8-13,2019,8,13,21,cps9hv,10 Must-Try Open Source Tools for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cps9hv/10_musttry_open_source_tools_for_machine_learning/,johnperkins899,1565698302,,0,1
802,2019-8-13,2019,8,13,21,cpsb9l,[N] Awesome Artificial Intelligence Research and Projects on Computer Vision News (with codes!) August 2019,https://www.reddit.com/r/MachineLearning/comments/cpsb9l/n_awesome_artificial_intelligence_research_and/,Gletta,1565698566,"The August issue of Computer Vision News: 38 pages about AI and Deep Learning through both research and practical applications.

Newly improved graphics for easier reading. Don't miss the review of the new Google Research paper and the interview with Julia Elliott, the leader of the competitions team at Kaggle.

[HTML5 version (recommended)](https://www.rsipvision.com/ComputerVisionNews-2019August/)

[PDF version](https://www.rsipvision.com/computer-vision-news-2019-august-pdf/)

Subscribe for free on page 38.

&amp;#x200B;

https://i.redd.it/mjt15zb4k7g31.jpg",1,8
803,2019-8-13,2019,8,13,21,cpscjq,Dataset about inertial data of dogs an their stride length,https://www.reddit.com/r/MachineLearning/comments/cpscjq/dataset_about_inertial_data_of_dogs_an_their/,Louma007,1565698771,[removed],0,1
804,2019-8-13,2019,8,13,21,cpso83,Anybody knows what happens if AlphaZero search was restricted to human level?,https://www.reddit.com/r/MachineLearning/comments/cpso83/anybody_knows_what_happens_if_alphazero_search/,samuelknoche,1565700483,"In the [Alphazero blog post](https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go), there is a graphic comparing the amount of search per decision between grand-master humans, Alphazero and traditional chess engines. Alphazero is in the middle. At first glance, the implication seems to be that humans are still better at ""intuitive"" play than Alphazero. But then again, Alphazero is significantly stronger than any human player, and so I wonder how well Alphazero would perform if restricted to the same amount of search as a human player. 

There is a graphic in the [AlphaGo Zero nature paper](https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ) that shows that the raw network on Go with no search has an elo of 3000, almost as high as AlphaGo Fan. That seems to indicate that AlphaZero with the same amount of search as a human player might already be super-human. Especially since I suspect that there might be diminishing returns to search. 

Am I missing anything? Might AlphaZero have beaten humans at ""intuitive"" chess as well?",0,1
805,2019-8-13,2019,8,13,21,cpsooj,blackbox: A Python module for parallel optimization of expensive black-box functions,https://www.reddit.com/r/MachineLearning/comments/cpsooj/blackbox_a_python_module_for_parallel/,paulknysh,1565700544,,0,1
806,2019-8-13,2019,8,13,21,cpsqup,[P] NLP Problem,https://www.reddit.com/r/MachineLearning/comments/cpsqup/p_nlp_problem/,raesharma,1565700838," 

Hi all, I am a beginner stage ML practitioner. I need help.

I have a large dataset of free text for work orders for a power plant. I need to identify the cause, symptom and damage by these work order data. I do have the list of causes, symptoms and damage which is to be used to classify each work order. The issue is I do not have a labelled data set so I have to take the unsupervised approach. But I am not getting useful results.

Could you help me with approaches I can work on to get better results.

Thanks.",1,0
807,2019-8-13,2019,8,13,21,cpssar,[D] How to load subset of large Oracle table into Dask dataframe?,https://www.reddit.com/r/MachineLearning/comments/cpssar/d_how_to_load_subset_of_large_oracle_table_into/,Professionalsimracin,1565701059," Here's what I tried:

    dask_rf = dd.from_pandas(pd.read_sql('select ...)', conn_cx_Oracle), npartitions = 10) 

This gives me a 'large object' warning and recommends using client.scatter. Problem is that it appears that client.scatter requires data to be loaded into a Pandas dataframe first, which is why I'm using Dask in the first place because of RAM limitations.

The Oracle table is too large to read using Dask's read\_sql\_table because read\_sql\_table does not filter the table in any way.

Ideas? Dask not applicable to my use case?",3,0
808,2019-8-13,2019,8,13,21,cpssld,[D] What if AlphaZero search was restricted to human level?,https://www.reddit.com/r/MachineLearning/comments/cpssld/d_what_if_alphazero_search_was_restricted_to/,samuelknoche,1565701106,"In the [Alphazero blog post](https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go), there is a graphic comparing the amount of search per decision between grand-master humans, Alphazero and traditional chess engines. Alphazero is in the middle. At first glance, the implication seems to be that humans are still better at ""intuitive"" play than Alphazero. But then again, Alphazero is significantly stronger than any human player, and so I wonder how well Alphazero would perform if restricted to the same amount of search as a human player.

There is a plot in the [AlphaGo Zero nature paper](https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ) that shows that the raw network on Go with no search has an elo of 3000, almost as high as AlphaGo Fan. That seems to indicate that AlphaZero with the same amount of search as a human player might already be super-human. Especially since I suspect that there might be diminishing returns to search.

Am I missing anything? Might AlphaZero have beaten humans at ""intuitive"" chess as well?",11,6
809,2019-8-13,2019,8,13,22,cpt0iu,[D] Interviewing as researcher with big tech companies,https://www.reddit.com/r/MachineLearning/comments/cpt0iu/d_interviewing_as_researcher_with_big_tech/,urhen1512,1565702220,"Hi all,

I have a couple of interviews coming up for research scientist roles with FAANG-like companies and since it's my first time going through this process (finishing grad school, never applied to industry positions before) I was wondering how the interviews are conducted and what they look at the most. I can find tons of information on the recruitment process for SWE at big N companies, and how you have to grind leetcode for months to pass the coding bar, but not much info on the more research oriented roles. Do they have the same bar for coding? Do they put more emphasis on your performance during the ML and design interviews? What ""signal"" do they look for, presumably outside of your research portfolio, which they'd know already by the time you come for an onsite?

Finally, in case anyone here works at a the research groups in major tech companies, what's been in your experience the interview:offer ratio, is it as bad as for developer roles (I read that for Google it is about 1:7)? I would imagine that for research roles, it is easier to see if there is a good fit on data outside of algo&amp;ds / design interview performance, like research interests and publication record, but maybe I am naive.

Sorry for the many questions, just a scared grad student trying to understand what his chances are :D

Thanks.",23,18
810,2019-8-13,2019,8,13,22,cpt8rp,[D] How to manage a small Machine Learning lab?,https://www.reddit.com/r/MachineLearning/comments/cpt8rp/d_how_to_manage_a_small_machine_learning_lab/,_edmar,1565703360,"I am building a small machine learning lab at my company ( I am a founder).  What ate the differences on managing a ML team vs a dev team ? Any pointers to artickes, books, etc?",5,1
811,2019-8-13,2019,8,13,23,cptkep,[R] Building a Better CartPole - DM's new RL benchmarking suite,https://www.reddit.com/r/MachineLearning/comments/cptkep/r_building_a_better_cartpole_dms_new_rl/,andyljones,1565704971,"
[Behaviour Suite for Reinforcement Learning](https://arxiv.org/abs/1908.03568v1)

&gt; This paper introduces the Behaviour Suite for Reinforcement Learning, or bsuite for short. bsuite is a collection of carefully-designed experiments that investigate core capabilities of reinforcement learning (RL) agents with two objectives. First, to collect clear, informative and scalable problems that capture key issues in the design of general and efficient learning algorithms. Second, to study agent behaviour through their performance on these shared benchmarks. To complement this effort, we open source this http URL, which automates evaluation and analysis of any agent on bsuite. This library facilitates reproducible and accessible research on the core issues in RL, and ultimately the design of superior learning algorithms. Our code is Python, and easy to use within existing projects. We include examples with OpenAI Baselines, Dopamine as well as new reference implementations. Going forward, we hope to incorporate more excellent experiments from the research community, and commit to a periodic review of bsuite from a committee of prominent researchers.

This is a great paper. While the authors focus on comparing different agents, additional value is going to be in debugging algorithm variants. Many researchers already have their own zoos of duct-taped diagnostic envs to try and localise errors, but the community's been lacking anything ready-made and well-tested.

What is a little disappointing is that they don't carry this paper through to 'here we evaluated 17 different agents and this is the best one', though presumably other contributors will fix that in short order.",5,21
812,2019-8-13,2019,8,13,23,cptmgy,[R] https://heartbeat.fritz.ai/a-2019-guide-to-deep-learning-based-image-compression-2f5253b4d811,https://www.reddit.com/r/MachineLearning/comments/cptmgy/r/,mwitiderrick,1565705224,"Today I look at how deep learning can be used in image compression. The piece is from a comprehensive analysis of several research papers.

  https://heartbeat.fritz.ai/a-2019-guide-to-deep-learning-based-image-compression-2f5253b4d811",0,1
813,2019-8-13,2019,8,13,23,cptneg,[R] Video Analysis: Gauge Equivariant Convolutional Networks and the Icosahedral CNN,https://www.reddit.com/r/MachineLearning/comments/cptneg/r_video_analysis_gauge_equivariant_convolutional/,ykilcher,1565705342,"Ever wanted to do a convolution on a Klein Bottle? This paper defines CNNs over manifolds such that they are independent of which coordinate frame you choose. Amazingly, this then results in an efficient practical method to achieve state-of-the-art in several tasks!

[https://youtu.be/wZWn7Hm8osA](https://youtu.be/wZWn7Hm8osA)

Abstract: The principle of equivariance to symmetry transformations enables a theoretically grounded approach to neural network architecture design. Equivariant networks have shown excellent performance and data efficiency on vision and medical imaging problems that exhibit symmetries. Here we show how this principle can be extended beyond global symmetries to local gauge transformations. This enables the development of a very general class of convolutional neural networks on manifolds that depend only on the intrinsic geometry, and which includes many popular methods from equivariant and geometric deep learning. We implement gauge equivariant CNNs for signals defined on the surface of the icosahedron, which provides a reasonable approximation of the sphere. By choosing to work with this very regular manifold, we are able to implement the gauge equivariant convolution using a single conv2d call, making it a highly scalable and practical alternative to Spherical CNNs. Using this method, we demonstrate substantial improvements over previous methods on the task of segmenting omnidirectional images and global climate patterns.

Authors: Taco S. Cohen, Maurice Weiler, Berkay Kicanaoglu, Max Welling

Paper: [https://arxiv.org/abs/1902.04615](https://arxiv.org/abs/1902.04615)",2,16
814,2019-8-13,2019,8,13,23,cptnh3,[R] A 2019 Guide to Deep Learning-Based Image Compression,https://www.reddit.com/r/MachineLearning/comments/cptnh3/r_a_2019_guide_to_deep_learningbased_image/,mwitiderrick,1565705351,"Today I look at how deep learning can be used in image compression. The article is from a comprehensive analysis of several research papers. 

[https://heartbeat.fritz.ai/a-2019-guide-to-deep-learning-based-image-compression-2f5253b4d811](https://heartbeat.fritz.ai/a-2019-guide-to-deep-learning-based-image-compression-2f5253b4d811)",1,14
815,2019-8-13,2019,8,13,23,cptsco,Chess engine,https://www.reddit.com/r/MachineLearning/comments/cptsco/chess_engine/,corzoai,1565706000,"Hi, I would like to create a chess engine that uses a MCTS + PUTC and DCNN, in the AlphaZero style. However, I do not know what programming language to use. I know that most engines use C++, although I would like to try something new. What about Go or Python?

Thanks,",0,1
816,2019-8-13,2019,8,13,23,cpu06r,"[D] For samples of subsets, features are predictive in gradient boost algorithm.",https://www.reddit.com/r/MachineLearning/comments/cpu06r/d_for_samples_of_subsets_features_are_predictive/,DoIHAVeaNIdenTItY,1565707020,"I'm doing a project with my professor. We are analyzing impact of each feature on model performance, mostly we do it for gradient boost. Professor told me that for samples of subsets features are predictive and it is a problem that we can observe this by analyzing the shallow trees created during the process.

&amp;#x200B;

What does ""samples of subsets features are predictive"" means? I have been searching internet but couldn't find anything. Any ideas?",6,0
817,2019-8-13,2019,8,13,23,cpu5uc,[Research] What is the State of AutoML in 2019?,https://www.reddit.com/r/MachineLearning/comments/cpu5uc/research_what_is_the_state_of_automl_in_2019/,cdossman,1565707754," [https://medium.com/ai%C2%B3-theory-practice-business/what-is-the-state-of-automl-in-2019-64167f581dd1](https://medium.com/ai%C2%B3-theory-practice-business/what-is-the-state-of-automl-in-2019-64167f581dd1)  

AbstractDeep learning has penetrated all aspects of our lives and brought us great convenience. However, the process of building a high-quality deep learning system for a specific task is not only time-consuming but also requires lots of resources and relies on human expertise, which hinders the development of deep learning in both industry and academia. To alleviate this problem, a growing number of research projects focus on automated machine learning (AutoML). In this paper, we provide a comprehensive and up-to-date study on the state-of-the-art AutoML. First, we introduce the AutoML techniques in details according to the machine learning pipeline. Then we summarize existing Neural Architecture Search (NAS) research, which is one of the most popular topics in AutoML. We also compare the models generated by NAS algorithms with those human-designed models. Finally, we present several open problems for future research.",5,4
818,2019-8-14,2019,8,14,0,cpupzh, What happens when youre trying to push limits and break records?,https://www.reddit.com/r/MachineLearning/comments/cpupzh/what_happens_when_youre_trying_to_push_limits_and/,AIthatDrives,1565710190,,0,1
819,2019-8-14,2019,8,14,0,cpv112,[P] Kannada-MNIST: A new handwritten digits dataset for the Kannada language,https://www.reddit.com/r/MachineLearning/comments/cpv112/p_kannadamnist_a_new_handwritten_digits_dataset/,VinayUPrabhu,1565711553,[removed],2,17
820,2019-8-14,2019,8,14,1,cpv8ju,Harness The Power Of Machine Learning For Unsupervised &amp; Supervised Learning In Python,https://www.reddit.com/r/MachineLearning/comments/cpv8ju/harness_the_power_of_machine_learning_for/,luckyluck123luck,1565712477,,0,1
821,2019-8-14,2019,8,14,1,cpv9za,[News] Megatron-LM: NVIDIA trains 8.3B GPT-2 using model and data parallelism on 512 GPUs. SOTA in language modelling and SQUAD. Details awaited.,https://www.reddit.com/r/MachineLearning/comments/cpv9za/news_megatronlm_nvidia_trains_83b_gpt2_using/,Professor_Entropy,1565712629,,1,1
822,2019-8-14,2019,8,14,1,cpvssu,[News] Megatron-LM: NVIDIA trains 8.3B GPT-2 using model and data parallelism on 512 GPUs. SOTA in language modelling and SQUAD. Details awaited.,https://www.reddit.com/r/MachineLearning/comments/cpvssu/news_megatronlm_nvidia_trains_83b_gpt2_using/,Professor_Entropy,1565714888,"Code: [https://github.com/NVIDIA/Megatron-LM](https://github.com/NVIDIA/Megatron-LM)

Unlike Open-AI, they have released complete code for data processing, training, and evaluation.

Detailed writeup: [https://nv-adlr.github.io/MegatronLM](https://nv-adlr.github.io/MegatronLM)

From github:

&gt;Megatron  is a large, powerful transformer. This repo is for ongoing  research on  training large, powerful transformer language models at  scale.  Currently, we support model-parallel, multinode training of [GPT2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and [BERT](https://arxiv.org/pdf/1810.04805.pdf) in mixed precision.  
Our  codebase is capable of efficiently training a 72-layer, 8.3  Billion  Parameter GPT2 Language model with 8-way model and 64-way data   parallelism across 512 GPUs. We find that bigger language models are   able to surpass current GPT2-1.5B wikitext perplexities in as little as 5   epochs of training.  
For BERT  training our repository trains BERT Large on 64 V100 GPUs in  3 days. We  achieved a final language modeling perplexity of 3.15 and  SQuAD  F1-score of 90.7.

Their submission is not in the leaderboard of SQuAD, but this exceeds the previous best single model performance (RoBERTa 89.8).

For  language modelling they get zero-shot wikitext perplexity of 17.4 (8.3B  model) better than 18.3 of transformer-xl (257M). However they claim it  as SOTA when GPT-2 itself has 17.48 ppl, and another model has 16.4 ([https://paperswithcode.com/sota/language-modelling-on-wikitext-103](https://paperswithcode.com/sota/language-modelling-on-wikitext-103))

Sadly they haven't mentioned anything about release of the model weights.",71,334
823,2019-8-14,2019,8,14,1,cpvt5z,Protecting bananas through AI,https://www.reddit.com/r/MachineLearning/comments/cpvt5z/protecting_bananas_through_ai/,finphil,1565714924,,0,1
824,2019-8-14,2019,8,14,1,cpvxru,Behaviour Suite for Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/cpvxru/behaviour_suite_for_reinforcement_learning/,Bayes-Ian,1565715486,[removed],0,1
825,2019-8-14,2019,8,14,1,cpvxsn,[D] I need ideas on how to use the Google Trends data to build a ML model,https://www.reddit.com/r/MachineLearning/comments/cpvxsn/d_i_need_ideas_on_how_to_use_the_google_trends/,m-i-n-a-r,1565715488,"Like the title says, i had the idea to use the Google Trends data (both using the site or the unofficial API, if they still work) to train a model of some kind for a university project, but as often happens, when i started working i found out that my ideas were unrealistic or too much ambitious. 

I'm not an expert but i know the basics of Keras and TF. The only thing i did was downloading some csvs  from the site and using them to predict the present using the data from the past. This kind of elaboration works for periodic data of course (for example i tried ""ground zero""). i used simple networks based on LSTM, CNN or MLP. 

Knowing that i only have normalized data and monthly reports for 15 years (180 rows, more or less), how can i use one or more of this data? I just need an idea or some kind of reference!",2,0
826,2019-8-14,2019,8,14,2,cpw30m,Some of the best courses for AI and ML,https://www.reddit.com/r/MachineLearning/comments/cpw30m/some_of_the_best_courses_for_ai_and_ml/,mlait1908,1565716086,,0,1
827,2019-8-14,2019,8,14,2,cpwe03,Hi need help with ML linear regression model using dates as part of variables.,https://www.reddit.com/r/MachineLearning/comments/cpwe03/hi_need_help_with_ml_linear_regression_model/,collinl33t,1565717382,[removed],0,1
828,2019-8-14,2019,8,14,2,cpwiu0,Project Euphonias Personalized Speech Recognition for Non-Standard Speech,https://www.reddit.com/r/MachineLearning/comments/cpwiu0/project_euphonias_personalized_speech_recognition/,sjoerdapp,1565717953,,0,1
829,2019-8-14,2019,8,14,2,cpwju2,[P] Towards explainable video analysis - Visual Attention For Action Recognition,https://www.reddit.com/r/MachineLearning/comments/cpwju2/p_towards_explainable_video_analysis_visual/,dtransposed,1565718059,"I am currently researching practical applications of action recognition models with use of attention models. I have decided to share lessons learned from implementing several ideas from research papers in this field. The network learns to classify images from HMDB-51 dataset and creates attention heatmaps which focus on different parts on the image and thus justify model's decision. Heatmaps can be very accurate, to the point that one could probably use them for tracking.

The tutorial contains brief overview of action recognition and visual attention mechanisms. Then I present the network architecture and discuss the results of my project. Additionally, I include github repo with my implementation.

[Here are the results!](https://dtransposed.github.io/blog/Action-Recognition-Attention.html)

I hope you guys find it interesting! 

https://i.redd.it/l01bfscy59g31.gif",0,3
830,2019-8-14,2019,8,14,2,cpwjyk,"[P] Cox, a lightweight python logging library designed for automating and analyzing machine learning experiments.",https://www.reddit.com/r/MachineLearning/comments/cpwjyk/p_cox_a_lightweight_python_logging_library/,loganengstrom,1565718075,,0,2
831,2019-8-14,2019,8,14,2,cpwpuo,Google ML Crash Course videos not displaying,https://www.reddit.com/r/MachineLearning/comments/cpwpuo/google_ml_crash_course_videos_not_displaying/,CeramicVulture,1565718768,"I am looking at the [Google Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/ml-intro) but the videos are not showing for me on any of the pages.  I thought maybe Pihole was blocking them but apparently not.

&amp;#x200B;

Can anyone actually see these videos?",0,1
832,2019-8-14,2019,8,14,3,cpwtod,[D] Full time consulting/remote/contractor work as a PhD?,https://www.reddit.com/r/MachineLearning/comments/cpwtod/d_full_time_consultingremotecontractor_work_as_a/,moduluus,1565719224,"There's a lot of posts here about getting research jobs at one of the top labs in industry after PhD, but I'm curious if anyone else has the ultimate goal of living in a nice low CoL area and working remotely.  My area is machine learning, broadly applied to computer vision and robotics.

I think I'll almost certainly have to work a number of years after graduating in one of the large industry clusters (i.e. the Bay Area) but I would love to be able to transition into a remote/consulting/contracting role after that and buy a house somewhere else.  My wife is a physician so she can work pretty much anywhere there's a hospital (in fact for her the pay is better the more remote the location).

Has anyone gone down this route? What kinds of companies in the field are open to remote or hire contractors (and how do you go about getting gigs?) How do I plan for this now if it's my ultimate goal, or is this area so specialized/niche that almost all of the opportunities are onsite only?",28,11
833,2019-8-14,2019,8,14,3,cpxbv0,Multi-view vs FeatureUnion?,https://www.reddit.com/r/MachineLearning/comments/cpxbv0/multiview_vs_featureunion/,fghadami,1565721395,[removed],0,1
834,2019-8-14,2019,8,14,3,cpxcno,I'm just getting started with machine learning. Is it better for me to use tensorflow or try and make my own library?,https://www.reddit.com/r/MachineLearning/comments/cpxcno/im_just_getting_started_with_machine_learning_is/,386374581,1565721498,,0,1
835,2019-8-14,2019,8,14,3,cpxdla,3 lessons from running an AI-powered start-up in Africa,https://www.reddit.com/r/MachineLearning/comments/cpxdla/3_lessons_from_running_an_aipowered_startup_in/,savvy_bernice,1565721603,,0,1
836,2019-8-14,2019,8,14,4,cpxq1l,[P] Cox: a python logging library for machine learning experiments,https://www.reddit.com/r/MachineLearning/comments/cpxq1l/p_cox_a_python_logging_library_for_machine/,loganengstrom,1565723086,"Cox is a logging library for python designed for collecting and analyzing data from experiments! Read more and install it here: https://github.com/madrylab/cox

Cox is built for a pattern in experimental design where each individual run of an experiment (e.g. each hyperparam configuration) writes to a separate, database-like store (complete with schemes, indexing, etc), saving all information in tables. Experiments are collected and analyzed together by merging together tables, and Cox provides a really simple API/flow for merging and analyzing multiple experiments at the same time (e.g. comparing results across hyperparameters).

This pattern is particularly common in machine learning! Weve used this logging library for projects involving RL and supervised learning and found it really helpful. Check out the repository for more information, and let me know if you have any questions!",3,6
837,2019-8-14,2019,8,14,4,cpybwg,System recommendation,https://www.reddit.com/r/MachineLearning/comments/cpybwg/system_recommendation/,raprakashvi,1565725690,[removed],0,1
838,2019-8-14,2019,8,14,5,cpyv5w,[P] This conversational AI has feelings that respond to what you say,https://www.reddit.com/r/MachineLearning/comments/cpyv5w/p_this_conversational_ai_has_feelings_that/,James_Representi,1565727929,"Ri, a conversational AI, links different ideas. It has a vocabulary and doesn't need to be trained. Other features: You change the way Ri feels with conversation. Ri answers your questions. It can relate memories and tell stories. Ri will continue talking if it thinks it's said something clever. Try it at: [http://representi.com](http://representi.com).",6,0
839,2019-8-14,2019,8,14,5,cpz68z,"[R][BAIR] ""we show that a generative text model trained on sensitive data can actually memorize its training data"" - Nicholas Carlini",https://www.reddit.com/r/MachineLearning/comments/cpz68z/rbair_we_show_that_a_generative_text_model/,downtownslim,1565729272,"Evaluating and Testing Unintended Memorization in Neural Networks

Link: [https://bair.berkeley.edu/blog/2019/08/13/memorization/](https://bair.berkeley.edu/blog/2019/08/13/memorization/)

&gt;For example, we show that given access to a language model trained on the Penn Treebank with *one* credit card number inserted, it is possible to **completely extract** this credit card number from the model.",13,15
840,2019-8-14,2019,8,14,6,cpzgc7,Chess engine,https://www.reddit.com/r/MachineLearning/comments/cpzgc7/chess_engine/,corzoai,1565730468,"I am planning to create a chess engine from scratch. But I do not know whether to use Go or Python to write it, rather than the more common C++, as I want to use other language. What do you think is better in terms of performance/speed for a chess engine, Go or Python?
Thank you,",0,1
841,2019-8-14,2019,8,14,6,cpzx60,TensorFlow in Practice Specialization  Build high value-high demand skills which will boost your career and earning growth,https://www.reddit.com/r/MachineLearning/comments/cpzx60/tensorflow_in_practice_specialization_build_high/,internetdigitalentre,1565732511,[removed],0,1
842,2019-8-14,2019,8,14,6,cq03gs,[D] Training maskrcnn-benchmark,https://www.reddit.com/r/MachineLearning/comments/cq03gs/d_training_maskrcnnbenchmark/,crytoy,1565733283,[C](https://github.com/facebookresearch/maskrcnn-benchmark)an anyone create a video tutorial on how to train maskrcnn-benchmark,0,0
843,2019-8-14,2019,8,14,6,cq03pr,How to train deepfakes?,https://www.reddit.com/r/MachineLearning/comments/cq03pr/how_to_train_deepfakes/,jennysebastian,1565733315,[removed],0,1
844,2019-8-14,2019,8,14,7,cq0d33,"""Its very easy to confuse that artificial intelligence, machine learning, and deep learning are the same thing."" - Great explanation of difference between them",https://www.reddit.com/r/MachineLearning/comments/cq0d33/its_very_easy_to_confuse_that_artificial/,susanvilleula1,1565734493,,0,1
845,2019-8-14,2019,8,14,7,cq0u0v,How much data is sufficient to learn high-performing algorithms?,https://www.reddit.com/r/MachineLearning/comments/cq0u0v/how_much_data_is_sufficient_to_learn/,m0tivic,1565736646,[removed],0,1
846,2019-8-14,2019,8,14,7,cq0uen,high-res (+4MP) neural-style with home PC?,https://www.reddit.com/r/MachineLearning/comments/cq0uen/highres_4mp_neuralstyle_with_home_pc/,C0MPAQ,1565736690,[removed],2,2
847,2019-8-14,2019,8,14,8,cq10vy,[R] Biological learning curves outperform existing ones in artificial intelligence algorithms,https://www.reddit.com/r/MachineLearning/comments/cq10vy/r_biological_learning_curves_outperform_existing/,naxospade,1565737534,"I haven't seen any discussion in this subreddit yet, though it was published only a few days ago.

Paper:  [https://www.nature.com/articles/s41598-019-48016-4](https://www.nature.com/articles/s41598-019-48016-4) 

Abstract:

&gt;Recently, deep learning algorithms have outperformed human experts in various tasks across several domains; however, their characteristics are distant from current knowledge of neuroscience. The simulation results of biological learning algorithms presented herein outperform state-of-the-art optimal learning curves in supervised learning of feedforward networks. The biological learning algorithms comprise asynchronous input signals with decaying input summation, weights adaptation, and multiple outputs for an input signal. In particular, the generalization error for such biological perceptrons decreases rapidly with increasing number of examples, and it is independent of the size of the input. This is achieved using either synaptic learning, or solely through dendritic adaptation with a mechanism of swinging between reflecting boundaries, without learning steps. The proposed biological learning algorithms outperform the optimal scaling of the learning curve in a traditional perceptron. It also results in a considerable robustness to disparity between weights of two networks with very similar outputs in biological supervised learning scenarios. The simulation results indicate the potency of neurobiological mechanisms and open opportunities for developing a superior class of deep learning algorithms.",15,21
848,2019-8-14,2019,8,14,8,cq120g,Does AGI even have an incentive to self-improve?,https://www.reddit.com/r/MachineLearning/comments/cq120g/does_agi_even_have_an_incentive_to_selfimprove/,xsaav,1565737684,"When I read about the debate on whether the singularity will happen, people always assume that the AI will improve itself. Let's say that we create AGI that is smarter than us. Would it really bother trying to improve itself? ""To survive"", you might say but why assume that AI has a ""will to survive""? Humans have an inherent urge to reproduce, find meaning in relationships, work etc, but would the AGI even have any of those, or other, incentives? Maybe it would become depressed and shut itself off or something.",0,1
849,2019-8-14,2019,8,14,8,cq1bg0,useful cheat sheet credit in tweet,https://www.reddit.com/r/MachineLearning/comments/cq1bg0/useful_cheat_sheet_credit_in_tweet/,tradediscount,1565738959,,0,1
850,2019-8-14,2019,8,14,9,cq2dil,[D] IJCAI in Macau and EMNLP in Hong Kong,https://www.reddit.com/r/MachineLearning/comments/cq2dil/d_ijcai_in_macau_and_emnlp_in_hong_kong/,sensetime,1565744314,"Not sure if anyone is actively following the political situation in Hong Kong, but my understanding is that flights have been cancelled at the airport. Many parts of the city has also been subject to tear gas and there have been reports of police attacks on bystanders on the street near the political protests.

Is the situation affecting the people currently at IJCAI? Many people are flying to Macau via HK.

Also EMNLP will take place in HK in November, not far away. With the tensions between mainland China and Hong Kong escalating, I wonder if organizers have plans to move the conference to another location.

https://www.emnlp-ijcnlp2019.org",10,25
851,2019-8-14,2019,8,14,9,cq2e03,"[D] Thoughts on this video which frames learning across three categories at three timescales (evolution, in life &amp; abstract)",https://www.reddit.com/r/MachineLearning/comments/cq2e03/d_thoughts_on_this_video_which_frames_learning/,britcruise,1565744387,,0,1
852,2019-8-14,2019,8,14,10,cq2qba,Automatic Filling Plugging Capping Machine for E-liquid Bottling Filler,https://www.reddit.com/r/MachineLearning/comments/cq2qba/automatic_filling_plugging_capping_machine_for/,amygao1984,1565746157,,0,1
853,2019-8-14,2019,8,14,11,cq37v8,[1908.04577] StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding,https://www.reddit.com/r/MachineLearning/comments/cq37v8/190804577_structbert_incorporating_language/,HigherTopoi,1565748689,,4,15
854,2019-8-14,2019,8,14,11,cq3bj8,"[P] Simple Tensorflow implementation of ""Few-Shot Unsupervised Image-to-Image Translation (a.k.a. FUNIT) in ICCV 2019""",https://www.reddit.com/r/MachineLearning/comments/cq3bj8/p_simple_tensorflow_implementation_of_fewshot/,taki0112,1565749220,"&amp;#x200B;

[Our result](https://i.redd.it/c1xrm0ykqbg31.png)",1,1
855,2019-8-14,2019,8,14,11,cq3ckf,"[P] Simple Tensorflow implementation of ""Few-Shot Unsupervised Image-to-Image Translation (a.k.a. FUNIT)"" in ICCV 2019",https://www.reddit.com/r/MachineLearning/comments/cq3ckf/p_simple_tensorflow_implementation_of_fewshot/,taki0112,1565749375,"&amp;#x200B;

[Our result](https://i.redd.it/3x2lun84rbg31.png)

[Paper result](https://i.redd.it/63fk49h6rbg31.gif)",1,1
856,2019-8-14,2019,8,14,11,cq3e0o,[R] Neural Text Generation with Unlikelihood Training,https://www.reddit.com/r/MachineLearning/comments/cq3e0o/r_neural_text_generation_with_unlikelihood/,hardmaru,1565749585,,12,22
857,2019-8-14,2019,8,14,12,cq47du,"This new launch is going to be a turning point for you, and your experience with chatbots is only going to get better from here on! Making bot building effortless. Stay tuned at www.engati.com",https://www.reddit.com/r/MachineLearning/comments/cq47du/this_new_launch_is_going_to_be_a_turning_point/,getengati,1565754048,,0,1
858,2019-8-14,2019,8,14,14,cq50h2,Voice bots are the new hot thing!,https://www.reddit.com/r/MachineLearning/comments/cq50h2/voice_bots_are_the_new_hot_thing/,getengati,1565758825,[removed],0,1
859,2019-8-14,2019,8,14,16,cq6854,[D] UMAP (dimensionality reduction algorithm),https://www.reddit.com/r/MachineLearning/comments/cq6854/d_umap_dimensionality_reduction_algorithm/,timscarfe,1565766979,"Interested in dimensionality reduction? TSNE is so last century, these days its all about UMAP! Join [**Mihaela Curmei**](https://www.linkedin.com/in/ACoAAAutWGIByOMoQ-RhhTG8U-rGeH0Q48-y47c/) as she delivers a sublime presentation on UMAP! 

[https://www.youtube.com/watch?v=G9s3cE8TNZo](https://www.youtube.com/watch?v=G9s3cE8TNZo)

UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.",48,79
860,2019-8-14,2019,8,14,16,cq68zv,US President Hopeful Andrew Yang will match China's spending on AI instead of focusing on harmful trade wars,https://www.reddit.com/r/MachineLearning/comments/cq68zv/us_president_hopeful_andrew_yang_will_match/,aerowindwalker,1565767147,,0,1
861,2019-8-14,2019,8,14,16,cq69xj,MASHAgent: A mishmash of experimental approaches for reinforcement learning.,https://www.reddit.com/r/MachineLearning/comments/cq69xj/mashagent_a_mishmash_of_experimental_approaches/,ZeroMaxinumXZ,1565767330,,0,1
862,2019-8-14,2019,8,14,16,cq6ao5,ML solution to assist driver in car swerving situation,https://www.reddit.com/r/MachineLearning/comments/cq6ao5/ml_solution_to_assist_driver_in_car_swerving/,UCantHandleTheTruth8,1565767472,,0,1
863,2019-8-14,2019,8,14,16,cq6bmo,Machine Learning Playground,https://www.reddit.com/r/MachineLearning/comments/cq6bmo/machine_learning_playground/,Gisebert,1565767674,[removed],0,1
864,2019-8-14,2019,8,14,16,cq6c0d,Data Analysis &amp;amp; Machine Learning Using R Training in Kolkata - ICSS,https://www.reddit.com/r/MachineLearning/comments/cq6c0d/data_analysis_amp_machine_learning_using_r/,duttajoy1,1565767743,,0,1
865,2019-8-14,2019,8,14,16,cq6g16,Looking for background on RNN's and their use in modeling prefrontal cortex [Discussion],https://www.reddit.com/r/MachineLearning/comments/cq6g16/looking_for_background_on_rnns_and_their_use_in/,Stereoisomer,1565768576," Hi all,

I'm about to start a PhD in cognitive/computational neuroscience and I was having trouble finding some good background on this but I was wondering if anyone here has some good suggestions for reviews or landmark pieces of literature on the study of RNN's for modeling neural dynamics especially in prefrontal cortex?

I'm mostly thinking along the lines of Earl Miller's recent work in applying models using reservoir computing or the Shenoy labs use of a sequential variational autoencoder (LFADS) for modeling neural state space trajectories and the associated background. I have a BS and MS in Applied Math so technical reviews that unify and lend generality are preferred such as, my all-time favorite, A Unifying Review of Gaussian Linear Models by Roweis and Ghahramani (but for RNN's). I don't suppose there are any books about these yet.

Also, theoretical perspectives regarding the training and topology of RNN's are also of much interest!

Thanks in advance!",1,3
866,2019-8-14,2019,8,14,16,cq6ir2,Opensource service for realtime video analysis,https://www.reddit.com/r/MachineLearning/comments/cq6ir2/opensource_service_for_realtime_video_analysis/,FastoNoSQL,1565769138,,0,1
867,2019-8-14,2019,8,14,16,cq6kb2,Public popular video dataset for highlights detect,https://www.reddit.com/r/MachineLearning/comments/cq6kb2/public_popular_video_dataset_for_highlights_detect/,hVVanghsuanT12,1565769491,[removed],0,1
868,2019-8-14,2019,8,14,17,cq6oy0,Machine Learning Development Company | Artificial Intelligence | Cloud AI Solutions | RPA Development,https://www.reddit.com/r/MachineLearning/comments/cq6oy0/machine_learning_development_company_artificial/,Optisoldatalabs,1565770450,,0,1
869,2019-8-14,2019,8,14,17,cq6xtd,PCA and LDA reduction techniques,https://www.reddit.com/r/MachineLearning/comments/cq6xtd/pca_and_lda_reduction_techniques/,navdeepsony,1565772334,[removed],0,1
870,2019-8-14,2019,8,14,18,cq7h0g,test,https://www.reddit.com/r/MachineLearning/comments/cq7h0g/test/,taki0112,1565776250,[removed],0,1
871,2019-8-14,2019,8,14,19,cq7sit,Is There a Standard Heuristic for Model Tuning?,https://www.reddit.com/r/MachineLearning/comments/cq7sit/is_there_a_standard_heuristic_for_model_tuning/,mlait1908,1565778389,,0,1
872,2019-8-14,2019,8,14,19,cq7uxt,Artificial Intelligence is Improving the E-commerce Experience,https://www.reddit.com/r/MachineLearning/comments/cq7uxt/artificial_intelligence_is_improving_the/,MachineLearning001,1565778850,,0,1
873,2019-8-14,2019,8,14,20,cq89jz,INCORPORATING ML FOR DATA ANALYTICS CAN FATHOM BIG DATA STORAGE CONCERNS,https://www.reddit.com/r/MachineLearning/comments/cq89jz/incorporating_ml_for_data_analytics_can_fathom/,analyticsinsight,1565781489,,0,1
874,2019-8-14,2019,8,14,20,cq8kft,Machine Learning in JavaScript with TensorFlow.js,https://www.reddit.com/r/MachineLearning/comments/cq8kft/machine_learning_in_javascript_with_tensorflowjs/,inspiredDeveloper,1565783383,[removed],0,1
875,2019-8-14,2019,8,14,21,cq8vbu,"[R] A NetworkX toy implementation of ""EdMot: An Edge Enhancement Approach for Motif-aware Community Detection"" (KDD 2019)",https://www.reddit.com/r/MachineLearning/comments/cq8vbu/r_a_networkx_toy_implementation_of_edmot_an_edge/,benitorosenberg,1565785092,"&amp;#x200B;

https://i.redd.it/mm900nfcpeg31.jpg

Github: [https://github.com/benedekrozemberczki/EdMot](https://github.com/benedekrozemberczki/EdMot)

Paper:  [https://arxiv.org/abs/1906.04560](https://arxiv.org/abs/1906.04560)

Abstract: 

Network community detection is a hot research topic in network analysis.  Although many methods have been proposed for community detection, most  of them only take into consideration the lower-order structure of the  network at the level of individual nodes and edges. Thus, they fail to  capture the higher-order characteristics at the level of small dense  subgraph patterns, e.g., motifs. Recently, some higher-order methods  have been developed but they typically focus on the motif-based  hypergraph which is assumed to be a connected graph. However, such  assumption cannot be ensured in some real-world networks. In particular,  the hypergraph may become fragmented. That is, it may consist of a  large number of connected components and isolated nodes, despite the  fact that the original network is a connected graph. Therefore, the  existing higher-order methods would suffer seriously from the above  fragmentation issue, since in these approaches, nodes without connection  in hypergraph can't be grouped together even if they belong to the same  community. To address the above fragmentation issue, we propose an Edge  enhancement approach for Motif-aware community detection (EdMot ). The  main idea is as follows. Firstly, a motif-based hypergraph is  constructed and the top K largest connected components in the hypergraph  are partitioned into modules. Afterwards, the connectivity structure  within each module is strengthened by constructing an edge set to derive  a clique from each module. Based on the new edge set, the original  connectivity structure of the input network is enhanced to generate a  rewired network, whereby the motif-based higher-order structure is  leveraged and the hypergraph fragmentation issue is well addressed.  Finally, the rewired network is partitioned to obtain the higher-order  community structure. Extensive experiments have been conducted on eight  real-world datasets and the results show the effectiveness of the  proposed method in improving the community detection performance of  state-of-the-art methods.",1,58
876,2019-8-14,2019,8,14,21,cq933k,Finding a ML/DL job right out of undergrad,https://www.reddit.com/r/MachineLearning/comments/cq933k/finding_a_mldl_job_right_out_of_undergrad/,whitehathack123,1565786286,[removed],0,1
877,2019-8-14,2019,8,14,21,cq9466,AI &amp; Machine Learning Consulting Companies India | SignitySolutions,https://www.reddit.com/r/MachineLearning/comments/cq9466/ai_machine_learning_consulting_companies_india/,sameer-25,1565786451,,0,1
878,2019-8-14,2019,8,14,21,cq96ef,The Blowjob Singularity: Machine Learning Analysis of Movement in Oral Sex Performed Upon Men,https://www.reddit.com/r/MachineLearning/comments/cq96ef/the_blowjob_singularity_machine_learning_analysis/,byonge,1565786778,[removed],0,1
879,2019-8-14,2019,8,14,22,cq9lvt,https://datascienceplus.com/machine-learning-create-expert-systems/,https://www.reddit.com/r/MachineLearning/comments/cq9lvt/httpsdatasciencepluscommachinelearningcreateexpert/,sanjayit38,1565789005,[removed],0,1
880,2019-8-14,2019,8,14,22,cq9m9t,Deep Learning in Python: Getting Started,https://www.reddit.com/r/MachineLearning/comments/cq9m9t/deep_learning_in_python_getting_started/,andrea_manero,1565789054,[removed],0,1
881,2019-8-14,2019,8,14,22,cq9r1y,Machine Learning: Create Expert Systems,https://www.reddit.com/r/MachineLearning/comments/cq9r1y/machine_learning_create_expert_systems/,sanjayit38,1565789701,[removed],0,1
882,2019-8-14,2019,8,14,22,cq9s30,metric-learn: Metric Learning Algorithms in Python,https://www.reddit.com/r/MachineLearning/comments/cq9s30/metriclearn_metric_learning_algorithms_in_python/,terrytangyuan,1565789837,,6,28
883,2019-8-14,2019,8,14,22,cq9wtg,[D] Intermediate intro to Bias and Variance,https://www.reddit.com/r/MachineLearning/comments/cq9wtg/d_intermediate_intro_to_bias_and_variance/,s_u_t,1565790494,"Lurker, posting:

So I've written this medium article explaining the fundamentals of bias and variance of ML models. [https://medium.com/@kocherlakota/bias-variance-e4502eb4ad5?sk=6f9ed14ed41ead4ef8e3cb4b2ec1e204](https://medium.com/@kocherlakota/bias-variance-e4502eb4ad5?sk=6f9ed14ed41ead4ef8e3cb4b2ec1e204) 

&amp;#x200B;

Looking for any feedback or discussion you guys might have on the contents. 

&amp;#x200B;

&amp;#x200B;

PS. Not sure if this belongs on /r/learnmachinelearning, I guess there's some non-trivial statistics that warrants a discussion on this sub?",3,0
884,2019-8-14,2019,8,14,23,cqa67y,What names do you suggest for podcasts book about deep learning and machine learning and ........?,https://www.reddit.com/r/MachineLearning/comments/cqa67y/what_names_do_you_suggest_for_podcasts_book_about/,Doctor_who1,1565791699,[removed],0,0
885,2019-8-14,2019,8,14,23,cqa78o,[R] Opportunities and Challenges Of Machine Learning Accelerators In Production,https://www.reddit.com/r/MachineLearning/comments/cqa78o/r_opportunities_and_challenges_of_machine/,mttd,1565791828,,0,1
886,2019-8-14,2019,8,14,23,cqaa97,Are you looking for a free course in data science? here is a free course,https://www.reddit.com/r/MachineLearning/comments/cqaa97/are_you_looking_for_a_free_course_in_data_science/,sajad-52,1565792222,,0,1
887,2019-8-14,2019,8,14,23,cqabym,Do you use chatbots in your business?,https://www.reddit.com/r/MachineLearning/comments/cqabym/do_you_use_chatbots_in_your_business/,ksjurewicz,1565792439,[removed],0,1
888,2019-8-14,2019,8,14,23,cqakyg,"Assign Job, A Industry Based On Description",https://www.reddit.com/r/MachineLearning/comments/cqakyg/assign_job_a_industry_based_on_description/,Shoebsshaikh,1565793623,[removed],0,1
889,2019-8-15,2019,8,15,0,cqb0i9,"I'm proud to share my latest project, a linear optimization model in python",https://www.reddit.com/r/MachineLearning/comments/cqb0i9/im_proud_to_share_my_latest_project_a_linear/,ahershy,1565795562,,0,1
890,2019-8-15,2019,8,15,0,cqb9yj,ONNX model is 50x slower than in pytorch,https://www.reddit.com/r/MachineLearning/comments/cqb9yj/onnx_model_is_50x_slower_than_in_pytorch/,drr21,1565796706,[removed],0,1
891,2019-8-15,2019,8,15,0,cqbkgd,"Simple Questions Thread August 14, 2019",https://www.reddit.com/r/MachineLearning/comments/cqbkgd/simple_questions_thread_august_14_2019/,AutoModerator,1565797988,[removed],0,1
892,2019-8-15,2019,8,15,1,cqc510,Need help in generating wikisum data,https://www.reddit.com/r/MachineLearning/comments/cqc510/need_help_in_generating_wikisum_data/,Skrrohit,1565800461,[removed],0,1
893,2019-8-15,2019,8,15,1,cqcf34,[P] Deep learning can decode neural activity to predict a mouse's location,https://www.reddit.com/r/MachineLearning/comments/cqcf34/p_deep_learning_can_decode_neural_activity_to/,csn8587,1565801670,"A team of researchers have a blog post about using neural networks to decode hippocampal activity to accurately predict a mouse's location using.

[https://www.twosixlabs.com/translating-between-brain-and-world-decoding-biological-neural-nets-with-artificial-neural-nets/](https://www.twosixlabs.com/translating-between-brain-and-world-decoding-biological-neural-nets-with-artificial-neural-nets/)",38,222
894,2019-8-15,2019,8,15,2,cqcxnt,U-net segmentation blank image output,https://www.reddit.com/r/MachineLearning/comments/cqcxnt/unet_segmentation_blank_image_output/,fubbyy,1565803917,[removed],0,1
895,2019-8-15,2019,8,15,2,cqda1j,[P] Introduction to Optimization with Genetic Algorithm,https://www.reddit.com/r/MachineLearning/comments/cqda1j/p_introduction_to_optimization_with_genetic/,ahmedfgad,1565805410,"Selection of the optimal parameters values for machine learning tasks is challenging. Some results may be bad not because the data is noisy or the used learning algorithm is weak, but due to the bad selection of the parameters values. This article gives a brief introduction about evolutionary algorithms (EAs) and describes genetic algorithm (GA) which is one of the simplest random-based EAs.

[https://www.linkedin.com/pulse/introduction-optimization-genetic-algorithm-ahmed-gad](https://www.linkedin.com/pulse/introduction-optimization-genetic-algorithm-ahmed-gad/)",2,0
896,2019-8-15,2019,8,15,3,cqe0s6,Semantic Based Adversarial Examples Fool Face Recognition,https://www.reddit.com/r/MachineLearning/comments/cqe0s6/semantic_based_adversarial_examples_fool_face/,Yuqing7,1565808555,,0,1
897,2019-8-15,2019,8,15,3,cqe3cj,Some Of The Best Online Course For AI and ML,https://www.reddit.com/r/MachineLearning/comments/cqe3cj/some_of_the_best_online_course_for_ai_and_ml/,mlait1908,1565808853,,0,1
898,2019-8-15,2019,8,15,4,cqejj3,What is the difference between Kalman Filter and Reinforcement Learning?,https://www.reddit.com/r/MachineLearning/comments/cqejj3/what_is_the_difference_between_kalman_filter_and/,darchcruise,1565810790,[removed],0,1
899,2019-8-15,2019,8,15,4,cqepbo,I used haar cascade to make meme face from any photo,https://www.reddit.com/r/MachineLearning/comments/cqepbo/i_used_haar_cascade_to_make_meme_face_from_any/,nerdy_wits,1565811485,,0,1
900,2019-8-15,2019,8,15,4,cqeqod,Does Deep Learning Still Need Backpropagation?,https://www.reddit.com/r/MachineLearning/comments/cqeqod/does_deep_learning_still_need_backpropagation/,Yuqing7,1565811647,,0,1
901,2019-8-15,2019,8,15,6,cqfuev,[N] Let's help these Sudanese ML researchers get to the Deep Learning Indaba and strengthen ML in Africa,https://www.reddit.com/r/MachineLearning/comments/cqfuev/n_lets_help_these_sudanese_ml_researchers_get_to/,deepaurorasky,1565816512,[removed],1,10
902,2019-8-15,2019,8,15,6,cqgdjq,pytorch/pytorch-probot by pytorch,https://www.reddit.com/r/MachineLearning/comments/cqgdjq/pytorchpytorchprobot_by_pytorch/,sjoerdapp,1565818873,,0,1
903,2019-8-15,2019,8,15,8,cqi19q,"[P] Music Transformer(Google Brain, ICLR2019) Re-Implementation (tensorflow 2.0)",https://www.reddit.com/r/MachineLearning/comments/cqi19q/p_music_transformergoogle_brain_iclr2019/,LIKELIH00D,1565826723,"Hello, r/MachineLearning! 

As I finished implementation of Google's MusicTransformer, I want to share it to here.

* github : [https://github.com/jason9693/MusicTransformer-tensorflow2.0](https://l.facebook.com/l.php?u=https%3A%2F%2Fgithub.com%2Fjason9693%2FMusicTransformer-tensorflow2.0%3Ffbclid%3DIwAR0NqtbfUPl1vn0-97DdCMneJkf49dAupKSt0Lqzikup2p86hlERNZruuHs&amp;h=AT1fEV90wO2RCJtasVhcjsJohlnYjZ_eidMCZu_6_wF88h7yRFv8v7IS-CC9sgzhQvwu6udtdm-B6ZSRoU5c6V3FdqKu_RbZwIEeykPbeJV4DnyxOiQHgmTBZ1ViBAXMJlUbi6HNnqXmWiUH4hJqjmc_2zE)
* library : Tensorflow2.0 (beta)
* training env : v100 x 1GPU

&gt;*paper :*  [*https://arxiv.org/abs/1809.04281*](https://l.facebook.com/l.php?u=https%3A%2F%2Farxiv.org%2Fabs%2F1809.04281%3Ffbclid%3DIwAR0qxc8OmF0tX-YdALGUa1jQ-WTbtv60w0itxWiRLI_mM_4GLbu86w2-6P8&amp;h=AT0zuFl04LpF94qEOKHujHj_BrXSi0SL0hLo9rrljIOpy_ug_W8K99IKAI7H1NMxdDu8e4dJ3eJplM9oQfZsz_ikIbMAq3B2xIY6IDqZqBoHNDiY14v5Duq2RWqvxv8_UwEVWFYeCdSJjCukydibqnceQkE)  
&gt;  
&gt;*Google Magenta blog :* [*https://magenta.tensorflow.org/music-transformer*](https://magenta.tensorflow.org/music-transformer?fbclid=IwAR3uLJ_dWBwMafckJGiARryt0YsgLuSp5AiNB5HEUpmdSfIuCxjhlR9EVrc)  
&gt;  
&gt;*generated sample demo from this repo :* [*https://www.youtube.com/playlist?list=PLVopZAnUrGWrbIkLGB3bz5nitWThIueS2*](https://www.youtube.com/playlist?list=PLVopZAnUrGWrbIkLGB3bz5nitWThIueS2&amp;fbclid=IwAR1KVXlJ_OV7B0r-aFZaPaNvE6QnxZBpRbAkPxXN5qdF0XHEFrZoYKMEaWI)

For more details, you can see in README.md   
Thank you.

&amp;#x200B;

ps) The video bellow here is a generated sample that listened after ""Boy with Love (BTS)""

![video](g0ea2zt34ig31 ""Generated Music After \""Boy with Love (BTS)\"""")",0,1
904,2019-8-15,2019,8,15,9,cqic00,"[P] Music Transformer(Google Brain, ICLR2019) Re-Implementation (tensorflow 2.0)",https://www.reddit.com/r/MachineLearning/comments/cqic00/p_music_transformergoogle_brain_iclr2019/,LIKELIH00D,1565828245,[removed],0,1
905,2019-8-15,2019,8,15,9,cqitmg,How to Use Deep Learning to Clone Yourself as a Chatbot,https://www.reddit.com/r/MachineLearning/comments/cqitmg/how_to_use_deep_learning_to_clone_yourself_as_a/,LimarcAmbalina,1565830725,,0,1
906,2019-8-15,2019,8,15,10,cqj9es,[D] how can I get a global minimum,https://www.reddit.com/r/MachineLearning/comments/cqj9es/d_how_can_i_get_a_global_minimum/,GoBacksIn,1565832969,"here is my code 

&amp;#x200B;

    for _ in range(10):
        K.clear_session()
        model = Sequential()
    
        model.add(LSTM(256, input_shape=(None, 1)))
        model.add(Dropout(0.2))
    
        model.add(Dense(256))
        model.add(Dropout(0.2))
    
        model.add(Dense(1))
    
        model.compile(loss='mean_squared_error', optimizer='adam')
        hist = model.fit(x_train, y_train, epochs=20, batch_size=64, verbose=0)
    
        p = model.predict(x_test)
        print(mean_squared_error(y_test, p))
    
    
        plt.plot(y_test)
        plt.plot(p)
        plt.legend(['testY', 'p'], loc='upper right')
        plt.show()
        ...
        plt.plot(hist.history['loss'])

&amp;#x200B;

\`Total params\` : 330,241

\`samples\` : 2264

&amp;#x200B;

and below is the result

&amp;#x200B;

https://i.redd.it/2gjgr2uonig31.png

I haven't changed anything.

&amp;#x200B;

I only ran for loop.

&amp;#x200B;

As you can see in the picture, the result of the MSE is huge, even though I have just run the for loop.

&amp;#x200B;

I think the fundamental reason for this problem is that the optimizer can not find global maximum and find local maximum and converge.

The reason is that after checking all the loss graphs, the loss is no longer reduced significantly. (After 20 times)

So in order to solve this problem, I have to find the global minimum. How should I do this?

&amp;#x200B;

I tried adjusting the number of batch\_size, epoch.

Also, I tried hidden layer size, LSTM unit, kerner\_initializer addition, optimizer change, etc. but could not get any meaningful result.

&amp;#x200B;

&amp;#x200B;

I wonder how can I solve this problem.

&amp;#x200B;

Your valuable opinions and thoughts will be very much appreciated.

&amp;#x200B;

if you want to see full source here is link [https://gist.github.com/Lay4U/e1fc7d036356575f4d0799cdcebed90e](https://gist.github.com/Lay4U/e1fc7d036356575f4d0799cdcebed90e)",3,0
907,2019-8-15,2019,8,15,11,cqjors,"[P} Music Transformer ( Huang et al, Google Brain, ICLR2019 ) Re-Implementation (tensorflow 2.0)",https://www.reddit.com/r/MachineLearning/comments/cqjors/p_music_transformer_huang_et_al_google_brain/,LIKELIH00D,1565835190,"Hi, r/MachineLearning ! 

As I finished implementation of Google's MusicTransformer, 

I want to share it to here.

* github : [https://github.com/jason9693/MusicTransformer-tensorflow2.0](https://l.facebook.com/l.php?u=https%3A%2F%2Fgithub.com%2Fjason9693%2FMusicTransformer-tensorflow2.0%3Ffbclid%3DIwAR0_knevmtIX2M3l8D_lAhriE9QVAU98JpLHl6ANvDnuap7DBUIz-eM4FcM&amp;h=AT2xvBreBR5hpKs1y40q01UQBIzc3Y0oBH3JaRCQGwmN-RMM9n7UYdEJVbOwwWLDFQXrU3eoO95RXzGrAws4-XRtbn9DrRvLMzw5bZf5QRymTosSS5h4LBeg17WoLLZKq3MqMWIvRH0i6wCPoKXcRC6zfLA)
* library : Tensorflow2.0 (beta)
* training env : v100 x 1GPU

&gt;*paper :*  [*https://arxiv.org/abs/1809.04281*](https://l.facebook.com/l.php?u=https%3A%2F%2Farxiv.org%2Fabs%2F1809.04281%3Ffbclid%3DIwAR2reLP6v_tGPfzJygFLyXxilKWxhqlCycDMOaqAvFQWu377ujDqd2saFH0&amp;h=AT2V0ng7JgqR6FKk-6KhEijxOOa7r9AE6UNfKCdimM5H7m38_RUluHN4D2WJMypNf2-1c8dTm3qNDOWyksWXq-V1ZIp_XUMOjXBLT9MekWxWyVCzMjTxpCred2tY6B2bWY3RMVPhlnxRh0iPMowXoFG5KvY)  
&gt;  
&gt;*Google Magenta blog :* [*https://magenta.tensorflow.org/music-transformer*](https://l.facebook.com/l.php?u=https%3A%2F%2Fmagenta.tensorflow.org%2Fmusic-transformer%3Ffbclid%3DIwAR01rOvHHMtKUc0it4rt8cwhtDJ_FL3rQT0UFEfQE-FsQmVRR5xGg03Cb9U&amp;h=AT2WoRR5ssbSiumAq7dJtI12hRd-0w_GZErz6j5H3tdchMEYEnS-p8n8i90giVVmqTuW3Pj3D2De4VTDH4Db1TIfQeS7pZImnnalJ8JFlviz6rgZBOgR7ThpOqgW4lm5iK-hM-MZxRxAdbxTSVL3zRFDFWI)  
&gt;  
&gt;*generated sample demo from this repo :* [*https://www.youtube.com/playlist?list=PLVopZAnUrGWrbIkLGB3bz5nitWThIueS2*](https://www.youtube.com/playlist?list=PLVopZAnUrGWrbIkLGB3bz5nitWThIueS2&amp;fbclid=IwAR1isdEE8bhmdjtyO4HfHTroVMOWnIVOQKU3raGI_rxwfx6yPtztbgVTTWU)

For more details, you can see in README.md   
Thank you :)

&amp;#x200B;

ps) A video below here is generated sample after listened ""Boy with Love (BTS)""

![video](y5afpqt5uig31)",0,1
908,2019-8-15,2019,8,15,11,cqjsti,"[P] Music Transformer ( Huang et al, Google Brain, ICLR2019 ) Re-Implementation ( Tensorflow 2.0 )",https://www.reddit.com/r/MachineLearning/comments/cqjsti/p_music_transformer_huang_et_al_google_brain/,LIKELIH00D,1565835793,"Hi, r/MachineLearning !

As I finished implementation of Google's MusicTransformer,

I want to share it to here.

* github : [https://github.com/jason9693/MusicTransformer-tensorflow2.0](https://l.facebook.com/l.php?u=https%3A%2F%2Fgithub.com%2Fjason9693%2FMusicTransformer-tensorflow2.0%3Ffbclid%3DIwAR0_knevmtIX2M3l8D_lAhriE9QVAU98JpLHl6ANvDnuap7DBUIz-eM4FcM&amp;h=AT2xvBreBR5hpKs1y40q01UQBIzc3Y0oBH3JaRCQGwmN-RMM9n7UYdEJVbOwwWLDFQXrU3eoO95RXzGrAws4-XRtbn9DrRvLMzw5bZf5QRymTosSS5h4LBeg17WoLLZKq3MqMWIvRH0i6wCPoKXcRC6zfLA)
* library : Tensorflow2.0 (beta)
* training env : v100 x 1GPU

&gt;*paper :*  [*https://arxiv.org/abs/1809.04281*](https://l.facebook.com/l.php?u=https%3A%2F%2Farxiv.org%2Fabs%2F1809.04281%3Ffbclid%3DIwAR2reLP6v_tGPfzJygFLyXxilKWxhqlCycDMOaqAvFQWu377ujDqd2saFH0&amp;h=AT2V0ng7JgqR6FKk-6KhEijxOOa7r9AE6UNfKCdimM5H7m38_RUluHN4D2WJMypNf2-1c8dTm3qNDOWyksWXq-V1ZIp_XUMOjXBLT9MekWxWyVCzMjTxpCred2tY6B2bWY3RMVPhlnxRh0iPMowXoFG5KvY)  
&gt;  
&gt;*Google Magenta blog :* [*https://magenta.tensorflow.org/music-transformer*](https://l.facebook.com/l.php?u=https%3A%2F%2Fmagenta.tensorflow.org%2Fmusic-transformer%3Ffbclid%3DIwAR01rOvHHMtKUc0it4rt8cwhtDJ_FL3rQT0UFEfQE-FsQmVRR5xGg03Cb9U&amp;h=AT2WoRR5ssbSiumAq7dJtI12hRd-0w_GZErz6j5H3tdchMEYEnS-p8n8i90giVVmqTuW3Pj3D2De4VTDH4Db1TIfQeS7pZImnnalJ8JFlviz6rgZBOgR7ThpOqgW4lm5iK-hM-MZxRxAdbxTSVL3zRFDFWI)  
&gt;  
&gt;*generated sample demo from this repo :* [*https://www.youtube.com/playlist?list=PLVopZAnUrGWrbIkLGB3bz5nitWThIueS2*](https://www.youtube.com/playlist?list=PLVopZAnUrGWrbIkLGB3bz5nitWThIueS2&amp;fbclid=IwAR1isdEE8bhmdjtyO4HfHTroVMOWnIVOQKU3raGI_rxwfx6yPtztbgVTTWU)

For more details, you can see in README.md

Thank you :)

&amp;#x200B;

ps) A video below here is generated sample after listened ""Boy with Love (BTS)""

![video](1hm19pqtuig31)",0,1
909,2019-8-15,2019,8,15,11,cqk3to,"[P] Music Transformer ( Huang et al, Google Brain, ICLR2019 ) Re-Implementation ( Tensorflow 2.0 )",https://www.reddit.com/r/MachineLearning/comments/cqk3to/p_music_transformer_huang_et_al_google_brain/,LIKELIH00D,1565837398," Hi,[r/MachineLearning](https://www.reddit.com/r/MachineLearning/)!

As I finished implementation of Google's MusicTransformer,

I want to share it to here.  
 

* library : Tensorflow2.0 (beta)  
 
* training env : v100 x 1GPU

&amp;#x200B;

&gt;*paper :* [*https://arxiv.org/abs/1809.04281*](https://arxiv.org/abs/1809.04281)  
&gt;  
&gt;*github* :[https://github.com/jason9693/MusicTransformer-tensorflow2.0](https://github.com/jason9693/MusicTransformer-tensorflow2.0)  
&gt;  
&gt;*Google Magenta blog :* [https://magenta.tensorflow.org/music-transformer](https://magenta.tensorflow.org/music-transformer)

&amp;#x200B;

For more details, you can see in README.md

Thank you :)

&amp;#x200B;

ps) A video below here is generated sample after listened ""Boy with Love (BTS)""

![video](mwnl0yfo0jg31)",6,6
910,2019-8-15,2019,8,15,11,cqk5x7,"I want to get into machine learning/AI as a job, but I don't know what else to start learning in the future",https://www.reddit.com/r/MachineLearning/comments/cqk5x7/i_want_to_get_into_machine_learningai_as_a_job/,20gunasarj,1565837714,"I have been teaching myself learning algorithms to the best of my ability for the past three months, however, I have hit a point of concern. I notice that a lot of learning models are based on the human brain, and I completely understand how it works in code and the mathematics behind these models, but outside the realm of writing code how does this work? Let's say I want to make a robot that is capable of learning, sure I can write code for learning algorithms, however, to make a robot that can learn would I need to have an advanced knowledge of neurology? Let's say I wanted to apply machine learning to predict certain outcomes for some kind of chemical process let's say... would I have to be educated in advanced chemistry to apply it to the real world? To wrap this all up, my main question is to apply these learning algorithms to real world situations, do I have to be educated about these situations, or can one simply treat everything in a quantitative manner regardless of context?",0,1
911,2019-8-15,2019,8,15,11,cqk7o4,[N] Submissions now open for the NeurIPS 2019 MineRL Competition on Sample Efficient RL,https://www.reddit.com/r/MachineLearning/comments/cqk7o4/n_submissions_now_open_for_the_neurips_2019/,MadcowD,1565837988,"Hey friends!

Super excited to announce that after two months of bug squashing &amp; adventures in the Minecraft code base, the MineRL competition is now open for submissions! The general blog post about how to get started is here:

# [http://minerl.io/blog/](http://minerl.io/blog/) 

[Protocol for submission :\) ](https://i.redd.it/9szddj6s1jg31.png)

**Heres how you submit in Round 1**:

1. **Sign up** to join the competition [on the AIcrowd website.](https://www.aicrowd.com/challenges/neurips-2019-minerl-competition)
2. **Clone** the [AIcrowd starter template](https://github.com/minerllabs/competition_submission_starter_template) and start developing your submissions.
3. **Submit** an agent to the leaderboard:

* **Train your agents locally** (or on Azure) in under **8,000,000 samples** over **4 days**. Participants should use hardware **no more powerful than NG6v2 instances on Azure** (6 CPU cores, 112 GiB RAM, 736 GiB SDD, and a NVIDIA P100 GPU.)
* **Push your repository to** [**AIcrowd GitLab**](https://gitlab.aicrowd.com/), which verifies that it can successfully be re-trained by the organizers at the end of Round 1 and then runs the test entrypoint to evaluate the trained agents performance!

Once the full evaluation of the uploaded model/code is done, the participants submission will appear on the leaderboard!",0,2
912,2019-8-15,2019,8,15,12,cqk9tm,Help with question,https://www.reddit.com/r/MachineLearning/comments/cqk9tm/help_with_question/,morguefile,1565838294,"I am looking for some information about using machine learning to help deliver better content to users. Right now I have 200,000 users, 400,000 photos in a database. I was wondering if it is possible to build something that will deliver photo request to the right photographer. Any place I could start looking- or maybe what would be the best SAAS to begin with. Thanks in advance",0,1
913,2019-8-15,2019,8,15,12,cqkgzl,"GitHub - yzhao062/anomaly-detection-resources: Anomaly detection related books, papers, videos, and toolboxes",https://www.reddit.com/r/MachineLearning/comments/cqkgzl/github_yzhao062anomalydetectionresources_anomaly/,juanpabloaj,1565839396,,0,1
914,2019-8-15,2019,8,15,12,cqkmlx,A question from someone trying to self teach ML and AI,https://www.reddit.com/r/MachineLearning/comments/cqkmlx/a_question_from_someone_trying_to_self_teach_ml/,20gunasarj,1565840270,[removed],0,1
915,2019-8-15,2019,8,15,12,cqkq7d,[P] Simple PyTorch implementation of Autoregressive Language Model on Wikipedia text,https://www.reddit.com/r/MachineLearning/comments/cqkq7d/p_simple_pytorch_implementation_of_autoregressive/,lyeoni,1565840853,"A step-by-step tutorial on how to implement and adapt **Autoregressive language model** to Wikipedia text.

&amp;#x200B;

A  pre-trained BERT, XLNET is publicly available ! But, for NLP beginners,  It could be hard to use/adapt after full understanding. For  them, I covered whole, end-to-end implementation process for language  modeling, using unidirectional/bidirectional LSTM network, we already know. 

* **- do not use  torchtext library !**
* **+ include trained model file, training logs**

I hope that this repo can be a good solution for people who want to have their own language model :)

[https://github.com/lyeoni/pretraining-for-language-understanding](https://github.com/lyeoni/pretraining-for-language-understanding)",0,8
916,2019-8-15,2019,8,15,13,cql2yr,Deep learning without back-propagation,https://www.reddit.com/r/MachineLearning/comments/cql2yr/deep_learning_without_backpropagation/,El__Professor,1565842947,,119,228
917,2019-8-15,2019,8,15,13,cql5yb,Are Risk Assessment Algorithms Implicitly Racist?,https://www.reddit.com/r/MachineLearning/comments/cql5yb/are_risk_assessment_algorithms_implicitly_racist/,Blognoggl5,1565843494,,1,1
918,2019-8-15,2019,8,15,14,cqlkgd,Some interesting beginner ML Open Source Projects to contribute to,https://www.reddit.com/r/MachineLearning/comments/cqlkgd/some_interesting_beginner_ml_open_source_projects/,swaroop_2000,1565846135,[removed],0,1
919,2019-8-15,2019,8,15,14,cqllh4,My Youtube Channel,https://www.reddit.com/r/MachineLearning/comments/cqllh4/my_youtube_channel/,nirufeynman,1565846334,[removed],0,1
920,2019-8-15,2019,8,15,14,cqlnbf,"[P] Simple Tensorflow implementation of ""Few-Shot Unsupervised Image-to-Image Translation (a.k.a. FUNIT)"" in ICCV 2019",https://www.reddit.com/r/MachineLearning/comments/cqlnbf/p_simple_tensorflow_implementation_of_fewshot/,taki0112,1565846692,"&amp;#x200B;

![img](cumsu0fjsjg31 ""Our result"")

&amp;#x200B;

![gif](ki5onltlsjg31 ""Paper result"")",1,1
921,2019-8-15,2019,8,15,14,cqlnqr,"[P] Simple Tensorflow implementation of ""Few-Shot Unsupervised Image-to-Image Translation (a.k.a. FUNIT)"" in ICCV 2019",https://www.reddit.com/r/MachineLearning/comments/cqlnqr/p_simple_tensorflow_implementation_of_fewshot/,taki0112,1565846780,"&amp;#x200B;

![img](5k7ibqlusjg31 ""our result"")

![gif](jxp9npjvsjg31 ""paper result"")",0,1
922,2019-8-15,2019,8,15,14,cqlo0n,"[P] Simple Tensorflow implementation of ""Few-Shot Unsupervised Image-to-Image Translation (a.k.a. FUNIT)"" in ICCV 2019",https://www.reddit.com/r/MachineLearning/comments/cqlo0n/p_simple_tensorflow_implementation_of_fewshot/,taki0112,1565846833,"&amp;#x200B;

[our result](https://i.redd.it/fkkx25bysjg31.png)

[paper result](https://i.redd.it/5etdnm7zsjg31.jpg)",1,21
923,2019-8-15,2019,8,15,16,cqmice,[D] Reflinks vs. Symlinks vs. Hard Links: How They Can Help ML Projects,https://www.reddit.com/r/MachineLearning/comments/cqmice/d_reflinks_vs_symlinks_vs_hard_links_how_they_can/,thumbsdrivesmecrazy,1565853019,"In ML projects hard links and symbolic links can help us, when setting up new experiments, to rearrange data files quickly and efficiently. However, with traditional links, we run the risk of polluting the data files with erroneous edits.

The article explain details of using links, some cool new stuff in modern file systems (reflinks), and an example of how [DVC](https://dvc.org) (Data Version Control) tool leverages this for managing ML project datasets and workflow: [Reflinks vs symlinks vs hard links, and how they can help machine learning projects](https://dev.to/robogeek/reflinks-vs-symlinks-vs-hard-links-and-how-they-can-help-machine-learning-projects-1cj4)",0,1
924,2019-8-15,2019,8,15,16,cqmjsg,[P] Pose detection for Google Coral EdgeTPU,https://www.reddit.com/r/MachineLearning/comments/cqmjsg/p_pose_detection_for_google_coral_edgetpu/,makereven,1565853330,"this is the demo for showing how to use the PoseNet model to detect human poses from images and video, such as where someones elbow, shoulder or foot appear in the image. 

example code to shows how to run it on a camera stream.  

code here: [https://github.com/google-coral/project-posenet](https://github.com/google-coral/project-posenet)

&amp;#x200B;

https://i.redd.it/htrtp01zbkg31.gif

&amp;#x200B;

&amp;#x200B;

pppt: before you run this repo, you should get yourself a coral deb board or coral usb : [suggest this site](https://store.gravitylink.com/global/product/DevBoard)",5,6
925,2019-8-15,2019,8,15,16,cqmueb,New To Artificial Intelligence With Python?,https://www.reddit.com/r/MachineLearning/comments/cqmueb/new_to_artificial_intelligence_with_python/,stevie1mat,1565855692,[removed],0,1
926,2019-8-15,2019,8,15,17,cqmw7o,"[R] Reflinks vs symlinks vs hard links, and how they can help machine learning projects",https://www.reddit.com/r/MachineLearning/comments/cqmw7o/r_reflinks_vs_symlinks_vs_hard_links_and_how_they/,okrguy,1565856116,[removed],0,1
927,2019-8-15,2019,8,15,17,cqn5ty,Bitmaker and/or Brainstation bootcamps,https://www.reddit.com/r/MachineLearning/comments/cqn5ty/bitmaker_andor_brainstation_bootcamps/,Halalmomin,1565858323,"Hi there, 


I",0,1
928,2019-8-15,2019,8,15,17,cqn5zg,[R] Thoughts on Mutual Information: More Estimators and Formal Limitations,https://www.reddit.com/r/MachineLearning/comments/cqn5zg/r_thoughts_on_mutual_information_more_estimators/,asobolev,1565858356,"Mutual Information is an important measure of dependence between two variables and is often used in ML for various different reasons (e.g. representation learning). In the last year there were some new results and papers which made me curious about the topic and I wanted to share some of my thoughts.

In the [first post](http://artem.sobolev.name/posts/2019-08-10-thoughts-on-mutual-information-more-estimators.html) I introduce a couple of new estimators based on bounds on the log marginal likelihood. Some of them are very appealing as they give us 1) both lower and upper bounds, 2) a way to make bounds tighter by putting more computation into it.

In the [second post](http://artem.sobolev.name/posts/2019-08-14-thoughts-on-mutual-information-formal-limitations.html) I move to the analysis of (some of) these estimators, as well as address the Formal Limitations paper  a paper that, loosely speaking, forbids good only-samples-based (blackbox) bounds on the Mutual Information. In particular, I show how this issue manifests itself in several widely used blackbox bounds, and then contrast it with bounds  that use some distribution knowledge.

Discussion might not be quite introductory, so I recommend checking out a recent ICML paper [On Variational Bounds of Mutual Information](https://arxiv.org/abs/1905.06922).",12,14
929,2019-8-15,2019,8,15,18,cqnc2t,Intrinsic Curiosity Loss,https://www.reddit.com/r/MachineLearning/comments/cqnc2t/intrinsic_curiosity_loss/,ZeroMaxinumXZ,1565859720,"I've created an intrinsic motivator for my RL agent by taking the loss from my deep-RL agent and giving it as a reward... Any recommendations, papers, or suggestions for what I should do next? The agent performs okay on tasks, but I want to improve it...",0,1
930,2019-8-15,2019,8,15,19,cqnxtm,"[R] why doesn't this Ubuntu vm feel native (quad i7 @3.4 Ghz, 16 GB, Windows 10, nothing else running) in vmware?",https://www.reddit.com/r/MachineLearning/comments/cqnxtm/r_why_doesnt_this_ubuntu_vm_feel_native_quad_i7/,vmcuriouss,1565864316,"Here is the image:

* https://medium.com/@ageitgey/try-deep-learning-in-python-now-with-a-fully-pre-configured-vm-1d97d4c3e9b

It's a a fully configured deep learning VM.  It's about 7.7GB.

I am running on:

* an i7 3770 quad core at 3.4 Ghz (not the i7 3770K which has no VT-d virtualization extensions - this one does.)

* 16 GB of DDR 3 ram that is clocked at 1600 mhz.  I benchmarked it at 19000 MB/sec in Novabench.

* The GPU in the host is 1060 GPU with 6 GB of RAM - but the image file I linked above explains ""Due to licensing and installation complications, theres no GPU acceleration / CUDA support provided. So you dont need an Nvidia GPU to try this out, but it also wont take advantage of a GPU if you have one.""

* windows 10.

* I have an SSD.  It's not the fastest but the write speed is 214 MB/seconds and the read speed is 253 MB/seconds in Novabench.

* I turned the swap off entirely on the host system and have plenty of RAM left.

Without anything else running on the host computer, which is a pretty fresh install, this VM doesn't feel native at all when entering full screen, regardless if I give it 2 GB (default), 4 GB, or even 8 GB of memory. I turned on the virtualization extensions in my bios and in the vmware settings too!   I went from giving 1 CPU to 2 CPU's to the guest.  None of this helps.

why deson't it feel ""native""?  I mean things like moving the mouse around, opening a new firefox windows, etc.  It's ""obviously"" a VM.

Is it not supposed to feel native or buttery smooth?  I'm just at a loss.  Additionally, I did an update from within it and it updated Ubuntu files at 500 KB/second!  My connection is several megabytes per second.

I feel like I must be missing something....but what??",3,0
931,2019-8-15,2019,8,15,21,cqp1jw,pyforest - lazy-import of all popular Python Data Science libraries,https://www.reddit.com/r/MachineLearning/comments/cqp1jw/pyforest_lazyimport_of_all_popular_python_data/,__tobals__,1565871501,[removed],0,1
932,2019-8-15,2019,8,15,21,cqp299,"[P] GPT-2 medium fine-tuned on ""Top Gear"" subtitles, the cutting edge of cocking about!",https://www.reddit.com/r/MachineLearning/comments/cqp299/p_gpt2_medium_finetuned_on_top_gear_subtitles_the/,fsaifdiwq,1565871602,"I wonder why no one else has done it, so I do it myself. The results are impressive if you carefully do the cherry-picking.

It can babble car reviews in Clarkson's style

\&gt; The Audi has more power, more torque, more grip, more braking power, and I agree with you, it's a great car but look at it, look at that beautiful body.

\&gt; ""Am I better off as a Bentley S197 when it's half an inch wider?"" which is what you had to ask yourselves is that's really good car, when you buy it.

\&gt; The first of the two cars we looked at was the BMW M3, and from an engineering perspective, it doesn't look like much to look at but that was because BMWs were made in a similar way to Fords but with more space and better-looking headlights, and that's what really matters. The Porsche did as well.

Sometimes it can generate great introductions for The Stig

\&gt; **Some say he once smeared his milk onto his body,** and that he once had a threesome with a dead pig. All we know is, he's called The Stig.

\&gt; **Some say he has a penis implant.** And that he is the most boring man in the world. All we know is, he's called The Stig

Planning to improve the performance and make a showcase web app, any suggestions are appreciated.",14,62
933,2019-8-15,2019,8,15,21,cqp6h5,Nice self-contained overview of Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/cqp6h5/nice_selfcontained_overview_of_reinforcement/,MasterScrat,1565872282,,0,1
934,2019-8-15,2019,8,15,21,cqp76s,[P] pyforest - lazy-import of all popular Python Data Science libraries,https://www.reddit.com/r/MachineLearning/comments/cqp76s/p_pyforest_lazyimport_of_all_popular_python_data/,__tobals__,1565872386,"**pyforest** lazy-imports all popular Python Data Science and Machine Learning libraries so that they are always there when you need them. If you don't use a library, it won't be imported. When you are done with your script, you can export the Python code for the import statements (because ""explicit is better than implicit"" :)).

https://i.redd.it/egsllhkwwlg31.gif

You can already pip install pyforest. It will be available on conda soon.

Source code: [https://github.com/8080labs/pyforest](https://github.com/8080labs/pyforest)",31,4
935,2019-8-15,2019,8,15,21,cqp81t,The Dawn of AI (Machine Learning Tribes | Deep Learning | What Is Machine Learning),https://www.reddit.com/r/MachineLearning/comments/cqp81t/the_dawn_of_ai_machine_learning_tribes_deep/,enchorb,1565872517,,0,1
936,2019-8-15,2019,8,15,22,cqplit,What library would you recommend for voice recognition?,https://www.reddit.com/r/MachineLearning/comments/cqplit/what_library_would_you_recommend_for_voice/,Th3RealBobLazar,1565874550,[removed],0,1
937,2019-8-15,2019,8,15,23,cqqavi,[D] What are the areas that need to be improved in the field of Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/cqqavi/d_what_are_the_areas_that_need_to_be_improved_in/,_CRypt3R_,1565878051,"Guys, I am planning to get into Research and come out with better models/functionalities etc to solve our day to day problems using machine learning.

What are the various problems that you think can be solved using machine learning and the areas where machine learning could be implemented but hasn't yet due to real world constraints ?

&amp;#x200B;

\[sorry for my bad english\]",7,0
938,2019-8-15,2019,8,15,23,cqqbr8,Best API for end of the day US stock prices?,https://www.reddit.com/r/MachineLearning/comments/cqqbr8/best_api_for_end_of_the_day_us_stock_prices/,young_tamale,1565878175,[removed],0,1
939,2019-8-15,2019,8,15,23,cqqua1,Not Jordan Peterson - Speech synthesis using Google's Tacotron 2 and Nvidia's Waveglow,https://www.reddit.com/r/MachineLearning/comments/cqqua1/not_jordan_peterson_speech_synthesis_using/,satvikpendem,1565880591,,0,1
940,2019-8-16,2019,8,16,0,cqqzov,[P] Not Jordan Peterson - Speech synthesis using Google's Tacotron 2 and Nvidia's Waveglow,https://www.reddit.com/r/MachineLearning/comments/cqqzov/p_not_jordan_peterson_speech_synthesis_using/,satvikpendem,1565881283,"https://www.notjordanpeterson.com

About:

The technology used to generate audio on this site is a combination of two neural network models that were trained using audio data of Dr. Peterson speaking, along with the transcript of his speech. If you don't know who Jordan Peterson is or what his voice sounds like, you can find links to his podcast, lectures, and YouTube videos on his website.

The first model, developed at Google, is called Tacotron 2. It takes as input the text that you type and produces what is known as an audio spectrogram, which represents the amplitudes of the frequencies in an audio signal at each moment in time. The model is trained on text/spectrogram pairs, where the spectrograms are extracted from the source audio data using a Fourier transform.

The second model, developed at NVIDIA, is called Waveglow. It acts as a vocoder, taking in the spectrogram output of Tacotron 2 and producing a full audio waveform, which is what gets encoded into an audio file you can then listen to. The model is trained on spectrogram/waveform pairs of short segments of speech.

The implementations used to create this site were forked from NVIDIA's public implementations of Waveglow and Tacotron 2.",260,42
941,2019-8-16,2019,8,16,0,cqrodz,[D] I created a label embedding technique. Now what?,https://www.reddit.com/r/MachineLearning/comments/cqrodz/d_i_created_a_label_embedding_technique_now_what/,atif_hassan,1565884313,"Hi all!

I am an MS student and was working on an Extreme Multilabel classification task in NLP. My solution required me to generate label embeddings which I did using my own novel method.

The task is yet to be completed but our University has an upcoming conference in which my prof wants me to submit this technique.

So my question is, how should go about writing this paper? From what I know, once a novel embedding method is developed, it is tested on some downstream task like classification, etc. But my technique is for labels. So are there any such tasks for comparing and evaluating label embedding techniques?",1,2
942,2019-8-16,2019,8,16,1,cqrymg,"Facebook, Georgia Tech &amp; OSU ViLBERT Achieves SOTA on Vision-and-Language Tasks",https://www.reddit.com/r/MachineLearning/comments/cqrymg/facebook_georgia_tech_osu_vilbert_achieves_sota/,Yuqing7,1565885551,,0,1
943,2019-8-16,2019,8,16,1,cqs72q,[P] Hands-on with TensorFlow 2.0,https://www.reddit.com/r/MachineLearning/comments/cqs72q/p_handson_with_tensorflow_20/,coffeepants87,1565886570,,0,1
944,2019-8-16,2019,8,16,1,cqsa8c,Question About Standard GAN with Custom Dataset,https://www.reddit.com/r/MachineLearning/comments/cqsa8c/question_about_standard_gan_with_custom_dataset/,cracktoid,1565886965,"I have been playing around with GANs on a custom dataset of spikes that come from a wmpdictionary ( [https://www.mathworks.com/help/wavelet/ref/wmpdictionary.html](https://www.mathworks.com/help/wavelet/ref/wmpdictionary.html) ). Each spike is represented as a 1 x 512 vector. A datasample from this dataset looks like this: 

&amp;#x200B;

![img](kj9bpade2ng31 ""Spike Datasample"")

I created a 100k size dataset of these randomly shifted spikes and fed it to a simple 4 layer symmetric Vanilla GAN  with hopes that the generator would learn to recreate these spikes, but I haven't been able to get the GAN to converge. I've tried most of the training tricks from the gan hacks page ( [https://github.com/soumith/ganhacks](https://github.com/soumith/ganhacks) ), along with some other tricks that I personally have found useful, like spectral norm, PACGAN, etc. I've been able to make the GAN converge on a dataset of sinusoids, but no luck with spikes. I'm wondering if the manifold this data lies on is too small, thus the GAN can't converge, or is it possible that some hyperparameter tuning could fix this? 

&amp;#x200B;

If you do think it's a question of parameter / model tuning, I'd be happy to share in more detail what my model / hyperparameters look like. Either way, let me know your thoughts!",0,1
945,2019-8-16,2019,8,16,2,cqsue0,[R] [1908.03491] Bayesian Inference for Large Scale Image Classification,https://www.reddit.com/r/MachineLearning/comments/cqsue0/r_190803491_bayesian_inference_for_large_scale/,bobchennan,1565889379,,9,37
946,2019-8-16,2019,8,16,2,cqsvxb,AOD-Net Image Dehazing &amp; Neural Style Transfer (Tensorflow Tutorial),https://www.reddit.com/r/MachineLearning/comments/cqsvxb/aodnet_image_dehazing_neural_style_transfer/,tush1995,1565889563,,0,1
947,2019-8-16,2019,8,16,2,cqtarw,What's the state of privacy-preserving ML?,https://www.reddit.com/r/MachineLearning/comments/cqtarw/whats_the_state_of_privacypreserving_ml/,gsjbjt,1565891393,[removed],0,1
948,2019-8-16,2019,8,16,3,cqthhl,A Gentle Introduction to Machine Learning - Yasin Ceran Part (1/5),https://www.reddit.com/r/MachineLearning/comments/cqthhl/a_gentle_introduction_to_machine_learning_yasin/,Magniminda,1565892240,,0,1
949,2019-8-16,2019,8,16,3,cqtxos,Anyone else bothered by memes like this?,https://www.reddit.com/r/MachineLearning/comments/cqtxos/anyone_else_bothered_by_memes_like_this/,Pokyo,1565894152,,0,1
950,2019-8-16,2019,8,16,3,cqu3cn,[D] Selfie2Anime Synthesis using UGATIT,https://www.reddit.com/r/MachineLearning/comments/cqu3cn/d_selfie2anime_synthesis_using_ugatit/,samsamsamrox1212,1565894800,"The author has released the pre-trained models a 50 epoch and a 100 epoch variant, alongside the dataset.

Tool: [https://github.com/taki0112/UGATIT](https://github.com/taki0112/UGATIT) 

I have generated some images using it: [results](https://www.instagram.com/p/B1MdhrYAE-Y/?utm_source=ig_web_button_share_sheet)

Tip: 256x256 images work faster, as well as around 50% of the image being the face helps.

I am not disappointed with the results, what do you think?",24,134
951,2019-8-16,2019,8,16,3,cqu5ap,A Gentle Introduction to Machine Learning - Yasin Ceran Part (1/5),https://www.reddit.com/r/MachineLearning/comments/cqu5ap/a_gentle_introduction_to_machine_learning_yasin/,Magniminda,1565895044,,0,1
952,2019-8-16,2019,8,16,4,cquk8y,How I made a meme face using ML,https://www.reddit.com/r/MachineLearning/comments/cquk8y/how_i_made_a_meme_face_using_ml/,nerdy_wits,1565896786,[removed],0,1
953,2019-8-16,2019,8,16,4,cqunw0,"How to install Anaconda (Python 3.7) , Jupyter Notebook on Windows 10",https://www.reddit.com/r/MachineLearning/comments/cqunw0/how_to_install_anaconda_python_37_jupyter/,sajad-52,1565897233,,0,1
954,2019-8-16,2019,8,16,4,cquqlk,[P] How I made a meme face using ML,https://www.reddit.com/r/MachineLearning/comments/cquqlk/p_how_i_made_a_meme_face_using_ml/,nerdy_wits,1565897573,"video: [https://youtu.be/kwpb4TeZguo](https://youtu.be/kwpb4TeZguo)

repo:  [https://github.com/Suji04/Meme\_with\_ML](https://github.com/Suji04/Meme_with_ML)

This is a fun project I made while learning about object detection with haar cascades. Like, everyone, I too like the thug life memes. But to make a meme face of your friend you have to add the pixelated glasses on your own. I wrote a python program that automatically finds the eyes and places the pixelated glasses on them. I first tried with a haar cascade that can find the eyes but that didn't produce good results after placing the glasses. After that, I tried a face detecting haar cascade. My idea was...I don't have to find the eyes I just have to know where the eyes lie on the face. I divided the face(a rectangle) into 4 parts by 3 horizontal lines. The eyes generally lie on the 2nd part from the top. So the program just places the glasses on the 2nd part of the face. To my surprise, this simple idea produced better results than the former one!",0,0
955,2019-8-16,2019,8,16,4,cqutk9,[D] Self Tuning Networks,https://www.reddit.com/r/MachineLearning/comments/cqutk9/d_self_tuning_networks/,El__Professor,1565897924,"I Read this paper a while ago and got super excited about it, but didn't see it get implemented or talked about as i expected that it would, any ideas why? 

Link to the paper:
https://arxiv.org/abs/1903.03088",11,33
956,2019-8-16,2019,8,16,4,cquxtw,Optimize Your Investments using Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cquxtw/optimize_your_investments_using_machine_learning/,ahershy,1565898438,[removed],0,1
957,2019-8-16,2019,8,16,4,cqv391,Big learning gap to fill,https://www.reddit.com/r/MachineLearning/comments/cqv391/big_learning_gap_to_fill/,Maminizer,1565899111,[removed],0,1
958,2019-8-16,2019,8,16,5,cqv4eg,[D] Detecting when football players take a shot on goal (3DCNN),https://www.reddit.com/r/MachineLearning/comments/cqv4eg/d_detecting_when_football_players_take_a_shot_on/,timscarfe,1565899251,"[www.youtube.com/watch?v=YQ58Sz23PT0](https://www.youtube.com/watch?v=YQ58Sz23PT0)

 Join Tim Scarfe, Karol Zak, Tess Ferrandez and Hamid Reza Vaezi joze on a discussion of 3D CNNs for action detection (shots) in football games. We explain our baseline approach (2D CNN), interview Hamid and learn some background of 3DCNNS and then go through our new code (C3D pretrained on Sports1M dataset) in detail explaining the results. We didn't yet try the I3D but expect it to be even better.

This should be of interest to anyone who is interested in learning the basics of action detection in videos.",0,22
959,2019-8-16,2019,8,16,5,cqvbj5,Activation Functions,https://www.reddit.com/r/MachineLearning/comments/cqvbj5/activation_functions/,inspiredDeveloper,1565900121,"Activation functions can look very different depending on the parameters of the node. 

For visualisation use the [Plotting Activation Functions Tool](https://tech.courses/plotting-tensorflow-js-activation-functions/) . It covers elu, hardSigmoid, linear, relu, relu6, selu, sigmoid, softmax, softplus, softsign, tanh functions.",0,1
960,2019-8-16,2019,8,16,5,cqvm3x,Are you smarter than a London University Student?,https://www.reddit.com/r/MachineLearning/comments/cqvm3x/are_you_smarter_than_a_london_university_student/,DaveatAuquan,1565901394,[removed],0,1
961,2019-8-16,2019,8,16,5,cqvmh3,[R] [1908.03015v1] One Model To Rule Them All; autoencoding as regularization for classification,https://www.reddit.com/r/MachineLearning/comments/cqvmh3/r_190803015v1_one_model_to_rule_them_all/,Isinlor,1565901436,,10,0
962,2019-8-16,2019,8,16,5,cqvo6y,This AI startup claims to automate app making but actually just uses humans :),https://www.reddit.com/r/MachineLearning/comments/cqvo6y/this_ai_startup_claims_to_automate_app_making_but/,kristismart,1565901632,,0,1
963,2019-8-16,2019,8,16,6,cqw58x,Meta-Learning and Semi-Supervised Learning,https://www.reddit.com/r/MachineLearning/comments/cqw58x/metalearning_and_semisupervised_learning/,sota1997,1565903718,[removed],0,1
964,2019-8-16,2019,8,16,6,cqwncq,[D] Installing Mujoco Gym on MacOS,https://www.reddit.com/r/MachineLearning/comments/cqwncq/d_installing_mujoco_gym_on_macos/,rish-16,1565906008,"Hey everyone!

I tried to follow the steps on the repo but I keep getting an error where it says it can't run the wheel and it keeps crashing even with sudo. 

I have a license btw.

Happy to share any other details. 

Thank you!",3,0
965,2019-8-16,2019,8,16,6,cqwqfr,Underrated Machine Learning Advisor at a lesser known university,https://www.reddit.com/r/MachineLearning/comments/cqwqfr/underrated_machine_learning_advisor_at_a_lesser/,shageent-buffaloedu,1565906383,[removed],0,1
966,2019-8-16,2019,8,16,7,cqwsma,[R] Assumed Density Filtering Q-learning,https://www.reddit.com/r/MachineLearning/comments/cqwsma/r_assumed_density_filtering_qlearning/,em-el,1565906654,,0,1
967,2019-8-16,2019,8,16,7,cqwuo1,Which names in ML do you think deserves some spotlight?,https://www.reddit.com/r/MachineLearning/comments/cqwuo1/which_names_in_ml_do_you_think_deserves_some/,flawnson,1565906908,[removed],0,1
968,2019-8-16,2019,8,16,7,cqwyd5,[R] Assumed Density Filtering Q-learning,https://www.reddit.com/r/MachineLearning/comments/cqwyd5/r_assumed_density_filtering_qlearning/,em-el,1565907358,An efficient Bayesian Q-learning method improving the over-optimism and instability issues of the greedy-update of Q-learning.,1,5
969,2019-8-16,2019,8,16,7,cqx3hw,[P] CLI tool to run DL machines on AWS,https://www.reddit.com/r/MachineLearning/comments/cqx3hw/p_cli_tool_to_run_dl_machines_on_aws/,narenst,1565908010,"I created an open source tool to spin up Deep learning EC2 machines with a single command. The goal is to make it easy to use EC2 machines for development without fiddling with the AWS Console, managing SSH keys.

90-second demo of the tool: [https://www.youtube.com/watch?v=lXEeteH3-So](https://www.youtube.com/watch?v=lXEeteH3-So)

Link to the project: [https://github.com/narenst/infinity](https://github.com/narenst/infinity)

It takes less than a minute to set up and use your first Deep Learning machine. I would love to hear your feedback :)",13,49
970,2019-8-16,2019,8,16,7,cqx8bg,Which schools study human cognition to inform and inspire AI?,https://www.reddit.com/r/MachineLearning/comments/cqx8bg/which_schools_study_human_cognition_to_inform_and/,xZoks,1565908603,[removed],0,1
971,2019-8-16,2019,8,16,8,cqxn8o,"r/MachineLearning poster calls Jordan Peterson racist/sexist, epic thread ensues where they can't make up their mind if he is or not",https://www.reddit.com/r/MachineLearning/comments/cqxn8o/rmachinelearning_poster_calls_jordan_peterson/,sinshallah,1565910515,,0,1
972,2019-8-16,2019,8,16,9,cqyqse,[P] Pytorch Implementation of Autoregressive Language Model,https://www.reddit.com/r/MachineLearning/comments/cqyqse/p_pytorch_implementation_of_autoregressive/,lyeoni,1565915785,"A step-by-step tutorial on how to implement and adapt **Autoregressive language model** to Wikipedia text.

A  pre-trained BERT, XLNET is publicly available ! But, for NLP beginners,  It could be hard to use/adapt after full understanding. For  them, I covered whole, end-to-end implementation process for language  modeling, using unidirectional/bidirectional LSTM network, we already know.

* **- do not use  torchtext library !**
* **+ include trained model file, training logs**

I hope that this repo can be a good solution for people who want to have their own language model :)

[https://github.com/lyeoni/pretraining-for-language-understanding](https://github.com/lyeoni/pretraining-for-language-understanding)",0,16
973,2019-8-16,2019,8,16,9,cqywe6,Data Augmentation by Back-translation implementation.,https://www.reddit.com/r/MachineLearning/comments/cqywe6/data_augmentation_by_backtranslation/,thtrieu,1565916606,[removed],0,1
974,2019-8-16,2019,8,16,11,cr019t,"NVIDIA's DALI Library, Image Augmentations | Interview with James Dellinger",https://www.reddit.com/r/MachineLearning/comments/cr019t/nvidias_dali_library_image_augmentations/,init__27,1565922566,[removed],0,1
975,2019-8-16,2019,8,16,13,cr1gk8,Video Compression,https://www.reddit.com/r/MachineLearning/comments/cr1gk8/video_compression/,i_am_styrofoam,1565930557,[removed],0,1
976,2019-8-16,2019,8,16,15,cr2i2b,What kind of computers do most ML engineers use?,https://www.reddit.com/r/MachineLearning/comments/cr2i2b/what_kind_of_computers_do_most_ml_engineers_use/,jazilzaim,1565937042,I know that many use Macbooks or Linux powered machines because of the UNIX-like behavior. But do most people prefer Linux or macOS? I'm definitely keen to know!,0,1
977,2019-8-16,2019,8,16,15,cr2jd6,How can I make my own version of AlphaGo that plays a game of my choice?,https://www.reddit.com/r/MachineLearning/comments/cr2jd6/how_can_i_make_my_own_version_of_alphago_that/,Nick-Conner,1565937279,[removed],1,1
978,2019-8-16,2019,8,16,16,cr2xep,Do the speech recognition engines use speech enhancements processes on the audio they receive?,https://www.reddit.com/r/MachineLearning/comments/cr2xep/do_the_speech_recognition_engines_use_speech/,adj0nt47,1565939907,[removed],0,1
979,2019-8-16,2019,8,16,16,cr2zaa,What ML Model should I use for this?,https://www.reddit.com/r/MachineLearning/comments/cr2zaa/what_ml_model_should_i_use_for_this/,JJsRedditla,1565940258,[removed],0,1
980,2019-8-16,2019,8,16,16,cr33xt,getting started with NLP/ text analysis,https://www.reddit.com/r/MachineLearning/comments/cr33xt/getting_started_with_nlp_text_analysis/,MrHktrioot,1565941225,[removed],0,1
981,2019-8-16,2019,8,16,17,cr3gka,High level guide to ML,https://www.reddit.com/r/MachineLearning/comments/cr3gka/high_level_guide_to_ml/,ComputerGuyChris,1565943871,[removed],0,1
982,2019-8-16,2019,8,16,18,cr3uck,Data Analytics Course in Pune,https://www.reddit.com/r/MachineLearning/comments/cr3uck/data_analytics_course_in_pune/,dwivediabhinav,1565946672,,0,1
983,2019-8-16,2019,8,16,18,cr3vmn,Clustering Time Series Data,https://www.reddit.com/r/MachineLearning/comments/cr3vmn/clustering_time_series_data/,DipanshuShady,1565946920,[removed],0,1
984,2019-8-16,2019,8,16,18,cr45lt,[Research] FoveaBox code is available,https://www.reddit.com/r/MachineLearning/comments/cr45lt/research_foveabox_code_is_available/,taokongcn,1565948940,,1,1
985,2019-8-16,2019,8,16,18,cr47qi,[R] FoveaBox object detection code is available,https://www.reddit.com/r/MachineLearning/comments/cr47qi/r_foveabox_object_detection_code_is_available/,taokongcn,1565949355,"[https://github.com/taokong/FoveaBox](https://github.com/taokong/FoveaBox)

FoveaBox is an accurate, flexible and completely anchor-free object detection system for object detection framework, as presented in our paper [https://arxiv.org/abs/1904.03797](https://arxiv.org/abs/1904.03797): Different from previous anchor-based methods, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object.",15,87
986,2019-8-16,2019,8,16,18,cr48xx,Gaussian Mixture Model with Case Study - A Survival Guide for Beginners - DataFlair,https://www.reddit.com/r/MachineLearning/comments/cr48xx/gaussian_mixture_model_with_case_study_a_survival/,Aakashdata,1565949582,,0,1
987,2019-8-16,2019,8,16,19,cr4huz,Is Machine Learning and Deep Learning different?,https://www.reddit.com/r/MachineLearning/comments/cr4huz/is_machine_learning_and_deep_learning_different/,AnyBridge6,1565951225,[removed],0,1
988,2019-8-16,2019,8,16,19,cr4j6i,Terms to remember while building a Machine e model,https://www.reddit.com/r/MachineLearning/comments/cr4j6i/terms_to_remember_while_building_a_machine_e_model/,mlait1908,1565951476,,0,1
989,2019-8-16,2019,8,16,19,cr4osw, Welcome to the DeepMind podcast,https://www.reddit.com/r/MachineLearning/comments/cr4osw/welcome_to_the_deepmind_podcast/,sjoerdapp,1565952550,,0,1
990,2019-8-16,2019,8,16,20,cr4v3o,Probabilistic clustering with class information,https://www.reddit.com/r/MachineLearning/comments/cr4v3o/probabilistic_clustering_with_class_information/,paland3,1565953685,[removed],0,1
991,2019-8-16,2019,8,16,20,cr4y3f,A Step-by-Step Guide to Future Proof Your IT Career!,https://www.reddit.com/r/MachineLearning/comments/cr4y3f/a_stepbystep_guide_to_future_proof_your_it_career/,Albertchristopher,1565954219,,0,1
992,2019-8-16,2019,8,16,20,cr52gv,[P] GPT-2 small fine-tuned on The Stig intros from Top Gear.,https://www.reddit.com/r/MachineLearning/comments/cr52gv/p_gpt2_small_finetuned_on_the_stig_intros_from/,lilsmacky,1565954981,"I was struck by how funny the Stig intros generated from [this project](https://www.reddit.com/r/MachineLearning/comments/cqp299/p_gpt2_medium_finetuned_on_top_gear_subtitles_the/) by [u/fsaifdiwq](https://www.reddit.com/user/fsaifdiwq/) was yesterday. So I wanted to see if I can get a GPT-2 to only do Stig intros. Due to the size of the dataset I can tell that it shows signs of overfitting, but many results are still new and quite fun.

&amp;#x200B;

&gt;Some say that he is confused by clouds and thinks the wind is behind the clouds. All we know is hes called the Stig.  
&gt;  
&gt;Some say he has the worlds largest collection of toe nails that he has harvested off his bee hive and that he has a full body tattoo that could make a black hole. All we know is hes called the Stig.  
&gt;  
&gt;Some say that he once slipped on an orange peel, some say that he ate a porcelain mug, without any tea. All we know is hes called the Stig.  
&gt;  
&gt;Some say he tastes exactly the same as leprechauns, and that his favourite pastime is crushing other peoples skulls with his glass of milk. All we know is hes called the Stig.  
&gt;  
&gt;Some say he won the Nobel Peace Prize, and that hes convinced Jesus and The Lion King are The Lion King 1 and 2. All we know is hes called the Stig.  
&gt;  
&gt;Some say, ''There is a white paper on the problem with trees being bad for him but not for his skin''. Others say his eyelids are white, and that in 2007 he was convicted of the theft of The Lamps. All we know is hes called the Stig.  
&gt;  
&gt;Some say he once punched a lady in the face and he was confused until he realised people arent that good at sitting still; and that his favorite language is Chinese. All we know is hes called the Stig.

And you can begin with a topic yourself and see how it finishes the introduction. Here I made it mention reddit:

&amp;#x200B;

&gt;Some say he uses reddit to his own detriment, by posting anti-Donald tweeters. All we know is hes called the Stig.  
&gt;  
&gt;Some say he uses reddit to look at children's books and that he is lucky starstruck with Justin Bieber. All we know is hes called the Stig.  
&gt;  
&gt;Some say he uses reddit to look at nipples and that when he watches children on cell phone video he sees little to no reason to change them. All we know is hes called the Stig.  
&gt;  
&gt;Some say reddit is his new bathroom partner, and that when he goes to the beach to have a bonfire. All we know is hes called the Stig.  
&gt;  
&gt;Some say reddit is his social media following and that has not bought the entire worlds supply of earthworms for his new home planet on pluto just yet. All we know is hes called the Stig.",10,164
993,2019-8-16,2019,8,16,20,cr55c1,Insanely Clever Upgrades to Make Your Home 6,https://www.reddit.com/r/MachineLearning/comments/cr55c1/insanely_clever_upgrades_to_make_your_home_6/,GoGadegets,1565955462,,0,1
994,2019-8-16,2019,8,16,20,cr570b,Deep learning model optimization...,https://www.reddit.com/r/MachineLearning/comments/cr570b/deep_learning_model_optimization/,aminehy,1565955759,[removed],0,1
995,2019-8-16,2019,8,16,21,cr5g26,Screen Printing Machine,https://www.reddit.com/r/MachineLearning/comments/cr5g26/screen_printing_machine/,rapidtag,1565957219,,0,1
996,2019-8-16,2019,8,16,22,cr61it,Curriculum for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cr61it/curriculum_for_machine_learning/,pbharadwaj7,1565960491,[removed],0,1
997,2019-8-16,2019,8,16,22,cr66nm,What is the difference between permutation-equivariant and translation-equivariant in computer vision?,https://www.reddit.com/r/MachineLearning/comments/cr66nm/what_is_the_difference_between/,hitaho,1565961191,[removed],0,1
998,2019-8-16,2019,8,16,22,cr6czf,[P] Pytorch library of NLP pre-trained models has a new model to offer: RoBERTa,https://www.reddit.com/r/MachineLearning/comments/cr6czf/p_pytorch_library_of_nlp_pretrained_models_has_a/,jikkii,1565962077,"Huggingface has released a new version of their open-source library of pre-trained transformer models for NLP: *pytorch-transformers* 1.1.0.

On top of the already integrated architectures: Google's **BERT**, OpenAI's **GPT** &amp; **GPT-2**, Google/CMU's **Transformer-XL** &amp; **XLNet** and Facebook's **XLM**, they have added Facebook's **RoBERTa**, which has a slightly different pre-training approach than BERT while keeping the original model architecture.

The **RoBERTa** model gets SOTA results on SuperGLUE.

&amp;#x200B;

Install: `pip install pytorch-transformers`

Quickstart: [https://huggingface.co/pytorch-transformers/quickstart.html](https://huggingface.co/pytorch-transformers/quickstart.html)

Release notes: [https://github.com/huggingface/pytorch-transformers/releases/tag/1.1.0](https://github.com/huggingface/pytorch-transformers/releases/tag/1.1.0)

Documentation: [https://huggingface.co/pytorch-transformers/](https://huggingface.co/pytorch-transformers/)",2,63
999,2019-8-16,2019,8,16,22,cr6ehc,Help understanding Github project installation instructions,https://www.reddit.com/r/MachineLearning/comments/cr6ehc/help_understanding_github_project_installation/,JH1635,1565962286,[removed],0,1
1000,2019-8-16,2019,8,16,22,cr6mkb,[Testing] I did some naive test to bencmark my Models and it worked out very well,https://www.reddit.com/r/MachineLearning/comments/cr6mkb/testing_i_did_some_naive_test_to_bencmark_my/,WideWorry,1565963409,,0,1
1001,2019-8-16,2019,8,16,22,cr6ot2,Poker platform for bots (AI/ML bots).,https://www.reddit.com/r/MachineLearning/comments/cr6ot2/poker_platform_for_bots_aiml_bots/,maximedb,1565963713,,0,1
1002,2019-8-16,2019,8,16,23,cr6zyt,Israeli AI Research Company Led by Stanford Professor and MobileEye CEO Introduces Model to Solve Text Ambiguity,https://www.reddit.com/r/MachineLearning/comments/cr6zyt/israeli_ai_research_company_led_by_stanford/,Yuqing7,1565965149,,0,1
1003,2019-8-16,2019,8,16,23,cr76cn,A Tour of Convolutional Networks Guided by Linear Interpreters,https://www.reddit.com/r/MachineLearning/comments/cr76cn/a_tour_of_convolutional_networks_guided_by_linear/,pnavarre,1565965970,,1,21
1004,2019-8-16,2019,8,16,23,cr7b9l,Collect Training Data Using Amazon SageMaker Ground Truth &amp; Figure Eight,https://www.reddit.com/r/MachineLearning/comments/cr7b9l/collect_training_data_using_amazon_sagemaker/,htebsile,1565966580,,0,1
1005,2019-8-16,2019,8,16,23,cr7grs,Deep learning solutions start-up a good idea?,https://www.reddit.com/r/MachineLearning/comments/cr7grs/deep_learning_solutions_startup_a_good_idea/,bhushan_b_patil,1565967267,[removed],0,1
1006,2019-8-17,2019,8,17,0,cr7ko8,"[D] New State of the Art AI Optimizer: Rectified Adam (RAdam). Improve your AI accuracy instantly versus Adam, and why it works.",https://www.reddit.com/r/MachineLearning/comments/cr7ko8/d_new_state_of_the_art_ai_optimizer_rectified/,bobchennan,1565967758,,1,2
1007,2019-8-17,2019,8,17,0,cr7zoa,"Automated Machine Learning (AutoML): Methods, Systems, Challenges - Open-Access (Free) Textbook",https://www.reddit.com/r/MachineLearning/comments/cr7zoa/automated_machine_learning_automl_methods_systems/,ai-lover,1565969545,[removed],0,1
1008,2019-8-17,2019,8,17,0,cr892w,DDPG - Batch Normalization Implementation,https://www.reddit.com/r/MachineLearning/comments/cr892w/ddpg_batch_normalization_implementation/,Jandevries101,1565970640,,0,1
1009,2019-8-17,2019,8,17,0,cr89sb,[D] Why are Google's leaked ML fairness documents so bad??,https://www.reddit.com/r/MachineLearning/comments/cr89sb/d_why_are_googles_leaked_ml_fairness_documents_so/,Runninganddogs979,1565970727,"''' DISCLAIMER I have not read all of them but I have read most '''

I've been looking through the leaked documents and the ML fairness seems to actually address real problems facing bias in ML, why is it bad??",309,78
1010,2019-8-17,2019,8,17,0,cr8bts,[Q] GloVe and negative sampling,https://www.reddit.com/r/MachineLearning/comments/cr8bts/q_glove_and_negative_sampling/,BeggarInSpain,1565970997,[removed],0,1
1011,2019-8-17,2019,8,17,3,cra47j,Joint Speech Recognition and Speaker Diarization via Sequence Transduction,https://www.reddit.com/r/MachineLearning/comments/cra47j/joint_speech_recognition_and_speaker_diarization/,sjoerdapp,1565978719,,0,1
1012,2019-8-17,2019,8,17,3,crakhv,"[N] HGX-2 Deep Learning Benchmarks: The 81,920 CUDA Core Behemoth GPU Server",https://www.reddit.com/r/MachineLearning/comments/crakhv/n_hgx2_deep_learning_benchmarks_the_81920_cuda/,exxact-jm,1565980771,"Deep learning benchmarks for TensorFlow on Exxact TensorEX HGX-2 Server. 

Original Post from Exxact [Here](https://blog.exxactcorp.com/hgx2-benchmarks-for-deep-learning-in-tensorflow-16x-v100-exxact-tensorex-server/)

**Notable GPU Server Features**

* 16x NVIDIA Tesla V100 SXM3
* 81,920 NVIDIA CUDA Cores
* 10,240 NVIDIA Tensor Cores
* .5TB Total GPU Memory
* NVSwitch powered by NVLink 2.4TB/sec aggregate speed

[Source: blog.exxactcorp.com](https://i.redd.it/43a9oi0asug31.png)

[Source: blog.exxactcorp.com](https://i.redd.it/al4adj0asug31.png)

Tests were run on ResNet-50, ResNet-152, Inception V3, VGG-16. Also compared FP16 to FP32 performance, and used batch size of 256 (except for ResNet152 FP32, the batch size was 64).  Same tests run using 1,2,4,8 and 16 GPU configurations. All benchmarks were done using vanilla TensorFlow settings for FP16 and FP32.

For the full write-up + tables and numbers visit:  [https://blog.exxactcorp.com/hgx2-benchmarks-for-deep-learning-in-tensorflow-16x-v100-exxact-tensorex-server/](https://blog.exxactcorp.com/hgx2-benchmarks-for-deep-learning-in-tensorflow-16x-v100-exxact-tensorex-server/)",40,101
1013,2019-8-17,2019,8,17,3,craooh,Decaying [;\epsilon;] greedy algorithm regret analysis,https://www.reddit.com/r/MachineLearning/comments/craooh/decaying_epsilon_greedy_algorithm_regret_analysis/,shew-p,1565981309,[removed],0,1
1014,2019-8-17,2019,8,17,3,crapw2,[P]  Dual pane File Manager for Training Data: old school meets AI in Supervisely,https://www.reddit.com/r/MachineLearning/comments/crapw2/p_dual_pane_file_manager_for_training_data_old/,tdionis,1565981455,,0,1
1015,2019-8-17,2019,8,17,4,crb42h,DeepMind Bsuite Evaluates Reinforcement Learning Agents,https://www.reddit.com/r/MachineLearning/comments/crb42h/deepmind_bsuite_evaluates_reinforcement_learning/,Yuqing7,1565983215,,0,1
1016,2019-8-17,2019,8,17,4,crbeje,[R] ABD-Net Person Re-ID code is available,https://www.reddit.com/r/MachineLearning/comments/crbeje/r_abdnet_person_reid_code_is_available/,yang-explore,1565984550,"https://github.com/TAMU-VITA/ABD-Net

Attention mechanism has been shown to be effective for person re-identification (Re-ID). However, the learned attentive feature embeddings which are often not naturally diverse nor uncorrelated, will compromise the retrieval performance based on the Euclidean distance. We advocate that enforcing diversity could greatly complement the power of attention. To this end, we propose an Attentive but Diverse Network (ABD-Net), which seamlessly integrates attention modules and diversity regularization throughout the entire network, to learn features that are representative, robust, and more discriminative.",1,1
1017,2019-8-17,2019,8,17,5,crbz05,[D] Can this project work?,https://www.reddit.com/r/MachineLearning/comments/crbz05/d_can_this_project_work/,Mjjjokes,1565987164,[removed],0,1
1018,2019-8-17,2019,8,17,5,crc5pd,What do you feel is the best way to explain machine learning to someone who knows nothing about it?,https://www.reddit.com/r/MachineLearning/comments/crc5pd/what_do_you_feel_is_the_best_way_to_explain/,lurk--,1565988029,[removed],0,1
1019,2019-8-17,2019,8,17,5,crc96g,What information is encoded in embedding vector lengths?,https://www.reddit.com/r/MachineLearning/comments/crc96g/what_information_is_encoded_in_embedding_vector/,bwllc,1565988483,"I have started to investigate word2vec and related embedding strategies.  The word2vec training loss is a function of cosine distance and not Euclidean distance.   I have been reading various comments which suggest that Euclidean distances aren't very useful in high-dimensional spaces.  That makes me wonder why k-d trees are even a thing.

I am interested in examining neighborhoods in my trained embeddings.  It appears that normalizing all the embedding vectors to a unit length and then using the Euclidean distances is a good stand-in for measuring cosine similarities directly.  I am looking for neighborhoods this way because scikit-learn has the **KDTree** class which can accept my embedding as an input (normalized or not, as I choose), and it has a neighbor search method.

This makes me wonder: what information do the LENGTHS of the vectors in a trained embedding contain?  I am throwing that information away when I normalize.  If there's NO meaningful information in the lengths, why don't we just normalize all the vectors in an embedding at all times -- during training as well as when we search for neighbors?",0,1
1020,2019-8-17,2019,8,17,6,crchje,"[D] Thoughts on RAdam, a new optimizer? ""We propose RAdam, introducing a term to rectify the variance of the adaptive learning rate""",https://www.reddit.com/r/MachineLearning/comments/crchje/d_thoughts_on_radam_a_new_optimizer_we_propose/,permalip,1565989561,,1,1
1021,2019-8-17,2019,8,17,6,crcz0e,Is AI a career risk?,https://www.reddit.com/r/MachineLearning/comments/crcz0e/is_ai_a_career_risk/,beerato,1565991818,[removed],1,1
1022,2019-8-17,2019,8,17,7,crdal7,'Black Box' vs. Interpretable Model Performance,https://www.reddit.com/r/MachineLearning/comments/crdal7/black_box_vs_interpretable_model_performance/,TechVersion2,1565993316,[removed],0,1
1023,2019-8-17,2019,8,17,7,crdiz2,[P] Requests for Articles: a Call on AI Experts and Communicators,https://www.reddit.com/r/MachineLearning/comments/crdiz2/p_requests_for_articles_a_call_on_ai_experts_and/,regalalgorithm,1565994386,,1,1
1024,2019-8-17,2019,8,17,7,crdqct,NLP/Text Mining/ Deep Learning Trending Research topics,https://www.reddit.com/r/MachineLearning/comments/crdqct/nlptext_mining_deep_learning_trending_research/,Strange_Flatworm,1565995371,[removed],0,1
1025,2019-8-17,2019,8,17,8,crdzhs,creating AI,https://www.reddit.com/r/MachineLearning/comments/crdzhs/creating_ai/,Acelnnthegravity,1565996602,[removed],0,1
1026,2019-8-17,2019,8,17,8,cre2nj,Fundamentals &amp; Terms of Neural Nets,https://www.reddit.com/r/MachineLearning/comments/cre2nj/fundamentals_terms_of_neural_nets/,silizannek,1565997053,,0,1
1027,2019-8-17,2019,8,17,10,crfu1j,So i created an AI for the worlds hardest flappy bird level,https://www.reddit.com/r/MachineLearning/comments/crfu1j/so_i_created_an_ai_for_the_worlds_hardest_flappy/,WalterEhren,1566006392,,0,0
1028,2019-8-17,2019,8,17,11,crg1tl,[D] cardinality invariance?,https://www.reddit.com/r/MachineLearning/comments/crg1tl/d_cardinality_invariance/,thats_DR_chalupa_2u,1566007603,"My understanding is that a common way of dealing with a varying number of predictors is through setting making an assumption on the maximum number of predictors, and setting irrelevant components to 0.

For example: in a social media networking application, each vertex in the graph could potentially have a set of connecting edges with a distinct cardinality.

I've been studying some interesting work on neural networks for equivariance, permutation invariance, and predicting cardinalities, but I wonder: is there any existing work on models learning to be invariant to the cardinality of the set of predictors?

For example (going back to the social media network graphs): could we similarly predict the role of an arbitrary node in a community subgraph with 10 nodes, as we would in a subgraph with 10,000?",0,1
1029,2019-8-17,2019,8,17,12,crh5om,[D] Why have we not seen equivalent success in deep learning based image registration?,https://www.reddit.com/r/MachineLearning/comments/crh5om/d_why_have_we_not_seen_equivalent_success_in_deep/,deep-yearning,1566013932,"It seems that other computer vision tasks such as classification, segmentation and synthesis have seen huge advances in accuracy thanks to CNNs, but there seems to be no equivalent advance in image registration. I tried searching for advances in image registration, but it seems that researchers still use 'classical' image registration techniques like mutual information, cross-correlation, etc. Even though there are DL image registration research papers, they are not well adopted in the community.

&amp;#x200B;

Fundamentally, is there a reason why this task is more complex that the aforementioned ones?",25,35
1030,2019-8-17,2019,8,17,14,crhuai,Implementing Progressive Growing GAN Models in Keras,https://www.reddit.com/r/MachineLearning/comments/crhuai/implementing_progressive_growing_gan_models_in/,iamart_intelligence,1566018004,,0,1
1031,2019-8-17,2019,8,17,14,cri1ep,Autonomous Navigation in Unconstrained Environments,https://www.reddit.com/r/MachineLearning/comments/cri1ep/autonomous_navigation_in_unconstrained/,vector_machines,1566019278,"While several datasets for autonomous navigation have become available in recent years, they have tended to focus on structured driving environments. This usually corresponds to well-delineated infrastructure such as lanes, a small number of well-defined categories for traffic participants, low variation in an object or background appearance and strong adherence to traffic rules. 

I recently worked with IDD, dataset collected from India. It's relatively more challenging than other autonomous navigation-related datasets (such as Berkeley deep drive or cityscapes).

I'm releasing the code for this work, feel free to use it for your projects or research. 

Github: [https://github.com/prajjwal1/autonomous-object-detection](https://github.com/prajjwal1/autonomous-object-detection)

Dataset: [https://idd.insaan.iiit.ac.in/](https://idd.insaan.iiit.ac.in/)",0,1
1032,2019-8-17,2019,8,17,14,cri1ya,[R] Autonomous Navigation in Unconstrained Environments,https://www.reddit.com/r/MachineLearning/comments/cri1ya/r_autonomous_navigation_in_unconstrained/,vector_machines,1566019372,"While several datasets for autonomous navigation have become available in recent years, they have tended to focus on structured driving environments. This usually corresponds to well-delineated infrastructure such as lanes, a small number of well-defined categories for traffic participants, low variation in an object or background appearance and strong adherence to traffic rules. 

I recently worked with IDD, dataset collected from India. It's relatively more challenging than other autonomous navigation-related datasets (such as Berkeley deep drive or cityscapes).

I'm releasing the code for this work, feel free to use it for your projects or research. 

Github: [https://github.com/prajjwal1/autonomous-object-detection](https://github.com/prajjwal1/autonomous-object-detection)

Dataset: [https://idd.insaan.iiit.ac.in/](https://idd.insaan.iiit.ac.in/)",3,10
1033,2019-8-17,2019,8,17,17,crjn5s,Applied Machine Learning course downloading link,https://www.reddit.com/r/MachineLearning/comments/crjn5s/applied_machine_learning_course_downloading_link/,latcher_cx,1566031039,,0,1
1034,2019-8-17,2019,8,17,18,crjtfq,Accelerated CNN Training Through Gradient Approximation,https://www.reddit.com/r/MachineLearning/comments/crjtfq/accelerated_cnn_training_through_gradient/,raindrop_code,1566032484,,0,1
1035,2019-8-17,2019,8,17,18,crjxlc,Good Toy Dataset for speaker diarisation,https://www.reddit.com/r/MachineLearning/comments/crjxlc/good_toy_dataset_for_speaker_diarisation/,fabmilo,1566033336,[removed],0,1
1036,2019-8-17,2019,8,17,18,crk0vg,Accelerated CNN Training Through Gradient Approximation,https://www.reddit.com/r/MachineLearning/comments/crk0vg/accelerated_cnn_training_through_gradient/,raindrop_code,1566034031,,15,110
1037,2019-8-17,2019,8,17,21,crlagu,[D] Unstable performance during parameter search (Keras),https://www.reddit.com/r/MachineLearning/comments/crlagu/d_unstable_performance_during_parameter_search/,Zman420,1566043286,"Hi all,

I was hoping we could discuss the plot below:

https://imgur.com/TMK9RgD

That plot comes from a parameter search using Keras/Tensorflow for a binary classification problem with an unbalanced class distribution (as you can tell from the acc plot, the ratio is about 5:1 negative to positive).

The metric that I am most interested in is Precision, and as you can see in this example it is very unstable, bouncing around wildly between epochs - which obviously doesn't lend itself to being a good/stable model.

Whilst there is a little overfitting, there doesn't seem to be too much and I can confirm that the data itself is all properly scaled and normalised.  

Although the plot scale is a bit large (sorry) to tell properly, I think what we'd find is that Recall fluctuates in unison with Precision.  As Recall bounces upwards, I'd expect Precision to take a dive downwards.

I can't post the exact model because it's a parameter search with a wide range of possible configurations, but I'm optimising across a range of network depths, widths, dropouts, shapes, learning rate, etc. I'm using binary_crossentropy as the loss, Elu activations, and Nadam optimizer - though I've tried a various others with similar results.

What would be your suggestions for creating a more stable model? 

At the moment, the class_weight is set to 0:1, 1:1. I think upping the positive class ratio would somewhat stabilise the model (by increasing recall), but I'm shooting to have a high precision and accepting that my recall will be the trade-off and be somewhat low.  For example, I'd be happy with 57% precision at 5% recall. 
In fact - that's the exact result I got from a previous parameter search, but it didn't generalise well to the blind test set, and I'm suspecting that the cause was the unstable epoch-to-epoch precision we're seeing in this plot (though I can only see the plots for the ""current"" model being generated, so by the end of the many-hour parameter search all I have is a csv of the final values, with no plots to go along with them).",5,0
1038,2019-8-17,2019,8,17,21,crlil8,"I've reproduced 130+ research papers about ""predicting the stock market"", coded them from scratch and recorded the results. Here's what I've learnt.",https://www.reddit.com/r/MachineLearning/comments/crlil8/ive_reproduced_130_research_papers_about/,willcallsaul,1566044744,,0,1
1039,2019-8-17,2019,8,17,21,crlj6x,[Q] What't going on with PyPlot and ChartStudio?,https://www.reddit.com/r/MachineLearning/comments/crlj6x/q_whatt_going_on_with_pyplot_and_chartstudio/,BeggarInSpain,1566044853,[removed],0,1
1040,2019-8-17,2019,8,17,21,crlq2p,"We all hope quantum computing will really boost ML someday, but the actual numbers are staggering.",https://www.reddit.com/r/MachineLearning/comments/crlq2p/we_all_hope_quantum_computing_will_really_boost/,Agent_ANAKIN,1566046096,,0,1
1041,2019-8-17,2019,8,17,23,crmm3a,"[P] Implemented MPPI (Model Predictive Path Integral) in Python with OpenAI Gym pendulum environment (paper: ""Information Theoretic MPC for Model-Based Reinforcement Learning"", Williams et al., 2017)",https://www.reddit.com/r/MachineLearning/comments/crmm3a/p_implemented_mppi_model_predictive_path_integral/,whiletrue2,1566051057,,0,1
1042,2019-8-17,2019,8,17,23,crmm8u,Is a position as a ML-consultant a good career start?,https://www.reddit.com/r/MachineLearning/comments/crmm8u/is_a_position_as_a_mlconsultant_a_good_career/,DrudenSoap,1566051079,[removed],0,1
1043,2019-8-17,2019,8,17,23,crmqnd,[P] Selfie2Anime using UGATIT - Web Application,https://www.reddit.com/r/MachineLearning/comments/crmqnd/p_selfie2anime_using_ugatit_web_application/,t04glovern,1566051717,"Hi all,

With the release of the [UGATIT](https://github.com/taki0112/UGATIT) pre-trained models we decided to throw together a little web application to handle. 

https://i.redd.it/76z7s57tp0h31.png

We've forked the original repo and added a flask application that can be used for local inference (use the \`web\` flag in phase)

* **website**: [selfie2anime.com](https://selfie2anime.com/)
* **website \[code\]**: [https://github.com/SilentByte/selfie2anime-site](https://github.com/SilentByte/selfie2anime-site)
* **backend \[original\]**: [https://github.com/taki0112/UGATIT](https://github.com/taki0112/UGATIT)
* **backend \[fork\]**: [https://github.com/t04glovern/UGATIT](https://github.com/t04glovern/UGATIT)

So much thanks to [Junho Kim](http://bit.ly/jhkim_ai), Minjae Kim, Hyeonwoo Kang, Kwanghee Lee, along with all the awesome people in the [pre-trained model thread](https://github.com/taki0112/UGATIT/issues/5) for the conversations",0,1
1044,2019-8-17,2019,8,17,23,crmv3p,"Breast cancer diagnosis with neural networks, implemented with Keras and written in Python",https://www.reddit.com/r/MachineLearning/comments/crmv3p/breast_cancer_diagnosis_with_neural_networks/,antaloaalonso,1566052400,,0,0
1045,2019-8-17,2019,8,17,23,crmvus,Join an AI Challenge to Predict Forced Displacement and Violent Conflicts in Somalia,https://www.reddit.com/r/MachineLearning/comments/crmvus/join_an_ai_challenge_to_predict_forced/,Lordobba,1566052522,[removed],0,1
1046,2019-8-17,2019,8,17,23,crmygd,Deep Learning Projects?,https://www.reddit.com/r/MachineLearning/comments/crmygd/deep_learning_projects/,prabeshpaudel,1566052918,[removed],0,1
1047,2019-8-18,2019,8,18,0,crnszb,Are you looking for a free course in data science? here is a free course,https://www.reddit.com/r/MachineLearning/comments/crnszb/are_you_looking_for_a_free_course_in_data_science/,sajad-52,1566056781,,0,1
1048,2019-8-18,2019,8,18,1,croezk,Continuous Business Form Printing Machinery,https://www.reddit.com/r/MachineLearning/comments/croezk/continuous_business_form_printing_machinery/,Shoaibkaiser,1566059623,,1,1
1049,2019-8-18,2019,8,18,2,croxue,Creating a Comment Box with ASP.NET,https://www.reddit.com/r/MachineLearning/comments/croxue/creating_a_comment_box_with_aspnet/,hossainshahana,1566062222,[removed],0,1
1050,2019-8-18,2019,8,18,2,crp3qj,Where to find a data set of house pictures,https://www.reddit.com/r/MachineLearning/comments/crp3qj/where_to_find_a_data_set_of_house_pictures/,glaey,1566062786,[removed],0,1
1051,2019-8-18,2019,8,18,2,crpjfb,AlphaGo - glory of the past?,https://www.reddit.com/r/MachineLearning/comments/crpjfb/alphago_glory_of_the_past/,uwulove03,1566064675,[removed],0,1
1052,2019-8-18,2019,8,18,2,crpjqj,Linux Laptop for ML,https://www.reddit.com/r/MachineLearning/comments/crpjqj/linux_laptop_for_ml/,DaBobcat,1566064710,[removed],0,1
1053,2019-8-18,2019,8,18,3,crplri,AI generated voices of famous people,https://www.reddit.com/r/MachineLearning/comments/crplri/ai_generated_voices_of_famous_people/,Struuff,1566064953,"Now, I don't know if this is the right place to ask but machine learning is the first place that came in my mind when thinking about it. If there is a better place to ask this question then please let me know.

So I am looking for websites which, ideally for free, allow you to type in text and then it will be spoken by generated voice of a famous persona. So far I've found those:

[https://www.notjordanpeterson.com/](https://www.notjordanpeterson.com/) \- Extremely realistic voice of Jordan Peterson. 

[http://talkobamato.me/](http://talkobamato.me/) \- Now, this is not exactly what I am  looking for but just to give you an idea of what I am looking for. 

It can be a politician, actor, I don't care really.

Just for the record it is not to manipulate anything or use it for some political purposes. It's for my dumb personal project I thought of few hours ago. 

&amp;#x200B;

Thanks",0,1
1054,2019-8-18,2019,8,18,3,crq2ks,Youre studying data science ? here is how to install and use Jupyter Notebook  part 1 ,https://www.reddit.com/r/MachineLearning/comments/crq2ks/youre_studying_data_science_here_is_how_to/,sajad-52,1566067091,,0,1
1055,2019-8-18,2019,8,18,3,crqadm,[D] ComputerVision supported by DeepLearning to help SPORT ANALYTICS,https://www.reddit.com/r/MachineLearning/comments/crqadm/d_computervision_supported_by_deeplearning_to/,cmillionaire9,1566068100," [https://www.youtube.com/watch?v=kaslJ-8piSE](https://www.youtube.com/watch?v=kaslJ-8piSE) Achieving fully automated, without manual operators and wearables, real-time individual player tracking and ball tracking is extremely challenging on many dimensions. Startup Signality is getting that done, to help sport clubs excel and get access to 3 million data points per match in an easy and actionable way...",0,1
1056,2019-8-18,2019,8,18,3,crqbdt,[D] ComputerVision supported by DeepLearning to help SPORT ANALYTICS,https://www.reddit.com/r/MachineLearning/comments/crqbdt/d_computervision_supported_by_deeplearning_to/,cmillionaire9,1566068239,[removed],0,1
1057,2019-8-18,2019,8,18,3,crqbxq,[D] ComputerVision supported by DeepLearning to help SPORT ANALYTICS,https://www.reddit.com/r/MachineLearning/comments/crqbxq/d_computervision_supported_by_deeplearning_to/,cmillionaire9,1566068309,[removed],0,1
1058,2019-8-18,2019,8,18,4,crqcyb,[D] ComputerVision supported by DeepLearning to help SPORT ANALYTICS,https://www.reddit.com/r/MachineLearning/comments/crqcyb/d_computervision_supported_by_deeplearning_to/,cmillionaire9,1566068445,[removed],0,1
1059,2019-8-18,2019,8,18,4,crqe9y,[D] ComputerVision supported by DeepLearning to help SPORT ANALYTICS,https://www.reddit.com/r/MachineLearning/comments/crqe9y/d_computervision_supported_by_deeplearning_to/,PlayfulConfidence,1566068610,[removed],0,1
1060,2019-8-18,2019,8,18,4,crqhjh,[D] ComputerVision supported by DeepLearning to help SPORT ANALYTICS,https://www.reddit.com/r/MachineLearning/comments/crqhjh/d_computervision_supported_by_deeplearning_to/,MaleficentPepper,1566069011,[removed],0,1
1061,2019-8-18,2019,8,18,5,crrjjk,This pops up when I look for machine learning books at my local library!  Now that's proper machine learning right there!,https://www.reddit.com/r/MachineLearning/comments/crrjjk/this_pops_up_when_i_look_for_machine_learning/,BetoBob,1566074018,"&amp;#x200B;

https://i.redd.it/ememgkcek2h31.png",0,1
1062,2019-8-18,2019,8,18,6,crryfz,"In XLNet, why does first layer's query vector tied across positions?",https://www.reddit.com/r/MachineLearning/comments/crryfz/in_xlnet_why_does_first_layers_query_vector_tied/,bayareasurfer,1566075992,[removed],0,1
1063,2019-8-18,2019,8,18,6,crryxk,[N] Google files patent Deep Reinforcement Learning for Robotic Manipulation,https://www.reddit.com/r/MachineLearning/comments/crryxk/n_google_files_patent_deep_reinforcement_learning/,Dazzling_Help,1566076054,"Patent: [https://patents.google.com/patent/WO2018053187A1/en](https://patents.google.com/patent/WO2018053187A1/en)

Inventor: Sergey LEVINE, Ethan HOLLY, Shixiang Gu, Timothy LILLICRAP

Abstract

Implementations utilize deep reinforcement learning to train a policy neural network that parameterizes a policy for determining a robotic action based on a current state. Some of those implementations collect experience data from multiple robots that operate simultaneously. Each robot generates instances of experience data during iterative performance of episodes that are each explorations of performing a task, and that are each guided based on the policy network and the current policy parameters for the policy network during the episode. The collected experience data is generated during the episodes and is used to train the policy network by iteratively updating policy parameters of the policy network based on a batch of collected experience data. Further, prior to performance of each of a plurality of episodes performed by the robots, the current updated policy parameters can be provided (or retrieved) for utilization in performance of the episode.",82,262
1064,2019-8-18,2019,8,18,6,crs369,Artificial Intelligence for Everyone by Andrew Ng - Time to accelerate your career and earnings growth,https://www.reddit.com/r/MachineLearning/comments/crs369/artificial_intelligence_for_everyone_by_andrew_ng/,internetdigitalentre,1566076620,[removed],0,1
1065,2019-8-18,2019,8,18,6,crs3ye,[R] ma-gym: multi agent environments based on open ai gym,https://www.reddit.com/r/MachineLearning/comments/crs3ye/r_magym_multi_agent_environments_based_on_open_ai/,HeavyStatus4,1566076723,"ma-gym is a collection of simple multi-agent environments based on open ai gym with the intention of keeping the usage simple and exposing core challenges in multi-agent settings. 

I made it during my recent internship and I hope it could be useful for others in their research or getting someone started with multi-agent reinforcement learning.

Github: [https://github.com/koulanurag/ma-gym](https://github.com/koulanurag/ma-gym)",5,17
1066,2019-8-18,2019,8,18,7,crtazy,"[P][D] Pytorch Sparse training library. Sparse training = fraction of all parameters updated each step. Non-used parameters saved to disk -&gt; reduce GPU Memory Usage + Increase Training Speed. If you are working with such an architecture, let us know and we'll optimize and include it in our release.",https://www.reddit.com/r/MachineLearning/comments/crtazy/pd_pytorch_sparse_training_library_sparse/,Research2Vec,1566082708,"Hello,

We are creating a sparse training library for Pytorch. Sparse training is when only a fraction of the total parameters go through a forwards pass / backwards pass / update during each step. 

Having **all** parameters takes up a lot of GPU memory, and in some cases may limit the total number of parameters your system can hold. By having the parameters stored on disk when not in use, that would significantly reduce the GPU memory used at any given instance, allowing you to use many more parameters. 

A concern is that generally disk are not low enough latency to make this work. But we were able to figure out a pipeline to make it work. Not only that, but through a few Pytorch tricks we inadvertently discovered along the way, we think our set up may be (very slightly) faster, though we'll need to do a bunch of test to absolutely confirm. 

At the moment we need to code each adapt each architecture individually. If you or anyone you know have sparse training architecture you have in mind, point us to the paper or code and we'll optimize and include it. 

So far we've only been able to find recommender systems that make use of such architectures. If you know of any other architectures, please point them out.",9,17
1067,2019-8-18,2019,8,18,8,crteai,[D] Does it get better?,https://www.reddit.com/r/MachineLearning/comments/crteai/d_does_it_get_better/,jamesaliam,1566083167,"To give a bit of a background, I joined big N company for a bit over an year now for an ml research position, after finishing grad school and having some moderate success in academic research.

During my time here my team has been involved in 3 projects. And results have ranged from failure to success that hardly resulted from researching ml. time put into ml optimization resulting in only marginal improvements, and most actual value coming from unrelated stuff. Currently, even though our evaluations seem to be pretty good I think I am not bringing all that much value to the company. I really hope that I've had a rough start and it gets better, but wanted to hear others experiences.",20,22
1068,2019-8-18,2019,8,18,8,crtomd,"""Its very easy to confuse that artificial intelligence, machine learning, and deep learning are the same thing. The easiest way to understand the relationship between them is to visualize them as concentric circles""",https://www.reddit.com/r/MachineLearning/comments/crtomd/its_very_easy_to_confuse_that_artificial/,PeteyCruiser123,1566084678,,2,1
1069,2019-8-18,2019,8,18,9,cru2qb,[R] Best notes on probabilistic PCA with missing data?,https://www.reddit.com/r/MachineLearning/comments/cru2qb/r_best_notes_on_probabilistic_pca_with_missing/,carmichael561,1566086800,,0,1
1070,2019-8-18,2019,8,18,11,crvuux,Right approach to find orders which are likely to be upgraded to Air freight?,https://www.reddit.com/r/MachineLearning/comments/crvuux/right_approach_to_find_orders_which_are_likely_to/,benejababsch,1566096790,[removed],0,1
1071,2019-8-18,2019,8,18,12,crw0he,Question regarding feasability/scope of a potential project,https://www.reddit.com/r/MachineLearning/comments/crw0he/question_regarding_feasabilityscope_of_a/,dukeoflaser,1566097716,[removed],0,1
1072,2019-8-18,2019,8,18,12,crw19l,Deep learning using tumor HLA peptide mass spectrometry datasets improves neoantigen identification,https://www.reddit.com/r/MachineLearning/comments/crw19l/deep_learning_using_tumor_hla_peptide_mass/,ilikepancakez,1566097833,,0,1
1073,2019-8-18,2019,8,18,12,crw1tb,[R] Deep learning using tumor HLA peptide mass spectrometry datasets improves neoantigen identification,https://www.reddit.com/r/MachineLearning/comments/crw1tb/r_deep_learning_using_tumor_hla_peptide_mass/,ilikepancakez,1566097925,,0,1
1074,2019-8-18,2019,8,18,14,crx58s,[D] 1080ti vs 2080?,https://www.reddit.com/r/MachineLearning/comments/crx58s/d_1080ti_vs_2080/,pg13mvp,1566104724,"2080 has tensor core, and can do 16bits caculate.
1080ti has 11gb ram 

2080 is slightly cheaper 

So which should I get?
I'm wondering that is 16bit calculation means twice capacity?

Thanks",27,5
1075,2019-8-18,2019,8,18,14,crx98v,This may sound silly... but..,https://www.reddit.com/r/MachineLearning/comments/crx98v/this_may_sound_silly_but/,TheoriginalMr,1566105490,[removed],0,1
1076,2019-8-18,2019,8,18,16,cryhi1,"""AlphaZero"" for find/compute the best music/voices in the Universe",https://www.reddit.com/r/MachineLearning/comments/cryhi1/alphazero_for_findcompute_the_best_musicvoices_in/,itmanager85,1566114860,[removed],0,1
1077,2019-8-18,2019,8,18,17,cryo8o,    ,https://www.reddit.com/r/MachineLearning/comments/cryo8o/____/,gigaman12,1566116456,,0,1
1078,2019-8-18,2019,8,18,17,cryr4z,  ,https://www.reddit.com/r/MachineLearning/comments/cryr4z/__/,gigaman12,1566117153,[removed],0,1
1079,2019-8-18,2019,8,18,17,crysua,Buy Weed Online at Healing Buddha Shop | Online Cannabis Dispensary,https://www.reddit.com/r/MachineLearning/comments/crysua/buy_weed_online_at_healing_buddha_shop_online/,torydevorejmx,1566117534,,0,1
1080,2019-8-18,2019,8,18,18,crz083,"[D] I created forecasting model to forecast cryptocurrency using sentiment data, and this is the result.",https://www.reddit.com/r/MachineLearning/comments/crz083/d_i_created_forecasting_model_to_forecast/,huseinzol05,1566119245,"dataset and code can get from here, https://github.com/huseinzol05/Stock-Prediction-Models/blob/master/deep-learning/sentiment-consensus.ipynb

## How we gather the data, provided by Bitcurate, bitcurate.com

Because I don't have sentiment data related to stock market, so I will use cryptocurrency data, BTC/USDT from binance.

1. close data came from CCXT, https://github.com/ccxt/ccxt, an open source cryptocurrency aggregator.
2. We gather from streaming twitter, crawling hardcoded crpyocurrency telegram groups and Reddit. And we store in Elasticsearch as a single index. We trained 1/4 layers BERT MULTILANGUAGE (200MB-ish, originally 700MB-ish) released by Google on most-possible-found sentiment data on the internet, leveraging sentiment on multilanguages, eg, english, korea, japan. Actually, it is very hard to found negative sentiment related to bitcoin / btc in large volume.

And the we use elasticsearch-dsl, https://elasticsearch-dsl.readthedocs.io/, to query,
```python
s = s.filter(
    'query_string',
    default_field = 'text',
    query = 'bitcoin OR btc',
)
```

We only do text query only contain bitcoin or btc.

## Consensus introduction

We have 2 questions here when saying about consensus, what happened,

1. to future price if we assumed future sentiment is really positive, near to 1.0 . Eg, suddenly China want to adapt cryptocurrency and that can cause huge requested volumes.
2. to future price if we assumed future sentiment is really negative, near to 1.0 . Eg, suddenly hackers broke binance or any exchanges, or any news that caused wreck by negative sentiment.

So, we use deep-learning to simulate for us! I use CNN-Seq2Seq architecture this time, not required to bring last memory last RNN and fast to train.

## Step

1. We pulled last 100 hours data and aggregated every 20 minutes, Split the dataset to train and test. Test size is last 10 hours (30 datapoints, 3 * 10), and early remaining use to train.
2. Initiate the model and train the model by 200 epochs. `learning_rate` is very sensitive, I found `1e-3` is perfect. Here I never tried to do hyperparameters searching.

## Result

&lt;img src=""https://raw.githubusercontent.com/huseinzol05/Stock-Prediction-Models/master/output/sentiment-consensus.png"" width=""70%"" align=""""&gt;

## Discussion

1. The model learn, if positive and negative sentiments increasing, both will increase the price. That is why, using positive consensus or negative consensus caused price going up.
2. Volatility of price is higher if negative sentiment is higher, still positive volatility.
3. Momentum of price is higher if negative sentiment is higher, still positive momentum.
4. Even predicted trends are far from actual test trend, for me, it quite fascinating because I can simulate the models by N times to get different variances and from here I can calculate VaR, potential volatilities and momentums, trading ratios and etc. Well, if forecasted trends follow really close with actual test trend, do not believe it too much, there is no such model able to simulate stochastic trend that depends on a lot of real world parameters.

Any comment or feedback?",13,0
1081,2019-8-18,2019,8,18,18,crz6i5,AI Generated Rock Music Composed by AIVA,https://www.reddit.com/r/MachineLearning/comments/crz6i5/ai_generated_rock_music_composed_by_aiva/,itmanager85,1566120674,[removed],0,1
1082,2019-8-18,2019,8,18,18,crzc8x,"Are ""machine learning is just bunch of if and else statements"" memes true?",https://www.reddit.com/r/MachineLearning/comments/crzc8x/are_machine_learning_is_just_bunch_of_if_and_else/,signalboi,1566121984,[removed],0,1
1083,2019-8-18,2019,8,18,18,crzcvs,Terms to remember while building a Machine Learning Model | MLAIT,https://www.reddit.com/r/MachineLearning/comments/crzcvs/terms_to_remember_while_building_a_machine/,mlait1908,1566122137,,0,1
1084,2019-8-18,2019,8,18,19,crznjw,A very intuitive tutorial ,https://www.reddit.com/r/MachineLearning/comments/crznjw/a_very_intuitive_tutorial/,susmit410,1566124396,,0,1
1085,2019-8-18,2019,8,18,19,crzr4o,What is to SOTA in semantic segmentation for 2019?,https://www.reddit.com/r/MachineLearning/comments/crzr4o/what_is_to_sota_in_semantic_segmentation_for_2019/,orgad,1566125250,[removed],0,1
1086,2019-8-18,2019,8,18,20,crzwos,Didn't OpenAI make something similar to WaveNet with very promising results? I can't find it anymore.,https://www.reddit.com/r/MachineLearning/comments/crzwos/didnt_openai_make_something_similar_to_wavenet/,monsieurpooh,1566126395,[removed],0,1
1087,2019-8-18,2019,8,18,20,crzzic,I found this amazing post and I wanted to share with you ... Find your path in data science.,https://www.reddit.com/r/MachineLearning/comments/crzzic/i_found_this_amazing_post_and_i_wanted_to_share/,mister2020,1566126971,,0,1
1088,2019-8-18,2019,8,18,20,cs05xf,Tutorial on generative model with TF2.0 or keras,https://www.reddit.com/r/MachineLearning/comments/cs05xf/tutorial_on_generative_model_with_tf20_or_keras/,tonybonse,1566128239,[removed],0,1
1089,2019-8-18,2019,8,18,21,cs0kzw,How to present differences between clusters with a large number of categorical features?,https://www.reddit.com/r/MachineLearning/comments/cs0kzw/how_to_present_differences_between_clusters_with/,moqiwf,1566131151,[removed],0,1
1090,2019-8-18,2019,8,18,22,cs1gk0,Is something wrong with my train() function,https://www.reddit.com/r/MachineLearning/comments/cs1gk0/is_something_wrong_with_my_train_function/,cimmic,1566136455,[removed],0,1
1091,2019-8-18,2019,8,18,23,cs270i,Crash Overview on Keras Software Architecture Basics,https://www.reddit.com/r/MachineLearning/comments/cs270i/crash_overview_on_keras_software_architecture/,HealthyNatural0,1566140227,[removed],0,1
1092,2019-8-19,2019,8,19,0,cs2d4w,You use Python Environments with Conda ? here is how to manage them- part 1-,https://www.reddit.com/r/MachineLearning/comments/cs2d4w/you_use_python_environments_with_conda_here_is/,sajad-52,1566141057,,0,1
1093,2019-8-19,2019,8,19,0,cs2j3t,How the New York Police Department is Using Artificial Intelligence ? Answers (latest),https://www.reddit.com/r/MachineLearning/comments/cs2j3t/how_the_new_york_police_department_is_using/,updownvizzii,1566141857,,0,1
1094,2019-8-19,2019,8,19,0,cs2xcn,Python vs. Rust for Neural Networks,https://www.reddit.com/r/MachineLearning/comments/cs2xcn/python_vs_rust_for_neural_networks/,iamkeyur,1566143744,,0,1
1095,2019-8-19,2019,8,19,0,cs2xmh,Chess2vec: Learning Vector Representations for Chess (2018) [pdf],https://www.reddit.com/r/MachineLearning/comments/cs2xmh/chess2vec_learning_vector_representations_for/,iamkeyur,1566143784,,0,1
1096,2019-8-19,2019,8,19,1,cs328k,Why are the majority of the Winograd Schema scores on the GLUE leaderboard exactly 65.1?,https://www.reddit.com/r/MachineLearning/comments/cs328k/why_are_the_majority_of_the_winograd_schema/,OcelotRL,1566144372,[removed],0,1
1097,2019-8-19,2019,8,19,1,cs37bn,[p] Generating TechCrunch start-up news with GPT2,https://www.reddit.com/r/MachineLearning/comments/cs37bn/p_generating_techcrunch_startup_news_with_gpt2/,th3userscene,1566145016,"Hello.

It all started at a startup summit I visited. There I thought of teaching an AI to generate start-up ideas.

I couldn't find data, so I decided to make it generate start-up news from TechCrunch.

I decided this would be a good time to test GPT-2, which generated believable outputs.

I trained this network using [Google Colab](https://colab.research.google.com), since my ML machine has no GPU and a bad CPU.

Anyway, here are the results!

*Prompt: ""funding""*

&gt; s 26/Fitz Frames is planning on giving away pairs of goggles as an homage to the brand, 27/ Stella Brands has revealed it has had an ownership stake in Fitz Frames since 2012 Rex 28/ with Marco Benevento is the birth name of the latest wave of millennial investors Y Combinator 39/ Star Wars Celebration Star Wars Celebration  Fitz Frames is currently active on Y Combinators fundraising page, raising $15 million in seed funding and several more round funding rounds. Its fourth round is set to be its last, in early 2017, bringing its valuation to between $50 and $100 million, as it attempts to resuscitate the increasingly popular glasses. Fitz Frames hopes to become a more durable, lightweight option for travel, and is pursuing businesses that share its technology with bring the option of attaching sunglasses to people's feet. It stems from a game where two friends play jack-of-all-trades, attempting to become the first to have their glasses replaced. While other players in the jack-of-all-trades game have tried to make it to the final round, in which the winner is determined by best results from the next round. Players toss duct tape over their faces to make room for new glasses, and drop them somewhere cool for future decorating. The game is currently in its fourth season, and said fourth round will be its biggest to date, with all-new glasses added by the day. A new website is hoping to get around this by giving players a more standardized format for making their purchases. Its called Lux, and its set to inspirationbeite consumers. The way we think about technology is related to 1) what we are able to buy and 2) what we are able to want from the experience. If you have me walk you through the steps of making a new pair of glasses, I promise you that thats it. Making new glasses is kind of like making your first pair of shoes, but you cant replace the pair you got from the size chart on the website. 2) Thats the problem. You can probably make the case for creating greater accessibility in the least, Kiss 101 founder and CEO Gabrielle Esposito tells me. There are tons of opportunities out there. From helping you find a bus stop to helping make your childs doctor 

*Prompt: ""virtual reality""*

&gt; From a physical standpoint I think the physical world is actually kind of the only guidesarasin about 14   with little' to none guidances, added Binks. Most of us, by now, are aware that virtual reality (VR) can be a bit heavy for a country like India, where the current model focuses on physically reaching and leaving virtual walls. The problem is that there isnt much physical speed at which a user can experience a place . That balls of a city are a walk in the park right now, but they often arent possible in South Asia, where walking and running are the norm. Sri Lanka is an alternate model for India, focusing on smaller, backpack-accessible cities that dont rely on long lines to get by, and are often hard for Asian cities like Japan and the U.S. to get their cities out of the city. Inhibited by the physical and economic advantages of the Indian subcontinent, Binks ist seeking funding  and in this day and age of artificial intelligence and artificial deadlines, its crucial that governments prioritize proven methods of achieving long-term solutions. We need to be able to rely on the cards and the the thesles, pondering pal Jain Cheung, co-founder and CEO of DeepMind. We need to be able to rely on our bank and have it tell us whats happening online, Cheung said. In addition to launching treating surroundings as if they were holograms, the founders latest approach is also use virtual reality as an alternative to the human brain for assessing long-distance travel. For now, though, the focus is on using virtual reality to help make smarter batteries  and connect-and-disconnect sensors  that our bodies have built into our brains. The amount of power we have is going to have an effect on the way we do it. said FitzGerald. But from a health and security perspective, said Cheung. The goal is for the technology to be applied across all healthcare services, said Feridranco. Any healthcare organization would like to have accurate healthcare data, he said. The problem is that 

More on my [Pastebin](https://pastebin.com/KbRXMgtN).",3,2
1098,2019-8-19,2019,8,19,2,cs3v75,NLP / NLG/ NLI - Research Areas,https://www.reddit.com/r/MachineLearning/comments/cs3v75/nlp_nlg_nli_research_areas/,Strange_Flatworm,1566147967,[removed],0,1
1099,2019-8-19,2019,8,19,2,cs4aml,"[P] I created a Transformer Model package in Tensorflow 2.0 that is extensible and can be used to rebuild GPT-2, BERT, and XLNet.",https://www.reddit.com/r/MachineLearning/comments/cs4aml/p_i_created_a_transformer_model_package_in/,dfcHeadChair,1566149831,"Hi everyone,

 [https://pypi.org/project/transformer-model/](https://pypi.org/project/transformer-model/) 

 `pip install transformer-model` 

I recently took some time to build out an extensible Transformer Model in TF2, mostly for my own future use cases but I thought I'd share with you and possibly get some feedback as well. I have not created many python packages, so if there's something I missed or seems out of place feel free to create an issue on the repo.

&amp;#x200B;

The goal of this project was to create all of the core pieces of the Transformer Model discussed in the [""Attention is all you need""](https://arxiv.org/pdf/1706.03762.pdf) paper in a way that I could reuse them to create newer, more SOTA models like BERT and XLNet. I've left instructions on how to use this package to train a Transformer model and will be packaging this to go on pypi later today.

&amp;#x200B;

My hope is this package saves someone some dev time. If it does, please give the package a star!",18,229
1100,2019-8-19,2019,8,19,3,cs4o2p,[D] What are the performance metrices of word embedding model?,https://www.reddit.com/r/MachineLearning/comments/cs4o2p/d_what_are_the_performance_metrices_of_word/,uname_uknown,1566151426,"Basically I am trying to find similarity of sentences in my native language. I created word embeddings using word2vec and fasttext. And used word mover distance, to find similar text. Now how do I determine the performance of each model.
P.S there is no dataset for similar text in my native language.",5,4
1101,2019-8-19,2019,8,19,3,cs4qmt,[R] Crash Overview on Keras Software Architecture Basics,https://www.reddit.com/r/MachineLearning/comments/cs4qmt/r_crash_overview_on_keras_software_architecture/,penalvad00,1566151730,"# This is one of a series of small snippet-tutorials to share with you my experiences with APIs hacking and Tensorflow hacker.An effort to contribute to the community of Deep Learning software developers grow, from the basis of how to deal with Open Source Software (it obviously depends on language you are working, this time python).

[https://uiuran.github.io/keras/tensorflow/deeplearning/2019/08/18/Keras-Deep-Learning-High-Level-API-Dismistified.html](https://uiuran.github.io/keras/tensorflow/deeplearning/2019/08/18/Keras-Deep-Learning-High-Level-API-Dismistified.html)",1,0
1102,2019-8-19,2019,8,19,3,cs4xq4,GANimation implementation with pretrained models,https://www.reddit.com/r/MachineLearning/comments/cs4xq4/ganimation_implementation_with_pretrained_models/,viccpopa,1566152590,[removed],0,1
1103,2019-8-19,2019,8,19,3,cs4yyo,Beginner's guide to Exploratory Data Analysis.,https://www.reddit.com/r/MachineLearning/comments/cs4yyo/beginners_guide_to_exploratory_data_analysis/,YellowDragon420,1566152761,[removed],0,1
1104,2019-8-19,2019,8,19,3,cs52iy,[P] Pytorch Implementation of GANimation with pretrained weights.,https://www.reddit.com/r/MachineLearning/comments/cs52iy/p_pytorch_implementation_of_ganimation_with/,viccpopa,1566153201,"Hi all!

**TL;TR** I shared on GitHub an [implementation](https://github.com/vipermu/ganimation) of a Conditional GAN model called GANimation. I include pretrained weights and a preprocessed dataset as well.

&amp;#x200B;

A few months ago I became really interested in a project called [GANimation](https://arxiv.org/abs/1807.09251). The authors (Pumarola et al.) of this project were able to train a Conditional GAN model capable of modifying facial expressions in a continuous way. This sounded really interesting to me and I wanted to play with the model, but when I tried the author's implementation in my computer I had problems with the training process and I didn't find any pre-trained weights. As at that moment I also wanted to learn PyTorch, I decided to create my own implementation. As this project was really similar to StarGAN I started cloning their [repo](https://github.com/yunjey/stargan) and I used it as baseline.

In this implementation I provide pretrained models and a preprocessed dataset to facilitate the use of this model. I also included the functions to create the following video.

[Applying the expression of the face in the first column to each image in the top row.](https://i.redd.it/j4xsfahw29h31.gif)

Although there's a lot to improve and clean in the code, I hope it can be useful for anyone that wants to use this model.",0,19
1105,2019-8-19,2019,8,19,3,cs5c8f,[D] Thesis Subject Suggestion,https://www.reddit.com/r/MachineLearning/comments/cs5c8f/d_thesis_subject_suggestion/,mfarahmand98,1566154415,"I apologize if this post doesn't quite match this subreddit, but I'm really looking for suggestions from active practitioners and researchers in the field and this looks like a great place to find one!

I'm a computer engineering student, about to begin my last year of bachelor's degree. As you could imagine, the AI course I had to take was nothing but primitive search algorithms. But with the help of one of my professors, I was able to study ML and DL in particular, using a number of courses and a few books. Right now I've one paper almost ready for submission and a novel model on its way (both GAN-related).

Now I'm wondering about the subject of my thesis. I talked to my professor a while ago and he suggested to work on a model that could take in a face and output the same person at some specific age. But I was kinda hoping to work on something more practical which other people could use as well.

I had the idea of creating an application, sorta a graphical interface to Keras, which would allow the user to create the computation graph, make the training loop and normalize the data. After giving some thoughts to this idea, I came to the realization that such a tool can never offer the same range of possibilities a programmer would have with the library itself. Then I thought about targeting the practitioners and focusing more on the mainstream architectures but that sounded hard to finish in just one year, as well. I'm not sure if a beta version would be worth pursuing, with the hope that I could finish it during my masters.

The reason that I'm aiming at projects like this is that I'm hoping to get a scholarship to continue my studies abroad which I won't be able to afford on my own in any way (I'm Iranian and if you check the value of our currency, you'd realize why). I'm hoping to prove my capabilities in my thesis project. My professor says that if I get even a single paper published, that's enough for a bachelor student, but... you can never be too sure!

Another idea that I'm not exactly sure is worth the effort or not is working on some sort of a knowledge base for ML. With the every growing amount of publications in this field, it's getting harder and harder to dive through them, specifically when it comes to comparing models. Now imagine a website with a taxonomy system that allows for a tree-like categorizing system which researchers could introduce their models in and others add their own experiences, implementations, etc. Unfortunately, I'd only be able to implement the website and absolutely no where near knowledgeable enough to populate it... 

I'd greatly appreciate it if you could guide me towards a some type of a project that would be an impressive point in my resume to get an scholarship.",10,0
1106,2019-8-19,2019,8,19,4,cs5htz,Where can I find more information about fitting a model?,https://www.reddit.com/r/MachineLearning/comments/cs5htz/where_can_i_find_more_information_about_fitting_a/,kosar7,1566155121,[removed],0,1
1107,2019-8-19,2019,8,19,4,cs5n5g,[D] Hacker Learns The Price of The Cloud,https://www.reddit.com/r/MachineLearning/comments/cs5n5g/d_hacker_learns_the_price_of_the_cloud/,jan_mike_vincent,1566155793,,0,1
1108,2019-8-19,2019,8,19,4,cs620g,Kaggle: When Is It Better To Keep The Algorithm To Yourself?,https://www.reddit.com/r/MachineLearning/comments/cs620g/kaggle_when_is_it_better_to_keep_the_algorithm_to/,mystikaldanger,1566157609,[removed],0,1
1109,2019-8-19,2019,8,19,5,cs6zba,[D] The link between stationary distributions and SDEs,https://www.reddit.com/r/MachineLearning/comments/cs6zba/d_the_link_between_stationary_distributions_and/,chrisorm,1566161844,"Somewhat old paper, [https://arxiv.org/abs/1506.04696](https://arxiv.org/abs/1506.04696). I recently spent some time going over this, and the paper has some great proofs and discussion. I did however,find myself looking at their theorem and thinking ""how on earth did they find that form of the drift coefficient"", and found the proof to be mainly about showing if you use that form, the result holds. That's not too enlightening if you want insight into how they found the result, so I went the other way myself, and in 1D it turns out to be somewhat straight-forward. I wrote it up if anybody else finds this view interesting

&amp;#x200B;

[https://chrisorm.github.io/SDE-S.html](https://chrisorm.github.io/SDE-S.html)",2,9
1110,2019-8-19,2019,8,19,6,cs7hwk,Need help connecting to Google Cloud instance from Spyder IDE (Anaconda),https://www.reddit.com/r/MachineLearning/comments/cs7hwk/need_help_connecting_to_google_cloud_instance/,oezeadi,1566164252,[removed],0,1
1111,2019-8-19,2019,8,19,7,cs7zpc,Should I learn python before taking a Machine Learning course,https://www.reddit.com/r/MachineLearning/comments/cs7zpc/should_i_learn_python_before_taking_a_machine/,Tristen_3,1566166606,[removed],2,1
1112,2019-8-19,2019,8,19,7,cs82w6,Exploring the Smelliverse with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cs82w6/exploring_the_smelliverse_with_machine_learning/,craigspencersmith,1566167037,,0,1
1113,2019-8-19,2019,8,19,9,cs9j5i,[D] Is Neuroscience background useful for ML research?,https://www.reddit.com/r/MachineLearning/comments/cs9j5i/d_is_neuroscience_background_useful_for_ml/,Slayer10101,1566174451,"Is there anybody with neuroscience background who moved to ML research?
Any ML researchers who deliberately decided to learn some neuroscience to get new ideas?

Would you say it was worth it to learn neuroscience for you?
Would you say it would be better to just focus purely on ML?

I am finishing my CS undergrad and deciding between two options for grad school:
1) PhD in Optimization/ML theory
2) MS in Neuroinformatics (and then eventually going for PhD in ML theory)

I am generally interested in learning neuroscience and understanding how the brain works. However, it seems the theory is not quite here yet, and I do not want to work on experimental biological side. I ultimately want to work on ML theory as I think ML has the most impact here and now, while it will likely take some decades until neuroscience is sufficiently developed. That is why I am considering to learn some core neuroscience concepts, and then try to apply those concepts to find novel ideas for ML.

The Neuroinformatics MS program is quite flexible and will allow me to be primarily focused on ML and open-ended research, while 1/3 of my courses will be in neuroscience. I will work on bio-plausible backprop (some references are [here](https://psychology.stackexchange.com/questions/16269/is-back-prop-biologically-plausible)) and maybe spiking neural networks. Somewhat unrelatedly, being there may give me some insight into brain-computer interfaces research while there is growing interest in that. 

I think doing that MS will give me more diverse background and ideas for further research in ML theory, and open more doors. However, I am somewhat concerned if it is worth it, wouldn't doing pure ML leave me in in a better position?

Also, I am a bit sceptical about bio-plausible ML research. While it is really interesting, it seems to be a bit of a ""toy"" problem. We don't even know if something like backprop happens in the brain, so trying to make it more ""bio-plausible"" for its own sake is somewhat of an artificial problem.

There was a related discussion: [[D] Computational Neuroscience and Machine Learning](https://www.reddit.com/r/MachineLearning/comments/9pmqya/d_computational_neuroscience_and_machine_learning/)",47,41
1114,2019-8-19,2019,8,19,10,csaacm,How can I train a Deepfake?,https://www.reddit.com/r/MachineLearning/comments/csaacm/how_can_i_train_a_deepfake/,jennysebastian,1566178490,[removed],0,1
1115,2019-8-19,2019,8,19,11,csakxk,[D] Are there any machine learning frameworks that work on AMD GPU's?,https://www.reddit.com/r/MachineLearning/comments/csakxk/d_are_there_any_machine_learning_frameworks_that/,CallMeOutWhenImPOS,1566180101,"Hi there,

I recently built a new PC with an AMD RX5700 (with AMD Ryzen 3600 CPU) and was wondering if there are any popular frameworks for training on the GPU, preferably in Python. I cannot seem to find a concise answer on this. Any help/suggestions would be greatly appreciated. Thank you.",9,11
1116,2019-8-19,2019,8,19,11,csau62,[R] Photo-Realistic Facial Details Synthesis From Single Image,https://www.reddit.com/r/MachineLearning/comments/csau62/r_photorealistic_facial_details_synthesis_from/,PuzzledProgrammer3,1566181505,"paper: [https://arxiv.org/pdf/1903.10873.pdf](https://arxiv.org/pdf/1903.10873.pdf)

code: [https://github.com/apchenstu/Facial\_Details\_Synthesis](https://github.com/apchenstu/Facial_Details_Synthesis)",1,7
1117,2019-8-19,2019,8,19,12,csbkww,"Quant vs. ""regular"" Post-PhD Career Trajectories",https://www.reddit.com/r/MachineLearning/comments/csbkww/quant_vs_regular_postphd_career_trajectories/,donb1988,1566185665,[removed],0,1
1118,2019-8-19,2019,8,19,13,csbyiv,"[D] Quant vs. ""Regular"" Post-PhD Career Trajectories",https://www.reddit.com/r/MachineLearning/comments/csbyiv/d_quant_vs_regular_postphd_career_trajectories/,donb1988,1566187856,"Hi all, profuse apologies in advance if this is not the correct place to ask this question. I've attempted to look around for information (both online and offline), but perhaps I'm not hitting the right keywords, so I though I'd give this a try.

My question is specifically about ""quant researcher"" type careers, and what the pros/cons and other considerations are when taking up a job like that.

My understanding is that post-PhD (in ML, or whatever the department is that accommodated your ML research for the bulk of the PhD), the majority of people aim for (1) ""research scientist"" roles in industry, or (2) focus more on an academic career, or (3) both, simultaneously. Obviously this is a generalization, and there are many more things you can do with any STEM PhD for that matter, but these options seem to be the goals of many people.

What about (4) quant jobs in finance, such as in small/large trading / hedge funds / asset management / etc.? They frequently appear to offer extremely attractive packages, and require no experience in finance. However, for the most part the community seems to be rather separate from the (1) through (3) crowd I mentioned above, so I am unable to get a coherent picture of why some folks choose one path versus another, and the various things you should consider (e.g. long-term career trajectory, exit options, etc.) when taking your first job / internship in finance during / after your PhD.

Apologies for the naive question, and apologies again if this is not the right place to ask this kind of thing. Thank you in advance for your kind advice!",33,41
1119,2019-8-19,2019,8,19,14,cscpbe,How to get started with a chatbot?,https://www.reddit.com/r/MachineLearning/comments/cscpbe/how_to_get_started_with_a_chatbot/,getengati,1566192536,[removed],0,1
1120,2019-8-19,2019,8,19,15,csd92l,Stretch film making machine,https://www.reddit.com/r/MachineLearning/comments/csd92l/stretch_film_making_machine/,0207Miya,1566196282,[removed],0,1
1121,2019-8-19,2019,8,19,16,csdj7v,Basic Understanding of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/csdj7v/basic_understanding_of_machine_learning/,gauthamkolluru,1566198231,"Hi,

I have tried to explain Machine Learning in the simplest way I could think of by far in the article and the link is shared here.

Please go through it and share your valuable suggestions and comments. [understanding Machine Learning](https://gauthamsk.me/technology/ml-in-layman-terms/)

Thanks in advance",0,1
1122,2019-8-19,2019,8,19,16,csdnd6,How to Improve Accuracy Of Machine Learning Model?,https://www.reddit.com/r/MachineLearning/comments/csdnd6/how_to_improve_accuracy_of_machine_learning_model/,cogitotechllc,1566199034,,0,1
1123,2019-8-19,2019,8,19,16,csdy7g,"Reinforcement learning in a ""real"" environment",https://www.reddit.com/r/MachineLearning/comments/csdy7g/reinforcement_learning_in_a_real_environment/,OkRice10,1566201270,[removed],0,1
1124,2019-8-19,2019,8,19,17,cse3mf,Facial landmark detection,https://www.reddit.com/r/MachineLearning/comments/cse3mf/facial_landmark_detection/,flyhighwithai,1566202411,,0,1
1125,2019-8-19,2019,8,19,17,cseeu2,"Interview with Kaggle GrandMaster, Dr. Vladimir Iglovikov about Albumentations: a fast image augmentations library",https://www.reddit.com/r/MachineLearning/comments/cseeu2/interview_with_kaggle_grandmaster_dr_vladimir/,init__27,1566204819,[removed],0,1
1126,2019-8-19,2019,8,19,18,cseho7,"Multi-level data, what is the best approach?",https://www.reddit.com/r/MachineLearning/comments/cseho7/multilevel_data_what_is_the_best_approach/,throwwawwayz123,1566205369,[removed],0,1
1127,2019-8-19,2019,8,19,18,csek1t,In-Browser Object Detection Using Tensorflow.js [Tutorial],https://www.reddit.com/r/MachineLearning/comments/csek1t/inbrowser_object_detection_using_tensorflowjs/,manneshiva,1566205861,[removed],0,1
1128,2019-8-19,2019,8,19,18,csekox,Zooming into the world of computer vision applications,https://www.reddit.com/r/MachineLearning/comments/csekox/zooming_into_the_world_of_computer_vision/,MachineLearning001,1566205975,,0,1
1129,2019-8-19,2019,8,19,18,csektj,"[D] Multi-level data, what is the best approach?",https://www.reddit.com/r/MachineLearning/comments/csektj/d_multilevel_data_what_is_the_best_approach/,throwwawwayz123,1566206003,"Hi guys,

I'm working on a dataset and having some problems. I hope you can give me your insight.

So my objective is to predict customer churn based on incidents. Each incident is related to a contract which is related to a client. I need to predict the termination of the contract. The features can be grouped in 3 categories:

Client: client's ID and some basic information about them

Contract: contract's ID with their specific information and the target 'In service/Terminated'

Incidents: every entry is an incident related to a contract with information like number of calls, date of creation, last change, incident category

Some clients have up to 10 contracts, some contracts have up to 20 incidents.

What I did is create a fresh table with the contracts only (and client's information) and I now have to add relevant information for every contract.

I couldn't help but find myself cherry picking some 'relevant' information like: Total incidents for the contract, total calls, last incident's full information and also higher-level features like: number of contracts the user has, how much are terminated, total incidents for the user.

I feel it's getting very messy and I'm still losing A LOT of information by doing this. Is it the only approach I have?

This was supposed to be a machine learning problem but seriously there's nothing about machine learning at all, it's pure data science.",4,2
1130,2019-8-19,2019,8,19,18,cseu8y,Machine Learning for Intelligent Systems,https://www.reddit.com/r/MachineLearning/comments/cseu8y/machine_learning_for_intelligent_systems/,priyaleo,1566207854,,1,1
1131,2019-8-19,2019,8,19,19,csezrh,Can anyone helo me to learn machine learning from scratch ? I am a computer science student and I want to learn ML but I have no idea where to start and where to go !,https://www.reddit.com/r/MachineLearning/comments/csezrh/can_anyone_helo_me_to_learn_machine_learning_from/,north_ner,1566208931,[removed],0,1
1132,2019-8-19,2019,8,19,19,csf5pw,[P] torch-utils: Utility Functions that reoccur in my PyTorch Experiments,https://www.reddit.com/r/MachineLearning/comments/csf5pw/p_torchutils_utility_functions_that_reoccur_in_my/,slang03,1566210018,,1,1
1133,2019-8-19,2019,8,19,19,csf74g,Hospitals readmission prediction | Diabetes | Data Preprocessing,https://www.reddit.com/r/MachineLearning/comments/csf74g/hospitals_readmission_prediction_diabetes_data/,flyhighwithai,1566210277,,0,1
1134,2019-8-19,2019,8,19,19,csfd5j,Basic Guide To Starting A Career In Machine Learning,https://www.reddit.com/r/MachineLearning/comments/csfd5j/basic_guide_to_starting_a_career_in_machine/,kodygaylord,1566211410,,0,1
1135,2019-8-19,2019,8,19,21,csg3zs,100+ Basic Deep Learning Interview Questions and Answers,https://www.reddit.com/r/MachineLearning/comments/csg3zs/100_basic_deep_learning_interview_questions_and/,nkptcs,1566216039,,0,1
1136,2019-8-19,2019,8,19,21,csg40r,Small Automatic Screen Printing Machine,https://www.reddit.com/r/MachineLearning/comments/csg40r/small_automatic_screen_printing_machine/,rapidtag,1566216043,,0,1
1137,2019-8-19,2019,8,19,21,csgbxl,[D] Rectified Adam (RAdam): a new state of the art optimizer,https://www.reddit.com/r/MachineLearning/comments/csgbxl/d_rectified_adam_radam_a_new_state_of_the_art/,jwuphysics,1566217268,"https://medium.com/@lessw/new-state-of-the-art-ai-optimizer-rectified-adam-radam-5d854730807b

This blog post discusses a new optimizer built on top of Adam, [introduced in this paper by Liyuan Liu et al.](https://arxiv.org/abs/1908.03265v1). Essentially, they seek to understand why a warmup phase is beneficial for scheduling learning rates, and then identify the underlying problem to be related to high variance and poor generalization during the first few batches. They find that the issue can be remedied by using either a warmup/low initial learning rate, or by turning off momentum for the first couple of batches. As more training examples are fed in, the variance stabilizes and the learning rate/momentum can be increased. They therefore proposed a Rectified Adam optimizer that dynamically changes the momentum in a way that hedges against high variance. The author of the blog post tests an implementation in Fastai and finds that RAdam works well in many different contexts, enough to take the leaderboard of the Imagenette mini-competition. 

Implementations can be found on the [author's Github](https://github.com/LiyuanLucasLiu/RAdam).",105,252
1138,2019-8-19,2019,8,19,22,csgtaq,A reasonably good list of business machine learning vendors.,https://www.reddit.com/r/MachineLearning/comments/csgtaq/a_reasonably_good_list_of_business_machine/,OppositeMidnight,1566219911,,0,1
1139,2019-8-19,2019,8,19,22,csgvhc,[P] Top 1000 Business Machine Learning Companies,https://www.reddit.com/r/MachineLearning/comments/csgvhc/p_top_1000_business_machine_learning_companies/,OppositeMidnight,1566220215,,0,1
1140,2019-8-19,2019,8,19,22,cshaje,"If you know anyone who wants to get started with ML, Microsoft's Essential Math for Machine Learning course is fantastic: algebra, calculus, linear algebra, probability, and statistics in a concise, easy-to-understand package.",https://www.reddit.com/r/MachineLearning/comments/cshaje/if_you_know_anyone_who_wants_to_get_started_with/,Agent_ANAKIN,1566222357,,0,1
1141,2019-8-19,2019,8,19,22,cshdtp,[P] Ball &amp; beam gym - control theory lab simulations as Open AI gym environments,https://www.reddit.com/r/MachineLearning/comments/cshdtp/p_ball_beam_gym_control_theory_lab_simulations_as/,lilsmacky,1566222795,"While trying out reinforcement learning I built some custom ball &amp; beam environments since I was already familiar with it from control theory labs. I built it as a first order system where the angle of the beam is under full control (did not want to spend time simulating a motor). So it can be baselined by using a simple PID controller.

[https://github.com/simon-larsson/ballbeam-gym](https://github.com/simon-larsson/ballbeam-gym)

There are currently environments for three objectives:

* Balancing - just keeping the ball on beam
* Setpoint - keeping the ball as close as possible to a setpoint
* Throw - throwing the ball as far as possible to the right

The environments have two different state spaces. The agent can either use key-variables (position, velocity, angle) or the images from the visualization as state space.

Hope someone else wants to try it! :)",0,5
1142,2019-8-19,2019,8,19,22,cshghv,Cerebras Systems announces huge custom chip targeting ML training workloads,https://www.reddit.com/r/MachineLearning/comments/cshghv/cerebras_systems_announces_huge_custom_chip/,mabrowning,1566223181,,0,1
1143,2019-8-19,2019,8,19,23,cshjc3,My Monday ML motivation,https://www.reddit.com/r/MachineLearning/comments/cshjc3/my_monday_ml_motivation/,alanaparana,1566223542,"Just added this Chrome Extension to my browser for a little #MondayMotivation. Found it on product hunt, and wanted to share! :)

[https://www.producthunt.com/posts/ml-dictionary-2](https://www.producthunt.com/posts/ml-dictionary-2)",0,1
1144,2019-8-19,2019,8,19,23,cshoq5,[BLOG] Beginner's Guide To Exploratory Data Analysis.,https://www.reddit.com/r/MachineLearning/comments/cshoq5/blog_beginners_guide_to_exploratory_data_analysis/,YellowDragon420,1566224259,,0,1
1145,2019-8-19,2019,8,19,23,cshrn7,Galois: an auto code completer for code editors based on OpenAI GPT-2,https://www.reddit.com/r/MachineLearning/comments/cshrn7/galois_an_auto_code_completer_for_code_editors/,iedmrc,1566224638,[removed],0,1
1146,2019-8-19,2019,8,19,23,cshu53,Can anyone suggest me a good article on FaceApp's image filters with ai?,https://www.reddit.com/r/MachineLearning/comments/cshu53/can_anyone_suggest_me_a_good_article_on_faceapps/,debomastet335,1566224982,"Age filter,Smile filter,Hairstyle filter,Gender Swap filter,Skin tone Lightning filter etc.

How this works?
Any article or any information you know, please help me as I am writing a article on this topic.",0,1
1147,2019-8-20,2019,8,20,0,csiju4,B Kp Thng Carton - Xe Nng Kp Thng Carton Heli,https://www.reddit.com/r/MachineLearning/comments/csiju4/b_kp_thng_carton_xe_nng_kp_thng_carton_heli/,TranQuangHeli,1566228240,[removed],0,1
1148,2019-8-20,2019,8,20,0,csikeh,What are stuff that Neural Networks can and can't do?,https://www.reddit.com/r/MachineLearning/comments/csikeh/what_are_stuff_that_neural_networks_can_and_cant/,MidThought_Pause,1566228306,"I know that they're only good for classification and prediction.

Let's pretend I have an array of numbers.

    [1, 2, 3] 

I can make new arrays being switching 1, and 1 only, with a number that neighbors it. So like this:

    [2, 1, 3], and [2, 3, 1]. 

If I give a neural network a list of all the possible permutations of \[1,2,3\] can the neural network distinguish which are permutations that can be rearranged from the original array, versus permutations that are impossible to rearrange back to the original array? I can assign 1 to the former and 0 to the latter.

    dataset = [[[1,2,3], 1],
               [[2,1,3], 1],
               [[2,3,1], 1],
               [[3,2,1], 0],
               [[1,3,2], 0], 
               [[3,1,2], 0]]

Is it possible for neural networks to do this type of stuff with weights and biases alone?",0,1
1149,2019-8-20,2019,8,20,1,csj4b9,[D] SOTA topic extraction,https://www.reddit.com/r/MachineLearning/comments/csj4b9/d_sota_topic_extraction/,namnnumbr,1566230736,"TLDR: *Are there non-LDA algorithms for topic modeling that are performant or state-of-the-art?*

I'm working for a company that has a corpus of 10k articles for which they'd like to have topics identified and extracted.  The company is focused on our clientele; therefore the articles are already quite focused and topical (i.e., an engineering company would probably only write articles about engineering or engineering-adjacent things).  Essentially, I'm trying to mine articles for sub-topics within our area of expertise.

I'm aware of LDA/LDA2Vec for topic modeling.  In our case, since all of the articles are already of the same umbrella topic, the ""topics"" found via LDA tend to have an incredible amount of overlap relevance and salience metrics tend to prioritize words that relate both to the umbrella topic and the subtopic (unhelpful), or that are extremely rare occurrences (useless) - this is after multiple passes of filtering out frequent, rare, and low-value words.

I guess I'm hoping for something that either draws inferences from semantic meaning or uses a more sophisticated ""topic"" definition than probabilistic co-occurrence.

Thanks!",11,9
1150,2019-8-20,2019,8,20,1,csj4k1,Largest Chip Ever Built Tailored for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/csj4k1/largest_chip_ever_built_tailored_for_deep_learning/,artificial_intelect,1566230765,,1,1
1151,2019-8-20,2019,8,20,1,csjb3u,[N] Largest Chip Ever Made Tailored For Deep Learning,https://www.reddit.com/r/MachineLearning/comments/csjb3u/n_largest_chip_ever_made_tailored_for_deep/,artificial_intelect,1566231530,,0,2
1152,2019-8-20,2019,8,20,2,csjw66,Provide an example on how to implement a backend machine learning-driven data analysis platform?,https://www.reddit.com/r/MachineLearning/comments/csjw66/provide_an_example_on_how_to_implement_a_backend/,raf_nak,1566234000,,0,0
1153,2019-8-20,2019,8,20,2,csjwpy,[P] Google's wavenet API so good that it's synthetic speech can be used to train hotword detectors with no 'real' data?,https://www.reddit.com/r/MachineLearning/comments/csjwpy/p_googles_wavenet_api_so_good_that_its_synthetic/,VinayUPrabhu,1566234070," **TLDR**: *Google TTS -&gt; Simple Noise augment -&gt; {wav files} -&gt;SnowBoy -&gt;{.pmdl models} -&gt; Raspberry Pi*   


So, I trained a black-box deep net hotword detector (using Snowboy/kitt.ai) entirely out of synthetic speech samples generated using Google's Text-to-speech API and it was able to 'transfer to the real world' on a Raspberry Pi-3. Not entirely shocking. But reasonably neat I suppose given that you need to spend $0 for this. (Free GC credits + free 100 API calls from Snowboy + Colab)

I'd posit we are not too far off at least for this problem space from a point where we can directly do text-&gt;model generation directly, sans any data collection.

Blog:  [https://towardsdatascience.com/build-your-own-custom-hotword-detector-with-zero-training-data-and-0-35adfa6b25ea](https://towardsdatascience.com/build-your-own-custom-hotword-detector-with-zero-training-data-and-0-35adfa6b25ea)   
Code/Colab notebooks (pre-cleanup :P) : [https://github.com/vinayprabhu/BurningMan2019](https://github.com/vinayprabhu/BurningMan2019)",4,10
1154,2019-8-20,2019,8,20,2,csjzcv,BUILD FOR MACHINE LEARNING!?,https://www.reddit.com/r/MachineLearning/comments/csjzcv/build_for_machine_learning/,abshk_jr,1566234361,"Hi All,

This is my Freshman Year, and I am planning to build a desktop, the main purposes would be software development, and I will be focusing on Machine learning in the coming year. 

This is the build that I have planned currently 

 

CPU

AMD RYZEN 7 3700X

MOTHERBOARD

Asus ROG Strix B450-E Gaming (Wi-Fi)

(I chose this one, because it's a Tier II board, and VRAMs are gonna suffice for Overclocking with enough airflow. source :  [https://docs.google.com/spreadsheets/d/1wmsTYK9Z3-jUX5LGRoFnsZYZiW1pfiDZnKCjaXyzd1o/edit#gid=2112472504](https://docs.google.com/spreadsheets/d/1wmsTYK9Z3-jUX5LGRoFnsZYZiW1pfiDZnKCjaXyzd1o/edit#gid=2112472504) )

RAM

G.Skill Trident Z RGB 8GB DDR4 3200MHz

PSU

CORSAIR CX650

SSD

500GB SAMSUNG SSD 970 EVO PLUS NVME M.2

HDD

Seagate BarraCuda ST1000DM010 1 TB Internal Hard Drive

THERMAL COMPOUND

Noctua NT-H1 Thermal Compound

FANS

Cooler Master MasterFan MF120R ARGB (Triple Pack)

CABINET

Deepcool Matrexx 50 (Black)

&amp;#x200B;

GPU

I plan to buy something like an RX570 or GTX1060 for an year or two, then I will be upgrading to some higher card. Also I don't game frequently, but even if I do, it's CS:GO, which is not a much GPU taxing game.

I am from INDIA.

Open for all suggestions.",0,1
1155,2019-8-20,2019,8,20,2,csk3lr,"On-Device, Real-Time Hand Tracking with MediaPipe",https://www.reddit.com/r/MachineLearning/comments/csk3lr/ondevice_realtime_hand_tracking_with_mediapipe/,sjoerdapp,1566234859,,0,1
1156,2019-8-20,2019,8,20,2,cskkmt,[N] Cerebras Systems unveils a record 1.2 trillion transistor chip for AI,https://www.reddit.com/r/MachineLearning/comments/cskkmt/n_cerebras_systems_unveils_a_record_12_trillion/,MassivePellfish,1566236836,"New artificial intelligence company Cerebras Systems is unveiling the largest semiconductor chip ever built.

https://venturebeat.com/2019/08/19/cerebras-systems-unveils-a-record-1-2-trillion-transistor-chip-for-ai/",13,29
1157,2019-8-20,2019,8,20,2,cskntm,You use Python Environments with Conda ? here is how to manage them- part 2-,https://www.reddit.com/r/MachineLearning/comments/cskntm/you_use_python_environments_with_conda_here_is/,sajad-52,1566237200,,0,0
1158,2019-8-20,2019,8,20,3,cskt2l,[D] Is vision a solved problem?,https://www.reddit.com/r/MachineLearning/comments/cskt2l/d_is_vision_a_solved_problem/,Awill1aB,1566237808,"I am curious, as I've been thinking about this for a while. To me, it seems as though we seem to be making improvements, but there is not a ton left to solve within this sub field. I don't claim to be an expert by any stretch, but through all of the advancements we have made, we are capable of object detection, classification, image captioning for the contents of the image, image generation, and as we are closing in on depth perception and improvements on the 3D space, I feel like we are finding new applications for the tools we already have.

Thoughts?

I would love for someone to step in, call me a simpleton and give me all the reasons I am wrong and all of the problems we have yet to address within this space. :)",18,0
1159,2019-8-20,2019,8,20,3,cskw7d,[D] All papers claim that their NLP solutions exceed human comprehension. Is it true? Have we solved NLP?,https://www.reddit.com/r/MachineLearning/comments/cskw7d/d_all_papers_claim_that_their_nlp_solutions/,rafgro,1566238173,Every month I'm stumbling on at least a few papers with NLP models described as a state-of-the-art close to or even over human baseline. It seems like humanity should hail neural-net-overlords and retreat from any language-related work..,10,0
1160,2019-8-20,2019,8,20,3,cskyq6,"CrateDB, Machine Learning, and Hydroelectric Power: Part Two",https://www.reddit.com/r/MachineLearning/comments/cskyq6/cratedb_machine_learning_and_hydroelectric_power/,nachrieb,1566238459,,0,2
1161,2019-8-20,2019,8,20,3,cslcv3,Facial Recognition with OpenCV and Python (cv2),https://www.reddit.com/r/MachineLearning/comments/cslcv3/facial_recognition_with_opencv_and_python_cv2/,engineeringbigdata,1566240112,,0,1
1162,2019-8-20,2019,8,20,3,csletb,"[D] ""Inverse Design"" to create new optical chip components",https://www.reddit.com/r/MachineLearning/comments/csletb/d_inverse_design_to_create_new_optical_chip/,gburdell,1566240341,"I hope discussions of ML applications is OK in this sub.  I came across [this article recently](https://www.photonics.com/Articles/At_SPIE_Optics_Photonics_Going_Back_to_Get/a65024) about researchers in the field of photonics, which doesn't have a lot of analytical equations to calculate performance by hand, using some basic ML techniques to create high performance components for photonic integrated circuits.  They start with a black box, feed in the desired output performance, and then use basic electromagnetic boundary conditions and ML to work backward to what would be required to get there.  They call this ""inverse design"".

This paper goes into it a little more and shows an example of the result of the technique: https://arxiv.org/pdf/1504.00095.pdf",1,3
1163,2019-8-20,2019,8,20,3,csljw6,Of Mice and Machines: Can AI Read Rodents Minds?,https://www.reddit.com/r/MachineLearning/comments/csljw6/of_mice_and_machines_can_ai_read_rodents_minds/,Yuqing7,1566240947,,0,1
1164,2019-8-20,2019,8,20,4,cslrbh,[D] Is here something advanced for solving tabular data classification by neuro nets?,https://www.reddit.com/r/MachineLearning/comments/cslrbh/d_is_here_something_advanced_for_solving_tabular/,hadaev,1566241818,"I found only embeddings for categorical features.

Where are so many nets for pictures, but looks like for tabular data peoples just stack dense layers with random hyperparameters.

Am I miss something?",16,0
1165,2019-8-20,2019,8,20,5,csmjjd,Razer blade 15 oled or MacBook Pro 15 2019,https://www.reddit.com/r/MachineLearning/comments/csmjjd/razer_blade_15_oled_or_macbook_pro_15_2019/,ewelumokeke,1566245171,[removed],0,1
1166,2019-8-20,2019,8,20,5,csmjyq,Ride the AI tsunami: Introduction to Artificial Intelligence from Microsoft Professional Program  boost your career and income growth,https://www.reddit.com/r/MachineLearning/comments/csmjyq/ride_the_ai_tsunami_introduction_to_artificial/,internetdigitalentre,1566245223,[removed],0,1
1167,2019-8-20,2019,8,20,5,csmpka,Welcome to the DeepMind podcast,https://www.reddit.com/r/MachineLearning/comments/csmpka/welcome_to_the_deepmind_podcast/,Cock-tail,1566245890,,0,1
1168,2019-8-20,2019,8,20,5,csn4ck,A Gentle Introduction to Machine Learning - Yasin Ceran Part (4/5),https://www.reddit.com/r/MachineLearning/comments/csn4ck/a_gentle_introduction_to_machine_learning_yasin/,Magniminda,1566247620,,0,1
1169,2019-8-20,2019,8,20,6,csnx1y,What is the fastest way to understand a reinforcement learning repository and modify it as per your use case?,https://www.reddit.com/r/MachineLearning/comments/csnx1y/what_is_the_fastest_way_to_understand_a/,tellMeWhySo,1566251093,[removed],0,0
1170,2019-8-20,2019,8,20,7,csorwf,"Can Smart Watch sensors help recognise the act of nose picking through machine learning??? Have long been curious of the possiblity of the smart watches ability to recognise and help overcome daily bad and discusting habits, through Pavlos inspired digital punishments...",https://www.reddit.com/r/MachineLearning/comments/csorwf/can_smart_watch_sensors_help_recognise_the_act_of/,AmIgnorent,1566254859,,0,1
1171,2019-8-20,2019,8,20,8,cspfjo,Face and Eye Detection using OpenCV and Python (cv2) [OC],https://www.reddit.com/r/MachineLearning/comments/cspfjo/face_and_eye_detection_using_opencv_and_python/,engineeringbigdata,1566257807,,1,1
1172,2019-8-20,2019,8,20,11,csro1n,Machine Learning Projects related to Real Estate,https://www.reddit.com/r/MachineLearning/comments/csro1n/machine_learning_projects_related_to_real_estate/,Hyperduckultimate,1566269033,I want to get into machine learning and I want it to relate it to real estate data. So far I thought of making a model that predicts home prices based on different factors. Do any of you guys have more ideas that can be useful to the real estate field?,5,1
1173,2019-8-20,2019,8,20,12,css2jb,[N] Trump falsely claims Google 'manipulated' millions of 2016 votes,https://www.reddit.com/r/MachineLearning/comments/css2jb/n_trump_falsely_claims_google_manipulated/,errorsignal,1566271034,"[https://www.cnn.com/2019/08/19/politics/trump-google-manipulated-votes-claim/index.html](https://www.cnn.com/2019/08/19/politics/trump-google-manipulated-votes-claim/index.html)

The referenced article: [https://aibrt.org/downloads/EPSTEIN\_et\_al\_2017-SUMMARY-A\_Method\_for\_Detecting\_Bias\_in\_Search\_Rankings-EMBARGOED\_until\_March\_14\_2017.pdf](https://aibrt.org/downloads/EPSTEIN_et_al_2017-SUMMARY-A_Method_for_Detecting_Bias_in_Search_Rankings-EMBARGOED_until_March_14_2017.pdf)

Key point from the article referenced by CNN's story:  *Was the bias the same for all search engines? No. The level of pro-Clinton bias we found on Google (0.19) was more than twice as high as the level of pro-Clinton bias we found on Yahoo (0.09).*

Among other issues, one thing that CNN did not mention is the presumption that Google is wrong, Yahoo correct, given that there is no ground truth to compare to. Perhaps there were more pro-Clinton articles and news appearing those days. And more generally, I might guess that Yahoo's and Google's engines are simply different algorithms showing different things.

Before someone complains: yes, pagerank was considered ""machine learning"", though not deep learning of course. Though it feels more like graph theory to me.",0,0
1174,2019-8-20,2019,8,20,12,cssgce,Rapid large-scale fractional differencing to minimize memory loss while making a time series stationary. 6x-400x speed up over CPU implementation.,https://www.reddit.com/r/MachineLearning/comments/cssgce/rapid_largescale_fractional_differencing_to/,ritchieng,1566273248,"Happy to launch GFD: GPU-accelerated Fractional Differencing. A substantial 6x-400x speed-up for single GPU RAPIDS cuDF implementation over NumPy/Pandas CPU-implementation. 

Feel free to play with the code on Google Colab, run it on GCP/AWS or your local machine with the entirely self-contained notebook.  


**Summary**

Typically we attempt to achieve some form of stationarity via a transformation on our time series through common methods including integer differencing. However, integer differencing unnecessarily removes too much memory to achieve stationarity. An alternative, fractional differencing, allows us to achieve stationarity while maintaining the maximum amount of memory compared to integer differencing. While existing CPU-based implementations are inefficient for running fractional differencing on many large-scale time series, our GPU-based implementation enables rapid fractional differencing of up to 400x faster on a single machine.  


**Code**

[https://github.com/ritchieng/fractional\_differencing\_gpu](https://github.com/ritchieng/fractional_differencing_gpu)  


**Presentation**

[https://www.researchgate.net/publication/335159299\_GFD\_GPU\_Fractional\_Differencing\_for\_Rapid\_Large-scale\_Stationarizing\_of\_Time\_Series\_Data\_while\_Minimizing\_Memory\_Loss](https://www.researchgate.net/publication/335159299_GFD_GPU_Fractional_Differencing_for_Rapid_Large-scale_Stationarizing_of_Time_Series_Data_while_Minimizing_Memory_Loss)",0,1
1175,2019-8-20,2019,8,20,13,cssoc5,What Exactly is Machine Learning? A Definition - TheHackTech,https://www.reddit.com/r/MachineLearning/comments/cssoc5/what_exactly_is_machine_learning_a_definition/,Rahimansari031,1566275108,,0,1
1176,2019-8-20,2019,8,20,13,cssqhf,[D] Are there any GAN's out there capable of transitioning between images?,https://www.reddit.com/r/MachineLearning/comments/cssqhf/d_are_there_any_gans_out_there_capable_of/,Mangelius,1566275653,"Been looking at the results from crossbreeding images via GAN Breeder, with the obvious limitation of not being able to supply your own images.  Wondering if there's any workarounds to this or other GAN applications where this would be possible.  Would like to use this tech in motion graphics for a spot we're currently working on, but am struggling to find a way to implement it.  Any advice appreciated.  


Cheers!",0,1
1177,2019-8-20,2019,8,20,14,cst4i7,6 Types of Artificial Neural Networks Currently Being Used in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cst4i7/6_types_of_artificial_neural_networks_currently/,subhamroy021,1566279395,,0,0
1178,2019-8-20,2019,8,20,15,csthlg,Why is Sentiment Analysis crucial for Chatbots?,https://www.reddit.com/r/MachineLearning/comments/csthlg/why_is_sentiment_analysis_crucial_for_chatbots/,getengati,1566281386,[removed],0,1
1179,2019-8-20,2019,8,20,15,cstw8d,This hand-tracking algorithm could lead to sign language recognition,https://www.reddit.com/r/MachineLearning/comments/cstw8d/this_handtracking_algorithm_could_lead_to_sign/,makereven,1566283853," A new advance in real-time **hand tracking** from Google's AI labs, however, **could** be the breakthrough some have been waiting for. 

 [https://techcrunch.com/2019/08/19/this-hand-tracking-algorithm-could-lead-to-sign-language-recognition/](https://techcrunch.com/2019/08/19/this-hand-tracking-algorithm-could-lead-to-sign-language-recognition/) 

&amp;#x200B;

&amp;#x200B;

![img](70j4ct6awjh31)

&amp;#x200B;

![gif](yb9rtftiwjh31)",2,3
1180,2019-8-20,2019,8,20,16,cstz8b,How to Reconstruct 3d face from 2d images | Point cloud Matlab,https://www.reddit.com/r/MachineLearning/comments/cstz8b/how_to_reconstruct_3d_face_from_2d_images_point/,flyhighwithai,1566284591,,0,1
1181,2019-8-20,2019,8,20,16,csu1qf,[D] Which SOTA authorship attribution / text classification model to use?,https://www.reddit.com/r/MachineLearning/comments/csu1qf/d_which_sota_authorship_attribution_text/,mikkelmedm,1566285253,"Hello :) 
I'm currently doing research for my thesis project, and was wondering which models to experiment with. 
I have a large dataset of political speeches (around 180.000) annotated with the respective party (10 parties total), and would like a model to learn to classify each party given the speeches. 

My question is, which model is currently best for this type of task? I have some experience with Bi-LSTM models, and also CNN with LSTM - however I'm very interested if other models would perform better at this task, or if you any experience with the architecture of these type of models?",13,14
1182,2019-8-20,2019,8,20,16,csu266,Open-Unmix - Music Source Separation for PyTorch,https://www.reddit.com/r/MachineLearning/comments/csu266/openunmix_music_source_separation_for_pytorch/,data-alchemy,1566285388,"[https://github.com/sigsep/open-unmix-pytorch](https://github.com/sigsep/open-unmix-pytorch)

Hello everyone. Work of a friend who has been fighting music source separation for the last 4 years.  Here are the results,  code &amp; pre-trained model,and you can directly play with a Google Colab at [https://colab.research.google.com/drive/1mijF0zGWxN-KaxTnd0q6hayAlrID5fEQ](https://colab.research.google.com/drive/1mijF0zGWxN-KaxTnd0q6hayAlrID5fEQ)

We've been playing (artistically) with this stuff for the last 3 years and it has been a lot of fun, following the algorithm progress. Today, depending of the input audio file, you can have some pretty results :)",0,2
1183,2019-8-20,2019,8,20,16,csu2bh,Vibration Filler - Today Machine,https://www.reddit.com/r/MachineLearning/comments/csu2bh/vibration_filler_today_machine/,KEVINCHEN131,1566285432,,1,1
1184,2019-8-20,2019,8,20,16,csu2j3,How to Reconstruct 3d Face From 2d Images | Point Cloud Matlab,https://www.reddit.com/r/MachineLearning/comments/csu2j3/how_to_reconstruct_3d_face_from_2d_images_point/,flyhighwithai,1566285476,,0,1
1185,2019-8-20,2019,8,20,16,csu5c9,[D] Why isn't bayesian inference using Gibbs Sampling / MCMC / HMC done on GPUs?,https://www.reddit.com/r/MachineLearning/comments/csu5c9/d_why_isnt_bayesian_inference_using_gibbs/,sicp4lyfe,1566285925,"I've seen Multibugs which claims to achieve impressive speedups by exploiting multicore but for the most part, i've not seen any of the existing Bayesian Inference leverage the GPU. Does anyone know why or why not?",46,65
1186,2019-8-20,2019,8,20,16,csuajf,How to Increase Network Lifetime of LEACH 300% (WSN),https://www.reddit.com/r/MachineLearning/comments/csuajf/how_to_increase_network_lifetime_of_leach_300_wsn/,flyhighwithai,1566286234,,0,1
1187,2019-8-20,2019,8,20,16,csud3c,Volumetric Cup Filler - Today Machine,https://www.reddit.com/r/MachineLearning/comments/csud3c/volumetric_cup_filler_today_machine/,KEVINCHEN131,1566286575,,0,1
1188,2019-8-20,2019,8,20,17,csuouf,Linear Weigher - Today Machine,https://www.reddit.com/r/MachineLearning/comments/csuouf/linear_weigher_today_machine/,KEVINCHEN131,1566288275,,0,1
1189,2019-8-20,2019,8,20,17,csv1wa,Learn how ParallelDots has leveraged state-of-the-art Deep Learning technologies to create an NER model that works wonders across multiple languages.,https://www.reddit.com/r/MachineLearning/comments/csv1wa/learn_how_paralleldots_has_leveraged/,anantcoolblabla,1566290253,,0,1
1190,2019-8-20,2019,8,20,18,csvqct,Just released my project which I was working on as a pip package,https://www.reddit.com/r/MachineLearning/comments/csvqct/just_released_my_project_which_i_was_working_on/,MukundhBhushan,1566293804,[removed],0,1
1191,2019-8-20,2019,8,20,19,csw0f6,Edit facial attribution by StyleGAN,https://www.reddit.com/r/MachineLearning/comments/csw0f6/edit_facial_attribution_by_stylegan/,shartoo,1566295258,,1,1
1192,2019-8-20,2019,8,20,19,cswas6,SIDNet: object detection at 625 fps (run on NVIDIA Tesla V100) using INT8,https://www.reddit.com/r/MachineLearning/comments/cswas6/sidnet_object_detection_at_625_fps_run_on_nvidia/,adriacabeza,1566296667,,0,1
1193,2019-8-20,2019,8,20,19,cswelv,Looking for hand drawn wireframe dataset,https://www.reddit.com/r/MachineLearning/comments/cswelv/looking_for_hand_drawn_wireframe_dataset/,WebAI,1566297236,[removed],0,0
1194,2019-8-20,2019,8,20,19,cswenh,[R] Video Frame Interpolation via Cyclic Fine-Tuning and Asymmetric Reverse Flow,https://www.reddit.com/r/MachineLearning/comments/cswenh/r_video_frame_interpolation_via_cyclic_finetuning/,mohanne,1566297241,"Want to convert your video to slowmotion?  
[https://github.com/MortenHannemose/pytorch-vfi-cft](https://github.com/MortenHannemose/pytorch-vfi-cft)",17,35
1195,2019-8-20,2019,8,20,19,cswg4y,[D] Tools that allow 1-click deployment of pytorch/TF code?,https://www.reddit.com/r/MachineLearning/comments/cswg4y/d_tools_that_allow_1click_deployment_of_pytorchtf/,cbsudux,1566297444,[removed],0,1
1196,2019-8-20,2019,8,20,19,cswlhk, Episode 3: Life is like a game,https://www.reddit.com/r/MachineLearning/comments/cswlhk/episode_3_life_is_like_a_game/,sjoerdapp,1566298173,,0,1
1197,2019-8-20,2019,8,20,20,cswwst,[Discussion] Is Sagemaker just a glorified EC2 instance?,https://www.reddit.com/r/MachineLearning/comments/cswwst/discussion_is_sagemaker_just_a_glorified_ec2/,AlexSnakeKing,1566299654,"I'm data scientist with a lot of model and math knowledge, and experience with mostly on-prem tools and some GCP. I'm trying to pick up more cloud skills. As I'm experimenting more with Sagemaker, I can figure out how it is more than just an EC2 instance with the right libraries installed. Is there anything more to it? What am I missing?",15,11
1198,2019-8-20,2019,8,20,20,cswyo0,[D] Symmetry-equivalent representations,https://www.reddit.com/r/MachineLearning/comments/cswyo0/d_symmetryequivalent_representations/,throwervek,1566299895,"I'm training regression model, learning the mapping from integer-valued vectors to a single real-valued property. All cyclic permutations of my feature vectors are equivalent, that is they have the same y. I'm a bit lost in trying encode this. 

One idea I had was to augment the dataset by generating all the cyclic permutations, but I don't think this is a good way to go at all. I've stumbled on strategies to encode cyclic features such as months by mapping them to a periodic function, but in my case this wouldn't work as the elements of my vector have a different meaning.",4,1
1199,2019-8-20,2019,8,20,20,csxfgx,Types of Machine Learning !!,https://www.reddit.com/r/MachineLearning/comments/csxfgx/types_of_machine_learning/,freevideolectures,1566301958,,0,1
1200,2019-8-20,2019,8,20,20,csxgey,"[D] Uncover new, more meaningful KPIs with Machine Learning",https://www.reddit.com/r/MachineLearning/comments/csxgey/d_uncover_new_more_meaningful_kpis_with_machine/,seemingly_omniscient,1566302056," It is well known that machine learning is already helping companies achieve their performance goals by optimizing existing performance metrics. By leveraging the growing volume of data on customer behavior, pricing, competitive action, and operational statistics, it can deliver critical insights in a variety of ways. Machine learning offers many benefits from optimizing marketing or pricing to improving customer service and operational efficiency. However, a recent article in the [MIT Sloan Management Review](https://sloanreview.mit.edu/article/improving-strategic-execution-with-machine-learning/) shows that companies are increasingly using machine learning to identify entirely new KPIs to correlate with overall performance. 

Read more:  [https://www.aisoma.de/uncover-kpis-with-machine-learning/](https://www.aisoma.de/uncover-kpis-with-machine-learning/)",0,0
1201,2019-8-20,2019,8,20,21,csxnzg,The State of the art in speech emotion recognition,https://www.reddit.com/r/MachineLearning/comments/csxnzg/the_state_of_the_art_in_speech_emotion_recognition/,AmrMKayid,1566302940,[removed],0,1
1202,2019-8-20,2019,8,20,21,csy69s,"Grover's Algorithm is interesting because it essentially works backwards, determining unique inputs when it is the outputs that are known.",https://www.reddit.com/r/MachineLearning/comments/csy69s/grovers_algorithm_is_interesting_because_it/,Agent_ANAKIN,1566305070,,0,1
1203,2019-8-20,2019,8,20,22,csypgi,"Real-world AI Challenge with the UN Refugee Agency to predict climate change, forced displacement, and violent conflicts.",https://www.reddit.com/r/MachineLearning/comments/csypgi/realworld_ai_challenge_with_the_un_refugee_agency/,Lordobba,1566307206,,0,1
1204,2019-8-20,2019,8,20,22,csytp5,Machine Learning and Data Science Applications in Industry,https://www.reddit.com/r/MachineLearning/comments/csytp5/machine_learning_and_data_science_applications_in/,techpreneur_13,1566307670,,0,1
1205,2019-8-20,2019,8,20,22,csz17p,[P] Train CIFAR10 to 94% in 26 SECONDS on a single-GPU,https://www.reddit.com/r/MachineLearning/comments/csz17p/p_train_cifar10_to_94_in_26_seconds_on_a_singlegpu/,youali,1566308493,"In this blog post, the author introduces a bag of standard and not-so-standard tricks to reduce training time to 34s of a Resnet model on CIFAR10 dataset, or 26s with test-time augmentation.

&amp;nbsp;

**Blog post:** https://myrtle.ai/how-to-train-your-resnet-8-bag-of-tricks/

**Colab notebook:** https://colab.research.google.com/github/davidcpage/cifar10-fast/blob/master/bag_of_tricks.ipynb

&amp;nbsp;

Author: David Page

Original tweet: https://twitter.com/dcpage3/status/1163563850442182657",14,178
1206,2019-8-20,2019,8,20,23,csziqo,       ,https://www.reddit.com/r/MachineLearning/comments/csziqo/_______/,You98tube917,1566310405,,0,1
1207,2019-8-20,2019,8,20,23,cszkn8,Orchestrating Streaming and Batch ETL for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cszkn8/orchestrating_streaming_and_batch_etl_for_machine/,tzimmzzzz,1566310604,,0,1
1208,2019-8-20,2019,8,20,23,cszo8t,[P] Tensorflow live video generation,https://www.reddit.com/r/MachineLearning/comments/cszo8t/p_tensorflow_live_video_generation/,Remideza,1566310988,"I am currently working a project to generate live video clips using biggan

I started by using the default code from  the [deepmind colab](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb) but i am now facing a optimization problem

I need to have as many fps but doing a sess.run() for every frame is very heavy, i tried looking on maybe converting the model to a TF Lite model, but i didn't find any information on how to do it from a tensorflow hub module

I'm pretty new to tensorflow and i didn't find any other idea to improve my code, so i would apreciate any help",0,2
1209,2019-8-20,2019,8,20,23,cszpox,Can you rate our ML presentation?,https://www.reddit.com/r/MachineLearning/comments/cszpox/can_you_rate_our_ml_presentation/,DrYamuz,1566311147,[removed],0,1
1210,2019-8-20,2019,8,20,23,cszqgm,InspiroBot Really work this one out.,https://www.reddit.com/r/MachineLearning/comments/cszqgm/inspirobot_really_work_this_one_out/,TwistCable,1566311222,,0,1
1211,2019-8-20,2019,8,20,23,cszsef,[D] SELUs don't actually solve the dying ReLU problem,https://www.reddit.com/r/MachineLearning/comments/cszsef/d_selus_dont_actually_solve_the_dying_relu_problem/,relgukxilef,1566311439,"One frequently mentioned problem with ReLUs is that they can get stuck outputting nothing but 0s when their input shifts such that every value is negative. SELUs \[1\] claim to solve this problem. 

However, there is another way that activation functions can stop being useful to the network: when they degenerate to a linear function. This can happen with ReLUs, SELUs and some other activation functions when their input shifts such that every value is positive. To demonstrate this I made a simple toy network.

The task is to approximate the ReLU function itself with the function `f(x * a + b) * c + d`, where x is the input, a, b, c and d are learned scalar values and f is an activation function. Values for x are uniformly chosen from the range \[-0.5, 0.5\].

If we start with a = 1 and b = 0.5 then all inputs to f will be positive. For many starting points of c and d this will still converge when ReLU is used for f. But for c = 1 and d = -0.5 all common piecewise activation functions will fail, including SELU and ELU.

However, there is a potential activation function that does not exhibit that problem, that I don't see being talked about a lot: Softplus, defined as log(exp(x) + 1). Its derivative is strictly monotonically increasing and therefor non-linear in every sub range. Using softplus in place of f in the toy example allows it to converge from any starting point. *\[proof pending\]*

In the following images you can see the learned function at different numbers of iterations. The starting point a = 1, b = 0.5, c = 1 and d = -0.5 was used. All use Adam optimizer with a learning rate of 0.1 and default values for alpha and beta. The mean absolute difference is minimized. Tensorflow 1.14.0 was used.

[ReLU](https://i.redd.it/ln4smmm53mh31.png)

[SELU](https://i.redd.it/vmb47tjc3mh31.png)

[Softplus](https://i.redd.it/15veyxel2mh31.png)

In practice the inputs to activation functions may follow a long tail distribution making this very unlikely when the loss function is fixed. But for some problems, like adversarial networks, where the loss function itself is learned, this might not be the case. 

There are even situations where SELU fails to converge whereas ReLU and ELU do. The following images use the starting point a = 1, b = 0.5, c = 1 and **d = 0**. Again, all initial inputs to the activation function are positive. As we can see this does not necessarily mean that it is stuck.

[ReLU. The slight curve at 0 is a result of under-sampling the function.](https://i.redd.it/ga1s4qjf4mh31.png)

[SELU. Note how the initial increase in gradient below 0 creates an insurmountable wall of increased loss that gradient descent can't overcome. ](https://i.redd.it/9y1sehbq4mh31.png)

[ELU. By having a monotonic gradient it does not have the same problem as SELU.](https://i.redd.it/5sgquu7y4mh31.png)

[Softplus](https://i.redd.it/mj48idbd5mh31.png)

Alternative title for this post: SELU considered harmful.

\[1\] [https://arxiv.org/abs/1706.02515](https://arxiv.org/abs/1706.02515)",12,27
1212,2019-8-20,2019,8,20,23,cszugx,AI Generated T-Shirt designs,https://www.reddit.com/r/MachineLearning/comments/cszugx/ai_generated_tshirt_designs/,vladgl94,1566311671,,1,1
1213,2019-8-20,2019,8,20,23,cszyym,[N] Legendary venture capitalist Pierre Lamond on the release of the worlds largest machine learning chip...what are your guys thoughts?,https://www.reddit.com/r/MachineLearning/comments/cszyym/n_legendary_venture_capitalist_pierre_lamond_on/,YourAsssIsGrasss,1566312135,,0,1
1214,2019-8-20,2019,8,20,23,ct006u,[P] AI Generated T-Shirt designs,https://www.reddit.com/r/MachineLearning/comments/ct006u/p_ai_generated_tshirt_designs/,vladgl94,1566312270,,0,1
1215,2019-8-20,2019,8,20,23,ct05jv,[D] Node Embedding and GNN for Graphs with both Continuous and Categorical Atrributes,https://www.reddit.com/r/MachineLearning/comments/ct05jv/d_node_embedding_and_gnn_for_graphs_with_both/,suddenintent,1566312834,"My problem consists of classifying nodes of graphs (number of vertices &lt;500) , nodes are attributed with both Continuous (some numerical features) and Categorical (CountVectorizer) features. I've looked in to various articles both about node embedding and graph neural networks but I'm still not sure what algorithms will be the best fit for my task.

What is your recommended approach for my problem?

Sorry for my short explanation, please ask me if you need more details.",7,2
1216,2019-8-21,2019,8,21,0,ct0lfj,[N] DeepMind dropped 4 episodes of a new AI podcasts today,https://www.reddit.com/r/MachineLearning/comments/ct0lfj/n_deepmind_dropped_4_episodes_of_a_new_ai/,Piyh,1566314481,,0,1
1217,2019-8-21,2019,8,21,0,ct0tru,Selecting the right weight initialization for your deep neural network,https://www.reddit.com/r/MachineLearning/comments/ct0tru/selecting_the_right_weight_initialization_for/,CometML,1566315354,,0,1
1218,2019-8-21,2019,8,21,1,ct1det,GPT-2: 6-Month Follow-Up | OpenAI releases 774 million parameter model,https://www.reddit.com/r/MachineLearning/comments/ct1det/gpt2_6month_followup_openai_releases_774_million/,Rocketshipz,1566317370,,1,3
1219,2019-8-21,2019,8,21,1,ct1gfd,Am I aiming too high? Picking universities for ML/NLP.,https://www.reddit.com/r/MachineLearning/comments/ct1gfd/am_i_aiming_too_high_picking_universities_for/,revererosie,1566317685,,0,1
1220,2019-8-21,2019,8,21,1,ct1gnv,[D] Reflinks vs. Symlinks vs. Hard Links: How They Can Help ML Projects,https://www.reddit.com/r/MachineLearning/comments/ct1gnv/d_reflinks_vs_symlinks_vs_hard_links_how_they_can/,cmstrump,1566317707,"In ML projects hard links and symbolic links can help us, when setting up new experiments, to rearrange data files quickly and efficiently. However, with traditional links, we run the risk of polluting the data files with erroneous edits.

The article explain details of using links, some cool new stuff in modern file systems (reflinks), and an example of how [DVC](https://dvc.org) (Data Version Control) tool leverages this for managing ML project datasets and workflow: [Reflinks vs symlinks vs hard links, and how they can help machine learning projects](https://hackernoon.com/reflinks-vs-symlinks-vs-hard-links-and-how-they-can-help-machine-learning-projects-wz2ej3xa7)",0,1
1221,2019-8-21,2019,8,21,1,ct1pqu,[R] Facebook AI Releases XLM/mBERT PyTorch Models in 100 Languages!!!,https://www.reddit.com/r/MachineLearning/comments/ct1pqu/r_facebook_ai_releases_xlmmbert_pytorch_models_in/,omarsar,1566318646,,0,1
1222,2019-8-21,2019,8,21,1,ct20te,[D] what is the difference between Machine Learning and Data Science ?,https://www.reddit.com/r/MachineLearning/comments/ct20te/d_what_is_the_difference_between_machine_learning/,nemoonemoo,1566319781,what is the difference between Machine Learning and Data Science ?,2,0
1223,2019-8-21,2019,8,21,1,ct20xp,Using autoencoders on the periodic table??,https://www.reddit.com/r/MachineLearning/comments/ct20xp/using_autoencoders_on_the_periodic_table/,spad067,1566319791,[removed],0,1
1224,2019-8-21,2019,8,21,1,ct26af,Ways AI and machine learning are facilitating average peoples lives,https://www.reddit.com/r/MachineLearning/comments/ct26af/ways_ai_and_machine_learning_are_facilitating/,Magniminda,1566320345,[removed],0,1
1225,2019-8-21,2019,8,21,2,ct2crr,GPT-2: 6-Month Follow-Up [OpenAI],https://www.reddit.com/r/MachineLearning/comments/ct2crr/gpt2_6month_followup_openai/,Crul_,1566321008,,0,1
1226,2019-8-21,2019,8,21,2,ct2cx7,"Turbo, An Improved Rainbow Colormap for Visualization",https://www.reddit.com/r/MachineLearning/comments/ct2cx7/turbo_an_improved_rainbow_colormap_for/,sjoerdapp,1566321021,,0,1
1227,2019-8-21,2019,8,21,2,ct2o9h,[D] Why is KL Divergence so popular?,https://www.reddit.com/r/MachineLearning/comments/ct2o9h/d_why_is_kl_divergence_so_popular/,LemonByte,1566322173,"In most objective functions comparing a learned and source probability distribution, KL divergence is used to measure their dissimilarity. What advantages does KL divergence have over true metrics like Wasserstein (earth mover's distance), and Bhattacharyya? Is its asymmetry actually a desired property because the fixed source distribution should be treated differently compared to a learned distribution?",72,170
1228,2019-8-21,2019,8,21,2,ct2vw3,[N] OpenAI is releasing the GPT-2 774M parameter model,https://www.reddit.com/r/MachineLearning/comments/ct2vw3/n_openai_is_releasing_the_gpt2_774m_parameter/,[deleted],1566322937,[deleted],0,1
1229,2019-8-21,2019,8,21,2,ct30r0,Should I include ML weights in my docker container image?,https://www.reddit.com/r/MachineLearning/comments/ct30r0/should_i_include_ml_weights_in_my_docker/,aaronjl33,1566323421,[removed],0,1
1230,2019-8-21,2019,8,21,2,ct327n,[N] OpenAI is releasing the GPT-2 774M parameter model,https://www.reddit.com/r/MachineLearning/comments/ct327n/n_openai_is_releasing_the_gpt2_774m_parameter/,PyroLurker,1566323566,,0,1
1231,2019-8-21,2019,8,21,3,ct3d8t,Do you study Machine Learning ? here are the 3 best machine learning books ! with download link,https://www.reddit.com/r/MachineLearning/comments/ct3d8t/do_you_study_machine_learning_here_are_the_3_best/,sajad-52,1566324730,,0,1
1232,2019-8-21,2019,8,21,3,ct3vy8,A Neural Net compiler for low-memory/low-processing powered embedded devices,https://www.reddit.com/r/MachineLearning/comments/ct3vy8/a_neural_net_compiler_for_lowmemorylowprocessing/,en1gmarikki,1566326629,[removed],0,1
1233,2019-8-21,2019,8,21,3,ct41o0,[N] OpenAI releases 774m parameter GPT-2 model,https://www.reddit.com/r/MachineLearning/comments/ct41o0/n_openai_releases_774m_parameter_gpt2_model/,[deleted],1566327222,,0,1
1234,2019-8-21,2019,8,21,4,ct49c1,How to create a resume skill's classifier?,https://www.reddit.com/r/MachineLearning/comments/ct49c1/how_to_create_a_resume_skills_classifier/,AndersonHqds,1566328006,[removed],0,1
1235,2019-8-21,2019,8,21,4,ct4j3j,Infinite Staircase Problem,https://www.reddit.com/r/MachineLearning/comments/ct4j3j/infinite_staircase_problem/,Saswataunlimited,1566329019,,0,1
1236,2019-8-21,2019,8,21,4,ct4jer,[P] My first 'real' Machine Learning project,https://www.reddit.com/r/MachineLearning/comments/ct4jer/p_my_first_real_machine_learning_project/,iMilchshake,1566329048,"This is a quick showcase of my first 'real' Machine Learning project: 'Jumping over obstacles':

[https://youtu.be/73ekxXGMpMc](https://youtu.be/73ekxXGMpMc)

Everything was done from scratch using the Unity3D engine (C#).

Any Feedback or Questions will be appreciated!",11,3
1237,2019-8-21,2019,8,21,4,ct4jhn,Seoul National University AI Uses Text Tags to Colourize Line Drawings,https://www.reddit.com/r/MachineLearning/comments/ct4jhn/seoul_national_university_ai_uses_text_tags_to/,Yuqing7,1566329058,,0,1
1238,2019-8-21,2019,8,21,4,ct4srt,Quaternion-Recurrent-Neural-Networks,https://www.reddit.com/r/MachineLearning/comments/ct4srt/quaternionrecurrentneuralnetworks/,ml23453425245,1566330008,[removed],0,1
1239,2019-8-21,2019,8,21,4,ct4yv9,Question on Quaternion-Recurrent-Neural-Networks,https://www.reddit.com/r/MachineLearning/comments/ct4yv9/question_on_quaternionrecurrentneuralnetworks/,ml23453425245,1566330618,[removed],0,1
1240,2019-8-21,2019,8,21,5,ct58f1,[D] Should I include my weights inside my docker container?,https://www.reddit.com/r/MachineLearning/comments/ct58f1/d_should_i_include_my_weights_inside_my_docker/,aaronjl33,1566331579,"I am running my ML inference inside a docker container. Should I include my weights in the image, or should I download them from S3 when the container starts up? From what I can see, the benefits are as follows:

Pros for including: faster startup times since I don't need to download after startup. Less dependencies since everything is included in the container image

Pros for downloading: separation of weights and code. Easier weight tweaking since I won't need to redeploy image when changing weights

Thoughts?",7,0
1241,2019-8-21,2019,8,21,5,ct5hv8,Question on Quaternion-Recurrent-Neural-Networks,https://www.reddit.com/r/MachineLearning/comments/ct5hv8/question_on_quaternionrecurrentneuralnetworks/,ml23453425245,1566332542,[removed],0,1
1242,2019-8-21,2019,8,21,5,ct5okz,[D] Question on Quaternion-Recurrent-Neural-Networks,https://www.reddit.com/r/MachineLearning/comments/ct5okz/d_question_on_quaternionrecurrentneuralnetworks/,ml23453425245,1566333232,[removed],0,1
1243,2019-8-21,2019,8,21,5,ct5ufr,[D] Question on Quaternion-Recurrent-Neural-Networks,https://www.reddit.com/r/MachineLearning/comments/ct5ufr/d_question_on_quaternionrecurrentneuralnetworks/,ml23453425245,1566333833,[removed],0,1
1244,2019-8-21,2019,8,21,6,ct66h4,Character-based OCR model?,https://www.reddit.com/r/MachineLearning/comments/ct66h4/characterbased_ocr_model/,[deleted],1566335062,,0,1
1245,2019-8-21,2019,8,21,6,ct6oyv,[P] Character-based OCR model?,https://www.reddit.com/r/MachineLearning/comments/ct6oyv/p_characterbased_ocr_model/,hundley10,1566336958,"Has anybody achieved good results with character-based OCR? I've been struggling to train a CNN to recognize common fonts - seems like the best accuracy I can squeeze out is around 75-80%.

[This gist shows the Keras model I've been working with](https://gist.github.com/collinhundley/ce44603ebfc8d807e2030e0d890c41de) (and some variations). Fast inference is important for this application, so I'm trying to keep it as lightweight as possible. Character boxes are scaled down to 28x28.

We only need to recognize onscreen text (web pages, documents, etc) - pretty much ideal circumstances for OCR. Some mistakes are expected at the character level (e.g. I vs l vs 1), but what we're seeing is significantly worse and sometimes pure gibberish. Training data is synthetic but virtually identical to real-world text.

Is it realistic to achieve decent accuracy from a character-based net? Adding recurrent layers and a dictionary would be pretty heavy for this application, so we're hoping to avoid it. If anybody can provide recommendations I'd love to hear it.",5,0
1246,2019-8-21,2019,8,21,7,ct74rp,[D] Hosting multiple large models online,https://www.reddit.com/r/MachineLearning/comments/ct74rp/d_hosting_multiple_large_models_online/,mukaj,1566338763,"I want to host some of my models as APIs for a side project, wondering if anyone has done this on typical web hosts/ how well would that go? Looking to host some BERT models for example, which take a few seconds even on my GPU inference so is it doable on web host CPUs?

TalkToTransformer website seems to be hosting GPT2 models and can handle queries quite well, anyone know how that is running?

Obviously would like to keep costs low otherwise would just rent a server with GPU..",2,1
1247,2019-8-21,2019,8,21,7,ct7c8n,[R] Open AI Releases 774M GPT-2 Model,https://www.reddit.com/r/MachineLearning/comments/ct7c8n/r_open_ai_releases_774m_gpt2_model/,Skylion007,1566339655,,0,1
1248,2019-8-21,2019,8,21,7,ct7crd,Saliency Maps for Deep Learning Part 1: Vanilla Gradient,https://www.reddit.com/r/MachineLearning/comments/ct7crd/saliency_maps_for_deep_learning_part_1_vanilla/,[deleted],1566339722,[deleted],0,1
1249,2019-8-21,2019,8,21,7,ct7dsl,Saliency Maps for Deep Learning Part 1: Vanilla Gradient,https://www.reddit.com/r/MachineLearning/comments/ct7dsl/saliency_maps_for_deep_learning_part_1_vanilla/,taroth,1566339846,,0,1
1250,2019-8-21,2019,8,21,8,ct80af,Infra red humanoid image recognition?,https://www.reddit.com/r/MachineLearning/comments/ct80af/infra_red_humanoid_image_recognition/,_-rootkid-_,1566342679,[removed],0,1
1251,2019-8-21,2019,8,21,8,ct8hs7,Tipps for Image segmentation ?,https://www.reddit.com/r/MachineLearning/comments/ct8hs7/tipps_for_image_segmentation/,gimme-rewards,1566345002,[removed],0,1
1252,2019-8-21,2019,8,21,10,cta3do,Do you study Machine Learning ? here are the 3 best machine learning books ! with download link,https://www.reddit.com/r/MachineLearning/comments/cta3do/do_you_study_machine_learning_here_are_the_3_best/,sajad-52,1566352677,,0,1
1253,2019-8-21,2019,8,21,11,ctakgh,What are your requirements for blow molding machines?,https://www.reddit.com/r/MachineLearning/comments/ctakgh/what_are_your_requirements_for_blow_molding/,miyawang12138,1566355071,[removed],0,1
1254,2019-8-21,2019,8,21,11,ctapcv,"Plastic Crusher Maintenance For Blow Molding Machine [Easy To Get] Do you know that it is necessary to know the Plastic Crusher Maintenance For Blow Molding Machine? WHY? Because their state can directly affect the normal operation of blow molding machine. So, today well share with you the plast",https://www.reddit.com/r/MachineLearning/comments/ctapcv/plastic_crusher_maintenance_for_blow_molding/,miyawang12138,1566355788,,0,1
1255,2019-8-21,2019,8,21,12,ctazth,GPT-2: 6-Month Follow-Up,https://www.reddit.com/r/MachineLearning/comments/ctazth/gpt2_6month_followup/,devgameon,1566357335,[removed],0,1
1256,2019-8-21,2019,8,21,13,ctbhk4,Question on AWS ML services,https://www.reddit.com/r/MachineLearning/comments/ctbhk4/question_on_aws_ml_services/,lppier,1566360082,[removed],0,1
1257,2019-8-21,2019,8,21,13,ctbruh,Research groups doing DL for Medical Imaging,https://www.reddit.com/r/MachineLearning/comments/ctbruh/research_groups_doing_dl_for_medical_imaging/,Asmartoctopus,1566361740,[removed],0,1
1258,2019-8-21,2019,8,21,13,ctbsj9,Improving Business Communications and Human Interactions with NLP,https://www.reddit.com/r/MachineLearning/comments/ctbsj9/improving_business_communications_and_human/,MachineLearning001,1566361844,,0,1
1259,2019-8-21,2019,8,21,13,ctbxod,[D] Question on Quaternion-Recurrent-Neural-Networks,https://www.reddit.com/r/MachineLearning/comments/ctbxod/d_question_on_quaternionrecurrentneuralnetworks/,ml23453425245,1566362723,"concerning [https://arxiv.org/abs/1806.04418:](https://arxiv.org/abs/1806.04418:)

I understand that the forward pass needs to be adapted for QRNN; but what's unclear to me:

Why do they derive a backprop algorithm especially for quaternions? Shouldn't the automatic

differentiation frameworks like Pytorch or Tensorflow do that automatically, at least in the case

when the activation function is as they used it? In the final end, I guess

everything is implemented by ""real""-valued matrices and if the forward pass is according to quaternionic

multiplication, why would an automatically derived gradient not work?

So my question would be is there any difference of their backward pass to pytorch derived backward pass,

apart from computational speed or memory consumption?

If I understand correctly, in the copy-task of the github repo ([https://github.com/Orkis-Research/Quaternion-Recurrent-Neural-Networks](https://github.com/Orkis-Research/Quaternion-Recurrent-Neural-Networks)) 

even a pytorch backward pass is used by default?

&amp;#x200B;

Thanks for any answer or intuition that either confirms what I think or contradicts me.",11,9
1260,2019-8-21,2019,8,21,14,ctcaj3,[D] What are the measures to evaluate two probability distributions?,https://www.reddit.com/r/MachineLearning/comments/ctcaj3/d_what_are_the_measures_to_evaluate_two/,thnok,1566364946,"For a single learning approach, you can do F1 score measures, but how will you evaluate a probability distribution? (i.e. compare \[0.1,0.2,0.5,0.2\] and \[0.2,0.1,0.5,0.2\]) My reading on the area has pointed to KL-divergence to measure the information loss, but any other methods to consider?",6,0
1261,2019-8-21,2019,8,21,14,ctcc5u,How Train HMM model from scratch ?,https://www.reddit.com/r/MachineLearning/comments/ctcc5u/how_train_hmm_model_from_scratch/,JayRathod3497,1566365248,[removed],0,1
1262,2019-8-21,2019,8,21,14,ctcfr6,Using word embeddings as a feature in entity embedding?,https://www.reddit.com/r/MachineLearning/comments/ctcfr6/using_word_embeddings_as_a_feature_in_entity/,jennysebastian,1566365886,[removed],0,1
1263,2019-8-21,2019,8,21,14,ctclis,"Really cool idea - open-source tool that makes it easy to create Amazon EC2 instances for Data Scientists and ML engineers. You can choose any specs (CPU, GPU, RAM) that you want with a few simple commands from your terminal.",https://www.reddit.com/r/MachineLearning/comments/ctclis/really_cool_idea_opensource_tool_that_makes_it/,WildShallot,1566366962,,0,1
1264,2019-8-21,2019,8,21,15,ctcqkt,"Really cool idea - open-source tool that makes it easy to work with and configure Amazon EC2 instances. You can choose any specs (CPU, GPU, RAM) that you want from the command line.",https://www.reddit.com/r/MachineLearning/comments/ctcqkt/really_cool_idea_opensource_tool_that_makes_it/,WildShallot,1566367888,[removed],0,1
1265,2019-8-21,2019,8,21,15,ctctzv,"Really cool idea - open-source tool that makes it easy to create Amazon EC2 instances for Data Scientists and ML engineers. You can choose any specs (CPU, GPU, RAM) that you want with a few simple commands from your terminal.",https://www.reddit.com/r/MachineLearning/comments/ctctzv/really_cool_idea_opensource_tool_that_makes_it/,WildShallot,1566368497, [https://github.com/provisionpad/provisionpad](https://github.com/provisionpad/provisionpad),0,1
1266,2019-8-21,2019,8,21,16,ctd6wp,Categorical outlier detection with infrequent pattern mining?,https://www.reddit.com/r/MachineLearning/comments/ctd6wp/categorical_outlier_detection_with_infrequent/,TempoLJX,1566371009,[removed],0,1
1267,2019-8-21,2019,8,21,16,ctdbsr,How Chatbots Generate New Leads,https://www.reddit.com/r/MachineLearning/comments/ctdbsr/how_chatbots_generate_new_leads/,RubiksCodeNMZ,1566371989,,0,1
1268,2019-8-21,2019,8,21,16,ctddej,How to Create GUI in MATLAB Demo,https://www.reddit.com/r/MachineLearning/comments/ctddej/how_to_create_gui_in_matlab_demo/,flyhighwithai,1566372286,,0,1
1269,2019-8-21,2019,8,21,16,ctddv1,Blow Molding VS Rotomolding: What are the Advantages of Blow Molding,https://www.reddit.com/r/MachineLearning/comments/ctddv1/blow_molding_vs_rotomolding_what_are_the/,miyawang12138,1566372383,,0,1
1270,2019-8-21,2019,8,21,16,ctdexd,Using Machine Learning in Service Level Agreements,https://www.reddit.com/r/MachineLearning/comments/ctdexd/using_machine_learning_in_service_level_agreements/,RubiksCodeNMZ,1566372588,,0,1
1271,2019-8-21,2019,8,21,16,ctdfg2,[D] Testing different linking strategies (hard links vs symlinks vs reflinks) for managing ML projects,https://www.reddit.com/r/MachineLearning/comments/ctdfg2/d_testing_different_linking_strategies_hard_links/,cmstrump,1566372691,"Linking data or model files makes it possible to rearrange any amount of files very fast, while avoiding copying, thus saving disk space. Data science teams use symlinks to save space and avoid copying large datasets. 

The tutorial start with a strategy of copying files into place, then using hard links and symbolic links, then ending up with a new type of link, reflinks, which implements Copy On Write capabilities available in modern file systems on Linux and Mac OS: [Reflinks vs symlinks vs hard links, and how they can help machine learning projects](https://hackernoon.com/reflinks-vs-symlinks-vs-hard-links-and-how-they-can-help-machine-learning-projects-wz2ej3xa7)",1,0
1272,2019-8-21,2019,8,21,16,ctdjhe,How to Run Green Cloud in Virtual Box,https://www.reddit.com/r/MachineLearning/comments/ctdjhe/how_to_run_green_cloud_in_virtual_box/,flyhighwithai,1566373479,,0,1
1273,2019-8-21,2019,8,21,16,ctdlrb,Algorithm to recognize which person is speaking,https://www.reddit.com/r/MachineLearning/comments/ctdlrb/algorithm_to_recognize_which_person_is_speaking/,MUBTAAB,1566373969,[removed],0,1
1274,2019-8-21,2019,8,21,17,ctdqzo,[P] Hunga-Bunga: Because why not brute force all of sklearn?,https://www.reddit.com/r/MachineLearning/comments/ctdqzo/p_hungabunga_because_why_not_brute_force_all_of/,PhYsIcS-GUY227,1566375074,"*Disclaimer - this is not my project*, but it is simple and very cool at the same time, so I thought people might like it.

Say hello to [Hunga Bunga](https://github.com/ypeleg/HungaBunga)!

Like the title says, it lets you brute force all of sklearn (including parameters and model types) with a simple **fit / predict** interface.

All you need to do is:

    from hunga_bunga import HungaBungaClassifier, HungaBungaRegressor
    
    clf = HungaBungaClassifier()
    clf.fit(x, y)
    clf.predict(x)

As simple as that.

Do you have any other simple yet wonderful automation libraries? I'd love to know.",73,208
1275,2019-8-21,2019,8,21,17,cte1bz,[P] My simple dense layers implementation on OpenCL,https://www.reddit.com/r/MachineLearning/comments/cte1bz/p_my_simple_dense_layers_implementation_on_opencl/,airplanelesss,1566377223,,0,1
1276,2019-8-21,2019,8,21,18,cte9s3,Get $350 GCP free credit.,https://www.reddit.com/r/MachineLearning/comments/cte9s3/get_350_gcp_free_credit/,Zerotool1,1566378822,[removed],0,1
1277,2019-8-21,2019,8,21,18,cteaha,APPLIED MACHINE LEARNING PROBLEM SOLVING FRAME WORK,https://www.reddit.com/r/MachineLearning/comments/cteaha/applied_machine_learning_problem_solving_frame/,priyaleo,1566378958,,0,1
1278,2019-8-21,2019,8,21,18,ctech2,"I implemented pytorch version of the paper: ""A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers""",https://www.reddit.com/r/MachineLearning/comments/ctech2/i_implemented_pytorch_version_of_the_paper_a/,bzantium,1566379335,[removed],0,1
1279,2019-8-21,2019,8,21,19,cteurj,Image recognition API that recognises products,https://www.reddit.com/r/MachineLearning/comments/cteurj/image_recognition_api_that_recognises_products/,elie2222,1566382797,"I'm looking for a readily available API that will label products. For example, if I take a photo of Dove shampoo or Kit Kat chocolate, it will tell me that's what I've taken a picture of.

I've played Amazon Rekognition and some Google APIs, but they don't seem to do the exact job I need. They may be able to tell me I took a picture of chocolates, but not that I took a picture of Kit Kats.

Any suggestions for a good API to use? (Or a better subreddit to ask this question in?)",1,1
1280,2019-8-21,2019,8,21,19,ctf1m0,[Article + Code] Reinforcement Learning (DDPG and TD3) for News Recommendation,https://www.reddit.com/r/MachineLearning/comments/ctf1m0/article_code_reinforcement_learning_ddpg_and_td3/,dotapetro,1566384044,[removed],0,2
1281,2019-8-21,2019,8,21,20,ctf86o,Radeon RX Vega M GL for machine learning (on a Intel NUC 8 mini PC),https://www.reddit.com/r/MachineLearning/comments/ctf86o/radeon_rx_vega_m_gl_for_machine_learning_on_a/,Jigsus,1566385253,[removed],0,1
1282,2019-8-21,2019,8,21,20,ctf8bp,AmpliGraph 1.1.0: user-friendly knowledge graph embeddings library with state-of-the-art results,https://www.reddit.com/r/MachineLearning/comments/ctf8bp/ampligraph_110_userfriendly_knowledge_graph/,tabacof,1566385283,[removed],0,1
1283,2019-8-21,2019,8,21,20,ctfc84,Smarten Augmented Analytics: Sophistication and Simplicity,https://www.reddit.com/r/MachineLearning/comments/ctfc84/smarten_augmented_analytics_sophistication_and/,ElegantMicroWebIndia,1566385934,,0,1
1284,2019-8-21,2019,8,21,20,ctfems,Should we build interpretable models? What does that mean?,https://www.reddit.com/r/MachineLearning/comments/ctfems/should_we_build_interpretable_models_what_does/,m-cdf,1566386332,,0,1
1285,2019-8-21,2019,8,21,20,ctfj99,[D] Radeon RX Vega M GL for machine learning (on a Intel NUC 8 mini PC),https://www.reddit.com/r/MachineLearning/comments/ctfj99/d_radeon_rx_vega_m_gl_for_machine_learning_on_a/,Jigsus,1566387090,"I've been seeing the Intel NUC 8 mini PCs going on sale. Specifically the NUC8i7HNK2 with an i7-8705G 3.10 Ghz, Kaby Lake G processor and a Radeon RX Vega M GL is on sale for $700 (pretax) without the RAM or SSD.

This is a great value! I need a portable machine learning teaching tool. A big bulky gaming GPU laptop is not that great (unwieldy, price and performance wise). These Intel NUCs you can just throw in the luggage and plug it in anywhere to do a demo or try out an idea before I go out and rent a cloud GPU to do the heavy lifting.

But is that RX Vega M GPU any good for machine learning? 

I know last year they didn't support CUDA but is this still an issue with libraries? I tried digging into documentation but it's unclear.",13,5
1286,2019-8-21,2019,8,21,20,ctfql6,[D] Speech Pronunciation Recognition (and Feedback for pronunciation coaching),https://www.reddit.com/r/MachineLearning/comments/ctfql6/d_speech_pronunciation_recognition_and_feedback/,VividFee,1566388261,"Hi all,

  
I was checking out this video about an 'AI-enabled speech pronunciation coaching app' where it records your voice and gives feedback on your (English) pronunciation. With details on which part of the word needs more practice. [Please watch a snippet of the video demonstrating it here](https://www.youtube.com/watch?v=nJzpn8zuUhU&amp;feature=youtu.be&amp;t=86).  


**Can someone recommend me papers/repositories explaining the algorithms and method that (most likely) enables the app to do exactly this?**  


My take on it will be using ASR (Automatic Speech Recognition) to do speech to phonemes and phonemes to a special grammar (e.g. car -&gt; /k/ /aa/ /r/). Then the recognised speech, in the special grammar, is compared to the truth. If it matches, then your pronunciation is perfect. Otherwise, it will give you feedback on the specific part of the word that needs practice.

Thanks!",3,4
1287,2019-8-21,2019,8,21,20,ctft2q,[P] AmpliGraph 1.1.0: user-friendly knowledge graph embeddings library with state-of-the-art results,https://www.reddit.com/r/MachineLearning/comments/ctft2q/p_ampligraph_110_userfriendly_knowledge_graph/,tabacof,1566388652,"We have just released a new version of **AmpliGraph.** It is available from pypi, all you have to do is run pip install --upgrade ampligraph. It requires TensorFlow 1.13 or 1.14 to be installed.

For beginners, we have two new tutorials:
* [Basics with Game of Thrones data](http://docs.ampligraph.org/en/1.1.0/tutorials/AmpliGraphBasicsTutorial.html)
* [Clustering and classification with football data](http://docs.ampligraph.org/en/1.1.0/tutorials/ClusteringAndClassificationWithEmbeddings.html)

For advanced users, we have a new [knowledge discovery API](http://docs.ampligraph.org/en/1.1.0/ampligraph.discovery.html), including discover facts, clustering, near-duplicates detection, and topn query.

We also provide [state-of-the-art results](https://imgur.com/a/GklOXeS) on two benchmark datasets (WN18RR and YAGO3-10), plus competitive results on other benchmarks. These results are fully reproducible.

The updated documentation with the complete changelog is available here: [http://docs.ampligraph.org](http://docs.ampligraph.org)

If you have any questions or feedback, you can comment here or raise an issue on GitHub.",0,15
1288,2019-8-21,2019,8,21,21,ctfucx,"How can I assign weights to ""classes"" or sub-groups of features?[x-post]",https://www.reddit.com/r/MachineLearning/comments/ctfucx/how_can_i_assign_weights_to_classes_or_subgroups/,xk86,1566388849,,0,1
1289,2019-8-21,2019,8,21,21,ctfwqx,How do you come up with new ideas in neural networks?,https://www.reddit.com/r/MachineLearning/comments/ctfwqx/how_do_you_come_up_with_new_ideas_in_neural/,elhiruko,1566389205,[removed],0,1
1290,2019-8-21,2019,8,21,21,ctfxec,[P] Pytorch Implementation of Autoregressive Language Model,https://www.reddit.com/r/MachineLearning/comments/ctfxec/p_pytorch_implementation_of_autoregressive/,lyeoni,1566389309,"A step-by-step tutorial on how to implement and adapt **Autoregressive language model** to Wikipedia text.

A   pre-trained BERT, XLNET is publicly available ! But, for NLP  beginners,  It could be hard to use/adapt after full understanding. For   them, I covered whole, end-to-end implementation process for language   modeling, using unidirectional/bidirectional LSTM network, we already  know.

* **- do not use  torchtext library !**
* **+ include trained model file, training logs**

I hope that this repo can be a good solution for people who want to have their own language model :)

[https://github.com/lyeoni/pretraining-for-language-understanding](https://github.com/lyeoni/pretraining-for-language-understanding)",0,0
1291,2019-8-21,2019,8,21,21,ctfyib,[P] OpenCL framework with Dense layers,https://www.reddit.com/r/MachineLearning/comments/ctfyib/p_opencl_framework_with_dense_layers/,airplanelesss,1566389462," Hi everyone,

I made simple DL framework on OpenCL just with Dense layers and several activations and losses:  [https://github.com/Airplaneless/Hallgerd](https://github.com/Airplaneless/Hallgerd) 

[Matrix multiplication performance](https://i.redd.it/7kjj2r1ulsh31.png)

&amp;#x200B;

[MLP performance](https://i.redd.it/cjhkepjwlsh31.png)

Syntax is similar to Keras:

    In [1]: from hallgerd.core import Sequential
            gpu = Device(devices['GeForce GTX 660'], DTYPE=np.float32)
            model = Sequential(device=gpu, lr=1e-1, batch_size=1024, epochs=40, loss='cross_entropy')
    
    In [2]: model.add(Dense(200, 200, activation='relu'))
            model.add(Dense(200, 5, activation='softmax'))
    
    In [3]: model.fit(X, y) # here X.shape = (feature size, dataset size)
                            # y.shape = (output size, dataset size)
    
    In [4]: yp = model(X)

I think performance is quite sufficient on devices without PyTorch or Tensorflow support.

&amp;#x200B;

https://i.redd.it/4m3g6kfcmsh31.jpg",2,7
1292,2019-8-21,2019,8,21,21,ctfypq,Beginner in need of help,https://www.reddit.com/r/MachineLearning/comments/ctfypq/beginner_in_need_of_help/,Freddy_Wisch,1566389488,"Hey guys,

I have semi intermediate skills in python and have been deep into coding for a while now (3ish months). Naturally, with all the hype around it, I want to be involved in ML/AI. However I simply do not know where to start. I would be very grateful if any of you could guide me somewhat, based on your learning journey. Or recommend me any courses / books / projects that would help accelerate my learning curve.

Kind regards.",0,1
1293,2019-8-21,2019,8,21,21,ctg2il,What and where to start with for Vehicle Lane Detection - ML?,https://www.reddit.com/r/MachineLearning/comments/ctg2il/what_and_where_to_start_with_for_vehicle_lane/,narengs7,1566390052,[removed],0,1
1294,2019-8-21,2019,8,21,21,ctg48y,"Microsoft Releases Distributed Text Analytics, Speech to Text and Anamoly Detection on Spark",https://www.reddit.com/r/MachineLearning/comments/ctg48y/microsoft_releases_distributed_text_analytics/,mhamilton723,1566390319,,0,1
1295,2019-8-21,2019,8,21,21,ctg5zd,"[N] Microsoft Releases Distributed Text Analytics, Speech to Text, and Anamoly Detection on PySpark /Spark",https://www.reddit.com/r/MachineLearning/comments/ctg5zd/n_microsoft_releases_distributed_text_analytics/,mhamilton723,1566390568,,0,1
1296,2019-8-21,2019,8,21,21,ctgaix,"If anyone is thinking about taking the University of Toronto's Quantum Machine Learning course, it's worth taking because it is: 1) free, and 2) there is 1 video that is really, really worth watching.",https://www.reddit.com/r/MachineLearning/comments/ctgaix/if_anyone_is_thinking_about_taking_the_university/,Agent_ANAKIN,1566391239,,0,1
1297,2019-8-21,2019,8,21,21,ctgaoi,"[N] Brace yourselves, GPT-2 774M was released by OpenAI",https://www.reddit.com/r/MachineLearning/comments/ctgaoi/n_brace_yourselves_gpt2_774m_was_released_by/,csxeba,1566391263,,1,1
1298,2019-8-21,2019,8,21,21,ctgbva,The State of Artificial Intelligence and Its Growing Need for Professionals with The AI Skillset,https://www.reddit.com/r/MachineLearning/comments/ctgbva/the_state_of_artificial_intelligence_and_its/,Albertchristopher,1566391444,,0,1
1299,2019-8-21,2019,8,21,22,ctgp9q,[D] Looking for opinions on using 3d models for image recognition.,https://www.reddit.com/r/MachineLearning/comments/ctgp9q/d_looking_for_opinions_on_using_3d_models_for/,seek_freedom,1566393332,"I was thinking about why the training process for image recognition is such a resource intensive task, and I got caught up on the idea that information loss caused by using projections (i.e. pictures) as training data would be an interesting area to explore.

&amp;#x200B;

In comes mesh based 3d models. The training subject now has all of the necessary features in a single example My question is twofold. Do you see any potential in using 3d models as a source for an ML algo/NN, and if yes, how would you go about doing it?

&amp;#x200B;

I've done a bit of brain storming, but I can't really get anywhere. My initial thoughts were decompose the problem into parts, and solve it from both ends. One challenge is getting an isolated object from an image to use as a target, and the other is decomposing a 3d model into a useful 2d projection that can be compared with the target. 

&amp;#x200B;

Figuring that the 2D part would be easier, I decided to tackle that first, and I found the task of meaningful image segmentation to be a bit more difficult than I expected. It essentially came down to this: I can use pre existing methods to segment an image, or I can make my own; but nothing actually does decent job of pulling out objects with the exception of NNs which need to be trained to do so. In an attempt to improve the training process, I essentially run into the need to use methods that I'm trying to replace. 

&amp;#x200B;

The 3D part has a variety of challenges as well. I figured I could do something like take an image of the model that's in similar proportion to the image that contains my target, then try to align the centroid of the model with the centroid of the target. This would eventually get to needing to iteratively translate, rotate, and scale the model. Some other obvious issues include segmenting the 3d model, posing the model, and finding what model to use as a reference when there are multiple ones saved.

&amp;#x200B;

Thoughts?",9,0
1300,2019-8-21,2019,8,21,22,ctguev,"claasifier for source code c#, python, c++",https://www.reddit.com/r/MachineLearning/comments/ctguev/claasifier_for_source_code_c_python_c/,paulred70,1566394025,[removed],0,1
1301,2019-8-21,2019,8,21,22,ctgxmh,[Research] Help us record data for research on NLP from Radio Data for Agriculture,https://www.reddit.com/r/MachineLearning/comments/ctgxmh/research_help_us_record_data_for_research_on_nlp/,ghost_shaba7,1566394450,"Hello lovely inhabitants of r/MachineLearning. I am doing a residency at Artificial Intelligence and Data Science Research Lab, Makerere University, Uganda. We are collecting speech data for use in analysing radio programs related to agriculture, we will use this data to help in mapping and understanding the spread of crop disease.

Help us out by taking some time and recording some words.

Go [here!](https://rcrops.github.io) to help us record or [there!](http://www.air.ug/) to view our work.",6,1
1302,2019-8-21,2019,8,21,22,ctgycc,[D] The state of transfer learning in NLP,https://www.reddit.com/r/MachineLearning/comments/ctgycc/d_the_state_of_transfer_learning_in_nlp/,jwuphysics,1566394537,"http://ruder.io/state-of-transfer-learning-in-nlp/

This blog post by Sebastian Ruder is a quick review of how natural language processing has benefited from transfer learning. He ties together how recent advances (e.g., pretrained models/BERT, optimization schemes, multitask fine-tuning, etc) can work together to improve language modeling, and also poses some open problems in the field. See also the (somewhat empty) [HN discussion](https://news.ycombinator.com/item?id=20755923).",1,17
1303,2019-8-21,2019,8,21,22,cth61h,[N] Waymo releases their dataset,https://www.reddit.com/r/MachineLearning/comments/cth61h/n_waymo_releases_their_dataset/,therumsticks,1566395558,,0,1
1304,2019-8-21,2019,8,21,23,cth9n9,[P] Text classification w/ pytorch-transformers using RoBERTa,https://www.reddit.com/r/MachineLearning/comments/cth9n9/p_text_classification_w_pytorchtransformers_using/,rsilveira79,1566396036,"Hi
I just published a blog post on how to train a text classifier using `pytorch-transformers` using the latest RoBERTa model. Colab notebook is available: https://rsilveira79.github.io/fermenting_gradients/machine_learning/nlp/pytorch/text_classification_roberta/",5,13
1305,2019-8-21,2019,8,21,23,cthdx9,The Essential Tools of Scientific Machine Learning (Scientific ML),https://www.reddit.com/r/MachineLearning/comments/cthdx9/the_essential_tools_of_scientific_machine/,ChrisRackauckas,1566396557,,0,1
1306,2019-8-21,2019,8,21,23,cthhhh,[D] Numenta (neurocortical theory group behind the Thousand Brain Theory of Intelligence) is doing an AMA on /r/neuroscience.,https://www.reddit.com/r/MachineLearning/comments/cthhhh/d_numenta_neurocortical_theory_group_behind_the/,blueneuronDOTnet,1566397022,[Might be of interest to folks here](https://old.reddit.com/r/neuroscience/comments/cth95l/we_are_numenta_an_independent_research_company/).,0,1
1307,2019-8-21,2019,8,21,23,cthvg0,Success rate/scoring of categorical features as features? [Discussion],https://www.reddit.com/r/MachineLearning/comments/cthvg0/success_ratescoring_of_categorical_features_as/,JohnnyCaggz,1566398799,"Hi all,

Let's say I have a dataset with a mix of continuous and categorical variables and I'm creating an imbalanced binary classification model. A certain categorical variable has many (1000's) of non-ordinal values. This feature is very important, certain values have high success rates ( num success(num rows with value and flag of 1)) / num instances (all rows with value) ). I have created features that include cat\_var\_num\_success, cat\_var\_ success\_rate, and a feature that is a score of each value of the categorical feature. The score assigns the mean overall success rate as the score if the value low sampling, if the value has a sufficient number of observations, and the success rate is greater than the mean overall success rate, the score is raised, the score is lowered if the value performs worse than the overall mean.

These generated features have proven to be highly predictive and improve the model (xgb). My concern comes from the fact that I have calculated these values using the whole dataset, which I subsequently split into train-test. I am afraid that the performance of the model is increasing due to information testing information leaking into training via the generated features.

Should I create an additional holdout set which does not contribute to the calculations for feature generation and test on that?

Thoughts?

Any feedback is appreciated!",0,0
1308,2019-8-21,2019,8,21,23,cthxfq,[P] Short blog post on textual entailment and how one of my favourite models achieves SOTA with little complexity,https://www.reddit.com/r/MachineLearning/comments/cthxfq/p_short_blog_post_on_textual_entailment_and_how/,wannabeproprogrammer,1566399056,,1,1
1309,2019-8-22,2019,8,22,0,cti5uf,Funding ML research: from 0 to 0.1 in 6 months,https://www.reddit.com/r/MachineLearning/comments/cti5uf/funding_ml_research_from_0_to_01_in_6_months/,shadiakiki1986,1566400097,[removed],0,1
1310,2019-8-22,2019,8,22,0,ctipdu,"Simple Questions Thread August 21, 2019",https://www.reddit.com/r/MachineLearning/comments/ctipdu/simple_questions_thread_august_21_2019/,AutoModerator,1566402776,[removed],0,1
1311,2019-8-22,2019,8,22,0,ctisd1,Machine Learning and Bathtubs - How Small Visual Changes Improve User Experience,https://www.reddit.com/r/MachineLearning/comments/ctisd1/machine_learning_and_bathtubs_how_small_visual/,mre__,1566403141,,0,7
1312,2019-8-22,2019,8,22,1,ctj0p6,Waymo Open Dataset just DROPPED,https://www.reddit.com/r/MachineLearning/comments/ctj0p6/waymo_open_dataset_just_dropped/,ninguidgold,1566404114,[removed],0,1
1313,2019-8-22,2019,8,22,1,ctj23b,[P] TensorBoard Empty Scalar Hider: Chrome Extension of hiding empty scalar/panes,https://www.reddit.com/r/MachineLearning/comments/ctj23b/p_tensorboard_empty_scalar_hider_chrome_extension/,Jasonnor,1566404275,,0,1
1314,2019-8-22,2019,8,22,1,ctj8pc,[R] Acting without Rewards,https://www.reddit.com/r/MachineLearning/comments/ctj8pc/r_acting_without_rewards/,CireNeikual,1566405062,"Hello,

Here is our latest blog post. It is an ""aside"" from our regular demos - we have two new ones in the works, but we thought it would be interesting to share some research we did in the meantime.

Link the the post: [https://ogma.ai/2019/08/acting-without-rewards/](https://ogma.ai/2019/08/acting-without-rewards/)

The post talks about unsupervised behavior learning (UBL), a method for having an agent learn from every interaction with its environment. This method is similar in purpose to hindsight experience replay (HER), but functions very differently and offers different advantages.

Let us know what you think!",4,16
1315,2019-8-22,2019,8,22,1,ctjd8s,[CFP] 3rd edition of Emergent Communication workshop at NeurIPS'19 (EmeCom),https://www.reddit.com/r/MachineLearning/comments/ctjd8s/cfp_3rd_edition_of_emergent_communication/,dbg99,1566405615,[removed],0,1
1316,2019-8-22,2019,8,22,1,ctjg4k,[Discussion] Scikit-Learn vs mlr for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/ctjg4k/discussion_scikitlearn_vs_mlr_for_machine_learning/,exxact-jm,1566405986,"Just curious what most people use/prefer here? I assume that *'python is eating the data science world*' so i would lean towards Scikit Learn. Could I be wrong? Does it depend on the user? Does it even matter?

*""Scikit-Learn is known for its easily understandable API and for Python users and MLR became and alternative to the popular Caret package with more a large suite of algorithms available and an easy way of tuning hyperparameters. These two packages are somewhat in competition due to the debate where many people involved in analytics turn to Python for machine learning and R for statistical analysis.*

*One of the reasons for a preference to Python could be because that current R packages for machine learning are provided via other packages that contain the algorithm. The packages are called through MLR but still requires extra installation. Even external feature selection libraries are needed and they will have other external dependencies that need to be satisfied as well.""*

\- source  [https://blog.exxactcorp.com/scikitlearn-vs-mlr-for-machine-learning/](https://blog.exxactcorp.com/scikitlearn-vs-mlr-for-machine-learning/)",1,0
1317,2019-8-22,2019,8,22,2,ctjoon,Waymo made their Dataset Open,https://www.reddit.com/r/MachineLearning/comments/ctjoon/waymo_made_their_dataset_open/,bloodsoft-the7th,1566406984,,0,1
1318,2019-8-22,2019,8,22,2,ctjvyv,"GAN project, help needed",https://www.reddit.com/r/MachineLearning/comments/ctjvyv/gan_project_help_needed/,tealocked,1566407829,[removed],0,1
1319,2019-8-22,2019,8,22,2,ctjz1r,[N] The Waymo Open Dataset is now available.,https://www.reddit.com/r/MachineLearning/comments/ctjz1r/n_the_waymo_open_dataset_is_now_available/,kmh4321,1566408191,,0,1
1320,2019-8-22,2019,8,22,2,ctk0qd,"[D] I call horses ""big dogs"" all the time. Is iOS listening?!",https://www.reddit.com/r/MachineLearning/comments/ctk0qd/d_i_call_horses_big_dogs_all_the_time_is_ios/,_6C1,1566408388,,0,1
1321,2019-8-22,2019,8,22,2,ctk1d9,[D] 2019 NeurIPS ML Retrospectives,https://www.reddit.com/r/MachineLearning/comments/ctk1d9/d_2019_neurips_ml_retrospectives/,hughbzhang,1566408470,"Ryan Lowe talks about how he wants researchers to reflect on their past work.

[https://thegradient.pub/introducing-retrospectives/](https://thegradient.pub/introducing-retrospectives/)",9,76
1322,2019-8-22,2019,8,22,2,ctk3k0,Do you study Machine Learning ? here are the 3 best machine learning books ! with download link,https://www.reddit.com/r/MachineLearning/comments/ctk3k0/do_you_study_machine_learning_here_are_the_3_best/,sajad-52,1566408726,,0,0
1323,2019-8-22,2019,8,22,2,ctk5m1,How to Hide Your Feelings From AI Voice Assistants,https://www.reddit.com/r/MachineLearning/comments/ctk5m1/how_to_hide_your_feelings_from_ai_voice_assistants/,Yuqing7,1566408972,,0,1
1324,2019-8-22,2019,8,22,3,ctkroz,"I wish ML conferences allowed more ""engineering"" content",https://www.reddit.com/r/MachineLearning/comments/ctkroz/i_wish_ml_conferences_allowed_more_engineering/,rumborak,1566411616,[removed],0,1
1325,2019-8-22,2019,8,22,3,ctkt1t,Data Annotation/Data Labelling [Research],https://www.reddit.com/r/MachineLearning/comments/ctkt1t/data_annotationdata_labelling_research/,sahilshah9292,1566411776,"I work for a company that provides Data Annotation/Data Labelling services(https://innodata.com/dataforai/).
I am trying to understand the need and availability of Annotated Data.

Would like to know the importance/benefit of using Annotated Data on a scale of 1-10 (1 being the lowest)",4,0
1326,2019-8-22,2019,8,22,3,ctkyqk,Optimizing Customer Response Rates,https://www.reddit.com/r/MachineLearning/comments/ctkyqk/optimizing_customer_response_rates/,therockhound,1566412457,[removed],0,1
1327,2019-8-22,2019,8,22,3,ctl365,Intro to Machine Learning [Kaggle],https://www.reddit.com/r/MachineLearning/comments/ctl365/intro_to_machine_learning_kaggle/,Illenium7777,1566412976,[removed],0,1
1328,2019-8-22,2019,8,22,3,ctl4o6,[P] Robustness: a library for training and experimenting with standard and robust training,https://www.reddit.com/r/MachineLearning/comments/ctl4o6/p_robustness_a_library_for_training_and/,andrew_ilyas,1566413163,"Hi all,  


Robustness is a library we made for use in our research that has evolved over several projects we have used it in. The result is a library for standard and robust (adversarial) training that is designed to be super extendible/customizable with very minimal effort. For example, the library allows us to train networks with custom loss functions, adversaries, logging, data loaders, etc, and also to perform a variety of input manipulation tasks using pretrained networks. Finally we provide a CLI interface for training standard and robust models.  


Code is here: [https://github.com/MadryLab/robustness](https://github.com/MadryLab/robustness)  


Full documentation, walkthroughs, and examples are here: [https://robustness.readthedocs.io/](https://robustness.readthedocs.io/)",3,20
1329,2019-8-22,2019,8,22,4,ctlqnz,[Discussion] What tools and techniques do you use for neural network research? (can range from software to hardware stack),https://www.reddit.com/r/MachineLearning/comments/ctlqnz/discussion_what_tools_and_techniques_do_you_use/,elhiruko,1566415798,"I will take two examples to illustrate the discussion.

So from the hardware accelerator design perspective  
If you take a look at the BitFusion Paper : Github --&gt; [https://github.com/hsharma35/bitfusion](https://github.com/hsharma35/bitfusion)   
: ArXiv    --&gt; [https://arxiv.org/pdf/1712.01507](https://arxiv.org/pdf/1712.01507)  


If you look at the Quantized Neural networks example  
Github -- &gt;  https://github.com/MatthieuCourbariaux/BinaryNet   
https://github.com/itayhubara/BinaryNet  
 ArXiv   -- &gt; [https://arxiv.org/pdf/1609.07061](https://arxiv.org/pdf/1609.07061.pdf)

So one can see from reading the papers and the code what tools were used for the experimentation. Here the first one uses some hardware modelling tools (like CACTI) to get preliminary results. In the case of the latter they use torch and theano and modify it to get the results (as far as I understand).  


So are there any other suggestions that the Machine learning community would like to mention?",0,4
1330,2019-8-22,2019,8,22,4,ctm2ij,[N] Cerebras CEO talks about the big implications for machine learning in companys big chip,https://www.reddit.com/r/MachineLearning/comments/ctm2ij/n_cerebras_ceo_talks_about_the_big_implications/,artificial_intelect,1566417234,,0,1
1331,2019-8-22,2019,8,22,4,ctm3yk,[P] Deploy GPT-2 on AWS,https://www.reddit.com/r/MachineLearning/comments/ctm3yk/p_deploy_gpt2_on_aws/,ospillinger,1566417409,"I wrote a [post](https://medium.com/cortex-labs/deploy-gpt2-bc4ea9ea0e54) about how I deployed OpenAIs GPT-2 as a web API on my AWS account. I used code from the OpenAI repo to download and export the model and Cortex to run it on AWS.

You can use this command to test out the API:

curl -k -X POST -H ""Content-Type: application/json"" -d '{""samples"":\[{""text"": ""machine learning""}\]}' [https://aefa719d5c44011e9adc30ea9bac8e9a-1873012518.us-west-2.elb.amazonaws.com/text/generator](https://aefa719d5c44011e9adc30ea9bac8e9a-1873012518.us-west-2.elb.amazonaws.com/text/generator)",3,24
1332,2019-8-22,2019,8,22,5,ctmklu,"Official 774M GPT-2 model released. 1.5B model might be released, dependent on 4 research organizations.",https://www.reddit.com/r/MachineLearning/comments/ctmklu/official_774m_gpt2_model_released_15b_model_might/,permalip,1566419364,,0,1
1333,2019-8-22,2019,8,22,5,ctmw1w,Beginner Question with Boosted decision trees,https://www.reddit.com/r/MachineLearning/comments/ctmw1w/beginner_question_with_boosted_decision_trees/,Henn3ssey,1566420745,[removed],0,1
1334,2019-8-22,2019,8,22,5,ctmxzj,"[D] OpenAI's official 774M GPT-2 model released. 1.5B model might be released, dependent on 4 research organizations.",https://www.reddit.com/r/MachineLearning/comments/ctmxzj/d_openais_official_774m_gpt2_model_released_15b/,permalip,1566420985,"Here are the links:

[https://openai.com/blog/gpt-2-6-month-follow-up/](https://openai.com/blog/gpt-2-6-month-follow-up/)

[https://github.com/openai/gpt-2](https://github.com/openai/gpt-2)",65,167
1335,2019-8-22,2019,8,22,6,ctnlil,Predicting someone with having Chronic Kidney Disorder (ckd) using k-NN with 98% accuracy (first post*),https://www.reddit.com/r/MachineLearning/comments/ctnlil/predicting_someone_with_having_chronic_kidney/,hajjmode,1566423831,,0,1
1336,2019-8-22,2019,8,22,7,ctnw8i,"A command-line tool that spins-up EC2 instances of any CPU or GPU specification, configures your laptop so you can connect to secure EC2 remotely using VS-Code Remote-SSH and start writing machine algorithms using VS-Code's interactive Python with zero effort and configuration time.",https://www.reddit.com/r/MachineLearning/comments/ctnw8i/a_commandline_tool_that_spinsup_ec2_instances_of/,amirzainali,1566425149,[removed],0,1
1337,2019-8-22,2019,8,22,7,cto2ww,Hardware Questions PLAES HALLLLLLLLP,https://www.reddit.com/r/MachineLearning/comments/cto2ww/hardware_questions_plaes_hallllllllp/,creayt,1566425996,[removed],0,1
1338,2019-8-22,2019,8,22,7,ctoagp,"Looking for Paper: ~10% of X, Roughly Equivalent Accuracy of Larger Model",https://www.reddit.com/r/MachineLearning/comments/ctoagp/looking_for_paper_10_of_x_roughly_equivalent/,mtvatemybrains,1566426944,[removed],0,1
1339,2019-8-22,2019,8,22,7,ctoc51,"Doubt regarding F-Statistics. Page No. 77, Introduction to Statistical Learning by Gareth James. Can someone explain the selection in detail.(or point in right direction)",https://www.reddit.com/r/MachineLearning/comments/ctoc51/doubt_regarding_fstatistics_page_no_77/,Fourteen_is_14,1566427138,,0,1
1340,2019-8-22,2019,8,22,8,ctp7wn,[D] Numenta (neurocortical theory group behind the Thousand Brain Theory of Intelligence) is doing an AMA on /r/neuroscience.,https://www.reddit.com/r/MachineLearning/comments/ctp7wn/d_numenta_neurocortical_theory_group_behind_the/,blueneuronDOTnet,1566431355,"Link [here](https://old.reddit.com/r/neuroscience/comments/cth95l/we_are_numenta_an_independent_research_company/).

&gt; Joining us is Matt Taylor (/u/rhyolight), who is /u/Numenta's community manager. He'll be answering the bulk of the questions here, and will refer any more advanced neuroscience questions to [Jeff Hawkins](https://en.wikipedia.org/wiki/Jeff_Hawkins), Numenta's Co-Founder.
&gt;  
&gt; &gt; We are on a mission to figure out how the brain works and enable machine intelligence technology based on brain principles. We've made significant progress in understanding the brain, and we believe our research offers opportunities to advance the state of AI and machine learning.
&gt; &gt;
&gt; &gt; Despite the fact that scientists have amassed an enormous amount of detailed factual knowledge about the brain, how it works is still a profound mystery. We recently published a paper titled [A Framework for Intelligence and Cortical Function Based on Grid Cells in the Neocortex](https://www.frontiersin.org/articles/10.3389/fncir.2018.00121/full) that lays out a theoretical framework for understanding what the neocortex does and how it does it. It is commonly believed that the brain recognizes objects by extracting sensory features in a series of processing steps, which is also how today's deep learning networks work. Our new theory suggests that instead of learning one big model of the world, the neocortex learns thousands of models that operate in parallel. We call this the [Thousand Brains Theory of Intelligence](https://numenta.com/blog/2019/01/16/the-thousand-brains-theory-of-intelligence/).
&gt; &gt;
&gt; &gt; The Thousand Brains Theory is rich with novel ideas and concepts that can be applied to practical machine learning systems and provides a roadmap for building intelligent systems inspired by the brain. I am excited to be a part of this mission! Ask me anything about our theory, code, or community.
&gt;  
&gt; **Relevant Links:**
&gt;  
&gt; - [Past AMA](https://old.reddit.com/r/askscience/comments/bowie2/askscience_ama_series_were_jeff_hawkins_and/):  
&gt;     /r/askscience previously hosted Numenta a couple of months ago. Check for further Q&amp;A.
&gt; - [Numenta HTM School](https://numenta.org/htm-school/):    
&gt;    Series of videos introducing HTM Theory, no background in neuro, math, or CS required.",0,1
1341,2019-8-22,2019,8,22,9,ctpcwi,"[P] A command-line tool that spins-up EC2 instances of any CPU or GPU specification, configures your laptop so you can connect to secure EC2 remotely using VS-Code Remote-SSH and start writing machine-learning algorithms using VS-Code's interactive Python.",https://www.reddit.com/r/MachineLearning/comments/ctpcwi/p_a_commandline_tool_that_spinsup_ec2_instances/,amirzainali,1566432048,"GitHub page: [https://github.com/provisionpad/provisionpad](https://github.com/provisionpad/provisionpad)

Would like to know your feedback.",2,0
1342,2019-8-22,2019,8,22,9,ctpe3g,Indico's Finetune is a fantastically underappreciated library.,https://www.reddit.com/r/MachineLearning/comments/ctpe3g/indicos_finetune_is_a_fantastically/,dradientgrescent,1566432246,[removed],0,1
1343,2019-8-22,2019,8,22,9,ctpy04,Do you study Machine Learning ? here are the best machine learning books ! with download link  part 2 ,https://www.reddit.com/r/MachineLearning/comments/ctpy04/do_you_study_machine_learning_here_are_the_best/,sajad-52,1566434992,,0,0
1344,2019-8-22,2019,8,22,10,ctq9cy,Google DeepMind Co-Founder Placed on Leave From AI Lab,https://www.reddit.com/r/MachineLearning/comments/ctq9cy/google_deepmind_cofounder_placed_on_leave_from_ai/,KindlyBasket,1566436554,,0,1
1345,2019-8-22,2019,8,22,11,ctquf6,[D] Most outlandish application of Transformer Architechture,https://www.reddit.com/r/MachineLearning/comments/ctquf6/d_most_outlandish_application_of_transformer/,ThatAi_guy,1566439542,"I'm conducting some independent research on the effectiveness of Transformer, Attention, GPT, BERT structure on tasks outside the domain of Language. I was curious to know what the most outlandish implementation you have done may be or the coolest cross-domain application you can think of. Lets see how much we can Transform the Transformer!",1,0
1346,2019-8-22,2019,8,22,11,ctr0by,[D] What should I read/do to get into machine learning?,https://www.reddit.com/r/MachineLearning/comments/ctr0by/d_what_should_i_readdo_to_get_into_machine/,thetylerwolf,1566440421,"Im currently in high school looking to get into machine learning and I was wondering what I should read or what courses I should take.

I am planning on taking the MITx Python course and reading the 2nd edition of Aurlian Grons book sometime after it is released. I am also currently eyeing Andriy Burkovs The Hundred-Page Machine Learning Book.",4,0
1347,2019-8-22,2019,8,22,11,ctr7wv,[D] Is neural architecture race to beat ImageNet is actually relevant anymore,https://www.reddit.com/r/MachineLearning/comments/ctr7wv/d_is_neural_architecture_race_to_beat_imagenet_is/,tsauri,1566441519,"We've seen the limit of training a 2D CNN on RGB images result in texture bias, exploiting regularities, etc, etc.  
1. CNN ImageNet is bag-of-features ([https://openreview.net/forum?id=SkfMWhAqYQ](https://openreview.net/forum?id=SkfMWhAqYQ))

2. CNN ImageNet actually learn to classify texture instead of learning 3D shapes [https://openreview.net/forum?id=Bygh9j09KX](https://openreview.net/forum?id=Bygh9j09KX)  


3. Backprop on CIFAR learns  Learn Surface Statistical Regularitiese[https://arxiv.org/abs/1711.11561](https://arxiv.org/abs/1711.11561)

Is there any meaning to race to find best architecture and/or algorithm on ImageNet? We are hitting limits of training with monocular RGB images with unknown arbitrary poses. In the end what we get is oowerful monocular texture classifier but easily duped with adversarial attacks.

3D datasets and 3D computer vision models are promising but severely lacking. [One recent example is transforming 2D disparity maps to 3D point clouds, then use 3D convolution instead of 2D convolution doubles the accuracy](https://arxiv.org/abs/1812.07179). We need new pose-aware 3D ImageNet to replace ImageNet 2012.",0,1
1348,2019-8-22,2019,8,22,12,ctrkrg,How to validate a chatbot ?,https://www.reddit.com/r/MachineLearning/comments/ctrkrg/how_to_validate_a_chatbot/,ashutosj,1566443424,[removed],0,1
1349,2019-8-22,2019,8,22,12,ctrpnz,[D] Is neural architecture search race to beat ImageNet actually relevant anymore,https://www.reddit.com/r/MachineLearning/comments/ctrpnz/d_is_neural_architecture_search_race_to_beat/,tsauri,1566444194,"We've seen the limit of training a 2D CNN on RGB images, resulting in texture bias, exploiting regularities, etc, etc.

1. CNN ImageNet is bag-of-features ([https://openreview.net/forum?id=SkfMWhAqYQ](https://openreview.net/forum?id=SkfMWhAqYQ))
2. CNN ImageNet actually learn to classify texture instead of learning 3D shapes [https://openreview.net/forum?id=Bygh9j09KX](https://openreview.net/forum?id=Bygh9j09KX)
3. Backprop on CIFAR-10 exploit Surface Statistical Regularities to get good test accuracy [https://arxiv.org/abs/1711.11561](https://arxiv.org/abs/1711.11561)

Is there any meaning for race to find best neural architecture search (NAS) on ImageNet? We are hitting limits of training with monocular RGB images with unknown arbitrary camera poses and intrinsics (focal length, skew, etc). In the end what we get is powerful monocular texture classifier but easily duped by adversarial attacks.

And the found architecture hyperparams is easily overfit to one dataset. In my experience, directly using Imagenet EfficientNet-B0 to train CIFAR-10 from scratch (not transfer learning like the official paper), resulting accuracy worse than Resnet.

Is there ongoing work to create pose-aware ""3D ImageNet""? The closest I can found is probably ShapeNet and various robotics datasets like Princeton SUN-RGBD. But the scale and domain is too small and narrow.",12,37
1350,2019-8-22,2019,8,22,12,ctrqng,New to machine learning - My first project idea,https://www.reddit.com/r/MachineLearning/comments/ctrqng/new_to_machine_learning_my_first_project_idea/,Iam_nameless,1566444341,"Healthcare costs in the U.S. are too much to bear.  I found some [datasets](https://www.cms.gov/OpenPayments/Explore-the-Data/Dataset-Downloads.html) of the current bid and ask price of medical care in the U.S. for Medicare and Medicaid.  

Within the dataset you can look up any hospital and see their history of bills submitted which includes their ask price of coded procedures versus the actual price paid by the govt.   A quick look at the datset shows dizzying amounts paid for service.  

Currently there is no way to Google: ""how much does it cost to fix a broken leg"" or maybe ""how much does it cost to treat a concussion"".  If you are un-insured these are real Google question that gives no results.  

Within this dataset, if a person know what medical code to look up they can get a rough idea of how much they will pay out of pocket for a procedure.  Costs for services can vary astronimically from hospital to hospital.  A $40,000 procedure at one hospital may be $4,000 at the hospital in the next town.  Currently there exists no way for un-insured people to shop for medicine when they need it.  This has created an extreme situation of extortion in our healthcare system.  

What I would like to do with this dataset is create a map of sorts that people can use to help them shop for help with they need it the most.  Imagine Googling ""How much does it cost to fix a broken arm"" and being given a website you can use to shop around the hospitals near you.  Currently private hospitals keep the true cost of their medical procedures hidden away in ""chargemaster books"" which means we'll never know how much a hopsital profits off a birth, cancer, or broken bone, but I think this dataset can provide a strong proxy people can use to get help and have some knowledge with how much they may spend out of pocket after they're cycled into the healthcare system. 

I don't know how I am going to apply machine learning to this problem yet but I imagine I'll be using Keras (for ML), Numpy (for stats), Pandas (for stats), and Seaborn (for stat visualization) but I don't know if there's a python library out there I can use to create the map I envision.",0,1
1351,2019-8-22,2019,8,22,12,ctruo0,"CONCRETE SHOW IMPACT IN BANGKOK, THAI",https://www.reddit.com/r/MachineLearning/comments/ctruo0/concrete_show_impact_in_bangkok_thai/,ada2017,1566444979,,0,1
1352,2019-8-22,2019,8,22,12,ctrvpu,[R] FSGAN: Subject Agnostic Face Swapping and Reenactment,https://www.reddit.com/r/MachineLearning/comments/ctrvpu/r_fsgan_subject_agnostic_face_swapping_and/,PuzzledProgrammer3,1566445142,"paper: [https://arxiv.org/pdf/1908.05932.pdf](https://arxiv.org/pdf/1908.05932.pdf)

video: [https://www.youtube.com/watch?v=BsITEVX6hkE](https://www.youtube.com/watch?v=BsITEVX6hkE)",3,5
1353,2019-8-22,2019,8,22,13,ctsk6c,Audio Seperation - Open Source,https://www.reddit.com/r/MachineLearning/comments/ctsk6c/audio_seperation_open_source/,Cmakh,1566449190,[removed],0,1
1354,2019-8-22,2019,8,22,14,ctsrf6,[R] Saccader: Improving Accuracy of Hard Attention Models for Vision,https://www.reddit.com/r/MachineLearning/comments/ctsrf6/r_saccader_improving_accuracy_of_hard_attention/,xternalz,1566450471,,1,12
1355,2019-8-22,2019,8,22,14,ctsz61,GitHub - MateLabs/AutoOut: Automated Outlier Detection and Treatment Tool,https://www.reddit.com/r/MachineLearning/comments/ctsz61/github_matelabsautoout_automated_outlier/,kailashahirwar12,1566451902,,0,1
1356,2019-8-22,2019,8,22,15,cttd55,Compare two sentences for similarity,https://www.reddit.com/r/MachineLearning/comments/cttd55/compare_two_sentences_for_similarity/,dbaru10,1566454540,[removed],0,1
1357,2019-8-22,2019,8,22,15,cttefo,[D] Positional Encoding in Transformer,https://www.reddit.com/r/MachineLearning/comments/cttefo/d_positional_encoding_in_transformer/,amil123123,1566454792,"Hey all,

I was reading up the transformer paper [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762). This architecture uses positional encoding which the attention layers ignore.

I don't understand two things -

1. Why use Sin &amp; Cos as positional embeddings , why not any other function
2. They also talk about training these positional embeddings, how do you go about training such embeddings. As in how do you let the model know that these embeddings are for the position

Thanks !",18,11
1358,2019-8-22,2019,8,22,15,cttqmx,Anomaly detection for time series data with only positive samples?,https://www.reddit.com/r/MachineLearning/comments/cttqmx/anomaly_detection_for_time_series_data_with_only/,ashutosj,1566457181,[removed],0,1
1359,2019-8-22,2019,8,22,16,ctu0aj,[Research] A critique of pure learning and what artificial neural networks can learn from animal brains,https://www.reddit.com/r/MachineLearning/comments/ctu0aj/research_a_critique_of_pure_learning_and_what/,MTGTraner,1566459034,,98,190
1360,2019-8-22,2019,8,22,17,ctu9mj,[P] Introduction to self driving cars,https://www.reddit.com/r/MachineLearning/comments/ctu9mj/p_introduction_to_self_driving_cars/,mynercloud,1566460969,,0,1
1361,2019-8-22,2019,8,22,17,ctuj68,A critique of pure learning and what artificial neural networks can learn from animal brains,https://www.reddit.com/r/MachineLearning/comments/ctuj68/a_critique_of_pure_learning_and_what_artificial/,sensetime,1566463006,[removed],0,1
1362,2019-8-22,2019,8,22,17,ctulv2,[Code Review] Unofficial Keras implementation of the paper Instance Enhancement Batch Normalization,https://www.reddit.com/r/MachineLearning/comments/ctulv2/code_review_unofficial_keras_implementation_of/,cyril_9227,1566463618,[removed],0,1
1363,2019-8-22,2019,8,22,17,ctuowh,[P] Unofficial Keras implementation of the paper Instance Enhancement Batch Normalization,https://www.reddit.com/r/MachineLearning/comments/ctuowh/p_unofficial_keras_implementation_of_the_paper/,cyril_9227,1566464243,"Hello guys,

I recently implemented a Keras code for the paper [https://arxiv.org/abs/1908.04008](https://arxiv.org/abs/1908.04008) and I would be super glad if someone experienced would review my code and give me feedbacks ! (In terms of Python / Keras style or correctness of the implemented code...)

Here is the repo link : [https://github.com/Cyril9227/Keras\_IEBN](https://github.com/Cyril9227/Keras_IEBN)

The official Pytorch code is here : [https://github.com/gbup-group/IEBN](https://github.com/gbup-group/IEBN)

Thank's a lot :)",0,1
1364,2019-8-22,2019,8,22,18,ctuquz,5 Tips To Create A More Reliable Web Crawler,https://www.reddit.com/r/MachineLearning/comments/ctuquz/5_tips_to_create_a_more_reliable_web_crawler/,weihong95,1566464626,[removed],0,1
1365,2019-8-22,2019,8,22,18,ctuzz8,[R] Tracking Holistic Object Representations,https://www.reddit.com/r/MachineLearning/comments/ctuzz8/r_tracking_holistic_object_representations/,xl-sr,1566466377,"Paper: [https://arxiv.org/abs/1907.12920](https://arxiv.org/abs/1907.12920)

Code: [https://github.com/xl-sr/THOR](https://github.com/xl-sr/THOR)

Video: [https://www.youtube.com/watch?v=EAv7SvI3EH0&amp;amp=&amp;feature=youtu.be](https://www.youtube.com/watch?v=EAv7SvI3EH0&amp;amp=&amp;feature=youtu.be)

Project: [https://sites.google.com/view/vision-thor/](https://sites.google.com/view/vision-thor/)",1,5
1366,2019-8-22,2019,8,22,18,ctv2yk,The pretrained GPT-2-774M *really* like the navy seal copypasta meme,https://www.reddit.com/r/MachineLearning/comments/ctv2yk/the_pretrained_gpt2774m_really_like_the_navy_seal/,grey--area,1566466922,[removed],0,1
1367,2019-8-22,2019,8,22,18,ctv5tf,Building a Question Generation using PyTorch(LSTM&amp;Seq2Seq),https://www.reddit.com/r/MachineLearning/comments/ctv5tf/building_a_question_generation_using/,govinddaga,1566467454,[removed],0,1
1368,2019-8-22,2019,8,22,20,ctvxq7,GCP GPU instance,https://www.reddit.com/r/MachineLearning/comments/ctvxq7/gcp_gpu_instance/,airejie,1566472459,"Hello,

I have tried for a while now to get a GPU instance running on GCP but I always get rejected for lack of resources. Does this happen to anyone? Is it due to the fact that I am using the free credits?",0,1
1369,2019-8-22,2019,8,22,20,ctw864,[P] Time Series Analysis - Predicting Electricity Consumption using an LSTM network,https://www.reddit.com/r/MachineLearning/comments/ctw864/p_time_series_analysis_predicting_electricity/,plentyofnodes,1566474171,"In this example, an LSTM neural network is used to forecast energy consumption of the Dublin City Council Civic Offices in Ireland using data between April 2011  February 2013.

An LSTM model was generated and run on the data, and the mean percentage error was 6.1%.

The methodology and findings can be found [here](https://www.michael-grogan.com/electricity-consumption-neural/). Would be grateful for any feedback.",12,8
1370,2019-8-22,2019,8,22,20,ctwcrd,Global Military Robots Market Research Report 2019,https://www.reddit.com/r/MachineLearning/comments/ctwcrd/global_military_robots_market_research_report_2019/,jadhavni3,1566474899,[removed],1,1
1371,2019-8-22,2019,8,22,21,ctwl2v,A new sub r/datascienceproject,https://www.reddit.com/r/MachineLearning/comments/ctwl2v/a_new_sub_rdatascienceproject/,OppositeMidnight,1566476099,[removed],0,1
1372,2019-8-22,2019,8,22,21,ctwnl4,"Start your Machine Learning journey with a bang. Join our global session on September 1, 2019 (free entry limited seats)",https://www.reddit.com/r/MachineLearning/comments/ctwnl4/start_your_machine_learning_journey_with_a_bang/,MCAL_Training,1566476452,,0,1
1373,2019-8-22,2019,8,22,21,ctwnno,Artificial Intelligence vs. Machine Learning vs. Deep Learning: What is the Difference?,https://www.reddit.com/r/MachineLearning/comments/ctwnno/artificial_intelligence_vs_machine_learning_vs/,techgig11,1566476463,,0,1
1374,2019-8-22,2019,8,22,21,ctwq3h,[D] is there any tasks I can evaluate my laptop on?,https://www.reddit.com/r/MachineLearning/comments/ctwq3h/d_is_there_any_tasks_i_can_evaluate_my_laptop_on/,FellowOfHorses,1566476828,For bureaucratic reasons I got a Laptop for Machine Learning. Is there any tasks or code I can run to see how effective my laptop is for ML?,5,2
1375,2019-8-22,2019,8,22,21,ctwryc,Artificial Intelligence vs. Machine Learning vs. Deep Learning: What is the Difference?,https://www.reddit.com/r/MachineLearning/comments/ctwryc/artificial_intelligence_vs_machine_learning_vs/,techgig11,1566477092,,0,7
1376,2019-8-22,2019,8,22,21,ctww8q,Object detection in autonomous car,https://www.reddit.com/r/MachineLearning/comments/ctww8q/object_detection_in_autonomous_car/,SouvikMandal,1566477737,[removed],0,1
1377,2019-8-22,2019,8,22,22,ctxajh,What's the name of this optimization technique?,https://www.reddit.com/r/MachineLearning/comments/ctxajh/whats_the_name_of_this_optimization_technique/,HalfArmBandit,1566479735,[removed],0,1
1378,2019-8-22,2019,8,22,22,ctxc5x,[D] Tips for effectively monitoring my model in production?,https://www.reddit.com/r/MachineLearning/comments/ctxc5x/d_tips_for_effectively_monitoring_my_model_in/,research-panda,1566479959,"There is plenty of literature about training models. However: How can I effectively monitor my models once they are deployed and live?

Things that I would like to monitor:  
\- Accuracy  
\- Does my live input data represent my training data

Do you know any tools for monitoring these metrics? And are there other metrics that you think I should monitor?  
Thanks",5,6
1379,2019-8-22,2019,8,22,22,ctxc8b,[P] These Instagram Portrait Models Do Not Exist,https://www.reddit.com/r/MachineLearning/comments/ctxc8b/p_these_instagram_portrait_models_do_not_exist/,samsamsamrox1212,1566479965,[removed],0,1
1380,2019-8-22,2019,8,22,22,ctxdqt,Why You Should Use Artificial Intelligence in Cybersecurity,https://www.reddit.com/r/MachineLearning/comments/ctxdqt/why_you_should_use_artificial_intelligence_in/,inveritasoft,1566480188,,1,1
1381,2019-8-22,2019,8,22,22,ctxhrr,[P] These Instagram Portrait Models Do Not Exist,https://www.reddit.com/r/MachineLearning/comments/ctxhrr/p_these_instagram_portrait_models_do_not_exist/,samsamsamrox1212,1566480736,"I used transfer learning to train Stylegan using [Gwern](https://www.gwern.net/Faces)'s  This [Waifu Does not exist](https://www.thiswaifudoesnotexist.net/) 512px [pre-trained model](https://mega.nz/#!CRtiDI7S!xo4zm3n7pkq1Lsfmuio1O8QPpUwHrtFTHjNJ8_XxSJs). 

&amp;#x200B;

The dataset was originally around 13k images. After processing, removing images with artifacts and hands 3k images were left. The final dataset was augmented to 13k. 

&amp;#x200B;

Here are some [results](https://www.instagram.com/p/B1d4poSHAPW/?utm_source=ig_web_copy_link):",0,1
1382,2019-8-22,2019,8,22,22,ctxth9,On device ML with custom TFLite models in Flutter apps,https://www.reddit.com/r/MachineLearning/comments/ctxth9/on_device_ml_with_custom_tflite_models_in_flutter/,Sharonzac,1566482294,,0,1
1383,2019-8-22,2019,8,22,23,cty61o,"Amazon transfers inflection, rhythm from recorded speaker to synthesized voice",https://www.reddit.com/r/MachineLearning/comments/cty61o/amazon_transfers_inflection_rhythm_from_recorded/,georgecarlyle76,1566483917,,0,1
1384,2019-8-22,2019,8,22,23,ctygii,[D] What are some well established and widely used frameworks/models for action recognition from videos?,https://www.reddit.com/r/MachineLearning/comments/ctygii/d_what_are_some_well_established_and_widely_used/,LessTell,1566485286,"I was looking for some frameworks for action recognition, preferably real-time ones. Additional question, how would one go about action recognition of a specific person? Say I need to detect a specific person (let's assume he has a specific outfit) and go about recognizing his actions only. Any ideas for achieving that?",2,2
1385,2019-8-22,2019,8,22,23,ctyk69,Recommender Systems in Python --Part 1 -- Content Based Filtering,https://www.reddit.com/r/MachineLearning/comments/ctyk69/recommender_systems_in_python_part_1_content/,codegeass30,1566485736,,0,1
1386,2019-8-23,2019,8,23,0,ctyz3c,AutoML + GAN = AutoGAN! AI Can Now Design Better GAN Models Than Humans,https://www.reddit.com/r/MachineLearning/comments/ctyz3c/automl_gan_autogan_ai_can_now_design_better_gan/,Yuqing7,1566487600,,0,2
1387,2019-8-23,2019,8,23,0,ctzeai,The Essential Tools of Scientific Machine Learning (Scientific ML) - Stochastic Lifestyle,https://www.reddit.com/r/MachineLearning/comments/ctzeai/the_essential_tools_of_scientific_machine/,Arisngr,1566489473,,0,1
1388,2019-8-23,2019,8,23,1,ctzg1v,What features can be extracted from accelerometer sensor used for Structural Health Monitoring (SHM)?,https://www.reddit.com/r/MachineLearning/comments/ctzg1v/what_features_can_be_extracted_from_accelerometer/,Mojo11727,1566489689,[removed],0,1
1389,2019-8-23,2019,8,23,1,ctzkwm,Deep Learning Illustrated: Building Natural Language Processing Models,https://www.reddit.com/r/MachineLearning/comments/ctzkwm/deep_learning_illustrated_building_natural/,iamkeyur,1566490264,,0,2
1390,2019-8-23,2019,8,23,1,ctzwkt,What are essential and nice to know languages for a machine learning engineering?,https://www.reddit.com/r/MachineLearning/comments/ctzwkt/what_are_essential_and_nice_to_know_languages_for/,EdHerzriesig,1566491707,[removed],0,1
1391,2019-8-23,2019,8,23,2,cu0kip,Do you study Machine Learning ? here are the best machine learning books ! with download link  part 2 ,https://www.reddit.com/r/MachineLearning/comments/cu0kip/do_you_study_machine_learning_here_are_the_best/,sajad-52,1566494567,,0,1
1392,2019-8-23,2019,8,23,2,cu0n5l,Flawed Algorithms Are Grading Millions of Students.,https://www.reddit.com/r/MachineLearning/comments/cu0n5l/flawed_algorithms_are_grading_millions_of_students/,iamart_intelligence,1566494880,,0,1
1393,2019-8-23,2019,8,23,2,cu0wz8,[P] OpenGPT-2: We trained GPT-2 Because You Can Too: 1.5B Weights Released! - Colab and blogpost linked below.,https://www.reddit.com/r/MachineLearning/comments/cu0wz8/p_opengpt2_we_trained_gpt2_because_you_can_too/,Skylion007,1566496060,,10,18
1394,2019-8-23,2019,8,23,3,cu16cr,How to Start with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cu16cr/how_to_start_with_machine_learning/,atomlib_com,1566497172,,0,1
1395,2019-8-23,2019,8,23,3,cu1o13,[P] Create and connect to secure EC2 instances without dealing with tedious configurations.,https://www.reddit.com/r/MachineLearning/comments/cu1o13/p_create_and_connect_to_secure_ec2_instances/,WildShallot,1566499278,"This open-source tool allows you to create AWS instances and also takes care of all the ssh configs so you can connect to it from the command line or VS Code (using remote ssh plugin). You can choose what resources you need (CPU, GPU, memory) from the command line.

[https://github.com/provisionpad](https://github.com/provisionpad)

&amp;#x200B;

I always find it difficult and confusing to deal with AWS, so I think this makes is a lot easier to work with AWS and abstracts the clutter away. Would be curious to hear how you guys deal with this problem.

Here is an example of working directly in an AWS EC2 instance from my local VS Code:

[https://imgur.com/a/ZJMxsQl](https://imgur.com/a/ZJMxsQl)",0,1
1396,2019-8-23,2019,8,23,3,cu1r9e,AI Cheatsheets: Your favorite cheatsheets are now available on aicheatsheets.com,https://www.reddit.com/r/MachineLearning/comments/cu1r9e/ai_cheatsheets_your_favorite_cheatsheets_are_now/,kailashahirwar12,1566499683,,0,1
1397,2019-8-23,2019,8,23,3,cu1saa,[N] Keras 2.2.5 released. The 2.3.0 release will be the last major release of multi-backend Keras. Multi-backend Keras is superseded by tf.keras.,https://www.reddit.com/r/MachineLearning/comments/cu1saa/n_keras_225_released_the_230_release_will_be_the/,maksim-m,1566499800,,0,1
1398,2019-8-23,2019,8,23,3,cu1t0i,[P] Connect to AWS without dealing with tedious configurations.,https://www.reddit.com/r/MachineLearning/comments/cu1t0i/p_connect_to_aws_without_dealing_with_tedious/,WildShallot,1566499894,"This open-source tool allows you to create AWS instances and also takes care of all the ssh configs so you can connect to it from the command line or VS Code (using remote ssh plugin). You can choose what resources you need (CPU, GPU, memory) from the command line.

[https://github.com/provisionpad](https://github.com/provisionpad)

&amp;#x200B;

I always find it difficult and confusing to deal with AWS, so I think this makes is a lot easier to work with AWS and abstracts the clutter away. Would be curious to hear how you guys deal with this problem.

Here is an example of working directly in an AWS EC2 instance from my local VS Code:

[https://imgur.com/a/ZJMxsQl](https://imgur.com/a/ZJMxsQl)",9,5
1399,2019-8-23,2019,8,23,4,cu21cg,Looking for interesting topic for my PhD research,https://www.reddit.com/r/MachineLearning/comments/cu21cg/looking_for_interesting_topic_for_my_phd_research/,barbek,1566500883,"So, from today I'm a PhD student and am looking for interesting topic of my research.

A little bit about me\^  
 \- got masters' degree in 2013

\- worked at two companies from 2012 till now with very different tasks: from porting linux to new arch to developing activity recognition system

\- got into Computer vision in 2016(maybe 2015), now my main job is development of different CV systems

\- not really sure why, but decided to get into university and try myself at academia a little bit, without quiting my job

Potentially interesting fields for research  
\- Generative algorithms. One of my first projects was pix2pix-based system, which converts winter photos to summer photos and vice versa

\- interpretable machine learning - just nice to know what those black boxes learn

\- 3d(where third d is depth) algorithms(deep learning including)

&amp;#x200B;

At first I thought about somehow to continue  my work on summer-to-winter translation, but it was so long ago that I'll probably have to repeat dataset gathering stage and might as well think about something else, plus it's not as exciting now that I already reached some kind of solution. Other than that I have no ideas about what to do",0,1
1400,2019-8-23,2019,8,23,4,cu25mz,Shared Machine Learning: Ant Financials Solution For Data Privacy,https://www.reddit.com/r/MachineLearning/comments/cu25mz/shared_machine_learning_ant_financials_solution/,Yuqing7,1566501404,,0,1
1401,2019-8-23,2019,8,23,4,cu29al,Popularity bias on collaborative filtering recommendations,https://www.reddit.com/r/MachineLearning/comments/cu29al/popularity_bias_on_collaborative_filtering/,Vichnaiev,1566501837,"I have an app with multiple restaurants and multiple dishes and I'm coding a dish recommendation system for it. My problem is: our data is not always very well distributed (real data never is). I don't want every user to open the restaurant and the first dish recommended is inevitable the most popular dish in that particular restaurant and somehow disregard that individual's particularities.

In order to achieve that, would it be healthy to limit the number of ratings one dish can have in the training data and discard any ratings after the limit is reached? If so, how do I choose which ratings to keep and which ones to discard? Randomly?",0,1
1402,2019-8-23,2019,8,23,5,cu2ynl,PhD in Machine Learning (computer vision) in another country?,https://www.reddit.com/r/MachineLearning/comments/cu2ynl/phd_in_machine_learning_computer_vision_in/,DaBobcat,1566504895,[removed],0,1
1403,2019-8-23,2019,8,23,5,cu35dy,[R] OpenAI's new method to test robustness against nnforeseen adversaries.,https://www.reddit.com/r/MachineLearning/comments/cu35dy/r_openais_new_method_to_test_robustness_against/,youali,1566505712,"Testing Robustness Against Unforeseen Adversaries.

""Weve developed a method to assess whether a neural network classifier can reliably defend against adversarial attacks not seen during training. Our method yields a new metric, UAR (Unforeseen Attack Robustness), which evaluates the robustness of a single model against an unanticipated attack, and highlights the need to measure performance across a more diverse range of unforeseen attacks.""

- Blog post: https://openai.com/blog/testing-robustness/
- Paper: https://arxiv.org/abs/1908.08016
- Code: https://github.com/ddkang/advex-uar",0,1
1404,2019-8-23,2019,8,23,5,cu3621,[R] Few-shot Text Classification with Distributional Signatures,https://www.reddit.com/r/MachineLearning/comments/cu3621/r_fewshot_text_classification_with_distributional/,Boga2510,1566505795,"What happens if you take meta-learning for vision and apply it to NLP? Prototypical Networks with lexical features perform worse than nearest neighbors on new classes. How can we do better?

arXiv: [arxiv.org/abs/1908.06039](https://arxiv.org/abs/1908.06039)

code: [https://github.com/YujiaBao/Distributional-Signatures](https://github.com/YujiaBao/Distributional-Signatures)",0,4
1405,2019-8-23,2019,8,23,5,cu36bw,OpenAI's new method to test robustness against unforeseen adversaries,https://www.reddit.com/r/MachineLearning/comments/cu36bw/openais_new_method_to_test_robustness_against/,youali,1566505831,[removed],0,1
1406,2019-8-23,2019,8,23,5,cu38py,Cool introductory tutorial to machine learning with RunwayML and Processing by creating generative animated landscapes,https://www.reddit.com/r/MachineLearning/comments/cu38py/cool_introductory_tutorial_to_machine_learning/,heaversm,1566506124,[removed],0,1
1407,2019-8-23,2019,8,23,6,cu42dq,Start-Ups using Bayesian Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/cu42dq/startups_using_bayesian_deep_learning/,misery_buzinezz,1566509723,[removed],0,1
1408,2019-8-23,2019,8,23,6,cu44hs,"Data Engineering, Big Data, and Machine Learning on Google Cloud Platform Specialization  Time to accelerate your income and career growth",https://www.reddit.com/r/MachineLearning/comments/cu44hs/data_engineering_big_data_and_machine_learning_on/,internetdigitalentre,1566509995,[removed],0,1
1409,2019-8-23,2019,8,23,7,cu4f06,GPT-2 345M suppost to be much slower than 117M?,https://www.reddit.com/r/MachineLearning/comments/cu4f06/gpt2_345m_suppost_to_be_much_slower_than_117m/,orenog,1566511303,[removed],0,1
1410,2019-8-23,2019,8,23,7,cu4i33,[D] GPT-2 345M suppost to be much slower than 117M?,https://www.reddit.com/r/MachineLearning/comments/cu4i33/d_gpt2_345m_suppost_to_be_much_slower_than_117m/,orenog,1566511664," I am using runway, I have an i7 4770k @3.9GHZ

I generates stuff from 117M and it took about 29-35 sec, tried 774M but it didn't start, now I am running 345, and it's already running for more than 20 minutes and nothing happenes, I was waiting 20 minutes, then restarted, and now I am at 20 minutes again just waiting, does it support to take that much longer?",4,1
1411,2019-8-23,2019,8,23,7,cu4lmm,"[Research] New design strategies achieve unprecedented levels of inference accuracy for all-optical, neural-network-based machine learning.",https://www.reddit.com/r/MachineLearning/comments/cu4lmm/research_new_design_strategies_achieve/,PhotonicsWest,1566512123,,0,1
1412,2019-8-23,2019,8,23,7,cu4oxo,AI: A meme is also a cat,https://www.reddit.com/r/MachineLearning/comments/cu4oxo/ai_a_meme_is_also_a_cat/,datramt,1566512544,"Me: a cat is a furry being  
AI: a cat is a furry being often accompanied by bottom text  
Me: ok

![video](bpzs8rbjr2i31)",0,1
1413,2019-8-23,2019,8,23,7,cu4ria,What ML project can help save this planet?,https://www.reddit.com/r/MachineLearning/comments/cu4ria/what_ml_project_can_help_save_this_planet/,niceguy__I_,1566512859,[removed],0,1
1414,2019-8-23,2019,8,23,7,cu4tom,AI: A meme is also a type of cat,https://www.reddit.com/r/MachineLearning/comments/cu4tom/ai_a_meme_is_also_a_type_of_cat/,datramt,1566513161,,0,1
1415,2019-8-23,2019,8,23,7,cu4yg0,[R][OpenAI] Testing Robustness Against Unforeseen Adversaries,https://www.reddit.com/r/MachineLearning/comments/cu4yg0/ropenai_testing_robustness_against_unforeseen/,downtownslim,1566513755,"Weve developed a method to assess whether a neural network classifier can reliably defend against adversarial attacks not seen during training. Our method yields a new metric, UAR (Unforeseen Attack Robustness), which evaluates the robustness of a single model against an unanticipated attack, and highlights the need to measure performance across a more diverse range of unforeseenattacks.

Website: [https://openai.com/blog/testing-robustness/](https://openai.com/blog/testing-robustness/)

Paper: [http://arxiv.org/abs/1908.08016](http://arxiv.org/abs/1908.08016)

Code: [https://github.com/ddkang/advex-uar](https://github.com/ddkang/advex-uar)",1,36
1416,2019-8-23,2019,8,23,7,cu4ype,Top 10 IPython Tutorials for Data Science and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cu4ype/top_10_ipython_tutorials_for_data_science_and/,andrea_manero,1566513790,http://www.datasciencecentral.com/profiles/blogs/top-10-ipython-tutorials-for-data-science-and-machine-learning,0,1
1417,2019-8-23,2019,8,23,7,cu4yvs,Linear loss estimators,https://www.reddit.com/r/MachineLearning/comments/cu4yvs/linear_loss_estimators/,sudeepraja,1566513809,,0,1
1418,2019-8-23,2019,8,23,8,cu5gtv,[R] Adversarial point perturbations on 3D objects,https://www.reddit.com/r/MachineLearning/comments/cu5gtv/r_adversarial_point_perturbations_on_3d_objects/,c0deb0t,1566516196,"I am a high school student, and I've been working on a very niche field featuring the intersection of 3D deep learning and adversarial robustness.

My recent paper is on generating adversarial point sets against neural networks like PointNet. In total, there are four novel attacks in two categories: distributional and shape attacks. Distributional attacks barely change the shape of a point set because we use the shape-aware Hausdorff distance instead of a p-norm. Shape attacks are focused on changing the shape (that results in changes to the point clouds), which is realistic and robust against point removal defenses to restore an adversarial point cloud that some other groups and I have previously proposed. Note that these shape attacks do *not* need any extra info other than the raw 3D points.

You can read about point set adversarial attacks in my blog post: https://blog.liudaniel.com/birth-of-a-new-sub-sub-field

Or read my latest paper: https://arxiv.org/abs/1908.06062",15,70
1419,2019-8-23,2019,8,23,9,cu61is,"I asked a.i to complete my YouTube channel's AD, but it went crazy, thank you 744M for making my channel muslin",https://www.reddit.com/r/MachineLearning/comments/cu61is/i_asked_ai_to_complete_my_youtube_channels_ad_but/,orenog,1566519037,,0,1
1420,2019-8-23,2019,8,23,9,cu61ok,"[p] I asked a.i to complete my YouTube channel's AD, but it went crazy, thank you 744M for making my channel muslin",https://www.reddit.com/r/MachineLearning/comments/cu61ok/p_i_asked_ai_to_complete_my_youtube_channels_ad/,orenog,1566519056,,0,1
1421,2019-8-23,2019,8,23,9,cu693a,Amazon Forecast Now Generally Available,https://www.reddit.com/r/MachineLearning/comments/cu693a/amazon_forecast_now_generally_available/,devopscript,1566520104,,0,1
1422,2019-8-23,2019,8,23,9,cu6gqm,ELI5 CTC,https://www.reddit.com/r/MachineLearning/comments/cu6gqm/eli5_ctc/,pilnix,1566521206,[removed],0,1
1423,2019-8-23,2019,8,23,10,cu6rww,ELI5 Beam Search,https://www.reddit.com/r/MachineLearning/comments/cu6rww/eli5_beam_search/,pilnix,1566522809,[removed],0,1
1424,2019-8-23,2019,8,23,10,cu77gw,Trying to learn machine learn but having trouble copying sample images into my bucket.,https://www.reddit.com/r/MachineLearning/comments/cu77gw/trying_to_learn_machine_learn_but_having_trouble/,RustTakesMyTime,1566525106,[removed],0,1
1425,2019-8-23,2019,8,23,11,cu7vcn,[N] Announcing the xView 2 Prize Challenge: Assess Building Damage,https://www.reddit.com/r/MachineLearning/comments/cu7vcn/n_announcing_the_xview_2_prize_challenge_assess/,nirav_diu,1566528736," 

https://i.redd.it/6h9ydxj644i31.jpg

When disaster strikes, speed is critical. The time it takes to properly assess damage in the wake of a major event can be the difference between life and death. However, emergency responders must often navigate disruptions to local communication and transportation infrastructure, making accurate assessments dangerous, difficult, and slow. While satellite and aerial imagery offer a less risky alternative that covers more ground, analysts must still conduct manual, time-intensive assessments of images. 

The [Defense Innovation Unit](https://www.diu.mil/)s (DIUs) xView2 Challenge (Challenge) seeks to automate post-disaster damage assessment. DIU is challenging machine learning experts to develop computer vision algorithms that will speed up analysis of satellite and aerial imagery by localizing and categorizing various types of building damage caused by natural disasters. The xView2 Challenge is DIUs second prize competition focused on furthering innovation in computer vision for humanitarian assistance and disaster relief (HADR) efforts. This years competition builds upon the xView1 Challenge, which sought out computer vision algorithms to locate and identify distinct objects on the ground useful to first responders. 

DIUs goal in hosting this Challenge is to enlist the global community of machine learning experts to tackle a critically hard problem: detecting key objects in overhead imagery in context and assessing damage in a disaster situation, said Mike Kaul, DIU AI Portfolio Director. 

We are always looking for ways to improve rapid damage assessment to ensure we and our partners deliver the right resources to the right places at the right time, and we are confident the DIU Challenge can contribute to that goal, said FEMA Regional Administrator Robert Fenton, a partner in the Challenge.

DIU led a team of experts from academia and industry to create a new dataset, xBD, to enable both localization and damage assessment before and after disasters. The dataset will provide the foundation for the Challenge. While several open datasets for object detection from satellite imagery already exist (e.g. SpaceNet, [xView 1](http://xviewdataset.org/)), each represent only a single snapshot in time and lack information about the type and severity of damage following a disaster. xBD is currently the largest and most diverse annotated building damage dataset, allowing ML/AI practitioners to generate and test models to help automate building damage assessment. The open source electro-optical imagery (0.3 m resolution) xBD dataset will encompass \~544,556 building annotations across \~19,520 square kilometers of freely available imagery from multiple countries\*. Six disaster types are included: wildfire, landslides, volcanic eruptions, earthquakes/tsunamis, and wind and flooding damage (\*more data is being added to xBD as the labeling becomes available). 

There are three competition prize tracks for the xView2 Challenge: 

1. **Open source.** Teams compete for leaderboard position and awards for top scores. By releasing their models publicly under a permissive open source license, teams also become eligible for an additional open source award. 
2. **Non-exclusive Government purpose rights.** Teams grant government purpose rights to become eligible for awards or top scores on the leaderboard. Solutions can be used to help future disaster recovery efforts.
3. **Evaluation Only.** Teams retain IP and only grant rights to benchmark their solution and compete for leaderboard position. Top teams in this category will still be eligible for a special monetary prize pool for their submissions.

The best solutions for all three categories will be eligible for a share of a $150,000 prize purse. Top solvers will also be invited to present their work at the December NeurIPS 2019 Workshop on AI for HADR. Winners of any cash prize will be considered eligible to be awarded follow-on work with the Department of Defense. The competition will start in September 2019 and run through November 2019. 

Findings will be applied in both operational and academic use cases that include, but are not limited to: obstructed roads, rerouting across obstructed roads, force of nature identification, resource allocation decision-making, object recognition, and object identification . Baseline models, developed collaboratively between DIU and Carnegie Mellons Software Engineering Institute, will be publicly available as a starting point for the Challenge. In addition to advancing the state of the art in damage assessment, it is envisioned that the xBD dataset will provide researchers, companies, and other groups with the means and motive to develop algorithms that bring humanitarian assistance and disaster response into the age of AI. 

The Challenges partners represent a first-of-its-kind coalition between the artificial intelligence and disaster response communities including NASA Earth Science Disasters Program, Federal Emergency Management Agencys Region 9, California Governors Office of Emergency Services (CAL OES), California Department of Forestry and Fire Protection (CAL FIRE), California Guard, DoDs Joint Artificial Intelligence Center, Carnegie Mellon s Software Engineering Institute, the United States Geological Service, National Geospatial-Intelligence Agency, and the National Security Innovation Network (NSIN). 

Additional details about the dataset can be found at ([CVPR 2019 xBD Paper](http://openaccess.thecvf.com/content_CVPRW_2019/papers/cv4gc/Gupta_Creating_xBD_A_Dataset_for_Assessing_Building_Damage_from_Satellite_CVPRW_2019_paper.pdf)) which was first discussed at the IEEE CVPR in June . Terms and rules of the Challenge can be found on the xView2 website, [xview2.org/terms](https://xview2.org/terms).

Visit [xview2.org](https://xview2.org) to register. 

##",10,41
1426,2019-8-23,2019,8,23,13,cu8rfy,[Question] contextual spell checker,https://www.reddit.com/r/MachineLearning/comments/cu8rfy/question_contextual_spell_checker/,khatrimann,1566533735,"I want to create a spell checker that corrects the spelling mistakes contextually.

For example,

Erroneous sentence: I want to apply for creditcart

Corrected sentence: I want to apply for creditcard

Here, the respective spellings ofcartandcardare correct but thecartis contextually incorrect.

So what methods we can apply for contextual errors like this?",0,1
1427,2019-8-23,2019,8,23,14,cu9jzc,[D] Can you use LBFGS to solve Huber linear regression problems?,https://www.reddit.com/r/MachineLearning/comments/cu9jzc/d_can_you_use_lbfgs_to_solve_huber_linear/,sbarratt,1566538763,"That is, is it guarantee to converge to a minimum even though the Huber loss isnt second-differentiable?",1,2
1428,2019-8-23,2019,8,23,15,cu9r1k,Ready to try something new?,https://www.reddit.com/r/MachineLearning/comments/cu9r1k/ready_to_try_something_new/,getengati,1566540097,[removed],0,1
1429,2019-8-23,2019,8,23,15,cu9r65,Dice metric increases while BCE loss increases,https://www.reddit.com/r/MachineLearning/comments/cu9r65/dice_metric_increases_while_bce_loss_increases/,hilil077,1566540118,[removed],0,1
1430,2019-8-23,2019,8,23,15,cu9xgi,[P] OpenGPT-2: We Replicated GPT-2 Because You Can Too,https://www.reddit.com/r/MachineLearning/comments/cu9xgi/p_opengpt2_we_replicated_gpt2_because_you_can_too/,baylearn,1566541300,"The author trained a 1.5 billion param GPT-2 model on a similar sized text dataset called [OpenWebTextCorpus](https://skylion007.github.io/OpenWebTextCorpus/) and they reported perplexity results that can be compared with the original model.

*Recently, large language models like BERT, XLNet, GPT-2, and Grover have demonstrated impressive results in generating text and on multiple NLP tasks. Since Open-AI has not released their largest model at this time (but has released their 774M param model), we seek to replicate their 1.5B model to allow others to build on our pretrained model and further improve it.*

https://medium.com/@vanya_cohen/opengpt-2-we-replicated-gpt-2-because-you-can-too-45e34e6d36dc",64,226
1431,2019-8-23,2019,8,23,16,cuab9g,easy ways to create cool GIFs out of your plots ?,https://www.reddit.com/r/MachineLearning/comments/cuab9g/easy_ways_to_create_cool_gifs_out_of_your_plots/,pigdogsheep,1566543950,[removed],0,1
1432,2019-8-23,2019,8,23,16,cuad71,Resources for memorable Ai/ML and Open Source BigDat processing workshop,https://www.reddit.com/r/MachineLearning/comments/cuad71/resources_for_memorable_aiml_and_open_source/,iCode4Beer,1566544324,[removed],0,1
1433,2019-8-23,2019,8,23,16,cuakyd,"Meet Mindar, the robotic Buddhist priest",https://www.reddit.com/r/MachineLearning/comments/cuakyd/meet_mindar_the_robotic_buddhist_priest/,abhayit2000,1566545860,,0,1
1434,2019-8-23,2019,8,23,16,cuaqdq,"[D] Where can I find CNN baseline architectures for MNIST, CIFAR-10?",https://www.reddit.com/r/MachineLearning/comments/cuaqdq/d_where_can_i_find_cnn_baseline_architectures_for/,adelredjimi,1566546885,"For my research in semi-supervised learning, I am looking for good CNN architectures with their respective optimization hyperparameters and pre-processing schemes to be used as baselines for my work on semi-supervised learning.",9,0
1435,2019-8-23,2019,8,23,16,cuar3j,Wired for Success: How to Keep Your Security Operations Team Satisfied,https://www.reddit.com/r/MachineLearning/comments/cuar3j/wired_for_success_how_to_keep_your_security/,MariaMiladinovikj,1566547044,,0,1
1436,2019-8-23,2019,8,23,16,cuarh1,Why does keras implementation of MobileNetV2 has only 16 bottleneck layers compared to 19 bottleneck layers in MobileNetV2 paper?,https://www.reddit.com/r/MachineLearning/comments/cuarh1/why_does_keras_implementation_of_mobilenetv2_has/,passwordprotet,1566547117,[removed],0,1
1437,2019-8-23,2019,8,23,17,cuate5,Image captioning model refuses to look at image features,https://www.reddit.com/r/MachineLearning/comments/cuate5/image_captioning_model_refuses_to_look_at_image/,YuansongFeng,1566547528,[removed],1,1
1438,2019-8-23,2019,8,23,17,cub0g2,Automatic Data Augmentation for NLP task,https://www.reddit.com/r/MachineLearning/comments/cub0g2/automatic_data_augmentation_for_nlp_task/,I_ai_AI,1566549073,,1,6
1439,2019-8-23,2019,8,23,17,cub3gl,r/datascienceproject is live,https://www.reddit.com/r/MachineLearning/comments/cub3gl/rdatascienceproject_is_live/,OppositeMidnight,1566549732,[removed],0,1
1440,2019-8-23,2019,8,23,18,cubh35,Friday freestyle tutorial- a upperbody syncopated wrist roll into the b...,https://www.reddit.com/r/MachineLearning/comments/cubh35/friday_freestyle_tutorial_a_upperbody_syncopated/,thetrickshotone,1566552469,,0,1
1441,2019-8-23,2019,8,23,18,cubiq5,"What is the difference between Artificial Intelligence, Machine Learning, Natural Language Processing and Deep Learning?",https://www.reddit.com/r/MachineLearning/comments/cubiq5/what_is_the_difference_between_artificial/,maryiakot,1566552798,[removed],0,1
1442,2019-8-23,2019,8,23,18,cubkxy,[D] Attention layer yields inconsistent results,https://www.reddit.com/r/MachineLearning/comments/cubkxy/d_attention_layer_yields_inconsistent_results/,lazywiing,1566553187,"Hi reddit,

I am currently working on a problem that involves Recurrent Neural Networks. More precisely, I am dealing with sequences of inputs and try to make a prediction at each time step. As such, I decided to try to include an attention layer that is to look on the left context only.

The problem with this is that the results vary depending on the validation data, and by this, I mean a difference of 10-15% accuracy ! I suspect that something is not going so well. I even wonder if the attention layer does not look at both the left and right contexts, which would partly explain why sometimes performances are so good (but I set `history_only=True` ).

Has it ever happened to you, and what would you do in such situations ? Thank you :)",6,0
1443,2019-8-23,2019,8,23,18,cubnzh,[D] How do you handle the high uncertainty of your timeline/deadline for delivering a ml/dl product?,https://www.reddit.com/r/MachineLearning/comments/cubnzh/d_how_do_you_handle_the_high_uncertainty_of_your/,pirate7777777,1566553774,"Hi everyone! I found [this interesting post on LinkedIn](https://www.linkedin.com/posts/mundher-alshabi_datasciences-activity-6570536425006166016-tOS4) and I would like to know your opinion about.  


Here's the post for those of you who don't have a LinkedIn account or the author of the post in your connections.

""The most challenging problem data scientists are facing today is having a highly uncertain timeline/deadline for delivering a product. I find it challenging to predict the due delivery date for a data science-related product. Because you need much experiment to understand the problem and then propose the solution, before that, it's impossible to determine the timeline or even the accuracy that can be achieved. Also, sometimes after the EDA you may face many difficulties that may change the deadline. How can we design a data science project in a more deterministic way like regular software? e.g., the first-month design database, then in the second design the login page, etc. I would love to know how do you deal with it."" - Mundher Al-Shabi - Data Scientist at CADS",26,52
1444,2019-8-23,2019,8,23,19,cubyg5,Visualizing Class Activation Maps to make ConvNets interpretable in medical imaging,https://www.reddit.com/r/MachineLearning/comments/cubyg5/visualizing_class_activation_maps_to_make/,ahmedbesbes,1566555763,,0,1
1445,2019-8-23,2019,8,23,19,cuc5gl,Quality Servo Voltage Stabilizer Manufacturer Suppliers India,https://www.reddit.com/r/MachineLearning/comments/cuc5gl/quality_servo_voltage_stabilizer_manufacturer/,IECO_INDIA,1566557057,[removed],0,1
1446,2019-8-23,2019,8,23,20,cuccd0,What is adversarial autoencoders?,https://www.reddit.com/r/MachineLearning/comments/cuccd0/what_is_adversarial_autoencoders/,clean_pegasus,1566558309,[removed],0,1
1447,2019-8-23,2019,8,23,20,cuciay,Machine Learning Course,https://www.reddit.com/r/MachineLearning/comments/cuciay/machine_learning_course/,dwivediabhinav,1566559320,"[***Machine Learning Course in Pune***](https://www.excelr.com/machine-learning-course-training-in-pune/).Excelr is the best institute for Machine Learning  course. Here you got a very Top-notch faculty with much experience,and they are also providing the Certifications from the University of Malaysia. ExcelR is a global leader in technical and management training catering the training needs of the professionals in more than 27 countries with offices in USA, Malaysia, India etc. with over 21 branches across the globe.

&amp;#x200B;

![video](cx4c5gqhn6i31 ""Raising Excellence"")",0,1
1448,2019-8-23,2019,8,23,20,cucnt0,[R] A collection of notes on 150 papers in neural dialog modeling and related areas,https://www.reddit.com/r/MachineLearning/comments/cucnt0/r_a_collection_of_notes_on_150_papers_in_neural/,ricsinaruto,1566560243, [https://github.com/ricsinaruto/Seq2seqChatbots/wiki/Chatbot-and-Related-Research-Paper-Notes-with-Images](https://github.com/ricsinaruto/Seq2seqChatbots/wiki/Chatbot-and-Related-Research-Paper-Notes-with-Images),7,71
1449,2019-8-23,2019,8,23,20,cuctiv,Transformers from Scratch,https://www.reddit.com/r/MachineLearning/comments/cuctiv/transformers_from_scratch/,iamkeyur,1566561187,,0,1
1450,2019-8-23,2019,8,23,20,cucvrm,[P] r/datascienceproject is live,https://www.reddit.com/r/MachineLearning/comments/cucvrm/p_rdatascienceproject_is_live/,OppositeMidnight,1566561563,Post your data science project to [r/datascienceproject](https://www.reddit.com/r/datascienceproject/) without restrictions. It's a way to notify the public of what you are working on .,0,0
1451,2019-8-23,2019,8,23,21,cud6d3,How do you bulid a model to predict sparse multivariate timeseries?,https://www.reddit.com/r/MachineLearning/comments/cud6d3/how_do_you_bulid_a_model_to_predict_sparse/,SubstantialSwimmer4,1566563210,[removed],0,1
1452,2019-8-23,2019,8,23,21,cud82g,Language and cognition vs theoretical ML,https://www.reddit.com/r/MachineLearning/comments/cud82g/language_and_cognition_vs_theoretical_ml/,revererosie,1566563474,[removed],0,1
1453,2019-8-23,2019,8,23,21,cud8rg,Oracles Machine Learning Strategy,https://www.reddit.com/r/MachineLearning/comments/cud8rg/oracles_machine_learning_strategy/,the_spotless_mind,1566563582,,0,1
1454,2019-8-23,2019,8,23,21,cudcul,[D] Deep Reinforcement Learning (research) engineer as MSc?,https://www.reddit.com/r/MachineLearning/comments/cudcul/d_deep_reinforcement_learning_research_engineer/,Roboserg,1566564190,"I am from Germany and I looked for jobs both here in Germany and USA for Deep Reinforcement Learning positions. Every single position I've found required a Ph.D. I understand why, the field is new and mostly academic / research work. Still I wonder if anyone has any information about getting a job maybe not as a researcher scientist (where Ph.D. would be required) but maybe as a research engineer when having a [M.Sc](https://M.Sc) degree? As a research engineer you implement papers to solve current problems. The question is, is there any hope for the field of Deep Reinforcement Learning currently? I know for ""classic"" Deep Learning (supervised etc) such positions exist, but I am very interested in deep RL. 

I am nearning the end of my [M.Sc](https://M.Sc). in robotics with the master thesis being on Deep Learning. I am teaching myself deep RL on my free time and would like to pursue a career in that field. I find RL and agents interacting with the environment fascinating. 

Would like to hear your opinion.",38,3
1455,2019-8-23,2019,8,23,21,cudge8,What is An Algorithm? 2mins Explanation [Easy],https://www.reddit.com/r/MachineLearning/comments/cudge8/what_is_an_algorithm_2mins_explanation_easy/,updownvizzii,1566564696,,0,1
1456,2019-8-23,2019,8,23,21,cudiz8,Join Discord: Healthcare Artifical Inteliigence,https://www.reddit.com/r/MachineLearning/comments/cudiz8/join_discord_healthcare_artifical_inteliigence/,Mike_Litoris_Isawer,1566565080,[removed],0,1
1457,2019-8-23,2019,8,23,22,cudkyp,[Project] Keras implementations of Attention-based BatchNormalization papers,https://www.reddit.com/r/MachineLearning/comments/cudkyp/project_keras_implementations_of_attentionbased/,cyril_9227,1566565367,"Hello guys,

I have recently implemented two papers about attention-based BatchNormalization.

&amp;#x200B;

2) Attentive Normalization :

Arxiv link : [https://arxiv.org/abs/1908.01259](https://arxiv.org/abs/1908.01259)

Official Pytorch implementation : Not yet released but will be available here [https://github.com/ivMCL/AttentiveNorm](https://github.com/ivMCL/AttentiveNorm)

My Keras implementation : [https://github.com/Cyril9227/Keras\_AttentiveNormalization](https://github.com/Cyril9227/Keras_AttentiveNormalization)

&amp;#x200B;

2) Instance Enhancement Batch Normalization :

Arxiv link : [https://arxiv.org/abs/1908.01259](https://arxiv.org/abs/1908.01259)

Official Pytorch implementation : [https://github.com/gbup-group/IEBN](https://github.com/gbup-group/IEBN)

My Keras implementation : [https://github.com/Cyril9227/Keras\_IEBN](https://github.com/Cyril9227/Keras_IEBN)

&amp;#x200B;

Both implementations work as a simple droppin replacement of standard BatchNorm layer. Any feedbacks are welcome !

Thank's :)",2,7
1458,2019-8-23,2019,8,23,22,cue0p1,Small Datasets SVM's struggle on,https://www.reddit.com/r/MachineLearning/comments/cue0p1/small_datasets_svms_struggle_on/,TumericAndreShow,1566567505,[removed],0,1
1459,2019-8-23,2019,8,23,22,cue1z5,Phd for Artificial intelligence in chemistry,https://www.reddit.com/r/MachineLearning/comments/cue1z5/phd_for_artificial_intelligence_in_chemistry/,riley_kel,1566567675,"Hello, I'll soon be through with my undergraduate work in chemistry major, I have a good background in deep Learning and reinforcement learning, does anyone know where I can do a graduate program that combines these fields?",0,1
1460,2019-8-23,2019,8,23,22,cue8il,[D] Generative AI for cursive writing,https://www.reddit.com/r/MachineLearning/comments/cue8il/d_generative_ai_for_cursive_writing/,Redstoner7,1566568559,"Has anyone made an AI trained on hand writing to generate a specific sentience?  
If not how would one go about making this?",3,0
1461,2019-8-23,2019,8,23,23,cuehoy,[P] Encrypted Deep Learning Training and Predictions with TF Encrypted Keras,https://www.reddit.com/r/MachineLearning/comments/cuehoy/p_encrypted_deep_learning_training_and/,eugenecamus,1566569777,[removed],0,1
1462,2019-8-23,2019,8,23,23,cuew06,Shared Machine Learning: An Alternative to Federated Learning?,https://www.reddit.com/r/MachineLearning/comments/cuew06/shared_machine_learning_an_alternative_to/,Yuqing7,1566571661,,0,1
1463,2019-8-23,2019,8,23,23,cuey2n,best tipes for creating videos date - Violence Detection,https://www.reddit.com/r/MachineLearning/comments/cuey2n/best_tipes_for_creating_videos_date_violence/,bardpeter,1566571922,[removed],0,1
1464,2019-8-24,2019,8,24,2,cugo45,PSA: Vultr is giving away $50 for new accounts,https://www.reddit.com/r/MachineLearning/comments/cugo45/psa_vultr_is_giving_away_50_for_new_accounts/,desertstrike2000,1566579704,,0,1
1465,2019-8-24,2019,8,24,2,cugvvd,[Hardware] Using AMD and NVidia cards on the same workstation,https://www.reddit.com/r/MachineLearning/comments/cugvvd/hardware_using_amd_and_nvidia_cards_on_the_same/,Panosls,1566580660,"Greetings.

I am a developer working with machine learning, and I'm building a new system with a Ryzen 7 3700X and NVidia RTX 2080Ti for my workstation needs.

I also have a Sapphire Vega 64 GPU for a ROCm project and I was wondering if the two cards would play nice together on a single system/single boot.

    Ryzen 7 3700X w/ Corsair H150i Pro
    Asus Strix X570 Strix-E
    Crucial Ballistix Sport LT 32GB 3200C16
    Corsair HX1000
    
    MSI RTX 2080Ti Gaming X Trio
    Sapphire Vega 64 Nitro+ 

Both projects can run within docker containers, but they still need drivers installed in the core OS, which is going to be Ubuntu 18.04 until April when 20.04 LTS comes out.

Has anyone tried anything like this?",0,1
1466,2019-8-24,2019,8,24,2,cuhggy,What are some of the most correct ways to tune and compare different models in an academic context?,https://www.reddit.com/r/MachineLearning/comments/cuhggy/what_are_some_of_the_most_correct_ways_to_tune/,busql,1566583173,"Hi,

Those days, I have been reviewing different academic papers which mainly compare the performance of different machine learning methods on a particular problem. And I was surprised by the variety of techniques used as a strategy to optimize the hyperparameters of each model and to compare the different models generated (in other words if I want to measure the predictive power of different models such as random forest, neural network, etc. as I optimize their parameters to maximize the predictive power of each one and then compare the different models to choose the best one).

Specifically, the techniques I have seen used can be classified into 3 groups:

* **Using nested cross validation**: outer loop is used to train and compare the different models and inner loop is used to tune the model hyperparameters (an example of this procedure can be found in [1])

* **Using flat cross validation**: flat cross validation is used to tune each of the different models and to train them, using the performance metrics obtained from each cross validation to compare the different models (an example of this procedure can be found in [2]). I assume that in this case in order to correctly compare the models the folds in each Cross Validation should be equal. This procedure also seems to be one of the most commons in internet posts.

* **Using flat cross validation + hold out set**: initially partitioning the data between train and test, using the train with CV to tune and train the models. After this, the different models with the optimized hyperparameters were tested on the test set, selecting the model that outperformed the others.

Also, I found some recent papers that still use a simple hold-out approach. For this reason, I find myself quite confused about what would be an adequate (or also the most accepted) procedure to tune and train different models and then select the best one, specially in the academic context.

Thanks!

[1] Wang, H., Zhou, Z., Li, Y., Chen, Z., Lu, P., Wang, W., ... &amp; Yu, L. (2017). Comparison of machine learning methods for classifying mediastinal lymph node metastasis of non-small cell lung cancer from 18 F-FDG PET/CT images. EJNMMI research, 7(1), 11.

[2] Li, J., Dai, W., Metze, F., Qu, S., &amp; Das, S. (2017, March). A comparison of deep learning methods for environmental sound detection. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 126-130). IEEE.",0,1
1467,2019-8-24,2019,8,24,3,cuhj9i,Machine Learning From Scratch With Python,https://www.reddit.com/r/MachineLearning/comments/cuhj9i/machine_learning_from_scratch_with_python/,codingislife496,1566583514,"  [https://play.google.com/store/apps/details?id=com.qa.machinelearningfromscratch2](https://play.google.com/store/apps/details?id=com.qa.machinelearningfromscratch2) 

Hi. i developed new app for machine learning from scratch with python. I wish, you like it. It is really useful for learning advance machine learning. Please download my app and if you liked the app click to ads for support me. Machine Learning is everywhere ! Thank you so much guys.",1,1
1468,2019-8-24,2019,8,24,3,cuhkwa,Improved Hypergradient based optimizers.,https://www.reddit.com/r/MachineLearning/comments/cuhkwa/improved_hypergradient_based_optimizers/,harshalmittal4,1566583704,"This work tries improvements to the existing 'Hypergradient' based optimizers proposed in the paper [Online Learning Rate Adaptation with Hypergradient Descent](https://arxiv.org/pdf/1703.04782.pdf). 

We expect that the hypergradient based learning rate update could be more accurate and aim to exploit the gains much better by boosting the learning rate updates with momentum and adaptive gradients, experimenting with

1. Hypergradient descent with momentum, and
2. Adam with Hypergradient,

alongside the model optimizers SGD, SGD with Nesterov(SGDN) and Adam. 

The new optimizers are compared with their resepective hypergradient-descent baselines and provide advantages such as better generalization and faster convergence for the loss function. The code and the results of our experiments are available [here](https://github.com/harshalmittal4/Hypergradient_variants).",0,1
1469,2019-8-24,2019,8,24,3,cuhvu8,Transformers from scratch,https://www.reddit.com/r/MachineLearning/comments/cuhvu8/transformers_from_scratch/,certain_entropy,1566585092,,0,1
1470,2019-8-24,2019,8,24,3,cui9ep,"Could Intelligent machines find intelligent aliens? A small team of SETI researchers is training artificial intelligence to look for alien messages we might already be receiving, but arent able to recognize. If ET calls, it might be AI that picks up the phone.",https://www.reddit.com/r/MachineLearning/comments/cui9ep/could_intelligent_machines_find_intelligent/,EricFromOuterSpace,1566586786,,0,1
1471,2019-8-24,2019,8,24,4,cuiexa,Huaweis First Commercial AI Chip Doubles the Training Performance of Nvidias Flagship GPU,https://www.reddit.com/r/MachineLearning/comments/cuiexa/huaweis_first_commercial_ai_chip_doubles_the/,Yuqing7,1566587459,,0,1
1472,2019-8-24,2019,8,24,4,cuiiwn,Microsoft Build 2019 | Want to *actually* do machine learning? Wrangle data build models and deploy them,https://www.reddit.com/r/MachineLearning/comments/cuiiwn/microsoft_build_2019_want_to_actually_do_machine/,LividDragonfly,1566587969,,0,1
1473,2019-8-24,2019,8,24,4,cuikii,"Made my first real project - a Sudoku solver, feedback appreciated!",https://www.reddit.com/r/MachineLearning/comments/cuikii/made_my_first_real_project_a_sudoku_solver/,Empan15,1566588169,"Hi!

I am in my last year of studies (taking a master in statistics) and next year I want to purse a career in something related to Data Science. I've read multiple times that it is really good to have some projects to show to land an interview. So I've just finished my first project, which is a Sudoku solver that takes an image of a Sudoku as input and solves it. I've uploaded it to GitHub and since this is my first real project (and first time using GitHub) I would heavily appreciate if I could get any feedback on the project. I appreciate any comments on any part of the project. The project can be viewed at:

[https://github.com/EmpanS/Project-Sudoku](https://github.com/EmpanS/Project-Sudoku)

Maybe this is the wrong forum to ask these kind of questions, if so, do you know a better place?

Thanks!",0,1
1474,2019-8-24,2019,8,24,4,cuisb4,Recommended Object Recognition API for custom model of small finite set of objects?,https://www.reddit.com/r/MachineLearning/comments/cuisb4/recommended_object_recognition_api_for_custom/,uveski,1566589171,[removed],0,1
1475,2019-8-24,2019,8,24,5,cuj9n7,Latest literature work related to Adversarial examples and to making robust model,https://www.reddit.com/r/MachineLearning/comments/cuj9n7/latest_literature_work_related_to_adversarial/,aniket_agarwal,1566591344,[removed],0,1
1476,2019-8-24,2019,8,24,5,cujf8m,10 books to learn machine learning,https://www.reddit.com/r/MachineLearning/comments/cujf8m/10_books_to_learn_machine_learning/,EnvironmentalWalrus7,1566592035,[removed],0,1
1477,2019-8-24,2019,8,24,5,cujoqv,Request for participants in a User-experience study!,https://www.reddit.com/r/MachineLearning/comments/cujoqv/request_for_participants_in_a_userexperience_study/,sbhuyan,1566593270,,0,1
1478,2019-8-24,2019,8,24,6,cujxhk,Huawei Mindspore features video,https://www.reddit.com/r/MachineLearning/comments/cujxhk/huawei_mindspore_features_video/,[deleted],1566594349,[deleted],0,1
1479,2019-8-24,2019,8,24,6,cuk4pg,[N] Huawei MindSpore features,https://www.reddit.com/r/MachineLearning/comments/cuk4pg/n_huawei_mindspore_features/,Digital_Akrasia,1566595291,,0,1
1480,2019-8-24,2019,8,24,7,cul5fx,"In regards to Sam Harris, this clears it up (post if you you don't agree)",https://www.reddit.com/r/MachineLearning/comments/cul5fx/in_regards_to_sam_harris_this_clears_it_up_post/,psi43,1566600056,,0,1
1481,2019-8-24,2019,8,24,8,culqul,"Looking for resources(books, online lectures...) about voice recognition,or anything to do with creating computer generated voices.",https://www.reddit.com/r/MachineLearning/comments/culqul/looking_for_resourcesbooks_online_lectures_about/,spad067,1566602943,[removed],0,1
1482,2019-8-24,2019,8,24,9,cumd11,"What is the absolute latest and greatest research/code available for taking Photographs and making them into clean, illustrated designs?",https://www.reddit.com/r/MachineLearning/comments/cumd11/what_is_the_absolute_latest_and_greatest/,The_McFly_Guy,1566606136,,0,1
1483,2019-8-24,2019,8,24,12,cuo4lg,ADMM for Efficient Deep Learning with Global Convergence,https://www.reddit.com/r/MachineLearning/comments/cuo4lg/admm_for_efficient_deep_learning_with_global/,xianggebenben,1566615988,[removed],0,1
1484,2019-8-24,2019,8,24,12,cuod19,ADMM for Efficient Deep Learning with Global Convergence,https://www.reddit.com/r/MachineLearning/comments/cuod19/admm_for_efficient_deep_learning_with_global/,xianggebenben,1566617386,,37,74
1485,2019-8-24,2019,8,24,16,cuqmtn,Research loss function list?,https://www.reddit.com/r/MachineLearning/comments/cuqmtn/research_loss_function_list/,Apsylem,1566633291,Is there a list of loss functions implementations from recent papers for Tensorflow Backend?,0,1
1486,2019-8-24,2019,8,24,17,cuqvez,"To Stop Killer Robots, a coalition of scientists and human rights leaders seeking to halt the development of autonomous robotic weapons.",https://www.reddit.com/r/MachineLearning/comments/cuqvez/to_stop_killer_robots_a_coalition_of_scientists/,iamart_intelligence,1566635207,,0,1
1487,2019-8-24,2019,8,24,17,cuqz2o,Data science skills Roles and Responsibilities,https://www.reddit.com/r/MachineLearning/comments/cuqz2o/data_science_skills_roles_and_responsibilities/,priyaleo,1566636094,,0,1
1488,2019-8-24,2019,8,24,18,cur8pr,"Questions has raised.. People treat white robots better than black robots, a recent study found.",https://www.reddit.com/r/MachineLearning/comments/cur8pr/questions_has_raised_people_treat_white_robots/,iamart_intelligence,1566638251,,0,1
1489,2019-8-24,2019,8,24,19,cury4a,[D] Interview with two senior data scientists at Microsoft about deep learning,https://www.reddit.com/r/MachineLearning/comments/cury4a/d_interview_with_two_senior_data_scientists_at/,timscarfe,1566643732," Join [**Mathew Salvaris**](https://www.linkedin.com/in/ACoAAAjsB18BSozniHxF4x6pZS01-AuMKuZqJTI/) and [**Ilia Karmanov**](https://www.linkedin.com/in/ACoAABMy83YBhwbx5N_5Kx71i5IWrYKgjJv_gqc/) an on a lively discussion about all things deep learning. I was blown away by these two incredibly talented data scientists. Nothing inspires me more than having a conversation with people who are literally 10 times smarter than me.   We discuss Mats work on building out patterns for distributed deep learning on Azure. Ilia discusses the latest and greatest on video action detection. We talk about computer vision, interpretability, robustness, ML engineering and the democratisation of deep learning. Finishing off we discuss where the deep learning space is going in the next 5 years! 

[https://youtu.be/Zw\_h1h0f\_qA](https://youtu.be/Zw_h1h0f_qA)",21,202
1490,2019-8-24,2019,8,24,19,cus17n,Gpt-2 online version,https://www.reddit.com/r/MachineLearning/comments/cus17n/gpt2_online_version/,susmit410,1566644398,,0,1
1491,2019-8-24,2019,8,24,20,cus6wn,Search and find your next job in Artificial Intelligence.,https://www.reddit.com/r/MachineLearning/comments/cus6wn/search_and_find_your_next_job_in_artificial/,Aijobs_com,1566645523,[removed],0,1
1492,2019-8-24,2019,8,24,21,cusjjo,What's the best way to a demo your MVP to your clients,https://www.reddit.com/r/MachineLearning/comments/cusjjo/whats_the_best_way_to_a_demo_your_mvp_to_your/,crashbundicoot,1566648009,[removed],0,1
1493,2019-8-24,2019,8,24,21,cusu5n,[P] A data platform to label using AI and share datasets,https://www.reddit.com/r/MachineLearning/comments/cusu5n/p_a_data_platform_to_label_using_ai_and_share/,jubashun,1566649953,"I'm working on a data labeling project to assist with the labeling and sharing of datasets. The goal is to minimize your time scraping the web for images and labeling data manually. Why let your side projects go to waste when you can share your datasets with others? I have included an AI powered annotation tool as well. 

Do check out [https://hungryai.com/home](https://hungryai.com/home) and let me know what you think.",7,9
1494,2019-8-24,2019,8,24,21,cuszd7,Training final model after oversampling,https://www.reddit.com/r/MachineLearning/comments/cuszd7/training_final_model_after_oversampling/,badmrsynth,1566650901,[removed],0,1
1495,2019-8-24,2019,8,24,22,cutix9,Project approach consultation - RF / DL / ?,https://www.reddit.com/r/MachineLearning/comments/cutix9/project_approach_consultation_rf_dl/,progmayo,1566654087,"Hi :) hopefully this isn't too long. I tried accentuating things in bold.

I have a specific project in mind I wish to do using AI, but Im unsure which method to use, and where to focus my time in terms of the different courses and lessons available (ML / DL / etc.). The details are less important, but I'd be happy to explain more if needed. For now Ill just describe what Im trying to achieve in general.

**I have sampled points on a general mult-dimensional tensor, and the goal is to find a mapping function of these points that achieves good results in terms of a criteria I set it (MSE), under a certain constraint - some sort of upper bound limitation on the mapping.** The details are less important, the bottom line is that I'm trying to find a mapping for these points g()=? that minimizes MSE under a constraint. So, for X sampled points, I need to find X points that correspond with their mapping. Everything is continuous of course - the sampled points and their mapping.

**Ive already built** a program that does this using scipy.optimize.minimize, which uses **an iterative method** for constrained nonlinear optimization - uses gradient decent to find a local minimum that achieves the criteria under the constraint.

**I want to try to use and test the performance of a neural network, or random forest,** or something along those lines using AI, but I got a bit lost in the videos I started watching, and I did not find a suitable example that was close to what Im trying to achieve. In short, I'm unsure which algorithm I should be using, and if AI is suited for this task at all, or is what I've already built the ""standard"" way of approaching this task, and that's that.

**Id like to ask which approach you think would suit my problem (RF? DL? etc.), and if you believe that these approaches could possibly outperform the straight-forward approach I already implemented using the iterative method**, which suffers from the unwanted local minimum phenomenon, and is relatively slow in general (especially in high dimensions).",1,1
1496,2019-8-24,2019,8,24,22,cutpq5,What do you think of the Machine Commons?,https://www.reddit.com/r/MachineLearning/comments/cutpq5/what_do_you_think_of_the_machine_commons/,symmys,1566655147,"This is the passion project of a few professionals operating around the machine learning space. Started journalistically but aiming to evolve to a helpful [free] community. 

Would love feedback, or any thoughts on how to improve.

https://www.machinecommons.org 


Content is all tagged, advice for data scientists as an example:

https://www.machinecommons.org/interviews/categories/advice-for-data-scientists",0,1
1497,2019-8-24,2019,8,24,23,cutvsy,Is deep learning a black box?,https://www.reddit.com/r/MachineLearning/comments/cutvsy/is_deep_learning_a_black_box/,ChessOrCheckers2,1566656044,"Is it truly black box, or is there possibilities to gain useful information by looking what is inside?

Is there some publications related to this?",0,1
1498,2019-8-24,2019,8,24,23,cuu01v,Mapping problem for a deep learning algorithm,https://www.reddit.com/r/MachineLearning/comments/cuu01v/mapping_problem_for_a_deep_learning_algorithm/,ChessOrCheckers2,1566656686,[removed],0,1
1499,2019-8-24,2019,8,24,23,cuu3l4,[P] Machine Learning @ VU,https://www.reddit.com/r/MachineLearning/comments/cuu3l4/p_machine_learning_vu/,asuagar,1566657225,,0,1
1500,2019-8-24,2019,8,24,23,cuu7ce,[R] YOLACT: Real-time Instance Segmentation,https://www.reddit.com/r/MachineLearning/comments/cuu7ce/r_yolact_realtime_instance_segmentation/,dbolya,1566657752,"[Paper](https://arxiv.org/abs/1904.02689) - [Code](https://github.com/dbolya/yolact)

**tl;dr**: Instance Segmentation slow, YOLACT make fast (29.8 COCO mAP, 33.5 Titan Xp fps).

Hi all, my paper was recently accepted ICCV 2019 Oral so I thought I'd post it here. (Note: fps numbers were rebenchmarked for ICCV and I haven't updated it elsewhere).

Today, object detection has several methods that do well (e.g., Faster R-CNN+++, RetinaNet), and several that do well enough but are also fast (e.g., YOLOv2-3, SSD). On the other hand, the same isn't true for instance segmentation. We have good methods (e.g., Mask R-CNN and its derivatives, Retina-Mask), but no fast methods that do well enough on a complex dataset like COCO.

YOLACT changes this. We obtain 29.8 mAP (30.1 after a stupid bug fix, but the paper's out now &gt;.&gt;) on COCO at 33.5 fps on a single Titan Xp, making YOLACT the best fast instance segmention method out at the moment. And it's simple: predict a set of *k* basis masks (prototypes) over the whole image and in parallel predict a set of *k* linear combination coefficients (mask coefficients) for each detection. Then to generate masks for a detection, just multiply the mask coefficients into the prototypes and add (which can be implemented as one matrix multiplication per image). This whole process takes ~5-6 ms to add a masks to any existing object detector.

I also came up with ""Fast NMS"", a close approximation to traditional per-class NMS that's 12ms faster.

Feel free to AMA.",12,74
1501,2019-8-24,2019,8,24,23,cuu7uh,[P] Machine Learning @ VU University Amsterdam,https://www.reddit.com/r/MachineLearning/comments/cuu7uh/p_machine_learning_vu_university_amsterdam/,asuagar,1566657828,,0,1
1502,2019-8-25,2019,8,25,0,cuufrz,Simple Perceptron Explanation,https://www.reddit.com/r/MachineLearning/comments/cuufrz/simple_perceptron_explanation/,nottingpill,1566658943,,0,1
1503,2019-8-25,2019,8,25,1,cuvdvw,Writing tutorial series with code examples on neural networks from simplest to the most advanced. Any feedback is appreciated! ,https://www.reddit.com/r/MachineLearning/comments/cuvdvw/writing_tutorial_series_with_code_examples_on/,guiviko,1566663470,,0,1
1504,2019-8-25,2019,8,25,1,cuvgzl,[P] Pretrained human presence detection models?,https://www.reddit.com/r/MachineLearning/comments/cuvgzl/p_pretrained_human_presence_detection_models/,Octosaurus,1566663869,I have a project to detect if myself or guests are present in a given room using a picamera. Does anyone know of any pretrained human presence detection models available?,3,0
1505,2019-8-25,2019,8,25,2,cuvxhg,TOP 3 USES OF ARTIFICIAL INTELLIGENCE [EXPLAINED],https://www.reddit.com/r/MachineLearning/comments/cuvxhg/top_3_uses_of_artificial_intelligence_explained/,updownvizzii,1566666003,,0,1
1506,2019-8-25,2019,8,25,2,cuw0a2,[D] Topological Data Analysis on time series,https://www.reddit.com/r/MachineLearning/comments/cuw0a2/d_topological_data_analysis_on_time_series/,thats_DR_chalupa_2u,1566666340,"I've recently encountered some of the point cloud to persistence diagram representations in time series analysis, and they seem very interesting, if not familiar to those of us who ended up here from a computational/algorithmic background.

I've so far primarily encountered these topics written on in, naturally, topology and analysis settings, but was wondering if there's yet been heavy adaptation in the ML sphere? With an influx of interest in geometric/topological domains these days, I would think this would be a popular topic.

So I suppose that my objective is to learn from more informed people on some of the relevant existing work in ML, as well as what we know of the strengths and criticisms regarding these representations of time series.",14,2
1507,2019-8-25,2019,8,25,2,cuwc78,7 Most Common machine learning algorithms you need to know in 2019,https://www.reddit.com/r/MachineLearning/comments/cuwc78/7_most_common_machine_learning_algorithms_you/,tk_tamani,1566667801,,0,1
1508,2019-8-25,2019,8,25,2,cuwjnk,DVC vs pachyderm,https://www.reddit.com/r/MachineLearning/comments/cuwjnk/dvc_vs_pachyderm/,EBorza,1566668717,[removed],0,1
1509,2019-8-25,2019,8,25,3,cuwsrv,"What are some industrial deployments of Transformer models (e.g. Transformer-XL, GPT-2, BERT)?",https://www.reddit.com/r/MachineLearning/comments/cuwsrv/what_are_some_industrial_deployments_of/,CA_archer,1566669838,[removed],0,1
1510,2019-8-25,2019,8,25,3,cuwzs4,[Q] Tests for comparing predictive accuracy of regression models,https://www.reddit.com/r/MachineLearning/comments/cuwzs4/q_tests_for_comparing_predictive_accuracy_of/,Megaslaking,1566670705,[removed],0,1
1511,2019-8-25,2019,8,25,3,cux5v4,[D] Tests for comparing predictive accuracy of regression models,https://www.reddit.com/r/MachineLearning/comments/cux5v4/d_tests_for_comparing_predictive_accuracy_of/,Megaslaking,1566671468,"I'm trying to compare the predictive accuracy of few regression models. For simplicity, lets say that I have a polynomial of degree 6 and a GAM model with many knots. One simple approach would be to compare the RMSE and/or MAE. In this case, the GAM model has a lower RMSE and MAE than those of the polynomial model, but the difference is small. Now based on the RMSE and MAE values, I should choose the GAM model, but the small difference is making me question whether it makes sense to take the GAM model over the simpler polynomial model. 

Searching around, I found that one can use the Diebold-Mariano (or the similar HLN) test to compare the predictive accuracy of two forecasts in time series. The DM/HLN tests determine whether there is any significant difference between the forecasts. However, I think it would not be appropriate to use the DM/HLN-test in my case, since the tests compute autocovariance at lags in order to derive the test-statistic and that would make little sense in the context of non-time series forecasts. 

Are there any similar tests that can be used for non-time series forecasts?",9,0
1512,2019-8-25,2019,8,25,4,cuxsty,Do you study Machine Learning ? here are the best machine learning books ! with download link,https://www.reddit.com/r/MachineLearning/comments/cuxsty/do_you_study_machine_learning_here_are_the_best/,sajad-52,1566674427,,0,0
1513,2019-8-25,2019,8,25,4,cuy2fh,[P] These Instagram Portrait Models Do Not Exist,https://www.reddit.com/r/MachineLearning/comments/cuy2fh/p_these_instagram_portrait_models_do_not_exist/,samsamsamrox1212,1566675728,"I used transfer learning to train Stylegan using Gwern's [This Waifu Does not exist](https://www.thiswaifudoesnotexist.net/) 512px pre-trained model. 

&amp;#x200B;

the dataset was originally around 3k images augmented to 13k. Collecting and processing data was the hardest part for sure.

&amp;#x200B;

Here are some [results](https://www.instagram.com/p/B1d4poSHAPW/?utm_source=ig_web_copy_link):",0,1
1514,2019-8-25,2019,8,25,4,cuy7yi,Exploring the Smelliverse with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cuy7yi/exploring_the_smelliverse_with_machine_learning/,craigspencersmith,1566676487,,0,1
1515,2019-8-25,2019,8,25,4,cuy9bv,Never Worry about AI Again  There Will Be a Job for You in the Future,https://www.reddit.com/r/MachineLearning/comments/cuy9bv/never_worry_about_ai_again_there_will_be_a_job/,Aijobs_com,1566676668,[https://medium.com/@aijobs.com/never-worry-about-ai-again-there-will-be-a-job-for-you-in-the-future-b1f3bebd2546](https://medium.com/@aijobs.com/never-worry-about-ai-again-there-will-be-a-job-for-you-in-the-future-b1f3bebd2546),0,1
1516,2019-8-25,2019,8,25,6,cuz215,[R] FacebookAI releases Adaptive attention span and All-attention layer to reduce decrease computation time / memory footprint,https://www.reddit.com/r/MachineLearning/comments/cuz215/r_facebookai_releases_adaptive_attention_span_and/,BatmantoshReturns,1566680572,"https://video.twimg.com/tweet_video/ECqxU2AU0AAkBwc.mp4

&gt;To enable wider use of this powerful deep learning architecture, we propose two new methods. The first, adaptive attention span is a way to make Transformer networks more efficient for longer sentences. With this method, we were able to increase the attention span of a Transformer to over 8,000 tokens without significantly increasing computation time or memory footprint. The second, all-attention layer is a way to simplify the model architecture of Transformer networks. Even with a much simpler architecture, our all-attention network matched the state-of-the-art performance of Transformer networks. 

https://ai.facebook.com/blog/making-transformer-networks-simpler-and-more-efficient/",26,191
1517,2019-8-25,2019,8,25,9,cv1ri9,AI And Healthcare: Is The Bloom Finally Off The Rose?,https://www.reddit.com/r/MachineLearning/comments/cv1ri9/ai_and_healthcare_is_the_bloom_finally_off_the/,bioquarkceo,1566694775,,0,1
1518,2019-8-25,2019,8,25,10,cv205l,C++ Implementation of the Side Window Filtering(CVPR 2019),https://www.reddit.com/r/MachineLearning/comments/cv205l/c_implementation_of_the_side_window_filteringcvpr/,Ldpe2G,1566696149,,0,1
1519,2019-8-25,2019,8,25,14,cv4wl6,"Interview with the leader of mlcourse.ai: Dr. Yury Kashnitsky | mlcourse.ai is an open course with the right balance of theory, practise and kaggle",https://www.reddit.com/r/MachineLearning/comments/cv4wl6/interview_with_the_leader_of_mlcourseai_dr_yury/,init__27,1566712785,"mlcourse.ai is a unique course with the right balance of theory+practise and Kaggle. The final iteration of the course starts on 2nd Sept, 2019. Be sure to sign up!

Following is a link to the interview with the leader of the course. Available both as a podcast, video.

In the interview, we talk all about the efforts and decisions behind the course structure. Yury also shares tips for both future students and alumni of the course.

Audio: https://anchor.fm/chaitimedatascience/episodes/Interview-with-the-Leader-of-mlcourse-ai--Dr--Yury-Kashnitsky--Chai-Time-Data-Science-e52r5u/a-alh171

Video: https://www.youtube.com/watch?v=ZmKGQdCyOFY&amp;list=PLyMom0n-MBrrGL8w-nD9kc2q5dOwN7Hb1&amp;index=12",0,1
1520,2019-8-25,2019,8,25,15,cv4xy2,GOOGLES MACHINE LEARNING TECHNOLOGY CAN ACE UP THE TRAVEL INDUSTRY,https://www.reddit.com/r/MachineLearning/comments/cv4xy2/googles_machine_learning_technology_can_ace_up/,analyticsinsight,1566713049,,0,1
1521,2019-8-25,2019,8,25,16,cv5f2f,could we use transformer to replace lstm in reinforcement learning?,https://www.reddit.com/r/MachineLearning/comments/cv5f2f/could_we_use_transformer_to_replace_lstm_in/,resyes,1566716691,[removed],0,1
1522,2019-8-25,2019,8,25,17,cv66mb,Self driving car project,https://www.reddit.com/r/MachineLearning/comments/cv66mb/self_driving_car_project/,clean_pegasus,1566722852,,0,1
1523,2019-8-25,2019,8,25,18,cv6owi,EMNLP 2019 Accepted papers (Long/Short/Demo),https://www.reddit.com/r/MachineLearning/comments/cv6owi/emnlp_2019_accepted_papers_longshortdemo/,skrish13,1566727035,,0,1
1524,2019-8-25,2019,8,25,18,cv6p75,[N] EMNLP 2019 Accepted Papers List (Long/Short/Demo),https://www.reddit.com/r/MachineLearning/comments/cv6p75/n_emnlp_2019_accepted_papers_list_longshortdemo/,skrish13,1566727098,,0,1
1525,2019-8-25,2019,8,25,19,cv6xwb,Machine Learning Benefits From Big Data,https://www.reddit.com/r/MachineLearning/comments/cv6xwb/machine_learning_benefits_from_big_data/,Magniminda,1566729026,[removed],0,1
1526,2019-8-25,2019,8,25,19,cv6yn2,[P] Self driving car project,https://www.reddit.com/r/MachineLearning/comments/cv6yn2/p_self_driving_car_project/,clean_pegasus,1566729195,"I have worked on a behavioral cloning project for self-driving car using the Udacity simulator.

For further information, check out the link below.

[https://github.com/CleanPegasus/Behavioural-Cloning](https://github.com/CleanPegasus/Behavioural-Cloning)",0,1
1527,2019-8-25,2019,8,25,22,cv8b1t,[P] Self driving car project,https://www.reddit.com/r/MachineLearning/comments/cv8b1t/p_self_driving_car_project/,clean_pegasus,1566738625,,0,1
1528,2019-8-25,2019,8,25,23,cv9aob,Study buddy,https://www.reddit.com/r/MachineLearning/comments/cv9aob/study_buddy/,vncvishalchauhan,1566744055,[removed],0,1
1529,2019-8-25,2019,8,25,23,cv9h24,I'm interested in a sub-field of AI but don't know what to call it.,https://www.reddit.com/r/MachineLearning/comments/cv9h24/im_interested_in_a_subfield_of_ai_but_dont_know/,tmf1988,1566744968,[removed],0,1
1530,2019-8-26,2019,8,26,0,cv9t5h,Whats your opinion about Siraj Raval.,https://www.reddit.com/r/MachineLearning/comments/cv9t5h/whats_your_opinion_about_siraj_raval/,staircase7,1566746632,[removed],0,1
1531,2019-8-26,2019,8,26,0,cv9uo0,[P] Self driving car project,https://www.reddit.com/r/MachineLearning/comments/cv9uo0/p_self_driving_car_project/,clean_pegasus,1566746829,,1,1
1532,2019-8-26,2019,8,26,0,cv9ydn,Whats you opinion abou Siraj Raval.,https://www.reddit.com/r/MachineLearning/comments/cv9ydn/whats_you_opinion_abou_siraj_raval/,staircase7,1566747335,"I sorted the posts in this subreddit by controversial and by all time and the second one was about one of Siraj's Raval oldest video:&lt;Build a TensorFlow Image Classifier in 5 Min&gt;. I remember that then he was a controversial figure in the machine learning community and the main problem with his content was that it didn't have a specific target audience. I have seen lot of his videos, but to be honest the only ones i like and find usefull are not about machine learning but are the more personal ones,such as book reviews or podcasts. When he uploads a machine learning video, i feel like that he have said the same thing in previous videos, and the new video is just a recap of his previous videos, with a couple new images and memes. So, i what to ask,who do you think his videos are for and who the target audience is? Also, has any of his videos helped you learn or clarify something, or you watch it just for fun?",0,1
1533,2019-8-26,2019,8,26,0,cva55a,solve a challenge and you learn machine learning!!!! by Debojyoti Chakraborty,https://www.reddit.com/r/MachineLearning/comments/cva55a/solve_a_challenge_and_you_learn_machine_learning/,debomastet335,1566748255,,0,1
1534,2019-8-26,2019,8,26,1,cvadrm,Accelerated Computing and Deep Learning,https://www.reddit.com/r/MachineLearning/comments/cvadrm/accelerated_computing_and_deep_learning/,andrea_manero,1566749385,[removed],0,1
1535,2019-8-26,2019,8,26,1,cvagvl,"How would I find papers on vectorization (which way a person is facing, with their face not their body)?",https://www.reddit.com/r/MachineLearning/comments/cvagvl/how_would_i_find_papers_on_vectorization_which/,DarkSkullMango,1566749804,[removed],0,1
1536,2019-8-26,2019,8,26,3,cvc75f,A question on constructing training data,https://www.reddit.com/r/MachineLearning/comments/cvc75f/a_question_on_constructing_training_data/,VWXYZadam,1566757757,"Hi /r/machinelearning 

Quick question on construction of training data. Im looking for a technical term or literature on a method I'm considering applying, that I am sure I am not the first to try.

Fundameally, I have a decent rule-engine and a lot of data (in this case text). My plan is to generate labels from the rule- engine which I know deliver OK results, and then train a somewhat complex model on top of it (like a TF-IDF with logistic regression, nothing fancy). 

I will then start going through the data and manually edit labels where I see obvious errors, and hope to see iterative improcements in the model. 

I might go through and make a fully manually labeled test set, just as a baseline, but it will be too costly to produce a full training set.

Questions are:
1) Is there a term for this ""model boosting""?
2) Has anyone tried it or similar?
3) Is there any obvious pitfalls I'm missing?",0,1
1537,2019-8-26,2019,8,26,3,cvcbu6,[D] Why is PyTorch as fast as (and sometimes faster than) TensorFlow?,https://www.reddit.com/r/MachineLearning/comments/cvcbu6/d_why_is_pytorch_as_fast_as_and_sometimes_faster/,student_at_uw,1566758327,"Since both libraries use cuDNN under the hood, I would expect the individual operations to be similar in speed. However, TensorFlow (in graph mode) compiles a graph so when you run the actual train loop, you have no python overhead outside of the [session.run](https://session.run) call. In PyTorch, you are in Python a lot due to the dynamic graph, so I would expect that to add some overhead. But in many benchmarks I see online, PyTorch has no problems keeping up with TensorFlow on GPUs. 

A specific example is the Adam implementations in both libraries:

 [https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py](https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) 

 [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/adam.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/adam.py) 

PyTorch has all the ops as you would expect. For TensorFlow in the {\_resource}\_apply\_dense case (which is the common case, AFAIK), TensorFlow has a dedicated C++ implementation.  So here, TensorFlow does not spend extra time in Python AND it has an optimized implementation in C++. In this case, why isn't the TensorFlow version straight up faster?

&amp;#x200B;

I've heard that PyTorch is better optimized on the cuDNN level. Can anyone provide more details about this? What's preventing TensorFlow from doing the same thing?  The only optimization I know of is that PyTorch uses the NCHW format (which is better optimized for cuDNN) whereas TensorFlow by default uses  NHWC.

&amp;#x200B;

I saw these two discussions but did not see a satisfactory answer: 

 [https://www.reddit.com/r/MachineLearning/comments/7ujc6y/d\_can\_someone\_give\_a\_technical\_explanation\_as\_to/](https://www.reddit.com/r/MachineLearning/comments/7ujc6y/d_can_someone_give_a_technical_explanation_as_to/) 

 [https://www.reddit.com/r/MachineLearning/comments/8iguaw/d\_why\_is\_tensorflow\_so\_slow/](https://www.reddit.com/r/MachineLearning/comments/8iguaw/d_why_is_tensorflow_so_slow/)",44,244
1538,2019-8-26,2019,8,26,3,cvcgrd,android ChatBot,https://www.reddit.com/r/MachineLearning/comments/cvcgrd/android_chatbot/,_ouss_,1566758935,[removed],0,1
1539,2019-8-26,2019,8,26,4,cvcrzw,Here are the best machine learning courses of 2019,https://www.reddit.com/r/MachineLearning/comments/cvcrzw/here_are_the_best_machine_learning_courses_of_2019/,BlogUpost,1566760327,If you are interested in machine learning and want to make your career in machine learning then you should read this article about the best machine learning courses of 2019.,0,1
1540,2019-8-26,2019,8,26,4,cvd1bs,Here's My Top 10 ML Methods Feeding AI-based Use-Cases. Which Ones Would You Add?,https://www.reddit.com/r/MachineLearning/comments/cvd1bs/heres_my_top_10_ml_methods_feeding_aibased/,castanan2,1566761513,,0,1
1541,2019-8-26,2019,8,26,5,cvde5a,[D] Machine Learning - WAYR (What Are You Reading) - Week 69,https://www.reddit.com/r/MachineLearning/comments/cvde5a/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,1566763207,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|
|----|-----|-----|-----|-----|-----|-----|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)||

Most upvoted papers two weeks ago:

/u/Cantrill1758: [Anomaly Detection Using Autoencoders with Nonlinear Dimensionality Reduction](https://dl.acm.org/citation.cfm?id=2689747)

/u/lysecret: [https://arxiv.org/abs/1904.01681](https://arxiv.org/abs/1904.01681)

Besides that, there are no rules, have fun.",42,7
1542,2019-8-26,2019,8,26,5,cve0ku,Python or R for Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/cve0ku/python_or_r_for_machine_learning/,rylanpfowers,1566766129,[removed],0,1
1543,2019-8-26,2019,8,26,6,cvenvq,"[R] [1906.03671] Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds",https://www.reddit.com/r/MachineLearning/comments/cvenvq/r_190603671_deep_batch_active_learning_by_diverse/,wordbag,1566769168,,8,8
1544,2019-8-26,2019,8,26,7,cvfcyk,[N] [CFP] 3rd edition of Emergent Communication workshop at NeurIPS'19 (EmeCom),https://www.reddit.com/r/MachineLearning/comments/cvfcyk/n_cfp_3rd_edition_of_emergent_communication/,dbg99,1566772555,"Hi everyone,

We are pleased to announce the Call for Papers for the 3rd Workshop on Emergent Communication to be held at NeurIPS 2019 on Dec 13 or 14 at Vancouver, Canada.

We invite submissions from researchers both inside and outside the machine learning community in the following areas:

\- multi-agent communication  
\- grounding emergent protocols to natural language  
\- compositionality in emergent/natural languages  
\- linguistic generalization  
\- learning cognitive skills through language  
\- language evolution  
\- deep multi-agent learning  
\- any other area related to the subject of the workshop

Submission Format:  
The submitted work should be an extended abstract not exceeding 4 pages (excluding references and supplementary material). The submission should be in pdf format and should follow the style guidelines for NeurIPS 2019. The review process is double-blind. The submissions should not have been previously published in any ML conference nor have appeared in the NeurIPS main conference. We do however appreciate submitting published work from other non-ML conferences. Work currently under submission to another conference is also welcome. We discourage submitting the same work to other NeurIPS workshops. There will be no formal publication of workshop proceedings. However, the accepted papers will be made available online on the workshop website as non-archival reports to allow submissions to future conferences/journals.

Workshop Website: [https://sites.google.com/view/emecom2019/](https://sites.google.com/view/emecom2019/)  
Submissions Link: [https://cmt3.research.microsoft.com/emecom2019/](https://cmt3.research.microsoft.com/emecom2019/)

Important Dates:  
Submissions Open: Aug 15  
Submissions Deadline: Sep 15  
Accepted papers notification: Sep 30  
Camera ready Deadline: Nov 15  
Upload poster/video (optional): Dec 7  
Workshop Date: Dec 13 or 14

(All deadlines expire at 11:59pm AoE on the respective dates)

For any queries, reach out to us at [emecomworkshop@gmail.com](mailto:emecomworkshop@gmail.com). We look forward to receiving your submissions!

On behalf of all organizers, Cheers!

Abhinav Gupta (Mila)  
Michael Noukhovitch (Mila)  
Cinjon Resnick (NYU)  
Natasha Jaques (MIT)  
Angelos Filos (Oxford)  
Marie Ossenkopf (Uni Kassel)  
Jakob Foerster (FAIR)  
Angeliki Lazaridou (DeepMind)  
Ryan Lowe (Mila)  
Douwe Kiela (FAIR)  
Kyunghyun Cho (NYU/FAIR)",0,5
1545,2019-8-26,2019,8,26,7,cvff4i,[D] Classification of irregular time series,https://www.reddit.com/r/MachineLearning/comments/cvff4i/d_classification_of_irregular_time_series/,CuzImLonelyWannaDie,1566772848,"I have been working on classification of variable stars using light curves. However, the curves have different number of data points, from 40 data points up to 100. I have been training my network by randomly removing points until having about 50 points per star, and also augmented the data with different combination of eliminated points, but it seems to introduce a lot of loss. 

&amp;#x200B;

I am interested in different approaches or ideas on how to handle the irregularity.",11,2
1546,2019-8-26,2019,8,26,7,cvfm8v,Training an AI to Imitate my Voice,https://www.reddit.com/r/MachineLearning/comments/cvfm8v/training_an_ai_to_imitate_my_voice/,AzimuthBlast,1566773821,,0,1
1547,2019-8-26,2019,8,26,8,cvg3at,Identify a typer by the timing of their keystrokes using a 1D convolutional model,https://www.reddit.com/r/MachineLearning/comments/cvg3at/identify_a_typer_by_the_timing_of_their/,tyrilu,1566776255,,0,1
1548,2019-8-26,2019,8,26,8,cvg63q,Recipes for common NLP scenarios from Microsoft,https://www.reddit.com/r/MachineLearning/comments/cvg63q/recipes_for_common_nlp_scenarios_from_microsoft/,sharatsc,1566776646,[removed],0,1
1549,2019-8-26,2019,8,26,9,cvgjl4,Is ML powered costume voice synthesis possible? Example train a Musk voice based of content library.,https://www.reddit.com/r/MachineLearning/comments/cvgjl4/is_ml_powered_costume_voice_synthesis_possible/,bitman_moon,1566778618,[removed],0,1
1550,2019-8-26,2019,8,26,10,cvh1jg,The Ethical Debate on AI Applications (Interview with Dr. Iain Brown),https://www.reddit.com/r/MachineLearning/comments/cvh1jg/the_ethical_debate_on_ai_applications_interview/,LimarcAmbalina,1566781238,"[The Ethical Debate on AI Applications (interview with Data Scientist Dr. Iain Brown)](https://lionbridge.ai/articles/ethical-debate-ai-applications-interview-data-scientist-iain-brown/)

This interview focusses on the controversial topics in the field of AI. For example, who is held accountable when a system fails (autonomous vehicle crashes and leads to damage or even death), bias is machine learning systems, and more.",0,1
1551,2019-8-26,2019,8,26,11,cvib9r,I made a song using lyrics from Kanye West! It's off beat but a neat concept nonetheless.,https://www.reddit.com/r/MachineLearning/comments/cvib9r/i_made_a_song_using_lyrics_from_kanye_west_its/,askingquestions1337,1566788118,,0,1
1552,2019-8-26,2019,8,26,12,cviolq,Will The Machine Learning Approach Work?,https://www.reddit.com/r/MachineLearning/comments/cviolq/will_the_machine_learning_approach_work/,HumanAGI,1566790346,,3,1
1553,2019-8-26,2019,8,26,12,cviq91,What do you think of this over-specialization in Machine Learning / Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/cviq91/what_do_you_think_of_this_overspecialization_in/,CzoKc,1566790667,[removed],0,1
1554,2019-8-26,2019,8,26,15,cvk51x,[Project] Saving yall money! I made a social iOS app that uses ML to Price Compare Food Deliveries- link in comments,https://www.reddit.com/r/MachineLearning/comments/cvk51x/project_saving_yall_money_i_made_a_social_ios_app/,the1foodie,1566799857,,1,1
1555,2019-8-26,2019,8,26,15,cvk8uo,"Interview with the creator of MuseNet: Christine Payne all about MuseNet, OpenAI and Deep Learning Research",https://www.reddit.com/r/MachineLearning/comments/cvk8uo/interview_with_the_creator_of_musenet_christine/,init__27,1566800598,[removed],0,1
1556,2019-8-26,2019,8,26,15,cvkdqh,[P] PyTorch 1.2 Quickstart with Google Colab,https://www.reddit.com/r/MachineLearning/comments/cvkdqh/p_pytorch_12_quickstart_with_google_colab/,omarsar,1566801610,,0,1
1557,2019-8-26,2019,8,26,15,cvkeog,Anomaly Detection in a set of Logs using Machine learning,https://www.reddit.com/r/MachineLearning/comments/cvkeog/anomaly_detection_in_a_set_of_logs_using_machine/,kartikeySom,1566801802,[removed],0,1
1558,2019-8-26,2019,8,26,15,cvkgtm,Enhancing Customer Relationships with AI and CRM,https://www.reddit.com/r/MachineLearning/comments/cvkgtm/enhancing_customer_relationships_with_ai_and_crm/,erp_oodles,1566802252,,0,1
1559,2019-8-26,2019,8,26,16,cvkrur,[D] How can I reduce the difference between real and predicted stock price?,https://www.reddit.com/r/MachineLearning/comments/cvkrur/d_how_can_i_reduce_the_difference_between_real/,GoBacksIn,1566804502,"I'm using LSTM in deep learning to predict indexes.

&amp;#x200B;

I used MinMaxSacler(0\~1) and The lowest MSE obtained through the model is 0.0000033043723.

&amp;#x200B;

However, there is a big difference between the predicted price and the real price.

&amp;#x200B;

I wonder if it is possible to bridge this gap.

&amp;#x200B;

If  I can, I wonder which method I should use.

&amp;#x200B;

Your valuable opinions and thoughts will be very much appreciated.",16,0
1560,2019-8-26,2019,8,26,17,cvl1k6,Machine Learning Course,https://www.reddit.com/r/MachineLearning/comments/cvl1k6/machine_learning_course/,dwivediabhinav,1566806568,[removed],0,1
1561,2019-8-26,2019,8,26,17,cvl1qk,OBJECT DETECTION,https://www.reddit.com/r/MachineLearning/comments/cvl1qk/object_detection/,joeljuzreddit,1566806606,[removed],0,1
1562,2019-8-26,2019,8,26,17,cvl2am,Convert prediction pipeline to a model file,https://www.reddit.com/r/MachineLearning/comments/cvl2am/convert_prediction_pipeline_to_a_model_file/,thomas_ver,1566806727,"I am having some issues trying to convert my entire prediction pipeline into a single model file, and could use some advice.

My prediction pipeline receives an input image which gets sent through the trained model. Afterwards I apply some functions to get more accurate results. I can easily convert my trained model to a h5 model and just use this to get my predictions. However I would like to add my functions to this file.

Is this in any case possible or am I doomed?",0,1
1563,2019-8-26,2019,8,26,17,cvl2ed,[R] AdvHat: Real-world adversarial attack on ArcFace Face ID system,https://www.reddit.com/r/MachineLearning/comments/cvl2ed/r_advhat_realworld_adversarial_attack_on_arcface/,AleksanderPet,1566806747,"Hi! We have done some interesting research on breaking the current best public Face ID system - ArcFace - using the adversarial attack technique. It's quite ordinary but what we succeed is to do it in the real world (i.e. made it not in digital domain only): someone can print the color sticker and stick it to a hat, and after that the similarity with the ground truth drops significantly. Even some sort of attack transferability to other top Face ID models from insightface exists.

Paper:  [https://arxiv.org/abs/1908.08705](https://arxiv.org/abs/1908.08705) 

Code:  [https://github.com/papermsucode/advhat](https://github.com/papermsucode/advhat) 

Video demonstration:  [https://www.youtube.com/watch?v=a4iNg0wWBsQ](https://www.youtube.com/watch?v=a4iNg0wWBsQ&amp;feature=youtu.be)

Any comments are welcome!",20,146
1564,2019-8-26,2019,8,26,17,cvlbei,ML/NLP jokes,https://www.reddit.com/r/MachineLearning/comments/cvlbei/mlnlp_jokes/,theophrastzunz,1566808751,[removed],0,1
1565,2019-8-26,2019,8,26,17,cvlfj5,"CONCRETE SHOW IMPACT IN BANGKOK, THAI TIME: 5TH TO 7TH, SEP BOOTH NUMBER: J11 WELCOME YOUR COMING IN THE NEAR SEPTEMBER",https://www.reddit.com/r/MachineLearning/comments/cvlfj5/concrete_show_impact_in_bangkok_thai_time_5th_to/,ada2017,1566809623,,0,1
1566,2019-8-26,2019,8,26,18,cvllnd,AI Trained on Old Scientific Papers Makes Discoveries Humans Missed,https://www.reddit.com/r/MachineLearning/comments/cvllnd/ai_trained_on_old_scientific_papers_makes/,debate2,1566810955,,0,1
1567,2019-8-26,2019,8,26,18,cvlte1,Handmade Straw bag,https://www.reddit.com/r/MachineLearning/comments/cvlte1/handmade_straw_bag/,sohn0987,1566812549,,0,1
1568,2019-8-26,2019,8,26,18,cvlu9f,Future Proof Your Career with the Help of Machine Learning Online Courses!,https://www.reddit.com/r/MachineLearning/comments/cvlu9f/future_proof_your_career_with_the_help_of_machine/,multisoftmva0,1566812753,[removed],0,1
1569,2019-8-26,2019,8,26,18,cvlwbp,"Near infrared image data, sources?",https://www.reddit.com/r/MachineLearning/comments/cvlwbp/near_infrared_image_data_sources/,ReasonableRoll7,1566813166,[removed],0,1
1570,2019-8-26,2019,8,26,19,cvm0vm,Building Engaging Conversational Interfaces with DialogFlow,https://www.reddit.com/r/MachineLearning/comments/cvm0vm/building_engaging_conversational_interfaces_with/,MachineLearning001,1566814027,,0,1
1571,2019-8-26,2019,8,26,19,cvm3hv,Knowledge base for a chat bot,https://www.reddit.com/r/MachineLearning/comments/cvm3hv/knowledge_base_for_a_chat_bot/,BeggarInSpain,1566814524,[removed],0,1
1572,2019-8-26,2019,8,26,19,cvm4q7,Patch Based Image in-Painting Multi-Spectral SAR Images,https://www.reddit.com/r/MachineLearning/comments/cvm4q7/patch_based_image_inpainting_multispectral_sar/,flyhighwithai,1566814727,,0,1
1573,2019-8-26,2019,8,26,19,cvm5j1,Enhancing Customer Experience with A Customer 360 View Using StreamAnalytix,https://www.reddit.com/r/MachineLearning/comments/cvm5j1/enhancing_customer_experience_with_a_customer_360/,Emma-Thompson,1566814877,,0,1
1574,2019-8-26,2019,8,26,19,cvm6nu,"[P] Introducing Deepkit - the first collaborative desktop app for deep learning experiments. Experiment tracking, model debugging, infrastructure management.",https://www.reddit.com/r/MachineLearning/comments/cvm6nu/p_introducing_deepkit_the_first_collaborative/,marcjschmidt,1566815090,"[https://deepkit.ai](https://deepkit.ai/)

Hi guys, I'm the founder of Deepkit. An app that helps you visualize, track, and run ML/DL experiments, directly on your workstation or on your own servers, in your LAN or in the cloud. Deepkit will be free for individual users.

We're are looking for alpha users that want to help us building a better, cheaper and more efficient way of doing ML/DL experiments. If you're interested, please register at the website directly or use [this link](https://deepkit.typeform.com/to/a356Lh). We currently only support MacOS, but Windows &amp; Linux will follow. If you got any questions, I'm happy to answer in the comments.",50,87
1575,2019-8-26,2019,8,26,19,cvmd9i,Accelerate Apache Spark Development and Operations,https://www.reddit.com/r/MachineLearning/comments/cvmd9i/accelerate_apache_spark_development_and_operations/,Emma-Thompson,1566816345,,0,1
1576,2019-8-26,2019,8,26,19,cvmh7o,Ebook at $1 | Learn Artificial Intelligence &amp; Machine Learning from scratch,https://www.reddit.com/r/MachineLearning/comments/cvmh7o/ebook_at_1_learn_artificial_intelligence_machine/,Slight_Role,1566817092,,0,1
1577,2019-8-26,2019,8,26,20,cvmp35,The impact of machine learning and analytics on the banking sector - Visionet,https://www.reddit.com/r/MachineLearning/comments/cvmp35/the_impact_of_machine_learning_and_analytics_on/,iammarksmith,1566818429,,0,1
1578,2019-8-26,2019,8,26,20,cvmtwz,GitHub - MateLabs/AutoOut: Automated Outlier Detection and Treatment Tool,https://www.reddit.com/r/MachineLearning/comments/cvmtwz/github_matelabsautoout_automated_outlier/,kailashahirwar12,1566819234,,0,1
1579,2019-8-26,2019,8,26,20,cvmuao,Boosting customer experience with real-time streaming analytics in travel industry,https://www.reddit.com/r/MachineLearning/comments/cvmuao/boosting_customer_experience_with_realtime/,Emma-Thompson,1566819303,,0,1
1580,2019-8-26,2019,8,26,20,cvn2mi,"Grover's Algorithm is like a backward neural network, determining the input when you know the output.",https://www.reddit.com/r/MachineLearning/comments/cvn2mi/grovers_algorithm_is_like_a_backward_neural/,Agent_ANAKIN,1566820631,,0,1
1581,2019-8-26,2019,8,26,22,cvnuqv,Machine Learning with C++ - Polynomial regression with Eigen,https://www.reddit.com/r/MachineLearning/comments/cvnuqv/machine_learning_with_c_polynomial_regression/,andrea_manero,1566824889,[removed],0,1
1582,2019-8-26,2019,8,26,22,cvoan6,Predicting probability distribution with partial information about the distribution,https://www.reddit.com/r/MachineLearning/comments/cvoan6/predicting_probability_distribution_with_partial/,Busy_Stranger,1566827137,[removed],0,1
1583,2019-8-26,2019,8,26,22,cvodw0,Rich model/Poor model and Logarithmic Loss - an exploratory analysis of comparing model performance.,https://www.reddit.com/r/MachineLearning/comments/cvodw0/rich_modelpoor_model_and_logarithmic_loss_an/,datadatadata84,1566827580,[removed],0,1
1584,2019-8-26,2019,8,26,23,cvohv6,What should I know to get internship in machine learning field???,https://www.reddit.com/r/MachineLearning/comments/cvohv6/what_should_i_know_to_get_internship_in_machine/,sbr666,1566828140,[removed],0,1
1585,2019-8-26,2019,8,26,23,cvolny,Read a paper: Automatic Patch Generation by Learning Correct Code,https://www.reddit.com/r/MachineLearning/comments/cvolny/read_a_paper_automatic_patch_generation_by/,ggvh,1566828642,,0,1
1586,2019-8-26,2019,8,26,23,cvos1m,[D] Leveraging Learning in Robotics: RSS 2019 Highlights,https://www.reddit.com/r/MachineLearning/comments/cvos1m/d_leveraging_learning_in_robotics_rss_2019/,aseembits93,1566829535,"[https://thegradient.pub/leveraging-learning-in-robotics-rss-2019-highlights/](https://thegradient.pub/leveraging-learning-in-robotics-rss-2019-highlights/)

I recently wrote a blog post, summarizing interesting works presented at the Robotics Science and Systems Conference. Would love your feedback!",6,9
1587,2019-8-26,2019,8,26,23,cvp4tp,"[R] A short tutorial on distributed gradient boosted tree training, and block-distributed training",https://www.reddit.com/r/MachineLearning/comments/cvp4tp/r_a_short_tutorial_on_distributed_gradient/,thvasilo,1566831274,,0,1
1588,2019-8-26,2019,8,26,23,cvp563,[D] ML approaches to Fuzzy Matching for MDM?,https://www.reddit.com/r/MachineLearning/comments/cvp563/d_ml_approaches_to_fuzzy_matching_for_mdm/,Fender6969,1566831321,"I am currently working on a project with an interesting task that I am struggling with an approach. I am tasked to implement ML algorithms to replace fuzzy matching for MDM purposes. The fields will include PII such as SSN, First Name, Last Name, Address etc. I am quite new to Fuzzy Matching would love to hear some opinions on some other approaches to this problem! The development language for Python (I did work through FuzzyWuzzy and I believe the current implementation uses something similar to this and the similarity score)",0,1
1589,2019-8-27,2019,8,27,0,cvpe7i,[D] Rich model/Poor model: Logarithmic Loss and comparing model performance - an exploratory analysis,https://www.reddit.com/r/MachineLearning/comments/cvpe7i/d_rich_modelpoor_model_logarithmic_loss_and/,datadatadata84,1566832490,"I was reading about logarithmic loss and I became curious about a few things, so I did an exploratory analysis using a ""good"" model and a ""bad"" model.

Some things I looked at include: distribution of log loss, mean vs. median and calculating it separately for the target and non-target classes.

I'd like to know if anyone has any thoughts or ideas to discuss regarding some of the more nuanced aspects of the metric.

[https://emilyswebber.github.io/LogLoss/](https://emilyswebber.github.io/LogLoss/)",2,1
1590,2019-8-27,2019,8,27,0,cvpgne,Open Problems in Unsupervised Time Series Analysis?,https://www.reddit.com/r/MachineLearning/comments/cvpgne/open_problems_in_unsupervised_time_series_analysis/,chris9larsen,1566832803,[removed],0,1
1591,2019-8-27,2019,8,27,0,cvpufx,"ePub of the early release of ""Hands-on Machine Learning With Scikit-learn, Keras, and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems""?",https://www.reddit.com/r/MachineLearning/comments/cvpufx/epub_of_the_early_release_of_handson_machine/,Albertobagnacani,1566834588,[removed],0,1
1592,2019-8-27,2019,8,27,1,cvq6j5,[R] Call for Papers: Shared Visual Representations in Human and Machine Intelligence (SVRHM) NeurIPS 2019 workshop,https://www.reddit.com/r/MachineLearning/comments/cvq6j5/r_call_for_papers_shared_visual_representations/,NeuroSurfer77,1566836068,"The goal of the Shared Visual Representations in Human and Machine Intelligence (SVRHM) workshop at NeurIPS 2019 is to discuss and disseminate relevant findings and parallels between the computational neuro/cognitive science and machine learning/artificial intelligence communities.

In the past few years, machine learning tools  especially deep neural networks  have permeated the vision/cognitive/neuro science communities to become the leading computational models that describe many cognitive tasks. Huge strides are also being made on the machine learning/artificial intelligence community with biologically inspired algorithms providing large efficiency gains in both computational and learning capabilities. However, many mysteries remain with regards to the alignment of human and machine perception, and there are cases where we see divergent rather than convergent representations. To resolve such questions, this workshop aims to bring fruitful discussions between scientists and engineers with multi-disciplinary backgrounds to review the recent progress in shared visual representations in both humans and machines, and in doing so identifying road-blocks and areas of interest to further accelerate the growth of both fields.

The workshop will include a series of talks and panel discussions from a diverse group of speakers from both industry and academia who will share their research at the intersection of humans and machines that pushes the field of vision forward. The aim of our Call for Papers is to bring together scientists and engineers to share their work in progress at the Poster Session that are applicable to the scope of the Workshop.

The following areas provide a sense of suitable topics for 2-4 page paper submissions:

* Biological inspiration and inductive bias in vision
* Human-relevant strategies for robustness and generalization
* New datasets (e.g., for comparing humans/animals and machines)
* Biologically-driven self-supervision
* Perceptual invariance and metamerism
* Biologically-informed strategies to mitigate adversarial vulnerability
* Foveation, active perception, and attention models
* Intuitive physics
* Perceptual and cognitive robustness
* Nuances and noise in perceptual and cognitive systems
* Creative problem-solving
* Differences and similarities between humans and deep neural networks
* Canonical computations in biological and artificial systems
* Alternative architectures for deep neural networks
* Reverse engineering of the human visual system via deep neural networks

We will be awarding an **NVIDIA Titan RTX** and an **Oculus Quest** as best paper and poster prize respectively at the conference.

Link to the workshop with additional details for the Call for Papers: [https://www.svrhm2019.com/](https://www.svrhm2019.com/)

Link to Paper workshop submission: [https://cmt3.research.microsoft.com/SVRHM2019](https://cmt3.research.microsoft.com/SVRHM2019)

Questions regarding the workshop should be sent to: [info@svrhm2019.com](mailto:info@svrhm2019.com)

Sincerely,

The Organizers

Arturo Deza, Joshua Peterson, Apruva Ratan Murty, Tom Griffiths

*The SVRHM workshop is currently sponsored by NVIDIA, MITs Center from Brains, Minds and Machines (CBMM), National Science Foundation (NSF), Oculus and MITs Quest for Intelligence.*",1,2
1593,2019-8-27,2019,8,27,1,cvq8i9,Stochastic Variance Reduction Gradient Descent (SVRG) optimizer for Keras,https://www.reddit.com/r/MachineLearning/comments/cvq8i9/stochastic_variance_reduction_gradient_descent/,tilkb,1566836307,[removed],0,1
1594,2019-8-27,2019,8,27,1,cvqdso,[Project] Stochastic Variance Reduction Gradient Descent (SVRG) optimizer for Keras,https://www.reddit.com/r/MachineLearning/comments/cvqdso/project_stochastic_variance_reduction_gradient/,tilkb,1566836960,"I've implemented SVRG (Stochastic Variance Reduction Gradient Descent) optimizer for Keras. The goal is to make this optimizer available in Keras as well, which may be beneficial in the case of RL as some [papers](http://www.ifaamas.org/Proceedings/aamas2019/pdfs/p2318.pdf) claimed it is advantageous over Adam.

The paper: [https://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf](https://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf)Link to the project: [https://github.com/tilkb/SVRGoptimizerKeras](https://github.com/tilkb/SVRGoptimizerKeras)",3,31
1595,2019-8-27,2019,8,27,1,cvqk4n,[R] BasisConv: A method for compressed representation and learning in CNNs,https://www.reddit.com/r/MachineLearning/comments/cvqk4n/r_basisconv_a_method_for_compressed/,schrodingershit,1566837744,,12,18
1596,2019-8-27,2019,8,27,1,cvqlrh,[Q] Go environment for training an agent using self play?,https://www.reddit.com/r/MachineLearning/comments/cvqlrh/q_go_environment_for_training_an_agent_using_self/,Carcaso,1566837950,[removed],0,1
1597,2019-8-27,2019,8,27,2,cvra3c,Best online course for someone with a bit of experience in machine learning,https://www.reddit.com/r/MachineLearning/comments/cvra3c/best_online_course_for_someone_with_a_bit_of/,Lightreez,1566840918,[removed],0,0
1598,2019-8-27,2019,8,27,2,cvrac7,Bi-Tempered Logistic Loss for Training Neural Nets with Noisy Data,https://www.reddit.com/r/MachineLearning/comments/cvrac7/bitempered_logistic_loss_for_training_neural_nets/,sjoerdapp,1566840945,,0,1
1599,2019-8-27,2019,8,27,2,cvrbt8,How to train a custom ONNX model for object detection with ML.net,https://www.reddit.com/r/MachineLearning/comments/cvrbt8/how_to_train_a_custom_onnx_model_for_object/,AnderssonPeter,1566841122,"I'm trying to analyse/index some old screenshots, I found the following https://github.com/dotnet/machinelearning-samples/blob/master/samples/csharp/getting-started/DeepLearning_ObjectDetection_Onnx/README.md but it uses a pretrained object model.

I have quite a large image base that I can use for training but I have no idea where to start.",0,1
1600,2019-8-27,2019,8,27,2,cvrhe2,"Humans Dont Realize How Biased They Are Until AI Reproduces the Same Bias, Says UNESCO AI Chair",https://www.reddit.com/r/MachineLearning/comments/cvrhe2/humans_dont_realize_how_biased_they_are_until_ai/,Yuqing7,1566841828,,0,1
1601,2019-8-27,2019,8,27,3,cvrs9t,Data scientist VS Data analyst ?,https://www.reddit.com/r/MachineLearning/comments/cvrs9t/data_scientist_vs_data_analyst/,sajad-52,1566843152,,0,1
1602,2019-8-27,2019,8,27,3,cvrtu7,[Project] Style transfer,https://www.reddit.com/r/MachineLearning/comments/cvrtu7/project_style_transfer/,BUGODI,1566843362,,0,1
1603,2019-8-27,2019,8,27,3,cvryub,[Project] Style transfer - Human into Art,https://www.reddit.com/r/MachineLearning/comments/cvryub/project_style_transfer_human_into_art/,BUGODI,1566843978,"What's up redditors, wanted to share a video I made using pytorch to make art, produced via machine learning algorithms. With the use of a green screen and neural networks, I was able to style transfer actual art onto the singer. If you have any questions I would be more than glad to answer, let me know what you think!

Video: [https://www.youtube.com/watch?v=TW99ygvKFq4](https://www.youtube.com/watch?v=TW99ygvKFq4)",6,4
1604,2019-8-27,2019,8,27,3,cvsbh2,Looking for Research Positions worldwide for undergraduate thesis,https://www.reddit.com/r/MachineLearning/comments/cvsbh2/looking_for_research_positions_worldwide_for/,goonermaxx,1566845552,[removed],0,1
1605,2019-8-27,2019,8,27,4,cvsg4b,Tensorflow Object Detection API validation question,https://www.reddit.com/r/MachineLearning/comments/cvsg4b/tensorflow_object_detection_api_validation/,ChowderII,1566846120,"Hi everyone,

I am using the Tensorflow Object Detection API to recognize specific vessels. So far, I retrained the original ssd mobilenet v2 to recognize different types of ships (cargo, tanker, fishing etc.). I exported that model and did transfer learning on the exported model to try to produce a model capable of recognizing a specific ship among the same class (ship A vs B but they are both tankers for example). My approach was to create a network for each ship with only a single class. The problem now is that the trained models performs well on a 80% 20% train/test split dataset of the same ship, but as soon as I show the network a new ship, it labels it with very high confidence. 

I was wondering if you gurus had any idea why that might happen. My dataset is not very large (\~300 pictures) but I have one with about 1000. Should I perform k-fold cross validation? This post on SO suggests not to do so but it doesn't explain why: [https://stackoverflow.com/questions/51989259/k-fold-cross-validation-tensorflow-object-detection](https://stackoverflow.com/questions/51989259/k-fold-cross-validation-tensorflow-object-detection)

Thanks for your help.",0,1
1606,2019-8-27,2019,8,27,5,cvtml5,[P] I applied the recent 'Progressive Face Super-Resolution via Attention to Facial Landmark' to create 'photo-realistic' Emojis and Emotes.,https://www.reddit.com/r/MachineLearning/comments/cvtml5/p_i_applied_the_recent_progressive_face/,JonathanFly,1566851333,,0,1
1607,2019-8-27,2019,8,27,5,cvtprp,[P] I applied the recent 'Progressive Face Super-Resolution via Attention to Facial Landmark' to create 'photo-realistic' Emojis and Emotes.,https://www.reddit.com/r/MachineLearning/comments/cvtprp/p_i_applied_the_recent_progressive_face/,JonathanFly,1566851722,"*Progressive Face Super-Resolution via Attention to Facial Landmark* [arxiv.org](https://arxiv.org/abs/1908.08239) is a machine learning model trained to reconstruct face images from tiny 1616 pixel input images, scaling them up to 128128 with nearly photo-realistic results. I tried running emojis, Twitch emotes, and a few game sprites through it. 

I did have to do quite a fit of cherry picking, and I also iteratively ran the output back into the inputs to encourage the model the add human features. Some of the best examples:

https://i.redd.it/porrhn5gsui31.png

 I also created a (very sloppy) [Colab Version](https://gist.github.com/JonathanFly/80b669a72bf624d17b56a1cfec742588#file-progressivefacesuperresolutiondemo-ipynb) of the paper's github demo if you want to try this yourself.",22,196
1608,2019-8-27,2019,8,27,5,cvtzma,Venues for Reproducibility Studies,https://www.reddit.com/r/MachineLearning/comments/cvtzma/venues_for_reproducibility_studies/,fail_daily,1566852951,[removed],0,1
1609,2019-8-27,2019,8,27,5,cvu1o8,Features to get from Pcap packets?,https://www.reddit.com/r/MachineLearning/comments/cvu1o8/features_to_get_from_pcap_packets/,reddit1110,1566853198,[removed],0,1
1610,2019-8-27,2019,8,27,6,cvum9k,Model training sanity check,https://www.reddit.com/r/MachineLearning/comments/cvum9k/model_training_sanity_check/,vishalagarwal,1566855739,[removed],0,1
1611,2019-8-27,2019,8,27,6,cvut8u,Using Open Source Material for Commercial Project?,https://www.reddit.com/r/MachineLearning/comments/cvut8u/using_open_source_material_for_commercial_project/,100Camels,1566856609,"Hello all - sorry if this is a common question or one that has been answered before now. 

I am doing some freelance data science work and am looking at a proposed work contract that has the following terms:  
""However, in no event will Consultant incorporate into the Work Product any software code licensed under the GNU GPL or LGPL or any similar open source license. Consultant represents and warrants that Consultant has an unqualified right to license to Client all Preexisting IP as provided in this section.""  


Does this mean that I can't use tools like TensorFlow (or indeed, an open source data science tool like ScikitLearn?)

&amp;#x200B;

Has anyone encountered terms like this before? Any advice?",0,1
1612,2019-8-27,2019,8,27,7,cvv41u,How could ML be used in ecommerce,https://www.reddit.com/r/MachineLearning/comments/cvv41u/how_could_ml_be_used_in_ecommerce/,javascript_dev,1566858015,[removed],0,1
1613,2019-8-27,2019,8,27,7,cvv9e7,Talking to professors about their research,https://www.reddit.com/r/MachineLearning/comments/cvv9e7/talking_to_professors_about_their_research/,DaBobcat,1566858710,"I'm considering applying to a PhD in Machine Learning and I was going to send an email to a professor.  
I wanted to start with something along the lines of ""I enjoyed your research on..."" and fill in the blank one of the papers he's on that I did actually enjoy

But now that I'm thinking about it I'm not sure if that's just a research paper that his PhD students wrote or something else?  
Can I refer to this paper as ""his research"" or should I go with ""research you were supervising...""/ etc.?",0,1
1614,2019-8-27,2019,8,27,7,cvvixe,Is it hard to get a job in Artificial Intelligence without a degree?,https://www.reddit.com/r/MachineLearning/comments/cvvixe/is_it_hard_to_get_a_job_in_artificial/,MrJJGH,1566859958,the question says it all :),0,1
1615,2019-8-27,2019,8,27,8,cvvwa0,"[N] Huawei launches Ascend 910, the ""world's most powerful"" AI processor and MindSpore, an AI computing framework",https://www.reddit.com/r/MachineLearning/comments/cvvwa0/n_huawei_launches_ascend_910_the_worlds_most/,MassivePellfish,1566861709,"Today, the Chinese firm reached a major milestone in its AI roadmap, announcing Ascend 910, the ""world's most powerful AI processor"", and MindSpore, an AI computing framework. With this launch, the firm has unveiled all the key components of its full-stack, all-scenario AI portfolio.

Speaking at Huawei's headquarters in Shenzhen, Eric Xu, the company's rotating chairman, spoke regarding the new products, noting:

""We have been making steady progress since we announced our AI strategy in October last year. Everything is moving forward according to plan, from R&amp;D to product launch. We promised a full-stack, all-scenario AI portfolio. And today we delivered, with the release of Ascend 910 and MindSpore. This also marks a new stage in Huawei's AI strategy.""

The Ascend 910, in conjunction with the MindSpore framework, is used to train AI models. Surpassing even Huawei's own expectations in terms of its performance, the new processor delivers 256 TeraFLOPS of computing speed for half-precision floating point (FP16) operations, and 512 TeraFLOPS for integer precision calculations (INT8). Its maximum power consumption is 310W, lower than the initially planned 350W, despite being around two times faster in training AI models based on standard deep neural networks like ResNet-50. The new AI processor belongs to Huawei's series of Ascend-Max chipsets.

Similarly, with the release of MindSpore, Huawei believes it has furthered its AI framework development goals. These comprised of reduction in training times and costs, efficient execution, and more adaptability. Aside from fulfilling these requirements, MindSpore also puts privacy protection and security at the forefront. To clarify this a bit further, the new AI framework deals only with processed information, not actual user data. It also has built-in model protection tech to ensure that the AI models being utilized are trustworthy.

Furthermore, with regards to actual performance, MindSpore has 20% fewer lines of code than other frameworks in a typical neural network for natural language processing (NLP). Combined with its ""AI Algorithm As Code"" design concept, the product provides a high degree of adaptability and efficiency. Aside from Ascend processors, support is also offered for GPUs, CPUs, and other types of processors. Importantly, Xu also offered some insight into the framework's future, noting that ""MindSpore will go open source in the first quarter of 2020. We want to drive broader AI adoption and help developers do what they do best.""

Despite the new unveilings, Huawei isn't slowing down its pursuit of AI dominance, and will be revealing more such products in its upcoming Connect 2019 conference, scheduled to be held between September 18 and 20 in Shanghai, China.

https://www.neowin.net/news/huawei-launches-ascend-910-the-world039s-most-powerful-ai-processor-and-mindspore/",11,5
1616,2019-8-27,2019,8,27,8,cvw568,Influence functions for Keras or PyTorch,https://www.reddit.com/r/MachineLearning/comments/cvw568/influence_functions_for_keras_or_pytorch/,milkteaoppa,1566862908,[removed],0,1
1617,2019-8-27,2019,8,27,8,cvw9u3,Effect of encoding on predictive algorithm,https://www.reddit.com/r/MachineLearning/comments/cvw9u3/effect_of_encoding_on_predictive_algorithm/,shiolovesgod,1566863536,[removed],0,1
1618,2019-8-27,2019,8,27,10,cvxs5c,"[D] Those who do computer vision, how do you handle dataset management?",https://www.reddit.com/r/MachineLearning/comments/cvxs5c/d_those_who_do_computer_vision_how_do_you_handle/,iocuydi,1566871165,"Hi all! I'm curious about the best ways to manage large image and video datasets for computer vision projects.

I'm an ML engineer on a team of \~10, supported by 5 data labellers.

I was wondering how other teams in CV space manage:

\-Storing the datasets in a central (hosted?) location, and version controlling them as needed, with minimal overhead

\-Allowing for querying and visual exploration of the datasets for quick adjustment or examination of labels

\-Efficiently pulling a dataset or subset of a dataset to a local machine.

\-Automating the flow of datasets as much as possible, i.e. train x model on y subset of z dataset.

\-Compressing less frequently used data as much as possible for ""cold storage"" and handling uncompression/recompression when the data is needed for training or when new data is added

&amp;#x200B;

So far we've used 3 solutions:

1. Storing everything on a local machine sitting under an unoccupied desk, and everybody manually updated the data there
2. Storing compressed tar files of the data on AWS storage and retrieving/updating it manually every so often.
3.  Assigning one of the data labellers to spend some time as a ""dataset manager"" and try to do this for us.

Each of these has had it's own set of problems, and I feel I waste a lot of time dealing with the overhead of this stuff.

How do you guys deal with this situation? Is there an ""industry standard"" correct way of managing this stuff? Like a github for CV? At places like Waymo/Tesla for example where they are constantly growing and updating their dataset to improve weak points, I would think an elegant solution for this has been devised.

One caveat is that I'd like to avoid using things like AWS and Azure ML ""low code"" services that might do some data management for you but then take away most of the freedom of working in TF/Pytorch, and make the model into a black box.",35,24
1619,2019-8-27,2019,8,27,12,cvz0i3,Mesa 7i96 with closed loop motors by linuxcnc 2.8,https://www.reddit.com/r/MachineLearning/comments/cvz0i3/mesa_7i96_with_closed_loop_motors_by_linuxcnc_28/,TERA360,1566877808,,0,1
1620,2019-8-27,2019,8,27,14,cw08s1,Data Preprocessing in Machine learning,https://www.reddit.com/r/MachineLearning/comments/cw08s1/data_preprocessing_in_machine_learning/,nehapandey01,1566885452,,0,1
1621,2019-8-27,2019,8,27,15,cw0fpa,Embeddings used in Attention is all you need.,https://www.reddit.com/r/MachineLearning/comments/cw0fpa/embeddings_used_in_attention_is_all_you_need/,fduprealbad,1566886763,"In [Attention is all you need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf), it is written that attention transforms a sequence of vectors of dimension 512, how are those vectors initially obtained from normal words?",0,1
1622,2019-8-27,2019,8,27,15,cw0ktn,[R] Artificial Intelligence: A Tale of Social Responsibility,https://www.reddit.com/r/MachineLearning/comments/cw0ktn/r_artificial_intelligence_a_tale_of_social/,TParcollet,1566887760,[removed],0,1
1623,2019-8-27,2019,8,27,15,cw0l28,Best way to learn Machine Learning after having done a Computer Science Bachelor,https://www.reddit.com/r/MachineLearning/comments/cw0l28/best_way_to_learn_machine_learning_after_having/,Lightreez,1566887797,[removed],0,1
1624,2019-8-27,2019,8,27,15,cw0rq8,Interesting facts of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cw0rq8/interesting_facts_of_machine_learning/,prih_yah,1566889098,,0,1
1625,2019-8-27,2019,8,27,16,cw0y5a,TOP 11 JavaScript Machine Learning &amp; Data Science Libraries,https://www.reddit.com/r/MachineLearning/comments/cw0y5a/top_11_javascript_machine_learning_data_science/,marylai22,1566890355,,0,1
1626,2019-8-27,2019,8,27,16,cw160x,NLP Town Blog | Distilling Bert Models with spaCy,https://www.reddit.com/r/MachineLearning/comments/cw160x/nlp_town_blog_distilling_bert_models_with_spacy/,yvespeirsman,1566891971,,0,1
1627,2019-8-27,2019,8,27,16,cw186o,"Instagram's Image classifier at work, visible when network is down.",https://www.reddit.com/r/MachineLearning/comments/cw186o/instagrams_image_classifier_at_work_visible_when/,shakeebbk,1566892405,"&amp;#x200B;

![img](i7dbms206yi31)",0,1
1628,2019-8-27,2019,8,27,16,cw196h,End to End Deep Learning.,https://www.reddit.com/r/MachineLearning/comments/cw196h/end_to_end_deep_learning/,andrea_manero,1566892637,[removed],0,1
1629,2019-8-27,2019,8,27,17,cw1cgh,r/datascienceproject gained 1600+ followers in a week.,https://www.reddit.com/r/MachineLearning/comments/cw1cgh/rdatascienceproject_gained_1600_followers_in_a/,OppositeMidnight,1566893328,[removed],0,1
1630,2019-8-27,2019,8,27,17,cw1dow,Discriminator Loss on Real Images in GAN,https://www.reddit.com/r/MachineLearning/comments/cw1dow/discriminator_loss_on_real_images_in_gan/,DebajyotiS,1566893572,[removed],0,1
1631,2019-8-27,2019,8,27,17,cw1e79,[P] r/datascienceproject gained 1600+ followers in a week.,https://www.reddit.com/r/MachineLearning/comments/cw1e79/p_rdatascienceproject_gained_1600_followers_in_a/,OppositeMidnight,1566893685,"Follow and add your project to [r/datascienceproject](https://www.reddit.com/r/datascienceproject/)

Examples:

[Predicting Collegiate Basketball Win Shares from ESPN Ratings and High School Statistics](https://www.reddit.com/r/datascienceproject/comments/cui22t/predicting_collegiate_basketball_win_shares_from/)

[Impact of training hours on employee performance](https://www.reddit.com/r/datascienceproject/comments/cvm6yd/impact_of_training_hours_on_employee_performance/)

[Any data science applications in Physics?](https://www.reddit.com/r/datascienceproject/comments/cu7dbn/any_data_science_applications_in_physics/)",0,0
1632,2019-8-27,2019,8,27,17,cw1gm0,Next Generation Automated Machine Learning (AML),https://www.reddit.com/r/MachineLearning/comments/cw1gm0/next_generation_automated_machine_learning_aml/,andrea_manero,1566894204,[removed],0,1
1633,2019-8-27,2019,8,27,17,cw1ivb,"convert video data (.avi, .mp4 etc.) to TensorFlow tfrecords",https://www.reddit.com/r/MachineLearning/comments/cw1ivb/convert_video_data_avi_mp4_etc_to_tensorflow/,whiletrue2,1566894704,,0,1
1634,2019-8-27,2019,8,27,18,cw1wqd,What is the best language to learn?,https://www.reddit.com/r/MachineLearning/comments/cw1wqd/what_is_the_best_language_to_learn/,samcharchil,1566897599,[removed],0,2
1635,2019-8-27,2019,8,27,18,cw1wrn,Is anybody using Snorkel in practice?,https://www.reddit.com/r/MachineLearning/comments/cw1wrn/is_anybody_using_snorkel_in_practice/,lambdaofgod,1566897607,[removed],0,1
1636,2019-8-27,2019,8,27,18,cw24dw,[P] Distilling Bert Models with spaCy,https://www.reddit.com/r/MachineLearning/comments/cw24dw/p_distilling_bert_models_with_spacy/,yvespeirsman,1566899055,,0,1
1637,2019-8-27,2019,8,27,18,cw292z,"Building Machine Learning Systems with Python By Luis Pedro Coelho, Willi Richert PDF",https://www.reddit.com/r/MachineLearning/comments/cw292z/building_machine_learning_systems_with_python_by/,psychonekk,1566899926,,0,1
1638,2019-8-27,2019,8,27,19,cw2cbb,"[N] Inside the UK unicorn that's about to become the Intel of AI: Bristol-based Graphcore's ""intelligence processing unit"" aims to do for AI what the graphics processing unit did for computing",https://www.reddit.com/r/MachineLearning/comments/cw2cbb/n_inside_the_uk_unicorn_thats_about_to_become_the/,platdujour,1566900491,,0,1
1639,2019-8-27,2019,8,27,19,cw2eom,"[P] JAXnet (preview) - An alternative to TensorFlow2/Keras/PyTorch for more concise, robust and optimized deep learning code",https://www.reddit.com/r/MachineLearning/comments/cw2eom/p_jaxnet_preview_an_alternative_to/,juliuskunze,1566900913,,0,1
1640,2019-8-27,2019,8,27,19,cw2gdk,[P] These Instagram Portrait Models Do Not Exist,https://www.reddit.com/r/MachineLearning/comments/cw2gdk/p_these_instagram_portrait_models_do_not_exist/,ai_imagine,1566901198,"I used transfer learning to train Stylegan using [Gwern](https://www.gwern.net/Faces)'s This Waifu Does not exist anime model.

the dataset was originally around 3k images augmented to 13k.

Here are some interpolation [results](https://www.instagram.com/p/B1eJib4HEmt/?utm_source=ig_web_copy_link).

Interactive/controlled image generation results.",5,8
1641,2019-8-27,2019,8,27,19,cw2l9p,"[P] JAXnet (preview) - An alternative to TensorFlow2/Keras/PyTorch for more concise, robust and optimized deep learning code",https://www.reddit.com/r/MachineLearning/comments/cw2l9p/p_jaxnet_preview_an_alternative_to/,juliuskunze,1566902104,"[On GitHub](https://github.com/JuliusKunze/jaxnet)

JAXnet is a deep learning library based on Google's new JAX library. JAXnet's functional API provides unique benefits over TensorFlow2, Keras and PyTorch, while maintaining user-friendliness, modularity and scalability:

* More robustness through immutable weights, no global compute graph.
* GPU-compiled numpy code for networks, training loops, pre- and postprocessing.
* Regularization and reparametrization of any module or whole networks in one line.
* No global random state, flexible random key control.

I have been working on this project over the past weeks. Feedback and questions are welcome!",4,23
1642,2019-8-27,2019,8,27,19,cw2n0a,Well. Girl it is. Thanks auto-captions.,https://www.reddit.com/r/MachineLearning/comments/cw2n0a/well_girl_it_is_thanks_autocaptions/,headphones_bulldog,1566902423,,0,1
1643,2019-8-27,2019,8,27,20,cw2yui, Episode 5: Out of the lab,https://www.reddit.com/r/MachineLearning/comments/cw2yui/episode_5_out_of_the_lab/,sjoerdapp,1566904487,,0,1
1644,2019-8-27,2019,8,27,20,cw39dx,[P] I applied Mark Zuckerberg's face to Facebook emojis,https://www.reddit.com/r/MachineLearning/comments/cw39dx/p_i_applied_mark_zuckerbergs_face_to_facebook/,rybakovcom,1566906229,"Seeing the post on photorealistic emojis reminded me of a project I did last year: [Zuckerberg Emojis](https://rybakov.com/blog/zuckerberg_emojis/)

&amp;#x200B;

[Sad Mark](https://i.redd.it/669tx1a7azi31.jpg)

Why? Well, facebook forces us to use quite specific representation of emotions to react to things. In a way, these emojis become our facial expression. So it would only fair to apply the same expression to Zuckerberg's face.

I used CNNMRF, Deep Image Analogy and jcjohnsons neural style in sequence to apply the face and upscale it to a good resolution.

[ 	1.Original 2.CNNMRF result 3. Deep Image Analogy output 4.Upscaled with Neural-style ](https://i.redd.it/yd0dmyoyazi31.jpg)

The full write-up with all emojis is here: [https://rybakov.com/blog/zuckerberg\_emojis/](https://rybakov.com/blog/zuckerberg_emojis/)",70,543
1645,2019-8-27,2019,8,27,21,cw3lve,Learning Suggestion: Python Machine Learning (book) vs. Kaggle ML Course,https://www.reddit.com/r/MachineLearning/comments/cw3lve/learning_suggestion_python_machine_learning_book/,hpdipto,1566908119,[removed],0,1
1646,2019-8-27,2019,8,27,21,cw3ola,"[N] DeepMind releases OpenSpiel: a framework for reinforcement learning in games. It contains over 25 games, and 20 algorithms, including tools for visualisation and evaluation.",https://www.reddit.com/r/MachineLearning/comments/cw3ola/n_deepmind_releases_openspiel_a_framework_for/,Corp-Por,1566908522,,1,1
1647,2019-8-27,2019,8,27,22,cw44h0,About Deep Neural Embeddings,https://www.reddit.com/r/MachineLearning/comments/cw44h0/about_deep_neural_embeddings/,pk12_,1566910833,,0,1
1648,2019-8-27,2019,8,27,22,cw47iy,Part 2: Selecting the right weight initialization for your deep neural network,https://www.reddit.com/r/MachineLearning/comments/cw47iy/part_2_selecting_the_right_weight_initialization/,CometML,1566911247,,0,4
1649,2019-8-27,2019,8,27,22,cw4bo9,[D] What do you use to keep you update on ML/DL?,https://www.reddit.com/r/MachineLearning/comments/cw4bo9/d_what_do_you_use_to_keep_you_update_on_mldl/,pirate7777777,1566911820,[removed],0,1
1650,2019-8-27,2019,8,27,23,cw52kj,"[Research] A Low-Cost, Open-Source Robotic Racecar for Education and Research",https://www.reddit.com/r/MachineLearning/comments/cw52kj/research_a_lowcost_opensource_robotic_racecar_for/,cdossman,1566915406,[removed],0,1
1651,2019-8-27,2019,8,27,23,cw551j,MuJoCo Registration Captcha,https://www.reddit.com/r/MachineLearning/comments/cw551j/mujoco_registration_captcha/,hain9,1566915724,[removed],0,1
1652,2019-8-27,2019,8,27,23,cw5kcn,How to go about teaching a pc to be a commentator on FIFA Soccer,https://www.reddit.com/r/MachineLearning/comments/cw5kcn/how_to_go_about_teaching_a_pc_to_be_a_commentator/,AboutVR018734,1566917712,[removed],0,1
1653,2019-8-27,2019,8,27,23,cw5kk8,[N] Huaweis First Commercial AI Chip Doubles the Training Performance of Nvidias Flagship GPU,https://www.reddit.com/r/MachineLearning/comments/cw5kk8/n_huaweis_first_commercial_ai_chip_doubles_the/,Empty_Lecture,1566917739,"Billed as the single chip with the greatest computing density, **Ascend 910 delivers performance of up to 256 teraFLOPS under FP16 and 512 teraOPS under IN8 with declared max power consumption of 310W.** In comparison, the GPU Tesla V100 delivers up to 125 teraFLOPS with a max power consumption of 300W, while Googles TPU 2.0 with four ASICs can reach 180 teraFLOPS.

&amp;#x200B;

Link: [https://medium.com/syncedreview/huaweis-first-commercial-ai-chip-doubles-the-training-performance-of-nvidia-s-flagship-gpu-86e4d0078f6f](https://medium.com/syncedreview/huaweis-first-commercial-ai-chip-doubles-the-training-performance-of-nvidia-s-flagship-gpu-86e4d0078f6f)",25,41
1654,2019-8-28,2019,8,28,0,cw6ash,Find the minimum distance between a list of points and a point using cosine distance formula?,https://www.reddit.com/r/MachineLearning/comments/cw6ash/find_the_minimum_distance_between_a_list_of/,budines,1566921013,[removed],0,1
1655,2019-8-28,2019,8,28,1,cw6she,Seeking Cloud Based TPU solution that is not Google.,https://www.reddit.com/r/MachineLearning/comments/cw6she/seeking_cloud_based_tpu_solution_that_is_not/,Actual__Wizard,1566923158,[removed],0,1
1656,2019-8-28,2019,8,28,1,cw6vzo,Has deepmind published any papers on alpha star?,https://www.reddit.com/r/MachineLearning/comments/cw6vzo/has_deepmind_published_any_papers_on_alpha_star/,space_physics,1566923583,[removed],0,1
1657,2019-8-28,2019,8,28,1,cw6ykn,[D] Jeremy Howard: fast.ai | Artificial Intelligence Podcast,https://www.reddit.com/r/MachineLearning/comments/cw6ykn/d_jeremy_howard_fastai_artificial_intelligence/,UltraMarathonMan,1566923894,"Jeremy Howard is the founder of [fast.ai](https://fast.ai), a research institute dedicated to make deep learning more accessible. He is also a Distinguished Research Scientist at the University of San Francisco, a former president of Kaggle as well a top-ranking competitor there, and in general, he's a successful entrepreneur, educator, research, and an inspiring personality in the AI community.

**Video**: [https://www.youtube.com/watch?v=J6XcP4JOHmk](https://www.youtube.com/watch?v=J6XcP4JOHmk)

**Audio**: [https://lexfridman.com/jeremy-howard](https://lexfridman.com/jeremy-howard)

https://i.redd.it/qzl0xu3kr0j31.png

**Outline:**

0:00 - Introduction

1:18 - First program

3:07 - Favorite programming languages

15:01 - Programming languages for machine learning

23:35 - [Fast.ai](https://Fast.ai) intro

24:31 - Ai and deep learning in medicine

32:30 - Privacy

37:55 - [Fast.ai](https://Fast.ai)

40:42 - Theory vs practice

45:43 - DAWNBench - Stanford deep learning benchmark

56:24 - Fusing multiple audio and image sources

59:01 - Learning rate &amp; deep learning as an experimental science

1:04:32 - Working with data

1:06:16 - Deep learning cloud options

1:09:12 - Deep learning frameworks

1:17:51 - How long does it take to finish [fast.ai](https://fast.ai) courses?

1:19:49 - Lessons from teaching deep learning

1:21:34 - Advice for people starting with deep learning

1:27:02 - Startups and entrepreneurship

1:32:21 - Anki and spaced repetition

1:40:06 - Next breakthrough in deep learning

1:41:17 - Job displacement and Andrew Yang",0,1
1658,2019-8-28,2019,8,28,1,cw700s,Jeremy Howard: fast.ai Deep Learning Courses and Research | Artificial Intelligence Podcast,https://www.reddit.com/r/MachineLearning/comments/cw700s/jeremy_howard_fastai_deep_learning_courses_and/,UltraMarathonMan,1566924080,[removed],0,1
1659,2019-8-28,2019,8,28,1,cw71ut,OpenAI Method Evaluates Model Defense Against Unforeseen Adversarial Examples,https://www.reddit.com/r/MachineLearning/comments/cw71ut/openai_method_evaluates_model_defense_against/,Yuqing7,1566924308,,0,1
1660,2019-8-28,2019,8,28,2,cw7b1e,How can I improve my ability in machine learning?,https://www.reddit.com/r/MachineLearning/comments/cw7b1e/how_can_i_improve_my_ability_in_machine_learning/,elyasbeshkani,1566925442,[removed],0,1
1661,2019-8-28,2019,8,28,2,cw7fsm,[D] NLP's Clever Hans Moment Has Arrived,https://www.reddit.com/r/MachineLearning/comments/cw7fsm/d_nlps_clever_hans_moment_has_arrived/,hughbzhang,1566926036,"Do neural networks learn what we think they learn? Benjamin Heinzerling reviews research that suggests that they often instead fall prey to the so-called Clever Hans effect and discusses its implications for NLP.

&amp;#x200B;

[https://thegradient.pub/nlps-clever-hans-moment-has-arrived/](https://thegradient.pub/nlps-clever-hans-moment-has-arrived/)",4,60
1662,2019-8-28,2019,8,28,2,cw7kqd,Trying to get into AI ML industry-I am trying to double major in math-stats &amp; computer science AI track. Am I going in the right direction?,https://www.reddit.com/r/MachineLearning/comments/cw7kqd/trying_to_get_into_ai_ml_industryi_am_trying_to/,mathguynumberone,1566926651,[removed],0,1
1663,2019-8-28,2019,8,28,2,cw7mr8,Check out the latest DVC features in this blog post,https://www.reddit.com/r/MachineLearning/comments/cw7mr8/check_out_the_latest_dvc_features_in_this_blog/,naxty1995,1566926904,[removed],0,1
1664,2019-8-28,2019,8,28,2,cw7q49,Exploring Weight Agnostic Neural Networks,https://www.reddit.com/r/MachineLearning/comments/cw7q49/exploring_weight_agnostic_neural_networks/,sjoerdapp,1566927317,,0,1
1665,2019-8-28,2019,8,28,3,cw8avv,Tech Stack for Ctrl Shift Face?,https://www.reddit.com/r/MachineLearning/comments/cw8avv/tech_stack_for_ctrl_shift_face/,iwantedthisusername,1566929875,"Is anyone familiar with the tech stack used to generate the videos on [Ctrl Shift Face](https://www.youtube.com/channel/UCKpH0CKltc73e4wh0_pgL3g)?  


There's also this [lion king video](https://www.youtube.com/watch?v=Y1HGgICqZ3c) which suggests it's using deep fake tech, but an [Instagram post](https://www.instagram.com/p/B04QVN9nda5/) from the creator says it's actually just style transfer using a manually drawn set of [stylized images](https://www.instagram.com/p/B0ThqQHo-Gn/?utm_source=ig_web_copy_link) to transfer from  


I normally work with NLP but I'm intrigued to explore some of the recent work with video. In particular if anyone can point me to research on temporal consistency between frames that would be great as it appears that most implementations (though not all) struggle with that part.",0,1
1666,2019-8-28,2019,8,28,3,cw8mf8,Development of Artificial Intelligence  A Brief History,https://www.reddit.com/r/MachineLearning/comments/cw8mf8/development_of_artificial_intelligence_a_brief/,rr3300,1566931314,[removed],0,1
1667,2019-8-28,2019,8,28,3,cw8o3i,"A Beginner's Guide to Business Intelligence Definitions, tools and techniques",https://www.reddit.com/r/MachineLearning/comments/cw8o3i/a_beginners_guide_to_business_intelligence/,facele,1566931516,,1,1
1668,2019-8-28,2019,8,28,3,cw8pie,Machine Learning Rules from Google,https://www.reddit.com/r/MachineLearning/comments/cw8pie/machine_learning_rules_from_google/,azzipog,1566931696,,0,1
1669,2019-8-28,2019,8,28,4,cw9kbn,How to create Experience Replay table?,https://www.reddit.com/r/MachineLearning/comments/cw9kbn/how_to_create_experience_replay_table/,Kralex68,1566935497,[removed],0,1
1670,2019-8-28,2019,8,28,4,cw9kpx,[D] How to create Experience Replay table?,https://www.reddit.com/r/MachineLearning/comments/cw9kpx/d_how_to_create_experience_replay_table/,Kralex68,1566935542," Hello  I want to create a DQN for a 2 player strategic board game. I read that  experience replay is recommended to break correlation. I want to know  how I create the experience replay table?

Can  I build it using random agent?(Playing random moves). After I have  built the table I want to proceed like that: I want to randomly take one  state out of experience replay table, take an action according to  epsilon greedy policy. Put in the next state into a second neural  network and backpropagate the difference Q values(From second neural  network and first) in the first neuronal network. After that I randomly  choose another state and repeat the step from above. Is this correct?  Thank you",1,0
1671,2019-8-28,2019,8,28,5,cwa713,[D] Is there an accepted state-of-the-art for Video Action Localisation/Region Suggestion?,https://www.reddit.com/r/MachineLearning/comments/cwa713/d_is_there_an_accepted_stateoftheart_for_video/,lantern_lol,1566938291,"I am working on a project which involves video action recognition, and am using the fantastic [I3D](https://arxiv.org/abs/1705.07750) approach.

However, I am now interested in localising the region of video which contains the action being classified (i.e. with a bounding box). For example, if my I3D network classifies a segment of video containing the action of a human ""Eating an Apple"", I now want to draw a bounding box over the person eating the apple in each frame of video. Note that I am not interested in drawing a region around ""Apple"" or ""Human"", but instead the region in which the action itself is being performed.

I am familiar with similar approaches for image classification (e.g. YOLO), but am having trouble finding work in the video domain for actions. Can anyone point me to some good papers which cover this if they exist?",4,2
1672,2019-8-28,2019,8,28,6,cwavku,[P] photo realistic character art with stylegan,https://www.reddit.com/r/MachineLearning/comments/cwavku/p_photo_realistic_character_art_with_stylegan/,PuzzledProgrammer3,1566941345,"model: [https://www.dropbox.com/s/y8petm9dd74uv9g/allface2.pkl](https://www.dropbox.com/s/y8petm9dd74uv9g/allface2.pkl)

interpolation: https://twitter.com/roadrunning01/status/1156368137669005314?s=20",5,6
1673,2019-8-28,2019,8,28,6,cwayuq,[Google AI] Exploring Weight Agnostic Neural Networks,https://www.reddit.com/r/MachineLearning/comments/cwayuq/google_ai_exploring_weight_agnostic_neural/,iamdiegovincent,1566941760,,0,1
1674,2019-8-28,2019,8,28,7,cwbebl,[D] How to add direction to MLP shape recognition?,https://www.reddit.com/r/MachineLearning/comments/cwbebl/d_how_to_add_direction_to_mlp_shape_recognition/,fusedotcore,1566943545,"I'm using a MLP to recognise gestures for my VR game.    
The gesture gets preprocessed into a 2D grid, every grid cell becomes an input.  
However I would like to add the direction into the recognition.    
My first thought was for each cell also input a 0 value for never entered and 0.5 - 1 for first - last cell.    
Any tips?",11,1
1675,2019-8-28,2019,8,28,7,cwbi5s,How to think like a programmer,https://www.reddit.com/r/MachineLearning/comments/cwbi5s/how_to_think_like_a_programmer/,susanvilleula1,1566944014,,0,0
1676,2019-8-28,2019,8,28,8,cwcdrd,Non Technical ML careers,https://www.reddit.com/r/MachineLearning/comments/cwcdrd/non_technical_ml_careers/,tshirtguy2000,1566948144,[removed],0,1
1677,2019-8-28,2019,8,28,9,cwczik,Starting a Ph.D. in Computer Science a big mistake?,https://www.reddit.com/r/MachineLearning/comments/cwczik/starting_a_phd_in_computer_science_a_big_mistake/,james75yw,1566951181,[removed],0,1
1678,2019-8-28,2019,8,28,9,cwdeyb,Isn't self-attention basically a discriminator?,https://www.reddit.com/r/MachineLearning/comments/cwdeyb/isnt_selfattention_basically_a_discriminator/,ZeroMaxinumXZ,1566953357,[removed],0,1
1679,2019-8-28,2019,8,28,10,cwdtz6,Personal avatar that my kids' kids' kids can ask questions,https://www.reddit.com/r/MachineLearning/comments/cwdtz6/personal_avatar_that_my_kids_kids_kids_can_ask/,muilenta,1566955433,[removed],0,1
1680,2019-8-28,2019,8,28,10,cwdvfs,[R]The Path to Nash Equilibrium,https://www.reddit.com/r/MachineLearning/comments/cwdvfs/rthe_path_to_nash_equilibrium/,lansiz,1566955634,"Main point: *Nash equilibrium can be achieved without any beyond-player mediation, and the path towards it can be clearly visualized.*

[https://arxiv.org/abs/1908.09021](https://arxiv.org/abs/1908.09021)

[Demos at Github to try.](https://github.com/lansiz/eqpt) **Fun guaranteed.**

And the following figures from paper shows  visualizations of the paths towards Nash equilibrium:

[3X3 two-person game.](https://i.redd.it/8ckt7y9ec3j31.png)

&amp;#x200B;

[60X40 two-person game with 60 or 40 dimensions being reduced to 3 dimensions by PCA. ](https://i.redd.it/tlc0l9w2d3j31.png)

&amp;#x200B;

[Equilibrium point is always the final destination of strategy path.](https://i.redd.it/3k4mzdn8d3j31.png)",15,113
1681,2019-8-28,2019,8,28,11,cwegiu,Stand out from the Crowd with Our Fairy Floss Machine and Supplies,https://www.reddit.com/r/MachineLearning/comments/cwegiu/stand_out_from_the_crowd_with_our_fairy_floss/,slushieco,1566958539,,0,1
1682,2019-8-28,2019,8,28,12,cwezgl,"Relationship between #AI, #MachineLearning #DeepLearning &amp; #DataScience ?",https://www.reddit.com/r/MachineLearning/comments/cwezgl/relationship_between_ai_machinelearning/,corpnce,1566961359,"&amp;#x200B;

![img](hz2eje2js3j31)

[Read More&gt;](https://corpnce.com/2019/08/27/relationship-ai-ml-dl-ds/)",0,1
1683,2019-8-28,2019,8,28,14,cwga3l,[D] Halt the use of facial-recognition technology until it is regulated,https://www.reddit.com/r/MachineLearning/comments/cwga3l/d_halt_the_use_of_facialrecognition_technology/,hardmaru,1566969171,,0,1
1684,2019-8-28,2019,8,28,14,cwgbka,Facebook open sources fastText hyperparameter autotuning,https://www.reddit.com/r/MachineLearning/comments/cwgbka/facebook_open_sources_fasttext_hyperparameter/,aviniumau,1566969435,,0,1
1685,2019-8-28,2019,8,28,15,cwgtir,Hi! So its my first text on reddit :),https://www.reddit.com/r/MachineLearning/comments/cwgtir/hi_so_its_my_first_text_on_reddit/,MadMax2100,1566972879,[removed],0,1
1686,2019-8-28,2019,8,28,15,cwgxqq,Can anyone explain Adversarial Variational Bayes working procedure. I see some real negligence to this topic and really need to scrutinise my understanding of the paper on AVB.,https://www.reddit.com/r/MachineLearning/comments/cwgxqq/can_anyone_explain_adversarial_variational_bayes/,DashDeipayan,1566973748,[removed],0,1
1687,2019-8-28,2019,8,28,16,cwh7md,Help me choose a chatbot !!,https://www.reddit.com/r/MachineLearning/comments/cwh7md/help_me_choose_a_chatbot/,thealexin05,1566975760,[removed],0,1
1688,2019-8-28,2019,8,28,16,cwh8vw,Computing `q dot q` instead of `q dot k` when calculating scores for self-attention in Transformer,https://www.reddit.com/r/MachineLearning/comments/cwh8vw/computing_q_dot_q_instead_of_q_dot_k_when/,kingsiguk,1566976015,[removed],0,1
1689,2019-8-28,2019,8,28,16,cwh9hl,[D] Is learning label embedding by factorizing label co-occurrence matrix unsupervised learning?,https://www.reddit.com/r/MachineLearning/comments/cwh9hl/d_is_learning_label_embedding_by_factorizing/,atif_hassan,1566976147,"Hi all!

I was working on creating embeddings for medical concepts. These terms/phrases are used for annotating biomedical documents. Now usually the method of creating a co-occurrence matrix and then factorizing it to obtain dense, lower-dimensional vectors is termed as unsupervised learning since annotated data is not involved. I am using the same process but for the annotations themselves. Does this qualify as supervised learning since I need annotated data or does this qualify as unsupervised learning since the method of obtaining the embeddings is unsupervised?",3,4
1690,2019-8-28,2019,8,28,16,cwhcjp,[D] Computing `q dot q` instead of `q dot k` when calculating scores for self-attention in Transformer,https://www.reddit.com/r/MachineLearning/comments/cwhcjp/d_computing_q_dot_q_instead_of_q_dot_k_when/,kingsiguk,1566976733,"Going through the Transformer paper, and its implementation, I have had a question:

In the self-attention routine in the encoder, is it plausible to compute `q dot q` instead of `q dot k` when calculating scores for each input token?

I see that in the self-attention, the `memory_antecedent = query_antecedent` and q, k, v is computed (and trained) separately (c.f. [`compute_qkv` in T2T](https://github.com/tensorflow/tensor2tensor/blob/ffe4cf4051a8fc517daf7fde0c1b74dfc7a8eb19/tensor2tensor/layers/common_attention.py#L4365-L4366)).

Would utilizing the same `q` for the computation of scores (rather than having a separate `k`) seriously deteriorate the performance?",3,5
1691,2019-8-28,2019,8,28,16,cwhot3,Pandas trick for the day - Dataframe Align helps to synchronize two dataframes together,https://www.reddit.com/r/MachineLearning/comments/cwhot3/pandas_trick_for_the_day_dataframe_align_helps_to/,caroleber,1566979190,,0,1
1692,2019-8-28,2019,8,28,17,cwhrfq,Develop your own AI Chatbot with a team of right skill sets,https://www.reddit.com/r/MachineLearning/comments/cwhrfq/develop_your_own_ai_chatbot_with_a_team_of_right/,MachineLearning001,1566979759,,0,1
1693,2019-8-28,2019,8,28,17,cwhsmd,[P] Awesome Online Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cwhsmd/p_awesome_online_machine_learning/,Lemax0,1566980018,,0,1
1694,2019-8-28,2019,8,28,17,cwhzs9,"[P] Simple Tensorflow implementation of ""On the Variance of the Adaptive Learning Rate and Beyond"" (a.k.a. RAdam optimizer)",https://www.reddit.com/r/MachineLearning/comments/cwhzs9/p_simple_tensorflow_implementation_of_on_the/,taki0112,1566981648,[removed],1,1
1695,2019-8-28,2019,8,28,17,cwi01a,"[P] Simple Tensorflow implementation of ""On the Variance of the Adaptive Learning Rate and Beyond"" (a.k.a. RAdam optimizer)",https://www.reddit.com/r/MachineLearning/comments/cwi01a/p_simple_tensorflow_implementation_of_on_the/,taki0112,1566981700,[removed],0,1
1696,2019-8-28,2019,8,28,17,cwi0dx,"[P] Simple Tensorflow implementation of ""RAdam optimizer"" (On the Variance of the Adaptive Learning Rate and Beyond)",https://www.reddit.com/r/MachineLearning/comments/cwi0dx/p_simple_tensorflow_implementation_of_radam/,taki0112,1566981771,[removed],1,1
1697,2019-8-28,2019,8,28,18,cwi9hv,The MLOps NYC conference agenda is now online,https://www.reddit.com/r/MachineLearning/comments/cwi9hv/the_mlops_nyc_conference_agenda_is_now_online/,IguazioDani,1566983688,,0,1
1698,2019-8-28,2019,8,28,18,cwiar9,Is the implemention of the spectral normlization right?,https://www.reddit.com/r/MachineLearning/comments/cwiar9/is_the_implemention_of_the_spectral_normlization/,sjmdhr,1566983943,[removed],0,1
1699,2019-8-28,2019,8,28,18,cwieju,"[P] Simple Tensorflow implementation of ""RAdam optimizer"" (On the Variance of the Adaptive Learning Rate and Beyond)",https://www.reddit.com/r/MachineLearning/comments/cwieju/p_simple_tensorflow_implementation_of_radam/,taki0112,1566984738,,0,1
1700,2019-8-28,2019,8,28,18,cwifuf,How to create a Pelican blog with Jupyter Notebooks support on GitHub Pages (up to date in 2019),https://www.reddit.com/r/MachineLearning/comments/cwifuf/how_to_create_a_pelican_blog_with_jupyter/,anon101101101,1566984981,"Hey  guys, I just started with learning about data science recently and I  found creating a fully functional blog on it tough (there was like no  100% up to date info), so I've created an article detailing precisely  how to do it in the simplest way possible (without losing out on  anything big).  
So here it is, hopefully it helps someone out!",0,1
1701,2019-8-28,2019,8,28,18,cwiglm,"Evaluate Construction Site Safety on iOS using Machine Learning - Building an iOS application for safety on site with Swift, Turi Create, and Core ML",https://www.reddit.com/r/MachineLearning/comments/cwiglm/evaluate_construction_site_safety_on_ios_using/,omarmhaimdat,1566985125,[removed],0,1
1702,2019-8-28,2019,8,28,18,cwih3h,"""Seeking machine learning and pseudoscientific psychotherapy expert""",https://www.reddit.com/r/MachineLearning/comments/cwih3h/seeking_machine_learning_and_pseudoscientific/,TimPRJohnson,1566985240,"&amp;#x200B;

*Processing img 2lt2us0kt5j31...*",0,1
1703,2019-8-28,2019,8,28,19,cwj11x,Available Now: Open-Source Implementation of Hintons Matrix Capsules with EM Routing,https://www.reddit.com/r/MachineLearning/comments/cwj11x/available_now_opensource_implementation_of/,ibmzrl,1566988890,,0,1
1704,2019-8-28,2019,8,28,19,cwj3cq,Machine Learning Training in Pune,https://www.reddit.com/r/MachineLearning/comments/cwj3cq/machine_learning_training_in_pune/,dwivediabhinav,1566989307,[removed],0,1
1705,2019-8-28,2019,8,28,20,cwj7qz,[D] Design a network what combines supervised (CNN) and unsupervised (AE) for classification task,https://www.reddit.com/r/MachineLearning/comments/cwj7qz/d_design_a_network_what_combines_supervised_cnn/,brhrrr,1566990100,"Hello everyone! Working under one interesting problem, as you can read from post name, and wonder does anyone have ideas or hints for it? As we know autoencoders take input (in my case it's an image from the popular dataset) and reconstruct it as an output. Let's call input - node 1, output - node 3. It creates valuable features at its hidden layers (let's call it node 2)  during the process. Let's hypothesize, that if node 2 is used as input for CNN then the classification will be improved. My current ideas are:  
1 - For now, it sounds interesting and reasonable to try use output of the encoder - latent space representation as an input for following CNN. 

2 - Use one of the decoder layers as input for CNN.

A possible purpose of it - try to get more important futures from class imbalanced data. (As an example - from 5 classes 1 of them contain 50% fewer images than other). Let's discuss?",16,1
1706,2019-8-28,2019,8,28,20,cwj88a,Gradient descent in linear regression,https://www.reddit.com/r/MachineLearning/comments/cwj88a/gradient_descent_in_linear_regression/,manikantan2001,1566990166,[removed],0,1
1707,2019-8-28,2019,8,28,20,cwj90m,Sentiment Analysis of Tweets - Predicting Sentiments using Machine Learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/cwj90m/sentiment_analysis_of_tweets_predicting/,Shilpa_Opencodez,1566990295,,0,1
1708,2019-8-28,2019,8,28,20,cwjd39,"building a deep learning rig, any advice?",https://www.reddit.com/r/MachineLearning/comments/cwjd39/building_a_deep_learning_rig_any_advice/,gopnik_conscript,1566990998,"I have decided to try training models locally for a change and wan't to build a decent PC capable of deep learning. What are some of the best CPU's, GPU's, etc for dl that I should know about? Thanks.",0,1
1709,2019-8-28,2019,8,28,20,cwjdxm,How does YouTube's recommendation engine work?,https://www.reddit.com/r/MachineLearning/comments/cwjdxm/how_does_youtubes_recommendation_engine_work/,BeggarInSpain,1566991125,[removed],0,1
1710,2019-8-28,2019,8,28,20,cwjpqx,Has anybody here worked with the Berkeley Deep Drive dataset?,https://www.reddit.com/r/MachineLearning/comments/cwjpqx/has_anybody_here_worked_with_the_berkeley_deep/,zimmer550king,1566993088,[removed],0,1
1711,2019-8-28,2019,8,28,20,cwjqlu,Looking for the application of transformers in domains other than NLP,https://www.reddit.com/r/MachineLearning/comments/cwjqlu/looking_for_the_application_of_transformers_in/,AlleUndKalle,1566993221,[removed],0,1
1712,2019-8-28,2019,8,28,21,cwjwd0,Popular AI Tools Used in Postmodern ERP,https://www.reddit.com/r/MachineLearning/comments/cwjwd0/popular_ai_tools_used_in_postmodern_erp/,erp_oodles,1566994102,,0,1
1713,2019-8-28,2019,8,28,21,cwjyy8,Buffalo: A fast and scalable production-ready open source project for recommender systems,https://www.reddit.com/r/MachineLearning/comments/cwjyy8/buffalo_a_fast_and_scalable_productionready_open/,ummae,1566994500,,0,2
1714,2019-8-28,2019,8,28,21,cwk1gf,[D] Alternatives to Backpropagation,https://www.reddit.com/r/MachineLearning/comments/cwk1gf/d_alternatives_to_backpropagation/,Berdas_,1566994878,"As now it is widespread that backpropagation is not a biologically plausible approach, I would like to raise a discussion around alternatives for the method.   
In my mind, a cool idea would be to evaluate the outputs of each layer individually, i.e., what should we expect to see as output for the hidden layer number L? This would remove the need of backward sweeps (because a layer's 'accuracy' would depend only of itself) and make transfer learning a lot easier (cause if it's a layer-by-layer learning, we can put pieces together for similar task, with minor adjusments if necessary, e.g. the first layer of a CNN that identifies cats might be useful to identifying other felines).  
However, nothing comes to my mind as to how we could achieve that. Because, as I see, this would require us to have labels (or at least some representations for us to compare what we're getting to what we want) and I don't think labels are required when we humans learn (at least not *too* many labels).  
Anyway, I'd love to hear ideas from other minds, as I think this is the best way for us to come up with newer ideas.  
Cheers guys, have a good one :)",45,11
1715,2019-8-28,2019,8,28,21,cwk480,Not sure if this is the right place to post this.But shall I enroll into a data science course or computer science at undergraduate degree?,https://www.reddit.com/r/MachineLearning/comments/cwk480/not_sure_if_this_is_the_right_place_to_post/,[deleted],1566995271,,0,1
1716,2019-8-28,2019,8,28,21,cwk4a6,Tensorflow 2.0 implementations of Interpretability Methods,https://www.reddit.com/r/MachineLearning/comments/cwk4a6/tensorflow_20_implementations_of_interpretability/,raphaelmeudec,1566995284,,0,2
1717,2019-8-28,2019,8,28,21,cwk7bn,Why use dilated convolutions on 4x4x512 feature map?,https://www.reddit.com/r/MachineLearning/comments/cwk7bn/why_use_dilated_convolutions_on_4x4x512_feature/,Telcrome,1566995752,[removed],0,1
1718,2019-8-28,2019,8,28,22,cwl1ap,[R] A 2019 Guide to Speech Synthesis with Deep Learning,https://www.reddit.com/r/MachineLearning/comments/cwl1ap/r_a_2019_guide_to_speech_synthesis_with_deep/,mwitiderrick,1566999972,"In this research, I look at how deep learning has been used in voice generation. 

[https://heartbeat.fritz.ai/a-2019-guide-to-speech-synthesis-with-deep-learning-630afcafb9dd](https://heartbeat.fritz.ai/a-2019-guide-to-speech-synthesis-with-deep-learning-630afcafb9dd)",7,17
1719,2019-8-29,2019,8,29,0,cwm87b,"[R] DistilBERT: A smaller, faster, cheaper, lighter BERT trained with distillation!",https://www.reddit.com/r/MachineLearning/comments/cwm87b/r_distilbert_a_smaller_faster_cheaper_lighter/,jikkii,1567005552,"HuggingFace released their first NLP transformer model ""DistilBERT"", which is similar to the BERT architecture: only 66 million parameters (instead of 110 million) while keeping 95% of the performance on GLUE.

They released a [blogpost detailing the procedure with a hands-on](https://medium.com/huggingface/distilbert-8cf3380435b5).

It is also available on their repository [pytorch-transformers](https://github.com/huggingface/pytorch-transformers) alongside 7 other transformer models.",22,100
1720,2019-8-29,2019,8,29,0,cwm9op,[D] Do VAEs have a manifold?,https://www.reddit.com/r/MachineLearning/comments/cwm9op/d_do_vaes_have_a_manifold/,eigenlaplace,1567005732,"I am kind of confused as to how VAEs do manifold learning. 

&amp;#x200B;

While I can grasp that regular AEs perform deterministic transformation from the input vector space to the latent space with the encoder, it is very hard for me to understand how that would work on a VAE. Is the manifold on the parameters of the distribution MU and SIGMA? 

&amp;#x200B;

Can anyone clarify that for me, maybe point to a paper? Thanks",4,7
1721,2019-8-29,2019,8,29,0,cwmbco,[R] Google AI Blog: Exploring Weight Agnostic Neural Networks,https://www.reddit.com/r/MachineLearning/comments/cwmbco/r_google_ai_blog_exploring_weight_agnostic_neural/,Marha01,1567005958,"[Google AI Blog: Exploring Weight Agnostic Neural Networks](https://ai.googleblog.com/2019/08/exploring-weight-agnostic-neural.html)

&gt;In Weight Agnostic Neural Networks (WANN), we present a first step toward searching specifically for networks with these biases: neural net architectures that can already perform various tasks, even when they use a random shared weight. Our motivation in this work is to question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. By exploring such neural network architectures, we present agents that can already perform well in their environment without the need to learn weight parameters. Furthermore, in order to spur progress in this field community, we have also open-sourced the code to reproduce our WANN experiments for the broader research community.

&gt;We start with a population of minimal neural network architecture candidates, each with very few connections only, and use a well-established topology search algorithm (NEAT), to evolve the architectures by adding single connections and single nodes one by one.

https://weightagnostic.github.io/

Very interesting results from Google, using evolution-like approach to create network topologies. Thoughts?",46,251
1722,2019-8-29,2019,8,29,0,cwmo0k,"Simple Questions Thread August 28, 2019",https://www.reddit.com/r/MachineLearning/comments/cwmo0k/simple_questions_thread_august_28_2019/,AutoModerator,1567007571,[removed],0,1
1723,2019-8-29,2019,8,29,0,cwmqzb,Need open source-source to transcribe voice to text.,https://www.reddit.com/r/MachineLearning/comments/cwmqzb/need_open_sourcesource_to_transcribe_voice_to_text/,7FigQuant,1567007958,[removed],0,1
1724,2019-8-29,2019,8,29,1,cwn5kp,[P] Deepfakes Face swapping with any face without training,https://www.reddit.com/r/MachineLearning/comments/cwn5kp/p_deepfakes_face_swapping_with_any_face_without/,PuzzledProgrammer3,1567009735,"Generative adversarial networks integrating modules from FUNIT and SPADE for face-swapping

repo: [https://github.com/shaoanlu/fewshot-face-translation-GAN](https://github.com/shaoanlu/fewshot-face-translation-GAN)

tried it with elon musk on taylor swift

![video](upll8fkou7j31)",6,41
1725,2019-8-29,2019,8,29,1,cwnf80,"[xpost] I am Dr. Wil Koch, creator of the world's first neural network flight controller, Neuroflight, AMA!",https://www.reddit.com/r/MachineLearning/comments/cwnf80/xpost_i_am_dr_wil_koch_creator_of_the_worlds/,flowwiththechaos,1567010981,,0,1
1726,2019-8-29,2019,8,29,2,cwnusz,How To master Machine learning with Self Study without need to do masters &amp; PHD ?!,https://www.reddit.com/r/MachineLearning/comments/cwnusz/how_to_master_machine_learning_with_self_study/,moemen95,1567012895,"I'm asking to discuss what is the best tracks of study for machine learning, beginning from Simple linear algebra, prob &amp; stats to Advanced deep learning.",0,2
1727,2019-8-29,2019,8,29,2,cwnzmb,need help using 8 in 1 heat press machine,https://www.reddit.com/r/MachineLearning/comments/cwnzmb/need_help_using_8_in_1_heat_press_machine/,justvision1025,1567013493,[removed],0,1
1728,2019-8-29,2019,8,29,2,cwo6a6,Is there a deep learning algorithm or method that predicts hierarchical data similar to how Recurrent Neural Networks predict normal sequences?,https://www.reddit.com/r/MachineLearning/comments/cwo6a6/is_there_a_deep_learning_algorithm_or_method_that/,abcKD,1567014307,,0,1
1729,2019-8-29,2019,8,29,3,cwomtx,Which are the must read probability and statistic books or papers that's worth to look at?,https://www.reddit.com/r/MachineLearning/comments/cwomtx/which_are_the_must_read_probability_and_statistic/,anshul_negi,1567016363,[removed],0,1
1730,2019-8-29,2019,8,29,3,cwp1m5,Data Annotation: The Billion Dollar Business Behind AI Breakthroughs,https://www.reddit.com/r/MachineLearning/comments/cwp1m5/data_annotation_the_billion_dollar_business/,Yuqing7,1567018232,,0,1
1731,2019-8-29,2019,8,29,4,cwpszr,Welding a tow ball on the excavator to pull a trailer // splitting firewood,https://www.reddit.com/r/MachineLearning/comments/cwpszr/welding_a_tow_ball_on_the_excavator_to_pull_a/,tenberghe,1567021650,,0,1
1732,2019-8-29,2019,8,29,4,cwpxuq,"[D] Eric Drexler's ""Reframing Superintelligence""",https://www.reddit.com/r/MachineLearning/comments/cwpxuq/d_eric_drexlers_reframing_superintelligence/,regalalgorithm,1567022246,"Following the Slate Star Codex [review](https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/) of  ""Reframing Superintelligence"" I (as an AI researcher) have become pretty excited to see such a comprehensive reply exists to Bostrom-type ""paperclip maximizer"" fears of AGI. A good summary here -  [Less Like Us: An Alternate Theory of Artificial General Intelligence](https://singularityhub.com/2019/06/02/less-like-us-an-alternate-theory-of-artificial-general-intelligence/)  \- basically the idea is that realistically AI is not developed with the ability to self improve and do whatever it wants, so we should not fear AGIs that get out of control in this way. 

What do you think of this reply to AGI concerns? Certainly given present day AI and how it is developing, the ""service ai' seems like a cogent prediction of what we can actually say is likely to come about and we need to be wary of doing wrong.",12,3
1733,2019-8-29,2019,8,29,5,cwpzb5,[R] Evolving Space-Time Neural Architectures for Videos (Google Brain) ICCV,https://www.reddit.com/r/MachineLearning/comments/cwpzb5/r_evolving_spacetime_neural_architectures_for/,Himalun,1567022412,"Paper: https://arxiv.org/abs/1811.10636


Code: https://github.com/piergiaj/evanet-iccv19


Abstract:


We present a new method for finding video CNN architectures that capture rich spatio-temporal information in videos. Previous work, taking advantage of 3D convolutions, obtained promising results by manually designing video CNN architectures. We here develop a novel evolutionary search algorithm that automatically explores models with different types and combinations of layers to jointly learn interactions between spatial and temporal aspects of video representations. We demonstrate the generality of this algorithm by applying it to two meta-architectures, obtaining new architectures superior to manually designed architectures. Further, we propose a new component, the iTGM layer, which more efficiently utilizes its parameters to allow learning of space-time interactions over longer time horizons. The iTGM layer is often preferred by the evolutionary algorithm and allows building cost-efficient networks. The proposed approach discovers new and diverse video architectures that were previously unknown. More importantly they are both more accurate and faster than prior models, and outperform the state-of-the-art results on multiple datasets we test, including HMDB, Kinetics, and Moments in Time. We will open source the code and models, to encourage future model development.",0,1
1734,2019-8-29,2019,8,29,5,cwpzq1,GPT-2 774M Large Model (guide and discussion),https://www.reddit.com/r/MachineLearning/comments/cwpzq1/gpt2_774m_large_model_guide_and_discussion/,Jrowe47,1567022470,[removed],0,1
1735,2019-8-29,2019,8,29,7,cwrly5,"[D] Is ""Wasserstein metric"" the right name to use?",https://www.reddit.com/r/MachineLearning/comments/cwrly5/d_is_wasserstein_metric_the_right_name_to_use/,746645147,1567029742,"According to wiki: "" The name ""Wasserstein distance"" was coined by [R. L. Dobrushin](https://www.wikiwand.com/en/Roland_Dobrushin) in 1970, after the [Russian](https://www.wikiwand.com/en/Russia) [mathematician](https://www.wikiwand.com/en/Mathematician) [Leonid Vaserten](https://www.wikiwand.com/en/Leonid_Vaser%C5%A1te%C4%ADn) who introduced the concept in 1969. "". And indeed, I found the [paper](http://www.mathnet.ru/links/b81df875a976971bc39a291f7675e5de/tvp1856.pdf) written in Russian by Dobrushin, which mentioned in reference:""  ... ,               ,                 .    .    ,     .     5,  3   (1969),   6473.    "", and Leonid Vaserten is just english for  .

Although I could not read Russian and I could not find the content of the original papr by Leonid Vaserten, the wiki still seems convincing. 

However, it seems Frchet distance is identical to 2-Wasserstein distance, and Frchet distance was introduced in 1957, according to the original French paper ""[Sur la distance de deux lois de probabilit.](https://www2.sonycsl.co.jp/person/nielsen/infogeo/Seminar/Frechet-Fondamental-Distance-Wasserstein.pdf)""  


Does it means Frchet discovered it first and wiki is wrong about the origin? What's more, should we call it Frchet distance instead of Wasserstein distance?

&amp;#x200B;

P.S.

If you search ""Frchet distance"" on google, what comes out is not a distance for distribution but distance for path. I am confused by the relationship between ""Frchet distance of path"" with ""Frchet distance of distribution"".",12,16
1736,2019-8-29,2019,8,29,7,cwru7z,[N] Deep Graph Library new release (v0.3.1),https://www.reddit.com/r/MachineLearning/comments/cwru7z/n_deep_graph_library_new_release_v031/,jermainewang,1567030765,"Though only a minor release, this new release includes a bunch of very useful Graph Neural Network modules and model examples that can be directly used in your project. Here is a list of new modules:

## New NN Modules

* GATConv from [Graph Attention Network](https://arxiv.org/pdf/1710.10903.pdf)
* RelGraphConv from [Modeling Relational Data with Graph Convolutional Networks](https://arxiv.org/abs/1703.06103)
* TAGConv from [Topology Adaptive Graph Convolutional Networks](https://arxiv.org/pdf/1710.10370.pdf)
* EdgeConv from [Dynamic Graph CNN for Learning on Point Clouds](https://arxiv.org/pdf/1801.07829)
* SAGEConv from [Inductive Representation Learning on Large Graphs](https://arxiv.org/pdf/1706.02216.pdf)
* GatedGraphConv from [Gated Graph Sequence Neural Networks](https://arxiv.org/pdf/1511.05493.pdf)
* GMMConv from [Geometric Deep Learning on Graphs and Manifolds using Mixture Model CNNs](http://openaccess.thecvf.com/content_cvpr_2017/papers/Monti_Geometric_Deep_Learning_CVPR_2017_paper.pdf)
* GINConv from [How Powerful are Graph Neural Networks?](https://arxiv.org/pdf/1810.00826.pdf)
* ChebConv from [Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering](https://arxiv.org/pdf/1606.09375.pdf)
* SGConv from [Simplifying Graph Convolutional Networks](https://arxiv.org/pdf/1902.07153.pdf)
* NNConv from [Neural Message Passing for Quantum Chemistry](https://arxiv.org/pdf/1704.01212.pdf)
* APPNPConv from [Predict then Propagate: Graph Neural Networks meet Personalized PageRank](https://arxiv.org/pdf/1810.05997.pdf)
* AGNNConv from [Attention-based Graph Neural Network for Semi-Supervised Learning](https://arxiv.org/abs/1803.03735)
* DenseGraphConv (Dense implementation of GraphConv)
* DenseSAGEConv (Dense implementation of SAGEConv)
* DenseChebConv (Dense implementation of ChebConv)

## New global pooling module

* Sum/Avg/MaxPooling
* SortPooling
* GlobalAttentionPooling from GGNN model
* Set2Set from [Order Matters: Sequence to sequence for sets](https://arxiv.org/pdf/1511.06391.pdf)
* SetTransformerEncoder and SetTransformerDecoder from [Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks](https://arxiv.org/pdf/1810.00825.pdf)

## New graph transformation routines

* dgl.transform.khop\_adj
* dgl.transform.khop\_graph
* dgl.transform.laplacian\_lambda\_max
* dgl.transform.knn\_graph
* dgl.transform.segmented\_knn\_graph

This DGL release also includes a model zoo for chemistry applications such as using GNNs to predict molecular property or generate new molecule structures that is valuable for drug discovery. Pre-trained models are also available for download in simply two lines of codes:

```python
from dgl.data import Tox21
from dgl import model_zoo

dataset = Tox21()
model = model_zoo.chem.load_pretrained('GCN_Tox21') # Pretrained model loaded
model.eval()

smiles, g, label, mask = dataset[0]
feats = g.ndata.pop('h')
label_pred = model(g, feats)
print(smiles)                   # CCOc1ccc2nc(S(N)(=O)=O)sc2c1
print(label_pred[:, mask != 0]) # Mask non-existing labels
# tensor([[-0.7956,  0.4054,  0.4288, -0.5565, -0.0911,  
# 0.9981, -0.1663,  0.2311, -0.2376,  0.9196]])
```

Check it out if you are using GNNs, working with molecules or just interested in this whole new field.

See full release note here: [https://www.dgl.ai/release/2019/08/28/release.html](https://www.dgl.ai/release/2019/08/28/release.html).",9,29
1737,2019-8-29,2019,8,29,8,cwsz9j,Question: How to do LSH properly on Word2Vec Spark MLLib,https://www.reddit.com/r/MachineLearning/comments/cwsz9j/question_how_to_do_lsh_properly_on_word2vec_spark/,fazz21,1567036380,[removed],0,1
1738,2019-8-29,2019,8,29,9,cwtah1,"""Many Sets of Model Parameters Fit the Data Well""",https://www.reddit.com/r/MachineLearning/comments/cwtah1/many_sets_of_model_parameters_fit_the_data_well/,arjundupa,1567038030,[removed],0,1
1739,2019-8-29,2019,8,29,9,cwtmb9,[P] Test,https://www.reddit.com/r/MachineLearning/comments/cwtmb9/p_test/,taki0112,1567039704,,0,1
1740,2019-8-29,2019,8,29,9,cwtomp,[P] Tensorflow implementation of RAdam optimizer (On the Variance of the Adaptive Learning Rate and Beyond),https://www.reddit.com/r/MachineLearning/comments/cwtomp/p_tensorflow_implementation_of_radam_optimizer_on/,taki0112,1567040030,"&amp;#x200B;

[result](https://i.redd.it/08xlqt9zcaj31.png)",7,24
1741,2019-8-29,2019,8,29,11,cwunn2,Would this add any value?,https://www.reddit.com/r/MachineLearning/comments/cwunn2/would_this_add_any_value/,techsensus,1567045130,[removed],0,1
1742,2019-8-29,2019,8,29,11,cwuscu,Looking for pictures of the different eevee evolutions for a tensorflow project.,https://www.reddit.com/r/MachineLearning/comments/cwuscu/looking_for_pictures_of_the_different_eevee/,HopingToGoToCarnegie,1567045859,[removed],0,1
1743,2019-8-29,2019,8,29,12,cwvbdx,Pruning,https://www.reddit.com/r/MachineLearning/comments/cwvbdx/pruning/,arjundupa,1567048789,"What are some ways to know that pruning might be possible on your architecture (given your results, train/val loss curve, etc.)? Is there any literature out there that tries to answer this question?",0,1
1744,2019-8-29,2019,8,29,13,cwvwpl,Microsoft's best practices on NLP,https://www.reddit.com/r/MachineLearning/comments/cwvwpl/microsofts_best_practices_on_nlp/,headphones_bulldog,1567052350,,0,1
1745,2019-8-29,2019,8,29,14,cwwg9g,Machine Learning Training in Pune,https://www.reddit.com/r/MachineLearning/comments/cwwg9g/machine_learning_training_in_pune/,dwivediabhinav,1567055887,"&amp;#x200B;

![video](70xndcj3obj31)

ExcelR Solutions is the best Institute for Machine Learning Training in Pune,Basic requirements for Machine Learning Training is Computer skills,Basic Mathematics Knowledge,Basic Data Science Concepts with much Experienced Staff,they are providing certifications from the University of Malaysia.ExcelR is a global leader in technical and management training catering the training needs of the professionals in more than 27 countries with offices in USA, Malaysia, India etc. with over 21 branches across the globe.

[https://www.excelr.com/machine-learning-course-training-in-pune/](https://www.excelr.com/machine-learning-course-training-in-pune/)",0,1
1746,2019-8-29,2019,8,29,14,cwwj65,HyperparameterHunter - Tuning of Models in Python,https://www.reddit.com/r/MachineLearning/comments/cwwj65/hyperparameterhunter_tuning_of_models_in_python/,WorthApricot,1567056456,[removed],0,1
1747,2019-8-29,2019,8,29,14,cwwq3c,Applying a filter over RGB image or a feature map with d=32 (for example),https://www.reddit.com/r/MachineLearning/comments/cwwq3c/applying_a_filter_over_rgb_image_or_a_feature_map/,avgMLenthusiast,1567057768,[removed],1,1
1748,2019-8-29,2019,8,29,14,cwwrrp,Why do people integrate Spark with TensorFlow even if there is a distributed TensorFlow framework.,https://www.reddit.com/r/MachineLearning/comments/cwwrrp/why_do_people_integrate_spark_with_tensorflow/,alvaro_advent,1567058103,[removed],0,1
1749,2019-8-29,2019,8,29,15,cwx0qf,People Tracking with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cwx0qf/people_tracking_with_machine_learning/,bugggster,1567059879,,0,1
1750,2019-8-29,2019,8,29,16,cwxca7,A Subreddit with Awesome Data Science Projects,https://www.reddit.com/r/MachineLearning/comments/cwxca7/a_subreddit_with_awesome_data_science_projects/,OppositeMidnight,1567062193,,0,1
1751,2019-8-29,2019,8,29,16,cwxcvm,Start to buy only yummy watermelons!,https://www.reddit.com/r/MachineLearning/comments/cwxcvm/start_to_buy_only_yummy_watermelons/,ideas_for_business,1567062322,[removed],1,1
1752,2019-8-29,2019,8,29,16,cwxedd,[P] A Subreddit with Awesome Data Science Projects,https://www.reddit.com/r/MachineLearning/comments/cwxedd/p_a_subreddit_with_awesome_data_science_projects/,OppositeMidnight,1567062608,,0,1
1753,2019-8-29,2019,8,29,16,cwxi42,"Essentials tools for Machine Learning &amp; AI | Introduction To Matplotlib, Bar Plots, Box Plots, Subplots and Plotting with Real Data",https://www.reddit.com/r/MachineLearning/comments/cwxi42/essentials_tools_for_machine_learning_ai/,aasthaarora,1567063333,"Two of the most popular technological trends of this age are AI and machine learning. It is because we have just started to scratch the surface of AI and machine learning and the results have been stunning. Also, with many companies investing heavily in AI and machine learning, the trend is only expected soar through the roof in the coming years.

&gt;**E-degree course:**  [Artificial Intelligence and Machine Learning E-Degree](https://www.eduonix.com/ai-machine-learning-edegree?utm_source=social&amp;utm_medium=reddit&amp;utm_campaign=asha)

So, for an AI and machine learning enthusiast like me, I wanted to learn more about these two fascinating concepts. One of the subsets of learning AI and machine learning are to know some of its essential tools. One of these tools is Matplotlib, which is a Python 2D plotting library. It produces publication quality figures in a plethora of hard-copy formats as well as interactive environments across platforms. It is used in web application servers, Python scripts, the Jupyter notebook, the Python and IPython shells, web application servers, and four graphical user interface toolkits.

While there are many courses that promise to teach about Matplotlib, most of them are hard to grasp. So, after a lot of glancing over other courses, I found this online tutorial. The course explains the essentials of Matplotlib with all the terminologies and also giving a lot of examples which made me understand the concept much easily compared to other tutorials. The tutorial also goes onto explain some of the most vital sections of Matplotlib such as bar plots, box plots and subplotting. I learned how to use boxplot in Matplotlib and also make a timeseries plot along with many other features of it.

Furthermore, I was able to build a graph from a real data set along with learning many other statistical tricks.

The tutor teaching the course is an expert on machine learning and the tutorial will also gave me several examples which helped me understand the concepts in a better way. Upon the completion of the course, I was also awarded with a certificate of completion.I have also found a short link to the video that i really recommend to watch so that you can have a better undertsanding of the course!",0,1
1754,2019-8-29,2019,8,29,16,cwxooe,How AI and Chatbots are Enriching Mobile Apps,https://www.reddit.com/r/MachineLearning/comments/cwxooe/how_ai_and_chatbots_are_enriching_mobile_apps/,MachineLearning001,1567064672,,0,1
1755,2019-8-29,2019,8,29,17,cwxvuf,[P] Buffalo: A fast and scalable production-ready open source project for recommender systems,https://www.reddit.com/r/MachineLearning/comments/cwxvuf/p_buffalo_a_fast_and_scalable_productionready/,cyberbot97,1567066210,,0,1
1756,2019-8-29,2019,8,29,17,cwxxgk,Curated open-source AutoML &amp; model optimization tools ,https://www.reddit.com/r/MachineLearning/comments/cwxxgk/curated_opensource_automl_model_optimization_tools/,Fartin_dog,1567066583,,0,1
1757,2019-8-29,2019,8,29,17,cwy81d,[D] Research shows too large of a mini batch can lead to huge overfitting. Why doesn't batch gradient descent have this problem?,https://www.reddit.com/r/MachineLearning/comments/cwy81d/d_research_shows_too_large_of_a_mini_batch_can/,DstnB3,1567068871,"Here is an example paper showing test accuracy getting very bad as batch size gets too large: https://arxiv.org/pdf/1804.07612.pdf


But batch gradient descent runs over the whole dataset. Why doesn't it massively overfit like a SGD with too large of a batch?",0,1
1758,2019-8-29,2019,8,29,17,cwy89o,[D] Research shows SGD with too large of a mini batch can lead to huge overfitting. Why doesn't batch gradient descent have this problem?,https://www.reddit.com/r/MachineLearning/comments/cwy89o/d_research_shows_sgd_with_too_large_of_a_mini/,DstnB3,1567068917,"Here is an example paper showing test accuracy getting very bad as batch size gets too large: https://arxiv.org/pdf/1804.07612.pdf



But batch gradient descent runs over the whole dataset. Why doesn't it massively overfit like a SGD with too large of a batch?",0,1
1759,2019-8-29,2019,8,29,17,cwy8vo,[D] Research shows SGD with too large of a mini batch can lead to huge overfitting in deep learning. Why doesn't batch gradient descent have this problem?,https://www.reddit.com/r/MachineLearning/comments/cwy8vo/d_research_shows_sgd_with_too_large_of_a_mini/,DstnB3,1567069046,"Here is an example paper showing test accuracy getting very bad as batch size gets too large: https://arxiv.org/pdf/1804.07612.pdf

But batch gradient descent runs over the whole dataset. Why doesn't it massively overfit like a SGD with too large of a batch? Or does it?",54,101
1760,2019-8-29,2019,8,29,18,cwygh1,ML Datasets,https://www.reddit.com/r/MachineLearning/comments/cwygh1/ml_datasets/,np497,1567070554,[removed],0,1
1761,2019-8-29,2019,8,29,18,cwyh33,[D] Inter-annotator agreement: how does it work for computer vision?,https://www.reddit.com/r/MachineLearning/comments/cwyh33/d_interannotator_agreement_how_does_it_work_for/,arkady_red,1567070701,"We have a dataset which we need to annotate: the task is object detection, thus we need to create bounding boxes. We're going to use

https://github.com/wkentaro/labelme

But I'mm open to alternative suggestions, if you think there are better tools. Since the dataset is very large and very confidential, we're going to annotate it in-house. I've heard of people trying to estimate the error due to subjectivity/mistakes in human annotation, but I don't quite understand how it works. Let's suppose for the sake of example that I have 900 images and 3 annotators. If I understand correctly, rather than partitioning the dataset in three subsets of size 300 and sending each subset to a different annotator, I divide it in three datasets of size, say, 330, which means that some images will necessarily be annotated by multiple users.

I don't understand how to use these multiple annotations in practice, though: when I prepare my dataset, for each image which has been annotated by multiple users  I'll have to choose which annotations to use. It's not like I can have three different bounding boxes (three different ground truths) for each object in the image. So, how does it work in practice?",16,1
1762,2019-8-29,2019,8,29,18,cwyjpb,Felt matured...,https://www.reddit.com/r/MachineLearning/comments/cwyjpb/felt_matured/,sanket_2000,1567071250,,0,1
1763,2019-8-29,2019,8,29,18,cwyp0w,Global Hydrogen Generation Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/cwyp0w/global_hydrogen_generation_market_report_2019/,jadhavni3,1567072314,[removed],1,1
1764,2019-8-29,2019,8,29,20,cwzegp,Benefits and Applications of Blockchain &amp; Machine Learning when combined,https://www.reddit.com/r/MachineLearning/comments/cwzegp/benefits_and_applications_of_blockchain_machine/,cwadamsmith,1567077011,,0,1
1765,2019-8-29,2019,8,29,20,cwzewl,Global Ocean Freight Forwarding Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/cwzewl/global_ocean_freight_forwarding_market_report_2019/,jadhavni3,1567077093,[removed],1,1
1766,2019-8-29,2019,8,29,21,cx00vg,[D] Vehicle damage inspection using AI?,https://www.reddit.com/r/MachineLearning/comments/cx00vg/d_vehicle_damage_inspection_using_ai/,cbsudux,1567080603,"What are the most impactful startups working on this? And any papers?

Thanks!",7,0
1767,2019-8-29,2019,8,29,21,cx0exc,White Paper - Accelerating Advanced Analytics in an Immature Analytics Culture,https://www.reddit.com/r/MachineLearning/comments/cx0exc/white_paper_accelerating_advanced_analytics_in_an/,ElegantMicroWebIndia,1567082676,,0,1
1768,2019-8-29,2019,8,29,22,cx0qob,What is the current state of Neural Ordinary Differential Equations?,https://www.reddit.com/r/MachineLearning/comments/cx0qob/what_is_the_current_state_of_neural_ordinary/,al10101,1567084341,[removed],0,1
1769,2019-8-29,2019,8,29,22,cx0zxy,[D] Specific tips on Machine Learning research in a PhD,https://www.reddit.com/r/MachineLearning/comments/cx0zxy/d_specific_tips_on_machine_learning_research_in_a/,schrowawey,1567085625,"I am a new Machine Learning PhD and my topic is roughly vision, i.e. semantic/instance segmentation, and to be honest I am a little lost.

How exactly, specifically do you conduct research in this field? How does the day to day work look like?

* Do you think of new NN architectures and test them experimentally?
* Do you download others models and just try them out with own datasets?
* How do you keep track on different architectures, papers, etc. Maybe make an excel document with all the papers you've read with a short summary?

I would be really interested in how the day to day work of other researchers in the field looks like and what specific tips you might have.",47,129
1770,2019-8-29,2019,8,29,23,cx1hux,What is the best python package for chat bots????,https://www.reddit.com/r/MachineLearning/comments/cx1hux/what_is_the_best_python_package_for_chat_bots/,sreevatsansridh,1567088068,[removed],0,1
1771,2019-8-29,2019,8,29,23,cx1ohx,How to Make Neural Language Models Practical for Speech Recognition,https://www.reddit.com/r/MachineLearning/comments/cx1ohx/how_to_make_neural_language_models_practical_for/,georgecarlyle76,1567088958,,0,1
1772,2019-8-29,2019,8,29,23,cx1pma,How to use CornerstoneJS and Orthanc to support Deep Learning projects,https://www.reddit.com/r/MachineLearning/comments/cx1pma/how_to_use_cornerstonejs_and_orthanc_to_support/,FMCalisto,1567089119,[removed],0,1
1773,2019-8-29,2019,8,29,23,cx20lb,[P] Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/cx20lb/p_deep_reinforcement_learning/,gregory_k,1567090570,,0,1
1774,2019-8-30,2019,8,30,0,cx29g1,folks. decision tree vs. linear regression,https://www.reddit.com/r/MachineLearning/comments/cx29g1/folks_decision_tree_vs_linear_regression/,marksteve4,1567091695,[removed],0,1
1775,2019-8-30,2019,8,30,0,cx29y1,[D] Are conferences interested in papers that introduce new datasets?,https://www.reddit.com/r/MachineLearning/comments/cx29y1/d_are_conferences_interested_in_papers_that/,searchingundergrad,1567091760,"I've been working with a new dataset and running standard models on it. Would any conferences be interested in a paper introducing this NLP dataset detailing what's special about it, results of current sota methods?",13,1
1776,2019-8-30,2019,8,30,0,cx2b4w,Yoshua Bengio at RIIAA 19 - Mexico City,https://www.reddit.com/r/MachineLearning/comments/cx2b4w/yoshua_bengio_at_riiaa_19_mexico_city/,Kunatax,1567091911,,0,1
1777,2019-8-30,2019,8,30,0,cx2gt1,[Research] Conditional LSTM-GAN for Melody Generation from Lyrics,https://www.reddit.com/r/MachineLearning/comments/cx2gt1/research_conditional_lstmgan_for_melody/,enverx,1567092638,,2,1
1778,2019-8-30,2019,8,30,0,cx2msl,[D] BatchNorm alternatives 2019,https://www.reddit.com/r/MachineLearning/comments/cx2msl/d_batchnorm_alternatives_2019/,tsauri,1567093411,"The main reason why people BatchNorm despite being compute hungry (~25% of total model) is because of fast official cudnn implementations. Same reason why RNNs other than LSTM and GRU never went popular.

Anyway, are there any new methods that can dethrone BatchNorm entirely? 

Some papers:
Equinormalization
https://openreview.net/forum?id=r1gEqiC9FX

Generalized Hamming Network https://arxiv.org/abs/1710.10328",25,60
1779,2019-8-30,2019,8,30,1,cx34vc,[Project] I made a voice based control interface to play video games and control your PC using voice commands from your mic,https://www.reddit.com/r/MachineLearning/comments/cx34vc/project_i_made_a_voice_based_control_interface_to/,Andohuman,1567095670,"The project works by employing a siamese network to predict what you say and performs the respective action. Unlike traditional neural networks, this method does not require a large dataset to train on, powerful hardware resources or a long training time. You can get this up and running typically in 10-15 minutes and you do not need a GPU to run this program.

Github link :- [https://github.com/andohuman/Shadowcol](https://github.com/andohuman/Shadowcol)

I've provided very detailed installation instructions in the repo and instructions on how to train your own model with your own commands to aid your gaming experience.

I'd also like to mention that this project is still under development and there might be a few bugs. Please **DO NOT ATTEMPT TO PLAY ANY COMPETITIVE GAMES WITH THIS** (Had to learn this the hard way after dying a couple of times in overwatch).

The typical use case I see for this is mapping hard-to-reach keys to your voice instead. Or use your voice commands to open/close applications in your PC. I'll leave you to your imagination.

Play with the code and let me know what you guys think, and if you have any suggestions I'd be glad to hear it.

Special thanks to u/chaosparrot and u/jonnor for their insights !",4,48
1780,2019-8-30,2019,8,30,1,cx3ld7,Applying for PhD programs for next Fall. Should I give up?,https://www.reddit.com/r/MachineLearning/comments/cx3ld7/applying_for_phd_programs_for_next_fall_should_i/,Climb2K2,1567097784,[removed],0,1
1781,2019-8-30,2019,8,30,2,cx3q7p,The cloud machine learning workflow,https://www.reddit.com/r/MachineLearning/comments/cx3q7p/the_cloud_machine_learning_workflow/,2wolfy2,1567098373,,1,1
1782,2019-8-30,2019,8,30,2,cx49kg,NLP recipes using BERT,https://www.reddit.com/r/MachineLearning/comments/cx49kg/nlp_recipes_using_bert/,sharatsc,1567100819,[removed],0,1
1783,2019-8-30,2019,8,30,3,cx4os0,Under what exactly conditions is a 3 layer MLP a universal approximator?,https://www.reddit.com/r/MachineLearning/comments/cx4os0/under_what_exactly_conditions_is_a_3_layer_mlp_a/,isomorphiclambda,1567102739,[removed],0,1
1784,2019-8-30,2019,8,30,3,cx577p,AI Creates Fashion Models With Custom Outfits and Poses,https://www.reddit.com/r/MachineLearning/comments/cx577p/ai_creates_fashion_models_with_custom_outfits_and/,Yuqing7,1567105064,,0,1
1785,2019-8-30,2019,8,30,4,cx5etv,[Project] Senior Capstone Project within Unity on Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cx5etv/project_senior_capstone_project_within_unity_on/,Marear,1567106035,"Hey everyone,

I'm a Computer Science(BS) senior looking for ideas for my capstone project that gives care to the scope of a semester (until about early December, 15ish weeks) . The thing about this capstone is that while it needs to be finished by that time, if there is significant room for expansion, it can be improved upon in my 2nd semester capstone. Other relevant information is that I'm going to be working on a video game throughout the semester for a separate class (nothing complex due to time). Requirements for the capstone are pretty relaxed so long as the project exists for a clear purpose.

&amp;#x200B;

Knowing that, do you guys think that within a semester it is feasible to design a manageable project which would then still be relevant (for a 2nd semester continuation) to a small game I can make alongside the capstone?

&amp;#x200B;

I'd appreciate any feedback on your minds! Thanks",8,0
1786,2019-8-30,2019,8,30,4,cx5fu9,GitHub - modern-fortran/neural-fortran: A parallel neural net microframework,https://www.reddit.com/r/MachineLearning/comments/cx5fu9/github_modernfortranneuralfortran_a_parallel/,Smith4242,1567106173,,0,1
1787,2019-8-30,2019,8,30,4,cx5fzs,How to monitor gradient flow in tensorflow 2.0?,https://www.reddit.com/r/MachineLearning/comments/cx5fzs/how_to_monitor_gradient_flow_in_tensorflow_20/,mutatedmonkeygenes,1567106193,[removed],0,1
1788,2019-8-30,2019,8,30,4,cx5grd,[1902.06714] A parallel Fortran framework for neural networks and deep learning,https://www.reddit.com/r/MachineLearning/comments/cx5grd/190206714_a_parallel_fortran_framework_for_neural/,Smith4242,1567106295,,99,128
1789,2019-8-30,2019,8,30,4,cx5i0j,[P] Looking for Research Collaborators!,https://www.reddit.com/r/MachineLearning/comments/cx5i0j/p_looking_for_research_collaborators/,Ash3nBlue,1567106447,"I posted here a few weeks back looking for members to join me in putting together an AI research organization. As of today, we've got the groundwork laid for another round of recruitment, so here we go again!

&amp;#x200B;

We currently have six figures of compute funding and several deep learning experts (as well as domain experts like scientists, doctors, executives, and engineers) on our team. Our goal is to work together to do what we otherwise couldn't alone, and create opportunities for people of all backgrounds to work on research that they are passionate about while publishing work that is valuable to the world. We can provide technical and moral support as well as huge amounts of compute for research projects.

&amp;#x200B;

If you're interested in joining, shoot me a message with your background/experience as well as what you're interested in working on. We're open to all topics, but we're especially interested in ideas that can readily be implemented into projects with potential for substantial progress within a 1-2 month time frame. There are no qualifications; we're just looking for people who are passionate about AI and are willing to put in work to publish research and build technology that is valuable to the world. We're also looking for directors and mentors!

&amp;#x200B;

We look forward to hearing from all of you!",6,0
1790,2019-8-30,2019,8,30,4,cx5k2i,[D] Slow pytorch distributed training,https://www.reddit.com/r/MachineLearning/comments/cx5k2i/d_slow_pytorch_distributed_training/,borislestsov,1567106698,"My network is 1 Gbit ethernet and i am trying to use pytorch distributed training on two 8-gpu servers. Training procedure is simple classification objective with feed-forward network. I experience significant slowdown in comparison with single 8-gpu server training. Also ""nload"" tool shows full bandwidth usage even for small model (resnet18).

Is my network too slow for distributed training? If it is, what bandwidth (in Gbit/s) do I need to train heavy models like resnet101?",6,5
1791,2019-8-30,2019,8,30,4,cx5w5i,How to think like a programmer,https://www.reddit.com/r/MachineLearning/comments/cx5w5i/how_to_think_like_a_programmer/,susanvilleula1,1567108199,,0,1
1792,2019-8-30,2019,8,30,5,cx6bg6,[P] 12 Activation Functions and When to Use Them,https://www.reddit.com/r/MachineLearning/comments/cx6bg6/p_12_activation_functions_and_when_to_use_them/,chsjr,1567110074,[removed],0,1
1793,2019-8-30,2019,8,30,5,cx6e1v,Adversarial Patch on Hat Fools SOTA Facial Recognition,https://www.reddit.com/r/MachineLearning/comments/cx6e1v/adversarial_patch_on_hat_fools_sota_facial/,Yuqing7,1567110419,,0,1
1794,2019-8-30,2019,8,30,6,cx789w,"[D] How do you combine two learned models trained on partitioned data of the same distribution, to a single model?",https://www.reddit.com/r/MachineLearning/comments/cx789w/d_how_do_you_combine_two_learned_models_trained/,MasterInternet,1567114143,"If we have a single large dataset D, and partitioned into A and B and put on devices. 

I make two replicas of M^1 and M^2 from a same model M and also with same initial weights(same seed). I put these on the above two devices and train them separately. 

How do I combine(if its the word) these two models experience into a single one(like a model which has learned the entire data D)?",9,11
1795,2019-8-30,2019,8,30,6,cx78lx,[P] Multipart Tutorial on Graph Neural Networks for Computer Vision and Beyond with PyTorch examples,https://www.reddit.com/r/MachineLearning/comments/cx78lx/p_multipart_tutorial_on_graph_neural_networks_for/,bknyazev,1567114190,"I published a multipart ""**Tutorial on Graph Neural Networks for Computer Vision and Beyond**"" starting from some basics \[1\], then an overview explaining several important methods \[2\] and a separate post on spectral convolution \[3\].

I know there are a lot of blog posts on graph networks already, but in my tutorial I tried to explain key (and sometimes complicated) ideas in very simple terms from a **computer vision perspective**, so it should be good for those with a computer vision and machine learning background. I provide detailed Python and PyTorch examples to clarify differences between methods. 

I wasn't sure if to publish it here due to this discussion (Regarding beginner's guides: [https://www.reddit.com/r/MachineLearning/comments/co37ut/regarding\_beginners\_guides/](https://www.reddit.com/r/MachineLearning/comments/co37ut/regarding_beginners_guides/) ), but hopefully it will be appreciated here. Otherwise, feel free to downvote or remove.

Any questions or feedback is very welcome, especially, if you notice some mistakes or confusing info.

\[1\] Part 1 of the Tutorial: convolution on graphs and differences between simple fully-connected neural networks (MLPs) and graph networks: [https://medium.com/@BorisAKnyazev/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-1-3d9fada3b80d](https://medium.com/@BorisAKnyazev/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-1-3d9fada3b80d)

\[2\] Anisotropic, Dynamic, Spectral and Multiscale Filters Defined on Graphs: [https://towardsdatascience.com/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49](https://towardsdatascience.com/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49)

\[3\] Spectral Graph Convolution Explained and Implemented Step By Step: [https://towardsdatascience.com/spectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801](https://towardsdatascience.com/spectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801)",3,34
1796,2019-8-30,2019,8,30,7,cx7wp8,[P] Building projects to get an internship,https://www.reddit.com/r/MachineLearning/comments/cx7wp8/p_building_projects_to_get_an_internship/,Krokodeale,1567117192," Hi guys,

I'll start my last year in my master's degree and I've got an internship to do for 6 months in 2020. This year is specialized   
 in AI and I've already done some basic projects in DP. However, for my internship, I wanna be ambitious and try to look for some goods labs in order to get a PhD after.

But I don't know really what kind of projects that I can do to present my lvl in CS. I know it's important to have a good Github profile, but what do people expect from a student at my level ? Any idea from where to start ? I've got the feeling that doing things that have already been done isn't really worth it, I'm a bit lost.

Thank you for you time",5,5
1797,2019-8-30,2019,8,30,9,cx9amr,Looking for an advanced text to speech editor,https://www.reddit.com/r/MachineLearning/comments/cx9amr/looking_for_an_advanced_text_to_speech_editor/,KalynnCampbell,1567124012,[removed],0,1
1798,2019-8-30,2019,8,30,9,cx9k1j,[D] Policy Distillation in a continuous action space with no knowledge of teacher distribution,https://www.reddit.com/r/MachineLearning/comments/cx9k1j/d_policy_distillation_in_a_continuous_action/,CartPole,1567125385,"Has anyone seen any work related to performing Policy Distillation in a continuous action space with no knowledge of the teacher distribution(black box policy returning only the action)? My guess is to perform something along the lines of negative log-likelihood(NLL).

\[Imgur\]([https://imgur.com/gDnZyVZ](https://imgur.com/gDnZyVZ))",3,4
1799,2019-8-30,2019,8,30,10,cxaa8l,Recommendation for GANs which can be trained on cheap hardware.,https://www.reddit.com/r/MachineLearning/comments/cxaa8l/recommendation_for_gans_which_can_be_trained_on/,TheInnocuousOne,1567129280,[removed],0,1
1800,2019-8-30,2019,8,30,14,cxcm8i,How can I solve yandex captcha?,https://www.reddit.com/r/MachineLearning/comments/cxcm8i/how_can_i_solve_yandex_captcha/,hello-world0,1567143055,[removed],0,1
1801,2019-8-30,2019,8,30,14,cxcv0q,Machine Learning !,https://www.reddit.com/r/MachineLearning/comments/cxcv0q/machine_learning/,freevideolectures,1567144734,[removed],0,1
1802,2019-8-30,2019,8,30,16,cxdi63,Does anyone know how I can add to Wikidata using Quickstatements?,https://www.reddit.com/r/MachineLearning/comments/cxdi63/does_anyone_know_how_i_can_add_to_wikidata_using/,RestingForrest,1567149381,[removed],0,1
1803,2019-8-30,2019,8,30,16,cxdrwb,All-optical neural network for deep learning,https://www.reddit.com/r/MachineLearning/comments/cxdrwb/alloptical_neural_network_for_deep_learning/,mystikaldanger,1567151418,,0,1
1804,2019-8-30,2019,8,30,16,cxdts6,Natural Language Processing: the age of Transformers,https://www.reddit.com/r/MachineLearning/comments/cxdts6/natural_language_processing_the_age_of/,cryptoz,1567151843,[removed],0,1
1805,2019-8-30,2019,8,30,17,cxdxy6,[R] All-Optical Neural Network For Deep Learning,https://www.reddit.com/r/MachineLearning/comments/cxdxy6/r_alloptical_neural_network_for_deep_learning/,mystikaldanger,1567152751,,1,26
1806,2019-8-30,2019,8,30,17,cxe69g,Article: Data Science Projects: common mistakes and how to avoid them?,https://www.reddit.com/r/MachineLearning/comments/cxe69g/article_data_science_projects_common_mistakes_and/,naxty1995,1567154565,,1,1
1807,2019-8-30,2019,8,30,19,cxf2qq,[D] Paper review + interview; Learning spatiotemporal [video action] features with 3d convolutional networks,https://www.reddit.com/r/MachineLearning/comments/cxf2qq/d_paper_review_interview_learning_spatiotemporal/,timscarfe,1567161070,"Join Karol Zak on a tour of video action detection using deep learning using 3DCNNs. Karol is going to cover off a breakthrough paper from 2015 ""Learning spatiotemporal \[video action\] features with 3d convolutional networks"". 3D convnets are conceptually very easy to use and understand, they work like normal 2D CNNs but they use the third (depth) dimension to capture the time domain. Several large video action datasets have appeared on the scene too which have significantly democratised the practice i.e. Sports1M, Kinetics.

Karols style is very practical and hands-on and like last time will demonstrate the models working live in a Jupyter notebook and talk to some of his experience with video action detection.

[https://www.youtube.com/watch?v=Ty-gww6GwHY](https://www.youtube.com/watch?v=Ty-gww6GwHY)

Paper link; [https://arxiv.org/abs/1412.0767](https://arxiv.org/abs/1412.0767)",8,6
1808,2019-8-30,2019,8,30,20,cxfh9n,Best Machine Learning YouTube Channels,https://www.reddit.com/r/MachineLearning/comments/cxfh9n/best_machine_learning_youtube_channels/,rankingmachine,1567163663,,0,1
1809,2019-8-30,2019,8,30,20,cxfjaj,inquiry,https://www.reddit.com/r/MachineLearning/comments/cxfjaj/inquiry/,machinelearner76,1567164015,[removed],0,1
1810,2019-8-30,2019,8,30,21,cxgeav,Are memes allowed?,https://www.reddit.com/r/MachineLearning/comments/cxgeav/are_memes_allowed/,EpicGamer9173,1567168982,,0,1
1811,2019-8-30,2019,8,30,22,cxgwvu,[Discussion] Where to get started using NLP for building a Discord Bot,https://www.reddit.com/r/MachineLearning/comments/cxgwvu/discussion_where_to_get_started_using_nlp_for/,stat30fbliss,1567171607,"Hey there! Im not sure if this is the correct place to ask this question, but it seemed like a reasonable place to start. If theres a better place for me to post this please let me know.

Ive had this idea for a while of wanting to build a Discord bot that is somewhat of a grammar nazi and will shame people for incorrect use of their, there, and theyre. I dont know if this is even possible, but was wondering if theres an NLP framework that could take a users message as input, and if the grammar is incorrect regarding the use of their, there, or theyre I could generate a message based off of the response. Is this feasible?

Discord supports JavaScript and Python for bots. Im a web developer by day and comfortable with JS. If this concept is feasible Id love to use Python for it so I can start learning the language.

Thanks!",15,6
1812,2019-8-30,2019,8,30,22,cxh10z,I need code for answer-focused and position-aware neural question generation.,https://www.reddit.com/r/MachineLearning/comments/cxh10z/i_need_code_for_answerfocused_and_positionaware/,govinddaga,1567172162,[removed],0,1
1813,2019-8-30,2019,8,30,22,cxh346,"5 Must Have Skills Needed For Machine Learning Jobs ""[D]""",https://www.reddit.com/r/MachineLearning/comments/cxh346/5_must_have_skills_needed_for_machine_learning/,MealPlan,1567172433,,0,1
1814,2019-8-30,2019,8,30,22,cxh8pi,Seeking ML partner for funded research project.,https://www.reddit.com/r/MachineLearning/comments/cxh8pi/seeking_ml_partner_for_funded_research_project/,mc2015,1567173171,[removed],0,1
1815,2019-8-30,2019,8,30,23,cxhf0m,"""Why blockchain?"" the viewpoint of a representative of the Microsoft team working on researching AI &amp; Machine Learning",https://www.reddit.com/r/MachineLearning/comments/cxhf0m/why_blockchain_the_viewpoint_of_a_representative/,John_Muck,1567173996,,0,1
1816,2019-8-30,2019,8,30,23,cxhjx1,"""Why blockchain?"" the viewpoint of a representative of the Microsoft team working on researching AI &amp; Machine Learning",https://www.reddit.com/r/MachineLearning/comments/cxhjx1/why_blockchain_the_viewpoint_of_a_representative/,John_Muck,1567174647,,0,1
1817,2019-8-30,2019,8,30,23,cxhrli,Awesome Machine Learning and Data Science Career Resources - GitHub Repository,https://www.reddit.com/r/MachineLearning/comments/cxhrli/awesome_machine_learning_and_data_science_career/,OppositeMidnight,1567175676,,0,1
1818,2019-8-30,2019,8,30,23,cxhvbd,[D] What is the reality of machine learning engineer?,https://www.reddit.com/r/MachineLearning/comments/cxhvbd/d_what_is_the_reality_of_machine_learning_engineer/,Knackmanic,1567176150,"I'm a physics engineer but I don't find much attraction for the jobs and I feel kind of like escaping reality/responsibilities for a little bit by going back to school. 

Before finishing school, I remembered telling people how I wanted to do ML and that my internship I did on computer vision was inspiring, that I wanted to do more project on that, etc. Now I have a job and, while very serious and ""important"" I'm left contemplating this avenue once more. I see at my current job how data crunching is important and tedious. I'm not sure how a ML project could easily be incorporated in a company that still relies on DOS systems but I see how crucial statistical analysis are to find root cause to production problems. 

I'm increasingly tempted for the above reason to hop into a 1 year professional master program on AI. However, I wonder what's the kind of job in medium/big corporate for data/ML engineer? I'm not looking to be a programmer because I'm not that young (28) and have a big physics background (I'm not competitive vs. someone who studied computer science for example). Should I attempt this? I know asking strangers is not the wisest but I find helpful to hear from some one else experience.",126,165
1819,2019-8-30,2019,8,30,23,cxhvqw,Optimal transport mapping via input convex neural networks,https://www.reddit.com/r/MachineLearning/comments/cxhvqw/optimal_transport_mapping_via_input_convex_neural/,pikachuchameleon,1567176210,"[https://arxiv.org/abs/1908.10962](https://arxiv.org/abs/1908.10962) 

I am one of the co-authors. In this work we present a novel approach to learn the convex Kantorovich potential and thus the optimal transport map for Wasserstein-2 distance, via input convex neural networks. Currently we are working extending our approach to high dimensional images such as MNIST. Please feel free to ask any questions!",0,1
1820,2019-8-30,2019,8,30,23,cxi2jc,$1 Million Challenge: Develop Predictive Biomarkers For SUDEP Institute,https://www.reddit.com/r/MachineLearning/comments/cxi2jc/1_million_challenge_develop_predictive_biomarkers/,mystikaldanger,1567177094,,0,1
1821,2019-8-31,2019,8,31,0,cxi3np,Questions about UCT (UCB applied to Trees),https://www.reddit.com/r/MachineLearning/comments/cxi3np/questions_about_uct_ucb_applied_to_trees/,yyt224,1567177244,[removed],0,1
1822,2019-8-31,2019,8,31,0,cxiumr,Metric to optimize on an imbalanced dataset?,https://www.reddit.com/r/MachineLearning/comments/cxiumr/metric_to_optimize_on_an_imbalanced_dataset/,seeaemearohin,1567180606,[removed],0,1
1823,2019-8-31,2019,8,31,1,cxj1y3,[Help] Seq2seq for speech to text,https://www.reddit.com/r/MachineLearning/comments/cxj1y3/help_seq2seq_for_speech_to_text/,sicp4lyfe,1567181483,[removed],0,1
1824,2019-8-31,2019,8,31,1,cxja03,2001 Dodge ram 1500 steering,https://www.reddit.com/r/MachineLearning/comments/cxja03/2001_dodge_ram_1500_steering/,anicholas22,1567182501,[removed],0,1
1825,2019-8-31,2019,8,31,1,cxjbke,Resources to keep up with current advances in machine learning,https://www.reddit.com/r/MachineLearning/comments/cxjbke/resources_to_keep_up_with_current_advances_in/,e_dimensional,1567182708,"How can one keep up with the current advances in machine learning, such as new algorithms, techniques etc. as old ways become outdated, any resources would be highly appreciated",0,1
1826,2019-8-31,2019,8,31,2,cxjqpp,[D] No TF-GPU 2.0.0rc0 on Kaggle?,https://www.reddit.com/r/MachineLearning/comments/cxjqpp/d_no_tfgpu_200rc0_on_kaggle/,Yaus,1567184590,"Hello,

I was attempting to !pip install tensorflow-gpu==2.0.0rc0 in Kaggle today, but kept getting the error message that no matching package can be found.

I can install tf2-gpu locally just fine, but this is my first time using kaggle and I'm not sure whether I'm doing something incorrectly or if the release candidate cannot be installed in a Kaggle kernel.

Any help would be appreciated, thank you.",0,1
1827,2019-8-31,2019,8,31,3,cxkpit,GPU-accelerated inference for decision forests with RAPIDS FIL (technical blog),https://www.reddit.com/r/MachineLearning/comments/cxkpit/gpuaccelerated_inference_for_decision_forests/,Zstats,1567188931,,0,1
1828,2019-8-31,2019,8,31,3,cxkr5f,[Research] Deep learning and the renormalization group,https://www.reddit.com/r/MachineLearning/comments/cxkr5f/research_deep_learning_and_the_renormalization/,Marha01,1567189141,,0,1
1829,2019-8-31,2019,8,31,3,cxkyfj,Meet Microsoft Suphx: The Worlds Strongest Mahjong AI,https://www.reddit.com/r/MachineLearning/comments/cxkyfj/meet_microsoft_suphx_the_worlds_strongest_mahjong/,Yuqing7,1567190078,,0,1
1830,2019-8-31,2019,8,31,3,cxl4om,This is how chemists are trying to adapt to the use of AI and ML (some of the most recent baby steps),https://www.reddit.com/r/MachineLearning/comments/cxl4om/this_is_how_chemists_are_trying_to_adapt_to_the/,InAlteredState,1567190870,,0,1
1831,2019-8-31,2019,8,31,3,cxl4sk,[Research] Neural Networks Are Essentially Polynomial Regression,https://www.reddit.com/r/MachineLearning/comments/cxl4sk/research_neural_networks_are_essentially/,semidecided,1567190885,,0,1
1832,2019-8-31,2019,8,31,3,cxl8lk,GPU-accelerated decision forest inference with RAPIDS FIL (technical blog post),https://www.reddit.com/r/MachineLearning/comments/cxl8lk/gpuaccelerated_decision_forest_inference_with/,Zstats,1567191374,,0,1
1833,2019-8-31,2019,8,31,4,cxlqoo,Time series forecasting,https://www.reddit.com/r/MachineLearning/comments/cxlqoo/time_series_forecasting/,noura_7ussein,1567193673,[removed],0,1
1834,2019-8-31,2019,8,31,4,cxlr5z,H2O run time differences,https://www.reddit.com/r/MachineLearning/comments/cxlr5z/h2o_run_time_differences/,XannyFairy,1567193737,,0,1
1835,2019-8-31,2019,8,31,4,cxlu29,[P] GPU-accelerated decision forest inference with RAPIDS FIL (technical blog),https://www.reddit.com/r/MachineLearning/comments/cxlu29/p_gpuaccelerated_decision_forest_inference_with/,Zstats,1567194100,,1,3
1836,2019-8-31,2019,8,31,4,cxlyr1,Help in arxiv submission error!,https://www.reddit.com/r/MachineLearning/comments/cxlyr1/help_in_arxiv_submission_error/,nile6499,1567194693,[removed],0,1
1837,2019-8-31,2019,8,31,5,cxmajl,Where noobs can learn machine learning,https://www.reddit.com/r/MachineLearning/comments/cxmajl/where_noobs_can_learn_machine_learning/,raviraushan,1567196213,,1,1
1838,2019-8-31,2019,8,31,5,cxmiqz,What is the v_r in the NeurIPS logo?,https://www.reddit.com/r/MachineLearning/comments/cxmiqz/what_is_the_v_r_in_the_neurips_logo/,leonoverweel,1567197275,[removed],0,1
1839,2019-8-31,2019,8,31,6,cxmxie,[D] Additional maths exam: worth it?,https://www.reddit.com/r/MachineLearning/comments/cxmxie/d_additional_maths_exam_worth_it/,Hybr1d97,1567199206,"Hi everyone, I'm about to start a MS in CE (controls/robotics focus). I have the possibility to (unnecessarily) add a course called ""Mathematics in Machine Learning"", 8 ECTS, in my second semester, making it go from 24 to 32 ECTS. The temptation arises from the fact that proving that I own strong maths foundations in ML could result in good ML research positions and salaries. However, I'll also have to do compulsive courses like Convex Optimization and general ML, so I'd get a good basis anyway.

My question is: is the ""extra"" work worth it, or won't companies care at all?

The course programme is the following:

* Mathematical representations of data: spaces (including Hilbert spaces), metrics, distances, dissimilarities and kernels. Geometry of very high dimensional spaces and the curse of dimensionality.
* Learning theory, PAC, Rademacher and VC dimension. Trade-off Bias vs Model Variance and Model Complexity.
* Cross validation, bootstrap and applications.
* Linear algebra-based methods: Principal Component Analysis, Linear Discriminant Analysis, Independent Component Analysis and Stochastic projections (Johnson - Lindenstrauss Transform).
* Linear Models (regression, ANOVA, DOE).
* Generalized linear models (categorical data, logistic and multinomial regression).
* Model and feature selection, hyperparameter tuning (e.g. lasso, AIC, BIC, ridge).
* Bayesian networks (basic concepts, exact and MCMC-based computations).",0,1
1840,2019-8-31,2019,8,31,6,cxn900,Data Annotation: The Billion Dollar Business Behind AI Breakthroughs,https://www.reddit.com/r/MachineLearning/comments/cxn900/data_annotation_the_billion_dollar_business/,trcytony,1567200752,[removed],0,1
1841,2019-8-31,2019,8,31,6,cxn9i4,[N] Data Annotation: The Billion Dollar Business Behind AI Breakthroughs,https://www.reddit.com/r/MachineLearning/comments/cxn9i4/n_data_annotation_the_billion_dollar_business/,trcytony,1567200815,[removed],0,1
1842,2019-8-31,2019,8,31,6,cxncz1,[R] Additional maths exam: worth it?,https://www.reddit.com/r/MachineLearning/comments/cxncz1/r_additional_maths_exam_worth_it/,Hybr1d97,1567201294,"Hi everyone, I'm about to start a MS in CE (controls/robotics focus). I have the possibility to (unnecessarily) add a course called ""Mathematics in Machine Learning"", 8 ECTS, in my second semester, making it go from 24 to 32 ECTS. The temptation arises from the fact that proving that I own strong maths foundations in ML could result in good ML research positions and salaries. However, I'll also have to do compulsive courses like Convex Optimization and general ML, so I'd get a good basis anyway.

My question is: is the ""extra"" work worth it, or won't companies care at all?

The course programme is the following:

* Mathematical representations of data: spaces (including Hilbert spaces), metrics, distances, dissimilarities and kernels. Geometry of very high dimensional spaces and the curse of dimensionality.
* Learning theory, PAC, Rademacher and VC dimension. Trade-off Bias vs Model Variance and Model Complexity.
* Cross validation, bootstrap and applications.
* Linear algebra-based methods: Principal Component Analysis, Linear Discriminant Analysis, Independent Component Analysis and Stochastic projections (Johnson - Lindenstrauss Transform).
* Linear Models (regression, ANOVA, DOE).
* Generalized linear models (categorical data, logistic and multinomial regression).
* Model and feature selection, hyperparameter tuning (e.g. lasso, AIC, BIC, ridge).
* Bayesian networks (basic concepts, exact and MCMC-based computations).",4,0
1843,2019-8-31,2019,8,31,6,cxnivr,What are the main tech difficulties question arising to CV developers? NLP developers during work?,https://www.reddit.com/r/MachineLearning/comments/cxnivr/what_are_the_main_tech_difficulties_question/,throwawaylalallal,1567202111,[removed],0,1
1844,2019-8-31,2019,8,31,7,cxnnm0,"What are the main tech challenges,questions CV, nlp devs face at their work?",https://www.reddit.com/r/MachineLearning/comments/cxnnm0/what_are_the_main_tech_challengesquestions_cv_nlp/,throwawaylalallal,1567202741,[removed],0,1
1845,2019-8-31,2019,8,31,7,cxnyah,I want to be riding this wave and not standing in its way,https://www.reddit.com/r/MachineLearning/comments/cxnyah/i_want_to_be_riding_this_wave_and_not_standing_in/,RiftyM,1567204206,,0,1
1846,2019-8-31,2019,8,31,8,cxoysm,Looking for a good dataset with images of people,https://www.reddit.com/r/MachineLearning/comments/cxoysm/looking_for_a_good_dataset_with_images_of_people/,ajt9000,1567209432,[removed],0,1
1847,2019-8-31,2019,8,31,10,cxpzui,[P] Mona Lisa Stylegan FUNIT SPADE deepfake face swap video from a single image no training for any face pair,https://www.reddit.com/r/MachineLearning/comments/cxpzui/p_mona_lisa_stylegan_funit_spade_deepfake_face/,PuzzledProgrammer3,1567215261,"result video: [https://twitter.com/roadrunning01/status/1167558507547320320?s=20](https://twitter.com/roadrunning01/status/1167558507547320320?s=20)

github repo: [https://github.com/shaoanlu/fewshot-face-translation-GAN](https://github.com/shaoanlu/fewshot-face-translation-GAN)",4,6
1848,2019-8-31,2019,8,31,11,cxqh2e,Applied Probability theory in Computer Vision,https://www.reddit.com/r/MachineLearning/comments/cxqh2e/applied_probability_theory_in_computer_vision/,PyWarrior,1567218077,"I am not able to relate the concepts of probability theory to image theory like 
1. What is the probability distribution of Image data
2. What is fitting a gaussian to an image data and what is it's practicality?

There are many more concepts. I think there is a huge gap between Neural Networks and Probability theory.

Can anyone refer me some good resources including videos or blogs or books which can give me detailed knowledge about probability theory in case of image dataset and help me bridge the gap between Neural Networks and Probability theory.

Thanks in advance",0,1
1849,2019-8-31,2019,8,31,11,cxql2h,100+ Basic Deep Learning Interview Questions and Answers,https://www.reddit.com/r/MachineLearning/comments/cxql2h/100_basic_deep_learning_interview_questions_and/,nkptcs,1567218726,,0,1
1850,2019-8-31,2019,8,31,14,cxs7sr,"What stream should I choose for my MS ,data science or CS with specialization in ML (if such a thing exists) if I want to work on A.I only (as I am more interested in working on NLP , Computer vision and other AI related stuff rather than predicting some data for a firm as a data scientist) ?",https://www.reddit.com/r/MachineLearning/comments/cxs7sr/what_stream_should_i_choose_for_my_ms_data/,Alex55936,1567229413,[removed],0,1
1851,2019-8-31,2019,8,31,14,cxsf9l,How Facebook Uses Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cxsf9l/how_facebook_uses_machine_learning/,madhu_SEO,1567230943,,0,0
1852,2019-8-31,2019,8,31,15,cxsik6,Reinforcement Learning Specialization,https://www.reddit.com/r/MachineLearning/comments/cxsik6/reinforcement_learning_specialization/,frenchdic,1567231631,,0,1
1853,2019-8-31,2019,8,31,15,cxsk6a,Upcoming Breakthrough Technologies and Futuristic Inventions 2020 [Updated] | TechLurn,https://www.reddit.com/r/MachineLearning/comments/cxsk6a/upcoming_breakthrough_technologies_and_futuristic/,omkar72566,1567231962,,0,1
1854,2019-8-31,2019,8,31,15,cxskj1,[D] Fraudsters Used AI to Mimic CEOs Voice in Unusual Cybercrime Case,https://www.reddit.com/r/MachineLearning/comments/cxskj1/d_fraudsters_used_ai_to_mimic_ceos_voice_in/,Deepblue129,1567232034,"Link: https://www.wsj.com/articles/fraudsters-use-ai-to-mimic-ceos-voice-in-unusual-cybercrime-case-11567157402

I think this is an important issue to discuss in our community. There are a number of community members that have publicly released their speech synthesis models and they have cloned other peoples voices without consent. I believe that sets a dangerous precedent.

Its important that speech synthesis models are not posted online for everyone to use. Those open source models may easily be abused.

Its important that we dont clone celebrities voices without their explicit consent. A speech synthesis model of a celebrity can put that person at great risk of identity fraud.

What do you all think?

I am not without my own biases. I do work at a speech synthesis company. We have a concrete focus on the ethics of speech synthesis.",32,0
1855,2019-8-31,2019,8,31,15,cxsrmh,a bit confused about the norm in Ordinary Least Squares,https://www.reddit.com/r/MachineLearning/comments/cxsrmh/a_bit_confused_about_the_norm_in_ordinary_least/,Armin71,1567233509,[removed],0,1
1856,2019-8-31,2019,8,31,16,cxsyio,1D Kalman Is Exponential Or Cumulative Average,https://www.reddit.com/r/MachineLearning/comments/cxsyio/1d_kalman_is_exponential_or_cumulative_average/,vackosar,1567235007,,0,1
1857,2019-8-31,2019,8,31,16,cxt276,Refractory gunning machine working test,https://www.reddit.com/r/MachineLearning/comments/cxt276/refractory_gunning_machine_working_test/,Wendy-Wu615,1567235790,,0,1
1858,2019-8-31,2019,8,31,19,cxu9d7,"[D] I started writing a book on practical considerations of ML, keen for feedback on its direction",https://www.reddit.com/r/MachineLearning/comments/cxu9d7/d_i_started_writing_a_book_on_practical/,katnz,1567245672,"I was finding I was constantly having the same conversations with people about implementing ML in practice, so I tried to find a resource I could provide to people that might help. However, I found there wasn't much on the practical side of implementing ML - so about a year ago I drafted out a table of contents and started writing. I ended up shelving it for a bit, and have just picked it up again now - but am torn between just blogging what I've already got, or trucking on to create a unified resource (i.e. the book). 

I'm keen to hear what the reddit ML community thinks - whether I should continue (maybe it's been superceded?), and if I continue, if there's anything you'd like to see covered in the book?  

My intention - should I continue and complete it - is to self-publish online through something like LeanPub. I have no burning desire to see the book in print or make money off it, I'd really just like to raise awareness of what we all need to think about when we create ML solutions in the real world.

This is me:  [https://twitter.com/drkatnz](https://twitter.com/drkatnz) 

Here's how the table of contents looks (about \~25% of the content is written already, and subsections aren't shown. Feedback so far has been to include a section on biases, which has been added):

1. Introduction  
1.1 Terminology  
1.2 How do I get started using machine learning?

2. Do you really need machine learning?  
2.1 Data availability  
2.2 Liability  
2.3 Capability  
2.4 Other solutions  
2.5 Pre-requisite checklist

3. Team  
3.1 Skills  
3.2 Common team structures  
3.3 Forming a team and getting started

4. Building your first machine learning solution

5. Data collection  
5.1 Collecting the data  
5.2 Data set size - how much is enough?  
5.3 Labeled versus unlabeled data

6. Pre-processing  
6.1 Automatically cleaning the data  
6.2 Dealing with missing values  
6.3 Applying domain knowledge  
6.4 Feature cleanup  
6.5 Dealing with the minority class

7. Algorithm considerations  
7.1 Unsupervised versus supervised  
7.2 Good enough accuracy  
7.3 Storage  
7.4 Speed

8. Measuring accuracy  
8.1 Metrics  
8.2 Minimum required accuracy  
8.3 Test set  
8.4 Investigating prediction errors  
8.5 A/B Testing

9. Identifying and Mitigating Biases  
9.1 Biases from data  
9.2 Biases from trained models  
9.3 Inventors bias  
9.4 Biases caused by perception of machine learning

10 Getting an algorithm to production  
10.1 Infrastructure  
10.2 Documentation  
10.3 User interface  
10.4 Abstaining classifiers  
10.5 Runtime environment

11. Managing live algorithms  
11.1 Monitoring  
11.2 Effect on the real world  
11.3 Auditing results  
11.4 Updating models  
11.5 Technical debt  


**What do y'all think?**",27,79
1859,2019-8-31,2019,8,31,20,cxur24,Parsing forum comments to find out possible points of failures,https://www.reddit.com/r/MachineLearning/comments/cxur24/parsing_forum_comments_to_find_out_possible/,doggo16818,1567249623,[removed],0,1
1860,2019-8-31,2019,8,31,20,cxv2hs,Has anyone try FPGA(Field Programmable Gate Array) with ML ? How was your result and speed compared to GPU?,https://www.reddit.com/r/MachineLearning/comments/cxv2hs/has_anyone_try_fpgafield_programmable_gate_array/,umargan,1567251923,[removed],0,1
1861,2019-8-31,2019,8,31,23,cxw2eq,[D] Overlapping instance segmentation (MASK RCNN?),https://www.reddit.com/r/MachineLearning/comments/cxw2eq/d_overlapping_instance_segmentation_mask_rcnn/,mackie__m,1567261231,"I'm trying to solve a problem where I'm trying to train a network for instance segmentation. Here, unlike the examples present in natural images, most of the masks overlap with each other. Here's an example with 2 masks that I expect. Notice the significant overlap.

https://imgur.com/a/QJQB7MM

I'm trying to go about this with Matterport's Mask RCNN implementation. And first attempt of results weren't great. But, I'm just starting, and I probably am not configuring it correctly (to train). But, just in case this is impossible to do or there's a better way to do this, I thought I'd ask this community.",5,7
1862,2019-8-31,2019,8,31,23,cxw5gw,Completing your first publishing worthy work,https://www.reddit.com/r/MachineLearning/comments/cxw5gw/completing_your_first_publishing_worthy_work/,dsengupta16,1567261806,[removed],0,1
