,sbr,sbr_id,date,year,month,hour,day,id,domain,title,full_link,author,created,selftext,num_comments,score,over_18,thumbnail,media_provider,media_thumbnail,media_title,media_description,media_url
0,tensorflow,t5_3alkk,2017-6-4,2017,6,4,15,6f5zow,stackoverflow.com,Recurrent neural net in TensorFlow. I've got stuck. Could anyone fix it?,https://www.reddit.com/r/tensorflow/comments/6f5zow/recurrent_neural_net_in_tensorflow_ive_got_stuck/,HarambeTownley,1496557327,,0,0,False,default,,,,,
1,tensorflow,t5_3alkk,2017-6-6,2017,6,6,2,6ffnhh,github.com,Sequence-to-sequence implementation using tf.contrib.seq2seq.BeamSearchDecoder,https://www.reddit.com/r/tensorflow/comments/6ffnhh/sequencetosequence_implementation_using/,uwanggood,1496683616,,0,2,False,default,,,,,
2,tensorflow,t5_3alkk,2017-6-6,2017,6,6,8,6fi2ao,self.tensorflow,HELP! Understanding TensorFlow?,https://www.reddit.com/r/tensorflow/comments/6fi2ao/help_understanding_tensorflow/,samiejg,1496705973,"I've watched a ton of different Youtube videos on TensorFlow, as well as following along with some tutorials on there too. But I'm still having a difficult time understanding how it works.

Just feeling frustrated with myself now (and feeling dumb). I'm sure part of it is because I'm only an amateur when it comes to programming... and not having a College/University level English comprehension, nor that of a Computer Science graduate and the verbiage they use.

I hear a lot of talk about TensorFlow being comparable to AI, and I just don't understand that at all... and I wonder if I did understand it, if maybe the rest would just ""click"" for me.

The part I'm not understanding is how you can throw in a few files with data, and somehow TensorFlow can train off the data. Like... how does TensorFlow know what's correct when it's training?

Like one example is the TensorFlow chatbot stuff which was posted by Siraj Raval on Youtube. Apparently you can just have a couple files full of random data phrases/responses/text and somehow TensorFlow can train off that? And then become some sort of super-functional AI chatbot? Just not getting it :( and it doesn't help that it's not working for me.. but sounds like I'm not the only one because there are others reporting completely gibberish responses from the bot (even after days of training).

Any suggestions/thoughts? Aside from going to university or being smarter? :P",5,1,False,self,,,,,
3,tensorflow,t5_3alkk,2017-6-6,2017,6,6,21,6flgy7,self.tensorflow,How to evaluate tf-slim network using native tensorflow!,https://www.reddit.com/r/tensorflow/comments/6flgy7/how_to_evaluate_tfslim_network_using_native/,princedhiman,1496752438,"Can somebody please help me in evaluating tf-slim trained network using native tensorflow. 
My deployment server doesn't support tf-slim.",3,2,False,self,,,,,
4,tensorflow,t5_3alkk,2017-6-7,2017,6,7,13,6fr6j6,self.tensorflow,Creating bottlenecks takes forever?,https://www.reddit.com/r/tensorflow/comments/6fr6j6/creating_bottlenecks_takes_forever/,mushm0m,1496809234,"Hi guys,

I'm new to TF and I'm following this tutorial: https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/index.html#4

However I have my own dataset. I have roughly 400K images spread across 100 classes.

I'm now at the step of retraining the final layer of the network. I'm noticing that the retrain script generates a bottleneck file for every image, sequentially (there's no multi-threading). I'm on a machine with 1 GPU and 8 CPUs, if that matters. At the pace this is taking, I'd estimate that it would take ~20 hours to create all the bottleneck files. 

This is really slow - is there anything I can do?

EDIT: I've checked my CPU usage and it does appear that all 8 CPUs are being used quite heavily, actually. Is there anything else I can do to cut down on time (besides reducing images)",6,2,False,self,,,,,
5,tensorflow,t5_3alkk,2017-6-7,2017,6,7,15,6frnph,self.tensorflow,Stuck at one small error since the past 2 days. Please help?,https://www.reddit.com/r/tensorflow/comments/6frnph/stuck_at_one_small_error_since_the_past_2_days/,rjmessibarca,1496816112,"I was following tensorflow from sentdex tutorials but got stuck on [this](https://pythonprogramming.net/data-size-example-tensorflow-deep-learning-tutorial/) tutorial. Basically, I did everything as mentioned there, and also downloaded the `model.ckpt` and `lexicon.pickle` file from there.

&amp;nbsp;

However when I run the `test_neural_network()`, I get the following error:

`InvalidArgumentError (see above for traceback): Expected to restore a tensor of type float, got a tensor of type int32 instead: tensor_name = Variable`

&amp;nbsp;

I have tried googling it since the past 2 days and even posted the question on stack overflow, but have not got any answer.

&amp;nbsp;

This is my last hope. Please help",3,2,False,self,,,,,
6,tensorflow,t5_3alkk,2017-6-7,2017,6,7,16,6fry5e,self.tensorflow,Help estimator for distribution,https://www.reddit.com/r/tensorflow/comments/6fry5e/help_estimator_for_distribution/,Cz1975,1496820670,"I am trying to look for resources on how to build an estimator for a bi-modal distribution. An example of what I want to do:

My distribution looks similar to this -b, -a, 0 , a, b. The histogram is centered on 0.

X0=[0.2, 0.1, 0, 0.5, 0.2] y=[1] Weight is to the right, things will go up.

X1=[0.2, 0.5, 0, 0.1, 0.2] y=[-1] Weight is to the left, things will go down.



Xn

After working through a 300p book about tensorflow I do not feel any closer to a solution. Any pointers on where to look on building a graph would be greatly appreciated.
",0,1,False,self,,,,,
7,tensorflow,t5_3alkk,2017-6-8,2017,6,8,3,6fv7tc,self.tensorflow,Building Tensorflow Graphs Inside of Functions,https://www.reddit.com/r/tensorflow/comments/6fv7tc/building_tensorflow_graphs_inside_of_functions/,FiniteDelight,1496859004,"I'm learning Tensorflow and am trying to properly structure my code. I (more or less) know how to build graphs either bare or as class methods, but I'm trying to figure out how best to structure the code. I've tried the simple example:

    def build_graph():                
        g = tf.Graph()     
        with g.as_default():                       
            a = tf.placeholder(tf.int8)
            b = tf.add(a, tf.constant(1, dtype=tf.int8))
        return g   

    graph = build_graph()
    with tf.Session(graph=graph) as sess:
        feed = {a: 3}      
        print(sess.run(b, feed_dict=feed))

which should just print out 4. However, when I do that, I get the error:

    Cannot interpret feed_dict key as Tensor: Tensor 
    Tensor(""Placeholder:0"", dtype=int8) is not an element of this
    graph.

I'm pretty sure this is because the placeholder inside the function `build_graph` is private, but shouldn't the with `tf.Session(graph=graph)` take care of that? If not, what exactly does the `tf.Session(graph=)` keyword argument do? Is there a better way of using a feed dict in a situation like this?
",0,2,False,self,,,,,
8,tensorflow,t5_3alkk,2017-6-8,2017,6,8,8,6fx8nu,self.tensorflow,Does the CPU/chipset matter much if you're using a good GPU?,https://www.reddit.com/r/tensorflow/comments/6fx8nu/does_the_cpuchipset_matter_much_if_youre_using_a/,kyledrake,1496878279,"I'm about to throw a 1080 Ti into an older sandy bridge machine. I was looking at upgrading to kaby lake for this, but I can't find any evidence that I actually need to, and TensorFlow doesn't really cite CPU requirements or recommendations.

I wanted to ask if anyone had any insights/opinions/experience on throwing fast new GPUs into older machines, and if you thought there might be bottleneck issues caused by that. The impression I'm getting is that for the most part, most of the heavy lifting is being done on the GPU and the CPU/bus performance isn't as important. The correct answer is probably ""it depends on what you're doing"", but I wanted to see if anyone had any good anecdotal evidence before I dropped a bunch of $ on new parts I might not need. Thanks!",3,1,False,self,,,,,
9,tensorflow,t5_3alkk,2017-6-8,2017,6,8,9,6fxlxg,self.tensorflow,Tensorboard 'Site cannot be found'?,https://www.reddit.com/r/tensorflow/comments/6fxlxg/tensorboard_site_cannot_be_found/,mushm0m,1496882350,"I just followed this Tensorflow tutorial, doing the final retraining step on a classification problem (but using my own dataset): https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#3

I used a very large dataset so I let the training run overnight. Now it's completed, and I didn't see how it performed as I wasn't at the computer.

I need to visualize the results, so I tried:

    :/tf_files# tensorboard --logdir training_summaries --debug  
    Starting TensorBoard 47 at http://0.0.0.0:6006
    (Press CTRL+C to quit)

There are no additional messages in terminal.

But when I visit http://0.0.0.0:6006 it does not load, and says This site cant be reached

0.0.0.0 refused to connect.
Search Google for 6006
ERR_CONNECTION_REFUSED

What did I do wrong? How do I visualize the results?",1,1,False,self,,,,,
10,tensorflow,t5_3alkk,2017-6-8,2017,6,8,21,6g0nj8,github.com,defaultcontext: Tiny Python util for creating tensorflow-like context managers for default instances of classes,https://www.reddit.com/r/tensorflow/comments/6g0nj8/defaultcontext_tiny_python_util_for_creating/,hidden-markov,1496925116,,0,1,False,default,,,,,
11,tensorflow,t5_3alkk,2017-6-10,2017,6,10,0,6g938x,self.tensorflow,How can I see histograms of weights and/or gradients (I'm looking for vanishing gradients) in an iPython notebook?,https://www.reddit.com/r/tensorflow/comments/6g938x/how_can_i_see_histograms_of_weights_andor/,TheMoskowitz,1497021885,"Haven't used the visual features of tensorflow at all so any code examples would be great, thanks.",0,3,False,self,,,,,
12,tensorflow,t5_3alkk,2017-6-10,2017,6,10,3,6ga603,self.tensorflow,Started my career.,https://www.reddit.com/r/tensorflow/comments/6ga603/started_my_career/,dhanush_ramuk,1497031853,Just installed tensorflow in my windows machine and I think I may have started my career but it's too soon to tell.,0,0,False,self,,,,,
13,tensorflow,t5_3alkk,2017-6-10,2017,6,10,5,6gb4on,self.tensorflow,One hot encoding,https://www.reddit.com/r/tensorflow/comments/6gb4on/one_hot_encoding/,mlpyotr,1497040968,"Is there a programmatic way to get back the encoding after using a one hot encoding function? 

I'm assuming people in (insert w/e company) don't just train models to get accuracy numbers and move on. The models have to be saved and useable for actually future classification. If so, how do they retrieve the encodings? 

It seems that a lot of the examples just worry about doing OHC for training, validation but don't worry about the ""let's make it useable"" part.

Thanks for any clarification!",2,0,False,self,,,,,
14,tensorflow,t5_3alkk,2017-6-10,2017,6,10,6,6gbdeh,infoq.com,Google Announces Tensorflow Lite: A Neural Network Library for Mobile Phones,https://www.reddit.com/r/tensorflow/comments/6gbdeh/google_announces_tensorflow_lite_a_neural_network/,Dutchcheesehead,1497043390,,0,23,False,default,,,,,
15,tensorflow,t5_3alkk,2017-6-12,2017,6,12,4,6gn8cs,self.tensorflow,How to install Tensorflow on Windows 10?,https://www.reddit.com/r/tensorflow/comments/6gn8cs/how_to_install_tensorflow_on_windows_10/,trillykins,1497208700,"Guide on the official site says to just use pip, but Tensorflow is not on pip.

&gt; Could not find a version that satisfies the requirement tensorflow (from versions: ) No matching distribution found for tensorflow

Looking on the internet, it seems like the issue has persisted for almost a year now, so I can only assume the site just hasn't been updated. I'm using the latest version of Python 3.6.x and Pip is up-to-date. Tried 3.5 because it says it only supports 3.5.x, but same story. 

Anyone know how you're supposed to install it now? ",12,2,False,self,,,,,
16,tensorflow,t5_3alkk,2017-6-12,2017,6,12,4,6gncak,self.tensorflow,Using TFRecords as inputs to a CNN in Python,https://www.reddit.com/r/tensorflow/comments/6gncak/using_tfrecords_as_inputs_to_a_cnn_in_python/,MekaMuffin,1497209861,"Hello all. So currently, I want to use TFRecords (tensorflow - python) containing image data and image labels in a dictionary. More specifically, I want to use this data to train a CNN (Convolutional Neural Network) for the SVHN 32x32 image dataset. What is the best way to go about doing this? As of now, I have trained a CNN with the 32x32 SVHN image dataset using .bin files as inputs. I used the tensorflow cifar10 code and changed the input to be the SVHN images and the classes to be integers from 0-9. To clarify, instead of using .bin files as inputs, I want to use a TFRecord containing training image data and labels. How would I do this? Thanks in advance for your help!",0,1,False,self,,,,,
17,tensorflow,t5_3alkk,2017-6-12,2017,6,12,5,6gnjjb,self.tensorflow,Is there a way to partially load a tensor into memory? Or is my computer just a potato,https://www.reddit.com/r/tensorflow/comments/6gnjjb/is_there_a_way_to_partially_load_a_tensor_into/,ronsap123,1497211989,"Hey everyone I'm just getting into tensorflow and yesterday I implemented my first generative adversarial network. And there is a dense layer there that goes from 25600 to 40000 computational units. The last one being a 200200 image. It seems my computer can't handle that, it says that there is not enough memory for a tensor with a size of[25600, 40000]. Am I not supposed to load it at once into memory? Am I doing something wrong? Or is my computer really is that bad. (Dual core i5, intel graphics, 8 gb ram, sad I know)",3,1,False,self,,,,,
18,tensorflow,t5_3alkk,2017-6-12,2017,6,12,16,6gqvbx,softwaremill.com,"Upgrading AWS ""Deep Learning AMI Ubuntu Version"" to TensorFlow 1.1.0 with GPU support",https://www.reddit.com/r/tensorflow/comments/6gqvbx/upgrading_aws_deep_learning_ami_ubuntu_version_to/,adamw1pl,1497254387,,0,7,False,default,,,,,
19,tensorflow,t5_3alkk,2017-6-12,2017,6,12,18,6gr9k0,self.tensorflow,Tensorflow: Error when trying to import,https://www.reddit.com/r/tensorflow/comments/6gr9k0/tensorflow_error_when_trying_to_import/,Anonymous1893,1497261441,"So I was trying to import tensorflow for the first time, and this monstrosity came up...

&gt;Traceback (most recent call last):
&gt;  File ""C:\Users\Azhaan Haq\Anaconda3\envs\tensorflow\lib\site-&gt;packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
&gt;    return importlib.import_module(mname)
&gt;  File ""C:\Users\Azhaan Haq\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
&gt;    return _bootstrap._gcd_import(name[level:], package, level)
&gt;  File ""&lt;frozen importlib._bootstrap&gt;"", line 986, in _gcd_import
&gt;  File ""&lt;frozen importlib._bootstrap&gt;"", line 969, in _find_and_load
&gt;  File ""&lt;frozen importlib._bootstrap&gt;"", line 958, in _find_and_load_unlocked
&gt;  File ""&lt;frozen importlib._bootstrap&gt;"", line 666, in _load_unlocked
&gt;  File ""&lt;frozen importlib._bootstrap&gt;"", line 577, in module_from_spec
&gt;  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 919, in create_module
&gt;  File ""&lt;frozen importlib._bootstrap&gt;"", line 222, in _call_with_frames_removed
&gt;ImportError: DLL load failed: The specified module could not be found.
&gt;
&gt;During handling of the above exception, another exception occurred:
&gt;
&gt;Traceback (most recent call last):
&gt;  File ""C:\Users\Azhaan Haq\Anaconda3\envs\tensorflow\lib\site-&gt;packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in &lt;module&gt;
&gt;    from tensorflow.python.pywrap_tensorflow_internal import *
&gt;  File ""C:\Users\Azhaan Haq\Anaconda3\envs\tensorflow\lib\site-&gt;packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in &lt;module&gt;
&gt;    _pywrap_tensorflow_internal = swig_import_helper()
&gt;  File ""C:\Users\Azhaan Haq\Anaconda3\envs\tensorflow\lib\site-&gt;packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
&gt;    return importlib.import_module('_pywrap_tensorflow_internal')
&gt;  File ""C:\Users\Azhaan Haq\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
&gt;    return _bootstrap._gcd_import(name[level:], package, level)
&gt;ImportError: No module named '_pywrap_tensorflow_internal'
&gt;
&gt;During handling of the above exception, another exception occurred:
&gt;
&gt;Traceback (most recent call last):
&gt;  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
&gt;  File ""C:\Users\Azhaan Haq\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 24, in &lt;module&gt;
&gt;    from tensorflow.python import *
&gt;  File ""C:\Users\Azhaan Haq\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py"", line 51, &gt;in &lt;module&gt;
&gt;    from tensorflow.python import pywrap_tensorflow
&gt;  File ""C:\Users\Azhaan Haq\Anaconda3\envs\tensorflow\lib\site-&gt;packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in &lt;module&gt;
&gt;    raise ImportError(msg)
&gt;ImportError: Traceback (most recent call last):
&gt;  File ""C:\Users\Azhaan Haq\Anaconda3\envs\tensorflow\lib\site-&gt;packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
&gt;    return importlib.import_module(mname)
&gt;  File ""C:\Users\Azhaan Haq\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
&gt;    return _bootstrap._gcd_import(name[level:], package, level)
&gt;  File ""&lt;frozen importlib._bootstrap&gt;"", line 986, in _gcd_import
&gt;  File ""&lt;frozen importlib._bootstrap&gt;"", line 969, in _find_and_load
&gt;  File ""&lt;frozen importlib._bootstrap&gt;"", line 958, in _find_and_load_unlocked
&gt;  File ""&lt;frozen importlib._bootstrap&gt;"", line 666, in _load_unlocked
&gt;  File ""&lt;frozen importlib._bootstrap&gt;"", line 577, in module_from_spec
&gt;  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 919, in create_module
&gt;  File ""&lt;frozen importlib._bootstrap&gt;"", line 222, in _call_with_frames_removed
&gt;ImportError: DLL load failed: The specified module could not be found.
&gt;
&gt;During handling of the above exception, another exception occurred:
&gt;
&gt;Traceback (most recent call last):
&gt;  File ""C:\Users\Azhaan Haq\Anaconda3\envs\tensorflow\lib\site-&gt;packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in &lt;module&gt;
&gt;    from tensorflow.python.pywrap_tensorflow_internal import *
&gt;  File ""C:\Users\Azhaan Haq\Anaconda3\envs\tensorflow\lib\site-&gt;packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in &lt;module&gt;
&gt;    _pywrap_tensorflow_internal = swig_import_helper()
&gt;  File ""C:\Users\Azhaan Haq\Anaconda3\envs\tensorflow\lib\site-&gt;packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
&gt;    return importlib.import_module('_pywrap_tensorflow_internal')
&gt;  File ""C:\Users\Azhaan Haq\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
&gt;    return _bootstrap._gcd_import(name[level:], package, level)
&gt;ImportError: No module named '_pywrap_tensorflow_internal'
&gt;
&gt;
&gt;Failed to load the native TensorFlow runtime.
&gt;
&gt;See https://www.tensorflow.org/install/install_sources#common_installation_problems
&gt;
&gt;for some common reasons and solutions.  Include the entire stack trace
&gt;above this error message when asking for help.**
&gt;

*ImportError: DLL load failed: The specified module could not be found.*

This was, as far as I know, the first error that comes up

I was using an Anaconda Environment if that helps
Thanks",2,1,False,self,,,,,
20,tensorflow,t5_3alkk,2017-6-14,2017,6,14,19,6h6jxt,self.tensorflow,Ive got problens with rnns. In what form do I input a seqeuce?,https://www.reddit.com/r/tensorflow/comments/6h6jxt/ive_got_problens_with_rnns_in_what_form_do_i/,HarambeTownley,1497434849,"I want it to learn 1,2,3,4,5... But I don't know in what format should the sequence be?

Python 3.5",0,3,False,self,,,,,
21,tensorflow,t5_3alkk,2017-6-15,2017,6,15,1,6h8fy1,self.tensorflow,Easy Question,https://www.reddit.com/r/tensorflow/comments/6h8fy1/easy_question/,WooxJr,1497456173,"I'm just getting started with Tensorflow: 

Using the example they give on the website, I initialize and print: 

node1 = tf.constant(3.0)
node2 = tf.constant(4.0)
print node1
print node2

and I get 

Tensor(""Const:0"", shape=(), dtype=float32)
Tensor(""Const_1:0"", shape=(), dtype=float32)

I'm just wondering: What's Const and what's Const_1?

I understand that I need to evaluate the nodes to actually get 3 and 4 but what am I looking at when I print the nodes themselves and what is ""const""?
Thanks!
",2,1,False,self,,,,,
22,tensorflow,t5_3alkk,2017-6-15,2017,6,15,11,6hc7t5,self.tensorflow,Question about activating tensor flow,https://www.reddit.com/r/tensorflow/comments/6hc7t5/question_about_activating_tensor_flow/,WooxJr,1497493231,"I'm still rather new to this, so it might seem like quite a stupid question:

Before I use tensor flow do I always need to do this: ""source ~/tensorflow/bin/activate"" when I open a new shell? 

I usually do that then do python filename.py

Is there a way to have tensor flow always ""activated"" in terminal (not sure what you call it). 

Thanks!",5,1,False,self,,,,,
23,tensorflow,t5_3alkk,2017-6-15,2017,6,15,23,6hf9lp,self.tensorflow,Easy q - Does this look like overfitting?,https://www.reddit.com/r/tensorflow/comments/6hf9lp/easy_q_does_this_look_like_overfitting/,[deleted],1497536177,[deleted],0,1,False,default,,,,,
24,tensorflow,t5_3alkk,2017-6-16,2017,6,16,1,6hfxon,self.tensorflow,Tensor Flow Optimizers,https://www.reddit.com/r/tensorflow/comments/6hfxon/tensor_flow_optimizers/,WooxJr,1497542546,"Okay I think I know the answer but I'm sure someone can clarify. 

For reference, I'm looking at this program:
http://web.stanford.edu/class/cs20si/lectures/notes_03.pdf

More specifically, the lines: 

loss  =  tf . square ( Y  -  Y_predicted ,  name = ""loss"")

optimizer  =  tf . train . GradientDescentOptimizer ( learning_rate = 0.001 ). minimize ( loss)


Not a deep question, but what I'm wondering is how does the optimizer know what to optimize with respect to? Here we have two variables, so since loss is a ""function"" (don't know if that's what we call it) of two variables, does it automatically know to minimize ""loss"" by changing those two variables (taking the gradient etc..)

Thanks!",1,1,False,self,,,,,
25,tensorflow,t5_3alkk,2017-6-16,2017,6,16,1,6hfy51,github.com,A new easy-to-use open source project which contains tutorials on how to implement different models using TensorFLow.,https://www.reddit.com/r/tensorflow/comments/6hfy51/a_new_easytouse_open_source_project_which/,irsina,1497542657,,0,3,False,default,,,,,
26,tensorflow,t5_3alkk,2017-6-16,2017,6,16,17,6hl6zv,github.com,TensorFlow 1.2.0 has been released!,https://www.reddit.com/r/tensorflow/comments/6hl6zv/tensorflow_120_has_been_released/,archdria,1497600442,,1,15,False,default,,,,,
27,tensorflow,t5_3alkk,2017-6-16,2017,6,16,22,6hmgzt,infoq.com,Google Released MobileNets: Efficient Pre-Trained Tensorflow Computer Vision Models,https://www.reddit.com/r/tensorflow/comments/6hmgzt/google_released_mobilenets_efficient_pretrained/,Dutchcheesehead,1497619210,,0,6,False,default,,,,,
28,tensorflow,t5_3alkk,2017-6-17,2017,6,17,0,6hn8xu,self.tensorflow,Question about batches,https://www.reddit.com/r/tensorflow/comments/6hn8xu/question_about_batches/,WooxJr,1497627168,"For reference, I'm looking at the code in: http://web.stanford.edu/class/cs20si/lectures/notes_03.pdf

I'm a little confused on the difference between n_epochs and n_batches? 

Specifically I'm looking at: 

...
n_batches  =   int ( MNIST . train . num_examples / batch_size)


for  i  in  range ( n_epochs ):   # train the model n_epochs times


for  _  in  range ( n_batches ):
...


What I don't get is the need for epochs. Why can't we just say batch_size = 100, then num_batches = amount of data/batch_size then just iterate over num_batches so that we use all the training data?",1,1,False,self,,,,,
29,tensorflow,t5_3alkk,2017-6-17,2017,6,17,14,6hromu,m.youtube.com,"So I've been wondering, the new Kurzgesagt video about AI talks about Bots that use Freelancers to learn what tasks they do to eventually replace that entire job. Are there any real examples of this?",https://www.reddit.com/r/tensorflow/comments/6hromu/so_ive_been_wondering_the_new_kurzgesagt_video/,thisisRio,1497675874,,2,0,False,image,,,,,
30,tensorflow,t5_3alkk,2017-6-17,2017,6,17,15,6hs0t5,self.tensorflow,Cheapest iPhone I can buy to test TF?,https://www.reddit.com/r/tensorflow/comments/6hs0t5/cheapest_iphone_i_can_buy_to_test_tf/,mushm0m,1497681483,"Hey guys, I'm attempting to make an iOS app that relies on TensorFlow for image recognition. Sadly I don't have an iPhone, so I need to buy one for testing purposes.

I'd like to buy the oldest/cheapest iPhone that could still feasibly run TensorFlow to do image recognition from the camera without too many issues. 

I'm wondering if an iPhone 4/4s would be OK, or if I need a 5 (or newer than a 5)?",3,4,False,self,,,,,
31,tensorflow,t5_3alkk,2017-6-18,2017,6,18,7,6hwaze,datasciencecentral.com,Image Segmentation using deconvolution layer in Tensorflow,https://www.reddit.com/r/tensorflow/comments/6hwaze/image_segmentation_using_deconvolution_layer_in/,psangrene,1497739041,,0,1,False,default,,,,,
32,tensorflow,t5_3alkk,2017-6-18,2017,6,18,19,6hz3fq,self.tensorflow,Using Tensorflow-gpu with Keras,https://www.reddit.com/r/tensorflow/comments/6hz3fq/using_tensorflowgpu_with_keras/,trillykins,1497783493,"I have gotten Tensorflow up and running on Windows 10 using Python 3.5.2 and made a CNN. Problem is, it's very slow because it is using my CPU instead of my Nvidia GTX 1070 GPU. I have already installed tensorflow-gpu, but it is not registering my GPU for some reason.

When I run this: 

    from tensorflow.python.client import device_lib
    device_lib.list_local_devices() 

It only shows ` ""/cpu:0""`.

Anyone have any suggestions? I've tried following the installation steps mention in [this thread](https://github.com/fchollet/keras/issues/5776), but that didn't change anything. If I install tensorflow, then install Keras, uninstall and re-install Tensorflow-GPU, Tensorflow is not found at all when running the code. If I install Tensorflow and then Tensforflow-GPU, it still only uses the CPU.",5,2,False,self,,,,,
33,tensorflow,t5_3alkk,2017-6-19,2017,6,19,5,6i1z4t,self.tensorflow,Question on Tensorflow seq2seq implementation,https://www.reddit.com/r/tensorflow/comments/6i1z4t/question_on_tensorflow_seq2seq_implementation/,hassanzadeh,1497819186,"Hi everyone,
The following code snippet is from Tensorflow github page for an RNN decoder of a seq2seq model. What I don't understand is the part that tries to reuse the variables. I'm not sure what exactly are we reusing here and why, any help?
    
    with variable_scope.variable_scope(scope or ""rnn_decoder""):
      state = initial_state
      outputs = []
      prev = None
      for i, inp in enumerate(decoder_inputs):
        if loop_function is not None and prev is not None:
          with variable_scope.variable_scope(""loop_function"", reuse=True):
            inp = loop_function(prev, i)
        if i &gt; 0:
          variable_scope.get_variable_scope().reuse_variables()
        output, state = cell(inp, state)
        outputs.append(output)
        if loop_function is not None:
          prev = output
    return outputs, state",9,2,False,self,,,,,
34,tensorflow,t5_3alkk,2017-6-19,2017,6,19,8,6i2tqv,self.tensorflow,seq2seq with dynamic rnn,https://www.reddit.com/r/tensorflow/comments/6i2tqv/seq2seq_with_dynamic_rnn/,hassanzadeh,1497828893,"Hi Everyone,
It looks like the seq2seq model in tf is written with the static_rnn, but my input and output seq len is not determined, is there any implementation of that with dynamic_rnn?
Also, I see from the implementation that in order to have the encoder vs decoder cell weights untied, in the decoder the reuse flag is set to one from the second step, however, according to the latest release of the TF, the scope is cached and reused at every step, which means that the current implementation for the untied version is similar to the tied one, am I right?",1,1,False,self,,,,,
35,tensorflow,t5_3alkk,2017-6-19,2017,6,19,13,6i45fn,github.com,Using 3D Convolutional Neural Networks for Speaker Verification,https://www.reddit.com/r/tensorflow/comments/6i45fn/using_3d_convolutional_neural_networks_for/,irsina,1497845495,,2,5,False,default,,,,,
36,tensorflow,t5_3alkk,2017-6-20,2017,6,20,1,6i7cub,self.tensorflow,question on re-training the final layer of inception5h (and question on its structure),https://www.reddit.com/r/tensorflow/comments/6i7cub/question_on_retraining_the_final_layer_of/,hugokhf,1497888280,"
So I am looking into retraining the inception5h model that is used in the TFClassify demo: 

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md

I want to retrain the final layer to add a new label as the output. However, from my research in the internet, it seems that it is impossible to retrain the 5h model as it is quantized and some of the layers have been stripped out to make it run more effeciently in the mobile. So have anyone around here tried retraining the 5h model??


Also, I am looking into the exact structure of inception5h or how it is made. From what I understand, it has the exact same structure as inceptionv3, but I cannot find any documentation as to what was done to it to make it more mobile efficient. Can anyone point me to a direction where I can look it up? thanks. Are there tutorials showing how to convert a v3 model into a 5h model?

thanks in advance",0,1,False,self,,,,,
37,tensorflow,t5_3alkk,2017-6-20,2017,6,20,13,6ibofk,self.tensorflow,Any reason why the GPU wouldn't be used when running this?,https://www.reddit.com/r/tensorflow/comments/6ibofk/any_reason_why_the_gpu_wouldnt_be_used_when/,ERLindeman,1497931495,"I have CUDA v8.0, cuDNNv5.1, Python3.5.2, tensorflow-gpu installed with pip(3), code run with IDLE; PC has 13-6100, GTX 1050 Ti, with CPU only the first cycle reports about an hour remaining. 

If there is anything else I need to attach please let me know. (I assume I need a log from somewhere, but I can't seem to figure it out)

            #Imports
    import tensorflow as tf
    import time, math, datetime
    from tensorflow.examples.tutorials.mnist import input_data
        #Data Definitions
    mnist = input_data.read_data_sets(""MNIST_data"", one_hot = True)

            #Graph
        #Definitions
    n_nodes_hl1 = 100
    n_nodes_hl2 = 100
    n_nodes_hl3 = 100
    data_size = 784
    n_classes = 10
    hm_cycles = 512
    batch_size = round((data_size * n_classes)/hm_cycles)
    learning_rate = 0.001
    final_output = """"
    cycle_time_l = []
    #constants

    #Variables
    hl1_w = tf.Variable(tf.random_normal([data_size, n_nodes_hl1]))
    hl1_b = tf.Variable(tf.random_normal([n_nodes_hl1]))
    hl2_w = tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2]))
    hl2_b = tf.Variable(tf.random_normal([n_nodes_hl2]))
    hl3_w = tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3]))
    hl3_b = tf.Variable(tf.random_normal([n_nodes_hl3]))
    ol_w = tf.Variable(tf.random_normal([n_nodes_hl3, n_classes]))
    ol_b = tf.Variable(tf.random_normal([n_classes]))
    #placeholders
    x = tf.placeholder('float', [None, data_size])
    y = tf.placeholder('float', [None, n_classes])
        #Computing
    def neural_network_model(data):
        hidden_layer_1 = {hl1_w,hl1_b}
        hidden_layer_2 = {hl2_w,hl2_b}
        hidden_layer_3 = {hl3_w,hl3_b}
        output_layer = {ol_w,ol_b}
        l1 = tf.add(tf.matmul(data, hl1_w), hl1_b)
        l1 = tf.nn.relu(l1)
        l2 = tf.add(tf.matmul(l1, hl2_w), hl2_b)
        l2 = tf.nn.relu(l2)
        l3 = tf.add(tf.matmul(l2, hl3_w), hl3_b)
        l3 = tf.nn.relu(l3)
        output = tf.add(tf.matmul(l3, ol_w), ol_b)
        return output
    def train_neural_network(x):
        prediction = neural_network_model(x)
        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))
        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)


            #Sessions
        #Loop
        with tf.Session() as sess1:
            sess1.run(tf.global_variables_initializer())
            print('Learning from Batches of ' + str(batch_size) + ' data points for ' + str(hm_cycles) + ' Cycles.')
            for cycle in range(hm_cycles):
                cycle_time_s = time.time()
                cycle_loss = 0
                for _ in range(int(mnist.train.num_examples/batch_size)):
                    ex, ey = mnist.train.next_batch(batch_size)
                    _, c = sess1.run([optimizer, cost], feed_dict = {x: ex, y: ey})
                    cycle_loss += c
                correct = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))
                accuracy = tf.reduce_mean(tf.cast(correct,'float'))
                percentage = 100 * accuracy.eval({x: mnist.test.images, y:mnist.test.labels})
                cycle_time_e = time.time()
                cycle_time = (cycle_time_e - cycle_time_s)
                cycle_time_l.insert(cycle, cycle_time)
                cycle_time_a = (sum(cycle_time_l)/len(cycle_time_l))
                seconds_remaining = math.floor((hm_cycles - cycle - 1)*(cycle_time_a))
                print('Cycle ' + str(cycle + 1) + ' of ' + str(hm_cycles) + ' completed in ' + str(round((cycle_time),3)) + ' seconds. Loss: ' + str(round((cycle_loss),3)) + ' | Accuracy: ' + str(round((percentage),3)) + '% | ' + str(datetime.timedelta(seconds= (seconds_remaining) )) + ' remaining')
                if cycle + 1 == hm_cycles or cycle_loss == 0 or accuracy == 1:
                    run_time = cycle_time_a * hm_cycles
                    str(datetime.timedelta(seconds=run_time))
                    rt_m, rt_s = divmod(run_time, 60)
                    rt_h, rt_m = divmod(rt_m, 60)
                    efficiency = (percentage ** 3)/(run_time)
                    print('Finished. Cycles: ' + str(hm_cycles) + ' | Batch Size: ' + str(batch_size) + ' | Time: ' + str(datetime.timedelta(seconds= (run_time) )) + ' | Final Loss: ' + str(round((cycle_loss),3)) + ' | Final Accuracy: ' + str(round((percentage),3)) + '% | Efficiency: ' + str(round((efficiency),4)))
        #Sequential
    #Create

    #Run/Close
    train_neural_network(x)


            #Output
    print(final_output)
",4,1,False,self,,,,,
38,tensorflow,t5_3alkk,2017-6-20,2017,6,20,23,6iefns,self.tensorflow,Loss Does Change?,https://www.reddit.com/r/tensorflow/comments/6iefns/loss_does_change/,FiniteDelight,1497969633,"TL;DR: I can't find my mistake when using the Tensorflow optimizer to train an extremely small neural net. The loss either doesn't move or moves once then gets stuck (it seems to really like the value 0.693147 which is ln(2)...).

Issue and Code: I'm trying to implement the 12-net part of the cascade classifier in Li et al (here) in Tensorflow. It's an extremely simple net, but nothing I try seems to get it training.

    import tensorflow as tf
    import tensorflow.contrib.slim as slim
    import cv2
    import numpy as np


    input_tensor = tf.placeholder(tf.float32, shape=[1, 12, 12, 3])
    input_label = tf.placeholder(tf.float16, shape=[1, 2])
    conv_1 = slim.conv2d(input_tensor, 16, (3, 3), scope='conv1')
    pool_1 = slim.max_pool2d(conv_1, (3, 3), 2, scope='pool1')
    flatten = slim.flatten(pool_1)
    fully_con = slim.fully_connected(flatten, 16, scope='full_con')
    fully_con_2 = slim.fully_connected(fully_con, 2, scope='output')
    probs = tf.nn.softmax(fully_con_2)
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=input_label, logits=fully_con_2))
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=.001).minimize(loss)

This defines the net. It takes in a (for now, single) 12x12 image and label, does a single 3x3 convolution with stride 1 and 16 filters, a 3x3 max pool with stride 2, then fully connects to 16 features, and finally makes a binary classification. I am able to perform a forward pass through the code, so I don't think the issue is here. This is my training loop - I have 3 12x12 images (2 faces, 1 tree) and just alternately feed them to the optimizer (clearly not best training practice, but I'm just trying to get it to work):

    if __name__ == '__main__':
        im = cv2.imread('resized.jpg').reshape(1, 12, 12, 3).astype('float16')
        im2 = cv2.imread('resized2.jpg').reshape(1, 12, 12, 3).astype('float16')
        im3 = cv2.imread('resize3.jpg').reshape(1, 12, 12, 3).astype('float16')
        im_lab_1 = np.array([[0, 1]], dtype='float16')
        im_lab_2 = np.array([[0, 1]], dtype='float16')
        im_lab_3 = np.array([[1, 0]], dtype='float16')

        with tf.Session() as sess:
            sess.run(tf.global_variables_initializer())
            print(sess.run(loss, feed_dict={input_tensor: im3, input_label: im_lab_3}))
            for i in range(50000):
                 if i % 3 == 0:
                    # _, l = sess.run([optimizer, loss], feed_dict=feed1)
                    # print(l)
                    optimizer.run(feed_dict={input_tensor: im, input_label: im_lab_1})
                 elif i % 4 == 0:
                    # _, l = sess.run([optimizer, loss], feed_dict=feed2)
                    # print(l)
                    optimizer.run(feed_dict={input_tensor: im2, input_label: im_lab_2})
                 elif i % 5 == 0:
                    optimizer.run(feed_dict={input_tensor: im3, input_label: im_lab_3})
            print(sess.run(loss, feed_dict={input_tensor: im3, input_label: im_lab_3}))

I've tried both optimizer.run(...) and the commented out sess.run([optimizer, loss]...). The first sess.run(loss...) seems to spit out something correct, but after that, the loss gets stuck and never moves again. Clearly, I'm doing something wrong here, and any help would be appreciated!
",7,2,False,self,,,,,
39,tensorflow,t5_3alkk,2017-6-22,2017,6,22,22,6itjuq,self.tensorflow,"Looking for knowledgeable TensorFlow enthusiast, possible paid gig, thanks guys! &lt;3",https://www.reddit.com/r/tensorflow/comments/6itjuq/looking_for_knowledgeable_tensorflow_enthusiast/,heyguysitsdaniel,1498138886,"Hey TensorFlow reddit community, hope you guys are having a good day.

Pardon my directness guys. I neither program nor do I know the true extent of what TensorFlow is capable of.

Right now is a truly exciting time for finding out though.

I'm looking for a consultant, someone knowledgeable about what TensorFlow is truly capable of. I have investors eagerly waiting for these answers (I know that kind of sounds pretentious, but it's true, haha). If things go the right way, some kind of business venture would hopefully follow. 

**I know this is very vague and sounds kind of shady. :P** 

I agree, I don't trust me re-reading this shit post, haha. But I'd just like a conversation at this point. If anyone could point me in the right direction in who to ask, or just wants to chat about it, I would be extremely appreciative! 

thank you in advance for helping me realize some business dreams here guys!

**Thanks guys! Have a good day!** ",0,0,False,self,,,,,
40,tensorflow,t5_3alkk,2017-6-24,2017,6,24,3,6j35cv,blog.insightdatascience.com,Using Deep Learning to Reconstruct High-Resolution Audio &amp; TensorFlow contribution,https://www.reddit.com/r/tensorflow/comments/6j35cv/using_deep_learning_to_reconstruct_highresolution/,mwakanosya,1498243115,,0,3,False,default,,,,,
41,tensorflow,t5_3alkk,2017-6-24,2017,6,24,4,6j3afc,self.tensorflow,"Swift, Perfect and Tensorflow",https://www.reddit.com/r/tensorflow/comments/6j3afc/swift_perfect_and_tensorflow/,perfectlysoft,1498244443,"Hello all! 

After months of work, we have reached a milestone with our Server Side Swift (Perfect) implementation of Tensorflow. It works!

https://github.com/PerfectlySoft/Perfect-TensorFlow

We'd really appreciate some enterprising devs with Tensorflow experience to come give Server Side Swift a try and make suggestions on how we can improve the project. Maybe even join the Perfect community while you are at it!

We are truly excited for some feedback! It's been a huge effort.

Join Slack at http://Perfect.ly to discuss live.

Thanks!",2,3,False,self,,,,,
42,tensorflow,t5_3alkk,2017-6-24,2017,6,24,10,6j5kk4,self.tensorflow,Trouble with tf.contrib.learn.learn_runner.run,https://www.reddit.com/r/tensorflow/comments/6j5kk4/trouble_with_tfcontriblearnlearn_runnerrun/,[deleted],1498268967,[deleted],0,1,False,default,,,,,
43,tensorflow,t5_3alkk,2017-6-24,2017,6,24,12,6j5xgl,self.tensorflow,"A question about ""trainable"" in Tensorflow",https://www.reddit.com/r/tensorflow/comments/6j5xgl/a_question_about_trainable_in_tensorflow/,WooxJr,1498273628,"I've noticed in tutorials that when we declare global_step, e.g., 
""global_step = tf.Variable(initial_value=0,
                          name='global_step', trainable=False)"" 
we set trainable to false. What I'm wondering is: is this necessary, or just a good habit? Say the loss function depends only on a matrix and a bias (W and b), then the optimization would only try to change those two variables when taking a step in the direction of the gradient, no? 

Basically, what I'm wondering is: why do we need to specify that it's not trainable if the value we are minimizing doesn't even depend on it? ",1,1,False,self,,,,,
44,tensorflow,t5_3alkk,2017-6-24,2017,6,24,22,6j82fe,self.tensorflow,Any blog post or tutorial regarding the model creation for Android tensorflow demo?,https://www.reddit.com/r/tensorflow/comments/6j82fe/any_blog_post_or_tutorial_regarding_the_model/,me_random_123,1498309663,"I am specifically talking about https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/StylizeActivity.java

The model they have used here covers 26 styles and the size is in KBs. 

All the posts that I have seen online or on Github can create only one style, and even after quantization, the size for that model of just one style is at least 1 MB. 

Has Google posted how they trained the model, or is there any other post regarding this? Thanks",1,1,False,self,,,,,
45,tensorflow,t5_3alkk,2017-6-25,2017,6,25,9,6jbfzs,self.tensorflow,Does anybody know how to install tensorflow on a headless Raspberry Pi 3 running 32 bit Kali Linux?,https://www.reddit.com/r/tensorflow/comments/6jbfzs/does_anybody_know_how_to_install_tensorflow_on_a/,madogson,1498349519,"I have been grinding at this for about 2 hours now. Numerous tutorials, many roadblocks and too many 'architecture not supported' messages to count have led me nowhere. Really hope someone will come by and at least take pity on me.",6,1,False,self,,,,,
46,tensorflow,t5_3alkk,2017-6-26,2017,6,26,3,6jfwa1,datasciencecentral.com,Deep Learning with TensorFlow in Python,https://www.reddit.com/r/tensorflow/comments/6jfwa1/deep_learning_with_tensorflow_in_python/,psangrene,1498415532,,0,12,False,default,,,,,
47,tensorflow,t5_3alkk,2017-6-27,2017,6,27,0,6jlvfw,github.com,Novel Deep Atrous CNN Architecture for Sentiment Analysis  Tensorflow Implementation,https://www.reddit.com/r/tensorflow/comments/6jlvfw/novel_deep_atrous_cnn_architecture_for_sentiment/,gvssvg,1498491787,,0,3,False,default,,,,,
48,tensorflow,t5_3alkk,2017-6-27,2017,6,27,1,6jm9a5,self.tensorflow,AWS vs. New Laptop for Tensorflow GPU?,https://www.reddit.com/r/tensorflow/comments/6jm9a5/aws_vs_new_laptop_for_tensorflow_gpu/,ragnarkar,1498495316,"I haven't gotten a new computer since 2012 when I got a Macbook with only a 1 GB GPU which frequently runs out of memory when running even most toy Tensorflow examples I find online.

As a newbie who is still learning the ropes, I'm wondering if it's worthwhile to invest in a new computer just to run Tensorflow on the GPU or it makes more economical sense to get a AWS account with GPU support and pay as I go.  Thanks.",8,3,False,self,,,,,
49,tensorflow,t5_3alkk,2017-6-27,2017,6,27,4,6jnd2j,self.tensorflow,Need some help with retraining inception v3,https://www.reddit.com/r/tensorflow/comments/6jnd2j/need_some_help_with_retraining_inception_v3/,Thebroser,1498505438,"My experience with tensorflow involves successfully using the MNIST data set, and a positive and negative string program. I decided to try and give inception a try and downloaded to windows the models from github but whenever I try to run inception_train.py I keep getting the error ""No module named 'inception' "" and I am not sure how to fix it. Tensorflow was installed natively with pip, and I am using python 3.5, as well as CUDA 8.0 and CudNN 5.1 with an nvidia 960m.",1,1,False,self,,,,,
50,tensorflow,t5_3alkk,2017-6-27,2017,6,27,15,6jqy13,self.tensorflow,TensorFlow vs Pytorch,https://www.reddit.com/r/tensorflow/comments/6jqy13/tensorflow_vs_pytorch/,yoniker,1498545318,"Hi guys!

So I've been playing around for a few weeks with both frameworks.
I know that TF has an amazing PR thanks to Google, and many people use it,including researchers (academic articles).

But I've found TF's API to be absolutely TERRIBLE and awkward (not intuitive at all), 
while I was able to implement my ideas easily using PyTorch.
So I want to know your honest opinion-which one is your favorite and why?",9,3,False,self,,,,,
51,tensorflow,t5_3alkk,2017-6-27,2017,6,27,23,6jt39x,self.tensorflow,Getting Started with TensorFlow,https://www.reddit.com/r/tensorflow/comments/6jt39x/getting_started_with_tensorflow/,verystrangecloud,1498574292,"Hey there guys,

I am currently a student who only has some basic familiarity with objected orientated programming (Java and some Arduino coding; so very little programming knowledge) Any recommendations on tutorials for Tensorflow and Python language tutorials?",6,7,False,self,,,,,
52,tensorflow,t5_3alkk,2017-6-29,2017,6,29,21,6k85bm,github.com,"tfgo: Tensorflow + Go, the gopher way",https://www.reddit.com/r/tensorflow/comments/6k85bm/tfgo_tensorflow_go_the_gopher_way/,pgaleone,1498740941,,0,2,False,default,,,,,
53,tensorflow,t5_3alkk,2017-6-29,2017,6,29,22,6k8bj2,self.tensorflow,"Looking for expert: loss distribution functions, matrix math, discontinuous nonlinear systems of equations, comparative geometry =&gt; [tensor math] _AND_ how to profile and optimize CPU time, unrolling and parallelizing to achieve maximum speed.",https://www.reddit.com/r/tensorflow/comments/6k8bj2/looking_for_expert_loss_distribution_functions/,gar37bic,1498742838,"I'm posting for a friend. He's the only person he can find who can do this, and he doesn't have time. I believe it to be a paid gig. PM if you are _very good_. You need to understand the math to, for example, use L'Hopital's Rule. This involves Tensor RT2.",0,0,False,self,,,,,
54,tensorflow,t5_3alkk,2017-6-30,2017,6,30,1,6k9gdg,self.tensorflow,Denoising Autoencoder output,https://www.reddit.com/r/tensorflow/comments/6k9gdg/denoising_autoencoder_output/,AlloraQuesto,1498753521,"Good evening,

I've a question on denoising autoencoder created with tensorflow, for a project i need to create this ANN and use it for determinate anomaly detection on a data series and this part it's ok.
The question is: the output of a autoencoder is the reconstruction error, is it possible return the column/s (or features) who have the anomaly? 
Can someone help me? Thanks!",0,1,False,self,,,,,
55,tensorflow,t5_3alkk,2017-6-30,2017,6,30,7,6kbwow,i.redd.it,"The harder it is, the easier it is",https://www.reddit.com/r/tensorflow/comments/6kbwow/the_harder_it_is_the_easier_it_is/,iamdamned,1498775663,,1,24,False,image,,,,,
56,tensorflow,t5_3alkk,2017-6-30,2017,6,30,11,6kd9d0,self.tensorflow,Training a CNN with probability labels,https://www.reddit.com/r/tensorflow/comments/6kd9d0/training_a_cnn_with_probability_labels/,shadow12348,1498790981,"I'm trying to do simple regression on images and ran into a dead end figuring out how to pick a point on the sigmoid curve in the output layer of a CNN. The training data is a large set of images and each image has a score associated with it between [0,8] that corresponds to how good it looks (Based on some other irrelevant metric). Now, if I was to use this data to train a CNN and scale [0,8] to [0,1], I'd like to be able to give in a random image later and expect the network to return to me a value between 0 and 1 that corresponds to the score of that image. In Tensorflow particularly how do I achieve this?

Any help is appreciated, thanks!",2,1,False,self,,,,,
57,tensorflow,t5_3alkk,2017-6-30,2017,6,30,17,6kelct,self.tensorflow,"I don't understand ""Getting Started"" guide on TensorFlow. Please suggest some basic Machine Learning and Tensor Flow fundamentals (I am an SOC Firmware R&amp;D Engineer)",https://www.reddit.com/r/tensorflow/comments/6kelct/i_dont_understand_getting_started_guide_on/,user0user,1498809969,,4,1,False,self,,,,,
