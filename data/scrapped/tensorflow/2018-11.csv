,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2018-11-2,2018,11,2,23,9tkjap,Contributing to privacy-preserving ML: Crafting building blocks for secure AI,https://www.reddit.com/r/tensorflow/comments/9tkjap/contributing_to_privacypreserving_ml_crafting/,morgangiraud,1541168632,,0,1
1,2018-11-3,2018,11,3,3,9tmzmg,Contributing to privacy-preserving ML: Crafting building blocks for secure AI,https://www.reddit.com/r/tensorflow/comments/9tmzmg/contributing_to_privacypreserving_ml_crafting/,morgangiraud,1541185160,,0,1
2,2018-11-3,2018,11,3,11,9tqpiy,Error setting up tensorflow gpu,https://www.reddit.com/r/tensorflow/comments/9tqpiy/error_setting_up_tensorflow_gpu/,Yogi_DMT,1541213104,"I'm getting this error while trying to import tensorflow gpu. The files are on my path

https://imgur.com/WkoTpdF

and i do see the file it is asking for

https://imgur.com/7wGoOD3

The solution to this error seems to be that i downloaded the wrong cuda ie. cudart64_91.dll but as you can see i have the right version.

any ideas?",1,1
3,2018-11-3,2018,11,3,13,9trb7y,how can i solve this issue on implementing virtuLENV,https://www.reddit.com/r/tensorflow/comments/9trb7y/how_can_i_solve_this_issue_on_implementing/,maprogrammer,1541218730,"I have ubuntu 18.04 64 bit . i bought a hard ssd . and gpu is intel Haswall, laptob hp probook

i typed sudo apt install virtualenv on terminal

but i cant complete steps for installing and use tensorflow codes for object detection task and inpplementing algorithmsOR USING SPYDER AND NOTEBOOK

&amp;#x200B;",6,1
4,2018-11-4,2018,11,4,7,9tyh7t,What exactly are the parameters of tf.metric.mean_iou?,https://www.reddit.com/r/tensorflow/comments/9tyh7t/what_exactly_are_the_parameters_of_tfmetricmean/,pocketMAD,1541285263,"I'm somewhat new to tensorflow, but I'm really having a lot of fun learning it. I have a somewhat incomplete understanding of graphs and sessions.

 I'm part of a robotics club on the machine learning team. They have a somewhat good, somewhat shitty SSD and I'm tasked to create a model evaluation script using tf.metric. I am able to splice out a 4-dim vector from the model's output -- (x\_min, y\_min, x\_max, y\_max) -- which is the opposite corners of the single bounding box on the image. Since I have the ground truth and prediction bboxes, how do I use tf.metric.mean\_iou to utilize it?

Also, the prediction and labels are correct, I've checked.

Here is some code that I have:

    print(label) // correct ground truth (x_min, y_min, x_max, y_max)
    print(pred_bbox) // correct prediction (x_min, y_min, x_max, y_max)
    iou, conf_mat = tf.metrics.mean_iou(tf.constant(label), tf.constant(pred_bbox), 2) // 2 classes because there are only two objects I trained the model on
    sess.run(tf.local_variables_initializer())
    miou = sess.run([iou])
    print(miou)
    

This outputs '\[0.0\]' whenever I feed the program an image. Any idea what I am doing wrong? I am happy to give you more info if you ask. :) ",0,1
5,2018-11-4,2018,11,4,19,9u2fqz,How can DNNClassifier handle imbalanced data,https://www.reddit.com/r/tensorflow/comments/9u2fqz/how_can_dnnclassifier_handle_imbalanced_data/,6ixEggs,1541327847,"Hi guys, I am a newbie to TensorFlow. I am now using estimator.DNNClassifier to handle binary classification. Ive got 87% of accuracy for my test data. But in fact, 9X% for negative and only 10% for positive.

The dataset gives 70,000 training data and only ~600 of them are positive. I can get 90% accuracy by returning all 0s. But it means nothing. 

Is there a way to tackle this problem? ",4,1
6,2018-11-5,2018,11,5,2,9u52qz,ImportError,https://www.reddit.com/r/tensorflow/comments/9u52qz/importerror/,duplido,1541351835,"So I am trying to get tensorflow to run, but I always get an error that there is no module named \_pywrap\_tensorflow\_internal.

&amp;#x200B;

Googling it suggested that I am missing a PATH reference, which I don't there also isn't any missing dll mentioned in the traceback. I tried a lot of things and am out of ideas right now, maybe someone can help me ?

&amp;#x200B;

The entire traceback

Traceback (most recent call last):

  File ""C:\\Program Files (x86)\\Python35-32\\lib\\site-packages\\tensorflow\\python\\pywrap\_tensorflow\_internal.py"", line 18, in swig\_import\_helper

fp, pathname, description = imp.find\_module('\_pywrap\_tensorflow\_internal', \[dirname(\_\_file\_\_)\])

  File ""C:\\Program Files (x86)\\Python35-32\\lib\\[imp.py](https://imp.py)"", line 297, in find\_module

raise ImportError(\_ERR\_MSG.format(name), name=name)

ImportError: No module named '\_pywrap\_tensorflow\_internal'

&amp;#x200B;

During handling of the above exception, another exception occurred:

&amp;#x200B;

Traceback (most recent call last):

  File ""C:\\Program Files (x86)\\Python35-32\\lib\\site-packages\\tensorflow\\python\\pywrap\_tensorflow.py"", line 58, in &lt;module&gt;

from tensorflow.python.pywrap\_tensorflow\_internal import \*

  File ""C:\\Program Files (x86)\\Python35-32\\lib\\site-packages\\tensorflow\\python\\pywrap\_tensorflow\_internal.py"", line 28, in &lt;module&gt;

\_pywrap\_tensorflow\_internal = swig\_import\_helper()

  File ""C:\\Program Files (x86)\\Python35-32\\lib\\site-packages\\tensorflow\\python\\pywrap\_tensorflow\_internal.py"", line 20, in swig\_import\_helper

import \_pywrap\_tensorflow\_internal

ImportError: No module named '\_pywrap\_tensorflow\_internal'

&amp;#x200B;

During handling of the above exception, another exception occurred:

&amp;#x200B;

Traceback (most recent call last):

  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;

  File ""C:\\Program Files (x86)\\Python35-32\\lib\\site-packages\\tensorflow\\\_\_init\_\_.py"", line 24, in &lt;module&gt;

from tensorflow.python import \*

  File ""C:\\Program Files (x86)\\Python35-32\\lib\\site-packages\\tensorflow\\python\\\_\_init\_\_.py"", line 49, in &lt;module&gt;

from tensorflow.python import pywrap\_tensorflow

  File ""C:\\Program Files (x86)\\Python35-32\\lib\\site-packages\\tensorflow\\python\\pywrap\_tensorflow.py"", line 74, in &lt;module&gt;

raise ImportError(msg)

ImportError: Traceback (most recent call last):

  File ""C:\\Program Files (x86)\\Python35-32\\lib\\site-packages\\tensorflow\\python\\pywrap\_tensorflow\_internal.py"", line 18, in swig\_import\_helper

fp, pathname, description = imp.find\_module('\_pywrap\_tensorflow\_internal', \[dirname(\_\_file\_\_)\])

  File ""C:\\Program Files (x86)\\Python35-32\\lib\\[imp.py](https://imp.py)"", line 297, in find\_module

raise ImportError(\_ERR\_MSG.format(name), name=name)

ImportError: No module named '\_pywrap\_tensorflow\_internal'

&amp;#x200B;

During handling of the above exception, another exception occurred:

&amp;#x200B;

Traceback (most recent call last):

  File ""C:\\Program Files (x86)\\Python35-32\\lib\\site-packages\\tensorflow\\python\\pywrap\_tensorflow.py"", line 58, in &lt;module&gt;

from tensorflow.python.pywrap\_tensorflow\_internal import \*

  File ""C:\\Program Files (x86)\\Python35-32\\lib\\site-packages\\tensorflow\\python\\pywrap\_tensorflow\_internal.py"", line 28, in &lt;module&gt;

\_pywrap\_tensorflow\_internal = swig\_import\_helper()

  File ""C:\\Program Files (x86)\\Python35-32\\lib\\site-packages\\tensorflow\\python\\pywrap\_tensorflow\_internal.py"", line 20, in swig\_import\_helper

import \_pywrap\_tensorflow\_internal

ImportError: No module named '\_pywrap\_tensorflow\_internal'

&amp;#x200B;

&amp;#x200B;

Failed to load the native TensorFlow runtime.

&amp;#x200B;

See [https://www.tensorflow.org/install/install\_sources#common\_installation\_problems](https://www.tensorflow.org/install/install_sources#common_installation_problems)

&amp;#x200B;

for some common reasons and solutions.  Include the entire stack trace

above this error message when asking for help.",16,1
7,2018-11-5,2018,11,5,4,9u675p,Tensorflow loading and saving models not correct way?,https://www.reddit.com/r/tensorflow/comments/9u675p/tensorflow_loading_and_saving_models_not_correct/,Jandevries101,1541359235,"hi, i am not sure if this method works for saving and loading, especially since i do tf init twice.... could somebody confirm for me? (No i can't print any NN variables, please just keep it if the code is correct, thanks)

&amp;#x200B;

**For saving:**

&amp;#x200B;

1 i just create my variables

2 i do [self.sess.run](https://self.sess.run)(tf.global\_variables\_initializer()) (inside the class)

3 do some training shizzles

4 when stop training: (Outside the class) 

                init = tf.global_variables_initializer()                
                saver = tf.train.Saver()
                with tf.Session() as sess:
                    sess.run(init)
                    save_path = saver.save(sess, ""C:\\Users\\Gebruiker\\Downloads\\model.ckpt"")

&amp;#x200B;

**For Loading:**

&amp;#x200B;

1 i just create my variables

2 i do [self.sess.run](https://self.sess.run)(tf.global\_variables\_initializer()) (inside the class)

3 i call my load function (inside the class)

&amp;#x200B;

        def SaveDing(): 
    			
            print(checksavedmodelexists[0])
            my_file = Path(""C:\\Users\\Gebruiker\\Downloads\\model.ckpt.index"")   
    		
            if not checksavedmodelexists[0]: #if there is no file   
                checksavedmodelexists[0] = True            
                if my_file.is_file():            
                    saver = tf.train.Saver()                
                    with tf.Session() as sess:
                        saver.restore(sess, ""C:\\Users\\Gebruiker\\Downloads\\model.ckpt"")                
                    print(""Loaded in previous Saved Model."")                
                else:
                    print(""no previous model found, will start from here then."")                
            else:
                print(""If you see this message there may be something wrong."")        
            return sess

&amp;#x200B;

it should check if a model exists (a saved one) and then load that one in, if not it should do nothing so the algorithme can train the current model of course, very logic....

&amp;#x200B;

&amp;#x200B;

i am fairly confident that saving and loading/saving or loading is not workin, because i can see a performance drop when i try to load in the older model.  see picture: 

&amp;#x200B;

you can clearly see where i started my next run on the same model

[https://cdn.discordapp.com/attachments/455970083617898496/504351789265715210/2018-10-23\_19h25\_39.png](https://cdn.discordapp.com/attachments/455970083617898496/504351789265715210/2018-10-23_19h25_39.png)

&amp;#x200B;

the code doesn't give any errors and the prints print like they ""should""

&amp;#x200B;

i believe its something about that double init call when saving?

please let me know what you think / what the fix is

&amp;#x200B;

thanks for reading,

&amp;#x200B;

Jan

&amp;#x200B;

&amp;#x200B;",8,1
8,2018-11-5,2018,11,5,5,9u6ssx,Tensorflow 2.0: models migration and new design,https://www.reddit.com/r/tensorflow/comments/9u6ssx/tensorflow_20_models_migration_and_new_design/,pgaleone,1541363401,,9,1
9,2018-11-5,2018,11,5,18,9uc4lb,How to write a custom loss function,https://www.reddit.com/r/tensorflow/comments/9uc4lb/how_to_write_a_custom_loss_function/,MrFerixx,1541410011,"Hi everyone!

I'm pretty new to Tensorflow and I'm trying to write a simple Cross Entropy loss function. I write something that seems good to me:

`def cross_entropy(y_pred, y_true):`  
  `cross_entropy = tf.reduce_sum(y_true * tf.log(y_pred) + (tf.subtract(1.0, -y_true)) * tf.log(tf.subtract(1.0, -y_pred)), axis=[1])`  
 `return cross_entropy`

&amp;#x200B;

`loss = cross_entropy(x, y)`    
`optimizer = tf.train.GradientDescentOptimizer(0.01)`  
`train = optimizer.minimize(loss)`

&amp;#x200B;

But I get some values at the beginning and then only NaN values. What am I doing wrong?

Thank you very much in advance!

&amp;#x200B;",7,1
10,2018-11-5,2018,11,5,19,9uce4p,object detection fine tune reduce to single object,https://www.reddit.com/r/tensorflow/comments/9uce4p/object_detection_fine_tune_reduce_to_single_object/,c94jk,1541413243,"I can't find any clear documentation online. What I can do is load the protobuf file containing trained model and modify the config file to change class number - but I am unsure how to then remove the final layer, freeze the lower weights and re-append an output layer for my single class. 

&amp;#x200B;

Any pointers in the right direction would be appreciated.",4,1
11,2018-11-6,2018,11,6,0,9ue7bp,I am wondering which algorithm tf.contrib.embed_sequence() exactly uses behind the hood,https://www.reddit.com/r/tensorflow/comments/9ue7bp/i_am_wondering_which_algorithm_tfcontribembed/,Cokdewer,1541430054,"So it maps sequence of symbols (usually words) to sequence of embedding, where we pass the size of our embedding vector that we want as a result.

But, am just curious that does it use CBOW, skip-gram or just dimension reduction technique on one hot vectors behind the scene to produce embedding vectors.

",0,1
12,2018-11-7,2018,11,7,2,9uqjht,Where does the retrained model of inception v3 go after retraining on mac?,https://www.reddit.com/r/tensorflow/comments/9uqjht/where_does_the_retrained_model_of_inception_v3_go/,PhoebusElpollo,1541526498,,0,1
13,2018-11-7,2018,11,7,3,9uqpnr,Is there a way to get order of operations/variables from a trained model?,https://www.reddit.com/r/tensorflow/comments/9uqpnr/is_there_a_way_to_get_order_of/,marcotb12,1541527553,"I have a trained model that I need to convert to another format to do the inference that doesn't user tensorflow. I am probably going to convert the variables to numpy arrays and do numpy operations on them. I do not care about training. Only about inference.

Visualizing the model is hard because the model is huge and complex. What I am trying to do is read the operations/variables on from the saved model's graph. The problem is that the order is not entirely clear. 

I can use graph.get_operations() to get ops but there are so many operations that it is hard to tell which ones are for inferencing and which ones are not. Is there a way to get a computational graph from a trained model where the order of ops and what variables go into the ops for inference are clear?

What I can do now is get all ops and look at the inputs attribute, but that doesn't really narrow anything down.",2,1
14,2018-11-7,2018,11,7,22,9uzlwi,A few days ago I heard about AdaNet,https://www.reddit.com/r/tensorflow/comments/9uzlwi/a_few_days_ago_i_heard_about_adanet/,Good_Development,1541598903,"What is really AdaNet? A framework different from TensorFlow or a complementary api? Does it work together?

If someone could clarify these concepts I would appreciate it.",3,1
15,2018-11-8,2018,11,8,7,9v42tx,Video inference with queues?,https://www.reddit.com/r/tensorflow/comments/9v42tx/video_inference_with_queues/,munkeegutz,1541629375,"Hello, all!

I am trying to build a pipeline for performing inference on videos, but it looks like I'm starving the GPU of data.  I would like to build a queue which serves my model frames from a cv2.videocapture() object.  It sounds like py\_func will help me here, but I'm not super clear on how to maintain the state within that object, or how to indicate to the system that the end of the file has been reached.

This is my first foray into using tensorflow queues, so I might be getting a little over my head ;-)

Any recommendations?  Thanks in advance!",2,1
16,2018-11-8,2018,11,8,18,9v8m6m,Retraining from a frozen GraphDef,https://www.reddit.com/r/tensorflow/comments/9v8m6m/retraining_from_a_frozen_graphdef/,thibault_c,1541669492,"From a GraphDef and variables stored in a checkpoint file, [freeze\_graph](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py) converts all variables into const ops and outputs a new GraphDef.

Now,  I would like to fine-tune a pertained model but no checkpoint is provided, only the code to build the graph, and a frozen GraphDef (outputted by freeze\_graph) in a .pb file. Is there any way to do the opposite, that is, convert the const ops back to get trainable variables?",1,1
17,2018-11-8,2018,11,8,20,9v992c,"Object detection with TensorFlow.JS and YOLO (supports Tiny YOLO v1, 2, 3 and YOLO v3)",https://www.reddit.com/r/tensorflow/comments/9v992c/object_detection_with_tensorflowjs_and_yolo/,x_ash,1541676934,,2,1
18,2018-11-8,2018,11,8,22,9va24q,Help Please! iterating through a tensor aka re-sampling a tensor,https://www.reddit.com/r/tensorflow/comments/9va24q/help_please_iterating_through_a_tensor_aka/,MachinaDoctrina,1541684437,"Hi developers who know tensorflow better than I do,  

I am trying to implement a custom loss function (based on Sequential Monte Carlo aka Particle Filters) in tensorflow that includes a re-sampling routine but I can't seem to figure how to implement the indexing correctly so that it can be run within a `tf.Session()`

The equivalent python function would be:
``
    import numpy as np
    
    def resampling(w, x):
        N = w.shape[0]
        bins = np.cumsum(w)
        ind = np.arange(N)
        u = (ind + np.random.rand(N))/N
        indx = np.digitize(u, bins)
        return x[indx]
``
    
where `w` is a set of normalised weights, and `x` is a vector of states. Both `w` and `x` would be tensors, any help would be appreciated!

the crux of my problem is that I can't seem to find any ops that would allow me to iterate over one dimension of a tensor (in this case it would be a `shape = [1000, 1]`) and based on some criteria i.e what index of bins and thus `X` (as the `w` is a vector of weights of `X`) has a weight that given some sample from a Uniform distribution is `&lt;=` to some value in the `cdf := bins`. 

I hope I've explained myself clear enough, but if you need further clarification please contact me, I would really like to figure out how to do this. Also I've asked on SO but to no avail (https://stackoverflow.com/questions/53193841/implementing-sequential-monte-carlo-resampling-routine-in-tensorflow)

Cheers,
Chris
",5,1
19,2018-11-9,2018,11,9,1,9vbkyc,When tensorflow v2 is going to be released?,https://www.reddit.com/r/tensorflow/comments/9vbkyc/when_tensorflow_v2_is_going_to_be_released/,aziz_22,1541695370,,5,1
20,2018-11-9,2018,11,9,7,9vejnk,Can some 1 help me with a vgg19 pre trained model ?,https://www.reddit.com/r/tensorflow/comments/9vejnk/can_some_1_help_me_with_a_vgg19_pre_trained_model/,jabbaluck,1541715259,"Hey guys, 

&amp;#x200B;

I got a script from github and i wanna try it out .It seems it uses vgg19 and i need to give it the path of the model . I looked on internet and didnt found one allready trained. 

Could u give me  a hand with 1 model vgg19 pre trained   ?(probl in time i'll train 1 for myself but for now wanna try a script ) . 

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;",4,1
21,2018-11-10,2018,11,10,0,9vl55o,Instalation problems,https://www.reddit.com/r/tensorflow/comments/9vl55o/instalation_problems/,Folkwang,1541775850,"Hello guys. So, Ive been trying to install tensorflow both on Windows and Ubuntu, unsuccessfully.
So, does Tensorflow requires Anaconda? It only supports Python3.7, and I cant download the 3.6 version, which seems to be supported. 
Until now, I tried to follow Mike Jay yt video, and it didnt work.
So, what should I do?
",7,1
22,2018-11-10,2018,11,10,2,9vmlj3,Any tutorials on using tensorflow_probability with Keras?,https://www.reddit.com/r/tensorflow/comments/9vmlj3/any_tutorials_on_using_tensorflow_probability/,o-rka,1541785821,Im looking for tutorials on how to use both together to make probabilistic neural nets. ,2,1
23,2018-11-10,2018,11,10,23,9vupof,Tensorflow learning Help,https://www.reddit.com/r/tensorflow/comments/9vupof/tensorflow_learning_help/,vamos47,1541860300,"Hi, I trained until now using keras functional API, I want to explore tensorflow. All the tutorials i find in tensorlow.org are using tfEstimators, tfLearn, tfkeras can any one know where I can find training using tensorflow core . I am looking into how to save and restore checkpoints and how to add signatures to make it servvable. ",1,1
24,2018-11-11,2018,11,11,4,9vx4vi,Build and install TensorFlow from source with MKL DNN support and AVX enabled,https://www.reddit.com/r/tensorflow/comments/9vx4vi/build_and_install_tensorflow_from_source_with_mkl/,rewqasdfsw,1541878432,,0,1
25,2018-11-11,2018,11,11,9,9vzg63,Do I need to reload the entire model to get a new prediction?,https://www.reddit.com/r/tensorflow/comments/9vzg63/do_i_need_to_reload_the_entire_model_to_get_a_new/,infinitykick,1541896286,"Is it neccesary to reload a FrozenModel every time I want to get a result for new input? Loading the model takes over twice as long as the actual execution...

Without reloading, I get `Error: Tensor is disposed`. All sample code and tutorials I can find reload the model on every run.",0,1
26,2018-11-11,2018,11,11,18,9w2nm2,Build and install TensorFlow 1.12 Python package with CUDA 10 (and cuDNN 7.4) for Windows,https://www.reddit.com/r/tensorflow/comments/9w2nm2/build_and_install_tensorflow_112_python_package/,amsokol,1541930000,I have updated my tutorial to help you to build and install TensorFlow 1.12 Python package with CUDA 10 (and cuDNN 7.4) for Windows. Enjoy: [https://medium.com/@amsokol.com/update-2-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-61c26553f7e8](https://medium.com/@amsokol.com/update-2-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-61c26553f7e8),1,1
27,2018-11-12,2018,11,12,0,9w4kc1,Tensorflow sigmoid returning high values early and much,https://www.reddit.com/r/tensorflow/comments/9w4kc1/tensorflow_sigmoid_returning_high_values_early/,Jandevries101,1541949762,"hi

&amp;#x200B;

my sigmoid returns 

&amp;#x200B;

0.94659156

&amp;#x200B;

0.20719022

&amp;#x200B;

0.8733577

&amp;#x200B;

like very constant for specific actions, it's really annoying, cause it happens everytime my LR is 0.0001 and this turns in almost the first 3 steps with reward 0 for that action, why?",5,1
28,2018-11-12,2018,11,12,0,9w4tii,tensorflow or keras?,https://www.reddit.com/r/tensorflow/comments/9w4tii/tensorflow_or_keras/,bharddwaj,1541951751,Is it better to learn a high level api like keras or tensorflow? What are the benefits of knowing one as opposed to the other? ,8,1
29,2018-11-12,2018,11,12,1,9w4xw4,What do the tf hub imagenet inception v3 feature vectors mean?,https://www.reddit.com/r/tensorflow/comments/9w4xw4/what_do_the_tf_hub_imagenet_inception_v3_feature/,Zak_Wormald,1541952652,"I have written a program that takes an image and passes it through tf hub's off-the-shelf inception classifier. The classifier returns a feature vector, but i dont know which number in the vector refers to what object. For example, i have passed in a picture of a cat and the returned feature vector's maximum value is it's 1847th value, but i don't know whether a high value in the 1847th position refers  to the model finding  a cat, a dog, or anything. If anyone knows how to translate between inception's output and what that means it thinks the picture contains i would be grateful for your help.",1,1
30,2018-11-12,2018,11,12,10,9w9cf2,How to control / protect your AI models?,https://www.reddit.com/r/tensorflow/comments/9w9cf2/how_to_control_protect_your_ai_models/,taewoo,1541984625,"i work on biometrics / face recognition,..... and as with most data sensitive stuff, the customer I deal wants to in-house to not violate privacy laws &amp; regulations.  The way we charge is per use. 

&amp;#x200B;

Of course, we can do all kinds of obfuscation / compile model to binary and all that.. but at the end of the day, we have to protect our company's interest and this doesnt' give us the protection necessary.

&amp;#x200B;

How would you go about doing it? The only obvious solution is to run it as web service in virtual private cloud in servers located in THEIR countries. Any suggestions?",4,1
31,2018-11-13,2018,11,13,6,9wilgt,School project (205 features binary classification),https://www.reddit.com/r/tensorflow/comments/9wilgt/school_project_205_features_binary_classification/,sleepy3005,1542059637,"Hello,

&amp;#x200B;

I have a school project that I'm working on and I was wondering if anyone on here could give a suggestion about how I should proceed using TensorFlow. As the title states, the text training file will have many instances that have 205 features each with the class value at the last column (0/1). The features will have values with different ranges. I am supposed to use these features and produce a binary classification. I can use any algorithm and technique that I want. I'm leaning towards using Neural Networks with TensorFlow but I am open to suggestions. 

&amp;#x200B;

Thanks!",6,1
32,2018-11-13,2018,11,13,9,9wjy7o,Advanced Machine Learning with TensorFlow on Google Cloud,https://www.reddit.com/r/tensorflow/comments/9wjy7o/advanced_machine_learning_with_tensorflow_on/,skj8,1542069017,,0,1
33,2018-11-13,2018,11,13,12,9wlfhc,"Tried using tensorflows retrain.py to train An object detector on new categories. Console said it had 98% train accuracy and 50% test accuracy, yet after I tested it myself, it wasnt accurate in the least. Any idea?",https://www.reddit.com/r/tensorflow/comments/9wlfhc/tried_using_tensorflows_retrainpy_to_train_an/,pocketMAD,1542080158,,7,1
34,2018-11-13,2018,11,13,14,9wm5sg,How to read gradient of all convolution layers of a tensorflow model while training?,https://www.reddit.com/r/tensorflow/comments/9wm5sg/how_to_read_gradient_of_all_convolution_layers_of/,4joyalbin,1542086356,"What is the method to get the gradient of all convolution layers of a tensorflow model while training?

I tried to register  *RegisterGradient* () for the 'Conv2D' operators but failed with error ""two registration for gradient"".

Anyone please share me an example to get gradient of all convolution layers?",0,1
35,2018-11-13,2018,11,13,19,9wnseq,What do you think about Tensorflow 2.x compared to Tensorflow 1.x?,https://www.reddit.com/r/tensorflow/comments/9wnseq/what_do_you_think_about_tensorflow_2x_compared_to/,thisisiron,1542103269,"If the tensor flow is updated to 2.x, there will be a few changes. What about you?

i read this blog [post](https://pgaleone.eu/tensorflow/gan/2018/11/04/tensorflow-2-models-migration-and-new-design/), and i took the time to think about tensorflow.",7,1
36,2018-11-14,2018,11,14,10,9wvb6u,Correct me If I am wrong,https://www.reddit.com/r/tensorflow/comments/9wvb6u/correct_me_if_i_am_wrong/,vamos47,1542158507,I cant take a checkpoint folder from someone and turn into into a servable code as I wont have computation graph from it. I will also need the code to build the model. I should only take Save_model or frozen_graph models.,2,1
37,2018-11-14,2018,11,14,10,9wvc07,How to transform music into tensors?,https://www.reddit.com/r/tensorflow/comments/9wvc07/how_to_transform_music_into_tensors/,thefernandito,1542158680,"I am looking for a tutorial that explains how to transform music into tensors to work with tensorflow, but I have not been able to find material. If someone could give me a little explanation or direct me to a tutorial, I appreciate it.",1,1
38,2018-11-14,2018,11,14,22,9wzxz1,"Sigmoid returning high values, but why? - Support",https://www.reddit.com/r/tensorflow/comments/9wzxz1/sigmoid_returning_high_values_but_why_support/,Jandevries101,1542200894,"Hi Reader,

&amp;#x200B;

so in my (RL) Algorithme i am using the Sigmoid (also tried softmax), but when training i quickly noticed something ""strange"" in my sigmoid returns:

&amp;#x200B;

    0.9995

&amp;#x200B;

for output 1, output 1 having reward of 0, at the end of the episode more, but this action gives by default just 0 reward, whiles other other actions may be more likely to be chosen

&amp;#x200B;

the problem i am having here is that first of all that value is so darn high, i can't even consider it learned that. second issue this occurs after ca 5-10 steps only into the training sessions, with LR of only 0,0001!

&amp;#x200B;

I know you might wanna see code or something, but i'd rather know (also for others in the future) what are causes to this to occur? my state is nothing special just 7 values and i have 3 actions to choose from (the values returned by the sigmoid are very low of course since action 1 is 0.9995).

&amp;#x200B;

i am really confused by what caused this and i do understand that reward higher/lowers the value returned by sigmoid, but this goes from 0.x (random) to 0.9995 in no time, only having a reward of 0, is there something with a reward of zero that i missed out on?

&amp;#x200B;

Let me know what you think and if you know what may caused it, please let me know i am intrested to know what it is?

&amp;#x200B;

thanks for reading,

&amp;#x200B;

Jan

&amp;#x200B;

&amp;#x200B;",6,1
39,2018-11-15,2018,11,15,4,9x39bt,Tensorflow script _impl.InvalidArgumentError:,https://www.reddit.com/r/tensorflow/comments/9x39bt/tensorflow_script_implinvalidargumenterror/,jabbaluck,1542223826,"Hey guys I'm trying a script i found on github. In the past at the same point i had some batch size problems.. i lowered them down. NOW I'm getting this error .. 

 

&amp;#x200B;

python/client/session.py"", line 1312, in \_extend\_graph  
tf\_session.ExtendSession(self.\_session)  
tensorflow.python.framework.errors\_impl.InvalidArgumentError: Cannot assign a device for operation 'save/SaveV2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.  
Registered kernels:  
device='CPU'

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

I tried to add to the script:  
tf.Session(config=tf.ConfigProto(allow\_soft\_placement=True, log\_device\_placement=True))

But it didn't change really anything .. 

I'm using tensorflow '1.11.0'.

&amp;#x200B;

do you have any clue on how to fix this ?",0,1
40,2018-11-15,2018,11,15,5,9x3u6j,Help. How can I refine this?,https://www.reddit.com/r/tensorflow/comments/9x3u6j/help_how_can_i_refine_this/,chaiboy,1542227625,"I am new to Tensorflow and Neural Networks in general so I am trying to replicate an idea I saw a while back. An AI generated list of monster names.   


What would the proper settings be to work with just a couple of words per line?

&amp;#x200B;

Any tips on what to look into would help. There is a lot of documentation out there and I'm not sure where to start.

&amp;#x200B;

Here is an example of my code:

&amp;#x200B;

gen-monster.py

    from textgenrnn import textgenrnn
    import os
    
    textgen = textgenrnn()
    
    
    textgen.train_from_file(
        'monsters.txt',
        new_model=         True,
        rnn_size=          128,
        rnn_layers=        4,
        
        rnn_bidirectional= False,
        line_delimited=    True,
    
        max_length=        60,
        max_words=         10000,
    
        num_epochs=        50,
        gen_epochs=        10,
        train_size=        0.8
        dim_embeddings=    100,
        word_level=        True,
        single_text=       False,
        validation=        True,
        name='textgenrnn')
    
    

I run the next to print out a list to a file once the previous one has run.

get-monster.py

    from textgenrnn import textgenrnn
    import os
    
    textgen = textgenrnn(weights_path='textgenrnn_weights.hdf5',
                           vocab_path='textgenrnn_vocab.json',
                           config_path='textgenrnn_config.json')
    
    textgen.generate_to_file('newmonsters-list.txt', n=1000)

&amp;#x200B;

&amp;#x200B;

the data is a text file.

monsters.txt

    Death Crone
    Flesh Crone
    Norn
    Primeval Crone
    Gorebull
    Chosen of Baphomet
    Grave Hag
    Great Cockatrice
    Griffin
    Royal Griffin
    Black Root
    Leshen
    Ancient Leshen
    Chort
    Morvudd
    Relic Morvudd
    Crypt Horror
    Drowned Dead
    Drowner
    Foglet
    Ignis Fatuus
    Devourer
    Rotfiend
    Nekker
    Nekker Warrior
    Phoocas
    Thornheart
    Thornheart Spellcaster
    Primal Wolf
    Primal Bear
    Primal Murder of Crows
    Alp
    Bruxa
    Bruxa Night Mother
    Ekimmara
    Fleder
    Garkain
    Katakan
    Nekurat
    Mula
    Ancient Vargheist
    ...
    another 3000 more assorted names

Pretty simple. When I run the training many of the names it comes up with are the same ones from the list. What would I change to have it be more creative?   


&amp;#x200B;",9,1
41,2018-11-15,2018,11,15,11,9x6u8s,convert videos (eg. .avi) to TensorFlow's tfrecords file format,https://www.reddit.com/r/tensorflow/comments/9x6u8s/convert_videos_eg_avi_to_tensorflows_tfrecords/,whiletrue2,1542248457,,0,1
42,2018-11-15,2018,11,15,16,9x957c,Tensor Flow Container Setup with Nvidia GPU Support for Ubuntu 16.04,https://www.reddit.com/r/tensorflow/comments/9x957c/tensor_flow_container_setup_with_nvidia_gpu/,nullbyte91,1542267842,,3,1
43,2018-11-15,2018,11,15,22,9xbbz5,How can i feed multiple 3D Arrays into Tensorflow?,https://www.reddit.com/r/tensorflow/comments/9xbbz5/how_can_i_feed_multiple_3d_arrays_into_tensorflow/,oneuseroranother,1542289865,"I have a training set consisting of multiple directorys with several images inside of them. Each directory represents an label. I am loading the images into 3 dimensional numpy arrays, but what is the best approach to feed these arrays into my neural network? The total size of the dataset is several GB and quite big.",0,1
44,2018-11-16,2018,11,16,4,9xem1r,Tensorflow server to .com,https://www.reddit.com/r/tensorflow/comments/9xem1r/tensorflow_server_to_com/,Throwaway_geology,1542311570,"Hello,

I am looking for help to understand how to deploy a tf DL model to a .com. I read about tf serving and how you can deploy it to production, but I don't understand how to get it on to your own dns. All the how to guides show you how to set the server up but I don't 
know how to get this to a hosting website for my .com. 

Are there any YouTube videos or guides someone could link me to with a step by step to deploy to a website? 

The best kinda guide I found was:

https://www.kubeflow.org/docs/guides/gke/gcp-e2e/

What I also don't understand is when Google says : tensorflow serving to production! Production to a local app or production to online dns? 

I want to use my model on my .com website as a business. Not interested in local apps. Old people and computer illiterate people won't understand how to use local apps, but I believe they'll understand how to navigate to a simple .com website to use my predictive model for their benefit. This is my intention. Please help me understand!! Thanks! ",0,1
45,2018-11-16,2018,11,16,6,9xfrgc,How do I remove the training-related nodes from a graph?,https://www.reddit.com/r/tensorflow/comments/9xfrgc/how_do_i_remove_the_trainingrelated_nodes_from_a/,rumborak,1542319137,"I have a model trained that I'm trying to convert to TFlite format. However, two training-related operations (\*QueueDequeueUpToV2\* and \*RandomShuffleQueueV2\*) show up as unsupported operations, and even when I allow the conversion to continue with \*tflite\_convert\*'s ""--allow-custom-ops"" flag, it eventually fails when loading it in TFLite with the same issue (i.e. unsupported operations).

&amp;#x200B;

The weird part is, as I realized I would need a way of actually pushing my data in, I used the Identity operation to mark the model's data input node as ""model\_input\_raw"". That node label I then use in \*tflite\_convert\*  to denote the input. Meaning, the actual training operations should be no-ops and automatically pruned away anyway, but clearly they are not!

&amp;#x200B;

I have tried using transform\_graph on the graph, by using the transformation ""\*remove\_nodes(op=QueueDequeueUpToV2, op=RandomShuffleQueueV2)\*"", but that does not result in any modification of the graph at all.

&amp;#x200B;

Does anybody have any ideas/suggestions?",0,1
46,2018-11-16,2018,11,16,22,9xm4mt,Tensorflow implementation of recurrent batch normalization (any bugs???),https://www.reddit.com/r/tensorflow/comments/9xm4mt/tensorflow_implementation_of_recurrent_batch/,aziz_22,1542374680,"Following my previous post in r/MachineLearning. 

I would like to get some feed-back about my implementation of the [recurrent-batch normalization](https://arxiv.org/pdf/1603.09025.pdf).

Here is my code 

&amp;#x200B;

https://i.redd.it/i1uobvw62py11.png

and here the batch\_norm function 

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/dblt8vy82py11.png

Any help will be very appreciated :)",0,1
47,2018-11-17,2018,11,17,14,9xtykw,I need help with running two graphs sequentially.,https://www.reddit.com/r/tensorflow/comments/9xtykw/i_need_help_with_running_two_graphs_sequentially/,downvotedbylife,1542433128,"I'll start by saying I'm a beginner (MATLAB migrant), so this might be obvious but has had me frustrated for a couple of nights now.  

I have a previously trained and saved model [NN1]. I want to refine its functionality by training a second network [NN2], directly connected to [NN1]'s output. [NN2] will be trained with the same target samples used to train [NN1], and the training input samples would be the same ones used for [NN1], but processed through it.  
What is the correct way to go about this within TF? My first instinct was running them in separate sessions (one containing each graph), in order to use [NN1] solely as an operator of sorts for input training data inside the training loop for [NN2], but I'm not sure if this is the best approach. Are there any TF-specific advantages I can take to make this more straightforward?",3,1
48,2018-11-17,2018,11,17,15,9xucw7,Building Footprint Extraction using Deep learning.,https://www.reddit.com/r/tensorflow/comments/9xucw7/building_footprint_extraction_using_deep_learning/,girishpillai17,1542437392,"How can you extract the footprints of the building rooftops from an satellite image using computer vision, machine learning and deep learning. My aim is to just extract the buildings rooftops from the image. What should be the approach for this and what all technologies should be used to achieve the output which is shown in the image?",2,1
49,2018-11-17,2018,11,17,16,9xuj1o,Using a AMD GPU for TensorFlow - R9 285 on Fedora 29?,https://www.reddit.com/r/tensorflow/comments/9xuj1o/using_a_amd_gpu_for_tensorflow_r9_285_on_fedora_29/,Im_BrokeForever,1542439333,"Hi! I'm wondering if my AMD GPU is compatible with Tensorflow. I believe that there is support of AMD gpus according to the [ROCM project](https://rocm.github.io/install.html#supported-gpus). However, the instructions to install tensorflow seem to be only supported natively on Ubuntu, and RHEL and Centos with some extra dependencies and a ""special run-time environment"". It is possible that I could use Tensorflow with the GPU on Fedora? Thanks. 

(I do realize there's an option to include Tensorflow in a docker container, but I would prefer having it installed directly onto my host computer.) ",7,1
50,2018-11-17,2018,11,17,23,9xwkv9,ubuntu18.04 + python 3.6.5 + cuda 9.0 + tensorflow 1.12.0 install problem,https://www.reddit.com/r/tensorflow/comments/9xwkv9/ubuntu1804_python_365_cuda_90_tensorflow_1120/,pg13mvp,1542463314,"I have tried several methods setting up the environment  ( from youtube github  )

but i still can't install successfully 

&amp;#x200B;

is there any guide recommend for me?

( don't change the version of cuda 9.0 tensorflow 1.12.0 python 3.6.5",9,1
51,2018-11-18,2018,11,18,5,9xzih8,Beginner python tensorflow ml,https://www.reddit.com/r/tensorflow/comments/9xzih8/beginner_python_tensorflow_ml/,Zi6st,1542485205,"So i want to learn tensor flow ,but i dont have any python or ml knowledge.I do however know c++  but that doesnt help much.Any tips for how should i start this journey? ",3,1
52,2018-11-18,2018,11,18,10,9y21kz,How can I tell if my tensorflow is training my model?,https://www.reddit.com/r/tensorflow/comments/9y21kz/how_can_i_tell_if_my_tensorflow_is_training_my/,pocketMAD,1542504835,"I'm trying to train my object detector using model_main.py. Everything is working and there are no errors. However, when I activate tensor board, I only have access to the tensorflow graph structure. I have no access to the graphs of the mAP and loss. Is this bad? ",3,1
53,2018-11-18,2018,11,18,11,9y2fmi,"When I'm training from a MobileNet for object detection, tensor board doesn't show scalars when I train using model_main.py. This tells me it likely isn't training the network. What should I do?",https://www.reddit.com/r/tensorflow/comments/9y2fmi/when_im_training_from_a_mobilenet_for_object/,pocketMAD,1542508331,,0,1
54,2018-11-18,2018,11,18,15,9y414s,Is Tensorflow machine learning cookbook too old?,https://www.reddit.com/r/tensorflow/comments/9y414s/is_tensorflow_machine_learning_cookbook_too_old/,pg13mvp,1542524115,"I recently got the book, but when I run the sample code, I found that some function like sub() is already change into subtract().

So I'm wondering is it still ok to learn from this book or should i get another one?

If so, any other recommendations?
(I prefer book instead of video",12,1
55,2018-11-19,2018,11,19,12,9ycz07,with tf.device('/cpu:0'): identation problem?,https://www.reddit.com/r/tensorflow/comments/9ycz07/with_tfdevicecpu0_identation_problem/,pg13mvp,1542598385,"I type "" with tf.device('/cpu:0'): ""

in the front of my code 

and right tab all the codes below

&amp;#x200B;

but when i execute it shows an error massage 

"" TabError: inconsistent use of tabs and spaces in indentation ""

&amp;#x200B;

and I have no idea what's going on 

&amp;#x200B;

thanks for help!",2,1
56,2018-11-20,2018,11,20,0,9yhhv7,Facial landmark detection - in dire need of some guidance,https://www.reddit.com/r/tensorflow/comments/9yhhv7/facial_landmark_detection_in_dire_need_of_some/,ned334,1542639713,"Hi, I am just starting out with TensorFlow, but I have some prior experience with training networks for classification.  Also some theoretical background.

I have a project where I am supposed to train a resnet18 for facial landmark prediction. I understand the concept, I think: I must feed the network both images and coordinates of the eyes, as GT. The model will hopefully predict where the eyes are at on new images. 

Now, as to the implementation, I have looked at some TF source code, training resnet18 for detection and it's simple enough. Feeding the GT images + labels but I don't know how to input the images and coordinates. I don't know strictly code-wise. I looked over the first few pages of google in search of an example of resnet or any network trained like that but found none. 

I also tried to understand how to do this from the TF documentation but again, I found nothing useful for my case. 

I'm not saying there is not the info I need out there, just that I couldn't find it. 

Could you please help with some instructions of what I should read to understand what I have to do? 

I wouldn't mind raw code either but I'll take whatever I can get. 

Thanks!",0,1
57,2018-11-20,2018,11,20,3,9yjbry,How can I read a graph definition for a tensorflow model?,https://www.reddit.com/r/tensorflow/comments/9yjbry/how_can_i_read_a_graph_definition_for_a/,_D_Money_,1542651484,"I'm currently taking the [Udacity Deep Learning](https://www.udacity.com/course/deep-learning--ud730) course, and they have a graph definition of a convolutional neural net (see below). I'd be curious to read a graph definition of a larger CNN, for example MobileNet, in order to help understand how the code works. Is it possible to somehow read a .pb file or some other file and see the python graph definition of MobileNet or Inception or something? Or is there a python graph definition available of these networks that I can read?

Here is the example CNN from the Udacity course, I'm wondering if I can read a similar graph definition but for MobileNet:

    batch_size = 16
    patch_size = 5
    depth = 16
    num_hidden = 64

    graph = tf.Graph()

    with graph.as_default():

        # Input data.
        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))
        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
        tf_valid_dataset = tf.constant(valid_dataset)
        tf_test_dataset = tf.constant(test_dataset)
        
        # Variables.
        layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))
        layer1_biases = tf.Variable(tf.zeros([depth]))
        layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))
        layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
        layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))
        layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
        layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))
        layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))
        
        # Model.
        def model(data):
            conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')
            hidden = tf.nn.relu(conv + layer1_biases)
            conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')
            hidden = tf.nn.relu(conv + layer2_biases)
            shape = hidden.get_shape().as_list()
            reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])
            hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)
            return tf.matmul(hidden, layer4_weights) + layer4_biases
        
        # Training computation.
        logits = model(tf_train_dataset)
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))
            
        # Optimizer.
        optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)
        
        # Predictions for the training, validation, and test data.
        train_prediction = tf.nn.softmax(logits)
        valid_prediction = tf.nn.softmax(model(tf_valid_dataset))
        test_prediction = tf.nn.softmax(model(tf_test_dataset))",0,1
58,2018-11-20,2018,11,20,4,9yk29f,Run TensorFlow Convolutional Neural Network (TF CNN) benchmarks in CPU,https://www.reddit.com/r/tensorflow/comments/9yk29f/run_tensorflow_convolutional_neural_network_tf/,raindrop_code,1542655984,,1,1
59,2018-11-20,2018,11,20,6,9ykzt7,4 Reasons to Distribute TensorFlow Processing Among Everyday Objects - DZone AI,https://www.reddit.com/r/tensorflow/comments/9ykzt7/4_reasons_to_distribute_tensorflow_processing/,cloudster314,1542661793,,2,1
60,2018-11-20,2018,11,20,11,9yniyt,Difference between model_main.py and train.py in Object Detection API?,https://www.reddit.com/r/tensorflow/comments/9yniyt/difference_between_model_mainpy_and_trainpy_in/,pocketMAD,1542679284,,0,1
61,2018-11-20,2018,11,20,17,9yqfh9,Are there any plans for a statistical programming book using tensorflow_probability?,https://www.reddit.com/r/tensorflow/comments/9yqfh9/are_there_any_plans_for_a_statistical_programming/,o-rka,1542703852,"I really want to brush up on my statistics and would love to learn the concepts again using tfp as my medium.  

",2,1
62,2018-11-20,2018,11,20,20,9yrbji,Same (?) model converges in Keras but not in Tensorflow,https://www.reddit.com/r/tensorflow/comments/9yrbji/same_model_converges_in_keras_but_not_in/,zedt,1542712994,,0,1
63,2018-11-21,2018,11,21,14,9z045p,[D] Debate on TensorFlow 2.0 API,https://www.reddit.com/r/tensorflow/comments/9z045p/d_debate_on_tensorflow_20_api/,skj8,1542776640,,0,1
64,2018-11-22,2018,11,22,4,9z6l23,Python 3.7 was released almost half a year ago and still no support...?,https://www.reddit.com/r/tensorflow/comments/9z6l23/python_37_was_released_almost_half_a_year_ago_and/,ArgonTorr,1542829513,"Tracking my python versioning is starting to become a real pain and is really interfering with my setup. Is there like not going to ever be a plan to upgrade Tensorflow to be compatible with the most recent python? IIRC the problem is that `async` is now a python standard keyword, but what's the endgame here?

&amp;#x200B;",11,1
65,2018-11-22,2018,11,22,10,9z99v5,"GPU usage is significantly higher with Session (as opposed to Model/Keras), but not faster.",https://www.reddit.com/r/tensorflow/comments/9z99v5/gpu_usage_is_significantly_higher_with_session_as/,ptrkhh,1542848467,"As the title says, I am still learning TF, and found out there seem to be two ways to create a model and train, the Keras-style and the session-style. I dont know what the real name is but anyway, here's the Keras-style

`
model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(x_length,)))
for node in nodes:
     tf.keras.layers.Dense(node, activation=tf.nn.relu)
model.add(tf.keras.layers.Dense(index_length))

loss = 'mse'
metrics = ['mae']

t0 = time.time()

history = model.fit(train_x, train_y, epochs=epochs, validation_data=(validation_x, validation_y), batch_size=batch_size)

t1 = time.time()
print('Total Time:' + str(t1-t0))

`


And here's the TF one

`

batch_size = tf.placeholder(tf.int64)
x, y = tf.placeholder(tf.float32, shape=[None, x_length]), tf.placeholder(tf.float32, shape=[None, index_length])
dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size).repeat()

iter = dataset.make_initializable_iterator()
features, labels = iter.get_next()
 
net = tf.layers.dense(features, 2000, activation=tf.nn.relu)  # pass the first value from iter.get_next() as input
net = tf.layers.dense(net, 500, activation=tf.nn.relu)
net = tf.layers.dense(net, 100, activation=tf.nn.relu)
net = tf.layers.dense(net, 50, activation=tf.nn.relu)
prediction = tf.layers.dense(net, 1, activation=tf.nn.sigmoid)
loss = tf.losses.mean_squared_error(prediction, labels)  # pass the second value from iter.get_net() as label
train_op = tf.train.AdamOptimizer(learning_rate=learning_rates[0]).minimize(loss)

t0 = time.time()

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    sess.run(iter.initializer, feed_dict={x: train_x, y: train_y, batch_size: 100})
    for i in range(epochs):
        tot_loss = 0
        _, loss_value = sess.run([train_op, loss])
        sess.run(iter.initializer, feed_dict={x: validation_x, y: validation_y, batch_size: validation_x.shape[0]})

t1 = time.time()
print('Total Time:' + str(t1-t0))

`

The printout seems to be about the same, probably even faster on the Keras one, and **yet** when I monitor the GPU usage (GTX 1070), the Keras one has around 10% use, while the TF one has around 60%.


TF shows that it uses the GPU on both trainings, so its not CPU training either, I assume.
",6,1
66,2018-11-22,2018,11,22,13,9zaoje,Has anyone come across a t-SNE implementation in Tensorflow/Tensorflow Probability?,https://www.reddit.com/r/tensorflow/comments/9zaoje/has_anyone_come_across_a_tsne_implementation_in/,o-rka,1542860119,I'm still using the [sklearn version](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) and it takes a long time to run for larger datasets.  I came across a [multicore version](https://github.com/DmitryUlyanov/Multicore-TSNE) but I have trouble with dependencies sometimes.  I feel like this could be a good candidate for `TFP`. ,3,1
67,2018-11-22,2018,11,22,13,9zarzw,"Are you interested in Tensorflow and want to start learning more with Tutorials? Check out this new Youtube Channel, called Discover Artificial Intelligence. :)",https://www.reddit.com/r/tensorflow/comments/9zarzw/are_you_interested_in_tensorflow_and_want_to/,ailearn12,1542860967,,0,1
68,2018-11-22,2018,11,22,14,9zb04e,TF 2.0 preview,https://www.reddit.com/r/tensorflow/comments/9zb04e/tf_20_preview/,_spicyramen,1542862930,Is there a way to activate TF 2.0 behavior in nightly or latest release? ,0,1
69,2018-11-22,2018,11,22,14,9zbae9,Creative Applications of Deep Learning with TensorFlow,https://www.reddit.com/r/tensorflow/comments/9zbae9/creative_applications_of_deep_learning_with/,skj8,1542865344,,0,1
70,2018-11-22,2018,11,22,17,9zc7yt,Free eBook today: Building Machine Learning Projects with TensorFlow [PDF],https://www.reddit.com/r/tensorflow/comments/9zc7yt/free_ebook_today_building_machine_learning/,PacktStaff,1542874187,,0,1
71,2018-11-22,2018,11,22,20,9zdi3j,"Converting Tensorflow Graph to use Tensorflow Estimator, getting 'TypeError: data type not understood', at loss function",https://www.reddit.com/r/tensorflow/comments/9zdi3j/converting_tensorflow_graph_to_use_tensorflow/,AdditionalWay,1542887721,"I am trying to convert a working Tensorflow graph to use Tensorflow Estimator, using a custom Estimator. My model works when I was just using a model and then running it with a session. But when I try to use it with the Estimator API, it's not working. 

This is where I defined my model

    def my_model( features, labels, mode, params):
    
        train_dataset = features
        train_labels = labels
    
        batch_sizeE=params[""batch_size""]
        embedding_sizeE=params[""embedding_size""]
        num_inputsE=params[""num_inputs""]
        num_sampledE=params[""num_sampled""]
        
        print(features)
        print(labels)
    
        epochCount = tf.get_variable( 'epochCount', initializer= 0) #to store epoch count to total # of epochs are known
        update_epoch = tf.assign(epochCount, epochCount + 1)
    
        embeddings = tf.get_variable( 'embeddings', dtype=tf.float32,
            initializer= tf.random_uniform([vocabulary_size, embedding_sizeE], -1.0, 1.0, dtype=tf.float32) )
    
        softmax_weights = tf.get_variable( 'softmax_weights', dtype=tf.float32,
            initializer= tf.truncated_normal([vocabulary_size, embedding_sizeE],
                                 stddev=1.0 / math.sqrt(embedding_sizeE), dtype=tf.float32 ) )
    
        softmax_biases = tf.get_variable('softmax_biases', dtype=tf.float32,
            initializer= tf.zeros([vocabulary_size], dtype=tf.float32),  trainable=False )
    
        embed = tf.nn.embedding_lookup(embeddings, train_dataset) #train data set is
    
        embed_reshaped = tf.reshape( embed, [batch_sizeE*num_inputs, embedding_sizeE] )
    
        segments= np.arange(batch_size).repeat(num_inputs)
    
        averaged_embeds = tf.segment_mean(embed_reshaped, segments, name=None)
    
        if mode == ""train"":
        
            sSML = tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=averaged_embeds,
                labels=train_labels, num_sampled=64, num_classes=3096637)
    
            loss = tf.reduce_mean( sSML )
    
            optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss) 
    
        saver = tf.train.Saver()

This is where I call the training

    #Define the estimator
    word2vecEstimator = tf.estimator.Estimator(
            model_fn=my_model,
            params={
                'batch_size': 16,
                'embedding_size': 10,
                'num_inputs': 3,
                'num_sampled': 128
            })
    
    word2vecEstimator.train(
        input_fn=generate_batch,
        steps=10)

And this is the error I get

    INFO:tensorflow:Calling model_fn.
    
    &lt;tf.Variable 'softmax_weights:0' shape=(3096637, 50) dtype=float32_ref&gt;
    &lt;tf.Variable 'softmax_biases:0' shape=(3096637,) dtype=float32_ref&gt;
    ---------------------------------------------------------------------------
    TypeError                                 Traceback (most recent call last)
    &lt;ipython-input-49-955f44867ee5&gt; in &lt;module&gt;()
          1 word2vecEstimator.train(
          2     input_fn=generate_batch,
    ----&gt; 3     steps=10)
    
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
        352 
        353       saving_listeners = _check_listeners_type(saving_listeners)
    --&gt; 354       loss = self._train_model(input_fn, hooks, saving_listeners)
        355       logging.info('Loss for final step: %s.', loss)
        356       return self
    
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
       1205       return self._train_model_distributed(input_fn, hooks, saving_listeners)
       1206     else:
    -&gt; 1207       return self._train_model_default(input_fn, hooks, saving_listeners)
       1208 
       1209   def _train_model_default(self, input_fn, hooks, saving_listeners):
    
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)
       1235       worker_hooks.extend(input_hooks)
       1236       estimator_spec = self._call_model_fn(
    -&gt; 1237           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
       1238       global_step_tensor = training_util.get_global_step(g)
       1239       return self._train_with_estimator_spec(estimator_spec, worker_hooks,
    
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)
       1193 
       1194     logging.info('Calling model_fn.')
    -&gt; 1195     model_fn_results = self._model_fn(features=features, **kwargs)
       1196     logging.info('Done calling model_fn.')
       1197 
    
    &lt;ipython-input-47-95d390a50046&gt; in my_model(features, labels, mode, params)
         47 
         48         sSML = tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=averaged_embeds,
    ---&gt; 49             labels=train_labels, num_sampled=64, num_classes=3096637)
         50 
         51         loss = tf.reduce_mean( sSML )
    
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py in sampled_softmax_loss(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, remove_accidental_hits, partition_strategy, name, seed)
       1347       partition_strategy=partition_strategy,
       1348       name=name,
    -&gt; 1349       seed=seed)
       1350   labels = array_ops.stop_gradient(labels, name=""labels_stop_gradient"")
       1351   sampled_losses = nn_ops.softmax_cross_entropy_with_logits_v2(
    
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py in _compute_sampled_logits(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, subtract_log_q, remove_accidental_hits, partition_strategy, name, seed)
       1029   with ops.name_scope(name, ""compute_sampled_logits"",
       1030                       weights + [biases, inputs, labels]):
    -&gt; 1031     if labels.dtype != dtypes.int64:
       1032       labels = math_ops.cast(labels, dtypes.int64)
       1033     labels_flat = array_ops.reshape(labels, [-1])
    
    TypeError: data type not understood

Here is a link to the Google Colab notebook for people to run on their own. For anyone looking to execute this, this will download a data file that is ~500 mbs.

https://colab.research.google.com/drive/1LjIz04xhRi5Fsw_Q3IzoG_5KkkXI3WFE

And here is the full code, from the notebook.

    import math
    import numpy as np
    import random
    import zipfile
    import shutil
    from collections import namedtuple
    
    import os
    import pprint
    
    import tensorflow as tf
    
    import pandas as pd
    import pickle
    from numpy import genfromtxt
    
    !pip install -U -q PyDrive
    
    from google.colab import files
    from pydrive.auth import GoogleAuth
    from pydrive.drive import GoogleDrive
    from google.colab import auth
    from oauth2client.client import GoogleCredentials
    
    auth.authenticate_user()
    gauth = GoogleAuth()
    gauth.credentials = GoogleCredentials.get_application_default()
    drive = GoogleDrive(gauth)
    
    vocabulary_size = 3096637 #updated 10-25-18 3096636
    
    import gc



    dl_id = '19yha9Scxq4zOdfPcw5s6L2lkYQWenApC' #updated 10-22-18
    
    myDownload = drive.CreateFile({'id': dl_id})
    myDownload.GetContentFile('Data.npy')
    my_data = np.load('Data.npy')
    #os.remove('Data.npy')
    np.random.shuffle(my_data)
    print(my_data[0:15])
    
    data_index = 0 
    epoch_index = 0 
    recEpoch_indexA = 0 #Used to help keep store of the total number of epoches with the models
    
    def generate_batch(): 
        global data_index, epoch_index
    
        features = np.ndarray(shape=(batch_size, num_inputs), dtype=np.int32) 
        labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)
    
        n=0
        while n &lt; batch_size:
          if len(    set(my_data[data_index, 1])   ) &gt;= num_inputs:
            labels[n,0] = my_data[data_index, 0]
            features[n] = random.sample( set(my_data[data_index, 1]),  num_inputs)
            n = n+1
            data_index = (data_index + 1) % len(my_data) #may have to do something like len my_data[:]
            if data_index == 0:
              epoch_index = epoch_index + 1
              print('Completed %d Epochs' % epoch_index)
          else:
            data_index = (data_index + 1) % len(my_data)
            if data_index == 0:
              epoch_index = epoch_index + 1
              print('Completed %d Epochs' % epoch_index)
    
        return features, labels     



    def my_model( features, labels, mode, params):
    
        train_dataset = features
        train_labels = labels
    
        batch_sizeE=params[""batch_size""]
        embedding_sizeE=params[""embedding_size""]
        num_inputsE=params[""num_inputs""]
        num_sampledE=params[""num_sampled""]
        
        print(features)
        print(labels)
    
        epochCount = tf.get_variable( 'epochCount', initializer= 0) #to store epoch count to total # of epochs are known
        update_epoch = tf.assign(epochCount, epochCount + 1)
    
        embeddings = tf.get_variable( 'embeddings', dtype=tf.float32,
            initializer= tf.random_uniform([vocabulary_size, embedding_sizeE], -1.0, 1.0, dtype=tf.float32) )
    
        softmax_weights = tf.get_variable( 'softmax_weights', dtype=tf.float32,
            initializer= tf.truncated_normal([vocabulary_size, embedding_sizeE],
                                 stddev=1.0 / math.sqrt(embedding_sizeE), dtype=tf.float32 ) )
    
        softmax_biases = tf.get_variable('softmax_biases', dtype=tf.float32,
            initializer= tf.zeros([vocabulary_size], dtype=tf.float32),  trainable=False )
    
        embed = tf.nn.embedding_lookup(embeddings, train_dataset) #train data set is
    
        embed_reshaped = tf.reshape( embed, [batch_sizeE*num_inputs, embedding_sizeE] )
    
        segments= np.arange(batch_size).repeat(num_inputs)
    
        averaged_embeds = tf.segment_mean(embed_reshaped, segments, name=None)
    
        print(softmax_weights )
        print(softmax_biases )
        
        if mode == ""train"":
        
            sSML = tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=averaged_embeds,
                labels=train_labels, num_sampled=64, num_classes=3096637)
    
            loss = tf.reduce_mean( sSML )
    
            optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss) 
    
        saver = tf.train.Saver()



    word2vecEstimator = tf.estimator.Estimator(
            model_fn=my_model,
            params={
                'batch_size': 16,
                'embedding_size': 10,
                'num_inputs': 3,
                'num_sampled': 128
            })
    
    word2vecEstimator.train(
        input_fn=generate_batch,
        steps=10)",0,1
72,2018-11-22,2018,11,22,22,9zdzgo,Help with GPU-ResourceExhaustedError,https://www.reddit.com/r/tensorflow/comments/9zdzgo/help_with_gpuresourceexhaustederror/,FrStealer,1542892298,"Hello,

I have got a memory error while training on inception_resnet_v2.
Implementation : https://github.com/davidsandberg/facenet/blob/master/src/models/inception_resnet_v2.py.

I can test in pretty well, I can calculate the loss, the embedding so I assume the error is in the gradiant calculation.  As soon as I want to train I've got a memory error:

""Limit:                 11502629683
InUse:                 11487078144
MaxInUse:              11487078144
NumAllocs:                   46485
MaxAllocSize:           4294967296""

The training code is here : https://github.com/davidsandberg/facenet/blob/master/src/facenet.py  line 168

I have got Tensorflow 1.10.1, Cuda: 7.5.17 with the graphic TITAN X (Pascal) On Ubuntu16.

I've tried different things i saw in other forum, but nothing works so far.
Thank you in advance.",6,1
73,2018-11-22,2018,11,22,23,9zed1o,A single RTX 2070 or Multiple 1070 ti ?,https://www.reddit.com/r/tensorflow/comments/9zed1o/a_single_rtx_2070_or_multiple_1070_ti/,crytoy,1542895548,"In a tensorflow workspace, would you recommend a single RTX 2070 or 2 (1070 ti) cards ?",11,1
74,2018-11-23,2018,11,23,1,9zfghe,What is the difference between Model.train_on_batch from keras and Session.run([train_optimizer]) from tensorflow?,https://www.reddit.com/r/tensorflow/comments/9zfghe/what_is_the_difference_between_modeltrain_on/,zedt,1542903763,,0,1
75,2018-11-23,2018,11,23,18,9zmteu,Huawei Matebook X Pro - Tensorflow Support and eGPU Tensorflow Support?,https://www.reddit.com/r/tensorflow/comments/9zmteu/huawei_matebook_x_pro_tensorflow_support_and_egpu/,ladiesman217-ab,1542964398,"Hey guys, is there anyone out there who has a Huawei Matebook X Pro and has installed Tensorflow on it? I'm really considering on buying the laptop but I haven't seen any posts on anyone using this laptop for machine learning applications.

I'm a university student, and I'd like to have a single portable laptop I can use on the go. The computer has the GeForce MX150, and because of this GPU there have apparently been some issues with installing CUDA 9 (which is a requirement to install Tensorflow), and it seems that the solution is to generally install CUDA 9.1 and build Tensorflow from source. ([https://devtalk.nvidia.com/default/topic/1023574/announcements/cuda-toolkit-9-is-not-available-in-geforce-mx150/](https://devtalk.nvidia.com/default/topic/1023574/announcements/cuda-toolkit-9-is-not-available-in-geforce-mx150/))

I understand the Matebook X Pro doesn't have the best GPU for machine learning, but ideally back at the dorm, I would have an eGPU and hook it up to the Matebook X Pro since it has Thunderbolt 3 to do the more hardcore machine learning. On the go though, I could run basic machine learning programs on the MX150. Also, the laptop is renowned for it's amazing specs better than a Macbook Pro and costs less than a Macbook Pro, so it's an ideal ""on the go"" laptop for other general purpose work.

So I guess to enumerate my questions:

1. Have you installed CUDA on your Huawei Matebook X Pro? If so, what version?
2. Were you able to successfully install Tensorflow?
3. Do you think it's possible to designate ML tasks on the MX150 or the eGPU (would this be as easy as a call to cudaSetDevice)? This would be helpful since on the go, I'd be running ML code on the MX150 while in my dorm I'd be running ML code exclusively on the eGPU like I mentioned before.
4. Would I have to have a separate CUDA installation for the eGPU, and if I do, would that interfere with the CUDA installation for the MX150?
5. Have you run into any problems doing any of the above, or do you have any tips/advice for what I should consider?

I don't really want to gamble on spending more than a thousand bucks to see whether or not this setup could potentially work, so I wanted to know if anyone else has done this before.

Also for some additional details, the Huawei Matebook X Pro will be running on Windows 10, and let's say the eGPU is a high-end model like the 1080 Ti.

Thanks!",1,1
76,2018-11-24,2018,11,24,8,9zt97k,FIRST style transfer,https://www.reddit.com/r/tensorflow/comments/9zt97k/first_style_transfer/,jabbaluck,1543015670,"Hey guys i just did one of my style transfers !!!

&amp;#x200B;

What do you think ??&gt;:d&lt; +

&amp;#x200B;

&amp;#x200B;

[https://www.facebook.com/photo.php?fbid=2087571774597327&amp;set=a.162418863779304&amp;type=3&amp;theater](https://www.facebook.com/photo.php?fbid=2087571774597327&amp;set=a.162418863779304&amp;type=3&amp;theater)",1,1
77,2018-11-24,2018,11,24,10,9zu0qz,Creating TFRecords from dataset with multiple annotated classes per image.,https://www.reddit.com/r/tensorflow/comments/9zu0qz/creating_tfrecords_from_dataset_with_multiple/,acidafterglow,1543021725,"I've been using a modified script from the pets example to deal with my dataset, but that was using a single class per image (and for the whole dataset too, lol).

&amp;#x200B;

Now I have bounding boxes for vehicles, tires, license plates, etc. all in a single image/annotation, but I don't know how to proceed for creating the TFRecord files since, AFAIK, it only works for a single class per image.  


Is this even the right approach to take? Either way, I'd be grateful if someone could point me out to the right resources to solve my problem.  


I'm very new (3-4\~ months) to this since it was basically slapped to my face at my job. So please let me know if any other info is needed to provide a helpful reply.",1,1
78,2018-11-25,2018,11,25,22,a08nab,Why tf.data is much better than feed_dict and how to build a simple data pipeline in 5 minutes.,https://www.reddit.com/r/tensorflow/comments/a08nab/why_tfdata_is_much_better_than_feed_dict_and_how/,dominik_schmidt,1543154125,,10,1
79,2018-11-26,2018,11,26,3,a0b1hs,"For anyone looking to get into deep learning, I would advise that you consider not learning the behemoth libraries like Tensorflow or Theano, but instead learn how to use a high-level API like Keras. Here's a quick video to explain what it is. Hope I was helpful!",https://www.reddit.com/r/tensorflow/comments/a0b1hs/for_anyone_looking_to_get_into_deep_learning_i/,antaloaalonso,1543171371,,3,1
80,2018-11-26,2018,11,26,12,a0fjbm,tf metric accuracy is zero,https://www.reddit.com/r/tensorflow/comments/a0fjbm/tf_metric_accuracy_is_zero/,hassanzadeh,1543202478,"Hello guys,

I have an acc and acc\_update\_op which I initialize with the output of tf.metric.accuracy()

when I initialize the local variables and run it, i.e. acc\_np, acc\_op\_np = [session.run](https://session.run)(\[acc,acc\_update\_op\]) I get 0 for the acc\_np. I'm confused, acc\_np was supposed to a tensor that is equal to count/total, in the computational graph, why is zero then?

of course if I run [session.run](https://session.run)(acc) for the second time I get the right accuracy, but don't I get it in the first run? After all this the main idea behind the computational graph (i.e. operations are done in proper order), is it not? what am i missing?",1,1
81,2018-11-28,2018,11,28,18,a14tsi,Free PDF eBook: TensorFlow Machine Learning Cookbook,https://www.reddit.com/r/tensorflow/comments/a14tsi/free_pdf_ebook_tensorflow_machine_learning/,PacktStaff,1543398708,,1,1
82,2018-11-29,2018,11,29,4,a198ih,TensorFlow Object Detection API in 5 clicks from Colaboratory,https://www.reddit.com/r/tensorflow/comments/a198ih/tensorflow_object_detection_api_in_5_clicks_from/,nbortolotti,1543431885,interesting approach to represent object-detection model using some clicks. [Colab-Proposal](https://medium.com/@nickbortolotti/tensorflow-object-detection-api-in-5-clicks-from-colaboratory-843b19a1edf1). ,0,1
83,2018-11-29,2018,11,29,10,a1cksr,VideoCapture of IP Camera RTSP with Digest Authentication,https://www.reddit.com/r/tensorflow/comments/a1cksr/videocapture_of_ip_camera_rtsp_with_digest/,stateofidleness,1543454306,"Hi all! New to Tensorflow and wanting to do some object detection. I have a very small working example in Python 3 with opencv, keras, and tensorflow. Ive been successful with a still image file, and also using VideoCapture(0) with local USB webcam. 

Now I simply wanted to substitute the 0 for the RTSP url to an IP camera on the LAN, but it gives me a 401 Unauthorized error. I HAVE tested inclusion of the credentials in the URL, but same error.

What is confusing, is the same exact url streams perfectly in VLC.

Ive yet to find a working example online of someone doing this with an IP camera behind digest authentication.

Additional notes:
Bosch camera
Disabling authentication is not an option
Login works fine through VLC

Has anyone managed to use the VideoCapture function with an RTSP stream using digest auth?",3,1
84,2018-11-29,2018,11,29,11,a1dah5,Pluto Open Project: author name disambiguation,https://www.reddit.com/r/tensorflow/comments/a1dah5/pluto_open_project_author_name_disambiguation/,yo__on,1543459732,"Hello reddit/tensorflow,  
[Pluto network](https://pluto.network) is a team working to break down the academic barriers, and we are offering [Scinapse](https://scinapse.io), academic search engine service.

We are putting a lot of efforts on cleansing the large database of academic records usually referred to as citation graph. Currently, the focus is on disambiguating authors using machine learning. 

We plan to OPEN this project and make anyone can participate in the project.   
If you are interested, please feel free to contact us!   
For more detail, please check the link!  
[https://medium.com/pluto-network/pluto-open-project-author-name-disambiguation-4ce956471efb](https://medium.com/pluto-network/pluto-open-project-author-name-disambiguation-4ce956471efb)",0,1
85,2018-11-29,2018,11,29,17,a1fobc,DropoutWrapper Question,https://www.reddit.com/r/tensorflow/comments/a1fobc/dropoutwrapper_question/,thisisiron,1543481397,"Could you explain about input\_keep\_prob and output\_keep\_prob?

&amp;#x200B;

You can give a brief explanation and you can add a link to your comment.",0,1
86,2018-11-29,2018,11,29,19,a1gcp8,Caveat Emptor: Broken TensorFlow &amp; Keras Integration (both Eager and Graph),https://www.reddit.com/r/tensorflow/comments/a1gcp8/caveat_emptor_broken_tensorflow_keras_integration/,Mr_Ubik,1543488902,"While all examples from the TensorFlow team have been moved to the new Keras + TF 2.0 API, and by watching/reading promotional material you might be getting the idea that you can start using these API today, well you are mistaken. We are currently trying to shed light on the following issues:

[https://github.com/tensorflow/tensorflow/issues/23873](https://github.com/tensorflow/tensorflow/issues/23873)

[https://github.com/tensorflow/tensorflow/issues/23875](https://github.com/tensorflow/tensorflow/issues/23875)

I personally hope that the mistake is on my part but it would be nice to have either eyes look into it.",1,1
87,2018-11-29,2018,11,29,22,a1h941,Question about tf.nn.bidirectional_dynamic_rnn,https://www.reddit.com/r/tensorflow/comments/a1h941/question_about_tfnnbidirectional_dynamic_rnn/,thisisiron,1543497444,"What is difference between state and output in tf.nn.bidirectional\_dynamic\_rnn ?

&amp;#x200B;

When will the state and output be used respectively? 

&amp;#x200B;

 If I look at some code, why do it use state for ""last"" and output for other?

Example.

if last:

Using state that bidirectional\_dynamic\_rnn returns

....

else:

Using output that bidirectional\_dynamic\_rnn returns

....

 

A link or a simple answer is okay.",0,1
