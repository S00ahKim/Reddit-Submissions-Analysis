,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2019-7-2,2019,7,2,4,c7yw94,I'm having some troubles with my first TF AI,https://www.reddit.com/r/tensorflow/comments/c7yw94/im_having_some_troubles_with_my_first_tf_ai/,ph04,1562009544,"basically, i wanted to try to build an AI to generate some text based on the original italian version of the ""Divine Comedy"".

the problem is that i've never built anything with TF on my own so i stole the code from a TF tutorial from here   [https://www.tensorflow.org/tutorials/sequences/text\_generation](https://www.tensorflow.org/tutorials/sequences/text_generation). this is the script i came out with, it's basically the same thing [https://gist.github.com/ph04/0084a1478079f138667a8bf7e2b78623](https://gist.github.com/ph04/0084a1478079f138667a8bf7e2b78623), except for the fact that i tried to thange every single possile editable or tweakable thing but no matter what i do i always end up with the same results (which you can see in the picture down below)

i know it seems like 14 epochs were not enough to change, but trust me it sticks on the same values

LOSS: \~2.4

ACC: \~0.02

i really need to know what am i missing, since i've also tried to wait all 200 epochs, change everything i could, i generated text with those weights many times but had always the same results because, you know, no loss change, no better AI performance.

 [https://gist.github.com/ph04/331de1e1ff2b23a415fef4604ca3ccc3](https://gist.github.com/ph04/331de1e1ff2b23a415fef4604ca3ccc3) generator script

&amp;#x200B;

(also sorry for the broken codes but they were tests, i think the main parts are pretty much understandable)

https://i.redd.it/3x4r74zztq731.png",15,6
1,2019-7-2,2019,7,2,10,c832sr,Bunch of Web Development books including TensorFlow's are now discounted!,https://www.reddit.com/r/tensorflow/comments/c832sr/bunch_of_web_development_books_including/,PatAllman,1562031296,,7,0
2,2019-7-2,2019,7,2,20,c882k0,"Best way to convert traditional images into MNIST format, for testing purposes on a CNN trained by MNIST dataset?",https://www.reddit.com/r/tensorflow/comments/c882k0/best_way_to_convert_traditional_images_into_mnist/,Fengax,1562066681,"I have just trained my first CNN network by using the MNIST dataset, it is the most famous handwriting dataset. However, instead of using their testing images, I want to **utilize my own 28x28 testing images.**

The rationale behind this, is that I want to make a handwriting recognition program, so obviously I need a way to convert traditional image format to the one-dimensional MNIST format, so that the CNN can read it.

What is the best way to accomplish this task?",8,4
3,2019-7-3,2019,7,3,0,c8aa4h,"Does TFRecord file have to contain the original image, or can it contain a reference?",https://www.reddit.com/r/tensorflow/comments/c8aa4h/does_tfrecord_file_have_to_contain_the_original/,neuronet,1562079739,"I am learning the tf object detection API, and working with images that are *very* large (often 10kpixel by 20kpixel). The point of this research is to do high-res object tracking, so we do not want to downscale and then track, but to do full resolution tracking (we often have a hundred objects in our images that need to be tracked).  

The problem is using the standard TFRecord construction, where you literally include the image itself as a feature, by the time we reach a dozen training images we will have prohibitively large TFRecord files. 

Is there a way to build the record files/do transfer learning in a way that the record files only need to include a pointer to/reference to the original file? Or do I need to just create a bunch of TFRecord files, maybe one for each image, or one every N images?",7,1
4,2019-7-3,2019,7,3,2,c8cec1,What does the output of Keras model.predict mean?,https://www.reddit.com/r/tensorflow/comments/c8cec1/what_does_the_output_of_keras_modelpredict_mean/,roset_ta,1562090067,"Hello,


I'm working on an image segmentation problem where each image of my test set has a counterpart binary mask. So, the ground truth masks are one - hot encoded, so each pixel is either 0 or 1. 

I used Keras model.predict to evaluate my model. My question is about the predicted masks I get, since each pixel is a value between 0 and 1. What do these values mean? Are they the probabilities of each pixel belonging to the class of interest (i.e. being the segmented object) ? And if so, how do I know the class the model predicted for each pixel value?",2,3
5,2019-7-3,2019,7,3,7,c8g071,Trying out TF2.0 Keras - error on model.fit(),https://www.reddit.com/r/tensorflow/comments/c8g071/trying_out_tf20_keras_error_on_modelfit/,ME_PhD,1562106408,"I'm playing around with the new Keras API and want to fit -4x\^3 + 2x\^2 -x + 5 . The program should optimize the variable ""w"" to the value \[-4, 2, -1, 5\] .

Here's my code which can run the model.predict(x) without problems, but [model.fit](https://model.fit)() doesn't work:

    import tensorflow as tf
    import numpy as np
    
    # making ""training set""
    def f(x, w):
        return w[0] * x**3 + w[1] * x**2 + w[2] * x + w[3]
    
    coeffs = [-4, 2, -1, 5]
    x_train = np.linspace(0, 5, 100)
    y_train = f(x_train, coeffs)
    
    # Model:
    x = tf.keras.layers.Input(shape=(), dtype=tf.float32)
    w = tf.Variable(np.zeros(4, dtype=np.float32))
    yhat = w[0] * x**3 + w[1] * x**2 + w[2] * x + w[3]
    model = tf.keras.Model(inputs=x, outputs=yhat)
    model.compile(loss='mse', optimizer='adam')
    
    model.predict(x_train) # works fine
    model.fit(x_train, y_train, epochs=100) 
    # ValueError: operands could not be broadcast together with shapes (32,) (4,) (32,)",1,2
6,2019-7-3,2019,7,3,9,c8h75w,[Beginner] How can I add independent values to an InceptionV3 CNN model?,https://www.reddit.com/r/tensorflow/comments/c8h75w/beginner_how_can_i_add_independent_values_to_an/,NingaMEW,1562113010,"I'm using a slightly-modified version of the InceptionV3 CNN model to create a self-driving AI.  To collect data, I record myself playing the game, and every frame records the image of the game on my screen, and the state of a few inputs on my game controller (turn, throttle, and break).  Each value ranges from 0 to 1.

The model takes in an X value, which is the screen, and a Y value, which contains the values for each control for that frame.

The problem is that when I use the prediction function after training my AI, it returns a balanced array which adds up to 1.  It's not treating the inputs separately, but as a whole.

&amp;#x200B;

[INPUT CONTROLS ARRAY](https://i.redd.it/4svlz8aqdz731.png)

The input controls array is paired with the screen for each frame.

The left variable is turn; .5 would represent straight, 0 left, and 1 right.  Middle variable is throttle intensity, right is brake intensity. 

[OUTPUT ARRAY](https://i.redd.it/rg3mj60udz731.png)

&amp;#x200B;

Sorry I'm such a noob at this.",9,1
7,2019-7-3,2019,7,3,12,c8iu85,Beginner Question on Hardware,https://www.reddit.com/r/tensorflow/comments/c8iu85/beginner_question_on_hardware/,BMXnotFIX,1562122957,"I am brand new to TensorFlow and ml in general. I have a couple options on what to use to run it and was wondering which would be best. The first option would be a Windows machine with an i7 8700k, gtx 1080 gpu, and 16gb of ddr4. The second option, which I assume would make the most sense, is my homelab server which is a Dell r710 (I can technically run any OS via virtualization, but I would be using Ubuntu most likely as I am more familiar with the Python api.).

&amp;#x200B;

Now, I'm assuming the r710 would be best suited due to it's more workhorse characteristics, but I read that gaming oriented CPU/GPUs actually worked in TensorFlow's favor due to hyperthreading/parallel processing, so I figured I'd double check with the community.",8,1
8,2019-7-3,2019,7,3,12,c8j07y,Any resources for learning Tensorflow for Deep Learning.,https://www.reddit.com/r/tensorflow/comments/c8j07y/any_resources_for_learning_tensorflow_for_deep/,pradeep_sinngh,1562123990,,2,0
9,2019-7-3,2019,7,3,21,c8npjv,Need more tutorials in TensorFlow sub classing,https://www.reddit.com/r/tensorflow/comments/c8npjv/need_more_tutorials_in_tensorflow_sub_classing/,begooboi,1562158265,I love keras and I have worked with its sequential and functional api but not the sub classing api. I searched the web and there is not much tutiorials explaing sub classing in keras or tf.keras. Even if it exists the example codes or explanations would be complex. I find this method more flexible and powerful than sequential or functional api's.,1,1
10,2019-7-3,2019,7,3,23,c8orum,Tensorflow Keras Layer Reshape: Is this a bug?,https://www.reddit.com/r/tensorflow/comments/c8orum/tensorflow_keras_layer_reshape_is_this_a_bug/,Mushoz,1562164247,"The following sample code doesn't work for me, and I am suspecting it's a bug. Could someone please confirm my suspicion? Or am I doing something wrong?

&amp;#x200B;

    import tensorflow as tf
    import numpy as np
    
    def Model():
        
        x = tf.keras.layers.Input((4,4,3)) # 4x4 image with 3 channels
        y = tf.keras.layers.Conv2DTranspose(3, 4, 2, padding='same') (x) # Creates a 8x8 images with 3 channels by upsampling with stride 2
        
        linear = tf.keras.layers.Dense(8*8*3) (x) # Linear transformation of the input
        linear = tf.keras.layers.Reshape([8, 8, 3]) (linear) # Reshapes the output of the linear transformation to the same shape as the upsampled image
        y = tf.keras.layers.Add() ([y, linear]) # adds them together
        
        return tf.keras.models.Model(inputs=x, outputs=y)
    
    # model
    model = Model()
    
    # input data
    array_range = np.random.randn(128, 4, 4,  3).astype(np.float32)
    dataset = tf.data.Dataset.from_tensor_slices(array_range).batch(8)
    iterator = dataset.make_one_shot_iterator()
    next_element = iterator.get_next()
    dataset_output = model(next_element)
    
    # session
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())
    
    # evaluate
    print(sess.run(dataset_output))
    print(sess.run(dataset_output))

&amp;#x200B;

As far as I can tell, the dimensions as correct. Reshape works correctly in regular feed forward models. But when used within a skip connection as is done in the sample connection, I am getting the following error:

&amp;#x200B;

    Input to reshape is a tensor with 24576 values, but the requested shape has 1536",3,2
11,2019-7-4,2019,7,4,1,c8pz7o,TensorFlow Lite Ported to Arduino by Adafruit,https://www.reddit.com/r/tensorflow/comments/c8pz7o/tensorflow_lite_ported_to_arduino_by_adafruit/,blinka_friendlysnake,1562170381,,1,20
12,2019-7-4,2019,7,4,3,c8rqzi,noob trying to install Tensorflow for a class - getting error message?,https://www.reddit.com/r/tensorflow/comments/c8rqzi/noob_trying_to_install_tensorflow_for_a_class/,highc1157,1562178838,"i have python 3.7

in CMD i ran 'pip install tensorflow'

generated this after a bunch of installs happened :

t**ensorboard 1.14.0 has requirement setuptools&gt;=41.0.0, but you'll have setuptools 40.8.0 which is incompatible**",3,0
13,2019-7-4,2019,7,4,7,c8uu44,Tensorflow Broke My Oculus,https://www.reddit.com/r/tensorflow/comments/c8uu44/tensorflow_broke_my_oculus/,sstovall19,1562193998,I did a lot of nvidia-related things to my Windows 10 GeForce GTX-1070 MAX-Q Laptop to get Tensorflow-gpu to work but now my previously working Oculus rig gives me black screen in the headset. Any thoughts?,4,0
14,2019-7-5,2019,7,5,6,c984bk,Eigenvalue tasks in Tensorflow?,https://www.reddit.com/r/tensorflow/comments/c984bk/eigenvalue_tasks_in_tensorflow/,HypoCelsus,1562276754,"Hello all    
I am looking for a library that can find the eigenvalues of a matrix that has the following characteristics:    
* Sparse (&lt;5% non-zero entries)    
* Complex + Hermitian (equal to its conjugate transpose)    
It ain't exactly what TF is about but I've tried MAGMA  with no luck. Maybe something new has come along since I've looked around last.",1,2
15,2019-7-5,2019,7,5,7,c98r65,Should I learn TF1 or TF2?,https://www.reddit.com/r/tensorflow/comments/c98r65/should_i_learn_tf1_or_tf2/,AnomalyNexus,1562280528,"Right at the start of my TF journey so opted for TF1 on the assumption that there will be better resources and guides available.

This thread has made me second-guess that thinking though:

https://www.reddit.com/r/tensorflow/comments/c6yaqt/after_tf2_can_we_quietly_forget_tf1_ever_happened/

Get the basics on TF1 first or dive straight into TF2 and potentially struggle with incompatible tutorials?",6,2
16,2019-7-5,2019,7,5,8,c991ht,TF vs LuaTorch,https://www.reddit.com/r/tensorflow/comments/c991ht/tf_vs_luatorch/,hassanzadeh,1562282328,"Hey Guys,

I had two set of legacy codes one which does a 1D CNN and the other that does 1D CNN + 2 layers of LSTM. I recently changed that into Tensorflow, the logic is similar, but my torch code runs 10 times faster while using only ONE cpu, my TF runs on 8 cpu and yet is about 10 times slower for the 2nd model and about 5 times slower for the first one. 

Is that what it is really supposed to be? The running time difference is huge. 

&amp;#x200B;

Thanks",2,1
17,2019-7-5,2019,7,5,14,c9ce4m,TensorFlow.js Bringing Machine Learning to the Web and Beyond,https://www.reddit.com/r/tensorflow/comments/c9ce4m/tensorflowjs_bringing_machine_learning_to_the_web/,RosemaryRol,1562306245, [https://morioh.com/p/d3f3aa2a604f](https://morioh.com/p/d3f3aa2a604f),0,1
18,2019-7-5,2019,7,5,19,c9eaek,Linkedin Opensources Avro2TF: An Open Source Deep learning tool for Feature Transformation for TensorFlow,https://www.reddit.com/r/tensorflow/comments/c9eaek/linkedin_opensources_avro2tf_an_open_source_deep/,bhavinjawade,1562321365,,0,12
19,2019-7-6,2019,7,6,11,c9ofji,Urgent help needed. I'm required to learn TensorFlow and basically all of DL superficially in 3 DAYS. Please help me do this!,https://www.reddit.com/r/tensorflow/comments/c9ofji/urgent_help_needed_im_required_to_learn/,anotheraccount97,1562380012," I got an internship through my university's 6 month industrial experience program where I was allowed to brag/overstate with fake Skills in order to get allotted a decent organization. I know Soft Dev (Java,DSA etc etc), Python somewhat.

&amp;#x200B;

They've given me a deep learning profile on the basis of my mention of Tensorflow, Keras etc. Now the intern starts on Monday(3 days from now), so I'll need to learn it superficially so that my manager doesn't get the hint that I was lying. In the course of first week, I wish to learn as much as possible, and then with the ongoing internship of 6 months, I'll try to do my best to become an expert in the same.

&amp;#x200B;

Please help me by stating exact resources, short courses that I can binge upon, and links of videos of workshops/bootcamps that I can go through this Sat/Sun/Mon to gain a presentable knowledge of the DL field, also some ML (I don't know ML too, extremely sorry for the disrespect to this beautiful field). How do I go about this humongous task? It's a question upon my career. Apologies and Thanks.",25,0
20,2019-7-7,2019,7,7,2,c9w7zy,Is there a way to do this without a loop?,https://www.reddit.com/r/tensorflow/comments/c9w7zy/is_there_a_way_to_do_this_without_a_loop/,ronsap123,1562434400,"Suppose I have two tensors:

Tensor A of shape (10, 100, 32)

Tensor B of shape (10, 32)

&amp;#x200B;

Now I do something like this:

&amp;#x200B;

result = \[\]

for i in range(0,10):

result.append(  dot(B\[i\],A\[i\].transpose() )

&amp;#x200B;

Is there a way to get this result without a for loop?",6,3
21,2019-7-7,2019,7,7,19,ca5md8,Can anyone please help me to solve this problem,https://www.reddit.com/r/tensorflow/comments/ca5md8/can_anyone_please_help_me_to_solve_this_problem/,Pratik668,1562496909,"Create a Variable that will contain an array of numbers. Create a Variable ""n"" that will contain a number. 

 Now, create a Session. Run it for 10 iteration. At every iteration, take a value as input from user. 

Use that input to modify the value of ""n"" .

 Then modify the \[n % len(array)\] th index of the array node value by multiplying the previous value with n.

 Example: arr = \[1, 2, 3\] 

n = 0,  Inside Loop:  1st iteration: 

n = 2 (input by user) 

index = n % len(arr) =&gt; 2 % 3 =&gt; 2 

Therefore, arr\[2\] = arr\[2\] \* n =&gt; 2\*2 =&gt; 4   

 Updated value of arr = \[1, 4, 3",2,1
22,2019-7-8,2019,7,8,0,ca7w1f,TensorFlow 2.x C++ API - are any changes planned from TensorFlow 1.x C++ API?,https://www.reddit.com/r/tensorflow/comments/ca7w1f/tensorflow_2x_c_api_are_any_changes_planned_from/,cdahms,1562512525,"As we all know by now, the TensorFlow 2.x Python interface is moving away from old-school TensorFlow with sessions to Keras as the default, which we pretty much all feel is a good change.

My question is, is there any change to this effect planned for the TensorFlow 2.x C++ interface?

Currently I'm training TensorFlow graphs in Python and inferencing them in C++ (the inferencing has to be done from a product written in C++ so this is a necessity). The C++ TensorFlow inferencing code I've developed is based on the TensorFlow GitHub C++ examples, especially this one:

[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label\_image/main.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc)

Referring to the TensorFlow webpage for the C++ API as of when I'm writing this (June 2019):

[https://www.tensorflow.org/guide/extend/cc](https://www.tensorflow.org/guide/extend/cc)

They are still using sessions in the C++ explanation, whereas all the Python examples on the TensorFlow site now use the Keras interface.

Also, all the articles I can find about TensorFlow 2.x mention moving away from sessions towards the Keras interface, but none ever mention C++ specifically or have any C++ examples to this effect.

Based on this, I'm inclined to believe at this time that when TensorFlow 2.x gets past Beta into the regular releases that the C++ interface will still be old-school session based. Can anybody confirm or deny this is correct? Is there any official word on staying with sessions vs. moving away from sessions from Google for the C++ interface specifically?",0,8
23,2019-7-8,2019,7,8,7,cad6ok,Need help exporting retrained MobileNetV2 web,https://www.reddit.com/r/tensorflow/comments/cad6ok/need_help_exporting_retrained_mobilenetv2_web/,NebulousGoat,1562540132,"Hello,

I am currently in the process of retraining the ssd_mobilenet_v2_coco from the [tensorflow zoo examples](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models). In order to do so I created record files out of tf.Examples for my test and train images by following this [tutorial](https://medium.com/object-detection-using-tensorflow-and-coco-pre/object-detection-using-tensorflow-and-coco-pre-trained-models-5d8386019a8). With the only difference being that I am using `model_main.py` instead of the legacy `train.py`, since I ran into the same issues with the legacy version and was hoping the new version would fix them. 

The issue comes when trying to convert one of the trained checkpoints, or the final model to tfjs. 

Running `export_inference_graph.py --input_type=image_tensor ...` on the generated `model.chkpt-???` files creates a saved_model where all the BatchNorm tensors are empty. This works fine in Python and I'm able to extract the num_detections, scores, boxes, and classes after feeding in an image. However, running the tfjs converter leaves in all the `is_training : true` data, resulting in the web model to crash as it tries to utilize the empty BatchNorm tensors. 

If I try using the saved_model that `model_main.py` produces after reaching the max number of training steps I run into a different issue. The converted model.json file still contains all the is_training data, but the expected input is now a tf_example instead of an image_tensor. I can't seem to find any documentation that talks about how to transform an html img into a tf_example, so I am unsure of where to go from here. 

Most issues reported on github that are similar claim to have been fixed, or don't have any purported solution. I'd prefer to somehow work around my already trained model, without needing to retrain the whole thing. 

Any help will be greatly appreciated. 

---

It should be noted that running the `tensorflowjs_converter` on the model straight from the zoo example works without issue and is able to make predictions in the web. Just that introducing tf.Examples to retrain it seems to have removed that ability and somehow mucked up the final model.json file. 

Also I'm running tensorflow 1.13.1, and the converter issues are present in both 0.8.6 and 1.2.2.1.",1,2
24,2019-7-8,2019,7,8,14,cagsu2,A Beginners Guide to Tensorflow,https://www.reddit.com/r/tensorflow/comments/cagsu2/a_beginners_guide_to_tensorflow/,javatpoints,1562562352,,0,1
25,2019-7-8,2019,7,8,22,cakuir,"AdaBoost, Simplest Example",https://www.reddit.com/r/tensorflow/comments/cakuir/adaboost_simplest_example/,bhavesh91,1562591500,,0,3
26,2019-7-9,2019,7,9,2,canxi3,Tensorflow Graph Construction Optimization,https://www.reddit.com/r/tensorflow/comments/canxi3/tensorflow_graph_construction_optimization/,timsua,1562606425,"The graph construction process for my model is taking ~4 minutes at the moment, which is super annoying. Does anyone know any tips or tricks for debugging the actual graph construction process of tensorflow? Are there common ""best practices"" that you are aware of?",2,1
27,2019-7-9,2019,7,9,4,caprrn,"I started recreating the This Person Does Not Exist project, generating faces with a GAN. I had to write nearly everything from scratch!",https://www.reddit.com/r/tensorflow/comments/caprrn/i_started_recreating_the_this_person_does_not/,Fear_UnOwn,1562614545,,14,21
28,2019-7-9,2019,7,9,6,carkjn,TensorflowJS Math Operations?,https://www.reddit.com/r/tensorflow/comments/carkjn/tensorflowjs_math_operations/,Simusid,1562622934,"I'm writing my first TFJS demo app.    I have a well trained cats/dogs binary classifier that I've converted using the converter.    I can load the model and pass images from an iphone or android camera for classification.   Unfortunately, the results are terrible.    My model was trained using a keras ImageDataGenerator with samplewise\_center and samplewise\_std\_normalization both set to true.   I'm now pretty sure I need to do the same normalization in my app.   

&amp;#x200B;

I see tf.math.reduce\_mean and reduce\_std in the full API.   In the JS API I see ts.mean but no ts.std.    Do I need to calculate this from scratch?",0,1
29,2019-7-9,2019,7,9,9,catmbw,This video goes over a breast cancer diagnosis model that uses neural networks (implemented in python),https://www.reddit.com/r/tensorflow/comments/catmbw/this_video_goes_over_a_breast_cancer_diagnosis/,antaloaalonso,1562633403,,0,1
30,2019-7-10,2019,7,10,1,cb38bo,How to get text and numeric labels into correct format for regression?,https://www.reddit.com/r/tensorflow/comments/cb38bo/how_to_get_text_and_numeric_labels_into_correct/,colonel_farts,1562690160,"Using TF 2.0 beta.

&amp;#x200B;

I have a pandas dataframe consisting of lines of text and an associated numeric score. I know what I need to do is convert each line of text into an array of integers, pass that into an embedding layer, then through the rest of the network, and have my last layer be a single output with ReLU. Then I can optimize MSE of the output against the true numeric score associated with that text. 

&amp;#x200B;

Problem is, this sounds easier than it is proving to be to implement. 

I'm trying to loosely follow this tutorial: [https://www.tensorflow.org/beta/tutorials/load\_data/text](https://www.tensorflow.org/beta/tutorials/load_data/text)

I can't seem to appropriate this part of the code for my purposes:

&amp;#x200B;

def labeler(example, index):  
 return example, tf.cast(index, tf.int64)   
labeled\_data\_sets = \[\]  
for i, file\_name in enumerate(FILE\_NAMES):  
 lines\_dataset = tf.data.TextLineDataset(os.path.join(parent\_dir, file\_name))  
 labeled\_dataset = lines\_dataset.map(lambda ex: labeler(ex, i))  
 labeled\_data\_sets.append(labeled\_dataset)

&amp;#x200B;

I can get a TextLineDataset from my text, but I need to label each with the associated numeric score, rather than 0,1,2 like in this tutorial. 

&amp;#x200B;

Any ideas? Thanks in advance.",0,1
31,2019-7-10,2019,7,10,1,cb3ar6,Debugging Question,https://www.reddit.com/r/tensorflow/comments/cb3ar6/debugging_question/,CanadianColours,1562690475,"I was wondering if there was a way to profile the number of Matrix multiplies a model is using on a forward pass.

Additionally, if it's possible to see processor network traffic.",0,2
32,2019-7-10,2019,7,10,3,cb4n02,Help! 4 GPUs Out of Memory for medium sized model.,https://www.reddit.com/r/tensorflow/comments/cb4n02/help_4_gpus_out_of_memory_for_medium_sized_model/,helpmymodel,1562696393,"I am trying to figure out why I get OOM errors when trying to run my CNN-LSTM model on 4 16GB Nvidia gpus.

&amp;#x200B;

My input size is (batch\_size,4,240,576,23). I have no trouble loading the data into memory using a generator so I only load in one batch at a time. The trouble is when I attempt to call the train\_on\_batch function for my model. The model in question only has \~800,000 numbers of parameters. I have tried many different ways of using the GPU but I always end up getting a OOM error even if I decrease my batch size as low as 4.

I have tried using multi\_gpu\_model and Mirror Variable strategy and neither work, any batch size over 4 fails.

&amp;#x200B;

I want to get a batch size of 16 and it shouldn't take 10 secs to train on one image. I shouldn't have to use all 4 gpus either, preferably one or two.

&amp;#x200B;

What should I do? Any help is much appreciated.

&amp;#x200B;

&amp;#x200B;

Details

OS: Red Hat Enterprise 7.5

Using Keras 2.2.4

Tensorflow,tensorflow-gpu: 1.13.1",10,6
33,2019-7-10,2019,7,10,3,cb4qgj,Question About Number of Nodes from Layer to Layer,https://www.reddit.com/r/tensorflow/comments/cb4qgj/question_about_number_of_nodes_from_layer_to_layer/,sammybh123,1562696810,"I am a beginner with respect to TF, but have a technical background. I was reading though the basic classification guide on the TF website (https://www.tensorflow.org/tutorials/keras/basic_classification), and I had a questions about how to determine the amount of nodes in layers.

After flattening, the number of nodes for one of the images in the guide above is 784. The next layer is a relu layer that has 128 nodes. 

My questions are: How did we determine that 128 was the best number of nodes to use? My understanding of relu in this context was that it outputted its input, provided the value is positive. If that is the case, how does the number of nodes decrease by a fixed amount every time an image is processed by this layer?

Thanks in advance for your help, and please let me know if I need to clarify my questions!",2,3
34,2019-7-10,2019,7,10,4,cb58gx,Tensorflow and Keras For Neural Networks and Deep Learning,https://www.reddit.com/r/tensorflow/comments/cb58gx/tensorflow_and_keras_for_neural_networks_and_deep/,dhakadeal,1562699018,,0,1
35,2019-7-10,2019,7,10,11,cbaham,Introduction to Machine Learning with TensorFlow.js,https://www.reddit.com/r/tensorflow/comments/cbaham/introduction_to_machine_learning_with_tensorflowjs/,Dorothy109zxy,1562724446, [https://morioh.com/p/d53a9bde1027](https://morioh.com/p/d53a9bde1027),1,2
36,2019-7-11,2019,7,11,0,cbi52w,"How to use a pre-trained tensorflow model ""out-of-the-box""?",https://www.reddit.com/r/tensorflow/comments/cbi52w/how_to_use_a_pretrained_tensorflow_model/,Sinbad_07,1562772585,"   

I am trying to use a pre-trained tensorflow model to be used in  object detection. Specifically, I am looking at the  faster\_rcnn\_inception\_v2\_coco model as many of the objects I would like  to detect are in the COCO dataset. Am I able to use this model out of  the box, or are they more so like templates to train on? Also, how would  I go about testing these then. I am still very new to Tensorflow and  appreciate any help I can get. Thanks!",8,8
37,2019-7-11,2019,7,11,6,cbn0m4,Help needed in converting queues to regular placeholders,https://www.reddit.com/r/tensorflow/comments/cbn0m4/help_needed_in_converting_queues_to_regular/,nlpredditproject,1562795475,"Hi,

&amp;#x200B;

I've been following this tutorial : [https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/](https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/) 

It is using queues for generating the inputs and the targets (next word in text). How can I do the same trick of generating the sliced data, just with placeholders instead of the used queue ?

&amp;#x200B;

Thanks",1,1
38,2019-7-11,2019,7,11,19,cbu8ve,Tensorflow probability layers during training,https://www.reddit.com/r/tensorflow/comments/cbu8ve/tensorflow_probability_layers_during_training/,chaosbambi,1562841860,"Hi I am currently exploring variational autoencoders with tensorflow probability. I understand that instead of the distribution, a sampled tensor is used during training, but how does keras calculate the gradient? Is it a reparametrization trick?",3,2
39,2019-7-11,2019,7,11,20,cbupek,Looking for best reference to learn tensorflow :),https://www.reddit.com/r/tensorflow/comments/cbupek/looking_for_best_reference_to_learn_tensorflow/,cedzkii,1562845109,,7,2
40,2019-7-11,2019,7,11,20,cburer,Mask RCNN predicted class values and logits do not match,https://www.reddit.com/r/tensorflow/comments/cburer/mask_rcnn_predicted_class_values_and_logits_do/,ambodi,1562845462,"I am using the ""mask\_rcnn\_inception\_v2\_coco\_2018\_01\_28"" model downloaded from Zoo model ([https://github.com/tensorflow/models/blob/master/research/object\_detection/g3doc/detection\_model\_zoo.md](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)).

I am trying to access the tensors that are holding the logits for  each class for the image that is fed to the model. I found out that **SecondStageBoxPredictor/Reshape\_1** is holding those values, however when comparing them with **detection\_classes**,  I realized that the values do not match. I want to first find out where  are the correct logits and then document this as it is useful to have  these documented correctly.

&amp;#x200B;

    with tf.Session(graph=tf.Graph()) as sess:
        tf.saved_model.loader.load(sess, ['serve'], GRAPH_PB_PATH)
        graph = tf.get_default_graph()
    
        image = Image.open(IMAGE_PATH)
        (im_width, im_height) = image.size
        image_np = np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)
        image_np_expanded = np.expand_dims(image_np, axis=0)
        
        image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')
    
        tensor_dict = {}
        
        tensor_dict['SecondStageBoxPredictor/Reshape_1:0'] = graph.get_tensor_by_name('SecondStageBoxPredictor/Reshape_1:0')
        tensor_dict['detection_classes:0'] = graph.get_tensor_by_name('detection_classes:0')
        
        output_dict = sess.run(tensor_dict,
                                 feed_dict={image_tensor: image_np_expanded})

Btw, the same goes with the following tensors:

&amp;#x200B;

    tensor_dict['Reshape_12'] = graph.get_tensor_by_name('Reshape_12:0')
    tensor_dict['scale_logits'] = graph.get_tensor_by_name('scale_logits:0')
    tensor_dict['convert_scores'] = graph.get_tensor_by_name('convert_scores:0')
    tensor_dict['BatchMultiClassNonMaxSuppression_1/map/TensorArrayStack_2/TensorArrayGatherV3'] = graph.get_tensor_by_name('BatchMultiClassNonMaxSuppression_1/map/TensorArrayStack_2/TensorArrayGatherV3:0')
    
    for i in range(0, 92):
            np.argmax(output_dict['Reshape_12'][i])
            np.argmax(output_dict['scale_logits'][0][i])
            np.argmax(output_dict['convert_scores'][0][i])
            np.argmax(output_dict['BatchMultiClassNonMaxSuppression_1/map/TensorArrayStack_2/TensorArrayGatherV3'])

Does anyone have an idea why that is the case and how to find the correct logits per class?",0,1
41,2019-7-12,2019,7,12,1,cbxcr7,How to solve this question using tensorflow ?,https://www.reddit.com/r/tensorflow/comments/cbxcr7/how_to_solve_this_question_using_tensorflow/,Pratik668,1562860903,"Create a Variable that will contain an array of numbers. Create a Variable ""n"" that will contain a number.

Now, create a Session. Run it for 10 iteration. At every iteration, take a value as input from user.

Use that input to modify the value of ""n"" .

Then modify the \[n % len(array)\] th index of the array node value by multiplying the previous value with n.

Example: arr = \[1, 2, 3\]

n = 0,  Inside Loop:  1st iteration:

n = 2 (input by user)

index = n % len(arr) =&gt; 2 % 3 =&gt; 2

Therefore, arr\[2\] = arr\[2\] \* n =&gt; 2\*2 =&gt; 4

Updated value of arr = \[1, 4, 3\]",3,0
42,2019-7-12,2019,7,12,1,cbxtv1,Why is my tensorflow model so stupid? (minimal multilayer perceptron),https://www.reddit.com/r/tensorflow/comments/cbxtv1/why_is_my_tensorflow_model_so_stupid_minimal/,alteer,1562863104,"I built a simple multilayer perceptron to solve an XOR problem. I was hoping it would consistently converge on 100% accuracy after a few epochs (2000- which seems like a lot for a domain size of 8), but often the accuracy is still &lt; 100%! Am I missing something?

    from tensorflow.keras import layers
    from itertools import product
    import numpy as np
    
    # Data
    X = np.array(list(product([0,1], repeat=3)))
    y = np.array([a ^ b ^ c for a, b, c in X])
    
    # Define the Model
    INPUT_UNITS = 3
    HIDDEN_LAYER_UNITS  = 5
    OUTPUT_UNITS = 2
    
    model = tf.keras.Sequential()
    model.add(layers.Input(INPUT_UNITS))
    model.add(layers.Dense(HIDDEN_LAYER_UNITS, tf.nn.relu))
    model.add(layers.Dense(HIDDEN_LAYER_UNITS, tf.nn.relu))
    model.add(layers.Dense(HIDDEN_LAYER_UNITS, tf.nn.relu))
    
    model.add(layers.Dense(OUTPUT_UNITS, tf.nn.softmax))
    
    # Specify Train, Test, and Predict
    N_EPOCHS = 200
    model.compile(loss='sparse_categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])
    history = model.fit(x=X,  
        epochs=N_EPOCHS, 
        y=y,
        verbose=0)
    
    model.evaluate(X, y)
    
    predicted = np.argmax(model.predict(x=X), axis=1)
    print(f""Predicted: {predicted}"")
    print(f""Actual: {y}"")",4,3
43,2019-7-12,2019,7,12,7,cc22nx,"What is the optimal eGPU for running Tensorflow on macOS, let's say for a 500 budget and 1000 budget?",https://www.reddit.com/r/tensorflow/comments/cc22nx/what_is_the_optimal_egpu_for_running_tensorflow/,HenslerSoftware,1562882771,,7,2
44,2019-7-12,2019,7,12,7,cc2egq,Lottery Scratcher GAN network using tensorflow and pix2pix,https://www.reddit.com/r/tensorflow/comments/cc2egq/lottery_scratcher_gan_network_using_tensorflow/,skryshtafovychReal,1562884430,,4,3
45,2019-7-12,2019,7,12,18,cc8g1w,What is the optimal eGPU for running Tensorflow on Ubuntu?,https://www.reddit.com/r/tensorflow/comments/cc8g1w/what_is_the_optimal_egpu_for_running_tensorflow/,HenslerSoftware,1562922378,"After posting a request for any eGPU devices with the capability of working with Tensorflow on macOS, it loos as though limitations set by Apple are causing issues.

So I am now looking towards a Ubuntu option.

What are the best benchmarked eGPU devices that work with Tensorflow on Ubuntu with the a maximum budget of 200 and a maximum of 500?",10,4
46,2019-7-13,2019,7,13,1,ccd6zs,How to get submodel' s weights from .hd5 file?,https://www.reddit.com/r/tensorflow/comments/ccd6zs/how_to_get_submodel_s_weights_from_hd5_file/,roset_ta,1562949460,"Hello,


I have a pretrained Keras model saved as a .hd5 file. The model consists of two submodels and a fusion module, where the submodels have been trained independently and then their outputs were used in fusion.
 Is it possible that I only extract the weights for one of the submodels from that file? How can I do that in keras?",0,2
47,2019-7-13,2019,7,13,20,ccoii7,"How to automatically ""fit domain to data"" in Tensorboard",https://www.reddit.com/r/tensorflow/comments/ccoii7/how_to_automatically_fit_domain_to_data_in/,Roboserg,1563018862,Basically you have to press this button ([https://puu.sh/DSha5/1d4d77e9cf.png](https://puu.sh/DSha5/1d4d77e9cf.png)) over and over again during training to fir the graph to the screen. There should be a way to do this problematically / automatically.,7,2
48,2019-7-14,2019,7,14,0,ccqwum,How to count the number of elements in variable in Tensor Flow,https://www.reddit.com/r/tensorflow/comments/ccqwum/how_to_count_the_number_of_elements_in_variable/,Pratik668,1563033580," Suppose I have variable A = tf.Variable(\[1,2,3,4\], dtype=tf.float32) and I want to count the no of elements in it. so the output becomes   count = 4 

how to do this with tensorflow ?",4,3
49,2019-7-14,2019,7,14,2,ccruf1,TypeError: Input 'x' of 'LogicalOr' Op has type float32 that does not match expected type of bool,https://www.reddit.com/r/tensorflow/comments/ccruf1/typeerror_input_x_of_logicalor_op_has_type/,geek_ki01100100,1563038519,"I'm using tf.where in a subroutine with the single argument x that is used in a Keras Lambda layer but keep on getting this error which google showed no matches for

    Traceback (most recent call last):
      File ""viz.py"", line 70, in &lt;module&gt;
        model = model_setup()
      File ""viz.py"", line 51, in model_setup
        scaled_img = Lambda(lambda x: DifferentiableRound(x)/255)(Lambda(scale(0,255))(img))
      File ""/usr/local/lib/python3.5/dist-packages/keras/engine/base_layer.py"", line 457, in __call__
        output = self.call(inputs, **kwargs)
      File ""/usr/local/lib/python3.5/dist-packages/keras/layers/core.py"", line 687, in call
        return self.function(inputs, **arguments)
      File ""viz.py"", line 51, in &lt;lambda&gt;
        scaled_img = Lambda(lambda x: DifferentiableRound(x)/255)(Lambda(scale(0,255))(img))
      File ""viz.py"", line 36, in DifferentiableRound
        return where(cond, (x - tan(K.sin((1 / 1 + 7 ^ (-1 * tan(2 * x)))))), (x - tan(K.sin((1 / 1 + 7 ^ (-1 * tan(2 * x))))) - 0.5))
      File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py"", line 894, in r_binary_op_wrapper
        return func(x, y, name=name)
      File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py"", line 1165, in logical_xor
        gen_math_ops.logical_or(x, y),
      File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 4484, in logical_or
        ""LogicalOr"", x=x, y=y, name=name)
      File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 533, in _apply_op_helper
        (prefix, dtypes.as_dtype(input_arg.type).name))
    TypeError: Input 'x' of 'LogicalOr' Op has type float32 that does not match expected type of bool.",0,1
50,2019-7-14,2019,7,14,12,ccye98,KerasTPUModel has no attribute '_ckpt_saved_model',https://www.reddit.com/r/tensorflow/comments/ccye98/kerastpumodel_has_no_attribute_ckpt_saved_model/,rish-16,1563074977,"Hey everyone,

I'm trying to train a NMT model on a TPU on Colab and when I called `tpu_model.fit(...)` it gives me the error in &lt;TITLE&gt;.

My model is a standard LSTM and I've already done the whole connecting thing with the TPU_COLAB_ADDR and whatnot. That's all set.

Has anyone been through this? Any help would be greatly appreciated!
Cheers!",6,3
51,2019-7-14,2019,7,14,14,ccznui,"Looking for buddies to figure out Tensorflow's Tensor2Tensor library. One of the most prominent TF libraries out there, developed jointly by Google Brain and Google Research.",https://www.reddit.com/r/tensorflow/comments/ccznui/looking_for_buddies_to_figure_out_tensorflows/,AdditionalWay,1563083923,,1,5
52,2019-7-14,2019,7,14,15,cczwsy,Issues with installing Tensorflow,https://www.reddit.com/r/tensorflow/comments/cczwsy/issues_with_installing_tensorflow/,UpmaPesarattu,1563085962,"I am trying to install on Ubuntu 18.0.4 TF using instructions [here](https://www.tensorflow.org/install/gpu) (gpu);  I am installing stable version and not 2.0 beta.

I am able to install all the nvidia drivers (checked the installation), CUDA libraries (verified them), and development libraries i.e., I was able to successfully complete the following instruction on the page
    
    # Install development and runtime libraries (~4GB)
    sudo apt-get install --no-install-recommends \
        cuda-10-0 \
        libcudnn7=7.6.0.64-1+cuda10.0  \
        libcudnn7-dev=7.6.0.64-1+cuda10.0
    
But moving forward, the next step of installing TensorRT is failing i.e., the following command

    # Install TensorRT. Requires that libcudnn7 is installed above.
    sudo apt-get update &amp;&amp; \        
            sudo apt-get install -y --no-install-recommends libnvinfer-dev=5.1.5-1+cuda10.0

The  error message I see is this:

---

The following packages have unmet dependencies:
 libnvinfer-dev : Depends: libnvinfer5 (= 5.1.5-1+cuda10.0) but 5.1.5-1+cuda10.1 is to be installed

---

1. It appears that libnvinfer-dev depends on libnvinfer5. But I cannot locate where is this libnvinfer5 package. 
2. I've actually installed cuda10.0 (I found that pre-build Tensorflow libraries use 10.0 and there are issues with 10.1). So it's confusing why the message says cuda10.1 is to be installed. Can anyone provide some clarity what's going on here.

Next steps.

Though instructions to install TensorRT are provided on Tensorflow website (link [here](https://www.tensorflow.org/install/gpu)), I found that they are also available on developer page of Nvidia (requires login):  https://developer.nvidia.com/nvidia-tensorrt-5x-download.

I've downloaded ubuntu18 CUDA package from the above page (~800 MB download). Having seen MANY MANY posts on issues with version conflicts, I am quite apprehensive to install this package. So I am looking for advice here.

Questions

1. Are instructions provided on Tensorflow [page](https://www.tensorflow.org/install/gpu)  specifc to installing TensorRT inadequate? Or any packages missing from the repo. Can I download them from elsewhere?
2. Can I install TensorRT from Nvidia dev page?",0,1
53,2019-7-14,2019,7,14,17,cd0jlw,Need some HELP with tf.argmax,https://www.reddit.com/r/tensorflow/comments/cd0jlw/need_some_help_with_tfargmax/,slmde,1563091311,"Here is my code:

 

`def build_model():`  
`main_input=Input(shape=(64,),name='main_input')`  
`x=Dense(64,activation='sigmoid',name='hidden_1')(main_input)`  
`second_input=Input(shape=(32,),name='second_input')`  
`x2=Dense(32,activation='softmax',name='hidden_2')(second_input)`  
`x2=tf.argmax(x2)`  
`y=tf.keras.layers.concatenate([x,x2])`  
 `return Model(inputs=[main_input,second_input],outputs=y)`

&amp;#x200B;

And an error happened:

A \`Concatenate\` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: \[(None, 64), (32,)\]

&amp;#x200B;

I'd like to know how can I concatenate a tensor with a label index",1,1
54,2019-7-15,2019,7,15,1,cd4uih,Does anyone know how to convert NVIDIA's StyleGAN to tflite?,https://www.reddit.com/r/tensorflow/comments/cd4uih/does_anyone_know_how_to_convert_nvidias_stylegan/,Ashar7,1563121204,I want to use [NVIDIA's StyleGAN](https://github.com/NVlabs/stylegan) in firebase. They provide a pickle file for the network but I don't have much experience in tensorflow so I couldn't convert their model to tflite. Can anyone help me here?,0,1
55,2019-7-15,2019,7,15,4,cd6z58,Annotated simple word2vec in Tensorflow 2 [Feedback Please!],https://www.reddit.com/r/tensorflow/comments/cd6z58/annotated_simple_word2vec_in_tensorflow_2/,RedPillDevil,1563131978,,0,1
56,2019-7-15,2019,7,15,4,cd73jp,Simple annotated word2vec using Tensorflow 2 [Feedback please!],https://www.reddit.com/r/tensorflow/comments/cd73jp/simple_annotated_word2vec_using_tensorflow_2/,richardanaya,1563132581,[https://github.com/richardanaya/word2vec-tensorflow2/blob/master/word2vec.ipynb](https://github.com/richardanaya/word2vec-tensorflow2/blob/master/word2vec.ipynb),1,1
57,2019-7-15,2019,7,15,5,cd7mq0,Not able to install Object Identification API on Google Collab notebook,https://www.reddit.com/r/tensorflow/comments/cd7mq0/not_able_to_install_object_identification_api_on/,learningeveryday111,1563135227,"Hello!
I followed all the steps on the installation page of the object detection API (and changed the PYTHONPATH) too but the main_builder_test.py file is not running. The error that pops up is that nets module is not found. Has anyone worked with object detection on google collab earlier? Can you help me out? Ill share my code then (its only 6 standard lines though)",6,2
58,2019-7-15,2019,7,15,11,cdbyna,How to test MAML?,https://www.reddit.com/r/tensorflow/comments/cdbyna/how_to_test_maml/,dcopz,1563159002,"I successfully reproduce this repo [https://github.com/dragen1860/MAML-TensorFlow/](https://github.com/dragen1860/MAML-TensorFlow/) about Meta Learning. Now, I'm confusing how test this model with only forward network with weight from n-shot training. Does anyone have idea?

Thanks",0,1
59,2019-7-15,2019,7,15,19,cdfn5d,Unable to import Tensorflow while SSH.,https://www.reddit.com/r/tensorflow/comments/cdfn5d/unable_to_import_tensorflow_while_ssh/,FrStealer,1563185441,"Hello,

&amp;#x200B;

I can import tensorflow (with python3.4, CUDA 10.0) on my main computer.

However when i connect with ssh, i get the error: 

    ImportError: libcusolver.so.10.0: cannot open shared object file: No such file or directory
    

He does not find my CUDA path, but on my main computer i indicate it, with `export LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64/` and that works when i am directly on my main computer but when i connect through ssh, it does not.

&amp;#x200B;

I don't see why, i thought connected to ssh meant i was working on the computer directly so i should not have these problem. 

&amp;#x200B;

Maybe this is not the place to post because it might be a ssh problem but if someone worked with Tensorflow on ssh, i would be glad to have some advice !

&amp;#x200B;

Thank you in advance",6,2
60,2019-7-15,2019,7,15,23,cdi7nu,"I get out of memory error when making prediction with network, but code still seems to work.",https://www.reddit.com/r/tensorflow/comments/cdi7nu/i_get_out_of_memory_error_when_making_prediction/,ml_runway,1563201304,"I am using the TF object detection API, have trained up my network on some new data, and when I run even the smallest image for prediction, get an error like:
    2019-07-15 10:20:01.104556: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.83G (3037544448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
    2019-07-15 10:20:01.855784: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.58GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.

Then things keep going and the image is correctly sorted out. I'm using the faster_rcnn_resnet101_pets model from the google models zoo, and RTX-2070 graphics card with Ubuntu 18. 

I'm not sure how to fix this error, or whether I should be worried about it, or if I need to try with a better graphics card, or??? I'm pretty new to tensorflow and gpu computation in general, so any general pointers to things I might read/absorb/learn would be super helpful. Does it mean the GPU is too small, so it is starting to use the CPU?",7,3
61,2019-7-16,2019,7,16,1,cdjq3i,Unusual audio recognition - how to get started?,https://www.reddit.com/r/tensorflow/comments/cdjq3i/unusual_audio_recognition_how_to_get_started/,ehowardhill,1563208620,"I'm experienced with Python but very new to Machine Learning and more specifically, Tensorflow. I've already run a few tutorials, so I understand some concepts, but not enough to take care of what I'm looking to complete.

Essentially I'm building a phoneme recognizer, but not like most speech-to-text programs. I have recorded a set of audio and separated it into numpy arrays that are 1024 items in size. My intention was to be able to categorize and label these, so that if I spoke into my computer, it could isolate a 1024-item-size numpy array and identify the label of whichever preexisting recorded chunk I have stored on my computer.

It works like so:

1. Template audio (44100hz) is recorded and separated into lists of size 1024.
2. Each one is associated with an individual number, even if multiple lists are nearly identical.
3. Microphone input comes in and is compared to the existing values.
4. The label of the most similar audio clip is returned to the user.

My main issue is that I have no idea where to begin, or even if any of the Tensorflow modules I've come across in tutorials are optimized for a project like this. I'm not asking for anyone to write this for me, I just want an idea of how to get started on my own.

Thank you all so much for checking this out!",3,1
62,2019-7-16,2019,7,16,2,cdk9rn,"Tensorflow + CSharp. I want to build simply library to train, and predict images.",https://www.reddit.com/r/tensorflow/comments/cdk9rn/tensorflow_csharp_i_want_to_build_simply_library/,flumoo,1563210930,,4,2
63,2019-7-16,2019,7,16,13,cdsebg,Best mAP model for object detection on fairly small dataset with class imbalance,https://www.reddit.com/r/tensorflow/comments/cdsebg/best_map_model_for_object_detection_on_fairly/,ski233,1563251861,"I was wondering what model people recommend (trying to maximize mAP where speed is not a concern) for object detection with roughly 20 classes on a small dataset of roughly 12 thousand images with fairly high class imbalance. Ive tried Retinanet50 with mAP of 33 and Retinanet101 with mAP of 36. Models Im considering trying in the future are as follows (Id love to test them all but as these models take a long time to train Im trying to not waste too much training time):
YOLOv3
SNIPER
NASnet
Faster R-CNN
Mask-R-CNN

What models (or strategies) do people recommend for this scenario to try to maximize mAP?",4,2
64,2019-7-16,2019,7,16,21,cdwhc2,"This video goes over a model that predicts the number of views on a youtube video based on likes, dislikes, and subscribers. Really interesting and educative",https://www.reddit.com/r/tensorflow/comments/cdwhc2/this_video_goes_over_a_model_that_predicts_the/,antaloaalonso,1563280642,,0,13
65,2019-7-17,2019,7,17,1,cdzjxt,r/tensorflow,https://www.reddit.com/r/tensorflow/comments/cdzjxt/rtensorflow/,shwetashri,1563295449,"I want to practice transfer learning on some datasets, please help me with some pointers where I can find such example .",4,0
66,2019-7-17,2019,7,17,5,ce2ebb,"Audio training dataset for the four Alexa hot words: Alexa, Echo, Amazon, Computer",https://www.reddit.com/r/tensorflow/comments/ce2ebb/audio_training_dataset_for_the_four_alexa_hot/,jonsmirl,1563308194,"I've built a custom hot word engine and I'd like to train it for the four Alexa keywords. I've search all over and I can't locate a training dataset for these words.  Surely this must exist somewhere, can someone give me a pointer?",0,1
67,2019-7-17,2019,7,17,10,ce6dux,Make a Face Generative Adversarial Network in 15 MINUTES!,https://www.reddit.com/r/tensorflow/comments/ce6dux/make_a_face_generative_adversarial_network_in_15/,alwayslearningnewai,1563328149,,0,8
68,2019-7-17,2019,7,17,13,ce7zs2,Gradient accumulation and batchnorm in tensorflow,https://www.reddit.com/r/tensorflow/comments/ce7zs2/gradient_accumulation_and_batchnorm_in_tensorflow/,dchatterjee172,1563337577,"So, I implemented gradient accumulation, so that I can simulate a bigger batch size. Batch size 64, subdivision 8. But the moving mean and variance are still updated based on the statistics of a very small batch. And I believe that causing my validation IOU to go crazy. 

&amp;#x200B;

[batch size 64 subdivision8](https://i.redd.it/cunu2q6fjsa31.png)

training iou does not flactuate this much.

any ideas?

is there any way I can update the moving mean and variance after combining statistics of multiple batches?",9,2
69,2019-7-17,2019,7,17,22,cecl1a,Convolutional Neural Networks for Beginners,https://www.reddit.com/r/tensorflow/comments/cecl1a/convolutional_neural_networks_for_beginners/,alwayslearningnewai,1563368759,,0,19
70,2019-7-18,2019,7,18,0,ceenlm,TensorFlow Extended (TFX): Machine Learning Pipelines,https://www.reddit.com/r/tensorflow/comments/ceenlm/tensorflow_extended_tfx_machine_learning_pipelines/,ConnieGCarr,1563378668,[https://www.youtube.com/watch?v=wOk2vfjfcDw](https://www.youtube.com/watch?v=wOk2vfjfcDw),0,2
71,2019-7-18,2019,7,18,3,cegy8k,Easily swapping encoders in a network,https://www.reddit.com/r/tensorflow/comments/cegy8k/easily_swapping_encoders_in_a_network/,ptlil,1563389055,"Hi everyone,

The project I'm working on involves a single RL agent trained on multiple environments. I want to have multiple encoders: one for each environment (as the environments have different features), while having a controller network that is the same for all environments. Essentially I want to easily swap out the encoders for training on different environments.

Is there an easy way to do this? I've been thinking about simply feeding the outputs of the encoders to the controller, and feeding zeros into encoders not currently in use, but I'm sure there's a better solution.

Has anyone done something similar?",1,1
72,2019-7-18,2019,7,18,5,cei3ec,Any book to learn machine learning?,https://www.reddit.com/r/tensorflow/comments/cei3ec/any_book_to_learn_machine_learning/,datnell,1563394226,"I'm tempted to buy this one: [https://www.amazon.fr/gp/product/1491962291/ref=ox\_sc\_act\_title\_1?smid=A2VSCSN8IXFNBL&amp;psc=1](https://www.amazon.fr/gp/product/1491962291/ref=ox_sc_act_title_1?smid=A2VSCSN8IXFNBL&amp;psc=1)  


But it is from 2017 and I'm afraid it may be a little bit outdated.  


Any recommandation?",9,3
73,2019-7-18,2019,7,18,8,ceku92,How to deploy a TensorFlow model as a JSON API in 6 lines of code,https://www.reddit.com/r/tensorflow/comments/ceku92/how_to_deploy_a_tensorflow_model_as_a_json_api_in/,ospillinger,1563407384,,0,8
74,2019-7-18,2019,7,18,15,cep0s1,Gogle coral board image classification and transfer learning demos with Captain America and BB-8,https://www.reddit.com/r/tensorflow/comments/cep0s1/gogle_coral_board_image_classification_and/,makereven,1563432810,"&amp;#x200B;

https://i.redd.it/tz8ah3gqe0b31.gif",2,1
75,2019-7-19,2019,7,19,15,cf47me,"Create a ""simple"" image recognition app in JavaScript ?",https://www.reddit.com/r/tensorflow/comments/cf47me/create_a_simple_image_recognition_app_in/,burton6666,1563518900,"I am trying to create a small app that picks out relevant fields on invoices using OCR. I have got the OCR part to work pretty well but then realized that the invoices that I want to read differ a lot in layout so it gets difficult to know where the relevant parts is located. 

&amp;#x200B;

As all invoices uses different templates I was thinking it would be much simpler if I could just detect the different layout templates which probably is &lt; 10 instead of detecting each company specific invoice 100+.

&amp;#x200B;

Is there some simple way to train to detect if I place invoices of same templates in different folders and then train a application to  recognize what template a new invoice uses?",3,2
76,2019-7-19,2019,7,19,19,cf615w,"For me, one of the main barriers to the world of deep learning was setting up all the tools. Here's a video that I hope will eliminate this barrier. Hope you guys found it helpful!",https://www.reddit.com/r/tensorflow/comments/cf615w/for_me_one_of_the_main_barriers_to_the_world_of/,antaloaalonso,1563533217,,0,27
77,2019-7-19,2019,7,19,23,cf851f,How to train an NN using more than one dataset which is not in the same machine and can't be merged?,https://www.reddit.com/r/tensorflow/comments/cf851f/how_to_train_an_nn_using_more_than_one_dataset/,sezaru,1563546058,"Hello,

&amp;#x200B;

I have 2 data-sets (A and B) that I want to use as one train/eval/test data-set to train my neural network with \[1\] . The problem is that these data-sets are from separated institutions and confidential data. So I can access then locally but can't simply copy then and join then to do the training.

&amp;#x200B;

My question is, is it possible, and if yes, what is the best way, to start my train with data-set A and then get the generated model and train with the data-set B?

&amp;#x200B;

Does this even works and/or does it have some constrains or problems compared to directly train with both data-sets merged?

&amp;#x200B;

One way I was thinking on doing it was with checkpoints, I would do one epoch in one data-set, get the checkpoint, and do the another epoch in the other data-set and so on. I'm not so sure if this would work or if it's the best way to do it though.

&amp;#x200B;

I'm totally open to suggestions.

Thanks in advance.

&amp;#x200B;

\[1\] To be more specific I want to use bert pre\_trainning.py to generate a a NLP domain-specific model using both data-sets.",10,5
78,2019-7-20,2019,7,20,0,cf8pdl,using only one gpu in a multi gpu machine without similar vram,https://www.reddit.com/r/tensorflow/comments/cf8pdl/using_only_one_gpu_in_a_multi_gpu_machine_without/,smashedshanky,1563548966,"I have a 2080ti and a 1080 running together, they work fine in my machine since I can run games where the games use all 11gb of the 2080ti VRAM.

&amp;#x200B;

When I try to train models using tensorflow CUDA 10, for some reason tensorflow allocates only the vram\_MIN(gpu:0,gpu:1) even after I add os.environ\['CUDA\_VISIBLE\_DEVICES'\]=""0"" I have also tried gpu\_growth it still only allocates up to 8g no matter what I try. Is there something I am missing?",2,1
79,2019-7-20,2019,7,20,0,cf95i9,"One dataset, three sets of labels. How to organize my input to predict three outputs?",https://www.reddit.com/r/tensorflow/comments/cf95i9/one_dataset_three_sets_of_labels_how_to_organize/,colonel_farts,1563551141,"Ive been beating my head against the wall for a few days over this. Basically I have a column of text in pandas, and three other columns that represent features of this text, indexed by integer. 

Feature1 is 0-15, Feature2 is 0-4, feature3 is 0-4

All I want to do is have my x data be the text, and my y data be [feat1,feat2,feat3], corresponding to the three softmax outputs of my tf.keras model. 

I cant get the data in the proper format!! Ive tried data.Dataset.from_tensor_slices, Ive tried feeding in numpy arrays, Ive tried making each column its own tensor. Nothing. 

Link to my stackoverflow question which has some code snippets: https://stackoverflow.com/questions/57104485/how-to-structure-input-tensor-to-get-three-different-softmax-outputs-at-the-end",3,1
80,2019-7-20,2019,7,20,0,cf99nw,When is the complete tensorflow 2.0 releasing for use?,https://www.reddit.com/r/tensorflow/comments/cf99nw/when_is_the_complete_tensorflow_20_releasing_for/,AbinavR,1563551720,I am a pytorch user but have seen that tensorflow 2.0 has better features for debugging and now that the keras interface and eager execution has become the norm I would like to use it for my thesis.,2,1
81,2019-7-20,2019,7,20,1,cf9env,"AshPy: TensorFlow 2.0 library for distributed training, evaluation, model selection, and fast prototyping.",https://www.reddit.com/r/tensorflow/comments/cf9env/ashpy_tensorflow_20_library_for_distributed/,pgaleone,1563552383,,0,1
82,2019-7-20,2019,7,20,2,cfaphe,"""Q2 next year"" - TF2 release",https://www.reddit.com/r/tensorflow/comments/cfaphe/q2_next_year_tf2_release/,clanleader,1563558484,"So obviously this deadline was missed. Is there any word from Google or any other source as to when TF2 will be released? I'm not really interested in the beta but rather the actual release or an RC since that is the time that proper tutorials and an ecosystem can be built around it once everyone has a more solid idea of what the final version will look like.

So does anyone know when TF2 will officially release?",0,1
83,2019-7-20,2019,7,20,6,cfd7pr,MKT.js,https://www.reddit.com/r/tensorflow/comments/cfd7pr/mktjs/,loaiabdalslam,1563570587,"# (MKT)[https://github.com/loaiabdalslam/MKT]

![MKT.JS](https://github.com/loaiabdalslam/MKT/raw/master/media/mkt.jpg)

![mkt](https://img.shields.io/npm/dw/@mkt-eg/mkt.svg)
![mkt](https://img.shields.io/github/stars/loaiabdalslam/MKT.svg?style=social)
![mkt](https://img.shields.io/github/forks/loaiabdalslam/MTK.svg?style=social)
![mkt](https://img.shields.io/github/last-commit/loaiabdalslam/mkt.svg)

MKT.js is an Exchange Price Service , Stocks , Cryptocurrency,Stock prediction and more \
This package contains hundreds of currencies, cryptocurrencies and stocks prices.\
6,096 coin , 283,037 TRADING PAIRS , 31 News Provider It also works with the TensorFlow  Read more here [Read more about crypto-compare service](https://min-api.cryptocompare.com/faq)
for market forecasting / stock prediction using RNN and also works on the visualizing of stocks data using canvas.js

## Dependencies
- Neural Networks (brain.js)
- Tensorflow Framework (tensorflow.js)
- Data visualization (canvas.js)
- Main Api ( min-api.cryptocompare.com )


###  Get started : 
#### 1 -  Get Full details response (multiaple fsym &amp; tsym)

```
const { MKT } = require('@mkt-eg/mkt')

const mkt = new MKT(
  'bbbc22c3a13c74456a6d4bb7ba5745476ebfdc81c867fc240258122b78eb6a6f'
)
const data = mkt
  .exchange({
    fsym: 'BTC',
    tsyms: 'USD',
    type: 'full'
  })
  .then(response =&gt; {
    console.log(JSON.stringify(response.data))
  })
  .catch(error =&gt; {
    console.log(error)
  })
  
// JSON OUTPUT 
/* 
{
   ""RAW"":{
      ""BTC"":{
         ""USD"":{
            ""TYPE"":""5"",
            ""MARKET"":""CCCAGG"",
            ""FROMSYMBOL"":""BTC"",
            ""TOSYMBOL"":""USD"",
            ""FLAGS"":""2"",
            ""PRICE"":9885.11,
            ""LASTUPDATE"":1563398729,
            ""LASTVOLUME"":0.1,
            ""LASTVOLUMETO"":986.6100000000001,
            ""LASTTRADEID"":""379345663"",
            ""VOLUMEDAY"":93692.97987050914,
            ""VOLUMEDAYTO"":893517565.3549776,
            ""VOLUME24HOUR"":104598.9946433591,
            ""VOLUME24HOURTO"":997000834.8997525,
            ""OPENDAY"":9423.44,
            ""HIGHDAY"":9982.24,
            ""LOWDAY"":9086.51,
            ""OPEN24HOUR"":9649.99,
            ""HIGH24HOUR"":9988.35,
            ""LOW24HOUR"":9076.48,
            ""LASTMARKET"":""Bitfinex"",
            ""VOLUMEHOUR"":2210.51459713301,
            ""VOLUMEHOURTO"":21755061.31969251,
            ""OPENHOUR"":9692.2,
            ""HIGHHOUR"":9943.53,
            ""LOWHOUR"":9663.39,
            ""TOPTIERVOLUME24HOUR"":101424.52271706509,
            ""TOPTIERVOLUME24HOURTO"":966363837.9391046,
            ""CHANGE24HOUR"":235.1200000000008,
            ""CHANGEPCT24HOUR"":2.436479208786753,
            ""CHANGEDAY"":461.6700000000001,
            ""CHANGEPCTDAY"":4.899166334162472,
            ""SUPPLY"":17823212,
            ""MKTCAP"":176184411173.32,
            ""TOTALVOLUME24H"":720083.9899007804,
            ""TOTALVOLUME24HTO"":7081137716.36884,
            ""TOTALTOPTIERVOLUME24H"":425384.18596477184,
            ""TOTALTOPTIERVOLUME24HTO"":4168740744.7056427,
            ""IMAGEURL"":""/media/19633/btc.png""
         }
      }
   },
   ""DISPLAY"":{
      ""BTC"":{
         ""USD"":{
            ""FROMSYMBOL"":"""",
            ""TOSYMBOL"":""$"",
            ""MARKET"":""CryptoCompare Index"",
            ""PRICE"":""$ 9,885.11"",
            ""LASTUPDATE"":""Just now"",
            ""LASTVOLUME"":"" 0.1000"",
            ""LASTVOLUMETO"":""$ 986.61"",
            ""LASTTRADEID"":""379345663"",
            ""VOLUMEDAY"":"" 93,693.0"",
            ""VOLUMEDAYTO"":""$ 893,517,565.4"",
            ""VOLUME24HOUR"":"" 104,599.0"",
            ""VOLUME24HOURTO"":""$ 997,000,834.9"",
            ""OPENDAY"":""$ 9,423.44"",
            ""HIGHDAY"":""$ 9,982.24"",
            ""LOWDAY"":""$ 9,086.51"",
            ""OPEN24HOUR"":""$ 9,649.99"",
            ""HIGH24HOUR"":""$ 9,988.35"",
            ""LOW24HOUR"":""$ 9,076.48"",
            ""LASTMARKET"":""Bitfinex"",
            ""VOLUMEHOUR"":"" 2,210.51"",
            ""VOLUMEHOURTO"":""$ 21,755,061.3"",
            ""OPENHOUR"":""$ 9,692.20"",
            ""HIGHHOUR"":""$ 9,943.53"",
            ""LOWHOUR"":""$ 9,663.39"",
            ""TOPTIERVOLUME24HOUR"":"" 101,424.5"",
            ""TOPTIERVOLUME24HOURTO"":""$ 966,363,837.9"",
            ""CHANGE24HOUR"":""$ 235.12"",
            ""CHANGEPCT24HOUR"":""2.44"",
            ""CHANGEDAY"":""$ 461.67"",
            ""CHANGEPCTDAY"":""4.90"",
            ""SUPPLY"":"" 17,823,212.0"",
            ""MKTCAP"":""$ 176.18 B"",
            ""TOTALVOLUME24H"":"" 720.08 K"",
            ""TOTALVOLUME24HTO"":""$ 7.08 B"",
            ""TOTALTOPTIERVOLUME24H"":"" 425.38 K"",
            ""TOTALTOPTIERVOLUME24HTO"":""$ 4.17 B"",
            ""IMAGEURL"":""/media/19633/btc.png""
         }
      }
   }
}


*/

```

#### 2 -  Get Single price response (Single Ftsym only)

```
const { MKT } = require('@mkt-eg/mkt')

const mkt = new MKT(
  'bbbc22c3a13c74456a6d4bb7ba5745476ebfdc81c867fc240258122b78eb6a6f'
)
const data = mkt
  .exchange({
    fsym: 'BTC', // Single Fysm only 
    tsyms: 'USD,EGP', // Multiaple Tsyms is allowed
    type: 'single'
  })
  .then(response =&gt; {
    console.log(JSON.stringify(response.data))
  })
  .catch(error =&gt; {
    console.log(error)
  })

// JSON OUTPUT 

{
   ""USD"":9888.01,
   ""EGP"":182256.26
}

```

#### 3 -  Get Multiaple price response 

```
const { MKT } = require('@mkt-eg/mkt')

const mkt = new MKT(
  'bbbc22c3a13c74456a6d4bb7ba5745476ebfdc81c867fc240258122b78eb6a6f'
)
const data = mkt
  .exchange({
    fsym: 'BTC,ETH', // Single Fysm only 
    tsyms: 'USD,EGP', // Multiaple Tsyms is allowed
    type: 'multi'
  })
  .then(response =&gt; {
    console.log(JSON.stringify(response.data))
  })
  .catch(error =&gt; {
    console.log(error)
  })

// JSON OUTPUT 

{
   ""BTC"":{
      ""USD"":9906.65,
      ""EGP"":182256.26
   },
   ""ETH"":{
      ""USD"":215.27,
      ""EGP"":3964.07
   }
}

```


#### 3 - Historical Day/hour/minute OHLCV
Get open, high, low, close, volumefrom and volumeto from the daily historical data.The values are based on 00:00 GMT time. It uses BTC conversion if data is not available because the coin is not trading in the specified currency. If you want to get all the available historical data, you can use limit=2000 and keep going back in time using the toTs param. You can then keep requesting batches using: &amp;limit=2000&amp;toTs={the earliest timestamp received}.

 * apiType parms : 'day' or 'hour' or 'minute'
 * you can left some parameter empty its okay 
 * to know more about Request Params please read [Here](https://min-api.cryptocompare.com/documentation?key=Historical&amp;cat=dataHistoday) 

```
const MKT = new module.exports.MKT('bbbc22c3a13c74456a6d4bb7ba5745476ebfdc81c867fc240258122b78eb6a6f')
MKT.historical({
  sympolPrice: 'true',
  e: 'CCCAGG',
  fsym: 'BTC',
  tsyms: 'USD',
  type: 'single',
  aggregate: '1',
  aggregatePredictableTimePeriods: true,
  limit: 100,
  allData: 'false',
  extraParams: 'NotAvailable',
  sign: 'false',
  apiType: 'hour'
}).then((results)=&gt;{
 console.log(results.data)
})

// JSON OUTPUT 
/*
{
   ""Response"":""Success"",
   ""Type"":100,
   ""Aggregated"":false,
   ""Data"":[
      {
         ""time"":1563526800,
         ""close"":10358.27,
         ""high"":10406.85,
         ""low"":10277.92,
         ""open"":10376.84,
         ""volumefrom"":2507.84,
         ""volumeto"":25941945.52
      },
      {
         ""time"":1563530400,
         ""close"":10342.75,
         ""high"":10402.72,
         ""low"":10271.27,
         ""open"":10358.27,
         ""volumefrom"":2464.21,
         ""volumeto"":25476339.6
      },
      {
         ""time"":1563534000,
         ""close"":10297.03,
         ""high"":10412.81,
         ""low"":10287.51,
         ""open"":10342.75,
         ""volumefrom"":2049.12,
         ""volumeto"":21172424.41
      },
      {
         ""time"":1563537600,
         ""close"":10506.18,
         ""high"":10654.99,
         ""low"":10234.52,
         ""open"":10297.03,
         ""volumefrom"":5671.63,
         ""volumeto"":59565785.39
      },
      {
         ""time"":1563541200,
         ""close"":10319.53,
         ""high"":10510.44,
         ""low"":10135.16,
         ""open"":10506.18,
         ""volumefrom"":7043.95,
         ""volumeto"":72409649.25
      },
      {
         ""time"":1563544800,
         ""close"":10341.37,
         ""high"":10425.08,
         ""low"":10284.69,
         ""open"":10319.53,
         ""volumefrom"":1326,
         ""volumeto"":13724171.79
      }
   ],
   ""TimeTo"":1563544800,
   ""TimeFrom"":1563526800,
   ""FirstValueInArray"":true,
   ""ConversionType"":{
      ""type"":""direct"",
      ""conversionSymbol"":""""
   },
   ""RateLimit"":{

   },
   ""HasWarning"":false
}

*/

```




## Some of the ideas I put forward and you can get started:
- Add processing of natural languages to increase confidence in prices that have been predicted 
- Add simulation of the investment process and the development of some strategies of trades.
- Monitor the markets and manufacture a global dashboard.
- add simples and examples using MKT.JSfdaf


## contributions
- For the first contributor you can delete the file and be the first shareholder (I left it to you)
- For the rest, if you think of an idea, you should make pull request and apply it immediately.


Author : Loaii abdalslam",0,3
84,2019-7-20,2019,7,20,10,cfg2fh,Vram questions,https://www.reddit.com/r/tensorflow/comments/cfg2fh/vram_questions/,Stanley_C,1563585961,"I have some questions about Vram when using 2 gpu in tensorflow. 

If  I have gpu a with a\_vram gbs of Vram and gpu b with b\_vram, with a\_vram &gt; b\_vram, how much effective Vram do I get? Is it limited by b\_vram, or is it b\_vram+a\_vram?",0,1
85,2019-7-20,2019,7,20,12,cfh1cu,"Training on 2 GPUS, which method to use",https://www.reddit.com/r/tensorflow/comments/cfh1cu/training_on_2_gpus_which_method_to_use/,Stanley_C,1563592080,"[https://github.com/tmulc18/Distributed-TensorFlow-Guide/blob/master/Multiple-GPUs-Single-Machine/dist\_mult\_gpu\_sing\_mach.py](https://github.com/tmulc18/Distributed-TensorFlow-Guide/blob/master/Multiple-GPUs-Single-Machine/dist_mult_gpu_sing_mach.py)

This first post shows turning multiple gpus into a ""server"" for ubuntu. First, however, would I implement this on windows? Second, how would I use this ""server"" for training on a normal tensorflow training script? Third, how do I use the distributed training method without splitting my training data into 2 batches? Also, I will be using 2060 and a 1060, so I would like to know how I can adapt a script to automatically feed as much data into the gpu without making large changes to the define graph and training part. Basically, I'm asking about how to add another device to the for loop for the training section and have tensorflow automatically split the data and train on each gpu without large refits to the script. Finally, which method is quicker to implement and setup and quicker?

&amp;#x200B;

\- Thank you",0,1
86,2019-7-21,2019,7,21,0,cfmsn3,How to find something in an image,https://www.reddit.com/r/tensorflow/comments/cfmsn3/how_to_find_something_in_an_image/,suguuss,1563635218,"Hi ! 

&amp;#x200B;

I want to build a Face Detection Neural Network, but I'm not sure this is the best for me as I am very inexperienced in the subject. It will be a goal for later.

&amp;#x200B;

Right now after following some videos I managed to build a Cats vs Dogs detector, you just give him an image and it will tell you if there's a cat or a dog in the image. 

&amp;#x200B;

My CNN knows there's a cat or a dog in the image but how can i make it find the dog/cat ? Let's say i want to put a rectangle on him how do I find the animal ?",8,14
87,2019-7-21,2019,7,21,10,cftnwe,PC restarts when under high GPU load using small minibatches vs fine using slightly larger minibatch,https://www.reddit.com/r/tensorflow/comments/cftnwe/pc_restarts_when_under_high_gpu_load_using_small/,smashedshanky,1563673148,"running a 2080ti standalone with 850watt psu gold rated, when training a keras sequential model I notice that after certain amount of epochs my PC will reset and restart over. I do not notice this while using say batch size of 512 compared to using a batch size of 128, seems to happen when I set batch size to 128.

&amp;#x200B;

I also noticed that while the GPU is under heavy load from tf, I hear distinct \*CLICK\*..\*CLICK\* noise from the GPU for each batch it completes at the end I hear a accumulated \*CLICK\* noise. I assume this is literally me hearing the gpu power regulator or the massive amounts of transistors switching on and off (Not the fan, made sure check leaving the fan off for a few seconds and still heard the noise)

&amp;#x200B;

Is it my power supply not being efficient enough at higher power load, I thought gold standard was sufficient enough.

&amp;#x200B;

I even have a hybrid EVGA gpu (2080ti xc hybrid) and temps look good (checking with nvidia-smi -l 1)

&amp;#x200B;

Any thoughts as to why this is happening?",11,2
88,2019-7-22,2019,7,22,1,cg15j1,Constrain weights to sum to 1,https://www.reddit.com/r/tensorflow/comments/cg15j1/constrain_weights_to_sum_to_1/,Algogator,1563728026,"Hello

I have a question similar to [https://stackoverflow.com/questions/49858920/tensorflow-weight-constraint](https://stackoverflow.com/questions/49858920/tensorflow-weight-constraint)

I need a way to constrain the values in a Variable so that during learning they always sum to 1

So I was thinking about doing something like this 

X = X / tf.reduce\_sum( X )",4,2
89,2019-7-22,2019,7,22,15,cg9q9v,Python vs C++ API?,https://www.reddit.com/r/tensorflow/comments/cg9q9v/python_vs_c_api/,imhere4youbby,1563777615,Is one more efficient than the other?,2,5
90,2019-7-22,2019,7,22,23,cgdogt,What does following syntax mean - function()(parameter) ?,https://www.reddit.com/r/tensorflow/comments/cgdogt/what_does_following_syntax_mean_functionparameter/,Shushrut,1563804613," I recently started learning deep learning with tensorflow and came across following line :

`a = Bidirectional(LSTM(n_a, return_sequences=True))(X)`

What does it mean ?

And how it is any different from

`a = Bidirectional(X, LSTM(n_a, return_sequences=True))`",5,1
91,2019-7-23,2019,7,23,0,cgetkq,faster-rcnn I get different results depending on the size of the image I crop (exact same image subset),https://www.reddit.com/r/tensorflow/comments/cgetkq/fasterrcnn_i_get_different_results_depending_on/,ml_runway,1563810162,"Depending on the size of the image, I either get two objects or one object recognized. There is a little blemish in my picture that has a little dog shape that is recognized as dog when I crop the image to 650x650, but when I use 1500x1500, it does not. In each case, the large and crystal-clear dog is recognized as dog. I am not changing the resolution, or downscaling.

I am using object detection api, resnet network. 

Could this be because an anchor is in the middle of the blemish in the 650x650 case but not the 1500x1500 case?",2,2
92,2019-7-23,2019,7,23,7,cgjvxo,TPU training is difficult,https://www.reddit.com/r/tensorflow/comments/cgjvxo/tpu_training_is_difficult/,rish-16,1563833661,"Hey everyone! 

Recently, I've been asked by some of you through DM to write a tutorial on training TensorFlow models on Cloud TPUs on Google Colab. 

TPU training is deemed to be this daunting task that's only meant for wizards. That's not the case!

Here, I distill TPU training into 4 very simple steps that anyone with any ML proficiency can follow and get their models to be faster, and more superior in performance.

Check it out here and if you like it, please do give it some . It means a lot to me so that I can continue writing good content for beginners!

https://link.medium.com/InBcsRYAxY",14,7
93,2019-7-23,2019,7,23,10,cgm6p7,Suggestions on using TensorFlow for action prediction,https://www.reddit.com/r/tensorflow/comments/cgm6p7/suggestions_on_using_tensorflow_for_action/,dingin93,1563845913,"Hey everyone, first time poster here. I am currently developing (more as a learning experience than a serious project) an RTS game and using TensorFlow for it's AI. 

We are using the world state (numerical values that represent current resources, units and buildings) as the input values and the output values as the desired world state. We have the same amount of inputs as outputs. Each output value represents the new value of the input stats (eg. more gold, more peasants, less wood, etc).

Here is the current model we are implementing in tensorflow:

    neuralnet = Sequential()
    neuralnet.add(Dense(output_dim = 13, init = 'uniform', activation = 'relu', input_dim = 13))  
    neuralnet.add(Dense(output_dim = 13, init = 'uniform', activation = 'relu'))  
    neuralnet.add(Dense(output_dim = 13, init = 'uniform', activation = 'sigmoid')) 
    optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0); 
    neuralnet.compile(optimizer = optimizer, loss = 'mean_squared_error', metrics = ['mean_absolute_error', 'mean_squared_error']) 

We are training this network using recorded world states saved as snapshots from real players. We are using these snapshots as inputs, and their next snapshot as ouputs (eg. if the input is the snapshot n, its output would be the snapshot n+1). For testing we are using this code to simulate a match:

    simulation_outputs = []  
    for i in range(1,100):     
        output_pred = neuralnet.predict(initial_state)     
        initial_state = output_pred     
        denorm_output = sc.inverse_transform(output_pred)     
        simulation_outputs.append(denorm_output) 

We expect the output numbers to be (almost) always incremental, but after a few of these outputs are reached, the numbers start stagnating until they come to a full stop. Is there a reason for this? Or is it expected behaviour?

We need to know which should be the best way to model the network to accomplish this task and prevent the match progression to stagnate like this.

Any kind of help or suggestion would be welcomed

Thanks!",1,2
94,2019-7-23,2019,7,23,18,cgqdf1,Not able to use tensorflow_datasets,https://www.reddit.com/r/tensorflow/comments/cgqdf1/not_able_to_use_tensorflow_datasets/,shwetashri,1563874152,"I am not able to import the tensorflow datasets. I installed tensorflow dataset using pip install tensorflow-datasets , but the statement import tensorflow\_datasets as tfdsgives error. 

Please help to resolve I am totally stuck.",6,2
95,2019-7-23,2019,7,23,23,cgtgz5,How do you train a multi-output network on only a single output at a time?,https://www.reddit.com/r/tensorflow/comments/cgtgz5/how_do_you_train_a_multioutput_network_on_only_a/,alteer,1563892956,"I'm going through the Deep Q Network paper from a few years ago:  
 [https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)   


&gt;This part caught my attention. We instead use an architecture in which there is a separate output unit for each possible action, and only the state representation is an input to the neural network. The outputs correspond to the predicted Q-values of the individual actions for the input state.

&amp;#x200B;

The problem is that we can only explore one action at a time. What black magic can you use to train on only one output at a time?  


Faking the other outputs by predicting them with the network and feeding them back in doesn't seem plausible, as it would affect the optimizer.",8,4
96,2019-7-24,2019,7,24,18,ch5sf5,What is up with the total lack of documentation,https://www.reddit.com/r/tensorflow/comments/ch5sf5/what_is_up_with_the_total_lack_of_documentation/,somejob,1563959382,"I'm a junior machine learning engineer and a major part of the job is transfer learning models for custom object detection for deployment to android devices. 

I've been stuck trying to implement a model on device for about 3 weeks now and it's holding up our deployment. The fairly sparse Medium article from 3 tf developers walks through training and converting a model on Google cloud services  (which of course, is a premium feature) using Linux and seems to be the only real official documentation for mobile deployments. Am I wrong in thinking the documentation for tf in general and tflite in particular is shite? To get started I thankfully found a github repo that basically steps through every process in training a model in Windows, but now I'm stuck using both Windows and Linux  (or rather, documenting this cross-OS pipeline for other engineers in the future) without any real documentation for tensorflow. Am I dumb or is this the case?",16,6
97,2019-7-24,2019,7,24,18,ch61pv,Introducing TensorFlow Addons,https://www.reddit.com/r/tensorflow/comments/ch61pv/introducing_tensorflow_addons/,Azzu98,1563961314,,0,26
98,2019-7-24,2019,7,24,18,ch65fv,"Google Coral Edge TPU limited edition, not fake!",https://www.reddit.com/r/tensorflow/comments/ch65fv/google_coral_edge_tpu_limited_edition_not_fake/,makereven,1563962073,"&amp;#x200B;

*Processing img grmf0a2o48c31...*",1,0
99,2019-7-24,2019,7,24,22,ch877g,Should I install tensorflow-gpu and delete regular tensorflow?,https://www.reddit.com/r/tensorflow/comments/ch877g/should_i_install_tensorflowgpu_and_delete_regular/,NanoVash,1563974962,"I built an application that uses tensorflow-gpu to make use of an NVIDIA GPU, however someone that uses my systems reports that occasionally the GPU is used and that occasionally the CPU is used.

Could it be an issue that I have both tensorflow-gpu and tensorflow installed? Should I uninstall regular tensorflow?

I'd really appreciate any explanation regarding to how these packages work.",2,1
100,2019-7-24,2019,7,24,23,ch8pwc,Why is TensorFlow 1.14 slower than TensorFlow 1.13?,https://www.reddit.com/r/tensorflow/comments/ch8pwc/why_is_tensorflow_114_slower_than_tensorflow_113/,yaoyaowd,1563977688,"I recently tried TensorFlow 1.14 because I want to use the mix precision training. But I noticed some of my previous models become 50% slower than before. Then I tried the demo custom estimator model here:

[https://github.com/tensorflow/models/blob/master/samples/core/get\_started/custom\_estimator.py](https://github.com/tensorflow/models/blob/master/samples/core/get_started/custom_estimator.py)

&amp;#x200B;

It looks like TensorFlow 1.13 is at least 10% faster than TensorFlow 1.14. Do you have similar experience? Any hints on which feature or which bug may cause this regression?",5,1
101,2019-7-24,2019,7,24,23,ch93wl,Total Beginner: Learn Tensorflow 1 or go straight to 2 Beta?,https://www.reddit.com/r/tensorflow/comments/ch93wl/total_beginner_learn_tensorflow_1_or_go_straight/,Warhost,1563979607,"Hi,

&amp;#x200B;

I was wondering what you guys think is the best way of getting the hang with Tensorflow for a total ML beginner. I noticed that they are advertising their new v2 Beta everywhere, so what is actually the best way of starting out? There is probably much more material on v1, however v2 is supposed to be simpler overall to pick up.

Cheers",3,1
102,2019-7-25,2019,7,25,2,chav0h,Tensorflow Docs PDF,https://www.reddit.com/r/tensorflow/comments/chav0h/tensorflow_docs_pdf/,ketanIP,1563987793,Can anybody tell me link to download TENSOR FLOW ( Python ) Docs. PDF ???,3,0
103,2019-7-25,2019,7,25,14,chjkap,Can't reuse LSTM cell from after loading a model using saver,https://www.reddit.com/r/tensorflow/comments/chjkap/cant_reuse_lstm_cell_from_after_loading_a_model/,hassanzadeh,1564032995,"I have a simple LSTM model stored by a saver. Now, if I load the graph and create a scope with the same name to reuse LSTM cell, I get the following error:

&amp;#x200B;

ValueError: Variable Recurrent\_Layers/rnn/multi\_rnn\_cell/cell\_0/basic\_lstm\_cell/kernel does not exist, or was not created with tf.get\_variable(). Did you mean to set reuse=tf.AUTO\_REUSE in VarScope?

Here is my code:

&amp;#x200B;



with tf.Graph().as\_default() as graph:

meta\_files = glob.glob(f'./\*meta')

new\_saver = tf.train.import\_meta\_graph(meta\_files\[0\])



with tf.variable\_scope('Recurrent\_Layers', reuse=True):

fw = tf.nn.rnn\_cell.MultiRNNCell(\[BasicLSTMCell(n) for n in \[30,20\]\])

relu1= tf.Variable(tf.constant(\[1,2,3,4,5,6,7,8\], shape=\[2,2,2\], dtype=tf.float32))

lstm\_out\_, \_ = tf.nn.dynamic\_rnn(cell=fw, inputs=relu1, dtype=tf.float32)

&amp;#x200B;

&amp;#x200B;

But I can reuse it in the original model (i.e. the one which is saved), it is just when I store it, load it into another graph and try to reuse it fails. Is that a bug in saver?",1,1
104,2019-7-25,2019,7,25,15,chk2ce,learn tensorflow or stay with keras,https://www.reddit.com/r/tensorflow/comments/chk2ce/learn_tensorflow_or_stay_with_keras/,m4nik1,1564036539,I want to learn more about facial recognition but it's mostly in TensorFlow and I know mostly Keras. Should I learn TensorFlow or are is there code out there that relates to facial recognition.,6,2
105,2019-7-25,2019,7,25,17,chl667,Is it possible to write a customize pooling function(op) in Tensorflow?,https://www.reddit.com/r/tensorflow/comments/chl667/is_it_possible_to_write_a_customize_pooling/,McVeigh1983,1564044953,"Hi all, we've develop a new algorithm so we need a customized pooling function to be implemented.

I know a new op could be written in C++, but unfortunately I don't know how to.

So here I ask, is it possible to add a python written customized pooling function into the graph, and it would update the weight also?

Thank you for your idea in advanced.

Jack",1,1
106,2019-7-25,2019,7,25,18,chlddd,Tensorflow not working with Anaconda Prompt and Jupyter Notebook.,https://www.reddit.com/r/tensorflow/comments/chlddd/tensorflow_not_working_with_anaconda_prompt_and/,chirayushya,1564046482,"I installed tensorflow through anaconda with following command:
 
conda create -n tensorflow_gpuenv tensorflow-gpu

When I import tensorflow in jupyter I get the following error: ModuleNotFoundError: No module named 'tensorflow'",2,2
107,2019-7-25,2019,7,25,20,chmp0g,C api signal processing?,https://www.reddit.com/r/tensorflow/comments/chmp0g/c_api_signal_processing/,nicetryho,1564055732,"Hey all, Ive done some work using a binding over the current c api to run some networks and its been working great. I just have one question, does the c api support the functions in pythons tf.signal ? Reading the documentation I see that only the core tf libraries are included and I assumed that included signal. Can I use those functions through the c api ?",0,1
108,2019-7-25,2019,7,25,23,chomd7,How to utilize a gpu without cuda,https://www.reddit.com/r/tensorflow/comments/chomd7/how_to_utilize_a_gpu_without_cuda/,ski233,1564065926,"I am looking for a way to utilize a computers gpu without using cuda (or any installable software). The reason for this is I have an application where the neural network runs on a users computer and I cant assume they will have the knowledge/ spend the time and effort to install CUDA. I see that it is possible to utilize the gpu using WebGL but from my reading, it sounds like due to other limitations with this approach it is not actually faster than using cpu. Does anyone know how to do this maybe using openGL or some other way to do it?",11,3
109,2019-7-26,2019,7,26,6,chtujt,How to add one value from a Tensor,https://www.reddit.com/r/tensorflow/comments/chtujt/how_to_add_one_value_from_a_tensor/,c2dog430,1564090250,"Say I have a rank 1 tensor a=[1,2,3] and another rank 1 tensor b=[4,5,6] now I want to add just the first elements of the tensors so I am left with a final tensor c = [5,2,3]

What is the best way to do this?",4,1
110,2019-7-26,2019,7,26,12,chxrhf,Tensorflow Tutorial - Modelling with Tensorflow 2.0,https://www.reddit.com/r/tensorflow/comments/chxrhf/tensorflow_tutorial_modelling_with_tensorflow_20/,Amber3825,1564112200, [https://morioh.com/p/5ff6e81919a3](https://morioh.com/p/5ff6e81919a3),0,4
111,2019-7-26,2019,7,26,13,chxzu2,DCGANS Tensor Flow Tutorial Issue,https://www.reddit.com/r/tensorflow/comments/chxzu2/dcgans_tensor_flow_tutorial_issue/,Theotherguy151,1564113671,"I'm having trouble with the DCGANS Tensor Flow Tutorial

 [https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/generative\_examples/dcgan.ipynb](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb) 

I'm using Pycharm and for some reason by the time I get to the run the Code I keep encountering errors

&amp;#x200B;

=============================================================================================

&amp;#x200B;

Here's The Error:

WARNING: Logging before flag parsing goes to stderr.

W0725 22:56:49.102968 14656 deprecation\_wrapper.py:119\] From C:/Users/Dickbutt69/Desktop/DCGANS/DCGANS.py:2: The name tf.enable\_eager\_execution is deprecated. Please use tf.compat.v1.enable\_eager\_execution instead.

&amp;#x200B;

2019-07-25 22:56:51.644654: I tensorflow/core/platform/cpu\_feature\_guard.cc:142\] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2

W0725 22:56:51.868099 14656 deprecation\_wrapper.py:119\] From C:/Users/Dickbutt69/Desktop/DCGANS/DCGANS.py:89: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

&amp;#x200B;

W0725 22:56:51.868099 14656 deprecation\_wrapper.py:119\] From C:/Users/Dickbutt69/Desktop/DCGANS/DCGANS.py:106: The name tf.random\_normal is deprecated. Please use tf.random.normal instead.

&amp;#x200B;

W0725 22:56:52.859696 14656 lazy\_loader.py:50\] 

The TensorFlow contrib module will not be included in TensorFlow 2.0.

For more information, please see:

  \* [https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md](https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md)

  \* [https://github.com/tensorflow/addons](https://github.com/tensorflow/addons)

  \* [https://github.com/tensorflow/io](https://github.com/tensorflow/io) (for I/O related ops)

If you depend on functionality not listed there, please file an issue.

&amp;#x200B;

W0725 22:56:53.554222 14656 deprecation\_wrapper.py:119\] From C:\\Users\\Dickbutt69\\AppData\\Local\\Continuum\\anaconda3\\envs\\DCGANS\\lib\\site-packages\\tensorflow\\python\\autograph\\converters\\[directives.py:117](https://directives.py:117): The name tf.losses.sigmoid\_cross\_entropy is deprecated. Please use tf.compat.v1.losses.sigmoid\_cross\_entropy instead.

&amp;#x200B;

W0725 22:56:53.599102 14656 [deprecation.py:323](https://deprecation.py:323)\] From C:\\Users\\Dickbutt69\\AppData\\Local\\Continuum\\anaconda3\\envs\\DCGANS\\lib\\site-packages\\tensorflow\\python\\ops\\nn\_impl.py:180: add\_dispatch\_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array\_ops) is deprecated and will be removed in a future version.

Instructions for updating:

Use tf.where in 2.0, which has the same broadcast rule as np.where

================================================================================================

Is there a solution to this?

Is anyone else having issues with it?",1,2
112,2019-7-26,2019,7,26,19,ci11h3,Trying to use YOLO9000,https://www.reddit.com/r/tensorflow/comments/ci11h3/trying_to_use_yolo9000/,whatevererer098,1564135955,"Howdy people, I'm trying to use yolo9000 weights. when i try loading them using this function :  

flow --model cfg/yolo-new.cfg --load bin/yolo-new.weights --demo videofile.avi 

I get AssertionError: Over-read bin/yolo9000.weights

I read online that the problem lies in a different between the .weight file and the .cfg file. Some sort of mismatch. 

Do you know how to fix this? or Better yet does any of you have the working yolo9000 weights and cfg files? if so can you please somehow share them via google drive or something?",1,8
113,2019-7-27,2019,7,27,0,ci4bo0,How to use the Omniglot TensorFlow Dataset,https://www.reddit.com/r/tensorflow/comments/ci4bo0/how_to_use_the_omniglot_tensorflow_dataset/,mathhelpermann,1564155083,"I am new to the TensorFlow Datasets and I am trying to do some one-shot learning with the Omniglot dataset. I have done this in PyTorch, but my collaborators use TensorFlow so here I am.

The Omniglot dataset has structure: 

`Class-&gt;subclass-&gt;examples`

I am trying to create a DataLoader that will generate pairs of examples and label 0 if they are from the same subclass and 1 if they are from different subclasses. 

&amp;#x200B;

The the `get_batch(...)` function here:  [https://github.com/hlamba28/One-Shot-Learning-with-Siamese-Networks/blob/master/Siamese%20on%20Omniglot%20Dataset.ipynb](https://github.com/hlamba28/One-Shot-Learning-with-Siamese-Networks/blob/master/Siamese%20on%20Omniglot%20Dataset.ipynb) pretty much implements exactly what I am trying to do but does so without using the TensorFlow datasets.

&amp;#x200B;

I have figured out how the TensorFlow structures the Omniglot dataset 

`omni_train = tfds.load(name=""omniglot"", split=tfds.Split.TRAIN)`

outputs:

 `&lt;_OptionsDataset shapes: {image: (105, 105, 3), alphabet: (), alphabet_char_id: (), label: ()}, types: {image: tf.uint8, alphabet: tf.int64, alphabet_char_id: tf.int64, label: tf.int64}&gt;`

So ideally I would sample random alphabets and if label matches for both I label the pair 0 else I label it 1. But I can't seem to figure out how use the TensorFlow dataset in this way. Does anyone have any tips or even know if it's possible?",0,1
114,2019-7-29,2019,7,29,23,cjbqcv,"My Model Wont Learn, Please Help",https://www.reddit.com/r/tensorflow/comments/cjbqcv/my_model_wont_learn_please_help/,SteamSaajj,1564409339,"Thanks for clicking on this post, as the title is saying my neural network is failing to learn the task i am giving it, and i'm not sure why could you help me? 

Heres some details.

Im trying to get the network to output a number between 0 and 1 (0 corresponding to left, 0.25 to right, 0.5 to up and 1 to down), to achieve this i am giving it some other data (its current position 0 to 1, if the grid above, left, right and below are walls or paths, 0 being walls, 1 paths, and where the goal position is. In conclusion all values are between 0 and 1.

For context i want to  be able to use this to make an ai that can navigate random mazes/obstacle courses.

An example of train\_X = (0.5675, 1, 0, 1, 0 0.95), train\_y = 0.5

&amp;#x200B;

Heres my code, sorry if its not neat. 

&amp;#x200B;

import numpy as np

import pandas as pd

import tensorflow as tf

from tensorflow.keras.models import Sequential

from tensorflow.keras.layers import Dense

model = Sequential()

&amp;#x200B;

\#add model layers

model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu, input\_shape=(6,)))

model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))

model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))

model.add(Dense(1, activation=tf.nn.sigmoid))

&amp;#x200B;

\#compile model using mse as a measure of model performance

model.compile(optimizer=""adam"", loss='mean\_squared\_error', metrics=\[""mean\_absolute\_error""\])

&amp;#x200B;

data = pd.read\_csv(""C:\\\\Users\\\\James\\\\PycharmProjects\\\\Tensor\\\\Maze\\\\all\_moves.csv"")

data.iloc\[np.random.permutation(len(data))\]

dropped\_cols = \[0, 1, 2, 3\]

&amp;#x200B;

train\_x = data.drop(data.columns\[dropped\_cols\], axis=1)

train\_y = data.iloc\[:, 3\]

&amp;#x200B;

train\_x = train\_x.to\_numpy()

train\_y = train\_y.to\_numpy()

&amp;#x200B;

\#print(""Move mean: "", train\_y.mean())

\#print(""Move mode: "", train\_y.mode())

\#print(""Move median: "", train\_y.median())

&amp;#x200B;

\#train model

\#[model.fit](https://model.fit)(train\_x, train\_y, validation\_split=0.2, epochs=30, callbacks=\[early\_stopping\_monitor\], verbose=2)

[model.fit](https://model.fit)(train\_x, train\_y, validation\_split=0.1, epochs=90, verbose=2, batch\_size=24)

&amp;#x200B;

[model.save](https://model.save)(""model\_8.h5"")

&amp;#x200B;

The reason i drop columns 0, 1, 2 and 3 is becuase of the way i get my data. Just not 0, 1 and 2 are row numbers and 3 is what train y is.

I have a fealing its todo with activation types but im not sure, thanks in advance.

&amp;#x200B;

Some Logs:

Train on 1032750 samples, validate on 114750 samples

Epoch 1/50

1032750/1032750 - 83s - loss: 0.0680 - mean\_absolute\_error: 0.2267 - val\_loss: 0.0679 - val\_mean\_absolute\_error: 0.2266

Epoch 2/50

1032750/1032750 - 82s - loss: 0.0679 - mean\_absolute\_error: 0.2264 - val\_loss: 0.0680 - val\_mean\_absolute\_error: 0.2277

Epoch 3/50

1032750/1032750 - 80s - loss: 0.0679 - mean\_absolute\_error: 0.2264 - val\_loss: 0.0679 - val\_mean\_absolute\_error: 0.2265

Epoch 4/50

1032750/1032750 - 80s - loss: 0.0679 - mean\_absolute\_error: 0.2264 - val\_loss: 0.0679 - val\_mean\_absolute\_error: 0.2256

Epoch 5/50

1032750/1032750 - 80s - loss: 0.0679 - mean\_absolute\_error: 0.2263 - val\_loss: 0.0679 - val\_mean\_absolute\_error: 0.2265

Epoch 6/50

1032750/1032750 - 79s - loss: 0.0679 - mean\_absolute\_error: 0.2263 - val\_loss: 0.0679 - val\_mean\_absolute\_error: 0.2268

Epoch 7/50

1032750/1032750 - 80s - loss: 0.0679 - mean\_absolute\_error: 0.2263 - val\_loss: 0.0679 - val\_mean\_absolute\_error: 0.2262

Epoch 8/50

1032750/1032750 - 80s - loss: 0.0679 - mean\_absolute\_error: 0.2263 - val\_loss: 0.0679 - val\_mean\_absolute\_error: 0.2264

Epoch 9/50

1032750/1032750 - 85s - loss: 0.0679 - mean\_absolute\_error: 0.2263 - val\_loss: 0.0679 - val\_mean\_absolute\_error: 0.2264

Epoch 10/50",8,1
115,2019-7-29,2019,7,29,23,cjbt2y,Practical Introduction to Neural Networks - Image prediction,https://www.reddit.com/r/tensorflow/comments/cjbt2y/practical_introduction_to_neural_networks_image/,rishi_devan,1564409710,,0,3
116,2019-7-30,2019,7,30,1,cjdu9o,Multiclassification-Keras,https://www.reddit.com/r/tensorflow/comments/cjdu9o/multiclassificationkeras/,nicetryho,1564418859,"How do I process the input data so that each feature in the matrix is a class ? I have 3 classes, have loaded the data from csvs into 3 column vectors that each correspond to each class of data. I have no idea how to feed my model these 3 vectors and train it on them so it knows each vector is another class, help !!",6,1
117,2019-7-30,2019,7,30,6,cjhvyu,"""Learning TensorFlow"" for $1 on the Machine Learning and Data Analysis ebook bundle",https://www.reddit.com/r/tensorflow/comments/cjhvyu/learning_tensorflow_for_1_on_the_machine_learning/,zackentertainment,1564436392,,0,1
118,2019-7-30,2019,7,30,8,cjiz5h,Tensor Variables wont save in spyder,https://www.reddit.com/r/tensorflow/comments/cjiz5h/tensor_variables_wont_save_in_spyder/,MauriceTheMonster,1564441455,"Hi, I have only started looking at tensorflow today and I began a tutorial. I used the first code in the tutorial to check if it was working in spyder. My problem is that the print function wont print x1 or x2 or results and the constants are not saved in the variable finder. Does anyone know how to fix this??

Here's the code:

&amp;#x200B;

import tensorflow as tf

\# Initialise constants

x1 = tf.constant(\[1, 2, 3, 4\])

x2 = tf.constant(\[5, 6, 7, 8\])

&amp;#x200B;

result = tf.multiply(x1,x2)

print(result)",4,0
119,2019-7-30,2019,7,30,9,cjjnzv,Prune Pretrained models without checkpoint,https://www.reddit.com/r/tensorflow/comments/cjjnzv/prune_pretrained_models_without_checkpoint/,Stanley_C,1564444861,How would I prune a model without a checkpoint?,1,3
120,2019-7-30,2019,7,30,9,cjk8zv,Is there any working guide/tutorial to run quantization-aware training in TF2?,https://www.reddit.com/r/tensorflow/comments/cjk8zv/is_there_any_working_guidetutorial_to_run/,jingw222,1564447883,,2,2
121,2019-7-30,2019,7,30,16,cjok1x,Converting .pb to .tflite in Windows,https://www.reddit.com/r/tensorflow/comments/cjok1x/converting_pb_to_tflite_in_windows/,somejob,1564473411,"I've successfully transfer learned and deployed a model to Android using a weird Windows -&gt; Linux -&gt; Windows pipeline, but have been tasked with documenting the process and converting the pipeline to run just on Windows. Obviously Windows has no ""make"" command as in the semi-official Medium post on the subject so I'm exploring using Cygwin to perform this, has anybody done something similar?",2,1
122,2019-7-30,2019,7,30,17,cjolrc,"Dynamic/static outputs are not same, why?",https://www.reddit.com/r/tensorflow/comments/cjolrc/dynamicstatic_outputs_are_not_same_why/,gecicihesap17,1564473759,"Link for better viewing: [https://stats.stackexchange.com/questions/419778/dynamic-static-outputs-are-not-same-why](https://stats.stackexchange.com/questions/419778/dynamic-static-outputs-are-not-same-why)

I am trying to implement a  patch creation function with using tensorflow's extract\_image\_patches function but dynamic output shape is not same as my expectation. 

Let me tell briefly what it does. Input shape is supposed to be 

6000x4000. We first find its greatest common denominator. It turns out it is 3. then we pass '64' argument to our function to create patches with size of 3x64,2x64=192,128. This returns us 31x31 distinct patches. Everything works ok with static output, but when it comes to dynamic output things are not ok. I could not find which part caused a different dynamic output. 









\#       input\_shape\_inbuild:  (None, 6000, 4000, 1)

\#         ---LAYER---

\#         Input Size: (None, 6000, 4000, 1)

\#         Patch Size: (x,y) = 192, 128

\#         Aspect ratio: (3, 2)

&amp;#x200B;



!wget [https://www.fujifilm.com/products/digital\_cameras/x/fujifilm\_x\_pro2/sample\_images/img/index/ff\_x\_pro2\_001.JPG](https://www.fujifilm.com/products/digital_cameras/x/fujifilm_x_pro2/sample_images/img/index/ff_x_pro2_001.JPG)

img = cv2.imread('ff\_x\_pro2\_001.JPG', 0)  

img = tf.reshape(img, \[1,img.shape\[0\],img.shape\[1\],1\])   

   







\*\*\*tensorflow takes images as (y, x)

so a 6000x4000 im is given as tf.func(4000, 6000)





\# Here I define custom layer in tensorflow.

class create\_patches(Layer):



def \_\_init\_\_(self, patchMultiplier):

super(create\_patches, self).\_\_init\_\_()

self.patchMultiplier = patchMultiplier





def build(self, input\_shape):



print('input\_shape\_inbuild: ', input\_shape)

def aspect\_ratio(width, height):

\#find greatest common divider of input\_shape

def gcd(x, y):

while y != 0:

(x, y) = (y, x % y)

return x



r = gcd(width, height)

x = int(width/r)

y = int(height/r)

return x, y



self.aspect\_ratio = aspect\_ratio(input\_shape\[1\], input\_shape\[2\])

self.patchSize\_x = self.aspect\_ratio\[0\] \* self.patchMultiplier

self.patchSize\_y = self.aspect\_ratio\[1\] \* self.patchMultiplier





def call(self, inputs):



print('---LAYER---')



print('Input Size:', inputs.\_keras\_shape)



print('Patch Size: (x,y) = {}, {}'.format(self.patchSize\_x, self.patchSize\_y))



print('Aspect ratio: {}'.format(self.aspect\_ratio))



print('---LAYER---')

\#call tf.extract\_image\_patches to return it.

out = tf.extract\_image\_patches(images=inputs, 

ksizes=\[1, self.patchSize\_y, self.patchSize\_x, 1\], 

strides=\[1, self.patchSize\_y, self.patchSize\_x, 1\], 

rates=\[1, 1, 1, 1\], 

padding='VALID')





return out



def compute\_output\_shape(self, input\_shape):



""""""

ksize\_cols = patchSize\_x

ksize\_rows = patchSize\_y

""""""



\#output shape=\[batch, out\_rows, out\_cols, ksize\_rows \* ksize\_cols \* depth\]



""""""

shape = (self.patchSize\_x, self.patchSize\_y, 

(input\_shape\[1\]/self.patchSize\_x) \* (input\_shape\[2\]/self.patchSize\_y))

""""""



shape =(input\_shape\[0\],

input\_shape\[1\]/self.patchSize\_x, # patch row count

input\_shape\[2\]/self.patchSize\_y, # patch col count

self.patchSize\_x \* self.patchSize\_y) # patch pixel count





return shape









\#here is input with 6000x4000 pixels.

input\_shape\_1 = Input(shape=(6000, 4000, 1))

\#here I fed input to my custom layer.

x1 = create\_patches(64)(input\_shape\_1)

print('Output shape: ', x1.shape)



\# here I build a model to see static output

f = K.function(\[input\_shape\_1\], \[x1\])







import numpy as np

\#result = f(\[np.random.randint(256, size=(1,4000,6000,1))\])

result = f(\[img\])





\#print(result)

result = np.array(result)



\# \[batch, out\_rows, out\_cols, ksize\_rows \* ksize\_cols \* depth\]

\# Result shape:  (1, 1, 125, 125, 1536) 

print('Result shape: ', result.shape, '\\n\\n')



\#print(result\[:, :, :, 0\].shape)



\#here is output I get. 





input\_shape\_inbuild:  (None, 6000, 4000, 1)

\---LAYER---

Input Size: (None, 6000, 4000, 1)

Patch Size: (x,y) = 192, 128

Aspect ratio: (3, 2)

\---LAYER---

Output shape:  (?, 46, 20, 24576)

Result shape:  (1, 1, 31, 31, 24576) 

\#####Result Shape is as I expected but at output shape I could not resolve where 46 and 20 come from. Could you tell me why it is like this?",0,0
123,2019-7-31,2019,7,31,4,cjwfxk,How to quantize resnet101?,https://www.reddit.com/r/tensorflow/comments/cjwfxk/how_to_quantize_resnet101/,bwsyn,1564513620,"I'm trying to get an int8 version of resnet101.tflite but I have the following questions:

1. There is only a fp32 version of resnet101 at [https://www.tensorflow.org/lite/guide/hosted\_models](https://www.tensorflow.org/lite/guide/hosted_models). Is there a way to convert this tflite model to an int8 tflite model?
2. I know you can convert a 'saved model' to a tflite model but I can't find a 'saved model' at [https://github.com/tensorflow/models/tree/master/official/resnet](https://github.com/tensorflow/models/tree/master/official/resnet). Is there a pretrained ''saved model' of resnet101?
3. TensorFlow also has model checkpoints. If I have a resnet101.ckpt, how do I convert it to a .tflite model?

Please help me. TensorFlow is so complicated that I feel I'm getting brain damage.",4,6
124,2019-7-31,2019,7,31,7,cjze8v,Confused by the (official) documentation.. am I missing something?,https://www.reddit.com/r/tensorflow/comments/cjze8v/confused_by_the_official_documentation_am_i/,mexiKobe,1564526536,"I'm learning Tensorflow right now and I recently was looking at [the documentation for `tf.Session()` specifically the `run()` method](https://www.tensorflow.org/api_docs/python/tf/Session#run), and I was hoping someone could help me better navigate the official documentation because I was very confused. 

I ultimately called `sess.run([optimizer, cost], feed_dict = {x:batch_x, y:batch_y})` with optimizer and cost being the appropriate tf objects, and it seems impossible to figure out these were valid parameters by scanning the documentation, which I linked to above. Where (or how) is this use case listed? It lists the method `run(fetches, feed_dict=None, options=None, run_metadata=None)` but it's not clear to me where [tf.train.AdamOptimizer, tf.nn.softmax_cross_entropy_with_logits] fits in here based on the descriptions of any of these parameters...",9,1
125,2019-7-31,2019,7,31,13,ck3mui,How to convert PSP(Pyramid Scene Parsing Network) net to TensorFlow lite.,https://www.reddit.com/r/tensorflow/comments/ck3mui/how_to_convert_psppyramid_scene_parsing_network/,srinu1746,1564548606,I was unable to convert psp net to Tensorflow Lite.Can anyone help me out,0,1
126,2019-7-31,2019,7,31,18,ck5wqf,"Export trained model to production (Target platform is PC, offline).",https://www.reddit.com/r/tensorflow/comments/ck5wqf/export_trained_model_to_production_target/,Michael496445,1564563889,"This topic may have been asked so many times, but after hours of seaching i just got lot of fragments and still have no clue how to implement.

My project is audio related and has complicated AutoEncoder structure. 

The most hardcore way is that i implement all the forward layers in C/C++, export the trained model in the form of numpy ziped array, and finally eject those weights to self-made C/C++ layers. 

Another way is building required environment on target PC, so that scripts can be run with no problem. However, this means the app must be written in python so that everything stays in python.

One more option is tensorflow serving, but instead of providing web service, i need the server run and serve locally to address running offline problem. So far i believe this option is the most appropriate. 

Have anyone of you successfully export complicated and trained model to other plateform like PC, phone? Which is the best way to my scenario?

Thanks a lot",0,1
127,2019-7-31,2019,7,31,21,ck84m8,Multi labal classification in tensorflow lite,https://www.reddit.com/r/tensorflow/comments/ck84m8/multi_labal_classification_in_tensorflow_lite/,ConcealedImages,1564577651,"Hello, 

I really want to deploy a model which classifies images on 2 or more labels. But, I am not sure how to do this with tensorflow lite. Can someone point me to some resources?  An example of multi label classification could be  if I want to classify whether a given image is a vegetable or fruit, and also what type of vegetable or fruit. Is it possible on google ML kit and Firebase?",0,1
128,2019-7-31,2019,7,31,22,ck8skq,What type of model is used to create deep fakes?,https://www.reddit.com/r/tensorflow/comments/ck8skq/what_type_of_model_is_used_to_create_deep_fakes/,WilkinsMicawbers,1564581194,I'm doing a general study of machine learning and am coming up short finding information on how deepfakes are created. Is there any publicly available information on what models were used to make deepfakes?,4,6
