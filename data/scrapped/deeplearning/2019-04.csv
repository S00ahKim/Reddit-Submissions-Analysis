,sbr,sbr_id,date,year,month,hour,day,id,domain,title,full_link,author,created,selftext,num_comments,score,over_18,thumbnail,media_provider,media_thumbnail,media_title,media_description,media_url
0,deeplearning,t5_2t5eh,2019-4-1,2019,4,1,13,b7xkjb,i.redd.it,Does anyone know what this in Google Colab ?,https://www.reddit.com/r/deeplearning/comments/b7xkjb/does_anyone_know_what_this_in_google_colab/,dev853,1554094542,,10,5,False,image,,,,,
1,deeplearning,t5_2t5eh,2019-4-1,2019,4,1,14,b7xn31,self.deeplearning,Deep learning benchmark tool | Benchmark GPU's to know which one is the best for your deep learning workflow,https://www.reddit.com/r/deeplearning/comments/b7xn31/deep_learning_benchmark_tool_benchmark_gpus_to/,gimel1213,1554094977,"**So you are building your new Deep Learning workstation to perform some state-of-the-art computations and run really deep and sophisticated models, but you are indecisive as to which GPU to go for, or you already have a set of GPUs that you are planning to use, but need to know just how efficient are these when compared to whats out there. In this blog post, I plan to present to you an app that will solve both of these problems to you, with no cost associated.**

**Deep Learning is a field that requires some serious computational power, and by using a CPU, you might spend weeks training your model, while a strong GPU would finish the job during the day. This is mainly because of the difference between these two pieces of hardware regarding the design, as we shall see in a minute when we discuss the different types of HW used for Deep Learning, but for now its just good to bear in mind that more efficient hardware will mean not only faster training experiences, but also more room for model tuning and algorithms testing, that will make your life as a Deep Learning developer a lot easier.**

&amp;#x200B;

&amp;#x200B;

[DLBT app | Deep learning Benchmark tool ](https://i.redd.it/ltnpfpqi3lp21.png)

&amp;#x200B;

## Types of Hardware

**If we are going to discuss what are the best pieces of hardware to perform deep learning tasks, we should first take a look at the different types, the following diagram shows the classification breaking it down to four classes.**

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/blcdc78z3lp21.png

**As we can see in the previous diagram, general purpose hardware category splits into Central Processing Units (CPU) and Graphic Processing Units (GPU). The former is specifically designed to be latency oriented, this means it should be able to do complicated big tasks, one after the other, just like a big elephant. As for the GPU, this one is throughput oriented, which implies it specializes in performing many many small dumb tasks simultaneously, resembling a group of small ants.**

**Field Programmable Gate Arrays (FPGA) is a special piece of hardware that allows for programmable logic, this means that the developer can design the hardware structure of the device several times to implement a particular application. This might really come in handy if you want to try out new ideas and prototypes, and its performance increases relative to the general purpose hardware as long as the design is efficient enough.**

**Application Specific Integrated Circuits (ASIC) are much rarer to come by, it implies someone took the job of carefully designing the hardware that solves the problem at hand, and printed the circuit, so this hardware would only make sense when used for that application. Googles Tensor Processing Units (TPU) are a state of the art ASIC circuit. Although ASICs turn out to be faster than FPGAs, they are harder to obtain and assemble into our deep learning workstations.**

**The Deep Learning Bench Tools Application focuses on the General Purpose hardware, as it is by far the most repeatedly used.**

&amp;#x200B;

## DLBT Application

**Suppose you just bought your Graphics Card(s) and plugged it into your motherboard, expecting to run some next level algorithms very fast. It would be very useful if you had a tool that told you how fast is the combination of your CPU with the Graphic Processing Units at your disposal, and that on top of it let you compare the results to other deep learning workstations around the globe to see if youre happy where you stand. Well, look no more, DLBT is the answer.**

**This hardware bench tool automatically recognizes the Machine Learning capable hardware in your computer, this might be just the CPU, in case you have no GPU, or you havent installed the required drivers (if this is the case, we walk you through how to do this, line by line), or it may be multiple GPUs, in which case you have the choice of where to run the benchmark models.**

### Model Used

**In its current version the DLBT app is running a Convolutional Neural Net, with a standard structure in the background, while taking note of how long an episode lasts, as well as splitting this time into the prediction time and the back-propagation time for more advanced users.**

**The structure of the model used, might be seen in the following image.**

&amp;#x200B;

&amp;#x200B;

[Convolutional Neural Net used in the Test Bench](https://i.redd.it/6bxfc4e64lp21.png)

&amp;#x200B;

**As a future update we are currently working on extending this feature into multiple known benchmarks having to do with Recurrent Neural Networks, Natural Language Processing, etc.**

### Obtaining the rating

**How to measure exactly how efficient is the device running? We use the formula displayed below. Intuitively, it would be better for the ratings to increase as the hardware efficiency rises. The K scaling factor serves the purpose of spreading the results more to allow for better comparison.**

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/kzmk3wsd4lp21.png

### Results

**This application has been run on many GPUs to measure their performance running the model explained previously, the following table depicts some of the results thrown by the app. In** [**here**](https://www.technopremium.com/results) **you will find many more results of other pieces of hardware.**

&amp;#x200B;

## Conclusions

**There you have it, you just discovered an easy way to measure your hardware performance, without writing a single line of code.** [**DLBT**](https://drive.google.com/file/d/1gDMs3pLG-UzcEZ0AunnWzWHMwGdfqKxm/view?usp=sharing) **is a GUI application that automatically detects your GPUs, lets you monitor them and run deep learning benchmarks to compare their performance to the standards.** ",3,0,False,self,,,,,
2,deeplearning,t5_2t5eh,2019-4-1,2019,4,1,14,b7y14c,self.deeplearning,How to prepare your Ubuntu 18.04 ready for deep learning | Full stack installation,https://www.reddit.com/r/deeplearning/comments/b7y14c/how_to_prepare_your_ubuntu_1804_ready_for_deep/,gimel1213,1554097383,"    I remember the first time I was setting the computer to start developing for deep learning, it was a hassle because at that time the information online was not enough and there was a lot of problems when installing the frameworks and drivers to make the GPU's ( Nvidia ) work properly with the stack. 

&amp;#x200B;

That's why I decided to create a guide so everyone can do the full stack installation and get their machine ready for development, this part will install Nvidia driver 415.27, CUDA 10 for the full stack check our blog [here](https://technopremium.com/blog/) the process is pretty straight forward, just copy and paste the commands from here to your terminal and you should be ready to go. 

&amp;#x200B;

&gt;!This install can be used by anyone using Ubuntu 18.04, gamers can benefits from it too, so you are WELCOME :))))!&lt;

&amp;#x200B;

What we are going to install: 

&amp;#x200B;

Drivers: 

1- Nvidia drivers 

2- CUDA 10 

3- Cuddn 7.4 

4- Nccl - For multi GPU's training 

&amp;#x200B;

Frameworks: 

1- Pytorch 

2- Tensorflow 

3- Theano 

4- Caffe2 

5- Julia 

&amp;#x200B;

&gt;!Ok let's get started!&lt; 

&amp;#x200B;

**EACH LINE MUST BE ""COPIED AND PASTED"" ONE LINE AT A TIME**

    sudo apt-get update
    sudo apt-get upgrade

&amp;#x200B;

    lspci | grep -i nvidia
    uname -m &amp;&amp; cat /etc/*release
    sudo apt-get install linux-headers-$(uname -r)
    sudo apt autoremove

&amp;#x200B;

    sudo apt-get install build-essential -y
    sudo apt-get install cmake git unzip zip -y

&amp;#x200B;

    sudo add-apt-repository ppa:graphics-drivers/ppa -y
    sudo apt install nvidia-driver-415 -y

&amp;#x200B;

    sudo wget
    https://developer.nvidia.com/compute/cuda/10.0/Prod/local_installers/cuda-repo-ubuntu1
    804-10-0-local-10.0.130-410.48_1.0-1_amd64
    sudo dpkg -i cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64
    sudo apt-key add /var/cuda-repo-10-0-local-10.0.130-410.48/7fa2af80.pub
    sudo apt-get update
    sudo apt-get install cuda-toolkit-10-0

&amp;#x200B;

    sudo reboot
    nvidia-smi

&amp;#x200B;

&gt;At this point Nvidia drivers and Cuda should be installed 

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;",13,17,False,self,,,,,
3,deeplearning,t5_2t5eh,2019-4-1,2019,4,1,15,b7y9bd,self.deeplearning,RTX 2060 and 2070 Deep learning benchmarks 2019 | Tensorflow Pytorch,https://www.reddit.com/r/deeplearning/comments/b7y9bd/rtx_2060_and_2070_deep_learning_benchmarks_2019/,gimel1213,1554098861,"&amp;#x200B;

With the release of the RTX 2060 and 2070, it came the idea to measure this cards in order to see the difference between them for deep learning, since the RTX 2060 is $349 it makes sense to see the performance on Tensorflow and Pytorch

&amp;#x200B;

 We use the Yasuko benchmark that can be found here: [https://github.com/u39kun/deep-learni](https://github.com/u39kun/deep-learni)...  The results are based on running the models with images of size 224 x 224 x 3 with a batch size of 16. ""Eval"" shows the duration for a single forward pass averaged over 20 passes. ""Train"" shows the duration for a pair of forwarding and backward passes averaged over 20 runs. In both scenarios, 20 runs of warm-up are performed and those are not counted towards the measured numbers.

&amp;#x200B;

&amp;#x200B;

![video](sypfbv8oflp21 ""DL Benchmark test | RTX 2060 VS 2070"")

&amp;#x200B;

Let me know in the comments what do you think about it, I think the problem with the RTX 2060 is the amount of ram which is 6 GB, but since it has tensor cores inside, it should give it a boost using the new CUDA X and Tensorflow 2.0, hence I have not tested yet. ",3,1,False,self,,,,,
4,deeplearning,t5_2t5eh,2019-4-1,2019,4,1,16,b7z3sz,i.redd.it,8x RTX TITAN workstation | I cannot wait to run some models on this beast !!!!,https://www.reddit.com/r/deeplearning/comments/b7z3sz/8x_rtx_titan_workstation_i_cannot_wait_to_run/,gimel1213,1554104673,,37,117,False,image,,,,,
5,deeplearning,t5_2t5eh,2019-4-1,2019,4,1,17,b7zahd,self.deeplearning,Problem when Loading data in Pytorch using Spyder,https://www.reddit.com/r/deeplearning/comments/b7zahd/problem_when_loading_data_in_pytorch_using_spyder/,smukh98,1554106002,"Hey all,
I was working with a dataset of images (3*150*150)  in Pytorch in Spyder ide .wrote a simple classifier.Tge problem I am facing is that when I am loading the data using Dataset class ,the Spyder ide hogs all the ram and memory usage goes upto 99% ,and then it just hangs .I waited for half an hour to let it load and still nothing happens. The dataset size is about 25000. Can anyone help me with this problem?
",4,1,False,self,,,,,
6,deeplearning,t5_2t5eh,2019-4-1,2019,4,1,17,b7zmgx,go.codetrick.net,Deep Learning on Mobile Devices,https://www.reddit.com/r/deeplearning/comments/b7zmgx/deep_learning_on_mobile_devices/,RobertCotterman,1554108540,,0,1,False,default,,,,,
7,deeplearning,t5_2t5eh,2019-4-1,2019,4,1,18,b800lb,self.deeplearning,Does anyone have links to a tutorial that uses pytorch for classification/regression on a tabular dataset? Many thanks! :),https://www.reddit.com/r/deeplearning/comments/b800lb/does_anyone_have_links_to_a_tutorial_that_uses/,ai_badger,1554111392,"
",0,1,False,self,,,,,
8,deeplearning,t5_2t5eh,2019-4-1,2019,4,1,22,b82932,blockdelta.io,Uses of Artificial Intelligence in Health Care and How It Helps in Diagnosing Rare Genetic Disorders,https://www.reddit.com/r/deeplearning/comments/b82932/uses_of_artificial_intelligence_in_health_care/,BlockDelta,1554124598,,0,1,False,default,,,,,
9,deeplearning,t5_2t5eh,2019-4-1,2019,4,1,23,b833bh,mlwhiz.com,NLP Learning Series: Part 4 - Transfer Learning Intuition for Text Classification,https://www.reddit.com/r/deeplearning/comments/b833bh/nlp_learning_series_part_4_transfer_learning/,kiser_soze,1554128903,,0,1,False,default,,,,,
10,deeplearning,t5_2t5eh,2019-4-2,2019,4,2,0,b83qe6,self.deeplearning,How to share weights inside an tf.estimator.Estimator model_fn?,https://www.reddit.com/r/deeplearning/comments/b83qe6/how_to_share_weights_inside_an/,nobodywillobserve,1554131948,"I was trying to do something with the following pattern:

&amp;#x200B;

`def model_fn(features, labels):`

   `x_input = tf.placeholder(...)`

`def inner_thing(x):`

`with tf.variable_scope('asdf', reuse=tf.AUTO_REUSE):`

`return tf.layers.dense(x, 10, name='thing')`

`a = inner_thing(features['a'])`

`b = inner_thing(features['b'])`

`... combine etc deal with ModeKeys etc`

&amp;#x200B;

&amp;#x200B;

But I don't think that works as you need some sort of [sess.run](https://sess.run) and a feed\_dict to make the placeholders eat some data and estimator is supposed to do all that for you.

&amp;#x200B;

Any suggestions on how to restructure? 

&amp;#x200B;

&amp;#x200B;



",2,1,False,self,,,,,
11,deeplearning,t5_2t5eh,2019-4-2,2019,4,2,1,b84zod,livestream.com,SysML '19 Livestream,https://www.reddit.com/r/deeplearning/comments/b84zod/sysml_19_livestream/,xldrx,1554137752,,1,1,False,default,,,,,
12,deeplearning,t5_2t5eh,2019-4-2,2019,4,2,10,b8cp7o,self.deeplearning,RTX TITAN Benchmark results 2019 | VGG16 - FP 16 &amp; FP32 | Tensorflow 1.4.0,https://www.reddit.com/r/deeplearning/comments/b8cp7o/rtx_titan_benchmark_results_2019_vgg16_fp_16_fp32/,gimel1213,1554169144,"**VGG16 - FP 16 Combine Chart - Training values** 

**FP 16 - Using the new tensor cores on the Graphic cards.** 

&amp;#x200B;

**Card used:** 

**GTX 1080 TI, RTX 2070, 2080, 2080 TI, TITAN RTX, TITAN V** 

&amp;#x200B;

After a long day testing, here are the results I got from the Benchmarks: 

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/j63buij98rp21.png

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/anu6o9ob8rp21.png

&amp;#x200B;

The performance Benchmark show us that using FP16 the time for training is much better, this is because FP16 use the tensor cores on the new RTX cards and the Titan V, I think the reason why the Titan V is the best is because of the HBM2 memory, looking at the TITAN RTX should be the most powerful since it has 24 GB of Ram but actually is not. 

&amp;#x200B;

In terms of price I would say that the RTX 2080 Ti has the best value here, if you want to check and test your hardware you can use our app [DLBT ( Deep learning benchmark tool )](https://technopremium.com/) and upload your results to our community site so you can be one of the top 10. 

&amp;#x200B;

I am planning to build a deep learning workstation under $3000 using 4x GTX 1080 TI and a Threadripper 1900 , since this CPU has 64 PCI lanes should perform even better than the intel CPU X series that has 44 PCI lanes. 

&amp;#x200B;

&amp;#x200B;",10,2,False,self,,,,,
13,deeplearning,t5_2t5eh,2019-4-2,2019,4,2,15,b8fjdw,i.redd.it,"4x RTX 2080 Ti | Deep learning workstation, overheating issue fixed !!!!",https://www.reddit.com/r/deeplearning/comments/b8fjdw/4x_rtx_2080_ti_deep_learning_workstation/,gimel1213,1554186999,,24,91,False,image,,,,,
14,deeplearning,t5_2t5eh,2019-4-2,2019,4,2,16,b8fzsb,self.deeplearning,Face Liveness Detection,https://www.reddit.com/r/deeplearning/comments/b8fzsb/face_liveness_detection/,Ck_LeGrande,1554190160,"I'm a beginner in DL and I just trained my first CNN on the NUAA dataset [http://parnec.nuaa.edu.cn/xtan/data/NuaaImposterdb.html](http://parnec.nuaa.edu.cn/xtan/data/NuaaImposterdb.html) (only worked on 1000 random images) and got a test accuracy of 97.5%. However as this dataset is composed of low resolution images of fake faces, in real life testing, it was fooled by high resolution face images. 

I wanted to know what architecture of a neural network would suit this dataset (or similar datasets ) the best and what measures I must take to make a highly accurate face liveness detection model. ( I know improving my data would be the best in this situation and that is in progress )",1,3,False,self,,,,,
15,deeplearning,t5_2t5eh,2019-4-2,2019,4,2,18,b8gqul,self.deeplearning,How PV-DBOW works,https://www.reddit.com/r/deeplearning/comments/b8gqul/how_pvdbow_works/,jdyr1729,1554195901,"The authors of the Paragraph Vector paper describe PV-DBOW with:

&gt;**2.3. Paragraph Vector without word ordering: Distributed bag of words**  
&gt;  
&gt;The above method considers the concatenation of the paragraph vector with the word vectors to predict the next word in a text window. Another way is to ignore the context words in the input, but force the model to predict words randomly sampled from the paragraph in the output. In reality, what this means is that at each iteration of stochastic gradient descent, we sample a text window, then sample a random word from the text window and form a classification task given the Paragraph Vector.

I have a couple of questions:

1. Why do you need to sample a text window before sampling a random word? To create a batch, why can't you just randomly sample from a list of the form `[(1, ""cat""), (1, ""sat""), ..., (1, ""mat""), (2, ""humpty""), (2, ""dumpty""), ... (2, ""wall""), ...]` where the first item in each tuple represents the paragraph?
2. If hierarchical softmax or negative sampling is used, is stochastic gradient descent still used to update the weights in the network? Or are these optimization methods themselves?",0,1,False,self,,,,,
16,deeplearning,t5_2t5eh,2019-4-2,2019,4,2,19,b8hdow,self.deeplearning,where to find good practical implementations?,https://www.reddit.com/r/deeplearning/comments/b8hdow/where_to_find_good_practical_implementations/,jast26,1554200415,"Hello first time poster in this sub :)

To keep it short, i have a lot of theoretical knowledge in machine learning and neural networks, and a tiny beat of genetic algorithms. ( i know about CNN, RNN, LSTM, Q learning, NEAT... , more basic concepts as backprop etc...) the thing is that following theoretical courses online i never had proper assignments so i struggle to implement them. Also Im a basic coder, ( i understand loops, arrays, list, basic operators, structs, a bit of vectorization regarding ML,) 

I dont really know the difference between the various libraries out there (tensorflow,pytorch ...?).

Any good tutorials/ courses to get my hands dirty? Im willing to pay (not an exorbitant amount, around 100 bucks) 

I basically learned to code in order to use Unity3d, so c# neural nets using unity would suit me well for a start, otherwise python its ok

PLEASE DONT LINK ME TO A GITHUB CONTAINING BADLY COMMENT CODE 

I prefer a  proper walkthrough because they are faster and easier to understand than reading code from github. 

Thx everyone for their time :D 

",4,1,False,self,,,,,
17,deeplearning,t5_2t5eh,2019-4-2,2019,4,2,23,b8jmv6,self.deeplearning,Awesome papers and engineering reviews on Computer Vision News of March. Links for free reading!,https://www.reddit.com/r/deeplearning/comments/b8jmv6/awesome_papers_and_engineering_reviews_on/,Gletta,1554213836,"Here are the links to the April 2019 issue of Computer Vision News, the magazine of the algorithm community published by RSIP Vision: many articles about Artificial Intelligence, Deep Learning, Computer Vision and more.

Free subscription on page 32.

[HTML5 version (recommended)](https://www.rsipvision.com/ComputerVisionNews-2019April/)

[PDF version](https://www.rsipvision.com/computer-vision-news-2019-april-pdf/)

Enjoy!

https://i.redd.it/7y92kgi2yup21.jpg",0,2,False,self,,,,,
18,deeplearning,t5_2t5eh,2019-4-3,2019,4,3,2,b8mith,self.deeplearning,Hyperparameters Tuning - Deep Learning,https://www.reddit.com/r/deeplearning/comments/b8mith/hyperparameters_tuning_deep_learning/,19abhilash19,1554227552,"Hello! I wrote a blog in which I compiled all the methods I use to keep track of my grid search results for hyperparameter tuning. 

&amp;#x200B;

[https://www.redmarlin.ai/hyperparamter-tuning-deep-learning/](https://www.redmarlin.ai/hyperparamter-tuning-deep-learning/)",1,1,False,self,,,,,
19,deeplearning,t5_2t5eh,2019-4-3,2019,4,3,5,b8ormv,github.com,DiffAI v3: provably defending networks 99 layers deep against adversarial attacks.,https://www.reddit.com/r/deeplearning/comments/b8ormv/diffai_v3_provably_defending_networks_99_layers/,mmirman,1554238396,,0,6,False,default,,,,,
20,deeplearning,t5_2t5eh,2019-4-3,2019,4,3,6,b8pc3n,self.neuralnetworks,convNets and image resizing,https://www.reddit.com/r/deeplearning/comments/b8pc3n/convnets_and_image_resizing/,shsshs1,1554241175,,0,1,False,default,,,,,
21,deeplearning,t5_2t5eh,2019-4-3,2019,4,3,6,b8pe4t,self.deeplearning,Best way to deal with class imbalance problem in deep learning?,https://www.reddit.com/r/deeplearning/comments/b8pe4t/best_way_to_deal_with_class_imbalance_problem_in/,MrAaronW,1554241449,I have a dataset which is roughly 1 (pos) : 10 (neg). Is there a recommended way to deal with such kind of dataset besides over/under sampling?,3,2,False,self,,,,,
22,deeplearning,t5_2t5eh,2019-4-3,2019,4,3,9,b8qwnv,i.redd.it,"DIY Deep learning workstation, better performance than most of the WS sellers | overheating issue fixed on multiple cards system",https://www.reddit.com/r/deeplearning/comments/b8qwnv/diy_deep_learning_workstation_better_performance/,gimel1213,1554249730,,14,33,False,image,,,,,
23,deeplearning,t5_2t5eh,2019-4-3,2019,4,3,12,b8t7gi,self.deeplearning,Best way to evaluate segmentation score,https://www.reddit.com/r/deeplearning/comments/b8t7gi/best_way_to_evaluate_segmentation_score/,hpifueshopiuhefp,1554263696,"I am working on an image segmentation and I am wondering what the best way to compute mIOU. For example if my network is trained on images of size 300x300 but the test set has images of arbitrary but strictly smaller size should I resize the images to 300x300 and predict, then resize back to the original resolution or should I pad the images and then crop the real image portion out of the prediction. Or something else?",0,1,False,self,,,,,
24,deeplearning,t5_2t5eh,2019-4-3,2019,4,3,17,b8v6xz,self.deeplearning,Demand forecasting LSTM model,https://www.reddit.com/r/deeplearning/comments/b8v6xz/demand_forecasting_lstm_model/,naifmeh,1554279121,"Hello,   


Is anyone aware of an available already trained demand forecasting LSTM model ? I would like to fine tune it in order to fit my needs as my dataset is not large enough to give me convincing results by training a raw network from my data :)

&amp;#x200B;

Thanks in advance :)",0,1,False,self,,,,,
25,deeplearning,t5_2t5eh,2019-4-3,2019,4,3,19,b8wa61,self.deeplearning,Imp Announcement : BTech Engineering Students - Summer Internship with Industrial Training 2019,https://www.reddit.com/r/deeplearning/comments/b8wa61/imp_announcement_btech_engineering_students/,Summerinternship,1554287849,[removed],0,1,False,self,,,,,
26,deeplearning,t5_2t5eh,2019-4-3,2019,4,3,19,b8wfop,venturebeat.com,"Google open-sources GPipe, a library for efficiently training large deep neural networks",https://www.reddit.com/r/deeplearning/comments/b8wfop/google_opensources_gpipe_a_library_for/,techgig11,1554288911,,1,55,False,default,,,,,
27,deeplearning,t5_2t5eh,2019-4-3,2019,4,3,21,b8xb9y,dragan.rocks,Deep Learning from Scratch to GPU - 12 - A Simple Neural Network Training API,https://www.reddit.com/r/deeplearning/comments/b8xb9y/deep_learning_from_scratch_to_gpu_12_a_simple/,dragandj,1554294495,,0,2,False,default,,,,,
28,deeplearning,t5_2t5eh,2019-4-3,2019,4,3,22,b8xx8h,zdnet.com,"Run:AI takes your AI and runs it, on the super-fast software stack of the future | ZDNet",https://www.reddit.com/r/deeplearning/comments/b8xx8h/runai_takes_your_ai_and_runs_it_on_the_superfast/,ariehkovler,1554297924,,0,1,False,default,,,,,
29,deeplearning,t5_2t5eh,2019-4-4,2019,4,4,1,b90gse,self.deeplearning,Object detection over monocular single channel images.,https://www.reddit.com/r/deeplearning/comments/b90gse/object_detection_over_monocular_single_channel/,maheshmaceee,1554310279,"Hi folks, I have been working on an interesting project which involves detection of corners of cardboard boxes inside a particular image frame.  I created a train set of 400 images and a test set of 100 images which I manually annotated for the task of object detection. 
I am used yolov3 to break in the idea and try if this is even possible. Yolov3 apparently works great but breaks in particular cases some of which being:
1. Being trained on a small set with not many variation on the box corners, model struggles to detect corners when the boxes have different patterns and shapes. 
2. I tried to change the color space from rgb to grey and did sobel x and y with the same data I have and retrained the model, but not much of a luck with this approach. 

Please feel free to throw in your thoughts and ideas on this and I also have some more related question like:
1. How to come up with an annotation strategy for this particular case as box corners are pretty generic and there isnt a great way to specify boundaries for annotating them.
2. Is there any good approach than object detection which can be leveraged here? 

Peace!",1,1,False,self,,,,,
30,deeplearning,t5_2t5eh,2019-4-4,2019,4,4,2,b90n1c,youtube.com,3D Advanced Neural Network Simulation - Computer vision - Digit Recognit...,https://www.reddit.com/r/deeplearning/comments/b90n1c/3d_advanced_neural_network_simulation_computer/,DevTechRetopall,1554311087,,0,6,False,image,,,,,
31,deeplearning,t5_2t5eh,2019-4-4,2019,4,4,2,b9146e,self.deeplearning,Appending new classes to recognizer,https://www.reddit.com/r/deeplearning/comments/b9146e/appending_new_classes_to_recognizer/,anilmaddala,1554313328,"I have a pre-trained Image classification model. Is there a way to append a new image classes without loosing the previously trained class recognition?

Which model architectures support this requirement?",1,1,False,self,,,,,
32,deeplearning,t5_2t5eh,2019-4-4,2019,4,4,4,b929xh,self.deeplearning,Keras Learning Rate Finder: Loss is too sporadic,https://www.reddit.com/r/deeplearning/comments/b929xh/keras_learning_rate_finder_loss_is_too_sporadic/,_sleepyotter,1554318777,"I'm trying to implement the learning rate finder which is what Jeremy Howard uses in [Fast.ai](https://Fast.ai) on a 3D convolutional neural network in Keras. Here are two resources for reference:

* [https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0](https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0)
* [https://www.kaggle.com/paultimothymooney/learning-rate-finder-for-keras](https://www.kaggle.com/paultimothymooney/learning-rate-finder-for-keras)

&amp;#x200B;

In brief, I am trying to predict a 3D bounding box, so this is a regression problem with 6 outputs (x1,y1,z1,x2,y2,z2). The idea is to slowly increment the learning rate until the loss explodes. The inflection point sets the maximum range for the learning rate that you want to use. However, my results don't seem to follow this idea. The loss never seems to dip, unless I make the stopping condition larger.

&amp;#x200B;

The second link is the code that I am trying to use for the LR finder. My network is as follows:

    # Basic Feature Extractor 
    x = Conv3D(filters=32, kernel_size = (3, 3, 3), strides=(1,1,1), padding='same', \
               activation='relu', name='block1_conv1',kernel_initializer='random_normal')            
          (img_input)
    x = Conv3D(filters=32, kernel_size = (3, 3, 3), strides=(1,1,1), padding='same', \
               activation='relu', name='block1_conv2',kernel_initializer='random_normal')(x)
    x = Conv3D(filters=64, kernel_size = (3, 3, 3), strides=(1,1,1), padding='same', \
               activation='relu', name='block1_conv3',kernel_initializer='random_normal')(x)
    x = Conv3D(filters=64, kernel_size = (3, 3, 3), strides=(1,1,1), padding='same', \
               activation='relu', name='block1_conv4',kernel_initializer='random_normal')(x)
    x = Conv3D(filters=128, kernel_size = (3, 3, 3), strides=(1,1,1), padding='same', \
               activation='relu', name='block1_conv5',kernel_initializer='random_normal')(x)
    x = BatchNormalization(name='bn_5')(x)
    x = Conv3D(filters=128, kernel_size = (3, 3, 3), strides=(1,1,1), padding='same', \
               activation='relu', name='block1_conv6',kernel_initializer='random_normal')(x)
    x = Flatten(name='flatten')(x)    
    out = Dense(6, activation='linear', kernel_initializer='random_normal', name='regr_output')(x)

For reference, this model does pretty well on my dataset (aside from the overfitting). Here's a loss vs epoch curve on train and validation :

https://i.redd.it/73ib0m38h3q21.png

&amp;#x200B;

&amp;#x200B;

I'm trying to find a more optimal learning rate. In the results above I used a constant learning rate of 1e-5 which I picked arbitrarily to start off with.

&amp;#x200B;

Starting at a learning rate of 1e-11 and incrementing it over a batch size of 64, 5 epochs, and dataset size of 10,000 images, my loss vs learning rate plot never decreases:

&amp;#x200B;

https://i.redd.it/pssappmul3q21.png

&amp;#x200B;

It isn't until I bump up my stopping condition that the loss goes down, but at this point i'm assuming that my model has already learned enough to decrease the loss.

&amp;#x200B;

https://i.redd.it/qwym1r45m3q21.png

&amp;#x200B;

&amp;#x200B;

**Does this mean that my model is not as sensitive to the learning rate, or is something going wrong here?**

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
33,deeplearning,t5_2t5eh,2019-4-4,2019,4,4,4,b92fgf,self.deeplearning,Keras Learning Rate Finder: Loss doesn't decrease,https://www.reddit.com/r/deeplearning/comments/b92fgf/keras_learning_rate_finder_loss_doesnt_decrease/,_sleepyotter,1554319460,"I'm trying to implement the learning rate finder which is what Jeremy Howard uses in [Fast.ai](https://Fast.ai) on a 3D convolutional neural network in Keras. Here are two resources for reference:

* [https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0](https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0)
* [https://www.kaggle.com/paultimothymooney/learning-rate-finder-for-keras](https://www.kaggle.com/paultimothymooney/learning-rate-finder-for-keras)

&amp;#x200B;

In brief, I am trying to predict a 3D bounding box, so this is a regression problem with 6 outputs (x1,y1,z1,x2,y2,z2). The idea is to slowly increment the learning rate until the loss explodes. The inflection point sets the maximum range for the learning rate that you want to use. However, my results don't seem to follow this idea. The loss never seems to dip, unless I make the stopping condition larger.

&amp;#x200B;

The second link is the code that I am trying to use for the LR finder. My network is as follows:

    # Basic Feature Extractor 
    x = Conv3D(filters=32, kernel_size = (3, 3, 3), strides=(1,1,1), padding='same', \
               activation='relu', name='block1_conv1',kernel_initializer='random_normal')            
          (img_input)
    x = Conv3D(filters=32, kernel_size = (3, 3, 3), strides=(1,1,1), padding='same', \
               activation='relu', name='block1_conv2',kernel_initializer='random_normal')(x)
    x = Conv3D(filters=64, kernel_size = (3, 3, 3), strides=(1,1,1), padding='same', \
               activation='relu', name='block1_conv3',kernel_initializer='random_normal')(x)
    x = Conv3D(filters=64, kernel_size = (3, 3, 3), strides=(1,1,1), padding='same', \
               activation='relu', name='block1_conv4',kernel_initializer='random_normal')(x)
    x = Conv3D(filters=128, kernel_size = (3, 3, 3), strides=(1,1,1), padding='same', \
               activation='relu', name='block1_conv5',kernel_initializer='random_normal')(x)
    x = BatchNormalization(name='bn_5')(x)
    x = Conv3D(filters=128, kernel_size = (3, 3, 3), strides=(1,1,1), padding='same', \
               activation='relu', name='block1_conv6',kernel_initializer='random_normal')(x)
    x = Flatten(name='flatten')(x)    
    out = Dense(6, activation='linear', kernel_initializer='random_normal', name='regr_output')(x)

For reference, this model does pretty well on my dataset (aside from the overfitting). Here's a loss vs epoch curve on train and validation :

https://i.redd.it/40ptvlqvm3q21.png

I'm trying to find a more optimal learning rate. In the results above I used a constant learning rate of 1e-5 which I picked arbitrarily to start off with.

&amp;#x200B;

Starting at a learning rate of 1e-11 and incrementing it over a batch size of 64, 5 epochs, and dataset size of 10,000 images, my loss vs learning rate plot never decreases:

https://i.redd.it/0x09u6mtm3q21.png

It isn't until I bump up my stopping condition that the loss goes down, but at this point i'm assuming that my model has already learned enough to decrease the loss.

https://i.redd.it/e8koorlum3q21.png

**Does this mean that my model is not as sensitive to the learning rate, or is something going wrong here?**

&amp;#x200B;",2,2,False,self,,,,,
34,deeplearning,t5_2t5eh,2019-4-4,2019,4,4,5,b92z19,pgaleone.eu,How does tf.function work? Weird behaviors and bad performance analysis,https://www.reddit.com/r/deeplearning/comments/b92z19/how_does_tffunction_work_weird_behaviors_and_bad/,pgaleone,1554321984,,0,1,False,default,,,,,
35,deeplearning,t5_2t5eh,2019-4-4,2019,4,4,6,b93xcu,welcome.ai,5 AI Companies using Deep Learning in Finance,https://www.reddit.com/r/deeplearning/comments/b93xcu/5_ai_companies_using_deep_learning_in_finance/,jdv9,1554326597,,0,0,False,default,,,,,
36,deeplearning,t5_2t5eh,2019-4-4,2019,4,4,13,b98gg0,self.deeplearning,"Math question for a non-math person in the topic of ""norms"".",https://www.reddit.com/r/deeplearning/comments/b98gg0/math_question_for_a_nonmath_person_in_the_topic/,eyun89,1554353782,"I am thoroughly reading through [Deep Learning](https://www.deeplearningbook.org/contents/TOC.html) and really want to understand the math. I never had the motivation to understand math until my recent interest in machine learning and deep learning in general. I have this mathematical notations [page](https://en.wikipedia.org/wiki/List_of_mathematical_symbols) from Wikipedia open on the side as I am trying to decode the formulas but am having trouble understanding them as a whole. 

I'm in the section of the book describing [norms](https://en.wikipedia.org/wiki/Norm_(mathematics)) and how a norm is any function that satisfies three listed properties. One of those properties is written as $\forall\alpha\in\mathbb{R},f(\alpha\mathbf{x})=|\alpha|f(\mathbf{x})$  

My current understanding as I look at the formula is that ""For all numbers in alpha(which confuses me from the start, because I found it means proportional to, but it is being used as a variable) is an element of the set of all real numbers and the function of alpha times vector x is equal to the absolute value of alpha times the function of vector x"".

Can anyone help me decode this into regular people words?  ",0,1,False,self,,,,,
37,deeplearning,t5_2t5eh,2019-4-4,2019,4,4,14,b98jvp,self.deeplearning,"Math question from a non-math person in the topic of ""norms"".",https://www.reddit.com/r/deeplearning/comments/b98jvp/math_question_from_a_nonmath_person_in_the_topic/,eyun89,1554354454,"I am thoroughly reading through \[Deep Learning\]([https://www.deeplearningbook.org/contents/TOC.html](https://www.deeplearningbook.org/contents/TOC.html)) and really want to understand the math. I never had the motivation to understand math until my recent interest in machine learning and deep learning in general. I have this mathematical notations \[page\]([https://en.wikipedia.org/wiki/List\_of\_mathematical\_symbols](https://en.wikipedia.org/wiki/List_of_mathematical_symbols)) from Wikipedia open on the side as I am trying to decode the formulas but am having trouble understanding them as a whole. 

&amp;#x200B;

I'm in the section of the book describing \[norms\]([https://en.wikipedia.org/wiki/Norm\_(mathematics)](https://en.wikipedia.org/wiki/Norm_(mathematics))) and how a norm is any function that satisfies three listed properties. One of those properties is written as \`R,f(x)=f(x)\`.

&amp;#x200B;

My current understanding as I look at the formula is that ""For all numbers in alpha(which confuses me from the start, because I found it means proportional to, but it is being used as a variable) is an element of the set of all real numbers and the function of alpha times vector x is equal to the absolute value of alpha times the function of vector x"".

&amp;#x200B;

Can anyone help me decode this into regular people words?  ",6,8,False,self,,,,,
38,deeplearning,t5_2t5eh,2019-4-4,2019,4,4,21,b9cbe6,self.deeplearning,[Advice] Best network architecture to use,https://www.reddit.com/r/deeplearning/comments/b9cbe6/advice_best_network_architecture_to_use/,Null_State,1554382286,"I am trying to train a keras network to predict employee performance. I have a large data set that is broken down into discreet time-steps per employee. The issue is the number of time-steps per employee is not fixed.

&amp;#x200B;

What's the best way to handle that? Should I just pad all the sequences to the max length, or should I train the model in batches of matching lengths?

&amp;#x200B;

Also, I was planning on using LSTM layers, does that sound right?

&amp;#x200B;

Sorry if these are basic questions, I'm very new to deep learning.

&amp;#x200B;

Thanks!",0,0,False,self,,,,,
39,deeplearning,t5_2t5eh,2019-4-4,2019,4,4,22,b9csck,youtube.com,Donald Trump AI model tries to sing 'Lose Yourself' by Eminem (1st attempt),https://www.reddit.com/r/deeplearning/comments/b9csck/donald_trump_ai_model_tries_to_sing_lose_yourself/,hanyuqn,1554384953,,20,51,False,default,,,,,
40,deeplearning,t5_2t5eh,2019-4-4,2019,4,4,23,b9dcoy,self.deeplearning,Classifying logos if one class is a subset of another?,https://www.reddit.com/r/deeplearning/comments/b9dcoy/classifying_logos_if_one_class_is_a_subset_of/,VeniVidiReliqui,1554387811,"\[Advice\] I'm working on making a dataset of logos to train on, but some of the items exist as a part of others. For example, I'm hoping to recognize something like the Windows XP and Windows Vista logos below, but also recognize the Windows logo on its own.

&amp;#x200B;

[Recognize these...](https://i.redd.it/md7d5xzf89q21.jpg)

&amp;#x200B;

[...vs this](https://i.redd.it/bgff0ywg89q21.png)

Am I able to censor subsets of the larger labels with black boxes and learn each piece individually? For example, could I turn the Windows Vista logo above into a ""Windows Vista Text"" class and a ""Windows Icon"" class by covering parts of the image with a black box? Then group both labels together into a ""Windows Vista Icon"" at a different step?",0,1,False,self,,,,,
41,deeplearning,t5_2t5eh,2019-4-5,2019,4,5,0,b9dt24,self.deeplearning,"GPU Server for ML | GTX 1070Ti $0,12/h",https://www.reddit.com/r/deeplearning/comments/b9dt24/gpu_server_for_ml_gtx_1070ti_012h/,render_rapidly,1554390070,"Hi, I'm offering this very affordable server built for ML:

* 1x GTX 1070Ti, Intel pentium 2 core\*\*, 12 GB RAM, 120 GB SSD
* GPU on motherboard with **PCIe x16** bandwidth
* **Dedicated** server, no VM
* Ubuntu 16.04. Nvidia driver and CUDA  toolkit **pre-installed**
* 100 Mbps download and unlimited bandwidth
*  $25/week, $89/month **($0,12/h)**

*Minimum rental is 1 week. Thanks.*

\*\* [should not be a bottleneck](http://timdettmers.com/2018/12/16/deep-learning-hardware-guide/)

&amp;#x200B;",0,1,False,self,,,,,
42,deeplearning,t5_2t5eh,2019-4-5,2019,4,5,0,b9ef8r,self.deeplearning,I want to classify multiple numbers in a single image.,https://www.reddit.com/r/deeplearning/comments/b9ef8r/i_want_to_classify_multiple_numbers_in_a_single/,gthm1,1554392970,"Recently, i have trained a ml model on mnist dataset using tensorflow. Now want my model to classify multiple numbers in a  single image (just like yolo or any other object recognizer). How can i do it? ",2,4,False,self,,,,,
43,deeplearning,t5_2t5eh,2019-4-5,2019,4,5,3,b9g72y,medium.com,New Google Brain Optimizer Reduces BERT Pre-Training Time From Days to Minutes,https://www.reddit.com/r/deeplearning/comments/b9g72y/new_google_brain_optimizer_reduces_bert/,gwen0927,1554401376,,5,16,False,default,,,,,
44,deeplearning,t5_2t5eh,2019-4-5,2019,4,5,3,b9g8ct,medium.com,Is the Fashion World Ready for AI-Designed Dresses?,https://www.reddit.com/r/deeplearning/comments/b9g8ct/is_the_fashion_world_ready_for_aidesigned_dresses/,gwen0927,1554401544,,1,2,False,default,,,,,
45,deeplearning,t5_2t5eh,2019-4-5,2019,4,5,4,b9guy7,self.deeplearning,Interesting demos for grade school children,https://www.reddit.com/r/deeplearning/comments/b9guy7/interesting_demos_for_grade_school_children/,justanator101,1554404463,"I am tasked with coming up with a demo of AI and deep learning to make grade school kids interested and aware of what is out there. Our theme is myth busting. One demo we currently have is deep fakes and fake news. 

can anyone propose exciting demos that could be done?",2,1,False,self,,,,,
46,deeplearning,t5_2t5eh,2019-4-5,2019,4,5,5,b9hzdp,self.deeplearning,LSTM Time Series Prediction,https://www.reddit.com/r/deeplearning/comments/b9hzdp/lstm_time_series_prediction/,AwareDoor,1554409975,"I am trying to understand how time series prediction with LSTM works. For starters, I generated a dataset in which label of each sample is equal to the first feature of the sample three steps before.

i.e

    ```
    1 ,  1  -&gt;  0
    0 ,  0  -&gt;  0
    0 ,  0  -&gt;  0
    1 ,  1  -&gt;  1     
    0 ,  0  -&gt;  0
    0 ,  0  -&gt;  0
    1 ,  1  -&gt;  1
    0 ,  0  -&gt;  0
    1 ,  1  -&gt;  0
    0 ,  0  -&gt;  1
    ``` 

I generated training set and test set with 100000 elements and used the model below.

    ```
    model = Sequential()
    model.add(CuDNNLSTM(20, batch_input_shape=(batch_size, 10, x.shape[2]), return_sequences=True, stateful=True))
    model.add(TimeDistributed(Dense(1, kernel_initializer='normal', activation='sigmoid')))
    # Compile model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.fit(x,y,validation_data=(xt,yt),verbose=1,epochs=50, batch_size=batch_size, shuffle=False)
    
    predictions = model.predict(xt, batch_size=batch_size)  
    
    ``` 
    
    Results
    
    Epoch 50/50
    10000/10000 [==============================] - 0s 23us/step - loss: 0.4230 - acc: 0.8503 - val_loss: 0.4233 - val_acc: 0.
    
    Confusion Matrix
    array([[42506,  7504],
           [ 7502, 42488]], dtype=int64)

However, the best accuracy I got is  0.8503 and my loss value never changes after 11th epoch.  I tried using different hidden unit values, optimizers and learning rates but could not improve the accuracy.

&amp;#x200B;

Since this is a simple problem I think the accuracy should be better than this. But I cant find what I did wrong.  Any tips would be appreciated. 

&amp;#x200B;",0,1,False,self,,,,,
47,deeplearning,t5_2t5eh,2019-4-5,2019,4,5,8,b9jwyj,medium.com,Father of GANs Ian GoodFellow Splits Google For Apple,https://www.reddit.com/r/deeplearning/comments/b9jwyj/father_of_gans_ian_goodfellow_splits_google_for/,gwen0927,1554420368,,1,0,False,default,,,,,
48,deeplearning,t5_2t5eh,2019-4-5,2019,4,5,8,b9jxgx,self.deeplearning,AutoML for deep reinforcement learning?,https://www.reddit.com/r/deeplearning/comments/b9jxgx/automl_for_deep_reinforcement_learning/,ZeroMaxinumXZ,1554420455,I'm trying to think of ways to mix autoML with reinforcement learning. I don't think Network Architecture Search will work as it requires some kind of validation set.  Any papers/suggestions to help me? Thanks.,0,1,False,self,,,,,
49,deeplearning,t5_2t5eh,2019-4-5,2019,4,5,14,b9nh76,patentsandtech.com,AI in healthcare: Google advances in predictive analytics,https://www.reddit.com/r/deeplearning/comments/b9nh76/ai_in_healthcare_google_advances_in_predictive/,patentsandtech,1554443808,,0,10,False,default,,,,,
50,deeplearning,t5_2t5eh,2019-4-5,2019,4,5,16,b9o5lo,self.deeplearning,"Keras, resize images in np.array with fit.generator?",https://www.reddit.com/r/deeplearning/comments/b9o5lo/keras_resize_images_in_nparray_with_fitgenerator/,granular2,1554449189,"I am using a pretrained model that was trained on images of size 150,150,3. I am experimenting with the fashion MNIST dataset, which has images of the size 28,28,1. So in order to use the pretrained model I should resize the fmnist numpy arrays that contain the images. I would use something like `training_images_resized  = (skimage.transform.resize(image, (150,150,3)) for image in training_images)`  but the resulting array takes up to much memory (13gib limit). So I gather I can use fit.generator and do the resizing there, right? But not sure how to get there, any pointers?

Also I think I have seen something about Keras being able to treat grayscale images as color? maybe not necessary to make three dimensions?

Thanks",5,1,False,self,,,,,
51,deeplearning,t5_2t5eh,2019-4-5,2019,4,5,18,b9ov67,self.deeplearning,Know any books i could use to write music to oppose the uk government,https://www.reddit.com/r/deeplearning/comments/b9ov67/know_any_books_i_could_use_to_write_music_to/,16yearoldwritingrap,1554455391,Know any books i could use to write music to oppose the uk government masons millionaires billonaires the ones who hide in the shadows i aint afraid anymore ,10,0,False,self,,,,,
52,deeplearning,t5_2t5eh,2019-4-5,2019,4,5,23,b9s581,self.deeplearning,Do you need a lot of resources to utilize the network you trained?,https://www.reddit.com/r/deeplearning/comments/b9s581/do_you_need_a_lot_of_resources_to_utilize_the/,Seroy,1554475900,"I'm kind of curious about DL, since training it requires a lot of resources.

But what about the part after training it?

Say would be than possible to use the final network on a low powered device (raspberry pi zero, etc ) or do you still need a lot of resources?",6,5,False,self,,,,,
53,deeplearning,t5_2t5eh,2019-4-6,2019,4,6,0,b9svlm,apriorit.com,Deep Learning for Overcoming Challenges of Detecting Moving Objects in Video,https://www.reddit.com/r/deeplearning/comments/b9svlm/deep_learning_for_overcoming_challenges_of/,RyanTmthn,1554479536,,0,1,False,default,,,,,
54,deeplearning,t5_2t5eh,2019-4-6,2019,4,6,3,b9uub3,youtu.be,Can this be implemented in any games?,https://www.reddit.com/r/deeplearning/comments/b9uub3/can_this_be_implemented_in_any_games/,GamesRealmTV,1554489095,,1,4,False,default,,,,,
55,deeplearning,t5_2t5eh,2019-4-6,2019,4,6,5,b9w3b5,self.deeplearning,I am interested in space and deep learning. I have taken deep Learning specialization course on Coursera. Still a month left to complete that specialization. Can anyone help me with the ideas or resources to gather space related datasets and apply some deep learning techniques.,https://www.reddit.com/r/deeplearning/comments/b9w3b5/i_am_interested_in_space_and_deep_learning_i_have/,cherry324,1554495361,,5,2,False,self,,,,,
56,deeplearning,t5_2t5eh,2019-4-6,2019,4,6,7,b9xbi5,youtube.com,How Neural Networks Work- Simply Explained by a Machine Learning Engineer,https://www.reddit.com/r/deeplearning/comments/b9xbi5/how_neural_networks_work_simply_explained_by_a/,DiscoverAI,1554501875,,0,9,False,image,,,,,
57,deeplearning,t5_2t5eh,2019-4-6,2019,4,6,9,b9ylba,self.computervision,Guidance for PhD in Computer Vision and Deep Learning,https://www.reddit.com/r/deeplearning/comments/b9ylba/guidance_for_phd_in_computer_vision_and_deep/,RohitDulam,1554509466,,0,2,False,default,,,,,
58,deeplearning,t5_2t5eh,2019-4-7,2019,4,7,0,ba5db0,self.deeplearning,Video Dialogue Replacement: World leaders singing 'Imagine',https://www.reddit.com/r/deeplearning/comments/ba5db0/video_dialogue_replacement_world_leaders_singing/,reobb,1554564144,"Hi, I'm from Canny AI, this is a project we did to showcase the potential of the technology:

[https://www.youtube.com/watch?v=R4UxVmYKiGA](https://www.youtube.com/watch?v=R4UxVmYKiGA)",5,20,False,self,,,,,
59,deeplearning,t5_2t5eh,2019-4-7,2019,4,7,4,ba7s2r,self.deeplearning,"""ICface"", AI that can make an face image lively",https://www.reddit.com/r/deeplearning/comments/ba7s2r/icface_ai_that_can_make_an_face_image_lively/,soumya6097,1554577754,"**Please watch this video to see the ICface in action**: [**https://lnkd.in/gHkmPcS**](https://lnkd.in/gHkmPcS)

Please follow our project page for updates on implementation: [**https://lnkd.in/gusAyjb**](https://lnkd.in/gusAyjb)

Please see our paper for more details: [**https://lnkd.in/gFpGgyS**](https://lnkd.in/gFpGgyS)

![video](chxd86xfzoq21 ""ICface"")",0,1,False,self,,,,,
60,deeplearning,t5_2t5eh,2019-4-7,2019,4,7,4,ba86lp,self.deeplearning,Anybody have a simple tensorflow 2.0 optimizer example working?,https://www.reddit.com/r/deeplearning/comments/ba86lp/anybody_have_a_simple_tensorflow_20_optimizer/,nobodywillobserve,1554580211,"Should be something like this but I think eager does something or the API has changed?  


    import tensorflow as tf
    import numpy as np
    
    x = tf.Variable(2, name='x', trainable=True, dtype=tf.float32)
    with tf.GradientTape() as t:
        t.watch(x)
        log_x = tf.math.log(x)
        y = tf.math.square(log_x)
    
    opt = tf.optimizers.Adam(0.5)
    # train = opt.minimize(lambda: y, var_list=[x]) # FAILS

&amp;#x200B;",0,5,False,self,,,,,
61,deeplearning,t5_2t5eh,2019-4-7,2019,4,7,6,ba9ej6,reddit.com,Semantic Segmentation on Raspberry Pi Zero,https://www.reddit.com/r/deeplearning/comments/ba9ej6/semantic_segmentation_on_raspberry_pi_zero/,9_ties,1554587554,,0,1,False,default,,,,,
62,deeplearning,t5_2t5eh,2019-4-7,2019,4,7,14,bad9gt,self.deeplearning,Data Science Deep Learning Training In Bangalore,https://www.reddit.com/r/deeplearning/comments/bad9gt/data_science_deep_learning_training_in_bangalore/,SunilAhujaa,1554615233,If you are looking for data science deep learning institute in Bangalore then Analytixlabs is one of the best options for deep learning training in Bangalore. This is a specialization course which will help you to get a break into AI and Deep Learning domain.,2,0,False,self,,,,,
63,deeplearning,t5_2t5eh,2019-4-7,2019,4,7,18,baep59,self.deeplearning,Converting y_true and y_pred to numpy arrays in custom loss function in Keras,https://www.reddit.com/r/deeplearning/comments/baep59/converting_y_true_and_y_pred_to_numpy_arrays_in/,Andohuman,1554630279," Hey guys, I was wondering if there was any way to convert my y\_true and y\_pred to numpy arrays as my loss involves a ton of morphological operations depending on y\_true and y\_pred.

In my own testing my loss function works because I supply it with the data for y\_true and y\_pred and convert them to numpy arrays using keras.backend.eval(). However, If i try to compile the model using my loss function in model.compile(loss=my\_loss\_fn()), it gives me errors about feeding values to placeholder, because K.eval can't run during model.compile().

Is there any way to get around this ? I'm kind of on a deadline too and I never expected to encounter such a problem in the first place.

Loss function Code:- [https://pastebin.com/wUJiuNf3](https://pastebin.com/wUJiuNf3)",3,1,False,self,,,,,
64,deeplearning,t5_2t5eh,2019-4-7,2019,4,7,22,bag6lg,self.deeplearning,Having Trouble with a denoising image with DCGAN .,https://www.reddit.com/r/deeplearning/comments/bag6lg/having_trouble_with_a_denoising_image_with_dcgan/,__sumguy,1554643389," 

I trying to implement a Denoising the image with DCGAN. I am trying to follow this [link](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) .

The implementation in the above link is trying to generate ""Real Looking Fake images"" from the celeb dataset. I have a doubt in how will i implement DCGAN for denoising images.

1. I would take any image data-set and create a noisy images from the original image using opencv (or some other technique)
2. The discriminator will discriminate between noisy and original image.
3. The generator will generate denoised images from the noisy images.

How will i implement this generator part. In the original [link](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) their dataset has only celeb data unlike my data which consists of original and noisy data. How will i modify the generator such that it takes noisy image and generates denoised image . Help me out if i am not understanding things correctly I am relatively new to DL and i want this as my project to be done in the next 7 days . Thanks in Advance",1,6,False,self,,,,,
65,deeplearning,t5_2t5eh,2019-4-7,2019,4,7,23,bagr56,self.deeplearning,Resources for Anomaly detection of Images?,https://www.reddit.com/r/deeplearning/comments/bagr56/resources_for_anomaly_detection_of_images/,kesaroid,1554647216,"I wanted to build a One class classifier and then detect outliers in my test data. But couldn't find much luck with anomaly detection for images.  Initially I tried extracting the features using Resnet and passing it through One class SVM and even Isolation Tree, but didn't find satisfactory results. Any good resources for the same would be really useful. 
Thanks",1,3,False,self,,,,,
66,deeplearning,t5_2t5eh,2019-4-8,2019,4,8,2,baijre,self.deeplearning,Final Year Project,https://www.reddit.com/r/deeplearning/comments/baijre/final_year_project/,meishc,1554657308,"Hey guys, Me and my group have some average ML skills. Can anyone recommend some good deep learning project idea for our final year project? or maybe point us in a direction to find an idea?
",2,0,False,self,,,,,
67,deeplearning,t5_2t5eh,2019-4-8,2019,4,8,3,bajn04,self.deeplearning,Deep Learning With NO Code,https://www.reddit.com/r/deeplearning/comments/bajn04/deep_learning_with_no_code/,lomiag,1554663014,"Hello everyone,

The project is called Deep Learner. This is a project my and my friends worked on during one of the Hackathons I thought it would be appropriate for this sub. The purpose of this project is to allow people prototype Deep Neural networks very very quickly and with writing 0 lines of code. For now you can only build regular dense networks. It also allows for data visualization using Tableau and you can also save models in json format. Please note this is very much work in progress and was made by 4 college students in 24 hours, so there will be bugs. We decided to make this open source, so you are more than welcome to contribute, I have posted a couple issues on github so feel free to contribute.  We are hoping to expand on it in the near future.

[Deep Learner](https://github.com/GioLomia/Deep_Learner)

Let me know if you have any questions &lt;3.",11,36,False,self,,,,,
68,deeplearning,t5_2t5eh,2019-4-8,2019,4,8,6,balhio,self.deeplearning,Deep Learning and Music - Survey and Discussion,https://www.reddit.com/r/deeplearning/comments/balhio/deep_learning_and_music_survey_and_discussion/,g2ransom,1554673210,"[https://forms.gle/qfkm3HPoj5v2fFfr8](https://forms.gle/qfkm3HPoj5v2fFfr8) \- check out this survey around music and AI

Companies like Google, Facebook, Sony, IBM, and Spotify have spent considerable time and resources towards the intersection of music and artificial intelligence. Think about an AI system that can create music all by itself or a tool that aids a human in the music creation process. Google Magenta has models that generate sequences of music, sounds, and more using different deep learning architectures (GANs, LSTMs, autoencoders).

I've done a good bit of researching projects within the space and want to gather more opinions from folks interested in deep learning. I've attached a 3-5 minute survey and I'd love to get this community's input. It's been the general consensus that AI could not be used for creative tasks, but we see that's not quite the case. Let's start a dialogue!",0,0,False,self,,,,,
69,deeplearning,t5_2t5eh,2019-4-8,2019,4,8,6,balomk,self.deeplearning,Setup deep learning rig,https://www.reddit.com/r/deeplearning/comments/balomk/setup_deep_learning_rig/,MidnightMiasma,1554674358,"I am a researcher doing work in deep learning, and I just bought a Lambda rig running Ubuntu 18.04 LTS (mandate from my employer to not use cloud services).

Excited to have this machine, but now I have to do system administration for my entire research group and I have relatively little experience with this. I currently have a blank slate and I am hoping that my early decisions will be good ones.

With that in mind:

1) Is there a low footprint system monitoring dashboard I can use to continuously monitor resource utilization and sensors (e.g., GPU temperature)?

2) Is there some way to track (and potentially prioritize) resource use over time by user? (I want to make sure that every one of my students has reasonable access.)

3) I would like a system-wide repository for training data so that each student isn't needlessly replicating data in his/her own directory. I was thinking of a system level folder containing each dataset in a separate folder with appropriate group-level permissions. Is there a smarter way to do this?

4) What would be best practice for working on code across the team? Something like GitLab or Bitbucket? I am a bit ashamed to say that we have mostly just updated our code in the raw as a matter of simplicity, but documentation is a problem especially as students come and go.

&amp;#x200B;

Thanks!",0,4,False,self,,,,,
70,deeplearning,t5_2t5eh,2019-4-8,2019,4,8,13,bapb5h,self.deeplearning,4x RTX 2080 TI with Quadro Nvlink | Performance Test,https://www.reddit.com/r/deeplearning/comments/bapb5h/4x_rtx_2080_ti_with_quadro_nvlink_performance_test/,gimel1213,1554697294,Check full post here: [https://technopremium.com/blog/4x-rtx-2080-ti-with-quadro-nvlink-performance-test/](https://technopremium.com/blog/4x-rtx-2080-ti-with-quadro-nvlink-performance-test/),0,1,False,self,,,,,
71,deeplearning,t5_2t5eh,2019-4-8,2019,4,8,14,bapx3b,self.deeplearning,Very small data being predicted as 0,https://www.reddit.com/r/deeplearning/comments/bapx3b/very_small_data_being_predicted_as_0/,Cyclonedx,1554701691,"I have an output that consists of very small values (0.00X) which are being predicted as a 0 by the network.

Do I need to store these values in a particular format before training to prevent this? Also is it a bad idea to normalize impute features which are of a similar size?",0,1,False,self,,,,,
72,deeplearning,t5_2t5eh,2019-4-8,2019,4,8,19,barzd1,self.deeplearning,Deep Learning Market,https://www.reddit.com/r/deeplearning/comments/barzd1/deep_learning_market/,nareshkumar02,1554718109,[removed],0,1,False,self,,,,,
73,deeplearning,t5_2t5eh,2019-4-9,2019,4,9,0,bauwcp,self.deeplearning,How can one begin the grad school journey for researching on generative models? What are some good places to start seeking for programs and faculty that supports such uncertain topics?,https://www.reddit.com/r/deeplearning/comments/bauwcp/how_can_one_begin_the_grad_school_journey_for/,alias_is,1554735971,,2,9,False,self,,,,,
74,deeplearning,t5_2t5eh,2019-4-9,2019,4,9,7,bb042v,blog.nanonets.com,Automating Visual Inspection using Deep Learning: everything you need to know,https://www.reddit.com/r/deeplearning/comments/bb042v/automating_visual_inspection_using_deep_learning/,manneshiva,1554762661,,0,3,False,default,,,,,
75,deeplearning,t5_2t5eh,2019-4-9,2019,4,9,9,bb1fv6,self.deeplearning,"[Question] Tensorflow inference run time high on first data point, decreases on subsequent data points. How to reduce?",https://www.reddit.com/r/deeplearning/comments/bb1fv6/question_tensorflow_inference_run_time_high_on/,naboo_random,1554770163,"I am running inference using one of the models from tensorflow's object detection module. I'm looping over my test images in the same session, and doing session.run(). However, on profiling these runs, I realize the first run always has a higher time as compared to the subsequent runs.

I found an answer [here](https://stackoverflow.com/questions/45063489/first-tf-session-run-performs-dramatically-different-from-later-runs-why), as to why that happens, but there was no solution on how to fix.

I'm deploying the object detection inference pipeline on an intel i7 CPU. The time for one session.run(), for 1,2,3, and 4th image looks something like (in seconds):

1. 84.7132628
2. 1.495621681
3. 1.505012751
4. 1.501652718

Just a background on what all I have tried:

* I tried using the TFRecords approach tensorflow gave as a sample [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/oid_inference_and_evaluation.md). I hoped it would work better because it doesn't use a feed\_dict. But since more I/O operations are involved, I'm not sure it'll be ideal. I tried making it work without writing to the disk, but always got some error regarding the encoding of the image.
* I tried using the tensorflow datasets to feed the data, but I wasn't sure how to provide the input, since the during inference I need to provide input for ""image tensor"" key in the graph. Any ideas how to use this to provide input to a frozen graph?

Any help will be greatly appreciated!

TLDR: Looking to reduce the run time of inference for the first image - for deployment purposes.",0,1,False,self,,,,,
76,deeplearning,t5_2t5eh,2019-4-9,2019,4,9,9,bb1knp,self.deeplearning,"What does it mean ""GANs are notoriously susceptible to mode collapse""?",https://www.reddit.com/r/deeplearning/comments/bb1knp/what_does_it_mean_gans_are_notoriously/,Catherine_Fang,1554770945,"What does it mean ""GANs are notoriously susceptible to mode collapse""?

Can someone explain it in more details?",2,2,False,self,,,,,
77,deeplearning,t5_2t5eh,2019-4-9,2019,4,9,10,bb2870,self.deeplearning,Free cloud GPU credits for deep learning,https://www.reddit.com/r/deeplearning/comments/bb2870/free_cloud_gpu_credits_for_deep_learning/,whitezl0,1554774926,"Hi, I am offering free credits for you to access 1080Ti GPU instances for deep learning purposes.

I am a co-founder of Tensorpad; where we are creating a service for AI startups to train neural networks. We have paid traffic, but some servers are idle. Hence, we are offering some credits for free, so that students, startups, and others can benefit from ML technologies and help us by using our product and providing honest feedback for us to improve.

You can Sign Up at https://dashboard.tensorpad.com/signup and redeem the code ""REDDIT200"" in the Billing tab. (https://dashboard.tensorpad.com/billing)

Hope this explains our story and motivation for providing free credits.

Here is additional information:
* The instances have 16GB RAM, 4 CPUs cores and one 1080Ti GPU. You can run multiple instances in parallel
* You get access to the JupyterLab environment
* We have pre-installed Tensorflow, Keras, and other ML frameworks
* You can access the terminal through the JupyterLab
* By default, persistent storage is enabled
* TensorBoard comes pre-installed

Here are the available software versions https://docs.tensorpad.com/jobs_env/ For extra free trial hours, use promo code: reddit200

And for questions, please contact me at [ilie@tensorpad.com](mailto:ilie@tensorpad.com). I am looking forward to seeing you on our platform! Sincerely, Ilie Diacov Co-founder and UX researcher at Tensorpad",15,41,False,self,,,,,
78,deeplearning,t5_2t5eh,2019-4-9,2019,4,9,14,bb45xn,amazon.in,Generative Adversarial Networks Projects: Build next-generation generative models using TensorFlow and Keras,https://www.reddit.com/r/deeplearning/comments/bb45xn/generative_adversarial_networks_projects_build/,kailashahirwar12,1554788135,,0,0,False,default,,,,,
79,deeplearning,t5_2t5eh,2019-4-9,2019,4,9,14,bb47m4,self.deeplearning,MOOC for GANs.,https://www.reddit.com/r/deeplearning/comments/bb47m4/mooc_for_gans/,ragingpot,1554788519,Is there an MOOC which explains GANs in details or a similar resource? Wanna learn about them from scratch. Preferably free resource.,3,5,False,self,,,,,
80,deeplearning,t5_2t5eh,2019-4-9,2019,4,9,16,bb4v6c,self.deeplearning,Need to Increase Accuracy in SSD-Mobilenet-V1,https://www.reddit.com/r/deeplearning/comments/bb4v6c/need_to_increase_accuracy_in_ssdmobilenetv1/,8222Tamil,1554793767,"I want to deploy tf object detection api in videos.but when using Faster RCNN i get accuracy bt the inference time is too high ,so i changed to mobilenet v1,but has low [accuracy.how](https://accuracy.how) to fine tune SSD-mobilenet-V1 or how to develop the model from scratch?",9,1,False,self,,,,,
81,deeplearning,t5_2t5eh,2019-4-9,2019,4,9,18,bb5odi,self.deeplearning,How do you prepare your own datasets?,https://www.reddit.com/r/deeplearning/comments/bb5odi/how_do_you_prepare_your_own_datasets/,mehdital,1554800723,"Hi everybody,

I have been facing this problem that most implementations that can be found are made to work directly on some preexisting datasets (Pascal VOC, Coco, Cityscapes etc). I want to create my own dataset (for semantic segmentation purposes) and can't really find any good documentation on how these datasets are formed or how their labels are structured. Until now it has been pure guessing by looking at the data and the xml/json labels. How do you guys do that? Am I missing something somewhere?",0,1,False,self,,,,,
82,deeplearning,t5_2t5eh,2019-4-9,2019,4,9,22,bb7pje,self.deeplearning,Curated List of 3D Morphable Model Software and Data,https://www.reddit.com/r/deeplearning/comments/bb7pje/curated_list_of_3d_morphable_model_software_and/,sircalvin86,1554815144,[https://github.com/3d-morphable-models/curated-list-of-awesome-3D-Morphable-Model-software-and-data](https://github.com/3d-morphable-models/curated-list-of-awesome-3D-Morphable-Model-software-and-data),0,3,False,self,,,,,
83,deeplearning,t5_2t5eh,2019-4-9,2019,4,9,22,bb876i,self.deeplearning,Google Colab ram memory overflow,https://www.reddit.com/r/deeplearning/comments/bb876i/google_colab_ram_memory_overflow/,crazy_lazy_life,1554817967,What happens when I am training a model and the ram overflows. Does it allocate new memory while keeping the previous information in a buffer or is it initialised from the beginning.,5,4,False,self,,,,,
84,deeplearning,t5_2t5eh,2019-4-10,2019,4,10,13,bbhqs0,self.deeplearning,WIFI Dongle that works on Windows 10 and Ubuntu 18.04.2 dual boot?,https://www.reddit.com/r/deeplearning/comments/bbhqs0/wifi_dongle_that_works_on_windows_10_and_ubuntu/,steminist1,1554869904,"Hi! PC building and deep learning noob here.  My apologies in advance if I have left any important information out or if I should have posted this somewhere else. I am about at my wits end with trying to figure this out, so I was hoping some nice people here would have suggestions for me. 

&amp;#x200B;

I recently built a PC (for gaming and to delve into deep learning) running the i7 8700K with a Zotac Amp Extreme 1080TI on the ASUS ROG Strix Z370-F Gaming MOBO. I want to dual boot with Windows 10 (for gaming) and Ubuntu (for GPU accelerated DL). The MOBO does not have Bluetooth or WIFI, which I thought wouldn't be a problem since I can just purchase USB dongles. HA! Bluetooth adapter works great. WIFI..not at all. Joke has been on me for the last week trying to figure this out!

&amp;#x200B;

I wanted a higher-grade WIFI dongle for gaming purposes. The one I have that is working on both Windows 10 and Ubuntu 18.04.2 is only 150 Mbps, so I bought the TP Link 1900AC Archer T4UH adapter that says it has Linux support.  I have tried every driver I could find for the T4UH and worked through countless solutions in forums. One driver that I installed somehow managed to prevent WIFI from even working on the Windows side. Another driver resulted in chaos and I had to completely reinstall Ubuntu. 

&amp;#x200B;

I am at a total loss here. I have searched the internet and cannot find a dongle that people confirm works on both OS's that is above 150 Mbps. Is there anyone out there who is currently running Windows 10 and Ubuntu 18.04.2 dual-boot while using a WIFI adapter for gaming? If so, what adapter are you using? Or maybe I should just get a MOBO that has WIFI and Bluetootht? What about a PCIe WIFI card or will I Just run into the same issues? Does anyone have any other suggestions? I am really excited to start delving into deep learning...kinda sad that I cannot figure this out lol. 

&amp;#x200B;

Thanks all! :)",3,0,False,self,,,,,
85,deeplearning,t5_2t5eh,2019-4-10,2019,4,10,15,bbir18,self.deeplearning,Multi-server GPU Monitoring Program,https://www.reddit.com/r/deeplearning/comments/bbir18/multiserver_gpu_monitoring_program/,kairos9603,1554877698,"Hi, I'm introducing multi-server gpu monitoring tool.

it it simple to install and setting. Please, use and some comment to github.

Thank you.

Have a nice day!

&amp;#x200B;

pip install ksmi

&amp;#x200B;

[https://github.com/kairos03/kairos-smi](https://github.com/kairos03/kairos-smi)

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/2s4zx496sdr21.png",1,3,False,self,,,,,
86,deeplearning,t5_2t5eh,2019-4-10,2019,4,10,19,bbk9zj,opencodez.com,A Quick Easy Guide to Deep Learning with Java  Deeplearaning4j / DL4J - Free sample code to download,https://www.reddit.com/r/deeplearning/comments/bbk9zj/a_quick_easy_guide_to_deep_learning_with_java/,Shilpa_Opencodez,1554891035,,0,1,False,default,,,,,
87,deeplearning,t5_2t5eh,2019-4-10,2019,4,10,20,bbkq39,self.deeplearning,MRI image pre-processing,https://www.reddit.com/r/deeplearning/comments/bbkq39/mri_image_preprocessing/,GW_KIM,1554894566,"Hi there, i am building a classifier using MRI image(nii data) and i notice that it is important to apply pre-processing on it.

my datum are from ADNI and they are done in some pre-processing( Intensity correction, Scaling)

anyone who recommand  another pre-processing to enhance performance...?",1,1,False,self,,,,,
88,deeplearning,t5_2t5eh,2019-4-10,2019,4,10,21,bblobk,dragan.rocks,Deep Learning from Scratch to GPU: Initializing Weights,https://www.reddit.com/r/deeplearning/comments/bblobk/deep_learning_from_scratch_to_gpu_initializing/,dragandj,1554900905,,0,23,False,default,,,,,
89,deeplearning,t5_2t5eh,2019-4-10,2019,4,10,23,bbmx1e,self.deeplearning,Keras: How Should I Approach When a Sample Can Belong To Multiple Categories?,https://www.reddit.com/r/deeplearning/comments/bbmx1e/keras_how_should_i_approach_when_a_sample_can/,jl303,1554907861,"Let's say you're trying to analyze data with images that sometimes have only one object but sometimes have multiple objects on it. For example:

&amp;#x200B;

""person"", ""dog"", ""cat"", ""person,dog"", ""dog,cat""...

&amp;#x200B;

My understanding of multilabels is when you try to predict multiple dimensions: like type and color. ""shirts,red"", ""pants,black""

&amp;#x200B;

Is this still considered multilabels? If not, is there a term for this type of problem?

&amp;#x200B;

As far as preprocessing, should I use sklearn.preprocessing.MultiLabelBinarizer that produces an array for each sample indicating which categories each sample belongs to.

&amp;#x200B;

Could someone could give me some pointers on how I can aproach this?

&amp;#x200B;

Thanks!",4,2,False,self,,,,,
90,deeplearning,t5_2t5eh,2019-4-10,2019,4,10,23,bbmyzn,self.deeplearning,Beginner Deep Learning Project for Embedded Applications,https://www.reddit.com/r/deeplearning/comments/bbmyzn/beginner_deep_learning_project_for_embedded/,BalajiKulkarni,1554908151,"Hi All, 

I am an Embedded Software developer ,who have developed interest towards to deep learning in recent past. I have gained some good amount of fundamentals in CNN via Coursera and external meet up groups, with hands on experience at beginner level (implemented NN for MNIST dataset, ResNet and DenseNet in progress)

I want to develop an application (preferably vision related) on low foot print devices using pre trained models offline ,which will perform inference on embedded devices.

Please suggest or share any pointers ,if anyone here has some really interesting and useful things which could be done at beginner level for embedded systems.",0,1,False,self,,,,,
91,deeplearning,t5_2t5eh,2019-4-11,2019,4,11,0,bbn2sm,dev.to,"Bach to the Future (or, Humanising Music With Neural Nets)",https://www.reddit.com/r/deeplearning/comments/bbn2sm/bach_to_the_future_or_humanising_music_with/,point_against_point,1554908736,,0,2,False,default,,,,,
92,deeplearning,t5_2t5eh,2019-4-11,2019,4,11,0,bbn7ku,self.deeplearning,Resources for GANs?,https://www.reddit.com/r/deeplearning/comments/bbn7ku/resources_for_gans/,kesaroid,1554909439,"I needed a simple approach to learn GAN's. 
Any help would be appreciated.",1,7,False,self,,,,,
93,deeplearning,t5_2t5eh,2019-4-11,2019,4,11,0,bbnf2j,self.deeplearning,Generative models' likelihood estimation,https://www.reddit.com/r/deeplearning/comments/bbnf2j/generative_models_likelihood_estimation/,lil_uzi_kek,1554910501,"Hi everyone, I'm interested in generative models (e.g. GAN / VAE), but i never thought of ""how can we measure the likelihood of generated samples"". When I asked this question to myself and studied it a little, i've found a lot of approaches for VAE, but almost nothing (except [this](https://arxiv.org/pdf/1611.04273.pdf) paper) for GANs or other decoder-based models.

Can you give me any links to papers which focus on this question or any possible approaches? Thanks a lot!",0,1,False,self,,,,,
94,deeplearning,t5_2t5eh,2019-4-11,2019,4,11,3,bbpikl,i.redd.it,Small budget system for data scientists | I will be posting the building guide soon,https://www.reddit.com/r/deeplearning/comments/bbpikl/small_budget_system_for_data_scientists_i_will_be/,gimel1213,1554920935,,25,49,False,image,,,,,
95,deeplearning,t5_2t5eh,2019-4-11,2019,4,11,3,bbpn75,self.deeplearning,Does anyone remember colour gradient generating GANs?,https://www.reddit.com/r/deeplearning/comments/bbpn75/does_anyone_remember_colour_gradient_generating/,nivm321,1554921572,"Hi,

I had some time ago seen a GAN which produced colour gradients. I don't remember whose work it was. Do you happen to know of any? 

Thanks!",0,2,False,self,,,,,
96,deeplearning,t5_2t5eh,2019-4-11,2019,4,11,4,bbq5mi,medium.com,Creating a Custom OpenAI Gym Environment for Stock Trading,https://www.reddit.com/r/deeplearning/comments/bbq5mi/creating_a_custom_openai_gym_environment_for/,notadamking,1554924112,,0,8,False,default,,,,,
97,deeplearning,t5_2t5eh,2019-4-11,2019,4,11,4,bbqajc,t.me,DataScience Digest (Telegram channel),https://www.reddit.com/r/deeplearning/comments/bbqajc/datascience_digest_telegram_channel/,flyelephant,1554924804,,0,1,False,default,,,,,
98,deeplearning,t5_2t5eh,2019-4-11,2019,4,11,10,bbu4d2,self.deeplearning,i was wondering if is there any solution has both KNM(Intel Kights Mill) and NVIDIA GPUs in one workstation.,https://www.reddit.com/r/deeplearning/comments/bbu4d2/i_was_wondering_if_is_there_any_solution_has_both/,hawking90a,1554945503,I just want use these solutions (KNL/KNM and GPU) in one workstation.,0,1,False,self,,,,,
99,deeplearning,t5_2t5eh,2019-4-11,2019,4,11,13,bbw2g5,self.deeplearning,DLBT | User-friendly deep learning benchmark app | Anyone can test and benchmark their hardware GPU-CPU,https://www.reddit.com/r/deeplearning/comments/bbw2g5/dlbt_userfriendly_deep_learning_benchmark_app/,gimel1213,1554958429,"Hi Guys, we have developed a user-friendly app, easy to install for everyone to download and run the benchmark to know how their hardware perform for deep learning applications, you can also upload the results to our wall of fame and be on the Top 10. [\---- DOWNLOAD ----](https://www.technopremium.com)

[DLBT \(User-friendly deep learning app \) ](https://i.redd.it/ag436l5gekr21.png)

&amp;#x200B;

The app helps IT departments to identify a system of multiple GPU's, which one is damage in case of failure and help to purchase department to know what GPU workstation to buy depend on the GPU score in our web. 

&amp;#x200B;

We are updating more features every day and our goal is to make DLBT stand Benchmarking app for ML/DL datacenters and consumer use. 

&amp;#x200B;

We are happy to receive any feedback about, you can download ITS FREE !!!! [\---- DOWNLOAD ----](https://www.technopremium.com)

&amp;#x200B;

If need assistance installing our app after following the steps on the web, please contact our team, and we will be more than happy to help - [support@technopremium.com](mailto:support@technopremium.com)",1,2,False,self,,,,,
100,deeplearning,t5_2t5eh,2019-4-11,2019,4,11,17,bbxi8x,self.deeplearning,What is the meaning of optimizing input to the model?,https://www.reddit.com/r/deeplearning/comments/bbxi8x/what_is_the_meaning_of_optimizing_input_to_the/,sriharsha_0806,1554972289,"&amp;#x200B;

In the following video,  Ian good fellow explains at 16:00 ""The mapping from parameters of the network to the output of the network is non-Linear because of the weight matrices at each layer of network are multiplied together. So we get extremely nonlinear interactions between parameters and the output. That's what makes the training of Neural Network difficult. But the mapping from input to output is much more linear and predictable, and it means that optimization problems that aim to optimize the input to the model are much easier than optimization problems that aim to optimize the parameters .""  
What did he mean when saying optimizing the input to the model?  
[https://www.youtube.com/watch?v=CIfsB\_EYsVI&amp;list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&amp;index=16](https://www.youtube.com/watch?v=CIfsB_EYsVI&amp;list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&amp;index=16)",1,1,False,self,,,,,
101,deeplearning,t5_2t5eh,2019-4-11,2019,4,11,22,bbzqsi,self.deeplearning,Looking for datasets of Python code.,https://www.reddit.com/r/deeplearning/comments/bbzqsi/looking_for_datasets_of_python_code/,ZeroMaxinumXZ,1554988528,"Hi, don't know where to post this but...

&amp;#x200B;

I'm looking for a dataset of Python code (or any language) along with brief descriptions of the code to basically try and train a model to generate code.",4,1,False,self,,,,,
102,deeplearning,t5_2t5eh,2019-4-11,2019,4,11,22,bbzsa0,self.deeplearning,First Time PC Build for Deeplearning,https://www.reddit.com/r/deeplearning/comments/bbzsa0/first_time_pc_build_for_deeplearning/,HUSKODILE,1554988775,"Just put up a new build list for Deeplearning, first time build, any pointers?

I choose a 1600 watts PSU because ultimately i want to expand to 4 graphics card.

Not really sure if the case has heat issues with 4 GPUs running.

With the help of a warmhearted redditor @Really\_popular\_lobster, this is the new list, how do you guys like it?? Thanks in advance.

&amp;#x200B;

\[PCPartPicker part list\]([https://pcpartpicker.com/list/GtP7xG](https://pcpartpicker.com/list/GtP7xG)) / \[Price breakdown by merchant\]([https://pcpartpicker.com/list/GtP7xG/by\_merchant/](https://pcpartpicker.com/list/GtP7xG/by_merchant/))

&amp;#x200B;

&amp;#x200B;

\*\*CPU\*\* | \[AMD - Threadripper 1950X 3.4 GHz 16-Core Processor\]([https://pcpartpicker.com/product/CF7CmG/amd-threadripper-1950x-34ghz-16-core-processor-yd195xa8aewof](https://pcpartpicker.com/product/CF7CmG/amd-threadripper-1950x-34ghz-16-core-processor-yd195xa8aewof)) | $587.90 @ OutletPC

\*\*CPU Cooler\*\* | \[Noctua - NH-U14S TR4-SP3 82.52 CFM CPU Cooler\]([https://pcpartpicker.com/product/nCNypg/noctua-nh-u14s-tr4-sp3-1402-cfm-cpu-cooler-nh-u14s-tr4-sp3](https://pcpartpicker.com/product/nCNypg/noctua-nh-u14s-tr4-sp3-1402-cfm-cpu-cooler-nh-u14s-tr4-sp3)) | $79.90 @ Amazon

\*\*Motherboard\*\* | \[ASRock - X399 Taichi ATX TR4 Motherboard\]([https://pcpartpicker.com/product/kjmxFT/asrock-x399-taichi-atx-tr4-motherboard-x399-taichi](https://pcpartpicker.com/product/kjmxFT/asrock-x399-taichi-atx-tr4-motherboard-x399-taichi)) | $259.99 @ Newegg

\*\*Memory\*\* | \[G.Skill - Ripjaws V Series 64 GB (4 x 16 GB) DDR4-3200 Memory\]([https://pcpartpicker.com/product/7Xbkcf/gskill-memory-f43200c16q64gvk](https://pcpartpicker.com/product/7Xbkcf/gskill-memory-f43200c16q64gvk)) | $400.98 @ Newegg

\*\*Storage\*\* | \[Western Digital - Black NVMe 1 TB M.2-2280 Solid State Drive\]([https://pcpartpicker.com/product/2K22FT/western-digital-black-nvme-1tb-m2-2280-solid-state-drive-wds100t2x0c](https://pcpartpicker.com/product/2K22FT/western-digital-black-nvme-1tb-m2-2280-solid-state-drive-wds100t2x0c)) | $244.90 @ OutletPC

\*\*Video Card\*\* | \[Asus - GeForce RTX 2080 Ti 11 GB Turbo Video Card\]([https://pcpartpicker.com/product/vtqhP6/asus-geforce-rtx-2080-ti-11gb-turbo-video-card-turbo-rtx2080ti-11g](https://pcpartpicker.com/product/vtqhP6/asus-geforce-rtx-2080-ti-11gb-turbo-video-card-turbo-rtx2080ti-11g)) (2-Way SLI) | $1300.00

\*\*Video Card\*\* | \[Asus - GeForce RTX 2080 Ti 11 GB Turbo Video Card\]([https://pcpartpicker.com/product/vtqhP6/asus-geforce-rtx-2080-ti-11gb-turbo-video-card-turbo-rtx2080ti-11g](https://pcpartpicker.com/product/vtqhP6/asus-geforce-rtx-2080-ti-11gb-turbo-video-card-turbo-rtx2080ti-11g)) (2-Way SLI) | $1300.00

\*\*Case\*\* | \[Corsair - 750D Airflow Edition ATX Full Tower Case\]([https://pcpartpicker.com/product/Rwhj4D/corsair-case-cc9011078ww](https://pcpartpicker.com/product/Rwhj4D/corsair-case-cc9011078ww)) | $119.99 @ Newegg

\*\*Power Supply\*\* | \[Corsair - 1600 W 80+ Titanium Certified Fully-Modular ATX Power Supply\]([https://pcpartpicker.com/product/cJbwrH/corsair-1600w-80-titanium-certified-fully-modular-atx-power-supply-cp-9020087-na](https://pcpartpicker.com/product/cJbwrH/corsair-1600w-80-titanium-certified-fully-modular-atx-power-supply-cp-9020087-na)) | $339.99 @ Amazon

| \*Prices include shipping, taxes, rebates, and discounts\* |

| Total (before mail-in rebates) | $4673.65

| Mail-in rebates | -$40.00

| \*\*Total\*\* | \*\*$4633.65\*\*",6,4,False,self,,,,,
103,deeplearning,t5_2t5eh,2019-4-11,2019,4,11,23,bc0qwo,medium.com,A Google Brain Program Is Learning How to Program,https://www.reddit.com/r/deeplearning/comments/bc0qwo/a_google_brain_program_is_learning_how_to_program/,gwen0927,1554994082,,10,40,False,default,,,,,
104,deeplearning,t5_2t5eh,2019-4-12,2019,4,12,0,bc11jc,opendatascience.com,Deep Learning for Business: 5 Use Cases,https://www.reddit.com/r/deeplearning/comments/bc11jc/deep_learning_for_business_5_use_cases/,OpenDataSciCon,1554995609,,0,1,False,default,,,,,
105,deeplearning,t5_2t5eh,2019-4-12,2019,4,12,0,bc13dj,theverge.com,A glitch in the LSTM?,https://www.reddit.com/r/deeplearning/comments/bc13dj/a_glitch_in_the_lstm/,polyglotdev,1554995878,,0,1,False,default,,,,,
106,deeplearning,t5_2t5eh,2019-4-12,2019,4,12,2,bc2lo4,medium.com,TableBank: Benchmark for Image-based Table Detection and Recognition,https://www.reddit.com/r/deeplearning/comments/bc2lo4/tablebank_benchmark_for_imagebased_table/,gwen0927,1555003524,,0,1,False,default,,,,,
107,deeplearning,t5_2t5eh,2019-4-12,2019,4,12,5,bc53a8,medium.com,NAACL 2019 List of Best Papers,https://www.reddit.com/r/deeplearning/comments/bc53a8/naacl_2019_list_of_best_papers/,gwen0927,1555016338,,0,2,False,default,,,,,
108,deeplearning,t5_2t5eh,2019-4-12,2019,4,12,7,bc5vv4,self.deeplearning,Neural ODE applications.,https://www.reddit.com/r/deeplearning/comments/bc5vv4/neural_ode_applications/,ragingpot,1555020551,"Has anyone here applied the Neural ODE concept to their own dataset? If so, can you share the results. The dataset must not be a standard one like MNIST, CIFAR etc.",0,3,False,self,,,,,
109,deeplearning,t5_2t5eh,2019-4-12,2019,4,12,7,bc6b7l,medium.com,Fusing Data Transformations with Neural Network for Fast Inference with MXNet,https://www.reddit.com/r/deeplearning/comments/bc6b7l/fusing_data_transformations_with_neural_network/,gigasquid,1555022927,,0,1,False,default,,,,,
110,deeplearning,t5_2t5eh,2019-4-12,2019,4,12,8,bc6z9t,self.deeplearning,"[Question] Tensorflow object detection retraining, confidence score really low",https://www.reddit.com/r/deeplearning/comments/bc6z9t/question_tensorflow_object_detection_retraining/,naboo_random,1555026985,"Hi,

I am trying to retrain the tensorflow object detection model - faster r cnn ( pretrained on iNaturalist) data. However, after training for around 60k steps ( where the total loss stabilizes), if I run an evaluation the confidence scores of my detections are too low, of the order of e-5.

The original model has around 2k classes. I'm training on 4 classes - almost equal distribution, with around 900 samples. The data augmentation option is ""random\_horizontal\_flip"".

Should I be increasing the number of samples I have? Maybe have more augmentation options?

Any help will be appreciated! Thanks!",0,0,False,self,,,,,
111,deeplearning,t5_2t5eh,2019-4-12,2019,4,12,14,bc9skp,self.deeplearning,Does PyTorch or TensorFlow actually use tensors?,https://www.reddit.com/r/deeplearning/comments/bc9skp/does_pytorch_or_tensorflow_actually_use_tensors/,justinecarolin,1555045666,"I don't have a degree in physics nor one in mathematics, 
so I don't know much about tensors.

And I have just started with deep learning.

But as far as I can tell, after reading through the docs, my impression is that PyTorch actually implements good-and-old matrix-and-vector linear algebra, and in addition,

1 names n-d arrays as tensors, which is correct mathematically

2 has some elementary operations on n-d arrays, which demand no knowledge of (mathematical) tensors and may not necessarily be a part of tensor algebra , such as torch.cat(), torch.chunk(), etc.

It occurs to me that tensor algebra is not actually implemented. The closest thing I have found is torch.tensordot() which I am not sure what tensor algebra operation(s) it corresponds to.

Am I missing something? Or is it just PyTorch, while TensorFlow implements proper tensor algebra? Or are deep learning frameworks all like that?
Do I really need to understand tensors other than that they are n-d arrays to do research in deep learning?",19,15,False,self,,,,,
112,deeplearning,t5_2t5eh,2019-4-12,2019,4,12,14,bca3d7,arxiv.org,Diagnosis of Celiac Disease and Environmental Enteropathy on Biopsy Images Using Color Balancing on Convolutional Neural Networks,https://www.reddit.com/r/deeplearning/comments/bca3d7/diagnosis_of_celiac_disease_and_environmental/,kk7nc,1555048106,,0,2,False,default,,,,,
113,deeplearning,t5_2t5eh,2019-4-12,2019,4,12,19,bcc68f,self.deeplearning,"Deeplearning in OpenCV, retraining network",https://www.reddit.com/r/deeplearning/comments/bcc68f/deeplearning_in_opencv_retraining_network/,tobix10,1555066609,"I am making a video surveillance app and I need to detect people. I don't need to detect other stuff that is available in yolov3 (80 classes on COCO dataset). So I think about retraining it, but I am new to the field and really confused about a few things.

First of all, I have quick questions:
1. What is the difference between yolov3 and yolov3-tiny? Accuracy for sure. Smaller training set? Smaller network?
2. Is it reasonable to assume that single class network will have much greater accuracy, will be smaller and faster?
3. Is yolo the fastest out there? Should I use other network for my use case? Remark: my target device has ARM CPU and yolov3 is too slow. yolov3-tiny inference time is acceptable.

Questions related to training:
1. Which dataset should I use: Pascal, COCO, OpenImages?
2. Should I use original dataset or a smaler subset, but with augmented images?
3. OpenImages has 220k real person images. That is a lot. How long may it take to train network?
4. Can I train a network in reasonable time on quad-core CPU or older card GTX770 (2GB only)?

I've also found a repository https://github.com/thatbrguy/Pedestrian-Detection/blob/master/object_detection/g3doc/detection_model_zoo.md, but I can't make models run in OpenCV and from the description those networks don't output confidence values.",0,1,False,self,,,,,
114,deeplearning,t5_2t5eh,2019-4-12,2019,4,12,20,bcc7az,self.deeplearning,2080ti slower than 2080,https://www.reddit.com/r/deeplearning/comments/bcc7az/2080ti_slower_than_2080/,Den3b_poe,1555066815,"Hello, I'm training an LSTM (2 LSTM layers + 1 fully-connected layer) with Keras (with Tensorflow backend) on two different machines using a single GPU per machine. Surprisingly I obtain slower training on the GPU that in principle should perform better.

There are a lot of discussions about training speed for GPU vs. CPU, or for single vs multi-GPU. However, I wasn't able to find a discussion comparing single GPU vs. single GPU.

The data, the code, the libraries (including versions and releases), the random seed, and the Nvidia driver are exactly the same on the two machines.

First machine: laptop with GeForce RTX 2080 mobile version  
Second machine: workstation with GeForce RTX 2080 Ti

Given these two configurations, I'm expecting a faster training on the second machine: not only the GeForce RTX 2080 Ti is better than GeForce RTX 2080, but also the latter is the laptop version.

Using a batch size of 32 I get the following performances:

* first machine: 27 s per epoch - 38 ms per batch - 1st epoch train loss: 0.8772 - 1st epoch val\_loss: 0.1873
* second machine: 52 s per epoch - 72 ms per batch - 1st epoch train loss: 0.8772 - 1st epoch val\_loss: 0.1873

As you can see the losses are identical, however the RTX 2080 Ti is almost two times slower. I can't understand why this is happening, since in my opinion the RTX 2080 Ti should be faster than the RTX 2080 mobile.  


Any suggestion?",3,1,False,self,,,,,
115,deeplearning,t5_2t5eh,2019-4-12,2019,4,12,21,bccypm,self.deeplearning,Image augmentation reduced my model's accuracy,https://www.reddit.com/r/deeplearning/comments/bccypm/image_augmentation_reduced_my_models_accuracy/,huzaifakhan771,1555072019,"I trained a model for classifying male and female gender based on their faces. I used 250 images (face only) for each class (male and female) and got an accuracy of about 60 percent (it was incorrect in 8 of the test images). I then augmentee my dataset by apply geometric (rotation, flip) and got a dataset 5 times as big and used it to train the model with it, including the original images. The model then gave me a reduced accuracy if 50 percent. Why did that happen? I though image augmentation can increase accuracy by expanding the dataset. Thank you.",3,1,False,self,,,,,
116,deeplearning,t5_2t5eh,2019-4-12,2019,4,12,22,bcdb8x,youtu.be,Donald Trump AI Demonstrates New Facial Manipulation Project ICface,https://www.reddit.com/r/deeplearning/comments/bcdb8x/donald_trump_ai_demonstrates_new_facial/,hanyuqn,1555074195,,0,2,False,image,,,,,
117,deeplearning,t5_2t5eh,2019-4-12,2019,4,12,22,bcde9h,self.deeplearning,Udacity Deep Learning Course,https://www.reddit.com/r/deeplearning/comments/bcde9h/udacity_deep_learning_course/,clintdk,1555074683,"Hi everyone! I have currently finished my deep learning course in my data science master but I still feel like quite some knowledge is missing. I have heard good things about the Udacity deep learning course, but I would like to hear you guys opinion! Do you think it has some added value for me? 

&amp;#x200B;

Here is my course description from the university so you get an insight about my foreknowledge: [https://catalogus.tilburguniversity.edu/osiris\_student\_tiuprd/OnderwijsCatalogusSelect.do?selectie=cursus&amp;taal=en&amp;collegejaar=2018&amp;cursus=880008-M-6](https://catalogus.tilburguniversity.edu/osiris_student_tiuprd/OnderwijsCatalogusSelect.do?selectie=cursus&amp;taal=en&amp;collegejaar=2018&amp;cursus=880008-M-6)   


This is what I learned in machine learning: [https://catalogus.tilburguniversity.edu/osiris\_student\_tiuprd/OnderwijsCatalogusSelect.do?selectie=cursus&amp;taal=en&amp;collegejaar=2018&amp;cursus=880083-M-6](https://catalogus.tilburguniversity.edu/osiris_student_tiuprd/OnderwijsCatalogusSelect.do?selectie=cursus&amp;taal=en&amp;collegejaar=2018&amp;cursus=880083-M-6)

&amp;#x200B;

Thanks in advance you guys!",7,22,False,self,,,,,
118,deeplearning,t5_2t5eh,2019-4-12,2019,4,12,22,bcdegs,self.deeplearning,GanSeg for medical images,https://www.reddit.com/r/deeplearning/comments/bcdegs/ganseg_for_medical_images/,DifficultDifficulty,1555074720,"Hi all,

At my university, I am taking a grad course in which we're select a deep learning project and work on it by the end of the course.  
I had previously developed a set of tools for my [M.Sc](https://M.Sc) project where I need to manipulate sets of medical images in different formats (DICOM, Niftii, Nrrd), pre-process them using SimpleITK and feed them into a deep learning pipeline.  
I figured I could execute that course project using those tools and put it up on github for everyone to use and for me to showcase my skills, as I will finish school very soon and that will give me some visibility for my upcoming job search.

The platform relies on visdom for visualization capabilities, I had implemented functions that allow you to visualize your network's computation graph, the experiment options and hyperparameters, your loss and accuracy curves, the gradient flow graphs through your networks as well as histograms for weight distribution in your layers. The python classes offer different functions to easily extend functionality where you can sample images during training if you're working with images and display them on the browser.

The repository comes with implementations of UNets and ResNets as well as many GAN loss functions such as Wasserstein GAN with gradient penalty. The same class can also be set in non-GAN mode so that it uses a simple cross-entropy loss function.

I had used this code to segment vertebrae from MRIs, reaching a 0.87 dice coefficient.

The code is generic enough to be used for tasks other than image segmentation if you wish to play around with it.

I hope this will be helpful to you.

Please find it in the link down below:

[https://github.com/Roulbac/GanSeg](https://github.com/Roulbac/GanSeg)",0,1,False,self,,,,,
119,deeplearning,t5_2t5eh,2019-4-13,2019,4,13,0,bcexek,self.deeplearning,Questions About 3D Model Deep Learning Project,https://www.reddit.com/r/deeplearning/comments/bcexek/questions_about_3d_model_deep_learning_project/,_AICurious_,1555083198,"# Background:

I have recently undertaken a project attempting to localize the position of a computer-aided design (CAD) object in 3D space by manipulating the projection of the 3D CAD object onto two orthogonal plane projections of the 3D. Please see image below.

https://i.redd.it/xe38gklwgur21.png

As you can see, the location of the 3D object in the global coordinate system can be determined by shifting the projections of the 3D object on the 2D images to have the silhouette match the previously determined outline.

Previously, this problem has been handled manually by shifting the the 3D object around until its silhouette overlaps with the images. This is a time-consuming process, and with thousands of images, it can quickly become overwhelming.

I proposed that we could utilize deep learning to automate this process, as we have many instances with corresponding solutions, and additionally, it's a repetitive process.

For clarity, the input data contains two 1024x1024 gray-scale images and a variable size .stl file, describing the 3D object. The output data contains a 3x3 rotational matrix (see [here](https://en.wikipedia.org/wiki/Rotation_formalisms_in_three_dimensions) for more details), describing the rotation of the object's unit vectors in the global coordinate system and a 1x3 translation matrix describing the movement of the coordinate system's origin.

The .stl file will typically include two matrices. One matrix describes the vertices of the model, and the other matrix describes indexes of three vertices to connect to form a surface (see [here](https://s3-eu-west-1.amazonaws.com/3dhubs-knowledgebase/how-export-and-3d-print-your-stl-file-right-resolution/visual4.png) for an example).

# Proposed Solution:

In terms of the actual deep learning network, I propose using convolutional and pooling layers to analyze the two images. This will help scale down the data and will hopefully identify features that are relevant to the silhouette of the 3D model. For the 3D model, I was considering using `tensorflow.layers.conv3d()` to create a convolution layer for the 3D inputs, but I am unsure of how to incorporate the matrix with information about what vertices to connect, which is responsible for creating the surfaces. The plan currently is to create a few fully connected layers at the top of the neural network where the information from the 2 images, the 3D model points, and the 3D model connection surfaces is incorporated to produce the desired output. The desired output will most likely be a 1x12 vector (1x9 for rotation matrix concatenated with 1x3 for translation matrix). 

# Concerns:

1) How would you handle the variable input length sequences? I understand that recurrent neural networks (RNNs) can handle variable length input sequences, but my understanding is that this is primarily used in natural language processing and time series data. Is it appropriate to use RNNs in this setting? I could also zero-pad my data to create uniform data lengths, but the problem is that the .stl files have between \~50,000 and 75,000 vertices and between \~110,000-130,000 connections.

2) How would I analyze the 3D model? Is there a way to incorporate the surfaces early on in the 3D convolutional layer? Although unorthodox, do you think that the surface matrix (containing connections between the vertices) could also be analyzed by a 3D convolutional layer, as it has 3 columns of data?

3) Do you think it's practical to have the fully connected layer as described above to combine the outputs from the various convolutional layers (including CNN output for two separate images, one 3D point cloud, and the surface matrix)? Are there other practical ways to combine the data to optimize training?

&amp;#x200B;

Any help, guidance, or advice would be greatly appreciated. Thanks for reading this far!

&amp;#x200B;

Language: Python3.6 

DL Package: Keras on top of TensorFlow",0,0,False,self,,,,,
120,deeplearning,t5_2t5eh,2019-4-13,2019,4,13,3,bcgo47,self.deeplearning,Thanks to everyone who attended the @missinglinkai #Meetup last night. We had a great turnout! Thanks to our guest speaker @drsrinathsridha &amp; Stanford. Big thanks to @WeWork for hosting us! Watch the recording here: youtu.be/hxGNAREbXmY #DeepLearning,https://www.reddit.com/r/deeplearning/comments/bcgo47/thanks_to_everyone_who_attended_the_missinglinkai/,treguess,1555092041,,0,1,False,self,,,,,
121,deeplearning,t5_2t5eh,2019-4-13,2019,4,13,3,bcgtnt,self.deeplearning,Stanford presents: Deep Learning for Digitizing Human Physical Skills,https://www.reddit.com/r/deeplearning/comments/bcgtnt/stanford_presents_deep_learning_for_digitizing/,treguess,1555092793,https://www.youtube.com/watch?v=hxGNAREbXmY&amp;feature=share[Deep Learning for Digitizing Human Physical Skils](https://www.youtube.com/watch?v=hxGNAREbXmY&amp;feature=share),1,1,False,self,,,,,
122,deeplearning,t5_2t5eh,2019-4-13,2019,4,13,6,bcj2ca,self.deeplearning,State-of-the-art multilabel classification algorithm?,https://www.reddit.com/r/deeplearning/comments/bcj2ca/stateoftheart_multilabel_classification_algorithm/,peroquepas923,1555104421,"Hi redditors, I have a database composed of 1000 training samples that have 800 features each in a vectorized form. Each of these samples can belong to 14 different classes at the same; for instance, the label of some sample vector x can be expressed in the following one-hot-coding way:

label\_x = \[1,0,0,0,1,1,0,0,1,0,0,0,0,0\]

Could you recommend me a good state-of-the-art algorithm capable of resolving this kind of problem? I am thinking in using neural networks but since I don't have a lot of training samples it may be hard to obtain a good generalizing model. Another option I've thought of is random forests. However, since I have never worked in multilabel-classification I am a bit lost.

Regards!",3,1,False,self,,,,,
123,deeplearning,t5_2t5eh,2019-4-13,2019,4,13,7,bcjjzf,self.deeplearning,Deep Learning PhD,https://www.reddit.com/r/deeplearning/comments/bcjjzf/deep_learning_phd/,NikolasTs,1555107151,"Does anybody know any interesting PhD positions in Europe for deep learning? Any recommendation on possible professors as supervisors? 

Thanks in advance!",4,16,False,self,,,,,
124,deeplearning,t5_2t5eh,2019-4-13,2019,4,13,8,bckdwg,blog.paperspace.com,Detecting and Localizing Pneumonia from Chest X-Ray Scans with PyTorch,https://www.reddit.com/r/deeplearning/comments/bckdwg/detecting_and_localizing_pneumonia_from_chest/,dkobran,1555112037,,0,3,False,default,,,,,
125,deeplearning,t5_2t5eh,2019-4-13,2019,4,13,18,bcosxe,self.deeplearning,Usage of fast.ai library in Google Colab,https://www.reddit.com/r/deeplearning/comments/bcosxe/usage_of_fastai_library_in_google_colab/,visvats,1555147531,How to install fast.ai library in Google Colab?,2,1,False,self,,,,,
126,deeplearning,t5_2t5eh,2019-4-13,2019,4,13,20,bcpegq,self.deeplearning,Looking for a Good Seq2Seq Chatbot Library,https://www.reddit.com/r/deeplearning/comments/bcpegq/looking_for_a_good_seq2seq_chatbot_library/,ZeroMaxinumXZ,1555153374,Is there any good seq2seq chatbot libraries for Python (any ML backend) that allow for plain-text preprocessing? Don't really want to have to build my own seq2seq chatbot from complete scratch... Thanks...,2,7,False,self,,,,,
127,deeplearning,t5_2t5eh,2019-4-14,2019,4,14,4,bctx11,youtube.com,Katie Bauman shows how the black hole imaging reduces to a computer vision problem,https://www.reddit.com/r/deeplearning/comments/bctx11/katie_bauman_shows_how_the_black_hole_imaging/,treguess,1555182129,,0,56,False,default,,,,,
128,deeplearning,t5_2t5eh,2019-4-14,2019,4,14,5,bcuvl5,i.imgur.com,Which feature detection is used here?,https://www.reddit.com/r/deeplearning/comments/bcuvl5/which_feature_detection_is_used_here/,treguess,1555187382,,0,69,False,default,,,,,
129,deeplearning,t5_2t5eh,2019-4-14,2019,4,14,13,bcz7s6,self.deeplearning,How could this happen?,https://www.reddit.com/r/deeplearning/comments/bcz7s6/how_could_this_happen/,hungrybear2005,1555216113,"I'm developing some cuda algorithm. For fun, i tested my laptop cpus performance just now. 16384*16384 matrix addition, 3840qm takes 8 seconds while 7700hq takes 38.5seconds. Former one compile&amp;link on vs2013+win7 while later vs2017+win10. Can't believe that 7700hq was so slow. My 3840qm is six years old!",3,1,False,self,,,,,
130,deeplearning,t5_2t5eh,2019-4-14,2019,4,14,13,bczea7,self.deeplearning,"Can I do Machine learning, Deep learning research if I dont have GPU?",https://www.reddit.com/r/deeplearning/comments/bczea7/can_i_do_machine_learning_deep_learning_research/,manishghimire,1555217620,,3,0,False,self,,,,,
131,deeplearning,t5_2t5eh,2019-4-14,2019,4,14,14,bczk3e,medium.com,Humans Call GG! OpenAI Five Bots Beat Top Pros OG in Dota 2,https://www.reddit.com/r/deeplearning/comments/bczk3e/humans_call_gg_openai_five_bots_beat_top_pros_og/,gwen0927,1555219017,,0,7,False,default,,,,,
132,deeplearning,t5_2t5eh,2019-4-14,2019,4,14,16,bd0hb7,self.deeplearning,How to view a HDFS file,https://www.reddit.com/r/deeplearning/comments/bd0hb7/how_to_view_a_hdfs_file/,Pik000,1555227523,"Hi Guys,

&amp;#x200B;

I want to use the fashion-gen dataset for a project I'm working on, the dataset is in the HDFS file format, there are pictures and text etc inside the file but I can't workout how to access anything. Can someone point me in the correct direction to use the file? Sorry if this is a basic question but I seem to have spent alot of the time day googling and still can figure it out. Would like to get the text into pandas if possible.",1,1,False,self,,,,,
133,deeplearning,t5_2t5eh,2019-4-14,2019,4,14,16,bd0ifu,self.deeplearning,What the SPP block does in yolov3-spp?,https://www.reddit.com/r/deeplearning/comments/bd0ifu/what_the_spp_block_does_in_yolov3spp/,drr21,1555227841,"Hi, I'm currently working with yolo with a dataset with small objects. It seems that yolov3-spp ([https://github.com/AlexeyAB/darknet/blob/master/cfg/yolov3-spp.cfg](https://github.com/AlexeyAB/darknet/blob/master/cfg/yolov3-spp.cfg)) works better than normal yolo when small objects are present. However, I don't understand why? How the spp block makes yolov3 more suitable for small objects? Any idea?

&amp;#x200B;

Thanks!",0,3,False,self,,,,,
134,deeplearning,t5_2t5eh,2019-4-14,2019,4,14,16,bd0j5m,youtube.com,Convolutional Neural Networks: An Intuitive Approach,https://www.reddit.com/r/deeplearning/comments/bd0j5m/convolutional_neural_networks_an_intuitive/,DiscoverAI,1555228042,,0,2,False,image,,,,,
135,deeplearning,t5_2t5eh,2019-4-14,2019,4,14,22,bd2qov,self.deeplearning,C++ in deep leaning!Help me get trough this!:),https://www.reddit.com/r/deeplearning/comments/bd2qov/c_in_deep_leaninghelp_me_get_trough_this/,Zi6st,1555248530," 

Hi im a new to the deep learning world but im really excited to learn and mabye specialize in this field!

At the moment I know only c++ at a pretty good level .I got a financial aid from Coursera about deep learning ,that was taught in python which i know at a basic level , and i hate it. So i read some articles about c++ in machine learning and in deep learning and i found out that many people use it and actually 90% of the frameworks are coded in c++.So my question is how do i start this adventure into deep learning using c++,what framework does c++ support,is it better to go with ubuntu (atm using windows 10) ,does someone have any resources that they found useful (courses ,articles or blogs).I have to mention that i do have a Nvidia gpu with CUDA capability .",0,0,False,self,,,,,
136,deeplearning,t5_2t5eh,2019-4-15,2019,4,15,2,bd5dzs,blog.nanonets.com,A 2019 guide to Human Pose Estimation with Deep Learning,https://www.reddit.com/r/deeplearning/comments/bd5dzs/a_2019_guide_to_human_pose_estimation_with_deep/,manneshiva,1555264290,,1,21,False,default,,,,,
137,deeplearning,t5_2t5eh,2019-4-15,2019,4,15,5,bd6yqc,self.deeplearning,How to create a pass a keras variable to keras Model as output in Tensorflow 2.0?,https://www.reddit.com/r/deeplearning/comments/bd6yqc/how_to_create_a_pass_a_keras_variable_to_keras/,nobodywillobserve,1555272525,"For example


```python

V = K.variable(V0, name='V', dtype=tf.float32)                                                       

model = keras.models.Model(inputs=a\_input, outputs=\[q, V\])       

```

Doesn't work. Gives 

AttributeError: Tensor.op is meaningless when eager execution is enabled.

Is the only way to create variables in layers?",0,1,False,self,,,,,
138,deeplearning,t5_2t5eh,2019-4-15,2019,4,15,5,bd6zk8,gfycat.com,Robot solves a Rubiks cube in a fraction of a second,https://www.reddit.com/r/deeplearning/comments/bd6zk8/robot_solves_a_rubiks_cube_in_a_fraction_of_a/,treguess,1555272661,,1,2,False,default,,,,,
139,deeplearning,t5_2t5eh,2019-4-15,2019,4,15,5,bd71ww,self.deeplearning,Reducing hdf5 size: VGG16 is over 500MB?,https://www.reddit.com/r/deeplearning/comments/bd71ww/reducing_hdf5_size_vgg16_is_over_500mb/,_sleepyotter,1555273025,"I have an architecture that is using VGG as the base network and the hdf5 file is HUGE. It's roughly 500 MB! I'm using keras and saving the weights by using `model.save_weights('weights.hdf5')`

&amp;#x200B;

From some quick google searches, it seems like the size of the hdf5 is not uncommon: 

* [https://forums.fast.ai/t/why-are-the-hierarchical-data-h5-format-files-so-large/469](https://forums.fast.ai/t/why-are-the-hierarchical-data-h5-format-files-so-large/469)
* [https://github.com/jcjohnson/fast-neural-style/issues/47](https://github.com/jcjohnson/fast-neural-style/issues/47)

&amp;#x200B;

Does anyone know of some ways to reduce the size of these hdf5 files? I'm trying to run it on a server for a web application. 

&amp;#x200B;

Thanks!",2,1,False,self,,,,,
140,deeplearning,t5_2t5eh,2019-4-15,2019,4,15,5,bd75vo,youtube.com,1 How Convolutional Neural Networks Work: An Intuitive Approach,https://www.reddit.com/r/deeplearning/comments/bd75vo/1_how_convolutional_neural_networks_work_an/,DiscoverAI,1555273641,,0,0,False,image,,,,,
141,deeplearning,t5_2t5eh,2019-4-15,2019,4,15,7,bd8bxk,self.deeplearning,What should the error distribution be across the sample of the KL or cross-entropy of distributions?,https://www.reddit.com/r/deeplearning/comments/bd8bxk/what_should_the_error_distribution_be_across_the/,nobodywillobserve,1555279925,"The title \*is\* confusing.

&amp;#x200B;

Suppose you predict a \*distribution\* y, and your error metric is the per-sample KL or cross-entropy: KL(yp\_i, y\_i) or something like that.

&amp;#x200B;

I see people using the mean cross-entropy but I don't see why that is justified. Sampling it shows that it could tend toward lognormal in some cases but not all. Is there a result published somewhere that I am missing about this?",0,2,False,self,,,,,
142,deeplearning,t5_2t5eh,2019-4-15,2019,4,15,7,bd8tbz,self.deeplearning,How does one dynamically add new parameters to optimizers in Pytorch?,https://www.reddit.com/r/deeplearning/comments/bd8tbz/how_does_one_dynamically_add_new_parameters_to/,brandojazz,1555282633,"[https://discuss.pytorch.org/t/dynamically-add-parameters-to-optimizer/11537](https://discuss.pytorch.org/t/dynamically-add-parameters-to-optimizer/11537)

[https://stackoverflow.com/questions/55640836/how-does-one-dynamically-add-new-parameters-to-optimizers-in-pytorch](https://stackoverflow.com/questions/55640836/how-does-one-dynamically-add-new-parameters-to-optimizers-in-pytorch)",0,1,False,self,,,,,
143,deeplearning,t5_2t5eh,2019-4-15,2019,4,15,8,bd8yr1,self.deeplearning,A Rant,https://www.reddit.com/r/deeplearning/comments/bd8yr1/a_rant/,patronus816,1555283558,"Im not really sure if some of you guys may feel the same way but im beginning to think of deep learning as a mechanical process wherein for a project, 

you would make an NN -&gt; CNN or RNN depending on purpose -&gt; look up papers about which is the best one  -&gt; select it and then feed data -&gt; hope it gets high on accuracy or something of the sort -&gt; and then youre finished!

I was wondering what keeps you going or am i wrong to think of it this way? 
I loved Machine Learning of Andrew Ng because it involved multiple paradigms/models but Im struggling to finish the DeepLearning.ai course because i may have lost my passion due to my way of thinking... (p.s. am already on computer vision which is course three or four of the curriculum.)",9,14,False,self,,,,,
144,deeplearning,t5_2t5eh,2019-4-15,2019,4,15,10,bdaa12,self.deeplearning,Accuracy and Sparse Categorical Cross Entropy decreases,https://www.reddit.com/r/deeplearning/comments/bdaa12/accuracy_and_sparse_categorical_cross_entropy/,rlamarr,1555291734,"Hi guys,
I've been trying to solve this for a few weeks now.
I'm training an Audio classification model which uses float32 40 log-mel filter banks as input to the model which I normalized using min-max normalization.

During training, the validation accuracy and training accuracy increase until they reach a peak of 0.15 and start to decrease and never increase.
I've tried removing BatchNorm and Dropout, and even reducing learning rate to no avail. Though the cross entropy loss keeps on reducing for the train and validation set continously.

I've tried this for a CNN and an LSTM based model but still get same results. 

Please, I really need this.",3,1,False,self,,,,,
145,deeplearning,t5_2t5eh,2019-4-15,2019,4,15,10,bdac9v,self.deeplearning,How to feed the output back to the input in LSTM?,https://www.reddit.com/r/deeplearning/comments/bdac9v/how_to_feed_the_output_back_to_the_input_in_lstm/,Rinzler187,1555292121,"input1 -(LSTM)-&gt; output1 
output1 -(LSTM) -&gt; output2 
output2 - (LSTM) -&gt; output3",0,0,False,self,,,,,
146,deeplearning,t5_2t5eh,2019-4-15,2019,4,15,11,bdauuy,youtube.com,Developing a Robust Face Generative Adversarial Network with Tensorflow,https://www.reddit.com/r/deeplearning/comments/bdauuy/developing_a_robust_face_generative_adversarial/,DiscoverAI,1555295235,,0,0,False,image,,,,,
147,deeplearning,t5_2t5eh,2019-4-15,2019,4,15,11,bdb5fu,youtube.com,How Neural Networks Work: Simply Explained,https://www.reddit.com/r/deeplearning/comments/bdb5fu/how_neural_networks_work_simply_explained/,DiscoverAI,1555296862,,0,0,False,image,,,,,
148,deeplearning,t5_2t5eh,2019-4-15,2019,4,15,13,bdc3h5,self.deeplearning,Optimisation theory,https://www.reddit.com/r/deeplearning/comments/bdc3h5/optimisation_theory/,Yonkou94,1555303534,Can someone please suggest me an introductory level course/book on Optimisation Theory?,6,14,False,self,,,,,
149,deeplearning,t5_2t5eh,2019-4-15,2019,4,15,16,bdd7j9,youtube.com,Recurrent Neural Networks: Algorithms and Applications,https://www.reddit.com/r/deeplearning/comments/bdd7j9/recurrent_neural_networks_algorithms_and/,DiscoverAI,1555313178,,0,13,False,image,,,,,
150,deeplearning,t5_2t5eh,2019-4-15,2019,4,15,18,bddv3c,self.deeplearning,Music Generation with multiple instruments.,https://www.reddit.com/r/deeplearning/comments/bddv3c/music_generation_with_multiple_instruments/,ApenguinElement,1555319557,"I was trying to generate piano music by using LSTM model, now I want to add another instrument by modifying the model. How should I do it? How should I design the model to train on different instruments? Any help would be great.",3,2,False,self,,,,,
151,deeplearning,t5_2t5eh,2019-4-15,2019,4,15,19,bdegbk,datasciencedigest.org,DataScience Digest - Issue #16,https://www.reddit.com/r/deeplearning/comments/bdegbk/datascience_digest_issue_16/,flyelephant,1555324513,,0,1,False,default,,,,,
152,deeplearning,t5_2t5eh,2019-4-15,2019,4,15,21,bdf5ak,dragan.rocks,Deep Learning from Scratch to GPU: Learning a Regression,https://www.reddit.com/r/deeplearning/comments/bdf5ak/deep_learning_from_scratch_to_gpu_learning_a/,dragandj,1555329781,,0,1,False,default,,,,,
153,deeplearning,t5_2t5eh,2019-4-15,2019,4,15,21,bdfdwq,habr.com,Identifying dog breed with neural networks: from Keras to Android app,https://www.reddit.com/r/deeplearning/comments/bdfdwq/identifying_dog_breed_with_neural_networks_from/,atomlib_com,1555331323,,0,0,False,default,,,,,
154,deeplearning,t5_2t5eh,2019-4-16,2019,4,16,0,bdhmk5,medium.com,ReWork Deep Learning in Finance Summit,https://www.reddit.com/r/deeplearning/comments/bdhmk5/rework_deep_learning_in_finance_summit/,Yuqing7,1555343741,,0,1,False,default,,,,,
155,deeplearning,t5_2t5eh,2019-4-16,2019,4,16,8,bdmkqa,self.deeplearning,ML for Chemistry: Jetson tx2 for inference,https://www.reddit.com/r/deeplearning/comments/bdmkqa/ml_for_chemistry_jetson_tx2_for_inference/,expericonatus,1555369338,"New to ML... I have a large data set (300+ experiments, each with 4 unique processing conditions (duration, pressure, temperature, and water activity and a main outcome being chemical % of removal by vapor ). I want to train an ml model to be able to predict optimal processing conditions for a user-specified % removal and vice versa; to predict removal percentage from processing conditions. 

&amp;#x200B;

Any tips (ex.RNN or CNN needed?; software Tensorflow?) or literature you could point me towards would be greatly appreciated!

&amp;#x200B;

Also, can I train the model on the jetson, how long would it take with that number of experiments, each with 5 numerical points of data?",6,3,False,self,,,,,
156,deeplearning,t5_2t5eh,2019-4-16,2019,4,16,11,bdoisv,youtube.com,"Interested in Artificial Intelligence, Machine Learning, Computer Vision, or NLP? Check out this channel for excellent, well-explained, video tutorials.",https://www.reddit.com/r/deeplearning/comments/bdoisv/interested_in_artificial_intelligence_machine/,DiscoverAI,1555380897,,0,4,False,default,,,,,
157,deeplearning,t5_2t5eh,2019-4-16,2019,4,16,11,bdokp9,self.deeplearning,Problem of multiply tasks training for one network,https://www.reddit.com/r/deeplearning/comments/bdokp9/problem_of_multiply_tasks_training_for_one_network/,ldl19691031,1555381225,"Dear everyone, this is my first time to post on reddit so if there are anything wrong please tell me and I will fix. 

My problem is about training a multiply task network but the dataset do not contains all information.  I want to build a network for 3D pose estimation.  The network will produce a 2d joint heat map like openpose, and a depth map for 3D.

&amp;#x200B;

[Network architecture for two task](https://i.redd.it/ripzxkptajs21.jpg)

Because I cannot got enough good 3d pose data, I decided to mix two datasets: one is a high quality 2D pose dataset (MSCOCO) for 2d joint, and a 3d dataset JTA dataset which is captured from game. I tested only trained in JTA dataset but the 2d heat map result seems not good. So I decided to mix the two dataset. 

I randomly select a data from these two datasets. But the 2d dataset does not contains depth info. So for that, I directly set the depth map loss to zero. I do not know if this is right.

  

[During 2d dataset training, I just set the depth path's loss to zero](https://i.redd.it/fgzyi3utbjs21.jpg)

I also tried these methods:

1. Training 2d heatmaps first. Then fix the 2D layers (contains the public part). Then train the depth map part. The depth map loss convergence to a really high value. This do not work.
2. Only training on 3d dataset. the result do not good. Although 2d heat maps loss convergence to a small value, but the network do not perform will on real life. I think it is because the data is captured in game.
3. I tried real life 3d dataset like MPI-INF. But the lack of variance( especially different light environment and clothes) causes bad result. I tried changing different background. It still do not perform will.

So now these are my questions:

1. My depth maps loss  convergence to a really higher value. And the training time seems really long. Is my training design affect this?
2. I choose to use Adam as optimizer. But since the depth map loss will be randomly set to zero, will this affect the adam's optimization progress because the loss surface is always changing ?
3. Is there any better solution for solving this multiply task training?

&amp;#x200B;

Really thanks for any idea and discussion.",2,4,False,self,,,,,
158,deeplearning,t5_2t5eh,2019-4-16,2019,4,16,13,bdpnd9,guillaume-chevalier.com,LSTMs for Human Activity Recognition,https://www.reddit.com/r/deeplearning/comments/bdpnd9/lstms_for_human_activity_recognition/,GChe,1555388468,,2,1,False,default,,,,,
159,deeplearning,t5_2t5eh,2019-4-16,2019,4,16,14,bdqc1t,medium.com,Analyzing Source Code Using Neural Networks: A Case Study,https://www.reddit.com/r/deeplearning/comments/bdqc1t/analyzing_source_code_using_neural_networks_a/,TuSharma,1555393778,,0,11,False,default,,,,,
160,deeplearning,t5_2t5eh,2019-4-16,2019,4,16,22,bdtxyi,apriorit.com,Applying Long Short-Term Memory for Video Classification Issues,https://www.reddit.com/r/deeplearning/comments/bdtxyi/applying_long_shortterm_memory_for_video/,RyanTmthn,1555421264,,0,16,False,default,,,,,
161,deeplearning,t5_2t5eh,2019-4-17,2019,4,17,1,bdvsf0,medium.com,Bengio and Marcus at World AI Summit in Montral,https://www.reddit.com/r/deeplearning/comments/bdvsf0/bengio_and_marcus_at_world_ai_summit_in_montral/,gwen0927,1555431222,,0,5,False,default,,,,,
162,deeplearning,t5_2t5eh,2019-4-17,2019,4,17,3,bdxqcr,discoverai-community.herokuapp.com,"Looking to join a community of Machine Learning Students and Developers passionate about AI, Computer Vision, Deep Learning, and Natural Language Processing? Join the DiscoverAI Slack Community here.",https://www.reddit.com/r/deeplearning/comments/bdxqcr/looking_to_join_a_community_of_machine_learning/,DiscoverAI,1555441154,,0,1,False,default,,,,,
163,deeplearning,t5_2t5eh,2019-4-17,2019,4,17,6,bdzrz2,medium.com,"Boston Dynamics Spot Toughens Up, Hauls a Truck",https://www.reddit.com/r/deeplearning/comments/bdzrz2/boston_dynamics_spot_toughens_up_hauls_a_truck/,gwen0927,1555451777,,0,3,False,default,,,,,
164,deeplearning,t5_2t5eh,2019-4-17,2019,4,17,6,bdzsiy,self.deeplearning,Color preservation and image sharpening in GANs,https://www.reddit.com/r/deeplearning/comments/bdzsiy/color_preservation_and_image_sharpening_in_gans/,adi1709,1555451865,"I am working on extreme image compression using GANs. The reconstruction from the compressed images are missing color information and low level details. I am using adversarial loss, perceptual loss and MSE for optimization. Is there anything I can do to obtain better (color and sharpness) reconstructed images?",0,1,False,self,,,,,
165,deeplearning,t5_2t5eh,2019-4-17,2019,4,17,16,be5039,self.deeplearning,Why deep learning may not be the right solution for your business,https://www.reddit.com/r/deeplearning/comments/be5039/why_deep_learning_may_not_be_the_right_solution/,thumbsdrivesmecrazy,1555486352,"Way too many businesses reach for deep learning solutions when they shouldnt.

There are several factors that make relatively simpler models more suitable than their deep learning counterparts: [Why deep learning may not be the right solution for your business](https://dlabs.pl/blog/article/why-deep-learning-may-not-be-the-right-solution-for-your-business)

* Costs - The problems most, especially small, businesses are facing do not really require very complex and sophisticated methods which only increase costs and time.
* Not enough good-quality data - in some cases data sets are not big enough for deep learning which usually demands huge sample sizes.
* Limited interpretability - It is important because of new insights into relationships between numerous variables and expected outcomes, it increases the trust and understandability.",0,5,False,self,,,,,
166,deeplearning,t5_2t5eh,2019-4-17,2019,4,17,18,be5nro,i.redd.it,Learning to paint: A Painting AI,https://www.reddit.com/r/deeplearning/comments/be5nro/learning_to_paint_a_painting_ai/,hzwer,1555492568,,6,86,False,image,,,,,
167,deeplearning,t5_2t5eh,2019-4-17,2019,4,17,18,be5t1o,self.MachineLearning,[Project] Training models and running Jupyter Notebooks on AWS Spot Instances (cheaper and simpler than SageMaker),https://www.reddit.com/r/deeplearning/comments/be5t1o/project_training_models_and_running_jupyter/,apls777,1555493868,,0,3,False,default,,,,,
168,deeplearning,t5_2t5eh,2019-4-17,2019,4,17,20,be6p6d,youtu.be,"""Machine Learning: Alchemy for the Modern Computer Scientist"" with Erik Meijer (45min talk from GOTO Copenhagen 2018)",https://www.reddit.com/r/deeplearning/comments/be6p6d/machine_learning_alchemy_for_the_modern_computer/,goto-con,1555500870,,1,1,False,image,,,,,
169,deeplearning,t5_2t5eh,2019-4-17,2019,4,17,21,be6xx2,self.deeplearning,Experiment: 70fps real-time object detection with Google's Coral Dev Board with Edge TPU,https://www.reddit.com/r/deeplearning/comments/be6xx2/experiment_70fps_realtime_object_detection_with/,paul_read_it,1555502550,"We made a video to share our experience with the Google's Coral Dev Board with Edge TPU: [https://youtu.be/bOYWx1jJCZo](https://youtu.be/bOYWx1jJCZo)

&amp;#x200B;

We tested an object detection live stream under the following conditions:

\- a pretrained MobileNet v2 model, trained on the common objects in context (coco) dataset 

\- a bounding boxes threshold of 45% confidence because there were way too many boxes displayed in the default configuration

\- a camera connected via USB, not the official camera from Coral

&amp;#x200B;

We used this command to run the object detection server described above:

`edgetpu_classify_server \ --source /dev/video1:YUY2:800x600:24/1  \ --model path/to/model/mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite \ --labels path/to/labels/coco_labels.txt --threshold=0.45`

&amp;#x200B;

You can find more demos to play with here:

[https://coral.withgoogle.com/docs/dev-board/camera/](https://coral.withgoogle.com/docs/dev-board/camera/)

&amp;#x200B;

We hope this example helps you to get started with your own project!

&amp;#x200B;

If you have any idea, what we could build with the Coral device, let us know :-)

&amp;#x200B;

Paul",2,6,False,self,,,,,
170,deeplearning,t5_2t5eh,2019-4-17,2019,4,17,22,be7irs,self.computervision,Modelling relationships between objects in an image.,https://www.reddit.com/r/deeplearning/comments/be7irs/modelling_relationships_between_objects_in_an/,RohitDulam,1555506051,,0,3,False,default,,,,,
171,deeplearning,t5_2t5eh,2019-4-18,2019,4,18,0,be902e,self.deeplearning,Advancing in Deep Learning,https://www.reddit.com/r/deeplearning/comments/be902e/advancing_in_deep_learning/,smukh98,1555514225,"Hey all,
i am an undergrad student and i did a small course on machine learning for engineers.I got acquainted with algorithms like linear regression,logistic regression and all the way upto variational autoencoders and GANS.The course was a simple one as it was an introductory course. I have implemented vanilla classifiers,cnns,vaes etc.

So my question is whats the next step?
I have seen some of my friends writing research papers and publishing them in conferences and journals.Should i too start to write one? If so how should i start. 
Also since i am undergrad ,also tell something that recruiters want from an dl enthusiast.

Thanks in advance",2,4,False,self,,,,,
172,deeplearning,t5_2t5eh,2019-4-18,2019,4,18,3,bebk18,self.deeplearning,What are the minimum required skills to get a job in ML/DL coming from a Software Engineering (iOS) background?,https://www.reddit.com/r/deeplearning/comments/bebk18/what_are_the_minimum_required_skills_to_get_a_job/,ilikerum2,1555527295,"Hi 

Im currently a masters student learning about ML/AI and prior to my masters I was working as a Software Engineer working on iOS and Android platform. In my deep learning class i'm learning about MLPs, CNNs, RNNs, GANS, VAEs and RL. For assignments we have been doing things like Face Recognition using CNNs, Sequence to Sequence Translations, Building MLP from scratch using numpy. 

I want to know how these skills match up to industry requirements and what are some core skills you think someone with my background should have to qualify to be an ML/DL Engineer. Im very very passionate about this field and even though I don't have a strong math background (undergrad CS) but i'm able to work hard ask a lot of questions and understand the research papers to some extent. I want to work in speech tech or NLP. 

&amp;#x200B;

I would like to think there is  some sort of 80-20 split in terms of skills that are used in the industry. So i just wanted to know how I can acquire them and also how can i build a presence online if I want to stand out and be really good in this area.

&amp;#x200B;

Thanks",4,5,False,self,,,,,
173,deeplearning,t5_2t5eh,2019-4-18,2019,4,18,4,bec8e7,self.deeplearning,Training an autoencoder for multiple time-series together?,https://www.reddit.com/r/deeplearning/comments/bec8e7/training_an_autoencoder_for_multiple_timeseries/,pk12_,1555530826,"Any tips or suggestions? A reference even?

The time-series are correlated at some intervals. I wonder if I can integrate this information in the training process",2,4,False,self,,,,,
174,deeplearning,t5_2t5eh,2019-4-18,2019,4,18,10,befikm,self.deeplearning,Want to learn more about how deep Learning broadens the reach of Artificial Intelligence?,https://www.reddit.com/r/deeplearning/comments/befikm/want_to_learn_more_about_how_deep_learning/,treguess,1555549382,Want to learn more about how deep Learning broadens the reach of Artificial Intelligence? Read my article:[Read full article here. ](https://thenewstack.io/deep-learning-broadens-the-reach-of-artificial-intelligence/),0,0,False,self,,,,,
175,deeplearning,t5_2t5eh,2019-4-18,2019,4,18,12,beh18f,expoundai.wordpress.com,Deep Learning: Our journey begins  My Journey with Deep Learning and Computer Vision,https://www.reddit.com/r/deeplearning/comments/beh18f/deep_learning_our_journey_begins_my_journey_with/,msminhas93,1555559057,,1,0,False,default,,,,,
176,deeplearning,t5_2t5eh,2019-4-18,2019,4,18,14,beho8c,self.deeplearning,Face Recognition: An Introduction for Beginners,https://www.reddit.com/r/deeplearning/comments/beho8c/face_recognition_an_introduction_for_beginners/,spmallick,1555563780,"Face Recognition has been one of the most researched Computer Vision areas till date. So, it is natural to have too much information overload around the same.   
In our latest article, we have tried to simplify the topic and hope that it serves as a beginners' guide on Face Recognition.  
Feel free to comment if you think we have missed out on anything important.

[https://www.learnopencv.com/face-recognition-an-introduction-for-beginners/](https://www.learnopencv.com/face-recognition-an-introduction-for-beginners/) 

Mention reviews and what you want us to work next, in the comments!

P.S : More articles ( with code ) to come.  
[\#LearnOpenCV](https://www.facebook.com/hashtag/learnopencv?source=feed_text&amp;epa=HASHTAG&amp;__xts__%5B0%5D=68.ARA-mqImCdcmNoesLTLnq1cHNx1qybEcB2vWv0U-LCVooLDVbt6twgiEf_1ZdH1eFnuxGrlk-qovZie_GKHbK9JuHS7gwOzoQlew_P-o3By52IF6bkino3DlFiv1hW6__v-RJ83fIE3I4z6rmAlqKS7J5zP2ZG__d1RAql-eqhVXFtX7VUWedJ7uwZTMLaykIT2Ickd_8UC5VfMo5xc6HT5nOG3OIaht8E_lQnjxIG4LcGyYA4W2Vmm9FEPdvEyEW_JXGb_bsi33Jv0TCRtHyQT0EvR_DOIyo51B6ykXO3DkAHtMplZAjpPE3XW6dU2hPSsDuWWhqBMQ78GQuEwY3OE&amp;__tn__=%2ANK-R) [\#OpenCV](https://www.facebook.com/hashtag/opencv?source=feed_text&amp;epa=HASHTAG&amp;__xts__%5B0%5D=68.ARA-mqImCdcmNoesLTLnq1cHNx1qybEcB2vWv0U-LCVooLDVbt6twgiEf_1ZdH1eFnuxGrlk-qovZie_GKHbK9JuHS7gwOzoQlew_P-o3By52IF6bkino3DlFiv1hW6__v-RJ83fIE3I4z6rmAlqKS7J5zP2ZG__d1RAql-eqhVXFtX7VUWedJ7uwZTMLaykIT2Ickd_8UC5VfMo5xc6HT5nOG3OIaht8E_lQnjxIG4LcGyYA4W2Vmm9FEPdvEyEW_JXGb_bsi33Jv0TCRtHyQT0EvR_DOIyo51B6ykXO3DkAHtMplZAjpPE3XW6dU2hPSsDuWWhqBMQ78GQuEwY3OE&amp;__tn__=%2ANK-R) [\#MachineLearning](https://www.facebook.com/hashtag/machinelearning?source=feed_text&amp;epa=HASHTAG&amp;__xts__%5B0%5D=68.ARA-mqImCdcmNoesLTLnq1cHNx1qybEcB2vWv0U-LCVooLDVbt6twgiEf_1ZdH1eFnuxGrlk-qovZie_GKHbK9JuHS7gwOzoQlew_P-o3By52IF6bkino3DlFiv1hW6__v-RJ83fIE3I4z6rmAlqKS7J5zP2ZG__d1RAql-eqhVXFtX7VUWedJ7uwZTMLaykIT2Ickd_8UC5VfMo5xc6HT5nOG3OIaht8E_lQnjxIG4LcGyYA4W2Vmm9FEPdvEyEW_JXGb_bsi33Jv0TCRtHyQT0EvR_DOIyo51B6ykXO3DkAHtMplZAjpPE3XW6dU2hPSsDuWWhqBMQ78GQuEwY3OE&amp;__tn__=%2ANK-R) [\#DeepLearning](https://www.facebook.com/hashtag/deeplearning?source=feed_text&amp;epa=HASHTAG&amp;__xts__%5B0%5D=68.ARA-mqImCdcmNoesLTLnq1cHNx1qybEcB2vWv0U-LCVooLDVbt6twgiEf_1ZdH1eFnuxGrlk-qovZie_GKHbK9JuHS7gwOzoQlew_P-o3By52IF6bkino3DlFiv1hW6__v-RJ83fIE3I4z6rmAlqKS7J5zP2ZG__d1RAql-eqhVXFtX7VUWedJ7uwZTMLaykIT2Ickd_8UC5VfMo5xc6HT5nOG3OIaht8E_lQnjxIG4LcGyYA4W2Vmm9FEPdvEyEW_JXGb_bsi33Jv0TCRtHyQT0EvR_DOIyo51B6ykXO3DkAHtMplZAjpPE3XW6dU2hPSsDuWWhqBMQ78GQuEwY3OE&amp;__tn__=%2ANK-R) [\#AI](https://www.facebook.com/hashtag/ai?source=feed_text&amp;epa=HASHTAG&amp;__xts__%5B0%5D=68.ARA-mqImCdcmNoesLTLnq1cHNx1qybEcB2vWv0U-LCVooLDVbt6twgiEf_1ZdH1eFnuxGrlk-qovZie_GKHbK9JuHS7gwOzoQlew_P-o3By52IF6bkino3DlFiv1hW6__v-RJ83fIE3I4z6rmAlqKS7J5zP2ZG__d1RAql-eqhVXFtX7VUWedJ7uwZTMLaykIT2Ickd_8UC5VfMo5xc6HT5nOG3OIaht8E_lQnjxIG4LcGyYA4W2Vmm9FEPdvEyEW_JXGb_bsi33Jv0TCRtHyQT0EvR_DOIyo51B6ykXO3DkAHtMplZAjpPE3XW6dU2hPSsDuWWhqBMQ78GQuEwY3OE&amp;__tn__=%2ANK-R)[\#ComputerVision](https://www.facebook.com/hashtag/computervision?source=feed_text&amp;epa=HASHTAG&amp;__xts__%5B0%5D=68.ARA-mqImCdcmNoesLTLnq1cHNx1qybEcB2vWv0U-LCVooLDVbt6twgiEf_1ZdH1eFnuxGrlk-qovZie_GKHbK9JuHS7gwOzoQlew_P-o3By52IF6bkino3DlFiv1hW6__v-RJ83fIE3I4z6rmAlqKS7J5zP2ZG__d1RAql-eqhVXFtX7VUWedJ7uwZTMLaykIT2Ickd_8UC5VfMo5xc6HT5nOG3OIaht8E_lQnjxIG4LcGyYA4W2Vmm9FEPdvEyEW_JXGb_bsi33Jv0TCRtHyQT0EvR_DOIyo51B6ykXO3DkAHtMplZAjpPE3XW6dU2hPSsDuWWhqBMQ78GQuEwY3OE&amp;__tn__=%2ANK-R) [\#FaceRecognition](https://www.facebook.com/hashtag/facerecognition?source=feed_text&amp;epa=HASHTAG&amp;__xts__%5B0%5D=68.ARA-mqImCdcmNoesLTLnq1cHNx1qybEcB2vWv0U-LCVooLDVbt6twgiEf_1ZdH1eFnuxGrlk-qovZie_GKHbK9JuHS7gwOzoQlew_P-o3By52IF6bkino3DlFiv1hW6__v-RJ83fIE3I4z6rmAlqKS7J5zP2ZG__d1RAql-eqhVXFtX7VUWedJ7uwZTMLaykIT2Ickd_8UC5VfMo5xc6HT5nOG3OIaht8E_lQnjxIG4LcGyYA4W2Vmm9FEPdvEyEW_JXGb_bsi33Jv0TCRtHyQT0EvR_DOIyo51B6ykXO3DkAHtMplZAjpPE3XW6dU2hPSsDuWWhqBMQ78GQuEwY3OE&amp;__tn__=%2ANK-R)

https://i.redd.it/xpftawm8gys21.jpg",1,24,False,self,,,,,
177,deeplearning,t5_2t5eh,2019-4-18,2019,4,18,16,beij6s,medium.com,Visualizing stock trading agents using Matplotlib and Gym,https://www.reddit.com/r/deeplearning/comments/beij6s/visualizing_stock_trading_agents_using_matplotlib/,notadamking,1555570847,,0,8,False,default,,,,,
178,deeplearning,t5_2t5eh,2019-4-18,2019,4,18,16,beim7j,old.reddit.com,free video: Deep Learning with Python (registration required),https://www.reddit.com/r/deeplearning/comments/beim7j/free_video_deep_learning_with_python_registration/,PacktStaff,1555571553,,0,2,False,default,,,,,
179,deeplearning,t5_2t5eh,2019-4-18,2019,4,18,16,beipjl,czbiohub.org,"Livestreamed Conference on Deep Learning, AI, Big Data, ML applied to Brain - Jure Leskovec speaking",https://www.reddit.com/r/deeplearning/comments/beipjl/livestreamed_conference_on_deep_learning_ai_big/,ScienTecht,1555572380,,0,3,False,default,,,,,
180,deeplearning,t5_2t5eh,2019-4-19,2019,4,19,0,bemoop,medium.com,Upgrading CNN With OctConv,https://www.reddit.com/r/deeplearning/comments/bemoop/upgrading_cnn_with_octconv/,gwen0927,1555600267,,2,12,False,default,,,,,
181,deeplearning,t5_2t5eh,2019-4-19,2019,4,19,6,ber9tr,self.deeplearning,Convolutional layer output size,https://www.reddit.com/r/deeplearning/comments/ber9tr/convolutional_layer_output_size/,RealOden,1555624518,"Hi people,

I'm stuck on a problem and was wondering if anyone would be able to provide me with some answers, as I have not found any when googling. In the following code snippet from ChainerRL they list the output of the last convolutional layer as 3136 when put into the fully-connected (linear) layer:

&amp;#x200B;

&amp;#x200B;

1. **class** NatureDQNHead(chainer.ChainList):
2. """"""DQN's head (Nature version)""""""
3. **def** \_\_init\_\_(self, n\_input\_channels=4, n\_output\_channels=512,
4.         activation=F.relu, bias=0.1):
5. self.n\_input\_channels = n\_input\_channels
6. self.activation = activation
7. self.n\_output\_channels = n\_output\_channels
8.     layers = \[
9.       L.Convolution2D(n\_input\_channels, 32, 8, stride=4,
10.               initial\_bias=bias),
11.       L.Convolution2D(32, 64, 4, stride=2, initial\_bias=bias),
12.       L.Convolution2D(64, 64, 3, stride=1, initial\_bias=bias),
13.       L.Linear(**3136**, n\_output\_channels, initial\_bias=bias), &lt;--------------HERE
14. \]
15. super(NatureDQNHead, self).\_\_init\_\_(\*layers)
16. **def** \_\_call\_\_(self, state):
17.     h = state
18. **for** layer **in** self:
19.       h = self.activation(layer(h))
20. **return** h

&amp;#x200B;

&amp;#x200B;

How do you calculate that output? I need to know as normally None works fine in ChainerRL, as it automatically sets the variable to whatever the output is, but when trying to create an A3C model this is no longer ok. I need the output of the last convolutional layer for the following NN:

&amp;#x200B;

&amp;#x200B;

1. **class** QFunction(chainer.ChainList):
2. **def** \_\_init\_\_(self, obs\_size):
3. self.conv0 = L.ConvolutionND(2, None, 32, ksize=(8,8), stride=(4,4), pad=(3,3))
4. self.conv1 = L.ConvolutionND(2, 32, 64, ksize=(4,4), stride=(2,2), pad=(3,3))
5. self.conv2 = L.ConvolutionND(2, 64, 128, ksize=(3,3), stride=(1,1), pad=(3,3))
6. self.l0 = L.Linear(**None**, 512) &lt;------What should the input be at None?
7.     layers = \[
8. self.conv0,
9. self.conv1,
10. self.conv2,
11. self.l0
12. \]
13. super(QFunction, self).\_\_init\_\_(\*layers)
14. **def** \_\_call\_\_(self, state):
15.     h = state
16. **for** layer **in** self:
17.       h = F.relu(layer(h))
18. **return** h

&amp;#x200B;

&amp;#x200B;

Notice that it also has padding, as it's needed to satisfy an assertion in ChainerRL of some arbitrary calculation of the parameters in the layer. If someone could answer this, I would greatly appreciate it.",1,2,False,self,,,,,
182,deeplearning,t5_2t5eh,2019-4-19,2019,4,19,10,betllg,self.deeplearning,Helping a total newbie run SC-FEGAN,https://www.reddit.com/r/deeplearning/comments/betllg/helping_a_total_newbie_run_scfegan/,sytrix,1555637836,"I'm sorry if this is the wrong place to ask but I've Googled my issues and I'm at a loss on helpful resources.

I'm not familiar with neural networks but would like to try [SC-FEGAN](https://github.com/JoYoungjoo/SC-FEGAN) out. I have the files, the model, and Python downloaded and I'm stuck on getting this working:

    mv /${HOME}/SC-FEGAN.ckpt.* /${HOME}/ckpt/
    python3 demo.py

Should I be using command prompt or Python to launch the GUI? How do I use the other dependencies listed (tensorflow, numpy, Python3, PyQt5, opencv-python, pyyaml)?

ELI5, any advice would be appreciated, thank you for your time :)",2,1,False,self,,,,,
183,deeplearning,t5_2t5eh,2019-4-19,2019,4,19,10,betokv,youtube.com,"1 Interested in Artificial Intelligence, Machine Learning, Computer Vision, or NLP? Check out this channel for excellent, well-explained, video tutorials.",https://www.reddit.com/r/deeplearning/comments/betokv/1_interested_in_artificial_intelligence_machine/,Bobber123yxz,1555638371,,0,0,False,default,,,,,
184,deeplearning,t5_2t5eh,2019-4-19,2019,4,19,11,betue7,self.deeplearning,random noise is classified as some class..!,https://www.reddit.com/r/deeplearning/comments/betue7/random_noise_is_classified_as_some_class/,GW_KIM,1555639418,"Hi, i am making some classifier using CNN.

there is 2 class and train loss go low well. and training acc shows good result.

But, when i test it on test data, the result is extremely biased to some class. 

So i feed a random noise to network ,and got a same result (biased)

what's wrong with it...? HELP :(",5,1,False,self,,,,,
185,deeplearning,t5_2t5eh,2019-4-19,2019,4,19,12,beuhrx,youtube.com,AI Draws Awesome Caricatures (WarpGAN),https://www.reddit.com/r/deeplearning/comments/beuhrx/ai_draws_awesome_caricatures_warpgan/,hanyuqn,1555643661,,0,31,False,image,,,,,
186,deeplearning,t5_2t5eh,2019-4-19,2019,4,19,14,bevqqw,self.deeplearning,Going to #ODSCEast? Get a Free 1-hour consultation,https://www.reddit.com/r/deeplearning/comments/bevqqw/going_to_odsceast_get_a_free_1hour_consultation/,treguess,1555653046,[removed],0,1,False,self,,,,,
187,deeplearning,t5_2t5eh,2019-4-19,2019,4,19,16,bewc7n,self.deeplearning,new podcasts in the field of artificial intelligence,https://www.reddit.com/r/deeplearning/comments/bewc7n/new_podcasts_in_the_field_of_artificial/,Doctor_who1,1555658135," 

Hello . I want to create podcasts in the field of artificial intelligence, but unfortunately I have no idea what to say in the first issue. I wanted to help if you know the article or topics you can post on the first episode of the podcast.",0,0,False,self,,,,,
188,deeplearning,t5_2t5eh,2019-4-20,2019,4,20,2,bf257k,self.deeplearning,Are there any significant differences between deep learning and early neural networks? (besides number of layers),https://www.reddit.com/r/deeplearning/comments/bf257k/are_there_any_significant_differences_between/,bobmichal,1555696513,,3,0,False,self,,,,,
189,deeplearning,t5_2t5eh,2019-4-20,2019,4,20,5,bf3y2k,codesmithdev.com,10 Deep Learning Resources for Audio Processing,https://www.reddit.com/r/deeplearning/comments/bf3y2k/10_deep_learning_resources_for_audio_processing/,oblivionreb,1555705926,,1,33,False,default,,,,,
190,deeplearning,t5_2t5eh,2019-4-20,2019,4,20,6,bf4fih,self.deeplearning,Good tutorials for Sentiment Analysis for EXTREME noob,https://www.reddit.com/r/deeplearning/comments/bf4fih/good_tutorials_for_sentiment_analysis_for_extreme/,stackoverflowcoder,1555708539,"Currently I've only built CNN'S in Keras, and understand python code.",0,1,False,self,,,,,
191,deeplearning,t5_2t5eh,2019-4-20,2019,4,20,7,bf5bnl,expoundai.wordpress.com,Convolutions Convoluted? Nah  My Journey with Deep Learning and Computer Vision,https://www.reddit.com/r/deeplearning/comments/bf5bnl/convolutions_convoluted_nah_my_journey_with_deep/,msminhas93,1555713578,,0,3,False,default,,,,,
192,deeplearning,t5_2t5eh,2019-4-20,2019,4,20,13,bf8amj,self.learnpython,(Python) Easy to follow NeuroEvolution tutorial,https://www.reddit.com/r/deeplearning/comments/bf8amj/python_easy_to_follow_neuroevolution_tutorial/,EzitoKo,1555733288,,0,1,False,default,,,,,
193,deeplearning,t5_2t5eh,2019-4-21,2019,4,21,1,bfdyhy,youtube.com,Deep Reinforcement Learning AILearning to Paint like humans,https://www.reddit.com/r/deeplearning/comments/bfdyhy/deep_reinforcement_learning_ailearning_to_paint/,hzwer,1555777372,,1,7,False,default,,,,,
194,deeplearning,t5_2t5eh,2019-4-21,2019,4,21,2,bfevml,udemy.com,Tensorflow and Keras For Neural Networks and Deep Learning,https://www.reddit.com/r/deeplearning/comments/bfevml/tensorflow_and_keras_for_neural_networks_and_deep/,system4norcal,1555782121,,0,1,False,default,,,,,
195,deeplearning,t5_2t5eh,2019-4-21,2019,4,21,3,bffhhn,okai.brown.edu,OKAI - An Interactive Introduction to Artificial Intelligence (AI),https://www.reddit.com/r/deeplearning/comments/bffhhn/okai_an_interactive_introduction_to_artificial/,tydlwav,1555785346,,0,1,False,default,,,,,
196,deeplearning,t5_2t5eh,2019-4-21,2019,4,21,3,bffmve,okai.brown.edu,OKAI - An Interactive Introduction to Artificial Intelligence (AI),https://www.reddit.com/r/deeplearning/comments/bffmve/okai_an_interactive_introduction_to_artificial/,tydlwav,1555786184,,2,28,False,default,,,,,
197,deeplearning,t5_2t5eh,2019-4-21,2019,4,21,5,bfgovk,youtube.com,This video goes over a breast cancer diagnosis model that uses neural networks. Really interesting,https://www.reddit.com/r/deeplearning/comments/bfgovk/this_video_goes_over_a_breast_cancer_diagnosis/,antaloaalonso,1555792049,,0,2,False,image,,,,,
198,deeplearning,t5_2t5eh,2019-4-21,2019,4,21,11,bfkcp7,self.deeplearning,Activation Functions,https://www.reddit.com/r/deeplearning/comments/bfkcp7/activation_functions/,ZeroMaxinumXZ,1555814998,"What makes a good activation function? What qualities should I look for in an activation function? For example, in RELU, the output is zero when the input is negative, and in sigmoid, the output is always between zero and one...

&amp;#x200B;

So, what exactly makes a good activation function?",3,4,False,self,,,,,
199,deeplearning,t5_2t5eh,2019-4-21,2019,4,21,12,bfktmx,youtube.com,[R] FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained Object Generation and Discovery (CVPR'19 Oral presentation),https://www.reddit.com/r/deeplearning/comments/bfktmx/r_finegan_unsupervised_hierarchical/,utkarsh2254,1555818356,,3,38,False,default,,,,,
200,deeplearning,t5_2t5eh,2019-4-22,2019,4,22,0,bfpsr7,self.deeplearning,A few questions,https://www.reddit.com/r/deeplearning/comments/bfpsr7/a_few_questions/,TheUSARMY45,1555859540,"Hello everyone! Im relatively new to the field of Deep Learning, but am itching to learn more. I have academic background in statistical theory with a focus on data science, but have also been taking some Deep Learning classes through Coursera (Andrew Ngs deep learning specialization). I just have a few questions that I wanted to get some input on from this community:

1) I am very well versed in R for most of the analytical things that I do, though I do know a little bit of Python (the coursera course is done in Python). While I know that the big Deep Learning frameworks like Keras and Tensorflow were developed in Python, I was wondering what more I could do to learn more about Deep Learning with R

2) I really want to get into GPU Learning. I have an AMD gpu in my desktop PC and an NVIDIA Quadra K1100m in my laptop. I know that there is limited support for AMD GPUs in Deep Learning right now, but is there anything at all that I can try with this? Alternatively, would my NVIDIA card be up to snuff for practicing with some shallower neural nets? I dont want to fry my GPU just because I know very little about the subject 

Any comments are greatly appreciated! This stuff is so fascinating to me and I am really excited to learn more!",0,1,False,self,,,,,
201,deeplearning,t5_2t5eh,2019-4-22,2019,4,22,0,bfpv6k,self.deeplearning,Topic in deep learning with potential for most growth,https://www.reddit.com/r/deeplearning/comments/bfpv6k/topic_in_deep_learning_with_potential_for_most/,nicetryho,1555859945,"What areas in deep learning are worth exploring that arent really saturated ? Im seeing a lot of GAN research is being done to generate art, whats another Avenue with a lot less hype and a lot less data contributed?",9,15,False,self,,,,,
202,deeplearning,t5_2t5eh,2019-4-22,2019,4,22,4,bfsaa2,self.deeplearning,Sparse deep neural topology,https://www.reddit.com/r/deeplearning/comments/bfsaa2/sparse_deep_neural_topology/,ToolTechSoftware,1555873288,"More and more people are looking at Neural Networks that doesnt follow the traditional tensor math or forward feed topology.  

I have been working for years to get Nvidia or other GPU/FPGA to realise this fact that we need new ways to accellerate generic interconnected networks.  

My suggestion is a Neural Network language.  An assembler that can describe low level Neural Network topologies.  I call it CAL.  

By using CAL you can construct any graph cyclic connected topography and run it in an optimizer and a compiler to produce efficient code for backprop or genetic evolution of code.  

Would love to discuss...",2,3,False,self,,,,,
203,deeplearning,t5_2t5eh,2019-4-22,2019,4,22,6,bftnms,self.deeplearning,EDM music generated using deep recurrent neural networks  please help me to evaluate!,https://www.reddit.com/r/deeplearning/comments/bftnms/edm_music_generated_using_deep_recurrent_neural/,MaxCb,1555880893,"This is for my final year project. Any responses are greatly appreciates, only takes 2 mins! Thank you.

Survey link: https://www.surveygizmo.com/s3/4968707/Generated-Music-Evaluation",11,35,False,self,,,,,
204,deeplearning,t5_2t5eh,2019-4-22,2019,4,22,6,bftsyr,self.deeplearning,[Question] BERT performing worse than word2vec,https://www.reddit.com/r/deeplearning/comments/bftsyr/question_bert_performing_worse_than_word2vec/,naboo_random,1555881716,"Hi, 

&amp;#x200B;

I am trying to use BERT for a document ranking problem. My task is pretty straightforward. I have to do a similarity ranking for an input document. The only issue here is that I dont have labels - so its more of a qualitative analysis.

I am on my way to try a bunch of document representation techniques - word2vec, para2vec and BERT mainly. 

&amp;#x200B;

For BERT, i came across [this](https://github.com/huggingface/pytorch-pretrained-BERT) library.  I fine tuned the bert-small-uncased model, with around 150,000 documents. I ran it for 5 epochs, with a batch size of 16 and max seq length 128. However, if I compare the performance of Bert representation vs word2vec representations, for some reason word2vec is performing better for me right now. For BERT, I used the last four layers for getting the representation. 

&amp;#x200B;

I am not too sure why the fine tuned model didnt work. I read up [this](https://arxiv.org/pdf/1903.05987.pdf) paper, and [this](https://github.com/huggingface/pytorch-pretrained-BERT/issues/493) other link also that said that BERT performs well when fine tuned for a classification task. However, since I dont have the labels, I fined tuned it as it's done in the paper - in an unsupervised manner.

&amp;#x200B;

Also, my documents vary a lot in their length. So Im sending them sentence wise right now. In the end I have to average over the word embeddings anyway to get the sentence embedding. Any ideas on a better method? I also read [here](https://github.com/hanxiao/bert-as-service)  \- that there are different ways of pooling over the word embeddings to get a fixed embedding. Wondering if there is a comparison of which pooling technique works better? 

&amp;#x200B;

Any help on training BERT better or a better pooling method will be greatly appreciated! 

&amp;#x200B;

Thanks,",0,12,False,self,,,,,
205,deeplearning,t5_2t5eh,2019-4-22,2019,4,22,8,bfv1he,self.deeplearning,Am I being underpaid? Would a DL position offer a significant pay increase?,https://www.reddit.com/r/deeplearning/comments/bfv1he/am_i_being_underpaid_would_a_dl_position_offer_a/,74throwaway,1555888888,"I currently work in image processing just developing code in Matlab and make around $100K/yr in California, but I get to work from home in CA (near LA). I've only gotten to work with deep learning for a couple months in this job. I have alot of gaps in my resume, but I have about 1.5 yrs experience in this current job along with some freelance work/personal projects in machine learning.

Deep learning seems alot more interesting to me than image processing. Plus, it seems Data Scientists/ML Engineers near LA with around the same experience as me make around 90-175K/yr. So I feel underpaid, but I guess being allowed to work remotely kind of makes up for that. 

Am I right that I'm being underpaid? If I were to spend a few months working on Kaggle projects using Deep Learning, would I have a realistic chance at jobs offering much higher salaries near LA or the Bay Area, such as $150-175K?",1,2,False,self,,,,,
206,deeplearning,t5_2t5eh,2019-4-22,2019,4,22,12,bfxh1l,self.deeplearning,DL study suggestion,https://www.reddit.com/r/deeplearning/comments/bfxh1l/dl_study_suggestion/,gabrie-ll,1555904058,"Hi,
I've gotten started in ML and DL by my own at uni, but now that I've been using what I studied so far @ a research project, I've been pretty reliant on my teachers to give me pointers as to what to do. What do you guys recommend as far as reading/course material? My main focus is ConvNets, but deep learning in general is really interesting to me and is likely what I'll pursue further on.

So far I've done a ML class (and a couple statistics classes), the ML Andrew Ng course, going through the deeplarning.ai classes on YouTube and reading an image processing book (Gonzalez I think). I'm open to any suggestions, thx in advance bros",1,1,False,self,,,,,
207,deeplearning,t5_2t5eh,2019-4-22,2019,4,22,20,bg15ri,self.deeplearning,Load word embedding on cpu and training on gpu,https://www.reddit.com/r/deeplearning/comments/bg15ri/load_word_embedding_on_cpu_and_training_on_gpu/,rishabh279,1555933809,"Hey folks,

I am using keras framework to train my bidirectional LSTM model. I am training my model on multiple gpu of amazon instance. The word embedding(WE) that I am using has a size of 24 gb, in addition I have 3-4 lakhs  of training records with each record consisting of 900-1000 words. For this to load ie. WE + training data it requires 256 gb ram of gpu. This is costing me a lot. I am using p3dn.24xlarge instance of amazon. Is there a way to load the WE on cpu and do training on gpu so that I can use a low config instance of amazon which would require low gpu memory and save pretty descent bucks. Thanks in advance.",0,1,False,self,,,,,
208,deeplearning,t5_2t5eh,2019-4-23,2019,4,23,0,bg37ne,medium.com,Everyone Is an Artist: GauGAN Turns Doodles Into Photorealistic Landscapes,https://www.reddit.com/r/deeplearning/comments/bg37ne/everyone_is_an_artist_gaugan_turns_doodles_into/,Yuqing7,1555945862,,1,29,False,default,,,,,
209,deeplearning,t5_2t5eh,2019-4-23,2019,4,23,0,bg3jze,self.deeplearning,Bioinformatics,https://www.reddit.com/r/deeplearning/comments/bg3jze/bioinformatics/,prabin96,1555947674,"You are working with a pharma company, who released a new drug that can cure cancer. However, this drug is fatal for patients with a particular set of molecules in their DNA. Let us assume that a single DNA strand is enough to identify if a drug is fatal for a person. Let us also assume that each molecule in the DNA can be one among four types. Problem is, each DNA strand is two billion molecules long, and it is hard to identify problematic molecules manually. 

The company is interested in training a classifier to automatically decide if this treatment is safe for the patient. They have collected DNA strands from ten thousand people. The drug is fatal for hundred among these ten thousand people. You are provided with both the DNA strand data, and its corresponding label (safe or fatal). For clarity, lets assume the label safe is a positive label and label fatal is a negative label. 

This is problem statement, what could be the solution? Could anyone give workflow of this problem from start to end??",8,1,False,self,,,,,
210,deeplearning,t5_2t5eh,2019-4-23,2019,4,23,4,bg67ym,self.deeplearning,Class activation mapping for videos,https://www.reddit.com/r/deeplearning/comments/bg67ym/class_activation_mapping_for_videos/,kzhang3256,1555961161," Does anyone know how to do class activation mapping for a video (during classification for action detection)? Im using resnet3d50 architecture as pre-trained model. Both pytorch and keras work for me. If anyone knows there is code/github repo doing similar things will be best.   
Thanks!",2,3,False,self,,,,,
211,deeplearning,t5_2t5eh,2019-4-23,2019,4,23,10,bga5d0,self.deeplearning,"Deep Learning Build, Need some Expertise :)",https://www.reddit.com/r/deeplearning/comments/bga5d0/deep_learning_build_need_some_expertise/,Atralb,1555982259,"Hi guys,

&amp;#x200B;

So I'm planning to build my own computer for making deep learning experiments, but also want to be able to use it as a private server simultaneously, and be able to use it with browser, heavy code editor, etc... sometimes.  
For reference I want to make personal deep learning training experiments in music (and more generally sound) analysis and maybe a bit of video game AI applications experiments (not in relation with the music thing).

&amp;#x200B;

It's my first time building a pc but I will have an experienced friend there to help me when the critical time comes.

&amp;#x200B;

So I mainly followed this guide [https://medium.com/the-mission/how-to-build-the-perfect-deep-learning-computer-and-save-thousands-of-dollars-9ec3b2eb4ce2](https://medium.com/the-mission/how-to-build-the-perfect-deep-learning-computer-and-save-thousands-of-dollars-9ec3b2eb4ce2) in my research along with a bit of that one [https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/](https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/) and many others.

&amp;#x200B;

I don't have very much money but I want to use in potential professional use and I'm in the post-graduation-first-job phase as an IA engineer and paid 1k2 for at least 6 months in an internship so I thought asking a 2k-advance on my parents was a good budget. High end powerful experiment while maintaining not exorbitant prices relatively to my financial means.

&amp;#x200B;

&amp;#x200B;

Therefore, with 2000EUR in mind then, I settle with this temporary build (sorry for the different language) :  
**- Power : Corsair HX1200i** [https://www.amazon.fr/gp/product/B00S8HY0BW/ref=ox\_sc\_act\_title\_2?smid=A1X6FK5RDHNB96&amp;psc=1](https://www.amazon.fr/gp/product/B00S8HY0BW/ref=ox_sc_act_title_2?smid=A1X6FK5RDHNB96&amp;psc=1)

**-** **Motherboard : Gigabyte X399 Aorus Pro, AMD X399**  [https://www.amazon.fr/gp/product/B07KF7M46X/ref=ox\_sc\_act\_title\_3?smid=A1X6FK5RDHNB96&amp;psc=1](https://www.amazon.fr/gp/product/B07KF7M46X/ref=ox_sc_act_title_3?smid=A1X6FK5RDHNB96&amp;psc=1)  
**- Storage : Samsung SSD 970 EVO NVMe M.2 (1TB)- MZ-V7E1T0BW**  [https://www.amazon.fr/gp/product/B07CGJNLBB/ref=ox\_sc\_act\_title\_4?smid=A862111F3B7OV&amp;psc=1](https://www.amazon.fr/gp/product/B07CGJNLBB/ref=ox_sc_act_title_4?smid=A862111F3B7OV&amp;psc=1)  
**- CPU : AMD Ryzen 7 Threadripper 1920X**   [https://www.amazon.fr/gp/product/B074CBJHCT/ref=ox\_sc\_act\_title\_5?smid=A1X6FK5RDHNB96&amp;psc=1](https://www.amazon.fr/gp/product/B074CBJHCT/ref=ox_sc_act_title_5?smid=A1X6FK5RDHNB96&amp;psc=1)

**- GPU : A GTX 1080 ti** but got no amazon link, cause things move fast for it on amazon, so I give it time for when I see a good deal (you can see from 550 to 800)

So first I have a couple questions :

* **GPU :** So here my prime argument is getting the threshold of 10Gb of VRAM. Secondly, in the guide I provided, it is said that the performance (at least in deep learning) is proportional to the amount of CUDA cores, so I made calculation that in my country the 1080 GTX is the best deal. What do you think of these choices in relation to the practice I want to make of it ? Do you have any other thoughts regarding the GPU ? Maybe RTX 2070 or 2080 is better for the price ?
* **Case :** I got it was really important to take dimensions in account when looking for the case.  
The guy is suggesting this (150EUR 270\*465\*476mm) : [https://www.amazon.fr/Lian-Li-PC-O11AIR-Bo%C3%AEtier-pour/dp/B07FJ6PMKW/ref=sr\_1\_fkmrnull\_1?\_\_mk\_fr\_FR=%C3%85M%C3%85%C5%BD%C3%95%C3%91&amp;keywords=%3A+Lian-Li+PC-O11AIR&amp;qid=1555175276&amp;s=gateway&amp;sr=8-1-fkmrnull](https://www.amazon.fr/Lian-Li-PC-O11AIR-Bo%C3%AEtier-pour/dp/B07FJ6PMKW/ref=sr_1_fkmrnull_1?__mk_fr_FR=%C3%85M%C3%85%C5%BD%C3%95%C3%91&amp;keywords=%3A+Lian-Li+PC-O11AIR&amp;qid=1555175276&amp;s=gateway&amp;sr=8-1-fkmrnull)  
Considering the 1080 measures 370\*280\*114mm (for a Gigabyte) Do you think this won't go (50EUR 470\*201\*429mm) : [https://www.amazon.fr/dp/B00RORBQNW/ref=psdc\_430338031\_t1\_B00XPUFY0I](https://www.amazon.fr/dp/B00RORBQNW/ref=psdc_430338031_t1_B00XPUFY0I) ?
* **Fans :** Besides, considering the motherboard configuration (available here [https://c1.neweggimages.com/ProductImageCompressAll1280/13-145-109-V01.jpg](https://c1.neweggimages.com/ProductImageCompressAll1280/13-145-109-V01.jpg)), and considering the case dimension (470\*201\*429) and the GPU dimension and its placing on the motherboard, do you think a fan like this one would be able to fit in :  
Noctua NH-U9 TR4-SP3 [https://www.amazon.fr/gp/product/B074DXFB66/ref=ox\_sc\_saved\_title\_2?smid=A38F5RZ72I2JQ&amp;psc=1](https://www.amazon.fr/gp/product/B074DXFB66/ref=ox_sc_saved_title_2?smid=A38F5RZ72I2JQ&amp;psc=1) ?  
We've done a bit of calculation and considering where the different components will be placed on the motherboard, and, while it was very approximate, we found it would be at least a bit difficult.
* **CPU :** I've done my research mostly last weekend so I don't remember everything but I know I settled for the 1920X cause of the numerous cores and because I kinda believed (but really vaguely) it was better for the price than Intel.But I have since heard that unless using 4 GPUs it was not necessary putting 400EUR for a CPU and maybe taking a 200-300 one and putting the rest of the money in a better gpu was not a bad idea. What is your take on this ?
* **Network Card :** For experienced Deep learning engineers do you think going for a 10Gb Network card like this [https://www.amazon.fr/gp/product/B071JR2ZW8/ref=ox\_sc\_saved\_title\_3?smid=A1X6FK5RDHNB96&amp;psc=1](https://www.amazon.fr/gp/product/B071JR2ZW8/ref=ox_sc_saved_title_3?smid=A1X6FK5RDHNB96&amp;psc=1) a good idea or not really useful ?I currently have 800Mb/s down rates with my laptop in Ethernet, and I don't see myself having to download Terabytes of data so I thought it was more of a gadget than anything, but I perfectly may be wrong.
* **Power :** Is 1200W too much with that build (if the guy's calcuations are right and I understood well, I've done that : GPU 250W (500W for future second card) + 180 W CPU + 150W = 830W =&gt; 1000W and the 1200W is only 25EUR more) ?Is it even overkill to have more than what I need since it wil apparently consume more for nothing and so inflate the electricity bill ?
* **RAM :** Are 16Gb enough for this (10Gb model training + 3 Gb for smth else +  3 System (maybe it's a bit short) ?

&amp;#x200B;

&amp;#x200B;

I think that's all, but I may have forgotten something (you know 420 is just 3 days ago :p), I will maybe add something later. By the way, I'm still open to anything else you might think and will be glad to read you.

Thanks a lot in advance for your help !",9,2,False,self,,,,,
212,deeplearning,t5_2t5eh,2019-4-23,2019,4,23,13,bgc2z1,self.deeplearning,"[P] Trump, Obama, Jordan Peterson and Neil deGrasse Tyson TTS models sing Straight Outta Compton",https://www.reddit.com/r/deeplearning/comments/bgc2z1/p_trump_obama_jordan_peterson_and_neil_degrasse/,hanyuqn,1555993900,"This is a great demonstration of some of the different TTS models I've trained and how I can control style:

https://www.youtube.com/watch?v=SXTdnk7-2i0

These models were trained using my implementation of the papers ""Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis"" (https://arxiv.org/abs/1803.09017) and ""Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron"" (https://arxiv.org/abs/1803.09047).",2,2,False,self,,,,,
213,deeplearning,t5_2t5eh,2019-4-23,2019,4,23,21,bgfxjz,dragan.rocks,Deep Learning from Scratch to GPU: Weight Decay,https://www.reddit.com/r/deeplearning/comments/bgfxjz/deep_learning_from_scratch_to_gpu_weight_decay/,dragandj,1556023463,,0,12,False,default,,,,,
214,deeplearning,t5_2t5eh,2019-4-24,2019,4,24,1,bgif7o,mdpi.com,Text Classification Algorithms: A Survey,https://www.reddit.com/r/deeplearning/comments/bgif7o/text_classification_algorithms_a_survey/,kk7nc,1556036886,,0,2,False,default,,,,,
215,deeplearning,t5_2t5eh,2019-4-24,2019,4,24,2,bgj8t9,medium.com,Respected AI pioneer and visionary Nils John Nilsson passed away early this morning,https://www.reddit.com/r/deeplearning/comments/bgj8t9/respected_ai_pioneer_and_visionary_nils_john/,gwen0927,1556040955,,0,62,False,default,,,,,
216,deeplearning,t5_2t5eh,2019-4-24,2019,4,24,6,bglqc7,medium.com,"TensorFlow, PyTorch or MXNet? A comprehensive evaluation on NLP &amp; CV tasks with Titan RTX",https://www.reddit.com/r/deeplearning/comments/bglqc7/tensorflow_pytorch_or_mxnet_a_comprehensive/,gwen0927,1556053623,,0,10,False,default,,,,,
217,deeplearning,t5_2t5eh,2019-4-24,2019,4,24,6,bglznw,self.MachineLearning,[N] Google Colab now comes with free T4 GPUs,https://www.reddit.com/r/deeplearning/comments/bglznw/n_google_colab_now_comes_with_free_t4_gpus/,tlkh,1556054950,,2,19,False,default,,,,,
218,deeplearning,t5_2t5eh,2019-4-24,2019,4,24,10,bgoo1q,self.deeplearning,Help! I have to build a ROI pooling layer for my CNN based off Tensorflow.,https://www.reddit.com/r/deeplearning/comments/bgoo1q/help_i_have_to_build_a_roi_pooling_layer_for_my/,SenSyllable,1556070007,"I searched for ROI pooling layer on Tensorflow. But apparently, it's not there, there's one in Caff, but shifting the entire architecture onto Caff is... Difficult.
What are the other options I should look into?",1,1,False,self,,,,,
219,deeplearning,t5_2t5eh,2019-4-24,2019,4,24,15,bgri3t,i.redd.it,Query on annotation style,https://www.reddit.com/r/deeplearning/comments/bgri3t/query_on_annotation_style/,Mahe7,1556089051,,12,4,False,image,,,,,
220,deeplearning,t5_2t5eh,2019-4-24,2019,4,24,20,bgtdxg,medium.com,Achieving State Of The Art Results In Natural Language Processing - Part 1: Fundamental Concepts,https://www.reddit.com/r/deeplearning/comments/bgtdxg/achieving_state_of_the_art_results_in_natural/,lauram16_hello,1556104934,,0,19,False,default,,,,,
221,deeplearning,t5_2t5eh,2019-4-24,2019,4,24,20,bgtknx,medium.com,"Achieving State Of The Art Results In Natural Language Processing - Part 2: ELMo, BERT and MT-DNN",https://www.reddit.com/r/deeplearning/comments/bgtknx/achieving_state_of_the_art_results_in_natural/,lauram16_hello,1556106185,,0,3,False,default,,,,,
222,deeplearning,t5_2t5eh,2019-4-25,2019,4,25,0,bgw2cs,i-programmer.info,How Do Open Source Deep Learning Frameworks Stack Up?,https://www.reddit.com/r/deeplearning/comments/bgw2cs/how_do_open_source_deep_learning_frameworks_stack/,pmz,1556120122,,0,0,False,default,,,,,
223,deeplearning,t5_2t5eh,2019-4-25,2019,4,25,2,bgxfrf,self.deeplearning,Python code,https://www.reddit.com/r/deeplearning/comments/bgxfrf/python_code/,yohandp,1556126935,"Hey! I'm new in machine and deep learning stuffs and I'm learning without import libraries, on python.  I have an question about how calculate between hidden layers in a multiple y network. I'm trying to create an network with 4 entries, 6 hidden, 24 hidden and 8 exits.",4,0,False,self,,,,,
224,deeplearning,t5_2t5eh,2019-4-25,2019,4,25,4,bgyxax,expoundai.wordpress.com,Image Data Generators in Keras  My Journey with Deep Learning and Computer Vision,https://www.reddit.com/r/deeplearning/comments/bgyxax/image_data_generators_in_keras_my_journey_with/,msminhas93,1556134415,,0,1,False,default,,,,,
225,deeplearning,t5_2t5eh,2019-4-25,2019,4,25,6,bh04dh,coursera.org,Convolutional Neural Networks in TensorFlow | Coursera,https://www.reddit.com/r/deeplearning/comments/bh04dh/convolutional_neural_networks_in_tensorflow/,lopespm,1556140577,,2,27,False,default,,,,,
226,deeplearning,t5_2t5eh,2019-4-25,2019,4,25,9,bh2fkw,self.deeplearning,Question about Object Detection..!,https://www.reddit.com/r/deeplearning/comments/bh2fkw/question_about_object_detection/,GW_KIM,1556153731,"Hi, i am studying about object detection and have some question about it.

In Faster RCNN or YOLO, they predict a bounding boxes from training.

And i wonder if the bounding box information is unique along the classes or general along all objects.

In Yolo the predictions are form of S\*S\*(B\*5 +C). (S: grid, C: class probability, B: bounding boxes + confidence)

In the case, do 'B' have information about the class? (such as position or aspect ratio of the object in general scene)",2,3,False,self,,,,,
227,deeplearning,t5_2t5eh,2019-4-25,2019,4,25,16,bh5v5n,self.deeplearning,Automate the conversion of unstructured data (PDFs) to structured data,https://www.reddit.com/r/deeplearning/comments/bh5v5n/automate_the_conversion_of_unstructured_data_pdfs/,ak96,1556177912," 

Hey!

I  want to build a program which takes PDFs of electronic product  manuals  as input and converts it into structured data. An example of a  product  manual is this: [http://www8.hp.com/h20195/v2/GetDocument.aspx?docname=c05951186](http://www8.hp.com/h20195/v2/GetDocument.aspx?docname=c05951186)

But  it is not necessary that all product manuals will be of the same  format, they may vary based on company and product sub-category. So, how   do I build an AI engine to automate this: whenever a user uploads a  pdf  like this, the program should be able to extract the information  into structured format? How do I go about that and where do I start? I  guess,  it's necessary to use NLP. Any help or advice would be greatly   appreciated.

Thanks",1,6,False,self,,,,,
228,deeplearning,t5_2t5eh,2019-4-25,2019,4,25,17,bh6ax6,i.redd.it,Good for remote neural net? Nivada Quadro K5000 ++,https://www.reddit.com/r/deeplearning/comments/bh6ax6/good_for_remote_neural_net_nivada_quadro_k5000/,ErlingStagge,1556182010,,3,2,False,image,,,,,
229,deeplearning,t5_2t5eh,2019-4-25,2019,4,25,18,bh6ka3,self.deeplearning,Qutoable Quote,https://www.reddit.com/r/deeplearning/comments/bh6ka3/qutoable_quote/,tvganesh,1556184446,[removed],0,1,False,self,,,,,
230,deeplearning,t5_2t5eh,2019-4-25,2019,4,25,18,bh6nji,self.deeplearning,Quote of the year!,https://www.reddit.com/r/deeplearning/comments/bh6nji/quote_of_the_year/,tvganesh,1556185242,[removed],0,1,False,self,,,,,
231,deeplearning,t5_2t5eh,2019-4-25,2019,4,25,19,bh76tr,udemy.com,Data Science:Data Mining &amp; Natural Language Processing in R,https://www.reddit.com/r/deeplearning/comments/bh76tr/data_sciencedata_mining_natural_language/,systems4facility,1556189564,,0,1,False,default,,,,,
232,deeplearning,t5_2t5eh,2019-4-25,2019,4,25,20,bh7ag2,techindustan.com,The Contrast Machine Vs. Deep Learning,https://www.reddit.com/r/deeplearning/comments/bh7ag2/the_contrast_machine_vs_deep_learning/,technewsninja,1556190272,,1,0,False,default,,,,,
233,deeplearning,t5_2t5eh,2019-4-25,2019,4,25,20,bh7jnw,medium.com,Researchers find a way to reduce CNN training by 31%,https://www.reddit.com/r/deeplearning/comments/bh7jnw/researchers_find_a_way_to_reduce_cnn_training_by/,BettyWaihenya,1556192113,,0,1,False,default,,,,,
234,deeplearning,t5_2t5eh,2019-4-25,2019,4,25,20,bh7p8j,medium.com,Newly found variations in backpropagation can significantly improve model training,https://www.reddit.com/r/deeplearning/comments/bh7p8j/newly_found_variations_in_backpropagation_can/,BettyWaihenya,1556193186,,0,1,False,default,,,,,
235,deeplearning,t5_2t5eh,2019-4-25,2019,4,25,21,bh84d4,self.deeplearning,"Deep Learning Newbie, building a PC, considering Amd CPU",https://www.reddit.com/r/deeplearning/comments/bh84d4/deep_learning_newbie_building_a_pc_considering/,Atralb,1556195875,"Hi guys,

&amp;#x200B;

I'm a new player in deep learning and wanted to ask experienced guys in DL who use AMD's products. I have bought an RTX 2070 as GPU. And I am considering getting a 2nd GPU in the future.

I must say as a disclaimer that this is my first time building a Desktop PC. (A friend will be there to help me put it toghether).

So, I had a couple questions I wanted to ask you, if you please. :)  


* I don't really understand when I need ROCm ? Is it only when I use an Amd GPU or also when having an Amd CPU even if my GPU is Nvidia ?  

* Here are prices in my area :  
\- 2600 =&gt; 162 EUR ($180)  
\- 2600X =&gt; 190 EUR ($211)  
\- 2700 =&gt; 223 EUR ($248)  
\- 2600X =&gt; 291 EUR ($323)  
Regarding my GPU and the fact that I want to be able to run a second one, I thought the 2700 was the best deal here, cause 8 cores (I want to be able to do other things while training is processing, like use it as a server, coding, editing some shit). But since I saw that 2 cores were the max amount needed by GPU, do you think 6 cores will be ok with 2 GPU + my additional usages ?And additionally since clock rate is higher on 2600X than 2700, maybe performance would be even better ? What is your take on this ?

Furthermore, am I aiming at the right CPU level regarding my GPU needs ? if I bought a 2080 ti for the second GPU, would it still be ok ?

* I'm quite lost when choosing a Motherboard. Even if I read a lot about PCIe Lanes and such, different storage sockets, I'm hesistating regarding questions like :what the heck are PCIe x16 in 8x or 4x mode ?? Do I have to consider they are simply the latter ?will the 2 gpus fit in the MOBO ?will it also fit with a Noctua fan (like the double Noctua NH-U9 TR4-SP3) ?How much SATA ports do I need (I bought an Evo Plus 970 1TB and want to have at least 10 TB HDD) ?  
Regarding all that, do you think the ROG STRIX B450-F GAMING is a good deal ?  
Indeed if I looked correctly :  
\-  this is the cheapest AM4 Motherboard that has 2 PCIe x16 really acting as x16 (is it right ?), and I want to have x16 for each GPU  
\- The two PCIe x16 seem to be far enough for the two GPUs, what do you think ? ([look at it here](https://c1.neweggimages.com/NeweggImage/ProductImageCompressAll1280/13-119-140-V07.jpg))  
\- I've got M.2 socket for the Evo Plus. Then it says there are 2 SATA slots. I assume buying 2 big 8-10 Tb HDDs will be okay, right ?  
\- It seems to be a pretty good motherboard for the price, according to what I read. Do you confirm ?  


Thanks a lot in advance for your help :)",7,1,False,self,,,,,
236,deeplearning,t5_2t5eh,2019-4-26,2019,4,26,0,bha0th,self.deeplearning,Inquiry about best deep learning courses using Tensorflow,https://www.reddit.com/r/deeplearning/comments/bha0th/inquiry_about_best_deep_learning_courses_using/,arnavc,1556206417,"I have been doing deep learning since quite a while, but tensorflow still seems too convoluted to learn.
For example I want to build a seq2seq model in tensorflow, I have done my research and understood the architecture, but I have no idea of how to start building it in tensorflow or how the seq2seq models in tensorflow work.
Could anyone please recommend good tutorials/courses/articles for the same?",4,5,False,self,,,,,
237,deeplearning,t5_2t5eh,2019-4-26,2019,4,26,2,bhb5hr,self.deeplearning,Andrej Karpathy's recipe for training DNNs,https://www.reddit.com/r/deeplearning/comments/bhb5hr/andrej_karpathys_recipe_for_training_dnns/,intvar,1556212178,[https://karpathy.github.io/2019/04/25/recipe/](https://karpathy.github.io/2019/04/25/recipe/),1,70,False,self,,,,,
238,deeplearning,t5_2t5eh,2019-4-26,2019,4,26,2,bhb6js,self.deeplearning,Crowd Behavior Analysis,https://www.reddit.com/r/deeplearning/comments/bhb6js/crowd_behavior_analysis/,crazy_lazy_life,1556212318,"So I have tried to build a video classification model that will be able to predict from crowd video footages if the crowd behavior is violent/non-violent. I have this github repo up, containing the details. Any suggestion at improvements is highly appreciated. 
https://github.com/crazylazylife/Crowd-Behaviour-Analysis",0,2,False,self,,,,,
239,deeplearning,t5_2t5eh,2019-4-26,2019,4,26,4,bhcgs6,medium.com,Close Your Eyes and Ill Scan You: Chinese Face Payment System Vulnerabilities,https://www.reddit.com/r/deeplearning/comments/bhcgs6/close_your_eyes_and_ill_scan_you_chinese_face/,gwen0927,1556218942,,0,1,False,default,,,,,
240,deeplearning,t5_2t5eh,2019-4-26,2019,4,26,4,bhcp23,self.deeplearning,Can anyone recommend a tutorial for segmentation using deep learning?,https://www.reddit.com/r/deeplearning/comments/bhcp23/can_anyone_recommend_a_tutorial_for_segmentation/,mahadmajeed,1556220134,"Hi, I am really a beginner. I have trained some classifiers in keras but that's pretty much it. I am stuck on a segmentation task. I have png masks of the images but I can't find a good tutorial for how to train using this data. Almost all tutorials recommend making JSON labels. Any help in this regard would be much appreciated. Thanks.",2,1,False,self,,,,,
241,deeplearning,t5_2t5eh,2019-4-26,2019,4,26,4,bhcxao,self.deeplearning,How ro modify the generator of a GAN?,https://www.reddit.com/r/deeplearning/comments/bhcxao/how_ro_modify_the_generator_of_a_gan/,gaurav__1998,1556221360,"How to tell a neural network, while it's training, to emphasize on certain features? Like I want to tell my generator in GAN to generate pictures of a man with black eyes how would I do that?",2,3,False,self,,,,,
242,deeplearning,t5_2t5eh,2019-4-26,2019,4,26,6,bhdz4o,openai.com,MuseNet,https://www.reddit.com/r/deeplearning/comments/bhdz4o/musenet/,lopespm,1556226788,,0,5,False,default,,,,,
243,deeplearning,t5_2t5eh,2019-4-26,2019,4,26,12,bhhsvd,self.deeplearning,Deep dreams vs neural style transfer,https://www.reddit.com/r/deeplearning/comments/bhhsvd/deep_dreams_vs_neural_style_transfer/,Believinginself,1556250054,"Hey guys! After attending a lecture about them, I am a bit confused in the 2 terminologies. I thought both are same but the Prof used both differently. Could someone explain the difference in the two. Thanks.",2,3,False,self,,,,,
244,deeplearning,t5_2t5eh,2019-4-26,2019,4,26,14,bhipcx,self.deeplearning,Convert xml label to csv,https://www.reddit.com/r/deeplearning/comments/bhipcx/convert_xml_label_to_csv/,nicetryho,1556256623,I used labelimg to label my photos. I have the annotations saved in another folder and they are xml format. I need to use these annotations in Keras to do transfer learning with the data. From what I have read the labels need to be in csv format. How can I make this work?,1,2,False,self,,,,,
245,deeplearning,t5_2t5eh,2019-4-26,2019,4,26,18,bhkej2,self.deeplearning,"Just starting out, Learned Python .. now what?",https://www.reddit.com/r/deeplearning/comments/bhkej2/just_starting_out_learned_python_now_what/,han1337,1556271376,"I am just starting out with Deep Learning.

Till now I invested some time to learn Python3, which was straight forward.

&amp;#x200B;

From what I understand, currently it is looking like TF will get Keras API.

going the PyTorch Route would be the other option.

&amp;#x200B;

I have been looking into the following Courses for my Next Step:

&amp;#x200B;

* ""Deep Learning A-Z: Hands-On Artificial Neural Networks"" Course on Udemy.
*  Machine Learning Certification by Stanford University (Coursera)
*  Deep Learning Certification by [deeplearning.ai](https://deeplearning.ai) (Coursera)

&amp;#x200B;

Which one would you recommend and why, as of this date?

Which of the Courses is more Keras focused?

&amp;#x200B;

I was thinking of concentraing on Keras.

And once TF2 is not in alpha state anymore, looking into that as well.",9,8,False,self,,,,,
246,deeplearning,t5_2t5eh,2019-4-26,2019,4,26,19,bhklca,self.deeplearning,Build a deep learning image search to find images on the web,https://www.reddit.com/r/deeplearning/comments/bhklca/build_a_deep_learning_image_search_to_find_images/,gv47,1556273023,"I need to build an deep learning image search to find images containing buildings, and ideally use it to search images on the web - is this feasible?

Most examples appear to run the model on their sample training datasets - what is the feasibility of using the web to run the search on?",0,4,False,self,,,,,
247,deeplearning,t5_2t5eh,2019-4-26,2019,4,26,21,bhlz4e,self.deeplearning,Creating a realistic deepfake with ML,https://www.reddit.com/r/deeplearning/comments/bhlz4e/creating_a_realistic_deepfake_with_ml/,hanyuqn,1556283521,"This is a realistic deepfake I created for Ben Shapiro using my text-to-speech model to synthesise speech and my method for transferring facial expressions:

https://www.youtube.com/watch?v=sgkiS6wUZ98",8,15,False,self,,,,,
248,deeplearning,t5_2t5eh,2019-4-26,2019,4,26,23,bhn8m6,self.deeplearning,"CPU's got only 24 PCIe lanes, is it problematic for 2 GPUs ?",https://www.reddit.com/r/deeplearning/comments/bhn8m6/cpus_got_only_24_pcie_lanes_is_it_problematic_for/,Atralb,1556290646,"Hu guys,

&amp;#x200B;

So I've bought the AMD Ryzen 7 2700 for my DL rig. I saw that out of its 32 PCIe lanes only 24 are really usable. So I guess, I will be able to use the first GPU with x16 and the other with 8x. (For reference, I'm considering RTX 2070)  


Will it cause performance drops for the second one. I've seen an article where the author explains less PCIe lanes actually only cause little drops in performance (3% from x4 to x16 according to [source](https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/), ""CPU and PCI-Express"" part), but I'm not sure of it.  


What is your view on this matter ?  


Thanks,",0,1,False,self,,,,,
249,deeplearning,t5_2t5eh,2019-4-27,2019,4,27,0,bhnkok,self.deeplearning,Probability and Information theory Book for deep learning,https://www.reddit.com/r/deeplearning/comments/bhnkok/probability_and_information_theory_book_for_deep/,bikanation,1556292366,Can anybody recommend for me a book where i can learn the needed concepts to complete my deep learning research ? (other than deeplearning book by ian goodfellow),4,3,False,self,,,,,
250,deeplearning,t5_2t5eh,2019-4-27,2019,4,27,1,bho9y0,self.deeplearning,Interacting with the latent space of an AutoEncoder,https://www.reddit.com/r/deeplearning/comments/bho9y0/interacting_with_the_latent_space_of_an/,HeavyStatus4,1556296011,"I recently made a simple tool for interacting with the latent space of an autoencoder. This helps you to interpret the role played by each neuron at the bottleneck by visualizing the changes in the reconstructed output.

Project : [https://github.com/koulanurag/visTorch](https://github.com/koulanurag/visTorch)

&amp;#x200B;

I hope this could be useful to others as well.",2,25,False,self,,,,,
251,deeplearning,t5_2t5eh,2019-4-27,2019,4,27,4,bhqfui,self.MachineLearning,[D] Im writing a book: Neural Networks with Swift for TensorFlow,https://www.reddit.com/r/deeplearning/comments/bhqfui/d_im_writing_a_book_neural_networks_with_swift/,rahulbhalley,1556307189,,0,0,False,default,,,,,
252,deeplearning,t5_2t5eh,2019-4-27,2019,4,27,6,bhrsn7,self.deeplearning,Efficient Optimizer,https://www.reddit.com/r/deeplearning/comments/bhrsn7/efficient_optimizer/,ZeroMaxinumXZ,1556314672,I'm looking for an efficient Keras Optimizer for an autoencoder so that I can use it to get the loss between two states of an agent and then use it with the agent to produce curiosity. I just don't want to burn my GPU out. Any suggestions?,3,2,False,self,,,,,
253,deeplearning,t5_2t5eh,2019-4-27,2019,4,27,11,bhufpj,self.deeplearning,Floating problems in csv dataset,https://www.reddit.com/r/deeplearning/comments/bhufpj/floating_problems_in_csv_dataset/,weberfrases,1556331159,"Hello Friends,
I have a csv dataset that is increasing for minute, but when have new lines, the backward lines like 5.789123 becomes a 5.789129999999999999999

Some help?
Thanks",1,1,False,self,,,,,
254,deeplearning,t5_2t5eh,2019-4-27,2019,4,27,11,bhujoo,youtube.com,How to Create Machine Learning Models Without Code (AutoML),https://www.reddit.com/r/deeplearning/comments/bhujoo/how_to_create_machine_learning_models_without/,tim_macgyver,1556331941,,1,0,False,image,,,,,
255,deeplearning,t5_2t5eh,2019-4-27,2019,4,27,11,bhunzr,youtube.com,"Interested in Artificial Intelligence, Machine Learning, Computer Vision, or NLP? Check out this channel for excellent, well-explained, video tutorials.",https://www.reddit.com/r/deeplearning/comments/bhunzr/interested_in_artificial_intelligence_machine/,DiscoverAI,1556332797,,0,22,False,default,,,,,
256,deeplearning,t5_2t5eh,2019-4-27,2019,4,27,13,bhvbhu,self.deeplearning,Problem with Generative Adversarial Networks,https://www.reddit.com/r/deeplearning/comments/bhvbhu/problem_with_generative_adversarial_networks/,A01u,1556337676,"Hi, I've just started learning about Generative Adversarial Networks and I created this NN that attempts to generate a parabola. I was wondering if anyone could take a look at my jupyter notebook and tell me why my beginning iteration has my generated data clustered at the minimum of the parabola.

Here is the notebook:

 [https://github.com/Pie31415/GAN/blob/master/Simple%20GAN.ipynb](https://github.com/Pie31415/GAN/blob/master/Simple%20GAN.ipynb)",1,1,False,self,,,,,
257,deeplearning,t5_2t5eh,2019-4-27,2019,4,27,13,bhvoht,self.MachineLearning,[D] Im writing a book: Neural Networks with Swift for TensorFlow,https://www.reddit.com/r/deeplearning/comments/bhvoht/d_im_writing_a_book_neural_networks_with_swift/,rahulbhalley,1556340554,,0,1,False,default,,,,,
258,deeplearning,t5_2t5eh,2019-4-27,2019,4,27,13,bhvpns,self.deeplearning,PC build thoughts?,https://www.reddit.com/r/deeplearning/comments/bhvpns/pc_build_thoughts/,stronomia,1556340820,"Hi,

&amp;#x200B;

I'm planning to build a PC so that I can start experimenting with deep learning. Here's a build I've finalised:

&amp;#x200B;

PCPartPicker Part List: [https://pcpartpicker.com/list/zd3X8Y](https://pcpartpicker.com/list/zd3X8Y)

&amp;#x200B;

CPU: AMD - Ryzen 7 2700X 3.7 GHz 8-Core Processor  ($292.89 @ OutletPC) 

Motherboard: Gigabyte - B450 AORUS ELITE ATX AM4 Motherboard  ($99.99 @ Amazon) 

Memory: Corsair - Vengeance LPX 16 GB (2 x 8 GB) DDR4-3000 Memory  ($78.99 @ Amazon) 

Storage: Samsung - 860 QVO 1 TB 2.5"" Solid State Drive  ($117.99 @ Amazon) 

Video Card: Gigabyte - GeForce RTX 2080 8 GB WINDFORCE Video Card  ($699.99 @ Amazon) 

&amp;#x200B;

I'm planning to go with a 650W power supply for this, which I think would be more than adequate.

&amp;#x200B;

What do you guys think? I'm primarily going to start my journey by trying to implement some GAN papers, e.g. DTN. Would this be powerful enough?",0,1,False,self,,,,,
259,deeplearning,t5_2t5eh,2019-4-28,2019,4,28,2,bi1g9z,self.deeplearning,Advanced Deep Learning Courses,https://www.reddit.com/r/deeplearning/comments/bi1g9z/advanced_deep_learning_courses/,Msadat97,1556386410,"A few months ago, I saw this post on Reddit.  
[Ph.D. level courses:](https://www.reddit.com/r/MachineLearning/comments/51qhc8/phdlevel_courses/)[ Machine Learning](https://www.reddit.com/r/MachineLearning/comments/51qhc8/phdlevel_courses/)  
I am wondering if anyone can mention such courses in deep learning. 

This course from CMU is the course in deep learning I've ever seen.

&amp;#x200B;

[http://deeplearning.cs.cmu.edu/](http://deeplearning.cs.cmu.edu/)",21,35,False,self,,,,,
260,deeplearning,t5_2t5eh,2019-4-28,2019,4,28,6,bi3rci,medium.com,Creating Bitcoin trading bots that dont lose money using deep RL,https://www.reddit.com/r/deeplearning/comments/bi3rci/creating_bitcoin_trading_bots_that_dont_lose/,notadamking,1556400062,,0,0,False,default,,,,,
261,deeplearning,t5_2t5eh,2019-4-28,2019,4,28,7,bi4av4,youtube.com,How Recurrent Neural Networks Work: An Application and Algorithm-based Approach,https://www.reddit.com/r/deeplearning/comments/bi4av4/how_recurrent_neural_networks_work_an_application/,DiscoverAI,1556403340,,0,3,False,default,,,,,
262,deeplearning,t5_2t5eh,2019-4-28,2019,4,28,7,bi4cuq,youtube.com,Simply Explained: How Neural Networks Work,https://www.reddit.com/r/deeplearning/comments/bi4cuq/simply_explained_how_neural_networks_work/,DiscoverAI,1556403676,,0,2,False,default,,,,,
263,deeplearning,t5_2t5eh,2019-4-28,2019,4,28,7,bi4gfc,youtube.com,How Convolutional Neural Networks Work: An Intuitive Approach,https://www.reddit.com/r/deeplearning/comments/bi4gfc/how_convolutional_neural_networks_work_an/,DiscoverAI,1556404281,,0,0,False,default,,,,,
264,deeplearning,t5_2t5eh,2019-4-28,2019,4,28,7,bi4q1n,expoundai.wordpress.com,Maxima vs Minima and Global vs Local  Derivative Test for finding them,https://www.reddit.com/r/deeplearning/comments/bi4q1n/maxima_vs_minima_and_global_vs_local_derivative/,msminhas93,1556405911,,2,1,False,default,,,,,
265,deeplearning,t5_2t5eh,2019-4-28,2019,4,28,15,bi8d4g,self.deeplearning,how to make my model smaller?,https://www.reddit.com/r/deeplearning/comments/bi8d4g/how_to_make_my_model_smaller/,dalalaa,1556432237,"I have a yolov3 model for text\_detection, how can I make it smaller enough to run on CPU without retraining?",0,1,False,self,,,,,
266,deeplearning,t5_2t5eh,2019-4-28,2019,4,28,19,bi9xj9,self.deeplearning,"I have started learning deep learning. I am taking various courses, so should I go for pytorch or tensorflow 2.0?",https://www.reddit.com/r/deeplearning/comments/bi9xj9/i_have_started_learning_deep_learning_i_am_taking/,datavinci,1556447550,"Apologies if already asked, but I wanted latest takes of people on tensorflow 2.0. I heard it is much better than the previous versions. So wanted to get an opinion of experienced people on this.",22,23,False,self,,,,,
267,deeplearning,t5_2t5eh,2019-4-29,2019,4,29,1,bictck,self.deeplearning,What next ?,https://www.reddit.com/r/deeplearning/comments/bictck/what_next/,smukh98,1556467791,"Hey all,I am a deep learning enthusiast and I have just completed a course on it. I am confused on what to do next. I have heard fee people writing research papers. 

Can you guys help by telling which conferences to look for and how can I get into research field and also instructing how to a write a research paper.

Thanks in advance",7,0,False,self,,,,,
268,deeplearning,t5_2t5eh,2019-4-29,2019,4,29,2,bidq1s,datastuff.tech,3 Machine Learning Books that Helped me Level Up,https://www.reddit.com/r/deeplearning/comments/bidq1s/3_machine_learning_books_that_helped_me_level_up/,papereraser,1556472890,,1,37,False,default,,,,,
269,deeplearning,t5_2t5eh,2019-4-29,2019,4,29,13,bikxlg,blockdelta.io,ReWork AI Events in the coming Months - BlockDelta,https://www.reddit.com/r/deeplearning/comments/bikxlg/rework_ai_events_in_the_coming_months_blockdelta/,BlockDelta,1556512609,,0,4,False,default,,,,,
270,deeplearning,t5_2t5eh,2019-4-29,2019,4,29,14,bilpuf,self.deeplearning,Advice on which cloud to use.,https://www.reddit.com/r/deeplearning/comments/bilpuf/advice_on_which_cloud_to_use/,Pik000,1556515512,"While writing this is seem very obvious what I should do but wondering if anyone can offer advice etc.

&amp;#x200B;

Im currently building a webapp which utilises Google Vision API, which will then store info returned into a database and then use tensorflow serving for a NN that I have coded and return all this data to SaaS backend. The majority of developers that I talk too only really have experience in AWS. My thoughts are to try and stay in GCP due to the API usage but I am unsure if as people have AWS expernice I should just use that and deal with the transfer.

&amp;#x200B;

Any help, comments etc would be great.",3,5,False,self,,,,,
271,deeplearning,t5_2t5eh,2019-4-29,2019,4,29,21,bip75a,self.deeplearning,30 Frequently asked Deep Learning Interview Questions and Answers,https://www.reddit.com/r/deeplearning/comments/bip75a/30_frequently_asked_deep_learning_interview/,ritesh1928,1556542760,"Deep Learning is one of the fastest growing fields of information technology. It is a set of techniques that permits machines to predict outputs from a layered set of inputs. Deep Learning is being embraced by companies all over the world, and anyone with software and data skills can find numerous job opportunities in this field.",0,0,False,self,,,,,
272,deeplearning,t5_2t5eh,2019-4-29,2019,4,29,23,bipu7x,self.deeplearning,"A PyTorch implementation of ""Semi-Supervised Graph Classification: A Hierarchical Graph Perspective"" (WWW 2019)",https://www.reddit.com/r/deeplearning/comments/bipu7x/a_pytorch_implementation_of_semisupervised_graph/,benitorosenberg,1556546601,"&amp;#x200B;

https://i.redd.it/hsoze1lhm7v21.jpg

GitHub: [https://github.com/benedekrozemberczki/SEAL-CI](https://github.com/benedekrozemberczki/SEAL-CI)

Paper: [https://arxiv.org/pdf/1904.05003.pdf](https://arxiv.org/pdf/1904.05003.pdf)

Abstract:

Node classification and graph classification are two graph learning  problems that predict the class label of a node and the class label of a  graph respectively. A node of a graph usually represents a real-world  entity, e.g., a user in a social network, or a protein in a  protein-protein interaction network. In this work, we consider a more  challenging but practically useful setting, in which a node itself is a  graph instance. This leads to a hierarchical graph perspective which  arises in many domains such as social network, biological network and  document collection. For example, in a social network, a group of people  with shared interests forms a user group, whereas a number of user  groups are interconnected via interactions or common members. We study  the node classification problem in the hierarchical graph where a \`node'  is a graph instance, e.g., a user group in the above example. As labels  are usually limited in real-world data, we design two novel  semi-supervised solutions named Semi-supervised graph classification via  Cautious/Active Iteration (or SEAL-C/AI in short). SEAL-C/AI adopt an  iterative framework that takes turns to build or update two classifiers,  one working at the graph instance level and the other at the  hierarchical graph level. To simplify the representation of the  hierarchical graph, we propose a novel supervised, self-attentive graph  embedding method called SAGE, which embeds graph instances of arbitrary  size into fixed-length vectors. Through experiments on synthetic data  and Tencent QQ group data, we demonstrate that SEAL-C/AI not only  outperform competing methods by a significant margin in terms of  accuracy/Macro-F1, but also generate meaningful interpretations of the  learned representations.",0,36,False,self,,,,,
273,deeplearning,t5_2t5eh,2019-4-29,2019,4,29,23,biq7x9,self.deeplearning,Are LSTM RNN still relevant?,https://www.reddit.com/r/deeplearning/comments/biq7x9/are_lstm_rnn_still_relevant/,NikolasTs,1556548744," Hey guys,

I  am relatively new to deep learning and I've mostly worked with CNNs. I  always thought that RNNs are very powerful and that are here to stay,  especially in NLP. However, today I read this article:

[https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0](https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0)

where it basically states that LSTM is not that good today. Is this true?",12,3,False,self,,,,,
274,deeplearning,t5_2t5eh,2019-4-30,2019,4,30,0,biqpy7,self.deeplearning,Free cloud GPU credits for deep learning,https://www.reddit.com/r/deeplearning/comments/biqpy7/free_cloud_gpu_credits_for_deep_learning/,whitezl0,1556551398,"Hi, I am offering free credits for 1080Ti GPU instances for deep learning purposes.
I am working on [https://www.tensorpad.com/](https://www.tensorpad.com/?utm_source=reddit&amp;utm_campaign=deeplearning)  developing cloud infrastructure for machine learning.
Part of our computational capacity is idle; hence, were offering credits at a free and discounted rate, so that data scientists can benefit from the resources available, and work on neural networks.
Specs:
	16GB of RAM, 4 CPUs, 1080Ti GPU
	JupyterLab environment with access to the terminal
	Pre-installed Tensorflow, Keras, and other ML frameworks (software versions https://docs.tensorpad.com/jobs_env/)
You can access the free credits by signing up ([https://dashboard.tensorpad.com/](https://dashboard.tensorpad.com/signup?utm_source=reddit&amp;utm_campaign=deeplearning) and redeeming ""REDDIT200"" promo code in the Billing tab ([https://dashboard.tensorpad.com/billing](https://dashboard.tensorpad.com/billing?utm_source=reddit&amp;utm_campaign=deeplearning)).
For any questions, please contact us here, through support@tensorpad.com, or the Intercom on the site.",0,1,False,self,,,,,
275,deeplearning,t5_2t5eh,2019-4-30,2019,4,30,0,biqu3p,self.deeplearning,Medical Images Application,https://www.reddit.com/r/deeplearning/comments/biqu3p/medical_images_application/,rotyweb,1556552002,"Hey,  I wanted to make an application related to dermatology(Find the skin  disease and recommend treatment or dermatologist accordingly). Then I  came across this [dataset](http://www.dermnet.com/dermatology-pictures-skin-disease-pictures/)  which contains all kind of disease images. So is it easy to identify  disease on the basis of just  dataset and  knowledge of CNN???

P.S:I  have worked only on couple of datasets(MNIST, Dog&amp; Cat, CIFAR-10)  and according to me, we just need to tune hyper-parameters, Good ML Algorithm and find good model for our dataset &amp; our work  is done. Am I right or there are other things too in order to train a model and good results??",1,1,False,self,,,,,
276,deeplearning,t5_2t5eh,2019-4-30,2019,4,30,2,birwid,dragan.rocks,Deep Learning from Scratch to GPU: Momentum,https://www.reddit.com/r/deeplearning/comments/birwid/deep_learning_from_scratch_to_gpu_momentum/,dragandj,1556557403,,0,0,False,default,,,,,
277,deeplearning,t5_2t5eh,2019-4-30,2019,4,30,3,bist8p,self.deeplearning,Easier to get jobs in production or research side?,https://www.reddit.com/r/deeplearning/comments/bist8p/easier_to_get_jobs_in_production_or_research_side/,74throwaway,1556562053,"I currently work in image processing using Matlab. I have worked with C++ and CNNs in Python briefly before. From what I understand, most DL jobs fall under the production or research side. In the production roles, C++ is more important, whereas in Python/Tensorflow/Keras/Pytorch are more important in the research roles

Since I don't have much work experience, which types of roles should I aim for? I'm guessing it's easier to get hired at large companies. I'm guessing I should aim for the production roles because the research roles are mostly reserved for just PhDs? I only have an MS

So to get a job at a large company, should I focus on improving my C++? Or use my time to work on projects that demonstrate mastery of DL concepts such as using Variational Autoencoders, GANs, Reinforcement Learning, etc?",2,3,False,self,,,,,
278,deeplearning,t5_2t5eh,2019-4-30,2019,4,30,3,bit2ah,self.deeplearning,"Grokking Deep Learning by Andrew Trask , possible critical errors in chapters 8 and 9 ?",https://www.reddit.com/r/deeplearning/comments/bit2ah/grokking_deep_learning_by_andrew_trask_possible/,webman19,1556563351,"I was planning to buy the deep learning book , but i saw a review on amazon stating about major flaws in code snippets in the 8th chapter and onward where  activation functions have been wrongly written , thus inaccurate conclusions.

I am a novice in deep learning . I want to know if there are any actual merits to his claims. I'll post the  review.

&amp;#x200B;

 Chapter 8.  
1. The very first code snippet is a simple network that learns from MNIST dataset how to decipher handwritten digits. Its test accuracy is \~70% at iteration 349, and this low number is supposed to show how easily neural network overfits. The real problem though is that this low test accuracy is the result of incorrect implementation of the 'relu2deriv' function that is used in backpropagation. With the fixed function, test accuracy of the same network reaches \~82%. Overfitting is still there, but 82% for fairly simple example is not that bad. Unfortunately all other examples in chapter 8 and 9 that use relu and relu2deriv functions use the same erroneous implementation and are not valid.  
2. To reduce overfitting dropout technique is applied. While high level arguments in the books are clear and valid, dropout implementation is problematic on several levels. First of all, dropout example is incomparable to non-dropout one since they have different layout - the hidden layer size was increased from 40 to 100 nodes and iteration count was decreased from 350 to 300. Moreover, the last logged iteration now is 290 instead of 349. For some reason, these changes were not announced in the text. Secondly, even if we compare two performances, the boost of \~10% that a reader can see should be explained as the result of incorrect implementation of 'relu2deriv'. Despite of what text is saying, dropout in this example is just trying to curb effects of that mistake. If you implement relu2deriv correctly, set up hidden layer nodes count to 100 and iterations to 290, you will not see much difference between non-dropout (88.88% peak test accuracy, 87.6% final test accuracy) and dropout versions (88.09% and 87.34% respectively). So either a reader will be misguided by the book's examples to think that dropout gives an enormous boost, or disillusioned, s/he will be left wondering and searching elsewhere what dropout real efficiency is and if this implementation is correct.  
3. Batch example is just plain wrong. The book says that batched network will be updating weights once per batch, but the implementation keeps updating it with each row of input data. What is really perplexing is that the network wouldn't work with such loop logic. Instead of doing the right thing and fixing the latter, the code divides values of layer\_2\_delta by batch size. And still neither accuracy nor speed are improved. If we implement it correctly with the proper looping logic and proper 'relu2deriv' implementation, we will not get much improvements for test accuracy, but training will be done much faster.  
4. So bottom line is that all improvements in chapter 8 were done just to battle the 'original sin' - incorrect 'relu2deriv' function. With this function fixed and increased number of nodes in the hidden layer, the very fist simple network with relu activation function will perform as good as the network with dropout and batching. The only gain was in speed, and only with the properly implemented batching logic. Of course, all this says nothing about how good dropout is or what the real power of batching is - a reader must go and find the answers in some other sources.  
Chapter 9.  
The biggest part of the chapter is the explanation of how different activation functions can be used in different contexts for greater accuracy. While on theoretical level everything looks valid, code implementation is again misleading.  
1. The same as in chapter 8, it uses a divisor when calculating layer\_2\_delta. While in chapter 8 it was necessary because of the incorrect loop implementation, here it is absolutely uncalled for. The loop this time is correct, but divisor is actually became larger since now it's a product of batch size and layer\_2.shape\[0\], or 100\*100 = 10000. Why? No explanation. Also, alpha is set up unusually high as 2. So, in order to fix that, one has to get rid of the divisor and divide alpha by the same value of 10000. Test accuracy will not change (0.8701 at iteration 290), but the network logic will not be dependent on batch size anymore.  
2. Still, whether's it's the book's version or the fixed one, accuracy doesn't go above 87%. It's still less than 88% of 'plain vanilla' relu example, granted that relu2deriv is implemented properly and hidden nodes count is 100. So all the improvements were in vain - and yes, I realize that test accuracy is not the only parameter to look at. But in the book it's a decisive one, and there's no gain in it from more complex tanh/softmax example.  
3. Just out of curiosity I tried to use properly implemented relu/relu2deriv functions in the fixed example, while keeping softmax function in place. This is first time we can see any gain in accuracy. It centers around 89% which is 1% gain to plain vanilla version. 

&amp;#x200B;

&amp;#x200B;

If so please encourage the author to make necessary fixes. Thank you.",2,2,False,self,,,,,
279,deeplearning,t5_2t5eh,2019-4-30,2019,4,30,3,bit5b6,mdpi.com,| Free Full-Text | Text Classification Algorithms: A Survey,https://www.reddit.com/r/deeplearning/comments/bit5b6/free_fulltext_text_classification_algorithms_a/,kk7nc,1556563788,,0,1,False,default,,,,,
280,deeplearning,t5_2t5eh,2019-4-30,2019,4,30,7,bivi1l,self.deeplearning,How is training a CNN using a validation set different from training a basic neural network with hidden layers,https://www.reddit.com/r/deeplearning/comments/bivi1l/how_is_training_a_cnn_using_a_validation_set/,hellomotorcycle,1556575944,"In regards to training for image classification:

How is training using a validation set in CNNs (mini batch gradient descent) different from using a validation set(s) in a neural network

Do they both use k-nearest neighbor or cross validation...etc?  Every time I read about this I get even more confused.",3,1,False,self,,,,,
281,deeplearning,t5_2t5eh,2019-4-30,2019,4,30,9,bix60o,self.deeplearning,Understanding Convolution in Neural Net,https://www.reddit.com/r/deeplearning/comments/bix60o/understanding_convolution_in_neural_net/,jai00271,1556585786,Convolution Neural Network: Background &amp; Basics https://dev.to/jai00271/background-basics-21bl,0,1,False,self,,,,,
282,deeplearning,t5_2t5eh,2019-4-30,2019,4,30,11,biy03k,self.deeplearning,State of the art in accurate object tracking &amp; detection,https://www.reddit.com/r/deeplearning/comments/biy03k/state_of_the_art_in_accurate_object_tracking/,throwaway_8320,1556590842,[removed],0,1,False,self,,,,,
283,deeplearning,t5_2t5eh,2019-4-30,2019,4,30,13,biz4sw,mihaileric.com,Beginner's Deep Dive Into Neural Networks,https://www.reddit.com/r/deeplearning/comments/biz4sw/beginners_deep_dive_into_neural_networks/,MusingEtMachina,1556598309,,0,10,False,default,,,,,
284,deeplearning,t5_2t5eh,2019-4-30,2019,4,30,18,bj155g,linkedin.com,"I highly recommend the Cornell University's ""Machine Learning for Intelligent Systems (CS4780/ CS5780)"" course taught by Associate Professor Kilian Q. Weinberger",https://www.reddit.com/r/deeplearning/comments/bj155g/i_highly_recommend_the_cornell_universitys/,aiforworld2,1556615121,,6,25,False,default,,,,,
285,deeplearning,t5_2t5eh,2019-4-30,2019,4,30,19,bj1vjq,self.deeplearning,Salient Object Detection in the Deep Learning Era: An In-Depth Survey,https://www.reddit.com/r/deeplearning/comments/bj1vjq/salient_object_detection_in_the_deep_learning_era/,HzFU,1556621415,"[https://arxiv.org/abs/1904.09146](https://arxiv.org/abs/1904.09146)

&amp;#x200B;

All the saliency prediction maps, our constructed dataset with annotations, and codes for evaluation are made publicly available at [https://github.com/wenguanwang/SODsurvey](https://github.com/wenguanwang/SODsurvey)

&amp;#x200B;

As an important problem in computer vision, salient object detection (SOD) from images has been attracting an increasing amount of research effort over the years. Recent advances in SOD, not surprisingly, are dominantly led by deep learning-based solutions (named deep SOD) and reflected by hundreds of published papers. To facilitate the in-depth understanding of deep SODs, in this paper we provide a comprehensive survey covering various aspects ranging from algorithm taxonomy to unsolved open issues. In particular, we first review deep SOD algorithms from different perspectives including network architecture, level of supervision, learning paradigm and object/instance level detection. Following that, we summarize existing SOD evaluation datasets and metrics. Then, we carefully compile a thorough benchmark results of SOD methods based on previous work, and provide detailed analysis of the comparison results. Moreover, we study the performance of SOD algorithms under different attributes, which have been barely explored previously, by constructing a novel SOD dataset with rich attribute annotations. We further analyze, for the first time in the field, the robustness and transferability of deep SOD models w.r.t. adversarial attacks. We also look into the influence of input perturbations, and the generalization and hardness of existing SOD datasets. Finally, we discuss several open issues and challenges of SOD, and point out possible research directions in future.",0,2,False,self,,,,,
286,deeplearning,t5_2t5eh,2019-4-30,2019,4,30,20,bj27nk,self.MachineLearning,"[D] How to Build OpenAI's GPT-2: ""The AI That's Too Dangerous to Release""",https://www.reddit.com/r/deeplearning/comments/bj27nk/d_how_to_build_openais_gpt2_the_ai_thats_too/,pirate7777777,1556623930,,0,2,False,default,,,,,
287,deeplearning,t5_2t5eh,2019-4-30,2019,4,30,20,bj2g41,self.deeplearning,Model interpretation,https://www.reddit.com/r/deeplearning/comments/bj2g41/model_interpretation/,veranceftw,1556625561,"Hello, is there any article with the SoTA of Model interpretation? I was doing some research on this topic and I would like to know if there are any better alternatives to SHAP for image processing models interpretation.   


Kind regards",2,3,False,self,,,,,
288,deeplearning,t5_2t5eh,2019-4-30,2019,4,30,21,bj2n8a,self.deeplearning,Any recent interesting Gait Research papers?,https://www.reddit.com/r/deeplearning/comments/bj2n8a/any_recent_interesting_gait_research_papers/,supp0rtlife,1556626848,,0,2,False,self,,,,,
289,deeplearning,t5_2t5eh,2019-4-30,2019,4,30,22,bj3l39,self.deeplearning,Is Dual Fan enough for RTX 2080 used in deep learning ?,https://www.reddit.com/r/deeplearning/comments/bj3l39/is_dual_fan_enough_for_rtx_2080_used_in_deep/,Atralb,1556632466,,10,3,False,self,,,,,
