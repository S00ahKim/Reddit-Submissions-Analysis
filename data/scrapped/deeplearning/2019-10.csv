,sbr,sbr_id,date,year,month,hour,day,id,domain,title,full_link,author,created,selftext,num_comments,score,over_18,thumbnail,media_provider,media_thumbnail,media_title,media_description,media_url
0,deeplearning,t5_2t5eh,2019-10-1,2019,10,1,14,dboh2h,medium.com,Tensorflow 2.0,https://www.reddit.com/r/deeplearning/comments/dboh2h/tensorflow_20/,dhanno65,1569906492,,0,1,False,default,,,,,
1,deeplearning,t5_2t5eh,2019-10-1,2019,10,1,15,dbpf0e,kumarujjawal.github.io,Sequence to Sequence Learning with Neural Networks : Paper Overview,https://www.reddit.com/r/deeplearning/comments/dbpf0e/sequence_to_sequence_learning_with_neural/,i_amujjawal,1569912407,,0,2,False,default,,,,,
2,deeplearning,t5_2t5eh,2019-10-1,2019,10,1,16,dbprap,self.deeplearning,Wake word detection model using GRU vs LSTM,https://www.reddit.com/r/deeplearning/comments/dbprap/wake_word_detection_model_using_gru_vs_lstm/,raikarsagar,1569914626,Which model architecture is preferable for wake word detection on an embedded platform (prefereably RNN based)? Any suggestions in this regard would be helpful,1,3,False,self,,,,,
3,deeplearning,t5_2t5eh,2019-10-1,2019,10,1,18,dbqu2b,self.deeplearning,No Code Deep Learning Software,https://www.reddit.com/r/deeplearning/comments/dbqu2b/no_code_deep_learning_software/,MistaSurr,1569922135,"I'm a newbie and looking into using deep learning for some projects, and I am very interested in transfer learning.

I understand that the results may not be the best, but as I am not a maths or coding guru, I like the idea of taking an existing algorithm and retraining it.

To the end, I have been looking for a tool that allows me to upload my own data set and just hit run with minimum configuration, preferably in as a GUI application not through an IDE or command prompt.

I have had a look at Ludwig already, but although it claims to need 'no code' from the reviews and tutorials I have seen that isn't true.

Any recommended tools?",18,1,False,self,,,,,
4,deeplearning,t5_2t5eh,2019-10-1,2019,10,1,22,dbt2zn,self.deeplearning,Mrcnn,https://www.reddit.com/r/deeplearning/comments/dbt2zn/mrcnn/,raghu_1809,1569935024,Can anybody help me out with mrcnn and how to input to that model ? I'm working on hair segmentation from a face. Thanks in advance.,8,1,False,self,,,,,
5,deeplearning,t5_2t5eh,2019-10-2,2019,10,2,0,dbuvry,medium.com,Tensorflow 2.0 is out!,https://www.reddit.com/r/deeplearning/comments/dbuvry/tensorflow_20_is_out/,alejandrohall,1569943159,,0,41,False,default,,,,,
6,deeplearning,t5_2t5eh,2019-10-2,2019,10,2,2,dbwvq5,self.deeplearning,Deep network design principles,https://www.reddit.com/r/deeplearning/comments/dbwvq5/deep_network_design_principles/,sami_abobala,1569951911,"when we build deep learning model(CNN, LSTM, autoencoder..) how to determine the number of layers, number of units in each layer and sometimes the connection between it? The structure of the models in we see in academic papers cannot be purely experimental !!!",0,1,False,self,,,,,
7,deeplearning,t5_2t5eh,2019-10-2,2019,10,2,6,dc0jwc,self.deeplearning,Which of these Undergrad Courses will be most useful for getting into Graduate Research (PhD)?,https://www.reddit.com/r/deeplearning/comments/dc0jwc/which_of_these_undergrad_courses_will_be_most/,Allentownyeera,1569966924,"I am currently an undergraduate studying math and computer science, and I am interested in getting into graduate school for deep learning research. I am trying to plan out my coursework for the next couple years and would greatly appreciate some help in deciding which courses will add the most value to my skillset (... or will be valued by an admissions committee) 

&amp;#x200B;

I am currently considering: 

&amp;#x200B;

**Math** 

Multivariable Calc II (Just a second semester of Multivariable Calc)

Probability II 

Stochastic Processes (Taken after Probability II)

Statistics II 

Linear Algebra II

Real Analysis

Functional Analysis

Complex Analysis

&amp;#x200B;

**Computer Science**

Numerical Methods

Genetic Algorithms

Theory of Computation 

Natural Language Processing

&amp;#x200B;

&amp;#x200B;

I have already taken Probability I, Linear Algebra I, Multivariable Calc I, Frequentist Statistics, Bayesian Statistics, Ordinary Differential Equations, Discrete Math, Data Mining &amp; Warehousing, Data Structures, and Algorithms. I feel like I have my bases covered, but have read that grad schools value Analysis very highly as a sign of mathematical maturity.",5,1,False,self,,,,,
8,deeplearning,t5_2t5eh,2019-10-2,2019,10,2,11,dc4bwn,self.deeplearning,Speech Recognition - how to split a sentence into words?,https://www.reddit.com/r/deeplearning/comments/dc4bwn/speech_recognition_how_to_split_a_sentence_into/,mutatedmonkeygenes,1569984713,"Hey there, I'm new to Speech Recognition, and I'm looking for an approach to split a sentence (or multiple sentences) in the form of audio/wav files, into individual words? This sounds like a standard problem, so I'm wondering how people in the industry approach it. Thanks!",0,2,False,self,,,,,
9,deeplearning,t5_2t5eh,2019-10-2,2019,10,2,13,dc5mzn,self.deeplearning,Deep Learning with Keras  Part 7: Recurrent Neural Networks (7 Series included..),https://www.reddit.com/r/deeplearning/comments/dc5mzn/deep_learning_with_keras_part_7_recurrent_neural/,ai-lover,1569992290,"1. [Deep Learning with Keras Tutorial  Part 1](https://www.marktechpost.com/2019/06/11/deep-learning-with-keras-tutorial-part-1/)

2. [Data Pre-processing for Deep Learning models (Deep Learning with Keras ...Part 2](https://www.marktechpost.com/2019/06/14/data-pre-processing-for-deep-learning-models-deep-learning-with-keras-part-2/)

3. [Regression with Keras (Deep Learning with Keras  Part 3)](https://www.marktechpost.com/2019/06/17/regression-with-keras-deep-learning-with-keras-part-3/)

4. [Deep Learning with Keras  Part 4: Classification](https://www.marktechpost.com/2019/06/24/deep-learning-with-keras-part-4-classification/)

5. [Deep Learning with Keras  Part 5: Convolutional Neural Networks](https://www.marktechpost.com/2019/07/04/deep-learning-with-keras-part-5-convolutional-neural-networks/)

6. [Deep Learning with Keras  Part 6: Textual Data Preprocessing](https://www.marktechpost.com/2019/09/13/deep-learning-with-keras-part-6-textual-data-preprocessing/)

7. [Deep Learning with Keras  Part 7: Recurrent Neural Networks](https://www.marktechpost.com/2019/10/01/deep-learning-with-keras-part-7-recurrent-neural-networks/)",0,18,False,self,,,,,
10,deeplearning,t5_2t5eh,2019-10-2,2019,10,2,17,dc7e1s,/r/deeplearning/comments/dc7e1s/deepfakes_video_from_theoffice_creeped_out_max/,Deepfakes video from #TheOffice. Creeped out max.,https://www.reddit.com/r/deeplearning/comments/dc7e1s/deepfakes_video_from_theoffice_creeped_out_max/,warmachine0609,1570005085,,12,148,False,default,,,,,
11,deeplearning,t5_2t5eh,2019-10-2,2019,10,2,23,dcb7xa,self.deeplearning,CNN with different image sizes,https://www.reddit.com/r/deeplearning/comments/dcb7xa/cnn_with_different_image_sizes/,clean_pegasus,1570027703,How do you work with datasets that have different image sizes? I don't want to resize the image or pad it. Is there any other way?,2,2,False,self,,,,,
12,deeplearning,t5_2t5eh,2019-10-3,2019,10,3,0,dcblcn,self.deeplearning,Research paper ideas (Computer Vision).,https://www.reddit.com/r/deeplearning/comments/dcblcn/research_paper_ideas_computer_vision/,datguy_paarth,1570029357,"Hi everyone, I am a third year undergraduate student and want to get started into writing research papers on vision. I have completed some projects but can't figure out what to write a paper on. Any ideas will be appreciated.

Also to the mods, will it not be a good idea to have a thread where people can post ideas they have for research or projects and if interested can collaborate?",3,0,False,self,,,,,
13,deeplearning,t5_2t5eh,2019-10-3,2019,10,3,3,dcdzbc,self.deeplearning,[Blog Post] Saving and Loading Deep Learning Models to resume training in Pytorch,https://www.reddit.com/r/deeplearning/comments/dcdzbc/blog_post_saving_and_loading_deep_learning_models/,thesanerachit,1570039744,"I recently wrote a medium post on how to save and load a deep learning model to resume training it in PyTorch.
Medium post link: https://medium.com/@rachit221195/saving-and-loading-your-model-to-resume-training-in-pytorch-cb687352fa61?source=friends_link&amp;sk=39046d215126bb0153d6ab70b0cb837d

I am hoping this would be helpful to people who are new to PyTorch and DeepLearning.
This is my first tech post ever. I would love to get all kinds of feedback on it.",0,1,False,self,,,,,
14,deeplearning,t5_2t5eh,2019-10-3,2019,10,3,3,dceafy,github.com,Object Detection in Real Time | Code repo | With Demo,https://www.reddit.com/r/deeplearning/comments/dceafy/object_detection_in_real_time_code_repo_with_demo/,826krishna,1570041119,,1,1,False,default,,,,,
15,deeplearning,t5_2t5eh,2019-10-3,2019,10,3,3,dcef41,self.deeplearning,Multiclass image classification,https://www.reddit.com/r/deeplearning/comments/dcef41/multiclass_image_classification/,danush321,1570041668,Can anyone give me a good link or even the code for this. I want to give a single image as an input and it should output the top single prediction. Thanks in advance!,0,1,False,self,,,,,
16,deeplearning,t5_2t5eh,2019-10-3,2019,10,3,5,dcfnt9,self.deeplearning,"Generalized Policy Iteration, Dynamic Programming, and RL for Car Rental Resource Allocation - Reinforcement Learning Chapter 4!",https://www.reddit.com/r/deeplearning/comments/dcfnt9/generalized_policy_iteration_dynamic_programming/,HenryAILabs,1570046749,[https://youtu.be/pcZFjPHO4c0](https://youtu.be/pcZFjPHO4c0),0,1,False,self,,,,,
17,deeplearning,t5_2t5eh,2019-10-3,2019,10,3,7,dchsdp,lambdalabs.com,Quickstart TensorFlow 2.0 Tutorial - Basic Image Classification,https://www.reddit.com/r/deeplearning/comments/dchsdp/quickstart_tensorflow_20_tutorial_basic_image/,mippie_moe,1570055661,,0,5,False,default,,,,,
18,deeplearning,t5_2t5eh,2019-10-3,2019,10,3,7,dchzve,self.deeplearning,trying to confirm my understanding of image segmentation,https://www.reddit.com/r/deeplearning/comments/dchzve/trying_to_confirm_my_understanding_of_image/,fieldcady,1570056580,"I'm trying to wrap my head around image segmentation (i.e. the output is a 2D array the size of the input image, giving classifications for pixels) rather than a 1D array for classification.  I \*think\* that I get it, but it seems really crude and I would like to confirm.

Looking at \[UNet\]([https://github.com/zhixuhao/unet/blob/master/model.py](https://github.com/zhixuhao/unet/blob/master/model.py)) for example it seems as if each pixel is in effect classified by looking at the area around it and assigning that area to one of k classes that you're trying to identify.  k=2 if you are trying to separate landscape from the sky, for example.  Basically you have a smaller neural network that I will call miniNN - that takes in a sub-image and produces a k-dimensional vector - and you tile miniNN over the whole image like a souped-up convolutional kernel.

This is a limited approach.  For example, if you draw a large circular perimeter (with radius much larger than miniNN's dimensions) on an otherwise homogenous background you cannot segment out the inside from the outside.  You would have to do something like using deep learning to segment out the pixels that are near the perimeter, and then use connectedness to determine whether the other pixels were inside or outside it.

Am I understanding this correctly?  Is there any big innovation that I'm missing?  Thanks!!",0,3,False,self,,,,,
19,deeplearning,t5_2t5eh,2019-10-3,2019,10,3,18,dco99e,self.deeplearning,[Blog Post] Dense and Sparse Crowd Counting Methods and Techniques - DIY pedestrian detection model,https://www.reddit.com/r/deeplearning/comments/dco99e/blog_post_dense_and_sparse_crowd_counting_methods/,manneshiva,1570093242,"Estimate the number of people from CCTV footage or drone imagery.

[https://nanonets.com/blog/crowd-counting-review/](https://nanonets.com/blog/crowd-counting-review/?utm_source=reddit&amp;utm_medium=social&amp;utm_campaign=drcrco&amp;utm_content=dl)

&amp;#x200B;

*Processing gif gggbo2tehaq31...*",0,7,False,self,,,,,
20,deeplearning,t5_2t5eh,2019-10-3,2019,10,3,22,dcr02e,self.deeplearning,Deep Learning from scratch,https://www.reddit.com/r/deeplearning/comments/dcr02e/deep_learning_from_scratch/,zepmck,1570109772,"Dear all,

I need to develop a deep learning application for object detection from scratch using tons of images. Can you please point me to a resource that may explain me where to start from? Which technology to chose? Which libraries? DataLoaders?

Thanks",1,1,False,self,,,,,
21,deeplearning,t5_2t5eh,2019-10-3,2019,10,3,22,dcr90n,self.deeplearning,Awesome AI Research and Projects on Computer Vision News (with codes!) October 2019,https://www.reddit.com/r/deeplearning/comments/dcr90n/awesome_ai_research_and_projects_on_computer/,Gletta,1570111000,"The October issue of Computer Vision News: 36 pages about AI and Deep Learning through research and practical applications.

[HTML5 version (recommended)](https://www.rsipvision.com/ComputerVisionNews-2019October/)

[PDF version](https://www.rsipvision.com/computer-vision-news-2019-october-pdf/)

Technical articles on pages 4-8 and 18-23. Subscribe for free on page 36.

&amp;#x200B;

https://i.redd.it/nsya4b6i0cq31.jpg",0,25,False,self,,,,,
22,deeplearning,t5_2t5eh,2019-10-3,2019,10,3,23,dcrc7b,self.deeplearning,Keras SWA: Stochastic weight averaging callback for Keras,https://www.reddit.com/r/deeplearning/comments/dcrc7b/keras_swa_stochastic_weight_averaging_callback/,lilsmacky,1570111421,"As an exercise for myself I decided to implement SWA, from the paper [Averaging Weights Leads to Wider Optima and Better Generalization](https://arxiv.org/abs/1803.05407). I did it with Keras and decided it might make a nice package.

Repo:

[https://github.com/simon-larsson/keras-swa](https://github.com/simon-larsson/keras-swa)

pip:

`pip install keras-swa`

If you are not familiar with SWA, it is a trick to approximate ensembling by taking a running average of your weights towards the end of training a model. You can read more in this nice [blog post](https://pechyonkin.me/stochastic-weight-averaging/) explaining SWA and it's relatives SSE and FGE.

I currently only implement the constant learning rate schedule from the paper, hoping to add the cyclic one from the paper soon. It is also possible to leave the learning rate to the optimizer or other schedulers. I have also not implemented the batch normalization fix. It requires a forward pass over training data, which I don't know how to do from a callback. So any help there would be appreciated.

I would love for people to try it! Feedback is also welcome! :)",0,2,False,self,,,,,
23,deeplearning,t5_2t5eh,2019-10-4,2019,10,4,5,dcx27t,self.deeplearning,Model-Free and Off-Policy Learning - Reinforcement Learning Chapter 5!,https://www.reddit.com/r/deeplearning/comments/dcx27t/modelfree_and_offpolicy_learning_reinforcement/,HenryAILabs,1570136140,[https://youtu.be/uiPhlFrwcw8](https://youtu.be/uiPhlFrwcw8),0,4,False,self,,,,,
24,deeplearning,t5_2t5eh,2019-10-4,2019,10,4,16,dd3uoy,i.redd.it,Deep,https://www.reddit.com/r/deeplearning/comments/dd3uoy/deep/,connor123646,1570172574,,13,0,False,image,,,,,
25,deeplearning,t5_2t5eh,2019-10-4,2019,10,4,16,dd3vy3,medium.com,Transforming eCommerce Businesses with Robotic Process Automation,https://www.reddit.com/r/deeplearning/comments/dd3vy3/transforming_ecommerce_businesses_with_robotic/,Ak_Bansal,1570172811,,0,1,False,default,,,,,
26,deeplearning,t5_2t5eh,2019-10-4,2019,10,4,17,dd4n8u,self.deeplearning,Is Neural Style Transfer is good for final year undergrad project??,https://www.reddit.com/r/deeplearning/comments/dd4n8u/is_neural_style_transfer_is_good_for_final_year/,Pratik668,1570178616,,6,0,False,self,,,,,
27,deeplearning,t5_2t5eh,2019-10-4,2019,10,4,21,dd6fw9,self.deeplearning,How can I create a nice NN architecture diagram - Beginner?,https://www.reddit.com/r/deeplearning/comments/dd6fw9/how_can_i_create_a_nice_nn_architecture_diagram/,dalastspartan,1570190724,"I am very new to deep learning but I have followed a few online tutorials and I now have a graph of one of my models on TensorBoard: [https://imgur.com/a/cUxWs39](https://imgur.com/a/cUxWs39)

While this is useful I want to also explore other methods in making a nice looking diagram, something along the lines of what was posted here: [https://datascience.stackexchange.com/questions/14899/how-to-draw-deep-learning-network-architecture-diagrams](https://datascience.stackexchange.com/questions/14899/how-to-draw-deep-learning-network-architecture-diagrams), possibly using the NN-SVG tool suggested by Pablo Rivas:  [https://imgur.com/7hCjPNJ](https://imgur.com/7hCjPNJ).

I think I need some more experience to do this right but I want to make sure im accurately portraying the model. Could someone help me in creating a NN diagram using the NN-SVG tool or suggest something else that I can use?

Model below, thanks in advance:

dense\_layers = \[0,1,2\]  
layer\_sizes = \[32, 64, 128\]  
conv\_layers = \[1, 2, 3\]  


for dense\_layer in dense\_layers:  
 for layer\_size in layer\_sizes:  
 for conv\_layer in conv\_layers:  
NAME = ""{}-conv-{}-nodes{}-dense-{}"".format(conv\_layer, layer\_size, dense\_layer, int(time.time()))  
tensorboard = TensorBoard(log\_dir=""logs\\{}"".format(NAME))  
 model = Sequential()

model.add(Conv2D(layer\_size, (3,3), input\_shape = X.shape\[1:\]))  
model.add(Activation(""relu""))  
model.add(MaxPooling2D(pool\_size=(2,2)))  
 for l in range(conv\_layer - 1):  
model.add(Conv2D(layer\_size, (3,3)))  
model.add(Activation(""relu""))  
model.add(MaxPooling2D(pool\_size=(2,2)))  
 model.add(Flatten())  
 for l in range(dense\_layer):  
model.add(Dense(layer\_size))  
model.add(Activation(""relu""))  
model.add(Dropout(0.2))   
 model.add(Dense(1))  
 model.add(Activation('sigmoid'))  


 model.compile(loss='binary\_crossentropy',  
 optimizer=""adam"",  
 metrics=\['accuracy'\])  
model.fit(X\_train, y\_train, batch\_size=32, epochs=15, validation\_split=0.2, shuffle=True, callbacks=\[tensorboard\])  
score = model.evaluate(X\_test, y\_test, batch\_size=32)",3,1,False,self,,,,,
28,deeplearning,t5_2t5eh,2019-10-4,2019,10,4,22,dd76k4,self.deeplearning,Need help finding supplementary material and dataset for a paper,https://www.reddit.com/r/deeplearning/comments/dd76k4/need_help_finding_supplementary_material_and/,AhmedZubairGCU,1570194736,"The paper in question is [DeepLens: Shallow Depth of Field from a Single Image](https://deeplensprj.github.io/deeplens/DeepLens.html)

In the paper there is mention of supplementary material which I cannot find on the project website. Also the link to database on the website just redirects to github. I have emailed them but received no reply. If any one here have access to these materials kindly share them. Or if you know about any related paper kindly mention them as well.",0,5,False,self,,,,,
29,deeplearning,t5_2t5eh,2019-10-4,2019,10,4,22,dd7amk,youtu.be,Drones and Artificial Intelligence.,https://www.reddit.com/r/deeplearning/comments/dd7amk/drones_and_artificial_intelligence/,cmillionaire9,1570195328,,3,34,False,image,,,,,
30,deeplearning,t5_2t5eh,2019-10-5,2019,10,5,0,dd913a,self.deeplearning,Label Studio - flexible data labeling and annotation tool,https://www.reddit.com/r/deeplearning/comments/dd913a/label_studio_flexible_data_labeling_and/,michael_htx,1570203507,"It's [https://labelstud.io/](https://labelstud.io/)

Hey, I'm excited to share the news with the community. We're releasing our data labeling tool into the open-source. It's called **Label Studio**. Why yet another one? While working as ML engineers, most of the tools we've used were very specific and required at least some tunning to work with our datasets. Thus the idea of a configurable UI was born.

As you'd build a webpage, you can create a data labeling UI specifically for your needs. The config language is so expressive it usually takes no more than 10-20 lines.

I hope somebody finds it useful. Enjoy and send your feedback!

[https://github.com/heartexlabs/label-studio](https://github.com/heartexlabs/label-studio)

&amp;#x200B;

https://i.redd.it/a1xeuy5nnjq31.png",0,10,False,self,,,,,
31,deeplearning,t5_2t5eh,2019-10-5,2019,10,5,1,dd9iqd,youtu.be,Top 10 techniques for feature selection,https://www.reddit.com/r/deeplearning/comments/dd9iqd/top_10_techniques_for_feature_selection/,nerdy_wits,1570205684,,0,1,False,image,,,,,
32,deeplearning,t5_2t5eh,2019-10-5,2019,10,5,2,ddaj88,self.deeplearning,Looking for where to start in Computational Photography research,https://www.reddit.com/r/deeplearning/comments/ddaj88/looking_for_where_to_start_in_computational/,soundofhorse,1570210176,"Hey guys,

I know posts like these are a drag but bear with me! I'm fortunate enough to have the opportunity to complete a research project with the help of amazing researching faculty at my University. I can really pick my path to complete my studies. Now I'm no newbie to DL. I've done a small amount of research in using CNN to learn traits of inorganic molecules, I've taken Master's level courses in NLP, DL, Knowledge bases and Imaging. I've read a few research papers on the topic of inverse rendering, and using DL to create HDR images. Those papers really sparked my interest. I really want to use my education to research *something* in the realm of computational photography but don't where to start. So, what are some of your guys favorite papers on the topic that I could read? 

Thanks!",0,3,False,self,,,,,
33,deeplearning,t5_2t5eh,2019-10-5,2019,10,5,6,dddpp4,self.deeplearning,[Hardware] Advice about picking my next GPU: Turing vs. Pascal,https://www.reddit.com/r/deeplearning/comments/dddpp4/hardware_advice_about_picking_my_next_gpu_turing/,scapocchione,1570223952,"Hi. I own a GTX 1080ti right now. Now, I'd like to buy a new GPU. Budget is around 1000 bucks.

I have the following options:

\- Another 1-2 1080ti, to used just in parallel via the PCIe bus. Using FP16 on a Pascal card gives a modest speedup (5-8%), but whith the effect of almost doubling the memory, which is the main thing about FP16 (in other words, if your training takes a little more, you just wait, but if your minibatches don't fit in memory, you are done). The big advantage of the 1080TIs, for me, is that you have a 11gb memory card for \~500usd/eur.

\- A couple of 1070Super. They are substantially faster (\~30%) than 1080TI in FP16, and a bit slower in FP32. They have the advantage of being NVlink capable, but with half bandwidth. It is somewhat unclear if memory pooling via nvlink does actually work with consumer Nvidia cards. Still, I would have \~16Gb of vram against \~33Gb of 3x1080ti. One potential advantage is that as the main frameworks will cease to support Pascal, Turing would be presumably still supported. Throwing three 1080ti in the trashcan within 1-2 yrs would be a shame (I don't game).

\- A single 2080ti. Fastest consumer card for DL, but still 11Gb at twice the price of a 1080TI. Potential for adding another one in future. Full nvlink bandwidth, but still no memory pooling (at least from what I understand).

What do you think??",12,3,False,self,,,,,
34,deeplearning,t5_2t5eh,2019-10-5,2019,10,5,14,ddj7mv,self.deeplearning,How do you go from a 750-sampled input to a 600 x 3 output using a CNN?,https://www.reddit.com/r/deeplearning/comments/ddj7mv/how_do_you_go_from_a_750sampled_input_to_a_600_x/,arjundupa,1570254338,"I am trying to reproduce the results of [this](https://www.researchgate.net/publication/327375583_A_Simple_and_Effective_Method_for_Detecting_Myocardial_Infarction_Based_on_Deep_Convolutional_Neural_Network) paper titled: ""A Simple and Effective Method for Detecting Myocardial Infarction Based on Deep Convolutional Neural Network""

The paper says:

&gt;The input to our CNN network is a fixed-size 750 samples.

Because the first layer is a 2D Convolutional layer, which requires a 4D input, I assume the input shape is (batch\_size, 1, 1, 750) -- their input only has one channel (making the 2nd dimension a 1), and that one channel is 1D (making the 3rd dimension a 1 and the 4th dimension equal to 750).

The table on page three with the network architecture says that the first (convolutional) layer has:

&gt;Kernel size: 151 x 3, Stride: 1, Output shape: 600 x 3, and Valid Padding

I tried implementing this with:

    self.conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(1, 453), stride=1)

But, this gives me an output shape of (1 x 1 x 298). How can I get a (600 x 3) output which is consistent with all of their implementation details? Any ideas what I'm doing wrong?

Any ideas will be much appreciated, thank you!",5,2,False,self,,,,,
35,deeplearning,t5_2t5eh,2019-10-5,2019,10,5,15,ddjiky,self.deeplearning,"Interview with the CTO of Hugging Face: Julien Chaumond | Hugging Face, Transformers | NLP Research and Open Source + 3 AMA Announcements",https://www.reddit.com/r/deeplearning/comments/ddjiky/interview_with_the_cto_of_hugging_face_julien/,init__27,1570256590,"Interview with the CTO of Hugging Face, Julien Chaumond all about his journey into the field and his path as a CTO of Hugging Face. We also discuss all the amazing work being done at Hugging Face, research and open source as well as about their team.

Audio: https://anchor.fm/chaitimedatascience/episodes/Hugging-Face--Transformers--NLP-Research-and-Open-Source--Interview-with-Julien-Chaumond-e5o819/a-apcirt

Video: https://www.youtube.com/watch?v=kqPEwJVkpnA

AMA Announcements: 
Ill also be interviewing 3 more amazing people in the upcoming week and they have been kind enough to agree to an AMA interview, here are the links to submit your questions:


AMA Interview with Victor Sanh, Research Scientist at Hugging Face, AMA: https://twitter.com/bhutanisanyam1/status/1179836740569223168?s=21

Kaggle IEEE-CIS Fraud Detection Comp, 2nd Pos winner (Team: 2 Uncles and 3 Puppies), Grandmaster CPMP AMA: https://www.kaggle.com/c/ieee-fraud-detection/discussion/111242

6th Pos winner (Team: Zoo), Kaggle Master, Philipp Singer, AMA Interview: https://www.kaggle.com/c/ieee-fraud-detection/discussion/111281",0,23,False,self,,,,,
36,deeplearning,t5_2t5eh,2019-10-6,2019,10,6,0,ddoc3s,youtube.com,From The Brain To AI (Neural Networks | What Is Deep Learning | Deep Learning Basics),https://www.reddit.com/r/deeplearning/comments/ddoc3s/from_the_brain_to_ai_neural_networks_what_is_deep/,enchorb,1570287752,,0,1,False,image,,,,,
37,deeplearning,t5_2t5eh,2019-10-6,2019,10,6,7,ddul8z,self.deeplearning,Multi Instance Learning,https://www.reddit.com/r/deeplearning/comments/ddul8z/multi_instance_learning/,thisisabujee,1570315423,"Can someone suggest me a good repository for implementing multi instance learning algorithm. I have this problem where i am stuck a bit. if we jave any expert on multi instance learning here, please do comment  below, i have to ask few questions. Thanks",4,13,False,self,,,,,
38,deeplearning,t5_2t5eh,2019-10-6,2019,10,6,8,ddv3pn,datasciencecentral.com,Free Book: Getting Started with TensorFlow 2.0,https://www.reddit.com/r/deeplearning/comments/ddv3pn/free_book_getting_started_with_tensorflow_20/,psangrene,1570317678,,0,1,False,default,,,,,
39,deeplearning,t5_2t5eh,2019-10-6,2019,10,6,18,de0yi5,elink.io,Zooming into the world of computer vision applications,https://www.reddit.com/r/deeplearning/comments/de0yi5/zooming_into_the_world_of_computer_vision/,Ansh_Techie,1570353506,,0,1,False,default,,,,,
40,deeplearning,t5_2t5eh,2019-10-7,2019,10,7,0,de4d9l,self.deeplearning,Multiple inputs and different training samples,https://www.reddit.com/r/deeplearning/comments/de4d9l/multiple_inputs_and_different_training_samples/,dlnewb69,1570374201,"I have a model that takes 2 inputs, however the number of training samples in both of them are not supposed to be the same. Although it is situational, most of the time samples in input a will be greater than input b. 

Let's say input a has 10,000 training samples and input b has 2000. Is there a way to train them together or I'll have to duplicate the samples in input b?",2,1,False,self,,,,,
41,deeplearning,t5_2t5eh,2019-10-7,2019,10,7,0,de4l53,youtu.be,"AR + AI | AutoML Vision Edge, AutoML Video, and Video Intelligence API",https://www.reddit.com/r/deeplearning/comments/de4l53/ar_ai_automl_vision_edge_automl_video_and_video/,cmillionaire9,1570375240,,3,38,False,image,,,,,
42,deeplearning,t5_2t5eh,2019-10-7,2019,10,7,5,de8ifc,self.deeplearning,"AI Research Weekly Update Video! - October 6th, 2019",https://www.reddit.com/r/deeplearning/comments/de8ifc/ai_research_weekly_update_video_october_6th_2019/,HenryAILabs,1570392161,[https://youtu.be/rCpvXZSaNlQ](https://youtu.be/rCpvXZSaNlQ),0,1,False,self,,,,,
43,deeplearning,t5_2t5eh,2019-10-7,2019,10,7,6,dea3ww,self.deeplearning,"Guys, I'm confused with the hypothesis",https://www.reddit.com/r/deeplearning/comments/dea3ww/guys_im_confused_with_the_hypothesis/,kkingbbob,1570399128,"Hey guys! Recently, I'm quite confused about the logic running inside methodology. I did some data science methodology course. It seems quite clear. For example, clarity the research question, aim and objectives -&gt; which approach to use -&gt; data preparation -&gt; data collection -&gt; data evaluation -&gt; deployment etc. Then I read some PhD's dissertation. It's a similar pattern that from the question, based on literature review, proposed a framework or system, then modelling and evaluation. However, I read some social science and medical methodology. OMG! I was lost there. For example, hypothetical deductive is the foundation. First, u need to have a hypothesis. Then sampling, data collection and analyse the data to test the hypothesis. So I'm stuck here! The hypothesis. Many of the data science research doesn't have a hypothesis. Then have a research question, such as how to xxxx to improve xxxx. Is it necessary to have hypothesis in data science project? Or does having a hypothesis means that just transfer this research question into a hypothesis like ""whether this xxxx model can improve xxxx?""",2,0,False,self,,,,,
44,deeplearning,t5_2t5eh,2019-10-7,2019,10,7,8,deb6cv,zeldatech.com,"Deep Learning with TensorFlow By Giancarlo Zaccone, Md. Rezaul Karim PDF",https://www.reddit.com/r/deeplearning/comments/deb6cv/deep_learning_with_tensorflow_by_giancarlo/,oussamaouti,1570404223,,1,0,False,default,,,,,
45,deeplearning,t5_2t5eh,2019-10-7,2019,10,7,14,def1b9,self.deeplearning,Need dataset of rgb+d images for portrait mode,https://www.reddit.com/r/deeplearning/comments/def1b9/need_dataset_of_rgbd_images_for_portrait_mode/,AhmedZubairGCU,1570425438,"I need a rgbd image dataset for training a model for portrait mode. Most rgbd images dataset include indoor scenes of furniture, streets, pedestrian or closeups of people. I need a dataset that contain pictures that people take from their smartphones using portrait mode such as portraits of people and some close ups of objects.",1,1,False,self,,,,,
46,deeplearning,t5_2t5eh,2019-10-7,2019,10,7,15,defu32,medium.com,Accelerating Sales Growth with Artificial Intelligence,https://www.reddit.com/r/deeplearning/comments/defu32/accelerating_sales_growth_with_artificial/,Anshul_Oodles,1570431009,,1,0,False,default,,,,,
47,deeplearning,t5_2t5eh,2019-10-7,2019,10,7,16,deg94k,kumarujjawal.github.io,Attention is all you need : Paper Overview,https://www.reddit.com/r/deeplearning/comments/deg94k/attention_is_all_you_need_paper_overview/,i_amujjawal,1570434138,,0,2,False,default,,,,,
48,deeplearning,t5_2t5eh,2019-10-7,2019,10,7,19,dehj4t,self.deeplearning,Looking for resources on Image Analysis and Computer Vision,https://www.reddit.com/r/deeplearning/comments/dehj4t/looking_for_resources_on_image_analysis_and/,JyotinderSingh,1570443686,"I'm starting work on a research project at my university, regarding DL and CV, I'm a newbie to this field and have limited knowledge of the subject. I was wondering if someone could hook me up with some resources for reference smd learning regarding topics such as Image Validation/Analysis and Computer Vision.
My experience includes having worked on basic projects thst were a part of Stanford's CS231n on YouTube.
Thank you!",0,1,False,self,,,,,
49,deeplearning,t5_2t5eh,2019-10-7,2019,10,7,21,deivue,self.deeplearning,LSTM for solving Class Overlapping Issues,https://www.reddit.com/r/deeplearning/comments/deivue/lstm_for_solving_class_overlapping_issues/,data_autopsy,1570452010,"Do you guys think RNN can help in classification problems with high overlap among classes. I'm working on a dataset having a considerable overlap among the classes. I have tried a lot of stuff from augmentation to transfer learning, by doing binary classification for each class but it's not perfect yet. Do you guys think a feedback loop over cnn will make the matter easy? I'm thinking of writing custom Image generator to generate multiple augmented images of single and then passing it to my cnn block and then using the features extracted there as an input for my lstm. Has anyone worked on this? I need some help. If I'm against the wall, please let me know. Thanks.",4,2,False,self,,,,,
50,deeplearning,t5_2t5eh,2019-10-7,2019,10,7,21,deix64,aiprobook.com,Deep Learning for Programmers new release 0.10.0 + Momentum and Nesterov Momentum,https://www.reddit.com/r/deeplearning/comments/deix64/deep_learning_for_programmers_new_release_0100/,dragandj,1570452209,,2,7,False,default,,,,,
51,deeplearning,t5_2t5eh,2019-10-7,2019,10,7,22,dej5ee,facebook.com,The Deep Learning Revolution (The MIT Press) hardcover is 28% off,https://www.reddit.com/r/deeplearning/comments/dej5ee/the_deep_learning_revolution_the_mit_press/,Tanderies80,1570453430,,1,0,False,default,,,,,
52,deeplearning,t5_2t5eh,2019-10-7,2019,10,7,22,dej5f6,self.deeplearning,[Research] On achieving accurate object detection,https://www.reddit.com/r/deeplearning/comments/dej5f6/research_on_achieving_accurate_object_detection/,cdossman,1570453432,"Object Detection Accuracy (mAP) Cheat Sheet: 6 Freebies to Help You Increase the Performance of Your Object Detection Models

 [https://www.linkedin.com/pulse/object-detection-accuracy-map-cheat-sheet-christopher-dossman/?trackingId=uL1wSwcoFRj3dzy%2B3SmmKQ%3D%3D](https://www.linkedin.com/pulse/object-detection-accuracy-map-cheat-sheet-christopher-dossman/?trackingId=uL1wSwcoFRj3dzy%2B3SmmKQ%3D%3D) 

 [https://towardsdatascience.com/object-detection-accuracy-map-cheat-sheet-8f710fd79011](https://towardsdatascience.com/object-detection-accuracy-map-cheat-sheet-8f710fd79011)",0,5,False,self,,,,,
53,deeplearning,t5_2t5eh,2019-10-8,2019,10,8,0,dekqk7,self.deeplearning,GPU rental for rent.,https://www.reddit.com/r/deeplearning/comments/dekqk7/gpu_rental_for_rent/,WINDY_WINDWARD,1570460962,"i can provide you with a remote access to GPU rigs for training your machine learning purposes 

prices 

2$ for a 24 hour session with a single 1080ti

i also provide amd rigs running vega 64's",0,1,False,self,,,,,
54,deeplearning,t5_2t5eh,2019-10-8,2019,10,8,6,depsjy,self.deeplearning,"Temporal-Difference Learning, SARSA, &amp; Q-Learning - Reinforcement Learning Chapter 6!",https://www.reddit.com/r/deeplearning/comments/depsjy/temporaldifference_learning_sarsa_qlearning/,HenryAILabs,1570482455,[https://youtu.be/L64E\_NTZJ\_0](https://youtu.be/L64E_NTZJ_0),0,6,False,self,,,,,
55,deeplearning,t5_2t5eh,2019-10-8,2019,10,8,7,der0fr,self.deeplearning,Survival Analysis with Deep Learning,https://www.reddit.com/r/deeplearning/comments/der0fr/survival_analysis_with_deep_learning/,backtoreality123,1570487763,Hey Im just wondering if anyone has any recommendations on resources to learn how to implement survival analyses with Deep Learning? Ive found some things through Google but just wanted to see if there was anyone with experience in such research and what methods they used.,3,8,False,self,,,,,
56,deeplearning,t5_2t5eh,2019-10-8,2019,10,8,15,devzag,self.deeplearning,PyTorch Custom Padding,https://www.reddit.com/r/deeplearning/comments/devzag/pytorch_custom_padding/,arjundupa,1570514727,"When I do:

    nn.Conv1d(in_channels=10, out_channels=10, kernel_size=20, stride=1, padding=10)

PyTorch applies a padding of 10 to both sides. 

How do I apply a custom padding of 9 on one side and 10 on the other in PyTorch?",3,4,False,self,,,,,
57,deeplearning,t5_2t5eh,2019-10-8,2019,10,8,20,deydlr,elink.io,Accelerating Sales Growth with Artificial Intelligence,https://www.reddit.com/r/deeplearning/comments/deydlr/accelerating_sales_growth_with_artificial/,Binny_gupta,1570532500,,0,0,False,default,,,,,
58,deeplearning,t5_2t5eh,2019-10-8,2019,10,8,20,deylf5,self.deeplearning,Video Stream Analysis pipeline.,https://www.reddit.com/r/deeplearning/comments/deylf5/video_stream_analysis_pipeline/,lakshaytalkstocomput,1570533836,How to make a scalable system to do video stream analysis using deep learning on cctv cameras? any tips or readings?,2,14,False,self,,,,,
59,deeplearning,t5_2t5eh,2019-10-8,2019,10,8,23,df0ppf,github.com,[EMNLP-IJCNLP 2019] Statistics and Accepted paper list with arXiv link,https://www.reddit.com/r/deeplearning/comments/df0ppf/emnlpijcnlp_2019_statistics_and_accepted_paper/,roomylee,1570544895,,0,4,False,default,,,,,
60,deeplearning,t5_2t5eh,2019-10-9,2019,10,9,0,df1e80,self.deeplearning,]Research] A liquid warping GAN,https://www.reddit.com/r/deeplearning/comments/df1e80/research_a_liquid_warping_gan/,cdossman,1570547883,[removed],0,1,False,self,,,,,
61,deeplearning,t5_2t5eh,2019-10-9,2019,10,9,0,df1tso,self.deeplearning,Privacy and Learnability in AI?,https://www.reddit.com/r/deeplearning/comments/df1tso/privacy_and_learnability_in_ai/,potato-question-mark,1570549762,"Can anyone guide me to some interesting works on privacy preserving deep learning?

I want to implement a cnn that would preserve the privacy of the images used for training and all. I am not exactly sure of what I want, but I would appreciate any input.

Thanks!",2,1,False,self,,,,,
62,deeplearning,t5_2t5eh,2019-10-9,2019,10,9,6,df6cjw,self.deeplearning,"n-step Bootstrapping, enabling more data efficient Model-Free Learning - Reinforcement Learning Chapter 7!",https://www.reddit.com/r/deeplearning/comments/df6cjw/nstep_bootstrapping_enabling_more_data_efficient/,HenryAILabs,1570569064,[https://youtu.be/1i5a4yj0Mwg](https://youtu.be/1i5a4yj0Mwg),0,15,False,self,,,,,
63,deeplearning,t5_2t5eh,2019-10-9,2019,10,9,7,df7gi0,self.deeplearning,"Things impossible for classic Machine Learning, but Deep Learning succeeded in",https://www.reddit.com/r/deeplearning/comments/df7gi0/things_impossible_for_classic_machine_learning/,padschu,1570573565,"What did Deep Learning achieve, that was/seemed to classic Machine Learning?  
Like AlphaGo winning the board game Go against a human top player.  
So, not just like outperforming other Machine Learning, but enabling new stuff.  


What is your favorite?",8,5,False,self,,,,,
64,deeplearning,t5_2t5eh,2019-10-9,2019,10,9,16,dfdwrs,self.deeplearning,"What is the definition of ""vector-to-vector function""?",https://www.reddit.com/r/deeplearning/comments/dfdwrs/what_is_the_definition_of_vectortovector_function/,otakutyrant,1570607993,I encounter this in Deep Learning.,3,6,False,self,,,,,
65,deeplearning,t5_2t5eh,2019-10-9,2019,10,9,18,dfegvy,onlineitguru.com,Difference of Deep Learning vs Neural Networks,https://www.reddit.com/r/deeplearning/comments/dfegvy/difference_of_deep_learning_vs_neural_networks/,swethasree12,1570612248,,0,0,False,default,,,,,
66,deeplearning,t5_2t5eh,2019-10-9,2019,10,9,19,dffa4n,medium.com,Accelerating Sales Growth with Artificial Intelligence,https://www.reddit.com/r/deeplearning/comments/dffa4n/accelerating_sales_growth_with_artificial/,Kushal_Bansal001,1570618045,,1,1,False,default,,,,,
67,deeplearning,t5_2t5eh,2019-10-9,2019,10,9,21,dfgbba,github.com,"Implementation of deep learning related scripts(model defs, loss functions etc.) using tensorflow 2",https://www.reddit.com/r/deeplearning/comments/dfgbba/implementation_of_deep_learning_related/,vandit_15,1570624101,,0,2,False,default,,,,,
68,deeplearning,t5_2t5eh,2019-10-9,2019,10,9,23,dfhtp1,ai-pool.com,"HarDNet, which achieves ~36% inference time reduction",https://www.reddit.com/r/deeplearning/comments/dfhtp1/hardnet_which_achieves_36_inference_time_reduction/,hazarapet,1570631349,,0,1,False,default,,,,,
69,deeplearning,t5_2t5eh,2019-10-10,2019,10,10,3,dflmdf,self.deeplearning,What is the validation set used for?,https://www.reddit.com/r/deeplearning/comments/dflmdf/what_is_the_validation_set_used_for/,ssd123456789,1570647545,"So I am a but confused about the role of the validation set. I train my model and then use the validation set to tune the hyperparameters? 
That just sounds wrong to me. Wouldn't you want to set your hyperparameters first and then train. For example, the learning rate, what use is it to tune it after the entire training process has taken place?",8,8,False,self,,,,,
70,deeplearning,t5_2t5eh,2019-10-10,2019,10,10,5,dfn2xp,self.deeplearning,GAN from Scratch,https://www.reddit.com/r/deeplearning/comments/dfn2xp/gan_from_scratch/,Flex1206,1570653803,"Anyone know where I can find a good source on learning how to code up a GAN from scratch? I took Andrew Ng's course on Deep Learning and found it useful to build networks up from scratch; however, I can't seem to find a good tutorial for GAN's. I'm also trying to learn more about the finer details of them so if there aren't any good tutorials I'm also open to any other sources.",19,23,False,self,,,,,
71,deeplearning,t5_2t5eh,2019-10-10,2019,10,10,8,dfpfbc,self.deeplearning,Question about pretrained models applied to medical imaging,https://www.reddit.com/r/deeplearning/comments/dfpfbc/question_about_pretrained_models_applied_to/,nickryd,1570664041,"Hey, Im pretty new to deep learning but interested in applying previously trained deep learning models onto medical imaging to see if any useful information could be gained from this. Im not aware of any available pre trained models that include thousands/millions of medical images (if there is let me know!) and so my question is more so wondering if there are pre trained models based on large data sets that may not be medical images but could still be useful in analyzing medical images.

For example, wondering if in medical imaging or really any field there is research on using models that were trained on one thing to help classify an unrelated image. Like train on shapes to identify animals. Dont know if this is an area with a lot of research or not much or even what it would be called but would greatly appreciate any recommendations.

My main goal is to run deep learning models off of small datasets that I have access to and would want to improve the predictive ability by applying various other pretrained models. Appreciate any comments or suggestions!",2,2,False,self,,,,,
72,deeplearning,t5_2t5eh,2019-10-10,2019,10,10,11,dfrk16,i.redd.it,ClearnRL: RL library that focuses on easy experimental research with cloud logging,https://www.reddit.com/r/deeplearning/comments/dfrk16/clearnrl_rl_library_that_focuses_on_easy/,vwxyzjn,1570674618,,4,1,False,image,,,,,
73,deeplearning,t5_2t5eh,2019-10-10,2019,10,10,12,dfsiaz,self.deeplearning,Total Beginner (Zero programming experience) trying to develop deep learning algorithm for research...,https://www.reddit.com/r/deeplearning/comments/dfsiaz/total_beginner_zero_programming_experience_trying/,chinnyachebe,1570679842,"I've been assigned to develop a deep learning algorithm for research in laser manufacturing. The problem is that I am in my FIRST year in college and from what I've read online, many of the necessary mathematics used in deep learning requires an in depth understanding of linear algebra (one of the first topics in Andrew Ng's Deep Learning course is REVIEW of linear algebra, meanwhile I'm taking Calc 2) 

I have already done the necessary background research regarding the basic structure and functionality of a deep learning system from a conceptual view, but the thing is, I have absolutely no experience coding in Python or any other language. Will carrying out this research project be impossible? I understand that I can use pre trained models and stuff as a reference point, but will it be doable if someone like me who has zero coding skills and a lack of mathematical knowledge were to try to develop this algorithm? 

By the way, I'm an engineering major...",3,1,False,self,,,,,
74,deeplearning,t5_2t5eh,2019-10-10,2019,10,10,15,dftw0f,self.deeplearning,Lightweight DL,https://www.reddit.com/r/deeplearning/comments/dftw0f/lightweight_dl/,timisis,1570688378,"Hi, I am trying to get Google Cloud's ""free VM"" and similar to work for me, my reasoning is that the 600MB RAM should be enough for 100MB (and smaller) networks, and I can live with however long that would take on the 1/5 core or whatever Google can guarantee me. Sadly this kind of RAM is certainly not enough to compile python modules when that is necessary, and loading the standard ML libs is a hit and miss. I am generally hitting memory errors. Is there a well known small ML/DL setup? I want to do slow and steady training, for the inference part there are several candidates a google search away. I realize it may be more ""cloudy"" to use some ML free tier and control it from the VM, but I thought there must be some virtue in self-hosting DL for free, however slow.",4,1,False,self,,,,,
75,deeplearning,t5_2t5eh,2019-10-10,2019,10,10,18,dfvf1s,medium.com,Best Ways to Improve Cloud ERP with AI and Machine Learning,https://www.reddit.com/r/deeplearning/comments/dfvf1s/best_ways_to_improve_cloud_erp_with_ai_and/,erp_oodles,1570699227,,0,1,False,default,,,,,
76,deeplearning,t5_2t5eh,2019-10-10,2019,10,10,20,dfwqtq,onlineitguru.com,Deep Learning with Python,https://www.reddit.com/r/deeplearning/comments/dfwqtq/deep_learning_with_python/,swethasree12,1570707590,,0,1,False,default,,,,,
77,deeplearning,t5_2t5eh,2019-10-10,2019,10,10,23,dfyxmc,self.deeplearning,[Research] QUANTIZED REINFORCEMENT LEARNING (QUARL),https://www.reddit.com/r/deeplearning/comments/dfyxmc/research_quantized_reinforcement_learning_quarl/,cdossman,1570718275," [https://medium.com/ai%C2%B3-theory-practice-business/what-if-quantization-was-applied-for-reinforcement-learning-c3e6af50b054](https://medium.com/ai%C2%B3-theory-practice-business/what-if-quantization-was-applied-for-reinforcement-learning-c3e6af50b054) 

**Abstract:**  Quantization can help reduce the memory, compute, and energy demands of deep neural networks without significantly harming their quality. However, whether these prior techniques, applied traditionally to image-based models, work with the same efficacy to the sequential decision-making process in reinforcement learning remains an unanswered question. To address this void, we conduct the first comprehensive empirical study that quantifies the effects of quantization on various deep reinforcement learning policies with the intent to reduce their computational resource demands. We apply techniques such as post-training quantization and quantization aware training to a spectrum of reinforcement learning tasks (such as Pong, Breakout, BeamRider and more) and training algorithms (such as PPO, A2C, DDPG, and DQN). Across this spectrum of tasks and learning algorithms, we show that policies can be quantized to 6-8 bits of precision without loss of accuracy. We also show that certain tasks and reinforcement learning algorithms yield policies that are more difficult to quantize due to their effect of widening the models distribution of weights and that quantization aware training consistently improves results over post-training quantization and oftentimes even over the full precision baseline. Finally, we demonstrate real-world applications of quantization for reinforcement learning. We use mixed/half-precision training to train a Pong model 50% faster, and deploy a quantized reinforcement learning-based navigation policy onto an embedded system, achieving an 18 speedup and a 4 reduction in memory usage over an unquantized policy. 

 [https://arxiv.org/pdf/1910.01055.pdf](https://arxiv.org/pdf/1910.01055.pdf)",0,11,False,self,,,,,
78,deeplearning,t5_2t5eh,2019-10-11,2019,10,11,3,dg1y1q,self.deeplearning,How convert custom Keras code to Onnx?,https://www.reddit.com/r/deeplearning/comments/dg1y1q/how_convert_custom_keras_code_to_onnx/,74throwaway,1570730914,"I'm using the code from https://github.com/qubvel/segmentation_models/blob/master/examples/multiclass%20segmentation%20(camvid).ipynb

I'm trying to use keras2onnx with it also. I tried

	import keras2onnx
	import onnxruntime

	# convert to onnx model
	onnx_model = keras2onnx.convert_keras(model, model.name)

	# runtime prediction
	content = onnx_model.SerializeToString()
	sess = onnxruntime.InferenceSession(content)
	x = x if isinstance(x, list) else [x]
	feed = dict([(input.name, x[n]) for n, input in enumerate(sess.get_inputs())])
	pred_onnx = sess.run(None, feed)

But I got the following error:

	InvalidArgumentError                      Traceback (most recent call last)
	~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)
	    426         results = c_api.TF_GraphImportGraphDefWithResults(
	--&gt; 427             graph._c_graph, serialized, options)  # pylint: disable=protected-access
	    428         results = c_api_util.ScopedTFImportGraphDefResults(results)

	InvalidArgumentError: Node 'block1b_drop/cond/mul/y': Unknown input node '^block1b_drop/cond/switch_t'

	During handling of the above exception, another exception occurred:

	ValueError                                Traceback (most recent call last)
	&lt;ipython-input-16-a5d3ba8b443c&gt; in &lt;module&gt;
	      3 
	      4 # convert to onnx model
	----&gt; 5 onnx_model = keras2onnx.convert_keras(model, model.name)
	      6 
	      7 # runtime prediction

	~/anaconda3/lib/python3.6/site-packages/keras2onnx/main.py in convert_keras(model, name, doc_string, target_opset, channel_first_inputs, debug_mode, custom_op_conversions)
	     98                         custom_op_dict=custom_op_conversions)
	     99     topology.debug_mode = debug_mode
	--&gt; 100     parse_graph(topology, sess.graph, target_opset, output_names)
	    101     topology.compile()
	    102 

	~/anaconda3/lib/python3.6/site-packages/keras2onnx/parser.py in parse_graph(topo, graph, target_opset, output_names)
	    647         topo.raw_model.add_input_name(str_value)
	    648 
	--&gt; 649     return _parse_graph_scope(graph, keras_layer_ts_map, topo, top_level, output_names)

	~/anaconda3/lib/python3.6/site-packages/keras2onnx/parser.py in _parse_graph_scope(graph, keras_node_dict, topology, top_scope, output_names)
	    597             _convert_keras_timedistributed(graph, nodes, layer_key_, model_, varset)
	    598         elif layer_key_ is None or get_converter(type(layer_key_)) is None:
	--&gt; 599             _convert_general_scope(nodes, varset)
	    600         else:
	    601             _convert_keras_scope(graph, nodes, layer_key_, model_, varset)

	~/anaconda3/lib/python3.6/site-packages/keras2onnx/parser.py in _convert_general_scope(node_list, varset)
	    299 
	    300     sess = keras.backend.get_session()
	--&gt; 301     subgraph, replacement = create_subgraph(sess.graph, node_list, sess, operator.full_name)
	    302     setattr(operator, 'subgraph', subgraph)
	    303     vars_, ts = _locate_inputs_by_node(node_list, varset)

	~/anaconda3/lib/python3.6/site-packages/keras2onnx/subgraph.py in create_subgraph(tf_graph, node_list, sess, dst_scope)
	    135     with tf.Graph().as_default() as sub_graph:
	    136         im_scope = """" if dst_scope is None else dst_scope
	--&gt; 137         tf.import_graph_def(output_graph_def, name=im_scope)
	    138         if im_scope:
	    139             replacement = {k_: im_scope + '/' + k_ for k_ in replacement}

	~/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
	    505                 'in a future version' if date is None else ('after %s' % date),
	    506                 instructions)
	--&gt; 507       return func(*args, **kwargs)
	    508 
	    509     doc = _add_deprecated_arg_notice_to_docstring(

	~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)
	    429       except errors.InvalidArgumentError as e:
	    430         # Convert to ValueError for backwards compatibility.
	--&gt; 431         raise ValueError(str(e))
	    432 
	    433     # Create _DefinedFunctions for any imported functions.

	ValueError: Node 'block1b_drop/cond/mul/y': Unknown input node '^block1b_drop/cond/switch_t'

how can I convert this code to Onnx?",0,1,False,self,,,,,
79,deeplearning,t5_2t5eh,2019-10-11,2019,10,11,5,dg434w,medium.com,"Facebook Debuts PyTorch 1.3 With PyTorch Mobile, Quantization, TPU Support and More",https://www.reddit.com/r/deeplearning/comments/dg434w/facebook_debuts_pytorch_13_with_pytorch_mobile/,Yuqing7,1570739583,,8,73,False,default,,,,,
80,deeplearning,t5_2t5eh,2019-10-11,2019,10,11,15,dgaz5g,thegradient.pub,The State of Machine Learning Frameworks in 2019,https://www.reddit.com/r/deeplearning/comments/dgaz5g/the_state_of_machine_learning_frameworks_in_2019/,asuagar,1570774501,,0,2,False,default,,,,,
81,deeplearning,t5_2t5eh,2019-10-11,2019,10,11,15,dgb4h4,self.deeplearning,Sentence Generation from specific words,https://www.reddit.com/r/deeplearning/comments/dgb4h4/sentence_generation_from_specific_words/,masternachiket,1570775490,"So I am working on a project which requires me to build a sentence generation system, but unlike other sentence generation systems I want my system to take an array of random words and generate a proper sentence using those words only.
Suppose I give my system input as ['Mumbai', 'I', 'from', 'am']
The system's output should be I am from Mumbai. 

Can someone please tell me how I can do this? Python packages and library.

Thank you!",0,1,False,self,,,,,
82,deeplearning,t5_2t5eh,2019-10-11,2019,10,11,16,dgbrng,self.deeplearning,"Why COCO evaluate with AP, AR by size? what are AR max =1, 10, 100's meaning?",https://www.reddit.com/r/deeplearning/comments/dgbrng/why_coco_evaluate_with_ap_ar_by_size_what_are_ar/,i_m_eden,1570779808,"I'm reading [COCO Metrics](http://cocodataset.org/#detection-eval) right now. And I have 2 questions about it.

This is the Metrics of COCO 

&amp;#x200B;

[Metrics of COCO](https://i.redd.it/r2z923249vr31.png)

&amp;#x200B;

1. I'm wondering why COCO evaluate AP and AR by size. What effect does image size have?
2. They measure AR by max which are 1, 10, 100. And they said AR max=1 is 'AR given 1 detection per image"". Then, if model detect multiple objects per image, how to calculate AR? I can't understand the meaning of 'max'.",0,1,False,self,,,,,
83,deeplearning,t5_2t5eh,2019-10-11,2019,10,11,18,dgcf2x,self.deeplearning,Understanding Adversarial Attack,https://www.reddit.com/r/deeplearning/comments/dgcf2x/understanding_adversarial_attack/,Mesode,1570784599,"I have watched the Standford deep learning lecture 16, which is about ""Adversarial Examples and Adversarial Training"" held by Ian Goodfellow (the inventor of the GANs). He explains that it is very easy to fool a CNN-classifier by changing an input image while the difference of the images is unrecognizable by human perception. As an example, given an image of a cat, and given that the CNN classifies it correctly as a cat, you can find a transformation of the image such that the image still looks the same by human perception but is classified as a ship! Ian Goodfellow already mentioned that in his well known deep learning book. 

However, I am wondering if you have 10 different CNN-classifier, would it become exponentially harder to find an adversarial image that fools all 10 classifiers or is there some systematic - some ""magic"" behind the  adversarial images?",5,8,False,self,,,,,
84,deeplearning,t5_2t5eh,2019-10-11,2019,10,11,20,dge1qu,youtu.be,AI &amp; Architecture,https://www.reddit.com/r/deeplearning/comments/dge1qu/ai_architecture/,cmillionaire9,1570794781,,3,27,False,default,,,,,
85,deeplearning,t5_2t5eh,2019-10-12,2019,10,12,1,dghv6t,trumptraveling.blogspot.com,FB Detectron2,https://www.reddit.com/r/deeplearning/comments/dghv6t/fb_detectron2/,qikpal,1570812530,,0,1,False,default,,,,,
86,deeplearning,t5_2t5eh,2019-10-12,2019,10,12,4,dgjpyz,self.deeplearning,Looking for rgb + depth dataset that contains humans for portrait mode,https://www.reddit.com/r/deeplearning/comments/dgjpyz/looking_for_rgb_depth_dataset_that_contains/,AhmedZubairGCU,1570820863,I am looking at rgb + depth dataset but all such datasets have indoor and outdoor scenes but no humans. I am looking for a dataset that contains humans and animals so it is appropriate to use in training a cnn that creates portrait mode effect. I am also thinking if such dataset is not available I creat a synthetic dataset. I have this idea to use image segmentation and add these segments to photos with known depth map at varying distances. Can anybody guide me how to create such dataset.,2,3,False,self,,,,,
87,deeplearning,t5_2t5eh,2019-10-12,2019,10,12,6,dglo8l,self.deeplearning,Multi-class semantic segmentation convertible to Onnx?,https://www.reddit.com/r/deeplearning/comments/dglo8l/multiclass_semantic_segmentation_convertible_to/,74throwaway,1570829689,"I need to train a multi-class semantic segmentation NN that can be convertible to Onnx

The only multi-class semantic segmentation NN that I was able to get to run properly was from https://github.com/qubvel/segmentation_models/blob/master/examples/multiclass%20segmentation%20(camvid).ipynb

However, when I tried to convert that code to Onnx, it wouldn't work

Anyone know of any alternatives?",0,1,False,self,,,,,
88,deeplearning,t5_2t5eh,2019-10-12,2019,10,12,16,dgrzpq,self.deeplearning,"How do you identify a top article in the field of artificial intelligence such as Deep Landing, Machine Learning, etc.?",https://www.reddit.com/r/deeplearning/comments/dgrzpq/how_do_you_identify_a_top_article_in_the_field_of/,Doctor_who1,1570865764,"How do you identify a top article in the field of artificial intelligence such as Deep Landing, Machine Learning, etc.?",3,1,False,self,,,,,
89,deeplearning,t5_2t5eh,2019-10-12,2019,10,12,18,dgsv4w,self.deeplearning,Combine #Reinforcement_learning with #deep_learning for abstractive #text_summarization,https://www.reddit.com/r/deeplearning/comments/dgsv4w/combine_reinforcement_learning_with_deep_learning/,theamrzaki,1570872314,[removed],0,1,False,self,,,,,
90,deeplearning,t5_2t5eh,2019-10-12,2019,10,12,21,dgup5t,self.deeplearning,Anyone with experience with training NLP models?,https://www.reddit.com/r/deeplearning/comments/dgup5t/anyone_with_experience_with_training_nlp_models/,kajobkajob,1570884402,"Hi, I'm looking to do some work with architectures like transformers e.g. bert, albert but I would like to know how much training could cost first.

Could anyone give me ballpark figures for the cost of training from scratch or finetuning with cloud TPUs or AWS?

If I were forced to work on an RTX 2080 what type of models would be feasible?

Since I'm only looking for ballpark figures if the answer depends on some variables could you give the answer with respect to some common cases?",12,8,False,self,,,,,
91,deeplearning,t5_2t5eh,2019-10-13,2019,10,13,0,dgwt7i,self.deeplearning,"Seeking resources to have a deeper understanding of CNN's intermediate conv layers, how they work to extract feature.",https://www.reddit.com/r/deeplearning/comments/dgwt7i/seeking_resources_to_have_a_deeper_understanding/,bikigoogler,1570895146,"Hey guys, I have started with deep learning.I know basic concept of CNN but i want to know about how the inner layers extract feature . eg. in case of training dog images how the inner layers extract feature like paw, tail etc. Please suggest something that works.",8,2,False,self,,,,,
92,deeplearning,t5_2t5eh,2019-10-13,2019,10,13,2,dgy378,self.deeplearning,Using Tensorflow/keras with Python multiprocessing pool,https://www.reddit.com/r/deeplearning/comments/dgy378/using_tensorflowkeras_with_python_multiprocessing/,shahriar49,1570901080,"I want to do a neural network training in Tensorflow/Keras but prefer to use python multiprocessing module to maximize use of system resources and save time. What I do is simply like this (I want to run this code on a system without GPU or with one or more GPUs):

        import ... (required modules)
        from multiprocessing import Pool
        import tensorflow as tf
        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        sess = tf.Session(config=config)
        tf.keras.backend.set_session(sess)
        
        ... some tf and non-tf variable initializations...
        ... some functions to facilitate reading tensorflow datasets in TFRecord format...
        ... function defining keras model...
        
        # Main worker function
        def doWork(args):
            from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
            from tensorflow.keras.models import load_model
        
            train_data = read_datasets(...)
            val_data = read_datasets(...)
            test_data = read_datasets(...)
        
            if (NumGPUs &gt; 1):
                strategy = tf.distribute.MirroredStrategy()
                with strategy.scope():
                    model = keras_model(...)
                    model.compile(...)
            else:
                model = keras_model(...)
                model.compile(...)
        
            model.fit(train_data, epochs=epochs, steps_per_epoch=train_steps, ...)
            _, test_acc = model.evaluate(test_data, steps=test_steps)
            ...log results...
        
        if __name__ == '__main__':
            pool = Pool(processes=2)
            a1 = &lt;set of parameters for the first run&gt;
            a1 = &lt;set of parameters for the second run&gt;
            pool.map(doWork, (a1, a2))
    

I can run this code on different computers and get my results, but some times I face system hangups (especially if I want to abort execution by pressing CTRL+C) or program termination with different errors, and I guess the above is not the right style of combining Tensorflow/Keras and Python multiprocessing. What is the correct style of writing the above code?",4,1,False,self,,,,,
93,deeplearning,t5_2t5eh,2019-10-13,2019,10,13,6,dh1dn7,self.deeplearning,"Ubuntu 18.04, Windows 10 - Dual Hard Drive, Dual Boot - best practices?",https://www.reddit.com/r/deeplearning/comments/dh1dn7/ubuntu_1804_windows_10_dual_hard_drive_dual_boot/,Jollyhrothgar,1570916582,"I have a PC with a nice nvidia graphics card, and two fast hard drives. One annoying thing that I have dealt with for years is dual-booting, and getting both systems stable.  


I can always set up the dual boot, but inevitably, the boot loader breaks with an update, the the linux kernel updates, the drivers update (in either windows or linux) and then everything breaks.

&amp;#x200B;

Common symptoms include: loss of the ability to boot into Windows, ability to boot into Linux only, but loss of the ability to log in to the GUI (ssh only). Linux always breaks with an update, and I assume this has to do with CUDA drivers, NVIDIA drivers, and some kind of kernel mismatch. I get so tired of booting into a live CD, repairing the boot loader, reinstalling the OS(s), etc.  


I know this is vague and terrible, but does anyone have a guide, set of best practices, etc that can lead to a harmonious, stable dual boot set up? Does this exist yet?",30,6,False,self,,,,,
94,deeplearning,t5_2t5eh,2019-10-13,2019,10,13,14,dh6c2f,self.deeplearning,Keras custom loss function with Mahalanobis distance loss how to,https://www.reddit.com/r/deeplearning/comments/dh6c2f/keras_custom_loss_function_with_mahalanobis/,frenomer,1570943417," 

I am trying to implement a custom loss function in Keras using Mahalanobis distance loss. however I always run into this annoying ERROR. 

&gt; ValueError: Shape must be at least rank 2 but is rank 0 for 'loss\_88/dense\_270\_loss/MatrixInverse' (op: 'MatrixInverse') with input shapes: \[\]. 

Mahalanobis distance (or ""generalized squared interpoint distance"" for its squared value\[3\]) can also be defined as a dissimilarity measure between two random vectors x and y of the same distribution with the covariance matrix S.

d(x,y) = square \[Transpose(x-y) \* Inverse(S)\* (x-y)\]

([https://en.wikipedia.org/wiki/Mahalanobis\_distance](https://en.wikipedia.org/wiki/Mahalanobis_distance))

&amp;#x200B;

 

    n_classes = 4 
    n_samples=800 X, y = make_classification(n_samples=n_samples, n_features=20, n_informative=4, n_redundant=0, n_classes=n_classes, n_clusters_per_class=2) 

y = to\_categorical(y)

Xtrainb, testXb, ytrainb, ytestb = train\_test\_split(X, y, test\_size = 0.3, random\_state=42)

&amp;#x200B;

 

    x_trainb = np.reshape(Xtrainb, (Xtrainb.shape[0], Xtrainb.shape[1], 1))
    Xtestb = np.reshape(testXb, (testXb.shape[0], testXb.shape[1], 1))  
    
    densesize = 4
    input_datab = Input(shape=(Xtrainb.shape[1],1)) 
    epochs = 10
    batch_size = 32
    
    
    ########
    def mahalanobis(y_true, y_pred):
        x_minus_mn_with_transpose = K.transpose(y_true - y_pred)
        Covariance = covr1(y_true, y_pred)
        inv_covmat = tf.linalg.inv(Covariance)
        x_minus_mn = y_true - y_pred
        left_term = K.dot(x_minus_mn, inv_covmat)
        D_square = K.dot(left_term, x_minus_mn_with_transpose)
        return D_square
    
    def covr1(y_true, y_pred):
        #x_mean = K.mean(y_true)
        #y_mean = K.mean(y_pred)
        Cov_numerator = K.sum(((y_true - y_pred)*(y_true - y_pred)))
        Cov_denomerator = len(Xtrainb)-1
        Covariance = (Cov_numerator / Cov_denomerator)
        return Covarianc
    
    conv1= Conv1D(filters=80, kernel_size=2, padding='same',   input_dim=Xtrainb.shape[1])(input_datab)
    maxpool = MaxPooling1D(pool_size=3, stride=3 )(conv1)
    conv2= Conv1D(filters=50, kernel_size=2, padding='same',   input_dim=Xtrainb.shape[1])(maxpool)
    maxpool = MaxPooling1D(pool_size=3, stride=3)(conv2)
    flatten = Flatten()(maxpool)
    dense = Dense(84, activation='relu')(flatten)
    dense = Dense(1024, activation='relu')(flatten)
    dense = Dense(densesize, activation='softmax')(dense)
    model = Model(inputs=[input_datab],outputs=[dense])
    model.compile(loss= mahalanobis,  optimizer='adam', metrics=['acc'])
    hist = model.fit(x_trainb, ytrainb, validation_data=(Xtestb, ytestb), epochs=epochs, batch_size=batch_size)",3,6,False,self,,,,,
95,deeplearning,t5_2t5eh,2019-10-13,2019,10,13,21,dh9uy5,self.deeplearning,nn_builder - builds neural networks with less boilerplate code,https://www.reddit.com/r/deeplearning/comments/dh9uy5/nn_builder_builds_neural_networks_with_less/,__data_science__,1570969104,"nn\_builder is a package that lets you build simple NNs, CNNs and RNNs in 1 line by reducing the amount of boilerplate code. It works with PyTorch and TensorFlow 2.0. Check it out!

https://github.com/p-christ/nn\_builder",2,8,False,self,,,,,
96,deeplearning,t5_2t5eh,2019-10-14,2019,10,14,1,dhd1xq,self.deeplearning,"AI Weekly News (October 13th, 2019) - PyTorch 1.3, Google's ROBEL, NVIDIA's inception showcase, and many more!",https://www.reddit.com/r/deeplearning/comments/dhd1xq/ai_weekly_news_october_13th_2019_pytorch_13/,HenryAILabs,1570985358,[removed],0,1,False,self,,,,,
97,deeplearning,t5_2t5eh,2019-10-14,2019,10,14,2,dhdpen,youtu.be,Eliminating car collisions with #AI,https://www.reddit.com/r/deeplearning/comments/dhdpen/eliminating_car_collisions_with_ai/,cmillionaire9,1570988320,,0,0,False,default,,,,,
98,deeplearning,t5_2t5eh,2019-10-14,2019,10,14,4,dhfjyr,self.deeplearning,2x Quadro RTX 5000 or Titan RTX,https://www.reddit.com/r/deeplearning/comments/dhfjyr/2x_quadro_rtx_5000_or_titan_rtx/,Red_HeadRedemption,1570996333,"I'm sorry, if you get tired from these kind of questions, but I got a special offer for 2 Quadro 5000s and I don't know what to do. At the moment I use a MSI 1060 Gaming 6G, but I wamt an upgrade. So there is the oppertunity to get a Titan RTX as well as 2 Quadro RTX 5000, both for 1900. Which one is the better choice ?",7,12,False,self,,,,,
99,deeplearning,t5_2t5eh,2019-10-14,2019,10,14,7,dhhivh,self.deeplearning,Data transmitting if you can not simulate noise,https://www.reddit.com/r/deeplearning/comments/dhhivh/data_transmitting_if_you_can_not_simulate_noise/,Filarius,1571005345,"I'm just start learning Deep Learning and being curious to see how NN will  find solution for problem from my hobby project.

I'm digging internet and can't find example what I can start from, or any really good idea to use.

I need to create Encoder and Decoder to transmit data over one noisy environment, and you can only observate noise by trasmitting data, and can not make right simulation of this noise. Transmition had to be made manually and take some time.

With my knowladge I think only about next:  
\- source data splited by small chunks, maybe start research with only 1 byte long chunk, next extract bits, so there will be 1D array of ones and zeros  
\- Encoder convert N bits into, lets say... 10\*N bytes

\- using Encoder convert pretty many source data, transmit all at one time, receive signal with noise.

\- and Decoder must recover bits from signal+noise with as low count of errors as possible

&amp;#x200B;

I did some research using things what people usually use to solve it in classic way, so I have starting point to choose data rate to be sure its really possible to recover data after this kind of noise.

 I wish NN Encoder and Deconder will learn noise pattern to allow highest possible data rate without any bit inverted.

But I have no idea where to start from to train NN.  There is Autoencoder NN whats look similar, but I can not simulate right kind of noise in programming code or make classical train because of problem definition. I wonder what will be best way to train NN to solve such problem.",0,3,False,self,,,,,
100,deeplearning,t5_2t5eh,2019-10-14,2019,10,14,7,dhhjx3,self.deeplearning,"AI Weekly Update #8 - October 13th, 2019!",https://www.reddit.com/r/deeplearning/comments/dhhjx3/ai_weekly_update_8_october_13th_2019/,HenryAILabs,1571005497,[https://youtu.be/cTevyDueL3Q](https://l.facebook.com/l.php?u=https%3A%2F%2Ft.co%2FB3ZqWfRvkW%3Famp%3D1%26fbclid%3DIwAR3zbWz-Ju6AjhIV01qvXNDdruTE9iYGXaWNW4C0roJz8dbZQvyNTqM6QUQ&amp;h=AT2jj0bMHOnY5NFnpl43iF1aHk__VrgSbAZp-nnbUOfTtcChInNrcuGyKx29pRjFVKLUeE74wIfRjp8oCeJbBGu3-uY1XPPOvBVSA_ZaQs5TEC8lBTdDv8qxIB-FJlC8Gm8SiwMvQLrQuB8qOVNUzMQPFGY),0,1,False,self,,,,,
101,deeplearning,t5_2t5eh,2019-10-14,2019,10,14,19,dhoplm,medium.com,Business Benefits of Using Python for AI Projects,https://www.reddit.com/r/deeplearning/comments/dhoplm/business_benefits_of_using_python_for_ai_projects/,Rohan_Sharma124,1571049108,,0,0,False,default,,,,,
102,deeplearning,t5_2t5eh,2019-10-14,2019,10,14,21,dhpswp,self.deeplearning,Speech Noise Surpression,https://www.reddit.com/r/deeplearning/comments/dhpswp/speech_noise_surpression/,theShm00,1571055749,"Hey guys,

im starting to work on my masters thesis which should be heading in the direction of implementing a real time noise surpression for speech using neural networks. Im quite new to the field of DL/ML. Im looking for some good example code which helps me understand the procedures needed for training nets on audio. Ive come across the RNNoise project from Mozilla, which has its code open source: [https://people.xiph.org/\~jm/demo/rnnoise/](https://people.xiph.org/~jm/demo/rnnoise/) Trying to understand the code really confuses me atm. So while im trying to understand whats happening there, maybe some of you have some good resources that help me learn? Really nice would be a sort of End-to-End System, which directly outputs the noise free data. 

Do you have anymore tips you can give me as im starting with this topic for my thesis?

thank you :)",5,9,False,self,,,,,
103,deeplearning,t5_2t5eh,2019-10-14,2019,10,14,23,dhrfp3,self.deeplearning,A group of sites and courseware will benefit you too ..,https://www.reddit.com/r/deeplearning/comments/dhrfp3/a_group_of_sites_and_courseware_will_benefit_you/,saradinto,1571063811,[removed],0,1,False,self,,,,,
104,deeplearning,t5_2t5eh,2019-10-14,2019,10,14,23,dhrj07,self.deeplearning,Study group- deep learning for molecules generation,https://www.reddit.com/r/deeplearning/comments/dhrj07/study_group_deep_learning_for_molecules_generation/,mostafabenh,1571064227,"I am helping a friend to organize an online study group about neural networks for molecule generation, who is interested in participating?

 You need to be very motivated and have enough free time to study this topic

Add me on LinkedIn if you are interested:  https://www.linkedin.com/in/mostapha-benhenda",5,5,False,self,,,,,
105,deeplearning,t5_2t5eh,2019-10-15,2019,10,15,1,dht76e,self.deeplearning,[Research] New Dataset for DeepFake Forensics,https://www.reddit.com/r/deeplearning/comments/dht76e/research_new_dataset_for_deepfake_forensics/,cdossman,1571071299," A new dataset and detection algorithms for DeepFake Forensics  
[http://www.cs.albany.edu/\~lsw/celeb-deepfakeforensics.html](http://www.cs.albany.edu/~lsw/celeb-deepfakeforensics.html?fbclid=IwAR2CYyTv91KaEHykj5cN8X3Q_TCcn2auIrivCs5m5TpMs683_hamTXosSOs)",0,1,False,self,,,,,
106,deeplearning,t5_2t5eh,2019-10-15,2019,10,15,3,dhunvy,self.deeplearning,Pretrained model weights for generative chatbot for interactive conversation.,https://www.reddit.com/r/deeplearning/comments/dhunvy/pretrained_model_weights_for_generative_chatbot/,nik9993,1571077289,"Where can i get a pretrained model weights for rnn, seq2seq chatbot for general purpose interactive chatting.",0,1,False,self,,,,,
107,deeplearning,t5_2t5eh,2019-10-15,2019,10,15,4,dhvdqu,self.deeplearning,Career Advice in ML and how to read research papers - Andrew Ng's Notes,https://www.reddit.com/r/deeplearning/comments/dhvdqu/career_advice_in_ml_and_how_to_read_research/,deep_ak,1571080318,"Here I have made notes of the **Deep Learning CS230** Lecture given by ***Andrew Ng*** on how to navigate a career in ML/DL and how to read research papers.

[https://deeps.site/blog/2019/10/14/reading-research-papers-career-advice/](https://deeps.site/blog/2019/10/14/reading-research-papers-career-advice/)

The **one hour lecture** has been **summarised into concise 5 minutes** read to ***save out on your time*** with  
visualisations to enrich the delivered content.  


Hope it helps you to build on top of Andrew's insights and save time.",10,91,False,self,,,,,
108,deeplearning,t5_2t5eh,2019-10-15,2019,10,15,6,dhx6hu,self.deeplearning,Predict future image/cloud,https://www.reddit.com/r/deeplearning/comments/dhx6hu/predict_future_imagecloud/,nii3lsh,1571087492,"I have a lot of images of the sky at some location.   
It is well structured and I have plenty of them.   
Every 15 seconds there is a new picture and I have data of the last year(s).   


At the moment I use optical flow the predict an future image.   
I do this by detecting clouds and calculating the velocity by looking at multiple images in a row.   
2 are enough but it will be more accurate using more images. 

I was wondering if you guys could suggest me a deep learning method to do this? Or some papers that does something similar.  I was thinking of an CNN network where I input 2 images and the output will be 1 image. This image can I compare to the ground truth picture.

Anyone thinks if this might work? Or have other architecture suggestions?

Kind regards =D",0,1,False,self,,,,,
109,deeplearning,t5_2t5eh,2019-10-15,2019,10,15,6,dhxbc9,self.deeplearning,"Reinforcement Learning Chapter 8! This chapter unifies Model-based and Model-free learning and presents Monte Carlo Tree Search, a key component to the AlphaGo algorithm!",https://www.reddit.com/r/deeplearning/comments/dhxbc9/reinforcement_learning_chapter_8_this_chapter/,HenryAILabs,1571088056,[removed],0,1,False,self,,,,,
110,deeplearning,t5_2t5eh,2019-10-15,2019,10,15,10,di0cln,self.deeplearning,"Dataset to detect inappropriate content(nudity, violence,drugs etc.,) in images for content moderation",https://www.reddit.com/r/deeplearning/comments/di0cln/dataset_to_detect_inappropriate_contentnudity/,Nike_Zoldyck,1571101858,"I'm looking for some information on how the content filtering works in Facebook, Instagram etc to flag nudity, violence, offensive or toxic comments,religious and racial stereotypes etc. I wasn't able to find any scientific papers about the inner workings or the use of deep learning or Computer vision for it. I'm fairly new to CV and i'm hoping for some guidance on a data set , perhaps for detecting inappropriate content on T-shirts in retail. Any suggestions on how to go about searching or to create a dataset and what to keep in mind? Any algorithms to focus on like yolov3 or faster RCNN to read the text on the t-shirts and understand the context of it would also be extremely helpful",0,0,False,self,,,,,
111,deeplearning,t5_2t5eh,2019-10-15,2019,10,15,11,di13oc,self.deeplearning,Using RTX 2080 Ti and GTX 1080 Ti in the same computer for distributed training,https://www.reddit.com/r/deeplearning/comments/di13oc/using_rtx_2080_ti_and_gtx_1080_ti_in_the_same/,deep_888,1571105704,"Currently, I have a single GTX 1080 Ti but it is easy to overflow when running large jobs. I'd like to set up a double-GPU workstation and train the model using PyTorch distributed. I read some posts saying that using two different GPUs in one build can work, so I'm considering buying an RTX 2080 Ti. My questions are: 

1. What's the performance of using a combination of RTX 2080 Ti + GTX 1080 Ti, compared to using two GTX 1080 Ti, for PyTorch distributed training? Any communication problem?
2. My motherboard is ROG STRIX Z390-E GAMING, which supports PCI-E 3.0 X16 for single GPU and X8 + X8 for two GPUs. Under this condition, if I buy RTX 2080 Ti as the second GPU, will the performance be equal to that of two GTX 1080 Ti?

Thank you very much in advance!",2,7,False,self,,,,,
112,deeplearning,t5_2t5eh,2019-10-15,2019,10,15,17,di4ome,towardsdatascience.com,Is Deep Learning Already Hitting its Limitations?,https://www.reddit.com/r/deeplearning/comments/di4ome/is_deep_learning_already_hitting_its_limitations/,RubiksCodeNMZ,1571126935,,0,1,False,default,,,,,
113,deeplearning,t5_2t5eh,2019-10-15,2019,10,15,17,di4p6s,storage.googleapis.com,Neural Graph Learning: Training Neural Networks Using Graphs - paper,https://www.reddit.com/r/deeplearning/comments/di4p6s/neural_graph_learning_training_neural_networks/,RubiksCodeNMZ,1571127039,,0,3,False,default,,,,,
114,deeplearning,t5_2t5eh,2019-10-15,2019,10,15,17,di4t42,github.com,Implementation of a character based CNN for text classification in PyTorch + Video Demo,https://www.reddit.com/r/deeplearning/comments/di4t42/implementation_of_a_character_based_cnn_for_text/,ahmedbesbes,1571127799,,0,4,False,default,,,,,
115,deeplearning,t5_2t5eh,2019-10-15,2019,10,15,18,di57s1,self.deeplearning,How a neural net automatically learn the new data in prod?,https://www.reddit.com/r/deeplearning/comments/di57s1/how_a_neural_net_automatically_learn_the_new_data/,redditaddict07,1571130632,"I am a novice trying to implemented Siamese-LSTM with co-attention to re-write user search queries to queries we understand or we can serve. Still not sure about the architecture or end to end flow. Please correct me if something is missing or wrongly assumed.

Example - 

**Female pregnancy clothe** to  **Women maternity dress.**

As suggested in attached [paper](https://cse.iitkgp.ac.in/~pawang/papers/sigir19.pdf), I will train LSTM on a data-set having two queries(x1, x2), y = 1(same) / 0 (not same).

Once trained and tested, 

I will deploy final model in prod. 

1. Whether it will be a sequence to sequence generation type model, we use for language translation or it will map user entered query to one of our input query **?**   
  \[I prefer to generate a new sequence(s) corresponding to the user entered sequence\] 
2.  Cannot understand, how this model will accommodate new queries which are very specific to our organisation/customers**?** Will I have to retrain the model again and again after analysing the new queries?

\---

It will be really appreciative,  If you can mentor or partner with me for this project. Thank you!",0,1,False,self,,,,,
116,deeplearning,t5_2t5eh,2019-10-15,2019,10,15,20,di6tff,medium.com,Zooming into the world of computer vision applications,https://www.reddit.com/r/deeplearning/comments/di6tff/zooming_into_the_world_of_computer_vision/,amarsingh1990,1571140525,,0,1,False,default,,,,,
117,deeplearning,t5_2t5eh,2019-10-15,2019,10,15,21,di76je,self.deeplearning,Properly setting up Linux for DL,https://www.reddit.com/r/deeplearning/comments/di76je/properly_setting_up_linux_for_dl/,AusBarbell,1571142541,"Hi, I'm currently dual booting Ubuntu 18.04 and windows on separate SSDs but I've managed to mess something up and have trouble booting into windows. Are there any tips or guides that I can follow to get a Linux OS up and running with pytorch and full GPU function without affecting my windows installation?",3,3,False,self,,,,,
118,deeplearning,t5_2t5eh,2019-10-15,2019,10,15,22,di7u1g,youtu.be,How to extract features from images,https://www.reddit.com/r/deeplearning/comments/di7u1g/how_to_extract_features_from_images/,nerdy_wits,1571145753,,0,0,False,image,,,,,
119,deeplearning,t5_2t5eh,2019-10-15,2019,10,15,23,di8ioz,self.deeplearning,[Research] Google AI: Robotics Benchmarks for Learning with Low-Cost Robots,https://www.reddit.com/r/deeplearning/comments/di8ioz/research_google_ai_robotics_benchmarks_for/,cdossman,1571148957," [https://medium.com/ai%C2%B3-theory-practice-business/modular-easy-to-build-and-extend-and-open-source-robotic-platforms-ca719585fb50](https://medium.com/ai%C2%B3-theory-practice-business/modular-easy-to-build-and-extend-and-open-source-robotic-platforms-ca719585fb50) 

**Abstract:**  ROBEL is an open-source platform of cost-effective robots designed for reinforcement learning in the real world. ROBEL introduces two robots, each aimed to accelerate reinforcement learning research in different task domains: DClaw is a three-fingered hand robot that facilitates learning dexterous manipulation tasks, and DKitty is a four-legged robot that facilitates learning agile legged locomotion tasks. These low-cost, modular robots are easy to maintain and are robust enough to sustain on-hardware reinforcement learning from scratch with over 14000 training hours registered on them to date. To leverage this platform, we propose an extensible set of continuous control benchmark tasks for each robot. These tasks feature dense and sparse task objectives and additionally introduce score metrics as hardware-safety. We provide benchmark scores on an initial set of tasks using a variety of learning-based methods. Furthermore, we show that these results can be replicated across copies of the robots located in different institutions.   [https://ai.googleblog.com/2019/10/robel-robotics-benchmarks-for-learning.html](https://ai.googleblog.com/2019/10/robel-robotics-benchmarks-for-learning.html)",0,0,False,self,,,,,
120,deeplearning,t5_2t5eh,2019-10-16,2019,10,16,1,dia26s,towardsdatascience.com,Trade Smarter w/ Deep Reinforcement Learning,https://www.reddit.com/r/deeplearning/comments/dia26s/trade_smarter_w_deep_reinforcement_learning/,notadamking,1571155727,,2,1,False,default,,,,,
121,deeplearning,t5_2t5eh,2019-10-16,2019,10,16,1,diah50,math.ias.edu,"Workshop on Theory of Deep Learning by IAS from Oct 15 to Oct 18 2019. Few talks presented here will also be presented at Neurips2019 / Nips2019. Day1: GAN optimization, Hierarchical learning (Spotlight talk), Curse of dimensionality.",https://www.reddit.com/r/deeplearning/comments/diah50/workshop_on_theory_of_deep_learning_by_ias_from/,adssidhu86,1571157467,,0,18,False,default,,,,,
122,deeplearning,t5_2t5eh,2019-10-16,2019,10,16,1,diandi,self.deeplearning,What is Causal Convolution?,https://www.reddit.com/r/deeplearning/comments/diandi/what_is_causal_convolution/,Sameer3079,1571158170,"I know that it is related to sequential data (time series), but I don't know what it is exactly.",4,13,False,self,,,,,
123,deeplearning,t5_2t5eh,2019-10-16,2019,10,16,3,dibwxg,self.deeplearning,Workshop on Theory of Deep Learning: Where next?,https://www.reddit.com/r/deeplearning/comments/dibwxg/workshop_on_theory_of_deep_learning_where_next/,aiforworld2,1571163291,"Workshop on Theory of Deep Learning: Where next?

2019-2020

Tuesday, October 15, 2019 - 09:00toFriday, October 18, 2019 - 06:00

At Institute for Advanced Studies 
Princeton University 

#deeplearning

Live Stream: https://www.ias.edu/livestream",0,8,False,self,,,,,
124,deeplearning,t5_2t5eh,2019-10-16,2019,10,16,5,die18r,self.deeplearning,Explaining OpenAI's Robotic Hand Rubik's Cube Solver!,https://www.reddit.com/r/deeplearning/comments/die18r/explaining_openais_robotic_hand_rubiks_cube_solver/,HenryAILabs,1571171856,"This video will explain some of the details behind the amazing research study from OpenAI using a Meta-Learning Automatic Domain Randomization algorithm to bridge the Sim2Real gap and solve a Rubik's Cube with a Robotic Hand!!

https://youtu.be/2AqGocPOOG4",2,1,False,self,,,,,
125,deeplearning,t5_2t5eh,2019-10-16,2019,10,16,5,died8k,self.deeplearning,How convert custom Keras code to Onnx?,https://www.reddit.com/r/deeplearning/comments/died8k/how_convert_custom_keras_code_to_onnx/,74throwaway,1571173184,"I'm using the code from https://github.com/qubvel/segmentation_models/blob/master/examples/multiclass%20segmentation%20(camvid).ipynb

I'm trying to use keras2onnx with it also. I tried

	import keras2onnx
	import onnxruntime

	# convert to onnx model
	onnx_model = keras2onnx.convert_keras(model, model.name)

	# runtime prediction
	content = onnx_model.SerializeToString()
	sess = onnxruntime.InferenceSession(content)
	x = x if isinstance(x, list) else [x]
	feed = dict([(input.name, x[n]) for n, input in enumerate(sess.get_inputs())])
	pred_onnx = sess.run(None, feed)

But I got the following error:

	InvalidArgumentError                      Traceback (most recent call last)
	~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)
	    426         results = c_api.TF_GraphImportGraphDefWithResults(
	--&gt; 427             graph._c_graph, serialized, options)  # pylint: disable=protected-access
	    428         results = c_api_util.ScopedTFImportGraphDefResults(results)

	InvalidArgumentError: Node 'block1b_drop/cond/mul/y': Unknown input node '^block1b_drop/cond/switch_t'

	During handling of the above exception, another exception occurred:

	ValueError                                Traceback (most recent call last)
	&lt;ipython-input-16-a5d3ba8b443c&gt; in &lt;module&gt;
	      3 
	      4 # convert to onnx model
	----&gt; 5 onnx_model = keras2onnx.convert_keras(model, model.name)
	      6 
	      7 # runtime prediction

	~/anaconda3/lib/python3.6/site-packages/keras2onnx/main.py in convert_keras(model, name, doc_string, target_opset, channel_first_inputs, debug_mode, custom_op_conversions)
	     98                         custom_op_dict=custom_op_conversions)
	     99     topology.debug_mode = debug_mode
	--&gt; 100     parse_graph(topology, sess.graph, target_opset, output_names)
	    101     topology.compile()
	    102 

	~/anaconda3/lib/python3.6/site-packages/keras2onnx/parser.py in parse_graph(topo, graph, target_opset, output_names)
	    647         topo.raw_model.add_input_name(str_value)
	    648 
	--&gt; 649     return _parse_graph_scope(graph, keras_layer_ts_map, topo, top_level, output_names)

	~/anaconda3/lib/python3.6/site-packages/keras2onnx/parser.py in _parse_graph_scope(graph, keras_node_dict, topology, top_scope, output_names)
	    597             _convert_keras_timedistributed(graph, nodes, layer_key_, model_, varset)
	    598         elif layer_key_ is None or get_converter(type(layer_key_)) is None:
	--&gt; 599             _convert_general_scope(nodes, varset)
	    600         else:
	    601             _convert_keras_scope(graph, nodes, layer_key_, model_, varset)

	~/anaconda3/lib/python3.6/site-packages/keras2onnx/parser.py in _convert_general_scope(node_list, varset)
	    299 
	    300     sess = keras.backend.get_session()
	--&gt; 301     subgraph, replacement = create_subgraph(sess.graph, node_list, sess, operator.full_name)
	    302     setattr(operator, 'subgraph', subgraph)
	    303     vars_, ts = _locate_inputs_by_node(node_list, varset)

	~/anaconda3/lib/python3.6/site-packages/keras2onnx/subgraph.py in create_subgraph(tf_graph, node_list, sess, dst_scope)
	    135     with tf.Graph().as_default() as sub_graph:
	    136         im_scope = """" if dst_scope is None else dst_scope
	--&gt; 137         tf.import_graph_def(output_graph_def, name=im_scope)
	    138         if im_scope:
	    139             replacement = {k_: im_scope + '/' + k_ for k_ in replacement}

	~/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
	    505                 'in a future version' if date is None else ('after %s' % date),
	    506                 instructions)
	--&gt; 507       return func(*args, **kwargs)
	    508 
	    509     doc = _add_deprecated_arg_notice_to_docstring(

	~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)
	    429       except errors.InvalidArgumentError as e:
	    430         # Convert to ValueError for backwards compatibility.
	--&gt; 431         raise ValueError(str(e))
	    432 
	    433     # Create _DefinedFunctions for any imported functions.

	ValueError: Node 'block1b_drop/cond/mul/y': Unknown input node '^block1b_drop/cond/switch_t'",0,0,False,self,,,,,
126,deeplearning,t5_2t5eh,2019-10-16,2019,10,16,9,dihht2,self.deeplearning,I want to train a neural network on my e-mails to generate text that sounds like me. how?,https://www.reddit.com/r/deeplearning/comments/dihht2/i_want_to_train_a_neural_network_on_my_emails_to/,britcruise,1571187029,are there cloud services which I can access to train a recurrent network on any text I want to provide? looking to just have some fun,9,6,False,self,,,,,
127,deeplearning,t5_2t5eh,2019-10-16,2019,10,16,11,dij2p2,self.deeplearning,image segmentation: why use u-net for tasks that would benefit from instance segmentation,https://www.reddit.com/r/deeplearning/comments/dij2p2/image_segmentation_why_use_unet_for_tasks_that/,molly12219,1571194755,"ive noticed a lot of people used u-net for tasks like nucleus segmentation and achieved great results.

doesn't u-net use semantic segmentation? im confused why you wouldn't use mask r-cnn for instance segmentation?

im clearly missing something. can someone explain?",3,14,False,self,,,,,
128,deeplearning,t5_2t5eh,2019-10-16,2019,10,16,17,dimhjh,self.deeplearning,"In autonomous driving, I want to use deep learning to judge whether the camera is occluded(clean, blur, blocked).",https://www.reddit.com/r/deeplearning/comments/dimhjh/in_autonomous_driving_i_want_to_use_deep_learning/,leozyc,1571215776,"I want to do research on autonomous driving cameras.

In autonomous driving, I want to use deep learning to detect various situations of partial visibility loss, for example, camera overlapping by obstacles, blur , blocked, etc.

What should I do?",2,1,False,self,,,,,
129,deeplearning,t5_2t5eh,2019-10-16,2019,10,16,18,dimunw,self.deeplearning,NLP - Classification problem,https://www.reddit.com/r/deeplearning/comments/dimunw/nlp_classification_problem/,MistaPrincu,1571218381,"Hi,

Im currently analyzing raw customer reviews .
I cleaned, audited and I used different models to represent textual data ( Word2Vec, Bert). 

I applied Kmeans clustering on this representation and manage to identify useless comment. The idea now is to find a way to analyze the remaining reviews, that are not superrr linearly separable.

I tried LDA, and other clustering algos such as GMM and sphericals but no concluding results.


Do you have any suggestions ? 


Thank you in advance  !",1,3,False,self,,,,,
130,deeplearning,t5_2t5eh,2019-10-16,2019,10,16,22,dipmu0,blog.floydhub.com,Introduction to Adversarial Machine Learning,https://www.reddit.com/r/deeplearning/comments/dipmu0/introduction_to_adversarial_machine_learning/,pirate7777777,1571234047,,0,24,False,default,,,,,
131,deeplearning,t5_2t5eh,2019-10-17,2019,10,17,1,dirn2b,self.deeplearning,Padding tensorflow dataset sequences to a maximum length,https://www.reddit.com/r/deeplearning/comments/dirn2b/padding_tensorflow_dataset_sequences_to_a_maximum/,shahriar49,1571242893,"I have a TFRecord format dataset spread over multiple files with each element in each file being a tuple of (data, label). Data itself is an nx12 array, for which n is variable from element to element. Therefore, I have added the number of rows in the features of each TFRecord entry and then parse it as below:

&amp;#x200B;

    
    featuresDict = {'data': tf.FixedLenFeature([], dtype=tf.string),
                    'rows': tf.FixedLenFeature([], dtype=tf.int64),
                    'label': tf.FixedLenFeature([], dtype=tf.int64)
                   }
        
    def parse_tfrecord(example):
        features = tf.parse_single_example(example, featuresDict)
        label = features['label']
        weight = classWeights[features['label']]
        rows = features['rows']
        data = tf.decode_raw(features['data'], tf.int64)
        data = tf.reshape(data, (rows,12))
        return data, label, weight
        
    def read_datasets(pattern, numFiles, numEpochs=None, batchSize=None):
        files = tf.data.Dataset.list_files(pattern)
    
        def _parse(x):
            x = tf.data.TFRecordDataset(x, compression_type='GZIP')
            return x
       
        dataset = files.interleave(_parse, cycle_length=numFiles, block_length=1).map(parse_tfrecord)
        dataset = dataset.prefetch(buffer_size=batchSize)
        dataset = dataset.repeat(numEpochs)
        return dataset
    

I have used padded\_batch (in read\_datasets function) to fix the input size per batch and it works fine. But I want to do a test with setting a global length for padding such as 100 (same for all batches). I tried with adding this line:

&amp;#x200B;

        data = tf.pad(data, tf.constant([[0,100-rows],[0,0]]))
    

to the parse\_tfrecord() function right before return clause, but I get below error:

&amp;#x200B;

          File ""C:/Users/sshahhey.ESFADMIN.000/.PyCharmCE2019.2/config/scratches/scratch.py"", line 423, in doWork
            test_data = read_datasets(files_test, numFiles, numEpochs=1, batchSize=batch_size)
          File ""C:/Users/sshahhey.ESFADMIN.000/.PyCharmCE2019.2/config/scratches/scratch.py"", line 84, in read_datasets
            dataset = files.interleave(_parse, cycle_length=numFiles, block_length=1, num_parallel_calls=min(numFiles,np)).map(parse_tfrecord, num_parallel_calls=np)
          File ""C:\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1040, in map
            return ParallelMapDataset(self, map_func, num_parallel_calls)
          File ""C:\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 2649, in __init__
            use_inter_op_parallelism)
          File ""C:\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 2611, in __init__
            map_func, ""Dataset.map()"", input_dataset)
          File ""C:\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1860, in __init__
            self._function.add_to_graph(ops.get_default_graph())
          File ""C:\Python36\lib\site-packages\tensorflow\python\framework\function.py"", line 479, in add_to_graph
            self._create_definition_if_needed()
          File ""C:\Python36\lib\site-packages\tensorflow\python\framework\function.py"", line 335, in _create_definition_if_needed
            self._create_definition_if_needed_impl()
          File ""C:\Python36\lib\site-packages\tensorflow\python\framework\function.py"", line 344, in _create_definition_if_needed_impl
            self._capture_by_value, self._caller_device)
          File ""C:\Python36\lib\site-packages\tensorflow\python\framework\function.py"", line 864, in func_graph_from_py_func
            outputs = func(*func_graph.inputs)
          File ""C:\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1794, in tf_data_structured_function_wrapper
            ret = func(*nested_args)
          File ""C:/Users/sshahhey.ESFADMIN.000/.PyCharmCE2019.2/config/scratches/scratch.py"", line 73, in parse_tfrecord
            data = tf.pad(data, tf.constant([[0, 100-rows],[0,0]]))
          File ""C:\Python36\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 208, in constant
            value, dtype=dtype, shape=shape, verify_shape=verify_shape))
          File ""C:\Python36\lib\site-packages\tensorflow\python\framework\tensor_util.py"", line 442, in make_tensor_proto
            _AssertCompatible(values, dtype)
          File ""C:\Python36\lib\site-packages\tensorflow\python\framework\tensor_util.py"", line 350, in _AssertCompatible
            raise TypeError(""List of Tensors when single Tensor expected"")
        TypeError: List of Tensors when single Tensor expected
    

What is the problem with it and how can I solve it?

&amp;#x200B;

P.S. The reason that I want to impose a global pad is that I get incompatible results when testing a trained model with different batch combinations (having the same model, the aggregation of test results from two separate datasets are not the same as if I interleave both datasets and apply them to the model). So I was suspicious that dynamic per-batch padding may cause this difference.",0,1,False,self,,,,,
132,deeplearning,t5_2t5eh,2019-10-17,2019,10,17,1,dis29u,youtube.com,The Pattern Machine (DL series by Art of the Problem),https://www.reddit.com/r/deeplearning/comments/dis29u/the_pattern_machine_dl_series_by_art_of_the/,britcruise,1571244622,,0,3,False,image,,,,,
133,deeplearning,t5_2t5eh,2019-10-17,2019,10,17,2,disqhg,mlfromscratch.com,"Optimizers in Neural Networks Explained: The Math Clearly Explained and Visualization for your Intuition - Adam, RMSprop, AdaGrad, Momentum and Stochastic Gradient Descent.",https://www.reddit.com/r/deeplearning/comments/disqhg/optimizers_in_neural_networks_explained_the_math/,permalip,1571247328,,0,1,False,default,,,,,
134,deeplearning,t5_2t5eh,2019-10-17,2019,10,17,2,dit0rv,self.deeplearning,Simple Latent/Feature Space Augmentation methods improve BERT's classification performance,https://www.reddit.com/r/deeplearning/comments/dit0rv/simple_latentfeature_space_augmentation_methods/,intvar,1571248498,"[https://arxiv.org/abs/1910.04176](https://arxiv.org/abs/1910.04176)  


This paper shows that simple methods like upsampling, random perturbation improves BERT classification performance. Moreover, these methods are almost as good as complex methods such as CVAE, Delta Encoder etc.",0,2,False,self,,,,,
135,deeplearning,t5_2t5eh,2019-10-17,2019,10,17,4,diukh3,self.deeplearning,[Natural Language Processing] Extracting attention weights of each token at each layer of transformer in python,https://www.reddit.com/r/deeplearning/comments/diukh3/natural_language_processing_extracting_attention/,h56cho,1571254918,"I am doing some NLP and I am interested in extracting attention weights of individual test token at each layer of transformer via Python.

Is coding up a Transformer (any transformers like Transformer-XL, OpenAL-GPT, GPT2 ,etc.) from the scratch the only way to get attention weights of individual test token at each transformer layer? Is there easier way to perform this task in Python?

Thank you,",2,1,False,self,,,,,
136,deeplearning,t5_2t5eh,2019-10-17,2019,10,17,4,diur96,youtu.be,Artificial intelligence will begin to add ads to films,https://www.reddit.com/r/deeplearning/comments/diur96/artificial_intelligence_will_begin_to_add_ads_to/,cmillionaire9,1571255684,,1,2,False,default,,,,,
137,deeplearning,t5_2t5eh,2019-10-17,2019,10,17,5,divkmr,self.deeplearning,Best Deep Learning Courses Updated 2019,https://www.reddit.com/r/deeplearning/comments/divkmr/best_deep_learning_courses_updated_2019/,data_datum,1571259030,[https://blog.floydhub.com/best-deep-learning-courses-updated-for-2019/](https://blog.floydhub.com/best-deep-learning-courses-updated-for-2019/),0,1,False,self,,,,,
138,deeplearning,t5_2t5eh,2019-10-17,2019,10,17,11,dj02ev,i.redd.it,PyTorch or die,https://www.reddit.com/r/deeplearning/comments/dj02ev/pytorch_or_die/,philippemnoel,1571279946,,23,139,False,image,,,,,
139,deeplearning,t5_2t5eh,2019-10-17,2019,10,17,16,dj2qew,self.deeplearning,computer vision eye tracking with pupils.,https://www.reddit.com/r/deeplearning/comments/dj2qew/computer_vision_eye_tracking_with_pupils/,tayyabikhlaq,1571296057," 

Hi everyone!

I need some help from you.I want to create an extension which use computer vision to detect where our eye pupil is exactly looking at exact pixel on the screen.

Also kindly answer these questions.

1)Which framework i should use for that purpose?

2)How to integrate it with web browser ?

3)Plz share your resources from wherever they are.

Thank you.",3,2,False,self,,,,,
140,deeplearning,t5_2t5eh,2019-10-17,2019,10,17,20,dj5b44,medium.com,Automating Business Operations with Artificial Intelligence,https://www.reddit.com/r/deeplearning/comments/dj5b44/automating_business_operations_with_artificial/,aakansha_chaudhary,1571313268,,0,1,False,default,,,,,
141,deeplearning,t5_2t5eh,2019-10-17,2019,10,17,22,dj6kc3,self.deeplearning,Julia vs Swift for Deep Learning,https://www.reddit.com/r/deeplearning/comments/dj6kc3/julia_vs_swift_for_deep_learning/,jyoti_6727,1571319694,"Hello, 

I've been using Python for all of my deep learning needs. I am looking to learn another language. Which one would  be better, julia or swift?

Thanks!",5,1,False,self,,,,,
142,deeplearning,t5_2t5eh,2019-10-17,2019,10,17,23,dj7eah,"[guide] reinforcement learning, part 1",https://medium.com/ai%C2%B3-theory-practice-business/reinforcement-learning-part-1-a-brief-introduction-a53a849771cf,https://www.reddit.com/r/deeplearning/comments/dj7eah/httpsmediumcomaic2b3theorypracticebusinessreinforc/,cdossman,1571323560,,0,1,False,default,,,,,
143,deeplearning,t5_2t5eh,2019-10-17,2019,10,17,23,dj7f9x,medium.com,"[Guide] Reinforcement Learning, Part 1: A Brief Introduction",https://www.reddit.com/r/deeplearning/comments/dj7f9x/guide_reinforcement_learning_part_1_a_brief/,cdossman,1571323681,,0,2,False,default,,,,,
144,deeplearning,t5_2t5eh,2019-10-18,2019,10,18,0,dj7m2u,self.deeplearning,Does Adversarial Example Robustness imply Traditional Perturbation Robustness ?,https://www.reddit.com/r/deeplearning/comments/dj7m2u/does_adversarial_example_robustness_imply/,alfredlaugros,1571324525,"An answer is provided in our paper accepted at ICCV 2019 RLQ Workshop :

[https://arxiv.org/abs/1909.02436](https://arxiv.org/abs/1909.02436)",0,9,False,self,,,,,
145,deeplearning,t5_2t5eh,2019-10-18,2019,10,18,0,dj7q7v,self.deeplearning,How can I study 10+ hours a day?,https://www.reddit.com/r/deeplearning/comments/dj7q7v/how_can_i_study_10_hours_a_day/,saradinto,1571325014," 

Ive done that for six years, every single day. Weekends, vacations. I wrote an entire book on the topic.

But first, let me tell what to do : *not* that.

Asking    this question, what you ask is really pointless. Your goal should be,    for instance, a certain grade, or being in the top ten percent of  your   class at university. That would be a wiser choice.

What    is the point of studying a lot and go straight to burnout (been  there,   done that), when you can actually be efficient, and work  towards   meaningful objectives, such as grades ?

If you *have* to study ten hours a day every day for more than a year, then something is wrong. *You*    are doing something wrong. For there are a few very hard programs   which  will push you to your limits purposefully, but they often dont   last  this long. Im speaking of the military for instance.

Even    if your grades matter as much as your life, if they can make the    difference between being happy for the rest of it or totally miserable,    as was my case, you dont need to do that.

I    was plagued with life-threatening psychological conditions. I wasnt    sleeping (worst thing for a student). Nightmares every night. I was    deeply depressed, on the border of psychiatric hospital for essentially    the whole [time](http://edumefree.com/welcome/CourseDetails/757/Time-Management-Skills).

The quality of my study was thus, understandably poor.

I *had*    to get efficient. I wasnt fully myself. Its as if I was drunk for    most of the time. Theoretical Physics when youre drunk is hard. Yet I    managed to be on the top of my class for five years, with the highest    grades, and get into Cambridge (in arguably one of the hardest  programs   in the world). And there, you were brutalized if you went  along what  you  were said. Pointless overly difficult exercises.  Basically a kind  of  military boot camp for the mind.

I    wrote an entire book on how you can study efficiently. Its aimed   first  at scientists, but 3/4 of it are applicable to any students.

[Here is](https://www.quora.com/How-do-top-students-manage-their-study-time/answer/Beno%C3%AEt-Seron-2) an example of my ideas.

If you like them, checkout my book ! Most of it is free, on my blog.",1,0,False,self,,,,,
146,deeplearning,t5_2t5eh,2019-10-18,2019,10,18,0,dj7z70,medium.com,Sotabench: Benchmarking Open Source Models Directly From GitHub,https://www.reddit.com/r/deeplearning/comments/dj7z70/sotabench_benchmarking_open_source_models/,Yuqing7,1571326107,,0,1,False,default,,,,,
147,deeplearning,t5_2t5eh,2019-10-18,2019,10,18,9,djf7io,self.deeplearning,Any pre-trained models geared towards top-view human detection?,https://www.reddit.com/r/deeplearning/comments/djf7io/any_pretrained_models_geared_towards_topview/,aweeeezy,1571357583,"I'm working on a project that involves detecting/tracking people using an overhead mounted camera (could go either way with regards to standard vs. fish eye lens). 

None of the models I've been benchmarking (MobileNet SSD, YOLOv3, Faster R-CNN, etc.) have had any success when running inference on [top-view footage like this](https://www.youtube.com/watch?v=ReKpjZDRjiE).

Non-deep-learning approaches to human detection from this perspective seem a little daunting at the present moment and I'm in a bit of a time crunch that is making me nervous when thinking about implementing some of the [research I've looked at](https://eprints.soton.ac.uk/363113/). However, as time goes on, it unfortunately seems that this may be the only path forward for me.

Again, the time crunch makes training models an unviable path -- I'm _really_ hoping to find some pre-trained human detection model which I can compile to a tflite model, test, and deploy in the next couple weeks.

Has anyone come across something like this?",18,9,False,self,,,,,
148,deeplearning,t5_2t5eh,2019-10-18,2019,10,18,17,djkd52,medium.com,Business Benefits of Using Python for AI Projects,https://www.reddit.com/r/deeplearning/comments/djkd52/business_benefits_of_using_python_for_ai_projects/,aaku-101,1571386161,,0,1,False,default,,,,,
149,deeplearning,t5_2t5eh,2019-10-18,2019,10,18,17,djkq4i,self.deeplearning,visualizing yolo v3,https://www.reddit.com/r/deeplearning/comments/djkq4i/visualizing_yolo_v3/,nanno3000,1571388825,"im looking for ways to visualize the detections that yolo v3 makes. I've found some libraries that do this for yolo v1 or v2, but they don't work on v3.

I want to see how the layer and filter activations look. Something like a detection heatmap ([like here](https://github.com/xueeinstein/darknet-vis)) would be awesome as well.

Does anyone know a repo that does this, or is there a (decently easy) way to do it from scratch?  


the mentioned github repos that dont work:

[github.com/schwittlick/ofxDarknet](https://github.com/schwittlick/ofxDarknet)

[https://github.com/xueeinstein/darknet-vis](https://github.com/xueeinstein/darknet-vis)",0,0,False,self,,,,,
150,deeplearning,t5_2t5eh,2019-10-18,2019,10,18,18,djl2g3,self.deeplearning,Python Tutorial for Computer Vision and Face Detection with OpenCV,https://www.reddit.com/r/deeplearning/comments/djl2g3/python_tutorial_for_computer_vision_and_face/,Rogers911z,1571391312, [https://www.youtube.com/watch?v=cmyXoFS3G00](https://www.youtube.com/watch?v=cmyXoFS3G00),0,1,False,self,,,,,
151,deeplearning,t5_2t5eh,2019-10-18,2019,10,18,19,djld2z,self.deeplearning,Research papers.,https://www.reddit.com/r/deeplearning/comments/djld2z/research_papers/,kuthubuddinshaik123,1571393333,"Where can I find them?  
Any particular blog or page?.
For ML &amp; AI.",13,6,False,self,,,,,
152,deeplearning,t5_2t5eh,2019-10-18,2019,10,18,20,djm1sm,onlineitguru.com,Data Science with Deep learning,https://www.reddit.com/r/deeplearning/comments/djm1sm/data_science_with_deep_learning/,tejasree15,1571397622,,0,1,False,default,,,,,
153,deeplearning,t5_2t5eh,2019-10-18,2019,10,18,20,djm4pv,analyticspath.com,"Get Registered with Deep Learning Training Free Interactive session on, 19th Oct, 10 AM, Hyderabad by Analytics Path",https://www.reddit.com/r/deeplearning/comments/djm4pv/get_registered_with_deep_learning_training_free/,Jony1223,1571398101,,0,1,False,default,,,,,
154,deeplearning,t5_2t5eh,2019-10-18,2019,10,18,20,djm86j,self.deeplearning,"When a plane crashes they look for black box for the reasons, but when a deep learning solution crashes they look for black box and then look at each other and look at black box and look at each other and look at...",https://www.reddit.com/r/deeplearning/comments/djm86j/when_a_plane_crashes_they_look_for_black_box_for/,ashutoshatpandey,1571398654,,0,0,False,self,,,,,
155,deeplearning,t5_2t5eh,2019-10-18,2019,10,18,21,djmuhh,self.deeplearning,Neural Network weights initialization,https://www.reddit.com/r/deeplearning/comments/djmuhh/neural_network_weights_initialization/,maciej386,1571401993,"Initialiation of weights is very important for a successful training.

However, all initializations I have seen so far are initializations of a particular layer of neural network in isolation in function of the number of incoming or outgoing neurons.

Is there a deep reason for that or it's just that noone has tried to look into it so far?",4,2,False,self,,,,,
156,deeplearning,t5_2t5eh,2019-10-18,2019,10,18,21,djn3ha,self.deeplearning,8 GPU Monster Build Thoughts for Deep Learning,https://www.reddit.com/r/deeplearning/comments/djn3ha/8_gpu_monster_build_thoughts_for_deep_learning/,eCurb247,1571403272,"I was thinking of building an 8 GPU build for expandability but struggle to find much on this or guides from anyone who's done it. 

Does anyone know of any useful resources towards it?
Is it much more hassle than a 4GPU build?",12,3,False,self,,,,,
157,deeplearning,t5_2t5eh,2019-10-18,2019,10,18,22,djn916,self.deeplearning,Padded_batch with pre-padding,https://www.reddit.com/r/deeplearning/comments/djn916/padded_batch_with_prepadding/,shahriar49,1571404023,"I have a dataset of variable-length sequences to feed an LSTM network in Tensorflow/Keras and I want to try and compare pre- and post-padding in the batches, but current padded\_batch function only pads at the sequences end. I know that  pad\_sequences function in keras do padding at either side, but I don't know how to use this function for padded\_batch. My code right now is like this, and I am reading multiple TFRecord files and interleave them to make my mixed dataset:

    featuresDict = {'data': tf.FixedLenFeature([], dtype=tf.string),
                    'rows': tf.FixedLenFeature([], dtype=tf.int64),
                    'label': tf.FixedLenFeature([], dtype=tf.int64)
                   }
    
    def parse_tfrecord(example):
        features = tf.parse_single_example(example, featuresDict)
        label = tf.one_hot(features['label'],N)
        rows = features['rows']
        data = tf.decode_raw(features['data'], tf.int64)
        data = tf.reshape(data, (rows,num_features)
        return data, label
    
    def read_datasets(pattern, numFiles, numEpochs=None, batchSize=None):
        files = tf.data.Dataset.list_files(pattern)
    
        def _parse(x):
            x = tf.data.TFRecordDataset(x, compression_type='GZIP')
            return x
    
        dataset = files.interleave(_parse, cycle_length=numFiles, block_length=1).map(parse_tfrecord)
        padded_shapes = (tf.TensorShape([None, num_features]), tf.TensorShape([N,])))
        dataset = dataset.padded_batch(batchSize, padded_shapes)
        dataset = dataset.prefetch(buffer_size=batchSize)
        dataset = dataset.repeat(numEpochs)
        return dataset",0,5,False,self,,,,,
158,deeplearning,t5_2t5eh,2019-10-18,2019,10,18,22,djnade,docs.google.com,Would love you to find mistakes/problems with the outline for my next video on Neural Network history (for my channel art of the problem),https://www.reddit.com/r/deeplearning/comments/djnade/would_love_you_to_find_mistakesproblems_with_the/,britcruise,1571404205,,0,4,False,default,,,,,
159,deeplearning,t5_2t5eh,2019-10-18,2019,10,18,22,djnn01,youtube.com,Energy-based Approaches to Representation Learning - Yann LeCun,https://www.reddit.com/r/deeplearning/comments/djnn01/energybased_approaches_to_representation_learning/,DrJohanson,1571405907,,1,31,False,default,,,,,
160,deeplearning,t5_2t5eh,2019-10-19,2019,10,19,0,djp5dv,self.deeplearning,Reinforcement learning: Number of epochs not matching calculation in paper.,https://www.reddit.com/r/deeplearning/comments/djp5dv/reinforcement_learning_number_of_epochs_not/,rrz0,1571412717,"I am trying to reproduce results presented in this \[paper\]\[1\]. On page 4, the authors state:

&amp;#x200B;

\&gt; ... we train for 50 epochs (one epoch consists of 19\*2\*50 = 1900 full

\&gt; episodes), which amounts to a total of 4.75\*10\^6 timesteps.

&amp;#x200B;

The 1900 episodes are broken down into Rollouts per MPI worker (2) \* Number of MPI Workers (19) \* Cycles per epoch (50), as shown in the hyper parameters section on page 10.

&amp;#x200B;

When testing on my local machine, using the GitHub \[Baselines repo\]\[2\], I am using 1 MPI worker and the following hyperparams:

&amp;#x200B;

'n\_cycles': 50,  # per epoch

'rollout\_batch\_size': 2,  # per mpi thread

&amp;#x200B;

By the same calculation, this means that I should have 1\*50\*2 = 100 episodes per epoch.

&amp;#x200B;

However when I run \`her\` on \`FetchReach-v1\` turns out I only have 10 episodes per epoch. Her is a log sample:

&amp;#x200B;

&amp;#x200B;

Training...

\---------------------------------

| epoch              | 0        |

| stats\_g/mean       | 0.893    |

| stats\_g/std        | 0.122    |

| stats\_o/mean       | 0.269    |

| stats\_o/std        | 0.0392   |

| test/episode       | 10       |

| test/mean\_Q        | -0.602   |

| test/success\_rate  | 0.5      |

| train/episode      | 10       |  &lt;-- 10 episodes/epoch

| train/success\_rate | 0        |

\---------------------------------

&amp;#x200B;

Why is there this discrepancy? Any suggestions would be appreciated.

&amp;#x200B;

&amp;#x200B;

  \[1\]: [https://arxiv.org/pdf/1802.09464.pdf](https://arxiv.org/pdf/1802.09464.pdf)

  \[2\]: [https://github.com/openai/baselines](https://github.com/openai/baselines)",0,2,False,self,,,,,
161,deeplearning,t5_2t5eh,2019-10-19,2019,10,19,4,djs7bf,github.com,Semi-automatic image annoation tool for computer vision tasks. Opensource on github.,https://www.reddit.com/r/deeplearning/comments/djs7bf/semiautomatic_image_annoation_tool_for_computer/,gitarre94,1571425648,,0,3,False,default,,,,,
162,deeplearning,t5_2t5eh,2019-10-19,2019,10,19,5,djtk63,self.deeplearning,Why is random normal weight initialisation preferred over random uniform weight initialisation in neural networks?,https://www.reddit.com/r/deeplearning/comments/djtk63/why_is_random_normal_weight_initialisation/,pranav2109,1571431432,,5,5,False,self,,,,,
163,deeplearning,t5_2t5eh,2019-10-19,2019,10,19,7,djuxwf,youtu.be,Faces of DeepFake,https://www.reddit.com/r/deeplearning/comments/djuxwf/faces_of_deepfake/,cmillionaire9,1571437756,,3,48,False,image,,,,,
164,deeplearning,t5_2t5eh,2019-10-19,2019,10,19,21,dk37q5,self.deeplearning,pytorch 1.3 namedtensor,https://www.reddit.com/r/deeplearning/comments/dk37q5/pytorch_13_namedtensor/,stevethesteve2,1571488679,"How helpful is the new API?

I dont see how I can expressively write down

mytensor2 = mytensor1\[:,:-1\]

using the new API",1,5,False,self,,,,,
165,deeplearning,t5_2t5eh,2019-10-19,2019,10,19,22,dk3h7b,medium.com,[Article] The Difference Between AI and Machine Learning,https://www.reddit.com/r/deeplearning/comments/dk3h7b/article_the_difference_between_ai_and_machine/,cdossman,1571490142,,0,0,False,default,,,,,
166,deeplearning,t5_2t5eh,2019-10-19,2019,10,19,23,dk4g7d,self.deeplearning,Benchmarking /Transformers on both PyTorch and TensorFlow,https://www.reddit.com/r/deeplearning/comments/dk4g7d/benchmarking_transformers_on_both_pytorch_and/,jikkii,1571495210,"Since our recent release of [Transformers](https://github.com/huggingface/transformers) (previously known as pytorch-pretrained-BERT and pytorch-transformers), we've been working on a comparison between the implementation of our models in PyTorch and in TensorFlow.

We've released a [detailed report](https://medium.com/huggingface/benchmarking-transformers-pytorch-and-tensorflow-e2917fb891c2) where we benchmark each of the architectures hosted on our repository (BERT, GPT-2, DistilBERT, ...) in PyTorch with and without TorchScript, and in TensorFlow with and without XLA. We benchmark them for inference and the results are visible in the [following spreadsheet](https://docs.google.com/spreadsheets/d/1sryqufw2D0XlUH4sq3e9Wnxu5EAQkaohzrJbd5HdQ_w/edit#gid=0).

We would love to hear your thoughts on the process.",1,26,False,self,,,,,
167,deeplearning,t5_2t5eh,2019-10-20,2019,10,20,2,dk6i45,self.deeplearning,Regression on very small images,https://www.reddit.com/r/deeplearning/comments/dk6i45/regression_on_very_small_images/,gulzainali,1571504634,"I am working on a problem where i am trying to build a model to do regression on very small images. These images are simple filled colors of size 20x20 pixels. These are extracted patches.from larger images and due to difference in light i can't simply fetch the color and output a value. Can anyone suggest a good model for this problem? given a simple image filled with color of very small size, i need to predict a certain value Y.",2,1,False,self,,,,,
168,deeplearning,t5_2t5eh,2019-10-20,2019,10,20,2,dk6jlf,pgaleone.eu,"I published a book about neural networks, deep learning and TensorFlow 2.0",https://www.reddit.com/r/deeplearning/comments/dk6jlf/i_published_a_book_about_neural_networks_deep/,pgaleone,1571504817,,2,4,False,default,,,,,
169,deeplearning,t5_2t5eh,2019-10-20,2019,10,20,2,dk6wyb,youtu.be,"Neural networks taught to ""read minds"" in real time",https://www.reddit.com/r/deeplearning/comments/dk6wyb/neural_networks_taught_to_read_minds_in_real_time/,cmillionaire9,1571506440,,12,59,False,default,,,,,
170,deeplearning,t5_2t5eh,2019-10-20,2019,10,20,6,dk9upg,self.deeplearning,Auto ML dataset subsets: How to design a subset to for hyperparameter testing,https://www.reddit.com/r/deeplearning/comments/dk9upg/auto_ml_dataset_subsets_how_to_design_a_subset_to/,ensaladamental,1571519870,"Im reading about AutoML. There's a lot of vibrant work on the topic. Most interesting nets require however massive resources.   
For argument's sake, say, if you need to optimize the hyperparameters of Imagenet 1000, on a TitanRTX with Resnet18 a single run of 90 epochs is about 30 hours. Resnet50 is 110 hours ( these are my tests on pytorch). Deeper resnets achieve significant improvement on top1 accuracy.  
The resnet paper settles on learning updates every 30 epochs, but the difference in Top1 accuracy between 15 and 30 epochs is minimal. The space of optimizers is large, would second order methods work better? But even within SGD, could we be losing information  on the backpropagation because we are using FP32, or could we be wasting resources by not doing FP16. Could we be losing information because some salient gradient on a latent layer is getting clobbered by the simple weighted sum of the residuals?   
An biological analogy, when signal travels through the dendrite trees it decays with distance and increases nonlinearly with proximity of other dedndrites carrying the same information. Maybe you can say that this is modifying the architecture, but all I'm thinking of is a  variation on adaptive SGDs.   


Question. Is there hope for a principled reduction on a dataset where output dimensions, number of samples, size of data and depth of an architecture can give us clues of what to do with hyperparameters?   
A physical analogy of this could be minaturized testing on fluids; e.g. you want to test flows on a ship you cant just miniaturize the ship, you also have to change to fluid to account for how forces change with scale.   


I found some work on this from 2013, Kwesky et al [Multi-Task Bayesian Optimization,](https://papers.nips.cc/paper/5086-multi-task-bayesian-optimization.pdf) To anyone's knowledge, Is there further work on this topic? This isnt new and standard architectures we are working on today are huge compared to 2013.    
There is an interesting work that I need to understand and implement, Feurer et al 2015 [Efficient and Robust Automated Machine Learning](https://ml.informatik.uni-freiburg.de/papers/15-NIPS-auto-sklearn-preprint.pdf), but it uses dataset similarity.

C Wong et al 2018 [Transfer Learning with Neural AutoML](https://papers.nips.cc/paper/8056-transfer-learning-with-neural-automl.pdf), sidesteps the question - and perhaps this is the way to go, using transfer learning. While I think this approach is very promising as, I am specifically looking at training completely new deep nets, so I have to put it on hold.

Now, I mentioned ImageNet1000 being too large, for a deep search. But while it contains lots of data, it is really tiny compared to the full image net with \~ &gt; 21000 categories and 14M + images; which in turn uses only 1/4 of the nouns in wordnet, which in turn is a barebones (and imbalanced) categorization of our use of nouns. 

Is there an information theory approach to answering this question?",1,3,False,self,,,,,
171,deeplearning,t5_2t5eh,2019-10-20,2019,10,20,6,dk9wh2,self.deeplearning,Turning a pdf question/answer file into flash cards,https://www.reddit.com/r/deeplearning/comments/dk9wh2/turning_a_pdf_questionanswer_file_into_flash_cards/,backtoreality0101,1571520090,Hey I was wondering if there were any pre trained models that could let me feed a question/answer pdf and the output would be to make flash cards. I have a text book of thousands of questions followed by answers that I could make on my own into flash cards but would take awhile and was looking for a way that an algorithm could look at the pdf and turn that into flash cards. Any suggestions?,0,0,False,self,,,,,
172,deeplearning,t5_2t5eh,2019-10-20,2019,10,20,7,dkam9f,self.learnmachinelearning,ML OS Discussion,https://www.reddit.com/r/deeplearning/comments/dkam9f/ml_os_discussion/,ZeroMaxinumXZ,1571523489,,0,0,False,default,,,,,
173,deeplearning,t5_2t5eh,2019-10-20,2019,10,20,20,dkijk7,self.deeplearning,When will CVPR 2019 proceedings appear in IEEE Xplore?,https://www.reddit.com/r/deeplearning/comments/dkijk7/when_will_cvpr_2019_proceedings_appear_in_ieee/,youkaichao,1571571637,It has been quite a long time since the conference took place.,2,7,False,self,,,,,
174,deeplearning,t5_2t5eh,2019-10-21,2019,10,21,3,dknads,self.deeplearning,"I need help, please",https://www.reddit.com/r/deeplearning/comments/dknads/i_need_help_please/,loveapi,1571594972,"Hi Reddit!
I am new to learning neural networks. Once I found an article about Norman AI. (https://www.bbc.com/news/technology-44040008)
As I understand is it an AI that is trained to perform image captioning, right? This really interested me and I wanted to create something like that. If someone can tell me exactly what to do I will be very grateful. Sorry for my bad english I use google translate",5,0,False,self,,,,,
175,deeplearning,t5_2t5eh,2019-10-21,2019,10,21,6,dkq5m7,self.deeplearning,"AI Weekly Update #9 - (October 20th, 2019)",https://www.reddit.com/r/deeplearning/comments/dkq5m7/ai_weekly_update_9_october_20th_2019/,HenryAILabs,1571606813,"https://www.youtube.com/watch?v=03kCD18H5nQ

This week's update covers OpenAI's amazing robotic hand and exciting updates such as Facebook's Semi-Weakly Supervised learning framework, Google's MASSIVE (50BN params) Multilingual NMT models, and many more!",3,8,False,self,,,,,
176,deeplearning,t5_2t5eh,2019-10-21,2019,10,21,23,dl1ell,rubikscode.net,5 Awesome New Features  Python 3.8,https://www.reddit.com/r/deeplearning/comments/dl1ell/5_awesome_new_features_python_38/,RubiksCodeNMZ,1571668390,,4,38,False,default,,,,,
177,deeplearning,t5_2t5eh,2019-10-21,2019,10,21,23,dl1hqe,self.deeplearning,In case of LSTM implementation with tensorflow is it advisable to reset the initial state to zero for a new batch of data?,https://www.reddit.com/r/deeplearning/comments/dl1hqe/in_case_of_lstm_implementation_with_tensorflow_is/,pranav2109,1571668781,,4,5,False,self,,,,,
178,deeplearning,t5_2t5eh,2019-10-22,2019,10,22,2,dl3k29,self.deeplearning,My first GAN,https://www.reddit.com/r/deeplearning/comments/dl3k29/my_first_gan/,MartyMcfagg,1571677606,"[https://github.com/ale100584/GAN](https://github.com/ale100584/GAN)

Hello, this is my first attempt with GANs and I wanted to share it with you guys. Please let me know what you think! Thanks",0,1,False,self,,,,,
179,deeplearning,t5_2t5eh,2019-10-22,2019,10,22,3,dl4u1o,self.deeplearning,Facebook's Semi-Weak Supervised Learning framework explained!,https://www.reddit.com/r/deeplearning/comments/dl4u1o/facebooks_semiweak_supervised_learning_framework/,HenryAILabs,1571683105,"Video Explanation: [https://youtu.be/5cySIwg49RI](https://youtu.be/5cySIwg49RI)

Blog Post: [https://medium.com/@connorshorten300/unlabelled-datas-stock-is-rising-71ed1cf909b7](https://medium.com/@connorshorten300/unlabelled-datas-stock-is-rising-71ed1cf909b7)",0,2,False,self,,,,,
180,deeplearning,t5_2t5eh,2019-10-22,2019,10,22,5,dl6ekv,self.learnmachinelearning,Self-play by next-state imagination from current action,https://www.reddit.com/r/deeplearning/comments/dl6ekv/selfplay_by_nextstate_imagination_from_current/,ZeroMaxinumXZ,1571689698,,0,1,False,default,,,,,
181,deeplearning,t5_2t5eh,2019-10-22,2019,10,22,7,dl86yq,self.deeplearning,How/where to learn ML devops? Ie how do I get up and running in GCP or AWS or locally?,https://www.reddit.com/r/deeplearning/comments/dl86yq/howwhere_to_learn_ml_devops_ie_how_do_i_get_up/,zalamandagora,1571697315,"I'm learning ML/DL on-line and am participating in a few Kaggle competitions to drive learning. Unfortunately, the biggest challenge for the last several weeks has been to get a good platform going. Neither Kaggle nor Colab work really well, and trying to run on my GPU-equipped Windows machine was hell as well. 

Looking at Docker and GCP, I just don't understand the instructions for provisioning and installing everything I need to get to a functioning ML environment connected to sufficient high-performance storage. 

I would really appreciate suggestions for how to learn this ""ML devops"" stuff.",2,1,False,self,,,,,
182,deeplearning,t5_2t5eh,2019-10-22,2019,10,22,9,dl9oew,zeldatech.com,Deep Learning for Computer Vision with Python By Adrian Rosebrock PDF,https://www.reddit.com/r/deeplearning/comments/dl9oew/deep_learning_for_computer_vision_with_python_by/,oussamaouti,1571703880,,2,0,False,default,,,,,
183,deeplearning,t5_2t5eh,2019-10-22,2019,10,22,15,dldnlp,self.deeplearning,ImageNet State-of-the-art,https://www.reddit.com/r/deeplearning/comments/dldnlp/imagenet_stateoftheart/,arjundupa,1571724340,"Is code for the current state-of-the-art (or one of the best performing models) on ImageNet publicly available?

How much compute does training a state-of-the-art ImageNet model require? Would one NVIDIA V100 suffice?",2,3,False,self,,,,,
184,deeplearning,t5_2t5eh,2019-10-22,2019,10,22,15,dldroc,github.com,GitHub - sommerschield/ancient-text-restoration: Restoring ancient text using deep learning: a case study on Greek epigraphy.,https://www.reddit.com/r/deeplearning/comments/dldroc/github_sommerschieldancienttextrestoration/,bil-sabab,1571725070,,1,28,False,default,,,,,
185,deeplearning,t5_2t5eh,2019-10-22,2019,10,22,16,dle9le,hanxiao.github.io,[P] GNES Flow: a Pythonic Way to Build Cloud-Native Neural Search Pipelines,https://www.reddit.com/r/deeplearning/comments/dle9le/p_gnes_flow_a_pythonic_way_to_build_cloudnative/,h_xiao,1571728134,,0,3,False,default,,,,,
186,deeplearning,t5_2t5eh,2019-10-22,2019,10,22,17,dlf0r5,self.deeplearning,AnnoMachine: an application that provides a user-friendly way to visualize &amp; interactively update object detection results,https://www.reddit.com/r/deeplearning/comments/dlf0r5/annomachine_an_application_that_provides_a/,machine_talk,1571733486,"Hi everyone, I'm Trung Tran. I occasionally write on [https://machinetalk.org](https://machinetalk.org).

Recently I have been working on an app called AnnoMachine, an application that aims to provide a user-friendly way to visualize &amp; interactively update object detection results. So what does that even mean?

Here is the introduction video illustrating the idea above:

[https://www.youtube.com/watch?v=NBO1acBqBlo](https://www.youtube.com/watch?v=NBO1acBqBlo)

# Why AnnoMachine?

Most deep learning engineers/practitioners (including myself) spend most of our time on creating deep learning models. So AnnoMachine is an effort of mine to make use of a deep learning model to create an app that can actually solve some kind of problem.

#  How did I create AnnoMachine?

The heart of AnnoMachine is, of course, an object detection model using deep learning. I chose SSD and implemented it on Tensorflow 2.0. Then, the model needs a way to serve the results to the world, right? I use Flask to create a RESTful API. As for the frontend, I used ReactJS to create the UI. I also used nginx for a simple web server.

The project uses the microservice architecture and docker to deploy locally or to AWS EC2. The project can run flawlessly on a single t2.micro instance.

# Source code

The project is open-sourced at [https://github.com/ChunML/AnnoMachine](https://github.com/ChunML/AnnoMachine).

For folks who want to train their own dataset, please use the source code from this repo: [https://github.com/ChunML/ssd-tf2](https://github.com/ChunML/ssd-tf2), a Tensorflow 2.0 implementation of SSD.

Please let me know f you have any questions or feedbacks :D Cheers.",4,2,False,self,,,,,
187,deeplearning,t5_2t5eh,2019-10-22,2019,10,22,21,dlh08f,technologyreview.com,Military artificial intelligence can be easily and dangerously fooled,https://www.reddit.com/r/deeplearning/comments/dlh08f/military_artificial_intelligence_can_be_easily/,atomlib_com,1571746242,,1,6,False,default,,,,,
188,deeplearning,t5_2t5eh,2019-10-22,2019,10,22,22,dli2hp,towardsdatascience.com,Exploring Data Augmentation with Keras and TensorFlow,https://www.reddit.com/r/deeplearning/comments/dli2hp/exploring_data_augmentation_with_keras_and/,thisissumit,1571751523,,0,4,False,default,,,,,
189,deeplearning,t5_2t5eh,2019-10-22,2019,10,22,22,dlib1f,lionbridge.ai,Over 300 Open Datasets for Machine Learning,https://www.reddit.com/r/deeplearning/comments/dlib1f/over_300_open_datasets_for_machine_learning/,LimarcAmbalina,1571752674,,0,2,False,default,,,,,
190,deeplearning,t5_2t5eh,2019-10-23,2019,10,23,2,dll8g8,medium.com,Harvard &amp; Google Seismic Paper Hit With Rebuttals: Is Deep Learning Suited to Aftershock Prediction?,https://www.reddit.com/r/deeplearning/comments/dll8g8/harvard_google_seismic_paper_hit_with_rebuttals/,Yuqing7,1571765368,,1,23,False,default,,,,,
191,deeplearning,t5_2t5eh,2019-10-23,2019,10,23,4,dln4f5,self.deeplearning,Fastest Data Structure for Very Large Directed Graph,https://www.reddit.com/r/deeplearning/comments/dln4f5/fastest_data_structure_for_very_large_directed/,raijinraijuu,1571773001,I am trying to store Wikipedia pages as a directed graph and I managed to create a SQL database of the wiki pages. I was curious if you know any data management technology more suited for this task. I posted here because I couldn't find any clear options.,0,1,False,self,,,,,
192,deeplearning,t5_2t5eh,2019-10-23,2019,10,23,7,dlpmb5,zeldatech.com,Deep Learning for Computer Vision with Python By Adrian Rosebrock PDF,https://www.reddit.com/r/deeplearning/comments/dlpmb5/deep_learning_for_computer_vision_with_python_by/,oussamaouti,1571783035,,0,0,False,default,,,,,
193,deeplearning,t5_2t5eh,2019-10-23,2019,10,23,15,dlv6hj,self.deeplearning,What is so fascinating about artificial intelligence?,https://www.reddit.com/r/deeplearning/comments/dlv6hj/what_is_so_fascinating_about_artificial/,SunilAhujaa,1571811750,Artificial intelligence courses in India are facilitating a lot of youngsters to delve deep into the subject and we can expect a rise in AI innovation from India in the near future.,2,0,False,self,,,,,
194,deeplearning,t5_2t5eh,2019-10-23,2019,10,23,17,dlwbp0,self.deeplearning,Powerful Deep learning build,https://www.reddit.com/r/deeplearning/comments/dlwbp0/powerful_deep_learning_build/,prateekish,1571819820,"I have been meaning to assemble a power-packed heavy-duty deep learning machine for a home set-up in India. This should be for long term usage keeping in mind most use-cases \[kaggle, competitive research, a startup idea, etc\]

While I have been through most famous and informative blogs(eg. tim dettmers), I still have a few wrinkles to iron before I jump the gun.

\- 1 rtx titan vs 2 rtx 2080 ti \[solely for AI tasks\] or any other gpu suggestion for that matter. I could even go for 2 rtx titans but i need to be sure they are worth it by offering more than say 4 RTX 2080 TIs . Its a long term investment and i wouldnt want to feel high and dry down the timeline

\- for large networks such as BERT, would the connected GPUs offer linearly added advantages (4 x 11 GB = 44 GB, for tensor cores, etc,) ?

If there are posts that address my queries (and more on the similar build lines) please be kind to point me to those.

Thanks in advance",13,0,False,self,,,,,
195,deeplearning,t5_2t5eh,2019-10-23,2019,10,23,20,dlxp30,elink.io,Accelerating Sales Growth with Artificial Intelligence,https://www.reddit.com/r/deeplearning/comments/dlxp30/accelerating_sales_growth_with_artificial/,akira_chaudhary,1571828901,,2,0,False,default,,,,,
196,deeplearning,t5_2t5eh,2019-10-23,2019,10,23,21,dlyjjk,self.deeplearning,I'm looking for success/failure stories applying unsupervised document embedding techniques,https://www.reddit.com/r/deeplearning/comments/dlyjjk/im_looking_for_successfailure_stories_applying/,shaypal5,1571833509,"Hey everyone! :)

As the title says, I am looking for both success stories and disappointing failures of applications of **modern** unsupervised document embedding techniques on actual problems (as opposed to academic benchmarks, toy datasets, academic evaluation tasks, etc.). The main focus is naturally on industry uses for business/product problems, but I would also love to hear about cases from government bodies, non-profits, use in research (with empirical measurement and where document embedding is one of the tools, not the subject of research) and any other ""real life"" use. I would love to hear about your experience, but connecting me to people you know or even hinting me towards companies or projects you know used these techniques (or tried to) would also be of tremendous help.

What's in it for you? Well, I'm preparing a talk for [the data science track of the CodeteCON #KRK5 conference](https://codetecon.pl/en/#program) based on my [literature review-y blog post on document embedding techniques](https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d), and while I feel I have a pretty good overview of the academic papers, benchmarks and SOTA status up until the most recent stuff to come out in the field at this point in time, I can't say the same for uses in the industry; I have a partial view from my experience in one ongoing project to actually use this, and experience shared by some of my data scientist friends (all in Israel, naturally) - most of it, so far, by the way, is that averaging (good) word embeddings is a very tough ""baseline"" to beat.

This is why I thought reaching out to get a better sense of things in the industry world-wide, and enriching my talk with the status of actual successes and industry applications will give people attending my talk more value, and will serve my attempt to make my talk a status report on the topic.

And (coming back to WIIFM) naturally (I think), I intend to share any (share-able) knowledge I accumulate not only in my talk, but also by adding a section dedicated to it to [the aforementioned blog post](https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d), and maybe even by writing an extended post around it (if enough interesting trends and issues come up). So, hopefully, if you are (like me) interested in this, we might also end up getting, together, a nice overview of where the industry stands at the moment.

What **modern** techniques (so no variants of bag-of-words or topic modeling techniques) am I talking about? These are the ones that I know of (I'd love to hear about others!):

* n-gram embeddings
* Averaging word embeddings (including all variants, e.g. SIF)
* Sent2Vec
* Paragraph vectors (doc2vec)
* Doc2VecC
* Skip-thought vectors
* FastSent
* Quick-thought vectors
* Word Movers Embedding (WME)
* Sentence-BERT (SBERT)
* GPT/GPT2 (can also be supervised)
* Universal Sentence Encoder (can also be supervised)
* GenSen (can also be supervised)

Thank you and cheers,  
Shay :)",5,19,False,self,,,,,
197,deeplearning,t5_2t5eh,2019-10-23,2019,10,23,21,dlyoh9,self.deeplearning,Deep Learning for automating KYC verification and aml compliances,https://www.reddit.com/r/deeplearning/comments/dlyoh9/deep_learning_for_automating_kyc_verification_and/,manneshiva,1571834191,"Deep Learning in KYC and aml compliances

[https://nanonets.com/blog/kyc-automation-using-deep-learning/](https://nanonets.com/blog/kyc-automation-using-deep-learning/?utm_source=reddit&amp;utm_medium=social&amp;utm_campaign=kyca&amp;utm_content=dl)",1,0,False,self,,,,,
198,deeplearning,t5_2t5eh,2019-10-23,2019,10,23,22,dlyzzs,self.deeplearning,"[Research] Researchers Propose MIMO-Speech, a New Neural Sequence-to-Sequence Architecture",https://www.reddit.com/r/deeplearning/comments/dlyzzs/research_researchers_propose_mimospeech_a_new/,cdossman,1571835789,"  [https://medium.com/ai%C2%B3-theory-practice-business/researchers-propose-mimo-speech-a-new-neural-sequence-to-sequence-architecture-19b1e791a53b](https://medium.com/ai%C2%B3-theory-practice-business/researchers-propose-mimo-speech-a-new-neural-sequence-to-sequence-architecture-19b1e791a53b) 

Recently, the end-to-end approach has proven its efficacy in monaural multi-speaker speech recognition. However, high word error rates (WERs) still prevent these systems from being used in practical applications. On the other hand, the spatial information in multi-channel signals has proven helpful in far-field speech recognition tasks. In this work, we propose a novel neural sequence-to-sequence (seq2seq) architecture, MIMO-Speech, which extends the original seq2seq to deal with multi-channel input and multi-channel output so that it can fully model multi-channel multi-speaker speech separation and recognition. MIMO-Speech is a fully neural end-to-end framework, which is optimized only via an ASR criterion. It is comprised of: 1) a monaural masking network, 2) a multi-source neural beamformer, and 3) a multi-output speech recognition model. With this processing, the input overlapped speech is directly mapped to text sequences. We further adopted a curriculum learning strategy, making the best use of the training set to improve the performance. The experiments on the spatialized wsj1-2mix corpus show that our model can achieve more than 60% WER reduction compared to the single-channel system with high quality enhanced signals (SI-SDR = 23.1 dB) obtained by the above separation function.",0,10,False,self,,,,,
199,deeplearning,t5_2t5eh,2019-10-23,2019,10,23,22,dlzjzh,self.deeplearning,Best tools to know to be marketable?,https://www.reddit.com/r/deeplearning/comments/dlzjzh/best_tools_to_know_to_be_marketable/,nicetryho,1571838430,[removed],0,1,False,self,,,,,
200,deeplearning,t5_2t5eh,2019-10-23,2019,10,23,22,dlznnd,self.deeplearning,"Please Mods, Make This Sub Karma-Locked",https://www.reddit.com/r/deeplearning/comments/dlznnd/please_mods_make_this_sub_karmalocked/,Atralb,1571838904,"This sub is completely bloated by swarms of posts of random offers, or shitty courses, or low-if-not-null quality medium articles, or even links to github repos. I swear this represents more 40% of the activity on this sub right now.

Someone has to do something. Could you at least set a soft threshold like 200 karma ? And minimum accoimt age ?

Cause I'm telling you this sub is decaying and slowly dying and I find it very sad for our community.

I honestly always go to others subs now if I'm looking for relevant information on the discipline.",0,1,False,self,,,,,
201,deeplearning,t5_2t5eh,2019-10-23,2019,10,23,23,dm0biy,self.deeplearning,Padded_batch with pre- or post-padding option,https://www.reddit.com/r/deeplearning/comments/dm0biy/padded_batch_with_pre_or_postpadding_option/,shahriar49,1571841918,"I have a dataset of variable-length sequences (a tensorflow TFRecord dataset) to feed an LSTM network and I want to try and compare pre- and post-padding in the batches, but current padded\_batch function only pads at the sequences end. I know that we have tf.keras.preprocessing.sequence.pad\_sequences function in API but I don't know how to apply this function to dataset batch processor. The padded\_batch function in tensorflow does both padding and batching, and it will find the required paddding size per batch dynamically. How can I implement this myself? My code right now is like this, and I am reading multiple TFRecord files and interleave them to make my mixed dataset:

    featuresDict = {'data': tf.FixedLenFeature([], dtype=tf.string),
                    'rows': tf.FixedLenFeature([], dtype=tf.int64),
                    'label': tf.FixedLenFeature([], dtype=tf.int64)
                   }
    
    def parse_tfrecord(example):
        features = tf.parse_single_example(example, featuresDict)
        label = tf.one_hot(features['label'],N)
        rows = features['rows']
        data = tf.decode_raw(features['data'], tf.int64)
        data = tf.reshape(data, (rows,num_features)
        return data, label
    
    def read_datasets(pattern, numFiles, numEpochs=None, batchSize=None):
        files = tf.data.Dataset.list_files(pattern)
    
        def _parse(x):
            x = tf.data.TFRecordDataset(x, compression_type='GZIP')
            return x
    
        dataset = files.interleave(_parse, cycle_length=numFiles, block_length=1).map(parse_tfrecord)
        padded_shapes = (tf.TensorShape([None, num_features]), tf.TensorShape([N,])))
        dataset = dataset.padded_batch(batchSize, padded_shapes)
        dataset = dataset.prefetch(buffer_size=batchSize)
        dataset = dataset.repeat(numEpochs)
        return dataset",0,2,False,self,,,,,
202,deeplearning,t5_2t5eh,2019-10-23,2019,10,23,23,dm0g5p,self.deeplearning,The Best TensorFlow Class?,https://www.reddit.com/r/deeplearning/comments/dm0g5p/the_best_tensorflow_class/,ChrisBlaaa,1571842517,"Hey folks,
I'm fairly familiar with programming and machine / deep learning. But I'm almost non experienced to tensorFlow and at this point I'm looking for a good Tensorflow class I could enroll.

What I'm not looking for is a keras high level tutorial.
But my feeling after the first little research is, that all provided tf classes are exactly this, keras tutorials...

Do you guys know any real tf class? Either 1.x or 2.0 is welcome. 

Highly appreciate your hints and answers.",2,1,False,self,,,,,
203,deeplearning,t5_2t5eh,2019-10-24,2019,10,24,6,dm5yxj,self.deeplearning,Deep Learning Specialization - Master Deep Learning &amp; Break into Artificial Intelligence (New career &amp; earnings opportunities),https://www.reddit.com/r/deeplearning/comments/dm5yxj/deep_learning_specialization_master_deep_learning/,internetdigitalentre,1571865590,[removed],0,1,False,self,,,,,
204,deeplearning,t5_2t5eh,2019-10-24,2019,10,24,15,dmcmdr,physicsworld.com,Deep learning algorithm helps diagnose neurological emergencies,https://www.reddit.com/r/deeplearning/comments/dmcmdr/deep_learning_algorithm_helps_diagnose/,samcharchil,1571899934,,1,22,False,default,,,,,
205,deeplearning,t5_2t5eh,2019-10-24,2019,10,24,17,dmdgvr,ai-jobs.net,AI Jobs + Internships,https://www.reddit.com/r/deeplearning/comments/dmdgvr/ai_jobs_internships/,ai_jobs,1571905996,,0,1,False,default,,,,,
206,deeplearning,t5_2t5eh,2019-10-24,2019,10,24,18,dmdq2g,self.deeplearning,Neural Network Training Times,https://www.reddit.com/r/deeplearning/comments/dmdq2g/neural_network_training_times/,TobyTheCamel,1571907859,"Sorry for such a simple question but I'm struggling to find confirmation on this. I'm new to neural networks after a background in traditional ML. I've got a computer with an RTX 2070. I've set up all of the NVIDIA/CUDA drivers and installed TF. I don't get any errors and when building models the console output references my GPU.

The problem is that I had a go at running the TF quickstart code ( [https://www.tensorflow.org/tutorials/quickstart/beginner](https://www.tensorflow.org/tutorials/quickstart/beginner)) training a NN on the MNIST dataset with one hidden layer of 128 neurons. This took 5 days to complete 5 epochs. This seemed much slower than I would have expected but maybe that's just my inexperience speaking. Is this a typical training time? If not, what could be at play?",3,0,False,self,,,,,
207,deeplearning,t5_2t5eh,2019-10-24,2019,10,24,18,dmdshw,self.deeplearning,Deep Image - online upscale and enhance image with deep learning app!,https://www.reddit.com/r/deeplearning/comments/dmdshw/deep_image_online_upscale_and_enhance_image_with/,bartbdm,1571908350,"Hello guys!  
I would like to share with you a new tool - Deep Image ([https://deep-image.ai/](https://deep-image.ai/) ) which is an online AI image enhancer based on Artificial Neural Networks (ANN). It can upscale, enhance and improve picture/photo/image quality. 

Currently the app ([https://deep-image.ai/application/](https://deep-image.ai/application/)) is available in beta version. We're working on improvement of the algorithms to receive better image quality after enhancement. If you use the app and upload your images it would be helpful for us in training the neural networks!

In 2020 we are also going to add some new features to the app.

Hope you find it useful and thanks in advance for your help of uploading.  
Any feedback on the app will be appreciated!

PS below you can find short summary of DeepImage in numbers

*Processing img sr6bj65xggu31...*",0,2,False,self,,,,,
208,deeplearning,t5_2t5eh,2019-10-24,2019,10,24,19,dmenea,visflow.org,visflow for deeplearning,https://www.reddit.com/r/deeplearning/comments/dmenea/visflow_for_deeplearning/,loopy_fun,1571914159,,0,1,False,default,,,,,
209,deeplearning,t5_2t5eh,2019-10-24,2019,10,24,20,dmf76y,self.deeplearning,Advice for Deep Learning / Computer Vision build,https://www.reddit.com/r/deeplearning/comments/dmf76y/advice_for_deep_learning_computer_vision_build/,Jurge92,1571917408,"Hi there!

I'm currently looking into building a deep learning desktop that will be used for training, validation and maybe even server usage.

The budget is around $2500 USD.

Do you guys have any advice?

Cheers!",1,1,False,self,,,,,
210,deeplearning,t5_2t5eh,2019-10-24,2019,10,24,21,dmfgs1,medium.com,5 Facial Recognition Trends and Market Predictions 2019,https://www.reddit.com/r/deeplearning/comments/dmfgs1/5_facial_recognition_trends_and_market/,ankur_bansal123,1571918874,,0,1,False,default,,,,,
211,deeplearning,t5_2t5eh,2019-10-24,2019,10,24,21,dmfmx7,self.deeplearning,LSTM implementation in C for byte level predictions,https://www.reddit.com/r/deeplearning/comments/dmfmx7/lstm_implementation_in_c_for_byte_level/,rickardicus,1571919804,"Hi folks!

I wanted to share a project I have been working on. It is a recurrent neural network that predicts bytes which can be used to mimic e.g. lyrical content (such as books). It uses Adam gradient optimization algorithm and the only requirement for using it is a C compiler (I use GCC). No third parties, just a bunch of C source files needed to be compiled. I have included some pretrained models that might be fun to play around with.

I hope this can serve as an inspiration to anyone wanting to make their own stuff from scratch. 

Feel free to come with constructive comments on what I can add/change or any such requests.
Have a great day!

Here is a link to the GitHub repository:

[LSTM implementation (GitHub)](https://github.com/Ricardicus/recurrent-neural-net) 

Dont forget to give the repository a star if you like it, it would be really appreciated!",0,1,False,self,,,,,
212,deeplearning,t5_2t5eh,2019-10-24,2019,10,24,21,dmfr4p,self.deeplearning,[Project] LSTM implementation in C for byte level prediction,https://www.reddit.com/r/deeplearning/comments/dmfr4p/project_lstm_implementation_in_c_for_byte_level/,rickardicus,1571920431,"Hi folks!

I wanted to share a project I have been working on. It is a recurrent neural network that predicts bytes which can be used to mimic e.g. lyrical content (such as books). It uses Adam gradient optimization algorithm and the only requirement for using it is a C compiler (I use GCC). No third parties, just a bunch of C source files needed to be compiled. I have included some pretrained models that might be fun to play around with.

I hope this can serve as an inspiration to anyone wanting to make their own stuff from scratch. 

Feel free to come with constructive comments on what I can add/change or any such requests.
Have a great day!

Here is a link to the GitHub repository:

[LSTM implementation (GitHub)](https://github.com/Ricardicus/recurrent-neural-net) 

Dont forget to give the repository a star if you like it, it would be really appreciated!",4,5,False,self,,,,,
213,deeplearning,t5_2t5eh,2019-10-24,2019,10,24,21,dmftfk,self.deeplearning,Diagnosing brain tumours by routine blood tests using machine learning,https://www.reddit.com/r/deeplearning/comments/dmftfk/diagnosing_brain_tumours_by_routine_blood_tests/,marco_mihajlow,1571920767,"Researchers from Slovenia trained a model that is able to detect brain tumors from simple blood test. This practice is now already in use at the Slovenia's main clinical center, where this model is assiting doctors.

[LINK](https://www.nature.com/articles/s41598-019-51147-3.epdf?author_access_token=PRTjI6CVk0PpBLvEXPRwXdRgN0jAjWel9jnR3ZoTv0OG-qznjOIEjlwb7jpx4Kt-QDpA9EtNMXoSQ1fx666zg80UmD5T3bo8PKtHyVgHgSj4jgXCxaeh-UH5GbXK3_M2PHXJHwFxmo4XgESBecAuTA==)",0,3,False,self,,,,,
214,deeplearning,t5_2t5eh,2019-10-24,2019,10,24,23,dmh9kc,self.deeplearning,Share the implementation of 44.1K Enhanced Singing Voice Separation.,https://www.reddit.com/r/deeplearning/comments/dmh9kc/share_the_implementation_of_441k_enhanced_singing/,choiilji,1571927670,"Hi guys! I did share my Source Separation (Single Channel) implementation.

github :  [https://github.com/AppleHolic/source\_separation](https://github.com/AppleHolic/source_separation)

Recently I add ""Singing Voice Separation"" with DSD100 dataset.

Mainly, it is trained with 44.1k sample rate to satisfy my ears. And it is trained jointly Voice Bank, DSD100 and Audioset dataset.

&amp;#x200B;

You can checkout the test sample on my youtube playlist that has 5 my favorite songs!

\- List :

  \- Sung Si Kyung - On the Street

  \- Linkin Park - Pushing me Away

  \- Baek Ye Rin - His Ocean

  \- Cheeze - Moments for everyone

  \- Taeyeon - Four Seasons

\- link :  [https://www.youtube.com/playlist?list=PLQ4ukFz6Ieir5bZYOns08\_2gMjt4hYP4I](https://www.youtube.com/playlist?list=PLQ4ukFz6Ieir5bZYOns08_2gMjt4hYP4I)

&amp;#x200B;

One of them, ""Sung Si Kyung - On the street"" is added the shifted vocal track with 2 db volume down. So, it can feel fresh to listeners who did already hear that song.

\- link :  [https://www.youtube.com/watch?v=xmoBUf\_6b0c&amp;list=PLQ4ukFz6Ieir5bZYOns08\_2gMjt4hYP4I&amp;index=1](https://www.youtube.com/watch?v=xmoBUf_6b0c&amp;list=PLQ4ukFz6Ieir5bZYOns08_2gMjt4hYP4I&amp;index=1)

&amp;#x200B;

I will keeping to try my best on application and research area on Speech and Music. If you find some problems or improvement for this model, freely contact me.

Thanks.",5,32,False,self,,,,,
215,deeplearning,t5_2t5eh,2019-10-24,2019,10,24,23,dmhksx,self.deeplearning,Slam Dunk Video Classification Tutorial (with TF 2.0 Distributed Training!),https://www.reddit.com/r/deeplearning/comments/dmhksx/slam_dunk_video_classification_tutorial_with_tf/,HenryAILabs,1571929061,https://youtu.be/xizl9vZ81jQ,0,5,False,self,,,,,
216,deeplearning,t5_2t5eh,2019-10-25,2019,10,25,0,dmhz2x,self.deeplearning,Any references on Super-Resolution on Graph using Graph (Convolutional )Neural Network?,https://www.reddit.com/r/deeplearning/comments/dmhz2x/any_references_on_superresolution_on_graph_using/,pradeep_sinngh,1571930726,"Hi everyone,

I was wondering if there is any research/ paper on doing super-resolution on graphs -- going from coarse resolution mesh/ graph to fine resolution mesh/ graph. Any pointers/ references/ thoughts would be appreciated. Thanks a lot.

Q.) How to do DeConvolution on Graph ?

NOTE: I'm considering mesh as a graph and applying graph (conv) neural network to it.",1,2,False,self,,,,,
217,deeplearning,t5_2t5eh,2019-10-25,2019,10,25,1,dmit7v,self.deeplearning,Literature on Recursive Neural Networks for Tabular Data,https://www.reddit.com/r/deeplearning/comments/dmit7v/literature_on_recursive_neural_networks_for/,cgarciae,1571934315,"I've been watching Fastai's videos and got very interested in embeddings for categorical variables ([https://arxiv.org/abs/1604.06737](https://arxiv.org/abs/1604.06737)), I've also been watching Jeremy's other videos on Random Forrest's, this got me thinking on the possibility of creating a NN with a more complex tree-like structure instead of just concatenating all the embeddings. Recursive Neural Networks have this behavior but I haven't seen literature on these for tabular data after searching on Google.  


Is there something similar that goes by another name or is this just unexplored by researchers?",0,1,False,self,,,,,
218,deeplearning,t5_2t5eh,2019-10-25,2019,10,25,5,dmluln,self.deeplearning,Attention Visualiztion for Graph Convolution,https://www.reddit.com/r/deeplearning/comments/dmluln/attention_visualiztion_for_graph_convolution/,hmsajjad,1571947291,"Hi, I am quite new to the concept of attention. I am working with graph data and running graph convolution on it to learn node level embedding first. Then an attention layer to aggregate the nodes to learn a graph level embedding. Here is the setup:

graph-&gt;Conv1(Filter size 128)-&gt;Conv2-(Filter size 64&gt;Conv3(Filter size 32) -&gt; Attention -&gt; Some other layers

After three convolution pass i get a matrix of size number\_of\_nodes\_in\_the\_graph X 32 (embedding length). After the attention layer i get a flat vector representation of the graph with length 32. Now i would like to see which graph nodes were important for the final graph embedding. I am using pytorch for this. I cannot seem to figure out how to map the attention output to input. I would greatly appreciate any help!

Thanks",2,5,False,self,,,,,
219,deeplearning,t5_2t5eh,2019-10-25,2019,10,25,5,dmme4g,self.deeplearning,Project ideas for advanced deep learning course,https://www.reddit.com/r/deeplearning/comments/dmme4g/project_ideas_for_advanced_deep_learning_course/,ranran9991,1571949516,"Hey, reddit  
I have participated a course on advanced deep learning, the main covered topics were

1. GANs and their variations, CycleGan
2. Hypernetworks (and meta learning)
3. Self supervision

We were asked to come up with project ideas (the project should take about 2 weeks) and I would love some suggestions if you have any cool ideas.",2,1,False,self,,,,,
220,deeplearning,t5_2t5eh,2019-10-25,2019,10,25,16,dmtr21,arxiv.org,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://www.reddit.com/r/deeplearning/comments/dmtr21/exploring_the_limits_of_transfer_learning_with_a/,iordanissh,1571987271,,0,2,False,default,,,,,
221,deeplearning,t5_2t5eh,2019-10-25,2019,10,25,16,dmtyqw,self.deeplearning,CNN network behavior when a class is having multi-objects data in it.,https://www.reddit.com/r/deeplearning/comments/dmtyqw/cnn_network_behavior_when_a_class_is_having/,VeeranjaneyuluToka,1571988704,"Hi All,  I am training a model with image data and each class has different  distributions of data and still converges. Am wondering what could be  the reason and how the representation are at the end of training?  Anybody has any suggestions or comments on this kind of behavior?  Thanks.",3,9,False,self,,,,,
222,deeplearning,t5_2t5eh,2019-10-25,2019,10,25,18,dmuxcs,self.reinforcementlearning,Calculate Empowerment in Python,https://www.reddit.com/r/deeplearning/comments/dmuxcs/calculate_empowerment_in_python/,ZeroMaxinumXZ,1571995529,,0,3,False,default,,,,,
223,deeplearning,t5_2t5eh,2019-10-25,2019,10,25,20,dmwa91,self.deeplearning,"[R] Netflix just open-sourced Polynote, a cool Machine Learning, and Data Science workflow tool",https://www.reddit.com/r/deeplearning/comments/dmwa91/r_netflix_just_opensourced_polynote_a_cool/,cdossman,1572004001," Netflix just open-sourced Polynote, a cool Machine Learning, and Data Science workflow tool

Polynote Website: https://polynote.org/

Github:https://github.com/polynote/polynote?fbclid=IwAR1bF4fcoe7R0rqZIrIeV7c5pYTPprMBzhqsmpEgliOGkf4iJ8I6CNuha\_8",7,50,False,self,,,,,
224,deeplearning,t5_2t5eh,2019-10-25,2019,10,25,20,dmwauv,self.deeplearning,Prevent Depth Map in Images,https://www.reddit.com/r/deeplearning/comments/dmwauv/prevent_depth_map_in_images/,ap_1690,1572004099,"Hey Everyone is there is some  way from which i can prevent depth map algorithm from predicting depth in an image on some screen .
Example if i have input image on the screen on my laptop then depth map algo will predict the depth in the image rather than predicting the screen and boundaries of my laptop.",5,2,False,self,,,,,
225,deeplearning,t5_2t5eh,2019-10-25,2019,10,25,23,dmxvii,medium.com,Intro to CoordConv,https://www.reddit.com/r/deeplearning/comments/dmxvii/intro_to_coordconv/,07williamsm,1572012129,,0,2,False,default,,,,,
226,deeplearning,t5_2t5eh,2019-10-26,2019,10,26,1,dmzqc3,self.deeplearning,Deep Learning Project Consulting with Henry AI Labs,https://www.reddit.com/r/deeplearning/comments/dmzqc3/deep_learning_project_consulting_with_henry_ai/,HenryAILabs,1572020268,[https://www.patreon.com/henryailabs](https://www.patreon.com/henryailabs),1,0,False,self,,,,,
227,deeplearning,t5_2t5eh,2019-10-26,2019,10,26,3,dn1c2w,logicalclocks.com,[R] Asynchronous Search for Directed Hyperparameter Search with PySpark for TensorFlow,https://www.reddit.com/r/deeplearning/comments/dn1c2w/r_asynchronous_search_for_directed_hyperparameter/,jpdowlin,1572027037,,0,5,False,default,,,,,
228,deeplearning,t5_2t5eh,2019-10-26,2019,10,26,9,dn6a2q,medium.com,[Helpful] Part 2 of Reinforcement Learning: Introduction to the Markov Process,https://www.reddit.com/r/deeplearning/comments/dn6a2q/helpful_part_2_of_reinforcement_learning/,cdossman,1572048838,,0,1,False,default,,,,,
229,deeplearning,t5_2t5eh,2019-10-26,2019,10,26,10,dn77a0,ai.googleblog.com,Using Deep Learning to Predict the Olfactory Properties of Molecules,https://www.reddit.com/r/deeplearning/comments/dn77a0/using_deep_learning_to_predict_the_olfactory/,HN_Crosspost_Bot,1572053599,,0,3,False,default,,,,,
230,deeplearning,t5_2t5eh,2019-10-26,2019,10,26,14,dn9wny,self.deeplearning,Coursera Staff:If you want Andrew Ng to come out and teach the reinforcement learning? Vote this one,https://www.reddit.com/r/deeplearning/comments/dn9wny/coursera_staffif_you_want_andrew_ng_to_come_out/,JonathanSum777,1572069472,"Please vote here in this post if: **you want Andrew Ng to come out and teach the reinforcement learning.**

For more info: please check this post too.

 [https://coursera.community/course-suggestions-51/a-reinforcement-learning-course-by-andrew-ng-2528](https://coursera.community/course-suggestions-51/a-reinforcement-learning-course-by-andrew-ng-2528)",13,156,False,self,,,,,
231,deeplearning,t5_2t5eh,2019-10-26,2019,10,26,16,dnak03,trumptraveling.blogspot.com,"Google Waymo internship, summer 2020",https://www.reddit.com/r/deeplearning/comments/dnak03/google_waymo_internship_summer_2020/,qikpal,1572074205,,1,1,False,default,,,,,
232,deeplearning,t5_2t5eh,2019-10-27,2019,10,27,1,dngg2x,self.deeplearning,Deep-Fakes Detection Research,https://www.reddit.com/r/deeplearning/comments/dngg2x/deepfakes_detection_research/,pranjalranjan299,1572108780,"https://youtu.be/cYKxJk3lz8E

This is our latest work on Deep-Fake detection. Watch the whole video to know more. Please like, share and subscribe ",4,4,False,self,,,,,
233,deeplearning,t5_2t5eh,2019-10-27,2019,10,27,4,dnil58,self.deeplearning,HELP : How to build encoder decoder model,https://www.reddit.com/r/deeplearning/comments/dnil58/help_how_to_build_encoder_decoder_model/,vaibhavkumar049,1572118453,"I am trying to build an encoder-decoder model. encoder as CNN and decoder as RNN or LSTM, so should I pass my image into CNN and then flatten it out and then pass to decoder? or complete channels to decoder?",2,1,False,self,,,,,
234,deeplearning,t5_2t5eh,2019-10-27,2019,10,27,6,dnjtji,self.deeplearning,"Your feedbacks on my ""Visual Text Correction"" research",https://www.reddit.com/r/deeplearning/comments/dnjtji/your_feedbacks_on_my_visual_text_correction/,amirmz,1572124191,"Hi all,

I would like to get some feedback on one of my current projects, named Visual Text Correction.

[http://openaccess.thecvf.com/content\_ECCV\_2018/html/Amir\_Mazaheri\_Visual\_Text\_Correction\_ECCV\_2018\_paper.html](http://openaccess.thecvf.com/content_ECCV_2018/html/Amir_Mazaheri_Visual_Text_Correction_ECCV_2018_paper.html)

The problem is basically to find an inaccuracy in the description of a video, and simply fix it. However, it is trained on synthetic data (MPI Movie dataset with audio descriptions).

Also, I found the main idea of the newly released VideoBERT paper very similar to my work; however, they mainly focus on rich feature learning and I focus on the application.

Besides your feedback, I appreciate any comments about 

First of all, does anybody aware of any human-made mistakes/typos dataset on video captions?

Secondly, do you think if it is ultimately a useful application to pursue my research on? Like a Software that can rearrange the YouTube captions, detect and remove unrelated texts in Facebook uploaded videos' captions, and etc?

Thanks",1,5,False,self,,,,,
235,deeplearning,t5_2t5eh,2019-10-27,2019,10,27,8,dnlkd3,self.deeplearning,Legality of celebrity deepfake for music video?,https://www.reddit.com/r/deeplearning/comments/dnlkd3/legality_of_celebrity_deepfake_for_music_video/,whereskob,1572132527,"Looking for some advice!

Shot a music video (cover of meatloaf - Bat Out of hell) and at one point in the story, the band call up Meatloaf who is currently a lookalike but I would really love to deepfake it.

Could this be potentially opening up for legal action or considered just satire? It won't be defamatory or show him in a bad light. Just a bit of fun in a short scene.

Thanks all!",0,1,False,self,,,,,
236,deeplearning,t5_2t5eh,2019-10-27,2019,10,27,14,dnp2ku,self.deeplearning,Deep Learning portable workstation - Student Advice,https://www.reddit.com/r/deeplearning/comments/dnp2ku/deep_learning_portable_workstation_student_advice/,KrakenML,1572152787,"Hi All,

Was hoping to get some suggestions as I'm starting my Comp science/ Genetics dual major degree next year and would like some laptop advice. Very interested in DL and building my own small projects 

Would the Nvidia Quadro RTX 5000 GPU be beneficial for deep learning?  The razer studio laptop has recently been released with the option to add the Quadro Rtx 5000 or would I be better off just getting the NVIDIAGeForce RTX 2080 with Max-Q Design (8GB GDDR6 VRAM) instead 


https://www.razer.com/gaming-laptops/razer-blade/shop#rtx5000--oled-60hz-9thgen--1tb-mercury",7,0,False,self,,,,,
237,deeplearning,t5_2t5eh,2019-10-27,2019,10,27,14,dnp7wy,self.deeplearning,How do I classify voice emotion with deep learning?,https://www.reddit.com/r/deeplearning/comments/dnp7wy/how_do_i_classify_voice_emotion_with_deep_learning/,archx64,1572153787,I'm developing a chat bot that can recognize emotion of the user by listening to the voice of the user,6,16,False,self,,,,,
238,deeplearning,t5_2t5eh,2019-10-27,2019,10,27,15,dnps7i,self.deeplearning,Help: need some advices to convert a 2D model to 3D,https://www.reddit.com/r/deeplearning/comments/dnps7i/help_need_some_advices_to_convert_a_2d_model_to_3d/,oqader,1572157945,"Hello there,

I'm trying to convert an inpainting model ( Nvidia's model: Partial Convolution) from 2D to 3D. I've never worked on 3D models before and this project is considered my graduation project.

I need some guidance in doing that. Any help or advice is highly appreciated",2,2,False,self,,,,,
239,deeplearning,t5_2t5eh,2019-10-28,2019,10,28,4,dny7eo,neuraxio.com,How to Code Neat Machine Learning Pipelines,https://www.reddit.com/r/deeplearning/comments/dny7eo/how_to_code_neat_machine_learning_pipelines/,GChe,1572204158,,0,13,False,default,,,,,
240,deeplearning,t5_2t5eh,2019-10-28,2019,10,28,7,dnzxhf,self.deeplearning,Training convolutional variational autoencoders,https://www.reddit.com/r/deeplearning/comments/dnzxhf/training_convolutional_variational_autoencoders/,Mike_Sv86,1572214178,"Hi all.

Iam trying to train a convolutional variational autoencoder (CVAE) on computed tomography (CT) IMAGES (176X224 px) . The training data is normalized between 0 and 1 and Iam using approximately the same model structure as in keras autoencoder tutorial.

https://keras.io/examples/variational_autoencoder/

I only changed the depth and the size of the latent space to 128.

For the loss function I use Mse and KL, with a weight annealing for the KL part. 

When I train the network it seems like it is learning something, but if I try to reconstruct images after training, the output images are just noisy. 

I have no clue what it is Iam doing wrong. 

Any advice would be really great. 

Cheers, 

M",0,1,False,self,,,,,
241,deeplearning,t5_2t5eh,2019-10-28,2019,10,28,9,do1v4v,lionbridge.ai,"Intro to Facial Recognition: What it is, how it works, accuracy",https://www.reddit.com/r/deeplearning/comments/do1v4v/intro_to_facial_recognition_what_it_is_how_it/,LimarcAmbalina,1572224324,,0,2,False,default,,,,,
242,deeplearning,t5_2t5eh,2019-10-28,2019,10,28,14,do4pkq,self.deeplearning,Training convolutional variational autoencoders,https://www.reddit.com/r/deeplearning/comments/do4pkq/training_convolutional_variational_autoencoders/,Mike_Sv86,1572241515,"Hi all.

Iam trying to train a convolutional variational autoencoder (CVAE) on computed tomography (CT) Images (176X224 px) . The training data is normalized between 0 and 1 and Iam using approximately the same model structure as in keras autoencoder tutorial.

https://keras.io/examples/variational_autoencoder/

I only changed the depth and the size of the latent space to 128.

For the loss function I use Mse and KL, with a weight annealing for the KL part. 

When I train the network it seems like it is learning something, but if I try to reconstruct images after training, the output images are just noisy. 

I have no clue what it is Iam doing wrong. 

Any advice would be really great. 

Cheers, 

M",9,6,False,self,,,,,
243,deeplearning,t5_2t5eh,2019-10-28,2019,10,28,19,do789m,rubikscode.net,Business Value of Artificial Intelligence,https://www.reddit.com/r/deeplearning/comments/do789m/business_value_of_artificial_intelligence/,RubiksCodeNMZ,1572260188,,0,0,False,default,,,,,
244,deeplearning,t5_2t5eh,2019-10-28,2019,10,28,22,do8yq2,self.deeplearning,A PyTorch implementation for StyleGAN with full features.,https://www.reddit.com/r/deeplearning/comments/do8yq2/a_pytorch_implementation_for_stylegan_with_full/,huangzh13,1572269963,Github:  [https://github.com/huangzh13/StyleGAN.pytorch](https://github.com/huangzh13/StyleGAN.pytorch),0,3,False,self,,,,,
245,deeplearning,t5_2t5eh,2019-10-29,2019,10,29,2,docbbe,self.deeplearning,"AI Weekly Update - October 28th, 2019",https://www.reddit.com/r/deeplearning/comments/docbbe/ai_weekly_update_october_28th_2019/,HenryAILabs,1572284861,"This update covers Google AI's learning to smell with Graph Neural Networks, NVIDIA's Mellotron Rhymthic Speech Synthesis, Facebook's SlowFast model and many more!

https://youtu.be/S16YXORfZLg",0,0,False,self,,,,,
246,deeplearning,t5_2t5eh,2019-10-29,2019,10,29,4,dodybe,zeldatech.com,Deep Learning: A Practitioners Approach By Josh Patterson &amp; Adam Gibson PDF,https://www.reddit.com/r/deeplearning/comments/dodybe/deep_learning_a_practitioners_approach_by_josh/,psychonekk,1572291305,,0,0,False,default,,,,,
247,deeplearning,t5_2t5eh,2019-10-29,2019,10,29,5,doemyv,self.deeplearning,How does ReLu update weights if derivatives are always 1 or 0,https://www.reddit.com/r/deeplearning/comments/doemyv/how_does_relu_update_weights_if_derivatives_are/,StrasJam,1572293942,"I am surely missing something here, but from what I understand, weights are updated based on the gradient of the backpropogated error. But if the gradient of the ReLu is always 1 for values &gt;0, then how does the network learn which weights need to be adjusted compared to others? Wouldn't all weights be updated the same amount due to the gradient being the same?",9,9,False,self,,,,,
248,deeplearning,t5_2t5eh,2019-10-29,2019,10,29,10,doiqv6,self.deeplearning,Recommender Systems Specialization  Your gateway to new career and income opportunities,https://www.reddit.com/r/deeplearning/comments/doiqv6/recommender_systems_specialization_your_gateway/,internetdigitalentre,1572311746,[removed],0,1,False,self,,,,,
249,deeplearning,t5_2t5eh,2019-10-29,2019,10,29,12,dokjab,self.deeplearning,How to build a system which can tell which product(s) are being picked up and duration using Computer Vision/Deep Learning?,https://www.reddit.com/r/deeplearning/comments/dokjab/how_to_build_a_system_which_can_tell_which/,deeplearning2018,1572320920,"So a marketing company reached out saying that they need to build a system where a unmanned kiosk has products eg perfumes, markup when customers pick up a product and try, they need to know which product has been picked up and the duration.

I am pretty familiar with the Computer Vision and Deep Learning, so far I have tried edge detection using Canny and Holistically-Nested Edge Detection and place the products on 4 contrasting round/rectangle markers on a white paper background, so far so good

&amp;#x200B;

But what are your ideas around building such a system? Am I on the right track? Any code/ideas that you can please share with me would help fast-track its development",2,0,False,self,,,,,
250,deeplearning,t5_2t5eh,2019-10-29,2019,10,29,13,dokw7e,self.deeplearning,Where can I find the deep learning TFLOPS performance of Nvidia graphics ?,https://www.reddit.com/r/deeplearning/comments/dokw7e/where_can_i_find_the_deep_learning_tflops/,archx64,1572323008,"I searched for GPU benchmarks and what I found was texture units, shaders and clock speeds. My company wants to build a deep learning server and I have to decide which gpu will be suitable.",3,2,False,self,,,,,
251,deeplearning,t5_2t5eh,2019-10-29,2019,10,29,14,doldcv,self.deeplearning,How small Resnet can we develop,https://www.reddit.com/r/deeplearning/comments/doldcv/how_small_resnet_can_we_develop/,ap_1690,1572326132,"Hey how small can a Resnet we can make.
Like is resnet8 is possible 
Thanks in advance",2,1,False,self,,,,,
252,deeplearning,t5_2t5eh,2019-10-29,2019,10,29,17,domyhc,self.deeplearning,KL divergence between variational autoencoder predictions,https://www.reddit.com/r/deeplearning/comments/domyhc/kl_divergence_between_variational_autoencoder/,Mike_Sv86,1572338119,"Hi all.

I trained a VAE and now I wanted to use the KL divergence as a similarity measure. My idea was to send in example to different images through the encoder part, get the latent variables and compare the two distributions.

Unfortunately I have no clue how to implement that in python.

Does anyone know if there is an implementation?

Thanks for any advice,

&amp;#x200B;

cheers,

&amp;#x200B;

M",0,2,False,self,,,,,
253,deeplearning,t5_2t5eh,2019-10-29,2019,10,29,17,don1y4,self.deeplearning,"[Building a Dataset] How I created a 40,000 labeled audio dataset in 4 hours of work and $500",https://www.reddit.com/r/deeplearning/comments/don1y4/building_a_dataset_how_i_created_a_40000_labeled/,cdossman,1572338848," How I created a 40,000 labeled audio dataset in 4 hours of work and $500

[https://towardsdatascience.com/how-i-created-a-40-000-labeled-audio-dataset-in-4-hours-of-work-and-500-17ad9951b180](https://towardsdatascience.com/how-i-created-a-40-000-labeled-audio-dataset-in-4-hours-of-work-and-500-17ad9951b180)",6,14,False,self,,,,,
254,deeplearning,t5_2t5eh,2019-10-29,2019,10,29,17,don2uu,self.deeplearning,How to save image Embeddings?,https://www.reddit.com/r/deeplearning/comments/don2uu/how_to_save_image_embeddings/,abhiksark,1572339038,"I am performing a facial recognition task. For this, I am extracting facial embeddings (2048 vector) using a Pretrained Model. For matching the embeddings(features)  I am using Faiss([https://github.com/facebookresearch/faiss](https://github.com/facebookresearch/faiss))

Currently, I am saving embeddings(features) as .npy file. For 50k images, it is approximately taking 500MB of disk space. What is the best way to save embeddings?",3,2,False,self,,,,,
255,deeplearning,t5_2t5eh,2019-10-29,2019,10,29,18,donip0,i.redd.it,Introduction to Anomaly Detection with Auto-Encoder on time series,https://www.reddit.com/r/deeplearning/comments/donip0/introduction_to_anomaly_detection_with/,AstroThese,1572342127,,6,90,False,image,,,,,
256,deeplearning,t5_2t5eh,2019-10-29,2019,10,29,19,donoi3,medium.com,5 Facial Recognition Trends and Market Predictions 2019,https://www.reddit.com/r/deeplearning/comments/donoi3/5_facial_recognition_trends_and_market/,Neil_Patel001,1572343249,,0,1,False,default,,,,,
257,deeplearning,t5_2t5eh,2019-10-29,2019,10,29,19,doo0aj,nature.com,A deep learning framework for neuroscience,https://www.reddit.com/r/deeplearning/comments/doo0aj/a_deep_learning_framework_for_neuroscience/,eleitl,1572345433,,0,1,False,default,,,,,
258,deeplearning,t5_2t5eh,2019-10-30,2019,10,30,1,dos2h1,self.deeplearning,I want to start a blog to share my work !!! But how??,https://www.reddit.com/r/deeplearning/comments/dos2h1/i_want_to_start_a_blog_to_share_my_work_but_how/,author31,1572365778,"Hi guys! I have been learning deep learning from scratch for about 11 months. Along the way of learning I tackled a lot of problems, I found that in this DL field there was really few tutorials from scratch to application, they just covered the intermediate layer which is how to build a model. I want to start a blog about covering how to use a certain model to solve real life problem. I really hope that I can get some opinions or advices from you guys.",3,3,False,self,,,,,
259,deeplearning,t5_2t5eh,2019-10-30,2019,10,30,4,doufrr,self.deeplearning,[Book] Facebook Page of Generative Adversarial Networks Projects by Kailash Ahirwar,https://www.reddit.com/r/deeplearning/comments/doufrr/book_facebook_page_of_generative_adversarial/,kailashahirwar12,1572375726,"Generative Adversarial Networks Projects\[Book\] is my book on GANs projects and practical applications of GANs. To create awareness around my book and GANs, I created a Facebook page. People who are interested in GANs and want to keep themselves updated can follow the page. 

[https://www.facebook.com/gansprojects1](https://www.facebook.com/gansprojects1)

Provide your suggestions on how to improve the page and you might get a free digital copy of the book for free.",0,1,False,self,,,,,
260,deeplearning,t5_2t5eh,2019-10-30,2019,10,30,4,dov8ji,medium.com,BANANAS: A new method for neural architecture search,https://www.reddit.com/r/deeplearning/comments/dov8ji/bananas_a_new_method_for_neural_architecture/,afathman,1572379063,,0,13,False,default,,,,,
261,deeplearning,t5_2t5eh,2019-10-30,2019,10,30,5,dovvaj,self.deeplearning,Inference with sequences shorter than training ones,https://www.reddit.com/r/deeplearning/comments/dovvaj/inference_with_sequences_shorter_than_training/,shahriar49,1572381586,"I have an LSTM network trained to classify 1-year sequences of pixel values taken by Landsat to a set of land covers (each time step contains six bands per pixel, and I add 'Day-of-Year' also at each time step before training or testing.

Now my question is that what will happen if I feed the network during test time with sequences shorted than 1-year (say 3 months)? I assume that the network will learn to recognize different time patterns through the year so it can classify it. I can imagine that the network will try to find the best match of the shorter (e.g. 3-month) sequence pattern to the full patterns and classify it accordingly. But is there any theoretical ground for such a thing?",0,1,False,self,,,,,
262,deeplearning,t5_2t5eh,2019-10-30,2019,10,30,6,dowubh,self.deeplearning,Overview of Facebook Research at ICCV!,https://www.reddit.com/r/deeplearning/comments/dowubh/overview_of_facebook_research_at_iccv/,HenryAILabs,1572385578,[https://youtu.be/W5EsADGw9CA](https://youtu.be/W5EsADGw9CA),0,2,False,self,,,,,
263,deeplearning,t5_2t5eh,2019-10-30,2019,10,30,10,dp04sn,self.deeplearning,How do I get an ANN to do what I want?,https://www.reddit.com/r/deeplearning/comments/dp04sn/how_do_i_get_an_ann_to_do_what_i_want/,Mjbgtaad56,1572400274,"So, I know this is a fairly broad question, so I'm looking for a fairly broad answer. If I think of some task that I want a neural network to optimize or automate, how should I decided what the inputs and outputs are and what the structure, complexity, and functionality (ie recursion) should be.

For example, tic tac to, a self driving car, or perhaps an ANN powered videogame bot (something I've been interested in).

I know that an ANN is meant to simulate a brain, so I'd assume the inputs should be thought of as senses, and the outputs as muscles, actions, or results (thoughts), but I'm not sure what they should be sometimes or what would be the most effective I/O.",4,0,False,self,,,,,
264,deeplearning,t5_2t5eh,2019-10-30,2019,10,30,11,dp0apq,self.deeplearning,Advanced Deep Learning Topics,https://www.reddit.com/r/deeplearning/comments/dp0apq/advanced_deep_learning_topics/,data_datum,1572401059, [https://lilianweng.github.io/lil-log/](https://lilianweng.github.io/lil-log/),4,30,False,self,,,,,
265,deeplearning,t5_2t5eh,2019-10-30,2019,10,30,13,dp1ugm,self.deeplearning,Multiple weak/cheap GPUs or one powerful/expensive one?,https://www.reddit.com/r/deeplearning/comments/dp1ugm/multiple_weakcheap_gpus_or_one_powerfulexpensive/,2assakalan9,1572409317,"All other factors kept constant, what are the differences between training DL models on let's say two 4GB GPUs vs one 8GB GPU? If I know my way around DL libraries and how to use multiple devices, are there any differences?",2,1,False,self,,,,,
266,deeplearning,t5_2t5eh,2019-10-30,2019,10,30,14,dp2idd,lionbridge.ai,AI Learns to Exploit Bugs in OPEN AI's Hide and Seek Game(Best Machine Learning Youtube Videos Under 10 Minutes),https://www.reddit.com/r/deeplearning/comments/dp2idd/ai_learns_to_exploit_bugs_in_open_ais_hide_and/,LimarcAmbalina,1572413774,,0,0,False,default,,,,,
267,deeplearning,t5_2t5eh,2019-10-30,2019,10,30,16,dp3848,medium.com,Significance and Use Cases of AI Weather Forecasting Across Industries,https://www.reddit.com/r/deeplearning/comments/dp3848/significance_and_use_cases_of_ai_weather/,Neil_Patel001,1572419064,,0,1,False,default,,,,,
268,deeplearning,t5_2t5eh,2019-10-30,2019,10,30,17,dp3p38,self.deeplearning,Padding with 0s vs Dropping out the last few features?,https://www.reddit.com/r/deeplearning/comments/dp3p38/padding_with_0s_vs_dropping_out_the_last_few/,SEAsFinest,1572422686,"I have a 1d signal which after convoluting gives me a  feature vector of 220480. I have another 1d signal which after convoluting gives me a feature vector of 22030. I am to stack the first feature vector on top of the second one. To do that, I have to max pool the first one to reduce it to 22030. However, as 22030 does not completely divide 220480, I can not accurately use a fixed kernel size of 10 (220480/10 is 22048 which is slightly greater than 22030). To fix this, do I pad with 0s so I can use a kernel size of 11 or do I just drop out the last few features and bring it down to 220300? What are the pros and cons of each approach?

&amp;#x200B;

Thanks",2,6,False,self,,,,,
269,deeplearning,t5_2t5eh,2019-10-30,2019,10,30,18,dp4ent,self.deeplearning,YOLO object detection - how to use meta file after training a model on custom data,https://www.reddit.com/r/deeplearning/comments/dp4ent/yolo_object_detection_how_to_use_meta_file_after/,yazmaz54,1572428195,"Hi everyone,

I am trying to make a customized yolo model that will learn how to predict ""hands"". I have trained my model and found after some searching that the new generated weights are in the ckpt folder under a .meta file. How can I use this meta file in my code to test my trained model please?",0,1,False,self,,,,,
270,deeplearning,t5_2t5eh,2019-10-30,2019,10,30,19,dp4xv7,self.deeplearning,"[Research] Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities, and Challenges",https://www.reddit.com/r/deeplearning/comments/dp4xv7/research_explainable_artificial_intelligence_xai/,cdossman,1572431797,[removed],0,1,False,self,,,,,
271,deeplearning,t5_2t5eh,2019-10-31,2019,10,31,0,dp81be,hashtagtechgeek.com,250+ Machine Learning and Deep Learning Resources (Courseware and Lecture Videos),https://www.reddit.com/r/deeplearning/comments/dp81be/250_machine_learning_and_deep_learning_resources/,BlisteringBernacle,1572448175,,2,32,False,default,,,,,
272,deeplearning,t5_2t5eh,2019-10-31,2019,10,31,0,dp84n2,nature.com,A Deep Learning framework for Neuroscience,https://www.reddit.com/r/deeplearning/comments/dp84n2/a_deep_learning_framework_for_neuroscience/,aiforworld2,1572448585,,0,4,False,default,,,,,
273,deeplearning,t5_2t5eh,2019-10-31,2019,10,31,1,dp93mp,self.deeplearning,[Research] Harnessing Indirect Training Data for End-to-End Automatic Speech Translation: Tricks of the Trade,https://www.reddit.com/r/deeplearning/comments/dp93mp/research_harnessing_indirect_training_data_for/,cdossman,1572452791,[removed],0,1,False,self,,,,,
274,deeplearning,t5_2t5eh,2019-10-31,2019,10,31,3,dpad3x,medium.com,Yoshua Bengio on Human vs Machine Intelligence,https://www.reddit.com/r/deeplearning/comments/dpad3x/yoshua_bengio_on_human_vs_machine_intelligence/,Yuqing7,1572458858,,0,1,False,default,,,,,
275,deeplearning,t5_2t5eh,2019-10-31,2019,10,31,3,dparzb,github.com,netflow.js Neural Network Visualization,https://www.reddit.com/r/deeplearning/comments/dparzb/netflowjs_neural_network_visualization/,hmkcode,1572460663,,0,1,False,default,,,,,
276,deeplearning,t5_2t5eh,2019-10-31,2019,10,31,3,dpaxt8,self.deeplearning,netflow.js Neural Network Visualization,https://www.reddit.com/r/deeplearning/comments/dpaxt8/netflowjs_neural_network_visualization/,hmkcode,1572461365,[removed],0,1,False,self,,,,,
277,deeplearning,t5_2t5eh,2019-10-31,2019,10,31,5,dpcaov,self.deeplearning,What type of network would you use to count things in a still image,https://www.reddit.com/r/deeplearning/comments/dpcaov/what_type_of_network_would_you_use_to_count/,megaku,1572467181,"So, say I want to make a software that counts things from a picture, not too complicated. The images would be simple but I would want the software to ""guess"" what I want it to count, be it apples on a table or coins on the floor. To keep it simple it'll always be round-ish objects on plain backgrounds.  Should I use a convolutional or fully connected network? Got any ideas? Is it doable even? Should I use another ML method?",0,1,False,self,,,,,
278,deeplearning,t5_2t5eh,2019-10-31,2019,10,31,6,dpcyk4,self.deeplearning,Google Research at ICCV,https://www.reddit.com/r/deeplearning/comments/dpcyk4/google_research_at_iccv/,HenryAILabs,1572469995,[https://youtu.be/z-yvY8iAaHM](https://youtu.be/z-yvY8iAaHM),0,3,False,self,,,,,
279,deeplearning,t5_2t5eh,2019-10-31,2019,10,31,11,dpheni,self.deeplearning,Why do we use L2 norms and what are the practical use other than getting vector distance?,https://www.reddit.com/r/deeplearning/comments/dpheni/why_do_we_use_l2_norms_and_what_are_the_practical/,mlslayer,1572489814,"Why do we use L2 norms, instead of just taking a sum of all absolute values of a vector? Is there some mathematically reason behind it?  


Also, I've learned that norms represent distance of vectors. For example, if you had two error vectors, you can take the L2 norm to see which vector is worse(distance of error). Are there any other practical uses for norms?",9,4,False,self,,,,,
280,deeplearning,t5_2t5eh,2019-10-31,2019,10,31,12,dpi61w,self.deeplearning,Bayesian Deep Learning Summer School 2019,https://www.reddit.com/r/deeplearning/comments/dpi61w/bayesian_deep_learning_summer_school_2019/,data_datum,1572493922, [https://github.com/bayesgroup/deepbayes-2019](https://github.com/bayesgroup/deepbayes-2019),9,27,False,self,,,,,
281,deeplearning,t5_2t5eh,2019-10-31,2019,10,31,16,dpjzed,self.deeplearning,Are there any studies on the generalization performance of FlowNet or FlowNet2?,https://www.reddit.com/r/deeplearning/comments/dpjzed/are_there_any_studies_on_the_generalization/,mr_denoza,1572505972,"I was reading the original paper on FlowNet and the authors have presented their findings on four different data sets, however, I was curious to know if there is any study which highlights the generalization capabilities of these network architectures. For example: the performance of the network trained on Flying Chairs and tested on Sintel data set.",0,3,False,self,,,,,
282,deeplearning,t5_2t5eh,2019-10-31,2019,10,31,16,dpk4lc,self.deeplearning,Getting same results with half the number of time-steps as in original Hindsight Experience Replay Paper?,https://www.reddit.com/r/deeplearning/comments/dpk4lc/getting_same_results_with_half_the_number_of/,rrz0,1572507066,"I am reproducing the results from Hindsight Experience Replay by Andrychowicz et. al. In the original paper they present the results below, where the agent is trained for 200 epochs.

200 epochs \* 800 episodes \* 50 time steps = 8,000,000 total time steps.

&amp;#x200B;

https://preview.redd.it/bxc42reaxtv31.png?width=1044&amp;format=png&amp;auto=webp&amp;s=5fdbd7fe9b99a4a7951a507cc286225a16024851

I try to reproduce the resutls but instead of using 8 cpu cores, I am using 19 CPU cores.

I train the FetchPickAndPlace for 120 epochs, but with only 50 episodes per epoch. Therefore 120 \* 50 \* 50 = 300,000 iterations. I present the curve below:

&amp;#x200B;

&amp;#x200B;

https://preview.redd.it/i0wqunibxtv31.png?width=651&amp;format=png&amp;auto=webp&amp;s=a836c8df1968a76dabdf1101911bf701e44f708d

&amp;#x200B;

and logger output for the first two epochs:

&amp;#x200B;

&amp;#x200B;

https://preview.redd.it/vo56xz0cxtv31.png?width=233&amp;format=png&amp;auto=webp&amp;s=e1da41ba81ed3460edc27f235313705ac723e2c1

&amp;#x200B;

Now, as can be seen from my tensorboard plot, after 30 epochs we get a steady success rate very close to 1. 30 epochs \* 50 episodes \* 50 time steps = 75,000 iterations. Therefore it took the algorithm 75,000 time steps to learn this environment.

&amp;#x200B;

The original paper took approximately 50 \* 800 \* 50 = 2,000,000 time steps to achieve the same goal.

&amp;#x200B;

How is it that in my case the environment was solved nearly 30 times faster? Are there any flaws in my workings above? Surely I am doing something wrong, right?

&amp;#x200B;

NB: This was not a one off case. I tested again and got the same results.",0,2,False,self,,,,,
283,deeplearning,t5_2t5eh,2019-10-31,2019,10,31,21,dpmu3c,medium.com,Analyzing 2019 Trends of Artificial Intelligence in Cybersecurity,https://www.reddit.com/r/deeplearning/comments/dpmu3c/analyzing_2019_trends_of_artificial_intelligence/,Harshitkansal,1572525015,,0,1,False,default,,,,,
284,deeplearning,t5_2t5eh,2019-10-31,2019,10,31,22,dpnge6,self.deeplearning,Error when running model.predict(),https://www.reddit.com/r/deeplearning/comments/dpnge6/error_when_running_modelpredict/,raghu_1809,1572528093,"I am using VGG16 model for which whenever I'm trying to predict, the model gives me an error saying 

""Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. 	 

\[\[{{node block1\_conv1/convolution}}\]\] 	 

\[\[fc2/Relu/\_199\]\]   
""

Can somebody tell me a solution for this, please? thanks in advance.",10,1,False,self,,,,,
