,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2017-7-2,2017,7,2,13,6krl3p,Practical Deep Learning with PyTorch,https://www.reddit.com/r/deeplearning/comments/6krl3p/practical_deep_learning_with_pytorch/,deeplearningwizard,1498971103,,0,1
1,2017-7-2,2017,7,2,15,6krxry,Deep Learning Audio files,https://www.reddit.com/r/deeplearning/comments/6krxry/deep_learning_audio_files/,choseausernamereddit,1498977270,"I want to find out what can DL do with audio files. Like, what kind of algorithms are there (other than cocktail party problem) that can help analyse an audio (maybe an mp3 song).",3,3
2,2017-7-2,2017,7,2,23,6ktkpq,Looking for machine learning investment strategies,https://www.reddit.com/r/deeplearning/comments/6ktkpq/looking_for_machine_learning_investment_strategies/,MachineLearnInvest,1499006732,"Hi Reddit!

I and a group of partners with significant investment management industry experience in Canada are looking at the machine learning space to potentially build a new ai focused investment products in the Canadian marketplace.  If you are working on machine learning equity investment strategies and have a meaningful track record or back test we would love to hear from you, especially if you are in the Toronto-Waterloo region corridor.  Send me a DM! Cheers!",0,0
3,2017-7-3,2017,7,3,2,6kugkz,A backpropagation example you can actually understand,https://www.reddit.com/r/deeplearning/comments/6kugkz/a_backpropagation_example_you_can_actually/,sohel888,1499016782,,0,3
4,2017-7-3,2017,7,3,5,6kvc7l,Distribution of Deep Learning Supertalent in Industry - Any feedback?,https://www.reddit.com/r/deeplearning/comments/6kvc7l/distribution_of_deep_learning_supertalent_in/,alecmgo,1499026324,,3,1
5,2017-7-3,2017,7,3,23,6l023q,DeepMinds Relational Networks  Demystified,https://www.reddit.com/r/deeplearning/comments/6l023q/deepminds_relational_networks_demystified/,harvey_slash,1499090532,,0,10
6,2017-7-4,2017,7,4,10,6l46jg,"KDD 2017 | Halifax, Nova Scotia - Canada",https://www.reddit.com/r/deeplearning/comments/6l46jg/kdd_2017_halifax_nova_scotia_canada/,hylihitic,1499133314,,0,3
7,2017-7-4,2017,7,4,23,6l7egc,A fresh start to understanding Neural Nets using Tensorflow,https://www.reddit.com/r/deeplearning/comments/6l7egc/a_fresh_start_to_understanding_neural_nets_using/,raksham97,1499178861,,0,7
8,2017-7-5,2017,7,5,2,6l8j5b,Train your Deep Learning models on the Cloud,https://www.reddit.com/r/deeplearning/comments/6l8j5b/train_your_deep_learning_models_on_the_cloud/,goddamnsteve,1499190248,,0,0
9,2017-7-7,2017,7,7,2,6ln6dv,Build an AI Programmer using Recurrent Neural Network,https://www.reddit.com/r/deeplearning/comments/6ln6dv/build_an_ai_programmer_using_recurrent_neural/,ryanlr,1499360938,,0,1
10,2017-7-7,2017,7,7,13,6lrd8w,Backpropagation Through Time: recurrent neural network training technique,https://www.reddit.com/r/deeplearning/comments/6lrd8w/backpropagation_through_time_recurrent_neural/,sudheeran,1499401963,,0,1
11,2017-7-7,2017,7,7,17,6lse9h,Features and capabilities of Deep Learning,https://www.reddit.com/r/deeplearning/comments/6lse9h/features_and_capabilities_of_deep_learning/,zinerminer,1499417011,"I have here analyzed the concept of deep learning in detail. Any other points/opinions?

teks.co.in/site/blog/artificial-intelligence-3-0-13-things-to-know-about-deep-learning/

",0,0
12,2017-7-9,2017,7,9,1,6m1upj,Introduction to Apache MXNet on AWS (AWS podcast),https://www.reddit.com/r/deeplearning/comments/6m1upj/introduction_to_apache_mxnet_on_aws_aws_podcast/,julsimon,1499530937,,4,2
13,2017-7-9,2017,7,9,13,6m5jzu,A Neural Network in 10 lines of C++ Code,https://www.reddit.com/r/deeplearning/comments/6m5jzu/a_neural_network_in_10_lines_of_c_code/,[deleted],1499573846,[deleted],2,0
14,2017-7-9,2017,7,9,17,6m6elp,I made a parody song about deep learning,https://www.reddit.com/r/deeplearning/comments/6m6elp/i_made_a_parody_song_about_deep_learning/,zephiem,1499588394,,5,6
15,2017-7-10,2017,7,10,9,6mb762,When not to use deep learning,https://www.reddit.com/r/deeplearning/comments/6mb762/when_not_to_use_deep_learning/,_alphamaximus_,1499647978,,2,1
16,2017-7-10,2017,7,10,12,6mbzyb,A comprehensive and organized collection of resources for TensorFlow,https://www.reddit.com/r/deeplearning/comments/6mbzyb/a_comprehensive_and_organized_collection_of/,irsina,1499657774,,0,4
17,2017-7-10,2017,7,10,19,6mdpql,Adding attention mechanism to IMDB dataset using keras,https://www.reddit.com/r/deeplearning/comments/6mdpql/adding_attention_mechanism_to_imdb_dataset_using/,dude_perf3ct,1499684137,"I am trying to add attention mechanism to IMDB dataset which is basically sentiment analysis. The attention mechanism will contain the information as to which word contributed more or less to the sentiment. All the model code and problem are posted on stack overflow and github. Any suggestions?

Github Issue:
https://github.com/fchollet/keras/issues/4962#issuecomment-313859541

Stack Overflow question:
https://stackoverflow.com/questions/44966840/training-loss-and-accuracy-remain-constant-after-adding-attention-mechanism",0,2
18,2017-7-10,2017,7,10,22,6mefhq,MatchNet,https://www.reddit.com/r/deeplearning/comments/6mefhq/matchnet/,burn_in_flames,1499693240,"I wanted to find out what models have replaced/improved on MatchNet for matching two images and getting a classification of matched/not match?

Currently, MatchNet is 2 years old, but I haven't come across many papers of networks which are solving the same task as MatchNet did originally.",1,1
19,2017-7-11,2017,7,11,0,6mf03i,Is there a deep learning program or similar system that can help me do analyses based on knowledge/data it is fed?,https://www.reddit.com/r/deeplearning/comments/6mf03i/is_there_a_deep_learning_program_or_similar/,[deleted],1499699052,[deleted],2,1
20,2017-7-11,2017,7,11,7,6mi4rg,Deep Learning powered Facial Recognition APIs - Have a play around with it :),https://www.reddit.com/r/deeplearning/comments/6mi4rg/deep_learning_powered_facial_recognition_apis/,truefaceAI,1499727303,"https://trueface.ai/
Face Detect
Fact Matching
Identification
Raw Landmark Detection
Picture Attack Detection
We welcome developer involvement and feedback",0,2
21,2017-7-11,2017,7,11,15,6mkcob,New CS224n: Natural Language Processing with Deep Learning,https://www.reddit.com/r/deeplearning/comments/6mkcob/new_cs224n_natural_language_processing_with_deep/,cognitivedemons,1499752903,Stanford has started a new version of [CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/) with both Chris Manning and Richard Socher. Are you looking forward to watching it?,1,11
22,2017-7-11,2017,7,11,17,6mkxbs,I'm a newbie and need a bit of help in deciding whether to install the CPU or the GPU version of tensorflow,https://www.reddit.com/r/deeplearning/comments/6mkxbs/im_a_newbie_and_need_a_bit_of_help_in_deciding/,vallsin,1499761911,So i'm very new to the machine learning and deep learning field and i only recently have been thinking of using tensorflow. Now i have a i7 6700hq processor @2.60hz and a 960m Nvidia graphic card with 4GB Memory. I'm confused whether i should install the Gpu version or the cpu version and which one would ensure faster training times in my case?,16,4
23,2017-7-12,2017,7,12,4,6mogb3,[Question] Feeding raw signal data into a Deep Belief Network in Python?,https://www.reddit.com/r/deeplearning/comments/6mogb3/question_feeding_raw_signal_data_into_a_deep/,eragonngo,1499800303,"I have a question regarding inputting raw signal data into a Deep Belief Network. The nature of input data includes 1,000 signal data of 3 classifications. Each signal data has 6 features and contain 10,000 samples in it. Instead of using statistical feature extraction (which will extract the RMS, mean, max, kurtosis, etc of those signal data) and feed it into a standard Neural Network to do the classification task, I wonder can I feed the Deep Belief Network with 3 hidden layers with X = [1,000 x 10,000; 6] and y = [1,000 ; 1] matrix to do the classification task ?

If it's possible, how can I map X correspond to y?

Thank you all for reading my questions",0,3
24,2017-7-12,2017,7,12,6,6mpab9,Deep Learning with Python and Keras,https://www.reddit.com/r/deeplearning/comments/6mpab9/deep_learning_with_python_and_keras/,vasira,1499807722,,0,2
25,2017-7-12,2017,7,12,9,6mqc24,www.marwane.cf,https://www.reddit.com/r/deeplearning/comments/6mqc24/wwwmarwanecf/,youmarwane,1499818264,,0,1
26,2017-7-12,2017,7,12,10,6mqwrt,Deep learning projects are coming!,https://www.reddit.com/r/deeplearning/comments/6mqwrt/deep_learning_projects_are_coming/,[deleted],1499824697,[deleted],4,3
27,2017-7-12,2017,7,12,14,6mrwan,GPU temperature reads 88 C when training a LSTM on tensorflow. Is it normal (and safe)?,https://www.reddit.com/r/deeplearning/comments/6mrwan/gpu_temperature_reads_88_c_when_training_a_lstm/,dmdmello,1499836795,"I've got a 1 layer LSTM model in tensorflow and the temperature reading of my GPU gets rather high during the training phase. Always varying between 80 C and 90 C. My GPU is a water cooled gtx 1080 ""Super-clocked"" edition in a 24/7 refrigerated room. The model works, but this temperature worries me. I'd like to know if this is normal and safe.

I'm training the LSTM for a next-word-prediction problem with tokenized reddit comments. I got the idea from different tutorials in wildml.com. Here are some details about it:

    Tensorflow 1.2.1, Cuda tk 8.0, Cudnn 6.0, Nvidia Driver 375.66
    My training data consists of 200 K reddit comments.
    My word dictionary consists of 8000 words, which means 8000 classes of classification for each prediction
    I use GLOVE pre-trained 100 Dimensions embeddings of Wikipedia words
    I'm not using placeholders to feed my input. It's all done with TFRecordfiles readers, which input the examples to a 100k capacity random shuffle queue
    From the random shuffle queue, it goes to a padding FIFO queue, where I generated zero-paddaded mini-batches of 20
    The 20 size mini batches go to a tf.dynamic_rnn() with LSTM cell with Hidden dimension of 150
    I mask the losses using tf.sign() and minimize the result with Adam optimizer

I've noticed that the temperature rises a lot when I raise the mini-batch size. 1 size mini-batches (single examples), it reads between 72-75 C. With 10 size mini-batches, it immediately goes to 78 C and stays in the range of 78-84 C. With 20 size mini-batches, 84-88 C. With 30 size mini-batches, 87-92 C.

If I raise the hidden dimension to 200, 250, 300, etc, while maintaining the minibatch size fixed, I also get similar temperature raises.

I've also trained the same model, but feeding the data with placeholders only, i.e, not using TFRecord, Queues and mini-batches. It stays around 65 C, but it's obviously far from optimized and ideal to use placeholders for feeding the net.

I really appreciate your help, I'm kinda desperate, to be honest.
",11,1
28,2017-7-12,2017,7,12,17,6msk8m,"Tensorflow: RNN model works with LSTM cell, but returns NaN with GRU cell",https://www.reddit.com/r/deeplearning/comments/6msk8m/tensorflow_rnn_model_works_with_lstm_cell_but/,dmdmello,1499846904,"Hi.
I've created a simple LSTM model for the problem of prediction of the next word in a phrase and I'm getting a rather strange result when I try the same thing with GRU cell instead. I use dynamic RNN to implement the internal ops of the neural net, and use a LSTM cell with ""State_is_tuple"" set to ""True"" as input. After 50 epochs, with each epoch iterating over the entire dataset, it gets 72 % accuracy. I'm still working on getting a better result, but that's not the point. I'm running the model on a GTX 1080.

However, when I substitute the LSTM cell for a GRU cell, during aprox. 20 epochs of training, something goes wrong and one of the parameters is calculated as NaN, which subsequently makes all other parameters be calculated as NaN too, which ruins the training.

So, what could it be? Could it be a memory issue? I'm almost sure it doesn't have nothing to do with cross entropy, since I use the highlevel function sparse_softmax_cross_entropy_with_logits(), which handles log(0) cases. I really have no idea why it's happening. 

Another important detail is that I'm not using tf.placeholder to feed the input, but a TFRecord reader that inputs data to a 100k capacity Random Shuffle Queue, which in turn inputs examples to a Padding FIFO Queue, which dequeues zero-padded batches of 20 elements to the model. I also use a pre-trained embedding layer. For optimization I use Adam.  ",2,2
29,2017-7-12,2017,7,12,22,6mtrol,A deep learning and reinforcement learning experimentation library on top of tensorflow.,https://www.reddit.com/r/deeplearning/comments/6mtrol/a_deep_learning_and_reinforcement_learning/,pipado,1499864466,,0,11
30,2017-7-13,2017,7,13,1,6mv8qy,[1707.03300] New paper explores how agents can learn from unintentional accomplishments.,https://www.reddit.com/r/deeplearning/comments/6mv8qy/170703300_new_paper_explores_how_agents_can_learn/,cognitivedemons,1499878447,,0,2
31,2017-7-13,2017,7,13,2,6mvaxo,[1707.03141] 1-shot classification: 56.48% accuracy on 5-Way Mini-ImageNet!,https://www.reddit.com/r/deeplearning/comments/6mvaxo/170703141_1shot_classification_5648_accuracy_on/,cognitivedemons,1499878984,,2,4
32,2017-7-13,2017,7,13,12,6mz6cj,"Audio + Video Manipulation Detection: Combatting CycleGAN, Face2Face, etc.",https://www.reddit.com/r/deeplearning/comments/6mz6cj/audio_video_manipulation_detection_combatting/,mhdempsey,1499917778,,1,2
33,2017-7-13,2017,7,13,14,6mzp7v,Mate Labs mixes machine learning with IFTTT,https://www.reddit.com/r/deeplearning/comments/6mzp7v/mate_labs_mixes_machine_learning_with_ifttt/,kailashahirwar12,1499924589,,0,2
34,2017-7-13,2017,7,13,19,6n0qka,Computer Vision News for the Algorithm Community - July,https://www.reddit.com/r/deeplearning/comments/6n0qka/computer_vision_news_for_the_algorithm_community/,Gletta,1499941192,"This is Computer Vision News of July, published by RSIP Vision, with 50 pages of exclusive content about computer vision, artificial intelligence, deep learning and image processing! It's free for everyone to read and subscribe.
This month, don't miss our exclusive review about ""Understanding Googles Transformer"", the new attention unit by Google.
HTML5 version (recommended) ==&gt; http://www.rsipvision.com/ComputerVisionNews-2017July/
and PDF version ==&gt; http://www.rsipvision.com/computer-vision-news-2017-july-pdf/
Enjoy!",0,2
35,2017-7-14,2017,7,14,4,6n42mj,Google's DeepMind AI just taught itself to walk,https://www.reddit.com/r/deeplearning/comments/6n42mj/googles_deepmind_ai_just_taught_itself_to_walk/,[deleted],1499975579,[deleted],1,1
36,2017-7-15,2017,7,15,4,6nb4v1,Data Science-Deep Learning in Python,https://www.reddit.com/r/deeplearning/comments/6nb4v1/data_sciencedeep_learning_in_python/,LocalStar,1500058997,,1,1
37,2017-7-15,2017,7,15,8,6nclz2,Doubt in YOLO implementation,https://www.reddit.com/r/deeplearning/comments/6nclz2/doubt_in_yolo_implementation/,[deleted],1500074046,[deleted],0,1
38,2017-7-15,2017,7,15,13,6ne5p0,Lip Reading  Who Said What? Answered by Deep Learning,https://www.reddit.com/r/deeplearning/comments/6ne5p0/lip_reading_who_said_what_answered_by_deep/,irsina,1500094296,,0,2
39,2017-7-16,2017,7,16,13,6nkj3w,Siamese Networks for One Shot Learning in PyTorch - part 1,https://www.reddit.com/r/deeplearning/comments/6nkj3w/siamese_networks_for_one_shot_learning_in_pytorch/,harvey_slash,1500181081,,0,4
40,2017-7-17,2017,7,17,8,6npfyb,Useful Data Science Resources &amp; Recommended Study Routes,https://www.reddit.com/r/deeplearning/comments/6npfyb/useful_data_science_resources_recommended_study/,datasciencelover,1500247740,,0,6
41,2017-7-17,2017,7,17,14,6nr5h5,Practical Deep Learning on Images and Text.,https://www.reddit.com/r/deeplearning/comments/6nr5h5/practical_deep_learning_on_images_and_text/,snlpatel001213,1500269450,,0,4
42,2017-7-17,2017,7,17,17,6nrs93,Debugging &amp; Visualising training of Neural Network with TensorBoard,https://www.reddit.com/r/deeplearning/comments/6nrs93/debugging_visualising_training_of_neural_network/,jalFaizy,1500279816,,0,4
43,2017-7-18,2017,7,18,4,6nvdzr,Choice of Biometric Identification/Authentification,https://www.reddit.com/r/deeplearning/comments/6nvdzr/choice_of_biometric_identificationauthentification/,truefaceAI,1500319575,"If you are required to give your biometric information, are you more comfortable with iris, finger print, or facial recognition?",5,1
44,2017-7-18,2017,7,18,15,6nyxam,Logical and Lucid way to learn Machine Learning.,https://www.reddit.com/r/deeplearning/comments/6nyxam/logical_and_lucid_way_to_learn_machine_learning/,snlpatel001213,1500357792,,2,5
45,2017-7-19,2017,7,19,0,6o1hiv,AI Co-Pilot: RNNs for Dynamic Facial Analysis,https://www.reddit.com/r/deeplearning/comments/6o1hiv/ai_copilot_rnns_for_dynamic_facial_analysis/,harrism,1500391818,,1,5
46,2017-7-19,2017,7,19,3,6o2ps4,SSD Object detection,https://www.reddit.com/r/deeplearning/comments/6o2ps4/ssd_object_detection/,sirLuckyLuke,1500402641,"Hey folks,

I am currently trying to build a door detector with the singleshot multibox detector. I have roundabout 3k images with labeled doors from imagenet. Training is fine and I get up to 75 percent mAP on my test set.       But if I put ""real world"" images through my model I get many wrong detections. Especially Square shaped objects like computer monitors are labeled as doors with a confidence near 100 percent. Further  the detector run in trouble with Windows and so on...

Have someone an idea how to get rid of these miss detection without labeling a bunch of ""anti classes"" or fine tune my model with domain specific images?
",1,1
47,2017-7-19,2017,7,19,17,6o73a2,The future of deep learning,https://www.reddit.com/r/deeplearning/comments/6o73a2/the_future_of_deep_learning/,cloudgentleman,1500451803,,0,10
48,2017-7-20,2017,7,20,22,6og6l3,Facial Similarity with Siamese Networks in PyTorch  Hacker Noon,https://www.reddit.com/r/deeplearning/comments/6og6l3/facial_similarity_with_siamese_networks_in/,harvey_slash,1500556837,,0,2
49,2017-7-20,2017,7,20,23,6ogfrm,How the Leading Stationery Company Optimized Their Sales Strategies,https://www.reddit.com/r/deeplearning/comments/6ogfrm/how_the_leading_stationery_company_optimized/,cevizligizem,1500559566,https://www.exastax.com/case-studies/how-the-leading-stationery-company-optimized-their-sales-strategies/,1,0
50,2017-7-21,2017,7,21,0,6ogzuh,"I started a new sub dedicated to the intersection between connected device, time-series data and machine learning. Your thoughts?",https://www.reddit.com/r/deeplearning/comments/6ogzuh/i_started_a_new_sub_dedicated_to_the_intersection/,onegazillion,1500564847,,2,6
51,2017-7-21,2017,7,21,15,6om5q8,"When training a Deep Learning model, does it matter if I input examples of my dataset randomly or in a order?",https://www.reddit.com/r/deeplearning/comments/6om5q8/when_training_a_deep_learning_model_does_it/,dmdmello,1500618097,"To be more specific, Im training a LSTM to word prediction given a initial sequence of words, and my dataset is 200k reddit comments. Does it matter if I randomly feed the examples one at a time (allowing repeated inputs) or if I feed them in a sequence (not allowing repetitions)?",2,0
52,2017-7-21,2017,7,21,22,6onvaq,Very hard to work around continuous latent code in InfoGAN,https://www.reddit.com/r/deeplearning/comments/6onvaq/very_hard_to_work_around_continuous_latent_code/,kudo1026,1500643372,"It is always a pleasure to work with InfoGAN as it can unsupervisedly decode the structure of the data. The discrete latent worked very well for me to categorize different trajectories. However, I've been a little frustrated recently as I'm trying to implement the continuous latent code.

I first tried the mnist example given by the original InfoGAN paper, and it worked fine as the first continuous latent code represents the leaning angle and the second continuous latent represents the width. However, when I tried it on my own toy data sets,(something like
[first][1], [second][2]) nothing really works out. I expect to see continuous variation on my output, but they are instead completely random, like the latent code doesn't work at all.

I use convolutional and deconvolutional networks to discriminate and generate samples and follow the same manner for the discrete latent code as in the original openAI implementation. My treatment towards the continuous latent code is a little different, I have my discriminating network output continuous output with the same dimension as the input continuous latent code, and then try to minimize the MSE of the two. On the contrary, openaAI was modelling the code as a Gaussian distribution, and calculate its distance based on the distribution property.


    loss_c_cont = tf.reduce_mean(tf.reduce_sum(tf.square(c_cont_fake - c_cont), 1))


I think about it for a while, but don't think it could cause a huge difference as the gaussian distribution distance also mainly takes into account the square of epsilon = (x_var - mean).

    return tf.reduce_sum(
            - 0.5 * np.log(2 * np.pi) - tf.log(stddev + TINY) - 0.5 * tf.square(epsilon),
            reduction_indices=1,
        )

So my question would be: is this really the problem that caused all the problems? Or is there anything I need to pay more attention to but I did not notice?

Or in a more general sense, is there anything special about what kind of characteristics of the data can be represented by the continuous latent code? Is there any limitations to the representation capability of the continuous latent code(translation or rotation)? Can anyone share their experience about what kind of features they represented using the continuous latent code except for the examples given by the openAI paper?

I'd really welcome all your thoughts! Thanks!

P.S. Another thing I noticed was when I try to use a 5-category discrete code(in one-hot encoding) and 1-dim continuous code, it will take 5 digit for the discrete code and only 1 digit for the continuous code in the feeding input noise to the generator. Is it somehow unbalanced? Each of these code actually just represents one characteristic of the dataset. In mnist dataset, it could either number, width or rotation angle. Can this be a problem?


  [1]: https://i.stack.imgur.com/YXAMm.jpg
  [2]: https://i.stack.imgur.com/WNRTM.jpg
",4,2
53,2017-7-22,2017,7,22,0,6oopcu,Deep Learning for Automated Driving with MATLAB,https://www.reddit.com/r/deeplearning/comments/6oopcu/deep_learning_for_automated_driving_with_matlab/,harrism,1500651682,,0,9
54,2017-7-22,2017,7,22,4,6oq68u,"Fujitsu Enters Deep Learning, AI Markets With Custom Architecture",https://www.reddit.com/r/deeplearning/comments/6oq68u/fujitsu_enters_deep_learning_ai_markets_with/,chlordane2501,1500664833,,0,1
55,2017-7-22,2017,7,22,12,6osuot,How do you version control your neural net? [x-post from /r/MachineLearning],https://www.reddit.com/r/deeplearning/comments/6osuot/how_do_you_version_control_your_neural_net_xpost/,thumbsdrivesmecrazy,1500693985,,0,4
56,2017-7-23,2017,7,23,4,6owu77,Why would a pre trained word embedding like 'Glove' don't work with a GRU model for word prediction?,https://www.reddit.com/r/deeplearning/comments/6owu77/why_would_a_pre_trained_word_embedding_like_glove/,dmdmello,1500751146,"Its a GRU for predicting the next word given an initial sequence of words. Dataset is 200k reddit comments. Each comment is an example. Ive used glove 50 dim word embeddings pretrained on 6 billion wikipedia occurrences. I have not created it. Ive downloaded it from Gloves web site. The net trains and minimizes the loss, but strangely performs better with a random matrix instead of the embedding.

My model is supposed to receive a reddit comment, input it to a GRU and learn sequential patterns of words from it. It tries to predict the next word given the first word of the comment, and then the 3rd word given the first two, then the 4th given the first three and so on. Each example = one comment. This model is well explained on Wildml.com, from where I got it

The length of my dictionary of words is 8000. So each word is an index, and I use it to index a column of the embedding matrix, which has dims of  8000 x 50 (50 being the dim of the Glove embedding) .

Hidden dims of my GRU is 350. Ive tried 120, 150, 200, 250, results are almost the same. I'm minimizing the results for the sparse cross entropy with softmax of logits. I use Adam as optimizer, clipping the gradients between -1e16 and 1e16, and I have tried learning rates of 0.001 (default) 0.0001 and 0.00001.

Its all being done with tensorflow and Im sure everything is being done right. But the result on the training set is not doing well. With embedding the best accuracy of prediction when the lowest loss is achieved is always something like 25%. With random matrix instead of Glove's embedding,  I get 30 %.

IMPORTANT: All is being done considering the embedding matrix is not trainable (i.e, I set the flags trainable=FALSE) so tensorflow wont change it during BP. Ive done that both for Glove embedding and for the Random embedding.

I really appreciate your help.





",4,5
57,2017-7-23,2017,7,23,17,6p08t7,The most popular deep learning libraries - code(love),https://www.reddit.com/r/deeplearning/comments/6p08t7/the_most_popular_deep_learning_libraries_codelove/,pmz,1500797206,,2,1
58,2017-7-23,2017,7,23,20,6p0qxu,2 Deep Learning for Enterprise Workshops in New York This Week,https://www.reddit.com/r/deeplearning/comments/6p0qxu/2_deep_learning_for_enterprise_workshops_in_new/,tfzb,1500807651,,1,2
59,2017-7-24,2017,7,24,1,6p2dac,Install Keras + tensorflow-gpu with a NVidia Card,https://www.reddit.com/r/deeplearning/comments/6p2dac/install_keras_tensorflowgpu_with_a_nvidia_card/,nostub,1500828941,,2,0
60,2017-7-24,2017,7,24,4,6p36u0,http://www.thegeeklegacy.com/t/install-keras-tensorflow-gpu-with-a-nvidia-card/101/new/,https://www.reddit.com/r/deeplearning/comments/6p36u0/httpwwwthegeeklegacycomtinstallkerastensorflowgpuw/,nostub,1500837107,,0,0
61,2017-7-24,2017,7,24,5,6p3saz,"[NSFW] 50,000 tasteful nudes with neural network search",https://www.reddit.com/r/deeplearning/comments/6p3saz/nsfw_50000_tasteful_nudes_with_neural_network/,driftwheeler,1500843082,"http://driftwheeler.com

Citation: Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks (https://arxiv.org/abs/1406.6909)

50,000 MetArt-style nudes. No banners, no ads, no hassles, no distractions. Smart image zoom to fit the woman to the screen.

See random pics (WANDER). When you like one, see the photoshoot on repeat (TRANCE).

To search for something-- e.g., grass, beach, face, pussy-- long press the image (DREAM). This finds nearest neighbors in deep feature space, but only works reliably for simple concepts because it's fully unsupervised. The features you selected by long pressing (inside the box that appears) are more likely, but only very simple searches are ""pure"".

For example, long press a close-up pussy to see more pussies... find one you like, then press TRANCE to see the rest of her. Long press sand and water to see more girls on the beach. Long press forest greenery to see more girls in the forest. Long press a close-up face to see similar faces... And so on.

If this is not familiar to you, take a look at: http://cs.stanford.edu/people/karpathy/cnnembed/ Consider one of the big ""maps"" on that webpage. Notice how the tiny image patches clustered together in a map tend to be similar to each other. When you long press the image in Melondream, a box appears. That box is like one of the tiny image patches. Melondream's DREAM shows you images having patches near the patch you selected, in Melondream's map. Also notice how impure even a supervised dream would be.

This is an Android app. You may ask whether it's safe. Our reply:

1. We wrote it and we're telling you it's clean. And awesome.

2. Two independent security teams, one at The Register and and one at Wired, vetted the APK before publishing. In their opinion, it's clean and safe.

3. It requires no Android system permissions. It runs in the Android sandbox, with Android's maximum security. If you don't understand what this means, please see: https://developer.android.com/training/permissions/index.html

4. Melondream has thousands of regular users, and has been installed on tens of thousands of Android devices. No one complains and no one has problems.

Enjoy!",1,1
62,2017-7-24,2017,7,24,10,6p5f3a,cosine similarity can be object function for deep learning?,https://www.reddit.com/r/deeplearning/comments/6p5f3a/cosine_similarity_can_be_object_function_for_deep/,[deleted],1500861149,[deleted],1,2
63,2017-7-24,2017,7,24,23,6p8wfv,5 Free Resources for Getting Started with Deep Learning for Natural Language Processing,https://www.reddit.com/r/deeplearning/comments/6p8wfv/5_free_resources_for_getting_started_with_deep/,lalypopa123,1500908302,,0,1
64,2017-7-25,2017,7,25,0,6p98hb,Why deep learning isnt always the best AI solution,https://www.reddit.com/r/deeplearning/comments/6p98hb/why_deep_learning_isnt_always_the_best_ai_solution/,jonfla,1500911379,,2,1
65,2017-7-25,2017,7,25,2,6p9sfj,JetPack 3.1 Doubles Jetsons Low-Latency Inference Performance,https://www.reddit.com/r/deeplearning/comments/6p9sfj/jetpack_31_doubles_jetsons_lowlatency_inference/,harrism,1500916294,,0,4
66,2017-7-25,2017,7,25,5,6pb3ko,Intel Movidius Neural Compute Stick - On Mouser  r/iotml,https://www.reddit.com/r/deeplearning/comments/6pb3ko/intel_movidius_neural_compute_stick_on_mouser/,onegazillion,1500927702,,0,2
67,2017-7-25,2017,7,25,17,6peuym,37 ways to debug a DL model,https://www.reddit.com/r/deeplearning/comments/6peuym/37_ways_to_debug_a_dl_model/,[deleted],1500971666,[deleted],0,0
68,2017-7-25,2017,7,25,21,6pfth0,5 Reasons to Consider AI Automation for Banking,https://www.reddit.com/r/deeplearning/comments/6pfth0/5_reasons_to_consider_ai_automation_for_banking/,cevizligizem,1500985940,,0,0
69,2017-7-26,2017,7,26,2,6phpg3,Dimensionality reduction,https://www.reddit.com/r/deeplearning/comments/6phpg3/dimensionality_reduction/,dgl85,1501003729,"Hello, does anybody have experience in using CNN's or Autoencoders for dimensionality reduction? I need to reduce the dimension of my features and I thought of stacking a CNN or an autoencoder over a MLP or LSTM in order to do so. Any thoughts on which would be more appropriate?",2,3
70,2017-7-26,2017,7,26,3,6pi176,Squeeze Net Download,https://www.reddit.com/r/deeplearning/comments/6pi176/squeeze_net_download/,DecentMakeover,1501006499,"Has anyone tried downloading SqueezeNet-
they have mentioned that they have provided us a pretrained network,but im wondering do i need to have the ImageNet data downloaded for me to use the model or two train or install it??",0,0
71,2017-7-26,2017,7,26,13,6plr2e,"Are the Videos of Deep Learning Summer School, Montreal 2017 avaiable either in youtube or videolectures.net or facebook live?",https://www.reddit.com/r/deeplearning/comments/6plr2e/are_the_videos_of_deep_learning_summer_school/,rnnandi,1501043611,"Deep Learning Summer School, Montreal 2017 took place recently .
But I don't find the recorded videos in Internet. Is it available?",2,25
72,2017-7-26,2017,7,26,22,6pnwrq,CAN (Creative Adversarial Network),https://www.reddit.com/r/deeplearning/comments/6pnwrq/can_creative_adversarial_network/,lalypopa123,1501074575,,0,1
73,2017-7-26,2017,7,26,22,6po7ry,"Medical Image Analysis with Deep Learning , Part 4",https://www.reddit.com/r/deeplearning/comments/6po7ry/medical_image_analysis_with_deep_learning_part_4/,digitalson,1501077592,,0,6
74,2017-7-26,2017,7,26,23,6po8tg,Occupancy detection in the office by analyzing surveillance videos and its application to building energy conservation  r/iotml,https://www.reddit.com/r/deeplearning/comments/6po8tg/occupancy_detection_in_the_office_by_analyzing/,onegazillion,1501077860,,0,1
75,2017-7-27,2017,7,27,0,6poxrr,"Deep Learning Zero to One: 5 Awe-Inspiring Demos with Code for Beginners, part 2",https://www.reddit.com/r/deeplearning/comments/6poxrr/deep_learning_zero_to_one_5_aweinspiring_demos/,jackblun,1501084212,,0,0
76,2017-7-27,2017,7,27,1,6pp82q,Any good project ideas in Keras easy to implement?,https://www.reddit.com/r/deeplearning/comments/6pp82q/any_good_project_ideas_in_keras_easy_to_implement/,waverick,1501086702,"I want to practice in Keras/Tensorflow, do you have some project ideas for training?
",5,3
77,2017-7-27,2017,7,27,9,6psbir,A collection of best practices for using neural networks in Natural Language Processing,https://www.reddit.com/r/deeplearning/comments/6psbir/a_collection_of_best_practices_for_using_neural/,cognitivedemons,1501114762,,0,11
78,2017-7-27,2017,7,27,16,6pudtu,training - no. images vs no. objects in images,https://www.reddit.com/r/deeplearning/comments/6pudtu/training_no_images_vs_no_objects_in_images/,c94jk,1501140540,"I'm training an object detection network on a currently small dataset of 1000 images. In each image however there are on average 10-15 objects, sometimes more, sometimes less. Is the number of individual images an isolated factor, or does having relatively densely distributed objects help?

For reference I am getting an mAP or 80-90% using SSD on my dataset.",0,1
79,2017-7-27,2017,7,27,18,6pur7m,Using Deep Learning to find genetic causes of conditions such as Autism,https://www.reddit.com/r/deeplearning/comments/6pur7m/using_deep_learning_to_find_genetic_causes_of/,artificialbrainxyz,1501146624,,0,5
80,2017-7-27,2017,7,27,22,6pvvmh,How to train the RPN in Faster R CNN?,https://www.reddit.com/r/deeplearning/comments/6pvvmh/how_to_train_the_rpn_in_faster_r_cnn/,mimi_the_kid,1501161781,"[Link to paper](https://arxiv.org/abs/1506.01497)


I'm trying to understand the region proposal network in faster rcnn. I understand what it's doing, but I still don't understand how training exactly works, especially the details.

Let's assume we're using VGG16's last layer with shape 14x14x512 (before maxpool and with 228x228 images) and k=9 different anchors. At inference time I want to predict 9*2 class labels and 9*4 bounding box coordinates. My intermediate layer is a 512 dimensional vector.
(image shows 256 from ZF network)
[![from the paper][1]][1]


  [1]: https://i.stack.imgur.com/sK37S.png

In the paper they write 

&gt; ""we randomly sample 256 anchors in an image to compute the loss
&gt; function of a mini-batch, where the sampled positive and negative
&gt; anchors have a ratio of up to 1:1""

That's the part I'm not sure about. __Does this mean that for each one of the 9(k) anchor types the particular classifier and regressor are trained with minibatches that only contain positive and negative anchors of that type?__

Such that I basically train k different networks with shared weights in the intermediate layer? Therefore each minibatch would consist of the training data x=the 3x3x512 sliding window of the conv feature map and y=the ground truth for that specific anchor type.
And at inference time I put them all together.

I appreciate your help.",0,3
81,2017-7-27,2017,7,27,23,6pw8k3,Applying Deep Learning to Real-world Problems,https://www.reddit.com/r/deeplearning/comments/6pw8k3/applying_deep_learning_to_realworld_problems/,friscotime,1501165271,,0,12
82,2017-7-27,2017,7,27,23,6pw90h,Question: Can a mining rig be used for deep learning?,https://www.reddit.com/r/deeplearning/comments/6pw90h/question_can_a_mining_rig_be_used_for_deep/,snapo84,1501165389,"I found a kinda cheap mining rig with 6xGTX1070, but they are all connected with something like pcie 1x ... is this feasible to use it with for example tensorflow or not? can cudnn run with pcie 1x? or does it requrie 8x like in the specifications?",3,3
83,2017-7-27,2017,7,27,23,6pwez7,Using the TensorFlow API: An Introductory Tutorial Series,https://www.reddit.com/r/deeplearning/comments/6pwez7/using_the_tensorflow_api_an_introductory_tutorial/,lalypopa123,1501166971,,0,1
84,2017-7-28,2017,7,28,3,6pxqsf,Identifying Traffic Signs with Deep Learning,https://www.reddit.com/r/deeplearning/comments/6pxqsf/identifying_traffic_signs_with_deep_learning/,psangrene,1501178491,,0,5
85,2017-7-28,2017,7,28,8,6pzzki,"GANs, more than pretty pictures?",https://www.reddit.com/r/deeplearning/comments/6pzzki/gans_more_than_pretty_pictures/,pderuxx,1501198791,"Goodfellow done changed the game with GANs, or course. Generative modeling is intellectually more interesting that discriminative modeling, IMHO, as these issues relate to fundamental questions in AI and ultimately Philosophy of Mind. BUT, do GAN-based models have a place in *business* contexts?

A simple example is that of **semi-supervised GANs**, which can perform state-of-the-art classification on eg SVHN while a mere 1% of training data has a class label (the rest are only labeled ""real"" v ""fake"").

Any more?",4,1
86,2017-7-28,2017,7,28,16,6q2e98,Prediction pixels using RNN,https://www.reddit.com/r/deeplearning/comments/6q2e98/prediction_pixels_using_rnn/,pjavia,1501228661,"How to predict pixel values from previous pixels? Besides Pixel RNN is there any causal system that can achieve the same. Also, Is there a simpler way to do that? I tried using RNN but it heavily overfits. The loss does not decrease it oscillates even after decreasing learning rate. Any idea why that is happening?
Thank you",0,3
87,2017-7-28,2017,7,28,20,6q3877,Deep Learning with R + Keras,https://www.reddit.com/r/deeplearning/comments/6q3877/deep_learning_with_r_keras/,lalypopa123,1501241842,,0,1
88,2017-7-28,2017,7,28,20,6q3av8,Deep Learning Zero to One: 5 Awe-Inspiring Demos with Code for Beginners,https://www.reddit.com/r/deeplearning/comments/6q3av8/deep_learning_zero_to_one_5_aweinspiring_demos/,jackblun,1501242889,,0,7
89,2017-7-28,2017,7,28,21,6q3c1y,Taxonomy of Methods for Deep Meta Learning,https://www.reddit.com/r/deeplearning/comments/6q3c1y/taxonomy_of_methods_for_deep_meta_learning/,digitalson,1501243344,,0,9
90,2017-7-29,2017,7,29,12,6q8n8y,Traning GAN from scratch by using pytorch?,https://www.reddit.com/r/deeplearning/comments/6q8n8y/traning_gan_from_scratch_by_using_pytorch/,Alirezag,1501297366,"Can you anyone suggest any good tutorial for training GAN from scratch by using pytorch, please?
Thanks",0,3
91,2017-7-29,2017,7,29,18,6qa1x3,Pixel RNN - Row LSTM explanation in simplest manner possible? https://arxiv.org/abs/1601.06759,https://www.reddit.com/r/deeplearning/comments/6qa1x3/pixel_rnn_row_lstm_explanation_in_simplest_manner/,pjavia,1501319659,"Q-1 Row LSTM?
--&gt; Is this correct  sigmoid(Kss*h_i-1 + Kis*xi) --- (1)
or you meant to say sigmoid(Wh . Kss*h_i-1 + Wx . Ksi*x_i + b_x + b_h) ___ (2)

If authors meant (1) then how do they explain the hidden units = 512 in 4 layers for input size 64. If the authors meant (2) and are increasing hidden units what is the kernel size for the next layer?
Also, is Row LSTM described in the paper is same as ConvLSTM?

Q-2 Triangular context in ROW LSTM?

If padding is used during convolution and k=3, the output feature size is same as input, then how context is lost? 

Is training possible on single GPU or it requires multi GPU environment?

I would be more than grateful to receive any advice on this.",1,5
92,2017-7-30,2017,7,30,17,6qgeay,Breaking down the process of deep learning into components,https://www.reddit.com/r/deeplearning/comments/6qgeay/breaking_down_the_process_of_deep_learning_into/,maxtheman45,1501403950,"Hi everyone, new to this sub. Doing some research around the complexity of deep learning and was hoping to get some feedback on the process I've created. As I am not technical, I'm sure I'm missing some aspects and I would love to get as granular as possible. Thanks in advance for your help. 

1.	Determine how much training data is required
2.	Decide how to acquire the training data
3.	Decide to labeling the data post-training vs. pre-training &amp; post-training
4.	Deciding which framework to use
5.	Which CNN to run the data through 
6.	Defining the desired parameters of the CNN
7.	Configuring the parameters of the CNN
8.	Which computational environment to use to train the model
a.	Which type of server (IBM, Intel, Nvidia, etc)  either cloud or on-prem
9.	How to build an application that leverages one or more trained models
",0,4
93,2017-7-30,2017,7,30,19,6qgos9,How to get a free environment to run my deep code?,https://www.reddit.com/r/deeplearning/comments/6qgos9/how_to_get_a_free_environment_to_run_my_deep_code/,daicoolb,1501410031,"I want to run the code in GPU. But our laboratory does not have any support to achieve it. Can anyone give me some advice how to solve it? I have applied in google cloud and amazon service, but they only support for CPU.",11,6
94,2017-7-31,2017,7,31,10,6qlel8,How do people gather training images for Deep Learning?,https://www.reddit.com/r/deeplearning/comments/6qlel8/how_do_people_gather_training_images_for_deep/,Facial_Tissue,1501465758,"I hope you guys are having a good weekend.

I am currently working on a image classification part of deep learning and I am writing this to ask you guys about the gathering process of images for training.

I know that to train my network, I need to gather a lot of images for certain object. Usually, I did this with google image search and such, but I realized that when it comes to a certain object that doesn't have many useful images, then I had to roll up my sleeves to take photos of that object... which was darn tedious and time consuming to gather every possible angle, tilt, brightness and such for perfect image classification result.

Long story short, I haven't heard or found any specific image tool yet... but deep within the corners of my mind I just know that such tool exist for deep learning training process and I want to find it so badly.",1,1
95,2017-7-31,2017,7,31,11,6qlhdq,Robust Physical-World Attacks on Machine Learning Models - paper,https://www.reddit.com/r/deeplearning/comments/6qlhdq/robust_physicalworld_attacks_on_machine_learning/,dattgoswami,1501466756,,0,4
96,2017-7-31,2017,7,31,13,6qm7oq,Are ImageNet pre-trained models good for logo recognition?,https://www.reddit.com/r/deeplearning/comments/6qm7oq/are_imagenet_pretrained_models_good_for_logo/,brakaza,1501476008,"Hi there. I'm new to deep learning. I'm starting a project, where I have a dataset of logos and I want to find most similar logos to the one being uploaded by user.  
Is it a good idea to use ImageNet pre-trained models for this task? Or are there any better options?",2,1
97,2017-7-31,2017,7,31,18,6qnfsd,Artificial Neural Networks Tutorial - A Complete Guide,https://www.reddit.com/r/deeplearning/comments/6qnfsd/artificial_neural_networks_tutorial_a_complete/,pooja_edureka,1501495147,,0,3
98,2017-7-31,2017,7,31,23,6qoxs6,Time difference dual core cpu vs gtx 10 series,https://www.reddit.com/r/deeplearning/comments/6qoxs6/time_difference_dual_core_cpu_vs_gtx_10_series/,angularion,1501513080,"Im planning to get a new laptop for computer science.
I have just started learning tensorflow and found that it has gpu support.
So would Dell XPS 15 with GTX 1050 be a better fit than the XPS 13 as a laptop for a beginner?
Note that whatever I get will be the only computer I have and I prefer something portable hence considering the XPS line",5,3
