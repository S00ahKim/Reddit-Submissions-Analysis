,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2017-8-1,2017,8,1,21,6qvrvc,Understanding Deep Learning Requires Re-thinking Generalization,https://www.reddit.com/r/deeplearning/comments/6qvrvc/understanding_deep_learning_requires_rethinking/,digitalson,1501589242,,0,1
1,2017-8-1,2017,8,1,21,6qvsxy,"Medical Image Analysis with Deep Learning , Part 3",https://www.reddit.com/r/deeplearning/comments/6qvsxy/medical_image_analysis_with_deep_learning_part_3/,friscotime,1501589596,,0,1
2,2017-8-2,2017,8,2,20,6r3lit,The Difference Between AI and Machine Learning,https://www.reddit.com/r/deeplearning/comments/6r3lit/the_difference_between_ai_and_machine_learning/,cevizligizem,1501674235,,0,0
3,2017-8-3,2017,8,3,0,6r4u6p,Deep Learning: TensorFlow Programming via XML and PMML,https://www.reddit.com/r/deeplearning/comments/6r4u6p/deep_learning_tensorflow_programming_via_xml_and/,magneticono,1501686891,,0,1
4,2017-8-3,2017,8,3,2,6r5ws5,Convolutional Networks for High Resolution Images,https://www.reddit.com/r/deeplearning/comments/6r5ws5/convolutional_networks_for_high_resolution_images/,waxymcrivers,1501695928,"Can anyone recommend a technique for predicting/classifying on high resolution (2000x2000 pixel) images?

I'm aware of the aggressive down-sampling technique that allows normal conv networks to handle these images by reducing computational complexity, but the images I'm working with are unlike medical images with high detail and many features. My images are high frequency with subtle features; every bit of detail counts.

I had a thought of possibly breaking the image into sub patches and feeding each patch into an LSTM, so that the patches are fed in as if they were frames of a video and each prediction feeds into the next. Not sure if it would work as I imagine it might.",9,3
5,2017-8-3,2017,8,3,6,6r7hgg,"I have a working LSTM that learns to predict the next character in a text, now struggling to make it produce its own text",https://www.reddit.com/r/deeplearning/comments/6r7hgg/i_have_a_working_lstm_that_learns_to_predict_the/,JustinQueeber,1501709265,"I have just recently begun studying Deep Learning and also have no previous experience with TensorFlow. I am very confident in my understanding of the theoretical side of Neural Networks and have written a couple of very simple RNNs from scratch. I have now started to become familiar with using TensorFlow and have been trying to implement slightly more advanced NNs using it.

Currently, I am writing an LSTM that reads sequences of characters from a text, character-by-character, and then (hopefully) learns to accurately predict the next character. I encoded each character in the vocab as a one-hot vector of length equal to the size of the vocab, and the 1 represents the key to that character in my `int : char` map. The model takes as input `n` batches (100 in the example) of `m` character input and label sequences (25 in the example). These input sequences are just any random sequence of `m` characters from the first 80% of the text, the labels are the same sequences, but shifted one character forward. The last 20% is then used for validating the model's accuracy after it has been trained.

It can train itself to over 90% accuracy over 10,000 iterations of 100 batches of 25 character sequences, and and slightly lower level of accuracy on the unseen 20%  of the text (unless there are errors in my model somewhere).

I am now attempting to allow my model to generate its own text after it has been trained. You will see my attempt at the bottom of my code. This is completely wrong as it is still takes in sequences of length 25, predicts the next 25 characters, and then uses this sequence of 25 predicted characters as the next input. Therefore, each time it is still only shifting along by one character and so each sequence it produces is nearly identical.

In order to correct this, I need my model to still train itself with the `m` length sequences, but then in the generating stage it must just take one character as input, predict the next character, and then use this predicted character as the next input. To do this, I will need to adjust my TensorFlow graph so that the `sequenceLength` variable used throughout is implemented using a `placeholder` instead of a fixed integer. That way, I will be able to set `sequenceLength` to 25 when training, and 1 when generating.

I did something similar with the `batchSz` placeholder, but I cannot seem to reproduce something similar for `sequenceLength`.

    from __future__ import print_function
    import tensorflow as tf
    from tensorflow.contrib import rnn
    import numpy as np
    import random

    text = open(""/home/kev/Documents/NeuralNetworks/CharacterPredicter/input.txt"", 'r').read()
    #text = ""abcdefghi""
    vocab = list(set(text))
    textSize = len(text)
    vocabSize = len(vocab)
    testingStartChar = int(textSize * 0.8)

    charToInt = {char : i for i, char in enumerate(vocab)}
    intToChar = {i : char for i, char in enumerate(vocab)}

    sequenceLength = 25  # Must be strictly < (textSize / 5)
    trainingIterations = 10000
    batchSize = 100
    numTestingSequences = int((textSize - testingStartChar - 1) / sequenceLength)

    hiddenDimension = 100
    learningRate = 0.01
    forgetRate = 1.0
    printStep = 1000

    # Returns two 3D arrays of shape (batchSize x seqLength x vocabSize)
    def generateData(batchSize, seqLength, isTraining):
        inputs = []                                                 # e.g.  a, b = generateData(2, 3, True)
        labels = []
        charPointer = testingStartChar                              #       a -> [[[0, 1, ..., 0], [1, 0, ..., 0], [0, 0, ..., 1]]
        for _ in range(batchSize):                                  #             [[1, 0, ..., 0], [0, 0, ..., 1], [0, 1, ..., 0]]]
            inputSequence = []
            labelSequence = []
            if isTraining:
                charPointer = random.randint(0, testingStartChar - seqLength)
            for _ in range(seqLength):
                oneHotInput = charToOneHot(text[charPointer])
                oneHotLabel = charToOneHot(text[charPointer + 1])
                inputSequence.append(oneHotInput)
                labelSequence.append(oneHotLabel)
                charPointer += 1
            inputs.append(inputSequence)
            labels.append(labelSequence)
        return inputs, labels

    # Takes in a single character and returns its corresponding
    # one-hot vector of length vocabSize
    def charToOneHot(char):
        oneHot = [0] * vocabSize
        oneHot[charToInt[char]] = 1
        return oneHot

    # Takes in a one-hot vector and returns its corresponding char
    def oneHotToChar(oneHot):
        charIndex = np.argmax(oneHot)
        return intToChar[charIndex]

    # Takes in a 3D array (last axis is one-hot vectors) and returns
    # an array of the corresponding strings per row (batchSize)
    # (Used for printing outputs)
    def oneHotsToChars(array):                                          # e.g. [[[0, 1, ..., 0], [1, 0, ..., 0], [0, 0, ..., 1]],
        chars = []                                                      #       [[1, 0, ..., 0], [0, 0, ..., 1], [0, 1, ..., 0]]]
        for batchIndex in range(len(array)):
            batchChars = """"                                             # ->   ['hpb',
            for oneHotIndex in range(len(array[batchIndex])):           #      'pbh']
                batchChars += oneHotToChar(array[batchIndex][oneHotIndex])
            chars.append(batchChars)
        return chars

    # Takes a 3D array of real numbers (batchSize x sequenceLength x vocabSize)
    # and returns a 3D array with the last axis being one-hot vectors
    # corresponding to the real number of the highest value
    def predictionToOneHots(prediction):
        oneHots = []
        for batchIndex in range(len(prediction)):
            batchOneHots = []
            for charArrayIndex in range(len(prediction[batchIndex])):
                charArray = prediction[batchIndex][charArrayIndex]
                charOneHot = [0] * len(charArray)
                charIndex = np.argmax(charArray)
                charOneHot[charIndex] = 1
                batchOneHots.append(charOneHot)
            oneHots.append(batchOneHots)
        return oneHots

    batchSz = tf.placeholder(tf.int32, shape=())
    x = tf.placeholder(tf.float32, [None, sequenceLength, vocabSize])
    y = tf.placeholder(tf.float32, [None, sequenceLength, vocabSize])
    xFlat = tf.contrib.layers.flatten(x)                                                # [batchSize, sequenceLength*vocabSize]

    W = tf.Variable(tf.random_normal([hiddenDimension, sequenceLength, vocabSize]))
    b = tf.Variable(tf.random_normal([1, sequenceLength, vocabSize]))
    WFlat = tf.contrib.layers.flatten(W)                                                # [hiddenDimension, sequenceLength*vocabSize]
    bFlat = tf.contrib.layers.flatten(b)                                                # [1, sequenceLength*vocabSize]

    cell = rnn.BasicLSTMCell(hiddenDimension, forget_bias=forgetRate)
    outputs, states = tf.nn.static_rnn(cell, [xFlat], dtype=tf.float32)                 # outputs    = [[batchSize, hiddenDimension]]
    predictionFlat = tf.add(tf.matmul(outputs[0], WFlat), bFlat)                        # outputs[0] = [batchSize, hiddenDimension]
    prediction = tf.reshape(predictionFlat, [batchSz, sequenceLength, vocabSize])

    # 2D array corresponding to whether character per sequence per batch was predicted correctly
    # A correct prediction is when the highest predicted value is the same index as the 1 of the one-hot label
    correctPrediction = tf.equal(tf.argmax(prediction, axis=2), tf.argmax(y, axis=2))   # [batchSize, sequenceLength]
    accuracy = tf.reduce_mean(tf.cast(correctPrediction, tf.float32))

    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=prediction, labels=y))
    optimiser = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(loss)

    with tf.Session() as session:
        session.run(tf.global_variables_initializer())

        ############################################################################
        #   TRAINING
        ############################################################################
        for iteration in range(trainingIterations):
            batchX, batchY = generateData(batchSize, sequenceLength, isTraining=True)
            dict = {batchSz: batchSize, x: batchX, y: batchY}
            session.run(optimiser, dict)
            if (iteration + 1) % printStep == 0 or iteration in (0, trainingIterations - 1):
                batchAccuracy = session.run(accuracy, dict)
                batchLoss  = session.run(loss, dict)
                # inputOneHots = session.run(x, dict)
                # labelOneHots = session.run(y, dict)
                # predictions = session.run(prediction, dict)
                # correctPredictions = session.run(correctPrediction, dict)
                print(""Iteration:\t"" + str(iteration + 1))
                print(""Accuracy:\t"" + str(""%.2f"" % (batchAccuracy * 100) + ""%""))
                print(""Loss:\t\t"" + str(batchLoss) + ""\n"")
                # print(""Inputs:\n"" + str(oneHotsToChars(inputOneHots)) + ""\n"")
                # print(""Labels:\n"" + str(oneHotsToChars(labelOneHots)) + ""\n"")
                # print(""Prediction:\n"" + str(oneHotsToChars(predictionToOneHots(predictions))) + ""\n"")
                # print(""Correct:\n"" + str(correctPredictions) + ""\n"")

        ############################################################################
        #   TESTING
        ############################################################################
        testX, testY = generateData(numTestingSequences, sequenceLength, isTraining=False)
        testAccuracy = session.run(accuracy, feed_dict={batchSz: numTestingSequences, x: testX, y: testY})
        print(""Testing Accuracy: "" + str(""%.2f"" % (testAccuracy * 100) + ""%""))

        ############################################################################
        #   GENERATING
        ############################################################################
        def stringToOneHots(string):
            oneHots = []
            for char in string:
                oneHots.append(charToOneHot(char))
            return oneHots

        randIndex = random.randint(0, textSize - sequenceLength)
        seedSequence = text[randIndex: randIndex + sequenceLength]
        inputOneHots = stringToOneHots(seedSequence)

        generatedText = """"
        for _ in range(100):
            dict = {x: [inputOneHots], batchSz: 1}
            pred = session.run(prediction, dict)
            predString = oneHotsToChars(predictionToOneHots(pred))[0]
            inputOneHots = stringToOneHots(predString)
            generatedText += predString

        print(generatedText)



*Side Notes:*

1. If you uncomment all the `print` lines in the training loop, and reduce `sequenceLength` and `batchSize` to lower values, like 3, it is easier to follow exactly how the model works and what  outputs it produces at each stage.

2. Is my method of initially keeping things in 3D arrays (batchSize x sequenceLength x vocabSize), and then flattening it all to be processed with TensorFlow ok? I find it easier to visualise all of the data in these dimensions, rather than having huge arrays of all the one-hot vectors concatenated into one. However, the code does get a bit messy by flattening everything, and then reshaping it all back into its original format. ",0,2
6,2017-8-3,2017,8,3,23,6rcjum,Train your Deep Learning model faster and sharper: Snapshot Ensembling,https://www.reddit.com/r/deeplearning/comments/6rcjum/train_your_deep_learning_model_faster_and_sharper/,digitalson,1501769604,,1,1
7,2017-8-3,2017,8,3,23,6rckbz,DeepSense: A unified deep learning framework for time-series mobile sensing data processing,https://www.reddit.com/r/deeplearning/comments/6rckbz/deepsense_a_unified_deep_learning_framework_for/,magneticono,1501769738,,0,12
8,2017-8-4,2017,8,4,0,6rd2l2,Deep Learning Hyperparameter Optimization with Competing Objectives,https://www.reddit.com/r/deeplearning/comments/6rd2l2/deep_learning_hyperparameter_optimization_with/,harrism,1501774367,,1,4
9,2017-8-4,2017,8,4,7,6rfyn7,Encode not resolved embeddings,https://www.reddit.com/r/deeplearning/comments/6rfyn7/encode_not_resolved_embeddings/,riku_iki,1501798270,"I am building TensorFlow model for NLP task, and I am using pretrained Glove 300d word-vector/embedding dataset.

Obviously some tokens can't be resolved as embeddings, because were not included into training dataset for word vector embedding model, e.g. rare names.

I can replace those tokens with vectors of 0s, but rather than dropping this information on the floor, I prefer to encode it somehow and include to my training data.

Say, I have 'raijin' word, which can't be resolved as embedding vector, what would be the best way to encode it consistently with Glove embedding dataset? What is the best approach to convert it to 300d vector?

Thank you.",1,1
10,2017-8-4,2017,8,4,9,6rgphj,Building Tensorflow from Scratch,https://www.reddit.com/r/deeplearning/comments/6rgphj/building_tensorflow_from_scratch/,czhu12,1501805276,,0,12
11,2017-8-4,2017,8,4,14,6riheo,Image Labelling Tools,https://www.reddit.com/r/deeplearning/comments/6riheo/image_labelling_tools/,uridah,1501825121,I am looking for a tool that can help me in labelling image data. I have satellite images of roofs and I am trying to label roof sections and trees. Somebody suggested http://labelme.csail.mit.edu/Release3.0/browserTools/php/mechanical_turk.php but this looks too complicated. Are there any other tools available for this?,8,1
12,2017-8-4,2017,8,4,19,6rjmcl,Data Science Digest - Issue #9,https://www.reddit.com/r/deeplearning/comments/6rjmcl/data_science_digest_issue_9/,flyelephant,1501842924,,0,1
13,2017-8-4,2017,8,4,23,6rku6z,Train your Deep Learning Faster: FreezeOut,https://www.reddit.com/r/deeplearning/comments/6rku6z/train_your_deep_learning_faster_freezeout/,molode,1501857138,,0,11
14,2017-8-5,2017,8,5,2,6rlxes,Deep Learning 101: Demystifying Tensors,https://www.reddit.com/r/deeplearning/comments/6rlxes/deep_learning_101_demystifying_tensors/,magneticono,1501866758,,0,3
15,2017-8-5,2017,8,5,4,6rmoet,Data preprocessing for deep learning with nuts-ml,https://www.reddit.com/r/deeplearning/comments/6rmoet/data_preprocessing_for_deep_learning_with_nutsml/,jackblun,1501873298,,0,2
16,2017-8-5,2017,8,5,6,6rnobe,DeepRL bootcamp-Berkeley,https://www.reddit.com/r/deeplearning/comments/6rnobe/deeprl_bootcampberkeley/,hull11,1501882470,"Anybody here attending the bootcamp from Aug 26-Aug 27,2017?",4,2
17,2017-8-6,2017,8,6,20,6rxw2k,Is there any deep learning software where I can specify the number of CPU cores to work on ?,https://www.reddit.com/r/deeplearning/comments/6rxw2k/is_there_any_deep_learning_software_where_i_can/,dexterdev30,1502019092,"Hi I am from a University in India where we have CPU cluster facility for doing MD (molecular dynamics) simulations etc. Unfortunately there are no GPUs. *I want to know if there exists any ML/DL software existing, where I can train models by specifying the number of cores?* I use a MD software called NAMD where we run it using *mpirun* by specifying the number of cores. So I was thinking if I can get something similar for ML/DL.",4,1
18,2017-8-7,2017,8,7,3,6rzyqx,When Not to Use Deep Learning,https://www.reddit.com/r/deeplearning/comments/6rzyqx/when_not_to_use_deep_learning/,psangrene,1502043681,,0,7
19,2017-8-7,2017,8,7,5,6s0njr,Deep Learning with Python and Keras,https://www.reddit.com/r/deeplearning/comments/6s0njr/deep_learning_with_python_and_keras/,alphatym,1502050563,,0,1
20,2017-8-7,2017,8,7,15,6s3vvv,DeepLearning for text analysis and NLP,https://www.reddit.com/r/deeplearning/comments/6s3vvv/deeplearning_for_text_analysis_and_nlp/,rajnp,1502088291,"Hi,

I am new to ML/DL and thinking about whether DL would help me to solve the below use case,

Users will raise a support request thru our existing ticketing system. Now my ML should read the description and understand the below key points,

  1. Severity
  2. Sentiment (+ve or -ve)
  3. If any key information like user id,  email, etc (already configured) are missing

If it is possible, what type of model I have to choose and what would be the high level NN architecture required for this?

Thanks.",3,1
21,2017-8-7,2017,8,7,22,6s5i8x,Deep Learning in Medical Imaging Diagnosis,https://www.reddit.com/r/deeplearning/comments/6s5i8x/deep_learning_in_medical_imaging_diagnosis/,artificialbrainxyz,1502112485,,0,0
22,2017-8-7,2017,8,7,23,6s5v1w,Recent Breakthroughs in Deep Learning and Artificial Intelligence,https://www.reddit.com/r/deeplearning/comments/6s5v1w/recent_breakthroughs_in_deep_learning_and/,artificialbrainxyz,1502116166,,0,0
23,2017-8-8,2017,8,8,3,6s7ho9,Step by Step Example to Version Control Machine Learning and Deep Learning Tasks - Numerai Model Prediction,https://www.reddit.com/r/deeplearning/comments/6s7ho9/step_by_step_example_to_version_control_machine/,thumbsdrivesmecrazy,1502130254,,0,4
24,2017-8-8,2017,8,8,18,6schsj,Inside Google: in this way we use the Deep Learning,https://www.reddit.com/r/deeplearning/comments/6schsj/inside_google_in_this_way_we_use_the_deep_learning/,chvleo,1502186306,,0,1
25,2017-8-8,2017,8,8,21,6sd9m6,An Introduction to the MXNet Python API,https://www.reddit.com/r/deeplearning/comments/6sd9m6/an_introduction_to_the_mxnet_python_api/,janemoz,1502196705,,0,1
26,2017-8-8,2017,8,8,23,6sdtgf,IBM claims big Deep Learning breakthrough,https://www.reddit.com/r/deeplearning/comments/6sdtgf/ibm_claims_big_deep_learning_breakthrough/,chlordane2501,1502202386,,1,0
27,2017-8-9,2017,8,9,0,6seepb,Using AI to Super Compress Images  Hacker Noon,https://www.reddit.com/r/deeplearning/comments/6seepb/using_ai_to_super_compress_images_hacker_noon/,harvey_slash,1502207797,,0,1
28,2017-8-9,2017,8,9,1,6selby,Andrew Ng launches new deep learning course on Coursera,https://www.reddit.com/r/deeplearning/comments/6selby/andrew_ng_launches_new_deep_learning_course_on/,pfrcks,1502209385,,2,16
29,2017-8-9,2017,8,9,6,6sgllc,Top Deep Learning Books,https://www.reddit.com/r/deeplearning/comments/6sgllc/top_deep_learning_books/,kjahan,1502226569,,0,1
30,2017-8-9,2017,8,9,10,6si56e,TensorFlow Serving 1.0,https://www.reddit.com/r/deeplearning/comments/6si56e/tensorflow_serving_10/,_rusht,1502241696,,0,2
31,2017-8-9,2017,8,9,18,6sk9kv,Computer Vision News and BEST OF CVPR2017,https://www.reddit.com/r/deeplearning/comments/6sk9kv/computer_vision_news_and_best_of_cvpr2017/,Gletta,1502269511,"This is Computer Vision News of August, published by RSIP Vision, with 56 pages of exclusive content about computer vision, artificial intelligence, deep learning and image processing. Most of it is a BEST OF CVPR section, including many reviews of work presented 2 weeks ago at CVPR2017 in Honolulu, Hawaii.
HTML5 version (recommended) ==> http://www.rsipvision.com/ComputerVisionNews-2017August/
and
PDF version ==> http://www.rsipvision.com/computer-vision-news-2017-august-pdf/
Enjoy!",1,3
32,2017-8-9,2017,8,9,22,6slklz,Autonomio | Unlike Human Intelligence,https://www.reddit.com/r/deeplearning/comments/6slklz/autonomio_unlike_human_intelligence/,mikkokotila,1502286432,,0,4
33,2017-8-10,2017,8,10,3,6snmp7,UK based company was looking to hire a few hundred people in AI domain,https://www.reddit.com/r/deeplearning/comments/6snmp7/uk_based_company_was_looking_to_hire_a_few/,chvleo,1502304770,,0,1
34,2017-8-10,2017,8,10,16,6srr5w,Autonomous Selfie Drone by Crazyflie using Deep Learning Models,https://www.reddit.com/r/deeplearning/comments/6srr5w/autonomous_selfie_drone_by_crazyflie_using_deep/,ildoonet,1502349967,,0,1
35,2017-8-10,2017,8,10,23,6stimm,"Who is the child of the artificial intelligence for which Google has paid a transfer price of nearly 600 million dollars PHOTO, VIDEO",https://www.reddit.com/r/deeplearning/comments/6stimm/who_is_the_child_of_the_artificial_intelligence/,chvleo,1502373930,,0,1
36,2017-8-10,2017,8,10,23,6stoac,Deep Learning Framework- TensorFLow and PyTorch,https://www.reddit.com/r/deeplearning/comments/6stoac/deep_learning_framework_tensorflow_and_pytorch/,sudheeran,1502375470,,0,1
37,2017-8-11,2017,8,11,0,6su5nf,Preparing for the Transition to Applied AI - A Guide for Software Engineers,https://www.reddit.com/r/deeplearning/comments/6su5nf/preparing_for_the_transition_to_applied_ai_a/,e_ameisen,1502379935,,0,7
38,2017-8-11,2017,8,11,14,6sz25o,Anyone interested in Kaggle competition,https://www.reddit.com/r/deeplearning/comments/6sz25o/anyone_interested_in_kaggle_competition/,chitrang6,1502428296,"Hello folks, I am planning to work on some image based kaggle datasets or competition. Kindly let me know if anyone is interested. My Gmail - chitrang6@gmail.com. LinkedIn- www.linkedin.com/in/chitrangtalaviya",0,1
39,2017-8-11,2017,8,11,20,6t0fis,Trailblazing Tech in Facial Recognition,https://www.reddit.com/r/deeplearning/comments/6t0fis/trailblazing_tech_in_facial_recognition/,truefaceAI,1502449683,,0,1
40,2017-8-11,2017,8,11,21,6t0xuv,The Future of Artificial Intelligence: Predictions for 2018,https://www.reddit.com/r/deeplearning/comments/6t0xuv/the_future_of_artificial_intelligence_predictions/,cevizligizem,1502455982,,0,0
41,2017-8-12,2017,8,12,2,6t2sva,Number plate detection with Supervisely and Tensorflow,https://www.reddit.com/r/deeplearning/comments/6t2sva/number_plate_detection_with_supervisely_and/,tdionis,1502473469,,0,2
42,2017-8-13,2017,8,13,6,6tb93a,Tensorflow from Scratch pt 2,https://www.reddit.com/r/deeplearning/comments/6tb93a/tensorflow_from_scratch_pt_2/,czhu12,1502574977,,0,8
43,2017-8-14,2017,8,14,5,6thltr,Mind Reading: Using Artificial Neural Nets to Predict Viewed Image Categories From EEG Readings,https://www.reddit.com/r/deeplearning/comments/6thltr/mind_reading_using_artificial_neural_nets_to/,trumtra,1502656796,,0,1
44,2017-8-14,2017,8,14,5,6thms7,Going deeper with recurrent networks: Sequence to Bag of Words Model,https://www.reddit.com/r/deeplearning/comments/6thms7/going_deeper_with_recurrent_networks_sequence_to/,lalypopa123,1502657095,,0,1
45,2017-8-14,2017,8,14,14,6tkg62,Is Spark useful in deep learning?,https://www.reddit.com/r/deeplearning/comments/6tkg62/is_spark_useful_in_deep_learning/,jobsforlifkla,1502688805,I'm getting started with deep learning starting with Tensor Flow. Do you recommend learning Spark as well?,4,4
46,2017-8-14,2017,8,14,17,6tl34w,I open source a Deep Learning toolkit base on iOS Metal on github:https://github.com/amazingyyc/Brouhaha Someone who is interested in Mobile-Deep Learning could use it,https://www.reddit.com/r/deeplearning/comments/6tl34w/i_open_source_a_deep_learning_toolkit_base_on_ios/,amazingyyc,1502698809,,0,2
47,2017-8-14,2017,8,14,19,6tljm0,Must read breakthrough research papers about Image Classification using Deep Learning,https://www.reddit.com/r/deeplearning/comments/6tljm0/must_read_breakthrough_research_papers_about/,parth10,1502706449,,0,1
48,2017-8-14,2017,8,14,23,6tmmjy,The largest AI event in Germany,https://www.reddit.com/r/deeplearning/comments/6tmmjy/the_largest_ai_event_in_germany/,DeeJayDeepLearning,1502719651,"https://www.gputechconf.eu/Register.aspx

Use code DanielSaaristoGTCEU17 for a 25% discount on any ticket",0,0
49,2017-8-15,2017,8,15,6,6tpl10,"Learning, Deep Learning. Where to start?",https://www.reddit.com/r/deeplearning/comments/6tpl10/learning_deep_learning_where_to_start/,eardil,1502745943,"Hi! First time redditing (is that a word here?). Anyway, I'm trying to learn to implement deep learning, specifically RNNs. And I'm struggling to decide where to start.

My problem is not with the theory, I'm an applied mathematician, I have implemented other models (including NN) in the past and work doing ML and NLP. I've been reading and getting familiar with the theory of DL for a while now. Nor is it configuration, since I've installed and tested TF for GPU.

The thing is the technology seems to be moving really fast and I don't know which tool to learn (TF, Keras, TF+Keras) and every tutorial I've started seems to be outdated by some new language or interface so I keep getting paranoid of learning syntax that I'll have to forget soon (I'm working in Python BTW). Does anyone have some insight/advice on where to start if I'm doing it at this instant?

tl;dr Fairly familiar with theory of DL but don't know what language/API to start learning.",3,1
50,2017-8-15,2017,8,15,21,6ttuad,Gpu selection for server grid,https://www.reddit.com/r/deeplearning/comments/6ttuad/gpu_selection_for_server_grid/,abhi_annamraju,1502800868,"Which gpu architecture should be used to create a server grid? - Nvidia Tesla series (P40, p100, M40, etc) or Nvidia GeForce (Titan XP, 1080 TI, 980 TI etc)
As pointed out over the comments, adding here more specs:
1. Need to accommodate convnet, semantic segmentation, and GAN based networks requiring GPU RAM specification of more than 20 GB.
2. Data is satellite imagery ranging upto 1-1.5 TB.
3. Softwares/Tools: Keras/TF.
4. OS: Linux.
5. CPUs: Yet to decide. Will select a one compatible with GPU architecture.

Consider the case associated: If I have 2 P40s, each having 24Gigs of RAM. Now suppose I am running a CycleGAN module training (on Keras/TF) which requires 30 GB RAM with the batch size I selected. Will these P40s communicate to assign the required GPU. {Read this from a blog: Tesla/Quadro Pascal Unified Memory allows GPUs to share each others memory to load even larger datasets]. I haven't got the chance to test it out, so asking it here. 


 ",4,1
51,2017-8-16,2017,8,16,2,6tvnts,Free Deep Learning Book (MIT Press),https://www.reddit.com/r/deeplearning/comments/6tvnts/free_deep_learning_book_mit_press/,psangrene,1502818036,,4,11
52,2017-8-16,2017,8,16,2,6tvqq6,Two Great Courses on Deep Learning and AI,https://www.reddit.com/r/deeplearning/comments/6tvqq6/two_great_courses_on_deep_learning_and_ai/,psangrene,1502818726,,0,5
53,2017-8-16,2017,8,16,3,6tw3h3,Are there any papers comparing the ImageNet problem vs. binary ImageNet (hot_dog/not_hot_dog) problem?,https://www.reddit.com/r/deeplearning/comments/6tw3h3/are_there_any_papers_comparing_the_imagenet/,gukjoon,1502821708,Are there any studies on whether we get much higher performance when building a NN to detect a particular ImageNet class vs. building a general NN to detect all the ImageNet classes. Is the precision of the former >> precision of the latter? What about recall?,0,0
54,2017-8-16,2017,8,16,4,6twqf8,Tutorial on deep learning in Python,https://www.reddit.com/r/deeplearning/comments/6twqf8/tutorial_on_deep_learning_in_python/,pvigier,1502827191,"Hello everyone!

I've written two blog posts:

* [Part 1 is about computational graphs and how to code a little framework for doing deep learning](https://pvigier.github.io/2017/07/21/pychain-part1-computational-graphs.html)
* [Part 2 is about using this framework on MNIST dataset](https://pvigier.github.io/2017/08/13/pychain-part2-mnist.html)

And I want to share them with you so please, feel free to give me your thoughts on them!

I am planning on writing about RNNs and LSTM next and also reproducing the results of Karparthy on text generation.",2,5
55,2017-8-16,2017,8,16,17,6u0sdp,Data Science Digest - Issue #10,https://www.reddit.com/r/deeplearning/comments/6u0sdp/data_science_digest_issue_10/,flyelephant,1502873803,,0,0
56,2017-8-16,2017,8,16,22,6u1xlv,Top 5 Benefits of Containerization,https://www.reddit.com/r/deeplearning/comments/6u1xlv/top_5_benefits_of_containerization/,cevizligizem,1502889148,,0,0
57,2017-8-16,2017,8,16,22,6u2649,"The Difference between Artificial Intelligence, Machine Learning and Deep Learning?",https://www.reddit.com/r/deeplearning/comments/6u2649/the_difference_between_artificial_intelligence/,truefaceAI,1502891621,,0,0
58,2017-8-16,2017,8,16,23,6u2c31,DataScan Digest: Issue #43,https://www.reddit.com/r/deeplearning/comments/6u2c31/datascan_digest_issue_43/,[deleted],1502893177,[deleted],0,0
59,2017-8-16,2017,8,16,23,6u2d1v,Is the movidius stick suitable for training a semantic segmentation network ?,https://www.reddit.com/r/deeplearning/comments/6u2d1v/is_the_movidius_stick_suitable_for_training_a/,jean-pat,1502893451,,1,0
60,2017-8-17,2017,8,17,1,6u32ml,Andrew Ng is raising a $150M AI Fund,https://www.reddit.com/r/deeplearning/comments/6u32ml/andrew_ng_is_raising_a_150m_ai_fund/,milly1993,1502899809,,1,4
61,2017-8-17,2017,8,17,3,6u47i6,Scaling Keras Model Training to Multiple GPUs,https://www.reddit.com/r/deeplearning/comments/6u47i6/scaling_keras_model_training_to_multiple_gpus/,harrism,1502909571,,0,6
62,2017-8-17,2017,8,17,16,6u8b57,Tutorial: quick guide on how to train keras UNet implementation on Cityscapes dataset,https://www.reddit.com/r/deeplearning/comments/6u8b57/tutorial_quick_guide_on_how_to_train_keras_unet/,tdionis,1502953353,,0,0
63,2017-8-17,2017,8,17,22,6ua2hk,First Steps of Learning Deep Learning: Image Classification in Keras,https://www.reddit.com/r/deeplearning/comments/6ua2hk/first_steps_of_learning_deep_learning_image/,dearpetra,1502977713,,0,3
64,2017-8-17,2017,8,17,22,6ua2p9,A Guide to Understanding AI Toolkits,https://www.reddit.com/r/deeplearning/comments/6ua2p9/a_guide_to_understanding_ai_toolkits/,molode,1502977775,,0,1
65,2017-8-17,2017,8,17,23,6ua71q,The Two Phases of Gradient Descent in Deep Learning,https://www.reddit.com/r/deeplearning/comments/6ua71q/the_two_phases_of_gradient_descent_in_deep/,digitalson,1502978961,,0,0
66,2017-8-17,2017,8,17,23,6uabsq,Using Deep Learning To Extract Knowledge From Job Descriptions,https://www.reddit.com/r/deeplearning/comments/6uabsq/using_deep_learning_to_extract_knowledge_from_job/,dearpetra,1502980228,,0,0
67,2017-8-18,2017,8,18,18,6ugri2,Deep Learning In Retail,https://www.reddit.com/r/deeplearning/comments/6ugri2/deep_learning_in_retail/,avilash100,1503048982,"How do you detect individual instances of retail boxes and classify them on supermarket shelves.
The number of classes may range from 10000 - 100000.",3,2
68,2017-8-19,2017,8,19,2,6uje5g,Dogs vs. Cats: Image Classification with Deep Learning using TensorFlow in Python,https://www.reddit.com/r/deeplearning/comments/6uje5g/dogs_vs_cats_image_classification_with_deep/,psangrene,1503077583,,0,1
69,2017-8-19,2017,8,19,7,6ulb1r,Assembling a team of Deep Learning enthusiasts for a chatroom for General AI research.,https://www.reddit.com/r/deeplearning/comments/6ulb1r/assembling_a_team_of_deep_learning_enthusiasts/,nkoutrou,1503095342,"The basic idea behind this project is the creation of a *chat-room* of highly qualified individuals interested in the development of novel applications of AI towards product development as well as academic research. 

Our team wants to focus on the Deep Learning approach to general AI so we are specifically looking for people with experience and ambition to succeed in this particular field. Your position in such a group aids both the quality of it as a whole as much as it aids you to better yourself since youll have access to many other deep learners with whom you can discuss area-specific concerns. Of course, to create such a community we need to implement a safeguard in the form of an interview/ exam to ensure that we are both headed in the same direction. Bear in mind that down the line this chat-room is also a prospect for professional employment so your temporal investment will most likely also pay out in the literal sense, should you choose to engage in the commercial side of things.

This project will be a **paid** venture for anyone who chooses to assist in the development of any product that comes from it. 

If being a part of such a team interests you can find a full description of what our idea is and of the way we plan to implement it, go ahead and read this document:  https://docs.google.com/document/d/1I4LiAJB-mxcXrQmfUJfSUj_V8_5XwNz_8CKxEZC1dfI/edit

**TL;DR:** Does a chatroom for General AI through Deep Learning sound like your kind of thing? *Check out our google doc!*
",2,3
70,2017-8-19,2017,8,19,9,6um1ev,SimpleDNN is a machine learning library written in Kotlin whose purpose is to support the development of feed-forward and recurrent Neural Networks,https://www.reddit.com/r/deeplearning/comments/6um1ev/simplednn_is_a_machine_learning_library_written/,matteo_reddit,1503103445,,0,3
71,2017-8-19,2017,8,19,16,6unqwx,What can be the best ways to fit on failed test cases?,https://www.reddit.com/r/deeplearning/comments/6unqwx/what_can_be_the_best_ways_to_fit_on_failed_test/,raghuemani,1503127033,"I have been trying to solve a recognition problem on real world handwriting dataset using EMNIST data by various forms of augmentation on EMNIST which looks like real.

So after testing, I found out there are some close cases where it is very much evident that for example it is a '8' but predicted as '3' due to bad image quality.

Since, I don't have much real data which I can use for training and testing so basically dependent on EMNIST data. 
Now, what I would like to know are the best practices Deep learning community adopts while fitting model on failed cases, how would you use failed cases to your advantage? 
Can we augment the failed cases and try to fit a model on it from pre-trained weights, but I feel that would somehow overfit on test data since augmentation doesn't create independent samples.
Thanks   ",0,1
72,2017-8-19,2017,8,19,17,6unzv3,New Gitter chat channel about Artificial General Intelligence / Strong AI,https://www.reddit.com/r/deeplearning/comments/6unzv3/new_gitter_chat_channel_about_artificial_general/,razvanpanda,1503131606,,1,1
73,2017-8-19,2017,8,19,19,6uoa1p,Facebook's controversial patent grants extend to caffe as well,https://www.reddit.com/r/deeplearning/comments/6uoa1p/facebooks_controversial_patent_grants_extend_to/,sandys1,1503137006,"What is the opinion of the community on the patent grants that facebook has in place for caffe ?
Basically, the issue came up from facebook's [refusal to change](https://github.com/facebook/react/issues/10191#issuecomment-323486580) reactjs license to apache2. This is because, Facebook has license terms that allows them to *revoke* patent grants and countersue... if you sue them first.
Even if your lawsuit is justified (for example Facebook wilfully infringed on something that you invented). Caffe/Caffe2 is covered by this license as well.
FWIW, Tensorflow is under an irrevocable Apache2 license.

For people who are using caffe/caffe2 in production, what does your legal team have to say about this ?",2,11
74,2017-8-20,2017,8,20,3,6uqudo,Teaching Machines How To Spot Diseases,https://www.reddit.com/r/deeplearning/comments/6uqudo/teaching_machines_how_to_spot_diseases/,artificialbrainxyz,1503168864,,0,1
75,2017-8-20,2017,8,20,20,6uvbho,Towards Artificial General Intelligence: Oriol Vinyals,https://www.reddit.com/r/deeplearning/comments/6uvbho/towards_artificial_general_intelligence_oriol/,artificialbrainxyz,1503229936,,0,1
76,2017-8-20,2017,8,20,22,6uvpxk,A compiled list of resources for machine learning and deep learning engineers working with neural style transfer or deep photo style transfer.,https://www.reddit.com/r/deeplearning/comments/6uvpxk/a_compiled_list_of_resources_for_machine_learning/,kailashahirwar12,1503235805,,0,8
77,2017-8-21,2017,8,21,2,6ux5z8,Deep Learning and Neural Networks Primer: Basic Concepts for Beginners,https://www.reddit.com/r/deeplearning/comments/6ux5z8/deep_learning_and_neural_networks_primer_basic/,molode,1503251801,,0,1
78,2017-8-21,2017,8,21,4,6uxjv0,Interesting examples of transfer learning? (xpost /r/machinelearning),https://www.reddit.com/r/deeplearning/comments/6uxjv0/interesting_examples_of_transfer_learning_xpost/,ProfThrowaway17,1503255601,,0,3
79,2017-8-21,2017,8,21,5,6uy7se,Deep Learning in Minutes with this Pre-configured Python VM Image,https://www.reddit.com/r/deeplearning/comments/6uy7se/deep_learning_in_minutes_with_this_preconfigured/,friscotime,1503262228,,1,3
80,2017-8-21,2017,8,21,18,6v1tl2,Data protection and AI: How can we control our creations?,https://www.reddit.com/r/deeplearning/comments/6v1tl2/data_protection_and_ai_how_can_we_control_our/,[deleted],1503307719,[deleted],1,1
81,2017-8-21,2017,8,21,20,6v2d4y,I've Got a Lot of Questions,https://www.reddit.com/r/deeplearning/comments/6v2d4y/ive_got_a_lot_of_questions/,TheNASAguy,1503315877,"	Are there any Weights Libraries, I mean Neural Nets Are Trained on Huge Datasets and then Implemented into Products, Services and Research so, are there any libraries of Pre-Trained Neural Nets that we could use 

	How can we tackle an ML Problem if the Data is Heterogeneous or Complex or Recursive? Like can you use a Neural Net to Solve Definite Integrals?

	What are the ways we can prepare and feed data sets into TensorFlow?

	Projects that involve Generating Insights or Further building on Trained Data? What are the ways we can acquire outputs?

	Can we combine multiple Neural Nets into a Modular Pipeline to Process data? ",2,0
82,2017-8-21,2017,8,21,22,6v31bg,DataScan: Issue #44,https://www.reddit.com/r/deeplearning/comments/6v31bg/datascan_issue_44/,[deleted],1503323529,[deleted],0,1
83,2017-8-22,2017,8,22,0,6v3jrs,"Top 10 Machine Learning Videos on YouTube, updated",https://www.reddit.com/r/deeplearning/comments/6v3jrs/top_10_machine_learning_videos_on_youtube_updated/,lalypopa123,1503328353,,0,1
84,2017-8-22,2017,8,22,1,6v3xfe,Learning to write Hindi Fonts in style (x-post r/machinelearning),https://www.reddit.com/r/deeplearning/comments/6v3xfe/learning_to_write_hindi_fonts_in_style_xpost/,slartibartfst,1503331705,,1,1
85,2017-8-22,2017,8,22,1,6v4498,open source: channel pruning for accelerating very deep CNN,https://www.reddit.com/r/deeplearning/comments/6v4498/open_source_channel_pruning_for_accelerating_very/,ikkiho,1503333371,"We are glad that we have released source code of [channel pruning for accelerating very deep CNN](https://arxiv.org/abs/1707.06168), *ICCV 2017*. It's an elegant algorithm that effectively prune channels each layer.

* accelerate VGG-16 4x without losing accuracy
* accelerate ResNet-50 2x with 1.4% increase of Top-5 error
* accelerate Xception-50 2x with 1.0% increase of Top-5 error

The address is: [github.com/yihui-he/channel-pruning](https://github.com/yihui-he/channel-pruning). Please feel free to test and leave your comments.",1,3
86,2017-8-22,2017,8,22,2,6v4dbl,Lecture on How to build a recognition system (Part 1): best practices,https://www.reddit.com/r/deeplearning/comments/6v4dbl/lecture_on_how_to_build_a_recognition_system_part/,tdionis,1503335570,,0,1
87,2017-8-22,2017,8,22,3,6v4um6,Lecture. Evolution: from vanilla RNN to GRU &amp; LSTMs,https://www.reddit.com/r/deeplearning/comments/6v4um6/lecture_evolution_from_vanilla_rnn_to_gru_lstms/,tdionis,1503339570,,0,3
88,2017-8-22,2017,8,22,4,6v56au,Deep Learning with Python and Keras,https://www.reddit.com/r/deeplearning/comments/6v56au/deep_learning_with_python_and_keras/,dnonsense,1503342197,,0,4
89,2017-8-23,2017,8,23,0,6vbrnr,How Not To Program the TensorFlow Graph,https://www.reddit.com/r/deeplearning/comments/6vbrnr/how_not_to_program_the_tensorflow_graph/,friscotime,1503415769,,0,0
90,2017-8-23,2017,8,23,2,6vchtt,Using deep learning to colorise black &amp; white images,https://www.reddit.com/r/deeplearning/comments/6vchtt/using_deep_learning_to_colorise_black_white_images/,harvey_slash,1503422106,,0,8
91,2017-8-23,2017,8,23,15,6vh39c,Existence of Shallow networks which perform nearly good?,https://www.reddit.com/r/deeplearning/comments/6vh39c/existence_of_shallow_networks_which_perform/,dexterdev30,1503468604,"Is there any shallow network implementation of a ""tough"" classification problem or so? To make it specific let me give an example: Think CAT vs DOG images classification problem. Which is the most light weight network ever designed to solve this problem? Can someone give me resources (like papers or articles etc) related to this kind of stuff?",1,2
92,2017-8-23,2017,8,23,19,6vi5dv,THIS IS HOW SCALING LOOKS LIKE | The Hidden Cost of Deep Learning,https://www.reddit.com/r/deeplearning/comments/6vi5dv/this_is_how_scaling_looks_like_the_hidden_cost_of/,mikkokotila,1503485175,,14,3
93,2017-8-23,2017,8,23,22,6vj00b,The Guerrilla Guide to Machine Learning with Python,https://www.reddit.com/r/deeplearning/comments/6vj00b/the_guerrilla_guide_to_machine_learning_with/,jackblun,1503495409,,0,4
94,2017-8-23,2017,8,23,23,6vj6zm,One Deep Learning Virtual Machine to Rule Them All,https://www.reddit.com/r/deeplearning/comments/6vj6zm/one_deep_learning_virtual_machine_to_rule_them_all/,friscotime,1503497309,,0,1
95,2017-8-24,2017,8,24,2,6vkplo,Secret Sauce behind the beauty of Deep Learning: Beginners guide to Activation Functions,https://www.reddit.com/r/deeplearning/comments/6vkplo/secret_sauce_behind_the_beauty_of_deep_learning/,kailashahirwar12,1503510809,,1,4
96,2017-8-25,2017,8,25,4,6vtbaj,Deep Learning with MXNet livecoding on twitch,https://www.reddit.com/r/deeplearning/comments/6vtbaj/deep_learning_with_mxnet_livecoding_on_twitch/,ranman96734,1503603838,,0,3
97,2017-8-25,2017,8,25,6,6vtx7j,"Deep Learning, Where are we going?",https://www.reddit.com/r/deeplearning/comments/6vtx7j/deep_learning_where_are_we_going/,artificialbrainxyz,1503609185,,0,1
98,2017-8-25,2017,8,25,21,6vy65d,Deep Learning From Scratch: Theory and Implementation,https://www.reddit.com/r/deeplearning/comments/6vy65d/deep_learning_from_scratch_theory_and/,deepideas,1503664941,,2,1
99,2017-8-26,2017,8,26,1,6vzii0,Conv net combined with recurrent net for video analysis? (xpost /r/MachineLearning),https://www.reddit.com/r/deeplearning/comments/6vzii0/conv_net_combined_with_recurrent_net_for_video/,[deleted],1503677907,[deleted],0,1
100,2017-8-26,2017,8,26,2,6vztr5,Training a unet to resolve pairs of overlapping chromosomes,https://www.reddit.com/r/deeplearning/comments/6vztr5/training_a_unet_to_resolve_pairs_of_overlapping/,jean-pat,1503680889,,0,1
101,2017-8-26,2017,8,26,3,6w0au5,"Approaches for analyzing the shape of 2D or 3D objects from segmented (i.e., binarized) images",https://www.reddit.com/r/deeplearning/comments/6w0au5/approaches_for_analyzing_the_shape_of_2d_or_3d/,ProfThrowaway17,1503685181,"Medical images that have already been segmented are basically representations of just the *shape* of 2D or 3D objects, without any notion of color, shading or other scalar features.  I imagine this type of ""shape"" data occurs in other applications too. 

A raw representation in 2D would be an image where pixels take binary values (i.e., a binary matrix). In 3D, you'd have a stack of such images or, equivalently, a 3D array of binary voxels. For most data sets, the ""ON"" or ""white"" pixels or voxels would be grouped into 1-3 contiguous blobs. 

Would convolutional layers make sense for these types of data?  A conv layer would reduce the number of pixels/voxels and perform operations like edge detection with a relatively small number of parameters, but it would also change the binary values into floating point values, partly destroying the simplicity of the raw data. Would this be a good approach if we want to learn about the shape of the represented object?

What about using fewer conv layers than normal? Or skipping the convolution altogether and jumping to max-pool? The idea is that we don't really need the powerful feature detection provided by convolution since the features of contiguous binary pixels are so simple.

Another approach would be to convert the input to a set of points, i.e. a list of the voxels that are ""white.""  This is certainly a more compact representation, especially if the image is sparse, but would it be any easier for a deep net to analyze?  What architecture would work well for this data format?  Just a bunch of fully connected ReLu layers?  

Are there other representations and architectures that make sense for this type of data?",1,1
102,2017-8-26,2017,8,26,4,6w0ows,Data Science - Deep Learning in Python,https://www.reddit.com/r/deeplearning/comments/6w0ows/data_science_deep_learning_in_python/,dnonsense,1503688741,,0,4
103,2017-8-26,2017,8,26,21,6w5c0e,Is there any free trained networks available to create art paintings like in deepart.io?,https://www.reddit.com/r/deeplearning/comments/6w5c0e/is_there_any_free_trained_networks_available_to/,dexterdev30,1503751219,"So if I have an input image and a style images, I can give the inputs to [deepart](https://deepart.io) to get artisitc output images. I assume that there exists an underlying trained network for these purposes. If my understanding is wrong, I am very sorry. Is there is any such free trained deep networks available to download and play with?",2,2
104,2017-8-27,2017,8,27,2,6w6wx1,Network architecture for large number of classes,https://www.reddit.com/r/deeplearning/comments/6w6wx1/network_architecture_for_large_number_of_classes/,riku_iki,1503769370,"Hi,

I am trying to design NN, which suppose to recognize large number of classes (500k). Obviously it produces very large number of connections between output and last hidden layers. Say last hidden layer has 10k neurons, then number of connections will be 10k * 500k.

Something I have in my mind is to use class label as encoded combination from output layer. For example in primitive binary encoding, class 1 would correspond to firing output #1, class 6 (110 in binary) would correspond to firing output #2 and #3 (represented as ones in binary encoding).

Another idea is to use convolution for output layer.

What kind of NN architecture you would chose to solve this problem?
",6,1
105,2017-8-27,2017,8,27,5,6w7slh,Signal Prediction with seq2seq RNNs [TensorFlow exercises],https://www.reddit.com/r/deeplearning/comments/6w7slh/signal_prediction_with_seq2seq_rnns_tensorflow/,GChe,1503778059,,0,7
106,2017-8-28,2017,8,28,1,6wcz1c,Learning Hierarchical Features from Generative Models: A Critical Paper Review,https://www.reddit.com/r/deeplearning/comments/6wcz1c/learning_hierarchical_features_from_generative/,alexmlamb,1503850039,,0,2
107,2017-8-28,2017,8,28,1,6wd69a,A small Interspeech 2017 recap,https://www.reddit.com/r/deeplearning/comments/6wd69a/a_small_interspeech_2017_recap/,troltilla,1503852093,,0,2
108,2017-8-28,2017,8,28,2,6wde1n,The math of logistic regression,https://www.reddit.com/r/deeplearning/comments/6wde1n/the_math_of_logistic_regression/,marshalization,1503854221,,0,6
109,2017-8-28,2017,8,28,3,6wdx4u,End to End Deep Learning,https://www.reddit.com/r/deeplearning/comments/6wdx4u/end_to_end_deep_learning/,psangrene,1503859432,,0,0
110,2017-8-28,2017,8,28,22,6wji3h,Any docker-image for deep learning?,https://www.reddit.com/r/deeplearning/comments/6wji3h/any_dockerimage_for_deep_learning/,Siddas27,1503928246,"Is there any docker image that consists of all deep learning and computer vision tools such as opencv, keras, tensorflow, theano, etc??",5,3
111,2017-8-29,2017,8,29,2,6wl2ul,Good project? Trumpet coach,https://www.reddit.com/r/deeplearning/comments/6wl2ul/good_project_trumpet_coach/,LeScanMars,1503942692,"I thought I'd learn about deep-learning by working on a project that would be useful to me. 


I'm intrigued to develop a trumpet practice coach.

Trumpet is a pretty difficult musical instrument. Lots of different challenges. I've been at it for four years now, can finally see a bit of the light.

In terms of learning any musical instrument, one likes to be able to set up goals and then measure progress. Then work on improving.

Unfortunately there is no formula for measurement as such. Human teachers adapt their knowledge to their students, there's lots that goes into that. Diagnose problems, recommend what to change. Send away, repeat next lesson.

So I'm thinking to build a computer systems to measure progress.

In terms of what kinds of progress there is, number one with trumpet is always ""nice tone."" And that would be a universe in itself. Next comes intonation. And then you might look at alternating notes, phrases, scales. Is the rhythm even, is the volume balanced. Also, of course, expanding the range is going to take effort.

(BTW intonation is ok to practice with a simple tuner. Doesn't pick out problems in different aspects of a note (attack, overtones, but still helps a lot)

So I'm posting to ask if anyone has a recommendation how to start on something like this. Is there anything related in machine learning? What's a good entry point to get started?",2,2
112,2017-8-29,2017,8,29,8,6wnb62,Hi.,https://www.reddit.com/r/deeplearning/comments/6wnb62/hi/,[deleted],1503962937,[deleted],0,1
113,2017-8-29,2017,8,29,14,6wp64b,Attention Mechanisms in Recurrent Neural Networks (RNNs) - IGGG,https://www.reddit.com/r/deeplearning/comments/6wp64b/attention_mechanisms_in_recurrent_neural_networks/,GChe,1503983984,,1,5
114,2017-8-29,2017,8,29,23,6wrqjh,Deep Learning for Video Game Playing,https://www.reddit.com/r/deeplearning/comments/6wrqjh/deep_learning_for_video_game_playing/,dezzion,1504018551,,0,4
115,2017-8-30,2017,8,30,3,6wt1l6,Leverage Big Data to Deliver Better Business Outcomes,https://www.reddit.com/r/deeplearning/comments/6wt1l6/leverage_big_data_to_deliver_better_business/,cevizligizem,1504030071,,0,0
116,2017-8-30,2017,8,30,6,6wuava,"training for two things, where the second is only trained on successful actions",https://www.reddit.com/r/deeplearning/comments/6wuava/training_for_two_things_where_the_second_is_only/,fuckme,1504041071,"Im a bit of a newbie here, but Im trying to build a model that does multiple things. 
First its trying to see if a action will be successful, and secondly its trying to see when the optimal time would be for a successful transaction. 

My approach is to first train the net on success/non-success. (Sigmoid single node)

I was then planning on adding a set of softmax nodes to get day of week (and another set for hour of day) and doing another round of backprop with only the successful actions. 

Im concerned that it will screw up the initial success/fail node if I do backprop again. 

Is this the correct approach?

Is there a better way than going through the data twice in two large batches like that?

Ideally Id like a single pass where the softmaxs only get activated on a successful action (I think)

Btw Im using tensorflow. 
",0,1
117,2017-8-30,2017,8,30,10,6wvx2n,What is the best approach to learn Deep Learning using Python starting from syntax?,https://www.reddit.com/r/deeplearning/comments/6wvx2n/what_is_the_best_approach_to_learn_deep_learning/,TheSexyDuckling,1504056974,"Hello world! To give you some background, I am a beginner in programming. I always found programming courses/tasks that I was  forced to do in university easy (I have a degree in Mechanical Engineering, so didn't program much), but I never ventured into actually learning it in depth besides some basic problem solving.

I have some ideas to use computer vision with deep learning in the real-life. As my first step to achieve this, I went through the basic Python syntax by completing the Codecademy course, which was easy. However, now I am completely lost as to what my next step should be. I want the most efficient, but sensible approach to go from learning Python syntax to becoming good at Deep Learning and computer vision. I would appreciate your input on a methodical approach I should take to reach my goal.

I am not interesting in developing a game or website. I want to learn Deep Learning and computer vision. Thank you! And sorry for the lengthy post!",14,1
118,2017-8-30,2017,8,30,23,6wzgjp,The brain most likely does backpropagation,https://www.reddit.com/r/deeplearning/comments/6wzgjp/the_brain_most_likely_does_backpropagation/,Roboserg,1504102972,,9,15
119,2017-8-31,2017,8,31,0,6wzzuo,What to do after Andrew Ng's deep learning courses?,https://www.reddit.com/r/deeplearning/comments/6wzzuo/what_to_do_after_andrew_ngs_deep_learning_courses/,Northstat,1504107899,"I've covered deep learning in school so I mostly took the courses as a refresher and to gain additional insight 
 from Andrew.  I've completed the first 3 courses and while I wait for the other sessions to start I'd like to continue working with deep learning. What should the next steps be? Additional courses, books, kaggle, research papers? I'm sure all of these are good, just curious what others are doing and what they find helpful.",4,2
120,2017-8-31,2017,8,31,4,6x1mp6,Generate image dataset for shape classification,https://www.reddit.com/r/deeplearning/comments/6x1mp6/generate_image_dataset_for_shape_classification/,uridah,1504121845,"I want to create a dataset for some particular shapes that I have.
So given a binary image of a perfect shape, I need to create many different variations of that shape in many different rotations. This will serve as a dataset for training a classifier which should be able to classify a binary shape image. 
I am working with 12 different shapes here (Right-angled triangle, triangle, rectangle, parallelogram, trapezoid, right-angled trapezoid, concave pentagon, equilateral pentagon etc.) 

I came up with an approach where a given number of sides, I randomly generate points and make a polygon out of that and see which shape it fits but I don't think that'll give me a good number of samples for all the shapes. Any ideas on how I should go about it (using Python preferably)? ",0,1
121,2017-8-31,2017,8,31,5,6x1su2,Practical techniques for getting style transfer to work,https://www.reddit.com/r/deeplearning/comments/6x1su2/practical_techniques_for_getting_style_transfer/,singlasahil14,1504123331,,1,2
122,2017-8-31,2017,8,31,9,6x3nci,The future of neural networks depend on neuroscience,https://www.reddit.com/r/deeplearning/comments/6x3nci/the_future_of_neural_networks_depend_on/,droidarmy95,1504141077,,0,2
123,2017-8-31,2017,8,31,19,6x65au,"A Mathematical Framework for the Analysis of Neural Networks [Anthony 2017, pdf]",https://www.reddit.com/r/deeplearning/comments/6x65au/a_mathematical_framework_for_the_analysis_of/,cbeak,1504177097,,0,9
124,2017-8-31,2017,8,31,20,6x67om,What is Intelligence?,https://www.reddit.com/r/deeplearning/comments/6x67om/what_is_intelligence/,mikkokotila,1504178077,,0,0
