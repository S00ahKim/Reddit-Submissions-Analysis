,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2015-5-1,2015,5,1,12,34hb57,Learning for sensor data,https://www.reddit.com/r/MachineLearning/comments/34hb57/learning_for_sensor_data/,xkSeeD,1430451529,"I am working on a robot which has a few IR sensors. I'm trying to find the distance of a wall to each sensor but there is a lot of noise on the data that I'm getting from the sensors. I'm using some filters and [this](http://imgur.com/89WTT5z) is the output from the filter. There is a 10% error in the readings after the filter. I would like to decrease the error to around 5%, but I am not sure how I should proceed. I was thinking if I could drive the robot in a known environment, where I can easily measure the real distance of the walls and sensors with a very high accuracy, and then use the data from the sensors and some how train(??) the sensors to get a better reading from the sensors. Is this possible? How should I do it? I have absolutely no idea if training is the right term to use in here. I would greatly appreciate any help. Thanks!",5,2
1,2015-5-1,2015,5,1,20,34ib1e,Stereovision Autoencoder: Learning better object representations from 3D assets. Does this idea make sense?,https://www.reddit.com/r/MachineLearning/comments/34ib1e/stereovision_autoencoder_learning_better_object/,fhuszar,1430481496,,12,0
2,2015-5-1,2015,5,1,22,34imod,Reminder: tonight submission deadline for Deep Learning @ ICML'15. Minimum length: 2 pages. World-class reviewers.,https://www.reddit.com/r/MachineLearning/comments/34imod/reminder_tonight_submission_deadline_for_deep/,dpkingma,1430488476,,9,3
3,2015-5-1,2015,5,1,22,34imr8,"Knowing precision on one data set, can I make claims about precision on another?",https://www.reddit.com/r/MachineLearning/comments/34imr8/knowing_precision_on_one_data_set_can_i_make/,ulta,1430488516,"I am building a spam classifier for email.

Let's say I have 3 million emails and I have a hypothesis that 1% of them are spam. So, I expect 30,000 of these 3 million emails to be spam but I don't know which ones.

Now, lets say I randomly choose 1 million of these emails and label them using a classifier I've trained. My classifier predicts that 40,000 of these 1 million are spam. I go review those 40,000 and find that only 10,000 of them are actually spam.

So, my precision is 10,000 out of 40,000 or 25%.

Now, let's say I go review those 3 million emails manually and find that my initial guess is correct and 30,000 of them actually are spam.

Can I make any guesses about what the precision of my classifier would be given another set of email with different distribution? Say 50% spam? 20% spam?

On one hand, it seems like precision says nothing except how my classifier did on this particular data set. But, on the other hand, it seems that if my classifier has a precision of 25% on a data set that was only 1% spam then surely it will do better on a data set that is 50% spam!

Is this logic flawed?",3,0
4,2015-5-2,2015,5,2,0,34iyrz,Want to test your machine learning skills? Work on real problem with real data? Join us at GMUM Challenge!,https://www.reddit.com/r/MachineLearning/comments/34iyrz/want_to_test_your_machine_learning_skills_work_on/,szampa,1430494367,,10,19
5,2015-5-2,2015,5,2,2,34jcqe,Fast R-CNN for object detection with 9x faster at training and 213x faster at test-time (for VGG16),https://www.reddit.com/r/MachineLearning/comments/34jcqe/fast_rcnn_for_object_detection_with_9x_faster_at/,vkhuc,1430500955,,7,17
6,2015-5-2,2015,5,2,3,34jkz7,Practical Text Analysis using Deep Learning,https://www.reddit.com/r/MachineLearning/comments/34jkz7/practical_text_analysis_using_deep_learning/,michaelfi,1430504777,,2,25
7,2015-5-2,2015,5,2,3,34jlwf,What is the error on my regression value from a BDT?,https://www.reddit.com/r/MachineLearning/comments/34jlwf/what_is_the_error_on_my_regression_value_from_a/,soveraign,1430505214,"Hello all,  in my research I am using boosted decision trees (as a regressor) in TMVA (Root) and sklearn.  I want an error estimate along with the prediction (e.g. predict(X) --&gt; 2.5 +- 0.5).  I've combed through the documentation and cannot find any reference to this feature.  Is this possible with the tools above?  Other tools?  Is it even common or clearly defined?

I have implemented a simple decision tree on my own primarily to learn how they work and behave.  My intuition is to use the RMS of the values in the leaves to estimate the error of that leaf.  This seems to produce results that are consistent with being correct (i.e. my pull distribution is unit normal).  But my code is very simple and cannot match the performance of much more mature implementations (like forests and fancier splitting).

I would prefer not to use my custom code for this purpose and instead take a best practices approach.  Thoughts?  Did I miss something obvious?

Edit: words",2,1
8,2015-5-2,2015,5,2,4,34jq5h,A perceptron estimates only the average in batch learning,https://www.reddit.com/r/MachineLearning/comments/34jq5h/a_perceptron_estimates_only_the_average_in_batch/,FluffyMarsianin,1430507304,"Hi folks!

I am stuck with this problem with a perceptron and really need advice. Everything I've googled so far didn't give me a clear answer, although this problem seems to be not completely uncommon.

I checked the code several times and also back propagation (BP) itself works just fine. But something weird happens in batch learning.

A snippet looks like this basically:

    weights = weights_old
    
    weight_deltas_sum = 0

    for I = 1:number of patterns
    
    [neuron_outputs, weighted_sums] = perceptron ( pattern_input(I), weights_old, ... )
    
    weight_deltas = backpropagaition( neuron_outputs, weighted_sums, ... ) 
    
    weight_deltas_sum = weight_deltas_sum + weight_deltas
    
    end;
    
    weights = weights_old + weight_deltas_sum 

The result that I get doing simple scalar function approximation looks like this:

[Parabola fit](http://imgur.com/MfzCPtd)

I am sure some of you have experienced something like this. I'd appreciate any further information on this issue.

Here is where some more details may be found:
[SO question](http://stackoverflow.com/questions/29973691/perceptron-learns-to-reproduce-just-one-pattern-all-the-time)",5,1
9,2015-5-2,2015,5,2,15,34lkg0,"Speech synthesis from collection of (audio sample, transcript) pairs?",https://www.reddit.com/r/MachineLearning/comments/34lkg0/speech_synthesis_from_collection_of_audio_sample/,Speedloaf,1430546431,"Hello everyone!

I'm new to the ML world and would greatly appreciate any guidance with a project I might soon undertake.    
    
I am wondering if it would be possible to take a set of audio samples of a particular person (and associated transcripts) and generate a synthesized voice.     

An issue I foresee would be handling words that I don't have a sample of. For instance, if I don't have a clip of the person saying ""banana"" but in my mind I can imagine a plausible sound for the word in their voice. How could I go about generating that ""plausible sound""?    

I imagine that in addition to developing a map of words that have an available transcription, I would also need to construct a mapping between specific letters (or combinations of letters) and a phonetic sample for that sub-word element so I can generate new words (like banana).    

I would appreciate any guidance, links, terminology, or papers which might be relevant that I should investigate.    

Thanks in advance!",5,0
10,2015-5-2,2015,5,2,18,34ly4j,ML challenge: Model Reuse with Bike rental Station data,https://www.reddit.com/r/MachineLearning/comments/34ly4j/ml_challenge_model_reuse_with_bike_rental_station/,szampa,1430560053,,0,1
11,2015-5-2,2015,5,2,18,34lyl2,EM vs gradient descent,https://www.reddit.com/r/MachineLearning/comments/34lyl2/em_vs_gradient_descent/,letitgo12345,1430560565,"In what cases is it better to use EM vs just simple gradient descent (using Monte Carlo approximation if you've integrals in your likelihood function)

The general context is that I was reading the paper here (http://www.cs.toronto.edu/~tang/papers/sfnn.pdf) and they present a generalized EM algorithm for training stochastic feedforward neural nets and I was wondering if I'm just being dumb and using gradient descent isn't possible in this context or if not, then if there's a well known reason why gradient descent wouldn't work well here.",16,15
12,2015-5-2,2015,5,2,20,34m42d,Autoencoders for image classification,https://www.reddit.com/r/MachineLearning/comments/34m42d/autoencoders_for_image_classification/,cenkbircanoglu,1430566153,,0,1
13,2015-5-2,2015,5,2,20,34m44j,Understanding ripples in my learning curve.,https://www.reddit.com/r/MachineLearning/comments/34m44j/understanding_ripples_in_my_learning_curve/,donnaprima,1430566206,"I have a [learning curve](https://drive.google.com/file/d/0B-vJfhWswxvdanVSN1pMMHE4MEk/view?usp=sharing) that from time to time develops ripples, that fade away. What could be causing this?

Increasing momentum often increases the amplitude of these ripples.",13,1
14,2015-5-2,2015,5,2,22,34mbh2,How can I perform symbolic differentiation on a function with indexed terms?,https://www.reddit.com/r/MachineLearning/comments/34mbh2/how_can_i_perform_symbolic_differentiation_on_a/,fuckinghelldad,1430572432,"I ask because I'm inexperienced with symbolic differentiation programs and Google returns no results which seem useful to me.

The problem is that I'm trying to differentiate a function. It's too complex for me to differentiate by hand, so I need a computer to do it for me. However, the function contains a summation whose summands' factors are indexed (i.e. when you write them down they contain subscripts). If it's even possible, I'd like to know how to differentiate such a function symbolically.

I've considered automatic differentiation and numerical approximations for my application, though they seem like lesser alternatives.

Thanks!

**Edit:** /u/carette has suggested I give an example: http://i.imgur.com/JxaYLkG.png",5,2
15,2015-5-2,2015,5,2,23,34mflu,roc area score (classification): weka vs scikit-learn. Is there a difference?,https://www.reddit.com/r/MachineLearning/comments/34mflu/roc_area_score_classification_weka_vs_scikitlearn/,[deleted],1430575310,"Do this scores calculate exactly the same thing in the exact same way?

- in scikit-learn this is called roc_auc: 

- in weka it is called ROC area (in the gui)

The weird thing is that WEKA's ROC area seems to be always higher (upto 0.05) than scikit-learns roc_auc, even when they use exactly the same data and both use logistic regression with exactly the same parameters.

So the thing is, I'd prefer to use my scikit-learn solution, as I feel like I have more control over what happens. But the Weka solution gives a better score. So I wanted to know if they are comparable or that there will be real differences for predicting new items.

",3,0
16,2015-5-3,2015,5,3,0,34mobi,Text detection on images,https://www.reddit.com/r/MachineLearning/comments/34mobi/text_detection_on_images/,SuperGore,1430580663,"What are possible ways to improve a method (Naive Bayes for example) to get better results classifying images into text or non-text images, instead of just inputing a x number of images and telling the system which have text and which do not?

Thank you for the help reddit
Regards",3,8
17,2015-5-3,2015,5,3,1,34mwoo,How to deal with very skewed distribution of training instances,https://www.reddit.com/r/MachineLearning/comments/34mwoo/how_to_deal_with_very_skewed_distribution_of/,onewugtwowugs,1430585253,"I'm having a classification problem where I am to train a neural network on a set of sentences to predict one out of about ten possible words, based on the content of the sentences as well as a few other features. One of my problems is that the class distribution is very skewed, where some of the classes only occur in a few percent of the entire training set, while others could be up towards 60%. 

I was wondering if there are some general guidelines for dealing with this type of problem? Are there some cost functions or activation functions that are better than others, or do you have tips of other tricks to get around the worst of it?

Thanks",11,7
18,2015-5-3,2015,5,3,4,34nfeh,Matlab NN: Problem with my backProp Algo,https://www.reddit.com/r/MachineLearning/comments/34nfeh/matlab_nn_problem_with_my_backprop_algo/,WorkingoNN,1430595024,"Dear Redditors,
i'm an undergrad writing his thesis about financial forecasting using ANNs. I try to write as much code as possible by myself, so I can understand whats happening. The Input data is X (2517x11) where the first column is the bias unit, Y is the binary output data, Theta(j) are matrizes containing the weights, iter=iterations and LR=learning rate. When i let this rum a few thousand iterations Output J (which is the cost function) begins to oscilate, so i think i just found a local minima. Maybe theres something wrong with my gradient descent backprop Algo? I guess its the updating or delta rule part. Heres the code: 
",7,0
19,2015-5-3,2015,5,3,4,34nico,Best way to train image classifier model?,https://www.reddit.com/r/MachineLearning/comments/34nico/best_way_to_train_image_classifier_model/,bears343,1430596583,"Let's say you are trying to predict what type of vehicle is in an image. You can assume that all images will have the vehicle as the main object in the image (so mostly center and infront of everything), but there will not be a plain background. For example, a car may be on the street, or a tractor may be on a field of grass with blue sky above. 

Given that you want to classify the vehicle type, is it better to train a CNN on images with no filtering, then when classifying a new image, to classify on images with the noisy background? Or is it better to run some object detection / graph cut algorithm to get the main object, then train on that for all images, then at test time filter the image and classify using the same CNN?

Example1: http://nadacarvalue.org/wp-content/uploads/2014/11/nada-car-value.jpg
Example2:http://car-pictures.cars.com/images/?IMG=cac10foc051c01401.png&amp;WIDTH=624&amp;AUTOTRIM=1&amp;SPECIAL=&amp;ACT=F",1,0
20,2015-5-3,2015,5,3,5,34nkj9,First steps in data science: author-aware sentiment analysis,https://www.reddit.com/r/MachineLearning/comments/34nkj9/first_steps_in_data_science_authoraware_sentiment/,yanirse,1430597750,,0,2
21,2015-5-3,2015,5,3,7,34nz0n,How would you describe your one typical day at work?,https://www.reddit.com/r/MachineLearning/comments/34nz0n/how_would_you_describe_your_one_typical_day_at/,warriorkitty0,1430605786,"I'm a software engineer for a company where I mostly do BI development in CakePHP and ExtJS. I'm interested in ML, AI, recommendation algorithms, etc.


I'm from Croatia and I'm finishing college next year. My problem is that there are no jobs related to ML here and I can't find internship in ML (even though I'm senior in a company where I work now).


After college (next year), my plan is to move to Germany and start looking for some sort of ML job and they will (for sure) ask me where I've worked in this field. Since I don't have any professional experience in this field, I would like to start doing open source projects related to machine learning to ""mimic"" a professional day-to-day job.


If I could describe my typical day to a person who asked me about web development in CakePHP and ExtJS I would say:

1.) Add another function to your API for Android apps.

2.) Write a few unit tests for your new functions.

3.) Add another column inside the Users grid in ExtJS.

4.) Make an AJAX call to get all the active users and paginate them.

...

(you get the point)


From my answers, you can see really important keywords like unit tests, API, grids, AJAX, pagination, etc.


Could you describe me your normal day as data scientists and ML professionals so I could create my own tasks and solve them in my free time?


Thank you very much!",14,21
22,2015-5-3,2015,5,3,8,34o520,CNNs: What happened at ~130 epoch ?,https://www.reddit.com/r/MachineLearning/comments/34o520/cnns_what_happened_at_130_epoch/,Tom-Demijohn,1430609292,"[plot](http://postimg.org/image/3lebpsqip/)

So my impression is that somehow, at late point in time, model escaped local minima/plateau. Right?",24,6
23,2015-5-3,2015,5,3,8,34o5eb,Best Deep Learning Packages For Python?,https://www.reddit.com/r/MachineLearning/comments/34o5eb/best_deep_learning_packages_for_python/,YourWelcomeOrMine,1430609489,"I have googled this question, and realize there is an abundance of available packages. What are some of your experiences with the different packages? Which do you recommend/not recommend? ",4,2
24,2015-5-3,2015,5,3,16,34p734,Machine Learning for Emoji Trends (Instagram),https://www.reddit.com/r/MachineLearning/comments/34p734/machine_learning_for_emoji_trends_instagram/,sko2sko,1430638333,,11,66
25,2015-5-3,2015,5,3,20,34piyi,Why can Constant Error Carousels (CECs) prevent LSTM from the problems of vanishing/exploding gradients?,https://www.reddit.com/r/MachineLearning/comments/34piyi/why_can_constant_error_carousels_cecs_prevent/,freemind2009,1430652071,,5,2
26,2015-5-4,2015,5,4,0,34q50i,[Question] developing/testing Basic Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/34q50i/question_developingtesting_basic_recurrent_neural/,RossoFiorentino,1430668398,"I have been writing my own neural network package and i have until now only worked with feed-forward neural networks. I might be interested in extending my work towards recurrent neural networks. I have never worked with recurrent neural networks in practice, i have only read theory. 

I was wondering if there exists some basic one hidden layer neural network problem for developing, debugging and testing? For example similar to the and/or/xor problems for feed forward neural networks.

And maybe some basic data set like the MNIST data set for feed forward nets?",4,5
27,2015-5-4,2015,5,4,4,34qw21,"Follow up on ""Competing in a data science contest without reading the data""",https://www.reddit.com/r/MachineLearning/comments/34qw21/follow_up_on_competing_in_a_data_science_contest/,RinzeWind,1430682464,"This is very related to [this other thread](https://www.reddit.com/r/MachineLearning/comments/2ypyt7/competing_in_a_data_science_contest_without/).

In Kaggle's [*Restaurant Revenue Prediction*](https://www.kaggle.com/c/restaurant-revenue-prediction) challenge, a team has managed to obtain a perfect score following the same strategy. [Here is the leaderboard with their perfect entry](https://www.kaggle.com/c/restaurant-revenue-prediction/leaderboard) and [here is the forum post talking about their strategy](https://www.kaggle.com/c/restaurant-revenue-prediction/forums/t/13950/our-perfect-submission). They will hopefully follow up with a detailed explanation after the competition ends.",10,25
28,2015-5-4,2015,5,4,7,34rfc1,Canova: A Vectorization Lib for Machine-Learning Tools (WIP),https://www.reddit.com/r/MachineLearning/comments/34rfc1/canova_a_vectorization_lib_for_machinelearning/,vonnik,1430692318,,3,7
29,2015-5-4,2015,5,4,11,34s5de,Need a new idea for a little application to demonstrate my prowess in machine learning.,https://www.reddit.com/r/MachineLearning/comments/34s5de/need_a_new_idea_for_a_little_application_to/,[deleted],1430706506,"I am making a little application for a potential job in machine learning and my first idea fell through because of how Facebook set up their app permissions. Here is what I *was* going to do. 

1. User is able to log in with Facebook. 
2. Get the user's likes and their friends likes. 
3. Get the user's ""best friends"" (i.e. the top 5 people by the number of shared likes). &lt;-- This is the step that fell through because Facebook no longer lets an app view friends and their likes.
4. Suggest new restaurants to like to the user based on what the user's best friends liked. This would demonstrate my use of collaborative filtering / recommendation system. 
5. ... Profit? 


I could try to do the same thing with Twitter, but I can't understand how to use their API just yet... Any other suggestions to specifically demonstrate my knowledge of recommendation systems in an interactive manner? ",4,1
30,2015-5-4,2015,5,4,13,34shoe,Interview question- how did I do?,https://www.reddit.com/r/MachineLearning/comments/34shoe/interview_question_how_did_i_do/,mltape,1430713710,"I had an interview where some of the interviewers set up the following scenario:

You are trying to predict how many days a certain employee of a company will stay with that company.  You have three models that predict an output of 80 days, 90 days, and 100 days.  If the CEO of a company will not hire anyone that will stay under 85 days, do you reccomend hiring this employee? 


My answer was yes, you do if you mix the models and the output is more than 85 days, which is essentially a small ensemble method.  My reasoning was that if you made a couple models that you considered to be applicable, they would probably have similar errors, and mixing them would probably produce a better output with a lower overall error.  They didn't like that answer...the answer they were looking for was to pick the single model with the lowest error and go with that.  Was my reasoning out of line? ",4,4
31,2015-5-4,2015,5,4,16,34svtp,Data Scientists Thoughts that Inspired Me !,https://www.reddit.com/r/MachineLearning/comments/34svtp/data_scientists_thoughts_that_inspired_me/,HappyDataScientist,1430724996,,0,1
32,2015-5-4,2015,5,4,17,34sxb8,"The connection between sensing, signal processing and machine learning",https://www.reddit.com/r/MachineLearning/comments/34sxb8/the_connection_between_sensing_signal_processing/,compsens,1430726470,,0,11
33,2015-5-4,2015,5,4,19,34t6ma,Datumbox Machine Learning Framework 0.6.0 Released,https://www.reddit.com/r/MachineLearning/comments/34t6ma/datumbox_machine_learning_framework_060_released/,datumbox,1430735748,,0,0
34,2015-5-4,2015,5,4,19,34t7sm,What is the state of the art on recommender system applications for e-commerce?,https://www.reddit.com/r/MachineLearning/comments/34t7sm/what_is_the_state_of_the_art_on_recommender/,RecSysDude,1430736798,"Hi everyone.

I've been looking at [this](http://link.springer.com/chapter/10.1007/978-1-4615-1627-9_6) and [this](http://linkinghub.elsevier.com/retrieve/pii/S0167923615000627) that sort of have surveys around modern applications of recommender systems for ecommerce. Nonetheless, the first paper is from 2001 and even though the 2nd is from 2015, the applications presented in the survey section for ecommerce are ~10 years old.

I'm mostly interested in the applicability ~~space~~ of Recommender Systems and not so much around the technical details of the different algorithms. 

Any papers worth looking at?

Thank you :)",3,1
35,2015-5-4,2015,5,4,22,34tjk7,"RapidMiner Boosts Security, Collaboration &amp; Extensibility in Big Data with Latest Platform Updates",https://www.reddit.com/r/MachineLearning/comments/34tjk7/rapidminer_boosts_security_collaboration/,[deleted],1430745003,,0,1
36,2015-5-4,2015,5,4,22,34tl7d,Why so awfully deep? [Google+ repost],https://www.reddit.com/r/MachineLearning/comments/34tl7d/why_so_awfully_deep_google_repost/,tabacof,1430745958,"This interesting question was posted in [Google+ Deep Learning community](https://plus.google.com/u/0/+HouYunqing/posts/LuBVhALFrnN) by Hou Yunqing but there wasn't a single answer there, so I'd like to see what you guys think about it (this community usually gives great answers):

&gt; Why so awfully deep?

&gt; Recent ImageNet CNN architectures such as GoogLeNet, NIN, and VGG's net are usually really deep, having 10-20 convolutional layers. I can't help but ask: why so deep?

&gt; When I think about a conv layer, I think about composition. That is, a conv filter composes multiple ""parts"" into a ""whole"". In the early conv layers (1-3) this is clearly true. In the later layers it's less clear what is going on.

&gt; My hypothesis is that in the later convs the composition transits from ""hierarchical"" to ""combinatorial. That is, they become more like those kernel classifiers and no longer give us the exponentially better representation power they are supposed to deliver.

&gt; What do you guys think?",13,30
37,2015-5-4,2015,5,4,23,34trgw,What is the best way to calculate similarity among discussion trees?,https://www.reddit.com/r/MachineLearning/comments/34trgw/what_is_the_best_way_to_calculate_similarity/,tocunha,1430749425,"I am trying to cluter 80.000 trees that represent discussion cascades of reddit, I tried to use the tree edit distance (http://www.inf.unibz.it/dis/projects/tree-edit-distance/) as my similarity distance, but unfortunatelly this algorithm is O(n) and is not feasible to my context. What you guys suggest as similarity measure?",0,0
38,2015-5-4,2015,5,4,23,34trvq,ML/NLP competition to predict Boston hygiene violations using Yelp reviews,https://www.reddit.com/r/MachineLearning/comments/34trvq/mlnlp_competition_to_predict_boston_hygiene/,isms_,1430749638,,0,3
39,2015-5-5,2015,5,5,0,34u0go,"Yann LeCun: What's so great about ""Extreme Learning Machines""?",https://www.reddit.com/r/MachineLearning/comments/34u0go/yann_lecun_whats_so_great_about_extreme_learning/,downtownslim,1430753712,"Yann LeCun's thoughts about ELMs:

&gt; What's so great about ""Extreme Learning Machines""?
&gt; 
&gt; There is an interesting sociological phenomenon taking place in some corners of machine learning right now. A small research community, largely centered in China, has rallied around the concept of ""Extreme Learning Machines"".
&gt; 
&gt; Frankly, I don't understand what's so great about ELM. Would someone please care to explain?
&gt; 
&gt; An ELM is basically a 2-layer neural net in which the first layer is fixed and random, and the second layer is trained. There is a number of issues with this idea.
&gt; 
&gt; First, the name: an ELM is *exactly* what Minsky &amp; Papert call a Gamba Perceptron (a Perceptron whose first layer is a bunch of linear threshold units). The original 1958 Rosenblatt perceptron was an ELM in that the first layer was randomly connected.
&gt; 
&gt; Second, the method: connecting the first layer randomly is just about the stupidest thing you could do. People have spent the almost 60 years since the Perceptron to come up with better schemes to non-linearly expand the dimension of an input vector so as to make the data more separable (many of which are documented in the 1974 edition of Duda &amp; Hart). Let's just list a few: using families of basis functions such as polynomials, using ""kernel methods"" in which the basis functions (aka neurons) are centered on the training samples, using clustering or GMM to place the centers of the basis functions where the data is (something we used to call RBF networks), and using gradient descent to optimize the position of the basis functions (aka a 2-layer neural net trained with backprop).
&gt; 
&gt; Setting the layer-one weights randomly (if you do it in an appropriate way) can possibly be effective if the function you are trying to learn is very simple, and the amount of labelled data is small. The advantages are similar to that of an SVM (though to a lesser extent): the number of parameters that need to be trained supervised is small (since the first layer is fixed) and easily regularized (since they constitute a linear classifier). But then, why not use an SVM or an RBF net in the first place?
&gt; 
&gt; There may be a very narrow area of simple classification problems with small datasets where this kind of 2-layer net with random first layer may perform OK. But you will never see them beat records on complex tasks, such as ImageNet or speech recognition.
&gt; 
&gt; http://www.extreme-learning-machines.org/",28,48
40,2015-5-5,2015,5,5,1,34u9kv,Word2Vec for C#,https://www.reddit.com/r/MachineLearning/comments/34u9kv/word2vec_for_c/,Dark_Messiah,1430757881,"Hey all, does anyone know where I can get a c# implemtation or library of Word2Vec for C#? Any help apreciated.",1,0
41,2015-5-5,2015,5,5,2,34udxk,Ordinal regression for ranking customers,https://www.reddit.com/r/MachineLearning/comments/34udxk/ordinal_regression_for_ranking_customers/,PoddyOne,1430759857,"I was recently made aware of ordinal regression through a post here, unfortunately I can't find the original link.

My question is, is ordinal regression ever a sensible approach when one wants to rank individuals. Example: You want to rank your customers from 1-&gt;N in order of those most likely to respond to marketing. 

Most examples I've found have ordinal regression with the dependent being in a small number of categories, but here there are the same number of categories as people to be ranked.

I'm not suggesting this should be the approach, I'm just curious as to if this makes any sense at all.. and if so, does it just amount to choosing a loss function to define the 'relative ranking' of two customers.",7,1
42,2015-5-5,2015,5,5,2,34ueht,Looking for an R equivalent of sklearn's PolynomialFeatures function,https://www.reddit.com/r/MachineLearning/comments/34ueht/looking_for_an_r_equivalent_of_sklearns/,proxyformyrealname,1430760114,,7,2
43,2015-5-5,2015,5,5,2,34uejv,Azure Machine Learning: simplified predictive analytics,https://www.reddit.com/r/MachineLearning/comments/34uejv/azure_machine_learning_simplified_predictive/,alexcasalboni,1430760139,,0,0
44,2015-5-5,2015,5,5,4,34uvub,KDD CUP 2015 - MOOC dropout prediction,https://www.reddit.com/r/MachineLearning/comments/34uvub/kdd_cup_2015_mooc_dropout_prediction/,pilooch,1430767897,,8,18
45,2015-5-5,2015,5,5,5,34v7u0,What are some interesting papers that use convolutional neural networks in video?,https://www.reddit.com/r/MachineLearning/comments/34v7u0/what_are_some_interesting_papers_that_use/,lumuba,1430773181,,4,5
46,2015-5-5,2015,5,5,6,34vce1,Interpreting Fourier transforms of Time Series ?,https://www.reddit.com/r/MachineLearning/comments/34vce1/interpreting_fourier_transforms_of_time_series/,muktabh,1430775197,"I am trying to use Fourier Transforms to cluster time series. The idea is to try and use Fourier Transforms of time series in distance matrix of a clustering algorithm in place of the occurrence vectors.
The potential use cases may include things like telling which are busier traffic signals and less busy ones if number of cars passing through them every minute is known.
For example I would consider traffic signals 2,0,0,0,0,2 and 1,1,0,0,1,1 busier as compared to 1,0,0,0,0,1. Now if we try to apply k-Means over data like this, instead of passing just the total number of cars passing, we might want to ""deorder"" the series into frequency domain using Fourier transform and then give it to distance matrix.
First of all does this line of thought make sense at all ? When I do it for some sample data, I can see that the Fourier transforms do give some estimate of both total frequency and the spread of the data. Also if it makes sense, what might be the possible real world interpretation of such Time Series transforms ?
I have time series in discrete numbers per unit time:

julia&gt; map(x-&gt;x^2 , abs(fft([1,1,0,0,1,1])))
6-element Array{Float64,1}:
 16.0
  3.0
  1.0
  0.0
  1.0
  3.0

julia&gt; map(x-&gt;x^2 , abs(fft([2,0,0,0,0,2])))
6-element Array{Float64,1}:
 16.0
 12.0
  4.0
  0.0
  4.0
 12.0

julia&gt; map(x-&gt;x^2 , abs(fft([1,0,0,0,1,0])))
6-element Array{Float64,1}:
 4.0
 1.0
 1.0
 4.0
 1.0
 1.0

julia&gt; map(x-&gt;x^2 , abs(fft([1,0,1,1,1,0])))
6-element Array{Float64,1}:
 16.0
  1.0
  1.0
  4.0
  1.0
  1.0

julia&gt; map(x-&gt;x^2 , abs(fft([3,0,0,2,0,1])))
6-element Array{Float64,1}:
 36.0
  3.0
 21.0
  0.0
 21.0
  3.0",5,0
47,2015-5-5,2015,5,5,7,34vk21,[1504.08215] Lateral Connections in Denoising Autoencoders Support Supervised Learning,https://www.reddit.com/r/MachineLearning/comments/34vk21/150408215_lateral_connections_in_denoising/,[deleted],1430778645,,0,1
48,2015-5-5,2015,5,5,8,34vny6,Standard problem sets for metaheuristics,https://www.reddit.com/r/MachineLearning/comments/34vny6/standard_problem_sets_for_metaheuristics/,[deleted],1430780595,"I'm wanting to dabble with metaheuristics and am interested to know what the ""hello world"" problem sets are.

In other words, what are the common problems (e.g Traveling Salesman, Vehicle Routing Problem, Maze Solving) to test various algorithms against in order to compare algorithm design?",3,2
49,2015-5-5,2015,5,5,9,34vz99,When will the next Andrew Ng ML class on coursera start?,https://www.reddit.com/r/MachineLearning/comments/34vz99/when_will_the_next_andrew_ng_ml_class_on_coursera/,assassin_of_damned,1430786253,,4,0
50,2015-5-5,2015,5,5,12,34witd,A Recurrent Neural Network Based Alternative to Convolutional Networks,https://www.reddit.com/r/MachineLearning/comments/34witd/a_recurrent_neural_network_based_alternative_to/,downtownslim,1430796469,,11,23
51,2015-5-5,2015,5,5,13,34wo63,clustering datapoints with chinese restaurant process,https://www.reddit.com/r/MachineLearning/comments/34wo63/clustering_datapoints_with_chinese_restaurant/,koormoosh,1430799655,"

If I have data points (scalar) and want to incorporate each data points characteristics into how its being clustered, is there a way of doing it? I assume the standard generative process of CRP is not going to be enough since it ignores the data, picks its cluster and then generates the data from the distribution of each component (cluster) it falls into.
",2,0
52,2015-5-5,2015,5,5,13,34wppd,Deep Learning for Decision Making and Control,https://www.reddit.com/r/MachineLearning/comments/34wppd/deep_learning_for_decision_making_and_control/,MyMomSaysImHot,1430800631,,1,27
53,2015-5-5,2015,5,5,15,34wwdw,A Statistical View of Deep Learning (IV): Recurrent Nets and Dynamical Systems,https://www.reddit.com/r/MachineLearning/comments/34wwdw/a_statistical_view_of_deep_learning_iv_recurrent/,iori42,1430805741,,0,10
54,2015-5-5,2015,5,5,15,34wzqn,On the evolution of machine learning,https://www.reddit.com/r/MachineLearning/comments/34wzqn/on_the_evolution_of_machine_learning/,codefly212,1430808458,,6,5
55,2015-5-5,2015,5,5,18,34x8au,Reinforcement Learning Neural Turing Machines,https://www.reddit.com/r/MachineLearning/comments/34x8au/reinforcement_learning_neural_turing_machines/,clbam8,1430816590,,1,21
56,2015-5-5,2015,5,5,19,34xcnc,How to use hierarchical tags associated with data in a linear regression?,https://www.reddit.com/r/MachineLearning/comments/34xcnc/how_to_use_hierarchical_tags_associated_with_data/,sunilnandihalli,1430820647,"Let us say I have a predictor vector X and to-be-predicted value Y. One of the predictors is a ""set of tags"" which are selected from a predefined hierarchical tree of tags.  I am trying to build a linear model to predict the value of Y. How can I use the set of tags as a feature? If a,b,c are three possible tags, and either of 'b' or 'c' automatically implies the presence of tag 'a' since the path from 'c' to the root of the hierarchy contains 'a' along the way. It should also be noted that all the tags need not lie on the same path to the root. I initially considered just having an indicator-vector but that would not capture the tree-structure of the categories. I would love to hear any input you may have. Thanks in advance.

EDIT : Another way of asking the same question is how would I use the background-knowledge (or ontology) in some form of regression. There seems to be so much literature about to how to infer ontologies.. but less about how to use the ontology that is already known.. Looks like I am missing something very fundamental. I am all ears for any advice the community may have...",4,2
57,2015-5-5,2015,5,5,19,34xegg,"The Great Convergence: Deep Learning, Compressive Sensing, Advanced Matrix Factorization (x-post: r/CompressiveSensing )",https://www.reddit.com/r/MachineLearning/comments/34xegg/the_great_convergence_deep_learning_compressive/,compsens,1430822285,,2,3
58,2015-5-5,2015,5,5,23,34y2nk,"The ELM Scandal, a formal complaint launched against Extreme Learning Machines",https://www.reddit.com/r/MachineLearning/comments/34y2nk/the_elm_scandal_a_formal_complaint_launched/,downtownslim,1430837359,,45,69
59,2015-5-6,2015,5,6,0,34ybh9,Can we compress any Neural Network to 3 Layers?,https://www.reddit.com/r/MachineLearning/comments/34ybh9/can_we_compress_any_neural_network_to_3_layers/,erogol,1430841492,Learning a multi-layer neural network is learning a way of data projection per layer. Then I believe we can just compress all these different transformations a s a single layer and apply the model with a simple 3 layer structure. What do you think about the validity of this idea?,1,0
60,2015-5-6,2015,5,6,1,34yc6f,What could be wrong with my LSTM implementation?,https://www.reddit.com/r/MachineLearning/comments/34yc6f/what_could_be_wrong_with_my_lstm_implementation/,ticcky,1430841781,"I'm trying to compare my Theano LSTM implementation to some reference one to verify the implementation's correctness, and I chose [1].

However, on the LM task in [1], my final perplexity is about 10 points higher than the one in [1], so I'm trying to find out why. 

Also the convergence is much slower (350 perplexity (after first epoch) -&gt; 330 (after second, ...) -&gt; 280 ..., whereas [1] converges much better(180 (after first epoch) -&gt; ...). 

One interesting thing I noticed is that my gradient does not need clipping, whereas if I remove gradient clipping from [1], it explodes.

Do you have any ideas what could possibly be wrong? Thanks!

[1] https://github.com/wojzaremba/lstm/blob/master/main.lua",1,8
61,2015-5-6,2015,5,6,1,34yein,Thoughts on duration modelling w.r.t. RNNs?,https://www.reddit.com/r/MachineLearning/comments/34yein/thoughts_on_duration_modelling_wrt_rnns/,sidsig,1430842836,"Hi all. Just wanted to see what different people thought about this problem. Recent work in NLP has shown impressive results with RNNs (language modelling, language translation etc). However, these problems involve mainly modelling the transition probability between states. For example for language modelling, the number of occurrences of a repeating word will be negligible. 

However, there are several problems where the symbols in the discrete space being modelled repeat themselves. For example RNN phoneme models when used for phone recognition have to deal with the problem of repeating phonemes over frames (Boulanger-Lewandowski et. al.). Similarly, RNNs when used to model frames of MIDI music have to model repeating frames (Suskever, Boulanger etc). 

I was wondering if there is any literature on duration modelling and RNN states. What kind of duration models are implicitly learnt by RNNs? HMMs implicitly learn exponentially decaying distributions for state distributions. Is there any way to infer what duration distributions RNNs learn? Are there any simple ideas that could be tried to improve/evaluate their duration modelling capabilities?


",4,3
62,2015-5-6,2015,5,6,2,34ynig,How to make a Hacker News classifier with MonkeyLearn,https://www.reddit.com/r/MachineLearning/comments/34ynig/how_to_make_a_hacker_news_classifier_with/,wildcodegowrong,1430846955,,0,0
63,2015-5-6,2015,5,6,2,34ynks,Recursive Neural Networks Can Learn Logical Semantics,https://www.reddit.com/r/MachineLearning/comments/34ynks/recursive_neural_networks_can_learn_logical/,evc123,1430846982,,0,4
64,2015-5-6,2015,5,6,2,34yo0u,Simple and common clustering Problem. Need an answer,https://www.reddit.com/r/MachineLearning/comments/34yo0u/simple_and_common_clustering_problem_need_an/,alok29,1430847166,,3,0
65,2015-5-6,2015,5,6,2,34yqig,Need help for school assignment,https://www.reddit.com/r/MachineLearning/comments/34yqig/need_help_for_school_assignment/,[deleted],1430848260,"I'm working on an assignment together with a coulpe of friends, with our subject being A.I. My task was to write about machinelearning, and I immediately remembered seeing a video about the subject. In the video (which was captured from some sort of a 3D engine/software) had a cube in the middle and on either side a bunch of geometrical shapes stitched together to form a ...thing (I dunno). The objective was to reach the cube first. The one that won, survived, while the other one was terminated and replaced with a new version. Over time the things(?) evolved into complex machineries that countered each other and so forth. I've tried to find the video, but without a result, so I thought maybe someone here remembers seeing something like that.

TL;DR: Need to find video where things try to reach box, winner survives, evolution simulated through software.

Also, English is not my first language.... Yeah.",2,0
66,2015-5-6,2015,5,6,4,34z3k6,What're the funniest/original mdp you have ever seen?,https://www.reddit.com/r/MachineLearning/comments/34z3k6/whatre_the_funniestoriginal_mdp_you_have_ever_seen/,Aumanidol,1430853954,"I want to open a little blog to show examples of simple and funny mdp and Ai examples for the students I was assigned to help.

Also, an informal post was lacking from the frontpage of the sub...so why not?

my personal candidate:  super mario and td learning (this paper lacks a formal model,but still is awesome)http://x3ro.de/downloads/MarioSarsa.pdf and all of the Mario AI Championship. 

what's yours?",4,3
67,2015-5-6,2015,5,6,5,34za7q,Advice on machine learning task.,https://www.reddit.com/r/MachineLearning/comments/34za7q/advice_on_machine_learning_task/,celeritas365,1430856920,"I am an intermediate level programmer and I am excited to learn about machine learning. I recently got a position (unpaid) as the IT specialist for my job (our organization is also a bit of a club/volunteer organization). Right now, shifts have start times and no end times, which I think is really inconvenient for our members. Unfortunately, we are not told end times. On the bright side, we know how long shifts took after they were worked and similar shifts take similar amounts of time. Factors like day of the week, words in the shift title, and start time influence the length of the shift. I want to make a program that is integrated into our shift picking program that predicts the length of shifts based on priors. I know a bit about bayesian networks and I at first I thought this would be a job for them. However, my knowledge of bayesian networks ends at random variables with a finite set of values and nothing in between. I am not looking for someone to solve my problem for me but I was wondering where I should start. I know bayesian networks can be used for this kind of thing. Is there something I could read? Am I headed in the wrong direction? Is there some sort of better method for predicting these values? Any help would be appreciated. Thanks in advance.",5,1
68,2015-5-6,2015,5,6,5,34zg0y,Help with music recommendations,https://www.reddit.com/r/MachineLearning/comments/34zg0y/help_with_music_recommendations/,OSUperson,1430859520,"One of the first ML tasks I'm working on is musical album recommendations based on a bunch of data I have about the albums. I feel like the genre information for the album is important, but I'm not sure how to represent that data in my model. The albums can also have one or more genres (e.g., ""Soul"" and ""Funk""). My initial thought was to translate each genre into a number (e.g.,"" Funk"" =&gt; 1, ""Soul""  =&gt; 2) but that seems to give more weight to certain genres which doesn't make much sense. 

I'm fairly new to this, so please let me know if any other information is necessary to help out. Thanks in advance!",3,0
69,2015-5-6,2015,5,6,8,3500xx,What are Extreme Learning Machines? [PDF] : includes the author's response to critics and comparisons to prior work,https://www.reddit.com/r/MachineLearning/comments/3500xx/what_are_extreme_learning_machines_pdf_includes/,[deleted],1430869478,,0,1
70,2015-5-6,2015,5,6,10,350a9n,"Published ELM overview, ""Trends in extreme learning machines: A review"". Make your own opinion! :)",https://www.reddit.com/r/MachineLearning/comments/350a9n/published_elm_overview_trends_in_extreme_learning/,test3545,1430874381,,15,10
71,2015-5-6,2015,5,6,16,351cdo,Is there a machine learning algorithm that can features when given raw data?,https://www.reddit.com/r/MachineLearning/comments/351cdo/is_there_a_machine_learning_algorithm_that_can/,[deleted],1430898985,Or is this more of a data science problem?,0,1
72,2015-5-6,2015,5,6,17,351eiz,Are RBMs/DBNs/DBMs obsolete?,https://www.reddit.com/r/MachineLearning/comments/351eiz/are_rbmsdbnsdbms_obsolete/,treebranchleaf,1430901150,"It seems that since people started using rectified-linear units in backprop, there's been no need for doing unsupervised pretraining with stacked Restricted Boltzmann Machines.  More recently, it seems that you can train generative models more easily using [variational autoencoders](http://arxiv.org/abs/1312.6114), with the added benefit that sampling from the model requires no MCMC.  In a recent [Quora post](http://www.quora.com/Are-there-general-principles-that-tell-me-when-to-use-a-stacked-Boltzmann-machine-versus-some-other-deep-learning-neural-network), Ian Goodfellow makes the bold claim that stacked Boltzmann Machines are obsolete.  So, my question is: Is there still a place for this class of models, of have they gone the way of the dinosaurs?",7,24
73,2015-5-6,2015,5,6,21,351yb3,Image Scaling using Deep Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/351yb3/image_scaling_using_deep_convolutional_neural/,feedtheaimbot,1430916914,,14,55
74,2015-5-6,2015,5,6,22,352458,"Have TEDs 2,000 Presenters Answer Your Deepest, Probing Questions With Help From IBM's Watson",https://www.reddit.com/r/MachineLearning/comments/352458/have_teds_2000_presenters_answer_your_deepest/,CaptainHoek,1430920055,,5,7
75,2015-5-6,2015,5,6,23,3529xm,Parallel Machine Learning with Hogwild!,https://www.reddit.com/r/MachineLearning/comments/3529xm/parallel_machine_learning_with_hogwild/,srikris_sridhar,1430923025,,11,6
76,2015-5-6,2015,5,6,23,352c6s,Find my own way on Machine Learning,https://www.reddit.com/r/MachineLearning/comments/352c6s/find_my_own_way_on_machine_learning/,Aerospacio,1430924133,"Help me learn Machine Learning, good books, articles, that you consider is a must read please. I have an engineering degree, so give me hardcore stuff.

Thank you!",1,0
77,2015-5-7,2015,5,7,0,352d61,"A hypothesis on what it means to ""understand"" something -- feedback welcome",https://www.reddit.com/r/MachineLearning/comments/352d61/a_hypothesis_on_what_it_means_to_understand/,chaddjohnson,1430924577,"I recently discovered the impressive demo at [clarifai.com](http://www.clarifai.com/) which allows you upload any photo and provides relevant tags based on things it finds in the photo. This software leverages a relatively new machine learning technique known as [Deep Learning](http://en.wikipedia.org/wiki/Deep_learning) which relies on artificial neural networks. It enables a machine to know what is in a photo, but I argue that a machine cannot yet understand what it sees in the photo.

I have a hypothesis on what it means to ""understand"" something versus merely having knowledge of a concept, and I am interested in feedback.

**In summary:** Understanding a concept requires the aggregate of having 1) knowledge of the singular concept's existence and 2) knowledge of and associations with closely-related singular concepts.

**In detail:**

1. A concept is physically stored as the weighted connections between neurons within a cluster of neurons (a ""neural cluster"").

2. Having knowledge of a single concept means that a neural cluster representing the concept (a ""concept cluster"") exists for that concept.

3. Understanding a concept requires having knowledge of related concepts as well as associations between the concept's concept cluster and the other concepts' concept clusters. Thus, the concept cluster is associated/linked via synapses with concept clusters for other, closely-related concepts.

4. A concept merely acts as a very concise summary of the other concepts linked to -- much like a word in human language is a fast way to reference something. Because a concept cluster is associated with other concept clusters, the whole of the linkage to other concept clusters is what actually defines a concept and gives it meaning.

5. Understanding allows for recall of a concept and a subset of its associated knowledge. The action of recalling a concept results in activation of multiple, closely-associated neural clusters. When one cluster is activated, it quite possibly (depending on signal strength) activates other strongly connected clusters (i.e., causes action potentials in multiple neurons within these clusters) in a cascade effect. Further, an activation feedback loop ([more info](http://www.reddit.com/r/neuro/comments/28xf1x/how_can_we_learn_things_so_quickly/cifhykf)) may occur as a byproduct of recall such that neurons are constantly reactivated over time with gradual activational decay. This feedback loop could be what is known as short-term memory.",11,2
78,2015-5-7,2015,5,7,1,352lqx,Text classification template now available at Azure ML,https://www.reddit.com/r/MachineLearning/comments/352lqx/text_classification_template_now_available_at/,MLBlogTeam,1430928528,,0,1
79,2015-5-7,2015,5,7,1,352mjm,"IBM's supercomputer Watson will be used to make decisions about cancer care in 14 hospitals in the US and Canada, it has been announced.",https://www.reddit.com/r/MachineLearning/comments/352mjm/ibms_supercomputer_watson_will_be_used_to_make/,CaptainHoek,1430928903,,0,2
80,2015-5-7,2015,5,7,1,352q0a,Audio features storage and search,https://www.reddit.com/r/MachineLearning/comments/352q0a/audio_features_storage_and_search/,tehgargoth,1430930500,Has anyone heard of someone attempting to archive audio features like MFCC/HMM and attempting to to search/aggregate on that data with keywords?,2,0
81,2015-5-7,2015,5,7,2,352sub,[1505.00853] Empirical Evaluation of Rectified Activations in Convolutional Network,https://www.reddit.com/r/MachineLearning/comments/352sub/150500853_empirical_evaluation_of_rectified/,j1395010,1430931807,"Comparison of ReLU, LReLU, PReLU and randomized LReLU (Insanity activation from the Kaggle NDSB contest).",19,7
82,2015-5-7,2015,5,7,3,3534tz,Your Prediction Gets As Good As Your Data [AI Optify],https://www.reddit.com/r/MachineLearning/comments/3534tz/your_prediction_gets_as_good_as_your_data_ai/,kjahan,1430937205,,1,0
83,2015-5-7,2015,5,7,4,3539vm,Artificial Intelligence Research Project - need players and survey data,https://www.reddit.com/r/MachineLearning/comments/3539vm/artificial_intelligence_research_project_need/,airesearchproject,1430939600,"Hey everyone,

I am a computer science student conducting a research project on Artificial Intelligence. Specifically, I am seeing how online game players perceive their opponents, as human or AI.

I have a short survey (2 questions that will take a minute at most) that I would appreciate any responses to: https://docs.google.com/forms/d/1EBdIY-22c_kAvUKLLvpKooEXXHsMYRSp7ODmE0ZjdU4/viewform?usp=send_form.

In addition, I have a small game of Checkers that people can play - you are randomly paired with either another human player or an AI. Then, at the end of the game, you are asked for some feedback (whether you think your opponent was human or not and any details). That is located here: https://panchr.me/focus.

Both the survey and game are very short and would greatly aid in my project, so I appreciate any responses (to either the game or the survey, or both). Thank you in advance!",0,0
84,2015-5-7,2015,5,7,4,353a5g,"Nervana Systems open sources Neon, a python-based deep learning framework",https://www.reddit.com/r/MachineLearning/comments/353a5g/nervana_systems_open_sources_neon_a_pythonbased/,[deleted],1430939720,,0,1
85,2015-5-7,2015,5,7,4,353foi,Do convolutional autoencoders exist?,https://www.reddit.com/r/MachineLearning/comments/353foi/do_convolutional_autoencoders_exist/,regularized,1430942187,I have recently read Schmidhuber's paper on Convolutional Autoencoders. Are there any new papers/advancements on conv autoencoders? If not do you know why? ,2,2
86,2015-5-7,2015,5,7,5,353lz0,"Neon, an open-source, Python-based, deep learning framework from Nervana Systems",https://www.reddit.com/r/MachineLearning/comments/353lz0/neon_an_opensource_pythonbased_deep_learning/,meepmeepmoopmoop,1430944975,,27,92
87,2015-5-7,2015,5,7,7,35433h,Anyone heard of Re Work Deep Learning Conference or know of a discount code for it?,https://www.reddit.com/r/MachineLearning/comments/35433h/anyone_heard_of_re_work_deep_learning_conference/,matlab484,1430952986,"Debating going, I'm a student so the prices are pretty expensive, has anyone gone to these before? Are they worth attending?

https://www.re-work.co/events/deep-learning-boston-2015",0,2
88,2015-5-7,2015,5,7,8,3544ij,Anyone down to review my resume (I'll return the favor if needed!)?,https://www.reddit.com/r/MachineLearning/comments/3544ij/anyone_down_to_review_my_resume_ill_return_the/,[deleted],1430953641,"Hey, PM me if you want to look at my resume and suggest any edits. I have a B.S. + M.S. in Applied Mathematics and a year in the industry, but I am looking into moving to California and find a Data Scientist or Machine Learning job. ",2,1
89,2015-5-7,2015,5,7,8,3546js,"Problem: to reliably detect a human, in nature, wearing blue jeans, from &gt;30m away, with sensors mounted on a quadcopter. How would you go about solving this?",https://www.reddit.com/r/MachineLearning/comments/3546js/problem_to_reliably_detect_a_human_in_nature/,about3fitty,1430954723,"Have been thinking - would OpenCV library with camera work? Is there a role for IR/UV/LIDAR? How would one overcome the problem of the camera being mounted on a moving quadcopter? How would you go about using Machine Learning for this task? Was exploring the use of Support Vector Machines to help solve this challenge, but don't have a good idea of where to start. Has anyone on this sub done something similar?",9,2
90,2015-5-7,2015,5,7,11,354plh,math degree for ML?,https://www.reddit.com/r/MachineLearning/comments/354plh/math_degree_for_ml/,dsocma,1430964743,"I want to be a researcher in deep learning.  I've thought about it for a while and I think the best undergrad education for this would be to get a math degree, heavy in stats, with a minor in CS, and specifically take the more fundamental theoretical CS classes like complexity theory, automata theory, ect, and skip the classes about compilers, software engineering, and other low level or practical stuff, and also take a few neuroscience classes, and also do practical machine learning on my own and take online machine learning MOOCs.

Does this sound like the best undergrad education?

For people that have a math degree, do you feel it was the best preparation? ",4,1
91,2015-5-7,2015,5,7,11,354sdg,ICML '15 Accepted Papers,https://www.reddit.com/r/MachineLearning/comments/354sdg/icml_15_accepted_papers/,goblin_got_game,1430966252,,0,29
92,2015-5-7,2015,5,7,13,3555q6,I could use a bit of help implementing a decision tree in SQL,https://www.reddit.com/r/MachineLearning/comments/3555q6/i_could_use_a_bit_of_help_implementing_a_decision/,[deleted],1430974172,"I've played around a bit with weka decision tree analysis and I've found a good combination of parameters that work well to classify my data. The issue is the number is number of nested case statements, SQL throws an error if the nest depth is greater than 10. 

I'm guessing that I'm just not doing it right. Are there tricks to implementing a highly nested decision tree in SQL I should know about?",5,0
93,2015-5-7,2015,5,7,16,355jqc,"The Past, Present, and Future of Machine Learning APIs",https://www.reddit.com/r/MachineLearning/comments/355jqc/the_past_present_and_future_of_machine_learning/,osroca,1430985377,,0,1
94,2015-5-7,2015,5,7,17,355man,ICLR 2015 Conference Schedule/Publication overview,https://www.reddit.com/r/MachineLearning/comments/355man/iclr_2015_conference_schedulepublication_overview/,AlcaDotS,1430987869,,2,2
95,2015-5-7,2015,5,7,18,355qm9,"The Past, Present, and Future of Machine Learning APIs",https://www.reddit.com/r/MachineLearning/comments/355qm9/the_past_present_and_future_of_machine_learning/,czuriaga,1430991971,,0,1
96,2015-5-7,2015,5,7,19,355scr,Machine Tools Suppliers,https://www.reddit.com/r/MachineLearning/comments/355scr/machine_tools_suppliers/,Md-Malik,1430993558,,0,1
97,2015-5-7,2015,5,7,21,3562o7,"Does anyone have thoughts/documentation on how to build a recommendation system (i.e. collaborative filtering, matrix factorization) for binary data (probability of 1 occurring) rather than ranking data (movie ratings etc)?",https://www.reddit.com/r/MachineLearning/comments/3562o7/does_anyone_have_thoughtsdocumentation_on_how_to/,TheRandomForest,1431001636,Most documentation seems to be around problems such as the Netflix prize which uses rankings of 1-5 - but my data is around which offer is best suited for each customer based on other offers as well as customer demographics...  ,8,0
98,2015-5-7,2015,5,7,21,356571,How does the twitter handle work ?,https://www.reddit.com/r/MachineLearning/comments/356571/how_does_the_twitter_handle_work/,deathbullet,1431003268,The twitter handle @mxlearn auto tweets if there is any update on the MachineLearning subreddit. How dies it work ?,2,0
99,2015-5-7,2015,5,7,23,356cv2,Richmond ECPI University Student Wows Faculty with Senior Project,https://www.reddit.com/r/MachineLearning/comments/356cv2/richmond_ecpi_university_student_wows_faculty/,taylorsmith22,1431007431,,0,1
100,2015-5-7,2015,5,7,23,356e76,Ask r/machinelearning: Is this CloudSight API just smoke and mirrors (humans) as opposed to deep learning as implied in their PR? Try it out yourself here.,https://www.reddit.com/r/MachineLearning/comments/356e76/ask_rmachinelearning_is_this_cloudsight_api_just/,MyMomSaysImHot,1431008152,,49,23
101,2015-5-8,2015,5,8,1,356swa,Who's in San Diego right now (@ICLR)?,https://www.reddit.com/r/MachineLearning/comments/356swa/whos_in_san_diego_right_now_iclr/,taion,1431015004,"Which posters are you excited about?

It's friggin' cold here.",8,2
102,2015-5-8,2015,5,8,3,357dwl,Is there a way to denote the non-independence of features in any machine learning framework? [cognitive neuroscience / neuroimaging application for spatiotemporal patterns of brain activity],https://www.reddit.com/r/MachineLearning/comments/357dwl/is_there_a_way_to_denote_the_nonindependence_of/,mobiuscydonia,1431024536,"Hey guys,

I've been using machine-learning in conjunction with neuroimaging for quite some time now.

The long and short of it is that:

1) I have time-series of brain data (~1 3D picture of the brain every second)

2) I denote activity-states of the brain based on onsets-of-interest (e.g at this point in the time-series they were doing a memory task)

3) separate each picture into voxels (3D pixels)

4) Use subsets of those voxels as features in a classification that aims to dissociate activity states from one another.

Typically, someone may be doing a task over the course of 10 seconds. In this case, we'd have 10 pictures. Normally, we confound this with a hemodynamic response function, sum, and end up with 1 set of features-- 1 average activity value for each voxel.

Recently, I've been playing around with skipping this part and feeding all 10 pictures in at once, so as to capture a spatio-temporal pattern of activity instead of just a spatial pattern.

While classification improves significantly, I was worried about the lack of independence amongst my features. That is, one feature is the same as another feature, just one time-point in the future.

Is this okay? If not (or even if it is), is there a way to ""bind"" certain features such that they can still contribute to a classifier, but the classifier knows about their non-independence?

I'm used to SVM, LASSO, and logistic regression, but would be more than happy to explore new classifiers.

Thanks for any and all insights!",6,0
103,2015-5-8,2015,5,8,4,357jo6,Detect the facing direction of a distorted rect (4 points given) in a Augmented Reality Manner,https://www.reddit.com/r/MachineLearning/comments/357jo6/detect_the_facing_direction_of_a_distorted_rect_4/,Indyaner,1431027107,"Hello

I'm working with a small pattern recognition class that is able to detect a QR Codes. I noticed that that class gives me the shape of the detected QR code and its position on the screen. [I draw a diagram to make this clearer.](http://i.imgur.com/Uhr5KZT.jpg). I could solve X by some geometry but how can I figure out the Direction/Angle the red axis would face? I would like to achieve something like a very simple Augmented Reality Effect.

Thanks in advance for any links.",0,0
104,2015-5-8,2015,5,8,4,357k9k,What approach should I use?,https://www.reddit.com/r/MachineLearning/comments/357k9k/what_approach_should_i_use/,[deleted],1431027365,"I'm looking at a project that would require me to determine if product descriptions across multiple websites (ex: amazon, ebay, etc...) are referring to the same product. I was wondering if anyone had some input on the best approach to tackle this problem.",0,0
105,2015-5-8,2015,5,8,5,357nu6,Is there a term for regularization to a non-zero value?,https://www.reddit.com/r/MachineLearning/comments/357nu6/is_there_a_term_for_regularization_to_a_nonzero/,rasmoo,1431028918,"Basically, I needed a method to keep the weights of an encoder matrix from becoming zero, since this is a trivial, but low-cost solution to my reconstruction error.

I came up with a 'regularization' cost based on summing up the squared difference between 1 and the 1-norm of each row in the matrix (sorry, no LaTeX here), although any vector norm could've been used. I've implemented it, and it works quite well. Now, this both achieves my initial goal and also results in a relatively sparse matrix, but now I'm left wondering whether anyone has done this before?",7,0
106,2015-5-8,2015,5,8,5,357qbl,List of Deep Learning Tools with brief descriptions and links,https://www.reddit.com/r/MachineLearning/comments/357qbl/list_of_deep_learning_tools_with_brief/,bigblue53,1431030026,,0,1
107,2015-5-8,2015,5,8,5,357qh1,The tensor renaissance in data science,https://www.reddit.com/r/MachineLearning/comments/357qh1/the_tensor_renaissance_in_data_science/,gradientflow,1431030089,,11,29
108,2015-5-8,2015,5,8,5,357qtj,Why do people use GPUs for calculations?,https://www.reddit.com/r/MachineLearning/comments/357qtj/why_do_people_use_gpus_for_calculations/,bwaxxlo,1431030248,"I've been following a few cool ML guys on twitter. A couple of months back, before being bought out by Google, one of them posted an image of his rig using a GPU for calculations. I thought video cards wouldn't be so good at performing such tasks (considering they were built to do rendering) but I can only speculate. So, what's the deal with using GPUs? Is it a non-intrusive memory thing where they avoid overloading the processor/machine-memory or is the GPU just better at dealing with certain ML tasks? If the answer is the latter, could someone expand on how GPU are better than the processor/RAM?",16,3
109,2015-5-8,2015,5,8,5,357rot,Free Open Source Bayesian Network Software,https://www.reddit.com/r/MachineLearning/comments/357rot/free_open_source_bayesian_network_software/,bigblue53,1431030648,,0,1
110,2015-5-8,2015,5,8,6,357x7s,"Convolutional NN. If loss is going down but accuracy i not increasing, what can I do?",https://www.reddit.com/r/MachineLearning/comments/357x7s/convolutional_nn_if_loss_is_going_down_but/,[deleted],1431033207,"I have CNN with softmax at the end. ~50k of images, binary classification. Loss graph looks like this:

http://i.imgur.com/gTffPBZ.png?1

Loss is going down, but no gain in accuracy.What can you recommend?




",2,1
111,2015-5-8,2015,5,8,6,3580q5,ARTMAP in Python?,https://www.reddit.com/r/MachineLearning/comments/3580q5/artmap_in_python/,bschu,1431034847,"Hello, I am having trouble finding an ARTMAP implementation in Python. Before I write one has anyone heard of ARTMAP in python source?",0,0
112,2015-5-8,2015,5,8,7,35854i,"Computational Network Toolkit (CNTK) from Microsoft: NN, CNN, RNN.",https://www.reddit.com/r/MachineLearning/comments/35854i/computational_network_toolkit_cntk_from_microsoft/,Foxtr0t,1431036932,"A new deep learning framework every day! Yesterday Neon, today this: http://cntk.codeplex.com/

182-page manual: http://research.microsoft.com/apps/pubs/?id=226641

Someone please translate the license to English for me.",3,7
113,2015-5-8,2015,5,8,10,358qr9,Getting Started with Deep Learning,https://www.reddit.com/r/MachineLearning/comments/358qr9/getting_started_with_deep_learning/,carmichael561,1431048071,,2,57
114,2015-5-8,2015,5,8,10,358u81,Interdisciplinary Data and Helping Humans Be Creative,https://www.reddit.com/r/MachineLearning/comments/358u81/interdisciplinary_data_and_helping_humans_be/,vkhuc,1431049907,,0,4
115,2015-5-8,2015,5,8,12,3593oz,Exposure to Diverse Information on Facebook,https://www.reddit.com/r/MachineLearning/comments/3593oz/exposure_to_diverse_information_on_facebook/,kunalj101,1431055137,,0,4
116,2015-5-8,2015,5,8,15,359iwb,Grad School Choices,https://www.reddit.com/r/MachineLearning/comments/359iwb/grad_school_choices/,a_technicolor_skye,1431065296,"Hi,

I've done a BEng in Aeronautics and am switching to ML. I have a background in MatLab and wrote my bachelor's thesis in Python. I have been accepted to UCL's MS ML and Columbia's MS CS programs. I'll be focused solely on ML in UCL while I'll have a more general education at Columbia as they have systems and theory requirements. I'm concerted about my lack of a formal CS background which might adversely impact my chances of getting hired after I graduate and going to Columbia seems like a good way to rectify that. At the end of the day, I hope to learn more about ML through the graduate program, gain employable skills and figure out if I want to do a PhD. Any thoughts?",9,1
117,2015-5-8,2015,5,8,18,359uv7,Datamining for Growth Hacking,https://www.reddit.com/r/MachineLearning/comments/359uv7/datamining_for_growth_hacking/,[deleted],1431076212,,0,1
118,2015-5-8,2015,5,8,18,359xq7,CNNs: Strange plots.,https://www.reddit.com/r/MachineLearning/comments/359xq7/cnns_strange_plots/,Tom-Demijohn,1431079103,"CIFAR-10.  Conv + MaxPool + Conv + MaxPool + Conv + FC
SGD + sigmoid. No regularization
I was trying different learning rates values. 
[plot](http://postimg.org/image/p5i1zk417/)
Why there is a flat plot at the beginning? I though it might be because pictures were in 0 .. 255 values format and sigmoid suck at these values so I normalized these values to 0 .. 1 but I keep getting the same.
",10,3
119,2015-5-8,2015,5,8,21,35a9bw,Using K-Means to cluster wine dataset,https://www.reddit.com/r/MachineLearning/comments/35a9bw/using_kmeans_to_cluster_wine_dataset/,brotherrain,1431088801,,4,7
120,2015-5-8,2015,5,8,22,35afr3,Visual Turing Test Dataset: VQA Visual Question Answering,https://www.reddit.com/r/MachineLearning/comments/35afr3/visual_turing_test_dataset_vqa_visual_question/,glassackwards,1431092588,,0,10
121,2015-5-8,2015,5,8,23,35an18,Pushing the Frontiers of Computer Vision: A Q&amp;A with Google's Christian Szegedy,https://www.reddit.com/r/MachineLearning/comments/35an18/pushing_the_frontiers_of_computer_vision_a_qa/,reworksophie,1431096299,,0,1
122,2015-5-8,2015,5,8,23,35ao1a,[1505.01809] Language Models for Image Captioning: The Quirks and What Works,https://www.reddit.com/r/MachineLearning/comments/35ao1a/150501809_language_models_for_image_captioning/,exgr,1431096816,,1,8
123,2015-5-9,2015,5,9,2,35b7a6,[Question][Noob] Text Classification question,https://www.reddit.com/r/MachineLearning/comments/35b7a6/questionnoob_text_classification_question/,Iwdst,1431106006,"I've been working on a way to classify SMSs based on whether they are useful for an app my company currently makes. I obtained a large dataset and managed to make a sparse matrix of all the words, SMS-wise. I'm looking for an algorithm which will tell me which words occur together the most in this data set(and perhaps with what probability), I'm probably going to write it myself over the weekend but thought I'd ask once before possibly reinventing the wheel. Something which gave me conditional probability as well would be great. The purpose of all these output will be to create a scoring system since the number of useless messages vastly outnumber the useful ones. Any alternate method suggestions would be greatly appreciated as well :) . 


As always, I apologise for the noobness",0,0
124,2015-5-9,2015,5,9,3,35bdlm,Help sanity checking LSTMs with Hochreiter &amp; Schmidhuber's sequence order task,https://www.reddit.com/r/MachineLearning/comments/35bdlm/help_sanity_checking_lstms_with_hochreiter/,spurious_recollectio,1431109016,"I've been trying to sanity check my LSTM implementation because its had problems when I try to use it for language modelling (though I'm not sure this is related to the LSTM itself).  I've checked that finite differences matches my back-propped derivatives even over large sequences (at least when I use a CPU implementation...on a GPU there are often significant precision issues).

But to really check the architecture I want my implementation to pass the sequence order task (6a) of the Hochreiter-Schmidhuber 1997 paper.  I modified the test slightly so the two tokens are placed somewhat randomly but always with a significant seperation between them (that grows with the sequence length) and also I ask the network to output the correct answer when it sees a particular input token rather than at the end of the sequence.  So e.g. I have a random alphabet (a,b,c,d) for most of the sequence, two special tokens (x,y) input at random timesteps but spaced far apart and a final input token e which always appears at a random timestep near the end of the sequence.  After the network sees the input e it should output 1-4 depending on the possible orderings of x and y (xx, xy, yx, yy).

Unlike HS97 I train with minibatch learning (rather than online) and backprop through time rather than RTRL (which I don't know anything about).  This makes it very hard to compare results unfortunately or to steal hyper-params from them.

My LSTMs are able to acheive 100% accuracy after several epochs (of a few thousand examples) for sequence up to length ~75 (here the min seperation between the x,y tokens is ~30) but when I try sequences of length 100 I can't get it to work.  They claim to get 100% accuracy after only 30k examples but of course they use online learning (so they're doing far more updates than I am).  

Even for sequences of length 50 or 75 there is a lot of hyper-parameter tuning needed to get it to work so I'm wondering if  my implementation really has a problem or if I just need to work harder to find the right parameters.  

Has anyone had any luck with this task using BPTT and if so can you share your hyper-params and # of training examples?  ",6,3
125,2015-5-9,2015,5,9,3,35beva,[Paper] Facebook studying ways to identify people in different angles and positions in photos,https://www.reddit.com/r/MachineLearning/comments/35beva/paper_facebook_studying_ways_to_identify_people/,unchandosoahi,1431109594,,0,9
126,2015-5-9,2015,5,9,3,35biev,"Julia, the scripting language of the future. Really.",https://www.reddit.com/r/MachineLearning/comments/35biev/julia_the_scripting_language_of_the_future_really/,LokiNjord,1431111303,,65,84
127,2015-5-9,2015,5,9,6,35byty,Please review my method,https://www.reddit.com/r/MachineLearning/comments/35byty/please_review_my_method/,mobone,1431119374,"I've been working with stock market data for some time now and have developed this method, I'm wondering if this is a good approach or if there is a better one.


The goal is to have machine learning pick which stocks will outperform the market indexes following earnings announcement. This should be recognizable because there is a lot of known and successful research in ""post earnings announcement drift"", showing stocks do drift upwards following announcements of specific quality levels.


This being the case, I setup the problem basically as follows:
Input for SVM:
% Beat EPS, % Beat Rev, (A couple other columns)
Mapped to an Observation state of percent change after x days, where x can be 1, 5, 10, 20, etc.


I train on data from 2011, and test on 2012, (etc) asking the machine what predicted state the stock will end up in, and then reveal what the actual changes were. I have data from 2011 to today.


I bin the input data into 3 or 4 different states, so that something like 857% is bin 4, and -20% is bin 2, and -756% is bin 0, etc.


To optimize the problem I am running through all the different combinations of the bin cut offs and finding the model that has the most consistent median and average return, and with the smallest standard deviation, for each year.


My question is, is there a better way to find this ""most consistent model"". And the thing that really stumps me is the ""number of days"" to hold the stock. The research papers say 1/6th of the movement is in the first day, but I've had good results on each x days I've tried (5, 10, 20). 


My plan is to add all the days, from 1 - 90, to the model search, so I have to perform 40*40*40*90 model tests essentially. This will take my computer all week, so I was wondering if there might be a better way. Is there a way to map the attributes to two observation states?


Thanks, let me know if anything is not clear. 



",4,0
128,2015-5-9,2015,5,9,11,35cuot,"bagofwords.net, my naive bayesian subreddit classifier webapp",https://www.reddit.com/r/MachineLearning/comments/35cuot/bagofwordsnet_my_naive_bayesian_subreddit/,[deleted],1431137394,,6,11
129,2015-5-9,2015,5,9,13,35d61u,How to use Thompson Sampling for non-Beta?,https://www.reddit.com/r/MachineLearning/comments/35d61u/how_to_use_thompson_sampling_for_nonbeta/,gogogadgetlegz,1431144833,"Hi,

How would I use thompson sampling for optimizing a value such as price or maximizing the time on a page?  

I'm familiar with thompson sampling on the Beta distribution, but I'm not clear how you extend this for rewards that are greater than 1.

Do you just use a normal distribution, calculate a mean / variance for your arms using an online / streaming method and then just sample that gaussian?

THANKS!",1,4
130,2015-5-9,2015,5,9,13,35d89s,[Question] Which one will be the best approach to create a model to predict soccer (football) winner?,https://www.reddit.com/r/MachineLearning/comments/35d89s/question_which_one_will_be_the_best_approach_to/,unchandosoahi,1431146501,"Soon the oldest football cup will be played in Chile and I want to do some predictions to know if my country team will have chances to win or not. 
Should I use nnets or SVM? Forest trees or something else? I don't know how to start.

Thanks in advance :D",7,5
131,2015-5-9,2015,5,9,16,35dji7,Bayesian Inference Successes in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/35dji7/bayesian_inference_successes_in_machine_learning/,lol__wut,1431156370,"Hi /r/machinelearning,

I'm giving a brief talk at my university to students about bayesian inference and would like to give some good real life examples of where bayesian inference has achieved state of the art performance or where a bayesian inference approach is commonly used in some relatable application. Does anyone have any good examples off the top of their head?",11,5
132,2015-5-9,2015,5,9,21,35e2ai,[1505.00387] Highway Networks,https://www.reddit.com/r/MachineLearning/comments/35e2ai/150500387_highway_networks/,SuperFX,1431176164,,24,39
133,2015-5-10,2015,5,10,0,35eitw,Neural Nets with Caffe Utilizing the GPU,https://www.reddit.com/r/MachineLearning/comments/35eitw/neural_nets_with_caffe_utilizing_the_gpu/,joyofdata,1431186934,,0,0
134,2015-5-10,2015,5,10,1,35emhu,Looking to take Data science specialization from John Hopkins University. Will it be good enough to fetch an entry level job for me in Data Science field ?,https://www.reddit.com/r/MachineLearning/comments/35emhu/looking_to_take_data_science_specialization_from/,nerdof2014,1431188999,,4,2
135,2015-5-10,2015,5,10,3,35f29o,"""Robots will not be flawless, and the best future of human-robot partnerships will lie not in a race for who is more moral but in a symbiosis that lets each of the partners do what they do best, with the other available as a reality check.""",https://www.reddit.com/r/MachineLearning/comments/35f29o/robots_will_not_be_flawless_and_the_best_future/,jcpeeples,1431197602,,0,0
136,2015-5-10,2015,5,10,6,35fm8p,Is there any free standard dataset like MNIST for RNNs ?,https://www.reddit.com/r/MachineLearning/comments/35fm8p/is_there_any_free_standard_dataset_like_mnist_for/,piccoloDeSardi,1431208608,I would like to learn RNNs. Usually for simple feed-forward models I use MNIST dataset which is good and short enough to check different training algorithm quickly. It is also quite popular in many papers. But what is MNIST-like dataset of similar scale for recurrent models?,10,9
137,2015-5-10,2015,5,10,11,35gdyk,"Genetic Programming in Python, with a scikit-learn inspired API",https://www.reddit.com/r/MachineLearning/comments/35gdyk/genetic_programming_in_python_with_a_scikitlearn/,____init____,1431224866,,13,43
138,2015-5-10,2015,5,10,15,35gx2k,Actually Useful Accepted ICML Papers (Arxiv Links),https://www.reddit.com/r/MachineLearning/comments/35gx2k/actually_useful_accepted_icml_papers_arxiv_links/,fhadley,1431238506,,19,36
139,2015-5-10,2015,5,10,21,35hj34,"Interview with the #2 Kaggler on his competition plans, iteration cycles and other tips",https://www.reddit.com/r/MachineLearning/comments/35hj34/interview_with_the_2_kaggler_on_his_competition/,kunjaan,1431262085,,1,15
140,2015-5-10,2015,5,10,22,35hnqw,How are you storing and organizing your test results? It starts to look like a civil war in my projects folder...,https://www.reddit.com/r/MachineLearning/comments/35hnqw/how_are_you_storing_and_organizing_your_test/,onewugtwowugs,1431265629,"Right now I'm storing results from experiments in a folder together with all the code, but my system really won't allow me to rerun the results since I'm constantly updating the code. I have started using git in the hope of being able to return to previous commit when the experiment was run, but I seem to lack the self-discipline to actually commiting my code before running the experiments since it usually takes a few bug fixes before getting the code to run. To make it short, it's a clusterfuck . 

I was hoping to get some inspiration from you on your current workflow to make sure to keep your projects and experiment results structured. ",7,16
141,2015-5-10,2015,5,10,22,35hodr,Deep Learning ELI5,https://www.reddit.com/r/MachineLearning/comments/35hodr/deep_learning_eli5/,dogbuddies,1431266078,I'm pretty new to machine learning and want to get a better handle on deep learning. Can any of you all point me in a good direction or provide a relatively accessible explanation of how it works? ,2,0
142,2015-5-10,2015,5,10,23,35hoxv,Zero-bias autoencoders and the benefits of co-adapting features,https://www.reddit.com/r/MachineLearning/comments/35hoxv/zerobias_autoencoders_and_the_benefits_of/,downtownslim,1431266452,,0,2
143,2015-5-10,2015,5,10,23,35hqu3,[1503.04881] Long Short-Term Memory Over Tree Structures,https://www.reddit.com/r/MachineLearning/comments/35hqu3/150304881_long_shortterm_memory_over_tree/,SuperFX,1431267650,,1,4
144,2015-5-10,2015,5,10,23,35hr6o,Has the deep learning stampede cooled off?,https://www.reddit.com/r/MachineLearning/comments/35hr6o/has_the_deep_learning_stampede_cooled_off/,SuperFX,1431267878,"Judging from the list of accepted papers at ICML, I feel like the dominance of deep learning during the past two years has somewhat waned? Does anyone else feel the same?",9,5
145,2015-5-10,2015,5,10,23,35ht11,Google Deepmind's Gorila - parallel actor / critic reinforcement learning - David Silver [pdf],https://www.reddit.com/r/MachineLearning/comments/35ht11/google_deepminds_gorila_parallel_actor_critic/,ford_beeblebrox,1431269021,,14,55
146,2015-5-10,2015,5,10,23,35hu4f,Theano stacked autoencoder tutorial,https://www.reddit.com/r/MachineLearning/comments/35hu4f/theano_stacked_autoencoder_tutorial/,mlaway,1431269700,"I'm reading and trying to understand [this tutorial]( http://deeplearning.net/tutorial/SdA.html).
During the pretraining, I would assume that when training layer 2, you would give it the output from layer 1 as input, for each training example.
From what I see in this code though, each layer is trained separately using the training data as input, instead of the output of the layer below.

Am I missing something or is this actually the case?",1,1
147,2015-5-11,2015,5,11,0,35hus2,Emojineering Part 2: Implementing Hashtag Emoji [Instagram],https://www.reddit.com/r/MachineLearning/comments/35hus2/emojineering_part_2_implementing_hashtag_emoji/,sko2sko,1431270099,,1,2
148,2015-5-11,2015,5,11,0,35hyh1,RNN controlled creatures evolved to survive killer planks in JS. Life is tough.,https://www.reddit.com/r/MachineLearning/comments/35hyh1/rnn_controlled_creatures_evolved_to_survive/,wei_jok,1431272182,,12,26
149,2015-5-11,2015,5,11,1,35i4xq,DNN: What heuristics for dropout?,https://www.reddit.com/r/MachineLearning/comments/35i4xq/dnn_what_heuristics_for_dropout/,Tom-Demijohn,1431275773,"* What dropout probabilities are most effective? 
* How many and what layers should I regularize with this method?

* Case: Conv + Conv + FC + FC + Softmax.  
should I use dropout in all FCs and Softmax? What if I mix probabilities? 

* Is there any paper covering 'best' heuristics for dropout?",4,5
150,2015-5-11,2015,5,11,1,35i5f5,Can the Denoising layer in denoising auto-encoders be considered as dropout applied to the input layer?,https://www.reddit.com/r/MachineLearning/comments/35i5f5/can_the_denoising_layer_in_denoising_autoencoders/,deep_learner,1431276049,"These techniques seem almost the same, is my intuition correct?
Or is there a subtlety I am missing?",1,0
151,2015-5-11,2015,5,11,4,35ill4,"ML Algorithms: Representation, Evaluation &amp; Optimization.",https://www.reddit.com/r/MachineLearning/comments/35ill4/ml_algorithms_representation_evaluation/,DontBelieveMeiLie,1431284491,"I was reading through a paper published by the University of Washington with tips about Machine Learning algorithms.

The way they break down an ML algorithm is into 3 parts. A representation, evaluation and optimization. I would like to understand how these three parts work together, so what is the process like throughout a typical machine learning algorithm.

I understand that my question is very abstract and each algorithm will be different, but if you know of a way to explain it abstractly please do. If not feel free to use a specific algorithm to explain.

Paper: [Washington CS](http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) ~ See Table 1.",1,2
152,2015-5-11,2015,5,11,6,35j1sf,Gensim vs other libraries in Python,https://www.reddit.com/r/MachineLearning/comments/35j1sf/gensim_vs_other_libraries_in_python/,[deleted],1431292964,"Simple question really: I've been using Gensim for things like preparing bag of words, tf-idf, LSA and LDA. 

I'm a beginner so it's good to ask experts: do you use Gensim or prefer to use different Python libraries? Why?",1,1
153,2015-5-11,2015,5,11,7,35j6jb,Ada boost with linear classifiers?,https://www.reddit.com/r/MachineLearning/comments/35j6jb/ada_boost_with_linear_classifiers/,fpoint1,1431295463,"I want to implement Ada boost algorithm for own learning purposes, as base classifier i chose logistic regression learned with stochastic gradient descent [github link](https://github.com/olologin/AdaBoost-with-logistic-regression). Is such composition should work in theory? Because in practice i can't separate non-linear artificial dataset with such classifiers (Accuracy ~50%). I will be grateful if someone can help with theoretical or practical advice.",7,0
154,2015-5-11,2015,5,11,7,35j7s7,"Growing datasets, multiclass classifiers and crowdsourced feedback.",https://www.reddit.com/r/MachineLearning/comments/35j7s7/growing_datasets_multiclass_classifiers_and/,klbr111,1431296134,"My dataset is a list of schedules between January and June (closed set) and each schedule is bind to a list of courses that the person is taking. It is growing everyday at superlinear rate.

Each courses has events (problem sets, midterms, finals). I am operating under the assumption that the schedules are *complete* i.e people actually added all the events related to the different courses they are taking (up to 5).

What I am trying to figure out are the list of events bind to each courses. I can ask users if my predictions are correct and the feedback loop is very short.

Any ideas on where to begin, which topics/paper should I read or what approach to avoid? If you are aware of existing tools that would help me solve this problem it is welcome too! I want to improve a side project that is pretty basic right now and learn more about ML.",0,1
155,2015-5-11,2015,5,11,8,35jixw,Grad School in ML,https://www.reddit.com/r/MachineLearning/comments/35jixw/grad_school_in_ml/,thatotherdudedude,1431302202,"Hello,

I am currently a junior studying computer engineering.  Its too late for me switch majors, and my schedule is locked in stone at this point.

How difficult would it be to transition to a Master's in machine learning, and is there anything I can do on my that would better prepare me?

Thank you.",6,1
156,2015-5-11,2015,5,11,16,35koaq,PTFE Teflon Irons with the Best Quality By Balacchihf,https://www.reddit.com/r/MachineLearning/comments/35koaq/ptfe_teflon_irons_with_the_best_quality_by/,hiyogacentre22,1431329442,,0,0
157,2015-5-11,2015,5,11,19,35kyp6,Maximizing difference between samples in a batch with generative models?,https://www.reddit.com/r/MachineLearning/comments/35kyp6/maximizing_difference_between_samples_in_a_batch/,TheAlienDude,1431339552,"Hi all,

I am currently working on a generative model which generates samples from a target distribution given random noise.

I have implemented a batch training method and I am noticing that while my network is generating valid samples, the samples are a bit too similar to each other to my liking.

So my question is: is there anything I can add to my loss function, or can I employ any other method in order to maximize the difference between samples in a batch in an efficient manner?

Thanks!",6,1
158,2015-5-11,2015,5,11,19,35kzlo,A Fixed-Size Encoding Method for Variable-Length Sequences,https://www.reddit.com/r/MachineLearning/comments/35kzlo/a_fixedsize_encoding_method_for_variablelength/,Foxtr0t,1431340409,,24,20
159,2015-5-11,2015,5,11,20,35l3pa,What do you guys think about Microsoft Azure Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/35l3pa/what_do_you_guys_think_about_microsoft_azure/,Wazzymandias,1431344034,"Was curious on everyone's thoughts about this product. On the surface it seems like the ML version of a WYSIWYG web dev IDE, but I'm wondering if it's a good alternative to the current ML contenders (R,Cpp,Python,etc).

Edit: And Amazon Machine Learning Services too for that matter.
",18,11
160,2015-5-11,2015,5,11,21,35l5qs,PyNeural - Python library for training neural networks written in Cython,https://www.reddit.com/r/MachineLearning/comments/35l5qs/pyneural_python_library_for_training_neural/,kunjaan,1431345605,,15,52
161,2015-5-11,2015,5,11,23,35ljru,Image Question Answering: A Visual Semantic Embedding Model and a New Dataset,https://www.reddit.com/r/MachineLearning/comments/35ljru/image_question_answering_a_visual_semantic/,clbam8,1431354186,,0,9
162,2015-5-12,2015,5,12,0,35lrtw,"Hello ML folks, What is a good book or resource to learn about RNN and LSTM in context of time series data ? Thanks for your help.",https://www.reddit.com/r/MachineLearning/comments/35lrtw/hello_ml_folks_what_is_a_good_book_or_resource_to/,t_minus_1,1431358216,"I am working on time series forecasting and came to know that RNN , LSTM can learn the deeper structure and can be used to detect anomalies, make predictions like ARIMA. I do want a good foundation but on the other hand  don't want get lost in mathematical complexity. Also, purely code examples or blogs might miss the foundations and algorithmic theory.  Currently, I have Murphy's book in my reading list. 

Thanks all for your help.",9,0
163,2015-5-12,2015,5,12,0,35ls14,What interesting learning techniques can be applied to foot steps data (How many footsteps you take in a day/hour etc)?,https://www.reddit.com/r/MachineLearning/comments/35ls14/what_interesting_learning_techniques_can_be/,vj77,1431358313,,3,0
164,2015-5-12,2015,5,12,1,35m1q0,Question: GPUs role in DNNs,https://www.reddit.com/r/MachineLearning/comments/35m1q0/question_gpus_role_in_dnns/,Tom-Demijohn,1431362856,"So I get that:

* we can parallelize SGD algorithm by setting mini-batch-size to let's say 128 so it can run on 128 gpu cores. 
* also we can perform efficient convolution on GPUs. 
* also we can run many DNNs on many GPUs if available.

Is there sth else that puts GPUs in favour over CPUs?

What are limitations of GPUs? 

For example I've seen that I easily hit 3GB of DDRAM on MNIST and CIFAR10. And there are way way bigger datasets like ImageNet or Diabetic Retinopathy Kaggle datasets.  How to run DNNs on those huge datasets on machine with lets say 32GB CPU RAM and 6 GB GPU RAM. Is cloud the only option?",9,0
165,2015-5-12,2015,5,12,2,35m4ov,Machine Learning with Training Samples with Different Reliabilities,https://www.reddit.com/r/MachineLearning/comments/35m4ov/machine_learning_with_training_samples_with/,marriedinabathroom,1431364126,"Hi all,

I would like to use machine learning for a set of data.  However, within the training population, each sample has a different level of reliability (e.g. We trust the feature values of 1 sample more than we trust the feature values of another sample.)

Is there a way of incorporating the reliability of each individual training sample with a machine learning algorithm?

Thanks",5,3
166,2015-5-12,2015,5,12,3,35mdlp,Talk | IEPY: Una plataforma para Extraccin de Informacin en Python,https://www.reddit.com/r/MachineLearning/comments/35mdlp/talk_iepy_una_plataforma_para_extraccin_de/,copybin,1431368105,,0,0
167,2015-5-12,2015,5,12,3,35mhqk,"MS in Statistics with focus on ML, what CS classes to take?",https://www.reddit.com/r/MachineLearning/comments/35mhqk/ms_in_statistics_with_focus_on_ml_what_cs_classes/,[deleted],1431369910,"I have a strong programming background but little formal CS education (aside from two introductory algorithms and data structures classes) and I've already taken a few ML classes. I've used very little of that since I do pretty much everything in R.

What ""pure"" CS classes should I take to complement my statistics MS? I assume most ML jobs will want some ""big data"" skills like Spark, so maybe a parallel computing class?",6,3
168,2015-5-12,2015,5,12,5,35mv9u,Weekly collection of the best news and resources on Artificial Intelligence and Machine Learning - in your inbox.,https://www.reddit.com/r/MachineLearning/comments/35mv9u/weekly_collection_of_the_best_news_and_resources/,mylittleai,1431375904,,3,4
169,2015-5-12,2015,5,12,6,35n58e,Particle swarm optimization in F#,https://www.reddit.com/r/MachineLearning/comments/35n58e/particle_swarm_optimization_in_f/,DanielSlater8,1431380269,,0,2
170,2015-5-12,2015,5,12,6,35n657,Object Detection for Red Barrels spray painted with logos? X-Post from /r/computervision,https://www.reddit.com/r/MachineLearning/comments/35n657/object_detection_for_red_barrels_spray_painted/,yxsilentxy,1431380693,"Hi all,
Currently I am working on a university IEEE project called Robomagellan that will be competing with AVC Sparkfun's 2015 competition (stupid, I know, we dropped out of the actual RoboMagellan competition).

Being that this is my first real project dealing with machine vision, I've done some research into object detection of unique objects (unique as in anything, really). Based on what I've read, this means I need to use some sort of classifier (I've only been reading on HAAR(?) classifier). My question is how would I go about detecting images such as http://imgur.com/3Tg1J4P  ?
Can I just find any sort of red barrel and train the classifier or would I need to scour youtube for videos of the actual competition and take stills where there are these barrels?

Additionally, I know that all the positive images required need to be similar as in they are cropped to a specific size showing only the object of interest, but does this mean they need to be the same size or need to be the same aspect ratio? So for example would it be fine if I find some object that is cropped to 50x50 pixels, and have another image, maybe its taken closer or further way and if it turns out to be 100x100 or 25x25 cropped?

Thanks for taking the time to read this. Terribly sorry if I double posted in two subreddits.
",5,0
171,2015-5-12,2015,5,12,13,35ocmg,Nuances of eXtreme Gradient Boosting,https://www.reddit.com/r/MachineLearning/comments/35ocmg/nuances_of_extreme_gradient_boosting/,arshakn,1431405233,,2,1
172,2015-5-12,2015,5,12,15,35okep,Deep Boltzmann Machines with Fine Scalability,https://www.reddit.com/r/MachineLearning/comments/35okep/deep_boltzmann_machines_with_fine_scalability/,iori42,1431410461,,4,6
173,2015-5-12,2015,5,12,16,35ose1,"A nice selection of Machine Learning papers, books and tutorials",https://www.reddit.com/r/MachineLearning/comments/35ose1/a_nice_selection_of_machine_learning_papers_books/,galapag0,1431416860,,0,2
174,2015-5-12,2015,5,12,18,35ozh9,Echo State Machines - Suggested reading?,https://www.reddit.com/r/MachineLearning/comments/35ozh9/echo_state_machines_suggested_reading/,AmazingMuffin,1431423479,"Hi,

Currently I am trying to figure out how Echo State Machines work and having a hard time. Can anyone suggest some reading material that explain how these networks work on a high level / in layman's terms?",5,5
175,2015-5-12,2015,5,12,20,35p729,Finding Topics in Harry Potter using K-Means Clustering,https://www.reddit.com/r/MachineLearning/comments/35p729/finding_topics_in_harry_potter_using_kmeans/,kunjaan,1431429981,,2,9
176,2015-5-12,2015,5,12,20,35p9a4,IEPY 0.9.4 released: An open source tool for InformationExtraction in Python,https://www.reddit.com/r/MachineLearning/comments/35p9a4/iepy_094_released_an_open_source_tool_for/,copybin,1431431577,,1,9
177,2015-5-12,2015,5,12,21,35pcs9,fastFM: A Library for Factorization Machines (x-post r/CompressiveSensing),https://www.reddit.com/r/MachineLearning/comments/35pcs9/fastfm_a_library_for_factorization_machines_xpost/,compsens,1431433876,,5,8
178,2015-5-12,2015,5,12,22,35pirx,"Benchmark: Machine Learning as a service, available on-demand, for everyone - Amazon vs Google vs Azure",https://www.reddit.com/r/MachineLearning/comments/35pirx/benchmark_machine_learning_as_a_service_available/,andresirgado,1431437290,,0,1
179,2015-5-12,2015,5,12,23,35ptgc,"Yes, Neural Networks Have Grandmother Cells [OC]",https://www.reddit.com/r/MachineLearning/comments/35ptgc/yes_neural_networks_have_grandmother_cells_oc/,michaelmalak,1431442577,,3,0
180,2015-5-13,2015,5,13,0,35pvlr,Infographic: R vs Python for data science,https://www.reddit.com/r/MachineLearning/comments/35pvlr/infographic_r_vs_python_for_data_science/,martijnT,1431443549,,15,8
181,2015-5-13,2015,5,13,0,35pvw4,"Andrew Ng: Why 'Deep Learning' Is a Mandate for Humans, Not Just Machines | WIRED",https://www.reddit.com/r/MachineLearning/comments/35pvw4/andrew_ng_why_deep_learning_is_a_mandate_for/,salazarj,1431443678,,7,14
182,2015-5-13,2015,5,13,0,35q0p6,Keras: Fast Deep Learning Prototyping for Python,https://www.reddit.com/r/MachineLearning/comments/35q0p6/keras_fast_deep_learning_prototyping_for_python/,alephnil,1431445849,,11,30
183,2015-5-13,2015,5,13,0,35q1lr,Bumpy Bar Codes,https://www.reddit.com/r/MachineLearning/comments/35q1lr/bumpy_bar_codes/,tomjames05,1431446271,,0,0
184,2015-5-13,2015,5,13,1,35q37a,So what is the bleeding edge?,https://www.reddit.com/r/MachineLearning/comments/35q37a/so_what_is_the_bleeding_edge/,llSourcell,1431447024,I'm looking to start working on Artificial General Intelligence. Any suggestions as to a bleeding edge paper to read up on? Is the Deep Q algorithm by Deep Mind the most bleeding edge? ,2,0
185,2015-5-13,2015,5,13,1,35q8ps,Google Prediction API: a Machine Learning black box for developers,https://www.reddit.com/r/MachineLearning/comments/35q8ps/google_prediction_api_a_machine_learning_black/,[deleted],1431449473,,0,1
186,2015-5-13,2015,5,13,2,35qa7g,What are the recent developments on semi-supervised learning?,https://www.reddit.com/r/MachineLearning/comments/35qa7g/what_are_the_recent_developments_on/,regularized,1431450116,Can you point me some recent advancements on semi-supervised learning? ,3,0
187,2015-5-13,2015,5,13,2,35qang,Google Prediction API: a Machine Learning black box for developers,https://www.reddit.com/r/MachineLearning/comments/35qang/google_prediction_api_a_machine_learning_black/,alexcasalboni,1431450295,,19,54
188,2015-5-13,2015,5,13,8,35rqyv,Deep Learning for Image Understanding in Planetary Science,https://www.reddit.com/r/MachineLearning/comments/35rqyv/deep_learning_for_image_understanding_in/,harrism,1431473584,,1,7
189,2015-5-13,2015,5,13,11,35sav3,What is the state of the art in ML on human relationships romantic and otherwise?,https://www.reddit.com/r/MachineLearning/comments/35sav3/what_is_the_state_of_the_art_in_ml_on_human/,loveComputer,1431483617,"I can't seem to find anything that allows you to, for example, analyze your messaging history with your girl or boyfriend and get insight",2,0
190,2015-5-13,2015,5,13,11,35sbnx,Deciding between postdoc or the industry after PhD,https://www.reddit.com/r/MachineLearning/comments/35sbnx/deciding_between_postdoc_or_the_industry_after_phd/,ginger_beer_m,1431484033,"Hi all. 

I'm a third year PhD student in the UK who is nearing the end of my study, and have some questions to anybody who has transitioned from the academia to the industry before:

1. Is going to a postdoc considered necessary if I want to go to the industry after finishing my PhD? If I do a postdoc, that would be pretty much on the same field (bioinformatics) as what i do now. I'm kind of feeling burnt out from academia, so I'm considering the jump to the other side. For example, [this kind of job ads](http://www.indeed.co.uk/viewjob?jk=1a8ee57050c8bd48&amp;amp;amp;amp;q=machine+learning&amp;amp;amp;amp;l=london&amp;amp;amp;amp;tk=19l5gk4ds9n1edcf) or [this one](https://boards.greenhouse.io/workingatbooking/jobs/41969?t=7cu1qi) are pretty appealing to me (these are just random examples). I also got in touch with a recruiter from Google, although everything is still pretty tentative now -- haven't been to any interview yet. I suspect doing a post-doc might make me overqualified for such positions or make me viewed as too 'sticky' to the academia, if that's true?
2. If I go to the industry straight after the PhD, I take it that this is a one-way trip then? Seems like the chances are low to return to the academia after this, is low or at least I haven't heard any who has successfully done so? Apart from some famous figures in the field.
3. For those who are already in the industry, how's your working day like and what's your background that led you to the job? Do you spend most of your time on feature extractions, like what people tend to say about such work, or there are interesting ML/CS/engineering challenges involved in the work too?

-------

Thanks for reading. My background is below.

My PhD work is mostly on ML/Bayesian modelling applied to biological data -- so I worked on the development of Bayesian models, e.g. hierarchical mixture models with non-parametric priors or topic modelling like LDA, to find interesting things/produce a generative process to explain my data. I also work on feature extractions and cleaning, implementing various inference procedures (mostly MCMC-based, with the occasional variational inference throw in), followed by model selection, evaluation of performance etc. 

I worked as a software engineering for a couple of years before grad school, so I have experiences with enterprise-y stuff too, like Java EE and others. Coming from a mostly CS background, my math isn't that great as those from a math/stats background, although I have enough general machine learning knowledge (classification, regression, clustering) to get by and get my work done. The same goes with the biological side of my knowledge. It's a bit of ""fake it till you make it"" situation here.",34,8
191,2015-5-13,2015,5,13,11,35sey6,Detecting whether one of a set of candidate largeish substrings exists in a larger body of text using feature extraction?,https://www.reddit.com/r/MachineLearning/comments/35sey6/detecting_whether_one_of_a_set_of_candidate/,[deleted],1431485738,"Say I have a collection of 10 million paragraphs about sports.  Then I'm given a chunk of text.  I want to quickly find out with decent accuracy whether one of my 10 million paragraphs is contained somewhere within that chunk.  Essentially finding a substring or near substring.  Maybe a substring match can be done to confirm a positive, but that's expensive and I don't want too many false positives.

A naive approach I thought of would be to compute the top (say) 5 ngrams with the most entropy of each paragraph I have, and store them on disk with those top 5 ngrams as 5 separate keys for that paragraph.  Now I'm given a chunk of text which *might* contain one of my paragraphs.  Now I compute the top 5 ngrams of it, maybe use the top 2 to do a look up, and then with the paragraphs that look up returns, see if one or two other ngrams match.

Additionally, the surrounding 'noise' text is not guaranteed to not be about sports.

Any ideas?

Thanks
",1,1
192,2015-5-13,2015,5,13,12,35sjr4,Kaggle - Improving the public leaderboard,https://www.reddit.com/r/MachineLearning/comments/35sjr4/kaggle_improving_the_public_leaderboard/,[deleted],1431488446,"In my opinion, even though the dataset that is used to compute the final scores in a competition is not the same as the one that is used to for the public leaderboard, there is a certain bias in the final results. 

Nested cross-validation is a useful technique to select among different algorithms. Via the outer loop, you select the training and test folds, and in the inner loop, you tune the hyperparameter tuning and select the model based on the performance of the test set selected in the outer loop. Pooling the average performance scores and looking at the different models is then a useful indicator for the model stability and its performance. However, after nested cross validation, it's still useful to tune the hyperparameters of the best selected algorithm and perform model selection in a new round of k-fold cross-validation to tune its hyperparameters, and a separate test set (data that the algorithm really hasn't seen before, neither in the tuning nor the model selection process) is required to get a fairly unbiased estimate of it's true performance.

In my opinion, this is kind of violated in the current kaggle leaderboard as well as the independent validation set since the validation set is a subset of the public leaderboard, which leads to biases.

I think that kaggle does a great job, however, I think that most of the submitted models are biased. I am not exactly sure about how to solve this problem, but one idea could be to hold out more data and ask the participants to rerun their model on a new hold-out data set every week or month (only 1 submission should be allowed to avoid overfitting), and the leaderboard could be constructed from the average performance on those holdout sets. 

I am just curious about what you think about the current kaggle approach, and if you have any better idea for how to improve the current situation!",0,0
193,2015-5-13,2015,5,13,14,35su8k,I would like to recommend a book for the FAQ,https://www.reddit.com/r/MachineLearning/comments/35su8k/i_would_like_to_recommend_a_book_for_the_faq/,Nixonite,1431495274,"Hello everyone,

Note, I'm not the author and I have no financial ties to the book. Just a student with an opinion. 

I finished working through the book ""Mastering Machine Learning with Scikit-Learn"" several months ago, and I found it to be an incredibly helpful introductory level book on ML. 

Why this book specifically?

This is mainly in contrast to the ""Building Machine Learning Systems with Python"" book. That book assumed some level of understanding of ML while the Mastering ML with Scikit-learn book does not.

The reason why I recommend this book specifically is because I think it does a very fine job of merging light theory (using lots of simple examples, plain English, without sacrificing the equations) with usable code. I used it to learn how to work with scikit-learn for a research project at my university alongside a few other people, and we were able to learn by tweaking the code and reading ISL alongside it for further theory. 

Anyway, just a suggestion for the mods to add it to the list of introductory ML books. It's not free, but it's a pleasant beginning.

On a side note, I would also like to recommend ""Introduction to Machine Learning (Adaptive Computation and Machine Learning series)"" by Ethem Alpaydin as an intermediate level theory book. I purchased it as a step above ISL while still being more friendly than ESL. Again, just a student opinion with no financial interest. I really struggled for a while to find some intermediary step between ISL and ESL, and I think this book can bridge the gap. ",4,20
194,2015-5-13,2015,5,13,14,35svzo,Why not dropout + relu for non-greedy deep unsupervised learning?,https://www.reddit.com/r/MachineLearning/comments/35svzo/why_not_dropout_relu_for_nongreedy_deep/,brockl33,1431496567,"As I understand it, greedy layerwise unsupervised pre-training fell out of favor after
1. dropout + relu
2. more labelled data
3. better weight initialization + tanh
which helped overcome the vanishing gradient problem.

Unsupervised training still appears useful for:
1. cases with few labelled data
2. transfer learning
3. denoising
4. generative modeling

Although target propagation by Bengio's group is over my head at the moment, it seems like one of the primary motivations was to allow for cooperation between layers during representation learning in contrast to greedy layerwise training which does not (""greedy problem"").

So comes the question, has anyone worked on or have information about deep autoencoders using dropout + relu (or softplus) to overcome the vanishing gradient and greedy problems? It seems like the simplest nongreedy way to train autoencoders.",2,3
195,2015-5-13,2015,5,13,16,35t3j0,"Slides Paris Machine Learning Meetup #9, Season 2: ML @Quora and @Airbus and in HFT, Tax, APIs war (Info on streaming but will probaby be in spoken French. Slides in English)",https://www.reddit.com/r/MachineLearning/comments/35t3j0/slides_paris_machine_learning_meetup_9_season_2/,compsens,1431502776,,0,1
196,2015-5-13,2015,5,13,22,35tqoq,Construction Machinery Suppliers,https://www.reddit.com/r/MachineLearning/comments/35tqoq/construction_machinery_suppliers/,Md-Malik,1431522289,,0,1
197,2015-5-13,2015,5,13,22,35tqvg,"Understanding ""Optimizing Neural Networks that Generate Images"" (Capsules)",https://www.reddit.com/r/MachineLearning/comments/35tqvg/understanding_optimizing_neural_networks_that/,dylanbyte,1431522392,"I have been trying to understand this take on Hinton's capsules that we have heard about. 

http://www.cs.toronto.edu/~tijmen/tijmen_thesis.pdf

Tieleman builds a system where each capsule has a template, and after the image is inputed, it outputs pose variables (x translation y translation scaling etc) as well as a probability that the template is in the image.

The template is then decoded to the output by applying the the specified affine transformation to the output.

The thesis seems to be claiming that the pose outputs for the capsule are learned through back propagating the error between the output and the target, but how is this possible? It seems to me (knowing nothing of vision/graphics), that a translation of an image must be non differentiable, since it is a discrete choice.

Am I misunderstanding? Any help would be much appreciated.",13,9
198,2015-5-13,2015,5,13,22,35trrv,Coil Winding Machines Suppliers,https://www.reddit.com/r/MachineLearning/comments/35trrv/coil_winding_machines_suppliers/,Md-Malik,1431522898,,0,1
199,2015-5-13,2015,5,13,22,35tvst,Deep Learning Machine Solves the Cocktail Party Problem | MIT Technology Review,https://www.reddit.com/r/MachineLearning/comments/35tvst/deep_learning_machine_solves_the_cocktail_party/,salazarj,1431525097,,7,0
200,2015-5-14,2015,5,14,0,35u6jp,Convert nl-string for use in a neural-network ( perhaps while retaining semantic value )?,https://www.reddit.com/r/MachineLearning/comments/35u6jp/convert_nlstring_for_use_in_a_neuralnetwork/,theirfReddit,1431530433,"I've been trying to convert natural language strings into integers for use in a long short-term neural-network. I tried converting to binary, using a bag-of-words, and an associative-array with each letter corresponding to a prime-number.

I looked into Google's word2vec just to convert the words into word-vectors, but I'm looking for something I can implement in the browser. This is why I am looking for an algorithm that I can write in js.

I know there's node.js implementations of word2vec, but they just run word2vec in the command-line.

This is different than this question, [here][1], that I asked earlier because I am looking for something that retains semantic meaning. I thought about using word similarity techniques, but didn't know how to implement resnik similarity in js.

I greatly appreciate any help or direction in converting nl sentences, or just the topic of them, to word-vectors or an array of ints.",7,4
201,2015-5-14,2015,5,14,1,35uhae,Will NNs permanently replace other ML methods?,https://www.reddit.com/r/MachineLearning/comments/35uhae/will_nns_permanently_replace_other_ml_methods/,asymptotics,1431535470,"I've been listening to some talks about deep learning RNNs, etc. and it seems like deep neural nets and RNNs are beating SVMs, HMMs (speech recognition and NLP), various CV techniques (especially things like Gaussian mixture models) at essentially every application in those domains.

In 5-10 years will ML essentially become the study of neural networks? Is that already becoming the case?",70,28
202,2015-5-14,2015,5,14,2,35uo5i,"A Private Machine Learning Model for Your App Company (ask us anything, using scikit)",https://www.reddit.com/r/MachineLearning/comments/35uo5i/a_private_machine_learning_model_for_your_app/,andrewljohnson,1431538645,,0,0
203,2015-5-14,2015,5,14,5,35v75i,New web service from Wolfram identifies and classifies images.,https://www.reddit.com/r/MachineLearning/comments/35v75i/new_web_service_from_wolfram_identifies_and/,[deleted],1431547249,,0,0
204,2015-5-14,2015,5,14,6,35vllb,Best mechanism/tool for paraphrasing/condensing natural language utterances?,https://www.reddit.com/r/MachineLearning/comments/35vllb/best_mechanismtool_for_paraphrasingcondensing/,bpsick,1431553712,"I've been surveying what seems to be the latest in various parsers / papers on paraphrasing. http://nlp.stanford.edu/pubs/berant14paraphrasing.pdf is pretty interesting, and SEMPRE and SPF seem to be the most commonly used / accessible parsers at the moment. 

I just wanted to see if anyone has some thoughts / ideas for papers to examine etc?",1,12
205,2015-5-14,2015,5,14,7,35vo3i,Zip of Andrew Ng's machine learning coursera videos anywhere?,https://www.reddit.com/r/MachineLearning/comments/35vo3i/zip_of_andrew_ngs_machine_learning_coursera/,polalavik,1431554916,Anyone have a zip or rar of the videos?,3,1
206,2015-5-14,2015,5,14,8,35vwzm,In what applications CNNs are implemented?,https://www.reddit.com/r/MachineLearning/comments/35vwzm/in_what_applications_cnns_are_implemented/,[deleted],1431559181,"Besides digit recognizer in post offices and banks. 

I know CNNs can do pretty good at recognizing sth in images. But where can one really benefit from them? Are they used in self-driving cars? How they can be applied to robotics? ",7,0
207,2015-5-14,2015,5,14,9,35w5ee,"I am looking for good demos and applications (elevator pitch type) that use automatic differentiation and it's immediate applications like zero-finding, gradient descent, etc.",https://www.reddit.com/r/MachineLearning/comments/35w5ee/i_am_looking_for_good_demos_and_applications/,nikofeyn,1431563525,"i am writing an automatic differentiation library. i currently only have forward-mode implemented, but i will soon be implementing reverse-mode as well. i can compute derivatives, gradients, directional derivatives, and jacobians as of now.

to present this to others, i am looking for some eye candy demos and whizbang applications of this library. i am familiar with advanced mathematics, but i know very little about machine learning. can someone point me to an application that uses automatic differentiation, gradient descent, zero-solvers, etc.?

the best case scenario is an existing example that i could port to the language my library is in. thanks!",3,3
208,2015-5-14,2015,5,14,11,35wj9e,Feed grain feed grain machine on the role of feed grain feed processing machine,https://www.reddit.com/r/MachineLearning/comments/35wj9e/feed_grain_feed_grain_machine_on_the_role_of_feed/,qiangwin-pellet,1431570911,,0,0
209,2015-5-14,2015,5,14,11,35wjvg,using SVM and SVD based feature selection to predict search results relevance @CrowdFlower @kaggle,https://www.reddit.com/r/MachineLearning/comments/35wjvg/using_svm_and_svd_based_feature_selection_to/,abhi1thakur,1431571236,,0,0
210,2015-5-14,2015,5,14,16,35xa5e,"The European Innovation Academy -extreme tech accelerator- takes Data ScienceTech Institute ""Innovation Chair""",https://www.reddit.com/r/MachineLearning/comments/35xa5e/the_european_innovation_academy_extreme_tech/,datasciencetech,1431589606,,0,1
211,2015-5-14,2015,5,14,17,35xayn,Thermal Lamination Machine,https://www.reddit.com/r/MachineLearning/comments/35xayn/thermal_lamination_machine/,sonalbisht101,1431590409,,0,1
212,2015-5-14,2015,5,14,17,35xe4a,Graph connectivity when using Mutual Nearest Neighbor Value,https://www.reddit.com/r/MachineLearning/comments/35xe4a/graph_connectivity_when_using_mutual_nearest/,[deleted],1431593552,"Hello fellow redditors. 
I am currently implementing a clustering method named Superparamagnetic clustering. You can find the paper [here](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=2B77C7CF7A768EEAD915121237AD8CEE?doi=10.1.1.77.5569&amp;rep=rep1&amp;type=pdf)

However, my problem does not concern the clustering algorithm itself but the construction of the nearest neighbor graph:


The authors write on p. 1814:

&gt;For higher dimensions, we use the mutual neighborhood value; we say that vi and vj have a mutual neighborhood value K, if and only if vi is one of theK-nearest neighbors of vj and vj is one of theK-nearest neighbors of vi. We chose K such that the interactions connect all data points to one connected graph. 

",0,1
213,2015-5-14,2015,5,14,18,35xevn,difference between Latent and Explicit Semantic Analysis,https://www.reddit.com/r/MachineLearning/comments/35xevn/difference_between_latent_and_explicit_semantic/,sme1776,1431594331,,0,0
214,2015-5-14,2015,5,14,18,35xew4,Graph connectivity when using the Mutual Nearest Neighborhood value,https://www.reddit.com/r/MachineLearning/comments/35xew4/graph_connectivity_when_using_the_mutual_nearest/,food_hacking,1431594339,"Hello fellow redditors. 
I am currently implementing a clustering method named Superparamagnetic clustering. You can find the paper [here](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=2B77C7CF7A768EEAD915121237AD8CEE?doi=10.1.1.77.5569&amp;rep=rep1&amp;type=pdf)

However, my problem does not concern the clustering algorithm itself but the construction of the nearest neighbor graph:

I am currently implementing the method in Java and I want to verify my implementation with the Iris dataset (see ch. 5.4).  

Now my problem: 

The authors write on p. 1814:

&gt;For higher dimensions, we use the mutual neighborhood value; we say that vi and vj have a mutual neighborhood value K, if and only if vi is one of theK-nearest neighbors of vj and vj is one of theK-nearest neighbors of vi. **We chose K such that the interactions connect all data points to one connected graph**. 

in ch. 5.4 (clustering the Iris dataset) the authors write:
&gt;We determined neighbors in the D = 4 dimensional space according to the mutual K **(K = 5)** nearest neighbors definition

A can't see how this K value is enough to fully connect the graph. I need 50 MNNs in order to fully connect the graph. I checked this with my own implementation and with a Matlab implementation i found [here](http://www.mathworks.com/matlabcentral/fileexchange/34412-fast-and-efficient-spectral-clustering/content/files/SimilarityGraph/SimGraph_NearestNeighbors.m)

Does anybody have an idea what I could be doing wrong? Or confirm, that indeed one needs 50 MNN (what seems to be a really high number...) to connect the graph? 
What I can think of is that the authors somehow transformed the dataset or something like this before clustering..... 

Thank you, any help is highly appreciated! ",2,1
215,2015-5-14,2015,5,14,18,35xh3t,"Could you use machine learning to read people's comments on youtube or from blog posts, and classify them into groups of highly similar people?",https://www.reddit.com/r/MachineLearning/comments/35xh3t/could_you_use_machine_learning_to_read_peoples/,dsocma,1431596572,"How accurate do you think it would be (obviously would depend on how much you said, how serious it was. ect)

It would be kind of cool if you could find a bunch of people who were similar to you, and maybe more accurate than something like OK cupid, since you can actually see how people think if you analyze enough of their writing, right?  (analysis of grammatical constructions, ect.)

I would assume you could of course do it in theory, but it might be hard to do in practice.  Would a regular person be capable of doing this on their desktop computer?",5,8
216,2015-5-14,2015,5,14,18,35xhcg,Google x Amazon x Microsoft ML service comparison,https://www.reddit.com/r/MachineLearning/comments/35xhcg/google_x_amazon_x_microsoft_ml_service_comparison/,seni9,1431596782,,7,54
217,2015-5-14,2015,5,14,20,35xmj6,The Wolfram Language Image Identification Project,https://www.reddit.com/r/MachineLearning/comments/35xmj6/the_wolfram_language_image_identification_project/,galapag0,1431601339,,0,2
218,2015-5-14,2015,5,14,20,35xor8,QUESTION: How do multple neural networks work?,https://www.reddit.com/r/MachineLearning/comments/35xor8/question_how_do_multple_neural_networks_work/,ilevakam316,1431603148,"Hello -

I am new to the ML scene and I am trying to understand the best practice for designing neural networks that ""feed"" other neural networks.

For instance in the following example (image) we have two networks (1 &amp; 2) which are using input data feed by a file. They were trained on 80% of this file. Network 3 gets its inputs from the OUTPUTS of the previous two neural networks. 

My questions are the following:

1. Would I simply code three separate neural networks, run the first two and take their outputs and feed it to the third?
2. How would you go about training the third neural network? Would I create a separate file and manually recognize the patterns that should be found in 1&amp;2 and their expected output?",7,0
219,2015-5-14,2015,5,14,20,35xqvk,New here need help please.,https://www.reddit.com/r/MachineLearning/comments/35xqvk/new_here_need_help_please/,xtrancea,1431604774,"Hi I'm new to machine learning, just started Andrew NG's machine learning couse on coursera, is there anything else I should be doing side by side? Furthermore I wanted to start learning to program, is there anything that compliments these two skills? I'm just about to finish my bachelor's in science, physics major and want to diverge more towards data science, is it possible to on my own or you reckon I should just stick with what I know?",3,0
220,2015-5-14,2015,5,14,21,35xtin,How to select hyper parameters in support vector regression ?,https://www.reddit.com/r/MachineLearning/comments/35xtin/how_to_select_hyper_parameters_in_support_vector/,sharda2309,1431606517,"I want to use support vector regression to predict the future values in a time series. But how can I select the optimum value of hyper parameters like epsilon,C etc. 

What other machine learning methods can be used for time series prediction",1,0
221,2015-5-14,2015,5,14,23,35y84y,ICLR 2015 highlights,https://www.reddit.com/r/MachineLearning/comments/35y84y/iclr_2015_highlights/,yggdrasilly,1431614593,,1,21
222,2015-5-15,2015,5,15,0,35yht2,Identifying music genres using Clarifai's deep learning API,https://www.reddit.com/r/MachineLearning/comments/35yht2/identifying_music_genres_using_clarifais_deep/,terraces,1431619131,,1,6
223,2015-5-15,2015,5,15,3,35z1z5,[1406.3896] Freeze-Thaw Bayesian Optimization,https://www.reddit.com/r/MachineLearning/comments/35z1z5/14063896_freezethaw_bayesian_optimization/,downtownslim,1431628480,,2,14
224,2015-5-15,2015,5,15,4,35z6fr,Reinforcement Learning function approximation advice,https://www.reddit.com/r/MachineLearning/comments/35z6fr/reinforcement_learning_function_approximation/,ckrwc,1431630601,"I'm new to using reinforcement learning, and having having read through a bunch of papers and theses, I remain confused about the practical issues in using a non-linear function approximator.

The tutorials and classes I've seen only address the tabular (discrete state) version of RL, yet for larger (real) problems function approximation is a necessity for state generalization.

With advice from ""don't worry about it"" to Norvig/Russell's warning ""[convergence of] active learning and non-linear functions ... all bets are off"" leaves me cautious in approach.

Yet there are examples of successful non-linear solutions, but no practical documentation of the failures or warnings.

So I'm looking for guidance.  Given enough data (1 TB) , a continuous state space, no model or policy, what would be the best way to apply RL?

Synthesizing episodes and calculating (delayed) rewards is not a problem.

Thanks!",6,1
225,2015-5-15,2015,5,15,4,35zaqc,"This week's top news, announcements and resources in Machine Learning and Artificial Intelligence",https://www.reddit.com/r/MachineLearning/comments/35zaqc/this_weeks_top_news_announcements_and_resources/,mylittleai,1431632610,,0,1
226,2015-5-15,2015,5,15,9,360d9y,Coursera ML course assignments in Python,https://www.reddit.com/r/MachineLearning/comments/360d9y/coursera_ml_course_assignments_in_python/,mcdonalds_king,1431651411,Title sums up what I'm looking for. I found assignments on github but they already have solutions. I'm sure I had a link bookmarked earlier with only problem sets (no solutions) in python but I couldn't find it now. If anyone knows any source please share. Thanks.,7,5
227,2015-5-15,2015,5,15,10,360i0x,Deep Learning Pioneer Pushing GPU Neural Network Limits,https://www.reddit.com/r/MachineLearning/comments/360i0x/deep_learning_pioneer_pushing_gpu_neural_network/,[deleted],1431654001,,0,0
228,2015-5-15,2015,5,15,10,360jou,Maximizing accuracy in a regression forest,https://www.reddit.com/r/MachineLearning/comments/360jou/maximizing_accuracy_in_a_regression_forest/,internetidentity,1431654910,"Hi,

I am looking for tips for maximizing predictive accuracy in a regression forest by varying tuning parameters.  I am somewhat familiar with tuning classification forests to minimize the OOB error from projects in the past, e.g. by varying number of trees, number of bootstrapped predictors, assigning class weights, etc.  Any recommendations for ways I might be missing to reduce my OOB RMSE?  I am using R's randomForest package BTW.

Thanks! ",5,0
229,2015-5-15,2015,5,15,11,360n43,Deep Recurrent Neural Networks for Time Series Prediction,https://www.reddit.com/r/MachineLearning/comments/360n43/deep_recurrent_neural_networks_for_time_series/,peyman_n,1431656744,,3,19
230,2015-5-15,2015,5,15,11,360ona,"On Machine Learning Startups, Ostrich Mania, and the Uncanny Valley",https://www.reddit.com/r/MachineLearning/comments/360ona/on_machine_learning_startups_ostrich_mania_and/,highorderbits,1431657547,,0,1
231,2015-5-15,2015,5,15,12,360tnk,Cleanup feed production line auxiliary equipment,https://www.reddit.com/r/MachineLearning/comments/360tnk/cleanup_feed_production_line_auxiliary_equipment/,qiangwin-pellet,1431660455,,0,0
232,2015-5-15,2015,5,15,13,360z1i,ML in Healthcare?,https://www.reddit.com/r/MachineLearning/comments/360z1i/ml_in_healthcare/,Dsinglazers,1431663868,"Hi there,

I'm a 4th year medical student taking time off from medical school for research. I'm interested in applications of ML in healthcare, and have a basic programming background. I have backend access to all of Epic (the electornic health records system) data at my institution and have collaborated with a ML guy on creating a mortality prediction model using hospital data (labs, monitors, meds, etc)...

We have had great success with the results (implementation is the hard part) but I am hoping to dig into ML a little more myself before continuing my clinical training. I am comfortable using R and SQL, but have had difficulty in understanding how the ML topics I've been reading about online can be applied.


Just wondering if anyone here knows a health IT - ML community I could tap into, or if anyone had any suggestions of how I could learn about applications of ML for other similar questions.


TLDR: Medical student looking for advice on how to learn more about applying ML using healthcare data.",13,0
233,2015-5-15,2015,5,15,13,361044,Free Ebooks for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/361044/free_ebooks_for_machine_learning/,yokohua,1431664591,,3,96
234,2015-5-15,2015,5,15,14,36146i,Multi-Layer Perceptron OCR in JavaScript,https://www.reddit.com/r/MachineLearning/comments/36146i/multilayer_perceptron_ocr_in_javascript/,mateogianolio,1431667563,,0,0
235,2015-5-15,2015,5,15,15,3618bo,My first experience with machine learning using Amazon Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3618bo/my_first_experience_with_machine_learning_using/,LarsHoldgaard,1431670627,,1,1
236,2015-5-15,2015,5,15,18,361kcf,Handling variable size inputs efficiently in Torch,https://www.reddit.com/r/MachineLearning/comments/361kcf/handling_variable_size_inputs_efficiently_in_torch/,subszero,1431681531,"I have thinking about a way of implementing the model from this paper efficiently : http://arxiv.org/pdf/1412.6815v2.pdf
 
[The Model](http://i.imgur.com/RH4ZJD9.png)

How would one go about handling variable sized inputs for neural networks like in this case? Also, I'm unable to think of a way of processing documents in batches using this technique, would appreciate some pointers.


",3,2
237,2015-5-15,2015,5,15,20,361t2x,New York R Conference 2015 Talks &amp; Videos,https://www.reddit.com/r/MachineLearning/comments/361t2x/new_york_r_conference_2015_talks_videos/,kunjaan,1431689407,,0,6
238,2015-5-15,2015,5,15,23,3629lu,Would you buy your child a Chatterbot for *mas?,https://www.reddit.com/r/MachineLearning/comments/3629lu/would_you_buy_your_child_a_chatterbot_for_mas/,back_ache,1431699438,"In the news is that in addition to Barbie, another company is bringing out a talking-toy powered by an internet based Chatterbot, in this case powered by IBM's Watson.

Would you buy one for a child in your life?

As a parent and a techie, I'm torn!

https://en.wikipedia.org/wiki/Chatterbot#Incorporation_in_Childrens_Toys",2,0
239,2015-5-16,2015,5,16,2,362wzf,Segmenting time series into a bunch of known Gaussian distributions.,https://www.reddit.com/r/MachineLearning/comments/362wzf/segmenting_time_series_into_a_bunch_of_known/,cyanogenmod,1431710586,"I have N Gaussians whose parameters are known in advance. I have a series of time sequence values. The series actually is a concatenation of different sized samples drawn from any of these Gaussians. For example, the first 100 samples is from Gaussian A, the next 35 are from Gaussian B, next 50 are again from A and so on. Is there any established technique to segment the time sequence into these distributions? I want to split up the time series like this: segment I: (sample x to sample y, Gaussian K), segment J: (sample x' to sample y', Gaussian K') etc.",7,1
240,2015-5-16,2015,5,16,7,363vta,"How could you train a NN to detect ""fake"" images? (image inside)",https://www.reddit.com/r/MachineLearning/comments/363vta/how_could_you_train_a_nn_to_detect_fake_images/,omniron,1431727582,"https://i.imgur.com/uhMq0Re.jpg

This was posted in another thread, and it occurred to me that looking at it, i know this isn't a real object, but it's only after thinking about this that I become conscious of why I think that (background/foreground color and resolution mismatch).

I can also tell, but I don't know how exactly, that the refractions are off. It's interesting that without doing the snells laws and maxwell's laws equations, I know that light wouldn't bend that way through common materials of that type, given the background. Internally, there's some part of my brain that has a model for light refraction.

Does anyone know of any research or models that are able to learn features this way? Seems like these learned intuitions about how nature works are what lead to insights like Einstein and other great thinkers about unobserved thinks may work.",5,5
241,2015-5-16,2015,5,16,8,364417,"Best results on MNIST, CIFAR-10, CIFAR-100, STL-10, SVHN",https://www.reddit.com/r/MachineLearning/comments/364417/best_results_on_mnist_cifar10_cifar100_stl10_svhn/,[deleted],1431732082,,8,44
242,2015-5-16,2015,5,16,8,3647x4,How does ML applied to text differ from other types of problems?,https://www.reddit.com/r/MachineLearning/comments/3647x4/how_does_ml_applied_to_text_differ_from_other/,5bits,1431734219,"I have experience with applying ML algorithms to numerical and categorical data, but I can't seem to imagine how it can be applied to text (natural languages specifically).

I'm going to have to look into it, but I was hoping for a quick summary, analogy, and/or comparison to help me with the intuition of it.

I expect to have some answers by the end of the day, or you're fired.",6,1
243,2015-5-16,2015,5,16,9,364dip,What Type of Problems Can't be Solved with Neuronal Networks?,https://www.reddit.com/r/MachineLearning/comments/364dip/what_type_of_problems_cant_be_solved_with/,FractalNerve,1431737483,"It's understandable how and what Turing Machines are capable of, but what are the limits of neuronal networks? Are there things that can't be solved with a neuronal network, or are very inefficient to do so?",14,3
244,2015-5-16,2015,5,16,14,3651xg,Searching text in voice recordings,https://www.reddit.com/r/MachineLearning/comments/3651xg/searching_text_in_voice_recordings/,zhamisen,1431753751,"A new [iOS app](https://itunes.apple.com/us/app/english-us-keyword-search/id980529113) lets users search for words in voice recordings. According to [this site](http://www.pcworld.com/article/2914012/casio-iphone-app-does-keyword-searches-in-voice-recordings.html), the technology involves machine learning algorithms that compare the phonetic characteristics of input keywords to the recorded audio.
Have been any research papers published about this approach to searching text in voice recording?",2,2
245,2015-5-16,2015,5,16,17,365fkj,any one has experience of Boltzmann Machine?,https://www.reddit.com/r/MachineLearning/comments/365fkj/any_one_has_experience_of_boltzmann_machine/,godspeed_china,1431766475,"I'm testing my Boltzmann Machine code, but it produces 12% errors on a dataset. other methods such as random forest produces &lt;1% errors on this dataset. I'm not sure what's wrong.
My steps:
1: transform all variable to interval [0,1] by their rank/N. Thus, I treat them as binary variables for the BM training.
2: run positive-negative phase training with weight decay. train and test samples are pooled and trained together. In positive phase, test labels are not clamped. for clamped variables, I provide a random 0-1 value according to the corresponding probability at every iteration.
3: prediction by counting the probability of the ""free"" test labels.",9,1
246,2015-5-17,2015,5,17,0,36672v,[1412.6071] Fractional Max-Pooling,https://www.reddit.com/r/MachineLearning/comments/36672v/14126071_fractional_maxpooling/,downtownslim,1431789524,,0,20
247,2015-5-17,2015,5,17,13,368g3u,Is there an error correcting algorithm for this problem?,https://www.reddit.com/r/MachineLearning/comments/368g3u/is_there_an_error_correcting_algorithm_for_this/,Nixonite,1431836944,"Hello everyone,

So I have an array of class labels (integers ranging from 0 to 9), and it has a variable length but it's about 10-20 elements long.

It kind of looks like

[8 8 8 8 8 8 2 2 2 2 2 2 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]

Sometimes there are errors where a 2 is a 3 or any other digit, and sometimes there are a lot (although oftentimes &lt;5 errors in any instance). 

I need to convert that into a list of 4 elements. For the above list, the 4 element transformation would be [8 2 1 0]

I tried using the KISS principle and just taking the mode of the quartiles, it seems to work well but I'm wondering if there are any alternative ideas that the community here can suggest for such a problem.

Oh and some more rules to keep in mind:

The list doesn't have to have 4 distinct labels in it, it can have 1, 2, 3, 4, 5, up to 10 different labels (almost always &lt;5) if it's a really noisy list. The quartile mode system works well in the weird situations where the list might look like

[1 1 1 9 1 1 1 1 1 1 1 1 3 3 3 3 3]

where the above list should be converted into

[1 1 1 3]


Anyway, any ideas would be greatly appreciated. ",2,1
248,2015-5-17,2015,5,17,15,368pwk,Time-lapse Mining from Internet Photos,https://www.reddit.com/r/MachineLearning/comments/368pwk/timelapse_mining_from_internet_photos/,ehmo,1431844911,,18,187
249,2015-5-17,2015,5,17,16,368r87,"Artificial Neural Networks, to the point",https://www.reddit.com/r/MachineLearning/comments/368r87/artificial_neural_networks_to_the_point/,ai_maker,1431846216,,6,0
250,2015-5-17,2015,5,17,16,368si7,Open source implementation of FAIR's Memory Networks?,https://www.reddit.com/r/MachineLearning/comments/368si7/open_source_implementation_of_fairs_memory/,captcompile,1431847598,"Hi everyone

Does anyone know of an open-source implementation of Weston et. al.'s Memory Networks?",6,5
251,2015-5-17,2015,5,17,22,369chl,Hacker News Classification,https://www.reddit.com/r/MachineLearning/comments/369chl/hacker_news_classification/,tomtom1808,1431868337,"I'm currently working on a project that streamlines an HN-aggregation and classification service into a single API. I did a  writeup and thought I'll ask the community here what you think, since its packed with professionals. It's not too in-depth detailed, but it should still be interesting for tech-folks as well as non-tech folks. My mother-tongue is German, so please don't kill me if there are some mistakes. 

I'm interested in: 
1. is the article length okay and the content engaging enough to read on? I've deliberately tried to avoid a too scientific writing style. 

2. what do you think about the idea of the project? What would you do differently in terms of used libraries/practices/tools? Did you try something similar?

Thanks.

The (yet unofficial draft) link: http://www.newscombinator.com/2015-05-16",0,4
252,2015-5-17,2015,5,17,23,369hz9,Your path to now work in the area of machine learning,https://www.reddit.com/r/MachineLearning/comments/369hz9/your_path_to_now_work_in_the_area_of_machine/,bagelorder,1431872171,"What did you guys study, what where your milestones on the way to doing what you do today? And what do you do?",22,7
253,2015-5-18,2015,5,18,6,36ark5,Yann LeCun on Stringing GPU Clusters for Massive Neural Networks,https://www.reddit.com/r/MachineLearning/comments/36ark5/yann_lecun_on_stringing_gpu_clusters_for_massive/,[deleted],1431896823,,0,16
254,2015-5-18,2015,5,18,7,36b1zv,"To a MachineLearning laymen, what's the state of the art in the field right now?",https://www.reddit.com/r/MachineLearning/comments/36b1zv/to_a_machinelearning_laymen_whats_the_state_of/,GrilBTW,1431902297,,12,3
255,2015-5-18,2015,5,18,7,36b2nk,MOOC vs reading a book,https://www.reddit.com/r/MachineLearning/comments/36b2nk/mooc_vs_reading_a_book/,bagelorder,1431902648,"As someone who can program and understands some math topics, what is the best way to get into machine learning?
I see the two big alternatives MOOC and reading some book

Pro **MOOC**:
- Interactive which makes the learning process easier (for me)
- You sometimes get an certificate
- I heard that some pretty important researchers already have MOOCs online

Pro **Book**:
- MOOC are often kept easy (or that is how I feel it), difficult subjects are ommited
- You can skip around in a book between the subjects you already know easier

Is there a third alternative I am missing? What do you suggest for someone to get into the subject?",7,2
256,2015-5-18,2015,5,18,7,36b3kt,Data cleaning using multi-target decision trees,https://www.reddit.com/r/MachineLearning/comments/36b3kt/data_cleaning_using_multitarget_decision_trees/,BScHolder,1431903144,,5,12
257,2015-5-18,2015,5,18,8,36b7ci,How far can applied ML books get me in the work place?,https://www.reddit.com/r/MachineLearning/comments/36b7ci/how_far_can_applied_ml_books_get_me_in_the_work/,Nixonite,1431905180,"Hello everyone,

I'm curious, how far can applied ML books go as far as knowledge and usefulness in the workplace with regards to data analyst/scientist jobs, building models, and knowing how to do things 'the right way?'

Applied books such as:

* Introduction to Statistical Learning by Tibshirani
* Applied Predictive Modeling by Kuhn, Johnson
* Machine Learning: The Art and Science of Algorithms that Make Sense of Data by Peter Flach
* Mastering Machine Learning with Scikit-Learn

I'm just wondering what the differences are between someone tasked with a data job with the knowledge contained in the above books vs someone who has read books more like ESL. It's hard to tell from my applied-book bubble.",1,1
258,2015-5-18,2015,5,18,8,36bado,[Talk] Demis Hassabis - The Theory of Everything,https://www.reddit.com/r/MachineLearning/comments/36bado/talk_demis_hassabis_the_theory_of_everything/,vikkamath,1431906859,,1,12
259,2015-5-18,2015,5,18,11,36bnwy,Ring die pellet mill feed Common failure analysis and troubleshooting,https://www.reddit.com/r/MachineLearning/comments/36bnwy/ring_die_pellet_mill_feed_common_failure_analysis/,qiangwin-pellet,1431914539,,0,0
260,2015-5-18,2015,5,18,14,36c6he,Nvidia says Pascal GPUs to arrive next year with 10x speedup for deep learning,https://www.reddit.com/r/MachineLearning/comments/36c6he/nvidia_says_pascal_gpus_to_arrive_next_year_with/,dribnet,1431925940,,23,47
261,2015-5-18,2015,5,18,14,36c9qw,How to Get a Good Deal on Industrial Press Brakes,https://www.reddit.com/r/MachineLearning/comments/36c9qw/how_to_get_a_good_deal_on_industrial_press_brakes/,fcredo,1431928346,,0,0
262,2015-5-18,2015,5,18,15,36cbas,Implementing Neural Turing Machines,https://www.reddit.com/r/MachineLearning/comments/36cbas/implementing_neural_turing_machines/,iori42,1431929535,,8,13
263,2015-5-18,2015,5,18,15,36cdao,In natural language processing what are good ways of dealing with compound terms and contractions,https://www.reddit.com/r/MachineLearning/comments/36cdao/in_natural_language_processing_what_are_good_ways/,chchan,1431931123,"I am playing with NLP for document classification. I want to know what is a good way of processing compound terms such as ""The United States"", ""Wal-Mart Stores"", ""San Francisco"", and ""King James III"" and contractions such as ""don't"", ""won't"", ""ya'll"", and ""you're"".

Do I use a find and replace to replace these terms using regex before tokenization? (eg. United States =&gt; USA , you're =&gt; you are, San Francisco =&gt; San_Francisco) If so is there a library for a generic list of them?



",5,1
264,2015-5-18,2015,5,18,16,36cgm1,Best 1D Convolutions in Theano,https://www.reddit.com/r/MachineLearning/comments/36cgm1/best_1d_convolutions_in_theano/,alexmlamb,1431933891,"Hello, 

Does anyone have an opinion / experience with 1D convolutions in Theano?  I'm going to use a large # of filters, so I still want to keep things on the GPU.  

So far I've tried conv.conv2D, which works, but I'm not sure about how efficient it is.  From profiling, it looks like it doesn't replace it with GPU Corr MM.  ",3,1
265,2015-5-18,2015,5,18,16,36chtq,Petuum: a distributed machine learning framework,https://www.reddit.com/r/MachineLearning/comments/36chtq/petuum_a_distributed_machine_learning_framework/,galapag0,1431934998,,2,5
266,2015-5-18,2015,5,18,18,36cq05,"[help] lstm won't finish training. I think something in the input is causing it to not converge. I've found which piece it is, but not why.",https://www.reddit.com/r/MachineLearning/comments/36cq05/help_lstm_wont_finish_training_i_think_something/,[deleted],1431942565,"I'm training an lstm that I eventually want to use for sentiment analysis.

My plan to to convert the natural lang input into word vectors and then to binary. Currently, I'm just converting to binary, but the network won't finish it's initial training.

Here's the piece of input that is giving me trouble: `{input:binairise('I am happy'),output:[0,0,0,1,0]};`

And the binairise function:
  
    function binairise(input){
	var output=[];
	var inputArray=input.toString().split(/\s+/g);
	for(var i=0; i&lt;inputArray.length; i++){
		for(var j=0; j&lt;inputArray[i].length; j++){
			output.push(parseFloat(inputArray[i][j].charCodeAt(0).toString(2))/10000000);
		}
	}
	return output;
    }

The binairise function works, here's what it returns for binairise('I am happy'): `[0.10010011, 0.11000012, 0.11011013, 0.11014, 0.11000015, 0.1116, 0.1117, 0.1111001]`

I've tried it with the `/10000000` and without. The reason I'm doing that is to get the inputs in-between 0 and 1 so that the network can interpret them.

The network works fine with an input like [0,0] and the same ideal-output.

Does anyone have any ideas why this isn't finishing training? ",0,0
267,2015-5-18,2015,5,18,21,36d4hu,Why am I getting empty matrix from svmpredict ?,https://www.reddit.com/r/MachineLearning/comments/36d4hu/why_am_i_getting_empty_matrix_from_svmpredict/,sharda2309,1431953730,"I want to make predictions from a simple time series. The observations y=[11,22,33,44,55,66,77,88,99,110] at time x=[1,2,3,4,5,6,7,8,9,10]. I am using epsilon-SVR from libsvm toolbox.My code is as follows :

x1 = (1:7)'; % training set

y1 = [11, 22, 33, 44, 55, 66, 77]'; % observations from time series

options = ' -s 3 -t 2 -c 100 -g 0.05 -p 0.0003 ';

model = svmtrain(y1, x1, options)

x2 = (8:10)'; % test set

y2 = [88, 99, 110]'; % hidden values that are not used for training

[y2_predicted, accuracy] = svmpredict(y2, x2, model)

but the svmpredict function is giving me null output as shown below

y2_predicted =

     []
accuracy =

     []
Please help me resolve this .I am new to libsvm.",1,0
268,2015-5-18,2015,5,18,21,36d4wc,Time-lapse Mining from Internet Photos,https://www.reddit.com/r/MachineLearning/comments/36d4wc/timelapse_mining_from_internet_photos/,vadiiim,1431953967,,7,123
269,2015-5-19,2015,5,19,1,36dv77,How accurate are current weather forecasting systems? What sort of machine learning processes are common to the task of predicting weather patterns?,https://www.reddit.com/r/MachineLearning/comments/36dv77/how_accurate_are_current_weather_forecasting/,123A321,1431967227,,6,8
270,2015-5-19,2015,5,19,2,36e5jx,ICLR 2015's videos,https://www.reddit.com/r/MachineLearning/comments/36e5jx/iclr_2015s_videos/,[deleted],1431971833,,0,1
271,2015-5-19,2015,5,19,2,36e5s9,ICLR 2015's videos,https://www.reddit.com/r/MachineLearning/comments/36e5s9/iclr_2015s_videos/,vkhuc,1431971937,,6,10
272,2015-5-19,2015,5,19,3,36e7b7,"Deep Reinforcement Learning (ICLR2015, David Silver, Google DeepMind) [x-post /r/artificial]",https://www.reddit.com/r/MachineLearning/comments/36e7b7/deep_reinforcement_learning_iclr2015_david_silver/,True-Creek,1431973019,,3,19
273,2015-5-19,2015,5,19,3,36e9l6,NNs vs kernel methods,https://www.reddit.com/r/MachineLearning/comments/36e9l6/nns_vs_kernel_methods/,asymptotics,1431973957,"My understanding is that both classes of methods are ""universal approximators"", so what are the cases where we might prefer one over the other?",10,2
274,2015-5-19,2015,5,19,5,36eqpo,The Five Elements of Data Science Process,https://www.reddit.com/r/MachineLearning/comments/36eqpo/the_five_elements_of_data_science_process/,[deleted],1431981242,,0,0
275,2015-5-19,2015,5,19,5,36ernv,Special care when using global average pooling?,https://www.reddit.com/r/MachineLearning/comments/36ernv/special_care_when_using_global_average_pooling/,entron,1431981637,"I have a working CNN code which has some convolutional layers and dense layers with softmax output in the finial layer. The CIFAR-10 error rate of the network is about 20% without pre-processing and data augmentation. I want to try the global average pooling as in the paper: Network In Network (arXiv:1312.4400) and Striving for Simplicity: The All Convolutional Net (arXiv:1412.6806). However, the network stops learning after I added global average pooling. If I connect the output of global average pooling to another logistic regression layer, it learns again. I suspect the reason is due to bad initialization or learning rate. However after some experiment I still could not find any solution. Could some one tell me what might be the cause of my problem?",0,0
276,2015-5-19,2015,5,19,6,36eure,"""Is unsupervised pre-training still useful given recent advances? If so, when?"" (ICLR 2015 workshop paper)",https://www.reddit.com/r/MachineLearning/comments/36eure/is_unsupervised_pretraining_still_useful_given/,[deleted],1431982913,,7,12
277,2015-5-19,2015,5,19,6,36evlc,Particle Swarm Optimization in F# part 2,https://www.reddit.com/r/MachineLearning/comments/36evlc/particle_swarm_optimization_in_f_part_2/,DanielSlater8,1431983281,,1,0
278,2015-5-19,2015,5,19,6,36exww,"One piece of input per input-neuron, but what does one do if the size of the input is unknown?",https://www.reddit.com/r/MachineLearning/comments/36exww/one_piece_of_input_per_inputneuron_but_what_does/,[deleted],1431984298,"I'm wondering what one is to do in constructing an rnn where there is an input-neuron per piece of input, how does one deal with input that is of an unspecified length? Trim it? For example an natural-sentence that I am vectorising ( each word ). I don't know how many words will be in the sentence or even the maximum words in the sentence unless I set a limit. Is there a technique that I am missing for dealing with input of an unspecified length?",10,0
279,2015-5-19,2015,5,19,8,36fdeo,"My output is a scalar vs. time curve, can this be used to make training more efficient?",https://www.reddit.com/r/MachineLearning/comments/36fdeo/my_output_is_a_scalar_vs_time_curve_can_this_be/,EndingPop,1431991294,"I'm new to ML, having completed Andrew Ng's class fairly recently. I have a problem where the key output is a scalar vs. time curve. I know that I can discretize that curve and just use it as is, but I'm wondering if there's a way to use that information to make the training more efficient. 

I know that the value of the scalar at a given time is dependent, in part, on the value of the scalar in the past. If we put in the past values as input to compute the current value, would that make it train faster?

I've done some literature searches on this, but since I'm new to the field I may not know the right keywords. Does anyone know if this has been done before? I appreciate input and any relevant references.",0,2
280,2015-5-19,2015,5,19,10,36fq51,Kingsland iron Workers,https://www.reddit.com/r/MachineLearning/comments/36fq51/kingsland_iron_workers/,fcredo,1431997402,,0,1
281,2015-5-19,2015,5,19,10,36fq6x,Edu-Videos | 100 Most Popular Machine Learning Talks at VideoLectures.Net,https://www.reddit.com/r/MachineLearning/comments/36fq6x/eduvideos_100_most_popular_machine_learning_talks/,Friars1993,1431997423,,0,1
282,2015-5-19,2015,5,19,10,36fqt2,Simple trick to speedup dropout: Efficient batchwise dropout training using submatrices,https://www.reddit.com/r/MachineLearning/comments/36fqt2/simple_trick_to_speedup_dropout_efficient/,downtownslim,1431997728,,1,7
283,2015-5-19,2015,5,19,11,36fxs1,Grinding--mixing--pelletizing--cooling--packing,https://www.reddit.com/r/MachineLearning/comments/36fxs1/grindingmixingpelletizingcoolingpacking/,qiangwin-pellet,1432001183,,1,0
284,2015-5-19,2015,5,19,11,36fy5z,Courser: Introduction to Recommender Systems,https://www.reddit.com/r/MachineLearning/comments/36fy5z/courser_introduction_to_recommender_systems/,[deleted],1432001388,,0,0
285,2015-5-19,2015,5,19,11,36g094,Salary Range for Lead Data position at Fortune 500 Company.,https://www.reddit.com/r/MachineLearning/comments/36g094/salary_range_for_lead_data_position_at_fortune/,[deleted],1432002476,"Hello /r/machinelearning, I'm looking for some help on figuring out the salary range for the follow position:

**Location**: Central United States (Midwest) -  Metro population of a few million.

**Position Title**: Data Integrity Specialist (Lead) at corporate headquarters of a Fortune 500 company.

**Job Summary**

* Daily supervision of operations and functions of the team: scheduling changes, tracking associate attendance, monitoring associate performance, as well as coaching and training associates.
* Participate in hiring and interviewing process / candidate selection.
* Regularly monitor performance, workflow, and provide feedback to the associates, as well as the manager.
* Responsible for providing full coverage of workload and meeting SLAs with overtime and budget constraints in mind; developing weekly team schedules. Monitor these schedules to ensure staff is working the hours, including any modifications.
* Participate and assist in staff department meetings.
* Develop training material and/or make changes changes to existing materials.
* Identify issues in current processes and develop solutions; notify management of concerns and solutions.
* Serve as the liaison between cross-functional departments.
* Through the analysis of tasks and resolution by linking or separating records involved, you will ensure the integrity of data within our enterprise-wide database; responsible for completing linkage, as well as separation requests.
* To ensure all records are from one unique entity, review the records within an Enterprise ID.
* To determine whether records belong to one unique entity or separate ones, it is your responsibility to review records within two separate Enterprise IDs.
* Review daily tasks lists and resolves.
* Ensure data integrity by updating MDM and source system information.
* Work with project team and ensure theyre aware of all program changes/upgrades/outages/.
* Take direction from manager and communicate to the associates on a regular basis.
* Assist Manager to ensure staff is properly trained and is built to execute on the department's needs.

**Minimum Requirements**

* Supervisor Experience, 1 Year Minimum
* Associates Degree
* Previous Data Stewardship Experience
* Administrative Experience, 1-2 Years Minimum
* Intermediate Microsoft Office Knowledge
* Windows OS Experience

**Hierarchy**

Manager &gt; **Lead / Specialist** &gt; Associate / Assistant

Thanks for the help ahead of time!",1,0
286,2015-5-19,2015,5,19,13,36gfke,Looking for imagenet CNNs that are available for both theano and matconvnet.,https://www.reddit.com/r/MachineLearning/comments/36gfke/looking_for_imagenet_cnns_that_are_available_for/,deep_learner,1432011007,"Hi,
I wanted to try out a comparison between a code written in matconvnet and its theano implementation by me. For this I need the same imagenet CNN in both platforms, so i can compare layer wise values etc. 

so far I have been unable to find a network apart from Alex-net.  Is there any such CNN i am missing?",0,0
287,2015-5-19,2015,5,19,15,36gloz,"Weak Learning, Boosting, and the AdaBoost algorithm",https://www.reddit.com/r/MachineLearning/comments/36gloz/weak_learning_boosting_and_the_adaboost_algorithm/,iori42,1432015222,,0,8
288,2015-5-19,2015,5,19,15,36gmm0,Machined Learnings: ICLR 2015 Review,https://www.reddit.com/r/MachineLearning/comments/36gmm0/machined_learnings_iclr_2015_review/,iori42,1432015827,,0,12
289,2015-5-19,2015,5,19,18,36h22b,Experiments and grid search management,https://www.reddit.com/r/MachineLearning/comments/36h22b/experiments_and_grid_search_management/,ciolaamotore,1432028916,"What do you use for keep track of experiments like in hyper parameter optimization?

I've seen tools like jobman from deeplearning.net or even the use of hyperopt (even though bayesian model selection is unnecessary). I would be great to store things on a db or on some kind of persistence layer; jobman supports sqlite e postresql, while hyperopt is integrated with pymongo. However both still use python2...(which is a major deterrent to me)

Any other alternatives out there? What is your experimental research routine?",7,4
290,2015-5-19,2015,5,19,20,36h8n8,waifu2x: anime art upscaling and denoising with deep convolutional neural networks,https://www.reddit.com/r/MachineLearning/comments/36h8n8/waifu2x_anime_art_upscaling_and_denoising_with/,linuxjava,1432034060,,43,81
291,2015-5-19,2015,5,19,21,36hgm6,Plots from book 'pattern recognition and machine learning'?,https://www.reddit.com/r/MachineLearning/comments/36hgm6/plots_from_book_pattern_recognition_and_machine/,phoenixbai,1432039033,"Hi,

where can I find the detailed explanations about the plots presented in the bishop`s book? 
if the corresponding plot code is available, would be really awesome.

thanks",0,0
292,2015-5-19,2015,5,19,22,36hnfg,"Automatic Inference of Search Patterns for Taint-Style Vulnerabilities by Fabian Yamaguchi, Alwin Maier, Hugo Gascon, and Konrad Rieck [PDF]",https://www.reddit.com/r/MachineLearning/comments/36hnfg/automatic_inference_of_search_patterns_for/,turnersr,1432042483,,0,1
293,2015-5-20,2015,5,20,0,36hz8h,Neural Networks: varying latent distributions,https://www.reddit.com/r/MachineLearning/comments/36hz8h/neural_networks_varying_latent_distributions/,bge0,1432047769,"I'm guessing when a neural network converges the weights generally converge to a specific distribution (or linear combination of them). Are there any techniques that can be applied to make the network update the weights to conform to a new distribution?


I'm looking at mainly an on-line algorithm that utilizes SGD. Would SGD **itself** be enough to pull the network out of the minima that it might have converged to?  Ideally I would like to adapt to the new distribution rather than completely retrain.

**Update: adding more info**

You are generally minimizing dW/dE where dE is the error. So 'convergence' is generally when the updates to W are very small. I.e. in the simplest sense: 

  - W(t+1)=W(t)-alpha*dW/dE 
  - abs(W(t+1) - W(t)) &lt; epsilon

However, these weights are hypothetically positioned at some local (or global) minima for some distribution that properly enabled y = f(W*x) + b for all given x **up till** now. Now supposing x changes entirely, eg: you are say modeling some sort of traffic pattern and then it changes altogether (i.e. add another cluster node or something). If the alpha is too small it won't escape this minima. So is there a way to perhaps scale alpha dynamically at some point to make this occur? In momentum based SGD the momentum should drop pretty low after convergence I'm guessing. So something like an inverse operation of this. Just curious if anyone has heard of something like this.",12,0
294,2015-5-20,2015,5,20,0,36i2p6,We are ignoring the new machine age at our peril | The Guardian - Comment is free,https://www.reddit.com/r/MachineLearning/comments/36i2p6/we_are_ignoring_the_new_machine_age_at_our_peril/,salazarj,1432049315,,0,0
295,2015-5-20,2015,5,20,0,36i55d,Implementing the Logistical Regression Classifier,https://www.reddit.com/r/MachineLearning/comments/36i55d/implementing_the_logistical_regression_classifier/,mfournierca,1432050385,,0,0
296,2015-5-20,2015,5,20,3,36ird5,"Planning, implementation, and analysis of a Web application using Python, PlanOut, and R (WWW215, with notebooks)",https://www.reddit.com/r/MachineLearning/comments/36ird5/planning_implementation_and_analysis_of_a_web/,cast42,1432059232,,0,0
297,2015-5-20,2015,5,20,6,36jj3h,Neural Network Summer Curriculum?,https://www.reddit.com/r/MachineLearning/comments/36jj3h/neural_network_summer_curriculum/,YourWelcomeOrMine,1432070019,"I have a relatively light schedule this summer. I'd like to have a solid understanding of neural networks by the end of the summer? Could you provide, or point me to, a very specific curriculum?

**EDIT**  
My background:  
I have been programming for a few years, and know Python, C++, C# and Java. I haven't taken Calculus since college (8 years ago). I understand the basic idea of linear algebra, and am pretty comfortable with basic statistics.",19,5
298,2015-5-20,2015,5,20,6,36jk8r,Part I of the Deep Learning textbook is now complete - Version 19/05/2015,https://www.reddit.com/r/MachineLearning/comments/36jk8r/part_i_of_the_deep_learning_textbook_is_now/,clbam8,1432070459,,12,125
299,2015-5-20,2015,5,20,7,36jrbk,[1505.04771] DopeLearning: A Computational Approach to Rap Lyrics Generation,https://www.reddit.com/r/MachineLearning/comments/36jrbk/150504771_dopelearning_a_computational_approach/,downtownslim,1432073350,,15,32
300,2015-5-20,2015,5,20,9,36kd5v,Time Series Predictions and More,https://www.reddit.com/r/MachineLearning/comments/36kd5v/time_series_predictions_and_more/,CireNeikual,1432083140,,6,7
301,2015-5-20,2015,5,20,12,36kzf5,Game dev turned AI researcher.,https://www.reddit.com/r/MachineLearning/comments/36kzf5/game_dev_turned_ai_researcher/,abignold,1432094151,,4,5
302,2015-5-20,2015,5,20,14,36larp,Metal Fabrication: Its Processes &amp; Applications,https://www.reddit.com/r/MachineLearning/comments/36larp/metal_fabrication_its_processes_applications/,ESolpk-SEO,1432100744,,0,1
303,2015-5-20,2015,5,20,15,36lcwq,Hologram Hot Stamping Machine,https://www.reddit.com/r/MachineLearning/comments/36lcwq/hologram_hot_stamping_machine/,sonalbisht101,1432102182,"Foil is one of the significant factors in hot hologram hot stamping machine made from polyester film material and foil must be selected to take clear print, depending on product material. @ 
http://glipho.com/sonal-bisht/brief-introduction-on-hot-stamping-hologram-foils	",0,1
304,2015-5-20,2015,5,20,15,36lgm0,Use of computational linguistics in NLP?,https://www.reddit.com/r/MachineLearning/comments/36lgm0/use_of_computational_linguistics_in_nlp/,[deleted],1432104854,"With the success of methods such as deep learning that don't assume anything about syntax or grammar, are there still uses for linguistics in machine translation/spelling correction/information retrieval/etc?",4,0
305,2015-5-20,2015,5,20,16,36ljgf,Cliques from http://www.mmds.org/(2nd edition),https://www.reddit.com/r/MachineLearning/comments/36ljgf/cliques_from_httpwwwmmdsorg2nd_edition/,mohanradhakrishnan,1432106921,"I am reading this chapter and understand up to this section.  What is the significance of this separate section ?

Section 10.3.1 in chapter 'Miining Social-Network graphs'

Example 10.10 : Suppose our graph has nodes numbered 1, 2, . . . , n and there
is an edge between two nodes i and j unless i and j have the same remainder
when divided by k. Then the fraction of possible edges that are actually
present is approximately (k  1)/k. There are many cliques of size k, of which
{1, 2, . . . , k} is but one example.
Yet there are no cliques larger than k. To see why, observe that any set of
k + 1 nodes has two that leave the same remainder when divided by k. This
point is an application of the pigeonhole principle. Since there are only k
different remainders possible, we cannot have distinct remainders for each of
k + 1 nodes. Thus, no set of k + 1 nodes can be a clique in this graph.",3,0
306,2015-5-20,2015,5,20,16,36ljow,An introduction to Random Indexing word space models [PDF],https://www.reddit.com/r/MachineLearning/comments/36ljow/an_introduction_to_random_indexing_word_space/,alexeyr,1432107085,,0,7
307,2015-5-20,2015,5,20,16,36lkj8,Using Abs() in RMSProp,https://www.reddit.com/r/MachineLearning/comments/36lkj8/using_abs_in_rmsprop/,personalityson,1432107732,"Hi,
RMSProp from how I have seen it is usually implemented:

    g_hist = 0.9 * g_hist + (1 - 0.9) * g^2
    W = W + lr * g / Sqrt(g_hist + 0.000001)


Is it not essentially the same as:

    g_hist = 0.9 * g_hist + (1 - 0.9) * Abs(g)
    W = W + lr * g / (g_hist + 0.000001)


And requires less computation? Plus it does not distort the moving average around 1 (gradients smaller than 1 become smaller when squared, and the opposite above 1)",15,3
308,2015-5-20,2015,5,20,16,36llt8,Estimating treatment effects and power analysis with R in ipython,https://www.reddit.com/r/MachineLearning/comments/36llt8/estimating_treatment_effects_and_power_analysis/,cast42,1432108757,,1,0
309,2015-5-20,2015,5,20,17,36llwr,Rotary Screw Air Compressors Purchasing,https://www.reddit.com/r/MachineLearning/comments/36llwr/rotary_screw_air_compressors_purchasing/,fcredo,1432108837,,0,0
310,2015-5-20,2015,5,20,19,36lxeg,Ottobox smart plug turns devices on and off based on usage patterns,https://www.reddit.com/r/MachineLearning/comments/36lxeg/ottobox_smart_plug_turns_devices_on_and_off_based/,ponyisleam,1432118012,,0,0
311,2015-5-21,2015,5,21,0,36n041,A really easy way to discover Experts in Machine Learning. We can also discover the latest blogs/opinions/articles/videos shared by these experts &amp; the conversations these experts are engaging in. A one stop shop to get all your content in 35000 deep topics and communities.,https://www.reddit.com/r/MachineLearning/comments/36n041/a_really_easy_way_to_discover_experts_in_machine/,rmi6,1432137593,,0,0
312,2015-5-21,2015,5,21,1,36n4di,"A handy flowchart to help data scientists pick the ""right"" ML algorithm",https://www.reddit.com/r/MachineLearning/comments/36n4di/a_handy_flowchart_to_help_data_scientists_pick/,MLBlogTeam,1432139250,,0,1
313,2015-5-21,2015,5,21,1,36n5qj,BigML: Machine Learning made easy,https://www.reddit.com/r/MachineLearning/comments/36n5qj/bigml_machine_learning_made_easy/,alexcasalboni,1432139789,,0,2
314,2015-5-21,2015,5,21,2,36ng8l,Data Mining Techniques for image processing,https://www.reddit.com/r/MachineLearning/comments/36ng8l/data_mining_techniques_for_image_processing/,kunal4097,1432143845,"Hey. I have a project about using data mining techniques for image processing but i don't know where to start. Currently i am learning about cluster analysis in data mining.
What would you suggest? What techniques should I use ?

Thanks guys!",5,0
315,2015-5-21,2015,5,21,4,36nz0r,Would love some feedback on my blog from someone who knows about deep learning,https://www.reddit.com/r/MachineLearning/comments/36nz0r/would_love_some_feedback_on_my_blog_from_someone/,SEOClear,1432150996,,4,0
316,2015-5-21,2015,5,21,5,36o3g2,Master's programs in data science,https://www.reddit.com/r/MachineLearning/comments/36o3g2/masters_programs_in_data_science/,fuzzy191,1432152762,"Hey all, 

I've been working as a data analyst at a startup incubator since graduating in 2014 and I'd like to continue my education by getting a master's in data science. 

My question is what are the top schools for this program? I've been checking out MIT, Stanford and CMU but what are the other good programs? If anyone has any anecdotal experience with any of these programs I'd love to hear it as well! My application would be for fall 2016.

I'm really trying to find a program that is funded as well! ",29,3
317,2015-5-21,2015,5,21,6,36oj5o,[1412.6806] Striving for Simplicity: The All Convolutional Net,https://www.reddit.com/r/MachineLearning/comments/36oj5o/14126806_striving_for_simplicity_the_all/,[deleted],1432159172,,7,17
318,2015-5-21,2015,5,21,7,36oli6,Is Machine Learning Important For Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/36oli6/is_machine_learning_important_for_deep_learning/,bagelorder,1432160156,"I thougt I needed to learn ML first but yesterday someone posted a deep Learning book which just offers a quick intro to ML for people who don't know it yet and then goes straight to deep Learning.

In long run I want to get to research in the area but in short-term it would be pretty cool to get fast to the current technology.

How important is it to get a good understanding of ML to understand DL and in the long run get a deep understanding of both?",7,0
319,2015-5-21,2015,5,21,8,36ouxg,"Resig, Using Waifu2x to Upscale Japanese Prints",https://www.reddit.com/r/MachineLearning/comments/36ouxg/resig_using_waifu2x_to_upscale_japanese_prints/,ford_beeblebrox,1432164176,,5,8
320,2015-5-21,2015,5,21,10,36p85y,new DeepMind paper: Weight Uncertainty in Neural Networks,https://www.reddit.com/r/MachineLearning/comments/36p85y/new_deepmind_paper_weight_uncertainty_in_neural/,DavidJayHarris,1432170234,,18,14
321,2015-5-21,2015,5,21,11,36pjnp,"[poll] /r/machinelearning, what's your level of education?",https://www.reddit.com/r/MachineLearning/comments/36pjnp/poll_rmachinelearning_whats_your_level_of/,whizkid77,1432175680,,3,0
322,2015-5-21,2015,5,21,12,36pphc,"$10 hedge fund supercomputer sweeps Wall Street Big Data with AI, ML &amp; power from the cloud (X-Post r/bigdata)",https://www.reddit.com/r/MachineLearning/comments/36pphc/10_hedge_fund_supercomputer_sweeps_wall_street/,iamraybies,1432178587,,0,2
323,2015-5-21,2015,5,21,13,36pyxj,"NPR's Consider The Following on the ""Ellie"" program used to screen for PTSD",https://www.reddit.com/r/MachineLearning/comments/36pyxj/nprs_consider_the_following_on_the_ellie_program/,NukeNoodles,1432183716,,0,0
324,2015-5-21,2015,5,21,16,36qa7o,What makes a good Haar feature response?,https://www.reddit.com/r/MachineLearning/comments/36qa7o/what_makes_a_good_haar_feature_response/,lp0_on_fire_,1432192569,"Hello! I'm currently working on implementing the good old, tried and tested, Viola Jones algorithm using a known classifier, but quite stuck on few points. Specifically understanding the classifier, and I think, what even makes a response from applied a haar-like feature. I feel like I'm missing something basic here, and I would be infinitely grateful for clarification

So, the response first. All the literature states that calculating the response means summing the intensities of two adjacent rectangles and subtracting their respective values. It's here that my confusion starts: what then, does the most idle response look like? I thought that if one took, lets say, a side-by-side white-black (type 1a) kernel and applied it on a white pixel next to a black pixel, then a perfect ""hit' or ""match"" would mean that our response would equal 0! But... this clearly can't ever be the case. A black pixel is just equal to zero, and plainly 255-0=255. 

This result only adds to my confusion when I look at any xml classifier.

    &lt;!-- tree 0 --&gt;
       &lt;_&gt;
          &lt;!-- root node --&gt;
             &lt;feature&gt;
                &lt;rects&gt;
                   &lt;_&gt;3 7 14 4 -1.&lt;/_&gt;
                   &lt;_&gt;3 9 14 2 2.&lt;/_&gt;&lt;/rects&gt;
                &lt;tilted&gt;0&lt;/tilted&gt;&lt;/feature&gt;
             &lt;threshold&gt;4.0141958743333817e-003&lt;/threshold&gt;
            &lt;left_val&gt;0.0337941907346249&lt;/left_val&gt;
            &lt;right_val&gt;0.8378106951713562&lt;/right_val&gt;&lt;/_&gt;&lt;/_&gt;

Here the threshold for this feature is a tiny 0.004! Quite a far cry from 255, and a floating value! How this happened.. I can't begin to understand :/ ",0,0
325,2015-5-21,2015,5,21,17,36qdyi,Demistifying LSTM Neural Networks,https://www.reddit.com/r/MachineLearning/comments/36qdyi/demistifying_lstm_neural_networks/,iori42,1432195429,,8,21
326,2015-5-21,2015,5,21,17,36qhhs,Question about mixing a c# application with machine learning,https://www.reddit.com/r/MachineLearning/comments/36qhhs/question_about_mixing_a_c_application_with/,mcboman,1432198695,"Hi Folks
I'm about to make my description for next semesters specialization and I wan't to use machine learning for specialization.

I have created this C# web application (a mix of MVC and web API) that collect user info. The application does also collect some information about genres that the user looks at and which films they wan't to watch.

I'm completely new in the data science area and I wan't the application to be able to use a recommendation algorithm to give the user some suggestions for what they might also wan't to see (like netflix). 

What have confused me the most about the topic is that all the online courses and books I find is using R or Python. Am I supposed to do the same?

I am supposed to write a description on what I will gather information about the topic (machine learning) and how I wanna use it. I also have a time limit of 10 weeks, but I can add 3 months unofficial time (summer vacation) where I can get learning.

Machine Learning is a wide area so I has to limit to subject, so hope you can help.",5,0
327,2015-5-21,2015,5,21,18,36qkex,Uses of Machine Tools,https://www.reddit.com/r/MachineLearning/comments/36qkex/uses_of_machine_tools/,Md-Malik,1432201327,,0,1
328,2015-5-21,2015,5,21,21,36qy69,Train a lstm with a sequence,https://www.reddit.com/r/MachineLearning/comments/36qy69/train_a_lstm_with_a_sequence/,[deleted],1432211908,"Due to the recursive nature, I've been able to activate an lstm, which has only 1 input-neuron, a sequence by inputting one item at a time.

However, when I attempt to train the network with the same technique, it never converges. The training goes one forever.

Here's what I'm doing, I'm converting a natural-language string to binary and then feeding one digit as a time. Later, I will vectorise the words and then convert them into binary. The reason I am converting into binary is because the network only takes values between 0 and 1.

I know the training work because when I train with an array of as many values as the input-neurons, in this case 1 so: [0], it converges and trains fine.

I guess I could pass each digit individually, but then it would have an individual ideal-output for each digit. And when the digit appears again with another ideal-output in another training set, it won't converge because how could for example 0 be of class 0 and 1?
Please tell me if I am wrong on this assumption.

Here is the code for the trainer I am using if you have any idea for implementation, which I would greatly appreciate.

I'd actually appreciate any help in figuring this out.",5,0
329,2015-5-21,2015,5,21,22,36r1hx,VDiscover: large-scale vulnerability discovery using Machine Learning,https://www.reddit.com/r/MachineLearning/comments/36r1hx/vdiscover_largescale_vulnerability_discovery/,galapag0,1432213800,,0,7
330,2015-5-21,2015,5,21,22,36r2xi,First steps with policy gradients in reinforcement learning.,https://www.reddit.com/r/MachineLearning/comments/36r2xi/first_steps_with_policy_gradients_in/,[deleted],1432214551,,0,1
331,2015-5-21,2015,5,21,22,36r5n5,Slightly meta:,https://www.reddit.com/r/MachineLearning/comments/36r5n5/slightly_meta/,dunnowhattoputhere,1432215985,,30,104
332,2015-5-21,2015,5,21,23,36rd91,Any reason behind the fact that the filter size of convolutional layers are mostly odd numbers in the literature?,https://www.reddit.com/r/MachineLearning/comments/36rd91/any_reason_behind_the_fact_that_the_filter_size/,entron,1432219610,,4,2
333,2015-5-21,2015,5,21,23,36re30,Data science makes an impact on Wall Street,https://www.reddit.com/r/MachineLearning/comments/36re30/data_science_makes_an_impact_on_wall_street/,gradientflow,1432220019,,2,0
334,2015-5-22,2015,5,22,0,36rlo6,Multi-hidden layer Feed Forward Neural Network,https://www.reddit.com/r/MachineLearning/comments/36rlo6/multihidden_layer_feed_forward_neural_network/,mikeDepies,1432223601,"I've been working on an inhouse ML framework and I've just finished getting the essentials built to run a Neural Network. I've arranged a simple Feed Forward NN Fully connected with 1 and 2 hidden layers. With a few parameter adjustments I've been able to get both architectures to work. I'm utilizing Momentum and Weight decay as additional parameters.

When I move to 3 layers I see massive weight explosions and I suspect other things may be going awry. I haven't been able to find much information about the details in ""deep""(beyond 1 hidden layer) networks. I suspect I need to limit my weight growth, but I am not entirely sure how to approach an appropriate solution. Any suggestions, information or resources(academic papers, website, videos, etc.) would be really awesome.

So my question more clearly stated is: How do I contain the exploding weights? and more importantly, what else do I need to be aware of as I move into more complicated network structures? ",12,0
335,2015-5-22,2015,5,22,0,36rlxq,The Brain is a Universal Dynamical Systems Computer  Hierarchical Temporal Memory,https://www.reddit.com/r/MachineLearning/comments/36rlxq/the_brain_is_a_universal_dynamical_systems/,fergbyrne,1432223722,,6,3
336,2015-5-22,2015,5,22,2,36s2uy,"Machine-Learning Algorithm Mines Rap Lyrics, Then Writes Its Own",https://www.reddit.com/r/MachineLearning/comments/36s2uy/machinelearning_algorithm_mines_rap_lyrics_then/,[deleted],1432230726,,0,0
337,2015-5-22,2015,5,22,3,36s673,The Unreasonable Effectiveness of Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/36s673/the_unreasonable_effectiveness_of_recurrent/,Tatsu23456,1432232086,,60,200
338,2015-5-22,2015,5,22,3,36s68c,Big jump in CIFAR 10/100 accuracy: Spatially-sparse convolutional neural networks,https://www.reddit.com/r/MachineLearning/comments/36s68c/big_jump_in_cifar_10100_accuracy_spatiallysparse/,downtownslim,1432232100,,13,11
339,2015-5-22,2015,5,22,4,36se9d,Ideas on measuring number of authors or stylistic consistency in a given text source?,https://www.reddit.com/r/MachineLearning/comments/36se9d/ideas_on_measuring_number_of_authors_or_stylistic/,[deleted],1432235519,"I am trying to measure how coherent and internally consistent laws are across different time periods. I want to be able to measure, for example, how many different issues are discussed in a given law, how writing style varies across the law, etc. I figure a topic model might be able to measure how topically consistent the contents are, but am not quite certain how otherwise I might measure this or determine how many different authors a law has. Anyone have any ideas?",1,0
340,2015-5-22,2015,5,22,7,36t39n,Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?,https://www.reddit.com/r/MachineLearning/comments/36t39n/do_we_need_hundreds_of_classifiers_to_solve_real/,alexcasalboni,1432246301,,0,1
341,2015-5-22,2015,5,22,8,36t9mg,"CNNs: relationship between CNNs, Pooling and Nonliniarities?",https://www.reddit.com/r/MachineLearning/comments/36t9mg/cnns_relationship_between_cnns_pooling_and/,luvmunky,1432249273,"So I've been playing around with CNNs, using Caffe. Looking at the sample networks they have, I'm surprised to see some interesting differences.

I was under the impression that you'd typically have layers like these: Convolution --&gt; Nonliniarity (ReLU or Tanh) --&gt; Pooling (for reduction). Or, you'd have Convolution --&gt; Nonliniarity --&gt; Convolution --&gt; NonLiniarity ... --&gt; Pooling

But in some of the sample networks I've seen, you can have Convolution --&gt; Pooling --&gt; Nonliniarity; or Convolution --&gt; Pooling --&gt; Convolution --&gt; Pooling --&gt; Nonliniarity; or heck even Convolution --&gt; Convolution --&gt; Convolution --&gt; Nonliniarity.

I guess my question is: shouldn't a Convolutional layer always be followed by a Nonliniarity layer, before doing anything else?",7,0
342,2015-5-22,2015,5,22,8,36tbpo,So I used Latent Dirichlet Allocation to automatically group Steam games by genre based on the words used on the store page. The results are pretty interesting.,https://www.reddit.com/r/MachineLearning/comments/36tbpo/so_i_used_latent_dirichlet_allocation_to/,Aks95,1432250235,,3,9
343,2015-5-22,2015,5,22,8,36tbyb,How We Think About Privacy and Finding Features in Black Boxes,https://www.reddit.com/r/MachineLearning/comments/36tbyb/how_we_think_about_privacy_and_finding_features/,vkhuc,1432250331,,0,1
344,2015-5-22,2015,5,22,9,36tkvp,"Measuring how ""different"" two data sets are?",https://www.reddit.com/r/MachineLearning/comments/36tkvp/measuring_how_different_two_data_sets_are/,jesussqueegee,1432254753,"I'm really, really new to Machine Learning (but not CS as a whole) so forgive me if this is a dumb question.

Say I have a bunch of N-dimensional points, that already have known classifications (two different categories). I select two dimensions and graph all of the points in the xy plane.

Is there a way to quantify how ""separate"" the two categories are, just working with those 2 dimensions? Basically, I want to find which two dimensions (out of the N) would give the clearest difference between the two categories, but I don't have a definitive way to measure that.",6,5
345,2015-5-22,2015,5,22,11,36tz2p,Using SGDRegressors in scikit-learn,https://www.reddit.com/r/MachineLearning/comments/36tz2p/using_sgdregressors_in_scikitlearn/,jbrambledc,1432262277,"I am trying to figure out how to properly use scikit-learn's SGDRegressor model. in order to fit to a dataset I need to call a function fit(X,y) where x is a numpy array of shape (n_samples,n_features), and y is a 1d numpy array of length n_samples. I am trying to figure out what y is supposed to represent.

for instance my data appears as so:

http://i.stack.imgur.com/q81gM.png

my features are years starting in 1972, and the values are a corresponding value for that year. I am trying to predict the values for years in the future such as 2008, or 2012. I am assuming that each row in my data should represent a row/sample in X where each element in that is the value for a year. in that case what would y be? I was thinking that y should just be the years, but then y would be of length n_features instead of n_samples. if y is to be of length n_samples then what could y possibly be that is of length 5(number of samples in the data shown below). I am thinking I must transform this data some way.",0,0
346,2015-5-22,2015,5,22,15,36uk79,Learning on an ML team or ramping up a DL stack?,https://www.reddit.com/r/MachineLearning/comments/36uk79/learning_on_an_ml_team_or_ramping_up_a_dl_stack/,5stringedcube,1432275795,"I expressed my desire at work to get into ML. It's a relatively big company, having an established ML stack and starting up a Deep Learning stack in Lua with torch. I was given the opportunity to choose between working on a team that is applying ML on an daily basis (wide range of problems) or work on ramping up the DL stack on a GPU cluster and applying it for the first time on image problems.

My background is minimal in both: an ML course in college + many youtube/web tutorials on DL. I am deeply interested in the topics and want to learn more. I am fresh out of college and can learn stuff fast and I am hungry.

On one side, I feel that I may get to learn better fundamentals on the ML team that I could apply later in DL side. While on the DL side, I would get more exposure to GPU optimizations and DL specific optimizations. ML team would be less risky, since the stack is already there and I can just apply and learn.

Can you help me, reddit?",5,0
347,2015-5-22,2015,5,22,15,36ukz2,Does anybody know is there a high opportunity that a ML student get a job after finishing his study in real world business ?,https://www.reddit.com/r/MachineLearning/comments/36ukz2/does_anybody_know_is_there_a_high_opportunity/,lavecoi,1432276381,,2,0
348,2015-5-22,2015,5,22,15,36ultl,[1505.05612] Are You Talking to a Machine?,https://www.reddit.com/r/MachineLearning/comments/36ultl/150505612_are_you_talking_to_a_machine/,[deleted],1432276975,,0,1
349,2015-5-22,2015,5,22,18,36uyve,Top must check Machine learning Articles from Last week. Selection based on experts Influence score and number of shares. check influencers tab for Expert list &amp; login to create feeds,https://www.reddit.com/r/MachineLearning/comments/36uyve/top_must_check_machine_learning_articles_from/,[deleted],1432287903,,0,0
350,2015-5-22,2015,5,22,19,36v3s6,David Silver's UCL Reinforcement Learning Lecture Videos,https://www.reddit.com/r/MachineLearning/comments/36v3s6/david_silvers_ucl_reinforcement_learning_lecture/,kullback-leibler,1432292044,,4,40
351,2015-5-22,2015,5,22,21,36vcyv,Sorry no whitepaper: Whats in This Picture? AI Becomes as Smart as a Toddler,https://www.reddit.com/r/MachineLearning/comments/36vcyv/sorry_no_whitepaper_whats_in_this_picture_ai/,drecklia,1432298208,,1,0
352,2015-5-22,2015,5,22,21,36vd65,MS/PhD required for working in a Data Scientist role?,https://www.reddit.com/r/MachineLearning/comments/36vd65/msphd_required_for_working_in_a_data_scientist/,pcunneen19,1432298323,"So I'll be entering my second year in college, double majoring in Computer Science and Statistics. As one might glean from my majors, data mining, particularly machine learning applications, are of particular interest to me. 

I know that this is a field that more MS and PhDs enter in industry. So excluding academia, is working in a role involved with machine learning and data mining possible for someone without a degree higher than a BS (particularly in my case with double majoring in CS and Statistics?) Or is it hard to break into those roles without at the very least an MS. 

Edit: To clarify, I don't necessarily mean right out of college. But do people every start in more entry level positions in a company and then switch into position that involves machine learning applications? I personally would not mind getting an MS, I just want to know whether that would be necessary in order to break into that field.  Thanks",26,4
353,2015-5-22,2015,5,22,23,36vmh2,Would you let a ML trained ROBOT teach you how to have sex?,https://www.reddit.com/r/MachineLearning/comments/36vmh2/would_you_let_a_ml_trained_robot_teach_you_how_to/,[deleted],1432303327,,0,0
354,2015-5-22,2015,5,22,23,36vqkz,Whats in This Picture? AI Becomes as Smart as a Toddler,https://www.reddit.com/r/MachineLearning/comments/36vqkz/whats_in_this_picture_ai_becomes_as_smart_as_a/,aadeshnpn,1432305269,,0,1
355,2015-5-22,2015,5,22,23,36vsyz,Teradeep - Deep neural network for large-scale object recognition,https://www.reddit.com/r/MachineLearning/comments/36vsyz/teradeep_deep_neural_network_for_largescale/,clbam8,1432306408,,0,0
356,2015-5-23,2015,5,23,0,36vv10,"Question: Classifying Instagram posts containing products based on comments, description and tags",https://www.reddit.com/r/MachineLearning/comments/36vv10/question_classifying_instagram_posts_containing/,NiteLite,1432307406,"I am hoping to extract information about the brand, any pricing information and any shops that match a list I already have stored in a database. Does anyone have suggestions for an approach to classifying and extracting information about products from Instagram posts, mainly based on their tags, comments and description (ignoring the actual image)? Whitepapers, blog posts or even just hints as to an algorithm would be welcome.

I realize that any solution for this will most likely be ""estimations"" at best and that it will most likely need to be verified / corrected by a human before I can actually use the information.",0,7
357,2015-5-23,2015,5,23,0,36vxha,"Awesome Tryo: a curated list of awesome resources related to Python, NLP &amp; Machine Learning",https://www.reddit.com/r/MachineLearning/comments/36vxha/awesome_tryo_a_curated_list_of_awesome_resources/,wildcodegowrong,1432308570,,0,0
358,2015-5-23,2015,5,23,2,36wa91,10 Python Machine Learning Projects on GitHub,https://www.reddit.com/r/MachineLearning/comments/36wa91/10_python_machine_learning_projects_on_github/,urinec,1432315460,,1,1
359,2015-5-23,2015,5,23,3,36wg37,[1404.3606] PCANet: A Simple Deep Learning Baseline for Image Classification?,https://www.reddit.com/r/MachineLearning/comments/36wg37/14043606_pcanet_a_simple_deep_learning_baseline/,downtownslim,1432317993,,7,4
360,2015-5-23,2015,5,23,3,36whv3,"I'm a high school student very interested in machine learning, how can I get more involved?",https://www.reddit.com/r/MachineLearning/comments/36whv3/im_a_high_school_student_very_interested_in/,dogedickguy,1432318760,"I'm in my junior year of high school, and about to get out for summer. I've always had passion for computers, but AI in particular seems to really excite me. It seems to me that math may be one of the current largest barriers to really getting into machine learning. I'm very comfortable with python and javascript, but I'm only in algebra 2 currently.  Whenever I read up on stuff about machine learning I have to skip over the math parts because it's just all Greek to me. I do have a job for summer but I don't work many hours, so I'll have lots of time to work on this kinda stuff. Thing is I really don't know how to best get more involved.   I want to play around with CNNs and RNNs and other things like that but I don't know where to start, or even if I can. Maybe I should just spend the time on khan Academy instead? What do the more experienced users of this subreddit think? Also I don't know if I really have enough knowledge to be useful but a machine learning related internship for the summer would be cool

Edit: did not expect that many responses that quickly, all very helpful and encouraging, I'm actually way more excited for this now, thanks! ",64,32
361,2015-5-23,2015,5,23,3,36wk8z,PRML Bishop Exercise 11.1. Question about solution,https://www.reddit.com/r/MachineLearning/comments/36wk8z/prml_bishop_exercise_111_question_about_solution/,okayshokay,1432319811,"I am referring to Q11.1 from Bishop's PRML book. I checked the solution to the question here: http://research.microsoft.com/en-us/um/people/cmbishop/PRML/pdf/prml-web-sol-2009-09-08.pdf 
It says \frac{1}{L} \sum_{l=1}^{L} \int f(z^{(l)}) p(z^{(l)}) dz^{(l)} = \frac{1}{L} \sum_{l=1}^{L} E[f] . I am not understanding this step. How is taking the integral of function of a single sample multiplied with its probability gives us the expected value of the function. What is confusing to me is that we are only integrating with that single sample (i.e the dz^{(l)} term).

Any pointers or explanations would be really helpful.",2,5
362,2015-5-23,2015,5,23,5,36x2ho,Optimizing Deep Neural Networks without Gradient Normalization?,https://www.reddit.com/r/MachineLearning/comments/36x2ho/optimizing_deep_neural_networks_without_gradient/,alexmlamb,1432327787,"Hello, 

In my experience, when I try to train a neural network with &gt;4 layers (either convolutional or fully connected), the initial gradients end up being either very large or very small, depending on how the weights are initialized (I initialize them by sampling from a normal distribution with the same variance for each weight).  

In my experience the best way to train such networks is to use small initial weights, like N(0.0, 0.01) and then to renormalize the gradients for each layer by dividing by the L2 norm of the gradient: 

renormalized_gradient = gradient / ||gradient||

I then use sgd + momentum to train.  

However, I see a lot of papers where the networks are very deep and the authors don't use this sort of gradient clipping.  Are the authors doing something special with the initialization that I'm unaware of?  

Best, 

Alex.  ",4,3
363,2015-5-23,2015,5,23,7,36xcvx,"GPU accelerated Deep Belief Network in Python (based on cudamat), Jupyter notebook example of training &amp; generating with MNIST data.",https://www.reddit.com/r/MachineLearning/comments/36xcvx/gpu_accelerated_deep_belief_network_in_python/,rodrigosetti,1432332741,,0,6
364,2015-5-23,2015,5,23,8,36xjbw,What is the most hard-to-understand part of machine learning -- what separates the experts from the rest of us?,https://www.reddit.com/r/MachineLearning/comments/36xjbw/what_is_the_most_hardtounderstand_part_of_machine/,iamwell,1432335858,,22,11
365,2015-5-23,2015,5,23,8,36xjea,Artificial Life Simulation - considering the simulation setup and need input,https://www.reddit.com/r/MachineLearning/comments/36xjea/artificial_life_simulation_considering_the/,BinaryAlgorithm,1432335892,"I have run interesting simulations in the past (think ant colony) but I would like thoughts on different things that I can do to increase the complexity of agent behaviors and capabilities. My setup is currently like this:

I have a 2D grid which represents the world map. Typically both ""food units"" of some sort and agents are spawned and distributed randomly on the map. Agents have a neural network, the inputs represent the data from the tiles around them, and the outputs represent movement possibilities to new tiles or actions on the same tile (whichever output has the highest value is the action taken). Over time, agents lose ""energy"" and die unless they eat food. They also require a minimum energy to reproduce, for which I generally create a clone with mutations in the neural network weight, structural links, and neuron units.

There are some things I would like to work towards:

- Understanding how an NN structure may be encoded for the purpose of genetic algorithms such that minor mutations do not always disrupt the network's ability to function (preventing fragility, increasing redundancy).
- Determine if a grid system is inherently limiting or if agents should have continuous movement (and collision detect)
- Symbolic representation is a problem; I want to move toward more complex behavior than instinctual, which means agents communicating ""verbally"" and ""in writing"" by dropping artifacts. However, I don't know how they would encode some knowledge in their output so that it may transferred to the input of another; considering that even basic ""memory storage"" is a problem
- I have only seen multiple discrete value environmental variables encoded as a set of bits in the input neurons; for example, if an agent queries a tile for its basic type (say ""W"" is water and ""D"" is dirt), if there were types A-Z then I need 26 input neurons (that would be 0.0 or 1.0 depending) to encode just that one thing which seems very inefficient. Is there a better way to encode inputs?
- The environment provides a lot of preprocessed information; for example in Polyworld the agents had color/visual neurons to detect their environment and classification happened in a higher layer; for mine I am pre-classifying tiles because I don't care about evolving that classification ability, only the higher level ""what do I do with that information?"" processing. Is this a mistake, is lower level processing essential for learning?
- Increasing simulation complexity from gathering food to specific things like finding items and ""crafting"" new items from it. I'd like some ideas on how I can make the simulation more ""challenging"" without the agents getting stuck at a basic level where they cannot move toward the goal. I was thinking of having multiple goals that are unlocked as part of the fitness calculation as previous ones are sustainably met; example: they are consistently learning to feed themselves, now they need to deal with another environmental factor; maybe they even get a new sensory node to deal with it.

- For the purpose of persistent agents: I feel like my choice of NN units and structure doesn't fit with what I am trying to do. Sigmoid units are good for what I originally was doing which are things like market prediction, or predictive models which is about relating data sets but the answer is known in training. I have tried LSTM cells with limited success. The agents need to do more than I/O processing; for example, how can it ""choose to remember"" a location on the map, and how can that be ""stored for later"" in a neural net? I've learned that sparsely connected network clusters can be better in some ways than fully connected networks (and computationally cheaper). I've also recently discovered spiking NN which may have capabilities that I didn't anticipate. It is incredibly important to choose a structure that fits the function you are going for and despite all my research into it, I still do not know what I should do. For example, the simulation runs in steps (where it is updated, agent actions are taken, etc), agent output is parsed for a ""decision"" and all neural networks are updated (input update, firing neurons transfer their values, output is updated). This presents a few challenges:
- In contrast to many NN applications, I cannot evaluate ""fitness"" of any given output/action; this is because fitness is based on the agent's lifetime: what did it do, what did it accomplish (if specific goals are being rewarded)? At this point the network ""dies"" because the agent ""dies"". Although a better performing network will have a higher chance of replication, it doesn't help me ""train"" the NN at all, it only performs natural selection on random NN mutations (thus it doesn't systematically ""evolve"" the NN structure or weights).
- I am unable to encode symbolic memory into traditional NN units. This could be many things such as: recalling the set of inputs at some time T, storing and recalling the value of some neural unit when another condition exists, and linking action to consequence more than a single step in the past. It is possible for recurrent connections to store something small for a time, and LSTM cells to store a value for an indefinite period, but to store more complex data (related values vs. a single value) I have not found a good way to link neurons to ""memory blocks"" or any such thing and could use recommendation on that.
- Traditional NN neurons that store a value and pass it on with weighting seem very ""brittle"" to any changes when attempting to use genetic algorithms to restructure them slightly, as a small change causes a cascade effect where all downstream units fail.
- In examining spiking NN, I'm not sure how the simulation time step should process in the NN since time is a factor. I'm also learning that a ""tonal"" signal may be needed to ""keep-alive"" the NN, and the perturbation of this signal by input is what causes ""activity"" and ultimately output.
- It's not clear how an NN in the simulation can learn or evolve its weights while alive (ie, within one life cycle). Currently changes are only happening when spawning new agents.


What kinds of designs for both the simulation and the agents (body nodes + mind structure/NN) can improve the system's range of capabilities? What sort of data and processing structures might be appropriate here?
",2,1
366,2015-5-23,2015,5,23,9,36xs8o,Toward using rbm to design better rbm learning algorithms - What are the parameters to tune or code to evolve?,https://www.reddit.com/r/MachineLearning/comments/36xs8o/toward_using_rbm_to_design_better_rbm_learning/,BenRayfield,1432340592,"EDIT: There is a lack of working together on exploring large spaces of possible parameters and variations of algorithms. I dont see any threads trying to explore this. Everyone wants to pick a few parameters and explore them on their own, so nowhere near the best solution is found.

I'm looking for a set of good learning algorithms, some better at speed, others better at using less memory, some that work better in sparse networks, and other specializations.

The set of parameters to tune and possible code to evolve is too big to be searched manually, so after choosing a good set of these I plan to, as opensource GPL, hook them into another rbm to explore these on a few small datasets, together as a bigdata problem where the data is generated by the different algorithms. The data is the weights of new rbms during training and predicting, some encoding of the algorithms, and how they perform on various empirical and generated datasets.
 
To make it easier to vote and debate on each separately, I'm posting each as a separate reply prefixed by PARAM. Please post any other params that may lead to better rbm algorithms:",18,0
367,2015-5-23,2015,5,23,10,36xwsf,Partially Derivative Episode 23: Political Science Rulez,https://www.reddit.com/r/MachineLearning/comments/36xwsf/partially_derivative_episode_23_political_science/,chrisalbon,1432343236,,0,0
368,2015-5-23,2015,5,23,10,36y09f,Weight Uncertainty in Neural Networks,https://www.reddit.com/r/MachineLearning/comments/36y09f/weight_uncertainty_in_neural_networks/,madisonmay,1432345156,,0,4
369,2015-5-23,2015,5,23,10,36y0t9,"On Sensible Baselines, Diminishing Returns, and the Unreasonable Effectiveness of Ensembles",https://www.reddit.com/r/MachineLearning/comments/36y0t9/on_sensible_baselines_diminishing_returns_and_the/,[deleted],1432345486,,0,0
370,2015-5-23,2015,5,23,10,36y0yf,Who employs people with Machine Learning education?,https://www.reddit.com/r/MachineLearning/comments/36y0yf/who_employs_people_with_machine_learning_education/,faore,1432345566,"I'm considering a course called ""Statistical Data Mining and Machine Learning"" but I just wanted to know what non-academic areas exist (and if one course means anything in that context)",5,0
371,2015-5-23,2015,5,23,10,36y1db,Open Robotics/AI Workshop/Lab/Meetup happening tomorrow in San Francisco,https://www.reddit.com/r/MachineLearning/comments/36y1db/open_roboticsai_workshoplabmeetup_happening/,ctwiz,1432345832,,3,1
372,2015-5-23,2015,5,23,11,36y5k5,"Why does the machine learning community usually use scripting languages (Matlab, Python) instead of compiled code which is much faster?",https://www.reddit.com/r/MachineLearning/comments/36y5k5/why_does_the_machine_learning_community_usually/,iamwell,1432348346,,39,31
373,2015-5-23,2015,5,23,17,36yx00,2 papers in Deep Sensorimotor Learning tobepresented 2015-05-27 and 28 @ICRA,https://www.reddit.com/r/MachineLearning/comments/36yx00/2_papers_in_deep_sensorimotor_learning/,Ciphertext008,1432368313,,3,4
374,2015-5-23,2015,5,23,17,36yxvz,Azure Machine Learning - Simplified Predictive Analytics,https://www.reddit.com/r/MachineLearning/comments/36yxvz/azure_machine_learning_simplified_predictive/,[deleted],1432369174,,0,0
375,2015-5-23,2015,5,23,17,36yyan,"Scipy, Numpy: Audio classifier,Voice/Speech Activity Detection",https://www.reddit.com/r/MachineLearning/comments/36yyan/scipy_numpy_audio_classifiervoicespeech_activity/,linuxexperi,1432369593,"I am writting a program to automatically classify recorded audio phone calls files (wav files) which contain atleast some Human Voice or not (only DTMF, Dialtones, ringtones, noise).

My first approach was implementing simple VAD (voice activity detector) using ZCR (zero crossing rate) &amp; calculating Energy, but both of these paramters confuse DTMF, Dialtones with Voice. This techquie failed so I implemented a trivial method to calculate variance of FFT inbetween 200Hz and 300Hz. My numpy code is as follows

	wavefft = np.abs(fft(frame))
	n = len(frame)
	fx = np.arange(0,fs,float(fs)/float(n))
	stx = np.where(fx&gt;=200)
	stx = stx[0][0]
	endx = np.where(fx&gt;=300)
	endx = endx[0][0]
	return np.sqrt(np.var(wavefft[stx:endx]))/1000

This resulted in 60% accuracy. 

Next, I tried implementing a machine learning based approach using SVM (Support Vector Machine) and MFCC (Mel-frequency cepstral coefficients). The results were totally incorrect, almost all samples were incorrectly marked. How should one train a SVM with MFCC feature vectors? My rough code using scikit-learn is as follows

	[samplerate, sample] = wavfile.read ('profiles/noise.wav')
	noiseProfile = MFCC(samplerate, sample)
	[samplerate, sample] = wavfile.read ('profiles/ring.wav')
	ringProfile =  MFCC(samplerate, sample)
	[samplerate, sample] = wavfile.read ('profiles/voice.wav')
	voiceProfile = MFCC(samplerate, sample)
	
	machineData = []
	for noise in noiseProfile:
		machineData.append(noise)
		
	for voice in voiceProfile:
		machineData.append(voice)
		
	dataLabel = []
	for i in range(0, len(noiseProfile)):
		dataLabel.append (0)
	for i in range(0, len(voiceProfile)):
		dataLabel.append (1)

	clf = svm.SVC()
	clf.fit(machineData, dataLabel)

I want to know what alternative approach I could implement?",5,2
376,2015-5-23,2015,5,23,21,36zfgd,Automatic Differentiation in Deep/Machine Learning Platforms,https://www.reddit.com/r/MachineLearning/comments/36zfgd/automatic_differentiation_in_deepmachine_learning/,T_hank,1432385402,"Theano's greatest feature probably is automatic differentiation which takes one gotcha, whether i calculate/ implemented the gradients correctly, out of the picture. However, the compiled nature makes it a little tedious to debug, especially when you are tired and burning the midnight oil.

So, has automatic differentiation been included in any other platform, more of a matlab/torch like JIT rather than compiled. 

That would be the best of both worlds.",20,7
377,2015-5-23,2015,5,23,22,36zi75,I used Andrej Karpathy's char-rnn to compose Irish Folk songs. Here are the results!,https://www.reddit.com/r/MachineLearning/comments/36zi75/i_used_andrej_karpathys_charrnn_to_compose_irish/,jfsantos,1432387334,,26,72
378,2015-5-23,2015,5,23,22,36zilb,Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering,https://www.reddit.com/r/MachineLearning/comments/36zilb/are_you_talking_to_a_machine_dataset_and_methods/,iori42,1432387616,,1,7
379,2015-5-24,2015,5,24,1,3703uy,Data Mining Techniques,https://www.reddit.com/r/MachineLearning/comments/3703uy/data_mining_techniques/,kunal4097,1432399819,"Hey . I want to study about data mining techniques but not in detail. I have a project on applying data mining techniques on image processing. So, I just want to get a clear idea about all the data mining techniques first. What would you suggest?",2,0
380,2015-5-24,2015,5,24,2,370as0,The unreasonable effectiveness of Character-level Language Models (and why RNNs are still cool),https://www.reddit.com/r/MachineLearning/comments/370as0/the_unreasonable_effectiveness_of_characterlevel/,vkhuc,1432403412,,12,60
381,2015-5-24,2015,5,24,3,370gks,PCANet: A Simple Deep Learning Baseline for Image Classification? (x-post r/CompressiveSensing ),https://www.reddit.com/r/MachineLearning/comments/370gks/pcanet_a_simple_deep_learning_baseline_for_image/,compsens,1432406314,,3,0
382,2015-5-24,2015,5,24,4,370jh3,[Question] Separate individual speakers in conversation (audio),https://www.reddit.com/r/MachineLearning/comments/370jh3/question_separate_individual_speakers_in/,hannson,1432407740,"IIRC there was an algorithm that separates an audio input of two (or more?) speakers into different channels. 

As in, if you have two people speaking at the same time in an audio file you get two audio files each with what a single person said.

Does anyone recall which algorithm I'm talking about?",15,1
383,2015-5-24,2015,5,24,4,370q7y,"Has anybody worked with these common IR test collections? (CRAN, CISI, MED)",https://www.reddit.com/r/MachineLearning/comments/370q7y/has_anybody_worked_with_these_common_ir_test/,[deleted],1432411042,"I have been working with [these](http://web.eecs.utk.edu/research/lsi/corpa.html) common IR test collections. Basically, I wrote a clustering algorithm that shuffles the documents from every collection and tries to reassemble the collections afterwards (the purpose of this is to see how well the algorithm performs). 

I decided to print the distances between every element of a set composed of the first three documents in every collection (i.e. a set composed of the first three documents of CRAN, the first three documents of CISI, and the first three documents of MED). It turns out that the collections do not seem to be very separated. For example, some of the documents in CRAN are closer to documents in CISI than they are to other documents in CRAN. All of the first three documents in MED are closer to documents in other collections than to one another. 

I want to know if somebody else has worked with these collections and got similar results or could give me some insight into how to use these collections. It could also be that there are bugs in my program and it is not reading the CCS sparse matrix format right. However, I wrote three small and very separated test collections in the same format as the given collections and the program runs great. ",0,0
384,2015-5-24,2015,5,24,5,370td5,"Given multiple time series (each in their own file) to train on, is there anyway to classify a sequence according to which time series it most resembles to ?",https://www.reddit.com/r/MachineLearning/comments/370td5/given_multiple_time_series_each_in_their_own_file/,[deleted],1432412598,How does one measure likelihood of  that a sequence is part of a given time series ? ,0,1
385,2015-5-24,2015,5,24,6,3710de,Question about LSTMs,https://www.reddit.com/r/MachineLearning/comments/3710de/question_about_lstms/,my_sane_persona,1432416278,"Hi all,

I'm fairly novice to the ML field, having only recently taken some introductory ML courses, and I have a question about LSTMs that I thought maybe you could help me with.

My understanding is that LSTMs are RNNs with memory cells. The way a memory cell works is like this:

* we have an **input gate**, which determines whether the current input to the cell will be ""written to memory""
* we have an **output gate**, which determines whether the current state of the cell will be ""read out of memory""
* we have a **forget gate** (or alternatively, a keep gate) which determines whether to erase the memory of the cell

These memory cells, like any other units, can have the weights of their gates learned through backpropagation.

The part I am having trouble with is this: where are these memory cells placed in the system? Do they replace your typical hidden unit, and a hidden layer is just a row of LSTM cells, or do they work in conjunction with typical hidden units - in which case, what does the architecture look like? Does each typical hidden unit have an LSTM cell attached to it, or something similar?

I hope this makes sense, if not perhaps I can clarify further.

Thank you!",2,0
386,2015-5-24,2015,5,24,7,3718ku,Why are Eight Bits Enough for Deep Neural Networks?,https://www.reddit.com/r/MachineLearning/comments/3718ku/why_are_eight_bits_enough_for_deep_neural_networks/,petewarden,1432420592,,2,22
387,2015-5-24,2015,5,24,7,371a8j,Document clustering,https://www.reddit.com/r/MachineLearning/comments/371a8j/document_clustering/,hahze,1432421449,"Hi,

I'm trying to cluster documents based on their content. I've tried a couple of different methods and I'm just not sure about how they're shaping up. None of them really are satisfactory.

The documents are questions from a reddit AMA.

I've tried doing DBSCAN on TFIDF cluster of documents with large or small n-grams. It works OK, but they're leaving out a lot of documents that could be still clustered.

Doc2Vec was another idea and it was OK but I get connections between sentences and words where they don't quite seem to cluster like I thought they did. Maybe an AMA, even a large AMA, doesn't have enough questions? Should I try to train on a lot of AMAs or other text? I suppose NNs need a lot of data.

Other ideas were semantic hash or Random Forest Classifiers (or gradient boosting tree). 

Any ideas?",7,0
388,2015-5-24,2015,5,24,13,3729vj,How would one use a lstm to generate similar nl sentences? What input and ideal-output would I train it with?,https://www.reddit.com/r/MachineLearning/comments/3729vj/how_would_one_use_a_lstm_to_generate_similar_nl/,[deleted],1432442978,"how would I go about nl generation with sentences similar to ones I have trained a lstm with?

For example, how could I do per this input, create a sentence that looks like this. I assume I'd train it as a sequence with different words at time-steps, but how would I conceptually go about starting that?

Thank you!",0,0
389,2015-5-24,2015,5,24,18,372tyj,Naive Bayes for document clustering,https://www.reddit.com/r/MachineLearning/comments/372tyj/naive_bayes_for_document_clustering/,komitori,1432461332,I've read some studies that it is possible to cluster documents using Naive bayes but how does it fare with soft or hard clustering algorithms like LDA or k means?,3,0
390,2015-5-24,2015,5,24,21,3731ns,How does word2vec allow me to train an rnn the semantic value of a word in vector form when it gives me the same length vector if I give it 5 or 100005 words?,https://www.reddit.com/r/MachineLearning/comments/3731ns/how_does_word2vec_allow_me_to_train_an_rnn_the/,[deleted],1432469057,,5,1
391,2015-5-24,2015,5,24,22,3738d5,Trend (regime) change detection with t-test,https://www.reddit.com/r/MachineLearning/comments/3738d5/trend_regime_change_detection_with_ttest/,mhfirooz,1432474185,,4,16
392,2015-5-25,2015,5,25,2,37409m,Sum of functions using RNN,https://www.reddit.com/r/MachineLearning/comments/37409m/sum_of_functions_using_rnn/,pedromnasc,1432489763,"Hi! 
Is anyone aware of some paper or work related to a RNN trying to sum time series? Given N time series the RNN try to infer the sum of these time series.
I tried to implement this using LSTM and it doesn't seem to be an easy task. Am i making some mistake?

Edit: Yes, i am :) The problem is just with deep LSTM. They are harder to train even in an easy task.

Thanks in Advance. ",8,0
393,2015-5-25,2015,5,25,4,374e8h,UC Berkley labs develops a deep learning algorithm that enables robot mastery of skills via trial &amp; error.,https://www.reddit.com/r/MachineLearning/comments/374e8h/uc_berkley_labs_develops_a_deep_learning/,ovidius007,1432496735,"Article: http://newscenter.berkeley.edu/2015/05/21/deep-learning-robot-masters-skills-via-trial-and-error/

Video: https://youtu.be/JeVppkoloXs

Edit:

* Here's the comment thread on /u/futurology:
     * https://www.reddit.com/r/Futurology/comments/375o39/brett_the_robot_learns_to_put_things_together_on/
* /u/buck-nasty posted this YouTube video from one of the robot's creators on the /u/futurology thread: 
     * https://youtu.be/EtMyH_--vnU
* /u/gwern provided some useful links in the /r/singularity thread:
     * https://www.reddit.com/r/singularity/comments/375oio/brett_the_robot_learns_to_put_things_together_on/crkjhqt
",9,9
394,2015-5-25,2015,5,25,4,374eco,Building a self-learning crawler to screen most referenced durations in youtube videos,https://www.reddit.com/r/MachineLearning/comments/374eco/building_a_selflearning_crawler_to_screen_most/,Noweightsatthegym,1432496798,"We want to build a crawler or algorithm that takes the most referenced durations in YouTube videos. We want that algorithm to eventually be able to identify the reoccurring durations so that it can eventually splice up  Youtube videos according to those most common durations. How would I go about building that? 

Keep in mind. I'm new to machine learning. So any references to where to start in building a project like this would be greatly appreciated. I apologize in advance if this explanation is too vague. ",11,1
395,2015-5-25,2015,5,25,6,374roc,"Stanford's Deep Learning Javascript library is learning to draw Reddit Mascot (Snoo), just go to link and wait for Image Regresssion procedure",https://www.reddit.com/r/MachineLearning/comments/374roc/stanfords_deep_learning_javascript_library_is/,cagbal,1432503555,,19,63
396,2015-5-25,2015,5,25,6,374scs,20 Articles on deep learning &amp; neural networks Andrew Y NG shared in last 6 weeks!,https://www.reddit.com/r/MachineLearning/comments/374scs/20_articles_on_deep_learning_neural_networks/,[deleted],1432503908,,0,1
397,2015-5-25,2015,5,25,6,374tyv,20 articles on Deep learning shared by Andrew Y NG in 4 weeks. He literally brought back Neural Network from dead!,https://www.reddit.com/r/MachineLearning/comments/374tyv/20_articles_on_deep_learning_shared_by_andrew_y/,nlp911,1432504720,,0,0
398,2015-5-25,2015,5,25,15,376b28,Comparison of official test scores of current Image Captioning systems,https://www.reddit.com/r/MachineLearning/comments/376b28/comparison_of_official_test_scores_of_current/,flukeskywalker,1432534145,,5,2
399,2015-5-25,2015,5,25,16,376gpy,Data mining algorithms in plain English,https://www.reddit.com/r/MachineLearning/comments/376gpy/data_mining_algorithms_in_plain_english/,bckygldstn,1432538490,,0,4
400,2015-5-25,2015,5,25,17,376msi,Feature Extraction from arbitrary Time-Series help?,https://www.reddit.com/r/MachineLearning/comments/376msi/feature_extraction_from_arbitrary_timeseries_help/,ddofer,1432543851,"I work in a (non-standard) domain, in which I can represent sequences of variable (or fixed length sub-sequences) length as (different) time-series.
I don't have any experience working with time-series, or extracting them into fixed-length feature vectors, and was hoping for some intuitions. 

I know that an option is using the Fourier Transform or wavelets, but I don't know which type of transform is best, or how to interprete it. (The FFT/DFT gives the ""coefficients"" - and I can choose the top coefficients to keep, right?)
 Is this a good way to represent the time-series in a way that's more ""informative"" than simply taking the time-series' Average, Min, Max ? 

Some Python code examples would be great (I'm assuming SciPy would be best or PyWavelets, but I would want to see some examples of feature extraction code)... 
[PS - Just to reiterate, these time series are not speech, audio or stock/market data].

(The end goal is to apply this to a supervised machine learning classification task, to improve the represented features).
",17,0
401,2015-5-25,2015,5,25,19,376ufe,Help with classifying fMRI sequences using neural networks,https://www.reddit.com/r/MachineLearning/comments/376ufe/help_with_classifying_fmri_sequences_using_neural/,ppries,1432549986,"Hi machine learning experts    

I'm working with a motor-task fMRI dataset where I want to classify which action they are doing from a sequence of 3D images. I've tried different neural network architectures, but I'm having trouble getting them to learn anything.

Here's what I've done so far - I'll try to keep it brief.

Originally, the dataset is split up in two experiments of 284 frames of 91 x 109 x 91 voxels per subject. In each experiment, there is 170 frames where they are performing an action (moving each hand, each foot and the tongue twice continually for 17 frames). I've extracted each action from the data, delayed according to a cannonical hemodynamic response function peak (~6 seconds). That is, when I know they are starting an action, I extract 17 frames 6 seconds later in the experiment and label them accordingly. Additionally, the data is normalized to values between 0 and 1.

OK, on to the stuff I've tried. I've tried both CNNs and RNNs with LSTM units. For the CNNs, I've tried both flattening the image, treating the sequences as a dimension in the image (that is, the input is #timesteps x (h x w x d)) and collapsing the depth of the 3D images by taking the mean, treating the sequences as channels of the image. The architecture has been different number of convolutional -&gt; max pooling layers, with 32 filters of size 5 or 15 (arbitrarily chosen, I'm not sure how to pick these hyperparameters) followed by one or several fully connected layers of sizes 128-512 and a softmax output layer. The results has been around random (~20 %) in all cases. 

The RNN has been trained on the flattened images (either downsampled or collapsed, to reduce input dimensionality), where I only take the last output of the recurrent layer (in a sense, I want the network to process the entire sequence before attempting to predict what action is being performed). The architecture has been one or more (mostly one) recurrent layers with 128-512 LSTM units followed by zero or more fully connected layers with 128-512 units, with a softmax output layer. Again, only random accuracy was achieved.

In both cases I have been able to get the network to overfit on very small subsets of the data set (8-16 data points), but when training on the full dataset, the accuracy does not change significantly. However, the mean training loss (categorical cross entropy) does seem to be declining, albeit on the 5th or 6th decimal point.

I am completely new to the world of fMRI, and not an expert on deep learning at all, so it might be that I have missed something completely. I have tried finding similar work, but have not been successful. If anyone have any pointers, or know of someone who has done something similar with some success, the help would be appreciated. Let me know if something I have written is not clear.

(For the records - I am using Theano with Lasagne to implement these networks)",14,9
402,2015-5-25,2015,5,25,20,376wza,For those who are getting into Machine Learning,https://www.reddit.com/r/MachineLearning/comments/376wza/for_those_who_are_getting_into_machine_learning/,_blub,1432552109,"All the peeps who try to give help in subreddit will always recommend the the basic shit. Cliche coursera courses with Andrew Ng (geeezus that guy is a king), and ""do this"" style of guidance. So here is my advice as an individual who has already taken courses with artificial intelligence and machine learning.

So as an unorthodox approach, I will describe things in terms of microsoft excel.

**Math** - Machine learning is the most practical harmony of statistics, calculus, linear algebra and discrete mathematics. You have to think multidimensionally all the time, and not just 3d. You need to be very comfortable with thinking in nearly infinitely many dimensions.

Surprisingly (actually very surprisingly) it's actually quite easy to think multi-dimensionally (*cue the linear algebra*) where each column of an excel spreadsheet is a particular dimension. So if we were to assume small images of handwriting that are 30 pixels by 30 pixels then we can flatten this into 900 columns of a spreadsheet (aka a 900 dimensional vector).


Considering that each row is a character image, we then load thousands upon thousands of rows of images that we already know as the particular characters (courtesy of [the MNIST](http://yann.lecun.com/exdb/mnist/) dataset). 

Divide the dataset into the 52 (uppercase and lowercase english characters) that we have known since kindergarten. And for each ""classification"" (there are 52 classes) of letters that we already know, take a statistical average (*cue the statistics*) for each 900 columns per each few thousand images that are an ""a"", ""b"", ""c"", ... ""Z"". 

We now have 52 averages in 900 dimensions. You now just figured out the near perfect averages of handwritten characters. 

What if we add in a new letter? Well, we figure out which statistical mean (in 900 dimensions) our new letter is close to. If you think of Euclidian distance, then its actually quite easy ([Pythagoreans theorem in multiple dimensions](http://en.wikipedia.org/wiki/Norm_%28mathematics%29))

And now we can do character recognition. Fuck yeah.


**Programming** - Excel Sucks, so here is where Python, R and Haskell come into play. The machine learning model I described above is similar to the K-Means and Logistic Regression models, but if you want to get into the more brilliant models of ML, then learn yourself some set-comprehensions to chef up data-representations and multi dimensional sets in to the way you are the most comfortable with. [Discrete mathematics](http://www.people.vcu.edu/~rhammack/BookOfProof/), Its easier than calculus.

**The Bigger Picture**

Honestly, have fun with this. Allow yourself to be very, very confused. The best advise is to be patient and allow your mind some time to all of a sudden *click*, This moment you realize something, you will feel as though you just got projected to fucking pluto. For those who are Mathematicians or Computational Scientists, this is a frequent sensation that often occurs right before sleep and results in frantic ""Holy shit, I know how the world works"" chants until 8:00am the next morning. The following models will make sure you have multiple of these ""HOLY SHIT"" moments that induce said insomnia.

- Gaussian Mixed Models
- Support Vector Machines
- Fast Fourier Transforms
- Convolutional Neural Networks
- Hidden Markov Models
- Statistics (Honestly simple confidence intevals and Regression, but Multi-Dimensionally)",39,35
403,2015-5-26,2015,5,26,0,377lh5,6 Tricks I Learned From The OTTO Kaggle Challenge,https://www.reddit.com/r/MachineLearning/comments/377lh5/6_tricks_i_learned_from_the_otto_kaggle_challenge/,john_philip,1432566960,,4,48
404,2015-5-26,2015,5,26,1,377z6b,Optimization and functional analysis,https://www.reddit.com/r/MachineLearning/comments/377z6b/optimization_and_functional_analysis/,bagelorder,1432573089,"I know read quite a few times that optimization and functional analysis are math topics with relevance (or even important?) topics in ML.
What do you mean by optimization? My university has two directions here: numerical optimization (non-linear) and linear optimization/combinatory optimization. Which of these is meant here?

And what do you meant by functional analysis? Which parts of it are useful?",9,1
405,2015-5-26,2015,5,26,3,3789d0,New 'Deep Learning' Robot Learns Skills Through Trial and Error,https://www.reddit.com/r/MachineLearning/comments/3789d0/new_deep_learning_robot_learns_skills_through/,[deleted],1432577479,,0,0
406,2015-5-26,2015,5,26,3,378but,Are there any good resources for learning about neural networks?,https://www.reddit.com/r/MachineLearning/comments/378but/are_there_any_good_resources_for_learning_about/,Kurren123,1432578509,"I'm looking for resources on neural nets which:

* Don't move too fast. The more ""for dummies"" the better.
* Introduce you to the basics (perceptron, sigmoid neurons etc), then take you through all the flavours like convolutional and recurrent neural nets.
* Are easy to read, and keep things interesting
* Preferably have code samples

I did maths and comp sci at university although I'm a little rusty. So resources with maths in are fine as long as they don't drop you in the deep end.

So far I've worked through [this online book](http://neuralnetworksanddeeplearning.com/) which is great for beginners and I recommend anyone reading who knows some math to look at.

Thanks in advance for any help!",12,20
407,2015-5-26,2015,5,26,3,378fju,"What are your opinions on the paper ""On event-based optical flow detection""?",https://www.reddit.com/r/MachineLearning/comments/378fju/what_are_your_opinions_on_the_paper_on_eventbased/,andrewbarto28,1432580025,"Is it original? Is it really useful for detecting objects? Are there better alternatives?

Link: http://journal.frontiersin.org/article/10.3389/fnins.2015.00137/abstract",0,1
408,2015-5-26,2015,5,26,4,378o6a,Master Thesis - Machine Learning - Survey (About Games),https://www.reddit.com/r/MachineLearning/comments/378o6a/master_thesis_machine_learning_survey_about_games/,sheepsy90,1432583685,"Hello,

I am currently writing my master thesis in computer science with the Topic ""Ordinal Data in Games"". I already did a small study and want to refine my results now.

I therefore need a lot more people who can answer my small survey. You decide yourself how long do you want to participate. The questions are not depended on each other.

Just give it a try and read the one page description on http://heleska.de:8000/ and continue afterwards directly to the study.

I would be glad if you help me.

Greetings Sheepy
",3,0
409,2015-5-26,2015,5,26,5,378r8j,New Keyword Extractor: smarter and more flexible extractor,https://www.reddit.com/r/MachineLearning/comments/378r8j/new_keyword_extractor_smarter_and_more_flexible/,wildcodegowrong,1432584994,,0,0
410,2015-5-26,2015,5,26,5,378ssh,Do you have a good way of debugging Torch7 code?,https://www.reddit.com/r/MachineLearning/comments/378ssh/do_you_have_a_good_way_of_debugging_torch7_code/,ticcky,1432585670,"In Python I use the magical ""import pdb; pdb.set_trace()"" for debugging. However, in Torch7 I couldn't find anything as useful. Therefore, I'd like to ask if you have some tricks how do you go about debugging Torch7 code? =)

Thanks!",6,8
411,2015-5-26,2015,5,26,7,379cmt,I Let IBMs Robot Chef Tell Me What to Cook for a Week,https://www.reddit.com/r/MachineLearning/comments/379cmt/i_let_ibms_robot_chef_tell_me_what_to_cook_for_a/,bs2sb,1432594540,,10,57
412,2015-5-26,2015,5,26,8,379kn4, Machine Learning: Algorithms Terminology,https://www.reddit.com/r/MachineLearning/comments/379kn4/machine_learning_algorithms_terminology/,kingvicc,1432598320,"I'm a bit confused on terminology, so I would appreciate some help.

I've seen many sites and resources refer to Linear Regression, Logistic Regression, Neural Networks, SVMs, etc. as machine learning algorithms.

However I've also seen the same sources refer to Ordinary Least Squares, Elastic Net, Batch Gradient Descent, etc. referred to as machine learning algorithms.

So my question is, If I had to group these into two groups, what would a better less ambiguous term be?

AlgorithmTypes &amp; OptimizationTypes? 

(P.S. I know what they all do, I'm just trying to find a simple way to denote the difference between categories for a project I'm building).
",4,0
413,2015-5-26,2015,5,26,9,379rbk,How do ML researchers approach collecting training data from humans?,https://www.reddit.com/r/MachineLearning/comments/379rbk/how_do_ml_researchers_approach_collecting/,mike_bolt,1432601666,"I'm considering creating a website geared towards allowing researchers to gather training data from human users for specialized machine learning projects. I'm trying to do some background research here to figure out if there is a need for this website. I would appreciate it if you could answer some of these questions.

How do researchers or research teams typically approach this problem? Is it common to use a site like mechanical turk? Does mechanical turk meet the needs of most researchers? Is it too expensive? Are there any alternative sites like the one I described? Would you like to see a site like this? Would you use it?",6,1
414,2015-5-26,2015,5,26,11,37a5wx,Classifying Golf Players with K-Means and Hierarchical Clustering,https://www.reddit.com/r/MachineLearning/comments/37a5wx/classifying_golf_players_with_kmeans_and/,bigdatabucket,1432608820,,0,3
415,2015-5-26,2015,5,26,12,37aal6,Setting up a GPU accelerated deep learning environment on Elementary OS freya with CUDA,https://www.reddit.com/r/MachineLearning/comments/37aal6/setting_up_a_gpu_accelerated_deep_learning/,wolfchimneyrock,1432611233,,2,6
416,2015-5-26,2015,5,26,12,37acsk,Linear Algebra for Beginners Course,https://www.reddit.com/r/MachineLearning/comments/37acsk/linear_algebra_for_beginners_course/,rickmister24,1432612462,,0,1
417,2015-5-26,2015,5,26,13,37ahe9,Do tree models benefit from feature selection?,https://www.reddit.com/r/MachineLearning/comments/37ahe9/do_tree_models_benefit_from_feature_selection/,Nixonite,1432614959,"Hello everyone,

I'm learning about ML and I've been thinking about this a lot, but are tree models sensitive to feature selection? 

I mean, do they improve by cutting out features of little importance/variance? By benefit, I mean in model performance not computational time.",4,0
418,2015-5-26,2015,5,26,15,37ardy,Softmax vs sigmoid for output of a neural network,https://www.reddit.com/r/MachineLearning/comments/37ardy/softmax_vs_sigmoid_for_output_of_a_neural_network/,XalosXandrez,1432621231,"It is common practice to use a softmax function for the output of a neural network. Doing this gives us a probability distribution over the classes.

What if, instead, we use a sigmoid activation on each output neuron? We get one-vs-all probabilities for each class. If this is conceptually sound, has someone does this before? Does this affect training in any way? ",11,3
419,2015-5-26,2015,5,26,17,37b0cy,Using k-means to downsample data,https://www.reddit.com/r/MachineLearning/comments/37b0cy/using_kmeans_to_downsample_data/,__ImI__,1432628541,"I have used 10-fold cross-validation for rbf SVM to obtain the parameters that best suited for my dataset. Now I would like to finally train a model for my data by selecting a subset of my dataset. This is a problem because training on the complete dataset would just give me a overfitted model. If I randomly sample some data it would give me underfitting.

I am wondering what would be the best way to select data.

To answer this question, I decided to take my classes and perform k-means to create cluster centers of about 10% of the data from each class. Then use these cluster centers to train the final model and test on my actual data.

I tested on selecting 10%, 15% ... 30% cluster centers and as expected I see an increase in classification rate.

I am curious if this is meaningful to do?",2,0
420,2015-5-26,2015,5,26,17,37b0zd,[Question]Topic Modelling with Meta Data,https://www.reddit.com/r/MachineLearning/comments/37b0zd/questiontopic_modelling_with_meta_data/,FoolofGod,1432629081,"Hi /r/MachineLearning, 

I'm fairly new to machine learning and I recently discovered topic modelling.

My data set is 10's of thousands of customer support tickets, with the back forth responses between the customers and company employees.

So that's my corpus. I have decided that the best thing to be considered a ""document"" in this corpus is the name of the support ticket (oftentimes it includes import key words) and the many notes that have been left on the ticket.

My process right now is the following pipeline:

* Step 1. Compile title and notes into one document.

* Step 2. Clean the living daylights out of each document.

* Step 3. Using Gensim create a dictionary of words from the corpus.

* Step 4. Transform the corpus using Term Frequency * Inverse Document Frequency (Tf-Idf).

* Step 5. Train a Latent Semantic Analysis model.

* Step 6. Given a new document or set of documents use the Tf-Idf and LSA models to categorize the new document in some meaningful way.

I have gotten OK results with this pipeline. In the course of Step 2 (the cleaning part), I realized that I could pull some meaningful information from the notes that would allow me to go to some other, more rich, data sources to get further details. For example, if I this was for a retailer, perhaps I would be able to scrape a PLU and could go get some product information.

So my question is, how should I handle this additional data?

My first, and most simple, instinct would be: scrub this additional data (it might contain some other extraneous information), and then just plop it on to the end of the document for which it's relevant. But this seems like a regression...I'm going from some pretty specific and relevant data, to then having it transformed in step 4, and possibly rendered ""extraneous"" by the model in step 5. But I know for certain, some of the information is QUITE relevant and important, and I don't want to risk having it swallowed up by my models.

I am wondering if anyone could lend a thought to the best way to deal with rich metadata while topic modeling?

Any help would be greatly appreciated.

A final note is that I cannot get this metadata for every document...so I think there is still a lot of benefit to be had by actually topic modelling vs. **just** using the metadata.

**TECH NOTES:**

I am using python 2.7.

I am using pattern 2.6 for my text parsing/cleaning.

I am using gensim for my topic modelling.

I am toying around with scikit-learn for clustering.

And I'm using pandas/numpy/matplotlib for exploration with my results.
",0,0
421,2015-5-26,2015,5,26,17,37b1bl,word2vec has been patented. What does it change for NLP practitioners?,https://www.reddit.com/r/MachineLearning/comments/37b1bl/word2vec_has_been_patented_what_does_it_change/,shmel39,1432629393,,77,58
422,2015-5-26,2015,5,26,18,37b5hs,Use of SPM in Adaptive Deconvolutional Networks,https://www.reddit.com/r/MachineLearning/comments/37b5hs/use_of_spm_in_adaptive_deconvolutional_networks/,segaspacemonkey,1432633006,"I am currently reading this paper: http://www.matthewzeiler.com/pubs/iccv2011/iccv2011.pdf. I can't understand how exactly SPM is used with sets of latent feature maps of every image?

Is it correct to assume that for every max activation m, we take all the reconstructions of first layer feature maps \hat{z^i,1} and feed them into SPM. For every pyramid level SPM subdivides these feature maps appropriately and uses the average of each subdivision as a frequency value stored in the bin that represents that particular feature in the histogram generated for that location. Is my assumption correct?

",0,1
423,2015-5-26,2015,5,26,21,37big7,End-to-End Training of Deep Visuomotor Policies,https://www.reddit.com/r/MachineLearning/comments/37big7/endtoend_training_of_deep_visuomotor_policies/,evc123,1432642525,,1,3
424,2015-5-26,2015,5,26,22,37btp4,Which is the easiest to use sentiment analysis library which can be integrated with Apache Storm for a stream sentiment analysis?,https://www.reddit.com/r/MachineLearning/comments/37btp4/which_is_the_easiest_to_use_sentiment_analysis/,dhp1phd,1432648637,,1,4
425,2015-5-26,2015,5,26,23,37c1va,"""Smart Alec"": Syllogistic logic simulator from 1980s computer magazine",https://www.reddit.com/r/MachineLearning/comments/37c1va/smart_alec_syllogistic_logic_simulator_from_1980s/,gmsc,1432652379,,2,2
426,2015-5-27,2015,5,27,0,37c3hu,Mean Shift Clustering,https://www.reddit.com/r/MachineLearning/comments/37c3hu/mean_shift_clustering/,pl0d,1432653081,,0,1
427,2015-5-27,2015,5,27,0,37c4d3,Deep Convolutional Networks for Super-Resolution Image Reconstruction at Flipboard,https://www.reddit.com/r/MachineLearning/comments/37c4d3/deep_convolutional_networks_for_superresolution/,aadeshnpn,1432653450,,0,0
428,2015-5-27,2015,5,27,1,37cffz,Real-Time Interactive Movie Recommendation - ML,https://www.reddit.com/r/MachineLearning/comments/37cffz/realtime_interactive_movie_recommendation_ml/,john_philip,1432658136,,0,3
429,2015-5-27,2015,5,27,2,37cjfc,CVPR 2015 papers clustered using t-sne,https://www.reddit.com/r/MachineLearning/comments/37cjfc/cvpr_2015_papers_clustered_using_tsne/,jhartford,1432659739,,4,7
430,2015-5-27,2015,5,27,2,37ck1v,How can I let an lstm know that it's the beginning of a new element / item / sequence to train with?,https://www.reddit.com/r/MachineLearning/comments/37ck1v/how_can_i_let_an_lstm_know_that_its_the_beginning/,[deleted],1432659982,"I'm working with a neural network framework and made an lstm from it. I'm trying to build sentiment analysis with it.

I train it with one emotion and then another, but the last one always overpowers and I think that is because it this is a sequence and the short-term memory lasts.

Can I clear this memory without removing the training? How do I start the training of a new sequence?

The way the trainer is built, when I try to retrain the lstm the previous training is reset.

I'd greatly appreciate and advice, helps, or tips?

**Edit** Solved. Remove connections to previous states. Re-projected after training for recurrency.",6,3
431,2015-5-27,2015,5,27,2,37clsm,Leveraging Machine Learning to Discover Research,https://www.reddit.com/r/MachineLearning/comments/37clsm/leveraging_machine_learning_to_discover_research/,benjaminwilson,1432660693,,3,2
432,2015-5-27,2015,5,27,3,37ct47,Be Part of the Next National Data Science Bowl  Submit You Ideas for the Next Challenge,https://www.reddit.com/r/MachineLearning/comments/37ct47/be_part_of_the_next_national_data_science_bowl/,DataScienceOverlord,1432663674,,11,4
433,2015-5-27,2015,5,27,3,37cxp7,Mean shift clustering - a single hyper parameter and determines N automatically,https://www.reddit.com/r/MachineLearning/comments/37cxp7/mean_shift_clustering_a_single_hyper_parameter/,gromgull,1432665549,,18,58
434,2015-5-27,2015,5,27,7,37dy3p,Confused about non-convexity in feed-forward neural networks... Does the cost function have to be non-convex?,https://www.reddit.com/r/MachineLearning/comments/37dy3p/confused_about_nonconvexity_in_feedforward_neural/,Coneylake,1432680262,"So in Coursera's ML videos by Andrew Ng, Andrew uses a cost function like this for the feed-forward neural network:
http://i.stack.imgur.com/dW7RN.png

And I think this is non-convex. However, other sources use the squared error cost function (such as here: http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm). 

Are both of these cost functions non-convex or just the first one? I am having trouble using the first cost function as it looks like my implementation of gradient descent for a feed-forward neural network is very susceptible to finding local minimums in many of my example data-sets (running the algorithm several times, with randomly initialized weights, only gives me the global minimum a few times out of the several times). I am thinking if I should switch to a different cost function such as the second one to make a more convex surface for gradient descent to work over. 
",12,2
435,2015-5-27,2015,5,27,10,37eiir,A self- or group-study subreddit,https://www.reddit.com/r/MachineLearning/comments/37eiir/a_self_or_groupstudy_subreddit/,AddemF,1432689405,,0,5
436,2015-5-27,2015,5,27,10,37ekc2,Experimenting with AWS Machine Learning for Classification,https://www.reddit.com/r/MachineLearning/comments/37ekc2/experimenting_with_aws_machine_learning_for/,fontechevade,1432690233,,0,0
437,2015-5-27,2015,5,27,11,37eoj0,UAI 2015 Accepted Papers,https://www.reddit.com/r/MachineLearning/comments/37eoj0/uai_2015_accepted_papers/,pierrelux,1432692162,,0,1
438,2015-5-27,2015,5,27,13,37f3bx,Using C4.5 to predict Diabetes in Pima Indian Women,https://www.reddit.com/r/MachineLearning/comments/37f3bx/using_c45_to_predict_diabetes_in_pima_indian_women/,brotherrain,1432699315,,0,0
439,2015-5-27,2015,5,27,13,37f6xt,"Latent Dirichlet Allocation, need guidance!",https://www.reddit.com/r/MachineLearning/comments/37f6xt/latent_dirichlet_allocation_need_guidance/,zeferinix,1432701253,"I've read the papers a few times, [this](http://www.arbylon.net/publications/text-est.pdf) and [this](http://www.umiacs.umd.edu/~resnik/pubs/LAMP-TR-153.pdf) and I'm still somewhat lost. 

I'd like to hear your opinion on what I've ""probably understand"" about each node and from this model I just made w/ labels - [LDA Model](http://oi57.tinypic.com/28kkgwl.jpg). Kindly correct me if I'm wrong

1. **** &amp; **** are hyper-parameters that control the ""sparseness"" of the clustering. Are there other things that they control?

2. **** is the mixture of topics within the document (in my case, the possible topics in the tweets). I'm not sure how this works

3. **** is the distribution of words to topics. Let's say for example: 
    animals: {cat, dog, bird, etc.} 
    fruits: {apple, banana, grapes, etc.} 
    education: {school, student, teacher, etc.} 
This is also where the Inference does its job right? The burn-in iterations for learning with *Gibbs Sampling*? (correct me if I'm wrong)

4. ***Z*** is the decision making where the word (***W***) should be assigned. (How I imagine this is every word captured on every iteration, it will be assigned a score where it's most probable then it's passed to ***Z*** and assign it where it's near. Like let's say word (***W***) ""bird"" got score of 0.33. It is then passed to ***Z*** to place the word where it's appropriate. Let's say the topic ***Z*** ""animals"" is assigned to a score of 0.35, since the word has a score of 0.33 and it is near to topic ""animals"" with a score of 0.35, the word is assigned there and that's that. Then word (***W***) iterates and moves to the next word and loops this until the tweet is finished.)

5. ***W*** is basically the word being observed at every iteration?

6. Plate ***N*** iterates over all the words in the tweet and repeats the process of ***W*** and ***N***. Basically it gets the current word ***W***, based on the score, it assigns it to the proper topic ***Z***

7. Plate ***D*** and Plate ***K***, I find them confusing because of the parameters and I'm having a hard time understanding *Dirichlet Distribution* because of the overwhelming math figures that barely understand. Got totally lost when I arrived on the part where there are formulas with integrals and limits.

Can someone explain in simpler terms the figures below without using too much math on how LDA flows and what really happens on each node in words? Found from this [post](http://www.reddit.com/r/MachineLearning/comments/372tyj/naive_bayes_for_document_clustering/)

    [i] ~ Dirichlet() for i=1:M
    [k] ~ Dirichlet() for k=1:K
    z[i,j] ~ Categorical([i]) for i=1:M, j=1:N[i] (topics in documents)
    w[i,j] ~ Categorical([z[i,j]]) for i=1:M, j=1:N[i] (words in documents)",8,5
440,2015-5-27,2015,5,27,15,37fkax,455,https://www.reddit.com/r/MachineLearning/comments/37fkax/455/,unique-crusher,1432709965,"&lt;a href=""baidu.com""&gt;baidu&lt;/a&gt;",0,1
441,2015-5-27,2015,5,27,16,37fn5j,"Four chapters of ""Model-Based Machine Learning"" by John Winn and Christopher Bishop",https://www.reddit.com/r/MachineLearning/comments/37fn5j/four_chapters_of_modelbased_machine_learning_by/,iori42,1432712243,,10,57
442,2015-5-27,2015,5,27,18,37fvvv,"Expresso : A Python-based GUI for designing, training and exploring deep-learning frameworks.",https://www.reddit.com/r/MachineLearning/comments/37fvvv/expresso_a_pythonbased_gui_for_designing_training/,VAL_IISc,1432719679,"**Expresso** is a Python-based GUI for designing, training and exploring deep-learning frameworks. It is built atop [Caffe](http://caffe.berkeleyvision.org/), the open-source, prize-winning framework popularly used to develop Convolutional Neural Networks.

For details on features, installation and usage, check out the [project page](http://val.serc.iisc.ernet.in/expresso) !  ",0,38
443,2015-5-27,2015,5,27,18,37fx4d,Experimenting with AWS Machine Learning for Classification wia @rightrelevance,https://www.reddit.com/r/MachineLearning/comments/37fx4d/experimenting_with_aws_machine_learning_for/,badadata200M,1432720720,,0,0
444,2015-5-27,2015,5,27,19,37fy12,How NoSQL Fundamentally Changed Machine Learning,https://www.reddit.com/r/MachineLearning/comments/37fy12/how_nosql_fundamentally_changed_machine_learning/,vm1942,1432721396,,0,0
445,2015-5-27,2015,5,27,21,37g8jz,DataCoder Digest - a free weekly email of interesting links about Machine Learning and Data Science,https://www.reddit.com/r/MachineLearning/comments/37g8jz/datacoder_digest_a_free_weekly_email_of/,aidanf,1432728445,,0,0
446,2015-5-27,2015,5,27,21,37gd0d,A Murder Mystery - Model based Machine Learning,https://www.reddit.com/r/MachineLearning/comments/37gd0d/a_murder_mystery_model_based_machine_learning/,john_philip,1432730949,,0,6
447,2015-5-27,2015,5,27,21,37gd1w,Setting up Python for machine learning: scikit-learn and IPython Notebook,https://www.reddit.com/r/MachineLearning/comments/37gd1w/setting_up_python_for_machine_learning/,aranag,1432730979,,0,3
448,2015-5-28,2015,5,28,0,37gy5p,Question about Maximum Likelihood Estimation.,https://www.reddit.com/r/MachineLearning/comments/37gy5p/question_about_maximum_likelihood_estimation/,okayshokay,1432741101,"The goal of MLE is to estimate the true \theta that the data D comes from. I am having some confusion here. How can we say that we want to estimate the \theta that a D comes from, when we are giving the data D itself as input? I know that I am missing something basic here, but am unable to figure out what is that. Any pointers will be really helpful.",4,1
449,2015-5-28,2015,5,28,2,37hhgl,"In a time series regression problem, is there a technique which maps the next step's space onto the current step parameter space?",https://www.reddit.com/r/MachineLearning/comments/37hhgl/in_a_time_series_regression_problem_is_there_a/,[deleted],1432748979,"Kind of difficult to explain, but imagine you're searching a parameter space with, for example, gradient descent. That will search a parameter space which is defined by all prior data, which will hopefully give you a good solution for the next step.

So I'm thinking of an algorithm which takes the next step's space during training and learns the features of that space which correspond to the current step's space. 

Essentially a meta SGD algorithm which takes the next step as a parameter during training as a teacher signal, to get a better next step prediction during testing.",2,0
450,2015-5-28,2015,5,28,4,37hxre,Does anyone know any good paper or resources on generating questions from facts,https://www.reddit.com/r/MachineLearning/comments/37hxre/does_anyone_know_any_good_paper_or_resources_on/,DanielSlater8,1432755518,"e.g. If the fact is ""the number of pot holes in the new york is 180,000"" generating the question ""How many pot holes are in new york"" if the fact ""Tories won the general election in 2015"" the question is ""Who won the generatl election in 2015"", etc.",5,2
451,2015-5-28,2015,5,28,4,37hxss,Go ahead and break this robots legs. It can figure out how to chase you without them.,https://www.reddit.com/r/MachineLearning/comments/37hxss/go_ahead_and_break_this_robots_legs_it_can_figure/,redadit,1432755534,,0,1
452,2015-5-28,2015,5,28,4,37hz2e,"I thought you might be interested in knowing that Vapnik's ""Statistical Learning Theory"" is the third most cited paper in computer science of all time. That is all.",https://www.reddit.com/r/MachineLearning/comments/37hz2e/i_thought_you_might_be_interested_in_knowing_that/,[deleted],1432756052,,5,9
453,2015-5-28,2015,5,28,4,37i0b6,Multilabel classifier using spark,https://www.reddit.com/r/MachineLearning/comments/37i0b6/multilabel_classifier_using_spark/,[deleted],1432756535,"Hi all.

I'm attempting to build a multilabel classifier using apache spark and the Randomforest / LogisticRegressionWithLBFGS models.

I'm my data is in the format: 

https://www.refheap.com/101767

I am creating labeledpoints out of each of these labels with the features. 

I'm running into trouble figuring out how to have a prediction of multiple labels with the confidence ratings

I'm truely stuck and any guidance would be greatly appreciated",1,0
454,2015-5-28,2015,5,28,4,37i0dy,[Ask ML] How much does it slow down GPU training of a neural net to use the GPU for the monitor at the same time?,https://www.reddit.com/r/MachineLearning/comments/37i0dy/ask_ml_how_much_does_it_slow_down_gpu_training_of/,dhammack,1432756562,"Title should be sufficient. I've got a 970 that I'm using for training deep nets and also for powering the monitor display. Disconnecting the monitor didn't seem to have much of an impact on training time, so I'm wondering if it's worth it to get a separate GPU for display purposes while training. Currently I can't use batches larger than 32 without running out of memory and crashing either, which I suspect may be related.

Thanks!


",9,1
455,2015-5-28,2015,5,28,5,37i8ng,LSTM Implementation question (Torch7),https://www.reddit.com/r/MachineLearning/comments/37i8ng/lstm_implementation_question_torch7/,dlcu,1432759699,"Hi guys,

I am trying to implement a LSTM RNN but I am having trouble understanding this piece of [code](http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/).

    local function lstm(i, prev_c, prev_h)
      local function new_input_sum()
        local i2h            = nn.Linear(params.rnn_size, params.rnn_size)
        local h2h            = nn.Linear(params.rnn_size, params.rnn_size)
        return nn.CAddTable()({i2h(i), h2h(prev_h)})
      end
      local in_gate          = nn.Sigmoid()(new_input_sum())
      local forget_gate      = nn.Sigmoid()(new_input_sum())
      local in_gate2         = nn.Tanh()(new_input_sum())
      local next_c           = nn.CAddTable()({
        nn.CMulTable()({forget_gate, prev_c}),
        nn.CMulTable()({in_gate,     in_gate2})
      })
      local out_gate         = nn.Sigmoid()(new_input_sum())
      local next_h           = nn.CMulTable()({out_gate, nn.Tanh()(next_c)})
      return next_c, next_h
    end

In this paper http://arxiv.org/pdf/1308.0850v5.pdf the formula for the input gate is:

$i_t = \sigma(W_{xi}x_{t} + W_{hi}h_{t-1} + W_{ci}c_{t-1} + b_i)$

But this code seems to be implementing only 

i_t = \sigma(W_{xi}x_{t} + W_{hi}h_{t-1})

Is this correct? Where is W_{ci}c_{t-1} and bias b_i summed?

Thanks for your time",3,6
456,2015-5-28,2015,5,28,5,37i9og,Deep Learning : review paper.,https://www.reddit.com/r/MachineLearning/comments/37i9og/deep_learning_review_paper/,muktabh,1432760111,,20,94
457,2015-5-28,2015,5,28,7,37intg,"I made this, MOOCs recommendation from text input.",https://www.reddit.com/r/MachineLearning/comments/37intg/i_made_this_moocs_recommendation_from_text_input/,shrimpMasta,1432765644,"I just finished the alpha version of this project. I am using Latent Dirichlet Allocation (LDA) on courses subtitles to generate feature representations of MOOCs and link them to any text input. I use mallet for the Latent Dirichlet Allocation part, I'm using the stanford NLP lib to tokenize/pos tag/lemmatize my data. The current pipeline is really not optimized and most of the request time is about loading the models (LDA and pos tagger) each time ... I want to show you this because I'm looking for some feedback and ways to improve. It is important to mention that I'm running this on a small digitalOcean machine with 512Mo of RAM and only one processor, please don't hug it to death. Thanks for your recommendations !

Here is the link to the application:  http://findmoocs.net/#from-text",12,7
458,2015-5-28,2015,5,28,9,37j1jy,Recognizing Human Activities with Kinect - Choosing a temporal model,https://www.reddit.com/r/MachineLearning/comments/37j1jy/recognizing_human_activities_with_kinect_choosing/,zuzami,1432771434,,0,0
459,2015-5-28,2015,5,28,10,37jae1,Sketch Recognition: Yet another victory of machine over human,https://www.reddit.com/r/MachineLearning/comments/37jae1/sketch_recognition_yet_another_victory_of_machine/,SleepAura,1432775286,"A few months ago, a group from Microsoft Research reported a result surpassing human-level performance on ImageNet classification.

Similar to ImageNet (most are photos), but for sketch images, there is a dataset published by TU-Berlin, where human can, on average, achieve 73% accuracy.

Since it was published in 2012, many researchers have worked on producing better sketch classification models, however, none of those has successfully beat human so far.

Quite recently, a team in University of London tried to nuke this problem by deep learning, and not surprisingly, they have managed to make machine intelligence surpass human (75% vs 73%).

Preprint can be found: http://arxiv.org/abs/1501.07873",3,0
460,2015-5-28,2015,5,28,10,37jaem,Question about hierarchical clustering,https://www.reddit.com/r/MachineLearning/comments/37jaem/question_about_hierarchical_clustering/,Phooey138,1432775293,"K-means (for example) can find the same low level features in an image as a neural network, can hierarchical clustering have similar performance to deep neural nets for classification tasks? What are the major differences? ",5,0
461,2015-5-28,2015,5,28,10,37jcuq,Introduction to Neural Machine Translation (part 1),https://www.reddit.com/r/MachineLearning/comments/37jcuq/introduction_to_neural_machine_translation_part_1/,harrism,1432776361,,0,13
462,2015-5-28,2015,5,28,10,37jfw8,Nature article by Michael Littman on Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/37jfw8/nature_article_by_michael_littman_on/,pierrelux,1432777717,,10,10
463,2015-5-28,2015,5,28,13,37k1xx,Hands-on dplyr tutorial for faster data manipulation in R,https://www.reddit.com/r/MachineLearning/comments/37k1xx/handson_dplyr_tutorial_for_faster_data/,aranag,1432788803,,0,0
464,2015-5-28,2015,5,28,14,37k6vm,Find Here the Best Product of Ultrasonic Fabric Cutting Machines,https://www.reddit.com/r/MachineLearning/comments/37k6vm/find_here_the_best_product_of_ultrasonic_fabric/,balacchihf,1432792298,,0,0
465,2015-5-28,2015,5,28,15,37kaso,"Deep Learning, Self-Taught Learning and Unsupervised Feature Learning",https://www.reddit.com/r/MachineLearning/comments/37kaso/deep_learning_selftaught_learning_and/,aranag,1432794849,,2,13
466,2015-5-28,2015,5,28,17,37kk0a,Excavator Rubber Yanmar VIO50 offset,https://www.reddit.com/r/MachineLearning/comments/37kk0a/excavator_rubber_yanmar_vio50_offset/,mistertrackandpad,1432801873,,1,0
467,2015-5-28,2015,5,28,17,37kkb2,[CfP] Call for Participation: DSL Shared and Unshared Task at RANLP 2015,https://www.reddit.com/r/MachineLearning/comments/37kkb2/cfp_call_for_participation_dsl_shared_and/,DSLSharedTask,1432802153,"**Call For Participation**

DSL Shared and Unshared Task at LT4VarDial Workshop - RANLP 2015 (http://ttg.uni-saarland.de/lt4vardial2015/dsl.html)

Discriminating between similar languages and language varieties is one of the bottlenecks of language identification systems. This aspect has been topic of a number of papers published in the last years as well as the first edition of the DSL shared task in 2014. This year we are organizing a new edition of the DSL shared task with new languages, a new evaluation methodology and an unshared task track.

In the unshared task, teams can use any version of DSLCC as a resource to investigate differences between similar languages and language varieties using NLP methods. The fundamental question that unshared task participant should answer is: ""What are the most important differences between these similar languages and language varieties?""

**Important Dates**:

Training set release: May 20th, 2015 
Test set release: June 15th, 2015 
Results submission due: June 17th, 2015 
Results announced: June 25th, 2015 
Paper submission (unshared task): July 6th, 2015 
Paper submission (shared task): July 20th, 2015 
Acceptance notification (shared and unshared): August 1st, 2015 
Camera-ready versions: August 9th, 2015

**Organizers**

Liling Tan (Saarland University, Germany), Marcos Zampieri (Saarland University and DFKI, Germany), Nikola Ljubei (University of Zagreb, Croatia), Jrg Tiedemann (Uppsala University, Sweden), Preslav Nakov (Qatar Computing Research Institute, Qatar)",0,0
468,2015-5-28,2015,5,28,19,37kr88,Agrison Tractors Reviews,https://www.reddit.com/r/MachineLearning/comments/37kr88/agrison_tractors_reviews/,agrison,1432807947,,0,1
469,2015-5-28,2015,5,28,22,37l8ku,Averaging mini-batch gradients in dropout,https://www.reddit.com/r/MachineLearning/comments/37l8ku/averaging_minibatch_gradients_in_dropout/,personalityson,1432818677,"Let's assume I set my mini-batches to size 10.
If units in two hidden layers are dropped out on random, it is possible that, say, a single weight matrix cell W(2,4) is updated 7 times, while W(2,5) has had 2 updates.
Is it then fair to divide each by 10?

I do online dropout-scaling, so the updates are already adjusted for dropped units in layer below/above... But averaging is something separate, no?",5,2
470,2015-5-28,2015,5,28,22,37lbk9,Interactive R Coding Tutorial on Machine Learning by Kaggle,https://www.reddit.com/r/MachineLearning/comments/37lbk9/interactive_r_coding_tutorial_on_machine_learning/,martijnT,1432820059,,9,105
471,2015-5-28,2015,5,28,23,37lg34,Optimal feature for convolutional networks?,https://www.reddit.com/r/MachineLearning/comments/37lg34/optimal_feature_for_convolutional_networks/,seilgu,1432822153,"Machine learning newb nere. Some question about the first-layer features of a convNet.

I've heard that nowadays people just use backprop to train a convnet. But I think, as in animals, the features in the first layers should always look like edge detectors, and it should be independent of the goal of the whole network : No matter what the task is, caption labeling or image segmentation, you should always get edge detectors in the first layer.

But if that assumption is correct, then the features in the first layer must only depend on the input statistics of its reception field. The only way I know at the moment is to pretrain it using autoencoders or maybe RBM. But my question is : is there some optimal criteria for these features?

For example, if I have 2 neurons, each sees a 10x10 pixels patch in the image, and most of the time in the 10x10 area it's either a uniform patch or there is an edge, then, is the horizontal/vertical edge detectors the optimal features I'll get (or up to a rotation)?

",4,0
472,2015-5-28,2015,5,28,23,37llnk,"Despite Speedups for Machine Learning/DNN, GPUs Will Never Penetrate",https://www.reddit.com/r/MachineLearning/comments/37llnk/despite_speedups_for_machine_learningdnn_gpus/,[deleted],1432824618,,2,0
473,2015-5-29,2015,5,29,0,37lp5c,Thought vectors could revolutionize artificial intelligence | ExtremeTech,https://www.reddit.com/r/MachineLearning/comments/37lp5c/thought_vectors_could_revolutionize_artificial/,aadeshnpn,1432826027,,0,1
474,2015-5-29,2015,5,29,1,37lyh2,Dato blog series on evaluating Machine Learning Models,https://www.reddit.com/r/MachineLearning/comments/37lyh2/dato_blog_series_on_evaluating_machine_learning/,ssushant,1432829895,,0,0
475,2015-5-29,2015,5,29,2,37m67v,[ebook] #MusicTech by Alexandre Passant - Experimenting with Data Science and Recommender Systems,https://www.reddit.com/r/MachineLearning/comments/37m67v/ebook_musictech_by_alexandre_passant/,fhoffa,1432833184,,1,4
476,2015-5-29,2015,5,29,3,37mgpl,"I, Marketer: How Machine Learning Robots Will Replace Human Marketers",https://www.reddit.com/r/MachineLearning/comments/37mgpl/i_marketer_how_machine_learning_robots_will/,[deleted],1432837639,,1,0
477,2015-5-29,2015,5,29,4,37mo1e,7 Ways to Improve your Predictive Models,https://www.reddit.com/r/MachineLearning/comments/37mo1e/7_ways_to_improve_your_predictive_models/,D33B,1432840710,,2,5
478,2015-5-29,2015,5,29,7,37nbfz,Implementing the Hamming Distance with KNN?,https://www.reddit.com/r/MachineLearning/comments/37nbfz/implementing_the_hamming_distance_with_knn/,bonfire09,1432850852,"I have a data set with with both categorical and continuous attributes. The categorical data has already been coded. My target variable is a 0,1 binary variable. I am currently using R to implement my models on but I am unable to find a package that performs knn with the Hamming distance. I tried searching over the internet but I am unable to find an example to work off from in R basially after I had split my data set into training and test sets with target attribute removed from both sets and which is split into train target and test_target. Any help would be great thanks. ",3,0
479,2015-5-29,2015,5,29,8,37np5j,Script Example: Snowball Stemmer in NLTK on Crowdflower Search Relevance Kaggle Competition,https://www.reddit.com/r/MachineLearning/comments/37np5j/script_example_snowball_stemmer_in_nltk_on/,[deleted],1432857153,,0,0
480,2015-5-29,2015,5,29,9,37nv6r,How to Perform Clustering in a Single Command Line,https://www.reddit.com/r/MachineLearning/comments/37nv6r/how_to_perform_clustering_in_a_single_command_line/,czuriaga,1432860024,,0,1
481,2015-5-29,2015,5,29,11,37obuz,"Get your free copy of ""Instant R Starter"" just for today",https://www.reddit.com/r/MachineLearning/comments/37obuz/get_your_free_copy_of_instant_r_starter_just_for/,GopalaPK,1432868236,,0,1
482,2015-5-29,2015,5,29,12,37og1o,List of Funds or Trading Firms Using Artificial Intelligence or Machine Learning,https://www.reddit.com/r/MachineLearning/comments/37og1o/list_of_funds_or_trading_firms_using_artificial/,mattrobust,1432870338,,3,0
483,2015-5-29,2015,5,29,15,37ox98,Which extra senses can I buy today?,https://www.reddit.com/r/MachineLearning/comments/37ox98/which_extra_senses_can_i_buy_today/,[deleted],1432881158,"I read these cool articles about people seeing with their tongue, people learning to hear with a vest, learning to feel the direction to the north pole etc. I want to try this! So which extra senses can I actually buy today? ",3,0
484,2015-5-29,2015,5,29,17,37p7s3,Tracking Employment Shocks Using Mobile Phone Data,https://www.reddit.com/r/MachineLearning/comments/37p7s3/tracking_employment_shocks_using_mobile_phone_data/,cast42,1432889704,,5,2
485,2015-5-29,2015,5,29,20,37phy4,Trying to learn machine learning in a week,https://www.reddit.com/r/MachineLearning/comments/37phy4/trying_to_learn_machine_learning_in_a_week/,mrborgen,1432898483,,2,0
486,2015-5-29,2015,5,29,22,37psy9,Independent Component Analysis - Multivariate Time Series Decomposition,https://www.reddit.com/r/MachineLearning/comments/37psy9/independent_component_analysis_multivariate_time/,domac,1432905297,"Hello everyone,

I am deeply interested in time series; moreover, I wanted to know more about the topic of multivariate time series decomposition (e.g. with ICA).
I already found this interesting paper [0] and wanted to ask if someone is experienced with the topic and mind sharing some information about it, i.e. other interesting papers, etc.?

Best wishes and thanks in advance!

[0] http://www.lmd.jussieu.fr/~falmd/articles/2000_AiresChedin.pdf",5,2
487,2015-5-30,2015,5,30,2,37qq3p,Learning holiness teaching a rnn the King James Bible.,https://www.reddit.com/r/MachineLearning/comments/37qq3p/learning_holiness_teaching_a_rnn_the_king_james/,[deleted],1432919593,,0,0
488,2015-5-30,2015,5,30,2,37qt78,"Tecsaw also provides world class machine repair, blade repair and tooth sharpening services in the Toronto area",https://www.reddit.com/r/MachineLearning/comments/37qt78/tecsaw_also_provides_world_class_machine_repair/,similura1,1432920845,,0,1
489,2015-5-30,2015,5,30,4,37r93h,Domain-Adversarial Training of Neural Networks,https://www.reddit.com/r/MachineLearning/comments/37r93h/domainadversarial_training_of_neural_networks/,clbam8,1432927276,,7,5
490,2015-5-30,2015,5,30,4,37raoo,A good source to learn Recurrent Neural Nets and Long Short Term Memory Nets?,https://www.reddit.com/r/MachineLearning/comments/37raoo/a_good_source_to_learn_recurrent_neural_nets_and/,TheWittyCat,1432927934,I tried Google but came up with only vague powerpoint and pdf tutorials that give an general overview but don't go into a lot of depth. ,15,46
491,2015-5-30,2015,5,30,4,37rbu1,"VJ Pamensky is proud to be the Canadian business partner, and preferred National vendor of WEG branded electric motors, variable frequency drives and softstarters",https://www.reddit.com/r/MachineLearning/comments/37rbu1/vj_pamensky_is_proud_to_be_the_canadian_business/,similura1,1432928421,,0,1
492,2015-5-30,2015,5,30,6,37rqvd,Anyone doing ML research in areas other than NNs/Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/37rqvd/anyone_doing_ml_research_in_areas_other_than/,quattro,1432934647,"First off: I am not a pure ML researcher. I develop methods in statistical/medical genetics and tend to mostly use linear mixed models and variations around that. My background is computer science and I'm familiar with other advanced learning methods, but this sub seems saturated with posts about NNs. While they are interesting for many people (both within and outside of ML) they aren't yet useful for my own research (or at least I'm not yet sure how to meaningfully apply them). With that being said, if any of you are working in ML on non-NN methods what are they like? How do they differ?

EDIT: To clarify, I'm familiar with other methods, but I am more interested in those of you doing research in non-NN ML. What are you researching? Tell me about your work! :D",25,28
493,2015-5-30,2015,5,30,14,37td4f,Does anyone know a dataset about IoT security?,https://www.reddit.com/r/MachineLearning/comments/37td4f/does_anyone_know_a_dataset_about_iot_security/,rilut,1432965542,"It could be similar to this [KDD '99 cup dataset](https://kdd.ics.uci.edu/databases/kddcup99/task.html) about Intrusion Detection Learning or not. But it should be about IoT. I already googled with no luck.

Thank you!",5,0
494,2015-5-30,2015,5,30,16,37tkyp,I'm learning ML. I want to build a neural network and analyze some data from reddit with it. What kind of simple but cool thing could I do? Can you help me come up with a practical project idea?,https://www.reddit.com/r/MachineLearning/comments/37tkyp/im_learning_ml_i_want_to_build_a_neural_network/,raymestalez,1432971316,"Hi! I'm trying to come up with some simple but practical and useful project.
Ideally it would be a website that would take data from reddit API and then use a neural network to analyze it in some cool way.

I need  something that a novice could accomplish, yet useful/interesting to other people.

Can you help me to come up with some ideas?",17,4
495,2015-5-30,2015,5,30,23,37ufmd,Team RBO from Berlin wins Amazon Picking Challenge convincingly,https://www.reddit.com/r/MachineLearning/comments/37ufmd/team_rbo_from_berlin_wins_amazon_picking/,Articulated-rage,1432995201,,11,40
496,2015-5-31,2015,5,31,1,37uxwp,Logistic Regression Bigram Text Classification Help w/ Patsy,https://www.reddit.com/r/MachineLearning/comments/37uxwp/logistic_regression_bigram_text_classification/,lovestowritecode,1433004121,"I'm working on upgrading a LogisticRegression text classification from single word features to bigrams (two word features). However when I include the two word feature in the formula sent to patsy.dmatrices, I receive the following error...

    y, X = dmatrices(""is_host ~ dedicated + hosting + dedicated hosting"", df, return_type=""dataframe"")

      File ""&lt;string&gt;"", line 1
        dedicated hosting
                ^
    SyntaxError: unexpected EOF while parsing
I've looked around online for any examples on how to approach this and haven't found anything.

What is the proper way to include multi-word features in the formula passed to dmatricies?",8,0
497,2015-5-31,2015,5,31,3,37vbfw,Predict a probability distribution,https://www.reddit.com/r/MachineLearning/comments/37vbfw/predict_a_probability_distribution/,Tulip-Stefan,1433010302,"I have a series of continuous events which i would like to predict.

But rather than predicting the value of the next event, i would like to predict the probability distribution for the next event. Or more precisely, i would like to predict the value of Y where i have P% confidence that the next sample will be less than the predicted value Y. When i vary P i expect to get some kind of S curve.

I am familiar with the math behind basic machine learning instruments, neural networks, svm/knnc and trees, but i don't feel that i can apply either of these to solve this problem. What algorithms can i apply?",15,0
498,2015-5-31,2015,5,31,4,37vh6v,How is Andrew Ng Stanford Machine Learning course?,https://www.reddit.com/r/MachineLearning/comments/37vh6v/how_is_andrew_ng_stanford_machine_learning_course/,bagelorder,1433012836,"I really like the enthusiastic and motivating way he teaches the lectures. (I just watched the first lesson, so no real material yet.)

Can anyone tell me how deep this course dives into the theory? I plan on doing all the homework also, he posted some. His coursera course is said to be very practical, and I want to really understand the theory. Will this lecture get me started off with understanding the fundamental concepts very well? For example, would I be able to understand what the different chapters of Bishop's book are about and be able, if I wanted to, to just go to the chapter I want to learn more about and read that?

Also, the programming parts of the homework are in Matlab. Are they feasible in Python too? Are there even Python solutions out there?

I am talking about his Stanford lectures on YouTube ( https://www.youtube.com/playlist?list=PLA89DCFA6ADACE599 ), not his coursera course!

",37,42
499,2015-5-31,2015,5,31,8,37wd3w,"MachineLearning Startup going into Beta, help design the future! Limited spots available.",https://www.reddit.com/r/MachineLearning/comments/37wd3w/machinelearning_startup_going_into_beta_help/,[deleted],1433027681,,0,1
500,2015-5-31,2015,5,31,10,37wpud,Conceptual LSTM Question(s),https://www.reddit.com/r/MachineLearning/comments/37wpud/conceptual_lstm_questions/,[deleted],1433034276,"I'm finally looking over the math (in [Graves' PhD thesis](http://www.cs.toronto.edu/~graves/phd.pdf)) for an LSTM memory block. I understand that the input, output, and forget gates of a block depend on (the nonlinear activation of) a weighted sum of other block outputs, the values in the cells, and the input data. 

Furthermore, I see that when these weighted sums are squashed by a sigmoid, the gate value is in [0,1]. The values in the cells are then either updated (written) (if the input gate is ""open"" and forget gate ""closed"") or kept the same (if the input gate is ""closed"" and the forget gate ""open""). Similarly, the value in the cell is either output (read) or not, according to the value of the output gate. This leads me to two questions.

1. I don't see anything mathematically that makes these reads and writes perfect - i.e. since the gates don't seem to be constrained to be exactly equal to 0 or 1 (by some min/max or floor/ceiling business or something), the data in the cell is not perfectly overwritten or perfectly kept. 

2.  Given that the data is kept if the forget gate is close to 1, shouldn't it be called a ""remember"" gate? :)

",4,4
501,2015-5-31,2015,5,31,12,37x7ta,[X-post from /r/philosophy] No Time Like The Present For AI Safety Work,https://www.reddit.com/r/MachineLearning/comments/37x7ta/xpost_from_rphilosophy_no_time_like_the_present/,chipbag01,1433044161,,1,0
502,2015-5-31,2015,5,31,16,37xol9,Can ML detect a recent change in the behaviour of a condition and extrapolate that the change will continue rather than averaging from before the change?,https://www.reddit.com/r/MachineLearning/comments/37xol9/can_ml_detect_a_recent_change_in_the_behaviour_of/,validated1,1433055638,"Example. I am running a direct response ad campaign in Europe, but suddenly my product becomes illegal and sales plummet. How long until my ML (naive bayes) updates to predict low sales.

Just started learning, so any tips are welcome!",5,0
503,2015-5-31,2015,5,31,19,37y0vd,Feeding an image to a RNN or dealing with images of different dimensions?,https://www.reddit.com/r/MachineLearning/comments/37y0vd/feeding_an_image_to_a_rnn_or_dealing_with_images/,ganarajpr,1433067364,"I was just wondering if anyone has tried feeding an image to an RNN ? If we think about it an image is a sequence of pixels. What are some possible downsides of this ? The reason I am thinking about this is that currently as far as I have read you can only feed images of fixed size to CNN's ( generally 256x256x3 ) etc.. But I was wondering what would be a good strategy if I wanted to use images of different dimensions ? say 600x8000 or even 8000x600 ? What would be a good strategy for that ?  
Are there any papers that relate to feeding an Image to RNN ? ",16,13
504,2015-5-31,2015,5,31,19,37y2jf,why bagging works,https://www.reddit.com/r/MachineLearning/comments/37y2jf/why_bagging_works/,godspeed_china,1433069014,"Bagging works, as it provides a mean estimation. But is there an alternative explanation? bagging of 1-nearest neighbor predictor is better than the original one. Is it because bagging reduces the effective degree of freedom? Additionally, in my bagging tree predictor, I see that sometime, setting a minimum leaf node size provides better result. Can bagging replace regularization?",8,9
505,2015-5-31,2015,5,31,20,37y4ts,Web Scale Document Clustering: Clustering 733 Million Web Pages,https://www.reddit.com/r/MachineLearning/comments/37y4ts/web_scale_document_clustering_clustering_733/,cmmdevries,1433071106,,4,14
506,2015-5-31,2015,5,31,20,37y54n,lstm producing an output for each memory cell not output layers,https://www.reddit.com/r/MachineLearning/comments/37y54n/lstm_producing_an_output_for_each_memory_cell_not/,[deleted],1433071354,"I'm building my own lstm from [synaptic.js](https://github.com/theirf/synaptic) and I think I have my connections wrong because when I activate my lstm network, I get 16 outputs when in the instance I have defined I have given it 5 outputs. Now, in that instance there are 16 memory-cells in the memory-block.

Each of the layers and gates are defined in the construction, I'm just connecting them here, but I believe I have my connections confused somewhere I was wondering if someone could help me sort them out. Could it be that I have 16 cells, but it is projecting as if I have 1 so it is making 16 connections to the output? If so, how would I fix that please?

    //connections from input layer
    var input = this.inputLayer.project(this.memoryCell);
    this.inputLayer.project(this.inputGate);
    this.inputLayer.project(this.forgetGate);
    this.inputLayer.project(this.outputGate);
    //connections from memory cell
    var output = this.memoryCell.project(this.outputLayer);
    //self-connection
    var self = this.memoryCell.project(this.memoryCell);
	
    //peepholes
    this.memoryCell.project(this.inputGate,  Layer.connectionType.ONE_TO_ONE);
    this.memoryCell.project(this.forgetGate, Layer.connectionType.ONE_TO_ONE);
    this.memoryCell.project(this.outputGate, Layer.connectionType.ONE_TO_ONE);

    //gates
    this.inputGate.gate(input, Layer.gateType.INPUT);
	this.forgetGate.gate(self, Layer.gateType.ONE_TO_ONE);
    this.outputGate.gate(output, Layer.gateType.OUTPUT);
	
    //input to output direct connection
    this.inputLayer.project(this.outputLayer);
	
    this.set({
        input: this.inputLayer,
        hidden: [this.inputGate, this.forgetGate, this.memoryCell, this.outputGate],
        output: this.outputLayer
    });

Sometimes I have even noticed 21 outputs, which I assume comes from the 16 memory-cells + the 5 outputs.

Here's how I'm constructing the object: `var lstm=new LSTM(1, 16, 5); //1 input later, 16 memory cells, and 5 output layers`",0,0
507,2015-5-31,2015,5,31,22,37yg4i,How can I check the polarity for a new word that is not present in a training set?,https://www.reddit.com/r/MachineLearning/comments/37yg4i/how_can_i_check_the_polarity_for_a_new_word_that/,mhass24,1433079260,"I'm doing sentiment analysis for tweets, using the naive bays algorithm.",5,0
