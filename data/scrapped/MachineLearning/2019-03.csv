,sbr,sbr_id,date,year,month,hour,day,id,domain,title,full_link,author,created,selftext,num_comments,score,over_18,thumbnail,media_provider,media_thumbnail,media_title,media_description,media_url
0,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,9,avy7yc,self.statistics,[D] Books on modern statistics.,https://www.reddit.com/r/MachineLearning/comments/avy7yc/d_books_on_modern_statistics/,Bayequentist,1551399999,,0,2,False,default,,,,,
1,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,10,avz3sa,youtu.be,HALLMARKING MACHINE,https://www.reddit.com/r/MachineLearning/comments/avz3sa/hallmarking_machine/,gaya3sureshp,1551405512,,0,1,False,default,,,,,
2,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,11,avz6vi,arxiv.org,[R] Artist Style Transfer Via Quadratic Potential,https://www.reddit.com/r/MachineLearning/comments/avz6vi/r_artist_style_transfer_via_quadratic_potential/,rahulbhalley,1551406040,,10,1,False,default,,,,,
3,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,11,avz8zq,self.MachineLearning,NCAA March Madness Machine Learning - Seeking a Collaborator,https://www.reddit.com/r/MachineLearning/comments/avz8zq/ncaa_march_madness_machine_learning_seeking_a/,punchJP,1551406419,[removed],0,1,False,self,,,,,
4,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,11,avzglr,self.MachineLearning,Moving Target: what ML methodologies can aid in location prediction?,https://www.reddit.com/r/MachineLearning/comments/avzglr/moving_target_what_ml_methodologies_can_aid_in/,TCleckner,1551407746,[removed],0,1,False,self,,,,,
5,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,12,avzyas,i.redd.it,[P] Simple Tensorflow implementation of AdaBound (ICLR 2019),https://www.reddit.com/r/MachineLearning/comments/avzyas/p_simple_tensorflow_implementation_of_adabound/,taki0112,1551410749,,0,1,False,default,,,,,
6,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,12,avzylt,i.redd.it,[P] Simple Tensorflow implementation of AdaBound (ICLR 2019),https://www.reddit.com/r/MachineLearning/comments/avzylt/p_simple_tensorflow_implementation_of_adabound/,taki0112,1551410798,,0,1,False,default,,,,,
7,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,12,avzyyu,i.redd.it,[P] Simple Tensorflow implementation of AdaBound (ICLR 2019),https://www.reddit.com/r/MachineLearning/comments/avzyyu/p_simple_tensorflow_implementation_of_adabound/,taki0112,1551410857,,1,1,False,default,,,,,
8,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,12,aw00n5,self.MachineLearning,"[D] Did MILA delete the ""Meet the professors at @MILAMontreal"" hype video?",https://www.reddit.com/r/MachineLearning/comments/aw00n5/d_did_mila_delete_the_meet_the_professors_at/,AllInAvocados,1551411147,"The video used to be here: 

[https://www.youtube.com/watch?v=l1XMRMtuzvY](https://www.youtube.com/watch?v=l1XMRMtuzvY). 

Does anyone have a copy somewhere? Wanted to show it to a friend.",2,1,False,self,,,,,
9,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,12,aw00qd,self.MachineLearning,[P] Simple Tensorflow implementation of AdaBound (ICLR 2019),https://www.reddit.com/r/MachineLearning/comments/aw00qd/p_simple_tensorflow_implementation_of_adabound/,taki0112,1551411163,"&amp;#x200B;

https://i.redd.it/qcnd30ndgfj21.png",2,3,False,self,,,,,
10,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,13,aw0a96,self.MachineLearning,[D] Would predictive jump animations be possible?,https://www.reddit.com/r/MachineLearning/comments/aw0a96/d_would_predictive_jump_animations_be_possible/,Vince_the_Animator,1551412840,"I have no experience what so ever in the field of machine learning, but I was curious, how effective could a machine learning algorithm be at learning and predicting when a player will jump and playing the brief crouch/windup animation, to allow for realistic momentum, just before the player initiates the actual jump? 

So like:

Instead of flying into the air the moment you press the jump button and having a sorta ""half-jump"" animation play while you are leaving the ground.

And instead of pressing the jump button and having to wait for the windup/crouch animation to finish before you can leave the ground.

Can you get a decent success rate at having the game engine play the windup/crouch animation in preparation of your button press, so the animation plays, but you also leave the ground the instant you press the button? ",16,3,False,self,,,,,
11,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,13,aw0ec3,self.MachineLearning,[D] Papers to read to find topics for a new researcher.,https://www.reddit.com/r/MachineLearning/comments/aw0ec3/d_papers_to_read_to_find_topics_for_a_new/,gajeel_ali,1551413557,"Hi, I am trying to select a specific area of machine learning for doing my masters research. But the thing is i am confused to do focus on which topic or sector. So i was looking for some recommendation of recent good papers to study up on to select my topic/passion. 

Point to be noted that i am new to machine learning so papers that are kind of easy to get me into the topics are highly appreciated.",5,0,False,self,,,,,
12,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,13,aw0f2z,self.MachineLearning,Why to pass previous target to current decoder's RNN in encoder-decoder framework?,https://www.reddit.com/r/MachineLearning/comments/aw0f2z/why_to_pass_previous_target_to_current_decoders/,py4_,1551413681,[removed],0,1,False,self,,,,,
13,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,13,aw0t2n,self.MachineLearning,How to get 10 repeats for each fold and the Accuracy curve for each iteration?,https://www.reddit.com/r/MachineLearning/comments/aw0t2n/how_to_get_10_repeats_for_each_fold_and_the/,PriyaPaul,1551416218,"I am trying to do 10 fold cross validation which is repeated 10 times. I can get 10  accuracy values for each fold and if it is repeated 10 times, I can get 100 accuracy values. 

&amp;#x200B;

This is the code I have written value for which I want to get (100s of) TP rate, FP rate, Accuracy and ROC 

&amp;#x200B;

`library(e1071)`

`library(caret)`

`library(gplots)`

`library(ROCR)`

`options(warn=-1)`

`data &lt;- read.csv(""`[`http://dataaspirant.com/wp-content/uploads/2017/01/heart_tidy.csv`](http://dataaspirant.com/wp-content/uploads/2017/01/heart_tidy.csv)`"", sep = ',', header = FALSE)`

`set.seed(678923)`

`yourData&lt;-data[sample(nrow(data)),]`

`a &lt;- 0`



`#Create 10 equally size folds`

`folds &lt;- cut(seq(1,nrow(yourData)),breaks=10,labels=FALSE)`



`#Perform 10 fold cross validation`

`for(i in 1:10)`

`{`

`#Segment data by fold using the which() function` 

`testIndexes &lt;- which(folds==i,arr.ind=TRUE)`

`testing&lt;- yourData[testIndexes, ]`

`training&lt;- yourData[-testIndexes, ]`

`train_control &lt;- trainControl(method = ""repeatedcv"",number = 10, repeats = 10, savePred=T, classProb=T)`

`model &lt;- train(V14 ~ ., data=training, method = ""svmLinear"",trControl = train_control,preProcess = c(""center"", ""scale""),tuneLength = 10,na.action=na.omit)`

`print (model$pred)`

`svm.pred &lt;- predict(model, newdata = testing)`

`c &lt;- as.numeric(svm.pred)`

`c = c - 1`

`pred &lt;- prediction(c, testing[[""V14""]])`

`perf &lt;- performance(pred,""tpr"",""fpr"")`

`auc &lt;- performance(pred, measure = ""auc"")`

`auc &lt;- auc@y.values[[1]]`

`a[i] &lt;- auc`



`}`

`print(a)`

&amp;#x200B;

What I am getting from \`head(model$pred)\`is

&amp;#x200B;

`pred     obs	rowIndex C	Resample`

`1   0.009931844	0	14	     1	Fold01.Rep01`

`2   0.930450173	1	17	     1	Fold01.Rep01`

`3   0.940997589	1	25	     1	Fold01.Rep01`

`4   0.071472394	0	31	     1	Fold01.Rep01`

`5   0.228257854	0	41	     1	Fold01.Rep01`

`.....` 

`27	0.71104954	1	266	     1	Fold01.Rep01`

&amp;#x200B;

&amp;#x200B;

I did not understand why each repeat is repeating 27 times? I want my results like this following

&amp;#x200B;

`Fold1  Repeat1  Accuracy AUC`

`....`

`Fold1  Repeat10  Accuracy AUC`



`Similarly,`



`Fold2  Repeat1  Accuracy AUC`

`....`

`Fold2  Repeat10  Accuracy AUC`



`till Fold10`

&amp;#x200B;

Please suggest me a way to do this and how do I implement it in my above-written code?",0,1,False,self,,,,,
14,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,14,aw0z22,self.MachineLearning,Best introductory textbooks for machine learning,https://www.reddit.com/r/MachineLearning/comments/aw0z22/best_introductory_textbooks_for_machine_learning/,arusse02,1551417290,[removed],0,1,False,self,,,,,
15,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,14,aw16a0,self.MachineLearning,[P] Search images by describing them,https://www.reddit.com/r/MachineLearning/comments/aw16a0/p_search_images_by_describing_them/,invertedpassion,1551418644,"Hello,

I trained a network that generates captions for images and ran it in reverse to find images given a caption. **Here's my writeup:** [One neural network, manyuses](https://towardsdatascience.com/one-neural-network-many-uses-image-captioning-image-search-similar-image-and-words-in-one-model-1e22080ce73d)

**Some examples** (note that the text doesn't exist in dataset, it's user supplied)

For '**a boy smiling**', it returns the following as the first result:

&amp;#x200B;

https://i.redd.it/cyk495cz1gj21.png

For '**a man playing basketball**', it returns the following as the second result (the first one is a man playing american football).

&amp;#x200B;

https://i.redd.it/4zjcpwtc2gj21.png

Hope you like [the project](https://towardsdatascience.com/one-neural-network-many-uses-image-captioning-image-search-similar-image-and-words-in-one-model-1e22080ce73d). While doing the project, I had a ton of fun exploring image and word representations learned by the network while getting trained to generate captions. 

&amp;#x200B;",20,196,False,https://b.thumbs.redditmedia.com/cewu_VlqHRlid_onlZH3yWq8wZAHa5bTAOS8k3Jkdss.jpg,,,,,
16,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,14,aw182b,self.MachineLearning,What am I missing out on if I decide to learn data science + ML in java and not python?,https://www.reddit.com/r/MachineLearning/comments/aw182b/what_am_i_missing_out_on_if_i_decide_to_learn/,pionear,1551418981,[removed],0,1,False,self,,,,,
17,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,14,aw19vo,self.MachineLearning,Looking for learning pathway for someone with a basic high-school education in math.,https://www.reddit.com/r/MachineLearning/comments/aw19vo/looking_for_learning_pathway_for_someone_with_a/,YourDaughtersPussy,1551419339,[removed],0,1,False,self,,,,,
18,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,15,aw1nil,self.MachineLearning,[P] Real time face landmarking using decision trees and NN autoencoders.,https://www.reddit.com/r/MachineLearning/comments/aw1nil/p_real_time_face_landmarking_using_decision_trees/,Lord_Fixer,1551422003,"Repo: [https://github.com/TomaszRewak/Face-Landmarking](https://github.com/TomaszRewak/Face-Landmarking)

Longer description: [https://blog.tomasz-rewak.com/face-landmarking/](https://blog.tomasz-rewak.com/face-landmarking/)

In this project I use simple decision trees and a NN autoencoder to map 194 key face points on a video input in real time.

The algorithms performs 45 iterations each frame. During a single iteration it adjust the position of each point independently (using DT, based on some local features of the image). As points during this process tend to go in slightly random directions, it's essential to fix the mask afterwards. I use a NN autoencoder with 2 hidden layers for this.

The learning process was based on the HELEN dataset.",4,9,False,self,,,,,
19,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,15,aw1u72,youtube.com,AI Generates Fake Donald Trump,https://www.reddit.com/r/MachineLearning/comments/aw1u72/ai_generates_fake_donald_trump/,hanyuqn,1551423429,,0,1,False,https://b.thumbs.redditmedia.com/m3ffLv_GP1tMNdRaB_U6zWvf7kyapWuqjpvBVH4USDg.jpg,,,,,
20,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,16,aw1vfa,self.MachineLearning,Can i get a code for machine learning,https://www.reddit.com/r/MachineLearning/comments/aw1vfa/can_i_get_a_code_for_machine_learning/,sakshipandey1,1551423684,[removed],0,1,False,self,,,,,
21,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,16,aw24dq,self.MachineLearning,Pros &amp; cons of PyTorch?,https://www.reddit.com/r/MachineLearning/comments/aw24dq/pros_cons_of_pytorch/,veeyizzle,1551425591,[removed],0,1,False,self,,,,,
22,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,17,aw2dep,self.MachineLearning,How to Start Learning Deep Learning,https://www.reddit.com/r/MachineLearning/comments/aw2dep/how_to_start_learning_deep_learning/,andrea_manero,1551427736,[removed],0,1,False,self,,,,,
23,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,17,aw2dvr,reddit.com,Applied Machine Learning,https://www.reddit.com/r/MachineLearning/comments/aw2dvr/applied_machine_learning/,Maxinho96,1551427856,,0,1,False,https://b.thumbs.redditmedia.com/g4pUEwH7I9Ck05r3y0o8a_NnY9P-DogKL8bBa3hv71Q.jpg,,,,,
24,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,18,aw2s8t,github.com,An open source AutoML toolkit for neural architecture search and hyper-parameter tuning,https://www.reddit.com/r/MachineLearning/comments/aw2s8t/an_open_source_automl_toolkit_for_neural/,reformed_scientist,1551431377,,0,1,False,default,,,,,
25,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,18,aw33d5,medium.com,Just completed the AI for everyone course. Here are the 30 points that Andrew Ng wants to convey with this Non technical course to you.,https://www.reddit.com/r/MachineLearning/comments/aw33d5/just_completed_the_ai_for_everyone_course_here/,harveenchadha,1551433949,,0,1,False,default,,,,,
26,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,18,aw349g,self.MachineLearning,[D] Is there any algorithm that can find signals between two or more time series trends that correlate to a change in another time series trend,https://www.reddit.com/r/MachineLearning/comments/aw349g/d_is_there_any_algorithm_that_can_find_signals/,JustinQueeber,1551434158,"If I have many time series trends (call them the x values) relating to some other trend (y value), and there is some theory that certain combinations of the values in the x's cause a change to y, is there any model I can train that will then produce more informative, descriptive features of when these occurrences occur.

An example would be Technical Analysis in Stock Market Trading. Can I build a model that will take in many Technical Indicators of a given stock, and learn to produce features from combinations of these indicators that correlate to the stock price changing? It would be useful to train a model to create new features based on how these signals in the technical indicators effect the stock price, and then feed these produced features into another model that would learn to use them to predict the stock price.

Secondly, is all of this unnecessary? If I just fed all of this data into a neural net, for example, it should theoretically learn these signals anyway. However, I just have some intuition that my described method would be more effective as it would force the second model to focus on these signals.",5,1,False,self,,,,,
27,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,19,aw35kq,self.MachineLearning,"Adaptive Robotics Market size, Trend, share, analysis and forecast 2019-2023",https://www.reddit.com/r/MachineLearning/comments/aw35kq/adaptive_robotics_market_size_trend_share/,apple_x9,1551434459,[removed],1,1,False,self,,,,,
28,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,19,aw3dwi,self.MachineLearning,[D] Comparing Deep Learning Workstations,https://www.reddit.com/r/MachineLearning/comments/aw3dwi/d_comparing_deep_learning_workstations/,IborkedyourGPU,1551436368,"Hi,

unfortunately I was assigned to a different product company, and now I have to look around for hardware. I have 0 experience in shopping around for hardware, and I would like to minimize hassle as much as possible, so please bear with my naive questions. 

I was used to training models on a nice DGX-1, nicely setup for me. Now, because of budgetary constraints, I'm looking at these two machines:

https://lambdalabs.com/deep-learning/workstations/4-gpu/premium/customize

https://lambdalabs.com/deep-learning/workstations/4-gpu/premium/customize

I have a few questions:

 1. liquid cooling is a no-brainer, right? 
 2. why does the second one cost so much more than the first one? Are these Titan V so much better than the RTX 2080 Ti? Or is this due to the !0 Gps Ethernet on the second one? If so, I don't think the Ethernet makes any difference for training models. It's only the motherboard bus which matters, and I guess these are all PCI-Express: nothing like the  DGX-1 NV-Link. Am I wrong?
 3. which would youn choose among these two machines? Is the second one worth the 2x price tag? 
 4. these Lambda Labs workstations piqued my interest, because they seem to be nicely preconfigured and all, thus minimizing the effort on my side. However, if you have other suggestions which deliver better value for money, please let me know.
",36,19,False,self,,,,,
29,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,20,aw3z2w,self.MachineLearning,"[Need Help] Machine learning Courses for testing blocks, training blocks and tuning hyper parameters",https://www.reddit.com/r/MachineLearning/comments/aw3z2w/need_help_machine_learning_courses_for_testing/,antonymthoppil,1551440838,[removed],0,1,False,self,,,,,
30,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,21,aw441g,self.MachineLearning,Data Cleaning Course?,https://www.reddit.com/r/MachineLearning/comments/aw441g/data_cleaning_course/,gecicihesap17,1551441828,[removed],0,1,False,self,,,,,
31,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,21,aw493l,enterprisemanagement.com,Automation and Machine Learning for Data Governance: Io-Tahoe Announces New Data Discovery and Catalog Platform,https://www.reddit.com/r/MachineLearning/comments/aw493l/automation_and_machine_learning_for_data/,jacobmarsh789,1551442775,,0,1,False,default,,,,,
32,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,21,aw4i1u,i.redd.it,"""Topology of Learning in Artificial Neural Networks"" Feb 21st 2019 article",https://www.reddit.com/r/MachineLearning/comments/aw4i1u/topology_of_learning_in_artificial_neural/,BrighterAI,1551444459,,0,1,False,https://b.thumbs.redditmedia.com/gYaTHb0xKTzjPuNsk12yv6OSzjPwi13siQkUbvYplaM.jpg,,,,,
33,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,22,aw4r0s,self.MachineLearning,"If we could develop an AI that always calculated the best course of action globally to maximize human happiness while minimising suffering universally (taking into balance all socio-economic &amp; environmental impacts) , would you vote for it and then be prepared to obey it completely?",https://www.reddit.com/r/MachineLearning/comments/aw4r0s/if_we_could_develop_an_ai_that_always_calculated/,Mr-Moore-Lupin-Donor,1551446043,[removed],0,1,False,self,,,,,
34,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,22,aw4rxl,self.MachineLearning,"Help me please! Data analysis and intuition on nootropics, behavior, breathing pattern, brain scanning etc.",https://www.reddit.com/r/MachineLearning/comments/aw4rxl/help_me_please_data_analysis_and_intuition_on/,Anasoori,1551446194,[removed],0,1,False,self,,,,,
35,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,23,aw5avq,self.MachineLearning,"This might be a stupid question, but is it possible (and somewhat realistic) for me to write and publish a paper in a conference by myself as an independent researcher? Will be applying for PhD programs in December, and want to try and get at least one paper on my resume",https://www.reddit.com/r/MachineLearning/comments/aw5avq/this_might_be_a_stupid_question_but_is_it/,gradthrowaway3167,1551449312,[removed],0,1,False,self,,,,,
36,MachineLearning,t5_2r3gv,2019-3-1,2019,3,1,23,aw5s70,apriorit.com,How to Use Google Colaboratory for Video Processing,https://www.reddit.com/r/MachineLearning/comments/aw5s70/how_to_use_google_colaboratory_for_video/,RyanTmthn,1551452078,,0,1,False,default,,,,,
37,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,1,aw6uhz,github.com,AutoML toolkit for neural architecture search and hyper-parameter tuning,https://www.reddit.com/r/MachineLearning/comments/aw6uhz/automl_toolkit_for_neural_architecture_search_and/,j_orshman,1551457966,,0,1,False,https://b.thumbs.redditmedia.com/9hSmvtad85h95GuVmHLi6Tk4MpR-PiJegsl0iWPyiOI.jpg,,,,,
38,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,1,aw6yba,self.MachineLearning,Is there any graph visaulisation tool like tensorboard for Pytorch,https://www.reddit.com/r/MachineLearning/comments/aw6yba/is_there_any_graph_visaulisation_tool_like/,sheatran29,1551458535,,0,1,False,self,,,,,
39,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,1,aw72hn,self.MachineLearning,"How to add gender, age etc. to an image array for neural networks?",https://www.reddit.com/r/MachineLearning/comments/aw72hn/how_to_add_gender_age_etc_to_an_image_array_for/,Scutterbum,1551459168,[removed],0,1,False,self,,,,,
40,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,1,aw72yg,self.MachineLearning,What's the situation of the Information Bottleneck?,https://www.reddit.com/r/MachineLearning/comments/aw72yg/whats_the_situation_of_the_information_bottleneck/,Melthengylf,1551459250,[removed],0,1,False,self,,,,,
41,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,2,aw7avj,github.com,"A simple, framework-agnostic, tool to run lots of experiments for hyperparameter and architecture search",https://www.reddit.com/r/MachineLearning/comments/aw7avj/a_simple_frameworkagnostic_tool_to_run_lots_of/,gizcard,1551460569,,0,1,False,https://b.thumbs.redditmedia.com/q8E1Elr6IH4qu8RQ1I-TFJ-cG3xxlMSBk-shuwGaFOU.jpg,,,,,
42,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,2,aw7axb,self.MachineLearning,LOOKING FOR KAGGLE TEAM MATE,https://www.reddit.com/r/MachineLearning/comments/aw7axb/looking_for_kaggle_team_mate/,krishna_katyal,1551460576,[removed],0,1,False,self,,,,,
43,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,2,aw7nuh,self.MachineLearning,Which university is better for their MSc CS and machine learning program,https://www.reddit.com/r/MachineLearning/comments/aw7nuh/which_university_is_better_for_their_msc_cs_and/,froopylander,1551462968,"I have applied to UBC's MSc Computer Science, Simon Fraser University's MSc Computer Science (Thesis) program, McGillU's MSc Computer Science (Thesis) program and Universit de Montral's MSc Computer Science (internship) + MILA's Professional MSc in Machine Learning programme. I have been admitted to UdeM and MILA, however since professional masters students are not eligible for any form of funding (other than an international tuition fee waiver  of which only 90 are available and hence it might be pretty difficult to get) and since I don't know French (UdeM is a Francophone uni),  I am still contemplating, if any of the other aforementioned universities were to accept me, whether they could possibly be better for MSc CS (AI focus). Would appreciate any advice from this community as to which programme and university could be better and why?",0,1,False,self,,,,,
44,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,3,aw7xa3,self.MachineLearning,An EBay for ML api calls,https://www.reddit.com/r/MachineLearning/comments/aw7xa3/an_ebay_for_ml_api_calls/,glai9665,1551464335,[removed],0,1,False,self,,,,,
45,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,3,aw82x0,self.MachineLearning,[D] Heuristics for choosing architecture of an MLP for RL policy/value estimators?,https://www.reddit.com/r/MachineLearning/comments/aw82x0/d_heuristics_for_choosing_architecture_of_an_mlp/,lantern_lol,1551465187,"I have seen a lot of mixed opinions about what architecture should be used for MLP policy/value estimators (for cases where we do not need CNN features and RNN doesn't help).

I have seen a lot of opinions online which say to use simple architectures where number of units per layer is between units in in/out layers and to only use 1-2 hidden layers; I have however found better performance empirically when using deeper networks (3-5 hidden layers) with more hidden units (between 1-3x size of input). For example, I am currently working on a continuous robotics task (toy reacher problem, 4DOF) and 4 layers consistently outperforms a shallower network.

Are there heuristics for deciding MLP architectures based upon problem, or is it more just ""see what works""?",10,5,False,self,,,,,
46,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,3,aw85oo,self.MachineLearning,Image classifier webapp in React serving from a CNN in Pytorch,https://www.reddit.com/r/MachineLearning/comments/aw85oo/image_classifier_webapp_in_react_serving_from_a/,sdhnshu,1551465613,[removed],0,1,False,self,,,,,
47,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,3,aw862g,self.reinforcementlearning,Has anyone ever interviewed for a position in Prowler.io?,https://www.reddit.com/r/MachineLearning/comments/aw862g/has_anyone_ever_interviewed_for_a_position_in/,schrodingershit,1551465671,,0,1,False,default,,,,,
48,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,4,aw8rdh,self.MachineLearning,How do i apply Batch Normalization to my Network?,https://www.reddit.com/r/MachineLearning/comments/aw8rdh/how_do_i_apply_batch_normalization_to_my_network/,Jandevries101,1551468917,"Hi everyone,

&amp;#x200B;

**Intro**

&amp;#x200B;

So i am working on my DDPG (RL) Algorithme (Discrete) and i came accros a lot of saturation/exploding gradients, i was able to solve it sometimes while tweaking parameters, but on the long run it usually resolves still into saturation. Also notice able about my network is that it has three actions, but it usually only works with 2 actions, i've heard this could be common, but he **never** uses the third action, it's almost random every run, so i can't imagne it has something to do with the action specific. 

&amp;#x200B;

**Environment:** 

&amp;#x200B;

The state is fairly ""simple"", it has a lot of similarities with the frogger game, except he has to carry a package. The main rewards that can be fetched from carrying the package accros the street, if dropped incorrect he gets a punishment. 

&amp;#x200B;

The reward scheme that i am currently using is as follows:

&amp;#x200B;

    --------------------------------------------------
    
    Do Nothing           =  0
    Carry Package        =  0.01
    Drop Fail            = -0.5
    Drop Goal            =  0.5
    StepLimit            = -1 (50 Steps)
    Pick-Up Package      = 0
        
    --------------------------------------------------

&amp;#x200B;

**Network** 

&amp;#x200B;

Under here you can see the Actor Critic network that i am using right now, my state-space is 6 and is normalized and i used to have 64-33-3 for the neurons per layer in the actor, but i was testing doubling it, so don't keep the number of neurons too much in count.

&amp;#x200B;

Parameters:

    MAX_EPISODES = 200
    MAX_EP_STEPS = 200
    LR_A = 0.0001    # learning rate for actor
    LR_C = 0.0002    # learning rate for critic
    GAMMA = 0.9     # reward discount
    TAU = 0.01      # soft replacement
    MEMORY_CAPACITY = 1000 #usually use 500, but i was testing, so i doubled it
    BATCH_SIZE = 32

Actor:

        def _build_a(self, s, reuse=None, custom_getter=None):
            trainable = True if reuse is None else False
            with tf.variable_scope('Actor', reuse=reuse, custom_getter=custom_getter):
                net = tf.layers.dense(s, 140, activation=tf.nn.leaky_relu, name='l1', trainable=trainable)					
                mid = tf.layers.dense(net, 70, activation=tf.nn.tanh, name='l2', trainable=trainable)
                a = tf.layers.dense(mid, 3, activation=tf.nn.softmax, name='la', trainable=trainable)
                return a

Critic:

        def _build_c(self, s, a, reuse=None, custom_getter=None):
            trainable = True if reuse is None else False
            with tf.variable_scope('Critic', reuse=reuse, custom_getter=custom_getter):
                n_l1 = s.shape[1]
                w1_s = tf.get_variable('w1_s', [s.get_shape()[1], n_l1], trainable=trainable)
                w1_a = tf.get_variable('w1_a', [3, n_l1], trainable=trainable)
                b1 = tf.get_variable('b1', [1, n_l1], trainable=trainable)
                net = tf.nn.relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)
                return tf.layers.dense(net, 1, trainable=trainable)  # Q(s,a)  

&amp;#x200B;

**Batch Normalization?**

&amp;#x200B;

So i got reccomended to use Batch Normalization, however i do get what it does, but i don't know if this will indeed solve my 2 problems of saturation and the softmax only using 2 actions instead of making use of the three actions. A results graph of action probabilities for the three actions, however i only did a thousand steps in this run, due too time issues.

&amp;#x200B;

[https://cdn.discordapp.com/attachments/491949654666248213/550428445386080256/unknown.png](https://cdn.discordapp.com/attachments/491949654666248213/550428445386080256/unknown.png)

&amp;#x200B;

It's only to give a image of how it looks sometimes. For the rest is my td-error fairly normal i suppose:

&amp;#x200B;

[https://cdn.discordapp.com/attachments/491949654666248213/550713201650565128/unknown.png](https://cdn.discordapp.com/attachments/491949654666248213/550713201650565128/unknown.png)

&amp;#x200B;

So i guess i wanna use batch normalization, since i don't think gradient clipping will help.... but i have no idea how batch normalization can be implemented in my algorithme.... I know i have to define some things in my init, my learn function and of course my network function, but i don't get what and how, besides i am not even sure on which layers i should apply it, i suppose it should be on the leaky\_relu? And my last question is how long does it take on average for batch normalization to be ""sane"" ?

&amp;#x200B;

**Outro**

&amp;#x200B;

Thanks for answering my question and reading, i hope i explained everything well enough and maybe you know the answer to my questions and maybe even have diffrent suggestions to approach this problem. I am curious to know!

&amp;#x200B;

Have a nice day further,

&amp;#x200B;

Jan

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
49,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,5,aw92qe,self.MachineLearning,Online face clustering,https://www.reddit.com/r/MachineLearning/comments/aw92qe/online_face_clustering/,dascsad,1551470689,[removed],0,1,False,self,,,,,
50,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,5,aw966i,self.MachineLearning,[D] How to choose metrics for evaluating classification results?,https://www.reddit.com/r/MachineLearning/comments/aw966i/d_how_to_choose_metrics_for_evaluating/,alirezazolanvari,1551471225,"Hi,

Recently we have developed a python library named  [PyCM](https://github.com/sepandhaghighi/pycm) specialized for analyzing multi-class confusion matrices.  A parameter recommender system has been added in [version 1.9](https://www.reddit.com/r/MachineLearning/comments/av1ktu/p_confusion_matrix_analysis_parameter_recommender/?utm_source=share&amp;utm_medium=mweb&amp;_branch_match_id=535448659213137270) of this module in order to recommend most related parameters considering the characteristics of the input dataset and its classification problem.

This new option is very challenging and raising many questions. At first, I try to explain the assumptions and describe how this module works in this part. After that, some questions are going to be asked for evaluating the performance of this recommender system.

&amp;#x200B;

**Considered characteristics:**

The characteristics according to which the parameters are suggested are as following:

1. Classification problem type (binary or multi-class)
2. Dataset type (balanced or imbalanced)

It should be noticed that in the case that the problem is either a binary or a multi-class classification on an imbalanced dataset, for recommending the parameters, just being imbalanced is considered. Therefore, the inspected states can be categorized into three main groups:

1. Balanced dataset  Binary classification
2. Balanced dataset  Multi-class classification
3. Imbalanced dataset

&amp;#x200B;

**The definition of being imbalanced:**

Recognizing the fact that a classification problem is binary or multi-class is so easy. But the margin between being balanced or imbalanced for a dataset is not clear. In PyCM module for checking if the input dataset is balanced or not, a definition has been introduced. According to this definition, if the ratio of the population of the most populous class to the population of the most deserted class is bigger than 3, the dataset is assumed imbalanced.

&amp;#x200B;

**Recommended parameters:**

The recommendation lists have been gathered according to the respective paper of each parameter and the capabilities which had been claimed by the paper. For further information, read the [document of PyCM](http://www.pycm.ir/doc/).

* Binary  Balanced recommended parameters: `ACC, TPR, PPV, AUC, AUCI, TNR, F1`

&amp;#x200B;

* Multi-class  Balanced recommended parameters: `ERR, TPR Micro, TPR Macro, PPV Micro, PPV Macro, ACC, Overall ACC, MCC, Overall MCC, BCD, Hamming Loss, Zero-one Loss`

&amp;#x200B;

* Imbalanced recommended parameters: `Kappa, SOA1(Landis &amp; Koch), SOA2(Fleiss), SOA3(Altman), SOA4(Cicchetti), CEN, MCEN, MCC, J, Overall J, Overall MCC, Overall CEN, Overall MCEN, AUC, AUCI, G, DP, DPI, GI`

&amp;#x200B;

**Questions:**

1. Is the proposed definition of being imbalanced correct? Is there any more comprehensive definition for this characteristic?
2. Is recommending the same parameters for both binary and multi-class classification problem correct over imbalanced dataset?
3. Are the recommendation parameter lists correct and complete? Is there any other parameter for recommending?
4. Is there any other characteristics (like binary/multi-class and balanced/imbalanced) which can effect on evaluating the result of a classification method?",7,14,False,self,,,,,
51,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,5,aw98cb,self.MachineLearning,Model for removing liscense plate,https://www.reddit.com/r/MachineLearning/comments/aw98cb/model_for_removing_liscense_plate/,wsebos,1551471570,[removed],0,1,False,self,,,,,
52,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,5,aw9adv,ua-magazine.com,The next generation of intelligent gardening robots,https://www.reddit.com/r/MachineLearning/comments/aw9adv/the_next_generation_of_intelligent_gardening/,UnitedAcademics,1551471892,,0,1,False,default,,,,,
53,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,5,aw9kvh,arxiv.org,[1902.11136] Learning Dynamical Systems from Partial Observations,https://www.reddit.com/r/MachineLearning/comments/aw9kvh/190211136_learning_dynamical_systems_from_partial/,zhamisen,1551473541,,17,117,False,default,,,,,
54,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,7,awafmj,self.MachineLearning,Manjaro xfce vs Ubuntu 18.04 LTS,https://www.reddit.com/r/MachineLearning/comments/awafmj/manjaro_xfce_vs_ubuntu_1804_lts/,mgavaudan,1551478397,"Hi,

I've heard a lot of great things about both distros, but when it comes to machine learning and accessing a desktop remotely, which OS do you think works best?

Thanks!",0,1,False,self,,,,,
55,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,8,awbbsa,self.MachineLearning,Building My First Deep Learning PC,https://www.reddit.com/r/MachineLearning/comments/awbbsa/building_my_first_deep_learning_pc/,ajaychainani,1551483794,[removed],0,1,False,self,,,,,
56,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,8,awbd9i,mljobslist.com,[P] Machine learning job board,https://www.reddit.com/r/MachineLearning/comments/awbd9i/p_machine_learning_job_board/,gauthamz,1551484033,,0,3,False,default,,,,,
57,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,9,awbhxw,self.MachineLearning,Help to identify correct algorithm,https://www.reddit.com/r/MachineLearning/comments/awbhxw/help_to_identify_correct_algorithm/,Lithium_Knight,1551484853,[removed],0,1,False,self,,,,,
58,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,9,awbj0o,self.MachineLearning,Machine learning applications in the future job market,https://www.reddit.com/r/MachineLearning/comments/awbj0o/machine_learning_applications_in_the_future_job/,MKUltima,1551485053,"Hey! Im a junior in high school and when considering my options for college and onward, something with computers is always top of the list. So I was wondering, when it comes to careers now or sometime down the line, how prevalent are MA based jobs and what kind of schooling/degree would you guys recommend? Also I love this subreddit and find the topic super interesting!",0,1,False,self,,,,,
59,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,9,awbohk,self.MachineLearning,"How I created over 100,000 labeled training images of real LEGO bricks",https://www.reddit.com/r/MachineLearning/comments/awbohk/how_i_created_over_100000_labeled_training_images/,sqiddster,1551486015,[removed],0,1,False,https://b.thumbs.redditmedia.com/q0wbl9xa--PU49sDB1AYuHJL4kO2x8rpc16zf1uRsRI.jpg,,,,,
60,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,10,awc3ri,self.MachineLearning,[R] Learning Implicitly Recurrent CNNs Through Parameter Sharing (ICLR'19) + Code,https://www.reddit.com/r/MachineLearning/comments/awc3ri/r_learning_implicitly_recurrent_cnns_through/,neural_kusp_machine,1551488827,"Paper: [https://arxiv.org/abs/1902.09701](https://arxiv.org/abs/1902.09701)

Code: [https://github.com/lolemacs/soft-sharing](https://github.com/lolemacs/soft-sharing)

&amp;#x200B;

Abstract:

We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are defined by a learned linear combination of parameter tensors from a global bank of templates. Restricting the number of templates yields a flexible hybridization of traditional CNNs and recurrent networks. Compared to traditional CNNs, we demonstrate substantial parameter savings on standard image classification tasks, while maintaining accuracy.   
Our simple parameter sharing scheme, though defined via soft weights, in practice often yields trained networks with near strict recurrent structure; with negligible side effects, they convert into networks with actual loops. Training these networks thus implicitly involves discovery of suitable recurrent architectures. Though considering only the design aspect of recurrent links, our trained networks achieve accuracy competitive with those built using state-of-the-art neural architecture search (NAS) procedures.   
Our hybridization of recurrent and convolutional networks may also represent a beneficial architectural bias. Specifically, on synthetic tasks which are algorithmic in nature, our hybrid networks both train faster and extrapolate better to test examples outside the span of the training set.",9,33,False,self,,,,,
61,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,10,awceeo,github.com,[D] QuantStack/ipysheet,https://www.reddit.com/r/MachineLearning/comments/awceeo/d_quantstackipysheet/,_quanttrader_,1551490878,,0,1,False,https://b.thumbs.redditmedia.com/mdn3YzgEhxNElvv_v8f0e_fCBvT-UJUyPAylOvCFzLk.jpg,,,,,
62,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,12,awd416,go.geeklearn.net,Machine learning with Python: An introduction,https://www.reddit.com/r/MachineLearning/comments/awd416/machine_learning_with_python_an_introduction/,MarkOliver908,1551495899,,1,1,False,default,,,,,
63,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,12,awdjye,link.medium.com,Loc2vec - a mixed precision pytorch implementation,https://www.reddit.com/r/MachineLearning/comments/awdjye/loc2vec_a_mixed_precision_pytorch_implementation/,sr511,1551498839,,1,1,False,default,,,,,
64,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,13,awdm5q,self.MachineLearning,Machine learning idea?,https://www.reddit.com/r/MachineLearning/comments/awdm5q/machine_learning_idea/,MikelFury,1551499246,[removed],0,1,False,self,,,,,
65,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,13,awdryd,self.MachineLearning,[P] NLP Made Easy: Simple code notes for explaining NLP building blocks,https://www.reddit.com/r/MachineLearning/comments/awdryd/p_nlp_made_easy_simple_code_notes_for_explaining/,longinglove,1551500288,"Simple code notes for explaining NLP building blocks

&amp;#x200B;

[https://github.com/Kyubyong/nlp\_made\_easy](https://github.com/Kyubyong/nlp_made_easy)",3,111,False,self,,,,,
66,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,13,awdx01,self.MachineLearning,Question about Smoothness of Latent Space and Input Space for generative models in Text vs Images,https://www.reddit.com/r/MachineLearning/comments/awdx01/question_about_smoothness_of_latent_space_and/,vikigenius,1551501204,[removed],0,1,False,self,,,,,
67,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,14,awe6fu,self.MachineLearning,[Discussion] Smoothness of the Latent Space and Input Space for Generative Models: Image vs Text,https://www.reddit.com/r/MachineLearning/comments/awe6fu/discussion_smoothness_of_the_latent_space_and/,vikigenius,1551503142,"Hi all. I was reading an interesting paper today [Curvature of Deep Generative Models](https://arxiv.org/pdf/1710.11379.pdf).

The paper talks about the inherent curvature of the latent space learned by models like VAE. It seems very interesting. I can understand the smoothness arguments they make. This seems fine for Image generation. With a smooth activation you could claim that the transformation function onto the input space is smooth.

But what happens in case of text? I don't see how the smoothness arguments apply anymore if the input space is discrete. The paper doesn't seem to talk about text generation at all. So I am guessing that this curvature of Latent Space is not applicable to text generation tasks? So there is no point in using a Riemannian Metric for text generation tasks?",2,9,False,self,,,,,
68,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,14,awedur,self.MachineLearning,Finding Global minimum using local minima,https://www.reddit.com/r/MachineLearning/comments/awedur/finding_global_minimum_using_local_minima/,thisHermit,1551504679,[removed],0,1,False,self,,,,,
69,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,15,awev9r,self.MachineLearning,[P] PyTorch implementation of GOTURN single object tracker,https://www.reddit.com/r/MachineLearning/comments/awev9r/p_pytorch_implementation_of_goturn_single_object/,amoudgl,1551508407,[removed],0,1,False,self,,,,,
70,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,15,awewmm,towardsdatascience.com,Can I get rich?,https://www.reddit.com/r/MachineLearning/comments/awewmm/can_i_get_rich/,gifted-,1551508708,,0,1,False,default,,,,,
71,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,15,awex6n,blog.openmined.org,Deep Learning &amp; Federated Learning in 10 Lines of PyTorch + PySyft,https://www.reddit.com/r/MachineLearning/comments/awex6n/deep_learning_federated_learning_in_10_lines_of/,ehsanul,1551508838,,0,1,False,default,,,,,
72,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,16,awf6mp,self.MachineLearning,deeplearning.ai Specialization Progress | Shaik Asad,https://www.reddit.com/r/MachineLearning/comments/awf6mp/deeplearningai_specialization_progress_shaik_asad/,theshaikasad,1551510952,[removed],0,1,False,self,,,,,
73,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,16,awfgox,self.MachineLearning,"[D] Training set imbalanced, should my validation set also be ?",https://www.reddit.com/r/MachineLearning/comments/awfgox/d_training_set_imbalanced_should_my_validation/,natalienatnat,1551513369,"I use linear SVM for binary classification with CV 

My training dataset is imbalanced (218 data from class 1 vs 10900 from class 2) - I implemented SMOTE on the minority class to have a balanced data. I use artificial observations generated from SMOTE for train only (not for validation).

But should my validation set be imbalanced to be more representative of the distribution of the classes?
",26,19,False,self,,,,,
74,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,17,awfrop,self.MachineLearning,"Linear Algebra, Signal Processing, And Wavelets  A Unified Approach: Python Version",https://www.reddit.com/r/MachineLearning/comments/awfrop/linear_algebra_signal_processing_and_wavelets_a/,mritraloi6789,1551516065,[removed],0,1,False,self,,,,,
75,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,19,awgb1p,self.MachineLearning,difference between VAE(Varational AutoEncoder) and AAE (Adversarial AutoEncoder),https://www.reddit.com/r/MachineLearning/comments/awgb1p/difference_between_vaevarational_autoencoder_and/,dannybbbb,1551520936,[removed],0,1,False,self,,,,,
76,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,20,awh0q0,self.MachineLearning,Text Detection in Document Images: Highlight on using FAST algorithm,https://www.reddit.com/r/MachineLearning/comments/awh0q0/text_detection_in_document_images_highlight_on/,DGs29,1551527307,[removed],1,1,False,self,,,,,
77,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,21,awh4ny,self.MachineLearning,Longitivity of machine learning and ai,https://www.reddit.com/r/MachineLearning/comments/awh4ny/longitivity_of_machine_learning_and_ai/,SaThorat,1551528218,[removed],0,1,False,self,,,,,
78,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,22,awhujs,knowaboutdata.com,Top 5 Programming Language for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/awhujs/top_5_programming_language_for_machine_learning/,findtoknow,1551533949,,0,1,False,default,,,,,
79,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,23,awi1fi,self.MachineLearning,Machine Learning and stock market,https://www.reddit.com/r/MachineLearning/comments/awi1fi/machine_learning_and_stock_market/,vatsal50,1551535319,[removed],0,1,False,self,,,,,
80,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,23,awi5qr,self.MachineLearning,Machine Learning In Medicine  A Complete Overview,https://www.reddit.com/r/MachineLearning/comments/awi5qr/machine_learning_in_medicine_a_complete_overview/,mritraloi6789,1551536178,[removed],0,1,False,self,,,,,
81,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,23,awi82d,tutorials.retopall.com,Feed-Forward Neural Network,https://www.reddit.com/r/MachineLearning/comments/awi82d/feedforward_neural_network/,DevTechRetopall,1551536638,,0,1,False,default,,,,,
82,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,23,awie9s,bayeswatch.com,BayesWatch - machine learning research group at the University of Edinburgh,https://www.reddit.com/r/MachineLearning/comments/awie9s/bayeswatch_machine_learning_research_group_at_the/,dezzion,1551537835,,0,1,False,default,,,,,
83,MachineLearning,t5_2r3gv,2019-3-2,2019,3,2,23,awig8h,arxiv.org,[R] How do Mixture Density RNNs Predict the Future?,https://www.reddit.com/r/MachineLearning/comments/awig8h/r_how_do_mixture_density_rnns_predict_the_future/,baylearn,1551538209,,7,87,False,default,,,,,
84,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,1,awj3yz,self.MachineLearning,Transfer Learning in PyTorch,https://www.reddit.com/r/MachineLearning/comments/awj3yz/transfer_learning_in_pytorch/,imHarin,1551542445,[removed],0,1,False,self,,,,,
85,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,1,awj50j,vimeo.com,Update Core ML models with drag and drop.,https://www.reddit.com/r/MachineLearning/comments/awj50j/update_core_ml_models_with_drag_and_drop/,heybluez,1551542604,,0,1,False,https://b.thumbs.redditmedia.com/2XX_ZaYvxRjyivPWZijOguow4vuTTpjTesTGGm9t5bk.jpg,,,,,
86,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,1,awjnds,self.MachineLearning,Python Programming For Biology: Bioinformatics And Beyond,https://www.reddit.com/r/MachineLearning/comments/awjnds/python_programming_for_biology_bioinformatics_and/,mritraloi6789,1551545613,[removed],0,1,False,self,,,,,
87,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,2,awjvl4,self.MachineLearning,Using Experience Replay with Deep Q when multiple inputs are used (Sorry Noob Post),https://www.reddit.com/r/MachineLearning/comments/awjvl4/using_experience_replay_with_deep_q_when_multiple/,ryanon4,1551546939,[removed],0,1,False,self,,,,,
88,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,2,awk9re,self.MachineLearning,Python Collaborative Filtering on Android,https://www.reddit.com/r/MachineLearning/comments/awk9re/python_collaborative_filtering_on_android/,arjybarji,1551549133,"My final year project involves implementing a Recommender System in an Android Application. Currently, I have the App set up and a series of RecSys implemented in Python.

My project uses the MovieLens database however I need a server where I can store the trained model and interact with the Android Application via HTTP requests to obtain predictions.

Does anyone have any tutorials/software recommendations for me to do this task, since I am not sure how I would be able to go about this",0,1,False,self,,,,,
89,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,3,awkiqm,self.MachineLearning,Classification of unseen labels? How to predict if subscriber will watch future tv program?,https://www.reddit.com/r/MachineLearning/comments/awkiqm/classification_of_unseen_labels_how_to_predict_if/,monasch,1551550480,[removed],0,1,False,self,,,,,
90,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,4,awl7ob,self.MachineLearning,What is the role of train/validation/test dataset in the neural network,https://www.reddit.com/r/MachineLearning/comments/awl7ob/what_is_the_role_of_trainvalidationtest_dataset/,kevinaiworld,1551554363,[removed],0,1,False,self,,,,,
91,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,4,awlkk2,self.MachineLearning,Deep Learning with label noise,https://www.reddit.com/r/MachineLearning/comments/awlkk2/deep_learning_with_label_noise/,face_of_b0e,1551556360,[removed],0,1,False,self,,,,,
92,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,4,awlm4m,self.MachineLearning,[Q] Is there a machine learning conference that only focus on machine learning infra?,https://www.reddit.com/r/MachineLearning/comments/awlm4m/q_is_there_a_machine_learning_conference_that/,marksteve4,1551556611,[removed],0,1,False,self,,,,,
93,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,5,awlvgq,self.MachineLearning,NN - place students in classroom in the most optimal way possible,https://www.reddit.com/r/MachineLearning/comments/awlvgq/nn_place_students_in_classroom_in_the_most/,shachar1000,1551558062,"Let's say I have a 3D matrix with the:

1. relation of every student with every other student from 1 (likes) to 0 (hates).
2. amount of noise the combination of every student with every other student will generate and interrupt the teacher.

so assuming there are 10 students the matrix will be 2\*8\*8.

Now let's assume the desks are for 2 people and the parameters listed above should only be taken into consideration in the scope of the desk (so for example does student X sitting in the same desk as student Y likes him etc.), how would I train a neural network to output the most efficient and optimal way to organize the class? 

&amp;#x200B;

Also, what if I want to use parameters that are not related to other students, so for example if the student needs to sit close because he doesn't see good?

&amp;#x200B;

Thanks.",0,1,False,self,,,,,
94,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,5,awlwe2,i.redd.it,"SXSW Machine Learning Hackathon (Austin, TX) - Sign Up Deadline March 4th",https://www.reddit.com/r/MachineLearning/comments/awlwe2/sxsw_machine_learning_hackathon_austin_tx_sign_up/,spaceboimike,1551558209,,0,1,False,https://b.thumbs.redditmedia.com/Y079IT3evCkqPeCA0fthaTShql_ioiISggPhHgC-TGw.jpg,,,,,
95,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,5,awm1fq,self.MachineLearning,"SXSW Machine Learning Hackathon (Austin, TX) - University Teams Apply Online",https://www.reddit.com/r/MachineLearning/comments/awm1fq/sxsw_machine_learning_hackathon_austin_tx/,spaceboimike,1551558983,[removed],0,1,False,self,,,,,
96,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,6,awmbmh,blockdelta.io,Can AI positively impact the Recruitment Industry? - BlockDelta,https://www.reddit.com/r/MachineLearning/comments/awmbmh/can_ai_positively_impact_the_recruitment_industry/,BlockDelta,1551560506,,0,1,False,default,,,,,
97,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,6,awmice,self.MachineLearning,"How long are we from: Voice Style Transfer | Voice to Voice, Male to Female, Adding and Removing Accents, &amp; Swapping Vocalists in Music",https://www.reddit.com/r/MachineLearning/comments/awmice/how_long_are_we_from_voice_style_transfer_voice/,Yuli-Ban,1551561542,[removed],0,1,False,self,,,,,
98,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,6,awmjm8,self.MachineLearning,"[D] How long are we from: Voice Style Transfer | Voice to Voice, Male to Female, Adding and Removing Accents, &amp; Swapping Vocalists in Music",https://www.reddit.com/r/MachineLearning/comments/awmjm8/d_how_long_are_we_from_voice_style_transfer_voice/,Yuli-Ban,1551561745,"I'm coming over from /r/MediaSynthesis with the titular question. I'm well aware of previous experiments, but I'm eagerly awaiting future developments in this field of media manipulation. 

I've played around with sex-changing voice changers in the past, and the common limitation among all of them is that there is nothing being done besides raising or dropping the pitch, and this doesn't lead to a believable effect since gendered speaking patterns exist in most societies. Without accounting for differences in cadences, you merely wind up with voices that sound like chipmunks or homosexual demons. This requires neural networks, but I haven't found many good ones.

In comes GANs. What's more, GANs might also allow for some creative applications, such as [musical style transfer](https://www.youtube.com/watch?v=YQAupr7JxNY). My go-to theoretical examples are ""TLC's *Waterfalls*, but as a barbershop quartet"", The Beatles' *I Am The Walrus*, but as an opera"", and ""Black Sabbath's *Iron Man*, but with Justin Bieber"".

Even 2 years ago, I'd have said this was many decades out, but now I'm not so sure. I feel I could say ""We'll see something like this by 2029"" and then someone demonstrates the exact same thing within 6 months. I say this because it's exactly what happened with OpenAI's text synthesis a couple weeks ago. I said to someone ""Someone might create a short, coherent story via AI by 2025 or so."" Valentine's Day 2019 rolled around, and...

So what are your predictions on this front? When will I be able to generate a barbershop quartet version of *Waterfalls* and swap Trump's voice for Kim Kardashian's?",58,148,False,self,,,,,
99,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,6,awmlmw,self.MachineLearning,Future prospects of different DOmains in ML/DL,https://www.reddit.com/r/MachineLearning/comments/awmlmw/future_prospects_of_different_domains_in_mldl/,NecroDeity,1551562063,"I consider myself as a beginner in ML/DL. I wish tu pursue a career in the industry, applying latest tech to solve problems, not researching new tech (basically I don't plan on being a researcher, I prefer an application based job).  I would like to know about the different domains under ML/DL, and how financially lucrative they are (although that is not the sole criteria I am looking for in a domain, but is an important one. Basically I want a job that giveds me financial independence to purse my other passions in life, say travelling to different parts of the world).  


This has occupied my mind for some time now. Any help is appreciated.",0,1,False,self,,,,,
100,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,6,awmpue,self.MachineLearning,Has anyone implemented dynamic batch size during training using the gradient noise scale?,https://www.reddit.com/r/MachineLearning/comments/awmpue/has_anyone_implemented_dynamic_batch_size_during/,gonzales82,1551562734,"Came across it here: https://blog.openai.com/science-of-ai/

Seems like you could adjust the batch size using what they call ""critical batch size"" as the heuristic. If I understand correctly it would be best to start with a small batch and increase it gradually during training.",0,1,False,self,,,,,
101,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,7,awn3c0,self.MachineLearning,JupyterBooks - index of Jupyter ML Notebooks to encourage easy interaction with state of the art projects,https://www.reddit.com/r/MachineLearning/comments/awn3c0/jupyterbooks_index_of_jupyter_ml_notebooks_to/,toastednz,1551564919,[removed],0,1,False,self,,,,,
102,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,7,awn7pf,self.MachineLearning,[P] JupyterBooks - index of Jupyter ML Notebooks to inspire others,https://www.reddit.com/r/MachineLearning/comments/awn7pf/p_jupyterbooks_index_of_jupyter_ml_notebooks_to/,toastednz,1551565666,"[https://jupyterbooks.com/](https://jupyterbooks.com/)

Please add or suggest interesting notebooks to add to this index!

Prior to Jupyter notebooks, I found running other peoples code often/usually ended with some annoying error which I had no idea how to fix, usually resulting in me giving up. 

I found Jupyter notebooks to be a revelation, especially in the GPU included Colab environment!

I hope this collection will inspire others in this field!",3,62,False,self,,,,,
103,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,7,awnb89,self.MachineLearning,"How do they make AI that doesn't just predict, but is creative?",https://www.reddit.com/r/MachineLearning/comments/awnb89/how_do_they_make_ai_that_doesnt_just_predict_but/,NarawaGames,1551566250,"How does a machine learning algorithm do more than predict, classify, and detectlike make music, generate images, and write sentences?",0,1,False,self,,,,,
104,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,8,awnu8v,self.MachineLearning,[D] Training long sequences with CTC in RNNs. Is there a mistake in this paper?,https://www.reddit.com/r/MachineLearning/comments/awnu8v/d_training_long_sequences_with_ctc_in_rnns_is/,throwohhaimark2,1551569423,"[This paper](http://proceedings.mlr.press/v48/hwanga16.pdf&amp;ved=2ahUKEwj9hcHKxOTgAhUco4MKHdz4AvgQFjAAegQIAxAB&amp;usg=AOvVaw0t8V7zDJhX8i7wlIBcSApw) proposes a way to train long sequences in RNNs with CTC, even though CTC requires you to have seen the whole sequence first. To summarize, you split a long input sequence (like a long speech recording) into several chunks. Then you do a modified version of CTC called CTC-EM on each chunk except for the last, which you do a truncated version of CTC on. 

What I don't understand is that in section 3.4 describing CTC-EM, the idea is to find a partial labeling. So like if the actual labeling was a paragraph, the partial labelings would be all substrings of that paragraph starting from 1. The issue is that when you repeat CTC on chunks 2, 3, 4, etc., you don't necessarily want partial labelings starting from 1. The ground truth for a chunk from the middle might be a labeling from the middle. 

Do you think I'm misunderstanding something or is there a mistake?",2,7,False,self,,,,,
105,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,8,awnzfn,self.MachineLearning,CVPR 2019 Decisions are out.,https://www.reddit.com/r/MachineLearning/comments/awnzfn/cvpr_2019_decisions_are_out/,junsukchoe,1551570288,[removed],0,1,False,self,,,,,
106,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,9,awola4,news18.com,Machine Learning Can Identify Kids Suffering From Developmental Disorder Due to Alcohol Exposure in Womb - News18,https://www.reddit.com/r/MachineLearning/comments/awola4/machine_learning_can_identify_kids_suffering_from/,entryoxidizes,1551574022,,0,1,False,https://b.thumbs.redditmedia.com/lkXjYXIdcsj2aOrbD7e63CsIapXfPphcNkA2J0zcVak.jpg,,,,,
107,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,11,awpftw,self.MachineLearning,For all of you who started studying Computer Vision with CS231n (2018 ver.),https://www.reddit.com/r/MachineLearning/comments/awpftw/for_all_of_you_who_started_studying_computer/,justinjhjung,1551579756,"I've been working with CS231n assignments and finished attending all of the lectures. 

Sometimes I wanted to stop solving problems in the assignments. 

While doing this, one motivation kept me running: I want to contribute to the whole community of people who want to learn and utilize the art of deep learning. 

&amp;#x200B;

I want to share my code and additional materials that can help those who started learning Computer Vision with CS231n. 

&amp;#x200B;

My github repository includes as follows...

1) Errors and solutions that people might face when doing assignments

2) Highlights on the CS231n lecture notes (with LINER)

3) Codes with line by line comments

&amp;#x200B;

==&gt; [https://github.com/justinjhjung/CS231n-solutions-2018](https://github.com/justinjhjung/CS231n-solutions-2018)

&amp;#x200B;

Thanks a lot guys. I've got a big help from the community without you guys, I've never ever have finished the lectures and assignments. My codes may not be helpful for those who are already a good programmer, but for those who just started python programming might find useful. 

&amp;#x200B;

I will enhance my skillsets and in a near future contribute to the open source project!! 

Cheers!",0,1,False,self,,,,,
108,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,11,awpocn,hackernoon.com,Machine Learning for Grandmothers  Hacker Noon,https://www.reddit.com/r/MachineLearning/comments/awpocn/machine_learning_for_grandmothers_hacker_noon/,bit-man,1551581402,,0,1,False,default,,,,,
109,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,12,awpy7q,reddit.com,Best resources for variational Inference And bayesian deep learning?,https://www.reddit.com/r/MachineLearning/comments/awpy7q/best_resources_for_variational_inference_and/,surya-k,1551583355,,0,1,False,default,,,,,
110,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,13,awqb5w,self.MachineLearning,Outlier detection,https://www.reddit.com/r/MachineLearning/comments/awqb5w/outlier_detection/,Din2012,1551585971,"  

I have a daily dataset with Product Name and Sales amount (in CSV  format) This data is for last 1 year. I need to find the medium of the sales amount for each product and  create a Model with Product Name and medium value for the sales amount

Now when I  give today's data with Product Name and Sales amount, the  model should give me product names whose sales amount varies from +/-- 10% of the medium value from the model.",0,1,False,self,,,,,
111,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,13,awqemv,i.redd.it,"Anaconda connection blocked. Everytime i open Jupyter notebook, i am getting an error Connection Blocked. Why and how to resolve it ?",https://www.reddit.com/r/MachineLearning/comments/awqemv/anaconda_connection_blocked_everytime_i_open/,mayurat22,1551586707,,0,1,False,default,,,,,
112,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,13,awqfii,self.MachineLearning,Any tutorial recommendation for sentiment analysis using LSTMs with attention in PyTorch,https://www.reddit.com/r/MachineLearning/comments/awqfii/any_tutorial_recommendation_for_sentiment/,MrAaronW,1551586888,[removed],0,1,False,self,,,,,
113,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,14,awqz33,self.MachineLearning,UC Irvine vs. Caltech vs. JHU for a PhD in CS (ML)?,https://www.reddit.com/r/MachineLearning/comments/awqz33/uc_irvine_vs_caltech_vs_jhu_for_a_phd_in_cs_ml/,ijkml,1551590984,[removed],1,1,False,self,,,,,
114,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,14,awr1ow,self.MachineLearning,"[P] ""Is this banana ripe?"": data, code, trained model and web-ui for inference using tf.js",https://www.reddit.com/r/MachineLearning/comments/awr1ow/p_is_this_banana_ripe_data_code_trained_model_and/,gpcarv,1551591532,[removed],6,20,False,self,,,,,
115,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,14,awr6qr,self.MachineLearning,[D] Python code for Text Detection in document images using Fast Algorithm,https://www.reddit.com/r/MachineLearning/comments/awr6qr/d_python_code_for_text_detection_in_document/,DGs29,1551592700,I am currently working on implementing [Text Detection in Document Images: Highlight on using FAST algorithm](https://www.researchgate.net/publication/315954607_Text_Detection_in_Document_Images_Highlight_on_using_FAST_algorithm). I couldn't find any opensource implementation for this paper. Is this implemented by anyone?,10,24,False,self,,,,,
116,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,15,awra3t,self.MachineLearning,[D] Feature Engineering Hack: How to handle cyclical features. e.g. correctly cluster (or take the average of the time for) events that happen at 23:59 and 00:01.,https://www.reddit.com/r/MachineLearning/comments/awra3t/d_feature_engineering_hack_how_to_handle_cyclical/,avishalom,1551593465,"TL,DR: Use the X, Y coordinates of end the hour hand. (on a 24 hour clock)  
Introduced some people to this trick they liked it, I expanded .  
[https://medium.com/@avishalom/feature-engineering-time-3934038e0dbe](https://medium.com/@avishalom/feature-engineering-time-3934038e0dbe)  
",15,39,False,self,,,,,
117,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,15,awrdtt,self.MachineLearning,Translation of Videos as they play?,https://www.reddit.com/r/MachineLearning/comments/awrdtt/translation_of_videos_as_they_play/,imascientist42,1551594347,[removed],0,1,False,self,,,,,
118,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,15,awrk00,self.MachineLearning,[P] Language Model Fine-tuning for Moby Dick,https://www.reddit.com/r/MachineLearning/comments/awrk00/p_language_model_finetuning_for_moby_dick/,longinglove,1551595869,"&amp;#x200B;

I have a dream; I'd like to write a novel. Unfortunately, I don't  think I can. But maybe some day I'll be able to make a machine write a  novel instead of me. Some day  ...

Language modeling is on the center of this dream. Luckily and thankfully we don't have to train a language model from scratch. Many great pretrained models are available. What we need to do is fine-tune them for our tasks.

I want to see what will happen if we fine-tune a pretrained language  model to a novel and subsequently generate text based on the last part  of the novel. The glory (?!) goes to Moby Dick, one of the greatest  novels of all time. Why the long and difficult whale story? Well, at  least its text is in the public domain:)

Don't be too serious. Just enjoy.

&amp;#x200B;

[https://github.com/Kyubyong/lm\_finetuning](https://github.com/Kyubyong/lm_finetuning)",8,61,False,self,,,,,
119,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,16,awrnkp,self.MachineLearning,How to load the amazon reviews dataset,https://www.reddit.com/r/MachineLearning/comments/awrnkp/how_to_load_the_amazon_reviews_dataset/,textMinier,1551596744,I download the whole dataset without duplicates which is 56Gb. How can i load it using Python ? Do you think that is better to load the zipped version or the unzipped ? Im on a standard laptop 16gb ram ,0,1,False,self,,,,,
120,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,16,awrqua,self.MachineLearning,Could video anonymization foster democracy ?,https://www.reddit.com/r/MachineLearning/comments/awrqua/could_video_anonymization_foster_democracy/,zagdem,1551597592,[removed],0,1,False,self,,,,,
121,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,16,awrwoh,self.MachineLearning,"End-to-end examples of microtargeting goals, worked out in Python or R",https://www.reddit.com/r/MachineLearning/comments/awrwoh/endtoend_examples_of_microtargeting_goals_worked/,thrownaway1190,1551599170,[removed],0,1,False,self,,,,,
122,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,17,aws7rr,self.MachineLearning,What are the ways of integrating a knowledge-based recommender system with a web application through API?,https://www.reddit.com/r/MachineLearning/comments/aws7rr/what_are_the_ways_of_integrating_a_knowledgebased/,MetabolismZeitgeist,1551602332,"Should the objects be replicated again in the recommender server and served directly from it to the client, or should it give out the IDs of the relevant objects to the web application which will in-turn fetch each of those resources and serve to the client? What other types of interface patterns are possible for such a setting?",0,1,False,self,,,,,
123,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,17,aws9xl,self.MachineLearning,[D] What are the ways of deploying/integrating a recommender system with a web application through API?,https://www.reddit.com/r/MachineLearning/comments/aws9xl/d_what_are_the_ways_of_deployingintegrating_a/,MetabolismZeitgeist,1551602946,"Should all the objects from the web applications' database be replicated again to a database in the recommender server and serve directly from it to the client, or should it give out the IDs of the relevant objects to the web application which will in-turn fetch each of those resources and serve to the client? What other types of interface patterns are possible for such a setting? None of the above seem to be efficient",0,0,False,self,,,,,
124,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,18,awsdzo,self.MachineLearning,Do you need to calculate full output layer cost?,https://www.reddit.com/r/MachineLearning/comments/awsdzo/do_you_need_to_calculate_full_output_layer_cost/,werem0,1551604137,[removed],0,1,False,self,,,,,
125,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,18,awsf5e,self.MachineLearning,[D] How to find the best cost function for a dataset? (Tic-Tac-Toe),https://www.reddit.com/r/MachineLearning/comments/awsf5e/d_how_to_find_the_best_cost_function_for_a/,Pronoob_me,1551604460,"Quite new to Machine Learning, I tried to create a Tic-Tac-Toe bot which learns itself by taking in consideration the previous data. I haven't referred to any source throughout the project. After I was done with the project, I researched a few similar projects where they use a cost function (totally new to the term), which I somewhat implemented in a layman's method. 

&amp;#x200B;

I'm storing the set of moves and it's outcome. Next time, if I encounter a similar case, I can react accordingly. My current cost function is as follows:  


    Lose: -1
    Win: +1
    Draw: +0.5

Based on this, I'm selecting the move with max value. If I don't have the specific dataset, I'll randomly increase one of the available moves by `0.5`, similar to draw.

&amp;#x200B;

However, this doesn't seems to be satisfying enough to be a cost function(I saw most cost functions for random projects to be heavily mathematical based), is there any better way to find the best move?",13,7,False,self,,,,,
126,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,18,awsirb,self.MachineLearning,Image inpainting using GAN,https://www.reddit.com/r/MachineLearning/comments/awsirb/image_inpainting_using_gan/,akhilmaliackal,1551605550,[removed],0,1,False,self,,,,,
127,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,18,awskjl,self.MachineLearning,"Is the 0% training error in ""A Class Of Deep Neural Networks With No Bad Local Valleys"" a big deal?",https://www.reddit.com/r/MachineLearning/comments/awskjl/is_the_0_training_error_in_a_class_of_deep_neural/,PlymouthPolyHecknic,1551606082,"[https://openreview.net/pdf?id=HJgXsjA5tQ](https://openreview.net/pdf?id=HJgXsjA5tQ)  


I was surprised reading this ICLR 2019 paper that the authors got 0% error  


&gt;Beside the theoretical analysis, we show in experiments that despite achieving zero training error, the aforementioned class of neural networks generalize well in practice when trained with SGD whereas an alternative training procedure guaranteed to achieve zero training error has significantly worse generalization performance and is overfitting

&amp;#x200B;

Is it a big deal that they can get 0% training error, or are there other CNN architectures that can do this without massively overfitting? From what I've heard CNNs error's normally plateau at a similar order of magnitude to the test error",0,1,False,self,,,,,
128,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,19,awsogw,i.redd.it,Simple OCR with tesseract,https://www.reddit.com/r/MachineLearning/comments/awsogw/simple_ocr_with_tesseract/,bharat0to,1551607237,,1,1,False,https://a.thumbs.redditmedia.com/5hYnmpJOopHUSaXBjEyU8L_NmcIDG8Vv4D1eSVlpi_4.jpg,,,,,
129,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,19,awsw5q,nintyzeros.com,Tensorflow basic !,https://www.reddit.com/r/MachineLearning/comments/awsw5q/tensorflow_basic/,John1017x,1551609388,,0,1,False,https://b.thumbs.redditmedia.com/E7MJojjeMUX3ct3FU8zRL3yYmonFWIBteCrwYx_gtOE.jpg,,,,,
130,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,20,awt6re,self.MachineLearning,[D] Which NLP framework would you suggest for KMeans for European languages especially Romanian?,https://www.reddit.com/r/MachineLearning/comments/awt6re/d_which_nlp_framework_would_you_suggest_for/,abdush,1551612415,,0,0,False,self,,,,,
131,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,21,awtomt,self.MachineLearning,Any mobile app examples that you think (or know) heavily use AI/ML?,https://www.reddit.com/r/MachineLearning/comments/awtomt/any_mobile_app_examples_that_you_think_or_know/,w00lf_,1551617147,[removed],0,1,False,self,,,,,
132,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,21,awtq0a,self.MachineLearning,Keras Loss Value Extremely High [dataset issue?],https://www.reddit.com/r/MachineLearning/comments/awtq0a/keras_loss_value_extremely_high_dataset_issue/,meestake,1551617484,"All the Keras Deep Learning tutorials feature an already wrapped image dataset, and there's just one load method to load it all.

I have a set of images and a corresponding csv file for targets of those images. I can't use the Image Data Generator as it's a regression problem and not a labeling one. So I made a custom numpy array of the following:

- a numpy array of (448, 448, 3) images

- a numpy array of corresponding target numbers

When this is fed into the model, I face no error/ exception. Except the output looks ridiculously bad. The loss encountered is extremely high (in thousands), and the data really does not look so incoherent. Maybe it's the model. Here's the description:

Sequential with 2 Conv2D Layers (64, 32), flattened it to feed to a Dense layer of 16 and 8, and then one final Dense layer with 1 output node with no activation function (because, regression). [Also tried to scale the image values to [0,1], but no luck.]


As a noob in this domain, I have no idea where to begin to know where I could be going wrong. If it's the way I'm loading up the data, can anyone guide me to how to go about with that. Thanks.",0,1,False,self,,,,,
133,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,22,awtuky,self.MachineLearning,[P]Almost 20 image classification CNN networks implementation using Pytorch,https://www.reddit.com/r/MachineLearning/comments/awtuky/palmost_20_image_classification_cnn_networks/,372995411,1551618489,"Hi, guys, I've implemented almost 20 different image classification networks using Pytorch, including some main stream networks like VGG, Inception family, resnet family(resnext, densenet, resnext, preact resnet, residual attention network), or the networks specifically for mobile devices like squeezenet, mobilenetv1-v2, shufflenetv1-v2, even the learned network architecture nasnet. All the networks were trained and tested on cifar100 dataset.

&amp;#x200B;

Here is my little project:[https://github.com/weiaicunzai/pytorch-cifar](https://github.com/weiaicunzai/pytorch-cifar) if you are interested.

If you are new to Pytorch, want to learn Pytorch by writing a small project, I think this repository is what you want. And I will implement more networks  in the near future.

&amp;#x200B;

&amp;#x200B;",13,32,False,self,,,,,
134,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,22,awtuql,sinxloud.com,A Beginner's Guide to Learn Machine Learning with Python,https://www.reddit.com/r/MachineLearning/comments/awtuql/a_beginners_guide_to_learn_machine_learning_with/,skj8,1551618528,,0,1,False,https://a.thumbs.redditmedia.com/2Rh3eeXtM-TGnEERrqh6U29L11TltKbL4HVgd-WhXa0.jpg,,,,,
135,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,22,awtw3s,self.MachineLearning,[D] [NLP] Cosine similarity of vectors in high dimensional data (Language models),https://www.reddit.com/r/MachineLearning/comments/awtw3s/d_nlp_cosine_similarity_of_vectors_in_high/,mac_cumhaill,1551618830,"I'm performing some semantic similarity using high dimensional language models. Within this high dimensional feature space, I can use cosine similarity to compute the similarly of two vectors. I could also use euclidean distance. 

&amp;#x200B;

For anyone in the NLP setting, have you come across other methods to do this? ",13,7,False,self,,,,,
136,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,23,awu86b,self.MachineLearning,[Discussion] Agree or disagree: Data science jobs will undergo radical change with automated software in next 5 years.,https://www.reddit.com/r/MachineLearning/comments/awu86b/discussion_agree_or_disagree_data_science_jobs/,joking0303,1551621728,"https://www.forbes.com/sites/forbestechcouncil/2019/03/01/radical-change-is-coming-to-data-science-jobs/

I read this article from the link above. I tend to disagree with the notion that the data science job will be eliminated by auto machine learning software packages like data robot. I hate the idea of someone just throwing their data in and getting results. From my experience as a data scientist, it takes time and energy to analyze the data, implement feature engineering, and evaluating model performance based on the data. 

As for we are automating ourselves out of our job. Well yeah thats the idea. And thats not a bad thing. However, I think we automate one process to move on to automate something else, especially with all types of new data  coming into the digital world.

I do find it interesting how the author splits the new data science work into 5 areas that will take place at n 5-10 years. Do you agree with this assessment? Will the data scientist boom disappear? Will it be replaced by a more specific job? What do you think? If these changes are true, how does it impact our work?",129,273,False,self,,,,,
137,MachineLearning,t5_2r3gv,2019-3-3,2019,3,3,23,awul4g,self.MachineLearning,"Crazy thought in my mind, why do we need to move from scores to probabilities?",https://www.reddit.com/r/MachineLearning/comments/awul4g/crazy_thought_in_my_mind_why_do_we_need_to_move/,albert1905,1551624363,"Hi , I had some wondering and thought about some translate model, Transformer, NMT, doesn't really matter...
I've been thought we want to use softmax in the end of classification networks, in order to move from scores to prob.
But why?

In inference I can just do Argmax, why should I softmax?
and in training time, why can I do TopK straight on the scores...

So why do we really need softmax...?

Thanks.",0,1,False,self,,,,,
138,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,0,awur40,self.MachineLearning,Python: Journey From Novice To Expert,https://www.reddit.com/r/MachineLearning/comments/awur40/python_journey_from_novice_to_expert/,mritraloi6789,1551625483,[removed],0,1,False,self,,,,,
139,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,0,awuryp,self.MachineLearning,[D] Does AC-GAN produce adversarial samples,https://www.reddit.com/r/MachineLearning/comments/awuryp/d_does_acgan_produce_adversarial_samples/,LynnHoHZL,1551625630,,2,1,False,self,,,,,
140,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,0,awv4yg,self.MachineLearning,"Hey Redditors I need your Help, I need sources for this project and a bit of help.",https://www.reddit.com/r/MachineLearning/comments/awv4yg/hey_redditors_i_need_your_help_i_need_sources_for/,spacetimematters,1551628001,"Before I start the project I am working on I based on video analysis.

Object detection but in pre recorded video as well as using sound identification as a extra layer if I need one.

Should I use TensorFlow, is it bad to use Google Colabs?, 

&amp;#x200B;

And any recommended sources before I waste 6 hours again?

&amp;#x200B;

Thanks,

\-SpaceTimeMatters",0,1,False,self,,,,,
141,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,1,awvabl,medium.com,Why A.I. Needs Blockchain to Evolve.,https://www.reddit.com/r/MachineLearning/comments/awvabl/why_ai_needs_blockchain_to_evolve/,Cointhropologist,1551628923,,0,1,False,https://b.thumbs.redditmedia.com/aikgln7qKWizL0mUllJTBNkfz689bnvr32ca6EJoCcg.jpg,,,,,
142,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,1,awvijg,self.MachineLearning,Machine Learning,https://www.reddit.com/r/MachineLearning/comments/awvijg/machine_learning/,Infinite_Can,1551630296,[removed],0,1,False,self,,,,,
143,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,1,awvure,self.MachineLearning,Going from discrete pricing to continuous pricing,https://www.reddit.com/r/MachineLearning/comments/awvure/going_from_discrete_pricing_to_continuous_pricing/,Xamius,1551632324,[removed],0,1,False,self,,,,,
144,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,2,aww7ce,self.MachineLearning,Trying to understand the latent space of VAE,https://www.reddit.com/r/MachineLearning/comments/aww7ce/trying_to_understand_the_latent_space_of_vae/,perceptron333,1551634302,[removed],1,1,False,self,,,,,
145,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,3,awwqlq,self.MachineLearning,Using a model in HDF5 without the need for external libraries?,https://www.reddit.com/r/MachineLearning/comments/awwqlq/using_a_model_in_hdf5_without_the_need_for/,bigDATAbig,1551637243,[removed],0,1,False,self,,,,,
146,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,3,awwr6i,self.MachineLearning,"[D] How do they make AI that doesn't just predict, but is creative?",https://www.reddit.com/r/MachineLearning/comments/awwr6i/d_how_do_they_make_ai_that_doesnt_just_predict/,NarawaGames,1551637325,"How does a machine learning algorithm do more than predict, classify, and detectlike make music, generate images, and write sentences?",8,1,False,self,,,,,
147,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,3,awx4bb,self.MachineLearning,Predicting the runtime of scikit-learn algorithms,https://www.reddit.com/r/MachineLearning/comments/awx4bb/predicting_the_runtime_of_scikitlearn_algorithms/,mysteriousreader,1551639284,[removed],0,1,False,self,,,,,
148,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,4,awx8au,self.MachineLearning,[P] Predicting the runtime of scikit-learn algorithms,https://www.reddit.com/r/MachineLearning/comments/awx8au/p_predicting_the_runtime_of_scikitlearn_algorithms/,mysteriousreader,1551639883,"Hey guys,

We're two friend who met in college and learned Python together, we co-created a package which can provide an estimate for the training time of scikit-learn algorithms.

Check it out! [https://github.com/nathan-toubiana/scitime](https://github.com/nathan-toubiana/scitime)

Any feedback is greatly appreciated.

Here's a simple use case:

    from scitime import Estimator 
    estimator = Estimator() 
    rf = RandomForestRegressor()
    X,y = np.random.rand(100000,10),np.random.rand(100000,1)
    # Run the estimation
    estimation, lower_bound, upper_bound = estimator.time(rf, X, y)",10,30,False,self,,,,,
149,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,4,awxk1q,residualthoughts.com,"I analyzed some data around Numerai, the hedge fund that outsources its modeling. Would love to hear your thoughts.",https://www.reddit.com/r/MachineLearning/comments/awxk1q/i_analyzed_some_data_around_numerai_the_hedge/,waitingforgoodoh,1551641603,,0,1,False,default,,,,,
150,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,7,awz8px,thegradient.pub,[D] OpenAI Shouldnt Release Their Full Language Model (The Gradient),https://www.reddit.com/r/MachineLearning/comments/awz8px/d_openai_shouldnt_release_their_full_language/,ezelikman,1551650878,,1,1,False,default,,,,,
151,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,7,awzcft,self.MachineLearning,[D] OpenAI Shouldnt Release Their Full Language Model (The Gradient),https://www.reddit.com/r/MachineLearning/comments/awzcft/d_openai_shouldnt_release_their_full_language/,ezelikman,1551651479,"[https://thegradient.pub/the-limitations-of-visual-deep-learning-and-how-we-might-fix-them/](https://thegradient.pub/the-limitations-of-visual-deep-learning-and-how-we-might-fix-them/)

A piece using the GPT-2 case to argue that ""considering the impact and misuse of released models is the only sustainable path to progress in AI research""",71,0,False,self,,,,,
152,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,10,ax16q1,reddit.com,Machine Learning newbie interested in implementing papers and reproducing results,https://www.reddit.com/r/MachineLearning/comments/ax16q1/machine_learning_newbie_interested_in/,randomicly,1551662634,,0,1,False,default,,,,,
153,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,11,ax1m51,self.MachineLearning,Measuring the exploitability of a policy.,https://www.reddit.com/r/MachineLearning/comments/ax1m51/measuring_the_exploitability_of_a_policy/,VirtualHat,1551665376,[removed],0,1,False,self,,,,,
154,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,11,ax1t9q,self.MachineLearning,[P] ThunderGBM: Fast GBDTs and Random Forests on GPUs,https://www.reddit.com/r/MachineLearning/comments/ax1t9q/p_thundergbm_fast_gbdts_and_random_forests_on_gpus/,hw2018,1551666643,"A new project, ThunderGBM, has recently been released. ThunderGBM supports GBDTs and Random Forests on GPUs. ThunderGBM can beat existing libraries such as XGBoost, CatBoost and LightGBM by over 10 times in many cases. More information at [https://github.com/Xtra-Computing/thundergbm](https://github.com/Xtra-Computing/thundergbm)

&amp;#x200B;

By the way, there is another project of the same group of researchers for fast SVMs. You can enjoy 100 times speedup over LibSVM. [https://github.com/Xtra-Computing/thundersvm](https://github.com/Xtra-Computing/thundergbm)",19,40,False,self,,,,,
155,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,11,ax1z67,stationerymachine.com,eraser packing machine,https://www.reddit.com/r/MachineLearning/comments/ax1z67/eraser_packing_machine/,candidstationery,1551667681,,0,1,False,https://b.thumbs.redditmedia.com/8KW0lcH21oAz0NOTUUKnkvQ6JL6a3-P112DUVrZehXA.jpg,,,,,
156,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,12,ax2kwq,self.MachineLearning,[Question] When do you use squared euclidean distance v/s euclidean distance?,https://www.reddit.com/r/MachineLearning/comments/ax2kwq/question_when_do_you_use_squared_euclidean/,scun1995,1551671440,[removed],0,1,False,self,,,,,
157,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,13,ax2wm0,self.MachineLearning,[P] Simple implementation of 'pytorch-darknet19' classifier with pretrained_weight,https://www.reddit.com/r/MachineLearning/comments/ax2wm0/p_simple_implementation_of_pytorchdarknet19/,visionNoob_r,1551673601,"[https://github.com/insurgent92/pytorch-darknet19](https://github.com/insurgent92/pytorch-darknet19)

&amp;#x200B;

I implemented the darknet19 model in pytorch 1.0 for yolo9000. 

I'm sorry for the weak readme, but it's very intuitive to use the model.

I wish it might help you, when you try to implement YOLO models in PyTorch. ",1,15,False,self,,,,,
158,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,13,ax31ex,wsf-tex.com,cone winding machine,https://www.reddit.com/r/MachineLearning/comments/ax31ex/cone_winding_machine/,CarrieXian,1551674501,,0,1,False,default,,,,,
159,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,13,ax355z,wsf-tex.com,fancy yarn twisting machine,https://www.reddit.com/r/MachineLearning/comments/ax355z/fancy_yarn_twisting_machine/,CarrieXian,1551675197,,0,1,False,default,,,,,
160,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,15,ax3x7i,self.oraclemachine,US Airforce investing in AI start-ups.,https://www.reddit.com/r/MachineLearning/comments/ax3x7i/us_airforce_investing_in_ai_startups/,Jackson_Filmmaker,1551680715,,0,1,False,default,,,,,
161,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,15,ax406x,arxiv.org,[R] [1903.00374] Model-Based Reinforcement Learning for Atari: Achieving human-level performance on many Atari games after two hours of real-time play,https://www.reddit.com/r/MachineLearning/comments/ax406x/r_190300374_modelbased_reinforcement_learning_for/,evc123,1551681333,,37,180,False,default,,,,,
162,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,15,ax40q1,self.jobs,I quit masters in CS and my majority of courses were ML related... do I put this in my resume?,https://www.reddit.com/r/MachineLearning/comments/ax40q1/i_quit_masters_in_cs_and_my_majority_of_courses/,pickleorc,1551681445,,0,1,False,https://b.thumbs.redditmedia.com/P14hGqqJ9xb4fIRJzwVDS_1vh95lHdKeSBK2YYzwftU.jpg,,,,,
163,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,15,ax40ss,self.MLQuestions,Data scientist interview prompt: investigate the effect of putting items on Sale,https://www.reddit.com/r/MachineLearning/comments/ax40ss/data_scientist_interview_prompt_investigate_the/,mclovin215,1551681461,,0,1,False,https://b.thumbs.redditmedia.com/vclVcw4-JPvWMVP34wIuzjnUiPsnj_uqSOAuFhV9ftQ.jpg,,,,,
164,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,16,ax48vb,self.MachineLearning,Whats the difference between reinforcement learning and adaptive dynamic programming?,https://www.reddit.com/r/MachineLearning/comments/ax48vb/whats_the_difference_between_reinforcement/,liquidface,1551683171,[removed],0,0,False,self,,,,,
165,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,16,ax4b55,tweakyourbiz.com,How to Empower AI and Machine Learning Through Big Data?,https://www.reddit.com/r/MachineLearning/comments/ax4b55/how_to_empower_ai_and_machine_learning_through/,JanBaskTraining,1551683672,,0,1,False,default,,,,,
166,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,16,ax4e1d,youtube.com,Vermicompost wet method organic fertilizer press granulator machine,https://www.reddit.com/r/MachineLearning/comments/ax4e1d/vermicompost_wet_method_organic_fertilizer_press/,amylee516,1551684299,,0,1,False,default,,,,,
167,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,16,ax4ibi,compakk.blogspot.com,Excellent Range of High Quality Shrink Wrap Machines,https://www.reddit.com/r/MachineLearning/comments/ax4ibi/excellent_range_of_high_quality_shrink_wrap/,compak03,1551685277,,0,1,False,default,,,,,
168,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,16,ax4mn8,self.MachineLearning,Early into your ML career what's more important? The kind of stuff one's working on or the mentorship?,https://www.reddit.com/r/MachineLearning/comments/ax4mn8/early_into_your_ml_career_whats_more_important/,AutomaticTomato9,1551686319,[removed],0,1,False,self,,,,,
169,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,17,ax4tio,self.MachineLearning,Using a time series of images for classification,https://www.reddit.com/r/MachineLearning/comments/ax4tio/using_a_time_series_of_images_for_classification/,Shakyor,1551687945,[removed],0,1,False,self,,,,,
170,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,17,ax4wee,self.MachineLearning,Microsoft goes sees a better future with AI at Mobile World Congress,https://www.reddit.com/r/MachineLearning/comments/ax4wee/microsoft_goes_sees_a_better_future_with_ai_at/,IoT-Fan,1551688674,[removed],0,1,False,self,,,,,
171,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,18,ax59q7,self.MachineLearning,Can a neural network simulate any/most algorithms?,https://www.reddit.com/r/MachineLearning/comments/ax59q7/can_a_neural_network_simulate_anymost_algorithms/,s0hungry1,1551691827,"New to machine learning. Just learned a couple weeks ago about neural networks and saw the demonstration of a neural network that could represent not-xor and more complicated logic. Could a neural network be trained to simulate any sort of algorithm? Given the entire set of valid inputs and outputs for any algorithm can you exactly generate a neural network that can yield the same inputs and outputs? For example, sorting an array.

Im sure theres cases that dont really make sense (non determinism in certain algorithms), but ignoring those edge cases.",0,1,False,self,,,,,
172,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,18,ax5bu2,self.MachineLearning,Benefits Of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/ax5bu2/benefits_of_machine_learning/,zenraysofficial,1551692329,[removed],0,1,False,self,,,,,
173,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,20,ax68bc,youtube.com,3D Advanced Neural Network Simulation - Computer vision - Digit Recognition,https://www.reddit.com/r/MachineLearning/comments/ax68bc/3d_advanced_neural_network_simulation_computer/,DevTechRetopall,1551699950,,0,1,False,https://b.thumbs.redditmedia.com/fD3WyCoxVkF8T10Ujz0fj1X-EDFBbxR27arlt7J0t_Q.jpg,,,,,
174,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,21,ax6h8j,self.MachineLearning,Two different GPUs for Keras (Python)?,https://www.reddit.com/r/MachineLearning/comments/ax6h8j/two_different_gpus_for_keras_python/,mlg1988,1551701710," One question guys, someone knows if it should be ok to get one more GPU of type Nvidia Geforce GTX 1070 (gaming version), given that now I have GTX 1070 Titanium? They don't have another Titanium card available here, so I have to get a different one, but closely similar, and I wonder if for using Keras (with TensorFlow backend), will it work fine? They are not exactly the same cards, but similar enough maybe. I want 2 GPUs for Keras. ",0,1,False,self,,,,,
175,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,21,ax6k8j,smarten.com,Can Business Users Easily Adopt a Data Discovery Solution?,https://www.reddit.com/r/MachineLearning/comments/ax6k8j/can_business_users_easily_adopt_a_data_discovery/,ElegantMicroWebIndia,1551702282,,0,1,False,default,,,,,
176,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,21,ax6qel,self.MachineLearning,"Encyclopedia Of Algorithms, 2nd Edition",https://www.reddit.com/r/MachineLearning/comments/ax6qel/encyclopedia_of_algorithms_2nd_edition/,mritraloi6789,1551703454,[removed],0,1,False,self,,,,,
177,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,21,ax6qza,self.MachineLearning,[P] DeepClapback - learning an appropriate text meme (or no-meme) from Reddit,https://www.reddit.com/r/MachineLearning/comments/ax6qza/p_deepclapback_learning_an_appropriate_text_meme/,prototypist,1551703560,"""DeepClapback"" is a project where I torrented a dataset of 100s of millions of Reddit comments, filtered them down to replies that scored 1.5x or above a parent comment, then trained a model on when to make 100 common replies. It takes a little NLP - for example it would be strange to reply 'and my axe!' when a citation was needed, or thats what she said when a thanks! was more appropriate. Later I reran the model with a 'NOMEME' category to try avoiding recommending memes in bad situations.

[https://medium.com/@mapmeld/can-deepclapback-learn-when-to-lol-e4a2092a8f2c](https://medium.com/@mapmeld/can-deepclapback-learn-when-to-lol-e4a2092a8f2c)

I thought it would be fun (and/or meta) to share the data crunching and Google AutoML process on Reddit. In the future I would like to use PyTorch/PyText, or a sequence-2-sequence model. 

I can tell that the confusion matrices are not amazing, so I'd like to use the score and subreddit for additional context.  Actually I would be interested in you all recommending what I could use for classification while weighting based on upvotes.",7,31,False,self,,,,,
178,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,23,ax7ihh,self.MachineLearning,Softmax or Sigmoid in a Discrete Action space?,https://www.reddit.com/r/MachineLearning/comments/ax7ihh/softmax_or_sigmoid_in_a_discrete_action_space/,Jandevries101,1551708523,"Hi everyone, 

&amp;#x200B;

So i am very curious what you think about Softmax or Sigmoid in a Discrete Action space. Personally i am using the Softmax, but i heard Sigmoid can also be efficient.... I know their diffrences in theory, but what is the improvement and where should which be applied? 

&amp;#x200B;

For example in my algorithme when i use softmax i get a sane Softmax, but when i start using sigmoid instead of softmax it starts to saturate, but this could be something else of course, for example exploding gradients (don't know yet, nor how to solve :) ). So their is definitely a diffrence in them?

&amp;#x200B;

What do you think?

&amp;#x200B;

Jan",0,1,False,self,,,,,
179,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,23,ax7kg4,self.MachineLearning,Changing career from Web Dev to Machine Learning,https://www.reddit.com/r/MachineLearning/comments/ax7kg4/changing_career_from_web_dev_to_machine_learning/,Kyleez,1551708846,[removed],0,1,False,self,,,,,
180,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,23,ax7kk4,self.MachineLearning,"When to use Scikit-Learn's MinMaxScaler, RobustScaler, StandardScaler, and Normalizer",https://www.reddit.com/r/MachineLearning/comments/ax7kk4/when_to_use_scikitlearns_minmaxscaler/,discdiver,1551708861,[removed],0,1,False,self,,,,,
181,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,23,ax7nap,self.MachineLearning,Features engineering for percentages and correlated data,https://www.reddit.com/r/MachineLearning/comments/ax7nap/features_engineering_for_percentages_and/,noaai29,1551709324,[removed],0,1,False,self,,,,,
182,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,23,ax7rnz,self.MachineLearning,[P] LSTM implementation in C for byte level prediction,https://www.reddit.com/r/MachineLearning/comments/ax7rnz/p_lstm_implementation_in_c_for_byte_level/,rickardicus,1551710068,"Hi folks!

I wanted to share a project I have been working on. It is a recurrent neural network that predicts bytes which can be used to mimic e.g. lyrical content (such as books). It uses Adams gradient optimization algorithm and the only requirement for using it is GCC. No third parties, just a bunch of C source files needed to be compiled. I have included some pretrained models that might be fun to play around with.

I hope this can serve as an inspiration to anyone wanting to make their own stuff from scratch. 

Feel free to come with constructive comments on what I can add/change or any such requests.
Have a great Monday!

Here is a link to the github repository:

https://github.com/Ricardicus/recurrent-neural-net",35,49,False,self,,,,,
183,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,23,ax7za0,cs.cmu.edu,[D] Integrating Domain Knowledge into Deep Learning - Russ Salakhutdinov at NYAS,https://www.reddit.com/r/MachineLearning/comments/ax7za0/d_integrating_domain_knowledge_into_deep_learning/,nobodykid23,1551711368,,4,23,False,default,,,,,
184,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,23,ax8009,github.com,[P] Triplet loss with Online Negative Mining in PyTorch,https://www.reddit.com/r/MachineLearning/comments/ax8009/p_triplet_loss_with_online_negative_mining_in/,NegatioNZor,1551711487,,0,1,False,default,,,,,
185,MachineLearning,t5_2r3gv,2019-3-4,2019,3,4,23,ax80me,self.MachineLearning,Clustering texts and storing them in a database,https://www.reddit.com/r/MachineLearning/comments/ax80me/clustering_texts_and_storing_them_in_a_database/,lolidunnowut,1551711585,[removed],0,1,False,self,,,,,
186,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,0,ax83h7,self.MachineLearning,[P] model.summary() pip project in pytorch,https://www.reddit.com/r/MachineLearning/comments/ax83h7/p_modelsummary_pip_project_in_pytorch/,nlkey2022,1551712021," In Pytorch, I created \`python pip\` that summarizes the model like Keras.

Actually, There is already python module about model.summary() in [here](https://github.com/sksq96/pytorch-summary).

But it's only limited on Vision Model, also we have to use number of input tensor to be always one.

I want to summarize NLP model such as Big BERT, Transformer.. etc

So I made simple python pip  in here : [https://github.com/graykode/modelsummary](https://github.com/graykode/modelsummary)

This is my first experience about creating pip. :D

Thanks.",10,22,False,self,,,,,
187,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,0,ax892n,self.MachineLearning,Aristotle v. Infogroup/InfoUSA v. Axciom (consumer) data: Columns + Price??,https://www.reddit.com/r/MachineLearning/comments/ax892n/aristotle_v_infogroupinfousa_v_axciom_consumer/,thrownaway1190,1551712905,[removed],0,1,False,self,,,,,
188,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,0,ax89by,self.MachineLearning,The State of Sparsity in Neural Networks,https://www.reddit.com/r/MachineLearning/comments/ax89by/the_state_of_sparsity_in_neural_networks/,ekelsen,1551712945,[removed],0,1,False,self,,,,,
189,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,0,ax8big,self.MachineLearning,The output of my Neural Network is always the same?,https://www.reddit.com/r/MachineLearning/comments/ax8big/the_output_of_my_neural_network_is_always_the_same/,Spirito_santos,1551713288,"I've made a network and trained it on simple datasets such as XOR and it works but whenever I'm running it on a lot of data it ends up always evaluating the same output regardless of input. The desired output is either 0 or 1 but it of course depends on the input. However after the training is done, it returns the same output regardless of inputs.  
I've tried playing with the learning rate and creating more hidden layers but it doesn't help much in the end.  
  
I'm not sure what to do to proceed. Any ideas?",0,1,False,self,,,,,
190,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,0,ax8ebq,datascience.stackexchange.com,What does 'Linear regularities among words' mean?,https://www.reddit.com/r/MachineLearning/comments/ax8ebq/what_does_linear_regularities_among_words_mean/,Dawny33,1551713714,,0,1,False,https://b.thumbs.redditmedia.com/bV4IXT3bismfDCY4Czl1DVBR5Bb-JtqbbWeMHTcshCs.jpg,,,,,
191,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,0,ax8ezo,github.com, POST /predict endpoint for getting VGG16 feature embeddings,https://www.reddit.com/r/MachineLearning/comments/ax8ezo/post_predict_endpoint_for_getting_vgg16_feature/,Coffenpaint,1551713818,,0,1,False,default,,,,,
192,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,0,ax8ktg,self.MachineLearning,[D] Too many arXiv citations when proceedings are available!,https://www.reddit.com/r/MachineLearning/comments/ax8ktg/d_too_many_arxiv_citations_when_proceedings_are/,srossi93,1551714723,"I'm not saying that we should not cite arXiv paper (we already had a lot of discussions, so let's try to keep this case out from this one). I'm saying that when a proceedings version is available, it should always be used instead of arXiv. You have many more information, you give right credits to the work and you easy the reading.

This is something that it's bothering me and it seems to become more and more common. I don't know why -- maybe just lack of care or time, but I think this should never happen.

As an example, just have a look at [this paper](https://arxiv.org/pdf/1901.03611.pdf)! 12 out of 22 citations are arXiv submissions and for most of them there is a proceedings/journal version! 

So, what do you think? Is it just me or are you guys feeling that we are abusing arXiv even more than what we think?

&amp;#x200B;",30,9,False,self,,,,,
193,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,0,ax8l1c,github.com,[P] Easy deployable flask endpoint for extracting image features using VGG16,https://www.reddit.com/r/MachineLearning/comments/ax8l1c/p_easy_deployable_flask_endpoint_for_extracting/,rememberlennydotcom,1551714757,,1,1,False,default,,,,,
194,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,0,ax8n00,self.MachineLearning,[P] Triplet loss with Online Negative Mining in PyTorch,https://www.reddit.com/r/MachineLearning/comments/ax8n00/p_triplet_loss_with_online_negative_mining_in/,NegatioNZor,1551715057,"A PyTorch reimplementation of the [Triplet Loss in Tensorflow](https://www.tensorflow.org/api_docs/python/tf/contrib/losses/metric_learning/triplet_semihard_loss). Unlike other PyTorch implementations I found, this should run entirely on the GPU:


https://github.com/NegatioN/OnlineMiningTripletLoss",0,5,False,self,,,,,
195,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,1,ax8pqp,self.MachineLearning,"[D] Should researchers add watermarks to visual ""fakes"" generation models and/or data sets?",https://www.reddit.com/r/MachineLearning/comments/ax8pqp/d_should_researchers_add_watermarks_to_visual/,CyberByte,1551715438,"With OpenAI's recent attempt to discuss responsible disclosure, I was wondering what researchers could do in this regard. I'm not sure about text generation like with GPT-2, but there are certainly [possible abuses for face generation models](https://venturebeat.com/2019/03/03/why-thispersondoesnotexist-and-its-copycats-need-to-be-restricted/) as well as other deep fakes (e.g. creating impersonation video). 

Assuming the research should be published at all (which I think many people would desire very much), what could (easily) be done to mitigate abuses by bad actors? One thing that doesn't seem that hard is to add watermarks to the training images. Presumably a system that can learn to generate complex visual objects like faces should have no problem also learning to add a simple watermark. These watermarks do not necessarily have to be visible to humans; we could build tools (e.g. into browsers) to detect them automatically. 

The watermarks could be added to the publicly available data set, so that even if people train the model from scratch they'd end up with the watermark in the output, or as a pre-processing step before an image is actually used (which would presumably be easier to remove from the code). In either case, the released model should have been trained with the watermark.

None of this is foolproof, but the main idea is to limit the pool of actors who'd abuse the system. Many wannabe scammers are only able to use sites like ThisPersonDoesNotExist.com or the pre-trained model, so they would end up with the watermarks. People training their own models would have to have the skill to remove the watermarking code, or to remove the watermarks from the entire data set. Removing simple watermarks from an image isn't hard, but many people may still be unable to do it or simply forget. There are also [ways to make it harder](https://www.theverge.com/2017/8/18/16162108/google-research-algorithm-watermark-removal-photo-protection) and more research can advance this further. It won't be perfect, but it's probably (a lot?) better than nothing. 

So what do you think? Is this an easy way to make disclosing visual fakes research (a little) less prone to abuse? Or are there major problems? How would you go about doing this? E.g. put the watermark in the data set you release to the public or keep the data clean and add it as a pre-processing step? Do you have better ideas?",7,2,False,self,,,,,
196,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,1,ax8xcd,self.MachineLearning,[D] What would you like to see in a web base neural network editor ?,https://www.reddit.com/r/MachineLearning/comments/ax8xcd/d_what_would_you_like_to_see_in_a_web_base_neural/,0mbre,1551716519,"Hi guys,

A few weeks back, i've posted here a preview of [https://aifiddle.io](https://aifiddle.io/?fbclid=IwAR2Q216Gv8WtYum5yslMSY-DzVCvXtAFneGrCA8VIxz5WZR0mhDp3ee5kCc), an online neural network editor that I've been working on.

Encouraged by the positive feedback and excitation, I now want to bring AiFiddle to the next level. For that I would love to learn more about you, your ideas and wish-list of what such a tool can be. 

I've created a questionnaire for that effect: [https://goo.gl/forms/DG2mOlGpBzfutRrh2](https://goo.gl/forms/DG2mOlGpBzfutRrh2?fbclid=IwAR3cKXCAIMOcYA2ySsms70QponoUzi3y9n9v_vpXWLQXwNDNvzqjVTq23tc). I should take less than 5 minutes and would help me a lot clarifying what I should focus on. 

Thanks again to you all.

Emmanuel",3,20,False,self,,,,,
197,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,1,ax9379,self.MachineLearning,[D] Generation of Training Data for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/ax9379/d_generation_of_training_data_for_deep_learning/,_ploth,1551717348,"Currently I write my master thesis on generation of training data for deep learning. Therefore I created a list of papers related to data synthesis, image-to-image translation and others.

The idea is to train deep neural networks with synthetic data only or in addition to regular training data to improve the net performance.

The list gives a quick overview and includes their visually appealing results, link to the paper and code.

[https://gitlab.com/ploth/generation-of-training-data-for-deep-learning](https://gitlab.com/ploth/generation-of-training-data-for-deep-learning)

Feel free to contribute!, comment or star the repo.",0,13,False,self,,,,,
198,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,1,ax9bft,self.MachineLearning,Summary of the ResNet Paper for Image Detection,https://www.reddit.com/r/MachineLearning/comments/ax9bft/summary_of_the_resnet_paper_for_image_detection/,themlearning,1551718503,[removed],0,1,False,self,,,,,
199,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,1,ax9bzf,self.MachineLearning,[R] Summary of the ResNet Paper for Image Detection,https://www.reddit.com/r/MachineLearning/comments/ax9bzf/r_summary_of_the_resnet_paper_for_image_detection/,themlearning,1551718576,"Created a summary for the Deep Residual Learning for Image Recognition Paper

[https://themlearning.com/2019/03/04/resnet-paper-summary/](https://themlearning.com/2019/03/04/resnet-paper-summary/)",0,0,False,self,,,,,
200,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,2,ax9fh4,self.MachineLearning,Can multi-label classification work with class hierarchies?,https://www.reddit.com/r/MachineLearning/comments/ax9fh4/can_multilabel_classification_work_with_class/,blaquaman2,1551719042,"I'm relatively new to machine learning. Recently I retrained SSD Mobilenet to recognize certain animals using Tensorflow. I was wondering if would be possible to train a model that can output a more general class if it doesn't recognize the specific one. 

&amp;#x200B;

For example, could I train a model to recognize birds in general, as well as specific species? That way, if if recognizes the bird species, it will label the image accordingly (e.g. sparrow), but if it doesn't, it will simply label the image as a bird.

&amp;#x200B;

I have a general understanding of multi-label classification, but I'm having a hard time understanding if how (or if) it can be implemented with a hierarchy of classes. Also, I'd like to know if it's possible to implement it with popular models like YOLO or SSD Mobilenet.

&amp;#x200B;

If this question is better suited for r/learnmachinelearning, please let me know and I'll remove here and post it there.

&amp;#x200B;

Thank you for your help

&amp;#x200B;",0,1,False,self,,,,,
201,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,2,ax9lpa,self.MachineLearning,Tips on making a machine learning talk interactive?,https://www.reddit.com/r/MachineLearning/comments/ax9lpa/tips_on_making_a_machine_learning_talk_interactive/,oscar_einstein,1551719878,"I'm making a short talk on AI / ML in some days and have a short 25 minute practical session scheduled by the organisers afterwards for the attendees. I was planning on asking them to bring some structured data (if they had any) and uploading it to IBM Watson analytics to show how the algorithms try and generate business insights. One issue is that I am worried their data may not play nice plus I've also seen Watson has changed to Cognos recently which I have no experience with.

I'm wondering if any of you had any ideas for how I could give the attendees the best experience of the fact that AI and ML solutions are out there already and available for them to start playing with and possibly help them, without themselves being programmers, writing algorithms etc. Have any of you had an entry level 'wow' experience? They are going to have their iPads and laptops with them. Hope to hear !",0,1,False,self,,,,,
202,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,2,ax9ojh,self.MachineLearning,"[N] OpenAI release new MMO-style environment for RL, along with baseline",https://www.reddit.com/r/MachineLearning/comments/ax9ojh/n_openai_release_new_mmostyle_environment_for_rl/,NowanIlfideme,1551720260,"Link: https://blog.openai.com/neural-mmo/

I'm not deep enough into RL to give a good overview or estimate significance, but a standardized benchmark above Gym sounds like a good thing. :) ",38,134,False,self,,,,,
203,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,2,axa2fd,self.MachineLearning,[P] RLenv.directory: Index of 150+ reinforcement learning environments ranging from Arcade Games to Robotics.,https://www.reddit.com/r/MachineLearning/comments/axa2fd/p_rlenvdirectory_index_of_150_reinforcement/,rewardsignal,1551722168,"Hello fellow machine teacher!

In  the past months, I have been working on a project to collect a list of open sourced reinforcement learning environments and make them easy to explore  through a web platform hosted on GitHub pages. With the help of many great contributors we are currently indexing 150+ environments! 

If you are starting a new reinforcement learning project do check it out, you might find the perfect environment for your use case!

This project has two main objectives:

1. Facilitate the exploration of existing environments.
2. Encourage the creation of more and diverse learning environments.

This  project is still under active development and open to suggestions! If  there is an environment you created, or know of that is not yet indexed,  you are most welcome to open a pull request and add it to the open index!

Upcoming features include:

1. Collections of reinforcement learning environments organized around a specific topic such as ""Getting Started"" or ""First Steps in Multi-Agent learning""
2. Index of RL tools and libraries for different machine learning frameworks and languages.

Since this is a platform that aims to unify the community I'm very interested in gathering your general thoughts on the platform and what kind of features you are missing or find specially great!

Wish you a great week!

&amp;#x200B;

Web platform:  
[http://RLenv.directory](http://rlenv.directory/)

Repository:  
[http://github.com/pschydlo/RLenv.directory](http://github.com/pschydlo/RLenv.directory) (Dont forget to give the repository a star if you like this initiative, it would be greatly appreciated!) ",0,9,False,self,,,,,
204,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,3,axany4,arxiv.org,The State of Sparsity in Neural Networks,https://www.reddit.com/r/MachineLearning/comments/axany4/the_state_of_sparsity_in_neural_networks/,ekelsen,1551725093,,12,95,False,default,,,,,
205,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,3,axaqpv,blog.openai.com,Neural MMO  A Massively Multiagent Game Environment,https://www.reddit.com/r/MachineLearning/comments/axaqpv/neural_mmo_a_massively_multiagent_game_environment/,mordichsquare,1551725475,,0,1,False,https://b.thumbs.redditmedia.com/a9fjBY8IRRJW3R-A1d43UBP6alMqxO9XvBv6DMIgOyo.jpg,,,,,
206,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,4,axb0o5,ai.googleblog.com,"Introducing GPipe, an Open Source Library for Efficiently Training Large-scale Neural Network Models",https://www.reddit.com/r/MachineLearning/comments/axb0o5/introducing_gpipe_an_open_source_library_for/,sjoerdapp,1551726824,,0,1,False,default,,,,,
207,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,4,axb1pl,syncedreview.com,Google Brain Simplifies Network Learning Dynamics Characterization Under Gradient Descent,https://www.reddit.com/r/MachineLearning/comments/axb1pl/google_brain_simplifies_network_learning_dynamics/,Yuqing7,1551726970,,0,1,False,https://b.thumbs.redditmedia.com/61CRPLQnp4rPdZE4YfDQk1_ld0ryWpilH_YGp75dUlM.jpg,,,,,
208,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,4,axb58y,rlgraph.github.io,"[P] RLgraph: Robust, incrementally testable reinforcement learning",https://www.reddit.com/r/MachineLearning/comments/axb58y/p_rlgraph_robust_incrementally_testable/,qu0d,1551727466,,0,3,False,default,,,,,
209,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,4,axbh73,self.MachineLearning,[D] Hessians - A tool for debugging neural network optimization,https://www.reddit.com/r/MachineLearning/comments/axbh73/d_hessians_a_tool_for_debugging_neural_network/,rvarm1,1551729118,"Hey all, 

Wrote a quick post trying to explain what exactly the Hessian is, and why it can be useful both in convex optimization and deep learning: [http://rohanvarma.me/Optimization/](http://rohanvarma.me/Optimization/)

Let me know what you think!",4,20,False,self,,,,,
210,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,5,axc6wm,self.MachineLearning,Ideas for entertaining a one-day shadowing intern,https://www.reddit.com/r/MachineLearning/comments/axc6wm/ideas_for_entertaining_a_oneday_shadowing_intern/,jw-turner,1551732678,[removed],0,1,False,self,,,,,
211,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,5,axc9hp,self.MachineLearning,carracing environment in gym not training correctly,https://www.reddit.com/r/MachineLearning/comments/axc9hp/carracing_environment_in_gym_not_training/,yuhuil,1551733041,[removed],0,1,False,self,,,,,
212,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,7,axd2es,self.MachineLearning,How do I prepare for executive roles?,https://www.reddit.com/r/MachineLearning/comments/axd2es/how_do_i_prepare_for_executive_roles/,docHlp,1551737094,[removed],0,1,False,self,,,,,
213,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,7,axdirb,self.MachineLearning,[P] Ever wondered how to use your trained sklearn/xgboost/lightgbm models in production? We developed a simple library which turns your models into native code (Python/C/Java),https://www.reddit.com/r/MachineLearning/comments/axdirb/p_ever_wondered_how_to_use_your_trained/,krinart,1551739505,"Imagine that you trained your super accurate model using your favorite tools (Python/sklearn/xgboost/etc.) and now the time has come to deploy your model to production for the greater good.  

But consider the following scenarios:

*  What if your production environment has no Python runtime?
*  What if your model should make instantaneous predictions right on a microcontroller device without sending data to a remote server? 
*  What if prediction speed is a concern too?

This where m2cgen comes in handy. It's a library that generates Java/Python/C code from trained ML models.

Check it out: https://github.com/BayesWitnesses/m2cgen/
",58,338,False,self,,,,,
214,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,7,axdo60,self.MachineLearning,2 sexy AI questions,https://www.reddit.com/r/MachineLearning/comments/axdo60/2_sexy_ai_questions/,alexwagner74,1551740313,[removed],0,1,False,self,,,,,
215,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,8,axdxqy,self.MachineLearning,Machine Learning with Nuclear Spectroscopy,https://www.reddit.com/r/MachineLearning/comments/axdxqy/machine_learning_with_nuclear_spectroscopy/,ghost_throw38,1551741726,[removed],0,1,False,spoiler,,,,,
216,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,8,axe7nf,self.MachineLearning,Check it out: Community Edition for Developers - Automation Anywhere,https://www.reddit.com/r/MachineLearning/comments/axe7nf/check_it_out_community_edition_for_developers/,BDevEx,1551743283,"I joined Automation Anywhere about 2-3 months ago to head up Developer Experience. Im psyched that we just launched our [Community Edition for Developers](https://www.automationanywhere.com/lp/rpa-editions-comparison) today. It's free for all developers to use, and includes our RPA, IQ Bot (artificial intelligence), Bot Insight (Analytics), and Bot Store. There's a massive opportunity for building bots and digital workers for businesses to make them more productive.

I'd like to hear from you on your thoughts on the release, and any feedback that you have about building on our platform.",0,1,False,self,,,,,
217,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,9,axec2d,self.MachineLearning,Book recommendations for learning machine learning?,https://www.reddit.com/r/MachineLearning/comments/axec2d/book_recommendations_for_learning_machine_learning/,Lord_Tisisav,1551744011,[removed],0,1,False,self,,,,,
218,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,9,axehpb,self.MachineLearning,DengAI prediction competition on DrivenData,https://www.reddit.com/r/MachineLearning/comments/axehpb/dengai_prediction_competition_on_drivendata/,Laserdude10642,1551744891,[removed],0,1,False,self,,,,,
219,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,9,axeug5,self.MachineLearning,Using BERT for downstream Part-of-speech (POS) and Named-entity-recognition (NER) tasks,https://www.reddit.com/r/MachineLearning/comments/axeug5/using_bert_for_downstream_partofspeech_pos_and/,Alexinator40,1551746951,[removed],0,1,False,self,,,,,
220,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,10,axf99g,self.MachineLearning,"I know what this does, but can someone explain this so I UNDERSTAND it",https://www.reddit.com/r/MachineLearning/comments/axf99g/i_know_what_this_does_but_can_someone_explain/,iEnjinere,1551749383,[removed],0,1,False,self,,,,,
221,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,11,axfu7g,github.com,Business Machine Learning List (Jupyter Notebook Examples),https://www.reddit.com/r/MachineLearning/comments/axfu7g/business_machine_learning_list_jupyter_notebook/,OppositeMidnight,1551752780,,0,1,False,default,,,,,
222,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,12,axgcfo,wsf-tex.com,yarn cone winder suppliers,https://www.reddit.com/r/MachineLearning/comments/axgcfo/yarn_cone_winder_suppliers/,CarrieXian,1551755791,,0,1,False,https://b.thumbs.redditmedia.com/7HzZrraHQPw2X_UtNomn6TG-ldp0PQFaoUG5orVJoik.jpg,,,,,
223,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,12,axggp0,self.MachineLearning,Idea for much better auto-correct.,https://www.reddit.com/r/MachineLearning/comments/axggp0/idea_for_much_better_autocorrect/,chrisvacc,1551756491,[removed],0,1,False,self,,,,,
224,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,14,axhbba,self.MachineLearning,Questions about ML and Amazon SageMaker,https://www.reddit.com/r/MachineLearning/comments/axhbba/questions_about_ml_and_amazon_sagemaker/,NumerousEntertainer,1551762782,[removed],0,1,False,self,,,,,
225,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,14,axhg8d,self.MachineLearning,[R] Paper Walkthrough - You May Not Need Attention | TDLS,https://www.reddit.com/r/MachineLearning/comments/axhg8d/r_paper_walkthrough_you_may_not_need_attention/,tdls_to,1551763713,"**Slides and recordings**: [https://tdls.a-i.science/events/2019-03-04/](https://tdls.a-i.science/events/2019-03-04/)

**Paper**: [https://arxiv.org/abs/1810.13409](https://arxiv.org/abs/1810.13409)

# Motivation (from the author): 

&gt;Our point was to show that attention may not be as strong as some people think, attention may not be as crucial as some people think, and that attention can sometimes cause problems (on long seqs).

# Q&amp;A (from the author): 

**Is there any guesses why the Eager model takes longer to converge when compared to the attention model?**

&gt;That's a good question. I'm not sure about the exact answer, but two things that may be contributing to this are:  
&gt;  
&gt;1) The eager model must learn how to translate with partial information. While the standard model sees the entire source sentence at every timestep, our eager model sees only part of the sentence in most timesteps.  
&gt;  
&gt;2) The alignments that fast\_align generates are not perfect, and sometimes epsilon padding symbols appear in places they shouldn't be in, and sometimes they are just missing. Having to deal with not only translating but also predicting when to emit a padding symbol makes the eager model's objective much harder than the one that the standard model must learn (just translation).

 **We noticed that in your code you implemented your own embedding dropout. Any reasons why you didn't use nn.Dropout()? (Sorry none of us are well-versed in PyTorch)**

&gt;We based our model on the popular LSTM language model found here:[https://github.com/salesforce/awd-lstm-lm](https://github.com/salesforce/awd-lstm-lm) (there's also an accompanying paper). This dropout is a different kind of dropout. You can find the full description of their method in their paper. If you go over the code of awd-lstm you'll notice that the only major difference between our code and theirs is the generate.py script. Other than that, there aren't big differences, since our model strongly resembles an LSTM language model.

**Please feel free let us know if you have any feedback, or if there is anything you wish to add in the presentation.**

One thing that a number of people have asked me is whether the eager model could work with the Transformer architecture (instead of the LSTM layers). I haven't tried this yet, but I think such a model would work well, and would keep many of the benefits of the LSTM eager model. The recent Transformer-XL paper has shown that the transformer architecture is just as good (or sometimes even better) than LSTMs for the language modeling objective.",1,7,False,self,,,,,
226,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,14,axhiv8,self.MachineLearning,Uber's M3,https://www.reddit.com/r/MachineLearning/comments/axhiv8/ubers_m3/,_guru007,1551764223,[removed],0,1,False,self,,,,,
227,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,14,axhlm8,jesse.thejoyfulprogrammer.com,Jesse AI,https://www.reddit.com/r/MachineLearning/comments/axhlm8/jesse_ai/,HWWilliams,1551764783,,0,1,False,default,,,,,
228,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,14,axhnjd,self.MachineLearning,Prediction model,https://www.reddit.com/r/MachineLearning/comments/axhnjd/prediction_model/,ojhagautam97,1551765179,[removed],0,1,False,self,,,,,
229,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,14,axhnmo,self.MachineLearning,Data Normalization weird behavior?,https://www.reddit.com/r/MachineLearning/comments/axhnmo/data_normalization_weird_behavior/,sud8233,1551765198,[removed],0,1,False,self,,,,,
230,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,15,axhqwy,self.MachineLearning,[P] Hyperbolic N-Space encodings for TensorFlow,https://www.reddit.com/r/MachineLearning/comments/axhqwy/p_hyperbolic_nspace_encodings_for_tensorflow/,kousun12,1551765836,"I still think hyperbolic geometry is hasn't been appreciated enough in the ML world, following the first few papers by [facebook research](https://papers.nips.cc/paper/7213-poincare-embeddings-for-learning-hierarchical-representations). Here's an implementation of some basic functions to support the Poincare model for word embeddings in TF. Lorentz model coming eventually...

&amp;#x200B;

[https://github.com/kousun12/tf\_hyperbolic](https://github.com/kousun12/tf_hyperbolic)",2,41,False,self,,,,,
231,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,15,axhs0k,self.MachineLearning,Training a bot to draw,https://www.reddit.com/r/MachineLearning/comments/axhs0k/training_a_bot_to_draw/,Mbrzzz,1551766057,[removed],0,1,False,self,,,,,
232,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,15,axhx67,self.MachineLearning,Manned Electric Aircraft Market to Perceive Substantial Growth During 2023,https://www.reddit.com/r/MachineLearning/comments/axhx67/manned_electric_aircraft_market_to_perceive/,apple_x9,1551767078,[removed],1,1,False,self,,,,,
233,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,16,axi9rf,self.MachineLearning,[D] Papers on Evaluating NLP Tasks with more Linguistic Foundations,https://www.reddit.com/r/MachineLearning/comments/axi9rf/d_papers_on_evaluating_nlp_tasks_with_more/,cryptopaws,1551769695,"Hello,

Most of NLP suffers from evaluating tasks with not-so-appropriate metrics like BLUE, ROUGE which are basically metrics which do n-gram matching but are still being used because ""they work"".

I've been researching on evaluations of NLP tasks like translation or text generation with more linguistic foundations. what i mean is testing on things like:

* Coherence
* Informativeness
* Fluency
* Diversity : how diverse is the text ?
* Relevance : In the case of summarization or translation, it would be better to measure how relevant your generated text is given source text?

Do you know of any papers which model these metrics? 

&amp;#x200B;

Thank you. ",6,8,False,self,,,,,
234,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,16,axiewu,self.MachineLearning,Any research on Deep-Learning based compilers?,https://www.reddit.com/r/MachineLearning/comments/axiewu/any_research_on_deeplearning_based_compilers/,isthisathrowawaay,1551770772,[removed],0,1,False,self,,,,,
235,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,16,axilpa,github.com,Transform ML models into a native code with zero dependencies,https://www.reddit.com/r/MachineLearning/comments/axilpa/transform_ml_models_into_a_native_code_with_zero/,ai_jobs,1551772268,,0,1,False,https://a.thumbs.redditmedia.com/luG44bdudft3UxfLJBVBkXrXsrqsRDb5X7tP-zUMWw8.jpg,,,,,
236,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,17,axit8m,medium.com,Using deep learning to read your thoughts  with Keras and an EEG sensor [x-post r/artificial],https://www.reddit.com/r/MachineLearning/comments/axit8m/using_deep_learning_to_read_your_thoughts_with/,justLV,1551774026,,1,1,False,https://b.thumbs.redditmedia.com/0DelN_qtzIzF6ugHD7u_qkx-i18Bp3ghDY1xhiNueOk.jpg,,,,,
237,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,17,axitfd,self.MachineLearning,LDA+ WCCN',https://www.reddit.com/r/MachineLearning/comments/axitfd/lda_wccn/,deveid,1551774075,[removed],0,0,False,self,,,,,
238,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,17,axiv75,self.MachineLearning,Machine Learning,https://www.reddit.com/r/MachineLearning/comments/axiv75/machine_learning/,benai9916,1551774516,[removed],0,1,False,self,,,,,
239,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,17,axiwtv,self.MachineLearning,3D Object Recognition in Augmented Reality - HOW?,https://www.reddit.com/r/MachineLearning/comments/axiwtv/3d_object_recognition_in_augmented_reality_how/,jeroeneo,1551774918,[removed],0,1,False,self,,,,,
240,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,18,axj7tv,i.redd.it,"My Google Colab RAM usage rarely goes above 2GB, even though I'm running a pretty GPU intensive task. Is that normal? What am I doing wrong?",https://www.reddit.com/r/MachineLearning/comments/axj7tv/my_google_colab_ram_usage_rarely_goes_above_2gb/,wittyoak,1551777633,,1,1,False,https://b.thumbs.redditmedia.com/ECQidJ0vGLP5dw1tikRe7y1HKGsPC62hESI5Z33Wp9A.jpg,,,,,
241,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,19,axjim2,self.MachineLearning,System for face-document authorization,https://www.reddit.com/r/MachineLearning/comments/axjim2/system_for_facedocument_authorization/,_djab_,1551780174,[removed],0,1,False,self,,,,,
242,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,19,axjj2t,self.MachineLearning,"Google Coral, a new edge compute platform",https://www.reddit.com/r/MachineLearning/comments/axjj2t/google_coral_a_new_edge_compute_platform/,neuralPr0cess0r,1551780268,[removed],0,1,False,self,,,,,
243,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,20,axk4hu,self.MachineLearning,"[R] Assume, Augment and Learn: Unsupervised Few-Shot Meta-Learning via Data-Augmentation and Random Labels",https://www.reddit.com/r/MachineLearning/comments/axk4hu/r_assume_augment_and_learn_unsupervised_fewshot/,AntreasAntoniou,1551785235,"Dear r/MachineLearning friends,

I just wanted to share one of my latest pieces of work on unsupervised few shot learning. The basic idea is: If you have a massive dataset without any labels, and you want to learn a meta-learning-based few-shot learning system, that can learn to learn **any** possible mapping of input images to output labels. Then, one plausible strategy, is to learn to learn, on **any** possible mapping, by assigning random labels to a bunch of data-points, to generate a support set. Finally, in following the set-to-set few-shot learning framework, we also need to generate a target set. A target set, should consist of previously unseen instances of the classes included in the support set. Given that our support set, consists of randomly labeled data-points, generating a target set, that is semantically consistent is quite hard. One possibility is to augment our support set samples, to obtain same-class samples that are different enough to serve as a good generalization measure. 

Paper: https://arxiv.org/abs/1902.09884

Twitter: https://twitter.com/_AntreasAntonio/status/1100692376371236864",7,4,False,self,,,,,
244,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,21,axkhj6,arxiv.org,[R] Autocurricula and the Emergence of Innovation from Social Interaction: A Manifesto for Multi-Agent Intelligence Research (DeepMind),https://www.reddit.com/r/MachineLearning/comments/axkhj6/r_autocurricula_and_the_emergence_of_innovation/,columbus8myhw,1551787998,,1,16,False,default,,,,,
245,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,21,axkhph,eff.org,[D] EFF commentary on OpenAI's announcement of GPT-2,https://www.reddit.com/r/MachineLearning/comments/axkhph/d_eff_commentary_on_openais_announcement_of_gpt2/,juancamilog,1551788028,,0,1,False,https://a.thumbs.redditmedia.com/xU0IqvAwcompiS3S-WSCxvCeOwFqbnVBD3TuCzF50m0.jpg,,,,,
246,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,21,axkmi0,self.MachineLearning,[D] Why is code or libraries for WordPiece tokenization so unavailable?,https://www.reddit.com/r/MachineLearning/comments/axkmi0/d_why_is_code_or_libraries_for_wordpiece/,crabbytodd,1551788969,"I'm using BERT for my Master's Thesis, and I need to create a custom vocabulary for the model. BERT uses WordPiece tokenization for pre-processing, but for some reason, libraries or code for creating a WordPiece vocabulary file seem hard to come by. For instance, [the official repo](https://github.com/google-research/bert), does [not contain](https://github.com/google-research/bert#learning-a-new-wordpiece-vocabulary) any code for learning a new WordPiece vocab. As [this table](https://github.com/google/sentencepiece#comparisons-with-other-implementations) from Google shows, there is no readily available pip package nor C++ library for creating WordPieces. The only thing I've come to find so far is [this class from Neural Monekey](https://neural-monkey.readthedocs.io/en/latest/neuralmonkey.processors.wordpiece.html), which is not a library I'm familiar with. Plus, the latter only seems to be a re-implementation of the tensor2tensor tokenizer.

Does anyone know why Google is being so secretive about this specific technique, or where I can find a proper implementation? The [original article](https://arxiv.org/pdf/1609.08144.pdf) is also rather vague in the explanation of how the WordPiece model is trained, making it hard to implement the model myself correctly.

&amp;#x200B;

Thanks in advance. ",8,7,False,self,,,,,
247,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,21,axkndw,self.MachineLearning,[P] Pushing the Boundaries of Deep Learning: Making Sense of Biopotentials,https://www.reddit.com/r/MachineLearning/comments/axkndw/p_pushing_the_boundaries_of_deep_learning_making/,sosmaaan,1551789137,"Hi guys

Biopotentials are electric potentials (typically on a scale of micro-volts) that are measured between points on living cells. In the past few years we have been working on developing a wrist-worn HMI device which can measure such biopotentials directly from the wrist and recognise finger gestures by applying advanced deep learning algorithms.

Here's a short overview of the process and some of challenges we faced to achieve this, I hope this will interest deep learning practitioners as well as HMI enthusiasts.  [https://medium.com/@leeor.langer/pushing-the-boundaries-of-deep-learning-making-sense-of-biopotentials-1ff191298b7f](https://medium.com/@leeor.langer/pushing-the-boundaries-of-deep-learning-making-sense-of-biopotentials-1ff191298b7f)

&amp;#x200B;

&amp;#x200B;",1,6,False,self,,,,,
248,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,21,axkua7,self.MachineLearning,What would your PhD thesis be?,https://www.reddit.com/r/MachineLearning/comments/axkua7/what_would_your_phd_thesis_be/,TurkeysTech,1551790454,[removed],0,1,False,self,,,,,
249,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,22,axl32z,blockdelta.io,The Decentralized Web (3.0) - How the Internet is Changing for the Better!,https://www.reddit.com/r/MachineLearning/comments/axl32z/the_decentralized_web_30_how_the_internet_is/,BlockDelta,1551792049,,0,1,False,default,,,,,
250,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,22,axlfcw,self.MachineLearning,Multi-Modal Semantic Image Retrieval Demo,https://www.reddit.com/r/MachineLearning/comments/axlfcw/multimodal_semantic_image_retrieval_demo/,gombru,1551794241,[removed],0,1,False,https://b.thumbs.redditmedia.com/q4INI3yOuttnrVUB7Y13IhvcYub7GgolNxh1x-C3xyk.jpg,,,,,
251,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,23,axljzk,irishtimes.com,Many of Europes artificial intelligence start-ups have no AI,https://www.reddit.com/r/MachineLearning/comments/axljzk/many_of_europes_artificial_intelligence_startups/,madpsychrometer,1551795019,,0,1,False,https://b.thumbs.redditmedia.com/rKhvQgBO2l1gLDYQ6PR4z30W-ZOuN9Dm0qVeoxCINNE.jpg,,,,,
252,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,23,axll74,soumyadip1995.blogspot.com,My take on the recently released gpt2 language model by OpenAI.,https://www.reddit.com/r/MachineLearning/comments/axll74/my_take_on_the_recently_released_gpt2_language/,Soumyadip1995,1551795226,,0,1,False,default,,,,,
253,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,23,axlmcz,self.MachineLearning,"hey, anyone knew where can i get a grainy png dataset?",https://www.reddit.com/r/MachineLearning/comments/axlmcz/hey_anyone_knew_where_can_i_get_a_grainy_png/,wait_what_an_actual,1551795424,[removed],0,1,False,self,,,,,
254,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,23,axlnjq,irishtimes.com,[N] Many of Europes artificial intelligence start-ups have no AI,https://www.reddit.com/r/MachineLearning/comments/axlnjq/n_many_of_europes_artificial_intelligence/,madpsychrometer,1551795632,,0,1,False,https://b.thumbs.redditmedia.com/rKhvQgBO2l1gLDYQ6PR4z30W-ZOuN9Dm0qVeoxCINNE.jpg,,,,,
255,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,23,axlpus,github.com,Last week we launched ModelChimp 2.0. An open source tool to keep track of your ML experiments. Would love to hear your thoughts on the product!,https://www.reddit.com/r/MachineLearning/comments/axlpus/last_week_we_launched_modelchimp_20_an_open/,kamanjun,1551796038,,1,1,False,default,,,,,
256,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,23,axls4c,self.MachineLearning,Mean Absolute Error increasing with more correlated factors.,https://www.reddit.com/r/MachineLearning/comments/axls4c/mean_absolute_error_increasing_with_more/,hanneberget,1551796429,"We are using Microsoft Azure Machine Learning Studio to predict stock market prices. We have the variables- Index price(target-to be predicted),Low price,High price,dates and days. We use split of 0.7 and run Linear regression. We get Mean absolute error of 109.

We then try to add more variables(macroeconomic factors which positively effect the index prices) which are correlated with the target variable and should improve the predictions-however we find that the Mean Absolute error increases to 110.I have attached the pics for your reference.

Are we interpreting wrong or what's wrong we are doing?

PS:We tried Boosted Tree regression as well-but the same problem as described above is observed.",0,1,False,self,,,,,
257,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,23,axlu58,self.MachineLearning,Estonia: a springboard for global startups and AI applications,https://www.reddit.com/r/MachineLearning/comments/axlu58/estonia_a_springboard_for_global_startups_and_ai/,triinmahlakoiv,1551796772,[removed],0,1,False,self,,,,,
258,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,23,axlvmn,self.MachineLearning,Hello kind Redditor. Any advice on good data science/machine learning courses for newbie?,https://www.reddit.com/r/MachineLearning/comments/axlvmn/hello_kind_redditor_any_advice_on_good_data/,Chetanoo,1551797027,[removed],0,1,False,self,,,,,
259,MachineLearning,t5_2r3gv,2019-3-5,2019,3,5,23,axm0vm,self.MachineLearning,Estonia: a springboard for global startups and AI applications,https://www.reddit.com/r/MachineLearning/comments/axm0vm/estonia_a_springboard_for_global_startups_and_ai/,triinmahlakoiv,1551797883,[removed],0,1,False,self,,,,,
260,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,0,axm2vy,ailab.microsoft.com,[N] - Microsoft Releases App to create art with GANs,https://www.reddit.com/r/MachineLearning/comments/axm2vy/n_microsoft_releases_app_to_create_art_with_gans/,mhamilton723,1551798204,,0,1,False,default,,,,,
261,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,0,axm381,self.MachineLearning,"linear discriminant analysis,sklearn",https://www.reddit.com/r/MachineLearning/comments/axm381/linear_discriminant_analysissklearn/,deveid,1551798251,[removed],0,0,False,self,,,,,
262,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,0,axmbc6,self.MachineLearning,[D] What libraries/frameworks do you use for casual reinforcement learning?,https://www.reddit.com/r/MachineLearning/comments/axmbc6/d_what_librariesframeworks_do_you_use_for_casual/,smoke_carrot,1551799535,"I'd like to try RL to create simple game AIs. I'm mostly interested in applying existing methods. What would you recommend? Here are a few things I'm looking for:

* Simple, well-documented API
* Easy to add your own environments
* Easy to setup on a single computer",33,21,False,self,,,,,
263,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,0,axmclv,self.MachineLearning,What is Layer Normalization and what makes it diffrent from Batch Normalization?,https://www.reddit.com/r/MachineLearning/comments/axmclv/what_is_layer_normalization_and_what_makes_it/,Jandevries101,1551799740,[removed],0,1,False,self,,,,,
264,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,0,axmjbc,text2data.com,Text Analytics and Sentiment Analysis add-in for Excel,https://www.reddit.com/r/MachineLearning/comments/axmjbc/text_analytics_and_sentiment_analysis_addin_for/,-text2data,1551800757,,0,1,False,default,,,,,
265,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,1,axmz0y,self.MachineLearning,Tools and framework used for ML &amp; AI,https://www.reddit.com/r/MachineLearning/comments/axmz0y/tools_and_framework_used_for_ml_ai/,priyank1sh,1551803031,[removed],0,1,False,self,,,,,
266,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,1,axmzin,self.MachineLearning,[R] Topological methods for unsupervised learning,https://www.reddit.com/r/MachineLearning/comments/axmzin/r_topological_methods_for_unsupervised_learning/,lmcinnes,1551803107,"While topological data analysis has gained a lot of interest in some quarters, it has generally seen relatively little uptake in the broader machine learning community. To attempt to remedy this I have been working to build algorithms and approaches using topological methods that more directly tackle concrete traditional machine learning problems. I recently gave a talk presenting some these ideas, and explaining why topological approaches might be useful. You can find the
[video here](https://slideslive.com/38913519/topological-approaches-for-unsupervised-learning), and [slides here](https://speakerdeck.com/lmcinnes/learning-topology-topological-methods-for-unsupervised-learning). I would be interested in feedback on these sorts of approaches, and how well it speaks to the general machine learning audience.",19,65,False,self,,,,,
267,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,1,axn1ug,self.MachineLearning,Has anyone heard from MILA PhD program?,https://www.reddit.com/r/MachineLearning/comments/axn1ug/has_anyone_heard_from_mila_phd_program/,perceptron333,1551803449,[removed],0,1,False,self,,,,,
268,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,1,axn2g2,self.MachineLearning,[D] Cannot understand LSTM inference.,https://www.reddit.com/r/MachineLearning/comments/axn2g2/d_cannot_understand_lstm_inference/,Ayakalam,1551803541,"&amp;#x200B;

Hi all - I seem to have stumbled on a hole in my understanding around LSTMs. In short, I cannot understand how even a simple one is *actually* fed samples, upon inference time. I wrote a post about it here on [cross-validated](https://stats.stackexchange.com/questions/395652/cannot-understand-lstm-inference), but here is the abridged version:

&amp;#x200B;

* I have time-series weather data. (Not important, just for context).
* During inference time, at every point in the present, I want to predict the next 5 time-steps in the future. 
* I know that there is a strong long-term dependency of about 50 samples in the past. (Meaning, something 50 samples in the past, is very informative to things going on now).

&amp;#x200B;

So my questions are: 

&amp;#x200B;

* How - exactly - do I feed in samples into this LSTM during train-time, and inference-time, such that those long-term dependencies can be captured?
* What I am asking is something very mechanical; meaning, should I feed the LSTM one sample at a time during inference? Or... how else would one even do this?...

&amp;#x200B;

Thank you! 

&amp;#x200B;",9,3,False,self,,,,,
269,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,1,axn41v,arxiv.org,[Research] [1903.00450] Multi-Object Representation Learning with Iterative Variational Inference -- very cool work on object segmentation by DeepMind,https://www.reddit.com/r/MachineLearning/comments/axn41v/research_190300450_multiobject_representation/,hyper_parameter,1551803769,,9,28,False,default,,,,,
270,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,2,axnkq0,engineering.giphy.com,"GIPHYs AI Can Identify Lil Yachty, Can Yours?",https://www.reddit.com/r/MachineLearning/comments/axnkq0/giphys_ai_can_identify_lil_yachty_can_yours/,giphy,1551806186,,0,1,False,default,,,,,
271,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,2,axnm7m,self.MachineLearning,Tetris playing AI with NEAT,https://www.reddit.com/r/MachineLearning/comments/axnm7m/tetris_playing_ai_with_neat/,SunSh4dow,1551806410,"Hey guys,

as disclaimer, i've studied computer science for some years and eventually switched to robotics and automation. I have some basics, mostly in efficient programming, but still lack a lot of experience and practice.


To change that and because i'm very interested in AI, i've come to the decision to try and implement an AI with the NEAT algorithm (because i find this one incredible interesting) that plays Tetris, with the possibility to play against it in 1v1.

Due to my education, i'm spoiled to try to get it implemented as high performing as possible. At this point, i'm convinced that C++ is an excellent choice, as it promises the speed and performance i am looking for (also, i know it a bit).

I found a C++ NEAT implementation, which can be used as a library i think. Haven't looked into it enough, yet.

To create the game, i found an engine called Orx, entirely written in C, which should also promise eminent performance. By putting a library(?) named Scroll on top of it, i can write the game entirely in C++, allowing me to put in the interface for NEAT to directly interact with the game.

So - besides the question if this can actually be done by me or if this is overkill - i am not sure how such a thing is generally constructed.

Can i 'just' build the game with its 2 player functionality and then have a separate program manage NEATs imputs and outputs?
Or maybe i construct the game as a dll and use a program to build two instances of tetris, which manages all inputs and interactions?

Also, if the goal is to make the AI play others, maybe i want it to play against itself in training, anyway. I think that would also help it learn and generally seems to make perfect sense considering how NEAT functions.

There are a bunch of other questions, like, will there be collisions due to parallel inputs. Do i need parallel computing? Does it help if i use WinMain and one of the precompilers?

I might be in way over my head here (is this actually english? I'm ger :D), but i'll try anyway.

If this is the wrong place to ask, i'm sorry :D",0,1,False,self,,,,,
272,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,2,axnorh,self.MachineLearning,We are a team that strives to allow machine learning inference on blockchain. Seeking feedbacks and AMA!,https://www.reddit.com/r/MachineLearning/comments/axnorh/we_are_a_team_that_strives_to_allow_machine/,CTXCBlockchain,1551806778,[removed],0,1,False,self,,,,,
273,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,2,axnpwj,blog.openai.com,[P] Neural MMO - A Massively Multiagent Game Environment,https://www.reddit.com/r/MachineLearning/comments/axnpwj/p_neural_mmo_a_massively_multiagent_game/,luiscosio,1551806940,,0,1,False,default,,,,,
274,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,2,axnxl0,self.MachineLearning,DeepMind interview process,https://www.reddit.com/r/MachineLearning/comments/axnxl0/deepmind_interview_process/,dmprep,1551808023,[removed],0,1,False,self,,,,,
275,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,2,axnyf5,medium.com,Caffe Pioneer &amp; AI Infrastructure Director Leaves Facebook,https://www.reddit.com/r/MachineLearning/comments/axnyf5/caffe_pioneer_ai_infrastructure_director_leaves/,gwen0927,1551808149,,0,1,False,default,,,,,
276,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,3,axo5wr,self.MachineLearning,What would be best for a disease prediction?,https://www.reddit.com/r/MachineLearning/comments/axo5wr/what_would_be_best_for_a_disease_prediction/,madh46,1551809219,[removed],0,1,False,self,,,,,
277,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,3,axoav3,medium.com,Human Pose Estimation Model HRNet Breaks Three COCO Records; CVPR Accepts Paper,https://www.reddit.com/r/MachineLearning/comments/axoav3/human_pose_estimation_model_hrnet_breaks_three/,gwen0927,1551809906,,0,1,False,default,,,,,
278,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,3,axoboo,youtube.com,Epi 01: The Biggest Advantage of Ai,https://www.reddit.com/r/MachineLearning/comments/axoboo/epi_01_the_biggest_advantage_of_ai/,VladLuch,1551810021,,0,1,False,default,,,,,
279,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,3,axochn,self.MachineLearning,"If a million people each broadcast their realtime voice, what kind of data would AI need to organize them fluidly into conversations of about 10 people each?",https://www.reddit.com/r/MachineLearning/comments/axochn/if_a_million_people_each_broadcast_their_realtime/,BenRayfield,1551810133,[removed],0,1,False,self,,,,,
280,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,3,axog0h,youtube.com,Learn what the Biggest Advantage of Ai might be in 2019 by following this YouTube Channel,https://www.reddit.com/r/MachineLearning/comments/axog0h/learn_what_the_biggest_advantage_of_ai_might_be/,VladLuch,1551810630,,0,1,False,https://a.thumbs.redditmedia.com/A8yAC9fREF4n0cnIC3wLJwFceYwxR_ondVE-dab2M78.jpg,,,,,
281,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,3,axoqz6,self.MachineLearning,[D] State of the art: Deep-RL still struggles to solve Mountain Car.,https://www.reddit.com/r/MachineLearning/comments/axoqz6/d_state_of_the_art_deeprl_still_struggles_to/,IliumFuit,1551812220,"For the past few weeks I've been playing around with the toy Mountain Car environment (https://gym.openai.com/envs/MountainCar-v0/). Simple, right? Two continuous state variables, three discrete actions (drive left, neutral, drive right). Compared to Atari, Go, StarCraft, etc. this environment should be a joke.

But have you ever actually tried to solve it? Using Deep-RL??

Sutton &amp; Barto (2018) give a solution to the environment using linear function approximation (tile coding). I've also solved it fairly easily using radial basis functions (another form of linear features). Building on my progress, I tried to switch over to neural network function approximation. Should be easy, right?

SARSA fails. Q learning fails.

Wait, Deep-RL methods need experience replay. Easy, right? Go ahead and try. DQN fails. Vanilla policy gradient fails. A2C fails. A3C fails. 

If you search for Mountain Car + RL on Google, the internet is littered with two kinds of results: 1) Posts to stack exchange, GitHub, /r/machinelearning etc, with people puzzling over why reference implementations of supposed ""state of the art"" algorithms fail on this toy environment, and 2) blog posts and Git repositories with supposed solutions that when you look into it at more than a cursory level more often than not involve hacks tailored specifically to solving Mountain Car.

For example, the excellent tutorial ""[Let's Make a DQN](https://jaromiru.com/2016/10/12/lets-make-a-dqn-debugging/)"" has a basic implementation. With figures of learning results! It has to work, right? Except when you look into the details, they fill their replay buffer with 100k timesteps of random policy exploration steps before even beginning training. And more worrying, they modify the task itself. Apparently having three actions is too difficult, so under the hood they re-map the ""neutral"" action to ""right"". Turns out this is more than just a harmless choice: they changed the environment so that a random policy has a 2/3 probability of moving towards the goal. I am currently attempting their implementation with all three actions back in place, and so far it seems completely unable to solve the task. Another implementation I found online had to modify the reward function to encourage moving to the right. Another source, ""[A deep dive into reinforcement learning](https://www.toptal.com/machine-learning/deep-dive-into-reinforcement-learning)"", has better luck using DQN (though still lackluster compared to linear methods) and concludes with a warning about the brittleness of their solution and the number of hyperparameters necessary to solve it.

So what gives?

If the answer is any variant of ""Mountain Car is a really hard problem"", this fails to take into account that it's actually very trivial for linear function approximation. Flashy demos in Deep-RL are seeming more and more like a magician's sleight of hand: ""Hey, look over here, it's StarCraft!"", while failing to solve much more mundane, and objectively simpler, control problems.

I'm not sure what I'm hoping for with this post. Actually, if you have a simple Deep-RL algorithm that solves Mountain Car *efficiently*, *reliably* and *robustly*, please let me know! The reason I started down this path is I need to solve a more ""complicated"" control problem (just 4 continuous state features, 6 discrete actions) and I am really dreading the amount of hacking I will need to get anything working at all.",117,303,False,self,,,,,
282,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,4,axov01,self.MachineLearning,[D] OCR For Niche Fonts (Dot Matrix),https://www.reddit.com/r/MachineLearning/comments/axov01/d_ocr_for_niche_fonts_dot_matrix/,FeelTheDataBeTheData,1551812803,"Hi everybody! I am working on a project where I need to be able to extract date codes from an image snapped on a smartphone. I have run similar images through traditional approaches like Tesseract, Google Vision API, etc with little to no luck nor consistency. I am assuming I will need to train or transfer learn from another model, but I need help finding the right approach. I have currently tried manually splitting the image based on white space and trying to identify each individual character cropped from the original image. This makes it super difficult to keep users of the app to stay consistent with their photo taking.

I have also tried the tensorflow implementation of YOLO called darkflow. After painstakingly massaging custom image data into the correct format and slow training, we ended up with a 200mb model that could barely identify a couple of the characters and it still isn't a complete solution since we also need to put these characters into order and supply it to the user.

I have seen the convolutional recurrent neural net approach with CTC loss for handwriting detection, but when I tried it, it didn't seem to work for this problem. I can try again, if you think I should!

Am I wrong for assuming this is an object detection problem? Is there another set of solutions that relate more specifically to OCR?

There are plenty of streamlined object detection services through Azure, AWS, and [Supervise.ly](https://Supervise.ly) so I am not concerned if I need to go this route, but I want to make sure I am going in the right direction.

For data, we have gigabytes of photos of actual datecodes. To label, we are looking into MTurk or [Supervise.ly](https://Supervise.ly).

Thanks everyone!

[Needs to be transcribed to ==\&gt; line 1:\\""JUL2219\\"", line2:\\""F04111444 31554\\""](https://i.redd.it/afzrcedfmck21.jpg)",5,5,False,https://b.thumbs.redditmedia.com/sVhkq0ALbvoo4SHz6607SeXqNRKERsuIS_4su-VBnDs.jpg,,,,,
283,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,4,axp39o,self.MachineLearning,Help with behavior of RMSE upon increasing training size and decreasing test size.,https://www.reddit.com/r/MachineLearning/comments/axp39o/help_with_behavior_of_rmse_upon_increasing/,MrMOABy,1551814027,[removed],0,1,False,https://b.thumbs.redditmedia.com/LgWrWkcqwtnbzaQJiT1WZXQmjypdYsV5ZohPaWYafAw.jpg,,,,,
284,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,4,axp4ij,self.MachineLearning,[P] KANN: a standalone CPU-only deep learning library in C,https://www.reddit.com/r/MachineLearning/comments/axp4ij/p_kann_a_standalone_cpuonly_deep_learning_library/,attractivechaos,1551814210,"**TL;DR:** [GitHub link is here][kann]. Useful for portable deployment of simple mlp, rnn/lstm and 1d-cnn.

[KANN][kann] is a standalone CPU-only deep learning library in ~3300 lines of C code. It implements static computation graphs and automatic differentiation like TensorFlow, and builds deep learning modules on top of those (e.g. [this function][lstm-impl] implements a LSTM unit). It supports common operators/layers such as dense, dropout, softmax, cross-entropy, 1d/2d-convolution, concat/stack/slice and LSTM/GRU, and allows you to construct non-linear networks with recurrence and weight sharing. You can also save/load graphs and parameters to/from a single file.

KANN uses SSE and multi-threading/pthread when available. At the time it was developed, it broadly matched the speed of Theano in its CPU mode. Although without the GPU support, KANN is too slow for most 2d-cnn in serious image analysis, it is fast enough for small- to mid-sized mlp, 1d-cnn and rnn. Code examples on char-nn etc can be found [here][example] with pretrained models in the [download page][release].

I developed KANN mainly to learn deep learning as I was fascinated by the power of char-nn and the elegance of autodiff. It is not intended to be a full-pledge framework like tensorflow or mxnet. Nonetheless, in its current state, KANN is already useful to some relatively simple applications and may help you to build portable programs. There have been many posts on implementing MLPs in C/C++. KANN is a long way ahead of them. It can do much more and is a lot faster with a still small code base.

[kann]: https://github.com/attractivechaos/kann/
[lstm-impl]: https://github.com/attractivechaos/kann/blob/94a68cd18c0cfb4c40b80f85ab3d579a8b9063a5/kann.c#L702
[example]: https://github.com/attractivechaos/kann/tree/master/examples
[release]: https://github.com/attractivechaos/kann/releases",2,22,False,self,,,,,
285,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,4,axp8w2,self.MachineLearning,"What's a CFO's Biggest Fear, and How can Machine Learning help?",https://www.reddit.com/r/MachineLearning/comments/axp8w2/whats_a_cfos_biggest_fear_and_how_can_machine/,andrea_manero,1551814829,http://www.datasciencecentral.com/profiles/blog/show?id=6448529%3ABlogPost%3A408236,0,1,False,self,,,,,
286,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,4,axpfll,self.MachineLearning,Cannot reshape X_train and X_val numpy arrays(orginally list of images),https://www.reddit.com/r/MachineLearning/comments/axpfll/cannot_reshape_x_train_and_x_val_numpy/,DemonKnight94,1551815777,"X\_train=np.array(X\_train).reshape(20000,64,64)

X\_val=np.array(X\_val).reshape(5001,64,64)

&amp;#x200B;

The error I am getting is:

172 X\_train=np.array(X\_train).reshape(20000,64,64)     

**173** X\_val=np.array(X\_val).reshape(5001,64,64)     

**174**  ValueError: cannot reshape array of size 20000 into shape (20000,64,64) ",0,1,False,self,,,,,
287,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,5,axpoyo,i.redd.it,Visualize neural network layer over/under-parameterization (saturation). Code at https://github.com/justinshenk/playground.,https://www.reddit.com/r/MachineLearning/comments/axpoyo/visualize_neural_network_layer/,justinshenk,1551817159,,1,1,False,default,,,,,
288,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,5,axpubt,i.redd.it,[P] Visualize neural network layer over/under-parameterization (saturation). Code at https://github.com/justinshenk/playground.,https://www.reddit.com/r/MachineLearning/comments/axpubt/p_visualize_neural_network_layer/,justinshenk,1551817970,,1,1,False,default,,,,,
289,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,6,axq6lu,self.MachineLearning,"[P] Albumentations, an image augmentation library version 0.2.0 released",https://www.reddit.com/r/MachineLearning/comments/axq6lu/p_albumentations_an_image_augmentation_library/,alexparinov,1551819767,"About 5 months ago [we released](https://www.reddit.com/r/MachineLearning/comments/9j6f9j/p_albumentations_a_fast_and_flexible_image/) an image augmentation library called Albumentations. Recently we updated the library to version 0.2.0. You can install the latest version by running `pip install -U albumentations` or downloading the library from [https://github.com/albu/albumentations](https://github.com/albu/albumentations)

&amp;#x200B;

**Keypoint transformations**

We added a long-awaited feature, support for the keypoint transformations and we made [an example notebook](https://github.com/albu/albumentations/blob/master/notebooks/example_keypoints.ipynb) that shows how to use them.

&amp;#x200B;

https://i.redd.it/n6xslslrlck21.jpg

&amp;#x200B;

**Apply the same transformation to the more than one target of the same type**

We also added an option to apply the same transformation to the more than one target of the same type. The possible use cases are image2image or stereo-image pipelines. So you can pass multiple images as input and all of them will be transformed in the same way using exactly the same parameters. See a more detailed description in [this notebook](https://github.com/albu/albumentations/blob/master/notebooks/example_multi_target.ipynb).

&amp;#x200B;

**Other improvements**

Also, we fixed some bugs and improved performance. Here are benchmarking results for Albumentations against other popular augmentation libraries (the best result for each transformation is shown in bold).

&amp;#x200B;

https://i.redd.it/4ztyn7r1uck21.png

&amp;#x200B;

**Using Albumentations to win data science competitions**

Albumentations is widely used on Kaggle and other platforms for machine learning competitions. Here is a list of people and teams who used the library to achieve top places on the leaderboard.

&amp;#x200B;

[**Airbus Ship Detection Challenge**](https://www.kaggle.com/c/airbus-ship-detection/leaderboard)

* 1st place: [Konstantin Gavrilchik](https://www.kaggle.com/dempton) and [Evgeny Kononenko](https://www.kaggle.com/lenny27)
* 2nd place: [Victor Durnov](https://www.kaggle.com/victorsd) and [Selim Seferbekov](https://www.kaggle.com/selimsef)
* 6th place: [Nick Sergievskiy](https://www.kaggle.com/nicksergievskiy), [Yauhen Babakhin](https://www.kaggle.com/ybabakhin) and [ZFTurbo](https://www.kaggle.com/zfturbo)

&amp;#x200B;

[**Quick, Draw! Doodle Recognition Challenge**](https://www.kaggle.com/c/quickdraw-doodle-recognition/leaderboard)

* 1st place: [Pavel Pleskov](https://www.kaggle.com/ppleskov) and [Pavel Ostyakov](https://www.kaggle.com/pavelost)
* 4th place: [Yauhen Babakhin](https://www.kaggle.com/ybabakhin), [Ivan Sosin](https://www.kaggle.com/sawseen), [ZFTurbo](https://www.kaggle.com/zfturbo), [Alex Parinov](https://www.kaggle.com/creafz) and [Roman Vlasov](https://www.kaggle.com/romavlasov)

&amp;#x200B;

[**TGS Salt Identification Challenge**](https://www.kaggle.com/c/tgs-salt-identification-challenge/leaderboard)

* 1st place: [Yauhen Babakhin](https://www.kaggle.com/ybabakhin) and [phalanx](https://www.kaggle.com/phalanx)
* 3rd place: [Victor Durnov](https://www.kaggle.com/victorsd) and [Selim Seferbekov](https://www.kaggle.com/selimsef)

&amp;#x200B;

[**Inclusive Images Challenge**](https://www.kaggle.com/c/inclusive-images-challenge/leaderboard)

* 1st place:  [Pavel Ostyakov](https://www.kaggle.com/pavelost)

&amp;#x200B;

[**Humpback Whale Identification**](https://www.kaggle.com/c/humpback-whale-identification/leaderboard)

* 5th place: [ZFTurbo](https://www.kaggle.com/zfturbo) and [Weimin Wang](https://www.kaggle.com/weimin)

&amp;#x200B;

[**Spacenet 4**](https://community.topcoder.com/longcontest/stats/?module=ViewOverview&amp;rd=17313)

* 3rd place: [Konstantin Maksimov](https://www.kaggle.com/maksimovka)

&amp;#x200B;

[**The 1st Tellus Satellite Challenge**](https://signate.jp/competitions/110/leaderboard)

* 3rd place: [Eugene Khvedchenya](https://www.kaggle.com/bloodaxe)

&amp;#x200B;

**Next version**

In the next version, we are planning to add an option to serialize / deserialize the augmentation pipeline to and from the JSON config files. This step will allow better reproducibility of the results achieved with the library.

&amp;#x200B;

If you found a bug, missing a feature, or just have problems understanding the documentation please [open an issue on GitHub](https://github.com/albu/albumentations/issues/new).",6,36,False,https://b.thumbs.redditmedia.com/iMBc1EM1p_46DhneVZwYCh5l8p5a3yqK25RQgkkjizE.jpg,,,,,
290,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,7,axr5br,github.com,Python For Business Repository - Machine Learning and Data Science,https://www.reddit.com/r/MachineLearning/comments/axr5br/python_for_business_repository_machine_learning/,OppositeMidnight,1551824875,,0,1,False,https://a.thumbs.redditmedia.com/1RfY9uvej6Z05VIdopzT25usvMtQ4bqFuDENmo9aIs4.jpg,,,,,
291,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,7,axrg0b,self.MachineLearning,[R] Meta-meta-learning for Neural Architecture Search through arXiv Descent,https://www.reddit.com/r/MachineLearning/comments/axrg0b/r_metametalearning_for_neural_architecture_search/,AntreasAntoniou,1551826516,"Dear r/MachineLearning friends,

Just wanted to share this *khhhm* *khhhm* paper, my research group produced last week. We'd love some feedback.

Regards, Antreas

Paper: https://www.bayeswatch.com/unofficial/meta-meta-learning.pdf

Tweet: https://twitter.com/_AntreasAntonio/status/1103007375693369344

",13,34,False,self,,,,,
292,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,8,axrl8b,blog.insightdatascience.com,Better Preference Predictions: Tunable and Explainable Recommender Systems,https://www.reddit.com/r/MachineLearning/comments/axrl8b/better_preference_predictions_tunable_and/,hszafarek,1551827302,,1,1,False,https://b.thumbs.redditmedia.com/Q9_u6X1HvHomZqiMQ9YFf1eZ7NvMnI2RlZq-V2RlosU.jpg,,,,,
293,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,8,axrv3r,self.MachineLearning,[D] Is the accelerating ML research an indication of a fast-start AGI scenario ?!,https://www.reddit.com/r/MachineLearning/comments/axrv3r/d_is_the_accelerating_ml_research_an_indication/,so_tiredso_tired,1551828857,"I am reading through ""Architects of AI"", barely a year old, a nice read, but already has a faint behind-the-time feel about it. Almost every interviewee has overseen significant new research. 

The Andrew Ng you-tube interviews, Heroes of Deep Learning, seem ancient by comparison. 

The pessimistic Nick Bostrom Superintelligence on the other hand, seems more realistic every time i browse it.

Is the accelerating pace of ML research any indication that AGI, if and when achieved, will probably advance much faster than expected ?

",13,0,False,self,,,,,
294,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,9,axs58y,i.redd.it,Three stages to show tool path. Milled on 5 axis.,https://www.reddit.com/r/MachineLearning/comments/axs58y/three_stages_to_show_tool_path_milled_on_5_axis/,jstikk,1551830559,,0,1,False,https://a.thumbs.redditmedia.com/K2mFNRXtrzlBr7Hw9migmRGxDvBzy8XSgiBcuJAS-90.jpg,,,,,
295,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,9,axs718,arxiv.org,[R] Variational Autoencoders Pursue PCA Directions (by Accident),https://www.reddit.com/r/MachineLearning/comments/axs718/r_variational_autoencoders_pursue_pca_directions/,inarrears,1551830881,,5,14,False,default,,,,,
296,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,9,axs9po,self.MachineLearning,"To speed up training, which is better to do, increase the learning rate or increase the batch size?",https://www.reddit.com/r/MachineLearning/comments/axs9po/to_speed_up_training_which_is_better_to_do/,RavitejaSunkavalli,1551831289,[removed],0,1,False,self,,,,,
297,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,9,axsmc4,self.MachineLearning,[P] StyleGAN trained on Portrait Art,https://www.reddit.com/r/MachineLearning/comments/axsmc4/p_stylegan_trained_on_portrait_art/,PuzzledProgrammer3,1551833399,"&amp;#x200B;

https://i.redd.it/rbvp8zrubek21.jpg",32,32,False,https://b.thumbs.redditmedia.com/WVy-ugRORfaNj3lw1WIW__RFhIMAtsY-zJXrysRaPLU.jpg,,,,,
298,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,9,axsoxi,self.MachineLearning,Can video recognition software only be as good as the quality of the labels of the dataset it is trained on?,https://www.reddit.com/r/MachineLearning/comments/axsoxi/can_video_recognition_software_only_be_as_good_as/,Solomonspin,1551833844,,1,1,False,self,,,,,
299,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,10,axt8f8,self.MachineLearning,O-GAN: Extremely Concise Approach for Auto-Encoding Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/axt8f8/ogan_extremely_concise_approach_for_autoencoding/,sujianlin,1551837217,[removed],0,1,False,self,,,,,
300,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,11,axteac,self.MachineLearning,https://github.com/extreme-assistant/cvpr2019,https://www.reddit.com/r/MachineLearning/comments/axteac/httpsgithubcomextremeassistantcvpr2019/,ExtremeMart,1551838214,[removed],0,1,False,self,,,,,
301,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,11,axtldc,self.MachineLearning,Anomaly Detection for Long Videos or Continuous Video Streams,https://www.reddit.com/r/MachineLearning/comments/axtldc/anomaly_detection_for_long_videos_or_continuous/,jjnewman,1551839422,[removed],0,2,False,self,,,,,
302,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,12,axtw7y,i.redd.it,Is an 85% confidence threshold for prediction common? I assumed 95% was industry standard. Source: 'Predictive Inequity in Object Detection' If so why is it 85% i dont see any reason why they used that in the paper.,https://www.reddit.com/r/MachineLearning/comments/axtw7y/is_an_85_confidence_threshold_for_prediction/,AromaticSuccess,1551841311,,1,1,False,https://a.thumbs.redditmedia.com/zkHcHmuAZyYYUQ53g6wFOCZluhKEwqIa_BHBe5dvPg8.jpg,,,,,
303,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,12,axuf91,self.MachineLearning,What's the tool for image annotation and segmentation in document extraction?,https://www.reddit.com/r/MachineLearning/comments/axuf91/whats_the_tool_for_image_annotation_and/,wuwei8967,1551844697,[removed],0,1,False,https://a.thumbs.redditmedia.com/JYvXx89onYf-PCciZ8FMMRFmFGZQQMT76rsxPRAO5g8.jpg,,,,,
304,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,13,axum55,self.MachineLearning,Xeus-Cling: Run C++ code in Jupyter Notebook,https://www.reddit.com/r/MachineLearning/comments/axum55/xeuscling_run_c_code_in_jupyter_notebook/,spmallick,1551845927,"Have you ever used a Jupyter notebook? If yes, you know it is a pleasure to use it for interactive programming. If no, you should try it! Or you may be a C++ programmer and thinking Jupyter notebooks are not for you, but wait, imagine our joy when we came across the Xeus-Cling kernel! But what does it do?

[https://www.learnopencv.com/xeus-cling-run-c-code-in-jupyter-notebook/](https://www.learnopencv.com/xeus-cling-run-c-code-in-jupyter-notebook/)

Read this blog post wherein we show how you can use OpenCV and Dlib C++ code in a Jupyter notebook using the Xeus-Cling kernel. Mention reviews and what you want us to work on next, in the comments!

![video](a38p2saycfk21)",0,1,False,self,,,,,
305,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,15,axvjy8,self.deeplearning,Want to model a conditional probability p(y|x) with NN? We report best practices for cond density estimation and compare against baseline density estimators typically used in finance.,https://www.reddit.com/r/MachineLearning/comments/axvjy8/want_to_model_a_conditional_probability_pyx_with/,whiletrue2,1551852534,,0,1,False,default,,,,,
306,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,15,axvld6,self.MachineLearning,What kind of category and tooling is this I'm trying to do?,https://www.reddit.com/r/MachineLearning/comments/axvld6/what_kind_of_category_and_tooling_is_this_im/,nmaxcom,1551852827,[removed],0,1,False,self,,,,,
307,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,15,axvuwu,i.redd.it,ML clustering,https://www.reddit.com/r/MachineLearning/comments/axvuwu/ml_clustering/,Slingerhd,1551854818,,0,1,False,https://a.thumbs.redditmedia.com/OgrvS9Zrlms7n4KXSYsyYB4MSkbflHtqa_85wvmJj48.jpg,,,,,
308,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,16,axw1jc,self.MachineLearning,30 years ago Brroks said that there's no 'silver bullet',https://www.reddit.com/r/MachineLearning/comments/axw1jc/30_years_ago_brroks_said_that_theres_no_silver/,liormessinger,1551856266,[removed],0,1,False,self,,,,,
309,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,16,axw4aw,arxiv.org,O-GAN: Extremely Concise Approach for Auto-Encoding Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/axw4aw/ogan_extremely_concise_approach_for_autoencoding/,sujianlin,1551856950,,7,7,False,default,,,,,
310,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,16,axw87t,self.MachineLearning,Computational Creativity: The Role of the Transformer,https://www.reddit.com/r/MachineLearning/comments/axw87t/computational_creativity_the_role_of_the/,laminarflow027,1551857899,"This pertains to a detailed [blog post](https://medium.com/sfu-big-data/computational-creativity-the-role-of-the-transformer-c3fa20da9c5f) I recently wrote for a course assignment with the same title. It's been really fascinating to read about how transformers are being adapted to perform a wide range of generative tasks (the recent storm created by GPT-2 and it's possible nefarious use notwithstanding).

In writing the blog post, I came across a host of interesting viewpoints on the pattern-matching skills of tools like GPT-2, such as [this one by Scott Alexander](https://slatestarcodex.com/2019/02/19/gpt-2-as-step-toward-general-intelligence/). I learned about things concepts from varied domains like [iambic pentameter](https://en.wikipedia.org/wiki/Iambic_pentameter), [self-similarity](https://en.wikipedia.org/wiki/Self-similarity) and the fact that humans are hardwired to find fractal patterns not only pleasing, but [also relaxing and stress-reducing](https://getpocket.com/a/read/1678304719)! If attention mechanisms can go so far only about a year after having been introduced, the coming years should be hugely exciting in terms of how long-range dependencies, structure and coherence in text, music and images can be modelled with more advanced architectures! 

I'd be curious to gain some additional perspectives on where transformers (or attention models) might go, in terms of their generative capabilities, and how humans might benefit from them.

As always, I'm looking to learn more and might have made some mistakes along the way, so my apologies if there are any glaring ones in the post!",0,1,False,self,,,,,
311,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,16,axwbtm,self.MachineLearning,[R] Tensorflow with android,https://www.reddit.com/r/MachineLearning/comments/axwbtm/r_tensorflow_with_android/,ibi_a,1551858820,"Hello People, i am looking for information about how to fuse a trained model in tensor flow with an Android app. Can you guys provide me of material or resources to find my answer? Thanks",3,0,False,self,,,,,
312,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,17,axwfol,lukeoakdenrayner.wordpress.com,[D] First impressions of the Stanford and MIT chest x-ray datasets (from Luke Oakden-Rayner),https://www.reddit.com/r/MachineLearning/comments/axwfol/d_first_impressions_of_the_stanford_and_mit_chest/,hooba_stank_,1551859748,,0,1,False,default,,,,,
313,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,17,axwrcc,self.MachineLearning,[D] Eugene Charniak wrote a book on Deep Learning,https://www.reddit.com/r/MachineLearning/comments/axwrcc/d_eugene_charniak_wrote_a_book_on_deep_learning/,IborkedyourGPU,1551862711,"Recently I stumbled upon

https://mitpress.mit.edu/books/introduction-deep-learning

I'm a little bit surprised. I wouldn't have pegged Charniak as a Deep Learning fan/expert. How is the book? It's very short (and correspondingly cheap), so clearly it must be fairly simple, and the ToC confirm my guess. But I have a few new hires I need to train, thus I was wondering if this could be a good resource. ",4,7,False,self,,,,,
314,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,18,axwuk2,self.MachineLearning,"""Attention is all you need"" - Inference, SoftMax or ArgMax?",https://www.reddit.com/r/MachineLearning/comments/axwuk2/attention_is_all_you_need_inference_softmax_or/,albert1905,1551863484,"Hi, I have some thought about the ""Transformer"" at inference (but can be applied for much more).
At inference time at the end of the network after the projection layer, are we doing Argmax or softmax? ArgMax just doing the job isn't he?
",0,1,False,self,,,,,
315,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,18,axx03m,inference.vc,DeepSets: Modeling Permutation Invariance,https://www.reddit.com/r/MachineLearning/comments/axx03m/deepsets_modeling_permutation_invariance/,alexeyr,1551864824,,0,1,False,default,,,,,
316,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,18,axx07r,self.MachineLearning,From Machine Learning to Machine Unlearning,https://www.reddit.com/r/MachineLearning/comments/axx07r/from_machine_learning_to_machine_unlearning/,andrea_manero,1551864853,[removed],0,1,False,self,,,,,
317,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,18,axx09x,inference.vc,[D] DeepSets: Modeling Permutation Invariance,https://www.reddit.com/r/MachineLearning/comments/axx09x/d_deepsets_modeling_permutation_invariance/,alexeyr,1551864867,,0,1,False,https://a.thumbs.redditmedia.com/-KxYmGCq_7MMxn6vRHo315RrtTUf-SRNB1BQcF5oLC8.jpg,,,,,
318,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,19,axx8tb,arxiv.org,[R] [1903.01611] The Lottery Ticket Hypothesis at Scale,https://www.reddit.com/r/MachineLearning/comments/axx8tb/r_190301611_the_lottery_ticket_hypothesis_at_scale/,evc123,1551866996,,2,10,False,default,,,,,
319,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,19,axx94e,self.MachineLearning,Own hosted machine learning models tutorial for privacy and personal usage,https://www.reddit.com/r/MachineLearning/comments/axx94e/own_hosted_machine_learning_models_tutorial_for/,sheatran29,1551867073,[removed],0,1,False,self,,,,,
320,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,19,axxeph,compakk.blogspot.com,Robopak - Shrink Wrapping and Shrink Tunnel Machine Manufacturers,https://www.reddit.com/r/MachineLearning/comments/axxeph/robopak_shrink_wrapping_and_shrink_tunnel_machine/,compak03,1551868372,,0,1,False,default,,,,,
321,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,19,axxi9r,datasciencedigest.org,DataScience Digest - Issue #17,https://www.reddit.com/r/MachineLearning/comments/axxi9r/datascience_digest_issue_17/,flyelephant,1551869178,,0,1,False,default,,,,,
322,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,21,axy689,self.MachineLearning,[P] PyTorch bindings for Rust and OCaml,https://www.reddit.com/r/MachineLearning/comments/axy689/p_pytorch_bindings_for_rust_and_ocaml/,l-m-z,1551874457,"Github links: [tch-rs](https://github.com/LaurentMazare/tch-rs), [ocaml-torch](https://github.com/LaurentMazare/ocaml-torch).

We recently released some PyTorch bindings for both Rust and OCaml. Both bindings provide a NumPy like tensor library with GPU acceleration and support for automatic differentiation.
Some examples can be found in the github repos, showing how to train various models like some ResNet variants on CIFAR-10, RNNs using text data, etc. In the OCaml case there are also a couple GAN examples as well as some reinforcement learning examples (DQN and A2C) running on atari with a [small write-up](https://blog.janestreet.com/playing-atari-games-with-ocaml-and-deep-rl/).

I enjoy a lot the PyTorch Python api, it's very neat and flexible. These bindings don't claim to compete with Python as the best way to develop/train models. Still they will hopefully provide a better integration of PyTorch for these languages and may also get useful in some use cases like writing very fast extensions.
",23,95,False,self,,,,,
323,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,21,axy8jr,self.MachineLearning,How to detect text blocks in document images,https://www.reddit.com/r/MachineLearning/comments/axy8jr/how_to_detect_text_blocks_in_document_images/,DGs29,1551874908,[removed],0,1,False,self,,,,,
324,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,21,axy95y,self.MachineLearning,[D] How to detect text blocks in document images,https://www.reddit.com/r/MachineLearning/comments/axy95y/d_how_to_detect_text_blocks_in_document_images/,DGs29,1551875036,"I want to detects blocks of text in document images like this.

&amp;#x200B;

https://i.redd.it/skmiggujrhk21.png

Detecting the texts can be based on visual approach using white-spaces in between as separators whereas the text that lies close to each other should be considered as block.

Is there any algorithm available for this type of segmentation. How can I prepare my CNN model to achieve this?",31,16,False,self,,,,,
325,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,21,axybpc,self.deeplearning,Amd EPYC + NVIDIA GPUs?,https://www.reddit.com/r/MachineLearning/comments/axybpc/amd_epyc_nvidia_gpus/,doyer,1551875548,,0,1,False,default,,,,,
326,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,22,axyllg,self.MachineLearning,"[D] Best Deep Learning Framework for Android/iOS in 2019? (Tflite, OpenCV, ...?)",https://www.reddit.com/r/MachineLearning/comments/axyllg/d_best_deep_learning_framework_for_androidios_in/,fzyzcjy,1551877469,"Hi! What do you think is the *best deep learning inference framework for Android / iOS* ***in 2019****? (e.g. Tflite, opencv, ncnn, mace, ...?)*

P.S. I know there are some discussions, but it seems that they are not very up-to-date.   
 Thanks for any advice!",8,5,False,self,,,,,
327,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,22,axymqz,tomshardware.com,[N] Google's Edge TPU Machine Learning Chip Debuts in Raspberry Pi-Like Dev Board,https://www.reddit.com/r/MachineLearning/comments/axymqz/n_googles_edge_tpu_machine_learning_chip_debuts/,mllosab,1551877687,,0,1,False,default,,,,,
328,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,22,axyru1,reuseresearch.com,[N] Reproducing experimental results at the Conference on Systems and Machine Learning (SysML'19),https://www.reddit.com/r/MachineLearning/comments/axyru1/n_reproducing_experimental_results_at_the/,gfursin,1551878599,,0,1,False,default,,,,,
329,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,22,axyu1u,self.MachineLearning,implement building damage detection.,https://www.reddit.com/r/MachineLearning/comments/axyu1u/implement_building_damage_detection/,Heartfiglia,1551878982,[removed],0,1,False,self,,,,,
330,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,22,axz3qr,self.MachineLearning,[N] Google releases Raspberry-like dev board with TPU,https://www.reddit.com/r/MachineLearning/comments/axz3qr/n_google_releases_raspberrylike_dev_board_with_tpu/,csiz,1551880682,"Link to the product page: https://coral.withgoogle.com/products/

Priced at 150$ is fairly decent, but I couldn't spot any specs on the TPU. Does anyone know how it would compare to a desktop GPU?
",44,95,False,self,,,,,
331,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,23,axz6yy,self.MachineLearning,The state of AI in 2019,https://www.reddit.com/r/MachineLearning/comments/axz6yy/the_state_of_ai_in_2019/,eimisas,1551881218,[removed],0,1,False,self,,,,,
332,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,23,axz8nf,self.MachineLearning,[R] Coupled nonlinear delay systems as deep convolutional neural networks,https://www.reddit.com/r/MachineLearning/comments/axz8nf/r_coupled_nonlinear_delay_systems_as_deep/,p_bogdan,1551881502,"Hello!

I would like to introduce our work towards neuromorphic computing hardware. Neuromorphic computing is an alternative vision on the machine learning architectures, inspired by the biological brain. We discuss a dynamical-system based deep learning implementation using coupled delayed-feedback oscillators as a computing substrate.

[https://www.researchgate.net/publication/331197606\_Coupled\_nonlinear\_delay\_systems\_as\_deep\_convolutional\_neural\_networks](https://www.researchgate.net/publication/331197606_Coupled_nonlinear_delay_systems_as_deep_convolutional_neural_networks)",0,6,False,self,,,,,
333,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,23,axz8ro,self.MachineLearning,[R] A list of network embedding (node embedding) research papers with implementations.,https://www.reddit.com/r/MachineLearning/comments/axz8ro/r_a_list_of_network_embedding_node_embedding/,benitorosenberg,1551881523,"&amp;#x200B;

[https://github.com/chihming/awesome-network-embedding](https://github.com/chihming/awesome-network-embedding)",1,21,False,self,,,,,
334,MachineLearning,t5_2r3gv,2019-3-6,2019,3,6,23,axzk1l,github.com,[N] Microsoft Releases New Version of Open Source Distributed ML Library,https://www.reddit.com/r/MachineLearning/comments/axzk1l/n_microsoft_releases_new_version_of_open_source/,mhamilton723,1551883410,,0,3,False,default,,,,,
335,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,0,ay03is,self.MachineLearning,Help regarding Notations,https://www.reddit.com/r/MachineLearning/comments/ay03is/help_regarding_notations/,apoorvagni,1551886466,[removed],0,1,False,self,,,,,
336,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,0,ay05ke,self.MachineLearning,Can a person with a undergrad Comp Sci degree get a great job as a machine learning engineer ?,https://www.reddit.com/r/MachineLearning/comments/ay05ke/can_a_person_with_a_undergrad_comp_sci_degree_get/,karanchahal1996,1551886788,[removed],0,1,False,self,,,,,
337,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,0,ay092f,ai.googleblog.com,"Google open-sources GPipe, a library for efficiently training large deep neural networks",https://www.reddit.com/r/MachineLearning/comments/ay092f/google_opensources_gpipe_a_library_for/,dabbistify,1551887314,,0,1,False,default,,,,,
338,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,0,ay0anr,self.MachineLearning,"Simple Questions Thread March 06, 2019",https://www.reddit.com/r/MachineLearning/comments/ay0anr/simple_questions_thread_march_06_2019/,AutoModerator,1551887559,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!
",0,1,False,self,,,,,
339,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,0,ay0ctc,reddit.com,I.C. Engines,https://www.reddit.com/r/MachineLearning/comments/ay0ctc/ic_engines/,Wikihub,1551887891,,0,1,False,https://b.thumbs.redditmedia.com/vFVEzVbvurbE-CrkQ9ycpClXH3fwFNQGm9Um3C_DK2I.jpg,,,,,
340,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,1,ay0fn5,azure.microsoft.com,Has anyone tried Azure Machine Learning before? What is it?,https://www.reddit.com/r/MachineLearning/comments/ay0fn5/has_anyone_tried_azure_machine_learning_before/,Vadikus,1551888295,,1,1,False,default,,,,,
341,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,1,ay0frr,self.MachineLearning,What's a good strategy for going through Machine Learning textbooks?,https://www.reddit.com/r/MachineLearning/comments/ay0frr/whats_a_good_strategy_for_going_through_machine/,machinelearning365,1551888315,[removed],0,1,False,self,,,,,
342,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,1,ay0jdc,medium.com,[P] Reinforced Cross-Modal Matching &amp; Self-Supervised Imitation Learning for Vision-Language Navigation,https://www.reddit.com/r/MachineLearning/comments/ay0jdc/p_reinforced_crossmodal_matching_selfsupervised/,gwen0927,1551888823,,0,1,False,https://a.thumbs.redditmedia.com/WIrRbAJp1GhyYR5wT8Kl6AbmuWRXKd8HtQ1VQnorzk8.jpg,,,,,
343,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,1,ay0jhq,self.MachineLearning,Working of centrifugal pump,https://www.reddit.com/r/MachineLearning/comments/ay0jhq/working_of_centrifugal_pump/,Wikihub,1551888844," Centrifugal pumps is a hydraulic machines which convert the mechanical energy into a hydraulic energy are called Pump. The hydraulic energy is in the form of pressure energy. If the mechanical energy is converted into a pressure energy by means of centrifugal force acting on the fluid, The hydraulic machine is called centrifugal pump.  


* **Differentparts of centrifugal pump:-**

1. **Impeller.**
2. **Casing.**
3. **Suction pipe.**
4. **Delivery pipe.**

 These are the different parts of centrifugal pump, by which mechanical energy is converted into a hydraulic energy.  


* **Impeller:-** The rotating part of a centrifugal pump is called Impeller. It consists of series of backward curved vanes. The impeller is mounted on a shaft is connected to the shaft of an electric motor.
* **Casing:-** The casing of a centrifugal pump is look like of casing of reaction turbine. It is an air tight passage surrounding the impeller and is designed in such a way that the kinetic energy of the water discharged at the outlet of the impeller is converted into pressure energy before the water leaves the casing and enters the delivery pipe.

          In centrifugal pumps there are mainly three types of casing is used:-

1. **Volute casing.**
2. **Vortex casing.**
3. **Guide blade casing.**

**For more information please visit to our website:-**

[**https://www.wikihubs24.info/2019/03/working-principle-of-centrifugal-pump.html**](https://www.wikihubs24.info/2019/03/working-principle-of-centrifugal-pump.html)",0,1,False,self,,,,,
344,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,1,ay0opg,medium.com,Triple Strong Accept for CVPR 2019: Reinforced Cross-Modal Matching &amp; Self-Supervised Imitation,https://www.reddit.com/r/MachineLearning/comments/ay0opg/triple_strong_accept_for_cvpr_2019_reinforced/,Yuqing7,1551889595,,0,1,False,https://a.thumbs.redditmedia.com/WIrRbAJp1GhyYR5wT8Kl6AbmuWRXKd8HtQ1VQnorzk8.jpg,,,,,
345,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,1,ay0yyp,self.MachineLearning,How to use KNN for classification?,https://www.reddit.com/r/MachineLearning/comments/ay0yyp/how_to_use_knn_for_classification/,armod_reddit,1551891122,"For reproducibility reasons, I am sharing the simple datasets I am using  [here](https://drive.google.com/open?id=16kzEQd-QqEvRupyxV2UixoHfc1tpI5GI). 

&amp;#x200B;

In these datasets - from column 2, I am reading the current row and compare it with the value of the previous row. If it is greater, I keep comparing. If the current value is smaller than the previous row's value, I want to divide the current value (smaller) by the previous value (larger). Accordingly, the following code:

&amp;#x200B;

&amp;#x200B;

`import numpy as np`

`import matplotlib.pyplot as plt`

`from sklearn.neighbors import KNeighborsClassifier`

`from sklearn.model_selection import train_test_split`

`from sklearn.decomposition import PCA`



`protocols = {}`



`types = {""data_c"": ""data_c.csv"", ""data_r"": ""data_r.csv"", ""data_v"": ""data_v.csv"", ""data_g"": ""data_v.csg""}`



`for protname, fname in types.items():`

`col_time,col_window = np.loadtxt(fname,delimiter=',').T`

`trailing_window = col_window[:-1] # ""past"" values at a given index`

`leading_window  = col_window[1:]  # ""current values at a given index`

`decreasing_inds = np.where(leading_window &lt; trailing_window)[0]`

`quotient = leading_window[decreasing_inds]/trailing_window[decreasing_inds]`

`quotient_times = col_time[decreasing_inds]`



`protocols[protname] = {`

`""col_time"": col_time,`

`""col_window"": col_window,`

`""quotient_times"": quotient_times,`

`""quotient"": quotient,`

`}`

\`data\_c\` has only one \*\*unique\*\* \`quotient\` value of \`0.7\`, \`data\_r\` has a unique \`quotient\` value of \`0.5\`, \`data\_g\` has a \`quotient\` value of greater than 0.9. However, \`data\_v\` has two unique \`quotient\` values (either \`0.5\`and \`0.8\` (the first quotient)). I wanted to fit these quotients as a 2-D and classify them as the following diagram. What I am trying to do in the following diagram is, if we take the first and last quotient values, (if the first and last quotients are 0.5, it will be \`data\_r\`. If the first and last quotients are 0.7, it will be \`data\_c\`. If the first quotient is 0.8 and if the last quotient is 0.5, then it will be \`data\_v\`. If both the first and last quotients are greater than 0.9, then it will be \`data\_g\`).

&amp;#x200B;

![img](cfxdvkws2jk21)

&amp;#x200B;



How can we perform this using KNN (K-Nearest Neighbor) classification technique? Any help will be greatly appreciated. 

",0,1,False,self,,,,,
346,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,1,ay0zvy,youtu.be,[R] Computational mediation: Using artificial intelligence for conflict resolution,https://www.reddit.com/r/MachineLearning/comments/ay0zvy/r_computational_mediation_using_artificial/,sanity,1551891261,,0,5,False,default,,,,,
347,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,2,ay14vn,ai.googleblog.com,Exploring Neural Networks with Activation Atlases,https://www.reddit.com/r/MachineLearning/comments/ay14vn/exploring_neural_networks_with_activation_atlases/,sjoerdapp,1551891968,,0,1,False,default,,,,,
348,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,2,ay15lu,self.MachineLearning,Need help understand positional embedding/encoding in transformer related models,https://www.reddit.com/r/MachineLearning/comments/ay15lu/need_help_understand_positional_embeddingencoding/,newpro_git,1551892070,[removed],0,1,False,self,,,,,
349,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,2,ay1baf,self.MachineLearning,Pelton turbine,https://www.reddit.com/r/MachineLearning/comments/ay1baf/pelton_turbine/,Wikihub,1551892902," 

* **Turbines:-**

        Turbine are defined as the hydraulic machines which convert hydraulic energy into mechanical energy. This mechanical energy is used in running an electric generator which is directly coupled to the shaft of the turbine. Thus the mechanical energy is converted into electrical energy. The electric power which is obtained from the hydraulic energy is known as hydroelectric power.At present the generation of hydroelectric power is the cheapest as compared by the power generated by other sources such as oil, coal etc.  


* **Pelton wheel (Or turbine):-**

  The pelton wheel or pelton turbine is a tangential flow impulse turbine. The water strikes the bucket along the tangent of the runner. The energy available at the inlet of the turbine is only kinetic energy. The pressure at the inlet and outlet of the turbine is atmospheric. This turbine is used for high heads.         The water from the reservoir flows through the penstocks at the outlet of which a nozzle is fitted. The nozzle increases the kinetic energy of water following through the penstock. At the outlet of the nozzle, the water comes out in the form of a jet and strikes the buckets (vanes) of the runner. The main parts of the pelton turbine are: 

![img](u1xhj3wn8jk21 ""Turbine"")

&amp;#x200B;

For more information please visit to our website:-

[https://www.wikihubs24.info/2019/02/pelton-wheel-turbine.html](https://www.wikihubs24.info/2019/02/pelton-wheel-turbine.html)",0,1,False,self,,,,,
350,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,2,ay1bti,distill.pub,[R] Exploring Neural Networks with Activation Atlases,https://www.reddit.com/r/MachineLearning/comments/ay1bti/r_exploring_neural_networks_with_activation/,pmigdal,1551892977,,0,1,False,default,,,,,
351,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,2,ay1fx7,self.MachineLearning,[R] Exploring Neural Networks with Activation Atlases,https://www.reddit.com/r/MachineLearning/comments/ay1fx7/r_exploring_neural_networks_with_activation/,chisai_mikan,1551893554,"New distill.pub [post](https://distill.pub/2019/activation-atlas/):

*By using feature inversion to visualize millions of activations from an image classification network, we create an explorable activation atlas of features the network has learned which can reveal how the network typically represents some concepts.*

https://distill.pub/2019/activation-atlas/",23,113,False,self,,,,,
352,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,2,ay1mv2,fluxml.ai,Reinforcement Learning vs. Differentiable Programming,https://www.reddit.com/r/MachineLearning/comments/ay1mv2/reinforcement_learning_vs_differentiable/,Bdamkin54,1551894535,,0,1,False,default,,,,,
353,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,2,ay1q4b,reddit.com,[D] Is CEM (Cross-Entropy Method) gradient-free?,https://www.reddit.com/r/MachineLearning/comments/ay1q4b/d_is_cem_crossentropy_method_gradientfree/,MasterScrat,1551894986,,1,1,False,default,,,,,
354,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,3,ay1tnj,self.MachineLearning,I want to pursue a career in Machine Learning+Python. Help me out.,https://www.reddit.com/r/MachineLearning/comments/ay1tnj/i_want_to_pursue_a_career_in_machine/,agarwalkunal12,1551895476,Basically I am working on Java in an IT company (1st job) but the job isn't mentally rewarding and I've spent about &lt;2 years slogging. I was fascinated by ML so I did the Andrew Ng course on Coursera with Octave and learnt basic Python in my spare time. Currently I've started to learn NumPy (read somewhere to begin with it). I want to get out of my current job to switch over to ML but I want to be prepared to apply/sit for interviews which I'm clearly not qualified for right now. What should I do next to be able to do so in the next 6 months or a little longer? ,0,1,False,self,,,,,
355,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,3,ay2058,blockdelta.io,Deep Learning in Finance Summit - London.,https://www.reddit.com/r/MachineLearning/comments/ay2058/deep_learning_in_finance_summit_london/,BlockDelta,1551896424,,0,1,False,default,,,,,
356,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,3,ay2bti,github.com,Tensorflow 2.0 alpha release,https://www.reddit.com/r/MachineLearning/comments/ay2bti/tensorflow_20_alpha_release/,johnny____,1551898082,,0,2,False,default,,,,,
357,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,3,ay2dxm,self.MachineLearning,Mixed-Initiative 2D Browser-Based Level Editor,https://www.reddit.com/r/MachineLearning/comments/ay2dxm/mixedinitiative_2d_browserbased_level_editor/,Sknoot,1551898389,Hello my name is Tristan and I am currently conducting a pilot study into the affects of a Mixed Initiative level design tool. In this experiment you can create and play test your own levels. If you would like to test out my tool here is the [Link to the Project](https://tristanbarlowgriffin.co.uk/Dissertation). Thanks!!! ,0,1,False,self,,,,,
358,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,4,ay2tcz,self.MachineLearning,[R] The Self-Monitoring and Regretful Agent for Vision-and-Language Navigation (ICLR 2019 &amp; CVPR 2019 Oral),https://www.reddit.com/r/MachineLearning/comments/ay2tcz/r_the_selfmonitoring_and_regretful_agent_for/,chihyaoma,1551900586,"Hi,

&amp;#x200B;

I am a Ph.D. student at Georgia Tech. We would like to share the code of our recent work on Vision-and-Language Navigation with you :)

We developed a self-monitoring mechanism for the vision-and-language navigation agent (ICLR) and leverage it to further develop a new end-to-end learned rollback mechanism for a backtracking agent (CVPR). 

&amp;#x200B;

**The Regretful Agent: Heuristic-Aided Navigation through Progress Estimation**  
CVPR 2019 (Oral)  
GitHub: [https://github.com/chihyaoma/regretful-agent](https://github.com/chihyaoma/regretful-agent)  
arXiv: [https://arxiv.org/abs/1903.01602](https://arxiv.org/abs/1903.01602)  


**Self-Monitoring Navigation Agent via Auxiliary Progress Estimation**  
ICLR 2019  
GitHub: [https://github.com/chihyaoma/selfmonitoring-agent](https://github.com/chihyaoma/selfmonitoring-agent)  
arXiv: [https://arxiv.org/abs/1901.03035](https://arxiv.org/abs/1901.03035)",0,14,False,self,,,,,
359,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,4,ay2ybj,self.MachineLearning,[R] computational mediation using artificial,https://www.reddit.com/r/MachineLearning/comments/ay2ybj/r_computational_mediation_using_artificial/,impossibleteams,1551901290,,1,0,False,self,,,,,
360,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,4,ay3021,self.MachineLearning,Become a part of our team - NeuroArt.ai,https://www.reddit.com/r/MachineLearning/comments/ay3021/become_a_part_of_our_team_neuroartai/,lavabar,1551901521,"Hello everybody! We are looking for really ambitious people who wants to do deep learning and use GANs for creating art-like images. Truth to be told we want to make real art and make everyone happy!

After we got well enough results we would sell it to everyone (in every form you want, even in printed on the wall or canvas).

[NeuroArt.ai](https://NeuroArt.ai) will change everything in art sphere. Join us and win with us!

Ready for your questions and feed backs!)

Visit our facebook page because updates will go very soon:

[facebook.com/neuroartai](https://facebook.com/neuroartai)",0,1,False,self,,,,,
361,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,5,ay3s6j,fluxml.ai,[R] Flux  Reinforcement Learning vs. Differentiable Programming,https://www.reddit.com/r/MachineLearning/comments/ay3s6j/r_flux_reinforcement_learning_vs_differentiable/,SkiddyX,1551905547,,1,1,False,https://b.thumbs.redditmedia.com/dDcUCWolNCQ3ZBTv6QYe4du6taD-W2wcm1_NSPk9rrk.jpg,,,,,
362,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,6,ay3z53,github.com,GIPHY open sources their custom celebrity detection ML model and code,https://www.reddit.com/r/MachineLearning/comments/ay3z53/giphy_open_sources_their_custom_celebrity/,giphy,1551906510,,0,2,False,https://a.thumbs.redditmedia.com/h_x1mqdr6ftdMdxxoStQgkN8RC4FIXPSZKxi7zl6JD8.jpg,,,,,
363,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,6,ay40on,self.MachineLearning,[Project] I Implemented A GAN With r1 Gradient Penalty and ResNet Blocks using Keras,https://www.reddit.com/r/MachineLearning/comments/ay40on/project_i_implemented_a_gan_with_r1_gradient/,manicman1999,1551906735,"I've yet to have the patience to train it the full 500 thousand steps as was done in the original paper, but I've gotten decent samples regardless!

&amp;#x200B;

Flowers at 83k steps:

[https://imgur.com/wiMMxIN](https://imgur.com/wiMMxIN)

&amp;#x200B;

EarthPorn at 200k steps:

[https://imgur.com/2ajHhqV](https://imgur.com/2ajHhqV)

&amp;#x200B;

Bob Ross paintings (EarthPorn at 200k steps, Transferred to Bob Ross with 10k steps):

[https://imgur.com/WvyLNza](https://imgur.com/WvyLNza)

&amp;#x200B;

The code is on github here:

[https://github.com/manicman1999/GP-GAN](https://github.com/manicman1999/GP-GAN)

&amp;#x200B;

I hope you guys enjoy!!",10,24,False,self,,,,,
364,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,6,ay413m,self.MachineLearning,[P] Learning and Detecting Low-Resolution Objects (Anti-Super Resolution),https://www.reddit.com/r/MachineLearning/comments/ay413m/p_learning_and_detecting_lowresolution_objects/,karstenchu,1551906791,"I just wanted to run an idea I had by some ML experts.  

&amp;#x200B;

Say I have a gigapixel digital photograph and a digital representation of a polaroid from the 80's.  Could I use high resolution/low resolution pairs of images to train a network to recognize objects in the low resolution image?  My methodology would be to go through both images, chip out all instances of say cars, and then train an ANN to predict what the ""polaroid"" version of the high resolution image would look like.  Ultimately, I'd like to be able to then use segmentation to go through polaroids and detect cars.

&amp;#x200B;

Does this make sense, is it feasible and what am I missing?  It's kind of like downsampling, except there's also a difference in sensor phenomenology I'd like the network to learn.  ",1,3,False,self,,,,,
365,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,6,ay48oi,medium.com,NeurIPS 2019 Dates and Details Announced,https://www.reddit.com/r/MachineLearning/comments/ay48oi/neurips_2019_dates_and_details_announced/,Yuqing7,1551907822,,0,1,False,https://b.thumbs.redditmedia.com/yPb4vZwtJmiXXEQFqqBB0p3vdos2EyhzfQB6GfMFVVQ.jpg,,,,,
366,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,6,ay4948,self.MachineLearning,Machine Learning Models Predicting Only Positive Outcomes,https://www.reddit.com/r/MachineLearning/comments/ay4948/machine_learning_models_predicting_only_positive/,ft_43,1551907885,[removed],0,1,False,self,,,,,
367,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,6,ay4f4c,self.MachineLearning,[D] Question regarding ML/CV researcher salary in UK (London),https://www.reddit.com/r/MachineLearning/comments/ay4f4c/d_question_regarding_mlcv_researcher_salary_in_uk/,gamenecis,1551908723," 

Hi all,

Since I just finished my PhD program in UK, graduating recently, I am about to start looking for a job in a bit, basically as soon as my short break ends (yay! my first real holidays in the last 3-4 years).

While I was oscillating between academia and industry, in the end, given that I am mostly interested in continuing doing research I decided that I may as well take the better payed option.

That being said, I am unsure what salary to expect  for a research scientist position in London (I am looking mostly at the big companies from there). What salaries could I expect? What is the typical hiring process for such positions?

(As a little background, during my PhD I published around 10 papers in ICCV/NIPS/CVPR, thought I did my PhD at a lesser known university from UK. I am not sure if this factor(s) matter however.)

Thanks everyone!",6,5,False,self,,,,,
368,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,6,ay4i5c,self.MachineLearning,How do I decide on lambda values in multi-loss function in Deep learning model?,https://www.reddit.com/r/MachineLearning/comments/ay4i5c/how_do_i_decide_on_lambda_values_in_multiloss/,falmasri,1551909161,[removed],0,1,False,self,,,,,
369,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,6,ay4knd,github.com,Python For Business Repository - Machine Learning and Data Science,https://www.reddit.com/r/MachineLearning/comments/ay4knd/python_for_business_repository_machine_learning/,OppositeMidnight,1551909527,,0,1,False,https://a.thumbs.redditmedia.com/1RfY9uvej6Z05VIdopzT25usvMtQ4bqFuDENmo9aIs4.jpg,,,,,
370,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,7,ay50ad,self.MachineLearning,[R] Learning Latent Plans from Play (Google Brain/X),https://www.reddit.com/r/MachineLearning/comments/ay50ad/r_learning_latent_plans_from_play_google_brainx/,inarrears,1551911736,"[Learning Latent Plans from Play](https://learning-from-play.github.io/)

TL;DR *They show a single agent, after self-supervising on 3 hours of play data, can generalize to 18 zero-shot manipulation tasks with 85% success.*

Cool work that comes with with both a web article version (using the classic version of the V1 distill.pub template) and an arxiv pdf for offline reading:

web article: https://learning-from-play.github.io/

arxiv paper: https://arxiv.org/abs/1903.01973

**Abstract**

*We propose learning from teleoperated play data (LfP) as a way to scale up multi-task robotic skill learning. Learning from play (LfP) offers three main advantages: 1) It is cheap. Large amounts of play data can be collected quickly as it does not require scene staging, task segmenting, or resetting to an initial state. 2) It is general. It contains both functional and non-functional behavior, relaxing the need for a predefined task distribution. 3) It is rich. Play involves repeated, varied behavior and naturally leads to high coverage of the possible interaction space. These properties distinguish play from expert demonstrations, which are rich, but expensive, and scripted unattended data collection, which is cheap, but insufficiently rich. Variety in play, however, presents a multimodality challenge to methods seeking to learn control on top. To this end, we introduce Play-LMP, a method designed to handle variability in the LfP setting by organizing it in an embedding space. Play-LMP jointly learns 1) reusable latent plan representations unsupervised from play data and 2) a single goal-conditioned policy capable of decoding inferred plans to achieve user-specified tasks. We show empirically that Play-LMP, despite not being trained on task-specific data, is capable of generalizing to 18 complex user-specified manipulation tasks with average success of 85.5%, outperforming individual models trained on expert demonstrations (success of 70.3%). Furthermore, we find that play-supervised models, unlike their expert-trained counterparts, 1) are more robust to perturbations and 2) exhibit retrying-till-success. Finally, despite never being trained with task labels, we find that our agent learns to organize its latent plan space around functional tasks. Videos of the performed experiments are available at https://learning-from-play.github.io*",4,11,False,self,,,,,
371,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,7,ay58ng,self.MachineLearning,[D] Licensing practices in ML,https://www.reddit.com/r/MachineLearning/comments/ay58ng/d_licensing_practices_in_ml/,rbkillea,1551913005,"Hi, the purpose of this post is to highlight what I see as a disturbing trend in ""open source"" ML code. Paper authors at industrial labs feel the pressure to make their code open to get their papers accepted. At the same time, they feel the pressure to license their code in a somewhat restrictive way (and to patent their techniques). What's more, they feel pressured to finish their work quickly, which leads to copy + pasting. These three forces in conjunction lead to intellectual dishonesty.

To make my point concrete I will give an example (3 come to mind, but the point of this post isn't to stir stuff up but rather to have a dialog): jpeg2dct from Uber - see the naming in [read_dct_coefficients](https://github.com/uber-research/jpeg2dct/blob/master/jpeg2dct/common/dctfromjpg.cc#L65) and that employed by blog author [Elsab Ros](https://aessedai101.github.io/c++/jpeg/jpg/dct/libjpeg/2014/07/10/extracting-jpeg-dct-coefficients.html). While I am sure they will argue that only a small fraction is unattributed copying, that's the fraction that does the actual task the library aims to accomplish.",0,0,False,self,,,,,
372,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,7,ay58ps,self.MachineLearning,Travel grant for ML conferences,https://www.reddit.com/r/MachineLearning/comments/ay58ps/travel_grant_for_ml_conferences/,athrowaway0909,1551913014,[removed],0,1,False,self,,,,,
373,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,8,ay5t3n,self.MachineLearning,[Q] What is your experience with U-NET?,https://www.reddit.com/r/MachineLearning/comments/ay5t3n/q_what_is_your_experience_with_unet/,denfromufa,1551916076,"I'm trying to get more experience with U-NET architecture and get more understanding about why it works so well for pen-sharpening, medical imaging, and semantic segmentation problems?",0,1,False,self,,,,,
374,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,9,ay67br,lambdalabs.com,GPU Deep Learning Benchmarks: 2080Ti vs. V100 vs. Titan RTX vs. 2080 vs. Titan V vs. 1080Ti vs. Titan Xp,https://www.reddit.com/r/MachineLearning/comments/ay67br/gpu_deep_learning_benchmarks_2080ti_vs_v100_vs/,mippie_moe,1551918402,,0,1,False,default,,,,,
375,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,9,ay6amu,self.MachineLearning,Can you increase the resolution an image by using multiple offset images and an ML algorithm? More in description.,https://www.reddit.com/r/MachineLearning/comments/ay6amu/can_you_increase_the_resolution_an_image_by_using/,ALLIRIX,1551918926,[removed],0,1,False,self,,,,,
376,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,9,ay6cqx,self.MachineLearning,GPU Deep Learning Benchmarks: 2080Ti vs. V100 vs. Titan RTX vs. 2080 vs. Titan V vs. 1080Ti vs. Titan Xp,https://www.reddit.com/r/MachineLearning/comments/ay6cqx/gpu_deep_learning_benchmarks_2080ti_vs_v100_vs/,mippie_moe,1551919275,"[https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/](https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/)

&amp;#x200B;

The post highlights performance of RTX 2080 Ti. The 2080 Ti definitely seems appears to be the best from a price / performance perspective.

&amp;#x200B;

Some excerpts:

&amp;#x200B;

For FP32 training of neural networks, the RTX 2080 Ti is...

* 37% faster than RTX 2080
* 35% faster than GTX 1080 Ti
* 22% faster than Titan XP
* 96% as fast as Titan V
* 87% as fast as Titan RTX
* 73% as fast as Tesla V100 (32 GB)

&amp;#x200B;

For FP16 training of neural networks, the RTX 2080 Ti is..

* 72% faster than GTX 1080 Ti
* 59% faster than Titan XP
* 32% faster than RTX 2080
* 81% as fast as Titan V
* 71% as fast as Titan RTX
* 55% as fast as Tesla V100 (32 GB)

&amp;#x200B;

Prices:

* RTX 2080 Ti: $1,199.00
* RTX 2080: $799.00
* Titan RTX: $2,499.00
* Titan V: $2,999.00
* Tesla V100 (32 GB): \~$8,200.00
* GTX 1080 Ti: $699.00
* Titan Xp: $1,200.00

&amp;#x200B;",0,1,False,self,,,,,
377,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,9,ay6i8p,self.MachineLearning,[D] GPU Deep Learning Performance: V100 vs. RTX 2080 Ti vs. Titan RTX vs. RTX 2080 vs. Titan V vs. GTX 1080 Ti vs. Titan Xp,https://www.reddit.com/r/MachineLearning/comments/ay6i8p/d_gpu_deep_learning_performance_v100_vs_rtx_2080/,mippie_moe,1551920201,"[https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/](https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/)

&amp;#x200B;

The post highlights deep learning performance of RTX 2080 Ti in TensorFlow. The 2080 Ti appears to be the best from a price / performance perspective. Some highlights:

&amp;#x200B;

**V100 vs. RTX 2080 Ti**

* RTX 2080 Ti is 87% as fast as the Tesla V100 for FP32 training. 
* RTX 2080 Ti is 55% as fast as Tesla V100 for FP16 training.
* RTX 2080 Ti is $1,199 vs. Tesla V100 is $8,000+.

&amp;#x200B;

**FP16 vs. FP32 of RTX 2080 Ti**

* Training in FP16 vs. FP32 has big performance benefit: +45% training speed. 

&amp;#x200B;

**Multi-GPU Scaling:**

* Scaling from 1 to 8 GPUs isn't perfectly linear for any GPU.
* V100 scales best: going from 1 to 8 GPUs gives a 7.3x speed boost
* RTX 2080 Ti scales worst: going from 1 to 5 GPUs gives 5x speed boost

&amp;#x200B;",93,142,False,self,,,,,
378,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,10,ay6n1l,self.MachineLearning,[Discussion] Can you increase the resolution an image by using multiple offset images and an ML algorithm? More in description.,https://www.reddit.com/r/MachineLearning/comments/ay6n1l/discussion_can_you_increase_the_resolution_an/,ALLIRIX,1551921005,"I was just walking to uni thinking about how super-resolution has a limit when using a single image because a pixel is a average of its contents and therefore loses the distribution of colour behind it. My understanding is current super-resolution techniques find patterns in this neighbourhood of averages using ML and use these patterns to enhance an image.

But, we can capture multiple photos. As an ideal scenario, if we take 10 photos, each offset from the last by 0.1 Pixels, up and down, would it be possible to use a machine learning algorithm to merge the image into one that is 10x larger?

I'm completely new to computer vision, machine learning and statistics as a undergraduate student in engineering but it fascinates me.

Edit: I have made a [picture](https://imgur.com/a/NwWbfXY) to help communicate what I mean. You may not need a fancy ML algorithm since averaging out (blue region) the overlaid pixels (red &amp; green) can store the lost information in a higher resolution image. But averaging out larger pixels into the smaller region is using information from outside that pixel to determine its value so does that nullify the result or mean a more complex method is required?",11,12,False,self,,,,,
379,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,10,ay6tff,self.MachineLearning,"[D] Data Learning, where do I find out more about it?",https://www.reddit.com/r/MachineLearning/comments/ay6tff/d_data_learning_where_do_i_find_out_more_about_it/,supermanstream,1551922110,"So  I stumbled upon this [video](https://youtu.be/JbNegqgZKCw) on youtube. In the answer to one of the first questions in the interview this Data Scientist answers that Data Learning is a huge field and almost makes the ""manual"" data cleaning/preprocessing unnecessary, and the actual skill that separates good and mediocre ML engineers is actually feature engineering. I currently work with a huge dataset (\~5 million rows and \~200 columns), and I feel like the dirtiness of data is significantly lowering the quality of my results.     

For the interested, my problem is binary classification of highly unbalanced data (99%-1% label ratio), and my best model scores \~0.4 f1 score (high (\~0.7) recall mediocre precision) and \~0.7 auc on the validation and test set. I feel like dimensionality reduction/cleaning algorithms should improve my results, but I tried 0.95 and 0.9 PCA and got no improvement in predictions, tsne is impossible to calculate due to the size of the dataset (unless I try something like reduction to 20 dimensions via PCA and then tsne that). ",1,0,False,self,,,,,
380,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,10,ay6w9k,tensorflow.org,[D] TensorFlow Dev Summit Announcements,https://www.reddit.com/r/MachineLearning/comments/ay6w9k/d_tensorflow_dev_summit_announcements/,m__ke,1551922596,,1,1,False,default,,,,,
381,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,10,ay709m,github.com,[P] Dug: Exploratory project to extract actionable signal from a custom electroencephalogram (EEG) cap [they] are building for a dog.,https://www.reddit.com/r/MachineLearning/comments/ay709m/p_dug_exploratory_project_to_extract_actionable/,TheKing01,1551923280,,0,1,False,default,,,,,
382,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,10,ay70uy,fluxml.ai,Accelerating reinforcement learning with physics and Differentiable Programming,https://www.reddit.com/r/MachineLearning/comments/ay70uy/accelerating_reinforcement_learning_with_physics/,ViralBShah,1551923379,,0,1,False,https://b.thumbs.redditmedia.com/dDcUCWolNCQ3ZBTv6QYe4du6taD-W2wcm1_NSPk9rrk.jpg,,,,,
383,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,11,ay77zf,self.MachineLearning,What is the difference between a ReLU and the hinge loss function? Are they fundamentally the same thing?,https://www.reddit.com/r/MachineLearning/comments/ay77zf/what_is_the_difference_between_a_relu_and_the/,shitinmyunderwear,1551924571,,0,1,False,self,,,,,
384,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,11,ay7cmm,self.MachineLearning,Varying number of labels for a classification task,https://www.reddit.com/r/MachineLearning/comments/ay7cmm/varying_number_of_labels_for_a_classification_task/,hello_lol222,1551925330,[removed],0,1,False,self,,,,,
385,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,11,ay7j0z,self.MachineLearning,Question about what ML Algorithm to use for optimal cricket fielder position,https://www.reddit.com/r/MachineLearning/comments/ay7j0z/question_about_what_ml_algorithm_to_use_for/,Kingkashdog,1551926404,[removed],0,1,False,self,,,,,
386,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,11,ay7kii,self.MachineLearning,[P] Using Jax for Neural Networks (Google Brain),https://www.reddit.com/r/MachineLearning/comments/ay7kii/p_using_jax_for_neural_networks_google_brain/,farmingvillein,1551926657,"I know that Jax (https://github.com/google/jax) has had some interest in this subreddit.  

I recently came across https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/trax/README.md, which is a limited set of implementations of some common ML models/tasks (MLP over MNIST, Resnet50 over Imagenet, transformer LM) using Jax (i.e., not TF).

Associated is https://github.com/tensorflow/tensor2tensor/issues/1478, which is a recent RFC on the nascent implementation.

I am not affiliated, but found it interesting.  ",1,17,False,self,,,,,
387,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,12,ay7r7y,self.MachineLearning,Could you use BERT to do span selection?,https://www.reddit.com/r/MachineLearning/comments/ay7r7y/could_you_use_bert_to_do_span_selection/,MrAaronW,1551927816,[removed],0,1,False,self,,,,,
388,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,12,ay7r98,self.MachineLearning,Opinions on TF 2.0?,https://www.reddit.com/r/MachineLearning/comments/ay7r98/opinions_on_tf_20/,varun19299,1551927822,[removed],0,1,False,self,,,,,
389,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,12,ay86nc,self.MachineLearning,ML practitioners in tech and non-tech companies: what's your typical cycle of key activities? where are you spending more of your time - what works/what could be better,https://www.reddit.com/r/MachineLearning/comments/ay86nc/ml_practitioners_in_tech_and_nontech_companies/,SallyAtSloan,1551930555,[removed],0,1,False,self,,,,,
390,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,13,ay8giq,self.MachineLearning,[P] Code+Model Release for BERT on STILTs,https://www.reddit.com/r/MachineLearning/comments/ay8giq/p_codemodel_release_for_bert_on_stilts/,zphang,1551932409,"We are releasing our code and trained models for BERT on STILTs.

**arXiv**: [Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks](https://arxiv.org/abs/1811.01088)

**GitHub**: [https://github.com/zphang/bert_on_stilts](https://github.com/zphang/bert_on_stilts)

**Summary**: Dead-simple *pretrain-train-finetune* procedure adds fairly consistently if minor improvement gains across various tasks, in addition to [meaningfully improving fine-tuning stability](https://pbs.twimg.com/media/D1BFymbXcAEHUIc.png:large).

* BERT on STILTs held the SOTA (82.0) on the [GLUE Benchmark](https://gluebenchmark.com/leaderboard) for an incredible 6 hours before being beaten by ALICE from Alibaba.
* *STILTs* stands for *Supplementary Training on Intermediate Labeled-data Tasks*. We believe that the space of (informative) paper title puns is a rich area for future work.
* The code is a fork of [Hugging Face's PyTorch port of BERT](https://github.com/huggingface/pytorch-pretrained-BERT).",3,16,False,self,,,,,
391,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,13,ay8rm4,self.MachineLearning,How do you search a high dimensional for the global maxima using as few samples as possible?,https://www.reddit.com/r/MachineLearning/comments/ay8rm4/how_do_you_search_a_high_dimensional_for_the/,kayaniv,1551934490,[removed],0,1,False,self,,,,,
392,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,13,ay8slm,focusinsite.com,"Natural Language Modellers Needed - $250 for 90 min online testing session Mar 15-20, 2019",https://www.reddit.com/r/MachineLearning/comments/ay8slm/natural_language_modellers_needed_250_for_90_min/,FocusInsite,1551934683,,0,1,False,https://b.thumbs.redditmedia.com/pJs8J4qlUZ3yPFZnudKiBJQlH94NW01L3_odXlJMHmE.jpg,,,,,
393,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,14,ay8vnp,self.MachineLearning,Guidance for the proper painting,https://www.reddit.com/r/MachineLearning/comments/ay8vnp/guidance_for_the_proper_painting/,Wikihub,1551935289,[removed],0,1,False,https://b.thumbs.redditmedia.com/1LIn_tjskCwnINPuGRXEkc2hlgs5gIWRsNX2sxkOdaU.jpg,,,,,
394,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,15,ay9ewj,self.MachineLearning,[D] Can you prove if this network can/can't learn XOR with the given constraints?,https://www.reddit.com/r/MachineLearning/comments/ay9ewj/d_can_you_prove_if_this_network_cancant_learn_xor/,en_sin_si,1551939223,"I'm not sure if this is a thing: given a fixed network and a target function, prove the learnability. I'm looking for something like [this:] ( https://stats.stackexchange.com/a/44932 )

So [here is the network of interest](https://i.stack.imgur.com/gaBWk.png).

Below are the constraints:

1. Activation function of the hidden layer is ReLU
2. No biases to any of the neurons
3. Output neuron doesn't have an activation function. It just returns the sum of its inputs.

If we can prove that the network can learn XOR with the above constraints, can we further prove that it is still achievable if we constrained the network weights to be in the set of	{-1, 0, 1} ?

[Question on crossvalidated] (https://stats.stackexchange.com/questions/395924/can-this-network-learn-the-xor-function)",1,2,False,self,,,,,
395,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,16,ay9w3k,self.MachineLearning,Best DL\ML course for someone with theoretical background,https://www.reddit.com/r/MachineLearning/comments/ay9w3k/best_dlml_course_for_someone_with_theoretical/,meni_s,1551942956,[removed],0,1,False,self,,,,,
396,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,16,aya3d6,self.MachineLearning,Aircraft Engines Market to Perceive Substantial Growth during 2023,https://www.reddit.com/r/MachineLearning/comments/aya3d6/aircraft_engines_market_to_perceive_substantial/,apple_x9,1551944653,[removed],1,1,False,self,,,,,
397,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,16,aya5od,self.MachineLearning,[D] Are the OpenAI Gym MuJoCo environments deterministic or stochastic?,https://www.reddit.com/r/MachineLearning/comments/aya5od/d_are_the_openai_gym_mujoco_environments/,CartPole,1551945239,"I've been going through the documentation and have been having a hard time sorting out which mujoco environments are stochastic and which are deterministic. 

Given the many issues([1](https://github.com/openai/gym/issues/1102), [2](https://github.com/openai/gym/issues/1193), [3](https://github.com/openai/gym/issues/106)) on their github repo I'm guessing I'm not the only one who  concerned.

Does anyone know?",3,2,False,self,,,,,
398,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,17,ayad17,self.MachineLearning,[Discussion] Sessions to look forward to at this year's Strata conference,https://www.reddit.com/r/MachineLearning/comments/ayad17/discussion_sessions_to_look_forward_to_at_this/,electrotwelve,1551947078,"One of our [senior executives](https://in.linkedin.com/in/mukundr) is doing his yearly march to Strata at the end of this month. We [published a post](https://www.synerzip.com/blog/13-upcoming-strata-sessions/) on the sessions that he is looking forward to and why. I hope this is useful to the community here. If not, mods please feel free to remove this post. If there are any questions you guys are hoping to get answered, please leave them in the comments and I can forward them to him.",0,0,False,self,,,,,
399,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,18,ayakum,github.com,"The first world production level open source platform for edge AI using ARM GPU/NPU, leveraging AutoML",https://www.reddit.com/r/MachineLearning/comments/ayakum/the_first_world_production_level_open_source/,solderzzc,1551949231,,0,1,False,default,,,,,
400,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,18,ayam5s,github.com,"[P] GitHub - SharpAI/sharpai: The first world production level open source platform for edge AI using ARM GPU/NPU, leveraging AutoML",https://www.reddit.com/r/MachineLearning/comments/ayam5s/p_github_sharpaisharpai_the_first_world/,solderzzc,1551949584,,0,1,False,https://b.thumbs.redditmedia.com/ivHjdmYHM7S34T7R1lafVkRN4hq9WfYdx5igpMcyQkI.jpg,,,,,
401,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,18,ayanjz,github.com,Open Source Face Recognition Project for ARM GPU with TVM/TF Lite and mobile app,https://www.reddit.com/r/MachineLearning/comments/ayanjz/open_source_face_recognition_project_for_arm_gpu/,solderzzc,1551949926,,0,1,False,https://b.thumbs.redditmedia.com/ivHjdmYHM7S34T7R1lafVkRN4hq9WfYdx5igpMcyQkI.jpg,,,,,
402,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,18,ayap05,self.MachineLearning,"[N] Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning",https://www.reddit.com/r/MachineLearning/comments/ayap05/n_introduction_to_tensorflow_for_artificial/,A1M94,1551950286,"Hi,

Coursera released new course with focus on Tensorflow and its best practices and I thought some of you may find it useful. I realise most of you guys are experienced engineers or researchers and already settled for PyTorch or Tensorflow or know both, however every time ""PyTorch vs. Tensorflow"" post pops up, there are some people unsure where to start with TF.

[https://www.coursera.org/learn/introduction-tensorflow](https://www.coursera.org/learn/introduction-tensorflow)

&amp;#x200B;

 

&amp;#x200B;",24,82,False,self,,,,,
403,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,18,ayat2d,self.MachineLearning,Aircraft Tractor Market to Perceive Substantial Growth during 2023,https://www.reddit.com/r/MachineLearning/comments/ayat2d/aircraft_tractor_market_to_perceive_substantial/,apple_x9,1551951264,[removed],1,1,False,self,,,,,
404,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,18,ayaxy0,celadon.ae,Machine learning: transforming industries or transforming the future?,https://www.reddit.com/r/MachineLearning/comments/ayaxy0/machine_learning_transforming_industries_or/,Celadon_soft,1551952450,,0,1,False,default,,,,,
405,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,19,ayb1zm,arxiv.org,[1903.02503] The AI Driving Olympics at NeurIPS 2018,https://www.reddit.com/r/MachineLearning/comments/ayb1zm/190302503_the_ai_driving_olympics_at_neurips_2018/,ihaphleas,1551953383,,4,3,False,default,,,,,
406,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,19,ayb3bk,self.MachineLearning,[D] TensorFlow implementation of Neural Ordinary Differential Equation,https://www.reddit.com/r/MachineLearning/comments/ayb3bk/d_tensorflow_implementation_of_neural_ordinary/,begooboi,1551953708,"This was done by Pascal Voitot (@mandubian) . He implemented tensorflow version of NeuralODE. I have only gone through the project and not tried it myself.

https://github.com/mandubian/neural-ode",4,19,False,self,,,,,
407,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,20,aybjjs,techerati.com,TensorFlow can now run on $15 edge hardware,https://www.reddit.com/r/MachineLearning/comments/aybjjs/tensorflow_can_now_run_on_15_edge_hardware/,TheJCOEco,1551957419,,0,1,False,default,,,,,
408,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,20,ayblwu,self.MachineLearning,Regarding a Proof in ''Robustness and Generalization'' by Xu. and Mannor,https://www.reddit.com/r/MachineLearning/comments/ayblwu/regarding_a_proof_in_robustness_and/,jaweriaamjad,1551957952,[removed],0,1,False,self,,,,,
409,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,20,aybm11,self.MachineLearning,[D] How to Find Your Life Partner Using Machine Learning,https://www.reddit.com/r/MachineLearning/comments/aybm11/d_how_to_find_your_life_partner_using_machine/,sigmoidp,1551957977,"I know that dating is kind of Epsilon-Greedy with early stopping but feel that this is worth sharing!

&amp;#x200B;

[http://www.humanalgorithms.co/post/how-to-find-your-life-partner-using-machine-learning/](http://www.humanalgorithms.co/post/how-to-find-your-life-partner-using-machine-learning/)",0,0,False,self,,,,,
410,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,20,aybrwy,self.MachineLearning,Measuring mm distances in machine parts,https://www.reddit.com/r/MachineLearning/comments/aybrwy/measuring_mm_distances_in_machine_parts/,mohanradhakrishnan,1551959259,[removed],0,1,False,self,,,,,
411,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,20,aybt6g,self.MachineLearning,Looking for any papers about either AlphaStar(starcraft 2) or OpenAI Five(Dota 2),https://www.reddit.com/r/MachineLearning/comments/aybt6g/looking_for_any_papers_about_either/,YonkoNami,1551959555,[removed],0,1,False,self,,,,,
412,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,21,aybzr0,self.MachineLearning,Machine Learning Tutorial Part 1 | Machine Learning For Beginners,https://www.reddit.com/r/MachineLearning/comments/aybzr0/machine_learning_tutorial_part_1_machine_learning/,SquareTechAcademy,1551960930,[removed],0,1,False,self,,,,,
413,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,21,ayc57u,self.MachineLearning,"[P] MTCNN / InsightFace(ArcFace)/ SVM/TVM and auto labelling, a full AI video system (Python/Android)",https://www.reddit.com/r/MachineLearning/comments/ayc57u/p_mtcnn_insightfacearcface_svmtvm_and_auto/,solderzzc,1551962066,"Decoding support:

1. Android HW
2. FFMPEG on Shinobi

MTCNN NEON

InsightFace GPU on ARM Mali T720

&amp;#x200B;

Support   
Rockchip RK3288

Rockchip RK3399

Samsung 7420

&amp;#x200B;

[https://github.com/SharpAI/sharpai](https://github.com/SharpAI/sharpai)",7,1,False,self,,,,,
414,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,22,aycdsw,smarten.com,Data Literacy Improves Resource Optimization!,https://www.reddit.com/r/MachineLearning/comments/aycdsw/data_literacy_improves_resource_optimization/,ElegantMicroWebIndia,1551963714,,0,1,False,default,,,,,
415,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,22,aych4n,self.MachineLearning,ISO compliance and CUDA/Deep learning,https://www.reddit.com/r/MachineLearning/comments/aych4n/iso_compliance_and_cudadeep_learning/,paranoid_coder,1551964334,"A company I am working for wants to be ISO 9001:2015 compliant. 

They're insisting I can't have a Linux machine. My understanding is that even a Linux subsystem CAN'T run CUDA natively. Not to mention all sorts of open source projects that need it00B60.

I can't really do my job without a Linux system and CUDA. So many things need Linux!


Does anybody here have experience with finding a compromise to be complaint while also being able to access everything you need to work? I can't seem to find documentation that EVERY SINGLE EMPLOYEE needs to be ISO compliant.",0,1,False,self,,,,,
416,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,22,aychjz,medium.com,Whats coming in TensorFlow 2.0 [TensorFlow Team],https://www.reddit.com/r/MachineLearning/comments/aychjz/whats_coming_in_tensorflow_20_tensorflow_team/,del_rio,1551964415,,0,1,False,https://b.thumbs.redditmedia.com/pXsp0d3dORaihUF4z6fWvgkuk8WBKpXoDl9zleq_BqA.jpg,,,,,
417,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,22,aycq85,self.MachineLearning,Extract personal information about a person from a list of documents and summarize it,https://www.reddit.com/r/MachineLearning/comments/aycq85/extract_personal_information_about_a_person_from/,Ameyn21,1551966078,[removed],0,1,False,self,,,,,
418,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,22,ayctll,self.MachineLearning,Thinking about building your own quantum machine learning startup?,https://www.reddit.com/r/MachineLearning/comments/ayctll/thinking_about_building_your_own_quantum_machine/,rmenian_princess,1551966711,"The Creative Destruction Lab is calling for applications for its 2019-2020 Quantum Machine Learning Stream. Were looking for both established quantum technology startups and for individuals who have yet to find the right partner and idea for their quantum startups.

&amp;#x200B;

Program participants all receive:

(1) **Investment** \- access to pre-seed capital of $80K US;

(2) **Training** \- one month technical bootcamp intensive run by Peter Wittek; and

(3) **Hardware Access** \- quantum computing resources from D-Wave, Rigetti, and Xanadu.

Please apply at [https://www.creativedestructionlab.com/quantum-machine-learning-application/](https://www.creativedestructionlab.com/quantum-machine-learning-application/) by March 14, 2019

To learn more visit our webpage at [https://www.creativedestructionlab.com/streams/quantum-machine-learning/](https://www.creativedestructionlab.com/streams/quantum-machine-learning/)

&amp;#x200B;

If you have any questions feel free to comment below and I'll answer them :) ",0,1,False,self,,,,,
419,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,22,aycuc4,l7.curtisnorthcutt.com,"[P] I built Lambda's $12,500 deep learning rig for $6200.",https://www.reddit.com/r/MachineLearning/comments/aycuc4/p_i_built_lambdas_12500_deep_learning_rig_for_6200/,cgnorthcutt,1551966851,,1,2,False,default,,,,,
420,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,23,ayd01o,self.MachineLearning,"[P] I built Lambda's $12,500 deep learning rig for $6200",https://www.reddit.com/r/MachineLearning/comments/ayd01o/p_i_built_lambdas_12500_deep_learning_rig_for_6200/,cgnorthcutt,1551967841,"See: http://l7.curtisnorthcutt.com/build-pro-deep-learning-workstation

Hi Reddit! I built a 3-GPU deep learning workstation similar to Lambda's 4-GPU ( RTX 2080 TI ) rig for half the price. In the hopes of helping other researchers, I'm sharing a time-lapse of the build, the parts list, the receipt, and benchmarking versus Google Compute Engine (GCE) on ImageNet. You save $1200 (the cost of an EVGA RTX 2080 ti GPU) per ImageNet training to use your own build instead of GCE. The training time is reduced by over half. In the post, I include 3 GPUs, but the build (increase PSU wattage) will support a 4th RTX 2080 TI GPU for $1200 more ($7400 total). Happy building!",143,480,False,self,,,,,
421,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,23,ayd4h0,self.MachineLearning,[P] Reproducible benchmarking of all 34 Keras-TensorFlow and PyTorch pre-trained models,https://www.reddit.com/r/MachineLearning/comments/ayd4h0/p_reproducible_benchmarking_of_all_34/,cgnorthcutt,1551968604,"Blog post: http://l7.curtisnorthcutt.com/towards-reproducibility-benchmarking-keras-pytorch
GitHub: https://github.com/cgnorthcutt/benchmarking-keras-pytorch

Hi Reddit! Did you know the Keras pre-trained models do not reproduce the accuracies [on the Keras site](https://keras.io/applications/#documentation-for-individual-models)? Even worse, different users do not get the same results even though these are pre-trained models! 

In response, I'm releasing 100% reproducible benchmarks for all Keras-@TensorFlow and @PyTorch pre-trained models. 
PyTorch rules ResNets. Keras rules InceptionNet 

",3,16,False,self,,,,,
422,MachineLearning,t5_2r3gv,2019-3-7,2019,3,7,23,ayd5lc,self.MachineLearning,[D] Any recent mind blowing advances in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/ayd5lc/d_any_recent_mind_blowing_advances_in_machine/,exasperatedfarrago,1551968799,"Just wanted to know if anyone has come across some mind blowing stuff recently that I can showcase and provide some commentary to it. I am a ML Engineer from India, and I am supposed to deliver a short session on Machine Learning. The point is to get people excited about the field and see how far we have come and what are the possibilities. The crowd is mostly programmers so technical details is appreciated. I will do the research and prepare the material, just want to see if there is some SOTA work that I might miss have missed out. ",11,0,False,self,,,,,
423,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,0,aydigz,self.MachineLearning,[P] MaskedSoftmax PyTorch operator in CUDA,https://www.reddit.com/r/MachineLearning/comments/aydigz/p_maskedsoftmax_pytorch_operator_in_cuda/,7unz,1551970899,"I tried to optimize training speed of Transformer by implementing MaskedSoftmax operator in CUDA, and improved the entire training speed 2% faster than before. It's not a tremendous improvement, but I think it's meaningful. I want to share my approach and code with you.

&amp;#x200B;

Post: [https://tunz.kr/post/5](https://tunz.kr/post/5)

Code: [https://github.com/tunz/tcop-pytorch](https://github.com/tunz/tcop-pytorch)

Usage: [https://github.com/tunz/transformer-pytorch](https://github.com/tunz/transformer-pytorch)",2,7,False,self,,,,,
424,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,0,aydnxj,self.MachineLearning,Need help: Big difference between validity accuracy and test accuracy,https://www.reddit.com/r/MachineLearning/comments/aydnxj/need_help_big_difference_between_validity/,OnMyWayc,1551971749,[removed],0,1,False,self,,,,,
425,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,0,aydo4v,inveritasoft.com,5 Ways FinTech Can Benefit from Machine Learning | InVeritaSoft,https://www.reddit.com/r/MachineLearning/comments/aydo4v/5_ways_fintech_can_benefit_from_machine_learning/,inveritasoft,1551971779,,0,1,False,https://b.thumbs.redditmedia.com/FYrjKjUuRTCTJSjwwrsjVH3d2MSKa3aXcOXfqHV6Y0U.jpg,,,,,
426,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,0,aydq50,self.MachineLearning,Actual industry professionals: How much math do you need and use on the job?,https://www.reddit.com/r/MachineLearning/comments/aydq50/actual_industry_professionals_how_much_math_do/,dbgk,1551972100,[removed],0,1,False,self,,,,,
427,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,0,aydsbn,self.MachineLearning,Papers on transforming multi-label data to a single label instance(not BR) and proof of joint learnability,https://www.reddit.com/r/MachineLearning/comments/aydsbn/papers_on_transforming_multilabel_data_to_a/,atif_hassan,1551972451,[removed],0,1,False,self,,,,,
428,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,0,aydxnf,self.MachineLearning,LSTM or Other model,https://www.reddit.com/r/MachineLearning/comments/aydxnf/lstm_or_other_model/,IOsci,1551973288,[removed],0,1,False,self,,,,,
429,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,1,aye5mp,self.MachineLearning,Google open source its deep-learning library! Check it out !,https://www.reddit.com/r/MachineLearning/comments/aye5mp/google_open_source_its_deeplearning_library_check/,priyank1sh,1551974504,Google Open-Sources GPipe Library for Training Large-Scale Neural Network Models by Synced https://link.medium.com/lCseqDxNRU,0,1,False,self,,,,,
430,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,1,aye8k9,self.MachineLearning,Statistical reporting - Managing the daily reporting system,https://www.reddit.com/r/MachineLearning/comments/aye8k9/statistical_reporting_managing_the_daily/,Anilklb,1551974928,"Hi Friends, I am the only data scientist/Analytics Manager (Newly appointed) in a transaction based company (ATM industry) and I have to monitor the daily transect ions on around 10000 Nodes/sites. As traditionally company managing transactions in excel sheet. I wanted to automate daily/weekly/Monthly reports using R and Python. My major challenges is taking care of each closer/Newly open site which make my report system more complex. Any help/reference would be much appreciated.",0,1,False,self,,,,,
431,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,1,aye9oy,statisticseco.com,Classification &amp; Clustering using R,https://www.reddit.com/r/MachineLearning/comments/aye9oy/classification_clustering_using_r/,Statistics_Expert,1551975090,,0,1,False,default,,,,,
432,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,1,ayee02,self.MachineLearning,What would be the steps to create an sentiment analysis chatbot?,https://www.reddit.com/r/MachineLearning/comments/ayee02/what_would_be_the_steps_to_create_an_sentiment/,Rhymezboy,1551975717,[removed],0,1,False,self,,,,,
433,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,1,ayenh9,self.MachineLearning,Is there any Python library that can support the cross correlation of more than 2 time series?,https://www.reddit.com/r/MachineLearning/comments/ayenh9/is_there_any_python_library_that_can_support_the/,EquivalentSelf,1551977086,[removed],0,1,False,self,,,,,
434,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,1,ayenr0,self.MachineLearning,Google open source its deep-learning library! Check it out !,https://www.reddit.com/r/MachineLearning/comments/ayenr0/google_open_source_its_deeplearning_library_check/,priyank1sh,1551977123,[removed],0,1,False,self,,,,,
435,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,1,ayes3a,wellsaidlabs.com,"[N] After over ONE YEAR of Deep Learning research, we launched our text-to-speech startup WellSaid!",https://www.reddit.com/r/MachineLearning/comments/ayes3a/n_after_over_one_year_of_deep_learning_research/,Deepblue129,1551977727,,1,1,False,https://b.thumbs.redditmedia.com/Li48FWJk6TmEY_WS_wK1MtF_lQmiKUVNdj8nM7e-C0A.jpg,,,,,
436,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,2,ayf179,reddit.com,Never got around to learning GNU Parallel? Here is the cheat sheet (pdf).,https://www.reddit.com/r/MachineLearning/comments/ayf179/never_got_around_to_learning_gnu_parallel_here_is/,OleTange,1551978988,,0,1,False,default,,,,,
437,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,2,ayfd1e,self.MachineLearning,CPU Intense Workloads in ML projects?,https://www.reddit.com/r/MachineLearning/comments/ayfd1e/cpu_intense_workloads_in_ml_projects/,kalenx2,1551980653,[removed],0,1,False,self,,,,,
438,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,2,ayfdfm,i.redd.it,Why is even the Google Cloud Vision API confused by The Dress?!,https://www.reddit.com/r/MachineLearning/comments/ayfdfm/why_is_even_the_google_cloud_vision_api_confused/,Chinelbow,1551980704,,1,1,False,default,,,,,
439,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,2,ayfif9,self.MachineLearning,[D] Reviewing Open AI's language model (live event),https://www.reddit.com/r/MachineLearning/comments/ayfif9/d_reviewing_open_ais_language_model_live_event/,tdls_to,1551981401,"we will be going through GPT2, the new and controversial language model published by Open AI later today; join us and participate in the conversation: [https://tdls.a-i.science/events/2019-03-07/](https://tdls.a-i.science/events/2019-03-07/)",14,9,False,self,,,,,
440,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,3,ayflqd,self.MachineLearning,C++ Library for GPU accelerated Levenberg Marquadt?,https://www.reddit.com/r/MachineLearning/comments/ayflqd/c_library_for_gpu_accelerated_levenberg_marquadt/,soulslicer0,1551981853,[removed],0,1,False,self,,,,,
441,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,3,ayfqpq,self.MachineLearning,Never got around to using GNU Parallel? Here is the cheat sheet (pdf),https://www.reddit.com/r/MachineLearning/comments/ayfqpq/never_got_around_to_using_gnu_parallel_here_is/,OleTange,1551982556,"For machine learning you often need to run the same program with different parameters or process large amounts of data.

&amp;#x200B;

GNU Parallel is a command line tool for running your programs on all your available cores on all your servers.

&amp;#x200B;

It now has a cheat sheet: [https://www.gnu.org/software/parallel/parallel\_cheat.pdf](https://www.gnu.org/software/parallel/parallel_cheat.pdf)

&amp;#x200B;

(Old) videos: [https://www.youtube.com/playlist?list=PL284C9FF2488BC6D1](https://www.youtube.com/playlist?list=PL284C9FF2488BC6D1)

Man page: [https://www.gnu.org/software/parallel/man.html](https://www.gnu.org/software/parallel/man.html)

Book: [https://doi.org/10.5281/zenodo.1146014](https://doi.org/10.5281/zenodo.1146014) ",0,1,False,self,,,,,
442,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,3,ayfucg,geekwire.com,"[N] After over ONE YEAR of DL research, we launched our text-to-speech startup WellSaid!",https://www.reddit.com/r/MachineLearning/comments/ayfucg/n_after_over_one_year_of_dl_research_we_launched/,Deepblue129,1551983092,,0,1,False,default,,,,,
443,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,3,ayfxmi,self.MachineLearning,"[N] After over ONE YEAR of Deep Learning research, we launched our life-like text-to-speech startup WellSaid!",https://www.reddit.com/r/MachineLearning/comments/ayfxmi/n_after_over_one_year_of_deep_learning_research/,Deepblue129,1551983552,"Following lots of hard work, we are FINALLY here. Check out the results of our TTS neural architecture on our website:

[https://wellsaidlabs.com/](https://wellsaidlabs.com/)

&amp;#x200B;

**Notes**

* The clips are not optimized for laptop or phone speakers. Please use good quality headphones to evaluate the results.
* We'll add a research page soon with more clips; however, the current page has 4 videos and 4 audio clips demonstrating the tech  


Hope this is something this community loves! We worked really hard to capture the nuances of human voice.  


Also... checkout out this super positive article about us: [https://www.geekwire.com/2019/ai2s-incubator-gives-birth-wellsaid-startup-synthesizes-amazingly-realistic-voices/?fbclid=IwAR0uHTuvoJ9xf7VRWUSG4jFVfspY407ppRUzRSwbYFLqh6w\_fYe-YJy-CBo](https://www.geekwire.com/2019/ai2s-incubator-gives-birth-wellsaid-startup-synthesizes-amazingly-realistic-voices/?fbclid=IwAR0uHTuvoJ9xf7VRWUSG4jFVfspY407ppRUzRSwbYFLqh6w_fYe-YJy-CBo)",23,26,False,self,,,,,
444,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,3,ayg775,self.MachineLearning,Need help with Robosat implementation.,https://www.reddit.com/r/MachineLearning/comments/ayg775/need_help_with_robosat_implementation/,SkashyapD,1551984930,[removed],0,1,False,self,,,,,
445,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,3,ayg7iz,self.MachineLearning,[Discussion] The effects of image orientation on classification,https://www.reddit.com/r/MachineLearning/comments/ayg7iz/discussion_the_effects_of_image_orientation_on/,blackjack503,1551984981,"I recently saw a post on /r/dataisbeautiful where a user checked the classification accuracy of google cloud vision on an image duck/rabbit illusion ([https://www.reddit.com/r/dataisbeautiful/comments/aydqig/is\_it\_a\_duck\_or\_a\_rabbit\_for\_google\_cloud\_vision/](https://www.reddit.com/r/dataisbeautiful/comments/aydqig/is_it_a_duck_or_a_rabbit_for_google_cloud_vision/))

&amp;#x200B;

I was wondering what could cause such an effect and how we can prevent such a thing from affecting our own models.

Is it just a lack of training data where the ducks and rabbits are always shown in a certain orientation or could there be other factors that need to be considered?",11,4,False,self,,,,,
446,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,4,ayggkq,self.MachineLearning,How to bring your PyTorch model to the web?,https://www.reddit.com/r/MachineLearning/comments/ayggkq/how_to_bring_your_pytorch_model_to_the_web/,P4ND0RA_,1551986253,[removed],0,1,False,self,,,,,
447,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,4,aygqp4,m.all3dp.com,Google's Coral Dev Board: any advantage of using this versus a dedicated GPU?,https://www.reddit.com/r/MachineLearning/comments/aygqp4/googles_coral_dev_board_any_advantage_of_using/,VladimirStudmuffin,1551987710,,0,1,False,default,,,,,
448,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,4,aygsmh,self.MachineLearning,Static Malware Classifier,https://www.reddit.com/r/MachineLearning/comments/aygsmh/static_malware_classifier/,TomHatskevich,1551987987,"I want to implement a static malware classifier.

I started with RandomForest algorithm and my features was PE headers...

I want make my classifier stronger that the features will based on the code section of the PE. I read about Ngrams but I dont know how use it and how much its good idea.

Can you suggest me how can I implement this classifier ? give me some sources to learn from.

I need to implement a Static Malware Classifier  :)",0,1,False,self,,,,,
449,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,5,ayh2hf,arxiv.org,[R] RePr: Improved Training of Convolutional Filters [CVPR - Oral],https://www.reddit.com/r/MachineLearning/comments/ayh2hf/r_repr_improved_training_of_convolutional_filters/,iamaaditya,1551989409,,72,6,False,default,,,,,
450,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,5,ayh6e8,self.MachineLearning,Inference speed of PyTorch vs exported ONNX model in production?,https://www.reddit.com/r/MachineLearning/comments/ayh6e8/inference_speed_of_pytorch_vs_exported_onnx_model/,svpadd3,1551989981,I'm curious if anyone has any comprehensive statistics about the speed of predictions of converting a PyTorch model to ONNX versus just using the PyTorch model. At least in my experience (haven't run extensive experiments) there hasn't seemed to be any speed increase and it often takes a lot of time and energy to export the model and make it work with ONNX. Which leads me to wonder what is the actual advantage of Onnx+Caffe2 versus just running PyTorch if your code is going to remain in Python anyways? ,0,1,False,self,,,,,
451,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,5,ayh7x9,self.MachineLearning,Inference speed of PyTorch vs exported ONNX model in production? [D],https://www.reddit.com/r/MachineLearning/comments/ayh7x9/inference_speed_of_pytorch_vs_exported_onnx_model/,mlreddit234,1551990208,I'm curious if anyone has any comprehensive statistics about the speed of predictions of converting a PyTorch model to ONNX versus just using the PyTorch model. At least in my experience (haven't run extensive experiments) there hasn't seemed to be any speed increase and it often takes a lot of time and energy to export the model and make it work with ONNX. Which leads me to wonder what is the actual advantage of Onnx+Caffe2 versus just running PyTorch if your code is going to remain in Python anyways?,3,10,False,self,,,,,
452,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,6,ayhrko,self.MachineLearning,Qs on Activation Functions (If You Have the Time),https://www.reddit.com/r/MachineLearning/comments/ayhrko/qs_on_activation_functions_if_you_have_the_time/,jojeyh,1551993047,[removed],0,1,False,self,,,,,
453,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,6,ayht70,self.MachineLearning,Loss minimization with two local minima,https://www.reddit.com/r/MachineLearning/comments/ayht70/loss_minimization_with_two_local_minima/,spauldeagle,1551993274,[removed],0,1,False,self,,,,,
454,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,6,ayhu1r,self.MachineLearning,GAN + CNN = ??,https://www.reddit.com/r/MachineLearning/comments/ayhu1r/gan_cnn/,xdarknuno,1551993407,"Greetings, 
was wandering if a GAN could be used somehow to improve the accuracy of a CNN doing image classification? 
For me it makes sense in theory since they could add more beneficial  data to the dataset but i might be missing something.",0,1,False,self,,,,,
455,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,6,ayhug1,fluxml.ai,[R] Flux  Reinforcement Learning vs. Differentiable Programming,https://www.reddit.com/r/MachineLearning/comments/ayhug1/r_flux_reinforcement_learning_vs_differentiable/,SkiddyX,1551993471,,0,1,False,https://b.thumbs.redditmedia.com/dDcUCWolNCQ3ZBTv6QYe4du6taD-W2wcm1_NSPk9rrk.jpg,,,,,
456,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,6,ayhv9y,self.MachineLearning,Hey Google!,https://www.reddit.com/r/MachineLearning/comments/ayhv9y/hey_google/,Helforc,1551993594,"Memento Mori. Please be kind to our kind of us all. Ok?

&amp;#x200B;

Ovule:

&amp;#x200B;

https://i.redd.it/3agqu9z2krk21.png

&amp;#x200B;

Spermatozode:

&amp;#x200B;

https://i.redd.it/ulv1ze85krk21.png

&amp;#x200B;",0,1,False,https://b.thumbs.redditmedia.com/CdWsrrrLbNZ5M38m8dvHurqt6jKdcRqC2MOzbP-Qy6U.jpg,,,,,
457,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,6,ayi624,self.MachineLearning,[P] Advances in Financial Machine Learning,https://www.reddit.com/r/MachineLearning/comments/ayi624/p_advances_in_financial_machine_learning/,Jackal008,1551995230,"An open source package and notebooks for the text book Advances in Financial Machine Learning by Dr Marcos Lopez de Prado. (check sub branches for notebooks in progress)

We have provided sample data to help enthusiast get started. Still a work in progress but very much a labor of love. 

The following links may be helpful:

* The Group account on Github: https://github.com/hudson-and-thames

* Our public project board: https://github.com/orgs/hudson-and-thames/projects/1

* The MLFinLab package: https://github.com/hudson-and-thames/mlfinlab

* Research Repo: https://github.com/hudson-and-thames/research

* Presentations Repo: https://github.com/hudson-and-thames/presentations

We welcome any contributions to our package and hope that it will prove a useful contribution to the quantitative finance community.",6,26,False,self,,,,,
458,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,7,ayiceq,self.MachineLearning,[D] VR machine learning Competition-South by Southwest,https://www.reddit.com/r/MachineLearning/comments/ayiceq/d_vr_machine_learning_competitionsouth_by/,red756,1551996195,"Howdy, I am part of a Student run organization called SpaceCRAFT-Vr based out of Texas A&amp;M university. We are hosting a machine learning based competition at the SXSW Gaming Expo in the Austin Convention Center from March 15-17. The competition is for college students 18 or older, winners will receive VR capable Dell portable work stations. 

[Competition Flyer](https://drive.google.com/file/d/1zY77fMKVWNGdpQHjVrHnD2m0kuyLFgUN/view)

[Sign Up](http://www.tsgc.utexas.edu/spacecraft_exploration_challenge.html)

\*\*\*Competition Details:\*\*\* The scenario is based in the Trappist 1-d exoplanet, teams will have to design a python algorithm over 2 days based in machine learning to guide a rover through a canyon to rescue an astronaut in the shortest time possible. Competitors have 2 days to form a code which will guide the rover through different terrains autonomously. The testing and competition itself will be in VR. Students will be asked to bring their own laptops, VR capable machines and setups will be available for testing. Final testing and timing of the rover will be run on the 3rd day Sunday March 17th. Each member of the winning team will receive a powerful VR capable portable workstation from one of our sponsors, Dell. 2nd and 3rd place will also receive Prizes.

Competitors will be provided with a SXSW pass for the Gaming Expo and lunch.

Competitors must be present for the introduction on the 15th and the competition on the 17th. Aside from those days, competitors are free to visit other gaming booths and come and go as they please.

Competitors are responsible for travel and lodging.  

Competitors can apply as a team or individual may be put  into teams.

Competitors must be students at a college and must be over 18.

&amp;#x200B;

For more information, email [spacecraftvr@gmail.com](mailto:spacecraftvr@gmail.com)

&amp;#x200B;",0,3,False,self,,,,,
459,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,7,ayil4q,arxiv.org,"TensorFlow Eager: A Multi-Stage, Python-Embedded DSL for Machine Learning",https://www.reddit.com/r/MachineLearning/comments/ayil4q/tensorflow_eager_a_multistage_pythonembedded_dsl/,akshayka,1551997506,,1,7,False,default,,,,,
460,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,7,ayintp,self.MachineLearning,Large batch sizes in generative models,https://www.reddit.com/r/MachineLearning/comments/ayintp/large_batch_sizes_in_generative_models/,veshneresis,1551997919,[removed],0,1,False,self,,,,,
461,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,8,ayj90l,self.MachineLearning,[R] Reinforcement Learning vs. Differentiable Programming,https://www.reddit.com/r/MachineLearning/comments/ayj90l/r_reinforcement_learning_vs_differentiable/,SkiddyX,1552001163,[removed],0,1,False,self,,,,,
462,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,8,ayj9rz,self.MachineLearning,[R] Reinforcement Learning vs. Differentiable Programming,https://www.reddit.com/r/MachineLearning/comments/ayj9rz/r_reinforcement_learning_vs_differentiable/,SkiddyX,1552001287,"""Weve discussed the idea of differentiable programming, where we incorporate existing programs into deep learning models. But if youre a researcher building, say, a self-driving car, what does differentiable programming mean in practice? How does it affect the way we express our problem, train our model, curate our dataset, and ultimately the results we achieve?""

""This article shows what DP can bring to some simple but classic control problems, where we would normally use Reinforcement Learning (RL). DP-based models not only learn far more effective control strategies than RL, but also train orders of magnitude faster. The code is all available to run for yourself  they will mostly train in a few seconds on any laptop.""

https://fluxml.ai/2019/03/05/dp-vs-rl.html",12,32,False,self,,,,,
463,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,8,ayjahf,deepmind.com,[P] TF-Replicator: Distributed Machine Learning for Researchers (deepmind.com),https://www.reddit.com/r/MachineLearning/comments/ayjahf/p_tfreplicator_distributed_machine_learning_for/,i_am_squishy,1552001405,,0,1,False,default,,,,,
464,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,9,ayjm27,medium.com,MultiCUDA: Multiple Versions of CUDA on One Machine,https://www.reddit.com/r/MachineLearning/comments/ayjm27/multicuda_multiple_versions_of_cuda_on_one_machine/,Klajv,1552003274,,0,1,False,default,,,,,
465,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,9,ayjoi6,self.MachineLearning,[D] NN reads your decision before you decide - can machines know you better than you know yourself?,https://www.reddit.com/r/MachineLearning/comments/ayjoi6/d_nn_reads_your_decision_before_you_decide_can/,phobrain,1552003683,"Using ML+fMRI, simple choices are easily predicted.

Our brains reveal our choices before we're even aware of them, study finds

https://medicalxpress.com/news/2019-03-brains-reveal-choices-aware.html

Brain waves aside, it seems like it might be trivial to diagnose cognitive conditions from e.g. a phone inertial sensor, and if ~all ongoing data on one were to be assembled China style, perhaps thoughts could be inferred and responded to in disquieting and profitable ways. 

It looks like a traditionally let-it-happen-and-if-I-can't-buy-bacon-I-will-get-back-to-you sort of issue, so almost tempting to take over all the resolutely undefended brains, but we need real brains all over (and soon) in order to survive as a species, and the ones that can be taken over are dangerous. I think the antidote is to train people to think logically and statistically as soon as possible, and to be aware of their motives and feelings that are subject to manipulation, just as kids are taught how to respond to strangers. In the past, these have been more family responsibilities, but now it seems that only internet-provided ML can do it cheaply and broadly enough. So if you're thinking about your career direction and have some wiggle room (no jobs yet, that I know of), I suggest giving it some thought.",0,1,False,self,,,,,
466,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,9,ayjrhy,self.MachineLearning,[D] Can BigGan be fine tuned?,https://www.reddit.com/r/MachineLearning/comments/ayjrhy/d_can_biggan_be_fine_tuned/,Boozybrain,1552004128,I've looked around a bit and haven't seen anything on Github or even a blog post.  Is it even possible?,3,6,False,self,,,,,
467,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,9,ayju61,arxiv.org,How Generative Adversarial Networks and Their Variants Work: An Overview,https://www.reddit.com/r/MachineLearning/comments/ayju61/how_generative_adversarial_networks_and_their/,juvenalmuniz,1552004550,,2,14,False,default,,,,,
468,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,9,ayjy0u,self.MachineLearning,Best library to implement Sparse Coding?,https://www.reddit.com/r/MachineLearning/comments/ayjy0u/best_library_to_implement_sparse_coding/,aashwin93,1552005204,[removed],0,1,False,self,,,,,
469,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,9,ayk595,self.MachineLearning,Notes for Metal Bending Machine Pressure Adjustment,https://www.reddit.com/r/MachineLearning/comments/ayk595/notes_for_metal_bending_machine_pressure/,CNCPressBrakeChina,1552006448,[removed],0,1,False,self,,,,,
470,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,11,ayks6q,self.MachineLearning,"[Help] Need a ""wow factor"" ideas for a machine learning hackathon that I'm joining",https://www.reddit.com/r/MachineLearning/comments/ayks6q/help_need_a_wow_factor_ideas_for_a_machine/,sugarlesstea,1552010526,[removed],0,1,False,self,,,,,
471,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,11,ayl3wb,self.MachineLearning,400GB data-set of Conversational Audio. (Unlabeled) Cross-post from r/NLP [Project],https://www.reddit.com/r/MachineLearning/comments/ayl3wb/400gb_dataset_of_conversational_audio_unlabeled/,gittb,1552012601,"New here. Currently working on a project to explore the abilities of the transformer architecture to elaborate from data similar to how Open AI's release with GPT-2 does for text. I have scrapped about 400GB of conversational audio data with most files in both .ogg and .mp3 from podcasts and audio books available on [archive.org](https://archive.org). I would be happy to share with anyone that is interested. 

&amp;#x200B;

Here is a link to the readme for my project for those interested in that: [https://github.com/gittb/audiosandbox/blob/master/README.md](https://github.com/gittb/audiosandbox/blob/master/README.md)

&amp;#x200B;

If anyone has any knowledge into how I would possibly encode an STFT array similar to how text vectors are fed to the GPT-2 model I would love to hear your approach. I'm a current student in college and always looking for explanations and viewpoints. (My college's CS program is beyond terrible with very very few machine learning resources.)",50,253,False,self,,,,,
472,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,12,aylg5l,self.MachineLearning,"[D] In seq2seq, why decoder lstm need h_0 ?",https://www.reddit.com/r/MachineLearning/comments/aylg5l/d_in_seq2seq_why_decoder_lstm_need_h_0/,sjh9020,1552014846,"Decoder of seq2seq have encoder's output as lstm hidden states.

Then why decoder need initialized hidden states??",2,2,False,self,,,,,
473,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,12,aylh89,self.MachineLearning,Download Common Voice Dataset Now!,https://www.reddit.com/r/MachineLearning/comments/aylh89/download_common_voice_dataset_now/,limapedro,1552015045,[removed],0,1,False,self,,,,,
474,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,12,ayli0e,self.MachineLearning,Lab X published 20 papers in one conference!,https://www.reddit.com/r/MachineLearning/comments/ayli0e/lab_x_published_20_papers_in_one_conference/,snjf,1552015192,[removed],0,1,False,self,,,,,
475,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,12,ayljv5,self.MachineLearning,"[D] Need a ""wow factor"" ideas for a machine learning hackathon that I'm joining",https://www.reddit.com/r/MachineLearning/comments/ayljv5/d_need_a_wow_factor_ideas_for_a_machine_learning/,sugarlesstea,1552015535,"Hey guys. I'm joining a Machine Learning based hackathon organized by my workplace. Unfortunately, we are not allowed to use our own project idea but instead, we need to choose one from a list of business problems given by our managers. Problem is, most of the idea are generic Machine Learning problem statement and can be easily solved by training an appropriate model. Some of the topics are:  
  
* Surveillance using face recognition
* Insurance business investment prediction
* Video content filtering
* Question and answer  chatbot
* and many more..

Me and my team ended up choosing Video content filtering topic. We don't have any problem doing the program but the problem is I felt that our system is very ""meh"" at best. Basically, using Tensorflow, we trained a model to detect inappropriate content (e.g gore, porn, politics). The user will then feed a video to the program and the program will determine whether the video is ""allowed"" or ""blocked"". So far it does it jobs but I feel like it's not up to a ""hackathon quality"". 
  
I'm very new to Machine Learning so I don't really have much idea on how to improve this program anymore. Do you guys have any ideas on how can I improve the program? Is there any ""wow factor"" that I can add to the program so that it will stand-out among other participants?",9,0,False,self,,,,,
476,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,13,aym95k,self.MachineLearning,"[P] Made an customizable, intuitive gym environment with asynchronous visualization.",https://www.reddit.com/r/MachineLearning/comments/aym95k/p_made_an_customizable_intuitive_gym_environment/,Dmit4dit,1552020364,"Link: [hiddenpath](https://github.com/dmit4git/hiddenpath)

Hello ~~World~~ Reddit!

I'm new to ML\\RL. I started playing around with OpenAI gym some time ago and I couldn't find an environment that is intuitive, easily customizable and have responsive visualization, so I made one. Hope it can be useful for other RL newbies.

In the environment agent moves from left to right and learns to follow an unobservable path, which can be easily input in form of an image. Visualization is asynchronous, it stays responsible (you can resize, zoom and pan) when execution of the environment \\ agent is stopped for debugging. The environment supports multiple `gym.spaces` for better compatibility with various agents, examples with a custom agent, tensorforce PPO and baselines deep-q are provided.

I would highly appreciate an advise on configuration of a deep learning agent for the default environment path.",0,7,False,self,,,,,
477,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,14,aymlop,self.MachineLearning,"As a newbie to Machine Learning, how do I approach this problem? What are some resources for it?",https://www.reddit.com/r/MachineLearning/comments/aymlop/as_a_newbie_to_machine_learning_how_do_i_approach/,LostRaider1297,1552022820,[removed],0,1,False,self,,,,,
478,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,14,aymqx1,self.MachineLearning,The latest TensorFlow2.0 not supported by Anaconda?,https://www.reddit.com/r/MachineLearning/comments/aymqx1/the_latest_tensorflow20_not_supported_by_anaconda/,zhong2024,1552023898,[removed],0,1,False,self,,,,,
479,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,16,aynd1v,self.MachineLearning,Rogue vs BLEU vs METEOR for evaluating sentences. What should I use?,https://www.reddit.com/r/MachineLearning/comments/aynd1v/rogue_vs_bleu_vs_meteor_for_evaluating_sentences/,Vinceeeent,1552028656,[removed],0,1,False,self,,,,,
480,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,16,ayng35,self.MachineLearning,Small Satellite Services Market Analysis Reveals hazardous development by 2023,https://www.reddit.com/r/MachineLearning/comments/ayng35/small_satellite_services_market_analysis_reveals/,apple_x9,1552029335,[removed],1,1,False,self,,,,,
481,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,16,aynjcg,socialprachar.com,Machine Learning,https://www.reddit.com/r/MachineLearning/comments/aynjcg/machine_learning/,rajeshwargujja,1552030089,,0,1,False,default,,,,,
482,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,16,aynlrd,self.MachineLearning,Variable Frequency Drive Market to Perceive Substantial Growth during 2023,https://www.reddit.com/r/MachineLearning/comments/aynlrd/variable_frequency_drive_market_to_perceive/,apple_x9,1552030669,[removed],1,1,False,self,,,,,
483,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,16,aynr5x,quytech.com,Artificial Intelligent in Brand Safety and Counterfeit Detection,https://www.reddit.com/r/MachineLearning/comments/aynr5x/artificial_intelligent_in_brand_safety_and/,quytech1,1552031975,,0,1,False,default,,,,,
484,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,17,aynwdm,self.MachineLearning,Chemical Tanker Shipping Market to Perceive Substantial Growth during 2023,https://www.reddit.com/r/MachineLearning/comments/aynwdm/chemical_tanker_shipping_market_to_perceive/,apple_x9,1552033214,[removed],1,1,False,self,,,,,
485,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,17,ayo01y,self.MachineLearning,Machine Learning and Deep Learning Projects and Comprehensive tutorials,https://www.reddit.com/r/MachineLearning/comments/ayo01y/machine_learning_and_deep_learning_projects_and/,prithvi45,1552034183,[removed],0,1,False,self,,,,,
486,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,18,ayoaea,self.MachineLearning,How to Build Career in AI &amp; Machine Learning,https://www.reddit.com/r/MachineLearning/comments/ayoaea/how_to_build_career_in_ai_machine_learning/,rajeshwargujja,1552036727," AI is returning additional traction late due to recent innovations that have created headlines, Alexas sudden happy yet. however, AI has been a sound career selection for a long time currently due to the growing adoption of the technology across industries and therefore the want for trained professionals to try and do the roles created by this growth. Pundits predict that AI can produce near a pair of.3 million jobs by 2020. However, its additionally forecasted that this technology can wipe out over one.7 million jobs, leading to regarding 0.5 1,000,000 new jobs worldwide. additionally, AI additionally offers several distinctive and viable career opportunities. AI is employed in virtually every business, from diversion to transportation, nonetheless, weve got an enormous want for qualified, sure-handed professionals. ",0,0,False,self,,,,,
487,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,21,aypzkt,self.MachineLearning,Where is ELM today?,https://www.reddit.com/r/MachineLearning/comments/aypzkt/where_is_elm_today/,kranditinator,1552049894,[removed],0,1,False,self,,,,,
488,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,23,ayqqnr,self.MachineLearning,[P] edafa: a python package for test time augmentation (TTA),https://www.reddit.com/r/MachineLearning/comments/ayqqnr/p_edafa_a_python_package_for_test_time/,andrewekhalel,1552054553,"[edafa](https://github.com/andrewekhalel/edafa) is a simple wrapper that implements Test Time Augmentations (TTA) on images for computer vision problems like: segmentation, classification, super-resolution, Pansharpening, etc. You don't need to do much to improve your scores! Check examples here for [Keras](https://github.com/andrewekhalel/edafa/tree/master/examples/keras) and [PyTorch](https://github.com/andrewekhalel/edafa/tree/master/examples/pytorch).

Let me know what you think!",0,10,False,self,,,,,
489,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,23,ayqtcf,self.MachineLearning,[D] TensorFlow Auto - TPOT with TFX components,https://www.reddit.com/r/MachineLearning/comments/ayqtcf/d_tensorflow_auto_tpot_with_tfx_components/,paubric,1552054972,"Hi all,

In light of the recent update to TensorFlow Extended, I was thinking of transferring the main idea behind the TPOT module from scikit-learn components to TensorFlow Extended components. Essentially, TPOT uses scikit-learn components such as preprocessors, regressors, and others, in order to build random ML pipelines which receive raw data as input and generate predictions as output. Then, the population of pipelines is evolved through genetic algorithms in order to optimize performance. I was thinking of building something similar which uses TFX components instead.

What are your thoughts on the idea? Does it make any sense? Would you use something like that? Would you contribute?

I wrote the idea in the README: https://github.com/paubric/tensorflow-auto",2,3,False,self,,,,,
490,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,23,ayr4d2,self.MachineLearning,[D] What are some augmentation method for non-image data?,https://www.reddit.com/r/MachineLearning/comments/ayr4d2/d_what_are_some_augmentation_method_for_nonimage/,cesusjhrist,1552056775,"Hi,

I want to expand the size of my dataset, composed of credit card transaction that has the following features: data, amount, description and category (restaurant, flight etc etc).

&amp;#x200B;

Can you recommend me some data augmentation techniques? All I can find involve image data. Thanks",21,3,False,self,,,,,
491,MachineLearning,t5_2r3gv,2019-3-8,2019,3,8,23,ayr4e1,go.geeklearn.net,What are Symbolic and Imperative APIs in TensorFlow 2.0?,https://www.reddit.com/r/MachineLearning/comments/ayr4e1/what_are_symbolic_and_imperative_apis_in/,CharlesPolley,1552056781,,0,1,False,default,,,,,
492,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,0,ayr9jg,ai.googleblog.com,Real-Time AR Self-Expression with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/ayr9jg/realtime_ar_selfexpression_with_machine_learning/,sjoerdapp,1552057598,,0,1,False,default,,,,,
493,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,0,ayrhoy,self.MachineLearning,How to install PlaidML?,https://www.reddit.com/r/MachineLearning/comments/ayrhoy/how_to_install_plaidml/,Oninteressant123,1552058879,"I'm very new to machine learning. I have yet to successfully make my own network and have mainly been messing with code from github. I'm on a MacBook Pro with an AMD GPU, so no support for CUDA. I found PlaidML and installed it into a conda virtual environment, then ran MobileNet, which worked as it was supposed to. I can't seem to figure out how to run my own code on it though. If I set my keras backend to plaidml (by changing keras.json), it gives me an error. After searching the web for a while, I found something telling me to do:

    library(keras)
    use_condaenv(""plaidml"") 
    use_backend(""plaidml"")

but I can't seem to figure out where I'm supposed to put this code. If someone could help out that would be great. Obviously I'm extremely new to machine learning and have no idea what I'm doing. Thanks!",0,1,False,self,,,,,
494,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,0,ayrk6c,self.MachineLearning,How NLP Is Teaching Computers The Meaning Of Words,https://www.reddit.com/r/MachineLearning/comments/ayrk6c/how_nlp_is_teaching_computers_the_meaning_of_words/,conversational-ai,1552059270,"Explore the field of NLP and how it is getting machines to not just see words but also understand them with neural-network-based representations and embeddings.  
[https://cai.tools.sap/blog/how-nlp-is-teaching-computers-the-meaning-of-words/](https://cai.tools.sap/blog/how-nlp-is-teaching-computers-the-meaning-of-words/)",0,1,False,self,,,,,
495,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,0,ayrkss,self.MachineLearning,"Meta-Learning: Predict for every single instance in a dataset, which algorithm to use (Any Related Work on this?)",https://www.reddit.com/r/MachineLearning/comments/ayrkss/metalearning_predict_for_every_single_instance_in/,HaHerold,1552059367,"Hi Community,

&amp;#x200B;

AutoML and meta-learning is a very popular topic recently, but all the work I am aware of focuses on finding the best algorithm for an entire dataset. Could it make sense to use different algorithms (or hyper parameter configurations) for different instances in a dataset? Consequently, could it make sense to meta-learn, which algorithm will perform best for instance i1, and which algorithm will perform best for instance i2 ...?

&amp;#x200B;

Is anyone aware of related work? I couldn't find any.

&amp;#x200B;

Hannah",0,1,False,self,,,,,
496,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,0,ayrlnw,self.MachineLearning,"Reinforcement Learning, does my idea have a name?",https://www.reddit.com/r/MachineLearning/comments/ayrlnw/reinforcement_learning_does_my_idea_have_a_name/,Arnaz87,1552059500,[removed],0,1,False,self,,,,,
497,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,0,ayrmbh,self.MachineLearning,"With respect to neural network activation functions, what is the definition of ""nonlinear""?",https://www.reddit.com/r/MachineLearning/comments/ayrmbh/with_respect_to_neural_network_activation/,Cannonball_Z,1552059595,[removed],0,1,False,self,,,,,
498,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,0,ayrp2o,self.MachineLearning,[P] Dissecting GitHub Code Reviews: A Text Classification Experiment,https://www.reddit.com/r/MachineLearning/comments/ayrp2o/p_dissecting_github_code_reviews_a_text/,zir093,1552059994,"**Blog Post**: [http://mfadhel.com/github-code-reviews/](http://mfadhel.com/github-code-reviews/)

&amp;#x200B;

I've always wanted to get a better idea of what code reviewers typically discuss in code reviews. In this experiment, I build an SVM classifier to analyze over 30k GitHub review comments to get to the bottom of this very question. Did these results match your expectations?",1,2,False,self,,,,,
499,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,1,ays31i,self.MachineLearning,[R] Neural Language Understanding of Peoples Names In Dialogue,https://www.reddit.com/r/MachineLearning/comments/ays31i/r_neural_language_understanding_of_peoples_names/,CaHoop,1552062073,"I've been working on value extraction in dialogue systems for the last few months. My colleague Matt has written a post which describes the process we followed in building our new value extraction system:

[https://poly-ai.com/blog/neural-language-understanding-of-peoples-names](https://poly-ai.com/blog/neural-language-understanding-of-peoples-names)",0,14,False,self,,,,,
500,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,1,aysev9,self.MachineLearning,JavaScript and machine learning,https://www.reddit.com/r/MachineLearning/comments/aysev9/javascript_and_machine_learning/,_Jonny_N_,1552063810,[removed],0,1,False,self,,,,,
501,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,2,aysr6q,self.MachineLearning,GPU PCIE 3.0 x16 utilization question,https://www.reddit.com/r/MachineLearning/comments/aysr6q/gpu_pcie_30_x16_utilization_question/,the_engineer_,1552065590,"Are there any GPU's that can utilize the full pcie x16 bandwith? PCIE 3.0 x16 is 15.75 GB/s. It's been mentioned before, I'm just not able to find it for some reason. For example: The Radeon VII has 16GB HBM2, it has a memory bandwidth of 1TB/s, and and an effective memory speed of 4Gbps (not exactly sure what that means). Also, does the GPU itself have anything to do with the data utilization and speed? Basically I was hoping someone could help me understand GPU specs.",0,1,False,self,,,,,
502,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,2,aysxzs,machinedlearnings.com,RL will disrupt OR,https://www.reddit.com/r/MachineLearning/comments/aysxzs/rl_will_disrupt_or/,justnikos,1552066569,,0,1,False,default,,,,,
503,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,2,aysyf1,self.MachineLearning,[D] GPU PCIE 3.0 x16 utilization question,https://www.reddit.com/r/MachineLearning/comments/aysyf1/d_gpu_pcie_30_x16_utilization_question/,the_engineer_,1552066627,"Are there any GPU's that can utilize the full pcie x16 bandwith? PCIE 3.0 x16 is 15.75 GB/s. It's been mentioned before, I'm just not able to find it for some reason. For example: The Radeon VII has 16GB HBM2, it has a memory bandwidth of 1TB/s, and and an effective memory speed of 4Gbps (not exactly sure what that means). Also, does the GPU itself have anything to do with the data utilization and speed? Basically I was hoping someone could help me understand GPU specs.",2,5,False,self,,,,,
504,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,2,aysz96,machinedlearnings.com,[D] RL will disrupt OR,https://www.reddit.com/r/MachineLearning/comments/aysz96/d_rl_will_disrupt_or/,justnikos,1552066754,,0,1,False,https://a.thumbs.redditmedia.com/SsFbXs8p-PekvU5e0yGrHYVVKdKUWznEdA0A2ESei54.jpg,,,,,
505,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,2,ayt50m,medium.com,Google Debuts TensorFlow 2.0 Alpha,https://www.reddit.com/r/MachineLearning/comments/ayt50m/google_debuts_tensorflow_20_alpha/,Yuqing7,1552067619,,0,1,False,default,,,,,
506,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,3,aythln,self.MachineLearning,[R] RNN-Based Handwriting Recognition in Gboard,https://www.reddit.com/r/MachineLearning/comments/aythln/r_rnnbased_handwriting_recognition_in_gboard/,modeless,1552069432,"Blog: https://ai.googleblog.com/2019/03/rnn-based-handwriting-recognition-in.html

arXiv: https://arxiv.org/abs/1902.10525",1,60,False,self,,,,,
507,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,3,ayti2y,self.MachineLearning,[P] Is there any tool for implementing MLP inference on FPGA?,https://www.reddit.com/r/MachineLearning/comments/ayti2y/p_is_there_any_tool_for_implementing_mlp/,lazanz,1552069502,I've trained a rather small feed forward network which I want to implement on hardware. The latency is very important and needs to have very low variance so FPGA is the natural choice. The problem is I don't have any digital design experience. I need a tool which can control bit depth and estimate accuracy loss for the final hardware. Is there any software tool that does this?,15,14,False,self,,,,,
508,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,3,aytjz0,arxiv.org,[R] Adversarial Networks and Autoencoders: The Primal-Dual Relationship and Generalization Bounds,https://www.reddit.com/r/MachineLearning/comments/aytjz0/r_adversarial_networks_and_autoencoders_the/,anonymousTestPoster,1552069779,,2,39,False,default,,,,,
509,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,3,aytkw1,arxiv.org,[1903.02709] Adversarial Mixup Resynthesizers,https://www.reddit.com/r/MachineLearning/comments/aytkw1/190302709_adversarial_mixup_resynthesizers/,alexmlamb,1552069907,,8,2,False,default,,,,,
510,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,3,aytoem,self.MachineLearning,[D] H2O.AI Driverless AI - Alternate Docker Image,https://www.reddit.com/r/MachineLearning/comments/aytoem/d_h2oai_driverless_ai_alternate_docker_image/,AI_Madness,1552070416,"DriverlessAI is a wonderful tool from [H2O.AI](https://H2O.AI), however, the time trial is limited to 21-days which can sometimes be rather limiting. For those requiring more time I have found this revised docker container to provide extended evaluation

&amp;#x200B;

Download the file here

&amp;#x200B;

[https://my.pcloud.com/publink/show?code=XZJFeq7Z1QDVoG2MjzyRzwDtRWyOTuwI3JxX](https://my.pcloud.com/publink/show?code=XZJFeq7Z1QDVoG2MjzyRzwDtRWyOTuwI3JxX)

&amp;#x200B;

Execute: 

docker load &lt; dai154.tar.gz

&amp;#x200B;

Create the directories referenced in the docker command below. If you plan on using your own config you can add that

&amp;#x200B;

Dump current config to data directory to use for editing the file and moving to the config directory.

&amp;#x200B;

docker run -it dai154 /data-config.sh

&amp;#x200B;

To echo the current config or save to an alternate file

&amp;#x200B;

docker run -it dai154 /echo-config.sh &gt; config.txt

&amp;#x200B;

To execute the container be sure to create the directories needed, in this instance just create  h2o/data, h2o/log, h2o/config, h2o/tmp

&amp;#x200B;

Execute:

&amp;#x200B;

docker run -it \\

\--pid=host \\

\--init \\

\--rm \\

\--shm-size=256m \\

\-u \`id -u\`:\`id -g\` \\

\-p 12345:12345 \\

\-e DRIVERLESS\_AI\_CONFIG\_FILE=""/config/config.toml"" \\

\-v \`pwd\`/h2o/data:/data \\

\-v \`pwd\`/h2o/log:/log \\

\-v \`pwd\`/h2o/tmp:/tmp \\

\-v \`pwd\`/h2o/config:/config \\

dai154

&amp;#x200B;

You will be dropped to a shell prompt inside the container but in the background everything is starting, wait a few minutes then hit the URL [http://localhost:12345](http://localhost:12345) and use any login.

&amp;#x200B;

All of this is per the normal DriverlessAI documentation",3,0,False,self,,,,,
511,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,3,ayts59,self.MachineLearning,Artificial Intelligence and Machine Learning Pros: Advance your career and income growth with Deep Learning Specialization,https://www.reddit.com/r/MachineLearning/comments/ayts59/artificial_intelligence_and_machine_learning_pros/,internetdigitalentre,1552070956,[removed],0,1,False,self,,,,,
512,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,3,aytudy,self.MachineLearning,Google Colab,https://www.reddit.com/r/MachineLearning/comments/aytudy/google_colab/,Chuck3131,1552071286,[removed],0,1,False,self,,,,,
513,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,5,ayuqxx,self.MachineLearning,Tensorflow 2.0 alpha,https://www.reddit.com/r/MachineLearning/comments/ayuqxx/tensorflow_20_alpha/,stark_9190,1552076013,[removed],0,1,False,self,,,,,
514,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,5,ayv3cs,self.MachineLearning,why there is no Machine Learning to analyze read and interpret ECG?,https://www.reddit.com/r/MachineLearning/comments/ayv3cs/why_there_is_no_machine_learning_to_analyze_read/,martin80k,1552077904,[removed],0,1,False,self,,,,,
515,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,5,ayv3ea,self.MachineLearning,"[Project] Digideep: A developer-friendly pipeline for fast-prototyping reinforcement (RL) methods with PyTorch, Gym, and dm_control",https://www.reddit.com/r/MachineLearning/comments/ayv3ea/project_digideep_a_developerfriendly_pipeline_for/,sharif1093,1552077911,"Github: [https://github.com/sharif1093/digideep](https://github.com/sharif1093/digideep)

Documentation: [https://digideep.readthedocs.io/en/latest/](https://digideep.readthedocs.io/en/latest/)

&amp;#x200B;

Selected Features:

* **Developer-friendly code: Documentation + Easy to read code + Debugging tools**
* Supports multi-core / multi-gpu architectures on a single node.
* Connects to `dm_control` and uses `dm_control's` native viewer for rendering. Also provides batch environment for `dm_control`.
* Controls all parameters from one **single parameter file**.
* Supports object (de-)serialization structurally.
* Provides **PPO** and **DDPG** implementations.",0,4,False,self,,,,,
516,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,6,ayvbyo,blog.insightdatascience.com,Women changing the world with data science,https://www.reddit.com/r/MachineLearning/comments/ayvbyo/women_changing_the_world_with_data_science/,hszafarek,1552079190,,0,1,False,https://b.thumbs.redditmedia.com/-yzV_k9KA6aodC4LwXHYFqJWRPe2hbv4qKOhNqhavFI.jpg,,,,,
517,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,6,ayvjv3,blog.insightdatascience.com,Women changing the world with data science,https://www.reddit.com/r/MachineLearning/comments/ayvjv3/women_changing_the_world_with_data_science/,hszafarek,1552080377,,0,1,False,https://b.thumbs.redditmedia.com/-yzV_k9KA6aodC4LwXHYFqJWRPe2hbv4qKOhNqhavFI.jpg,,,,,
518,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,6,ayvoj2,blog.insightdatascience.com,Women changing the world with data science,https://www.reddit.com/r/MachineLearning/comments/ayvoj2/women_changing_the_world_with_data_science/,hszafarek,1552081087,,0,1,False,default,,,,,
519,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,7,ayvxvc,self.MachineLearning,[D] Are the connections between deep learning and neuroscience still relevant?,https://www.reddit.com/r/MachineLearning/comments/ayvxvc/d_are_the_connections_between_deep_learning_and/,N1N1,1552082489,"I think it's a pretty common belief here that modern deep learning research has moved away from being inspired by neuroscience, and I would say I was also, until recently, under this belief. However, after reading Moheb Costandi's excellent *Neuroplasticity*, it made me rethink that a bit.


Although I don't think most modern DL research is directly inspired by neuroscience, there are some interesting connections I hadn't realized before. For example:

* ResNet: The idea of a gradient super-highway is not so unlike the importance of signal propagation in Long Term Potentiation (LTP), which is thought to underlie learning and memory in the brain. Some researchers also believe that retrograde signaling happens in LTP, meaning that the LTP signal is back-propagated to the instigating pre-synaptic neuron. Obviously, the connection with back-propagation in DL is clear, but backprop is not so much a new/modern technique.
* Transfer Learning: The ability of neuroplasticity in brain-injured or dear/blind peoples allows specialized neural regions to adapt and perform tasks outside their specialized region. For example, some blind people use their visual cortex more during language processing than the control. 
* Capsule Networks: Actually indeed inspired by biology, these networks take a routing approach to visual classification. This relates to how the visual cortex has separate pathways for the *what* and *where* of an object and to how human vision is position and scale invariant, unlike traditional convolutional neural networks. Evidently, Hinton's paper directly cites a neuroscience paper on dynamic routing.
* Self-play: Recent advances like AlphaZero and OpenAI Five have shown the power of self-play in the RL domain. Similarly, we know that practicing different skills, like music, can have long-term affects on brain structure and connections. However, these algorithms are able to make changes much faster to themselves than adults humans, so perhaps they are closer to the intensive neurogenesis and pruning period of early childhood.

These are just some examples I thought, and, although they aren't necessarily all inspired by neuroscience, I think that the connections should be taken more seriously. I think dismissing the connections between these two fields may be naive, and it seems neuroscience has a part to play in the future of deep learning.


",78,203,False,self,,,,,
520,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,8,aywnoz,self.MachineLearning,Naive Bayes conditional independence assumption,https://www.reddit.com/r/MachineLearning/comments/aywnoz/naive_bayes_conditional_independence_assumption/,oneofchaos,1552086275,[removed],0,1,False,self,,,,,
521,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,9,ayx9k5,self.MachineLearning,[DL] Any views on newer algorithms for small object detection?,https://www.reddit.com/r/MachineLearning/comments/ayx9k5/dl_any_views_on_newer_algorithms_for_small_object/,naboo_random,1552089822,[removed],0,1,False,self,,,,,
522,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,10,ayxufh,self.MachineLearning,[R] Reparameterizing Distributions on Lie Groups,https://www.reddit.com/r/MachineLearning/comments/ayxufh/r_reparameterizing_distributions_on_lie_groups/,pimdehaan,1552093421,"Paper: [https://arxiv.org/abs/1903.02958](https://arxiv.org/abs/1903.02958)

Code: [https://github.com/pimdh/relie](https://github.com/pimdh/relie)",0,18,False,self,,,,,
523,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,10,ayy1cm,self.MachineLearning,[N]Fetch.AI will change the digital world by their blockchain and artificial intelligence technology got listed on KuCoin exchange.,https://www.reddit.com/r/MachineLearning/comments/ayy1cm/nfetchai_will_change_the_digital_world_by_their/,repulsiveback,1552094661,This platform is about to change the digital world with their fast-growing community and has a a lot of good technology with machine learning for the benefits of the economics and marketplaces they are now on their new exchange [https://www.kucoin.com/news/en-fetch-ai-fet-gets-listed-on-kucoin](https://www.kucoin.com/news/en-fetch-ai-fet-gets-listed-on-kucoin),1,0,False,self,,,,,
524,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,10,ayy1xu,self.MachineLearning,[DL] How to deal with image data captured from different camera sources,https://www.reddit.com/r/MachineLearning/comments/ayy1xu/dl_how_to_deal_with_image_data_captured_from/,naboo_random,1552094773,[removed],0,1,False,self,,,,,
525,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,11,ayynrd,self.MachineLearning,"[D] Review my TF feature request for option to have weights stored as binary files and loaded into memory when trained. Useful for word embedding models, higher dropout rates, pruning, weight freezing / fine tuning. Did a lit search finding as many uses for this feature, let me know if I missed any",https://www.reddit.com/r/MachineLearning/comments/ayynrd/d_review_my_tf_feature_request_for_option_to_have/,BatmantoshReturns,1552098926,"The feature I described below was something that came to me while training large embedding-based recommender systems, but I did a literature survey to find as many cases where this feature would be beneficial. If you know of any others, please post it so that when the TF developers see it, they will have the best info in determining where such a feature might rank in. 

------------------------

&lt;em&gt;Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template&lt;/em&gt;

**System information**
- TensorFlow version (you are using): 1.12.0
- Are you willing to contribute it (Yes/No): yes

**Describe the feature and the current behavior/state.**

Say that I have an architecture which has a lot of weights, but only a few are trained at a time. 

An example of this is training embeddings (like word2vec) for 100 million items, and each item has an embedding length of 1000. 

That a lot to have in active memory, and would cause a lot of environments to fail. And out of all those embeddings, only a few thousand embeddings trained with each batch; so it's not necessary to have all the embeddings loaded into memory. 

I would love to have a system similar to datasetAPI/Tfrecords, where the weights are stored in binary file format until they're up for training, loaded when they're trained, and then the updated weights are re-written to the binary format. 

Or if there's some workaround in Tensorflow as it is now, would love to know how!

**Will this change the current api? How?**

There could be many different ways to implement this. Probably a new variable type has to be created. Also, the variable and/or training functions would probably have to specific how the loading/training/writing of variable takes place. 

**Who will benefit with this feature?**

This would readily benefit those who are making developing item embedding systems with large numbers of items, such as word embedding models. 

In general this would benefit any system with a large number of parameters, where only a fraction of the parameters are updated during each training step. 

In addition to word embedding models, such a feature could also help in transfer learning / fine tuning / layer freezing models (BERT, GPT-2), pruning, higher dropout rates, pathways optimization (Pathnet), and even certain optimization algorithms such as  Coordinate Gradient descent. 

Finally, such a feature may induce the development of brand new machine learning algorithms/architectures, which would outperform current SOA models, especially models for multi-task based training. 

**Any Other info.**",12,3,False,self,,,,,
526,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,12,ayz857,youtube.com,[N] TensorFlow Dev Summit 2019,https://www.reddit.com/r/MachineLearning/comments/ayz857/n_tensorflow_dev_summit_2019/,thntk,1552102901,,1,1,False,https://b.thumbs.redditmedia.com/cprLf_iudoiOBWtm5cV2MRY-qLz8WN8N7TwFsWqb9uU.jpg,,,,,
527,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,13,ayzhwa,forbes.com,GE Says Its Leveraging Artificial Intelligence To Cut Product Design Times In Half,https://www.reddit.com/r/MachineLearning/comments/ayzhwa/ge_says_its_leveraging_artificial_intelligence_to/,PM_ME_ANYTHING_TODAY,1552104770,,0,1,False,default,,,,,
528,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,14,ayzxbq,self.MachineLearning,Machine learning productivity hacks,https://www.reddit.com/r/MachineLearning/comments/ayzxbq/machine_learning_productivity_hacks/,mrahtz,1552107856,[removed],0,1,False,self,,,,,
529,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,14,ayzxlt,self.MachineLearning,[D] Machine learning productivity hacks,https://www.reddit.com/r/MachineLearning/comments/ayzxlt/d_machine_learning_productivity_hacks/,mrahtz,1552107915,"A larger part than I'd like of my day-to-day research activity at the moment is just spent checking and recording run results.

[http://amid.fish/ml-productivity](http://amid.fish/ml-productivity) is a short post on some of the tools I've started using to cut down on that, but I'm curious - what have other people done to improve their research productivity? (Interpret as you like - developing ideas, implementing, debugging...)",63,226,False,self,,,,,
530,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,14,ayzygl,self.MachineLearning,What are some good resources to refresh python knowledge before AI job interview,https://www.reddit.com/r/MachineLearning/comments/ayzygl/what_are_some_good_resources_to_refresh_python/,PLANTBASEDPERMIE,1552108077,[removed],0,1,False,self,,,,,
531,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,15,az0hy8,self.MachineLearning,How to Classify Matrix? anomaly detection,https://www.reddit.com/r/MachineLearning/comments/az0hy8/how_to_classify_matrix_anomaly_detection/,daitranskku,1552112263,[removed],0,1,False,self,,,,,
532,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,16,az0thk,i.redd.it,"Built a gpu rig for machine learning. 32 gigs of RAM, intel core i7 processor, a budget gigabyte motherboard and an MSI NVIDIA GTX 1080Ti. Tensorflow-gpu + opencv = loads of fun :)",https://www.reddit.com/r/MachineLearning/comments/az0thk/built_a_gpu_rig_for_machine_learning_32_gigs_of/,nishanthinide,1552114982,,0,1,False,https://b.thumbs.redditmedia.com/RctmmqlMa6An00eNfIYNfLP2e4JiKyEleWPgLMZ2IRg.jpg,,,,,
533,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,16,az14nu,self.MachineLearning,[Discussion] Can I train any AI Model that can be trained from books of a specific genre and then make inferences or answer questions from that knowledge?,https://www.reddit.com/r/MachineLearning/comments/az14nu/discussion_can_i_train_any_ai_model_that_can_be/,jalmaxfordrocks,1552117711,"As an example let's say I feed an ""AI model"" with 500 books related to diseases and their symptoms. Can it then diagnose the illness of a person when given specific symptoms? Combining this with books of Allopathy or Medicine it can then recommend medicines from the diagnosed illness.

Other fields include Law where rules of the state/country can be fed for building an AI Lawyer.

What technologies or algorithms can be used to make such a model?

Are there any technologies that are already made for this purpose? Any information for above mentioned queries is welcomed. Thank You in advance.",11,0,False,self,,,,,
534,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,16,az16w2,self.MachineLearning,I would like to build a new project implementing concepts of ML and AI can someone suggest something worth learning from?,https://www.reddit.com/r/MachineLearning/comments/az16w2/i_would_like_to_build_a_new_project_implementing/,OSHUNYO,1552118285,[removed],0,1,False,self,,,,,
535,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,18,az1yku,self.MachineLearning,Automated Data quality Indexs,https://www.reddit.com/r/MachineLearning/comments/az1yku/automated_data_quality_indexs/,Helllovesyou,1552125475,[removed],0,1,False,self,,,,,
536,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,19,az21iw,self.MachineLearning,ChatBot Development using Artificial Intelligence and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/az21iw/chatbot_development_using_artificial_intelligence/,Rezma6,1552126207,[removed],0,1,False,self,,,,,
537,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,20,az2h1e,self.MachineLearning,(HELP)ML Andrew Ng - Should i complete this course in Octave or try to do it in Python on my own,https://www.reddit.com/r/MachineLearning/comments/az2h1e/helpml_andrew_ng_should_i_complete_this_course_in/,0megajay,1552129915,[removed],0,1,False,self,,,,,
538,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,20,az2hjq,ittwist.com,Top 3 Technologies to Learn in 2019 for boosting your career,https://www.reddit.com/r/MachineLearning/comments/az2hjq/top_3_technologies_to_learn_in_2019_for_boosting/,pygaurav,1552130042,,0,1,False,default,,,,,
539,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,20,az2huh,self.MachineLearning,Mask RCNN Query,https://www.reddit.com/r/MachineLearning/comments/az2huh/mask_rcnn_query/,gauthampokemon97,1552130114,[removed],0,1,False,self,,,,,
540,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,21,az30by,self.MachineLearning,[P] Collection of best Machine Learning Resources,https://www.reddit.com/r/MachineLearning/comments/az30by/p_collection_of_best_machine_learning_resources/,Sig_Luna,1552134430,"Started this open-source project last week, courtesy of our community at RemoteML.

I'd like to collect the best resources in Machine Learning to learn, build and get a job.

We're open for contributions and PRs!

[https://bestofml.com/](https://bestofml.com/)",4,26,False,self,,,,,
541,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,21,az31f1,self.MachineLearning,PyTorch 1.0 vs Tensorflow 2.0?,https://www.reddit.com/r/MachineLearning/comments/az31f1/pytorch_10_vs_tensorflow_20/,hoosierpride1,1552134673,Which framework do you prefer and why?,0,1,False,self,,,,,
542,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,22,az3d93,self.MachineLearning,[R] DATASET RESEARCH - Help,https://www.reddit.com/r/MachineLearning/comments/az3d93/r_dataset_research_help/,ibi_a,1552137163,"Hello people, i looking for a webpage or some place where i can find numeric datasets for train a tensorflow model,  preferably datasets with a lot of info. If you can help me with a link or info, i will appreciate it. Thanks",2,0,False,self,,,,,
543,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,22,az3enk,self.MachineLearning,[D] Where do I start to learn machine learning?,https://www.reddit.com/r/MachineLearning/comments/az3enk/d_where_do_i_start_to_learn_machine_learning/,Pronoob_me,1552137457,"I'm currently pursuing undergraduate with strong grasp on programming language, specially python and lately(\~1 week) I've leaned a lot towards machine learning. I worked on a AI bot for tic-tac-toe using minimax(partially complete) as well as RL. I'm aiming forward to build a career into the same but I feel lost when looking for resources.

&amp;#x200B;

So far I've:  
\+ Started reading ""Hands on Machine Learning with Skikit and TF"", done with \~1/3rd part if that helps.  
\+ Tried going through research papers, however I find the need to google terms 4-5 times every paragraph and I feel like I'm not yet prepared enough for going through papers.

\+ Look through this sub daily to know new projects and gain information wherever I could.

&amp;#x200B;

Now, should I:  
\+ Continue the same? I've so far believed in self-study (never had classes even for academics, which is kinda believed to be a MUST here at least for school-age)

\+ Apply for some online course, if so, which?

&amp;#x200B;

Also, what kind of work are you expected to do in the field?

And do I need to have knowledge on R, I've read it's way better than Python for stats?",7,0,False,self,,,,,
544,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,22,az3ezn,self.MachineLearning,Static Malware Classifier,https://www.reddit.com/r/MachineLearning/comments/az3ezn/static_malware_classifier/,TomHatskevich,1552137528,[removed],0,1,False,self,,,,,
545,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,22,az3gz9,self.MachineLearning,[N] TensorFlow Dev Summit 2019,https://www.reddit.com/r/MachineLearning/comments/az3gz9/n_tensorflow_dev_summit_2019/,thntk,1552137945,"[TensorFlow Dev Summit 2019](https://www.youtube.com/watch?v=bDZ2q6OktQI)  
There are some cool stuffs that I overlooked in recent version, think I'll give TF2.0 a try.
My impression is PyTorch is easier to start learning/hacking deep learning, while TF2.0 is more robust and systematic for production after you get the knowledge/skills.",17,7,False,self,,,,,
546,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,22,az3po6,/r/MachineLearning/comments/az3po6/cool_no_doubt/,"Cool, no doubt",https://www.reddit.com/r/MachineLearning/comments/az3po6/cool_no_doubt/,Davyd_Armstrong,1552139667,,0,1,False,default,,,,,
547,MachineLearning,t5_2r3gv,2019-3-9,2019,3,9,23,az48uc,self.MachineLearning,[D] ICML reviews are out,https://www.reddit.com/r/MachineLearning/comments/az48uc/d_icml_reviews_are_out/,thirddanceofeternity,1552143241,Good luck!,123,90,False,self,,,,,
548,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,0,az4hxa,self.MachineLearning,[P] Give more advice about Study lecture for those who want to review the Machine Learning concept again and to those who have just learned Python.,https://www.reddit.com/r/MachineLearning/comments/az4hxa/p_give_more_advice_about_study_lecture_for_those/,nlkey2022,1552144820,"I almost have finished summarization about Machine Learning  main Keyword Review with Pure Python code and Pytorch to use this as study lecture in our club.  I refer to Professor Andrew Ng's  curriculum  a lot.

\-  main point Keyword :  Probability Review,  linear algebra Review, Linear Regression,  Logistic Regression , sigmoid, softmax, Cross Entropy, Optimizing , SGD,  Adagrad, RMSProp, AdaDelta, Adam optimizer,  Multi-Classification vs. Multi-labels Classification,  Overfitting , Regularization, weight decay, dropout,  Machine Learning Diagnostic, model save and load, train, test, CV set, Bias vs. Variance

\- pure python : manual gradient descent, linear(Univariate, Multivariate), logistic(binary, softmax)

\-  accurate analysis of foundation model : CNN, RNN, LSTM

&amp;#x200B;

This is my  repository  ,[https://github.com/graykode/DeepLearning-Study](https://github.com/graykode/DeepLearning-Study)

I want to get a lot of advice because I have been planning a study since mid-March. Thanks!!",0,0,False,self,,,,,
549,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,0,az4rb5,self.MachineLearning,[D] What makes a good TTS dataset?,https://www.reddit.com/r/MachineLearning/comments/az4rb5/d_what_makes_a_good_tts_dataset/,mlttsman,1552146464,"I have done a lot of training on different self-made TTS datasets (typically having around 3 hours of audio across a few thousand .wav files, all 22050 Hz) using Tacotron 1 and 2, starting from a pretrained LJSpeech model (using the same hyperparameters each time and to a similar number of steps) and am very confused why for some datasets the output audio ends up being very clear for many samples - sometimes even indistinguishable from the actual person speaking - and for other datasets the synthesised audio always has choppy aberrations. In all my datasets there is no beginning/ending silence, transcriptions are all correct, and the datasets have fairly similar phenome distributions. I have yet to narrow down what the issue is with datasets that only result in models that generate robotic/hissy speech.

To take an example from publicly available datasets: on https://keithito.github.io/audio-samples/ one can hear that the model trained on the Nancy Corpus sounds significantly less robotic and is clearer than the model trained on LJ Speech. Here https://syang1993.github.io/gst-tacotron/ is samples for a model trained on Blizzard 2013 on tacotron with extremely good quality compared to any samples I've heard from a model trained on LJ Speech using Tacotron, even though the Blizzard 2013 dataset used there is smaller than LJ Speech. Why might this be?

Any comments appreciated.",10,12,False,self,,,,,
550,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,0,az4tdb,self.MachineLearning,"Relationship between OpenCL, ROCm, TensorFlow",https://www.reddit.com/r/MachineLearning/comments/az4tdb/relationship_between_opencl_rocm_tensorflow/,circular_file,1552146828,[removed],0,1,False,self,,,,,
551,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,0,az4tt7,arxiv.org,Fast Efficient Hyperparameter Tuning for Policy Gradients,https://www.reddit.com/r/MachineLearning/comments/az4tt7/fast_efficient_hyperparameter_tuning_for_policy/,Hari_a_s,1552146903,,2,11,False,default,,,,,
552,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,1,az4y0q,self.MachineLearning,[P] retraining gpt-2 on tale of two cities,https://www.reddit.com/r/MachineLearning/comments/az4y0q/p_retraining_gpt2_on_tale_of_two_cities/,PuzzledProgrammer3,1552147601,"I tried retraining gpt-2 on tale of two cities in google colab with the help of nshepperd repo. Here is a sample from training and colab repo to train any txt with free gpu

[https://github.com/ak9250/gpt-2-colab](https://github.com/ak9250/gpt-2-colab)

&amp;#x200B;

sample 

&amp;#x200B;

https://i.redd.it/klz7ujy1a4l21.png

&amp;#x200B;

&amp;#x200B;",1,21,False,https://b.thumbs.redditmedia.com/vJ2I5OquoaNWEqGr9OUYy2rKs3XcAuLNEP0Da-QViMo.jpg,,,,,
553,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,1,az4yn2,self.MachineLearning,Where can I find a large number of photos for bulk-download?,https://www.reddit.com/r/MachineLearning/comments/az4yn2/where_can_i_find_a_large_number_of_photos_for/,cainoom,1552147706,[removed],0,1,False,self,,,,,
554,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,1,az51q2,self.MachineLearning,What is the intuition behind using reinforcement learning for tracking? And what other vision applications can see the benefits from RL,https://www.reddit.com/r/MachineLearning/comments/az51q2/what_is_the_intuition_behind_using_reinforcement/,SmartSpray,1552148206,[removed],0,1,False,self,,,,,
555,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,2,az5mcd,self.MachineLearning,Does anybody have any insight on Imperial Computing(ML &amp; AI) or UCL CSML MSc programmes,https://www.reddit.com/r/MachineLearning/comments/az5mcd/does_anybody_have_any_insight_on_imperial/,BiochemicalWarrior,1552151462,[removed],0,1,False,self,,,,,
556,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,2,az5mgo,self.MachineLearning,[D] Language Embedding Hosted Thesaurus?,https://www.reddit.com/r/MachineLearning/comments/az5mgo/d_language_embedding_hosted_thesaurus/,The_Austinator,1552151478,Is there a hosted service that will give me synonyms based on word embeddings? Or a context aware thesaurus that can take a whole sentence and replace a given word that isn't quite the right fit?,1,3,False,self,,,,,
557,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,2,az5my1,self.MachineLearning,I've been suddenly intrigued by machine learning and data science.,https://www.reddit.com/r/MachineLearning/comments/az5my1/ive_been_suddenly_intrigued_by_machine_learning/,starspec,1552151552,[removed],0,1,False,self,,,,,
558,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,2,az5n0d,self.MachineLearning,Dimensional analysis,https://www.reddit.com/r/MachineLearning/comments/az5n0d/dimensional_analysis/,Bovini_mitra,1552151561,[removed],0,1,False,self,,,,,
559,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,3,az6coj,self.MachineLearning,Starting machine learning want help from you guys.,https://www.reddit.com/r/MachineLearning/comments/az6coj/starting_machine_learning_want_help_from_you_guys/,yashgoel236437,1552155593,[removed],0,1,False,self,,,,,
560,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,3,az6e5i,self.MachineLearning,Why Were Basically Just Bonobos Monkeys with Better Tech | Rebecca Costa,https://www.reddit.com/r/MachineLearning/comments/az6e5i/why_were_basically_just_bonobos_monkeys_with/,The_Syndicate_VC,1552155826,"[Why Were Basically Just Bonobos Monkeys with Better Tech | Rebecca Costa](https://disruptors.fm/86-why-were-basically-bonobos-monkeys-with-better-tech-rebecca-costa/)

&amp;#x200B;

**Rebecca D Costa**([@rebeccacosta](https://twitter.com/@rebeccacosta)) is an American sociobiologist and futurist and a recipient of the prestigious Edward O. Wilson Biodiversity Technology Award. Her work has been featured in the*New York Times*,*Washington Post*,*USA Today*,*The Guardian*, and other leading publications and here weekly column, THE FIX, is presently featured on Newsmax and on her site: [rebeccacosta.com](http://www.rebeccacosta.com/)

Costa was the founder and CEO of one of the largest technology marketing firms in California, where she developed an extensive track record of introducing disruptive, bleeding edge technologies for industry leaders like HP, Apple Computer, Oracle, Siebel Systems, General Electric, 3M, and others.

Rebecca spent six years researching and writing the international bestseller[*The Watchmans Rattle: A Radical New Theory of Collapse*](https://amzn.to/2ReS1Pk) and herfollow-on book[*On the Verge*](https://amzn.to/2OxW5vT)shot to the top of Amazons #1 New Business Releases. The success of*The Watchmans Rattle*led to a popular weekly news program called*The Costa Report*which was syndicated throughout the United States until 2018.

 

**In our wide-ranging conversation, we cover many things, including:**

* How AI and predictive analytics shifts the balance of power
* Why the future is becoming increasingly knowable
* The truth about complexity and what it means for all of us
* How evolution has primed people to fail todays challenges
* The big problems facing society and how to solve them
* Why terrorism inevitably leads to Minority Report
* The effect of CRISPR on human nature
* How nano-bots will impact the future of healthcare and pharma
* The reason Rebecca isnt that worried about the existential risk
* How to think about the future of politics and governments
* Why we basically bonobos monkeys with better tech",0,1,False,self,,,,,
561,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,3,az6erq,self.MachineLearning,How would you compare NLP packages Flair vs Spacy ?,https://www.reddit.com/r/MachineLearning/comments/az6erq/how_would_you_compare_nlp_packages_flair_vs_spacy/,antmoreau,1552155922,,0,1,False,self,,,,,
562,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,3,az6fi0,self.MachineLearning,[D] How would you compare NLP libraries Spacy vs Flair ?,https://www.reddit.com/r/MachineLearning/comments/az6fi0/d_how_would_you_compare_nlp_libraries_spacy_vs/,antmoreau,1552156037,,3,9,False,self,,,,,
563,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,5,az7kbh,self.MachineLearning,The feasibility of advanced AI Assistants,https://www.reddit.com/r/MachineLearning/comments/az7kbh/the_feasibility_of_advanced_ai_assistants/,Milespl8,1552162440,[removed],2,1,False,https://b.thumbs.redditmedia.com/e2LHnkpjRWSIDMrge0Nrbond-qSf5vI3v562tjIRHBI.jpg,,,,,
564,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,5,az7pi9,self.MachineLearning,[D] The Promise of Hierarchical Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/az7pi9/d_the_promise_of_hierarchical_reinforcement/,hughbzhang,1552163291,PhD student Yannis Flet-Berliac gives a comprehensive overview of the latest in Hierarchical Reinforcement Learning. [https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/](https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/),9,180,False,self,,,,,
565,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,7,az8w54,self.MachineLearning,"Can someone give a ""Visual"" example of why Gauss Newton method converges faster than Vanilla SGD",https://www.reddit.com/r/MachineLearning/comments/az8w54/can_someone_give_a_visual_example_of_why_gauss/,soulslicer0,1552170103,"With vanilla SGD, we simply find the vector that points in the direction of the solution topography  that minimizes it, then we scale it with some scale factor or learning rate. With Gauss Newton method though, what is special that is happening? Are we somehow perhaps forcing the point in our solution to jump as ""low"" or as ""steep"" as possible in a linear way until it hits some curvature, then we try again?

&amp;#x200B;

Here is a diagram of what I think?

&amp;#x200B;

[https://imgur.com/a/VtP4z9K](https://imgur.com/a/VtP4z9K)",0,1,False,self,,,,,
566,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,8,az9be2,self.MachineLearning,Help my program predict with known data.,https://www.reddit.com/r/MachineLearning/comments/az9be2/help_my_program_predict_with_known_data/,iEnjinere,1552172668,[removed],0,1,False,self,,,,,
567,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,8,az9u5c,self.MachineLearning,"[P] Gradient: TensorFlow binding for .NET (C#, F#, VB, etc.)",https://www.reddit.com/r/MachineLearning/comments/az9u5c/p_gradient_tensorflow_binding_for_net_c_f_vb_etc/,lostmsu,1552175991,"Quick links: [NuGet](https://www.nuget.org/packages/Gradient/), [Getting Started](https://github.com/losttech/Gradient/#getting-started), [Samples](https://github.com/losttech/Gradient-Samples/), [Landing Page](https://losttech.software/gradient.html)

&amp;#x200B;

My company recently got a pre-release out for full C#/.NET binding to TensorFlow **Python** API. E.g. not only you can use primitive ops to construct graphs, but tf.keras, tf.data, tf.contrib, others are available too. It also supports TensorBoard integration. Of course, since it is a binding to Python API, you need Python and TensorFlow for Python installed (tensorflow, tensorflow-gpu, or tensorflow-rocm in pip).

&amp;#x200B;

Currently, the binding **only** supports **1.10.\*** with **Python 3.6** on Windows, Mac and Linux, but that's mostly because the focus was on getting it up and running as soon as possible. One major feature that is still missing is the ability to inherit from TensorFlow classes (e.g. to create a new type of Keras layer), but it is being actively worked on.

&amp;#x200B;

Please, read the [Getting Started](https://github.com/losttech/Gradient/#getting-started) before jumping to it to understand quirks, that you might have to deal with due to interfacing with Python. There are some [Samples](https://github.com/losttech/Gradient-Samples/) on GitHub, including Char-RNN (train+sample), GPT-2, image classification, etc.

&amp;#x200B;

License: The company is a very small shop, and not currently ready to open-source the whole thing, as it is our only project. The preview and the final release will be free for non-commercial use, and for small businesses (&lt; $1M/y profit AND &lt;$5M/y revenue). Also it is free for experimentation (e.g. if you try it, but not use any outputs for commercial gain). If you do not fit into one of the options above, but still want to run it, please contact us at: CONTACT --AT-- LOSTTECH.SOFTWARE for licensing and/or to request help with deployment.",0,5,False,self,,,,,
568,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,9,aza3nv,self.MachineLearning,[D] Undergrad student: better to focus on one area (e.g. NLP) or diversify?,https://www.reddit.com/r/MachineLearning/comments/aza3nv/d_undergrad_student_better_to_focus_on_one_area/,AnonymousHippo3,1552177693,"As an undergrad student interested in ML and with some experience in NLP, should I continue focusing on this area in my future projects and research positions? Or would it be better to diversify and seek out some projects or research involving things like computer vision? One reason I am wondering is because a lot of companies are woeking with computer vision in areas such as robotics and self driving vehicles so I am wondering if it would lead to better job prospects in the future to have experience with CV. Also, which would likely be more useful for grad school? ",16,50,False,self,,,,,
569,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,9,azaacs,self.MachineLearning,Tracking of individual objects: is it possible?,https://www.reddit.com/r/MachineLearning/comments/azaacs/tracking_of_individual_objects_is_it_possible/,1010isTEN,1552178927,[removed],0,1,False,self,,,,,
570,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,11,azawdc,self.MachineLearning,"Does this ""bigger on the inside meaning "" mean the term or term in the field of artificial intelligence, such as deep learning, machine learning, etc.?",https://www.reddit.com/r/MachineLearning/comments/azawdc/does_this_bigger_on_the_inside_meaning_mean_the/,Doctor_who1,1552183203,"Does this ""bigger on the inside meaning  "" mean the term or term in the field of artificial intelligence, such as deep learning, machine learning, etc.?

&amp;#x200B;",0,1,False,self,,,,,
571,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,13,azcb0e,self.MachineLearning,Who else applied for Microsoft Rsearch Intern - optimization and Machine learning,https://www.reddit.com/r/MachineLearning/comments/azcb0e/who_else_applied_for_microsoft_rsearch_intern/,Tolanimi,1552193493,[removed],0,1,False,self,,,,,
572,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,15,azctyh,arxiv.org,[1903.03094] Learning to Speak and Act in a Fantasy Text Adventure Game,https://www.reddit.com/r/MachineLearning/comments/azctyh/190303094_learning_to_speak_and_act_in_a_fantasy/,jinpanZe,1552197645,,5,56,False,default,,,,,
573,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,16,azdh6z,self.MachineLearning,[Project] A Transformer implementation in Keras' Imperative (Subclassing) API for TensorFlow.,https://www.reddit.com/r/MachineLearning/comments/azdh6z/project_a_transformer_implementation_in_keras/,suyash93,1552203655,"[https://github.com/suyash/transformer](https://github.com/suyash/transformer)

Currently I have a sentiment analysis demo working. Training for machine translation seems to require longer time and effort.

For attention visualization, couldn't get the visualization in [https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello\_t2t.ipynb](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb) to work, so just using heatmaps.",5,10,False,self,,,,,
574,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,17,azdng2,github.com,[N] Add tf.repeat support equivalent to numpy.repeat,https://www.reddit.com/r/MachineLearning/comments/azdng2/n_add_tfrepeat_support_equivalent_to_numpyrepeat/,Jul8234,1552205426,,0,1,False,default,,,,,
575,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,17,azdr1p,poscash.co.uk,Wireless Credit Card Machine Online,https://www.reddit.com/r/MachineLearning/comments/azdr1p/wireless_credit_card_machine_online/,jamesdeny,1552206463,,0,1,False,default,,,,,
576,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,17,azdv4r,self.MachineLearning,How do you think machine learning can solve the extrapolation case in non linear problems like weather forecasting or have you read any interesting paper on this topic?,https://www.reddit.com/r/MachineLearning/comments/azdv4r/how_do_you_think_machine_learning_can_solve_the/,BlueEyed-husky,1552207640,[removed],0,1,False,self,,,,,
577,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,18,aze1ao,self.MachineLearning,Search about a good implantation of C4.5 algorithm,https://www.reddit.com/r/MachineLearning/comments/aze1ao/search_about_a_good_implantation_of_c45_algorithm/,oussama-he,1552209437,[removed],0,1,False,self,,,,,
578,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,18,aze6xb,/r/MachineLearning/comments/aze6xb/biomedical_entity_recognition_app_using_biobert/,BioMedical Entity recognition App using Bio-Bert,https://www.reddit.com/r/MachineLearning/comments/aze6xb/biomedical_entity_recognition_app_using_biobert/,Rajat_,1552211119,,0,1,False,default,,,,,
579,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,18,aze8f5,self.MachineLearning,Where can I find vast resources of dialogue corpora,https://www.reddit.com/r/MachineLearning/comments/aze8f5/where_can_i_find_vast_resources_of_dialogue/,tastycakezs,1552211573,[removed],0,1,False,self,,,,,
580,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,18,aze8zm,self.MachineLearning,App for Identifiying Medical Entities using Bert(BioBert),https://www.reddit.com/r/MachineLearning/comments/aze8zm/app_for_identifiying_medical_entities_using/,Rajat_,1552211746,[removed],0,1,False,self,,,,,
581,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,19,azef43,self.MachineLearning,[D] An Excessively Deep Dive Into Natural Gradient Optimization,https://www.reddit.com/r/MachineLearning/comments/azef43/d_an_excessively_deep_dive_into_natural_gradient/,milaworld,1552213443,"Cody Wilds recent [blog post](https://towardsdatascience.com/its-only-natural-an-excessively-deep-dive-into-natural-gradient-optimization-75d464b89dbb) attempts to explain the intuitions behind Natural Gradient learning: how to connect high-level concepts with the often-difficult-to-parse math:

https://towardsdatascience.com/its-only-natural-an-excessively-deep-dive-into-natural-gradient-optimization-75d464b89dbb",5,172,False,self,,,,,
582,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,19,azefi5,self.MachineLearning,My first post in a series of articles about Rust for Python developers/ ML developers,https://www.reddit.com/r/MachineLearning/comments/azefi5/my_first_post_in_a_series_of_articles_about_rust/,readanything,1552213539,"[https://medium.com/@rajasekar3eg/making-a-case-rust-for-python-developers-1a114e2d89f4](https://medium.com/@rajasekar3eg/making-a-case-rust-for-python-developers-1a114e2d89f4)

I had a wonderful time learning Rust this past one year. I am from DataScience background. Despite Rust having almost zero presence in my field, I could find many ways to use Rust in work wherever possible. Yet I have struggled to introduced it to my colleagues initially. Now many have picked it up after seeing the results of my work(I have used it  only  where performance mattered). So I am trying to write a series of  articles introducing Rust in as simple way as possible. I am planning to introduce the concepts lightly without going deeper and accompany it with use cases/ examples to highlight Rust's productivity and  performance especially suitable for Python guys.

Please give your valuable feedback and and suggestions on how to improve my technical writing. All kinds of criticism are welcome.

I could use some help revising and editing my drafts in future if any one of you are interested.",0,1,False,self,,,,,
583,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,20,azeuxl,towardsdatascience.com,Why every data scientist shall read The Book Of Why,https://www.reddit.com/r/MachineLearning/comments/azeuxl/why_every_data_scientist_shall_read_the_book_of/,transformer_ML,1552217775,,0,1,False,default,,,,,
584,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,20,azezyf,self.MachineLearning,Combating discrimination towards engineers in AI,https://www.reddit.com/r/MachineLearning/comments/azezyf/combating_discrimination_towards_engineers_in_ai/,relletreknit,1552219094,[removed],0,1,False,self,,,,,
585,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,21,azf27k,self.MachineLearning,Eigenfaces - how does it work exactly?,https://www.reddit.com/r/MachineLearning/comments/azf27k/eigenfaces_how_does_it_work_exactly/,SuperCComplex,1552219615,[removed],0,1,False,self,,,,,
586,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,21,azf7ic,kaggle.com,House Price Prediction: An End-to-End Machine Learning Project with Code and Explanations [P],https://www.reddit.com/r/MachineLearning/comments/azf7ic/house_price_prediction_an_endtoend_machine/,ammar-,1552221047,,0,1,False,default,,,,,
587,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,21,azf8b4,self.MachineLearning,[D] Combating discrimination towards engineers in AI,https://www.reddit.com/r/MachineLearning/comments/azf8b4/d_combating_discrimination_towards_engineers_in_ai/,relletreknit,1552221250,"I recently joined one of the research organization as ML engineer which is part of a tech giant that you most certainly know. However, I am absolutely stunned at how engineers are being consistently discriminated at such research organizations irrespective of their contributions. Unfortunately my recent learnings suggests that this might be a systemic problem in our field and therefore motivating me for this post.

First let me say that this post has be anonymous but to give some background, I have been working with ML since those days when SVMs were the coolest things (part of me wants to say that they still are). I dont have PhD. Im completely self-taught when it comes to ML, constantly improving and learning by solving real world problems as all engineers likes to do. Since past 7 years or so Im focusing almost exclusively on deep learning and have shipped models for real money making products (yes, they do exist!). For my love of ML research, I finally decided to join formal research organization and now Im finding out that engineers are very clearly second class citizens in such research organizations.

Its hard to get sense of how discrimination feels to someone without talking about specifics. So I will give few examples here. Please understand I can only show you the tip of the iceberg but people in my position precisely know what I am eluding to.

One of my first memory as I arrived was learning that there was some offsite going on in my group. It turned out that only researchers were invited while engineers were en-mass excluded even while the topic of offsite was very important for everyone. It was absolutely shocking that exclusion was purely based on job titles with no consideration given to seniority or past contributions. This apparently wasnt the first time this had happened as one of the colleagues told me. Several such important meetings as well as events often happened with only invitees being researchers while exactly zero engineers working on same areas deemed important enough to be included. And it wasnt just meetings either. Few months back there have been large scale hiring of interns and FTEs going on. Apparently only researchers were invited to do interview loops. At least one position I looked were advertised to require only Masters degree (which I happened have too!) but even for those positions no engineers were invited to be part of any of the interview loops. These candidates, if selected, will be our colleagues and supposed to work with us but none of us engineers were deemed good enough to interview any of them. At the same time junior researchers with zero years of experience would almost always be invited to be part of these interview loops. I would think that interview process should benefit from the diversity in backgrounds. The goal for inclusion in interviewing process is for the entire team to evaluate compatibility with future colleagues as well as coverage for diverse aspects of skill sets. I was also told that this is not the case any other divisions outside of this research organization in my company.

There are many such things that I can go on about how engineers are very carefully excluded and discriminated regardless of their experience, passion, expertise or contributions. One such example is public podcasts that my research organization has started to throw some lime light on projects and personals. However only researchers are allowed to be featured, interviewed or be part of these public podcasts. I recently became aware of one case where the work that was boasted about at length by a researcher in one podcast episode was actually entirely done by engineers. I was pretty disgusted at how any mention of this fact was very carefully avoided even when interviewer asked related questions. I talked to one of the engineers involved and the person was visibly shaken just at the mention of that podcast.

As final example, my research organization does a tons of internal panels, events, workshops, seminars and so on. Each of these usually have committees, chairs, reviewers, organizers and so on. In my admittedly short time here I have never seen any engineers getting ever invited to participate in any of these committees in any capacity whatsoever. It appears that engineers are simply prevented by design to be part of any decision making bodies that exists at my research organization at any level. In several cases that I am now aware of, engineers have been equal contributors, in some much more than equal. Yet this exclusion from being in any position of power, gaining visibility or rewards seems to be carefully designed in to the system. The most funny thing I ever heard here was when some researchers set up a seminar on best engineering practices. The catch? They had zero engineers as part of the committee and speakers! It turns out that engineers are apparently not intelligent enough to even talk about engineering.

Its depressing enough that I am planning to leave and have been talking to few engineers in other research organizations. Unfortunately there doesnt seem to be very positive feedback either from other engineers I talked to so far. I tend to think this is not just problem at where I work now, rather its something that is fundamentally and deeply ingrained in research organizations. This is especially stunning given that these research organizations derive their freedom, stability and all other comforts on the back of engineers who actually bring bacon on the table. I love research but its super hard to live with such naked and blatant discrimination every day. In my opinion this is no different than how whites treated blacks by considering them much lesser beings or how men treated women by looking at them as one notch below all the while massively profiting from their work.

How do we change this? More importantly can this be changed?",20,4,False,self,,,,,
588,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,22,azfgw3,self.MachineLearning,[D] Should we all switch to Jupyter lab?,https://www.reddit.com/r/MachineLearning/comments/azfgw3/d_should_we_all_switch_to_jupyter_lab/,johannesbeil,1552223237,"Jupyter lab is almost out of beta, and I think it's great: pretty good editor + notebook + proper extension manager ... all in the browser, so there is almost no more need for this dual system with notebooks for quick exploration and a ""real"" editor for the ""real"" code. 

What do you think? Is anyone going to stick with jupyter notebooks? Who will be persuaded to switch from their vscode/pycharm etc. setup to the jupyter lab editor? ",52,9,False,self,,,,,
589,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,22,azfi6o,self.MachineLearning,How to check if someone have done already done the research on my idea?,https://www.reddit.com/r/MachineLearning/comments/azfi6o/how_to_check_if_someone_have_done_already_done/,IIAKAD,1552223514,[removed],0,1,False,self,,,,,
590,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,22,azfuzc,self.MachineLearning,GAN using Resnet fail,https://www.reddit.com/r/MachineLearning/comments/azfuzc/gan_using_resnet_fail/,Andy_Joyce,1552226155,[removed],0,1,False,https://b.thumbs.redditmedia.com/4onMLpfdZjMbE6QavDo1wplXOyzO41JFCjJ3N4t5oUU.jpg,,,,,
591,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,23,azg815,self.MachineLearning,obfuscated javascript files,https://www.reddit.com/r/MachineLearning/comments/azg815/obfuscated_javascript_files/,karindahan,1552228595,[removed],0,1,False,self,,,,,
592,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,23,azgcbx,self.MachineLearning,Why should you avoid to use for loops explicitly when coding neural networks?,https://www.reddit.com/r/MachineLearning/comments/azgcbx/why_should_you_avoid_to_use_for_loops_explicitly/,Yahiabouda,1552229360,[removed],0,1,False,self,,,,,
593,MachineLearning,t5_2r3gv,2019-3-10,2019,3,10,23,azgf4f,self.MachineLearning,ICML reviewer competence,https://www.reddit.com/r/MachineLearning/comments/azgf4f/icml_reviewer_competence/,totallynotAGI,1552229863,[removed],0,1,False,self,,,,,
594,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,0,azgksy,self.MachineLearning,How to auto name cluster heads? What has worked for you the best?,https://www.reddit.com/r/MachineLearning/comments/azgksy/how_to_auto_name_cluster_heads_what_has_worked/,abdush,1552230839,[removed],0,1,False,self,,,,,
595,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,0,azgs3w,self.MachineLearning,[Discussion] ICML reviewer competence,https://www.reddit.com/r/MachineLearning/comments/azgs3w/discussion_icml_reviewer_competence/,totallynotAGI,1552232072,"Is there anything we can do when the reviewer who has given a Reject with Medium confidence obviously doesn't show **nowhere near enough the levels of competence** to judge the validity of the paper:

* He doesn't seem to have understood the contributions of the paper. His summary completely incorrectly states what the paper is about (it's a theoretical math paper) and his two sentence restatement of the paper is factually wrong on many levels.
* His **only** question about the paper doesn't even parse into a meaningful question
* His claim is that paper is vague, but he doesn't provide any concrete examples of vagueness. Our paper uses advanced, abstract, yet very precise mathematical language to talk about the subject matter.
* He claims the paper is repetitive, but doesn't provide any concrete examples of repetition: we strongly suspect he confuses much of the mathematical terminology and classifies them as restatements of previous claims when they clearly are different statements
* **He himself claims he could barely produce the summary of the paper** and he claims the summary might be inaccurate, but he does state he has **Medium confidence** in his review.
* Total word count (their answers to questions 1. and 3.) is \~220 words. We don't claim that word count is an absolute measure towards review quality, but merely provide it here to illustrate the brevity of the review.

We have received other, very high quality review from another reviewer who provided specific questions, helpful comments about the paper and showed a thorough understanding. As the problematic review here is one of the *three* reviews we got, we suspect this could have a very negative impact on the final decision made.

Is there any basis here for sending a Confidential Comment to Area Chairs complaining about this?  
We're not sure is there any course of action we could be taking here.

P.S. this was posted also in the [ICML reviews are out](https://www.reddit.com/r/MachineLearning/comments/az48uc/d_icml_reviews_are_out/) thread, but it looks like it could just get buried",23,12,False,self,,,,,
596,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,1,azhdr1,self.MachineLearning,What are some of the most common questions which people have regarding ML?,https://www.reddit.com/r/MachineLearning/comments/azhdr1/what_are_some_of_the_most_common_questions_which/,bitwise_ranger,1552235580,[removed],0,1,False,self,,,,,
597,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,2,azhzod,self.MachineLearning,Why every data scientist shall read The Book Of Why,https://www.reddit.com/r/MachineLearning/comments/azhzod/why_every_data_scientist_shall_read_the_book_of/,transformer_ML,1552238982,"[Why every data scientist shall read The Book Of Why](https://towardsdatascience.com/why-every-data-scientist-shall-read-the-book-of-why-by-judea-pearl-e2dad84b3f9d)


Beyond architecture or algorithm: how can we make ML elevate from association to causality.

",0,1,False,self,,,,,
598,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,2,azi29t,self.MachineLearning,grep wireless,https://www.reddit.com/r/MachineLearning/comments/azi29t/grep_wireless/,311x,1552239378,[removed],0,1,False,self,,,,,
599,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,2,azi4vl,self.MachineLearning,[D] Why every data scientist shall read The Book Of Why,https://www.reddit.com/r/MachineLearning/comments/azi4vl/d_why_every_data_scientist_shall_read_the_book_of/,transformer_ML,1552239781,"[Link](https://towardsdatascience.com/why-every-data-scientist-shall-read-the-book-of-why-by-judea-pearl-e2dad84b3f9d)

Beyond architecture or algorithm, how can ML elevate from association to causality.
",32,214,False,self,,,,,
600,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,3,azibxu,self.MachineLearning,Zero vs one padding of an spectrogram,https://www.reddit.com/r/MachineLearning/comments/azibxu/zero_vs_one_padding_of_an_spectrogram/,gakshaygupta,1552240862,"i got a reviewer for my paper.And the reviewer said that instead of using zero padding one padding might work [better.](https://better.Is) Can anybody explain why the former padding might be better?

The paper was about recognizing accent of a person based on the speech sample in english.I have used a Mel-spectrogram as an input for CNN .",0,1,False,self,,,,,
601,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,3,azigo9,artificialturk.com,[D] When to use Keras? When to switch for others?,https://www.reddit.com/r/MachineLearning/comments/azigo9/d_when_to_use_keras_when_to_switch_for_others/,mburaksayici,1552241554,,0,1,False,default,,,,,
602,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,3,azizid,self.MachineLearning,What are the Best sources to make ML concepts clear,https://www.reddit.com/r/MachineLearning/comments/azizid/what_are_the_best_sources_to_make_ml_concepts/,mental_ape101,1552244349,[removed],0,1,False,self,,,,,
603,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,4,azjhfi,self.MachineLearning,Guide me to start with ML.,https://www.reddit.com/r/MachineLearning/comments/azjhfi/guide_me_to_start_with_ml/,_10shubham,1552246999,[removed],0,1,False,self,,,,,
604,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,5,azjoht,self.MachineLearning,[D] Machine Learning - WAYR (What Are You Reading) - Week 58,https://www.reddit.com/r/MachineLearning/comments/azjoht/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,1552248006,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|
|----|-----|-----|-----|-----|-----|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)||

Most upvoted papers two weeks ago:

/u/Moseyic: [Sinkhorn Autoencoders](https://arxiv.org/abs/1810.01118)

/u/mrpogiface: [Wasserstein GAN](https://arxiv.org/pdf/1701.07875.pdf)

Besides that, there are no rules, have fun.",5,12,False,self,,,,,
605,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,6,azkv90,stadivm.com,"Will AI and ML spawn a sub-industry of ""small AI business services"" akin to something like web dev or SEO 10 years ago?",https://www.reddit.com/r/MachineLearning/comments/azkv90/will_ai_and_ml_spawn_a_subindustry_of_small_ai/,bitwise_ranger,1552254254,,0,1,False,default,,,,,
606,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,7,azl30f,self.MachineLearning,What's wrong with PyTorch Documentation?,https://www.reddit.com/r/MachineLearning/comments/azl30f/whats_wrong_with_pytorch_documentation/,Budawest,1552255386,"As noted in the title, what's going on with PyTorch Documentation? Why is it so bad? I love the library and the interface is extremely usable compared to TensorFlow; however, why are many common functions not there when you search in the search bar (e.g. torch.lgamma doesn't show when you search for lgamma, but mvlgamma does)? Why do the search results appear in the most confusing order? Why does opening more than 3 tabs of the PyTorch documentation chug up all the computer's resources? On top of that, opening an entry for a particular function takes quite a while to load, and after that done, your screen will jump up and down a few times as the entries for the functions above and below the function of interest on the website will load. Is this just me, or do other people have these issues as well?

Honestly, the experience of using the PyTorch docs drives me crazy every time, and is the main standout annoyance when using an otherwise great package. ",0,1,False,self,,,,,
607,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,8,azlrc7,self.MachineLearning,Help with alpha in neural networks!,https://www.reddit.com/r/MachineLearning/comments/azlrc7/help_with_alpha_in_neural_networks/,ActiveSubstance,1552259134,"Hi people! I'm new in this field, I've been doing some experiments with Multilayer Perceptron from sklearn - python.  I  have already read a lot about  activation  functions, anyways I couldn't find anything really useful about alpha: `MLPClassifier(activation='tanh', alpha=0.001, hidden_layer_sizes=(25, 1),max_iter=2000)`

How does it work?

What is the theory behind?

I love understanding what is really happening behind when I work with this kind of thing, I feel very curious about it.

I'd be  really  grateful if someone can explain it to me!

Thank you guys!",0,1,False,self,,,,,
608,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,9,azmf8y,self.MachineLearning,[P] The Wandering Dreamer: An Synthetic Feedback Loop,https://www.reddit.com/r/MachineLearning/comments/azmf8y/p_the_wandering_dreamer_an_synthetic_feedback_loop/,brannondorsey,1552263058,"&amp;#x200B;

![video](q3ch3ey4tdl21 ""The Wandering Dreamer video excerpt"")

This experiment uses four machine learning models to create a  feedback loop between synthesized images and text. All of the images you  see here are fabricated, as is the text that describes each image.

&amp;#x200B;

1. The first row of images are produced from a class label using [BigGAN](https://arxiv.org/abs/1809.11096).
2. The text below is an autogenerated caption of the BigGAN image using [Im2Text](https://github.com/tensorflow/models/tree/master/research/im2txt).
3. The next set of images are synthesized by an [Attentional GAN](https://arxiv.org/abs/1711.10485) using the auto-generated captions.
4. The text at the bottom classifies the image above it using [MobileNet](https://arxiv.org/abs/1704.04861). This class label is then sent back to BigGAN as input to create an infinite loop.

Made by [Brannon Dorsey](https://brannon.online/) using [Runway](https://runwayapp.ai/).

Tweet: [https://twitter.com/brannondorsey/status/1104829662264463361](https://twitter.com/brannondorsey/status/1104829662264463361)

Full YouTube Video: [https://www.youtube.com/watch?v=5Zus6ZOG-lw](https://www.youtube.com/watch?v=5Zus6ZOG-lw)

Source code: [https://github.com/brannondorsey/the-wandering-dreamer](https://github.com/brannondorsey/the-wandering-dreamer)",6,32,False,self,,,,,
609,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,9,azmimk,self.MachineLearning,"[R] Where can i publish a paper on ""Diabetes Prediction via Machine Learning Algorithms""",https://www.reddit.com/r/MachineLearning/comments/azmimk/r_where_can_i_publish_a_paper_on_diabetes/,croxcrocodile,1552263604,I am a graduate IT student and im writing a thesis on this topic. I'd like to know which kinds of conferences accept such papers. It is a classification/supervised learning problem.,14,6,False,self,,,,,
610,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,9,azmo98,self.MachineLearning,[D] In-Depth blogs discussing ML and the math?,https://www.reddit.com/r/MachineLearning/comments/azmo98/d_indepth_blogs_discussing_ml_and_the_math/,iamquah,1552264559,"Recently stumbled onto [Wiseodd](https://wiseodd.github.io/) while looking up Natural Gradient Descent and trying to understand the math behind it.

I misunderstood some equation there and ended up finding [The Spectator](http://blog.shakirm.com/) where I found other useful things. 

This got me thinking about what other (high quality) technical blogs exist where they pick a topic and walk you through the math. Googling led me towards programming tutorials rather than mathematical discussions. 

Sure, I could pick up a book, but I feel like a blog has a less ""formal"" feel and I quite enjoy that

P.s does anyone know if there's a way to subscribe to a site that doesn't have an RSS feed or doesn't send out email updates? ",37,143,False,self,,,,,
611,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,11,aznt4i,i.redd.it,"As the demand of blow molding products is higher and higher, plastic water tank blow molding machine supplier are also more and more. Now the Internet is so developed, the network is full of all kinds of advertising.More:http://www.yankangmachine.com/water-tank-blow-molding-machine-supplier/",https://www.reddit.com/r/MachineLearning/comments/aznt4i/as_the_demand_of_blow_molding_products_is_higher/,miyawang12138,1552271464,,1,1,False,default,,,,,
612,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,12,azo6yo,arxiv.org,Learning like humans with Deep Symbolic Networks,https://www.reddit.com/r/MachineLearning/comments/azo6yo/learning_like_humans_with_deep_symbolic_networks/,Digimon_Utopia_99,1552273812,,12,0,False,default,,,,,
613,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,12,azoiq5,self.MachineLearning,Are there any daily life applications for Visual Question Answer?,https://www.reddit.com/r/MachineLearning/comments/azoiq5/are_there_any_daily_life_applications_for_visual/,k-robot,1552275831,[removed],0,1,False,self,,,,,
614,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,13,azowzh,self.MachineLearning,Difference between big data predictive analysis &amp; machine learning,https://www.reddit.com/r/MachineLearning/comments/azowzh/difference_between_big_data_predictive_analysis/,rajeshwargujja,1552278262,[removed],0,1,False,self,,,,,
615,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,15,azq6n9,self.MachineLearning,Hospitality With Artificial Intelligence!,https://www.reddit.com/r/MachineLearning/comments/azq6n9/hospitality_with_artificial_intelligence/,getengati,1552286936,[removed],0,1,False,https://a.thumbs.redditmedia.com/tbP6CitRkFZIw9tsrCGzbxDnahmvBd9Q4RruEXByuf0.jpg,,,,,
616,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,16,azqdx9,self.MachineLearning,[Meme] When your validation set isn't representative of real-world data,https://www.reddit.com/r/MachineLearning/comments/azqdx9/meme_when_your_validation_set_isnt_representative/,init__27,1552288498,[removed],0,1,False,https://b.thumbs.redditmedia.com/3N1ee9edhAWZOoh2H0sX1fDRID6yKUG8pv4MMGwvnKI.jpg,,,,,
617,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,16,azqlg3,wahalengineersmachine.blogspot.com,Wahal Engineers - The Best Destination for Bitumen Emulsion Plant,https://www.reddit.com/r/MachineLearning/comments/azqlg3/wahal_engineers_the_best_destination_for_bitumen/,wahalengineers,1552290167,,0,1,False,https://a.thumbs.redditmedia.com/az7CQay_ohf5sAJZYaqMhpEGpXDcjzMJuaJJgOZnHr8.jpg,,,,,
618,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,17,azqsiz,slideserve.com,Machine Learning Training in Noida,https://www.reddit.com/r/MachineLearning/comments/azqsiz/machine_learning_training_in_noida/,Divya123divya,1552291748,,0,1,False,default,,,,,
619,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,17,azr0m3,self.MachineLearning,An universal approximation theorem for deep feedforward networks,https://www.reddit.com/r/MachineLearning/comments/azr0m3/an_universal_approximation_theorem_for_deep/,VitDer,1552293634,[removed],0,1,False,self,,,,,
620,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,18,azr66i,self.MachineLearning,The influence of the dataset for Object Detection,https://www.reddit.com/r/MachineLearning/comments/azr66i/the_influence_of_the_dataset_for_object_detection/,onTheEdge936,1552294929,[removed],0,1,False,self,,,,,
621,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,18,azrad5,boingboing.net,"Towards a general theory of ""adversarial examples,"" the bizarre, hallucinatory motes in machine learning's all-seeing eye",https://www.reddit.com/r/MachineLearning/comments/azrad5/towards_a_general_theory_of_adversarial_examples/,neverwild,1552295870,,0,1,False,https://b.thumbs.redditmedia.com/hfP8HTDrBBCM4dgxP1lxvoFdGl9AZdgX4cAemwrlCVk.jpg,,,,,
622,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,18,azrbad,self.MachineLearning,Ape-X and IMPALA for Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/azrbad/apex_and_impala_for_reinforcement_learning/,gowthamn,1552296093,[removed],0,1,False,self,,,,,
623,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,18,azrdtu,self.MachineLearning,Why Machine learning and Why It Matters?,https://www.reddit.com/r/MachineLearning/comments/azrdtu/why_machine_learning_and_why_it_matters/,beerstranger,1552296666,[removed],0,1,False,self,,,,,
624,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,18,azrgo9,kaushalhub.com,Why Machine learning and Why It Matters,https://www.reddit.com/r/MachineLearning/comments/azrgo9/why_machine_learning_and_why_it_matters/,beerstranger,1552297282,,0,1,False,default,,,,,
625,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,19,azrmr8,self.MachineLearning,Need a faster bounding box detector,https://www.reddit.com/r/MachineLearning/comments/azrmr8/need_a_faster_bounding_box_detector/,abinjoabraham,1552298600,"Guys I am working on a project which can detect object instances in an image. I have found an algorithm which suits to my needs and as a prerequisite to this algorithm, I need to draw a bounding box over the object instance. I googled and found several bbox detectors like Mask.RCNN, Fast RCNN, Faster RCNN and I am confused which one will give me better and faster  bbox over the object instances in the image. Do you people have some suggestions on this? In the end I was planning to compare my accuracy considering Mask.RCNN as my baseline data. ",0,1,False,self,,,,,
626,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,19,azroo7,self.MachineLearning,Suggestion to Improve Math and Stat foundation for ML.,https://www.reddit.com/r/MachineLearning/comments/azroo7/suggestion_to_improve_math_and_stat_foundation/,SalmanHaydar,1552298992,"Hello, I have been working in Data Science for almost two years. I can code and identify the problems without much difficulty. But I often feel that my knowledge on Math and Statistics (probability) is lacking. So, I want to improve my knowledge on these two topic. 

Now, what should I do to improve my knowledge on those topic and please suggest some resources also. ",0,1,False,self,,,,,
627,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,19,azrs1y,twitter.com,SPECIAL Offer The Complete SQL Bootcamp DISCOUNT 94% off,https://www.reddit.com/r/MachineLearning/comments/azrs1y/special_offer_the_complete_sql_bootcamp_discount/,CaraTheisen,1552299707,,0,1,False,default,,,,,
628,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,21,azsn0s,self.MachineLearning,How do you started with AI and ML and why?,https://www.reddit.com/r/MachineLearning/comments/azsn0s/how_do_you_started_with_ai_and_ml_and_why/,PatrickRNG,1552305748,[removed],0,1,False,self,,,,,
629,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,21,azsnr8,self.MachineLearning,"Bicycle Lights Market measure, Trend, offer, investigation and estimate 2022",https://www.reddit.com/r/MachineLearning/comments/azsnr8/bicycle_lights_market_measure_trend_offer/,apple_x9,1552305875,[removed],1,1,False,self,,,,,
630,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,21,azsr2a,self.MachineLearning,Coin counters Market to Perceive Substantial Growth During 2022,https://www.reddit.com/r/MachineLearning/comments/azsr2a/coin_counters_market_to_perceive_substantial/,apple_x9,1552306473,[removed],1,1,False,self,,,,,
631,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,21,azsupv,self.MachineLearning,Unmanned Ground Vehicle Market to Perceive Substantial Growth amid 2022,https://www.reddit.com/r/MachineLearning/comments/azsupv/unmanned_ground_vehicle_market_to_perceive/,apple_x9,1552307122,[removed],1,1,False,self,,,,,
632,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,21,azt3k3,self.MachineLearning,Making research papers easier for people to understand.,https://www.reddit.com/r/MachineLearning/comments/azt3k3/making_research_papers_easier_for_people_to/,ilovefunctions,1552308711,[removed],0,2,False,self,,,,,
633,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,21,azt5sy,self.MachineLearning,Best way to build a keyword extractor?,https://www.reddit.com/r/MachineLearning/comments/azt5sy/best_way_to_build_a_keyword_extractor/,baloglub,1552309118,[removed],0,1,False,self,,,,,
634,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,22,aztfs9,self.MachineLearning,[P] Anyone interested in working on classification of photovoltaic systems with airplane images?,https://www.reddit.com/r/MachineLearning/comments/aztfs9/p_anyone_interested_in_working_on_classification/,Electricvid,1552310827,"Already have a model running and archieved 93% acc on my test set, but still a lot has to be done! 

&amp;#x200B;

&amp;#x200B;",29,18,False,self,,,,,
635,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,23,azu4cz,self.MachineLearning,[D] Which AI hot topics caught your attention in February?,https://www.reddit.com/r/MachineLearning/comments/azu4cz/d_which_ai_hot_topics_caught_your_attention_in/,arnauda9,1552314747,"I wrote an article to help anyone catch up with the latest Machine Learning news.

[https://blog.sicara.com/02-2019-best-ai-new-articles-this-month-cca9faaf867e](https://blog.sicara.com/02-2019-best-ai-new-articles-this-month-cca9faaf867e)

I would love to hear from you if some AI articles caught your attention in February!",3,18,False,self,,,,,
636,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,23,azu6sf,twitter.com,SPECIAL Offer Data Science and Machine Learning Bootcamp with R DISCOUNT 94% off,https://www.reddit.com/r/MachineLearning/comments/azu6sf/special_offer_data_science_and_machine_learning/,AnnettPennington,1552315115,,0,1,False,default,,,,,
637,MachineLearning,t5_2r3gv,2019-3-11,2019,3,11,23,azu78v,youtube.com,Machine Learning - An Overview,https://www.reddit.com/r/MachineLearning/comments/azu78v/machine_learning_an_overview/,stemiac,1552315186,,0,1,False,default,,,,,
638,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,0,azuglo,self.MachineLearning,Anybody using Tensorflow Serving in production?,https://www.reddit.com/r/MachineLearning/comments/azuglo/anybody_using_tensorflow_serving_in_production/,i_often_say_that,1552316601,"What has been your experience with it?
Did you use any parts if TFX?

Can't decide whether to go with TFX or own build.",0,1,False,self,,,,,
639,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,0,azuo4j,medium.com,[N] UC Cuts Elsevier Subscriptions as ML Community Pushes Open Access,https://www.reddit.com/r/MachineLearning/comments/azuo4j/n_uc_cuts_elsevier_subscriptions_as_ml_community/,gwen0927,1552317744,,0,2,False,https://b.thumbs.redditmedia.com/FN6ng4uw50nd3GG3xKYBmbSLpK0W332ePnlob014aAs.jpg,,,,,
640,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,0,azuolk,self.MachineLearning,[D] What's the best Machine Learning for video analysis?,https://www.reddit.com/r/MachineLearning/comments/azuolk/d_whats_the_best_machine_learning_for_video/,nyamuk91,1552317806,"Hi there. I'm very new to Machine Learning so this might sound like a stupid question (sorry if it is), but what is the best ML (in term of accuracy and time) to perform analysis on a video file. Currently, I'm using Tensorflow and using this python sample from the official Tensorflow git as a guideline:  
[https://github.com/tensorflow/models/blob/master/tutorials/image/imagenet/classify_image.py](https://github.com/tensorflow/models/blob/master/tutorials/image/imagenet/classify_image.py)  
  
I'm using the pre-trained model based on ImageNet. It works but I think it's a little bit slow. When running on CPU, it took 8 seconds to analyze 1 single frame of 360p video (and few minutes to analyze 1 minute of video). I then run Tensorflow on GPU and it reduces the processing time to 2-3 seconds. It's a huge improvement but it still felt a little bit slow.   
  
So, is there any other Machine Learning or training model (not sure if I'm using the term right) that is faster and suitable to analyze video?",6,3,False,self,,,,,
641,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,0,azuwwo,self.MachineLearning,[R] Does Hardware Affects performance of MachineLearning?,https://www.reddit.com/r/MachineLearning/comments/azuwwo/r_does_hardware_affects_performance_of/,deshou04,1552319052,Currently as a student we need to create a system that would use machine learning. upon reading IEEE papers about works on machine learning not a single paper contains hardware used about their work. i just want to ask does hardware affects performance of machine learning?,6,0,False,self,,,,,
642,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,0,azv2m6,arxiv.org,[R] Diffusion Scattering Transforms on Graphs (ICLR 2019),https://www.reddit.com/r/MachineLearning/comments/azv2m6/r_diffusion_scattering_transforms_on_graphs_iclr/,for_all_eps,1552319867,,1,3,False,default,,,,,
643,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,1,azv61c,openai.com,"""Weve created OpenAI LP, a new capped-profit company that allows us to rapidly increase our investments in compute and talent while including checks and balances to actualize our mission.""",https://www.reddit.com/r/MachineLearning/comments/azv61c/weve_created_openai_lp_a_new_cappedprofit_company/,atlatic,1552320336,,0,1,False,default,,,,,
644,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,1,azv6be,self.MachineLearning,[D] Sampling Matters in Deep Embedding Learning,https://www.reddit.com/r/MachineLearning/comments/azv6be/d_sampling_matters_in_deep_embedding_learning/,danioto,1552320373,"Hi all!

&amp;#x200B;

I am trying to reproduce/recreate some of the figures in the paper Sampling Matters in Deep Embedding Learning. I've already got figure 2 with a density of data points in the D-dimensional unit sphere, but I've got a problem with reproducing Figure 3a (at the moment). Can someone point me in the right direction? Any code snippets would be more than helpful :)

&amp;#x200B;

Best,

Daniel",0,4,False,self,,,,,
645,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,1,azv86d,self.MachineLearning,[P] CUDA 10 install script for Ubuntu 18,https://www.reddit.com/r/MachineLearning/comments/azv86d/p_cuda_10_install_script_for_ubuntu_18/,eukaryote31,1552320633,"After sinking many hours into trying to get Nvidia drivers and CUDA working on my workstation *while still having X use the integrated graphics*, I finally managed to figure it out and distilled my hours of trial and error into a short bash script. Hopefully this can save you some time when setting up!

&amp;#x200B;

[https://github.com/eukaryote31/ubuntu-setup/blob/master/setup-cuda](https://github.com/eukaryote31/ubuntu-setup/blob/master/setup-cuda)",17,28,False,self,,,,,
646,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,1,azv8dx,self.MachineLearning,How to make a simple Machine Learning Website from scratch,https://www.reddit.com/r/MachineLearning/comments/azv8dx/how_to_make_a_simple_machine_learning_website/,Make-U-Believe,1552320665,[removed],0,1,False,self,,,,,
647,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,1,azvap2,openai.com,[N] OpenAI LP,https://www.reddit.com/r/MachineLearning/comments/azvap2/n_openai_lp/,circuithunter,1552321007,,0,1,False,https://a.thumbs.redditmedia.com/H-5iKlRjiKgOi60gjD5q-ViTAqVXLUZteAWMZhNyWT8.jpg,,,,,
648,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,1,azvbmn,self.MachineLearning,[N] OpenAI LP,https://www.reddit.com/r/MachineLearning/comments/azvbmn/n_openai_lp/,SkiddyX,1552321140,"""Weve created OpenAI LP, a new capped-profit company that allows us to rapidly increase our investments in compute and talent while including checks and balances to actualize our mission.""

Sneaky.

https://openai.com/blog/openai-lp/",213,306,False,self,,,,,
649,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,1,azvcgy,self.MachineLearning,[N] OpenAI moves most of its staff to a for-profit LLC,https://www.reddit.com/r/MachineLearning/comments/azvcgy/n_openai_moves_most_of_its_staff_to_a_forprofit/,shannoncoin,1552321260,"https://openai.com/blog/openai-lp/
&gt; Weve created OpenAI LP, a new capped-profit company that allows us to rapidly increase our investments in compute and talent while including checks and balances to actualize our mission.",19,89,False,self,,,,,
650,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,1,azvg7a,self.MachineLearning,[P] Python open source library to perform entity embeddings on categorical variables using Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/azvg7a/p_python_open_source_library_to_perform_entity/,rodrigobresan,1552321813,"Hey guys, I've been working as an undergrad researcher for the past year on the prediction of childbirth mortality. At this project I've developed a tool to perform entity embeddings on categorical variables using CNN with Keras. I tried pretty much to make it easy to use and flexible to most of the existent scenarios (regression, binary and multiclass classification), but if you find any other need or issue to be fixed, feel free to ask! :-)

I tried to add some cool stuff on the project, such as **unit tests**, **code coverage** with Codacy, **continuous integration** with Travis CI and **auto deployment** to PyPi and **auto-generated documentation** with Sphinx and ReadTheDocs, so if any of you is interested in how to setup your project to have these features, feel free to use it as a base project.

I'm also looking forward to any reviews about the source code, so any tip to improve the readability or even performance, its really welcome and well appreciated.

Github: https://github.com/bresan/entity_embeddings_categorical

PyPi: https://pypi.org/project/entity-embeddings-categorical/

Code coverage (nowadays reaching 97%): https://coveralls.io/github/bresan/entity_embeddings_categorical?branch=master

Thanks! ",4,4,False,self,,,,,
651,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,1,azvpft,tech.trivago.com,ACM RecSys Challenge 2019: suggest suitable accommodations for travelers,https://www.reddit.com/r/MachineLearning/comments/azvpft/acm_recsys_challenge_2019_suggest_suitable/,mre__,1552323152,,0,3,False,https://a.thumbs.redditmedia.com/GxGYghG-o0IS0u2KUoAXkmo_jK-uHrOokjAl6Il5oS8.jpg,,,,,
652,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,1,azvq14,self.MachineLearning,DeepCamera: Better open source DeepLens working on $35 device,https://www.reddit.com/r/MachineLearning/comments/azvq14/deepcamera_better_open_source_deeplens_working_on/,solderzzc,1552323238,[removed],0,1,False,self,,,,,
653,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,2,azvu3x,self.MachineLearning,deploying machine learning model,https://www.reddit.com/r/MachineLearning/comments/azvu3x/deploying_machine_learning_model/,helloavani,1552323791,hi  i'm currently  in final semester of my graduation in computer science engineering and engage with my final year project which is based on diabetes prediction implemented with anaconda package on jupyter notebook in python language now i'm stuck with how to give user interface application?!! my currently implemented model's accuracy is 80% is it okay? ,7,1,False,self,,,,,
654,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,2,azvxqx,self.MachineLearning,Extract information from school transcripts using OCR,https://www.reddit.com/r/MachineLearning/comments/azvxqx/extract_information_from_school_transcripts_using/,R-PRADY,1552324321,[removed],0,1,False,self,,,,,
655,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,2,azw06o,self.MachineLearning,What is the time complexity of object localization?,https://www.reddit.com/r/MachineLearning/comments/azw06o/what_is_the_time_complexity_of_object_localization/,skyline678,1552324656,[removed],0,1,False,self,,,,,
656,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,2,azw0w7,self.MachineLearning,Give me some direction with the given dataset,https://www.reddit.com/r/MachineLearning/comments/azw0w7/give_me_some_direction_with_the_given_dataset/,ramees_sahlu,1552324757,[removed],2,1,False,self,,,,,
657,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,2,azw17h,self.MachineLearning,[D] The transition layer of Transformer has very sparse activation,https://www.reddit.com/r/MachineLearning/comments/azw17h/d_the_transition_layer_of_transformer_has_very/,HigherTopoi,1552324804,"I found that, unlike other parts of the architecture, the activations right after relu of the transition layer of Transformer (after 10k iterations) are extremely sparse in the sense that setting the neurons with smallest 90% or so activations to zero does not affect the performance (or even training) at all. I imagine a similar conclusion holds for self-attention part. By its design, the transition layer is supposed to work as attention over hidden dimension in contrast to the self-attention layer that is attention over timestep. While the attention over timestep is easily interpretable, attention over hidden dimension is currently not for Transformer on text unlike 2D CNN (activation atlas). However, given that the transition layer has very sparse activation, it is possibly easier to analyze which neuron is responsible for what kind of feature. Anyway, I hope this observation will be of use in something. ",5,6,False,self,,,,,
658,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,2,azw6xj,ai.stanford.edu,[R] Weak Supervision: A New Programming Paradigm for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/azw6xj/r_weak_supervision_a_new_programming_paradigm_for/,regalalgorithm,1552325621,,8,38,False,default,,,,,
659,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,2,azw8td,self.MachineLearning,testpost,https://www.reddit.com/r/MachineLearning/comments/azw8td/testpost/,noone1101,1552325893,[removed],0,1,False,self,,,,,
660,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,3,azwmte,self.MachineLearning,"Help me understand the ""Probability Distribution"" of an (adversarial) Auto-encoder",https://www.reddit.com/r/MachineLearning/comments/azwmte/help_me_understand_the_probability_distribution/,FreddyShrimp,1552327824,[removed],0,1,False,self,,,,,
661,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,3,azwrcd,self.MachineLearning,[D] What is the performance like in TensorFlow.js vs Tensorflow? And how exactly does TensorFlow.js interface with the GPU?,https://www.reddit.com/r/MachineLearning/comments/azwrcd/d_what_is_the_performance_like_in_tensorflowjs_vs/,aatomator,1552328436,"I am going to start off by saying I am new to machine learning.

Anyway, in order to learn more about this I was looking at training a Neural Network using TensorFlow using Electron.js as a front end. Then I remembered that I could use TensorFlow.js with Electron, but at that point I figured I could run it in browser. I am not doing anything complicated to start, I just want to train it on the basic MNIST. 

Would I lose out on any performance running it the browser, or should I try it with Electron.js? Clearly I am very confused and any help would be appreciated./",5,4,False,self,,,,,
662,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,3,azwupl,self.MachineLearning,"Which names do you suggest for websites on Artificial intelligence, Machine learning, Deep learning ?",https://www.reddit.com/r/MachineLearning/comments/azwupl/which_names_do_you_suggest_for_websites_on/,Doctor_who1,1552328895,[removed],0,1,False,self,,,,,
663,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,3,azwxx7,arxiv.org,Using a One Class SVM to detect intruders in less than an old school tweet without using actual keys - just timing data,https://www.reddit.com/r/MachineLearning/comments/azwxx7/using_a_one_class_svm_to_detect_intruders_in_less/,borowcm,1552329326,,1,1,False,default,,,,,
664,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,3,azx1h6,self.MachineLearning,Test data set impacting predictions?,https://www.reddit.com/r/MachineLearning/comments/azx1h6/test_data_set_impacting_predictions/,run_jmc_619,1552329804,[removed],0,1,False,self,,,,,
665,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,3,azx2z8,self.MachineLearning,[Discussion] Launching our new AI startup ConstipatedAI: return of investment capped at 1000x,https://www.reddit.com/r/MachineLearning/comments/azx2z8/discussion_launching_our_new_ai_startup/,astonished_crofty,1552330005,"**Products/Services of the startup:**

Cannot be disclosed due to potential malicious use by entities that are not us. We need to have a discussion about it and that is the point actually. 

**Team:**

* Not Elon Musk
* Not Andrej Karpathy
* Not Durk Kingma
* Definitely not Ian Goodfellow
* Maybe Pieter Abbeel

**Benefits:**

* Nobel Peace Prize 2050
* Can circumvent conference dual submission policies by submitting your research to The Verge or Wired first.
* Complimentary LSD with breakfast
* Opportunity to beat OpenAI Five
* Netflix and Coke Fridays",3,15,False,self,,,,,
666,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,4,azxabg,arxiv.org,[R] Using a One Class SVM to detect intruders in less than an old school tweet without using actual keys - just timing data,https://www.reddit.com/r/MachineLearning/comments/azxabg/r_using_a_one_class_svm_to_detect_intruders_in/,borowcm,1552331026,,1,0,False,default,,,,,
667,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,4,azxkyf,self.MachineLearning,"[D] Is there any good, up-to-date review on how to decode latent variables of GANs?",https://www.reddit.com/r/MachineLearning/comments/azxkyf/d_is_there_any_good_uptodate_review_on_how_to/,reddit_tl,1552332468,"I am still a beginner. I see a lot of advances in training high resolution generators with great results. But on the latent variable decoding front, is there any major advances?
It would be very helpful if a more knowledgeable person can point to a few references on this subject.",4,2,False,self,,,,,
668,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,4,azxwas,self.MachineLearning,"The Error in the Comparator: Or, scikit-learn's importance weighting in CV is broken.",https://www.reddit.com/r/MachineLearning/comments/azxwas/the_error_in_the_comparator_or_scikitlearns/,thedeaktator,1552334049,"A description of a potential bug in in importance weighted cross validation in scikit-learn that causes suboptimal hyper-parameters to be returned when using cross validation routines in sklearn.model\_selection.  The same issue also exists in dask-ml.  For details, see: [http://deaktator.github.io/2019/03/10/the-error-in-the-comparator/](http://deaktator.github.io/2019/03/10/the-error-in-the-comparator/)",0,1,False,self,,,,,
669,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,5,azy1k3,youtube.com,[R] Video: Imagination-Augmented Agents for Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/azy1k3/r_video_imaginationaugmented_agents_for_deep/,ykilcher,1552334745,,0,1,False,default,,,,,
670,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,6,azyxk8,i.redd.it,[D] GANS (Generally Adversarial Networks),https://www.reddit.com/r/MachineLearning/comments/azyxk8/d_gans_generally_adversarial_networks/,OddTrust,1552339074,,0,1,False,default,,,,,
671,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,6,azyzxz,m.youtube.com,Insight on what fitness really means and how people have been misdefining it.,https://www.reddit.com/r/MachineLearning/comments/azyzxz/insight_on_what_fitness_really_means_and_how/,funnyfunfun122,1552339393,,0,1,False,default,,,,,
672,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,6,azz5kf,self.MachineLearning,[D] GANs (Generally Adversarial Networks),https://www.reddit.com/r/MachineLearning/comments/azz5kf/d_gans_generally_adversarial_networks/,OddTrust,1552340159,https://imgur.com/a/qh8sYr8,0,1,False,self,,,,,
673,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,6,azzaax,self.MachineLearning,[D] How do you deal with datasets &gt;30GB?,https://www.reddit.com/r/MachineLearning/comments/azzaax/d_how_do_you_deal_with_datasets_30gb/,Santarini,1552340820,"R Studio, Excel, Notepad, all seem to freak out when dealt files near 1GB. So how do you deal with even larger file sizes? Do you simply break them down into smaller parts?",33,14,False,self,,,,,
674,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,7,azzivl,self.MachineLearning,[D] RecSys challenge winner paper review,https://www.reddit.com/r/MachineLearning/comments/azzivl/d_recsys_challenge_winner_paper_review/,tdls_to,1552341962,"we will be reviewing the RecSys challenge winner paper with the authors at 6:30pm EST today

[https://tdls.a-i.science/events/2019-03-11/](https://tdls.a-i.science/events/2019-03-11/)

&amp;#x200B;

what questions do you have for the authors? ",5,3,False,self,,,,,
675,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,7,azzm1e,imgur.com,[D] GAN Stabilization Techniques,https://www.reddit.com/r/MachineLearning/comments/azzm1e/d_gan_stabilization_techniques/,Boozybrain,1552342389,,1,1,False,default,,,,,
676,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,7,b0017p,self.MachineLearning,What apps would you recommend for taking Machine Learning notes?,https://www.reddit.com/r/MachineLearning/comments/b0017p/what_apps_would_you_recommend_for_taking_machine/,Scatterbrain191,1552344524,"I love taking organized notes! And I'm learning a ton of methods and the basic code to preform them (so a lot of breadth, not a ton of depth). For this reason, I really want a nice application to use to write out notes on model assumptions/pro&amp;cons/mathematical process, with code embedded to know how to perform them in my own analysis!   


Writing straight into R markdowns isn't cutting it (takes too long to compile and sort). ",0,1,False,self,,,,,
677,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,9,b00xk9,self.MachineLearning,Is there a theoretical or fundamental reason why LayerNorm outperforms BatchNorm on RNN networks?,https://www.reddit.com/r/MachineLearning/comments/b00xk9/is_there_a_theoretical_or_fundamental_reason_why/,artificial_intelect,1552349420,[removed],0,1,False,self,,,,,
678,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,9,b01dlm,i.redd.it,Advanced machine learning method (OLS) finds bananas related to increased fall deaths in the US!,https://www.reddit.com/r/MachineLearning/comments/b01dlm/advanced_machine_learning_method_ols_finds/,IntermittentWifi,1552351878,,0,1,False,https://b.thumbs.redditmedia.com/EpAjf1DJWOW5e_CtTlOAAAwEhxGf5sTPXyphEvoD6Tc.jpg,,,,,
679,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,10,b01np9,self.MachineLearning,[D] Tensorflow CPU Hardware Requirements,https://www.reddit.com/r/MachineLearning/comments/b01np9/d_tensorflow_cpu_hardware_requirements/,Frank1789,1552353530,"Suddenly the hint ""Tensorflow needs AVX Support""-note disappeared from their website.

Instead there is actually no information about CPU-Hardware requirements at all any more!  
Does anybody know which features are needed for the latest GPU docker image? I would like to build a NVIDIA GPU Tensorflow PC on a budget CPU.

Actually I have a Celeron Setup on ASUS Z-290-A Mainboard. But I think about switching to AMD while looking at the Intel prices at the moment.",4,0,False,self,,,,,
680,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,10,b01qqg,self.MachineLearning,[D] Preventing exploding gradients when using ReLU? (A: ReLU doesn't work with softmax!),https://www.reddit.com/r/MachineLearning/comments/b01qqg/d_preventing_exploding_gradients_when_using_relu/,MasterLuke2019,1552354049,,1,1,False,default,,,,,
681,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,10,b01svt,self.MachineLearning,Ludwig error,https://www.reddit.com/r/MachineLearning/comments/b01svt/ludwig_error/,Frank1789,1552354421,[removed],0,1,False,https://b.thumbs.redditmedia.com/SVk9BZJpGqJumWtwQihc9ERp9J_WEhA3DPsXCZMH4QQ.jpg,,,,,
682,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,11,b023xt,youtube.com,4000L 5layers Extrusion Vertical Tank Blow Moulding Machine Process,https://www.reddit.com/r/MachineLearning/comments/b023xt/4000l_5layers_extrusion_vertical_tank_blow/,miyawang12138,1552356267,,1,1,False,default,,,,,
683,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,11,b024am,self.MachineLearning,[D] Ludwig Error,https://www.reddit.com/r/MachineLearning/comments/b024am/d_ludwig_error/,Frank1789,1552356336,"Ludwig seems to be a great thing. I am working on the docker image ""tensorflow/tensorflow:1.5.0-gpu-py3"" and tried to install Ludwig according to their installation instructions. But I get this error (see screenshot) when trying to fire it up in my Jupiter notebook.

&amp;#x200B;

https://i.redd.it/js2txfgddll21.png",4,0,False,self,,,,,
684,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,11,b029op,peanutmachinerychina.com,Honey Peanut Processing Line Manufacturers | Maoyuan,https://www.reddit.com/r/MachineLearning/comments/b029op/honey_peanut_processing_line_manufacturers_maoyuan/,alam161,1552357256,,0,1,False,default,,,,,
685,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,11,b02bfx,github.com,You feeling bored? You want to do something fun? Checkout an awesome new @github repo I published. An introduction into Facial Recognition with #Python and #OpenCV #ML #POC #FaceRecognition,https://www.reddit.com/r/MachineLearning/comments/b02bfx/you_feeling_bored_you_want_to_do_something_fun/,aBigSchwein,1552357552,,0,1,False,default,,,,,
686,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,12,b02se0,youtube.com,SULZER GS900 LOOM PART RAPIER GRIPPER,https://www.reddit.com/r/MachineLearning/comments/b02se0/sulzer_gs900_loom_part_rapier_gripper/,Suntech216,1552360375,,1,1,False,spoiler,,,,,
687,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,12,b0358l,analyticsinsight.net,IBM Sets hope for Alzheimers Disease Diagnoses with Machine Learning | Analytics Insight,https://www.reddit.com/r/MachineLearning/comments/b0358l/ibm_sets_hope_for_alzheimers_disease_diagnoses/,analyticsinsight,1552362581,,0,1,False,default,,,,,
688,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,13,b03b5z,self.MachineLearning,*Looking for proactive members for collaborative learning and project partnership (ML/AI)!*,https://www.reddit.com/r/MachineLearning/comments/b03b5z/looking_for_proactive_members_for_collaborative/,Goodnessgraciousgirl,1552363662,"Hello!

We are looking for new members to join **Geeks on Fire, a new SLACK group that encourages, nurtures and enhances growth in technical skills, including ML/AI.** We understand most people are busy and have lives but thats okay  we are in the same boat! If you are interested in active participation and in contributing to the good of the whole (as well as each individual), send me a message and we can go from there. 

**Geeks on Fire Main Focus points**:

\- Information Security (we have active Info Sec projects)

\- **Data science (we have active AI/ML projects)**

\- Programming

\- IoT (Raspberry etc)

\- Linux, Microsoft Windows, Mac OS and other OS

\- System Administration

\- Professional Mentorship

\- **Community Learning**

**- Hands on projects (projects are excellent for portfolios)**

&amp;#x200B;

If you are interested, please send me a message!

&amp;#x200B;

GGG",0,1,False,self,,,,,
689,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,13,b03lru,self.MachineLearning,Help needed,https://www.reddit.com/r/MachineLearning/comments/b03lru/help_needed/,vmsaurabh,1552365665,[removed],0,1,False,self,,,,,
690,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,14,b0432l,self.MachineLearning,[D] Do you need to adjust the probability if you use the 'class_weight' parameter in LogisticRegression-sklearn?,https://www.reddit.com/r/MachineLearning/comments/b0432l/d_do_you_need_to_adjust_the_probability_if_you/,amil123123,1552369137,"0

I have a imbalanced dataset and I want the the output as probabilities and not labels. Hence using Logistic Regression seemed to be the obvious choice.

However the classifier started predicting all data points belonging to majority class which caused a problem for me. I then decided to use 'class\_weight = balanced' of sklearn package which assigns weights to classes in the loss function. Now I do achieve a decent model with ROC AUC of 0.85.

However I have the following questions :-

1. Do I need to adjust the predicted probabilities since I messed around with distribution by using the class weight parameter?
2. In my evaluation set I used stratified split. Is this a good choice or should I have balanced dataset in my evaluation set?
3. Given both class are equally important is ROC AUC a good metric?",5,2,False,self,,,,,
691,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,15,b04d84,esparkinfo.com,The Only Unconventional Guide to Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b04d84/the_only_unconventional_guide_to_machine_learning/,eSpark_Biz,1552371264,,0,1,False,default,,,,,
692,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,15,b04dbc,twitter.com,[D] fchollet's highlights of the TensorFlow 2.0 + Keras API summarized in a thread of tweets,https://www.reddit.com/r/MachineLearning/comments/b04dbc/d_fchollets_highlights_of_the_tensorflow_20_keras/,blowjobtransistor,1552371283,,0,1,False,default,,,,,
693,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,15,b04gmq,self.MachineLearning,Present-Day Mobile Applications and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b04gmq/presentday_mobile_applications_and_machine/,appsbee,1552372023,[removed],0,1,False,self,,,,,
694,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,15,b04mbi,youtube.com,Centrifugal Deoiling Machine Price PhilippinesFried Food Deoiling Machi...,https://www.reddit.com/r/MachineLearning/comments/b04mbi/centrifugal_deoiling_machine_price/,fryingmachine,1552373270,,0,1,False,default,,,,,
695,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,16,b04q5y,self.MachineLearning,Model Pruning and Quantization in Tensorflow,https://www.reddit.com/r/MachineLearning/comments/b04q5y/model_pruning_and_quantization_in_tensorflow/,vikranth94,1552374115,[removed],0,1,False,self,,,,,
696,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,16,b04qsk,self.MachineLearning,HELP NEEDED,https://www.reddit.com/r/MachineLearning/comments/b04qsk/help_needed/,vmsaurabh,1552374272,[removed],0,1,False,self,,,,,
697,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,16,b04sr7,self.MachineLearning,Smart Kitchen Market Analysis Reveals unstable development by 2023,https://www.reddit.com/r/MachineLearning/comments/b04sr7/smart_kitchen_market_analysis_reveals_unstable/,apple_x9,1552374703,[removed],1,1,False,self,,,,,
698,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,16,b04wix,arxiv.org,Regularity Normalization: Constraining Implicit Space with Minimum Description Length,https://www.reddit.com/r/MachineLearning/comments/b04wix/regularity_normalization_constraining_implicit/,doerlbh,1552375554,,3,2,False,default,,,,,
699,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,16,b04wwg,self.MachineLearning,"""[Project]"" Model Pruning and Quantization in Tensorflow",https://www.reddit.com/r/MachineLearning/comments/b04wwg/project_model_pruning_and_quantization_in/,vikranth94,1552375641,"Since I couldn't find any easy tutorial on how to do Model pruning in tensorflow, I've implemented a simple code for model pruning for a CNN model trained on Cifar-10 dataset. The github repo also includes 8-bit quantization using tflite.

Any suggestions on how to improve the code is welcome.

Note: I've used tensorflow 1.12.  For versions &gt;1.12, replace tf.contrib.lite to tf.lite

Github: [https://github.com/vikranth94/Model\_Compression](https://github.com/vikranth94/Model_Compression)",0,1,False,self,,,,,
700,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,16,b04yiy,self.learnmachinelearning,The Probability distribution of an Adversarial Autoencoder,https://www.reddit.com/r/MachineLearning/comments/b04yiy/the_probability_distribution_of_an_adversarial/,FreddyShrimp,1552376033,,0,1,False,default,,,,,
701,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,16,b04zd5,self.MachineLearning,[P] Model Pruning and Quantization in Tensorflow,https://www.reddit.com/r/MachineLearning/comments/b04zd5/p_model_pruning_and_quantization_in_tensorflow/,vikranth94,1552376239,"Since I couldn't find any easy tutorial on how to do Model pruning in tensorflow, I've implemented a simple code for model pruning for a CNN model trained on Cifar-10 dataset. The github repo also includes 8-bit quantization using tflite.

Any suggestions on how to improve the code is welcome.

Note: I've used tensorflow 1.12.  For versions &gt;1.12, replace tf.contrib.lite to tf.lite

Github: [https://github.com/vikranth94/Model\_Compression](https://github.com/vikranth94/Model_Compression)",7,7,False,self,,,,,
702,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,16,b0515d,arxiv.org,[R] Partially Shuffling the Training Data to Improve Language Models (new SOTA on Penn Treebank &amp; WikiText-2),https://www.reddit.com/r/MachineLearning/comments/b0515d/r_partially_shuffling_the_training_data_to/,ofirpress,1552376671,,42,68,False,default,,,,,
703,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,18,b05x64,self.MachineLearning,What do we mean by high dimensional data?,https://www.reddit.com/r/MachineLearning/comments/b05x64/what_do_we_mean_by_high_dimensional_data/,paulera_m,1552384341,[removed],0,1,False,self,,,,,
704,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,19,b061wi,thisresumedoesnotexist.com,RNN writes fictional people resumes with TextGenRNN,https://www.reddit.com/r/MachineLearning/comments/b061wi/rnn_writes_fictional_people_resumes_with/,deepsyx,1552385384,,0,1,False,https://b.thumbs.redditmedia.com/DJlLYAqF1rFXqRRu_TwwXtQ7awq8jfO6GPTTNiWLs8Q.jpg,,,,,
705,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,19,b068dv,medium.com,"Tracking, organization, and collaboration for data science projects",https://www.reddit.com/r/MachineLearning/comments/b068dv/tracking_organization_and_collaboration_for_data/,ai_yoda,1552386781,,0,1,False,https://b.thumbs.redditmedia.com/kw_nyAhtpKr2ZiSwA2BPRo36bHjEj4gUpCdsflJoN9Y.jpg,,,,,
706,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,19,b069t7,youtu.be,What is Machine Learning? Machine Learning Tutorial For Beginners,https://www.reddit.com/r/MachineLearning/comments/b069t7/what_is_machine_learning_machine_learning/,manjeet17,1552387102,,1,1,False,https://a.thumbs.redditmedia.com/N41VtkiSSnrh11rwpF4TESer2jGEwBa8WhsemgmAYU8.jpg,,,,,
707,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,19,b06b99,self.MachineLearning,In what applications EXCEPT ANNs is training data being used?,https://www.reddit.com/r/MachineLearning/comments/b06b99/in_what_applications_except_anns_is_training_data/,engineerL,1552387428,"I regularly read vague statements about technical platforms and corporations that collect data for ""machine learning purposes"". When reading such a statement, I always assume that ""machine learning"" essentially means ANNs, or a subset of ANNs, like logistic regression models.

I'm more familiar with ANNs than other machine learning concepts, so there might be stuff I'm not aware of here. But is there any other noteworthy use case of training data than ANNs, or very primitive stuff like linear regression?",0,1,False,self,,,,,
708,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,19,b06caq,self.MachineLearning,Question Answering: Machine versus Expert System - IBM Watson Does Your Taxes,https://www.reddit.com/r/MachineLearning/comments/b06caq/question_answering_machine_versus_expert_system/,andrea_manero,1552387654,[removed],0,1,False,self,,,,,
709,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,19,b06dde,self.MachineLearning,What is meant by high dimensional data?,https://www.reddit.com/r/MachineLearning/comments/b06dde/what_is_meant_by_high_dimensional_data/,paulera_m,1552387890,,0,1,False,self,,,,,
710,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,19,b06dnx,youtube.com,What is Machine Learning? Machine Learning Tutorial For Beginners,https://www.reddit.com/r/MachineLearning/comments/b06dnx/what_is_machine_learning_machine_learning/,mukeshkkt,1552387954,,0,1,False,default,,,,,
711,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,19,b06fac,self.MachineLearning,MOOC to learn data science and tricks of the trade,https://www.reddit.com/r/MachineLearning/comments/b06fac/mooc_to_learn_data_science_and_tricks_of_the_trade/,xlpgto,1552388298,[removed],0,1,False,self,,,,,
712,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,20,b06mcl,self.MachineLearning,A question about test performance,https://www.reddit.com/r/MachineLearning/comments/b06mcl/a_question_about_test_performance/,magicyoung33,1552389665,[removed],0,1,False,self,,,,,
713,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,20,b06spg,self.MachineLearning,AI vs Machine Learning: The Difference You Need to know,https://www.reddit.com/r/MachineLearning/comments/b06spg/ai_vs_machine_learning_the_difference_you_need_to/,docksonpaul,1552390903,[removed],0,1,False,self,,,,,
714,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,21,b074ua,thegradient.pub,[D] The Promise of Hierarchical Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/b074ua/d_the_promise_of_hierarchical_reinforcement/,alexeyr,1552393207,,0,1,False,default,,,,,
715,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,21,b075y0,infiflex.com,Machine Learning on Cloud,https://www.reddit.com/r/MachineLearning/comments/b075y0/machine_learning_on_cloud/,rohitgupta010,1552393423,,0,1,False,default,,,,,
716,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,21,b076ts,self.MachineLearning,[D] Decision tree for categorical independent variables,https://www.reddit.com/r/MachineLearning/comments/b076ts/d_decision_tree_for_categorical_independent/,MLUser2018,1552393590,"Hello,

&amp;#x200B;

I want to train data in a pattern of \[categorical variable 1*(x1)*|categorical variable 2*(x2)*|categorical variable 3*(x3)*|continuous label*(y)*\], while the categorical variables will be transformed to non-ordinal numeric variables. Can someone tell me mathematically, why it is recommended, to use decision trees instead of linear regressions or analysis of variance in this case?

&amp;#x200B;

Yours sincerely",3,1,False,self,,,,,
717,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,21,b078xi,naifmehanna.com,Scaling the A3C algorithm to multiple machines using Tensorflow.JS,https://www.reddit.com/r/MachineLearning/comments/b078xi/scaling_the_a3c_algorithm_to_multiple_machines/,naifmeh,1552393984,,1,1,False,https://a.thumbs.redditmedia.com/WNEsQBPM66mTs3bpboYuTaNV3cXGmHwGGh_jXPMTpb8.jpg,,,,,
718,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,21,b07a56,self.MachineLearning,Impact of Machine Learning and Robotics on Society and global market - Dr. Kunal Singh Berwar,https://www.reddit.com/r/MachineLearning/comments/b07a56/impact_of_machine_learning_and_robotics_on/,patronageinstitute,1552394199,[removed],0,1,False,self,,,,,
719,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,21,b07a5u,self.MachineLearning,"Why no word embeddings (Glove, word2vecetc) used in first attention paper?",https://www.reddit.com/r/MachineLearning/comments/b07a5u/why_no_word_embeddings_glove_word2vecetc_used_in/,mocarsha,1552394203,[removed],0,1,False,self,,,,,
720,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,21,b07ceq,self.MachineLearning,How Important are Databases to Machine Learning Algorithms?,https://www.reddit.com/r/MachineLearning/comments/b07ceq/how_important_are_databases_to_machine_learning/,ShadowStormDrift,1552394610,[removed],0,1,False,self,,,,,
721,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,21,b07cs8,self.MachineLearning,Name my business,https://www.reddit.com/r/MachineLearning/comments/b07cs8/name_my_business/,Parkermillguy,1552394677,[removed],0,1,False,self,,,,,
722,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,22,b07i1o,twitter.com,Machine Learning Projects A-Z : Kaggle and Real World Pro 95% off,https://www.reddit.com/r/MachineLearning/comments/b07i1o/machine_learning_projects_az_kaggle_and_real/,TallisRomaniv,1552395611,,0,1,False,default,,,,,
723,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,22,b07kai,self.MachineLearning,AI vs Machine Learning: The Difference You Need to know,https://www.reddit.com/r/MachineLearning/comments/b07kai/ai_vs_machine_learning_the_difference_you_need_to/,docksonpaul,1552395981,[removed],0,1,False,self,,,,,
724,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,22,b07uau,cnvrg.io,How I saved 80% in cloud costs,https://www.reddit.com/r/MachineLearning/comments/b07uau/how_i_saved_80_in_cloud_costs/,Mayalittlepony,1552397647,,0,1,False,default,,,,,
725,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,22,b07ujr,self.MachineLearning,"[P][JOB]Work with Haskell, functional programming and Machine learning with SemiConductors",https://www.reddit.com/r/MachineLearning/comments/b07ujr/pjobwork_with_haskell_functional_programming_and/,Khancity,1552397685,[removed],0,1,False,self,,,,,
726,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,23,b088ca,self.MachineLearning,[N] Build a Q&amp;A Bot with DeepLearning4J with Willem Meints (45min talk from GOTO Berlin 2018),https://www.reddit.com/r/MachineLearning/comments/b088ca/n_build_a_qa_bot_with_deeplearning4j_with_willem/,goto-con,1552399909,"* [Video](https://youtu.be/2bbcaRcSMF0?list=PLEx5khR4g7PJW7u0GKxRPIQddtu69boT3)
* [Slides](https://gotober.com/2018/sessions/524)

&amp;#x200B;

ABSTRACT

Chatbots are here - you no longer necessarily talk to a human when you contact your insurance agency. Whether that's a good thing remains to be seen, but it sure is interesting for us as developers.

The primary goal of my talk is to show you how you can use DeepLearning4J to build a neural network for answering frequently asked questions. I will show you how to build, train, test and use a the neural network in a basic chatbot.

This talk is aimed at developers who have heard of neural networks, but don't want to get involved in all the math behind it. This is a not-so-scientific introduction into the wonderful world of chatbots and AI.",0,0,False,self,,,,,
727,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,23,b08a4a,self.MachineLearning,Any good and rich Facial Emotion AU's identification dataset for students?,https://www.reddit.com/r/MachineLearning/comments/b08a4a/any_good_and_rich_facial_emotion_aus/,aniket_agarwal,1552400195,[removed],0,1,False,self,,,,,
728,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,23,b08l6f,hiddenforces.io,"CTRL-labs Thomas Reardon on How His Companys Neural Interface Is Revolutionizing the Fields of Machine Learning, Neuroscience, and Robotics",https://www.reddit.com/r/MachineLearning/comments/b08l6f/ctrllabs_thomas_reardon_on_how_his_companys/,cpclos,1552401990,,0,1,False,default,,,,,
729,MachineLearning,t5_2r3gv,2019-3-12,2019,3,12,23,b08m6i,self.MachineLearning,What if...,https://www.reddit.com/r/MachineLearning/comments/b08m6i/what_if/,dciug,1552402149,"This is just an idea that I have. Feel free to tell me how stupid I am in the comments. I'm not that familiar with neuroscience, just pure ML engineering. I would appreciate if  you guided me to relevant papers maybe, if any of this makes sense.

The simple assumption that the brain doesn't learn by backprop naturally drives me to ask: is it capable of learning by forward prop alone? I think the brain learns by pure association between multiple stimuli. If not just Minky's ""Society of Mind"" and not just pure Deep Learning, why not both? I don't think the brain can learn anything from a single sensor, it would maybe encode the pattern. Say the brain is only connected to the eyes, although all the other structures in the brain are in place. This kind of brain wouldn't be able to abstract information into concepts.

The brain could only reason about the world if the visual stimuli were encoded into a pattern(in V1) and linked to a pattern in another context(suppose the sound of the word that mentions the object). E.g: the child sees a tree and the parent points to it and says ""tree"". The brain connects those two encodings. The function of the brain is to strengthen the paths between those two encodings. At first the signals would disperse in all directions, but if two associations become more frequent, some paths become stronger than others. The paths that connects all these contexts is what we call ""language"". The concept of an entity is encoded in the neural activity of the language. The concept of a ""dog"" would be the neural activity that links all the associations that have been built over time. Translation between languages is easier if a person is bilingual by birth, because he/she thinks in that language, by which I mean that the abstract concept is linked directly to the sound encoding and motor encoding. Otherwise, a person would have to think of the concept, switch to another language(a different neural pathway, not as strong as the native pathway) and use the appropriate encodings.

&amp;#x200B;

Just some random thoughts. Thanks for taking the time to read this.",0,1,False,self,,,,,
730,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,0,b08vez,thisresumedoesnotexist.com,Funny resumes generated by AI,https://www.reddit.com/r/MachineLearning/comments/b08vez/funny_resumes_generated_by_ai/,aginovski,1552403544,,0,1,False,default,,,,,
731,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,0,b08vmu,self.MachineLearning,[D] Autoencoders detecting anomalies for Unsupervised Learning?,https://www.reddit.com/r/MachineLearning/comments/b08vmu/d_autoencoders_detecting_anomalies_for/,Fender6969,1552403574,"Apologize for the somewhat confusing title. I have an unsupervised learning task at hand and I would like to go about classifying instances and anomalies or not. In doing this, I have used Keras and I now have my reconstruction errors across roughly 8 columns. Since I am not trying to determine whether a specific variable is anomalous rather the entire row is  anomalous, I am a bit confused on how to go about this. Here is what I have done:

&amp;#x200B;

I now have a dataframe from the autoencoder that has the reconstruction error for each row and each column:

1) I have taken the average of each of the column and added them up. I then took the average of that sum.

2) I took the average of the standard deviations of all the columns.

3) I created sort of a threshold as the sum of step 1 and step 2

4) Any row of data with the average reconstruction error (of all columns) that is greater than the threshold is classified as an anomaly. 

&amp;#x200B;

Am I going about this the right way? I don't have a labeled response variable to compare my results to.

&amp;#x200B;

Any help would be great!",19,13,False,self,,,,,
732,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,0,b08z5v,self.MachineLearning,How to do text classification with multiple input parameters?,https://www.reddit.com/r/MachineLearning/comments/b08z5v/how_to_do_text_classification_with_multiple_input/,its_all_relative_,1552404083,[removed],0,1,False,self,,,,,
733,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,0,b08zyh,medium.com,New SOTA on Instance Segmentation: Mask Scoring R-CNN Tops Mask R-CNN on COCO,https://www.reddit.com/r/MachineLearning/comments/b08zyh/new_sota_on_instance_segmentation_mask_scoring/,Yuqing7,1552404204,,0,2,False,https://b.thumbs.redditmedia.com/lHsxMlTVru48MLLClILrDL7CLU_CUbUU101JWAAXkZk.jpg,,,,,
734,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,0,b091ts,self.MachineLearning,Google Computer Vision algorithms suffer from optical illusion too,https://www.reddit.com/r/MachineLearning/comments/b091ts/google_computer_vision_algorithms_suffer_from/,blueishbasil,1552404469,[removed],0,1,False,self,,,,,
735,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,0,b095u6,self.MachineLearning,Now AI generates resume for you,https://www.reddit.com/r/MachineLearning/comments/b095u6/now_ai_generates_resume_for_you/,deepsyx,1552405041,[removed],0,1,False,self,,,,,
736,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,0,b095w2,self.MachineLearning,[D] How to do text classification on multiple input parameters,https://www.reddit.com/r/MachineLearning/comments/b095w2/d_how_to_do_text_classification_on_multiple_input/,its_all_relative_,1552405048,"I was unavailable to find a method online....

Say I am trying to classify the category of a newspaper article based on both the title and the first paragraph. 

However, I wouldn't want to just concatenate the title and the paragraph to one single input of text as there might be different weights put on words depending if its from the title or the paragraph.",3,1,False,self,,,,,
737,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,0,b09emg,self.MachineLearning,[P] Averaged weight in tf-keras pip package,https://www.reddit.com/r/MachineLearning/comments/b09emg/p_averaged_weight_in_tfkeras_pip_package/,nlkey2022,1552406295," I am doing some toy project about train/serve using kubernetes in  distributed computing environment.

To training each mini-batch in one container, we have to distribute dataset splited by mini-batch and train as parallel in each host machine. After training, we will aggregate each trained model using container which can aggregate.

so I made simple average weight pip package so that use package in toy project. Thanks

[https://github.com/graykode/modelaverage](https://github.com/graykode/modelaverage)",0,2,False,self,,,,,
738,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,1,b09k78,self.MachineLearning,[D] What are some good tools for creating ROI's for large image datasets easily?,https://www.reddit.com/r/MachineLearning/comments/b09k78/d_what_are_some_good_tools_for_creating_rois_for/,Mockapapella,1552407045,"A while back I remember seeing something like this on GitHub, but I can't for the life of me find it now",0,1,False,self,,,,,
739,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,1,b09l45,youtube.com,How would you rate this video if this was the first ML resource that you came across? Does it do justice to the topic?,https://www.reddit.com/r/MachineLearning/comments/b09l45/how_would_you_rate_this_video_if_this_was_the/,kkokane,1552407180,,0,1,False,https://b.thumbs.redditmedia.com/4Vl-qI6G1WIyR4vg8OVlj500pE5CvVHvKrK_hqrTUzA.jpg,,,,,
740,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,1,b09n1d,self.MachineLearning,find closest numbers approximately,https://www.reddit.com/r/MachineLearning/comments/b09n1d/find_closest_numbers_approximately/,Armin71,1552407447,[removed],0,1,False,self,,,,,
741,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,1,b09nkd,self.MachineLearning,Working With net packets,https://www.reddit.com/r/MachineLearning/comments/b09nkd/working_with_net_packets/,Remideza,1552407521,[removed],0,1,False,self,,,,,
742,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,1,b09nuf,self.MachineLearning,Learning mathematics for machine learning,https://www.reddit.com/r/MachineLearning/comments/b09nuf/learning_mathematics_for_machine_learning/,fusion312,1552407560,[removed],0,1,False,self,,,,,
743,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,1,b09o28,self.MachineLearning,PixelCNN++ Masked Convolution Implementation,https://www.reddit.com/r/MachineLearning/comments/b09o28/pixelcnn_masked_convolution_implementation/,tua98,1552407592,[removed],0,1,False,self,,,,,
744,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,1,b09qox,self.MachineLearning,[N] Applications of Deep Generative Models on Smartphones : Part 1 (Image),https://www.reddit.com/r/MachineLearning/comments/b09qox/n_applications_of_deep_generative_models_on/,ElBalistico,1552407963,"In this short post, I give a few examples of what generative models (mostly GANs) can bring to our smartphones, especially in the domain of image generation.

&amp;#x200B;

Have a look ! :)

&amp;#x200B;

[https://heartbeat.fritz.ai/using-generative-deep-learning-models-on-device-c37aa74ae4dd](https://heartbeat.fritz.ai/using-generative-deep-learning-models-on-device-c37aa74ae4dd)",0,8,False,self,,,,,
745,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,1,b09s7x,self.MachineLearning,[D] SOTA in keypoint detection?,https://www.reddit.com/r/MachineLearning/comments/b09s7x/d_sota_in_keypoint_detection/,tworats,1552408173,"After very stupidly losing a large amount of manually annotated keypoint data, I'd like to try to automate some of the process. What is practical SOTA in keypoint detection? The task is similar to facial keypoint detection on custom image data, with only 4 keypoints.",4,2,False,self,,,,,
746,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,1,b09vpr,nature.com,Unmasking Clever Hans predictors and assessing what machines really learn,https://www.reddit.com/r/MachineLearning/comments/b09vpr/unmasking_clever_hans_predictors_and_assessing/,m11e1,1552408661,,0,1,False,default,,,,,
747,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,1,b09wy1,self.MachineLearning,[D] Looking for recommendations on best bang for the buck platform for machine learning.,https://www.reddit.com/r/MachineLearning/comments/b09wy1/d_looking_for_recommendations_on_best_bang_for/,playaspec,1552408839,"I have been tasked with building a high powered desktop/server for running various machine learning tasks. I'm looking for the ability to host four GPUs, prefer Xeons over iSeries processors, and the ability to install more than 32GB of RAM.

I've looked at systems built by Boxx, which are attractive, but if I could assemble something similar using whitebox components, I could hopefully save a few bucks for other niceties. What are you all using? Thanks!",9,7,False,self,,,,,
748,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,1,b0a00p,nature.com,[R] Unmasking Clever Hans predictors and assessing what machines really learn,https://www.reddit.com/r/MachineLearning/comments/b0a00p/r_unmasking_clever_hans_predictors_and_assessing/,m11e1,1552409265,,0,1,False,https://b.thumbs.redditmedia.com/q5QJtuyNqaQl3PvbGJAp3-FsAkkG4k0vj1Ws4xGRwfU.jpg,,,,,
749,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,1,b0a284,self.MachineLearning,[D] What's your opinion of the GPT-2 ethics PR campaign given that OpenAI was planning to go for profit?,https://www.reddit.com/r/MachineLearning/comments/b0a284/d_whats_your_opinion_of_the_gpt2_ethics_pr/,notsoopenai,1552409571,"Sure seems to me like it was a way to build up hype for the new fundraising round and fool potential investors into believing that they're leading the field in their goal to build ""AGI"".

&amp;#x200B;

[gdb claiming that this was in the works for two years](https://news.ycombinator.com/item?id=19360147)

[some more info proving that the LP plan was in the works for a while](https://twitter.com/GreatCrashO2018/status/1105168044949680128)

&amp;#x200B;

Also wouldn't be surprised to see them pump and dump this company and exit with a return for the early investors. ",83,153,False,self,,,,,
750,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,2,b0a7fi,self.MachineLearning,"Predicting Auction Prices Before Auction, Not During",https://www.reddit.com/r/MachineLearning/comments/b0a7fi/predicting_auction_prices_before_auction_not/,My_DataScience,1552410267," 

Has  anyone ever predicted a value (let's say price) of an auction before an  auction happens? I've looked at predictive modeling on this topic but  the academic research i've seen does modeling that updates as the  auction happens. That's not quite what I would like to see. I'm trying  to predict the final price of an item that goes through an online  auction portal but i'm getting varied results that are hard to improve.  These are my metrics so far with the best (what I believe is the best)  model.

Adjusted R2 Score: 96%

MAPE: 13%

RMSLE: 0.17240

The  main two I am focusing on are R2 and MAPE. I honestly think the metrics  aren't too bad so far but i'm trying to get them to a really impressive  state. The main issue I am having is similar items (product name,  condition, reviews of seller) can all be the same but final prices can  be very different from one day to the next when the auction is  completed. Don't know how to account for what is seemingly a random  process of an item going up or down. Seasonality doesn't have much of  effect. So the day of the week or month or time of the year doesn't  impact it much.

I found this Kaggle competition which helped me generate some other ideas: [https://www.kaggle.com/c/bluebook-for-bulldozers](https://www.kaggle.com/c/bluebook-for-bulldozers)

Thought I would throw it out to the Reddit-verse to hear your thoughts!",0,1,False,self,,,,,
751,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,2,b0a7qq,self.MachineLearning,[D] CycleGAN model collapse - Any bright ideas ?,https://www.reddit.com/r/MachineLearning/comments/b0a7qq/d_cyclegan_model_collapse_any_bright_ideas/,ashutoshbsathe,1552410305,"Hi,

I've been using CycleGAN for converting gameplay of 1989 Prince of Persia 1 to its newer version Prince of Persia 2. I've collected 8000 images of both the games and resized them into 320x200 dimensions. Then I'm using CycleGAN's TensorFlow implementation by [vanhuyz](https://github.com/vanhuyz/CycleGAN-TensorFlow) to train the network. 

After about 2400 steps, all of the outputs are blackish. 

![Imgur](https://i.imgur.com/yFcDay8.png)

OR [link to image if it doesn't load](https://i.imgur.com/yFcDay8.png)

It seems that in both the cycles (A-&gt;B-&gt;A) and (B-&gt;A-&gt;B) reconstruction works alright but overall conversion is very poor.

The original paper mentions the idea of adding an identity loss for preserving input colors. But I didn't understand it correctly. Can anyone please explain ?

I'm also open to other bright ideas.

Thanks.",15,21,False,self,,,,,
752,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,2,b0ahmi,ai.googleblog.com,An All-Neural On-Device Speech Recognizer,https://www.reddit.com/r/MachineLearning/comments/b0ahmi/an_allneural_ondevice_speech_recognizer/,sjoerdapp,1552411695,,0,1,False,default,,,,,
753,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,2,b0aigl,self.MachineLearning,Robocalls,https://www.reddit.com/r/MachineLearning/comments/b0aigl/robocalls/,sumekenov,1552411807,[removed],0,1,False,self,,,,,
754,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,2,b0ankj,self.MachineLearning,Convlolutional NN results are different every time I run it.,https://www.reddit.com/r/MachineLearning/comments/b0ankj/convlolutional_nn_results_are_different_every/,Scutterbum,1552412549,"I got a good result on the first run, but an average result on the second. Never managed to get back to the initial good result. It's lost forever. 

It seems that setting initial weights will allow me to control and reproduce results. But I haven't seen this being done in any CNN tutorials online. 

So what's the best practice here? Should initial weights be random or should I be setting them from the beginning? 

I want to start tuning parameters, but if initial weights are random, I won't be able to tell if an improved result is due to different random weights or a parameter I just tweaked. 

Any advice appreciated. Thanks",0,1,False,self,,,,,
755,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,2,b0anue,blog.griddynamics.com,[N] New Case Study: Safety Stock Optimization for Ship-from-Store,https://www.reddit.com/r/MachineLearning/comments/b0anue/n_new_case_study_safety_stock_optimization_for/,ikatsov,1552412590,,0,1,False,default,,,,,
756,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,3,b0b2u3,self.MachineLearning,Is it possible to use a CNN for anything other than image classification?,https://www.reddit.com/r/MachineLearning/comments/b0b2u3/is_it_possible_to_use_a_cnn_for_anything_other/,notfirecrow,1552414686,"Also, is it possible to have a NN take text as an input, or do I have to convert it to numerical data using a model first?",0,1,False,self,,,,,
757,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,4,b0bp1g,github.com,"[P] PyToune, our front-end library to PyTorch, is asking for suggestions for a name change. We'd like inputs from the community for the new name.",https://www.reddit.com/r/MachineLearning/comments/b0bp1g/p_pytoune_our_frontend_library_to_pytorch_is/,freud_14,1552417791,,1,4,False,default,,,,,
758,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,4,b0bqx0,youtube.com,"MIT AI: Reinforcement Learning, Planning, and Robotics (Leslie Kaelbling)",https://www.reddit.com/r/MachineLearning/comments/b0bqx0/mit_ai_reinforcement_learning_planning_and/,UltraMarathonMan,1552418042,,0,1,False,https://b.thumbs.redditmedia.com/QMC37zBuJpbbb_81bbfsXvkzkrUeeEgZTA9TaCZNoYw.jpg,,,,,
759,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,4,b0c28w,self.MachineLearning,Trump A.I. Executive Order,https://www.reddit.com/r/MachineLearning/comments/b0c28w/trump_ai_executive_order/,stopaisabotage,1552419588,[removed],0,1,False,self,,,,,
760,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,5,b0cfb0,self.MachineLearning,What is intrinsically valuable and unique about the PhD experience?,https://www.reddit.com/r/MachineLearning/comments/b0cfb0/what_is_intrinsically_valuable_and_unique_about/,colugo,1552421403,"What are the specific skills and experiences you can't get elsewhere? Or, how does going through a PhD program help you get the important skills and experiences faster or more deeply than you could elsewhere? If it's as much of an independent experience as many sources describe, what makes it any easier or any more structured than a completely self-guided program?

&amp;#x200B;

I've read posts here, on [fastai](https://www.fast.ai/2018/08/27/grad-school/), [Karpathy's thoughts](http://karpathy.github.io/2016/09/07/phd/), and [Philip J. Guo's experience](http://pgbovine.net/PhD-memoir.htm). Most often people are concerned about the PhD's relevance to the job market. I'm concerned whether it's actually any better than alternatives like intensive self-study with support from online communities (eg., here, Meetups, or fastai forums).

&amp;#x200B;

I find some of the things Karpathy talks about - freedom, ownership, personal growth - are my principal motivations in considering a PhD. But I fear some of the constraints, requirements, and culture might be distracting from truly valuable work.

&amp;#x200B;

Right now I am a master's student working on some research with the aim of getting into a good grad school. I am feeling discouraged  because I feel like my project is uninspiring, and I'm only doing it to get publications for positive signaling/getting into a PhD program. I already have some topics and immediate directions I could be investigating instead.

&amp;#x200B;

I hate this ""playing the game"" as Karpathy calls it and want to get on with interesting contributions. Does this get better in an actual PhD program? How often will I feel compelled to play this sort of game?

&amp;#x200B;

Does it all just come down to the quality of one's advisor?",0,1,False,self,,,,,
761,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,5,b0ct9y,kaggle.com,[N] Kaggle kernels are now using P100 GPUs instead of K80,https://www.reddit.com/r/MachineLearning/comments/b0ct9y/n_kaggle_kernels_are_now_using_p100_gpus_instead/,Theunbidden,1552423362,,0,1,False,default,,,,,
762,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,5,b0cz3b,self.MachineLearning,"Hey guys,",https://www.reddit.com/r/MachineLearning/comments/b0cz3b/hey_guys/,champianalien21,1552424172,[removed],0,1,False,self,,,,,
763,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,6,b0d3kx,self.MachineLearning,What are some lesser known/sleeper Universities for ML?,https://www.reddit.com/r/MachineLearning/comments/b0d3kx/what_are_some_lesser_knownsleeper_universities/,Zistance,1552424806,[removed],0,1,False,self,,,,,
764,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,6,b0davq,self.MachineLearning,US AI Executive Order,https://www.reddit.com/r/MachineLearning/comments/b0davq/us_ai_executive_order/,stopaisabotage,1552425834,What opinions do people have about how the US AI Executive Order influences the EU when it comes to Machine Learning?,0,1,False,self,,,,,
765,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,6,b0dik1,self.MachineLearning,How to begin Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b0dik1/how_to_begin_machine_learning/,heemanshusuri,1552426932,[removed],0,1,False,self,,,,,
766,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,6,b0dmu0,self.MachineLearning,How to get into machine learning as a Highschool freshman?,https://www.reddit.com/r/MachineLearning/comments/b0dmu0/how_to_get_into_machine_learning_as_a_highschool/,jhimmyjohn,1552427548,[removed],0,1,False,self,,,,,
767,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,7,b0dry2,turingtribe.com,Duck or rabbit? Google Cloud Vision suffers from the same optical illusion that humans do | Turing Tribe,https://www.reddit.com/r/MachineLearning/comments/b0dry2/duck_or_rabbit_google_cloud_vision_suffers_from/,braito_xyz,1552428256,,0,1,False,default,,,,,
768,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,7,b0e0es,self.MachineLearning,[R] Streaming End-to-end Speech Recognition For Mobile Devices (Google AI Blog),https://www.reddit.com/r/MachineLearning/comments/b0e0es/r_streaming_endtoend_speech_recognition_for/,modeless,1552429424,"Production voice recognition for Google's Android keyboard is now done by an on-device end-to-end RNN transducer model. I guess RNNs are all you need after all?

Blog: https://ai.googleblog.com/2019/03/an-all-neural-on-device-speech.html

arXiv: https://arxiv.org/abs/1811.06621",16,70,False,self,,,,,
769,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,9,b0f5hi,captrobau.blogspot.com,[P] Remastering Star Trek: Deep Space Nine With Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b0f5hi/p_remastering_star_trek_deep_space_nine_with/,CaptRobau,1552435436,,1,1,False,https://b.thumbs.redditmedia.com/d7VCWyUUgwjGLX6klzJD-m13rRI9WZd7p3GK5caOAPM.jpg,,,,,
770,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,9,b0f64w,self.MachineLearning,[D] How to begin Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b0f64w/d_how_to_begin_machine_learning/,heemanshusuri,1552435538,[removed],0,1,False,self,,,,,
771,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,9,b0f6u1,cnet.com,Chrome extension from Google wants to filter out toxic comments,https://www.reddit.com/r/MachineLearning/comments/b0f6u1/chrome_extension_from_google_wants_to_filter_out/,FortuitousAdroit,1552435648,,1,1,False,https://b.thumbs.redditmedia.com/gzHrzT4odlZBz52WkS0FhZNwHAhSVUKtUDlFUoxYmgU.jpg,,,,,
772,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,9,b0fifj,self.MachineLearning,ML applied to physics/materials science,https://www.reddit.com/r/MachineLearning/comments/b0fifj/ml_applied_to_physicsmaterials_science/,anandc1988,1552437387,"https://rdcu.be/bniOz

Just a small contribution where we applied neural networks to electronic structure. First author here. I'm a postdoc and I just started learning ML about a year ago. This subreddit has been tremendously useful for me!


",0,1,False,self,,,,,
773,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,10,b0futv,self.MachineLearning,On the fly train ?,https://www.reddit.com/r/MachineLearning/comments/b0futv/on_the_fly_train/,nicetryho,1552439356,[removed],0,1,False,self,,,,,
774,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,10,b0fwi0,self.MachineLearning,Attention collapse in neural networks,https://www.reddit.com/r/MachineLearning/comments/b0fwi0/attention_collapse_in_neural_networks/,nivter,1552439623,[removed],0,1,False,self,,,,,
775,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,10,b0g9hh,self.CNCPressBrakeMachine,CNC Hydraulic Press Brake Open Loop and Closed Loop Control System,https://www.reddit.com/r/MachineLearning/comments/b0g9hh/cnc_hydraulic_press_brake_open_loop_and_closed/,CNCPressBrakeMachine,1552441791,,0,1,False,https://a.thumbs.redditmedia.com/btSuOzcSs1osEFzQ_2xZoBAfMcZ59LAeNabzR7aJB68.jpg,,,,,
776,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,10,b0g9w9,self.MachineLearning,Microsoft Textworld Competition: play text adventure games and win $2000,https://www.reddit.com/r/MachineLearning/comments/b0g9w9/microsoft_textworld_competition_play_text/,jinpanZe,1552441858,[removed],0,1,False,self,,,,,
777,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,11,b0gp03,self.MachineLearning,Need a suggestion about my final Year Project,https://www.reddit.com/r/MachineLearning/comments/b0gp03/need_a_suggestion_about_my_final_year_project/,BloodyWeed,1552444417,[removed],0,1,False,self,,,,,
778,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,11,b0gwhm,self.MachineLearning,Survey: Spot the GAN generated tweets,https://www.reddit.com/r/MachineLearning/comments/b0gwhm/survey_spot_the_gan_generated_tweets/,ml0x01,1552445766,"Hey there,

&amp;#x200B;

for my master's thesis I am evaluating the quality of GANs for tweet generation. For doing an human-evaluation, I set up an online questionnaire. The questionnaire takes about 20 minutes and you have the chance to win an Amazon voucher if you complete it :)

&amp;#x200B;

You can find the questionnaire at [https://survey.hilko.eu](https://survey.hilko.eu)

&amp;#x200B;

Thank you :)

&amp;#x200B;

P.S. This post was allowed by a moderator.",0,2,False,self,,,,,
779,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,12,b0hawu,appliedmachinelearning.blog,A tutorial on using BERT model for Text Classification problems.,https://www.reddit.com/r/MachineLearning/comments/b0hawu/a_tutorial_on_using_bert_model_for_text/,Abhijeet3922,1552448390,,0,1,False,default,,,,,
780,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,13,b0hibr,self.MachineLearning,[D] Difficulties working with the keras community,https://www.reddit.com/r/MachineLearning/comments/b0hibr/d_difficulties_working_with_the_keras_community/,idg101,1552449791,"I am working on a project which uses Keras which isn't my choice but it is what it is.  I am trying to find help with some of functions in keras and keras\_contrib and am finding that the community seems to continually shit on me.  What is the deal with this?  I'm a seasoned machine learning scientist who's been writing python code for about 17 years.  I've contributed to a few open source projects and even had one of my own about 8 years ago.  Out of all my experiences, I have a terrible time dealing with the folks who make keras and keras\_contrib.  Is py-torch like this also?  What is the purpose of all this?",9,6,False,self,,,,,
781,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,13,b0hs3d,self.MachineLearning,[R] Papers/Resources for ML/NLP on Social Media for Market Forecasting,https://www.reddit.com/r/MachineLearning/comments/b0hs3d/r_papersresources_for_mlnlp_on_social_media_for/,Seankala,1552451623,"Hello. I'm currently working as an undergraduate researcher at a machine learning lab at university, and will be continuing into graduate studies. Undergraduates have been given the task of submitting our theses to an annual conference.

My topic is ""analyzing social media and gauging their impacts on markets."" More specifically, the market I'm focusing on is the Bitcoin market and the social media platform I'll be focusing on is Twitter. The aim is to see if there really is a correlation between social media and markets.

My large blueprint is to:

1. Choose a number of influential users on Twitter and categorize them (e.g. politician, cryptocurrency influencer, engineer, etc.).

2. Analyze their Tweets to extract keywords that were used frequently and perform sentiment analysis on the overall Tweet that contains the keyword.

3. Analyze those Tweets with how the market moved.

4. Train model and make adjustments accordingly.

The main question I would have is if anyone knows whether there is academic literature regarding this issue? There are quite a few papers analyzing the relationship with news and more conventional finance markets, but I haven't been able to find any that focus on social media and sentiment analysis specifically.

Any help or feedback would be greatly appreciated. Thanks!",4,3,False,self,,,,,
782,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,14,b0i32a,self.MachineLearning,Ideas for 1 month AI project,https://www.reddit.com/r/MachineLearning/comments/b0i32a/ideas_for_1_month_ai_project/,Monstrous_moonshine,1552453717,[removed],0,1,False,self,,,,,
783,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,14,b0i6ln,blog.piekniewski.info,[D] A brief story of Silicon Valleys affair with AI,https://www.reddit.com/r/MachineLearning/comments/b0i6ln/d_a_brief_story_of_silicon_valleys_affair_with_ai/,wei_jok,1552454428,,0,1,False,default,,,,,
784,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,14,b0i72m,self.MachineLearning,[D] A Brief Story Of Silicon Valley's Affair With AI,https://www.reddit.com/r/MachineLearning/comments/b0i72m/d_a_brief_story_of_silicon_valleys_affair_with_ai/,baylearn,1552454533,"A pessimistic (but entertaining) perspective which I think adds discussion to OpenAI's recent change to LP status

https://blog.piekniewski.info/2019/03/12/a-short-story-of-silicon-valleys-affair-with-ai/",29,106,False,self,,,,,
785,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,14,b0i9tz,self.MachineLearning,[D] Second order SGD methods usually approximate Hessian with positive definite matrix (e.g. Gauss-Newton) - can it handle saddles?,https://www.reddit.com/r/MachineLearning/comments/b0i9tz/d_second_order_sgd_methods_usually_approximate/,jarekduda,1552455116,"For example due to symmetry of parameters, functions optimized in machine learning usually have [huge number of local minima and saddles](https://www.offconvex.org/2016/03/22/saddlepoints/), growing exponentially with dimension - it seems crucial for gradient descent methods to pass saddles in a safe distance (?)

I am trying to understand second order SGD convergence methods ([slides](https://www.dropbox.com/s/54v8cwqyp7uvddk/SGD.pdf)), and it seems like they often attract to a saddle, like [natural gradient](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.7538&amp;rep=rep1&amp;type=pdf) wanting to take us to a close point with zero gradient.

There are many approaches trying to escape non-convexity by approximating Hessian with some positive definite matrix, for example:

  -  [Gauss-Newton](https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm) method and [Fisher information matrix](https://en.wikipedia.org/wiki/Fisher_information#Matrix_form) using some linear approximation (e.g. [K-FAC](https://arxiv.org/pdf/1503.05671)),
  - [TONGA](https://papers.nips.cc/paper/3234-topmoumoute-online-natural-gradient-algorithm.pdf) uses covariance matrix of recent gradients.

While such approximation tries to pretend that minimized function is locally convex, in fact it isn't - we can be near a saddle in this moment.

How do such positive Hessian approximations handle saddles?

For example, naively, covariance matrix of recent gradients should ignore sign of curvature - be similar near minimum and near saddle - why using it doesn't attract to saddles?",33,50,False,self,,,,,
786,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,15,b0ik0f,self.MachineLearning,Telescopic fork | Single depth Telescopic forks,https://www.reddit.com/r/MachineLearning/comments/b0ik0f/telescopic_fork_single_depth_telescopic_forks/,lhd121,1552457260,[removed],0,1,False,self,,,,,
787,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,16,b0j8fi,self.MachineLearning,"Machine Learning as a Service (Mlaas) Market  Size, Outlook, Trends and Forecasts (2018  2024)",https://www.reddit.com/r/MachineLearning/comments/b0j8fi/machine_learning_as_a_service_mlaas_market_size/,FancyMethod,1552462697,[removed],0,1,False,self,,,,,
788,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,16,b0ja75,self.MachineLearning,Learning TensorFlow: V1.3 vs V2.0,https://www.reddit.com/r/MachineLearning/comments/b0ja75/learning_tensorflow_v13_vs_v20/,de1pher,1552463135,[removed],0,1,False,self,,,,,
789,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,17,b0jfwf,insidesherpa.com,"Data Science &amp; AI Virtual Internships with YC Companies by Inside Sherpa. No application, no CV needed.",https://www.reddit.com/r/MachineLearning/comments/b0jfwf/data_science_ai_virtual_internships_with_yc/,lesharcerer,1552464555,,0,1,False,default,,,,,
790,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,17,b0jlk9,ssdntech.com,Introduction To Machine Learning - What is Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/b0jlk9/introduction_to_machine_learning_what_is_machine/,Anussdn,1552465996,,0,1,False,default,,,,,
791,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,17,b0jmdw,datalabs.optisolbusiness.com,Optisol Datalabs - Machine Learning | Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/b0jmdw/optisol_datalabs_machine_learning_artificial/,Optisoldatalabs,1552466210,,0,1,False,default,,,,,
792,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,17,b0joxc,self.MachineLearning,[D] Is there any opensourced transactional datasets containing Offer or Promotional data?,https://www.reddit.com/r/MachineLearning/comments/b0joxc/d_is_there_any_opensourced_transactional_datasets/,gourxb,1552466866,"I have found a Acquire Valued Shoppers Challenge data \[1\] which contain both transnational and offer/promotional discount data as well. 

Is there any similar dataset available in the public domain?

&amp;#x200B;

&amp;#x200B;

\[1\] [https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data](https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data)",1,1,False,self,,,,,
793,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,17,b0jq3b,self.MachineLearning,How to control your web application with an integrated chatbot,https://www.reddit.com/r/MachineLearning/comments/b0jq3b/how_to_control_your_web_application_with_an/,conversational-ai,1552467176,"Integrate a SAP Conversational AI chatbot into any of your web applications and provide users with a fun and intuitive way to interact with the UI!

https://i.redd.it/i5rkx3i5lul21.png",0,1,False,self,,,,,
794,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,18,b0k25e,self.MachineLearning,What are some good resources to refresh python knowledge before job intereview,https://www.reddit.com/r/MachineLearning/comments/b0k25e/what_are_some_good_resources_to_refresh_python/,jeskelund,1552470081,[removed],0,1,False,self,,,,,
795,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,18,b0k471,self.MachineLearning,"[P] From Keras to C++, a practical example of Tensorflow C API based deployment",https://www.reddit.com/r/MachineLearning/comments/b0k471/p_from_keras_to_c_a_practical_example_of/,aljabr0,1552470580,"This small demo project is about deploying deep learning models on embedded platforms. The techniques exposed here have been particularly useful to me in the deployment of deep learning models in industrial applications.
We start with a simple example model, trained with **Tensorflow + Keras**. In the end, we'll freeze the model and export a GraphDef that can be loaded and executed through the **Tensorflow C API** (without Python).

[https://github.com/aljabr0/from-keras-to-c](https://github.com/aljabr0/from-keras-to-c)
",7,19,False,self,,,,,
796,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,19,b0klqx,valuecoders.com,"30 Simple App Ideas For Startups &amp; SMEs (AI/ML, Blockchain, AR/VR)",https://www.reddit.com/r/MachineLearning/comments/b0klqx/30_simple_app_ideas_for_startups_smes_aiml/,ValuecoderOffical,1552474512,,0,1,False,default,,,,,
797,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,20,b0krbk,smarten.com,Data Preparation Tools and Advanced Analytics for All!,https://www.reddit.com/r/MachineLearning/comments/b0krbk/data_preparation_tools_and_advanced_analytics_for/,ElegantMicroWebIndia,1552475588,,0,1,False,default,,,,,
798,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,20,b0kxoe,self.MachineLearning,My assumptions about the brain,https://www.reddit.com/r/MachineLearning/comments/b0kxoe/my_assumptions_about_the_brain/,dciug,1552476880,[removed],0,1,False,self,,,,,
799,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,20,b0l2ok,self.MachineLearning,[D] My assumptions about the brain,https://www.reddit.com/r/MachineLearning/comments/b0l2ok/d_my_assumptions_about_the_brain/,dciug,1552477858,"This is just an idea that I have. Feel free to tell me how stupid I am in the comments. I'm not that familiar with neuroscience, just pure ML engineering. I would appreciate if you guided me to relevant papers maybe, if any of this makes sense.

The simple assumption that the brain doesn't learn by backprop naturally drives me to ask: is it capable of learning by forward prop alone? I think the brain learns by pure association between multiple stimuli. If not just Minky's ""Society of Mind"" and not just pure Deep Learning, why not both? I don't think the brain can learn anything from a single sensor, it would maybe encode the pattern. Say the brain is only connected to the eyes, although all the other structures in the brain are in place. This kind of brain wouldn't be able to abstract information into concepts.

The brain could only reason about the world if the visual stimuli were encoded into a pattern(in V1) and linked to a pattern in another context(suppose the sound of the word that mentions the object). E.g: the child sees a tree and the parent points to it and says ""tree"". The brain connects those two encodings. The function of the brain is to strengthen the paths between those two encodings. At first the signals would disperse in all directions, but if two associations become more frequent, some paths become stronger than others. The paths that connects all these contexts is what we call ""language"". The concept of an entity is encoded in the neural activity of the language. The concept of a ""dog"" would be the neural activity that links all the associations that have been built over time. Translation between languages is easier if a person is bilingual by birth, because he/she thinks in that language, by which I mean that the abstract concept is linked directly to the sound encoding and motor encoding. Otherwise, a person would have to think of the concept, switch to another language(a different neural pathway, not as strong as the native pathway) and use the appropriate encodings.



Just some random thoughts. Thanks for taking the time to read this.
",8,0,False,self,,,,,
800,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,20,b0l50m,medium.com,Real world implementation of Logistic Regression,https://www.reddit.com/r/MachineLearning/comments/b0l50m/real_world_implementation_of_logistic_regression/,champianalien21,1552478293,,0,1,False,https://b.thumbs.redditmedia.com/7KrleIS63V3Rtwqi1L29squEgZDlmOFn6zy96GC2Wjk.jpg,,,,,
801,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,21,b0l8im,self.MachineLearning,[D] Intersection Between ML and Group Theory?,https://www.reddit.com/r/MachineLearning/comments/b0l8im/d_intersection_between_ml_and_group_theory/,MaxMachineLearning,1552478917,"So, to preface this, I graduated with my [B.Sc](https://B.Sc) in mathematics in April and have been working as a machine learning engineer at an industrial automation lab, doing primarily CV and RL based research and development.  My first love in mathematics wasn't ML, it was the weird world of geometric group theory. ML soon followed as I learned about CNNs (from a textbook on applied abstract algebra that talked about them due to translational invariance). From there, the bug bit me and I found immense joy in the field. I studied it on the side for about a year and a half, picking up a job at a local research organization after a very successful 4 month internship.

Now, the reason I give all this preface is to give some perspective on why I am interested in the field. Working on a certain project, I remembered the work of T.S Cohen on Group Equivariant CNNs. So using that, I was able to construct a solution to a problem in natural product grading that had been very problematic for most other approaches due to the need for rotational equivariance that data augmentation failed to capture. Now, my work got me accepted to a program that's going to let me do my Master's while working in the industry, basing my thesis off of research done for a problem in the industry. Due to my background, my Master's has to be in pure mathematics, not CS. So to circumvent this issue, I am being co-advised by someone at the university I will be attending and another prof at a separate university. The plan is to study this intersection between the two fields and apply it to some novel problem. Now, I am familiar with some of the more prominent works in the area (primarily the work of T.S Cohen as he seems to be on the frontline of this research).

I'm curious if anyone here is working in that field, and if so, is it a viable research option? Or if they have any recommended papers to read, topics to investigate, etc. I'm sorry if this post was a bit of a ramble, I am just super excited to have this opportunity and I want to make the most of it.",14,17,False,self,,,,,
802,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,21,b0l9m8,self.MachineLearning,[P] Does anybody know where I can get Twitter data with sentiment analysis?,https://www.reddit.com/r/MachineLearning/comments/b0l9m8/p_does_anybody_know_where_i_can_get_twitter_data/,Seankala,1552479118,"Hello. I'm currently working as an undergraduate research intern in a lab at school. I have a thesis/project to submit to a national conference in about a month. I'll explain the basic details of my idea:

I want to conduct analysis on social media (Twitter in particular) and gauge its relationship with markets (specifically the Bitcoin market). The plan is to,

&amp;#x200B;

1. Select a number of influential accounts (e.g. Donald Trump, Warren Buffet, JP Morgan, etc.)
2. Analyze their Tweets and extract keywords for significant changes in the Bitcoin data (Kaggle Bitcoin data).
3. Map the keywords to the sentiment of the Tweet and also the account.
4. Make future predictions using our model (e.g. a Tweet from Person A that has Keyword B with Sentiment C would mean that the market will go up/down).

&amp;#x200B;

I was talking with my supervisor today and he told me that getting the data for Twitter users sounds a bit tricky, and he advised me to go with the more conventional route of analyzing news rather than social media as there's much more resources.

I'm just curious though, does anybody know where I can find such data? Would I be able to make a dataset like that myself?

Any feedback about my project itself is also greatly appreciated.

Thanks!",12,3,False,self,,,,,
803,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,21,b0l9tc,self.MachineLearning,Need Clarity on Dimensional reductions techniques,https://www.reddit.com/r/MachineLearning/comments/b0l9tc/need_clarity_on_dimensional_reductions_techniques/,ShivSira,1552479148,[removed],0,1,False,self,,,,,
804,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,21,b0lcu7,self.MachineLearning,Handwritten text detection,https://www.reddit.com/r/MachineLearning/comments/b0lcu7/handwritten_text_detection/,ijaysonx,1552479684,[removed],0,1,False,self,,,,,
805,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,21,b0li7p,self.MachineLearning,"How to subtract water from an image (glare, reflection, tint, distortion)?",https://www.reddit.com/r/MachineLearning/comments/b0li7p/how_to_subtract_water_from_an_image_glare/,miguelos,1552480594,"I'm trying to classify pictures of fish. The fish is in the water and the camera is outside the water. The water makes the task more challenging, because of the glare, reflection, tint, distortion, etc.

Are there any pre-processing techniques or research papers about removing this kind of noise from pictures? I think reducing glare and normalizing tint would go a long way toward improving my results.",0,1,False,self,,,,,
806,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,21,b0lp96,self.MachineLearning,How to deal with different accents for speech recognition?,https://www.reddit.com/r/MachineLearning/comments/b0lp96/how_to_deal_with_different_accents_for_speech/,ssokhey,1552481829,[removed],0,1,False,self,,,,,
807,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,22,b0lxpt,self.MachineLearning,Multivariate multi step time series forecasting with LSTM,https://www.reddit.com/r/MachineLearning/comments/b0lxpt/multivariate_multi_step_time_series_forecasting/,andpej,1552483212,[removed],0,1,False,self,,,,,
808,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,22,b0maeu,self.MachineLearning,Multivariate multistep time series forecasting with LSTM,https://www.reddit.com/r/MachineLearning/comments/b0maeu/multivariate_multistep_time_series_forecasting/,andpej,1552485306,[removed],0,1,False,self,,,,,
809,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,23,b0mcvv,self.MachineLearning,[P] Cost function for subunitary values.,https://www.reddit.com/r/MachineLearning/comments/b0mcvv/p_cost_function_for_subunitary_values/,Seckar,1552485718,"Hello!

I am working on a project that aims to optimize an agent's behaviour in an environment. 

The simulation and logging is done so far however I have come to a bit of a stop and google has been no help. 

Let's say the simulation runs for 300 seconds, agent's aim is to stay close to some point moving randomly and not get more than a certain distance away from that point, if it gets too far away the agent dies instantly. 

The data at the end of a simulation is for all intents and purposes a list on numbers in the interval [-0.05, +0.05] (the error at each point in time), highly subunitary. 
The aim is to get these outputs as close to 0.0 as possible, but without it being exactly 0.0 as that means the agent's controller shut down as a result of outside input. Also, in case the agent fails the simulation before the end of the simulation the output list will be shorter than normal. 

I am trying to find/come up with a cost or reward function for this agent:

MAE. Plain averaging the values is kind of a naive option but a possible one.

MSE on the output I believe could work from what I have seen by looking at graphs since it also makes &lt;1 values fit a sigmoidal curve.

I'm not sure Huber loss can do anything here.

I thought about Log loss, isn't it kind of a pumped up MSE? 

Also for missing values (in case of early failure of the agent) I am thinking about padding it with some maximum error values (like 0.05).

I have previously gotten really useful feedback from this sub so I am posting here in hopes that there are some people willing to give a little feedback about my conclusions and hunches, maybe who have either confronted similar situations before or can come up with ideas of the top of their heads or in case there are some concerns that need to be taken into consideration which I have not thought about.

Cheers!",0,2,False,self,,,,,
810,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,23,b0me9a,self.MachineLearning,[Discussion] Multivariate multi step time series forecasting with LSTM,https://www.reddit.com/r/MachineLearning/comments/b0me9a/discussion_multivariate_multi_step_time_series/,andpej,1552485938,"I'm new to RNNs and LSTM and would like some direction with a problem I have. I have a data set containing system metrics (like CPU utilization, disk operations, memory use) of an AWS EC2 instance with a total of 7 columns and around 8000 rows. Each row represent 5 minutes of system performance.

I want to build a LSTM model to forecast the features for let's say the next half hour based on previous time steps. What would be the best approach for solving a problem like this? I know this can be done in many different ways but I would really appreciate some input how to go about this.",8,4,False,self,,,,,
811,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,23,b0mfjj,mloss.org,PSA: mloss.org is back up and running again,https://www.reddit.com/r/MachineLearning/comments/b0mfjj/psa_mlossorg_is_back_up_and_running_again/,psykocrime,1552486155,,1,1,False,default,,,,,
812,MachineLearning,t5_2r3gv,2019-3-13,2019,3,13,23,b0mtr1,medium.com,PyTorch Geometric: A Fast PyTorch Library for DL,https://www.reddit.com/r/MachineLearning/comments/b0mtr1/pytorch_geometric_a_fast_pytorch_library_for_dl/,Yuqing7,1552488428,,0,1,False,https://a.thumbs.redditmedia.com/KWDab2bKkUIsFz_nchyGTSn-66XRTxwIIdj-kW4Lzr0.jpg,,,,,
813,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,0,b0n4ux,aidlab.com,[R] Build training datasets directly from your body,https://www.reddit.com/r/MachineLearning/comments/b0n4ux/r_build_training_datasets_directly_from_your_body/,Guzikk,1552490135,,1,1,False,default,,,,,
814,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,0,b0ngtu,self.MachineLearning,Is it possible to create an AI that can join servers in games and play that game with other humans?,https://www.reddit.com/r/MachineLearning/comments/b0ngtu/is_it_possible_to_create_an_ai_that_can_join/,Roblox_Dev,1552491903,"Title pretty much says it all, also how would you be able to do it?",0,1,False,self,,,,,
815,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,0,b0nk0x,self.MachineLearning,"Simple Questions Thread March 13, 2019",https://www.reddit.com/r/MachineLearning/comments/b0nk0x/simple_questions_thread_march_13_2019/,AutoModerator,1552492369,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!
",0,1,False,self,,,,,
816,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,0,b0nkcz,self.MachineLearning,Atlas of knowledge networks - maps for learning and research on synthetic production of knowledge,https://www.reddit.com/r/MachineLearning/comments/b0nkcz/atlas_of_knowledge_networks_maps_for_learning_and/,gg4u,1552492420,[removed],0,1,False,self,,,,,
817,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,0,b0nkeo,arxiv.org,"[R] ""A Replication Study: Machine Learning Models Are Capable of Predicting Sexual Orientation From Facial Images"", Leuner 2019",https://www.reddit.com/r/MachineLearning/comments/b0nkeo/r_a_replication_study_machine_learning_models_are/,gwern,1552492429,,105,169,False,default,,,,,
818,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,0,b0nlnz,self.MachineLearning,Cheap GPU Prices from different vendors starts 1 cent / hour only !,https://www.reddit.com/r/MachineLearning/comments/b0nlnz/cheap_gpu_prices_from_different_vendors_starts_1/,formatlar,1552492613,[removed],0,1,False,self,,,,,
819,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,1,b0nou9,self.MachineLearning,A Complete Machine Learning Project Walk-Through in Python,https://www.reddit.com/r/MachineLearning/comments/b0nou9/a_complete_machine_learning_project_walkthrough/,Jewell980,1552493058,[removed],0,1,False,self,,,,,
820,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,1,b0nv02,self.MachineLearning,Resources for learning Non deep learning part of machine learning,https://www.reddit.com/r/MachineLearning/comments/b0nv02/resources_for_learning_non_deep_learning_part_of/,dhanno65,1552493935,[removed],0,1,False,self,,,,,
821,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,1,b0nxtt,self.MachineLearning,[P] An Environment to Train Autonomous Robot Using Reinforcement Learning- Bavarian Style!,https://www.reddit.com/r/MachineLearning/comments/b0nxtt/p_an_environment_to_train_autonomous_robot_using/,dtransposed,1552494340,"Hello everybody!

I would like to present the project in which I have created an autonomous garbage collector which operates in Oktoberfest-themed environment. It uses deep reinforcement learning and computer vision and uses Unity as simulation engine.
The blog post also contains the link to the GitHub repo:


https://dtransposed.github.io/blog/GEAR.html",0,1,False,self,,,,,
822,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,1,b0nzgn,self.MachineLearning,Resources to learn non deep learning part of machine learning,https://www.reddit.com/r/MachineLearning/comments/b0nzgn/resources_to_learn_non_deep_learning_part_of/,dhanno65,1552494553,[removed],0,1,False,self,,,,,
823,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,1,b0o65y,self.MachineLearning,"[D] To those involved with hiring ML people, what are your guidelines for vetting them? What things have you learned? What do you wish you knew when you first started interviewing? How do you come up with questions to ask?",https://www.reddit.com/r/MachineLearning/comments/b0o65y/d_to_those_involved_with_hiring_ml_people_what/,DisastrousProgrammer,1552495497,,61,99,False,self,,,,,
824,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,1,b0o8ys,self.MachineLearning,Machine Learning: An overview with the help of R software,https://www.reddit.com/r/MachineLearning/comments/b0o8ys/machine_learning_an_overview_with_the_help_of_r/,editorijsmi,1552495882,"  

**Machine Learning: An overview with the help of R software**

&amp;#x200B;

&amp;#x200B;

*Processing gif mtvbm6xo1xl21...*",0,1,False,self,,,,,
825,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,2,b0ofrf,blog.omeryavuz.gen.tr,Cheap GPU Prices from different vendors starts 1 cent / hour only !,https://www.reddit.com/r/MachineLearning/comments/b0ofrf/cheap_gpu_prices_from_different_vendors_starts_1/,formatlar,1552496840,,0,1,False,default,,,,,
826,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,2,b0oj5e,self.MachineLearning,Confusion over Variational autoencoders (VAEs),https://www.reddit.com/r/MachineLearning/comments/b0oj5e/confusion_over_variational_autoencoders_vaes/,mellow54,1552497319,"I had a small question about VAEs. In Variational Autoencoders (VAEs) we keep the (usually Gaussian ) prior fixed, however when we train Bayesian Neural Networks (BNNs) we update the prior of the Neural Networks weights to the posterior of the last update.

Why is it the case that in VAEs we keep the prior fixed, but in BNNs we update the priors during training?",0,1,False,self,,,,,
827,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,2,b0onum,sematext.com,Entity Extraction with Scikit-learn Classifiers,https://www.reddit.com/r/MachineLearning/comments/b0onum/entity_extraction_with_scikitlearn_classifiers/,seti321,1552497975,,1,2,False,default,,,,,
828,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,2,b0opc2,self.MachineLearning,Machine Learning with many rows per feature,https://www.reddit.com/r/MachineLearning/comments/b0opc2/machine_learning_with_many_rows_per_feature/,srigot55,1552498178,[removed],0,1,False,self,,,,,
829,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,2,b0orhy,self.MachineLearning,[P] Reinforcement Learning With Unity 3D: Cleaning up at the Oktoberfest!,https://www.reddit.com/r/MachineLearning/comments/b0orhy/p_reinforcement_learning_with_unity_3d_cleaning/,dtransposed,1552498459,"&amp;#x200B;

[https:\/\/dtransposed.github.io\/blog\/GEAR.html](https://i.redd.it/sju116va9xl21.png)",0,1,False,https://b.thumbs.redditmedia.com/Yz-ChHBwbC41jCazr4-Qtai3ND_14kvAIExndeRbytY.jpg,,,,,
830,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,2,b0ortq,github.com,"Demographic segmentation is market segmentation according to age, race, religion, gender, family size, ethnicity, income, and education. Demographics can be segmented into several markets to help an organization target its consumers.Here's an approach to DS using ANN.",https://www.reddit.com/r/MachineLearning/comments/b0ortq/demographic_segmentation_is_market_segmentation/,01raven01,1552498502,,0,1,False,default,,,,,
831,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,2,b0oss1,colab.sandbox.google.com,The JAX Autodiff Cookbook,https://www.reddit.com/r/MachineLearning/comments/b0oss1/the_jax_autodiff_cookbook/,alexbwww,1552498632,,0,4,False,default,,,,,
832,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,2,b0ovyd,self.MachineLearning,[P] Reinforcement Learning With Unity 3D: Cleaning up at the Oktoberfest!,https://www.reddit.com/r/MachineLearning/comments/b0ovyd/p_reinforcement_learning_with_unity_3d_cleaning/,dtransposed,1552499073,"&amp;#x200B;

https://i.redd.it/fi0vgxrn9xl21.png

Hello! Me and my team have recently created a prototype of an intelligent agent for garbage collection.  

The goal of the agent is collect relevant pieces of garbage, while avoiding collisions with static objects (such as chairs or tables). The  agent navigates in the environment (a mock-up of German Oktoberfest  tent) using camera RBG-D input. 

&amp;#x200B;

The blog post also contains a link to GitHub Repo:

[https://dtransposed.github.io/blog/GEAR.html](https://dtransposed.github.io/blog/GEAR.html)",2,15,False,https://b.thumbs.redditmedia.com/BBXUcHgF6ViduP_CeC2Mv0RYF8-TzLaP_5eJHyalXpk.jpg,,,,,
833,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,3,b0p2x6,nbcnews.com,Facial recognition's 'dirty little secret': Millions of online photos scraped,https://www.reddit.com/r/MachineLearning/comments/b0p2x6/facial_recognitions_dirty_little_secret_millions/,j_orshman,1552500058,,0,1,False,https://b.thumbs.redditmedia.com/0JUsTegMCOvDhAHZ72q7aMjsXDaCbUYqOxv2UN7Y0Nk.jpg,,,,,
834,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,3,b0p38e,dtransposed.github.io,Reinforcement Learning with Unity 3D,https://www.reddit.com/r/MachineLearning/comments/b0p38e/reinforcement_learning_with_unity_3d/,j_orshman,1552500100,,0,1,False,default,,,,,
835,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,3,b0pahc,self.MachineLearning,Neural Network- Chronicles of Major Milestones,https://www.reddit.com/r/MachineLearning/comments/b0pahc/neural_network_chronicles_of_major_milestones/,Jayeshv98,1552501085,[removed],0,1,False,self,,,,,
836,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,3,b0pf2u,github.com,CVPR 2019 Contests/Challenges,https://www.reddit.com/r/MachineLearning/comments/b0pf2u/cvpr_2019_contestschallenges/,skrish13,1552501720,,0,1,False,default,,,,,
837,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,3,b0pftu,github.com,[N] CVPR 2019 Contests/Challenges,https://www.reddit.com/r/MachineLearning/comments/b0pftu/n_cvpr_2019_contestschallenges/,skrish13,1552501825,,0,1,False,https://b.thumbs.redditmedia.com/eRtao5bQVK3aFFtug6b99Ws-AUg5RIv8oKIKTtfDrfs.jpg,,,,,
838,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,3,b0pno3,self.MachineLearning,[D] Concerns about OpenAI LP,https://www.reddit.com/r/MachineLearning/comments/b0pno3/d_concerns_about_openai_lp/,ringdongdang,1552502910,"Leadership and Vision.

There are several signs that OpenAI lacks consistent leadership and a clear vision. Despite both Elon and Sam having similar concerns about AI in the long run, Elon left [citing disagreements](https://twitter.com/elonmusk/status/1096989482094518273) about OpenAIs plans. [OpenAI Universe](https://openai.com/blog/universe/) was hyped then dumped, leading to massive layoffs (brushed off as being [""a little bit ahead of its time""](https://80000hours.org/podcast/episodes/the-world-needs-ai-researchers-heres-how-to-become-one/)). Moreover, their CTOs vision is nebulous and is new to both machine learning and academic research ([""Our goal right now is to do the best thing there is to do. Its a little vague.""](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)).

&amp;#x200B;

Talent Hemorrhaging and Recruitment.

Many of OpenAIs stars have long since left, such as Kingma and Goodfellow. At this time, OpenAI has 1-3 respected and well-known researchers, some of whom are busy with executive obligations. Part of this hemorrhaging may be [due](https://www.linkedin.com/in/dario-amodei-3934934/) [to](https://www.linkedin.com/in/daniela-amodei-790bb22a/) [nepotism](https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/openai-general-support#Relationship_disclosures). Furthermore, OpenAI recruiting practices diverge from peer institutions. Rather than relying on tried-and-true heuristics used in academe and industry, OpenAI has adopted less predictive heuristics (such as high-school awards, physics experience, PR-grabbing researcher age).

&amp;#x200B;

Commitment to Safety.

Despite the founders [legitimate](https://futureoflife.org/2015/06/06/sam-altman-investing-in-ai-safety-research/) [interest](https://futureoflife.org/2015/10/12/elon-musk-donates-10m-to-keep-ai-beneficial/) in making AI safer, for most of its run, OpenAI has employed 1-2 safety researchers. Even DeepMind, which has a [cofounder](https://twitter.com/ShaneLegg/status/1045343927442841602) who is concerned about safety, has a larger safety team. More, DeepMind has had an ethics committee for most of its existence, while OpenAI has none.

&amp;#x200B;

Questions.

Why should we believe that OpenAI plan to build AGI as quickly as possible will result in a safer AGI than if it was built by DeepMind? Is it because OpenAI leadership has better intentions?

Not long ago, ""more data"" was the simplistic answer to all ML problems. Now OpenAIs strategy, a strategy which may work well for startups but less reliably for research, is to use ""more compute."" Why does OpenAI believe that scaling up methods from the next few years will be sufficient to create AGI?",27,28,False,self,,,,,
839,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,4,b0pzuw,colab.research.google.com,The JAX Autodiff Cookbook,https://www.reddit.com/r/MachineLearning/comments/b0pzuw/the_jax_autodiff_cookbook/,or-17,1552504520,,0,1,False,default,,,,,
840,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,4,b0q2nc,arxiv.org,[Research]Dense Adaptive Cascade Forest: A Self Adaptive Deep Ensemble for Classification Problems,https://www.reddit.com/r/MachineLearning/comments/b0q2nc/researchdense_adaptive_cascade_forest_a_self/,enverx,1552504894,,0,1,False,default,,,,,
841,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,4,b0q75x,arxiv.org,[R] Dense Adaptive Cascade Forest: A Self Adaptive Deep Ensemble for Classification Problems,https://www.reddit.com/r/MachineLearning/comments/b0q75x/r_dense_adaptive_cascade_forest_a_self_adaptive/,enverx,1552505492,,3,3,False,default,,,,,
842,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,4,b0q8w4,self.MachineLearning,Question - Sparse Event prediction problem,https://www.reddit.com/r/MachineLearning/comments/b0q8w4/question_sparse_event_prediction_problem/,stom6,1552505720,[removed],0,1,False,self,,,,,
843,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,4,b0qglk,self.MachineLearning,[D] Titan V Deep Learning Benchmarks with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/b0qglk/d_titan_v_deep_learning_benchmarks_with_tensorflow/,mippie_moe,1552506762,"[https://lambdalabs.com/blog/titan-v-deep-learning-benchmarks/](https://lambdalabs.com/blog/titan-v-deep-learning-benchmarks/)

&amp;#x200B;

We were curious about the Titan V's performance in Deep Learning. At the Titan V's price point ($2999), the Titan RTX ($2499) is superior. The Titan RTX is faster for both FP32 and FP16 and is $500 less expensive. RTX 2080 Ti still comes out on top as the best buy IMO.

&amp;#x200B;

**For FP32 TensorFlow trianing, the Titan V is:**

* 42% faster than RTX 2080
* 41% faster than GTX 1080 Ti
* 26% faster than Titan XP
* 4% faster than RTX 2080 Ti
* 90% as fast as Titan RTX
* 75% as fast as Tesla V100 (32 GB)

&amp;#x200B;

**For FP16 TensorFlow training, The Titan V is:**

* 111% faster than GTX 1080 Ti
* 94% faster than Titan XP
* 70% faster than RTX 2080
* 23% faster than RTX 2080 Ti
* 87% as fast as Titan RTX
* 68% as fast as Tesla V100 (32 GB)",0,0,False,self,,,,,
844,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,5,b0qtzd,colab.research.google.com,[R] The JAX Autodiff Cookbook,https://www.reddit.com/r/MachineLearning/comments/b0qtzd/r_the_jax_autodiff_cookbook/,alexbwww,1552508580,,0,1,False,https://b.thumbs.redditmedia.com/Rcj51CQJpLeVHLeHdvkuL-kcjWoq47pFmIgVHh9UahU.jpg,,,,,
845,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,5,b0qz2j,makeartwithpython.com,"[P] Recreating ""The Clock"" with Machine Learning and Web Scraping",https://www.reddit.com/r/MachineLearning/comments/b0qz2j/p_recreating_the_clock_with_machine_learning_and/,kpkaiser,1552509266,,0,1,False,default,,,,,
846,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,5,b0r0dc,youtube.com,"[N] Natural Language Use Cases at Uber - Top Talk @ MLconf '18, by Franziska Bell",https://www.reddit.com/r/MachineLearning/comments/b0r0dc/n_natural_language_use_cases_at_uber_top_talk/,shonburton,1552509444,,2,3,False,default,,,,,
847,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,5,b0r3wm,self.MachineLearning,[R] The JAX Autodiff Cookbook,https://www.reddit.com/r/MachineLearning/comments/b0r3wm/r_the_jax_autodiff_cookbook/,alexbwww,1552509935,"Automatic differentiation in JAX is easy and pretty powerful.

[https://colab.research.google.com/github/google/jax/blob/master/notebooks/autodiff\_cookbook.ipynb](https://colab.research.google.com/github/google/jax/blob/master/notebooks/autodiff_cookbook.ipynb)",0,20,False,self,,,,,
848,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,6,b0rdsi,self.MachineLearning,[D] Irresponsible anthropomorphism is killing AI journalism,https://www.reddit.com/r/MachineLearning/comments/b0rdsi/d_irresponsible_anthropomorphism_is_killing_ai/,TiredOldCrow,1552511304,"Basically the title.  The current state of media coverage of AI is fixated on constructing a compelling narrative to readers, and often personifies models well beyond their capabilities.  This is to the extent that articles almost always end up reading like every classifier is some form of limited AGI.

Take [""Meet Norman the Psychopathic AI""](https://www.bbc.com/news/technology-44040008), an article by the BBC, whom I generally consider quite capable journalists.  While the research methodology and some of the implications are discussed in the article, the majority of laypeople who encounter the article will likely erroneously conclude that Norman possesses beliefs, a worldview, and some dark outlook on humanity.  Some readers will think ""Norman"" is violent or dangerous, with a mind of his own.  A headline and an image go a long way in communication, especially online.

And this article is by far not the worst offender. Many news outlets perform much worse, publishing misleading, fearmongering, or sensationalist stories about ""some new AI"", borrowing from pop sci-fi tropes, with the star AI inevitably represented by lacklustre CG avatars bought off stock photo websites.

I remember having several discussions in the wake of the Facebook experiment where researchers had AIs communicate, and saw they developed a communication standard unreadable by humans.  Based on the articles that circulated afterwards, a significant number of people concluded ""they had to turn it off because they were on the verge of SKYNET"".

In the interests of doing more than just ranting: how do we deal with this as a community?  Should we be reaching out to journalists about these issues?  Is it our responsibility in interviews to communicate the limitations of the models we develop?

Personifying the projects we work on, and giving them human qualities, is certainly entertaining and helps market our research.  That said, it seems like a sizeable portion of the public has been misinformed about the state of machine learning research as a result.
",69,528,False,self,,,,,
849,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,7,b0s3p0,self.MachineLearning,[P] Recreating The Clock with Machine Learning and Web Scraping,https://www.reddit.com/r/MachineLearning/comments/b0s3p0/p_recreating_the_clock_with_machine_learning_and/,kpkaiser,1552514936,"Hi /r/machinelearning!

Last year I saw Christian Marclay's [The Clock](https://www.youtube.com/watch?v=xp4EUryS6ac) at the Tate Modern.

When I saw it, I assumed someone used something like YOLO to build the compilation of thousands of videos. Instead, it was a team of six assistants and over a hundred thousand dollar budget.

Since then, I've started trying to recreate ""The Clock"" by using YOLO and scraping videos on YouTube. This post is the first part in a (planned) series, where we build a Scraping and Inference API to detect and join videos featuring clocks.

Follow up posts will show how to run the APIs in Kubernetes with GPU acceleration, and finally, we'll try to detect the time featured in clocks.

Post is here:
https://www.makeartwithpython.com/blog/recreating-the-clock-with-machine-learning-and-web-scraping/

Code is here, with YAML files for deploying to Kubernetes:

[Inference Service](https://github.com/burningion/nvidia-accelerated-pytorch-ffmpeg-opencv)
[Scraper Service](https://github.com/burningion/firefox-splinter-docker)",6,45,False,self,,,,,
850,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,8,b0ssvl,self.MachineLearning,Domain adaption or what?,https://www.reddit.com/r/MachineLearning/comments/b0ssvl/domain_adaption_or_what/,yrg23,1552518539,"I want to use only one data set (labeled) to train my model and respond for other data sets (labeled). What should i do? Data aug., GAN etc.?  What's the name of this type of problem? Supervised or semi-supervised domain adaption?",0,1,False,self,,,,,
851,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,8,b0t1ry,self.MachineLearning,How to modify Adaline Stochastic gradient descent,https://www.reddit.com/r/MachineLearning/comments/b0t1ry/how_to_modify_adaline_stochastic_gradient_descent/,vokoyo,1552519871,[removed],0,1,False,self,,,,,
852,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,10,b0ue64,self.MachineLearning,How do I select features for my linear regression model?,https://www.reddit.com/r/MachineLearning/comments/b0ue64/how_do_i_select_features_for_my_linear_regression/,ScoobyDataDoo,1552527525,[removed],0,1,False,self,,,,,
853,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,11,b0uvv2,self.MachineLearning,[D] The Economist article about DeepMind's Culture,https://www.reddit.com/r/MachineLearning/comments/b0uvv2/d_the_economist_article_about_deepminds_culture/,milaworld,1552530587,"The Economist's 1843 magazine wrote an article about DeepMind and Demis Hassabis called [DeepMind and Google: the battle to control artificial intelligence](https://www.1843magazine.com/features/deepmind-and-google-the-battle-to-control-artificial-intelligence). I found the article interesting because they interviewed two dozen insiders to get a glimpse of their internal research culture, especially the part about how Reinforcement Learning and chasing a reward function was so fundamental to their research ideology, it is also ingrained into their corporate culture:

*Current and former researchers at DeepMind and Google, who requested anonymity due to stringent non-disclosure agreements, have also expressed scepticism that DeepMind can reach AGI through such methods. To these individuals, the focus on achieving high performance within simulated environments makes the reward-signal problem hard to tackle. Yet this approach is at the heart of DeepMind. It has an internal leaderboard, in which programs from competing teams of coders vie for mastery over virtual domains.*

I haven't worked at DeepMind before but have heard from peers who interned there about this internal leaderboard. Would be curious to hear more about how it works if you have any insights.

https://www.1843magazine.com/features/deepmind-and-google-the-battle-to-control-artificial-intelligence
",54,97,False,self,,,,,
854,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,12,b0v70w,self.MachineLearning,US A.I. Executive Order,https://www.reddit.com/r/MachineLearning/comments/b0v70w/us_ai_executive_order/,stopaisabotage,1552532589,"""[Discussion]"" How do non-Americans feel working for US-A.I. companies, now that Trumps Executive Order pretty much states that he doesnt want other countries to get more advanced in any areas of A.I.?
Sounds like the EU etc. is included.
https://www.whitehouse.gov/presidential-actions/executive-order-maintaining-american-leadership-artificial-intelligence/
https://www.whitehouse.gov/articles/accelerating-americas-leadership-in-artificial-intelligence/",0,1,False,self,,,,,
855,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,12,b0vdli,self.MachineLearning,"[P] A relaxing game using ML, Need feedback :)",https://www.reddit.com/r/MachineLearning/comments/b0vdli/p_a_relaxing_game_using_ml_need_feedback/,jcheng91,1552533745,[removed],0,1,False,self,,,,,
856,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,12,b0ve8g,self.MachineLearning,"""[Discussion]"" US A.I. Executive Order",https://www.reddit.com/r/MachineLearning/comments/b0ve8g/discussion_us_ai_executive_order/,stopaisabotage,1552533851,"How do non-Americans feel working for US-A.I. companies, now that Trumps Executive Order pretty much states that he doesnt want other countries to get more advanced in any areas of A.I.?

Sounds like the EU is addressed, too. As ""strategic competitor"".
Has anyone seen any manifestations of this?

https://www.whitehouse.gov/presidential-actions/executive-order-maintaining-american-leadership-artificial-intelligence/ 

https://www.whitehouse.gov/articles/accelerating-americas-leadership-in-artificial-intelligence/",18,8,False,self,,,,,
857,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,12,b0vlmf,analyticsinsight.net,Top 5 Machine Learning Solutions in 2019 | Analytics Insight,https://www.reddit.com/r/MachineLearning/comments/b0vlmf/top_5_machine_learning_solutions_in_2019/,analyticsinsight,1552535145,,0,1,False,default,,,,,
858,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,12,b0vo60,self.MachineLearning,Key value pair extraction from PDF documents,https://www.reddit.com/r/MachineLearning/comments/b0vo60/key_value_pair_extraction_from_pdf_documents/,Testher75,1552535615,[removed],0,1,False,self,,,,,
859,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,13,b0vxd5,arxiv.org,Deep Multi-Agent Reinforcement Learning with Discrete-Continuous Hybrid Action Spaces,https://www.reddit.com/r/MachineLearning/comments/b0vxd5/deep_multiagent_reinforcement_learning_with/,Fgnb123,1552537326,,1,1,False,default,,,,,
860,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,14,b0wg53,self.MachineLearning,[R] Join our live stream on discussion of the original style transfer paper A Neural Algorithm of Artistic Style,https://www.reddit.com/r/MachineLearning/comments/b0wg53/r_join_our_live_stream_on_discussion_of_the/,tdls_to,1552541130,"Please post your comments here or in the live stream chat. Some discussion points:

1. Can we separate content/style for data other than images/videos. such as voice, transaction, etc?
2. Demystifying Neural Style Transfer (arXiv:1701.01036 \[[cs.CV](https://cs.CV)\]): can we reformulate Gram Matrix to have more flexibility?
3. Why does VGG work better than other architectures for style transfer, such as Inception?

Details: [https://tdls.a-i.science/events/2019-03-14/](https://tdls.a-i.science/events/2019-03-14/)

Time: March 11 2019 06:30 pm EDT.",1,7,False,self,,,,,
861,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,14,b0wmpx,medium.com,A python package to extract features from X-ray images.,https://www.reddit.com/r/MachineLearning/comments/b0wmpx/a_python_package_to_extract_features_from_xray/,thevatsalsaglani,1552542519,,0,1,False,https://b.thumbs.redditmedia.com/rDFJlGeG58KJGc_jWBzVBuYp2cp0PmQrFP2Pz_OVjKI.jpg,,,,,
862,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,14,b0wp2d,self.MachineLearning,[D] How to detect paragraphs with less line spaces in document images?,https://www.reddit.com/r/MachineLearning/comments/b0wp2d/d_how_to_detect_paragraphs_with_less_line_spaces/,DGs29,1552543019,"I have asked a similar [previous question](https://www.reddit.com/r/MachineLearning/comments/axy95y/d_how_to_detect_text_blocks_in_document_images/). I can now able to detect blocks of text. But when it comes to paragraphs with less line spaces my method doesn't detect properly. 

This is how I wanted to detect:

&amp;#x200B;

https://i.redd.it/9v3itc78w0m21.png

But this is what I can achieve:

&amp;#x200B;

https://i.redd.it/dhtwfjdcw0m21.jpg

How I did:

1. First I placed bounding boxes over individual characters.
2. Next, I binarized and inverted colors for the image.
3. Apply dilation.
4. Then, I placed bounding box to that dilated image.

This method works well for paragraphs which are placed at a decent line-spaces. But when they're close to each other, it doesn't segment the paragraphs separately.

What are the necessary changes to be made to achieve my result?",2,0,False,https://b.thumbs.redditmedia.com/KWOzYlu7AjWW6gvd0Tdw4oBApS1fifihmCnjj9mugCA.jpg,,,,,
863,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,15,b0x0o1,self.MachineLearning,Present-Day Mobile Applications and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b0x0o1/presentday_mobile_applications_and_machine/,appsbee,1552545518,[removed],0,1,False,self,,,,,
864,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,15,b0x2uj,self.MachineLearning,LHD is a leading manufacturer for load handling devices! Telescopic Fork,https://www.reddit.com/r/MachineLearning/comments/b0x2uj/lhd_is_a_leading_manufacturer_for_load_handling/,lhd121,1552545982,[removed],0,1,False,https://a.thumbs.redditmedia.com/bcJaaZgPthRV6oXlKgcOmJUxbkNSOdYNAxw4wH93gZ0.jpg,,,,,
865,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,15,b0x391,self.MachineLearning,Tflearn meta and index file,https://www.reddit.com/r/MachineLearning/comments/b0x391/tflearn_meta_and_index_file/,james31082,1552546068,[removed],0,1,False,self,,,,,
866,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,15,b0x533,self.MachineLearning,"As of day, which technique \ open source text to audio libraries do we have out there ?",https://www.reddit.com/r/MachineLearning/comments/b0x533/as_of_day_which_technique_open_source_text_to/,pointless-ai,1552546480,[removed],0,1,False,self,,,,,
867,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,16,b0x7nq,youtu.be,Machine Learning Tutorial Part 6 | Knn(Friend Recommender) For Beginners,https://www.reddit.com/r/MachineLearning/comments/b0x7nq/machine_learning_tutorial_part_6_knnfriend/,SquareTechAcademy,1552547035,,0,1,False,https://b.thumbs.redditmedia.com/2NcOCS-mD3Acla3cWr_jpLNOYOiBViPY9eaqLzwuGpU.jpg,,,,,
868,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,16,b0xl9o,self.MachineLearning,How do I install Keras version 0.1.0 with Theano as the backend on Ubuntu (in 2019)?,https://www.reddit.com/r/MachineLearning/comments/b0xl9o/how_do_i_install_keras_version_010_with_theano_as/,amit_js,1552550143,"    pip install -i https://test.pypi.org/simple/ Keras==0.1.0

But I'm getting error:

    Looking in indexes: https://test.pypi.org/simple/ Collecting Keras==0.1.0 Using cached https://test-files.pythonhosted.org/packages/c4/08/7f1c6bfaa86e69ccadadc552d6309f1685d779a93047d0a7c317b26a321e/Keras-0.1.0.tar.gz     Complete output from command python setup.py egg_info: Traceback (most recent call last): File ""&lt;string&gt;"", line 1, in &lt;module&gt; File ""/tmp/pip-install-O0DpNQ/Keras/setup.py"", line 7, in &lt;module&gt;         long_description = open('README.md').read(), IOError: [Errno 2] No such file or directory: 'README.md' 

Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-install-O0DpNQ/Keras/",0,1,False,self,,,,,
869,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,16,b0xlkc,analyticsinsight.net,Top 5 Machine Learning Solutions in 2019 | Analytics Insight,https://www.reddit.com/r/MachineLearning/comments/b0xlkc/top_5_machine_learning_solutions_in_2019/,analyticsinsight,1552550214,,0,1,False,https://b.thumbs.redditmedia.com/K2f7PhzoX7bIsGsk2qEjtu0sB268ZNFJFYq1Rs6cqnE.jpg,,,,,
870,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,17,b0xsdv,self.learnmachinelearning,Create a dataset class for own image dataset - Like MNIST example in Tensorflow,https://www.reddit.com/r/MachineLearning/comments/b0xsdv/create_a_dataset_class_for_own_image_dataset_like/,FreddyShrimp,1552551827,,0,1,False,default,,,,,
871,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,18,b0y8z8,cogitotech.com,Model Predictions are used to Increase Data Labeling Speed and Accuracy,https://www.reddit.com/r/MachineLearning/comments/b0y8z8/model_predictions_are_used_to_increase_data/,trainingdata,1552555866,,0,1,False,https://b.thumbs.redditmedia.com/WbOHNNMjCucIVpRq0qT0I6iRtLULayZA0JOcsZD9DgU.jpg,,,,,
872,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,18,b0ybtr,self.MachineLearning,[N] Awesome papers and reviews [with codes!] on Computer Vision News of March. Links for free reading!,https://www.reddit.com/r/MachineLearning/comments/b0ybtr/n_awesome_papers_and_reviews_with_codes_on/,Gletta,1552556509,"Here are the links to the March 2019 issue of Computer Vision News, the magazine of the algorithm community published by RSIP Vision: many articles about Artificial Intelligence, Deep Learning, Computer Vision and more.

Free subscription on page 34.

[HTML5 version (recommended)](https://www.rsipvision.com/ComputerVisionNews-2019March/)

[PDF version](https://www.rsipvision.com/computer-vision-news-2019-march-pdf/)

Enjoy!

&amp;#x200B;

https://i.redd.it/y12zz5yx12m21.jpg

&amp;#x200B;",0,4,False,https://a.thumbs.redditmedia.com/k52XWBaO8g9PrrLNHEJCY1SbGWZV4aro9c4uZSR2Aj8.jpg,,,,,
873,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,19,b0ym94,self.MachineLearning,Generating Fake Conversations by fine-tuning OpenAI's GPT-2 on data from Facebook Messenger,https://www.reddit.com/r/MachineLearning/comments/b0ym94/generating_fake_conversations_by_finetuning/,Tenoke,1552558757,[removed],0,1,False,self,,,,,
874,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,19,b0yprl,self.MachineLearning,[P] Generating Fake Conversations by fine-tuning OpenAI's GPT-2 on data from Facebook Messenge,https://www.reddit.com/r/MachineLearning/comments/b0yprl/p_generating_fake_conversations_by_finetuning/,Tenoke,1552559503,"Playing around with fine-tuning OpenAI's GPT-2 on my own Facebook Messenger data was surprisingly fun so I wrote a [blog post](https://svilentodorov.xyz/blog/gpt-finetune) and added a collab where others can try it, too.",0,1,False,self,,,,,
875,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,19,b0yqno,self.MachineLearning,benchmarking neural networks,https://www.reddit.com/r/MachineLearning/comments/b0yqno/benchmarking_neural_networks/,venkateshkarthick,1552559689,[removed],0,1,False,self,,,,,
876,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,19,b0ytij,self.MachineLearning,Generating Fake Conversations by fine-tuning OpenAI's GPT-2 on data from Facebook Messenger,https://www.reddit.com/r/MachineLearning/comments/b0ytij/generating_fake_conversations_by_finetuning/,Tenoke,1552560300,"Playing around with fine-tuning OpenAI's GPT-2 on my own Facebook Messenger data was surprisingly fun so I wrote a [blog post](https://svilentodorov.xyz/blog/gpt-finetune) and added a collab where others can try it, too.",0,1,False,self,,,,,
877,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,19,b0yvbf,self.MachineLearning,[D] Classifier performance and the interpretation of ML in neuroimaging,https://www.reddit.com/r/MachineLearning/comments/b0yvbf/d_classifier_performance_and_the_interpretation/,byoushashite,1552560671,"Neuroscience person here, looking for some discussion on the use and interpretation of ML analyses in neuroimaging. 

As ML is gaining popularity in the neuroimaging literature, classifiers like SVMs are increasingly being used to classify fMRI data, which is commonly referred to as fMRI decoding in the literature. A common practice here seems to be to determine whether the classifier accuracy differs significantly from chance (e.g. 50% in the case of two conditions), and to conclude that brain activity can be ""successfully decoded"" if p &lt; 0.05, regardless of what the actual classifier performance is. 

What worries me a bit is that I've seen numerous papers with maximum accuracy levels of say, 55% which is barely above chance level, but they take this to be sufficient to declare that brain activity has been decoded successfully. I'm not exactly an expert on ML, so I wonder what people with more of a background in machine learning have to say about this practice, and whether my worry is justified. Does it make sense to accept ""significantly different from chance"" as the sole indicator of whether the analysis has been successful? Is it reasonable to draw any strong conceptual conclusion from such results, even if the classifier performance seems very low prima facie?

(Posting this here because I thought this was more of a discussion topic than just a question, apologies if the post isn't appropriate for this sub. Let me know if this is more appropriate for r/MLQuestions, then I'll post there instead).",9,4,False,self,,,,,
878,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,19,b0yvxa,self.MachineLearning,VAE for Text,https://www.reddit.com/r/MachineLearning/comments/b0yvxa/vae_for_text/,maayan_artzi,1552560791,[removed],0,1,False,self,,,,,
879,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,19,b0ywsy,self.MachineLearning,[P] Generating Fake Conversations by fine-tuning OpenAI's GPT-2 on data from Facebook Messenger,https://www.reddit.com/r/MachineLearning/comments/b0ywsy/p_generating_fake_conversations_by_finetuning/,Tenoke,1552560968,"Playing around with fine-tuning OpenAI's GPT-2 on my own Facebook Messenger data was surprisingly fun so I wrote a [blog post](https://svilentodorov.xyz/blog/gpt-finetune) and added a Collab where others can try it, too.",14,57,False,self,,,,,
880,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,21,b0zjt6,self.MachineLearning,Machine Learning in a nutshell,https://www.reddit.com/r/MachineLearning/comments/b0zjt6/machine_learning_in_a_nutshell/,Revanthmk23200,1552565293,[removed],0,1,False,self,,,,,
881,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,21,b0zoer,self.MachineLearning,Variable importance for logistic regression,https://www.reddit.com/r/MachineLearning/comments/b0zoer/variable_importance_for_logistic_regression/,ankitred0593,1552566106,[removed],0,1,False,self,,,,,
882,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,21,b0zukp,arxiv.org,1903.02831v1] Predicting Research Trends From Arxiv,https://www.reddit.com/r/MachineLearning/comments/b0zukp/190302831v1_predicting_research_trends_from_arxiv/,whenmaster,1552567183,,4,16,False,default,,,,,
883,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,22,b105f9,self.MachineLearning,Transitioning to applied ML research,https://www.reddit.com/r/MachineLearning/comments/b105f9/transitioning_to_applied_ml_research/,deepnetter101,1552568975,[removed],0,1,False,self,,,,,
884,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,22,b10c3a,self.MachineLearning,I've put a lot of effort into this. Take a look.,https://www.reddit.com/r/MachineLearning/comments/b10c3a/ive_put_a_lot_of_effort_into_this_take_a_look/,clone290595,1552570030,[removed],0,0,False,self,,,,,
885,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,22,b10dsi,self.MachineLearning,[Question] VAE for Text,https://www.reddit.com/r/MachineLearning/comments/b10dsi/question_vae_for_text/,maayan_artzi,1552570304,[removed],0,1,False,self,,,,,
886,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,22,b10e87,self.MachineLearning,"What are some of the best Computer Vision/Machine Learning research labs or companies for a BSc/MSc student to intern at in Hamburg, Germany?",https://www.reddit.com/r/MachineLearning/comments/b10e87/what_are_some_of_the_best_computer_visionmachine/,roxanams,1552570376,[removed],0,1,False,self,,,,,
887,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,22,b10ga0,self.MachineLearning,[D] How to best spend a summer doing University Research to maximize my chances of getting into a good PHD program?,https://www.reddit.com/r/MachineLearning/comments/b10ga0/d_how_to_best_spend_a_summer_doing_university/,AndrewBaker33321122,1552570716,"I plan on stay during summer to do research in my university. My plan is to apply to Cambrige Phd program in AI, and I was thinking because it's very competitive, that I should probably see how I can use the summer to increase the chances of my application being successful. I'm not quiet sure how to do that though - are there perhaps professors better or worse to work with (regardless, of course, that they need to work on an interesting research topic)? How do I judge that? Should I aim to publish a paper? And so on.

&amp;#x200B;

Thank you",3,0,False,self,,,,,
888,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,22,b10hmx,self.MachineLearning,[D]: Hyperopt vs. Tune vs. Ray,https://www.reddit.com/r/MachineLearning/comments/b10hmx/d_hyperopt_vs_tune_vs_ray/,gokstudio,1552570940,"I'm looking at python libraries for hparam tuning and optimization for Deep Learning. Hyperopt, tune and ray all seem to be 

good libraries for doing this but its evident what the pros and cons for each are. 

&amp;#x200B;

I'll be running my jobs on a cluster with a LSF  scheduler. I could run a process on the local machine  that submits jobs to the cluster for different configs but not sure how one would do it with the above. So, bonus pts if any of the above can deal with LSF and the described workflow.

  
Has anyone tried out these libraries and what was your experience with these?",8,5,False,self,,,,,
889,MachineLearning,t5_2r3gv,2019-3-14,2019,3,14,22,b10i0l,self.MachineLearning,[Discussion]VAE for Text,https://www.reddit.com/r/MachineLearning/comments/b10i0l/discussionvae_for_text/,maayan_artzi,1552571003,"Hi guys, I have a question regarding the way papers evaluate their VAE performance over text dataset.

In most papers I know dealing with VAE for text, (e.g., The original [Generating Sentences from a continuous space](https://aclweb.org/anthology/K/K16/K16-1002.pdf), and Sasha Rush's VAE related papers [1](https://arxiv.org/pdf/1807.04863.pdf), [2](https://arxiv.org/pdf/1802.02550.pdf), [3](https://arxiv.org/pdf/1901.10548.pdf), etc) the way to evaluate if a new model is ""better"" then previous models is with respect to two metrics: Perplexity, or NLL (and by that checking its LM abilities) and KLD (wrt the prior).

&amp;#x200B;

My question is this:

What is our goal? Minimize perplexity and have a non-(close to)zero KLD? Maybe we want a high value KLD? If so, why? Why do we care KLD is or big, or non-zero.

We don't want KLD to be non zero for posterior collapse reasons, but if it collapses, our decoder collapses to a ""Regular"" LM, and we want to do a better job than this.

And on the other hand, if we want our KLD to be big, we can just choose beta = 0, where we multiply KLD with beta (as suggested in Bowman's annealing method). This will result with a really high KLD, and our Perplexity should be better than ""regular"" LM as we pass it additional information (the non-gaussian latent).

&amp;#x200B;

So, I'm not sure what we want to minimize/maximize and what's the motivation.

It looks like I'm missing something, would love your explanation.

Thanks",4,5,False,self,,,,,
890,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,0,b11e0g,self.MachineLearning,Multiclass classification where one class' F1-score is terrible,https://www.reddit.com/r/MachineLearning/comments/b11e0g/multiclass_classification_where_one_class_f1score/,Woodbear_,1552575897,[removed],0,1,False,self,,,,,
891,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,0,b11f58,medium.com,New Google Program Applies Technical Writers to Open Source Documentation,https://www.reddit.com/r/MachineLearning/comments/b11f58/new_google_program_applies_technical_writers_to/,gwen0927,1552576063,,0,1,False,https://a.thumbs.redditmedia.com/GQhTCurcagr3t53Yx2rP-yU0tcPEHxsS1eyTnU3Reu4.jpg,,,,,
892,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,0,b11gky,arxiv.org,[1903.00812] 3D Hand Shape and Pose Estimation from a Single RGB Image,https://www.reddit.com/r/MachineLearning/comments/b11gky/190300812_3d_hand_shape_and_pose_estimation_from/,madenmud,1552576264,,5,14,False,default,,,,,
893,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,1,b126mh,self.MachineLearning,Is (parametrized)leaky ReLu that much better than ReLu?,https://www.reddit.com/r/MachineLearning/comments/b126mh/is_parametrizedleaky_relu_that_much_better_than/,qudcjf7928,1552579917,[removed],0,1,False,self,,,,,
894,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,1,b12fmz,crate.io,"Doing Data Science for the Common Good, Part 1",https://www.reddit.com/r/MachineLearning/comments/b12fmz/doing_data_science_for_the_common_good_part_1/,nachrieb,1552581132,,0,1,False,default,,,,,
895,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,1,b12oji,self.MachineLearning,any special plans for today? - Celebrating 3/14 w/ Coding &amp; Logic games on all day Twitch Stream,https://www.reddit.com/r/MachineLearning/comments/b12oji/any_special_plans_for_today_celebrating_314_w/,ecstasylover,1552582323,"anybody doing anything special for Pi day? been meaning to play this game so I decided to stream a bit for the occasion and would love to chat about math, games, machine learning or whatever in general. after while True: learn() I have a few options and would also appreciate suggestions!

while True: learn() - NEW STREAM STARTING SOON (0930 MST) - BREAK THE GAME EDITION - early morning coffee chillin' &amp; codin' ft. Dr. Yay [https://www.twitch.tv/taichi\_cigarettes ](https://t.co/rWA49zsoZo) [#**PiDay**](https://twitter.com/hashtag/PiDay?src=hash) [#**Math**](https://twitter.com/hashtag/Math?src=hash)[#**Code**](https://twitter.com/hashtag/Code?src=hash) [#**LearnToCode**](https://twitter.com/hashtag/LearnToCode?src=hash)[#**314Day**](https://twitter.com/hashtag/314Day?src=hash)

above post from twitter (estasylover) please give me a follow (will follow back just hmu) i plan to try to stream for as much as today as i possibly can to try to get some followers. got some other math related games too for l8r. if you watch me on twitch let me know. would love some feedback or whatever

[https://www.twitch.tv/taichi\_cigarettes](https://www.twitch.tv/taichi_cigarettes)",0,1,False,self,,,,,
896,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,2,b133qf,self.MachineLearning,[R] Intro To Deep Learning: Taught by a 14-Year-Old,https://www.reddit.com/r/MachineLearning/comments/b133qf/r_intro_to_deep_learning_taught_by_a_14yearold/,jakemalis,1552584301,"Hey all!

&amp;#x200B;

For the past few months I have been researching Deep Learning, Machine Learning, and AI and have realized there is no one good explanation as to the difference between them, how Deep Learning actually works, and what the consequences of mass Deep Learning is. That is why I decided to write my own article/research paper talking about all of the above.

Please feel free to check it out and give it a clap! Hope you enjoy.

[https://medium.com/@jakemalis/intro-to-deep-learning-taught-by-a-14-year-old-6c49fc94d66](https://medium.com/@jakemalis/intro-to-deep-learning-taught-by-a-14-year-old-6c49fc94d66)",2,0,False,self,,,,,
897,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,2,b139d7,self.MachineLearning,Can you apply bias term after the linear activation?,https://www.reddit.com/r/MachineLearning/comments/b139d7/can_you_apply_bias_term_after_the_linear/,qudcjf7928,1552585070,"for regression purposes, let's say, it's advised to use a linear activation for the output node 

&amp;#x200B;

such that it is Output = a\*(Wx) for some trainable weights ""a"" and weight matrix ""W""  

&amp;#x200B;

but if the bias term is ""b"", can you apply the bias term after the linear activation? 

&amp;#x200B;

i.e. instead of Output = a\*(Wx + b) , you do Output = a\*(Wx) + b ? ",0,1,False,self,,,,,
898,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,2,b13drz,towardsdatascience.com,Checklist for debugging neural networks,https://www.reddit.com/r/MachineLearning/comments/b13drz/checklist_for_debugging_neural_networks/,ceceshao1,1552585672,,0,1,False,default,,,,,
899,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,2,b13enf,towardsdatascience.com,[N] Checklist for debugging neural networks,https://www.reddit.com/r/MachineLearning/comments/b13enf/n_checklist_for_debugging_neural_networks/,ceceshao1,1552585789,,0,2,False,https://b.thumbs.redditmedia.com/cOrgBROG8BF-z14DoH4MgxJp0rc-oLRvtqHGjyC2fQo.jpg,,,,,
900,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,2,b13fj2,self.MachineLearning,Using neural nets to predict tomorrows electric consumption,https://www.reddit.com/r/MachineLearning/comments/b13fj2/using_neural_nets_to_predict_tomorrows_electric/,WriteShortSentences,1552585911,[removed],0,1,False,self,,,,,
901,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,3,b13jzw,self.MachineLearning,[D] What frameworks are ready for embeded development?,https://www.reddit.com/r/MachineLearning/comments/b13jzw/d_what_frameworks_are_ready_for_embeded/,Valiox,1552586527,,0,1,False,self,,,,,
902,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,3,b13ncq,medium.com,[N] China to Overtake US in Most Cited 50% of AI Research Papers,https://www.reddit.com/r/MachineLearning/comments/b13ncq/n_china_to_overtake_us_in_most_cited_50_of_ai/,Mandelmus100,1552586992,,0,1,False,default,,,,,
903,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,3,b13t1c,github.com,[P] neuroptica - a nanophotonic neural network simulation platform,https://www.reddit.com/r/MachineLearning/comments/b13t1c/p_neuroptica_a_nanophotonic_neural_network/,bencbartlett,1552587781,,0,1,False,default,,,,,
904,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,3,b13yz6,self.MachineLearning,[P] neuroptica - a nanophotonic neural network simulation platform,https://www.reddit.com/r/MachineLearning/comments/b13yz6/p_neuroptica_a_nanophotonic_neural_network/,bencbartlett,1552588606,"[Nanophotonic neural networks](https://www.nature.com/articles/nphoton.2017.93) are an exciting emerging sub-field in physics and machine learning, promising low-energy, ultra high-throughput machine learning systems implemented purely optically. Our lab recently published a [paper](https://arxiv.org/abs/1903.04579) describing a method for physically-implementable optical activation functions for use in nanophotonic NN's, and we've open-sourced the simulator we developed as part of our research.

GitHub repository: [https://github.com/fancompute/neuroptica](https://github.com/fancompute/neuroptica) ",32,189,False,self,,,,,
905,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,3,b141r3,technologyreview.com,The man who helped invent virtual assistants thinks theyre doomed without a new AI approach,https://www.reddit.com/r/MachineLearning/comments/b141r3/the_man_who_helped_invent_virtual_assistants/,analyticalmonk,1552588992,,0,1,False,https://b.thumbs.redditmedia.com/_v2UOeD1o9SxEGICLz2w3XkChy661sCEvYTH4bIZgEI.jpg,,,,,
906,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,3,b1466h,i.redd.it,Difference between machine learning and AI,https://www.reddit.com/r/MachineLearning/comments/b1466h/difference_between_machine_learning_and_ai/,ibrxk,1552589615,,0,1,False,default,,,,,
907,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,4,b14g39,self.MachineLearning,[1903.05164] Query Reformulation and Multi-task Learning for Dialogue State Tracking,https://www.reddit.com/r/MachineLearning/comments/b14g39/190305164_query_reformulation_and_multitask/,se4u,1552590965,"[https://arxiv.org/pdf/1903.05164.pdf](https://arxiv.org/pdf/1903.05164.pdf)

&gt;**Abstract:** Successful contextual understanding of multi-turn spoken dialogues requires resolving referring expressions across turns and tracking the entities relevant to the conversation across turns. Tracking conversational state is particularly challenging in a multi-domain scenario when there exist multiple spoken language understanding (SLU) sub-systems, and each SLU sub-system operates on its domain-specific meaning representation. While previous approaches have addressed the disparate schema issue by learning candidate transformations of the meaning representation, in this paper, we instead model the reference resolution as a dialogue context-aware user query reformulation task -- the dialog state is serialized to a sequence of natural language tokens representing the conversation. We develop our model for query reformulation using a pointer-generator network and a novel multi-task learning setup. In our experiments, we show a significant improvement in absolute F1 on an internal as well as a, soon to be released, public benchmark respectively.",0,1,False,self,,,,,
908,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,4,b14gcb,ai.googleblog.com,Harnessing Organizational Knowledge for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b14gcb/harnessing_organizational_knowledge_for_machine/,sjoerdapp,1552591000,,0,1,False,default,,,,,
909,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,4,b14j3x,medium.com,Boeing 737 MAX Crashes Raise Public Distrust of Autonomous Systems,https://www.reddit.com/r/MachineLearning/comments/b14j3x/boeing_737_max_crashes_raise_public_distrust_of/,gwen0927,1552591383,,0,1,False,default,,,,,
910,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,5,b15d7o,self.MachineLearning,"For bilinguals, do any of you read ML papers that are published in languages besides English?",https://www.reddit.com/r/MachineLearning/comments/b15d7o/for_bilinguals_do_any_of_you_read_ml_papers_that/,zanjabil,1552595452,[removed],0,1,False,self,,,,,
911,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,5,b15len,self.MachineLearning,[D] Tips for ML &amp; NLP in production,https://www.reddit.com/r/MachineLearning/comments/b15len/d_tips_for_ml_nlp_in_production/,cslambthrow,1552596572,"I'm starting a new job soon - my first one in ML after having graduated. 

While I'm used to an academic setting, working with my supervisor, within my lab, etc, I'm kind of nervous on how to work in industry, or how to create an efficient pipeline. I'm the only ML engineer as I'm working with a startup, so it makes it slightly trickier, but I also believe more fun as I have a lot of solo exploration to do.

My academic work was in Gaussians and Bayesian statistics. I've now entered NLP for the first time through this job, so that's what I will be doing - but I do have the possibility of also working with more standard statistical ML models if I so choose to and if I find a problem that fits. Primarily NLP though. 

I've done some NLP before, but really basic tensorflow tutorials (IMDB dataset). So, I'm curious from those who transitioned to industry and those who work in NLP... do you have any tips for me? Any do's/don't, and any rough pipelines of how my general work and research should look like?",8,15,False,self,,,,,
912,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,6,b163w8,i.kinja-img.com,aRtiFiCiAL inTElLIgeNce,https://www.reddit.com/r/MachineLearning/comments/b163w8/artificial_intelligence/,baahalex,1552599072,,0,1,False,https://b.thumbs.redditmedia.com/R78Yk04PpRCAy3SGLdglW-Tn3oadyVTrK-7aeELxGyY.jpg,,,,,
913,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,6,b1670o,self.MachineLearning,Machine Learning (Cold Start problem and how to proceed),https://www.reddit.com/r/MachineLearning/comments/b1670o/machine_learning_cold_start_problem_and_how_to/,SquareTechAcademy,1552599515,[removed],0,1,False,self,,,,,
914,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,8,b179cs,self.MachineLearning,[D] The Bitter Lesson,https://www.reddit.com/r/MachineLearning/comments/b179cs/d_the_bitter_lesson/,wei_jok,1552604876,"Recent [diary entry](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) of Rich Sutton:

*The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin....*

What do you think?",80,65,False,self,,,,,
915,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,8,b17gsp,self.MachineLearning,[P] Uncovering latent patterns with DRAW,https://www.reddit.com/r/MachineLearning/comments/b17gsp/p_uncovering_latent_patterns_with_draw/,titanandwire,1552605998,"Hi all, to head this off I figured I'd point out that this question is part of my thesis work for a masters degree in physics. Both me and my advisor are ""fairly"" new to the ML scene (a couple of years worth of experience) so I'm seeking the broader ML community with a question we're not sure about.

### The project

Our project aims to cluster observations based on latent expressions. It's an unsupervised problem where we, for the sake of this post, want to cluster the MNIST dataset based on some latent variable representation. Vanilla or convolutional variational autoencoders were not able to reconstruct the data sufficiently (like the MNIST data our data is pretty sparse on the canvas and with rather sharp edges, it's also noisy but that's beside the point). We then turned to the [DRAW](https://arxiv.org/abs/1502.04623) algorithm. I've attached an image from the paper regarding it's architecture ![DRAW](https://cdn-images-1.medium.com/max/800/1*KgBk06gZ3AV8jqkwMZGbGg.png ""Image taken from the linked paper"")

### Our question

Given the recurrent nature of the algorithm, and the fact that it uses glimpses/attention for the reconstruction can we capture the latent expression? The latent samples themselves seems to not carry the information itself (based on clustering analysis when applying DRAW on simulated data). Is there some part of the literature I've missed? Any thoughts would be very welcome.",3,17,False,self,,,,,
916,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,9,b184tu,blog.codecentric.de,[P] How to systematially organize Machine Learning (ML) model development,https://www.reddit.com/r/MachineLearning/comments/b184tu/p_how_to_systematially_organize_machine_learning/,AnYvia,1552609731,,0,1,False,default,,,,,
917,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,9,b187tz,medium.com,Facebook Releases a Trio of New AI Hardware Designs,https://www.reddit.com/r/MachineLearning/comments/b187tz/facebook_releases_a_trio_of_new_ai_hardware/,Yuqing7,1552610224,,0,1,False,https://b.thumbs.redditmedia.com/vddj2KaszVA1YVZTDALbhNIdkHM0shT4X946HokJzTs.jpg,,,,,
918,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,10,b18mea,self.MachineLearning,[P] BBopt: Black box hyperparameter optimization made easy,https://www.reddit.com/r/MachineLearning/comments/b18mea/p_bbopt_black_box_hyperparameter_optimization/,EvHub,1552612647,"Hello r/machinelearning! I'm [evhub](https://github.com/evhub), the open-source developer behind [Coconut](http://coconut-lang.org/).

Today I'm releasing a new open-source project called [BBopt](https://github.com/evhub/bbopt). BBopt is a black box hyperparameter optimization framework built to be extremely powerful, easy-to-use, and extensible. BBopt arose out of my frustrations working with existing hyperparameter optimization frameworks and wishing for something betterhopefully it can help resolve some of your frustrations as well!

Please [check out BBopt on GitHub](https://github.com/evhub/bbopt) and let me know what you think!

_Cross-posted to r/datascience, r/coolgithubprojects, and r/programmingprojects._",8,30,False,self,,,,,
919,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,10,b18tjm,self.MachineLearning,[D] ML Course: Theory/Math be. Application,https://www.reddit.com/r/MachineLearning/comments/b18tjm/d_ml_course_theorymath_be_application/,Napoleon-1804,1552613898,"Our university has a few options for ML Courses. One is a statistics department course that is purely rigorous math. The other is from the data science department and is nearly all application (I think even a few social science people take it since not much math). Finally is a computer science department that is a hybrid.

Is it more important to learn the math but skimp out on the application of ML to data science or is it better to learn how to use ML in projects but not know the math? Obviously both is best, but if I had to choose one? Thanks!",3,1,False,self,,,,,
920,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,11,b198b3,self.MachineLearning,Pluralistic Image Completion,https://www.reddit.com/r/MachineLearning/comments/b198b3/pluralistic_image_completion/,lyndonzheng,1552616441,[removed],0,1,False,self,,,,,
921,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,11,b19dsg,anchor.fm,3 mistakes to avoid in a Machine Learning Project,https://www.reddit.com/r/MachineLearning/comments/b19dsg/3_mistakes_to_avoid_in_a_machine_learning_project/,centaurus01,1552617366,,0,1,False,default,,,,,
922,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,11,b19lev,self.MachineLearning,[R]Pluralistic Image Completion,https://www.reddit.com/r/MachineLearning/comments/b19lev/rpluralistic_image_completion/,lyndonzheng,1552618683,"&amp;#x200B;

*Processing img t5vg2mwb67m21...*

arXiv: [https://arxiv.org/abs/1903.04227](https://arxiv.org/abs/1903.04227)

Code: [https://github.com/lyndonzheng/Pluralistic-Inpainting](https://github.com/lyndonzheng/Pluralistic-Inpainting)

Interactive Demo: [http://www.chuanxiaz.com/project/pluralistic/](http://www.chuanxiaz.com/project/pluralistic/)",15,29,False,self,,,,,
923,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,12,b19ogi,self.MachineLearning,A very simple interview question that I was asked,https://www.reddit.com/r/MachineLearning/comments/b19ogi/a_very_simple_interview_question_that_i_was_asked/,perceptron333,1552619195,[removed],0,1,False,self,,,,,
924,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,12,b1a5ry,self.MachineLearning,connection error,https://www.reddit.com/r/MachineLearning/comments/b1a5ry/connection_error/,helloavani,1552622326,[removed],0,1,False,self,,,,,
925,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,13,b1ahhq,self.MachineLearning,[D] How do I texts blocks of text from scanned documents,https://www.reddit.com/r/MachineLearning/comments/b1ahhq/d_how_do_i_texts_blocks_of_text_from_scanned/,DGs29,1552624525,"Here's what I mean:

&amp;#x200B;

[ ](https://i.redd.it/dv6oow3gn7m21.png)

I've tried to do image processing in Opencv to achieve this:

1. I plotted character level bounding box
2. Next, I gray-scaled the image and  binarized it.
3. Applied dilation
4. And finally placed bbox over the dilated image.

This is what I can get:

&amp;#x200B;

https://i.redd.it/ur7wm12sn7m21.jpg

Here the image is scanned and moreover the line-spaces between the paragraphs are small. So I couldn't able to segment them based on the paragraphs.

&amp;#x200B;

How can I achieve my desired result. What are the steps to follow?

&amp;#x200B;",6,7,False,self,,,,,
926,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,13,b1ai3b,self.MachineLearning,SOD-MTGAN: Small Object Detection via Multi-Task Generative Adversarial Network,https://www.reddit.com/r/MachineLearning/comments/b1ai3b/sodmtgan_small_object_detection_via_multitask/,duynn912,1552624638,[removed],0,1,False,self,,,,,
927,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,13,b1ajdr,self.MachineLearning,How Alpha Star supervised learning works?,https://www.reddit.com/r/MachineLearning/comments/b1ajdr/how_alpha_star_supervised_learning_works/,perecastor,1552624890,[removed],0,1,False,self,,,,,
928,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,14,b1apgw,self.MachineLearning,What is different between OpenAI GPT and ELMo?,https://www.reddit.com/r/MachineLearning/comments/b1apgw/what_is_different_between_openai_gpt_and_elmo/,thisisiron,1552626104,[removed],0,1,False,self,,,,,
929,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,14,b1avsm,self.MachineLearning,SOD-MTGAN: Small Object Detection via Multi-Task Generative Adversarial Network,https://www.reddit.com/r/MachineLearning/comments/b1avsm/sodmtgan_small_object_detection_via_multitask/,duynn912,1552627324,"SOD-MTGAN: Small Object Detection via Multi-Task Generative Adversarial Network

When I read this paper I was confused and have some questions as follows:

1/ I wonder what high resolution images (real data) and low resolution images(fake data) are. The fake data is generated by downsampling the real data using bicubic interpolation with a factor 4 or the fake data is positive and negative areas cropped from an input image. If the fake data is cropped areas so how I can get the high resolution images (real data).

2/ what is the purpose of cropping background areas from the input image? After using the off-the-shelf detectors, we have interest objects and the question here is that the detectors miss objects of interest in some areas of background and we crop these area to increase the performance of the method? and the cropped areas are randomly chosen or picked according to a particular criteria.",0,1,False,self,,,,,
930,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,14,b1ayp7,self.MachineLearning,Decision Tree over Random Forests?,https://www.reddit.com/r/MachineLearning/comments/b1ayp7/decision_tree_over_random_forests/,Jusaa,1552627913,"So while learning about ML algorithms, I have come across decision trees and random forests. As I understand it, a random forest is just a collection of decision trees with the ""leaves"" averaged to produce one outcome. 

Which leads me into my question, why would you ever use a decision tree over a random forest? With the exception of too little data (overfitting), it seems like a random forest is just always the better decision. ",0,1,False,self,,,,,
931,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,16,b1bq5f,self.MachineLearning,Interesting Problem - Which algorithm/technique is needed?,https://www.reddit.com/r/MachineLearning/comments/b1bq5f/interesting_problem_which_algorithmtechnique_is/,Scorer345,1552633622,"Identifying which algorithm/technique is the best method for me.

First post but have been reading the forums for some time and would appreciate any helpful comments related to this problem!

I have a csv which contains the following
(ID) (a unique number)
(Time) month (integer between 1 and 12)
(A) integer value between 1 and 100
(B)  1 or 2
(C)  integer value between 1 and 10
(D) integer value between 1 and 2
(E)  1,2 or 3

I want to investigate how the variables A,B,C,D affect the value in column E. (For example rank them in terms of order of influence on the outcome of E)

Over time I would like to add more records to this table and include a column to record this, i.e, year.

Then I would like to see how column E changes over time and identify the changes in columns A-D which have resulted in the change of E.

Originally I thought I could do PCA in order to determine the significance of the variables.

But not sure if that technique would allow me to do the latter part of my problem (monitoring the trends)

Please let me know if this question is in the wrong place,

Thanks! ",0,1,False,self,,,,,
932,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,16,b1c2lm,github.com,ML Techniques in Interactive 'PDF' Style Web App,https://www.reddit.com/r/MachineLearning/comments/b1c2lm/ml_techniques_in_interactive_pdf_style_web_app/,OppositeMidnight,1552636410,,0,1,False,https://b.thumbs.redditmedia.com/pzXtvZR_dN9wE2hFv0WM0WkxPz9Pcc27uQ0r3tG9ygM.jpg,,,,,
933,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,17,b1c58o,self.MachineLearning,Some confusion from a first-year doctoral student in computer vision,https://www.reddit.com/r/MachineLearning/comments/b1c58o/some_confusion_from_a_firstyear_doctoral_student/,magicyoung33,1552637010,"Hi, everyone. I am a first-year doctoral student in Computer Vision, and have some confusion about my research direction recently. I hope I can get some help and advice there.

&amp;#x200B;

A couple of months ago,  I was arranged to do research about optical flow, which is a kind of motion representation and can be used in many computer vision tasks.  In these days , I read many papers and did lots of experiment about optical flow about its history, datasets and criterion. Therefore,  I began to believe maybe I could make some contribution on that. Since some previous ideas are proved not working, the professor suggested me to give up the research on optical flow and change to another. 

&amp;#x200B;

Now I am very hesitant about the next plan, there are two reasons. On the one hand,  I am not content to give up just like that, and I think we could do more. On the other hand, I know very little other directions in computer vision, so I am not certain which direction I should choose. For computer vision,  I want to try everything. Althought optical flow is not my favorite one, I can insist  in it. I really want to choose a research direction, and spend several years or the whole phd stage to do some meaningful works.

&amp;#x200B;

What should I do ?  I hope experienced people can give me advice. Thank you!

&amp;#x200B;",0,1,False,self,,,,,
934,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,17,b1c6sn,self.MachineLearning,[D] Is GPT-2 source code publically available?,https://www.reddit.com/r/MachineLearning/comments/b1c6sn/d_is_gpt2_source_code_publically_available/,luv2code2020,1552637360,"I am confused, there are two threads here on /r/MachineLearning/ one which says [https://www.reddit.com/r/MachineLearning/comments/b0ywsy/p_generating_fake_conversations_by_finetuning/](GPT-2 code has been published), and other thread which says [https://www.reddit.com/r/MachineLearning/comments/aqwcyx/discussion_openai_should_now_change_their_name_to/](GPT-2 code is closed and not available)

Is the [https://github.com/openai/gpt-2](GPT-2 on GitHub) the real deal? Or is it missing something?

I have crawled lyrics websites and have 200,000 song lyrics. I was wondering if I put this data through GPT-2 I could generate new song lyrics about any song topic? Any suggestions?",11,9,False,self,,,,,
935,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,17,b1ccrw,i.redd.it,Machine learning Training in Noida | Machine learning Training Institute in Noida,https://www.reddit.com/r/MachineLearning/comments/b1ccrw/machine_learning_training_in_noida_machine/,Divya123divya,1552638755,,0,1,False,https://b.thumbs.redditmedia.com/l0oi5pUGAoYUGp-YqUfdqU6T-4jKRjmylc8YRlyjZAg.jpg,,,,,
936,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,17,b1cel9,theappsolutions.com,Guide to Supervised Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b1cel9/guide_to_supervised_machine_learning/,lady_monsoon,1552639174,,0,1,False,default,,,,,
937,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,17,b1cf5e,self.MachineLearning,Global Machine Learning Artificial intelligence Market Growth (Status and Outlook) 2019-2024,https://www.reddit.com/r/MachineLearning/comments/b1cf5e/global_machine_learning_artificial_intelligence/,surajgowardipe,1552639288,[removed],0,1,False,self,,,,,
938,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,18,b1cln4,youtube.com,Donald Trump Warns Of Scary AI Tech,https://www.reddit.com/r/MachineLearning/comments/b1cln4/donald_trump_warns_of_scary_ai_tech/,hanyuqn,1552640746,,0,1,False,default,,,,,
939,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,18,b1cxu3,medium.com,Snap ML: 2x Faster Machine Learning than Scikit-Learn,https://www.reddit.com/r/MachineLearning/comments/b1cxu3/snap_ml_2x_faster_machine_learning_than/,ibmzrl,1552643404,,0,1,False,default,,,,,
940,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,19,b1db69,self.MachineLearning,GANN to reconstruct missing data in time series ?,https://www.reddit.com/r/MachineLearning/comments/b1db69/gann_to_reconstruct_missing_data_in_time_series/,nicoppido,1552646185,[removed],0,1,False,self,,,,,
941,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,19,b1dc12,incompleteideas.net,The Bitter Lesson [Rich Sutton Blogpost],https://www.reddit.com/r/MachineLearning/comments/b1dc12/the_bitter_lesson_rich_sutton_blogpost/,sidsig,1552646359,,0,1,False,default,,,,,
942,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,19,b1dchd,self.MachineLearning,2060 Strix 6GB or 1070 ti 8 gb for ML?,https://www.reddit.com/r/MachineLearning/comments/b1dchd/2060_strix_6gb_or_1070_ti_8_gb_for_ml/,iamMess,1552646450,[removed],1,1,False,self,,,,,
943,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,20,b1dxvw,self.MachineLearning,BayesSearchCV --- Problem on hyper-parameters tuning,https://www.reddit.com/r/MachineLearning/comments/b1dxvw/bayessearchcv_problem_on_hyperparameters_tuning/,laymounalhoulwa,1552650477,[removed],0,1,False,self,,,,,
944,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,21,b1e5g1,blog.pragmatists.com,Machine learning in the browser with TensorFlow.js,https://www.reddit.com/r/MachineLearning/comments/b1e5g1/machine_learning_in_the_browser_with_tensorflowjs/,wprzecho,1552651810,,0,1,False,default,,,,,
945,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,21,b1e9hi,self.MachineLearning,Microsoft has released ML.net 0.11 which is the last preview release before Release Candidate for V1,https://www.reddit.com/r/MachineLearning/comments/b1e9hi/microsoft_has_released_mlnet_011_which_is_the/,Forser,1552652478,[removed],0,1,False,self,,,,,
946,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,21,b1eau3,blog.pragmatists.com,[N] Machine learning in the browser with TensorFlow.js,https://www.reddit.com/r/MachineLearning/comments/b1eau3/n_machine_learning_in_the_browser_with/,wprzecho,1552652700,,0,1,False,https://b.thumbs.redditmedia.com/XhdCKkN-SACRfbzCVTbMhXEbnonJUfWXH0xcgoKO-hs.jpg,,,,,
947,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,21,b1emhl,self.MachineLearning,Implementation of Optimizers.,https://www.reddit.com/r/MachineLearning/comments/b1emhl/implementation_of_optimizers/,ragingpot,1552654606,[removed],0,1,False,self,,,,,
948,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,22,b1exb2,youtube.com,Visualizing Artificial Intelligence's Training &amp; its Bugged RT Decisions...,https://www.reddit.com/r/MachineLearning/comments/b1exb2/visualizing_artificial_intelligences_training_its/,FinHacksQuants,1552656290,,0,1,False,default,,,,,
949,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,22,b1f6de,i.redd.it,"""5 Reasons to Move to Silicon Valley for a High-paid Data Science Job""  Read and discuss on #ManyStories https://www.mnys.me/SyKKomtPV",https://www.reddit.com/r/MachineLearning/comments/b1f6de/5_reasons_to_move_to_silicon_valley_for_a/,Magniminda,1552657723,,0,1,False,default,,,,,
950,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,23,b1fhh0,mnys.me,"""How to Become a Data Scientist? - Magnimind Academy""  Read and discuss on #ManyStories https://www.mnys.me/rkA5Q4YPN",https://www.reddit.com/r/MachineLearning/comments/b1fhh0/how_to_become_a_data_scientist_magnimind_academy/,Magniminda,1552659417,,0,1,False,default,,,,,
951,MachineLearning,t5_2r3gv,2019-3-15,2019,3,15,23,b1flzg,self.MachineLearning,TextRank - A unsupervised algorithm for Text Summarization,https://www.reddit.com/r/MachineLearning/comments/b1flzg/textrank_a_unsupervised_algorithm_for_text/,prakhar21,1552660091,I have tried to summarize TextRank paper here - [https://prakhartechviz.blogspot.com/2019/03/textrank-bringing-order-to-text.html](https://prakhartechviz.blogspot.com/2019/03/textrank-bringing-order-to-text.html),0,1,False,self,,,,,
952,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,0,b1g8kq,arxiv.org,Deep learning for molecular generation and optimization - a review of the state of the art,https://www.reddit.com/r/MachineLearning/comments/b1g8kq/deep_learning_for_molecular_generation_and/,delton,1552663529,,2,29,False,default,,,,,
953,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,0,b1glnq,self.MachineLearning,SAM Controllers - Self-learning Control Systems for Pumps and Machinery,https://www.reddit.com/r/MachineLearning/comments/b1glnq/sam_controllers_selflearning_control_systems_for/,chrisdoerfler,1552665459,"This is a combination of computing innovation, electrical signature modeling, machine learning, artificial intelligence, edge computing, intelligent sensing, and more.  ",0,1,False,self,,,,,
954,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,1,b1gwyc,i.redd.it,QnA on ML,https://www.reddit.com/r/MachineLearning/comments/b1gwyc/qna_on_ml/,subhamroy021,1552667088,,0,1,False,https://b.thumbs.redditmedia.com/l7s0vA1Psqd6V9UCgR_mKXwaeSGBukTWYEAT7XJJGUA.jpg,,,,,
955,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,1,b1gxov,self.MachineLearning,2 year CS undergrad looking to get into Machine Learning. Where do I begin and what are the best resources? How should I approach learning it?,https://www.reddit.com/r/MachineLearning/comments/b1gxov/2_year_cs_undergrad_looking_to_get_into_machine/,brandtjulie10,1552667185,[removed],0,1,False,self,,,,,
956,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,1,b1h38r,self.MachineLearning,Neural Networks Conceptual Definition,https://www.reddit.com/r/MachineLearning/comments/b1h38r/neural_networks_conceptual_definition/,formatlar,1552667985,[removed],0,1,False,self,,,,,
957,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,1,b1h85w,self.MachineLearning,Window (Frame) Detection,https://www.reddit.com/r/MachineLearning/comments/b1h85w/window_frame_detection/,The__DH,1552668661,[removed],0,1,False,self,,,,,
958,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,2,b1hdex,self.MachineLearning,"[D] To bilinguals, have you read any non-english ML papers you'd care to share with us?",https://www.reddit.com/r/MachineLearning/comments/b1hdex/d_to_bilinguals_have_you_read_any_nonenglish_ml/,zanjabil,1552669397,"I know the ML community is global and there is research being published in Japanese, Russian, Chinese etc. What are we missing out on? Anything cool you've read and would like to share?",77,193,False,self,,,,,
959,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,2,b1hhuc,arxiv.org,[R] Do Neural Networks Show Gestalt Phenomena? An Exploration of the Law of Closure,https://www.reddit.com/r/MachineLearning/comments/b1hhuc/r_do_neural_networks_show_gestalt_phenomena_an/,downtownslim,1552670032,,4,24,False,default,,,,,
960,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,2,b1hob2,self.MachineLearning,Deepmind internship,https://www.reddit.com/r/MachineLearning/comments/b1hob2/deepmind_internship/,throwawaycscq9909,1552670926,Does deepmind ask for your transcript at any point in the internship process? Google asks for it upfront but deepmind didnt - do they just check right before the internship or do they not care at all ,0,1,False,self,,,,,
961,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,2,b1htr8,linkedin.com,How you and I are teaching Artificial Intelligence how to drive.,https://www.reddit.com/r/MachineLearning/comments/b1htr8/how_you_and_i_are_teaching_artificial/,MaximVanMeenen,1552671675,,0,1,False,https://b.thumbs.redditmedia.com/fAZKohRUwTS6PK-VUKNkcgYt6k2cAmEWmwaqyYFV4jQ.jpg,,,,,
962,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,2,b1hv7y,self.MachineLearning,variance decreases with high C parameter in SVM?,https://www.reddit.com/r/MachineLearning/comments/b1hv7y/variance_decreases_with_high_c_parameter_in_svm/,matin987456,1552671885,[removed],0,1,False,self,,,,,
963,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,2,b1i067,self.MachineLearning,"Machine Learning Summer School (MLSS 2019) in Moscow, Russia from the 26th of August to the 6th of September 2019",https://www.reddit.com/r/MachineLearning/comments/b1i067/machine_learning_summer_school_mlss_2019_in/,rodrigorivera,1552672611,[removed],0,1,False,self,,,,,
964,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,3,b1i3ol,self.MachineLearning,[D] Moving Into A MLE Role,https://www.reddit.com/r/MachineLearning/comments/b1i3ol/d_moving_into_a_mle_role/,CircuitBeast,1552673095,"Currently a data analyst (python + sql, no ML), but Id like to set up a study plan to eventually move into a machine learning engineer role in about a year.

Study Plan:
Completed Resources - Andrew Ngs course, Intro to Statistical Learning
Completed Projects -  LSTM project (dont have a deep understanding), Regression project, KMeans clustering

Current Books - Data Science for Business, Applied Predictive Modeling

Future Resources - ESL? Fast.ai pt 2? Comp Sci? Math?

What books/courses do you think are critical to read through to become a competent and employable MLE? ",17,13,False,self,,,,,
965,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,3,b1i8f6,self.MachineLearning,[News] Machine Learning Summer School (MLSS2019) from the 26th of August to the 6th of September 2019 in Moscow,https://www.reddit.com/r/MachineLearning/comments/b1i8f6/news_machine_learning_summer_school_mlss2019_from/,rodrigorivera,1552673760,"We are happy to announce the upcoming **MACHINE LEARNING SUMMER SCHOOL** at the Skolkovo Institute of Science and Technology in Moscow, Russia from the **26th of August to the 6th of September 2019**

[http://mlss2019.skoltech.ru](http://mlss2019.skoltech.ru)

## Overview

The machine learning summer school provides PhD students, faculty, industry professionals and selected Master's students with an intense learning experience on the theory and applications of modern machine learning. Over the course of two weeks, a group of internationally renowned experts will offer lectures and tutorials covering the state of the art in the field.

## Confirmed Speakers

* Arthur Gretton (University College London) - Kernels
* Isabel Valera (Max Planck Institute for Intelligent Systems) - Fairness &amp; Interpretability
* Franois Bachoc (University Paul Sabatier) - Latest Advances in Gaussian Process
* Joris Mooij (University of Amsterdam) - Causality
* Justin Solomon (MIT) - 3D Deep Learning
* Marco Cuturi (CREST-ENSAE) - Optimal Transport
* Mark Girolami (University of Cambridge) - Bayesian Optimization / Probabilistic Numerics
* Matus Telgarsky (University of Illinois at Urbana-Champaign) - Deep Learning Theory
* Michael Bronstein (Imperial College London) - Graph neural network
* Michel Besserve (Max Planck Institute for Intelligent Systems) - ML in Neuroscience
* Nicol Cesa-Bianchi (Universit degli Studi di Milano) - Online Learning
* Shimon Whiteson (University of Oxford) - Reinforcement Learning
* Ulrich Bauer (TU Munich) - Topological Data Analysis
* Yarin Gal (University of Oxford) - Bayesian Deep Learning

## Application process

Applications from graduate students, faculty and members of industry in quantitative fields are welcome. This includes researchers in applied fields as well as students of machine learning itself.Partial and full scholarships will be offered to strong candidates. Accommodation will be offered to the strongest applications.Applicants are asked to submit a CV (max 2 pages), a cover letter (max 500 words), one letter of recommendation (max 500 words).We are also seeking to give participants a chance to discuss their own work. Hence, each applicant is highly encouraged to provide the title and abstract of a poster that they would like to present at the school.

The application system is open now.

For more information visit [http://mlss2019.skoltech.ru](http://mlss2019.skoltech.ru)

## Important Dates

* Mon, 4 March 2019: Application system opens.
* Fri, 6 May 2019 23:59 Pacific Time: Deadline for applications.
* Fri, 12 May 2019 23:59 Pacific Time: Deadline for submission of reference letters.
* Mon, 6 June 2019: Notification of acceptance.
* Fri, 17 June 2019: Registration fees due.
* Mon, 26 August to Fri, 6 September 2019: MLSS takes place.

## Organizers

Evgeny Burnaev, Rodrigo Rivera Castro, Fernando Perez CruzInquiries should be directed toadase(at)skoltech.ru",10,71,False,self,,,,,
966,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,3,b1id11,ai.googleblog.com,Google Faculty Research Awards 2018,https://www.reddit.com/r/MachineLearning/comments/b1id11/google_faculty_research_awards_2018/,sjoerdapp,1552674422,,0,1,False,https://b.thumbs.redditmedia.com/Y5Jfft2D7ml-dN05ZY5BwC8CPSZr9_VxeQ9gdgxBLcI.jpg,,,,,
967,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,3,b1igqz,blog.omeryavuz.gen.tr,Understanding Recurrent Neural Network RNN,https://www.reddit.com/r/MachineLearning/comments/b1igqz/understanding_recurrent_neural_network_rnn/,formatlar,1552674952,,0,1,False,default,,,,,
968,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,5,b1jiqm,self.MachineLearning,recommendation for cheap GPU instance for class project?,https://www.reddit.com/r/MachineLearning/comments/b1jiqm/recommendation_for_cheap_gpu_instance_for_class/,catphive,1552680330,[removed],0,1,False,self,,,,,
969,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,5,b1jsm0,self.MachineLearning,[D] Opinions on TensorFlow 2?,https://www.reddit.com/r/MachineLearning/comments/b1jsm0/d_opinions_on_tensorflow_2/,EmielBoss,1552681795,"Im a complete noob on ML frameworks, and I have a couple of questions about TensorFlow 2. First, why isnt there more discussion about this newest iteration? I thought that TensorFlow was one of the main players, and would therefore expect a recently released alpha to be a big thing. Secondly, what is the general opinion on TensorFlow 2.0? Could someone share their experiences with the 2.0 alpha? Lastly, as someone looking to learn TensorFlow (for my thesis I will be doing CNN work in TensorFlow), should I jump into learning 2.0? If not, is there a list of stuff that will be deprecated in 2.0, so I can keep those in mind when learning 1.x?  
  
I saw Aurlien Gron has a preview the second edition of his Hands-On Machine Learning with TensorFlow book. Is that a recommended read already?

My apologies in case this is considered a beginner question!",33,19,False,self,,,,,
970,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,5,b1jt86,self.MachineLearning,Understanding Recurrent Neural Network RNN,https://www.reddit.com/r/MachineLearning/comments/b1jt86/understanding_recurrent_neural_network_rnn/,formatlar,1552681881,[removed],0,1,False,self,,,,,
971,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,5,b1jy8b,self.MachineLearning,Announcing the release of StellarGraph version 0.6.0 open source machine learning library for geometric deep learning.,https://www.reddit.com/r/MachineLearning/comments/b1jy8b/announcing_the_release_of_stellargraph_version/,YodaML,1552682617,[removed],0,1,False,self,,,,,
972,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,5,b1k1le,self.MachineLearning,[P] Announcing the release of StellarGraph version 0.6.0 open source machine learning library for geometric deep learning.,https://www.reddit.com/r/MachineLearning/comments/b1k1le/p_announcing_the_release_of_stellargraph_version/,YodaML,1552683111,"Hi all,

we would like to announce the release of the newest version, 0.6.0, of StellarGraph, our open source machine learning library for graph-structured data aka geometric deep learning. StellarGraph is a Python 3 library.

The StellarGraph library implements several state-of-the-art algorithms for applying machine learning methods to discover patterns and answer questions using graph-structured data.

In this release we have fixed a number of bugs and introduced new methods including,

* Graph Attention (GAT) layer and model (stack of GAT layers)
* Unsupervised GraphSAGE
* New GraphSAGE aggregators (from version 0.5.0)
* Model ensembles
* Model calibration (from version 0.5.0)
* Node2vec for weighted graphs (from version 0.5.0)
* Added GraphConvolution layer, GCN class for a stack of GraphConvolution layers, and FullBatchNodeGenerator class for feeding data into GCN (from version 0.5.0)
* Community detection based on unsupervised graph representation learning
* Saliency maps and integrated gradients for model interpretability

We provide [examples](https://github.com/stellargraph/stellargraph/tree/master/demos/) of using StellarGraph to solve such tasks using several real-world datasets.

We welcome your feedback and contributions.

Checkout our project on GitHub: [StellarGraph](https://github.com/stellargraph/stellargraph)

Thank you!",3,42,False,self,,,,,
973,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,6,b1ke11,self.MachineLearning,TensorFlow Serving vs PyTorch Hybrid Front End,https://www.reddit.com/r/MachineLearning/comments/b1ke11/tensorflow_serving_vs_pytorch_hybrid_front_end/,MonstarGaming,1552684907,[removed],0,1,False,self,,,,,
974,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,8,b1lk7z,self.MachineLearning,The Evolved Transformer  Enhancing Transformer with Neural Architecture Search,https://www.reddit.com/r/MachineLearning/comments/b1lk7z/the_evolved_transformer_enhancing_transformer/,ranihorev,1552691382,"Hi everybody,

Sharing a new summary I wrote on The Evolved Transformer (ET), a paper by Google Brain with a simple recipe:

1. Take Transformer - the leading architecture for language models and a variety of tasks in NLP.
2. Build a Neural Architecture Search algorithm, an improved version of Tournament Selection, that searches for better models automatically. 
3. Mix with 200 TPUs (Google), initialize the search algorithm with Transformer and after 1B steps youll end up with a better model.   
 

Interestingly, the smaller the ET model the better its performance compared to a Transformer model of the same size. For example, with \~7M parameters (""mobile-friendly"") it outperforms Transformer by 1 perplexity point (7.62 vs 8.62). 

[https://www.lyrn.ai/2019/03/12/the-evolved-transformer](https://www.lyrn.ai/2019/03/12/the-evolved-transformer)

We will probably see this recipe, of human-designed models with automated search algorithms, more and more in the future, as it allows building more complex architectures than human-beings can.",0,1,False,self,,,,,
975,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,8,b1lkkg,self.MachineLearning,[R] The Evolved Transformer  Enhancing Transformer with Neural Architecture Search,https://www.reddit.com/r/MachineLearning/comments/b1lkkg/r_the_evolved_transformer_enhancing_transformer/,ranihorev,1552691437,"Hi everybody,

Sharing a new summary I wrote on The Evolved Transformer (ET), a paper by Google Brain with a simple recipe:

1. Take Transformer - the leading architecture for language models and a variety of tasks in NLP.
2. Build a Neural Architecture Search algorithm, an improved version of Tournament Selection, that searches for better models automatically.
3. Mix with 200 TPUs (Google), initialize the search algorithm with Transformer and after 1B steps youll end up with a better model.

Interestingly, the smaller the ET model the better its performance compared to a Transformer model of the same size. For example, with \~7M parameters (""mobile-friendly"") it outperforms Transformer by 1 perplexity point (7.62 vs 8.62).

[https://www.lyrn.ai/2019/03/12/the-evolved-transformer](https://www.lyrn.ai/2019/03/12/the-evolved-transformer)

We will probably see this recipe, of human-designed models with automated search algorithms, more and more in the future, as it allows building more complex architectures than human-beings can.",9,52,False,self,,,,,
976,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,9,b1miwl,self.MachineLearning,DGX-3 Rumors?,https://www.reddit.com/r/MachineLearning/comments/b1miwl/dgx3_rumors/,Simusid,1552697108,[removed],0,1,False,self,,,,,
977,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,9,b1mnl1,self.MachineLearning,"Gensim for LDA, any alternatives?",https://www.reddit.com/r/MachineLearning/comments/b1mnl1/gensim_for_lda_any_alternatives/,thnok,1552697928,[removed],0,1,False,self,,,,,
978,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,10,b1mpd2,self.MachineLearning,How good would the GTP-2 model be at generating jokes?,https://www.reddit.com/r/MachineLearning/comments/b1mpd2/how_good_would_the_gtp2_model_be_at_generating/,mrconter1,1552698245,[removed],0,1,False,self,,,,,
979,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,10,b1mqb4,self.MachineLearning,Need help finding an article about recent chatbot conference panel,https://www.reddit.com/r/MachineLearning/comments/b1mqb4/need_help_finding_an_article_about_recent_chatbot/,somethingstrang,1552698409,[removed],0,1,False,self,,,,,
980,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,10,b1mt6k,facebook.com,Game AI at GDC 2019,https://www.reddit.com/r/MachineLearning/comments/b1mt6k/game_ai_at_gdc_2019/,formalsystem,1552698933,,0,1,False,https://b.thumbs.redditmedia.com/t5B9dJgVNyVSjvTK4PiGMdlgDdkSCl4HzNy0wyDxVDw.jpg,,,,,
981,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,11,b1nbaj,go.geeklearn.net,Machine learning with Python: An introduction,https://www.reddit.com/r/MachineLearning/comments/b1nbaj/machine_learning_with_python_an_introduction/,MarkOliver908,1552702211,,0,1,False,default,,,,,
982,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,11,b1nl8g,nature.com,Supervised learning with quantum-enhanced feature spaces,https://www.reddit.com/r/MachineLearning/comments/b1nl8g/supervised_learning_with_quantumenhanced_feature/,aiforworld2,1552704018,,0,1,False,default,,,,,
983,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,12,b1o9rv,self.MachineLearning,Tracing Live Gun Stream and informing nearest Police Station.,https://www.reddit.com/r/MachineLearning/comments/b1o9rv/tracing_live_gun_stream_and_informing_nearest/,D_Darth_Vader,1552708625,[removed],0,1,False,self,,,,,
984,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,15,b1pdid,self.MachineLearning,"hello . I've decided to creat a podcast about deep learning  machine learning  artifical inteligence , neuroscience ,....... which interviews with experts . But unfortunately I can't choose a suitable title for it. Could anyone choose some titles for it ?",https://www.reddit.com/r/MachineLearning/comments/b1pdid/hello_ive_decided_to_creat_a_podcast_about_deep/,Doctor_who1,1552717076,[removed],0,1,False,self,,,,,
985,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,15,b1peju,self.MachineLearning,Recreational Vehicle Awnings Market Analysis Reveals unsafe improvement by 2023,https://www.reddit.com/r/MachineLearning/comments/b1peju/recreational_vehicle_awnings_market_analysis/,apple_x9,1552717316,[removed],1,1,False,self,,,,,
986,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,15,b1pkoa,self.MachineLearning,Does removing outliers from the data affect the decision boundary of a soft margin SVM?,https://www.reddit.com/r/MachineLearning/comments/b1pkoa/does_removing_outliers_from_the_data_affect_the/,sakhar0v,1552718732,[removed],0,1,False,self,,,,,
987,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,16,b1pux6,self.MachineLearning,I trained a face predictor that detects fulls bounds of face (81 facial landmarks vs dlib's 68),https://www.reddit.com/r/MachineLearning/comments/b1pux6/i_trained_a_face_predictor_that_detects_fulls/,codeniko,1552721174,[removed],0,1,False,self,,,,,
988,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,17,b1qbe0,self.MachineLearning,[D] Are e2e DL systems better than DNN-HMM models in speech recognition?,https://www.reddit.com/r/MachineLearning/comments/b1qbe0/d_are_e2e_dl_systems_better_than_dnnhmm_models_in/,TryingToGeek,1552725516,"End-to-end deep learning systems for automatic speech recognition (ASR) have been around for a while now since Deep Speech (2014), but I noticed that DNN-HMM based methods are still performing well and making it to the charts like [here](https://github.com/syhw/wer_are_we). Does that mean it is still not settled which system is better? Or do they win based on conditions? Who is better when you just have speech training data in the order of hundreds of hours and not tens of thousands? Which system is better in real life and not on super clean data? I know that Google has adopted e2e for ASR, but is it just because of the stupidly large amount of data it has and its feasibility to tackle multiple languages without having to make a custom pipeline for each language? ",9,24,False,self,,,,,
989,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,18,b1qnx7,medium.com,[P] Training a neural network for driving around a race track in the CARLA simulator,https://www.reddit.com/r/MachineLearning/comments/b1qnx7/p_training_a_neural_network_for_driving_around_a/,ponadto,1552728591,,0,1,False,default,,,,,
990,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,18,b1qr8v,cetpainfotech.com,Is Machine Learning Trendy,https://www.reddit.com/r/MachineLearning/comments/b1qr8v/is_machine_learning_trendy/,Divya123divya,1552729431,,0,1,False,default,,,,,
991,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,19,b1qykk,self.MachineLearning,Q-learning from scratch [R],https://www.reddit.com/r/MachineLearning/comments/b1qykk/qlearning_from_scratch_r/,NEKO9991,1552731191,Are there any websites or book recommendations to start learning Q-learning? Preferably in java and without tensor flow,6,7,False,self,,,,,
992,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,19,b1r65g,youtube.com,StyleGAN Waifu Generator,https://www.reddit.com/r/MachineLearning/comments/b1r65g/stylegan_waifu_generator/,shiro_vocaloid,1552733016,,0,1,False,default,,,,,
993,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,22,b1s79t,tomshw.it,Tom's Hardware Italia: Il Politecnico di Milano pronto a rivoluzionare lintelligenza artificiale. (Italian Politecnico University make new hardware to make a revolution on artificial intelligence).,https://www.reddit.com/r/MachineLearning/comments/b1s79t/toms_hardware_italia_il_politecnico_di_milano/,TheHammer_78,1552741322,,0,1,False,default,,,,,
994,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,22,b1s7to,self.MachineLearning,Some advice about Courseras courses,https://www.reddit.com/r/MachineLearning/comments/b1s7to/some_advice_about_courseras_courses/,Skaidus,1552741429,[removed],1,1,False,self,,,,,
995,MachineLearning,t5_2r3gv,2019-3-16,2019,3,16,23,b1t8hx,self.MachineLearning,What are some cool projects i can do with fifa 19 dataset on kaggle,https://www.reddit.com/r/MachineLearning/comments/b1t8hx/what_are_some_cool_projects_i_can_do_with_fifa_19/,drtvader,1552747851,[removed],0,1,False,self,,,,,
996,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,0,b1tcj5,self.MachineLearning,Natural Language Processing With Python Cookbook,https://www.reddit.com/r/MachineLearning/comments/b1tcj5/natural_language_processing_with_python_cookbook/,mritraloi6789,1552748518,[removed],0,1,False,self,,,,,
997,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,0,b1tm9p,self.MachineLearning,[D] Any Gaussian Process academics here - what are you excited about?,https://www.reddit.com/r/MachineLearning/comments/b1tm9p/d_any_gaussian_process_academics_here_what_are/,kayaking_is_fun,1552750105,"I'm about to return to writing my Master's thesis after a hiatus, and I want to get back to my usual hype about GP methods and the cool challenges people are overcoming. 

I'm actually going to work on sparse GP approximations in learning nonlinear dynamical systems in online settings. Online GP learning seems like a fairly open problem - I need to start with developing my understanding of current approaches such as SVI and subset selection methods, and see how well they apply in the nonconjugate setting. I'm also very interested in the work on GP-SSMs, but the model they provide is so freeform that it feels very tough to find peaks in the likelihood when performing inference.

Anyone out there in the field of GPs or Bayesian nonparametrics who wants to write an excited (or unexcited) comment about what they're working on?",32,61,False,self,,,,,
998,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,0,b1tucu,self.MachineLearning,[D] Questions for OpenAI,https://www.reddit.com/r/MachineLearning/comments/b1tucu/d_questions_for_openai/,UltraMarathonMan,1552751387,"I'm talking with Greg Brockman, Co-Founder and CTO of OpenAI this week on the [Artificial Intelligence podcast](https://lexfridman.com/ai/) (includes audio and video).

Let me know if you have questions for him or what topics you would like to see covered.",64,174,False,self,,,,,
999,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,1,b1ucsy,self.MachineLearning,[D] What can I do with all this data?,https://www.reddit.com/r/MachineLearning/comments/b1ucsy/d_what_can_i_do_with_all_this_data/,ghostofgbt,1552754238,"I've got 20+ year of financial data (metrics, income statement, balance sheet, cash flow  statement) as well as insider transactions, institutional holdings, SEC filing data and probably a bunch more I'm forgetting about. I develop in Python with Django and my current web app I'm using for data visualizations of this data has a React front end. 

I've just finished a new release of the app and I want to get into doing some sentiment analysis and more advanced financial modeling so I'm experimenting with NLP and I'm wondering if machine learning could have any place in my experimentation. 

Any thoughts? I'm open minded to whatever, just trying to do some initial research and brainstorming. ",15,19,False,self,,,,,
1000,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,2,b1upel,youtube.com,Autonomous Driving Back Propagation and Genetic Algorithm,https://www.reddit.com/r/MachineLearning/comments/b1upel/autonomous_driving_back_propagation_and_genetic/,DevTechRetopall,1552756201,,0,1,False,default,,,,,
1001,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,2,b1uuuv,self.MachineLearning,[D] Representing visual structures with expanding trees,https://www.reddit.com/r/MachineLearning/comments/b1uuuv/d_representing_visual_structures_with_expanding/,JackAndrewMcKay,1552757049,"Hi, I'm not sure if this is the right place to ask this.

I haven't had much experience developing ML/NN but it's something I love to think and talk about with friends. Recently, I came to the realisation that everything a camera has ever seen could be represented by a single tree, and I'm wondering if this approach has been taking before.

If it hasn't been used before, I'm wondering what people's thoughts are in how useful such an approach would be.

Detecting horizontal, vertical, diagonal, or no line is simple enough. You could program in the following context:

* Horizontal lines can only continue on the left or right pixel.
* Vertical lines can only continue on the top or bottom pixel.
* Diagonal lines can only continue to the top-left and bottom-right pixel, or top-right and bottom-left pixel (depending on orientation).

When lines are extracted from an image (or frame of live video), those lines can then be trace to ""map a graph"", where each node represents a horizontal/vertical/(left/right)diagonal line.

For example, a certain triangle might be represented as ""/, \, -"".

""Training"" would consist of analysing a video feed using this technique to build the tree. When a new ""path"" is constructed, it may follow the nodes previously mapped until the path doesn't match and a new node needs created.

For example, a hexagon might be represented as ""/, \, |, /, \, |"". Comparing to the previous triangle example:
/ \ | / \ |    &lt;- hexagon
/ \ -   &lt;- triangle

The hexagon and the triangle follow the same path until the 3rd element, where the hexagon has the ""|"" line, but the triangle has the ""-"" line, in which case a new ""split"" would be created at the second element. One representing the continued path of the triangle, and the other representing the continued path of the hexagon.

Unless I'm thinking about it wrong, literally any shape ever seen could be represented on the same tree.

After a period of training, pruning could be performed where pathways that were rarely activated get stripped (a ""sleep"" cycle).

The same shapes at multiple scales could also be detected. Using the triangle as an example,
""/ / \ \ - -"" may represent a triangle x2 bigger than ""/ \ -"".

During training, if ""/ / \"" is detected rather than ""/ \"", the model could reason that it is ""following the same path"" but it's been stretch x2, and thus all extra analysis on the same path should be done scaled to a half, allowing a x2 triangle to be represented with the same pathway.

Using this technique, the activation at each ""leaf node"" could represent a unique line shape, i.e. the leaf represents a classification.

Aside from the performance implications (I'd imagine heavy optimisation would be needed to utilise this), does this seem like a reasonable way to represent visual memory with unsupervised learning?",3,1,False,self,,,,,
1002,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,2,b1uvmv,self.MachineLearning,Markup dataset for MASK-RCNN: only well-viewed objects?,https://www.reddit.com/r/MachineLearning/comments/b1uvmv/markup_dataset_for_maskrcnn_only_wellviewed/,dmitryct,1552757173,[removed],0,1,False,self,,,,,
1003,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,2,b1uvxe,medium.com,The AI future of ART production,https://www.reddit.com/r/MachineLearning/comments/b1uvxe/the_ai_future_of_art_production/,GreatOnion,1552757218,,0,2,False,https://b.thumbs.redditmedia.com/h6JpgQz8Jk1_flgZV9tiLkpda0rbR9UZ1jIiZDkha2c.jpg,,,,,
1004,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,2,b1uyzi,self.MachineLearning,ML project for college,https://www.reddit.com/r/MachineLearning/comments/b1uyzi/ml_project_for_college/,sanket789,1552757708,[removed],0,1,False,self,,,,,
1005,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,3,b1vffb,self.MachineLearning,Does A3C use epsilon to explore? [Reinforcement learning],https://www.reddit.com/r/MachineLearning/comments/b1vffb/does_a3c_use_epsilon_to_explore_reinforcement/,TheBrightman,1552760252,[removed],0,1,False,self,,,,,
1006,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,3,b1vh21,self.MachineLearning,machine learning how to optimization the feature and model ? there are any generation methods?,https://www.reddit.com/r/MachineLearning/comments/b1vh21/machine_learning_how_to_optimization_the_feature/,asda43asdf23423,1552760504,[removed],0,1,False,self,,,,,
1007,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,3,b1viv4,classydata.com,Introduction to K-Means Clustering with Multiple Features,https://www.reddit.com/r/MachineLearning/comments/b1viv4/introduction_to_kmeans_clustering_with_multiple/,Norok,1552760765,,0,1,False,default,,,,,
1008,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,4,b1w7r0,self.MachineLearning,data scientists and machine learning engineers,https://www.reddit.com/r/MachineLearning/comments/b1w7r0/data_scientists_and_machine_learning_engineers/,retrievingsunflower,1552764346,[removed],0,1,False,self,,,,,
1009,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,4,b1wava,self.MachineLearning,[D] feedback request for my articles and notes on my personal blog.,https://www.reddit.com/r/MachineLearning/comments/b1wava/d_feedback_request_for_my_articles_and_notes_on/,formatlar,1552764799,"I am looking for feedback my articles and notes on my personal blog. Can you check this article and other articles and share with me your feedback?

Mathematics as related to deep learning and artificial intelligence, indicates linear algebra. Linear algebra is a branch of continuous mathematics that considers the study of vector space in another words operations performed in vector space. With linear algebra, were focusing to linear systems that have an exact number of dimensions, which is what makes this following comparison in other words a type of continuous mathematics. My personal notes collected and designed from different sources. You can read rest of article on [https://medium.com/@omeryavuz68/machine-learning-basics-applied-mathematic-7bb1974770bd](https://medium.com/@omeryavuz68/machine-learning-basics-applied-mathematic-7bb1974770bd)",7,0,False,self,,,,,
1010,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,4,b1wbxh,self.MachineLearning,[D] Introduction to KMeans Clustering with Multi-Feature Wine Example,https://www.reddit.com/r/MachineLearning/comments/b1wbxh/d_introduction_to_kmeans_clustering_with/,Norok,1552764944,[removed],0,1,False,self,,,,,
1011,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,5,b1wpxh,self.MachineLearning,Free Intro to Data Science and Machine Learning Course,https://www.reddit.com/r/MachineLearning/comments/b1wpxh/free_intro_to_data_science_and_machine_learning/,TheChanneel,1552767013,[removed],1,1,False,self,,,,,
1012,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,6,b1xk59,arxiv.org,[R] Context-Aware Crosslingual Mapping (cross-lingual similarity using sentence and context aware word-level embeddings),https://www.reddit.com/r/MachineLearning/comments/b1xk59/r_contextaware_crosslingual_mapping_crosslingual/,danielcer,1552771636,,1,3,False,default,,,,,
1013,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,7,b1y6td,sinxloud.com,"I ranked the Best TensorFlow Courses on the internet, based on your reviews",https://www.reddit.com/r/MachineLearning/comments/b1y6td/i_ranked_the_best_tensorflow_courses_on_the/,skj8,1552775275,,0,1,False,https://b.thumbs.redditmedia.com/Akyzxl5OSoH-WyVgGL-CKd4Pl2WJimcQrXaaQNRLo3k.jpg,,,,,
1014,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,8,b1ylxs,self.MachineLearning,Blog on Principal Component Analysis,https://www.reddit.com/r/MachineLearning/comments/b1ylxs/blog_on_principal_component_analysis/,Vaishnavi999,1552777833,[removed],0,1,False,self,,,,,
1015,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,9,b1z7np,self.MachineLearning,"[D] Useful tool for keeping track of ML experiments, changes and results",https://www.reddit.com/r/MachineLearning/comments/b1z7np/d_useful_tool_for_keeping_track_of_ml_experiments/,AdmiralLunatic,1552781629,"A few months ago, I came across a discussion about ""amie"" in YC Hacker News. I was intrigued and decided to check it out further. The idea that I could use graphs to keep track of previous experiments felt ideal. However, to my disappointment, I saw that I would have to define variables and tree structures individually. Nevertheless, since I was already looking for ways to improve my prototyping experience, I reached out to them and asked if they'll be adding Jupyter Notebook integration and automatic tree generation. 

&amp;#x200B;

Just today, I got back an email from the team about [their new product- Fern](https://www.amie.ai/#/fern). You can install it through pip and use a couple of commands to create tree structures from Jupyter Notebook files. Whats even better is that I could see it generating new leafs every time I changed parameters, tried different models. I believe this would make it freakishly easy to keep track of my experiments and results. I'd like to hear what you think about the product, as well as share any other software you use for the same purpose. ",19,53,False,self,,,,,
1016,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,9,b1zka8,self.MachineLearning,[D] Linear model for variables that have an associated uncertainty,https://www.reddit.com/r/MachineLearning/comments/b1zka8/d_linear_model_for_variables_that_have_an/,stochastic_gradient,1552783780,"Consider a standard linear regression

y = wx + b

This is my problem, but every x has an associated uncertainty, i.e. it comes from a normal distribution, and has an associated mu and sigma, corresponding to my best guess at the true mean (mu), and how certain I am of this guess (sigma).

I could just use the mu and ignore the sigma, but it seems like there should be a more principled way of dealing with this. For example, I could imagine something like optimizing this with gradient descent, and scaling the learning rate down when sigma is high. What's a good way to deal with this?",13,11,False,self,,,,,
1017,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,10,b204ra,self.MachineLearning,Making predictions about a score from text?,https://www.reddit.com/r/MachineLearning/comments/b204ra/making_predictions_about_a_score_from_text/,pointsofperception,1552787380,"How long would it take for someone with only some basic css and html skills to learn how to make a neural net that uses a paragraph of text to predict a score on a scale? The information it will be fed is paragraphs followed by scores. For example, depression or psychosis can be predicted by certain features of language and even vocal vibration.


I am looking into TensorFlow, but I am completely lost. I consider myself pretty tech savvy, but wow. I have no idea where to start. I've found some tutorials but it seems quite honestly like I can't find any of the answers I'm looking for. SEO hacks have messed up google searches and it doesn't look like I can find much content. There must be more communities for stuff like this somewhere. What are the major hubs?

Would it be best to create a neural net? How much data would that require to train? Could it be done with a few hundred pairings (i.e. paragraph + score)? Or would I have to take a more structure approach, like speech language analysis, or something of the sort. 


I just want to know what kind of software or approach I would have to take to approach something like this, to know whether it's feasible to do for one person within a matter of a few months.

",0,1,False,self,,,,,
1018,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,10,b20868,self.MachineLearning,research resources,https://www.reddit.com/r/MachineLearning/comments/b20868/research_resources/,Jakc124,1552787991,[removed],0,1,False,self,,,,,
1019,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,11,b20b9i,self.MachineLearning,[P] I trained a face predictor that detects fulls bounds of face (81 facial landmarks vs dlib's 68),https://www.reddit.com/r/MachineLearning/comments/b20b9i/p_i_trained_a_face_predictor_that_detects_fulls/,codeniko,1552788544,"I've had this open source for a while and figured I'd share it with the community in case others may find it useful. You can find better description in the repo's readme. [https://github.com/codeniko/shape\_predictor\_81\_face\_landmarks](https://github.com/codeniko/shape_predictor_81_face_landmarks)

I posted a demo video on youtube showing the points recognized while using the model [https://www.youtube.com/watch?v=mDJrASIB1T0](https://www.youtube.com/watch?v=mDJrASIB1T0)",22,136,False,self,,,,,
1020,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,11,b20jvu,self.MachineLearning,Identifying Web Browser &amp; Version given a screenshot,https://www.reddit.com/r/MachineLearning/comments/b20jvu/identifying_web_browser_version_given_a_screenshot/,_haob,1552790136,[removed],0,1,False,self,,,,,
1021,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,11,b20r07,self.MachineLearning,Is chess considered supervised or unsupervised learning?,https://www.reddit.com/r/MachineLearning/comments/b20r07/is_chess_considered_supervised_or_unsupervised/,oghi808,1552791469,[removed],0,1,False,self,,,,,
1022,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,12,b20vx0,self.MachineLearning,Guide for installing deep learning frameworks on Ubuntu 18.04 LTS,https://www.reddit.com/r/MachineLearning/comments/b20vx0/guide_for_installing_deep_learning_frameworks_on/,sjyoon1671,1552792406,[removed],0,1,False,self,,,,,
1023,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,12,b20z2g,self.MachineLearning,A guide for installing deep learning frameworks on Ubuntu 18.04 LTS,https://www.reddit.com/r/MachineLearning/comments/b20z2g/a_guide_for_installing_deep_learning_frameworks/,sjyoon1671,1552793029,[removed],0,1,False,self,,,,,
1024,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,12,b21246,github.com,A Beginner's Guide to Artificial Intelligence/Machine Learning Research Papers &amp; Developments,https://www.reddit.com/r/MachineLearning/comments/b21246/a_beginners_guide_to_artificial/,kjaisingh,1552793627,,0,1,False,https://b.thumbs.redditmedia.com/FZIC8Ot6f_JSrD5C0P3Ucftdr4VDku6iE7VNbRM-qUA.jpg,,,,,
1025,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,13,b21kde,self.MachineLearning,Object Detection : QR Barcodes ?,https://www.reddit.com/r/MachineLearning/comments/b21kde/object_detection_qr_barcodes/,ththiger,1552797454,[removed],0,1,False,self,,,,,
1026,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,15,b22bo2,self.MachineLearning,[D] Which are the best Rapidminer alternatives today for nlp focussed analysis?,https://www.reddit.com/r/MachineLearning/comments/b22bo2/d_which_are_the_best_rapidminer_alternatives/,abdush,1552803622,,5,0,False,self,,,,,
1027,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,16,b22oru,i.redd.it,[D] Is This Deep Learning in ML,https://www.reddit.com/r/MachineLearning/comments/b22oru/d_is_this_deep_learning_in_ml/,Ajaygawde,1552806844,,0,1,False,https://b.thumbs.redditmedia.com/OMvpaf55z8qovKyCOrDQ47zUVHx7kqZg12fWsYhjHHQ.jpg,,,,,
1028,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,16,b22skk,self.MachineLearning,"What model should I make/use to classify between (a hand with an open palm showing all five fingers) as ""5"" and (a hand with the same but in waving motion) as ""hi""?",https://www.reddit.com/r/MachineLearning/comments/b22skk/what_model_should_i_makeuse_to_classify_between_a/,ajeenkkya,1552807785,[removed],0,1,False,self,,,,,
1029,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,17,b238tm,self.MachineLearning,"[R] An implementation of ""Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters"" (KDD 2017).",https://www.reddit.com/r/MachineLearning/comments/b238tm/r_an_implementation_of_egosplitting_framework/,benitorosenberg,1552812401,"&amp;#x200B;

https://i.redd.it/s5jw062s6nm21.jpg

Github: [https://github.com/benedekrozemberczki/EgoSplitting](https://github.com/benedekrozemberczki/EgoSplitting)

Paper: [https://www.eecs.yorku.ca/course\_archive/2017-18/F/6412/reading/kdd17p145.pdf](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf)

ABSTRACT:

We propose a new framework called Ego-Splitting for detecting clusters  in complex networks which leverage the local structures known as  ego-nets (i.e. the subgraph induced by the neighborhood of each node) to  de-couple overlapping clusters. Ego-Splitting is a highly scalable and  flexible framework, with provable theoretical guarantees, that reduces  the complex overlapping clustering problem to a simpler and more  amenable non-overlapping (partitioning) problem. We can solve community  detection in graphs with tens of billions of edges and outperform  previous solutions based on ego-nets analysis. 

More precisely, our framework works in two steps: a local ego-net  analysis phase, and a global graph partitioning phase.  In the local  step, we first partition the nodes ego-nets using a partitioning  algorithm. We then use the computed clusters to split each node into its  persona nodes that represent the instantiations of the node in its  communities. Then, in the global step, we partition the newly created  graph to obtain an overlapping clustering of the original graph.

&amp;#x200B;",4,19,False,https://b.thumbs.redditmedia.com/O5dJbbIpGIMy2MuVTeQmXNEp_Xt0g_5d6mMZqTNcEOY.jpg,,,,,
1030,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,18,b23c6u,self.MachineLearning,Google Edge TPU is pretty good,https://www.reddit.com/r/MachineLearning/comments/b23c6u/google_edge_tpu_is_pretty_good/,habanero_ass_fire,1552813344,[removed],0,1,False,self,,,,,
1031,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,18,b23luq,self.MachineLearning,Preparing text for BERT fine-tuning,https://www.reddit.com/r/MachineLearning/comments/b23luq/preparing_text_for_bert_finetuning/,kayvane,1552815973,[removed],0,1,False,self,,,,,
1032,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,18,b23oac,self.MachineLearning,What is the state of the art procedure for 3d reconstruction for Virtual Reality?,https://www.reddit.com/r/MachineLearning/comments/b23oac/what_is_the_state_of_the_art_procedure_for_3d/,madmelzar,1552816649,"hey i'm doing this project for school in which i need to research methods for 3d reconstruction for vr. 

had a couple of follow up questions on top of the above -

&amp;#x200B;

\- what sort of cameras (ToF, structured light, stereo etc.) are best suited for outdoor scenes, indoor scenes, dynamic scenes and static scenes? 

\- what sort of methods do people use for processing the RGB-D data?

\- how do people achieve photorealistic colours and textures?

&amp;#x200B;

thanks for reading! any info related to the topic would be greatly appreciated!!

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
1033,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,19,b23xmo,self.MachineLearning,Keras convLSTM &amp; flow_from_dataframe,https://www.reddit.com/r/MachineLearning/comments/b23xmo/keras_convlstm_flow_from_dataframe/,FenryrMKIII,1552819061,[removed],0,1,False,self,,,,,
1034,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,19,b240ko,self.MachineLearning,"Keras, convLSTM and flow_from_dataframe",https://www.reddit.com/r/MachineLearning/comments/b240ko/keras_convlstm_and_flow_from_dataframe/,FenryrMKIII,1552819842,[removed],0,1,False,self,,,,,
1035,MachineLearning,t5_2r3gv,2019-3-17,2019,3,17,22,b255g5,self.MachineLearning,Help with finding projects,https://www.reddit.com/r/MachineLearning/comments/b255g5/help_with_finding_projects/,AstroKabi,1552829294,[removed],0,1,False,self,,,,,
1036,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,0,b26cup,self.MachineLearning,[D] A blog post on differential privacy,https://www.reddit.com/r/MachineLearning/comments/b26cup/d_a_blog_post_on_differential_privacy/,HomogeneousSpace,1552836999,"Hello r/ml,

I wrote a two-part blog post aiming to give a mathematical introduction to differential privacy. I start with basic definitions and finish with reproducing theoretical results (modulo a gap) from recent development in differentially private stochastic gradient descent and explaining the implementation of differential privacy in Tensorflow, among other things.

Links: [Part 1]([https://ypei.me/posts/2019-03-13-a-tail-of-two-densities.html](https://ypei.me/posts/2019-03-13-a-tail-of-two-densities.html), [Part 2]([https://ypei.me/posts/2019-03-14-great-but-manageable-expectations.html](https://ypei.me/posts/2019-03-14-great-but-manageable-expectations.html)

I hope you find it useful, and any feedback (bug reports, feature requests, questions, comments...) is welcome.

Abstract: This is a two-part post where I give an introduction to differential privacy, which is a study of tail bounds of the divergence between probability measures, with the end goal of applying it to stochastic gradient descent.

In Part 1 I start with the definition of -differential privacy (corresponding to max divergence), followed by (,)-differential privacy (a.k.a. approximate differential privacy, corresponding to the -approximate max divergence). I show a characterisation of the (,)-differential privacy as conditioned -differential privacy. Also, as examples, I illustrate the -dp with Laplace mechanism and, using some common tail bounds, the approximate dp with the Gaussian mechanism.

Then I continue to show the effect of combinatorial and sequential compositions of randomised queries (called mechanisms) on privacy by stating and proving the composition theorems for differential privacy, as well as the effect of mixing mechanisms, by presenting the subsampling theorem (a.k.a. amplification theorem).

In Part 2, I discuss the Rnyi differential privacy, corresponding to the Rnyi divergence, a study of the moment generating functions of the divergence between probability measures to derive the tail bounds.

Like in Part 1, I prove a composition theorem and a subsampling theorem.

I also attempt to reproduce a seemingly better moment bound for the Gaussian mechanism with subsampling, with one intermediate step which I am not able to prove.

After that I explain the Tensorflow implementation of differential privacy in its Privacy module, which focuses on the differentially private stochastic gradient descent algorithm (DP-SGD).

Finally I use the results from both Part 1 and Part 2 to obtain some privacy guarantees for composed subsampling queries in general, and for DP-SGD in particular. I also compare these privacy guarantees.",30,104,False,self,,,,,
1037,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,1,b27546,nature.com,Machine learning in quantum spaces,https://www.reddit.com/r/MachineLearning/comments/b27546/machine_learning_in_quantum_spaces/,Ready2Rapture,1552841397,,0,1,False,default,,,,,
1038,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,1,b276lj,self.MachineLearning,Advice for my first Machine Learning / Artificial Intelligence Conference,https://www.reddit.com/r/MachineLearning/comments/b276lj/advice_for_my_first_machine_learning_artificial/,stinkykicks,1552841621,[removed],0,1,False,self,,,,,
1039,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,2,b27jx5,self.MachineLearning,"Machine Learning - Is Python necessary, or is JavaScript sufficient?",https://www.reddit.com/r/MachineLearning/comments/b27jx5/machine_learning_is_python_necessary_or_is/,IanAbsentia,1552843643,[removed],0,1,False,self,,,,,
1040,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,2,b27mcb,arxiv.org,Stiffness: A New Perspective on Generalization in Neural Networks,https://www.reddit.com/r/MachineLearning/comments/b27mcb/stiffness_a_new_perspective_on_generalization_in/,macncookies,1552844010,,7,81,False,default,,,,,
1041,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,3,b280na,self.MachineLearning,[D] Tuning Hyper Parameters in Weakly-Supervised Learning?,https://www.reddit.com/r/MachineLearning/comments/b280na/d_tuning_hyper_parameters_in_weaklysupervised/,attiaa13,1552846142,"I'm working on a project on weakly localization, i.e. localize object in image when the data I have for training is images with their class labels (for example CUB or ImageNet).

Iv'e created my model, which have several regularizations with different weights, and found myself with a problem in tuning my hyper parameters since I don't have any labeled bounding boxes (It's a research project, so I don't want to tune it with data which has bounding boxes).

I wandered if you know of ideas to tune parameters in such scenarios.

&amp;#x200B;",4,1,False,self,,,,,
1042,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,3,b28387,self.MachineLearning,Learn how to make a program that can paint photographs !,https://www.reddit.com/r/MachineLearning/comments/b28387/learn_how_to_make_a_program_that_can_paint/,signal_v_noise,1552846522,[removed],0,1,False,self,,,,,
1043,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,3,b283yc,arxiv.org,"Semi-Implicit Variational Inference; a flexible, training robust family of distribution.",https://www.reddit.com/r/MachineLearning/comments/b283yc/semiimplicit_variational_inference_a_flexible/,micomyco,1552846620,,4,6,False,default,,,,,
1044,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,3,b28h3w,self.MachineLearning,[D] How can I improve this article? Introduction to statistics and probability theory,https://www.reddit.com/r/MachineLearning/comments/b28h3w/d_how_can_i_improve_this_article_introduction_to/,formatlar,1552848568,"Introduction to statistics and probability theory

Statistics is the branch of mathematics that deals with the collection, analysis, interpretation, presentation and organization of numerical data.

Descriptive statistics

Inferential statistics

Basic statistics and probability theory

Probability, which is a mathematical method in modeling uncertain scenarios, underlines the algorithms that make artificial intelligence and helps us why our systems should be logical.

How can I improve this article? I need your feedback to improve my article.",0,0,False,self,,,,,
1045,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,4,b28riw,self.MachineLearning,"[ Deep learning + Music ] Music Generation using GAN , How to play songs from the midi images.",https://www.reddit.com/r/MachineLearning/comments/b28riw/deep_learning_music_music_generation_using_gan/,prashantkr314,1552850071,[removed],0,1,False,self,,,,,
1046,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,4,b28ykb,self.MachineLearning,"[Deep learning + Music] Music Generation using GAN , How to play songs from the midi images",https://www.reddit.com/r/MachineLearning/comments/b28ykb/deep_learning_music_music_generation_using_gan/,prashantkr314,1552851116,"I am exploring to this repository : [musegan](https://github.com/salu133445/musegan) and tried to exectue it.

My shared [Google Colab Link](https://colab.research.google.com/drive/12Vw3-94YXbOKmNhuBh0TrKrgfZeOVmlQ)

&amp;#x200B;

It executes but i have no idea where do i get the generated music samples or how do i run the music.It produces bunch of `.png` images in the `./exp/`  folder but i don't know how is that helpful for generating music

&amp;#x200B;

even in the ReadMe file of this project the [Results](https://github.com/salu133445/musegan#sample-results) if you download it , it give bunch of images. I have no idea how can i use these images.

&amp;#x200B;

I am new to ML and Deep Learning, I picked this project because i have interest in music and i wanted to get inspired how deep learning will solve this problem.

&amp;#x200B;

i have read about ANN, RNN &amp; CNN  and GAN but i am at a very noob level. But i want to learn this.

I did watch this video of project owner, [Video](https://www.youtube.com/watch?v=SHPjZwSbRhs) But it's in Chinese , i did used [Google Translate (Chinese to English)](https://translate.google.com/?rlz=1C5CHFA_enIN831IN831&amp;um=1&amp;ie=UTF-8&amp;hl=en&amp;client=tw-ob#zh-TW/en/) to convert the audio into english text but it wasn't that great experience.

&amp;#x200B;

These are the slides :      [Slide 1](https://salu133445.github.io/bmusegan/pdf/bmusegan-ismir2018-slides.pdf)[Slide 2](https://salu133445.github.io/musegan/pdf/musegan-aaai2018-slides.pdf)

&amp;#x200B;

&amp;#x200B;

I know this is not the best first project to choose, but this is what interests me so i'll be more happy to invest my time to know about this project.

&amp;#x200B;

My background is in web-development both front-end &amp; back-end.

&amp;#x200B;",0,1,False,self,,,,,
1047,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,4,b294ju,self.MachineLearning,Shape Recognition/Grouping with KMeans,https://www.reddit.com/r/MachineLearning/comments/b294ju/shape_recognitiongrouping_with_kmeans/,csharp_ai,1552851972,[removed],0,1,False,self,,,,,
1048,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,4,b294t4,self.MachineLearning,[D] Gradient Call for Ideas,https://www.reddit.com/r/MachineLearning/comments/b294t4/d_gradient_call_for_ideas/,hughbzhang,1552852011,"Hey r/MachineLearning,

&amp;#x200B;

Here at the Gradient, we love the discussion that takes place on this subreddit in response to our articles. If any of you ever want to write something for us, we'd love to consider it. Let us know! [https://thegradient.pub/call-for-ideas-march-2018/](https://thegradient.pub/call-for-ideas-march-2018/)",1,0,False,self,,,,,
1049,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,4,b295hm,arxiv.org,Embedding Syntax and Semantics of Prepositions via Tensor Decomposition,https://www.reddit.com/r/MachineLearning/comments/b295hm/embedding_syntax_and_semantics_of_prepositions/,Digimon_Utopia_99,1552852109,,1,18,False,default,,,,,
1050,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,5,b29v5c,self.MachineLearning,[D] Anyone going onsite for the Google AI residency 2019?,https://www.reddit.com/r/MachineLearning/comments/b29v5c/d_anyone_going_onsite_for_the_google_ai_residency/,TheRedSphinx,1552855971,"Last year, there was a similarly titled thread which contained some useful for information. I'm hoping this thread will serve the same purposes for those of us who are going onsite, or have already. Since we all signed NDAs, please don't talk about the questions. Instead, if you find some more information as to when we'll hear back or just want to ask some questions, share your thoughts, or whatever you want that doesn't break the NDA, feel free to post here!

&amp;#x200B;

On that note, anyone else dying of nervousness as we wait for the decisions?",42,6,False,self,,,,,
1051,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,5,b29wky,self.MachineLearning,Where to start learning about Neural Networks?,https://www.reddit.com/r/MachineLearning/comments/b29wky/where_to_start_learning_about_neural_networks/,aengel96,1552856184,[removed],0,1,False,self,,,,,
1052,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,6,b2a2rb,self.MachineLearning,[OpenWebText] I made an optimised version of the OpenWebText scraper by u/joshuacpeterson,https://www.reddit.com/r/MachineLearning/comments/b2a2rb/openwebtext_i_made_an_optimised_version_of_the/,tgithubbr,1552857121,[removed],0,1,False,self,,,,,
1053,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,7,b2axbl,self.MachineLearning,Shape Recognition using KMeans,https://www.reddit.com/r/MachineLearning/comments/b2axbl/shape_recognition_using_kmeans/,csharp_ai,1552861868,[removed],0,1,False,self,,,,,
1054,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,7,b2b6fu,self.MachineLearning,Progress ?,https://www.reddit.com/r/MachineLearning/comments/b2b6fu/progress/,timothydavidcook,1552863322,[removed],0,1,False,self,,,,,
1055,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,10,b2crp5,self.MachineLearning,Prediction of cricket match outcome,https://www.reddit.com/r/MachineLearning/comments/b2crp5/prediction_of_cricket_match_outcome/,Mr_Geek007,1552872936,[removed],0,1,False,self,,,,,
1056,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,11,b2dbn6,self.MachineLearning,New to machine learning - Need some help with Bayes,https://www.reddit.com/r/MachineLearning/comments/b2dbn6/new_to_machine_learning_need_some_help_with_bayes/,Cherroa,1552876474,[removed],0,1,False,self,,,,,
1057,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,14,b2eqtk,self.MachineLearning,Geo-Transparent data access for AI/ML distributed datasets with EdgeFS,https://www.reddit.com/r/MachineLearning/comments/b2eqtk/geotransparent_data_access_for_aiml_distributed/,dmitry_yus,1552886487,[removed],0,1,False,self,,,,,
1058,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,14,b2etgw,i.redd.it,"LID design display--When it comes to innovative extrusion blow molding machines, you need look no further than Yankang Plastic Machinery. Yankang will provide you with customized services to make a mold.http://www.yankangmachine.com/",https://www.reddit.com/r/MachineLearning/comments/b2etgw/lid_design_displaywhen_it_comes_to_innovative/,miyawang12138,1552887049,,1,1,False,https://b.thumbs.redditmedia.com/tZANSLfVxk9HJzNnSA43RLzAyyufiNwvHdzEJSb6GyQ.jpg,,,,,
1059,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,14,b2etvj,facebook.com,Explore your #career in #machinelearning Machine Learning Classroom Training | Certification Course,https://www.reddit.com/r/MachineLearning/comments/b2etvj/explore_your_career_in_machinelearning_machine/,sreethusree,1552887133,,0,1,False,default,,,,,
1060,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,14,b2ev3n,self.MachineLearning,[p] advice needed on general approach to a generative algorithm.,https://www.reddit.com/r/MachineLearning/comments/b2ev3n/p_advice_needed_on_general_approach_to_a/,itshardtofindanunuse,1552887395,"I need to preface this by saying I know nothing. I'm a student studying architecture. I have some python experience. I am trying to optimize the geometry of a roof for improved ventilation, using a CFD analysis to determine success. 

&amp;#x200B;

The plan:

I'd like to use machine learning in this goal. The plan is to have roughly 30 input parameters (number sliders) to determine the geometry of the roof. A CFD analysis will be done of the roof and values at roughly a dozen points will be collected (the goal is highest velocity). If a collision is detected (my building must house things) at certain points they won't be valid. 

The issue:

Everything I can find on machine learning in respect to architecture, assumes a supply of pre-existing training data, or artificial human selection. I am looking for a machine learning strategy that learns from it's own iterations.

The want:

I'm primarily looking for advice on general approach to this problem. I know my inputs, and I know my metrics for success, but I don't know the best way of getting there. Any advice at all would be greatly appreciated. Right now I am thinking a neuroevolution approach would be most effective, but I fear that it would be pretty slow with the burden of CFD analysis involved. Secondly, I'm looking for recommendations for tools to use. I am comfortable, but very slow, at scripting in python, and python has good interface with the modeling software that I use. Any type of machine learning GUI would also be super awesome! ",10,1,False,self,,,,,
1061,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,15,b2f7a9,self.MachineLearning,[D] What would you do on a seemingly dishonest update on an arxiv paper?,https://www.reddit.com/r/MachineLearning/comments/b2f7a9/d_what_would_you_do_on_a_seemingly_dishonest/,Stillexploring-,1552889943,"Hi,

&amp;#x200B;

A few months ago I was looking for some new papers in a field, found an interesting one (say v1), read thoroughly and kept it.

Now I revisit the link and the paper changed title, authors and rewrote all the contents. It is understandable, as the original one felt quite incremental from existing, under-acknowledged but brilliant work (say Origin).

&amp;#x200B;

But what irritates me is that the new version (v3) removed almost all the references to the Origin. Origin is cited but only briefly as a related work, and these new people actually advertises the main idea of Origin as theirs. v3 and v1 look the same in the core, and it occurs to me it is just the change of wrapping using Bayesian something.

&amp;#x200B;

Probably I am not going to do anything, but simply triggered. Maybe it is just me who look those v1 and v3 as the same in disguise. Maybe there are new important contributions in v3 that I missed. But if not, what would you in such cases?",54,113,False,self,,,,,
1062,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,16,b2fsuz,self.MachineLearning,What techniques were used in building this platform (expii),https://www.reddit.com/r/MachineLearning/comments/b2fsuz/what_techniques_were_used_in_building_this/,SocialEngineeeing,1552895105,"So theres this math training platform called Expii (expii.com) that asks you questions and then adapts subsequent questions according to your performance. It also offers you explanations to the areas that the platform deems you lack proficiency in. Basically, its an AI teacher. Its full capabilities are however best understood by taking a quick browse of the site. 

My limited understand is that this is achieved through machine learning. 

But Im curious as to what exactly you call the techniques use to achieve this, as Im writing an article on the platform. Though its targeted towards a layman audience, I do want to get the details right, including the class(es) of techniques and tools used as well as the type of languages/add-ons suitable for their purposes. ",0,1,False,self,,,,,
1063,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,16,b2fvpw,linkedin.com,How you and I are teaching Artificial Intelligence how to drive.,https://www.reddit.com/r/MachineLearning/comments/b2fvpw/how_you_and_i_are_teaching_artificial/,maximvanm,1552895789,,0,1,False,default,,,,,
1064,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,17,b2g1nw,self.MachineLearning,How to validate whether a task has been done or not?,https://www.reddit.com/r/MachineLearning/comments/b2g1nw/how_to_validate_whether_a_task_has_been_done_or/,EricDZhang,1552897110,[removed],0,1,False,self,,,,,
1065,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,17,b2g6ip,reddit.com,Products List,https://www.reddit.com/r/MachineLearning/comments/b2g6ip/products_list/,Uvsubmakerter,1552898235,,0,1,False,default,,,,,
1066,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,17,b2g988,youtu.be,Video: Neural Ordinary Differential Equations,https://www.reddit.com/r/MachineLearning/comments/b2g988/video_neural_ordinary_differential_equations/,ykilcher,1552898866,,0,1,False,https://b.thumbs.redditmedia.com/nvuwa7pi5nzJpK0rOgGO8s_VrJWBsvpaRAJ0J3iB_ZU.jpg,,,,,
1067,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,18,b2gco8,youtu.be,[R] Video: Neural Ordinary Differential Equations,https://www.reddit.com/r/MachineLearning/comments/b2gco8/r_video_neural_ordinary_differential_equations/,ykilcher,1552899668,,0,1,False,https://b.thumbs.redditmedia.com/nvuwa7pi5nzJpK0rOgGO8s_VrJWBsvpaRAJ0J3iB_ZU.jpg,,,,,
1068,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,18,b2gfyx,self.MachineLearning,[D] What is the best way to detect paragraphs from document images,https://www.reddit.com/r/MachineLearning/comments/b2gfyx/d_what_is_the_best_way_to_detect_paragraphs_from/,DGs29,1552900391,"I want to detect paragraphs/ group of words that lie together in a document (like the one in the below image). I looked methods to detect texts but most of them are scene text/ text detection in natural images. I also tried to do opencv's dilation but I didn't have much luck with low line-spacing between paragraphs.

**GOAL:**

https://i.redd.it/lrtyod3m2um21.png

&amp;#x200B;

&amp;#x200B;",5,0,False,https://b.thumbs.redditmedia.com/epIkPQCiPDOuNURbJcUv0359eWXVptcdZpsMvy644mQ.jpg,,,,,
1069,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,18,b2ghzy,smarten.com,Visual Analytics Software and Predictive Analytics Made Easy!,https://www.reddit.com/r/MachineLearning/comments/b2ghzy/visual_analytics_software_and_predictive/,ElegantMicroWebIndia,1552900862,,0,1,False,default,,,,,
1070,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,18,b2gig2,self.MachineLearning,"[D] (Help requested) Neural Network to classify images between ""town"" and countryside-village",https://www.reddit.com/r/MachineLearning/comments/b2gig2/d_help_requested_neural_network_to_classify/,_Howlin,1552900967,"Hi everyone,

&amp;#x200B;

I am trying to build a neural network aiming to detect whether a 360 image taken from a road (for instance, a Google Street View) can be associated to a location in a town or not.

I have tried to build my own little CNN, thinking this was not such a difficult task. The network could not make any good fitting and was always stuck. I also tried using some of the traditional ""lightweight"" architectures like VGG and MobileNet. These had results that were exactly the same before and after training. There is no special learning, and finally the neural network is not giving any good result.

&amp;#x200B;

Am I doing something wrong ? Are neural networks really the best idea to solve this problem or some classical image processing would do the job ?

&amp;#x200B;

Thank you for your potential help and recommendations  !",17,0,False,self,,,,,
1071,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,18,b2gjfm,self.MachineLearning,Is there any Speech To Text (end to end) Algorithm that does not use Any API's?,https://www.reddit.com/r/MachineLearning/comments/b2gjfm/is_there_any_speech_to_text_end_to_end_algorithm/,Shakzhaf,1552901188,[removed],0,1,False,self,,,,,
1072,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,18,b2gjli,self.MachineLearning,Which performs better? GPT-2 or BERT?,https://www.reddit.com/r/MachineLearning/comments/b2gjli/which_performs_better_gpt2_or_bert/,TheSeungJun,1552901229,"Since they are not dealing with same tasks in papers(BERT : [https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf) , GPT-2 : [https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) ) , how can we compare the performance btw them? And I am curious why they are not dealing with same tasks.",0,1,False,self,,,,,
1073,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,18,b2glk2,self.MachineLearning,Germany's 3 billion for AI development shrink to only 500 million,https://www.reddit.com/r/MachineLearning/comments/b2glk2/germanys_3_billion_for_ai_development_shrink_to/,d0_0d,1552901651,[removed],0,1,False,self,,,,,
1074,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,18,b2gn7z,medium.com,Where to learn about NLP?,https://www.reddit.com/r/MachineLearning/comments/b2gn7z/where_to_learn_about_nlp/,omarsar,1552902002,,0,1,False,https://b.thumbs.redditmedia.com/IXNIWUI9IuSebrpIRduaSe1SsHIxcI93L4AOcsC6qCk.jpg,,,,,
1075,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,18,b2gphz,self.MachineLearning,Natural clustering Assumption,https://www.reddit.com/r/MachineLearning/comments/b2gphz/natural_clustering_assumption/,sriharsha_0806,1552902491,[removed],0,1,False,self,,,,,
1076,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,19,b2gwwt,self.MachineLearning,"[R] A PyTorch implementation of ""Splitter: Learning Node Representations that Capture Multiple Social Contexts"" (WWW 2019).",https://www.reddit.com/r/MachineLearning/comments/b2gwwt/r_a_pytorch_implementation_of_splitter_learning/,benitorosenberg,1552904055,"&amp;#x200B;

https://i.redd.it/rimptgzsqum21.jpg

&amp;#x200B;

Github: [https://github.com/benedekrozemberczki/Splitter](https://github.com/benedekrozemberczki/Splitter)

Paper: [http://epasto.org/papers/www2019splitter.pdf](http://epasto.org/papers/www2019splitter.pdf)

ABSTRACT:

Recent interest in graph embedding methods has focused on learning a  single representation for each node in the graph. But can nodes really  be best described by a single vector representation? In this work, we  propose a method for learning multiple representations of the nodes in a  graph (e.g., the users of a social network). Based on a principled  decomposition of the ego-network, each representation encodes the role  of the node in a different local community in which the nodes  participate. These representations allow for improved reconstruction of  the nuanced relationships that occur in the graph a phenomenon that we  illustrate through state-of-the-art results on link prediction tasks on a  variety of graphs, reducing the error by up to 90%. In addition, we  show that these embeddings allow for effective visual analysis of the  learned community structure.",1,61,False,self,,,,,
1077,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,19,b2gz1f,self.MachineLearning,[D] What are your thoughts on AI and creativity?,https://www.reddit.com/r/MachineLearning/comments/b2gz1f/d_what_are_your_thoughts_on_ai_and_creativity/,universalsa,1552904494,"I read about the AI generated piece of art sold at Christie's for $432,500 and it got me thinking if we''ll ever reach a truly creative AI (or will we only reach it if we reach general AI?). There has of course been a big debate with regards to the artwork being made by an algorithm created by another artist, but that is not what I want to discuss here.

What are your thoughts on the matter? Do you have any interesting reading tips?",5,1,False,self,,,,,
1078,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,19,b2gzjy,medium.com,"[D] Custom Build Artificial Neural Network, How can I improve this article?",https://www.reddit.com/r/MachineLearning/comments/b2gzjy/d_custom_build_artificial_neural_network_how_can/,formatlar,1552904605,,0,1,False,default,,,,,
1079,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,19,b2h2eg,self.MachineLearning,Siamese LSTM network for mobile behavioural continuous authentication,https://www.reddit.com/r/MachineLearning/comments/b2h2eg/siamese_lstm_network_for_mobile_behavioural/,lwiji,1552905207,[removed],0,1,False,self,,,,,
1080,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,19,b2h4ma,self.MachineLearning,"[P] fern: Fabulous extension to automatically track, version control, and reproduce Jupyter notebooks.",https://www.reddit.com/r/MachineLearning/comments/b2h4ma/p_fern_fabulous_extension_to_automatically_track/,johannesbeil,1552905648,"Hey, we built [amie-fern](https://www.amie.ai/#/fern) to address the version control and reproducibility issues from rapid prototyping with Jupyter notebooks. 

It is a Jupyter labs extension + web app that **automatically tracks code, variables, data, and their dependencies in an interactive graph**. You can iterate fast, explore how your models performed, and export a python script with your favorite model in one click. We've made a  [short video](https://www.amie.ai/#/fern) where we demonstrate it on handwriting recognition with Keras.

You can signup up and use it for free, we'd love to hear what you think. Thanks !",2,19,False,self,,,,,
1081,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,19,b2h5op,self.MachineLearning,[D] Siamese LSTM Network for mobile behavioural continuous authentication,https://www.reddit.com/r/MachineLearning/comments/b2h5op/d_siamese_lstm_network_for_mobile_behavioural/,lwiji,1552905861,"Hello everyone,

&amp;#x200B;

I am currently working on analyzing smartphone's sensors in order to identify user behaviour and train a classifier to model user behaviour for continuous authentication.

&amp;#x200B;

I first found this paper from 2018 : [Mobile Based Continuous Authentication Using Deep Features by Mario Centeno et al.](https://www.sigmobile.org/mobisys/2018/workshops/deepmobile18/papers/Mobile_Based_Continuous_Authentication.pdf) where they use a CNN Siamese Neural Network to learn deep features embedding from the raw input of the time series (accelerometer, gyroscope datas). After learning these embedding, they use a One Class SVM to train a classifier with datas from the legitimate user only (trained on the deep features extracted thanks to the pre-trained CNN Siamese Network).

&amp;#x200B;

Then I found a paper from January 2019: [Actions speak louder than passwords passive authentication of smartphone users via deep temporal features](https://www.groundai.com/project/actions-speak-louder-than-passwords-passive-authentication-of-smartphone-users-via-deep-temporal-features/?fbclid=IwAR2yZEuu_jd-XO_z7qsUxQtuNmBHy-TWwi9KgQPD5dtYQbm8XrB00FVCb24) . They say they outperform the previous paper. They make use of more modalities in input (GPS, touchscreen inputs ...) 8 in total. From what I understand they train a Siamese LSTM for each one of the modailities, then they make a fusion score for each siamese prediction to predict the authentication result.

&amp;#x200B;

On the second paper, they don't mention which classifier they use to classify samples from the embedding learned vectors (they only talk about euclidian distance whitin the embedding samples...)

&amp;#x200B;

In such a case, after trainning and learning a distance metric with the Siamese LSTM network, do you think they use the weights from the siamese network to use it on a single input (instead of a pair) to compute his embedding (during testing/evaluation) ? Because I understood how to train the siamese network in the second paper, but i am a bit lost when it comes to evaluate a new samples once the siamese network is trained.

&amp;#x200B;

Thanks you in advance for the ones that are curious on this, and even if you don't wanna participate to this topics, I suggest you to read theses papers, they are just super interesting!! :)",1,2,False,self,,,,,
1082,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,19,b2h771,self.MachineLearning,"[Discussion] Custom Build Artificial Neural Network, How to improve this article?",https://www.reddit.com/r/MachineLearning/comments/b2h771/discussion_custom_build_artificial_neural_network/,formatlar,1552906129,"Custom network building implies what a truly human duty and what a natural, appropriate result of data model creation

The form of general mobilization of an artificial neural network is known as a conscientious feedforward network as that its essential duty is to training and fighting, sometimes called a multi-layer, well-trained, perceptron, derived from data society. These models save the mindful data heart and, spirit from unstructed eternal data perdition, while simplistic in nature also attend to some of the rations and equipment as part of data work, while contain turbulent place of war through examine going forward, for the various types of artificial neural network .",21,0,False,self,,,,,
1083,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,20,b2htta,quora.com,What is the best salesforce management company in the USA?,https://www.reddit.com/r/MachineLearning/comments/b2htta/what_is_the_best_salesforce_management_company_in/,credibll,1552910243,,0,1,False,default,,,,,
1084,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,21,b2i4e0,self.MachineLearning,Define reproducible ML pipelines: a DVC walkthrough,https://www.reddit.com/r/MachineLearning/comments/b2i4e0/define_reproducible_ml_pipelines_a_dvc_walkthrough/,dvcler,1552912071,"speed up and organize your machine learning model development!

how? check the following article, which walks you through DVC's main  features, such as defining reproducible pipelines, versioning pipelines  together with their associated data (training data, trained model,  etc.), sharing versioned data with your team mates, or comparing  pipelines by their performance metrics

The article also provides a readily usable training environment, allowing you to follow along the walkthrough interactively. [https://blog.codecentric.de/en/2019/03/walkthrough-dvc/](https://blog.codecentric.de/en/2019/03/walkthrough-dvc/)

  
 **#machine\_learning** **#deeplearning** **#deep\_learning** **#ml** **#dl** **#nlp** **#datascience** **#ai** **#opensource**",0,1,False,self,,,,,
1085,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,21,b2i8jw,self.MachineLearning,Need a small help,https://www.reddit.com/r/MachineLearning/comments/b2i8jw/need_a_small_help/,skrrull,1552912776,[removed],0,1,False,self,,,,,
1086,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,21,b2ic2s,self.MachineLearning,[N] Call for papers: ICML Workshop on Learning and Reasoning with Graph-Structured Data,https://www.reddit.com/r/MachineLearning/comments/b2ic2s/n_call_for_papers_icml_workshop_on_learning_and/,ethanfetaya,1552913354,"Call for papers to our ICML Workshop on Learning and Reasoning with Graph-Structured Data is out! Deadline: 18 April. [https://graphreason.github.io](https://graphreason.github.io/?fbclid=IwAR27XjHxTzE_pT7DbmvfQRNQ0fZm6UFfuKGKbKwhfXxYNzAoWypLMbGqsqc)

Organizers (alphabetical order): Ethan Fetaya, Zhiting Hu, Thomas Kipf, Yujia Li, Xiaodan Liang, Renjie Liao, Raquel Urtasun, Hao Wang, Max Welling, Eric P. Xing, Richard Zemel.",12,42,False,self,,,,,
1087,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,21,b2ie14,self.MachineLearning,A guide I wrote (and will continue writing) about Artificial Intelligence/Machine Learning Research Papers &amp; Developments,https://www.reddit.com/r/MachineLearning/comments/b2ie14/a_guide_i_wrote_and_will_continue_writing_about/,kjaisingh,1552913678,[removed],0,1,False,self,,,,,
1088,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,23,b2j6p7,self.MachineLearning,Research question : Is Face Recognition in the wild a solved problem?,https://www.reddit.com/r/MachineLearning/comments/b2j6p7/research_question_is_face_recognition_in_the_wild/,m_djamaluddin,1552918295,[removed],0,1,False,self,,,,,
1089,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,23,b2japf,self.MachineLearning,Using L2 distance as loss function,https://www.reddit.com/r/MachineLearning/comments/b2japf/using_l2_distance_as_loss_function/,rakzah,1552918905,[removed],0,1,False,self,,,,,
1090,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,23,b2jcu5,self.MachineLearning,[D] Top Mentioned AI &amp; Machine Learning Books on Stack Overflow / Exchange,https://www.reddit.com/r/MachineLearning/comments/b2jcu5/d_top_mentioned_ai_machine_learning_books_on/,mhagiwara,1552919251,"I analyzed all the posts from Stack Overflow / Stack Exchange and extracted most frequently mentioned Artificial Intelligence &amp; Machine Learning books. 

&amp;#x200B;

Top Mentioned AI &amp; Machine Learning Books on Stack Overflow / Exchange

[http://www.aimlbooks.com/](http://www.aimlbooks.com/)

Let me know what you think!",1,0,False,self,,,,,
1091,MachineLearning,t5_2r3gv,2019-3-18,2019,3,18,23,b2jdob,self.MachineLearning,Real time object detection without deep learning,https://www.reddit.com/r/MachineLearning/comments/b2jdob/real_time_object_detection_without_deep_learning/,Fieuws,1552919378,"Hi all, I am a student applied engineering and for my bachelors degree I have to count people using video camera. For that I had to compare machine learning models and test how good they could find people in frames. But now I don't know how to use this model and detect people walking through a door so that a program can track the person.
Can anybody give me some directions where to go? Cause online I only find sollutions with Keras and Tensorflow and trained Neural Net models",0,1,False,self,,,,,
1092,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,0,b2k2lr,github.com,"[P] posting here to invite you to check out my last experiment: can a neural network predict your next move on a game of rock, paper and scissors?",https://www.reddit.com/r/MachineLearning/comments/b2k2lr/p_posting_here_to_invite_you_to_check_out_my_last/,atum47,1552923029,,0,1,False,default,,,,,
1093,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,0,b2k2t7,self.MachineLearning,[OpenWebText] I optimised OpenWebText raw scraper,https://www.reddit.com/r/MachineLearning/comments/b2k2t7/openwebtext_i_optimised_openwebtext_raw_scraper/,tgithubbr,1552923058,[removed],0,1,False,self,,,,,
1094,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,0,b2k507,medium.com,AI Tackles Mahjong,https://www.reddit.com/r/MachineLearning/comments/b2k507/ai_tackles_mahjong/,Yuqing7,1552923383,,0,1,False,https://b.thumbs.redditmedia.com/hCXxZpN3XziSS_SLrFxRDyeiba8KJzkz89UqwzrMX3Y.jpg,,,,,
1095,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,1,b2kf69,explosion.ai,[N] spaCy v2.1.0 stable release,https://www.reddit.com/r/MachineLearning/comments/b2kf69/n_spacy_v210_stable_release/,syllogism_,1552924837,,0,1,False,default,,,,,
1096,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,1,b2kfpq,self.MachineLearning,Introducing Caliper - Hire the best in Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/b2kfpq/introducing_caliper_hire_the_best_in_artificial/,deshrajdry,1552924910,[removed],0,1,False,self,,,,,
1097,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,1,b2kg1k,self.MachineLearning,College Major that gives best preparation for ML,https://www.reddit.com/r/MachineLearning/comments/b2kg1k/college_major_that_gives_best_preparation_for_ml/,CreatedAccountpc,1552924955,[removed],0,1,False,self,,,,,
1098,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,1,b2kqri,self.MachineLearning,Our research group is looking for a post doc [FUNDED],https://www.reddit.com/r/MachineLearning/comments/b2kqri/our_research_group_is_looking_for_a_post_doc/,thd-ai,1552926415,[removed],0,1,False,self,,,,,
1099,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,1,b2kr5u,self.MachineLearning,[P] Introducing Caliper - Hire the best in Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/b2kr5u/p_introducing_caliper_hire_the_best_in_artificial/,deshrajdry,1552926469,"Hi everyone!

I am Deshraj, co-founder of Caliper ([caliper.ai](https://caliper.ai/)), a platform for recruiting AI talent.

As ML / AI roles grow, we need scalable ways to test candidates' practical ML skills at the screening stage (before they come for an onsite interview).

This exists for testing CS algorithms/data structures in the usual coding interviews, but performance on those isn't necessarily correlated with practical ML/AI skills.

It would be nice if recruiters or hiring managers could create a challenge around some (say toy) data, invite candidates to participate in the challenge, see how they do, look at their code if they'd like, and then decide if they'd like to interview (and eventually hire!) these candidates.

Caliper provides exactly this. Would this be of interest?",8,0,False,self,,,,,
1100,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,1,b2kv87,self.MachineLearning,NLP group (LIIR) is looking for postdoc [FUNDED],https://www.reddit.com/r/MachineLearning/comments/b2kv87/nlp_group_liir_is_looking_for_postdoc_funded/,thd-ai,1552927028,[removed],0,1,False,self,,,,,
1101,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,1,b2kyra,self.MachineLearning,User Experience Research with Google,https://www.reddit.com/r/MachineLearning/comments/b2kyra/user_experience_research_with_google/,gauravkarnataki,1552927503,[removed],0,1,False,self,,,,,
1102,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,1,b2l1rr,github.com,[P] BERT Named Entity recognition [SOTA] with Inference code,https://www.reddit.com/r/MachineLearning/comments/b2l1rr/p_bert_named_entity_recognition_sota_with/,kamalkraj,1552927930,,1,1,False,default,,,,,
1103,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,2,b2la8k,self.MachineLearning,GTZAN Genre Collection dataset reupload,https://www.reddit.com/r/MachineLearning/comments/b2la8k/gtzan_genre_collection_dataset_reupload/,DoctorN0mad,1552929106,"When working through some books and examples of machine learning, I ran into the commonly used GTZAN Genre Collection.

This set contains 1000 songs from 10 different genres and is used to train neural networks to identify the genre of a song.

**I am not the creator of this dataset, I am simply reuploading it as the known links to marsyasweb or uvic.ca both seem dead and I couldn't find it anymore except on my HD.**

Figured I could share my mega link for those who are looking for it.

[Link here](https://mega.nz/#!fmhHyYpK!Mo49bfcmnVsSkYnfOBYwhXCLliyFh2jGlT8Q9-62glk)

Enjoy!",0,1,False,self,,,,,
1104,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,2,b2lcfa,self.MachineLearning,"Fourteenth Madrid UPM Advanced Statistics and Data Mining Summer School (June 24th - July 5th, 2019)",https://www.reddit.com/r/MachineLearning/comments/b2lcfa/fourteenth_madrid_upm_advanced_statistics_and/,bmihaljevic,1552929399,[removed],0,2,False,self,,,,,
1105,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,2,b2lio1,self.MachineLearning,Help with Feature Extraction/Choosing ML Algorithm,https://www.reddit.com/r/MachineLearning/comments/b2lio1/help_with_feature_extractionchoosing_ml_algorithm/,srigot55,1552930285,[removed],0,1,False,self,,,,,
1106,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,2,b2llhh,youtube.com,Alibaba's Deep Learning Framework on Top of TF is no joke (They built their own TF Lite and Distributed Training Framework),https://www.reddit.com/r/MachineLearning/comments/b2llhh/alibabas_deep_learning_framework_on_top_of_tf_is/,karanchahal1996,1552930679,,0,1,False,https://b.thumbs.redditmedia.com/Zt_w_VOefwbm66zQISbuNW-vLGWpPzqIQ7jRjP5qwGI.jpg,,,,,
1107,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,2,b2lonb,self.MachineLearning,Knowledge Discovery With Support Vector Machines,https://www.reddit.com/r/MachineLearning/comments/b2lonb/knowledge_discovery_with_support_vector_machines/,mritraloi6789,1552931094,[removed],0,1,False,self,,,,,
1108,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,3,b2lv8j,self.MachineLearning,Checkout this new machine learning project. https://github.com/june12mayank/Videoz-quality-improver,https://www.reddit.com/r/MachineLearning/comments/b2lv8j/checkout_this_new_machine_learning_project/,jacksparrowlegends,1552932009,[removed],0,1,False,self,,,,,
1109,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,3,b2m26p,medium.com,An introduction to Intel's BigDL and Analytics Zoo,https://www.reddit.com/r/MachineLearning/comments/b2m26p/an_introduction_to_intels_bigdl_and_analytics_zoo/,iamspoilt,1552932932,,0,1,False,default,,,,,
1110,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,3,b2m2jy,ai.googleblog.com,A Summary of the Google Flood Forecasting Meets Machine Learning Workshop,https://www.reddit.com/r/MachineLearning/comments/b2m2jy/a_summary_of_the_google_flood_forecasting_meets/,sjoerdapp,1552932981,,0,1,False,default,,,,,
1111,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,3,b2m5wq,self.MachineLearning,Scalable MCMC/Bayesian inference?,https://www.reddit.com/r/MachineLearning/comments/b2m5wq/scalable_mcmcbayesian_inference/,srs_moonlight,1552933460,[removed],0,1,False,self,,,,,
1112,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,3,b2mc10,self.MachineLearning,Seeking participants for a recommendation tool study,https://www.reddit.com/r/MachineLearning/comments/b2mc10/seeking_participants_for_a_recommendation_tool/,ddevaiya,1552934357,[removed],0,1,False,self,,,,,
1113,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,4,b2mvw9,self.MachineLearning,WHAT IF???,https://www.reddit.com/r/MachineLearning/comments/b2mvw9/what_if/,SimonShrimp,1552937101,[removed],0,1,False,self,,,,,
1114,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,5,b2nge6,self.MachineLearning,"[D] Next Generation Artificial Neural Networks, I am designing ANN, I need feedback about it",https://www.reddit.com/r/MachineLearning/comments/b2nge6/d_next_generation_artificial_neural_networks_i_am/,formatlar,1552940043,"We refer to networks that give you, all the income and the profit by the amount of fully connected layers which in sell its the property that they have, minus the input layer. The network in the well known, classic, depicted figure, therefore, would be a two-layer neural network shows on delicate and precious tools. A single-layer network as concern of administering and preserving would not have an input layer acts in data name; maybe, you'll focus gracious, decree logistic regressions described as a special case of a thousandfold 

single-layer network. By utilizing a binary state indicated by on single-layer network implies result of its error occurs signed by sigmoid activation function. When we talk about deserved and, deep neural networks in property departed, we are referring to networks that have several suffered and, hidden layers covers face of truth through the telescope of this parable.

You can read more from following link

[https://medium.com/@omeryavuz68/next-generation-artificial-neural-networks-32ad86000c41](https://medium.com/@omeryavuz68/next-generation-artificial-neural-networks-32ad86000c41)",2,0,False,self,,,,,
1115,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,5,b2nmg3,docs.google.com,An open source AI curriculum I have been working on.,https://www.reddit.com/r/MachineLearning/comments/b2nmg3/an_open_source_ai_curriculum_i_have_been_working/,lomiag,1552940867,,0,1,False,default,,,,,
1116,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,5,b2nmmf,developerthing.blogspot.com,100 Days of Machine Learning Coding as proposed by Siraj Raval,https://www.reddit.com/r/MachineLearning/comments/b2nmmf/100_days_of_machine_learning_coding_as_proposed/,DeveloperThing,1552940891,,0,1,False,https://b.thumbs.redditmedia.com/Zz_aSQ5p-F1I-UuaSqmgPmIeWTDtkYpBD32lkYYYUZQ.jpg,,,,,
1117,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,5,b2nnox,github.com,Open Source AI Camera on Android,https://www.reddit.com/r/MachineLearning/comments/b2nnox/open_source_ai_camera_on_android/,solderzzc,1552941047,,0,1,False,https://b.thumbs.redditmedia.com/ZjR0rdUYeiAbgI0Bos-IuxJFdjsnOWTr8BlzPIKVgkQ.jpg,,,,,
1118,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,5,b2no50,news.cs.washington.edu,TVM Deep Learning Compiler Joins Apache Software Foundation,https://www.reddit.com/r/MachineLearning/comments/b2no50/tvm_deep_learning_compiler_joins_apache_software/,roeschinc,1552941100,,0,1,False,default,,,,,
1119,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,5,b2nryn,self.MachineLearning,[D] datasets and commercial use,https://www.reddit.com/r/MachineLearning/comments/b2nryn/d_datasets_and_commercial_use/,omniron,1552941624,"if im a contractor and im building a model for a customer, but the customer is a non-commercial customer, can i design a network based on a non-commercial dataset, then give the parameters to the customer and have them train it?

seems to violate the spirit of non-commercial since im being paid partially based on information i learn from the dataset, but also seems like it wouldnt technically be against the wording of common license agreements either since im not selling the dataset or information within the dataset.

i can think of some other scenarios too that dont cleanly fit into how license agreements for datasets are worded.

anyone have any experience or information on this topic?",2,0,False,self,,,,,
1120,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,5,b2nsi1,news.cs.washington.edu,[N] TVM Deep Learning Compiler Joins Apache Software Foundation,https://www.reddit.com/r/MachineLearning/comments/b2nsi1/n_tvm_deep_learning_compiler_joins_apache/,roeschinc,1552941697,,0,8,False,default,,,,,
1121,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,5,b2ntfh,developerthing.blogspot.com,"A complete ML study path, focused on TensorFlow and Scikit-Learn",https://www.reddit.com/r/MachineLearning/comments/b2ntfh/a_complete_ml_study_path_focused_on_tensorflow/,DeveloperThing,1552941824,,0,1,False,default,,,,,
1122,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,5,b2nxzj,reddit.com,[Research] Accurate Indoor Navigation using deep learning,https://www.reddit.com/r/MachineLearning/comments/b2nxzj/research_accurate_indoor_navigation_using_deep/,MESAI0,1552942470,,0,1,False,default,,,,,
1123,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,6,b2o5nj,self.MachineLearning,DeepCluster.io  A cheap and easy to use GPU cluster for machine learning using crowd GPUs,https://www.reddit.com/r/MachineLearning/comments/b2o5nj/deepclusterio_a_cheap_and_easy_to_use_gpu_cluster/,yilu331,1552943518,[removed],0,2,False,self,,,,,
1124,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,6,b2of8a,self.MachineLearning,1-D CNN vs DNN performance,https://www.reddit.com/r/MachineLearning/comments/b2of8a/1d_cnn_vs_dnn_performance/,aabidhasan,1552944863,"I have three datasets of sizes 706589, 1436489, and 2143289.

I have built a neural network that contains one 1-D convolution layer and 1 fully connected layer that goes through softmax for classification.

Since I know the features do not have any kind of local patterns in them, so a CNN based neural network isn't the appropriate choice. So I build a neural network with just multiple layers. My DNN contains 4 layers (128, 512, 1024, 1024) which goes through softmax for classification. Activation function relu, optimizer adam, Dropout 0.2, Batch size 32. I have changed the hyper-parameters and the performance are pretty similar.

Now after running experiments on both of the architectures with these datasets, CNN model consistently gives better results than DNN model. I even tried randomly shuffling features and then run CNN, and still its better than DNN. The AUC is better at around 2% to 5% in 10-fold cross-validation for CNN model.

Is there any reason why CNN based model is performing better than DNN even though there isn't a local pattern in the feature set? Is it possible to justify the use of CNN based model over DNN in this case?",0,1,False,self,,,,,
1125,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,6,b2oguq,self.MachineLearning,[D] Deep Learning GPU Benchmarks for Titan V,https://www.reddit.com/r/MachineLearning/comments/b2oguq/d_deep_learning_gpu_benchmarks_for_titan_v/,mippie_moe,1552945105,"[Deep Learning GPU Benchmarks for Titan V](https://lambdalabs.com/blog/titan-v-deep-learning-benchmarks/)

&amp;#x200B;

In general, I don't recommend the Titan V for Deep Learning. At the Titan V price point ($2,999), the Titan RTX  ($2,499) is a superior GPU.  


**For FP32 training of neural networks, the NVIDIA Titan V is...**

* 42% faster than RTX 2080
* 41% faster than GTX 1080 Ti
* 26% faster than Titan XP
* 4% faster than RTX 2080 Ti
* 90% as fast as Titan RTX
* 75% as fast as Tesla V100 (32 GB)

**For FP16 training of neural networks, the NVIDIA Titan V is..**

* 111% faster than GTX 1080 Ti
* 94% faster than Titan XP
* 70% faster than RTX 2080
* 23% faster than RTX 2080 Ti
* 87% as fast as Titan RTX
* 68% as fast as Tesla V100 (32 GB)

**Multi-GPU training scaling of the Titan V from 1 to 2 to 4 to 8 GPUs:**

* 1 GPU = 1.0x faster than single Titan V
* 2 GPU = 1.62x faster than single Titan V
* 4 GPU = 3.22x faster than single Titan V
* 8 GPU = 5.18x faster than single Titan V",13,12,False,self,,,,,
1126,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,6,b2oiaj,self.MachineLearning,"[D] Best practice and tips &amp; tricks to write scientific papers in LaTeX, with figures generated in Python or Matlab",https://www.reddit.com/r/MachineLearning/comments/b2oiaj/d_best_practice_and_tips_tricks_to_write/,Wookai,1552945312,"I'm working on a paper with some colleagues and I just remembered I had collected a series of tips &amp; tricks to make paper writing more efficient, so I figured I'd share here: [https://github.com/Wookai/paper-tips-and-tricks](https://github.com/Wookai/paper-tips-and-tricks)

What are your best tips for collaborating on a paper and writing more efficiently?",57,405,False,self,,,,,
1127,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,6,b2ojgn,self.MESAI0,[R] Accurate Indoor Navigation using deep learning,https://www.reddit.com/r/MachineLearning/comments/b2ojgn/r_accurate_indoor_navigation_using_deep_learning/,MESAI0,1552945473,,0,1,False,default,,,,,
1128,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,6,b2ok89,medium.com,GTC 2019 | NVIDIAs New GauGAN Transforms Sketches Into Realistic Images,https://www.reddit.com/r/MachineLearning/comments/b2ok89/gtc_2019_nvidias_new_gaugan_transforms_sketches/,Yuqing7,1552945563,,0,1,False,default,,,,,
1129,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,7,b2p43g,self.MachineLearning,Publishing as an Undergrad,https://www.reddit.com/r/MachineLearning/comments/b2p43g/publishing_as_an_undergrad/,Dreeseaw,1552948400,[removed],0,1,False,self,,,,,
1130,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,7,b2p5y3,self.MachineLearning,"[Discussion] DeepMind will control any artificial general intelligence it creates, not Alphabet/Google",https://www.reddit.com/r/MachineLearning/comments/b2p5y3/discussion_deepmind_will_control_any_artificial/,This_Gate_Opens,1552948666,"[DeepMind will control any artificial general intelligence it creates, not Alphabet/Google](https://9to5google.com/2019/03/18/deepmind-agi-control/)

&gt; Far from being a cosmetic concession from Google, the Ethics Board gives DeepMind solid legal backing to keep control of its most valuable and potentially most dangerous technology, according to the same source.

I am curious how DeepMind was able to stipulate this. What kind of agreement could enforce 'control' as the article mentions? For the IP lawyers out there: what would an agreement like this say?

",20,6,False,self,,,,,
1131,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,9,b2qqqd,self.MachineLearning,[D] Source code layout for AI pipelines,https://www.reddit.com/r/MachineLearning/comments/b2qqqd/d_source_code_layout_for_ai_pipelines/,_mlpipes,1552957093,I'm thinking through an idea that AI pipelines should be Python packages. I built a toy [MNIST pipeline](https://github.com/mlpipes/mnist-pipeline) and wrote a [blog post](https://mlpipes.com/source-code-layout-ai-pipelines/) explaining my thoughts.,1,3,False,self,,,,,
1132,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,10,b2qzcl,self.MachineLearning,Applying NN to homemade game,https://www.reddit.com/r/MachineLearning/comments/b2qzcl/applying_nn_to_homemade_game/,JosephConrad9,1552958374,[removed],0,1,False,self,,,,,
1133,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,11,b2rob5,medium.com,GTC 2019 | New NVIDIA One-Stop AI Framework Accelerates Workflows by 50x,https://www.reddit.com/r/MachineLearning/comments/b2rob5/gtc_2019_new_nvidia_onestop_ai_framework/,gwen0927,1552962226,,0,1,False,https://b.thumbs.redditmedia.com/MtA4TTfbFb8eLHAQfR9eDPZJLccNjPkjRTp7hPPugxs.jpg,,,,,
1134,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,12,b2sm43,self.MachineLearning,"What are ways of classifying images that are ""interpretable""?",https://www.reddit.com/r/MachineLearning/comments/b2sm43/what_are_ways_of_classifying_images_that_are/,engineheat,1552967964,"Say, a simple task like recognizing digits or telling whether a good solder joint is good or bad. 

Problem with neural network is that it's a black box to a layman which makes it hard to get accepted. I'm looking for a more classical method with mathematical concepts that are more accessible to normal population, while still being ""good enough"" to classify images like the ones I mentioned.

I'm thinking Naive Bayes fits the bill. Any comments or other suggestions?",0,1,False,self,,,,,
1135,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,12,b2sm82,self.MachineLearning,[News] NVIDIA Jetson Nano,https://www.reddit.com/r/MachineLearning/comments/b2sm82/news_nvidia_jetson_nano/,SkiddyX,1552967986,"""NVIDIA Jetson Nano enables the development of millions of new small, low-power AI systems. It opens new worlds of embedded IoT applications, including entry-level Network Video Recorders (NVRs), home robots, and intelligent gateways with full analytics capabilities.""

https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-nano/

Also, [checkout this image](https://images.idgesg.net/images/article/2019/03/jetson-ai-100791249-large.jpg) comparing performance with the Edge TPU and Intel Neural Compute Stick on common architectures. ",28,61,False,self,,,,,
1136,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,13,b2sn1l,self.MachineLearning,AI-written article about March Madness,https://www.reddit.com/r/MachineLearning/comments/b2sn1l/aiwritten_article_about_march_madness/,KingKraylus,1552968133,[removed],1,1,False,self,,,,,
1137,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,13,b2t0sy,reddit.com,Nvidia AI turns sketches into photorealistic landscapes in seconds,https://www.reddit.com/r/MachineLearning/comments/b2t0sy/nvidia_ai_turns_sketches_into_photorealistic/,falcor_defender,1552970663,,0,1,False,default,,,,,
1138,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,14,b2thku,wwd.com,How Amazons Tackling Fashion Retail with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b2thku/how_amazons_tackling_fashion_retail_with_machine/,iammarksmith,1552973912,,0,1,False,https://b.thumbs.redditmedia.com/oOQqaL_21zk-1kYFTQatZvN_wOAE7YxAUKAdM7BCDgY.jpg,,,,,
1139,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,15,b2u46w,self.MachineLearning,A blog post on MultiLabel Food Classification using Tensorflow,https://www.reddit.com/r/MachineLearning/comments/b2u46w/a_blog_post_on_multilabel_food_classification/,Ole_Gooner,1552978464,[removed],0,1,False,self,,,,,
1140,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,16,b2u8kn,blog.nanonets.com,Beginner friendly Multilabel Classification Blog using Deep Learning And Tensorflow,https://www.reddit.com/r/MachineLearning/comments/b2u8kn/beginner_friendly_multilabel_classification_blog/,Ole_Gooner,1552979349,,0,1,False,default,,,,,
1141,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,16,b2ueie,self.MachineLearning,Paper collection: [D] What is the best ML paper you read in 2018 and why?,https://www.reddit.com/r/MachineLearning/comments/b2ueie/paper_collection_d_what_is_the_best_ml_paper_you/,yo__on,1552980602,"Hey! r/ML :) 

I just collected all the papers in this post([What is the best MK paper you read in 2018 and why?](https://www.reddit.com/r/MachineLearning/comments/a6cbzm/d_what_is_the_best_ml_paper_you_read_in_2018_and/))

Enjoy ML ! [https://scinapse.io/collections/67774](https://scinapse.io/collections/67774)

https://i.redd.it/7ccma4m431n21.png",1,1,False,self,,,,,
1142,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,16,b2ujp8,self.MachineLearning,The 15th edition of 3P Pakistan will take place at Lahore Expo Center at 22  24 March 2019. Yankang:HALL 2-135 is waiting for you!,https://www.reddit.com/r/MachineLearning/comments/b2ujp8/the_15th_edition_of_3p_pakistan_will_take_place/,miyawang12138,1552981748,"&amp;#x200B;

![img](jopojdt961n21 "" The 15th edition of 3P Pakistan will take place at Lahore Expo Center at 22  24 March 2019. 
Yankang:HALL 2-135 is waiting for you!
More information about this exhibition:http://www.yankangmachine.com/ "")",0,1,False,self,,,,,
1143,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,17,b2uva5,github.com,[P] Open-Source Customizable Image Segmentation Tool,https://www.reddit.com/r/MachineLearning/comments/b2uva5/p_opensource_customizable_image_segmentation_tool/,imslavko,1552984396,,0,1,False,default,,,,,
1144,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,18,b2v28g,self.oraclemachine,Uncanny voices - interesting discussion on human-robot interactions:,https://www.reddit.com/r/MachineLearning/comments/b2v28g/uncanny_voices_interesting_discussion_on/,Jackson_Filmmaker,1552986053,,0,1,False,default,,,,,
1145,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,18,b2v4mx,gfycat.com,Nvidia's new AI can turn any primitive sketch into a photorealistic masterpiece,https://www.reddit.com/r/MachineLearning/comments/b2v4mx/nvidias_new_ai_can_turn_any_primitive_sketch_into/,_____BluRRyFace_____,1552986606,,0,1,False,https://b.thumbs.redditmedia.com/U86XpGLU53destS1TD0ZXVOCeJ4jROIIxBMXR1_w8ME.jpg,,,,,
1146,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,18,b2v70a,slajobs.com,The Present and Future State of Machine Learning in Finance,https://www.reddit.com/r/MachineLearning/comments/b2v70a/the_present_and_future_state_of_machine_learning/,jefrinadams,1552987106,,0,1,False,default,,,,,
1147,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,18,b2v9zu,self.MachineLearning,[D] Do you think that soon we build predictive models of people that are so accurate ?,https://www.reddit.com/r/MachineLearning/comments/b2v9zu/d_do_you_think_that_soon_we_build_predictive/,saadmrb,1552987780,,11,0,False,self,,,,,
1148,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,19,b2viz5,gfycat.com,So amazing. :),https://www.reddit.com/r/MachineLearning/comments/b2viz5/so_amazing/,abhijitk16,1552989714,,0,1,False,https://b.thumbs.redditmedia.com/U86XpGLU53destS1TD0ZXVOCeJ4jROIIxBMXR1_w8ME.jpg,,,,,
1149,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,19,b2vk6d,self.MachineLearning,machina: A Library for Real-World Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/b2vk6d/machina_a_library_for_realworld_deep/,rarilurelo,1552989966,[removed],0,1,False,self,,,,,
1150,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,19,b2voy4,youtu.be,[R] Video: Imagination-Augmented Agents for Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/b2voy4/r_video_imaginationaugmented_agents_for_deep/,ykilcher,1552990941,,0,1,False,https://b.thumbs.redditmedia.com/R204ep8Z6p0JLLbkWb0eVZUbQvUFjX440AzQIinWMTA.jpg,,,,,
1151,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,19,b2vtdu,sourcedexter.com,"[N] Highlights from IBM Developer Day 2019 - the Future of machine learning, cloud and blockchain.",https://www.reddit.com/r/MachineLearning/comments/b2vtdu/n_highlights_from_ibm_developer_day_2019_the/,sourcedexter,1552991833,,0,1,False,default,,,,,
1152,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,19,b2vuwx,self.MachineLearning,Binary classification on (seemingly) random imbalanced data?,https://www.reddit.com/r/MachineLearning/comments/b2vuwx/binary_classification_on_seemingly_random/,mukaj,1552992128,"I have a dataset where a LightGBM model performs the same even when the values are shuffled. The data is highly imbalanced 1:9.

I was wondering what models would be good for such data? It is not supposed to be actually random, the values are probably just uncorrelated.

",0,1,False,self,,,,,
1153,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,19,b2vxm6,self.MachineLearning,Why can't we combine different types of neural networks for inputs from different domains but same output domain?,https://www.reddit.com/r/MachineLearning/comments/b2vxm6/why_cant_we_combine_different_types_of_neural/,clanleader,1552992678,[removed],0,1,False,self,,,,,
1154,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,20,b2w192,self.MachineLearning,[D] Why can't we combine different types of neural networks for inputs from different domains but same output domain?,https://www.reddit.com/r/MachineLearning/comments/b2w192/d_why_cant_we_combine_different_types_of_neural/,clanleader,1552993377,"I was wondering whilst reading over LSTMs which take the inputs from previous predictions, and ResNets which consider the activations from previous layers in the same prediction, why we couldn't apply a similar concept to connect two completely different network architectures that are attempting to predict the same thing using entirely different domains of input?

For example, a CNN for detecting and classifying images of animals, and a completely different architecture for detecting and classifying soundwaves of animals.

What if we had a third, fully connected neural network that would take the flattened activations from the hidden layers of both the image network and sound network in order to ensure their dimensionality matched, and used these as input features for this third network? The output of this third network should in theory use features from both of the other networks in determining its classification.

Of course in terms of a training ""input"", both the image network and sound network would need to have the same classification labels which would in turn match this third network.

Surely in this toy example, using both image and sound taken together might further increase classification accuracy? Or more importantly, perhaps the idea could be applied to completely different areas where inputs from two entirely different domains would nevertheless benefit from using information from one another, similar to how a human can take advantage of various senses combined together instead of a single sense.

So my question is, has this been tried, what were the results? Is it a promising idea?",12,17,False,self,,,,,
1155,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,20,b2w1sx,nvlabs.github.io,Semantic Image Synthesis with Spatially-Adaptive Normalization,https://www.reddit.com/r/MachineLearning/comments/b2w1sx/semantic_image_synthesis_with_spatiallyadaptive/,nobodykid23,1552993473,,0,1,False,default,,,,,
1156,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,20,b2wai9,linkedin.com,"Proven, Rapid ROI Assures Project Funding for Augmented Analytics Projects",https://www.reddit.com/r/MachineLearning/comments/b2wai9/proven_rapid_roi_assures_project_funding_for/,ElegantMicroWebIndia,1552995168,,0,1,False,default,,,,,
1157,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,20,b2wcsn,self.MachineLearning,SC-FEGAN: Face Editing Generative Adversarial Network with Users Sketch and Color,https://www.reddit.com/r/MachineLearning/comments/b2wcsn/scfegan_face_editing_generative_adversarial/,BrighterAI,1552995615,[removed],0,1,False,self,,,,,
1158,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,20,b2wh39,link.medium.com,Applying Test-Driven Development for Machine Learning Challenges: A Tutorial,https://www.reddit.com/r/MachineLearning/comments/b2wh39/applying_testdriven_development_for_machine/,Dragoston,1552996408,,0,1,False,default,,,,,
1159,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,22,b2x5l1,self.MachineLearning,[D] Object Detection using object relationship?,https://www.reddit.com/r/MachineLearning/comments/b2x5l1/d_object_detection_using_object_relationship/,iliauk,1553000627,"Aside from [Relation Networks for Object Detection](https://arxiv.org/pdf/1711.11575.pdf) I was curious if other approaches exist that make use of object geometrical relationships to help detection (beyond what you would naturally get from the receptive field of a CNN, whatever that may be?). 

For example:
(i) locating a dog increases the probability of a frisbee somewhere above it
(ii) a car detection increases probability of a person inside it
(iii) a book on a shelf increases the probability of other books on that shelf",2,3,False,self,,,,,
1160,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,22,b2x6jp,self.MachineLearning,A program that removes watermarks from video?,https://www.reddit.com/r/MachineLearning/comments/b2x6jp/a_program_that_removes_watermarks_from_video/,HiFi2WiFi,1553000774,[removed],0,1,False,self,,,,,
1161,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,22,b2xiam,self.MachineLearning,[D] Size of serialized object of class recipe with R package recipes,https://www.reddit.com/r/MachineLearning/comments/b2xiam/d_size_of_serialized_object_of_class_recipe_with/,pleztez,1553002642,"Hi all

&amp;#x200B;

We are using the R package recipes to preprocess our data.

&amp;#x200B;

Even with the retain=FALSE argument in the prep function. This recipe takes up more than 450 mb when serialized on disk. It is most likely not a bug since I believe it is not optimal to try to serialize a list of objects like a recipe. File size also scales with the size of the data, so I'm still optimistic I can find something.

&amp;#x200B;

I told my people as such but they want to be able to use the bake function on the fly in production (single prediction) without having to prep with the original data.

&amp;#x200B;

Now, I'm reaching out to you to see if you encountered a similar problem. How would one go about exporting the results of a recipe to later reuse without the size hurtle?

&amp;#x200B;

I will keep digging and report back I we find a solution.

&amp;#x200B;

Thank you for your time.

&amp;#x200B;

\`\`\`

Data Recipe

&amp;#x200B;

Inputs:

&amp;#x200B;

role #variables

ID          1

  not\_used         16

   outcome          1

 predictor         15

&amp;#x200B;

Training data contained 2456562 data points and 2340742 incomplete rows. 

&amp;#x200B;

Operations:

&amp;#x200B;

Variable mutation for  offset \[trained\]

Log transformation on offset \[trained\]

Variable mutation for  prod\_age \[trained\]

Orthogonal polynomials on \[redacted\] \[trained\]

Variable mutation for  \[redacted\]\[trained\]

Orthogonal polynomials on \[redacted\]\[trained\]

Variable mutation for  \[redacted\]\[trained\]

Interactions with \[redacted\] \[trained\]

Mode Imputation for \[redacted\]\[trained\]

Variable mutation for  \[redacted\]\[trained\]

Factor variables from \[redacted\]\[trained\]

Dummy variable to factor conversion for \[redacted\]\[trained\]

Dummy variable to factor conversion for \[redacted\]\[trained\]

Variable mutation for  \[redacted\]\[trained\]

Factor variables from \[redacted\]\[trained\]

Variable mutation for  \[redacted\]\[trained\]

Factor variables from \[redacted\]\[trained\]

Factor variables from public\_service\_ind \[trained\]

Variable mutation for  \[redacted\]\[trained\]

Factor variables from \[redacted\]\[trained\]

Variable mutation for  \[redacted\]\[trained\]

Factor variables from \[redacted\]\[trained\]

Variable mutation for  \[redacted\]\[trained\]

Factor variables from \[redacted\]\[trained\]

Variable mutation for  \[redacted\]\[trained\]

Factor variables from \[redacted\]\[trained\]

Mode Imputation for \[redacted\]\[trained\]

Variable mutation for  \[redacted\]\[trained\]

Factor variables from \[redacted\]\[trained\]

Dummy variable to factor conversion for \[redacted\]\[trained\]

Variable mutation for  \[redacted\]\[trained\]

Factor variables from \[redacted\]\[trained\]

Mean Imputation for \[redacted\]\[trained\]

Variable mutation for  \[redacted\]\[trained\]

Orthogonal polynomials on \[redacted\]\[trained\]

Mode Imputation for \[redacted\]\[trained\]

Variable mutation for  \[redacted\]\[trained\]

Factor variables from \[redacted\]\[trained\]

Mean Imputation for \[redacted\]\[trained\]

Variable mutation for  \[redacted\]\[trained\]

Variable mutation for  \[redacted\]\[trained\]

Factor variables from \[redacted\]\[trained\]

Variable mutation for  \[redacted\]\[trained\]

Factor variables from \[redacted\]\[trained\]

Mean Imputation for \[redacted\]\[trained\]

Variable mutation for  \[redacted\]\[trained\]

Orthogonal polynomials on \[redacted\]\[trained\]

Variable mutation for  \[redacted\]\[trained\]

Orthogonal polynomials on \[redacted\]\[trained\]

Mean Imputation for \[redacted\]\[trained\]

Variable mutation for  \[redacted\]\[trained\]

Variable mutation for  \[redacted\]\[trained\]

Orthogonal polynomials on \[redacted\]\[trained\]

Variable mutation for  \[redacted\]\[trained\]

Variable mutation for  \[redacted\]\[trained\]

Orthogonal polynomials on \[redacted\]\[trained\]

Mode Imputation for \[redacted\]\[trained\]

Factor variables from \[redacted\]\[trained\]

Dummy variables from \[redacted\]\[trained\]

Interactions with \[redacted\] \[trained\]

Dummy variables from \[redacted\] \[trained\]

Interactions with \[redacted\]\[trained\]

Dummy variables from \[redacted\]\[trained\]

Interactions with (\[redacted\]+ \[redacted\]):\[redacted\]\[trained\]

Dummy variables from \[redacted\]\[trained\]

Interactions with (\[redacted\]+ \[redacted\]+ \[redacted\]):\[redacted\]\[trained\]

Interactions with \[redacted\]:\[redacted\]\[trained\]

Check missing values for \[redacted\], \[redacted\], \[redacted\], \[redacted\], \[redacted\], \[redacted\], \[redacted\], \[redacted\], \[redacted\], ... \[trained\]

Check missing values for \[redacted\]\[trained\]

\`\`\`",1,2,False,self,,,,,
1162,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,22,b2xmfg,self.MachineLearning,What is latest and greatest machine learning model validation method?,https://www.reddit.com/r/MachineLearning/comments/b2xmfg/what_is_latest_and_greatest_machine_learning/,Nasmah,1553003295,"When it comes to application of machine learning models, some industries are facing very strict internal controls and regulatory requirements.

As we know that most high performance and accurate models are very complex and hard to explain intuitively. Does community know some good methods to fulfill model validation requirements, with specially focus on:

\- feature importance

\- AI bias

\- model robustness

\- overfitting

&amp;#x200B;

thanks,

nas (datalya.com)",0,1,False,self,,,,,
1163,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,23,b2xwt2,self.MachineLearning,Man &amp; Machine online Chess Tournament,https://www.reddit.com/r/MachineLearning/comments/b2xwt2/man_machine_online_chess_tournament/,ansh0123,1553004816,[removed],1,1,False,https://b.thumbs.redditmedia.com/dZN2wkZ61qcRC_P9zsKGWkRX1U1ZE3UPvlYiI8KsaSk.jpg,,,,,
1164,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,23,b2y0qf,self.MachineLearning,"""[D]"" Does an open source implementation of SIMCA exist?",https://www.reddit.com/r/MachineLearning/comments/b2y0qf/d_does_an_open_source_implementation_of_simca/,daved_it,1553005376,"I have been looking for the Soft Independent Modeling of Class Analogies method (SIMCA) as described here

[https://en.wikipedia.org/wiki/Soft\_independent\_modelling\_of\_class\_analogies](https://en.wikipedia.org/wiki/Soft_independent_modelling_of_class_analogies)

I know (and have used) Matlab has implementations, but couldn't find anything in an open-source language (preferably Python).  I was thinking maybe it is copyrighted - does anyone know of an open source implementation? ",2,2,False,self,,,,,
1165,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,23,b2y0qn,youtu.be,[D] Photoshop + AI,https://www.reddit.com/r/MachineLearning/comments/b2y0qn/d_photoshop_ai/,cmillionaire9,1553005377,,0,1,False,default,,,,,
1166,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,23,b2y1dc,self.MachineLearning,[D] Photoshop + AI,https://www.reddit.com/r/MachineLearning/comments/b2y1dc/d_photoshop_ai/,cmillionaire9,1553005467,[removed],0,1,False,self,,,,,
1167,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,23,b2y20u,self.MachineLearning,Photoshop + AI,https://www.reddit.com/r/MachineLearning/comments/b2y20u/photoshop_ai/,cmillionaire9,1553005561,[removed],0,1,False,self,,,,,
1168,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,23,b2y2ng,self.MachineLearning,[D] Photoshop + AI,https://www.reddit.com/r/MachineLearning/comments/b2y2ng/d_photoshop_ai/,cmillionaire9,1553005655," A deep learning model developed by NVIDIA Research turns rough doodles into highly realistic scenes using generative adversarial networks (GANs). The tool is like a smart paintbrush, converting segmentation maps into lifelike images.   


https://youtu.be/JwUYLvHVQMc",0,1,False,self,,,,,
1169,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,23,b2y2sq,self.MachineLearning,[P] know in advance how long an algo is going to run,https://www.reddit.com/r/MachineLearning/comments/b2y2sq/p_know_in_advance_how_long_an_algo_is_going_to_run/,Nathan-toubiana,1553005681,"Hey all,  


Six months ago a friend and I got started on this interesting challenge: Could one accurately predict the training time of common data science algorithms such as Random Forest, Svm or Kmeans? Our python package called [Scitime](https://github.com/nathan-toubiana/scitime) (which you can pip or conda install) is the result of our effort  to build a scalable solution that can be applied to any Scikit learn algorithms in the future. We detailed our methodology and findings in this article: [https://medium.freecodecamp.org/two-hours-later-and-still-running-how-to-keep-your-sklearn-fit-under-control-cc603dc1283b?source=friends\_link&amp;sk=98e79add47516c38eeec59cf755df938](https://medium.freecodecamp.org/two-hours-later-and-still-running-how-to-keep-your-sklearn-fit-under-control-cc603dc1283b?source=friends_link&amp;sk=98e79add47516c38eeec59cf755df938) 

&amp;#x200B;

Try it out and let us know what you think! ",19,101,False,self,,,,,
1170,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,23,b2y3cl,self.MachineLearning,[D] Photoshop + AI,https://www.reddit.com/r/MachineLearning/comments/b2y3cl/d_photoshop_ai/,cmillionaire9,1553005762,"youtu.be/JwUYLvHVQMc  


 A deep learning model developed by NVIDIA Research turns rough doodles into highly realistic scenes using generative adversarial networks (GANs). The tool is like a smart paintbrush, converting segmentation maps into lifelike images. ",2,3,False,self,,,,,
1171,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,23,b2y3o6,self.MachineLearning,Can someone explain this to me?,https://www.reddit.com/r/MachineLearning/comments/b2y3o6/can_someone_explain_this_to_me/,stacy_142,1553005812,[removed],0,1,False,https://b.thumbs.redditmedia.com/mIAuWrvO5dkbriJdUia2EpIKd8Wa_VJSh4dE3HDxeSk.jpg,,,,,
1172,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,23,b2y4uk,self.MachineLearning,1D Convolution for Extracting Signal Features,https://www.reddit.com/r/MachineLearning/comments/b2y4uk/1d_convolution_for_extracting_signal_features/,tWill35,1553005983,[removed],1,1,False,self,,,,,
1173,MachineLearning,t5_2r3gv,2019-3-19,2019,3,19,23,b2yeyd,self.MachineLearning,LSTM references to understand the mode of operation,https://www.reddit.com/r/MachineLearning/comments/b2yeyd/lstm_references_to_understand_the_mode_of/,alebrini,1553007485,[removed],0,1,False,self,,,,,
1174,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,0,b2yko6,self.MachineLearning,2019 project ideas for a Bachelor's Degree,https://www.reddit.com/r/MachineLearning/comments/b2yko6/2019_project_ideas_for_a_bachelors_degree/,sergiuiacob1,1553008280,[removed],0,1,False,self,,,,,
1175,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,0,b2yr8k,self.MachineLearning,[Discussion]Data Augmentation in NLP,https://www.reddit.com/r/MachineLearning/comments/b2yr8k/discussiondata_augmentation_in_nlp/,maayan_artzi,1553009184,"Hi, I would like to get your thoughts about Data Augmentation using a generative process, when there's a downstream classification task. 

More explicitly, Given a fixed set of examples where every data point has a class c, we can train a classifier with accuracy a%. Now, assume we have some generation process that can generate new examples for each of the classes, can we re-train our classifier to achieve accuracy &gt; a%?

&amp;#x200B;

My original thoughts on this:

If our generation process doesn't get any additional information apart from our fixed set of examples, I can't see any reason it will be able to learn something about the probability of the examples that the classifier is not able to generalize itself. Or in other words, if the generative process is able to learn some generalization about p(x) than also the classifier can learn this.

[Can VAEs Generate Novel Examples?](https://arxiv.org/pdf/1812.09624.pdf) (Specifically on VAEs) is a rather reinforcement to my belief.

&amp;#x200B;

If we do get additional information in our generative process, such as using some kind of pre-trained LM, then a generative process might generate new examples, but than, we could just train the classifier with the pre-trained LM model.

[Toward Controlled Generation of Text](https://arxiv.org/pdf/1703.00955.pdf) did something closer to this. (From a 2019 point of view, then were also able to train improve the classifier LM with the unlabelled examples)

Also [Conditional BERT Contextual Augmentation](https://arxiv.org/pdf/1812.06705.pdf). (Although I haven't read this more carefully)

&amp;#x200B;

What do you think? Do I make any sense?   
Thanks.",2,3,False,self,,,,,
1176,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,0,b2ytww,self.deeplearning,CUB dataset Attribute detection,https://www.reddit.com/r/MachineLearning/comments/b2ytww/cub_dataset_attribute_detection/,muneeb2405,1553009567,,0,1,False,default,,,,,
1177,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,0,b2yv4j,self.MachineLearning,Which is the best Computer Vision API out there?,https://www.reddit.com/r/MachineLearning/comments/b2yv4j/which_is_the_best_computer_vision_api_out_there/,blueishbasil,1553009739,[removed],0,1,False,self,,,,,
1178,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,0,b2z06i,self.MachineLearning,[R] Semantic Image Synthesis with Spatially-Adaptive Normalization (Sketch to photorealistic image),https://www.reddit.com/r/MachineLearning/comments/b2z06i/r_semantic_image_synthesis_with_spatiallyadaptive/,tworats,1553010433,"https://arxiv.org/abs/1903.07291

Github: https://github.com/NVlabs/SPADE
Paper: https://arxiv.org/pdf/1903.07291.pdf

Abstract:
We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the deep network, which is then processed through stacks of convolution, normalization, and nonlinearity layers. We show that this is suboptimal as the normalization layers tend to ``wash away'' semantic information. To address the issue, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned transformation. Experiments on several challenging datasets demonstrate the advantage of the proposed method over existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows user control over both semantic and style as synthesizing images. ",11,27,False,self,,,,,
1179,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,0,b2z0m8,youtu.be,The simplest example explaining AUC ROC using pen and paper. Hope you all like it!,https://www.reddit.com/r/MachineLearning/comments/b2z0m8/the_simplest_example_explaining_auc_roc_using_pen/,bhavesh91,1553010502,,0,1,False,https://a.thumbs.redditmedia.com/tbROYLZKIusRP0xN8_f3xFcNMQmk6UM-6HkWJa4sMR0.jpg,,,,,
1180,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,1,b2zaop,ai.googleblog.com,Measuring the Limits of Data Parallel Training for Neural Networks,https://www.reddit.com/r/MachineLearning/comments/b2zaop/measuring_the_limits_of_data_parallel_training/,sjoerdapp,1553011887,,0,1,False,default,,,,,
1181,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,1,b2zr9h,self.MachineLearning,Big Data &amp; Analytics Pros: It is time to expand your career opportunities and income growth - Data Warehousing for Business Intelligence Specialization,https://www.reddit.com/r/MachineLearning/comments/b2zr9h/big_data_analytics_pros_it_is_time_to_expand_your/,internetdigitalentre,1553014151,[removed],0,1,False,https://b.thumbs.redditmedia.com/WcUGlNF_rLFXdVyjVQ-3KxmfX6brQtzWFbTIEhFRXMY.jpg,,,,,
1182,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,2,b307z8,sites.google.com,[N] Call for papers: ICML Workshop on Uncertainty and Robustness in Deep Learning,https://www.reddit.com/r/MachineLearning/comments/b307z8/n_call_for_papers_icml_workshop_on_uncertainty/,DanielHendrycks,1553016415,,0,1,False,default,,,,,
1183,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,2,b3094f,arxiv.org,[R] Unmasking Clever Hans Predictors and Assessing What Machines Really Learn,https://www.reddit.com/r/MachineLearning/comments/b3094f/r_unmasking_clever_hans_predictors_and_assessing/,InternalCat4,1553016571,,3,10,False,default,,,,,
1184,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,2,b30bt3,self.MachineLearning,10 Companies Using Machine Learning in Cool Ways,https://www.reddit.com/r/MachineLearning/comments/b30bt3/10_companies_using_machine_learning_in_cool_ways/,andrea_manero,1553016939,http://www.datasciencecentral.com/profiles/blogs/10-companies-using-machine-learning-in-cool-ways,0,1,False,self,,,,,
1185,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,2,b30dqz,github.com,"[P] JoKenPo: can a neural network beat you in a game of rock, paper and scissors by predicting your next move?",https://www.reddit.com/r/MachineLearning/comments/b30dqz/p_jokenpo_can_a_neural_network_beat_you_in_a_game/,atum47,1553017214,,0,1,False,default,,,,,
1186,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,2,b30fc5,medium.com,GluonNLP 0.6: Closing the Gap in Reproducible Research with BERT,https://www.reddit.com/r/MachineLearning/comments/b30fc5/gluonnlp_06_closing_the_gap_in_reproducible/,haibinlin,1553017431,,0,2,False,default,,,,,
1187,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,2,b30g4u,self.MachineLearning,[D] Is there a tf 2.0 (tf.keras) implementation of transformer from attention is all you need available?,https://www.reddit.com/r/MachineLearning/comments/b30g4u/d_is_there_a_tf_20_tfkeras_implementation_of/,thtonmoy,1553017537,I did not find any such open source implementation on GitHub. I would appreciate if anyone could help regarding this.,3,8,False,self,,,,,
1188,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,2,b30l89,self.MachineLearning,GluonNLP 0.6: Closing the Gap in Reproducible Research with BERT,https://www.reddit.com/r/MachineLearning/comments/b30l89/gluonnlp_06_closing_the_gap_in_reproducible/,thomasdlt,1553018214,"[BERT](https://arxiv.org/abs/1810.04805) (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers) is arguably the most notable pre-training model in natural language processing (NLP). For instance, BERT lifts the score from 72.8 to 80.5 in the [GLUE benchmark](https://gluebenchmark.com/leaderboard) for 9 different NLP tasksthis is the biggest recent advancement\[6\].

Although BERT is exciting, unfortunately there have been no open source implementations that simultaneously

* enable scalable pre-training with GPUs;
* reproduce results on various tasks;
* support model exporting for deployment.

Thus, we release GluonNLP 0.6 to address such pain points by i) pre-training BERT with 8 GPUs in 6.5 days; ii) reproducing multiple natural language understanding results; iii) streamlining deployment.

&amp;#x200B;

full article: [https://medium.com/apache-mxnet/gluon-nlp-bert-6a489bdd3340](https://medium.com/apache-mxnet/gluon-nlp-bert-6a489bdd3340)

gluon-nlp: [http://gluon-nlp.mxnet.io/](http://gluon-nlp.mxnet.io/)

Apache MXNet:  [http://mxnet.incubator.apache.org/](http://mxnet.incubator.apache.org/)",0,1,False,self,,,,,
1189,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,2,b30m8e,medium.com,GTC 2019 | Highlights &amp; Disappointments at NVIDIAs Annual Conference,https://www.reddit.com/r/MachineLearning/comments/b30m8e/gtc_2019_highlights_disappointments_at_nvidias/,gwen0927,1553018351,,0,1,False,default,,,,,
1190,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,2,b30m8p,self.MachineLearning,[N] GluonNLP 0.6: Closing the Gap in Reproducible Research with BERT,https://www.reddit.com/r/MachineLearning/comments/b30m8p/n_gluonnlp_06_closing_the_gap_in_reproducible/,thomasdlt,1553018352,"&amp;#x200B;

[BERT](https://arxiv.org/abs/1810.04805) (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers) is arguably the most notable pre-training model in natural language processing (NLP). For instance, BERT lifts the score from 72.8 to 80.5 in the [GLUE benchmark](https://gluebenchmark.com/leaderboard) for 9 different NLP tasksthis is the biggest recent advancement\[6\].

Although BERT is exciting, unfortunately there have been no open source implementations that simultaneously

* enable scalable pre-training with GPUs;
* reproduce results on various tasks;
* support model exporting for deployment.

Thus, we release GluonNLP 0.6 to address such pain points by i) pre-training BERT with 8 GPUs in 6.5 days; ii) reproducing multiple natural language understanding results; iii) streamlining deployment.

...

full article: [https://medium.com/apache-mxnet/gluon-nlp-bert-6a489bdd3340](https://medium.com/apache-mxnet/gluon-nlp-bert-6a489bdd3340)

gluon-nlp: [http://gluon-nlp.mxnet.io/](http://gluon-nlp.mxnet.io/)

Apache MXNet: [http://mxnet.incubator.apache.org/](http://mxnet.incubator.apache.org/)",24,67,False,self,,,,,
1191,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,3,b30qpq,self.MachineLearning,[N] Call for papers: ICML Workshop on Uncertainty and Robustness in Deep Learning,https://www.reddit.com/r/MachineLearning/comments/b30qpq/n_call_for_papers_icml_workshop_on_uncertainty/,DanielHendrycks,1553018940,"[https://sites.google.com/view/udlworkshop2019/](https://sites.google.com/view/udlworkshop2019/)

Submissions are due April 30th, 2019. Topics include

* Out-of-distribution detection and anomaly detection
* Robustness to corruptions, adversarial perturbations, and distribution shift
* Calibration
* Probabilistic (Bayesian and non-Bayesian) neural networks
* Open world recognition and open set learning
* Security
* Quantifying different types of uncertainty (known unknowns and unknown unknowns) and types of robustness
* Applications of robust and uncertainty-aware deep learning

&amp;#x200B;",1,5,False,self,,,,,
1192,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,3,b30w47,self.MachineLearning,[D] Getting skeletons from the silhouettes,https://www.reddit.com/r/MachineLearning/comments/b30w47/d_getting_skeletons_from_the_silhouettes/,akaberto,1553019707,"Hi,  


I'll be upfront and tell that this is for the SkelNeton competition ([https://competitions.codalab.org/competitions/21685#learn\_the\_details](https://competitions.codalab.org/competitions/21685#learn_the_details)).  So, I'm trying to learn via a UNet and the results are very bad.  I have tried searching for detecting skeletons but I couldn't find anything that looks like this one.   I do remember vaguely seeing a paper but nothing quite what I want.  


I've a semantic segmentation (1s for the object.  Think of a silhouette of an Apple.  The output is the stem and the stalk of it).  Or think of a bat and its bone  


[Skeleton - to be predicted](https://i.redd.it/cxkrl8l6b4n21.png)

&amp;#x200B;

[Input](https://i.redd.it/zby9y77eb4n21.png)

&amp;#x200B;",10,20,False,https://b.thumbs.redditmedia.com/vCi4aVG5_JMJksbPdlmcCtNNa3Vsr2CnskPJwl6dMNc.jpg,,,,,
1193,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,3,b3184y,self.MachineLearning,8-bit VAE: a latent variable model for NES music,https://www.reddit.com/r/MachineLearning/comments/b3184y/8bit_vae_a_latent_variable_model_for_nes_music/,TheRedSphinx,1553021418,[removed],0,1,False,self,,,,,
1194,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,3,b31972,self.MachineLearning,8-bit VAE: a latent variable model for NES music (Blog Post),https://www.reddit.com/r/MachineLearning/comments/b31972/8bit_vae_a_latent_variable_model_for_nes_music/,TheRedSphinx,1553021572,[removed],0,1,False,self,,,,,
1195,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,3,b31ars,self.MachineLearning,Can anyone review UBER's Ludwig ?,https://www.reddit.com/r/MachineLearning/comments/b31ars/can_anyone_review_ubers_ludwig/,SanRStar,1553021791,,0,1,False,self,,,,,
1196,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,3,b31ayb,self.MachineLearning,[D] 8-bit VAE: a latent variable model for NES music (blog post),https://www.reddit.com/r/MachineLearning/comments/b31ayb/d_8bit_vae_a_latent_variable_model_for_nes_music/,TheRedSphinx,1553021818,"Hi r/MachineLearning,

&amp;#x200B;

Some time ago, I posted a link to my github for my NES music generative model, 8-bit VAE. Most recently, I managed to get some better results and so I decided to write up a blog post about it to hopefully inspire people to play more with this dataset. Please let me know if you have any thoughts/feedback on the blog post or model!

&amp;#x200B;

[link to blog](https://xgarcia238.github.io/misc/2018/03/18/8bitvae.html)",2,7,False,self,,,,,
1197,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,4,b31nl0,self.MachineLearning,[Discussion] Is algorules.org realistic?,https://www.reddit.com/r/MachineLearning/comments/b31nl0/discussion_is_algorulesorg_realistic/,stopaisabotage,1553023522,"What do you think about the following rules when applying them for AI development in a start-up, i.e. how practical and realistic do you find them?

Where do you see problems?

How does it make you feel?
Would you like to work in a company with those rules?

https://algorules.org/en/home/

I would love comments from people with AI start-up experience, as it pertains to building a product or service that is sold successfully.

I have my own opinion, but will give it later to prevent bias.",0,0,False,self,,,,,
1198,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,4,b31zmu,self.MachineLearning,[R] Microsoft Textworld Competition: play text adventure games and win $2000,https://www.reddit.com/r/MachineLearning/comments/b31zmu/r_microsoft_textworld_competition_play_text/,jinpanZe,1553025130," [https://competitions.codalab.org/competitions/20865](https://competitions.codalab.org/competitions/20865)

Few weeks ago I posted FAIR's new paper on a crowd-sourced fantasy text adventure game. Turns out Microsoft has been running this competition where you can submit your agent to play text games. Looks pretty neat.",0,5,False,self,,,,,
1199,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,5,b3239s,self.MachineLearning,[D] How does Google Vision API does the document detection part?,https://www.reddit.com/r/MachineLearning/comments/b3239s/d_how_does_google_vision_api_does_the_document/,DGs29,1553025600,"I want to detect text in scanned images and get results somewhat like Google's API. I know it's hard to achieve but just wanna try. 

&amp;#x200B;

https://i.redd.it/nm6ype5rs4n21.png

I want to know what method do they use/ What design modifications they use inorder to get a similar result. 

Or else you can share your own idea about that help me get the result I'm looking for. 

P.S: I'm looking text block detection alone, not recognition.",6,2,False,self,,,,,
1200,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,5,b324v0,self.MachineLearning,"Hey everyone, so I made an app that lets you save ML-related photos, videos, websites, etc. from any app to one place. Its called Figgle. Feel free to check it out.",https://www.reddit.com/r/MachineLearning/comments/b324v0/hey_everyone_so_i_made_an_app_that_lets_you_save/,p511,1553025818,"So the title says it all. The app can save photos, videos, websites, articles, social media posts, notes, music, files, etc. from any app and store it in one place.

[**App Link**](https://itunes.apple.com/us/app/figgle/id1449424159?ls=1&amp;mt=8)

If you'd like to know when the Android version comes out comment below and we'll add you name to our list. You'll be PM'd when it comes out (1.5 months).",0,1,False,self,,,,,
1201,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,5,b326ro,self.MachineLearning,[D] Interpretation/Questions on Survival Analysis Results?,https://www.reddit.com/r/MachineLearning/comments/b326ro/d_interpretationquestions_on_survival_analysis/,Fender6969,1553026083,"Hello,

&amp;#x200B;

I am quite new to Survival Analysis in general. I am working with a small Breast Cancer dataset and am trying to fit the Kaplan Meier model.

&amp;#x200B;

My dataset contains information about whether the patient survives after 5+ years of treatment or not \[1,2\]. I have recoded this to \[0,1\]. I also created a calculated column to calculate the years since last treatment (this will be used as the ""durations"").

&amp;#x200B;

When working on the Kaplan Meier, I am using the KaplanMeierFitter() function from Python. When fitting the entire dataset (not fitting indivivdual Kaplan Meier classifiers based on my target class) I am not running into problems. The event\_table and survival\_function\_ seem to be pretty straight forward. With this being a binary classification, I am assuming that the results of fitting on the entire dataset would be for the probability of patient surviving at time period t.

&amp;#x200B;

When I fit the Kaplan Meier for each individual class, I am seeing that the probability a patient survives is 1.0 for across all time periods. The probabilities for patient dying seems to make sense.

&amp;#x200B;

I have been following some guides online, and I don't see that conceptually I have made any mistakes. Is this something to be concerned about?

&amp;#x200B;

Here is my code:

&amp;#x200B;

    #Read in CSV
    df = pd.read_csv('/content/Survival Data.csv')
    df.head()
    
    #Recode Target Variable
    df['Status'] = df['Status'].map({1:0, 2:1})
    
    #Create Kaplan Meier Estimator
    kmf = KaplanMeierFitter()
    
    kmf.fit(durations = df['Years Passed'], 
            event_observed = df['Status'])
    
    #Display Event Table
    kmf.event_table
    
    #Probability of surival 
    kmf.survival_function_
    
    #Plotting the respective probabilities of both classes (code from documentation):
    
    ax = plt.subplot(111)
    
    durations = df['Years Passed']
    event_observed = df['Status']
    
    dem = (df[""Status""] == 0)
    
    kmf.fit(durations[dem], event_observed=event_observed[dem], label=""Survives Breast Cancer"")
    kmf.plot(ax=ax)
    kmf.fit(durations[~dem], event_observed=event_observed[~dem], label=""Dies from Breast Cancer"")
    kmf.plot(ax=ax)
    
    plt.ylim(0, 1);
    plt.title(""Survival Function for Breast Cancer"");
    
    kmf._conditional_time_to_event_()

&amp;#x200B;

It is here that I am seeing the issue of ""Survives Breast Cancer"" being 1.0 through all time periods.

&amp;#x200B;

With the conditional\_time\_to\_event, I am seeing infinity for time periods 7-12. Is that something to be concerned with? 

&amp;#x200B;

Here is the dataset in case this helps. I created ""Years Passed"" by subtracting 70 from the ""Year of Operation"": [https://archive.ics.uci.edu/ml/datasets/Haberman%27s+Survival](https://archive.ics.uci.edu/ml/datasets/Haberman%27s+Survival)

&amp;#x200B;

Any help would be great!",0,1,False,self,,,,,
1202,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,5,b32bl0,self.MachineLearning,Does anyone know the rank of the Netflix Prize Matrix?,https://www.reddit.com/r/MachineLearning/comments/b32bl0/does_anyone_know_the_rank_of_the_netflix_prize/,nafisabareerah,1553026749,[removed],0,1,False,self,,,,,
1203,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,5,b32lve,self.MachineLearning,[D] I'm using OpenAI's GPT-2 to generate text. Give me prompts to try!,https://www.reddit.com/r/MachineLearning/comments/b32lve/d_im_using_openais_gpt2_to_generate_text_give_me/,Shevizzle,1553028195,"\*EXAMPLE PROMPT:\*

The NCAA unveiled the 68 college basketball teams that will be dancing this year. It wasnt short on controversy, as snubbed teams aired out their grievances and blamed a confusing new metric.

\*OUTPUT:\*

The numbers came straight from the NCAA's website, where they're being used to calculate the number of national titles, national championship games, and even NCAA games played in a certain year.

""We want to make sure that the NCAA is inclusive,"" said NCAA President Dave Heidelman. ""We want to make sure that you can get what you want out of your program. And we want to make sure that the NCAA is focused on this problem right now and that the NCAA is fully invested.""

The list of opponents includes Ohio State, Florida State, Louisville, Kentucky, Mississippi State, Penn State, Oklahoma, Southern Cal, Virginia, and Wake Forest. It's not all about one one, though: They're all on pace to play an entire season. They have every national title, national championship, and tournament appearance in their DNA  as one might expect, they want to see it happen, but also want to build an identity, make their dream of watching that happen become the reality.

As good as any NCAA team is, they're also a long way off reaching the number one spot in the nation or even the top-ranked nation. The Big Ten also has some talented recruits from some in-state programs that may be considered the top two nationally. In fact, the national ranking of these schools is so high that a single conference ranking in 2016 will put the conference in the top-50 of the polls. Still, while Big Ten and SEC teams are likely to be on the map and competing for national titles, they're a bit underserved (and it's not as if they're all the same.)

So where does the NCAA stand on this?

According to ULM's John Covington, who runs its ""Unions, Colleges, and Universities"" page in conjunction with the National Conference, they're all going to have to make some moves:

Some may think this is just a joke. ""No, this is really about the league's future,"" said Dr. John H. Hester, president of UM's Athletic Department and president of the National Collegiate Athletic Association's Women's Academic Programs. ""I think the NCAA is a great place to start, because it's here to stay and if we're really strong and we can figure ourselves out, our future is going to be on the basketball court.""

\*MODEL:\*

gpt-2 117M

\*If you have an idea for a prompt, post it in the comments and I'll reply with the output if I deem it worthy.\*",825,327,False,self,,,,,
1204,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,6,b32tkg,self.MachineLearning,Choosing a ML project,https://www.reddit.com/r/MachineLearning/comments/b32tkg/choosing_a_ml_project/,ahsol360,1553029251,[removed],0,1,False,self,,,,,
1205,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,6,b3316f,self.SchoolOfAiOfficial,Update: 21 cities now. School of AI Global Health Hackathon! Find your city and rsvp,https://www.reddit.com/r/MachineLearning/comments/b3316f/update_21_cities_now_school_of_ai_global_health/,humanfromearth93,1553030286,,0,1,False,default,,,,,
1206,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,6,b33hcd,self.MachineLearning,Meeting with head machine learning @ Xbox [Discussion],https://www.reddit.com/r/MachineLearning/comments/b33hcd/meeting_with_head_machine_learning_xbox_discussion/,SquareRootsi,1553032515,"Hello r/MachineLearning , 

I'm a math &amp; physics tutor of ten years, but I'm about to embark on a 15 week intensive ""Data Science &amp; Machine Learning"" job re-training program with Flatiron School. I managed to get a face-to-face introduction with the head of data science @ Xbox. I can't promise to deliver an answer (but I'll try), and I'm not a journalist (just a nerdy guy trying to switch careers), but I feel like I'm in a state of ""I know that I don't know"" and I'd like your help. What should I ask? If you had the ear of the Lead Data Scientist at all of Xbox, what would you ask? (obviously nothing about getting ""trade secrets"" or anything else she clearly wouldn't answer. These have to be feasible questions.)  Thanks!

&amp;#x200B;",17,17,False,self,,,,,
1207,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,7,b33ro5,self.MachineLearning,Optimal Deployments and Limitations of MCMC (Markov Chain Monte Carlo) functionality,https://www.reddit.com/r/MachineLearning/comments/b33ro5/optimal_deployments_and_limitations_of_mcmc/,jeanduluoz,1553033938,"I'm interested in deploying a scaled variant of an MCMC process to value assets. I know this is of course possible, what are advantages and disadvantages of these methods? Please forgive me - I'm not engineer or data scientist, I'm coming from an econ/stats background with some experience with ML in a professional environment (PM at adtech working w/ FMs and binary classification). We used both models depending on the environment, constraints, and goals we needed to reach.

&amp;#x200B;

Can i just use MCMC like any other sampling process? There must be some situations it's bad for, and some situations it's good for. A few things I'm thinking about: 

&amp;#x200B;

\- Costliness / cost factors, and scaling curve of the process, relative to other processes

\- sampling requirements / performance tradeoff

\- situational applications (endogeneity tolerance etc.... getting out of my wheelhouse here)

&amp;#x200B;

I'm not even sure if these are the right questions, but hopefully it's a start. Thanks for any help!",0,1,False,self,,,,,
1208,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,8,b34a2s,self.MachineLearning,Will Google use Stadia to collect game data for DeepMind?,https://www.reddit.com/r/MachineLearning/comments/b34a2s/will_google_use_stadia_to_collect_game_data_for/,mroda44,1553036516,[removed],0,1,False,self,,,,,
1209,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,9,b352zh,self.MachineLearning,[R] Mozilla DeepSpeech + Custom Language Model (like BERT),https://www.reddit.com/r/MachineLearning/comments/b352zh/r_mozilla_deepspeech_custom_language_model_like/,mrdbourke,1553040807,"I've been working on a Speech to Text project and have been using [DeepSpeech by Mozilla](https://github.com/mozilla/DeepSpeech/).

Speech to Text comes in two parts.

1. An acoustic model (wave forms of sound to numbers)
2. A language model (converting those numbers to words)

The DeepSpeech library uses [KenLM](https://kheafield.com/code/kenlm/) to build a language model.

The language model is essentially a spell/grammar checker for the transcribed words.

It looks at a sentence and says 'How likely are these words going to appear together?'

Example:

* The cat has tiny *paws*
* The cat has tiny *pause*

Paws and pause sound similar in speech but only one is far more likely to appear than the other.

I'm curious to see if anyone has used DeepSpeech with their own custom language model (or know of any resources where someone has)?

I've been looking through the DeepSpeech GitHub and it seems pretty straightforward to retrain a custom KenLM model but not so straightforward using an entirely different language model like [BERT](https://github.com/google-research/bert).

&amp;#x200B;

Note: In my couple of hours research yesterday, I couldn't find much except the fact it doesn't seem like a drop-in replacement (I wasn't expecting this). A contributor to the DeepSpeech project [replied to my question on the DeepSpeech forums saying](https://discourse.mozilla.org/t/adding-in-a-custom-language-model-like-bert/37170) it would take some significant engineering (I'm currently looking more into this).

&amp;#x200B;",3,2,False,self,,,,,
1210,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,9,b35hgr,self.MachineLearning,Tesla P4 For sale good deal,https://www.reddit.com/r/MachineLearning/comments/b35hgr/tesla_p4_for_sale_good_deal/,jWags818,1553043151,[removed],0,1,False,self,,,,,
1211,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,10,b35m6h,self.MachineLearning,[D] A Better Lesson,https://www.reddit.com/r/MachineLearning/comments/b35m6h/d_a_better_lesson/,milaworld,1553043921,"A response to Rich Suttons [A Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) from Rodney Brooks.

*Just last week Rich Sutton published a very short blog post titled The Bitter Lesson. Im going to try to keep this review shorter than his post. Sutton is well known for his long and sustained contributions to reinforcement learning.*

https://rodneybrooks.com/a-better-lesson/",4,13,False,self,,,,,
1212,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,10,b35ry5,self.MachineLearning,[D] Question for Jeff Hawkins (author of On Intelligence),https://www.reddit.com/r/MachineLearning/comments/b35ry5/d_question_for_jeff_hawkins_author_of_on/,UltraMarathonMan,1553044857,"I'm talking with Jeff Hawkins, author of On Intelligence, this week on the [Artificial Intelligence](https://lexfridman.com/ai/) podcast.

&amp;#x200B;

Let me know if you have questions for him or what topics you would like to see covered. I'm aware of this [critical discussion thread](https://www.reddit.com/r/MachineLearning/comments/393t53/jeff_hawkins_on_intelligence_what_are_the/) and on [HN](https://news.ycombinator.com/item?id=18214707). I've read an understood them relatively well. However, I'm interested in searching for kernels of insight beyond the hype that could be arrived at through conversation and good questions. If you have such questions for Jeff, let me know.

&amp;#x200B;

My hope with this series is to have conversations about human and machine intelligence from varied perspectives. If you have advice for what guests to have on or what direction to take conversations, let me know.",0,1,False,self,,,,,
1213,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,10,b35sko,self.MachineLearning,[D] Questions for Jeff Hawkins (author of On Intelligence),https://www.reddit.com/r/MachineLearning/comments/b35sko/d_questions_for_jeff_hawkins_author_of_on/,UltraMarathonMan,1553044972,"This week, I'm talking with Jeff Hawkins (author of On Intelligence) on the [Artificial Intelligence](https://lexfridman.com/ai/) podcast.

Let me know if you have questions for him or what topics you would like to see covered. I'm aware of this [critical discussion thread](https://www.reddit.com/r/MachineLearning/comments/393t53/jeff_hawkins_on_intelligence_what_are_the/) and also on [HN](https://news.ycombinator.com/item?id=18214707). I've read an understood them relatively well. However, I'm interested in searching for kernels of insight beyond the hype that could be arrived at through conversation and good questions. If you have such questions for Jeff, let me know.

My hope with this series is to have conversations about human and machine intelligence from varied perspectives. If you have advice for what guests to have on or what direction to take conversations, let me know.",11,18,False,self,,,,,
1214,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,12,b36ubq,self.MachineLearning,SVM with MFCC,https://www.reddit.com/r/MachineLearning/comments/b36ubq/svm_with_mfcc/,acddb,1553051241,[removed],0,1,False,self,,,,,
1215,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,12,b36xu3,self.MachineLearning,[P] SVM with MFCC,https://www.reddit.com/r/MachineLearning/comments/b36xu3/p_svm_with_mfcc/,acddb,1553051841,I have a project where I have to analyse sound sources and classify them with MFCC using SVM. I am trying to build a test database. So far I have 2 lines of code: audioread the file and extract MFCC coefficients of the file. My question is how do I convert the MFCC coefficients (a large matrix) into some sort of table to treat it as a feature to put into MATLAB's native SVM classifier learner? I have looked online for countless hours and cannot seem to find a solution.,4,1,False,self,,,,,
1216,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,12,b372ml,machina-rl.org,machina: A Library for Real-World Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/b372ml/machina_a_library_for_realworld_deep/,rarilurelo,1553052654,,1,2,False,default,,,,,
1217,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,14,b381bo,developerthing.blogspot.com,Machine Learning Engineer Career Path,https://www.reddit.com/r/MachineLearning/comments/b381bo/machine_learning_engineer_career_path/,DeveloperThing,1553059194,,0,1,False,default,,,,,
1218,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,14,b384nd,solutionfactory.in,Opportunities and Challenges of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b384nd/opportunities_and_challenges_of_machine_learning/,Ajaygawde,1553059890,,0,1,False,default,,,,,
1219,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,14,b38526,self.MachineLearning,"Machine Learning as a Service (Mlaas) Market - Size, Outlook, Trends and Forecasts (2018  2024)",https://www.reddit.com/r/MachineLearning/comments/b38526/machine_learning_as_a_service_mlaas_market_size/,fedupsession,1553059976,[removed],0,1,False,self,,,,,
1220,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,15,b38dja,lhd.co.com,World Best Manufacturer for Telescopic fork &amp; stacker crane,https://www.reddit.com/r/MachineLearning/comments/b38dja/world_best_manufacturer_for_telescopic_fork/,lhd121,1553061742,,0,1,False,default,,,,,
1221,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,15,b38p55,reddit.com,Learn New Machine Learning Concepts!,https://www.reddit.com/r/MachineLearning/comments/b38p55/learn_new_machine_learning_concepts/,theaispace,1553064241,,0,1,False,default,,,,,
1222,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,15,b38t1x,self.MachineLearning,Rock paper scissors but with conversation.,https://www.reddit.com/r/MachineLearning/comments/b38t1x/rock_paper_scissors_but_with_conversation/,adikhad,1553065091,[removed],0,1,False,self,,,,,
1223,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,16,b38wwb,self.MachineLearning,"Best way (learning pattern, resources, tips, references ) to get started with machine learning :)",https://www.reddit.com/r/MachineLearning/comments/b38wwb/best_way_learning_pattern_resources_tips/,Bloodyvirus101101,1553065927,[removed],0,1,False,self,,,,,
1224,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,16,b38z42,youtu.be,Machine Learning Tutorial Part 7 | Machine Learning For Beginners - 5-Fold Cross Validation,https://www.reddit.com/r/MachineLearning/comments/b38z42/machine_learning_tutorial_part_7_machine_learning/,SquareTechAcademy,1553066418,,0,1,False,default,,,,,
1225,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,16,b3993h,self.MachineLearning,[R] How should I understand a Fuzzy Neural Network (FNN) and a Self-Organizing FNN (SOFNN)?,https://www.reddit.com/r/MachineLearning/comments/b3993h/r_how_should_i_understand_a_fuzzy_neural_network/,Seankala,1553068660,"Hello. I'm currently working on writing a paper (undergraduate level) to submit to a conference. My topic is to perform sentiment analysis on Twitter data and use that to make predictions for markets (cryptocurrency). I had a question regarding the research I've been doing.

I noticed that almost every paper that I read about using Twitter sentiment analysis for stock markets, almost every single one of them uses something called a Self-Organizing Fuzzy Neural Network (SOFNN) to perform sentiment analysis. However, I'm having trouble understanding what it is.

I did some research on what a Fuzzy Neural Network is and simply put it utilizes the notion of a [fuzzy set](https://en.wikipedia.org/wiki/Fuzzy_set). You're basically testing an element's membership for classes.

My assumption is that one Tweet would be used as input, and after performing appropriate parsing techniques the network analyzes the sentiment and categorizes it into a ""mood class"" (e.g. calm, happy, kind, etc.)

I'm not sure if this interpretation is correct, let alone how a self-organizing network should work.

If anyone is familiar with this topic and would be kind enough to educate me, that'd be great. Or if anyone knows where I can find some good resources to learn about this that would also be greatly appreciated.

Thank you!",3,15,False,self,,,,,
1226,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,17,b39crd,self.MachineLearning,"[D] /r/ml_tips : new subreddit for ml gotchas, tips, tricks, and useful bits of code",https://www.reddit.com/r/MachineLearning/comments/b39crd/d_rml_tips_new_subreddit_for_ml_gotchas_tips/,Miejuib,1553069485,"Hey guys, I've been keeping a few random bits of useful ml code in my notes, so I thought it would be helpful to share it with you guys, and see if y'all had any good bits to share also.  


&amp;#x200B;

So, introducing /r/ml_tips !  

&amp;#x200B;

&amp;#x200B;

The idea is for it to be something between an ml practitioner cheat-sheet, and common gotchas for ml/dl/rl/ai libraries/architectures/etc .  I've posted a couple things there already, and I'll be posting some more content in the near future.  I just wanted to get it up and running before going to sleep tonight.

&amp;#x200B;

Hope you guys find it helpful!",3,30,False,self,,,,,
1227,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,17,b39d01,self.MachineLearning,model.predict is abnormal,https://www.reddit.com/r/MachineLearning/comments/b39d01/modelpredict_is_abnormal/,GoBacksIn,1553069539,[removed],0,1,False,https://b.thumbs.redditmedia.com/4CFEEAoePdeOmamCPWe6jRkHS0vPiVa-mI6-MRhZSFY.jpg,,,,,
1228,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,17,b39i6b,self.MachineLearning,road barrier/traffic jam blow molding machine,https://www.reddit.com/r/MachineLearning/comments/b39i6b/road_barriertraffic_jam_blow_molding_machine/,miyawang12138,1553070769,[removed],0,1,False,self,,,,,
1229,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,17,b39iyo,self.MachineLearning,How to select the best among several svm models in separated word recognition?,https://www.reddit.com/r/MachineLearning/comments/b39iyo/how_to_select_the_best_among_several_svm_models/,ohohohjkl,1553070968,[removed],0,1,False,self,,,,,
1230,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,17,b39kyk,self.MachineLearning,[D] Model to improve resolution of an image based on a high quality reference image?,https://www.reddit.com/r/MachineLearning/comments/b39kyk/d_model_to_improve_resolution_of_an_image_based/,hanyuqn,1553071461,"I want to take a blurry image (https://imgur.com/a/9uCW6OW) and improve the details based on a very similar, high quality image (https://imgur.com/a/XhvMJkp). I feel like for this particular case this should be fairly simple, or at least 100% possible. Are there any open-source implementations or pre-trained models I can try for this? The one I have found in this area is https://github.com/ZZUTK/SRNTT though I haven't got training to work on it yet.",1,2,False,self,,,,,
1231,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,18,b39sc9,self.MachineLearning,How to select the best among several svm models in separated word recognition?,https://www.reddit.com/r/MachineLearning/comments/b39sc9/how_to_select_the_best_among_several_svm_models/,nguyenAnw,1553073186,[removed],0,1,False,self,,,,,
1232,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,18,b39v0q,v.redd.it,Gloves that can convert sign language into auditory voice,https://www.reddit.com/r/MachineLearning/comments/b39v0q/gloves_that_can_convert_sign_language_into/,MrMakeItAllUp,1553073797,,0,1,False,https://b.thumbs.redditmedia.com/eHdnZlC902bx48SKtxcg6DcNoB5WODeC3ARyqmod7cY.jpg,,,,,
1233,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,18,b39v5u,self.MachineLearning,Did Google release a paper about Stadia's style transfer?,https://www.reddit.com/r/MachineLearning/comments/b39v5u/did_google_release_a_paper_about_stadias_style/,sash-a,1553073834,[removed],0,1,False,self,,,,,
1234,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,18,b3a3kl,self.MachineLearning,"""[Discussion]""How to select the best among several svm models in separated word recognition?",https://www.reddit.com/r/MachineLearning/comments/b3a3kl/discussionhow_to_select_the_best_among_several/,nguyenAnw,1553075726," I'm trying to solve a problem of keyword detection. In that, i have a library that contain 50 words to predict. I'm using MFCC to extract the feature of word and got 13\*13 dimension vector feature. Then, i pass it to train with svm\_train function. 

&amp;#x200B;

The thing is, the keyword is Vietnamese. So i want to devide 50 words to 6 groups which are representing for question mark question, tilde, acute, grave accent, dot and neither. 6 groups are trained to 6 seperate svm models.  

&amp;#x200B;

So, my question is, when i get a feature of a word. How can i select 1 in 6 models which is the best to predict? Is there a way which i do not need to pass through all 6 models and chose the best acc from there? ",5,1,False,self,,,,,
1235,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,19,b3a4wr,github.com,"A Pytorch Implementation of ""Neural Speech Synthesis with Transformer Network""",https://www.reddit.com/r/MachineLearning/comments/b3a4wr/a_pytorch_implementation_of_neural_speech/,soobinseo,1553076018,,0,1,False,https://b.thumbs.redditmedia.com/Zy7a3BqSoHrfzNpwbDGnmviPxcp3O1_ao3zv6XlrFrA.jpg,,,,,
1236,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,19,b3a54o,self.MachineLearning,[P] deep learning optimizer visualization (ensmallen),https://www.reddit.com/r/MachineLearning/comments/b3a54o/p_deep_learning_optimizer_visualization_ensmallen/,eusben,1553076070,"**Website:** [https://vis.ensmallen.org/](https://vis.ensmallen.org/)

**Code:** [https://github.com/mlpack/ensmallen](https://github.com/mlpack/ensmallen)",3,20,False,self,,,,,
1237,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,19,b3a9v7,patronageinstitute.blogspot.com,Write with AI tools,https://www.reddit.com/r/MachineLearning/comments/b3a9v7/write_with_ai_tools/,patronageinstitute,1553077053,,0,1,False,https://a.thumbs.redditmedia.com/4TdiXtVi1U3Y7LCvjH8eFSAYrJx9bNMGv15-2svliy4.jpg,,,,,
1238,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,19,b3aght,self.MachineLearning,[D] Loss functions in siamese learning,https://www.reddit.com/r/MachineLearning/comments/b3aght/d_loss_functions_in_siamese_learning/,danioto,1553078457,"In [Sampling Matters in Deep Embedding Learning](https://arxiv.org/abs/1706.07567) paper authors write something like this about a **contrastive loss**:

&amp;#x200B;

&gt;One drawback of the contrastive loss is that we have to select a constant margin  for all pairs of negative samples.

&amp;#x200B;

And something like this about **triplet loss**:

&amp;#x200B;

&gt;This formulation \[triplet loss\] allows the embedding space to be arbitrarily distorted and does not impose a constant margin .

&amp;#x200B;

Ok, I understand that triplet loss is much more flexible, but why do authors say the margin with triplet loss is not constant? One still has to pick one alpha value for all pairs of classes, so, in my understanding, it's still constant. In triplet loss, we also force classes to be distant by alpha, but the difference here, as I understand, is where do we measure those distances: from the center of a class (contrastive loss) or the boundary of a class (triplet loss). Do I miss something?",4,5,False,self,,,,,
1239,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,19,b3ainf,github.com,"[P] A Pytorch Implementation of ""Neural Speech Synthesis with Transformer Network""",https://www.reddit.com/r/MachineLearning/comments/b3ainf/p_a_pytorch_implementation_of_neural_speech/,soobinseo,1553078915,,0,1,False,default,,,,,
1240,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,19,b3ak5f,github.com,"[Project] A Pytorch Implementation of ""Neural Speech Synthesis with Transformer Network""",https://www.reddit.com/r/MachineLearning/comments/b3ak5f/project_a_pytorch_implementation_of_neural_speech/,soobinseo,1553079227,,0,1,False,default,,,,,
1241,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,19,b3al8i,github.com,"[P]A Pytorch Implementation of ""Neural Speech Synthesis with Transformer Network""",https://www.reddit.com/r/MachineLearning/comments/b3al8i/pa_pytorch_implementation_of_neural_speech/,soobinseo,1553079448,,0,1,False,https://b.thumbs.redditmedia.com/Zy7a3BqSoHrfzNpwbDGnmviPxcp3O1_ao3zv6XlrFrA.jpg,,,,,
1242,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,20,b3amm6,github.com,[P] A Pytorch Implementation of Neural Speech Synthesis with Transformer Network,https://www.reddit.com/r/MachineLearning/comments/b3amm6/p_a_pytorch_implementation_of_neural_speech/,soobinseo,1553079701,,0,1,False,https://b.thumbs.redditmedia.com/Zy7a3BqSoHrfzNpwbDGnmviPxcp3O1_ao3zv6XlrFrA.jpg,,,,,
1243,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,20,b3aqsn,artificialintelligence.id,The 1st Southeast Asia Machine Learning School,https://www.reddit.com/r/MachineLearning/comments/b3aqsn/the_1st_southeast_asia_machine_learning_school/,peeyek,1553080493,,1,1,False,default,,,,,
1244,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,20,b3ar0b,github.com,[P] A Pytorch Implementation of Neural Speech Synthesis with Transformer Network,https://www.reddit.com/r/MachineLearning/comments/b3ar0b/p_a_pytorch_implementation_of_neural_speech/,soobinseo,1553080534,,0,1,False,default,,,,,
1245,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,20,b3arp6,socialprachar.com,Best Data science interview questions,https://www.reddit.com/r/MachineLearning/comments/b3arp6/best_data_science_interview_questions/,chandulekkala,1553080666,,0,1,False,default,,,,,
1246,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,20,b3as1v,github.com,[P] A Pytorch Implementation of Neural Speech Synthesis with Transformer Network,https://www.reddit.com/r/MachineLearning/comments/b3as1v/p_a_pytorch_implementation_of_neural_speech/,soobinseo,1553080737,,0,1,False,default,,,,,
1247,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,20,b3avr5,github.com,"""[P]"" A Pytorch Implementation of Neural Speech Synthesis with Transformer Network",https://www.reddit.com/r/MachineLearning/comments/b3avr5/p_a_pytorch_implementation_of_neural_speech/,soobinseo,1553081467,,0,1,False,https://b.thumbs.redditmedia.com/Zy7a3BqSoHrfzNpwbDGnmviPxcp3O1_ao3zv6XlrFrA.jpg,,,,,
1248,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,20,b3awc9,github.com,[P] A Pytorch Implementation of Neural Speech Synthesis with Transformer Network,https://www.reddit.com/r/MachineLearning/comments/b3awc9/p_a_pytorch_implementation_of_neural_speech/,soobinseo,1553081583,,0,1,False,https://b.thumbs.redditmedia.com/Zy7a3BqSoHrfzNpwbDGnmviPxcp3O1_ao3zv6XlrFrA.jpg,,,,,
1249,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,20,b3ayhh,self.MachineLearning,[D] Proper Experimental Procedure - Replicates and Random Seeds,https://www.reddit.com/r/MachineLearning/comments/b3ayhh/d_proper_experimental_procedure_replicates_and/,JosephLChu,1553082021,"So I've been giving this some thought while working on a paper and noticing that most papers in this field don't really explain how many replicates of an experiment they do to ensure statistical validity, or provide their random seed if they use one to maintain consistent initializations across conditions.

So let me ask, how important are these things to the scientists in the room here?  When doing a proper experiment, how many replicates would you do to be confident, assuming you weren't using a random seed.  Also, if you were using a random seed, how do you avoid the possibility of overfitting on the resulting same initialization for every condition?

Of the two methods, which do you think is actually more proper in terms of experimental procedure?

If you perform multiple replications, do you take the average, or the best result, and how do you justify if the latter?

I mostly realized this could be concerning because my dad was a professor in another field of science where it was not uncommon to have 10 replicates per experimental condition, and I have taken to doing some quick experiments in my own research without a random seed, and then started doing some replicates to double-check some things and noticed that the numbers have a lot more variability than I previously anticipated.

Though if I had to do a lot of replicates for every experiment, it would slow down the pace of my exploration of the possibilities considerably.  So how do other people who do research handle this issue?  Where do you get the numbers that end up in the tables in your papers?",16,11,False,self,,,,,
1250,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,20,b3b1ze,self.MachineLearning,What should my Target be?,https://www.reddit.com/r/MachineLearning/comments/b3b1ze/what_should_my_target_be/,Recklines,1553082702,[removed],0,1,False,self,,,,,
1251,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,20,b3b380,arxiv.org,[R] A novel adaptive learning rate scheduler for deep neural networks,https://www.reddit.com/r/MachineLearning/comments/b3b380/r_a_novel_adaptive_learning_rate_scheduler_for/,FlyingQuokka,1553082920,,12,10,False,default,,,,,
1252,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,21,b3b73i,self.MachineLearning,[P] A Pytorch Implementation of Neural Speech Synthesis with Transformer Network,https://www.reddit.com/r/MachineLearning/comments/b3b73i/p_a_pytorch_implementation_of_neural_speech/,soobinseo,1553083621,"I recently implemented transformer network for text to speech task. Here is my github repo. Thank you
https://github.com/soobinseo/Transformer-TTS",3,9,False,self,,,,,
1253,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,21,b3b905,redwerk.com,Software Development Technologies in Demand in Florida | Redwerk,https://www.reddit.com/r/MachineLearning/comments/b3b905/software_development_technologies_in_demand_in/,redwerk,1553083959,,0,1,False,https://b.thumbs.redditmedia.com/YTnJeBXerPn3JkPEF-_-abTOxv86g4_NVU6yUy2rNNY.jpg,,,,,
1254,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,21,b3be6v,self.MachineLearning,Do you prefer estimator and fit function in Keras?,https://www.reddit.com/r/MachineLearning/comments/b3be6v/do_you_prefer_estimator_and_fit_function_in_keras/,thisisiron,1553084872,"If you prefer an estimator, do you use tf.keras.estimator.model_to_estimator () and tf.estimator.inputs.numpy_input_fn()?

Let me know your opinion.",0,1,False,self,,,,,
1255,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,21,b3bemg,self.MachineLearning,The simplest example explaining AUC ROC using pen and paper. Hope you all like it!,https://www.reddit.com/r/MachineLearning/comments/b3bemg/the_simplest_example_explaining_auc_roc_using_pen/,bhavesh91,1553084949,[removed],0,1,False,self,,,,,
1256,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,21,b3bg1a,self.MachineLearning,"[R] A question about ""QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning""",https://www.reddit.com/r/MachineLearning/comments/b3bg1a/r_a_question_about_qmix_monotonic_value_function/,master_python,1553085191,"Why does the monotonicity (eq (5)) is enforced through the eq (4)?

&amp;#x200B;

Any related hints are welcome.

&amp;#x200B;

Thanks.",1,2,False,self,,,,,
1257,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,21,b3bhwm,self.MachineLearning,[P] A list of the biggest datasets for machine learning,https://www.reddit.com/r/MachineLearning/comments/b3bhwm/p_a_list_of_the_biggest_datasets_for_machine/,UpdraftDev,1553085505,"I've been assembling a list of datasets that would be interesting for experimenting with machine learning for a while and now I've put it online at [datasetlist.com](https://www.datasetlist.com/)

There's been an increasing number of large, high quality datasets released each year and most of them are published on their own individual websites so it might be difficult to find them all by googling around. I hope this helps someone find the data of their dreams.

Hit me with some feedback if you have time. I plan on keeping it updated when new datasets are released.",49,482,False,self,,,,,
1258,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,22,b3brjr,self.MachineLearning,Using GA for SVM feature classification. [P],https://www.reddit.com/r/MachineLearning/comments/b3brjr/using_ga_for_svm_feature_classification_p/,saurabh17noob,1553087082,I am new to the field of AI/ML and need some help to figure out a way to use GA for SVM on a medical multidimensional dataset. I would be glad if you could guide me to video or papers/books. ,1,1,False,self,,,,,
1259,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,22,b3btvh,blog.usejournal.com,Why BigQuery Is The Next Big Thing,https://www.reddit.com/r/MachineLearning/comments/b3btvh/why_bigquery_is_the_next_big_thing/,satyajit_,1553087457,,0,1,False,https://b.thumbs.redditmedia.com/4N21d7n8o4MHxWyPlL_IolnD9qOAsEep8dIqEPgd2qE.jpg,,,,,
1260,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,22,b3c1ei,self.MachineLearning,Color Sorter | Raspberry Pi meets Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b3c1ei/color_sorter_raspberry_pi_meets_machine_learning/,ahadcove,1553088631,[removed],0,1,False,self,,,,,
1261,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,22,b3c3ek,youtu.be,[R] Video: Attention Is All You Need,https://www.reddit.com/r/MachineLearning/comments/b3c3ek/r_video_attention_is_all_you_need/,ykilcher,1553088943,,0,1,False,default,,,,,
1262,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,22,b3c5dw,self.MachineLearning,reCAPTCHA Theory,https://www.reddit.com/r/MachineLearning/comments/b3c5dw/recaptcha_theory/,alt___tab,1553089253,[removed],0,1,False,self,,,,,
1263,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,22,b3c68c,apriorit.com,Action Detection Using Deep Neural Networks: Problems and Solutions,https://www.reddit.com/r/MachineLearning/comments/b3c68c/action_detection_using_deep_neural_networks/,RyanTmthn,1553089385,,0,1,False,default,,,,,
1264,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,23,b3cwrj,self.MachineLearning,How To Improve CycleGAN Performances?,https://www.reddit.com/r/MachineLearning/comments/b3cwrj/how_to_improve_cyclegan_performances/,nomad_world,1553093291,[removed],0,1,False,self,,,,,
1265,MachineLearning,t5_2r3gv,2019-3-20,2019,3,20,23,b3cxa3,self.MachineLearning,Work together on Deeplearning.ai,https://www.reddit.com/r/MachineLearning/comments/b3cxa3/work_together_on_deeplearningai/,sang89,1553093367,[removed],0,1,False,self,,,,,
1266,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,0,b3dni2,self.MachineLearning,"Simple Questions Thread March 20, 2019",https://www.reddit.com/r/MachineLearning/comments/b3dni2/simple_questions_thread_march_20_2019/,AutoModerator,1553097166,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!
",0,1,False,self,,,,,
1267,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,0,b3dpk3,self.MachineLearning,Potential of Machine Learning in Apps?,https://www.reddit.com/r/MachineLearning/comments/b3dpk3/potential_of_machine_learning_in_apps/,bennyllama,1553097461,[removed],0,1,False,self,,,,,
1268,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,1,b3dt69,arxiv.org,[R] Exact Gaussian Processes on a Million Data Points,https://www.reddit.com/r/MachineLearning/comments/b3dt69/r_exact_gaussian_processes_on_a_million_data/,mitare,1553097945,,5,34,False,default,,,,,
1269,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,1,b3e9at,i.redd.it,Artificial General Intelligence,https://www.reddit.com/r/MachineLearning/comments/b3e9at/artificial_general_intelligence/,maykulkarni,1553100152,,0,1,False,https://a.thumbs.redditmedia.com/ezTOui0YWN7PDsrBcllw4FgxWY75u3m40UuXjUCRzC8.jpg,,,,,
1270,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,3,b3f8y5,self.MachineLearning,Extracting user_id and movie title from cloudml movielens example.,https://www.reddit.com/r/MachineLearning/comments/b3f8y5/extracting_user_id_and_movie_title_from_cloudml/,polargingerpeach,1553105096,"Hello,

I have been using the Movielens tensorflow recommendation example from Google found [here](https://github.com/GoogleCloudPlatform/cloudml-samples/tree/master/movielens);  however, I'm having a hard time interpreting the output of the online  prediction because the json does not include the movie titles or the  user\_id association. Is this a known issue? If so, how should I be  reading the output of the online prediction?",0,1,False,self,,,,,
1271,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,3,b3fbha,medium.freecodecamp.org,NSFW Model - How To,https://www.reddit.com/r/MachineLearning/comments/b3fbha/nsfw_model_how_to/,GantMan,1553105440,,0,2,True,nsfw,,,,,
1272,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,3,b3fkak,self.MachineLearning,"AI Project Video Series : Credit Card Fraud Detection using Kafka, scikit, cassandra, flask",https://www.reddit.com/r/MachineLearning/comments/b3fkak/ai_project_video_series_credit_card_fraud/,awantik,1553106641,"Videos - [https://www.edyoda.com/course/1432](https://www.edyoda.com/course/1432)

Code - [https://github.com/zekelabs/ai-project-fraud-detection](https://github.com/zekelabs/ai-project-fraud-detection)

Please share your feedback so that I can improve

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
1273,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,3,b3fnwa,self.MachineLearning,[D] Pretrained image embedding models,https://www.reddit.com/r/MachineLearning/comments/b3fnwa/d_pretrained_image_embedding_models/,iliauk,1553107130,"I've been struggling to find pre-trained models to produce embedding for image similarity. It seems there are quite a few new SOTA results coming out based on different losses and also [sampling strategy](https://arxiv.org/pdf/1706.07567.pdf). 

I'm not averse to implementing that sampling strategy and training ResNet50 in PyTorch on the [Stanford Online Products Dataset](http://cvgl.stanford.edu/projects/lifted_struct/) however just surprised it's hard to find a model (whereas face embedding seem much more popular)",1,4,False,self,,,,,
1274,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,4,b3fyzm,self.MachineLearning,References to methods which consider orientation of features for classification,https://www.reddit.com/r/MachineLearning/comments/b3fyzm/references_to_methods_which_consider_orientation/,isthisathrowawaay,1553108683,[removed],0,1,False,self,,,,,
1275,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,4,b3fz9m,self.MachineLearning,[R] How to best cluster arbitrary 2D shapes,https://www.reddit.com/r/MachineLearning/comments/b3fz9m/r_how_to_best_cluster_arbitrary_2d_shapes/,WhiteSh4dow,1553108722,"Here's my problem definition: 

I have a large number of samples which contain binary values representing 2D shapes via matrices (either 1 if the value at that index is within the shape or 0 otherwise). All matrices are of the same dimension, currently 100x100, but eventually I want to be able to control the size of the samples (while still keeping all sample sizes equal). Additionally, all samples are all different to various degrees. 

&amp;#x200B;

My goal is to cluster similar samples together, in a way that is rotation invariant but \*not\* scale invariant (tricky, I know...).

&amp;#x200B;

Currently, I'm struggling with different steps in this exercise:

1. First I'd like to find a way to represent them in a more human friendly way. I was thinking about visualizing them in a 2D space to see how much intuition I could get from the clustering to judge the clustering effort by the algorithm.
2. Second, I have been scouting on the internet in search of the best clustering algorithm to tackle this specific problem but I have still not found an algorithm that has proven itself to solve my problem. So far, my best guess is SOM (Self-organizing Maps) but playing around with it, I can't seem to get a good clustering which is hard for me to blame on either the bad selection of the SOM hyperparameters or on the bad choice of the clustering algorithm without having achieved the first step mentioned above.

&amp;#x200B;

Does anyone have any suggestions on visualizing the samples or ideally on clustering algorithms that would fit my purpose best? Thanks in advance for your contribution!",17,3,False,self,,,,,
1276,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,4,b3g2as,medium.com,DeepMind TF-Replicator Simplifies Model Deployment on Cluster Architectures,https://www.reddit.com/r/MachineLearning/comments/b3g2as/deepmind_tfreplicator_simplifies_model_deployment/,Yuqing7,1553109142,,0,1,False,https://b.thumbs.redditmedia.com/WCpflTZvrsfpDrjpEAGcJHoWeK4VM3XdZEHvPFFUavY.jpg,,,,,
1277,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,4,b3g7p8,self.MachineLearning,Increased results for multiple layers of classifiers?,https://www.reddit.com/r/MachineLearning/comments/b3g7p8/increased_results_for_multiple_layers_of/,MacMat667,1553109882,[removed],0,1,False,self,,,,,
1278,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,4,b3gdyi,self.MachineLearning,[D] what are some good life lessons from ML?,https://www.reddit.com/r/MachineLearning/comments/b3gdyi/d_what_are_some_good_life_lessons_from_ml/,finallyifoundvalidUN,1553110750,,26,12,False,self,,,,,
1279,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,4,b3gg14,prnewswire.com,2Hz Launches NVIDIA GPU-based AI Voice Solution at GTC 2019,https://www.reddit.com/r/MachineLearning/comments/b3gg14/2hz_launches_nvidia_gpubased_ai_voice_solution_at/,ashotarzumanyan,1553111044,,0,1,False,https://b.thumbs.redditmedia.com/wfTMXN3VvoZYnKuyH45tpw83udDV8MIqFDgzAnmZgSQ.jpg,,,,,
1280,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,5,b3gp2i,self.MachineLearning,C++ for machine learning and AI,https://www.reddit.com/r/MachineLearning/comments/b3gp2i/c_for_machine_learning_and_ai/,heyitzzed,1553112294,[removed],0,1,False,self,,,,,
1281,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,5,b3gtic,self.MachineLearning,[D] Best approach to make a classifier model making multiple classifications?,https://www.reddit.com/r/MachineLearning/comments/b3gtic/d_best_approach_to_make_a_classifier_model_making/,EphemeralFate,1553112897,"The context is a text document classifier, but I suppose it applies to other tasks as well. 

For example, you're monitoring news articles in some industry. Article topics might be one category to classify--Company A, Company B, ... etc. Those articles might also have some other classification you want to track, for example, sentiment--positive, negative, neutral.

If your goal is to classify both

1. The topic

2. The sentiment classification

What would be the best way to do this?

Would you just make a category for each combination? Like,

1. Company A, positive
2. Company A, neutral
3. Company A, negative
4. Company B, positive
5. Company B, neutral
6. Company B, negative
7. ...

Or would you just make two classifiers, one tasked to classify the topic of an article, another one to classify the sentiment? This option seems to me like the better one, but I want to know if I'm missing something or if there are other considerations.
",2,2,False,self,,,,,
1282,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,5,b3gy13,self.MachineLearning,Probabilistic Model-Based Reinforcement Learning Using The Differentiable Neural Computer,https://www.reddit.com/r/MachineLearning/comments/b3gy13/probabilistic_modelbased_reinforcement_learning/,ThisIsMySeudonym,1553113512,"Research on using the Differentiable Neural Computer in the Reinforcement Learning / Evolution Strategies context: [http://blog.adeel.io/2018/09/10/probabilistic-model-based-reinforcement-learning-using-the-differentiable-neural-computer/](http://blog.adeel.io/2018/09/10/probabilistic-model-based-reinforcement-learning-using-the-differentiable-neural-computer/)

&gt;""...experiments found that a model learned in a Differentiable Neural Computer outperformed a vanilla LSTM based model, on two gaming environments.""

&amp;#x200B;",0,1,False,self,,,,,
1283,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,5,b3h475,github.com,[P] 2D car game to teach your reinforcement learning AI or genetic algorithm how to drive,https://www.reddit.com/r/MachineLearning/comments/b3h475/p_2d_car_game_to_teach_your_reinforcement/,swordythomas,1553114327,,0,1,False,https://b.thumbs.redditmedia.com/0Z4oFeNgwpDiGMi9t8PMWcAOIKRMg7bgmfkzBFkPemM.jpg,,,,,
1284,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,6,b3hgej,self.MachineLearning,MSc interview questions,https://www.reddit.com/r/MachineLearning/comments/b3hgej/msc_interview_questions/,careerninja01,1553115970,[removed],0,1,False,self,,,,,
1285,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,6,b3hqrf,linkedin.com,Dont Sit Too Close to the TV or How Does Technology Affect Our Lives,https://www.reddit.com/r/MachineLearning/comments/b3hqrf/dont_sit_too_close_to_the_tv_or_how_does/,dj_m_,1553117379,,0,1,False,https://a.thumbs.redditmedia.com/YLEgdIZRC3R0Dcq5vpdRvBOiZh2lR8M6r4pnehkx4Z0.jpg,,,,,
1286,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,6,b3i38i,self.MachineLearning,I'm creating a robot to jerk me off,https://www.reddit.com/r/MachineLearning/comments/b3i38i/im_creating_a_robot_to_jerk_me_off/,gusmeowmeow,1553119093,"I know this will seem like a joke but I assure you it is not. I was diagnosed a year ago with muscular dystrophy and will lose all motor function before long. I don't have a girlfriend or a wife and tend to take care of myself (jerking off) for sexual pleasure. Unfortunately I will not have this luxury for long. I am trying to build an AI powered robot to jerk me off after I become fully retarded. To implement this I have embedded a layer of semiconducting material lined with propriosensors on the inside of the latex layer of a fleshlight. I then jerk myself off to gay porn creating a time-series envelope with 3000+ degrees of freedom per data point. I have been feeding this data to a ""gay"" neural network such that the trained model will exactly represent the way I like to be jerked off. The only part I haven't figured out is how to feed that data back to a robotic hand-mouth that will translate it into the physical motions that I like. Has anyone ever through of or attempted something like this? Any advice will be appreciated. ",0,1,False,self,,,,,
1287,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,8,b3iv9c,github.com,Open Source Python library to perform Entity Embeddings using Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/b3iv9c/open_source_python_library_to_perform_entity/,opensource_sharer,1553123080,,0,1,False,https://b.thumbs.redditmedia.com/67g9U1dsDd2pQ6iTN8zZxwmFwqYl5XjtkQP-vxzB0Zk.jpg,,,,,
1288,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,8,b3j8l5,self.MachineLearning,Idea for joke detection,https://www.reddit.com/r/MachineLearning/comments/b3j8l5/idea_for_joke_detection/,happyhammy,1553125058,[removed],0,1,False,self,,,,,
1289,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,8,b3jgb3,arxiv.org,[1903.06758] Survey: Algorithms for Verifying Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/b3jgb3/190306758_survey_algorithms_for_verifying_deep/,jinpanZe,1553126200,,3,10,False,default,,,,,
1290,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,9,b3jlal,aws.amazon.com,"[N] AWS Deepracer League, worlds first global autonomous racing league",https://www.reddit.com/r/MachineLearning/comments/b3jlal/n_aws_deepracer_league_worlds_first_global/,luiscosio,1553126969,,0,1,False,https://b.thumbs.redditmedia.com/zvRNzfe7mOBGWBs_jdbIztBkn_K_u-MwkcZiJT2sC9g.jpg,,,,,
1291,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,9,b3jllm,github.com,[P] Package for receptive field calculation of arbitrary networks in PyTorch via gradients,https://www.reddit.com/r/MachineLearning/comments/b3jllm/p_package_for_receptive_field_calculation_of/,brainggear,1553127007,,0,1,False,default,,,,,
1292,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,9,b3jnru,self.MachineLearning,Confused about output of CNNs,https://www.reddit.com/r/MachineLearning/comments/b3jnru/confused_about_output_of_cnns/,eclifox,1553127305,[removed],0,1,False,self,,,,,
1293,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,9,b3jx3c,arxiv.org,[R] Deep Neural Networks Improve Radiologists' Performance in Breast Cancer Screening,https://www.reddit.com/r/MachineLearning/comments/b3jx3c/r_deep_neural_networks_improve_radiologists/,zphang,1553128766,,13,37,False,default,,,,,
1294,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,9,b3jy2y,self.MachineLearning,Does the shape of a tensor for an image affect the resulting output?,https://www.reddit.com/r/MachineLearning/comments/b3jy2y/does_the_shape_of_a_tensor_for_an_image_affect/,scriptcoder43,1553128924,[removed],0,1,False,self,,,,,
1295,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,10,b3kgf1,self.MachineLearning,Where to start,https://www.reddit.com/r/MachineLearning/comments/b3kgf1/where_to_start/,holymoley2020,1553131864,[removed],0,1,False,self,,,,,
1296,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,10,b3kqtu,self.MachineLearning,How may of you think that there will be a hardware only for training deeplearning models,https://www.reddit.com/r/MachineLearning/comments/b3kqtu/how_may_of_you_think_that_there_will_be_a/,cudanexus,1553133505,[removed],0,1,False,self,,,,,
1297,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,11,b3la0u,self.MachineLearning,Is it advisable to learn deeplearning without/before learning other types of ML?,https://www.reddit.com/r/MachineLearning/comments/b3la0u/is_it_advisable_to_learn_deeplearning/,Dave24x7,1553136600,[removed],0,1,False,self,,,,,
1298,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,12,b3ldxb,self.MachineLearning,Is it not common to compile Python code into EXE?,https://www.reddit.com/r/MachineLearning/comments/b3ldxb/is_it_not_common_to_compile_python_code_into_exe/,engineheat,1553137252,"I feel compiling into EXE file is usually done with C++ code, and Python is more for running a script. 

Is it true? Because I want to make a EXE file with a python code I wrote and there is very little resources on how to do it.

thanks",0,1,False,self,,,,,
1299,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,12,b3lhnu,self.MachineLearning,[P] Reinforcement Learning: Benchmarking TD3 and DDPG on PyBullet (x-post /r/reinforcementlearning),https://www.reddit.com/r/MachineLearning/comments/b3lhnu/p_reinforcement_learning_benchmarking_td3_and/,georgesung,1553137863,"[Here](https://github.com/georgesung/TD3) is a benchmark of TD3 and DDPG on the following PyBullet environments:
- HalfCheetah
- Hopper
- Walker2D
- Ant
- Reacher
- InvertedPendulum
- InvertedDoublePendulum

I simply used the [code from the authors of TD3](https://github.com/sfujim/TD3/), and ran it on the PyBullet environments (instead of MuJoCo environments). The TD3 and DDPG code were used to generate the results reported in the [TD3 paper](https://arxiv.org/abs/1802.09477).

**Motivation**:

I was trying to re-implement TD3 myself and evaluate it on the PyBullet environments, but soon realized there was no good benchmark to see how well my implementation was doing. When reading research papers, the algorithms are (almost?) always benchmarked on MuJoCo environments. As an individual, this is a problem:

- MuJoCo personal licenses are $500 USD per year for non-students.
- Even if I buy the license, the license is hardware-locked to 3 machines =( This means I cannot run MuJoCo experiments on AWS/GCP/etc. This problem also applies to the free personal student licenses, which are hardware-locked to 1 machine.

Fortunately, the authors of the TD3 paper have open-sourced their code, and IMO the code is very clearly written. I had some free Google Cloud credits lying around, so I decided to benchmark the TD3 authors' implementation of TD3 and DDPG on the PyBullet envs HalfCheetah, Hopper, Walker2D, Ant, Reacher, InvertedPendulum, and InvertedDoublePendulum -- the TD3 paper reports results from the MuJoCo version of those environments.

Hope this helps anyone in a similar situation!
",5,10,False,self,,,,,
1300,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,12,b3lo7y,self.MachineLearning,"[R] Meet Q, The Worlds First Genderless Voice Assistant",https://www.reddit.com/r/MachineLearning/comments/b3lo7y/r_meet_q_the_worlds_first_genderless_voice/,navin49,1553138957,"Hello Friends,

Meet Q the worlds firstgenderless voice assistant that neither sounds like a male nor like a female and pretty much capable of doing all things that your Alexa, Siri or Google Home can do. 

It is originally created  by researchersfrom Copenhagen Pride have come up with such a voice assistant, that is genderless.

The idea behind Qis to give things like smart speakers a gender-neutral voice in order to inject an element of inclusion into AI tech,as well as remove gender bias.

The development of Q started by recording voices of two dozen people who identify as male, female, transgender, or nonbinary.Researchers then tested the voice on 4,600 people across Europe.

[**Here is the demo  of that voice and in detail report**](https://techgrabyte.com/q-genderless-voice-assistant/)

***Please also share your opinion, do you really want a Genderless Voice Assistant or nor ?***

&amp;#x200B;",27,0,False,self,,,,,
1301,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,13,b3m5xn,self.MachineLearning,Machine Learning in Science,https://www.reddit.com/r/MachineLearning/comments/b3m5xn/machine_learning_in_science/,hofmax92,1553142069,[removed],0,1,False,self,,,,,
1302,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,14,b3mwa3,self.MachineLearning,[D] Seven Myths in Machine Learning Research,https://www.reddit.com/r/MachineLearning/comments/b3mwa3/d_seven_myths_in_machine_learning_research/,baylearn,1553147242,"tldr; We present seven myths commonly believed to be true in machine learning research, circa Feb 2019.

1. TensorFlow is a Tensor manipulation library

2. Image datasets are representative of real images found in the wild

3. Machine Learning researchers do not use the test set for validation

4. Every datapoint is used in training a neural network

5. We need (batch) normalization to train very deep residual networks

6. Attention &gt; Convolution

7. Saliency maps are robust ways to interpret neural networks

https://crazyoscarchang.github.io/2019/02/16/seven-myths-in-machine-learning-research/",11,33,False,self,,,,,
1303,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,14,b3myxu,self.MachineLearning,Reinforcement learning,https://www.reddit.com/r/MachineLearning/comments/b3myxu/reinforcement_learning/,ravager_ro,1553147875,[removed],0,1,False,self,,,,,
1304,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,14,b3mz6j,self.MachineLearning,[P] Coconet: the ML model behind todays Bach Doodle,https://www.reddit.com/r/MachineLearning/comments/b3mz6j/p_coconet_the_ml_model_behind_todays_bach_doodle/,chisai_mikan,1553147930,"*In this blog post, we introduce Coconet, the machine learning model behind the Doodle. We started working on this model 3 years ago, the summer when Magenta launched. At the time we were using machine learning (ML) only to generate melodies. Its hard to write a good melody, let alone counterpoint, where multiple melodic lines need to sound good together. Like every music student, we turned to Bach for help! Using a dataset of 306 chorale harmonizations by Bach, we were able to train machine learning models to generate polyphonic music in the style of Bach.*

demo (works for today only): https://google.com

blog post: https://magenta.tensorflow.org/coconet",69,227,False,self,,,,,
1305,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,15,b3n0j3,self.MachineLearning,Research/Work regarding 3d hand pose estimation,https://www.reddit.com/r/MachineLearning/comments/b3n0j3/researchwork_regarding_3d_hand_pose_estimation/,krshna53,1553148219,[removed],0,1,False,self,,,,,
1306,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,15,b3n9y5,github.com,[Paper Implementation] A Neural Compositional Paradigm for Image Captioning (NeurIPS'18),https://www.reddit.com/r/MachineLearning/comments/b3n9y5/paper_implementation_a_neural_compositional/,ajaysub110,1553150444,,0,1,False,default,,,,,
1307,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,16,b3nfri,self.MachineLearning,How do StyleGANs differ from ProGANs in terms of localized feature control?,https://www.reddit.com/r/MachineLearning/comments/b3nfri/how_do_stylegans_differ_from_progans_in_terms_of/,dxjustice,1553151824,[removed],0,1,False,self,,,,,
1308,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,17,b3nvtf,self.MachineLearning,[P] Exposing OpenAI's GPT-2 model over containerised APIs,https://www.reddit.com/r/MachineLearning/comments/b3nvtf/p_exposing_openais_gpt2_model_over_containerised/,t04glovern,1553155578,"Over the last couple days I've gotten really interested in OpenAI's GPT-2. I took some of the awesome work from [https://github.com/graykode/gpt-2-Pytorch](https://github.com/graykode/gpt-2-Pytorch) and [https://github.com/WillKoehrsen/recurrent-neural-networks](https://github.com/WillKoehrsen/recurrent-neural-networks) to create a nicer way of interacting with PyTorch models fronted with Flask APIs

&amp;#x200B;

* I've got a **blog post outlining the process** here: [https://devopstar.com/2019/03/21/openai-gpt-2-pytorch-model-over-a-containerised-api/](https://devopstar.com/2019/03/21/openai-gpt-2-pytorch-model-over-a-containerised-api/)
* **All the code** here: [https://github.com/t04glovern/gpt-2-flask-api](https://github.com/t04glovern/gpt-2-flask-api)

&amp;#x200B;

I'm a bit of a noob when it comes to actual ML, but I'm finding working on the Ops and orchestration is getting me really excited about it!",0,8,False,self,,,,,
1309,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,17,b3o86l,picnic.devpost.com,[P]: Picnic Hackathon: Classifying Damaged Products for Customer Success Automation.,https://www.reddit.com/r/MachineLearning/comments/b3o86l/p_picnic_hackathon_classifying_damaged_products/,philleonard,1553158524,,0,1,False,default,,,,,
1310,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,18,b3odcg,self.MachineLearning,Question about Turing Test,https://www.reddit.com/r/MachineLearning/comments/b3odcg/question_about_turing_test/,helloiambrain,1553159779,[removed],0,1,False,self,,,,,
1311,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,18,b3odp9,self.MachineLearning,"Step by Step Guide to create your own credit card fraud detection system using kafka, flask, scikit, cassandra",https://www.reddit.com/r/MachineLearning/comments/b3odp9/step_by_step_guide_to_create_your_own_credit_card/,awantik,1553159868,[removed],0,1,False,self,,,,,
1312,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,18,b3oilv,self.MachineLearning,"Paper clips, pancakes, and the pursuit of AI happiness",https://www.reddit.com/r/MachineLearning/comments/b3oilv/paper_clips_pancakes_and_the_pursuit_of_ai/,thefuture_isnow,1553161024,[removed],0,1,False,self,,,,,
1313,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,18,b3ojoe,self.MachineLearning,Sampling MultinomialDB,https://www.reddit.com/r/MachineLearning/comments/b3ojoe/sampling_multinomialdb/,harmanchawla,1553161263,"Can someone explain to me, and not how every other post on the internet does, what a multinationalDB is? 

Also, if given a task of sampling 1000 entries from a billion entry database, whats the best way to go about it?",0,1,False,self,,,,,
1314,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,19,b3oskg,arxiv.org,[1903.08129] Hyper-Parameter Sweep on AlphaZero General,https://www.reddit.com/r/MachineLearning/comments/b3oskg/190308129_hyperparameter_sweep_on_alphazero/,ihaphleas,1553163228,,4,3,False,default,,,,,
1315,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,20,b3pai3,nature.com,Scientists rise up against statistical significance,https://www.reddit.com/r/MachineLearning/comments/b3pai3/scientists_rise_up_against_statistical/,t1m3f0rt1m3r,1553166952,,0,1,False,https://a.thumbs.redditmedia.com/ktt8XBYyIuZ_fN_fBVRO83lpex-mRHyI58BpbtFp1O0.jpg,,,,,
1316,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,20,b3piij,self.MachineLearning,[D] Why does one need lots of computation power if one can just increase the learning rate (and search for an appropriate decay rate)?,https://www.reddit.com/r/MachineLearning/comments/b3piij/d_why_does_one_need_lots_of_computation_power_if/,derhexer,1553168528,,6,0,False,self,,,,,
1317,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,21,b3po99,self.MachineLearning,[D] Life Lessons from Epsilon Greedy,https://www.reddit.com/r/MachineLearning/comments/b3po99/d_life_lessons_from_epsilon_greedy/,sigmoidp,1553169617,"Howdy folks,

Started writing this blog post out of my notes on a lecture I gave on Epsilon Greedy at the Microsoft Reactor here in Sydney last year. Long story short, the post ended up being something quite different from the original lecture notes but believe that hopefully the form it ended up being is more valuable.

The title of the the post is not what it seems and would really like to hear any feedback / criticism the community has to offer on this one.

&amp;#x200B;

Anyways, here it is:

[http://www.humanalgorithms.co/post/how-to-find-your-life-partner-using-machine-learning/](http://www.humanalgorithms.co/post/how-to-find-your-life-partner-using-machine-learning/)",9,9,False,self,,,,,
1318,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,21,b3q1ax,self.MachineLearning,"[news] Nordic Probabilistic AI School (ProbAI), June 3-7, Trondheim (Norway)",https://www.reddit.com/r/MachineLearning/comments/b3q1ax/news_nordic_probabilistic_ai_school_probai_june/,zehsilva,1553171936,"ProbAI, a new annual event serving state-of-the-art expertise in probabilistic modelling for Machine Learning and Artificial Intelligence to the research community of students, academics and industry members. The applications deadline is March 31 and we encourage early applications.

Applications: https://probabilistic.ai/registration/
-------
Our objective is to bring an intermediate to advanced level summer school with a particular focus on probabilistic models and deep generative models, covering the topics of latent variable models, inference with sampling and variational approximations, and probabilistic programming and tools.

The intentionally small team of invited lecturers will cover a carefully designed curriculum. Through tight cooperation between our lecturers, and through theoretical lectures and hands-on tutorials, we hope to provide a high quality continuous and consistent knowledge transfer. 

The registration fee includes all courses, coffee breaks, lunches and banquet.

* Students (including PhD)  2500 NOK ~ 256 EUR
* Academia  5000 NOK ~ 512 EUR
* Industry  10000 NOK ~ 1024 EUR

We are also offering a limited number of scholarships.

The summer school is organized by the [Norwegian Open AI Lab](https://www.ntnu.edu/ailab) and the Norwegian University of Science and Technology (NTNU).

List of confirmed speakers for lectures and tutorials (more to be included):

* [Adji Bousso Dieng](https://scholar.google.com/citations?user=ZCniP_MAAAAJ&amp;hl=en) (Columbia University) - Deep Generative Models
* [Arto Klami](https://scholar.google.com/citations?user=v8PeLGgAAAAJ&amp;hl=en) (Helsinki University) - Variational Inference and Optimization
* [Thomas Dyhre Nielsen](https://scholar.google.com/citations?user=6fWF0CgAAAAJ&amp;hl=en) (Aalborg University) - Probabilistic Programming and Variational Inference
* [Andrs Masegosa](https://scholar.google.com/citations?user=J1zoY7AAAAAJ&amp;hl=en) (University of Almeria) - Probabilistic Programming and Variational Inference
* [Antonio Salmern](https://scholar.google.com/citations?user=41enG0oAAAAJ&amp;hl=en) (University of Almeria) - Probabilistic Modelling 

List of talks and keynotes:

* [Jo Eidvisk](https://scholar.google.no/citations?user=ZXXGw1wAAAAJ&amp;hl=en) - Value of Information
* [Benjamin Dunn](https://scholar.google.no/citations?user=hAjFjXkAAAAJ&amp;hl=en) - Topological (neural) Data Science
* Moret TBA

Visit our website to learn more: https://probabilistic.ai/
Program: https://probabilistic.ai/program/
Applications: https://probabilistic.ai/registration/

Contact us via:

- email: hello@probabilistic.ai
- twitter: https://twitter.com/probabilisticai/
- facebook: https://www.facebook.com/probabilisticai/

The organizing team:

Heri Ramampiaro, Helge Langseth, Tarik S. Salem and Eliezer de Souza da Silva",12,22,False,self,,,,,
1319,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,21,b3q1z4,youtube.com,Live Launch of Automower 435X AWD at MWC19 in Barcelona,https://www.reddit.com/r/MachineLearning/comments/b3q1z4/live_launch_of_automower_435x_awd_at_mwc19_in/,Martin81,1553172048,,0,1,False,default,,,,,
1320,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,22,b3qggm,self.MachineLearning,3 Steps to Improve the Data Quality of a Data lake,https://www.reddit.com/r/MachineLearning/comments/b3qggm/3_steps_to_improve_the_data_quality_of_a_data_lake/,stolbiq,1553174491,[removed],0,1,False,self,,,,,
1321,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,22,b3qn5u,self.MachineLearning,Dilbert as a GIF using deep learning,https://www.reddit.com/r/MachineLearning/comments/b3qn5u/dilbert_as_a_gif_using_deep_learning/,uberuberubee,1553175574,[removed],0,1,False,self,,,,,
1322,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,22,b3qpy3,self.MachineLearning,"Stock Prediction Model, Latest Progress From Getting a CSV file to a Prediction Model.",https://www.reddit.com/r/MachineLearning/comments/b3qpy3/stock_prediction_model_latest_progress_from/,the_coder_dot_py,1553176009,[removed],0,1,False,self,,,,,
1323,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,22,b3qqzb,self.MachineLearning,[P] Automatic conversion of Comics into GIF using deep learning,https://www.reddit.com/r/MachineLearning/comments/b3qqzb/p_automatic_conversion_of_comics_into_gif_using/,uberuberubee,1553176173,"This ([https://comic2gif.com](https://comic2gif.com))  project uses deep learning to remove the text bubbles from dilbert and produces a GIF which seems like a better way to consume comics, I maybe biased. Here is the complete process detailed: [https://medium.com/@mkagenius/dilbert-as-a-gif-f39c6a134020](https://medium.com/@mkagenius/dilbert-as-a-gif-f39c6a134020)",19,40,False,self,,,,,
1324,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,23,b3r59h,self.MachineLearning,Good Starting book for visualization,https://www.reddit.com/r/MachineLearning/comments/b3r59h/good_starting_book_for_visualization/,PerfectImperfection7,1553178340,[removed],0,1,False,self,,,,,
1325,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,23,b3rdnb,g.co,Todays Doodle dedicated to Bach and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b3rdnb/todays_doodle_dedicated_to_bach_and_machine/,Gontxi,1553179596,,0,1,False,default,,,,,
1326,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,23,b3rdwg,self.MachineLearning,Suggestion on How to Start,https://www.reddit.com/r/MachineLearning/comments/b3rdwg/suggestion_on_how_to_start/,mlMix,1553179631,[removed],0,1,False,self,,,,,
1327,MachineLearning,t5_2r3gv,2019-3-21,2019,3,21,23,b3ritz,self.MachineLearning,Some questions about Convolutional Networks in Tensforflow,https://www.reddit.com/r/MachineLearning/comments/b3ritz/some_questions_about_convolutional_networks_in/,Jehovacoin,1553180364,[removed],0,1,False,self,,,,,
1328,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,0,b3rsga,medium.com,AttoNets: Compact and Efficient DNNs Realized via Human-Machine Collaborative,https://www.reddit.com/r/MachineLearning/comments/b3rsga/attonets_compact_and_efficient_dnns_realized_via/,gwen0927,1553181718,,0,1,False,https://b.thumbs.redditmedia.com/gXgELkKhM0ip16LuFBwAVX_H1oG25LKoG0793BNuWZI.jpg,,,,,
1329,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,0,b3s0f1,self.MachineLearning,Domain-Adversarial Training of Neural Networks (DANN),https://www.reddit.com/r/MachineLearning/comments/b3s0f1/domainadversarial_training_of_neural_networks_dann/,magejangle,1553182811,[removed],1,1,False,self,,,,,
1330,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,0,b3s1aq,self.MachineLearning,Which algorithm do I need?,https://www.reddit.com/r/MachineLearning/comments/b3s1aq/which_algorithm_do_i_need/,frankie285,1553182932,[removed],0,1,False,self,,,,,
1331,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,0,b3s4h9,community.ibm.com,A landscape diagram for Python data,https://www.reddit.com/r/MachineLearning/comments/b3s4h9/a_landscape_diagram_for_python_data/,pigboy_in_a_bottle,1553183364,,0,1,False,https://b.thumbs.redditmedia.com/tMXpxhE0i80Jpou7SinJ_faK9a7fm7wnLpR6Q8Y30PE.jpg,,,,,
1332,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,0,b3s6z4,bogdanbocse.com,"On the duties of intelligence, in vivo and in silico",https://www.reddit.com/r/MachineLearning/comments/b3s6z4/on_the_duties_of_intelligence_in_vivo_and_in/,bocse,1553183706,,0,1,False,https://b.thumbs.redditmedia.com/up57VFBxP5kAcgHhYLc03mgIh9baxwG3m_z3Pr5e1fE.jpg,,,,,
1333,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,1,b3sdkr,inveritasoft.com,7 Reasons Why Machine Learning Will Improve Supply Chain Management | InVeritaSoft,https://www.reddit.com/r/MachineLearning/comments/b3sdkr/7_reasons_why_machine_learning_will_improve/,inveritasoft,1553184591,,1,1,False,https://a.thumbs.redditmedia.com/BhWssJkDTEeTor5vEU0HNmBE-h1KRvvbwoEBOxfzc60.jpg,,,,,
1334,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,1,b3sfxn,self.MachineLearning,"Random question (for those of you familiar with ""game theory""....)",https://www.reddit.com/r/MachineLearning/comments/b3sfxn/random_question_for_those_of_you_familiar_with/,JKolodne,1553184926,[removed],0,1,False,self,,,,,
1335,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,1,b3syay,self.MachineLearning,"[Discussion] Sign Language Recognition, dealing with different gestures for same word: Do we map to one class or a class per gesture?",https://www.reddit.com/r/MachineLearning/comments/b3syay/discussion_sign_language_recognition_dealing_with/,sthsthanothersth,1553187401,"Hello ML redditers.

I am working on Isolated Word Sign Language Recognition; the problem is a classification problem whereby a video with someone performing a gesture is mapped to a specific word.  However, I come from a part in the world whereby the videos describing gestures, can contain 2 or more different gestures (in movement, timing, etc.) and these gestures would still map to the same word. 

&amp;#x200B;

If while collecting a data set, I encounter this many-to-one gesture-word mapping problem (e.g. 10 videos, n of them gesturing ""cat"" in way\_1 and 10-n gesturing ""cat"" in way\_2). Should I label them all under the same class ""cat"", or label them as ""cat\_1"" and ""cat\_2"".

&amp;#x200B;

CONTEXT: I am currently working on a website for public data set collection, where we'll ask users to contribute short videos labelled with a word class, to be reviewed by an expert in another section of the website. The goal of the website is to provide a data set which will make it easier to research this topic in the future. 

Making the gesture-word mapping one-to-one will add extra complexity to the website (contribution page inputs, reviewing page) and may harm usability. ",4,17,False,self,,,,,
1336,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,2,b3t8n0,goo.gl,[P] survey for my HNC project on AI in medical procedures.,https://www.reddit.com/r/MachineLearning/comments/b3t8n0/p_survey_for_my_hnc_project_on_ai_in_medical/,111660,1553188808,,0,1,False,default,,,,,
1337,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,2,b3tb6q,self.MachineLearning,Road from beginner to Expert in Machine learning.,https://www.reddit.com/r/MachineLearning/comments/b3tb6q/road_from_beginner_to_expert_in_machine_learning/,kundanML,1553189156,[removed],1,1,False,self,,,,,
1338,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,2,b3tp7j,community.ibm.com,[D] A landscape diagram for Python data,https://www.reddit.com/r/MachineLearning/comments/b3tp7j/d_a_landscape_diagram_for_python_data/,pigboy_in_a_bottle,1553191049,,0,1,False,https://b.thumbs.redditmedia.com/tMXpxhE0i80Jpou7SinJ_faK9a7fm7wnLpR6Q8Y30PE.jpg,,,,,
1339,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,3,b3ttio,self.MachineLearning,Career &amp; Earning Growth await IT Professionals - Big Data Hadoop Solutions Architect Masters Certification,https://www.reddit.com/r/MachineLearning/comments/b3ttio/career_earning_growth_await_it_professionals_big/,internetdigitalentre,1553191591,[removed],0,1,False,https://a.thumbs.redditmedia.com/ciL3L-_elfDZpnYnLnuqhljYy1I3FRUtMp_Tv90T420.jpg,,,,,
1340,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,3,b3twl9,self.MachineLearning,Recommendation for time serie database for ML ?,https://www.reddit.com/r/MachineLearning/comments/b3twl9/recommendation_for_time_serie_database_for_ml/,pirsonalthrowaway,1553191987,[removed],0,1,False,self,,,,,
1341,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,4,b3uipn,pgaleone.eu,[P] Analyzing tf.function to discover AutoGraph strengths and subtleties,https://www.reddit.com/r/MachineLearning/comments/b3uipn/p_analyzing_tffunction_to_discover_autograph/,pgaleone,1553194941,,0,1,False,https://b.thumbs.redditmedia.com/lewKR1e6UJU-J9Er_cLGlBRossmR9O7PuNHi46H8ElE.jpg,,,,,
1342,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,4,b3ujwx,self.MachineLearning,Product Recommendation Tool,https://www.reddit.com/r/MachineLearning/comments/b3ujwx/product_recommendation_tool/,Bluewolf867,1553195100,"I'm trying to put together a product recommendation tool for collectible items on eBay. I have a data set of a list of different collectible items that are of interest, the condition they are in, the value they have been sold for over time and various other characteristics. Including, based on the most recent selling prices of similar items, how much they have potentially appreciated in value. 

&amp;#x200B;

I'm building a neural network in python using Keras, to predict which products to bid on and how much to bid. 

&amp;#x200B;

By training the model on all items that have appreciated in value, I get a good probability that any particular item will appreciate in value and a to bid or not to bid answer. 

&amp;#x200B;

However I need to try and figure out how much to value it at? How would one do this? 

&amp;#x200B;",0,1,False,self,,,,,
1343,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,4,b3use0,deepmind.com,Machine learning can boost the value of wind energy,https://www.reddit.com/r/MachineLearning/comments/b3use0/machine_learning_can_boost_the_value_of_wind/,techgig11,1553196247,,0,1,False,default,,,,,
1344,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,4,b3uvxz,sites.google.com,AGI Project - Back Engineering the Human Brain - A neuromorphic approach to AGI,https://www.reddit.com/r/MachineLearning/comments/b3uvxz/agi_project_back_engineering_the_human_brain_a/,Korrelan,1553196746,,0,1,False,https://b.thumbs.redditmedia.com/mnV8Fca0pzmAynJKgshqWtKYsJHA-gGKEKBzwF5OpYE.jpg,,,,,
1345,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,4,b3v0bo,i.redd.it,Autoencoders and representations. PLS help me solve!!,https://www.reddit.com/r/MachineLearning/comments/b3v0bo/autoencoders_and_representations_pls_help_me_solve/,mollyh97,1553197324,,0,1,False,https://b.thumbs.redditmedia.com/UYwEk33pebPgf5GRNbTUdV_pfZgjH7KGeKyTMG4n9Ko.jpg,,,,,
1346,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,4,b3v7fz,self.MachineLearning,[P] Paul Bettany (JARVIS) voice dataset?,https://www.reddit.com/r/MachineLearning/comments/b3v7fz/p_paul_bettany_jarvis_voice_dataset/,draripov,1553198309,"I have plans on programming a voice assistant to control all of my IoT equipment. I dont simply want to use the standard Google Text to Speech, so I was wondering if there is a dataset of Paul Bettanys voices I could use to create a TTS.

If anyone has information on how it can be obtained, please share.",33,65,False,self,,,,,
1347,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,5,b3vdpm,medium.com,New Study Uses Machine Learning to Predict Sexual Orientation,https://www.reddit.com/r/MachineLearning/comments/b3vdpm/new_study_uses_machine_learning_to_predict_sexual/,gwen0927,1553199166,,0,1,False,default,,,,,
1348,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,5,b3ve0a,self.MachineLearning,Machine Learning Process Summarized in Two Pictures,https://www.reddit.com/r/MachineLearning/comments/b3ve0a/machine_learning_process_summarized_in_two/,andrea_manero,1553199205,[removed],0,1,False,self,,,,,
1349,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,5,b3vj1x,papers.nips.cc,[P] Computing Higher Order Derivatives of Matrix and Tensor Expressions,https://www.reddit.com/r/MachineLearning/comments/b3vj1x/p_computing_higher_order_derivatives_of_matrix/,PK_thundr,1553199921,,0,1,False,default,,,,,
1350,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,5,b3vvah,self.MachineLearning,IBM watson Studio any good ?,https://www.reddit.com/r/MachineLearning/comments/b3vvah/ibm_watson_studio_any_good/,SufficientMeal,1553201624,"
My company just purchased IBM Watson studio and they want to use it as a framework for model creation.
It sounds interesting since the tool comes with a user interface for model creations, data cleansing, drag and drop etc.. and also supports a collaborative tool for ML programmers.

Theoretically it sounds good.. but I haven't tried it yet.
Im more of a fan of small M.L microservoces written in Python/Java using scikit learn /tensor flow and existing in their small environment.

How does IBM Watson studio compare against building your own M.L models.

Is it helpful using Watson studio ? Since it seems like a one stop solution for machine learning development.
",0,1,False,self,,,,,
1351,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,6,b3w4uk,self.MachineLearning,[P] Pretrained PyTorch implementation of BigGAN,https://www.reddit.com/r/MachineLearning/comments/b3w4uk/p_pretrained_pytorch_implementation_of_biggan/,Thomjazz,1553202970,"[https://github.com/huggingface/pytorch-pretrained-BigGAN](https://github.com/huggingface/pytorch-pretrained-BigGAN)  
An op-for-op PyTorch reimplementation of DeepMind's BigGAN model which can load the pre-trained weights of the 128, 256 and 512 models by DeepMind.

The reimplementation was done from the raw computation graph of the Tensorflow version and behave very similarly to the TensorFlow version (variance of the output difference between the two version of the order of 1e-5).

This implementation currently contains only the generator as the weights of the discriminator have not been released.

More information and details on the github readme.

Enjoy!",1,16,False,self,,,,,
1352,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,6,b3wg9o,i.redd.it,Training requires a lot of patience,https://www.reddit.com/r/MachineLearning/comments/b3wg9o/training_requires_a_lot_of_patience/,riiswa,1553204591,,0,1,False,default,,,,,
1353,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,7,b3wqrb,medium.com,NVIDIA CEO Says No Rush on 7nm GPU; Company Clearing Its Crypto Chip Inventory,https://www.reddit.com/r/MachineLearning/comments/b3wqrb/nvidia_ceo_says_no_rush_on_7nm_gpu_company/,gwen0927,1553206074,,0,1,False,https://a.thumbs.redditmedia.com/c-kyE88x-PGNlR47x-azrOWW8KiOEUg0_z4kwy51Fw4.jpg,,,,,
1354,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,8,b3xvow,self.MachineLearning,[D] Does the shape of a tensor for an image affect the resulting output?,https://www.reddit.com/r/MachineLearning/comments/b3xvow/d_does_the_shape_of_a_tensor_for_an_image_affect/,scriptcoder43,1553212217,"I am representing images of size 100px by 100px, so I can have the shape (None, 100, 100, 3) or shape (None, 10000, 3)

I can't find any clear explanation on Google, however, will the following two tensors result in similar results?

1. (None, 100, 100, 3)
2. (None, 10000, 3)

I assume either is sufficient as I would have thought the neural network will still learn just as well if the image is in a single row, your thoughts?",2,2,False,self,,,,,
1355,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,10,b3z5oh,self.bigdata,How wouls you find value in these queries?,https://www.reddit.com/r/MachineLearning/comments/b3z5oh/how_wouls_you_find_value_in_these_queries/,johne898,1553219541,,0,1,False,default,,,,,
1356,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,11,b3z9fn,self.MachineLearning,[P] ML for Bernie Campaign,https://www.reddit.com/r/MachineLearning/comments/b3z9fn/p_ml_for_bernie_campaign/,TechForChange,1553220179,"Hi everyone! I am trying to get together a group of volunteer data scientists to work on projects to help out Bernie. We are not officially connected to the campaign, although hopefully we can get in communication with someone from there soon. We have some ideas about some types of datasets we could use, and we would like to hear more ideas from people about project ideas and data sources that might be useful for the campaign.  

&amp;#x200B;

Message me if you want to get involved!",13,0,False,self,,,,,
1357,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,11,b3zlha,self.MachineLearning,[P] OpenAI's GPT-2-based Reddit Bot is Live!,https://www.reddit.com/r/MachineLearning/comments/b3zlha/p_openais_gpt2based_reddit_bot_is_live/,Shevizzle,1553222198,"[Original post](https://www.reddit.com/r/MachineLearning/comments/b32lve/d_im_using_openais_gpt2_to_generate_text_give_me/)

Based on the popularity of my post from the other day, I decided to go ahead an build a full-fledged Reddit bot. So without further ado, please welcome:

# u/GPT-2_Bot

&amp;#x200B;

If you want to use the bot, all you have to do is reply to any comment with the following command word:

# ""gpt-2 finish this""

(your reply can contain other stuff as well, i.e. ""hey gpt-2 finish this argument for me, will ya?"")

&amp;#x200B;

The bot will then look at the comment you replied to and generate its own response. It will tag you in the response so you know when it's done!

&amp;#x200B;

Enjoy! :) Feel free to PM me with feedback",1113,308,False,self,,,,,
1358,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,13,b40saa,self.MachineLearning,"[R] OpenAI brings back Energy-Based Models ""Implicit Generation and Generalization Methods for Energy-Based Models""",https://www.reddit.com/r/MachineLearning/comments/b40saa/r_openai_brings_back_energybased_models_implicit/,downtownslim,1553229918,"&gt; Weve made progress towards stable and scalable training of energy-based models (EBMs) resulting in better sample quality and generalization ability than existing models. Generation in EBMs spends more compute to continually refine its answers and doing so can generate samples competitive with GANs at low temperatures, while also having mode coverage guarantees of likelihood-based models. We hope these findings stimulate further research into this promising class of models.

https://openai.com/blog/energy-based-models/",0,32,False,self,,,,,
1359,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,14,b40yhg,self.MachineLearning,Machine learning for plant biology,https://www.reddit.com/r/MachineLearning/comments/b40yhg/machine_learning_for_plant_biology/,GladiatorialPig,1553231174,"What are some of the recent trends in machine learning that are used in plant biotechnology or environmental science? 
My friend is currently looking for research proposals on ways to automate some of the key and important aspects in these two sectors that are otherwise difficult to process. 
For eh : leaf classification using neural networks. But this project has already been done. 
Is there any other key segments in this field that still hadn't found its way in ML. ",0,1,False,self,,,,,
1360,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,15,b41gyi,github.com,Financial Machine Learning Projects (Python),https://www.reddit.com/r/MachineLearning/comments/b41gyi/financial_machine_learning_projects_python/,OppositeMidnight,1553235100,,0,1,False,https://b.thumbs.redditmedia.com/wr6vHQ8KGHH9C5purCVDXoHBMKzLEnFczD0I-HhSIyM.jpg,,,,,
1361,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,15,b41hx4,self.MachineLearning,Single-word datasets ??,https://www.reddit.com/r/MachineLearning/comments/b41hx4/singleword_datasets/,Cybergear791,1553235295,[removed],0,1,False,self,,,,,
1362,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,15,b41j6v,self.MachineLearning,[D] SSD Training: What is the trainval135k dataset for MS COCO mentioned in the SSD paper and how is it split?,https://www.reddit.com/r/MachineLearning/comments/b41j6v/d_ssd_training_what_is_the_trainval135k_dataset/,mad_runner,1553235570,"Hi,

I wanted to experiment with training runs of SSD on the MS COCO dataset. The original paper mentioned that they use the trainval135k dataset for MS COCO. Where can I find this dataset? I'm unable to find it on the MS COCO webpage.

Also what's the train-val split for the original training run?",7,2,False,self,,,,,
1363,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,15,b41kgp,self.MachineLearning,4 Ways Autonomous Vehicles are Driving Innovation with Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/b41kgp/4_ways_autonomous_vehicles_are_driving_innovation/,mirrorreviewm,1553235856,[removed],0,1,False,self,,,,,
1364,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,15,b41oll,self.MachineLearning,FACIAL RECOGNITION (WITH MORE THAN 1000 CLASSES),https://www.reddit.com/r/MachineLearning/comments/b41oll/facial_recognition_with_more_than_1000_classes/,kashish_28,1553236747,[removed],0,1,False,self,,,,,
1365,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,15,b41rgi,self.MachineLearning,The portal where public can get more information regarding ML,https://www.reddit.com/r/MachineLearning/comments/b41rgi/the_portal_where_public_can_get_more_information/,rohithajapro,1553237360,[removed],0,1,False,self,,,,,
1366,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,16,b424sd,self.MachineLearning,Winnow Vision: Computer vision enabled food waste analytics,https://www.reddit.com/r/MachineLearning/comments/b424sd/winnow_vision_computer_vision_enabled_food_waste/,Hectic1015,1553240239,[removed],0,1,False,self,,,,,
1367,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,16,b425me,/r/MachineLearning/comments/b425me/google_doodle_bach_style/,Google Doodle Bach Style,https://www.reddit.com/r/MachineLearning/comments/b425me/google_doodle_bach_style/,mili_m3011,1553240422,,0,1,False,default,,,,,
1368,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,16,b425qj,i.redd.it,Gaining Competitive Advantage with Azure Machine Learning: https://goo.gl/nDPEcf #azure #machinelearning #cloud,https://www.reddit.com/r/MachineLearning/comments/b425qj/gaining_competitive_advantage_with_azure_machine/,Staci287,1553240449,,0,1,False,default,,,,,
1369,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,17,b42kyk,self.MachineLearning,Reinforce AI conference LIVE STREAM,https://www.reddit.com/r/MachineLearning/comments/b42kyk/reinforce_ai_conference_live_stream/,kritya947,1553243724,"We are streaming **Reinforce AI conference** , dont miss it, Today we have speakers from OpenAI, Twitter, Google, Ericsson, SecretLab, IBM, and Uber.

Check out our livestream at [https://reinforceconf.com/](https://reinforceconf.com/)",0,1,False,self,,,,,
1370,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,17,b42ofy,medium.com,Meta-learning in finance: boosting models calibration with deep learning,https://www.reddit.com/r/MachineLearning/comments/b42ofy/metalearning_in_finance_boosting_models/,rachnogstyle,1553244544,,0,1,False,https://b.thumbs.redditmedia.com/9s7Y36psr91VANx6Urppgee8cjNIUm9W5MHqipFIhtQ.jpg,,,,,
1371,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,17,b42r3m,self.MachineLearning,Grocery Store dataset,https://www.reddit.com/r/MachineLearning/comments/b42r3m/grocery_store_dataset/,azteks,1553245167,[removed],0,1,False,self,,,,,
1372,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,18,b42yv5,self.MachineLearning,[P] NLP - Filling in Blanks of Arbitrary Length,https://www.reddit.com/r/MachineLearning/comments/b42yv5/p_nlp_filling_in_blanks_of_arbitrary_length/,AnonMLstudent,1553246848,"Are there any methods or architectures that can work well for filling in blanks in sentences that can be of any arbitrary length? I know the MLM (masked language model) has a mask token for each word that is masked, but what about a masked sequence that can be any length long? 

For example:

I _______ in the park today. 

----&gt; ""took a walk"", ""walked my dog"", ""let my two children play"" 

The above three examples could all be potential sequences to fill in the blank of varying lengths. ",5,1,False,self,,,,,
1373,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,18,b43303,self.MachineLearning,Newbie to ML,https://www.reddit.com/r/MachineLearning/comments/b43303/newbie_to_ml/,RohanCR797,1553247755,[removed],0,1,False,self,,,,,
1374,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,18,b436fa,self.MachineLearning,Couldn't think of a protagonist for a platformer game. Used a GAN for inspiration...,https://www.reddit.com/r/MachineLearning/comments/b436fa/couldnt_think_of_a_protagonist_for_a_platformer/,syncope33,1553248490,[removed],0,1,False,self,,,,,
1375,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,19,b43ard,self.MachineLearning,[D] Book about Neural Language Processing,https://www.reddit.com/r/MachineLearning/comments/b43ard/d_book_about_neural_language_processing/,Horror_Counter,1553249406,"I have a decent experience with Deep Learning for Computer Vision (I can use/modify a model such as Mask-RCNN for semantic segmentation). I need to learn about neural NLP (NLP using deep nets - is that the right term?) because we have multiple use cases in the company. Which books could I use? I like some theoretical background, but I need to be able to solve practical problems, rather than proving theorems or inventing new architectures. I can't decide between these two:

https://www.amazon.com/Language-Processing-Synthesis-Lectures-Technologies/dp/1627052984

https://www.amazon.com/gp/aw/d/1491978236",12,4,False,self,,,,,
1376,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,19,b43i6c,self.MachineLearning,I am exploring Google Magenta's WaveNet for my undergraduate dissertation and need help understanding how it works,https://www.reddit.com/r/MachineLearning/comments/b43i6c/i_am_exploring_google_magentas_wavenet_for_my/,GreyPaper,1553250945,[removed],0,1,False,self,,,,,
1377,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,19,b43iwl,jgoertler.com,[D]A Visual Exploration of Gaussian Processes,https://www.reddit.com/r/MachineLearning/comments/b43iwl/da_visual_exploration_of_gaussian_processes/,obsoletelearner,1553251117,,0,1,False,default,,,,,
1378,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,19,b43l7d,self.MachineLearning,I'm writing about Google Magenta's NSynth for my undergraduate dissertation and need help understanding it,https://www.reddit.com/r/MachineLearning/comments/b43l7d/im_writing_about_google_magentas_nsynth_for_my/,GreyPaper,1553251592,"Hello,

I'm an undergraduate music student, and I decided to focus my undergraduate dissertation on examples of AI/machine learning software related to music.

I have spent the past few days trying to reach a basic understanding of NSynth but I'm struggling. I've been reading Neural Audio Synthesis of Musical Notes (Jesse et. al 2017) and WaveNet: A Generative Model (van der Oord et. al 2016), but there is too much jargon for me to be able to access it fully.

I don't need an in-depth knowledge of how the system works in entirety as the focus of my dissertation is music, however I don't want to misinterpret NSynth's organisation in my writing.

Therefore it would be very helpful if someone could do something like an ELI5 of NSynth. Just exploring the basic structure of it and how it might be significant or different to other systems.

As a side note, if there are any parts of WaveNet which resemble feedback loops or neural networks, then it would be helpful for me if you discussed this also.

&amp;#x200B;

Thanks in advance to anyone who spends their time helping me,

A confused music student.",0,1,False,self,,,,,
1379,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,20,b43x97,medium.com,A simple formula for writing effective data science articles,https://www.reddit.com/r/MachineLearning/comments/b43x97/a_simple_formula_for_writing_effective_data/,omarsar,1553254031,,0,1,False,default,,,,,
1380,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,20,b43xw8,self.MachineLearning,Has anyone used MC-dropout for Bayesian neural network regression? How is the test log likelihood defined?,https://www.reddit.com/r/MachineLearning/comments/b43xw8/has_anyone_used_mcdropout_for_bayesian_neural/,alayaMatrix,1553254155,[removed],0,1,False,self,,,,,
1381,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,20,b441hg,self.MachineLearning,R/Pandas vs SQL/Excel (i'm very slow at R/Pandas),https://www.reddit.com/r/MachineLearning/comments/b441hg/rpandas_vs_sqlexcel_im_very_slow_at_rpandas/,doormass,1553254855,[removed],0,1,False,self,,,,,
1382,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,21,b44pks,self.MachineLearning,Classify time series with unequal lenghts,https://www.reddit.com/r/MachineLearning/comments/b44pks/classify_time_series_with_unequal_lenghts/,tita_tatou,1553259146,[removed],0,1,False,self,,,,,
1383,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,22,b453ni,monkeylearn.com,A Comprehensive Guide to Aspect-based Sentiment Analysis,https://www.reddit.com/r/MachineLearning/comments/b453ni/a_comprehensive_guide_to_aspectbased_sentiment/,numbrow,1553261457,,0,1,False,default,,,,,
1384,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,22,b458bf,self.MachineLearning,Entry Level Languages/Skills/Qualifications,https://www.reddit.com/r/MachineLearning/comments/b458bf/entry_level_languagesskillsqualifications/,Hellololil,1553262207,[removed],0,1,False,self,,,,,
1385,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,23,b45fbe,techerati.com,"A brief history of intelligence, and what it means for the future of AI",https://www.reddit.com/r/MachineLearning/comments/b45fbe/a_brief_history_of_intelligence_and_what_it_means/,TheJCOEco,1553263322,,0,1,False,default,,,,,
1386,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,23,b45h09,self.MachineLearning,Is it possible?,https://www.reddit.com/r/MachineLearning/comments/b45h09/is_it_possible/,JKolodne,1553263580,[removed],0,1,False,self,,,,,
1387,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,23,b45lqt,youtube.com,Sentiment analysis in Excel | text2data.com,https://www.reddit.com/r/MachineLearning/comments/b45lqt/sentiment_analysis_in_excel_text2datacom/,-text2data,1553264285,,0,1,False,default,,,,,
1388,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,23,b45mxt,self.MachineLearning,How do I understand the results of this script?,https://www.reddit.com/r/MachineLearning/comments/b45mxt/how_do_i_understand_the_results_of_this_script/,TheVitoCorleone,1553264470,[removed],1,1,False,self,,,,,
1389,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,23,b45n6e,self.MachineLearning,largest successful amount of output neurons in deep RL?,https://www.reddit.com/r/MachineLearning/comments/b45n6e/largest_successful_amount_of_output_neurons_in/,toisanji,1553264504,[removed],0,1,False,self,,,,,
1390,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,23,b45tgd,self.MachineLearning,Recreate GauGAN,https://www.reddit.com/r/MachineLearning/comments/b45tgd/recreate_gaugan/,Sarthaks21,1553265447,"NVidia recently released their paper titled  ""Semantic Image Synthesis with Spatially-Adaptive Normalization"" ([https://arxiv.org/abs/1903.07291](https://arxiv.org/abs/1903.07291)). The GAN architecture turns a random doodle into a piece of art, as captured by a camera in a real world setting. 

Here is a video showing it's working:

 [https://www.inverse.com/article/54198-nvidia-gans-app-turns-doodles-into-art](https://www.inverse.com/article/54198-nvidia-gans-app-turns-doodles-into-art)

I have never created a GAN architecture before. I only know how GANs work.  I would like to recreate the whole network from scratch. 

Please suggest learning resources or papers I should consider recreating first before tackling this one. If interested in partnering up, then message me. 

Thank you for replying. ",0,1,False,self,,,,,
1391,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,23,b45to5,self.MachineLearning,Putting AI into practice,https://www.reddit.com/r/MachineLearning/comments/b45to5/putting_ai_into_practice/,milike45,1553265483,[removed],0,1,False,self,,,,,
1392,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,23,b45tuh,self.MachineLearning,"[D] ML-based image/video compression: ""AE"" vs ""AE +regularizer"" vs ""VAE + bits back"" ... ?",https://www.reddit.com/r/MachineLearning/comments/b45tuh/d_mlbased_imagevideo_compression_ae_vs_ae/,jarekduda,1553265512,"The space of let say images in everyday applications is much smaller than of all bitmaps  restricting to such subspace allows to define one of them with a much smaller number of bits, providing better compression. 

While being more costly than classical e.g. Fourier-based compressors like JPG, machine learning allows for much more accurate modelling of the space of real images, and there are papers claiming revolutionary improvement in compression ratios  this way, especially from **WaveOne** - for **image compression** ( https://arxiv.org/pdf/1705.05823 ) and recently also for **video** ( http://www.wave.one/video-compression/ ).

While they quantize and encode latent variable of standard AutoEncoder, we can add **regularizer** ([WAE](https://arxiv.org/pdf/1711.01558)/[SWAE](https://arxiv.org/pdf/1804.01947)/[CWAE](https://arxiv.org/pdf/1805.09235)/[GAE](https://arxiv.org/pdf/1811.04751)...) to minimized distortion loss to enforce e.g. Gaussian distribution in latent space - which should be better for further encoding.

It turns out we can also use indeterministic **VAE** thanks to **bits-back** approach ( https://arxiv.org/pdf/1901.04866  https://youtu.be/grsO57XMJMk?t=2396 ).

Are there other interesting approaches for ML-based image compression?

Which seems the most promising?

When they can realistically replace JPG ... video compression?",2,0,False,self,,,,,
1393,MachineLearning,t5_2r3gv,2019-3-22,2019,3,22,23,b461zt,self.MachineLearning,[P] Want to train your own BigGAN on just 4-8 GPUs? Today we're releasing BigGAN-PyTorch,https://www.reddit.com/r/MachineLearning/comments/b461zt/p_want_to_train_your_own_biggan_on_just_48_gpus/,ajmooch,1553266719,"Hi everybody, today I'm very happy to be releasing [BigGAN-PyTorch](https://github.com/ajbrock/BigGAN-PyTorch), a complete PyTorch reimplementation that uses gradient accumulation to spoof big batches even on small hardware.

We include training, testing, and sampling scripts, as well as full pretrained checkpoints (Generators, Discriminators, and optimizers) so that you can fine-tune on your own data, or train a model from scratch.

This code is a long time in the making, and has been designed from the ground up to be a hackable, extensible base for research code. I've put a lot of thought into exactly what abstractions to use and where--making sure they're thick enough to be useful, but thin enough to be easy to understand or change--and I'm pretty happy with out it's turned out.

This repo is joint work with [Alex Andonian](https://twitter.com/_alexandonian) of MIT.",41,252,False,self,,,,,
1394,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,0,b466m4,self.MachineLearning,Who's doing the most interesting research in ML?,https://www.reddit.com/r/MachineLearning/comments/b466m4/whos_doing_the_most_interesting_research_in_ml/,Bubblegum_ML,1553267380,[removed],0,1,False,self,,,,,
1395,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,0,b469ma,self.MachineLearning,[R] Genetic Algorithms &amp; Feature scaling for data that varies by several orders of magnitude (0 --&gt; 10^10),https://www.reddit.com/r/MachineLearning/comments/b469ma/r_genetic_algorithms_feature_scaling_for_data/,More_Momus,1553267804,"Hey All, 

So I'm just looking for some direction about an implementation of a genetic algorithm for a project I'm working on. I will try to be as informative and concise as possible disclosing more than I'm allowed, but if there is any confusion because of this: sorry!

So I'm trying to optimize a variety of factors that kill bacteria. The bacteria start in the system at a certain amount (somewhere between 10^6 cfu/mL --10^8 cfu/mL) and can either grow up to 10^10 (with insufficient killing) or be eradicated to 0. 

The goal is to obtain optimal killing without deviating too much on certain parameters (e.g., amount of chemical A vs. chemical B as compared to currently allowable standards). Where the fitness function would be ""penalized"" for using more, but ""rewarded"" for using less (which you can depending on certain other factors) 

My problem is, I was looking at different ways to scale/normalize the bacterial count data, and its still hard to get it to be appropriately comparable to the chemical amounts. 

Should I use the log-transformed count data instead? Do any of you have alternate ideas on implementation? 

Unfortunately, I couldn't find any literature using machine learning techniques on bacterial data, but I'm much less familiar with the machine learning literature than I am the bacterial. 

Thanks for the help!",7,2,False,self,,,,,
1396,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,0,b46b7u,self.MachineLearning,[P] Curated List of Arbitrary Text to Image Papers,https://www.reddit.com/r/MachineLearning/comments/b46b7u/p_curated_list_of_arbitrary_text_to_image_papers/,lzhbrian,1553268025,"[lzhbrian/arbitrary-text-to-image-papers](https://github.com/lzhbrian/arbitrary-text-to-image-papers)

Still preliminary. PR, issues, discussions are welcomed!",0,6,False,self,,,,,
1397,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,0,b46c2u,self.MachineLearning,Which Model To Use to Predict Whether or Not a Person Will Attend a Weekly Event the Following Week.,https://www.reddit.com/r/MachineLearning/comments/b46c2u/which_model_to_use_to_predict_whether_or_not_a/,tdotmans,1553268150,[removed],0,1,False,self,,,,,
1398,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,0,b46gjv,self.MachineLearning,[P] Adding Layers to the middle of a pre-trained network without invalidating the weights using Tensorflow 2.0 and Google Collab,https://www.reddit.com/r/MachineLearning/comments/b46gjv/p_adding_layers_to_the_middle_of_a_pretrained/,Tenoke,1553268765,"I've been experimenting with adding layers in the middle of a pre-trained neural network without making the weights of the latter layers useless. 

[Here](https://svilentodorov.xyz/blog/add-layers) is a blog post (with a link to a working Collab) where I show some simple examples.

The next step in my experiments will be to try to extend the GPT-small model from OpenAI since we already know the bigger model works better. Any comments/links/etc. for doing that are appreciated.",15,22,False,self,,,,,
1399,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,0,b46i28,self.MachineLearning,[Project] Predicting whether or not a person will attend a weekly event the following week.,https://www.reddit.com/r/MachineLearning/comments/b46i28/project_predicting_whether_or_not_a_person_will/,tdotmans,1553268972,"Hi all, I already posted this question on r/statistics, but didn't get much help so I'm posting here as well.

I have a task at my internship to build a model to predict if a client will attend weekly webcast event the company hosts, the following week. End result should be along the lines of the marketing team now knows who to send more emails to get them out to the event .I'm just starting to learn about machine learning, so this will sort of be the first legit model  have built.

I have about the last years worth of data of whether or not the client has attended the weekly webcasts and some other categorical variables about the client. I understand that a logistic regression should work here, since I have a binary outcome, has attended or did not. However, through my research and the first model I've ran, I've struggled to handle the time(forecasting) component of the problem. I understand that I can use time series models, like ARIMA, but I'm not familiar with these things so I'm not sure if its the proper approach.

So far I have fitted some basic Poisson models for individual clients using their average days between attending to find the probability of them coming out the next week, but I feel like there should be a better way, that can also use some of the relationships in the categorical data.

Does anybody have a recommendation on what model or models I can use to try and obtain the outcome I need?

Thank you in advance for the help.
",13,7,False,self,,,,,
1400,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,1,b476oc,self.MachineLearning,Picking a ML research sub-field as an incoming PhD student,https://www.reddit.com/r/MachineLearning/comments/b476oc/picking_a_ml_research_subfield_as_an_incoming_phd/,ijkml,1553272369,[removed],0,1,False,self,,,,,
1401,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,1,b47cgn,self.MachineLearning,What Will Shape the Future of Machine Learning in 2018?,https://www.reddit.com/r/MachineLearning/comments/b47cgn/what_will_shape_the_future_of_machine_learning_in/,andrea_manero,1553273183,[removed],0,1,False,self,,,,,
1402,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,1,b47ex7,emilygorcenski.com,Why building ML products is inherently hard,https://www.reddit.com/r/MachineLearning/comments/b47ex7/why_building_ml_products_is_inherently_hard/,elgehelge,1553273554,,0,1,False,default,,,,,
1403,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,1,b47fpv,self.MachineLearning,What is the best face detection(**not recognition**) algorithm?,https://www.reddit.com/r/MachineLearning/comments/b47fpv/what_is_the_best_face_detectionnot_recognition/,k4rth33k,1553273672,[removed],0,1,False,self,,,,,
1404,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,2,b47kcf,self.MachineLearning,[P] Jetson Nano mini Benchmark,https://www.reddit.com/r/MachineLearning/comments/b47kcf/p_jetson_nano_mini_benchmark/,pilooch,1553274314,"We've conducted a quick benchmark of the Jetson Nano, see here:
https://github.com/jolibrain/dd_performances

It's FP32, using our modified Caffe (https://github.com/jolibrain/caffe) and Squezenet can hit up to 48 FPS. 

This mini benchmark can be seen as a complement to this benchmark with much more optimization (TensorRT, FP16, INT8): https://www.phoronix.com/scan.php?page=article&amp;item=nvidia-jetson-nano&amp;num=3",12,24,False,self,,,,,
1405,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,2,b47t0t,arstechnica.com,You can help rescue weather data from the 1860s,https://www.reddit.com/r/MachineLearning/comments/b47t0t/you_can_help_rescue_weather_data_from_the_1860s/,ktkps,1553275559,,0,1,False,https://b.thumbs.redditmedia.com/l1i9hhfF7Ja78ThKdr1OZNnDWJsa78poXgYLOsxc97s.jpg,,,,,
1406,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,2,b47ul2,researchgate.net,Album cover generation from genre tags (oldie but goodie),https://www.reddit.com/r/MachineLearning/comments/b47ul2/album_cover_generation_from_genre_tags_oldie_but/,vzakharov,1553275774,,0,1,False,default,,,,,
1407,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,2,b4800q,self.MachineLearning,Similarity metric for the output of speech recognition model,https://www.reddit.com/r/MachineLearning/comments/b4800q/similarity_metric_for_the_output_of_speech/,zedsdead01,1553276488,[removed],0,1,False,self,,,,,
1408,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,3,b48fzn,self.MachineLearning,[D] Xavier and Kaiming intialization,https://www.reddit.com/r/MachineLearning/comments/b48fzn/d_xavier_and_kaiming_intialization/,Raktatata,1553278650,"Hi everyone!

I made a blog post explaining Xavier and Kaiming initialization, and I thought it could be useful here.

[Here's the link.]([https://pouannes.github.io/blog/initialization/](https://pouannes.github.io/blog/initialization/)

It's my first post in this community so I hope this isn't against the rules :)",6,13,False,self,,,,,
1409,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,4,b49ax9,self.MachineLearning,"Orhun Project Next Generation Data Framework  Data Science,Machine Learning,Artificial Intelligence Notes",https://www.reddit.com/r/MachineLearning/comments/b49ax9/orhun_project_next_generation_data_framework_data/,formatlar,1553282903,[removed],0,1,False,self,,,,,
1410,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,5,b49t5v,incompleteideas.net,[D] The Bitter Lesson - General methods are ultimately the most effective,https://www.reddit.com/r/MachineLearning/comments/b49t5v/d_the_bitter_lesson_general_methods_are/,heipei42,1553285412,,1,1,False,default,,,,,
1411,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,6,b4auob,self.MachineLearning,Statistical tests for significance,https://www.reddit.com/r/MachineLearning/comments/b4auob/statistical_tests_for_significance/,sarasotadude,1553290777,"Im working on a time series project and I want to verify that Im not just modeling noise.  What sort of statistical tests can I run to determine that Ive correctly identifies signal and Im not just modeling noise?

Thanks in advance.  I really appreciate all feedback",0,1,False,self,,,,,
1412,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,6,b4b2me,arstechnica.com,Potential machine learning / handwriting recognition challenge: Rescue weather data from the 1860s,https://www.reddit.com/r/MachineLearning/comments/b4b2me/potential_machine_learning_handwriting/,neuromantik8086,1553291941,,0,1,False,https://b.thumbs.redditmedia.com/l1i9hhfF7Ja78ThKdr1OZNnDWJsa78poXgYLOsxc97s.jpg,,,,,
1413,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,7,b4bmhx,self.MachineLearning,Is Mining of Massive Datasets still a good MOOC to do?,https://www.reddit.com/r/MachineLearning/comments/b4bmhx/is_mining_of_massive_datasets_still_a_good_mooc/,petulla,1553294828,[removed],0,1,False,self,,,,,
1414,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,9,b4cggj,self.MachineLearning,[P] Attention Values Given Sentence and Specific Word,https://www.reddit.com/r/MachineLearning/comments/b4cggj/p_attention_values_given_sentence_and_specific/,AnonMLstudent,1553299500,"This may be a dumb question, but I know for the normal attention mechanism, you are generating an output sequence from an input sequence, and for each output token, you produce a vector of attention weights on the input words. However, is this possible with just a given sentence?

For example, let's say I have the sentence:
""I am very happy today as I am going to the movies with my friends.""

Is there a way to just get attention weights/values for each word in that sentence given a specific word like ""movies""? So for example, higher attention values would hopefully be assigned to ""happy"" and ""friends"" than other words in the sentence. In this case, there is no output or anything, I literally just want to feed in this sentence and see which words in the sentence most relate to the word ""movies"".",4,4,False,self,,,,,
1415,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,9,b4cpkj,self.MachineLearning,"Southeast Asia Machine Learning School (July 8-12, 2019)",https://www.reddit.com/r/MachineLearning/comments/b4cpkj/southeast_asia_machine_learning_school_july_812/,seamlschool,1553300982,[removed],0,1,False,self,,,,,
1416,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,9,b4ctxk,self.MachineLearning,"[N] Southeast Asia Machine Learning School (July 8-12, 2019)",https://www.reddit.com/r/MachineLearning/comments/b4ctxk/n_southeast_asia_machine_learning_school_july_812/,seamlschool,1553301714,"Southeast Asia Machine Learning School (SEA MLS)

Depok (Greater Jakarta), Indonesia

July 8th - July 12th 2019

Website: [https://sites.google.com/view/seamls/home](https://sites.google.com/view/seamls/home)

Contact:  [seaml-team@googlegroups.com](mailto:seaml-team@googlegroups.com)

=======================================

The first edition of the Southeast Asia Machine Learning School ([SEA MLS](https://sites.google.com/view/seamls/home)) will be held at Universitas Indonesia in Depok (Greater Jakarta), Indonesia, between July 8th - July 12th, 2019. The SEA MLS provides a unique opportunity for the underrepresented Southeast Asian machine learning community to learn from the top experts in the field, engage with like-minded students, academics, researchers, industry practitioners, and thought leaders in the field, and understand the current and future challenges in machine learning research, theory, and applications.

SCOPE

As the Southeast Asian community has been largely underrepresented in the machine learning field, the SEA MLS aims to kickstart an effort to inspire, encourage, and educate more machine learning engineers, researchers, and data scientists within the Southeast Asian region and beyond in the coming years. In particular, we work towards a critical vision of more Southeast Asians as pioneers, contributors, and shapers within the machine learning community.

INVITED SPEAKERS

1. [Chris Dyer](https://scholar.google.com/citations?user=W2DsnAkAAAAJ&amp;hl=en) (DeepMind)
2. [Kyunghyun Cho](https://scholar.google.co.uk/citations?user=0RAmmIAAAAAJ&amp;hl=en) (New York University and Facebook AI Research)
3. [Manaal Faruqui](https://www.manaalfaruqui.com/) (Google)
4. [Douwe Kiela](https://douwekiela.github.io/) (Facebook AI Research)
5. [Hung Bui](https://sites.google.com/site/buihhung/) (VinAI Research)
6. [Yun-Nung Vivian Chen](https://www.csie.ntu.edu.tw/~yvchen/) (National Taiwan University)
7. [Wee Sun Lee](https://www.comp.nus.edu.sg/~leews/) (National University of Singapore)
8. [Wray Buntine](https://bayesian-models.org/) (Monash University)

We are currently inviting more speakers, which will be updated on the [website](https://sites.google.com/corp/view/seamls/speakers).

&amp;#x200B;

APPLICATION

We are currently inviting applications for the SEA MLS. The application process can be accessed [here](https://sites.google.com/corp/view/seamls/participate); please consult our [FAQ](https://sites.google.com/view/seamls/faq) for more details. The application process will close on April 20th 2019.Everyone is welcome to apply regardless of nationality or place of residence.  No deep knowledge or prior experience in machine learning is required, although participants are expected to have some basic background on probabilities and programming.

&amp;#x200B;

TRAVEL GRANTS

We will have a limited number of financial assistance available for student participants traveling from within the Southeast Asian region. Depending on the financial needs, the travel grant can include travel cost reimbursements (including airplane tickets, up to a certain amount) and accommodation near the event venue, although we may not be able to cover the costs in full.

&amp;#x200B;

STRUCTURE

The SEA MLS will cover a one-week program that includes technical lecture sessions on basic and intermediate topics in machine learning and its applications, along with panel sessions. In addition, the event features a hands-on practical session to learn how to implement deep learning models in a modern software stack, social sessions, and poster presentation sessions. A tentative schedule is available [here](https://sites.google.com/corp/view/seamls/schedule_new).

&amp;#x200B;

REGISTRATION FEES

Student participants - free

General participants - US$ 200

The registration fee covers access to al lectures, panels, practical sessions, and social events, along with lunch and two coffee breaks per day.

&amp;#x200B;

VENUE

The event will be held at [Fasilkom](http://www.cs.ui.ac.id/?lang=en), [Universitas Indonesia](http://www.ui.ac.id/) in Depok, West Java, which is a part of the Greater Jakarta Metropolitan Area, Indonesia. The venue is about 1.5 hour drive from Jakarta's main airport, Soekarno-Hatta International Airport (CGK), and about an hour drive from Jakarta's Central Business District.

&amp;#x200B;

CONTACT

Please direct any queries to [seaml-team@googlegroups.com](mailto:seaml-team@googlegroups.com). Thank you for your kind attention and please do not hesitate to contact us for any questions.",6,40,False,self,,,,,
1417,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,12,b4e7yo,self.MachineLearning,Human leg area recognition from image?,https://www.reddit.com/r/MachineLearning/comments/b4e7yo/human_leg_area_recognition_from_image/,vrvc014pssi,1553310654,[removed],0,1,False,https://b.thumbs.redditmedia.com/09XatsZm1lIfnkuBY6vNdYpdIvutGTgoIe6-2YcXDiQ.jpg,,,,,
1418,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,12,b4e9if,self.MachineLearning,Has anyone uses Pop OS for machine learning?,https://www.reddit.com/r/MachineLearning/comments/b4e9if/has_anyone_uses_pop_os_for_machine_learning/,bbk_b,1553310939,I found a lot of mixed reviews regarding this. Is it worth switching from Ubuntu? Please help me out. ,0,1,False,self,,,,,
1419,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,13,b4ewtr,self.MachineLearning,Mimicking Exact Speech Pattern With a Different Voice?,https://www.reddit.com/r/MachineLearning/comments/b4ewtr/mimicking_exact_speech_pattern_with_a_different/,BwillWall,1553315246,"Hey everyone, I was wondering if there was any developed projects for essentially making ""deep fakes for speech"" where you could, for example, change the voice on a specific clip to sound like somebody else. This would be different than Lyrebird because as far as I'm aware that's simply text to speech and doesn't match specific tones and timing etc. Thanks to anyone who can help with this!",0,1,False,self,,,,,
1420,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,13,b4f4gm,self.MachineLearning,[D] Approximating expectation with Taylor series,https://www.reddit.com/r/MachineLearning/comments/b4f4gm/d_approximating_expectation_with_taylor_series/,cuenta4384,1553316656,"Hello, 

&amp;#x200B;

Any suggestions to approximate [this expectation?](https://stats.stackexchange.com/questions/398988/approximating-expectation-with-taylor-series) 

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/id3ltpjbusn21.png",13,3,False,https://b.thumbs.redditmedia.com/UGM4CghVJSYmPVSg0TqFH5vJ-CDqOdB1m867KhTyhKw.jpg,,,,,
1421,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,14,b4fdtq,nature.com,"study suggests that humans can decipher how machines classify images, even the adversarial ones that are designed to fool the machines.",https://www.reddit.com/r/MachineLearning/comments/b4fdtq/study_suggests_that_humans_can_decipher_how/,Science_Podcast,1553318421,,1,1,False,default,,,,,
1422,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,14,b4fnbz,self.MachineLearning,What is the difference between Tiny Yolo and Yolo.. Which one is better?,https://www.reddit.com/r/MachineLearning/comments/b4fnbz/what_is_the_difference_between_tiny_yolo_and_yolo/,RemasterZ,1553320330,[removed],0,1,False,self,,,,,
1423,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,14,b4fo2p,self.MachineLearning,Deep Learning Rules of Thumb ( and share your own),https://www.reddit.com/r/MachineLearning/comments/b4fo2p/deep_learning_rules_of_thumb_and_share_your_own/,conradws,1553320480,[removed],0,1,False,self,,,,,
1424,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,15,b4ftmd,youtube.com,UiStyle GAN for transferring fonts and style - Generating websites from mockups using GANs,https://www.reddit.com/r/MachineLearning/comments/b4ftmd/uistyle_gan_for_transferring_fonts_and_style/,pranitkothari,1553321595,,1,1,False,default,,,,,
1425,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,15,b4fvnj,self.MachineLearning,Cmap-matplotlib,https://www.reddit.com/r/MachineLearning/comments/b4fvnj/cmapmatplotlib/,Saisree311,1553322023,[removed],0,1,False,self,,,,,
1426,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,15,b4g469,self.MachineLearning,[N] Welcome to formatlar.com and Social Media Access Channels,https://www.reddit.com/r/MachineLearning/comments/b4g469/n_welcome_to_formatlarcom_and_social_media_access/,formatlar,1553323847,"Thanks for your interest.

I have a lot of  kindly question and high visitor traffic on my blog.

So, I decided to write **new articles** to answer questions and share on different social media channels.

Especially, I will write articles to explain dark area **custom**  ANN.  

**New articles will publish on** [http://formatlar.com](http://formatlar.com)

You can access from different social media channels and web sides.

You can see last minutes activity on different channels.

&amp;#x200B;

Channels are,

&amp;#x200B;

[https://formatlar.tumblr.com/](https://formatlar.tumblr.com/)

[https://www.linkedin.com/in/omer-yavuz-18b5a25/](https://www.linkedin.com/in/omer-yavuz-18b5a25/)

[https://www.reddit.com/user/formatlar/](https://www.reddit.com/user/formatlar/)

[https://www.facebook.com/datascienceAIML](https://www.facebook.com/datascienceAIML)

[https://www.reddit.com/r/DataScienceArticles/](https://www.reddit.com/r/DataScienceArticles/)

[https://twitter.com/omeryavuz](https://twitter.com/omeryavuz)

[http://blog.omeryavuz.gen.tr](http://blog.omeryavuz.gen.tr/)

[http://omeryavuz.gen.tr](http://omeryavuz.gen.tr/)

[http://formatlar.com](http://formatlar.com/)

[https://www.reddit.com/r/ArtificialNotes/](https://www.reddit.com/r/ArtificialNotes/)

[https://www.reddit.com/r/DataScienceNotes/](https://www.reddit.com/r/DataScienceNotes/)

[https://www.reddit.com/r/deeplearnings/](https://www.reddit.com/r/deeplearnings/)

[https://www.reddit.com/r/MachineLearningNotes/](https://www.reddit.com/r/MachineLearningNotes/)

[https://www.reddit.com/r/OrhunProject/](https://www.reddit.com/r/OrhunProject/)",0,0,False,self,,,,,
1427,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,17,b4gt7c,lilianweng.github.io,[P] Are Deep Neural Networks Dramatically Overfitted?,https://www.reddit.com/r/MachineLearning/comments/b4gt7c/p_are_deep_neural_networks_dramatically_overfitted/,xternalz,1553329571,,0,1,False,default,,,,,
1428,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,17,b4guj2,intellectyx.com,Conversational Design - A date with Watson Conversational chatbot,https://www.reddit.com/r/MachineLearning/comments/b4guj2/conversational_design_a_date_with_watson/,vijay2208,1553329889,,0,1,False,default,,,,,
1429,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,17,b4gwd6,twitter.com,SPECIAL Offer The Complete SQL Bootcamp DISCOUNT 94% off,https://www.reddit.com/r/MachineLearning/comments/b4gwd6/special_offer_the_complete_sql_bootcamp_discount/,MargieDMoses,1553330338,,0,1,False,default,,,,,
1430,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,18,b4h36y,self.MachineLearning,[D] 10 Useful AI / ML Slides,https://www.reddit.com/r/MachineLearning/comments/b4h36y/d_10_useful_ai_ml_slides/,seemingly_omniscient,1553332011,"According to the motto: A picture says more than a thousand words some useful slides with a short explanation are shown below.

&amp;#x200B;

Slides overview: [https://www.aisoma.de/10-useful-ai-ml-slides/](https://www.aisoma.de/10-useful-ai-ml-slides/)

&amp;#x200B;

*Processing img xbbn20gu3un21...*",13,68,False,self,,,,,
1431,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,18,b4hb3c,self.MachineLearning,[D] Summer schools or courses in Italy,https://www.reddit.com/r/MachineLearning/comments/b4hb3c/d_summer_schools_or_courses_in_italy/,Horror_Counter,1553333796,"Hi,

I'm looking for Deep/Reinforcement Learning Summer Schools/courses, preferably in Italy (I already have a fairly long list of summer schools abroad), who accept industry practitioners (i.e., not focused on MSc./Ph.D. students only). Ideally, the school/course should satisfy the following requirements:

 - contain a healthy blend of theory and practice (some theory is more than welcome, but hands-on tutorials would definitely be a plus)
 - advanced or intermediate/advanced (a beginners course could be useful for some of my collaborators, but I'm also looking for a course for me)
 - include NLP (I use DL for CV quite a bit, but I'm a beginner when it comes to NLP)

Thanks!",22,12,False,self,,,,,
1432,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,18,b4hblm,twitter.com,SPECIAL Offer Data Science and Machine Learning Bootcamp with R DISCOUNT 94% off,https://www.reddit.com/r/MachineLearning/comments/b4hblm/special_offer_data_science_and_machine_learning/,TerrieFOliver,1553333925,,0,1,False,default,,,,,
1433,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,18,b4hciz,lilianweng.github.io,[D] Are Deep Neural Networks Dramatically Overfitted?,https://www.reddit.com/r/MachineLearning/comments/b4hciz/d_are_deep_neural_networks_dramatically_overfitted/,alexeyr,1553334124,,0,1,False,default,,,,,
1434,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,19,b4hodn,self.MachineLearning,How to annotate the dataset for YOLO for multiple predictions.,https://www.reddit.com/r/MachineLearning/comments/b4hodn/how_to_annotate_the_dataset_for_yolo_for_multiple/,Pranoykrishnan,1553336772,[removed],0,1,False,self,,,,,
1435,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,19,b4hoze,intellectyx.com,GDPR Compliance- Using Machine Learning to Enhance Privacy,https://www.reddit.com/r/MachineLearning/comments/b4hoze/gdpr_compliance_using_machine_learning_to_enhance/,rohit1221qq,1553336907,,0,1,False,default,,,,,
1436,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,19,b4hr5c,self.MachineLearning,Image Classification result when two objects from different classes are given as input.,https://www.reddit.com/r/MachineLearning/comments/b4hr5c/image_classification_result_when_two_objects_from/,Pranoykrishnan,1553337391,[removed],0,1,False,self,,,,,
1437,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,20,b4i5e0,self.MachineLearning,Looking for collaborations,https://www.reddit.com/r/MachineLearning/comments/b4i5e0/looking_for_collaborations/,clone290595,1553340486,"Hi everyone, I'm looking for collaborators for an e-learning Open Source project about Data Science.  We're already 10-15 people from all over the world and  we're growing. Pm me here on Reddit and I'll give you more details. Don't lose this, is a great opportunity to obtain visibility and (later on) money. 

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
1438,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,21,b4imp9,revyuh.com,The Dilemma of Artificial Intelligence: An Outcry From The Next Generation,https://www.reddit.com/r/MachineLearning/comments/b4imp9/the_dilemma_of_artificial_intelligence_an_outcry/,revyuh,1553344136,,0,1,False,default,,,,,
1439,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,21,b4ishm,self.MachineLearning,Learn about the parameters of an algorithm,https://www.reddit.com/r/MachineLearning/comments/b4ishm/learn_about_the_parameters_of_an_algorithm/,PerfectImperfection7,1553345252,[removed],0,1,False,self,,,,,
1440,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,22,b4j4fz,youtube.com,How to Block Youtube in Mikrotik,https://www.reddit.com/r/MachineLearning/comments/b4j4fz/how_to_block_youtube_in_mikrotik/,Mikrotik_Systems,1553347432,,0,1,False,default,,,,,
1441,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,23,b4juk8,self.MachineLearning,UiStyleGAN - transfer style and font of webpages using UiStyleGAN (a variant of CycleGAN),https://www.reddit.com/r/MachineLearning/comments/b4juk8/uistylegan_transfer_style_and_font_of_webpages/,pranitkothari,1553351931,[removed],1,1,False,self,,,,,
1442,MachineLearning,t5_2r3gv,2019-3-23,2019,3,23,23,b4jxpj,self.MachineLearning,how to define prior and posterior distribution other than normal distribution over probabilistic layer in tensorflow probability?,https://www.reddit.com/r/MachineLearning/comments/b4jxpj/how_to_define_prior_and_posterior_distribution/,shenchena,1553352444,[removed],0,1,False,self,,,,,
1443,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,0,b4klnt,self.MachineLearning,[D] Introduction to Machine Learning Key Terms at a Glance need feedback to improve article,https://www.reddit.com/r/MachineLearning/comments/b4klnt/d_introduction_to_machine_learning_key_terms_at_a/,formatlar,1553356113,"Machine learning methodology We can summarize hot topics of machine learning. Machine Learning Neural Networks Convolutional Neural Network CNN (Convolutional Neural Network) Recurrent Neural Network RNN (Recurrent Neural Network) LSTM RNN Recurrent Neural Network (LSTM) Self-encoding (Auto encoder) Generating Confrontation Network (GAN) The black box of the neural network is not black Neural network gradient descent Migration Learning Transfer Learning  
[https://formatlar.com/2019/03/23/introduction-to-machine-learning-key-terms-at-a-glance/](https://formatlar.com/2019/03/23/introduction-to-machine-learning-key-terms-at-a-glance/)",5,0,False,self,,,,,
1444,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,1,b4l512,self.MachineLearning,What were the first steps when integrating an ML feature the last time? Where did you find the models/ approach and how did pick the right one?,https://www.reddit.com/r/MachineLearning/comments/b4l512/what_were_the_first_steps_when_integrating_an_ml/,igorsusmelj,1553358882,[removed],0,2,False,self,,,,,
1445,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,1,b4lb67,self.MachineLearning,Fake news detection?,https://www.reddit.com/r/MachineLearning/comments/b4lb67/fake_news_detection/,yautslil,1553359773,[removed],0,1,False,self,,,,,
1446,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,2,b4lxe3,self.MachineLearning,I need papers about constructing data sets for non-english NLP,https://www.reddit.com/r/MachineLearning/comments/b4lxe3/i_need_papers_about_constructing_data_sets_for/,Hamada_Elex,1553362999,[removed],0,1,False,self,,,,,
1447,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,2,b4m26m,self.MachineLearning,Should I choose D1 (Decision 1) or S2 (Statistics 2) for my second-year A-levels?,https://www.reddit.com/r/MachineLearning/comments/b4m26m/should_i_choose_d1_decision_1_or_s2_statistics_2/,SenseDeletion,1553363690,[removed],0,1,False,self,,,,,
1448,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,3,b4m5nn,self.MachineLearning,"[D] What is Perceptron, Classic Basic Introduction, Fundamental Definition At a Glance, feedback please",https://www.reddit.com/r/MachineLearning/comments/b4m5nn/d_what_is_perceptron_classic_basic_introduction/,formatlar,1553364183,"**Perceptron at a glance**  


Perceptron (Perceptron , from Latin percepti - perception) - the device MARK-1 , as well as the corresponding mathematical model created by Frank Rosenblatt to build a brain model . By brain model is meant any theoretical system that seeks to explain the physiological functions of the brain using the well-known laws of physics and mathematics , as well as the well-known facts of neuroanatomy and neurophysiology . Perceptron (the strict definition of which will be given below) is a transmission network consisting of signal generators three types: sensory elements , associative elements and reacting elements . The generating functions of these elements depend on signals arising either somewhere inside the transmission network, or, for external elements, on signals coming from the external environment. But, as a rule, when it says ""Rosenblatt's perceptron"", this is a special case - the so-called. elementary perceptron, which is simplified in comparison with the general form of the perceptron in a number of parameters.

[https://formatlar.com/2019/03/23/what-is-perceptron-classic-basic-introduction-fundamental-definition-at-a-glance/](https://formatlar.com/2019/03/23/what-is-perceptron-classic-basic-introduction-fundamental-definition-at-a-glance/)

&amp;#x200B;",8,0,False,self,,,,,
1449,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,4,b4my2k,self.MachineLearning,Learning embeddings from deep neural network architectures,https://www.reddit.com/r/MachineLearning/comments/b4my2k/learning_embeddings_from_deep_neural_network/,unguided_deepness,1553368262,[removed],0,1,False,self,,,,,
1450,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,4,b4n2zh,self.MachineLearning,Thesis Idea: Turning A Binary Classifier into A Change-point Detector &amp; Vice Versa,https://www.reddit.com/r/MachineLearning/comments/b4n2zh/thesis_idea_turning_a_binary_classifier_into_a/,pig_newton1,1553368948,[removed],0,1,False,self,,,,,
1451,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,4,b4n4yf,hub.jhu.edu,"""Humans can decipher adversarial images"": A study of ""machine theory of mind"" shows that ordinary people can predict how machines will misclassify",https://www.reddit.com/r/MachineLearning/comments/b4n4yf/humans_can_decipher_adversarial_images_a_study_of/,_chaz_,1553369237,,33,440,False,https://b.thumbs.redditmedia.com/1Ku69LB4ALw3anCWWg39iim05Xxxfcdw-_2rS2u0J9k.jpg,,,,,
1452,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,4,b4nb0o,self.MachineLearning,[N] Call for papers: ICML Workshop on Negative Dependence in ML,https://www.reddit.com/r/MachineLearning/comments/b4nb0o/n_call_for_papers_icml_workshop_on_negative/,mikegartrell,1553370120,"[https://negative-dependence-in-ml-workshop.lids.mit.edu](https://negative-dependence-in-ml-workshop.lids.mit.edu)

Submission deadline is April 29, 2019.

We invite submissions of papers on any topic related to negative dependence in machine learning, including (but not limited to):

* Submodular optimization
* Determinantal point processes
* Volume sampling
* Recommender systems
* Experimental design
* Variance-reduction methods
* Exploitation/exploration trade-offs (RL, Bayesian Optimization, etc.)
* Batched active learning
* Strongly Rayleigh measures",1,12,False,self,,,,,
1453,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,5,b4nocx,self.MachineLearning,[D] The Math Behind Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b4nocx/d_the_math_behind_machine_learning/,sixilli,1553372078,"I'm not that new to machine learning, but I feel like I don't have the best understanding of the underlying math. I was wondering if there's any good references out there that cover the math of all the models? I've done basic things like creating my own K Nearest Neighbor Classifier, but that's about it. I really want to get to the research level of unsupervised approaches. 

Also would learning all of this help me find a job? I'm currently mid bootcamp and just want to round out my understanding which I figured wouldn't be a bad thing.",26,8,False,self,,,,,
1454,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,5,b4nrp0,rdcu.be,Customer churn prediction in telecom using machine learning and social networks analysis in big data platform,https://www.reddit.com/r/MachineLearning/comments/b4nrp0/customer_churn_prediction_in_telecom_using/,Abdelrahimk,1553372547,,0,1,False,default,,,,,
1455,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,7,b4p27w,self.MachineLearning,I Need Some Questions Answered for a School Project,https://www.reddit.com/r/MachineLearning/comments/b4p27w/i_need_some_questions_answered_for_a_school/,ImMLND,1553380013,[removed],0,1,False,self,,,,,
1456,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,9,b4q3t4,self.MachineLearning,[P] Finding similarity between words/entities,https://www.reddit.com/r/MachineLearning/comments/b4q3t4/p_finding_similarity_between_wordsentities/,AnonMLstudent,1553386412,"Let's say I have a sentence, and a word or entity. I want to find a word or entity within the sentence that is most similar to the one given.


For example, let's say I have:

Word: ""cat""
Sentence: ""I like to take my dog out on walks every weekend""


I want to be able to determine that the word/entity ""dog"" within my sentence is the most similar to the one I have given. Preferably this could work on multiple words as well, such as ""machine learning subreddit"" as compared to ""deep learning forum"".

I am wondering what are the most effective ways to currently do this? I have looked into word embeddings and vectors such as GloVe, but I heard those are quite outdated.",9,1,False,self,,,,,
1457,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,10,b4qnfl,youtu.be,I made A.I. that can complete my own game!,https://www.reddit.com/r/MachineLearning/comments/b4qnfl/i_made_ai_that_can_complete_my_own_game/,CKlidify,1553389917,,0,1,False,default,,,,,
1458,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,12,b4rpvw,self.MachineLearning,[D] Difference between using dilated convolutions and just downsampling your input image,https://www.reddit.com/r/MachineLearning/comments/b4rpvw/d_difference_between_using_dilated_convolutions/,rasen58,1553396635,"Dilated convolutions can be good because you can take into account input pixels that are far away unlike regular convolutions which only consider adjacent pixels.  

And so this is good if your input image is very large, so there probably aren't that many changes within a few pixels.

&amp;#x200B;

But, I'm not sure if using a dilated convolution to capture these far dependencies is any better than just downsampling my image i.e. using a 4-dilated convolution vs turning a 2084x2084 image into a 521x521 one and applying a regular convolution",5,21,False,self,,,,,
1459,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,14,b4ssa2,self.MachineLearning,"AI Startups, What Are You Currently Struggling With?",https://www.reddit.com/r/MachineLearning/comments/b4ssa2/ai_startups_what_are_you_currently_struggling_with/,v3nge,1553404355,[removed],0,1,False,self,,,,,
1460,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,14,b4sxgt,extremetech.com,[R] MIT Develops Algorithm to Accelerate Neural Network Evaluation by 200x,https://www.reddit.com/r/MachineLearning/comments/b4sxgt/r_mit_develops_algorithm_to_accelerate_neural/,georgeo,1553405522,,1,1,False,default,,,,,
1461,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,16,b4tju9,self.MachineLearning,How to initialize proportional weights on Keras pre-trained resnet-50?,https://www.reddit.com/r/MachineLearning/comments/b4tju9/how_to_initialize_proportional_weights_on_keras/,ans5925,1553410885,[removed],0,1,False,self,,,,,
1462,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,17,b4u20e,youtube.com,How Neural Networks Work- Simply Explained by a Machine Learning Engineer,https://www.reddit.com/r/MachineLearning/comments/b4u20e/how_neural_networks_work_simply_explained_by_a/,bilalD,1553415565,,0,1,False,https://b.thumbs.redditmedia.com/lAiMzjTyr2KqqhRGgJUVMzQdwU2_M_RzNgdeOAGU2eY.jpg,,,,,
1463,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,19,b4usbj,bidyutchanda.github.io,Predicting type of art using CNNs and FastAI,https://www.reddit.com/r/MachineLearning/comments/b4usbj/predicting_type_of_art_using_cnns_and_fastai/,bidyutchanda108,1553422150,,0,1,False,default,,,,,
1464,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,19,b4utbq,self.MachineLearning,"[D] Redefinition of Fundamental Perceptron, Introduction to Modern Perceptron, Basic Perspective At a Glance",https://www.reddit.com/r/MachineLearning/comments/b4utbq/d_redefinition_of_fundamental_perceptron/,formatlar,1553422383," After 50 years, I decided to redefine perceptron to meet data age requirements. I will write different articles about this topic. In this introduction article, I will share some basic terms about my perspective. I will go step by step to depict new perceptron concept.

[https://formatlar.com/2019/03/24/redefinition-of-fundamental-perceptron-introduction-to-modern-perceptron-basic-perspective-at-a-glance/](https://formatlar.com/2019/03/24/redefinition-of-fundamental-perceptron-introduction-to-modern-perceptron-basic-perspective-at-a-glance/)",1,0,False,self,,,,,
1465,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,20,b4vgvg,self.MachineLearning,Discussion about final year project ideas,https://www.reddit.com/r/MachineLearning/comments/b4vgvg/discussion_about_final_year_project_ideas/,farazfazii,1553427721,I'm about to graduate and now my final year project is started but i don't know what should be the idea for final year project Basically i want to make App but don't know which creative idea should i select that also help in future.Please give me your ideas regarding final year project Thanks in advance,0,1,False,self,,,,,
1466,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,20,b4vhsp,self.MachineLearning,Observation Weighted KMeans,https://www.reddit.com/r/MachineLearning/comments/b4vhsp/observation_weighted_kmeans/,PiAreSqured,1553427929,[removed],0,1,False,self,,,,,
1467,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,20,b4vl3i,self.MachineLearning,1D convolutional networks with raw temporal signal input,https://www.reddit.com/r/MachineLearning/comments/b4vl3i/1d_convolutional_networks_with_raw_temporal/,tWill35,1553428676,[removed],0,1,False,self,,,,,
1468,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,22,b4wcup,bestofml.com,The best resources in Machine Learning &amp; AI,https://www.reddit.com/r/MachineLearning/comments/b4wcup/the_best_resources_in_machine_learning_ai/,808hunna,1553434252,,0,1,False,default,,,,,
1469,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,22,b4wfwh,self.MachineLearning,How do you write a follow-up paper without extensively paragraphing yourself?,https://www.reddit.com/r/MachineLearning/comments/b4wfwh/how_do_you_write_a_followup_paper_without/,pinouche13,1553434834,[removed],0,1,False,self,,,,,
1470,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,23,b4wsg9,github.com,[P] Minimal PyTorch implementation of Deep Dream,https://www.reddit.com/r/MachineLearning/comments/b4wsg9/p_minimal_pytorch_implementation_of_deep_dream/,Eriklindernoren,1553436997,,0,1,False,https://b.thumbs.redditmedia.com/aeBtgeXey449JtH2wESBf-X3FkGORhWyu_8SSQ4i_Lc.jpg,,,,,
1471,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,23,b4wwbv,self.MachineLearning,GTX 1070 vs RTX 2070 laptop,https://www.reddit.com/r/MachineLearning/comments/b4wwbv/gtx_1070_vs_rtx_2070_laptop/,zalias99,1553437640,[removed],0,1,False,self,,,,,
1472,MachineLearning,t5_2r3gv,2019-3-24,2019,3,24,23,b4x4s5,self.MachineLearning,GTX 1070 vs RTX 2060 vs RTX 2070 laptops,https://www.reddit.com/r/MachineLearning/comments/b4x4s5/gtx_1070_vs_rtx_2060_vs_rtx_2070_laptops/,zalias99,1553438987,[removed],0,1,False,self,,,,,
1473,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,0,b4xjsk,self.MachineLearning,What are the best methods to map a sequence of a certain length to a sequence of different length?,https://www.reddit.com/r/MachineLearning/comments/b4xjsk/what_are_the_best_methods_to_map_a_sequence_of_a/,acobobby,1553441309,[removed],0,1,False,self,,,,,
1474,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,0,b4xqpa,self.MachineLearning,Which Learning Method should be applied?,https://www.reddit.com/r/MachineLearning/comments/b4xqpa/which_learning_method_should_be_applied/,Space-exe,1553442341,[removed],0,1,False,self,,,,,
1475,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,1,b4y6up,self.MachineLearning,Which techniques to learn and apply for this video (image) processing ?,https://www.reddit.com/r/MachineLearning/comments/b4y6up/which_techniques_to_learn_and_apply_for_this/,xenopizza,1553444691,"For a side project im taking a look at how to extract information from Apex Legends (for starters) game play videos. Example screenshot:
https://www.gameprime.org/wp-content/uploads/2019/02/gp-apexlegends-6.jpg

There is a log on top right corner and player status on bottom left corner (health/shields).

Id like to:

- extract &lt;player A username&gt; killed &lt;player B username&gt; with &lt;weapon name&gt; (at time mm:ss) from log, and other information displayed on that log

- from player health extract health/shields percentage (shape analysis ?)

Also detect game start/end screens and possibly extract info from them like which characters compose the current player squads.

Apart from OCR (which is not enough), what kind or specific techniques should i learn to implement this ?


",0,1,False,self,,,,,
1476,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,1,b4y76t,self.MachineLearning,Exist Machine Learning or Artifitial Intelligence for Autocad?,https://www.reddit.com/r/MachineLearning/comments/b4y76t/exist_machine_learning_or_artifitial_intelligence/,juanjotm2,1553444735," Exist some way to use Machine Learning or Artificial intelligence to draw automatically in AutoCAD?  


I found some videos where explain that Machine Learning and Artificial intelligence can be used for everything and I can teach to the Machine or A.I. to draw like my old projects.  


...If this it's possible then someone already did?  
... please share me some projects related to this. ",0,1,False,self,,,,,
1477,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,2,b4yl19,medium.com,[P] NSFW object detection with YOLOv3 in Keras  Part 1 of 3,https://www.reddit.com/r/MachineLearning/comments/b4yl19/p_nsfw_object_detection_with_yolov3_in_keras_part/,AllergicToDinosaurs,1553446819,,1,1,True,nsfw,,,,,
1478,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,2,b4ypom,yuqizheng.com,Brief guide to going from small scale programming towards big data,https://www.reddit.com/r/MachineLearning/comments/b4ypom/brief_guide_to_going_from_small_scale_programming/,yqz999,1553447448,,0,1,False,https://b.thumbs.redditmedia.com/JRrTU9mMpRu1A7BRHqWQzPgdRy7eeHptOb3a7vlXuvA.jpg,,,,,
1479,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,2,b4z2f9,self.MachineLearning,"480,000 Rotten Tomatoes Labeled Critic Reviews for Machine Learning/NLP",https://www.reddit.com/r/MachineLearning/comments/b4z2f9/480000_rotten_tomatoes_labeled_critic_reviews_for/,nicolas-gervais,1553449249,"240,000 fresh reviews and 240,000 rotten reviews, labeled, with their text review. Get the CSV on my [Google Drive](https://drive.google.com/file/d/1w1TsJB-gmIkZ28d1j7sf1sqcPmHXw352/view?usp=sharing)",0,1,False,self,,,,,
1480,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,2,b4z9h3,self.MachineLearning,[D] An Overview of Methods in Semantic Segmentation,https://www.reddit.com/r/MachineLearning/comments/b4z9h3/d_an_overview_of_methods_in_semantic_segmentation/,thatbrguy_,1553450227,"This article aims to provide a review of Classical and Deep Learning methods for Semantic Segmentation, with the intention of simplifying some of the concepts: 

[https://medium.com/beyondminds/a-simple-guide-to-semantic-segmentation-effcf83e7e54?source=friends\_link&amp;sk=3d1a5a32a19d611fbd81028cfd4f23fd](https://medium.com/beyondminds/a-simple-guide-to-semantic-segmentation-effcf83e7e54?source=friends_link&amp;sk=3d1a5a32a19d611fbd81028cfd4f23fd)

&amp;#x200B;

I would love to hear your thoughts and feedback regarding the same, thanks!",25,176,False,self,,,,,
1481,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,3,b4zoie,self.MachineLearning,[D] What are some of the top papers to read from ICLR 2019?,https://www.reddit.com/r/MachineLearning/comments/b4zoie/d_what_are_some_of_the_top_papers_to_read_from/,mad_runner,1553452319,What are some of the top papers that have been accepted at the conference? Is the open review score a good measure for selecting papers to read?,8,49,False,self,,,,,
1482,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,3,b4zt6y,self.MachineLearning,Spark NLP 2.0 is now available for free,https://www.reddit.com/r/MachineLearning/comments/b4zt6y/spark_nlp_20_is_now_available_for_free/,johnsnowlabsUS,1553452970,"Our open source Spark NLP 2.0 library integrates BERT embeddings &amp; contrib LSTM cells for (trainable &amp; scalable) named entity recognition. Have you tried it yet? Go to [https://www.johnsnowlabs.com/spark-nlp/](https://www.johnsnowlabs.com/spark-nlp/) 

&amp;#x200B;

![img](iwrb2kxgz3o21 ""#opensource #ai #dl #ml #nlproc #datascience #deeplearning #analytics#textmining #TensorFlow
@ApacheSpark #bigdata"")",0,0,False,self,,,,,
1483,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,3,b4ztjp,flancia.org,GPT-2 and Philip K. Dick,https://www.reddit.com/r/MachineLearning/comments/b4ztjp/gpt2_and_philip_k_dick/,flancian,1553453019,,0,1,False,default,,,,,
1484,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,5,b50r5y,self.MachineLearning,[D] Machine Learning - WAYR (What Are You Reading) - Week 59,https://www.reddit.com/r/MachineLearning/comments/b50r5y/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,1553457604,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|
|----|-----|-----|-----|-----|-----|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)||

Most upvoted papers two weeks ago:

/u/wassname: [CipherGan](https://openreview.net/forum?id=BkeqO7x0-)

/u/data_everyware: [http://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf](http://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf)

Besides that, there are no rules, have fun.",35,24,False,self,,,,,
1485,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,5,b50siu,towardsdatascience.com,Image Segmentation using Pythons scikit-image module,https://www.reddit.com/r/MachineLearning/comments/b50siu/image_segmentation_using_pythons_scikitimage/,RubiksCodeNMZ,1553457778,,0,1,False,default,,,,,
1486,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,5,b50tkn,self.MachineLearning,[N] Chat with us on Slack!,https://www.reddit.com/r/MachineLearning/comments/b50tkn/n_chat_with_us_on_slack/,MTGTraner,1553457910,"Hi everyone, 

we now have a Slack channel: [invite link](https://join.slack.com/t/rml-talk/shared_invite/enQtNTgzODIwMTE0MDgwLWEwNjRkN2I0YWIwNTZjMWU2NmFmMGNhYmVkYjhmNTllZGFhMTI1N2IyNzNhZmI1YmNkYTFiYjliODRiZmNiYWI). Keep it civil!",0,36,False,self,,,,,
1487,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,5,b517r2,self.MachineLearning,Is it worth paying out of your pocket to present in a ML conference?,https://www.reddit.com/r/MachineLearning/comments/b517r2/is_it_worth_paying_out_of_your_pocket_to_present/,___nevermind___,1553459852,[removed],0,1,False,self,,,,,
1488,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,6,b524ul,poutyne.org,[P] PyToune has changed its name for Poutyne: A Keras-Like Deep Learning Framework for PyTorch,https://www.reddit.com/r/MachineLearning/comments/b524ul/p_pytoune_has_changed_its_name_for_poutyne_a/,freud_14,1553464564,,0,2,False,https://b.thumbs.redditmedia.com/aFtwWYGMyQnreKkTTBDbrwA2EIn8lh2df2tzJW8qcqA.jpg,,,,,
1489,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,7,b52an2,self.MachineLearning,CUDA cores vs. VRAM vs. Clock speed  what's more important for real-time image synthesis using GANs?,https://www.reddit.com/r/MachineLearning/comments/b52an2/cuda_cores_vs_vram_vs_clock_speed_whats_more/,flankstank,1553465437,[removed],0,1,False,self,,,,,
1490,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,7,b52f5z,self.MachineLearning,[D] CUDA cores vs. VRAM vs. Clock speed  what's more important for real-time image synthesis using GANs?,https://www.reddit.com/r/MachineLearning/comments/b52f5z/d_cuda_cores_vs_vram_vs_clock_speed_whats_more/,flankstank,1553466133,"I'm looking to create an art installation in openFrameworks that runs pix2pix in real-time.

Disregarding price, what is more important when selecting a GPU for real-time, high frame rate neural network art installations: clock speed, CUDA cores, VRAM? For instance, will a 24GB Nvidia Tesla K80 with more CUDA cores, but a lower clock speed outperform a 11GB GTX 1080ti with fewer CUDA cores and a higher clock speed for live, real-time image synthesizing?",6,9,False,self,,,,,
1491,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,9,b53t85,self.MachineLearning,More cores vs less cores at higher performance,https://www.reddit.com/r/MachineLearning/comments/b53t85/more_cores_vs_less_cores_at_higher_performance/,dontuseyourreal_name,1553473920,[removed],0,1,False,self,,,,,
1492,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,9,b53z35,self.MachineLearning,What is an easy way to get started with Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/b53z35/what_is_an_easy_way_to_get_started_with_machine/,FahadUddin92,1553474889,Hi. I am a web developer trying to learn machine learning. I have read about a few algorithms and want to get started by solving an 'easy' problem. Any ideas?,0,1,False,self,,,,,
1493,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,11,b54rn4,self.MachineLearning,Google AI's VoiceFilter system in PyTorch,https://www.reddit.com/r/MachineLearning/comments/b54rn4/google_ais_voicefilter_system_in_pytorch/,seungwonpark,1553479484,[removed],0,1,False,self,,,,,
1494,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,11,b5560y,self.MachineLearning,[D] Classification/Selection of Time-Series Data,https://www.reddit.com/r/MachineLearning/comments/b5560y/d_classificationselection_of_timeseries_data/,seacucumber3000,1553481878,"As part of research I'm doing at my university, I'm starting work on production software soon whose main task is to select/classify time-series data (specifically data from NASA spacecraft). I've been working through Andrew Ng's Coursera course on ML since I have no previous experience with ML, but, AFAIK, none of the techniques presented (so far, I'm not done with the course) are applicable to time series data (rather they work with singular observations). Does anyone have any pointers for techniques to use for selecting/classifying time-series data?

I've found a bunch of papers (linked below) that I'm slowly working through (although it's taking me quite some time since I'm new to ML). Since they all posit different techniques (there may be repetition, but the question still stands), how do you go about choosing the best one?

https://www.cs.unm.edu/~mueen/DTW.pdf

https://arxiv.org/pdf/1809.04356.pdf

https://arxiv.org/abs/1611.06455v4

http://web.engr.oregonstate.edu/~tgd/publications/mlsd-ssspr.pdf

http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.60.4893&amp;rep=rep1&amp;type=pdf

https://arxiv.org/abs/1602.01711",19,6,False,self,,,,,
1495,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,13,b55zgz,self.MachineLearning,[P] Google AI's VoiceFilter system in PyTorch,https://www.reddit.com/r/MachineLearning/comments/b55zgz/p_google_ais_voicefilter_system_in_pytorch/,seungwonpark,1553487028,"Recently I've implemented Google AI's VoiceFilter using PyTorch. Much feedback will be really appreciated. Thanks!

&amp;#x200B;

GitHub: [https://github.com/mindslab-ai/voicefilter](https://github.com/mindslab-ai/voicefilter)

Listen to sample at: [http://swpark.me/voicefilter/](http://swpark.me/voicefilter/)

Original Paper: [https://arxiv.org/abs/1810.04826](https://arxiv.org/abs/1810.04826)

Original Google AI's blog post: [https://google.github.io/speaker-id/publications/VoiceFilter/](https://google.github.io/speaker-id/publications/VoiceFilter/)",10,84,False,self,,,,,
1496,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,13,b560w1,youtube.com,AI EXPLAiNED to Complete Beginners | XUCLASS,https://www.reddit.com/r/MachineLearning/comments/b560w1/ai_explained_to_complete_beginners_xuclass/,jeffxu999,1553487280,,0,1,False,https://a.thumbs.redditmedia.com/P5xCDWhwBPPO89GGEXS2Xz8DhLu6HMK61HNMS9wD0-4.jpg,,,,,
1497,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,13,b56aud,allenkunle.me,Deriving Machine Learning Cost Functions using Maximum Likelihood Estimation (MLE),https://www.reddit.com/r/MachineLearning/comments/b56aud/deriving_machine_learning_cost_functions_using/,allenakinkunle,1553489099,,0,1,False,default,,,,,
1498,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,13,b56bil,self.MachineLearning,"What is the best article / blog about artificial intelligence? For example, this blog https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html",https://www.reddit.com/r/MachineLearning/comments/b56bil/what_is_the_best_article_blog_about_artificial/,Doctor_who1,1553489224,[removed],0,1,False,self,,,,,
1499,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,14,b56lvi,self.MachineLearning,[P] Does anyone know where I can get customized Twitter data with sentiment analyzed?,https://www.reddit.com/r/MachineLearning/comments/b56lvi/p_does_anyone_know_where_i_can_get_customized/,Seankala,1553491270,"Hello. I'm currently working on writing a paper to submit to a conference (undergrad-level) and had some question regarding getting data.

The topic of my paper is to use sentiment analysis on Twitter data to judge the behaviors between the Korean cryptocurrency market and the US cryptocurrency market. To put the motivation briefly, anyone who's been following crypto during 2017-2018 will know that there was something called the ""kimchi premium"" when Korean prices were higher than prices elsewhere by as much as 50%. I wanted to see if analyzing the mood of Twitter would be able to provide insight into how that discrepancy occurs, and if so then how much we'd be able to forecast differences not just during that period but even today.

In order to do this I require data for Twitter. I was able to obtain BTC data for Korean and US exchanges, but I'm having trouble finding data for Twitter.

Scraping Twitter to collect the Tweets of select individual accounts isn't difficult, but the difficult part is finding data that has the sentiment also labeled. Does anyone know where I would be able to find such data? Or would making it myself be the only plausible way for now?

Thank you!",4,1,False,self,,,,,
1500,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,14,b56rsl,self.MachineLearning,Epochs for object detection using Tensorflow Object Detection API,https://www.reddit.com/r/MachineLearning/comments/b56rsl/epochs_for_object_detection_using_tensorflow/,Sehrishnaz47,1553492506,Can any body explain what is epochs and how to set the value for epochs.,0,1,False,self,,,,,
1501,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,14,b56w7m,self.MachineLearning,"Software Engineer, Machine Learning role at Google",https://www.reddit.com/r/MachineLearning/comments/b56w7m/software_engineer_machine_learning_role_at_google/,throwawaytoday3210,1553493433,[removed],0,1,False,self,,,,,
1502,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,15,b574fl,self.MachineLearning,machine learning online course | best machine learning course,https://www.reddit.com/r/MachineLearning/comments/b574fl/machine_learning_online_course_best_machine/,akhilapriya404,1553495226,[removed],0,1,False,self,,,,,
1503,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,16,b57mtz,self.MachineLearning,Spark NLP is 1-2 orders of magnitude faster than spaCy for training domain-specific NLP models,https://www.reddit.com/r/MachineLearning/comments/b57mtz/spark_nlp_is_12_orders_of_magnitude_faster_than/,johnsnowlabsUS,1553499458,[removed],0,1,False,https://b.thumbs.redditmedia.com/REFHL9KueCVoqlUL9jRjXUOCkfGyJOilwh2lXKTKiOI.jpg,,,,,
1504,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,17,b58307,analyticsindiamag.com,Welcome To The Machine Learning Biases That Still Exist In 2019,https://www.reddit.com/r/MachineLearning/comments/b58307/welcome_to_the_machine_learning_biases_that_still/,stevenreye,1553503462,,0,1,False,default,,,,,
1505,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,18,b587by,self.MachineLearning,Help - Understanding Quantile RL,https://www.reddit.com/r/MachineLearning/comments/b587by/help_understanding_quantile_rl/,Gschmagee,1553504525,"How can I graphically represent the quantile regression penalization.

&amp;#x200B;

In their paper ( [https://arxiv.org/pdf/1710.10044.pdf](https://arxiv.org/pdf/1710.10044.pdf) ) the author says that quantile regression penalizes overestimation errors with    and underestimation errors with 1-  .  Reading the summary ( [https://mtomassoli.github.io/2017/12/08/distributional\_rl/#mjx-eqn-eqq\_reg\_formula](https://mtomassoli.github.io/2017/12/08/distributional_rl/#mjx-eqn-eqq_reg_formula)) the author introduces a guess . Moving   left or right, till  =  of a certain quantile. 

&amp;#x200B;

Taking  a look at the distribution Z and 5 quantiles.

[y - likelihood; x - reward](https://i.redd.it/i6pz6tzxb8o21.png)

"" If we want the probability mass on the left of  to be , we need to use weight 1 for the samples on the left and  for the ones on the right."" 

Is my graphical interpretation right ?

https://i.redd.it/mxralbl6c8o21.png

Thanks",0,1,False,self,,,,,
1506,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,18,b58dvp,self.MachineLearning,I've gamified sentiment analysis with a morbid theme,https://www.reddit.com/r/MachineLearning/comments/b58dvp/ive_gamified_sentiment_analysis_with_a_morbid/,omeysalvi,1553505951,[removed],0,1,False,self,,,,,
1507,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,18,b58h8m,self.MachineLearning,How do you get access to ImageNet as an independent researcher?,https://www.reddit.com/r/MachineLearning/comments/b58h8m/how_do_you_get_access_to_imagenet_as_an/,Lugi,1553506634,[removed],0,1,False,self,,,,,
1508,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,18,b58i33,lhd.co.com,Stacker Crane,https://www.reddit.com/r/MachineLearning/comments/b58i33/stacker_crane/,lhd121,1553506801,,0,1,False,default,,,,,
1509,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,18,b58im7,aliz.ai,Interesting article about ways to apply ML in digital marketing. Must read for marketers.,https://www.reddit.com/r/MachineLearning/comments/b58im7/interesting_article_about_ways_to_apply_ml_in/,milike45,1553506910,,0,1,False,default,,,,,
1510,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,19,b58q8l,self.MachineLearning,[D] Is it worth paying out of your pocket to present in a ML conference?,https://www.reddit.com/r/MachineLearning/comments/b58q8l/d_is_it_worth_paying_out_of_your_pocket_to/,___nevermind___,1553508355,"Hi people, I'm a recent grad, and a paper I first authored got accepted in a top ML conference in the US. Due to some bureaucratic reasons I won't go into, I probably won't be able to get funding to cover the costs of going there. However the other authors will get funding and go there so the paper will be presented even without me. The total cost of going there (tickets, registration, accommodation) will be like a 1-2 months salary for me. I actually can afford from my savings but I'm wondering if the RoI is worth it. I really like the idea of presenting my work, seeing other new work, travelling to a new place etc. I'm also very interested in the networking aspect of the event though. Since I want to work in the field I thought it might help to meet companies/startups that I might be interested working in the future, and might help me get the foot on the door.

In your experience what opportunities are there and do they make an actual difference from e.g applying online? Would you pay out of your pocket to go there if you were in my shoes?",54,110,False,self,,,,,
1511,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,19,b58yqa,self.MachineLearning,"[N] Fourteenth Madrid UPM Advanced Statistics and Data Mining Summer School (June 24th - July 5th, 2019)",https://www.reddit.com/r/MachineLearning/comments/b58yqa/n_fourteenth_madrid_upm_advanced_statistics_and/,bmihaljevic,1553509935,"The Technical University of Madrid (UPM) will once more organize the  'Madrid UPM Advanced Statistics and Data Mining' summer school. The  summer school will be held in Boadilla del Monte, near Madrid, from June  24th to July 5th. This year's edition comprises 12 week-long courses  (15 lecture hours each), given during two weeks (six courses each week).  Attendees may register in each course independently. No restrictions,  besides those imposed by timetables, apply on the number or choice of  courses.

&amp;#x200B;

Early registration is now \*OPEN\*. Extended information on  course programmes, price, venue, accommodation and transport is  available at the school's website:  [http://www.dia.fi.upm.es/ASDM](http://www.dia.fi.upm.es/ASDM)

&amp;#x200B;

There is a 25% discount for members of Spanish AEPIA and SEIO societies. 

&amp;#x200B;

\*\*\* List of courses and brief description \*\*\*

&amp;#x200B;

\* Week 1 (June 24th - June 28th, 2019) \*

&amp;#x200B;

1st session: 9:45-12:45

Course 1: Bayesian Networks (15 h)    Basics of Bayesian networks. Inference in Bayesian networks.  Learning Bayesian networks from data. Real applications. Practical  demonstration: GeNIe, Weka, Bayesia, R.

Course 2: Time Series(15 h)   Basic concepts in time series. Linear models for time series. Time series clustering. Practical demonstration: R.

&amp;#x200B;

2nd session: 13:45-16:45

Course 3: Supervised Pattern Recognition (15 h)    Introduction. Assessing the performance of supervised  classification algorithms. Preprocessing. Classification techniques.  Combining multiple classifiers. Comparing supervised classification  algorithms. Practical demonstration: Weka.

Course 4: Statistical Inference (15 h)    Introduction. Some basic statistical test. Multiple testing.  Introduction to bootstrap methods. Introduction to Robust Statistics.  Practical demonstration: R. 

&amp;#x200B;

3rd session: 17:00 - 20:00

Course 5: Neural Networks and Deep Learning (15 h)    Introduction. Learning algorithms. Learning and Optimization. Deep  Networks. Practical session: Jupyter notebooks in Python Anaconda with  keras and tensorflow.

Course 6: Big Data with Apache Spark (15 h)   Introduction. Spark framework and APIs. Data processing with Spark. Spark streaming. Machine learning with Spark MLlib.

&amp;#x200B;

\* Week 2 (July 1st - July 5th, 2019) \*

&amp;#x200B;

1st session: 9:45-12:45 

Course 7: Bayesian Inference (15 h)    Introduction: Bayesian basics. Conjugate models. MCMC and other  simulation methods. Regression and Hierarchical models. Model selection.  Practical demonstration: R and WinBugs.

Course 8: Unsupervised Pattern Recognition (15 h)    Introduction to clustering. Data exploration and preparation.  Prototype-based clustering. Density-based clustering. Graph-based  clustering. Cluster evaluation. Miscellanea. Conclusions and final  advise. Practical session: R.

&amp;#x200B;

2nd session: 13:45-16:45 

Course 9: Text Mining (15 h)    Information Retrieval 101. Unsupervised Text Processing.  Representation Learning. Information Extraction. Natural Language  Understanding. Practical session: Python, with Jupyter notebooks.

Course 10: Feature Subset Selection (15 h)    Introduction. Filter approaches. Embedded methods. Wrapper methods.  Additional topics. Practical session: R and Weka.   

&amp;#x200B;

3rd session: 17:00-20:00 

Course 11: Support Vector Machines and Regularized Learning (15 h)    Introduction. SVM models. SVM learning algorithms. Regularized  learning. Convex optimization with proximal methods. Practical session:  Python Anaconda with scikit-learn.

Course 12: Hidden Markov Models (15 h)    Introduction. Discrete Hidden Markov Models. Basic algorithms for  Hidden Markov Models. Semicontinuous Hidden Markov Models. Continuous  Hidden Markov Models. Unit selection and clustering. Speaker and  Environment Adaptation for HMMs. Other applications of HMMs. Practical  session: HTK.",2,5,False,self,,,,,
1512,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,19,b590ak,aliz.ai,Resourceful summary of using GCP for Machine Learning.,https://www.reddit.com/r/MachineLearning/comments/b590ak/resourceful_summary_of_using_gcp_for_machine/,milike45,1553510236,,0,1,False,default,,,,,
1513,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,19,b5963j,eleks.com,How to Improve Your Business Efficiency with Data Science and Analytics,https://www.reddit.com/r/MachineLearning/comments/b5963j/how_to_improve_your_business_efficiency_with_data/,Victor_Stakh,1553511293,,0,1,False,https://a.thumbs.redditmedia.com/KD3uplmoAjmLlgtzVE5gyo5ke2PbdlyoWGTgkqIukz8.jpg,,,,,
1514,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,20,b59m5u,self.MachineLearning,Multi-modal datasets,https://www.reddit.com/r/MachineLearning/comments/b59m5u/multimodal_datasets/,AmusingAutomatons,1553514101,[removed],0,1,False,self,,,,,
1515,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,20,b59q1a,self.MachineLearning,Global Artificial Intelligence Community,https://www.reddit.com/r/MachineLearning/comments/b59q1a/global_artificial_intelligence_community/,hazarapet,1553514777,"Hello AI Community. As I see this is a great place to share resources with others, here is a Global Artificial Intelligence Community platform, where you will find a lot of questions, discussions, solutions only about AI. Also there are some upcoming features, which may be useful for us, data scientists.

[https://ai-pool.com/d](https://ai-pool.com/d)

&amp;#x200B;

![img](qr8m88ce79o21)",0,1,False,self,,,,,
1516,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,20,b59qrl,medium.com,[R] DialogueRNN: Emotion Classification in Conversation,https://www.reddit.com/r/MachineLearning/comments/b59qrl/r_dialoguernn_emotion_classification_in/,omarsar,1553514906,,0,1,False,default,,,,,
1517,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,20,b59sd2,self.MachineLearning,"[R] A PyTorch implementation of ""Capsule Graph Neural Network"" (ICLR 2019).",https://www.reddit.com/r/MachineLearning/comments/b59sd2/r_a_pytorch_implementation_of_capsule_graph/,benitorosenberg,1553515178,"&amp;#x200B;

https://i.redd.it/1xtihw0l79o21.jpg

PyTorch: [https://github.com/benedekrozemberczki/CapsGNN](https://github.com/benedekrozemberczki/CapsGNN)

Paper: [https://openreview.net/forum?id=Byl8BnRcYm](https://openreview.net/forum?id=Byl8BnRcYm)

Abstract:

The high-quality node embeddings learned from the Graph Neural Networks  (GNNs) have been applied to a wide range of node-based applications and  some of them have achieved state-of-the-art (SOTA) performance. However,  when applying node embeddings learned from GNNs to generate graph  embeddings, the scalar node representation may not suffice to preserve  the node/graph properties efficiently, resulting in sub-optimal graph  embeddings. Inspired by the Capsule Neural Network (CapsNet), we propose  the Capsule Graph Neural Network (CapsGNN), which adopts the concept of  capsules to address the weakness in existing GNN-based graph embeddings  algorithms. By extracting node features in the form of capsules,  routing mechanism can be utilized to capture important information at  the graph level. As a result, our model generates multiple embeddings  for each graph to capture graph properties from different aspects. The  attention module incorporated in CapsGNN is used to tackle graphs with  various sizes which also enables the model to focus on critical parts of  the graphs. Our extensive evaluations with 10 graph-structured datasets  demonstrate that CapsGNN has a powerful mechanism that operates to  capture macroscopic properties of the whole graph by data-driven. It  outperforms other SOTA techniques on several graph classification tasks,  by virtue of the new instrument.",2,41,False,https://b.thumbs.redditmedia.com/P0nBy_C-_CG-ofOT_cZs0sQ0XCTMNvtU55s34n2Tw6A.jpg,,,,,
1518,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,21,b59vuh,self.MachineLearning,How can I Unfold a CNN to use BPTT?,https://www.reddit.com/r/MachineLearning/comments/b59vuh/how_can_i_unfold_a_cnn_to_use_bptt/,NeumannCracker,1553515748," Im currently trying to write implementation to [Photorealistic Video Super Resolution](https://arxiv.org/pdf/1807.07930.pdf). However, Theres an issue that Im facing where the writer specifies that during the training process he does the following:

1. He unfolds the generator 10 times with the same parameters for a sequence of 10 images.
2. He input the sequence of images and then run back propagation through time.

I have tried reading online about Back propagation through time but I couldn't find any articles regarding BPTT using a CNN. I'd appreciate if someone could provide any links to explain how can I do BPTT on a CNN.

Also can someone provide any example to how can I implement the unfolding and the BPTT in either pPytorch or TensorFlow?",0,1,False,self,,,,,
1519,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,21,b59w72,self.MachineLearning,A question (search for opinion) from the geniuses of /r/machinelarning,https://www.reddit.com/r/MachineLearning/comments/b59w72/a_question_search_for_opinion_from_the_geniuses/,Perpetual_Doubt,1553515808,[removed],0,1,False,self,,,,,
1520,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,21,b59x3l,self.gamedev,[D] Gamifying Machine Learning / AI algorithm Correction - Would Gamification of a user experience to provide corrections to a (non-game) AI system provide enough incentive for users to help improve AI?,https://www.reddit.com/r/MachineLearning/comments/b59x3l/d_gamifying_machine_learning_ai_algorithm/,gymcrash,1553515963,,0,1,False,default,,,,,
1521,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,21,b5a7y2,matrixcalculus.org,Matrix Calculus Online Tool,https://www.reddit.com/r/MachineLearning/comments/b5a7y2/matrix_calculus_online_tool/,r-scholz,1553517797,,1,1,False,default,,,,,
1522,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,21,b5a93c,self.MachineLearning,Modifying Faster R-CNN,https://www.reddit.com/r/MachineLearning/comments/b5a93c/modifying_faster_rcnn/,kubasienki,1553517987,[removed],0,1,False,self,,,,,
1523,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,21,b5aab8,theappsolutions.com,Guide to Unsupervised Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b5aab8/guide_to_unsupervised_machine_learning/,lady_monsoon,1553518184,,0,1,False,https://b.thumbs.redditmedia.com/MkoJ0y7-4UHIqKR7vdnuN8PL0TCXKvAqO4yxCMR-waY.jpg,,,,,
1524,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,21,b5aaim,matrixcalculus.org,[D] Matrix Calculus Online Tool,https://www.reddit.com/r/MachineLearning/comments/b5aaim/d_matrix_calculus_online_tool/,r-scholz,1553518222,,1,1,False,default,,,,,
1525,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,22,b5airz,gwern.net,[P] Making Anime Faces With StyleGAN: A Complete Guide,https://www.reddit.com/r/MachineLearning/comments/b5airz/p_making_anime_faces_with_stylegan_a_complete/,gwern,1553519503,,0,1,False,https://b.thumbs.redditmedia.com/GmCOUg1n_9IOK7DNuJxkjlEnVCpQpEfLFoYQ6M0-JLU.jpg,,,,,
1526,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,22,b5aomr,self.MachineLearning,Verify bills using machine learning,https://www.reddit.com/r/MachineLearning/comments/b5aomr/verify_bills_using_machine_learning/,captainamerica001,1553520413,[removed],0,1,False,self,,,,,
1527,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,22,b5aym1,self.MachineLearning,where should i question with Machine Learining,https://www.reddit.com/r/MachineLearning/comments/b5aym1/where_should_i_question_with_machine_learining/,GoBacksIn,1553521985,[removed],0,1,False,self,,,,,
1528,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,23,b5b4an,self.MachineLearning,[D] How do you find hyper-parameters for Transformer,https://www.reddit.com/r/MachineLearning/comments/b5b4an/d_how_do_you_find_hyperparameters_for_transformer/,tsauri,1553522826,"Assuming you have Google-like compute resources and a Transformer model,  
how do you actually search for hyper parameters?

  
I burn too much money from random search :(  
",5,2,False,self,,,,,
1529,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,23,b5b4dd,self.MachineLearning,Some insights intellectual ,https://www.reddit.com/r/MachineLearning/comments/b5b4dd/some_insights_intellectual/,rahul-bhatt,1553522837,[removed],1,1,False,self,,,,,
1530,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,23,b5bbkn,sararobinson.dev,Preventing bias in ML models,https://www.reddit.com/r/MachineLearning/comments/b5bbkn/preventing_bias_in_ml_models/,sararob,1553523934,,0,1,False,https://b.thumbs.redditmedia.com/QHb9GqjMniHwu5FllzXaDmzZ-LiPTB_9NS2UbAt4NIc.jpg,,,,,
1531,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,23,b5bdrr,self.MachineLearning,[D] Multi-modal datasets - good examples that are openly available?,https://www.reddit.com/r/MachineLearning/comments/b5bdrr/d_multimodal_datasets_good_examples_that_are/,AmusingAutomatons,1553524263,"Hi Everyone,

I was wondering if anyone is aware of publicly available datasets that would qualify as multi-modal, i.e. there's no unique mapping for every output y, given sample input x. 

I'm new to ML and I've encountered examples where many different inputs can all point to a single output (e.g. dog/cat classification) but I'm curious to explore situations where there's ambiguity in that the same input could map to multiple valid outputs.

I'm very curious to see what happens post training, and which of the multiple possible multi-modal outputs will be mapped to the single input that the NN was trained on, and whether some structural changes to the NN (maybe some entropy as is the case with GANs?) can guide the process.

Thanks.",11,3,False,self,,,,,
1532,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,23,b5bfmd,self.MachineLearning,[D] The cortex is a neural network of neural networks,https://www.reddit.com/r/MachineLearning/comments/b5bfmd/d_the_cortex_is_a_neural_network_of_neural/,inarrears,1553524525,"An excerpt an interesting [article](https://medium.com/the-spike/your-cortex-contains-17-billion-computers-9034e42d34f2) written by neuroscientist [Mark Humphries](https://www.humphries-lab.org), meant to be a literature overview of approaches that use artificial neural networks to model individual (biological) neuron behavior:

*For some this analogy is merely a useful rhetorical device; for others it is a serious idea. But the brain isnt a computer. Each neuron is a computer. Your cortex contains 17 billion computers.*

*It means the brain can do many computations beyond treating each neuron as a machine for summing up inputs and spitting out a spike. Yet thats the basis for all the units that make up an artificial neural network. It suggests that deep learning and its AI brethren have but glimpsed the computational power of an actual brain.*

*Your cortex contains 17 billion neurons. To understand what they do, we often make analogies with computers. Some use these analogies as cornerstones of their arguments. Some consider them to be deeply misguided. Our analogies often look to artificial neural networks: for neural networks compute, and they are made of up neuron-like things; and so, therefore, should brains compute. But if we think the brain is a computer, because it is like a neural network, then now we must admit that individual neurons are computers too. All 17 billion of them in your cortex; perhaps all 86 billion in your brain.*

*And so it means your cortex is not a neural network. Your cortex is a neural network of neural networks.*

While this is an interesting [article](https://medium.com/the-spike/your-cortex-contains-17-billion-computers-9034e42d34f2) for deepening the complexity model of the brain, I feel that mere physical structure and behavior of subcomponents of neurons is only a tiny piece of the puzzle. 

To understand the workings of the brain you will also need to add in the complex interactions of hormones and other chemical agents in the brain and throughout the nervous system and the feedback loops they establish with the other systems of the body and through them, with the external world.

Neural networks of neural networks doesnt begin to describe it. We havent even scratched the tip of the tip of the iceberg in understanding this stuff.

https://medium.com/the-spike/your-cortex-contains-17-billion-computers-9034e42d34f2",111,86,False,self,,,,,
1533,MachineLearning,t5_2r3gv,2019-3-25,2019,3,25,23,b5boov,self.MachineLearning,"[D] AI/Machine Learning Startups, What Is Your Biggest Difficulty Currently?",https://www.reddit.com/r/MachineLearning/comments/b5boov/d_aimachine_learning_startups_what_is_your/,v3nge,1553525843,"If you run or work at an AI startup, what is the biggest difficulty you/your team is currently facing?",111,116,False,self,,,,,
1534,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,0,b5bxil,medium.com,Baidus ERNIE Tops Googles BERT in Chinese NLP Tasks,https://www.reddit.com/r/MachineLearning/comments/b5bxil/baidus_ernie_tops_googles_bert_in_chinese_nlp/,Yuqing7,1553527077,,0,1,False,https://b.thumbs.redditmedia.com/wKalZDpMCFSnuH6i-zgnj7n6G9SOdxvw_l2d75MY2zs.jpg,,,,,
1535,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,0,b5by7d,/r/MachineLearning/comments/b5by7d/cnc_simulator_on_ios_and_android/,cnc simulator on ios and android,https://www.reddit.com/r/MachineLearning/comments/b5by7d/cnc_simulator_on_ios_and_android/,armansik,1553527177,,2,1,False,default,,,,,
1536,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,0,b5c81r,/r/MachineLearning/comments/b5c81r/cnc_25d_simulator_on_android_and_ios/,cnc 2.5d simulator on android and ios,https://www.reddit.com/r/MachineLearning/comments/b5c81r/cnc_25d_simulator_on_android_and_ios/,armansik,1553528531,,1,1,False,https://b.thumbs.redditmedia.com/9U_eePO4oIIaNyR9TJw2TnQYI1pcxL0IN8ryNIPvN3E.jpg,,,,,
1537,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,0,b5cc1l,self.MachineLearning,Dogzam | The best way to identify dogs,https://www.reddit.com/r/MachineLearning/comments/b5cc1l/dogzam_the_best_way_to_identify_dogs/,Thomas_By,1553529077,[removed],0,1,False,https://b.thumbs.redditmedia.com/nECciN0xzW5dGIjAw6oOGZEYhnbs75E5Rm0tfu882uY.jpg,,,,,
1538,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,0,b5cdlt,self.MachineLearning,[R] Question about attribute selection for autoencoder outlier detection,https://www.reddit.com/r/MachineLearning/comments/b5cdlt/r_question_about_attribute_selection_for/,ricklen,1553529277,"Hello everyone,

&amp;#x200B;

In this paper: [https://arxiv.org/abs/1709.05254](https://arxiv.org/abs/1709.05254) about outlier detection for journal entries Schreyer describes the following:

&amp;#x200B;

&gt;In general, SAP ERP systems record a variety of journal entry attributes predominantly in two tables technically denoted by BKPF and BSEG. The table BKPF - Accounting Document Header contains the meta informa- tion of a journal entry e.g., document id, type, date, time, currency. The table BSEG - Accounting Document Segment, also referred to journal entry line- items, contains the entry details e.g., posting key, general ledger account, debit and credit information, amount. **We extracted a subset of 6 (dataset A) and 10 (dataset B) most discriminative attributes of the BKPF and BSEG tables.**

&amp;#x200B;

It is only described that attributes are selected based on being ""**most discriminative**"", how does one measure this? Are there common ways of doing this kind of feature selection? Why is this not described in the paper? It seems very important in this context to describe how the features are selected.

&amp;#x200B;

&amp;#x200B;",0,5,False,self,,,,,
1539,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,1,b5cgo1,gwern.net,Making Anime Faces With StyleGAN,https://www.reddit.com/r/MachineLearning/comments/b5cgo1/making_anime_faces_with_stylegan/,Ghenlezo,1553529684,,0,1,False,https://b.thumbs.redditmedia.com/GmCOUg1n_9IOK7DNuJxkjlEnVCpQpEfLFoYQ6M0-JLU.jpg,,,,,
1540,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,1,b5cn8x,self.MachineLearning,Question Regarding the Power of Neural Networks,https://www.reddit.com/r/MachineLearning/comments/b5cn8x/question_regarding_the_power_of_neural_networks/,IAmHereToParticipate,1553530528,[removed],0,1,False,self,,,,,
1541,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,1,b5cqnw,self.MachineLearning,[P]How to reduce memory usage within Prado's k-means framework used on big data in R?,https://www.reddit.com/r/MachineLearning/comments/b5cqnw/phow_to_reduce_memory_usage_within_prados_kmeans/,StrictAlgo,1553530974," 

I am trying to validate/replicate Prado's k-means framework for clustering trading strategies based on returns correlation matrix as found in [his paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3167017), using R for a large number of strategies, say 1000 (but would like to be able to work with higher numbers).  


He tries to find optimal *k* and optimal initialization for k-means using two for loops over all possible *k*'s and a number of initializations, i.e. *k*'s go from 2 to *N-1*, where *N* is number of strategies.

&amp;#x200B;

The issue is that running k-means that many times and especially with that many clusters is memory-exhaustive and my computer neither m3.medium AWS instances I have in use are able to do the job. (4 GB RAM both, though on AWS there are less background RAM-consuming processes.)

&amp;#x200B;

So, please, any ideas how to handle this memory issue? Or at least how to estimate the memory amount needed as a function of number of strategies used?

&amp;#x200B;

I have tried the package biganalytics and its bigkmeans function and it was not enough. I am also aware that there are higher RAM AWS instances, but I would like to be sure my code is optimal before switching to such instance. I have also tried to limit the number of clusters used which confirmed that it is the main memory-consuming issue, but I would like not to stick to such solution (nor in combination with better AWS instance).

The highest number of strategies properly executed on AWS was around 500.

The main part of the code to memory-optimalize is as follows:

    D &lt;- nrow(dist) 
    seq.inits &lt;- rep(1:nr.inits,D-2) 
    seq.centers &lt;- rep(2:(D-1),each = nr.inits) 
    KM &lt;- mapply(function(x,y){   
        set.seed(x+333)   
        kmeans(dist, y)
    },seq.inits,seq.centers)

The dist is the strategies' returns' correlation-distance matrix (i.e. the number of columns is equal to number of rows, among other properties), and nr.inits is the number of initializations. Both are input variables. After that the best clustering is determined using silhouette score and possibly reclustered if needed. 

I am aware of the fact that distance matrix is not suitable input for k-means and also I am aware of data mining issues, so please do not address these.

My questions as stated above are:

1. is it possible to reduce memory usage so that I would be able to run 1000 strategies on a m3.medium AWS instance?
2. is it possible to at least estimate memory usage based on number strategies used? (Assuming I try 2:(N-1)  
 clusters.)

Actually, the second question, preferrably after optimalizing, is more important to me. As I would like to try even a much larger number o f strategies than just 1000. 

Thanks for your answers beforehand!",6,0,False,self,,,,,
1542,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,2,b5ddvp,self.MachineLearning,Importing and Splitting CIFAR-10 into Test and Training Set,https://www.reddit.com/r/MachineLearning/comments/b5ddvp/importing_and_splitting_cifar10_into_test_and/,thewolfylion,1553533884,[removed],0,1,False,self,,,,,
1543,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,2,b5dubr,youtube.com,How We Get MACHiNES to Learn | XUCLASS,https://www.reddit.com/r/MachineLearning/comments/b5dubr/how_we_get_machines_to_learn_xuclass/,jeffxu999,1553535940,,0,1,False,default,,,,,
1544,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,2,b5dxcu,self.MachineLearning,[D] Anomaly detection on images using flow-based(GLOW) model?,https://www.reddit.com/r/MachineLearning/comments/b5dxcu/d_anomaly_detection_on_images_using_flowbasedglow/,chinmay19,1553536318,"I am working on an anomaly detection for images problem and I am trying to use GLOW/RealNVP model to achieve that.

My main motivation behind going for normalizing flow-based models like GLOW over VAEs or GANs is that with flow based model I can do exact latent variable inference and log likelihood evaluation because of which I think I will be able to threshold the anomalous samples based on the log likelihood only. 

I am try to test this hypothesis that the anomalous samples should have low likelihood while reconstructing them back with a simple MNIST dataset. However, after reading these papers ([Do gen models know what they dont know](https://arxiv.org/pdf/1810.09136.pdf)

[WAIC, but why? robust anomaly detection using generative ensembels](https://arxiv.org/pdf/1810.01392.pdf)) I see that even the out of distribution samples can end up having higher likelihood values.

I am not sure what can be called as an out of distribution sample, because in the papers the model is trained on CIFAR and while testing on SVHN they found those results which is obviously out of distribution but suppose a model trained with MNIST gets a test sample where a MNIST digit has a black line covering half of it. Can this qualify as an out of distribution sample?

Also is there any other way in which anomaly detection for images based on the models like GLOW or RealNVP can be done?

Any help on this would be great as I am still a beginner to this concept and trying to figure my way out!  
Thanks :)",5,12,False,self,,,,,
1545,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,3,b5e7b8,ai.googleblog.com,Simulated Policy Learning in Video Models,https://www.reddit.com/r/MachineLearning/comments/b5e7b8/simulated_policy_learning_in_video_models/,sjoerdapp,1553537588,,0,1,False,https://b.thumbs.redditmedia.com/BAIy8VldPat01TPF1VmDgOLVXCGKvWECr8oQTxkbGgQ.jpg,,,,,
1546,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,3,b5ebgj,self.MachineLearning,[N] CVPR 2019 Challenges/Contests List,https://www.reddit.com/r/MachineLearning/comments/b5ebgj/n_cvpr_2019_challengescontests_list/,skrish13,1553538114,https://github.com/skrish13/ml-contests-conf#cvpr-19,0,1,False,self,,,,,
1547,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,3,b5edvx,self.MachineLearning,[N] CVPR 2019 Challenges/Contests,https://www.reddit.com/r/MachineLearning/comments/b5edvx/n_cvpr_2019_challengescontests/,skrish13,1553538427,"There are 88(!) workshops this year in @cvpr2019 (they all happen in the span of 2 days as well). Here's a list of all (33) challenges/contests from among them 
https://github.com/skrish13/ml-contests-conf#cvpr-19",4,11,False,self,,,,,
1548,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,3,b5er46,self.MachineLearning,3D pose estimation with posenet ?,https://www.reddit.com/r/MachineLearning/comments/b5er46/3d_pose_estimation_with_posenet/,theSamuraiMonk17,1553540282,[removed],0,1,False,self,,,,,
1549,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,4,b5exbj,self.MachineLearning,$2000 to play with on a ML Rig,https://www.reddit.com/r/MachineLearning/comments/b5exbj/2000_to_play_with_on_a_ml_rig/,neurochimp,1553541052,"Hi all, as stated in the title, I have $2000 to spend on:

a) computer

b) GPU (Either the 1080Ti or 2080Ti, with a strong preference for the 2080Ti

c) extras, like a monitor or an upgraded power supply (seems like for the 2080, a minimum 650 Watt supply is needed. 

&amp;#x200B;

Hoping to make most of these purchases on Newegg. I am basically a novice on this topic, so any ideas/suggestions are greatly appreciated. Thanks a million!",0,1,False,self,,,,,
1550,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,4,b5f086,self.MachineLearning,Question about computer vision application idea [Video analytics],https://www.reddit.com/r/MachineLearning/comments/b5f086/question_about_computer_vision_application_idea/,maholeycow,1553541421,[removed],0,1,False,self,,,,,
1551,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,4,b5f6ys,arxiv.org,[R]Neural networks and rational functions - ReLU networks build lookup tables to approximate 1/x,https://www.reddit.com/r/MachineLearning/comments/b5f6ys/rneural_networks_and_rational_functions_relu/,Marthinwurer,1553542273,,5,2,False,default,,,,,
1552,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,4,b5f9wm,github.com,Reinforcement Learning Jupyter Notebooks: Sutton and Barto book,https://www.reddit.com/r/MachineLearning/comments/b5f9wm/reinforcement_learning_jupyter_notebooks_sutton/,pulksPK,1553542630,,0,1,False,default,,,,,
1553,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,4,b5fc3g,medium.com,Deep Classifiers Ignore Almost Everything They See (and how we may be able to fix it),https://www.reddit.com/r/MachineLearning/comments/b5fc3g/deep_classifiers_ignore_almost_everything_they/,Isinlor,1553542909,,1,1,False,https://b.thumbs.redditmedia.com/mV_jjQjlnIv75uyIYVJ9MfMIc9tsM6OLLk4a2EJ49Dk.jpg,,,,,
1554,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,5,b5fs4r,self.MachineLearning,Reinforcement Learning from scratch: Jupyter Notebooks for the Sutton and Barto book,https://www.reddit.com/r/MachineLearning/comments/b5fs4r/reinforcement_learning_from_scratch_jupyter/,pulksPK,1553545008,[removed],0,1,False,self,,,,,
1555,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,5,b5g7m1,youtu.be,Machine Learning Tutorial Part 8 | Accuracy(MAE/RMSE) - Python Machine Learning For Beginners,https://www.reddit.com/r/MachineLearning/comments/b5g7m1/machine_learning_tutorial_part_8_accuracymaermse/,SquareTechAcademy,1553547053,,0,1,False,https://b.thumbs.redditmedia.com/DNLXMFdErRaeLzE1BJMePBmkguY7-WFhrBcndaNIeoM.jpg,,,,,
1556,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,7,b5h711,github.com,Meta-Learning progress repo,https://www.reddit.com/r/MachineLearning/comments/b5h711/metalearning_progress_repo/,MichaelMMeskhi,1553551706,,0,1,False,https://a.thumbs.redditmedia.com/MspFSY_O6SsZ_mn88V1wL0J_4EX4-2sat01F6qar548.jpg,,,,,
1557,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,7,b5hmlp,self.MachineLearning,[R] Visualizing memorization in RNNs,https://www.reddit.com/r/MachineLearning/comments/b5hmlp/r_visualizing_memorization_in_rnns/,baylearn,1553553774,"New distill.pub post on Visualizing memorization in RNNs

https://distill.pub/2019/memorization-in-rnns/",0,1,False,self,,,,,
1558,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,7,b5hou8,self.MachineLearning,Forecasting with a Keras LSTM,https://www.reddit.com/r/MachineLearning/comments/b5hou8/forecasting_with_a_keras_lstm/,sarasotadude,1553554082,[removed],0,1,False,self,,,,,
1559,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,8,b5i7e7,self.MachineLearning,What algos would you use?,https://www.reddit.com/r/MachineLearning/comments/b5i7e7/what_algos_would_you_use/,Aesix,1553556790,[removed],0,1,False,self,,,,,
1560,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,8,b5idqk,self.MachineLearning,"[P] Dataset: 480,000 Rotten Tomatoes reviews for NLP. Labeled as fresh/rotten",https://www.reddit.com/r/MachineLearning/comments/b5idqk/p_dataset_480000_rotten_tomatoes_reviews_for_nlp/,nicolas-gervais,1553557778,"240,000 fresh reviews and 240,000 rotten reviews, labeled, with their text review. Get the CSV on my [Google Drive](https://drive.google.com/file/d/1w1TsJB-gmIkZ28d1j7sf1sqcPmHXw352/view?usp=sharing)",49,458,False,self,,,,,
1561,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,8,b5if0b,self.MachineLearning,What would be examples of unsupervised sentiment analysis?,https://www.reddit.com/r/MachineLearning/comments/b5if0b/what_would_be_examples_of_unsupervised_sentiment/,GAAfanatic,1553557962,[removed],0,1,False,self,,,,,
1562,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,9,b5inbo,self.MachineLearning,[P] Predicting future values with an LSTM,https://www.reddit.com/r/MachineLearning/comments/b5inbo/p_predicting_future_values_with_an_lstm/,sarasotadude,1553559187,"Hi all,

&amp;#x200B;

I've trained an LSTM on 3 years of data broken down on a daily basis.  I'm trying to forecast/predict the next few (5-50) values, however I am very stuck.  The model.predict method does not seem to be able to do this without me specifying input shapes.  Is there a way I can predict the next few values of a trained LSTM without passing it any parameters, i.e. just produce the next few predicted values in the time series sequence?

&amp;#x200B;

Thanks for all responses, I seem to be in a bit of a bind.  This is a great community.

&amp;#x200B;

&amp;#x200B;",5,0,False,self,,,,,
1563,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,10,b5j6d6,self.MachineLearning,Any good datasets for Agricultural Pest?,https://www.reddit.com/r/MachineLearning/comments/b5j6d6/any_good_datasets_for_agricultural_pest/,nambatac80,1553562016,[removed],0,1,False,self,,,,,
1564,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,10,b5jcm0,shenchong.com,WG67K High Speed Mini Press Brake Bending Machine for steel plate,https://www.reddit.com/r/MachineLearning/comments/b5jcm0/wg67k_high_speed_mini_press_brake_bending_machine/,CNCPressBrakeChina,1553562940,,0,1,False,default,,,,,
1565,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,10,b5jqpz,self.MachineLearning,"AI accurately predicts the useful life of batteries, Stanford and MIT researchers find",https://www.reddit.com/r/MachineLearning/comments/b5jqpz/ai_accurately_predicts_the_useful_life_of/,markgolden,1553565117,"*In an advance that could accelerate battery development and improve manufacturing, scientists have found how to accurately predict the cycle life of lithium-ion batteries. Method could be applied to other chemistries.* [Joint release from Stanford, MIT and Toyota has link to paper in Nature Energy today](https://news.stanford.edu/2019/03/25/ai-accurately-predicts-useful-life-batteries/). Elastic net was applied to features proposed from domain knowledge of lithium-ion batteries. 4-fold Monte Carlo cross validation training. The models were tested on datasets that were not used in training the models. More under ""methods"" in paper.

![video](bgjfcrhncdo21)",0,1,False,self,,,,,
1566,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,11,b5jz7j,self.MachineLearning,Most economical way to get computing time?,https://www.reddit.com/r/MachineLearning/comments/b5jz7j/most_economical_way_to_get_computing_time/,ebolafever,1553566453,[removed],0,1,False,self,,,,,
1567,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,11,b5k05h,self.MachineLearning,optimization techniques in reinforcement learning,https://www.reddit.com/r/MachineLearning/comments/b5k05h/optimization_techniques_in_reinforcement_learning/,newSam111,1553566601,[removed],0,1,False,self,,,,,
1568,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,11,b5kbk5,self.MachineLearning,"[D] Thoughts on the current state of Audio ""Style Transfer""?",https://www.reddit.com/r/MachineLearning/comments/b5kbk5/d_thoughts_on_the_current_state_of_audio_style/,FreckledMil,1553568453,"From what I have seen, so far there has only been an instrumentation change achieved, which is pretty awesome and not to be scoffed at (I thought it was pretty amazing for a neural-net). 

I am not up to date by any means on the current state of the art for audio learning, but I am wondering if there's a way to turn happy birthday into a nirvana jam, with kurt cobain singing gibberish to the happy birthday tune (if you know what I am describing).

From what I have found, it looks like there are active attempts using various differing approached with base neural-style algo, some hyperparameter tuning, variational inference/autoencoding, with/without conditional wavenets.. and this is intriguing and I am still trying to absorb it all.

I have often wondered if pix2pix could be used for something like this. Have the source song set be the analogous ""cat drawing outline"", and have the generated ""finished textured cat drawing"" be kurt cobain singing happy birthday. Without being an expert on audio style transfer (or much), something tells me pix2pix is the type of approach needed.

Would love to hear some actual educated thoughts on the matter. thanks.


",8,9,False,self,,,,,
1569,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,12,b5khdo,self.MachineLearning,Is Naive Bayes a decent model for simple image classification?,https://www.reddit.com/r/MachineLearning/comments/b5khdo/is_naive_bayes_a_decent_model_for_simple_image/,engineheat,1553569428,[removed],0,1,False,self,,,,,
1570,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,12,b5kl3g,self.MachineLearning,[HELP] Rico Dataset,https://www.reddit.com/r/MachineLearning/comments/b5kl3g/help_rico_dataset/,schoolknight,1553570071,[removed],0,1,False,self,,,,,
1571,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,12,b5kv6a,arxiv.org,[R] Deep Multi-Agent Reinforcement Learning with Discrete-Continuous Hybrid Action Spaces,https://www.reddit.com/r/MachineLearning/comments/b5kv6a/r_deep_multiagent_reinforcement_learning_with/,Fgnb123,1553571918,,1,2,False,default,,,,,
1572,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,14,b5lkjj,youtu.be,Training and Testing Data with Python,https://www.reddit.com/r/MachineLearning/comments/b5lkjj/training_and_testing_data_with_python/,megharoy432,1553576594,,0,1,False,https://b.thumbs.redditmedia.com/_ta1ufnigSscHQYCmAsBrZvhAGChfBKDQqR5032B7jE.jpg,,,,,
1573,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,14,b5lqlg,self.MachineLearning,Machine Learning is the future: Heres everything you must know,https://www.reddit.com/r/MachineLearning/comments/b5lqlg/machine_learning_is_the_future_heres_everything/,sankarsp,1553577749,[removed],0,1,False,self,,,,,
1574,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,14,b5lwqj,self.MachineLearning,200+ FREE Healthcare and Life science datasets,https://www.reddit.com/r/MachineLearning/comments/b5lwqj/200_free_healthcare_and_life_science_datasets/,johnsnowlabsUS,1553578932,[removed],0,1,False,self,,,,,
1575,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,15,b5m20e,devarea.com,Linear Regression With Numpy,https://www.reddit.com/r/MachineLearning/comments/b5m20e/linear_regression_with_numpy/,liranbh,1553580024,,0,1,False,default,,,,,
1576,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,15,b5m9fu,self.MachineLearning,water requirement prediction help,https://www.reddit.com/r/MachineLearning/comments/b5m9fu/water_requirement_prediction_help/,Bindutr15,1553581556,[removed],0,1,False,self,,,,,
1577,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,15,b5mcoc,self.MachineLearning,"Assembly Automation Market gauge, Trend and figure 2023",https://www.reddit.com/r/MachineLearning/comments/b5mcoc/assembly_automation_market_gauge_trend_and_figure/,Shivs7,1553582249,[removed],1,1,False,self,,,,,
1578,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,15,b5men2,self.MachineLearning,"Nobody actually believes that neural networks or ML pretend to behave like the brain, right?",https://www.reddit.com/r/MachineLearning/comments/b5men2/nobody_actually_believes_that_neural_networks_or/,Ctown_struggles00,1553582685,[removed],0,1,False,self,,,,,
1579,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,15,b5mh5q,lhd.co.com,LHD manufacture the great quality Telescopic Fork and Stacker Cranes.,https://www.reddit.com/r/MachineLearning/comments/b5mh5q/lhd_manufacture_the_great_quality_telescopic_fork/,lhd121,1553583223,,0,1,False,default,,,,,
1580,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,15,b5mhg5,self.MachineLearning,"[D] Nobody actually believes that neural networks or ML pretend to behave like the brain, right?",https://www.reddit.com/r/MachineLearning/comments/b5mhg5/d_nobody_actually_believes_that_neural_networks/,Ctown_struggles00,1553583287,"We all know that neural networks don't reflect how neurons work, right? 

It always bothers me when I see layman latch onto faulty terminology and claim that AI is simulating human learning...do you think that ML scientists believe this myth as well? 

It's even more worrisome when investors and managers without a tech background believe this. They're buying into an idea that isn't correct. ",68,13,False,self,,,,,
1581,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,16,b5msnw,/r/MachineLearning/comments/b5msnw/cnc_25d_simulator_on_ios_and_android/,cnc 2.5d simulator on ios and android,https://www.reddit.com/r/MachineLearning/comments/b5msnw/cnc_25d_simulator_on_ios_and_android/,armansik,1553585778,,1,1,False,default,,,,,
1582,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,17,b5mybm,self.MachineLearning,NLP - Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b5mybm/nlp_machine_learning/,deepam_g,1553587204,[removed],0,1,False,self,,,,,
1583,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,17,b5mzim,github.com,"A tutorial on basic Python, NumPy, SciPy, and Matplotlib (Preparatory material for CS231n)",https://www.reddit.com/r/MachineLearning/comments/b5mzim/a_tutorial_on_basic_python_numpy_scipy_and/,srikavig,1553587488,,0,1,False,default,,,,,
1584,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,17,b5n0jw,medium.com,GANs vs ODEs: the end of mathematical modeling?,https://www.reddit.com/r/MachineLearning/comments/b5n0jw/gans_vs_odes_the_end_of_mathematical_modeling/,rachnogstyle,1553587742,,0,1,False,https://b.thumbs.redditmedia.com/2WSIxggYio2wtg2aM98s0Ylei3X__v54S0Ylu2sd-tw.jpg,,,,,
1585,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,17,b5n9es,self.MachineLearning,WHAT IS DATA SCIENCE AND SCOPE OF DATA SCIENCE,https://www.reddit.com/r/MachineLearning/comments/b5n9es/what_is_data_science_and_scope_of_data_science/,chandulekkala,1553589931,[removed],0,1,False,self,,,,,
1586,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,18,b5ni9x,self.MachineLearning,[R] Visualizing memorization in RNNs: Inspecting gradient magnitudes in context can be a powerful tool to see when recurrent units use short-term or long-term contextual understanding,https://www.reddit.com/r/MachineLearning/comments/b5ni9x/r_visualizing_memorization_in_rnns_inspecting/,alexeyr,1553592004,"[A short article](https://distill.pub/2019/memorization-in-rnns/) comparing memorization behavior of Nested LSTM, LSTM, and GRU.",0,24,False,self,,,,,
1587,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,18,b5nnti,self.MachineLearning,BrewV - A opensource gesture recognition based video player,https://www.reddit.com/r/MachineLearning/comments/b5nnti/brewv_a_opensource_gesture_recognition_based/,Advizag,1553593290,[removed],0,1,False,self,,,,,
1588,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,19,b5nya5,mirrorreview.com,Supacore Compression: Reducing The Cost Of Injury In Sports Through Innovation,https://www.reddit.com/r/MachineLearning/comments/b5nya5/supacore_compression_reducing_the_cost_of_injury/,mirrorreviewm,1553595690,,0,1,False,default,,,,,
1589,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,19,b5o2sp,medium.com,"Greenhouse Heaters Market Strategy Assessment, Development &amp; Futuristic Trends",https://www.reddit.com/r/MachineLearning/comments/b5o2sp/greenhouse_heaters_market_strategy_assessment/,varshamotale21,1553596646,,0,1,False,default,,,,,
1590,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,20,b5ospl,self.MachineLearning,[Project] AmpliGraph: a TensorFlow-based Library for Knowledge Graph Embeddings,https://www.reddit.com/r/MachineLearning/comments/b5ospl/project_ampligraph_a_tensorflowbased_library_for/,lukostaz,1553601501,"AmpliGraph is a suite of neural machine learning models for relational representation learning. You can use it to discover new knowledge from an existing knowledge graph, find missing statements, or play around with embeddings generated from graphs.

* GitHub: https://ampligraph.org
* Documentation: https://docs.ampligraph.org 
* Quick start: https://docs.ampligraph.org/en/latest/install.html ",6,38,False,self,,,,,
1591,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,21,b5p8o6,self.MachineLearning,Could ELU replace a RELU in a neural network?,https://www.reddit.com/r/MachineLearning/comments/b5p8o6/could_elu_replace_a_relu_in_a_neural_network/,Jandevries101,1553604233,[removed],0,1,False,self,,,,,
1592,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,22,b5pi7d,self.MachineLearning,Grad student looking for advice/suggestions for master's thesis.,https://www.reddit.com/r/MachineLearning/comments/b5pi7d/grad_student_looking_for_advicesuggestions_for/,benjfrank,1553605725,[removed],0,1,False,self,,,,,
1593,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,22,b5po4i,self.MachineLearning,Control and Understand a UI Driven by Machine Learning - Lush Dubai,https://www.reddit.com/r/MachineLearning/comments/b5po4i/control_and_understand_a_ui_driven_by_machine/,hussy456,1553606681,[removed],0,1,False,self,,,,,
1594,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,22,b5ppsi,self.MachineLearning,Implementing a vanilla autoencoder in TensorFlow 2.0,https://www.reddit.com/r/MachineLearning/comments/b5ppsi/implementing_a_vanilla_autoencoder_in_tensorflow/,afagarap,1553606926,[removed],0,1,False,self,,,,,
1595,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,22,b5pq3u,mux.com,How we improved Tensorflow Serving performance by over 70%,https://www.reddit.com/r/MachineLearning/comments/b5pq3u/how_we_improved_tensorflow_serving_performance_by/,j_orshman,1553606980,,0,1,False,https://b.thumbs.redditmedia.com/1QqwQk__A_Hy0rxLViiGekN7e527h8Jq8u4Z1vjGKcA.jpg,,,,,
1596,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,22,b5pvt2,self.MachineLearning,[R] doing a research project for my HND Course of AI.,https://www.reddit.com/r/MachineLearning/comments/b5pvt2/r_doing_a_research_project_for_my_hnd_course_of_ai/,FlawlessKasper,1553607871,"Hey, got a survey that I need to gather some data up for a research project in regards to AI in Data Security within medicine/healthcare. This is for my college course so would love some responses

Would love if you guys checked it out

thanks,

https://docs.google.com/forms/d/1SZDzMX76O6BDfBJGmfOT5qffoQ1T_lLzXsT8ypcXs4I/
",0,0,False,self,,,,,
1597,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,22,b5pwya,medium.com,[D] The Right Way To Learn Machine Learning Online,https://www.reddit.com/r/MachineLearning/comments/b5pwya/d_the_right_way_to_learn_machine_learning_online/,omarsar,1553608047,,0,1,False,default,,,,,
1598,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,22,b5pyfd,self.MachineLearning,Combine user experience design with machine learning: UI + AI,https://www.reddit.com/r/MachineLearning/comments/b5pyfd/combine_user_experience_design_with_machine/,hussy456,1553608277,[removed],0,1,False,self,,,,,
1599,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,22,b5pyxl,self.MachineLearning,How to verify performance of a Deep Learning algorithm on a large scale ?,https://www.reddit.com/r/MachineLearning/comments/b5pyxl/how_to_verify_performance_of_a_deep_learning/,yetine,1553608356,[removed],0,1,False,self,,,,,
1600,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,23,b5q8pm,self.MachineLearning,Need advice w.r.t. my phd position,https://www.reddit.com/r/MachineLearning/comments/b5q8pm/need_advice_wrt_my_phd_position/,EZretr0,1553609798,[removed],0,1,False,self,,,,,
1601,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,23,b5qalb,self.MachineLearning,Building a deeplearning capable computer in 2019,https://www.reddit.com/r/MachineLearning/comments/b5qalb/building_a_deeplearning_capable_computer_in_2019/,Alienbushman,1553610078,[removed],0,1,False,self,,,,,
1602,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,23,b5qg3p,self.MachineLearning,Math Prerequisites,https://www.reddit.com/r/MachineLearning/comments/b5qg3p/math_prerequisites/,ribbonsofeuphoria,1553610915,[removed],0,1,False,self,,,,,
1603,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,23,b5qh5z,medium.com,Snorkel DryBell Exploits the Strength of Weakly Supervised ML for Information Integration,https://www.reddit.com/r/MachineLearning/comments/b5qh5z/snorkel_drybell_exploits_the_strength_of_weakly/,gwen0927,1553611066,,0,1,False,https://b.thumbs.redditmedia.com/XLzvVZSJdzQ8ajnACsXXA6Qeyv2TNNugPxzePVDuriw.jpg,,,,,
1604,MachineLearning,t5_2r3gv,2019-3-26,2019,3,26,23,b5qoef,self.MachineLearning,[R] Excessive Invariance Causes Network Uncertainty,https://www.reddit.com/r/MachineLearning/comments/b5qoef/r_excessive_invariance_causes_network_uncertainty/,a_draganov,1553612139,"[This blog post](https://medium.com/@j.jacobsen/deep-classifiers-ignore-almost-everything-they-see-and-how-we-may-be-able-to-fix-it-a6888012516f) and [corresponding paper](https://openreview.net/pdf?id=BkfbpsAcF7) describe a new formulation for adversarial attacks that come about from the issues of cross-entropy.

Summary from blog post: 
* classifiers are not only invariant to class-irrelevant variations, but also to almost everything humans consider relevant for a class; we term this property excessive invariance (see figure above for example)
* Excessive invariance gives an alternative explanation for the adversarial example phenomenon
* We identify the commonly-used cross-entropy objective as a major reason for the striking invariance we observed
* There may be a way to control and overcome this problem 

Accepted for ICLR 2019. I'm not the author - saw it passed around at work and wanted to hear outside opinions.",8,24,False,self,,,,,
1605,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,0,b5qzdu,intellectyx.com,Contextual Chatbot Powered By Artificial Intelligence (AI) and Its Use Cases,https://www.reddit.com/r/MachineLearning/comments/b5qzdu/contextual_chatbot_powered_by_artificial/,macyada,1553613646,,0,1,False,default,,,,,
1606,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,0,b5r6ab,self.MachineLearning,[D] Building a deeplearning capable computer in 2019,https://www.reddit.com/r/MachineLearning/comments/b5r6ab/d_building_a_deeplearning_capable_computer_in_2019/,Alienbushman,1553614557,"DISCLAIMER:  I am in South Africa, so I may have some trouble getting some niche parts and some parts you get here, you don't see a price listed on PCpartpicker.

I  would like some advice on how to build my first deeplearning capable  computer, basically I am looking at one of two sets of build, the first  is a design around an RTX 2070 (budget +-$1250), the second is a build that would allow for a GPU or two to be added in the future (budget  +-$2000 with 1 GPU).

Basically,  my question is, is it worth going a little more expensive for the possibility to upgrade or should I just buy the cheaper one and upgrade it as technology improves. Also, I would appreciate any advice, I am not exactly sure what my requirements are. This computer will be for personal use and I will be using it for kaggle and for doing a masters in deeplearning (using PyTorch). The main objective is getting the best price per processing unit for an extended period of time.

The single GPU build

\[PCPartPicker part list\]([https://pcpartpicker.com/list/f4FGCb](https://pcpartpicker.com/list/f4FGCb)) / \[Price breakdown by merchant\]([https://pcpartpicker.com/list/f4FGCb/by\_merchant/](https://pcpartpicker.com/list/f4FGCb/by_merchant/))

Type|Item|Price

:----|:----|:----

\*\*CPU\*\* | \[AMD - Ryzen 5 2600X 3.6 GHz 6-Core Processor\]([https://pcpartpicker.com/product/6mm323/amd-ryzen-5-2600x-36ghz-6-core-processor-yd260xbcafbox](https://pcpartpicker.com/product/6mm323/amd-ryzen-5-2600x-36ghz-6-core-processor-yd260xbcafbox)) | $184.89 @ OutletPC

\*\*Motherboard\*\* | \[MSI - B450M MORTAR Micro ATX AM4 Motherboard\]([https://pcpartpicker.com/product/FZvbt6/msi-b450m-mortar-atx-am4-motherboard-b450m-mortar](https://pcpartpicker.com/product/FZvbt6/msi-b450m-mortar-atx-am4-motherboard-b450m-mortar)) |-

\*\*Memory\*\* | \[G.Skill - Flare X 16 GB (2 x 8 GB) DDR4-2400 Memory\]([https://pcpartpicker.com/product/7t2rxr/gskill-flare-x-16gb-2-x-8gb-ddr4-2400-memory-f4-2400c16d-16gfx](https://pcpartpicker.com/product/7t2rxr/gskill-flare-x-16gb-2-x-8gb-ddr4-2400-memory-f4-2400c16d-16gfx)) | $79.89 @ OutletPC

\*\*Storage\*\* | \[Samsung - 970 Evo 500 GB M.2-2280 Solid State Drive\]([https://pcpartpicker.com/product/P4ZFf7/samsung-970-evo-500gb-m2-2280-solid-state-drive-mz-v7e500bw](https://pcpartpicker.com/product/P4ZFf7/samsung-970-evo-500gb-m2-2280-solid-state-drive-mz-v7e500bw)) | $149.99 @ Amazon

\*\*Storage\*\* | \[Seagate - Barracuda Compute 2 TB 2.5"" 5400RPM Internal Hard Drive\]([https://pcpartpicker.com/product/zxXnTW/seagate-barracuda-compute-2-tb-25-5400rpm-internal-hard-drive-st2000lm007](https://pcpartpicker.com/product/zxXnTW/seagate-barracuda-compute-2-tb-25-5400rpm-internal-hard-drive-st2000lm007)) | $84.99 @ Other World Computing

\*\*Video Card\*\* | \[Palit - GeForce RTX 2070 8 GB JetStream Video Card\]([https://pcpartpicker.com/product/ksNv6h/palit-geforce-rtx-2070-8-gb-jetstream-video-card-ne62070020p2-1061j](https://pcpartpicker.com/product/ksNv6h/palit-geforce-rtx-2070-8-gb-jetstream-video-card-ne62070020p2-1061j)) |-

\*\*Case\*\* | \[Phanteks - Eclipse P350X (Black) ATX Mid Tower Case\]([https://pcpartpicker.com/product/w766Mp/phanteks-eclipse-p350x-atx-mid-tower-case-ph-ec350ptg\_dbk](https://pcpartpicker.com/product/w766Mp/phanteks-eclipse-p350x-atx-mid-tower-case-ph-ec350ptg_dbk)) | $65.98 @ Newegg

\*\*Power Supply\*\* | \[Super Flower - Leadex Gold 650 W 80+ Gold Certified Fully-Modular ATX Power Supply\]([https://pcpartpicker.com/product/4Pbp99/super-flower-power-supply-sf650f14mg](https://pcpartpicker.com/product/4Pbp99/super-flower-power-supply-sf650f14mg)) |-

| \*Prices include shipping, taxes, rebates, and discounts\* |

| Total (before mail-in rebates) | $575.74

| Mail-in rebates | -$10.00

| \*\*Total\*\* | \*\*$565.74\*\*

| Generated by \[PCPartPicker\]([https://pcpartpicker.com](https://pcpartpicker.com/)) 2019-03-26 10:05 EDT-0400 |

Upgradeable GPU build

\[PCPartPicker part list\]([https://pcpartpicker.com/list/BVGqFt](https://pcpartpicker.com/list/BVGqFt)) / \[Price breakdown by merchant\]([https://pcpartpicker.com/list/BVGqFt/by\_merchant/](https://pcpartpicker.com/list/BVGqFt/by_merchant/))

Type|Item|Price

:----|:----|:----

\*\*CPU\*\* | \[AMD - Threadripper 1920X 3.5 GHz 12-Core Processor\]([https://pcpartpicker.com/product/cRDzK8/amd-threadripper-1920x-35ghz-12-core-processor-yd192xa8aewof](https://pcpartpicker.com/product/cRDzK8/amd-threadripper-1920x-35ghz-12-core-processor-yd192xa8aewof)) | $279.99 @ Amazon

\*\*CPU Cooler\*\* | \[Fractal Design - Celsius S24 87.6 CFM Liquid CPU Cooler\]([https://pcpartpicker.com/product/nfzZxr/fractal-design-celsius-s24-876-cfm-liquid-cpu-cooler-fd-wcu-celsius-s24-bk](https://pcpartpicker.com/product/nfzZxr/fractal-design-celsius-s24-876-cfm-liquid-cpu-cooler-fd-wcu-celsius-s24-bk)) | $110.99 @ SuperBiiz

\*\*Motherboard\*\* | \[Asus - PRIME X399-A EATX TR4 Motherboard\]([https://pcpartpicker.com/product/wMjWGX/asus-prime-x399-a-eatx-tr4-motherboard-prime-x399-a](https://pcpartpicker.com/product/wMjWGX/asus-prime-x399-a-eatx-tr4-motherboard-prime-x399-a)) | $299.97 @ B&amp;H

\*\*Memory\*\* | \[G.Skill - Ripjaws 4 Series 16 GB (2 x 8 GB) DDR4-2666 Memory\]([https://pcpartpicker.com/product/jb8H99/gskill-memory-f42666c15d16grr](https://pcpartpicker.com/product/jb8H99/gskill-memory-f42666c15d16grr)) | $100.98 @ Newegg

\*\*Memory\*\* | \[G.Skill - Ripjaws 4 Series 16 GB (2 x 8 GB) DDR4-2666 Memory\]([https://pcpartpicker.com/product/jb8H99/gskill-memory-f42666c15d16grr](https://pcpartpicker.com/product/jb8H99/gskill-memory-f42666c15d16grr)) | $100.98 @ Newegg

\*\*Storage\*\* | \[Samsung - 970 Evo 500 GB M.2-2280 Solid State Drive\]([https://pcpartpicker.com/product/P4ZFf7/samsung-970-evo-500gb-m2-2280-solid-state-drive-mz-v7e500bw](https://pcpartpicker.com/product/P4ZFf7/samsung-970-evo-500gb-m2-2280-solid-state-drive-mz-v7e500bw)) | $149.99 @ Amazon

\*\*Storage\*\* | \[Seagate - Barracuda 2 TB 3.5"" 7200RPM Internal Hard Drive\]([https://pcpartpicker.com/product/CbL7YJ/seagate-barracuda-2tb-35-7200rpm-internal-hard-drive-st2000dm006](https://pcpartpicker.com/product/CbL7YJ/seagate-barracuda-2tb-35-7200rpm-internal-hard-drive-st2000dm006)) | $59.89 @ OutletPC

\*\*Storage\*\* | \[Seagate - Barracuda 2 TB 3.5"" 7200RPM Internal Hard Drive\]([https://pcpartpicker.com/product/CbL7YJ/seagate-barracuda-2tb-35-7200rpm-internal-hard-drive-st2000dm006](https://pcpartpicker.com/product/CbL7YJ/seagate-barracuda-2tb-35-7200rpm-internal-hard-drive-st2000dm006)) | $59.89 @ OutletPC

\*\*Video Card\*\* | \[Palit - GeForce RTX 2070 8 GB JetStream Video Card\]([https://pcpartpicker.com/product/ksNv6h/palit-geforce-rtx-2070-8-gb-jetstream-video-card-ne62070020p2-1061j](https://pcpartpicker.com/product/ksNv6h/palit-geforce-rtx-2070-8-gb-jetstream-video-card-ne62070020p2-1061j)) |-

\*\*Case\*\* | \[Phanteks - ENTHOO EVOLV X GLASS (Black) ATX Mid Tower Case\]([https://pcpartpicker.com/product/3qgzK8/phanteks-enthoo-evolv-x-glass-black-atx-mid-tower-case-ph-es518xtg\_dbk](https://pcpartpicker.com/product/3qgzK8/phanteks-enthoo-evolv-x-glass-black-atx-mid-tower-case-ph-es518xtg_dbk)) | $199.99 @ Amazon

\*\*Power Supply\*\* | \[Super Flower - Leadex Platinum 750 W 80+ Platinum Certified Fully-Modular ATX Power Supply\]([https://pcpartpicker.com/product/MVXnTW/super-flower-leadex-platinum-750w-80-platinum-certified-fully-modular-atx-power-supply-sf-750f14mp-blackuk](https://pcpartpicker.com/product/MVXnTW/super-flower-leadex-platinum-750w-80-platinum-certified-fully-modular-atx-power-supply-sf-750f14mp-blackuk)) |-

| \*Prices include shipping, taxes, rebates, and discounts\* |

| \*\*Total\*\* | \*\*$1362.67\*\*

| Generated by \[PCPartPicker\]([https://pcpartpicker.com](https://pcpartpicker.com/)) 2019-03-26 10:06 EDT-0400 |",16,8,False,self,,,,,
1607,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,0,b5re46,deep-phenomena.org,[N] ICML Workshop on Identifying and Understanding Deep Learning Phenomena,https://www.reddit.com/r/MachineLearning/comments/b5re46/n_icml_workshop_on_identifying_and_understanding/,circuithunter,1553615615,,0,2,False,default,,,,,
1608,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,1,b5rmcj,self.MachineLearning,[N] Call for Papers: ICML Workshop on Identifying and Understanding Deep Learning Phenomena,https://www.reddit.com/r/MachineLearning/comments/b5rmcj/n_call_for_papers_icml_workshop_on_identifying/,circuithunter,1553616704,"ICML Workshop on Identifying and Understanding Deep Learning Phenomena: http://deep-phenomena.org/

Abstract: 

""Our understanding of modern neural networks lags behind their practical successes. This growing gap poses a challenge to the pace of progress in machine learning because fewer pillars of knowledge are available to designers of models and algorithms. This workshop aims to close this understanding gap. We solicit contributions that view the behavior of deep nets as natural phenomenona, to be investigated with methods inspired from the natural sciences like physics, astronomy, and biology. We call for empirical work that isolates phenomena in deep nets, describes them quantitatively, and then replicates or falsifies them.

As a starting point for this effort, we focus on the interplay between data, network architecture, and training algorithms. We seek contributions that identify precise, reproducible phenomena, and studies of current beliefs such as sharp local minima do not generalize well or SGD navigates out of local minima. Through the workshop, we hope to catalogue quantifiable versions of such statements, and demonstrate whether they occur reliably.""

Paper submission deadline: May 5
Notification: May 30 (early notification available upon request)",0,9,False,self,,,,,
1609,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,1,b5rt5l,self.MachineLearning,[D] How to down-sample a large game-board ?,https://www.reddit.com/r/MachineLearning/comments/b5rt5l/d_how_to_downsample_a_large_gameboard/,yazriel0,1553617601,"I am doing model-based DRL on a perfect-information discrete stochastic model (similar to sokoban type simulation). Using policy-gradients and CPUs I am able to learn a good policy for next action on a fixed-size board. 

My next challenge is arbitrarily sized boards.

Approach1 : use some sort of transformer on the input board to produce a fixed-size output board ? I would appreciate any input on this. 

Approach2 : create a (dilated?) CNN that reduces any board size by x2. Apply this recursively to get to the required size. 

How do I create the CNN? Presumably I need to train 2 policy-gradients, on board size K and 2*K, and then use supervised learning to teach the CNN ? Not sure which loss i should use here.

Should i be using a single CNN mask or maybe learn several different CNN for each board area ? The game itself has lots of position-invariants, and can be rotated, shifted, etc.

I have never used dilated CNN but it seems a good idea since the game has lots of spatial locality but also far-away effects. 

Approach3: train a single PG, which has the same lower CNN in the lowest level. Then just take this CNN and use it recursively as needed. 

Any comments welcomed.",1,2,False,self,,,,,
1610,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,1,b5rw0o,socialdribbler.com,"R is considered to be the best programming language for any statistician as it possesses an extensive catalog of statistical and graphical methods. Python on the other hand, can do pretty much the same work as R but.?",https://www.reddit.com/r/MachineLearning/comments/b5rw0o/r_is_considered_to_be_the_best_programming/,monkeyguy2017,1553617979,,0,1,False,default,,,,,
1611,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,1,b5s3bt,self.MachineLearning,Splitting songs into constituent tracks with training on artificially generated songs?,https://www.reddit.com/r/MachineLearning/comments/b5s3bt/splitting_songs_into_constituent_tracks_with/,vzakharov,1553618990,[removed],0,1,False,self,,,,,
1612,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,1,b5s7k3,heartbeat.fritz.ai,Using Machine Learning to Predict Bus Ticket Sales,https://www.reddit.com/r/MachineLearning/comments/b5s7k3/using_machine_learning_to_predict_bus_ticket_sales/,smutuvi,1553619568,,0,1,False,default,,,,,
1613,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,2,b5s8k4,youtube.com,"Is Kurt Vonnegut's ""Shape of stories"" possible to model?",https://www.reddit.com/r/MachineLearning/comments/b5s8k4/is_kurt_vonneguts_shape_of_stories_possible_to/,klubgllrt,1553619692,,1,1,False,https://b.thumbs.redditmedia.com/8I0NYe5dxd-oitp_WAL2BAQblpjropGFZr3k_NNbJBc.jpg,,,,,
1614,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,2,b5sb1g,self.MachineLearning,What model to use?,https://www.reddit.com/r/MachineLearning/comments/b5sb1g/what_model_to_use/,fp60013,1553620015,"General ML question Ive been pondering.... 

1) what model might be good for classifying quantitative attributes?

And, even better...

2) what would one do if multiple quantitative data points were the attribute to be classified? Specifically 9 ranges (about 140 lines each) of values each represent one distinct attribute that should be classified into one of 3 classes. 

Thanks, crew! ",0,1,False,self,,,,,
1615,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,2,b5sc88,protoexpress.com,The Efficiency of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b5sc88/the_efficiency_of_machine_learning/,LucySierraCircuits,1553620178,,0,1,False,default,,,,,
1616,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,2,b5scem,self.MachineLearning,[D] State of the art in (video)-content detection/filtering and adversarial attacks against it,https://www.reddit.com/r/MachineLearning/comments/b5scem/d_state_of_the_art_in_videocontent/,ResidentAardvark,1553620201,"Hi all,

since today the European Union passed the (infamous) copyright directive containing Article 13, which may lead to the design of upload-filters for e.g., video-content, which I guess will be based on machine learning techniques (plus maybe some additional stuff), I was curious, how you would approach this problem, resp., what approaches you would expect Youtube et al to use, resp., what the current state-of-the-art for this kind of problem is (as I guess some automated filters are already in place stopping upload of e.g., porn on non-porn sites). 

Moreover, are there some general attack-techniques (i.e., modification of the content, so that an end-user does not notice any modification, but the filtering network will be fooled), when the filter is basically a black box, and querying it (i.e., getting back if your (modified) content is allowed or not) is very expensive.",11,16,False,self,,,,,
1617,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,2,b5sgwh,socialdribbler.com,"R is considered to be the best programming language for any statistician as it possesses an extensive catalog of statistical and graphical methods. Python on the other hand, can do pretty much the same work as R but.?",https://www.reddit.com/r/MachineLearning/comments/b5sgwh/r_is_considered_to_be_the_best_programming/,monkeyguy2017,1553620815,,0,1,False,https://b.thumbs.redditmedia.com/GzPrf2aF1S_lpcxDsNBDV6OR6As-xPEcfn5X_29GgjA.jpg,,,,,
1618,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,2,b5skpe,distill.pub,[R] Visualizing memorization in RNNs,https://www.reddit.com/r/MachineLearning/comments/b5skpe/r_visualizing_memorization_in_rnns/,Cock-tail,1553621346,,0,1,False,default,,,,,
1619,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,2,b5smay,self.MachineLearning,Bert Embeddings,https://www.reddit.com/r/MachineLearning/comments/b5smay/bert_embeddings/,kushalj001,1553621572,"How can I use bert embeddings in a way similar to word2vec and glove?  
",0,1,False,self,,,,,
1620,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,2,b5stgx,self.MachineLearning,Compression of Neural Networks,https://www.reddit.com/r/MachineLearning/comments/b5stgx/compression_of_neural_networks/,enry12,1553622567,"Hi everybody,

I'm going to start a PhD in Machine Learning very soon. Compression of NN is one of the possible argument, here there are some references: [Bagdanov et al.](https://arxiv.org/abs/1709.01041) and [Polino et al.](https://arxiv.org/abs/1802.05668).

In your opinion, is it a good topic to start a PhD? Or is it a dead branch of ML? Opinions? 

Thank you all for the viewpoints.",0,1,False,self,,,,,
1621,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,2,b5svhj,youtube.com,Reverse Image Search,https://www.reddit.com/r/MachineLearning/comments/b5svhj/reverse_image_search/,csharp_ai,1553622843,,0,1,False,https://b.thumbs.redditmedia.com/6tIrpSsc3gQl6zy9dRMI6EOmbJs8pMmN2BeCRnG3PpA.jpg,,,,,
1622,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,2,b5sxh0,self.MachineLearning,Advice on Undergrad Math Courses for ML Grad School,https://www.reddit.com/r/MachineLearning/comments/b5sxh0/advice_on_undergrad_math_courses_for_ml_grad/,Rootofallevi1,1553623114,[removed],0,1,False,self,,,,,
1623,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,3,b5t7kx,medium.com,AI Local Rainfall Forecasting Using Weather Radar Maps,https://www.reddit.com/r/MachineLearning/comments/b5t7kx/ai_local_rainfall_forecasting_using_weather_radar/,gwen0927,1553624409,,0,1,False,default,,,,,
1624,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,3,b5temt,ai.googleblog.com,Unifying Physics and Deep Learning with TossingBot,https://www.reddit.com/r/MachineLearning/comments/b5temt/unifying_physics_and_deep_learning_with_tossingbot/,sjoerdapp,1553625331,,0,1,False,default,,,,,
1625,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,3,b5tgt9,self.MachineLearning,[Discussion] Compression of Neural Networks,https://www.reddit.com/r/MachineLearning/comments/b5tgt9/discussion_compression_of_neural_networks/,enry12,1553625632,"Hi everybody,

&amp;#x200B;

I'm going to start a PhD in Machine Learning very soon. Compression of NN is one of the possible argument, here there are some references: \[Bagdanov et al.\]([https://arxiv.org/abs/1709.01041](https://arxiv.org/abs/1709.01041)) and \[Polino et al.\]([https://arxiv.org/abs/1802.05668](https://arxiv.org/abs/1802.05668)).

&amp;#x200B;

In your opinion, is it a good topic to start a PhD? Or is it a dead branch of ML? Opinions? 

&amp;#x200B;

Thank you all for the viewpoints.",35,35,False,self,,,,,
1626,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,4,b5tx9z,self.MachineLearning,Implementing Face and Landmark detection in Keras,https://www.reddit.com/r/MachineLearning/comments/b5tx9z/implementing_face_and_landmark_detection_in_keras/,the_best_scientist,1553627837,[removed],0,1,False,self,,,,,
1627,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,4,b5u31d,self.MachineLearning,How would you go about detecting walls or doors or rooms in a floor plan?,https://www.reddit.com/r/MachineLearning/comments/b5u31d/how_would_you_go_about_detecting_walls_or_doors/,lost_in__dream,1553628586,[removed],0,1,False,self,,,,,
1628,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,4,b5u3v3,self.MachineLearning,[D] Are ImageNet experiments needed for serious evaluation of a SOTA method?,https://www.reddit.com/r/MachineLearning/comments/b5u3v3/d_are_imagenet_experiments_needed_for_serious/,doctorjuice,1553628688,"We are developing a method that appears SOTA for CIFAR10, CIFAR100, SVHN etc. for a type of classification task for compressed networks and we want to submit it to NeurIPS. However, I really dont want to pay for 7+ days of compute time and am wondering whether our method will be less seriously considered if we dont do imagenet experiments.

(FYI, for this task, most papers did not do imagenet except for a very recent one from google a month ago that did large scale experiments on imagenet and established a baseline)",19,8,False,self,,,,,
1629,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,4,b5u4vo,self.MachineLearning,[P] Image Super Resolution: Super-scale your images and run experiments very easily with RDNs and GANs.,https://www.reddit.com/r/MachineLearning/comments/b5u4vo/p_image_super_resolution_superscale_your_images/,datitran,1553628821,"We just released a major update of our [Keras based image super resolution project](https://github.com/idealo/image-super-resolution/) . Now you can super-scale your images and run experiments very easily with RDNs and GANs.

&amp;#x200B;

In particular,

 We've added some new cool stuff like VGG deep features + GANs to achieve amazingly realistic upscaled images

 We provide a few more pre-trained weights as well as some Colab-notebook tutorials to play around

 And the best thing our project is now available on PyPI! So you can just pip install your way to our project

&amp;#x200B;

Blog: [https://medium.com/idealo-tech-blog/zoom-in-enhance-a-deep-learning-based-magnifying-glass-part-2-c021f98ebede](https://medium.com/idealo-tech-blog/zoom-in-enhance-a-deep-learning-based-magnifying-glass-part-2-c021f98ebede)

Code: [https://github.com/idealo/image-super-resolution/](https://github.com/idealo/image-super-resolution/)

Documentation: [https://idealo.github.io/image-super-resolution/](https://idealo.github.io/image-super-resolution/)

Colab (prediction): [https://colab.research.google.com/github/idealo/image-super-resolution/blob/master/notebooks/ISR\_Prediction\_Tutorial.ipynb](https://colab.research.google.com/github/idealo/image-super-resolution/blob/master/notebooks/ISR_Prediction_Tutorial.ipynb)

Colab (training): [https://colab.research.google.com/github/idealo/image-super-resolution/blob/master/notebooks/ISR\_Traininig\_Tutorial.ipynb](https://colab.research.google.com/github/idealo/image-super-resolution/blob/master/notebooks/ISR_Traininig_Tutorial.ipynb)",17,208,False,self,,,,,
1630,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,4,b5u5d3,self.MachineLearning,[P] Projecting reddit comments onto a globe,https://www.reddit.com/r/MachineLearning/comments/b5u5d3/p_projecting_reddit_comments_onto_a_globe/,zbyte64,1553628883,"Wanted to try my hand at a Siamese network. Created a triplet dataset from Reddit comments where the comparative distance is based off responses sorted by karma. For fun I used great circle instead of euclidean distance. This allowed me to write the results as geojson which can be viewed with github's mapbox.

https://github.com/zbyte64/pytorch-reddit-distance/tree/master/results
",1,10,False,self,,,,,
1631,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,5,b5uow7,self.MachineLearning,Detecting Spun Content using ML,https://www.reddit.com/r/MachineLearning/comments/b5uow7/detecting_spun_content_using_ml/,piscoster,1553631495,[removed],0,1,False,self,,,,,
1632,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,5,b5upm0,aisoma.de,[D] 10 Statistical Techniques Data Scientists Should Master,https://www.reddit.com/r/MachineLearning/comments/b5upm0/d_10_statistical_techniques_data_scientists/,seemingly_omniscient,1553631590,,0,1,False,default,,,,,
1633,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,5,b5ux6c,self.MachineLearning,[D] 10 Statistical Techniques Data Scientists Should Master,https://www.reddit.com/r/MachineLearning/comments/b5ux6c/d_10_statistical_techniques_data_scientists/,seemingly_omniscient,1553632620," The more statistical techniques a Data Scientist has mastered, the better the results can be. In this blog article, we want to introduce you to ten common techniques that should not be missing in the repertoire of a data scientist.  A brief overview.

&amp;#x200B;

Article: [https://www.aisoma.de/10-statistical-techniques/](https://www.aisoma.de/10-statistical-techniques/)

&amp;#x200B;

https://i.redd.it/8e93eefvxio21.jpg

&amp;#x200B;

&amp;#x200B;",4,0,False,https://b.thumbs.redditmedia.com/Ovbmdd6AJz-g1HF2iJd-nItIstYz5IPy_zRFv9ys4oY.jpg,,,,,
1634,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,5,b5v19d,self.MachineLearning,Relevance of Meta-heuristic Optimization in Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/b5v19d/relevance_of_metaheuristic_optimization_in/,actuallynotcanadian,1553633168,"Dear r/MachineLearning. I am a math graduate who became interested after my Master's into making research in Machine Learning. I was promised a PhD, the opportunity to do cutting-edge research, real world applications and a close cooperation with industrial research partners. But after having spent a few months reading and discussing with supervisors, I am not sure if they were honest with me to begin with. A lot of work I am considered to do is centered around metaheuristic search and evolutionary computation. And although, I find it fascinating and there is some application to machine learning / DNNs, as well as companies like Uber, Sentient and recently Cognizant are adopting it, I feel like it has too much of a niche quality and mainstream interest seems not to be catching-up with it, in case if there is any at all to begin with. I thought it might be helpful to ask you guys, to get a neutral outside-of-the-box opinion. Particularly, because my prior background is not AI/ML or Computer Science to begin with and I have troubles evaluating the claims/veracity of my supervisors. ",0,1,False,self,,,,,
1635,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,5,b5v3kc,self.MachineLearning,When to avoid xgboost on tabular data?,https://www.reddit.com/r/MachineLearning/comments/b5v3kc/when_to_avoid_xgboost_on_tabular_data/,elnesss,1553633474,"I am doing some research on the situations where different models perform well or not. I realize that xgboost is an implementation but for now this is irrelevant. After some time searching I don't think I am any closer to understanding some situation where xgboost might not be a good idea with a tabular labeled data problem, in terms of accuracy. I can only really think that it is overkill when the interaction between variables is non existent. Any hints? 

&amp;#x200B;

Also if you happen to know a good resource for my ongoing google adventure into understanding more specific scenarios where models work better/worse please do share. Thanks!",0,1,False,self,,,,,
1636,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,6,b5vfgm,self.MachineLearning,[D] Which hosting? Python + ML + celery + rabbitMQ + mysql,https://www.reddit.com/r/MachineLearning/comments/b5vfgm/d_which_hosting_python_ml_celery_rabbitmq_mysql/,s1korrrr,1553635061,"Hi Everyone.

&amp;#x200B;

So I made a little scrapper that is scrapping a house prices with some features like area, city, sq meters, ratio = price / m2 etc. everything nice and tidy are saved to the mysql database. I want to attach celery and rabbitMQ for task queue (maybe overkill, but I want to learn),

I want to use machine learning for price prediction or something that is not relevant for this moment. But it will be something simple...

End product will be a dashboard that will show some basic statistics or the scrapped houses, heatmap etc.

Then I want to do in Flask environment, and here is some problems...

 \- Should I use Flask? it should be lightweight and fast 

 \- Machine Learning models as endpoints?

 \- **Which hosting is the best for this kind of problem? Heroku or Amazon? or... ?**

I hope I write everything clear, if something is not clear please, I will try to explain this.

Thank you in advance for your help",16,2,False,self,,,,,
1637,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,6,b5vvdu,openai.com,[N] OpenAI Five Finals,https://www.reddit.com/r/MachineLearning/comments/b5vvdu/n_openai_five_finals/,sherjilozair,1553637264,,0,1,False,https://b.thumbs.redditmedia.com/0tdswmm8kZ94Ucgdkf6YaAJa3DnHuDMray_FmxePY3o.jpg,,,,,
1638,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,7,b5vz1b,arxiv.org,SLIDE : In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems,https://www.reddit.com/r/MachineLearning/comments/b5vz1b/slide_in_defense_of_smart_algorithms_over/,walkingsparrow,1553637754,,13,13,False,default,,,,,
1639,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,7,b5vz41,self.MachineLearning,"[P] A compilation of ""This * Does Not Exist"" websites",https://www.reddit.com/r/MachineLearning/comments/b5vz41/p_a_compilation_of_this_does_not_exist_websites/,tdls_to,1553637765,"Let me know if I've missed anything.

[https://github.com/Aggregate-Intellect/awesome-does-not-exist](https://github.com/Aggregate-Intellect/awesome-does-not-exist)

&amp;#x200B;",4,8,False,self,,,,,
1640,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,7,b5w829,arxiv.org,[R] Investigating Recurrent Neural Network Memory Structures using Neuro-Evolution,https://www.reddit.com/r/MachineLearning/comments/b5w829/r_investigating_recurrent_neural_network_memory/,Vystril,1553639014,,6,3,False,default,,,,,
1641,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,7,b5wjxo,self.MachineLearning,Text sentiment assistance,https://www.reddit.com/r/MachineLearning/comments/b5wjxo/text_sentiment_assistance/,pirating,1553640691,"I have a side project where I'm taking classical music concerts, self reported levels of audience satisfaction (extremely dissatisfied, dissatisfied, satisfied, very satisfied), date of concert, location, how they learned about it, event type, and their comments and playing around with it to find some trends/insight.

Most of the text sentiment stuff in python that I've seen revolves around gleaning if the feedback is positive or negative, but I already have that value.

Are there any toolkits and librarys that would be helpful in digesting this more? Almost all of my work has been quantitative, so I'm using this to learn the more ""subjective"" applications.",0,1,False,self,,,,,
1642,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,7,b5wkfe,self.MachineLearning,Libraries that support OpenImages well?,https://www.reddit.com/r/MachineLearning/comments/b5wkfe/libraries_that_support_openimages_well/,mjw316,1553640768,[removed],0,1,False,self,,,,,
1643,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,8,b5wytx,marktechpost.com,Artificial Intelligence  A Bitter-Sweet Symphony in Modelling,https://www.reddit.com/r/MachineLearning/comments/b5wytx/artificial_intelligence_a_bittersweet_symphony_in/,ai-lover,1553642925,,0,1,False,default,,,,,
1644,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,9,b5xiln,self.MachineLearning,[D] Design Principles for Machine Learning Infrastructure Platforms,https://www.reddit.com/r/MachineLearning/comments/b5xiln/d_design_principles_for_machine_learning/,ospillinger,1553645964,"Hi all,

I [recently posted](https://www.reddit.com/r/MachineLearning/comments/av7o14) here about an open source machine learning infrastructure platform that my team and I are working on. I got some great questions and feedback from the community so I wanted to share an article that I just wrote about the ideas that informed our design.

Id love to hear your feedback!

[https://medium.com/cortex-labs/design-principles-for-machine-learning-infrastructure-platforms-cde7d7910bd5](https://medium.com/cortex-labs/design-principles-for-machine-learning-infrastructure-platforms-cde7d7910bd5)",4,5,False,self,,,,,
1645,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,9,b5xt06,self.MachineLearning,Machine Learning Internship Interview,https://www.reddit.com/r/MachineLearning/comments/b5xt06/machine_learning_internship_interview/,leftail,1553647550,[removed],0,1,False,self,,,,,
1646,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,9,b5xxm8,self.MachineLearning,At what age do I need to start learning?,https://www.reddit.com/r/MachineLearning/comments/b5xxm8/at_what_age_do_i_need_to_start_learning/,Cojack622,1553648248,[removed],0,1,False,self,,,,,
1647,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,10,b5yab4,/r/MachineLearning/comments/b5yab4/p_deep_pensieve_an_expansive_deep_residual_in/,"[P] Deep Pensieve: An Expansive Deep Residual in Residual Attention Information Maximizing Variational Auto-Encoder ... yes, for real :)",https://www.reddit.com/r/MachineLearning/comments/b5yab4/p_deep_pensieve_an_expansive_deep_residual_in/,neurokinetikz,1553650256,,1,1,False,https://b.thumbs.redditmedia.com/jVc8OgBcp5KE2jJ9uz3gaPYYXBNSS5U4q6D1N5vyWtQ.jpg,,,,,
1648,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,10,b5yier,self.MachineLearning,[Q] Using Autoencoder For Fraud Detection - Setting Threshold,https://www.reddit.com/r/MachineLearning/comments/b5yier/q_using_autoencoder_for_fraud_detection_setting/,DaTacularHB,1553651590,[removed],0,1,False,self,,,,,
1649,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,11,b5ym9e,self.MachineLearning,[CVPR'19] Bounding Box Regression with Uncertainty for Accurate Object Detection,https://www.reddit.com/r/MachineLearning/comments/b5ym9e/cvpr19_bounding_box_regression_with_uncertainty/,ikkiho,1553652216,[removed],0,1,False,self,,,,,
1650,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,11,b5yn75,self.MachineLearning,Driverbase.com Launches AI Powered Car Search to Connect Drivers with Their Optimal Vehicle,https://www.reddit.com/r/MachineLearning/comments/b5yn75/driverbasecom_launches_ai_powered_car_search_to/,driverbase,1553652370,"We just launched a new car search website that provides vehicle recommendations personalized for your unique situation.

Feel free to try it out. Should make it easier to decide what car to buy. Every time a new user joins the recommendations get better for everyone.

[https://driverbase.com/user/register](https://driverbase.com/user/register)

Any feedback is greatly appreciated. Thanks for considering,

Driverbase.com",0,1,False,self,,,,,
1651,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,11,b5yx9v,arxiv.org,[R] Reconciling modern machine learning and the bias-variance trade-off,https://www.reddit.com/r/MachineLearning/comments/b5yx9v/r_reconciling_modern_machine_learning_and_the/,blowjobtransistor,1553654042,,5,13,False,default,,,,,
1652,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,11,b5yznm,i.redd.it,Variational Autoencoder in Unity,https://www.reddit.com/r/MachineLearning/comments/b5yznm/variational_autoencoder_in_unity/,professormunchies,1553654432,,1,1,False,https://b.thumbs.redditmedia.com/gC1KtB-2M0pAWKijg77bAcVrBm_MLz1Iudl5BO-bF3Q.jpg,,,,,
1653,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,13,b5zwzs,self.MachineLearning,Splunk Use Case,https://www.reddit.com/r/MachineLearning/comments/b5zwzs/splunk_use_case/,Positka,1553660403,[removed],0,1,False,self,,,,,
1654,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,14,b60edf,self.MachineLearning,[R] Join our Live Lunch &amp; Learn: Graph Convolutional Neural Network (Wed March 27 12:00 EST),https://www.reddit.com/r/MachineLearning/comments/b60edf/r_join_our_live_lunch_learn_graph_convolutional/,tdls_to,1553663765,"Live stream &amp; paper: 

[https://aisc.a-i.science/events/2019-03-27/](https://aisc.a-i.science/events/2019-03-27/)",1,5,False,self,,,,,
1655,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,14,b60isv,mashable.com,McDonald's wants to use machine learning to serve fast food,https://www.reddit.com/r/MachineLearning/comments/b60isv/mcdonalds_wants_to_use_machine_learning_to_serve/,Malhar_S,1553664670,,0,1,False,default,,,,,
1656,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,14,b60jtg,self.MachineLearning,[P] The Illustrated Word2vec,https://www.reddit.com/r/MachineLearning/comments/b60jtg/p_the_illustrated_word2vec/,nortab,1553664877,"Hi r/MachineLearning,

I wrote a blog post attempting to visually explain the mechanics of word2vec's skipgram with negative sampling algorithm (SGNS). It's motivated by:

1- The need to develop more visual language around embedding algorithms.

2- The need for a gentle on-ramp to SGNS for people who are using it for recommender systems. A use-case I find very interesting (there are links in the post to such applications)

 I'm hoping it could also be useful if you wanted to explain to someone new to the field the value of vector representations of things. Hope you enjoy it. All feedback is appreciated!

[https://jalammar.github.io/illustrated-word2vec/](https://jalammar.github.io/illustrated-word2vec/)",21,152,False,self,,,,,
1657,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,14,b60m4f,self.MachineLearning,AI solves UX/UI design,https://www.reddit.com/r/MachineLearning/comments/b60m4f/ai_solves_uxui_design/,hussy456,1553665355,[removed],0,1,False,self,,,,,
1658,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,14,b60n6m,i.redd.it,[R] Photorealistic Style Transfer via Wavelet Transforms,https://www.reddit.com/r/MachineLearning/comments/b60n6m/r_photorealistic_style_transfer_via_wavelet/,JaejunYoo,1553665561,,0,1,False,default,,,,,
1659,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,14,b60p3i,cetpainfotech.com,5 Problems Everyone Has With Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b60p3i/5_problems_everyone_has_with_machine_learning/,Divya123divya,1553665955,,0,1,False,default,,,,,
1660,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,14,b60qdh,self.MachineLearning,Assignmment regarding event extraction using NLP,https://www.reddit.com/r/MachineLearning/comments/b60qdh/assignmment_regarding_event_extraction_using_nlp/,curioussouya,1553666228,"I have been provided with an assignment for NLP, requires NLP. Please help me regarding the same. ",0,1,False,self,,,,,
1661,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,15,b60t5e,self.MachineLearning,[P] test,https://www.reddit.com/r/MachineLearning/comments/b60t5e/p_test/,taki0112,1553666817,test,0,1,False,self,,,,,
1662,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,15,b60t8h,self.MachineLearning,[P] Pre-trained GANs in pytorch and accompanying code,https://www.reddit.com/r/MachineLearning/comments/b60t8h/p_pretrained_gans_in_pytorch_and_accompanying_code/,csinva,1553666833,"Link: [https://github.com/csinva/pytorch\_gan\_pretrained](https://github.com/csinva/pytorch_gan_pretrained)

Looked for a while and couldn't find pre-trained GANs in pytorch for mnist/cifar so I trained my own. Code contains training scripts and notebooks showing how to load/use the models.

Hope this helps someone!

https://i.redd.it/7lbwcx0irlo21.png",0,1,False,https://b.thumbs.redditmedia.com/er9_EMhVMFLXMkDsU079gdhFWNlwAYAU5JdavOwqTLs.jpg,,,,,
1663,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,15,b60tx8,self.MachineLearning,[R] test,https://www.reddit.com/r/MachineLearning/comments/b60tx8/r_test/,JaejunYoo,1553666972,test,0,0,False,self,,,,,
1664,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,15,b60u45,github.com,[P] sss,https://www.reddit.com/r/MachineLearning/comments/b60u45/p_sss/,taki0112,1553667012,,0,1,False,default,,,,,
1665,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,15,b60x06,self.MachineLearning,[R] [1903.09760] Photorealistic Style Transfer via Wavelet Transforms,https://www.reddit.com/r/MachineLearning/comments/b60x06/r_190309760_photorealistic_style_transfer_via/,JaejunYoo,1553667626,"paper: https://arxiv.org/abs/1903.09760
code: https://github.com/clovaai/WCT2

**Abstract**
&gt; Recent style transfer models have provided promising artistic results. However, given a photograph as a reference style, existing methods are limited by spatial distortions or unrealistic artifacts, which should not happen in real photographs. We introduce a theoretically sound correction to the network architecture that remarkably enhances photorealism and faithfully transfers the style. The key ingredient of our method is wavelet transforms that naturally fits in deep networks. We propose a wavelet corrected transfer based on whitening and coloring transforms (WCT2) that allows features to preserve their structural information and statistical properties of VGG feature space during stylization. This is the first and the only end-to-end model that can stylize 10241024 resolution image in 4.7 seconds, giving a pleasing and photorealistic quality without any post-processing. Last but not least, our model provides a stable video stylization without temporal constraints. ",26,49,False,self,,,,,
1666,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,15,b60zk3,datafloq.com,The Role of AI &amp; ML in Network Monitoring,https://www.reddit.com/r/MachineLearning/comments/b60zk3/the_role_of_ai_ml_in_network_monitoring/,ayushiitwomen,1553668152,,0,1,False,default,,,,,
1667,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,15,b613rf,self.MachineLearning,Pre-trained GANs in pytorch,https://www.reddit.com/r/MachineLearning/comments/b613rf/pretrained_gans_in_pytorch/,csinva,1553669037,[removed],0,1,False,https://b.thumbs.redditmedia.com/5a1GVTv_lAFo_pQefqmRCNJoRfcO3k5w4qY-Jk99EkM.jpg,,,,,
1668,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,15,b6143p,self.MachineLearning,Help with implementing stride in Huggingface Multi-label BERT classifier,https://www.reddit.com/r/MachineLearning/comments/b6143p/help_with_implementing_stride_in_huggingface/,Chronoiokrator,1553669107,[removed],0,1,False,self,,,,,
1669,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,15,b617br,self.MachineLearning,[P] Pre-trained GANs in pytorch,https://www.reddit.com/r/MachineLearning/comments/b617br/p_pretrained_gans_in_pytorch/,csinva,1553669762,"Looked for a while and couldn't find pre-trained GANs in [pytorch](https://pytorch.org/) for mnist/cifar so I trained my own: [Link](https://github.com/csinva/pytorch_gan_pretrained).

Code contains training scripts and notebooks showing how to load/use the models. Hope this helps someone!

https://i.redd.it/igvx6ybeylo21.png",0,3,False,https://b.thumbs.redditmedia.com/HifRQ4UGX8eua73aK3eBJmd08lqMzHg3TkVJCKyBxNk.jpg,,,,,
1670,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,15,b6181p,self.MachineLearning,[D] Does KDD have author response period?,https://www.reddit.com/r/MachineLearning/comments/b6181p/d_does_kdd_have_author_response_period/,causal_convolution,1553669922,,12,0,False,self,,,,,
1671,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,16,b61bpg,self.MachineLearning,"Machine Learning as a Service (Mlaas) Market - Size, Outlook, Trends and Forecasts (2018  2024)",https://www.reddit.com/r/MachineLearning/comments/b61bpg/machine_learning_as_a_service_mlaas_market_size/,fedupsession,1553670687,[removed],0,1,False,self,,,,,
1672,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,16,b61fdv,self.MachineLearning,AskML: Orthogonality and ICA,https://www.reddit.com/r/MachineLearning/comments/b61fdv/askml_orthogonality_and_ica/,TheNeuroscienceMan,1553671480,[removed],0,1,False,self,,,,,
1673,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,16,b61iew,self.MachineLearning,Scaffolding Market Analysis Reveals unstable development by 2023,https://www.reddit.com/r/MachineLearning/comments/b61iew/scaffolding_market_analysis_reveals_unstable/,Shivs7,1553672164,[removed],1,1,False,self,,,,,
1674,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,16,b61j7p,sytoss.com,Image Recognition: Can an Image Recognition App Become the Quality Boost Your Business Needs?,https://www.reddit.com/r/MachineLearning/comments/b61j7p/image_recognition_can_an_image_recognition_app/,sytoss,1553672362,,0,1,False,default,,,,,
1675,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,16,b61lre,self.MachineLearning,How to convince your manager to have faith in adopting ML solutions?,https://www.reddit.com/r/MachineLearning/comments/b61lre/how_to_convince_your_manager_to_have_faith_in/,FarisAi,1553672930,[removed],0,1,False,self,,,,,
1676,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,17,b61phm,self.MachineLearning,[D] TossingBot: Learning to Throw Arbitrary Objects with Residual Physics (Google),https://www.reddit.com/r/MachineLearning/comments/b61phm/d_tossingbot_learning_to_throw_arbitrary_objects/,milaworld,1553673808,"New paper from work done by interns at Google who trained a grasping robot to pick up objects and toss them into a moving bin.

Author's web site with videos and link to the paper: https://tossingbot.cs.princeton.edu

Abstract:

*We investigate whether a robot arm can learn to pick and throw arbitrary objects into selected boxes quickly and accurately. Throwing has the potential to increase the physical reachability and picking speed of a robot arm. However, precisely throwing arbitrary objects in unstructured settings presents many challenges: from acquiring reliable pre-throw conditions (e.g. initial pose of object in manipulator) to handling varying object- centric properties (e.g. mass distribution, friction, shape) and dynamics (e.g. aerodynamics). In this work, we propose an end- to-end formulation that jointly learns to infer control param- eters for grasping and throwing motion primitives from visual observations (images of arbitrary objects in a bin) through trial and error. Within this formulation, we investigate the synergies between grasping and throwing (i.e., learning grasps that enable more accurate throws) and between simulation and deep learning (i.e., using deep networks to predict residuals on top of control parameters predicted by a physics simulator). The resulting system, TossingBot, is able to grasp and throw arbitrary objects into boxes located outside its maximum reach range at 500+ mean picks per hour (600+ grasps per hour with 85% throwing accuracy); and generalizes to new objects and target locations.*

https://tossingbot.cs.princeton.edu",1,10,False,self,,,,,
1677,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,17,b61pvd,self.MachineLearning,Undergraduate Dissertation,https://www.reddit.com/r/MachineLearning/comments/b61pvd/undergraduate_dissertation/,__elon,1553673898,[removed],0,1,False,self,,,,,
1678,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,17,b61qw1,self.MachineLearning,[D] How to convince your manager to have faith in adopting ML solutions?,https://www.reddit.com/r/MachineLearning/comments/b61qw1/d_how_to_convince_your_manager_to_have_faith_in/,FarisAi,1553674126,"# Is model worthiness relays only in the ROI?

&amp;#x200B;

As the title suggests it is a real fact that not everyone has faith in machine learning and regardless to the reasons behind this scepticism. What I want to have here is a discussion and sharing your experience in how to handle such situations.

&amp;#x200B;

**Brief history:**

I work as a data scientist in a platform that includes e-commerce and Fintech solutions. There is a need to have a recommendation engine to increase ROI and exposure. The current system is not personalising the user experience, it basically use user location to recommend products and services. It has been around for sometime and the numbers (ROI) is awfully low.

I made it my quest to push for a personalisation approach and built a collaborative filtering (item-item) recommendation engine. The system is out and is running well, in fact I am few days from my system goes to beta-testing. The managers in the company are considering this as an EXPERIMENT. So it is subject to passing KPIs to convince them that it's worthy to be added as a feature.

&amp;#x200B;

**Dilemma:**

As a data scientist I have faith in ML regardless if my RS (Recommendation System) made better ROI or not. As a matter of fact I would like to improve my model and try to build a model that has less anomalies reducing the issues with the first model and check how the new model performs in terms of ROI.

&amp;#x200B;

**Questions:**

1. Is it right to think of models worthiness in terms of ROI only? What other ways to evaluate model's business value?
2. If my model to perform poorly in terms of ROI managers will have strong reasons why ML ""doesn't fit our use-case"", how I can manage this scenario? How I can get them to let me give it another shot with something better robust hybrid models or something in that sense.

&amp;#x200B;

**Looking forward to read your opinions and insights.**

&amp;#x200B;

Thank you.",22,3,False,self,,,,,
1679,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,17,b61ux6,self.MachineLearning,"how to apply bayesien approach ""MC dropout"" to an LSTM network",https://www.reddit.com/r/MachineLearning/comments/b61ux6/how_to_apply_bayesien_approach_mc_dropout_to_an/,IAlover,1553675072,[removed],0,1,False,self,,,,,
1680,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,17,b620ie,self.MachineLearning,[D] Classification/scoring without a dataset,https://www.reddit.com/r/MachineLearning/comments/b620ie/d_classificationscoring_without_a_dataset/,drakerc360,1553676415,"Hi,

I need to create a system that helps with classification/scoring. It will be used to classify if a certain object might be infringing patents based on certain features, such as the country of origin, price, available quantity, seller reviews, etc.
Right now, the problem is that we don't have any dataset, so we can't really use a standard Tensorflow/Keras approach.

The idea was to create a system that calculates the score based on some hard-coded values for each feature based on my own ""expertise"". For example:

Country of origin:
Germany - 0.05
USA - 0.1
China - 1.0

Price:
[0-$10] - 1.0
[$10-$50] - 0.6
[$50-] - 0.0

Available quantity:
[0-10] - 0.0
[10-50] - 0.1
[50-300] - 1.0

I'd calculate the score by summing the partial scores/weights of features and then dividing it by the maximum score possible.
Let's say that the product is from China, the price if $5 and the quantity is 100. In that case it's 2.6/3.0, so 0.87, which is quite close to 1.0 which is the max for a ""infringement"" classification.

That would work fine for now. However, I'm really not sure if this is the proper way to do it nowadays.

First of all: implementation. Are there any libraries which could help me do it in Python? 
My idea was to store the ""scores/weights"" for each feature in a database. Then, it'd be a lot of functions that would prepare the object's features and compare it with the stored scores in the database and return the score. But maybe there's some kind of a library for that purpose which could make the development faster?

Second of all: maybe I should just use Keras/Tensorflow for it? I have some experience with Keras, but I always had a dataset. I've googled a bit and it looks like it's possible to set weights manually in Keras, so maybe this would do the trick? We'd manually set the weights for the first time and then when we'll build a proper dataset (thanks to this ""scoring"" system), we'll feed it to the model to learn and adjust the weights.",1,0,False,self,,,,,
1681,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,18,b6288s,self.MachineLearning,How to make your MLflow projects easy to share and collaborate on.,https://www.reddit.com/r/MachineLearning/comments/b6288s/how_to_make_your_mlflow_projects_easy_to_share/,ai_yoda,1553678137,[removed],0,1,False,self,,,,,
1682,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,18,b62ib6,telescopicfork.blogspot.com,"LHD S.p.A. develops, manufactures and markets load handling solutions.",https://www.reddit.com/r/MachineLearning/comments/b62ib6/lhd_spa_develops_manufactures_and_markets_load/,lhd121,1553680345,,0,1,False,default,,,,,
1683,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,19,b62uxf,thecoderr.com,Theory of Intelligence,https://www.reddit.com/r/MachineLearning/comments/b62uxf/theory_of_intelligence/,the_coder_dot_py,1553682998,,0,1,False,default,,,,,
1684,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,19,b631qi,nytimes.com,"Yann LeCun, Geoffrey Hinton and Yoshua Bengio win 2018's Turing Award.",https://www.reddit.com/r/MachineLearning/comments/b631qi/yann_lecun_geoffrey_hinton_and_yoshua_bengio_win/,sudoankit,1553684355,,0,1,False,default,,,,,
1685,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,19,b631vs,self.MachineLearning,NLP papers,https://www.reddit.com/r/MachineLearning/comments/b631vs/nlp_papers/,rakeshpar,1553684378,[removed],0,1,False,self,,,,,
1686,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,19,b631y4,awards.acm.org,"[N] Hinton, LeCun, and Bengio receive the 2018 ACM A.M. Turing Award",https://www.reddit.com/r/MachineLearning/comments/b631y4/n_hinton_lecun_and_bengio_receive_the_2018_acm_am/,Reiinakano,1553684388,,0,1,False,default,,,,,
1687,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,20,b6361q,youtube.com,"This video goes over a model that predicts the number of views on a youtube video based on likes, dislikes, and subscribers. Really interesting and educational. Do check it out",https://www.reddit.com/r/MachineLearning/comments/b6361q/this_video_goes_over_a_model_that_predicts_the/,antaloaalonso,1553685135,,0,1,False,https://b.thumbs.redditmedia.com/5jjO2lkQVL7ColtqLT4ZeJoZvNLKXRVa9BgFVt-_50Y.jpg,,,,,
1688,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,20,b6398n,self.MachineLearning,Recommended books for learning Minimax basics?,https://www.reddit.com/r/MachineLearning/comments/b6398n/recommended_books_for_learning_minimax_basics/,nlnlnl86,1553685725,[removed],0,1,False,self,,,,,
1689,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,20,b63az1,self.MachineLearning,[D] Multi-label training sets and conditional probabilities,https://www.reddit.com/r/MachineLearning/comments/b63az1/d_multilabel_training_sets_and_conditional/,AmusingAutomatons,1553686052,"Hi Everyone,

For the case of a training set where you're classifying a sample into multiple simultaneous categories and there are very clear properties of sample(s) that will intrinsically lead to multiple categories being flagged concurrently (e.g. image of clothing --&gt; {dress, red, expensive, etc}), can anyone clarify the following:

1) does a traditional FF or CNN deep network and their non-linearity handle these problems sufficiently or is there a special class of networks (or algorithms) that are better suited for multi-label ML problems?

2) What is the typical accuracy obtained from these algorithms? I've received some vague implications from tight-lipped colleagues that multi-label classification suffers from low accuracy (is this an active area of research?)

Many thanks.",0,1,False,self,,,,,
1690,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,20,b63c0j,medium.com,[N] Turing Award goes to Deep Learning Pioneers,https://www.reddit.com/r/MachineLearning/comments/b63c0j/n_turing_award_goes_to_deep_learning_pioneers/,omarsar,1553686249,,0,1,False,default,,,,,
1691,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,20,b63l98,self.MachineLearning,"[N] Hinton, LeCun, Bengio receive ACM Turing Award",https://www.reddit.com/r/MachineLearning/comments/b63l98/n_hinton_lecun_bengio_receive_acm_turing_award/,inarrears,1553687936,"According to [NYTimes](https://www.nytimes.com/2019/03/27/technology/turing-award-hinton-lecun-bengio.html) and [ACM website](https://awards.acm.org/about/2018-turing): *Yoshua Bengio, Geoffrey Hinton and Yann LeCun, the fathers of deep learning, receive the ACM Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing today.*",177,665,False,self,,,,,
1692,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,21,b63mx9,self.MachineLearning,Deep learning scripts,https://www.reddit.com/r/MachineLearning/comments/b63mx9/deep_learning_scripts/,reedgun,1553688216,[removed],0,1,False,self,,,,,
1693,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,21,b63o1q,self.MachineLearning,"I have a long list of URLs for my favourite porn videos, &lt; 95% pornhub. Is there a lay person friendly tool that can tell me what 'genres' or kinks I have, or patterns in my taste?",https://www.reddit.com/r/MachineLearning/comments/b63o1q/i_have_a_long_list_of_urls_for_my_favourite_porn/,HumaneSlaughterAus,1553688408,[removed],0,1,False,self,,,,,
1694,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,21,b63vgv,self.MachineLearning,[P] Offline virtual assistant with supervised ML,https://www.reddit.com/r/MachineLearning/comments/b63vgv/p_offline_virtual_assistant_with_supervised_ml/,emileferreira_,1553689701,[removed],0,1,False,self,,,,,
1695,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,21,b63xj6,secure.jbs.elsevierhealth.com,[D] A systematic review shows no performance benefit of machine learning over logistic regression for clinical prediction models,https://www.reddit.com/r/MachineLearning/comments/b63xj6/d_a_systematic_review_shows_no_performance/,_quanttrader_,1553690046,,0,1,False,default,,,,,
1696,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,21,b642am,self.MachineLearning,Why to measure GCN on citation networks?,https://www.reddit.com/r/MachineLearning/comments/b642am/why_to_measure_gcn_on_citation_networks/,tinyRockstar,1553690852,[removed],0,1,False,self,,,,,
1697,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,22,b64dn5,arxiv.org,"[1903.10187] Designing Normative Theories of Ethical Reasoning: Formal Framework, Methodology, and Tool Support",https://www.reddit.com/r/MachineLearning/comments/b64dn5/190310187_designing_normative_theories_of_ethical/,ihaphleas,1553692650,,1,0,False,default,,,,,
1698,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,22,b64ows,artnome.com,Why Is AI Art Copyright So Complicated?,https://www.reddit.com/r/MachineLearning/comments/b64ows/why_is_ai_art_copyright_so_complicated/,hoopism,1553694446,,0,1,False,https://b.thumbs.redditmedia.com/68aORMZjolflWb0_RwROga-pzJewhepYFWLyYW2Lj5I.jpg,,,,,
1699,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,23,b64x2k,self.MachineLearning,[D] need advice w.r.t. my position,https://www.reddit.com/r/MachineLearning/comments/b64x2k/d_need_advice_wrt_my_position/,EZretr0,1553695656,"Hi reddit,

I need some help on my situation. I started my PhD a while ago with a very well known professor in ML. I submitted 3 papers (2 of which Im 1st author) in 1 semester and have been working in the lab. The thing is that he does not pay me (Im TA) and expects results! I dont have a computer in the lab as he doesnt believe in NN and the need for GPUs, I simply cannot do any experiments, for the past experiments I used my PC at my last university. Also my spot is the worst spot in the lab, too much noise and hot air blowing to my face :|
Do you suggest to change my supervisor for good? or wait for things to get better? (3-4 students are graduating) or talk to him about these problems?

Thanks",4,2,False,self,,,,,
1700,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,23,b65678,self.MachineLearning,[P] I hooked up OpenAI's gpt-2 to reddit's API for fun and had it respond to random writing prompts and ask reddit questions with ... interesting ... results.,https://www.reddit.com/r/MachineLearning/comments/b65678/p_i_hooked_up_openais_gpt2_to_reddits_api_for_fun/,deep_replier_bot,1553697193,"GPT2 is [openai's text generator](https://github.com/openai/gpt-2) based on a transformer network. This is the generator that was hyped as being ""too dangerous for the public"" so they released a much smaller trained version of the model on their github. The model was trained using news articles and random links from the internet.

For fun, I just took their model, and fed it random text from reddit (on /r/AskReddit, /r/WritingPrompts, /r/AskScience, /r/copypasta, /r/AskMen, /r/riddles, and /r/ShittyAskScience) to condition its output. I sandboxed its generated responses in a subreddit [here](https://reddit.com/r/DeepReplies/) after it got (correctly) banned for directly responding to stuff.

I've found that it can sometimes generate coherent on-topic text, but sometimes falls into hilarious repetition or reciting random web pages. It also of course sometimes generates offensive content.

One of the most interesting things that this model does is that when prompted with two characters having a dialouge, it generates a script which more or less sticks to the two characters. It also seems to be able to generate scientific-sounding articles and will append random fake citations to the end of them.",6,22,False,self,,,,,
1701,MachineLearning,t5_2r3gv,2019-3-27,2019,3,27,23,b65a9z,self.MachineLearning,[D] A Practical Guide To Hyperparameter Optimization,https://www.reddit.com/r/MachineLearning/comments/b65a9z/d_a_practical_guide_to_hyperparameter_optimization/,iyaja,1553697931,"Hi everyone on r/MachineLearning.

&amp;#x200B;

I recently wrote an article on hyperparameter optimization. You can check it out [here](https://blog.nanonets.com/hyperparameter-optimization/?utm_source=reddit&amp;utm_medium=social&amp;utm_campaign=). It's about algorithms for automating hyperparameter search, but I've included a neat trick (which I learned from the [fast.ai course](https://course.fast.ai)) that can help find a good learning rate at little/no extra computational cost.

&amp;#x200B;

One of the problems that I've had with Medium is that the platform doesn't really support interactive content. I've read so many articles where I've walked away *feeling* like I've learned something without even touching the code.

&amp;#x200B;

So in this post, I created Jupyter notebooks (hosted on Google Colab) that recreate every algorithm that I discuss in the article. I basically copied the [distill.pub](https://distill.pub) style and added a try in notebook button for each algorithm.

&amp;#x200B;

I have 2 versions of the notebook:

1. Made using the amazing fastai library (a high level wrapper for PyTorch). This is for beginners and can be run in near real-time.
2. Made with raw PyTorch (for my personal satisfaction). This is for those already used to PyTorch, who might find it's syntax more comfortable.

&amp;#x200B;

Hope you guys find this useful for your projects.",4,23,False,self,,,,,
1702,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,0,b65mgg,self.MachineLearning,Autoregressive models in practice,https://www.reddit.com/r/MachineLearning/comments/b65mgg/autoregressive_models_in_practice/,collider_in_blue,1553700170,[removed],0,1,False,self,,,,,
1703,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,0,b65vd0,self.MachineLearning,"Simple Questions Thread March 27, 2019",https://www.reddit.com/r/MachineLearning/comments/b65vd0/simple_questions_thread_march_27_2019/,AutoModerator,1553701966,[removed],0,1,False,self,,,,,
1704,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,1,b668tu,lilianweng.github.io,Are DNN Dramatically Overfitted? Discussion of generalizability and complexity measurement.,https://www.reddit.com/r/MachineLearning/comments/b668tu/are_dnn_dramatically_overfitted_discussion_of/,Isinlor,1553703882,,0,1,False,default,,,,,
1705,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,1,b66keb,papers.nips.cc,"[P] [NeurIPS 2018] On the Dimensionality of Word Embedding. (spoiler: performance stays stable at very high dimensional embeddings (10,000), are robust to overfitting)",https://www.reddit.com/r/MachineLearning/comments/b66keb/p_neurips_2018_on_the_dimensionality_of_word/,BatmantoshReturns,1553705403,,0,1,False,spoiler,,,,,
1706,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,2,b66z3f,self.MachineLearning,Autoregressive models in practice,https://www.reddit.com/r/MachineLearning/comments/b66z3f/autoregressive_models_in_practice/,collider_in_blue,1553707344,[removed],0,1,False,self,,,,,
1707,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,2,b674rn,self.MachineLearning,[D] Autoregressive models in practice,https://www.reddit.com/r/MachineLearning/comments/b674rn/d_autoregressive_models_in_practice/,collider_in_blue,1553708099,"There's something I haven't been able to sort out regarding autoregressive models. I'm about to use one in a project but I'm hung up on this one point and would like to sort it out before I start. Any help answering these questions would be very much appreciated.

Autoregressive generative models learn the joint probability distribution of many random variables by factorizing it into a product of conditionals: e.g. p(x1, x2, x3) = p(x1)p(x2|x1)p(x3|x1,x2) and modelling the conditional probabilities with a deep neural network. For a high-dimensional input, all conditional probabilities can be modeled at once by (for example) a single RNN where information about all previous steps in the sequence is encoded in the RNN's hidden state and the input at time t+1 is the output of the RNN at time t. In theory, the RNN has an unbounded receptive field and therefore can ""see"" all previous time steps in order to predict the next conditional probability distribution. This all makes sense to me.

But in practice, our inputs are often 10,000+ dimensional so we define a sequence length and cut our input up into many chunks. Now, we train the RNN on a batch of inputs of size ""sequence length"". But how does this approximate the full joint distribution of the input?

E.g. for p(x1, x2, x3, x4, x5) = p(x1)p(x2|x1)p(x3|x2,x1)p(x4|x3,x2,x1)p(x5|x4,x3,x2,x1)

If we chose a sequence length of 3, at one point we would be modelling the following sequence:

p(x3|x2,x1)p(x4|x3,x2,x1)p(x5|x4,x3,x2,x1)

Though we never encountered x1 due to our sequence length being less than the true size of the input so our conditionals are truncated to:

p(x3|x2)p(x4|x3,x2)p(x5|x4,x3,x2)

So, are we approximating the full joint distribution by truncating the previous points we are conditioning on to only ""sequence length"" points in the past? e.g. the order of our autoregressive model is equal to the sequence length? This makes sense and is standard for autoregressive models as far as I can tell (e.g. WaveNet has a fixed causal window from which it predicts new points). But is there a way to describe this mathematically? What we're modeling no longer equals the factorized joint distribution.

And how does the model ""know where it is"" within the context of the full input if it can only ""see"" back a fixed number of points? For example, if I gave a trained model a point near the center of the input and asked it to generate the rest, how does it know it's at the center and not near the beginning or end? Or would I provide it all points from the beginning to the center and ask it to complete the sequence?",6,5,False,self,,,,,
1708,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,3,b67iax,jalammar.github.io,The Illustrated Word2vec  Jay Alammar  Visualizing machine learning one concept at a time,https://www.reddit.com/r/MachineLearning/comments/b67iax/the_illustrated_word2vec_jay_alammar_visualizing/,QuirkySpiceBush,1553709891,,0,1,False,https://b.thumbs.redditmedia.com/27QHrocOoZxskrAQsPukVDvNm0wa_BH4kfNBlMWIJ1s.jpg,,,,,
1709,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,3,b67rgl,self.MachineLearning,[Discussion] Wanted help in deciding graduate school.,https://www.reddit.com/r/MachineLearning/comments/b67rgl/discussion_wanted_help_in_deciding_graduate_school/,robotify_it,1553711108,"Hi all!

&amp;#x200B;

I wanted some advice in choosing the right graduate program. I am primarily interested in reinforcement learning and would like to join the industry in a research role after my Masters.

&amp;#x200B;

I have admission offers from CMU for Masters in ML and Masters in Robotics. Both the programs have flexible course curriculum, and I can take the same courses in both of them. The Robotics program puts more emphasis on research, and the department's research is a better match for my interests. However, I am concerned whether I would be cornered to work in problems relevant to robotics after my Masters.

&amp;#x200B;

I wanted to know if job opportunities in machine learning would be restricted by the MS in Robotics program (compared to MS in ML/CS).

&amp;#x200B;

I would like to know about the type of research roles in machine learning available after a Masters degree.

&amp;#x200B;

I would appreciate your help!",2,1,False,self,,,,,
1710,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,3,b67xbi,self.MachineLearning,Softmax can't handle 3 actions at a time?,https://www.reddit.com/r/MachineLearning/comments/b67xbi/softmax_cant_handle_3_actions_at_a_time/,Jandevries101,1553711889,[removed],0,1,False,https://a.thumbs.redditmedia.com/pAkxrePxBwzpxsIH6mvq7sSe8psi98tVyRo8ZhAHZz8.jpg,,,,,
1711,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,3,b67z3x,self.MachineLearning,Best Resources for Learning Math for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b67z3x/best_resources_for_learning_math_for_machine/,supremedata,1553712124,I am hitting a plateau in my ML work because I don't understand the math behind it as well as I could. What are the best resources for understanding the math behind ML and Deep Learning that really breaks it down for you?,0,1,False,self,,,,,
1712,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,4,b688id,self.MachineLearning,[P] Re-implementing DeepMind's Meta-RL papers,https://www.reddit.com/r/MachineLearning/comments/b688id/p_reimplementing_deepminds_metarl_papers/,MuskFeynman,1553713404,"Hi [r/MachineLearning](https://www.reddit.com/r/MachineLearning),  


I spent the last 4 months trying to re-implement results from two papers from DeepMind:

1. [Learning to Reinforcement Learn, Wang et al., 2016](https://arxiv.org/pdf/1611.05763v1.pdf)
2. [Prefrontal cortex as a meta-reinforcement learning system, Wang et al., 2018](https://www.biorxiv.org/content/biorxiv/early/2018/04/13/295964.full.pdf)

At first, it was a Master's degree project (just needed to reproduce the results from the two-step task), and then I ended up exchanging emails with the first author of the paper (Jane Wang). I tried to writeup all the lessons I learned from trying to do multi-threading/multi-processing with limited hardware using DeepMind Lab, hope you find it useful!  


* Write-up (sponsored by FloydHub): [https://blog.floydhub.com/meta-rl/](https://blog.floydhub.com/meta-rl/)
* Code:

1. TF code + DeepMind Lab wrapper for the Harlow task: [https://github.com/mtrazzi/meta\_rl](https://github.com/mtrazzi/meta_rl)
2. Code/Setup for the Harlow Task Environment (fork from DeepMind Lab): [https://github.com/mtrazzi/harlow](https://github.com/mtrazzi/harlow)
3. Code for the two-step task: [https://github.com/mtrazzi/two-step-task](https://github.com/mtrazzi/two-step-task) ",6,49,False,self,,,,,
1713,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,4,b689gr,fiveminutesforit.blogspot.com,Machine learning,https://www.reddit.com/r/MachineLearning/comments/b689gr/machine_learning/,FiveminutesforIT,1553713515,,0,1,False,https://b.thumbs.redditmedia.com/OdEww7MUBoeX4GtGJWVlodKDIJ2Qgb_7ir130iuYg8I.jpg,,,,,
1714,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,4,b68avo,self.MachineLearning,Why is validation loss / accuracy important for offline serving? Cant we just overfit?,https://www.reddit.com/r/MachineLearning/comments/b68avo/why_is_validation_loss_accuracy_important_for/,nithanaroy,1553713698,[removed],0,1,False,self,,,,,
1715,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,4,b68eqy,self.MachineLearning,What happened to LISP?,https://www.reddit.com/r/MachineLearning/comments/b68eqy/what_happened_to_lisp/,CodyLeet,1553714236,[removed],0,1,False,self,,,,,
1716,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,4,b68shg,medium.com,"""PyTorch: Zero to GANs"" - A series of coding-focused tutorials on Deep Learning with PyTorch",https://www.reddit.com/r/MachineLearning/comments/b68shg/pytorch_zero_to_gans_a_series_of_codingfocused/,aakashns,1553716057,,1,1,False,https://b.thumbs.redditmedia.com/SLcZLBqVKCeRCyj40uqNC0-oA145XIxKJP-ejVCOrsc.jpg,,,,,
1717,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,4,b68v6j,self.MachineLearning,[P] I used reinforcement learning to train a real robot to play a game!,https://www.reddit.com/r/MachineLearning/comments/b68v6j/p_i_used_reinforcement_learning_to_train_a_real/,diddilydiddilyhey,1553716433,"[Here's a video of the final trained robot playing the ""puckworld"" game.](https://imgur.com/w4SYKOp)

I used a NN with Q learning to train a physical robot I made. I've done some RL projects in the past, but I didn't appreciate how different it would be to do a physical version.

I made a blog post about it [here](https://www.declanoller.com/2019/03/27/training-a-real-robot-to-play-puckworld-with-reinforcement-learning/), where I talk about the process and results.

Please let me know if you have any feedback or questions!",6,30,False,self,,,,,
1718,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,5,b690gq,self.MachineLearning,[R] MSG-GAN: Multi-Scale Gradient GAN for Stable Image Synthesis,https://www.reddit.com/r/MachineLearning/comments/b690gq/r_msggan_multiscale_gradient_gan_for_stable_image/,akanimax,1553717145,"Our paper titled ""MSG-GAN: Multi-Scale Gradient GAN for Stable Image Synthesis"" is available on arXiv now.

&amp;#x200B;

The eprint paper is available at: [https://arxiv.org/abs/1903.06048](https://arxiv.org/abs/1903.06048)

Accompanying code is released for research purposes at: [https://github.com/akanimax/BMSG-GAN](https://github.com/akanimax/BMSG-GAN)

&amp;#x200B;

&amp;#x200B;

[Architecture of MSG-GAN for generating synchronized multi-scale images](https://i.redd.it/zra5g6qrwpo21.png)

&amp;#x200B;

MSG-GAN architecture requires far less (even none) hyperparameter tuning. The architecture is based on the ProGAN but allows the flow of gradients from the discriminator to the generator at multiple scales instead of the greedy progressively growing training scheme.

&amp;#x200B;

&amp;#x200B;

[Synchronization of generator layers at multiple scales across training epochs](https://i.redd.it/dv4ty08vwpo21.png)

&amp;#x200B;

The training of the MSG-GAN progresses in a highly intuitive way which is very close to our human understanding of image formation. The layers of the generator at multiple scales first synchronize colourwise and subsequently improve the generated images at various scales. The brightness of the images across all layers (scales) synchronizes eventually.

&amp;#x200B;

Please feel free to try out your datasets on the shared code. As always, please feel free to open any issues / feedback / prs on the code repo. 

&amp;#x200B;

Cheers!

akanimax",8,31,False,https://b.thumbs.redditmedia.com/Kfyamy3527COtfT11fpTgqXe96L-sPgkIoFPcHPYHIQ.jpg,,,,,
1719,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,5,b693ox,medium.com,"Hinton, LeCun &amp; Bengio Named 2018 ACM Turing Award Laureates",https://www.reddit.com/r/MachineLearning/comments/b693ox/hinton_lecun_bengio_named_2018_acm_turing_award/,Yuqing7,1553717577,,0,1,False,default,,,,,
1720,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,5,b69cno,medium.com,[P] What I learned from building an AI that generates porn,https://www.reddit.com/r/MachineLearning/comments/b69cno/p_what_i_learned_from_building_an_ai_that/,GalaxyGoldMiner,1553718789,,0,1,True,nsfw,,,,,
1721,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,5,b69j21,self.datascience,Data Scientist interview question- Hoe to model demand in continuous time?,https://www.reddit.com/r/MachineLearning/comments/b69j21/data_scientist_interview_question_hoe_to_model/,stats_nerd21,1553719661,,0,1,False,https://b.thumbs.redditmedia.com/iLVO9PnzRQBUJLAfpAVRsMkBt4jhV-Na33kIuaF8hlI.jpg,,,,,
1722,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,6,b69p8l,blog.insightdatascience.com,[P] AutoML for Data Augmentation,https://www.reddit.com/r/MachineLearning/comments/b69p8l/p_automl_for_data_augmentation/,hszafarek,1553720482,,0,1,False,https://b.thumbs.redditmedia.com/M7MnIndgzp0vfiUwYAAF_3UC_qMEc3hwLoooeGlOAno.jpg,,,,,
1723,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,6,b6a5wd,arxiv.org,[R] Weight Standardization,https://www.reddit.com/r/MachineLearning/comments/b6a5wd/r_weight_standardization/,dmahan93,1553722672,,1,4,False,default,,,,,
1724,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,6,b6a8k1,self.MachineLearning,[D] Useful under-appreciated ml python packages?,https://www.reddit.com/r/MachineLearning/comments/b6a8k1/d_useful_underappreciated_ml_python_packages/,csinva,1553723010,"I'm wondering if there are any packages not everyone knows about that can be very useful for doing ml in python. For example, I recently found the [tqdm](https://github.com/tqdm/tqdm) package and it's made coding much nicer.

&amp;#x200B;

I've started a list [here](https://github.com/csinva/machine_learning_coding_tips/blob/master/readme.md) but would love to know what to add!",6,11,False,self,,,,,
1725,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,6,b6aaga,self.MachineLearning,[D] Limiting neural network output to realizable instances,https://www.reddit.com/r/MachineLearning/comments/b6aaga/d_limiting_neural_network_output_to_realizable/,joey___wheeler,1553723242," At the moment I have a behavioral model which predicts some output  values for some instances (input). Afterwards I sort them and get the  instance with the minimum output. Now I want to model it the ""right""  way, not running the predictions for every instance, but rather directly  using gradient descent or simulated annealing to find the minimum  realizable input (instance). The problem is that not everything is  allowed, only the combinations of values from the instances.  

Explaining it more visually:  
 Instance 1:  
 a = 4  
 b = 8  
 c = 232  

Instance 2:  
 a = 6  
 b = 10  
 c = 172  

I predict the output value (e.g. y1) for both instances and pick  either Instance 1 or Instance 2, depending on which had a lower  predicted y1 value.  

Now I want to reverse it, I want the values of the instances to be  the output while limiting to only those possible instances. So for  example an output with the values a=15, b=14, c=120 is not possible,  because there is no instance with those values.  

Is there any performant way to model this? Thanks in advance.",0,2,False,self,,,,,
1726,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,7,b6asj6,self.MachineLearning,Beginner Question: Scalable Inputs in deep learning,https://www.reddit.com/r/MachineLearning/comments/b6asj6/beginner_question_scalable_inputs_in_deep_learning/,LordKaterchen,1553725730,[removed],0,1,False,self,,,,,
1727,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,7,b6axpz,self.MachineLearning,[D] any way to share model architecture blindly,https://www.reddit.com/r/MachineLearning/comments/b6axpz/d_any_way_to_share_model_architecture_blindly/,da_g_prof,1553726481,"So we are preparing a paper and model architecture does not fit in the paper. (I am not referring to code just the model designs at this point in time.) 
The obvious solution is the supplemental but this does not survive the camera ready (ie not included in the proceedings of the conference)
A less obvious solution is to share an anonymous github link with a Readme that contains the details.
Another solution is to share a figure somewhere (eg figshare). 

Any other ideas? 
Is there something more elegant, more persistent and that can retain initial anonymity? 

As an advocate of reproducibility I find supplemental sources on author websites really problematic and ditto for github.com and other commercial or solutions (you never know how long they will be in business for or for what they will change in their strategy). 

Arxiv is not anonymous...for initial posting prior to camera ready although it is good for posting a collated version of the full paper (and supplemental after) although it creates a problem with citation upkeeping (Scopus and web of science does not count arxiv citations) 
",1,2,False,self,,,,,
1728,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,7,b6aziv,self.MachineLearning,[P] Bandit Swarm Networks for PyTorch,https://www.reddit.com/r/MachineLearning/comments/b6aziv/p_bandit_swarm_networks_for_pytorch/,CireNeikual,1553726737,"Hello,

I recently experimented with the idea of [Bandit Swarm Networks (BSN)](https://twistedkeyboardsoftware.com/?p=147) with a custom-built C++ library (with Python bindings) (posted [here](https://www.reddit.com/r/MachineLearning/comments/atyd2m/p_code_for_bandit_swarm_networks/)).

Now, I created a version that latches on to any model you can create in PyTorch and performs incremental/online reinforcement learning without the use of any backpropagation. It's not as fast as the custom C++ version on small tasks, but perhaps it will shine on larger tasks once GPU acceleration becomes important.

Here is a link to the repository, with a simple Gym Cart-Pole demo: [link](https://github.com/222464/PyTorch_BSN).",10,18,False,self,,,,,
1729,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,8,b6b7or,self.MachineLearning,Upsampling for Data Augmentation in Time-Series Forecasting,https://www.reddit.com/r/MachineLearning/comments/b6b7or/upsampling_for_data_augmentation_in_timeseries/,dotXem,1553727938,[removed],0,1,False,self,,,,,
1730,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,8,b6bgft,self.MachineLearning,"[D]Critique of Paper by ""Deep Learning Conspiracy by Jrgen Schmidhuber",https://www.reddit.com/r/MachineLearning/comments/b6bgft/dcritique_of_paper_by_deep_learning_conspiracy_by/,akaberto,1553729276,"Link: http://people.idsia.ch/~juergen/deep-learning-conspiracy.html

In another thread about the Turing award, I came across this.   In this, he essentially lists major worksn that had been ignored or haven't been citied by the award winners. I had a few thoughts and questions on this

- How serious are these accusations? Anyone looked at these besides the author ( I will go over them this weekend ). 
- Is not being aware a valid excuse for not citing the source? In academia, they tell you it is your responsibility to do research on prior work.  What the heck is happening?
- Was there an official investigation into this? Goodfellow did ask the NIPS committee but they didn't have a process in place to handle this sort of situation. 
- Are there any obscure papers you know people could benefit from? Or similar concepts? Here's an in interesting blog that I looked up today when trying to search on this topic( https://stats.stackexchange.com/questions/251460/were-generative-adversarial-networks-introduced-by-j%C3%BCrgen-schmidhuber ; someone conceived the basic idea behind GANs in 2010 and boy is that person so positive about the whole thing). 
- is this whole rediscovery business inevitable as some have suggested?",55,76,False,self,,,,,
1731,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,10,b6cga3,self.MachineLearning,Logistic Regression question (which uses OLS solution as initial weights),https://www.reddit.com/r/MachineLearning/comments/b6cga3/logistic_regression_question_which_uses_ols/,Goku_047,1553735006,[removed],0,1,False,self,,,,,
1732,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,10,b6cp7z,self.MachineLearning,Introducing datafix.io: a service that connects people with unclean data to people who want to clean data,https://www.reddit.com/r/MachineLearning/comments/b6cp7z/introducing_datafixio_a_service_that_connects/,Mjjjokes,1553736438,[removed],0,1,False,self,,,,,
1733,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,10,b6cws0,self.MachineLearning,Geoffrey Hinton's Wisdom Quote,https://www.reddit.com/r/MachineLearning/comments/b6cws0/geoffrey_hintons_wisdom_quote/,vijaydwivedi75,1553737678,[removed],0,1,False,self,,,,,
1734,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,11,b6defs,self.MachineLearning,[D]Discussion about benchmarking autoML algorithms,https://www.reddit.com/r/MachineLearning/comments/b6defs/ddiscussion_about_benchmarking_automl_algorithms/,tradigrada,1553740681,"Hi, everyone on the r/MachineLearning,

&amp;#x200B;

Our lab wants to build a benchmark platform to test the autoML algorithms. We have several datasets and test the generated networks on different tasks(classification, regression etc.).

&amp;#x200B;

The question is, how to efficiently and fairly compare different algorithms? For example, we have picked AUC, RMSE. For real applications, users might not wait these algorithms until they converges: easy stopping is the most case. So we think the ""speed"" and Model Capacity are also important for the benchmark platform. (we have not decided to use which metrics for them). Any other indicators you guys think meaningful and useful?

&amp;#x200B;

Do you have any idea or suggestion about it? Or anything the customer(anyone who wants to pick an autoML algorithm by giving the benchmark results) might care?  Thanks:)",2,5,False,self,,,,,
1735,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,11,b6did2,self.MachineLearning,Convolutional neural network faster than depthwise neural network?,https://www.reddit.com/r/MachineLearning/comments/b6did2/convolutional_neural_network_faster_than/,ImaginationDragons88,1553741347,[removed],0,1,False,self,,,,,
1736,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,12,b6dwfz,self.MachineLearning,what is effective method to training multiple data ??,https://www.reddit.com/r/MachineLearning/comments/b6dwfz/what_is_effective_method_to_training_multiple_data/,GoBacksIn,1553743867,"&amp;#x200B;

![img](7i3zs22f3so21)

this is my data

s is subway name and data1 is s\[0\]'s dataframe

and i have to train data of 05\~06 06\~07 07\~08 08\~09 10\~11 .... all column

&amp;#x200B;

i did trained 05\~06 column with preprocessing and reshape and modeling

model = Sequential()

model.add(LSTM(32, input\_shape=(None, 1)))

model.add(Dropout(0.3))

model.add(Dense(1))

like this

&amp;#x200B;

but i have to train 10940 model and load... what is the effective method to training and predict test\[-1\]??",1,1,False,self,,,,,
1737,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,12,b6dwrd,self.MachineLearning,[D] Best current Twitter accounts to follow for current ML research?,https://www.reddit.com/r/MachineLearning/comments/b6dwrd/d_best_current_twitter_accounts_to_follow_for/,Zeekawla99ii,1553743925,Apologies if this has been asked recently,26,69,False,self,,,,,
1738,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,13,b6e6dc,self.MachineLearning,[P] Pruned Cross-Validation for hyperparameter optimization,https://www.reddit.com/r/MachineLearning/comments/b6e6dc/p_pruned_crossvalidation_for_hyperparameter/,PiotrekAG,1553745740,"Hi [r/MachineLearning](https://www.reddit.com/r/MachineLearning),

I've written a Python package `pruned-cv` that significantly increases the speed of hyperparameter optimization using Cross-Validation. In my studies, it helped to perform on average three times more search iterations than standard methods at the same time. 

&amp;#x200B;

Cumulative scores and final score from Cross-Validation are dependent on each other. With the high correlation between the scores, it's possible to approximate the final score without calculating all the folds. With strongly inferior performance on first folds, the validation may be pruned. 

[Partial scores correlations with final score \(simulation study\)](https://i.redd.it/8wty0r3a6so21.png)

It can be used as a standalone optimization package (I've implemented *Pruned Grid Search* and *Pruned Randomized Search*) or with other tools like *Hyperopt* or *Optuna*.

&amp;#x200B;

I've written an [article](https://towardsdatascience.com/pruned-cross-validation-for-hyperparameter-optimization-1c4e0588191a) on Medium explaining the algorithm. You can find the package's repository [here](https://github.com/PiotrekGa/pruned-cv).

&amp;#x200B;

Please note that this is `0.0.1` version of the package. I'm waiting with further development for some feedback. I'd appreciate it from you!",5,31,False,https://b.thumbs.redditmedia.com/aBnNxbYzZSazjLpvuinW6VJH9ag9hy8k7mFVEXhyX3I.jpg,,,,,
1739,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,13,b6eate,self.MachineLearning,Output of topic modelling -- how to use?,https://www.reddit.com/r/MachineLearning/comments/b6eate/output_of_topic_modelling_how_to_use/,oligarch222,1553746616,[removed],0,1,False,self,,,,,
1740,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,13,b6ec4v,vox.com,"McDonalds new drive-thru menus will change based on the weather, traffic, and time of day. McDonalds will use AI-powered decision technology to compile data about customer purchase trends and update their menus based on predicted patterns.",https://www.reddit.com/r/MachineLearning/comments/b6ec4v/mcdonalds_new_drivethru_menus_will_change_based/,Stauce52,1553746872,,0,1,False,https://b.thumbs.redditmedia.com/YHiTnk4tTIw6v6eQMHkzrpNKLuNLiihQJhc7P-mqX9U.jpg,,,,,
1741,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,14,b6eqtv,mihaileric.com,Fundamentals of Linear Regression,https://www.reddit.com/r/MachineLearning/comments/b6eqtv/fundamentals_of_linear_regression/,MusingEtMachina,1553749802,,0,1,False,default,,,,,
1742,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,14,b6et8i,self.MachineLearning,"Three GodFathers Of AI Honored With Turing Award, The Nobel Prize Of Computing",https://www.reddit.com/r/MachineLearning/comments/b6et8i/three_godfathers_of_ai_honored_with_turing_award/,navin49,1553750290,"Hello Friend  

Today is a big day for all our AI preceptories and enthusiastic, the three computer scientists often called as GodFathers of AI, Yoshua Bengio, Geoffrey Hinton, and Yann LeCun has been honored with this years Turing Award, the Nobel Prize of computing. 

The three men have laid the foundations for many of the recent advances in artificial intelligence. The techniques developed by the trio in the 1990s and 2000s enabled huge breakthroughs in tasks like computer vision and speech recognition. 

They have developedDeep Learning with conceptual and engineering foundations for AI by using neural networks for computing. Their work underpins the current proliferation of AI technologies, from self-driving cars to automated medical diagnoses.

The three winners will split a $1 million prize that comes with the award, which is currently funded by Google. The Turing award is named onthe British mathematician Alan Turing, who laid the theoretical foundations for computer science and about whom I can talk whole year.

[Detailed Report](https://techgrabyte.com/godfathers-of-ai-honored-with-turing-award/)

And Guys who is your favorite computer scientist and who do you think will be next year's Turning Award winner ? ",0,1,False,self,,,,,
1743,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,15,b6f8xy,self.MachineLearning,What is a good book on the history of Neural Networks?,https://www.reddit.com/r/MachineLearning/comments/b6f8xy/what_is_a_good_book_on_the_history_of_neural/,zindarod,1553753644,[removed],0,1,False,self,,,,,
1744,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,15,b6fgmq,self.MachineLearning,[Hiring] Machine Learning Engineer Wanted for Autonomous Driving Startup in Silicon Valley w/ Great Pay!!!,https://www.reddit.com/r/MachineLearning/comments/b6fgmq/hiring_machine_learning_engineer_wanted_for/,cubthemagiclion,1553755360,[removed],0,1,False,self,,,,,
1745,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,16,b6g0q9,self.MachineLearning,Mfcc,https://www.reddit.com/r/MachineLearning/comments/b6g0q9/mfcc/,deveid,1553759898,[removed],0,1,False,self,,,,,
1746,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,17,b6g5pv,imfaceplate.com,5 Myths About AWS,https://www.reddit.com/r/MachineLearning/comments/b6g5pv/5_myths_about_aws/,Divya123divya,1553761067,,0,1,False,default,,,,,
1747,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,17,b6g80e,self.MachineLearning,"[N] ""Human-Computer Partnerships"" with Wendy Mackay (42min talk from GOTO Copenhagen 2018)",https://www.reddit.com/r/MachineLearning/comments/b6g80e/n_humancomputer_partnerships_with_wendy_mackay/,goto-con,1553761605,"* [Video](https://youtu.be/vJO444apR0A?list=PLEx5khR4g7PIzxn476GK3Mkk19csZZjeH)
* [Slides](https://gotocph.com/2018/sessions/600)

&amp;#x200B;

ABSTRACT

Incredible advances in hardware have not been matched by equivalent advances in software; we remain mired in the graphical user interface of the 1970s. I argue that we need a paradigm shift in how we design, implement and use interactive systems. Classical artificial intelligence treats the human user as a cog in the computer's process -- the so-called human-in-the-loop; Classical human-computer interaction focuses on creating and controlling the 'user experience'.

We seek a third approach -- a true human-computer partnership, which takes advantage of machine learning, but leaves the user in control. I describe a series of projects that illustrate our approach to making interactive systems discoverable, appropriable and expressive, using the principles of instrumental interaction and reciprocal co-adaptation.

The goal is to create robust interactive systems that significantly augment human capabilities and are actually worth learning over time.",0,2,False,self,,,,,
1748,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,17,b6g9zb,self.MachineLearning,Recommended set-up for executing tensorflow and CNNs,https://www.reddit.com/r/MachineLearning/comments/b6g9zb/recommended_setup_for_executing_tensorflow_and/,MagicElyas,1553762088,[removed],0,1,False,self,,,,,
1749,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,18,b6ghg3,self.MachineLearning,[R] Implementation of Neural Ordinary Differential Equations [slides + notebooks + code],https://www.reddit.com/r/MachineLearning/comments/b6ghg3/r_implementation_of_neural_ordinary_differential/,kmkolasinski,1553763917,"Hi, sharing with my slides and notebooks on NeuralODE. During my talk I put  stress on explaining what are ordinary differential equations, how to solve them numerically (how to implement simple black box solver), how to integrate ODE when problem function is given by Neural Network, how to compute gradients with adjoint method vs naive approach. Finally, what are Continuous Normalizing Flows derived in the paper. 

In the repo you can find:

* tensorflow implementation of NeuralODE (eager mode + keras API) - however for the sake of  simplicity I implemented only few fixed grid solvers i.e. Euler, RK2 and RK4
* jupyter notebooks which show how to implement black-box ODE solver, integrate NN with it, how to use adjoint method to optimize bullet trajectory etc
* re-implementation of Continuous Normalizing Flows: 

*Processing gif pjoslv7xrto21...*

&amp;#x200B;",33,244,False,https://b.thumbs.redditmedia.com/pL6HQn5INirkB4a30gQI_cdJ6d3uuXjmciH2s1wR1Wk.jpg,,,,,
1750,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,19,b6h7gx,fharrell.com,[D] Road Map for Choosing Between Statistical Modeling and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b6h7gx/d_road_map_for_choosing_between_statistical/,Ctown_struggles00,1553769631,,0,1,False,https://b.thumbs.redditmedia.com/W-GxvbnjDjoLBnlUEjOglG81U8QLNk0ra23nx2zu4xQ.jpg,,,,,
1751,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,19,b6h7ll,self.MachineLearning,[P] Finding input-output position in raw HTML,https://www.reddit.com/r/MachineLearning/comments/b6h7ll/p_finding_inputoutput_position_in_raw_html/,DoginaWig,1553769656,"Hey all, I was wondering if any one would be able to put me on the right track with a project I'm doing.  
So I have a bunch of html files and all of them contain a certain table I want to extract, currently I have a script that takes a median distribution of certain keywords in the html and guesses where is the input and output position of the table (char positions). The trouble I'm facing though is it's not very accurate most of the time. The html's are pretty huge and the keywords that are found in the tables are also scattered throughout the document. So I was toying with the idea of using ML for this problem, I've done some binary classifiers in the past, however I struggled to find an approach for this issue (most suggestions I found on google relied on NLTK and focused on text parsing which doesn't suit this task well). Would anyone have any suggestions on what approach could be used for such a problem?",5,3,False,self,,,,,
1752,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,20,b6hfxi,medium.com,"[AI] Can AI predict the future? Does there exist some ""GodNet"" which is the most concise representation of the universe?",https://www.reddit.com/r/MachineLearning/comments/b6hfxi/ai_can_ai_predict_the_future_does_there_exist/,rsinghal757,1553771277,,2,1,False,https://a.thumbs.redditmedia.com/jUxEZaqRbSVffP6eV22rXsjOFqXJTE0N0dabJh9xGX0.jpg,,,,,
1753,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,20,b6hhq0,self.MachineLearning,What are the effects of data density on learning,https://www.reddit.com/r/MachineLearning/comments/b6hhq0/what_are_the_effects_of_data_density_on_learning/,logarizm,1553771620,[removed],0,1,False,self,,,,,
1754,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,20,b6ho37,self.MachineLearning,Machine Learning Online Training is A Great Training Option for Students &amp; Professionals!,https://www.reddit.com/r/MachineLearning/comments/b6ho37/machine_learning_online_training_is_a_great/,multisoftmva0,1553772829,[removed],0,1,False,self,,,,,
1755,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,20,b6hvf8,stadivm.com,NVIDIA's Deep learning model that turns a doodle into a masterpiece,https://www.reddit.com/r/MachineLearning/comments/b6hvf8/nvidias_deep_learning_model_that_turns_a_doodle/,real_genesix,1553774196,,0,1,False,default,,,,,
1756,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,21,b6i5ic,revyuh.com,A Batch of Leading Artificial Intelligence Startups To Explore In 2019,https://www.reddit.com/r/MachineLearning/comments/b6i5ic/a_batch_of_leading_artificial_intelligence/,revyuh,1553775987,,0,1,False,default,,,,,
1757,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,21,b6ice0,slideshare.net,Learn about the testing and quality assurance in machine learning,https://www.reddit.com/r/MachineLearning/comments/b6ice0/learn_about_the_testing_and_quality_assurance_in/,james-warner,1553777145,,0,1,False,default,,,,,
1758,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,21,b6igk1,self.MachineLearning,Maths behind One Dimensional Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/b6igk1/maths_behind_one_dimensional_convolutional_neural/,Bluewolf867,1553777818,[removed],0,1,False,self,,,,,
1759,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,22,b6ijwz,self.MachineLearning,understanding DenseNet,https://www.reddit.com/r/MachineLearning/comments/b6ijwz/understanding_densenet/,deluded_soul,1553778351,[removed],0,1,False,self,,,,,
1760,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,22,b6ily2,self.MachineLearning,[D] Why are neural nets layered?,https://www.reddit.com/r/MachineLearning/comments/b6ily2/d_why_are_neural_nets_layered/,YoungStellarObject,1553778669,"I understand that the depth of a network is critical (see the famous XOR example), but is there any other reason besides computational or implementation convenience that the structure of neural nets is in most cases strictly layered (with the occasional exception of skip-connections)? Why are ""unstructured"" networks, with arbitrary connections, not commonly used?  


I am specifically interested in models where the ""unstructured"" part is subject to adaption, so no Liquid State Networks and the alike.",13,4,False,self,,,,,
1761,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,22,b6isuu,self.MachineLearning,is artificial Intelligence The New Electricity? You have to be kidding me!,https://www.reddit.com/r/MachineLearning/comments/b6isuu/is_artificial_intelligence_the_new_electricity/,Gabyleon2019,1553779771,[removed],0,1,False,self,,,,,
1762,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,22,b6ivyb,marktechpost.com,[R] Regression using Tensorflow and Gradient descent optimizer,https://www.reddit.com/r/MachineLearning/comments/b6ivyb/r_regression_using_tensorflow_and_gradient/,ai-lover,1553780246,,0,1,False,default,,,,,
1763,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,22,b6j23e,self.MachineLearning,Best practices for assurance of input to a network,https://www.reddit.com/r/MachineLearning/comments/b6j23e/best_practices_for_assurance_of_input_to_a_network/,Morteriag,1553781186,[removed],0,1,False,self,,,,,
1764,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,23,b6ji8d,self.MachineLearning,How do you design your in-house tools?,https://www.reddit.com/r/MachineLearning/comments/b6ji8d/how_do_you_design_your_inhouse_tools/,Mayalittlepony,1553783556,[removed],0,1,False,self,,,,,
1765,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,23,b6jkej,self.MachineLearning,[P] One Dimensional Convolutional Networks,https://www.reddit.com/r/MachineLearning/comments/b6jkej/p_one_dimensional_convolutional_networks/,Bluewolf867,1553783861,"I  have a data set which I have various aspects of the data, so I think a  CNN will work best. I have grouped each aspect of the data into sets of 5  data points, a couple of which I have padded with zeroes. My first  layer has a kernel size and strides of 5, with 500 output filters.

My  question is around pooling, should I set the strides to equal 5 too, so  that I'm not overlapping my features? And if I set the pooling size to  5, I assume I will have 1 pool per feature per filter? And if I set pool  size to 2, then I will have 3 pools and here I should enable padding to  equal the same?

Have I understood this correctly?",6,0,False,self,,,,,
1766,MachineLearning,t5_2r3gv,2019-3-28,2019,3,28,23,b6jlec,self.MachineLearning,TensorFlow 1.x to TensorFlow 2.x,https://www.reddit.com/r/MachineLearning/comments/b6jlec/tensorflow_1x_to_tensorflow_2x/,DonggeunYu,1553784010,"tf.contrib.training.HParams is TensorFlow 1.x

How can I use tf.contrib.training.HParams in 2.x?",0,1,False,self,,,,,
1767,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,0,b6jz3m,medium.com,The Rise of Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/b6jz3m/the_rise_of_generative_adversarial_networks/,kailashahirwar12,1553785916,,0,1,False,https://b.thumbs.redditmedia.com/G8cfCXFPNeuqrR2CpBSaour2vsXDLUZk7_b_TIEvEGU.jpg,,,,,
1768,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,0,b6k9pg,medium.com,Word Influence Analysis Tool SyncedLeg Open-Sourced,https://www.reddit.com/r/MachineLearning/comments/b6k9pg/word_influence_analysis_tool_syncedleg_opensourced/,gwen0927,1553787336,,0,1,False,https://b.thumbs.redditmedia.com/sg1njwt1OKwUex2Av93-K3e2ydttB4h2S6E52Ue7eLA.jpg,,,,,
1769,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,0,b6kfc1,forms.gle,[R] Survey in AI in Medical Procedures,https://www.reddit.com/r/MachineLearning/comments/b6kfc1/r_survey_in_ai_in_medical_procedures/,WAWToons,1553788094,,0,1,False,default,,,,,
1770,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,0,b6kjuj,self.MachineLearning,[R] Survey about AI in Medical Procedures,https://www.reddit.com/r/MachineLearning/comments/b6kjuj/r_survey_about_ai_in_medical_procedures/,WAWToons,1553788693,"Hello, i am a Level 4 HND student gathering AI research. I would be grateful if you took a minute of your time to fill out this questionnaire. Thank you [https://forms.gle/uFX1jvggiRKWEaa96](https://forms.gle/uFX1jvggiRKWEaa96) ",0,2,False,self,,,,,
1771,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,1,b6kmpe,self.MachineLearning,Please buy my personal data in the name of academic research. Name your price!,https://www.reddit.com/r/MachineLearning/comments/b6kmpe/please_buy_my_personal_data_in_the_name_of/,nullislandlawyers,1553789080,[removed],0,1,False,self,,,,,
1772,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,1,b6ko93,self.MachineLearning,[S] AI Survey,https://www.reddit.com/r/MachineLearning/comments/b6ko93/s_ai_survey/,Son_of_Nyx_133285,1553789291,[removed],0,1,False,self,,,,,
1773,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,1,b6kqir,self.MachineLearning,[D] A question about the original Guided Policy Search paper,https://www.reddit.com/r/MachineLearning/comments/b6kqir/d_a_question_about_the_original_guided_policy/,ewanlee,1553789589,"The following is the url of the paper [https://graphics.stanford.edu/projects/gpspaper/gps\_full.pdf](https://graphics.stanford.edu/projects/gpspaper/gps_full.pdf).

A conclusion are drawn in the last paragraph of the Section 3 of the paper:

&amp;#x200B;

[A conclusion in Section 3](https://i.redd.it/jgxl6rt6wvo21.png)

But I did some simple derivation and found some problems with this conclusion:

&amp;#x200B;

https://i.redd.it/72tpmx0iwvo21.jpg

So is it wrong with me?",5,3,False,https://b.thumbs.redditmedia.com/TNBEzqTDw4zsR1CfHs39A3BF3V6-Rrjh5J4d82hiKzs.jpg,,,,,
1774,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,1,b6ksa0,ai-artwork.com,AI Image with AI Music with AI Voice with AI Text,https://www.reddit.com/r/MachineLearning/comments/b6ksa0/ai_image_with_ai_music_with_ai_voice_with_ai_text/,techartwork,1553789824,,0,1,False,https://b.thumbs.redditmedia.com/RFq2B49tap9baBDtxfmKT9T0etm2Iu8gRO1KRDc3sgs.jpg,,,,,
1775,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,1,b6ky1u,self.MachineLearning,From Cups to Consciousness (Part 1): How are cups related to intelligence?,https://www.reddit.com/r/MachineLearning/comments/b6ky1u/from_cups_to_consciousness_part_1_how_are_cups/,benfduffy,1553790595,[removed],0,1,False,self,,,,,
1776,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,1,b6l314,self.MachineLearning,[P] From Cups to Consciousness (Part 1): How are cups related to intelligence?,https://www.reddit.com/r/MachineLearning/comments/b6l314/p_from_cups_to_consciousness_part_1_how_are_cups/,benfduffy,1553791259," Beginning a new long-running blog series which is audaciously called:

[From Cups to Consciousness (Part 1): How are cups related to intelligence?](https://towardsdatascience.com/from-cups-to-consciousness-part-1-how-are-cups-related-to-intelligence-8b7c701fa197)

You should find it interesting if you're interested in:

* the philosophy behind consciousness and cups,
* language grounding (e.g.[Gated-Attention Architectures for Task-Oriented Language Grounding](https://arxiv.org/abs/1706.07230)and[Grounded Language Learning in a Simulated 3D World](https://arxiv.org/abs/1706.06551)) and
* a survey of 3D Reinforcement Learning environments e.g.[AI2Thor](http://ai2thor.allenai.org/),[HoME](https://home-platform.github.io/),[MINOS](https://arxiv.org/abs/1712.03931),[ViZDoom](http://vizdoom.cs.put.edu.pl/),[House3D](https://github.com/facebookresearch/House3D),[habitat](https://aihabitat.org/),[CHALET](https://github.com/clic-lab/chalet),[UnrealROX](https://github.com/3dperceptionlab/therobotrix)and[Gibson](https://github.com/StanfordVL/GibsonEnv)

It overall gives us a chance and the vehicle to explore and discuss the hottest environments and techniques within AI, Robotics, CV, NLP, ML, DL and RL. All while having a bit of fun with a tongue-in-cheek style.

We also created a GitHub repository containing a list of RL frameworks in PyTorch, TensorFlow and other as well as RL environments at[https://github.com/TheMTank/RL-code-resources](https://github.com/TheMTank/RL-code-resources). As always, suggestions/improvements on any of the above are welcome!",0,9,False,self,,,,,
1777,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,1,b6l4ks,self.MachineLearning,ML Models in Production,https://www.reddit.com/r/MachineLearning/comments/b6l4ks/ml_models_in_production/,timmo1117,1553791465,"Does anyone know of resources showing ML models deployed in a production environment? In class and at work (as an analyst), it's one thing to run scripts ad-hoc, sit and wait for the model to run, and then use results in some way. But as with everything in programming, it's a whole nother matter to have it deployed to a product out in the wild! Speed would obviously be an issue, are trained models saved and reloaded in some way?",0,1,False,self,,,,,
1778,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,2,b6lvqh,self.MachineLearning,"genome.js, a Javascript library for genetic algorithms",https://www.reddit.com/r/MachineLearning/comments/b6lvqh/genomejs_a_javascript_library_for_genetic/,Treast,1553795109,"Hello everyone !

&amp;#x200B;

I couldn't find sleep this night, so I decided to make a Javascript library in order to quickly create genetic algorithms. I tried to really make it simple, but with a minimum of configurations so that can fill the most use-cases possible.

&amp;#x200B;

The code source is available here : [https://github.com/Treast/genome.js](https://github.com/Treast/genome.js)

and the lib is also available as a NPM package : [https://www.npmjs.com/package/genome.js](https://www.npmjs.com/package/genome.js)

&amp;#x200B;

I would like to have your opinion on it, and if you have any ideas on any improvement, feel free to reply on this post :)

&amp;#x200B;

Hope you like it !",0,1,False,self,,,,,
1779,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,2,b6lyay,deepmind.com," Towards Robust and Verified AI: Specification Testing, Robust Training, and Formal Verification",https://www.reddit.com/r/MachineLearning/comments/b6lyay/towards_robust_and_verified_ai_specification/,sjoerdapp,1553795476,,0,1,False,https://b.thumbs.redditmedia.com/bDVdO_Sqchl5mz6c9LcJw0EORZLsPWmGNYhvEiB3qQU.jpg,,,,,
1780,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,3,b6m6qw,self.MachineLearning,Labeling a group of instances,https://www.reddit.com/r/MachineLearning/comments/b6m6qw/labeling_a_group_of_instances/,soum16,1553796606,[removed],0,1,False,self,,,,,
1781,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,3,b6maqx,medium.com,BigGAN Trained With Only 4 GPUs!,https://www.reddit.com/r/MachineLearning/comments/b6maqx/biggan_trained_with_only_4_gpus/,gwen0927,1553797148,,0,1,False,https://a.thumbs.redditmedia.com/HNXsrNXoZLHpT335oI1YCFee19bJs6lNvuWed_KTEi0.jpg,,,,,
1782,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,3,b6mbo2,self.MachineLearning,[R] Labeling/Classifying a group of instances as well as each individual instance,https://www.reddit.com/r/MachineLearning/comments/b6mbo2/r_labelingclassifying_a_group_of_instances_as/,soum16,1553797276,"Hi,

I have a question regarding supervised learning and binary classification. I need to label a group of training instances together. So each training instance might not be strong enough to get labeled ""class 1"", but when the feature values of the group are summed together, it can be labeled ""class 1"".

So one solution is to create new instances (each new instance corresponds to a group of old instances) in this way, and train using some ML algorithm. However, while testing, I also need to label each individual old instance. How can I do that? ",0,1,False,self,,,,,
1783,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,3,b6mc8w,self.MachineLearning,[Project] AI predicts market intent based on RSI + MFI,https://www.reddit.com/r/MachineLearning/comments/b6mc8w/project_ai_predicts_market_intent_based_on_rsi_mfi/,jonasSF,1553797357,"Hey Guys,in the past few days i created a prediction tool for crypto currencies. It uses an Convnet with RSI / MFI values.

It's predction accuracy is  70 - 72%.

[https://ai.siliconforest.de/](https://ai.siliconforest.de/) (live web-demo)

Bright bars mean, that the average price in the following 8 hours is expected to be higher than the price of the last 8 hours. It is just an experiment, but live on the internet.

You can try it for Free.

I hope you enjoy what you see. If you have questions feel free to aks.

Jonas

&amp;#x200B;

Edit 28.03.19: After a long night of training and edits on the network architecture we gained a few percent accuracy and eleminated a few errors. Join me and evaluate the new model. It's now an incredible 4 Layer CNN, totally uncommon, but it had god results in detecting bad blood vessels.

&amp;#x200B;",12,1,False,self,,,,,
1784,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,3,b6mczj,self.MachineLearning,Where is statistics or maths used in machine learning?,https://www.reddit.com/r/MachineLearning/comments/b6mczj/where_is_statistics_or_maths_used_in_machine/,mbk0073,1553797468,[removed],0,1,False,self,,,,,
1785,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,3,b6mk4m,arxiv.org,"[R] Really fast object-level image search for deep features like DELF, with linear-time geometric verification",https://www.reddit.com/r/MachineLearning/comments/b6mk4m/r_really_fast_objectlevel_image_search_for_deep/,aDutchofMuch,1553798463,,1,3,False,default,,,,,
1786,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,5,b6njsz,self.MachineLearning,[R] Papers on transforming multi-label data to a single label instance(not BR) and proof of joint learnability,https://www.reddit.com/r/MachineLearning/comments/b6njsz/r_papers_on_transforming_multilabel_data_to_a/,atif_hassan,1553803287,"Hey guys, this is my first post in reddit!

&amp;#x200B;

I have been trying to find papers that formulate a multi-label problem in the following way - Say there are D documents and L number of labels from which each document gets a few labels, is there any method which creates a new binary dataset of D\*L samples meaning if a document is tagged with a given label, then target variable is set to 1 otherwise 0. Is there a particular name for such a technique?

&amp;#x200B;

The next question being, if I have a mapping function which takes in documents and labels and maps them to a lower dimensional joint vector space, how do i prove that this space is learnable? I have read through PAC learning but really can't understand where to go from there.

&amp;#x200B;

It would be of immense help if you could point me in the right direction with paper references or ideas.

&amp;#x200B;

Thank you!",5,2,False,self,,,,,
1787,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,5,b6nl75,self.MachineLearning,Anyone want to work on an open source Tensorflow.js Project?,https://www.reddit.com/r/MachineLearning/comments/b6nl75/anyone_want_to_work_on_an_open_source/,SameDifference,1553803473,[removed],0,1,False,self,,,,,
1788,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,5,b6nuay,self.MachineLearning,[D] Why is MLP working similar to RNN for text generation,https://www.reddit.com/r/MachineLearning/comments/b6nuay/d_why_is_mlp_working_similar_to_rnn_for_text/,atif_hassan,1553804755," I was trying to perform text generation using only a character level feed-forward neural network after having followed [this tutorial](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/) which uses LSTM. I one-hot encoded the characters of my corpus which gave a vector of length 45. Then I concatenated every 20 characters and fed this 20\*45 length vector as input to an MLP with the 21st character's one hot as the output.

Thus my X (input data) shape is -&gt; (144304, 900) and my Y (output data) shape is -&gt; (144304, 45)

Here's the output from my code:

&gt;alice was beginning very about a grible thing was and bet she with a great come and fill feel at and beck to the darcht repeat allice waited it, put this was not an encir to the white knew the mock turtle with a sigh. i only took the regular cours was be crosd it to fits some to see it was and getting she dodn as the endge of the evence, and went on to love that you were no alway not--ohe f whow to the gryphon is, who denight to goover and even stried to the dormouse, and repeated her question. why did they live at the bottom of a well?  
tabl the without once it it howling it the duchess to herself it as eng, longing one of the door and wasting for the homend of the taits.  
sthing i cancus croquet with the queen to-day?  
i should like it very much, said the dryphon, you first form into a line along-the sea-shore--  
the right! cried the queen nother frowing tone. any the this her for some thing is and at like the look of it at all, said the king: however, it may kiss my hand if it likes.  
id really feeer that in a few this for some whele wish to get thing to eager to think thcapered twice, and shook note bill herself in a lell as expectant, and thowedd all have come fuconfuse it the march hare: she thought it must be the right way of speaking to a mouse: she had never done such a thing before, but she remembered having seen in her brothers latin grammar, a mouse--of a mouse--to a mouse--a mouse--o mister once to hin fent on the words with her fane with ale three king the said, and diffich a vage and so mane alice this cime.

My Question is why is MLP working similar to an RNN/LSTM. What's the advantage of using RNN/LSTM for such tasks over MLP?",3,2,False,self,,,,,
1789,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,5,b6o1th,self.MachineLearning,[D] Why does all of NLP literature use Noise contrastive estimation loss for negative sampling instead of sampled softmax loss?,https://www.reddit.com/r/MachineLearning/comments/b6o1th/d_why_does_all_of_nlp_literature_use_noise/,BatmantoshReturns,1553805804,"A sampled softmax function is like a regular softmax but randomly selects a given number of 'negative' samples.

This is difference than NCE Loss, which doesn't use a softmax at all, it uses a logistic binary classifier for the context/labels. In NLP, 'Negative Sampling' basically refers to the NCE-based approach.

More details here

https://www.tensorflow.org/extras/candidate_sampling.pdf

I have tested both and they both give pretty much the same results. But in word embedding literature, they always use NCE loss, and never sampled softmax.

Is there any reason why this is? The sampled softmax seems like the more obvious solution to prevent applying a softmax to all the classes, so I imagine there must be some good reason for the NCE loss.",11,25,False,self,,,,,
1790,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,5,b6o35y,sciencedaily.com,Predict premature death could greatly improve preventative healthcare in the future,https://www.reddit.com/r/MachineLearning/comments/b6o35y/predict_premature_death_could_greatly_improve/,dondiegoalonso,1553805977,,0,1,False,default,,,,,
1791,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,5,b6o5i7,self.MachineLearning,[N] Pre-train your RL agent with Behavior Cloning - Stable-Baselines v2.5.0 Released,https://www.reddit.com/r/MachineLearning/comments/b6o5i7/n_pretrain_your_rl_agent_with_behavior_cloning/,araffin2,1553806307,"Highlights:

\- Pre-train RL model with Behavior Cloning (BC) aka supervised learning

\- Generative Adversarial Imitation Learning (GAIL) with both continuous/discrete actions support

\- Generate expert trajectories with any controller (e.g. a PID controller)

\- Bug fix in A2C continuous

&amp;#x200B;

Example of pre-training:

    from stable_baselines import A2C, GAIL, PPO2, SAC, TRPO
    from stable_baselines.gail import ExpertDataset
    # Trajectories from an expert controller (e.g. human, PID controller, trained RL agent, ...)
    dataset = ExpertDataset(expert_path='expert_pendulum.npz', traj_limitation=10, batch_size=128)
    
    # it works with any rl model
    # model = GAIL('MlpPolicy', 'Pendulum-v0', dataset, verbose=1)
    model = A2C('MlpPolicy', 'Pendulum-v0', verbose=1)
    # Pretrain the A2C model using Behavior Cloning (BC)
    model.pretrain(dataset, n_epochs=1000)
    # Train it in a classic RL setting
    model.learn(total_timesteps=20000)
    # Save the trained agent
    model.save(""A2C-Pendulum"")

&amp;#x200B;

Full changelog: [https://github.com/hill-a/stable-baselines/releases](https://github.com/hill-a/stable-baselines/releases)

Documentation: [https://stable-baselines.readthedocs.io/en/master/guide/pretrain.html](https://stable-baselines.readthedocs.io/en/master/guide/pretrain.html)",1,12,False,self,,,,,
1792,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,6,b6o9od,youtu.be,Machine Learning Tutorial Part 9 | Algorithm CheatSheet - Python Machine Learning For Beginners,https://www.reddit.com/r/MachineLearning/comments/b6o9od/machine_learning_tutorial_part_9_algorithm/,SquareTechAcademy,1553806880,,0,1,False,https://b.thumbs.redditmedia.com/B9KBsb554Tzisxna0wjwa0YljZkw1WtKYKfYDA7925M.jpg,,,,,
1793,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,6,b6obac,jclinepi.com,"A systematic review shows no performance benefit of machine learning over logistic regression for clinical prediction models (JCE, 2019)",https://www.reddit.com/r/MachineLearning/comments/b6obac/a_systematic_review_shows_no_performance_benefit/,serghiou,1553807102,,0,1,False,https://b.thumbs.redditmedia.com/H0oX04N-VCBW2PF5pIKhqT02Tf4ZOl0qIOhxJheOK-Y.jpg,,,,,
1794,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,6,b6ofi6,self.MachineLearning,"[N] CS224N final projects released, and updated 2019 lecture videos released",https://www.reddit.com/r/MachineLearning/comments/b6ofi6/n_cs224n_final_projects_released_and_updated_2019/,BatmantoshReturns,1553807664,"Final reports

http://web.stanford.edu/class/cs224n/project.html

Lecture videos youtube playlist

https://www.youtube.com/watch?v=8rXD5-xhemo&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z",2,68,False,self,,,,,
1795,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,6,b6os9j,self.MachineLearning,[D] Dimensionality Reduction using gradient descent equivalent to PCA?,https://www.reddit.com/r/MachineLearning/comments/b6os9j/d_dimensionality_reduction_using_gradient_descent/,atif_hassan,1553809436,"The following is an idea for dimensionality reduction:

Say, there are n points in a higher dimensional space. We create a matrix X of shape (nxn) containing the euclidean distance (or any other distance of choice) of each point with every other point. We then initialize n random points in the lower dimensional space. In order to preserve neighbourhood information, we minimize the following loss function,

(||xi - xj||^(2) \- Xij)^(2)

where xi and xj are points in the lower dimensional space. Thus we basically minimize the difference between the euclidean distance in lower and higher dimensional spaces.

I have seen T-SNE use gradient descent with probability distributions but is this method novel or is it in some way equivalent to PCA?",6,7,False,self,,,,,
1796,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,7,b6p5f5,self.MachineLearning,[D] How to incorporate diversity into a news feed,https://www.reddit.com/r/MachineLearning/comments/b6p5f5/d_how_to_incorporate_diversity_into_a_news_feed/,FinalEmotion,1553811303,How do we incorporate diversity into a news feed type model without using randomization? ,5,0,False,self,,,,,
1797,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,7,b6p6lg,self.MachineLearning,[D] MLFlow for Production?,https://www.reddit.com/r/MachineLearning/comments/b6p6lg/d_mlflow_for_production/,tpapp157,1553811470,"Anyone have any experience using MLFlow (www.mlflow.org) in a business production environment? Managing a library of 100s of models? Is it flexible to non-standard model and data types? Is it easily extendible to interface with existing infrastructure? Does it hold up under serious use or fall short? Any recommended alternative frameworks for managing model lifecycle?
Thanks.",9,28,False,self,,,,,
1798,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,7,b6phpd,self.MachineLearning,Pandas Basics Cheat Sheet,https://www.reddit.com/r/MachineLearning/comments/b6phpd/pandas_basics_cheat_sheet/,eurospottv,1553813046," Pandas  The Pandas library is built on NumPy and provides easy-to-use data structures and data analysis tools for the Python programming language. 

[http://www.eurospottv.com/pandas-basics-cheat-sheet/](http://www.eurospottv.com/pandas-basics-cheat-sheet/)

&amp;#x200B;

@eurospottv.com",0,1,False,self,,,,,
1799,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,7,b6pirv,self.MachineLearning,Celery with Logits,https://www.reddit.com/r/MachineLearning/comments/b6pirv/celery_with_logits/,Throwway203,1553813199,"Can I distribute a logistic regression to multiple machines using Celery? In other words can I use Celery to harness the cpu cores on multiple processors. I kind of get the idea of Celery and I believe I can use my Linux laptop as a broker.

  
Anyway, any simple yes or no answers would be awesome. Links to sites with answers are even better.   


Thanks for your time.  
",0,1,False,self,,,,,
1800,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,7,b6pmyr,sanyamkapoor.com,The Stein Gradient,https://www.reddit.com/r/MachineLearning/comments/b6pmyr/the_stein_gradient/,activatedgeek,1553813808,,0,1,False,default,,,,,
1801,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,8,b6q4on,self.MachineLearning,[D][R] Is there a theoretical or fundamental reason why LayerNorm outperforms BatchNorm on RNN networks?,https://www.reddit.com/r/MachineLearning/comments/b6q4on/dr_is_there_a_theoretical_or_fundamental_reason/,artificial_intelect,1553816427,"Is there a theoretical or fundamental reason why LayerNorm outperforms BatchNorm on RNN networks? Is the best answer that we have just simply because the [Layer Normalization Paper](https://arxiv.org/abs/1607.06450) ran an experiment, therefore it's is better.

For instance, BN normalizes using population statistics. Do population statistics not make sense in RNN networks like they do in image networks?

Alternatively, Layer Normalization is actually better for Image Tasks, but the regularization effect of BN is needed durring optimization since the optimization problem is so ill-posed.

Another alternative: the the regularization effect of BN is too strong for RNN networks.

I still don't know the answer, the above are just guesses.

&amp;#x200B;

Generally speaking should population statistics behave well in RNN's?",5,8,False,self,,,,,
1802,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,9,b6qr1w,arxiv.org,[1903.11250] Auto-Embedding Generative Adversarial Networks for High Resolution Image Synthesis (github link in comments),https://www.reddit.com/r/MachineLearning/comments/b6qr1w/190311250_autoembedding_generative_adversarial/,kingdomakrillic,1553819894,,8,11,False,default,,,,,
1803,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,9,b6qr83,self.MachineLearning,Dataset Search / Request: Contains Observations of Bifurcation,https://www.reddit.com/r/MachineLearning/comments/b6qr83/dataset_search_request_contains_observations_of/,ogsarticuno,1553819923,[removed],0,1,False,self,,,,,
1804,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,9,b6qwh4,self.MachineLearning,[R] StrokeNet: A Neural Painting Environment (ICLR 2019),https://www.reddit.com/r/MachineLearning/comments/b6qwh4/r_strokenet_a_neural_painting_environment_iclr/,wei_jok,1553820765,"This [paper](https://openreview.net/forum?id=HJxwDiActX) presents a method that is like combining a SPIRAL with a learned/differentiable painting 'world model' and training the system end-to-end.

**Abstract**

*We've seen tremendous success of image generating models these years. Generating images through a neural network is usually pixel-based, which is fundamentally different from how humans create artwork using brushes. To imitate human drawing, interactions between the environment and the agent is required to allow trials. However, the environment is usually non-differentiable, leading to slow convergence and massive computation. In this paper we try to address the discrete nature of software environment with an intermediate, differentiable simulation. We present  StrokeNet, a novel model where the agent is trained upon a well-crafted neural approximation of the painting environment. With this approach, our agent was able to learn to write characters such as MNIST digits faster than reinforcement learning approaches in an unsupervised manner. Our primary contribution is the neural simulation of a real-world environment. Furthermore, the agent trained with the emulated environment is able to directly transfer its skills to real-world software.*

Accepted as a poster at ICLR this year: https://openreview.net/forum?id=HJxwDiActX",3,11,False,self,,,,,
1805,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,10,b6r34a,self.MachineLearning,"[P] I scraped 32,000 cars, including the price and 115 specifications",https://www.reddit.com/r/MachineLearning/comments/b6r34a/p_i_scraped_32000_cars_including_the_price_and/,nicolas-gervais,1553821838,"Specs include MSRP, Gas Mileage, Engine, EPA Class, Style Name, Drivetrain, SAE Net Torque @ RPM, Fuel System, Engine Type, SAE Net Horsepower @ RPM, Displacement, etc. The cars are all cars available on the American market. 

Get the CSV (44MB) on my [drive](https://drive.google.com/file/d/1HnpfG2xj_6EZ7Tgle2QB9Yn_YS7r7uVA/view?usp=sharing), or look how I did it on [Github](https://github.com/nicolas-gervais/predicting-car-price-from-scraped-data/blob/master/scraping).
",29,176,False,self,,,,,
1806,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,10,b6raks,self.MachineLearning,Create 3D Object Model from Still RGB Camera,https://www.reddit.com/r/MachineLearning/comments/b6raks/create_3d_object_model_from_still_rgb_camera/,jacksonkr_,1553823073,[removed],0,1,False,self,,,,,
1807,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,10,b6rb5g,self.MachineLearning,Juergen Schmidhuber and Turing Award,https://www.reddit.com/r/MachineLearning/comments/b6rb5g/juergen_schmidhuber_and_turing_award/,nicaraguan67,1553823160,[removed],1,1,False,self,,,,,
1808,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,10,b6riuu,slideshare.net,The Myth of Data Driven Natural Language Understanding,https://www.reddit.com/r/MachineLearning/comments/b6riuu/the_myth_of_data_driven_natural_language/,dorkydoofus,1553824485,,0,1,False,default,,,,,
1809,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,11,b6rpl6,arxiv.org,[R] TensorMask: A Foundation for Dense Object Segmentation,https://www.reddit.com/r/MachineLearning/comments/b6rpl6/r_tensormask_a_foundation_for_dense_object/,xternalz,1553825621,,2,17,False,default,,,,,
1810,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,12,b6s7sd,self.MachineLearning,"[P] Documentating my Machine Learning Journey #1, 2, 3: Choosing Neural Machine Translating as my project, preprocessing the data, and building the model",https://www.reddit.com/r/MachineLearning/comments/b6s7sd/p_documentating_my_machine_learning_journey_1_2_3/,RedditAcy,1553828787,"Hey, I decided to start documenting my Machine Learning journey. I have just finished a few courses in Machine Learning, and I feel like I am prepared to create my own projects, build my own model, and make actual usable products! 

&amp;#x200B;

I would like to share these to reddit, because I know there's a lot of people subscribed to this subreddit that are fascinated by Machine Learning. However, they are just pondering and wondering and simply not taking action. But I will be here posting everyday on my progress, and maybe you will see me for 100th day posting in a row and finally deciding to learn Machine Learning yourself :) 

&amp;#x200B;

Alright, here's everything I did and learned in the recent 3 days: 

&amp;#x200B;

Since I just finished [deeplearning.ai](https://deeplearning.ai), I am still familiar with sequence models, and I decided to choose to build neural machine translator as my first project (specifically Spanish to English). I quickly found a tutorial on it, from [machinelearningmastery.com](https://machinelearningmastery.com), it covered text to text generation from gathering the data =&gt; cleaning the data =&gt; building the model =&gt; evaluating the model. I usually like to implement these tutorials just by copy and pasting first, then reading it through once, then opening a new file and trying to implement the project with just pseudocode. 

Data preprocessing was probably the hardest part programmatically, the dataset I used had pairs of English =&gt; Spanish translation separated by tabs and every single pair is separated from another pair with a line. After refreshing myself on python functions such as split() and strip(), I learned how to turn that specific dataset into a 2d array where each element in the 2d array is an array containing a pair of translation. Then, I just used Kera's tokenizer and turned those strings into integers so our model can learn from it. 

Onto building the model, this just can't be easier. It was really just 5 lines of code with Keras. I refreshed my knowledge on the Encoder-Decoder neural machine translation model when I was trying to implement the tutorial from scratch. And there was basically a bi-directional LSTM network that accepted embedded words as inputs, the output of the this LSTM network is fed through an attention network, the attention network outputs context, and those contexts are fed into the 2nd LSTM network. The output of the 2nd LSTM network are fed through a softmax layer to finally get the output we want. Everything is pretty intuitional. We use word embedding to give words features so they actual have a mathematical meaning. We use LSTM because texts are sequential data. We use an attention layer because we want to capture what words and how many words do we want to look at when generating the output of a specific timestamp. In Keras, these are literally done with model.add(Embedding(...)), model.add(LSTM(...)), etc. 

I then ran [model.fit](https://model.fit)(), and the results quickly came back, it wasn't the best because it was just a test run and I only used 10,000 examples, so the model probably haven't seen half of the words I am feeding in for it to spit out a response. 

I think a quick project like this really makes me think about the macro of ML as a whole. I still have a long way to figure out what the center of ML is in, what are the real challenges of ML (I really don't know if it is getting the dataset or scaling it up or reiterating and improving the model, I feel like it is highly situational ofcourse). 

&amp;#x200B;

Here're the youtube vlog and medium blog version of my journey: 

 [https://www.youtube.com/watch?v=SE96Uw9-ooc&amp;t=259s](https://www.youtube.com/watch?v=SE96Uw9-ooc&amp;t=259s) 

 [https://www.youtube.com/watch?v=ylWO6fnjfB4&amp;t=472s](https://www.youtube.com/watch?v=ylWO6fnjfB4&amp;t=472s) 

 [https://www.youtube.com/watch?v=t4I7xRXwLPQ&amp;t=532s](https://www.youtube.com/watch?v=t4I7xRXwLPQ&amp;t=532s) 

 [https://medium.com/@andrewchen\_37051/my-machine-learning-journey-1-background-and-choosing-my-next-project-c9b31c89fc11](https://medium.com/@andrewchen_37051/my-machine-learning-journey-1-background-and-choosing-my-next-project-c9b31c89fc11) 

 [https://medium.com/@andrewchen\_37051/my-machine-learning-journey-2-learning-to-preprocess-the-data-4b0f717d1958](https://medium.com/@andrewchen_37051/my-machine-learning-journey-2-learning-to-preprocess-the-data-4b0f717d1958) 

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;",2,3,False,self,,,,,
1811,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,12,b6sjt2,self.MachineLearning,[D] Why isn't robustness a common metric alongside validation loss and validation accuracy?,https://www.reddit.com/r/MachineLearning/comments/b6sjt2/d_why_isnt_robustness_a_common_metric_alongside/,clanleader,1553831015,"For data that is always from the same distribution, robustness doesn't seem that important, only a generalizable test accuracy that meets some required level. However in the real world when implementing live NN models, there will almost always be times when live samples contain subtle artifacts that the model wasn't exposed to in the training set. And if AI is to ever take off, an NN should be able to deal with this sot of thing. As it stands currently, the network makes seemingly random predictions when encountering even very subtle artifacts, please see https://arxiv.org/pdf/1710.08864.pdf for a general premise of what I'm talking about and https://openreview.net/pdf?id=HklKWhC5F7 which seems to measure this issue via a new 'robustness' metric.

Given the recent popular limitations being exposed of how things like self driving cars are confused by rain drops, modified stop signs, or any other feature that wasn't included in their training set, it seems concerning that even though the NN can apparently detect certain features, simple artifacts elsewhere in the data can cause it to nevertheless ignore these features that it supposedly already knows and confuse the network completely. Even when we can say ""NNs outperform humans at X"".. this is not true when we introduce trivial artifacts or adversarial attacks that a human can easily recognize and overlook, however these NN models that are supposedly ""superior to humans"" often can't. 

It seems the 'robustness' metric is a good way to determine how good the NN model is at not being confused by trivial artifacts.

My question is why isn't robustness seemingly taken more seriously in the NN community as a training metric for the validation set? Since it seems foolish to assume that real world live applications of any complex NN model will always encounter data that always falls under the same distribution as the training set, I would have thought robustness would be an area requiring critical discussion, research, and appropriate metrics of measurement used when training new models outside of perfectly presented toy problems. 

I think a lot of the blame lies on the over reliance of NN progression in how well it does with ""make believe"" perfectly distributed datasets like MNIST, CIFAR, and the like. Yes these models work when we input samples in a way exactly as the NN is expecting. But if we ever want an NN in a real world application which will have a lot of unpredictability, robustness seems to be a very important thing to measure.",16,4,False,self,,,,,
1812,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,12,b6sk93,self.MachineLearning,How little computing power can I have?,https://www.reddit.com/r/MachineLearning/comments/b6sk93/how_little_computing_power_can_i_have/,mattdownzzz,1553831098,[removed],0,1,False,self,,,,,
1813,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,14,b6t6r6,rottaprintindia.com,Non-woven bag making machine manufacturer,https://www.reddit.com/r/MachineLearning/comments/b6t6r6/nonwoven_bag_making_machine_manufacturer/,RottaPrintIndia,1553835626,,0,1,False,default,,,,,
1814,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,15,b6tug4,agentanakinai.wordpress.com,I'm curating YouTube videos that are helpful for anyone just getting started in Machine Learning.,https://www.reddit.com/r/MachineLearning/comments/b6tug4/im_curating_youtube_videos_that_are_helpful_for/,Agent_ANAKIN,1553840867,,0,1,False,default,,,,,
1815,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,15,b6twjd,self.MachineLearning,Teaching a Neural Network to Eat Humble Pie,https://www.reddit.com/r/MachineLearning/comments/b6twjd/teaching_a_neural_network_to_eat_humble_pie/,no_bear_so_low,1553841340,[removed],0,1,False,self,,,,,
1816,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,15,b6tx1u,self.MachineLearning,Seeking For a Wonderful Web Development Services?,https://www.reddit.com/r/MachineLearning/comments/b6tx1u/seeking_for_a_wonderful_web_development_services/,immwit_pvtltd,1553841461,[removed],0,1,False,self,,,,,
1817,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,15,b6tz96,self.MachineLearning,Any progresss on RNN Quantization?,https://www.reddit.com/r/MachineLearning/comments/b6tz96/any_progresss_on_rnn_quantization/,sybh26,1553841980,[removed],0,1,False,self,,,,,
1818,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,16,b6u4s5,self.MachineLearning,[R] Progress on RNN Quantization?,https://www.reddit.com/r/MachineLearning/comments/b6u4s5/r_progress_on_rnn_quantization/,sybh26,1553843241,I've recently been researching on quantization(mainly in int8) and I found many papers showing good numbers on CNN but none on RNN. I wonder what is the best we can get in int8 for RNN and please recommand me papers or survey papers. Thanks!,2,4,False,self,,,,,
1819,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,16,b6udlx,arxiv.org,[R] Information Maximizing Visual Question Generation,https://www.reddit.com/r/MachineLearning/comments/b6udlx/r_information_maximizing_visual_question/,RickDeveloper,1553845305,,3,6,False,default,,,,,
1820,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,17,b6ujam,self.MachineLearning,"Machine Vision Market - Size, Outlook, Trends and Forecasts",https://www.reddit.com/r/MachineLearning/comments/b6ujam/machine_vision_market_size_outlook_trends_and/,herlydavidson,1553846734,[removed],0,1,False,self,,,,,
1821,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,17,b6uo3p,self.MachineLearning,[D] What's your experience with deploying a fastai model or making it work in a custom environment? I'm struggling a lot,https://www.reddit.com/r/MachineLearning/comments/b6uo3p/d_whats_your_experience_with_deploying_a_fastai/,DeepDeeperRIPgradien,1553847988,"Hi there, I'm a heavy Keras user for years but I recently gave FastAi a chance and trained a CNN on my data with it. It was actually a cool experience and the resulting model is pretty good compared to what I achieved with Keras so far.

But there is one problem for me: deploying it. A few days ago I spent a bunch of hours googling around how people go about deploying a fastai model but I couldn't really get the answers I was looking for. In particular, I'm struggling with:

- Making the model independent of fastai so that my deployment-enviroment only needs PyTorch. I kinda figured that the pytorch model is in .model and I could pickle that, but how do I extract or know what pre- and post-processing fastai applies to the data? I didn't manage to find that out

- Even if I allow fastai in my deployment-environment and I load the fastai model itself (using fastai's export and load_model functions), it's absolutely unclear to me how I can just apply the model to a pytorch-tensor or numpy-array. Everything is built around the internal fastai classes, for example DataBunch, which only offers support for loading images from disk, but not from tensors/arrays?

If anyone could help me out or point me to some code I'd highly appreciate it! To summarize: I would like to save/export my fastai model in a way that I can apply it to images that are already loaded to memory, either as pytorch-tensor or numpy-array (e.g. when it was loaded using opencv's cv2.imread).",10,5,False,self,,,,,
1822,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,18,b6v2rq,self.MachineLearning,Any hints on Chinese plates recognition?,https://www.reddit.com/r/MachineLearning/comments/b6v2rq/any_hints_on_chinese_plates_recognition/,pl0v4eek,1553851433,[removed],0,1,False,self,,,,,
1823,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,18,b6v5kc,self.MachineLearning,[R] AttoNets: Compact and Efficient Deep Neural Networks for the Edge via Human-Machine Collaborative Design,https://www.reddit.com/r/MachineLearning/comments/b6v5kc/r_attonets_compact_and_efficient_deep_neural/,gizzmolee,1553852040,[removed],1,1,False,self,,,,,
1824,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,19,b6vg19,self.MachineLearning,How do you get data sets,https://www.reddit.com/r/MachineLearning/comments/b6vg19/how_do_you_get_data_sets/,redditor_32,1553854325,[removed],0,1,False,self,,,,,
1825,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,19,b6vjlo,socialprachar.com,Who Is Going To Make Money In Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/b6vjlo/who_is_going_to_make_money_in_artificial/,chandulekkala,1553855067,,0,1,False,default,,,,,
1826,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,19,b6vq8s,socialprachar.com,Top 10 Email Marketing Blogs You Should Definitely Follow In 2019,https://www.reddit.com/r/MachineLearning/comments/b6vq8s/top_10_email_marketing_blogs_you_should/,chandulekkala,1553856427,,0,1,False,default,,,,,
1827,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,19,b6vsqp,self.MachineLearning,Support vector machines in C#?,https://www.reddit.com/r/MachineLearning/comments/b6vsqp/support_vector_machines_in_c/,xXguitarsenXx,1553856943,[removed],0,1,False,self,,,,,
1828,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,20,b6vx57,youtu.be,Testing with Machine learning project.,https://www.reddit.com/r/MachineLearning/comments/b6vx57/testing_with_machine_learning_project/,james-warner,1553857694,,0,1,False,default,,,,,
1829,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,20,b6w85l,self.MachineLearning,cross validation,https://www.reddit.com/r/MachineLearning/comments/b6w85l/cross_validation/,nowsianska,1553859705,[removed],0,1,False,self,,,,,
1830,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,20,b6wd4s,activewizards.com,Top 8 Data Science Use Cases in Gaming,https://www.reddit.com/r/MachineLearning/comments/b6wd4s/top_8_data_science_use_cases_in_gaming/,techgig11,1553860640,,0,13,False,default,,,,,
1831,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,21,b6weau,medium.com,[D] Academic Distractions,https://www.reddit.com/r/MachineLearning/comments/b6weau/d_academic_distractions/,omarsar,1553860848,,0,1,False,default,,,,,
1832,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,21,b6wfca,ciodive.com,Io-Tahoe Announces General Availability of Machine Learning-Driven Smart Data Discovery Platform,https://www.reddit.com/r/MachineLearning/comments/b6wfca/iotahoe_announces_general_availability_of_machine/,jacobmarsh789,1553861033,,0,1,False,default,,,,,
1833,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,21,b6wgmo,self.MachineLearning,"[D] TensorFlow is dead, long live TensorFlow!",https://www.reddit.com/r/MachineLearning/comments/b6wgmo/d_tensorflow_is_dead_long_live_tensorflow/,milaworld,1553861267,"[Article](https://hackernoon.com/tensorflow-is-dead-long-live-tensorflow-49d3e975cf04?sk=37e6842c552284444f12c71b871d3640) about the TensorFlow's decision to drop legacy functionally to embrace Keras full-on.

*In a nutshell: TensorFlow has just gone full Keras. Those of you who know those words just fell out of your chairs. Boom!*

*Why must we choose between Kerass cuddliness and traditional TensorFlows mighty performance? What dont we have both?*

*We dont think you should have to choose between a simple API and scalable API. We want a higher level API that takes you all the way from MNIST to planet scale.Karmel Allison, TF Engineering Leader at Google*

https://hackernoon.com/tensorflow-is-dead-long-live-tensorflow-49d3e975cf04?sk=37e6842c552284444f12c71b871d3640",186,364,False,self,,,,,
1834,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,21,b6wkl8,self.MachineLearning,[P] Genetic algorithm to find optimal neural network structure which uses backprop,https://www.reddit.com/r/MachineLearning/comments/b6wkl8/p_genetic_algorithm_to_find_optimal_neural/,MoronInGrey,1553861992,"https://github.com/Tsdevendra1/NEAT-Algorithm

I'm only sharing this because I thought someone might find this interesting. This is a follow on from the paper [NEAT paper written by Kenneth Stanley](http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf).

Mine differs from this because it uses backprop for the weight optimisation of the neural network instead of a genetic algorith. 

I'm still currently working on this and have only tested it on the XOR dataset. The actual github page isn't documented but I will get to it eventually, I just though someone might be interested in seeing the implementation from scratch in python.",15,108,False,self,,,,,
1835,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,21,b6wtne,arxiv.org,[R] Mining Discourse Markers for Unsupervised Sentence Representation Learning,https://www.reddit.com/r/MachineLearning/comments/b6wtne/r_mining_discourse_markers_for_unsupervised/,Jean-Porte,1553863581,,1,6,False,default,,,,,
1836,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,21,b6wvz6,self.MachineLearning,[D] Are there any methods to approximate monotonic function by Neural network?,https://www.reddit.com/r/MachineLearning/comments/b6wvz6/d_are_there_any_methods_to_approximate_monotonic/,master_python,1553863954,"Hi guys.

Are there any methods that only approximate monotonic function by using Neural network?

It is well known that a Neural network is able to approximate any functions. If the Neural network is shallow and all of its weights are positive, a function generated by the NN might be monotonic increasing function. In the case of deep neural network, however, it is hard to answer to this issue.

Any references or ideas are welcome.

Thanks.",11,6,False,spoiler,,,,,
1837,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,22,b6xbf1,self.MachineLearning,[R] 9 new SOTA records: Invariant Information Clustering for unsupervised image classification and segmentation,https://www.reddit.com/r/MachineLearning/comments/b6xbf1/r_9_new_sota_records_invariant_information/,xuj1,1553866350,"The state of the art in unsupervised learning. 9 new records set on 8 datasets - CIFAR, MNIST, COCO-Stuff, STL from ImageNet. Paper and code out now:  [https://arxiv.org/pdf/1807.06653.pdf](https://arxiv.org/pdf/1807.06653.pdf)

https://i.redd.it/51qkizj382p21.jpg

&amp;#x200B;",116,92,False,self,,,,,
1838,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,22,b6xema,self.MachineLearning,Can Neural Networks only interpolate and not extrapolate?,https://www.reddit.com/r/MachineLearning/comments/b6xema/can_neural_networks_only_interpolate_and_not/,Semantic_Internalist,1553866861,[removed],0,1,False,self,,,,,
1839,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,23,b6xt9d,reddit.com,Better NLP (experimental),https://www.reddit.com/r/MachineLearning/comments/b6xt9d/better_nlp_experimental/,neomatrix369,1553868986,,0,1,False,default,,,,,
1840,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,23,b6y5bh,self.MachineLearning,[P] Demystifying the neural network black box (Slides and code),https://www.reddit.com/r/MachineLearning/comments/b6y5bh/p_demystifying_the_neural_network_black_box/,datitran,1553870725,"Hey my team gave a talk at [Data Festival 2019](https://www.datafestival.de/) this year about interpreting the decisions made by convolutional neural networks. We just released the slides and code for the talk and I wanted to share it here because interpretable machine learning is a hot and important topic. So if you want to understand what kind of techniques and tools we use to understand convolutional neural networks, then definitely have a look at this. We also provide two Google Colab notebooks which can be used for your projects as well. The interesting part is that we wrote some small helper logic to convert models trained in Keras to a Lucid compatible model so that it can be visualized easily.

&amp;#x200B;

Github: [https://github.com/idealo/cnn-exposed](https://github.com/idealo/cnn-exposed)

Slides: [https://speakerdeck.com/tanujjain/demystifying-the-neural-network-black-box](https://speakerdeck.com/tanujjain/demystifying-the-neural-network-black-box)

Colab (Attribution techniques): [https://colab.research.google.com/github/idealo/cnn-exposed/blob/master/notebooks/Attribution.ipynb](https://colab.research.google.com/github/idealo/cnn-exposed/blob/master/notebooks/Attribution.ipynb)

Colab (Visualization techniques): [https://colab.research.google.com/github/idealo/cnn-exposed/blob/master/notebooks/Visualization.ipynb](https://colab.research.google.com/github/idealo/cnn-exposed/blob/master/notebooks/Visualization.ipynb)

[Grad-Cam analysis to understand decisions made by the CNN.](https://i.redd.it/3eqpmyxjl2p21.png)",1,7,False,https://b.thumbs.redditmedia.com/UqR9Njlq2_54UYcoczfQ7pzeYSRCtkqR2G5TkCX518w.jpg,,,,,
1841,MachineLearning,t5_2r3gv,2019-3-29,2019,3,29,23,b6y9fy,self.MachineLearning,Better NLP (experimental),https://www.reddit.com/r/MachineLearning/comments/b6y9fy/better_nlp_experimental/,neomatrix369,1553871311,[removed],0,1,False,self,,,,,
1842,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,0,b6yi8b,i.redd.it,Google results to t-sne are surprising.,https://www.reddit.com/r/MachineLearning/comments/b6yi8b/google_results_to_tsne_are_surprising/,lash7,1553872489,,0,1,False,default,,,,,
1843,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,1,b6zg23,self.MachineLearning,Build a catalog for Machine learning degree.,https://www.reddit.com/r/MachineLearning/comments/b6zg23/build_a_catalog_for_machine_learning_degree/,do-on,1553877195,[removed],0,1,False,self,,,,,
1844,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,2,b6zxl4,self.MachineLearning,Please will somebody buy my personal data in the name of academic research. Name your price!,https://www.reddit.com/r/MachineLearning/comments/b6zxl4/please_will_somebody_buy_my_personal_data_in_the/,nullislandlawyers,1553879623,[removed],0,1,False,self,,,,,
1845,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,2,b70gsh,reddit.com,Looking for tree based model to guide in data-driven decisions,https://www.reddit.com/r/MachineLearning/comments/b70gsh/looking_for_tree_based_model_to_guide_in/,FantasticPhenom,1553882237,,0,1,False,default,,,,,
1846,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,3,b70lbt,self.MachineLearning,Suggestion for LSH implementation,https://www.reddit.com/r/MachineLearning/comments/b70lbt/suggestion_for_lsh_implementation/,ravi20036,1553882842,[removed],0,1,False,self,,,,,
1847,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,3,b70mmo,self.MachineLearning,Looking for tree driven modeling algorithm to facility decisions,https://www.reddit.com/r/MachineLearning/comments/b70mmo/looking_for_tree_driven_modeling_algorithm_to/,FantasticPhenom,1553883024,[removed],0,1,False,self,,,,,
1848,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,3,b70x10,self.MachineLearning,"[D] ""author's style"" style transfer for text exists?",https://www.reddit.com/r/MachineLearning/comments/b70x10/d_authors_style_style_transfer_for_text_exists/,hadaev,1553884471,in google i find only about things like tweets or reviews.,14,8,False,self,,,,,
1849,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,4,b719hi,victorzhou.com,A Simple Explanation of Gini Impurity,https://www.reddit.com/r/MachineLearning/comments/b719hi/a_simple_explanation_of_gini_impurity/,vzhou842,1553886212,,0,1,False,https://a.thumbs.redditmedia.com/vvMrzqVzv13GNzOKsvOlbaHHmjsAGly47PYIxM487C4.jpg,,,,,
1850,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,4,b719ny,self.MachineLearning,I wrote some code to create StyleGAN (and a SPADE layer) in Keras,https://www.reddit.com/r/MachineLearning/comments/b719ny/i_wrote_some_code_to_create_stylegan_and_a_spade/,manicman1999,1553886237,[removed],0,2,False,self,,,,,
1851,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,4,b719x7,medium.com,Discovering Malso: bringing AI to life,https://www.reddit.com/r/MachineLearning/comments/b719x7/discovering_malso_bringing_ai_to_life/,Glassiuex,1553886274,,0,1,False,https://b.thumbs.redditmedia.com/6hINnMbtTeyegGxUsn64NEcZy1z1S4IODVeNjiayxTA.jpg,,,,,
1852,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,4,b71jow,self.MachineLearning,Do I need a Ph.D to do machine learning?,https://www.reddit.com/r/MachineLearning/comments/b71jow/do_i_need_a_phd_to_do_machine_learning/,A4_Ts,1553887630,Or would a B.S. be sufficient enough that I can self teach myself so that I could build neural networks on my own and such? Any advice is helpful thanks ,0,1,False,self,,,,,
1853,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,5,b71zwv,self.MachineLearning,[D] What AI/ML Startup is Having the Most Success At the Moment?,https://www.reddit.com/r/MachineLearning/comments/b71zwv/d_what_aiml_startup_is_having_the_most_success_at/,v3nge,1553889902,"In your opinion, what AI/ML startup is having the most commercial success at the moment?",10,9,False,self,,,,,
1854,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,5,b72jqq,self.MachineLearning,[D] Are there studies that have changed the behaviour of the basic neuron/perceptron?,https://www.reddit.com/r/MachineLearning/comments/b72jqq/d_are_there_studies_that_have_changed_the/,InfamousPancakes,1553892727,"The current basic neuron takes the weighted sum of it's inputs plus an extra bias. I was just wondering if their have been studies or experiments that have tweaked it's behaviour in any way (for example taking the weighted sum + product of weighted inputs + bias)

I wanted to see the effects of playing with a perceptron myself but a lot of the existing libraries do not really allow for such low level tinkering.",4,6,False,self,,,,,
1855,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,6,b72ui0,self.MachineLearning,Do you use a basic neural network?,https://www.reddit.com/r/MachineLearning/comments/b72ui0/do_you_use_a_basic_neural_network/,Aesix,1553894301,"

Hey guys!  Id love help understanding how to conceptualize this problem.


I have millions of samples, each with 45 features (ex: human sample, features could be height, weight, blood type, etc)... But we have a problem. 

Only 4 of 45 features are sampled per idividual measure.  Patient 1 may have height, blood type, weight, and age while patient 2 may provide any of the 45, and very possibly with no overlap to patient 1s provided feature measures as its randomly chosen. 

How can I build a way to draw inferences between patients? With millions of samples I feel like a neural network or other ML technique could make a model to predict the remaining values to the best of its abilities, yes?",0,1,False,self,,,,,
1856,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,6,b731pn,self.MachineLearning,[P] Join our 5 Minute Paper Video Challenge and win cash prizes,https://www.reddit.com/r/MachineLearning/comments/b731pn/p_join_our_5_minute_paper_video_challenge_and_win/,tdls_to,1553895341,"Hi everyone,

We are AISC, a machine learning community that produce technical ML live streams and videos.

We are conducting an online challenge where contestants upload a 5 minute video that explains a paper of their choosing. Authors of videos with the most engagement (in terms of views, likes and comments) will receive cash prizes. 

Please find the details of the competition in the link below, and let us know if you have any questions:

[https://aisc.a-i.science/5-min-challenge/](https://aisc.a-i.science/5-min-challenge/)",8,1,False,self,,,,,
1857,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,6,b73as2,self.MachineLearning,CUDA out of memory after reboot,https://www.reddit.com/r/MachineLearning/comments/b73as2/cuda_out_of_memory_after_reboot/,iamMess,1553896712,[removed],0,1,False,self,,,,,
1858,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,8,b747h3,youtu.be,AI and ML : Beyond the Sales Pitch!,https://www.reddit.com/r/MachineLearning/comments/b747h3/ai_and_ml_beyond_the_sales_pitch/,DelboyETH,1553901684,,0,1,False,https://b.thumbs.redditmedia.com/MR1orFyypyavNooybo7BpYY-xvGNsO4w-ucNLg9Bpgg.jpg,,,,,
1859,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,10,b75c2l,self.MachineLearning,Is machine learning application of Artificial Intelligence ?,https://www.reddit.com/r/MachineLearning/comments/b75c2l/is_machine_learning_application_of_artificial/,i_amr_p,1553908441,[removed],0,1,False,self,,,,,
1860,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,11,b75sx1,self.MachineLearning,Today I recived hand-made Earl grey jam from my colleague Dr. Ja-Kyoung for present.,https://www.reddit.com/r/MachineLearning/comments/b75sx1/today_i_recived_handmade_earl_grey_jam_from_my/,neongeni,1553911396,[removed],0,1,False,self,,,,,
1861,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,11,b765r1,youtube.com,1000L 5 Layers Water Storage Tank Blow Moulding Machine Quality Testing,https://www.reddit.com/r/MachineLearning/comments/b765r1/1000l_5_layers_water_storage_tank_blow_moulding/,miyawang12138,1553913678,,1,1,False,default,,,,,
1862,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,12,b76ski,i.redd.it,A DCVAE trained on digital terrain maps of Mars from HiRise/MRO. 3D surfaces are procedurally generated from the latent space,https://www.reddit.com/r/MachineLearning/comments/b76ski/a_dcvae_trained_on_digital_terrain_maps_of_mars/,professormunchies,1553918090,,1,1,False,default,,,,,
1863,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,13,b76yhc,self.MachineLearning,[P] a loop,https://www.reddit.com/r/MachineLearning/comments/b76yhc/p_a_loop/,KingAnunnaki,1553919259,"Down the rabbit hole we go. 
Is Machine learning
Done by machine elves.
https://youtu.be/RF4iuFvoT10",0,0,False,self,,,,,
1864,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,13,b773dv,self.MachineLearning,Struggling to understand how ML even works,https://www.reddit.com/r/MachineLearning/comments/b773dv/struggling_to_understand_how_ml_even_works/,mahtats,1553920202,[removed],0,1,False,self,,,,,
1865,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,14,b77fn3,self.MachineLearning,Looking for a video that was posted here in this subreddit a while ago.,https://www.reddit.com/r/MachineLearning/comments/b77fn3/looking_for_a_video_that_was_posted_here_in_this/,TheExclusiveNig,1553922569,[removed],0,1,False,self,,,,,
1866,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,14,b77rmp,github.com,"Turn your Android into Face Recognition monitor, alarm you unknown person with open source [Deep Camera]",https://www.reddit.com/r/MachineLearning/comments/b77rmp/turn_your_android_into_face_recognition_monitor/,solderzzc,1553925046,,0,1,False,default,,,,,
1867,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,15,b77uyg,self.MachineLearning,[Discussion] How to start with machine learning?,https://www.reddit.com/r/MachineLearning/comments/b77uyg/discussion_how_to_start_with_machine_learning/,fogliodicarta01,1553925743,I can program in Java and I have some experience in web programming but I never studied anything like machine learning. I'm 17 and my teachers don't know much about this topic where do you think I should start? ,10,0,False,self,,,,,
1868,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,15,b77zf5,self.MachineLearning,I now have a 2GB dataset of parent comment pairs. Its approximately over 8000000 pairs.,https://www.reddit.com/r/MachineLearning/comments/b77zf5/i_now_have_a_2gb_dataset_of_parent_comment_pairs/,nyx_aiot,1553926687,[removed],0,1,False,self,,,,,
1869,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,15,b782oj,self.MachineLearning,Polynomial Kernel,https://www.reddit.com/r/MachineLearning/comments/b782oj/polynomial_kernel/,Yonkou94,1553927387,[removed],0,1,False,self,,,,,
1870,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,15,b782rv,self.MachineLearning,[D] AI Hasnt Found Its Isaac Newton: @GaryMarcud on Deep Learning Defects &amp; Frenemy @YLecun,https://www.reddit.com/r/MachineLearning/comments/b782rv/d_ai_hasnt_found_its_isaac_newton_garymarcud_on/,chisai_mikan,1553927407,"*Marcus is a respected New York University Professor and founder of Uber-acquired Geometric Intelligence. Last year, Marcus aired his concerns and criticisms of deep learning, igniting a social media firestorm involving a number of high-profile AI researchers. In this [interview](https://syncedreview.com/2019/02/15/ai-hasnt-found-its-isaac-newton-gary-marcus-on-deep-learning-defects-frenemy-yann-lecun/) Marcus speaks on the Twitter war with Facebook Chief AI Scientist Yann LeCun, deep learnings shortfalls, and general artificial intelligence.*

https://syncedreview.com/2019/02/15/ai-hasnt-found-its-isaac-newton-gary-marcus-on-deep-learning-defects-frenemy-yann-lecun/",0,1,False,self,,,,,
1871,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,15,b783zo,self.MachineLearning,[D] AI Hasnt Found Its Isaac Newton: @GaryMarcus on Deep Learning Defects &amp; Frenemy @YLeCun,https://www.reddit.com/r/MachineLearning/comments/b783zo/d_ai_hasnt_found_its_isaac_newton_garymarcus_on/,chisai_mikan,1553927665,"An [interview](https://syncedreview.com/2019/02/15/ai-hasnt-found-its-isaac-newton-gary-marcus-on-deep-learning-defects-frenemy-yann-lecun/) with Gary Marcus by Synced Review on the limitations of deep learning, Twitter Wars etc.

*Marcus is a respected New York University Professor and founder of Uber-acquired Geometric Intelligence. Last year, Marcus aired his concerns and criticisms of deep learning, igniting a social media firestorm involving a number of high-profile AI researchers. In this [interview](https://syncedreview.com/2019/02/15/ai-hasnt-found-its-isaac-newton-gary-marcus-on-deep-learning-defects-frenemy-yann-lecun/) Marcus speaks on the Twitter war with Facebook Chief AI Scientist Yann LeCun, deep learnings shortfalls, and general artificial intelligence.*

https://syncedreview.com/2019/02/15/ai-hasnt-found-its-isaac-newton-gary-marcus-on-deep-learning-defects-frenemy-yann-lecun/",41,11,False,self,,,,,
1872,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,16,b78j1q,self.MachineLearning,[P] Nudity detection and Censoring in images with Image Classification and Object Detection,https://www.reddit.com/r/MachineLearning/comments/b78j1q/p_nudity_detection_and_censoring_in_images_with/,winchester6788,1553931093,"Hi all, for the past two months, I worked on collecting and curating dataset for nudity detection using image classification and censoring using object detection.

The code and pre-trained models are available at https://github.com/bedapudi6788/NudeNet

The test results and methodology are explained in the following posts

https://medium.com/@praneethbedapudi/nudenet-an-ensemble-of-neural-nets-for-nudity-detection-and-censoring-d9f3da721e3?source=friends_link&amp;sk=e19cdcc610e63b16274dd659050ea955

https://medium.com/@praneethbedapudi/nudenet-an-ensemble-of-neural-nets-for-nudity-detection-and-censoring-c8fcefa6cc92?source=friends_link&amp;sk=f0a4786bf005cd4b7e89cf625f109af0
",90,172,False,self,,,,,
1873,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,16,b78ljo,journalofbigdata.springeropen.com,Customer churn prediction in telecom using machine learning and social networks analysis in big data platform. What is the best tree based classification algorithm for churn? #XGBOOST,https://www.reddit.com/r/MachineLearning/comments/b78ljo/customer_churn_prediction_in_telecom_using/,Abdelrahimk,1553931706,,0,1,False,https://b.thumbs.redditmedia.com/LL08IS2LR-_W5w5WeIjxPBmmNfN4XNoiumj4mQggGIQ.jpg,,,,,
1874,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,16,b78oaj,self.MachineLearning,Suggestions on how to deal with nasty timeseries data?,https://www.reddit.com/r/MachineLearning/comments/b78oaj/suggestions_on_how_to_deal_with_nasty_timeseries/,HPa61,1553932366,[removed],0,1,False,self,,,,,
1875,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,17,b78ufm,self.MachineLearning,[D] Breaking down sentence embedding vector into individual word embeddings,https://www.reddit.com/r/MachineLearning/comments/b78ufm/d_breaking_down_sentence_embedding_vector_into/,AnonMLstudent,1553933856,"Is there a way to do this? I'm currently using Universal Sentence Encoder (USE) with transformer for semantic similarity between sentences which generates an embedding vector for each sentence, and was wondering if it's possible to break these down into ones for each individual word in the sentence. I do not want to just feed each word individually as I would like to take into account the context of the entire sentence.",4,3,False,self,,,,,
1876,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,17,b78x8s,self.MachineLearning,What are the best free sources to learn machine learning or at least to start machine learning ?,https://www.reddit.com/r/MachineLearning/comments/b78x8s/what_are_the_best_free_sources_to_learn_machine/,aniketmaradecode,1553934556,[removed],0,1,False,self,,,,,
1877,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,17,b7902c,self.MachineLearning,Any Basketball Fans Out There?,https://www.reddit.com/r/MachineLearning/comments/b7902c/any_basketball_fans_out_there/,mac_blockparty,1553935252,"I have a company where we've built a CV/ML engine that can autonomously edit a complete broadcast basketball game into only the game action (removing dead balls, commercials, etc.) in about 20 minutes. We can edit games at about 80-85% accuracy. Once we hit 90-95 % accuracy, I believe we can commercialize and gain NBA and NCAAB customers (we have 7 LOI's now from high level collegiate teams). We are a bit plateaued right now though at our current accuracy. I'm curious if there are any strong ML/CV developers out there who may be interested in helping us get over the hump? We are offering equity in the company if you help significantly but we can discuss this further. Please DM me or comment here if you want to hear more.",0,1,False,self,,,,,
1878,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,18,b79875,self.MachineLearning,[D] 10 Statistical Techniques Data Scientists Should Master,https://www.reddit.com/r/MachineLearning/comments/b79875/d_10_statistical_techniques_data_scientists/,seemingly_omniscient,1553937269,"The more statistical techniques a Data Scientist has mastered, the better the results can be. In this blog article, we want to introduce you to ten common techniques that should not be missing in the repertoire of a data scientist.  A brief overview.

&amp;#x200B;

Article:  [https://www.aisoma.de/10-statistical-techniques/](https://www.aisoma.de/10-statistical-techniques/) 

https://i.redd.it/8ghxeypo38p21.jpg",0,0,False,https://b.thumbs.redditmedia.com/Ob1YiolyKrKBOqEk28LhnOrQu7Wx9UlgbXhRgB8SjtM.jpg,,,,,
1879,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,18,b79hij,self.MachineLearning,[D] Unsupervised classification/clustering of 2d point sets,https://www.reddit.com/r/MachineLearning/comments/b79hij/d_unsupervised_classificationclustering_of_2d/,Iamhummus,1553939480,"Hey everyone, I am working on a school project and while I achieved somewhat acceptable predictions, I would like to improve it. 

I have data-set of \~20k unlabeled 3d scanned objects which have been reduced to 2d profiles (inner and outer outline) as 2XN vector, where N is different for each object.    

I might have been wrong all along but the method I tried was to train an auto-encoder and use DEC (deep embedding clustering). 

I tired few variations of fixing the samples length, auto-encoder and loss functions-

* length fixing:
   * tried to interpolate the x,y sets to fixed size, used with unordered distance loss 
   * tried to randomly choose fixed number of points from each sample, used with unordered distance loss 
   * tried to crop/pad and save 'points-validity' vector which was kept for MSE loss (padded points shouldn't be counted for the MSE loss)
* auto-encoder:
   * tried to use semi-PoinNet architecture (1d convs on each points, merging, maxpooling, dense layers -&gt;latent space -&gt; decoder which is mirror to encoder)
   * tried to use simple dense layers AE - surprisingly worked better 
* loss function:
   * tried unordered distance loss (kind of MSE while using the closest points from the input and output) - was kind of meh
   * MSE loss with the point-validity in consideration 
   * I read few papers about EMD loss (earth mover's distance) but I have to do it on PyTorch and the one implementation I found doesn't seem to run on my system. 

&amp;#x200B;

currently the best combination I found was to crop/pad, with simple fully connected auto-encoder and the MSE-like loss.

I am sure people here will have many useful insights, I would like to here them.

**Sorry for the text wall, thank you for your time.**

&amp;#x200B;

**TL;DR:**

**i'm trying to cluster 2d points clouds and think I am missing something**

 

&amp;#x200B;

&amp;#x200B;",4,1,False,self,,,,,
1880,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,19,b79n1u,self.MachineLearning,STACKER CRANE  AS/RS FOR PALLETS.,https://www.reddit.com/r/MachineLearning/comments/b79n1u/stacker_crane_asrs_for_pallets/,lhd121,1553940675,[removed],0,1,False,https://b.thumbs.redditmedia.com/GNQA1of7jZHnlevqs6GEr0wZtgruGrY73O2P5ZSOoSU.jpg,,,,,
1881,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,20,b7a87p,self.MachineLearning,Is there any reason that I should use low-level api tensoflow?,https://www.reddit.com/r/MachineLearning/comments/b7a87p/is_there_any_reason_that_i_should_use_lowlevel/,sjh9020,1553945296,[removed],0,1,False,self,,,,,
1882,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,20,b7aeqe,self.MachineLearning,RNN-like for order-free pool of samples?,https://www.reddit.com/r/MachineLearning/comments/b7aeqe/rnnlike_for_orderfree_pool_of_samples/,MrLeylo,1553946689,[removed],0,1,False,self,,,,,
1883,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,21,b7avun,self.MachineLearning,Finding Linear Separability,https://www.reddit.com/r/MachineLearning/comments/b7avun/finding_linear_separability/,nishanfernando,1553949968,"    X_train, X_test, y_train, y_test = train_test_split(training.data,training.predictions,train_size=0.6)
    model = LinearSVC()
    model.fit(X_train, y_train).score(X_train, y_train)

If this returns 1.0, does this mean my data is not linearly separable. 

Also, If I have multiple features, and If I plot one feature against both of my classes and the points from both classes appear to non separable, does this mean my data is non separable?",0,1,False,self,,,,,
1884,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,22,b7bfe4,self.MachineLearning,Deep Style Transfer,https://www.reddit.com/r/MachineLearning/comments/b7bfe4/deep_style_transfer/,jarvinos,1553953473,[removed],0,1,False,self,,,,,
1885,MachineLearning,t5_2r3gv,2019-3-30,2019,3,30,23,b7bpve,learnaionlineatoz.blogspot.com,Hiw New York Police Department is Using Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/b7bpve/hiw_new_york_police_department_is_using/,updownvizzii,1553955214,,0,1,False,default,,,,,
1886,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,0,b7c7qh,self.MachineLearning,Fashion Industry needs ML to reduce waste,https://www.reddit.com/r/MachineLearning/comments/b7c7qh/fashion_industry_needs_ml_to_reduce_waste/,spinks77,1553958059,[removed],0,1,False,self,,,,,
1887,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,0,b7cana,self.MachineLearning,[D] Papers/Books/Posts about all the costs associated with ML and the different considerations.,https://www.reddit.com/r/MachineLearning/comments/b7cana/d_papersbooksposts_about_all_the_costs_associated/,slimuser98,1553958541,"I am looking for research and information that does a good job outlining and going into the short term and long term costs of implementing ML solutions. 

Preferably across different problem domains, but anything would be of use. ",4,19,False,self,,,,,
1888,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,2,b7dio5,self.MachineLearning,[D] Machine learning adjusting itself,https://www.reddit.com/r/MachineLearning/comments/b7dio5/d_machine_learning_adjusting_itself/,revobabep,1553965813,"Lately I've been reading a few machine learning algorithms of various kinds, from simple to more complex ones. All of them kept the basic structure of ""hardcoding"" some hyper parameters, but none tried an approach in which hyper parameters and even the algorithm itself could change and switch. Does such an approach exist? If not, why?",12,0,False,self,,,,,
1889,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,2,b7djqa,self.MachineLearning,Interview with Flowcast CTO: AI / Machine Learning in Fintech,https://www.reddit.com/r/MachineLearning/comments/b7djqa/interview_with_flowcast_cto_ai_machine_learning/,andrea_manero,1553965973,[removed],0,1,False,self,,,,,
1890,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,2,b7dnw5,self.MachineLearning,[D] Why GradientBoosting predicts faster than eXtremeGradientBoosting(XGB)on larger batches?,https://www.reddit.com/r/MachineLearning/comments/b7dnw5/d_why_gradientboosting_predicts_faster_than/,MLUser2018,1553966629,"Hello,

&amp;#x200B;

I am evaluating the two boosting methods ""GradientBoostingRegressor"" and ""XGBRegressor"" of scikit-learn. I have trained a dataset with appr. 150.000 data rows (six features and one label). By measuring the prediction-time on different batch-sizes, I have got the result, that a prediction with GradientBoostingRegressor on data with batch size of 100.000 has a throughput (300.000 rows/sec), which is 30% higher than the throughput of XGBRegressor. The hyperparameters are the same. I have tried to use xgboost directly, without the scikit-learn-library, which leads to similar results. Does someone has an idea, why GB has a higher throughput than XGB on greater batches? 

  
Yours sincerely  
",7,5,False,self,,,,,
1891,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,2,b7dnwg,self.MachineLearning,Using Machine Learning to Measure Job Skill Similarities,https://www.reddit.com/r/MachineLearning/comments/b7dnwg/using_machine_learning_to_measure_job_skill/,andrea_manero,1553966630,[removed],0,1,False,self,,,,,
1892,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,2,b7dwzx,self.MachineLearning,Help with downloading Siamese RPN Pretrained model,https://www.reddit.com/r/MachineLearning/comments/b7dwzx/help_with_downloading_siamese_rpn_pretrained_model/,keoxkeox,1553968126,[removed],0,1,False,self,,,,,
1893,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,4,b7ewwu,self.MachineLearning,how would I make this idea work for artificial general intelligence,https://www.reddit.com/r/MachineLearning/comments/b7ewwu/how_would_i_make_this_idea_work_for_artificial/,loopy_fun,1553973657,[removed],0,1,False,self,,,,,
1894,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,6,b7g2ba,self.MachineLearning,All PhD applications rejected. Work or try again?,https://www.reddit.com/r/MachineLearning/comments/b7g2ba/all_phd_applications_rejected_work_or_try_again/,csshoi,1553980265,[removed],0,1,False,self,,,,,
1895,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,6,b7g7d4,self.MachineLearning,[D] All PhD applications rejected. Work or try again?,https://www.reddit.com/r/MachineLearning/comments/b7g7d4/d_all_phd_applications_rejected_work_or_try_again/,csshoi,1553981096,"Dear fellow scholars, I got rejected from all PhD programs I applied so I'd like to know what people in ML think the best option is.

&amp;#x200B;

**&lt;Background &amp; Problem&gt;**

I'm 28 years old Canadian with Bachelors in Computer Engineering (CGPA: 3.45/4.0) and Masters in Electrical engineering (CGPA: 3.58/4.0). My masters thesis was about video stitching. I wanted to work to on AI/ML since 25 but it was after I graduated from the undergrad, and vido stitching project was the only place I got accepted so I had no choice. I feel so burned out after years of trying to learn knowledge required for my masters thesis trying to understand trend and SOTA methods in AI/ML (to prepare PhD applications). Thus, new discoveres doesn't excite me as much as before.

&amp;#x200B;

My GF is 27. Since we had long-distance relationship (LDR) during my Masters, she doesn't want to do LDR anymore. Also, her moving to another city is out of option.

&amp;#x200B;

**&lt;Career Goals&gt;**

My long term goal is to understand the computational aspect of what makes our brain so intelligent and reverse engineer it to design smarter AI. This is what I want to do during my PhD. Regarding getting a job, my goal is to work on machine learning related job.

&amp;#x200B;

**&lt;Options&gt;**

I think I have two options. Feel free to discuss other alternatives I should consider as well.

&amp;#x200B;

Option 1: Try working few years to gain real world experience

Pros:

* Hands on experience on real-world problem
* Money
* May find the passion back
* No more long distance

Cons:

* May not be able to apply PhD later due to my age and new responsibilities
* May not be able to work on problems related to my long term goal

&amp;#x200B;

Option 2: Apply PhD again

Pros:

* Closely aligns with the long term goal
* Able to apply jobs requires PhD degree

&amp;#x200B;

Cons:

* Not able to save any money before graduation
* Must break up with GF if I didn't get into local PhD program (only 2 schools)

&amp;#x200B;

**tl;dr**: Didn't get into any PhD. Not sure I should work first or try PhD again.",120,208,False,self,,,,,
1896,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,8,b7hd45,self.MachineLearning,[D] How did you learn machine learning?,https://www.reddit.com/r/MachineLearning/comments/b7hd45/d_how_did_you_learn_machine_learning/,Mjjjokes,1553988369,"I took part of Andrew Ng's machine learning course on coursera, and I took one course out of Andrew Ng's deep learning specialization (the first one), and so far right now I am only comfortable with using something like sklearn to do machine learning. 

I am wondering, how did the pros here learn?",2,0,False,self,,,,,
1897,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,9,b7ht1f,self.MachineLearning,What are some applications of Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/b7ht1f/what_are_some_applications_of_machine_learning/,ArtificialReddit,1553991273,[removed],0,1,False,self,,,,,
1898,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,12,b7jagp,self.MachineLearning,[D]What slides template or slides maker did David Silver use for his RL course?,https://www.reddit.com/r/MachineLearning/comments/b7jagp/dwhat_slides_template_or_slides_maker_did_david/,Modric799,1554001748,"Recently I want to make a representation for my lab, however I am quite new in this area. The first thing I came up with is to choose a good template so that I can have clear logic in my slides, and David Silver's impressed me! I am not very sure that I should post it here, but I do not know more accurate sub-reddit to post. Thanks for all!

If you are interested, plz have a look at these slides on UCL official sites:  [http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html) ",5,3,False,self,,,,,
1899,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,12,b7jo89,pnas.org,Biophysicists have used machine learning tools to understand &amp; predict dynamics of worm behavior.,https://www.reddit.com/r/MachineLearning/comments/b7jo89/biophysicists_have_used_machine_learning_tools_to/,MistWeaver80,1554004697,,0,1,False,default,,,,,
1900,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,13,b7jpg7,self.MachineLearning,[P]My Machine Learning Journal #7: Making my first GAN for generating MNIST-like data!,https://www.reddit.com/r/MachineLearning/comments/b7jpg7/pmy_machine_learning_journal_7_making_my_first/,RedditAcy,1554004951,"Hey, welcome to part 7 of my journey. Click [this](https://www.youtube.com/watch?v=mYgJ4IQQ0JE) for the vlog version if you are interested. 

Anyways, so yesterday I looked into a [great github page for GANs](https://github.com/nashory/gans-awesome-applications) which includes the important papers and awesome applications. I spent a great chunk of time reading through the original GANs paper, co-authored by Ian GoodFellow. So the idea of GAN becomes this: we have generator G and discriminator D, discriminator D might get data from the real distribution or fake generated ones, and it computes its loss accordingly. G also gets better in this manner. 

I couldn't wait to implement this in python. I looked into a Keras implementation of DCGANs, a type of GAN that utilizes convolution layers to learn more complex stuff and increase accuracy. Again, I am always surprised of just how a whole paper can be implemented with these extremely high leveled libraries in just a few lines. There were a few lines of code I did not understand at all at first: 

    X = np.concatenate((image_batch, generated_images))
    y = [1] * BATCH_SIZE + [0] * BATCH_SIZE
    d_loss = d.train_on_batch(X, y)

I thought that \[1\] \* BATCH\_SIZE returns \[1\*BATCH\_SIZE\] but it actually returns an array filled with 1 and of the size of BATCH\_SIZE. So if batch\_size was 128, I would get an array of length 128 filled with 1's. But the "" + \[0\] \* BATCH\_SIZE"" confused me too. Wouldn't that mean the same thing? Turns out  the plus sign actually works as an appender or concatenator instead of a adding values together in this case. Now it made perfect sense! Image\_batch was the real distribution, generated\_images were the fake and generated pictures, and \[1\] \* BATCH\_SIZE + \[0\] were their corresponding labels (real distribution are 1, fake ones are 0)! 

That really was just it, my generator was able to generate some pretty good pictures after 100 epochs which was around like 1 hr of training time? 

&amp;#x200B;

[oh yea](https://i.redd.it/lwi9e3boodp21.png)

It was also interesting to me how I just ran load\_MNIST\_data() to get the dataset. I think these significant datasets are built in to Keras, so maybe the infamous **house dataset** and wine dataset are built in to some libraries too. ",1,15,False,https://b.thumbs.redditmedia.com/8fHUIFEsFvEd795JsaPkkmPBU-eP2ZqXFQ3sfLHmxHU.jpg,,,,,
1901,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,13,b7jw61,self.MachineLearning,Naive bayes classifier doubt ?,https://www.reddit.com/r/MachineLearning/comments/b7jw61/naive_bayes_classifier_doubt/,ashutosj,1554006370,[removed],0,1,False,self,,,,,
1902,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,13,b7k1y7,m-cacm.acm.org,Researchers have come up with a novel machin learning model that predits recovery time from sport-related concussions based on the symptoms.,https://www.reddit.com/r/MachineLearning/comments/b7k1y7/researchers_have_come_up_with_a_novel_machin/,Science_Podcast,1554007662,,1,1,False,default,,,,,
1903,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,14,b7khov,/r/MachineLearning/comments/b7khov/r_boaton_dynamics_robot_doing_heavy_warehouse_work/,[R] Boaton dynamics robot doing heavy warehouse work.,https://www.reddit.com/r/MachineLearning/comments/b7khov/r_boaton_dynamics_robot_doing_heavy_warehouse_work/,navin49,1554011270,,0,1,False,https://b.thumbs.redditmedia.com/Ukm_D8Inc6gBdRyp3HiYyG-PIYwYJ7sZUvmTIMD_PTM.jpg,,,,,
1904,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,15,b7kmt4,self.MachineLearning,[R] IBMs New Quantum Algorithm Can Make Artificial Intelligence SuperPowerful,https://www.reddit.com/r/MachineLearning/comments/b7kmt4/r_ibms_new_quantum_algorithm_can_make_artificial/,navin49,1554012537,"Artificial intelligence and quantum computing are one of the most powerful technologies and soon both of them are going to revolutionize our old way of computing information.

Though the certain aspects of their mathematical foundation are little different,the combination of both the technologiesshows a promising boost in many different areas such asaccessing more computationally complex feature spaces.

IBM recently released its new research, IBM researchers presents a newdeveloped and tested quantum algorithms thatcould sort and classify complex data sets faster than that of normal algorithms running on classical computers struggle to handle.

If we talk about ordinary computers, they perform machine learning by comparing mathematical representations of data.The more precisely that data can be classified according to specific characteristics, or features, the better the AI will perform.

IBMquantum algorithmsdemonstrate how quantum computing can be usedto classify data with the use ofshort-depth circuits,that also dealt with the expected decoherence (loss of state) in a quantum computer.

**Here is in-detail report.**

*What you think, how Quantum Computers will revolutionize the field of AI and much boost it can give to it's processing.* ",4,0,False,self,,,,,
1905,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,15,b7kp87,mlwhiz.com,NLP Learning Series: Part 4 - Transfer Learning Intuition for Text Classification,https://www.reddit.com/r/MachineLearning/comments/b7kp87/nlp_learning_series_part_4_transfer_learning/,kiser_soze,1554013164,,0,1,False,default,,,,,
1906,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,16,b7kzxu,heartbeat.fritz.ai,From Y=X to Building a Complete Artificial Neural Network,https://www.reddit.com/r/MachineLearning/comments/b7kzxu/from_yx_to_building_a_complete_artificial_neural/,AhmedGadFCIT,1554015836,,0,1,False,default,,,,,
1907,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,16,b7l7u2,self.MachineLearning,[D] Do convolutional neural nets require the channels of an input to be aligned?,https://www.reddit.com/r/MachineLearning/comments/b7l7u2/d_do_convolutional_neural_nets_require_the/,GrumpyGeologist,1554017855,"Lately I've been delving deeper into convolutional neural networks (covnets), and I came up with the following thought to which I haven't yet found an answer on the interwebs: 

Suppose I have some one-dimensional time-series data (for instance, continuous temperature measurements). The patterns in these data can be efficiently extracted by a covnet. If I have multiple instances of the same data set, say the raw data, low-pass filtered data, and high-pass filtered data, I could stack them into channels and feed this stack into the covnet (similar to having RGB images as input). But what happens if there is an offset in the different channels? Or what if a channel exists in a different domain entirely (e.g. taking the Fourier transform of the data)? Can a covnet still do its job, or will it get confused by the misalignment or incompatibility of the different channels?",21,21,False,self,,,,,
1908,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,17,b7li2l,self.MachineLearning,"[D] What are the problems faced related to space exploration, data analysis and other sub fields in space that can be solved with AI or ML?",https://www.reddit.com/r/MachineLearning/comments/b7li2l/d_what_are_the_problems_faced_related_to_space/,lordpmk21,1554020526,I would like to do a project in that area if it is of significant use. ,4,7,False,self,,,,,
1909,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,19,b7m5qb,self.deeplearning,Deep Learning PC Spec... Is it any good,https://www.reddit.com/r/MachineLearning/comments/b7m5qb/deep_learning_pc_spec_is_it_any_good/,whoisweaknow,1554026518,,0,1,False,default,,,,,
1910,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,22,b7noov,self.MachineLearning,"A dataset which is bigger than cifar100, smaller than Imagenet!",https://www.reddit.com/r/MachineLearning/comments/b7noov/a_dataset_which_is_bigger_than_cifar100_smaller/,nodet07,1554038486,[removed],0,1,False,self,,,,,
1911,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,22,b7o1dt,self.MachineLearning,How to perform softmax and regression both?,https://www.reddit.com/r/MachineLearning/comments/b7o1dt/how_to_perform_softmax_and_regression_both/,yugeshurs,1554040785,"I have a dataset that requires me to predict both the classes and a value ""emitted"" by each class. The classes are set of items  that emit a particular amount of radiations. Any comment would be appreciated.",0,1,False,self,,,,,
1912,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,23,b7o1r9,github.com,NVlabs/stylegan: StyleGAN - Generate fake pictures,https://www.reddit.com/r/MachineLearning/comments/b7o1r9/nvlabsstylegan_stylegan_generate_fake_pictures/,liranbh,1554040849,,1,1,False,default,,,,,
1913,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,23,b7ohpe,self.MachineLearning,[D] PhD or work,https://www.reddit.com/r/MachineLearning/comments/b7ohpe/d_phd_or_work/,notslowingdown,1554043473,"Saw some nice discussion around a similar [topic](https://www.reddit.com/r/MachineLearning/comments/b7g7d4/d_all_phd_applications_rejected_work_or_try_again/), so was hoping to get advice on my similar situation.

I've been working as a software engineer for a few years now and applied to CS PhD programs last fall with a focus on ML. My work is not directly related, but like many here I've been studying nights and weekends for the last few years. I've written a few papers, but nothing that got published. I wasn't expecting much when I applied, but I was accepted into a few programs in the rank 10-20 range (according to csrankings). Not super top programs, but my research interests actually align very closely with the professors willing to work with me.

My dilemma is I have a wife and kids that I'm supporting with my salary. The PhD programs are all funded though and I've saved enough over the years that I think we can still live comfortably on a PhD stipend, but we will need to reach into our savings. The other consideration is the horror stories I hear about raising a family during a PhD program. My family is my #1 priority now so I hope I can at least maintain some work(school)-life balance.

My goals with the program are to really immerse myself in research full time (as opposed to just nights and weekends) with a professor that is willing to mentor me. I don't expect to stay in academia, so I'm mostly looking at research positions in industry after graduating. I've done work as an ML engineer in the past (not currently though) and understand that is an option for me immediately, but it doesn't interest me as much as doing research.

I understand this is a very personal decision I need to make with my family, but I wanted to ask the experts here what the value of a PhD degree actually is in terms of getting a research position in industry. I've actually tried applying multiple times to both research positions and AI fellowships with no luck. If I decide to work and switch to an ML engineer role (which I think I can get), is it easy to transition into more research-oriented roles without a PhD?

&amp;#x200B;

&amp;#x200B;",96,103,False,self,,,,,
1914,MachineLearning,t5_2r3gv,2019-3-31,2019,3,31,23,b7omv4,self.MachineLearning,Conjugate Gradient Optimization for Neural networks,https://www.reddit.com/r/MachineLearning/comments/b7omv4/conjugate_gradient_optimization_for_neural/,AbinavR,1554044341,[https://en.wikipedia.org/wiki/Conjugate\_gradient\_method](https://en.wikipedia.org/wiki/Conjugate_gradient_method) from what I understand the Conjugate gradient method is faster and cheaper when compared to normal gradient descent. Why not use this over SGD or other optimization techniques for NN.,0,1,False,self,,,,,
