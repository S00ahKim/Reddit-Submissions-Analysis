,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2014-3-1,2014,3,1,10,1z8bsx,Advanced Neural Networks Online Video Lecture Series by Hugo Larochelle,https://www.reddit.com/r/MachineLearning/comments/1z8bsx/advanced_neural_networks_online_video_lecture/,[deleted],1393638868,"If you're like me and want to strengthen your neural network muscles, then this is a good follow on to the Machine Learning MOOC by Andrew Ng. It has a lot more mathematical depth than Hinton's MOOC and for me a lot easier to follow as a result. 
# Chapters:
* Intro/Refresher of Linear Algebra &amp; Python  
* Feedforward Neural Nets &amp; Backpropagation  
* Conditional Probability Distributions
* Conditional Random Fields
* Restricted Boltzmann Machines
* Autoencoders
* Deep Learning
* Sparse Coding
* Advanced Topics/Applications of Neural Nets

The website is listed in French language but all the video lectures, notes, and publication links are English language.
",1,1
1,2014-3-1,2014,3,1,11,1z8dm4,Advanced Neural Networks Course by Hugo Larochelle,https://www.reddit.com/r/MachineLearning/comments/1z8dm4/advanced_neural_networks_course_by_hugo_larochelle/,[deleted],1393640175,"If you're like me and want to strengthen your neural network muscles, then this is a good follow on to the Machine Learning MOOC by Andrew Ng. It has a lot more mathematical depth than Hinton's MOOC and for me a lot easier to follow as a result. 
# Chapters:
* Intro/Refresher of Linear Algebra &amp; Python  
* Feedforward Neural Nets &amp; Backpropagation  
* Conditional Probability Distributions
* Conditional Random Fields
* Restricted Boltzmann Machines
* Autoencoders
* Deep Learning
* Sparse Coding
* Advanced Topics/Applications of Neural Nets

The website is listed in French language but all the video lectures, notes, and publication links are English language.",0,1
2,2014-3-1,2014,3,1,11,1z8edq,Advanced Neural Networks Course by Hugo Larochelle,https://www.reddit.com/r/MachineLearning/comments/1z8edq/advanced_neural_networks_course_by_hugo_larochelle/,[deleted],1393640775,,20,48
3,2014-3-1,2014,3,1,14,1z8t05,Seeking literature on using neural networks for understanding,https://www.reddit.com/r/MachineLearning/comments/1z8t05/seeking_literature_on_using_neural_networks_for/,nullsigma,1393652222,"I just took an undergrad class on AI. We covered feed-forward neural networks but most of the material was on tuning them for better prediction results. One thing I found interesting when we developed a 3-layer net for gender classification with pictures of faces was that each of the hidden nodes ended up corresponding to very high level human-understandable features (the presence of a mustache/beard, thickness of eyebrows). Is there any good material to read about using neural networks for understanding rather than just prediction?",5,5
4,2014-3-1,2014,3,1,22,1z9gx4,Basic question about pattern recognition.,https://www.reddit.com/r/MachineLearning/comments/1z9gx4/basic_question_about_pattern_recognition/,gwtkof,1393679994,"Given a finite set of points in the plane such that no two of them share the same x coordinate, it is easy to find infinitely many polynomials which go through all of these points. So how is it possible to detect a pattern from discrete binary data?",11,8
5,2014-3-2,2014,3,2,2,1z9ydz,Physics bachelor's to online CS/ML master's. Need advice &amp; sanity check.,https://www.reddit.com/r/MachineLearning/comments/1z9ydz/physics_bachelors_to_online_csml_masters_need/,csp256,1393694236,"I am an undergraduate student with a 3.87 major GPA in Applied &amp; Theoretical Physics who will graduate in about a year. I want to get a masters in machine learning and need a sanity check on my career goals (especially since I am considering Georgia Tech's online CS masters program). I am also very interested in computer vision. Below are two sections: background about me and a list of questions I have. Feel free to skim/skip the first section. 

&amp;&amp;

&amp;&amp;

I have a certain wanderlust in me, and nothing to keep me from traveling (but money). My current plan is to teach English for a couple of years in South Korea after graduation, while enrolling in Georgia Tech's online computer science masters (machine learning specialty). That teaching position offers a strong compensation package and lots of free time. I would be able to put about 30,000 USD into savings in 2 years, even after the cost of the masters program. 

My resume is interesting and getting stronger. I have 5 years experience teaching robotics and programming to middle &amp; high school students. I worked as a (supernumerary) software engineer for a few months. I was president of the Unmanned Aerial Vehicle Club at my university and lead of a competition robotics team (for which I used OpenCV). I will be studying abroad in the fall (Norway). I am almost certain to get into a great robotics/computer vision undergraduate research position (REU) at a big-name school this summer (for once nepotism works in my favor!). I have done a number of self-directed programming projects including: computational fluid dynamics, simulation of plasma shocks in the interstellar medium/solar wind interface, motion planning, extended Kalman filters, electromechanical control systems, basic FPGA design, etc.

I am a self taught programmer, but I have a much stronger focus in algorithms and their analysis than that makes me sound. The only formal university courses I have taken are a first class on discrete structures, a first class on data structures, digital logic design (I built an 8 bit computer!), and a graduate level GPGPU computing course. I got an A+ in each, and switched majors when I realized the computer science and computer engineering programs at my university were diploma mills and not at all challenging. They have basically nothing to offer me, even at the graduate level. Thankfully, the physics program at my university is very mathematically intensive (even compared to other physics programs), despite not being very highly ranked.

I have taken online classes through MIT OpenCourseWare, Coursera, and elsewhere in machine learning, artificial intelligence, scientific computing, linear algebra/statistics, GPGPU computing, and many other things. My ability to self-teach as my strongest strength. I am really focusing the next year on developing an even stronger independent work ethic: where I can not just self-study a subject, but actually *work* my way through the problems in the standard literature on a subject without any outside direction.

I am particularly interested in computer vision and machine learning. I have read a few books on the subject, and am trying to put together a curriculum for myself to prep for grad school. I will be getting a PhD. I say this because I am chasing erudition, not because I believe it is strictly a good return on investment. I don't really care for money, beyond financial security and the freedom to travel.

&amp;&amp;

&amp;&amp;

Is a masters in computer science taken online worth anything? I am only considering it because of the name of the school attached (Georgia Tech). If your answer is ""it is what you make out of it"", then I have no worries. I am driven, and extremely studious.

What other online masters programs are worth considering?

By unfortunate happenstance the two weakest areas in my undergraduate transcript are linear algebra and probability/statistics. I have been, and will be, doing everything I can to fix this before graduation. Specifically, I have done a lot of independent study, I will take upper-level electives not required for my degree program, and I will get high marks in them. What else can I do to prove my mathematical preparation?

Searching through /r/machinelearning I see Kaggle brought up frequently. Any other ideas on what I can do to prepare myself for graduate school?

I am doubtlessly missing some of the background of someone with a CS/CPE degree. What can I best do to fix this; what are the most important things to take away from an undergraduate degree? I will also be talking to the people at Georgia Tech asking them specifically what they would like to see on my transcript. 

What are the biggest questions I am not asking? What general advice do you have? What are the best resources for me to know about?",17,7
6,2014-3-2,2014,3,2,8,1zax3h,Machine Learning at Scale,https://www.reddit.com/r/MachineLearning/comments/1zax3h/machine_learning_at_scale/,agconway,1393716800,,0,2
7,2014-3-2,2014,3,2,8,1zaxez,What problems are solved in machine learning...,https://www.reddit.com/r/MachineLearning/comments/1zaxez/what_problems_are_solved_in_machine_learning/,BSscience,1393717027,... with what degere and using what techniques?,1,0
8,2014-3-2,2014,3,2,8,1zaxtb,Moose Cave: Cross Validation is Over - Long Live Double One-Step Ahead OOB,https://www.reddit.com/r/MachineLearning/comments/1zaxtb/moose_cave_cross_validation_is_over_long_live/,hyphypants,1393717307,,1,0
9,2014-3-2,2014,3,2,11,1zbagd,ML troubleshooting: I'm looking for advice on classifying my data.,https://www.reddit.com/r/MachineLearning/comments/1zbagd/ml_troubleshooting_im_looking_for_advice_on/,conic_relief,1393726167,"Lets say I have an input with a range of 0-255. I know in advance that all data will fall close to 3 values within that range. What I do not know, is what those three values are, and how much of my data will be classified under each category.  
  
In what ways would you determine what these three values are, and it what way would you determine the acceptable thresholds/margins for effective classification?  
   
  

Bonus: If you guys help me find a good solution, I will do a thorough write-up of my implementation and share it the r/machinelearning community.",13,5
10,2014-3-2,2014,3,2,22,1zcd5d,Thoughts on using quantum computing to increase the intelligence of AI?,https://www.reddit.com/r/MachineLearning/comments/1zcd5d/thoughts_on_using_quantum_computing_to_increase/,[deleted],1393765594,"Hey everyone. I am not an expert in machine learning but an avid futurist and love thinking about the disruptive impact AI will have on our future. I thought this might be of interest to you.


An important factor of intelligence is the ability to analyze all potential decisions and their outcomes quickly and then make the decision that has the highest probability of achieving one's goals.


For example: We as humans make a choice of whether or not to go to a networking event. This may help us obtain a job in 5 years. Instead of going to the networking event we could have read a book or gone to lunch with friends. The book might give us an idea for a startup and lunch with friends might strengthen our relationship and make us happier.



The amount of time we spend thinking about a decision is based on how important it is. We don't spend much time thinking about where to go to lunch because it will not have a huge impact on our lives in 5 years but we do spend a lot of time thinking about the university we will attend because it has a huge impact on our future.


For example the decision of whether to go to Subway or McDonalds for lunch would be of minor importance - you would look a month into the future. Is the 1,000 calorie Big Mac going to add too many calories and make you lose the flat stomach for the upcoming Spring Break trip or is it worth the extra happiness to eat those delicious fries. A decision of which university to attend would be critically important and you would look 70 years into the future calculating how the location would impact happiness, how alumni might impact future career opportunities and if current student have similar interests as you.


Could quantum computing be used by AI to make better decisions and improve it's intelligence? It would cap how far it would calculate future outcomes based on the relative importance of the decision.",2,0
11,2014-3-2,2014,3,2,23,1zci1b,"Domino Data Lab lets you run R, Python, and Matlab code in the cloud, with automatic version control and collaboration for data, code, and results.",https://www.reddit.com/r/MachineLearning/comments/1zci1b/domino_data_lab_lets_you_run_r_python_and_matlab/,urish,1393770915,,26,56
12,2014-3-3,2014,3,3,7,1zdnts,Classyfing Words,https://www.reddit.com/r/MachineLearning/comments/1zdnts/classyfing_words/,lormayna,1393798210,"Hello, I extract all the tags from my RSS feeds.
I would to classify the documents in some group by meaning.
Which algorithm or method may I use?

I try with Latent Semantic Analysis, but the results was not good.

",3,5
13,2014-3-3,2014,3,3,11,1zecx5,Deft Design Deck Floor Roll Forming Machine With Great Performance,https://www.reddit.com/r/MachineLearning/comments/1zecx5/deft_design_deck_floor_roll_forming_machine_with/,shlytemple,1393813607,,0,1
14,2014-3-3,2014,3,3,11,1zeeu1,Decision tree is showing signs of previous trial bias when testing,https://www.reddit.com/r/MachineLearning/comments/1zeeu1/decision_tree_is_showing_signs_of_previous_trial/,[deleted],1393814840,"Essentially I'm trying to test a decision tree I coded up in python which at the end of a trial outputs the f1 score and the accuracy. I perform 5 trials which each shuffle the dataset, use half for training, and use 10 random samples of another 20% of the dataset (taken out of the other 50% portion) to test, finally averaging all the test scores of the 10 test sets.

So in the end I have five trials outputted, each of which *should* be completely independent and very similar. Unfortunately, that doesn't seem to be the case. 

Here's an example of a run I just performed:

.

Trial 1:

Time to build tree: 113.83 seconds

F1: .3714

Correct: .7041

.

Trial 2:

Time to build tree: 139.58 seconds

F1: .3536

Correct: .699

.

Trial 3:

Time to build tree: 142.68 seconds

F1: .3479

Correct: .6962

.

Trial 4:

Time to build tree: 145.17 seconds

F1: .3384

Correct: .7027

.

Trial 5:

Time to build tree: 160.54 seconds

F1: .3123

Correct: .7187

.

As you can see, each subsequent trial takes longer and has a lower f1 score. The same pattern occurs across multiple testing runs. What could be causing this to happen? All the variables are recalculated after each trial, so there shouldn't be any leakage from one to the next. I reshuffle the data after each trial, and I even try setting all the variables to None and running the garbage collector after each trial just in case. Here's the main method I'm running to test:

    data = loadData(labels, condition)
	for j in range(1,6): # Trial number
		for i in range(40,41,10): # Depth limit of tree
			shuffle(data)
			train = data[len(data)/2:]
			test  = data[:int(len(data)*.2)]
			
			s = time()
			root = Node(train, depth = i)
			print i,': Time to build tree: ',time()-s
			
			f1, acc = testTree(root, test, labels, condition)
			print f1, acc, '\n'
			train = test = root = None
			gc.collect()
		print",0,0
15,2014-3-3,2014,3,3,20,1zfckw,Measuring the Social Media Popularity of Pages with DEA in JAVA,https://www.reddit.com/r/MachineLearning/comments/1zfckw/measuring_the_social_media_popularity_of_pages/,datumbox,1393845170,,0,7
16,2014-3-3,2014,3,3,21,1zfhfx,Machine Learning : Classification algorithm for very high dimensional data which is uniquely definable in a very small sub-space,https://www.reddit.com/r/MachineLearning/comments/1zfhfx/machine_learning_classification_algorithm_for/,harshhemani,1393851109,"I am new to machine learning, so forgive me if i am doing something absolutely absurd.

I have a classification task (~100 classes) and have about 2 million training data points in a 2000 dimensional space. Coordinates of data points are integers (discrete). All points have non-zero coordinates only for &lt; 10 dimensions. That is, each point can be uniquely defined in &lt; 10 dimensional sub-space.

If i use a Gaussian Mixture Model (GMM) for each class, i will end up with ~100 GMMs in a 2000 dimensional space. I feel that given the fact that each point is uniquely definable in less than 10 dimensional space, there can possibly be a better way of doing it.

What am i missing here?",9,13
17,2014-3-3,2014,3,3,23,1zfmon,"Genetic data, large matrices and glmnet()",https://www.reddit.com/r/MachineLearning/comments/1zfmon/genetic_data_large_matrices_and_glmnet/,FlavioBarros,1393856060,,0,2
18,2014-3-4,2014,3,4,2,1zg32l,"Review of the Coursera's ""Machine Learning"" class that starts today (03/03)",https://www.reddit.com/r/MachineLearning/comments/1zg32l/review_of_the_courseras_machine_learning_class/,Inori,1393866879,,20,30
19,2014-3-4,2014,3,4,4,1zghiy,Machine learning in 10 pictures,https://www.reddit.com/r/MachineLearning/comments/1zghiy/machine_learning_in_10_pictures/,[deleted],1393874954,,0,1
20,2014-3-4,2014,3,4,10,1zhlal,"Proportion of time spent doing what, and how much do you like your work, and how much ""success"" do you enjoy?",https://www.reddit.com/r/MachineLearning/comments/1zhlal/proportion_of_time_spent_doing_what_and_how_much/,[deleted],1393897099,"Just curious. Personally, as a masters student, I'm spending about 40% of my time coding simulations, and adjusting code relating things, 40% of my time writing and editing, and 20% of my time reading and working through proofs. It also helps to add that I'm almost done my thesis, so I'm polishing.

Not sure how successful I will be, but I'm positive I'll graduate and achieve some initial publication(s).

I'm a little bored of my topic, and would prefer more reading and discussion in the mix, but a person can't do everything! what about you guys?",3,1
21,2014-3-4,2014,3,4,11,1zhp0n,Ideas for a deep learning project?,https://www.reddit.com/r/MachineLearning/comments/1zhp0n/ideas_for_a_deep_learning_project/,LionSupremacist,1393899274,"Hi!
I am currently taking a graduate level class in Machine learning at my University. I have a strong background in mathematical optimization(linear, non linear, integer, stochastic) and have in the past worked on projects using SVMs, decision trees, Naive bayes(and its variants). So, I was thinking about doing an implementation based project on deep learning wherein I am able to use some of my optimization knowledge. Could you guys give me some ideas and/or point me to some good resources and data sets?
Thanks.",2,5
22,2014-3-4,2014,3,4,13,1zi4us,Machine learning in 10 pictures (X-Post r/programming),https://www.reddit.com/r/MachineLearning/comments/1zi4us/machine_learning_in_10_pictures_xpost_rprogramming/,hourwithoutaname,1393909046,,14,125
23,2014-3-4,2014,3,4,15,1zia57,Websites that can receive raw data input and output a pattern?,https://www.reddit.com/r/MachineLearning/comments/1zia57/websites_that_can_receive_raw_data_input_and/,Nonabelian,1393912857,"I recently came across the concept of SVM and machine learning. I'm very new to it so I'm trying to understand it and apply it to my work as quickly as possible before my project ends/is terminated. So far, understanding and applying machine learning sounds like the heavenly answer to my problem.

I'm given all of these attributes of people who set an appointment with our company and people who didn't set an appointment. So we are trying to figure out the demographic of people who will set an appointment (so we can target them better). Initially I mapped everything to {0,1,} buckets. i.e. output: Getting an appointment is {0,1}. Input: Are you married {0,1}? Are you a homeowner {0,1}? It kind of got tricky with continuous data e.g. age? home market value? But I kind of arbitrarily split them into buckets e.g. Age less than 50 {0,1}?

A software engineer working in market research described machine learning as finding patterns from large amounts data. Apparently, we can input all of our attributes and it goes through this black box and poof! comes an answer. Is this an oversimplification?

I'm reading many articles and tutorials on machine learning and SVM, but I can't find much applications. I would *love* some magic software/website where I can input all of my data and it gives me an answer - does this exist?

So far, my Excel regression is giving me really bad models... with high p-values and bad overall fit. :-(

I would appreciate any insight into this! More applications-oriented articles? Or websites that can receive input and generate some pattern?

I am an undergraduate student majoring in mathematics. This is my first internship where I am actually processing raw data and I have to make recommendations for my boss. ",1,1
24,2014-3-4,2014,3,4,21,1ziwkb,"Coursera's ""Machine Learning"" study group",https://www.reddit.com/r/MachineLearning/comments/1ziwkb/courseras_machine_learning_study_group/,[deleted],1393937103,"I was thinking about giving this class a go, even though many of you guys here posted about how watered down that class actually is, compared to the live one (available on Stanford's website). I trust your judgment, but then again, newbie should start with something easy, right? 
If there's anyone else thinking the same, what do you say about the study group? It could be fun and we could learn alot from each other, help each other, keep our motivation high :)",2,6
25,2014-3-4,2014,3,4,22,1zixl6,Best Toroidal Winding Machine Manufacturers in India,https://www.reddit.com/r/MachineLearning/comments/1zixl6/best_toroidal_winding_machine_manufacturers_in/,uday02,1393938142,,0,1
26,2014-3-5,2014,3,5,0,1zj9mf,A Comparison of Event Models for Naive Bayes Text Classi cation,https://www.reddit.com/r/MachineLearning/comments/1zj9mf/a_comparison_of_event_models_for_naive_bayes_text/,pasmod,1393947082,,0,1
27,2014-3-5,2014,3,5,1,1zje5g,"[meta] Questions get downvoted in this sub, so let's put them in /r/MLQuestions",https://www.reddit.com/r/MachineLearning/comments/1zje5g/meta_questions_get_downvoted_in_this_sub_so_lets/,uber_kerbonaut,1393949704,"I've just made [/r/MLQuestions](http://www.reddit.com/r/MLQuestions/) for beginners to post machine learning questions and for experts to help them. 

It is understandable that questions are downvoted here, as this sub is for news and interesting articles, but I think that asking stupid questions is inevitable and helping to bring beginners on board is very important. Sure they can go read up on the multitude of resources out there on the net, but it's especially helpful sometimes to just hear a variety of answers to a simple question. 

If /r/MachineLearning is interested in diverting some of it's unwanted questions this way, feel free to place a link in the side bar.

If you are experienced in machine learning, please consider subscribing and answering questions from time to time.

Also, I'd like to call for a second moderator.

Thank you!",18,17
28,2014-3-5,2014,3,5,4,1zk00d,I'd like to invite everyone here to the new /r/mathematics,https://www.reddit.com/r/MachineLearning/comments/1zk00d/id_like_to_invite_everyone_here_to_the_new/,[deleted],1393961287,,2,1
29,2014-3-5,2014,3,5,5,1zk5lo,Project Idea for Machine Learning Class,https://www.reddit.com/r/MachineLearning/comments/1zk5lo/project_idea_for_machine_learning_class/,andrewff,1393964298,"Every now and then I see people ask for projects for Machine Learning classes so I thought I'd pass on an idea I had but will not have time to work on.

I think it would be really interesting to see someone attempt to remove watermarks from images using an autoencoder.  In many ways this can be see as similar to how a denoising autoencoder works, but effectively the noise would be the watermark.  It would be very easy to generate training data and it would be effectively infinitely available.

Just thought I'd throw this out there for anyone interested especially given the popularity of neural nets right now.",6,5
30,2014-3-5,2014,3,5,14,1zlozt,K-modes | The Shape of Data,https://www.reddit.com/r/MachineLearning/comments/1zlozt/kmodes_the_shape_of_data/,rrenaud,1393996892,,3,14
31,2014-3-5,2014,3,5,17,1zm1k9,Muyang Conveying System,https://www.reddit.com/r/MachineLearning/comments/1zm1k9/muyang_conveying_system/,Niki_Lei,1394008258,,0,1
32,2014-3-5,2014,3,5,18,1zm4dp,"AI, planning and Hearthstone",https://www.reddit.com/r/MachineLearning/comments/1zm4dp/ai_planning_and_hearthstone/,[deleted],1394011663,"So recently I've been playing some [Hearthstone](http://en.wikipedia.org/wiki/Hearthstone:_Heroes_of_Warcraft) - a free online card game by Blizzard, and it made me wonder how you would program an AI to win the game. Ignoring the big hurdles of computer vision to understand the game state, and effectively re-writing the game to model all of the rules, just the logic is still a major problem.

You can read the rules on the [wikipedia page](http://en.wikipedia.org/wiki/Hearthstone:_Heroes_of_Warcraft), or watch [a few matches](http://www.youtube.com/watch?v=OE5ULrd5sr8). But it brings up many interesting problems:

* Predicting what the opponent will play:

This could be done by having priors on the different types of decks for each class (or in arena mode, where one builds a deck from random given cards, for each card based on it' place in published ranking lists). Then update this by a Bayesian method, this would then inform the posterior on what the opponent has in the hand (though this is complicated, how do you produce a posterior for a (somewhat random) sample of 5 cards, from a MAP estimate for 30 cards?).

* Holding back for better potential

For example, there is a card which deals 3 damage spread over random enemies. Some early players will use this on the first turn to deal 3 damage directly to the enemy hero (killing the enemy hero is how you win), but this is stupid, because it is incredibly unlikely that you'll be in a situation where you'd have won if you'd used the card early but are then able to do so. 

However, getting the AI to understand this is difficult, since if you just used a simple search over gamestates, then one where you did 3 damage is better than one where you don't. Of course you could add some cost to using cards, etc. but it seems a bit messy for something which seems so logical.

* Weakly informative situations:

For example, if you play a very strong minion, and the enemy doesn't use a kill card (like polymorph, etc.) on it in their turn, then you can be certain that they don't have that card in their hand. This is another tricky thing to make use of.


Anyway, I thought it was an interesting problem, I realise this probably makes no sense whatsoever if you haven't played the game.
",5,3
33,2014-3-5,2014,3,5,21,1zmcqd,Can any existing Machine Learning structures perfectly emulate recursive functions like the Fibonacci sequence?,https://www.reddit.com/r/MachineLearning/comments/1zmcqd/can_any_existing_machine_learning_structures/,[deleted],1394021967,,23,47
34,2014-3-5,2014,3,5,22,1zmfmm,Implementation of RAKE algorithm in Java,https://www.reddit.com/r/MachineLearning/comments/1zmfmm/implementation_of_rake_algorithm_in_java/,pasmod,1394025089,,0,0
35,2014-3-6,2014,3,6,3,1zn7u4,Cosma Shalizi - Advanced Data Analysis book [pdf],https://www.reddit.com/r/MachineLearning/comments/1zn7u4/cosma_shalizi_advanced_data_analysis_book_pdf/,xamdam,1394043626,,1,18
36,2014-3-6,2014,3,6,3,1zn8xz,"MIT's 15.071x, The Analytics Edge -- a course on building ML and optimization models for real world applications -- launched yesterday",https://www.reddit.com/r/MachineLearning/comments/1zn8xz/mits_15071x_the_analytics_edge_a_course_on/,NithSahor,1394044281,,3,27
37,2014-3-6,2014,3,6,7,1zo0fp,Big speedup for Random Forest learning in scikit-learn 0.15,https://www.reddit.com/r/MachineLearning/comments/1zo0fp/big_speedup_for_random_forest_learning_in/,cypherx,1394059280,,2,26
38,2014-3-6,2014,3,6,9,1zods9,Identifying Patterns in Time Series Data,https://www.reddit.com/r/MachineLearning/comments/1zods9/identifying_patterns_in_time_series_data/,r3cycle,1394067063,,0,1
39,2014-3-6,2014,3,6,10,1zoirn,Fast algorithms/indexing techniques for finding similar strings?,https://www.reddit.com/r/MachineLearning/comments/1zoirn/fast_algorithmsindexing_techniques_for_finding/,cypherx,1394070229,"If I have a large set of fixed length strings (billions), what are some fast ways of identifying each string's near neighbors under either a Hamming distance or edit distance? Either exact or probabilistic methods are interesting to me but I want to avoid spending more than nlogn time on index construction and no more than klogn time finding each set of neighbors (where n is the number of strings, k is the number of neighbors). Any links or paper recommendations appreciated.",25,17
40,2014-3-6,2014,3,6,11,1zolqc,Available at Udacity March 17: ML Courses from Georgia Tech Masters in CS,https://www.reddit.com/r/MachineLearning/comments/1zolqc/available_at_udacity_march_17_ml_courses_from/,[deleted],1394072058,"Starting March 17, 2014 Georgia Tech will hold three new courses on Udacity. These will be supervised learning, unsupervised learning, and reinforcement learning (The latter two will start later in the year). I'm personally very excited by this. Between these courses, the coursera course, and the caltech lectures it seems like there is no excuse for not being able to learn ML anymore. ",4,17
41,2014-3-6,2014,3,6,17,1zpe77,Some Useful Machine Learning Libraries,https://www.reddit.com/r/MachineLearning/comments/1zpe77/some_useful_machine_learning_libraries/,srkiboy83,1394094151,,0,3
42,2014-3-6,2014,3,6,23,1zpxij,Unsupervised learning when labels are avaiable,https://www.reddit.com/r/MachineLearning/comments/1zpxij/unsupervised_learning_when_labels_are_avaiable/,PsychedelicStore,1394115804,Are there any good reasons to use unsupervised learning techniques over supervised ones when the labels are avaiable? Thanks in advance for the opinions.,12,13
43,2014-3-7,2014,3,7,3,1zqn2p,Is affinity propagation applicable to an ACF?,https://www.reddit.com/r/MachineLearning/comments/1zqn2p/is_affinity_propagation_applicable_to_an_acf/,pablo208,1394131505, If anyone could point me to a good resource for this that would also be much appreciated,3,1
44,2014-3-7,2014,3,7,7,1zr949,"Anyone else feel like getting deep learning into the hard science community, like trying to explain magic? He's a funny story of our past year trying to accomplish that. (p.s. you all rock!)",https://www.reddit.com/r/MachineLearning/comments/1zr949/anyone_else_feel_like_getting_deep_learning_into/,[deleted],1394143921,,0,1
45,2014-3-7,2014,3,7,7,1zraw9,I'm really curious how each of you would approach solving this problem,https://www.reddit.com/r/MachineLearning/comments/1zraw9/im_really_curious_how_each_of_you_would_approach/,bluepublius,1394144914,,21,17
46,2014-3-7,2014,3,7,8,1zrft5,"Deep Learning + Hard Sciences, it's already been one hell of a ride. Here's our startups year in review.",https://www.reddit.com/r/MachineLearning/comments/1zrft5/deep_learning_hard_sciences_its_already_been_one/,ucsdsu,1394147711,,0,0
47,2014-3-7,2014,3,7,11,1zrzr5,Strategies for automatic feature generation on heterogeneous data set,https://www.reddit.com/r/MachineLearning/comments/1zrzr5/strategies_for_automatic_feature_generation_on/,Zelazny7,1394160475,"I work with public records data to predict consumer credit risk and fraud. My datasets have roughly 1,000 features of mixed data types. Following the maxim that a standard learning algorithm applied to quality features will outperform a cutting-edge algorithm applied to mediocre features I am trying to engineer some features automatically.

I am nearly always trying to predict a binary, good/bad outcome. My features are comprised of binary flags, ordinal, nominal, discrete and continuous fields in roughly equal proportions.

My question is what might be some effective strategies for creating new features on such a dataset? Most of the methods I read about are applied to mostly homogeneous datasets.  Should I break my dataset into sets of related features?

Right now, I am playing around with naive bayes classifiers as well as gradient boosting classifiers. My idea is to send small batches of related features through these algorithms to create compound features. Is this a sound approach?",3,8
48,2014-3-7,2014,3,7,14,1zsda6,Can clustering be used for time series forecasting?,https://www.reddit.com/r/MachineLearning/comments/1zsda6/can_clustering_be_used_for_time_series_forecasting/,zanzilove,1394170299,"I'm new to ML, so if anyone has any sources or links for this I'd really appreciate it! 

I'm trying to find out if you can use clustering to enhance a regular forecasting method such as regression? Like, if you could cluster each data point on whether it's rising or falling from the previous value, perhaps the different clusters could have a slightly different regression equation fit to them? 

If that makes sense?",9,11
49,2014-3-7,2014,3,7,14,1zsejy,"""Deep learning: Methods and Applications"", Deng and Yu [PDF book draft]",https://www.reddit.com/r/MachineLearning/comments/1zsejy/deep_learning_methods_and_applications_deng_and/,gtani,1394171379,,7,48
50,2014-3-7,2014,3,7,15,1zsicd,Extracting lines and features from rough image. Looking for hints and tips!,https://www.reddit.com/r/MachineLearning/comments/1zsicd/extracting_lines_and_features_from_rough_image/,stokeml,1394175038,,0,7
51,2014-3-7,2014,3,7,18,1zsp3v,Suggestions for an undergraduate thesis combining econometrics and machine learning for financial forecasting? (x-post from econometrics),https://www.reddit.com/r/MachineLearning/comments/1zsp3v/suggestions_for_an_undergraduate_thesis_combining/,zanzilove,1394183167,,3,9
52,2014-3-8,2014,3,8,0,1ztbwx,Data Analytics &amp; Visualization Blog - Toying with Topological Data Analysis (Part 1),https://www.reddit.com/r/MachineLearning/comments/1ztbwx/data_analytics_visualization_blog_toying_with/,rustyoldrake,1394206650,,2,5
53,2014-3-8,2014,3,8,3,1ztskm,Learning to detect salient object in an image based on boolean user input,https://www.reddit.com/r/MachineLearning/comments/1ztskm/learning_to_detect_salient_object_in_an_image/,Kazz47,1394216960,"I am a developer on the Wildlife@Home volunteer computing project and  am in the process of researching different supervised machine learning algorithms.

Data for the project is collected by volunteers who classifying wildlife video and the results are grouped for classifying each frame of the video. We have a set of data to determine which frames contain a bird and which do not. The goal is to use this as training data for a machine learning algorithm to predict bird presence in unwatched videos.

Do you know of any previous research in this area or have any suggestions for implementation?",4,7
54,2014-3-8,2014,3,8,5,1zu6d5,How to classify 'everything else'?,https://www.reddit.com/r/MachineLearning/comments/1zu6d5/how_to_classify_everything_else/,Should_I_say_this,1394225290,"Hypothetical scenario. 

I have 10 types of dogs and then a whole host of 'noisy samples' which can be literally anything: cats, humans, tables, phones, buildings, etc etc.

I couldn't find any resources online for how to categorize 'everything else' so I'm wondering if anyone has experience doing so?

Right now I'm leaning towards doing a classification by 2 and then further classifying the dogs by 10....",7,6
55,2014-3-8,2014,3,8,11,1zuzd6,"ToC, bibliog. and (4)draft chapters, Y. Bengio Deep Learning book",https://www.reddit.com/r/MachineLearning/comments/1zuzd6/toc_bibliog_and_4draft_chapters_y_bengio_deep/,gtani,1394244434,,0,9
56,2014-3-8,2014,3,8,15,1zvioc,Robust PCA and GoDec for background/foreground separation in video (self-post),https://www.reddit.com/r/MachineLearning/comments/1zvioc/robust_pca_and_godec_for_backgroundforeground/,kkastner,1394261041,,5,16
57,2014-3-9,2014,3,9,13,1zxz3v,"Now this might be a very dumb question, but what exactly is the difference between the terms ""Probabilistic Graphic Model"" and ""Bayesian Network Model""?",https://www.reddit.com/r/MachineLearning/comments/1zxz3v/now_this_might_be_a_very_dumb_question_but_what/,adanoopdixith,1394338478,I've often heard these two being used interchangeably. Are they one and the same? or one is a subset of another? What exactly is the difference? I've just taken Machine Learning course online and can't seem to make out the exact difference!,3,27
58,2014-3-9,2014,3,9,13,1zxzdn,Machine Monitoring Software should be adaptive and provide optimal monitoring processes,https://www.reddit.com/r/MachineLearning/comments/1zxzdn/machine_monitoring_software_should_be_adaptive/,[deleted],1394338667,,1,0
59,2014-3-9,2014,3,9,15,1zy7bu,I have experience with ML but am moving to an unfamiliar territory: sign language recognition,https://www.reddit.com/r/MachineLearning/comments/1zy7bu/i_have_experience_with_ml_but_am_moving_to_an/,randombozo,1394345643,"From what I deduct, sign language (what the deaf use) recognition would likely be a hybrid of gesture and speech recognition. 

Specifically, I'm thinking about an app in which a deaf person (which I am) can wrap a smartphone around their wrist. That approach wouldn't capture every sign but possibly just enough to be useful. At this point, I'm simply looking to experiment. 

While I've worked with ML before, this is a new territory for me. I've been going through readouts from iPhone's sensors. As you might expect, they're basically time series of motion data. How to turn them into single rows of Y -&gt; classification, X -&gt; features?

Since every signer, like speakers, delivers at varying speeds, I'm trying to figure out how to make the input less time orientated and more ""shape orientated"", if that makes any sense. To simplify this as much as possible, think of somebody circling their hand. Instead of time series, it'd be lovely to turn the input into, well, circular motions. I'm thinking perhaps polynomial regression? But how to turn time series of varying intervals into single lines of Y (classification) and Xs -&gt; polynomials?

Hope I'm somehow making sense. ",4,3
60,2014-3-9,2014,3,9,19,1zyjst,What would be a essentials only reading list for the newer supervised deep learning techniques?,https://www.reddit.com/r/MachineLearning/comments/1zyjst/what_would_be_a_essentials_only_reading_list_for/,T_hank,1394362266,"I've heard there is a recent surge in purely supervised techniques in deep learning, in contrast to earlier pretraining based models.

Which would be the essential techniques and architectures that characterize this ideology? 

And which papers would you recommend for someone familier with older work (RBMs,Autoencoders,CNN etc.) to get up to speed quickly on this?",2,3
61,2014-3-9,2014,3,9,19,1zyk4g,(200 BOUNTY) (UPDATED) Can any existing Machine Learning structures perfectly emulate recursive functions like the Fibonacci sequence?,https://www.reddit.com/r/MachineLearning/comments/1zyk4g/200_bounty_updated_can_any_existing_machine/,[deleted],1394362773,,8,3
62,2014-3-9,2014,3,9,23,1zyv79,Can a Neural Network Find the i-th Permutation of a fixed size list?,https://www.reddit.com/r/MachineLearning/comments/1zyv79/can_a_neural_network_find_the_ith_permutation_of/,[deleted],1394376951,,6,6
63,2014-3-10,2014,3,10,12,200o7y,What is it called when you use a video to train an image based neural network?,https://www.reddit.com/r/MachineLearning/comments/200o7y/what_is_it_called_when_you_use_a_video_to_train/,rememberlenny,1394420961,,12,2
64,2014-3-10,2014,3,10,16,2016ug,Two Main Types of the Engraving Machine,https://www.reddit.com/r/MachineLearning/comments/2016ug/two_main_types_of_the_engraving_machine/,laserengraveing,1394437351,,0,0
65,2014-3-10,2014,3,10,17,2018sl,Producing Small Aquafeed Pellets by Single-screw Extruder,https://www.reddit.com/r/MachineLearning/comments/2018sl/producing_small_aquafeed_pellets_by_singlescrew/,Niki_Lei,1394439833,,0,1
66,2014-3-10,2014,3,10,19,201dc1,Excellent Anti-bending Z Purlin Roll Forming Machine Is Supplied,https://www.reddit.com/r/MachineLearning/comments/201dc1/excellent_antibending_z_purlin_roll_forming/,shlytemple,1394447091,,0,1
67,2014-3-11,2014,3,11,4,202imb,How Google uses ML to serve ads: ad click prediction: A view from the trenches,https://www.reddit.com/r/MachineLearning/comments/202imb/how_google_uses_ml_to_serve_ads_ad_click/,rrenaud,1394478265,,7,27
68,2014-3-11,2014,3,11,6,202zm6,2 FREE tickets to MLconf NYC in exchange for video recording / streaming help on 4/11/14,https://www.reddit.com/r/MachineLearning/comments/202zm6/2_free_tickets_to_mlconf_nyc_in_exchange_for/,shonburton,1394487910,"We're shocked that MLconf NYC is almost sold out. We have only 12 tickets left and are still a month away from the event. We have lined up amazing speakers and would love to stream the event  for free or at least record it since we only have room for 150 people.

See http://mlnyc.eventbrite.com for details

We are looking for help recording and live streaming the event. If you are in NYC, have skill in this area and are interested in ML, please get in touch via @mlconf on twitter. Equipment is not required but is a plus if you have it. If not we can provide.",5,0
69,2014-3-11,2014,3,11,11,203pqk,Machine learning and comparing times,https://www.reddit.com/r/MachineLearning/comments/203pqk/machine_learning_and_comparing_times/,jeff3yan,1394503210,"I'm familiar with vectorizing textual data and normalizing numerical data for allowing machine-learning algorithms to find similarities.

I'm a bit stuck on comparing data such as *time of day*. If I represent it as hour of day from 0-23, then times around midnight won't be compared properly. Is there a well known method for normalizing time data so that it can be compared numerically against other times?",14,1
70,2014-3-11,2014,3,11,13,2044ic,Need direction for a retail customer cluster analysis,https://www.reddit.com/r/MachineLearning/comments/2044ic/need_direction_for_a_retail_customer_cluster/,greatluck,1394512649,"I am intensely interested in learning about machine learning techniques and I thought this would make a good project to learn on.

I have access to customer transactional data down to the SKU level with product hierachies.  I have access to customer geographic data.  I do not have demographic although I suppose I could append zip code averages based on census or other public data if this sort of thing is appropriate.


The end goal is build a group of customer profiles based on what's available.

I need help in deciding what kind of clusters/segments to attempt and what insights they could potentially bring.  Should I use K-means/ neural net/ etc...

Tools at my disposal: SQL, SAS, R (beginner).  

BONUS QUESTION: What could I do with demographic data that I can't do with the resources above?  Demo data is expensive to purchase but perhaps I could justify the expense to my employer.

Any direction would be much appreciated.",0,2
71,2014-3-11,2014,3,11,14,20498t,The Holy Grail Of Trading Has Been Found: HFT Firm Reveals 1 Losing Trading Day In 1238 Days Of Trading | Algorithms beat the market (which is essentially a consensus of millions of human mind's emotions and predictions),https://www.reddit.com/r/MachineLearning/comments/20498t/the_holy_grail_of_trading_has_been_found_hft_firm/,beefjerking,1394516298,,11,6
72,2014-3-11,2014,3,11,17,204hxq,"Betti Barcodes for Beginners (or ""R Code for your Barcode"" ;)",https://www.reddit.com/r/MachineLearning/comments/204hxq/betti_barcodes_for_beginners_or_r_code_for_your/,rustyoldrake,1394526803,,0,1
73,2014-3-11,2014,3,11,22,204vqr,"Has anyone here used the IBM Watson API? If so, how useful is it and how easy is it to work with?",https://www.reddit.com/r/MachineLearning/comments/204vqr/has_anyone_here_used_the_ibm_watson_api_if_so_how/,redmage123,1394543379,"Hi,

I was just wondering if anyone has used the IBM Watson API?  What do you all think of it?  Is it easy to use?  Easy to work with?  

",6,62
74,2014-3-11,2014,3,11,23,2052hj,Fairly basic question about neural net function approximators,https://www.reddit.com/r/MachineLearning/comments/2052hj/fairly_basic_question_about_neural_net_function/,cosmic_cow_ck,1394548468,"When using a Neural Net as a function approximator, is it standard practice to use a linear neuron on the output and thus just get the value directly, or to use good old sigmoid and scale it? And if it's the later, how is the scaling done, since sigmoid isn't just a linear scaling? ",7,1
75,2014-3-12,2014,3,12,6,2066ns,Emerging probabilites in Naive Bayes rather than static priors; am I on the right track or lost in the woods?,https://www.reddit.com/r/MachineLearning/comments/2066ns/emerging_probabilites_in_naive_bayes_rather_than/,kezalb,1394572364,"I am writing my thesis on Naive Bayes classification in a time-series context (customer churn... sorry, I know I'll annoy some of you).

My understanding is that the typical approach is to train a model on a sample set, using the Bayesian prior as the occurrence of that label in the set and then yada, yada.  Then one would apply the model to the real-time date.

I see customer (player) change in behavior as the most important aspect (demographics and other static parameters seem to have little information). So, I am trying to get time-series data into the mix.  However, it also occurs to me that the probabilities of churn associated with each player should emerge, not be retrained at each iteration. So, I would set new player likelihood to be equal to the baseline churn, but thereafter the prior probability at each iteration (daily batches) would be the previous day's forecast.  The probability would then be updated based on the observed behaviour.

As far as I can tell, this is common practice with Bayesian statistics, however, it doesn't seem to be a common way of applying naive bayes in machine learning.  Can anyone point me to either research or examples that show what I am doing is clearly not novel or discuss why my approach isn't currently used?

Any help is greatly appreciated!

",0,1
76,2014-3-12,2014,3,12,8,206nb4,Simulating Data,https://www.reddit.com/r/MachineLearning/comments/206nb4/simulating_data/,SpaceWizard,1394582077,What are your favorite packages for simulating data to test out different ML approaches?,2,3
77,2014-3-12,2014,3,12,12,20768u,AskML: How to compute marginals in Sum-Product Networks?,https://www.reddit.com/r/MachineLearning/comments/20768u/askml_how_to_compute_marginals_in_sumproduct/,[deleted],1394594002,"I remember reading some comments about Sum-Product Networks here on /r/MachineLearning, so I hope someone could help me understand this part.

I submitted the details of the question to CrossValidated: http://stats.stackexchange.com/questions/89682/how-to-compute-marginals-in-sum-product-networks

Basically, I'm trying to compute the posterior marginals of the sum nodes, i.e P(Y_k = i | e), where Y_k is a sum node, i is one of its child and e is the evidence, but for some reason I keep getting probabilities that don't depend on the evidence. 

I would really appreciate if anyone could help me with this.
",2,3
78,2014-3-12,2014,3,12,22,2086y4,"Tonight : Paris Machine Learning Meetup #9: GraphLab, LocalSolver, Import.io and Matrix Factorization and Machine Learning",https://www.reddit.com/r/MachineLearning/comments/2086y4/tonight_paris_machine_learning_meetup_9_graphlab/,compsens,1394631028,,0,14
79,2014-3-13,2014,3,13,6,209gpg,Introducing Season of NuPIC,https://www.reddit.com/r/MachineLearning/comments/209gpg/introducing_season_of_nupic/,numenta,1394658588,,2,0
80,2014-3-13,2014,3,13,8,209v4v,Startups doing Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/209v4v/startups_doing_machine_learning/,[deleted],1394666860,"Does anyone know of startups that are primarily doing Machine Learning (including deep learning, visualization, ""data science,"" computer vision, and general AI)?

Also, if anyone can point me to a list of startups doing ML, that'd be great. 

Thanks!",8,2
81,2014-3-13,2014,3,13,14,20ard7,How Centrifugal Pumps Have Become Important for Heavy Industries,https://www.reddit.com/r/MachineLearning/comments/20ard7/how_centrifugal_pumps_have_become_important_for/,marcosesteban1,1394688724,,0,1
82,2014-3-13,2014,3,13,20,20b7ep,Brute force 'training' to find weights useful ?,https://www.reddit.com/r/MachineLearning/comments/20b7ep/brute_force_training_to_find_weights_useful/,HamAndEase,1394711161,"I was thinking that by choosing a minimum weight value, a maximum weight value and a step value, one could iterate over all weight value combinations in the range to find the best fitting weights. This could be repeated after the initial 'best' weights were found by searching a smaller range around each weight again with a smaller step value.

On a GPU, it should be possible to calculate the results in parallel for many weight sets by computing one layer at the time.
",8,1
83,2014-3-14,2014,3,14,1,20bs8w,ML video course from UBC (YT playlist),https://www.reddit.com/r/MachineLearning/comments/20bs8w/ml_video_course_from_ubc_yt_playlist/,xamdam,1394727978,,18,25
84,2014-3-14,2014,3,14,9,20d1b0,Are there any inexpensive or open source operations research applications or libraries?,https://www.reddit.com/r/MachineLearning/comments/20d1b0/are_there_any_inexpensive_or_open_source/,godless_communism,1394755297,"I apologise for using MachineLearning to ask an OR question, but the OR subreddit is basically dead, and I thought this is a close cousin.  I started learning OR back in 1992 and used a book &amp; DOS software called Storm.  

I've recently been trying to find an inexpensive or open source application or software library that solves the same kinds of problems.  Thanks for any ideas.

Also, do you think it would be OK to fold Operations Research questions into /r/MachineLearning from here on?",6,5
85,2014-3-14,2014,3,14,10,20db1q,"Newbie here. Anyone know of free AI, or MachLearn software for Win 8 pc I can get and playwith?",https://www.reddit.com/r/MachineLearning/comments/20db1q/newbie_here_anyone_know_of_free_ai_or_machlearn/,Timoleonwash,1394761765,They used to have such things in the early days and I haven't been able to find any today.,8,0
86,2014-3-14,2014,3,14,14,20dqhg,Good Papers on Multiview Learning?,https://www.reddit.com/r/MachineLearning/comments/20dqhg/good_papers_on_multiview_learning/,Omega037,1394773214,"My research team has a sorta ""journal club"" where we take turns choosing papers/topics and presenting them.  I wanted to present something on multiview learning, and was wondering if anyone had an good suggestions for papers.

",1,5
87,2014-3-14,2014,3,14,15,20duhu,Math equation could help find missing Malaysian plane,https://www.reddit.com/r/MachineLearning/comments/20duhu/math_equation_could_help_find_missing_malaysian/,meandtree,1394777193,,2,0
88,2014-3-14,2014,3,14,20,20e915,JavaScript Feed Forward Neural Network with Sigmoid Activation and Compounded MSE Fitness (CONTAINS NO TRAINER) (99 lines + XOR sample),https://www.reddit.com/r/MachineLearning/comments/20e915/javascript_feed_forward_neural_network_with/,[deleted],1394797249,,1,0
89,2014-3-14,2014,3,14,23,20emeh,Real-time NLP with Twitter and Yhat,https://www.reddit.com/r/MachineLearning/comments/20emeh/realtime_nlp_with_twitter_and_yhat/,hernamesbarbara,1394809128,,2,27
90,2014-3-15,2014,3,15,3,20f6kh,Videos of the 1st Zurich Machine Learning Meetup,https://www.reddit.com/r/MachineLearning/comments/20f6kh/videos_of_the_1st_zurich_machine_learning_meetup/,compsens,1394821964,,0,2
91,2014-3-15,2014,3,15,6,20fmym,How could I beat 2048 with ML?,https://www.reddit.com/r/MachineLearning/comments/20fmym/how_could_i_beat_2048_with_ml/,are595,1394832286,"There is already an AI for playing the game 2048 [here](https://github.com/ov3y/2048-AI). It uses minimax, but I was wondering about the possibility of using machine learning to make an AI. How would I set up a neural network or something similar to play the game?",10,3
92,2014-3-15,2014,3,15,12,20gfyy,Clustering points around a circle,https://www.reddit.com/r/MachineLearning/comments/20gfyy/clustering_points_around_a_circle/,jeff3yan,1394853421,"I've been trying to use scikit learn to cluster some points around a circle, namely timestamps represented as their cosine, sine components.

I've tried using a number of clustering methods, KMeans, DBSCAN and have tried using euclidean and cosine distance measurements, as well as using precomputed distances for DBSCAN. Precomputed distances would simply be the angular difference between points.

None of the clustering algorithms seems to give any expected results, with each cluster usually having times that are spread out around the day with heavy overlap with other clusters.

Is there something I should be looking into for this type of data, or this is a reasonable approach?",7,5
93,2014-3-15,2014,3,15,14,20gob1,API/OSS for processing handwriting to text?,https://www.reddit.com/r/MachineLearning/comments/20gob1/apioss_for_processing_handwriting_to_text/,aMLnoob,1394860718,"I'm a web developer with a brief background in machine learning. An application I'm working on has potential for an intelligent word recognition algorithm to be implemented to save a client a bunch of time. I don't have enough knowledge about CV/ML to fully understand the problem domain and potential solutions, so I thought I'd come here for advice!

**Background:**

I have 30k images containing handwritten text. They are all in English, have similar features in the fact the background color and text heavily contrast, and there is very little noise in general. Almost all of the images were written by different people with unique handwriting. Each photo is 600x600px and contains anything from a single word, up to a short paragraph. 10k of the images have been manually analyzed. I have the extracted text for these 10k images available for use as a training set.

**Problem:**

I'd like to be able to script the extraction of text for the remaining 20k images.

**Questions:**

-Is it possible to achieve accuracy of 95% on this problem with the current state of OCR/IWR/ICR?

-I've looked at some open source OCR solutions, but most seem to not have great handwriting recognition options. Are there any FOSS projects that would fit this? I looked at tesseract but I read somewhere that for handwriting, most academics were able to get at most 90%.

-Are there any proprietary APIs out there that could help with this? I don't want to buy a big enterprise product, I just need to be able to POST an image to an endpoint and get back the text.

-Other ideas on how to tackle this problem? I thought about taking the output of the OCR/IWR program and use some sort of NLP technique to further improve accuracy, but this will in be possible in some cases.

Thanks in advance!
",0,0
94,2014-3-15,2014,3,15,16,20gvom,Natural Language Processing Applied to Meeting Note-taking,https://www.reddit.com/r/MachineLearning/comments/20gvom/natural_language_processing_applied_to_meeting/,arrowoftime,1394869067,,4,1
95,2014-3-15,2014,3,15,18,20gzn9,What to do after Andrew Ng's Coursera Class?,https://www.reddit.com/r/MachineLearning/comments/20gzn9/what_to_do_after_andrew_ngs_coursera_class/,[deleted],1394875649,"I am now studying Andrew Ng's class and I was just wondering that what would be good tutorials, courses, books etc to follow up after it? 

I'm probably will start with Kaggle tutorials.  By the way, does Ng's Coursera class give enough skills to get started with Kaggle tutorials or should I learn something else before it?",19,27
96,2014-3-16,2014,3,16,1,20hjcg,Why does this sub have no wiki?,https://www.reddit.com/r/MachineLearning/comments/20hjcg/why_does_this_sub_have_no_wiki/,Droidx4_66,1394899618,"I think this sub should have a wiki containing all the sources and the courses a beginner (like me) needs to get started with learning ML. A lot of the posts I see are related to learning ML. I think someone like me would benefit from having a peer-reviewed list of sources for learning ML. I'm not sure if such a list exists somewhere else or not, but even if it does, a copy should be in the wiki for this sub. ",8,19
97,2014-3-16,2014,3,16,1,20hmjn,Regarding autoencoders and codes,https://www.reddit.com/r/MachineLearning/comments/20hmjn/regarding_autoencoders_and_codes/,[deleted],1394902145,"I was reading Hinton &amp; Salakhutdinov's 2006 paper on autoencoders and Figure 3b (as well as S4 in the Supplementary Materials) are rather confusing due the absence of any axis? Anyone knows how the figures are obtained?

Intuitively I figured that the axis are the probabilities [0,1] of the 2 hidden units on the top-most layer and the recovered digit is based on a downpass using the associated probabilities?",3,5
98,2014-3-16,2014,3,16,4,20i0vi,[META] Collection of Links for Beginners / FAQ,https://www.reddit.com/r/MachineLearning/comments/20i0vi/meta_collection_of_links_for_beginners_faq/,BeatLeJuce,1394912442,"**MOOCs**

Nowadays, there are a couple of really excellent online lectures to get you started.
The list is too long to include them all. Every one of the major MOOC sites offers
not only one but several good Machine Learning classes, so please check
 [coursera](https://www.coursera.org/), [edX](https://www.edx.org/), 
 [Udacity](https://www.udacity.com/) yourself to see which ones are interesting to you.
 

However, there are a few that stand out, either because they're very popular
or are done by people who are famous for their work in ML. Roughly in order 
from easiest to hardest, those are:

* Andrew Ng's [ML-Class at coursera](https://www.coursera.org/course/ml): Focused
 on application of techniques. Easy to understand, but mathematically very shallow.
 Good for beginners!

* Hasti/Tibshirani's [Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/): Also aimed at beginners and focused more on applications.

* Yaser Abu-Mostafa's [Learning From Data](https://www.edx.org/course/caltechx/caltechx-cs1156x-learning-data-1120):
 Focuses a lot more on theory, but also doable for beginners
 
* Geoff Hinton's [Neural Nets for Machine Learning](https://www.coursera.org/course/neuralnets):
 As the title says, this is almost exclusively about Neural Networks. 

* Hugo Larochelle's [Neural Net lectures](http://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH):
 Again mostly on Neural Nets, with a focus on Deep Learning 

* Daphne Koller's [Probabilistic Graphical Models](https://www.coursera.org/course/pgm)
  Is a very challenging class, but has a lot of good material that few of the other
  MOOCs here will cover
 
 -------------------


**Books**


The most often recommended textbooks on general Machine Learning are (in no particular order):

 * Bishop's [Pattern Recognition and Machine Learning](http://research.microsoft.com/en-us/um/people/cmbishop/prml/)
 * Hasti/Tibshirani/Friedman's [Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/) *FREE VERSION ONLINE*
 * Barber's [Bayesian Reasoning and Machine Learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.HomePage) *FREE VERSION ONLINE*
 * Murphy's [Machine Learning: a Probabilistic Perspective](http://www.cs.ubc.ca/~murphyk/MLbook/)
 * MacKay's [Information Theory, Inference and Learning Algorithms](http://www.inference.phy.cam.ac.uk/itila/book.html) *FREE VERSION ONLINE*
 
Note that these books delve deep into math, and might be a bit heavy for complete beginners. If you don't care so much about derivations or how exactly the methods work but would rather just apply them, then the folllowing are good practical intros:

* [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)  *FREE VERSION ONLINE*
* Machine Learning for Hackers, 
* Machine Learning in Action
* Machine Learning with R
* [Probabilistic Programming and Bayesian Methods for Hackers](http://nbviewer.ipython.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Prologue/Prologue.ipynb)  *FREE VERSION ONLINE*
* Building Machine Learning Systems with Python

(I've
 stolen most of the books in this 2nd list from /u/rvprasad's post [here](http://www.reddit.com/r/MachineLearning/comments/1wda3y/best_intro_to_ml_books/cf0xmyn)).
 

 There are of course a whole plethora on books that only cover specific subjects, 
 as well as many books about surrounding fields in Math. A very good list has
 been collected by /u/ilsunil [here](http://www.reddit.com/r/MachineLearning/comments/1jeawf/machine_learning_books/)
 

 -------------------


**Programming Languages and Software**


In general, the most used languages in ML are probably Python, R and Matlab (with the latter
losing more and more ground to the former two). Which one suits you better
depends wholy on your personal taste. For R, a lot of functionality is either
already in the standard library or can be found through various packages in
CRAN. For Python, NumPy/SciPy are a must. From there, [Scikit-Learn](http://scikit-learn.org)
covers a broad range of ML methods. 

If you just want to play around a bit and don't do much programming yourself
then things like [WEKA](http://www.cs.waikato.ac.nz/ml/weka/), [KNIME](http://www.knime.org/)
or [RapidMiner](http://rapidminer.com/) might be of your liking. Word of caution:
a lot of people in this subreddit are very critical of WEKA, so even though
it's listed here, it is probably not a good tool to do anything more than
just playing around a bit. A more detailed discussion can be found [here](http://www.reddit.com/r/MachineLearning/comments/1rwj8p/why_are_python_r_so_much_more_popular_here_than/)


 -------------------

**Datasets and Challenges for Beginners**


There are a lot of good datasets here to try out your new Machine Learning skills.

 * [Kaggle](http://www.kaggle.com/) have a lot of challenges to sink your teeth into. Some even offer prize money!
 * The [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/) is a collection of a lot of good datasets
 * /r/datasets has a nice place to ask for data
 * http://blog.mortardata.com/post/67652898761/6-dataset-lists-curated-by-data-scientists lists some more datasets
 * [Here](http://www.quora.com/Data/Where-can-I-find-large-datasets-open-to-the-public) is a very extensive list of large-scale datasets of all kinds.
 * [Another dataset list](http://www.datawrangling.com/some-datasets-available-on-the-web)
 
 
 -------------------

**Communities**

* http://www.datatau.com/ is a data-science centric hackernews
* http://metaoptimize.com/qa/ and http://stats.stackexchange.com/ are Stackoverflow-like discussion forums


-------------------

**ML Research**

Machine Learning is a very active field of research. The two most prominent conferences are without a doubt [NIPS](http://nips.cc/) and [ICML](http://icml.cc).  Both sites contain the pdf-version of the papers accepted there, they're a great way to catch up on the most up-to-date research in the field. Other very good conferences include UAI (general AI), COLT (covers theoretical aspects)  and AISTATS.

Good journals for ML papers are the [Journal of Machine Learning Research](http://jmlr.org/),  the [Journal of Machine Learning](http://www.springer.com/computer/ai/journal/10994) and [arxiv](http://arxiv.org).

-------------------


**Other sites and Tutorials**

* http://datasciencemasters.org/ is an extensive list of lectures and textbooks for a whole Data Science curriculum
* http://deeplearning.net/
* http://en.wikipedia.org/wiki/Machine_learning
* http://videolectures.net/Top/Computer_Science/Machine_Learning/



**FAQ**


 *How much Math/Stats should I know?*

That depends on how deep you want to go. For a first exposure (e.g. Ng's
Coursera class) you won't need much math, but in order to understand how the
methods really work,having at least an undergrad level of Statistics, Linear Algebra
and Optimization won't hurt.",31,148
99,2014-3-16,2014,3,16,21,20jose,Top 7 Ipython Notebooks on Data Science and Machine Learning (a subjective selection),https://www.reddit.com/r/MachineLearning/comments/20jose/top_7_ipython_notebooks_on_data_science_and/,furukama,1394971578,,0,3
100,2014-3-16,2014,3,16,21,20jpej,Genetically Trained Neural Network: in 136 lines of fully commented JavaScript (IRIS Example Included),https://www.reddit.com/r/MachineLearning/comments/20jpej/genetically_trained_neural_network_in_136_lines/,[deleted],1394972455,,26,24
101,2014-3-17,2014,3,17,2,20k8i9,International Conference on Artificial Intelligence and Pattern Recognition - Let's discuss how machines and humans interact,https://www.reddit.com/r/MachineLearning/comments/20k8i9/international_conference_on_artificial/,yolly28,1394990275,,0,0
102,2014-3-17,2014,3,17,3,20kgth,Advice regarding the usefulness of a Data Mining elective,https://www.reddit.com/r/MachineLearning/comments/20kgth/advice_regarding_the_usefulness_of_a_data_mining/,[deleted],1394995981,"I am an undergraduate Computer Science major / Math minor looking to get a graduate degree in Machine Learning. My department is offering a Data Mining elective next semester, and I am wondering if it would be useful to take this course. Any insight would be much appreciated.",3,2
103,2014-3-17,2014,3,17,4,20kjxh,Predicting Student Exam's Scores by Analyzing Social Network Data,https://www.reddit.com/r/MachineLearning/comments/20kjxh/predicting_student_exams_scores_by_analyzing/,hrb1979,1394997941,,4,14
104,2014-3-17,2014,3,17,14,20m09b,How do linear decoders work?,https://www.reddit.com/r/MachineLearning/comments/20m09b/how_do_linear_decoders_work/,strayadvice,1395034819,"I was going through this [popular tutorial on deep learning](http://ufldl.stanford.edu/wiki/index.php/Linear_Decoders).

From what I understand, a linear decoder is used to obtain autoencoder outputs that can go beyond the [0,1] interval that a sigmoid activation would produce. The tutorial says that instead of using the sigmoid activation at the output layer of the autoencoder, we simply use the linear activation function. 

Here's what I'm thinking is happening: the output layer activations are a linear transformation of the hidden layer activations (by definition). This also means that the hidden layer activations are a linear transformation of the output layer activations (a1 = W.a2 &lt;=&gt; a2 = pinv(W).a1). 

Further, the output activations = input values by constraint, as that's what an autoencoder does. Hence, the input is a linear transformation of the hidden layer activations, and conversely, the hidden layer activations are a linear transformation of the input layer.

Perhaps I'm looking at this wrong?",8,8
105,2014-3-17,2014,3,17,16,20m66q,Let's play with Theano : a simple linear regression example,https://www.reddit.com/r/MachineLearning/comments/20m66q/lets_play_with_theano_a_simple_linear_regression/,underflow404,1395042088,,6,18
106,2014-3-18,2014,3,18,1,20my9s,From word2vec to doc2vec: an approach driven by Chinese restaurant process,https://www.reddit.com/r/MachineLearning/comments/20my9s/from_word2vec_to_doc2vec_an_approach_driven_by/,whatcausespolitics,1395072282,,10,33
107,2014-3-18,2014,3,18,1,20n0b5,Big Data: Taking the guesswork out of March Madness,https://www.reddit.com/r/MachineLearning/comments/20n0b5/big_data_taking_the_guesswork_out_of_march_madness/,sailormoonandteam,1395073548,,0,0
108,2014-3-18,2014,3,18,3,20nem7,SARSA Help,https://www.reddit.com/r/MachineLearning/comments/20nem7/sarsa_help/,calvinwylie,1395082087,"Hello everyone,

I'm looking for someone who is experienced with various reinforcement learning algorithms (Or, just SARSA tbh.) to give me a little personal tuition. I have a somewhat working sarsa algorithm in place i just need some expertise to help debug what i have. Send me a pm to get the party started :)",8,1
109,2014-3-18,2014,3,18,4,20nkre,Moving to a data science role for someone with a rather peculiar background.,https://www.reddit.com/r/MachineLearning/comments/20nkre/moving_to_a_data_science_role_for_someone_with_a/,diamond-merchant,1395085658,"Hello ML, 

I have read previous posts on this, but I am not sure if they answered my questions; so am posting anew. Sorry for the long post. 

I went to top 5 CS university for grad school in a research focused program, and have a MS with major in ML/ NLP. I also published often in various conferences and was fortunate enough to have worked and authored with some big names in CS. 

Unfortunately my last paper was in mid 2011, and I drifted to working on my startup. It involved some NLP/ IR work, but a lot of it was grind work, and other non data-ish things that were quantitative in nature like image processing and combinatorial optimization. We have been mildly successful in terms of users, press and revenue. Though nothing spectacular by big exit standards.

I had to recently move back to my home country due to family issues, and am no longer involved with the startup. Though I am helping the rest of the team transition for future product pipeline. And now I have to look for a more conventional job, which brings us to my questions. 

I have around six years of engineering experience post undergrad, split about evenly between academia/ research labs and the startup. Academia work also included working on live projects with sizable user base besides more pure experiments driven research work. But I have never worked in a normal corporate setting. And I also don't have a specialized engineering expertise. I have done everything from mobile[1] and web development to building recommender systems and optimizing elasticsearch to improve relevance. 

[1] This was primarily for the startup that included a very well reviewed app that was featured by Apple multiple times. Though I focused on harder algorithmic code and a team member lent hand for most UI/UX/API work.

I would love some general advice for me. Also posting some specific questions here:

- How much does my general background hurt my chances and for what roles? Does my experience in semi-managing technical teams help?

- I am taking a break of a few months to realign my career; also doing some side projects. How badly is that viewed?

- I have almost no knowledge of R/ SAS. Is it really important? Especially with the growth in scikit/ pandas community. Not great at them either, but am catching up fast.

- Among broad data science tasks, I think I am least experienced in data munging. Any good pointers for that? I have started practicing on Kaggle. 

- I would prefer moving back to the US in near term. How hard would it be to interview directly there? I have reasonable contacts and an approved work visa, but not sure if someone will interview at all sitting on the opposite end of the world.

Thank you for the attention; x-posting to /r/datascience.
",6,0
110,2014-3-18,2014,3,18,5,20nnfa,Deep Learning/ Neural Networks: Project recommendations,https://www.reddit.com/r/MachineLearning/comments/20nnfa/deep_learning_neural_networks_project/,discretemathematics,1395087164,"ML Folks,
I need some help picking a project for my Computational Learning Theory course. The project can be anything from implementing an existing algorithm to presenting several publications within a topic. I am broadly interested in Neural Networks and the emerging areas of Deep Learning. Would you have suggestions on a what I could do as a class project? Perhaps a set of new and interesting research papers?
",4,0
111,2014-3-18,2014,3,18,7,20o0t3,"The Data Analytics Handbook: Interviews from Data Scientists &amp; Analysts at Cloudera, Facebook, LinkedIn, and More!",https://www.reddit.com/r/MachineLearning/comments/20o0t3/the_data_analytics_handbook_interviews_from_data/,brainy1991,1395094768,,20,32
112,2014-3-18,2014,3,18,8,20oabs,Could meditation on thought processes lead to breakthroughs in machine learning?,https://www.reddit.com/r/MachineLearning/comments/20oabs/could_meditation_on_thought_processes_lead_to/,tubameister,1395100521,"I came across this in Godel Escher Bach:

&gt; Look at your conversations, he says. Youll see over and over again, to your surprise, that this is the process of analogy-making. Someone says something, which reminds you of something else; you say something, which reminds the other person of something elsethats a conversation. It couldnt be more straightforward. But at each step, Hofstadter argues, theres an analogy, a mental leap so stunningly complex that its a computational miracle: somehow your brain is able to strip any remark of the irrelevant surface details and extract its gist, its skeletal essence, and retrieve, from your own repertoire of ideas and experiences, the story or remark that best relates.
&gt; 
&gt; Beware, he writes, of innocent phrases like Oh, yeah, thats exactly what happened to me!  behind whose nonchalance is hidden the entire mystery of the human mind.

After reading, I decided I wanted to get a better look at this phenomenon, so I mentally leapt through a sequence of 'random' thoughts/images as quickly as I could, while taking note of the inherent connection they held in tandem with each other. As I watched this process, I noticed that as a thought arose, many little categorical markers would accompany it, the pool of which grew as certain patterns in the thought were noted. Once this pool grew great enough to distract from the initial thought, it would 'spill over' into a thought that shared a large subset of markers. It should be added that emotions guide the subset selection process greatly as well. The ratio of emotion to size in regards to the selection outcome would be difficult to observe as concentration meditation tends to quell emotions.

Now, I've heard of people who come to realize through meditation, on one experiential level, that 'everything' is cause and effect. Daniel Ingram writes about this realization:

&gt; ...if one is using a mantra, one may notice that at some point one shifts to being able to stay with mantra clearly and perceive it as an object, ... Once the mantra is clear, one may notice all sorts of things about the process of mentally creating the mantra, such as the stream of intentions being followed shortly behind by the string of the mantra itself follow slightly behind by the mental echo of the perception of the mantra, making what appear to be three separate streams of the mantra. This is direct insight into Cause and Effect

Essentially, my mantra is that phenomenon. If I could observe a mantra closely enough to believe it's just cause and effect, then could I observe cause and effect the same way through this phenomenon? If a scientist could see clearly enough into the workings of the mind that they become convinced that 'everything' is cause and effect, would the mind then be known sufficiently enough for them to make great contributions in the field of machine learning? Has anyone out there already proved this true?",3,0
113,2014-3-18,2014,3,18,12,20ose0,Document Visualization with JS Divergence Matrix and Multi Dimensional Scaling,https://www.reddit.com/r/MachineLearning/comments/20ose0/document_visualization_with_js_divergence_matrix/,bugra,1395111717,,0,1
114,2014-3-18,2014,3,18,12,20ox30,Lasso and ADMM,https://www.reddit.com/r/MachineLearning/comments/20ox30/lasso_and_admm/,augustus2010,1395114750,"Anybody read ""See All by Looking at A Few: Sparse Modeling for Finding Representative Objects"" by Ehsan Elhamifar? Do you understand how he implemented Eq (20) by using ADMM? It seems to me that he minimized only the l1-norm part i.e. trying to get as sparse as possible. I don't know how he derived the update in each iteration.",2,5
115,2014-3-18,2014,3,18,18,20pf84,Is there a simple way to integrate a Gaussian process?,https://www.reddit.com/r/MachineLearning/comments/20pf84/is_there_a_simple_way_to_integrate_a_gaussian/,[deleted],1395133460,So that one could obtain an integral with appropriate uncertainty bounds from the underlying Gaussian process.,2,0
116,2014-3-19,2014,3,19,0,20q4f4,How Statisticians Could Help Find That Missing Plane [x-post /r/Statistics],https://www.reddit.com/r/MachineLearning/comments/20q4f4/how_statisticians_could_help_find_that_missing/,meandtree,1395157771,,3,0
117,2014-3-19,2014,3,19,3,20qhr3,Insightful Neural Network lectures that are a perfect next step to Andrew Ng's Introduction to Machine Learning course on Coursera,https://www.reddit.com/r/MachineLearning/comments/20qhr3/insightful_neural_network_lectures_that_are_a/,slyrp,1395165893,,11,85
118,2014-3-19,2014,3,19,4,20qoh0,Differential Equations in Data Science,https://www.reddit.com/r/MachineLearning/comments/20qoh0/differential_equations_in_data_science/,turnersr,1395169570,,2,9
119,2014-3-19,2014,3,19,4,20qt0r,Free sentiment analysis API with support for batch processing,https://www.reddit.com/r/MachineLearning/comments/20qt0r/free_sentiment_analysis_api_with_support_for/,v1v3kn,1395172091,,0,3
120,2014-3-19,2014,3,19,5,20qzw9,Self Adaptive Mutation Rate For Genetic Algorithms,https://www.reddit.com/r/MachineLearning/comments/20qzw9/self_adaptive_mutation_rate_for_genetic_algorithms/,tomcrom,1395175767,I am in the process of designing a GA for a data mining problem I have been given as coursework. I've been reading that implementing self adaption using Gaussian distribution as opposed to a static mutation rate can significantly improve results. Although I don't seem to understand much of the literature on the subject.. could someone please ELI5?,6,0
121,2014-3-19,2014,3,19,6,20r4xd,"Interview with Yann LeCun, Deep Learning Expert, Director of Facebook AI Lab",https://www.reddit.com/r/MachineLearning/comments/20r4xd/interview_with_yann_lecun_deep_learning_expert/,rrenaud,1395178520,,2,14
122,2014-3-19,2014,3,19,12,20s6we,Overview of the field?,https://www.reddit.com/r/MachineLearning/comments/20s6we/overview_of_the_field/,SrPeixinho,1395200945,"Hello. Where can I find a brief overview of the field, including its branches, the problem they solve and how?",2,0
123,2014-3-19,2014,3,19,15,20sizx,experiment in using a DSL (gherkin) to encode knowledge for a home network router configuration expert system (pyclips). feedback welcome.,https://www.reddit.com/r/MachineLearning/comments/20sizx/experiment_in_using_a_dsl_gherkin_to_encode/,meekprize,1395211133,,0,0
124,2014-3-19,2014,3,19,21,20t1d2,Recent Advancement in Industrial Applications Machineries,https://www.reddit.com/r/MachineLearning/comments/20t1d2/recent_advancement_in_industrial_applications/,marcosesteban1,1395233095,,0,1
125,2014-3-20,2014,3,20,1,20tlb0,Near human level of face recognition accuracy achieved,https://www.reddit.com/r/MachineLearning/comments/20tlb0/near_human_level_of_face_recognition_accuracy/,zestinc,1395246030,,18,67
126,2014-3-20,2014,3,20,2,20tr9f,Differential Equations in Data Science,https://www.reddit.com/r/MachineLearning/comments/20tr9f/differential_equations_in_data_science/,jisaacso,1395249235,,0,3
127,2014-3-20,2014,3,20,4,20u7yx,"Live demo of ""Krizhevsky convolutional neural network architecture for object recognition in images"", using WebGL.",https://www.reddit.com/r/MachineLearning/comments/20u7yx/live_demo_of_krizhevsky_convolutional_neural/,idliketobeapython,1395258678,,1,16
128,2014-3-20,2014,3,20,5,20u96n,Wise.io Leverages RESTful APIs to Deliver Machine Learning Applications,https://www.reddit.com/r/MachineLearning/comments/20u96n/wiseio_leverages_restful_apis_to_deliver_machine/,NotEltonJohn,1395259342,,0,0
129,2014-3-20,2014,3,20,5,20ue0z,What are the practical difference between an RBM and autoencoder?,https://www.reddit.com/r/MachineLearning/comments/20ue0z/what_are_the_practical_difference_between_an_rbm/,scoopula36,1395261993,"I've been working with stacked autoencoders for unsupervised training and using it for some (semi)supervised classification. As I read more, I wonder how I can justify using stacked autoencoders over stacked RBMs (a deep belief network). I know the fundamental differences between them (AE learns a representation of the inputs and is deterministic; RBMs learn the statistical distribution and is probablistic). But how do they differ in practical settings? When would you use one over the other? (or specifically for me, how can I justify using stacked AEs for classification over RBMs?) 
I'm trying to do my training and classification as unsupervised as possible (yet I need some labeled examples for the classifier), so I was wondering about an advantage specifically for it being unsupervised. 
EDIT: Thanks for the input!",5,7
130,2014-3-20,2014,3,20,8,20uvqa,Could someone make a Chrome extension that clustered tabs by topic and made each cluster a new window?,https://www.reddit.com/r/MachineLearning/comments/20uvqa/could_someone_make_a_chrome_extension_that/,mrtransisteur,1395272352,"Managing tabs manually is pretty tedious when I have 20+ open. Does anyone have the know-how and determination to whip up such a thing?

When I browse, certain tabs have a lot of similar text in common. Plus, tabs could share referring URLs, if not immediately then n generations prior.. Maybe I should make this myself?",3,0
131,2014-3-20,2014,3,20,17,20vzjr,Need Book recommendation: Shalizi book (Advanced Data Analysis from an Elementary Point of View) vs. Hastie/Tibshirani (Elements of Statistical Learning/ ESL),https://www.reddit.com/r/MachineLearning/comments/20vzjr/need_book_recommendation_shalizi_book_advanced/,T_hank,1395303868,"Hi,
I understand there have been many threads on such topics before. However I needed  an opinion on comparing 2 books.
Basically, I am looking for a ML book with a focus on the statistics side of things.

I saw the contents of shalizi's book (http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf) , topics such as the ""truth about linear regression' seem like they will go to the heart of the matter.
In comparison there is Hastie/Tibshirani's ESL, which is quite famous.  The range of topics seem pretty much the same in both.

So I  need an opinion from somebody who has read Shalizi and maybe also ESL: 
Would you say Shalizi is a self contained enough resource, which doesn't skimp on the math? How would you compare this to ESL?",8,12
132,2014-3-20,2014,3,20,22,20wh21,How does regularization affect dropout while training autoencoders?,https://www.reddit.com/r/MachineLearning/comments/20wh21/how_does_regularization_affect_dropout_while/,eubarch,1395323030,"I was reading this paper on dropout:


http://arxiv.org/pdf/1207.0580v1.pdf



...And after a rather straightforward explanation of what dropout is and what it is supposed to do, they use the third paragraph to detail a threshold-based ~~regularization~~ procedure that they use, instead of the typical L2 norm weight penalty.  Unless I missed something in the paper, the authors never really come back to this or explain if it has any fundamental role in the effectiveness of dropout.


I'm open to having missed something, but is a threshold-based ~~regularization procedure~~ weight penalty a wholly independent aspect of training, or does it have some interaction with Dropout?  In either case, they also don't seem to mention what an appropriate value for the weight divisor is after the L2 breaches said threshold, or the value of the threshold itself.


**Edit:** Regularization is the wrong word for penalizing weight growth.  I'm aware that dropout is itself a regularization procedure.  Post was written pre-coffee.",14,15
133,2014-3-21,2014,3,21,1,20wuvi,IBMs Watson to do genome research on cancer treatment,https://www.reddit.com/r/MachineLearning/comments/20wuvi/ibms_watson_to_do_genome_research_on_cancer/,aryastarklives,1395331596,,0,0
134,2014-3-22,2014,3,22,1,2107z4,Predicting customer churn with scikit-learn,https://www.reddit.com/r/MachineLearning/comments/2107z4/predicting_customer_churn_with_scikitlearn/,hernamesbarbara,1395418931,,0,25
135,2014-3-22,2014,3,22,2,210bp4,Current fashion face detection approach?,https://www.reddit.com/r/MachineLearning/comments/210bp4/current_fashion_face_detection_approach/,rickyngk,1395421222,"I'm an amateur ML researcher (hobby). I wonder that among many techniques for face detection, which one is main approach (current fashion)?
I'm learning: SVM, Adaboost, PCA, Haar-like feature. Is that the correct way?
",2,2
136,2014-3-22,2014,3,22,3,210j8j,"Good representations, distance, metric learning and supervised dimensionality reduction",https://www.reddit.com/r/MachineLearning/comments/210j8j/good_representations_distance_metric_learning_and/,Foxtr0t,1395425693,,0,28
137,2014-3-22,2014,3,22,3,210n7v,Predicting Sine Waves with NuPIC (tutorial screencast),https://www.reddit.com/r/MachineLearning/comments/210n7v/predicting_sine_waves_with_nupic_tutorial/,numenta,1395428194,,1,0
138,2014-3-22,2014,3,22,7,2118vp,Ashton Kutcher &amp; Mark Zuckerberg Invest In AI Company Vicarious.,https://www.reddit.com/r/MachineLearning/comments/2118vp/ashton_kutcher_mark_zuckerberg_invest_in_ai/,drofew,1395441737,,2,0
139,2014-3-22,2014,3,22,9,211ikg,Statistician interested in/curious about machine learning work.,https://www.reddit.com/r/MachineLearning/comments/211ikg/statistician_interested_incurious_about_machine/,boogahwoogah,1395448759,"I'm almost finishing up my post grad in statistics and thinking of work options a little more. One of where I'm interested in is machine learning and I have some of questions:

1. Does it make sense to dive into machine learning? 
2. What are the opportunities out there, in comparison to say clinical work? Say, how is progress in career and so fort.
3. Where to start learning more in-depth? I'm proficient with R and SAS. I'm reading posts here that mostly involves other languages/programs used.

Any advice would be much appreciated. Thanks!",7,0
140,2014-3-22,2014,3,22,21,212ml9,Am I setting myself up for success in landing a data science or similar job?,https://www.reddit.com/r/MachineLearning/comments/212ml9/am_i_setting_myself_up_for_success_in_landing_a/,[deleted],1395493150,"Ok, a bit about my background in cliffs form.

* BS in Business Management, Economics/Econometrics field

* MS (all but thesis right now) in Economics with PhD core (killed math and micro prelim, failed macro prelim)

* Thesis is an optimal control problem with hazard functions, jointly solves for an action and a time, bellman equations, all that, written in Mathematica. Should be done soon.

* Worked as a research assistant putting together fiscal impact models (OLS, Time Series, 2SLS, 3SLS) and doing basic testing on them (things like granger causality, looking for heteroskedasticity, stationarity, all that but I mean I don't remember a lot of it. Used STATA so it was fairly...canned)

* After/alongside my MS coursework I took a lot of mathematics, like 3 Real Analysis courses, a senior level Matrix Theory class, Abstract Algebra, Proofs (obviously for RA), ODE to see what was after calculus and did very, very well at that stuff, so I'm sound at abstraction.

* Been working in IT for a year and 10 months or so. TL;DR my wife was diagnosed with an autoimmune disease and living as poor grad students was something I couldn't do to her any longer, so I took a good job with health insurance. 

* I do information security engineering for regulatory compliance controls on user accounts and computing environment. So it's half soft/business-y and half technical.* While doing this I was promoted early, learned SQL, Python, and R (not amazing at any of them but comfortable and love them all) to get some stuff done. SQL daily, Python weekly, R sparingly when people want me to look through their data or management wants metrics. Nothing killer, mostly GGPLOT and ODBC/SQL. I've played with Pandas, Numpy, and Matplotlib in Python but most of it is for automating regulatory compliance issues and reading/writing to SQL tables.

* Currently taking Machine Learning on Coursera, planning to do the Specialization when it starts. 

This week, one of the programmers on the team left, and I've been offered a spot on the development team since they know I like it and they can teach me to do it for 'real'. I've accepted since a) I love to learn and b) I think my quantitative and modeling background is solid, but my programming could use some improvement if I want to get into data science. It won't be *scientific* programming, but I'll become solid at Python, JS, SQL/NoSQL and working with different data structures like JSON and ugly flat files as well as working primarily in Linux.

Am I doing this right? 

Aside: I'm worried that by the time I'm 'done' with learning programming, the fad will have come to a head and the industry saturated and extremely competitive. I feel like I'm just *OK* at a lot of this stuff, but I love it all. That said, I don't want to be a pure programmer for life and will eventually of course need the quantitative component, since math/modeling/explaining things with math and modeling are my true interests. In a way, piping economics skills to things that aren't stereotypical like trading stocks.",13,0
141,2014-3-22,2014,3,22,23,212ro7,Think China is using ML to scan then ocean?,https://www.reddit.com/r/MachineLearning/comments/212ro7/think_china_is_using_ml_to_scan_then_ocean/,[deleted],1395498390,"I impressed with with the use of satellites power on that side of the globe.
That a lot of area to scan, which will produce large amount of imaging
data. I think they are using algorithms that pick up on square edges.",1,0
142,2014-3-23,2014,3,23,4,213dw1,Mining and Understanding Software Enclaves by Jagannathan [PDF],https://www.reddit.com/r/MachineLearning/comments/213dw1/mining_and_understanding_software_enclaves_by/,turnersr,1395515661,,1,0
143,2014-3-23,2014,3,23,4,213ftp,What happened to theoretical machine learning in the last 5 years ?,https://www.reddit.com/r/MachineLearning/comments/213ftp/what_happened_to_theoretical_machine_learning_in/,joelthelion,1395517017,,7,14
144,2014-3-23,2014,3,23,5,213iwq,Random features + fast linear methods can outperform dropout nets,https://www.reddit.com/r/MachineLearning/comments/213iwq/random_features_fast_linear_methods_can/,DavidJayHarris,1395519116,,4,28
145,2014-3-23,2014,3,23,10,214758,"[Open Research] Learning from 1.2k bugs (part I: ""It's a science experiment!"")",https://www.reddit.com/r/MachineLearning/comments/214758/open_research_learning_from_12k_bugs_part_i_its_a/,galapag0,1395536989,,0,17
146,2014-3-23,2014,3,23,11,214cvt,maxout + dropout help?,https://www.reddit.com/r/MachineLearning/comments/214cvt/maxout_dropout_help/,rishok,1395541510,is there anyone who knows how to implement maxout + dropout technique in a autoencoder,7,0
147,2014-3-24,2014,3,24,1,215o4f,How to quantify urine test strip color,https://www.reddit.com/r/MachineLearning/comments/215o4f/how_to_quantify_urine_test_strip_color/,osazuwa,1395593239,"I am trying to build a model predicting ketone levels in the body.  I have ketone urinalysis test strips that display a color when tested, the deeper the color the higher the ketone bodies in the blood.  I want a way to easily quantify these color values.  Obviously I have to take photos and quantify the values of the color in the image files.  I just don't know the smartest and most efficient way to go about doing it.  Any suggestions?",7,1
148,2014-3-24,2014,3,24,8,216osr,What would be a good path for me to take to end up with a career in machine learning,https://www.reddit.com/r/MachineLearning/comments/216osr/what_would_be_a_good_path_for_me_to_take_to_end/,[deleted],1395616884,"As of right now, I'm an undergraduate CS student with 2 more semesters left until graduation. I haven't taken a machine learning course yet at my school (there are surprisingly few), but I've been teaching myself. I've gone through the majority of Introduction to Statistical Learning with applications in R. I learned a lot, but I feel like my lack of knowledge in probability/statistics/and linear algebra limited its use for me. I'll be enrolling in a linear algebra course fall semester next year, and also an intro to AI course, a Data Mining course, and an NLP course. I'm also going through Andrew Ng's coursera course right now to get a different perspective on some of the ideas taught in the ISLR book. 
After I get my math up to snuff I plan on going through the ESL book and Bishop's book.

What would be a next logical step for me to take?  Should I be doing anything different? How necessary is a Masters degree? I've been considering the OMSCS from GA Tech based on price solely. There are plenty of other MOOCs I know of that I could go through if anybody has a good recommendation for one, I know of CMU, UoW, Caltech. ",6,5
149,2014-3-24,2014,3,24,11,21759l,Are State of the Union Addresses all basically the same?,https://www.reddit.com/r/MachineLearning/comments/21759l/are_state_of_the_union_addresses_all_basically/,bluepublius,1395627828,,14,26
150,2014-3-24,2014,3,24,20,21816x,Simple approach to using the search engine as recommendation engine,https://www.reddit.com/r/MachineLearning/comments/21816x/simple_approach_to_using_the_search_engine_as/,larsga,1395660873,,8,14
151,2014-3-24,2014,3,24,20,21824y,Why hasn't deep learning made CAPTCHAs obsolete?,https://www.reddit.com/r/MachineLearning/comments/21824y/why_hasnt_deep_learning_made_captchas_obsolete/,mooramoora,1395662036,"Or maybe it *has*?
Or is there something about the problem that makes it intrinsically hard?
",17,6
152,2014-3-25,2014,3,25,1,218r3h,Consciousness And The Bayesian Brain - Sandler Conference 2014,https://www.reddit.com/r/MachineLearning/comments/218r3h/consciousness_and_the_bayesian_brain_sandler/,jry_AIHub,1395680326,,0,19
153,2014-3-25,2014,3,25,2,218x4r,Questions about speech recognition via hidden Markov modeling,https://www.reddit.com/r/MachineLearning/comments/218x4r/questions_about_speech_recognition_via_hidden/,[deleted],1395683801,"Greetings guys, and apologies in advance in case I posted this in the wrong subreddit.


I'm really intrigued by the concept of HMM, and am trying to understand it by applying it in a small-vocabulary, speaker-dependent, isolated word recognizer. Did quite a lot of research, stumbled upon [this paper](https://www.google.com.ph/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0CCoQFjAA&amp;url=http%3A%2F%2Fwww.netlearning2002.org%2Ffou%2Fcuppsats.nsf%2Fall%2Fe63c30f71dde8e8dc1257142004558b3%2F%24file%2FSpeech%2520Recognition%2520using%2520Hidden%2520Markov%2520Model_MEE_03_19.doc&amp;ei=kW8wU6_uHITVkAWxyIGABA&amp;usg=AFQjCNFVnj5P7RUq3vCAAgr9Df7BwbjBVA&amp;bvm=bv.62922401,d.dGI) and just straight-up copied their methods verbatim, hoping to get the same results in MATLAB. I've taken a couple of intro DSP courses but we never really got into the topic of speech recognition, but at least I already have some basic knowledge, and I believe that I have the preprocessing/feature vector extraction parts figured out.


I've seen video tutorials (like [this one](http://www.youtube.com/playlist?list=PLE6Wd9FR--Ecf_5nCbnSQMHqORpiChfJf) and [that one](http://www.youtube.com/playlist?list=PLD0F06AA0D2E8FFBA)), studied other web documents (including [the seminal paper](http://www.cs.ubc.ca/~murphyk/Bayes/rabiner.pdf) from the patron saint of HMM ASR himself, L.R. Rabiner, cited in all other papers I've read); and I feel that I got a fair grasp on the general concepts, meanings of terms, and interpretations of equations.


However, I get stuck at implementing the algorithms for training and testing. More precisely: I'm confused by the notations for the Baum-Welch re-estimation procedure, and the alternate Viterbi algorithm. I'd really appreciate if anyone could provide a simple illustration to help me wrap my head around the notation, something like a step-by-step breakdown of the algorithms, with dimensions of matrices used in each step.


EDIT: For your consideration, here is [an annotated script detailing the parts that I understand so far](https://www.dropbox.com/s/8tbnglhstz8kzbr/train.m), for you to tear apart if you have the time. Any sort of criticism is most welcome - always looking to improve.",4,2
154,2014-3-25,2014,3,25,4,2198gz,"Neural Networks using Pylearn2  termination critera, momentum and learning rate",https://www.reddit.com/r/MachineLearning/comments/2198gz/neural_networks_using_pylearn2_termination/,arngarden,1395690184,,0,21
155,2014-3-25,2014,3,25,13,21aptk,Predicting Artists with Python,https://www.reddit.com/r/MachineLearning/comments/21aptk/predicting_artists_with_python/,daf1411,1395722276,,18,2
156,2014-3-25,2014,3,25,20,21bagn,how to optimize an autoencoder,https://www.reddit.com/r/MachineLearning/comments/21bagn/how_to_optimize_an_autoencoder/,rishok,1395746196,based on what do you optimize  an autoencoder's hyper parameters?,3,0
157,2014-3-25,2014,3,25,22,21bgyf,Training HMM's for classification.,https://www.reddit.com/r/MachineLearning/comments/21bgyf/training_hmms_for_classification/,alexgmcm,1395752831,"So I understand that when you train HMM's for classification the standard approach is:

1. Separate your data sets into the data sets for each class
2. Train one HMM per class
3. On the test set compare the likelihood of each model to classify each window

I can see that is a reasonable approach as previously I had thought of using the latent states as class labels, so having as many latent states as there are classes, training a single HMM using the known latent path in the dataset and then taking the resulting model and using the Viterbi algorithm on the test set to attempt to reconstruct the latent states for each point and thus the class. I wasn't able to try this due to difficulties with the libraries and an extreme lack of time, but I will safely assume it won't work for some reason or people would just do this.

However using the traditional method, how do I train the HMM on each class? Do I just concatenate the data pertaining to one class together? But isn't the time series data meant to be sequential - and if I do that then I am saying that some data points are consecutive when they are not?

",9,1
158,2014-3-25,2014,3,25,22,21bhjs,Data Science News and Great Articles (Flipboard Magazine),https://www.reddit.com/r/MachineLearning/comments/21bhjs/data_science_news_and_great_articles_flipboard/,mickaellegal,1395753299,,0,8
159,2014-3-26,2014,3,26,1,21bzsw,FREE eBOOK | Practical Machine Learning: Innovations in Recommendation,https://www.reddit.com/r/MachineLearning/comments/21bzsw/free_ebook_practical_machine_learning_innovations/,TedOBrien,1395765197,,4,24
160,2014-3-26,2014,3,26,2,21c2le,"As training set size increases, training performance decreases, but test performance increases?",https://www.reddit.com/r/MachineLearning/comments/21c2le/as_training_set_size_increases_training/,demchickenwings,1395766801,"So I am seeing the following in my experiment:

As I increase the number of samples in the training set size, the performance reported after training decreases a lot.

But in the test set, we see an equivalent INCREASE.

I'm not so sure why this is. Here are my ideas:
- As we increase the size of the training set (from 1000 to 2000, 2000 to 3000 etc), the classifier has more data to find the optimal line of separation. 
- So whilst more errors are made during training, when we generalize to the independent test set, we see an equivalent increase in classification performance.

I think that's right, but how the heck do I say these things using technical terminology?",9,9
161,2014-3-26,2014,3,26,4,21cloh,Purgatory: a deep generation machine learning tool set,https://www.reddit.com/r/MachineLearning/comments/21cloh/purgatory_a_deep_generation_machine_learning_tool/,galapag0,1395777565,,5,0
162,2014-3-26,2014,3,26,10,21dnlh,Weka J48 decision tree help?,https://www.reddit.com/r/MachineLearning/comments/21dnlh/weka_j48_decision_tree_help/,[deleted],1395799119,"Hey all! Might be a long-shot but nothing ventured, nothing gained.

I'm pretty sure that I have a correct .arff file but once I try running the J48 decision tree in Weka it just... doesn't work properly. It only uses 2 of my 7 attributes to build the tree which is just plain weird. I've tried everything, looked everywhere but I just can't seem to find what's wrong? I really don't think it's coming from my .arff file so yeah, is there anyone with (relatively basic) knowledge of J48 trees on Weka? 
I literally just need to get the trees, I already know how to interpret them.

Thanks in advance!",3,0
163,2014-3-26,2014,3,26,22,21eua1,What if NCAA basketball players were paid? An analysis in python,https://www.reddit.com/r/MachineLearning/comments/21eua1/what_if_ncaa_basketball_players_were_paid_an/,daf1411,1395840489,,11,3
164,2014-3-27,2014,3,27,4,21fsbx,8th Annual NYC Machine Learning Symposium on March 28th,https://www.reddit.com/r/MachineLearning/comments/21fsbx/8th_annual_nyc_machine_learning_symposium_on/,rrenaud,1395860883,,4,4
165,2014-3-27,2014,3,27,5,21g2xr,Question: Continuous Multivariate Markov Chains,https://www.reddit.com/r/MachineLearning/comments/21g2xr/question_continuous_multivariate_markov_chains/,the_morbid_reality,1395866787,"I am confused and wondering if such a thing makes any sense and does really exist? By ""continuous"" I am referring to the values that the (observed?) variables take, not time.

",8,2
166,2014-3-27,2014,3,27,10,21gtu3,IPAM Summer School Deep Learning lectures,https://www.reddit.com/r/MachineLearning/comments/21gtu3/ipam_summer_school_deep_learning_lectures/,xamdam,1395882695,,2,40
167,2014-3-27,2014,3,27,13,21hdfo,Need help with sklearn's GradientBoostingRegressor,https://www.reddit.com/r/MachineLearning/comments/21hdfo/need_help_with_sklearns_gradientboostingregressor/,davit_,1395896248,"I encountered a weird behavior while trying to train sklearn's __GradientBoostingRegressor__ and make prediction. I will bring an example to demonstrate the issue on a reduced dataset but issue remains on a larger dataset as well. I have the following 2 small datasets adapted from a big dataset. As you can see the target variable is identical for both cases but input variables are different though their values are close to each other

Column 1 | Column 2 | Column 3 | Column 4 | Column 5 | target 
:--------------|:--------------|:---------------|:---------------|:---------------|:---------
101869.2  | 102119.9 | 102138.0 | 101958.3 | 101903.7   | 12384900               
101809.1  | 102031.3 | 102061.7 | 101930.0 | 101935.2   | 11930700
101978.0  | 102208.9 | 102209.8 | 101970.0 | 101878.6   | 12116700
101869.2  | 102119.9 | 102138.0 | 101958.3 | 101903.7   | 12301200
102125.5  | 102283.4 | 102194.0 | 101884.8 | 101806.0   | 10706100
102215.5  | 102351.9 | 102214.0 | 101769.3 | 101693.6   | 10116900

Column 1 | Column 2 | Column 3 | Column 4 | Column 5 | target 
:--------------|:--------------|:---------------|:---------------|:---------------|:---------
101876.0  | 102109.8 | 102127.6 | 101937.0 | 101868.4 | 12384900             
101812.9  | 102021.2 | 102058.8 | 101912.9 | 101896.4 | 11930700
101982.5  | 102198.0 | 102195.4 | 101940.2 | 101842.5 | 12116700
101876.0  | 102109.8 | 102127.6 | 101937.0 | 101868.4 | 12301200
102111.3  | 102254.8 | 102182.8 | 101832.7 | 101719.7 | 10706100
102184.6  | 102320.2 | 102188.9 | 101699.9 | 101548.1 | 10116900

I have the following code: 

    re1 = ensemble.GradientBoostingRegressor(n_estimators=40,max_depth=None,random_state=1)
    re1.fit(X1,Y)
    pred1 = re1.predict(X1)

    re2 = ensemble.GradientBoostingRegressor(n_estimators=40,max_depth=None,random_state=3)
    re2.fit(X2,Y)
    pred2 = re2.predict(X2)

where 
X1 is a *pandas* DataFrame corresponding to *Column 1* through *Column 5* on the __1st__ dataset
X2 is a *pandas* DataFrame corresponding to *Column 1* through *Column 5* on the __2nd__ dataset
Y represents the target column.
The issue I am facing is that I cannot explain why __pred1__ is exactly the same as __pred2__?? As long as __X1__ and __X2__ are not the same  __pred1__ and __pred2__ must also be different, musn't they? Help me to find my false assumption, please.

P.S. To help you build the dataframe I wrote this code:

    d1 = {'0':[101869.2,102119.9,102138.0,101958.3,101903.7,12384900],  
            '1':[101809.1,102031.3,102061.7,101930.0,101935.2,11930700],
            '2':[101978.0,102208.9,102209.8,101970.0,101878.6,12116700],
            '3':[101869.2,102119.9,102138.0,101958.3,101903.7,12301200],
            '4':[102125.5,102283.4,102194.0,101884.8,101806.0,10706100],
            '5':[102215.5,102351.9,102214.0,101769.3,101693.6,10116900]}	 	 	 	 	
    data1 = pd.DataFrame(d1).T
    X1 = data1.ix[:,:4]
    Y = data1[5]

    d2 = {'0':[101876.0,102109.8,102127.6,101937.0,101868.4,12384900],
            '1':[101812.9,102021.2,102058.8,101912.9,101896.4,11930700],
            '2':[101982.5,102198.0,102195.4,101940.2,101842.5,12116700],
            '3':[101876.0,102109.8,102127.6,101937.0,101868.4,12301200],
            '4':[102111.3,102254.8,102182.8,101832.7,101719.7,10706100],
            '5':[102184.6,102320.2,102188.9,101699.9,101548.1,10116900]}
    data2 = pd.DataFrame(d2).T
    X2 = data2.ix[:,:4]
    Y = data2[5]",10,2
168,2014-3-27,2014,3,27,15,21hhv9,"My theories about graphs, cognition, and machine learning, opinions of experts and other amateurs welcome.",https://www.reddit.com/r/MachineLearning/comments/21hhv9/my_theories_about_graphs_cognition_and_machine/,[deleted],1395900476,,0,0
169,2014-3-27,2014,3,27,16,21hl72,Has anyone tried to build a distributed neural network over untrusted nodes?,https://www.reddit.com/r/MachineLearning/comments/21hl72/has_anyone_tried_to_build_a_distributed_neural/,_hooo,1395904192,There's been a lot of interest in deep learning and neural networks lately. Is anyone aware of research into distributing them over untrusted nodes?,9,2
170,2014-3-27,2014,3,27,18,21hqa0,Python implementation of Stacked Autoencoder,https://www.reddit.com/r/MachineLearning/comments/21hqa0/python_implementation_of_stacked_autoencoder/,[deleted],1395911374,,0,10
171,2014-3-27,2014,3,27,18,21hqz7,NLP Application Thematics Chrome Extension,https://www.reddit.com/r/MachineLearning/comments/21hqz7/nlp_application_thematics_chrome_extension/,ucerron,1395912387,"Hello Everyone,

I started this Natural Language Processing project a couple of weeks ago and have started developing it ever since. It may mess up some websites design so disable it if it does so, please report any bugs you may find, ideas are welcome. 

You can see the project at www.thematicsapp.com  Suggestions are welcome. I may open source this when it becomes something worthy to be open sourced. :p

The way it works is: you select the text and click the icon and it should display the themes and a basic word count on what it is about. i am still working on a smart news feed and a backend for distributed computation and learning.

**Next Update by April 28, 2014.**
Improved News Search Recommendation

**Update by May 7, 2014**
Text Grabbing Fully Automized. no need to grab text. =)",3,6
172,2014-3-28,2014,3,28,1,21iitq,Brain based machine learning software,https://www.reddit.com/r/MachineLearning/comments/21iitq/brain_based_machine_learning_software/,ApplySci,1395936668,,0,0
173,2014-3-28,2014,3,28,1,21ijqq,A paper on effective visualization of generalized additive models,https://www.reddit.com/r/MachineLearning/comments/21ijqq/a_paper_on_effective_visualization_of_generalized/,rrenaud,1395937205,,0,3
174,2014-3-28,2014,3,28,3,21iw6j,An Introduction to Deep Learning: From Perceptrons to Deep Networks,https://www.reddit.com/r/MachineLearning/comments/21iw6j/an_introduction_to_deep_learning_from_perceptrons/,dtk0101,1395944375,,7,107
175,2014-3-28,2014,3,28,4,21j617,Industrial Machine Learning with Feature Forge,https://www.reddit.com/r/MachineLearning/comments/21j617/industrial_machine_learning_with_feature_forge/,copybin,1395950123,,0,3
176,2014-3-28,2014,3,28,6,21jfar,ccv 0.6 open sources near state-of-the-art image classifier under Creative Commons,https://www.reddit.com/r/MachineLearning/comments/21jfar/ccv_06_open_sources_near_stateoftheart_image/,Dvorak_Simplified_Kb,1395955345,,1,4
177,2014-3-28,2014,3,28,17,21kw2q,Porfermance of Filling Machine,https://www.reddit.com/r/MachineLearning/comments/21kw2q/porfermance_of_filling_machine/,xunjiejixie,1395996942,,0,0
178,2014-3-28,2014,3,28,20,21l1ig,"Apache Mahout, Hadoops original machine learning project, is moving on from MapReduce",https://www.reddit.com/r/MachineLearning/comments/21l1ig/apache_mahout_hadoops_original_machine_learning/,mhausenblas,1396004907,,3,35
179,2014-3-28,2014,3,28,21,21l55u,Best Post Printing Machinery In Delhi,https://www.reddit.com/r/MachineLearning/comments/21l55u/best_post_printing_machinery_in_delhi/,graphogill,1396009190,,0,1
180,2014-3-28,2014,3,28,21,21l5yr,Your intuitions to scikit-learn in just 2 steps,https://www.reddit.com/r/MachineLearning/comments/21l5yr/your_intuitions_to_scikitlearn_in_just_2_steps/,copybin,1396010015,,0,6
181,2014-3-28,2014,3,28,23,21le3f,The Most Popular Machines Manufactured By Caterpillar | The Most Popular,https://www.reddit.com/r/MachineLearning/comments/21le3f/the_most_popular_machines_manufactured_by/,jessicperson,1396016839,,0,1
182,2014-3-29,2014,3,29,4,21m4q2,Big data: are we making a big mistake?,https://www.reddit.com/r/MachineLearning/comments/21m4q2/big_data_are_we_making_a_big_mistake/,[deleted],1396033382,,4,0
183,2014-3-29,2014,3,29,4,21m9hm,Maryland moving to replace current bail system to one based on logistic regression [pdf in comments],https://www.reddit.com/r/MachineLearning/comments/21m9hm/maryland_moving_to_replace_current_bail_system_to/,isep,1396036219,,1,15
184,2014-3-29,2014,3,29,6,21mi26,Chances to work in data science/machine learning with my background/recommendations on what to learn next.,https://www.reddit.com/r/MachineLearning/comments/21mi26/chances_to_work_in_data_sciencemachine_learning/,psychmas,1396041944,"Hi everyone, 

I know, this is not the first post in this direction, but so far I have only seen posts by people with computer science backgrounds and I think my situation is a little different.  
I am a psychology research-master student with a major in psychological methods. 

I had courses in basic statistics, structural equation moddeling and multivariate statistics, I also learned the R programming language.... Quite often I had the feeling that I did only grasp the  statistics on a rather shallow level, mostly because of the lack of knowledge in more basic math. 

I choose to do psychological methods because I wanted to learn what it means to do ""good"" research and was interested on a more philosophical level in what it means to really ""know"" something and why a simple t-test was supposed to be the answer :). But in high school, I sucked at math. The things is: an eigenvalue in multivariate stats (PCA) course remained a voodoo concept for me. I just did not see the whole thing as an unified concept and soon lost interest out of frustration, learning means understanding to me and not memorization. How the heck are you suposed to rotate a coordinate system? :) And as usual, things that do not get understood on a deeper level, quickly fade away in memory. (If I cant code it/reproduce it/explain it, I forget it)

Last year I decided that this way of learning does not satisfy me and that a PhD in psychology is not the right way to go for me. No one reads the papers anyway ;). I decided that I need a job, and since for me the most fun and interesting part of research has always been doing the analysis, the coding to clean up the data and making the plots (all in R), I want to do more of that and learn to work with bigger amounts of data. 

So i learned calculus (from the book: Calculus and ints applications by Goldstein) and python.  Yesterday I finished an undergrad course in linear algebra from the comp science department of my university. I am planning to do a minor in programming and two courses in machine learning next semester (thought from the bishop book). I have already followed the course of andrew ng.  I have time till the beginning of next semester to suck up as much knowledge as I can to prepare for the machine learning courses and finally for the job market. What do you guys think is worth reading? Books on probability theory? Should I re-read my stats and calculus books? Should i delve into books about information theory? Learn SQL and data warehousing?

Do you guys think it is realistic to get into an entry level position with my CV?

I would be really thankful for some recommendations for a guy with my skill set and maybe some good resources for self-study. 

Cheers!  ",1,4
185,2014-3-29,2014,3,29,6,21mi2g,Optimizing memory usage of scikit-learn models using succinct Tries,https://www.reddit.com/r/MachineLearning/comments/21mi2g/optimizing_memory_usage_of_scikitlearn_models/,hrb1979,1396041947,,0,20
186,2014-3-29,2014,3,29,12,21nbmr,2014 03 11 Yoshua Bengio - Deep Learning of Generative Models,https://www.reddit.com/r/MachineLearning/comments/21nbmr/2014_03_11_yoshua_bengio_deep_learning_of/,cswhjiang,1396063316,,0,17
187,2014-3-30,2014,3,30,4,21oxxm,8th NYAS Machine Learning Symposium 2014,https://www.reddit.com/r/MachineLearning/comments/21oxxm/8th_nyas_machine_learning_symposium_2014/,bugra,1396122430,,0,3
188,2014-3-30,2014,3,30,6,21p8yt,A growing IPython notebook about Sequential Selection Algorithms,https://www.reddit.com/r/MachineLearning/comments/21p8yt/a_growing_ipython_notebook_about_sequential/,rasbt,1396130052,,1,18
189,2014-3-30,2014,3,30,11,21pu3w,"I tried a variety of incrementally different neural network training techniques on a set of image data, and put together an album of the results.",https://www.reddit.com/r/MachineLearning/comments/21pu3w/i_tried_a_variety_of_incrementally_different/,eubarch,1396145100,"I thought this might be helpful to some people.  Maybe some others can comment on why certain techniques converge and others fail to do so (in particular, networks with ReLU units).



First, the data: I'm using a set of 8x8 whitened natural image patches.  You can get them from Bruno Olshausen's website:


http://redwood.berkeley.edu/bruno/sparsenet/


~~In addition to the whitening, I scaled and offset these images so that their values fell within the range of 0.2 - 0.8.~~


**Edit:** This statement is only partially correct.  The data was scaled so that (if I recall correctly) it has a mean of 0.5 and a standard deviation of 0.1 (meaning that the data falls within 0.2-0.8 to within three standard deviations).  [Here is a histogram.](http://imgur.com/OaZnqzR)


All of these experiments involve an autoencoding task, where the network is asked to simply reproduce the input (the image data is vectorized, making a single 64-dimensional vector).

You can see all the results in an album here:


http://imgur.com/a/8g9ST



Let's begin with regular old backpropagation, as you can find in books about neural networks from the 90's.  The network structure is 64-25-64, meaning the input and output layers have 64 neurons, and in between is a single 25-neuron hidden layer.  The neurons have a sigmoidal activation function.  We'll train it using Stochastic Gradient Descent, with the following parameters:


* 3000 iterations with a learning step of 1.0.

* 3000 iterations with a learning step of 0.1.

* Minibatches of 100 examples.

* Five gradient descent steps per minibatch.


What comes out is a network that has an average per-pixel error of 0.031, which is pretty good.  However, if we look at [Figure 1](http://i.imgur.com/u8HH11W.png) on the image album, you can see a visualization of the weight matrix between the input and first hidden layer (each image is made up of the weights that are incident on a single neuron).  They are more structured than noise, but they still don't tell us much.




Next, let's add a sparsity metric to the model, as well as some weight regularization.  Everything else stays the same, but now we penalize neurons for having activations that deviate from a target value (in our case, 0.01), and also penalize weights for growing too large.  The technique used below is detailed here: 

http://deeplearning.stanford.edu/wiki/index.php/Autoencoders_and_Sparsity 


See [Figure 2](http://i.imgur.com/JrsOzOS.png) for a picture of the weights that emerge after training.  Average error per pixel is 0.88, so we've lost some performance.  The weights seem to be trending toward edge detectors, but they aren't there yet.  What happens if we keep everything the same, but train the model for twice as many iterations (6000 at each step size)?  See [Figure 3](http://i.imgur.com/QcawaEJ.png).  That's more like it.  Average error is now 0.083.  Getting better.


So, great.  Can we do better?  It turns out we can.  RMSProp is a variant of SGD, and when combined with Nesterov momentum, can converge very quickly.  [Figure 4](http://i.imgur.com/yriPReW.png) shows the result we get when we train the same network, with the same sparsity and weight regularization parameters, for 3000 iterations of RMSProp.  Average error is 0.076.  That's the best so far.  



Can we do better still?  There are three things left to try that involve eschewing the idea of sparsity penalties during training altogether.  A technique called Dropout involves simply removing half the neurons during each minibatch.  [Figure 5](http://i.imgur.com/AhEl6js.png) shows the result we turn off all the sparsity and regularization, and implement dropout.  It looks a little like Figure 1.  The average error per pixel is 0.069.  We're still using sigmoidal units and SGD, and we're still training from 6000 iterations using a learning step of 1.0 and another 6000 steps at 0.1.  This is the best performance observed for a model that is attempting to be sparse, but we don't see edge detectors in the weights.  Why?



Next, let's add a different type of weight regularization.  Instead of simply penalizing weights for growing too large, let's threshold weights based on the L2 norm of all the weights feeding into a hidden neuron.  If the L2 norm passes a threshold, the weights are scaled so that the threshold is no longer broken.  [Figure 6](http://i.imgur.com/4ec08n1.png) shows the result when we threshold the weights to 1.0.  These look much more like edge detectors.  The average error per pixel for SGD, dropout, and weight regularization is 0.087.  Dropout seems to have a strong interacting with weight regularization.  


Finally, we can try using Rectified Linear Units (ReLU) instead of sigmoids.  [Figure 7](http://i.imgur.com/zexXfeE.png) shows the network trained with SGD, dropout, L2-norm weight regularization, and ReLU units.  Not so good anymore.  The average error per pixel is 0.41-- The output is barely better than noise.  



What went wrong?  Maybe the regularization threshold was set too low.  [Figure 8](http://i.imgur.com/VoXzLkP.png) shows the result when the threshold is set to 5.0.  Almost identical.  The error is still 0.41.  



Maybe ReLU units are only effective on large, deep networks and not small ones?  [Figure 9](http://i.imgur.com/xWQSRT8.png) shows the result when the network size is changed from 64-25-64 to 64-50-50-50-64 (other than that, the same configuration as figure 7 is used).  The weights between the input and first hidden layer are shown.  It looks like a few copies of one edge detector might be emerging, but it doesn't look like very much is being learned.  




So what is the story with ReLU units?  They're supposed to be the state of the art for deep learning, but in this case they don't seem to be able to learn much at all.  Are they being used improperly in the examples above?


**Edit:** I recalled the way that the data was preprocessed incorrectly.  See above for correction.",18,17
190,2014-3-30,2014,3,30,23,21qz6o,Machine Learning and Law - New Article,https://www.reddit.com/r/MachineLearning/comments/21qz6o/machine_learning_and_law_new_article/,cranburr,1396191529,,3,5
191,2014-3-31,2014,3,31,1,21r720,Google trains neural network in high-level visual feature recognition using only unlabled images,https://www.reddit.com/r/MachineLearning/comments/21r720/google_trains_neural_network_in_highlevel_visual/,TomBluefeather,1396197627,"This is the abstract, link to full PDF at the bottom of the page.

Edit:
Sorry, link didn't post for some reason. Actual paper is at: http://static.googleusercontent.com/media/research.google.com/en//archive/unsupervised_icml2012.pdf (Thanks higgs241!)",9,0
192,2014-3-31,2014,3,31,2,21redc,Big data: are we making a big mistake? - FT.com,https://www.reddit.com/r/MachineLearning/comments/21redc/big_data_are_we_making_a_big_mistake_ftcom/,tomaskazemekas,1396202381,,25,23
193,2014-3-31,2014,3,31,3,21rfbh,An AI that mimics our neocortex is taking on the neural networks,https://www.reddit.com/r/MachineLearning/comments/21rfbh/an_ai_that_mimics_our_neocortex_is_taking_on_the/,numenta,1396203021,,33,50
194,2014-3-31,2014,3,31,7,21s1vi,How much math do you recommend to learn before starting learning machine learning?,https://www.reddit.com/r/MachineLearning/comments/21s1vi/how_much_math_do_you_recommend_to_learn_before/,onewugtwowugs,1396217508,"I'm currently a student in computational linguistics, where we have scratched the surface of machine learning, learning how to implement and apply simple neural networks and decision trees. I find the subject very fascinating, but at the same time feel like I'm lacking the necessary math skills for actually understanding what's going on. I have some math background already, having finished courses in single-variable calculus and basic linear algebra, and it definitely helped to some degree, but not nearly enough. 

My questions are, if I continue taking some extra math courses in linear algebra, multi-variable calculus and probability, will it benefit my understanding of machine learning enough for it being worth it, and is there a point when it's worth stopping and start focusing on actual machine learning?",7,5
195,2014-3-31,2014,3,31,8,21s8rx,Mining Association Rules With R And Postgres,https://www.reddit.com/r/MachineLearning/comments/21s8rx/mining_association_rules_with_r_and_postgres/,snowelephant,1396222171,,0,4
196,2014-3-31,2014,3,31,10,21sj9f,Transforming Autoencoders,https://www.reddit.com/r/MachineLearning/comments/21sj9f/transforming_autoencoders/,captcompile,1396229317,,4,10
197,2014-3-31,2014,3,31,13,21sxxh,Fast and memory efficient Naive Bayes library for Python,https://www.reddit.com/r/MachineLearning/comments/21sxxh/fast_and_memory_efficient_naive_bayes_library_for/,v1v3kn,1396239778,,4,2
198,2014-3-31,2014,3,31,15,21t7cf,Movie Review Sentiment Analysis with Vowpal Wabbit (Kaggle Contest),https://www.reddit.com/r/MachineLearning/comments/21t7cf/movie_review_sentiment_analysis_with_vowpal/,vodkagoodmeatrotten,1396248634,,0,3
199,2014-3-31,2014,3,31,16,21t84r,Yahoo! Labs Datasets,https://www.reddit.com/r/MachineLearning/comments/21t84r/yahoo_labs_datasets/,vodkagoodmeatrotten,1396249594,,4,62
200,2014-3-31,2014,3,31,16,21tadw,Efficient Data Streaming in Python,https://www.reddit.com/r/MachineLearning/comments/21tadw/efficient_data_streaming_in_python/,piskvorky,1396252376,,0,10
201,2014-3-31,2014,3,31,21,21tmg0,Convolutional Neural Net Features off-the-shelf: an Astounding Baseline for Recognition,https://www.reddit.com/r/MachineLearning/comments/21tmg0/convolutional_neural_net_features_offtheshelf_an/,[deleted],1396268324,,0,2
202,2014-3-31,2014,3,31,22,21trfs,"Examples for solving pattern classification problems in Python (IPython Notebooks) - Just the beginning, everyone is welcome to contribute!",https://www.reddit.com/r/MachineLearning/comments/21trfs/examples_for_solving_pattern_classification/,[deleted],1396272938,,0,1
203,2014-3-31,2014,3,31,22,21tsfw,"xamples for solving pattern classification problems in Python (IPython Notebooks) - just the beginning, everyone is welcome to contribute",https://www.reddit.com/r/MachineLearning/comments/21tsfw/xamples_for_solving_pattern_classification/,[deleted],1396273752,,0,1
204,2014-3-31,2014,3,31,22,21tsix,"Examples for solving pattern classification problems in Python (IPython Notebooks) - just the beginning, everyone is welcome to contribute",https://www.reddit.com/r/MachineLearning/comments/21tsix/examples_for_solving_pattern_classification/,[deleted],1396273823,,0,1
205,2014-3-31,2014,3,31,22,21tskv,"Examples for solving pattern classification problems in Python (IPython Notebooks) - just the beginning, everyone is welcome to contribute",https://www.reddit.com/r/MachineLearning/comments/21tskv/examples_for_solving_pattern_classification/,[deleted],1396273862,,0,0
