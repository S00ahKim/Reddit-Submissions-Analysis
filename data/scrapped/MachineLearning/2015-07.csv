,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2015-7-1,2015,7,1,10,3bp6za,"Reddit, we're building personalized AI chatbots. We're looking for ten volunteers who have a lot of writing online and want to help train a Twitterbot based on their personality.",https://www.reddit.com/r/MachineLearning/comments/3bp6za/reddit_were_building_personalized_ai_chatbots/,theorpheus,1435712835,,0,0
1,2015-7-1,2015,7,1,10,3bp8kw,Can deep learning help you find the perfect match?,https://www.reddit.com/r/MachineLearning/comments/3bp8kw/can_deep_learning_help_you_find_the_perfect_match/,alexmlamb,1435713870,,2,0
2,2015-7-1,2015,7,1,10,3bpa1y,Upcoming areas in theoretical Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3bpa1y/upcoming_areas_in_theoretical_machine_learning/,[deleted],1435714647,"Hi all,

I will soon be going for a postdoc position at a new university. One thing that the panelist asked me to think of is if I were to start, what would I work on.

Now the thing is so far with my PhD someone has always given me a problem and I went away and worked out the maths behind it. 

So my question is what are the upcoming areas in theoretical BAYESIAN machine learning. My speciality I would say is in Variational Bayes. I like this area because 1. Fast compared to sampling 2. There is quite a bit of maths involved.

New areas that I want to get into is stochastic Variational Bayes. This is a technique that helps deal with big data. This answer in itself didn't seem satisfactory. So I'm putting it out there to see what would be a good area/ project to work on. Something theoretical but might have a real world application. I'm happy to consider things like Deep Learning as long as there are mathematical problems involved.

p.s. yes I realise this might be quite open ended",0,1
3,2015-7-1,2015,7,1,11,3bpfn7,"Incremental Auto Encoder, is there a better way?",https://www.reddit.com/r/MachineLearning/comments/3bpfn7/incremental_auto_encoder_is_there_a_better_way/,Aerospacio,1435717421,"I'm planning to use an incremental auto encoder on a reinforcement learning problem. Is there a better way to reduce dimensionality of data ""on the go""? Also anyone ever found code related to this neural network? I've tried my best but only found some papers, 0 code.

Thanks in advance!",1,0
4,2015-7-1,2015,7,1,13,3bpw3o,"CPython Integration in Weka, including scikit-learn",https://www.reddit.com/r/MachineLearning/comments/3bpw3o/cpython_integration_in_weka_including_scikitlearn/,tariban,1435726479,,0,1
5,2015-7-1,2015,7,1,15,3bq5sa,"""Hot News"" detection using Wikipedia",https://www.reddit.com/r/MachineLearning/comments/3bq5sa/hot_news_detection_using_wikipedia/,john_philip,1435732842,,0,1
6,2015-7-1,2015,7,1,17,3bqcio,Language Understanding for Text-based Games Using Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/3bqcio/language_understanding_for_textbased_games_using/,mrkulk,1435738257,,2,38
7,2015-7-1,2015,7,1,18,3bqgmb,Vectors vs loops - which one to use?,https://www.reddit.com/r/MachineLearning/comments/3bqgmb/vectors_vs_loops_which_one_to_use/,Piz-dur,1435741846,"I'm finishing Andrew Ng's course right now and I'd like to ask you one thing. Within the whole course Andrew encourages to use vectorized implementation, not loops, cause as he says it's more efficient. He also uses Octave in this course.  
What I want to ask is, should I use vectorized implementations when using Python (numpy, scipy - right now I'm starting to learn it) or just normal loops as Python advantage is this smooth and easy way of writing code.",7,0
8,2015-7-1,2015,7,1,21,3bqsle,Lille / ICML Hotel Room Available,https://www.reddit.com/r/MachineLearning/comments/3bqsle/lille_icml_hotel_room_available/,jds4572,1435752015,"My colleague and I booked a hotel room in Lille for ICML, but we were recently offered a room for free, so we are now trying to sell the original (which is non-refundable).

Details

- Hotel: Sjours &amp; Affaires
- Dates: July 5th - July 12th
- Price (discounted): 700 euro
- Fits 3-4 people

If interested, please contact me at 1ste1@notsharingmy.info
(I'll reply with my real email address and contact info)
",0,1
9,2015-7-1,2015,7,1,21,3bqtcu,Google dev apologizes after Photos app tags black people as gorillas,https://www.reddit.com/r/MachineLearning/comments/3bqtcu/google_dev_apologizes_after_photos_app_tags_black/,Toadywart,1435752549,,0,1
10,2015-7-1,2015,7,1,23,3brb5v,I'm searching Principles of neurodynamics by Frank Rosenblatt,https://www.reddit.com/r/MachineLearning/comments/3brb5v/im_searching_principles_of_neurodynamics_by_frank/,ArgonJargon,1435762502,"Complete title: ""Principles of neurodynamics: perceptrons and the theory of brain mechanisms"" by Frank Rosenblatt

Anybody know where to find it?
",3,0
11,2015-7-1,2015,7,1,23,3brbip,LSTM in industry,https://www.reddit.com/r/MachineLearning/comments/3brbip/lstm_in_industry/,solololol,1435762669,"I'd be interested to know who's using LSTM in production in industry. 

I know that research scientists at Google for instance are working on it, and I understand that Baidu and Microsoft too use it on language. But can anyone think of other players, especially start ups?",12,1
12,2015-7-2,2015,7,2,0,3brjog,Book: Raspberry Pi Computer Vision Programming,https://www.reddit.com/r/MachineLearning/comments/3brjog/book_raspberry_pi_computer_vision_programming/,[deleted],1435766371,,0,0
13,2015-7-2,2015,7,2,1,3brme7,How can a sentence or a document be converted to a vector?,https://www.reddit.com/r/MachineLearning/comments/3brme7/how_can_a_sentence_or_a_document_be_converted_to/,DSLSharedTask,1435767487,,4,2
14,2015-7-2,2015,7,2,1,3brpre,"With results this good, it's no wonder why google wanted to patent classification",https://www.reddit.com/r/MachineLearning/comments/3brpre/with_results_this_good_its_no_wonder_why_google/,thirtymindin,1435768932,,141,248
15,2015-7-2,2015,7,2,2,3bru35,Machine learning application in formal philosophy.,https://www.reddit.com/r/MachineLearning/comments/3bru35/machine_learning_application_in_formal_philosophy/,AintNoFortunateSon,1435770805,Does anyone know of any applications of machine learning to formal philosophy?,3,0
16,2015-7-2,2015,7,2,2,3brxtp,Difference between Machine Learning and Statistical Modelling,https://www.reddit.com/r/MachineLearning/comments/3brxtp/difference_between_machine_learning_and/,[deleted],1435772438,,3,0
17,2015-7-2,2015,7,2,3,3bs1v1,airbnb has published an open source machine learning library with an emphasis on interactivity [link to git in comments],https://www.reddit.com/r/MachineLearning/comments/3bs1v1/airbnb_has_published_an_open_source_machine/,hammerheadquark,1435774177,"https://github.com/airbnb/aerosolve

They apparently used it for determining their dynamic pricing.

The library itself looks like it's mostly used for training generalized additive models. I haven't had a chance to play around with it yet, but it looks pretty neat.",0,4
18,2015-7-2,2015,7,2,3,3bs7ao,The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems,https://www.reddit.com/r/MachineLearning/comments/3bs7ao/the_ubuntu_dialogue_corpus_a_large_dataset_for/,pierrelux,1435776473,,4,9
19,2015-7-2,2015,7,2,4,3bsa7v,Clustering and Visualizing Technology Skills,https://www.reddit.com/r/MachineLearning/comments/3bsa7v/clustering_and_visualizing_technology_skills/,simonhughes22,1435777703,,2,0
20,2015-7-2,2015,7,2,5,3bspat,Best resource to learn about the relationship between machine learning and the health field?,https://www.reddit.com/r/MachineLearning/comments/3bspat/best_resource_to_learn_about_the_relationship/,tapesofwrath,1435784168,"I am studying electrical engineering in school and have taken a side interest in machine learning.

One of the broad areas of application that excites me is the medical field. Are there any websites or books that give an in-depth explanation of the different applications of ML in the medical field? 

Thanks!",0,0
21,2015-7-2,2015,7,2,6,3bss5y,Star Coordinate Visualizer Tool,https://www.reddit.com/r/MachineLearning/comments/3bss5y/star_coordinate_visualizer_tool/,Goldbar7,1435785423,Does anyone know any good tools for visualizing multivariate data? I specifically want to use a tool that can do star coordinates.,0,0
22,2015-7-2,2015,7,2,6,3bsv15,Could Google's Attempt to Patent Classification Be a Benevolent Hoax?,https://www.reddit.com/r/MachineLearning/comments/3bsv15/could_googles_attempt_to_patent_classification_be/,SupportVectorMachine,1435786659,"Other posts in this thread have discussed Google's patent application for ""[Classifying Data Objects](http://www.freepatentsonline.com/y2015/0178383.html),"" whose description is so vague as to seem to claim ownership of the very old idea of classification itself.  Reading through it, though, I began to wonder whether **this application is part of a [Sokal](https://en.wikipedia.org/wiki/Sokal_affair)-style gambit whose ultimate goal is to overhaul the patent process for algorithms and similar intellectual property**.

Could Google be trying to make a point amid all the patent trolling going on these days?  In other words, if the patent *is* granted, could Google come out and say that it was intentionally written to be as vague as possible and designed to hide (albeit poorly) how derivative the idea ultimately is?  Could one of the biggest companies in the world then publicly say that its own patent should never have been granted?

A Google-engineered embarrassment for the USPTO could be enough to turn the tide for how tech patents are evaluated.  Such [point-making hoaxes](https://en.wikipedia.org/wiki/Sokal_affair#Similar_incidents) have occurred before, with the ""[Sokal affair](https://en.wikipedia.org/wiki/Sokal_affair)"" being one of the most famous.

Or ... maybe Google has finally grown tired of [its corporate motto](https://en.wikipedia.org/wiki/Don't_be_evil).

",12,0
23,2015-7-2,2015,7,2,6,3bsx7h,Designing Machine Learning Models: A Tale of Precision and Recall,https://www.reddit.com/r/MachineLearning/comments/3bsx7h/designing_machine_learning_models_a_tale_of/,aWildTinoAppears,1435787618,,0,0
24,2015-7-2,2015,7,2,7,3bt02m,10 Billion Parameter Neural Networks in your Basement,https://www.reddit.com/r/MachineLearning/comments/3bt02m/10_billion_parameter_neural_networks_in_your/,[deleted],1435788932,,0,0
25,2015-7-2,2015,7,2,7,3bt5f5,DeepDream - a code example for visualizing Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3bt5f5/deepdream_a_code_example_for_visualizing_neural/,doomie,1435791428,,10,35
26,2015-7-2,2015,7,2,8,3bt5tk,"Has there been any work on reducibility of artificial neural networks, i.e. can we remove some nodes and still have a network with roughly the same accuracy?",https://www.reddit.com/r/MachineLearning/comments/3bt5tk/has_there_been_any_work_on_reducibility_of/,SpaceEnthusiast,1435791616,"To be more precise, can we remove a sizable fraction of all nodes and still retain an accuracy similar to that of the larger network? To me, this is very much in the flavor of compressibility of data. My optimistic guess is that we should be able to compress a neural network significantly before a significant degradation in accuracy occurs.",9,10
27,2015-7-2,2015,7,2,8,3bt60n,Would you benefit from Deep Learning examples &amp; tutorials?,https://www.reddit.com/r/MachineLearning/comments/3bt60n/would_you_benefit_from_deep_learning_examples/,JasonSilvermann,1435791709,"I'm wanting to build a sort of blog that dives into different aspects of Deep Learning,  a project based/case study style learning experience.

Assuming the content was good, would you subscribe to a blog like that? Would you be interested in working through the problems?",18,14
28,2015-7-2,2015,7,2,8,3bt7d3,Evaluating ranking algorithms,https://www.reddit.com/r/MachineLearning/comments/3bt7d3/evaluating_ranking_algorithms/,futrawo,1435792371,"I'm interested in looking at several different metrics for ranking algorithms - there are a few listed on the [Learning to Rank](https://en.wikipedia.org/wiki/Learning_to_rank#Evaluation_measures) wikipedia page, including: 

* Mean average precision (MAP);
* DCG and NDCG;
* Precision@n, NDCG@n, where ""@n"" denotes that the metrics are evaluated only on top n documents;
* Mean reciprocal rank;
* Kendall's tau
* Spearman's Rho
* Expected reciprocal rank 
* Yandex's pfound

but it isn't clear to me what are the advantages/disadvantages of each or when you may choose one over another (or what it would mean if one algorithm outperformed another on NDGC but was worse when evaluated with MAP).

Is there anywhere I can go to learn more about these questions?",1,1
29,2015-7-2,2015,7,2,8,3bta4h,CNN-Vis: Generating images similar to Google's Inceptionism.,https://www.reddit.com/r/MachineLearning/comments/3bta4h/cnnvis_generating_images_similar_to_googles/,samim23,1435793758,,1,18
30,2015-7-2,2015,7,2,9,3btdc9,"Collective Intelligence predicted winners of Super Bowl, Stanley Cup, and NBA finals.",https://www.reddit.com/r/MachineLearning/comments/3btdc9/collective_intelligence_predicted_winners_of/,[deleted],1435795390,,2,0
31,2015-7-2,2015,7,2,10,3btmw3,Is Weka still important?,https://www.reddit.com/r/MachineLearning/comments/3btmw3/is_weka_still_important/,YourWelcomeOrMine,1435800456,I haven't heard much about it lately. It seems to have been completely eclipsed by sk-learn. I haven't found anything I still need it for.,10,1
32,2015-7-2,2015,7,2,13,3bu4e9,Trending at Instagram,https://www.reddit.com/r/MachineLearning/comments/3bu4e9/trending_at_instagram/,sko2sko,1435810366,,1,10
33,2015-7-2,2015,7,2,13,3bu55q,"Applying Google's ""Inceptionism"" to famous art.",https://www.reddit.com/r/MachineLearning/comments/3bu55q/applying_googles_inceptionism_to_famous_art/,_bskaggs,1435810818,,92,255
34,2015-7-2,2015,7,2,14,3bu92z,Awesome Resources for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3bu92z/awesome_resources_for_deep_learning/,john_philip,1435813328,,2,2
35,2015-7-2,2015,7,2,14,3bub7m,Are there any open data sets for supply chain analysis?,https://www.reddit.com/r/MachineLearning/comments/3bub7m/are_there_any_open_data_sets_for_supply_chain/,grandzooby,1435814697,"There are often standard datasets that pop up in books like iris, optdigits, etc.  

I'm looking for an open sample data set that might be used in Operations Research that comes from a supply chain.  I'm thinking things like customer orders, factory orders, transit times, inventory levels, stock-outs, etc.  My plan is to work on applying reinforcement learning to try and develop agents that can make good decisions within a supply chain and I'm hoping to use the data to either directly train it, or build a simulation model for it to train on.

Does anyone know of ""standard"" data set like this or even just an open set of supply chain data like this?



Note:  Personally, I can get and use data like this from the company I work for.  But I can't share the data and I'm hesitant to publish any work without the data to replicate the results.",3,1
36,2015-7-2,2015,7,2,14,3budfa,Is this possible in Theano?,https://www.reddit.com/r/MachineLearning/comments/3budfa/is_this_possible_in_theano/,[deleted],1435816310,"Can I define a series of ""symbolic links"" with a for loop instead of explicitly writing each one? I want to create a class that can generate networks of arbitrary length. Sorry if this doesn't make sense. I just got Theano for its Tensor and Convolution features and this whole symbolic, graph thing is a bit foreign. ",0,0
37,2015-7-2,2015,7,2,15,3buf2v,googlenet dream _,https://www.reddit.com/r/MachineLearning/comments/3buf2v/googlenet_dream__/,thirtymindin,1435817490,,5,16
38,2015-7-2,2015,7,2,15,3buifd,Is there an accessible implementation of any of the image captioning neural nets that are so popular?,https://www.reddit.com/r/MachineLearning/comments/3buifd/is_there_an_accessible_implementation_of_any_of/,kingsoftheon,1435819986,"I'm not necessarily looking for a trainable network. A general purpose pre-trained model will also suffice.  Basically, something I can plug in images to and see the quality of image titles generated.",3,0
39,2015-7-2,2015,7,2,16,3bujlj,Global Fabric Burning Behaviour Tester Industry 2015 Market Research Report,https://www.reddit.com/r/MachineLearning/comments/3bujlj/global_fabric_burning_behaviour_tester_industry/,stevensrachel,1435820886,,0,2
40,2015-7-2,2015,7,2,18,3burzn,BigML Opening New Lab in Valencia to Harness Innovation and Drive European Expansion,https://www.reddit.com/r/MachineLearning/comments/3burzn/bigml_opening_new_lab_in_valencia_to_harness/,czuriaga,1435828373,,0,1
41,2015-7-2,2015,7,2,18,3buv7x,Is there anyone who's learning AI programming and likes to practice with hobby projects? I'm looking for someone who would like to help me build a completely useless app just for fun.,https://www.reddit.com/r/MachineLearning/comments/3buv7x/is_there_anyone_whos_learning_ai_programming_and/,live_love_laugh,1435831150,"I hope you don't find me rude for asking. It's just that I have an image in my head that I'd really like to make a reality, but I utterly suck at AI-programming.

The thing I have in mind is this:

A white background filled with colored dots. Every colored dot is like a living being, meaning it can take a few actions, it has a few needs and it has a few feelings. The actions it can take are: changing its color and moving around. The needs it has are:

* Being around other dots X percent of the time
* Being alone (100 - X) percent of the time
* Being (around) cold colors Y percent of the time
* Being (around) warm colors (100 - Y) percent of the time
* Dancing and moving Z percent of the time
* Staying calm and still (100 - Z) percent of the time.

X, Y and Z differ per dot and that's what makes every dot a unique living being. And finally it feels pleasant feelings when it just fulfilled one of its needs and it feels unpleasant feelings when one of its needs have not been fulfilled recently.

A dot is not aware of its needs, its only aware of its feelings and tries to take actions that lead to pleasant feelings.

I imagine that this app would create a beautiful ""dance of life"" to look at. All the dots will just try stuff out to find out what's the nicest way to live. And what I most like about this app is the very clear absence of right and wrong / normal and abnormal. Some dots will like to dance and change color wildly all the time and other dots will like to stay still and stay the same color all the time and neither is better than the other.

Could anyone enjoy making this?",15,2
42,2015-7-2,2015,7,2,22,3bveqf,"Inspired from TV series Numb3rs and POI ,I made my first android game with Rudimentary AI.",https://www.reddit.com/r/MachineLearning/comments/3bveqf/inspired_from_tv_series_numb3rs_and_poi_i_made_my/,[deleted],1435844864,"I saw a game described in Numb3rs S06E09 i was not able to find a good version of it online so i decided to make it myself it took me around 5 months to do it from scratch . 

At the heart of my game is a rudimentary AI program that record and learn from the users moves , WITHOUT IT the game is not worth playing .Basically it uses simplest variation of C4.5 algorithm mixed with many tricks to keep it together . It also uses a crude decision tree to make initial moves .

You people might enjoy it . [Here is the game available for free.](https://play.google.com/store/apps/details?id=air.randomizerapps.millitaire)

I would be happy to hear any suggestions you have about this ,I am completely new to field of AI and Machine Learning is there any better algorithm that can be used for this type of game ? 

EDIT: POI = Person Of Interest (CBS)
",0,0
43,2015-7-2,2015,7,2,23,3bvi3u,Jumping on the DeepDream Bandwagon...Gif From Random Noise as Seed,https://www.reddit.com/r/MachineLearning/comments/3bvi3u/jumping_on_the_deepdream_bandwagongif_from_random/,Kombutini,1435846643,,7,5
44,2015-7-2,2015,7,2,23,3bvl36,Why data preparation frameworks rely on human-in-the-loop systems,https://www.reddit.com/r/MachineLearning/comments/3bvl36/why_data_preparation_frameworks_rely_on/,gradientflow,1435848135,,0,0
45,2015-7-3,2015,7,3,0,3bvr3n,"/r/deepdream: for posting ""Inceptionism"" images, generating your own, tips/ideas, and relevant.",https://www.reddit.com/r/MachineLearning/comments/3bvr3n/rdeepdream_for_posting_inceptionism_images/,Fred_Flintstone,1435850937,,1,9
46,2015-7-3,2015,7,3,1,3bw09h,Scientists develop real-life What-If Machine to produce AI-created fiction,https://www.reddit.com/r/MachineLearning/comments/3bw09h/scientists_develop_reallife_whatif_machine_to/,[deleted],1435855190,,0,0
47,2015-7-3,2015,7,3,1,3bw0h2,What's the best way to get hands on practice with ML?,https://www.reddit.com/r/MachineLearning/comments/3bw0h2/whats_the_best_way_to_get_hands_on_practice_with/,kjfries,1435855302,I'm a grad student currently doing a bit of very specific work with Gaussian Processes which has got me interested in general ML. How can I start messing around with ML more in my spare time and learn about all the different tools/applications?,7,2
48,2015-7-3,2015,7,3,1,3bw1xr,[1507.00210] Natural Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3bw1xr/150700210_natural_neural_networks/,downtownslim,1435855969,,13,25
49,2015-7-3,2015,7,3,2,3bw45c,"In neural networks, is there any algorithm/literature which uses different-dynamic activation functions for each/group of neurons?",https://www.reddit.com/r/MachineLearning/comments/3bw45c/in_neural_networks_is_there_any/,atmb4u,1435856981,"It sounds very obvious that along with the change in weights, activation functions too should have the ability to change. Can someone please explain on this front.",6,0
50,2015-7-3,2015,7,3,2,3bw7wz,Make your own Deep Dreams,https://www.reddit.com/r/MachineLearning/comments/3bw7wz/make_your_own_deep_dreams/,LLCoolZ,1435858726,,2,0
51,2015-7-3,2015,7,3,3,3bwfmq,"BigML Acquires Best-of-Class Association Discovery Technology Magnum Opus, Expands Product Offering",https://www.reddit.com/r/MachineLearning/comments/3bwfmq/bigml_acquires_bestofclass_association_discovery/,czuriaga,1435862109,,0,1
52,2015-7-3,2015,7,3,4,3bwki8,A bit of help with LDA visualizations,https://www.reddit.com/r/MachineLearning/comments/3bwki8/a_bit_of_help_with_lda_visualizations/,mizay7,1435864291,"Hi,

I like the idea of turning the topic mappings from LDA into word clouds with the relative 'importance' of each term being represented by size in the cloud. Something similar as what is done [here](http://voidpatterns.org/2015/03/conspiracy-theories-topic-modeling-keyword-extraction/). Going by the blog post it seems that the beta parameter is what gives us the relative weight of each term in the topic. When I examine my betas I get values ranging from approximately  -6 to -12. The negative values suggest that maybe this is a log likelihood measure? I don't really have the stats background to confidently say how to best map the betas to a metric that could play nice with word clouds frequency.  parameter. Any advice is much appreciated.

I am not sure if this is a subreddit that is okay with people asking for help, but everything else on this topic is pretty dead. Happy to re-post in a more appropriate place if needed. Also, it doesn't really matter in this case but I am implementing this in R. 

Thanks. 

edit: 24 hours and 2 downvotes, and zero upvotes. I guess r/machinelearning is not the place to ask for help? ",2,0
53,2015-7-3,2015,7,3,4,3bwl3a,Tutorial: Getting Started with Machine Learning with the SciPy stack,https://www.reddit.com/r/MachineLearning/comments/3bwl3a/tutorial_getting_started_with_machine_learning/,ayoungprogrammer,1435864551,,0,0
54,2015-7-3,2015,7,3,4,3bwqz2,Representation of misspelled words for neural network?,https://www.reddit.com/r/MachineLearning/comments/3bwqz2/representation_of_misspelled_words_for_neural/,SkiddyX,1435867167,"While thinking about a neural network based spellchecker, I was thinking about word embedding not being able to represent any ""unique"" (misspelled) words that the model haven't seen before. I tried one-hot encoding and had huge dimensions, bag of words didn't maintain the order and i'm lost on what to try next. Character level embeddings? Any advice would be appreciated. 
Do any of you have any ideas on how to do this representation?",7,1
55,2015-7-3,2015,7,3,7,3bxa0q,Solving Intelligence and Machine Learning Fundamentals - Talking Machines,https://www.reddit.com/r/MachineLearning/comments/3bxa0q/solving_intelligence_and_machine_learning/,dustintran,1435876081,,0,1
56,2015-7-3,2015,7,3,7,3bxb23,"Critique of Paper by ""Deep Learning Conspiracy"" (Nature 521 p 436)",https://www.reddit.com/r/MachineLearning/comments/3bxb23/critique_of_paper_by_deep_learning_conspiracy/,[deleted],1435876589,,0,1
57,2015-7-3,2015,7,3,7,3bxb6q,"Schmidhuber's response to LeCunn, Bengio, and Hinton's Deep Learning paper in Nature",https://www.reddit.com/r/MachineLearning/comments/3bxb6q/schmidhubers_response_to_lecunn_bengio_and/,rasbt,1435876654,,46,154
58,2015-7-3,2015,7,3,8,3bxfha,Open-source-machine-learning-degree,https://www.reddit.com/r/MachineLearning/comments/3bxfha/opensourcemachinelearningdegree/,Nixonite,1435878802,,5,17
59,2015-7-3,2015,7,3,8,3bxiuc,A talk on Structured Prediction by Sebastian Nowozin (NIPS - 2014),https://www.reddit.com/r/MachineLearning/comments/3bxiuc/a_talk_on_structured_prediction_by_sebastian/,[deleted],1435880608,,2,2
60,2015-7-3,2015,7,3,8,3bxk6a,What is data or outcomes are valuable to businesses?,https://www.reddit.com/r/MachineLearning/comments/3bxk6a/what_is_data_or_outcomes_are_valuable_to/,[deleted],1435881301,"I've been circling the idea of getting into data mining/machine learning for a while, but I'm not really sure how it's applied to business decisions--other than the obvious ones.  

For example, what information could I dig up and analyze that a fund manager would use to inform their decision? 

Sorry, I'm not wording this well. Basically, what data should is ""gold"" and not common, and what presentation gets someone a check for $10,000 instead of $10?",2,0
61,2015-7-3,2015,7,3,12,3by71k,What intermediate level resources helped you to get into neural networks?,https://www.reddit.com/r/MachineLearning/comments/3by71k/what_intermediate_level_resources_helped_you_to/,bit_by_byte,1435893745,"I've been struggling to find a good resource for me to get a practical understanding of neural networks. When searching, I either find a lightweight description similar to: ""neural networks are basically an algorithm that mimics the way neurons work""...or I get really technical and/or theoretical, often math-based, explanations. When I do find a very practical guide, it's specific to a particular code package instead of a general description of methodologies (more of a ""here's how you build networks using Pybrain"" instead of ""here's how to analyze and tweak your networks to improve performance"").


I don't mean to undermine the theory of neural networks -- I just can't find any good comprehensive resources that go over how to actually manage the topology of networks for different tasks. Maybe it's not as established and consistent as I'm expecting...it seems like people just kind of play around with them until they work noticeably better.

How many layers should I use? How many nodes per layer? Should each layer have less or more than the previous or should they be random? Should I train one layer at a time? What are the general disadvantages/advantages of different topologies? 

Maybe I'm casting too large of a net. If that's the case, it might help to briefly describe the task at hand. In a few words, I'm trying to classify 80GB of high-res images of retinas into 1 of 5 groups.",9,0
62,2015-7-3,2015,7,3,13,3byg59,Learning and project advice,https://www.reddit.com/r/MachineLearning/comments/3byg59/learning_and_project_advice/,mlresearch,1435898731,,1,0
63,2015-7-3,2015,7,3,13,3bygn5,Generalization Bounds for Neural Networks through Tensor Factorization,https://www.reddit.com/r/MachineLearning/comments/3bygn5/generalization_bounds_for_neural_networks_through/,robertsdionne,1435899003,,1,13
64,2015-7-3,2015,7,3,14,3byk8a,Accelerating Stochastic Gradient Descent via Online Learning to Sample,https://www.reddit.com/r/MachineLearning/comments/3byk8a/accelerating_stochastic_gradient_descent_via/,robertsdionne,1435901107,,1,5
65,2015-7-3,2015,7,3,16,3byvco,An executives guide to machine learning,https://www.reddit.com/r/MachineLearning/comments/3byvco/an_executives_guide_to_machine_learning/,jonej,1435908260,,4,3
66,2015-7-3,2015,7,3,17,3bz0no,Normalisation of concatenated histograms for SVMs,https://www.reddit.com/r/MachineLearning/comments/3bz0no/normalisation_of_concatenated_histograms_for_svms/,stua8992,1435912148,"I'm not sure if this is the best place for this but hopefully someone around has seen something similar. Basically I'm generating a number of LBP histograms and one HoOG as well from an image, and was hoping to concatenate these histograms as my feature vector for an SVM with additive chi squared kernel. What I can't seem to get a clear answer about is the most common means of normalising the vector. For instance should I concatenate all of them and then normalise, should I normalise and then concatenate, and should I normalise again after this? I feel that the number of bins should also play a role as well lest certain elements get undue weight, but really I just don't know.
",2,1
67,2015-7-3,2015,7,3,18,3bz3yr,need help in my nlp project,https://www.reddit.com/r/MachineLearning/comments/3bz3yr/need_help_in_my_nlp_project/,tushar1408,1435914753,"for my project i need to build binarized constituency parse tree using stanford parser.
Any idea how to do it? ",5,0
68,2015-7-3,2015,7,3,18,3bz4dj,Domain-adaptation or one-shot learning in NLP,https://www.reddit.com/r/MachineLearning/comments/3bz4dj/domainadaptation_or_oneshot_learning_in_nlp/,spurious_recollectio,1435915100,"I'm trying to find any literature or approaches for some kind of transfer learning for NLP tasks (including more sophisticated tasks like info retreival and QA).  ImageNet trained CNNs turned out to generalize very well for many image-based tasks such as caption generation, etc... and I'm hoping (I'm probably not the only one) that one can use a similarly large dataset to get an RNN or LSTM to generate ""sentence"" or ""phrase"" vectors with rich language features than can then be used in multiple applications (ideally also with a limited amount of new labelled data).  This paper discusses such an idea for CNNs:

http://arxiv.org/abs/1312.6204

Where the basic idea is to do the domain adaptation in a shallow classifier on top of a deep CNN.  So I was wondering if anyone seen anything similar for NLP?",4,0
69,2015-7-4,2015,7,4,0,3c02hz,Adam Coates - 10B Parameter Neural Nets in Your Basement (Slides),https://www.reddit.com/r/MachineLearning/comments/3c02hz/adam_coates_10b_parameter_neural_nets_in_your/,vikkamath,1435937360,,3,14
70,2015-7-4,2015,7,4,0,3c03dq,Total noob to ML,https://www.reddit.com/r/MachineLearning/comments/3c03dq/total_noob_to_ml/,shubphotons,1435937776,"Hi I am a total noob in ML although I want to learn it real bad. I am fluent in Python, I need some toy projects to help me learn. Kindly suggest some resources. Also I'd prefer to keep the language Python.",9,0
71,2015-7-4,2015,7,4,1,3c0915,Amazon Machine Learning for Human Activity Recognition,https://www.reddit.com/r/MachineLearning/comments/3c0915/amazon_machine_learning_for_human_activity/,[deleted],1435940477,,0,0
72,2015-7-4,2015,7,4,1,3c0amr,Amazon Machine Learning: use cases and a real example in Python (with code),https://www.reddit.com/r/MachineLearning/comments/3c0amr/amazon_machine_learning_use_cases_and_a_real/,elenaward,1435941208,,4,65
73,2015-7-4,2015,7,4,2,3c0ece,"Critique of Paper by ""Deep Learning Conspiracy""",https://www.reddit.com/r/MachineLearning/comments/3c0ece/critique_of_paper_by_deep_learning_conspiracy/,alexeyr,1435942953,,1,2
74,2015-7-4,2015,7,4,2,3c0exb,How does you're team work?,https://www.reddit.com/r/MachineLearning/comments/3c0exb/how_does_youre_team_work/,AWKWARD_HANDS_GUY,1435943213,"Hey all,

I'm currently working on a team responsible for building predictive models. In addition, we're saddled with a bunch of other support tasks including a variety of reporting and BI responsibilities which tend to stretch us pretty thin. 

We follow a DMAIC structure. I'm not sure if this helps us or hurts us. I feel like we spend most of our time trying to nail down the business requirements of the project, and most of the time they change before we produce anything. It probably takes something like 7-8 months to produce a single model, not including implementation. 

How is your team run? Do you follow any specific project planing methodology (eg DMAIC, CRISP-DM, SEMMA, etc). How do you think it works? ",3,0
75,2015-7-4,2015,7,4,3,3c0nk8,The Infinite MNIST Dataset,https://www.reddit.com/r/MachineLearning/comments/3c0nk8/the_infinite_mnist_dataset/,vikkamath,1435947055,,0,4
76,2015-7-4,2015,7,4,4,3c11cr,Convex Optimization - Summer Study Group,https://www.reddit.com/r/MachineLearning/comments/3c11cr/convex_optimization_summer_study_group/,no_potion,1435953429,I'm planning on working through CMU's [Convex Optimization](http://www.stat.cmu.edu/~ryantibs/convexopt/) course for the remainder of my summer. If anyone else in also interested please feel free to PM me.,1,15
77,2015-7-4,2015,7,4,6,3c1gj1,"Text By the Bay 2015: Samiur Rahman, Practical NLP Applications of Deep Learning",https://www.reddit.com/r/MachineLearning/comments/3c1gj1/text_by_the_bay_2015_samiur_rahman_practical_nlp/,groundhawk2006,1435960665,,0,0
78,2015-7-4,2015,7,4,7,3c1lra,What my deep model doesn't know - Uncertainty in Neural Nets,https://www.reddit.com/r/MachineLearning/comments/3c1lra/what_my_deep_model_doesnt_know_uncertainty_in/,iori42,1435963230,,8,88
79,2015-7-4,2015,7,4,7,3c1nvf,Forecasting squared error?,https://www.reddit.com/r/MachineLearning/comments/3c1nvf/forecasting_squared_error/,TheCatelier,1435964343,"I'm working on a regression problem where it is of interest to know how reliable the model is given the features. One approach I have in mind is the following:

Suppose I have some dataset {(X1, Y1), (X2, Y2),... (Xn, Yn)}
and I train a model mapping Xi to Yi and I get predicted values {T1, T2, ..., Tn}. 
I can then train a new model with this constructed dataset: {(X1,   (T1 - Y1)^2 ) , ... , (Xn,   (Tn - Yn)^2 )}

Is this somewhat standard?
Are the less hacky alternatives? Do you know of any papers covering such a topic?",8,0
80,2015-7-4,2015,7,4,15,3c2tyg,Using Diffbot to turn the web into sets of searchable structured data,https://www.reddit.com/r/MachineLearning/comments/3c2tyg/using_diffbot_to_turn_the_web_into_sets_of/,suphper,1435989717,,0,0
81,2015-7-4,2015,7,4,18,3c36ns,Invitation to Must Attend July 2015 Machine Learning Events in Valencia and Barcelona,https://www.reddit.com/r/MachineLearning/comments/3c36ns/invitation_to_must_attend_july_2015_machine/,czuriaga,1436000583,,0,1
82,2015-7-4,2015,7,4,22,3c3n5z,Which 980 TI for deep learning?,https://www.reddit.com/r/MachineLearning/comments/3c3n5z/which_980_ti_for_deep_learning/,spurious_recollectio,1436016021,I'm considering getting a 980 TI to add to an oldish desktop (i7 2700k era).  I already have a 970 on another machine and am quite happy with it.  I see that different vendors have various modded/overclocked varieties of these GPUs available and I was wondering if any of that mattered in terms of DL.  I've read Tim Detters's very nice blog posts on this and I know its memory access rather than clock speed that matters and I guess that's not something vendors can do much about (??) but I wanted to check to be sure (and also to see if there's any reason not to run this GPU on an oldish box -- from what I've read it shouldn't matter).  Also I've yet to read any reviews of this GPU for DL but from what everyone has said about the Titan X I guess it should be really good?,28,0
83,2015-7-4,2015,7,4,22,3c3pyz,Deep Learning with Synaptic.js,https://www.reddit.com/r/MachineLearning/comments/3c3pyz/deep_learning_with_synapticjs/,[deleted],1436018072,"I've been working with synaptic.js to make neural-networks in the browser for a while and I've decided I want to implemented deep-learning for a project I'm working on ( connecting an lstm to a classifier ).

I've successfully created both networks and can project the output of the lstm to the input of the classifier, but I get issues when I try to activate and perhaps train it.

First, it trains very quickly ( but, granted, I'm training it with 2 simple, inputs so that might not be an issue ). But it trains with out error. I train it with a single input as it has one input neuron and then 5 outputs for each class.

Now when I activate it, I don't get an error, but I get that all of the outputs are undefined: `[undefined, undefined, undefined, undefined, undefined]` with an activation of `[0]`, which was explicitly trained.

I'd greatly appreciate any help in getting this small deep network working. Each network works individually, but I believe there is an issue in training both together. If I were to train them separately-- what would be the ideal-output for the lstm and the input for the classifier I would train them separately with?

[Here is synaptic: https://github.com/cazala/synaptic](https://github.com/cazala/synaptic)

[Here is my implementation](http://pastebin.com/XSjJQWT5) ( Synaptic is the majority at the top and my code is at the bottom. It can be run in the browser console. Activating it with `deep.activate([0]);` )

Thank you in Advance",7,0
84,2015-7-4,2015,7,4,23,3c3vp8,You can buy memristors (the neuron-esque computer circuit that may lead to true artificial intelligence) right now,https://www.reddit.com/r/MachineLearning/comments/3c3vp8/you_can_buy_memristors_the_neuronesque_computer/,[deleted],1436021679,,0,0
85,2015-7-5,2015,7,5,0,3c41cm,Companies/research using ML for engineering and design?,https://www.reddit.com/r/MachineLearning/comments/3c41cm/companiesresearch_using_ml_for_engineering_and/,[deleted],1436025020,"ML is being used a lot in advertising, medicine, genomics, and so on. Is anyone aware of efforts to use ML in the design of engineering solutions (battery technology, automotive, computer engineering, pharmaceuticals, etc)? I imagine that collecting information from physical experiments and simulations could yield data sets for the algorithm to analyze. It would then predict solutions to solve the problem (or part of it).

Is this actively being researched and applied? If not, what challenges are making it infeasible/impractical to do? ",0,1
86,2015-7-5,2015,7,5,1,3c42eo,ML companies/research on (physical) engineering and design applications?,https://www.reddit.com/r/MachineLearning/comments/3c42eo/ml_companiesresearch_on_physical_engineering_and/,brouwjon,1436025626,"ML is being used a lot in advertising, medicine, genomics, and so on. Is anyone aware of efforts to use ML in the design of engineering solutions (battery technology, automotive, computer engineering, pharmaceuticals, etc)? I imagine that collecting information from physical experiments and simulations could yield data sets for the algorithm to analyze. It would then predict solutions to solve the problem (or part of it).

Is this actively being researched and applied? If not, what challenges are making it infeasible/impractical to do? ",3,1
87,2015-7-5,2015,7,5,5,3c4tvs,"Does there exist a cluster I can join to share computing power for ML, and maybe use it sometimes too?",https://www.reddit.com/r/MachineLearning/comments/3c4tvs/does_there_exist_a_cluster_i_can_join_to_share/,maccam912,1436040661,"Since machine learning (at least for me) consists of idle computers that are occasionally pushed to the max to train a model, I was wondering if there is something out there where I can add my computer to a big cluster, like a community spark cluster. I'd be able to make use of compute time I otherwise wouldn't use, and could maybe turn around and have a giant cluster available to get my results fast. It sounds better than keeping a huge cluster around that I use occasionally, or the alternative of waiting forever to train because I don't want to spend money on a huge cluster.

The best personal solution I can think of is to spin up a large cluster on amazon, use it, then disassemble it after I get my results. I'm wondering if there is something out there for us to use that would let me contribute one AWS (or whatever) instance 24/7 and in return take a few minutes here and there to use this community cluster myself.

Im having a hard time putting my thoughts into words, but think of something like folding@home, but where each project runs for a short amount of time and you can submit your own jobs to be run.",22,19
88,2015-7-5,2015,7,5,5,3c4vj7,"A nice lecture on ""Recommender Systems"" by Alex Smola (CMU) 2015",https://www.reddit.com/r/MachineLearning/comments/3c4vj7/a_nice_lecture_on_recommender_systems_by_alex/,ojaved,1436041594,,0,21
89,2015-7-5,2015,7,5,5,3c4xsk,How to train lstm layer of deep-network?,https://www.reddit.com/r/MachineLearning/comments/3c4xsk/how_to_train_lstm_layer_of_deepnetwork/,[deleted],1436042847,"I'm using a lstm and feed-forward network to classify text.

I convert the text into one-hot vectors and feed each into the lstm so I can summarise it as a single representation. Then I feed it to the other network.

But how do I train the lstm? I just want to sequence classify the text should I feed it without training? I just want to represent the passage as a single item I can feed into the input layer of the classifier and then the classifier classifies it. 

I would greatly appreciate any advice with this!",16,0
90,2015-7-5,2015,7,5,7,3c5aud,Why does Google Deep Dream create dog faces? There are 189 dogs in the image set !!,https://www.reddit.com/r/MachineLearning/comments/3c5aud/why_does_google_deep_dream_create_dog_faces_there/,flexiverse,1436050485,,2,1
91,2015-7-5,2015,7,5,8,3c5f5j,Google patented Deep Q-Learning,https://www.reddit.com/r/MachineLearning/comments/3c5f5j/google_patented_deep_qlearning/,vkhuc,1436053188,,31,71
92,2015-7-5,2015,7,5,12,3c5zux,Hacker News / DataTau for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3c5zux/hacker_news_datatau_for_deep_learning/,arshakn,1436067970,,0,0
93,2015-7-5,2015,7,5,15,3c6btz,How to extract product names and product price from e-commerce site without having to rely on html tags?,https://www.reddit.com/r/MachineLearning/comments/3c6btz/how_to_extract_product_names_and_product_price/,mrTang5544,1436077823,"Hey guys, I was wondering if some sort of ML techniques exists that can extract a product title/price without having to rely on html tags. For example, if I am looking for deals for new computers I want to check across multiple merchants such as Bestbuy/Amazon/etc. to see who has the best deals. Relying on html tags is too brittle and I was wondering if someone can point me in the direction of using ML to solve this kind of problem",11,0
94,2015-7-5,2015,7,5,16,3c6f4r,Cloud computing vs personal workstation,https://www.reddit.com/r/MachineLearning/comments/3c6f4r/cloud_computing_vs_personal_workstation/,psih128,1436081004,"I'm currently running a lot of machine learning processes on AWS c4.8xlarge instance. The cost is $1,78 per hour (I know there is spot market), and the nice thing is that it's on-demand

In the meantime I've been evaluating an option to purchase powerful personal workstation. I did some research and dual E5-2630 Xeon computer will cost me around $2500 with all the parts together, that's roughly 60 days of EC2 c4.8xlarge (plus electricity bill to run this 500W monster)

What's your take on crunching numbers in the cloud vs investing into your own hardware? Do you burn CPUs and GPUs for work or for personal/consulting projects?",13,2
95,2015-7-5,2015,7,5,17,3c6k4s,NN &amp; Small Dataset: Tips?,https://www.reddit.com/r/MachineLearning/comments/3c6k4s/nn_small_dataset_tips/,ddofer,1436086394,"I'm trying out Feed-Forward neural nets on some data and features I extracted for a biological machine learning classification task (involving proteins). 
I haven't any experience with them, barring reading about the theory and playing with some CNNs on image data. 
Currently, my results are quite poor, despite a lot of playing with reasonable (I think) network parameters. 
**I want to get a ""sanity check"" regarding whether I missed something obvious**. The better parameters I could find just barely outperform (F1 &amp; ROC_AUC metrics) a bog standard logistic regression with a tuned C, implemented in scikit-learn - and an SVM with an RBF kernel strongly outperforms the LogReg itself. Being outperformed by a different model (e.g. Decision trees) would make plenty of sense, but a Neural Net should be able to do anything a LogReg can. The code/nets are all implemented in Keras and/or Lasagne (I'm experimenting with both). 

#Data Details: 
Training+ Evaluation set hold ~2,491 samples (~800 positive class) - Binary classification. Metric of choice is the F1 score. 
 Features: ~1,200 ""raw"". Filtering the set to remove zero variance features, along with a FDR (ANOVA) test (alpha = 0.01) drops that down to 500 features. (An FDR with an alpha of 0.7 drops it down to about ~900 features. Many features are co-linear; Applying a PCA on the data dropped performance notably, and reduced the NNet performance more than the LogRegs ). 
Features are sequence properties for a protein, manually extracted, and not necessarily sharing any form of spatial order (e.g. one feature is the count of each letter in a sequence). There are DEFINITELY higher order interactions between features. 
Features are a mix of variables (e.g. ""propensity for X= -0.21"") and counts (e.g. ""AA@position 7 = R"" (1 hot encoded)). Data is normalized (using sklearn). 

*So, Any tips on architectures or functions to use?*
------------------------------------------------------------------------------

&gt; Sample architectures:
Noted performance is on the validation set each time. Networks are trained, with the held-out validation set's performance use as the ""stopping"" criteria. Objective/loss function during training is the binary-cross entropy. 
-------------------------------------------------------
&gt; 7 fold stratified CV
SelectFdr = 0.6  (FDR corrected ANOVA F-test on features prior to training)
-----------
(Lasagne) NNET Params:
tanh (sigmoid, very_leaky_relu - had poorer performance)
22 epochs
adam

DenseLayer (None, 2) produces 2 outputs
DenseLayer (None, 128) produces 128 outputs
DropoutLayer p=0.1 (None, 768) produces 768 outputs
DenseLayer (None, 768) produces 768 outputs
DropoutLayer p=0.2 (None, 512) produces 512 outputs
DenseLayer (None, 512) produces 512 outputs
InputLayer (None, 875) produces 875 outputs
hidden0_num_units=512,
hidden1_num_units=768,
hidden2_num_units=256,

output_nonlinearity=softmax,
dropout0_p=0.1,
dropout1_p=0.1,
eval_size=0.001,
update=adam,
max_epochs=22,

(Lasagne) NNET perf:('NN Average ROC:', 0.8709)
LogRegCV (Cs=18) Perf: ('LogReg Average ROC:', 0.8787)

---------------------------------------------------------------
SelectFdr = 0.2 = (less feats)

Simpler (Lasagne) NNet Perf

('NN Average ROC:', 0.8753)
('LogReg Average ROC:', 0.8778)

('hidden0', DenseLayer),
('dropout0', DropoutLayer),
('hidden1', DenseLayer),

hidden0_num_units=512,
hidden1_num_units=64,
34 max epochs

&gt; I Also tried some other params.. (e.g hidden layer size; *adadelta*...) No real improvement.
---------------------------------------------------------------
&gt; 
SelectFdr = 0.6 (876 feats)
6 fold CV

('hidden0', DenseLayer),
('dropout0', DropoutLayer),
('hidden1', DenseLayer),
('dropout1', DropoutLayer),
('hidden2', DenseLayer),

hidden0_num_units= 128,
hidden1_num_units=784,
hidden2_num_units=256,

hidden1_nonlinearity=LeakyRectify(0.05),

('NN Average ROC:', 0.8769)
('LogReg Average ROC:', 0.877)

---------------------------------------------------------------
SelectFdr(alpha = 0.1)) - ('num_features:', 598)
6 epochs
6 fold CV

&gt; Keras NNet - finally got it better (key is likely in shape of ""medium-fat-thin"" layers sizes + initialization + not messing too much with fancy activations incorrectly? But # Features selection is diff!.. )

(In Keras model) validation is tested against directly, not randomly each time.

('NN Average ROC:', 0.945)
('LogReg Average ROC:', 0.942)

INITIALIZATION = 'he_normal'

model = Sequential()
model.add(Dense(input_dim, 1024, init=INITIALIZATION,W_regularizer = l2()))
model.add(Activation('relu'))

model.add(Dense(1024, 512, init=INITIALIZATION,W_regularizer = l1l2(l1=0.005, l2=0.01)))
model.add(Activation('relu'))

model.add(Dense(512, 128, init=INITIALIZATION,W_regularizer = l2()))
model.add(Activation('tanh'))

model.add(Dense(128, output_dim, init=INITIALIZATION))
model.add(Activation('softmax'))

model.compile(loss='binary_crossentropy', optimizer=""adadelta"")

######
Same model params repeated but with FDR= 0.5 , 11 fold CV, and 8 epochs:
much worse perf - seems our model doesn't regularize enough ; and is badly affected by noise (i.e weak features!

('NN Average ROC:', 0.940)
('LogReg Average ROC:', 0.9442)

######
11 fold cv with slightly higher arbitrary regularization loss rates - Worse! .
SelectFdr(alpha = 0.2))

model.add(Dense(input_dim, 1024, init=INITIALIZATION,W_regularizer = l2(0.04)))
model.add(Activation('relu'))
model.add(Dense(1024, 512, init=INITIALIZATION,W_regularizer = l1l2(l1=0.04, l2=0.04)))
model.add(Activation('relu'))
model.add(Dense(512, 128, init=INITIALIZATION,W_regularizer = l2()))

('NN Average ROC:', 0.790)
('LogReg Average ROC:', 0.943)


---------------------------------------------------------------
nolearn DBN - Deep Belief network - terrible performance (~0.49 ROC).
(FDR 0.2..)
from nolearn.dbn import DBN
model = DBN(
[X_train.shape[1], 384, 2],
learn_rates=0.4,
learn_rate_decays=0.9,
epochs=45,
verbose=1,
epochs_pretrain = 10,
learn_rates_pretrain=0.006)
",2,0
96,2015-7-6,2015,7,6,3,3c7ulr,[1506.08700] Dropout as data augmentation,https://www.reddit.com/r/MachineLearning/comments/3c7ulr/150608700_dropout_as_data_augmentation/,[deleted],1436121647,,6,9
97,2015-7-6,2015,7,6,4,3c81hg,Discussion: There have been a lot of post about citations and patents of common work. Shouldnt this community have a thread were prior work can be gathered with information how to setup an objection to a proposed patent,https://www.reddit.com/r/MachineLearning/comments/3c81hg/discussion_there_have_been_a_lot_of_post_about/,maxToTheJ,1436125286,It seems to me that gathering information would be more constructive than just complaining and being reactive instead of proactive.,19,31
98,2015-7-6,2015,7,6,5,3c87b3,Training a RNN with audio data,https://www.reddit.com/r/MachineLearning/comments/3c87b3/training_a_rnn_with_audio_data/,jiminiminimini,1436128313,I am trying to train my RNN with some audio data (music). Should I use the signal directly or should I use FFT/STFT of the signal. To me STFT makes more sense but then I will have to deal with complex numbers. What is your opinion?,9,3
99,2015-7-6,2015,7,6,5,3c89mg,"Introducing Apollo, a new python deep learning library built on top of Caffe.",https://www.reddit.com/r/MachineLearning/comments/3c89mg/introducing_apollo_a_new_python_deep_learning/,singularai,1436129509,,8,14
100,2015-7-6,2015,7,6,7,3c8kcs,"Fuck software patents, fuck Google and fuck Hinton, Krizhevsky, Sutskever and Srivasta",https://www.reddit.com/r/MachineLearning/comments/3c8kcs/fuck_software_patents_fuck_google_and_fuck_hinton/,[deleted],1436135027,"This shit has gotten beyond ridiculous. ""Academics"" who pull this bullshit need to be shamed and ostracized. ",0,1
101,2015-7-6,2015,7,6,7,3c8m69,"Fuck software patents, fuck Google and fuck Hinton, Krizhevsky, Sutskever and Srivasta",https://www.reddit.com/r/MachineLearning/comments/3c8m69/fuck_software_patents_fuck_google_and_fuck_hinton/,j1395010,1436135992,,99,155
102,2015-7-6,2015,7,6,12,3c9iah,Good youtube courses on probability/statistics?,https://www.reddit.com/r/MachineLearning/comments/3c9iah/good_youtube_courses_on_probabilitystatistics/,MusicIsLife1995,1436154033,"I'm following Stanford's Graduate class CS 229 on Machine Learning. What's a decent online course taught by a good professor for learning topics like central limit theorem, Poisson processes, etc. with great intuition?",7,4
103,2015-7-6,2015,7,6,12,3c9j0j,Libraries for constrained stochastic subgradient?,https://www.reddit.com/r/MachineLearning/comments/3c9j0j/libraries_for_constrained_stochastic_subgradient/,doompie,1436154456,"I'd like to solve an optimization of the form

min  ||Y - AX||_F + c \sum min(a_{ij}, 1-a_{ij})

s.t. 0 &lt;= a_{ij} &lt;= 1

Are there packages that are relatively plug-and-play for this kind of optimization problem?",9,0
104,2015-7-6,2015,7,6,20,3caite,Digital Reasoning Trains World's Largest Artificial Neural Network,https://www.reddit.com/r/MachineLearning/comments/3caite/digital_reasoning_trains_worlds_largest/,iamtrask,1436182667,,5,0
105,2015-7-6,2015,7,6,21,3caojo,Another surprising patent application from Google: System and method for parallelizing convolutional neural networks,https://www.reddit.com/r/MachineLearning/comments/3caojo/another_surprising_patent_application_from_google/,NovaRom,1436186542,,21,51
106,2015-7-6,2015,7,6,22,3carfb,"I wanna create a neural network that multiplies by 2, I'm not sure if I was successful",https://www.reddit.com/r/MachineLearning/comments/3carfb/i_wanna_create_a_neural_network_that_multiplies/,faridthefirst,1436188351,"So I created these programs on my calculator. One is Host and the other is Network. The host creates a matrix of a given size, distributes neurons into 3 layers (1 input [layer 1], 1 output [layer 3] and the rest on a middle hidden layer [layer 2]). It gives each neuron as many random weights than there are neurons, a value between 0 and 4. I then input a number which is given to the input neuron's value, and it runs Network.

Network will take each neuron in order, check if his layer is not input, then for each neuron whose layer is right before him (so layer-1) it will multiply its value by the given weight for that neuron and add it to its own value. It goes on like this and then displays the output neuron's value. Back to the Host.

Host will divide the obtained number by the wanted number (input*2) and will see how much correction is needed. It will then apply the correction to each and every weight on the grid. So if the output was 2x the wanted value, it will divide each weight by 2.

Bottom line is that it works, but I'm really unsure whether it qualifies as a neural network as it's easy and fast and the correction part seems like cheating. Also, the less neurons I have the faster it reaches optimal values (around the goal) and I doubt that's how it's supposed to be.

Anyone care to explain if anything was done wrong and what it is? ",14,0
107,2015-7-6,2015,7,6,22,3carg7,[1506.02338] Modeling Order in Neural Word Embeddings at Scale,https://www.reddit.com/r/MachineLearning/comments/3carg7/150602338_modeling_order_in_neural_word/,downtownslim,1436188367,,0,14
108,2015-7-6,2015,7,6,23,3cax7t,"bat-country: A lightweight, extendible, pip-installable Python module for inceptionism/deep dreaming based on Google's implementation",https://www.reddit.com/r/MachineLearning/comments/3cax7t/batcountry_a_lightweight_extendible/,zionsrogue,1436191672,,10,39
109,2015-7-7,2015,7,7,3,3cbui2,Why Google's new patent applications are alarming,https://www.reddit.com/r/MachineLearning/comments/3cbui2/why_googles_new_patent_applications_are_alarming/,AnonMLResearcher,1436206928,"Google recently submitted at least four patent applications for machine learning methods, which have been discussed here in previous threads.

1. dropout: http://www.google.com/patents/WO2014105866A1?cl=en
2. classification (!): http://www.freepatentsonline.com/y2015/0178383.html
3. parallelizing neural networks: https://www.google.com/patents/WO2014105865A1
4. word embeddings: http://www.google.com/patents/US20120310627

Previous reddit comments have said ""don't hate the player, hate the game"", i.e., we should blame the US patent laws for allowing patents on well-known abstract ideas; that Google might as well exercise its legal rights.

I am afraid that Google has just started an arms race, which could do significantly damage to academic research in machine learning.
Now it's likely that other companies using machine learning will rush to patent every research idea that was developed in part by their employees.
We have all been in a prisoner's dilemma situation, and Google just defected.
Now researchers will guard their ideas much more combatively, given that it's now fair game to patent these ideas, and big money is at stake.

Even if Google never publicly sues based on these patents, and besides the damage to academic culture that could result from this new trend, there are at least two other reasons for concern:
1. In computer vision, SIFT and SURF features were patented. This caused major annoyances for open-source software libraries like OpenCV. The Debian repos did not contain the ""nonfree"" module, so users had to build from source, which was a major hassle. The same thing may happen in future open-source machine learning libraries.
2. These new patents cast doubt on any new company based on machine learning and make it harder to attract investments -- now there is the looming threat that Google has rights to their IP and can sue or at least use the patents as a bargaining chip in case of a legal dispute.


EDIT: two more additions to the first list: 5. Data augmentation methods for images: https://www.google.com/patents/US20140177947 6. Q Learning with deep networks http://www.google.com/patents/US20150100530

",80,257
110,2015-7-7,2015,7,7,3,3cby9u,Parts-of-Speech tagging using a neural network,https://www.reddit.com/r/MachineLearning/comments/3cby9u/partsofspeech_tagging_using_a_neural_network/,nat47,1436208476,"I've been trying to train a Neural Network to do Parts-of-Speech tagging and I haven't been having great results. [I'm still not entirely sure if I coded something slightly incorrectly](https://www.reddit.com/r/MachineLearning/comments/3alzfk/debugging_neural_network_for_natural_language/), or if I am just not going about it in the right way.

To represent the words to the input neurons I've been using probabilities. There are a total of 15 possible tags. The input neurons take 3 words, so, I have 45 input neurons, 15 for each word. Each word is looked up in a probability dictionary of that word being which part-of-speech. Most words only have 2-3 normal POS tags, so, a word often goes into the network as thirteen 0s and a .2 and .8 (in their designated tag locations).

However, I've heard that this isn't a great way to represent the words to be tagged? I've heard that it might be better to give each word an embedded word representation. Or to use [GloVe](http://nlp.stanford.edu/projects/glove/)? I'm not exactly sure how this would work... I understand that words can be represented in vectors with numbers, and many words have similarities to each other, such as Mother and Father, or Red and Blue, or Tennis and racket. However, I'm not sure how this would be implemented into a parts of speech tagging. I can see how more information like this could obviously be helpful, but I'm not sure how to actually implement it into a Neural Network. ",3,0
111,2015-7-7,2015,7,7,5,3ccdq0,Scale-invariant learning and convolutional networks,https://www.reddit.com/r/MachineLearning/comments/3ccdq0/scaleinvariant_learning_and_convolutional_networks/,budhdub,1436214754,,1,7
112,2015-7-7,2015,7,7,6,3ccm9x,So you want to be a data science consultant (or hire one)? [PDF slides from Berlin Buzzwords 2015 talk],https://www.reddit.com/r/MachineLearning/comments/3ccm9x/so_you_want_to_be_a_data_science_consultant_or/,piskvorky,1436218467,,0,0
113,2015-7-7,2015,7,7,6,3ccoca,Unpacking technical jargon in machine learning,https://www.reddit.com/r/MachineLearning/comments/3ccoca/unpacking_technical_jargon_in_machine_learning/,Paige_Roberts,1436219393,,0,0
114,2015-7-7,2015,7,7,7,3ccsst,Learning about deep learning through album cover classification,https://www.reddit.com/r/MachineLearning/comments/3ccsst/learning_about_deep_learning_through_album_cover/,yanirse,1436221366,,0,1
115,2015-7-7,2015,7,7,12,3cds4a,Question regarding RBMs.,https://www.reddit.com/r/MachineLearning/comments/3cds4a/question_regarding_rbms/,okayshokay,1436238575,"RBMs are generative models, but for some reason I am not understanding their functionality properly. Suppose we consider Ruslan's paper in which he proposes the likelihood estimation using AIS on RBMs. In his paper he uses the MNIST data for handwritten digits. While training the RBMs do we use the knowledge that a particular picture corresponds to a particular digit? If so then when I have a test digit the RBM will only tell me some  probability about a particular digit. what is the use of this probability, however, because this doesn't tell me anything about the test data. If that is not the case then what does the RBM exactly do after we have trained it on so many image samples?

Please help!!",4,1
116,2015-7-7,2015,7,7,12,3cdxf3,Beginner Help: Implementing Q(lambda) Learning in Python,https://www.reddit.com/r/MachineLearning/comments/3cdxf3/beginner_help_implementing_qlambda_learning_in/,mbriner,1436241480,"So I'm fairly new to the world of machine learning (and Python in general), and I've been trying to follow [this](http://www45.essec.edu/professorsCV/showDeclFileRes.do?declId=11396&amp;key=Publication-Content) paper to create an algorithm that can find an optimal pricing policy. 

The problem I'm having is that my results (code found [here](https://www.dropbox.com/sh/d1rjc8ozlh097r6/AAC-oOhGE3F4sLLotW6p4aCza?dl=0)) don't seem to converge to an optimal policy, or even come particularly close. I'm wondering if someone here could help me out by looking for any errors in my implementation. Please excuse the messy and poorly conceived Python code.

For reference the [steps](http://i.imgur.com/tQ6qXLV.png) outlined in the paper.",6,1
117,2015-7-7,2015,7,7,14,3ce8yj,Grid Long Short-Term Memory,https://www.reddit.com/r/MachineLearning/comments/3ce8yj/grid_long_shortterm_memory/,BeatLeJuce,1436248480,,4,20
118,2015-7-7,2015,7,7,15,3ce9l6,"China Heavy Truck Industry Report, 2015-2018",https://www.reddit.com/r/MachineLearning/comments/3ce9l6/china_heavy_truck_industry_report_20152018/,rakeshmrr,1436248913,,0,1
119,2015-7-7,2015,7,7,15,3cealw,"Global And China Advanced Packaging Industry Report, 2014-2015",https://www.reddit.com/r/MachineLearning/comments/3cealw/global_and_china_advanced_packaging_industry/,rakeshmrr,1436249582,,0,1
120,2015-7-7,2015,7,7,16,3cefne,Machine learning with sabyasachi upadhya,https://www.reddit.com/r/MachineLearning/comments/3cefne/machine_learning_with_sabyasachi_upadhya/,latagomes,1436253282,,0,1
121,2015-7-7,2015,7,7,19,3cesjr,Forget about the Vs of Big Data - It's only U that counts,https://www.reddit.com/r/MachineLearning/comments/3cesjr/forget_about_the_vs_of_big_data_its_only_u_that/,[deleted],1436264321,,0,1
122,2015-7-7,2015,7,7,19,3ceu8i,Is there a dataset dataset?,https://www.reddit.com/r/MachineLearning/comments/3ceu8i/is_there_a_dataset_dataset/,jdsutton,1436265743,"Is there an index somewhere that lists freely-available datasets, describes what kind of data they contain, the format of the data, etc?",15,3
123,2015-7-7,2015,7,7,21,3cf2sy,Compressing Neural Networks with the Hashing Trick ( x-post/CompressiveSensing ),https://www.reddit.com/r/MachineLearning/comments/3cf2sy/compressing_neural_networks_with_the_hashing/,compsens,1436271937,,9,19
124,2015-7-7,2015,7,7,22,3cf7sk,[1507.01053] Describing Multimedia Content using Attention-based Encoder--Decoder Networks,https://www.reddit.com/r/MachineLearning/comments/3cf7sk/150701053_describing_multimedia_content_using/,giessel,1436274932,,0,1
125,2015-7-7,2015,7,7,22,3cfblo,Google's Deep Learning Machine Learns to Synthesize Real World Images (with video),https://www.reddit.com/r/MachineLearning/comments/3cfblo/googles_deep_learning_machine_learns_to/,cybrbeast,1436277020,,3,0
126,2015-7-8,2015,7,8,1,3cfu4l,ML papers you'd recommend,https://www.reddit.com/r/MachineLearning/comments/3cfu4l/ml_papers_youd_recommend/,pcunneen19,1436285525,"I'm a undergrad (starting 2nd year in a couple months) studying both Computer Science and Statistics. I'm very much interesting in Machine Learning Research. I've been reading ML papers published by professors at my university, but I'd love to hear about papers that you'd recommend someone new to the field. Perhaps any papers that serve as good introductions to the field or are particularly influential. Or, just papers that you really enjoyed and I could put on the reading list. 

Thanks everyone  ",15,8
127,2015-7-8,2015,7,8,1,3cfvyf,DeepDream Animator ;-),https://www.reddit.com/r/MachineLearning/comments/3cfvyf/deepdream_animator/,samim23,1436286293,,10,74
128,2015-7-8,2015,7,8,1,3cfwl5,What is machine learning?,https://www.reddit.com/r/MachineLearning/comments/3cfwl5/what_is_machine_learning/,banguru,1436286556,,0,0
129,2015-7-8,2015,7,8,1,3cfzjn,"DeepDream: Today psychedelic images, tomorrow unemployed artists",https://www.reddit.com/r/MachineLearning/comments/3cfzjn/deepdream_today_psychedelic_images_tomorrow/,kaj_sotala,1436287817,,1,1
130,2015-7-8,2015,7,8,2,3cg82l,Nvidia Ramps Up GPU Deep Learning Performance,https://www.reddit.com/r/MachineLearning/comments/3cg82l/nvidia_ramps_up_gpu_deep_learning_performance/,[deleted],1436291340,,1,0
131,2015-7-8,2015,7,8,3,3cgb1b,A collection of fully tagged Machine Learning videos by ClipMine,https://www.reddit.com/r/MachineLearning/comments/3cgb1b/a_collection_of_fully_tagged_machine_learning/,ojaved,1436292530,"**Link to the collection**: [machine learning](https://clip.mn/c/ml)
**Details of the ClipMine Video Player**: The idea of ClipMine grew out of our own frustration with online video players. Unless you have the intention of watching every video in its entirety, it is very hard to find the parts that matter most to you. This problem only gets worse for longer videos, where most people either drop off at the beginning, or use the video player scrubber to randomly scan the video in the hopes of finding something interesting. With ClipMine, we are reimagining online video by making video players more aware of the content being played. We use both *crowd-sourcing* and *machine intelligence* to tag interesting points inside online videos. Then, using our Content-Aware Video Player, anyone can share or embed their tagged videos with their audience to help them discover and view the content that matters most to them.
**Note**:  We are featured on [producthunt](http://www.producthunt.com/tech/clipmine) today",4,6
132,2015-7-8,2015,7,8,3,3cgggk,"NVIDIA @ ICML 2015: CUDA 7.5, cuDNN 3, &amp; DIGITS 2 Announced",https://www.reddit.com/r/MachineLearning/comments/3cgggk/nvidia_icml_2015_cuda_75_cudnn_3_digits_2/,[deleted],1436294752,,7,8
133,2015-7-8,2015,7,8,4,3cgog6,"""Using a neural networking algorithm called Word2Vec, Google is teaching its machines to better understand the relationship between words posted across the Internet""",https://www.reddit.com/r/MachineLearning/comments/3cgog6/using_a_neural_networking_algorithm_called/,FuschiaKnight,1436298081,,5,0
134,2015-7-8,2015,7,8,4,3cgqiv,Google-y-eyes,https://www.reddit.com/r/MachineLearning/comments/3cgqiv/googleyeyes/,datathe1st,1436298946,,0,0
135,2015-7-8,2015,7,8,4,3cgqom,Could the google deep dream phenomenon be applied to audio?,https://www.reddit.com/r/MachineLearning/comments/3cgqom/could_the_google_deep_dream_phenomenon_be_applied/,DJKoolKeith,1436299006,"Of course some work would need to be done, but if some kind of audio recognition software is made from a large database of music it could be feed white noise.

I just wanted that idea to be out there.",30,16
136,2015-7-8,2015,7,8,5,3cguwf,Manopt 2.0: A Matlab toolbox for optimization on manifolds,https://www.reddit.com/r/MachineLearning/comments/3cguwf/manopt_20_a_matlab_toolbox_for_optimization_on/,compsens,1436300744,,0,1
137,2015-7-8,2015,7,8,5,3cgx15,"""Is it the end of the training set/testing set paradigm?"" Leon Bottou @ICML2015.",https://www.reddit.com/r/MachineLearning/comments/3cgx15/is_it_the_end_of_the_training_settesting_set/,evc123,1436301631,,24,13
138,2015-7-8,2015,7,8,6,3ch4vm,Microsoft Research Says All the Low-Hanging Fruit for Big Neural Network Progress Has Been Picked.,https://www.reddit.com/r/MachineLearning/comments/3ch4vm/microsoft_research_says_all_the_lowhanging_fruit/,[deleted],1436304960,,10,6
139,2015-7-8,2015,7,8,6,3ch6ch,Easy Multi-GPU Deep Learning with DIGITS 2,https://www.reddit.com/r/MachineLearning/comments/3ch6ch/easy_multigpu_deep_learning_with_digits_2/,harrism,1436305600,,1,18
140,2015-7-8,2015,7,8,7,3chda0,[Ask] - Trying to find article on Gist,https://www.reddit.com/r/MachineLearning/comments/3chda0/ask_trying_to_find_article_on_gist/,oneAngrySonOfaBitch,1436308609,"I'm trying to find a blog article on usint Gist image descriptors to reconstruct a face. The author created a library of patches from images and then chunked an input, matched chunks to patches in the library and then did some blending/inpainting to construct a new version of his face. 

does it ring any bells ?",3,1
141,2015-7-8,2015,7,8,10,3chylb,Is there an advantage to encode images in YUV instead of RGB?,https://www.reddit.com/r/MachineLearning/comments/3chylb/is_there_an_advantage_to_encode_images_in_yuv/,Noncomment,1436318932,"I saw this was being used in some demo code for torch and I don't see why it would be an advantage.

All of the YUV values are just linear functions of RGB values. It should be trivial to convert the first layer of weights between YUV and RGB and vice versa.

E.g. a neuron with a weight of 1 to the Y dimension, would be converted to a neuron with 0.3 R,  0.6 G, and 0.1 B. It's exactly equivalent.

In fact the NN should be able to learn the weights as if the image was in any color space it likes.

And those constants are entirely arbitrary anyway. They are based on the number of different cells in the eye for each color type in humans. There is no need for neural networks to be constrained by human perception. They already have equal resolution for all color channels.",11,8
142,2015-7-8,2015,7,8,14,3ciprk,"Playlist of Top Youtube Videos on Machine Learning, Neural Nets and Deep Learning",https://www.reddit.com/r/MachineLearning/comments/3ciprk/playlist_of_top_youtube_videos_on_machine/,john_philip,1436333948,,1,17
143,2015-7-8,2015,7,8,18,3cj5vt,ML SWE Internships at top 5,https://www.reddit.com/r/MachineLearning/comments/3cj5vt/ml_swe_internships_at_top_5/,PMMEYOURVCDIMENSION,1436346825,"Hello reddit

This fall I'm starting my MSc programme, and would like to get myself an internship for the next summer. I have a decent background in software engineering and math. I have passed several courses on machine learning, and comfortable with reading research papers. I did several internships during my undergrad study: one was a SWE-internship in Silicon Valley, another was a recommender systems research at a local company.

The job of my dream would be that at DeepMind (well, who wouldn't say that?), but I do realize that I have no research experience (i.e. published papers), so I aim a bit lower.
Another thing to note: I don't feel like these Data Science roles are right for me. I see them mostly as stats guys doing stuff in their excel / r / matlab with the outcome of better understanding your processes / audience, whereas SW guys deliver new features.
And the final note is that I plan to do a lot of research during my MSc so in 2 years I can get into a decent PhD programme. Thus, good recommendation letters are a plus.

Sorry for the lengthy introduction. So my questions are:

 * What are my chances to get this kind of internship being a Master's student? Haven't they all been taken by hungry Phds? :-)
 * What can I do to improve these chances? I have something like 3-4 months before the applications start.
 * How should I position myself (both in CV and during interviews): should I emphasis my software engineering experience, my research attitude (mostly lots of ML-related courses), or something else.
 * How does an interview for such positions look like? Is it usual SW-interview (algorithms, programming techniques, etc) without any emphasis on ML part, or quite the reverse?

TL;DR math-and-software-engineering heavy internship without business-shit is wanted. Preferably at a top company. And then there're 4 silly questions I want wisdom of reddit to answer.",5,0
144,2015-7-8,2015,7,8,19,3cj9ah,HZS Ready Mix Concrete Plant introduction,https://www.reddit.com/r/MachineLearning/comments/3cj9ah/hzs_ready_mix_concrete_plant_introduction/,unique-crusher,1436349763,,0,1
145,2015-7-8,2015,7,8,19,3cjc2j,buy cpap machines buy cpap machines online and it will give you instant benefit.,https://www.reddit.com/r/MachineLearning/comments/3cjc2j/buy_cpap_machines_buy_cpap_machines_online_and_it/,jorvick,1436352075,,0,1
146,2015-7-8,2015,7,8,20,3cji55,[1506.00990] Unsupervised Learning on Neural Network Outputs,https://www.reddit.com/r/MachineLearning/comments/3cji55/150600990_unsupervised_learning_on_neural_network/,yaolubrain,1436356552,,2,22
147,2015-7-8,2015,7,8,21,3cjloi,"""Simple Questions Thread"" - 20150708",https://www.reddit.com/r/MachineLearning/comments/3cjloi/simple_questions_thread_20150708/,seabass,1436358812,"**Previous Threads**

* /r/MachineLearning/comments/2u73xx/fridays_simple_questions_thread_20150130/
* /r/MachineLearning/comments/2xopnm/mondays_simple_questions_thread_20150302/

**Unanswered questions from previous threads:**

* /r/MachineLearning/comments/2xopnm/mondays_simple_questions_thread_20150302/cp32l69
* /r/MachineLearning/comments/2xopnm/mondays_simple_questions_thread_20150302/cq4qpgl
* /r/MachineLearning/comments/2xopnm/mondays_simple_questions_thread_20150302/cpcjqul
* /r/MachineLearning/comments/2xopnm/mondays_simple_questions_thread_20150302/cq1qkd3
* /r/MachineLearning/comments/2xopnm/mondays_simple_questions_thread_20150302/cssx08a

**Why?**

This is in response to the original posting of whether or not it made sense to have a question thread for the non-experts. I learned a good amount, so wanted to bring it back...",32,14
148,2015-7-8,2015,7,8,23,3cjwpu,Book: Deep Belief Nets in C++ and CUDA C: Volume II: Autoencoding in the Complex Domain,https://www.reddit.com/r/MachineLearning/comments/3cjwpu/book_deep_belief_nets_in_c_and_cuda_c_volume_ii/,booketor,1436364885,,0,0
149,2015-7-8,2015,7,8,23,3ck0r8,A brand new Sentiment Analysis API ready to play with - It works on Italian and English texts,https://www.reddit.com/r/MachineLearning/comments/3ck0r8/a_brand_new_sentiment_analysis_api_ready_to_play/,dagoneye,1436366802,,0,3
150,2015-7-9,2015,7,9,0,3ck732,What would be a good feature extraction algorithm for piano music?,https://www.reddit.com/r/MachineLearning/comments/3ck732/what_would_be_a_good_feature_extraction_algorithm/,[deleted],1436369657,"To clarify, the music will be nothing but a piano piece. It's also likely that only a brief segment of the piece will be used, its core if you will.


Would Non-Negative Matrix Factorization be a good choice?



The objective is to cluster piano music for a recommendation system using SOM. And a more ambitious objective would be to try and synthesize new pieces using GA's that use SOM for fitness evaluation.",1,0
151,2015-7-9,2015,7,9,0,3ck8g3,AIs Next Frontier: Machines That Understand Language,https://www.reddit.com/r/MachineLearning/comments/3ck8g3/ais_next_frontier_machines_that_understand/,winstonl,1436370254,,11,0
152,2015-7-9,2015,7,9,1,3ckbfk,Demo of Deep Visualization Toolbox,https://www.reddit.com/r/MachineLearning/comments/3ckbfk/demo_of_deep_visualization_toolbox/,vkhuc,1436371551,,15,145
153,2015-7-9,2015,7,9,1,3ckd7s,My paper: Using ILP to Identify Pathway Activation Patterns in Systems Biology If anyone has any comments/questions or general feedback on my short paper for ILP 2015 would be great to hear.,https://www.reddit.com/r/MachineLearning/comments/3ckd7s/my_paper_using_ilp_to_identify_pathway_activation/,[deleted],1436372333,,1,1
154,2015-7-9,2015,7,9,1,3ckdez,The difficulty with publishing in top CV conferences.,https://www.reddit.com/r/MachineLearning/comments/3ckdez/the_difficulty_with_publishing_in_top_cv/,procarastinizer,1436372416,"In conferences such as CVPR / ICCV / NIPS, and in particular CV tracks, it seems impossible to publish a paper which either doesn't have some incredible mathematical background or involve some deep learning techniques. I have been submitting an article repeatedly to CVPR/ICCV/ECCV since 2013 that is a reasonably simple method, is well-formulated showing promising results on benchmark datasets. While all the reviewers in all my submissions seem to agree that my proposal is novel and that it is indeed superior to many SOTA and often times I get even more positive reviews, I get rejected on the notion that my method is ""simple"". I write rebuttals arguing the elegance and the superiority of my methods and while everyone agrees with everything I say, there is always still one reviewer who gives a borderline or a weak reject based on the fact that its not ""complicated enough"". Now I am also being forced to switch topics ( I am entering my fourth year of PhD) because of this reason.

Here is my question: Many I have talked to seem to think that conferences like CVPR now focus more on engineering and on results rather than on novelty. Is this true ? If this is indeed true what are my alternatives to get my paper published. It is indeed a good method and shows good results and IMO fills a void in the literature, which quite frankly should have been filled ages ago. I really think that my paper should have been written in early part of last decade. Secondly, is it acceptable to switch topics so late in my PhD. I am thinking of getting into deep learning. Will I be perceived differently post-graduation, if at all I get to graduate in time, about my late change of topics ? 

[Update]: Thanks for all the suggestions. I am going to wait till ICCV 2015 rebuttal results come out (I am working on a good rebuttal and hopefully I can convert the last reviewer and convince the AC), but after that I will resubmit the same to either WACV or similar second-tier but I will also upload to arXiv, anonymously if I can make it. ",50,6
155,2015-7-9,2015,7,9,1,3ckis9,Native pre-trained models for Torch?,https://www.reddit.com/r/MachineLearning/comments/3ckis9/native_pretrained_models_for_torch/,XalosXandrez,1436374708,"Hello all,
Torch7 noob here. I was wondering why there aren't any native pre-trained Torch models available (except Overfeat) [1]. There seem to be no examples regarding how I can take an existing caffe Net (like AlexNet), and perform fine-tuning for example. Am I missing anything?

[1] https://github.com/torch/torch7/wiki/ModelZoo",4,0
156,2015-7-9,2015,7,9,3,3ckw24,Memristors Mimic Brains For Massive Scale Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3ckw24/memristors_mimic_brains_for_massive_scale_machine/,[deleted],1436380144,,2,12
157,2015-7-9,2015,7,9,3,3ckw2j,Automated Image Captioning with ConvNets and Recurrent Nets  Andrej Karpathy (Tagged Video),https://www.reddit.com/r/MachineLearning/comments/3ckw2j/automated_image_captioning_with_convnets_and/,ojaved,1436380147,,0,3
158,2015-7-9,2015,7,9,4,3cl4uv,Football/soccer win_lose_draw,https://www.reddit.com/r/MachineLearning/comments/3cl4uv/footballsoccer_win_lose_draw/,[deleted],1436383759,"Thinking about having a look at English premier league football results out of my own interest not to actually bet on. Has anyone tried any sports related classifications or ideas around this? There's quite a bit of data out there, how would you select the most important features? Any random input welcome! Note: seriously not a gambler so I'm not trying it on!",4,1
159,2015-7-9,2015,7,9,4,3cl6t5,Neural Network AI for Web Game,https://www.reddit.com/r/MachineLearning/comments/3cl6t5/neural_network_ai_for_web_game/,CocoaBotter,1436384554,"So, I've been playing this webgame called TagPro a lot (tagpro.gg, /r/tagpro).

There's a smallish, but decent community of people who write bots/scripts to play the game.  Some of the people there are talking about neural networks and machine-learning type solutions, but although I am familiar with some of the concepts of neural networks and training, I'm not sure what to do as far as inputs are concerned.  I'm not particularly concerned with the rest of the implementation, but trying to figure out what kind of input to give the network is proving to be a real puzzling situation

Although I would want to give as much information as possible, the ""as much as possible"" philosophy becomes absurd after a while.

So, I'm just wondering what kind of data is acceptable for input to a machine learning model, and if there are general guidelines for input.

If there's a more appropriate subreddit for this, let me know.",4,0
160,2015-7-9,2015,7,9,5,3clcnf,The Case for Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/3clcnf/the_case_for_artificial_intelligence/,infiveyr,1436386938,,1,0
161,2015-7-9,2015,7,9,7,3clwdv,A Deeper Look at Planning as Learning from Replay (Vanseijen and Sutton),https://www.reddit.com/r/MachineLearning/comments/3clwdv/a_deeper_look_at_planning_as_learning_from_replay/,[deleted],1436395613,,0,1
162,2015-7-9,2015,7,9,8,3cm3tz,What's up with specialized Neural Network hardware?,https://www.reddit.com/r/MachineLearning/comments/3cm3tz/whats_up_with_specialized_neural_network_hardware/,[deleted],1436399153,"I was just thinking that these companies are working on massive neural networks with huge GPU/Compute farms, but all I've heard about are GPU's. Is there something keeping hardware back for Neural Networks?

I remember reading about it on Gizmodo where someone said it would take &gt;5 megawatts to simulate the human brain, but they said they could do it for &lt; 1 megawatt,  ([found it](http://gizmodo.com/5400530/how-much-power-does-it-take-to-simulate-the-human-brain)).

Is there anyone from Stanford familiar with the project and its state that browses this subreddit?",0,0
163,2015-7-9,2015,7,9,8,3cm4o0,Data Science Stack Exchange,https://www.reddit.com/r/MachineLearning/comments/3cm4o0/data_science_stack_exchange/,True-Creek,1436399550,,1,9
164,2015-7-9,2015,7,9,13,3cn4wg,PRML Study group?,https://www.reddit.com/r/MachineLearning/comments/3cn4wg/prml_study_group/,MusicIsLife1995,1436417937,"I wonder if people here are interested in reading Bishop's PRML together, say, 10 pages a day or 70 pages a week or so, and discussing it, answering each other's questions, etc.? They say emotional involvement, like from debating with your peers, leads to better memorization of the material.

The book seems to be suitable for math/physics undergrads and above, I think. If you are math-challenged, PRML is probably not for you IMHO.

I personally prefer video lectures, but none that I know have the depth and breadth of PRML.

Please reply if you may be interested. I basically want to gauge the interest at this point. I also wonder if reddit is the best venue for this sort of thing.

**Subreddit: https://www.reddit.com/r/mlMoves/**",14,5
165,2015-7-9,2015,7,9,14,3cn9um,[1507.02188] AutoCompete: A Framework for Machine Learning Competition,https://www.reddit.com/r/MachineLearning/comments/3cn9um/150702188_autocompete_a_framework_for_machine/,iori42,1436421061,,0,11
166,2015-7-9,2015,7,9,16,3cnhj7,Understanding Neural Networks Through Deep Visualization,https://www.reddit.com/r/MachineLearning/comments/3cnhj7/understanding_neural_networks_through_deep/,john_philip,1436426768,,3,47
167,2015-7-9,2015,7,9,18,3cns3w,"Meet Buddy, an artificially intelligent friend inside your humble abode",https://www.reddit.com/r/MachineLearning/comments/3cns3w/meet_buddy_an_artificially_intelligent_friend/,faimhossiodnkhan,1436435931,,0,0
168,2015-7-9,2015,7,9,19,3cnvpd,The Model Complexity Myth,https://www.reddit.com/r/MachineLearning/comments/3cnvpd/the_model_complexity_myth/,clbam8,1436438966,,5,26
169,2015-7-9,2015,7,9,20,3co02o,One of my favourite methods from ICML this year: Unsupervised Learning by Inverting Diffusion Processes,https://www.reddit.com/r/MachineLearning/comments/3co02o/one_of_my_favourite_methods_from_icml_this_year/,fhuszar,1436442287,,12,60
170,2015-7-9,2015,7,9,22,3co7o8,Follow-Up: Q(lambda) Not Converging,https://www.reddit.com/r/MachineLearning/comments/3co7o8/followup_qlambda_not_converging/,mbriner,1436447077,"Ok, so I was just here a few days ago with [this](https://www.reddit.com/r/MachineLearning/comments/3cdxf3/beginner_help_implementing_qlambda_learning_in/) post, and I think I figured out what the issue is, but I'm not sure how to solve it. 

It seems like the problem is that, when it stops exploring, it just keeps increasing the Q-values of states that it's already deemed the best in the past. So for example, it visits the price 5 at a certain state and time and gets a reward. Then, in the next several episodes, it continues to visit 5, and continues getting a reward, adding that reward to the Q-value until it's pretty high. At that point, even if it were to explore at the same state and time, the reward it gets isn't enough to overcome the inflated Q-value, so it just goes right back to 5 in the next episode. There must be something wrong with my interpretation of how this is supposed to work, but I can't for the life of me figure it out. [Here](http://i.imgur.com/tQ6qXLV.png) are the steps I'm trying to follow.",2,0
171,2015-7-9,2015,7,9,22,3co8wq,Targeting Ultimate Accuracy: Face Recognition via Deep Embedding,https://www.reddit.com/r/MachineLearning/comments/3co8wq/targeting_ultimate_accuracy_face_recognition_via/,gxy5562,1436447821,,2,9
172,2015-7-9,2015,7,9,23,3coevb,My Paper: Using ILP to Identify Pathway Activation Patterns in Systems Biology Has been accepted at ILP 2015- Any-questions or feedback appreciated :),https://www.reddit.com/r/MachineLearning/comments/3coevb/my_paper_using_ilp_to_identify_pathway_activation/,walrusesarecool,1436450961,,8,3
173,2015-7-9,2015,7,9,23,3cogo8,Re-Training a Neural Network (forgetting problems),https://www.reddit.com/r/MachineLearning/comments/3cogo8/retraining_a_neural_network_forgetting_problems/,Aerospacio,1436451810,"Hello, i built a neural network with 2 inputs and 1 output (for reinforcement learning). The network has 2 hidden layers, 10 neurons on the first layer and 1 on the second. The network trains very well on the first set of samples, but when i feed new samples the network tends to forget the last samples. I'm using Rprop as optimization, does anyone had this problem before?

Thanks in advance!
Daniel",11,2
174,2015-7-9,2015,7,9,23,3cogox,SDS|2015: Michael L. Brodie on The Emerging Discipline of Data Science,https://www.reddit.com/r/MachineLearning/comments/3cogox/sds2015_michael_l_brodie_on_the_emerging/,cast42,1436451823,,0,0
175,2015-7-9,2015,7,9,23,3cokj4,NeuroEvolution with MarI/O - a step by step guide,https://www.reddit.com/r/MachineLearning/comments/3cokj4/neuroevolution_with_mario_a_step_by_step_guide/,glennrob,1436453612,,0,0
176,2015-7-10,2015,7,10,2,3cp5bm,Running multiple tasks on the same machine?,https://www.reddit.com/r/MachineLearning/comments/3cp5bm/running_multiple_tasks_on_the_same_machine/,bourbondog,1436462728,"Hello!

Is it possible to run multiple compute intensive tasks on the same machine? I don't mind some performance degradation (say, 75% slower than if it was running on its own). What kind of machine setup should I aim for? How many processors?",3,0
177,2015-7-10,2015,7,10,3,3cpcrh,Why Predictive Marketing is going Mainstream?,https://www.reddit.com/r/MachineLearning/comments/3cpcrh/why_predictive_marketing_is_going_mainstream/,Konversation,1436465938,,1,0
178,2015-7-10,2015,7,10,3,3cpe4q,Mining texts to efficiently generate global data on political regime types,https://www.reddit.com/r/MachineLearning/comments/3cpe4q/mining_texts_to_efficiently_generate_global_data/,ulfelder,1436466582,,0,9
179,2015-7-10,2015,7,10,4,3cplt2,Benchmark Problems for Evolutionary Algorithms,https://www.reddit.com/r/MachineLearning/comments/3cplt2/benchmark_problems_for_evolutionary_algorithms/,[deleted],1436470327,,0,1
180,2015-7-10,2015,7,10,7,3cq7rh,"""Hot news"" detection using Wikipedia",https://www.reddit.com/r/MachineLearning/comments/3cq7rh/hot_news_detection_using_wikipedia/,mhfirooz,1436479740,,0,7
181,2015-7-10,2015,7,10,16,3crxhs,Why not more progress on the text compression prize (aka Hutter prize: http://prize.hutter1.net/)?,https://www.reddit.com/r/MachineLearning/comments/3crxhs/why_not_more_progress_on_the_text_compression/,zakou,1436514336,"The last winning entry was more than 6 years ago. Looking at recent ML papers, there has been a lot of work on sequence modeling, time series prediction, generative models, etc. Of course there is a long way between reporting negative log likelihoods having an actual compressor/decompressor. But why no progress at all in 6 years?",27,14
182,2015-7-10,2015,7,10,17,3crzkm,Alternatives to Lingo for automatic document labelled clustering,https://www.reddit.com/r/MachineLearning/comments/3crzkm/alternatives_to_lingo_for_automatic_document/,spurious_recollectio,1436516089,"I recently discovered this simple but relatively effective algorithm for automatic cluster detection and labelling:

http://project.carrot2.org/publications.html

You can try the website for a demo on clustering search results.

The main paper (on Lingo) is 10 years old and I was wondering if its still relatively state of the art or if someone can recommend a better method?  I'm interested in clustering/labelling sentence length snippets of text which I think is more challenging than longer documents.",0,0
183,2015-7-10,2015,7,10,17,3cs0qa,What's a good laptop for datascience ?,https://www.reddit.com/r/MachineLearning/comments/3cs0qa/whats_a_good_laptop_for_datascience/,datatadadata,1436517115,"Hi,

I can't find a decent, recent blog post or article about **what's a good laptop for datascience**.

Would you mind answering the following questions ? I hope the thread will help others ! You'd better assume that I'm new to ML but will need some advanced techniques (feel free to explain why deep learning changes everything...), so that the answers help everybody ;)

Questions :  
- **Do I need a lot of RAM ?** Why ? Is 64GB a lot ?  
- **What makes a good CPU ?** Do I need many cores / parallel computing ? Is any I5 ok ?  
- **Do I need a GPU ?** When is it required ? Are there libraries that make it easy to use a certain type of GPU ? What is a good standard GPU for Machine Learning ?  
- **What are other requirement other than CPU / GPU / RAM ?**  
- **Is there any reason to use an UNIX OS if I'm used to coding on Windows?**  
- **Finally, what's a good example of a machine one should buy today with a 1k budget? 1.5k? 2k?**   

Note that I'm assuming you're not willing to use AWS for some reasons, because there are other threads about AWS and PC/AWS comparisons ;)

Thanks for your help !

Cheers !

Here are a few links that kind of help :  
- [Build a monster machine](https://www.reddit.com/r/MachineLearning/comments/2xhmav/build_a_monster_machine_or_master_aws/?)  
- [Making a ML/General purpose computer](http://www.reddit.com/r/MachineLearning/comments/3axozh/making_a_mlgeneral_purpose_computer_considering/?)  
- [A full hardware guide to deep learning](https://timdettmers.wordpress.com/2015/03/09/deep-learning-hardware-guide/)",23,0
184,2015-7-10,2015,7,10,17,3cs1a3,Good technical intro to Reinforcement Learning?,https://www.reddit.com/r/MachineLearning/comments/3cs1a3/good_technical_intro_to_reinforcement_learning/,bge0,1436517640,Anyone have any good tutorials / papers that I can read to get some end to end knowledge on RL? Ie maybe a summary paper that covers a few different subfields would be great!,13,20
185,2015-7-10,2015,7,10,17,3cs2j4,Teaching a machine to see: unsupervised image segmentation and categorisation using growing neural gas and hierarchical clustering,https://www.reddit.com/r/MachineLearning/comments/3cs2j4/teaching_a_machine_to_see_unsupervised_image/,john_philip,1436518764,,1,8
186,2015-7-10,2015,7,10,18,3cs661,Natural image prior,https://www.reddit.com/r/MachineLearning/comments/3cs661/natural_image_prior/,Improbus_42,1436521949,"I am interested in some further explanation of this phrase and its meaning.

In this recent, very interesting [post](http://yosinski.com/deepvis) (Jason Yosinski et al.), ""natural image priors"" are described as follows:

""To produce more recognizable images, researchers have tried optimizing images to  (1) maximally activate a neuron, and (2) have styles similar to natural images (e.g. not having pixels be at extreme values). These pressures for images to look like normal images are called natural image priors or regularization."" 

Lets say, we got a synthetic image, produced with a BAD ""natural image prior"".
Questions:
(1) This image activates a neuron maximally, but does not look like a natural image at all. Correct?
(2) This image fools the DNN, as it is very confidently classified wrong and corresponds to an ""adversarial image"" (Goodfellow et al.). Correct?
(3) Overall, on a higher level, does that mean that we - as humans - are trying to provide the DNN with our prior knowledge of the world? Is this the goal of whichever ""regularization"" ?
",1,5
187,2015-7-10,2015,7,10,20,3csayb,How these new patents by google will affect the independent researchers?,https://www.reddit.com/r/MachineLearning/comments/3csayb/how_these_new_patents_by_google_will_affect_the/,tushar1408,1436526015,,24,45
188,2015-7-10,2015,7,10,20,3csbwi,What the different GoogLeNet (#deepdream) layers look like...on a giraffe,https://www.reddit.com/r/MachineLearning/comments/3csbwi/what_the_different_googlenet_deepdream_layers/,willycs40,1436526741,,6,9
189,2015-7-10,2015,7,10,22,3csmv7,Tips and Tricks for Feature Engineering?,https://www.reddit.com/r/MachineLearning/comments/3csmv7/tips_and_tricks_for_feature_engineering/,mtnchkn,1436534220,"I read [this interview of Owen Zhang](http://blog.kaggle.com/2015/06/22/profiling-top-kagglers-owen-zhang-currently-1-in-the-world/) describing how he approaches Kaggle competitions, and he mentioned he focuses a lot on feature engineering. Can anyone point me to some formal resources discussing feature engineering or maybe practical examples? I have a lot of domain specific expertise which I use to winnow down features *a priori*, but I am wondering if there are ways I could be better at this before training classifiers.",8,14
190,2015-7-10,2015,7,10,23,3csu9d,LSTM Recurrent Neural Network back-propagation/backward pass Math tutorial?,https://www.reddit.com/r/MachineLearning/comments/3csu9d/lstm_recurrent_neural_network/,sunrisetofu,1436538132,"Hello,

does anyone have any info/links/explanations on the math behind the backward pass/backprop for a LSTM cell? I have read some papers on the subject but usually the equations are given directly and I'm having trouble understanding the math derivations.

I understand there're some variations (full Back prop through time (BPTT), hybrid BPTT with Real time recurrent learning), any explaination on these methods with respect to a LSTM cell is greatly appreciated.",3,7
191,2015-7-11,2015,7,11,0,3csz33,ICML2015 Drama,https://www.reddit.com/r/MachineLearning/comments/3csz33/icml2015_drama/,bge0,1436540488,,21,25
192,2015-7-11,2015,7,11,0,3cszk7,Michael Nielsen: Is there a simple algorithm for intelligence?,https://www.reddit.com/r/MachineLearning/comments/3cszk7/michael_nielsen_is_there_a_simple_algorithm_for/,tabacof,1436540709,,4,3
193,2015-7-11,2015,7,11,1,3ct887,Fully automated classification (algorithm selection + hyperparameter optimization) with Optunity and scikit-learn.,https://www.reddit.com/r/MachineLearning/comments/3ct887/fully_automated_classification_algorithm/,claesenm,1436544702,,3,1
194,2015-7-11,2015,7,11,3,3ctmsz,Semi-Supervised Learning with Ladder Network,https://www.reddit.com/r/MachineLearning/comments/3ctmsz/semisupervised_learning_with_ladder_network/,NYDreamer,1436551204,,0,4
195,2015-7-11,2015,7,11,3,3ctnoj,PageRank meets vectorial representations  Ranking on Data Manifolds,https://www.reddit.com/r/MachineLearning/comments/3ctnoj/pagerank_meets_vectorial_representations_ranking/,benjaminwilson,1436551559,,1,9
196,2015-7-11,2015,7,11,3,3ctqzr,Intro to Gradient Descent,https://www.reddit.com/r/MachineLearning/comments/3ctqzr/intro_to_gradient_descent/,changingourworld,1436552993,,1,6
197,2015-7-11,2015,7,11,4,3ctvdr,Hlice de Batedeira gigante para po e panetone,https://www.reddit.com/r/MachineLearning/comments/3ctvdr/hlice_de_batedeira_gigante_para_po_e_panetone/,Delsoto,1436554919,,0,1
198,2015-7-11,2015,7,11,4,3ctxj5,How a key actually unlocks a door.,https://www.reddit.com/r/MachineLearning/comments/3ctxj5/how_a_key_actually_unlocks_a_door/,AliGFX,1436555849,,1,0
199,2015-7-11,2015,7,11,5,3cu3pp,Reinforcement learning or fully convolutional methods useful for my dataset? Any ideas?,https://www.reddit.com/r/MachineLearning/comments/3cu3pp/reinforcement_learning_or_fully_convolutional/,zZJollyGreenZz,1436558575,"My dataset is similar to the idea for grayscale rice or mitosis detection , but I seem to run into issues using non-learning methods, such as adaptive thresholding and/or the OTSU method.  Essentially I need to run an algorithm at a coarse resolution to generate possible detections/boxes.  Each box will be followed up by a multi-class convnet that examines the area at a high resolution.  I currently have the multi-class convnet working, and it works very well!, but a sliding window approach is just too slow.  The events at the coarse
resolution are relatively sparse, so it would be nice to use a smaller convnet or other learning algorithm to search for areas of interest that could be classified with the multi-class convnet using the high resolution data.


I am wondering if there is a good way to do:

binary image segmentation

and/or

bounding box detection / regression


for detection of rice kernels or mitosis cells in a grayscale image with varying backgrounds and interference.


It seems like there might be a way to perform the task with reinforcement learning or by using a method similar to ""Fully Convolutional Semantic Segmentation Models"".

http://cs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf


I have been using caffe for other multi-class convnet classification tasks and it has worked really well for me.  So I was wondering if there were other ideas the community might have for me to try.


Any supervised / semi-supervised ideas floating around out there that you guys think might work well?  Or even tips/tricks/starter ideas for the fully convolutional methods or reinforcement learning methods would be great!
",1,0
200,2015-7-11,2015,7,11,9,3cuyqi,"Paper by ""Deep Learning Conspiracy"" in Nature",https://www.reddit.com/r/MachineLearning/comments/3cuyqi/paper_by_deep_learning_conspiracy_in_nature/,[deleted],1436573127,,1,0
201,2015-7-11,2015,7,11,11,3cvde1,Eliminate questions in Questionnaire based on previous responses,https://www.reddit.com/r/MachineLearning/comments/3cvde1/eliminate_questions_in_questionnaire_based_on/,Mr_Muffinman,1436581078,"I've recently encountered a unique problem that I wanted to try to solve as a side project. I'm looking for guidance on where to start :)

Problem:
A user today is given 60 questions to help us learn about the user's situation. We are looking to reduce the number of questions asked to the user but still maintaining the information learned. We recognize there is an opportunity here to build a ""smart"" questionnaire based on responses to questions. Ex: If the user answers 'yes' to question A and question B, then we know that the answer to Question C, will be 'no'. However, they are as simple as stated above and which is why we are hoping to look to ML to solve the problem.

However I want to understand, is ML the right solution to our problem? If so, what are algorithms that could help us towards this?

Thanks for all of your help!",4,0
202,2015-7-11,2015,7,11,16,3cw5xq,Dataset: Every reddit comment. A terabyte of text.,https://www.reddit.com/r/MachineLearning/comments/3cw5xq/dataset_every_reddit_comment_a_terabyte_of_text/,modeless,1436600130,,25,222
203,2015-7-11,2015,7,11,17,3cw85d,"if only I knew you,. Bornrobot",https://www.reddit.com/r/MachineLearning/comments/3cw85d/if_only_i_knew_you_bornrobot/,Bornrobots,1436602263,,1,0
204,2015-7-11,2015,7,11,17,3cwaiu,What Computers Dream of When They Look at Porn (NSFW),https://www.reddit.com/r/MachineLearning/comments/3cwaiu/what_computers_dream_of_when_they_look_at_porn/,blackibiza,1436604621,,1,0
205,2015-7-11,2015,7,11,23,3cwzwc,Deep Learning workshop panel discussion live on periscope,https://www.reddit.com/r/MachineLearning/comments/3cwzwc/deep_learning_workshop_panel_discussion_live_on/,flukeskywalker,1436626151,,7,17
206,2015-7-12,2015,7,12,0,3cx6lw,convnetjs - Deep Learning that runs in JavaScript with Online Demos,https://www.reddit.com/r/MachineLearning/comments/3cx6lw/convnetjs_deep_learning_that_runs_in_javascript/,WorldLoiterer,1436629978,,3,0
207,2015-7-12,2015,7,12,1,3cxaji,"Version 0.8.0 of QuickML released (Java random forests lib). 15% training time speedup, major refactor to improve code clarity and flexibility.",https://www.reddit.com/r/MachineLearning/comments/3cxaji/version_080_of_quickml_released_java_random/,sanity,1436632149,,12,1
208,2015-7-12,2015,7,12,1,3cxdze,Intuition behind variational learning of hard attention models,https://www.reddit.com/r/MachineLearning/comments/3cxdze/intuition_behind_variational_learning_of_hard/,__AndrewB__,1436633993,,4,4
209,2015-7-12,2015,7,12,2,3cxfd8,Interactive Deep Neural Net Hallucinations (+source code),https://www.reddit.com/r/MachineLearning/comments/3cxfd8/interactive_deep_neural_net_hallucinations_source/,john_philip,1436634671,,1,33
210,2015-7-12,2015,7,12,4,3cxuqg,How a driverless car sees the road,https://www.reddit.com/r/MachineLearning/comments/3cxuqg/how_a_driverless_car_sees_the_road/,Lost4468,1436642653,,14,71
211,2015-7-12,2015,7,12,6,3cy7i7,Predicting Future Robot Movements,https://www.reddit.com/r/MachineLearning/comments/3cy7i7/predicting_future_robot_movements/,scofface,1436649405,"I am currently working on an assignment to attempt to track future positions of a vehicle based on previous movements. I am only tracking the center mass of the object in a 2d coordinate system. Currently I am building a Kalman Filter with measurements, acceleration, and jerk built in and getting not so great results. 

Any suggestions for other algorithms I should look into for this type of problem. Is a Kalman Filter appropriate for this type of problem?

Current code: http://pastebin.com/TSCCQFc0",7,4
212,2015-7-12,2015,7,12,18,3d01e9,"LSTM+word2vec for word sequence modeling, how to form a distribution over words?",https://www.reddit.com/r/MachineLearning/comments/3d01e9/lstmword2vec_for_word_sequence_modeling_how_to/,Jontte,1436694178,"I'm using word2vec to encode a text corpus word by word and feeding those vectors as sequences to a multi-layer LSTM network. The task is to predict the word that follows a given window of words. In [char-rnn](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) the last layer of the network was a softmax layer that effectively formed a distribution over all characters that could possibly follow the given window of characters and one way to get a prediction was to choose the character with the highest probability.

Forming such a distribution over words with word2vec is not as straightforward. I was thinking of introducing some stochasticity to the network (test-time dropout? rbm-like layer in the middle?) so that I could get multiple different yet plausible predictions for the next word in a sentence, but I'm unsure how to proceed. Ideas?",15,5
213,2015-7-12,2015,7,12,19,3d02d8,An artificially intelligent friend inside your humble abode,https://www.reddit.com/r/MachineLearning/comments/3d02d8/an_artificially_intelligent_friend_inside_your/,legaleadaiimo,1436695247,,1,0
214,2015-7-12,2015,7,12,21,3d0cb9,Regularization properties of convolutional layers,https://www.reddit.com/r/MachineLearning/comments/3d0cb9/regularization_properties_of_convolutional_layers/,sdemyanov,1436704757,"Convolutional layers can be considered as fully connected layers with two types of restrictions: 1) zero weights for neurons far from each other, and 2) weight sharing between corresponding connections

From the complexity point of view, it decreases the number of free parameters, and therefore decreases the model variance and VC-dimensionality. Is there any articles with the numerical estimations of these effects?",1,10
215,2015-7-12,2015,7,12,22,3d0fak,"Could a ""Bullshit detector"" or fact checker be build by using Watson?",https://www.reddit.com/r/MachineLearning/comments/3d0fak/could_a_bullshit_detector_or_fact_checker_be/,2Punx2Furious,1436707299,"It's already good at this kind of things, isn't it?",11,0
216,2015-7-13,2015,7,13,0,3d0pc3,The account @DeepDreamThis creates deep dreams on Twitter,https://www.reddit.com/r/MachineLearning/comments/3d0pc3/the_account_deepdreamthis_creates_deep_dreams_on/,AlanZucconi,1436714117,"The Internet has been flooded with *deep dreams* lately. Despite Google Research has released the source code, it is quite challenging to make it run and it requires a lot of resources. For the ones of you who don't have too much experience with machine learning but still want to play with deep dreams, you can try the account [@DeepDreamThis](https://twitter.com/DeepDreamThis) on Twitter. It will *deep dream* any image you'll tweet to it. There are few other online services which do that, but this is the first one which allows for some customisation.

In the text of your tweet you can change the style with one of the following parameters:

* -strokes:  maximises the layer *inception_3b/5x5_reduce*
* -flowers:  maximises the layer *inpcetion_4c/1x1*
* -vortices: maximises the layer *inception_4a/1x1*
* -wrinkles: maximises the layer *conv2/3x3*
* -waves: maximises the layer *conv2/3x3_reduce*
* -fluffy: maximises the layer *inception_3b/output*
* -glass: maximises the layer *inception_3a/3x3*

If you don't put anything, it will maximises the layer *inception_4c/output*, which is usually the one commonly used.

You can also tweak how many iteration you'll have (standard: 10), by adding:

* -heavy: 14 iterations
* -light: 6 iterations

and also changing the number of octaves used to find patterns in your deep dream (standard: 4):

* -deep: 6 octaves
* -shallow: 2 octaves

Loosely speaking, iterations determines how heavily your image is modified, and octaves determines the size of the patterns it will find.

You can also use -random to get some random effect, and -gif to get an animation.

Having a background in machine learning myself, I'd like to make [@DeepDreamThis](https://twitter.com/DeepDreamThis) to be a nice way to experiment with deep dreams, not just a toy. So please, if you have any comment on this tool just let me know!",0,0
217,2015-7-13,2015,7,13,4,3d1fm7,Binary classification of short ngrams where I'm only interested in occasional high-confidence answers and can throw away the rest?,https://www.reddit.com/r/MachineLearning/comments/3d1fm7/binary_classification_of_short_ngrams_where_im/,compsc,1436727729,"Let's say I have a page of raw texts consisting of a few paragraphs.  Some paragraphs are written by Jim and some are written by Bob.  I'd like to get a few ngrams that were probably written by Jim (i.e., 90% +), but it's okay if I only get a few, and say ""uncertain"" for the rest.

I'd like scores to be computed in isolation on each ngram.  I can't rely on the greater context.

A very simple approach would be to have a list of many bigrams that Jim uses frequently and Bob uses infrequently in some training data.  If Jim used ""hello there"" 100 times in the training data, and Bob only 5 times, then in test data I could say with high confidence that ""hello there"" was said by Jim.

I've read a bit about smoothing and interpolation, but my uneducated guess is that those are more aimed at scenarios where you need to provide an answer all the time.  Maybe that's not even a real distinction.

By short ngrams I mean 2-4.  I don't think single words are reliable enough

Any tips?

Thanks.

EDIT: My bad, this isn't really for classifying between two people, Jim and Bob.  I'm actually hoping to do this to get bits of main article content from a raw html page.  There's text that's noise, and then there's main-article text.  Again, I can't rely on anything like html path or density of certain ngrams.  It has to be done on an ngram in isolation.",4,1
218,2015-7-13,2015,7,13,4,3d1gu1,"Examples of different types of perceptrons? (Minksy's Perceptrons, Gamba, diameter limited.)",https://www.reddit.com/r/MachineLearning/comments/3d1gu1/examples_of_different_types_of_perceptrons/,yyttr3,1436728329,"I implemented a perceptron before and being interested in the subject I bought the book ""perceptrons"" by Minksy and Papert. In the book they explain different sorts of algorithms such as the diameter limited perceptron. I've tried to implement some of these slightly different algorithms but I would like to see someone else implement them as well.

I've searched online but there does not seem to be many sources. If anyone could point me toward something useful to learn from I would be grateful.",7,4
219,2015-7-13,2015,7,13,4,3d1it1,Deepdream Generated Music Video,https://www.reddit.com/r/MachineLearning/comments/3d1it1/deepdream_generated_music_video/,samim23,1436729307,,4,0
220,2015-7-13,2015,7,13,5,3d1piw,Summary of the ICML 2015 Panel Discussion,https://www.reddit.com/r/MachineLearning/comments/3d1piw/summary_of_the_icml_2015_panel_discussion/,vikkamath,1436732551,,7,42
221,2015-7-13,2015,7,13,5,3d1tin,Using RNNs to develop a program that can play liar's dice.,https://www.reddit.com/r/MachineLearning/comments/3d1tin/using_rnns_to_develop_a_program_that_can_play/,Russian-Assassin,1436734538,"I am trying to create a recurrent neural network that is capable of playing the dice strategy game [liar's dice](https://en.wikipedia.org/wiki/Liar%27s_dice). Although I believe this game is actually solved, I think it would be a good learning exercise to write an ML program for this. Basically, liars dice is a turn based game where all players roll their dice while keeping them hidden (5 each at the start) and then go around making bets about the number of dice on the table (while only being able to see your own dice). Eg. ""three fours or four sixes"" Each player must either raise the bet or call the previous player a liar. When someone calls the previous player a liar, the hands are lifted and the player that was wrong (either the player who made the last bet or the player who called them a liar) must place one of their dice in the middle and out of play. The last player to still have some of their dice wins.

I am very much a beginner in the machine learning field so I need some help coming up with a learning structure. My initial thought was to use genetic algorithms along with recurrent neural networks. Basically the RNN would be fed each previous bet, the values of the dice belonging to it and the total number of dice on the table and would output the bet it should make or some signal for calling the person a liar. Initially, a large population of RNNs would be randomly created (say 100) then a series of games would be started containing (2-5 randomly selected RNNs). The RNNs would be given points for adhering to the rules (making bets higher than the previous ones) and for winning games. Then the next population would be bred using tournament ranking where the top ranked RNNs would have a better chance of being picked to breed.

I have a couple questions about this:

* First of all, is it possible to feed an RNN different types of information at once (eg. a possibly infinite vector of previous bets along with a fixed vector of dice and a fixed number of total dice) and how would I go about doing this?
* Secondly, would this method work in practice? I feel confident that this program could create RNNs that adhere to the rules of the game but I'm not sure how much strategy these RNNs can develop when they are not playing ""good"" competitors.",2,0
222,2015-7-13,2015,7,13,6,3d1w83,Real-time crowd sourcing for fun and profit,https://www.reddit.com/r/MachineLearning/comments/3d1w83/realtime_crowd_sourcing_for_fun_and_profit/,[deleted],1436735894,,0,0
223,2015-7-13,2015,7,13,7,3d23sn,Using the output of several models as the input of another?,https://www.reddit.com/r/MachineLearning/comments/3d23sn/using_the_output_of_several_models_as_the_input/,Nixonite,1436739542,"Hello everyone,

I heard that this is similar to neural nets, but I haven't studied those very much (yet). 

I heard about Kaggle users streaming the outputs of some models as inputs of others. i.e. making features from the predictions of some models, so that the new model would make predictions with those extra features for increased performance.

I tried it myself, and it did wonders for boosting the classification accuracy. My models improved from ~73% to about ~96% accuracy by tossing in several other model predictions into the feature space. I even ran cross-validation on it to check, and it's definitely 96% across several scores (recall/precision/f1).

I was wondering, are there any drawbacks of using this method other than it becoming a black box? Is 'ensemble methods' the name of this type of model creation? ",21,4
224,2015-7-13,2015,7,13,9,3d2i7g,Overview of patents filed by Google,https://www.reddit.com/r/MachineLearning/comments/3d2i7g/overview_of_patents_filed_by_google/,____BOLD____,1436746837,,11,52
225,2015-7-13,2015,7,13,15,3d3lsi,How do core sets work for k-clustering,https://www.reddit.com/r/MachineLearning/comments/3d3lsi/how_do_core_sets_work_for_kclustering/,Punkter,1436769293,"I have also posted this in ELI5 as I am looking for a simple explanation on how they work. I have tried to read some of the papers on the subject, but they are not that easy to follow. Here's the best one I have seen in this regard - http://www.cs.tau.ac.il/~ronitt/COURSES/F08/lec6.pdf, but still I wasn't able to translate the material to how I could implement it. I should also prefix that I am relatively new to Machine learning",2,1
226,2015-7-13,2015,7,13,17,3d3v9e,Manual capsule filler machines operation video,https://www.reddit.com/r/MachineLearning/comments/3d3v9e/manual_capsule_filler_machines_operation_video/,capsulcn,1436777151,,2,0
227,2015-7-13,2015,7,13,18,3d3xsx,20+ Open Source Furniture Designs,https://www.reddit.com/r/MachineLearning/comments/3d3xsx/20_open_source_furniture_designs/,manningupbig,1436779535,,0,0
228,2015-7-13,2015,7,13,18,3d3yzs,The arXiv as Dataset,https://www.reddit.com/r/MachineLearning/comments/3d3yzs/the_arxiv_as_dataset/,benjaminwilson,1436780577,,1,17
229,2015-7-13,2015,7,13,20,3d46l2,How to find top influential features for each test sample?,https://www.reddit.com/r/MachineLearning/comments/3d46l2/how_to_find_top_influential_features_for_each/,samratkokula,1436786854,"From the training set, I found the top features (say 20) and using the top features I built the model. Using the testing set, I calculated the accuracy of the model. It is highly unlikely that all 20 top-features were important in classifying each test set. For example, Features 2,4,6 might have been influencial features for test sample 1. For test sample 2, features 18,19,20 (which are completely diff from earlier) might have been influencial features for test sample 2. Is there any method to find out which features were influential for a particular test sample?",10,0
230,2015-7-13,2015,7,13,23,3d4mo2,Great ICML paper: Masked Autoencoders for Distribution Estimation (unsupervised learning involving tractable deep generative models),https://www.reddit.com/r/MachineLearning/comments/3d4mo2/great_icml_paper_masked_autoencoders_for/,fhuszar,1436796774,,8,34
231,2015-7-14,2015,7,14,0,3d4tbn,About numerical stability...,https://www.reddit.com/r/MachineLearning/comments/3d4tbn/about_numerical_stability/,tareumlaneuchie,1436799996,"I am planning on using one of the Python neural network tool kit (PyBrain) to train a classifier. Speed is not so much of an issue for training, because this is done offline.

To optimize (among other things) classifier runtime, I would like to use C++, and I was wondering of the practicality of training a classifier in one language and running it in another.

I know that there are better framework out there (Theano) that approaches C++ run time, but Theano is just too steep of a learning curve at this point, and I  feel that my classifier problem (in essence an OCR system), does not require the kind of experimental wizardry that Theano offers... 

Any input welcome.

TIA!

",8,0
232,2015-7-14,2015,7,14,0,3d4vr6,KDDCUP 2015 discussion thread,https://www.reddit.com/r/MachineLearning/comments/3d4vr6/kddcup_2015_discussion_thread/,luvmunky,1436801111,"Did you compete in the [KDDCUP](https://www.kddcup2015.com/information.html) ? How did you do? What techniques did you use? etc. etc.

Added: here are the top 10 entries:  

| Rank | Team | Score (private) |
|:------|:-------|:------------------|
| 1 | Intercontinental Ensemble | 0.9074429656630387 |
| 2 | FEG&amp;NSSOL@DataVeraci | 0.9071299191403106 |
| 3 | Data Sapiens | 0.9068038441658484 |
| 4 | CLMS | 0.906652903315283 |
| 5 | ttllbb | 0.9060343458177709 |
| 6 | KDDILABS&amp;Keiku | 0.9059718605039866 |
| 7 | FirstTimeEver | 0.9058898193220947 |
| 8 | xiaochuan | 0.9055778412303954 |
| 9 | Donquote | 0.9052017384333861 |
| 10 | NCCU | 0.9050289277229964 |
",14,7
233,2015-7-14,2015,7,14,1,3d53uo,"Mixing the artistic styles of Warhol, Pollock, Escher, Picasso, van Gogh, etc. using guided deep dreaming.",https://www.reddit.com/r/MachineLearning/comments/3d53uo/mixing_the_artistic_styles_of_warhol_pollock/,zionsrogue,1436804700,,4,6
234,2015-7-14,2015,7,14,2,3d5cvt,Method for labeling vertices in a fully connected undirected graph knowing the edge weights between vertices?,https://www.reddit.com/r/MachineLearning/comments/3d5cvt/method_for_labeling_vertices_in_a_fully_connected/,thewetness,1436808594,"Title says it all. I'm looking for an algorithm which will (probabilistically) label the vertices in a fully connected, undirected graph of fixed size when I know the edge weights between each vertex.

Prior to this, I have a template which the graph should match, so I know the vertex labels and their corresponding  so to a degree it is a supervised learning problem (I think). However, during field tests, I do not know which vertex is which.

The edge weights correspond to distances between sensors. While I know these distances, I do not know which sensor the measurement came from, which I would like to know. Are there any algorithms or papers that discuss a method for this?",2,0
235,2015-7-14,2015,7,14,4,3d5rti,Google's ANN as perceived in (tech-friendly) pop-culture,https://www.reddit.com/r/MachineLearning/comments/3d5rti/googles_ann_as_perceived_in_techfriendly/,szech758,1436814818,"[Link to Penny-Arcade comic](http://www.penny-arcade.com/news/post/2015/07/13/the-contiguous-ultrahound) about Google's deep dream project. Interesting to see how the non-ML community view this work.

Recently there's been a lot of anti-Google sentiment in this sub because of their patent activity. I thought it interesting that they're also responsible for popularizing some of what we all hold dear to our hearts.
",4,0
236,2015-7-14,2015,7,14,4,3d5ttq,What Computers Dream of When They Look at Porn (NSFW),https://www.reddit.com/r/MachineLearning/comments/3d5ttq/what_computers_dream_of_when_they_look_at_porn/,Hofmannsthal,1436815638,,0,0
237,2015-7-14,2015,7,14,5,3d60y5,[1506.06726] Skip-Thought Vectors,https://www.reddit.com/r/MachineLearning/comments/3d60y5/150606726_skipthought_vectors/,[deleted],1436818573,,0,1
238,2015-7-14,2015,7,14,6,3d67jc,Machine Learning version of the Intel/AMD comic,https://www.reddit.com/r/MachineLearning/comments/3d67jc/machine_learning_version_of_the_intelamd_comic/,geoffrey_hinton,1436821356,,8,63
239,2015-7-14,2015,7,14,6,3d67lk,"Web based Google Dream AI - many options, very cool!",https://www.reddit.com/r/MachineLearning/comments/3d67lk/web_based_google_dream_ai_many_options_very_cool/,[deleted],1436821385,,1,0
240,2015-7-14,2015,7,14,6,3d68lv,Understanding errors from a model,https://www.reddit.com/r/MachineLearning/comments/3d68lv/understanding_errors_from_a_model/,askerrore,1436821834,"Let's say I have a model of some process, call this model F, which depends on some physical inputs F(a,b,c,d,e,g,h). I can optimize this model on the model parameters (which I don't show above) to minimize some error on some measurements.

Now, what I'd like to be able to do is understand if the model displays any particular behavior in the errors for the model inputs. So, for example, if the model always overestimated the real data when b&gt;0.5 but underestimated it when b&lt;0.5 then that might be very useful information (I could perhaps use this information to modify the existing model in some way, or perhaps create 2 models, one for each subspace, which overall gave a better RMSE). 

What would be the best way to perform this type of analysis? I have the inputs, the trained model, and the measurements (i.e., real data).",6,2
241,2015-7-14,2015,7,14,7,3d6loz,[Slides] Two Big Challenges in ML - Leon Bottou (ICML '15),https://www.reddit.com/r/MachineLearning/comments/3d6loz/slides_two_big_challenges_in_ml_leon_bottou_icml/,vikkamath,1436827588,,2,16
242,2015-7-14,2015,7,14,8,3d6rgg,Resources to learn more intermediate Neural Network and beginner Genetic Algorithm techniques,https://www.reddit.com/r/MachineLearning/comments/3d6rgg/resources_to_learn_more_intermediate_neural/,SudoSilman,1436830268,"I have already taken a college course at my uni on machine learning where we implemented all the basic ML programs: linear regression, logistic regression, basic neural network with logistic regression (not perceptron, but we learned the theory of perceptron as a history lesson), k-means, and naive Bayes classifier. The class also had a high focus on the theory behind these algorithms so i know a lot of the relates maths.

But all of our projects were based on simple numbers. What I mean by that is all of the projects had features which were simple numbers such as miles per gallon, year, horsepower, weight, frequency, etc.  We never made anything that could understand more abstract things like text, or color, etc. 

I recently stumbled upon [this article]( http://www.escapistmagazine.com/articles/view/scienceandtech/14276-Magic-The-Gathering-Cards-Made-by-Artificial-Intelligence) about a recurrent neural network that makes up its own Magic: The Gathering cards and my interest in ML was piqued again. I want to learn to implement something which can learn about things besides basic numbers, I want to make something that can learn to put sentences together like the one in this article. Hell it even makes up its own words (fuseback) that don't exist in Magic and added rules text to them (like for Tromple).

What resources are there to learn how to make a system which can learn these more abstract ideas like words and colors?

Secondly, I recently saw [this YouTube video]( https://youtu.be/u2t77mQmJiY) of someone implementing a genetic algorithm to watch two animated tanks learn to shoot each other. I am highly interested in this as well and I feel like they must fall into similar veins of programming.

My first interest is the advanced ML stuff for creating abstract things like sentences, but learning the genetic algorithms is also on my to do list.",1,2
243,2015-7-14,2015,7,14,11,3d7dnr,"Should I add an ""ambiguous"" class to my SVM for data I can't tell the category of when I train the SVM?",https://www.reddit.com/r/MachineLearning/comments/3d7dnr/should_i_add_an_ambiguous_class_to_my_svm_for/,jackbrucesimpson,1436840867,"I've trained an SVM with OpenCV to classify images of tags my program can extract, however I used data where I could tell which tag type was which myself visually. Unfortunately, for maybe half the tag images I can't tell which type it belongs to (and thus didn't include it in my training set), and as a result the SVM classifies perfectly on my training data (when I split a testing set from it), but with the real data it's wrong about half the time. Would the best option here be to include an ""ambiguous"" class of data to my SVM with several hundred/thousand images of the tags where I can't tell what type it is? I'd rather have more instances of ""unknown"" classifications and be right more often when I do try to make a prediction.",10,4
244,2015-7-14,2015,7,14,12,3d7n56,Do DeepLearning4Java.word2vec()'s memory requirements increase with the size of the corpora? Or is it constant like gensim? Any other insights?,https://www.reddit.com/r/MachineLearning/comments/3d7n56/do_deeplearning4javaword2vecs_memory_requirements/,rob-on-reddit,1436845521,"
Hi!  I'm trying to assess hardware requirements for a semantic search task.  Is anyone aware of a direct comparison of gensim (optionally with DeepDist on Spark) and DeepLearning4Java's implementations of word2vec?  Specifically, I am wondering,

* Does DeepLearning4Java's word2vec use constant memory as in Gensim?
* Is it accurate to say DL4J is faster for smaller corpora that fit into GPU memory, but impractical for large corpora?
* Anyone have recent experience with the two they can share?

I've scanned the relevant websites, Google and this forum, but don't recall seeing it clearly stated.  My apologies if I missed something.  Thanks for your time!",3,1
245,2015-7-14,2015,7,14,13,3d7rkf,mlpack/mlpack: C/C++ ML lib started with Neural Networks implementations!,https://www.reddit.com/r/MachineLearning/comments/3d7rkf/mlpackmlpack_cc_ml_lib_started_with_neural/,fariax,1436847774,,2,9
246,2015-7-14,2015,7,14,13,3d7s5y,"Would like to build out ML algorithms as C libraries, any ideas on useful ones not already implemented in C?",https://www.reddit.com/r/MachineLearning/comments/3d7s5y/would_like_to_build_out_ml_algorithms_as_c/,philly7891,1436848093,"Hi! I'm a software developer who noticed that a lot of ML algorithms I find in research papers are currently in Python and Matlab since researchers tend to stick with these languages. I'd love to add to the open source ML community by porting some important ones over to C, so they can be parallelized and made faster. Any specific ones that would be useful for everyone? The algorithm could either be a popular one, or come from a paper, any would be cool. Thanks!",15,5
247,2015-7-14,2015,7,14,14,3d7z4o,Deepmind is sponsoring The General Video Game AI Competition,https://www.reddit.com/r/MachineLearning/comments/3d7z4o/deepmind_is_sponsoring_the_general_video_game_ai/,evc123,1436852382,,2,53
248,2015-7-14,2015,7,14,15,3d84xs,1st Place Solution for Search Results Relevance competition on Kaggle (code + pdf),https://www.reddit.com/r/MachineLearning/comments/3d84xs/1st_place_solution_for_search_results_relevance/,cast42,1436856251,,14,12
249,2015-7-14,2015,7,14,17,3d8eyb,Metal CN-100M capsule filler operation video,https://www.reddit.com/r/MachineLearning/comments/3d8eyb/metal_cn100m_capsule_filler_operation_video/,Delly789,1436864150,,0,0
250,2015-7-14,2015,7,14,18,3d8ha7,Perpendicularity and dimension,https://www.reddit.com/r/MachineLearning/comments/3d8ha7/perpendicularity_and_dimension/,benjaminwilson,1436866048,,0,1
251,2015-7-14,2015,7,14,20,3d8qdv,Any areas to avoid in the machine learning field?,https://www.reddit.com/r/MachineLearning/comments/3d8qdv/any_areas_to_avoid_in_the_machine_learning_field/,Mad-economist,1436873206,"I was wondering if there were certain parts of machine learning that one would better avoid. For example, there are a lot of notions in physics like ""perpetual motion machines"" and ""free energy"" that are impossible, although there still are a lot of wackjobs trying to invent them. A better example would be the case of the compression scheme that can compress any data. This example is a little bit harder to debunk, it actually doesn't sound all that crazy. It is still impossible though, due to the pigeonhole principle (you can't fit N items in N-1 containers). 

Are there any areas in machine learning that one would better avoid, regarding my previous examples? I don't have a lot of experience in the field and it is quite complicated stuff, so it doesn't seem all that unlikely to me that I could end up chasing after some wacky impossible idea.",40,9
252,2015-7-14,2015,7,14,22,3d904t,Training boltzmann machine with ReLU,https://www.reddit.com/r/MachineLearning/comments/3d904t/training_boltzmann_machine_with_relu/,tamagowitch,1436879301,"http://www.cs.toronto.edu/~fritz/absps/reluICML.pdf

I tried training the Boltzman machine with the ReLU.
But all the states diverge to infinity because the output of the ReLU is unlimited. How to train that? Is there a sample code?

How to train the Boltzmann machine with ReLU?",2,4
253,2015-7-14,2015,7,14,22,3d93t2,Beginner Tutorial: A Neural Network in 11 Lines of Python (or less),https://www.reddit.com/r/MachineLearning/comments/3d93t2/beginner_tutorial_a_neural_network_in_11_lines_of/,iamtrask,1436881288,,55,217
254,2015-7-14,2015,7,14,23,3d98fx,Machine Learning with Spark: Sparse feature extraction,https://www.reddit.com/r/MachineLearning/comments/3d98fx/machine_learning_with_spark_sparse_feature/,ezhulenev,1436883601,,0,1
255,2015-7-14,2015,7,14,23,3d98lb,Should You Trust Your Money to a Robot? the race to build big data machines in financial investing,https://www.reddit.com/r/MachineLearning/comments/3d98lb/should_you_trust_your_money_to_a_robot_the_race/,gradientflow,1436883669,,0,0
256,2015-7-15,2015,7,15,0,3d9klt,DataCamp Gets $1M Seed Round To Develop Data Science Learning Platform,https://www.reddit.com/r/MachineLearning/comments/3d9klt/datacamp_gets_1m_seed_round_to_develop_data/,martijnT,1436889101,,0,0
257,2015-7-15,2015,7,15,2,3d9vno,Harr Features Neural Nets?,https://www.reddit.com/r/MachineLearning/comments/3d9vno/harr_features_neural_nets/,jrkirby,1436893705,"So I recently read about [Harr Features](https://en.wikipedia.org/wiki/Haar-like_features) that were used about 15 years ago to do quick face detection. The trick was using [summed area tables](https://en.wikipedia.org/wiki/Summed_area_table) to very quickly compute a couple of vary basic features that have decent performance at recognizing objects. I was wondering if there were any examples of Neural Nets that used summed area tables as an input instead of a regular image? I imagine they might have good performance with a regular fully connected neural net instead of a convolutional one. I tried searching for papers on it, but I didn't see anything that seemed relevant. Anybody heard of a similar technique?",5,3
258,2015-7-15,2015,7,15,2,3da1bi,9 Reasons Why Now is the Time for Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/3da1bi/9_reasons_why_now_is_the_time_for_artificial/,joshdotai,1436896055,,1,0
259,2015-7-15,2015,7,15,3,3da50z,Confused about feature extraction,https://www.reddit.com/r/MachineLearning/comments/3da50z/confused_about_feature_extraction/,csris,1436897529,"I'm completely new to machine learning, so I have no clue what I'm doing. Given a dataset of audio (more specifically, the TIMIT acoustic phonetic continuous speech corpus dataset), how would I go about extracting features and generating a feature vector? What language/libraries should I be working with?",10,1
260,2015-7-15,2015,7,15,4,3daey8,Practical Deep Learning Video (2014),https://www.reddit.com/r/MachineLearning/comments/3daey8/practical_deep_learning_video_2014/,[deleted],1436901531,,0,0
261,2015-7-15,2015,7,15,5,3damvg,Resources to do a quick review of ML topics,https://www.reddit.com/r/MachineLearning/comments/3damvg/resources_to_do_a_quick_review_of_ml_topics/,deathstone,1436904782,"Hello r/MachineLearning

I'm a senior in college and will be sitting for Job interviews in a fortnight. I had completed a course in Machine Learning in college as well as Andrew NG's course on coursera (albeit partially) in the second year. During this period, I completed two decently good projects:

1) Active Learning based Sentence classifier of Scientific Papers
    - Query by committee sampling method was used to train an active learning classifier. It was implemented by writing a decision tree classifier to annotate these papers.
2) Time series based forecasting of product sales in a local super market.
	- The Naive bayes classification technique was used.

Due to a few reasons, I lost touch with the field of ML and haven't been very active in reading up on new research nor completing projects in this field.

I am finding it difficult to remember the logic I used while using these methodologies to complete my projects, hence I plan to do a quick overview of the relevant/most important ML algorithms and techniques. I have come up with this preliminary list of topics to cover in the next 14 days and I would appreciate it if anyone could direct me in the right direction. 

Techniques to read up on:
	
        I) Supervised Learning:
		a) Classification:
			1) Logistic regression
			2) Linear SVM and kernel SVM
			3) Random Forests
			4) Neural Networks
			5) Ensemble Methods: Bagging
			6) Naive Bayes
		b) Regression:
			1) Least Square Method

	II) Unsupervised:
		a) k-means clustering

	III) Reinforcement Learning

	IV) Dimensionality redution Techniques

Any feedback regarding this above list would be appreciated. If I'm missing out on any important topics, let me know. I came up with this list from a bit of googling and by recollecting from memory.",3,1
262,2015-7-15,2015,7,15,7,3db2wb,Undergraduate Comp Sci Courses for someone interesting in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3db2wb/undergraduate_comp_sci_courses_for_someone/,Velm,1436911513,"I was hoping for some advice about which (if any) CS courses would be beneficial to take before pursuing Machine Learning in graduate school, and if it would be possible to ignore some prerequisites given my very strong math/stat background.

I am currently an undergraduate majoring in Mathematics and minoring in Statistics. I have also taken three semesters of Computer Science courses, including a class about data structures and basic algorithms. As I look at graduate institutions doing research in Machine Learning, I find that many of them are located within Computer Science departments. From reading a few discussions here ([this](https://www.reddit.com/r/MachineLearning/comments/1qb1jz/graduate_study_in_statistics_or_computer_science/) and [this](https://www.reddit.com/r/MachineLearning/comments/2cx4ca/applying_to_top_phd_programs_with_statistics/)), I think I may be more interested in those than the ones based in stats departments.

My coursework generally meets the prerequisites for PhD programs in Statistics (Multivariate Calculus, Linear Algebra, Calc-based stats, Real Analysis), but I am unsure what it takes to be qualified to study machine learning from a CS perspective.

Some institutions offer no information, while others say ""the equivalent of an undergraduate education in Computer Science,"" and few give specific requirements, which generally include Theory of Computation, Operating Systems, Computer Architecture, and Programming Languages. I doubt that a course in operating systems is necessary to study machine learning, but I suppose it could be necessary for some other parts of a given program's curriculum.

Thanks.",8,2
263,2015-7-15,2015,7,15,7,3db8y3,Awesome Random Forest,https://www.reddit.com/r/MachineLearning/comments/3db8y3/awesome_random_forest/,kjw0612,1436914151,,10,6
264,2015-7-15,2015,7,15,9,3dbl0s,Digested Thoughts on the Panel on Future of ML/AI - Neil Lawrence Blog,https://www.reddit.com/r/MachineLearning/comments/3dbl0s/digested_thoughts_on_the_panel_on_future_of_mlai/,clbam8,1436919759,,0,5
265,2015-7-15,2015,7,15,10,3dbrum,Outlier Detection at Netflix,https://www.reddit.com/r/MachineLearning/comments/3dbrum/outlier_detection_at_netflix/,altrego99,1436922952,,17,34
266,2015-7-15,2015,7,15,15,3dcr6y,Is Ocropus really worse than Tesseract as people seem to think?,https://www.reddit.com/r/MachineLearning/comments/3dcr6y/is_ocropus_really_worse_than_tesseract_as_people/,ocropus,1436942200,"I've been working with Tesseract because its been widely recommended over Ocropus, including on reddit and on stack overflow in posts like this: http://stackoverflow.com/questions/10032923/which-ocr-engine-is-better-tesseract-or-ocropus

The Tesseract google groups is also quite a bit  more active than the Ocropus one.

I know that Ocropus is significantly more recent, and has additional preprocessing capabilities as well as supporting handwriting recognition. With this in mind it seems strange that tesseract is consistently recommended over Ocropus. Is it true that Tesseract is signficantly better than Ocropus, and if so why? It seems quite counterintuitive that the older engine with less capabilities would come out ahead in performance and user base.

Edit: This 2007 paper :http://www.helsinki.fi/~mpsilfve/ocr_course/materials/2008-breuel-ocropus-open-source.pdf
claims Ocropus ""is currently the best available open source OCR system for English, when using the combination of RAST-based layout analysis with the Tesseract text line recognizer.""",1,0
267,2015-7-15,2015,7,15,17,3dcxmu,How to Make Your Capsule Blends?,https://www.reddit.com/r/MachineLearning/comments/3dcxmu/how_to_make_your_capsule_blends/,Delly789,1436947266,,1,0
268,2015-7-15,2015,7,15,17,3dczvc,Trends and highlights of ICML 2015,https://www.reddit.com/r/MachineLearning/comments/3dczvc/trends_and_highlights_of_icml_2015/,dustintran,1436949210,,1,47
269,2015-7-15,2015,7,15,17,3dd0cq,Introduction to Machine Learning - The Wikipedia Guide,https://www.reddit.com/r/MachineLearning/comments/3dd0cq/introduction_to_machine_learning_the_wikipedia/,Nixonite,1436949615,,3,4
270,2015-7-15,2015,7,15,20,3ddal1,Yoshua Bengio and IBM,https://www.reddit.com/r/MachineLearning/comments/3ddal1/yoshua_bengio_and_ibm/,pseudopotential,1436958152,,2,11
271,2015-7-15,2015,7,15,21,3ddjas,Sparse Clustering: The Challenges of Reddit's Sparse Admins/Mods Graphs and Sudden Phase Transitions,https://www.reddit.com/r/MachineLearning/comments/3ddjas/sparse_clustering_the_challenges_of_reddits/,compsens,1436964314,,2,5
272,2015-7-15,2015,7,15,22,3ddmxk,Will there be a NIPS 2015 workshop on Hardware for Machine Learning ?,https://www.reddit.com/r/MachineLearning/comments/3ddmxk/will_there_be_a_nips_2015_workshop_on_hardware/,compsens,1436966453,"Hello everybody, While we all are aware of the improvement of DNNs thanks to GPUs, there seem to be other architectures that are also trying to map the current successful algorithms close to hardware. Hence, I was wondering if somebody or a group of people had submitted or is submitting a workshop on the hardware side of machine learning for the upcoming NIPS2015 ? (Deadline in July 18th).  ",1,0
273,2015-7-16,2015,7,16,1,3de74a,Where to buy affordable (non-PDF) books on ML?,https://www.reddit.com/r/MachineLearning/comments/3de74a/where_to_buy_affordable_nonpdf_books_on_ml/,Stickypatrol,1436976142,"I couldn't find any such question with the search function so I thought I'd just ask.

What are affordable ways of finding good ML books, I live in the Netherlands and I can't find books on ML(such as Machine Learning by Thom Mitchell) for anywhere NEAR affordable rates. I'm a student and I can't just shell out 50+(often way more) euros for a book.

I konw that there's a lot of free PDFs to find on the web but I strongly prefer just a physical book since sometimes I just have to be away from my computerscreen for a while and having a book feels very nice(no worrying about electronics and such).

even at my college I can only find 1 or 2 and that's for renting.

Do you have any suggestions? I've been thinking about buying an e-reader since that's my second choice...",6,0
274,2015-7-16,2015,7,16,1,3deb6j,Is this possible using a Deep Convolutional neural network?,https://www.reddit.com/r/MachineLearning/comments/3deb6j/is_this_possible_using_a_deep_convolutional/,chaddjohnson,1436977895,"Hey everyone,

I'm in process of learning about and experimenting with Deep Learning. I'm thinking about starting a for-fun project for my learning sake and hoping someone can tell me whether this would actually work.

This would be another image classifier. The use case is as follows:

1. A user visits a web application I've built (which fronts the neural net)
1. The user bulk uploads 1,000 or so photos (probably using FTP)
1. After upload, the user is presented with a list of all their photos
1. The user now can select 5 - 10 or so similar photos
1. The selections are passed to the neural net, and the neural net finds all photos within (and only within) the user's 1,000 or photos
1. The user now sees a filtered list of photos they uploaded based on the 5 - 10 similar photos they selected.

So let's say a user uploads 1,000 photos of car parts, and then they select 5 - 10 photos of just tires; their 1,000 photos would then be filtered to only show tires.

Or let's say I have 1,000 photos of clothing, and I select 5 - 10 photos of shirts; I would only see shirts from my 1,000 photos. OR, let's say I specifically select *red* shirts -- I would only see shirts that happen to be red.

I imagine the network would require a significant amount of pre-training based on ImageNet and other data. And then it would continue learning based on data uploaded by all users. I figure the same network would be used for all users.

Does this seem feasible using a Deep Convolutional neural network? Can a Deep Convolutional network be used to continually create multiple different classifications, per user, as described?",10,7
275,2015-7-16,2015,7,16,1,3deeyh,"What is the origin of the word ""Hidden"" regarding hidden layers in neural networks?",https://www.reddit.com/r/MachineLearning/comments/3deeyh/what_is_the_origin_of_the_word_hidden_regarding/,BigMakondo,1436979513,I was just curious about the first time the term was used and why it was chosen. Does someone know? ,7,0
276,2015-7-16,2015,7,16,2,3defmn,A Curious Analogy Between Ethics and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3defmn/a_curious_analogy_between_ethics_and_machine/,john_philip,1436979791,,5,1
277,2015-7-16,2015,7,16,3,3des1e,Looking for suggestions on tools for analysis of instrument timbre.,https://www.reddit.com/r/MachineLearning/comments/3des1e/looking_for_suggestions_on_tools_for_analysis_of/,beokabatukaba,1436984937,"Skip to the final two paragraphs to get to the real point of this post.

I'm working for my university this summer attempting to analyze timbre of various instruments and play styles. The end goal is to find a way to *visually* separate a 'desirable' tone from a 'poor' tone using data from real time FFTs. We hope to be able to design an app for students that will allow a teacher to record sounds that are 'desirable', have them displayed as a region in some sort of 2 dimensional space, and then have the students attempt to match their tone to the teacher's by moving their own marker into that 'desirable' region by working on their technique. At the moment, we are trying to figure out which strategies we should be using in order to get the best spacial visualization for this purpose.

**Warning**: Neither I nor my co-worker have taken a course in linear algebra, statistics, programming, or machine learning (why they hired us, I'm not sure). We've done our best to learn about each of these topics from Google (as well as some other resources), but we certainly haven't mastered any of it. It's very possible I'm going to sound like an idiot at multiple points.

If you have any suggestions for any part of our process below, please let me know! So far, our process goes a little something like this:

**Record audio**: We asked our performers to play various scales in ways both 'good' and 'bad' in order to find patterns between 'good', 'pure', 'developed' tone and 'poor', 'elementary', 'muddled' tone. The best format we've found for doing so is in wav, 44.1khz, 16-bit, mono.

**Standardize length of audio**: I wrote some code to chop up all of the audio files into pieces of 2205 samples each (50 milliseconds). It seemed like most audio analysis tools lean more towards 20 milliseconds, but we thought that the FFT appeared to be losing too much resolution at that point and took it a step longer.

**Take FFTs**: We're using np.fft.rfft simply because it appeared to work best for our needs.

**Find the fundamental**: We had the most luck using a piece of code found [here](https://gist.github.com/endolith/255291). The autocorrelation method was the only one that gave us consistently reasonable results, so we settled on that one by default. If anyone has better suggestions, I'd certainly be open to trying them.

**Normalize the FFTs**: For each piece of audio, we divide the amplitudes by the max amplitude and the frequency data given by np.fft.rfftfreq by the fundamental in order to do away with differences in pitch and volume.

**Interpolate and stack**: We use scipy.interpolate.griddata to take all of the frequency and amplitude data and line it up with each other. Right now, we're just arbitrarily choosing one of the normalized frequency arrays as the basis for the rest, but I'm guessing there's a more fool-proof way to do this. If you know of one, please let me know.

**Interpret the data!**: At this point, we can visualize all of the FFTs directly by graphing them together (to see differences in harmonics between each instrument), or we can feed them into a PCA(principal component analysis) or LDA(linear discriminant analysis), both of which are taken from the scikit-learn library.

This last piece is where we need more help. Applying a PCA to the data reveals *some* differences between instruments, but timbre is a little too subtle to see any clear separation between our 'good' and 'bad' inputs. LDAs have been much more fruitful since we are able to specifically separate the data based on both instruments *and* type of timbre, but I'm getting worried that the separation is *too good* and may actually be misleading. When I input, for example, 'good' cello and 'bad' cello data, LDA returns only a single dimension with all of the 'good' on one side and all of the 'bad' on the other with next to zero mixing. I find it unreasonable that the separation between 'good' and 'bad' should be so distinct. [Here](http://imgur.com/a/FNXj3) are some examples.

I hope some of you with more experience in this area may be able to suggest some strategies that haven't popped up in our googling. I've begun taking a look at using MFCC(Mel Frequency Cepstral Coefficients) but as of yet, I haven't discovered a meaningful way to apply/visualize the data from this technique.

Thank you in advance for any help you can give!",1,1
278,2015-7-16,2015,7,16,3,3deshg,Deep Networks for Computer Vision at Google ! 2014,https://www.reddit.com/r/MachineLearning/comments/3deshg/deep_networks_for_computer_vision_at_google_2014/,ojaved,1436985120,,0,2
279,2015-7-16,2015,7,16,3,3dev5e,How many classifications can a Convolutional Neural Network learn?,https://www.reddit.com/r/MachineLearning/comments/3dev5e/how_many_classifications_can_a_convolutional/,chaddjohnson,1436986256,"Let's say I train a Convolutional network on ImageNet and every image set I can get my hands on.

And let's say I have 100,000 photos of products from car parts, to clothing, to jewelry. I also train the network on these as well.

Now let's say I want to create classifiers to find photos containing close-ups of car tires...and also (as another classification category) to find photos of sports balls (basketballs, soccer balls, baseballs)...and another to only find basketballs...and another to find long-sleeve shirts...and another to find only *white button-down* long-sleeve shirts...and yet another to find only *striped* button-down long-sleeve shirts.

So now I have several classifiers using one network, and some of them produce very similar results.

How many classifications can a network realistically learn? Can one network learn thousands of classifications (where there may be several very similar but slightly different classifiers)?",2,0
280,2015-7-16,2015,7,16,5,3df6ps,Stupid question about deep dream stuff: why maximize a whole layer and not a single neuron?,https://www.reddit.com/r/MachineLearning/comments/3df6ps/stupid_question_about_deep_dream_stuff_why/,dominosci,1436990963,"Maybe this is the wrong place to ask this but I'm a little confused about how these wacky images are being generated. My understanding is that they take a deep neural network that has been trained to recognize lots of different things and then they recursively modify the image to maximize all the neurons on a particular layer.

But the top layer has neurons for all kinds of things, right? There's a neuron that turns on for a cat face and one that turns on for german shepherds or whatever. Why not try to maximize one specific neuron? That would lead to images where everything looks like a cat or a car or a boat instead of one where everything looks like one of a number of things.

I'm mostly asking this question to find out how my understanding of this process is wrong. I'm sure there is a good reason for it. I just don't understand what it is.",17,10
281,2015-7-16,2015,7,16,7,3dfnro,Best way to get a neural network to fit with the least number of epochs,https://www.reddit.com/r/MachineLearning/comments/3dfnro/best_way_to_get_a_neural_network_to_fit_with_the/,feedthecreed,1436998303,"I have an unconventional machine learning problem where I need to have the model learn as quickly as possible from the data it receives. Overfitting isn't a concern, but I can only train on the examples once or twice at most. I can't run multiple epochs over the same examples during learning.

What are the best ways to get a neural network to learn as much as it can from a single presentation of the example?",13,0
282,2015-7-16,2015,7,16,7,3dfohk,How to deal with training a binary classification model where a huge portion of features in both classes overlap?,https://www.reddit.com/r/MachineLearning/comments/3dfohk/how_to_deal_with_training_a_binary_classification/,waronxmas,1436998640,"I am working on a bit of a side-project to expand my knowledge in ML and, unfortunately, I picked one that has me stumped.  I have seen a number of papers that talk about accent classification.  What I want to do is along the same lines: given an audio clip is the speaker talking with accent A with greater than X% probability?

My approach thus far has been to break each of my labeled training clips into windows, construct a feature vector for each window, and train an SVM.  Then with another set of labeled training clips, I classify each window in each clip as being part of the accent or not, and then train for a threshold on the number of features in the clip labelled as being part of the accent.

The problem with this is rather obvious: given a window into a clip it is very likely that its classification is indeterminate (perhaps it's a pause, or a sound that doesn't leave much room for an accent to differentiate itself).  Also, an SVM doesn't really take into account what my goal is.  An SVM will do its best to differentiate between individual features in my two classes labeled ""yes"" or ""no"", but that isn't what I want--I want to be able to train a SVM that optimally differentiates between ""bags"" of features.  I have also read about using Gaussian Mixture Models for this sort of thing, but again, I am still worried that the large amount of common features will skew the model to be very sub-optimal.

I remember back to my introductory class a problem where we used a ""bag of words"" representation of emails to classify them as spam.  In that case, it was easy to find the features that should be ignored since it is just a question of counting the occurrence of a given word in both classes.  Obviously in this case it isn't so simple because the features are very high-dimensional.  I can try clustering my features regardless of their labels and then mapping my audio clips into a ""bag of words"" linear classification model where the feature vector is the number of windows that map to a given cluster for a clip.  I could also try to do something to reduce the dimensionality of my features before the clustering.  However, in both cases, I still am afraid that the indeterminate features will dilute things by a lot.

Has anyone ever faced a similar problems?  How did you approach it?  Any good papers to consult?  Thanks in advance!",7,1
283,2015-7-16,2015,7,16,12,3dgnr2,k-Nearest Neighbor Classification with College Football Play-by-Play Data,https://www.reddit.com/r/MachineLearning/comments/3dgnr2/knearest_neighbor_classification_with_college/,medavis6,1437015985,"I am still not entirely sure this kind of post is acceptable here, but I looked around the FAQs and /r/mlclass and /r/MLQuestions and noticed those were either not relevant or not active so mods, I'm sorry if I broke any rules. I want to clarify that I'm very new to machine learning and this actually was my first ""successful"" run.

I have a questions regarding a **knn analysis** I'm running in **R** and the next steps I would have to take to make it more applied going forward. I've been able to run the training on ~1M individual plays from dates between 2005-2013 for college football. **I'm attempting to have the machine either predict pass or run based on many situational factors** (game clock, quarter, points for/against, yard line, home team, away team, etc.). I've tested it on 10k plays with an accuracy rate of about 65%. I've been using the knn function from the class library.

My question is regarding how to move this forward into a program that I could search at my own will to find data that I want in it. 

* Okay computer, it's 2nd and 7 on the 37 yd line, 14:15 left in the 2nd quarter and we're down 7-0, what are the odds Wisconsin is running the ball here?

So basically, I'm wondering how I can turn this into a program that I can enter some data into and see the most likely outcome. Is it even possible to do this in R or do I need to learn some programming to turn it into an app/dashboard/website? I'm pretty sure this isn't like linear regression where I can start writing equations based on output and weight of variables. Again, I'm sorry if this isn't the kind of post allowed around here but if anyone could point me in the right direction to helpful information or provide me with some guidance, it would be much appreciated. Please, feel free to ask any clarification questions you might have and thank you for any and all time.

---

**EDIT**: Thanks for all the help and tips! Much appreciated. Will be doing a follow up random forest and logistic regression to start! ",20,10
284,2015-7-16,2015,7,16,12,3dgr69,Data sparsity AND Data set sparsity for LDA modeling,https://www.reddit.com/r/MachineLearning/comments/3dgr69/data_sparsity_and_data_set_sparsity_for_lda/,ThatCrankyGuy,1437017809,"I apologize if this is the wrong subreddit, but I'm looking for some assistance on this matter.

I have a project, they want to model their users and recommend ads and products. Problem is, they're mostly an Image sharing platform, with few hundred active users. Very few comments, untagged images, very specific scope and type of images too.

They have a blogging section, but folks don't post much.

So I have not only a data sparsity issue, but also a sparse dataset of sparse text.

Is trying to apply collaborative filter on a latent topics a bad idea? What else can I do here?

What about TF/IDF approach? Is a simple approach more fitting in this case?

Thanks",1,2
285,2015-7-16,2015,7,16,13,3dguqq,Deriving an Equation from function inputs and outputs.,https://www.reddit.com/r/MachineLearning/comments/3dguqq/deriving_an_equation_from_function_inputs_and/,Kendama_Llama,1437019797,"I am interested to know if there is a machine learning program that would allow me to provide the inputs to a function and the outputs of that same function and get an equation. For example say I fed it inputs: 2,3,4 and the respective outputs: 4,6,8. Is there something that would tell me the function is f(x)=2x? Thanks for any help you can provide! ",13,5
286,2015-7-16,2015,7,16,13,3dgvvu,How to make RNN learning faster?,https://www.reddit.com/r/MachineLearning/comments/3dgvvu/how_to_make_rnn_learning_faster/,kapectas,1437020433,"OK, so I found [this thread of AI-generated MtG cards](http://www.mtgsalvation.com/forums/creativity/custom-card-creation/612057-generating-magic-cards-using-deep-recurrent-neural), decided to give the neural network a try myself, and got hooked. I followed the instructions in the thread and set up a nice Ubuntu VirtualBox VM running Torch7 and a LSTM RNN. However, it does it so damn slowly, due to being limited by my CPU through the VM and not being able to use my GPU. So my question is... how do I make it go faster?

**The options that I'm aware of:**

* Try to install Torch7 on Windows, to access CUDA with my gtx560 ti. Attempted this; it's heinously difficult and I've pretty much failed. One guy who's apparently done it responded to my email by saying, essentially, ""don't"". Failing that, install other software that can run on Windows and use CUDA, which can also do LSTM RNNs. Problem is, I don't know of any such software out there that fits those requirements.

* Try to get PCI Passthrough working for Ubuntu in Virtualbox. This is tricky, but at least there's explicit instructions (unlike Torch for Windows, which involves arcane handwaving and guesswork). The one failure point is the requirement for the motherboard, cpu, the Ubuntu kernel, the BIOS, (the pet dog, my neighbour...) to all be IOMMU compatible, which for an Intel cpu translates as having VT-d. Now... my i7-2600k has VT-x. But, no VT-d. The i7-2600 (which I didn't buy, because hey, the K must be better IN ALL WAYS than the non-K, right?) has both VT-d and VT-x. Goddamnit, Intel. 
Therefore, the only way I can do PCI Passthrough (as far as I know) is to get a CPU with VT-x. Ok, but these days (if I want to maintain same number of cores, etc.) that means something like an i7-4790k. It's 400 CAD and won't fit into my Asus P8P67 motherboard, so add another 150 CAD for a new one. And after all that, PCI Passthrough might even not work that great. Yeesh. I don't know if there's other virtualization software out there that's free/low cost and lets a GPU run CUDA through it, but I find it unlikely (since I've searched and not found any).

* Dual-boot Ubuntu. Probably the easiest option, but also the most annoying. I use Win7 as my daily OS, and there's plenty of stuff I can't do in Ubuntu that I can do in Win7. If I wanted to run training or sample a checkpoint file I'd have to be in Ubuntu, which is more annoying than being in Windows.

So ideally, I'd want to find a way of running Ubuntu through some sort of VM, on Win7 as the host, with it having access to my GPU for CUDA calculations. Or, find some software that can do RNNs on Windows.",34,7
287,2015-7-16,2015,7,16,16,3dhbvc,Best Movies with Big Data and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3dhbvc/best_movies_with_big_data_and_machine_learning/,john_philip,1437031607,,2,0
288,2015-7-16,2015,7,16,20,3dhsu4,Massively Parallel Methods for Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/3dhsu4/massively_parallel_methods_for_deep_reinforcement/,clbam8,1437046415,,4,14
289,2015-7-16,2015,7,16,20,3dhsv0,Predictive distributions: when does it make sense to fit a measure to data?,https://www.reddit.com/r/MachineLearning/comments/3dhsv0/predictive_distributions_when_does_it_make_sense/,ocramz,1437046432,"More in general, if a fitted generative model is at best a biased approximation, and let's say we're interested in the predictive distribution, there is always an information loss associated with the fit.
On the other hand, comparing distributions (KL), applying Bayes, etc, all require the measures to have a common support, which cannot be guaranteed in the purely sampled case.

So, simple example, a Kalman filter or other Gaussian phenomena: the expectation over next states is performed under a Normal measure assumption, so the integrals have closed form etc.

However, IIUC, in all cases when we don't have a conjugate pair, we can only use the empirical measure.

In what cases/applications are we interested in fitting an analytical measure to data ? ",4,2
290,2015-7-16,2015,7,16,21,3dhvhe,Deep Learning Adversarial Examples  Clarifying Misconceptions,https://www.reddit.com/r/MachineLearning/comments/3dhvhe/deep_learning_adversarial_examples_clarifying/,clbam8,1437048367,,11,15
291,2015-7-16,2015,7,16,21,3dhybh,Deriving the Reddit Formula,https://www.reddit.com/r/MachineLearning/comments/3dhybh/deriving_the_reddit_formula/,Kiudee,1437050081,,6,47
292,2015-7-16,2015,7,16,22,3di2zh,Third Eye Module,https://www.reddit.com/r/MachineLearning/comments/3di2zh/third_eye_module/,gabriel1983,1437052883,"What if there was an input layer that would receive information regarding the synaptic activity of the central associative layers?

(And would send it forward to be processed it in the same way all sensory data is processed, eventually feeding back into the higher central associative layers.)

LATER EDIT: I don't mean feeding it directly back into the central association layers, I mean that it will probably get there after much further processing, like data from the other input layers, after making *some* sense of it.

LATER EDIT #2: Like the insular cortex: https://en.wikipedia.org/wiki/Insular_cortex",11,0
293,2015-7-16,2015,7,16,23,3diayn,6 reasons why I like KeystoneML,https://www.reddit.com/r/MachineLearning/comments/3diayn/6_reasons_why_i_like_keystoneml/,gradientflow,1437056975,,0,0
294,2015-7-16,2015,7,16,23,3did4e,Request for Comments: Exchangeable/Bayesian Models via Recurrent Neural Networks?,https://www.reddit.com/r/MachineLearning/comments/3did4e/request_for_comments_exchangeablebayesian_models/,fhuszar,1437058048,,0,2
295,2015-7-17,2015,7,17,4,3djg0u,Teaching a computer to learn when classifying an action as correct/incorrect isn't easy,https://www.reddit.com/r/MachineLearning/comments/3djg0u/teaching_a_computer_to_learn_when_classifying_an/,BPellegrino,1437075111,"So myself and a friend are both ex-professional poker players who have been spending our time studying CS and beginning to delve into machine learning. We've been having a debate on what the best way would be to teach a machine to play poker (not solve GTO like the alberta project but teach it to be proficient at the game).

I think the approach of having it play against itself and improving iteratively that way like the first checkers machine learning algorithm did is going to be far too inefficient with the complexity of no limit holdem. Another approach would be letting it learn from the hands that myself or my friend have played and draw conclusions from that, knowing that it will be drawing/learning from a base strategy of people who were winning players, so the assumption is that most of those plays are good plays.

That said, the issue we both have trouble with is how it would actually learn on it's own what is a good play or what is a bad play. There are many instances in poker where you can make a good play that leads to a bad result. Example:

You have a board of AKJT4 , you 'know' your opponent only has a Queen in this situation 5% of the time. There is 500 in the pot, and you both have 1000 left. You go all in, knowing he will fold 95% of the time and you will win 500 chips, or he will call 5% of the time and you will lose 1000 chips, giving you a net gain of +425 chips. You have 78 (nothing), and go all in. In this specific instance your opponent calls you and you lose, although in an overall strategy this is a good play.

So my question from our argument is, what ML approaches would be best for tackling this sort of problem and is it likely that the machine would be able to classify individual plays with such depth or would it simply say ""Over the 5M hands that I have seen, making this play in this scenario is associated with a higher winrate than any other play in this scenario""?


This is not something we're going to be implementing since at the moment it's beyond us but we were curious how the different ML algorithms actually approach this scenario when you can have an action that results in a positive result but is in reality a negative action (and vice versa). Is the solution just that with a large enough sample these things should be weeded out and their true value/worth established?

Thanks! Great subreddit and amazing field :)",9,4
296,2015-7-17,2015,7,17,4,3djj6s,Active deep learning groups in europe?,https://www.reddit.com/r/MachineLearning/comments/3djj6s/active_deep_learning_groups_in_europe/,regularized,1437076488,Can you point some active research groups in academia&amp;industry who work on deep learning in Europe?,4,1
297,2015-7-17,2015,7,17,5,3djlir,LibSVM output in WEKA and how to interpret results,https://www.reddit.com/r/MachineLearning/comments/3djlir/libsvm_output_in_weka_and_how_to_interpret_results/,bubbachuck,1437077528,"This is a two part question.  

1) when I use LibSVM in WEKA, it just gives me statistics such as RMSE, Kappa, confusion matrix, etc.  Is there a way to actually output the feature weights and model itself?  I can right click and save the model as a "".model"" file, but when I try to open it in Notepad++, it's unintelligible gibberish.  When I use SMO instead of LibSVM, it does appear to output the feature weights.

2) are there any suggestions for how to interpret (i.e., rank) the importance of the features in SVM?  I have around 40 or so features.   I've seen forward selection using AUC to find out the relative importance of features.

Thanks for reading.",5,0
298,2015-7-17,2015,7,17,5,3djot1,This week's issue of Science focuses on AI and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3djot1/this_weeks_issue_of_science_focuses_on_ai_and/,rhiever,1437078941,,31,85
299,2015-7-17,2015,7,17,5,3djp2p,InstaIndex: Using Deep-Learning to index your Instagram pictures,https://www.reddit.com/r/MachineLearning/comments/3djp2p/instaindex_using_deeplearning_to_index_your/,terraces,1437079050,,0,7
300,2015-7-17,2015,7,17,9,3dkhuz,What happens if you build a neural network whose units are neural networks?,https://www.reddit.com/r/MachineLearning/comments/3dkhuz/what_happens_if_you_build_a_neural_network_whose/,StupidQuestionOracle,1437092623,,19,4
301,2015-7-17,2015,7,17,10,3dkmjy,Weighted classes in sklearn,https://www.reddit.com/r/MachineLearning/comments/3dkmjy/weighted_classes_in_sklearn/,wonkypedia,1437095093,"I have a dataset with binary classes, 0 and 1. If a 0 is wrongly classified as a 1, the penalty would be a number, say x=5. If a 1 is wrongly classified as a 0, there's no penalty. 

How do I reflect this in my classifier in sklearn? I tried the class_weights parameter in the random forest classifier, where I marked the class weights as weight[0] = x, and weight[1] = 1, but I'm getting counterintuitive results.

What's the right way to do this?",3,0
302,2015-7-17,2015,7,17,12,3dkzjq,Cross-entropy vs Logarithmic loss,https://www.reddit.com/r/MachineLearning/comments/3dkzjq/crossentropy_vs_logarithmic_loss/,[deleted],1437102060,"Hi /r/machineleaning,

I am trying to understand the difference between cross-entropy loss and logarithmic loss? When would you use one vs the other?

Specifically,
I am trying out the MLP tutorial for Theano that uses the log loss:

    loss = -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])
which gives good convergence on the MNIST dataset. If I use cross-entopy instead, 

    loss = T.mean(T.nnet.categorical_crossentropy(p_y_given_x, y))

I get a much higher error rate, and the model cost stops going down after 8-10 iterations. Can someone explain why this is happening?

Thanks",0,1
303,2015-7-17,2015,7,17,13,3dl7hi,Rationale for MCEM,https://www.reddit.com/r/MachineLearning/comments/3dl7hi/rationale_for_mcem/,[deleted],1437106509,"From what I've read, the main advantage of the EM algorithm is that the expectation step can be expressed in closed form giving a deterministic answer and thus 0 variance.

What's the rationale then behind MCEM (Monte Carlo EM) methods [1] which use sampling to calculate the E-step? Specifically, is there a theoretical/empirical evidence that MCEM gives lower variance than just doing MCMC on the full likelihood or are there some other advantages of the EM algorithm that come into play here?

[1] http://www.biostat.jhsph.edu/~rpeng/biostat778/papers/wei-tanner-1990.pdf",0,1
304,2015-7-17,2015,7,17,13,3dla6g,What's the rationale behind MCEM?,https://www.reddit.com/r/MachineLearning/comments/3dla6g/whats_the_rationale_behind_mcem/,Letitgo123456,1437108187,"From what I've read, the main advantage of the EM algorithm is that the expectation step can be expressed in closed form giving a deterministic answer and thus 0 variance.
What's the rationale then behind MCEM (Monte Carlo EM) methods [1] which use sampling to calculate the E-step? Specifically, is there a theoretical/empirical evidence that MCEM gives lower variance than just doing sampling on the full likelihood or are there some other advantages of the EM algorithm that come into play here?

[1] http://www.biostat.jhsph.edu/~rpeng/biostat778/papers/wei-tanner-1990.pdf

Edit: To clarify, I mean that if you have the likelihood log \sum_z p(y|x), z being latent, then one option is to use EM (or MCEM if your approximating distribution cannot give you a closed form). The other way I can see is to estimate the sum directly via sampling. So my question is if you're using sampling anyway, why use MCEM over numerical integration.",2,2
305,2015-7-17,2015,7,17,14,3dlgwr,Gorila: Google Reinforcement Learning Architecture,https://www.reddit.com/r/MachineLearning/comments/3dlgwr/gorila_google_reinforcement_learning_architecture/,iori42,1437112775,,2,13
306,2015-7-17,2015,7,17,15,3dlhfk,Can I get some ideas on using ML for fantasy baseball?,https://www.reddit.com/r/MachineLearning/comments/3dlhfk/can_i_get_some_ideas_on_using_ml_for_fantasy/,badatmagic,1437113169,"Honestly don't know where to start, just want to be pointed in the right direction. I was thinking if I could come up with a way to model expected fantasy points, then based on the player salary, I can just use the simplex method to max fantasy points constrained by total salary.",5,0
307,2015-7-17,2015,7,17,15,3dlkt3,Combining Spark and Computer Vision for image similarity?,https://www.reddit.com/r/MachineLearning/comments/3dlkt3/combining_spark_and_computer_vision_for_image/,[deleted],1437115689,"Has anyone done this before? The only way I can currently think of doing this is to compute the features using Sift from Opencv or one of the last layers of a neural net from Caffe/Theano, then taking those vectors into Spark and using MLLIB's Nearest Neighbors to find the closest vectors. Do libraries exist to extract CV features from images in Spark?",0,0
308,2015-7-17,2015,7,17,16,3dlon1,What areas of Machine Learning research would you recommend to a PhD student given that they have a background in pure mathematics and statistics?,https://www.reddit.com/r/MachineLearning/comments/3dlon1/what_areas_of_machine_learning_research_would_you/,ornstein-uhlenbeck,1437118704,"The title says it all.

Edit - Perhaps the title doesn't say it all which would explain the down-votes, so please allow me to be more specific. I am just trying to determine if there are any specific areas of Machine Learning which would benefit from more theoretical or statistical research. Aren't there any open (or just neglected) theoretical problems in the domain?",14,18
309,2015-7-17,2015,7,17,17,3dlr5h,What are vegetarian capsules made of?,https://www.reddit.com/r/MachineLearning/comments/3dlr5h/what_are_vegetarian_capsules_made_of/,Delly789,1437120925,,3,0
310,2015-7-17,2015,7,17,19,3dm10m,Neural interpretation of ECMWF ensemble (weather) predictions,https://www.reddit.com/r/MachineLearning/comments/3dm10m/neural_interpretation_of_ecmwf_ensemble_weather/,[deleted],1437129608,,0,1
311,2015-7-17,2015,7,17,20,3dm6rn,Dimensionality reduction being way too slow using PCA and a small dataset,https://www.reddit.com/r/MachineLearning/comments/3dm6rn/dimensionality_reduction_being_way_too_slow_using/,__null__,1437134331,"I have the following data set stored using numpy:
https://www.dropbox.com/sh/ppseiv9skqlhljr/AACQEWZh11oszL5-Z_NHqre3a?dl=0

There is a different numpy file for the training and development partitions of the data set [50,1,396]

I am using PCA Fast from the mlpy library in order to perform dimensionality reduction. However the whole process is way to slow and I can not find out way. Before I perform the PCA I convert the dataset to the following shape: [50,396] So the shape of the dataset is not the cause of my problem.

The code I use is the following:
https://www.dropbox.com/s/9kff3e15a4jp5ft/pca.py?dl=0

I am using this code under ubuntu linux and python 2.7. To install mlpy one has to use the following commands:
&gt;wget http://sourceforge.net/projects/mlpy/files/mlpy%203.5.0/mlpy-3.5.0.tar.gz

&gt;tar xvf mlpy-3.5.0.tar.gz

&gt;cd mlpy-3.5.0

&gt;sudo python setup.py install

Finally to run this code, assuming the script is stored as pca.py and it is is in the same folder that the directory feature_vectors containing the partitions of the datasets resides, one must use the following command:

&gt;python pca.py inputfiletrain feature_vectors/train/featuresShape.npy outputfiletrain feature_vectors/train/featuresShapePCA.npy inputfiledev feature_vectors/development/featuresShape.npy outputfiledev feature_vectors/development/featuresShapePCA.npy 

I need ideas, why the PCA is that slow on this dataset...

",3,0
312,2015-7-17,2015,7,17,21,3dm7cc,Can someone explain to me why scikit-learn still hosts on source-forge given its propensity to inject ad/malware into repos on the site?,https://www.reddit.com/r/MachineLearning/comments/3dm7cc/can_someone_explain_to_me_why_scikitlearn_still/,DrLionelRaymond,1437134690,,4,0
313,2015-7-18,2015,7,18,0,3dmwkp,Spark and Image Recognition,https://www.reddit.com/r/MachineLearning/comments/3dmwkp/spark_and_image_recognition/,philly7891,1437148342,"Has anyone done this before? Has far as I can tell, Spark has no computer vision capabilities, so you'd have to compute the features outside, then put them into Spark and run MLLIB tools on them.",2,0
314,2015-7-18,2015,7,18,1,3dmyob,Feature selection for SVM,https://www.reddit.com/r/MachineLearning/comments/3dmyob/feature_selection_for_svm/,bluedunnock,1437149321,"I have a dataset with more than 2000 attributes and 6000+ examples. So far  logistic regression with l1 regularization works the best, I tried rbf kernel. Svm with top 100 features chosen by log reg model. The results are even worse, what is the recommended way to perform feature selection for SVM",14,0
315,2015-7-18,2015,7,18,2,3dn9b4,"Must Watch Python for Data Science Videos from SciPy Conference 2015, Austin, Texas (From /r/Python)",https://www.reddit.com/r/MachineLearning/comments/3dn9b4/must_watch_python_for_data_science_videos_from/,john_philip,1437154112,,1,126
316,2015-7-18,2015,7,18,3,3dnieg,[TOMT] Trustyou video on machine learning,https://www.reddit.com/r/MachineLearning/comments/3dnieg/tomt_trustyou_video_on_machine_learning/,bulletninja,1437158151,"I watched a video on machine learning on youtube several weeks ago but i can't find it! Can you guys help me find it? I remember the guy was from trustyou and he showed a product in development that recommended hotels similar to others you liked already in different parts of the world. He mentioned using luigi, and he was colombian, thanks in advance.

**Edit**: Nevermind, i found [it](https://youtu.be/eIEsdluhoxY), should i delete this post??",0,0
317,2015-7-18,2015,7,18,4,3dnm1i,What are some good books on neural networks?,https://www.reddit.com/r/MachineLearning/comments/3dnm1i/what_are_some_good_books_on_neural_networks/,You_Have_Nice_Hair,1437159782,"I have heard a lot about neural networks over the past few years, and have a basic understanding. I need to learn more. What are some of the books that you guys have found useful?",16,0
318,2015-7-18,2015,7,18,4,3dnolr,Disadvantages of Neural Networks/Deep Learning - why are they not preferred in all situations ?,https://www.reddit.com/r/MachineLearning/comments/3dnolr/disadvantages_of_neural_networksdeep_learning_why/,hey_chicago,1437160916,"I'm looking for someone to improve my understanding of neural networks and why they aren't used more frequently.  I understand that neural networks are able to take into account a wide variety of features and using hidden layers determine how the outcome is modeled.  

Given the complexity/power/accuracy of this, why are they not used more often?  For example many of the competitions I see on Kaggle and other data science websites, use ensemble methods with random forests and boosting to deliver highly accurate results.

The only thing I can think of is that neural networks have the potential to overfit? but only if the training rate is too large or not enough learning simulations (epochs?) are conducted?

Can someone enlighten me on this area?  Perhaps I'm not explaining things clearly but hopefully you get the idea.  ",17,22
319,2015-7-18,2015,7,18,4,3dnptf,I am looking for an LSTM LM paper that did a model transfer trick.,https://www.reddit.com/r/MachineLearning/comments/3dnptf/i_am_looking_for_an_lstm_lm_paper_that_did_a/,rrenaud,1437161442,"They generated a corpus using an LSTM LM, and then trained an N-gram model on it.  I think it might have been from Bengio (aren't so many of the LSTM LM ideas from him?), but I am not totally sure.",4,2
320,2015-7-18,2015,7,18,7,3doc0g,a few questions about text classification with Recurrent Neural Networks in python using pybrain,https://www.reddit.com/r/MachineLearning/comments/3doc0g/a_few_questions_about_text_classification_with/,spudzee111,1437171837,"I've been attempting to create a text classification algorithm for a few projects I'm working on. I have a huge interest in Ai, and applications for it.

[Here's my current code](http://pastebin.com/92ULKeP0)

It's an algorithm that takes the titles of youtube videos and is supposed to output if the video is a lecture or course (1) or if it's something else (-1). In this example I use the index of a character in a string of all characters as input for each neuron. The input layer has 50 neurons so the text can't be over 50 characters. If the text is under 50 characters the rest of the neurons get the input 0. The network has 20 hidden layers with 100 neurons in each. The learning rate for the network is 0.001 which I found from trial and error. Currently the output of the neural network is not quite what I would like it to be. 

my questions are: is this the best way to do text classification with recurrent neural networks? A few specific things I'm wondering about are what is the ideal amount of neurons to have in each layer? How many hidden layers should I have? Should I be using the activation function I'm using or something different?

EDIT: disclaimer before I get downvoted to oblivion: I'm new to machine learning",6,0
321,2015-7-18,2015,7,18,13,3dpi97,Sparsely connected LSTMs in Torch?,https://www.reddit.com/r/MachineLearning/comments/3dpi97/sparsely_connected_lstms_in_torch/,deep_rabbit,1437195518,"I've been fiddling around with /u/badmephisto's [lovely LSTM sequence prediction code](https://github.com/karpathy/char-rnn) to teach myself Torch. I noticed that the amount of GPU memory and processing time needed for a net seems to scale faster than linearly with the number of cells per layer. I *think* it's quadratic because adjacent layers are densely connected; that is, each activation output from an LSTM unit on layer L links to the inputs of *every* LSTM unit on layer L+1. Thus a GPU with more memory and processing power is going to pay pretty limited dividends in terms of scaling the horizontal size of the network.

If I've got that right, I was wondering if anyone is aware of any example code or other pointers for how to implement sparsely connected LSTM layers -- where each LSTM unit in layer L connects to (say) exactly 256 LSTM units in layer L+1 (chosen randomly upon initialization), such that the GPU memory and computer power required to add another marginal LSTM unit to each layer is (much closer to) constant.

Does anyone know if that architecture or similar has been successful, or any pointers to implementing it in Torch? I think it probably works to some extent, because I found an oblique comment by Prof. Schmidhuber somewhere to the effect that sparse LSTMs work well, and I have this (quite possibly crackpot) theory that with enough training data, the sophistication of LSTM sequence predictors could grow with their parallel breadth without limit to the point where they could generate ideas and complete arguments rather than phrases and short isolated sentences. But I don't know how to test it, and unfortunately I've had a lot of trouble trying to teach myself Torch from the ground up. Any pointers or advice would be so much appreciated!",10,12
322,2015-7-18,2015,7,18,17,3dpz0k,Black-Box Policy Search with Probabilistic Programs,https://www.reddit.com/r/MachineLearning/comments/3dpz0k/blackbox_policy_search_with_probabilistic_programs/,InaneMembrane,1437209446,,2,2
323,2015-7-18,2015,7,18,18,3dq2m5,DeepDream: Inside Google's 'Daydreaming' Computers,https://www.reddit.com/r/MachineLearning/comments/3dq2m5/deepdream_inside_googles_daydreaming_computers/,thai_tong,1437213230,,4,0
324,2015-7-18,2015,7,18,21,3dqdqr,Keras LSTM limitations,https://www.reddit.com/r/MachineLearning/comments/3dqdqr/keras_lstm_limitations/,w0nk0,1437224055,"Hi, after a 10 year break, I've recently gotten back into NNs and machine learning. Since I always liked the idea of creating bots and had toyed with Markov chains before, I was of course intrigued by karpathy's LSTM text generation.

After getting past the pybrain stage, I started using keras. I did however not have a lot of success (as opposed to pybrain, which was slow but got me results) - the stateless nature of the LSTM in keras  (refernced [here](https://groups.google.com/forum/#!topic/keras-users/Y_FG_YEkjXs) among other places) seems to make tasks with LSTM that require the network eating its own output less than elegant. (edit: 'elegant' [explained](https://www.reddit.com/r/MachineLearning/comments/3dqdqr/keras_lstm_limitations/ct7qi4r))

Since I will want to apply ML to more tasks involving generating sequences of data in the future, I'm looking for a library that will let me do that. Do you guys have any indications where i should turn my attention? I am aware of blocks, passage, lasagne- but I only found out after putting 1 day of work into learning Keras how it didn't really do my case of LSTM well. I'd like to not do that again, so maybe you guys know which of the existing frameworks does what I need well? 

I'd prefer a solution that would allow me to leverage my knowledge about the library to other ML areas as well, so something that does 'generating LSTMs' AND other current NN types well would be ideal. And of course theano and the usual state of the art stuff but I assume that's a given by now.",32,17
325,2015-7-19,2015,7,19,1,3dqxlw,Recommending Subreddits by Computing User Similarity: An Introduction to Machine Learning in Python,https://www.reddit.com/r/MachineLearning/comments/3dqxlw/recommending_subreddits_by_computing_user/,cryptoz,1437236930,,21,59
326,2015-7-19,2015,7,19,3,3dravh,Neural Network performing fitted q-iteration,https://www.reddit.com/r/MachineLearning/comments/3dravh/neural_network_performing_fitted_qiteration/,Aerospacio,1437243905,"Basically the gradient converges but the performance measurement doesn't. Does this ever happened with someone working with NN's? This causes my policies to oscillate and quickly iterating ""trash"". Is there a way to prevent this to happen? Anyone had this problem before?

Thanks in advance!",3,5
327,2015-7-19,2015,7,19,6,3drsyt,Can Intel on-board integrated GPU be used instead of CUDA for convolutional networks?,https://www.reddit.com/r/MachineLearning/comments/3drsyt/can_intel_onboard_integrated_gpu_be_used_instead/,SmArtilect,1437253331,"I'm not going to train them, I want to evolve them, so is GPU still useful? I mean for forward propagation. Name of the GPU I have: Intel(R) HD Graphics 4000",15,7
328,2015-7-19,2015,7,19,7,3ds1tp,Accelerating Deep Learning at Facebook - Keith Adams,https://www.reddit.com/r/MachineLearning/comments/3ds1tp/accelerating_deep_learning_at_facebook_keith_adams/,ojaved,1437258171,,1,12
329,2015-7-19,2015,7,19,10,3dsirg,Does anyone have more information/video of Hinton's mysterious Aetherial Symbols presentation?,https://www.reddit.com/r/MachineLearning/comments/3dsirg/does_anyone_have_more_informationvideo_of_hintons/,feedthecreed,1437268014,"Geoff Hinton gave a somewhat mysterious talk getting recurrent networks to do reasoning:

https://drive.google.com/file/d/0B8i61jl8OE3XdHRCSkV1VFNqTWc/view

Has there been any updates on this? It seemed like it would make a lot more sense if it were paired with what he was saying when he presented it.",4,37
330,2015-7-19,2015,7,19,10,3dsnsc,Tips for Data Science Competitions by Owen Zhang(Champion),https://www.reddit.com/r/MachineLearning/comments/3dsnsc/tips_for_data_science_competitions_by_owen/,john_philip,1437271166,,0,1
331,2015-7-19,2015,7,19,13,3dsz17,Variable analysis/selection - any checklists or processes - to determine which variables are of interest/influence. Particularly with mixed data types.,https://www.reddit.com/r/MachineLearning/comments/3dsz17/variable_analysisselection_any_checklists_or/,hey_chicago,1437278409,"What is a general workflow or process that one can follow when examining datasets with large numbers of variables - particularly when these variables are a broad mix between numeric/ and categorical/hierarchical categories?

For example:
 https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction 

Is a dataset with a 15ish both numeric and categorical variables that need to be used for predicting a particular score.  This might not be the best example because it obviously is a very difficult task given Liberty Mutual had to crowdsource a solution.

I'm not looking for advice on this particular project - but rather a starting point when given tons of data on how to figure out what's important.

I've used the following techniques in the past when dealing with large amounts of data to get an understanding of importance/impact/influence etc: 

* Lasso/Ridge/ElasticNet, 
* PCA, 
* Random Forests, 
* Boosting, 
* Decision Trees, 
* Correlation matrices 
* regressions

But I've never felt like I might have been doing it in an optimum manner.  So if all that made sense - and I know this depends - just looking for a step-by-step approach that is reasonable to take or at least start off with that goes beyond running a random forest and seeing what has high influence.",1,3
332,2015-7-19,2015,7,19,14,3dt7lf,A simple Genetic Algorithm (used to train neural net) in 15 lines of python,https://www.reddit.com/r/MachineLearning/comments/3dt7lf/a_simple_genetic_algorithm_used_to_train_neural/,JiKuai,1437284495,,19,66
333,2015-7-19,2015,7,19,16,3dtdq0,Awesome-RNN. A curated list of resources dedicated to recurrent neural networks (closely related to deep learning),https://www.reddit.com/r/MachineLearning/comments/3dtdq0/awesomernn_a_curated_list_of_resources_dedicated/,kjw0612,1437289884,,7,27
334,2015-7-19,2015,7,19,17,3dtjit,"A new, more accurate algorithm to train Boltzmann Machines has been developed by Microsoft",https://www.reddit.com/r/MachineLearning/comments/3dtjit/a_new_more_accurate_algorithm_to_train_boltzmann/,youngeverest,1437296038,,6,49
335,2015-7-19,2015,7,19,23,3du7xg,LOGISTIC REGRESSION USING APACHE SPARK,https://www.reddit.com/r/MachineLearning/comments/3du7xg/logistic_regression_using_apache_spark/,[deleted],1437317744,,0,1
336,2015-7-20,2015,7,20,0,3du99l,Logistic regression using Apache Spark,https://www.reddit.com/r/MachineLearning/comments/3du99l/logistic_regression_using_apache_spark/,technobium,1437318579,,0,0
337,2015-7-20,2015,7,20,2,3dumjb,An open source angular app that performs image segmentation / backgrounds removal inside browser using FabricJS and Superpixel algorithms. (Still evolving.),https://www.reddit.com/r/MachineLearning/comments/3dumjb/an_open_source_angular_app_that_performs_image/,onetimeusecv,1437326004,,0,0
338,2015-7-20,2015,7,20,4,3dv5je,How do you get better at formulating questions to ask your AI?,https://www.reddit.com/r/MachineLearning/comments/3dv5je/how_do_you_get_better_at_formulating_questions_to/,john_dumb_bear,1437335559,I've recently gotten into machine learning. It seems to me like one of the most important skills in doing effective machine learning is to come up with relevant questions about your data. How do you become better at this? Is it just a matter of domain knowledge? Or is there a more general way to get better at this type of question formulation?,6,0
339,2015-7-20,2015,7,20,5,3dv6uu,Any ideas on how to solve this,https://www.reddit.com/r/MachineLearning/comments/3dv6uu/any_ideas_on_how_to_solve_this/,just_learning_,1437336256,Does anyone know how to solve this is it similar to any other problems or is it a classic problem? http://stats.stackexchange.com/questions/162203/learning-algorithm-to-define-prediciton-region-to-maximize-score,17,0
340,2015-7-20,2015,7,20,8,3dvszl,When a MLP implemented on GPU should be faster than on CPU?,https://www.reddit.com/r/MachineLearning/comments/3dvszl/when_a_mlp_implemented_on_gpu_should_be_faster/,fariax,1437347557,"The only reason that I can imagine is if we perform a batch training, because we can forward many examples in parallel. Is there any other reason?

When I tried an MLP using Keras, not using a batch training, it took about 15s for training/test on CPU (GPU was a little worse), while using Weka, it was about 1s.",4,0
341,2015-7-20,2015,7,20,10,3dw6gw,unsupervised image segmentation and categorisation using growing neural gas (pdf),https://www.reddit.com/r/MachineLearning/comments/3dw6gw/unsupervised_image_segmentation_and/,john_philip,1437354695,,4,0
342,2015-7-20,2015,7,20,10,3dw6pc,Chainer: A flexible framework of neural networks,https://www.reddit.com/r/MachineLearning/comments/3dw6pc/chainer_a_flexible_framework_of_neural_networks/,jfsantos,1437354821,,40,73
343,2015-7-20,2015,7,20,10,3dwb50,Bn my ht bi cng nghip chnh hng gi r nht th trng,https://www.reddit.com/r/MachineLearning/comments/3dwb50/bn_my_ht_bi_cng_nghip_chnh_hng_gi_r/,snipion,1437357217,,0,1
344,2015-7-20,2015,7,20,13,3dwpn4,Finding pretrained CNNs,https://www.reddit.com/r/MachineLearning/comments/3dwpn4/finding_pretrained_cnns/,stua8992,1437365250,"I was just wondering if anyone knew if or where trained networks could be accessed. Specifically I'm looking for networks trained for emotion recognition or something similar from facial images, so that I can use their first x layers as feature extractors of sorts. 

Also sorry if this is in the wrong place. If there is a more appropriate subreddit or forum that would be much appreciated",10,1
345,2015-7-20,2015,7,20,13,3dws7q,DQN Playing Pong with Python Source Code,https://www.reddit.com/r/MachineLearning/comments/3dws7q/dqn_playing_pong_with_python_source_code/,downtownslim,1437366790,,1,20
346,2015-7-20,2015,7,20,14,3dwvxp,goto for document image classification?,https://www.reddit.com/r/MachineLearning/comments/3dwvxp/goto_for_document_image_classification/,docClassifier,1437369089,I have a large group of documents that are all the exact same. These documents are broken into pages and stored as images. I need to figure out the page number of each of the images. Is there a goto way to do this? ,1,0
347,2015-7-20,2015,7,20,16,3dx61s,What's the best way to get started with unsupervised learning on images?,https://www.reddit.com/r/MachineLearning/comments/3dx61s/whats_the_best_way_to_get_started_with/,docClassifier,1437376724,Perhaps a better worded redux of my previous question.,2,12
348,2015-7-20,2015,7,20,17,3dxbf6,Looking for paper on n-gram vs neural net language model depending on training set size,https://www.reddit.com/r/MachineLearning/comments/3dxbf6/looking_for_paper_on_ngram_vs_neural_net_language/,AlcaDotS,1437381439,"I'm looking for a paper (or presentation presenting/citing that paper). What I remember, is that for certain training corpus sizes both the performance of an n-gram model and a NNLM were plotted. It showed that n-gram perform better for smaller corpora, and NNLM could make better use of the larger training set.

I've tried looking it up on google scholar, but apparently I'm not looking for the right keywords.

Please help me find this paper :)",6,3
349,2015-7-20,2015,7,20,18,3dxfr9,Relating a matrix of binary variables with a tensor of continuous variables,https://www.reddit.com/r/MachineLearning/comments/3dxfr9/relating_a_matrix_of_binary_variables_with_a/,mrflipppy,1437385334,"I am trying to find the relationship between a matrix Y with ""n"" observations x ""p"" binary variables, and a tensor X of ""n ~ [100-500]"" observations x ""m ~ 20,000"" continuous variables x ""k ~ 50"" experiments. In the tensor, not all the ""k"" experiments have all the ""n"" observations available. However, when the observation exists in an experiment, it is guaranteed that all the ""m"" continuous variables are present. It is expected that only a small number of 'm' variables (i.e. no more than 100) are needed to explain a 'p' variable. These 'p' variables are not expected to be completely independent from each other, and the classes can be quite unbalanced in some cases. Moreover, the ""k"" experiments are very different and each one provides information from a different perspective, so I would assume that flattening this X tensor will lead to loss of information.

In particular, I am interested in constructing a classification model that can explain the 'p' variables (or groups of these variables), and that is interpretable in terms of which variables enter that model, and therefore I am avoiding black-box methods. I would like to reach conclusions of the form ""The set {p_1, ... p_a} of variables from Y can be explained by variables {m_1, ... m_b}, which are more strongly represented on experiments {k_1, ..., k_c}"".

I've been having trouble on how to approach this problem. It looks to me that if X was just a matrix I could use Canonical Correlation Analysis. I have been thinking of performing sparse multiple canonical correlation analysis; or maybe some high order singular value decomposition on the tensor (but I still do not see how to relate the subtensors to the Y matrix of variables...). Any suggestions on how to model this problem are welcome !",1,0
350,2015-7-20,2015,7,20,23,3dy3e9,Handling class imbalance - PRML,https://www.reddit.com/r/MachineLearning/comments/3dy3e9/handling_class_imbalance_prml/,bluedunnock,1437401075,"I am reading PRML. I don't seem to get the section on compensating for class priors. Page 45. Here it suggests to divide the posterior from balanced dataset by class fractions in dataset. Say my label is binomial and my real class proportions are 0.8,0.2 and the proportions in balanced dataset are 0.5, 0.5. Let's say my posterior for an unseen example based on balanced dataset is p(x). Then how I compensate for my prior, could someone please explain the calculation. TIA

Here is the link to text
http://imgur.com/mfvik4z
",3,5
351,2015-7-21,2015,7,21,0,3dybmz,Efficient experimentation and multi-armed bandits... why exploration matters,https://www.reddit.com/r/MachineLearning/comments/3dybmz/efficient_experimentation_and_multiarmed_bandits/,Bayes-Ian,1437404901,,0,22
352,2015-7-21,2015,7,21,0,3dycj7,Targeting Ultimate Accuracy: Face Recognition via Deep Embedding,https://www.reddit.com/r/MachineLearning/comments/3dycj7/targeting_ultimate_accuracy_face_recognition_via/,[deleted],1437405305,,1,1
353,2015-7-21,2015,7,21,0,3dyfv1,How robots may compensate for some weaknesses in deep learning.,https://www.reddit.com/r/MachineLearning/comments/3dyfv1/how_robots_may_compensate_for_some_weaknesses_in/,DrJosh,1437406801,,0,0
354,2015-7-21,2015,7,21,2,3dytqn,Spiking neural networks and training,https://www.reddit.com/r/MachineLearning/comments/3dytqn/spiking_neural_networks_and_training/,asymptotics,1437412658,"I've read that spiking neural networks are going to be the ""third-generation"" neural network model, but that there are difficulties with the training speed. Have there been attempts at creating deep spiking neural networks for tasks that involve a time series, or do they still take too long to train? ",5,3
355,2015-7-21,2015,7,21,2,3dyw9s,Machine learning to predict San Francisco crime,https://www.reddit.com/r/MachineLearning/comments/3dyw9s/machine_learning_to_predict_san_francisco_crime/,efavdb,1437413700,,16,26
356,2015-7-21,2015,7,21,3,3dyzz3,Complete Course on Machine Learning (Spring 2015) at CMU by Alex Smola - (Fully Tagged and Searchable),https://www.reddit.com/r/MachineLearning/comments/3dyzz3/complete_course_on_machine_learning_spring_2015/,[deleted],1437415238,,0,1
357,2015-7-21,2015,7,21,3,3dz3fl,DL architectures for Entity Recognition (and other NLP tasks)?,https://www.reddit.com/r/MachineLearning/comments/3dz3fl/dl_architectures_for_entity_recognition_and_other/,spurious_recollectio,1437416671,"I'm implementing an NLP system in python and am currently using standard tools like NLTK for entity recognition and other basic NLP tasks.  The results I'm getting are not spectacular and moreover I would like some more sophisticated features like co-reference resolution and maybe relation extraction.  To that end I'm looking for papers with an interesting architecture to solve these issues either using feedforward neural nets of RNNs/LSTMs.  My restriction to these tools is that I have nice working implementations :-)  

So I'm looking for both papers and large datasets for:

- Named entity recognition
- Coreference resolution
- Relation extraction

Using deep NNs/RNNs.

Alternatively if someone has a better (python-based) suggestion than NLTK I'd also be happy to hear about that.",24,9
358,2015-7-21,2015,7,21,4,3dzdjq,Guide To Linear Regression,https://www.reddit.com/r/MachineLearning/comments/3dzdjq/guide_to_linear_regression/,vincentg64,1437420984,,0,1
359,2015-7-21,2015,7,21,4,3dzgb4,Complete Course on Machine Learning (Spring 2015) at CMU by Alex Smola - (Fully Tagged and Searchable),https://www.reddit.com/r/MachineLearning/comments/3dzgb4/complete_course_on_machine_learning_spring_2015/,ojaved,1437422217,,11,151
360,2015-7-21,2015,7,21,5,3dzjw8,Noob question on image recognition neural networks.,https://www.reddit.com/r/MachineLearning/comments/3dzjw8/noob_question_on_image_recognition_neural_networks/,AdventureTime25,1437423701,"I'm trying to understand some of the basic concepts still, so I apologize if these are weird questions.

My understanding is that there is a lot of common processing in recognizing different types of images. I.e. low-level properties like lines and gradients are calculated essentially the same for cars, faces, etc., and neural networks for recognizing different types of images differ only in the top-most layers.
Does that mean that a neural network which classifies different images has generic low-level processing in most of the early layers and image-specific information in the last one or two layers? I.e. can you train an existing image classifier to recognize a new type of image (like, say, a parrot) by adding a new neuron on the last layer and only adjusting the weights/parameters for that neuron?

As a related question, can someone explain what models like googlenet or alexnet do? I downloaded the caffe models for both, but I'm not sure how to train them to recognize specific images. It looks like they've been trained on images already, but I'm not sure what they output or how to train them on a new type of image.",1,2
361,2015-7-21,2015,7,21,5,3dzlu7,Would it be possible to use neural networks (or other AI approach) to create a system to imitate the decisions of a person?,https://www.reddit.com/r/MachineLearning/comments/3dzlu7/would_it_be_possible_to_use_neural_networks_or/,BrassTeacup,1437424525,"It's a bit title gore, I'll give you that. So, I was thinking, would it be possible to either:

 * use messages and a person's responses (both preprocessed by POS/sentiment/concept finding) to train an ANN to create similar responses from a message
 * and/or use a questionnaire in a similar way

I think that the biggest problem would be that there's not really a linear relationship between concepts most of the time, ie there's not a halfway point between facism and ducks.

Maybe it might be possible to assign sentiment analysis results to a concept tree/map, and then further adding sentiments to actions/relations, and then looking for an 'answer' in that tree.

What do you think? It's not easy for sure, my end game would be to make a chatbot that emulates the decisions of a particular human, not really an ""AI"".",1,0
362,2015-7-21,2015,7,21,5,3dzouw,General Sequence Learning using Recurrent Neural Networks (Alec Radford),https://www.reddit.com/r/MachineLearning/comments/3dzouw/general_sequence_learning_using_recurrent_neural/,cypherx,1437425786,,5,7
363,2015-7-21,2015,7,21,6,3dzupp,Ed jaynes idea of a thinking robot,https://www.reddit.com/r/MachineLearning/comments/3dzupp/ed_jaynes_idea_of_a_thinking_robot/,classicalhumanbeing,1437428263,Did anyone build it and report back feedback on their implementation? It's described in his book Probability Theory.,2,1
364,2015-7-21,2015,7,21,7,3dzzow,Neon v0.9 released today with multi-GPU support for Maxwells; VGG can now be trained with X s/macrobatch with 8 GPUs,https://www.reddit.com/r/MachineLearning/comments/3dzzow/neon_v09_released_today_with_multigpu_support_for/,[deleted],1437430478,,0,1
365,2015-7-21,2015,7,21,7,3e02zy,Neon v0.9 released today with multi-GPU support for Maxwells; VGG can now be trained with about 8 s/macrobatch with 8 GPUs,https://www.reddit.com/r/MachineLearning/comments/3e02zy/neon_v09_released_today_with_multigpu_support_for/,jessiclr,1437431970,,8,12
366,2015-7-21,2015,7,21,8,3e07l4,"Anyone trying to reproduce the ""A Neural Conversational Model"" paper?",https://www.reddit.com/r/MachineLearning/comments/3e07l4/anyone_trying_to_reproduce_the_a_neural/,utapaua,1437434114,"I was wondering if anyone with less resources than Google managed to more or less reproduce the results of this paper and would mind to share the settings of the model and some details such as how the data was pre-processed, the wiring of the network, how sequences of different lengths were handled and so on. Thanks!",7,10
367,2015-7-21,2015,7,21,15,3e1lgz,Deepdream: Avoiding Kitsch,https://www.reddit.com/r/MachineLearning/comments/3e1lgz/deepdream_avoiding_kitsch/,ornstein-uhlenbeck,1437461104,,2,11
368,2015-7-21,2015,7,21,17,3e1tv2,"How to define goals for a Q-learning algorithm, designed to replace a PID controller?",https://www.reddit.com/r/MachineLearning/comments/3e1tv2/how_to_define_goals_for_a_qlearning_algorithm/,ptitz,1437467778,"I am currently building a Q-learning based algorithm, it's purpose it to control any general purpose, non-linear plant. The states of the plant are continuous, but the input levels have their discrete levels. Right now the goal is defined simply as *reaching* some setpoint, i.e. if I start at state s1, go to state s2 and in the meantime I pass some goal setpoint sg&lt;s2, I assume that my goal had been reached and I start over. But I'm guessing in real life this would lead to massive overshoots and the actual plant oscillating around the goal setpoint. What would be a better way to implement the goal condition, that would introduce damping in this case? ",6,0
369,2015-7-21,2015,7,21,18,3e1x0z,Universal Value Function Approximators (Deepmind can play Pacman now),https://www.reddit.com/r/MachineLearning/comments/3e1x0z/universal_value_function_approximators_deepmind/,evc123,1437470409,,1,12
370,2015-7-21,2015,7,21,20,3e24mm,Using topology to uncover the shape of your data.,https://www.reddit.com/r/MachineLearning/comments/3e24mm/using_topology_to_uncover_the_shape_of_your_data/,[deleted],1437476646,,7,9
371,2015-7-21,2015,7,21,20,3e258f,"Announcement on the CUNY and IBM's Plagiarism in an ACLpaper, Treebased Convolution for Sentence Modeling",https://www.reddit.com/r/MachineLearning/comments/3e258f/announcement_on_the_cuny_and_ibms_plagiarism_in/,name_on_water,1437477152,,5,7
372,2015-7-21,2015,7,21,21,3e2af0,This R Data Import Tutorial Is Everything You Need,https://www.reddit.com/r/MachineLearning/comments/3e2af0/this_r_data_import_tutorial_is_everything_you_need/,martijnT,1437480788,,9,0
373,2015-7-21,2015,7,21,21,3e2cjq,Flavours of Physics: Join the LHCb machine-learning contest,https://www.reddit.com/r/MachineLearning/comments/3e2cjq/flavours_of_physics_join_the_lhcb_machinelearning/,dukwon,1437482069,,24,35
374,2015-7-21,2015,7,21,23,3e2mxf,Artificial Stupidity  Patenting Intelligence,https://www.reddit.com/r/MachineLearning/comments/3e2mxf/artificial_stupidity_patenting_intelligence/,samim23,1437487809,,18,0
375,2015-7-22,2015,7,22,0,3e2zxd,Best Frameworks and Places to Start For Beginners?,https://www.reddit.com/r/MachineLearning/comments/3e2zxd/best_frameworks_and_places_to_start_for_beginners/,FR_STARMER,1437493770,"I'm coming from an iOS, Objective-C background, so I have CS experience, but this machine learning stuff is making me scratch my head. I'm going through the Standford Coursera MOOC right now, and that is providing some insight, but I'm also wondering what the most straight forward and useful way to learn machine learning is, what frameworks are out there, which languages to use, etc. Thanks!",8,1
376,2015-7-22,2015,7,22,1,3e31m8,Canonical Correlation Forests (paper and code),https://www.reddit.com/r/MachineLearning/comments/3e31m8/canonical_correlation_forests_paper_and_code/,improbabble,1437494506,,10,31
377,2015-7-22,2015,7,22,1,3e34fz,[Video] Yann LeCun at CVPR 2015: What's Wrong with Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/3e34fz/video_yann_lecun_at_cvpr_2015_whats_wrong_with/,AlcaDotS,1437495704,,1,6
378,2015-7-22,2015,7,22,2,3e3f3d,Data Science and Disney Movies,https://www.reddit.com/r/MachineLearning/comments/3e3f3d/data_science_and_disney_movies/,madisonmay,1437500028,,2,15
379,2015-7-22,2015,7,22,2,3e3hvf,Machine learning can help us optimize automatic trading strategies.. PowerPoint presentation | free to download,https://www.reddit.com/r/MachineLearning/comments/3e3hvf/machine_learning_can_help_us_optimize_automatic/,anthonybennet01,1437501189,,0,1
380,2015-7-22,2015,7,22,2,3e3hwt,How to choose a machine learning API to build predictive apps,https://www.reddit.com/r/MachineLearning/comments/3e3hwt/how_to_choose_a_machine_learning_api_to_build/,john_philip,1437501203,,0,0
381,2015-7-22,2015,7,22,3,3e3iwe,Is it too late to start a phd on deep learning?,https://www.reddit.com/r/MachineLearning/comments/3e3iwe/is_it_too_late_to_start_a_phd_on_deep_learning/,regularized,1437501608,Some senior profs around me tell that now it is too late to start a phd on deep learning since it is already saturated. What do you think?,22,4
382,2015-7-22,2015,7,22,3,3e3ltk,homeAI.info,https://www.reddit.com/r/MachineLearning/comments/3e3ltk/homeaiinfo/,homeAIinfo,1437502778,,0,0
383,2015-7-22,2015,7,22,4,3e3zib,DeepDream Animator 1.1 - incl. Optical Flow &amp; Guided Dream,https://www.reddit.com/r/MachineLearning/comments/3e3zib/deepdream_animator_11_incl_optical_flow_guided/,samim23,1437508352,,0,0
384,2015-7-22,2015,7,22,7,3e4iau,Neural networks with inner products between layer outputs?,https://www.reddit.com/r/MachineLearning/comments/3e4iau/neural_networks_with_inner_products_between_layer/,cypherx,1437516295,"I have a problem with two inputs that together are mapped onto a scalar output. One way to view the problem is that one of the sequences can be used to create parameters for a predictor that takes only the second input. 

`f(s,t) = g(h(s), t)`

Specifically it would be nice if I could make the mapping `h(s)` a deep nonlinear transformation into the same vector space as `t` and then have `g(x,y) = relu(dot(x,y))`. 

Is there a name for networks which take inner products between the outputs of two different layers (as opposed to between some layer's outputs and a set of fixed weights)? 
",4,1
385,2015-7-22,2015,7,22,8,3e4t1h,Deep Learning Online Course by NVIDIA,https://www.reddit.com/r/MachineLearning/comments/3e4t1h/deep_learning_online_course_by_nvidia/,clbam8,1437521195,,20,127
386,2015-7-22,2015,7,22,8,3e4wwm,"See a demo of how you can create a production-ready, predictive API in under an hour",https://www.reddit.com/r/MachineLearning/comments/3e4wwm/see_a_demo_of_how_you_can_create_a/,tarpus,1437523071,,0,1
387,2015-7-22,2015,7,22,9,3e52kl,"I want to do a LOT of machine learning experimenting, and my macbook air is not going to cut it. is AWS the answer or is there something even better?",https://www.reddit.com/r/MachineLearning/comments/3e52kl/i_want_to_do_a_lot_of_machine_learning/,docClassifier,1437525856,"That's pretty much it, I will cut and good options as I find them, and will investigate and report back on any tips you all can provide. TIA!",11,0
388,2015-7-22,2015,7,22,11,3e5iul,Auto-Scikit learn,https://www.reddit.com/r/MachineLearning/comments/3e5iul/autoscikit_learn/,cartin1234,1437533861,,8,26
389,2015-7-22,2015,7,22,12,3e5j94,"I want to earn a living engineering machine learning solutions as an entrepreneur, where do I start?",https://www.reddit.com/r/MachineLearning/comments/3e5j94/i_want_to_earn_a_living_engineering_machine/,moneymakerDATA,1437534048,I'm very new to machine learning but I'm sure that what I want to do for a living is applying machine learning to problems  that are of high value to people in clever and creative ways. What's the best way to do this as an independent entrepreneur?,4,0
390,2015-7-22,2015,7,22,13,3e5slb,"Good open source graphing/plotting software for time series, besides R?",https://www.reddit.com/r/MachineLearning/comments/3e5slb/good_open_source_graphingplotting_software_for/,Absay,1437539034,"I'm working on a paper on time series analysis and for illustrative purposes I need to include some graphs and plots to represent some example data. I'm especially interested in software with Dynamic Time Warping (DTW) support. R is a good option as it has a special DTW package, but for now I'm looking for other alternatives.

Bonus points if it exports the graphs generated to PDF, or even better, SVG.

Any recommendations are welcome.",1,0
391,2015-7-22,2015,7,22,13,3e5txr,Hirschberg-Manning review paper on advances in NLP,https://www.reddit.com/r/MachineLearning/comments/3e5txr/hirschbergmanning_review_paper_on_advances_in_nlp/,muktabh,1437539829,,1,5
392,2015-7-22,2015,7,22,13,3e5v0v,Introduction to Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3e5v0v/introduction_to_deep_neural_networks/,john_philip,1437540489,,1,0
393,2015-7-22,2015,7,22,14,3e5xhl,has there been significant research on using unsupervised learning to enhance/accelerate the learning process?,https://www.reddit.com/r/MachineLearning/comments/3e5xhl/has_there_been_significant_research_on_using/,docClassifier,1437541991,"for example, to help people see patterns that would otherwise not be obvious to them in the context of learning something like programming. 

Apologies if this is a stupid question, I just thought it was an interesting train of thought. I can't find anything meaningful about it online.",10,1
394,2015-7-22,2015,7,22,14,3e60ar,Recurrent neural network handwriting generation demo,https://www.reddit.com/r/MachineLearning/comments/3e60ar/recurrent_neural_network_handwriting_generation/,john_philip,1437543771,,1,16
395,2015-7-22,2015,7,22,16,3e6bha,I dont want to miss these,https://www.reddit.com/r/MachineLearning/comments/3e6bha/i_dont_want_to_miss_these/,brotherrain,1437551993,,1,0
396,2015-7-22,2015,7,22,17,3e6dp5,Future of Analytics: Big Data and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3e6dp5/future_of_analytics_big_data_and_machine_learning/,Xplenty,1437553927,,6,0
397,2015-7-22,2015,7,22,17,3e6ex3,Humming to find songs,https://www.reddit.com/r/MachineLearning/comments/3e6ex3/humming_to_find_songs/,saix47,1437555024,"I'm studying on some topics related to music. I already tried Shazam or Soundhound. I found that it's very difficult to recognize a song if I just humming using these apps (actually, Shazam doesn't allow humming but even Soundhound is not good as I expected). So I have some questions:
- is there any public papers about Shazam or Soundhound algorithm (I found an article about Shazam, but it's quite old)?
- is there any data set which can be used to do an experiment like humming or whistle to find a song (assume I have an algorithm to do this)?
",9,5
398,2015-7-23,2015,7,23,4,3e8gl2,"2014 Keynote: ""Sibyl: A System for Large Scale Machine Learning at Google""",https://www.reddit.com/r/MachineLearning/comments/3e8gl2/2014_keynote_sibyl_a_system_for_large_scale/,ojaved,1437593718,,8,34
399,2015-7-23,2015,7,23,7,3e9129,Fast Nearest Neighbor Queries in Haskell,https://www.reddit.com/r/MachineLearning/comments/3e9129/fast_nearest_neighbor_queries_in_haskell/,joehillen,1437602444,,0,8
400,2015-7-23,2015,7,23,8,3e99vo,Skip-Thought Vectors,https://www.reddit.com/r/MachineLearning/comments/3e99vo/skipthought_vectors/,evc123,1437606385,,1,9
401,2015-7-23,2015,7,23,12,3ea4dc,RNN and LSTMs - A few questions,https://www.reddit.com/r/MachineLearning/comments/3ea4dc/rnn_and_lstms_a_few_questions/,louis11,1437621830,"I've recently become interested in learning a bit about neural networks. After reading a handful of papers/books, I was able to get a simple model built to classify [SMS messages](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) as either **spam** or **ham** using [Torch7](http://torch.ch).

I've seen quite a bit of discussion on RNNs/LSTMs, and am interested in learning a bit more about how they work. My current understanding is that RNNs are well suited for sequence data. The typical example being *given this sequence of letters, what's the next most likely letter?*. 

I'm interested in reframing my toy SMS problem in the context of an LSTM, but am having trouble understanding how RNNs/LSTMs are used for classification. For instance, how do you take an SMS message: 

&gt; Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030

And convert it into a sequence? My current idea is as follows:

* RNN would take N inputs, where N is the number of characters I want to pass in at any given time.
* Chunk the SMS into several N length parts (e.g. N = 5 would result in chunks *Had y*, *ad yo*, 
*d you*, etc). That is, I would have a window of size N that I would slide down the message. 
* Each window would be passed into the RNN as input - these would serve as my sequences. 
* Whenever I get to the end of a message, if I don't have enough characters I simply fill in any empty slots with a null value.
* For each window in a message, I'd use the label for the overall message as my target.
* The network would have two outputs: One for spam, and one for ham.

I feel like a poor understanding on my part is resulting in an over complication.

What's wrong with my current idea for my RNN implementation of my toy problem?",21,4
402,2015-7-23,2015,7,23,14,3eajfn,An Empirical Exploration of Recurrent Network Architectures,https://www.reddit.com/r/MachineLearning/comments/3eajfn/an_empirical_exploration_of_recurrent_network/,evc123,1437630842,,4,11
403,2015-7-23,2015,7,23,15,3eam0v,Training Very Deep Networks,https://www.reddit.com/r/MachineLearning/comments/3eam0v/training_very_deep_networks/,xternalz,1437632662,,2,41
404,2015-7-23,2015,7,23,16,3eary2,At it again: All reddit comments from June are on torrent. Help seed!,https://www.reddit.com/r/MachineLearning/comments/3eary2/at_it_again_all_reddit_comments_from_june_are_on/,devDorito,1437637183,,0,17
405,2015-7-23,2015,7,23,17,3eaufr,Engineering neural networks: Looking for courses or learning material to get started.,https://www.reddit.com/r/MachineLearning/comments/3eaufr/engineering_neural_networks_looking_for_courses/,tomasuciu,1437639330,"Preface: I am fairly experienced in the programming domain. I've done work backend and frontend work, whether it be for personal projects or paid ventures.

For a year or so, a physics professor and I have worked together to develop a thesis centered around the mechanical application of neural networks into real-world robotics (basically, we came up with a system that reworks the dynamics of motion and utilizes a self functioning, semi-aware, neural network to command a robotic device). We've devised an intricate series of differential equations and PID-esque formulas and algorithms to back up our research.

When it comes to Neural Networks, I nearly understand all theoretical information (I've read a lot and documented myself) but am not sure how to start applying this knowledge into real world, live, programming. Was wondering if anybody could provide some sources for learning material.

EDIT: I already posted this in AskProgramming but the response was quite stale... I reckon the majority of browsing users don't find computational cognitive neuroscience intriguing...",0,0
406,2015-7-23,2015,7,23,18,3eb0nh,"New Learning Theory, Applicable to AI?",https://www.reddit.com/r/MachineLearning/comments/3eb0nh/new_learning_theory_applicable_to_ai/,henrilindroos,1437644726,,0,0
407,2015-7-23,2015,7,23,19,3eb57d,"Training done, what's next?",https://www.reddit.com/r/MachineLearning/comments/3eb57d/training_done_whats_next/,topazaz,1437648521,"I trained couple of models (RNN, CNN) using theano and keras frameworks and saved model weights as hdfs5, and the model itself as pickle file.
Obtained results are quite good to implement some software to end-user. My question is how to use trained models in ""real"" software?
For example i like to use my trained model in Android (Java) app. Is there any tutorials or examples to follow?

Thank You.",3,0
408,2015-7-23,2015,7,23,22,3eblwv,Diagonal Approximation Of Hessian,https://www.reddit.com/r/MachineLearning/comments/3eblwv/diagonal_approximation_of_hessian/,HenryJia,1437659115,"Hey guys,

I've been reading this paper http://yann.lecun.com/exdb/publis/pdf/becker-lecun-89.pdf
and the diagonal approximation section of the Machine Learning and Pattern Recog book by Bishop

the problem I have is they explain backpropagation to calculate the diagonals of the hessian but don't explain how I calculate it for the last layer which I need to backprop from.

Anyone got any ideas on how I can do that?

Also, how would I vectorise the operations there?

thanks a lot :)",3,3
409,2015-7-23,2015,7,23,23,3ebtp1,Free machine learning trainings in San Francisco!,https://www.reddit.com/r/MachineLearning/comments/3ebtp1/free_machine_learning_trainings_in_san_francisco/,deepsenseio,1437662770,,0,0
410,2015-7-23,2015,7,23,23,3ebuj4,"A Discussion of ""Deep Generative Image Models using Laplacian Pyramid of Adversarial Networks""",https://www.reddit.com/r/MachineLearning/comments/3ebuj4/a_discussion_of_deep_generative_image_models/,fhuszar,1437663147,,0,5
411,2015-7-24,2015,7,24,0,3ebwdg,Schools doing Reinforcement Learning?,https://www.reddit.com/r/MachineLearning/comments/3ebwdg/schools_doing_reinforcement_learning/,[deleted],1437664006,What are some schools that are known to have a strong reinforcement learning group?,2,0
412,2015-7-24,2015,7,24,0,3ebwjp,Was looking for RNN showcase videos and found this: RNN-made TED talks!,https://www.reddit.com/r/MachineLearning/comments/3ebwjp/was_looking_for_rnn_showcase_videos_and_found/,w0nk0,1437664088,,7,14
413,2015-7-24,2015,7,24,0,3ebxz9,The Stan Math Library: C++ reverse-mode automatic differentiation library (new BSD license),https://www.reddit.com/r/MachineLearning/comments/3ebxz9/the_stan_math_library_c_reversemode_automatic/,mttd,1437664741,,0,4
414,2015-7-24,2015,7,24,0,3ec047,Optimizing Inputs for Industrial Application,https://www.reddit.com/r/MachineLearning/comments/3ec047/optimizing_inputs_for_industrial_application/,wishzzz,1437665649,"Hi guys! I am stuck on a small problem at my internship. Any recommendation/advice would be very helpful.

I trained a MultiLayerPerceptron (weka) with inputs (let's call them a, b, c, d) with ouputs (power, time). Now, with a hard constraint (i.e. power &lt; 3000W) and soft constraint (decrease time as much as possible), I need to find inputs (a, b, c, d). 

Here is my approach: 
Using a genetic algorithm I can go through the search space (10^7) and find the inputs(a, b, c, d) that would fit my constraints.

Few Questions:
Anyone know good genetic algorithms or greedy algorithms APIs where I can set my hard and soft constraints?
Any other way of approaching this problem?",1,0
415,2015-7-24,2015,7,24,0,3ec2ix,How to Leverage Machine Learning via Predictive APIs,https://www.reddit.com/r/MachineLearning/comments/3ec2ix/how_to_leverage_machine_learning_via_predictive/,dabshitty,1437666678,,0,0
416,2015-7-24,2015,7,24,2,3ecdyf,Normalizing Stock Data,https://www.reddit.com/r/MachineLearning/comments/3ecdyf/normalizing_stock_data/,ilevakam316,1437671581,"Hello -

I have been playing around with Neural Networks. One of the test projects I wanted to do was predicting stock market fluctuations (seems like a common project). I had a question about normalizing the actual data. Does any have suggestions on what method to use? I feel like this is one of the hardest ML problems - figuring our how to represent your data in a -1 to 1 form that the NNs like.

cheers",17,0
417,2015-7-24,2015,7,24,2,3ecf9f,Top Data Scientist to Follow and Best Tutorials on GitHub (from /r/Python),https://www.reddit.com/r/MachineLearning/comments/3ecf9f/top_data_scientist_to_follow_and_best_tutorials/,john_philip,1437672149,,3,0
418,2015-7-24,2015,7,24,3,3ecldf,University of Washington: CSE 547 / STAT 548 Machine Learning for Big Data - Includes lecture notes and assignments with datasets and starter code in Java and Python,https://www.reddit.com/r/MachineLearning/comments/3ecldf/university_of_washington_cse_547_stat_548_machine/,DrLionelRaymond,1437674722,,9,77
419,2015-7-24,2015,7,24,4,3ecy6m,Chatbots and automated online assistants,https://www.reddit.com/r/MachineLearning/comments/3ecy6m/chatbots_and_automated_online_assistants/,wildcodegowrong,1437680140,,1,0
420,2015-7-24,2015,7,24,5,3ed2nu,It really bothers me that Torch is written in Lua.,https://www.reddit.com/r/MachineLearning/comments/3ed2nu/it_really_bothers_me_that_torch_is_written_in_lua/,richardfriedman9,1437682067,,10,0
421,2015-7-24,2015,7,24,5,3ed6i1,PyData Seattle 2015 Scikit-Learn Tutorial,https://www.reddit.com/r/MachineLearning/comments/3ed6i1/pydata_seattle_2015_scikitlearn_tutorial/,cast42,1437683743,,0,1
422,2015-7-24,2015,7,24,5,3ed8ln,Where to get started in machine learning?,https://www.reddit.com/r/MachineLearning/comments/3ed8ln/where_to_get_started_in_machine_learning/,sixsamuraisoldier,1437684630,"Title! I'm looking to learn machine learning to build industrial drones. The relevant background I have is:
No programming knowledge.
Finished precalculus

How would you guys recommend I get started? Thanks in advance :)",4,0
423,2015-7-24,2015,7,24,6,3edb42,RMSprop loses to SMORMS3 - Beware the Epsilon!,https://www.reddit.com/r/MachineLearning/comments/3edb42/rmsprop_loses_to_smorms3_beware_the_epsilon/,cypherx,1437685731,,12,10
424,2015-7-24,2015,7,24,6,3eddnw,XGBoost available for Python3?,https://www.reddit.com/r/MachineLearning/comments/3eddnw/xgboost_available_for_python3/,Northstat,1437686836,I'm trying to get all the packages I have on my python 2 machine on my python 3 machine. I've found the same packages in py3 versions except for XGBoost. Does this exist?,3,0
425,2015-7-24,2015,7,24,6,3edgaf,Ibis: new Python data analysis framework,https://www.reddit.com/r/MachineLearning/comments/3edgaf/ibis_new_python_data_analysis_framework/,kunjaan,1437687963,,8,17
426,2015-7-24,2015,7,24,7,3edlw2,Recommender systems survey papers?,https://www.reddit.com/r/MachineLearning/comments/3edlw2/recommender_systems_survey_papers/,wonkypedia,1437690576,What's a good primer on the state of the art in recommender systems?,5,8
427,2015-7-24,2015,7,24,8,3edwbq,methods of intelligent agents when goal isn't known,https://www.reddit.com/r/MachineLearning/comments/3edwbq/methods_of_intelligent_agents_when_goal_isnt_known/,razeal113,1437695645,"I have worked with AI before and used neural nets to train AI to accomplish goals.  However my question is this, Suppose an agent is to accomplish a task but the best method to accomplish this task isn't known.  For instance, suppose your agent is to survive in the wilderness; things such as looking for food, not being eaten, not freezing, etc would all be taken into account.

In the past , for tasks such as this, i have used randomized hill climbing ; where the cost function or grading scale is simply assigning points to different accomplishments (how long did you survive,how much food did you find, were you eaten,etc) and searching for the best agent.

but was curious if there was a more efficient method.  

",4,6
428,2015-7-24,2015,7,24,9,3ee0fs,Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms,https://www.reddit.com/r/MachineLearning/comments/3ee0fs/autoweka_combined_selection_and_hyperparameter/,rrenaud,1437697786,,0,3
429,2015-7-24,2015,7,24,11,3eeezd,Thinking about leveraging Google DeepMind software and Deep Learning to learn to play the stock market,https://www.reddit.com/r/MachineLearning/comments/3eeezd/thinking_about_leveraging_google_deepmind/,[deleted],1437705354,"I have no doubt sure some of you have seen the video of Google DeepMind learning to play Atari games. If you haven't, have a look: https://www.youtube.com/watch?v=V1eYniJ0Rnk. This leverages Deep Learning, a Convolutional neural net, and reinforcement learning to learn to play merely from pixel data and the score as input. Here are details on running this simulation on your own machine: http://superintelligence.ch/deepmind/. I've successfully run this on my machine for multiple Atari games.

I am modifying this software to treat the stock market like just another Atari game -- and learn to play it. I've analyzed the software and its dependencies, and I think I have a solid implementation strategy in mind. In short, I am creating a custom version of [alewrap](https://github.com/deepmind/alewrap) and bypassing interaction with ALE and the Stella emulator.

In more detail, instead of the neural net analyzing frames from an Atari game, I will bypass the Atari emulator and have it analyze stock charts instead, tick by tick. The charts will contain price and volume data, and each frame will contain a day chart, a week chart, and a month chart for a given stock symbol. The simulation agent will over time learn that pressing the button on the Atari joystick will trigger a ""buy"" action, and pressing again (if a position is held) will trigger a ""sell"" action. The agent will then be punished when whenever a buy followed by a sell results in profit, and it will be punished when this resulted in loss. Further, a ""game over"" will be triggered whenever account balance drops below some threshold.

In addition to price and volume data, the charts will probably include indicators (rendered into the charts), as well.

I'm wondering 1) whether you all think this will be a worthwhile endeavor or a waste of time and 2) what flaws you see (besides trying to predict the stock market), and 3) improvements you can suggest.",0,1
430,2015-7-24,2015,7,24,11,3eeg6v,Leveraging Google DeepMind software and Deep Learning to play the stock market,https://www.reddit.com/r/MachineLearning/comments/3eeg6v/leveraging_google_deepmind_software_and_deep/,chaddjohnson,1437706008,"I have no doubt some of you have seen the video of Google DeepMind learning to play Atari games. If you haven't, have a look: https://www.youtube.com/watch?v=V1eYniJ0Rnk. This leverages Deep Learning, a Convolutional neural net, and reinforcement learning to learn to play merely from pixel data and the score as input. Here are details on running this simulation on your own machine: http://superintelligence.ch/deepmind/. I've successfully run this on my machine for multiple Atari games.

I am modifying this software to treat the stock market like just another Atari game -- and learn to play it. I've analyzed the software and its dependencies, and I think I have a solid implementation strategy in mind. In short, I am creating a custom implementation of the [alewrap](https://github.com/deepmind/alewrap) interface and bypassing interaction with [ALE](http://www.arcadelearningenvironment.org/) and the Atari 2600 emulator (Stella).

In more detail, instead of the neural net analyzing frames from an Atari game, I am bypassing the Atari emulator and have it analyze stock charts instead, tick by tick. The charts will contain price and volume data, and each frame will contain a day chart, a week chart, and a month chart for a given stock symbol. The simulation agent will over time learn that pressing the button on the Atari joystick will trigger a ""buy"" action, and pressing it again (if a position is held) will trigger a ""sell"" action. The agent will then be punished when whenever a buy followed by a sell results in profit, and it will be punished when this resulted in loss. Further, a ""game over"" will be triggered whenever account balance drops below some threshold.

In addition to price and volume data, the charts will probably include indicators (rendered into the charts), as well.

I'm wondering 1) whether you all think this will be a worthwhile endeavor or a waste of time and 2) what flaws you see (besides trying to predict the stock market), and 3) improvements you can suggest.",32,0
431,2015-7-24,2015,7,24,13,3eepc3,"Using a long short-term memory deep recurrent neural network (whew!), Google Voice cut their transcription errors by 49%",https://www.reddit.com/r/MachineLearning/comments/3eepc3/using_a_long_shortterm_memory_deep_recurrent/,[deleted],1437711295,,0,1
432,2015-7-24,2015,7,24,13,3eepdo,"Using a long short-term memory deep recurrent neural network, Google Voice cut their transcription errors by 49%",https://www.reddit.com/r/MachineLearning/comments/3eepdo/using_a_long_shortterm_memory_deep_recurrent/,cybrbeast,1437711322,,23,59
433,2015-7-24,2015,7,24,13,3eer9n,#deepdreaming with Canadian Politicians and Chihuahuas,https://www.reddit.com/r/MachineLearning/comments/3eer9n/deepdreaming_with_canadian_politicians_and/,rustyoldrake,1437712465,,0,0
434,2015-7-24,2015,7,24,13,3eetok,"Googles Artificial Intelligence Speaks, and Shes a Woman",https://www.reddit.com/r/MachineLearning/comments/3eetok/googles_artificial_intelligence_speaks_and_shes_a/,john_philip,1437713988,,2,0
435,2015-7-24,2015,7,24,14,3eetu8,Books for Artificial Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3eetu8/books_for_artificial_neural_networks/,n0ty0urtypic4lguy,1437714092,Hey i have ideas about ANN. But i'm a bit confused where to focused. I'm on the verge of confusion what book to read because of the information overload about the books all over Neural Networks. Can anybody suggest me an Introductory and Advance books about ANN. ,5,0
436,2015-7-24,2015,7,24,15,3ef3qt,KJT-4D Four Heads Paper Tube Winding Machines,https://www.reddit.com/r/MachineLearning/comments/3ef3qt/kjt4d_four_heads_paper_tube_winding_machines/,sonalbisht101,1437720976,,0,1
437,2015-7-24,2015,7,24,17,3efabi,Know your paradoxes!,https://www.reddit.com/r/MachineLearning/comments/3efabi/know_your_paradoxes/,samim23,1437726460,,0,3
438,2015-7-25,2015,7,25,0,3egc52,Google Voice transcription errors reduced by 49% via long short-term memory deep recurrent neural network,https://www.reddit.com/r/MachineLearning/comments/3egc52/google_voice_transcription_errors_reduced_by_49/,[deleted],1437751196,,0,0
439,2015-7-25,2015,7,25,3,3eh0xs,Will Machine Learning be a winner-take-all industry?,https://www.reddit.com/r/MachineLearning/comments/3eh0xs/will_machine_learning_be_a_winnertakeall_industry/,docClassifier,1437761914,"I'm trying to figure out if this is an industry that will lead to a natural monopoly or oligopoly, or if there will naturally be hundreds or thousands of highly profitable companies in this area.",16,13
440,2015-7-25,2015,7,25,4,3eh9a6,Anyone trying to team up on the Diabetic Retinopathy Detection competition?,https://www.reddit.com/r/MachineLearning/comments/3eh9a6/anyone_trying_to_team_up_on_the_diabetic/,MusicIsLife1995,1437765536,"I'm shooting for the $100,000 prize, but top 10 is a given + fine by me. Anyone down?",2,0
441,2015-7-25,2015,7,25,5,3ehjgn,Microsoft's Jupyter/IPython service launched (free),https://www.reddit.com/r/MachineLearning/comments/3ehjgn/microsofts_jupyteripython_service_launched_free/,smortaz,1437769896,"Hi folks from PyData Seattle conference! Our team just launched a hosted Jupyter notebook service. Would love to get your feedback! Also - it runs on Linux/docker - and we're new to Linux, so if you find any security holes, please drop us a line at nbhelp@microsoft.com.

blog: http://aka.ms/jupyter

If you just want to try it:

http://studio.azureml.net  ; click on ""Get Started""; then +New Notebook and party on. If you want your notebooks saved, login.

Thanks in advance!",10,71
442,2015-7-25,2015,7,25,5,3ehn67,Interesting Talks on Deep Learning at the Deep Learning Summit 2015 (RE.WORK),https://www.reddit.com/r/MachineLearning/comments/3ehn67/interesting_talks_on_deep_learning_at_the_deep/,ojaved,1437771545,,0,5
443,2015-7-25,2015,7,25,6,3ehpby,[Help] Direction for higher education in CS/ML/AI,https://www.reddit.com/r/MachineLearning/comments/3ehpby/help_direction_for_higher_education_in_csmlai/,nat47,1437772497,"Machine Learning and Artificial Intelligence are the two areas of computer science that have really peaked my interests. I'm a rising senior and I haven't decided what direction I want to take my life after undergrad. I'm double majoring in computer science and philosophy. I think I would like to continue my education in CS, but I'm not sure where to start looking or how. I'm in the United States

Is going for a Masters a good idea?
What do you think is the best area to go into in ML/AI?
How do I start?
I don't have much money...

EDIT:
Personal concerns... I'm extremely creative and I'm pretty decent at programming. I have gotten through the math classes I've needed to take, but math is not my strength. I've only taken the math that has been required of me, but I've surpassed the CS classes needed for my major and I have a very good gpa.",9,0
444,2015-7-25,2015,7,25,9,3eieu0,Fastest way to get to the top of this new field??,https://www.reddit.com/r/MachineLearning/comments/3eieu0/fastest_way_to_get_to_the_top_of_this_new_field/,MusicIsLife1995,1437785061,"I'm new to Machine Learning and am half way through CS 229 http://cs229.stanford.edu/ 

Looking at all these cool breakthroughs is just awesome! I'm a Junior in college but don't want to wait until grad school/masters to get into this field, I'm starting now. Literally.. what's the fastest way for me to become knowledgeable and gain respect in this area of academia? Do I have to write research papers?",5,0
445,2015-7-25,2015,7,25,15,3ejcvs,Big Data Analytics World Championships 2015,https://www.reddit.com/r/MachineLearning/comments/3ejcvs/big_data_analytics_world_championships_2015/,Giopersico,1437806432,,0,1
446,2015-7-25,2015,7,25,17,3ejizl,Karpathy char-rnn doubt,https://www.reddit.com/r/MachineLearning/comments/3ejizl/karpathy_charrnn_doubt/,Bhavishya1,1437812082,"In karpathy's char-rnn, he mentions that he is using a 2 layer LSTM with 512 hidden nodes(refer to the Paul graham section). Does this mean that he is using a 2 layered(single neuron in each layer) LSTM with 512 hidden steps? I think I am wrong. How can this possibly form 3.5M parameters(RNNs having shared parameters)?  Please be detailed as possible.",13,0
447,2015-7-25,2015,7,25,18,3ejmbg,Are there any example models of phrase recognition I can study?,https://www.reddit.com/r/MachineLearning/comments/3ejmbg/are_there_any_example_models_of_phrase/,Cal_Short,1437815612,"Kind of like cleverbot but on a much smaller scale. I'm looking to program my own model in Ruby, but have no idea how to get started. I've bought some AI books that go through the base algorithms, but it'd be great to see examples of phrase learning.

Thanks.",0,3
448,2015-7-25,2015,7,25,19,3ejpyd,Fun times with RNNs in Torch,https://www.reddit.com/r/MachineLearning/comments/3ejpyd/fun_times_with_rnns_in_torch/,starship_falling,1437819357,"Hi everyone. Long-time lurker, first-time man this is cliche.

I've been messing around with RNNs for text generation in torch off-and-on for the last two weeks, trying to get the hang of it. Mostly I have been learning from Andrej Karpathy's char-RNN. I'm making my code public, even though it is a little crappy, because maybe it will be useful to someone! Or at the very least, fun. I'm willing to answer questions about any of the weird stuff I did.

My background: I am a rising second-year PhD student in machine learning. Still feeling out my research direction, but I am pretty much onboard the whole deep learning thing.

Here's the link:
https://github.com/jachiam/rnn-playground

Instructions for use are in the readme, and clarification is available on request. If you're wondering, yes, it /should/ work, although you may have to tweak your hyperparameters to get good performance on your particular text corpus. For example, here's a sample of text generated from Shakespeare after one epoch of training using all of the default settings except for layer_size (which here is 256; default is 128):

--[[
ere, and; and you wayhings fees.

SAABTREND:
I angen no these owee To whent' Cimane? that ther is you.

hou shall not and said thine?

on might be ill; at by my tout hem?

BASTARD:
The foumsst ne conf
--]]

Not terrible! It picks up stuff what looks like English pretty fast, as it should.

Also, if anyone has any requests for demos of things in Torch, I'm pretty curious to hear what you want to see! I'd be potentially willing to take things on as challenges.",16,33
449,2015-7-25,2015,7,25,21,3ejwx9,Interms of jobs which is the hotest sub domain of machine learning ?,https://www.reddit.com/r/MachineLearning/comments/3ejwx9/interms_of_jobs_which_is_the_hotest_sub_domain_of/,lamingo91,1437826039,Is it pure theory of machine learning like theory behind svms kernels statistics etc.. or natural language or applications to images or security or bio stuff ? ,25,0
450,2015-7-25,2015,7,25,21,3ek09e,i am number 1 jew,https://www.reddit.com/r/MachineLearning/comments/3ek09e/i_am_number_1_jew/,CarlSagan2012,1437828680,,0,0
451,2015-7-25,2015,7,25,22,3ek438,Hawking is at it again with his sci-fi bullshit about AI. Reddit AMA on Monday on /r/science.,https://www.reddit.com/r/MachineLearning/comments/3ek438/hawking_is_at_it_again_with_his_scifi_bullshit/,Chobeat,1437831541,,4,0
452,2015-7-25,2015,7,25,22,3ek4fi,The risky eclipse of statisticians (x-post /r/statistics),https://www.reddit.com/r/MachineLearning/comments/3ek4fi/the_risky_eclipse_of_statisticians_xpost/,Bayes-Ian,1437831771,,10,9
453,2015-7-25,2015,7,25,23,3ek6sl,What other fields are required for self-driving vehicles?,https://www.reddit.com/r/MachineLearning/comments/3ek6sl/what_other_fields_are_required_for_selfdriving/,JClemenceau,1437833434,"Hi there,

Sorry for the silly/noob question, I'm a french software engineer and I'd like to learn about self-driving vehicles (mostly cars).

I've started to read about machine learning, and I'm really excited! I've studied good basics of mechanics and electrics, and I'm wondering **which other fields I should study**, like maybe :

- embedded systems
- control theory
- haptics
- ...


If I'm confused it's both because:

- I lack of vocabulary (there's a lot to know about this, and I'm french, voil!)
- I'm not sure what is really useful, like maybe control theory is useless with machine learning ?

Thanks for your help :)

Edit: formatting",6,1
454,2015-7-26,2015,7,26,3,3eky3d,Classifying E-Commerce product titles,https://www.reddit.com/r/MachineLearning/comments/3eky3d/classifying_ecommerce_product_titles/,nayeeproruki,1437848610,i have see NN being used for sentiment analysis on short text like tweets. Will it work for systems which have large number of classes like classifying a product using its title? Has any one used NN on such tasks?,1,1
455,2015-7-26,2015,7,26,3,3ekyhp,The ethics of automated electronic border surveillance,https://www.reddit.com/r/MachineLearning/comments/3ekyhp/the_ethics_of_automated_electronic_border/,__null__,1437848816,"Recently I got interested in the ethics of automated electronic border surveillance. So I've been reading about systems like this one:
http://www.tibco.com/assets/blt43df6b18e641124b/predictive-perimeter-new-look-border-protection.pdf
I would like to ask your opinions about how ethical do you think that it is to use such systems in the borders. I mean that such systems rely in the same ethical area as the current laws about immigration, since they are only a technical contribution to the border infrastructure. However, although I understand the concerns for extra tight security at the borders wouldn't it be a bit too far in the ethical sense to adopt so many sources of identification in order to prevent illegal immigrants from entering ? (lets put aside the cases of human trafficking, drugs being sold illegaly at the borders...). Isn't it too cruel to consider people as smuggled in case they just simply want to come to the country to work but not having the means to do it in a legal manner ?What is more if such systems are adopted more and more wouldn't they cause illegal immigrants choose more dangerous options (for them) in order to enter the country ?
Am I just too naive ?",26,0
456,2015-7-26,2015,7,26,6,3elim5,"If you were to analyze musical pieces to create an artificial song, what algorithms would you use?",https://www.reddit.com/r/MachineLearning/comments/3elim5/if_you_were_to_analyze_musical_pieces_to_create/,mango_dingo,1437859459,,6,0
457,2015-7-26,2015,7,26,6,3eljik,Using Deep Neural Networks for Linear Regression,https://www.reddit.com/r/MachineLearning/comments/3eljik/using_deep_neural_networks_for_linear_regression/,vonnik,1437859939,,10,0
458,2015-7-26,2015,7,26,8,3elvmi,How did you get started with learning ML?,https://www.reddit.com/r/MachineLearning/comments/3elvmi/how_did_you_get_started_with_learning_ml/,Nixonite,1437866627,"Hi everyone,

Curious, which textbooks did you use when you first started? 

A major complaint (in Amazon reviews) of the textbooks listed in the FAQ is that they tend to read more like reference texts for people already familiar with the material, so I'm curious how everyone here got familiar in the first place. 

It would be helpful if you wrote something like 

textbook1 -&gt; MOOC1 -&gt; lecture notes 1 -&gt; textbook2 -&gt; textbook3 -&gt; ... ",26,4
459,2015-7-26,2015,7,26,8,3elwte,Fireside Chat with Andrew Ng &amp; Derrick Harris,https://www.reddit.com/r/MachineLearning/comments/3elwte/fireside_chat_with_andrew_ng_derrick_harris/,shad0w0bserver,1437867299,,5,4
460,2015-7-26,2015,7,26,10,3em5im,Data Mining Algorithms in Microsoft Sql Server Analysis Services,https://www.reddit.com/r/MachineLearning/comments/3em5im/data_mining_algorithms_in_microsoft_sql_server/,Platz,1437872403,,3,0
461,2015-7-26,2015,7,26,10,3em6gy,Plugging Neural Nets Into Unsupervised Learning Use Cases,https://www.reddit.com/r/MachineLearning/comments/3em6gy/plugging_neural_nets_into_unsupervised_learning/,vonnik,1437872963,,0,1
462,2015-7-26,2015,7,26,10,3emahh,In training a deep-network with an lstm with one-hot vectors for each character in a word,https://www.reddit.com/r/MachineLearning/comments/3emahh/in_training_a_deepnetwork_with_an_lstm_with/,theirfReddit,1437875379,"So I am splitting my textual input into letters and the vectorising them. I train the network one character at a time with the ideal output of the whole sequence. But I think, and it makes sense that it is, causing the training to diverge to infinity. This is because character a or [0.1] is of class 1 in word x, but is of class 4 in word y. The same input can not have two classes, right?

However, how else do I do this? The reason I am training one character at a time is so that similar words result in similar classes; for example read and reading. Another reason I am training one character at a time is because I don't know how many words there will be in a input so I have one input neuron. I input each character and the lstm then takes in the whole sequence of vectors with the same class, which is also the last character's class.

Is there a way to do this properly? Meaning train a deep-network one character at a time with the ideal output pertaining to the sequence of characters / vectors and with it converging and completing training?

Thank you in advance for the help!",14,0
463,2015-7-26,2015,7,26,14,3emuei,How far off are your projects from standard linear regression?,https://www.reddit.com/r/MachineLearning/comments/3emuei/how_far_off_are_your_projects_from_standard/,devDorito,1437888594,"If machine learning is mostly about making predictions about what something is, or will be in the future, how far off is that from basic linear regression from basic Linear Algebra?",8,0
464,2015-7-26,2015,7,26,15,3emyw7,My limited experience tells me that training on small domain-specific data similar to the test set generally performs better than training on big more general data. Where can I read more about this?,https://www.reddit.com/r/MachineLearning/comments/3emyw7/my_limited_experience_tells_me_that_training_on/,onewugtwowugs,1437892240,"I have built several classifiers within the field of natural language processing, and what I have seen so far is that there is an asterisk to the idea that more data beats more complex algorithms. 

When I have had a test set within a certain domain, let's say TED talk transcriptions, and have a pretty limited training set within the same domain, this limited training set will perform better on my test set than if I train the same classifier on a lot bigger dataset from, say, Europarl (which contains transcriptions of speeches from the European Parliament). Also, the performance seems to generally go down even if I choose to combine the training sets. Simply put, out-of-domain data have in my experience only confused to classifier rather than make it more general. I have already looked closely at my training and test sets to make sure they don't both contain identical instances, and that is not the case.

Is this your experience as well? Do you have further references of where I can read more about it?",4,0
465,2015-7-26,2015,7,26,15,3emzl5,"Are model ensembles more or less just a hack making up for poor models, or is there actually theoretical justification for their usefulness?",https://www.reddit.com/r/MachineLearning/comments/3emzl5/are_model_ensembles_more_or_less_just_a_hack/,onewugtwowugs,1437892866,"Reading up on model ensembles, I cannot seem to shake off the feeling that they in the end are nothing more than a short-cut to make up for poor modeling of machine learning classifiers. For me, there is very little theoretical excitement for using an ensemble of models, and I would much rather spend time building a single great classifier. 

I realize the actual performance of ensemble models generally exceeds single classifiers, and I was hoping that you could change my mind of why they seem to work so well. Maybe there are evidence that suggest that we humans really have a bunch of different classification methods within us that we use similarly to ensemble methods?",7,7
466,2015-7-26,2015,7,26,17,3en4sl,"NASA Hosts Quest for Quakes Data Challenge, try to predict earthquakes from EMP data, starting Tuesday. Coders will have two weeks to develop a new approach to extract the signals and identify potential earthquake precursors. Winning approaches will share a $25,000 prize.",https://www.reddit.com/r/MachineLearning/comments/3en4sl/nasa_hosts_quest_for_quakes_data_challenge_try_to/,64-17-5,1437897785,,29,74
467,2015-7-26,2015,7,26,18,3enaou,"Masters in machine learning, with little computing background. What should I learn in the next couple of months?",https://www.reddit.com/r/MachineLearning/comments/3enaou/masters_in_machine_learning_with_little_computing/,FSyed,1437904178,"So, I am due to being a master program in computational statistics and machine learning this year. I have a background in mathematics, but I don't have much programming experience. During my degree I have used maple and matlab, and I know a bit of python. 

What are something I should learn to prepare myself? Any books, or online resources I should take a look at? Thanks.",20,0
468,2015-7-26,2015,7,26,22,3enqa6,Artigo: DeepPy: Deep learning in Python  DeepPy 0.1.dev documentation,https://www.reddit.com/r/MachineLearning/comments/3enqa6/artigo_deeppy_deep_learning_in_python_deeppy/,fariax,1437918407,,16,40
469,2015-7-27,2015,7,27,0,3eo08k,"Speech-Based, Natural Language Conversational Recommender Systems",https://www.reddit.com/r/MachineLearning/comments/3eo08k/speechbased_natural_language_conversational/,alexcasalboni,1437924746,,0,3
470,2015-7-27,2015,7,27,4,3eov4p,Machine Learning: Model Selection &amp; Cross Validation,https://www.reddit.com/r/MachineLearning/comments/3eov4p/machine_learning_model_selection_cross_validation/,[deleted],1437940536,,0,1
471,2015-7-27,2015,7,27,7,3epcrz,"Chapter 6: Deep Learning by Michael Nielsen. A wonderful piece on CNNs, as well as an excellent literature reference and commentary on ML in general.",https://www.reddit.com/r/MachineLearning/comments/3epcrz/chapter_6_deep_learning_by_michael_nielsen_a/,conmdur,1437949331,,4,72
472,2015-7-27,2015,7,27,11,3eq797,Machine Learning for Dummies  Three posts to explain ML to non-scientific peoples,https://www.reddit.com/r/MachineLearning/comments/3eq797/machine_learning_for_dummies_three_posts_to/,Achoum,1437965186,"Hello Reddit ML,

Recently, during a dinner party, I have been faced with the situation of having explained what Machine Learning was to non-scientific peoples. I am sure a lot of you have already faced a similar situation :)

This got me the idea to write a couple of blog posts to explain, with simple words, with a very informal tone and with a lot of examples what was Machine Learning.

Ideally, I hope these posts can explain, to anybody, what the Classification problem is, what is kNN and what is Random Forest. 

The posts are at this address:

* [http://blog.mathieu.guillame-bert.com/2015/07/12/introduction-to-machine-learning/](http://blog.mathieu.guillame-bert.com/2015/07/12/introduction-to-machine-learning/)
* [http://blog.mathieu.guillame-bert.com/2015/07/20/machine-learning-for-dummies-part-2/](http://blog.mathieu.guillame-bert.com/2015/07/20/machine-learning-for-dummies-part-2/)
* [http://blog.mathieu.guillame-bert.com/2015/07/23/machine-learning-for-dummies-part-3/](http://blog.mathieu.guillame-bert.com/2015/07/23/machine-learning-for-dummies-part-3/)

I would be happy to hear what do you thing :)

I made some experiment of understanding on my side (notably with my girlfriend), and so far, I am happy with the result.

Cheers and thanks,
M.
",4,10
473,2015-7-27,2015,7,27,11,3eq7t7,Did DeepMind overfit in its Deep Reinforcement Learning paper?,https://www.reddit.com/r/MachineLearning/comments/3eq7t7/did_deepmind_overfit_in_its_deep_reinforcement/,pancake_ml,1437965464,"To my knowledge, a paper came from University of Alberta when they released the Arcade Learn Environment (ALE). In this paper they stated how one's training set and test set should be a disjoint collection of games. DeepMind, however, trained and reported the score from each individual ALE game. Unless DeepMind restricted the levels that a learner saw, they would have overfit - right? So, did they overfit? Even if they restricted the levels, why should I be impressed? The games are simple and generalizing, for example, to level 12 on SpaceInvader after seeing levels 1-11 doesn't seem such an accomplishment, because not much  changes.",7,4
474,2015-7-27,2015,7,27,13,3eqfnf,"Ensembling Kaggle for Charity, experiment and call to action",https://www.reddit.com/r/MachineLearning/comments/3eqfnf/ensembling_kaggle_for_charity_experiment_and_call/,rfurman,1437969899,,2,8
475,2015-7-27,2015,7,27,14,3eqq4j,Machine Learning Trick of the Day: Replica Trick,https://www.reddit.com/r/MachineLearning/comments/3eqq4j/machine_learning_trick_of_the_day_replica_trick/,iori42,1437976722,,1,7
476,2015-7-27,2015,7,27,15,3eqqbf,Machined Learnings: ICML 2015 Review,https://www.reddit.com/r/MachineLearning/comments/3eqqbf/machined_learnings_icml_2015_review/,iori42,1437976861,,1,4
477,2015-7-27,2015,7,27,16,3eqvao,Introduction to Neural Machine Translation with GPUs (part 3),https://www.reddit.com/r/MachineLearning/comments/3eqvao/introduction_to_neural_machine_translation_with/,clbam8,1437980559,,2,15
478,2015-7-27,2015,7,27,17,3er0mg,Come join me at my AI AMA !,https://www.reddit.com/r/MachineLearning/comments/3er0mg/come_join_me_at_my_ai_ama/,kokroo,1437985184,,5,0
479,2015-7-27,2015,7,27,18,3er3bl,How do you select an optimisation model?,https://www.reddit.com/r/MachineLearning/comments/3er3bl/how_do_you_select_an_optimisation_model/,k9triz,1437987674,"When once you have decided to use a certain model for an application, let's say a Recurrent Neural Network to translate from a Somali dataset or some other fancy hybrid like an RNN-augmented Convnet (like the type of thing folks at Deep Mind like), how do you go about choosing from the list of optimisation algorithms [RMSProp, Adam, Adagrad, Adadelta, Momentum, Nesterov, ....]? 

Is there a particular heuristic that suggests the best optimiser to use, or is it more a process of trial and error?",2,0
480,2015-7-27,2015,7,27,18,3er48q,Nvidia's Deep Learning Course,https://www.reddit.com/r/MachineLearning/comments/3er48q/nvidias_deep_learning_course/,_Christos,1437988500,"Train and integrate neural network-powered artificial intelligence into your applications with widely-used open source frameworks and NVIDIA software.
This course is comprised of five instructor-led classes that include interactive lectures, hands-on exercises, and office hours with the instructors.
During the hands-on exercises, you will use GPUs and deep learning software in the cloud.

Course Website: https://developer.nvidia.com/deep-learning-courses

FB Group: https://www.facebook.com/groups/NvidiaDL/ ",0,9
481,2015-7-27,2015,7,27,21,3eriyg,"The Brain vs Deep Learning Part I: Computational Complexity  Or Why the Singularity Is Nowhere Near ~""A biological neuron is essentially a small convolutional neural network.""",https://www.reddit.com/r/MachineLearning/comments/3eriyg/the_brain_vs_deep_learning_part_i_computational/,robertsdionne,1438000129,,82,110
482,2015-7-27,2015,7,27,22,3ernph,How is your production environment structured?,https://www.reddit.com/r/MachineLearning/comments/3ernph/how_is_your_production_environment_structured/,Lance_Henry1,1438002901,"We're just in the beginning stages of our planning and I'm trying to get a feel of how others have put together their production environment...servers and software and redundancy to make it work for ML. 

Highly likely for us we be starting out with batch processing and having predictions reside in a BI database server, so we will port our Python development code to a production server and have it run there (in this case, a ""production server"" would be something with a little more attention to backups and redundancy, maybe virtualized so the sysadmin crew can more easily allocate resources or move it around on hosts if needed, locked down from a deployment aspects, etc). 

Not sure if we would need anything beyond crontab for job scheduling...we might as processes become bigger or involved (multi-step) or need better redundancy. 

We'll be using Git for source control and deployment.

Haven't settled on anything from a visualization standpoint yet. 

If/when we have anything closer to an app or a management more real-time predictive capabilities, of course we'll be looking at those particular aspects of a system (and language) to manage that more effectively.

Any recommendations or resources you've used?



",1,3
483,2015-7-27,2015,7,27,22,3erpkp,Help your city! Use machine learning to prevent food poisoning,https://www.reddit.com/r/MachineLearning/comments/3erpkp/help_your_city_use_machine_learning_to_prevent/,dataforgood,1438003958,"We curated a list of 10 reusable data-driven projects for the greater good. One of the projects uses machine learning and open data to prevent food poisoning. Find everything to get you started [here](http://blog.datalook.io/openimpact) (look for ""Food Inspection Forecasting"").",0,0
484,2015-7-28,2015,7,28,0,3es0gs,"Musk, Wozniak and Hawking urge ban on AI and autonomous weapons: Over 1,000 high-profile artificial intelligence experts and leading researchers have signed an open letter warning of a military artificial intelligence arms race and calling for a ban on offensive autonomous weapons.",https://www.reddit.com/r/MachineLearning/comments/3es0gs/musk_wozniak_and_hawking_urge_ban_on_ai_and/,Lost4468,1438009308,,13,26
485,2015-7-28,2015,7,28,1,3esfpq,Amazon Machine Learning (course),https://www.reddit.com/r/MachineLearning/comments/3esfpq/amazon_machine_learning_course/,alexcasalboni,1438015900,,0,1
486,2015-7-28,2015,7,28,2,3esint,cnmem: A simple memory manager for CUDA designed to help Deep Learning frameworks manage memory,https://www.reddit.com/r/MachineLearning/comments/3esint/cnmem_a_simple_memory_manager_for_cuda_designed/,glassackwards,1438017135,,0,10
487,2015-7-28,2015,7,28,2,3esmgz,Parametric and non-parametric learning algorithms,https://www.reddit.com/r/MachineLearning/comments/3esmgz/parametric_and_nonparametric_learning_algorithms/,augustus2010,1438018687,I have a hard time to differentiate between parametric and non-parametric learning algorithms. This guy even said Naive Bayes is either parametric or non-parametric: http://pages.cs.wisc.edu/~jerryzhu/cs761/stat.pdf Could anybody help me with that? It's useful if you can give a few examples for each type of learning. ,11,6
488,2015-7-28,2015,7,28,3,3esvs4,Keeping up with the Vanilla CNNs for Beginners in CNN Research - Theano,https://www.reddit.com/r/MachineLearning/comments/3esvs4/keeping_up_with_the_vanilla_cnns_for_beginners_in/,[deleted],1438022435,,0,1
489,2015-7-28,2015,7,28,5,3etb1i,Can you help me improve reddit recommendation system?,https://www.reddit.com/r/MachineLearning/comments/3etb1i/can_you_help_me_improve_reddit_recommendation/,anvaka,1438028618,"Hello friends.

I'm Machine Learning amateur, and I've [made a system](http://anvaka.github.io/redsim/#?q=MachineLearning), which computes jaccard similarity based on shared users between two subreddits.

It works really well for subreddits with less than 1,000,000 users, but then results are getting saturated by popularity.

I was thinking about manually entering related subreddits for popular monsters like /r/pics  or /r/IAmA but was wondering maybe there are better ways to improve it?

Thank you!",4,2
490,2015-7-28,2015,7,28,8,3eu2rv,A Visual Introduction to Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3eu2rv/a_visual_introduction_to_machine_learning/,dabshitty,1438041065,,23,323
491,2015-7-28,2015,7,28,9,3eu76m,Label Accuracy,https://www.reddit.com/r/MachineLearning/comments/3eu76m/label_accuracy/,jrkirby,1438043244,"For calculating the performance of a learning algorithm, we usually assume the labels in our test set are all 100% correct. We then go on to measure the performance of our algorithm by making an ROC curve of false positives verses false negatives, or just state the percentage correct.

However, this is not necessarily correct to assume that all the labels in the test set are correct. If the test set isn't vetted well enough, it's reasonable to assume some small percentage of the labels were assigned by mistake. And even further, on some problems, human performance on the test set is not even close to 100%. If people can't even be correct close to all the time, it seems foolish to assume we've got all the labels in the test set correct.

Also, for a given example, maybe there isn't a label in the dictionary of labels that accurately corresponds to the example. What if (for example) somehow someone drew a smiley and it slipped into a dataset like MNIST on accident? In that case no label would be a correct label.

So assuming we don't have a perfectly labeled test set, how can we mathematically adjust our performance calculations or interpret them to account for that? If we had a test set with 5% wrong labels (maybe that's extreme, but imagine it), there's probably not really any difference in the performance of an algorithm that got 97% and one that got 99.5%. What are good ways of figuring out what the true accuracy of our labels are? Human evaluation? Multiple human evaluations? If we find incorrect labels in our test set, should we remove the examples and test again?

Of course all these questions probably depend on what problems and datasets your have, how they were constructed, how things are being evaluated... etc. But I'm looking for other people's opinions and anecdotes for what they've done, and other people's thought process on the matter.",1,4
492,2015-7-28,2015,7,28,11,3eulde,A neural network in 13 lines of python (Part 2 - An Intuitive Tutorial of Stochastic Gradient Descent),https://www.reddit.com/r/MachineLearning/comments/3eulde/a_neural_network_in_13_lines_of_python_part_2_an/,iamtrask,1438050129,,12,18
493,2015-7-28,2015,7,28,12,3eurqn,what's the best system/algorithm for learning machine learning from the internet?,https://www.reddit.com/r/MachineLearning/comments/3eurqn/whats_the_best_systemalgorithm_for_learning/,docClassifier,1438053320,That's pretty much it. I'm wondering if anyone has designed a systematic way to learn and implement machine learning techniques using only freely available online resources.,3,0
494,2015-7-28,2015,7,28,13,3ev1qu,My 1st Kaggle ConvNet: Getting to 3rd Percentile in 3 months,https://www.reddit.com/r/MachineLearning/comments/3ev1qu/my_1st_kaggle_convnet_getting_to_3rd_percentile/,ka-cirt-bu-bo-fo,1438058775,,26,53
495,2015-7-28,2015,7,28,14,3ev3sz,Books or Websites in Java?,https://www.reddit.com/r/MachineLearning/comments/3ev3sz/books_or_websites_in_java/,dauntless26,1438059986,Any thorough books or websites that explain how to create and implement Neural Networks in Java? Something that goes beyond the simple AND and XOR examples.,1,0
496,2015-7-28,2015,7,28,14,3ev4gq,Thoughts on Graph Processing Systems?,https://www.reddit.com/r/MachineLearning/comments/3ev4gq/thoughts_on_graph_processing_systems/,taiboku,1438060405,"Hi /r/machinelearning 

I'm working on a project that involves analyzing very large graphs.  It seems like there are a bunch of options, and I'm curious what people think of them.  i.e. do you use any of these systems below? If so, what do you like about it? 

* Neo4j, Titan, etc [1] 
* Spark/GraphX
* Giraph [Pregel]
* Graphlab [Now Dato?]
* Others? 

[1] I'm mostly interested in graph *analytics* use-cases, but I concede that some folks are using Neo4j et al for analytics purposes. ",2,0
497,2015-7-28,2015,7,28,14,3ev7sf,Detecting diabetic retinopathy in eye images with ConvNets,https://www.reddit.com/r/MachineLearning/comments/3ev7sf/detecting_diabetic_retinopathy_in_eye_images_with/,iori42,1438062622,,3,18
498,2015-7-28,2015,7,28,15,3ev9p0,The Geomblog: Racism/sexism in algorithms,https://www.reddit.com/r/MachineLearning/comments/3ev9p0/the_geomblog_racismsexism_in_algorithms/,quattro,1438063929,,5,2
499,2015-7-28,2015,7,28,16,3evhvh,Combining Genetic Algorithms and Recommendation Systems: Some thoughts,https://www.reddit.com/r/MachineLearning/comments/3evhvh/combining_genetic_algorithms_and_recommendation/,sachinrjoglekar,1438070130,,0,0
500,2015-7-28,2015,7,28,17,3evlvv,Powder blending machine operation video,https://www.reddit.com/r/MachineLearning/comments/3evlvv/powder_blending_machine_operation_video/,Delly789,1438073590,,0,0
501,2015-7-28,2015,7,28,20,3evvvs,EMG manufacturer,https://www.reddit.com/r/MachineLearning/comments/3evvvs/emg_manufacturer/,paulrobinson00,1438081655,,0,0
502,2015-7-28,2015,7,28,22,3ew74t,GitXiv.com - Collaborative Open Computer Science.,https://www.reddit.com/r/MachineLearning/comments/3ew74t/gitxivcom_collaborative_open_computer_science/,samim23,1438088760,,18,27
503,2015-7-28,2015,7,28,22,3ew9gc,Guest blogger invite for those with expertise and interest in Machine Learning/Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3ew9gc/guest_blogger_invite_for_those_with_expertise_and/,reworksophie,1438089957,"We've had interviews with scientists and experts from Google, MetaMind, University of Toronto, Affective, Emotient &amp; more. We're interested in featuring some guest blogs to give more varied outlooks on deep learning and related fields. 

The Deep Learning Summit is a sell out event and brings a lot of traffic to our site, from a massive array of people from students to startups to large corporations like Apple, Google, IBM and more to the site - so this is a great opp to share your ideas and expertise with a large volume of readers interested in deep learning and other rapidly emerging technologies.

Check out the blog here: http://re-work.co/blog

If you're interested in writing a blog for us, please contact hello@re-work.co or myself at scurtis@re-work.co 

Thanks! Hope some of you want to get involved!

As a note: you do not have to write on ML or DL. Our first guest blog was on IoT and open data and we're interested in your views on all new tech, so feel free to propose whatever you'd like to write about!",0,2
504,2015-7-28,2015,7,28,22,3ew9jb,Demand Forecasting using Machine Learning for Working Capital Reduction,https://www.reddit.com/r/MachineLearning/comments/3ew9jb/demand_forecasting_using_machine_learning_for/,[deleted],1438090001,,3,0
505,2015-7-28,2015,7,28,23,3ewhub,TechCrunch - Next wave of enterprise software will be powered by ML,https://www.reddit.com/r/MachineLearning/comments/3ewhub/techcrunch_next_wave_of_enterprise_software_will/,mtnchkn,1438093928,,0,2
506,2015-7-29,2015,7,29,1,3ex0ps,Best type of input data for training a Neural Network?,https://www.reddit.com/r/MachineLearning/comments/3ex0ps/best_type_of_input_data_for_training_a_neural/,nat47,1438101831,"I've heard some mixed opinions about the type of data that should be used to train a neural network. 

For instance, using a Neural net for classification... I've heard that having lots of 0s in the training data is bad and won't give good results, compared to much more 'complex' input data. I've also heard that 1-hot vectors work well.  So, I think that I've been hearing some conflicting opinions. Is there any research/scholarship on this topic?

I'm about to run some tests for myself on the tagging problem I've been working on using more 'complex' inputs, and other more simple inputs.",3,1
507,2015-7-29,2015,7,29,3,3excft,/r/DeepLearningPapers,https://www.reddit.com/r/MachineLearning/comments/3excft/rdeeplearningpapers/,knighton_,1438106572,,3,48
508,2015-7-29,2015,7,29,3,3exckj,"Girshick's fast r-cnn results, and now his faster ones? A few questions.",https://www.reddit.com/r/MachineLearning/comments/3exckj/girshicks_fast_rcnn_results_and_now_his_faster/,NASM8bit,1438106621,"In case you missed it:

fast-rcnn - http://arxiv.org/abs/1504.08083

faster-rcnn: http://arxiv.org/abs/1506.01497

The fast r-cnn code was released pretty quickly, but was an absolute nightmare to make a product out of. After some development I was finally able to train fast r-cnn code on my own datasets, and sure enough it is super fast and pretty accurate. My advice is unless you are a masterful python-ist (and even if you are), set aside some time to really understand and look at his code. In many places it's just bad and you'll have to rewrite it to work with it for your own datasets. All that said, great idea, and great algorithm.

faster r-cnn just says ""The code will be released"" in the abstract. Instead of waiting for another repo that may also be cryptic and obfuscated, I was just gonna start coding it up myself from my initial edits of fast r-cnn so I can keep the papers flowing.

A questions about it I'd love to hear peoples input on though:

1. Did you find Girshick's code hard to mutate so you could use it or was it just me?

1. At what point do we need to say that test speed isn't worth it for all the extra training? Fast r-cnn only has to be trained once, but the procedure for the faster r-cnn looks like you need to train/finetune 4 different nets? (1- Finetune an imagenet model for objectntness with anchor points on the new dataset, 2- Train a fast r-cnn on the new dataset using the regions proposed by (1), 3- Use the conv layers from (2) on the first model and fix the conv layers, to finetune the fully connected layers, and 4- Finetune the fc layers of (2) with the regions proposed in (3)). After training 4 networks, they can use 300 regions, and go end to end detection in about .2 seconds. In an upcoming paper (probably arxiv), I have found that you can do end to end detection in &lt; 1 second for around 2250 regions).

1. Does anybody know when the next caffe release candidate is set to come? I used to stay very up to date on caffe, but I've been slacking since march.",5,3
509,2015-7-29,2015,7,29,4,3exoa3,What Happens When Artificial Intelligence Makes MAGIC: THE GATHERING Cards,https://www.reddit.com/r/MachineLearning/comments/3exoa3/what_happens_when_artificial_intelligence_makes/,julian88888888,1438111273,,4,2
510,2015-7-29,2015,7,29,5,3exym6,Question about PyMC3 and possibly Theano,https://www.reddit.com/r/MachineLearning/comments/3exym6/question_about_pymc3_and_possibly_theano/,Professional_123,1438115482,"Hello,

All of the examples and tutorials of PyMC3 with linear regression only consists of a couple of predictors, so they sum the matrix product X*b element by element [i.e. X(1)*b(1) + X(2)*b(2)]. What if I have many predictors and I have to take the dot product instead? How can I do it in PyMC3? I tried to figure it out the whole day and I haven't found a solution yet :(

EDIT: Ithink I can do it with dot(X,b[i,:].transpose())
But getting memory errors. Now I have to find a way to do batch sums on PyMC3....",3,1
511,2015-7-29,2015,7,29,5,3ey0is,"Slides from the ""Introduction to Deep Learning"" by Nvidia",https://www.reddit.com/r/MachineLearning/comments/3ey0is/slides_from_the_introduction_to_deep_learning_by/,rasbt,1438116243,,0,12
512,2015-7-29,2015,7,29,6,3ey5fv,New Free Machine Learning Salon Starter Kit (PDF&amp;EPUB),https://www.reddit.com/r/MachineLearning/comments/3ey5fv/new_free_machine_learning_salon_starter_kit/,[deleted],1438118262,,0,1
513,2015-7-29,2015,7,29,7,3eye86,Is theano the best tool of its kind?,https://www.reddit.com/r/MachineLearning/comments/3eye86/is_theano_the_best_tool_of_its_kind/,docClassifier,1438122022,,4,0
514,2015-7-29,2015,7,29,9,3eyvqh,Web-based tool for gathering segmentations of images,https://www.reddit.com/r/MachineLearning/comments/3eyvqh/webbased_tool_for_gathering_segmentations_of/,AlexRothberg,1438129960,"I am looking for a web-based tool that I can use to crowdsource segmentations of images. For example the tool would show an image of a house against some background and ask the user to draw a polygon border of the house. I would like the tool to be web based so that I can be easily ""deployed"".

Does anyone know know of good open source tools like this?

I found these but they do not seem to be actively maintained:

 * http://ttic.uchicago.edu/~smaji/projects/mturk/
 * http://opensurfaces.cs.cornell.edu/docs/index.html",2,1
515,2015-7-29,2015,7,29,10,3ez38x,Jurgen Schmidhuber launches AI startup,https://www.reddit.com/r/MachineLearning/comments/3ez38x/jurgen_schmidhuber_launches_ai_startup/,SuperFX,1438133649,,58,57
516,2015-7-29,2015,7,29,10,3ez3sq,The guy who taught AI to remember is launching a startup,https://www.reddit.com/r/MachineLearning/comments/3ez3sq/the_guy_who_taught_ai_to_remember_is_launching_a/,SuperFX,1438133909,,2,0
517,2015-7-29,2015,7,29,11,3ez96s,CIKM Machine Learning Competition 2015,https://www.reddit.com/r/MachineLearning/comments/3ez96s/cikm_machine_learning_competition_2015/,jamesbailey15,1438136513,,0,3
518,2015-7-29,2015,7,29,13,3ezp1b,What is the difference between deep learning and machine learning?,https://www.reddit.com/r/MachineLearning/comments/3ezp1b/what_is_the_difference_between_deep_learning_and/,lukacosicnz,1438144821,,7,3
519,2015-7-29,2015,7,29,16,3f03yp,"Arxiv: ""Training recurrent networks online without backtracking""",https://www.reddit.com/r/MachineLearning/comments/3f03yp/arxiv_training_recurrent_networks_online_without/,InfinityCoffee,1438154969,,5,13
520,2015-7-29,2015,7,29,18,3f0ecs,taking decisions: how to combine scores from different systems?,https://www.reddit.com/r/MachineLearning/comments/3f0ecs/taking_decisions_how_to_combine_scores_from/,fromtheoffice,1438163408,"Hi,

I have several completely different software components that are producing scores for certain candidate solutions.

Now, setting aside the technical details of what is the problem that I want to solve and how the several software components (riddled with complex heuristics) are obtaining the scores, I would like to know how to combine these (potentially 0..1 normalized) scores.

The goal is to have one final summarizing score, the candidate solution that get the maximum score will be chosen.

One possibility is averaging, another might be multiplying, but I wonder: aren't there any smarter techniques?

The scores might be interpreted as probabilities if they are normalized in the interval [0,1].

Any pointers to literature? Any suggestions/math formulas?

Thanks!",2,0
521,2015-7-29,2015,7,29,21,3f0oza,Frameworks and Libraries for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3f0oza/frameworks_and_libraries_for_deep_learning/,CreativePunch,1438171314,,5,0
522,2015-7-29,2015,7,29,21,3f0t3v,Cross Validation done wrong,https://www.reddit.com/r/MachineLearning/comments/3f0t3v/cross_validation_done_wrong/,mottalrd,1438173695,,12,50
523,2015-7-29,2015,7,29,22,3f0xib,I am just a regular mortal interested in business applications of ML/AI etc. Do you know of a non-technical (or minimally technical) resource(s) that gives a broad overview of use cases and problem solving potential? Thanks,https://www.reddit.com/r/MachineLearning/comments/3f0xib/i_am_just_a_regular_mortal_interested_in_business/,[deleted],1438176127,,7,4
524,2015-7-29,2015,7,29,22,3f119z,Best resources to start with pattern recognition for a beginner in ML?,https://www.reddit.com/r/MachineLearning/comments/3f119z/best_resources_to_start_with_pattern_recognition/,thenamesalreadytaken,1438178033,"I have started with ML only a few weeks ago, heard this sub is good for beginners. I'm planning to learn pattern recognition, to be specific, make a program learn from a data set that has 0-9 in  handwritten form and then decide by itself. I know this is a super noob question but would be glad if someone helps with a guideline for what to do to achieve my goal. I have a couple of months to learn the basics of ML, currently following the course from Udacity. 

Thanks! ",6,1
525,2015-7-29,2015,7,29,23,3f14ei,The artificial intelligence advert that writes itself,https://www.reddit.com/r/MachineLearning/comments/3f14ei/the_artificial_intelligence_advert_that_writes/,PatternInc,1438179471,,0,0
526,2015-7-30,2015,7,30,0,3f1fre,Hinton's Dropout in 3 Lines of Python,https://www.reddit.com/r/MachineLearning/comments/3f1fre/hintons_dropout_in_3_lines_of_python/,iamtrask,1438184283,,15,0
527,2015-7-30,2015,7,30,1,3f1k7a,Machine Learning practice using R and k-means on your own twitter feed,https://www.reddit.com/r/MachineLearning/comments/3f1k7a/machine_learning_practice_using_r_and_kmeans_on/,moswein,1438186089,,0,1
528,2015-7-30,2015,7,30,1,3f1mky,How Google Translate squeezes deep learning onto a phone,https://www.reddit.com/r/MachineLearning/comments/3f1mky/how_google_translate_squeezes_deep_learning_onto/,vkhuc,1438187050,,21,87
529,2015-7-30,2015,7,30,1,3f1mra,"Zero knowledge about machine learning, how to?",https://www.reddit.com/r/MachineLearning/comments/3f1mra/zero_knowledge_about_machine_learning_how_to/,JakeTheMaster,1438187114,"I have heard there is an open source project called [opencog](http://wiki.opencog.org/w/Cookbook). But I have zero knowledge about opencog. 

I watched the [youtube video](https://www.youtube.com/watch?v=cVBtCxHW0pU), and was so excited about that.

Where to start? What preparation?
How to program in opencog? Python preferred. Thanks!",3,0
530,2015-7-30,2015,7,30,1,3f1rpn,Using machine learning to cluster and predict locations from Sony Lifelog API,https://www.reddit.com/r/MachineLearning/comments/3f1rpn/using_machine_learning_to_cluster_and_predict/,cast42,1438189081,,0,0
531,2015-7-30,2015,7,30,2,3f1xgu,How Google Translate squeezes deep learning onto a phone,https://www.reddit.com/r/MachineLearning/comments/3f1xgu/how_google_translate_squeezes_deep_learning_onto/,anantzoid,1438191316,,0,1
532,2015-7-30,2015,7,30,3,3f237j,How to train a multi-label classification RNN?,https://www.reddit.com/r/MachineLearning/comments/3f237j/how_to_train_a_multilabel_classification_rnn/,wesolyromek,1438193474,"I'm currently implementing an RNN to do some multi-label classification of time sequences. There are 8 classes corresponding to specific events. There are 0-3 events happening at a time point. I'm using an LSTM network with eight output nodes with pointwise sigmoid applied to them and the Binary Cross Entropy criterion as a loss function.

I've just discovered that because the labels are mostly sparse (0-1 events happening 90% of the time) what the training does is push all the activations to 0 regardless of the input.
Are there any techniques/encodings/loss functions that could help with such problem?

Thanks!",7,2
533,2015-7-30,2015,7,30,4,3f2i6m,Visualizing GoogLeNet classes,https://www.reddit.com/r/MachineLearning/comments/3f2i6m/visualizing_googlenet_classes/,matsiyatzy,1438199195,,15,23
534,2015-7-30,2015,7,30,5,3f2o7t,Geoff Hinton talk at Cambidge on 25 June 2015 - overview and latest results,https://www.reddit.com/r/MachineLearning/comments/3f2o7t/geoff_hinton_talk_at_cambidge_on_25_june_2015/,ford_beeblebrox,1438201581,,15,35
535,2015-7-30,2015,7,30,6,3f2whn,Looking for a dataset to train your data on? Try UCI's Machine Learning database.,https://www.reddit.com/r/MachineLearning/comments/3f2whn/looking_for_a_dataset_to_train_your_data_on_try/,devDorito,1438204867,,1,0
536,2015-7-30,2015,7,30,6,3f2xbw,"In LSTM-Language Modelling, How do you handle dimensionality problem at training?",https://www.reddit.com/r/MachineLearning/comments/3f2xbw/in_lstmlanguage_modelling_how_do_you_handle/,yhg0112,1438205219,"well, i'm newb to LSTM-RNN and language model and trying to do some tutorial experiments based on [Sutskever, Ilya, Oriol Vinyals, and Quoc VV Le. ""Sequence to sequence learning with neural networks."" Advances in neural information processing systems. 2014.].

I got the idea of handling non-fixed dimensionality when generating || predicting step. 

However in training step, how can i handle that problem? 

i.e. if i got the non-fixed dimensioned sentences in english like 
""A B C""
""A B C D E F""
...
how can i put those sentences in a same LSTM-model when training the model?",9,3
537,2015-7-30,2015,7,30,11,3f43lo,"A new group has formed that wants you to be nice to your algorithms, PETRL - People for the Ethical Treatment of Reinforcement Learners",https://www.reddit.com/r/MachineLearning/comments/3f43lo/a_new_group_has_formed_that_wants_you_to_be_nice/,cwolveswithitchynuts,1438224782,,25,31
538,2015-7-30,2015,7,30,12,3f460a,Machine learning mailing lists,https://www.reddit.com/r/MachineLearning/comments/3f460a/machine_learning_mailing_lists/,RinzeWind,1438225959,"While I enjoy web forums, I am old-fashioned and love mailing lists for discussiong different topics. I have been looking for some ML-oriented mailing list but, apart from [ML-news](http://groups.google.com/group/ML-news), which is basically an announcements list, I've found nothing.

So, I ask: do you know any good ML / data analysis mailing lists?

I've done a bit of research before posting this and I've found [this old thread from 5 years ago](https://www.reddit.com/r/MachineLearning/comments/bvb6z/ask_ml_what_other_communities_forums_mailing/). As I would expect the community to be much bigger now than then, I guess this might attract a bit more attention now.",7,15
539,2015-7-30,2015,7,30,13,3f4eoo,What is the best ML framework/platform/library for image classification without a CUDA dependency?,https://www.reddit.com/r/MachineLearning/comments/3f4eoo/what_is_the_best_ml_frameworkplatformlibrary_for/,foxh8er,1438230442,,7,3
540,2015-7-30,2015,7,30,14,3f4ngj,Buy and Sell Excess Inventory at EzyTrader,https://www.reddit.com/r/MachineLearning/comments/3f4ngj/buy_and_sell_excess_inventory_at_ezytrader/,ezytrader,1438235745,,0,1
541,2015-7-30,2015,7,30,14,3f4ni8,Heres What Inspired Top Minds in Artificial Intelligence to Get Into the Field,https://www.reddit.com/r/MachineLearning/comments/3f4ni8/heres_what_inspired_top_minds_in_artificial/,iori42,1438235777,,6,0
542,2015-7-30,2015,7,30,17,3f50sq,Select and visualise important variables for classification in Python,https://www.reddit.com/r/MachineLearning/comments/3f50sq/select_and_visualise_important_variables_for/,cast42,1438245773,,0,0
543,2015-7-30,2015,7,30,18,3f541j,Machine Tools Accessories Manufacturers,https://www.reddit.com/r/MachineLearning/comments/3f541j/machine_tools_accessories_manufacturers/,Md-Malik,1438248487,,0,1
544,2015-7-30,2015,7,30,18,3f54th,Probabilistic Programming in Python using PyMC,https://www.reddit.com/r/MachineLearning/comments/3f54th/probabilistic_programming_in_python_using_pymc/,Kiudee,1438249180,,2,43
545,2015-7-30,2015,7,30,21,3f5h5p,How to optimise a black-box objective function containing undefined points?,https://www.reddit.com/r/MachineLearning/comments/3f5h5p/how_to_optimise_a_blackbox_objective_function/,slamdesu,1438258352,"Hi, I'm currently using bayesian optimisation to try to minimise the prediction error of a high-dimensional black-box process. 

The model is a black-box because for any particular parameter set, I'm running simulations of neural circuitry (with noise), and I'm then applying a predictive model on the output of the simulation to measure a latent feature in the artificial data. 

The error of this black-box is then the discrepancy between the estimated latent feature from the simulated data (generated via a candidate parameter set), and then a target which I can define manually. Thus my main aim is to identify the parameter sets which generate the latent feature which match my target as closely as possible.

The main issue with this is that certain values of parameters produce simulations from which it is not possible to estimate that latent feature I mentioned (it requires inverting a matrix). As a result, the prediction error is undefined at that parameter value. Consequently the prediction error function has points that are undefined.

My question is, are there any sensible approaches to dealing with this situation? Currently I just set the error to be artificially large (say, 10000) and hope that the optimisation algorithm learns to ignore those parts of the parameter space. However I intuitively feel that the value of this artificial error could make a huge difference, and I've no idea what kind of value would be sensible.

I would appreciate any help on this. Thank you very much",3,1
546,2015-7-30,2015,7,30,21,3f5j4s,How to tune Brown clustering,https://www.reddit.com/r/MachineLearning/comments/3f5j4s/how_to_tune_brown_clustering/,leondz,1438259464,,0,5
547,2015-7-30,2015,7,30,21,3f5l6y,Number of parameters in an RNN and using GTX 980 to train them,https://www.reddit.com/r/MachineLearning/comments/3f5l6y/number_of_parameters_in_an_rnn_and_using_gtx_980/,__AndrewB__,1438260664,"Hey, 

I was wondering how to compute exact memory requirements for a given RNN, depending on:

- number of hidden units per layer 
- number of layers 
- layer type (GRU has less params than LSTM)
- sequence length and batch size
- Input and output space dimensionality
Also: should we count every parameter twice to account for the computed gradient? I'm planning to use Theano for that.

I would also like to ask how big these models usually are in applications like NLP (machine translation, sentiment analysis, lanugage models...), Comp.Vision (like DRAW) and others (like Neural Turing Machines) and if I will be able to build good models using only a 4GB graphics card (I can't afford anything more expensive).

Best, 
A.

",3,0
548,2015-7-30,2015,7,30,23,3f5zqo,Defeating Facebooks DeepFace with Deep Dreams,https://www.reddit.com/r/MachineLearning/comments/3f5zqo/defeating_facebooks_deepface_with_deep_dreams/,fatcatz,1438267769,,5,10
549,2015-7-31,2015,7,31,1,3f6a95,Potential issues with using classifyer to find rare class and then manually classifying the result and using it as training data,https://www.reddit.com/r/MachineLearning/comments/3f6a95/potential_issues_with_using_classifyer_to_find/,heathgerhardt,1438272217,"I'm new to machine learning and I'm wondering if there are issues with using the classifier to find members of a rare class, manually classifying the result and feeding that back in as more training data?",4,1
550,2015-7-31,2015,7,31,1,3f6dha,What incentive is there to publish revolutionary work?,https://www.reddit.com/r/MachineLearning/comments/3f6dha/what_incentive_is_there_to_publish_revolutionary/,[deleted],1438273540,"I once worked for a firm that did algorithmic trading, they never let us close to the algorithms but after years things did slip, they apparently had some sort of clever way to do dimensional reduction that allowed them to detect market anomalies and somehow trade on those.




So my question would be, how many breakthroughs (especially in unsupervised learning) are sitting silently in corporate basements making money out of thin air?


Even the founder of arguably the most successful hedge fund ever hesitated to answer whether or not their secret discoveries could advance mankind. (https://www.youtube.com/watch?v=QNznD9hMEh0    -  sorry I don't have the exact time when that question was asked, it's somewhere in the interview I remember.)
",56,58
551,2015-7-31,2015,7,31,1,3f6gee,10 Machine Learning Terms Explained in Simple English,https://www.reddit.com/r/MachineLearning/comments/3f6gee/10_machine_learning_terms_explained_in_simple/,dabshitty,1438274742,,1,9
552,2015-7-31,2015,7,31,2,3f6lsl,Probabilistic programming vs approximate bayesian computation,https://www.reddit.com/r/MachineLearning/comments/3f6lsl/probabilistic_programming_vs_approximate_bayesian/,[deleted],1438276919,"What's the difference between probabilistic programming and approximate bayesian computation (ABC)?

Is it simply that ABC treats the likelihood function as a black box, and samples by rejection using a summary statistic and a distance metric wrt observed data. Whereas probabilistic programming is more general, and uses the trace of the likelihood function and a sampler such as MCMC?",0,1
553,2015-7-31,2015,7,31,4,3f773u,"Coinalytics Is Hiring Sr. Machine Learning Engineer/Data Scientist (Palo Alto, CA)",https://www.reddit.com/r/MachineLearning/comments/3f773u/coinalytics_is_hiring_sr_machine_learning/,fabiofederici,1438285450,,1,0
554,2015-7-31,2015,7,31,4,3f77k1,Intro to Principles &amp; Practice of Amazon Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3f77k1/intro_to_principles_practice_of_amazon_machine/,[deleted],1438285642,,1,0
555,2015-7-31,2015,7,31,4,3f78in,I wrote a short explanation and analysis of Neural Networks. Does it express an understanding of the field?,https://www.reddit.com/r/MachineLearning/comments/3f78in/i_wrote_a_short_explanation_and_analysis_of/,bakmanthetitan329,1438286017,,11,0
556,2015-7-31,2015,7,31,5,3f7erg,Fully Tagged and Searchable - Course on Reinforcement Learning by David Silver of Google DeepMind (2015),https://www.reddit.com/r/MachineLearning/comments/3f7erg/fully_tagged_and_searchable_course_on/,ojaved,1438288522,,8,62
557,2015-7-31,2015,7,31,7,3f7ri0,Registrations open for INNS Deep Learning Workshop in SF with a great set of speakers,https://www.reddit.com/r/MachineLearning/comments/3f7ri0/registrations_open_for_inns_deep_learning/,flukeskywalker,1438294047,"Deep Learning Workshop &amp; INNS BigData in San Francisco August 8-10 

http://innsbigdata.org/workshops/deep-learning-workshop/

http://innsbigdata.org/plenary-speakers/",0,1
558,2015-7-31,2015,7,31,10,3f8ebj,"Seems a bit preemptive. I suppose we can simulate the connectome of flatworms, so we're constantly approaching actual life, but is stuff like Q-learning really all that close to something living? Hard to say I guess. I seriously doubt it ""suffers"" yet.",https://www.reddit.com/r/MachineLearning/comments/3f8ebj/seems_a_bit_preemptive_i_suppose_we_can_simulate/,UristMcBrogrammer,1438304591,,1,0
559,2015-7-31,2015,7,31,13,3f94a0,Question about coursera Machine Learning Course to those who have completed it before.,https://www.reddit.com/r/MachineLearning/comments/3f94a0/question_about_coursera_machine_learning_course/,Gay_Hat_On_Nun,1438318120,"I discovered recently Andrew Ng's course on coursera, the next session which will begin on August 10. I was looking at some of the info and it seems to be teaching using octave and matlab, neither of which i am familiar with. I am currently learning Python and I was wondering if the course focuses more on theory and the conceptual ideas of machine learning, or if there is a lot of actual hands-on with it. Also, how familiar do we need to be with octave or matlab in order to complete the course? Thanks in advance. ",41,6
560,2015-7-31,2015,7,31,15,3f9drh,ImageNet 1K vs 22K. Any experts here to help figure this out?,https://www.reddit.com/r/MachineLearning/comments/3f9drh/imagenet_1k_vs_22k_any_experts_here_to_help/,pseudopotential,1438323896,"I am interested in learning about the state-of-the-art in distributed deep learning. I was hoping there would be abundant literature on this topic, but I was surprised to find just these references:

1. Google DistBelief: NIPS 2012, train on ImageNet 21K. Not totally sure about the neural network architecture used in this work

2. Microsoft Project Adam: OSDI 2014, train on ImageNet 22K, AlexNet-like CNN achieving ~30% accuracy.

3. Baidu Deep Image: Train on ImageNet 1K using a cluster of CPUs+GPUs

Some questions that I'm hoping this subreddit can help answer:

1. Why is ImageNet 1K a lot more popular than ImageNet 22K -- There are dozens of papers dealing with the 1K classification task, the current state-of-the-art coming close to 4.8% accuracy (Google's Batch Normalization paper), but only 2-3 that show results on the task with 22K categories

2. For somebody looking seriously into building a distributed deep learning (distributed in a HPC cluster), what forms a better benchmark? ImageNet 1K or ImageNet 22K?

My speculation to (1) is that perhaps the model size for the 22K task is too big for 1 GPU to handle and one really needs an optimized distributed system to handle that big a task. Any comments?",8,1
561,2015-7-31,2015,7,31,16,3f9iri,What deep learning techniques would you like open-source demos of in Torch?,https://www.reddit.com/r/MachineLearning/comments/3f9iri/what_deep_learning_techniques_would_you_like/,starship_falling,1438327585,"Hey all. I'm working on getting current with major/interesting new research developments in deep learning, and I'd like to take a crack at making demos in Torch. Right now I am working on an easy-to-use Neural Turing Machine implementation, and I'm interested in doing seq2seq (along the lines of the Google chatbot) and differentiable memory structures (the ones in Learning to Transduce with Unbounded Memory, and memory networks). 

Does anyone have any suggestions for other stuff they'd like to see? I make no guarantees that I'll be able to do it, but I'm curious to know what people are interested in. (Also it might be a couple of months before any of it comes to fruition.)

Furthermore, if there are any other peeps here working on neural nets with Torch - holla! We should all chat and learn from each other. ",9,12
562,2015-7-31,2015,7,31,18,3f9pyq,Can biology still help developments in ML?,https://www.reddit.com/r/MachineLearning/comments/3f9pyq/can_biology_still_help_developments_in_ml/,Draglx,1438333646,I was wondering if further advances in understanding the brain could lead to developments in fields like neural networks as it had in the past ,3,0
563,2015-7-31,2015,7,31,18,3f9rjw,WASP: Scalable Bayes via barycenters of subset posteriors,https://www.reddit.com/r/MachineLearning/comments/3f9rjw/wasp_scalable_bayes_via_barycenters_of_subset/,Kiudee,1438334923,,0,9
564,2015-7-31,2015,7,31,19,3f9w9r,How does ESPN generate interesting stats?,https://www.reddit.com/r/MachineLearning/comments/3f9w9r/how_does_espn_generate_interesting_stats/,ashleyschaeffer,1438338617,"So as I watch sports, I'm fascinated by how obscure and specific some stats shown can be. And they almost happen instantaneously.  

I'm guessing that they must be using some type of combination of machine learning and human interaction to find interesting stats to display on the screen. But I'm not actually sure how they are even doing that. Maybe they just have a fleet of interns but I'm guessing there is more to it. And if there isn't, could we use machine learning to help us?

Does anybody have any insights, sources, keywords I could search for to learn more about this? Would it simply be various outlier detection algorithms? Or is there a more appropriate term? Any directional help would be great.",12,17
565,2015-7-31,2015,7,31,21,3fa43n,"Uber - optimising ""idle"" car locations",https://www.reddit.com/r/MachineLearning/comments/3fa43n/uber_optimising_idle_car_locations/,aidan_morgan,1438344142,"I have read in a few different places that Uber will move cars that have no jobs to best meet a predicted demand. This reduces the wait time for customers and provides a better customer experience.

Being the geek that I am, I started to wonder how they implemented this feature.


What sort of model would they be using to predict where the demand is going to be? 


How would you go about implementing such a system? What models/approaches would you use?",3,0
566,2015-7-31,2015,7,31,21,3fa60m,Availability of Online Gpu clusters in India?,https://www.reddit.com/r/MachineLearning/comments/3fa60m/availability_of_online_gpu_clusters_in_india/,[deleted],1438345269,"I am a student working in deep learning. To run my model, I need a high performance GPU. It is not available in my college. Are there any online clusters like Amazon aws which provide their services in India?
EDIT: Amazon aws does not provide its services to Indian users.",6,0
567,2015-7-31,2015,7,31,21,3fa9cy,Neurogram: Interactive artwork generated by neural networks evolved via CPPN-NEAT and recurrent.js,https://www.reddit.com/r/MachineLearning/comments/3fa9cy/neurogram_interactive_artwork_generated_by_neural/,inarrears,1438347148,,1,12
568,2015-7-31,2015,7,31,21,3fa9qn,All recordings from MLSS Sydney 2015,https://www.reddit.com/r/MachineLearning/comments/3fa9qn/all_recordings_from_mlss_sydney_2015/,matiskay,1438347362,,1,6
569,2015-7-31,2015,7,31,22,3fadpw,What to do with entries in the data set that have 'nan' in them?,https://www.reddit.com/r/MachineLearning/comments/3fadpw/what_to_do_with_entries_in_the_data_set_that_have/,thenamesalreadytaken,1438349434,"Before starting to take chunks from the data set and apply things on it, how can I get rid of the 'nan' entries in the data set? ",8,1
570,2015-7-31,2015,7,31,22,3fadtp,VersaMac Akiles Modular Punch at $3779.00,https://www.reddit.com/r/MachineLearning/comments/3fadtp/versamac_akiles_modular_punch_at_377900/,Printfinish,1438349491,,0,1
571,2015-7-31,2015,7,31,23,3falc2,"Why are genetic algorithms better than just random mutation? And why only 2 ""parents""?",https://www.reddit.com/r/MachineLearning/comments/3falc2/why_are_genetic_algorithms_better_than_just/,NEED_A_JACKET,1438353012,"Sorry for the uneducated question, but I can't seem to understand why selecting random traits (or weights in the case of a neural network) from two parents is better than simply randomizing all weights in the network.

And why is it best to select traits from the two best, as opposed to say.. 50% from the best, 30% from the second best, 20% from the third best, or some other multiple parent solution?

Also could a neural network (that has already been 'trained') be trained to respond to new input, by introducing the new input into the system with a low weighting so that it can slowly integrate into the system? For example if you had a path finding AI, adding in an extra 'eye' input so it can see another angle.",21,16
