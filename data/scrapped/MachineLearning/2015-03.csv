,sbr,sbr_id,date,year,month,hour,day,id,domain,title,full_link,author,created,selftext,num_comments,score,over_18,thumbnail,media_provider,media_thumbnail,media_title,media_description,media_url
0,MachineLearning,t5_2r3gv,2015-3-1,2015,3,1,9,2xig3h,ml.posthaven.com,Machine Learning Done Wrong - Some common mistakes,https://www.reddit.com/r/MachineLearning/comments/2xig3h/machine_learning_done_wrong_some_common_mistakes/,kunjaan,1425168774,,13,84,False,http://b.thumbs.redditmedia.com/t0zcXXrXz4c4cVvSllodxgEUxcGSQcOtdrEd-U8poCo.jpg,,,,,
1,MachineLearning,t5_2r3gv,2015-3-1,2015,3,1,9,2xijqt,self.MachineLearning,Library for Hyperparameter Optimization Over AWS,https://www.reddit.com/r/MachineLearning/comments/2xijqt/library_for_hyperparameter_optimization_over_aws/,alexmlamb,1425170836,"Hello, 

Does anyone know if there's a tool that allows one to run a hyperparameter optimization tool like spearmint, and have each training run automatically go onto an AWS instance?  

I'm thinking that this should be possible by combining the StarCluster library with spearmint.  

Is there any existing tool that already has this set up end to end?  If not, it's something that I'm willing to write myself and share.  ",7,4,False,self,,,,,
2,MachineLearning,t5_2r3gv,2015-3-1,2015,3,1,11,2xirur,self.MachineLearning,Convert text file to arff,https://www.reddit.com/r/MachineLearning/comments/2xirur/convert_text_file_to_arff/,quantumlizard,1425175550,"Hi guys,
I am dealing with reddit comments - specifically, the body of the comment and the class value based on the score. I have a huge file containing the body and the class value, with a separator between them.
I know weka has TextDirectoryLoader where you give it a directory with multiple subdirectories and it will convert that to arff. The question is... should the text files I provide in each subdirectory not have the class value? Just the body of the comment? And should each ""instance"" of text be separated from the other by something?

The documentation is kinda lacking, so those of you who have used it previously, could you help a fellow redditor?",0,0,False,self,,,,,
3,MachineLearning,t5_2r3gv,2015-3-1,2015,3,1,15,2xjeo0,facebook.com,Yann LeCun bashing the MIT review for bad reporting,https://www.reddit.com/r/MachineLearning/comments/2xjeo0/yann_lecun_bashing_the_mit_review_for_bad/,leonoel,1425189834,,4,106,False,http://a.thumbs.redditmedia.com/-qTBqs2rZ0nnU7_9auloR-VgwLhg2tDZEnsyFOKq4z0.jpg,,,,,
4,MachineLearning,t5_2r3gv,2015-3-1,2015,3,1,17,2xjppa,win-vector.com,Does Balancing Classes Improve Classifier Performance?,https://www.reddit.com/r/MachineLearning/comments/2xjppa/does_balancing_classes_improve_classifier/,leewardx,1425199832,,0,1,False,http://b.thumbs.redditmedia.com/vskJ79t06TpvoVmliNpq-QZoaJShfzdcFJVXJcbXQww.jpg,,,,,
5,MachineLearning,t5_2r3gv,2015-3-1,2015,3,1,21,2xjzou,self.MachineLearning,Can we create a computer algorithm that untangles philosophical debates?,https://www.reddit.com/r/MachineLearning/comments/2xjzou/can_we_create_a_computer_algorithm_that_untangles/,[deleted],1425211493,"I was listening to a podcast called Very Bad Wizards with Sam Harris as a guest. Sam Harris was arguing for Consequentialism while the two hosts took the side of Deontology. I was struck by the amount of time the two sides where arguing about the same thing without realizing it. 

That's when I came up with the idea of creating a computer algorithm that untangles or keeps the 'flow' of philosophical debates by eliminating argumentative fallacies along the way. Keep in mind that the goal of said algorithm would not be to 'solve' the debate(if there's such a thing as solving a philosophical debate) by to keep the debating parties on a 'straight' path. 

The idea of a 'straight' path in a philosophical debate presupposes, in the case of the podcast, that there is an evident and distinct definition of ""Consequentialism"" which is separate from that of ""Deontology"". So there should be a way of building an algorithm on these definitions and 'feed' these to a machine which in turn will keep tabs of when the debate is steering off course.



",5,0,False,default,,,,,
6,MachineLearning,t5_2r3gv,2015-3-2,2015,3,2,0,2xke0v,denizyuret.com,Deep learning with 500 lines of Julia,https://www.reddit.com/r/MachineLearning/comments/2xke0v/deep_learning_with_500_lines_of_julia/,alephnil,1425223567,,4,23,False,http://b.thumbs.redditmedia.com/3TWDTASno0FS3u-y4Jl0Z2KO1O6F10FZBt4-dnd8F2w.jpg,,,,,
7,MachineLearning,t5_2r3gv,2015-3-2,2015,3,2,1,2xkide,stat.columbia.edu,Nonparametric Bayes Tutorial,https://www.reddit.com/r/MachineLearning/comments/2xkide/nonparametric_bayes_tutorial/,luoleicn,1425226125,,0,17,False,default,,,,,
8,MachineLearning,t5_2r3gv,2015-3-2,2015,3,2,1,2xkl8l,ai-maker.com,The simulated annealing algorithm (with an emphasis on the schedule function) [x-post /r/compsci],https://www.reddit.com/r/MachineLearning/comments/2xkl8l/the_simulated_annealing_algorithm_with_an/,ai_maker,1425227748,,0,0,False,http://b.thumbs.redditmedia.com/F29ktUudZ-8lzWL4gMqD7sLCg2RORZEpOxW_AUb70FQ.jpg,,,,,
9,MachineLearning,t5_2r3gv,2015-3-2,2015,3,2,2,2xksx0,crankylinuxuser.wordpress.com,uWho  Face recognition and tracking program,https://www.reddit.com/r/MachineLearning/comments/2xksx0/uwho_face_recognition_and_tracking_program/,crankylinuxuser,1425231798,,0,0,False,http://b.thumbs.redditmedia.com/6E0YXUncXhFZE-HgC8IH7COjePvS20nr4P30WrcpyiU.jpg,,,,,
10,MachineLearning,t5_2r3gv,2015-3-2,2015,3,2,4,2xl2wa,self.MachineLearning,[noob] Machine learning and deep learning resources,https://www.reddit.com/r/MachineLearning/comments/2xl2wa/noob_machine_learning_and_deep_learning_resources/,artur_oliver,1425236590,,4,1,False,default,,,,,
11,MachineLearning,t5_2r3gv,2015-3-2,2015,3,2,9,2xm7nw,austingwalters.com,PCA: Principal Component Analysis,https://www.reddit.com/r/MachineLearning/comments/2xm7nw/pca_principal_component_analysis/,austingwalters,1425256632,,6,26,False,http://b.thumbs.redditmedia.com/IZOPLXvQlZOJl8a8EdJn2Dhx3uoTBibhy4iZJ3QPHyE.jpg,,,,,
12,MachineLearning,t5_2r3gv,2015-3-2,2015,3,2,14,2xn5vg,datasciencecentral.com,Comparing linear regression with the Jackknife method,https://www.reddit.com/r/MachineLearning/comments/2xn5vg/comparing_linear_regression_with_the_jackknife/,vincentg64,1425275656,,0,0,False,http://b.thumbs.redditmedia.com/u_gqGHNfqBKJ-usI8NnTKMl4woKIJkbdB7AjLIR9xVA.jpg,,,,,
13,MachineLearning,t5_2r3gv,2015-3-2,2015,3,2,15,2xn97n,christianpeccei.com,Mapping your music collection with machine learning,https://www.reddit.com/r/MachineLearning/comments/2xn97n/mapping_your_music_collection_with_machine/,rhiever,1425278073,,11,80,False,http://a.thumbs.redditmedia.com/-OetcotY6m9zPi5j1YhdRk5c_24ROfwR01RlIBSYVa4.jpg,,,,,
14,MachineLearning,t5_2r3gv,2015-3-2,2015,3,2,18,2xnjw5,self.MachineLearning,How to fit a Gaussian Mixture Model to data that has correlated errors?,https://www.reddit.com/r/MachineLearning/comments/2xnjw5/how_to_fit_a_gaussian_mixture_model_to_data_that/,TypicalVillain,1425288211,"I have a 3D data set that has been blurred in an unknown manner. As a consequence each 3D voxel is correlated to its neighbors. I'd like to fit a Gaussian Mixture Model to this data. Here's the problem: the usual GMM pdf requires independent observations (it's the product of the probability of each point under the GMM). Are there any ways of considering this correlation when fitting (or even just computing the likelihood of) a GMM?

E.g. I like how in Gaussian Processes one simply has to specify a spatial dependence for the covariance matrix. Like cov(x_i,x_j) = exp(-k*dist_ij). But I don't know how to incorporate something like this in a GMM score.

Thanks!",8,1,False,self,,,,,
15,MachineLearning,t5_2r3gv,2015-3-2,2015,3,2,23,2xo4ce,self.MachineLearning,Why do Linkedin and Facebook keep recommending some persons to me?,https://www.reddit.com/r/MachineLearning/comments/2xo4ce/why_do_linkedin_and_facebook_keep_recommending/,timlee126,1425306171,Why do Linkedin and Facebook keep recommending some persons to me?    I may or may not have visited some of their Linkedin or Facebook pages. Does that mean they have visited my page? Thanks.,0,0,False,self,,,,,
16,MachineLearning,t5_2r3gv,2015-3-2,2015,3,2,23,2xo7vc,kickstarter.com,"Artificial Intelligence for Humans, Vol 3: Deep Learning &amp; Neural Networks on Kickstarter",https://www.reddit.com/r/MachineLearning/comments/2xo7vc/artificial_intelligence_for_humans_vol_3_deep/,jeffheaton,1425308212,,0,6,False,http://b.thumbs.redditmedia.com/NdBrKZ8tM5x64BIlcr2FPvosZoQ99qnMhADo2G_vg-M.jpg,,,,,
17,MachineLearning,t5_2r3gv,2015-3-3,2015,3,3,1,2xokww,jmlr.csail.mit.edu,Networks to vectors: using PageRank to recover latent embeddings and densities.,https://www.reddit.com/r/MachineLearning/comments/2xokww/networks_to_vectors_using_pagerank_to_recover/,Tatsu23456,1425314648,,4,10,False,default,,,,,
18,MachineLearning,t5_2r3gv,2015-3-3,2015,3,3,2,2xopnm,self.MachineLearning,"Monday's ""Simple Questions Thread"" - 20150302",https://www.reddit.com/r/MachineLearning/comments/2xopnm/mondays_simple_questions_thread_20150302/,seabass,1425316760,"Last time =&gt; /r/MachineLearning/comments/2u73xx/fridays_simple_questions_thread_20150130/

One a week seemed like too frequent, so let's try once a month...

This is in response to the original posting of whether or not it made sense to have a question thread for the non-experts. I learned a good amount, so wanted to bring it back...",35,7,False,self,,,,,
19,MachineLearning,t5_2r3gv,2015-3-3,2015,3,3,2,2xouzu,self.MachineLearning,"Exclusive discount code for redditors, for all of RE.WORK's upcoming events - London, Boston, San Francisco - Deep Learning, IoT, Robotics, Future Tech &amp; more",https://www.reddit.com/r/MachineLearning/comments/2xouzu/exclusive_discount_code_for_redditors_for_all_of/,reworksophie,1425319181,"We have a discount code for the Reddit community to receive 15% off all ticket types to upcoming summits for 2015 and 2016. 

Enter the discount code REDDIT (case sensitive) at the checkout of ALL currently released summits on our website and you'll get 15% off the cost of ANY ticket type! 

Upcoming summits include: Internet of Things Summit London, Deep Learning Summit Boston, Internet of Things Summit Boston, Future AI &amp; Robotics Summit London - and more!

To see all events this code can be applied to, go to: http://re-work.co/events 

And of course, if you attend an event, make sure to say hi to me! It'd be great to meet some of you.",0,0,False,self,,,,,
20,MachineLearning,t5_2r3gv,2015-3-3,2015,3,3,3,2xowbi,arxiv.org,Video Description Generation Incorporating Spatio-Temporal Features and a Soft-Attention Mechanism,https://www.reddit.com/r/MachineLearning/comments/2xowbi/video_description_generation_incorporating/,clbam8,1425319754,,1,0,False,default,,,,,
21,MachineLearning,t5_2r3gv,2015-3-3,2015,3,3,3,2xp1vb,cymetica.com,Financial data science related to finding hidden connections between stocks,https://www.reddit.com/r/MachineLearning/comments/2xp1vb/financial_data_science_related_to_finding_hidden/,biomimic,1425322227,,0,0,False,default,,,,,
22,MachineLearning,t5_2r3gv,2015-3-3,2015,3,3,5,2xpdrb,blogs.technet.com,Free Webinar: Overview of New Capabilities in Azure ML,https://www.reddit.com/r/MachineLearning/comments/2xpdrb/free_webinar_overview_of_new_capabilities_in/,MLBlogTeam,1425327420,,0,1,False,default,,,,,
23,MachineLearning,t5_2r3gv,2015-3-3,2015,3,3,5,2xpdtg,self.MachineLearning,Searching participants for my master thesis userstudy (involves steam/machine learning),https://www.reddit.com/r/MachineLearning/comments/2xpdtg/searching_participants_for_my_master_thesis/,[deleted],1425327445,"Hey,

for my master thesis I need a lot of people to take part in my user study. You are required to have a steam account, it doesn't take so much time (approx. 25 min) and is also a fun task - you will not encounter some boring text input fields. =)

So when you have some time it would be nice if you participate via http://heleska.de:8000

Furthermore it would be great to ask your friends on steam to participate also in the study.

Thanks a lot Sheepy
",0,0,False,default,,,,,
24,MachineLearning,t5_2r3gv,2015-3-3,2015,3,3,5,2xpfi0,blog.sigopt.com,Picking the Right Metric,https://www.reddit.com/r/MachineLearning/comments/2xpfi0/picking_the_right_metric/,Zephyr314,1425328165,,1,0,False,http://b.thumbs.redditmedia.com/sGFevTdH-cqwIPu3brviaUKCJ9GnVjadbi-2CEgMY4Y.jpg,,,,,
25,MachineLearning,t5_2r3gv,2015-3-3,2015,3,3,6,2xppzw,self.MachineLearning,NMT vs state of the art in translation?,https://www.reddit.com/r/MachineLearning/comments/2xppzw/nmt_vs_state_of_the_art_in_translation/,spurious_recollectio,1425332670,"This may be a rather dense question but from reading various neural machine translation papers one gets the impression that sequence-to-sequence RNN implementations come close to or beat ""state-of-the-art"" in e.g. French-English translation (e.g.  arXiv:1412.2007).  The BLEU scores cited in those papers are around 35-37 for English-French translation.  

In this article:

http://www.translationdirectory.com/articles/article2320.php

and the link they provide here:

http://faculty.bus.olemiss.edu/maiken/data.htm

It seems that google  translate's BLEU score for French-English is more like 90.  So what gives?

My guess is that the NMT articles are basing their ""state-of-the-art"" on what is acheivable with a constrained dataset (WMT14) and with this small dataset 37 is state of the art?  Is this right?

If so doesn't this undercut the general claim however as it already seems to take a long time to train a sufficiently powerful NN on that dataset so expanding it to whatever dataset google uses to acheive their ~90 score is essentially impractical.  Is this correct or am I missing something?
",2,1,False,self,,,,,
26,MachineLearning,t5_2r3gv,2015-3-3,2015,3,3,6,2xpqxn,self.MachineLearning,"Learn artifficial intelligence: neural network, machine learning...",https://www.reddit.com/r/MachineLearning/comments/2xpqxn/learn_artifficial_intelligence_neural_network/,jazzsound,1425333072,"Hello!
I wonder if someone know where can I find some resources to learn artificial intelligence as I said in the title. I was looking for a very beginner theory and practical exercises, but I found nothing really ""easy"" to me. It's a pity not to found some articles in portuguese, but anyway, could you share some links to learn? I want to put in practice my skills in C or C++ if possible. Thanks for the help!",6,0,False,self,,,,,
27,MachineLearning,t5_2r3gv,2015-3-3,2015,3,3,7,2xptdg,self.MachineLearning,Best book for machine learning in python ?,https://www.reddit.com/r/MachineLearning/comments/2xptdg/best_book_for_machine_learning_in_python/,bloodian91,1425334128,What's the best book for machine learning in python ? I want examples and some math to have a vague idea of what's going on inside the algorithm. Thanks.,28,45,False,self,,,,,
28,MachineLearning,t5_2r3gv,2015-3-3,2015,3,3,7,2xpz9i,datasciencecentral.com,101 new external resources and articles about ML and data science,https://www.reddit.com/r/MachineLearning/comments/2xpz9i/101_new_external_resources_and_articles_about_ml/,urinec,1425336761,,0,0,False,http://b.thumbs.redditmedia.com/cPlFCvSrbJxyjIx71bo-_ifGXldMnMSMx9cajwn8nXk.jpg,,,,,
29,MachineLearning,t5_2r3gv,2015-3-3,2015,3,3,7,2xpzst,self.MachineLearning,Waton-like system via NNs?,https://www.reddit.com/r/MachineLearning/comments/2xpzst/watonlike_system_via_nns/,spurious_recollectio,1425337016,"I am looking for any references on using NNs to develop Watson-like QA systems (i.e. a NLP QA system that can be trained off some text corpus or DB).  From looking into these systems a little, e.g.:

http://www.manning.com/ingersoll/Sample-ch08.pdf

There seems to be several different stages which involve a fair amount of manual feature engineering and I was wondering if any NN architectures existed to try to solve the same problem without e.g. feature engineering.   This could either be an end-to-end NN system or NNs used for part of this chain.

Some initial googling yielded some interesting looking papers (that i admittedly have not yet opened) by Monner &amp; Reggia and also others on factoid answering.  I plan to look over these but if anyone has any other references or thoughts I would very much appreciate them.  Actually even info on the original (non-NN) Watson system architecture would be interesting.

",1,0,False,self,,,,,
30,MachineLearning,t5_2r3gv,2015-3-3,2015,3,3,8,2xq0pb,rebelscience.blogspot.com,Critique: Deep learning is just GOFAI with lipstick on,https://www.reddit.com/r/MachineLearning/comments/2xq0pb/critique_deep_learning_is_just_gofai_with/,sixwings,1425337432,,9,0,False,default,,,,,
31,MachineLearning,t5_2r3gv,2015-3-3,2015,3,3,9,2xq9g0,self.MachineLearning,Data scientist vs Research Engineer?,https://www.reddit.com/r/MachineLearning/comments/2xq9g0/data_scientist_vs_research_engineer/,wonkypedia,1425341525,"as far as ML jobs go, what's the difference? 

Also, what's the deal with predictive analytics? Everyone seems to be doing it these days.... anyone who works on it care to weigh in? ",3,1,False,self,,,,,
32,MachineLearning,t5_2r3gv,2015-3-3,2015,3,3,11,2xqq2s,quickml.org,"QuickML 0.6.0 released, a powerful yet easy to use machine learning library in Java including an efficient and well-tested random forests implementation.",https://www.reddit.com/r/MachineLearning/comments/2xqq2s/quickml_060_released_a_powerful_yet_easy_to_use/,sanity,1425349667,,13,2,False,default,,,,,
33,MachineLearning,t5_2r3gv,2015-3-3,2015,3,3,13,2xr66e,sdtimes.com,Instart Logic brings machine learning to HTML and JavaScript code delivery,https://www.reddit.com/r/MachineLearning/comments/2xr66e/instart_logic_brings_machine_learning_to_html_and/,[deleted],1425357894,,1,1,False,default,,,,,
34,MachineLearning,t5_2r3gv,2015-3-3,2015,3,3,16,2xrle6,self.MachineLearning,Resources About Engineering Machine Learning Systems?,https://www.reddit.com/r/MachineLearning/comments/2xrle6/resources_about_engineering_machine_learning/,blowjobtransistor,1425368205,"Can /r/machinelearning suggest any papers/talks/blog posts about the arduous task of actually *building and maintaining* machine learning systems?  Understanding the algorithms is a fair bit of mental work, but at the end of the day the hardest part has consistently been keeping a whole system coherent and running properly.

Currently I'm leading engineering at a startup that relies heavily on machine learning, and our biggest development time sinks so far have been verifying (usually simple) changes, and debugging.  I have experience engineering systems with much clearer expectations, but engineering machine learning systems is an entirely different (and more uncomfortable) ball game.  It seems to boil down to a few factors:

* Requirements change consistently as we understand how well a given algorithm maps to a problem, adding to changing requirements from the business
* Some failures only occur at scale, in long running tasks
* It is difficult to set expectations about what output is ""correct"", making it difficult to write system level tests

What strategies have made your machine learning system development cycles more predictable? What are tractable testing/verification strategies that can increase dev productivity?  What is the mindset required to move quickly in developing these systems, as TDD doesn't seem to work?

Thank you!",3,12,False,self,,,,,
35,MachineLearning,t5_2r3gv,2015-3-3,2015,3,3,17,2xrpfn,self.MachineLearning,Khosla Machines success is based on few crucial strategies i.e. specialized knowledge and customer satisfaction. Our only objective is to strengthen our packaging innovation so that we could deliver you the most versatile and effective packaging machines which you was still looking for!!,https://www.reddit.com/r/MachineLearning/comments/2xrpfn/khosla_machines_success_is_based_on_few_crucial/,KhoslaMachines,1425371820,,1,0,False,default,,,,,
36,MachineLearning,t5_2r3gv,2015-3-3,2015,3,3,17,2xrqoo,cio.com.au,"Google, Stanford use machine learning on 37.8m data points for drug discovery",https://www.reddit.com/r/MachineLearning/comments/2xrqoo/google_stanford_use_machine_learning_on_378m_data/,ninacertain,1425373072,,3,7,False,http://b.thumbs.redditmedia.com/sbSOHwXc8a9vCt00mniftxZv9bMIF31jNXDrZ4Z1dDk.jpg,,,,,
37,MachineLearning,t5_2r3gv,2015-3-3,2015,3,3,18,2xrr7c,zdnet.com,Keeping the world's elevators running smoothly with machine learning and IoT,https://www.reddit.com/r/MachineLearning/comments/2xrr7c/keeping_the_worlds_elevators_running_smoothly/,ardathmittman,1425373588,,1,0,False,http://a.thumbs.redditmedia.com/Unj3DNe8xgAqElgKM7qNNbCXVhRtGvwtfKSK8-ECZV4.jpg,,,,,
38,MachineLearning,t5_2r3gv,2015-3-3,2015,3,3,18,2xrrav,cio.com.au,Machine learning used to predict clinical response to anti-cancer drugs,https://www.reddit.com/r/MachineLearning/comments/2xrrav/machine_learning_used_to_predict_clinical/,bobbyjerdon,1425373694,,0,0,False,http://b.thumbs.redditmedia.com/Y30J37SDYA_lFGGpnxDu5a9w5MOQ1UHD9YVIm6E9sxs.jpg,,,,,
39,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,0,2xsm0u,self.MachineLearning,What kinds of ML tutorials would you like to see?,https://www.reddit.com/r/MachineLearning/comments/2xsm0u/what_kinds_of_ml_tutorials_would_you_like_to_see/,joe-murray,1425397016,"Hey everyone, I'm working on a small project in my free time. It's basically a site for ML tutorials, links to good resources, and other things like that. It's not public facing yet as it's still in its early stages, but I'm interested to hear what kinds of tutorials people want/need.

As an example, [here's one](https://www.dashingd3js.com/table-of-contents) for data visualization in D3.js, and [here's a good one](http://note.sonots.com/SciSoftware/haartraining.html) for setting up a face detector in OpenCV. I've used both of these personally in my job and they helped me greatly.",22,23,False,self,,,,,
40,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,1,2xsqvp,self.MachineLearning,Big Data:We need you! Fields-Mprime Industrial Problem Solving Workshop,https://www.reddit.com/r/MachineLearning/comments/2xsqvp/big_datawe_need_you_fieldsmprime_industrial/,FieldsInstitute,1425399305,"[The Fields Institute for Research in Mathematical Sciences](http://www.fields.utoronto.ca/) is putting on a 5 day Big Data Industrial Problem Solving Workshop in late May. Its purpose is to have industry present a problem and then the assembled math researchers attempt to solve the problem during the course of the workshop. Typically the problems can cover a wide range of topics but to coincide with our thematic program on Big Data that we are running, we are looking for problems specifically in the field of Big Data. We are asking if anyone on Reddit works in industry in the mathematics field that could possibly put us in touch with their company in order to make this event a success. There is no fee for industry bodies that are participating. 

To get an idea of what we are talking about, [here is the link to last years event](http://www.fields.utoronto.ca/programs/cim/14-15/IPSW14/index.html). 

If you have any further questions please respond to this post or email our Industry Liaison Officer, Tyler Wilson at twilson@fields.utoronto.ca.",3,0,False,self,,,,,
41,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,3,2xtao2,machinalis.com,Introducing Iepy an Information Extraction tool,https://www.reddit.com/r/MachineLearning/comments/2xtao2/introducing_iepy_an_information_extraction_tool/,copybin,1425407934,,0,0,False,http://b.thumbs.redditmedia.com/xUaouPH6XPdUhjcTRQBOnHvLNIDb39-LljWxryyp8SQ.jpg,,,,,
42,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,4,2xtefz,kdnuggets.com,All Machine Learning Models Have Flaws,https://www.reddit.com/r/MachineLearning/comments/2xtefz/all_machine_learning_models_have_flaws/,[deleted],1425409549,,1,0,False,default,,,,,
43,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,4,2xtgl3,blog.yhathq.com,Machine Learning Pitfalls: Measuring Performance,https://www.reddit.com/r/MachineLearning/comments/2xtgl3/machine_learning_pitfalls_measuring_performance/,ericchiang,1425410436,,0,0,False,default,,,,,
44,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,4,2xtk5o,self.MachineLearning,Deep Learning architecture questions,https://www.reddit.com/r/MachineLearning/comments/2xtk5o/deep_learning_architecture_questions/,maxxxpowerful,1425411978,"I'm exploring DL, and was going through [Krizhevsky, Sutskever and Hinton's seminal 2012 paper](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) on their ILSVRC 2012 work. 

My understanding of DL is that it's a bunch of layers stacked on top of each other (to put it very broadly). So I am curious about Figure 2 on page 5: why do some of the layers appear 3-dimensional? I can understand that the input is 3-dimensional, as we treat the RGB values as different planes. But the first hidden layer has a dimension of 55x55x48; how do they get the figure 55, when the input has width 224, and convolution matrix is 11x11 and the stride is 4? Further confusingly, the description says ""*The first convolutional layer filters the 224x224x3 input image with 96 kernels of size 11x11x3 with a stride of 4 pixels*"" .... where did this 96 come from?",10,2,False,self,,,,,
45,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,6,2xtv3e,devblogs.nvidia.com,Facebook Research on Understanding Natural Language with Deep Neural Networks Using Torch,https://www.reddit.com/r/MachineLearning/comments/2xtv3e/facebook_research_on_understanding_natural/,harrism,1425416578,,1,48,False,http://b.thumbs.redditmedia.com/vGbTHFdPd7Kfa-dsQoeBrpszRJMlvMLjhO1G-d3IqfY.jpg,,,,,
46,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,6,2xtz18,self.MachineLearning,Help with clustering recipe ingredients,https://www.reddit.com/r/MachineLearning/comments/2xtz18/help_with_clustering_recipe_ingredients/,wonkypedia,1425418154,"I found [this paper](http://arxiv.org/ftp/arxiv/papers/1502/1502.03815.pdf) and was inspired to analyse this kind of data. 

I pulled about 2500 recipes, and wanted to see if ingredients cluster well. Ultimately, I end up with around 350 unique ingredients, but clustering them while using the 2500 recipes as features doesn't result in any meaningful clustering. 

What can I do to get better meaning out of my data? ",10,3,False,self,,,,,
47,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,6,2xu2ts,self.MachineLearning,Visualizing semantic Hashing,https://www.reddit.com/r/MachineLearning/comments/2xu2ts/visualizing_semantic_hashing/,pumpkin105,1425419756,"Hi,

I found the article of Ruslan Salakhutdinov about semantic hashing. I implemented something similiar like him using pylearn2:

* I have about 12,000 news articles
* I created binary vectors determining whether a word exists in that article or not
* My stacked RBMs have this structure: 2000 -&gt; 512 -&gt; 256 -&gt; 128 -&gt; 32
* After pretraining, I finetuning the weights. I do not add deterministic noise because I havn't found out yet how to do that in pylearn2

When doing pca, my articles look like [this](http://i.imgur.com/Ya2Do6e.jpg). When searching in them, the results are quite good. What bother me though is, why does the visualization does not look like [this](http://image.slidesharecdn.com/piotrmirowskiciunconf2014reviewautoencoders-140820024912-phpapp02/95/piotr-mirowski-review-autoencoders-deep-learning-ciuuk14-45-638.jpg%3Fcb%3D1408521107)? I already tried using two nodes for the topmost layer, but the result didn't change much. Is the determinist noise really that important?",5,0,False,self,,,,,
48,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,9,2xukvr,gorayni.blogspot.mx,Factor Analysis explained with classic example,https://www.reddit.com/r/MachineLearning/comments/2xukvr/factor_analysis_explained_with_classic_example/,acartasa,1425427948,,1,9,False,http://b.thumbs.redditmedia.com/EdPx1caIEXkMiBhVuCk-wNK33036s1kTWaqQ-qekdMY.jpg,,,,,
49,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,10,2xutjl,developers.500px.com,Large Scale Image Classification with Hadoop Streaming,https://www.reddit.com/r/MachineLearning/comments/2xutjl/large_scale_image_classification_with_hadoop/,regat,1425432230,,0,1,False,default,,,,,
50,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,10,2xuvsh,self.MachineLearning,Is PCA useful for 3 dimensions of data?,https://www.reddit.com/r/MachineLearning/comments/2xuvsh/is_pca_useful_for_3_dimensions_of_data/,Nixonite,1425433299,"Hello everyone,

So I'm only knowledgeable of machine learning up to the level of ""Introduction to Statistical Learning"" by Tibshirani and Hastie, so hopefully you can talk to me at that level.

My question is this: PCA has been useful for visualizing the spread of datasets of high dimensions, but to what extent is it useful for datasets of dimensions 1, 2, or 3 where they can be entirely plotted on a graph?

Can you tell me if my guess is correct?

My guess: It's useful because it shows the dimensions of highest variance, or in other words it shows the most telling of features.

So another question that extends from this: What is the difference in doing a PCA of the dataset compared to just listing the features in order of variance? By ""difference"" I mean what can I learn from doing it one way vs another way. 

This is not for a class, just a question for the sake of learning. 

Thanks for your time and any input you have. ",5,0,False,self,,,,,
51,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,13,2xvi5d,sciencedaily.com,Neuroscientists identify new way several brain areas communicate. (Deals with how reinforcement learning occurs.),https://www.reddit.com/r/MachineLearning/comments/2xvi5d/neuroscientists_identify_new_way_several_brain/,RushAndAPush,1425444690,,1,0,False,http://b.thumbs.redditmedia.com/ZVhKYglyAJF-uhaIx5l9MqR8Bi4V2LPuRJVlwtEnOSs.jpg,,,,,
52,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,14,2xvj43,people.idsia.ch,Towards an Actual Gdel Machine Implementation (PDF),https://www.reddit.com/r/MachineLearning/comments/2xvj43/towards_an_actual_gdel_machine_implementation_pdf/,[deleted],1425445217,,0,1,False,default,,,,,
53,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,14,2xvkqn,self.MachineLearning,Help with RBM implementation,https://www.reddit.com/r/MachineLearning/comments/2xvkqn/help_with_rbm_implementation/,[deleted],1425446055,"Hello everybody, I've been trying to implement an RBM, and AFAIK i'm really close to getting it right, but I am having a hiccup with one thing. Take a look at my generated input data: http://i.imgur.com/ZxVs7E3.png
and its reconstruction:http://i.imgur.com/wwo0jDO.png
My rbm does appear to be extracting some features and doing reconstruction partially correct and for the most part colors are group correctly. My RBM only has the input axis' x and y in the visible layer. 

I've tried varying the number of hidden units, learning rate, and iterations and have had no luck. My RBM consistently reconstructs data on a diagonal, either from bottom-left to top-right or bottom-right to top-left?

Has anybody run into this issue or have any recommendations?

My code can be found here:https://github.com/ddlutz/Deeplearning

I know that the implementation isn't vectorized and it runs slow, but I was writing it in a way that it was understandable at first, not for speed.

Side note: if you decide to run my rbm it is not going off of the data which I presented in this post, which was different randomly generated data I had made.",10,0,False,self,,,,,
54,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,14,2xvoai,kukuruku.co,Introduction to Machine Learning with Python and Scikit-Learn,https://www.reddit.com/r/MachineLearning/comments/2xvoai/introduction_to_machine_learning_with_python_and/,atrust16,1425448192,,9,111,False,http://b.thumbs.redditmedia.com/VBTDcyfLroRMV1I-wFMtQlLR8_lRcculHcS2uYh5bSg.jpg,,,,,
55,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,15,2xvryj,self.MachineLearning,What all does an aspiring data-driven startup do from the start wrt to everything data and ML? (xpost- r/datascience),https://www.reddit.com/r/MachineLearning/comments/2xvryj/what_all_does_an_aspiring_datadriven_startup_do/,gardinal,1425450518,"So, I am in the process of being a part of a data driven SaaS which is going to involve a lot of personalisation etc. A lot ML will be used in the near future.

I have been part of companies which have been collecting data in haphazard manner for years and I had to clean it up for any kind of research.

This new product is still in dev. Is there a todo checklist of sort which I can make sure is being followed to make sure we don't lose out on any data and are doing everything correct from the start?

In general, what other things should a start up make sure happens and is followed in a new startup keeping in mind ML will be the core to their algorithms in the future?

Any data advices for a startup still in dev phase?
Any literature you could point me to? Practical tips from experience?

Things like save more features and don't lose out on variance etc.
",0,0,False,self,,,,,
56,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,16,2xvvh8,newscientist.com,Facebook invents an intelligence test for machines,https://www.reddit.com/r/MachineLearning/comments/2xvvh8/facebook_invents_an_intelligence_test_for_machines/,clbam8,1425453075,,1,0,False,http://b.thumbs.redditmedia.com/CIv564nhObw8wHAuATvhBwOEuyct10ubouZ7fVnizmQ.jpg,,,,,
57,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,16,2xvvw3,analyticsvidhya.com,"We have created a learning path on R. Kindly let me know, if you have any feedback / suggestions.",https://www.reddit.com/r/MachineLearning/comments/2xvvw3/we_have_created_a_learning_path_on_r_kindly_let/,kunalj101,1425453395,,0,4,False,http://a.thumbs.redditmedia.com/0fccZ-eJReL5fROkkllKirLUg6qqFbiNtk_OOUlJar4.jpg,,,,,
58,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,17,2xw1j6,al-borj.ae,Sewing Machine,https://www.reddit.com/r/MachineLearning/comments/2xw1j6/sewing_machine/,waseempiddu,1425458460,,1,1,False,default,,,,,
59,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,19,2xw71t,venturebeat.com,How Googles using big data and machine-learning to aid drug discovery,https://www.reddit.com/r/MachineLearning/comments/2xw71t/how_googles_using_big_data_and_machinelearning_to/,WiltonPixler,1425463926,,0,1,False,default,,,,,
60,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,20,2xwbqn,self.MachineLearning,How can we determine how well we can reasonably expect to perform on a problem?,https://www.reddit.com/r/MachineLearning/comments/2xwbqn/how_can_we_determine_how_well_we_can_reasonably/,[deleted],1425468629,"In applications such as speech recognition and computer vision we have a natural benchmark as we can always compare the performance of our models to humans.

In essence these problems are slightly easier as we have an example system (the human) that has already solved them fairly well.

But how do we judge the performance of our models on other problems where there is no existing decent system? I mean we might know for example that SVM's do well at some classification problem but we don't know how much better we could expect to do?

As engineers we should always be asking 'can we do better'? Is there any way of determining how well we could expect to do on a given problem?

I had this problem in classification so I used t-SNE to see if my classes were separable (i.e. if there was underlying structure in the data that would allow the classes to be distinguished), the fact that the t-SNE plot was not easily separable demonstrated that there was pretty much no underlying structure and so any classification model could be expected to perform poorly.

Likewise in time series I expect that information theoretic measures like multiscale entropy might help uncover structure in the data and mutual information could be useful for regression models perhaps?

It is just driving me crazy that it is so hard to determine if my model is performing poorly because the problem is hard or my data is bad or is it just because my model is poorly chosen or configured.

It reminds me of the story about how the Soviet nuclear programme had it a lot easier than the Manhattan project because they knew it was possible, that it _could_ be done, and thus it was far easier to continue, confident that you weren't wasting time and effort on a futile endeavour.

__tl;dr__: How can we distinguish between a problem being hard/our data being bad and our models just being poorly chosen?",0,1,False,default,,,,,
61,MachineLearning,t5_2r3gv,2015-3-4,2015,3,4,20,2xwbx7,self.MachineLearning,How can we determine how well we can reasonably expect to perform on a problem?,https://www.reddit.com/r/MachineLearning/comments/2xwbx7/how_can_we_determine_how_well_we_can_reasonably/,alexgmcm,1425468787,"In applications such as speech recognition and computer vision we have a natural benchmark as we can always compare the performance of our models to humans.

In essence these problems are slightly easier as we have an example system (the human) that has already solved them fairly well.

But how do we judge the performance of our models on other problems where there is no existing decent system? I mean we might know for example that SVM's do well at some classification problem but we don't know how much better we could expect to do?

As engineers we should always be asking 'can we do better'? Is there any way of determining how well we could expect to do on a given problem?

I had this problem in classification so I used t-SNE to see if my classes were separable (i.e. if there was underlying structure in the data that would allow the classes to be distinguished), the fact that the t-SNE plot was not easily separable demonstrated that there was pretty much no underlying structure and so any classification model could be expected to perform poorly.

Likewise in time series I expect that information theoretic measures like multiscale entropy might help uncover structure in the data and mutual information could be useful for regression models perhaps?

I guess it is sort of a question about determining how much information our data contains about the problem we are trying to solve. This is of particular relevance when we begin to use representation learning ('deep learning') techniques because then we are acting exactly on the data itself and so it would be _really_ useful to know what is the limit of the data and what is our poor architecture choices.

It is just driving me crazy that it is so hard to determine if my model is performing poorly because the problem is hard or my data is bad or is it just because my model is poorly chosen or configured.

It reminds me of the story about how the Soviet nuclear programme had it a lot easier than the Manhattan project because they knew it was possible, that it _could_ be done, and thus it was far easier to continue, confident that you weren't wasting time and effort on a futile endeavour.

__tl;dr__: How can we distinguish between a problem being hard/our data being bad and our models just being poorly chosen?",6,2,False,self,,,,,
62,MachineLearning,t5_2r3gv,2015-3-5,2015,3,5,1,2xx152,self.MachineLearning,POS tag distances (for feature extraction),https://www.reddit.com/r/MachineLearning/comments/2xx152/pos_tag_distances_for_feature_extraction/,new2machinelearning,1425484831,"Hi folks .. i am trying to build a generic framework to extract features from product reviews (NOT those on e-commerce sites, but by reviewers on different websites), articles. blogs et al .. i have a POC where i don't use any supervision and depend totally on the structure of individual sentences (the grammar/ POS tags in them) to extract them features .. i was wondering if i could also somehow weave in distances between POS tags (after using a few 100 articles) to improve the methodology .. i am sorry if i am being vague , please let me know ..also would it make sense to use a graph to store the distance information between the tags ?",0,0,False,self,,,,,
63,MachineLearning,t5_2r3gv,2015-3-5,2015,3,5,1,2xx3p0,arxiv.org,Jouin and Mikolov: Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets,https://www.reddit.com/r/MachineLearning/comments/2xx3p0/jouin_and_mikolov_inferring_algorithmic_patterns/,improbabble,1425486023,,1,13,False,default,,,,,
64,MachineLearning,t5_2r3gv,2015-3-5,2015,3,5,1,2xx43v,davidchudzicki.com,An Interaction or Not? How a few ML Models Generalize to New Data,https://www.reddit.com/r/MachineLearning/comments/2xx43v/an_interaction_or_not_how_a_few_ml_models/,willis77,1425486212,,0,3,False,http://b.thumbs.redditmedia.com/0jBX5hwwFJCBOW92hE9J_3MfLD-IFqSPDq2eD9jJ57k.jpg,,,,,
65,MachineLearning,t5_2r3gv,2015-3-5,2015,3,5,1,2xx5dh,self.MachineLearning,What Learning Method to Apply?,https://www.reddit.com/r/MachineLearning/comments/2xx5dh/what_learning_method_to_apply/,WhereShouldILive_,1425486818,"Hi guys, so I'm somewhat of a beginner in this field, if there's a better place to ask this please let me know. 

Lets say I have a dataset that tells you if someone has fallen down today (denoted by 0 or 1 in a column). The rest of the columns are attributes of the individuals, whether they are continuous, categorical or boolean values (e.g. BAC, height, age, wearing shoes?). What would be the best method to apply if I want to predict whether someone will fall based on this data.

I would imagine that you could use a SVM by fitting the data and outcome and then predicting on a new dataset, is this a naive thought, or just wrong in general?

Also the difference between SVM and Kmeans is confusing to me (the high-level difference, not the details of how they're calculated). Is the only difference that you can't train and fit an SVM later and that Kmeans clustering all happens at once?  ",4,0,False,self,,,,,
66,MachineLearning,t5_2r3gv,2015-3-5,2015,3,5,1,2xx5pi,austinrochford.com,Maximum Likelihood Estimation of Custom Models in Python with StatsModels,https://www.reddit.com/r/MachineLearning/comments/2xx5pi/maximum_likelihood_estimation_of_custom_models_in/,clbam8,1425486968,,0,11,False,http://b.thumbs.redditmedia.com/jqC7kLcnvWABzH5oVMopd31LinRqz8kUI3plJuJfpmI.jpg,,,,,
67,MachineLearning,t5_2r3gv,2015-3-5,2015,3,5,2,2xx8rr,ppaml.galois.com,Probabilistic Programming Summer School,https://www.reddit.com/r/MachineLearning/comments/2xx8rr/probabilistic_programming_summer_school/,cscherrer,1425488405,,9,21,False,default,,,,,
68,MachineLearning,t5_2r3gv,2015-3-5,2015,3,5,2,2xxd3h,github.com,Explicit Decomposition with Neighborhoods (EDeN) in Python,https://www.reddit.com/r/MachineLearning/comments/2xxd3h/explicit_decomposition_with_neighborhoods_eden_in/,galapag0,1425490414,,0,1,False,http://b.thumbs.redditmedia.com/S7krJcmxJwkacxUwRpRtrLFON3o-XIe9QvhVuM5HbDk.jpg,,,,,
69,MachineLearning,t5_2r3gv,2015-3-5,2015,3,5,2,2xxf8g,self.MachineLearning,ML algorithm to filter explicit videos?,https://www.reddit.com/r/MachineLearning/comments/2xxf8g/ml_algorithm_to_filter_explicit_videos/,pavluk73,1425491378,"I need to implement an algorithm, preferably in Go, to filter videos based on nudity, sexuality, and general explicitness (e.g. verbal). Any pointers on where to start? I know of a few ML frameworks in Go, I'm looking for tips on what high-level approach to take. ",9,0,False,self,,,,,
70,MachineLearning,t5_2r3gv,2015-3-5,2015,3,5,4,2xxpwp,aka.ms,IoT in use at the Stanford Linear Accelerator,https://www.reddit.com/r/MachineLearning/comments/2xxpwp/iot_in_use_at_the_stanford_linear_accelerator/,MLBlogTeam,1425496127,,0,1,False,default,,,,,
71,MachineLearning,t5_2r3gv,2015-3-5,2015,3,5,5,2xy3av,sciencedaily.com,Twitter chatter predicts health insurance marketplace enrollment,https://www.reddit.com/r/MachineLearning/comments/2xy3av/twitter_chatter_predicts_health_insurance/,Captain_Filmer,1425502131,,0,1,False,http://b.thumbs.redditmedia.com/5Dz3X8Eip7S3UYnmzniKn2ul_gQOkrD88Irt92ZJFnk.jpg,,,,,
72,MachineLearning,t5_2r3gv,2015-3-5,2015,3,5,6,2xy606,self.MachineLearning,Quantifying Labeling Agreement,https://www.reddit.com/r/MachineLearning/comments/2xy606/quantifying_labeling_agreement/,iwantedthisusername,1425503362,"Hey,
So I have a task to create a classifier that describes a certain text corpus with top level labels and sub level labels. Unfortunately we also need to create the label set that describes the text as well which is a task in it's own right.

What I've ended up doing is creating a labeling tool that many labelers can use at once to create an unbiased decision matrix.
So what I have is a list of documents, and a list of the labels chosen by each labeler. 

What I want to do is create some sort of metric that describes how well labelers agree per label, so that we can remove or change ambiguous labels from the label set.

Thoughts? Is this a common task?",1,1,False,self,,,,,
73,MachineLearning,t5_2r3gv,2015-3-5,2015,3,5,8,2xynqp,self.MachineLearning,How should I choose questions for my survey?,https://www.reddit.com/r/MachineLearning/comments/2xynqp/how_should_i_choose_questions_for_my_survey/,aweeeezy,1425512384,"**Edit: I added a rough draft of my survey at the bottom...constructive criticism is welcomed!**

The project aims to draw correlations between meta characteristics of individuals and their opinion on a two-sided topic in several different ways using all the features (questions) as well as some of the features without considering the rest. Examples of ways I intend to interpret the data include:

	are respondents in support of the first view of the topic, or the other (binary classification/logistic regression)

	can the respondents opinions and meta attributes be clustered to reveal these two view points (dual centroid k-means) 

	and I dont know what else at the moment, but certainly other things as I develop the idea more fully

I guess my real question is, what types of questions should I ask and in what order so that I dont skew the results of the data.

For instance, lets say the first question asks them to classify themselves into one of these two groups explicitly, then Id be worried that they wont answer other questions as accurately because theyve been primed to think about them in the context of the first question. Alternatively, I could lead with the less obvious questions, but then they might also answer inaccurately for not understanding the appropriate context. No matter how I think about it, I see the error introduced and I was wondering if you guys have any personal suggestions or best practices for improving the quality of the data.

At this point, my conceptions about how I should administer this survey are as follows:

**DO** be very descriptive about what the survey is intended to reveal 

**DO NOT** ask multiple similar questions to reduce the chance of noise in the data

**DO NOT** ask questions that could be interpreted as threatening (causing a respondent to feel guilty and answer inaccurately)

What should I change or add?


--------------------------------------------------------


The following is a survey that will provide me with the necessary data to test and apply my knowledge of machine learning concepts and web development as well as add an element to my portfolio of coding projects. This is also for fun! The content and purpose of this survey originates from an essay prompt requiring the students of a critical thinking class to analyze the nature of mass education and its role in American society, both historically and in the future.

Please fill out all the portions of the survey to the best of your ability. Supplying an email is only necessary if, at the end of the analysis, youd like to receive a link to the pro bono results.

For the following, please rate 1-5, where 1 is strongly disagree and 5 is strongly agree.

1)	The main purpose of mass education is to produce laborers.

2)	The main purpose of mass education is to develop the interests of the individual.

3)	[Optional] If you feel neither of the statements fit your conception of the main purpose of mass education, please briefly describe it here in 50 words or less.

Please supply:

4)	City and state of your upbringing (where you spent most of your schooling)

5)	City and state of your parents upbringing

6)	Current city and state

7)	Desired location (city and state where you would most like to live)

8)	Highest educational achievement

*For these next three, try to be as general as possible (i.e. engineer instead of electrical engineer)*

9)	Worst job (if none of your jobs were that bad, pick one you imagine to be bad)

10)	Current job

11)	 Dream job

12)	Religious preference

	Christianity

	Islam

	Judaism 

	Hinduism

	Buddhism

	Atheist

	Agnostic

	Native American Religion

	New Age (humanism, transhumanism, secularism)

	Other: please specify

13)	 Ethnicity 

	Hispanic or Latino

	Middle Eastern

	Asian

	Native American

	Black

	Caucasian 

	Other: please specify

14)	Birth year

15)	 Do you have or want children?


*For the following, please rate 1-5 where 1 is strongly disagree and 5 is strongly agree.*

16)	 Education is intimately tied with many elements of society 

17)	 Education should be its own distinct facet of society

18)	 Decisions regarding changes to mass education should be handled by our representatives

19)	 Decisions regarding changes to mass education should be handled by educators 

20)	 The economy and the market should be carefully considered when discussing the state of 
mass education

21)	 Scientific data conducted by educational researchers should be considered carefully when discussing the state of mass education

22)	 Mass education doesnt help individuals target their interests 

23)	 Mass education doesnt provide individuals with the necessary skills/knowledge to obtain jobs

24)	 Overall, the quality of mass education is on the decline

25)	There currently exists untapped solutions to the problem of mass education

26)	 Good solutions to the problem of mass education requires dramatic changes

27)	The problem with poor performing schools are linked to personal life, income, race, and/or other factors unrelated to the school

28)	 The problem with poor preforming schools is unequal funding

Email (if you wish to be contacted about the results of this survey)
",4,0,False,self,,,,,
74,MachineLearning,t5_2r3gv,2015-3-5,2015,3,5,16,2y010u,hameddaily.blogspot.com,When not to use Gaussian Mixtures Model (EM clustering),https://www.reddit.com/r/MachineLearning/comments/2y010u/when_not_to_use_gaussian_mixtures_model_em/,mhfirooz,1425539281,,23,17,False,http://b.thumbs.redditmedia.com/S7pLGQjnt7b4w1SyLKqC2RkWd-XN09aozY0Aq2F4F3s.jpg,,,,,
75,MachineLearning,t5_2r3gv,2015-3-5,2015,3,5,23,2y0tgu,self.MachineLearning,Auto translation challenge level between languages,https://www.reddit.com/r/MachineLearning/comments/2y0tgu/auto_translation_challenge_level_between_languages/,ramiwi,1425564963,"What are the most machine translation ""suitable"" languages? Does google translate ""understands"" Engilsh at a better accuracy then other languages?",3,1,False,self,,,,,
76,MachineLearning,t5_2r3gv,2015-3-5,2015,3,5,23,2y0xte,randalolson.com,Using network analysis to revisit the Six Degrees of Kevin Bacon,https://www.reddit.com/r/MachineLearning/comments/2y0xte/using_network_analysis_to_revisit_the_six_degrees/,[deleted],1425567457,,3,20,False,default,,,,,
77,MachineLearning,t5_2r3gv,2015-3-6,2015,3,6,0,2y0z3b,youtube.com,AI: A Return To Meaning - David Ferrucci,https://www.reddit.com/r/MachineLearning/comments/2y0z3b/ai_a_return_to_meaning_david_ferrucci/,TheInvisibleHand89,1425568084,,1,32,False,http://b.thumbs.redditmedia.com/oYoM0zA_EtXbQ2sL4v4gcuMpw8oG5BLm4d6diRSC8XU.jpg,,,,,
78,MachineLearning,t5_2r3gv,2015-3-6,2015,3,6,0,2y13hx,xxx.tau.ac.il,Toxicity Prediction using Deep Learning,https://www.reddit.com/r/MachineLearning/comments/2y13hx/toxicity_prediction_using_deep_learning/,clbam8,1425570304,,4,23,False,default,,,,,
79,MachineLearning,t5_2r3gv,2015-3-6,2015,3,6,1,2y1cmi,mlconf.com,Who's hiring in ML,https://www.reddit.com/r/MachineLearning/comments/2y1cmi/whos_hiring_in_ml/,shonburton,1425574466,,9,5,False,default,,,,,
80,MachineLearning,t5_2r3gv,2015-3-6,2015,3,6,3,2y1pyb,drivendata.org,Countable Care: Modeling Women's Health Care competition,https://www.reddit.com/r/MachineLearning/comments/2y1pyb/countable_care_modeling_womens_health_care/,isms_,1425580372,,0,0,False,default,,,,,
81,MachineLearning,t5_2r3gv,2015-3-6,2015,3,6,12,2y3l8n,github.com,"Working Theano-based implementation of the DeepMind Atari-playing algorithm (NIPS, not Nature)",https://www.reddit.com/r/MachineLearning/comments/2y3l8n/working_theanobased_implementation_of_the/,alito,1425612029,,1,67,False,http://b.thumbs.redditmedia.com/FjA2hW53vLeVnvDkwiu34qg9Msz3cE5c9oxECPpRQhA.jpg,,,,,
82,MachineLearning,t5_2r3gv,2015-3-6,2015,3,6,18,2y4d6s,cs224d.stanford.edu,Stanford University: Deep Learning for NLP - course by Richard Socher,https://www.reddit.com/r/MachineLearning/comments/2y4d6s/stanford_university_deep_learning_for_nlp_course/,petrux,1425632528,,24,144,False,http://b.thumbs.redditmedia.com/GCyyuJwwGt_MaRhPybCyWF43q6co3hpQE17zoB9y3Ac.jpg,,,,,
83,MachineLearning,t5_2r3gv,2015-3-6,2015,3,6,22,2y4u1o,self.MachineLearning,[Question] What is the scientific name of the techniques that are used to generate questions from text,https://www.reddit.com/r/MachineLearning/comments/2y4u1o/question_what_is_the_scientific_name_of_the/,SomeoneisWondering,1425649223,"I'd like to generate questions from text. When I was looking for papers that describe these techniques, I couldn't find a lot of them. Can anyone help explain how can I reverse the text to generate all possible questions. 

Let's say.
""John is eating an apple in the kitchen""
Possible question:
- Who is eating the apple?
- Where do John eat apple?
- What do John eat? 

etc..",4,7,False,self,,,,,
84,MachineLearning,t5_2r3gv,2015-3-6,2015,3,6,22,2y4v31,self.MachineLearning,Jumping off point: classification problem (I think),https://www.reddit.com/r/MachineLearning/comments/2y4v31/jumping_off_point_classification_problem_i_think/,177mph,1425649968,"Hey all:

I just picked up a new contract for a law firm that involves text classification and am looking for some advice before getting started with the specification.  

Every day, contractors (1099s) for the company attend court proceedings all over the country and record the results of the proceeding.  They typically write down the details in paragraph format.  This includes things like case number, sentencing info (if applicable), disposition, comments the judge made, charges (if applicable - sometimes not if civil proceeding), etc...  When the day closes, the contractor emails in their typed version of the notes they made at the proceeding to a data entry pool which separates out the relevant info and enters it into the company's system which stores it to the RDBMS.  The company would like to have a classification system take a 'first pass' at the paragraph text and attempt to classify the information as their appropriate type (e.g. - the followings sentences are disposition information, these sentences are sentencing info, etc...).  A human would still examine and confirm the machine's selections but it would save hundreds of hours per week in transcribing if it was right just some of the time.     

I am fortunate (I guess...) in that I have significant corpus (literally millions or tens of millions of examples).  For a proof of concept, I went through and pulled the last 2.5 million sentences, stripped out the stop words and broke the sentences into uni-grams and bi-grams.  I then did the same with an example email.  I replaced things dates and dollar amounts with some static value (|DOLLARS| or |DATE|) since they are variable and don't really tell me anything. I have around  10000K unique bi-grams after doing all of this from the corpus data.   

This is where I'm stuck.  Most classification samples I've seen appear to be deciding 'which of two types but it's definitely one of these two...'.  In my sample, something could be one of several different types or none of the types at all.  (EX: The sentence ""The judge awarded the plaintiff attorney fees and default judgment"" has a high likelihood of belonging to one of my classifications whereas the sentence ""I couldn't hear the plaintiff attorney's comments due to a loud heater element"" belongs to nothing. )  What I'm looking for is some way to generate a ""confidence"" number that some sentence is likely some classification.  I don't need to worry about predicting anything from the data, just classification.  I have significant server resources available and things like storage, memory or processors aren't really a concern (meaning if my solution was resource-intensive, they'd spend the money to make it work).  What types of algorithms should I be looking at?  Is there any way to better approach this I'm not seeing?

Thanks guys!



    ",5,4,False,self,,,,,
85,MachineLearning,t5_2r3gv,2015-3-6,2015,3,6,23,2y4z7a,arxiv.org,Rectified Factor Networks,https://www.reddit.com/r/MachineLearning/comments/2y4z7a/rectified_factor_networks/,oclev,1425652602,,6,7,False,default,,,,,
86,MachineLearning,t5_2r3gv,2015-3-7,2015,3,7,1,2y5dma,self.MachineLearning,Batch normalization and PRelu successes?,https://www.reddit.com/r/MachineLearning/comments/2y5dma/batch_normalization_and_prelu_successes/,spurious_recollectio,1425660093,"I'm considering implementing batch normalization (http://arxiv.org/abs/1502.03167) and parametric relu (http://arxiv.org/pdf/1502.01852) in my NN library and was just wondering how useful people how found them.  E.g. is batch normalization as spectacular at reducing training time even on smaller networks than GoogleNet?  I would love to be able to get reasonable imagenet performance on a single GPU in a day or two (i.e. something like Alexnet with batch normalization).  

Also has anyone tried these for other kinds of networks?  I'm wondering if e.g. batch normalization might mitigate problems in deep RNNs (by renormalizing up the gradient along a time series).  I'd love to hear what people's experiences have been with these (and other recent) techniques. ",25,12,False,self,,,,,
87,MachineLearning,t5_2r3gv,2015-3-7,2015,3,7,5,2y64mv,self.MachineLearning,Implementation of convolutional neural networks for text classification,https://www.reddit.com/r/MachineLearning/comments/2y64mv/implementation_of_convolutional_neural_networks/,elsonidoq,1425672885,"Hi all!

I'm looking for an implementation of convolutional neural networks for text classification. 

I want to be able to compute a distributed representation of a sentence and use that distributed representation to preform either a classification task, or similarity computation.

Does anyone know an implementation of it? 
I'm struggling because from what I see most implementations assume that the input has constant size.

Thanks a lot!

Pablo",12,4,False,self,,,,,
88,MachineLearning,t5_2r3gv,2015-3-7,2015,3,7,5,2y661n,databoys.github.io,"Follow-up to my previous post, achieving the LeCun MLP benchmark with our simple neural network.",https://www.reddit.com/r/MachineLearning/comments/2y661n/followup_to_my_previous_post_achieving_the_lecun/,yourbuddyflo,1425673502,,5,11,False,http://b.thumbs.redditmedia.com/yeypdo1qMucAypOLst4lnFXqqwOXw33wPdsewfiOD7Q.jpg,,,,,
89,MachineLearning,t5_2r3gv,2015-3-7,2015,3,7,6,2y6cll,self.MachineLearning,How do I get my model to perform better?,https://www.reddit.com/r/MachineLearning/comments/2y6cll/how_do_i_get_my_model_to_perform_better/,Nixonite,1425676543,"The purpose of this thread is to ask if I have the right problem-solving or data scientist mindset here. I'm new to ML and data mining, only at the level of Introduction to Statistical Learning so although the algorithms are clearly presented, my data-problem-solving skills are underdeveloped. 

So I have time series data where an event takes place periodically in it. I can somewhat tell with clustering (or just graphing in 3d scatter plots) where in the x,y,z coordinate system this 'event' falls into since this is sensor output... but my problem is that my classifiers/models can only barely get above random predictions. 

11 classifiers should be about 9% prediction rate by average right? Mine get about maybe 16% using logistic regression. 

I tried taking out some low variance features and it's still the same. I tried using decision trees and random forests but it's worse. I have to predict one of eleven classes, and I have about 7 dimensions of data which can be reduced to 5 or maybe less considering the low variance in some, and I'm guessing my next step is to redo some preprocessing. 

My labels for classes are numeric, 0 - 10 which I think may be contributing some unrelated attribute to the real data, thus I come to mind that I may need to redo the labels perhaps with onehotencoding. This means to make it so that the 0 class would be something like 

&lt; 1, 0, 0, 0, .... 0&gt; and the 10 class would be &lt; 0, 0, .... 1&gt; right?

I just wanted to say all of this and ask if I have the right mindset here when it comes to problem solving? Am I going about solving this problem like a normal data analyst or am I just handwaving here? My data set is in the tens of thousands of records considering the sensor has a high sample rate. 

Thanks for the time and input.",11,0,False,self,,,,,
90,MachineLearning,t5_2r3gv,2015-3-7,2015,3,7,7,2y6mrw,self.MachineLearning,Dynamic NN Sizing [Question],https://www.reddit.com/r/MachineLearning/comments/2y6mrw/dynamic_nn_sizing_question/,jrkirby,1425681550,"I'm interested in reading about algorithms which create and train dynamically sized Neural Nets. Things like adding extra nodes or even layers during training to reduce error, or removing connections with low weights in order to minimize computation. Anything similar to this would be interesting to me.

Can anybody point me to papers or blogs describing this sort of thing? Thanks all.",5,5,False,self,,,,,
91,MachineLearning,t5_2r3gv,2015-3-7,2015,3,7,8,2y6rsp,arxiv.org,Domain-Adversarial Neural Networks,https://www.reddit.com/r/MachineLearning/comments/2y6rsp/domainadversarial_neural_networks/,alexmlamb,1425684150,,8,11,False,default,,,,,
92,MachineLearning,t5_2r3gv,2015-3-7,2015,3,7,12,2y7f18,self.MachineLearning,Noobie - First Model,https://www.reddit.com/r/MachineLearning/comments/2y7f18/noobie_first_model/,browns2112,1425697577,"Hi all.  I am using R and the caret package.  I have created my first model.

I have:

3695 samples

* 28 predictors
*  2 classes: 'N', 'Y'

Here is the basic code:

train_control &lt;- trainControl(method = ""repeatedcv"", number = 10, repeats = 3)

fit &lt;- train(eval ~ ., trControl = train_control, method = ""treebag"", data = trainWork)

&gt;fit

Resampling results

  Accuracy  Kappa    Accuracy SD    Kappa SD

    0.802   0.588   0.0231       0.0491 

&gt; confusionMatrix(fit)

Cross-Validated (10 fold, repeated 3 times) Confusion Matrix 

(entries are percentages of table totals)
 
Reference

Prediction      N         Y

      N   30.4    8.4

      Y   11.5   49.8

Does it appear I am on the correct path?
 
Does my model appear to be ""good"".  Have I supplied enough information?

 Any thoughts or suggestions on how to make it better?

Thanks in advance.",1,3,False,self,,,,,
93,MachineLearning,t5_2r3gv,2015-3-7,2015,3,7,13,2y7p5l,self.MachineLearning,ML Background for CS undergrad,https://www.reddit.com/r/MachineLearning/comments/2y7p5l/ml_background_for_cs_undergrad/,felm125,1425703975,"Hi,

So I have taken the introductory ML class at UIUC which covered many basic topics in ML (can be seen here https://courses.engr.illinois.edu/cs446/syllabus.html). I greatly enjoyed the course and hope to get into ML research in the fall semester (I am currently a sophomore). I have 2 questions:

1. What are some topics I should explore or books I should read to prepare for doing ML research? I don't have a specific area in mind yet, so I'd like to explore a breadth of concepts. What are some open topics of research?

2. What are some must take math or CS courses to complement ML? So far I have taken theory of probability, Linear Algebra, basic algorithms, Calc sequence. I plan on adding advanced algorithms, DiffEQ and a graduate level NLP course, but I'm guessing I need a stronger background in statistics and mathematics? And if so which specific courses will be useful?

Thanks in advance!",4,7,False,self,,,,,
94,MachineLearning,t5_2r3gv,2015-3-7,2015,3,7,17,2y83f4,nbviewer.ipython.org,Kalman and Bayesian Filters in Python,https://www.reddit.com/r/MachineLearning/comments/2y83f4/kalman_and_bayesian_filters_in_python/,cast42,1425715366,,3,48,False,default,,,,,
95,MachineLearning,t5_2r3gv,2015-3-7,2015,3,7,18,2y896s,youtube.com,Bio Matrics Attandence system- Corporate Services,https://www.reddit.com/r/MachineLearning/comments/2y896s/bio_matrics_attandence_system_corporate_services/,thecorporateservices,1425722072,,1,1,False,default,,,,,
96,MachineLearning,t5_2r3gv,2015-3-7,2015,3,7,19,2y8cg3,self.MachineLearning,What are some advanced [math] topics useful in ML?,https://www.reddit.com/r/MachineLearning/comments/2y8cg3/what_are_some_advanced_math_topics_useful_in_ml/,barmaley_exe,1425725943,"We all know Linear Algebra, Calculus, Probability, Stats and Optimization theory are the foundation of the ML. (Notice I omitted Differential Equations. If you know they're used somewhat extensively in the ML, please, correct me in the comments).

But I'm sure there are other math subjects that might be less frequently used, but still are useful to know, especially if one is interested in research. Things like:

 * [Differential Geometry](http://www.metacademy.org/roadmaps/rgrosse/dgml)
 * [Dynamical Systems](http://www.metacademy.org/roadmaps/DanielIm/dsml)

For example, I've got an impression that Bayesian Machine Learning is somewhat influenced by Statistical Physics. Is it true? Is it beneficial to study StatPhys (for example, Prof. Hinton [uses](http://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/clyoatr) physical intuition to reason about ML models). I'd like to hear your opinions.",21,19,False,self,,,,,
97,MachineLearning,t5_2r3gv,2015-3-7,2015,3,7,23,2y8p3w,self.MachineLearning,How do i fit this dataset?,https://www.reddit.com/r/MachineLearning/comments/2y8p3w/how_do_i_fit_this_dataset/,soulslicer0,1425738390,"http://imgur.com/w3hOgdQ

As you can see, as I move up the Y Axis, the graph plane becomes from cubic to more linear. Is there any special planar equation that can map this?",4,0,False,self,,,,,
98,MachineLearning,t5_2r3gv,2015-3-7,2015,3,7,23,2y8pny,self.MachineLearning,Help with transfer learning.,https://www.reddit.com/r/MachineLearning/comments/2y8pny/help_with_transfer_learning/,daithibowzy,1425738798,"Hello, my supervisor has suggested I start looking at transfer learning as part of my PhD, but I'm not too sure where to start. Can anyone point me in the direction of some good workshops or tutorials on it?",5,4,False,self,,,,,
99,MachineLearning,t5_2r3gv,2015-3-8,2015,3,8,1,2y93vs,groups.inf.ed.ac.uk,ML and CV video lectures,https://www.reddit.com/r/MachineLearning/comments/2y93vs/ml_and_cv_video_lectures/,mmahesh,1425747557,,0,34,False,default,,,,,
100,MachineLearning,t5_2r3gv,2015-3-8,2015,3,8,2,2y97jz,self.MachineLearning,Annotating Image segments? (deep learning?),https://www.reddit.com/r/MachineLearning/comments/2y97jz/annotating_image_segments_deep_learning/,newGuyML,1425749473,"Hey, I'm looking to learn more about learning to segment and semantically annotate images. Like, if there is a dog and a bowl in an image, the algorithm can find the region of pixels corresponding to dog and region of pixels corresponding to bowl and label as such.

I was told deep learning can do this but I'm kind of new to this. Any pointers will be appreciated.",0,7,False,self,,,,,
101,MachineLearning,t5_2r3gv,2015-3-8,2015,3,8,4,2y9krl,self.MachineLearning,Why do some models work better than others,https://www.reddit.com/r/MachineLearning/comments/2y9krl/why_do_some_models_work_better_than_others/,dileep31,1425756285,"For example, is there a theoretical explanation for why a random forest works better at a place compared to a gbm, and the other way in some other places? Without that explanation, I am quite confused about how to pick one of the existing hundreds of models. I mean how can I ever be sure that the model I picked is the best, without trying some ten different models?",10,12,False,self,,,,,
102,MachineLearning,t5_2r3gv,2015-3-8,2015,3,8,7,2ya29i,infoq.com,Machine Learning for Programming by Peter Norvig,https://www.reddit.com/r/MachineLearning/comments/2ya29i/machine_learning_for_programming_by_peter_norvig/,galapag0,1425765612,,6,80,False,default,,,,,
103,MachineLearning,t5_2r3gv,2015-3-8,2015,3,8,12,2yaxqq,csie.ntu.edu.tw,"Kaggle prize winning library for ""field-aware factorization machines"" - LIBFFM",https://www.reddit.com/r/MachineLearning/comments/2yaxqq/kaggle_prize_winning_library_for_fieldaware/,kunjaan,1425784227,,2,19,False,default,,,,,
104,MachineLearning,t5_2r3gv,2015-3-8,2015,3,8,12,2yazqv,databricks.com,DataFrames in Spark for Large Scale Data Science,https://www.reddit.com/r/MachineLearning/comments/2yazqv/dataframes_in_spark_for_large_scale_data_science/,kunjaan,1425785478,,2,21,False,http://b.thumbs.redditmedia.com/1W0KhfVg-CBhAqT8fYFZO5RSrPC4vWrrrn3UcBQrS5s.jpg,,,,,
105,MachineLearning,t5_2r3gv,2015-3-8,2015,3,8,12,2yazxj,github.com,Kayak: Library for Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/2yazxj/kayak_library_for_deep_neural_networks/,kunjaan,1425785600,,15,34,False,http://b.thumbs.redditmedia.com/B3YziAxspPOCJzkagpbh23mLE6FZgKjGsVEcw9csh9c.jpg,,,,,
106,MachineLearning,t5_2r3gv,2015-3-8,2015,3,8,22,2yc3tc,arxiv.org,Big Learning with Bayesian Methods,https://www.reddit.com/r/MachineLearning/comments/2yc3tc/big_learning_with_bayesian_methods/,Pillowrath,1425822537,,5,37,False,default,,,,,
107,MachineLearning,t5_2r3gv,2015-3-9,2015,3,9,7,2ydlmq,self.MachineLearning,Looking for deep NN code for regression?,https://www.reddit.com/r/MachineLearning/comments/2ydlmq/looking_for_deep_nn_code_for_regression/,TheInfelicitousDandy,1425852360,"I'm looking for NN code that does regression (with squared error loss function).  I could use theano and build something myself but why do that if it already exists, especially if it has some extra features like dropout or relu etc.  Everything I can find is set up for classification.  
",7,1,False,self,,,,,
108,MachineLearning,t5_2r3gv,2015-3-9,2015,3,9,7,2ydrbk,racketracer.wordpress.com,Predicting and Plotting Crime in Seattle,https://www.reddit.com/r/MachineLearning/comments/2ydrbk/predicting_and_plotting_crime_in_seattle/,racketracer,1425855278,,0,0,False,http://b.thumbs.redditmedia.com/_UJG1_N3NN4-vxRB_b52Xa_um51Wvz-nhVyAhrhGyPA.jpg,,,,,
109,MachineLearning,t5_2r3gv,2015-3-9,2015,3,9,16,2yf6o2,lumidatum.typeform.com,Help us with your knowledge understand the field of ML and Data Science!,https://www.reddit.com/r/MachineLearning/comments/2yf6o2/help_us_with_your_knowledge_understand_the_field/,[deleted],1425886809,,3,0,False,default,,,,,
110,MachineLearning,t5_2r3gv,2015-3-9,2015,3,9,16,2yf7tf,datanami.com,The 3 Key Steps to Building a Predictive App with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/2yf7tf/the_3_key_steps_to_building_a_predictive_app_with/,[deleted],1425887951,,0,0,False,default,,,,,
111,MachineLearning,t5_2r3gv,2015-3-9,2015,3,9,21,2yfmij,self.MachineLearning,[Request] Studies that apply deep learning to genotype and phenotype for marker discovery,https://www.reddit.com/r/MachineLearning/comments/2yfmij/request_studies_that_apply_deep_learning_to/,alexpheno,1425902657,"I'm interested in research that applies machine learning to find disease markers, this is one example:
http://www.sciencemag.org/content/347/6218/1254806
Please advise if you are aware of similar research.",5,2,False,self,,,,,
112,MachineLearning,t5_2r3gv,2015-3-9,2015,3,9,21,2yfn9p,self.MachineLearning,Does anyone work on deep learning in CMU?,https://www.reddit.com/r/MachineLearning/comments/2yfn9p/does_anyone_work_on_deep_learning_in_cmu/,YesIAmTheMorpheus,1425903232,In Carnegie Mellon ,1,1,False,self,,,,,
113,MachineLearning,t5_2r3gv,2015-3-9,2015,3,9,21,2yfpgt,hyperopt.github.io,Hyperopt: Distributed Asynchronous Hyperparameter Optimization in Python,https://www.reddit.com/r/MachineLearning/comments/2yfpgt/hyperopt_distributed_asynchronous_hyperparameter/,galapag0,1425904911,,13,29,False,default,,,,,
114,MachineLearning,t5_2r3gv,2015-3-9,2015,3,9,23,2yg0tx,web.archive.org,All machine learning models have flaws,https://www.reddit.com/r/MachineLearning/comments/2yg0tx/all_machine_learning_models_have_flaws/,rhiever,1425911829,,5,33,False,default,,,,,
115,MachineLearning,t5_2r3gv,2015-3-9,2015,3,9,23,2yg2y3,self.MachineLearning,970 SLI for deep learning?,https://www.reddit.com/r/MachineLearning/comments/2yg2y3/970_sli_for_deep_learning/,spurious_recollectio,1425913019,"This might be an overly technical (in a systems sort of way) question for this site but I'm considering upgrading an old system (Asus P8P67 pro motherboard) by adding two gtx 970s in SLI (the system has a i7 2600k cpu). 

My question has two parts: 

1. how well does the 970 work in SLI for deep learning (I use a single card right now on another system and am happy with it so far).  So far my nets haven't really been memory limited but that might change.  I imagine initialy I'll use the two GPUs to run independent experiments.

2. Will my old mobo present a bandwidth bottleneck?  I think it supports pci-e 2 16x which will become x8/x8 if I use two GPUs.  My concern is that's its PCI 2 not 3.

Once again sorry if this question is too specific for this subreddit.

",15,14,False,self,,,,,
116,MachineLearning,t5_2r3gv,2015-3-10,2015,3,10,0,2yg599,self.MachineLearning,RNN with LSTM with hyperparameter optimization and simple API?,https://www.reddit.com/r/MachineLearning/comments/2yg599/rnn_with_lstm_with_hyperparameter_optimization/,boltzmannbrain637,1425914253,"I am looking to enhance a product with LSTM RNNs for time series prediction, but I have not been able to find any good software packages or libraries. By ""good"", I mean that are advanced yet offer out-of-the-box hyperparameter optimization and a simple API. The time series prediction will be offered as a service, so the license just has to be compatible for that. Does anyone have any leads? I'm even open to using/licensing a software package that is published by an individual.

My experiences with attempting to setup PyBrain have not been good.

Thanks in advance.",4,0,False,self,,,,,
117,MachineLearning,t5_2r3gv,2015-3-10,2015,3,10,3,2ygxqr,austinrochford.com,Robust Regression with t-Distributed Residuals,https://www.reddit.com/r/MachineLearning/comments/2ygxqr/robust_regression_with_tdistributed_residuals/,clbam8,1425927594,,2,11,False,http://b.thumbs.redditmedia.com/o2TaOIMWvIYQT8wL1lsB-pyyHyWc0xq-YmFdfQ7xo5Y.jpg,,,,,
118,MachineLearning,t5_2r3gv,2015-3-10,2015,3,10,6,2yhev4,xxx.tau.ac.il,Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Brain's Ventral Visual Pathway,https://www.reddit.com/r/MachineLearning/comments/2yhev4/deep_neural_networks_reveal_a_gradient_in_the/,downtownslim,1425934938,,10,9,False,default,,,,,
119,MachineLearning,t5_2r3gv,2015-3-10,2015,3,10,7,2yhp6d,ai-maker.com,The genetic algorithms explained (x-post /r/compsci),https://www.reddit.com/r/MachineLearning/comments/2yhp6d/the_genetic_algorithms_explained_xpost_rcompsci/,ai_maker,1425939459,,0,10,False,http://b.thumbs.redditmedia.com/L-GQ6Q1MSYXKmGaWVhXSz3j7JkCsWiGRg5kCi0vCzgo.jpg,,,,,
120,MachineLearning,t5_2r3gv,2015-3-10,2015,3,10,13,2yizqd,mytechlogy.com,Top 9 self learning courses in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/2yizqd/top_9_self_learning_courses_in_machine_learning/,jagran1,1425963303,,0,0,False,default,,,,,
121,MachineLearning,t5_2r3gv,2015-3-10,2015,3,10,15,2yj97q,arxiv-web3.library.cornell.edu,Encoding Source Language with Convolutional Neural Network for Machine Translation,https://www.reddit.com/r/MachineLearning/comments/2yj97q/encoding_source_language_with_convolutional/,iori42,1425970503,,0,2,False,default,,,,,
122,MachineLearning,t5_2r3gv,2015-3-10,2015,3,10,15,2yj98a,timdettmers.wordpress.com,A Full Hardware Guide to Deep Learning,https://www.reddit.com/r/MachineLearning/comments/2yj98a/a_full_hardware_guide_to_deep_learning/,iori42,1425970517,,17,75,False,http://b.thumbs.redditmedia.com/MnfEY7oIqvRMfjst733BiQAVvhdFuFMa_eAqfB86pdk.jpg,,,,,
123,MachineLearning,t5_2r3gv,2015-3-10,2015,3,10,22,2yjzr9,arxiv.org,"Distilling knowledge of Neural Nets, by Geoffery Hinton, Vinyals and Jeff Dean",https://www.reddit.com/r/MachineLearning/comments/2yjzr9/distilling_knowledge_of_neural_nets_by_geoffery/,[deleted],1425993655,,0,1,False,default,,,,,
124,MachineLearning,t5_2r3gv,2015-3-10,2015,3,10,23,2yk3wk,webcodegeeks.com,Python: scikit-learn  Training a classifier with non numeric features | Web Code Geeks,https://www.reddit.com/r/MachineLearning/comments/2yk3wk/python_scikitlearn_training_a_classifier_with_non/,cast42,1425996112,,0,14,False,http://b.thumbs.redditmedia.com/73PXyb4SSX5aQL6hNAnSC7V0Lt4mcECt1HU_nlcVwTg.jpg,,,,,
125,MachineLearning,t5_2r3gv,2015-3-10,2015,3,10,23,2yk6a2,arxiv.org,"Distilling knowledge in Neural Nets, by Geoffery Hinton, Vinyals and Jeff Dean",https://www.reddit.com/r/MachineLearning/comments/2yk6a2/distilling_knowledge_in_neural_nets_by_geoffery/,muktabh,1425997421,,15,40,False,default,,,,,
126,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,1,2ykk1b,blogs.technet.com,Convolutional Neural Nets in Net#,https://www.reddit.com/r/MachineLearning/comments/2ykk1b/convolutional_neural_nets_in_net/,MLBlogTeam,1426004139,,0,1,False,http://b.thumbs.redditmedia.com/ftHpWQAnv3s0dZOYmKtT5se6GYnxC1PtnSwz-u2EnQg.jpg,,,,,
127,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,1,2yko1x,kickstarter.com,Project Markov - Easy to follow AI tutorials and simple GUI model builder with cloud support,https://www.reddit.com/r/MachineLearning/comments/2yko1x/project_markov_easy_to_follow_ai_tutorials_and/,[deleted],1426005934,,2,2,False,default,,,,,
128,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,2,2yktab,self.MachineLearning,A question to the subreddit about our efficiency at keeping up with the deep learning literature,https://www.reddit.com/r/MachineLearning/comments/2yktab/a_question_to_the_subreddit_about_our_efficiency/,thatguydr,1426008152,"I'm a data scientist and I use deep learning techniques both at my workplace and at home. This is a very fast-moving field, obviously, and I typically use a combination of this subreddit and the G+ deep learning forum to keep up with the state of the art. The two communities (as a whole) seem reasonably good at filtering out a lot of the most important work.

That having been said, there's always a paper or four that slips by my attention. I could sit on arXiv every day, but there must be a more efficient solution.

For those of you who are good at keeping up with the literature: how do you do it? Do you use any filter sites? And do you have any tips for efficiency?",8,9,False,self,,,,,
129,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,2,2ykxs6,blog.bigml.com,The Need for Machine Learning is Everywhere!,https://www.reddit.com/r/MachineLearning/comments/2ykxs6/the_need_for_machine_learning_is_everywhere/,czuriaga,1426010089,,0,0,False,http://b.thumbs.redditmedia.com/68PQJq9--Z0jQGOzwGtzqCNlRz35EzzDEsOemBqKbBw.jpg,,,,,
130,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,4,2yl8gs,self.MachineLearning,Use Time-series analysis or not?,https://www.reddit.com/r/MachineLearning/comments/2yl8gs/use_timeseries_analysis_or_not/,zedoul,1426014687,"Hi there,

I would ask a general question that which sort of data should NOT be analysed by a time-series approach - either Box-Jenkins and state-space models.

If I understand the approach correctly, it seems that time-series model can be applied to almost everything that has observable variables. Some of the requirements would be that they should be stationary and have a clear correlation between current and previous one. However, since I need to apply it on real data, I would like to know how to determine it. 

It would be also nice if somebody let me know if there is a nice book and/or references on this problem.
",7,4,False,self,,,,,
131,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,5,2ylg83,self.MachineLearning,Abstract theory of bias and variance,https://www.reddit.com/r/MachineLearning/comments/2ylg83/abstract_theory_of_bias_and_variance/,doubleFisted33,1426017912,"Has anyone seen an abstract treatment of bias versus variance?  The more abstract the better.  I use category theory in my research, so nothing would be too abstract for me.",9,3,False,self,,,,,
132,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,5,2ylgkp,thinktostart.com,The Key to Digital Transformation: Machine Learning,https://www.reddit.com/r/MachineLearning/comments/2ylgkp/the_key_to_digital_transformation_machine_learning/,julhillebrand,1426018047,,1,0,False,http://b.thumbs.redditmedia.com/22qJ5U-iL47LrOdgtlnDStSkWdey9nPzeqzpKZUGo0I.jpg,,,,,
133,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,8,2ym4yw,voidpatterns.org,ForecastThis DSX: predictive modeling of political statements,https://www.reddit.com/r/MachineLearning/comments/2ym4yw/forecastthis_dsx_predictive_modeling_of_political/,voidpatterns,1426028424,,0,0,False,http://b.thumbs.redditmedia.com/nyUgzWmZk6wL_VxXNO7vBaANnfwL0_ypgYKj8fHh9ts.jpg,,,,,
134,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,10,2ympwb,comp.nus.edu.sg,SINGA: A Distributed Deep Learning Platform,https://www.reddit.com/r/MachineLearning/comments/2ympwb/singa_a_distributed_deep_learning_platform/,improbabble,1426038294,,3,19,False,http://a.thumbs.redditmedia.com/LuWwUSMAQfYQcCwLY4lSWoI3cnO06sB-9wA6x2mANR0.jpg,,,,,
135,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,13,2yn7l3,self.MachineLearning,Characterizing an attractor state.,https://www.reddit.com/r/MachineLearning/comments/2yn7l3/characterizing_an_attractor_state/,[deleted],1426047533,"Hey everyone.  This might not be fully appropriate for this sub, but it came up in my work in relation to machine learning techniques and I thought you guys and gals might have some insight into the problem. Again: not necessarily a machine learning question, but perhaps close enough to home. 

  Say you have very high dimensional time course data (or a collection of pairwise dot products amongst vectors in an unknown space, where those vectors are ordered in time; this is actually my problem, but talking about approaches in some well defined space would be helpful too) and you believe that an attractor state exists in some subset of these dimensions.  How do you prove this? Maybe more simply put, how do you demonstrate that 1) the attractor exists and 2) if we're dealing with dot products from vectors in an unknown space, how do we demonstrate this without embedding the data in an arbitrary dimensional space? ",0,1,False,default,,,,,
136,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,13,2yn8ly,self.MachineLearning,Using ML for targeted marketing,https://www.reddit.com/r/MachineLearning/comments/2yn8ly/using_ml_for_targeted_marketing/,watersign,1426048152,"Hi guys,
I know some of you maybe using mL for marketing purposes and was hoping to get a discussion started regarding the type of results you've seen (if any) and any tricks/tips you'd like to share. Im a big fan of decision trees (CART, C4.5, randForests,etc) and would like to discuss different methods as I know deep learning is very big right now..curious to see if anyone is using it for marketing purposes. Would also like to talk about strategy/implementation as well. So what's up? ",4,0,False,self,,,,,
137,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,16,2ynlkj,self.MachineLearning,How to learn invariances when the training set doesn't have all possible invariances as examples?,https://www.reddit.com/r/MachineLearning/comments/2ynlkj/how_to_learn_invariances_when_the_training_set/,[deleted],1426057596,"Consider a 3x3 grid which can be black or white in each cell. Given two examples of the letter L written in such a grid, we can infer other ways of coloring the cells to write L. We can imagine transpositions, translations, and rotations which look like L in that grid.

Are there classifiers which can learn such invariances even if the training set do not contain all such transpositions, translations, and rotations?",4,2,False,default,,,,,
138,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,16,2ynlua,self.MachineLearning,"I'm looking for an article I read recently about a specific machine learning algorithm but can't find it, maybe you can help.",https://www.reddit.com/r/MachineLearning/comments/2ynlua/im_looking_for_an_article_i_read_recently_about_a/,kensaggy,1426057863,"Hey,

I recently came a cross and article which described some ML algo. that basically [read: roughly] (from what I remember), learned letter frequencies in wikipedia, and then after learning, could generate text letter-by-letter forming a complete paragraph.. which while may have not made sense, in it's correctness, it was rather good

I've tried Googling for it, checking my history, everything - all the keywords that are relevant are to generic to find the specific article

If anyone know's what I'm talking about or may have recently read it I would love to find it again :)

Thanks in advance,
Ken.

P.S.
I'm Ken - new to the group.. hope I'm not violating any rules here.. (If I am, terribly sorry and i'll delete it)",5,0,False,self,,,,,
139,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,17,2yns0b,arxiv.org,Neural Responding Machine for Short-Text Conversation,https://www.reddit.com/r/MachineLearning/comments/2yns0b/neural_responding_machine_for_shorttext/,[deleted],1426064059,,1,1,False,default,,,,,
140,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,18,2ynvg7,kachkach.com,Machine Learning and data-processing with Python (x-post /r/Python),https://www.reddit.com/r/MachineLearning/comments/2ynvg7/machine_learning_and_dataprocessing_with_python/,halflings,1426067550,,0,6,False,http://b.thumbs.redditmedia.com/fZHs6T4jvLpyW9_jw5KM_c8_lKlh5WQtwL9L84Jw9xo.jpg,,,,,
141,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,18,2ynvsa,self.MachineLearning,Learning with ambiguous labels,https://www.reddit.com/r/MachineLearning/comments/2ynvsa/learning_with_ambiguous_labels/,Chobeat,1426067889,"Greetings /r/MachineLearning 

I'm writing my thesis and it happens to be about a SVM+SVR algorithm that try to solve the problem of recognizing real ambiguous examples from misclassified examples with ambiguous labels. I've already found a couple of papers on the subject but I would like to have suggestions on where to look to find more diverse solutions (the ones I found are all SVM solutions) and compare my algorithm with their performance.

Thanks in advance.",0,0,False,self,,,,,
142,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,19,2ynw3w,arxiv-web3.library.cornell.edu,Neural Responding Machine for Short-Text Conversation,https://www.reddit.com/r/MachineLearning/comments/2ynw3w/neural_responding_machine_for_shorttext/,I_ai_AI,1426068182,,10,5,False,default,,,,,
143,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,19,2ynxbv,self.MachineLearning,Suggestions for a good starter set?,https://www.reddit.com/r/MachineLearning/comments/2ynxbv/suggestions_for_a_good_starter_set/,greenkiweez,1426069396,"Hi,
This semester, I've taken the course Data mining. I study computer programming and this subject is new to me. We need to pick our own datasets on which we will be working the whole semester. 
What i am asking is: what kind of problems are good for a beginner? How difficult can it be for example analysing demographic migrations of a country?
Thanx for taking your time.",2,0,False,self,,,,,
144,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,19,2ynz2r,nuit-blanche.blogspot.com,"Slides available and streaming inforomation for Paris Machine Learning Meetup #7 Season 2: Automatic Statistician, ML et Entreprise, Algo Fairness/ Certifying and removing Disparate Impact",https://www.reddit.com/r/MachineLearning/comments/2ynz2r/slides_available_and_streaming_inforomation_for/,compsens,1426071116,,0,2,False,http://b.thumbs.redditmedia.com/na48EN_mKtm95h2cxBTXksLQAYuw-7ZSi_k3aLzkj-Q.jpg,,,,,
145,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,20,2yo2xr,nikhilbuduma.com,The Curse of Dimensionality and the Autoencoder,https://www.reddit.com/r/MachineLearning/comments/2yo2xr/the_curse_of_dimensionality_and_the_autoencoder/,iori42,1426074537,,8,9,False,http://b.thumbs.redditmedia.com/zB4kV9V1Baefg5vOaMh_ARYG1qR4S6MsptXHYzlL1Fc.jpg,,,,,
146,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,22,2yob89,searchdatamanagement.techtarget.com,What's actually new for machine learning applications of big data,https://www.reddit.com/r/MachineLearning/comments/2yob89/whats_actually_new_for_machine_learning/,lefthandben,1426080279,,0,0,False,http://b.thumbs.redditmedia.com/7j8igkHYn7ldZ4Lh-ubWUoQsP9oDLpkUil4QDZ0pw6M.jpg,,,,,
147,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,22,2yocxj,tjo-en.hatenablog.com,Machine learning for package users with R (0): prologue - describing classifiers based on their decision boundary,https://www.reddit.com/r/MachineLearning/comments/2yocxj/machine_learning_for_package_users_with_r_0/,TJO_datasci,1426081298,,0,0,False,http://a.thumbs.redditmedia.com/gQTiafW9pLwOLd-0v4IDKzxv2RWGd7Sm8svJJwRb_-4.jpg,,,,,
148,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,23,2yog7k,self.MachineLearning,Challenge - basic deep learning speech recognition in 3 weeks.,https://www.reddit.com/r/MachineLearning/comments/2yog7k/challenge_basic_deep_learning_speech_recognition/,Tom-Demijohn,1426083094,"Hello! I am deep learning newbie. So far I covered some tutorials in NN/ML ([tutorial for beginners](http://neuralnetworksanddeeplearning.com/) , some of Hinton's MOOC and A. Ng's MOOC)  and since I have 3 weeks off I wanted to devote them to learn more and apply what I've learnt.  

So I wanted to try speech recognition. Could you recommend me some tutorials on that? At first I would like to test some existing frameworks/tools and maybe later write something on my own. 

* Database:  [VoxForge](http://voxforge.org/home/downloads)? [TIMIT](https://catalog.ldc.upenn.edu/LDC93S1)? Other? Any commercial database worth trying out? 

* Tools: I use C++ with CUDA and know some python. What tools would you recommend? [KALDI](http://kaldi.sourceforge.net/about.html)? [VOCE](http://voce.sourceforge.net/)? Others?

As I am new to deep learning, how hard this project is going to be? 3 weeks will do? 

Thanks in advance! ",6,0,False,self,,,,,
149,MachineLearning,t5_2r3gv,2015-3-11,2015,3,11,23,2yoi5v,self.MachineLearning,correct implementation of Hinge loss minimization for gradient descent,https://www.reddit.com/r/MachineLearning/comments/2yoi5v/correct_implementation_of_hinge_loss_minimization/,h1395010,1426084144,"I copied the hinge loss function from [here](https://code.google.com/p/java-statistical-analysis-tool/source/browse/trunk/JSAT/src/jsat/lossfunctions/HingeLoss.java?r=762) (also LossC and LossFunc upon which it's based. Then I included it in my gradient descent algorithm like so: 

	  do 
	  {
	    iteration++;
	    error = 0.0;
	    cost = 0.0;
	    
	    //loop through all instances (complete one epoch)
	    for (p = 0; p &lt; number_of_files__train; p++) 
	    {
	    	
	      // 1. Calculate the hypothesis h = X * theta
	      hypothesis = calculateHypothesis( theta, feature_matrix__train, p, globo_dict_size );

	      // 2. Calculate the loss = h - y and maybe the squared cost (loss^2)/2m
	      //cost = hypothesis - outputs__train[p];
	      cost = HingeLoss.loss(hypothesis, outputs__train[p]);
	      System.out.println( ""cost "" + cost );
	      
	      // 3. Calculate the gradient = X' * loss / m
	      gradient = calculateGradent( theta, feature_matrix__train, p, globo_dict_size, cost, number_of_files__train);
	      
	      // 4. Update the parameters theta = theta - alpha * gradient
	      for (int i = 0; i &lt; globo_dict_size; i++) 
	      {
	    	  theta[i] = theta[i] - LEARNING_RATE * gradient[i];
	      }

	    }
	    
		//summation of squared error (error value for all instances)
	    error += (cost*cost);	    
	  
	  /* Root Mean Squared Error */
	  //System.out.println(""Iteration "" + iteration + "" : RMSE = "" + Math.sqrt( error/number_of_files__train ) );
	  System.out.println(""Iteration "" + iteration + "" : RMSE = "" + Math.sqrt( error/number_of_files__train ) );
	  
	  } 
	  while( error != 0 );

But this doesnt work at all. Is that due to the loss function? Maybe how I added the loss function to my code? 

I guess it's also possible that my implementation of gradient descent is faulty. 

Here are my methods for calculating the gradient and the hypothesis, are these right?

	static double calculateHypothesis( double[] theta, double[][] feature_matrix, int file_index, int globo_dict_size )
	{
		double hypothesis = 0.0;

		 for (int i = 0; i &lt; globo_dict_size; i++) 
		 {
			 hypothesis += ( theta[i] * feature_matrix[file_index][i] );
		 }
		 //bias
		 hypothesis += theta[ globo_dict_size ];

		 return hypothesis;
	}

	static double[] calculateGradent( double theta[], double[][] feature_matrix, int file_index, int globo_dict_size, double cost, int number_of_files__train)
	{
		double m = number_of_files__train;

		double[] gradient = new double[ globo_dict_size];//one for bias?
		
		for (int i = 0; i &lt; gradient.length; i++) 
		{
			gradient[i] = (1.0/m) * cost * feature_matrix[ file_index ][ i ] ;
		}
		
		return gradient;
	}

The rest of the code is [here](https://github.com/h1395010/gradient_diss-1-ent_1-id) if you're interested to take a look. ",3,0,False,self,,,,,
150,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,0,2yonqs,blog.onliquid.com,Beginner's Guide to Machine Learning: Part 1 of 2,https://www.reddit.com/r/MachineLearning/comments/2yonqs/beginners_guide_to_machine_learning_part_1_of_2/,avazlqd,1426086989,,2,19,False,http://b.thumbs.redditmedia.com/BcDPHd-UQhgIz8BHlkqlXhVk5eAAiBxpdgBY-v9tEgo.jpg,,,,,
151,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,2,2yp3dp,blogs.technet.com,Connected cows? ML in the most unlikely of places...,https://www.reddit.com/r/MachineLearning/comments/2yp3dp/connected_cows_ml_in_the_most_unlikely_of_places/,MLBlogTeam,1426094167,,0,1,False,default,,,,,
152,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,2,2yp3mv,technology.stitchfix.com,A Word is Worth a Thousand Vectors,https://www.reddit.com/r/MachineLearning/comments/2yp3mv/a_word_is_worth_a_thousand_vectors/,juxtaposicion,1426094281,,14,113,False,http://b.thumbs.redditmedia.com/HtecJizWGEWAOq8tu03Tzzg2zU3dSEsm-f-GLMgXLQY.jpg,,,,,
153,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,3,2yp9jp,self.MachineLearning,Any recommendations for Information Retrieval and applied ML communities online?,https://www.reddit.com/r/MachineLearning/comments/2yp9jp/any_recommendations_for_information_retrieval_and/,mistertim,1426096860,"Hi all! 

I guess this might be slightly off topic (as well as hinting at a slight irony given the subject), but I'm about to enrol in graduate school working on applying active learning techniques in to information retrieval, and I was wondering if there were any online communities (similar to this one) focused on IR? the /r/informationretrieval sub seems to have fizzled out a while ago. Any suggestions would be gratefully received! Thanks.",1,8,False,self,,,,,
154,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,3,2ypbfs,arxiv.org,Gradient-based Hyperparameter Optimization through Reversible Learning,https://www.reddit.com/r/MachineLearning/comments/2ypbfs/gradientbased_hyperparameter_optimization_through/,[deleted],1426097670,,0,11,False,default,,,,,
155,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,3,2ypckm,github.com,HIPS/autograd : Computes derivatives of complicated numpy code (used in the hypergradient paper),https://www.reddit.com/r/MachineLearning/comments/2ypckm/hipsautograd_computes_derivatives_of_complicated/,[deleted],1426098170,,12,8,False,http://b.thumbs.redditmedia.com/B3YziAxspPOCJzkagpbh23mLE6FZgKjGsVEcw9csh9c.jpg,,,,,
156,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,5,2ypsf5,self.MachineLearning,"Ideas for grad project on movie ratings data (e.g. Movielens, recsys algos)",https://www.reddit.com/r/MachineLearning/comments/2ypsf5/ideas_for_grad_project_on_movie_ratings_data_eg/,janon123,1426105054,"Hi,

I am looking for algorithms to program and run over movie rating data sets. So far, I have only been looking at the {users, ratings} in the Movielens 100k with cosine similarity and SVD with gradient descent. My preliminary research has led me to restricted Boltzmann machines (RBMs). [link to discussion on Quora](https://www.quora.com/Has-there-been-any-work-on-using-deep-learning-for-recommendation-engines)

I am looking for other algorithms and ideas. Perhaps some in the ""deep learning"" vein and more recent than the Koren and Bell overview on collaborative filtering and 2012 Netflix prize algorithms? [link to paper](https://datajobs.com/data-science-repo/Collaborative-Filtering-[Koren-and-Bell].pdf)

Thanks!

I am able to code in C++, Java, and Python fairly proficiently. I think I can get a handle on most algorithms (I'm a graduate student in experimental particle physics and I've implemented a fair share in a number of languages from scratch).

Edit: this is just a end-of-course project but I intend to continue experimenting with it for my own personal interest and edification...it's kind of interesting!",1,1,False,self,,,,,
157,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,6,2ypyt7,blog.mrtz.org,Competing in a data science contest without reading the data,https://www.reddit.com/r/MachineLearning/comments/2ypyt7/competing_in_a_data_science_contest_without/,alexeyr,1426107791,,1,6,False,http://b.thumbs.redditmedia.com/1FXXKWKdjbUzqU2ZOEJcWD3PLkieiDKYTUggff_MKys.jpg,,,,,
158,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,6,2ypzib,self.MachineLearning,Would anyone be interested in giving me a fake ML interview over Skype today (in 1-5 hours from when this post was created)?,https://www.reddit.com/r/MachineLearning/comments/2ypzib/would_anyone_be_interested_in_giving_me_a_fake_ml/,Coneylake,1426108123,"It doesn't have to be long, maybe 30 minutes. You could ask me some basic conceptual questions on ML and/or incorporate some actual coding using Java, Python, Matlab (I'll give you a link to a scratch pad that we'll both look at). You could benefit yourself because you will gain a different perspective on interviews and I would gain practice for an upcoming job interview. PM me your Skype name. 

EDIT: I had one volunteer and he was phenomenal! I highly recommend doing mock interviews for anyone else in the same position. :D ",14,60,False,self,,,,,
159,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,6,2yq2xo,self.MachineLearning,Are you interested in a Weekly Discussions about ML/DS/Data in this ML-subreddit,https://www.reddit.com/r/MachineLearning/comments/2yq2xo/are_you_interested_in_a_weekly_discussions_about/,__Julia,1426109627,"Hi All,

It's really hard to find or join ML community if you are not in boston or SF. However, I think that there are many eager ML developers, data wranglers, big data nerds who wanna share their knowledge, discuss the trending technologies in the industry of data science, the great papers that they read recently, or the greatest open source that have been created. 
I am writing this thread to see if it's a good idea to do weekly discussions through AMA-like threads where we discuss things around [What happened this week in Data Science Industry]. If you are startup holder who launched his data-driven app, tell us about it, if you are researcher who published his/her paper, tell us about it, if you are blogger, who wrote a good piece, tell us about it, if you are curious reader like me, and came across a good piece, just tell us about it. 

Let me if you find this as a good idea, let's have our discussions every friday evening #FDS Friday evening is for data science ;) ",3,5,False,self,,,,,
160,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,7,2yq6ju,self.MachineLearning,Regressors keep converging towards median value,https://www.reddit.com/r/MachineLearning/comments/2yq6ju/regressors_keep_converging_towards_median_value/,americanabba,1426111280,"I am trying to train regressors on a dataset of ~595 entries and 59 variables. I have difficulties with SVM, randomforest, basic linear regressions, adaboost, and neural networks on this data as they keep converging towards the median value in the set. By this I mean that when asked to predict the training set (or a separate validation set), they will simply return the median target value. I have absolutely no idea why this is and have not encountered it before, although I have mostly dealt with classification problems and not regression problems. Does anybody have any idea as to why this might be? Any papers or books to point my in the right direction would be much appreciated. Thanks!",4,0,False,self,,,,,
161,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,8,2yqiiw,self.MachineLearning,Reinforcement Learning and Neural Networks?,https://www.reddit.com/r/MachineLearning/comments/2yqiiw/reinforcement_learning_and_neural_networks/,DemiPixel,1426117394,"*In reality, you can probably just skip to the bold question, I just wanted to give a little context so it didn't seem like I was pulling this out of nowhere*



Alright, so I'd like to admit, I'm fairly new to machine learning but I have some ideas I'd like some help on.

From what I can tell, neural networks are great at recognizing patterns from previous data. It requires some training of inputs and outputs and then can be used to get expected patterns of outputs from new inputs.

Reinforcement learning consists of set states and set action that a machine can take, meaning there are only a limited possible number of ""inputs"" and ""outputs"" so to speak.

So far, I've be writing a 2048 neural network that has been trained off of me playing some and I want it to play for itself. It works obviously less than ideal. At the moment, the 16 inputs are just fractions of the logs of 2 based on the highest tile (e.g. if the highest tile is 32, 32 would be 1, 16 would be 4/5, 8 would be 3/5, etc and blank would be 0). I got interested in reinforcement learning because it allowed it to plan ahead (which many games or actions in life do) with an expected reward. 

Eventually, if I would hope to change to RGB values (0-1) instead of my predefined numbers, and the machine could learn on it's own which colors were important. But this seems impossible to do with just limited states and actions.

**My question is, how do I get inputs and outputs with any value (0-1) like a neural network while still keeping a reward system and learning method similar to reinforcement learning?**",5,0,False,self,,,,,
162,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,9,2yqr79,dataschool.io,In-depth introduction to machine learning in 15 hours of expert videos,https://www.reddit.com/r/MachineLearning/comments/2yqr79/indepth_introduction_to_machine_learning_in_15/,matt_rudo,1426121563,,0,3,False,http://b.thumbs.redditmedia.com/kgH7MFlswAiOVw8KzOdhzgW8OxpoSowqUzKhdP6h4Pk.jpg,,,,,
163,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,11,2yr38h,genopharmix.com,Big Data vs. Milton Glaser - Reconciling design practices in an era of machine-learning.,https://www.reddit.com/r/MachineLearning/comments/2yr38h/big_data_vs_milton_glaser_reconciling_design/,biomimic,1426127567,,0,2,False,default,,,,,
164,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,12,2yr6nc,self.MachineLearning,Setting minimum weight updates when training NNs?,https://www.reddit.com/r/MachineLearning/comments/2yr6nc/setting_minimum_weight_updates_when_training_nns/,Vystril,1426129297,"So I've been playing around training unstructured (randomly generated so I don't have to have fully connected layers) NNs on the MNIST data set, and have noticed a couple weird things and was wondering if it was a bug in my implementation, common knowledge, or something interesting.

Basically, when I simply apply update the weights with the standard backpropagation value multiplied by a learning rate, my training error hops around a lot, before getting to a decent solution and then actually progressively getting worse.  When I looked into it a bit more, the weight updates were actually increasing, leading me to think that I was having a problem with exploding gradients.

When I applied a cap (0.0001) to to how much I could update eight weight after any pass through an image (I'm doing stochastic gradient descent) things started working a whole lot better, the training error would steadily improve and then hover around a good solution (as opposed to progressively getting worse after finding something decent).   This seemed to be pretty fine with 2 hidden layers and ~2000-4000 random edges.

When I bump things up to three hidden layers and ~3000-6000 random edges, stochastic gradient descent just hops around the initial training error, never getting better.  Here I'm assuming the problem is vanishing gradients (which seem to be the issue with &gt; 2 hidden layers).  When I added a minimum cap, basically if the weight update was between -0.0000001 and 0, I set it to -0.0000001, and if it was between 0 and 0.0000001 I set it to 0.0000001, now all of the sudden the 3 layer NNs are training like the 2 layer NNs.

So I'm wondering A. if there was a bug in my code, that's making me have to cap my weights, and if B. minimum weight updates are a thing for training deeper NNs?",4,1,False,self,,,,,
165,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,12,2yrcip,youtube.com,TC-100  AutoPack-Tabletop Tablet/Capsule Counter,https://www.reddit.com/r/MachineLearning/comments/2yrcip/tc100__autopacktabletop/,autopacker,1426132575,,0,0,False,default,,,,,
166,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,12,2yrcq1,self.MachineLearning,"Implementation of gradient descent, pernicious bug! who can spot it!",https://www.reddit.com/r/MachineLearning/comments/2yrcq1/implementation_of_gradient_descent_pernicious_bug/,h1395010,1426132711,[This question](http://stackoverflow.com/questions/28988732/correct-implementation-of-hinge-loss-minimization-for-gradient-descent/28999444#28999444) encapsulates much of the code and it's purpose but some modifications have but added the latest one is on my [github page](https://github.com/h1395010/gradient_diss-1-ent_1-id/blob/master/src/return_of_gradient_descent/GradientDescent.java),7,0,False,self,,,,,
167,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,16,2yrupm,arxiv.org,Tree-LSTMs for NLP,https://www.reddit.com/r/MachineLearning/comments/2yrupm/treelstms_for_nlp/,iori42,1426145981,,0,3,False,default,,,,,
168,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,18,2ys2d8,self.MachineLearning,CVPR 2015 Accepted Papers,https://www.reddit.com/r/MachineLearning/comments/2ys2d8/cvpr_2015_accepted_papers/,spidey-fan,1426153708,"[Here](http://www.pamitc.org/cvpr15/accepted_papers.php) is the list of accepted papers for this year's CVPR. Any idea when the actual paper names will be released, and the PDFs as well?",0,1,False,self,,,,,
169,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,19,2ys3h7,gigaom.com,Heres more evidence that sports is a goldmine for machine learning,https://www.reddit.com/r/MachineLearning/comments/2ys3h7/heres_more_evidence_that_sports_is_a_goldmine_for/,jeanninemaddaloni,1426154812,,0,5,False,http://b.thumbs.redditmedia.com/tYeX55UUwM20NTphYEBMFtTBY0LQ5koFab8BOoA370E.jpg,,,,,
170,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,19,2ys4ql,self.MachineLearning,Bayesian Learning for Neural Networks,https://www.reddit.com/r/MachineLearning/comments/2ys4ql/bayesian_learning_for_neural_networks/,lol__wut,1426156055,"Hi /r/machinelearning,
I'm an undergraduate looking to do an honors thesis on the topic of bayesian machine learning. I've been sifting through dozens of academic papers looking for novel applications of bayesian learning in the neural network field such as with radial basis functions, probablistic neural networks and other feed forward neural networks but a lot of extensive work has been done in these areas.

Can anyone recommend either neural network topologies that may warrant a bayesian approach to learning that has not been explored yet or in general novel papers with regards to bayesian learning and neural networks?
I have basic knowledge of bayesian learning methods using monte carlo methods, variational metheds etc.
as well as standard learning algorithms such as backpropagation, EM etc.
",38,2,False,self,,,,,
171,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,23,2ysm0w,radar.oreilly.com,Turning Ph.D.s into industrial data scientists and data engineers,https://www.reddit.com/r/MachineLearning/comments/2ysm0w/turning_phds_into_industrial_data_scientists_and/,gradientflow,1426168871,,0,0,False,http://b.thumbs.redditmedia.com/JikmiMOkEfOdIPCj3j5IO0OcNh2sAQXsm3XiOCJMzng.jpg,,,,,
172,MachineLearning,t5_2r3gv,2015-3-12,2015,3,12,23,2ysmp8,datalab.lu,Applying Machine Learning to Peer to Peer lending,https://www.reddit.com/r/MachineLearning/comments/2ysmp8/applying_machine_learning_to_peer_to_peer_lending/,kafka399,1426169248,,0,19,False,http://b.thumbs.redditmedia.com/hRX_GPSodoDtqszbV85mtPmG5-SZ3uzPhbI44fIoZFM.jpg,,,,,
173,MachineLearning,t5_2r3gv,2015-3-13,2015,3,13,0,2ysyfz,self.MachineLearning,fEMG?,https://www.reddit.com/r/MachineLearning/comments/2ysyfz/femg/,agentmu83,1426175094,"Hi! I am both new to this subreddit's existence and not an expert. I'm also, however, a student, with interest in NLP and its current barriers to advancement. I've conceived a research project for myself, to see if facial electromyography could be used as another learning set for NLP. Would it be possible to use the same sort of muscle-signal-sensors that I see being used for motion control in the consumer product Myo, for example, on the face to recognize the muscle contractions our mouths make when speaking phrases or phonemes? ",5,5,False,self,,,,,
174,MachineLearning,t5_2r3gv,2015-3-13,2015,3,13,1,2yt6qk,blogs.technet.com,5 lucrative tech careers to pursue in 2015,https://www.reddit.com/r/MachineLearning/comments/2yt6qk/5_lucrative_tech_careers_to_pursue_in_2015/,MLBlogTeam,1426178779,,0,0,False,http://b.thumbs.redditmedia.com/ftHpWQAnv3s0dZOYmKtT5se6GYnxC1PtnSwz-u2EnQg.jpg,,,,,
175,MachineLearning,t5_2r3gv,2015-3-13,2015,3,13,3,2ytir3,cireneikual.wordpress.com,Text2SDR,https://www.reddit.com/r/MachineLearning/comments/2ytir3/text2sdr/,CireNeikual,1426183966,,11,6,False,http://b.thumbs.redditmedia.com/6E0YXUncXhFZE-HgC8IH7COjePvS20nr4P30WrcpyiU.jpg,,,,,
176,MachineLearning,t5_2r3gv,2015-3-13,2015,3,13,3,2ytmni,self.MachineLearning,Extracting keywords from IEEE articles by month?,https://www.reddit.com/r/MachineLearning/comments/2ytmni/extracting_keywords_from_ieee_articles_by_month/,chestervonwinchester,1426185686,"This might be off-topic for this forum, but I assume many of the people browse IEEE online, so I figured I'd ask:

I'd like to gather keywords (and perhaps some other meta data such as # of citations) from articles from a particular IEEE journal by month (issue). Does anyone have any experience doing this?

I realize I could fix up some sort of script to collect this by parsing the html, but I think this would get real dirty real quick. This is why my question is specifically about IEEE. If anyone has experience about metadata analysis of academic literature in general though, I'd appreciate it, too.",3,1,False,self,,,,,
177,MachineLearning,t5_2r3gv,2015-3-13,2015,3,13,4,2ytt9w,hakkalabs.co,"Mattermark on Neural Networks, Vector Reduction and NLP",https://www.reddit.com/r/MachineLearning/comments/2ytt9w/mattermark_on_neural_networks_vector_reduction/,dot_2,1426188629,,0,4,False,http://a.thumbs.redditmedia.com/VB2-5GoIaJU8-j_pxXgeqY9zza4dPI7hLVPhUJi14b8.jpg,,,,,
178,MachineLearning,t5_2r3gv,2015-3-13,2015,3,13,5,2ytxu6,self.MachineLearning,Create artificial dataset for binary classification?,https://www.reddit.com/r/MachineLearning/comments/2ytxu6/create_artificial_dataset_for_binary/,gargravarrp,1426190658,I'm trying to evaluate Spark for machine learning algorithms and I need a really huge dataset (a few hundred GB perhaps). Is there anywhere one can find such a dataset? Or can one take a smaller dataset and expand it in some way (if that makes sense) so that it can't fit in memory on a single machine?,3,2,False,self,,,,,
179,MachineLearning,t5_2r3gv,2015-3-13,2015,3,13,6,2yuctw,youtu.be,"Build state-of-the-art supervised models from 3/4 datasets, without tuning anything.",https://www.reddit.com/r/MachineLearning/comments/2yuctw/build_stateoftheart_supervised_models_from_34/,Algorithm_1,1426197378,,0,0,False,http://a.thumbs.redditmedia.com/HINN8GgPS6sEXR7HyL4PmpCwISwTQ7dI4Ph88RudiF0.jpg,,,,,
180,MachineLearning,t5_2r3gv,2015-3-13,2015,3,13,9,2yuulb,arrayfire.com,Linear Classifiers for CUDA and OpenCL using ArrayFire,https://www.reddit.com/r/MachineLearning/comments/2yuulb/linear_classifiers_for_cuda_and_opencl_using/,pavanky,1426205831,,2,5,False,http://b.thumbs.redditmedia.com/3Q-5wPAxhtFtO3znsgCWj8N_2Jx4OqdiZ3u3X7l7tiY.jpg,,,,,
181,MachineLearning,t5_2r3gv,2015-3-13,2015,3,13,11,2yvc56,self.MachineLearning,is this a correct implementation of averaged perceptron?,https://www.reddit.com/r/MachineLearning/comments/2yvc56/is_this_a_correct_implementation_of_averaged/,h1395010,1426214972,"There varibale names are pretty descriptive and there are comments explaining what's going on. 

Basically after every iteration I save that weight. the algorithm runs until it hits the maximum iterations threshold, and then the final weights that I used is the average of every weight I saw during the course of running the algorithm. is that right?

		  
		  //LEARNING WEIGHTS
		  double localError, globalError;
		  int p, iteration, output;
		
		  iteration = 0;
		  do 
		  {
			  iteration++;
			  globalError = 0;
			  //loop through all instances (complete one epoch)
			  for (p = 0; p &lt; number_of_files__train; p++) 
			  {
				  // calculate predicted class
				  output = calculateOutput( theta, weights, feature_matrix__train, p, globo_dict_size );
				  // difference between predicted and actual class values
				  localError = outputs__train[p] - output;
				  //update weights and bias
				  for (int i = 0; i &lt; globo_dict_size; i++) 
				  {
					  weights[i] += ( LEARNING_RATE * localError * feature_matrix__train[p][i] );
				  }
				  weights[ globo_dict_size ] += ( LEARNING_RATE * localError );
				  
				  //summation of squared error (error value for all instances)
				  globalError += (localError*localError);
			  }

	           //store weights for averaging
	           cached_weights.add( Arrays.copyOf(weights, weights.length) );

			  /* Root Mean Squared Error */
			  System.out.println(""Iteration 0"" + iteration + "" : RMSE = "" + Math.sqrt( globalError/number_of_files__train ) );

		  } 
		  while(globalError != 0 &amp;&amp; iteration&lt;=MAX_ITER);
  
		  
	       //compute averages
	       double[] sums = new double[ globo_dict_size + 1 ];
	       double[] averages = new double[ globo_dict_size + 1 ];

	       for (Multiset.Entry&lt;double[]&gt; entry : cached_weights.entrySet() ) 
	       {
	           double[] value = entry.getElement();
	           
	           for(int pos=0; pos &lt; globo_dict_size + 1; pos++)
	           {
	               sums[ pos ] +=  value[ pos ]; 
	           }
	       }
	       for(int pos=0; pos &lt; globo_dict_size + 1; pos++)
	       {
	           averages[ pos ] = sums[ pos ] / cached_weights.size(); 
	       }


The full code can be found [here](https://github.com/h1395010/return_of_the_averaged_perceptron/blob/master/src/return_of_the_averaged_perceptron/AveragedPerceptron.java).",2,0,False,self,,,,,
182,MachineLearning,t5_2r3gv,2015-3-13,2015,3,13,12,2yvi94,willwhitney.github.io,Neural Graphics Engine,https://www.reddit.com/r/MachineLearning/comments/2yvi94/neural_graphics_engine/,mrkul,1426218384,,13,56,False,http://b.thumbs.redditmedia.com/RPGN-MeSbkpe2bWOQ00mgvsgpstYnXnUzUp8Mj2NQOE.jpg,,,,,
183,MachineLearning,t5_2r3gv,2015-3-13,2015,3,13,13,2yvnb0,youtube.com,Lecture on Sequence Generation by Alex Graves,https://www.reddit.com/r/MachineLearning/comments/2yvnb0/lecture_on_sequence_generation_by_alex_graves/,kkastner,1426221396,,16,22,False,http://b.thumbs.redditmedia.com/6Kv0LwzzyxMdwJGeBxxaFZIhH2h9XP5CdgoBOtuyhcE.jpg,,,,,
184,MachineLearning,t5_2r3gv,2015-3-13,2015,3,13,14,2yvscn,self.MachineLearning,Trying to see the differences between 'nolearn' and 'theano' python packages for DNN.,https://www.reddit.com/r/MachineLearning/comments/2yvscn/trying_to_see_the_differences_between_nolearn_and/,pharshal,1426224864,Can anyone elaborate the pros and cons of each package? and for a newbie like me which one would you recommend?,3,6,False,self,,,,,
185,MachineLearning,t5_2r3gv,2015-3-13,2015,3,13,18,2yw7j2,youtube.com,Variational Autoencoders and Image Generation by Karol Gregor of DeepMind,https://www.reddit.com/r/MachineLearning/comments/2yw7j2/variational_autoencoders_and_image_generation_by/,iori42,1426239053,,0,13,False,http://a.thumbs.redditmedia.com/J1l1_elYc_Y0ewJTRDRI8CAnA0V9bY_vcwFBllfow_8.jpg,,,,,
186,MachineLearning,t5_2r3gv,2015-3-13,2015,3,13,18,2yw7rw,datasciencelab.wordpress.com,Clustering With K-Means in Python,https://www.reddit.com/r/MachineLearning/comments/2yw7rw/clustering_with_kmeans_in_python/,cast42,1426239298,,0,14,False,http://b.thumbs.redditmedia.com/hfsHGmZgdSLR8IkxAUkg00YqD5jzIM180bssNutP4sk.jpg,,,,,
187,MachineLearning,t5_2r3gv,2015-3-13,2015,3,13,19,2ywb66,stackoverflow.com,shuffle 2D array in one dimension without effecting other dimension.,https://www.reddit.com/r/MachineLearning/comments/2ywb66/shuffle_2d_array_in_one_dimension_without/,[deleted],1426242676,,0,0,False,default,,,,,
188,MachineLearning,t5_2r3gv,2015-3-13,2015,3,13,20,2ywecy,arxiv.org,Google's Response to Facebooks Deepface,https://www.reddit.com/r/MachineLearning/comments/2ywecy/googles_response_to_facebooks_deepface/,AudioSaur,1426245619,,11,34,False,default,,,,,
189,MachineLearning,t5_2r3gv,2015-3-13,2015,3,13,20,2ywgj6,self.MachineLearning,seek.com.au scraped,https://www.reddit.com/r/MachineLearning/comments/2ywgj6/seekcomau_scraped/,[deleted],1426247522,"I scraped about 2k postings from the seek job site. Searched for 'machine learning' in the postings; counted it up; and

+------------------+-------+

| machine learning | Count |

+------------------+-------+

|      0-9999      |   0   |

|   10000-19999    |   0   |

|   20000-29999    |   0   |

|   30000-39999    |   0   |

|   40000-49999    |   0   |

|   50000-59999    |   0   |

|   60000-69999    |   0   |

|   70000-79999    |   3   |

|   80000-89999    |   3   |

|   90000-99999    |   2   |

|  100000-109999   |   3   |

|  110000-119999   |   5   |

|  120000-129999   |   5   |

|  130000-139999   |   2   |

|  140000-149999   |   2   |

|  150000-159999   |   0   |

|  160000-169999   |   3   |

|  170000-179999   |   3   |

|  180000-189999   |   3   |

|  190000-199999   |   0   |

+------------------+-------+

Can any meaningful conclusion be drawn from this?

*Edit:*
Considering 'front end' has:

| 130000-139999 |   49  |

| 140000-149999 |   39  |

it would be more optimal for my finances to learn html than statistics...",0,1,False,default,,,,,
190,MachineLearning,t5_2r3gv,2015-3-13,2015,3,13,23,2ywspn,github.com,LINE: toolkit for embedding large-scale information networks (w/ code and paper),https://www.reddit.com/r/MachineLearning/comments/2ywspn/line_toolkit_for_embedding_largescale_information/,improbabble,1426255623,,8,4,False,http://b.thumbs.redditmedia.com/c9ce5vmkrvqRxKe8kGmOLkegG9blR6ewD2VKPAFyLHs.jpg,,,,,
191,MachineLearning,t5_2r3gv,2015-3-14,2015,3,14,0,2yx1op,p-value.info,Advice to graduate students interviewing for industry positions (x-post /r/datascience),https://www.reddit.com/r/MachineLearning/comments/2yx1op/advice_to_graduate_students_interviewing_for/,seabass,1426260189,,1,14,False,http://b.thumbs.redditmedia.com/qD62f9SlTOtOaE7RxUUYJV-ECODJX0UTeHVdOEk6r8Q.jpg,,,,,
192,MachineLearning,t5_2r3gv,2015-3-14,2015,3,14,2,2yxefq,aka.ms,Tablets use cloud machine learning to offer personalized recommendations to restaurant guests,https://www.reddit.com/r/MachineLearning/comments/2yxefq/tablets_use_cloud_machine_learning_to_offer/,MLBlogTeam,1426266272,,0,1,False,http://b.thumbs.redditmedia.com/ftHpWQAnv3s0dZOYmKtT5se6GYnxC1PtnSwz-u2EnQg.jpg,,,,,
193,MachineLearning,t5_2r3gv,2015-3-14,2015,3,14,2,2yxj5l,self.MachineLearning,Get the intuition behind regularization in ML,https://www.reddit.com/r/MachineLearning/comments/2yxj5l/get_the_intuition_behind_regularization_in_ml/,underflow404,1426268383,"I would like to have a better understanding of the reasons behind regularization in machine learning.
Here are my current questions,  can you recommend me some readings about the topic to answer them and even learn more about how regularization is used across different ML models ?

I understand why L1 regularization applied to an optimization problem will encourage solution sparsity. This will hopefully produce models which generalize better (Occam's razor). About L2 regularization, I don't understand why minimizing the norm of the vector would lead to a ""simpler"" solution.

I also heard about ""tied weights"" in neural networks, a regularization method wich constraints weights of two layers of autoencoders to be transposed. This avoids local optima issues during training. I don't understand the reason again ?

Thanks :)",6,1,False,self,,,,,
194,MachineLearning,t5_2r3gv,2015-3-14,2015,3,14,2,2yxj6c,self.MachineLearning,Get the intuition behind regularization in ML,https://www.reddit.com/r/MachineLearning/comments/2yxj6c/get_the_intuition_behind_regularization_in_ml/,[deleted],1426268394,"I would like to have a better understanding of the reasons behind regularization in machine learning.
Here are my current questions,  can you recommend me some readings about the topic to answer them and even learn more about how regularization is used across different ML models ?

I understand why L1 regularization applied to an optimization problem will encourage solution sparsity. This will hopefully produce models which generalize better (Occam's razor). About L2 regularization, I don't understand why minimizing the norm of the vector would lead to a ""simpler"" solution.

I also heard about ""tied weights"" in neural networks, a regularization method wich constraints weights of two layers of autoencoders to be transposed. This avoids local optima issues during training. I don't understand the reason again ?

Thanks :)",0,1,False,default,,,,,
195,MachineLearning,t5_2r3gv,2015-3-14,2015,3,14,3,2yxpni,pamensky.com,Electric Motors Toronto,https://www.reddit.com/r/MachineLearning/comments/2yxpni/electric_motors_toronto/,similura1,1426271568,,0,1,False,default,,,,,
196,MachineLearning,t5_2r3gv,2015-3-14,2015,3,14,4,2yxw0y,arxiv.org,Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks,https://www.reddit.com/r/MachineLearning/comments/2yxw0y/improved_semantic_representations_from/,imanishshah,1426274452,,0,0,False,default,,,,,
197,MachineLearning,t5_2r3gv,2015-3-14,2015,3,14,6,2yygeg,efavdb.com,Crib notes on decision trees and forests w/ discussion on their efficient implementation.,https://www.reddit.com/r/MachineLearning/comments/2yygeg/crib_notes_on_decision_trees_and_forests_w/,efavdb,1426283776,,5,19,False,http://b.thumbs.redditmedia.com/omg9_hb_n-AuwMtfCSjH9N38OVQ7QlN95fNbffFfIbE.jpg,,,,,
198,MachineLearning,t5_2r3gv,2015-3-14,2015,3,14,7,2yyluz,thetalkingmachines.com,"New Talking Machines: Second part of conversation with Hinton, Bengio, LeCun",https://www.reddit.com/r/MachineLearning/comments/2yyluz/new_talking_machines_second_part_of_conversation/,Eurchus,1426286425,,3,47,False,http://b.thumbs.redditmedia.com/Bf48OF_9xBhrpE_TAleNaUaYage-tdtm5eViIjbDI4Q.jpg,,,,,
199,MachineLearning,t5_2r3gv,2015-3-14,2015,3,14,12,2yzj9p,arxiv.org,When Are Tree Structures Necessary for Deep Learning of Representations?,https://www.reddit.com/r/MachineLearning/comments/2yzj9p/when_are_tree_structures_necessary_for_deep/,vkhuc,1426304844,,2,5,False,default,,,,,
200,MachineLearning,t5_2r3gv,2015-3-14,2015,3,14,13,2yzl91,poker.srv.ualberta.ca,"Cepheus Poker Project: The ""Essentially"" Perfect Poker Bot",https://www.reddit.com/r/MachineLearning/comments/2yzl91/cepheus_poker_project_the_essentially_perfect/,Ninjakannon,1426306015,,0,1,False,http://b.thumbs.redditmedia.com/KVEmMEDXUw-ULkP-F3VFx0s-rObKmrHMK-WAx_eh7vQ.jpg,,,,,
201,MachineLearning,t5_2r3gv,2015-3-14,2015,3,14,14,2yzrsx,cs229.stanford.edu,Stanford CS229 Fall 2014 Project Reports,https://www.reddit.com/r/MachineLearning/comments/2yzrsx/stanford_cs229_fall_2014_project_reports/,neuromorphics,1426310457,,2,2,False,default,,,,,
202,MachineLearning,t5_2r3gv,2015-3-14,2015,3,14,14,2yzs4g,stackoverflow.com,difference between stochastic gradient descent and batch.,https://www.reddit.com/r/MachineLearning/comments/2yzs4g/difference_between_stochastic_gradient_descent/,h1395010,1426310679,,1,0,False,http://b.thumbs.redditmedia.com/IHFTGdD4Sly4xGviFH0i9qavZz9ELnmBmcahqZ6rTCE.jpg,,,,,
203,MachineLearning,t5_2r3gv,2015-3-14,2015,3,14,15,2yzxej,self.MachineLearning,Beginning Machine Learning as a Group. Want to Join?,https://www.reddit.com/r/MachineLearning/comments/2yzxej/beginning_machine_learning_as_a_group_want_to_join/,Ace88Coder,1426314921,"I have started studying Machine Learning via the following approach:
I have installed R and have started going through the book: http://shop.oreilly.com/product/0636920018483.do
Don't go by the name as I know it is silly. But in order to develop practical experience, I am finding it good as it takes a case study based approach.
Simultaneously I will be developing my skills on probability and statistics by reading the following:
First course in Probability, Ross
Probability and Statistics, Schaum's Outline
I will then brush up on Linear Algebra by following Gilbert Strang's book and lecture in Youtube
Finally, start reading books on Machine Learning.
Let me know if anyone else is interested in joining me in this journey :) I plan to read the books cover to cover and try solving every problem in the exercise. As I am job holder, sometimes depending upon project loads, I might go slow or fast. If there are a few of us, I will find this journey little motivational.

Thanks",0,0,False,self,,,,,
204,MachineLearning,t5_2r3gv,2015-3-14,2015,3,14,15,2yzxhi,self.MachineLearning,"A little help with the calculus, in Hinton's NN course",https://www.reddit.com/r/MachineLearning/comments/2yzxhi/a_little_help_with_the_calculus_in_hintons_nn/,maxxxpowerful,1426314989,"I'm going through Hinton's Coursera course on Neural Nets. It's been going OK so far, and I managed to muddle my way through it; but now I'm stuck at some calculus in Lecture 10. The relevant slide can be found here: http://imgur.com/Mmlxrny

Can someone please tell me how this derivative is derived(!)? ",5,6,False,self,,,,,
205,MachineLearning,t5_2r3gv,2015-3-14,2015,3,14,17,2z03b0,github.com,Novel Supervised Machine Learning Algorithm Called Gravitational Clustering,https://www.reddit.com/r/MachineLearning/comments/2z03b0/novel_supervised_machine_learning_algorithm/,ArmenAg,1426320749,,35,13,False,http://b.thumbs.redditmedia.com/ZV7RKHzQOsuK-9w0qVPXYZGNFBPpDkIfogeH5tXiJ9M.jpg,,,,,
206,MachineLearning,t5_2r3gv,2015-3-14,2015,3,14,19,2z0a7i,self.MachineLearning,Independent Component Analysis online course?,https://www.reddit.com/r/MachineLearning/comments/2z0a7i/independent_component_analysis_online_course/,josealb,1426328485,"Hello,

Let's see if you guys can help me, a while ago I found an online course from coursera/audacity or something similar, that explained machine learning or data science and used Independent Component Analysis as an example. It showed you how to write a code to solve the typical cocktail party problem.
I didn't finish this course (as I always do with online courses...) and now I need to understand ICA and I think this course would be great but I can't find it again, do you maybe know the course and care to share the link?

TL;DR: I am looking for an online course that explains independent component analysis

thanks",5,0,False,self,,,,,
207,MachineLearning,t5_2r3gv,2015-3-14,2015,3,14,19,2z0bbl,pic4.nipic.com,"Deep Network super-power: Can someone help me to find the ""suspension bridge"" in this image?",https://www.reddit.com/r/MachineLearning/comments/2z0bbl/deep_network_superpower_can_someone_help_me_to/,wenzhong,1426329745,,2,0,False,default,,,,,
208,MachineLearning,t5_2r3gv,2015-3-15,2015,3,15,2,2z1866,sebastianraschka.com,About Artificial Neurons and Single-Layer Neural Networks - How Machine Learning Algorithms Work Part 1,https://www.reddit.com/r/MachineLearning/comments/2z1866/about_artificial_neurons_and_singlelayer_neural/,rasbt,1426352472,,0,42,False,http://b.thumbs.redditmedia.com/UBmjxD0Ee0uGJ-YJ2WNvSDRlgMvZZsHZ99gPHIjzCoE.jpg,,,,,
209,MachineLearning,t5_2r3gv,2015-3-15,2015,3,15,2,2z1f1s,self.MachineLearning,Does this interesting research opens new domains in artificial creativity ?,https://www.reddit.com/r/MachineLearning/comments/2z1f1s/does_this_interesting_research_opens_new_domains/,frozen_in_reddit,1426355913,"I'm looking at this paper:

Deep self-taught learning for facial beauty prediction[1]

And i wonder: do you think something similar could work teaching a machine our concepts of beauty, for example in graphic design ? 

does it mean we're near the point of machine with taste? 

and do you think it could work in combination with generative methods(like genetic algorithms), to make software that creates beautiful(and maybe useful) things ? 


[1]http://www.sciencedirect.com/science/article/pii/S0925231214006468",1,0,False,self,,,,,
210,MachineLearning,t5_2r3gv,2015-3-15,2015,3,15,4,2z1taf,nuit-blanche.blogspot.com,Slides: Stanford's CS231n: Convolutional Neural Networks for Visual Recognition (x-post r/CompressiveSensing),https://www.reddit.com/r/MachineLearning/comments/2z1taf/slides_stanfords_cs231n_convolutional_neural/,compsens,1426362933,,0,1,False,http://b.thumbs.redditmedia.com/11Fv3elUcHQ3djzZfUMSbu4-AlyR9oH5oKe0JGTdxUQ.jpg,,,,,
211,MachineLearning,t5_2r3gv,2015-3-15,2015,3,15,6,2z251s,forum.eyewire.org,If anyone has some free time I highly suggest you learn about/help Eyewire in their mission to trace every neural connection in the brain,https://www.reddit.com/r/MachineLearning/comments/2z251s/if_anyone_has_some_free_time_i_highly_suggest_you/,overk4ll,1426368783,,1,4,False,default,,,,,
212,MachineLearning,t5_2r3gv,2015-3-15,2015,3,15,10,2z2xr9,self.MachineLearning,DeepNetwork Super-power: Can it recognize Arc de Triomphe?,https://www.reddit.com/r/MachineLearning/comments/2z2xr9/deepnetwork_superpower_can_it_recognize_arc_de/,wenzhong,1426384335," Deepnet trained using ImageNet database recognize the following image 

http://f5.topit.me/5/b4/3c/11449981182cc3cb45o.jpg

as ""birdhouse"". Need some imagination!",2,0,False,self,,,,,
213,MachineLearning,t5_2r3gv,2015-3-15,2015,3,15,11,2z30on,self.MachineLearning,Can I use sentiment analysis to answer some simple questions?,https://www.reddit.com/r/MachineLearning/comments/2z30on/can_i_use_sentiment_analysis_to_answer_some/,pawpawbears,1426386006,"Can I use sentiment analysis to answer some simple questions?
Example:

what is the perception of the way the government handled a disease?
did the disease affect productivity?

most of the books that I have read so far keep talking about word frequency. Where do you recommend that I begin? what kind of sentiment analysis should I use?",5,3,False,self,,,,,
214,MachineLearning,t5_2r3gv,2015-3-15,2015,3,15,12,2z34xk,self.MachineLearning,Background needed to learn EM algorithm?,https://www.reddit.com/r/MachineLearning/comments/2z34xk/background_needed_to_learn_em_algorithm/,_fysikz,1426388460,"I'm trying to pick a topic for an end-of-semester paper for a numerical methods class. I have a physics background, with a little bit of math and CS. I don't know much about machine learning. Is it realistic to assume I can learn how to implement the EM algorithm in a matter of weeks? Is it possible to learn a ""simplified"" version of it, or is it all one big animal?",4,0,False,self,,,,,
215,MachineLearning,t5_2r3gv,2015-3-15,2015,3,15,18,2z3xic,datasciencecentral.com,All Machine Learning Models Have Flaws,https://www.reddit.com/r/MachineLearning/comments/2z3xic/all_machine_learning_models_have_flaws/,alexeyr,1426410302,,0,0,False,http://a.thumbs.redditmedia.com/MnCiV-SAdNFvjoiwcXwjGl3651nOvczgtqSOLTFQLm0.jpg,,,,,
216,MachineLearning,t5_2r3gv,2015-3-15,2015,3,15,18,2z3ynb,self.MachineLearning,[Question] I have a clustering data mining project where I am stuck on preprocessing.,https://www.reddit.com/r/MachineLearning/comments/2z3ynb/question_i_have_a_clustering_data_mining_project/,stumblelightly22,1426411608,"I am working in Python (with the SciPy package installed) and I have to implement a k means clustering algorithm to classify my data. The first portion of the project is what has me stuck. We were instructed to preprocess the data so we fill any missing values, which are designated by a '?', with the mean or mode value for that attribute. I have tried both a Pandas data frame and working out a class with dictionaries for taking in the data, but I am a noob and simply stuck. I can post code if necessary, but right now I am just stuck and don't know what to do. I have experimented with and done online training for Python, but this is my first real project in this language. Thanks for your help and interest!",2,0,False,self,,,,,
217,MachineLearning,t5_2r3gv,2015-3-15,2015,3,15,18,2z3z7o,self.MachineLearning,Would there be any practical differences between using the absolute value rather than the square in functions such as least square function? Isn't the square more computationally expensive?,https://www.reddit.com/r/MachineLearning/comments/2z3z7o/would_there_be_any_practical_differences_between/,[deleted],1426412223,,12,3,False,default,,,,,
218,MachineLearning,t5_2r3gv,2015-3-15,2015,3,15,19,2z40l8,self.MachineLearning,"Simple Deep learning, Python, No Nvidia GPU: Options?",https://www.reddit.com/r/MachineLearning/comments/2z40l8/simple_deep_learning_python_no_nvidia_gpu_options/,ddofer,1426413800,"I'm a Bioinformatics student interested in playing with (deep) Neural nets and autoencoders (i.e. feature learning and classification). 
My lab and home PC don't have a Nvidia GPU, although I do have an AMD GPU at home (On a windows PC).

Are there any good libraries that don't require a nvidia GPU, and have ""out of the box"" implementations?
 I'm not looking to start inventing or reimplementing algorithms, I want to experiment with different methods and configurations (e.g. X layers of sparse autoencoders, a denoising autoencoder and a 3/4 layer neural network). I note that I won't be working with ""toy"" datasets, so I need something that's idiot proof (i.e. Not Theano X.AsTensor, etc' ) in terms of loading my cleaned datasets from CSVs... 
 Ideally I'd like something as friendly and consistent as scikit learn, i.e. ""Fit(X,Y), predict(), etc' . 
Breze, Lasagna and NoLearn look nice, though i've been having difficulty getting them working on my PC. (I also can't install Git on the lab computers, so installing is a pain. Currently I'm reliant on Anaconda's distribution). Any other options? 

Thank you very much!",48,23,False,self,,,,,
219,MachineLearning,t5_2r3gv,2015-3-15,2015,3,15,21,2z47no,self.MachineLearning,"Request: advice for developing deep learning / computer vision based quadcopters (""drones"") systems",https://www.reddit.com/r/MachineLearning/comments/2z47no/request_advice_for_developing_deep_learning/,JCondaLea,1426421305,"I'd like to ask if there are any recommended quadcopter models (or any models of ""drones"") for developing autonomous flight systems.

Background: I'm currently working on deep learning based computer vision applications that are fast enough to run in-real time with a GPU and would love to get them in a drone to experiment with autonomous flight. I'm currently using Caffe and looking at the Jetson TK1 from NVIDIA for autonomous systems.",7,0,False,self,,,,,
220,MachineLearning,t5_2r3gv,2015-3-16,2015,3,16,1,2z4sw8,self.MachineLearning,Memory Neural Networks (MemNN) Implementation?,https://www.reddit.com/r/MachineLearning/comments/2z4sw8/memory_neural_networks_memnn_implementation/,cryptocerous,1426435795,"I'm interested in experimenting with the tasks shown in:

""Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks""

https://research.facebook.com/researchers/1543934539189348

http://arxiv.org/pdf/1502.05698v3.pdf

and earlier:

http://arxiv.org/abs/1410.3916

Is there any MemNN implementation available? Or any other pre-built code so I can start experimenting with the tasks shown in that paper?

If not, are there any other textual sequence-predicting implementations for tasks like these?",6,2,False,self,,,,,
221,MachineLearning,t5_2r3gv,2015-3-16,2015,3,16,1,2z4t2h,self.MachineLearning,ML and human movement,https://www.reddit.com/r/MachineLearning/comments/2z4t2h/ml_and_human_movement/,rgibson7usa,1426435879,"A key value of deep learning is automated feature extraction. Can a deep neural network use a large compendium of human movements to extract a set of features that could be interpreted as basic movements? Example data sets could include kinematic models of 1) a day in the life of a military special operations team, 2) a professional football game, or 3) firefighters battling a four-alarm blaze.

The relevance is that many systematic approaches to physical training identify a foundational set of movements that are trained using repetitive drills, resistance training, etc. For example, a currently popular approach to resistance training is the consideration of push, pull, squat, and hip hinge patterns. Martial arts have their own compendium of basic movement patterns. A natural-fitness school called Movnat has various categories of locomotion, object manipulation, in addition to combative striking and grappling.

The selection of these basic movement patterns seem to be derived from the experience of human experts. Wouldnt a completely empirical derivation of movement motifs be a valuable basis upon which to build an optimal training system for a given activity?",4,0,False,self,,,,,
222,MachineLearning,t5_2r3gv,2015-3-16,2015,3,16,1,2z4xpg,self.MachineLearning,Predictive number of FB likes of the given article,https://www.reddit.com/r/MachineLearning/comments/2z4xpg/predictive_number_of_fb_likes_of_the_given_article/,terancee,1426438344,"Could anyone suggest some good articles where is shown how to:

1. Predict number of FB likes for a given article;
2. Predict who will put them?

Thank you for your answers. ",5,1,False,self,,,,,
223,MachineLearning,t5_2r3gv,2015-3-16,2015,3,16,1,2z4y16,self.MachineLearning,Anomaly Detection and Cross Validation,https://www.reddit.com/r/MachineLearning/comments/2z4y16/anomaly_detection_and_cross_validation/,[deleted],1426438500,"I apologize if my question is simple but I'm a beginner in this field and I have a problem that I couldn't figure out for months. 

I'm working on building an anomaly detection model to detect anomalous sequences of system calls. My model will be trained on normal ""legitimate"" system calls and will be tested on intrusive ones.  I have a data set that contains a normal system calls traces and intrusive traces. The traces vary in length and count so the number of traces in each data sets are not equal. Sometime, I have 2000 normal syscalls traces while I have only 2 intrusive syscalls traces. Other times, I have 1001 intrusive syscalls traces and more normal syscalls traces. 

**So my question is, how can I do a proper cross validation ? especially in sickit-learn ?** 

I know how to do a cross validation for classification problems when your training sets are mixed of both labels. This data set is different. Normal traces are separated from anomalous traces and I don't want to train my model on anomalous traces. So my training set needs to be clean. However, this messes up the whole cross validation. I've thought of many different scenarios. The close one is to do a cross validation on my normal dataset and test the model on the whole intrusive dataset. For example, if I do 3 folds, I will split the normal sets into 3 sets, train my model on 2 of them, combine 3rd dataset with intrusive dataset, and test the model on them. I need to measure the false positive 
rate and true positive rate. Is this approach good ? Isn't testing the on same dataset, which is the intrusive dataset, a bad practice ?

I hope the issue is clear now. This issue has been bothering me for a month and I couldn't figure out a good way to solve.  Also, English isn't my first language so please ask me if you didn't understand my words. 

Thanks",0,0,False,default,,,,,
224,MachineLearning,t5_2r3gv,2015-3-16,2015,3,16,2,2z51cr,medium.com,The Single Best Predictive Modeling Technique. Seriously.,https://www.reddit.com/r/MachineLearning/comments/2z51cr/the_single_best_predictive_modeling_technique/,[deleted],1426440144,,0,0,False,default,,,,,
225,MachineLearning,t5_2r3gv,2015-3-16,2015,3,16,4,2z5gam,s3.amazonaws.com,Why recombinative evolution works. And how it's computationally efficient.,https://www.reddit.com/r/MachineLearning/comments/2z5gam/why_recombinative_evolution_works_and_how_its/,[deleted],1426447396,,0,1,False,default,,,,,
226,MachineLearning,t5_2r3gv,2015-3-16,2015,3,16,4,2z5gnu,self.MachineLearning,Why recombinative evolution works. And how it's computationally efficient.,https://www.reddit.com/r/MachineLearning/comments/2z5gnu/why_recombinative_evolution_works_and_how_its/,kburjorj,1426447569,"The unit-mixability principle (Section 2.3) and the generative elimination assumption (Section 8) are key. 

Comments welcome.

http://s3.amazonaws.com/burjorjee/www/hypomixability_elimination_foga2015.pdf",2,2,False,self,,,,,
227,MachineLearning,t5_2r3gv,2015-3-16,2015,3,16,6,2z5tiy,self.MachineLearning,Tools for Unsupervised Feature Learning.,https://www.reddit.com/r/MachineLearning/comments/2z5tiy/tools_for_unsupervised_feature_learning/,strayadvice,1426453758,"Just checking what people are using for UFL, autoencoders, RBMs etc. I looked up what Torch has since I'm somewhat familiar with the tool, but the demo seems a bit outdated and the documentation is a little difficult to follow. What's the go-to tool today for training auto-encoders and layer-wise pretraining?",14,1,False,self,,,,,
228,MachineLearning,t5_2r3gv,2015-3-16,2015,3,16,6,2z5vtx,machineslikeus.com,Why no Artificial General Intelligence?,https://www.reddit.com/r/MachineLearning/comments/2z5vtx/why_no_artificial_general_intelligence/,MachinesLikeUs,1426454828,,0,0,False,http://b.thumbs.redditmedia.com/-HhffH9WOIdrqEmylex-Kh5-gr1eo_UG8BylYmQPhso.jpg,,,,,
229,MachineLearning,t5_2r3gv,2015-3-16,2015,3,16,7,2z60au,self.MachineLearning,Using an Amazon GPU Spot Instance for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/2z60au/using_an_amazon_gpu_spot_instance_for_deep/,datascience123,1426456972,"I want to play around with deep learning for image classification, so I'm looking into Amazon GPU Spot Instances. They can be about 10x cheaper, but you have to deal with the possibility of the instance terminating at random times and having to start it up again later. I am a complete newbie when it comes to using amazon instances, so forgive me if this is a stupid question:

Where would I put my image data (+- 40GB) when I'm using a spot instance? GPU instances have 60GB of storage, but I'm guessing I would have to download my data all over again everytime I start up a new instance because my old one got terminated? I read something about EBS storage, but wouldn't training be super-slow if the image data wasn't local? 
",31,14,False,self,,,,,
230,MachineLearning,t5_2r3gv,2015-3-16,2015,3,16,10,2z6no3,thetalkingmachines.com,Talking Machines - HUMAN CONVERSATION ABOUT MACHINE LEARNING,https://www.reddit.com/r/MachineLearning/comments/2z6no3/talking_machines_human_conversation_about_machine/,klogk,1426468714,,0,1,False,http://b.thumbs.redditmedia.com/Bf48OF_9xBhrpE_TAleNaUaYage-tdtm5eViIjbDI4Q.jpg,,,,,
231,MachineLearning,t5_2r3gv,2015-3-16,2015,3,16,11,2z6tvo,startup.ml,Reflections on Julia,https://www.reddit.com/r/MachineLearning/comments/2z6tvo/reflections_on_julia/,arshakn,1426471949,,11,30,False,http://a.thumbs.redditmedia.com/0mdRkLNpHqxqv6e075bCVyPV6vx5skyC2kdpE9L29j0.jpg,,,,,
232,MachineLearning,t5_2r3gv,2015-3-16,2015,3,16,11,2z6yfb,youtu.be,A TEDx talk from Dr. Dean Hougen of the University of Oklahoma on how nurturing can enhance learning in multi-generational learning schemes,https://www.reddit.com/r/MachineLearning/comments/2z6yfb/a_tedx_talk_from_dr_dean_hougen_of_the_university/,cosmic_cow_ck,1426474332,,0,0,False,http://b.thumbs.redditmedia.com/7JFOAL10Lc_a8DOglXPXp4KvfZ8p-l09dn8T4KU9ogY.jpg,,,,,
233,MachineLearning,t5_2r3gv,2015-3-16,2015,3,16,19,2z7xt1,arxiv.org,IDSIA's huge comparison of many different LSTM flavours,https://www.reddit.com/r/MachineLearning/comments/2z7xt1/idsias_huge_comparison_of_many_different_lstm/,[deleted],1426503039,,0,1,False,default,,,,,
234,MachineLearning,t5_2r3gv,2015-3-16,2015,3,16,19,2z7xtz,arxiv.org,IDSIA's huge comparison of many different LSTM flavours,https://www.reddit.com/r/MachineLearning/comments/2z7xtz/idsias_huge_comparison_of_many_different_lstm/,[deleted],1426503067,,0,1,False,default,,,,,
235,MachineLearning,t5_2r3gv,2015-3-16,2015,3,16,20,2z80sm,self.MachineLearning,Looking for help testing my tutorial on Deepmind's code at superintelligence.ch/deepmind,https://www.reddit.com/r/MachineLearning/comments/2z80sm/looking_for_help_testing_my_tutorial_on_deepminds/,Zan01,1426505705,"I wrote a tutorial on how to add graphics to Deepmind's recently released code on

http://superintelligence.ch/deepmind

It also features a part on how to train the network so that it becomes good at ATARI games. Unfortunately I don't have enough RAM to load the network after training it. If someone here could check that loading networks works as described, that would be amazing.",13,11,False,self,,,,,
236,MachineLearning,t5_2r3gv,2015-3-16,2015,3,16,23,2z8i9e,blog.shakirm.com,A Statistical View of Deep Learning (II): Auto-encoders and Free Energy,https://www.reddit.com/r/MachineLearning/comments/2z8i9e/a_statistical_view_of_deep_learning_ii/,iori42,1426517056,,0,5,False,http://b.thumbs.redditmedia.com/nj1FuiNZ_FrjmC878_5pLKuujC_9MseyS0FDNsPB8fY.jpg,,,,,
237,MachineLearning,t5_2r3gv,2015-3-17,2015,3,17,0,2z8r8y,docs.google.com,Exploring Deep Space: Discovering Factors of Variation Learned in Deep Networks,https://www.reddit.com/r/MachineLearning/comments/2z8r8y/exploring_deep_space_discovering_factors_of/,glassackwards,1426521475,,1,12,False,http://a.thumbs.redditmedia.com/ldwW7RR5g4mcKm9DuHzXyJgiNdQwgFOAcO4iTWAOOG8.jpg,,,,,
238,MachineLearning,t5_2r3gv,2015-3-17,2015,3,17,2,2z9277,fastml.com,FastML: What you wanted to know about AI,https://www.reddit.com/r/MachineLearning/comments/2z9277/fastml_what_you_wanted_to_know_about_ai/,tabacof,1426526348,,5,61,False,http://b.thumbs.redditmedia.com/YRveqAQtY5-uPA1JtMo7wNEkRPc--e-puEShKNq_qtE.jpg,,,,,
239,MachineLearning,t5_2r3gv,2015-3-17,2015,3,17,2,2z933c,self.MachineLearning,Simple Question about KL divergence for Variational Autoencoders,https://www.reddit.com/r/MachineLearning/comments/2z933c/simple_question_about_kl_divergence_for/,alexmlamb,1426526710,"Hello, 

I'm interested in figuring out the KL divergence from a univariate gaussian to a standard normal.  In the Google Draw paper and in the self-encoding variational bayes paper, it says that the KL divergence is: 

mu^2 + sigma^2 - log(sigma^2 )

http://arxiv.org/pdf/1312.6114v10.pdf (bottom of page 5)

The Google Draw paper gives the same equation.  This is confusing, because the minimum of that expression is achieved when sigma is about 0.65.  

Also, when I use the formula for the KL-divergence between univariate gaussians: 

http://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians

with the second gaussian being N(0,1), I get: 

0.5 * (sigma^2 - 2*log(sigma) + mu^2 )

This is also not minimized by setting sigma = 1, which doesn't make sense to me.  ",2,2,False,self,,,,,
240,MachineLearning,t5_2r3gv,2015-3-17,2015,3,17,3,2z9drw,aka.ms,Free webinar: Microsoft data scientists walk you through the cloud data science process,https://www.reddit.com/r/MachineLearning/comments/2z9drw/free_webinar_microsoft_data_scientists_walk_you/,MLBlogTeam,1426531445,,0,1,False,http://b.thumbs.redditmedia.com/ftHpWQAnv3s0dZOYmKtT5se6GYnxC1PtnSwz-u2EnQg.jpg,,,,,
241,MachineLearning,t5_2r3gv,2015-3-17,2015,3,17,5,2z9w8t,self.MachineLearning,[Modeling Question] Detecting the location of a (rare) pattern in a 1D sequence.,https://www.reddit.com/r/MachineLearning/comments/2z9w8t/modeling_question_detecting_the_location_of_a/,ryptophan,1426539317,"Hi /r/MachineLearning,

I've recently started working with some data that is a little outside of my comfort-zone. I was hoping someone around here may have experience with a problem that looks similar...
Basically, given a variable-length sequence I'm trying to detect the location of a 'rare' pattern. The data generally looks like this:

[sample plot 1](http://i.imgur.com/1Lmvlku.png)

[sample plot 2](http://i.imgur.com/d9uwaWC.png)

The point indicated by the red vertical line is what I'd like to detect.

I'd be grateful for any thoughts and discussion - even if it's just pointing me toward paper/model type that I can investigate.

Thanks,

-ryp

",8,0,False,self,,,,,
242,MachineLearning,t5_2r3gv,2015-3-17,2015,3,17,6,2z9zyn,self.MachineLearning,[Question] Memory and Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/2z9zyn/question_memory_and_recurrent_neural_networks/,RossoFiorentino,1426540985,I am looking at RNN's and the main problem i often see associated with RNN's is the vanishing gradient problem. However another seeming problem is the memory (GPU memory) associated with unrolling a RNN in order to apply BPTT (Backpropagation through time).  For each time step another duplicate of the network is added and it's associated memory that stores intermediate layer outputs (only the layer weights and such are not duplicated). I am wondering how people deal with this issue in practice. For example is the unrolling limited to a certain length? Is there some clever way of reusing memory? ,10,0,False,self,,,,,
243,MachineLearning,t5_2r3gv,2015-3-17,2015,3,17,7,2za8t1,self.MachineLearning,Metric Learning in the wild,https://www.reddit.com/r/MachineLearning/comments/2za8t1/metric_learning_in_the_wild/,barmaley_exe,1426544890,"Hello, fellow redditors!

I recently came across a previously unknown area of ML research, called Metric Learning. For those unfamiliar with it, there are [slides](http://www.slideshare.net/zukun/metric-learning-icml2010-tutorial) and several surveys: [1](http://web.cse.ohio-state.edu/~kulis/pubs/ftml_metric_learning.pdf), [2](http://arxiv.org/abs/1306.6709)

My question is: how useful are they in practice? Did someone use it? If yes, what was the application, what algorithm did you use, how well did it work? I'd like to know about any experience.",5,2,False,self,,,,,
244,MachineLearning,t5_2r3gv,2015-3-17,2015,3,17,9,2zam46,benanne.github.io,Classifying plankton with deep neural networks,https://www.reddit.com/r/MachineLearning/comments/2zam46/classifying_plankton_with_deep_neural_networks/,benanne,1426551028,,36,69,False,http://b.thumbs.redditmedia.com/cnf8o4cjTo2QasAXZyRb946Ngh8rsrgBZ1C1ez1tLSc.jpg,,,,,
245,MachineLearning,t5_2r3gv,2015-3-17,2015,3,17,10,2zash7,self.MachineLearning,Prepping data set for multi-nomial classification,https://www.reddit.com/r/MachineLearning/comments/2zash7/prepping_data_set_for_multinomial_classification/,watersign,1426554177,"I know about one vs one and one vs all as far as classification goes but are there any methods or certain ways I should go about prepping a data set to deal with multiple classes? Ive tried using CART &amp; C.45 decision trees on these problems and the results were pitiful. 

What am I doing wrong? ",0,0,False,self,,,,,
246,MachineLearning,t5_2r3gv,2015-3-17,2015,3,17,12,2zb94u,jmlr.org,Stable and Efficient Representation Learning with Nonnegativity Constraints (PDF),https://www.reddit.com/r/MachineLearning/comments/2zb94u/stable_and_efficient_representation_learning_with/,[deleted],1426562620,,0,1,False,default,,,,,
247,MachineLearning,t5_2r3gv,2015-3-17,2015,3,17,13,2zbde5,github.com,pyHTFE - A Sequence Prediction Algorithm,https://www.reddit.com/r/MachineLearning/comments/2zbde5/pyhtfe_a_sequence_prediction_algorithm/,CireNeikual,1426564993,,23,5,False,http://b.thumbs.redditmedia.com/Kio9T3yHI6WLroiWevu2SD2meMqPU9kOIhpF3Knte0k.jpg,,,,,
248,MachineLearning,t5_2r3gv,2015-3-17,2015,3,17,16,2zbtsz,self.MachineLearning,Machine Learning in Digital Advertising,https://www.reddit.com/r/MachineLearning/comments/2zbtsz/machine_learning_in_digital_advertising/,Arjun_Satya,1426576948,"Hi!, I am building an optimization service for an Adtech company. For ranking ads in terms of performance, I am using formula: Log(impressions)*eCPM. Impressions means the number of times ad was served/shown. eCPM = estimated Cost/Revenue per 1000 impressions. The problem with this metric is the cumulative effect of eCPM allows the ad to achieve higher rank after ad has been served for quite a some time. This is because ad had performed well earlier but hasn't been performing now, hence, eCPM remain high inspite for recent drop in performance. I would like to introduce recent trend of ctr or eCPM in the formula. Any recommendations?

One such recommendation is: Log(impressions)*eCPM*(d(eCPM)/dt) where dt is around 1 hour",0,0,False,self,,,,,
249,MachineLearning,t5_2r3gv,2015-3-17,2015,3,17,16,2zbvdb,self.MachineLearning,apsis - Bayesian Hyperparameter Optimization,https://www.reddit.com/r/MachineLearning/comments/2zbvdb/apsis_bayesian_hyperparameter_optimization/,frederikdiehl,1426578514,"**Summary:**

Github: https://github.com/frederikdiehl/apsis

Docs: http://apsis.readthedocs.org/en/latest/

Paper: http://arxiv.org/abs/1503.02946



**What is apsis?**

A toolkit for hyperparameter optimization for machine learning algorithms.

Our goal is to provide a flexible, simple and scaleable approach - parallel, on clusters and/or on your own machine.

Apsis is open-sourced under the MIT license.

**What's the current state?**

We currently are in a beta version. Functionality has been implemented, but it has not been used by many people, and there are still advances to tackle.

Especially cluster support is lacking, and is planned to be implemented in the next version.

**How good is the optimization?**

It's usually better than random search. See http://apsis.readthedocs.org/en/latest/evaluation.html#evaluation-on-neural-network-on-mnist

Note that it's not yet as good as state of the art bayesian optimization.

**Are there more details to be found?**

Look here: http://arxiv.org/abs/1503.02946

Or look at the documentation.",13,10,False,self,,,,,
250,MachineLearning,t5_2r3gv,2015-3-17,2015,3,17,17,2zbwr3,technologyreview.com,Data Mining Reveals When A Yellow Taxi Is Cheaper Than Uber,https://www.reddit.com/r/MachineLearning/comments/2zbwr3/data_mining_reveals_when_a_yellow_taxi_is_cheaper/,lravindr,1426579920,,3,0,False,http://b.thumbs.redditmedia.com/8L82EOmQIVQ94EL1z-ec45r-9SNFBmEhAcv_oHEDzSk.jpg,,,,,
251,MachineLearning,t5_2r3gv,2015-3-17,2015,3,17,18,2zbzjp,machinelearningmastery.com,A Tour of Machine Learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/2zbzjp/a_tour_of_machine_learning_algorithms/,DevFRus,1426582936,,0,0,False,http://b.thumbs.redditmedia.com/PKvYBau-OevyPQHbZuasxzw_QB3G2x51WY4h43t1BpA.jpg,,,,,
252,MachineLearning,t5_2r3gv,2015-3-17,2015,3,17,21,2zcd5m,radar.oreilly.com,Lets build open source tensor libraries for data science,https://www.reddit.com/r/MachineLearning/comments/2zcd5m/lets_build_open_source_tensor_libraries_for_data/,gradientflow,1426594871,,8,9,False,http://b.thumbs.redditmedia.com/JikmiMOkEfOdIPCj3j5IO0OcNh2sAQXsm3XiOCJMzng.jpg,,,,,
253,MachineLearning,t5_2r3gv,2015-3-17,2015,3,17,23,2zctuu,blog.onliquid.com,Beginner's Guide to Machine Learning: Part 2,https://www.reddit.com/r/MachineLearning/comments/2zctuu/beginners_guide_to_machine_learning_part_2/,avazlqd,1426604390,,1,0,False,http://b.thumbs.redditmedia.com/BFSzF552Y6zV1i6-us2Tu9XaQYPq6mAkPvM7LGj4XFw.jpg,,,,,
254,MachineLearning,t5_2r3gv,2015-3-18,2015,3,18,1,2zd1fv,personal.ie.cuhk.edu.hk,"Deep Representation Learning with Target Coding -- achieves 11% accuracy improvement, compared to 1-of-K, on MNIST, STL-10, CIFAR-100 (PDF)",https://www.reddit.com/r/MachineLearning/comments/2zd1fv/deep_representation_learning_with_target_coding/,[deleted],1426608010,,12,6,False,default,,,,,
255,MachineLearning,t5_2r3gv,2015-3-18,2015,3,18,3,2zdpmf,devblogs.nvidia.com,DIGITS: Deep Learning GPU Training System,https://www.reddit.com/r/MachineLearning/comments/2zdpmf/digits_deep_learning_gpu_training_system/,harrism,1426618707,,15,47,False,http://b.thumbs.redditmedia.com/eCIONxjGTnbTMw4SgVVwFSFskRGHASWoKgWl9XNW-2Q.jpg,,,,,
256,MachineLearning,t5_2r3gv,2015-3-18,2015,3,18,4,2zduqs,aka.ms,Washing machine meets machine learning - laundry innovator bets on power of cloud analytics,https://www.reddit.com/r/MachineLearning/comments/2zduqs/washing_machine_meets_machine_learning_laundry/,MLBlogTeam,1426620882,,0,1,False,http://b.thumbs.redditmedia.com/ftHpWQAnv3s0dZOYmKtT5se6GYnxC1PtnSwz-u2EnQg.jpg,,,,,
257,MachineLearning,t5_2r3gv,2015-3-18,2015,3,18,6,2ze79q,seldon.io,Seldon  Open Predictive AI released on GitHub under Apache 2.0,https://www.reddit.com/r/MachineLearning/comments/2ze79q/seldon_open_predictive_ai_released_on_github/,ahousley,1426626268,,0,0,False,http://b.thumbs.redditmedia.com/xmsRg4DUBOON1_V6IAXzs8IiOlGt2nhRAFp7SQxho6I.jpg,,,,,
258,MachineLearning,t5_2r3gv,2015-3-18,2015,3,18,11,2zfdwe,self.MachineLearning,Conformal Prediction in Python,https://www.reddit.com/r/MachineLearning/comments/2zfdwe/conformal_prediction_in_python/,donlnz,1426646879,"Hiya!

I'm a graduate student working in the field of conformal prediction (http://alrw.net/). 

During my research I've been implementing bits and pieces in Python, with the goal of eventually publishing my project as an open source library. I've been putting release off for quite some time now, mainly due to my code being absolutely hideous. In the last few days though, inspiration struck me, and I've been reimplementing my library from scratch!

Thus, I present to you, nonconformist: https://github.com/donlnz/nonconformist

I welcome any questions, comments, feedback etc. :)",5,8,False,self,,,,,
259,MachineLearning,t5_2r3gv,2015-3-18,2015,3,18,11,2zfefh,news.dice.com,Dice: How we Data-Mine Related Tech Skills,https://www.reddit.com/r/MachineLearning/comments/2zfefh/dice_how_we_datamine_related_tech_skills/,simonhughes22,1426647167,,0,1,False,http://a.thumbs.redditmedia.com/HAkzH2k8ui4wKdtKPQszF4EVXH2aSwC4kJfRyWfTFB8.jpg,,,,,
260,MachineLearning,t5_2r3gv,2015-3-18,2015,3,18,12,2zfk5n,datasciencecentral.com,100 R packages,https://www.reddit.com/r/MachineLearning/comments/2zfk5n/100_r_packages/,urinec,1426650380,,0,0,False,http://b.thumbs.redditmedia.com/VlRUUh9UjS6IPjPEI7joYQ2yyZ6Hgegyijmu9YAlyZw.jpg,,,,,
261,MachineLearning,t5_2r3gv,2015-3-18,2015,3,18,17,2zg4x2,nlp.stanford.edu,Generating 3D scenes from text [PDF],https://www.reddit.com/r/MachineLearning/comments/2zg4x2/generating_3d_scenes_from_text_pdf/,iori42,1426666728,,1,5,False,default,,,,,
262,MachineLearning,t5_2r3gv,2015-3-18,2015,3,18,17,2zg4xv,stackoverflow.com,pseudocode of gradient descent,https://www.reddit.com/r/MachineLearning/comments/2zg4xv/pseudocode_of_gradient_descent/,matthias_anglicus,1426666750,,3,0,False,http://b.thumbs.redditmedia.com/IHFTGdD4Sly4xGviFH0i9qavZz9ELnmBmcahqZ6rTCE.jpg,,,,,
263,MachineLearning,t5_2r3gv,2015-3-18,2015,3,18,18,2zg7fk,zdnet.com,Machine-learning security startup Darktrace aims $18m injection at overseas expansion,https://www.reddit.com/r/MachineLearning/comments/2zg7fk/machinelearning_security_startup_darktrace_aims/,marikoherrington,1426669456,,0,0,False,http://b.thumbs.redditmedia.com/hxTq6UM1X11--CRmIJZdiZk4WUKUMThAOaTda8rKykY.jpg,,,,,
264,MachineLearning,t5_2r3gv,2015-3-18,2015,3,18,18,2zgaef,gzimachinery.com,Everything you want to know about Tee Forming Machine,https://www.reddit.com/r/MachineLearning/comments/2zgaef/everything_you_want_to_know_about_tee_forming/,kapildogra0,1426672502,,0,1,False,default,,,,,
265,MachineLearning,t5_2r3gv,2015-3-18,2015,3,18,20,2zgiey,self.MachineLearning,Machine Learning : Can it Help in this case ?,https://www.reddit.com/r/MachineLearning/comments/2zgiey/machine_learning_can_it_help_in_this_case/,yantrik,1426679602,"Hi, readers i have this interesting problem, one of the FabMaker in Philippines is looking for software developers to develop a software to model the amputee limbs so that he can 3d Print it for poor and needy. He has already made a 3D scanner using Kinnect, but i was wondering if we can use Machine Learning to solve this modeling issue. We can take millions of reading of Limbs (both Right and Left) and them may be Machine can learn from it and given a set of parameter's it can predict the dimensions of other limb. So is this a valid use of machine learning or i am missing something ? Edit1 : I want to measure a lot of normal people with both right and left legs and then based on given input of either of Right or Left leg data, the Machine should be able to predict the other one. Which measurements i will take, i am thinking of like width of leg, lenght of bones, full body height, weight and other such factors.",7,0,False,self,,,,,
266,MachineLearning,t5_2r3gv,2015-3-18,2015,3,18,22,2zgpx0,nuit-blanche.blogspot.com,Everybody wants to be shallow: Compact Nonlinear Maps and Circulant Extensions (x-post r/CompressiveSensing ),https://www.reddit.com/r/MachineLearning/comments/2zgpx0/everybody_wants_to_be_shallow_compact_nonlinear/,compsens,1426684798,,0,7,False,http://b.thumbs.redditmedia.com/3zYdEKAUw4uhSGpAzy4OQE161qAwSrjHEAD1VDgSPjc.jpg,,,,,
267,MachineLearning,t5_2r3gv,2015-3-18,2015,3,18,22,2zgrfo,shivonzilis.com,Machine Intelligence Company Infographic (One Page),https://www.reddit.com/r/MachineLearning/comments/2zgrfo/machine_intelligence_company_infographic_one_page/,nogo09,1426685718,,12,58,False,http://b.thumbs.redditmedia.com/54E7qMPEZagXbFb97DAwWx0CC05aVrV2fXlFMOHcU2o.jpg,,,,,
268,MachineLearning,t5_2r3gv,2015-3-18,2015,3,18,22,2zgsxx,self.MachineLearning,Netflix' RAD and PCA on 1D time series,https://www.reddit.com/r/MachineLearning/comments/2zgsxx/netflix_rad_and_pca_on_1d_time_series/,Foxtr0t,1426686583,"Netflix open-sourced RAD, software for outlier detection on big data:
http://techblog.netflix.com/2015/02/rad-outlier-detection-on-big-data.html

They have nice plots of some time series, including their low-rank approximation from Candes' Robust PCA.

I'm wondering, how do you apply PCA to 1D time series?",3,2,False,self,,,,,
269,MachineLearning,t5_2r3gv,2015-3-18,2015,3,18,23,2zgy9d,self.MachineLearning,Suggestions for a problem statement surrounding mining of Twitter conversation threads.,https://www.reddit.com/r/MachineLearning/comments/2zgy9d/suggestions_for_a_problem_statement_surrounding/,satarupaguha11,1426689383,"I am interested in the application of machine learning in NLP, information retreival, etc. I have a background in sentiment analysis. I am looking to explore into newer problems which might or might not have relation to sentiment analysis. But it is preferable if they are even slightly related. ",4,5,False,self,,,,,
270,MachineLearning,t5_2r3gv,2015-3-19,2015,3,19,0,2zh50j,analyticsbot.ml,Tkinter Quick Navigation Guide (Python 2.7) - My experiments with making a GUI,https://www.reddit.com/r/MachineLearning/comments/2zh50j/tkinter_quick_navigation_guide_python_27_my/,YahooGuys,1426692623,,1,0,False,default,,,,,
271,MachineLearning,t5_2r3gv,2015-3-19,2015,3,19,1,2zhgak,blog.sense.io,Introducing Sense - A Next-Generation Platform for Data Science,https://www.reddit.com/r/MachineLearning/comments/2zhgak/introducing_sense_a_nextgeneration_platform_for/,equark,1426697660,,9,15,False,http://b.thumbs.redditmedia.com/ulqrJCwfZiTDWBSXfxUwIDohYFFIs0r9VC6CcFlot4w.jpg,,,,,
272,MachineLearning,t5_2r3gv,2015-3-19,2015,3,19,4,2zi5br,self.MachineLearning,I have a vector of data on national capabilities for states. What ML algorithim could I use to reasonably divide this data up into three groups?,https://www.reddit.com/r/MachineLearning/comments/2zi5br/i_have_a_vector_of_data_on_national_capabilities/,OhanianIsACreep,1426708188,"I was thinking of about just dividing it into thirds, but since state powers will be clustered together I turn to you ML peeps for some help. Thanks.",5,1,False,self,,,,,
273,MachineLearning,t5_2r3gv,2015-3-19,2015,3,19,6,2ziflw,self.MachineLearning,Calculating the gradient of Batch/Stochastic gradient descent,https://www.reddit.com/r/MachineLearning/comments/2ziflw/calculating_the_gradient_of_batchstochastic/,MLNoob001,1426712582,"I'm trying to write a python script to do Batch gradient descent. However, I'm having trouble understanding how to compute the gradient for each iteration.

Setup:
data: 2d array with each row being a vector feature
t: target vector
w: weight vector
eta: learning rate

Algorithm for Batch Gradient Descent:


Until converge:

1) Predict targets from data, w: p = predicted target values

2) Calculate error of each p vs t

3) Calculate gradient **

4) for each w, w = w - eta*gradient


I understand how to complete steps 1,2,4. However, I'm not sure how to calculate the gradient. In theory, the gradient is: derivative of data with respect to w. Any help would be appreciated. ",3,0,False,self,,,,,
274,MachineLearning,t5_2r3gv,2015-3-19,2015,3,19,7,2zip9t,self.MachineLearning,Will Copying the weights of a shallow Neural Networks to a Deep architecture help with performance?,https://www.reddit.com/r/MachineLearning/comments/2zip9t/will_copying_the_weights_of_a_shallow_neural/,serout7,1426716869,"I have read that in a feed forward network, as the numbers of layers increases the back propagation algorithm struggles to affect the weights in the first few hidden layers. This means with random initialised weights it is difficult to get a good result with a deep architecture. This made me wonder if training a number of shallow networks individually then copying their weights into a deep architecture could perhaps overcome this issue?",16,7,False,self,,,,,
275,MachineLearning,t5_2r3gv,2015-3-19,2015,3,19,11,2zjkrp,arxiv.org,From Maxout to Channel-Out: Encoding Information on Sparse Pathways,https://www.reddit.com/r/MachineLearning/comments/2zjkrp/from_maxout_to_channelout_encoding_information_on/,[deleted],1426732339,,1,0,False,default,,,,,
276,MachineLearning,t5_2r3gv,2015-3-19,2015,3,19,16,2zkb3b,arxiv.org,LSTM: A Search Space Odyssey | Comparison of LSTM variants,https://www.reddit.com/r/MachineLearning/comments/2zkb3b/lstm_a_search_space_odyssey_comparison_of_lstm/,iori42,1426750456,,29,21,False,default,,,,,
277,MachineLearning,t5_2r3gv,2015-3-19,2015,3,19,16,2zkc5a,infoworld.com,How Bing Distill could feed Microsoft machine learning,https://www.reddit.com/r/MachineLearning/comments/2zkc5a/how_bing_distill_could_feed_microsoft_machine/,jeanninemaddaloni,1426751500,,0,0,False,http://a.thumbs.redditmedia.com/77V9dccvj_7_ICCZcM15YKyYI4y-w9WP5UMNaj6c2m4.jpg,,,,,
278,MachineLearning,t5_2r3gv,2015-3-19,2015,3,19,22,2zl17y,self.MachineLearning,Questions about rectified linear activation function in neural nets,https://www.reddit.com/r/MachineLearning/comments/2zl17y/questions_about_rectified_linear_activation/,AlcaDotS,1426772861,"I have two questions about the rectified linear activation function, which seems to be quite popular.

Firstly, one property of sigmoid functions is that it bounds the output of a layer; usually max #units x 1 and at minimum #units x -1. For rectified linear units there is no maximum. For RNNs exploding/vanishing gradients are/were a problem, so isn't this also the case here, especially for deeper nets?

Secondly, how come that having a derivative of exactly 0 isn't a problem for backpropagating to lower layers?

I prefer intuitive answers, but I can handle most of the math ;) ",15,7,False,self,,,,,
279,MachineLearning,t5_2r3gv,2015-3-20,2015,3,20,0,2zladc,youtube.com,Deep Learning Lecture 15: Deep Reinforcement Learning - Policy search,https://www.reddit.com/r/MachineLearning/comments/2zladc/deep_learning_lecture_15_deep_reinforcement/,kkastner,1426777578,,13,51,False,http://b.thumbs.redditmedia.com/2XtiD9gSUS5cnrJ57EcGvvwsucQD_DYt-pyi4FDwJvA.jpg,,,,,
280,MachineLearning,t5_2r3gv,2015-3-20,2015,3,20,1,2zlkqd,arxiv.org,Differential privacy and the holdout-set overfitting problem,https://www.reddit.com/r/MachineLearning/comments/2zlkqd/differential_privacy_and_the_holdoutset/,iidealized,1426782471,,0,4,False,default,,,,,
281,MachineLearning,t5_2r3gv,2015-3-20,2015,3,20,1,2zlmz5,medium.com,A Math Trick Might Save This Butterfly,https://www.reddit.com/r/MachineLearning/comments/2zlmz5/a_math_trick_might_save_this_butterfly/,helloimheretoo,1426783528,,3,0,False,http://b.thumbs.redditmedia.com/pSef4zPy9YZlmIjuImBLWjdkcs6PFq2L1AP0kCQfgVA.jpg,,,,,
282,MachineLearning,t5_2r3gv,2015-3-20,2015,3,20,1,2zln8r,blogs.technet.com,New Python capabilities integrated into cloud machine learning,https://www.reddit.com/r/MachineLearning/comments/2zln8r/new_python_capabilities_integrated_into_cloud/,MLBlogTeam,1426783650,,0,1,False,default,,,,,
283,MachineLearning,t5_2r3gv,2015-3-20,2015,3,20,9,2zndvm,self.MachineLearning,Is real time facial recogniton possible?,https://www.reddit.com/r/MachineLearning/comments/2zndvm/is_real_time_facial_recogniton_possible/,cvnovice,1426812583,"I've been messing around with facial detection using Haar Cascades and OpenCV (Python). Detecting faces is pretty straightforwrd, so I was thinking it would be really cool to step it up and do detection+recognition.

My idea is to take an input video, on each frame scan for all visble faces. Then I would like to create a set of each unique face and use frames from the video to build a ""training set"" of the detected faces.

As the video progresses the training set becomes larger so each successive frame should be recognized more accurately. 

By the end of the video I should have an arrray of each unique face and what frames they appeared in. 

So far everything I can find uses a [pre-created training set and defined model to do face rec](http://docs.opencv.org/modules/contrib/doc/facerec/facerec_tutorial.html#duda01). Is there a reason it hasnt been done in real time? Is it not possbile /too computatinally intensive? 

If not, could anyone point me towards some resources that might help me figure out how to do this? (I'm familiar with image processing but quite new to machine learning).",5,1,False,self,,,,,
284,MachineLearning,t5_2r3gv,2015-3-20,2015,3,20,11,2znm0s,self.MachineLearning,Data-set advice: Tech consumer products,https://www.reddit.com/r/MachineLearning/comments/2znm0s/dataset_advice_tech_consumer_products/,valianto1e,1426816822,"So I want to do a little rating prediction, however can't really find a good source of product name/ rating bindings for train/test. Any ideas?
",1,0,False,self,,,,,
285,MachineLearning,t5_2r3gv,2015-3-20,2015,3,20,16,2zodxw,self.MachineLearning,"Given collection of articles, how to do binary classification: ""majority topic"" or not?",https://www.reddit.com/r/MachineLearning/comments/2zodxw/given_collection_of_articles_how_to_do_binary/,Quireman,1426835546,"This problem came up while working on a side project, and wanted to get some ideas on the best way to solve it: I have a group of news articles (has title, author, body, etc.). Most of them are about the same topic but not all are, so I wish to classify the articles by whether they belong to the ""majority topic"" or not. 

For instance, I could have a group of news articles and most are about Obama, but there could be a few articles in there about Italy's economy that mention Obama's name only in passing, and maybe 2-3 other articles are about a car crash and don't mention Obama at all. I want to classify these articles into binary classes of whether they are about Obama or not.

I know there's lots of text/document classification research done, but those are much more complicated than this problem, namely in that they involve multiple classes. This problem only involves binary classes--whether an article is relevant or not, and I'm not trying to find out what the topic is, nor am I testing new points coming in--so I expect a simpler solution exists instead of such advanced methods.

So far I've determined this is an unsupervised learning problem since I don't know what the ""majority topic"" is in advance. One possible solution is to construct a bag of words vector for each article and do k-means clustering and picking the largest cluster, which ought to work since articles about the same topic ought to share similar vocabulary. However, I don't know if there's a better way to do this and wanted to ask for your opinions. Also, I'd greatly appreciate links to existing code!
",2,0,False,self,,,,,
286,MachineLearning,t5_2r3gv,2015-3-20,2015,3,20,19,2zoowa,digitalmind.io,Getting Started with Deep Learning - resources and roadmap,https://www.reddit.com/r/MachineLearning/comments/2zoowa/getting_started_with_deep_learning_resources_and/,raymestalez,1426846669,,5,22,False,default,,,,,
287,MachineLearning,t5_2r3gv,2015-3-20,2015,3,20,20,2zovcw,carelesslearner.blogspot.com,Breaking bitcoin mining: Machine learning to rapidly search for the correct bitcoin block header nonce,https://www.reddit.com/r/MachineLearning/comments/2zovcw/breaking_bitcoin_mining_machine_learning_to/,arunsupe,1426852220,,44,0,False,default,,,,,
288,MachineLearning,t5_2r3gv,2015-3-20,2015,3,20,23,2zpb5p,fxguide.com,NVIDIA GPU Tech report (a bit about their CNN engine),https://www.reddit.com/r/MachineLearning/comments/2zpb5p/nvidia_gpu_tech_report_a_bit_about_their_cnn/,leonoel,1426861981,,1,5,False,http://b.thumbs.redditmedia.com/-1uaAhbrBBlSDu-6Bhrnn5iHuKvGA5-8QMPUwMUdD8Y.jpg,,,,,
289,MachineLearning,t5_2r3gv,2015-3-21,2015,3,21,0,2zpit8,partiallyderivative.com,Partially Derivative Episode 17: Get Back To Work You Slackers!,https://www.reddit.com/r/MachineLearning/comments/2zpit8/partially_derivative_episode_17_get_back_to_work/,sensorymaps,1426865698,,3,11,False,http://a.thumbs.redditmedia.com/rfWYrGkTZ6dXaIUAmqggbSHdRaXEiOoWeJETxTzWZV0.jpg,,,,,
290,MachineLearning,t5_2r3gv,2015-3-21,2015,3,21,1,2zpmfv,carelesslearner.blogspot.com,The intrinsic value of chess pieces inferred from an analysis of 4.6 million boards,https://www.reddit.com/r/MachineLearning/comments/2zpmfv/the_intrinsic_value_of_chess_pieces_inferred_from/,arunsupe,1426867422,,3,0,False,default,,,,,
291,MachineLearning,t5_2r3gv,2015-3-21,2015,3,21,3,2zq1kw,engineering.pinterest.com,Pinterest announces Pinnability: how they use machine learning to improve the home feed,https://www.reddit.com/r/MachineLearning/comments/2zq1kw/pinterest_announces_pinnability_how_they_use/,cloud_song,1426874400,,0,0,False,http://b.thumbs.redditmedia.com/r1c1BIDAtoe4xIss4phDgBzaowsrrLsPuoWWI5BUfJA.jpg,,,,,
292,MachineLearning,t5_2r3gv,2015-3-21,2015,3,21,3,2zq2wy,blogs.princeton.edu,"Deep Stuff About Deep Learning - Microsoft scientist talks about the math behind deep learning, and the effort to understand it on a theoretical level",https://www.reddit.com/r/MachineLearning/comments/2zq2wy/deep_stuff_about_deep_learning_microsoft/,notarowboat,1426874999,,24,43,False,default,,,,,
293,MachineLearning,t5_2r3gv,2015-3-21,2015,3,21,4,2zqb2c,self.MachineLearning,How do you initialize your recurrent network weights?,https://www.reddit.com/r/MachineLearning/comments/2zqb2c/how_do_you_initialize_your_recurrent_network/,rantana,1426878796,Has anyone found a nice scheme for initializing weights/biases of a recurrent network? ,6,3,False,self,,,,,
294,MachineLearning,t5_2r3gv,2015-3-21,2015,3,21,4,2zqdr6,blogs.princeton.edu,"Deep stuff about deep learning? ""no one has a real clue about whats going on""",https://www.reddit.com/r/MachineLearning/comments/2zqdr6/deep_stuff_about_deep_learning_no_one_has_a_real/,[deleted],1426880031,,0,4,False,default,,,,,
295,MachineLearning,t5_2r3gv,2015-3-21,2015,3,21,6,2zqrxl,arxiv.org,Optimizing Neural Networks with Kronecker-factored Approximate Curvature,https://www.reddit.com/r/MachineLearning/comments/2zqrxl/optimizing_neural_networks_with_kroneckerfactored/,[deleted],1426886811,,4,4,False,default,,,,,
296,MachineLearning,t5_2r3gv,2015-3-21,2015,3,21,6,2zqsdm,youtube.com,NVIDIA Digits DevBox and Deep Learning Demonstration - GTC 2015,https://www.reddit.com/r/MachineLearning/comments/2zqsdm/nvidia_digits_devbox_and_deep_learning/,[deleted],1426887040,,3,7,False,default,,,,,
297,MachineLearning,t5_2r3gv,2015-3-21,2015,3,21,6,2zqv80,self.MachineLearning,Clustering Classification With Less Features,https://www.reddit.com/r/MachineLearning/comments/2zqv80/clustering_classification_with_less_features/,MrTwiggy,1426888433,"Say I've trained a model with k-means clustering. I've run it over a range of k values and determined the batch amount that best minimizes distortion. Suppose the model was trained on X features.

Now, I have new incoming data that I'd like to classify among these clusters, however, I only have half as many features available (X/2). Is it possible to classify these new data points using a clustering that relies on more data than is currently available?

I'm a beginner to MachineLearning in general, so apologies if I'm completely missing the mark or my question doesn't make sense. I've attempted to search the problem, but couldn't find any discussions about classifying data using clusterings that rely on more features. Any links/articles/thoughts would be appreciated.

*Sub-Note:* For anyone familiar with League of Legends (the videogame), and interested in some background info on the problem. I'm attempting to cluster combinations of runes, summoner spells, and champion choice to classify different players by their champion/rune/spell choices before a game starts. However, the initial clustering is meant to train on post-game information, such as their average damage dealt or received, which is of course not available before a match starts when we are classifying players. Ideally, this classification would be used to reduce the features used in determining team composition.",3,0,False,self,,,,,
298,MachineLearning,t5_2r3gv,2015-3-21,2015,3,21,8,2zr58v,self.MachineLearning,[Research question] What are some non-empirical and theoretical areas in ML?,https://www.reddit.com/r/MachineLearning/comments/2zr58v/research_question_what_are_some_nonempirical_and/,5bits,1426893516,"Comp. Sci. undergrad here. I'm looking to go to graduate school and become a researcher. I have some experience with ML. I've done some projects which have taught me about the many types of algorithms in use, their pros/cons, and all the minor tweaks one can do to them.

Other CS fields I'm interested in are discrete mathematics (esp. graph theory), algorithms, complexity, and the like. Lately I've been turned off by ML because of its largely empirical nature: many papers involve tweaking an existing method, comparing them on a few datasets, and claiming success.

What areas of ML are less empirical, more theoretical, and even intersect with my above interests? Are these areas bubbling with new developments? In general, what advice would you give to someone who's interested in ML, but not its empirical nature?",6,1,False,self,,,,,
299,MachineLearning,t5_2r3gv,2015-3-21,2015,3,21,9,2zresl,self.MachineLearning,How to PCA large data sets? I'm running out of memory.,https://www.reddit.com/r/MachineLearning/comments/2zresl/how_to_pca_large_data_sets_im_running_out_of/,Registerml,1426898649,"I have a large data set of large dimensional vectors to which I am applying PCA (via scikit learn). Specifically I'm using the randomized version. 

Problem: It's not working because I'm running out of memory to even load such a big data set into ram. 

My current solution is to learn a PCA model on a small but representative subset of my data. Then apply that to the larger data, vector by vector in an iterative way. This works, but I am concerned on accuracy because the entire set was not learned. Are there better techniques? Your advice is very much appreciated.
",16,5,False,self,,,,,
300,MachineLearning,t5_2r3gv,2015-3-21,2015,3,21,10,2zrllv,github.com,"Reimplementation of DeepMind's DRAW in Theano, by Jorg Bornschein",https://www.reddit.com/r/MachineLearning/comments/2zrllv/reimplementation_of_deepminds_draw_in_theano_by/,kkastner,1426902641,,6,55,False,http://b.thumbs.redditmedia.com/k731W1XbUpATVetRyVAc00-ztEVNd7zntNDgcxP6Kfc.jpg,,,,,
301,MachineLearning,t5_2r3gv,2015-3-22,2015,3,22,1,2ztd1g,coursera.org,Self-Paced Coursera Machine Learning now available!,https://www.reddit.com/r/MachineLearning/comments/2ztd1g/selfpaced_coursera_machine_learning_now_available/,zen91,1426954230,,49,204,False,http://b.thumbs.redditmedia.com/k5y5hjIT7tTvN2sW1gQZp16iR8cThRBhRO2ObbZRj2o.jpg,,,,,
302,MachineLearning,t5_2r3gv,2015-3-22,2015,3,22,3,2ztw0w,self.MachineLearning,Can a convolutional neural network be used to classify malware?,https://www.reddit.com/r/MachineLearning/comments/2ztw0w/can_a_convolutional_neural_network_be_used_to/,DyingAdonis,1426964243,"I'm considering applying for an undergraduate research position at my university. My school is really pushing cyber security and I'd like to find an area where I can marry my interests in machine learning and neural networks with the kind of research my college would be interested in. I know that there has been some success with anomaly / intrusion detection via neural networks. I was inspired by Chris Domas' TED talk where he describes how he visualizes bytecode to recognize patterns that maybe convnets would be useful at the task of classifying software as malicious or benign. My experience is limited and so I thought that before getting in too deep I'd ask reddit what your thoughts are. I'm thinking that if this approach is technically pheasible that the major hurdle would be in the representation of the code. For example a binary 9 (1001) has only one bit difference to a binary 1 (0001) than the two bit difference to a binary 10 (1010) even though I would likely want the net to consider 9 to be more similar to an 10 than to 1. So a higher level of abstraction would likely be necessary. Do you think this approach is feasible? If so what do you think are the difficulties I have not yet considered? Thanks for your time.

TL;DR Could this work? 
Edit: binary fail",7,0,False,self,,,,,
303,MachineLearning,t5_2r3gv,2015-3-22,2015,3,22,4,2zu06r,self.MachineLearning,Should I worry about overfitting the cross-validation set?,https://www.reddit.com/r/MachineLearning/comments/2zu06r/should_i_worry_about_overfitting_the/,TheNotKnower,1426966365,"tl;dr: Is overfitting a correctly labeled cross-validation set a real problem when you test too many hypotheses, and if so what can be done about it?

---

I just came across this 1997 article where Andrew Ng talks about [Preventing ""Overfitting"" of Cross-Validation Data](http://ai.stanford.edu/~ang/papers/cv-final.pdf). This paper is specifically about the case where the cross validation (CV) data has some mislabeled samples, but I'm wondering about the case where the cross validation set is labeled perfectly, since it seems you can still overfit. 

For instance, see this [image](http://i.imgur.com/OZA3Vre.png) that depicts only CV data (since training data is irrelevant for this discussion). Line A is the true class separator, but it is not in our hypothesis set. Line B corresponds to a hypothesis with CV error = 1/6 and line C has a CV error of 0. However, B is actually much closer to A than C is, and has a lower (unknowable) ""real"" error.

In this case picking the hypothesis with the higher CV error would be better, which is similar to the cases in Ng's paper. He describes an algorithm for selecting a better hypothesis than the one with the lowest CV error, but it only works when some samples are mislabeled. Is there something that can be done in the analogous case where the CV data is correct (edit: other than getting more/better CV data or testing less hypotheses)? And is this something worth worrying about (maybe the odds of arriving at line C are pretty low)?",2,0,False,self,,,,,
304,MachineLearning,t5_2r3gv,2015-3-22,2015,3,22,4,2zu1ju,self.MachineLearning,"[Python] I have a clustering data mining project, and I am stuck on finding distances to build my clusters.",https://www.reddit.com/r/MachineLearning/comments/2zu1ju/python_i_have_a_clustering_data_mining_project/,stumblelightly22,1426967070,"I am developing a clustering program in Python using Pandas. I have a data set of about 48000 values with 13 different attributes that are unknown. I took this data into a Pandas DataFrame and did all relevant preprocessing with normalizing continuous variables and replacing others. 

To maximize efficiency, I am implementing the buckshot method, where I take sqrt(n) of the entries and use those to develop my clusters (with n being the count of entries in the data frame). 

I used the random library in numpy to find n random ints that fall between 0 and my greatest index; I then used 'isin' for the first data frame to select those entries that matched the randomly created list of indexes. 

I now need to compare each attribute for each entry to all of the other entries from that randomly selected subset, find the minimum overall Euclidean distance through the sum of the differences squared, cluster together those instances that are closest, and then repeat the process, selecting new random indexes, including the new centroid of the cluster developed for distance comparison, and keeping track of inter and intra cluster distance. 

My main concern right now is getting the differences between the different entries and keeping track of them. I tried using the 'diff()' function for a column in a data frame where it compares the first row to all others, but I don't know how to iterate that for other rows or keep track of all those differences/have them tied to the original data frame. If you have any advice or guidance for this sort of unsupervised algorithm, that would be greatly appreciated. I don't yet know for sure how I will be storing the clusters in comparison to the singleton entries in the dataframe, but my main confusion right now is how I will make and track differences among rows. Thanks for your help and time! ",4,1,False,self,,,,,
305,MachineLearning,t5_2r3gv,2015-3-22,2015,3,22,5,2zu86o,self.MachineLearning,Planning to implement Text Understanding from Scratch paper,https://www.reddit.com/r/MachineLearning/comments/2zu86o/planning_to_implement_text_understanding_from/,iwantedthisusername,1426970500,"So I'm looking to implement this paper: http://arxiv.org/pdf/1502.01710v1.pdf. 

I have an EC2 GPU optimized instance set up in with Theano and Lasagne installed. And everything is working smoothly. At this point it's just reconstructing what they've discussed in the paper. From what I can tell, everything that they use comes prebuilt into Lasange (http://lasagne.readthedocs.org/en/latest/index.html)

Has anyone else tried a replication and had any success? Any tips before I begin? This is my first time working with a GPU and my first time working with Deep Learning. I've used other ML models but with far less data and only on a CPU.",19,12,False,self,,,,,
306,MachineLearning,t5_2r3gv,2015-3-22,2015,3,22,13,2zvisv,self.MachineLearning,Is this a Hidden Markov Model?,https://www.reddit.com/r/MachineLearning/comments/2zvisv/is_this_a_hidden_markov_model/,fourhoarsemen,1426997681,"Hi guys and gals. This post is long, so scroll downwards for a tl;dr :)

I posted not too long ago a link to my project titled: [autocomplete](https://github.com/rodricios/autocomplete). What resulted from that post was a correction to my labeling of the project. I initially thought it was a HMM, where in reality it was a Markov chain. 

The thread of conversation that pointed out my mistake can be read [here.](https://www.reddit.com/r/programming/comments/2xwi3g/textbooks_dont_explain_the_practicality_of_hidden/cp41osh)

The thread is too long to post here, but here's a quote from a /u/pretz that detailed some of what was lacking my project: 

&gt; [..] You could use the forward algorithm to determine the probability of an entire sequence, but with words in a string [autocomplete] is just a standard markov model, not a hidden markov model (because you know the words, there is no hidden state to estimate for the sequence)

/u/pretz goes on to give a quick synopsis of the HMM he implemented for speech recognition, and then he ends the comment with: 

&gt; In your code you see the previous words (your states are your observations), there is no uncertainty about which observations map to which state. Hopefully that helps.

In response to this comment, and I'll be frank and say that much of the terminology went way above my head, I say:

&gt; Oh, so in essence, there's a lot more uncertainty being dealt with in HMM's?

I continue the above remark by giving what I think is a possible implementation for iPhone's autocomplete &amp; correction functionality, which I've been told IS a HMM (I'll refrain from naming names, but he's academic who's friends with someone who worked with the team at Apple that implemented its phone's autocomplete &amp; correct - yes, I do believe him). 

Now in the context of that comment, I get upvoted - hopefully a sign that my hunch was correct - and then /u/romanows responded with: 

&gt; More uncertainty because you can't observe the thing that is ""causing"" (or ""generating"") the only things you can observe. And the things you can observe are noisy. [..]

So with all that said, let me describe the latest implementation of the autocomplete module (the version in which I'm confused about):

Using a basic ""fat-finger"" model like so: 

    NEARBY_KEYS = {
        'a': 'qwsz',
        'b': 'vghn',
        'c': 'xdfv',
        'd': 'erfcxs',
        'e': 'rdsw',
        'f': 'rtgvcd',
        ... }

and assuming that you're attempting to write say... *body*:

    autocomplete.predict('the','bo')
    
    [('bone', 175),
     ('body', 149),
     ('bones', 122),
     ('boy', 46),
     ('bottom', 32),
     ('box', 24),
     ...]

and then you make the error of typing an ""f"" instead of a ""d"", the current implementation will 
recognize the ""f"" in ""bof"", and it will also use the fat-finger model to produce: 

    [""bor"", ""bot"", ""bog"", ""bov"", ""boc"", ""bod"", ""bof""]

Conditioned on ""the"", the implementation will then query for the top N frequent words prefixed by 
any one of the above ""bo[rtgvcdf]"" strings, resulting in:

    autocomplete.predict('the','bof')

    [('body', 149),
     ('bottom', 32),
     ('borzois', 16),
     ('bottle', 13),
     ('bodies', 13),
     ('border', 12)
     ...]

You can find the code where this occurs [here](https://github.com/rodricios/autocomplete/blob/master/autocomplete/autocomplete.py#L52)

Finally to my question: does the ""hidden"" step of generating potential fat-finger candidates constitute the ""hidden"" step in HMM's? Or is what I did just a clever hack?

Any insight is welcomed and appreciated! 

I guess I should also note that I do have an agenda with this post. An objective of mine is to come up with simple explanations and applications to some of the ""introductory"" machine learning material; basically approaching some of these concepts with intuition rather than theoretical foundation. I'm really not trying to enter a debate about which approach to learning is ""better,"" I'm just trying to lower the barrier to entry and get more young'ins and practically anyone into this awesome field of study!

tl;dr: 

I'm out to get your kids! Jk (read the last paragraph if you were creeped out). I originally built an autocomplete module via a Markov chain. I recently added a ""fat-finger"" error model to take into account the possibility that the user mistyped a letter, and provide the suggestions based off those extra considerations. Now I'm trying to see if the implementation is actually a HMM. But I'm insecure about my own understanding of what is ""required"" for such an implementation.

Edit: clarified that the current version of [autocomplete](https://github.com/rodricios/autocomplete) is the implementation I'm having questions about.",5,4,False,self,,,,,
307,MachineLearning,t5_2r3gv,2015-3-22,2015,3,22,18,2zw1ya,fivethirtyeight.com,Interview of America's first data scientist - Mr. D.J.Patil with FiveThirtyEight,https://www.reddit.com/r/MachineLearning/comments/2zw1ya/interview_of_americas_first_data_scientist_mr/,kunalj101,1427015229,,6,4,False,http://a.thumbs.redditmedia.com/uq9xacemFIMzt8lt9KBWfsAIp3v_ra2l4gbaFOLLLM8.jpg,,,,,
308,MachineLearning,t5_2r3gv,2015-3-22,2015,3,22,20,2zw8ab,tjo-en.hatenablog.com,Machine learning for package users with R (1): Decision Tree,https://www.reddit.com/r/MachineLearning/comments/2zw8ab/machine_learning_for_package_users_with_r_1/,TJO_datasci,1427022874,,0,1,False,default,,,,,
309,MachineLearning,t5_2r3gv,2015-3-22,2015,3,22,22,2zwfl3,kdnuggets.com,7 common mistakes when doing Machine Learning,https://www.reddit.com/r/MachineLearning/comments/2zwfl3/7_common_mistakes_when_doing_machine_learning/,[deleted],1427030025,,7,21,False,default,,,,,
310,MachineLearning,t5_2r3gv,2015-3-22,2015,3,22,23,2zwk80,motherboard.vice.com,"The Simple, Elegant Algorithm That Makes Google Maps Possible",https://www.reddit.com/r/MachineLearning/comments/2zwk80/the_simple_elegant_algorithm_that_makes_google/,keghn,1427033502,,2,1,False,http://b.thumbs.redditmedia.com/34lOfXebOdP0EIAc5pp7zRvaJzhHNKuzLw2edJDmb9k.jpg,,,,,
311,MachineLearning,t5_2r3gv,2015-3-23,2015,3,23,1,2zwxwn,self.MachineLearning,What is the Simplest LSTM Model to use?,https://www.reddit.com/r/MachineLearning/comments/2zwxwn/what_is_the_simplest_lstm_model_to_use/,[deleted],1427041625,"I am doing some research on sequential prediction. The problem is assigning labels to words based on an annotated dataset. This is not POS tagging or NER, the labels relate to concepts present in essays written by students. I want to try to use an LSTM model to predict each word label, having tried simpler methods with moderate success. There are a number of libraries out there that have LSTM's implemented, such as Torch, Lasagne, Caffe and DeepLearning4j. However, reading through the code they are somewhat incomprehensible to someone who knows a reasonable amount about deep learning but isn't an expert in the particular library. Ideally I'd like something that's almost as easy to use as Sklearn. I feed it some tagged dataset, set the model parameters and off we go. Or i'd settle for a more complex library with good documentation and tutorials. Does this exist? Any recommendations would be gratefully appreciated. ",0,1,False,default,,,,,
312,MachineLearning,t5_2r3gv,2015-3-23,2015,3,23,1,2zwz0v,voidpatterns.org,Topic Modeling with R and Keyword Extraction with GlossEx: A Case Study on Conspiracy Theory,https://www.reddit.com/r/MachineLearning/comments/2zwz0v/topic_modeling_with_r_and_keyword_extraction_with/,voidpatterns,1427042232,,0,9,False,http://b.thumbs.redditmedia.com/zWWQuSHJAXnmPPnSZsJV2HuY8kHoKmD9b7QmCYUzjiI.jpg,,,,,
313,MachineLearning,t5_2r3gv,2015-3-23,2015,3,23,2,2zx289,stuartreid.co.za,10 Common Misconceptions about Neural Networks,https://www.reddit.com/r/MachineLearning/comments/2zx289/10_common_misconceptions_about_neural_networks/,alexeyr,1427043857,,2,6,False,http://b.thumbs.redditmedia.com/E7f-J9VUPbUEslWhQf3pbdE22JGXJB0ktzwy_dlRZTU.jpg,,,,,
314,MachineLearning,t5_2r3gv,2015-3-23,2015,3,23,3,2zxari,yuviral.com,12 Girls That Never Skip Leg Day.,https://www.reddit.com/r/MachineLearning/comments/2zxari/12_girls_that_never_skip_leg_day/,Dale_Curtis299,1427048132,,0,1,False,default,,,,,
315,MachineLearning,t5_2r3gv,2015-3-23,2015,3,23,3,2zxeg5,self.MachineLearning,Bayesian Error question,https://www.reddit.com/r/MachineLearning/comments/2zxeg5/bayesian_error_question/,OlTartToter,1427049952,"Can someone please explain Bayesian error to me using a 2 class 1d Gaussian problem as an example please. I understand the basic of Bayes formula however I really don't understand the math behind the error rule. If needed I will share my class notes which sadly don't explain well enough ( just for clarification I asked my professor and he just gave me an answer which summed too "" do it yourself"") . please help me I really want to understand this.",0,0,False,self,,,,,
316,MachineLearning,t5_2r3gv,2015-3-23,2015,3,23,3,2zxfma,self.MachineLearning,What is the simplest LSTM Library to Use?,https://www.reddit.com/r/MachineLearning/comments/2zxfma/what_is_the_simplest_lstm_library_to_use/,simonhughes22,1427050526,"I am doing some research on sequential prediction. The problem is assigning labels to words based on an annotated dataset. This is not POS tagging or NER, the labels relate to concepts present in essays written by students. I want to try to use an LSTM model to predict each word label, having tried simpler methods with moderate success. There are a number of libraries out there that have LSTM's implemented, such as Torch, Lasagne, Caffe and DeepLearning4j. However, reading through the code they are somewhat incomprehensible to someone who knows a reasonable amount about deep learning but isn't an expert in the particular library. Ideally I'd like something that's almost as easy to use as Sklearn. I feed it some tagged dataset, set the model parameters and off we go. Or i'd settle for a more complex library with good documentation and tutorials. Does this even exist? Any recommendations would be gratefully appreciated.",13,11,False,self,,,,,
317,MachineLearning,t5_2r3gv,2015-3-23,2015,3,23,4,2zxkxy,self.MachineLearning,[Python] Am I able to compare entries in a pandas dataframe and output a difference without another data structure?,https://www.reddit.com/r/MachineLearning/comments/2zxkxy/python_am_i_able_to_compare_entries_in_a_pandas/,stumblelightly22,1427053141,"I have a DataFrame with 13 features and about 48000 entries (which will be randomly sampled down to ~200 for efficiency's sake) that is the input for a larger clustering program. 

I am looking to get the minimum distance between features (by summing the squares of differences of each of the features) and I want to return the two entries that have the minimum distance then combine those values.

I know the 'diff()' function to compare the first entry in a data frame to all other entries, but iterating that and squaring those results without creating a whole other data frame is confusing me. Any help or guidance would be greatly appreciated. ",1,0,False,self,,,,,
318,MachineLearning,t5_2r3gv,2015-3-23,2015,3,23,6,2zxvf3,self.MachineLearning,Beautiful Theano code to learn from?,https://www.reddit.com/r/MachineLearning/comments/2zxvf3/beautiful_theano_code_to_learn_from/,vikkamath,1427058179,"I've been coding in Theano for the past couple of months and I wouldn't really go so far as to say I'm a good programmer - my code is written in a 'quick and dirty' manner and works just to get the job done. Also, because Theano's a rather esoteric paradigm that's primarily used in academia, I'm of the opinion that 'beautifully written' code is hard to come by (possibly because I haven't looked hard enough for it). If there's any code (that uses Theano) that you find particularly elegant or ingenious, please post a link to it here",4,33,False,self,,,,,
319,MachineLearning,t5_2r3gv,2015-3-23,2015,3,23,6,2zy0jr,quantombone.blogspot.com,Deep Learning vs Machine Learning vs Pattern Recognition,https://www.reddit.com/r/MachineLearning/comments/2zy0jr/deep_learning_vs_machine_learning_vs_pattern/,nogo09,1427060609,,0,5,False,http://a.thumbs.redditmedia.com/ED51Kyj_ILz1b8b_nx4gvGePtYIkWxdNr-43F_--4v8.jpg,,,,,
320,MachineLearning,t5_2r3gv,2015-3-23,2015,3,23,7,2zy8tq,self.MachineLearning,University problem need help!,https://www.reddit.com/r/MachineLearning/comments/2zy8tq/university_problem_need_help/,xtrancea,1427064616,"Part of my university project is creating a diet plan for individuals with specific genes. What we want to do is use successful results of the diet plan we recommended for specific gene users and implement those successful diet plans for other users with a similar gene. Then the aim is to find out differences between successful and unsuccessful users both having the same gene type and how other factors can help guide users in a successful direction. What direction should i take in order to tackle such a thing? I am new to machine learning and do not have much programming skills so a layman description would work best which i can forward through to my peers. Thanks in advance!

Also: I know epigenetics, the human biome and tons of other factors influence the way we are expressed as a whole, but this is just some statistics we would like to work with and see where it takes us. ",3,0,False,self,,,,,
321,MachineLearning,t5_2r3gv,2015-3-23,2015,3,23,12,2zz1q0,self.MachineLearning,Applying for entry-level jobs without PhD in CS/Stats/ML?,https://www.reddit.com/r/MachineLearning/comments/2zz1q0/applying_for_entrylevel_jobs_without_phd_in/,[deleted],1427079664,.,1,0,False,default,,,,,
322,MachineLearning,t5_2r3gv,2015-3-23,2015,3,23,15,2zzif8,intelligentutility.com,7 reasons why utilities should be using machine learning,https://www.reddit.com/r/MachineLearning/comments/2zzif8/7_reasons_why_utilities_should_be_using_machine/,ZaneSpritzer,1427090810,,9,4,False,http://b.thumbs.redditmedia.com/5J6VHO19M6ac6UWzYqX6t9s_cFk2JlS72OPvtacMrng.jpg,,,,,
323,MachineLearning,t5_2r3gv,2015-3-23,2015,3,23,15,2zzj5t,zdnet.com,Pinterest built machine learning 'Pinnability' to surface more relevant content,https://www.reddit.com/r/MachineLearning/comments/2zzj5t/pinterest_built_machine_learning_pinnability_to/,EllisLongmore,1427091420,,1,1,False,http://b.thumbs.redditmedia.com/X-QcpMVPMPheYIIdnKeUpUq0Ng1xsHoCE_sjJpyiZfE.jpg,,,,,
324,MachineLearning,t5_2r3gv,2015-3-23,2015,3,23,16,2zznsq,foldl.me,Conditional generative adversarial networks for face generation,https://www.reddit.com/r/MachineLearning/comments/2zznsq/conditional_generative_adversarial_networks_for/,iori42,1427095547,,16,21,False,http://b.thumbs.redditmedia.com/1qV1DSbJpXO2f3TLvpF2P0_kZZC8SqcmmGBG_lCkxtE.jpg,,,,,
325,MachineLearning,t5_2r3gv,2015-3-23,2015,3,23,18,2zzw1r,self.MachineLearning,Where to start?,https://www.reddit.com/r/MachineLearning/comments/2zzw1r/where_to_start/,Eildosa,1427104117,"Hi,

I want to do a capcha breaker, I'm a JEE dev with no machine vision/learning background.
All I know is that ""neural network"" can help me.

I have a set a 5000 captcha (same type of captgcha) that I translated manually.
What kind of neural network should I use?
Can you point me toward some good courses?

Thanks.",6,0,False,self,,,,,
326,MachineLearning,t5_2r3gv,2015-3-23,2015,3,23,20,3000iw,github.com,PyGame Interactive KNN for Education,https://www.reddit.com/r/MachineLearning/comments/3000iw/pygame_interactive_knn_for_education/,sheepsy90,1427108407,,0,2,False,http://b.thumbs.redditmedia.com/JRvNjo6RTDRLkI1dyOmY5xWHM0a82L2yZcndE6LvoaA.jpg,,,,,
327,MachineLearning,t5_2r3gv,2015-3-23,2015,3,23,22,300a2g,arxiv.org,The Loss Surfaces of Multilayer Networks,https://www.reddit.com/r/MachineLearning/comments/300a2g/the_loss_surfaces_of_multilayer_networks/,iori42,1427115808,,4,7,False,default,,,,,
328,MachineLearning,t5_2r3gv,2015-3-23,2015,3,23,23,300gmg,burakkanber.com,Machine Learning: Full-Text Search in Javascript (Part 1: Relevance Scoring),https://www.reddit.com/r/MachineLearning/comments/300gmg/machine_learning_fulltext_search_in_javascript/,seabass,1427119713,,0,3,False,default,,,,,
329,MachineLearning,t5_2r3gv,2015-3-24,2015,3,24,0,300ssn,self.MachineLearning,Ranking on data manifolds.,https://www.reddit.com/r/MachineLearning/comments/300ssn/ranking_on_data_manifolds/,[deleted],1427125699,"Recently, I've been trying to understand manifolds and ranking on data manifolds as shown in [this paper](http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/pdfs/pdf2334.pdf).  There are newer articles on this particular topic, but they focus mostly on applications rather than the actual algorithm, which is what I'm interested in.

I've seen an implementation of this algorithm (uploaded to [github](https://github.com/chevineleven/manifold-ranking) from [here](http://csbl.bmb.uga.edu/publications/materials/qiliu/blood_secretory_protein.html)) which is composed of only matrix manipulations, but I'm not exactly sure what properties from linear algebra that makes it work.  From what I understand, the input list is sorted based on the max pairwise similarity descending, then a similarity matrix is constructed using the rbf kernel.  The matrix is then normalized.  Here I'm unsure of why the normalization formula works as well as the convergence step.

This post is partly to ask for help about manifolds and manifold-based ranking, partly to put my thoughts down on paper (sorry if this seems like rambling). Basically, I understand the steps to reproduce the algorithm, but not quite why it works.  Can anyone point me to where I can learn more about manifold ranking, or manifolds in general?",0,4,False,default,,,,,
330,MachineLearning,t5_2r3gv,2015-3-24,2015,3,24,1,300xkl,self.MachineLearning,Good classifier for 100+ classes,https://www.reddit.com/r/MachineLearning/comments/300xkl/good_classifier_for_100_classes/,YourWelcomeOrMine,1427127798,"I have a very large number of class, each with a relatively small number of training instances (10-12). What are some ideal classifiers for this task?

**EDIT:**  
Thank you all for your great responses. My advisor wants me to hold off on trying a new classifier at the moment, but this gives me some great resources for when we're ready.",17,15,False,self,,,,,
331,MachineLearning,t5_2r3gv,2015-3-24,2015,3,24,5,301vt1,self.MachineLearning,Gaussian mixtures and clustering: how to apply model?,https://www.reddit.com/r/MachineLearning/comments/301vt1/gaussian_mixtures_and_clustering_how_to_apply/,terancee,1427142462,"Suppose that I have a model that is the mixture of the Gaussians. I have estimated the parameters of such model by e.x. EM-algorithm.

The question is: how can I apply this model that it would assign some given point to the cluster?

I believe that I have to introduce the coincidence level as well, but might be mistaken.",4,0,False,self,,,,,
332,MachineLearning,t5_2r3gv,2015-3-24,2015,3,24,6,3024w2,en.wikipedia.org,"Can someone please explain how the formula is contructed? I understand intuitively how it works, but I cannot explain why this formula is correct.",https://www.reddit.com/r/MachineLearning/comments/3024w2/can_someone_please_explain_how_the_formula_is/,Postal2Dude,1427146215,,0,0,False,http://a.thumbs.redditmedia.com/daTYqSSeKo894GTriDjW6_FM_SeOjgIkE0ijhz7gEb4.jpg,,,,,
333,MachineLearning,t5_2r3gv,2015-3-24,2015,3,24,6,302811,self.MachineLearning,Fractional order PID implementation,https://www.reddit.com/r/MachineLearning/comments/302811/fractional_order_pid_implementation/,joeflux,1427147526,"I'm a programmer and having a hard time translating from the theoretical papers across to real c++ code.

How would I implement a Fractional order PID controller?  I've been searching google, but I can't find anything at all.",5,0,False,self,,,,,
334,MachineLearning,t5_2r3gv,2015-3-24,2015,3,24,6,3028hu,mlconf.com,"MLconf NYC is (3/27) next week. Talks from Google Research, Facebook, OpenTable, IBM Watson, Intel Labs, Yahoo Labs &amp; more. We will be live streaming!",https://www.reddit.com/r/MachineLearning/comments/3028hu/mlconf_nyc_is_327_next_week_talks_from_google/,shonburton,1427147737,,3,26,False,http://b.thumbs.redditmedia.com/xN9ZeMm71iZ-scgBstdB0uPW4i_ZUfdRJ7dtEA5sn1g.jpg,,,,,
335,MachineLearning,t5_2r3gv,2015-3-24,2015,3,24,8,302lgo,machineslikeus.com,Artificial intelligence systems more apt to fail than destroy,https://www.reddit.com/r/MachineLearning/comments/302lgo/artificial_intelligence_systems_more_apt_to_fail/,MachinesLikeUs,1427153564,,0,1,False,default,,,,,
336,MachineLearning,t5_2r3gv,2015-3-24,2015,3,24,8,302mhx,youtube.com,Fei Fei Li: How we're teaching computers to understand pictures (x-post r/futurology),https://www.reddit.com/r/MachineLearning/comments/302mhx/fei_fei_li_how_were_teaching_computers_to/,evc123,1427154049,,6,8,False,http://b.thumbs.redditmedia.com/5fMTnm9pMbEKAc6MuE_4NVJbRThxDFjPJ4bN3oKKfFo.jpg,,,,,
337,MachineLearning,t5_2r3gv,2015-3-24,2015,3,24,9,302rnq,self.MachineLearning,Stanford UFLDL Tutorial Help,https://www.reddit.com/r/MachineLearning/comments/302rnq/stanford_ufldl_tutorial_help/,[deleted],1427156464,"Link: http://deeplearning.stanford.edu/tutorial/

I've been trying to complete the linear regression tutorial but I've been having trouble getting it to work. I've done a few other machine learning linear regression tutorials and had no issues. Any help would be much appreciated.",0,0,False,default,,,,,
338,MachineLearning,t5_2r3gv,2015-3-24,2015,3,24,10,303039,mp.soulhackerslabs.com,A simple demonstration of how sentiment analysis will change the way business intelligence is delivered to app developers. (Beware App Annie),https://www.reddit.com/r/MachineLearning/comments/303039/a_simple_demonstration_of_how_sentiment_analysis/,carlos_argueta,1427160541,,8,0,False,http://b.thumbs.redditmedia.com/MwPAsQbMjloMbEQfxKOYyEXSwmeWohpdJPOj56jH3xw.jpg,,,,,
339,MachineLearning,t5_2r3gv,2015-3-24,2015,3,24,10,3032b1,self.MachineLearning,Ask ML: Anyone reproduced the results from the ADAM paper?,https://www.reddit.com/r/MachineLearning/comments/3032b1/ask_ml_anyone_reproduced_the_results_from_the/,[deleted],1427161616,"I'm having difficulty reproducing some results from the [ADAM paper](http://arxiv.org/abs/1412.6980), specifically their PI-MNIST result with no dropout (Figure 2b). 
Has anyone managed to do that? 

This is a 784-1000-1000-10 (ReLU-ReLU-SOFTMAX) classifier.

I got ADAM to work pretty well on some other problems (autoencoders), and AdaDelta and SGD (with the right learning rate) work really well on the same PI-MNIST objective (reducing the odds of a bug there), but my implementation of ADAM seems to perform quite poorly on PI-MNIST.

Some things were left unspecified in the paper, such as how the parameters are initialized, so I made what I think is the most conventional choice: 0-centered normal distribution with variance `1/number_of_layer_inputs`, except biases and the last layer's weights, which were set to 0. The inputs are left in the [0, 1] range.

So, while Figure 2b shows 1e-4 cross-entropy training error after 10 epochs, I see 1.5e-2, with 280/60,000 misclassifications.

I use the hyperparameter settings from v4 of the paper, but without `lambda` as in v1. I think the authors added `lambda` in v2 only to prove some theoretical convergence, while it has no effect on the experiments (if they even used it) -- after 10 epochs, or 4687 updates, `lambda` contributes a factor of only `0.99995`",18,15,False,self,,,,,
340,MachineLearning,t5_2r3gv,2015-3-24,2015,3,24,10,3032y4,news.cornell.edu,Images that fool computer vision raise security concerns,https://www.reddit.com/r/MachineLearning/comments/3032y4/images_that_fool_computer_vision_raise_security/,oreo_fanboy,1427161948,,43,103,False,http://b.thumbs.redditmedia.com/_TFeqXSjS4LIE0YVP8xuls3YfWwrdy0C31YtIWOG7Vw.jpg,,,,,
341,MachineLearning,t5_2r3gv,2015-3-24,2015,3,24,13,303m7u,arxiv.org,Understanding Deep Image Representations by Inverting Them,https://www.reddit.com/r/MachineLearning/comments/303m7u/understanding_deep_image_representations_by/,rantana,1427172435,,6,6,False,default,,,,,
342,MachineLearning,t5_2r3gv,2015-3-24,2015,3,24,15,303upd,self.MachineLearning,Need a clustering algorithm similar to DBSCAN that I can specify the number of clusters,https://www.reddit.com/r/MachineLearning/comments/303upd/need_a_clustering_algorithm_similar_to_dbscan/,Jonno_FTW,1427178765,"Is there a clustering algorithm that can find clusters of arbitrary shape, that I can specify the number of target clusters (similar to k-means)? 

I tried using grid search for the DBSCAN parameters epsilon and min_points, but it takes forever with about ~3k data points and didn't work very well (it mostly returned all noise or 2 tiny clusters and the rest noise).

Does such a method exist or do I have to roll my own? My data looks like this:

http://i.imgur.com/MM3asg3.png",11,3,False,self,,,,,
343,MachineLearning,t5_2r3gv,2015-3-24,2015,3,24,16,303ypt,kerke-extruder.com,Twin Screw Extruder - Kerke Extruder,https://www.reddit.com/r/MachineLearning/comments/303ypt/twin_screw_extruder_kerke_extruder/,kerkeextruder,1427182471,,0,0,False,default,,,,,
344,MachineLearning,t5_2r3gv,2015-3-24,2015,3,24,17,3042mt,self.MachineLearning,what is the point of distance supervision?,https://www.reddit.com/r/MachineLearning/comments/3042mt/what_is_the_point_of_distance_supervision/,matthias_anglicus,1427186452,"it seems to me that using an existing knowledge base and then applying that to unstructured text to learn relations is crazy, why not just use the knowledge base as training data?

of what utility is the process of applying the knowledge base to the unstructured text?",1,2,False,self,,,,,
345,MachineLearning,t5_2r3gv,2015-3-24,2015,3,24,18,3043r6,self.MachineLearning,Need help for my MSc Thesis!,https://www.reddit.com/r/MachineLearning/comments/3043r6/need_help_for_my_msc_thesis/,Narrevan,1427187639,"I am not sure if it is proper place to ask for this but still I am going to try.

Dear Redittors,

Currently I am undergoing my fight with MSc Thesis in CS and I need your help with some data gathering (at least for now).
My Thesis is about checking the results of classification for personalised crowdsourced data and to compare it with non personalised ones and as you can see I need a crowd to get some data so here is yours role in this project. I am asking about filling in this form every day ( if possible, you can even do this with previous days if you remember all of the facts needed :) ) with small surprise at the end ( like 58 of them, chosen randomly 1 each day ). So it will be really helpful if you could spend like 1-5 minutes each day to fill it in. 
Thank you very much in advance for your time and consideration.

Best regards,
Redittor.

Link: [http://sigma.ug.edu.pl/~pkrolik/datagatherer](http://sigma.ug.edu.pl/~pkrolik/datagatherer)

PS. If you have any questions, need more information about project and etc. feel free to ask here or private message.

PS2. If anyone will be interested in this data for own scientific reasons please feel free to ask me about it, probably it will be uploaded after 2nd phase of my MSc Thesis and I will try to announce that.",0,0,False,self,,,,,
346,MachineLearning,t5_2r3gv,2015-3-24,2015,3,24,18,30446c,karpathy.github.io,Hacker's guide to Neural Networks,https://www.reddit.com/r/MachineLearning/comments/30446c/hackers_guide_to_neural_networks/,clbam8,1427188064,,4,61,False,default,,,,,
347,MachineLearning,t5_2r3gv,2015-3-24,2015,3,24,22,304lv0,self.MachineLearning,Does anyone know how the Delphi Driverless Car is Doing?,https://www.reddit.com/r/MachineLearning/comments/304lv0/does_anyone_know_how_the_delphi_driverless_car_is/,Atrix621,1427202509,I am looking to find out how the Delphi Driverless Car trek across America is going.  I cannot find anything current on the vehicle. Thanks Reddit.,1,4,False,self,,,,,
348,MachineLearning,t5_2r3gv,2015-3-24,2015,3,24,22,304qlw,self.MachineLearning,"Silly showerthought: When we dream, are we doing some sort of autoencoding?",https://www.reddit.com/r/MachineLearning/comments/304qlw/silly_showerthought_when_we_dream_are_we_doing/,201109212215,1427205244,"This was the silly question of the day. Maybe /r/Showerthoughts would be more approriate (except for the audience, which is not familiar with machine learning)",2,0,False,self,,,,,
349,MachineLearning,t5_2r3gv,2015-3-24,2015,3,24,23,304uyp,self.MachineLearning,Resources for knowledge extraction from text,https://www.reddit.com/r/MachineLearning/comments/304uyp/resources_for_knowledge_extraction_from_text/,ChocoChooChoo,1427207562,"Are there any good resources (preferably books) on the subject of knowledge extraction (from text) that you could recommend? I wanted to work on that for a while, but couldn't find introductionary resources.",1,3,False,self,,,,,
350,MachineLearning,t5_2r3gv,2015-3-25,2015,3,25,1,305afm,imgur.com,"TIL 'Fucking, Austria' can be a difficult place name to automatically extract",https://www.reddit.com/r/MachineLearning/comments/305afm/til_fucking_austria_can_be_a_difficult_place_name/,LMR_adrian,1427214604,,3,2,False,http://a.thumbs.redditmedia.com/5xnOgmKgJtUs1KTxFExlV77LBmsc4qz8k9YvVNBan14.jpg,,,,,
351,MachineLearning,t5_2r3gv,2015-3-25,2015,3,25,1,305cie,blog.dominodatalab.com,10 Interesting Ways to Use Data Science,https://www.reddit.com/r/MachineLearning/comments/305cie/10_interesting_ways_to_use_data_science/,AnnaOnTheWeb,1427215550,,0,1,False,default,,,,,
352,MachineLearning,t5_2r3gv,2015-3-25,2015,3,25,2,305js3,blogs.technet.com,ML is powering the brains of the modern smart grid,https://www.reddit.com/r/MachineLearning/comments/305js3/ml_is_powering_the_brains_of_the_modern_smart_grid/,MLBlogTeam,1427218670,,0,1,False,default,,,,,
353,MachineLearning,t5_2r3gv,2015-3-25,2015,3,25,2,305me5,self.MachineLearning,Slow GPU performance on Amazon g2.2xlarge?,https://www.reddit.com/r/MachineLearning/comments/305me5/slow_gpu_performance_on_amazon_g22xlarge/,chrisjmccormick,1427219784,"We're trying to benchmark a particular task (vector distance calculations for clustering, nearest neighbor search, etc.) on a high end GPU, and we're playing with the g2.2xlarge instance. 

Trouble is, the performance is worse on the g2 instance than on my mid-range gaming card:

On the GTX 660 in my PC, it takes:

* 70ms to transfer the data to the GPU 

* 190ms to run the calculations

On a g2.2xlarge instance (GRID K520, 3,072 cores) it takes

* 290ms for the transfer 

* 430ms for the calculations. 


What are other peoples experiences working with these g2.2xlarge instances--is the performance any better than your personal computer?

I read that [Netflix ran into issues with GPU performance](http://techblog.netflix.com/2014/02/distributed-neural-networks-with-gpus.html):
&gt;In a virtualized environment such as the AWS cloud, these accesses cause a trap in the hypervisor that results in even slower access.
 
Not sure if I'm encountering the same issue; hoping to sanity check our results with others in the community.

Ideally, we'd like to benchmark a K80... Anyone know of a way to rent access to a Tesla K80?",16,3,False,self,,,,,
354,MachineLearning,t5_2r3gv,2015-3-25,2015,3,25,3,305qqb,youtube.com,GTC 2015: The Big Bang of Deep Learning,https://www.reddit.com/r/MachineLearning/comments/305qqb/gtc_2015_the_big_bang_of_deep_learning/,evc123,1427221642,,1,2,False,http://a.thumbs.redditmedia.com/G4il6P0ZavmZQKgq6i3shuwuIfaQXy6XqLD3L1RPYJ0.jpg,,,,,
355,MachineLearning,t5_2r3gv,2015-3-25,2015,3,25,4,305wcm,self.MachineLearning,Implementations and understanding of Semantic hashing,https://www.reddit.com/r/MachineLearning/comments/305wcm/implementations_and_understanding_of_semantic/,hnizdja2,1427224011,"Hi!

I'm going to use (maybe even implement) [Semantic hashing](http://www.cs.toronto.edu/~rsalakhu/papers/semantic_final.pdf) according to the paper by Ruslan Salakhutdinov.

1. I would like to ask if there is some implementation in Python (or in different languages) already? I have found just an original implementation in Matlab by Salakhutdinov, unfortunately the code is really hard to read and understand.

2. Is there any better and deeper explanation of Semantic hashing? Some parts of the paper are quite confusing and I haven't found anything more explanatory.

Thanks for all answers in advance!",2,2,False,self,,,,,
356,MachineLearning,t5_2r3gv,2015-3-25,2015,3,25,4,305x6o,blog.monkeylearn.com,Sentiment analysis on web scraped data,https://www.reddit.com/r/MachineLearning/comments/305x6o/sentiment_analysis_on_web_scraped_data/,wildcodegowrong,1427224382,,0,0,False,default,,,,,
357,MachineLearning,t5_2r3gv,2015-3-25,2015,3,25,4,305yt3,self.MachineLearning,Semantic memories now live,https://www.reddit.com/r/MachineLearning/comments/305yt3/semantic_memories_now_live/,Gavagai_Corp,1427225118,"Our semantic memories now see the daylight for the first time. 100% unsupervised, data driven machine learning: Try ""suit"" or ""asshat"" in our live lexicon: http://lexicon.gavagai.se 
",2,2,False,self,,,,,
358,MachineLearning,t5_2r3gv,2015-3-25,2015,3,25,4,30616e,self.MachineLearning,What are the pros an cons of Variational Autoencoders in relation to normal Autoencoders?,https://www.reddit.com/r/MachineLearning/comments/30616e/what_are_the_pros_an_cons_of_variational/,EggsAndGold,1427226128,In what problems I should prefer Variational version?,30,9,False,self,,,,,
359,MachineLearning,t5_2r3gv,2015-3-25,2015,3,25,5,30688g,work.thaslwanter.at,Introduction to Statistics using Python,https://www.reddit.com/r/MachineLearning/comments/30688g/introduction_to_statistics_using_python/,kunjaan,1427229107,,2,80,False,http://b.thumbs.redditmedia.com/KTnz9EklHARynEtcgL20FYsfrFKx0vv9qewfD46tBAE.jpg,,,,,
360,MachineLearning,t5_2r3gv,2015-3-25,2015,3,25,7,306oez,self.MachineLearning,Ask ML: Deep Learning - Where to start? What to implement? RNN's? RBM?,https://www.reddit.com/r/MachineLearning/comments/306oez/ask_ml_deep_learning_where_to_start_what_to/,lmf4o,1427236137,"Hi Reddit,

I have a short question. For my upcoming graduate studies I want to already start with some small implementations of certain basic-building blocks for Deep Learning. Like RNN's and Restricted Boltzman Machines. Can someone point me to the most important papers regarding these two? 
I think implementing them by myself would greatly help me in understanding them.

Thanks
lmf4o",17,6,False,self,,,,,
361,MachineLearning,t5_2r3gv,2015-3-25,2015,3,25,7,306qeu,self.MachineLearning,"Code to reproduce paper ""On the difficulty of training Recurrent Neural Networks""",https://www.reddit.com/r/MachineLearning/comments/306qeu/code_to_reproduce_paper_on_the_difficulty_of/,[deleted],1427237023,"link: http://arxiv.org/abs/1211.5063

I am looking for a topic to my undergraduate thesis and I am reading existing papers to get some ideas, testing how well some things work, but even though the ideas in this paper seem really helpful I can't find some actual code for the regularization term proposal ( equation (9) in the paper) from LISA lab on this. pylearn2 has some code for gradient clipping and maybe I am missing something obvious in the repository...

I try to use pure python+Theano without pylearn and I am really stumped by the way the regularization term  is supposed to be added to the loss and if theano's symbolic differentiation will work for bptt (which I am pretty sure won't and can't figure out how it can be done)",0,1,False,default,,,,,
362,MachineLearning,t5_2r3gv,2015-3-25,2015,3,25,10,307exe,self.MachineLearning,Bayesian Networks and ML. Not sure how they work together.,https://www.reddit.com/r/MachineLearning/comments/307exe/bayesian_networks_and_ml_not_sure_how_they_work/,daithibowzy,1427248379,"Can someone explain how I'd use say a Random Forest with a Bayesian Network? I'm just starting to learn about BN's now. I basically have a system that has very little data, but has loads of expert knowledge that can be applied to it. I'm just not sure how to model it or use the ML with it. Any tutorials, youtube links or something like that would be helpful. Just something for me to get the concept. I'm looking to apply it to audio analysis.",1,2,False,self,,,,,
363,MachineLearning,t5_2r3gv,2015-3-25,2015,3,25,11,307i2i,self.MachineLearning,[Question] EER in Weka 3.6,https://www.reddit.com/r/MachineLearning/comments/307i2i/question_eer_in_weka_36/,YourTechGuy,1427249836,"I'm looking to calculate the EER of a classifier in Weka 3.6. I know there's a package for 3.7, but so far I've only found Windows installers for that version. 

What is the common way of doing it? I know I can get an ROC curve from Weka, but I don't know of an established way to calculate EER (i.e. I'm not sure if there's an established ""threshold"" or way of calculating a threshold for acceptable difference between FMR and FNMR). Seeing as this is a fairly common metric, I imagine there must be some way to get this from Weka.

Ninja Edit: I'm working with Weka through Java (i.e. not the GUI) but a solution using either one would be fine.",2,0,False,self,,,,,
364,MachineLearning,t5_2r3gv,2015-3-25,2015,3,25,13,307vwr,self.MachineLearning,Text data classification,https://www.reddit.com/r/MachineLearning/comments/307vwr/text_data_classification/,[deleted],1427257272,"I am currently working on a project where I want to classify text data (tweets from a sports game) to separate fans from rival teams. I have tried to classify them based on the sentiments by knowing what has actually happened during the game. Could you suggest a more 'robust' method of classification where I need not know the events that have occurred during the game. 
",4,2,False,self,,,,,
365,MachineLearning,t5_2r3gv,2015-3-25,2015,3,25,13,307win,self.MachineLearning,"Does momentum make sense only for SGD or batch-GD, and not for GD?",https://www.reddit.com/r/MachineLearning/comments/307win/does_momentum_make_sense_only_for_sgd_or_batchgd/,BeijingChina,1427257662,"Hi,
The primary function of momentum is to combat against the noisy gradient due to us looking at only a part of the dataset in SGD or batch-GD. This would mean that it would be unnecessary for vanilla GD, where we would be working with all the data, and hence the true gradient. Is there any kink in this reasoning, or any reason to theorise it may be useful for GD as well?",6,1,False,self,,,,,
366,MachineLearning,t5_2r3gv,2015-3-25,2015,3,25,18,308iyx,self.MachineLearning,Blogs about predictive analytics,https://www.reddit.com/r/MachineLearning/comments/308iyx/blogs_about_predictive_analytics/,Sergiointelnics,1427277039,can someone please tell me similar blogs as predictiveanalyticstoday?,2,3,False,self,,,,,
367,MachineLearning,t5_2r3gv,2015-3-25,2015,3,25,19,308mae,ted.com,Fei-Fei Li: How we're teaching computers to understand pictures | Talk Video,https://www.reddit.com/r/MachineLearning/comments/308mae/feifei_li_how_were_teaching_computers_to/,[deleted],1427280121,,0,0,False,default,,,,,
368,MachineLearning,t5_2r3gv,2015-3-25,2015,3,25,21,308w9k,storify.com,On April 1st a Livestream event is taking place on how #watson artificial intelligence will impact you,https://www.reddit.com/r/MachineLearning/comments/308w9k/on_april_1st_a_livestream_event_is_taking_place/,j0nnymac,1427287870,,1,0,False,http://b.thumbs.redditmedia.com/ZbF7vkOqj2J9M86iauxPYn3lR9Zw4gdoi5C2hXNXA0M.jpg,,,,,
369,MachineLearning,t5_2r3gv,2015-3-25,2015,3,25,23,3098kq,theprojectspot.com,"A solution to the TSP with Google Maps, using genetic algorithms, written in just JavaScript and HTML",https://www.reddit.com/r/MachineLearning/comments/3098kq/a_solution_to_the_tsp_with_google_maps_using/,[deleted],1427294563,,0,0,False,default,,,,,
370,MachineLearning,t5_2r3gv,2015-3-26,2015,3,26,2,309t1z,deepmind.com,Google DeepMind publications all in one place,https://www.reddit.com/r/MachineLearning/comments/309t1z/google_deepmind_publications_all_in_one_place/,egrefen,1427303836,,5,91,False,default,,,,,
371,MachineLearning,t5_2r3gv,2015-3-26,2015,3,26,3,30a4j0,github.com,Libdeep: A deep learning library for C/C++/Python,https://www.reddit.com/r/MachineLearning/comments/30a4j0/libdeep_a_deep_learning_library_for_ccpython/,improbabble,1427308693,,5,4,False,http://b.thumbs.redditmedia.com/mT16-Pmwl9M_-LLEgmCSvVkEHavbzUXdSoIS_GmBXYc.jpg,,,,,
372,MachineLearning,t5_2r3gv,2015-3-26,2015,3,26,5,30akxf,self.MachineLearning,"How do I get into AI, machine learning, planning, etc.?",https://www.reddit.com/r/MachineLearning/comments/30akxf/how_do_i_get_into_ai_machine_learning_planning_etc/,KthProg,1427315835,"I've found that application development in general is not a challenging enough environment. I really would rather be in a more academic and less ""commercial"" environment, working on ideas that are new, not programming languages that are new (i.e. the next big thing).

How do you get into this kind of work? There seems to be an abundance of application development jobs, and almost none of the more challenging positions that I'm interested in. How should I start?

So far I've changed to a Math major (2 years to degree), and written genetic algorithm solutions to TSL problems related to logistics at my job. Everything else has been application development.

(Originally on SO, user recommended I post here)",3,1,False,self,,,,,
373,MachineLearning,t5_2r3gv,2015-3-26,2015,3,26,6,30awig,self.MachineLearning,"Data sets far too large for current hardware, need some recommendations.",https://www.reddit.com/r/MachineLearning/comments/30awig/data_sets_far_too_large_for_current_hardware_need/,FerretDude,1427320786,"A while ago someone here (One of the more prominent users) had mentioned that Theano only has single GPU support; however, it appears recently that thats changed. How is the scaling diminishing returns? Can I easily use 4 Titan Xs?

Here is my build information: http://www.reddit.com/r/buildapc/comments/30auay/build_ready_making_a_deep_learning_and_multiagent/

(Note the only reason the CPU is anything more than an i7 4 core processor is that I need it for the multiagent stuff.)",14,3,False,self,,,,,
374,MachineLearning,t5_2r3gv,2015-3-26,2015,3,26,7,30azbo,cireneikual.wordpress.com,My Attempt at Outperforming DeepMind's Atari Results - UPDATE 13,https://www.reddit.com/r/MachineLearning/comments/30azbo/my_attempt_at_outperforming_deepminds_atari/,CireNeikual,1427322001,,28,42,False,http://b.thumbs.redditmedia.com/sOcXRJ2v16Po-S7bVRGtk2Ff0dxvcCVlYnbDZHSqNaE.jpg,,,,,
375,MachineLearning,t5_2r3gv,2015-3-26,2015,3,26,7,30b43k,journals.plos.org,Knowm is a nascent company utilizing a new computational primitive (kt-RAM) to eliminate the Von Neumann bottleneck from cortical learning software. Here's their research paper. Also check out /r/knowm. What are your thoughts?,https://www.reddit.com/r/MachineLearning/comments/30b43k/knowm_is_a_nascent_company_utilizing_a_new/,BittyTang,1427324114,,7,1,False,default,,,,,
376,MachineLearning,t5_2r3gv,2015-3-26,2015,3,26,9,30beet,arxiv.org,Probabilistic Binary-Mask Cocktail-Party Source Separation in a Convolutional Deep Neural Network,https://www.reddit.com/r/MachineLearning/comments/30beet/probabilistic_binarymask_cocktailparty_source/,sixwings,1427328729,,1,2,False,default,,,,,
377,MachineLearning,t5_2r3gv,2015-3-26,2015,3,26,12,30c0k3,yuviral.com,20 Life Hacks Every Parent Needs to Know!,https://www.reddit.com/r/MachineLearning/comments/30c0k3/20_life_hacks_every_parent_needs_to_know/,Joseph_Bennett993,1427339759,,0,1,False,default,,,,,
378,MachineLearning,t5_2r3gv,2015-3-26,2015,3,26,12,30c327,self.MachineLearning,Gay-Nearest Neighbors,https://www.reddit.com/r/MachineLearning/comments/30c327/gaynearest_neighbors/,alexmlamb,1427341120,"If anyone starts a professional organization for gay machine learning researchers, this should be its name.  

That is all.  ",0,0,False,self,,,,,
379,MachineLearning,t5_2r3gv,2015-3-26,2015,3,26,14,30cbnn,self.MachineLearning,what's the value of VapnikChervonenkis dimension in machine learning?,https://www.reddit.com/r/MachineLearning/comments/30cbnn/whats_the_value_of_vapnikchervonenkis_dimension/,shmagom,1427346391,,6,3,False,self,,,,,
380,MachineLearning,t5_2r3gv,2015-3-26,2015,3,26,15,30cjt8,cs231n.stanford.edu,Stanford CNN course project reports,https://www.reddit.com/r/MachineLearning/comments/30cjt8/stanford_cnn_course_project_reports/,iori42,1427352602,,4,13,False,http://b.thumbs.redditmedia.com/s3rVXBZ-_KAB3bEuY-Vvsio7b70IR2gq66-9-vVTm4s.jpg,,,,,
381,MachineLearning,t5_2r3gv,2015-3-26,2015,3,26,19,30cwud,ki-blog.de,Was sind Fembots und warum gibt es sie?,https://www.reddit.com/r/MachineLearning/comments/30cwud/was_sind_fembots_und_warum_gibt_es_sie/,flezzfx,1427365147,,0,0,False,default,,,,,
382,MachineLearning,t5_2r3gv,2015-3-26,2015,3,26,21,30d7nv,self.MachineLearning,"New to machine learning, need general direction",https://www.reddit.com/r/MachineLearning/comments/30d7nv/new_to_machine_learning_need_general_direction/,jeriho,1427373811,"I am new to machine learning and I would like to learn a bit about it. 

In particular, I am interested in application of machine learning to control technology. E.g. let's say I have a error E and a control variable U (both scalar). Usually I would use a simple PID controller, but let say I don't know much about the underlying system or the system characteristics can change over time.
 
I guess my problem would fall under unsupervised learning (since I don't know much about the system). My initial idea is to let some kind of learning algorithm fine tune to PID controller. What would be the general approach to this?

Later I would like to control more complex systems, like a model helicopter.

EDIT: I am quit familiar with control technology. What I am looking for is, if one can apply machine learning to tune a controller.",4,3,False,self,,,,,
383,MachineLearning,t5_2r3gv,2015-3-26,2015,3,26,21,30d7pw,self.MachineLearning,Decorrelation for (long-term) dimensionality reduction possible?,https://www.reddit.com/r/MachineLearning/comments/30d7pw/decorrelation_for_longterm_dimensionality/,[deleted],1427373845,"I read a lot about decorrelation of data by dimensionality reduction but can this be done the other way around?

I am asking because I read about the Mahalanobis transformation which seems to be a common decorrelation tool in statistics and it reminded of PCA. Additionally, I am currently reading into the topic of place cells and grid cells which seem to function in the brain for spatial and mental navigation (i.e. memory). The idea is that input data is decorrelated and only disjoint sets are registered. I can imagine for this principle to apply also for learning within machines (in a different manner).

What do you think? I would be happy to receive some input for further ""speculations"". :)",0,0,False,default,,,,,
384,MachineLearning,t5_2r3gv,2015-3-26,2015,3,26,23,30dizx,blog.datacamp.com,Beginner intro to machine learning in R,https://www.reddit.com/r/MachineLearning/comments/30dizx/beginner_intro_to_machine_learning_in_r/,martijnT,1427380319,,1,51,False,http://b.thumbs.redditmedia.com/CJIz0FILXuxCMcQ5mKMSK8h78DMxerNG4OJCTuhdCzc.jpg,,,,,
385,MachineLearning,t5_2r3gv,2015-3-26,2015,3,26,23,30dlon,blog.monkeylearn.com,How to create a custom text classifier with MonkeyLearn,https://www.reddit.com/r/MachineLearning/comments/30dlon/how_to_create_a_custom_text_classifier_with/,wildcodegowrong,1427381642,,0,2,False,default,,,,,
386,MachineLearning,t5_2r3gv,2015-3-27,2015,3,27,0,30dnz2,thetalkingmachines.com,The Automatic Statistician,https://www.reddit.com/r/MachineLearning/comments/30dnz2/the_automatic_statistician/,vkhuc,1427382745,,1,14,False,http://b.thumbs.redditmedia.com/Bf48OF_9xBhrpE_TAleNaUaYage-tdtm5eViIjbDI4Q.jpg,,,,,
387,MachineLearning,t5_2r3gv,2015-3-27,2015,3,27,1,30e1pr,blogs.technet.com,Video - UK food delivery service company predicts customers orders - even before they shop,https://www.reddit.com/r/MachineLearning/comments/30e1pr/video_uk_food_delivery_service_company_predicts/,MLBlogTeam,1427389020,,0,1,False,default,,,,,
388,MachineLearning,t5_2r3gv,2015-3-27,2015,3,27,2,30e2jn,radar.oreilly.com,Redefining power distribution using big data,https://www.reddit.com/r/MachineLearning/comments/30e2jn/redefining_power_distribution_using_big_data/,gradientflow,1427389363,,0,1,False,http://b.thumbs.redditmedia.com/JikmiMOkEfOdIPCj3j5IO0OcNh2sAQXsm3XiOCJMzng.jpg,,,,,
389,MachineLearning,t5_2r3gv,2015-3-27,2015,3,27,2,30e5ua,efavdb.com,Forecasting Bike Sharing Demand,https://www.reddit.com/r/MachineLearning/comments/30e5ua/forecasting_bike_sharing_demand/,efavdb,1427390782,,0,4,False,http://b.thumbs.redditmedia.com/Pz2jtcC9O01yhhQeEwNdlfgZwhnD6iT-d2rQEQODVEk.jpg,,,,,
390,MachineLearning,t5_2r3gv,2015-3-27,2015,3,27,3,30efu1,self.MachineLearning,Learning Magic The Gathering,https://www.reddit.com/r/MachineLearning/comments/30efu1/learning_magic_the_gathering/,mlkrime,1427394907,"With the recent advances in deep reinforcement learning, I know there has been momentum in the direction of playing strategic games other than chess (such as go, atari ...etc). What are the challenges in creating a machine that learns to play Magic. For those unfamiliar, magic is multimodal in the sense that the learner would first have to guess how the text of each card pertains to the current boardstate, and from there estimate the value of the boardstate for each player. ",11,6,False,self,,,,,
391,MachineLearning,t5_2r3gv,2015-3-27,2015,3,27,3,30eges,self.MachineLearning,Batch normalization or other tricks for LSTMs?,https://www.reddit.com/r/MachineLearning/comments/30eges/batch_normalization_or_other_tricks_for_lstms/,spurious_recollectio,1427395160,"Has anyone tried using batch normalization to train an LSTM?  I'm trying to speed up training of a large LSTM and am a bit stumped for ideas.  Batch normalization (between timesteps) seems a bit strange to apply in this context because the idea is to normalize the inputs to each layer while in an RNN/LSTM its the same layer being used over and over again so the BN would be the same over all ""unrolled"" layers.

Alternately are there any training strategies people have successfully used with LSTMs?  I'm training a large net with ~7M parameters (two hidden layers of ~400 units each and a large input/output layer) and its taking a long time to improve using just SGD.

People posted several tricks on a [previous thread](http://www.reddit.com/r/MachineLearning/comments/2y5dma/batch_normalization_and_prelu_successes/) such as batchwise dropout, prelu and batch normalization but it doesn't seem like any will directly speed up my problem (except maybe batch normalization which is why I'm asking).",37,4,False,self,,,,,
392,MachineLearning,t5_2r3gv,2015-3-27,2015,3,27,3,30ej2j,self.MachineLearning,[Discussion] Hedge funds are starting Artificial Intelligence departments,https://www.reddit.com/r/MachineLearning/comments/30ej2j/discussion_hedge_funds_are_starting_artificial/,steinidna,1427396312,"Here is one sample: 
http://www.bloomberg.com/news/articles/2015-02-27/bridgewater-is-said-to-start-artificial-intelligence-team

This may be old news for some of you. When I saw this I started thinking about ways to reinforce some statistical arbitrage strategies using historical data and thought this might be a fun point to discuss. 

Another method might be to use some latent feature model to make better classifications when trying to diversify.

How would you tackle a problem like this? It would be fun to discuss these ideas.
",44,25,False,self,,,,,
393,MachineLearning,t5_2r3gv,2015-3-27,2015,3,27,3,30ej5z,self.MachineLearning,training CNNs with labeled-but-not-that-labeled data?,https://www.reddit.com/r/MachineLearning/comments/30ej5z/training_cnns_with_labeledbutnotthatlabeled_data/,j-m-h,1427396354,"supervised CNN training: set of images neatly classified into categories, train on labels

what if: instead of having neatly classified labels, generate data as follows:
1. randomly choose a category, e.g. cat (with replacement)
2. randomly choose a group of (e.g. 100) images (with replacement), label each as 0 or 1 depending on category (so, 1 if it's a picture of a cat)
3. resulting data:  a set of (0/1-labeled 100-image group)s

key complication: don't know what the category used to label each group.

probably have to iterate clustering/feature extraction.

Closest thing I've seen in the literature is training CNNs to be robust to mislabeling.  I haven't seen anything on this problem, however. Have any of you?",6,2,False,self,,,,,
394,MachineLearning,t5_2r3gv,2015-3-27,2015,3,27,4,30enmc,self.MachineLearning,Does anyone else find it fishy that Delphi has no shots in the passenger cabin demonstrating the car driving itself?,https://www.reddit.com/r/MachineLearning/comments/30enmc/does_anyone_else_find_it_fishy_that_delphi_has_no/,Atrix621,1427398197,"Seriously, did the car drive itself into the gas station in El Paso?  If not, at what point do they switch from driver to computer?",0,1,False,self,,,,,
395,MachineLearning,t5_2r3gv,2015-3-27,2015,3,27,8,30fho0,self.MachineLearning,How can I model this?,https://www.reddit.com/r/MachineLearning/comments/30fho0/how_can_i_model_this/,[deleted],1427410962,"I have a graph of ""events"", where some events mean others connected to it are more likely to occur. I have an event stream coming in that I can influence the behavior of indirectly (like a user's click stream). I.e. I can sort of make sure that some events happen more often, but can't guarantee it. How can I create a model that optimizes a certain set of events occur more frequently? It would have to be some sort of online machine learning model as the data is only known sequentially.",1,0,False,default,,,,,
396,MachineLearning,t5_2r3gv,2015-3-27,2015,3,27,9,30for5,self.MachineLearning,Has anyone replicated AlexNet in Theano?,https://www.reddit.com/r/MachineLearning/comments/30for5/has_anyone_replicated_alexnet_in_theano/,wolet,1427414434,"Hello all,

I wonder if anyone has replicated AlexNet results in Theano. Last semester in CMU we took [a Deep Learning class](http://deeplearning.cs.cmu.edu/) and for one of the homeworks we compared MLPs and CNNs. None of us was able to replicate AlexNet results in theano. Any suggestions?",9,6,False,self,,,,,
397,MachineLearning,t5_2r3gv,2015-3-27,2015,3,27,9,30fp92,youtube.com,"""Structural Analysis and Visualization of Networks""- video course at HSE about social network analysis.",https://www.reddit.com/r/MachineLearning/comments/30fp92/structural_analysis_and_visualization_of_networks/,leonidX,1427414662,,2,14,False,http://b.thumbs.redditmedia.com/ppI5bWeL3Xj1C4O-oqukTNk0rEazkY0Eu4nLJz3-EHE.jpg,,,,,
398,MachineLearning,t5_2r3gv,2015-3-27,2015,3,27,9,30fv3v,self.MachineLearning,Can anyone with experience using K-means clustering please help point me in the right direction?,https://www.reddit.com/r/MachineLearning/comments/30fv3v/can_anyone_with_experience_using_kmeans/,TPB-,1427417481,"I'm working on an existing code base that is designed to extract the color of persons facial features (eye color, skin color, etc).

This is written using Python and the SciPy cluster module, and I need to figure out how to implement it in C++ (with OpenCV).

The methods used are:

1. scipy.cluster.vq.kmeans2

2. scipy.cluster.vq.vq

3. scipy.histogram

I tried reading the SciPy source but it goes into cython which has kinda weird syntax that was confusing to look at.

I'm wondering for one if this kmeans2 and  histogram function are the same as the kmeans and hist methods available in OpenCV?

And/or more importantly, are there any good resources (write-ups  or academic journals) that would clearly show how to implement this from scratch in C++?

Sorry if this question isn't specific enough, I try to provide any additional info you need if you let me know.",4,1,False,self,,,,,
399,MachineLearning,t5_2r3gv,2015-3-27,2015,3,27,10,30g0td,self.MachineLearning,Looking for ML-flavored propagation-styled graph algorithms,https://www.reddit.com/r/MachineLearning/comments/30g0td/looking_for_mlflavored_propagationstyled_graph/,osazuwa,1427420381,"I wrote this graph algorithm that does the following recursive operation.

* Perform a calculation on the value stored in a vertex/edge, label the vertex/edge as ""updated"" when finished.
* The calculation depends on the values of certain other vertices/edges.  If any of those are not already updated, stop and update them.

I used this to fit a Bayesian network using a special form of back-propagation (updating edge weights), as well as prediction (updating node values).

To get the most out of my algo, I'd like to know of other machine-learning flavored graph algorithms that use a ""update this thing if other things it depends on are already updated"" approach.  The only thing I can think of is belief-propagation.  

Hope I'm making sense. :)",7,4,False,self,,,,,
400,MachineLearning,t5_2r3gv,2015-3-27,2015,3,27,11,30g9xb,self.MachineLearning,Feature selection w/ SVM RBF classifier,https://www.reddit.com/r/MachineLearning/comments/30g9xb/feature_selection_w_svm_rbf_classifier/,mcjoness,1427425104,"Hey everyone,

I've been looking at performing forward selection using an SVM w/ RBF kernel. Currently, I'm finding that a reduced subset of the original features results in better classification performance. Is this most aptly attributed to the *curse of dimensionality*? I'm attempting to explain why I'm finding this is the case to a co-worker and want to make sure I'm outlining all the typical causes of this. I realize this is a bit vague because many answers are specific to application, but I appreciate any advice!",3,4,False,self,,,,,
401,MachineLearning,t5_2r3gv,2015-3-27,2015,3,27,14,30gq6v,self.MachineLearning,Help need with Text Mining+Graph Theory idea,https://www.reddit.com/r/MachineLearning/comments/30gq6v/help_need_with_text_mininggraph_theory_idea/,sachinrjoglekar,1427435403,"Hello folks!
In my idle time, I have been working on a project I named MapGeist, a Python-based library to generate Mind-Maps given a body of text (and no other information). I have always loved the elegance and effectiveness of mind-maps in learning and brainstorming, and this is my attempt at building an intelligent system to develop them in an automated way. When I started out, these were my ideas(you may say a road-map) for the project-

1. To be able to generate a graphical representation of the information in a text document (Basically a mind-map) - irrespective of language/content.
2. Test it out on various sources of knowledge such as Wikipedia, Twitter, etc.
3. Being able to use it as a knowledge exploration/brainstorming tool. How? Something like this-
a. Take a starting phrase(the initial idea) and search it up on say, Twitter.
b. Extract the top N tweets on the search item from Twitter, and construct a text document from it.
c. Generate a Mind-Map from the document, with a few, 'most-important' nodes.
d. Let the user look at the Map, and decide the direction in which he wants to continue the 'exploration'. He does this by clicking on a node. Now repeat from Step a.

I may be thinking pretty idealistically, assuming that 'good-quality' Mind-Maps can be generated by a computer, that too without any other source of knowledge. In any case, the code is here: https://github.com/sachinrjoglekar/MapGeist, and if you are interested in the inner mechanisms, its all written here: https://github.com/sachinrjoglekar/MapGeist/blob/master/docs/methodology.pdf (I basically use word co-occurence similarities to build a distance matrix, and then use an MST algorithm on it).


Here are a few things I want to ask your opinion on (especially the Text Mining/ Graph Theory/ Clustering experts here)-

1. The tool seems to work well on documents that have good knowledge content. Here are the Mind-Maps I generated on Machine Learning-
a. http://imgur.com/GhQo6Hc - From the Wikipedia article on Machine Learning
b. http://imgur.com/VXmVF8W - From the top 1000 tweets about Machine Learning as on 25/3/2015 (Some noisy stuff here).
However, on say something like the Wikipedia article on Breaking Bad, things start going awry. Heres the Mind-Map: http://imgur.com/2wSKTmr. Is it only because the topic in question doesnt have properly defined sub-domains like 'Machine Learning' does, or maybe I could improve things in some way (You might want to read the document on the mechanisms behind MapGeist).
2. Any ideas on how a knowledge source like say DMOZ will help? However, this would mean the tool would be language-specific. I had also thought of using the page-links data that can be fetched from Wikipedia - I would group all keywords extracted from the document and categorize them (using occurences in the document) into one of the links, and then build a Map from the links themselves.
3. Any better way to perform the two main tasks involved-
a. Build a distance matrix involving all the key terms/phrases in the document.
b. Generating a Mind-Map using the above distance matrix.

Any other ideas/feedback are also welcome! I have been doing trial-and-error for a long time now on the code, so I want a fresh set of ideas/opinions to move forward. If possible, feel free to fork the code and send a PR - nothing better than an additional set of hands!


(Sorry about the really long post)
Cheers!",9,5,False,self,,,,,
402,MachineLearning,t5_2r3gv,2015-3-27,2015,3,27,15,30gv2f,analyticsvidhya.com,"Big Data / Analytics based startups at Y Combinator, Winter 2015 batch",https://www.reddit.com/r/MachineLearning/comments/30gv2f/big_data_analytics_based_startups_at_y_combinator/,kunalj101,1427439340,,0,4,False,http://b.thumbs.redditmedia.com/HV0KnpsgrVgCcglMamhpJgTZRCDi-ZEXZH75iCm1wBw.jpg,,,,,
403,MachineLearning,t5_2r3gv,2015-3-27,2015,3,27,18,30h4sh,markhneedham.com,Topic Modelling: Working out the optimal number of topics at Mark Needham,https://www.reddit.com/r/MachineLearning/comments/30h4sh/topic_modelling_working_out_the_optimal_number_of/,cast42,1427449205,,0,6,False,http://b.thumbs.redditmedia.com/PUytNtkoZsKvSxkOmBkpLredUVUmZ4_YZYR9GQ-kD3w.jpg,,,,,
404,MachineLearning,t5_2r3gv,2015-3-27,2015,3,27,20,30hbvf,datanami.com,Achieve Business Value with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/30hbvf/achieve_business_value_with_machine_learning/,PetronilaGroshek,1427455898,,1,0,False,http://b.thumbs.redditmedia.com/KusoEscl4yF9hsNeJYeuwkvDMXQkD_pME7AqqsPW-IU.jpg,,,,,
405,MachineLearning,t5_2r3gv,2015-3-27,2015,3,27,23,30hwk5,self.MachineLearning,Idea: using word2vec for dark knowledge,https://www.reddit.com/r/MachineLearning/comments/30hwk5/idea_using_word2vec_for_dark_knowledge/,spurious_recollectio,1427468374,"I've been trying to train an LSTM on a sentences (similar to seq2seq stuff using in NMT) and have found that it takes a very long time to train because the input and output dictionaries tend to be very large and word frequencies are very uneven.  The ultimate learning task is something I'm still building up to but I plan to use a sequence-to-sequence on sentences.

An idea I've had (but not yet implemented) to help learning is to try to use word2vec as a source of ""dark nowledge"" for my network.  This can take many forms but for instance instead of only inputing 1-hot-encoded dictionary entries I can supplement them with the relevant word2vec vector.  Also for the output targets I was thinking that rather than using 1-hot-encoded vectors I could compute a probability distribution over the entire dictionary by using the cosine distance (in word2vec) from the expected word.  This would generate ""soft"" targets similar to the dark-knowledge paper.  I think this would also help with less frequent words because they would also be trained a little whenever a ""nearby"" word showed up.  The idea is that if one replaced words in a sentence with their word2vec neighbors the sentence might still be comprehensible if inaccurate.

Another idea is to switch from classification to regression and simply aim to output the word2vec of a given word (this would vastly reduce the input/output dimensionality).  One could then ""beam search"" output sentences (when doing sequence-to-sequence in a generative way) using word2vec proximity and then use some other technique to rank the outputs.

As I said I haven't implemented any of this yet but so I'd be very happy get some feedback on the idea.",5,6,False,self,,,,,
406,MachineLearning,t5_2r3gv,2015-3-28,2015,3,28,0,30i2p8,new.livestream.com,"Watch |Live NOW| MLconf talks from Google Research, Facebook, OpenTable, IBM Watson, Intel Labs, Yahoo Labs &amp; more.",https://www.reddit.com/r/MachineLearning/comments/30i2p8/watch_live_now_mlconf_talks_from_google_research/,shonburton,1427471292,,4,42,False,http://b.thumbs.redditmedia.com/U6a8Xe8Ba14TJ-L0LLu4o5ekmx5rOZibpV9bf90jjQc.jpg,,,,,
407,MachineLearning,t5_2r3gv,2015-3-28,2015,3,28,1,30i4dg,blog.mikiobraun.de,Three main insights [About Data Science] you wont easily find in books.,https://www.reddit.com/r/MachineLearning/comments/30i4dg/three_main_insights_about_data_science_you_wont/,kunjaan,1427472057,,21,37,False,http://b.thumbs.redditmedia.com/kqlhm_VzZkZozBxh-chf50MfL-GuBQogYg-EMHmnqxc.jpg,,,,,
408,MachineLearning,t5_2r3gv,2015-3-28,2015,3,28,2,30ii1m,forbes.com,NVIDIA GTC: The Race To Perfect Voice Recognition Using GPUs,https://www.reddit.com/r/MachineLearning/comments/30ii1m/nvidia_gtc_the_race_to_perfect_voice_recognition/,sixwings,1427478116,,16,3,False,http://b.thumbs.redditmedia.com/WGN0GcPKO_QG3cykvaYEt5eQUJOH9mOdW90SFy4RSkk.jpg,,,,,
409,MachineLearning,t5_2r3gv,2015-3-28,2015,3,28,3,30isct,self.MachineLearning,Consistency Guarantees for Variational Autoencoders,https://www.reddit.com/r/MachineLearning/comments/30isct/consistency_guarantees_for_variational/,alexmlamb,1427482693,"Hello, 

I am curious about the statistical consistency properties for variational autoencoders.  The variational autoencoder for a random variable y, produces samples from p(y).  Without getting too bogged down with technical details, I'm interested in determining if the mean and the variance of the model samples p(y) approach the mean and the variance of the true distribution as the number of samples used to train the variational autoencoder goes to infinity.  I am also okay with assuming that the dimensionality of the latent variable z can go to infinity as well (with the usual prior distribution of a multivariate gaussian with diagonal covariance).  

Has anyone looked at this problem?  

Best, 

Alex.  ",15,2,False,self,,,,,
410,MachineLearning,t5_2r3gv,2015-3-28,2015,3,28,5,30j2cc,partiallyderivative.com,"Partially Derivative Episode 18: You Know Nothing, Jon Snow",https://www.reddit.com/r/MachineLearning/comments/30j2cc/partially_derivative_episode_18_you_know_nothing/,consistentlyanalysis,1427487329,,0,8,False,http://a.thumbs.redditmedia.com/rfWYrGkTZ6dXaIUAmqggbSHdRaXEiOoWeJETxTzWZV0.jpg,,,,,
411,MachineLearning,t5_2r3gv,2015-3-28,2015,3,28,5,30j4w1,self.MachineLearning,How often do you do model selection?,https://www.reddit.com/r/MachineLearning/comments/30j4w1/how_often_do_you_do_model_selection/,generating_loop,1427488613,"A lot of the articles and blog posts I read about people who do machine learning/data science in industry paints the following picture: get data, experiment with different models/algorithms/features, validate and find what works best, implement (or give to a dev to implement) into a production system.

What they never mention is how often they look at the problem after an extra week or month or year of new data and revisit whether the model and parameters they selected still perform the best in cross-validation.

The reason I ask is because, at my job, we have thousands of very volatile time series we need to forecast far into the future. Since the data changes so much from week to week, if we pick a model for each time series and use it for as short as a month, we find it often behaves very poorly. This is even true if we allow the parameters of the model to vary - sometimes the best thing to do is ditch that type of model altogether and use something else (i.e. linear instead of quadratic, apply different transformations, etc...).

Effectively, what this means is lots of computational and operational overhead - each week we need to rerun the entire forecasting and model selection process (hundreds of models each, times thousands of time series), and cross-validate each model for each time series.

I don't think we're doing any *wrong*, but I definitely feel like we're doing too much work. How often to you revisit your choice of model at your job? Do you have any automated processes in place to determine if the model you are using is performing poorly compared to some other choice?",10,6,False,self,,,,,
412,MachineLearning,t5_2r3gv,2015-3-28,2015,3,28,6,30j82i,aisecurity.org,Chapter 1 &amp; 2 of AI Security now online. Relevant to ML due to safety and security considerations at design level of algorithms.,https://www.reddit.com/r/MachineLearning/comments/30j82i/chapter_1_2_of_ai_security_now_online_relevant_to/,[deleted],1427490111,,0,9,False,default,,,,,
413,MachineLearning,t5_2r3gv,2015-3-28,2015,3,28,9,30jv85,self.MachineLearning,Multi-scale shape matching,https://www.reddit.com/r/MachineLearning/comments/30jv85/multiscale_shape_matching/,greatm31,1427501756,"Say you are determining how well your model of an object matches an image of it. 

We can use a scoring function to evaluate model quality- e.g, the cross-correlation coefficient (CCC), giving an overall shape match. This works for simple shapes, but there is a problem for complex objects (for example a biological object like an animal, an organ, or even a molecule) that has critically important but relatively *small features*. A global metric like the CCC is size-weighted: features of smaller sizes contribute less to the score! Yet these features are *more* important than the overall shape match. Are there alternative scoring functions that are multi-scale?

**Example**: matching a human body model to an image. It's quite easy to match the overall shape with a model. What we really care about are smaller-scale features like limb orientation, facial expressions, skin blemishes, all kinds of stuff. A global shape metric will not be affected by matching to these relatively small features.

Another way to think about this is in Fourier space. A complex object with lots of detail will have signal all the way out to high-frequency ranges. The high-frequency ranges are where the most important details exist. One idea is to calculate the CCC at different levels of frequency filtering and combine them somehow. However, the structure might not be uniformly multi-scale (for example, the image might be blurrier in some spots) so we may need to do this in real space.

Any thoughts?",0,2,False,self,,,,,
414,MachineLearning,t5_2r3gv,2015-3-28,2015,3,28,22,30lgxg,am207.org,"Harvard AM207: Monte Carlo Methods, Stochastic Optimization - videos, IPython notebooks on GitHub",https://www.reddit.com/r/MachineLearning/comments/30lgxg/harvard_am207_monte_carlo_methods_stochastic/,Foxtr0t,1427548286,,3,53,False,default,,,,,
415,MachineLearning,t5_2r3gv,2015-3-28,2015,3,28,22,30lh1d,github.com,"Keras: Theano-based deep learning lib, focused on fast prototyping. Supports RNNs and convnets.",https://www.reddit.com/r/MachineLearning/comments/30lh1d/keras_theanobased_deep_learning_lib_focused_on/,Foxtr0t,1427548374,,25,71,False,http://b.thumbs.redditmedia.com/K3jl5hz8Cn1uuWgffqvVdykUFFwxLB8rue9DQCwNAUU.jpg,,,,,
416,MachineLearning,t5_2r3gv,2015-3-28,2015,3,28,23,30ll8z,data-artisans.com,Implementation of the alternating least squares (ALS) algorithm in Apache Flink,https://www.reddit.com/r/MachineLearning/comments/30ll8z/implementation_of_the_alternating_least_squares/,kunjaan,1427551514,,0,8,False,http://b.thumbs.redditmedia.com/5L5sVVUh3D14YrY0cNamYnDnn3bneQGxxz_pLaXeViw.jpg,,,,,
417,MachineLearning,t5_2r3gv,2015-3-28,2015,3,28,23,30lny2,jackhoy.com,Summary of 'Computing Machinery and Intelligence' (1950) by Alan Turing,https://www.reddit.com/r/MachineLearning/comments/30lny2/summary_of_computing_machinery_and_intelligence/,[deleted],1427553313,,0,4,False,default,,,,,
418,MachineLearning,t5_2r3gv,2015-3-29,2015,3,29,1,30m0eb,self.MachineLearning,What is the state of the art in language modeling with neural networks?,https://www.reddit.com/r/MachineLearning/comments/30m0eb/what_is_the_state_of_the_art_in_language_modeling/,ndronen,1427560517,"I think I read somewhere that Mikolov's work on language modeling with simple RNNs result in best-in-class performance, but I'm not sure whether -- if true -- that still holds.  Has someone shown LSTMs to perform even better?",8,13,False,self,,,,,
419,MachineLearning,t5_2r3gv,2015-3-29,2015,3,29,10,30nsox,self.MachineLearning,How difficult it is to find jobs in machine learning for signal processing in the US without a PhD?,https://www.reddit.com/r/MachineLearning/comments/30nsox/how_difficult_it_is_to_find_jobs_in_machine/,gsmafra,1427594140,"I am finishing my (European) Master's in applied maths/signal processing, with undergrad in engineering and I already have plenty of internships and interesting projects in the bag, comfortable with many programming languages, international experience etc

Activity here in the tech sector is not that hot, so I thought going to the US

I want to apply maths/machine learning to vision/audio or maybe NLP, and most of the openings that I see even for interns (say that they) require PhDs (being worked on at least). Is that really necessary? I am wasting my time applying to these even if I have some nice experience?

I have really no academic aspirations, and I don't like the idea of going to grad school just to come back to the industry

So, any useful thoughts?",6,21,False,self,,,,,
420,MachineLearning,t5_2r3gv,2015-3-29,2015,3,29,13,30o4ys,self.MachineLearning,Am I finding the gradient right?,https://www.reddit.com/r/MachineLearning/comments/30o4ys/am_i_finding_the_gradient_right/,fuckinghelldad,1427601892,"I'm trying to optimise a function f(x, y) using gradient descent, but I keep running into local optima. I'm wondering if I've found the gradient correctly. Here's what I've done: http://i.imgur.com/MlZI8qH.png (I've written the E's to look like tensors, but they're not---the subscripts were just feeling crowded.)

The way the global optima arise (and part of what makes me question that I've found the gradient correctly) is that the (t_xy - f(x, y)) term in the last equation keeps going to zero. Another thing which makes me question if my gradient function is right is that coordinate descent finds very good solutions (though I'm worried about its performance).",6,2,False,self,,,,,
421,MachineLearning,t5_2r3gv,2015-3-29,2015,3,29,14,30oask,self.MachineLearning,ID 3.0 or ID 4.5,https://www.reddit.com/r/MachineLearning/comments/30oask/id_30_or_id_45/,tushar1408,1427605998,any libraries in python containing id 3.0 or 4.5 implementation?,4,1,False,self,,,,,
422,MachineLearning,t5_2r3gv,2015-3-29,2015,3,29,18,30oqaa,self.MachineLearning,constraint NN weights,https://www.reddit.com/r/MachineLearning/comments/30oqaa/constraint_nn_weights/,letitgo12345,1427621679,"Suppose I want to constrain the weights of a subset of the edge in an NN to the same or similar.

Is there some way I can architect the NN such that this will happen automatically and if not, are their preexisting libraries that would allow me to do this?",3,0,False,self,,,,,
423,MachineLearning,t5_2r3gv,2015-3-29,2015,3,29,20,30oxib,datasciencetech.institute,Data Science MSc course with scholarship opportunities,https://www.reddit.com/r/MachineLearning/comments/30oxib/data_science_msc_course_with_scholarship/,datasciencetech,1427629930,,6,0,False,http://b.thumbs.redditmedia.com/aWDaORyUmUSQONkAj2k9DdlH40ngSSB3yXm8_Fc1Aoo.jpg,,,,,
424,MachineLearning,t5_2r3gv,2015-3-29,2015,3,29,22,30p2q0,quora.com,How do you measure and test the quality of recommendation engines? [Quora],https://www.reddit.com/r/MachineLearning/comments/30p2q0/how_do_you_measure_and_test_the_quality_of/,kunjaan,1427634697,,0,9,False,http://a.thumbs.redditmedia.com/Aw1ayhD2bXJnS2JV1tMdX6GsPemwxHjz-ZP2ysMe_L0.jpg,,,,,
425,MachineLearning,t5_2r3gv,2015-3-29,2015,3,29,23,30p740,datasciencetech.institute,Data ScienceTech Institute partners with NVIDIA for Deep learning on GPU,https://www.reddit.com/r/MachineLearning/comments/30p740/data_sciencetech_institute_partners_with_nvidia/,datasciencetech,1427637876,,5,2,False,http://b.thumbs.redditmedia.com/wsb9CsCFIFAusQGNutkY1TwbP2UKeIguCg1xnhUhqCw.jpg,,,,,
426,MachineLearning,t5_2r3gv,2015-3-29,2015,3,29,23,30paf9,memkite.com,Update with 362 new (2015) Deep Learning papers to Deeplearning.University,https://www.reddit.com/r/MachineLearning/comments/30paf9/update_with_362_new_2015_deep_learning_papers_to/,atveit,1427640075,,4,22,False,http://b.thumbs.redditmedia.com/YJiLRsmLfh5Hr7HRYeU1sV7gFJAELNmBs_dRssUmoXw.jpg,,,,,
427,MachineLearning,t5_2r3gv,2015-3-30,2015,3,30,1,30pjzu,github.com,How to share data with a statistician,https://www.reddit.com/r/MachineLearning/comments/30pjzu/how_to_share_data_with_a_statistician/,kunjaan,1427645770,,0,6,False,http://b.thumbs.redditmedia.com/bPkl9VaMJGizoaAHQ2Bi8M4EGDy8xG4DUaRr40MzG3Q.jpg,,,,,
428,MachineLearning,t5_2r3gv,2015-3-30,2015,3,30,4,30q6ec,github.com,Tetris AI Environment - Build bots to play against this Tetris Sandbox,https://www.reddit.com/r/MachineLearning/comments/30q6ec/tetris_ai_environment_build_bots_to_play_against/,jehna1,1427657081,,1,16,False,http://b.thumbs.redditmedia.com/FPoY8Wcqsr1kDYlbfHwCVAbykuRZLhFZ0yA5Ha8YuvE.jpg,,,,,
429,MachineLearning,t5_2r3gv,2015-3-30,2015,3,30,4,30q9s9,self.MachineLearning,How to process MFCC Vectors to be used for Neural Network,https://www.reddit.com/r/MachineLearning/comments/30q9s9/how_to_process_mfcc_vectors_to_be_used_for_neural/,ismailzd,1427658778,"Hi guys,

I have a voice recognition project to complete, the aim is to record 0-9 and operators and perform classification (feedforward), the problem I am facing is, I am generating the MFCC vectors (13 by 100 frames) meaning each frame size contains 13 elements, the question I want to ask, how do I merge the 100 frames to make it a single vector with 13 elements. I have tried standard deviation, mean and other statistical methods (median,mode). When I do perform classification using MATLab Neural Network ToolBox it doesn't work accordingly as certain classes have similar vector values.

My supervisor for this project is not sure of my error as he claims many students from the past didn't have any issues with recognition system and the feature extraction. 


I am aware that DTW and other feature matching techniques can resolve this problem however I have to use Neural Network for the recogintion. 


Your help is much appreciated as I approach the deadline of my final year project.

Thank you in advance.  
",12,2,False,self,,,,,
430,MachineLearning,t5_2r3gv,2015-3-30,2015,3,30,5,30qd67,lifehacker.com.au,The Basic Recipe For Machine Learning Explained In A Single PowerPoint Slide,https://www.reddit.com/r/MachineLearning/comments/30qd67/the_basic_recipe_for_machine_learning_explained/,vikashkodati,1427660427,,12,7,False,http://b.thumbs.redditmedia.com/wd2cFLj3hmikz8a1XZdI9E4XBBydRO9X3wyNnABsLrE.jpg,,,,,
431,MachineLearning,t5_2r3gv,2015-3-30,2015,3,30,6,30qjp1,self.MachineLearning,What are the good techniques to project the given data into more efficient form for better classification?,https://www.reddit.com/r/MachineLearning/comments/30qjp1/what_are_the_good_techniques_to_project_the_given/,erogol,1427663668,"Suppose you are given a data-set for classification task and after a set of analysis and experiments you cannot get very far. Most probably, the most effective solution is to obtain more data but if it is not viable maybe we can talk about alternative ways to uncover some hidden features in the data to obtain the best possible.",0,1,False,default,,,,,
432,MachineLearning,t5_2r3gv,2015-3-30,2015,3,30,9,30r85n,facebook.com,Facebook's demo of Memory Networks,https://www.reddit.com/r/MachineLearning/comments/30r85n/facebooks_demo_of_memory_networks/,vkhuc,1427676420,,6,29,False,http://b.thumbs.redditmedia.com/4EAA7mMvi4dRZ7rhvJDo4owkp64BebYbQWXzqPRxo1s.jpg,,,,,
433,MachineLearning,t5_2r3gv,2015-3-30,2015,3,30,10,30reky,self.MachineLearning,How many cuda cores in a AWS G2 instance? I'm hearing different numbers.,https://www.reddit.com/r/MachineLearning/comments/30reky/how_many_cuda_cores_in_a_aws_g2_instance_im/,Registerml,1427679797,"On a g2.2xlarge instance on AWS, I have seen a post on this subreddit that claimed 3,072 cores. I asked a question to get confirmation but OP never replied. Has anyone else seen that many cores on their G2? If so, what region? I'm only seeing 1,536 on my instance.",10,5,False,self,,,,,
434,MachineLearning,t5_2r3gv,2015-3-30,2015,3,30,11,30rll6,self.MachineLearning,Stanford UFLDL Tutorial Help,https://www.reddit.com/r/MachineLearning/comments/30rll6/stanford_ufldl_tutorial_help/,DirectorChurch,1427683633,"Link: http://deeplearning.stanford.edu/tutorial/

I've been trying to complete the linear regression tutorial but I've been having trouble getting it to work. I've done a few other machine learning linear regression tutorials and had no issues. Any help would be much appreciated.",11,3,False,self,,,,,
435,MachineLearning,t5_2r3gv,2015-3-30,2015,3,30,14,30rzd1,self.MachineLearning,Hold out data performs better than test set?,https://www.reddit.com/r/MachineLearning/comments/30rzd1/hold_out_data_performs_better_than_test_set/,watersign,1427691817,"So..I have been tweaking a model for a project at work and I noticed that the hold out data I use for final validation scores higher on accuracy than the test set of the partition. Is this normal? I know that when there is a big difference in accuracy with the hold out set, it means overfitting but Ive never heard of this type of scenario. What do you guys think? 

",6,0,False,self,,,,,
436,MachineLearning,t5_2r3gv,2015-3-30,2015,3,30,14,30s0x8,self.MachineLearning,Vectorizing neural network back propagation,https://www.reddit.com/r/MachineLearning/comments/30s0x8/vectorizing_neural_network_back_propagation/,Russian-Assassin,1427692872,"I am having trouble applying vectorization to my neural network code. I am using java with the MTJ library for matrix math. For some reason, I am blanking on why the size of my matrices seems to be wrong.

Here is what I am doing:

1. Set `a0` to input values.
2. Calculate `z-n`as `Theta-n * a-n` and `a-n` as `transfer(z-n)` (I am using the sigmoid function)
3. Calculate the error in the last layer as `a-last - y`
4. Calculate error from layer `last` to layer `2` as `err-n = theta-n^T * err-(n+1) .* transfer'(z-n)` and remove index 0
5. Calculate delta as `delta-n += e-n^T * a-n`
6. ... can't get this far

I think I am getting confused with bias terms which is leading to some weird matrix sizing. I ran the code I have with a 2-3-1 neural net and got the following sizes for my matrices with a sample size of 1:

`theta1` 3x3

`theta2` 1x4

`a0` 3x1 (add bias term to index 0)

`z1` 3x1

`a1` 4x1 (add bias)

`z2` 1x1

`a2` 1x1 (output, no bias)

`err3` 1x1

`err2` 3x1

`delta1` 3x3 - same as theta1

`delta2` error (should be 1x4 - like theta2)

For `delta2`, I am calculating a 1x1 matrix times a 1x1 matrix which results in a 1x1 matrix and not a 1x4 matrix.

Can someone explain if I am using the bias term wrong or if my calculations are incorrect.",2,1,False,self,,,,,
437,MachineLearning,t5_2r3gv,2015-3-30,2015,3,30,15,30s6bn,futuretimeline.net,Neural Net-based conversational computers,https://www.reddit.com/r/MachineLearning/comments/30s6bn/neural_netbased_conversational_computers/,I_ai_AI,1427697103,,3,0,False,http://b.thumbs.redditmedia.com/rUkNo5vYmnKi52h4rlC3oRhTQsllmYPHMl7n6Y59ffc.jpg,,,,,
438,MachineLearning,t5_2r3gv,2015-3-30,2015,3,30,18,30si2t,highlyscalable.wordpress.com,Data Mining Problems in Retail,https://www.reddit.com/r/MachineLearning/comments/30si2t/data_mining_problems_in_retail/,alexeyr,1427708684,,0,19,False,http://b.thumbs.redditmedia.com/xfjallxI9caiu1WliYdjiwdk9myv08-gbQigVogxmGE.jpg,,,,,
439,MachineLearning,t5_2r3gv,2015-3-30,2015,3,30,20,30spyk,self.MachineLearning,How do I decide which error function to minimize when training a neural network?,https://www.reddit.com/r/MachineLearning/comments/30spyk/how_do_i_decide_which_error_function_to_minimize/,Mierzen,1427716123,E.g. MSE or RMSE,17,23,False,self,,,,,
440,MachineLearning,t5_2r3gv,2015-3-30,2015,3,30,22,30sy1u,blog.aylien.com,Sentiment Analysis of Online Reviews with Google Sheets,https://www.reddit.com/r/MachineLearning/comments/30sy1u/sentiment_analysis_of_online_reviews_with_google/,MikeWally,1427721865,,1,2,False,http://b.thumbs.redditmedia.com/RNa_QM8kMFMCSVlMIIpJ2n7F2BpefBuDjr5v2AQHcxQ.jpg,,,,,
441,MachineLearning,t5_2r3gv,2015-3-30,2015,3,30,23,30t48x,re-work.co,Computers That Can Learn: How Will Deep Learning Affect the Lives of Millions of People?,https://www.reddit.com/r/MachineLearning/comments/30t48x/computers_that_can_learn_how_will_deep_learning/,reworksophie,1427725284,,1,1,False,default,,,,,
442,MachineLearning,t5_2r3gv,2015-3-30,2015,3,30,23,30t7uw,scikit-learn.org,"scikit-learn 0.16.0 is out with more scalable clustering &amp; PCA, approximate NN, probability calibration and more",https://www.reddit.com/r/MachineLearning/comments/30t7uw/scikitlearn_0160_is_out_with_more_scalable/,ogrisel,1427727119,,11,76,False,http://b.thumbs.redditmedia.com/MYgYF7mu3NrRe97JVniiPCzf5jtLiw03DhrXkAf51FA.jpg,,,,,
443,MachineLearning,t5_2r3gv,2015-3-31,2015,3,31,0,30ta3m,self.MachineLearning,Q-learning where the agents are in multiple classes?,https://www.reddit.com/r/MachineLearning/comments/30ta3m/qlearning_where_the_agents_are_in_multiple_classes/,[deleted],1427728184,"I recently read through this paper: http://www.agent.ai/doc/upload/200302/sand95_6.pdf
that discusses q learning where multiple learners are present, and decisions made do not have strictly positive and negative influences.

However, for the purposes of my research each agent is in 2 or more different classes, as determined by feature extraction. For instance, an agent may be both a cook, and a parent. Each of these classes would act as a separate critic that would influence the agent's decision making.

As of right now I'm a bit unsure how to approach q learning from a multiclass perspective, as most algorithms seem to pertain to all agents having an equal amount of dimensions as well as availability of decisions.

Before anyone mentions this, I know my terminology sucks, so I can explain things if need be.",0,0,False,default,,,,,
444,MachineLearning,t5_2r3gv,2015-3-31,2015,3,31,1,30tij2,ai-maker.com,A closer look at problem state encoding in genetic algorithms through electronics (xpost /r/compsci),https://www.reddit.com/r/MachineLearning/comments/30tij2/a_closer_look_at_problem_state_encoding_in/,ai_maker,1427732077,,0,3,False,http://b.thumbs.redditmedia.com/N2qown2Q7z4JqiInYZgX_MalCqesqWBvon-WR47An8U.jpg,,,,,
445,MachineLearning,t5_2r3gv,2015-3-31,2015,3,31,1,30tl55,blogs.technet.com,Free webinar on how to building predictive models with large datasets using ML and Hadoop,https://www.reddit.com/r/MachineLearning/comments/30tl55/free_webinar_on_how_to_building_predictive_models/,[deleted],1427733287,,0,1,False,default,,,,,
446,MachineLearning,t5_2r3gv,2015-3-31,2015,3,31,1,30tnk2,blogs.technet.com,Free webinar tomorrow on how to building predictive models with large datasets using cloud ML and Hadoop,https://www.reddit.com/r/MachineLearning/comments/30tnk2/free_webinar_tomorrow_on_how_to_building/,[deleted],1427734368,,0,1,False,default,,,,,
447,MachineLearning,t5_2r3gv,2015-3-31,2015,3,31,2,30tpza,arxiv.org,Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks,https://www.reddit.com/r/MachineLearning/comments/30tpza/probabilistic_backpropagation_for_scalable/,[deleted],1427735400,,10,17,False,default,,,,,
448,MachineLearning,t5_2r3gv,2015-3-31,2015,3,31,5,30ud9i,self.MachineLearning,[IR] Organising dataset for word sense disambiguation using tf-idf,https://www.reddit.com/r/MachineLearning/comments/30ud9i/ir_organising_dataset_for_word_sense/,oOArneOo,1427745678,"I am using the senseval 3 datasets [(see here)] for WSD and have everything set up already. I have a large enough training set, for each meaning of some word maybe 30-40 examples, and can start comparing feature vectors with my test sentences.

Then I realised that there are two ways to define what a document in my training data is. Either, each labelled example individually, or the union of all examples with the same label as one big document.

What I think speaks in favour of individual comparison: Training and test documents have a more comparable format, each consist of just one long sentence. It is also simpler to do, and it seems like I am not doing anything wrong.

For one big document per label: Say I have training sets with 20 examples for each of two possible readings of some word. In the first reading, which is the right one, all examples score pretty high. In the wrong reading, all score pretty low, except a freak one which has the highest overall score. The wrong label is chosen, unifying all documents into one would get rid of this. But I might introduce a bias by merging all data.


[(see here)]: http://www.senseval.org/senseval3/#",0,1,False,self,,,,,
449,MachineLearning,t5_2r3gv,2015-3-31,2015,3,31,5,30udm1,www-labs.iro.umontreal.ca,A new version of the Deep Learning book draft (30/03/2015),https://www.reddit.com/r/MachineLearning/comments/30udm1/a_new_version_of_the_deep_learning_book_draft/,clbam8,1427745840,,3,36,False,default,,,,,
450,MachineLearning,t5_2r3gv,2015-3-31,2015,3,31,5,30udvh,timdettmers.wordpress.com,Understanding Convolution in Deep Learning,https://www.reddit.com/r/MachineLearning/comments/30udvh/understanding_convolution_in_deep_learning/,clbam8,1427745955,,4,27,False,http://b.thumbs.redditmedia.com/gYzwjTOktY3hGLCboy7K3tjqcJVbAV-oFwMigro85Og.jpg,,,,,
451,MachineLearning,t5_2r3gv,2015-3-31,2015,3,31,5,30ui94,self.MachineLearning,Storage formats for large datasets,https://www.reddit.com/r/MachineLearning/comments/30ui94/storage_formats_for_large_datasets/,benanne,1427747870,"I was wondering what everyone who's worked with large datasets uses to to store them on disk. By 'large' I mean: too large to keep loaded in RAM completely, i.e. more than a couple of GB. There seem to be quite a few options these days and people don't usually talk about this 'technical detail'. 

I guess the main requirements would be lightning fast reads and compactness (i.e. compression is a plus).

I've used numpy array files, Python pickle files and HDF5 files in the past. When I work with JPEG images, I tend to just keep them in that format because any form of pre-processing or conversion tends to make them take up a lot more disk space, and there is a potential loss of fidelity as well.

All of these approaches have their own problems. bcolz ( http://bcolz.blosc.org/ ) looks really interesting but I haven't used it. Caffe (which I've never used either) seems to use LevelDB or LMDB for storage, what are the advantages of that?

Is there any literature (blog posts, papers, ...) that discusses this? I'd also be very interested to hear everyone's experiences.

Personally I was a big fan of HDF5 until I found out that the h5py module for Python holds on to the GIL during HDF5 file reads and does not play well with threading / multiprocessing. When training a convnet it's nice to be able to load data in parallel with training, so that turned out to be an issue sometimes.",15,13,False,self,,,,,
452,MachineLearning,t5_2r3gv,2015-3-31,2015,3,31,6,30un7c,self.MachineLearning,Tools for Automatically Managing Experiments,https://www.reddit.com/r/MachineLearning/comments/30un7c/tools_for_automatically_managing_experiments/,alexmlamb,1427750015,"Hello, 

What do people on r/MachineLearning use to manage their experiment results (I'm especially interested in Python)?  A simple solution is to just manually update a text file or notebook every time an experiment is ran, but this is slow and unlikely to record all relevant details.  It's also a pain to copy all the generated figures into a document file. 

I think that it would be nice to have a program that: 

1.  Automatically generates a report folder every time an experiment is ran.  
2.  Saves figures from experiment runs.  Provides method calls for logging results.  
3.  Takes a snapshot of the current code and associates it with the experiment results.  It would be especially nice if this had git integration (i.e. it uses the most recent code from a certain branch and then it associates the results with a specific commit).  ",2,2,False,self,,,,,
453,MachineLearning,t5_2r3gv,2015-3-31,2015,3,31,6,30un85,jackhoy.com,Summary of 'Computing Machinery and Intelligence' (1950) by Alan Turing,https://www.reddit.com/r/MachineLearning/comments/30un85/summary_of_computing_machinery_and_intelligence/,hackjoy,1427750025,,0,0,False,http://b.thumbs.redditmedia.com/lEGjlx2uCL4uics635vPBR4T9VFzAegtD7lKK4x_Kzc.jpg,,,,,
454,MachineLearning,t5_2r3gv,2015-3-31,2015,3,31,10,30vhp5,github.com,Text Understanding from Scratch source code released.,https://www.reddit.com/r/MachineLearning/comments/30vhp5/text_understanding_from_scratch_source_code/,feedtheaimbot,1427764016,,6,15,False,http://b.thumbs.redditmedia.com/sYgjGg2-DMTZAaEZgi5tGLeu772krXMst-FxUTA9FDY.jpg,,,,,
455,MachineLearning,t5_2r3gv,2015-3-31,2015,3,31,11,30vqon,sqlstream.com,"Why Storm's ""record-by-record stream processing engines"" is better than Spark's ""batch-based processing"" for stream processing",https://www.reddit.com/r/MachineLearning/comments/30vqon/why_storms_recordbyrecord_stream_processing/,kunjaan,1427768517,,0,0,False,default,,,,,
456,MachineLearning,t5_2r3gv,2015-3-31,2015,3,31,11,30vu1i,snippus.com,Understanding Convolution in Deep Learning,https://www.reddit.com/r/MachineLearning/comments/30vu1i/understanding_convolution_in_deep_learning/,[deleted],1427770375,,0,0,False,default,,,,,
457,MachineLearning,t5_2r3gv,2015-3-31,2015,3,31,14,30w9lm,tetraduzione.tumblr.com,I wonder how many would understand this,https://www.reddit.com/r/MachineLearning/comments/30w9lm/i_wonder_how_many_would_understand_this/,ciolaamotore,1427779618,,3,0,False,default,,,,,
458,MachineLearning,t5_2r3gv,2015-3-31,2015,3,31,15,30wd57,karpathy.github.io,Breaking Linear Classifiers on ImageNet,https://www.reddit.com/r/MachineLearning/comments/30wd57/breaking_linear_classifiers_on_imagenet/,iori42,1427782331,,16,33,False,http://b.thumbs.redditmedia.com/yyPssbtZFXFhlNMD4o8muQJ5fle593x5Gxivq6UMpDk.jpg,,,,,
459,MachineLearning,t5_2r3gv,2015-3-31,2015,3,31,16,30witm,self.MachineLearning,"Global Glass Fiber Textile Machine Industry Size, Share, Market Trends, Growth, Report 2015",https://www.reddit.com/r/MachineLearning/comments/30witm/global_glass_fiber_textile_machine_industry_size/,MarkLesnar,1427787111,,0,0,False,default,,,,,
460,MachineLearning,t5_2r3gv,2015-3-31,2015,3,31,19,30wt7b,youtu.be,Peter Norvig: How Computers Learn (Vienna Gdel Lecture 2015),https://www.reddit.com/r/MachineLearning/comments/30wt7b/peter_norvig_how_computers_learn_vienna_gdel/,allay,1427797042,,1,53,False,http://b.thumbs.redditmedia.com/h5Pf87nx_On-PARPyrKenHlECMbxBxljJO75eTx8Y2A.jpg,,,,,
461,MachineLearning,t5_2r3gv,2015-3-31,2015,3,31,23,30xdo9,self.MachineLearning,Using non-standard datasets for DBN,https://www.reddit.com/r/MachineLearning/comments/30xdo9/using_nonstandard_datasets_for_dbn/,tharuniitk,1427810918,"Dear Dr.Bengio, 
I am a neophyte researcher working on using deep learning techniques for classification problems with regard to biological signals. My dataset is in the range (-1,1). I was reading your paper http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf  (section:Extension to continuous-valued inputs)
using RBM pretraining for image pixels scaling them to (0,1). My question is whether is there an option to use negative input patterns in DBN.  Is there a way I can use an RBM in my case?

Secondly, Can I use DBN to learn any kind of dataset. Could you please direct me to some work, proofs on this aspect.",3,0,False,self,,,,,
462,MachineLearning,t5_2r3gv,2015-3-31,2015,3,31,23,30xek9,self.MachineLearning,Graduation Project: anomaly detection in log files,https://www.reddit.com/r/MachineLearning/comments/30xek9/graduation_project_anomaly_detection_in_log_files/,Bazzibit,1427811347,"Dear redditors of /r/MachineLearning,

&amp;nbsp;

First of all excuse me for any spelling/grammatical errors I made in this post as English is not my primary language.
The purpose of this post is to collect guidelines, tips, experiences and references to literature (or any other form of information) as input for my graduation project.

&amp;nbsp;

**About Me:**


Before I delve into the details of the project, let me quickly introduce myself.
I am a 23-years old informatics / computer science student (not sure how the study is called abroad, but the goal is to become a software engineer) from The Netherlands who just started with his graduation project.
I have experience with multiple aspects of software engineering, but my experiences are limited to the 4 years I've been studying.

&amp;nbsp;

**The Project:**


I am doing a research for a middle-sized software company (around 45 employees).
The company is primarily focused around web development (mostly .NET applications), but they are trying to stay acquainted with new developments in the world of software development by reserving time for research.
Recently the company started with doing research on the field of machine learning to see if there lie any applications that are useful to them.
My project is part of this new research track.

&amp;nbsp;

Now I'll try and give some insight in the global setup and goals of my project.
The company collects a lot of log data from multiple websites they developed for customers, all this log data is stored in a Elasticsearch database.
The log data consists of all kinds of information regarding requests done to the websites (e.g. time of request, user agent, client ip, the requested page and much more).
The company now wants to see if they can use machine learning techniques to detect anomalies in log files.

&amp;nbsp;

The first part of my research will be targeted at *the separation of human and non-human users (e.g. web scrapers) by applying machine learning techniques on the log files*.
I have to do research on the topic and produce a prototype that analyses the log files and classifies users as 'humans' or 'bots' (and maybe other distinctions like 'malicious bot' or 'normal bot').
The company already did some research on the topic and decided they want to use Python with the scikit-learn machine learning library as tools to produce the prototype.

&amp;nbsp;

Neither the company nor me have prior experience in machine learning. 
I do have some prior Python experience, but it's quite limited.
That said we decided to start with the first part of my research and expand the research if time allows it (I have about 24 weeks for the project).

&amp;nbsp;

**My Questions:**


The first 2 weeks of my project I've been reading a lot of (online) literature on the subjects of machine learning and anomaly detection, but sometimes the amount of information gets pretty overwhelming and I find it hard to make a relevant selection of information that is needed for my project.

&amp;nbsp;

These are some of the topics/questions I have been struggling with so far:

1. I think my problem is a supervised learning classification problem (correct me if I am wrong). How can I construct a solid training set that I can use as input for building my model? The current log files have no indicators whether a user is a human or a bot. My current idea is to construct the training set myself by defining certain thresholds (e.g. if requests/min &gt; 1000 and ... then 'bot' else 'human').

2. Given the problem what (supervised) algorithms are best fit for training the models?

3. What features are 'contributing' to a user being either a human or a bot. Could unsupervised dimensionality reduction (like PCA) be useful here and if so, how should I approach this?

4. Could unsupervised learning (like clustering) help me with solving the given problem and if so, what unsupervised algorithms could be of use?

&amp;nbsp;

Any advice regarding where to start, where to find relevant (beginner-friendly) literature or general guidelines as to how I could approach the problem are highly appreciated.
Of course any thoughts are welcome.

&amp;nbsp;

I want to thank those that took the time to read my post.
I am happy to answer any questions you might have!

&amp;nbsp;
&amp;nbsp;

Edit: As requested I provided some samples of the log data I want to analyse using unsupervised learning techniques.",12,7,False,self,,,,,
