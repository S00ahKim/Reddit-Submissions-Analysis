,sbr,sbr_id,date,year,month,hour,day,id,domain,title,full_link,author,created,selftext,num_comments,score,over_18,thumbnail,media_provider,media_thumbnail,media_title,media_description,media_url
0,MachineLearning,t5_2r3gv,2016-2-1,2016,2,1,9,43lrf3,self.MachineLearning,Classifying a decision tree - understanding basic examples,https://www.reddit.com/r/MachineLearning/comments/43lrf3/classifying_a_decision_tree_understanding_basic/,plasticstone,1454285391,"Hello
How to classify a decision tree that has output 'true/false' and examples containing 5 parameters of n diffrent values (say n=2) and gain for any parameter is 0?
root = height 1
parent = height 2
child = height 3
and for height 3 can appear: child, child, parent
then height 4: child1 (of parent on height 3), child2... etc.
Is it correct to receive output:
root: parameter1
parent 1: value1, parent 2: (same height as parent 1) 
child 1: false (randomly chosen among true/false if count of examples with false output == count of examples with true output otherwise most common value of outputs )
child 2: same as above

example2 : for another tree lets say:
2 parameters with 3 possible values (gain1: 0,4; gain2:0)
output:
root: parameter1
parent1: value1, parent2: value2, parent3: value3
child1: false/true (chosen as desc above in ex1); child2: ...; child3: ...

example 3:
two parameters gain: 0,4 ; 0,5
2 possible values
root: parameter2
parent1: value1; parent2: value2; parent3: parameter1
child1:... (desc above); child2: ..., child3.1: value1 of parameter1, child3.2: value2 of parameter1,
height = 4 : child4.1  = true/false as desc above, child4.2 same",0,1,False,self,,,,,
1,MachineLearning,t5_2r3gv,2016-2-1,2016,2,1,9,43lupm,aagaaaaaaldiaatsai.photo-gid.com,Thiss Site Lik1ely Contains Sexuually Expliicit Photos Of Someeone You Know! khm..nsgd,https://www.reddit.com/r/MachineLearning/comments/43lupm/thiss_site_lik1ely_contains_sexuually_expliicit/,tfulinfloodov,1454286641,,0,0,False,default,,,,,
2,MachineLearning,t5_2r3gv,2016-2-1,2016,2,1,9,43lx28,rasbt.github.io,Implementations of Sequential Feature Algorithms cmpt. w. scikit-learn classifiers &amp; regressors,https://www.reddit.com/r/MachineLearning/comments/43lx28/implementations_of_sequential_feature_algorithms/,[deleted],1454287537,[deleted],0,1,False,default,,,,,
3,MachineLearning,t5_2r3gv,2016-2-1,2016,2,1,9,43lz29,arxiv.org,Learning authors/titles recent submissions,https://www.reddit.com/r/MachineLearning/comments/43lz29/learning_authorstitles_recent_submissions/,mlnewaccount,1454288294,,0,1,False,default,,,,,
4,MachineLearning,t5_2r3gv,2016-2-1,2016,2,1,11,43mbhs,research.microsoft.com,Deep Learning Tutorial by Y. LeCun and Y. Bengio [video],https://www.reddit.com/r/MachineLearning/comments/43mbhs/deep_learning_tutorial_by_y_lecun_and_y_bengio/,cryptoz,1454293224,,9,80,False,default,,,,,
5,MachineLearning,t5_2r3gv,2016-2-1,2016,2,1,12,43mntr,github.com,"PyDataset, load datasets in Python instantly just like in R",https://www.reddit.com/r/MachineLearning/comments/43mntr/pydataset_load_datasets_in_python_instantly_just/,iamaziz,1454298216,,3,39,False,http://a.thumbs.redditmedia.com/gZOe_BHcnX9oo8rRm1zpjLGKdd41KEcLz1cclq0Oa50.jpg,,,,,
6,MachineLearning,t5_2r3gv,2016-2-1,2016,2,1,14,43mz1y,self.MachineLearning,ELI5: Can someone explain the capabilities of Tensor Flow in layman's terms so I could think about some potential applications?,https://www.reddit.com/r/MachineLearning/comments/43mz1y/eli5_can_someone_explain_the_capabilities_of/,[deleted],1454303043,[deleted],6,0,False,default,,,,,
7,MachineLearning,t5_2r3gv,2016-2-1,2016,2,1,14,43n02x,self.MachineLearning,ConvNet (or other DNN) on mobile devices?,https://www.reddit.com/r/MachineLearning/comments/43n02x/convnet_or_other_dnn_on_mobile_devices/,ThatWillNeverShake,1454303528,"Is it possible, with mobile device processing power? Let's say I'm trying to build a facial recognition app, how deep the ConvNet should be? Which weight can I use? I was thinking of using NIN cifar 10 because it's relatively small, so maybe reshape the input size? Has anyone tried this before? ",13,8,False,self,,,,,
8,MachineLearning,t5_2r3gv,2016-2-1,2016,2,1,14,43n4x2,dezyre.com,Top 10 Machine Learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/43n4x2/top_10_machine_learning_algorithms/,margarettecrystal,1454305816,,0,1,False,default,,,,,
9,MachineLearning,t5_2r3gv,2016-2-1,2016,2,1,19,43nvts,blog.keras.io,How convolutional neural networks see the world,https://www.reddit.com/r/MachineLearning/comments/43nvts/how_convolutional_neural_networks_see_the_world/,BadGoyWithAGun,1454321973,,26,91,False,http://b.thumbs.redditmedia.com/elA8nwDq0C4LDm5JqZKZCjY8JlmJu9V7gHk18_kYLiY.jpg,,,,,
10,MachineLearning,t5_2r3gv,2016-2-1,2016,2,1,20,43o21i,self.MachineLearning,"Given geological relief data, infer a function that would generate similar terrains. Nudge beginner in the right direction.",https://www.reddit.com/r/MachineLearning/comments/43o21i/given_geological_relief_data_infer_a_function/,dmos62,1454325941,"I have chosen this problem to teach myself machine learning.

I have 2 requests. Second being more in-depth version of first:

1. Please help describe this problem in correct terminology, so I could search for information.

2. Please describe an overview of subjects I would have to become comfortable with to solve such problems.",8,2,False,self,,,,,
11,MachineLearning,t5_2r3gv,2016-2-1,2016,2,1,21,43o7m8,self.MachineLearning,Suggestions for Final Masters Project,https://www.reddit.com/r/MachineLearning/comments/43o7m8/suggestions_for_final_masters_project/,pighive,1454329209,"I am in second year of my masters in computer science and its time to come up with proposal for my project. I have done some work towards this requirement during summer of 2015, but I am not really satisfied with the level of masters and my adviser is fine with submitting. The work is basically taking two sets of sensor signals transforming them into signals by fft, dft and then training and testing on different classifiers in MatLab.
I would like to ask this community to give some suggestion on a topic and if possible a road map for a project. I am an average programmer. I have 4 weeks for proposal and another 8 weeks for completion including paper work. 
Thanks. Appreciate any kind of input.

TL:DR; Suggestions for Masters project.",5,0,False,self,,,,,
12,MachineLearning,t5_2r3gv,2016-2-1,2016,2,1,21,43oc02,scoop.it,Artificial Intelligence - Opportunities &amp; Threats,https://www.reddit.com/r/MachineLearning/comments/43oc02/artificial_intelligence_opportunities_threats/,Brett_Kelly,1454331581,,1,0,False,http://a.thumbs.redditmedia.com/-cczw9kPNP78n38jmhZxH3Xubj0bnNaKJ_YZNpW0pV8.jpg,,,,,
13,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,1,43p3fj,experiments.mostafa.io,Interactive step-by-step visualization of a basic Artificial Neural Network,https://www.reddit.com/r/MachineLearning/comments/43p3fj/interactive_stepbystep_visualization_of_a_basic/,droushi,1454342811,,5,60,False,default,,,,,
14,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,1,43p55r,self.MachineLearning,Stock Market Forecasting Using Machine Learning,https://www.reddit.com/r/MachineLearning/comments/43p55r/stock_market_forecasting_using_machine_learning/,QuantTrades,1454343418,"I need to start off by saying that I am completely new to machine learning and neural networks. 
I want to start experimenting with the technology and I have put together a very capable machine to start playing with.  There are many platforms to start experiment with but could someone recommend what they think is the best one for the financial training and stock market use.
I would also prefer to use the CUDA model, my test system has SLI 970s, and the next one will have the 2-4 k40s or k80s.  I just need to learn a lot more about everything before I invest in the system I want but do not currently deserve. 

So essentially could I have some guidance on what platforms you all are using for the creation of a black box? 
Why you are using that specific platform? 
Any learning material or books to get started that are not listed in the FAQ? 
Lessons learned or guidance to avoid problems that you have incurred?
 Thank you all in advance. 
",17,0,False,self,,,,,
15,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,1,43p7kq,primaryobjects.com,IBM Watson: Building a Cognitive App with Concept Insights,https://www.reddit.com/r/MachineLearning/comments/43p7kq/ibm_watson_building_a_cognitive_app_with_concept/,primaryobjects,1454344269,,0,0,False,http://b.thumbs.redditmedia.com/__H39IzBNHnEC7-PNEK6AvdKX1YbmmWccJgXRIvyFPk.jpg,,,,,
16,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,1,43p9ui,youtube.com,Basic Inference in Bayesian Networks,https://www.reddit.com/r/MachineLearning/comments/43p9ui/basic_inference_in_bayesian_networks/,virtualjd2015,1454345013,,0,9,False,http://b.thumbs.redditmedia.com/PtmkoB2pdhytmuJ9QTl6jiJXrrxrZouUuSNjRwK8ZRA.jpg,,,,,
17,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,2,43pevb,jxieeducation.com,Time series decomposition tutorial,https://www.reddit.com/r/MachineLearning/comments/43pevb/time_series_decomposition_tutorial/,Jxieeducation,1454346641,,4,12,False,http://b.thumbs.redditmedia.com/d0JWQ7BB-bbW--76_sGBgmo0i51XIgenqs1hcFCHatc.jpg,,,,,
18,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,2,43phkr,eugenezhulenev.com,Large scale Deep Learning with TensorFlow on EC2 Spot Instances,https://www.reddit.com/r/MachineLearning/comments/43phkr/large_scale_deep_learning_with_tensorflow_on_ec2/,ezhulenev,1454347534,,0,1,False,default,,,,,
19,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,2,43pk8i,blog.evorithmics.org,When Evolution Will Outperform Local Search,https://www.reddit.com/r/MachineLearning/comments/43pk8i/when_evolution_will_outperform_local_search/,kburjorj,1454348392,,21,35,False,http://a.thumbs.redditmedia.com/cWj3-V0bhvt1xyGd-bZptw5rpNQXPKT-L6sxfKIhj64.jpg,,,,,
20,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,3,43pqh8,self.MachineLearning,is GloVe the current state of the art in converting words to vectors?,https://www.reddit.com/r/MachineLearning/comments/43pqh8/is_glove_the_current_state_of_the_art_in/,textClassy,1454350358,,24,24,False,self,,,,,
21,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,4,43q13w,guessthecorrelation.com,Correlation Estimation Game,https://www.reddit.com/r/MachineLearning/comments/43q13w/correlation_estimation_game/,kzn,1454353765,,0,1,False,default,,,,,
22,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,4,43q5v8,self.MachineLearning,reading list to catch up on latest deep learning techniques?,https://www.reddit.com/r/MachineLearning/comments/43q5v8/reading_list_to_catch_up_on_latest_deep_learning/,[deleted],1454355316,[deleted],0,1,False,default,,,,,
23,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,4,43q74h,self.MachineLearning,"Are AI-Researchers focusing on the wrong aspects? If so, what should they focus on instead of e.g. ANNs?",https://www.reddit.com/r/MachineLearning/comments/43q74h/are_airesearchers_focusing_on_the_wrong_aspects/,DeapSoup,1454355733,"I am a total noobie when it comes to ML/AI, but do any of you guys have the feeling that the focus is on the wrong part? Can ANN ever lead to AGI? Should they switch their focus on another (maybe yet undiscovered) part to achieve AGI?",11,0,False,self,,,,,
24,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,5,43qdeu,self.MachineLearning,Predicting College Basketball Scores,https://www.reddit.com/r/MachineLearning/comments/43qdeu/predicting_college_basketball_scores/,nyd71839,1454357811,"Hello.  Over the past couple of months I have started to learn and familiarize myself with machine learning techniques and methods.  After reading some books and taking some online courses I have decided to put what I have learned into a personal project.  I have decided to try to predict college basketball scores.  I am aware that the gambling market is pretty efficient but I figure at the very least I will get better at programming and the different machine learning methods.

Now I know if anyone has actually been successful at this they are not going to share their secrets.  But I was hoping I could maybe get some general advice.

I have started by using a linear regression model. My thought is that I can use clustering and principal component analysis combined with the regression.   Beyond that however alot of the machine learning algorithms I have read about usually give an example of a classification problem instead of predicting a numeric value like I am trying to do.  Can anyone offer advice on other algorithms I should look into?  Maybe give me some direction on how they would go about or get started.

Thank you.",3,0,False,self,,,,,
25,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,5,43qfjz,mattturck.com,The Power of Data Network Effects,https://www.reddit.com/r/MachineLearning/comments/43qfjz/the_power_of_data_network_effects/,lokator9,1454358543,,0,2,False,http://b.thumbs.redditmedia.com/sgI-_YRCGuzWld34kUZfEdC-Lm8jCf3HO_-aYSmejnk.jpg,,,,,
26,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,6,43qn99,self.MachineLearning,I'm trying to make a list of the top 20 people who will push forward machine learning so I can set google alerts for them. Who would you recommend?,https://www.reddit.com/r/MachineLearning/comments/43qn99/im_trying_to_make_a_list_of_the_top_20_people_who/,textClassy,1454361051,"I'll start:
Richard Socher
Andrew Ng

",17,2,False,self,,,,,
27,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,6,43qqid,self.MachineLearning,Companion Post: Which machine learning researchers/machine learning results are most undervalued with respect to attention of press and ML community?,https://www.reddit.com/r/MachineLearning/comments/43qqid/companion_post_which_machine_learning/,textClassy,1454362189,,3,7,False,self,,,,,
28,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,7,43r34a,self.MachineLearning,Deep Belief Network,https://www.reddit.com/r/MachineLearning/comments/43r34a/deep_belief_network/,hassanzadeh,1454366415,"Hello everyone,
There is an python implementation of DBN at http://deeplearning.net/tutorial/code/DBN.py however, when I read the source code, it does not seem to be DBN but rather DBM. Can anyone let me know if they are really doing DBN or that's just an abuse of a technical term.
Thanks",2,9,False,self,,,,,
29,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,7,43r5r1,self.MachineLearning,Learn to do Sentiment Analysis with bag-of-words,https://www.reddit.com/r/MachineLearning/comments/43r5r1/learn_to_do_sentiment_analysis_with_bagofwords/,ataspinar,1454367355,"As a precursor to research about Sentiment Analysis with Text Classifiers (Naive Bayes, Maximum Entropy, SVM), Sentiment Analysis with bag-of-words was done and Positive / Negative Sentiment was detected with an accuracy of 60%. This is when only unigrams are used. This percentage will be much when bigrams or trigrams are used (in a next blog-post). See the results at: 
part 1: http://tinyurl.com/gnlfqqm
part 2: http://tinyurl.com/zcj226q ",0,2,False,self,,,,,
30,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,8,43r8c8,self.MachineLearning,What are the topics that are popular in the machine learning nowadays?(except for deep learning),https://www.reddit.com/r/MachineLearning/comments/43r8c8/what_are_the_topics_that_are_popular_in_the/,regularized,1454368224,I'd say variational inference and bayesian optimization. What do you think?,16,27,False,self,,,,,
31,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,10,43rv1a,ltdntannoaaldt.csrelax.com,dand TH1S IS NoooT  DTING S1TE. LARGEST IN WORLD ONLINE SERCH SEXXXX PARTNERS dand,https://www.reddit.com/r/MachineLearning/comments/43rv1a/dand_th1s_is_nooot__dting_s1te_largest_in/,tecinare,1454376705,,0,1,False,default,,,,,
32,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,14,43sqai,self.MachineLearning,"Looking for entrepreneurial technical peeps for a wearable product. Pre-funding, Big equity, big peeps involved and definitely, your working space will be in Vancouver, Canada with Permanent Residency. Any tips? PM me.",https://www.reddit.com/r/MachineLearning/comments/43sqai/looking_for_entrepreneurial_technical_peeps_for_a/,joshthegeek94,1454389257,[removed],0,0,False,default,,,,,
33,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,16,43t5bp,self.MachineLearning,Best techniques for dynamic content recommendation,https://www.reddit.com/r/MachineLearning/comments/43t5bp/best_techniques_for_dynamic_content_recommendation/,burn_in_flames,1454396519,"Hi,

I was wondering what the best techniques were for recommending dynamic content to users. I have a set of users who each define a set of topics they interested in, I have their history of rating previous tweets (1-5 rating on how much they enjoyed it), and I am looking to recommend a new tweet to a user as it comes in. This recommendation should be such that the user who rated a similar tweet the highest should get the new tweet. One subtlety is that a tweet is only even rated by one user, which makes traditional CF methods not work as far as I can tell.

Any suggestions on new techniques or alternative recommendation techniques which allow recommendations from a dynamic set A, to be made to users based on their history in a static set B.

To me it appears that training a neural network would be the best approach, but I am not sure how to deal with the dynamic size of the user set and incoming tweet queue.

Thanks",8,3,False,self,,,,,
34,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,17,43tcb2,github.com,A web-based interactive visualization of artificial neural networks.,https://www.reddit.com/r/MachineLearning/comments/43tcb2/a_webbased_interactive_visualization_of/,ahmgeek,1454400868,,3,59,False,http://b.thumbs.redditmedia.com/iKlk3bNCaSeF24PfxGt779egQJvJ8_gqTaxqu3yYm5E.jpg,,,,,
35,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,17,43tdpu,self.MachineLearning,Classifying Pairs of Text,https://www.reddit.com/r/MachineLearning/comments/43tdpu/classifying_pairs_of_text/,[deleted],1454401834,[deleted],1,1,False,default,,,,,
36,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,18,43thme,self.MachineLearning,Twitter Word Cloud from User Timelines,https://www.reddit.com/r/MachineLearning/comments/43thme/twitter_word_cloud_from_user_timelines/,backpackerzee,1454404575,"I have entire user timeline for a set of users.

I want to get a word cloud for a set of users like what Klout offer. Can someone please recommend/advice on how to go about this.

Thanks in Advance.

",1,1,False,self,,,,,
37,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,19,43toms,youtube.com,PredictionIO: Machine Learning in The Lab to On Production,https://www.reddit.com/r/MachineLearning/comments/43toms/predictionio_machine_learning_in_the_lab_to_on/,tstonez,1454409114,,2,15,False,http://a.thumbs.redditmedia.com/IETO0fk0bNdPS4dTfHHWgDvOcqE3edv9Wygw37x9Bn0.jpg,,,,,
38,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,19,43tq4j,self.MachineLearning,Is this a good way to start?,https://www.reddit.com/r/MachineLearning/comments/43tq4j/is_this_a_good_way_to_start/,Eildosa,1454410101,"Hi,

I want to do horse race machine learning prediction, with no background whatsoever in ML.

I'm a python develloper mainly doing database/excel related stuff.

would this book be a good way to start? : http://goo.gl/26qQ4D

I read a lot of stuff online and I'm getting nowhere.
Also it has been 5 years since I ended school and did not do any math since then, my level of math has enourmously dropped.

ps : I also code in JAVA or C++ with a preference for JAVA, what I'm looking for is a real basic ML book with concrete case and algorythm implementations.",0,2,False,self,,,,,
39,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,20,43ttf0,self.MachineLearning,Genetic algorithm exemple for Tensorflow?,https://www.reddit.com/r/MachineLearning/comments/43ttf0/genetic_algorithm_exemple_for_tensorflow/,hapliniste,1454412388,"Hi! I'm looking into GAs and currently using Tensorflow, is there some examples or notebooks of GA using Tensorflow?

Thanks!",5,3,False,self,,,,,
40,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,20,43tuzq,github.com,Character-Aware Neural Language Models in Tensorflow,https://www.reddit.com/r/MachineLearning/comments/43tuzq/characteraware_neural_language_models_in/,carpedm20,1454413337,,2,7,False,http://b.thumbs.redditmedia.com/idHCXNrU-ATqB-i59sj5JIgbn0yf5o0c_gcmvu5Btdo.jpg,,,,,
41,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,21,43tx6t,self.MachineLearning,"What does ""Double Blind, single review cycle"" mean?",https://www.reddit.com/r/MachineLearning/comments/43tx6t/what_does_double_blind_single_review_cycle_mean/,ICML2016,1454414624,,3,1,False,self,,,,,
42,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,22,43u5qz,aagaoniaagaaaais.ferrohomme.com,sda Exxx-girlfriend fucks wiith everyne! I even fund it nline seex-daating! sda,https://www.reddit.com/r/MachineLearning/comments/43u5qz/sda_exxxgirlfriend_fucks_wiith_everyne_i_even/,rihegegna,1454419145,,0,1,False,default,,,,,
43,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,22,43u6b4,medium.com,Deep Learning in Rust,https://www.reddit.com/r/MachineLearning/comments/43u6b4/deep_learning_in_rust/,racoonear,1454419437,,7,23,False,http://a.thumbs.redditmedia.com/7H1fhWBwRhQWLW5vzi1ILR-OdYkrCf9fMo7stAhMDL0.jpg,,,,,
44,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,22,43u6dt,arxiv.org,[1601.07925] Automating biomedical data science through tree-based pipeline optimization,https://www.reddit.com/r/MachineLearning/comments/43u6dt/160107925_automating_biomedical_data_science/,rhiever,1454419473,,0,2,False,default,,,,,
45,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,22,43uawi,youtu.be,AlphaGo's victory analysed during 1h30min by a 9 dan pro player.,https://www.reddit.com/r/MachineLearning/comments/43uawi/alphagos_victory_analysed_during_1h30min_by_a_9/,chaosintestinal,1454421543,,4,6,False,http://b.thumbs.redditmedia.com/2fBhlyva7z-WvUeZ7h5iDZ0wOZ76KgUvyUxPO_PMCos.jpg,,,,,
46,MachineLearning,t5_2r3gv,2016-2-2,2016,2,2,23,43ud0u,cloudacademy.com,Future as a Service: Google Vision API,https://www.reddit.com/r/MachineLearning/comments/43ud0u/future_as_a_service_google_vision_api/,[deleted],1454422399,[deleted],1,0,False,default,,,,,
47,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,1,43uthp,self.MachineLearning,Can we combine convolution and RNN in the same layer ?,https://www.reddit.com/r/MachineLearning/comments/43uthp/can_we_combine_convolution_and_rnn_in_the_same/,Schlagv,1454428891,"When I see big nets with space and time, I see lots of Conv layers, then fully connected layers when the spatial resolution is low enough.

Then, they sometimes add RNN/LSTM layers in addition or instead of fully connected layers. But I don't remember having ever seen LSTM layers between conv layers, why ?

For video processing, it would seem natural for me to have 2D convnets (and not 3D convnets) with LSTMs to store temporal visual patterns with several levels of details.

So why do people like to use 3D convnets instead of conv-LSTM layers to find temporal patterns ? LSTMs allow dynamic speed in patterns (the update/reset strength is computed at each time step) while 3D convnets would only allow very short term temporal dependency (unless you do pooling on the 3rd dimension, but then you lose short term memory).",7,10,False,self,,,,,
48,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,1,43uxpr,self.MachineLearning,Any good resources/algorithms for automatic feature extraction of time series data (to be used in classification)?,https://www.reddit.com/r/MachineLearning/comments/43uxpr/any_good_resourcesalgorithms_for_automatic/,Jamesbev,1454430376,"Working on a classification model based on time series data--looking for features that help to answer the following: how likely is X to occur in the next Y months if A has occurred in the past B days?

I'm familiar with creating time series features by hand (moving averages, exponential smoothing, one hot encoding time/day-of-week/month/holidays), but wondering if there are any good methods for automatically hunting for the best [time period] and [method of aggregation] for specific input variables.

Thanks!",1,0,False,self,,,,,
49,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,1,43uzh4,self.MachineLearning,What do?,https://www.reddit.com/r/MachineLearning/comments/43uzh4/what_do/,Hamush,1454431006,"WHO AM I?
So I'm a high school student with a great desire to learn about the scientific world of machine learning. I know Python, Java, C# and more languages, so I can code, but I really want to expand my prospects with machine learning.

WHAT I'VE TRIED:
I have tried talking to a friend who is competent with evolutionary algorithms, but he just doesn't have time to teach me from the ground up about them. I tried learning from the Google-made deep learning tutorial on Udacity, but that was way too advanced for me, and I subbed to this subreddit in hopes of some helpful starting tips/ guidelines, but most of the stuff here is too advanced for me. 

OK SO WHAT'S MY QUESTION?
Where do I start my journey to machine learning mastery? In case you're wondering about some prerequisites that I have, well I have all of the prerequisites to this course:
https://www.udacity.com/course/deep-learning--ud730
except the calculus and supervised learning ones, but calculus can be learned no problem.

MORE SPECIFICS (IF REQUIRED):
Generally speaking, I don't care about what resources I get, I just want a good starting point from which i can easily graduate to some more complex material. If you need a certain field in which I want to transition into, then that would be the field of NLP, so any materials that reference that would be great.

Thanks in advance!",10,0,False,self,,,,,
50,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,1,43v1x4,self.MachineLearning,Neural Networks Regression vs Classification with bins,https://www.reddit.com/r/MachineLearning/comments/43v1x4/neural_networks_regression_vs_classification_with/,RichardKurle,1454431842,"I have seen a couple of times that people transform Regression tasks into Classification, by distributing the output value on several bins.
Also I was told, that Neural Networks are bad for Regression Tasks. Is that true? I cannot find a reason that would support this claim.",15,8,False,self,,,,,
51,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,2,43va6p,self.MachineLearning,Is there any logic behind the design of architectures?,https://www.reddit.com/r/MachineLearning/comments/43va6p/is_there_any_logic_behind_the_design_of/,WTFseriously_,1454434636,"By architecture, I mean something like:

---
input size ?x80x80x4

convolutional layer 1: Weights of shape 8x8x4x32 stride of 4

2x2 pooling

convolutional layer 2: Weights of shape 4x4x32x64 stride of 2

2x2 pooling

convolutional layer 3: Weights of shape 3x3x64x64 stride of 1

2x2 pooling

fully connected layer 1: Weights of shape 256x256

fully connected layer 2: Weights of shape 256 x target size

output size ? x target size


---

I understand how the shapes change from one layer to the next, and how the number of filters / sizes / strides affect the computation. 

What I don't understand is, what makes the architecture anything more than just arbitrary? I can change the above numbers around to virtually anything (within constraints of making layers fit together) but I don't see how one architecture might or might not be better than any other. 

My assumption is there isn't any logic behind it, and it's kind of just putting pieces together and figuring out what works. If that's the case, what are the current 'best practices' for architecture design? 

Above I have decreasing filter size and stride, with an increasing number of filters - is that a standard thing to do? Any particular reason why?",8,13,False,self,,,,,
52,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,5,43w2do,self.MachineLearning,how can I get an intuition for the meaning of the 300 numbers in each word2vec vector.,https://www.reddit.com/r/MachineLearning/comments/43w2do/how_can_i_get_an_intuition_for_the_meaning_of_the/,textClassy,1454444308,can I do things like take the standard deviation of them to find out how specialized a word is for example? it seems like the possibilities are endless but it would be helpful to stand on the shoulders of machine learning researchers in learning how to make use of these pretrained vectors.,18,6,False,self,,,,,
53,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,5,43w4kp,self.MachineLearning,What are some open/active research areas in Deep Learning currently?,https://www.reddit.com/r/MachineLearning/comments/43w4kp/what_are_some_openactive_research_areas_in_deep/,awhitesong,1454445071,"I am a final year student with a decent background in machine learning(3yrs) and deep learning(1yr). So with that background, i am looking forward for a good research project in deep learning. I have tried considering program synthesis, reinforcement learning, multi-documents summarizations etc. A lot of emphasis has been given on unsupervised learning(in deep learning) by Andrew Ng and Yoshua Bengio in Quora's sessions as well as the openAI team here on reddit. But not able to figure out some active areas that could be researched at an undergrad level. So what are some open research areas in deep learning that could researched currently at this level?",5,4,False,self,,,,,
54,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,5,43w5w1,felixlaumon.github.io,Recognizing and Localizing Endangered Right Whales with Extremely Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/43w5w1/recognizing_and_localizing_endangered_right/,cryptoz,1454445521,,4,50,False,http://b.thumbs.redditmedia.com/hsAGSqp55eTHD1nec8WSft1FUBBERUOzymClac2B7HA.jpg,,,,,
55,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,5,43w8ym,news.ycombinator.com,summarizer seems to have some fancy ML/AI generating fantastic summaries,https://www.reddit.com/r/MachineLearning/comments/43w8ym/summarizer_seems_to_have_some_fancy_mlai/,[deleted],1454446588,[deleted],1,1,False,default,,,,,
56,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,6,43whh1,github.com,scorer: R package to quickly data science and machine learning models,https://www.reddit.com/r/MachineLearning/comments/43whh1/scorer_r_package_to_quickly_data_science_and/,paulhendricks,1454449557,,0,0,False,http://b.thumbs.redditmedia.com/uQ_NXdReCV2r9UoSxLxGh5xLaZ0pUbGvfJDwddAXHtg.jpg,,,,,
57,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,6,43wioe,arxiv.org,Gradient-based Hyperparameter Optimization through Reversible Learning,https://www.reddit.com/r/MachineLearning/comments/43wioe/gradientbased_hyperparameter_optimization_through/,woadwarrior,1454449956,,2,11,False,default,,,,,
58,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,7,43wqf4,quora.com,Quora Session with Pedro Domingos,https://www.reddit.com/r/MachineLearning/comments/43wqf4/quora_session_with_pedro_domingos/,clbam8,1454452747,,2,11,False,default,,,,,
59,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,7,43wtsy,youtube.com,DanDoesData: Visualizing Convolutional Neural Nets in Theano,https://www.reddit.com/r/MachineLearning/comments/43wtsy/dandoesdata_visualizing_convolutional_neural_nets/,vanboxel,1454453989,,0,1,False,default,,,,,
60,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,8,43wvx3,rasbt.github.io,Sequential Feature Selection algorithms for scikit-learn classifiers and regressors,https://www.reddit.com/r/MachineLearning/comments/43wvx3/sequential_feature_selection_algorithms_for/,rasbt,1454454765,,0,25,False,http://b.thumbs.redditmedia.com/gw9jlQuY9ZvRPOWjAzg2qygy6Inhn3MDIZl_y1rnzkI.jpg,,,,,
61,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,8,43wx2a,self.MachineLearning,how can I find papers about the composition of word vectors in order to obtain higher level representation?,https://www.reddit.com/r/MachineLearning/comments/43wx2a/how_can_i_find_papers_about_the_composition_of/,textClassy,1454455200,"according to this SO post it is an active research topic ... http://stackoverflow.com/questions/29760935/how-to-get-vector-for-a-sentence-from-the-word2vec-of-tokens-in-sentence

I don't think I have the right keywords though because googling doesn't give me papers about it. ",4,3,False,self,,,,,
62,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,9,43x3tq,self.MachineLearning,Best deep learning framework for deploying on iOS/Android?,https://www.reddit.com/r/MachineLearning/comments/43x3tq/best_deep_learning_framework_for_deploying_on/,555x,1454457783,"What's the best deep learning framework for deploying on iOS/Android? (It doesn't have to train there, but just do the forward propagation as efficiently as those devices allow)",7,7,False,self,,,,,
63,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,10,43xiz7,naatailtaldgasaana.exmhol.com,"My nAme is Nicole Miller, I'm divrced and I waant t find a guy fr the niight. D yu guys freee? PS Nicole",https://www.reddit.com/r/MachineLearning/comments/43xiz7/my_name_is_nicole_miller_im_divrced_and_i_waant/,feedsntesbullpran,1454463609,,0,1,False,default,,,,,
64,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,11,43xm26,self.MachineLearning,Coursera Machine Learning worth it?,https://www.reddit.com/r/MachineLearning/comments/43xm26/coursera_machine_learning_worth_it/,world_leader16,1454464902,"How worth it is machine learning courses on Coursera? I have been working in data analysis for a year now since graduating with a BA in econometrics - currently working at a division inside a hedge fund doing analysis. Why should I, or should not, pay for this coursera course? I need to make a decision by 12 midnight. Redittttttt! :-)",16,0,False,self,,,,,
65,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,11,43xmmr,self.MachineLearning,Essential CS skills for an ML practitioner?,https://www.reddit.com/r/MachineLearning/comments/43xmmr/essential_cs_skills_for_an_ml_practitioner/,[deleted],1454465123,[deleted],6,0,False,default,,,,,
66,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,13,43y5ue,self.MachineLearning,"Are there any Chess AI's that use Unsupervised Learning, and start from being bad at chess to becoming very good?",https://www.reddit.com/r/MachineLearning/comments/43y5ue/are_there_any_chess_ais_that_use_unsupervised/,cantquitreddit,1454473059,"If so, how many games does it take to beat an average player?  Is it something available on the web at all?",7,3,False,self,,,,,
67,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,13,43ybeb,submitanarticle.info,Pneumatic Pump Systems And Their Relation with Hydraulic Systems,https://www.reddit.com/r/MachineLearning/comments/43ybeb/pneumatic_pump_systems_and_their_relation_with/,jackerfrinandis,1454475572,,0,1,False,default,,,,,
68,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,14,43yc5u,arxiv.org,[1601.03317] Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder NMT Model,https://www.reddit.com/r/MachineLearning/comments/43yc5u/160103317_implicit_distortion_and_fertility/,ihsgnef,1454475920,,1,7,False,default,,,,,
69,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,14,43yfi7,scottsievert.com,Gradient descent and physical intuition for heavy-ball acceleration (and with visualization!),https://www.reddit.com/r/MachineLearning/comments/43yfi7/gradient_descent_and_physical_intuition_for/,skier_scott,1454477475,,3,13,False,http://b.thumbs.redditmedia.com/qEIA8V3W4rHGRXAd9wWB3A8Wz3yyJxw1_HeEPAaXhBQ.jpg,,,,,
70,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,15,43ymsl,self.MachineLearning,CNTK or Deeplearning4j ? what do you think will be better for a scalable image classification solution,https://www.reddit.com/r/MachineLearning/comments/43ymsl/cntk_or_deeplearning4j_what_do_you_think_will_be/,mumbaimaari,1454481138,,9,2,False,self,,,,,
71,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,16,43yswe,self.MachineLearning,Are there any methods to regularize parameters other than penalizing the cost function with a metric that measures extent of unwanted behavior?,https://www.reddit.com/r/MachineLearning/comments/43yswe/are_there_any_methods_to_regularize_parameters/,loopnn,1454484617,"Suppose I have a set of values and I want to make sure that at any point of time, only one of them is large, but the rest are really small. I have been told that a good way to do this is to get a good metric that measures this property (like max - mean of values) and add it to my overall network cost. 

Is there any other way to perform such regularization implicitly? The output should be like very low temperature softmax (I cannot use this because softmax just transforms the parameters, but they can still become equal and ruin everything)",11,6,False,self,,,,,
72,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,17,43yyam,blog.kaggle.com,Introducing Kaggle Datasets,https://www.reddit.com/r/MachineLearning/comments/43yyam/introducing_kaggle_datasets/,[deleted],1454488061,[deleted],0,1,False,default,,,,,
73,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,18,43z5yk,self.MachineLearning,Match Prediction Bot (x-post from /r/SoccerBetting),https://www.reddit.com/r/MachineLearning/comments/43z5yk/match_prediction_bot_xpost_from_rsoccerbetting/,Rmanolescu,1454493156,"I made a Football Prediction Bot under *[@fopabot](https://twitter.com/fopabot)* and was hoping to get some feedback. Current overall accuracy is around 74%.


Analysis is made for all matches from major leagues using a simple decision tree and statistical methods. Regression testing is done using several time intervals to evaluate the model over different stages of the competition (currently, the results do not affect the tree selection process).


I am continuously working on tweaking the algorithm and extending to more leagues. 
If you have suggestions or ways to improve, please comment.",44,25,False,self,,,,,
74,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,20,43zia8,cnet.com,Microsoft taps AI with SwiftKey keyboard acquisition,https://www.reddit.com/r/MachineLearning/comments/43zia8/microsoft_taps_ai_with_swiftkey_keyboard/,nyike,1454500798,,4,2,False,http://b.thumbs.redditmedia.com/Bmjk5SRTLkezw0yHnOBdwq_ytNzB3zbnt_JiwhotCAI.jpg,,,,,
75,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,22,43zvlb,journal.frontiersin.org,A Taxonomy of Deep Convolutional Neural Nets for Computer Vision,https://www.reddit.com/r/MachineLearning/comments/43zvlb/a_taxonomy_of_deep_convolutional_neural_nets_for/,suurajsrinivas,1454507639,,0,2,False,default,,,,,
76,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,23,43zx2q,youtube.com,Celebrating the Man Behind the Bayesian Theorem,https://www.reddit.com/r/MachineLearning/comments/43zx2q/celebrating_the_man_behind_the_bayesian_theorem/,virtualjd2015,1454508287,,1,13,False,http://a.thumbs.redditmedia.com/qnJftZeCQCcsfrMawXfkdn2iTBEMSwMBbQQ_qMIHWG8.jpg,,,,,
77,MachineLearning,t5_2r3gv,2016-2-3,2016,2,3,23,4401j9,github.com,I'm starting up a project to benchmark scikit-learn's machine learning models on a ton of data sets/params. Please feel free to suggest/contribute data sets via PRs and issues.,https://www.reddit.com/r/MachineLearning/comments/4401j9/im_starting_up_a_project_to_benchmark/,rhiever,1454510138,,33,67,False,http://b.thumbs.redditmedia.com/933AkAuK1GO_z4B307htii59Bcwo95Revncf068Pg9c.jpg,,,,,
78,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,2,440uit,youtube.com,Live - DeepHack Q&amp;A with Tomas Mikolov - The Roadmap towards Machine Intelligence,https://www.reddit.com/r/MachineLearning/comments/440uit/live_deephack_qa_with_tomas_mikolov_the_roadmap/,quirm,1454520563,,0,18,False,default,,,,,
79,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,2,440zdf,nextplatform.com,How Technical Debt Could Leave First Large Machine Learning Systems Bankrupt,https://www.reddit.com/r/MachineLearning/comments/440zdf/how_technical_debt_could_leave_first_large/,[deleted],1454522257,[deleted],1,2,False,default,,,,,
80,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,3,441125,self.MachineLearning,Highway networks are to deep residual networks what LSTMs are to ...?,https://www.reddit.com/r/MachineLearning/comments/441125/highway_networks_are_to_deep_residual_networks/,vintermann,1454522858,"Highway networks were inspired by LSTMs, according to [the paper describing them](http://arxiv.org/abs/1505.00387). They have a gated connection in the depth dimension analogous to the gated connection LSTMs have in the time dimension. There are also [Grid LSTMs](http://arxiv.org/abs/1507.01526), which have these gated connections in both dimensions.
 
Now the new deep residual networks have proven themselves by winning ILSVRC2015. But people are comparing deep residual networks to highway networks, saying it's like highway networks without the gate, with the path always open. It's surprising to me that an approach that is so much simpler and easier to understand (I still don't entirely understand the gating stuff in LSTMs!) can do so well.
 
Now my question is, can you do that in the temporal dimension too? What's such a network called? Has anyone tried it yet?",7,20,False,self,,,,,
81,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,3,4411vp,blog.kaggle.com,Introducing Kaggle Datasets,https://www.reddit.com/r/MachineLearning/comments/4411vp/introducing_kaggle_datasets/,benhamner,1454523151,,0,2,False,http://a.thumbs.redditmedia.com/oJsDsowuUIqrY3vDmU8xhL4USPPp40XO0YY5pDI-qW8.jpg,,,,,
82,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,3,4413mv,xcorr.net,How Deepmind mastered Go: a technical introduction,https://www.reddit.com/r/MachineLearning/comments/4413mv/how_deepmind_mastered_go_a_technical_introduction/,pmineault,1454523773,,1,18,False,http://b.thumbs.redditmedia.com/9DJfHy1s_DRXnmxZuofFAXd5Cnk3I5dBEbJ45LSV5oI.jpg,,,,,
83,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,3,4414zi,self.MachineLearning,RNN Attenton-based models vs Convnets,https://www.reddit.com/r/MachineLearning/comments/4414zi/rnn_attentonbased_models_vs_convnets/,[deleted],1454524261,[deleted],4,6,False,default,,,,,
84,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,3,44153y,self.MachineLearning,what's the most spectacular disruptions that advances in machine learning might realistically cause,https://www.reddit.com/r/MachineLearning/comments/44153y/whats_the_most_spectacular_disruptions_that/,textClassy,1454524306,"the most spectacular one I can think of is I can imagine it might no longer require things like user addresses or email ids to resolve identity in many cases which would probably change a lot of things, but I don't really understand machine learning enough to have a strong belief. EDIT: I mean in the immediate future such that we might reasonably predict it today ... not including more advanced things like machines doing AI research.",4,0,False,self,,,,,
85,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,3,4418dw,college-de-france.fr,If you live in Paris: you can go to Yann Lecunn deep learning class at the College de France,https://www.reddit.com/r/MachineLearning/comments/4418dw/if_you_live_in_paris_you_can_go_to_yann_lecunn/,JustFinishedBSG,1454525430,,26,27,False,http://b.thumbs.redditmedia.com/IWShZttS-kkLUFghAHtUdpuRjKLuWXHSf-oZB8WSk5E.jpg,,,,,
86,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,4,441dyv,blog.claymcleod.io,Bayes Theorem for Computer Scientists,https://www.reddit.com/r/MachineLearning/comments/441dyv/bayes_theorem_for_computer_scientists/,clmcl,1454527411,,1,19,False,http://b.thumbs.redditmedia.com/6EMZ3gb_eJgurRuz1BNLAvlJR6y31XeIk5h3LCYDWZY.jpg,,,,,
87,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,5,441kfj,github.com,Deep Residual Networks pretrained models (ILSVRC 2015 winners),https://www.reddit.com/r/MachineLearning/comments/441kfj/deep_residual_networks_pretrained_models_ilsvrc/,modeless,1454529730,,2,36,False,http://b.thumbs.redditmedia.com/nSjzbsdIpzyAWt7FJ-r6mtpLq5kP9Kzl-hxVShfdILk.jpg,,,,,
88,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,6,441yb8,self.MachineLearning,Recommendations for good caffe video tutorials?,https://www.reddit.com/r/MachineLearning/comments/441yb8/recommendations_for_good_caffe_video_tutorials/,anonDogeLover,1454534532,,2,7,False,self,,,,,
89,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,6,4424g3,csgohot24.com,HAHAH FUNNY!HAHAH FUNNY!HAHAH FUNNY!HAHAH FUNNY! i,https://www.reddit.com/r/MachineLearning/comments/4424g3/hahah_funnyhahah_funnyhahah_funnyhahah_funny_i/,guytratofpar,1454536664,,0,1,False,default,,,,,
90,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,8,442l1s,edx.org,"PSA: Excellent Probability course from MIT just started on edX. It exactly mirrors 6.041, Probabilistic Systems Analysis &amp; Applied Probability on MITs OCW",https://www.reddit.com/r/MachineLearning/comments/442l1s/psa_excellent_probability_course_from_mit_just/,michael_j_ward,1454542824,,28,118,False,http://b.thumbs.redditmedia.com/HNKnjPCe_o_S643s_lDvlICNVDNKxATrrtMpX-kMGNg.jpg,,,,,
91,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,9,442tiy,self.MachineLearning,Word2Vec and vector origin,https://www.reddit.com/r/MachineLearning/comments/442tiy/word2vec_and_vector_origin/,vega455,1454546037," So I read the two papers by Mikolov et al on Word2Vec (see [here](http://arxiv.org/pdf/1301.3781v3.pdf) and [here](http://arxiv.org/pdf/1310.4546.pdf)).

I understand the concept of word vectors and how they represent meaning. However, I don't understand where the final word vector comes from when training a neural network. The inputs are one-hot encodings of words, which try to predict a one-hot encoding of another word. So how do you get the final n-dimensional word vectors?",8,12,False,self,,,,,
92,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,12,443jeb,quora.com,"Andrew Ng on ""What do you think of Deep Learning?""",https://www.reddit.com/r/MachineLearning/comments/443jeb/andrew_ng_on_what_do_you_think_of_deep_learning/,[deleted],1454556524,[deleted],0,1,False,default,,,,,
93,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,12,443jj0,quora.com,"Andrew Ng's answer to ""What do you think of Deep Learning?""",https://www.reddit.com/r/MachineLearning/comments/443jj0/andrew_ngs_answer_to_what_do_you_think_of_deep/,[deleted],1454556581,[deleted],0,1,False,default,,,,,
94,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,14,443y8h,fdjohnson.v2.webself.net,What are the Different Types of Lubrication Pumps? Check out here!,https://www.reddit.com/r/MachineLearning/comments/443y8h/what_are_the_different_types_of_lubrication_pumps/,jackerfrinandis,1454563066,,0,1,False,default,,,,,
95,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,15,4443pt,self.MachineLearning,Why is the activation derivative needed in the weight update rule of the back propagation algorithm?,https://www.reddit.com/r/MachineLearning/comments/4443pt/why_is_the_activation_derivative_needed_in_the/,goingoout,1454565887,"I'm hoping someone here might have the answer to this. Unfortunately I can find any answer online.

In the back propagation algorithm to calculate the weight update delta we find the node error then multiple by the activation derivative. For a node in the output layer this would look like:

errorDelta = (target  output) * f'(input)

What I don't understand is why the f'(input) bit is needed. I ran a test where I changed the derivative for a constant in the range of what the activation derivative would have been and the network still trains - just a bit slower.

Also, if the derivative is a measurement of the rate of change of the activation function with respect to the input, then why would we want higher values for positions of the activation function which change quickly? Surely for these areas it's better we adjust the networks weights slower as small changes can have a large impact on the output?",3,2,False,self,,,,,
96,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,16,444bo2,self.MachineLearning,General methods for hidden layer fusion?,https://www.reddit.com/r/MachineLearning/comments/444bo2/general_methods_for_hidden_layer_fusion/,ihsgnef,1454572079,"In approaching multi-modal problems, we often use the hidden layers of multiple networks (vision + text for example) together. In some papers I see people using pretrained networks, for example using a trained VGG network for image feature extraction in a visual qa model.

Are there problems with this direct approach? Is there any research on doing this more properly? References &amp; experiences are all appreciated.",0,1,False,self,,,,,
97,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,17,444d8f,self.MachineLearning,What is in your opinion the best amazon book (or another resource) to learn basics about reinforcement learning?,https://www.reddit.com/r/MachineLearning/comments/444d8f/what_is_in_your_opinion_the_best_amazon_book_or/,zibenmoka,1454573149,"Hi, 

I would like to catch up a bit and learn basics about reinforcement learning. where to start? ",4,2,False,self,,,,,
98,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,17,444fe2,gitxiv.com,Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects [GitXiv],https://www.reddit.com/r/MachineLearning/comments/444fe2/are_elephants_bigger_than_butterflies_reasoning/,Bardelaz,1454574564,,0,3,False,default,,,,,
99,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,17,444fxq,self.MachineLearning,Did anyone download the deepmind go pdf?,https://www.reddit.com/r/MachineLearning/comments/444fxq/did_anyone_download_the_deepmind_go_pdf/,WookieTeabag,1454574900,"It looks like the original link is dead now, and I can't seem to find it anywhere.  I'd love to get a copy if anyone has it.",6,1,False,self,,,,,
100,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,18,444kwl,self.MachineLearning,MXNet in Production?,https://www.reddit.com/r/MachineLearning/comments/444kwl/mxnet_in_production/,Bardelaz,1454578288,MXNet seems to be very fast and suitable for production deployment (server scenario). Anyone care to share experience and gotchas? Are there any big players using it?,2,19,False,self,,,,,
101,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,19,444qsd,self.MachineLearning,"Hi! I'm working on a platform that allows anyone to build deep learning applications, and I'd love your feedback",https://www.reddit.com/r/MachineLearning/comments/444qsd/hi_im_working_on_a_platform_that_allows_anyone_to/,[deleted],1454582134,[removed],0,1,False,default,,,,,
102,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,22,44578p,self.MachineLearning,Noob Question : Is it possible to recognise the photo the exact same object from just 2-3 training samples ?,https://www.reddit.com/r/MachineLearning/comments/44578p/noob_question_is_it_possible_to_recognise_the/,[deleted],1454591828,[deleted],0,0,False,default,,,,,
103,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,22,445aw7,self.MachineLearning,Tackling Imbalanced dataset Deep Learning,https://www.reddit.com/r/MachineLearning/comments/445aw7/tackling_imbalanced_dataset_deep_learning/,[deleted],1454593551,[deleted],7,15,False,default,,,,,
104,MachineLearning,t5_2r3gv,2016-2-4,2016,2,4,23,445h68,stanfy.com,Machine Learning In Mobile Devices: What To Expect,https://www.reddit.com/r/MachineLearning/comments/445h68/machine_learning_in_mobile_devices_what_to_expect/,alexstanfy,1454596267,,0,1,False,default,,,,,
105,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,0,445qv1,samuispirit.com,BEST GIRLS FOR !SEX! a,https://www.reddit.com/r/MachineLearning/comments/445qv1/best_girls_for_sex_a/,treepgetoughter,1454600043,,0,1,False,default,,,,,
106,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,0,445s8o,self.MachineLearning,I've been working on a platform that allows anyone to start building deep learning applications. I'd love to hear your feedback!,https://www.reddit.com/r/MachineLearning/comments/445s8o/ive_been_working_on_a_platform_that_allows_anyone/,[deleted],1454600552,[deleted],3,7,False,default,,,,,
107,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,0,445skt,github.com,Awesome RNN,https://www.reddit.com/r/MachineLearning/comments/445skt/awesome_rnn/,merrly3,1454600682,,7,85,False,http://b.thumbs.redditmedia.com/arkaAK24OdaZudFS4whRl90hcC6IQXcfCk5VL3ddToU.jpg,,,,,
108,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,2,446fcw,self.MachineLearning,Structure for ML partner system to an application,https://www.reddit.com/r/MachineLearning/comments/446fcw/structure_for_ml_partner_system_to_an_application/,[deleted],1454608589,[deleted],6,3,False,default,,,,,
109,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,3,446kfk,self.MachineLearning,Looking for a CNN implementation for 3D images,https://www.reddit.com/r/MachineLearning/comments/446kfk/looking_for_a_cnn_implementation_for_3d_images/,manu2811,1454610331,"I'm looking for an implementation in python (or eventually matlab) of Convolutional Neural Networks for 3D images. By 3D I mean 3 spatial dimensions (i.e. not 2D+channels or 2D+time), so it should have 3D convolution and 3D max-pooling layers. Any advice?",6,2,False,self,,,,,
110,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,3,446pl1,self.MachineLearning,Looking for a networking + ML interesting project,https://www.reddit.com/r/MachineLearning/comments/446pl1/looking_for_a_networking_ml_interesting_project/,rgbimbochamp,1454612113,"I have immense interest in the networking domain such as protocols , cyber security , etc . Recently , I have started studying ML and quite frankly its looks wonderful from here on . 
There are tonnes of ideas I have about projects in ML albeit I want to   combine my interest in networking with ML to do an intermediate level project . No ideas so far !",2,4,False,self,,,,,
111,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,4,446usx,nextplatform.com,"""Technical Debt"" and the Software Engineering challenges of ""Deploying and Using Machine Learning Systems in Production""",https://www.reddit.com/r/MachineLearning/comments/446usx/technical_debt_and_the_software_engineering/,michael_j_ward,1454613900,,2,0,False,http://b.thumbs.redditmedia.com/9OKO6vy5pV-gfQfLxIHCWl7jtWGSP6_RMJc8MRXotyU.jpg,,,,,
112,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,4,446wi3,github.com,Deep Residual Networks in Torch from Facebook (+pretrained models),https://www.reddit.com/r/MachineLearning/comments/446wi3/deep_residual_networks_in_torch_from_facebook/,olBaa,1454614489,,1,34,False,http://b.thumbs.redditmedia.com/0EqOerlPPeYB8ZO88AgJjTAErf6SuJHkLsvJNIXDgjQ.jpg,,,,,
113,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,4,446wt9,self.MachineLearning,Is there a mathematically rigorous treatment of Adadelta?,https://www.reddit.com/r/MachineLearning/comments/446wt9/is_there_a_mathematically_rigorous_treatment_of/,TanktopModul,1454614597,"I was wondering if there is a mathematically rigorous treatment of [Adadelta](http://arxiv.org/abs/1212.5701), since the derivation of the algorithm is rather handwavy. Compare that to the paper of [Adagrad](http://www.magicbroom.info/Papers/DuchiHaSi10.pdf), which provides a clear motivation and proofs.
",1,4,False,self,,,,,
114,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,4,446y6j,self.MachineLearning,Tensorflow: RNN Tutorial question,https://www.reddit.com/r/MachineLearning/comments/446y6j/tensorflow_rnn_tutorial_question/,JustSomeAccount456,1454615078,"Hey guys! :)

Does somebody that did dig deeper into tensorflow then I did know how I can query the model that is created in the Tutorial after it is trained?
Example code trains it, but to use it I would need a way to A) save and B) query it.

Is anyone aware of a more usable version of this code?

Btw: I am talking about this model ( https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py ) that is talked about in this tutorial ( https://www.tensorflow.org/versions/0.6.0/tutorials/recurrent/index.html )


Thanks in advance!",0,3,False,self,,,,,
115,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,5,4470zm,torch.ch,Torch | Training and investigating Residual Nets,https://www.reddit.com/r/MachineLearning/comments/4470zm/torch_training_and_investigating_residual_nets/,HillbillyBoy,1454616051,,3,15,False,default,,,,,
116,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,5,4474gf,wired.com,AI Is Transforming Google Search. The Rest of the Web Is Next,https://www.reddit.com/r/MachineLearning/comments/4474gf/ai_is_transforming_google_search_the_rest_of_the/,vonnik,1454617229,,6,0,False,http://a.thumbs.redditmedia.com/eRmHb9awpP9SjPGFupXgTe8N0LChbb8DZ1FcUF3wsP0.jpg,,,,,
117,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,6,447lka,toptal.com,Ensemble Methods: Elegant Techniques to Produce Improved Machine Learning Results,https://www.reddit.com/r/MachineLearning/comments/447lka/ensemble_methods_elegant_techniques_to_produce/,ndemir,1454623073,,0,7,False,http://b.thumbs.redditmedia.com/TfH_ggyM-1k6p-OEy-Z4q7CdacH2GY6dFcK5sEl_qTU.jpg,,,,,
118,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,8,4482lm,self.MachineLearning,Using R to analyze FB ads?,https://www.reddit.com/r/MachineLearning/comments/4482lm/using_r_to_analyze_fb_ads/,kailovesdata,1454629580,"Has anyone had experience with analyzing FB ads on R?

I am a technical marketer and I believe data scientist should be the new marketers. However, I couldn't find any good tutorials on this area. Does anyone know of some? or maybe can help me get the ball rolled?

I am a new college graduate majored in Mechanical Engineering, and I have so much passion for analyzing data. Please help me. I will send whoever helps me hand-written Christmas post cards! :P",0,0,False,self,,,,,
119,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,10,448kl2,self.MachineLearning,"As far as I know, there are no training algorithms known that will train a threshold network/""perceptron"". Is this still true, and has threshold-forward-sigmoid-backprop been tried?",https://www.reddit.com/r/MachineLearning/comments/448kl2/as_far_as_i_know_there_are_no_training_algorithms/,lahwran_,1454636612,"In other words, does anyone know of a paper that uses `sign(x)` as the forward pass activation function, but pretends it used sigmoid on the backward prop pass?

If so, does anyone have a link to a paper that tries it? I imagine it'll be old and have disappointing performance. However, I'm still curious, because I suspect that asymmetric back/forward passes would be helpful for creating ""firm"" attention; I'd expect them to perform worse than soft attention, but better than RL attention, and do so in much less time than soft attention. If something like this has kind-of-worked before, I'd be interested in seeing the research that found this.",7,2,False,self,,,,,
120,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,11,448tp1,unnati.xyz,"We are Unnati,a data science startup conducting a series of workshops in Bangalore. The first one is : Introduction to Data Science using Python. This workshop is designed to help people kick start their careers in Data Science. If you in and around Bangalore, you should definitely drop by.",https://www.reddit.com/r/MachineLearning/comments/448tp1/we_are_unnatia_data_science_startup_conducting_a/,raghothams,1454640403,,0,0,False,http://b.thumbs.redditmedia.com/9gYMMsYdrF5Ap7nbEBEOxm-pugdbhOxXF4XcNNk_ZVc.jpg,,,,,
121,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,15,449l26,self.MachineLearning,"large text dataset, how to generate feature ideas",https://www.reddit.com/r/MachineLearning/comments/449l26/large_text_dataset_how_to_generate_feature_ideas/,textClassy,1454652712,"is there any automated way to generate feature ideas? for example using unsupervised learning possibly? I've basically tapped out all the features I can think of, but I suspect there are many more because the topic modelling/word2vec things I've been doing are only giving me very small accuracy gains and I think there's more structure here that is untapped.",0,0,False,self,,,,,
122,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,15,449plm,github.com,Deep Residual Networks in TensorFlow/skflow (WIP),https://www.reddit.com/r/MachineLearning/comments/449plm/deep_residual_networks_in_tensorflowskflow_wip/,terrytangyuan,1454655141,,3,25,False,http://b.thumbs.redditmedia.com/DTIc8Re5Wtr8FcYpadKuxbnQqiZfVeNkvw3yGFWnZHM.jpg,,,,,
123,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,16,449ubq,technologyreview.com,Will AI-Powered Hedge Funds Outsmart the Market?,https://www.reddit.com/r/MachineLearning/comments/449ubq/will_aipowered_hedge_funds_outsmart_the_market/,GeraldBaker10,1454657957,,28,16,False,http://b.thumbs.redditmedia.com/oD9XoKc2YPnDVVO81bMcTukEK6zyLZxU7DmKM-Ob8zg.jpg,,,,,
124,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,17,449xe8,self.MachineLearning,Cloning Model for training across multiple systems?,https://www.reddit.com/r/MachineLearning/comments/449xe8/cloning_model_for_training_across_multiple_systems/,JoshAlex,1454659885,"Hi All,

I'm reasonably new to deep learning and have what may be a very dumb or obvious question but here goes. This is in regards to a Fully Connected DNN for starters.

To scale training across multiple systems, it should be easy to load a network model on say 10 systems each with identical starting weights/biases. Now let's say I decide to run images through in batch sizes of 1000. I split these up and run 100 through per system which should all finish up at the same time. I can then calculate all 1000 deltas across the 10 systems, again nothing fancy here. Now for the next step can I then sum the deltas from all 1000 samples collectively, and then use that data to update the weights/biases on all 10 systems? They'd all be updated the same amount so thus should all have identical copies of the updated model. Then repeat for the next batch and so on.

Anyhow I'm wondering if I'm missing something about backprop making this unfeasible, or if the batch sizes required to be efficient due to the network overhead would be too large to be effective, or if this is already how they implement this sort of thing on say Multiple GPU systems.

Thanks in advance for any insight.
",1,0,False,self,,,,,
125,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,17,44a0rr,venturebeat.com,YouTube will livestream Google's AI playing Go superstar Lee Sedol in March. This is gonna be EPIC AI vs Human,https://www.reddit.com/r/MachineLearning/comments/44a0rr/youtube_will_livestream_googles_ai_playing_go/,swentso,1454662065,,17,5,False,http://b.thumbs.redditmedia.com/DF-PrNLVIT2sEIHBiOYAAuPLypODGGzJxU4Gb82kDGA.jpg,,,,,
126,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,17,44a0ww,self.MachineLearning,"UsMeU IOS app uses IBM Watson's ""Personality Insights"" service to do a psychological analysis of twitter people",https://www.reddit.com/r/MachineLearning/comments/44a0ww/usmeu_ios_app_uses_ibm_watsons_personality/,usmeu,1454662154,[removed],0,1,False,default,,,,,
127,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,18,44a67n,allkrym.com,*I am Nika! I neeed a sssex!* s,https://www.reddit.com/r/MachineLearning/comments/44a67n/i_am_nika_i_neeed_a_sssex_s/,lucahydni,1454665536,,0,1,False,default,,,,,
128,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,18,44a72t,self.MachineLearning,"[ICML 2016] [META] What makes a good paper, and submission in Deep Learning",https://www.reddit.com/r/MachineLearning/comments/44a72t/icml_2016_meta_what_makes_a_good_paper_and/,ICML2016,1454666101,"Hi All,

I was curious as to what the reviewers see in a Deep Learning or any paper in general?

Is SOTA setting a requirement. Can we put in partially baked ideas with some results?

Please share your experiences, as it will be helpful for a lot of people",18,16,False,self,,,,,
129,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,19,44a7mk,arxiv.org,"Article: Spatial Transformer Networks - learning with invariance to scale, rotation and translation",https://www.reddit.com/r/MachineLearning/comments/44a7mk/article_spatial_transformer_networks_learning/,boccaff,1454666454,,5,10,False,default,,,,,
130,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,21,44apg2,getlittlebird.com,Who do the global elite in Artificial Intelligence follow on Twitter?,https://www.reddit.com/r/MachineLearning/comments/44apg2/who_do_the_global_elite_in_artificial/,rfurlan,1454677042,,0,0,False,http://a.thumbs.redditmedia.com/oIHq8tL7QNponGcK1BEgq_uoEc337UkQ0Tmx9o_LDg0.jpg,,,,,
131,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,22,44aqpe,self.MachineLearning,Cool applications of ML in biotech labs: What examples do you know?,https://www.reddit.com/r/MachineLearning/comments/44aqpe/cool_applications_of_ml_in_biotech_labs_what/,AleksanderRousing,1454677650,"Hi friends!

What applications of integrating ML into the workflow of biotech labs do you know of?

I've been invited to create an ML project in a biotech lab and it would be very cool if you can help create a list of similar projects.

The lab in question is optimizing a eukaryotic cell for production of a particular molecule, and pretty much any collectible data will be available to learn from. The best algorithms would help them choose what experiments to run in the lab.

Thanks redditors!",0,1,False,self,,,,,
132,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,22,44arz9,self.MachineLearning,Conditional Random Fields - rookie question,https://www.reddit.com/r/MachineLearning/comments/44arz9/conditional_random_fields_rookie_question/,justafleetingmoment,1454678346,"I am currently working on a multilabel sequence prediction problem. There are a few domain knowledge rules that one can apply to the output which is not well captured in the HMM model I am currently using. I'd like to implement these rules probabilistically and from my reading the best way to do this is by using  a CRF. Rules that one can include would include things the percentage of the output sequence being of a specific label or the maximum length of a single-label sub-sequence based on the position of the sub-sequence.

My question is how one would implement something like this in Python. Most of the packages I have come across only focus on CRF's that can implement joint-features on any combination of the input features, but I'm not sure how one would add features based on the output? Or maybe a CRF is not the tool for the job?",5,11,False,self,,,,,
133,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,22,44atkf,thestack.com,Knowm's new memristor improves efficient AI processors,https://www.reddit.com/r/MachineLearning/comments/44atkf/knowms_new_memristor_improves_efficient_ai/,InaneMembrane,1454679131,,2,9,False,http://a.thumbs.redditmedia.com/ZTn9z7liQaRcFayj1TIW71CDjD0R8aQiiO99W83unH0.jpg,,,,,
134,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,22,44aunt,self.MachineLearning,Regression with GaussianProcess from sklearn,https://www.reddit.com/r/MachineLearning/comments/44aunt/regression_with_gaussianprocess_from_sklearn/,[deleted],1454679615,[deleted],0,1,False,default,,,,,
135,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,22,44ax13,self.MachineLearning,sentence/thought vs word embedding performance boost,https://www.reddit.com/r/MachineLearning/comments/44ax13/sentencethought_vs_word_embedding_performance/,textClassy,1454680669,how much additional performance can I expect from embedding vectors for entire passages rather than simply averaging its word vectors?,8,6,False,self,,,,,
136,MachineLearning,t5_2r3gv,2016-2-5,2016,2,5,23,44azs8,self.MachineLearning,Learning curve dropout,https://www.reddit.com/r/MachineLearning/comments/44azs8/learning_curve_dropout/,MarcoROG-SG,1454681876,"I noticed that using dropout gives unusually high training error(even higher than validation error if dropout rate is quite high).
This makes sense since during training only a portion of the network is used to make predictions.
However I find myself in need to study whether my network is affected by high bias or high variance.
I think it's high variance since the training error ends being much lower than the validation one in the last iterations of training. I tried using an higher dropout rate (0.5) but without a significant improvement.
I was considering using more data( data transformations for instance), one way to see if more data could help is to plot learning curves (training and validation error plotted against the number of training samples used, with a fixed size validation set) however the use of dropout during training ruins the training loss measurements and the plot doesn't help much (the training loss results higher than validation as I said).
So my questions are: how to correct the training loss to take into account dropout? Is there any way to see if I could benefit using more data? How to properly tune dropout rates?",11,4,False,self,,,,,
137,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,2,44bpze,self.MachineLearning,Deepmind navigates a 3D maze using Asynchronous Methods for Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/44bpze/deepmind_navigates_a_3d_maze_using_asynchronous/,rounderror,1454691969,"Article with videos:
https://www.newscientist.com/article/2076552-google-deepmind-ai-navigates-a-doom-like-3d-maze-just-by-looking/

Arxiv Paper.
http://arxiv.org/abs/1602.01783
",28,60,False,self,,,,,
138,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,2,44bvfz,urbanspatialanalysis.com,Predicting building demolitions to prioritize inspections,https://www.reddit.com/r/MachineLearning/comments/44bvfz/predicting_building_demolitions_to_prioritize/,proxyformyrealname,1454693876,,0,1,False,default,,,,,
139,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,2,44bxdj,self.MachineLearning,SCRN vs LSTM,https://www.reddit.com/r/MachineLearning/comments/44bxdj/scrn_vs_lstm/,asymptotics,1454694544,"Someone recently linked a talk on SCRN, which seems much easier to train (and faster!) but that gets comparable perplexity to LSTM. Is there any sense that a scaled-up version of SCRN will be able to replace LSTM?",20,36,False,self,,,,,
140,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,4,44cekl,research.microsoft.com,Google's Jeff Dean Tensorflow Tutorial: Large-Scale Distributed Systems for Training Neural Networks,https://www.reddit.com/r/MachineLearning/comments/44cekl/googles_jeff_dean_tensorflow_tutorial_largescale/,[deleted],1454700416,[deleted],0,1,False,default,,,,,
141,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,4,44cgdw,research.microsoft.com,[video] Google's Jeff Dean TensorFlow Tutorial: Large-Scale Distributed Systems for Training Neural Networks,https://www.reddit.com/r/MachineLearning/comments/44cgdw/video_googles_jeff_dean_tensorflow_tutorial/,fhoffa,1454701066,,0,9,False,default,,,,,
142,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,7,44da58,self.MachineLearning,Noob text classification question,https://www.reddit.com/r/MachineLearning/comments/44da58/noob_text_classification_question/,dwaxe,1454711814,"I have a text: score data set that I want to use to create a predictive model of what kinds of text will have high scores. Text is sentences and scores are exponentially distributed. I'm a total machine learning noob, how should I go about creating such a model? Are there any classifiers that work on exponentially distributed scores? I thought of discretizing the scores into high and low, but only about 1% of the scores are high. Are there any classifiers that work on sparse data like this?",2,1,False,self,,,,,
143,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,7,44dd4s,github.com,"Deep Residual Network implementation in neon, (cifar10, 56-layer network, 30s per epoch)",https://www.reddit.com/r/MachineLearning/comments/44dd4s/deep_residual_network_implementation_in_neon/,meepmeepmoopmoop,1454712937,,5,8,False,http://b.thumbs.redditmedia.com/-ePzRpWZiUrTzY4v3aMLbOffuaO95y5WtrGrKHR1lIw.jpg,,,,,
144,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,8,44dfyz,self.MachineLearning,"Seeking your recommendation on good books to learn the machine learning, data mining, and statistical learning!",https://www.reddit.com/r/MachineLearning/comments/44dfyz/seeking_your_recommendation_on_good_books_to/,MicrobeWorldHere,1454714046,"Dear all,
I am an undergraduate pursuing the double majors in mathematics and microbiology. I recently got a project in the computational biology where I need to develop the machine learning algorithms to construct the probabilistic network of genes and proteins, and elucidate the hidden regulators. I am going to use R for the programming.
I would like your recommendations on some good books to learn the statistical learning, machine learning, and data mining. Could you recommend two books (one on theoretical aspect and another one on practical/programming aspect) per each subject?
My background: real analysis, topology, axiomatic set theory, abstract linear algebra, calculus, and basic applied statistics. I am not familiar with the mathematical statistics and probability, but I am willing to learn them as I progress through the books on the learning theory.",6,2,False,self,,,,,
145,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,8,44dmfe,alexdonaldsonmusings.wordpress.com,Google DeepMinds champion Go AI: A Sign of Growing AI Complexity?,https://www.reddit.com/r/MachineLearning/comments/44dmfe/google_deepminds_champion_go_ai_a_sign_of_growing/,Age_Ark_Vim,1454716392,,14,14,False,http://b.thumbs.redditmedia.com/MET9X-QRazoRXghSejwgCmQBCyFQEJeFHqiByxOoWpo.jpg,,,,,
146,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,10,44e0xd,self.MachineLearning,LSTM peephole implementation.,https://www.reddit.com/r/MachineLearning/comments/44e0xd/lstm_peephole_implementation/,coskunh,1454722261,"I have been reading papers about LSTM and checking its implementations. There is one point that not clear for me. 
At most of paper it is mentioning that the weight matrices from the cell to gate vectors should be diagonal(ex: [Alex](http://arxiv.org/pdf/1308.0850v5.pdf) page 5, 2013), but i haven't seen this at any implementation. 
For example this :
[1](http://christianherta.de/lehre/dataScience/machineLearning/neuralNetworks/LSTM.php)
[2](https://github.com/jhb86253817/lstmlm/blob/master/code/lstm_ada_peep.py)
Another example is from mila lab.
[3](https://github.com/mila-udem/blocks/blob/83c5d2a853f8057be03b910ac0f0e601a59a01a9/blocks/bricks/recurrent.py#L334)

Are these people implementing wrong or am I missing something? ",5,6,False,self,,,,,
147,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,11,44e82q,quora.com,Kevin Murphy QA on Quora,https://www.reddit.com/r/MachineLearning/comments/44e82q/kevin_murphy_qa_on_quora/,nmjohn,1454725459,,0,10,False,default,,,,,
148,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,11,44ead9,self.MachineLearning,how significant is the cognitive overload when you are a deep learning researcher?,https://www.reddit.com/r/MachineLearning/comments/44ead9/how_significant_is_the_cognitive_overload_when/,textClassy,1454726433,"Do even the most seasoned and intelligent deep learning researchers find themselves overwhelmed? Or does everything become much more manageable with years of training and testing concepts, and high natural ability?",3,4,False,self,,,,,
149,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,13,44elbp,reddit.com,Fantastic explanation of the LSTM architecture in this buried comment,https://www.reddit.com/r/MachineLearning/comments/44elbp/fantastic_explanation_of_the_lstm_architecture_in/,[deleted],1454731323,[deleted],8,73,False,default,,,,,
150,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,14,44eyu6,self.MachineLearning,Any advancements in meta learning?,https://www.reddit.com/r/MachineLearning/comments/44eyu6/any_advancements_in_meta_learning/,thenerdstation,1454737746,Have there been any advancements in meta learning recently? Meta learning as in a machine learning program learning how to write a better machine learning program?,6,9,False,self,,,,,
151,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,15,44f3do,self.MachineLearning,is there research into using deep learning as a thinking tool to help see arbitrary connections?,https://www.reddit.com/r/MachineLearning/comments/44f3do/is_there_research_into_using_deep_learning_as_a/,[deleted],1454740247,[deleted],0,1,False,default,,,,,
152,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,15,44f4qg,self.MachineLearning,is there any high quality research into using deep learning for cognitive augmentation,https://www.reddit.com/r/MachineLearning/comments/44f4qg/is_there_any_high_quality_research_into_using/,textClassy,1454740974,"for example, helping people understand equations faster, things like that ... I know its a bit general and sci fi ish ... if the inspiration for deep neural networks is to some extent human neural networks, it might make sense to try to leverage some artificial neurons to do the work of your own brain on cognitive tasks that you would be doing anyway like researching things etc. ",4,0,False,self,,,,,
153,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,20,44fs73,self.MachineLearning,Why and when would you choose to publish ML research independently?,https://www.reddit.com/r/MachineLearning/comments/44fs73/why_and_when_would_you_choose_to_publish_ml/,syncoPete,1454756473,"The big universities and corporate donors to ML conferences seem to have a lot of power over what gets selected for publication in major journals. There may be people submitting authentic, original and significant work, and not getting the recognition they deserve for their work.

Publishing independently does not seem like a bad idea to me, if you believed your work was interesting and significant enough.

Have any major contributions been made via independent publishing? I was under the impression that 'A Neural Algorithm of Artistic Style' was independently published, but it seems I might have been wrong about this.",10,3,False,self,,,,,
154,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,20,44fsv5,forbes.com,What's Google's End Game For Machine Learning And Semantic Search?,https://www.reddit.com/r/MachineLearning/comments/44fsv5/whats_googles_end_game_for_machine_learning_and/,lokator9,1454756917,,0,0,False,http://b.thumbs.redditmedia.com/fvWfS1JE_PeO7268epLfxBMgFJMVIPwxSSrnCHXHBCw.jpg,,,,,
155,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,20,44ftsy,unofficialgoogledatascience.com,Variance and significance in large-scale online services,https://www.reddit.com/r/MachineLearning/comments/44ftsy/variance_and_significance_in_largescale_online/,lokator9,1454757547,,0,0,False,default,,,,,
156,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,21,44g224,self.MachineLearning,A few questions on Hidden Markov Models,https://www.reddit.com/r/MachineLearning/comments/44g224/a_few_questions_on_hidden_markov_models/,[deleted],1454762852,[deleted],0,1,False,default,,,,,
157,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,21,44g2d8,self.MachineLearning,What type of features can I extract from single variable functions?,https://www.reddit.com/r/MachineLearning/comments/44g2d8/what_type_of_features_can_i_extract_from_single/,[deleted],1454763041,[deleted],3,0,False,default,,,,,
158,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,22,44g9tr,arxiv.org,Asynchronous Methods for Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/44g9tr/asynchronous_methods_for_deep_reinforcement/,SuperFX,1454766879,,3,41,False,default,,,,,
159,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,23,44gb8f,youtu.be,"DeepMind plays 3D game, ""DOOM"".",https://www.reddit.com/r/MachineLearning/comments/44gb8f/deepmind_plays_3d_game_doom/,heliophobicdude,1454767538,,8,0,False,http://b.thumbs.redditmedia.com/9P9r6F2ds_0pWhlztNVdIQz1wo-_p1Cllwt4ShMeBos.jpg,,,,,
160,MachineLearning,t5_2r3gv,2016-2-6,2016,2,6,23,44ghh5,self.MachineLearning,What was that youtube channel ...,https://www.reddit.com/r/MachineLearning/comments/44ghh5/what_was_that_youtube_channel/,crayons610,1454770456,"It was machine learning explained really well with hands shuffling little bits of torn paper around and examples in python.
",3,1,False,self,,,,,
161,MachineLearning,t5_2r3gv,2016-2-7,2016,2,7,1,44gvq6,self.MachineLearning,"Pre-processing (center + scale, box-cox transformation) inside cross-validation?",https://www.reddit.com/r/MachineLearning/comments/44gvq6/preprocessing_center_scale_boxcox_transformation/,BlackHawk90,1454776380,"Hello

I have extracted features and I have now a matrix where the rows are the data points and the columns are the features.

Of course, I have to center and scale (zero mean and unit variance) each feature vector and I have to apply a box-cox transformation to some feature vectors due to skewed distributions.

Can I apply this kind of things just to the whole matrix or should I apply it whitin (nested) cross-validation? And if inside cross-validation how do I have to apply it?",5,2,False,self,,,,,
162,MachineLearning,t5_2r3gv,2016-2-7,2016,2,7,3,44hfo5,self.MachineLearning,Hidden Markov Models: problem to solve for Master's thesis,https://www.reddit.com/r/MachineLearning/comments/44hfo5/hidden_markov_models_problem_to_solve_for_masters/,hcl14,1454783606,"This year I'm finishing a Master's degree in the field of mathematics. I want to make some research and write a graduate thesis that (perhaps) could be interesting to my potential employer in the field of machine learning (data analytics, software development, etc) and will give me some really useful experience in ""actual"" tasks. My supervisor, after listening to me, let me propose my own topic in the field of Hidden Markov Models (Previously I was doing courseworks on Markov chains (coupling)).

So, currently I'm studying HMM applications in Natural Language Processing and Process Mining to find if I can do any research there and write my own project (I'm mathematician, so I need to perform some mathematical research or at least adapt an existing mathematical approach).

I'll be very grateful to everyone who will give me any advice or guidelines on the possible problem for me to solve in my thesis (HMM).

Thank you! ",19,7,False,self,,,,,
163,MachineLearning,t5_2r3gv,2016-2-7,2016,2,7,3,44hhx8,youtube.com,"What Is Winning on Kaggle?, Founder &amp; CEO of Kaggle",https://www.reddit.com/r/MachineLearning/comments/44hhx8/what_is_winning_on_kaggle_founder_ceo_of_kaggle/,TDaltonC,1454784382,,34,77,False,http://b.thumbs.redditmedia.com/XppzWKl3PQQ87qB1LnYbsIZPgvNfy63-_Je8l7Rbyug.jpg,,,,,
164,MachineLearning,t5_2r3gv,2016-2-7,2016,2,7,4,44hkz8,self.MachineLearning,Didn't get any responses from r/askcomputerscience -- how dependent would a ML model of our universe be on our perspective of the universe?,https://www.reddit.com/r/MachineLearning/comments/44hkz8/didnt_get_any_responses_from_raskcomputerscience/,aweeeezy,1454785469,"I had the thought the other day that went something like this...

Assume we are able to fold space-time and make great leaps across our local cluster or even super cluster. Assume that any such leap is a two-way trip (so that we could retrieve the information gathered from it). If we train a machine learning model to identify constellations, galaxies, or other celestial bodies and then warp a camera w/ computer to an arbitrary point in the universe and back, would we be able to infer where that location was without calculating it beforehand?

I'm thinking that if the model has no features for the depth of objects of interest in a given image of space, then the model would be hopeless at inferring a relative location to the training location. However, if each star or galaxy had a metric for determining the distance in light years from the origin of the training image, then there would be enough information to deduce the location of the warped camera/computer (assuming a sufficient amount of data is the same in both training and test scenarios).

If you agree that this is possible, what algorithm might you choose to reproduce this experiment and why?
",2,0,False,self,,,,,
165,MachineLearning,t5_2r3gv,2016-2-7,2016,2,7,4,44hm8u,imegalodon.com,"Im Amanda and i want sxxx, i wait for you",https://www.reddit.com/r/MachineLearning/comments/44hm8u/im_amanda_and_i_want_sxxx_i_wait_for_you/,icunen33568,1454785909,,0,1,False,default,,,,,
166,MachineLearning,t5_2r3gv,2016-2-7,2016,2,7,5,44hzaj,self.MachineLearning,How to use SymPy for symbolic maths and optimization,https://www.reddit.com/r/MachineLearning/comments/44hzaj/how_to_use_sympy_for_symbolic_maths_and/,bayeslaw,1454790713,SymPy is a great alternative  of Wolfram Alpha and Mathematica. Here's a blog post I wrote about using SymPy for some simple optimization: http://danielhomola.com/2016/02/06/solving-real-world-problems-with-sympy/,3,2,False,self,,,,,
167,MachineLearning,t5_2r3gv,2016-2-7,2016,2,7,5,44i2iu,hidco.com.tr,REDKTR,https://www.reddit.com/r/MachineLearning/comments/44i2iu/redktr/,reduktorler,1454791827,,0,1,False,default,,,,,
168,MachineLearning,t5_2r3gv,2016-2-7,2016,2,7,6,44i6vb,self.MachineLearning,Looking for Machine Learning Mentor,https://www.reddit.com/r/MachineLearning/comments/44i6vb/looking_for_machine_learning_mentor/,plasticstone,1454793486,"Hello,
I am looking for a person that is willing to go along my journey in the field. I use languages C#, Octave, Python (basics). If you cannot provide coding assistance your skills can be used to explain algorithms / detect misunderstandings and flaws in the thinking and we can share knowledge with pseudo-code.",12,0,False,self,,,,,
169,MachineLearning,t5_2r3gv,2016-2-7,2016,2,7,8,44ioai,crscardellino.me,Spanish Billion Words Corpus and Embeddings,https://www.reddit.com/r/MachineLearning/comments/44ioai/spanish_billion_words_corpus_and_embeddings/,crscardellino,1454800055,,5,33,False,http://b.thumbs.redditmedia.com/TkkbsiL8uOg5jQss1M_fN0wAdOhXHIPFnSNzzBL1-Og.jpg,,,,,
170,MachineLearning,t5_2r3gv,2016-2-7,2016,2,7,9,44iy7u,self.MachineLearning,Differences between Continuous Bag of Words (CBOW) and Skip-Gram?,https://www.reddit.com/r/MachineLearning/comments/44iy7u/differences_between_continuous_bag_of_words_cbow/,sprintletecity,1454803837,"I understand that Skip-Gram is considered the ""inverse"" of CBOW, but how are they different beyond that? Can someone describe the two algorithms? I'm having some difficulty understanding them from the papers on Word2Vec.",3,4,False,self,,,,,
171,MachineLearning,t5_2r3gv,2016-2-7,2016,2,7,10,44jb6t,self.MachineLearning,State of multimodal deep learning?,https://www.reddit.com/r/MachineLearning/comments/44jb6t/state_of_multimodal_deep_learning/,[deleted],1454809035,[deleted],1,1,False,default,,,,,
172,MachineLearning,t5_2r3gv,2016-2-7,2016,2,7,10,44jd6i,arxiv.org,Long Short-Term Memory-Networks for Machine Reading,https://www.reddit.com/r/MachineLearning/comments/44jd6i/long_shortterm_memorynetworks_for_machine_reading/,SuperFX,1454809839,,5,14,False,default,,,,,
173,MachineLearning,t5_2r3gv,2016-2-7,2016,2,7,11,44jgj7,10-meters.com,TH1S IS N0T  DTING S1T. Bs 216 prfiles nmarried age 23 . The base is available nly 1 hur. X,https://www.reddit.com/r/MachineLearning/comments/44jgj7/th1s_is_n0t__dting_s1t_bs_216_prfiles/,w35se74ym26fg,1454811202,,0,1,False,default,,,,,
174,MachineLearning,t5_2r3gv,2016-2-7,2016,2,7,11,44jkr5,4profs.com,TH1S IS N0T  DTING S1T. Bs 216 prfiles nmarried age 23 . The base is available nly 1 hur. kU,https://www.reddit.com/r/MachineLearning/comments/44jkr5/th1s_is_n0t__dting_s1t_bs_216_prfiles/,5z__8ma_4_7r3d,1454812978,,0,1,False,default,,,,,
175,MachineLearning,t5_2r3gv,2016-2-7,2016,2,7,18,44kybb,self.MachineLearning,Study Groups,https://www.reddit.com/r/MachineLearning/comments/44kybb/study_groups/,minato3421,1454838326,I am new to Machine Learning and have been thinking to form a study group of interested people... Please repy if you're interested. Thanks!,4,0,False,self,,,,,
176,MachineLearning,t5_2r3gv,2016-2-7,2016,2,7,21,44ld5c,self.MachineLearning,Interesting papers on learning automatically learning neural network topology,https://www.reddit.com/r/MachineLearning/comments/44ld5c/interesting_papers_on_learning_automatically/,DanielSlater8,1454847061,"I'm doing some research on this at the moment and has compiled a bunch of interesting research papers in this area. I wanted to present them here to anyone else who might be interested and also ask if anyone else knows any that could be added to this collection.

##Growing approaches

[Cascading neural networks](http://papers.nips.cc/paper/207-the-cascade-correlation-learning-architecture.pdf)

[Dynamic node creation in backpropagation netoworks](http://www.cogsci.ucsd.edu/research/documents/technical/TR-8901.pdf)

[Infinite Restricted Boltzmann Machine](http://arxiv.org/abs/1502.02476)

[SELF-INFORMED NEURAL NETWORK
STRUCTURE LEARNING](http://arxiv.org/pdf/1412.6563v2.pdf)


##Pruning approaches

[Optimal brain damage](http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf)

[Optimal brain surgeon](http://ee.caltech.edu/Babak/pubs/conferences/00298572.pdf)

[Tri-State ReLUs](http://arxiv.org/abs/1511.05497)

##Genetic Approaches

[NEAT](https://www.cs.ucf.edu/~kstanley/neat.html)

[HyperNEAT](http://eplex.cs.ucf.edu/hyperNEATpage/)",26,104,False,self,,,,,
177,MachineLearning,t5_2r3gv,2016-2-7,2016,2,7,22,44lkvz,carpedm20.github.io,Visualization of asian face generation with Tensorflow and convnetjs,https://www.reddit.com/r/MachineLearning/comments/44lkvz/visualization_of_asian_face_generation_with/,carpedm20,1454851520,,3,29,False,http://a.thumbs.redditmedia.com/ReJVwpJNjlW1owTZxVjCZIH5PXWz1pa80cz5ElXT4v8.jpg,,,,,
178,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,0,44lwh9,arstechnica.co.uk,GCHQs data-mining techniques revealed in new Snowden leak,https://www.reddit.com/r/MachineLearning/comments/44lwh9/gchqs_datamining_techniques_revealed_in_new/,lokator9,1454857553,,1,11,False,http://a.thumbs.redditmedia.com/GFw9cVq9H3mrddsqEeLnBhEv4aefFTM0624iprEOta8.jpg,,,,,
179,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,0,44lxst,andreasleine.com,I'm div0rced and I want t find a guy fr the night. D yu friends free? G,https://www.reddit.com/r/MachineLearning/comments/44lxst/im_div0rced_and_i_want_t_find_a_guy_fr_the/,r45ja29kp78es,1454858183,,0,1,False,default,,,,,
180,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,0,44m06x,arxiv.org,Generating Sequences With Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/44m06x/generating_sequences_with_recurrent_neural/,3eyedravens,1454859183,,0,2,False,default,,,,,
181,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,0,44m3in,pybloggers.com,Ensemble Methods: Elegant Techniques to Produce Improved Machine Learning Results,https://www.reddit.com/r/MachineLearning/comments/44m3in/ensemble_methods_elegant_techniques_to_produce/,LearnDataSci,1454860428,,0,3,False,http://b.thumbs.redditmedia.com/M7GFKzvWMwQYQvFTL1RjGo2jxulA3foLM8uON95OP-M.jpg,,,,,
182,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,1,44m839,microsoftventures.com,Microsoft's machine learning accelerator kicks off batch 3,https://www.reddit.com/r/MachineLearning/comments/44m839/microsofts_machine_learning_accelerator_kicks_off/,lokator9,1454862090,,0,13,False,http://b.thumbs.redditmedia.com/QGBBRDrBAYspR9Xwq-catF04e90Al0uGx9ngS7DC0CY.jpg,,,,,
183,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,1,44mcur,asianscientist.com,"Biological Evolution &amp; Machine Learning Are Similar, Says Turing Award Winner Leslie Valiant",https://www.reddit.com/r/MachineLearning/comments/44mcur/biological_evolution_machine_learning_are_similar/,lokator9,1454863803,,0,10,False,http://a.thumbs.redditmedia.com/85eOWa-CjafFoQHwkSVbjkqjiXkgaS8G7KwUlI3JCv4.jpg,,,,,
184,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,2,44mg96,youtube.com,Recent advances in deep learning - Oriol Vinyals of Google,https://www.reddit.com/r/MachineLearning/comments/44mg96/recent_advances_in_deep_learning_oriol_vinyals_of/,lokator9,1454865088,,0,20,False,http://b.thumbs.redditmedia.com/BOo-RmJQE-UJzv0YaSpbs6mx2ABUl6N2hD3r4AgNQ1Q.jpg,,,,,
185,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,2,44mgkf,blog.claymcleod.io,Parametric Activation Pools greatly increase performance and consistency in ConvNets,https://www.reddit.com/r/MachineLearning/comments/44mgkf/parametric_activation_pools_greatly_increase/,clmcl,1454865200,,6,14,False,http://b.thumbs.redditmedia.com/hwGzJn5-npN_9oKbYDxkSqdv2_mI5EZ1A6rEWsmnFdE.jpg,,,,,
186,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,2,44ml9n,annotatemydata.com,"Hey Machine Learning researchers ! We are providing service for generating ground truth data for images and videos. Whether its Bounding Box, Pose Estimation or Object Classification. See more info at the URL.",https://www.reddit.com/r/MachineLearning/comments/44ml9n/hey_machine_learning_researchers_we_are_providing/,ufazal,1454866835,,5,10,False,http://b.thumbs.redditmedia.com/tgLRqvTY2ilrC7wyCjmIP7y9Kqn6sUmZoWUqQ7gyo1U.jpg,,,,,
187,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,3,44mphq,self.MachineLearning,"What sort of ML should I use to refine ""magic"" numbers in a program I have?",https://www.reddit.com/r/MachineLearning/comments/44mphq/what_sort_of_ml_should_i_use_to_refine_magic/,maddoc74,1454868251,"I have a Expert Knowledge-based Control System that I'm trying to refine. 

The control takes 4 inputs: a measurement M, the rate of change dM/dt, a flow F1 and a second one F2. The output is a change to F1 and F2, in the form of a multiplier on F1, eg F1_new = .85 * F1_old.

The controller essentially partitions the possible values of the inputs into ~N partitions each, the value of which constitutes a state vector, eg a state vector may look like (M,dM,F1,F2) = (0,1,2,1). The 0 partition for M may be where M is in the range of 0 to 50, the 1 partition is 50 to 100 etc.

The controller then selects the multipliers based on which state the system is currently in. Each state has been exhausted, and a multiplier has been picked (based only on the expert's knowledge of the system) for that state. 

Here's a simpler model using only F1, dM, and 2 partitions each to illustrate what the code looks like:

    (0,0): if (F1 &gt; 0) and (dM &gt; 0) then F1 = .5 * F1
    (0,1): if (F1 &gt; 0) and (dM &lt;= 0) then F1 = 2 * F1
    (1,0): if (F1 &lt;= 0) and (dM &gt; 0) then F1 = -3 * F1
    (1,1): if (F1 &lt;= 0) and (dM &lt;= 0) then F1 = -1/4 * F1

The current partitioning scheme we have yields 341 states, so there's 682 ""magic"" numbers (341 for F1 and 341 for F2). When asking the expert how he came up with the numbers, it essentially boils down to his 30+ years of experience with the system. The partitions are also not uniformly sized, they follow a more logarithmic scheme, eg:

    0 -&gt; 0 &lt; log(dM) &lt;= 1
    1 -&gt; 1 &lt; log(dM) &lt;= 2

This is really only an approximation; the partitions look log normal but are a little more skewed than that. 

I spoke recently with a friend of mine about generalizing this system to an arbitrary number of partitions of nonuniform size, as well as optimizing those magic numbers. Maybe the are optimal, or maybe we need about 1000 more states to hit the sweet spot. He was of the opinion that ML would be a good tool to use to tackle this problem. It's obviously very intensive to modify any of these things manually requiring lots of rewrites to get where I need to be.

What do you think? Can I use some sort of genetic algorithm to optimize the controller's function? If I fix the number of states and just want to refine the magic numbers, would that be more possible? I do have a base model to run the controller against, but the fitness function would be pretty arbitrary, (eg percent of values within some range), as my experience with ML is limited at best.

",8,1,False,self,,,,,
188,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,3,44mzfh,self.MachineLearning,Implementing Dynamic memory networks in Theano,https://www.reddit.com/r/MachineLearning/comments/44mzfh/implementing_dynamic_memory_networks_in_theano/,HrantKhachatrian,1454871469,"During the [DeepHack.Q&amp;A](http://qa.deephack.me) hackathon we have implemented Dynamic memory networks by [Kumar et al.](http://arxiv.org/abs/1506.07285). We briefly experimented with [bAbI tasks](https://research.facebook.com/research/-babi/) and got some interesting results especially on the positional reasoning task (#17).

Details are [here](http://yerevann.github.io/2016/02/05/implementing-dynamic-memory-networks/). Code is [on Github](https://github.com/YerevaNN/Dynamic-memory-networks-in-Theano). Any feedback is appreciated.",1,9,False,self,,,,,
189,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,4,44n69p,self.MachineLearning,How to train word2vec for same words differently according to context?,https://www.reddit.com/r/MachineLearning/comments/44n69p/how_to_train_word2vec_for_same_words_differently/,n00bto1337,1454873953,"For example, I want the final output to be something like: 

Model A:

Similarity of USA and Canada: 0.8
(Since same continent)

Model B:

Similarity of USA an Canada: 0.2
(Since different country)

Also, if I don't have specific, grammatical sentences as such, can I train them on just a bunch of words/ phrases?",3,4,False,self,,,,,
190,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,4,44n7b4,self.MachineLearning,Does batch normalization mean that the assumption of normally distributed inputs can be relax in LeCun's loss surface paper?,https://www.reddit.com/r/MachineLearning/comments/44n7b4/does_batch_normalization_mean_that_the_assumption/,newbiethrownaway,1454874322,"Just a wild speculation. Since batch normalization turns the inputs'distribution into N(0,1).

LeCun's paper http://arxiv.org/abs/1412.0233
",3,3,False,self,,,,,
191,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,5,44ngsq,self.MachineLearning,Anyone took NLP intro course from Coursera? I have a question...,https://www.reddit.com/r/MachineLearning/comments/44ngsq/anyone_took_nlp_intro_course_from_coursera_i_have/,picardo,1454877840,"_Sorry to ask this here. The [class](https://www.coursera.org/course/nlpintro) forums have been inaccessible after the course ended, and I can't think of a better place to ask this question._

In the assignment1, there is a question that asks us to think of features to feed to the feature extractor for the dependency parser. The code sample we get has the following comment:


&gt; Think of some of your own features here! Some standard features are described in Table 3.2 on page 31 of Dependency Parsing by Kubler, McDonald, and Nivre
&gt; 
&gt; http://books.google.com/books/about/Dependency_Parsing.html?id=k3iiup7HB9UC

I can't access that part of the book with that link. It's under paywall and I can't afford to purchase the book right now. Does anyone have access to this book? Can you take a screencap of the table mentioned here and include it here? Would be very grateful.",6,2,False,self,,,,,
192,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,7,44nuc6,self.MachineLearning,CPU vs. GPU on regular non-conv networks,https://www.reddit.com/r/MachineLearning/comments/44nuc6/cpu_vs_gpu_on_regular_nonconv_networks/,trevinstein,1454882570,"I'm trying to find a benchmark that shows a factor by which a GPU beats a CPU on a task that does not involve convolutions.

Edit: CPU and GPU price is comparable.",11,8,False,self,,,,,
193,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,7,44nw3l,kdnuggets.com,50+ Data Science and Machine Learning Cheat Sheets,https://www.reddit.com/r/MachineLearning/comments/44nw3l/50_data_science_and_machine_learning_cheat_sheets/,PyBet,1454883169,,7,75,False,http://a.thumbs.redditmedia.com/0XDh_JmdC2XRgO6vvSBNURxyfRgkZm6xzhtaYhJ4Mu8.jpg,,,,,
194,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,8,44o71o,self.MachineLearning,Adjustable Recommendation System [xpost from r/datascience],https://www.reddit.com/r/MachineLearning/comments/44o71o/adjustable_recommendation_system_xpost_from/,slaw07,1454887025,"I've never built a recommendation system before but I understand the concepts behind a collaborative filter. Given a set of implicit feedback (e.g., students who get above a B on a course vs students who score a B and below), I am trying to figure out if it is possible to perform the matrix factorization once on the entire student population to create an adjustable course recommendation system. By ""adjustable"" I mean where the user could choose a subset of the student population to receive recommendations from (e.g., students who scored above an A-, students who are currently juniors, students who were part time, etc). From what I can understand, I'd have to regenerate a separate recommender for each student segment. However, I was hoping that I could somehow just reweight each student's contribution to the recommendation on the fly (limited to the prescribed segments) as it is requested by the user. So, the user can get recommendations generated from everyone OR specify only from students who scored belowan A and is sophomore or older.",0,4,False,self,,,,,
195,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,8,44o80a,self.MachineLearning,Image difference recognition?,https://www.reddit.com/r/MachineLearning/comments/44o80a/image_difference_recognition/,Mr_Dogood,1454887386,"I'll preface this as I am ML noob. I am interested in training a model that can recognize slight differences between two photos ie. say you take two photos of the same room and remove a single item, the model would be able to identify which item was removed. 

I was hoping I could get some advice here on where might be a good place to start (datasets/model etc.)

Thanks!",2,2,False,self,,,,,
196,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,8,44ochv,self.MachineLearning,Forcing learning rate to zero in Torch?,https://www.reddit.com/r/MachineLearning/comments/44ochv/forcing_learning_rate_to_zero_in_torch/,adagrad,1454889185,I have a layer where I'd like to set the learning rate to zero - is there a way to do this without doing layer-wise SGD as in [here](https://github.com/torch/nn/issues/466)?,4,4,False,self,,,,,
197,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,9,44odra,self.MachineLearning,Theory of how to imprint a neuralnet onto a Human mind by associating mouse speed with screen brightness and varying brightness to cause different hand movement of the mouse,https://www.reddit.com/r/MachineLearning/comments/44odra/theory_of_how_to_imprint_a_neuralnet_onto_a_human/,BenRayfield,1454889725,"Imagine that your screen changes between dark and bright depending on your mouse speed. The faster the brighter. You will learn this connection quickly.

If its brighter than you expected, at some tiny fraction of a second, you will subconsciously think you moved the mouse faster than you tried to move it, so you would next time be more likely to move it slower since you went too far. But you didnt actually go too far. The AI just wanted you to think that so it slightly changed the brightness from proportional to mouse speed.

It could work either direction, to cause the person to learn to move the mouse faster or slower, in context of the last few seconds of mouse speeds.

Where the neuralnet comes in is to choose, 20 or so times per second, how much to adjust screen brightness from the average expected from current mouse speed. As normal in mouse gestures, the recent history of movements (simplified to speeds in this case) are all used as vars.

Whatever else is hooked to the neuralnet could be imprinted into the Human mind through the mouse and change in screen brightness, through the bidirectional learning process. The person trains the neuralnet, and the neuralnet trains the person. This is where art and logic merge. You would dance with the AI in learning eachother's movements as a statistical process. To verify correct art, verify it spreads as a meme. To verify correct statistics, compare it to bayes rules and boltzmann/hopfield energy function, and maybe compare the flow of Human learning AI vs the flow of AI learning Human (vector of statistical inference / info flow). Some may dance by intuition alone, but there is a logic to holism and a holism to logic. I believe there is a way everything works even if we dont yet know what it is, and we will discover it through experiments like this, especially the massively multiplayer gaming kind.

This is an open research path with many possibilities to explore. I'm talking about only the basics of learning. We need to understand those first, in simple games like this, before moving on to bigdata. We missed something about the theory of intelligence, how minds teach eachother as these vector streams.

The most efficient learning algorithm I know of is Constrastive Divergence (of boltzmann neuralnet), and in second place comes Bayes Rule for example on 5 nodes at a time, though this is a more general process independent of any specific kind of mind or learning, and I mean to explain these more general properties of learning between 2 or more minds like a conversation of brainwaves.",4,0,False,self,,,,,
198,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,9,44ohps,faculty.smu.edu,Sample Selection Bias,https://www.reddit.com/r/MachineLearning/comments/44ohps/sample_selection_bias/,alexmlamb,1454891603,,1,2,False,default,,,,,
199,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,10,44on9y,self.MachineLearning,What is the most efficient learning algorithm?,https://www.reddit.com/r/MachineLearning/comments/44on9y/what_is_the_most_efficient_learning_algorithm/,BenRayfield,1454894296,"Efficiency may be measured in your choice of function of max compute cycles and bits of memory, as long as you define such function. Or to practically standardize, lets talk about a computer of 10^9 bits of memory and 10^15 compute cycles in each of which it may read or write an average of 1 bit. We will use the standard that all scalar positions in all vectors are in statistical/fraction/chance range of 0 to 1. You may scale to any range or curve desired for your algorithm of choice. Think of this as a percentile view.

Contrastive Divergence is the most efficient learning algorithm I know of, with Bayes Rule inference second. Variations by kind of data of course can do better, but I mean like AIXI or Neural Turing Machine or Eurisko or the philosophies of Socrates, to not assume knowledge of anything and learn it again from experience going forward.

What is the most efficient learning algorithm?",7,0,False,self,,,,,
200,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,10,44oqtn,self.MachineLearning,What is the difference between nodes in the hidden layer?,https://www.reddit.com/r/MachineLearning/comments/44oqtn/what_is_the_difference_between_nodes_in_the/,realistic_hologram,1454896054,"Hi, I'm trying to understand neural networks. I understand that you have inputs going into a hidden layer of neurons. The diagrams i see seem to imply that each neuron receives each of the inputs. Doesnt this mean that they are all getting the same inputs and therefore will all have the same output? So what is the point of multiple neuronsn the hidden layer? If this isnt the case then what is the difference between the hidden layer neurons?",5,0,False,self,,,,,
201,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,12,44p425,self.MachineLearning,Has anyone used an educational discount for the Jetson TX1?,https://www.reddit.com/r/MachineLearning/comments/44p425/has_anyone_used_an_educational_discount_for_the/,code_kansas,1454902263,"I submitted a request for an educational discount through [this](http://www.nvidia.com/object/jetson-tx1-dev-kit.html) page a couple weeks ago, but I never heard back about it (it was supposed to take up to 3 business days). When I called them, they suggested resubmitting it. I was wondering if anyone else had experienced this.",2,0,False,self,,,,,
202,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,12,44p5sa,self.MachineLearning,"RNNLM, what I did wrong?",https://www.reddit.com/r/MachineLearning/comments/44p5sa/rnnlm_what_i_did_wrong/,zhangj5,1454903059,"Hi all,
I am writing a RNNLM using Keras, but the PPL is really disappointed (on PTB data at http://rnnlm.org/). And I do not know what I did wrong, I attached the code here (https://codeshare.io/rxMuv), can you give me some hints, thanks a million!!!",4,0,False,self,,,,,
203,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,15,44poy1,self.MachineLearning,How to give different weightage to feature sets while training a classifier in Scikit?,https://www.reddit.com/r/MachineLearning/comments/44poy1/how_to_give_different_weightage_to_feature_sets/,mln00b13,1454912160,"I am currently training a SGDClassifier using Scikit in Python. My feature sets consists of names and descriptions. I need to give more weight to names, and less to descriptions, while training my classifier. I found in the documentation that 'class_weight' exists. Is that the same thing? If so, how do I use it?",2,2,False,self,,,,,
204,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,16,44pu9g,gitxiv.com,Long Short-Term Memory-Networks for Machine Reading,https://www.reddit.com/r/MachineLearning/comments/44pu9g/long_shortterm_memorynetworks_for_machine_reading/,[deleted],1454914992,[deleted],1,0,False,default,,,,,
205,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,18,44q4xa,self.MachineLearning,learning RBM,https://www.reddit.com/r/MachineLearning/comments/44q4xa/learning_rbm/,John_Smith111,1454922108,"As far as i know when performing gibbs sampling or RBM the network moves to equilibrium - it moves down to manifold landscape to some low point. 

https://charlesmartin14.files.wordpress.com/2015/03/manifold1.png

Am i get the process right? 
What kind of images are in the lowest point when the RBM converges - some average of all possible images or something else ? 

Thanks ",4,1,False,self,,,,,
206,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,18,44q9uo,self.MachineLearning,Recommender Engine with rich Facebook Data,https://www.reddit.com/r/MachineLearning/comments/44q9uo/recommender_engine_with_rich_facebook_data/,velcronicoov,1454925283,"I have the following problem:

I was provided with a rich dataset of about 5646 Facebook users. This dataset is split up in some datasets, each containing variables such as:

* Age
* Gender
* Interested in (females or males...)
* Relationship status
* Hometown
* Religion
* Political Views
* Work
* Sports
* Books
* Checkins
* Statuses, Comments, ...
* Language
* Games
* Education
* Interests
* Movies
* Music
*Facebook Groups they are member of
* ...

All of these (expect maybe some of them such as age,gender, ...) sometimes have multiple rows per user (For example user 333 knows 4 languages). 

I also have a variable likes. This is found in a dataset which contains a row for each liked page of a user. All of these datasets have a common **uid** key, which of course connects them. 

Some variables also contain categories. For example, likes have categories such as Athletes, Schools, Local Business

My task is to predict if a user will like a certain page or not, but I actually have a hard time figuring out how to achieve this. I am more used to working with simpler data sets with numerical/ financial data.


Can anyone help me out? I prefer doing this in R :)

Thanks in advance:)",2,1,False,self,,,,,
207,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,19,44qb9p,self.MachineLearning,"Python implementation of Boruta, an all relevant feature selection algorithm",https://www.reddit.com/r/MachineLearning/comments/44qb9p/python_implementation_of_boruta_an_all_relevant/,bayeslaw,1454926175,"Here's my Python implementation of Boruta (about 5-15 times faster than original R version thanks to scikit learn): https://github.com/danielhomola/boruta_py And here's a blog post I wrote it a few months ago, explaining how it works: http://danielhomola.com/2015/05/08/borutapy-an-all-relevant-feature-selection-method/",15,76,False,self,,,,,
208,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,21,44qpix,arxiv.org,Characteristics of Visual Categorization of Long-Concatenated and Object-Directed Human Actions by a Multiple Spatio-Temporal Scales Recurrent Neural Network Model,https://www.reddit.com/r/MachineLearning/comments/44qpix/characteristics_of_visual_categorization_of/,InaneMembrane,1454934811,,0,2,False,default,,,,,
209,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,21,44qpqq,self.MachineLearning,what are the problems with WEKA?,https://www.reddit.com/r/MachineLearning/comments/44qpqq/what_are_the_problems_with_weka/,eslam93,1454934931,"we all know that Weka is a tool of machine learning algorithms, for any who used Weka lately, what are the problems you think developers good to fix, or new features must be added, or even capabilities the fast the process or improve the quality of the results?",2,0,False,self,,,,,
210,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,22,44qwwt,self.MachineLearning,Overfitting when combining Variational AE with MLP?,https://www.reddit.com/r/MachineLearning/comments/44qwwt/overfitting_when_combining_variational_ae_with_mlp/,vinodrajendran001,1454938461,"I just need a general explanation for the cause of overfitting, when I integrate variational autoencoder with MLP for prediction.

",2,1,False,self,,,,,
211,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,23,44r3bd,self.MachineLearning,"Training an autoencoder, layerwise or full-stack?",https://www.reddit.com/r/MachineLearning/comments/44r3bd/training_an_autoencoder_layerwise_or_fullstack/,UnleashedBoltzman,1454941347,Is it important to train an autoencoder layer by layer or is it merely done to save some time?,3,3,False,self,,,,,
212,MachineLearning,t5_2r3gv,2016-2-8,2016,2,8,23,44r5oi,wired.com,The Hunt for the Algorithms That Drive Life on Earth,https://www.reddit.com/r/MachineLearning/comments/44r5oi/the_hunt_for_the_algorithms_that_drive_life_on/,eljoujat,1454942331,,3,12,False,http://b.thumbs.redditmedia.com/Grc4jGDFYBx8vxHHuMlqnpeqbgw1EHm75mFX6gKwecc.jpg,,,,,
213,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,1,44rjhu,innoarchitech.com,"Machine Learning: An In-Depth, Non-Technical Guide - Part 2",https://www.reddit.com/r/MachineLearning/comments/44rjhu/machine_learning_an_indepth_nontechnical_guide/,innoarchitech,1454947473,,0,2,False,http://b.thumbs.redditmedia.com/cYqha0vUwTZX4NwVjlANkt44N2Y6tN3OHO0Dnnp3jPE.jpg,,,,,
214,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,1,44rjr9,scholar.harvard.edu,Thou Shalt Kill: Using Machine Learning Algorithms to Measure Violence in Islamic and other Religious Scriptures,https://www.reddit.com/r/MachineLearning/comments/44rjr9/thou_shalt_kill_using_machine_learning_algorithms/,soulslicer0,1454947559,,19,11,False,default,,,,,
215,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,1,44rrzm,self.MachineLearning,pre-trained speech -&gt; text NNs?,https://www.reddit.com/r/MachineLearning/comments/44rrzm/pretrained_speech_text_nns/,HelmsmanRobertson,1454950385,"I was wondering if anyone could point me to any (freely available) speech transcription neural networks? I'm not looking for use cases (e.g. ""look at google voice"") but models I can download and run audio through.

Thanks!",3,3,False,self,,,,,
216,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,2,44rul9,re-work.co,Join the celebration of women working in Machine Intelligence at this dinner in London! Speakers include Google DeepMind. The dinner is open to anyone keen to support advancing women in the industry!,https://www.reddit.com/r/MachineLearning/comments/44rul9/join_the_celebration_of_women_working_in_machine/,reworksophie,1454951246,,0,1,False,default,,,,,
217,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,2,44rvtw,lab41.org,Nine Must-Have Datasets for Investigating Recommender Systems,https://www.reddit.com/r/MachineLearning/comments/44rvtw/nine_musthave_datasets_for_investigating/,amplifier_khan,1454951694,,0,0,False,http://a.thumbs.redditmedia.com/VcR1KHnelDgqcq8DfQkDzrsnoRHY7T75vqkBjnYuMg4.jpg,,,,,
218,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,3,44s568,ppaml.galois.com,Probabilistic Programming Summer School Announcement,https://www.reddit.com/r/MachineLearning/comments/44s568/probabilistic_programming_summer_school/,SuperFX,1454954883,,0,7,False,default,,,,,
219,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,4,44sgxs,self.MachineLearning,What is a good alternative for MATLAB Neural Network Toolbox?,https://www.reddit.com/r/MachineLearning/comments/44sgxs/what_is_a_good_alternative_for_matlab_neural/,hlyates,1454959018,"I want an alternative tool that has the same if not more features found in Matlab Neural Network toolbox, but with the ability to extend it programmatically with my own needs if needed. Phrasing the question in another way, does a good alternative exist in java, R, or python? If so, which of these are best? 
",6,6,False,self,,,,,
220,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,5,44spm6,mtyka.github.io,Deepdream-like model class visualizations using bilateral filters,https://www.reddit.com/r/MachineLearning/comments/44spm6/deepdreamlike_model_class_visualizations_using/,abhshkdz,1454961995,,4,23,False,http://b.thumbs.redditmedia.com/h4pFoMF7yaLUjw_kU2g1oEnBEgOSSjWoBLZO2o6ykHA.jpg,,,,,
221,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,5,44sqcg,pjreddie.com,Use Darknet to generate the next Game of Thrones book using RNNs,https://www.reddit.com/r/MachineLearning/comments/44sqcg/use_darknet_to_generate_the_next_game_of_thrones/,pjreddie,1454962245,,14,73,False,http://b.thumbs.redditmedia.com/u2qSfJJXPM-HptXohNtEZg3VXTpjZ6uTMnUvfM7czDI.jpg,,,,,
222,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,5,44swb0,self.MachineLearning,Looking to write a natural language query processor,https://www.reddit.com/r/MachineLearning/comments/44swb0/looking_to_write_a_natural_language_query/,bgard6977,1454964251,"I'm new to machine learning, but I'd like to build a system that can take in a natural language query in English, such as ""How many R&amp;B songs are in my music library from 1980"" and translate that into an actionable query plan against a real database. It can even be a handful of template queries to start with, as long as I can pull out the appropriate query parameter values.

* Is there research that has been done in this area? What are the key words? Can any one direct me to any appropriate papers?
* What languages and frameworks best fit this problem?
* How would I start modelling this? word2vec + convnet of some kind?",5,1,False,self,,,,,
223,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,6,44t9p2,techinsider.io,Google's AI is teaching itself how to solve a virtual maze  and it's working,https://www.reddit.com/r/MachineLearning/comments/44t9p2/googles_ai_is_teaching_itself_how_to_solve_a/,[deleted],1454968719,[deleted],1,0,False,default,,,,,
224,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,7,44tfie,self.MachineLearning,TTS (text to speech) via NN?,https://www.reddit.com/r/MachineLearning/comments/44tfie/tts_text_to_speech_via_nn/,trevinstein,1454970717,"I'm looking for a paper that documents an attempt (successful or otherwise) to generate speech sound by taking text as the input.
",4,3,False,self,,,,,
225,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,7,44th23,self.MachineLearning,Convnets that output images?,https://www.reddit.com/r/MachineLearning/comments/44th23/convnets_that_output_images/,LyExpo,1454971446,"I've been trying to learn how this works, but haven't had any success. 

An example of something that does this is the 'colorizer' that was posted [here](https://www.reddit.com/r/MachineLearning/comments/4007ma/colorizing_black_and_white_photos_with_deep/), and [here](http://tinyclouds.org/colorize/) and a more detailed write-up.

The architecture described in the second link, if you strip away the fancy skip connections and batch normalization, is basically a convolutional autoencoder. So this is where I started. I'm starting with the simple and pointless task of just trying to train a network that outputs the fed image. Example archtichture is: 
    
    100x100 rgb image  ---&gt;  convolution ---&gt; convolution ---&gt; deconvolution ---&gt; deconvolution  ---&gt;  100x100 rgb image

I've experimented with different architectures, different filter sizes, mean squared error loss function, cross entropy, etc... but can't seem to get good results. The network always seems to settle on outputting images that are just one color.

How should I tweak from here? I think the main problem here is that outputting 100x100x3 pixel values means learning a function that is in some very, very high dimensional space, but other projects seem to do it. How do they do it?",3,4,False,self,,,,,
226,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,8,44tk2j,darpa.mil,Brain interface tested on a sheep by DARPA,https://www.reddit.com/r/MachineLearning/comments/44tk2j/brain_interface_tested_on_a_sheep_by_darpa/,dendisuhubdy,1454972723,,0,2,False,http://b.thumbs.redditmedia.com/WVyVj_cmCRqL90czwTf34V9pm6bVzsGyXOR7wzBwDDs.jpg,,,,,
227,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,8,44tm55,self.MachineLearning,Regarding Discriminative features condition,https://www.reddit.com/r/MachineLearning/comments/44tm55/regarding_discriminative_features_condition/,[deleted],1454973563,[deleted],0,2,False,default,,,,,
228,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,8,44tmt6,youtube.com,Getting Started with Orange,https://www.reddit.com/r/MachineLearning/comments/44tmt6/getting_started_with_orange/,PyBet,1454973824,,0,3,False,http://b.thumbs.redditmedia.com/_oND-6wR5wPPpsUIBnEV4OX5peli7S46kQrKR92qfGU.jpg,,,,,
229,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,9,44u0yg,self.MachineLearning,Those of you with experience implementing real-time machine learning on the web - what is an acceptable latency in your experience?,https://www.reddit.com/r/MachineLearning/comments/44u0yg/those_of_you_with_experience_implementing/,kebabmybob,1454979040,"I'm currently seeing 20-30 ms response time for serializing inputs, hitting my model endpoint, and using them on the front-end. I'm being told my front end engineers that this is 'extremely high' and needs to be in the sub 10 ms range but that seems impossible to me, unless you literally hardcode linear model coefficients on the front-end in javascript.",6,5,False,self,,,,,
230,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,10,44u927,arxiv.org,"[1509.01240] Train faster, generalize better: Stability of stochastic gradient descent",https://www.reddit.com/r/MachineLearning/comments/44u927/150901240_train_faster_generalize_better/,mttd,1454982182,,3,33,False,default,,,,,
231,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,10,44u9nb,arxiv.org,[1602.02215] Swivel: Improving Embeddings by Noticing What's Missing,https://www.reddit.com/r/MachineLearning/comments/44u9nb/160202215_swivel_improving_embeddings_by_noticing/,colinhevans,1454982403,,9,14,False,default,,,,,
232,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,11,44ug79,self.MachineLearning,DNN -&gt; RNN -&gt; Seq2Seq -&gt; RL ... Coming full circle?,https://www.reddit.com/r/MachineLearning/comments/44ug79/dnn_rnn_seq2seq_rl_coming_full_circle/,physixer,1454984995,"I was watching [Oriol Vinyals](https://redd.it/44mg96) talk and when he talked about Seq2Seq it seemed to have resemblance with reinforcement learning, for which you have a sequence of input states and you have to produce a sequence of output states (e.g., while playing Atari games). Maybe one major difference might be that for Seq2Seq the whole input sequence is fed before an output sequence is generated whereas in RL output sequence elements start coming out with only a little bit of latency after the start of input sequence elements. Any thoughts.

Also in one of the talks last year LeCun said ""RL is cherry on top"" while implicitly disagreeing with DeepMind's approach (tackling RL head first).

If I have to put two things in perspective, it looks like both approaches are complementary. non-RL approach is bottom-up (develop the building blocks before using them in an RL enviroment) whereas RL (DeepMind's) approach is top-down (have RL environment in place and then see what building blocks can let us do the job best).

Corrections, comments, connections, insights?",3,6,False,self,,,,,
233,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,11,44uid5,arxiv.org,[1602.02261] WebNav: A New Large-Scale Task for Natural Language based Sequential Decision Making,https://www.reddit.com/r/MachineLearning/comments/44uid5/160202261_webnav_a_new_largescale_task_for/,downtownslim,1454985795,,3,9,False,default,,,,,
234,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,13,44uyh3,self.MachineLearning,What type of Neural Net should I use to optimize for clicks on an image?,https://www.reddit.com/r/MachineLearning/comments/44uyh3/what_type_of_neural_net_should_i_use_to_optimize/,thehitchhiker2,1454992279,"I have a site that has data on 100K+ images and whether or not people have clicked on each. Would like to build and train a NN (conv, etc...) that can take an image and some data about the image and determine how likely someone is to click on it.   Are there any examples or frameworks out there for this?

Thanks!
",2,0,False,self,,,,,
235,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,13,44uzl1,self.MachineLearning,How do I get informative features for each classification using Scikit?,https://www.reddit.com/r/MachineLearning/comments/44uzl1/how_do_i_get_informative_features_for_each/,mln00b13,1454992771,"For each classification, I want to know what features helped it pick that classification, and their probabilities. Also, I have a multi label classifier, SGDClassifier, so is it possible to get all the labels and their probabilities, for each classification my classifier does?",3,1,False,self,,,,,
236,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,13,44v2bh,edge.org,Roger Schank discusses the (lack of) potential for modern statistical machine learning approaches to achieve strong artificial intelligence.,https://www.reddit.com/r/MachineLearning/comments/44v2bh/roger_schank_discusses_the_lack_of_potential_for/,skrillexisokay,1454993961,,4,0,False,http://a.thumbs.redditmedia.com/XzKvq5zFjmXpxNOhML-v7WjCW3_LvDKGJ_dgR0zJRt0.jpg,,,,,
237,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,15,44vep4,iclr.cc,ICLR 2016: Accepted Papers,https://www.reddit.com/r/MachineLearning/comments/44vep4/iclr_2016_accepted_papers/,iori42,1454999902,,2,42,False,http://b.thumbs.redditmedia.com/JfNrz4_rqiBOFJV6zlfMr63hsB-hOlCK-tnZBsCdmto.jpg,,,,,
238,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,15,44vfqs,databricks.com,Auto-scaling scikit-learn with Spark,https://www.reddit.com/r/MachineLearning/comments/44vfqs/autoscaling_scikitlearn_with_spark/,igor_subbotin,1455000474,,11,81,False,http://b.thumbs.redditmedia.com/S9a4aIkk-UYuPvOyy0h9vB8Ak75nC1jXqkBk_BqjXXw.jpg,,,,,
239,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,17,44vov3,arxiv.org,"[ICLR16] ""Exploring the Limits of Language Modeling"" (Strong baselines! Parameter counts! 60 -&gt; 30 perplexity!)",https://www.reddit.com/r/MachineLearning/comments/44vov3/iclr16_exploring_the_limits_of_language_modeling/,bluecoffee,1455005730,,36,26,False,default,,,,,
240,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,19,44w2bg,glebartsablyuk.com,WARNING!! On this site a lot of people who want to find a sexy adventure a,https://www.reddit.com/r/MachineLearning/comments/44w2bg/warning_on_this_site_a_lot_of_people_who_want_to/,anvenmudeft,1455013765,,0,1,False,default,,,,,
241,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,21,44wdfn,sorinc.com,Explosion Proof Differential Pressure Switch,https://www.reddit.com/r/MachineLearning/comments/44wdfn/explosion_proof_differential_pressure_switch/,saviodsilva,1455019620,,0,1,False,default,,,,,
242,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,22,44wp9b,self.MachineLearning,Cloud-based machine learning platforms,https://www.reddit.com/r/MachineLearning/comments/44wp9b/cloudbased_machine_learning_platforms/,zibenmoka,1455025333,"Hi there, 

I recently tried to look at AWS Machine Learning service - but it really looks very basic. They only propose logistic regression (I am aware of its usefulness though!) for binary and multi-label classification + linear regression  (for regression problems). I find it  very limited. [ http://docs.aws.amazon.com/machine-learning/latest/dg/learning-algorithm.html ] 

Can you recommend any cloud-based platforms where I would have more flexibility to possibly create custom models (or whole pipelines) ? 

Thanks  ",1,2,False,self,,,,,
243,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,22,44wppe,self.MachineLearning,Any chunked version of the Yahoo News dataset available?,https://www.reddit.com/r/MachineLearning/comments/44wppe/any_chunked_version_of_the_yahoo_news_dataset/,InsideAndOut,1455025522,"Recently, Yahoo released the 'largest ever dataset', (found here: http://webscope.sandbox.yahoo.com/catalog.php?datatype=r&amp;did=75).

Unfortunately, I don't have access to 1.5 TB of disk space at the moment, and was wondering if any of you guys know if there is a subsample (like 1 week of search data) available from the dataset? Or is the only solution getting extra memory?",0,9,False,self,,,,,
244,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,22,44wqm9,self.MachineLearning,Can I solve localization task as a simple regression problem?,https://www.reddit.com/r/MachineLearning/comments/44wqm9/can_i_solve_localization_task_as_a_simple/,sunshineatnoon,1455025921,"Say I want to localize an object in image, one way is to treat it as a regression problem and train a network with L2 loss to predict four coordinates for the object. But this seems like a simple solution, has anyone tried on this?",6,11,False,self,,,,,
245,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,23,44wubl,self.MachineLearning,Am I Misunderstanding the Seq2Seq Model?,https://www.reddit.com/r/MachineLearning/comments/44wubl/am_i_misunderstanding_the_seq2seq_model/,throwawayGRANTS,1455027450,"An RNN can be used to generate text. My current understanding is that for such a model, the output at each timestep is a probability distribution over the vocab, and the number of params for the output layer = (layer size * vocab size)? Error is calculated using cross entropy between the predicted distribution for each timestep, and the target distribution (a one-hot distribution with the target word at that timestep having 100% probability)? I feel like I am misunderstanding something as this does not seem very efficient for an architecture as the size of the vocabulary increases. Are there other generational models for text?  ",8,6,False,self,,,,,
246,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,23,44wugk,self.MachineLearning,Are universal probabilistic programming languages good at solving constraint satisfaction problems?,https://www.reddit.com/r/MachineLearning/comments/44wugk/are_universal_probabilistic_programming_languages/,AnvaMiba,1455027505,"NP-complete constraint satisfaction problems, such as [boolean satisfiability](https://en.wikipedia.org/wiki/Boolean_satisfiability_problem) (SAT) admit, by definition, non-deterministic polynomial-time algorithms. These algorithms contain non-deterministic choice points such that, if you had an oracle which could always pick the correct choice, they would always run in polynomial time and output either ""unsatisfiable"" or ""satisfiable"" (and in this case often also return a satisfiable assignment to the problem instance variables).

Of course, assuming (BP)P != NP, no such oracle actually exists as a function that is itself (stochastically) computable in polynomial time, but these non-deterministic algorithms can be nevertheless useful as the base of practically useful algorithms by combining them with suitable heuristics for making choices and a search strategy.

For instance, for the SAT problem, a typical algorithm ([DPLL](https://en.wikipedia.org/wiki/DPLL_algorithm)) recursively picks a variable and sets it to either ""true"" or ""false"", then it simplifies the problem and recurses until it either finds a satisfiable assignment  of all variables, in which case it reports it and exists, or a conflict, in which case it backtracks, returning ""unsatisfiable"" when it runs out of options.

""Universal"" probabilistic programming languages, such as [Church](http://projects.csail.mit.edu/church/wiki/Church), execute arbitrary programs that contain stochastic choice points controlled by random variables which can be dependent on each other, and these systems are able to perform sampling w.r.t. these variables conditioned on arbitrary expression taking certain values. Typically, they operate by running MCMC in the space of program execution traces.

The intended use case of these probabilistic programming languages is Bayesian posterior inference in (typically non-parametric, recursive) graphical models. I was wondering if they can also be effectively used for solving constraint satisfaction problems.

We can run non-deterministic algorithm as a probabilistic program by making a stochastic choice at each choice point and condition the execution on outputting a satisfiable assignment. If the choices are sampled from a trivial distribution (e.g. uniform) and we implement conditioning by rejection sampling, this is essentially a form of random brute-force search, which is not effective for all but the smallest problem instances. But if instead of rejection sampling we use MCMC (e.g. Gibbs sampling or MetropolisHastings), this becomes much more similar to simulated annealing, which is usually a very effective (often state of the art) technique for many practical problem instances.

So do you think that probabilistic programming languages can be effectively used for this task in this way?
",0,6,False,self,,,,,
247,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,23,44wwc9,384.best-sex.ml,Beeeest partners for seeex in your city hereee! (FREE).,https://www.reddit.com/r/MachineLearning/comments/44wwc9/beeeest_partners_for_seeex_in_your_city_hereee/,earipe25083,1455028273,,0,1,False,default,,,,,
248,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,23,44wy6u,self.MachineLearning,Intuition/Theory behind ensemble methods?,https://www.reddit.com/r/MachineLearning/comments/44wy6u/intuitiontheory_behind_ensemble_methods/,Adamworks,1455029008,"I am having trouble finding a good explanation on why Ensemble Methods tend to produce better predictions. The best explanation I've found is from this paper [here](http://web.engr.oregonstate.edu/~tgd/publications/mcs-ensembles.pdf). They speak of Statistical, Representational, and Computational reasons why ensembles help.

The computational reason is pretty clear, as it protects against getting trapped in a local minima. But the statistical and representational reasoning is a little murky to me.

Any additional resources would be great! Thanks!

Edit: I found this really good book explaining the Bias-Variance trade off that is made when running ensembles. Essentially, you shrinking the variance of your prediction, while attempting to keep bias relatively low.

http://arxiv.org/pdf/1407.7502v3.pdf


",7,6,False,self,,,,,
249,MachineLearning,t5_2r3gv,2016-2-9,2016,2,9,23,44wzrv,self.MachineLearning,Just crashed an ssh server with a code similar to cifar-10 in Tensorflow. What might have caused this?,https://www.reddit.com/r/MachineLearning/comments/44wzrv/just_crashed_an_ssh_server_with_a_code_similar_to/,AwesomeDaveSome,1455029633,"I just tried to run my first neural net for image recognition on my research group's PC over ssh. There was a problem somewhere, which crashed the PC, and left me with no logs as of why this might have happened. Since I do not want to repeatedly interrupt everybody from working, I'd like to ask if anybody has got any idea what could have caused the whole PC (not just ipython in which I ran the code) to crash.

Here's what happened and important information: what I am trying to do is run the Cifar-10 example code from the Tensorflow website, but with our own images. The differences that will change anything about performance are: we use 424x424x3 images, instead of 32x32x3 as in Cifar-10. The rest is exactly the same, same amount of images, just readjusted the numbers to match the new image sizes. And left out a function which crops the images. Otherwise, nothing has been changed at all.


I used the screen command so I would be able to detach my terminal from it once the code is running. I then ran the code using ipython, which gave the message ""Filling queue with images before starting to train. This will take a few minutes."" as well as two messages with something about threads that are used or something similar. Everything looked fine, and I detached my screen and logged of. I told ipython to save the output to a text file, however, that file has nothing written in it at all, I think due to the crash and since it was opened and not saved yet, it got wiped.


Since the screen now isn't around anymore, and I have no output either, I have no idea how to debug what went wrong. Is it possible/very likely that the image size paired with the fact that we use 60000 in total (just like cifar-10) caused this crash? Is there any way I can find out more? I saw that two .pyc files were created for both the input.py and model.py files, but not for the train one. I am not sure if I could use those or what that means. I'm still very new to all this, and even though I am already coding for a long time, I've never debugged a code that crashes my PC upon running, so I have no idea on how to proceed.


I might want to add that I ran the code once before (mainly to debug), and let it run for a bit with screen attached. It didn't produce any kind of outputs after the three that I saw the second time for about 20 minutes, after which I canceled since not all CPU power was free at that point in time.",18,1,False,self,,,,,
250,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,0,44x1s2,self.MachineLearning,question: evolutionary algorithms without a defined fitness or selection function,https://www.reddit.com/r/MachineLearning/comments/44x1s2/question_evolutionary_algorithms_without_a/,ben_town,1455030362,"I have heard of this idea that you can evolve artificial creatures in an environment. The creatures can survive, die, mate and reproduce, but there is no fitness or selection function to decide which individuals pass on their genes. Does anyone have any links to information, articles or examples of this?",7,3,False,self,,,,,
251,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,0,44x4ve,self.MachineLearning,Newton's method with 10 lines of Python,https://www.reddit.com/r/MachineLearning/comments/44x4ve/newtons_method_with_10_lines_of_python/,bayeslaw,1455031452,"Started a new series of blog posts, where I hope to discuss some interesting topics from stats, and ML using less than 10 lines of code to demonstrate the core idea. Here's the first post about Newton's method: http://danielhomola.com/2016/02/09/newtons-method-with-10-lines-of-python/",8,9,False,self,,,,,
252,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,1,44xdqa,self.MachineLearning,"Does anyone work on sparse coding, dictionary learning after the advent of deep learning?",https://www.reddit.com/r/MachineLearning/comments/44xdqa/does_anyone_work_on_sparse_coding_dictionary/,regularized,1455034475,,2,10,False,self,,,,,
253,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,1,44xecz,self.MachineLearning,is it theoretically possible to use deep learning to generate arbitrary programming problems?,https://www.reddit.com/r/MachineLearning/comments/44xecz/is_it_theoretically_possible_to_use_deep_learning/,curiousLearn,1455034694,I want to generate arbitrary problems that can then be solved as programming exercises,0,0,False,self,,,,,
254,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,1,44xh7c,tensortalk.com,Tensorflow implementation of A Neural Attention Model for Abstractive Summarization (EMNLP 2015),https://www.reddit.com/r/MachineLearning/comments/44xh7c/tensorflow_implementation_of_a_neural_attention/,impairment,1455035615,,4,13,False,default,,,,,
255,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,1,44xj3q,medium.com,Thoughts on Motivation and Self-Motivated Software,https://www.reddit.com/r/MachineLearning/comments/44xj3q/thoughts_on_motivation_and_selfmotivated_software/,carlos_argueta,1455036194,,0,2,False,http://b.thumbs.redditmedia.com/1iGpGeuV9TXJMNHJ5BGT2jVOKEA1t5m6bxLLOuOmiYU.jpg,,,,,
256,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,2,44xmjt,self.MachineLearning,Google Hash Code 2016,https://www.reddit.com/r/MachineLearning/comments/44xmjt/google_hash_code_2016/,[deleted],1455037326,[deleted],2,0,False,default,,,,,
257,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,4,44yftl,wiki.opencog.org,What is your opinion of the OpenCog project?,https://www.reddit.com/r/MachineLearning/comments/44yftl/what_is_your_opinion_of_the_opencog_project/,InaneMembrane,1455046331,,3,1,False,default,,,,,
258,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,4,44yh0o,technologyreview.com,Googles Quantum Dream Machine,https://www.reddit.com/r/MachineLearning/comments/44yh0o/googles_quantum_dream_machine/,lokator9,1455046697,,0,5,False,http://b.thumbs.redditmedia.com/WgQTuoSWnb7-qGeZpqjKqc6m_c45oafvJSRsIyVhZpg.jpg,,,,,
259,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,5,44ym38,nervanasys.com,Scene recognition using Deep Residual Nets on a mini version of places2,https://www.reddit.com/r/MachineLearning/comments/44ym38/scene_recognition_using_deep_residual_nets_on_a/,coffeephoenix,1455048288,,0,2,False,default,,,,,
260,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,5,44ynqx,medium.com,Two Duck-Rabbit Paradigm-Shift Anomalies in Physics and One (maybe) in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/44ynqx/two_duckrabbit_paradigmshift_anomalies_in_physics/,lokator9,1455048846,,0,0,False,http://b.thumbs.redditmedia.com/7SJy2R0BPovBuE7v_pVzx64qrsVuGJYOlQsAZ7Jismg.jpg,,,,,
261,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,5,44yrqw,arxiv.org,[1602.02672] Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks,https://www.reddit.com/r/MachineLearning/comments/44yrqw/160202672_learning_to_communicate_to_solve/,iassael,1455050123,,1,11,False,default,,,,,
262,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,5,44yvku,github.com,A multithreaded job queue for distributed neural network tasks. Automates the deployment and termination of many AWS EC2 g2.2xlarge instances appropriately to minimize cost. Used here to apply neural style to video and very large images.,https://www.reddit.com/r/MachineLearning/comments/44yvku/a_multithreaded_job_queue_for_distributed_neural/,dontnormally,1455051286,,0,4,False,http://a.thumbs.redditmedia.com/FBnc2F2eWvuryh5cWtSDkdOgd-zUyVs-8AqakUbOMr4.jpg,,,,,
263,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,6,44yxel,self.MachineLearning,Model stability over consecutive builds. help!,https://www.reddit.com/r/MachineLearning/comments/44yxel/model_stability_over_consecutive_builds_help/,cdiddiest,1455051845,"We build a machine learning model that predicts the probability of a certain event happening. This probability is then made available to end users and they can set thresholds on which a certain action happens. eg. if prob &gt; 0.5 :then: do_thing

Our problem is that we have to periodically rebuild the model on fresher data and when the model is rebuilt, sometimes the range of probabilities the model produces  for these events shifts even though the ROC_AUC performance is equal. ie. modelA predicts in the range 0.3 =&gt; 0.8 and modelB predicts in the range 0.7 =&gt; 0.9
This is a headache for the end user since the thresholds they set for modelA may no longer make sense for modelB

Does anyone have any thoughts on how we can work to stabilise the prediction range across consecutive models? ",4,2,False,self,,,,,
264,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,6,44z5ei,self.MachineLearning,Examples for time-series Radial Basis Function?,https://www.reddit.com/r/MachineLearning/comments/44z5ei/examples_for_timeseries_radial_basis_function/,slow_one,1455054367,"I'm just starting to look in to Radial Basis Functions as a means to classify some data I've collected (joint angle and velocity).  This is time series data, at the moment.  Meaning, at a specific sample, I have a joint-angle and instantaneous velocity measurement.   
Most examples of RBF I've found show a x-y plane with groupings of data... and the goal is to place data points in to one of n-groups.  
I've not seen any examples of how to classify the sorts of data that I've got though...  
I do know that this is basically a Gaussian Mixture Model... but that's as far as my searching has gotten me so far.  

Any help would be appreciated (including letting me know if this isn't the right place to look).",2,2,False,self,,,,,
265,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,7,44z9h6,self.MachineLearning,Feedback on a RNN+word2vec algorithm I just thought up while high?,https://www.reddit.com/r/MachineLearning/comments/44z9h6/feedback_on_a_rnnword2vec_algorithm_i_just/,jheimon2,1455055724,"Disclaimer: more of an enthusiast than a professional, so low a priori probability that this could be useful. I am also just getting more in depth into LSTM and Word2Vec and can't really claim to understand them fully.

Here's the algorithm in short:

0. Initialize vector representations for each word in the dataset based on pretrained word vectors (or randomly).
1. Make LSTM or similar RNN predict the next word vector based on the previously introduced vectors.
2. Train RNN based on the actual vector representation of the next word.
3. Move the actual word vector towards the output of RNN.
4. Repeat.

So, any idea if this is a complete waste of time or something I should look into?",5,6,False,self,,,,,
266,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,7,44zbt1,self.MachineLearning,What is the state-of-the-art regarding activation functions?,https://www.reddit.com/r/MachineLearning/comments/44zbt1/what_is_the_stateoftheart_regarding_activation/,[deleted],1455056574,[deleted],13,8,False,default,,,,,
267,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,8,44zlgl,youtube.com,Two Minute Papers - How Do Genetic Algorithms Work?,https://www.reddit.com/r/MachineLearning/comments/44zlgl/two_minute_papers_how_do_genetic_algorithms_work/,rhiever,1455060047,,16,123,False,http://a.thumbs.redditmedia.com/SG3SFjTD9qHKKmYARJsG-0ftEJ7dta7v8_1WIexiZV4.jpg,,,,,
268,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,8,44zn8m,self.MachineLearning,AMD Firepro 500 usable for training conv NL,https://www.reddit.com/r/MachineLearning/comments/44zn8m/amd_firepro_500_usable_for_training_conv_nl/,neoteat,1455060693,"hey guys I was wondering if the graphic cards in the mac pro (Firepro 300, Firepro 500, Firepro 700) are useable for training neural networks using tensorflow / theano / caffee. Do you have any experience with these cards and if they are appropriate for GPU training ? ",4,0,False,self,,,,,
269,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,8,44znbk,self.MachineLearning,why deep learning energy landscapes appear to be convex ?,https://www.reddit.com/r/MachineLearning/comments/44znbk/why_deep_learning_energy_landscapes_appear_to_be/,John_Smith111,1455060724,"as far as i know the common perception is that the energy landscapes of deep learning appear to be roughly convex but why ?
 How it is proved or what are the evidence for convex energy landscape ?",6,2,False,self,,,,,
270,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,8,44zrho,youtube.com,DanDoesData: TensorFlow with skflow live stream,https://www.reddit.com/r/MachineLearning/comments/44zrho/dandoesdata_tensorflow_with_skflow_live_stream/,vanboxel,1455062275,,0,5,False,default,,,,,
271,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,10,4508ax,arxiv.org,[1602.02830] BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1,https://www.reddit.com/r/MachineLearning/comments/4508ax/160202830_binarynet_training_deep_neural_networks/,[deleted],1455068423,[deleted],0,1,False,default,,,,,
272,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,10,450a4p,arxiv.org,[1602.02830] BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1,https://www.reddit.com/r/MachineLearning/comments/450a4p/160202830_binarynet_training_deep_neural_networks/,MatthieuCourbariaux,1455069155,,48,50,False,default,,,,,
273,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,10,450aw3,self.MachineLearning,Has anyone tried machine learning for image enhancement?,https://www.reddit.com/r/MachineLearning/comments/450aw3/has_anyone_tried_machine_learning_for_image/,dig9900,1455069449,I haven't found any academic papers on it.,4,4,False,self,,,,,
274,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,11,450ern,medium.com,"Attempting to learn reinforcement learning in Python by learning a neural network to drive a virtual ""car"" - would love feedback",https://www.reddit.com/r/MachineLearning/comments/450ern/attempting_to_learn_reinforcement_learning_in/,harvitronix,1455070959,,0,27,False,http://b.thumbs.redditmedia.com/9QyIyA91umky2SViBN2BoDiTRPXAUHgSvMKyW58jaJo.jpg,,,,,
275,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,11,450fwl,self.MachineLearning,Lower dimensional AI problems can be solved deeper,https://www.reddit.com/r/MachineLearning/comments/450fwl/lower_dimensional_ai_problems_can_be_solved_deeper/,BenRayfield,1455071359,"Learning pictures, videos, and text takes bigdata and big money and lots of time and thought about how to design it all together, and even then many shallow paths go unexplored, while some deep paths succeed.

Compare that to learning simple shapes. A kind of bendable shape is choice of radius at each angle, and to define games as how these move around eachother by reshaping continuously. If there are 64 angles, and 1 choice of radius and 1 observation of distance outward per angle per shape, thats only 128 neural nodes (or other kind of AI vars) each, comparable to the number of pixels in a single row or column. So much more could be explored in the hundreds of dimensions instead of the millions, that would still teach us new things about AI theory. I'm building a game based on such shapes.

Whats deeper about it? Such shapes might learn to work together to do things none could alone. qlearning / reinforcement learning is the level above associative memory.",2,1,False,self,,,,,
276,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,13,450ya4,self.MachineLearning,Has anyone used Bengio's evolution RNN for tasks where LSTMs are used in the real world such as natural language or speech modelling?,https://www.reddit.com/r/MachineLearning/comments/450ya4/has_anyone_used_bengios_evolution_rnn_for_tasks/,wildtales,1455078471,"The unitary evolution RNN by Arjovsky et al. seems like a brilliant idea of training really deep RNNs without the exploding or vanishing gradient problem. The corresponding ICLR 2016 paper shows that they can model very long range dependencies better than LSTM and using much fewer parameters. I was wondering if anyone has used them for any natural language or speech modelling tasks, where LSTMs are used currently?",11,5,False,self,,,,,
277,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,19,45240g,self.MachineLearning,Using SVM for horse race prediction,https://www.reddit.com/r/MachineLearning/comments/45240g/using_svm_for_horse_race_prediction/,Eildosa,1455100435,"So I played with the scikitlearn tutorial about char recognition and tried to use their SVM for horse race.

I made this horse race generator but I have a problem.
Each race 15 horse competes and each horse is paired with a random jockey (there is 45 jockey, each horse is assigned to 3 jockeys)

since each race I do not have the same jockeys, how do I feed it to my svm?
In the case of char recognition is was 64 pixel value, here it's a little bit different.

example of the dataset I made :
https://dl.dropboxusercontent.com/u/91396766/horseRaceDataset.txt

I see no way of representing that ""this jockey is associated to this horse""

Thanks.",9,0,False,self,,,,,
278,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,21,452i2v,self.MachineLearning,contrastive divergence on RBM,https://www.reddit.com/r/MachineLearning/comments/452i2v/contrastive_divergence_on_rbm/,John_Smith111,1455108153,"Hello 

I would like to know how energy function looks liker when RBM is learned by contrastive divergence?
As far as i know contrastive divergence will perform few steps of gibbs sampling and will not converge to equilibrium.  In that way the network will be ""encoded"" with real data instead of the max likelihood of the data. How the energy function will looks like in comparison with equilibrium  converged energy function ?

Thank you ",4,3,False,self,,,,,
279,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,22,452oto,self.MachineLearning,ELI5: Shattering sets,https://www.reddit.com/r/MachineLearning/comments/452oto/eli5_shattering_sets/,i_am_erip,1455111084,"I'm taking a ML theory class. Currently we're discussing VC Dimensions and the notion of shattering arises quite often.

Wikipedia gives the following definition:
&gt; C shatters A when the power set P(A) = { c  A | c  C }.

given that

- A is a set. For example: a finite set of points.
- C is a class of sets. For example: the class of discs in the plane (each circle is a set of points).

Can anyone explain this more simply or intuitively?",5,4,False,self,,,,,
280,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,23,452wuv,self.MachineLearning,How can you use ML models/algorithms to help yourself with research?,https://www.reddit.com/r/MachineLearning/comments/452wuv/how_can_you_use_ml_modelsalgorithms_to_help/,DeapSoup,1455114136,Could you use the ideas used in e.g. Atari by DeepMind to collect/sort data?,1,2,False,self,,,,,
281,MachineLearning,t5_2r3gv,2016-2-10,2016,2,10,23,4532bm,youtube.com,1976 Matrix Singular Value Decomposition Film,https://www.reddit.com/r/MachineLearning/comments/4532bm/1976_matrix_singular_value_decomposition_film/,soulslicer0,1455116187,,19,213,False,http://a.thumbs.redditmedia.com/x6_KHO9E0Y7jjxq2kgwwMqWuv-3yQ3rLlFQx5xGdDB0.jpg,,,,,
282,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,1,453fcv,self.MachineLearning,Inquiry: Assistance in doing thorough review of NN topics leading me up to Deep Learning.,https://www.reddit.com/r/MachineLearning/comments/453fcv/inquiry_assistance_in_doing_thorough_review_of_nn/,hlyates,1455120631,"I am currently doing a thorough review of neural networks (NN) leading me up to Deep Learning. I have a couple of following questions: 

1. What good textbook(s) can be recommended to learn archaic NN topics? Any textbook that covers both archaic and new advances? Any that are centric to time series? 

2. What old NN topics should I review leading myself up to Deep Learning that I have not yet considered? I currently looking at ANNs for ARMA such as:  Simple recurrent networks (SRN), Elman-Jordan (RNN); Time Delay Neural Networks (TDNN); and Gamma Memories. 

Thanks for your patience and assistance in reading this post. I want to do new things by knowing where we have been too. Thanks. :) ",3,2,False,self,,,,,
283,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,1,453hxi,zdnet.com,Feds give Google's autonomous vehicles a win,https://www.reddit.com/r/MachineLearning/comments/453hxi/feds_give_googles_autonomous_vehicles_a_win/,UncleGriswold,1455121462,,0,0,False,http://b.thumbs.redditmedia.com/UUEdSSTZbivuD-cI6ON-jHgXhU4rjsygJbKEbZOM-Ds.jpg,,,,,
284,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,1,453j03,self.MachineLearning,Binary similarity,https://www.reddit.com/r/MachineLearning/comments/453j03/binary_similarity/,awl89,1455121787,"Greetings people

Currently i am working on a problem regarding binary classification.
The data set that i have consists of 300 samples (each of 10 features), where every sample belongs to one of the groups: 1,2,3,4 or 5.
Additionally every group of samples are nicely clustered in my feature space.

The goal is then to construct a classifier that are able to determine whether two samples comes from the same group.
So either two samples a related, or either they are unrelated (hence binary).
Furthermore, the final classifier should be able to classify samples from unknown groups.
So what do i mean by that?
What i mean is, if i exclude group 1 from the training set, the classifier should be able to determine 1-1 links in the test set.
Hence, it should be able to determine whether two samples are similarly enough.
My currently approach have been to:

1) calculate the cosine distance of every pairwise sample, and then construct a classifier using logistic regression.
Then is simply get a model that are able to make classification, based on a simple threshold value.

2) normalizeing the input vectors, and then i simply concatenate them pair-wisely. Next i create a model using SVM on the 20 feature input.

With the first approach i can achieve a f1 score of around 85, and get a model that can generalize to unseen groups.
In the second approach i can achieve a very high f1 score of around 95, but it cannot generalize to unseen groups at all.
So the question is, does anyone know a way to deal with this binary similarity problem.
Maybe someone know a nice way to transform the representation of my input, such that i can exploit the SVM approach?
Or maybe some one know a third approach, that can point me in a better direction?

That was all. 
Best regards.",4,2,False,self,,,,,
285,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,2,453xmo,learnopencv.com,Deep Learning made easy! Install NVIDIA DIGITS 3 on EC2.,https://www.reddit.com/r/MachineLearning/comments/453xmo/deep_learning_made_easy_install_nvidia_digits_3/,spmallick,1455126376,,0,1,False,http://b.thumbs.redditmedia.com/73gAvAf9zN1AgWkuaQ71PknhDsci7zr_ldjnTT0vepM.jpg,,,,,
286,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,3,4545ch,svail.github.io,Baidu's Deep Speech 2: Recognizing Both English and Mandarin with a Single Algorithm,https://www.reddit.com/r/MachineLearning/comments/4545ch/baidus_deep_speech_2_recognizing_both_english_and/,etzmor,1455128798,,7,49,False,http://b.thumbs.redditmedia.com/DClT0FaMGfaY-7mfjRDY69feGIQOCsIgyexhDyK0nfk.jpg,,,,,
287,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,3,4545qu,self.MachineLearning,Is there a deep learning framework capable of online deep reinforcement learning with automatic differentiation?,https://www.reddit.com/r/MachineLearning/comments/4545qu/is_there_a_deep_learning_framework_capable_of/,Jabberwockyll,1455128913,"I've been using Tensorflow for deep learning, but ran into [this problem](http://stackoverflow.com/questions/34536340/how-to-use-tensorflow-optimizer-without-recomputing-activations-in-reinforcement) when trying to use it for RL with NN function approximators. 

Essentially, my agent needs to perform an action in order to get the reward and following state that are used in computing the loss function.  In TF, I have to provide the entire computational graph to use automatic differentiation which requires recomputing the graph all over again.  Not only does this make programs take much longer, it would also be complicated to do with RNNs, since recomputing activations changes the states.

This isn't a problem when using offline learning with experience replay like with deep-q-network, but prevents me from using any on-policy/online learning methods.  Of course I could hard-code the gradient calculations, but I'd have to change this every time I change the network architecture, so I'd prefer not to.

Are there any deep learning frameworks that can do this with automatic differentiation without recomputing activations?  Preferably in Python, but more preferably, something that has a bigger community and more resources.  What would you do in this situation?

Thanks for the help.  I figured asking r/machinelearning would be much easier than going through the documentation of each framework given how many new ones there are.",12,3,False,self,,,,,
288,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,4,454ksm,self.MachineLearning,Tutorial on deconvolution?,https://www.reddit.com/r/MachineLearning/comments/454ksm/tutorial_on_deconvolution/,bourbondog,1455133663,I'm trying to understand how deconvolution works. Any links or tutorials would be greatly appreaciated! I tried searching google and couldn't locate anything useful. I searched this sub as well - couldn't find anything that had the keyword deconvolution.,4,6,False,self,,,,,
289,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,5,454tda,self.MachineLearning,Learning Facial Attractiveness,https://www.reddit.com/r/MachineLearning/comments/454tda/learning_facial_attractiveness/,Cybermancan,1455136376,"I'm trying to machine learn facial attractiveness. If you have time could you rate the faces on http://radiant-ice-84.herokuapp.com/ as above average or not.
REMINDER: This is about general attractiveness so rate based on above or below AVERAGE. 
We will input these results in Eigenface and if successful we will put it in our schools public science fair.

 I'll post a comment with any results.

You can use the arrow keys to quickly select attractive or unattractive.

Thank you to anyone who helps out!",3,9,False,self,,,,,
290,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,5,454w92,galvanize.com,4th Down Bot analyzes 13 years worth of NFL data to predict the best playcall to make. Here's how it works,https://www.reddit.com/r/MachineLearning/comments/454w92/4th_down_bot_analyzes_13_years_worth_of_nfl_data/,SmashBoomPow,1455137287,,0,1,False,http://b.thumbs.redditmedia.com/xK4_Z_N6OFOXgJ1KZDlmZ01m7FRA74DF2Cjx55MamGU.jpg,,,,,
291,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,6,4552n7,xxx.lanl.gov,Associative Long Short-Term Memory (Graves et al),https://www.reddit.com/r/MachineLearning/comments/4552n7/associative_long_shortterm_memory_graves_et_al/,vonnik,1455139302,,22,22,False,default,,,,,
292,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,6,4556jp,meetup.com,Thoughts on SkillSpeed Hadoop Intro Course?,https://www.reddit.com/r/MachineLearning/comments/4556jp/thoughts_on_skillspeed_hadoop_intro_course/,twopairisgood,1455140525,,0,0,False,http://b.thumbs.redditmedia.com/BxX8GYVNIebNDNj8rmsvDLjrodvI-A337ySp_nUlnpA.jpg,,,,,
293,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,7,455bs9,self.MachineLearning,Tutorial on predicting times?,https://www.reddit.com/r/MachineLearning/comments/455bs9/tutorial_on_predicting_times/,elemur,1455142132,"So I'm trying a project and would like any guidance or references on a good way to get started. I have a data set of people departing and arriving between locations, and along with the people I know some of their characteristics. (Age, height, weight, gender) I'm trying to see how I would predict the time it would take somebody to go between locations based on their supplied characteristics. 

Where would be a good place to start? And, how can I measure to determine which characteristics are actually useful in making a prediction?

Thanks!",4,2,False,self,,,,,
294,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,7,455erq,self.MachineLearning,Denoising RNN autoencoders?,https://www.reddit.com/r/MachineLearning/comments/455erq/denoising_rnn_autoencoders/,trevinstein,1455143110,"I'm looking for an RNN architecture that denoises a time series.

For example, you take alternating sine waves and add an element of a random walk to them, then have an RNN try to recover the original waves.",2,2,False,self,,,,,
295,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,8,455l6v,self.MachineLearning,What is the consensus on Deep Neural Decision Forests (ICCV 2015 best paper winner),https://www.reddit.com/r/MachineLearning/comments/455l6v/what_is_the_consensus_on_deep_neural_decision/,asdfcappa,1455145224,"Paper: http://research.microsoft.com/pubs/255952/ICCV15_DeepNDF_main.pdf

Has anyone successfully replicated the results of this paper? Why was it awarded best paper at ICCV? It seems like community adoption has been very minimal vs e.g. techniques like batch normalization, optimizers like Adam, etc?",0,3,False,self,,,,,
296,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,8,455nv3,deeplearningbook.org,Deep Learning (book) by Yoshua Bengio,https://www.reddit.com/r/MachineLearning/comments/455nv3/deep_learning_book_by_yoshua_bengio/,lifestil,1455146162,,7,1,False,default,,,,,
297,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,10,456a33,self.MachineLearning,Language agnostic deep learning approach for text classification?,https://www.reddit.com/r/MachineLearning/comments/456a33/language_agnostic_deep_learning_approach_for_text/,mimighost,1455154169,Looking for some literature that can deal with multiple languages without extensive feature engineering for classification. What is considered as the state-of-art for this problem? Thanks!,1,1,False,self,,,,,
298,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,11,456hv0,self.MachineLearning,Tensorflow attention Seq2Seq on multiple GPUs?,https://www.reddit.com/r/MachineLearning/comments/456hv0/tensorflow_attention_seq2seq_on_multiple_gpus/,rescue11,1455157125,"I'm trying to run the attention seq2seq encoder-decoder in tensorflow but am running into a limit on the model size. 
To address this, I wanted to put the encoder on one gpu and the decoder on another. Can this be achieved by creating the encoder and decoder in the embedding_attention_seq2seq() function under seq2seq.py on different gpus? 

Also, would there be other ways to put a single attention encoder decoder onto 2 gpus?",1,0,False,self,,,,,
299,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,14,457894,self.MachineLearning,Machine learning + geospatial data. What real world problems can we solve?,https://www.reddit.com/r/MachineLearning/comments/457894/machine_learning_geospatial_data_what_real_world/,[deleted],1455167532,[deleted],0,1,False,default,,,,,
300,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,14,4579jc,fdjohnson.pointblog.net,Check out the Best Platform For Quality Hydraulic Fittings and Adapters By Brennan!,https://www.reddit.com/r/MachineLearning/comments/4579jc/check_out_the_best_platform_for_quality_hydraulic/,jackerfrinandis,1455168126,,0,1,False,default,,,,,
301,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,14,457aj1,self.MachineLearning,Attractors in Google Translate,https://www.reddit.com/r/MachineLearning/comments/457aj1/attractors_in_google_translate/,aidanplenert,1455168552,"If you are bored, you can take two phones with the Google Translate app which has capability of the conversation translation.

Now, say something into one of them and have the other pick it up. Hitting the button for translation a couple times on either phone, the app will begin a conversation. They will feedback into each other and eventually converge to an attractor (sometimes oscillate about a certain phrase). Kind of interesting.

Enjoy",2,20,False,self,,,,,
302,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,14,457az3,self.MachineLearning,What do you want to see next in TensorFlow/skflow?,https://www.reddit.com/r/MachineLearning/comments/457az3/what_do_you_want_to_see_next_in_tensorflowskflow/,terrytangyuan,1455168742,"We will be releasing our first version of skflow soon - a simplified interface for TensorFlow. https://github.com/tensorflow/skflow

A lot of examples and functionalities were added, e.g. language models, image/text classifications using CNN, RNN/LSTM, ResNet, Seq2Seq, as well as early stopping, custom decay, out-of-core training, etc. 
https://github.com/tensorflow/skflow/tree/master/examples

What examples and functionalities do you want to see next? 

Contributors/PRs are welcomed! We want to help people using TensorFlow more easily and we now need your support/input/ideas/suggestions/etc! ",27,19,False,self,,,,,
303,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,15,457l7r,self.MachineLearning,Is there an existing sentiment analysis classifier you guys use in your projects or do you build your own?,https://www.reddit.com/r/MachineLearning/comments/457l7r/is_there_an_existing_sentiment_analysis/,DE0XYRIBONUCLEICACID,1455173950,"I'm looking to incorporate sentiment analysis in a project I'm doing to better understand some core ML topics. Basically I'd like to estimate whether a reddit comment was positive, negative, or neutral and then associate that with the number of upvotes to see whether there is a correlation.

EDIT: Woops, I ended up finding several APIs online. Is there a general consensus on which ones are better for which kinds of data or which ones are better in general?

EDIT2: Wow these are really cool. Thank you guys!",5,3,False,self,,,,,
304,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,18,4580jb,oataaaagiaa.qofest.com,a ATTENTION! On this site a lot-of people who want to find a sexy adventure! a,https://www.reddit.com/r/MachineLearning/comments/4580jb/a_attention_on_this_site_a_lotof_people_who_want/,buetricpasne,1455183427,,0,1,False,default,,,,,
305,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,18,4582s0,self.MachineLearning,Overview of Optimization Algorithms,https://www.reddit.com/r/MachineLearning/comments/4582s0/overview_of_optimization_algorithms/,themoosemind,1455184682,"I've recently learned a bit about neural networks and found that there are a couple of alternatives to standard gradient descent. Here is an overview.

edit: I can't change the title, but I was thinking about optimization algorithms for neural networks (mainly multi-layer perceptrons).

## Gradient Descent

* Stochastic: [Imgur](http://i.imgur.com/IeAEt4Z.png)
* Mini-Batch: [Imgur](http://i.imgur.com/JktJp2O.png)
* Learning Rate Scheduling:
    * Momentum: [Imgur](http://i.imgur.com/MGWost4.png)
    * [RProp](https://en.wikipedia.org/wiki/Rprop) and the mini-batch version RMSProp
    * [AdaGrad](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad)
    * Exponential Decay Learning Rate
    * Performance Scheduling
    * Newbob Scheduling
* [Quickprop](https://en.wikipedia.org/wiki/Quickprop)

## Alternatives

* [Quasi-Newton method](https://en.wikipedia.org/wiki/Quasi-Newton_method)
    * [BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)
    * L-BFGS
* [Conjugate gradient](https://en.wikipedia.org/wiki/Conjugate_gradient_method)
* Nesterov Accelerated Gradient (NAG) (What is a good paper / blog article explaining this one?)
* Adadelta (What is a good paper / blog article explaining this one?)
* [Adam](http://arxiv.org/abs/1412.6980) (Adaptive Moment Estimation)
    * AdaMax
* Genetic algorithms
* Simulated Annealing
* [Twiddle](https://martin-thoma.com/twiddle/)

Are there more? Are there papers which compare them or [graphical explanations like this](http://imgur.com/a/Hqolp)?",8,18,False,self,,,,,
306,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,19,458511,self.MachineLearning,Newton's Method for a System of Non-Linear Equations in 22 Lines of Python,https://www.reddit.com/r/MachineLearning/comments/458511/newtons_method_for_a_system_of_nonlinear/,seeitbelieveit,1455185945,"    import numpy as np
    from exceptions import ValueError
    
    def newtons_method_for_systems(F, J, x, TOL = 1e-08, N):
        sizeF = F.shape[0]
        k = 0
        while k &lt; N:
            tempF = np.array([])
            tempF.reshape((sizeF))
            for i in range(0, sizeF):
                tempF[i] = F[i](x)
            tempJ = np.matrix([[],[]])
            tempJ.reshape((sizeF,sizeF))
            for i in range(0, sizeF):
                for j in range(0, sizeF):
                    tempJ[i][j] = J[i][j](x)
            y = np.linalg.solve(tempJ, -tempF)
            x = x + y
            if max(y) &lt; TOL:
                return x
            k = k + 1
        raise ValueError('maximum number of iterations exceeded-the procedure was unsuccessful')
",2,1,False,self,,,,,
307,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,19,4586iq,linkedin.com,Does Machine Learning Allow Opposites to Attract?,https://www.reddit.com/r/MachineLearning/comments/4586iq/does_machine_learning_allow_opposites_to_attract/,BigCloudTeam,1455186783,,0,0,False,default,,,,,
308,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,20,458a02,self.MachineLearning,How much does a Deep Learning Researcher get paid in industry today?,https://www.reddit.com/r/MachineLearning/comments/458a02/how_much_does_a_deep_learning_researcher_get_paid/,nonap_,1455188781,"Basic, mid level, pro researchers/engineers in Deep Learning. 

At all major corporate labs (DeepMind, Baidu, FAIR.. )

Asking for a friend of course ;)

(If you are one, do reply with a throw away account)",69,55,False,self,,,,,
309,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,20,458et0,youtube.com,"Bayesian Probabilistic Modeling (5 lectures by Ghahramani, 2014)",https://www.reddit.com/r/MachineLearning/comments/458et0/bayesian_probabilistic_modeling_5_lectures_by/,Kiuhnm,1455191553,,1,19,False,http://b.thumbs.redditmedia.com/5zduWE4AuMEFCr4H2F1UBaHN1i3gTbA5jQ5uDnNBdEY.jpg,,,,,
310,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,21,458kd2,arxiv.org,A Convolutional Neural Network for Modelling Sentences,https://www.reddit.com/r/MachineLearning/comments/458kd2/a_convolutional_neural_network_for_modelling/,sidsig,1455194442,,4,31,False,default,,,,,
311,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,23,458vln,developer.nvidia.com,CuDNN v4 has been released,https://www.reddit.com/r/MachineLearning/comments/458vln/cudnn_v4_has_been_released/,shmel39,1455199268,,11,31,False,http://b.thumbs.redditmedia.com/cuNRxbDJb4bq3rsnrtr_fIGYCKimxSikxCea2VDV5Fw.jpg,,,,,
312,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,23,458vsy,biganalytics.me,15 Artificial Intelligence Real Time Applications You Need to Know | Apply Big Analytics,https://www.reddit.com/r/MachineLearning/comments/458vsy/15_artificial_intelligence_real_time_applications/,ApplyBigAnalytics,1455199337,,1,1,False,default,,,,,
313,MachineLearning,t5_2r3gv,2016-2-11,2016,2,11,23,4593ac,self.MachineLearning,ReLU Neural Nets on Small Data,https://www.reddit.com/r/MachineLearning/comments/4593ac/relu_neural_nets_on_small_data/,-TrustyDwarf-,1455202104,"Could someone explain why a tiny neural network with 8 ReLUs + 1 sigmoid unit works so well on that (raw) data?

It reaches 100% test set accuracy after training on only 500 samples (with 1000 features), while other models reach less than 100% accuracy with 10k samples.

https://gist.github.com/anonymous/4ca328a826d6ccdaac12

This uses an artificial data set, but I noticed the same on a real data set a few days ago and wanted to verify by implementing this test...",5,9,False,self,,,,,
314,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,0,459911,ri.cmu.edu,Improving Multi-step Prediction of Learned Time Series Models (Data as Demonstrator),https://www.reddit.com/r/MachineLearning/comments/459911/improving_multistep_prediction_of_learned_time/,sidsig,1455204079,,1,8,False,default,,,,,
315,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,0,459bgh,self.MachineLearning,Trouble getting start for supervised learning for document analysis,https://www.reddit.com/r/MachineLearning/comments/459bgh/trouble_getting_start_for_supervised_learning_for/,zhavey,1455204872,"I am new to computer vision, I'm trying to implement a system that can upload invoices to a system (with the end goal it can provide an aggregated, visualised method of all items purchased for a business).

I'm trying to create a technique that can capture certain parts of the invoice by the computer examining the image and extract the info in a structured (i.e. JSON) format. Things like the items, the prices, sent from, the date sent etc.

I have training data from a business which I took pictures of myself (about 1000 images of invoices with reasonable quality). A few questions if anyone could be so kind to help me...

Is the approach better to try and extract individual components? I.e. a computer vision program that can detect an address as opposed to the whole structure and build it up from there?

Would this be done by having ground truth data of a boundary where a component is in an image with the text that it is?

Or is there a more generalised approach? I was reading papers with a suggestion of the LogitBoost classifier for a whole document but honestly I'm not able to make any sense of it. Is this always a task of supervised classification?

There is a lot of different types of invoices and I'm confused as to how go about a generalised way that could give a reasonable amount of accuracy.

My apologies for coming across as a noob (I totally am at this, but keen to learn) and have found the resources such as OpenCV, Ocropus very overwhelming in understanding how they work. If anyone has any links to any good resources I'd be so appreciative! Thanks!",0,1,False,self,,,,,
316,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,0,459cqw,blog.evorithmics.org,Parallel Optimization of Independent Deeply Contingent Components,https://www.reddit.com/r/MachineLearning/comments/459cqw/parallel_optimization_of_independent_deeply/,[deleted],1455205258,[deleted],0,1,False,default,,,,,
317,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,0,459fgt,blog.evorithmics.org,Evolutionary Optimization of Independent Deeply Contingent Components in Parallel,https://www.reddit.com/r/MachineLearning/comments/459fgt/evolutionary_optimization_of_independent_deeply/,kburjorj,1455206153,,6,2,False,http://b.thumbs.redditmedia.com/tnQRNvmxAzEYAMy712c7-o1GevngyQ2E_8dNPcGP6Es.jpg,,,,,
318,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,1,459i4w,self.MachineLearning,Batch Normalization for convolutional layers in Theano,https://www.reddit.com/r/MachineLearning/comments/459i4w/batch_normalization_for_convolutional_layers_in/,deephive,1455207001,"In a typical convolutional neural network, do we implement batch normalization for all the layers or just the output / fully connected layers ? ",1,0,False,self,,,,,
319,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,1,459myj,self.MachineLearning,Spark combined with scikit-learn allows scaling ML to multiple machines like never before.,https://www.reddit.com/r/MachineLearning/comments/459myj/spark_combined_with_scikitlearn_allows_scaling_ml/,bayeslaw,1455208507,"This is amazing quite frankly:
https://databricks.com/blog/2016/02/08/auto-scaling-scikit-learn-with-spark.html",9,6,False,self,,,,,
320,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,2,459stk,experfylabs.typeform.com,"If you use Slack, join our vibrant community of Data Science, IoT, and Big Data and join our machine learning channel. Click here and you will receive an invitation.",https://www.reddit.com/r/MachineLearning/comments/459stk/if_you_use_slack_join_our_vibrant_community_of/,JyoC,1455210401,,10,0,False,http://b.thumbs.redditmedia.com/kgmv5JJhCAcNTOTGXve3T1IC83vrA1n0vU1a3n4CnNM.jpg,,,,,
321,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,2,459u5h,washingtonpost.com,What does artificial intelligence see when it watches political ads?,https://www.reddit.com/r/MachineLearning/comments/459u5h/what_does_artificial_intelligence_see_when_it/,fhoffa,1455210791,,0,0,False,http://b.thumbs.redditmedia.com/U7yJ4wXdeQZkqoVifh_pz-b3UPC5wKzqXswxHEYoewc.jpg,,,,,
322,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,2,459vp6,ianailatina.sotalkme.com,s ATTENTION! On this site a lot-of people who want to find a sexy adventure! s,https://www.reddit.com/r/MachineLearning/comments/459vp6/s_attention_on_this_site_a_lotof_people_who_want/,flicosidim,1455211261,,0,1,False,default,,,,,
323,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,3,45aeui,self.MachineLearning,Algorithm for finding similar words in text documents,https://www.reddit.com/r/MachineLearning/comments/45aeui/algorithm_for_finding_similar_words_in_text/,FutureIsMine,1455217080,"What Im looking for is an algorithm or a technique where I can automate going through a single document and replacing similar words into a single word. Lets say I have a document with ""good"", ""Great"", ""like"", I would like to group them all together into a single word lets say ""positive_mention"".",10,2,False,self,,,,,
324,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,3,45af4s,miguelgfierro.com,Is it possible for a robot to have emotions?,https://www.reddit.com/r/MachineLearning/comments/45af4s/is_it_possible_for_a_robot_to_have_emotions/,hoaphumanoid,1455217167,,4,0,False,default,,,,,
325,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,5,45ax2p,self.MachineLearning,HashedNets on RNNs?,https://www.reddit.com/r/MachineLearning/comments/45ax2p/hashednets_on_rnns/,deep_rabbit,1455222702,"Do you guys remember that [HashedNet paper](http://arxiv.org/abs/1504.04788) from almost a year ago? Whatever happened to that? Seemed like a breakthrough technique, but the code was never made available (was it?), and I haven't seen subsequent work on it. One would think it would be a fantastic addition to GRU/LSTM networks in particular -- the intrinsic memory capacity of the network is limited by the number of GRUs per layer, but the number of connections between layers goes up in O(n^2) with the number of units within each layer. HashedNets seem like the ideal way to increase network memory capacity while controlling that intractable quadratic expansion in memory requirements. But so far, nothing! What's the deal?",4,5,False,self,,,,,
326,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,6,45b35v,self.MachineLearning,Difference between adding layers and adding neurons per layer?,https://www.reddit.com/r/MachineLearning/comments/45b35v/difference_between_adding_layers_and_adding/,online204,1455224786,"I'm playing around with back propagation and I was wondering:

How does adding/subtracting layers affect the output? Does it make it more/less exact of an answer? Does it decrease the number of iterations needed to train the system?

The same goes for the number of neurons in a given layer. What kind of overall affect does it have?",29,49,False,self,,,,,
327,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,6,45b70y,self.MachineLearning,Topic modelling for web scraped data?,https://www.reddit.com/r/MachineLearning/comments/45b70y/topic_modelling_for_web_scraped_data/,techrat_reddit,1455226184,"I am trying to categorize different websites into different topics by scraping text contents and feeding them into the [LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) algorithm which spits out as many topics as I indicate. After running the algorithm a few days ago, the results were disappointing. I understand that it wasn't going to give clear, distinctive topics, but the results were simply not recognizable. Any ideas?",1,3,False,self,,,,,
328,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,6,45bafi,self.MachineLearning,Need help: generating new text from dead people's words,https://www.reddit.com/r/MachineLearning/comments/45bafi/need_help_generating_new_text_from_dead_peoples/,zebraskiin,1455227380,"Hi,

My name is James, Interaction Design Grad student at the School of Visual Arts. I have been using Nupic to design an algorithm that allows me to merge and generate new text from artists, authors, comedians and more. 

I am currently crowdsourcing any combinations (people and collaboration name) for this project. Ex:  For example, 2 Chainz + the Beatles... 2 Beatles!  If I end up going with your idea, I will list you as a contributor and you will get a print that consists of:

- text generated from a theory of the neocortex and your collaboration idea

- art (this is still in the works, but it will represent the collaboration)

Thank you.
",11,2,False,self,,,,,
329,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,9,45c1h4,self.MachineLearning,What kind of NN architecture I have to use for image recognition if there are a always new image objects but I don't want to do the whole learning process again?,https://www.reddit.com/r/MachineLearning/comments/45c1h4/what_kind_of_nn_architecture_i_have_to_use_for/,neoteat,1455237226,"I want to use a neural network  to recognize objects in an image (e.g. football, basketball, golfball). The system is working as expected but is there a possible architecture so I don't have to train the whole network with all training data again, if I add a new object (e.g. tennisball) to recognize ?",10,3,False,self,,,,,
330,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,11,45cjif,arxiv.org,"Discriminative Regularization for Generative Models - Lamb, Dumoulin, Courville",https://www.reddit.com/r/MachineLearning/comments/45cjif/discriminative_regularization_for_generative/,kkastner,1455244197,,8,17,False,default,,,,,
331,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,11,45clih,arxiv.org,"[1602.03609] Attentive Pooling Networks (IBM Watson, "" it has been applied to both convolutional neural networks (CNNs) and recurrent neural networks (RNNs)"", ""achieve state-of-the-art performance in all the benchmarks"")",https://www.reddit.com/r/MachineLearning/comments/45clih/160203609_attentive_pooling_networks_ibm_watson/,derRoller,1455244949,,2,5,False,default,,,,,
332,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,12,45cv9f,gsnidandasdiadd.sxeinject.com,n ATTENTION! On this site a lot-of people who want to find a sexy adventure! n,https://www.reddit.com/r/MachineLearning/comments/45cv9f/n_attention_on_this_site_a_lotof_people_who_want/,kingmacbetalc,1455248762,,0,1,False,default,,,,,
333,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,13,45d4u8,witicles.com,Keep Your Machinery in Optimum Working Condition. Lets Find Out How!,https://www.reddit.com/r/MachineLearning/comments/45d4u8/keep_your_machinery_in_optimum_working_condition/,jackerfrinandis,1455252734,,0,1,False,default,,,,,
334,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,13,45d59a,reddit.com,edating recommendse,https://www.reddit.com/r/MachineLearning/comments/45d59a/edating_recommendse/,darktooth2224,1455252905,,0,1,False,default,,,,,
335,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,14,45d9aq,hexahedria.com,Composing Music With Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/45d9aq/composing_music_with_recurrent_neural_networks/,Ferinex,1455254737,,14,138,False,http://b.thumbs.redditmedia.com/VUWqmSHMdIfmU3ZFQpsTFug7T6baWk_6Z57TafpUe2s.jpg,,,,,
336,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,15,45de57,self.MachineLearning,Why does layerwise whitening in neural networks helps to improve performance.,https://www.reddit.com/r/MachineLearning/comments/45de57/why_does_layerwise_whitening_in_neural_networks/,[deleted],1455256935,[deleted],0,1,False,default,,,,,
337,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,15,45dist,self.MachineLearning,What classification to use?,https://www.reddit.com/r/MachineLearning/comments/45dist/what_classification_to_use/,[deleted],1455259225,[deleted],0,0,False,default,,,,,
338,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,16,45dobm,self.MachineLearning,Top Hatlapa Compressor Spares ?,https://www.reddit.com/r/MachineLearning/comments/45dobm/top_hatlapa_compressor_spares/,Sl-Engineering,1455262242,[removed],0,1,False,default,,,,,
339,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,16,45dq0n,win-vector.com,"I would like to read your though about this: &lt;&lt; Data Science, Machine Learning, and Statistics: what is in a name? &gt;&gt;",https://www.reddit.com/r/MachineLearning/comments/45dq0n/i_would_like_to_read_your_though_about_this_data/,[deleted],1455263258,[deleted],0,1,False,default,,,,,
340,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,16,45dq85,ics.uci.edu,I would like to read your though about this &lt;&lt;Are ML and Statistics Complementary?&gt;&gt;,https://www.reddit.com/r/MachineLearning/comments/45dq85/i_would_like_to_read_your_though_about_this_are/,swentso,1455263395,,0,0,False,default,,,,,
341,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,18,45dz3h,research.microsoft.com,Modern Deep Learning through Bayesian Eyes,https://www.reddit.com/r/MachineLearning/comments/45dz3h/modern_deep_learning_through_bayesian_eyes/,thvasilo,1455269160,,0,17,False,default,,,,,
342,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,19,45e4fw,reddit.com,CHelp me! Photo of my sister! How do I remove it?C,https://www.reddit.com/r/MachineLearning/comments/45e4fw/chelp_me_photo_of_my_sister_how_do_i_remove_itc/,sue4mark3,1455272573,,0,1,False,default,,,,,
343,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,19,45e5fr,self.MachineLearning,Very cool youtube channel with short videos of research papers and general concepts,https://www.reddit.com/r/MachineLearning/comments/45e5fr/very_cool_youtube_channel_with_short_videos_of/,bayeslaw,1455273169,"Just found this guy's channel and I think he does an amazing job, check him out:
https://www.youtube.com/watch?v=ziMHaGQJuSI",0,5,False,self,,,,,
344,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,19,45e795,aaagaslsgataog.vikifur.com,n ATTENTION! On this site a lot-of people who want to find a sexy adventure! n,https://www.reddit.com/r/MachineLearning/comments/45e795/n_attention_on_this_site_a_lotof_people_who_want/,teivopine,1455274190,,0,1,False,default,,,,,
345,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,20,45e8ou,github.com,IPython notebook for Neural Artistic Style using tensorflow,https://www.reddit.com/r/MachineLearning/comments/45e8ou/ipython_notebook_for_neural_artistic_style_using/,simulacre7,1455275060,,6,30,False,http://b.thumbs.redditmedia.com/f_XkhDoMmDWlBzqaROHWJBlJmc05MIK6aApTy9Evs0o.jpg,,,,,
346,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,20,45eceb,artelnics.com,Kaggle - Higgs Boson machine learning challenge solved,https://www.reddit.com/r/MachineLearning/comments/45eceb/kaggle_higgs_boson_machine_learning_challenge/,Sergiointelnics,1455277225,,2,1,False,http://a.thumbs.redditmedia.com/cXcTGW6PWwv1TNTj8zG4fQILJF7aWm9oy9YAFr3DOd4.jpg,,,,,
347,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,21,45egf6,self.MachineLearning,How do I train a neural network to extract attributes from images?,https://www.reddit.com/r/MachineLearning/comments/45egf6/how_do_i_train_a_neural_network_to_extract/,n00bto1337,1455279381,"I want to use some open source thing first, like Tensor Flow or any other library. What I want is to extract some meaningful attributes from my images. I can give some images with attributes as a training set. How do I go about doing this?",5,1,False,self,,,,,
348,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,21,45eibn,medium.com,News in artificial intelligence and machine learning,https://www.reddit.com/r/MachineLearning/comments/45eibn/news_in_artificial_intelligence_and_machine/,nb410,1455280320,,1,3,False,http://b.thumbs.redditmedia.com/WQvBeg3dprINFae_Pq43YCmm9xl4J_kYlD1mRmVxqXg.jpg,,,,,
349,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,21,45ej8v,self.MachineLearning,Scheduling Examinations for a University,https://www.reddit.com/r/MachineLearning/comments/45ej8v/scheduling_examinations_for_a_university/,PrometheanVR,1455280781,"I'm working on a project with a small team of other students, and I'm wondering if an ML technique or a genetic algorithm would make sense to use as a solution. Essentially, we are trying to schedule examinations for all of the school's classes into a four day period. We're also trying to keep it so that an exam for a class is scheduled at a time that is close to the actual class time. For example, a morning class should simply have an exam in the morning as opposed to late in the afternoon.

The team is all experienced with programming and maths, but no one has any experience with ML or GA techniques. However, from what I've seen, everyone seems to be interested when I mentioned it. Would this problem be a good fit for ML or a GA and be feasible for an inexperienced team to complete in a few months? ",9,3,False,self,,,,,
350,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,22,45enbs,reddit.com,6Help me! Photo of my sister! How do I remove it6,https://www.reddit.com/r/MachineLearning/comments/45enbs/6help_me_photo_of_my_sister_how_do_i_remove_it6/,mulletwoman12,1455282718,,0,1,False,default,,,,,
351,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,22,45enh1,self.MachineLearning,Increasing the accuracy of a shallow deep belief network using nolearn?,https://www.reddit.com/r/MachineLearning/comments/45enh1/increasing_the_accuracy_of_a_shallow_deep_belief/,[deleted],1455282782,[deleted],0,0,False,default,,,,,
352,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,22,45esgp,self.MachineLearning,best neural network architecture for temporal-spatial data,https://www.reddit.com/r/MachineLearning/comments/45esgp/best_neural_network_architecture_for/,insider_7,1455284947,"I have labeled temporal spatial data for classification. I want to implement a supervised approach using neural networks. 
I have my data in the following 3D numpy array shape:
(samples, temporal data dimension, spatial data dimension)

What is the best suited neural network for this problem?",11,1,False,self,,,,,
353,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,23,45eyux,self.MachineLearning,What are the available feature extraction techniques (feature spaces)?,https://www.reddit.com/r/MachineLearning/comments/45eyux/what_are_the_available_feature_extraction/,[deleted],1455287616,[deleted],0,1,False,default,,,,,
354,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,23,45ezeb,arxiv.org,[1602.03616] Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/45ezeb/160203616_multifaceted_feature_visualization/,rhiever,1455287823,,9,14,False,default,,,,,
355,MachineLearning,t5_2r3gv,2016-2-12,2016,2,12,23,45f2o3,gas.paralelnipolis.cz,Gas reading with SimpleCV + Scikit,https://www.reddit.com/r/MachineLearning/comments/45f2o3/gas_reading_with_simplecv_scikit/,PaKr,1455289053,,2,19,False,http://a.thumbs.redditmedia.com/CJRVnqe5JAnEtiFkrGFsumogChsc1gAbyuuKXK2FTz0.jpg,,,,,
356,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,0,45f8xy,self.MachineLearning,Recurrent Neural Networks in R,https://www.reddit.com/r/MachineLearning/comments/45f8xy/recurrent_neural_networks_in_r/,forensics409,1455291298,"Hi all, 

I mostly use R and I have a time series problem where i want to classify a series of events as a certain type of known classes. The number of events per series does not have to be equal. I thought RNNs would be a good approach to solving this problem. Are there any good packages in R for RNNs?",12,2,False,self,,,,,
357,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,0,45fa9o,youtube.com,Why Montezuma Revenge doesnt work in DeepMind Arcade Learning Environment,https://www.reddit.com/r/MachineLearning/comments/45fa9o/why_montezuma_revenge_doesnt_work_in_deepmind/,com2mentator,1455291781,,37,30,False,http://b.thumbs.redditmedia.com/LtAL_LUAIFwXFJDL-7ID5ultA0vvudLeRGfoFsX470s.jpg,,,,,
358,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,1,45fgxh,srinimf.com,3 IT Skills Make You King Maker in Digital Career Age,https://www.reddit.com/r/MachineLearning/comments/45fgxh/3_it_skills_make_you_king_maker_in_digital_career/,ApplyBigAnalytics,1455294135,,1,1,False,default,,,,,
359,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,2,45ftke,self.MachineLearning,Is there any evidence to suggest that a trained NN is stuck in a local minima?,https://www.reddit.com/r/MachineLearning/comments/45ftke/is_there_any_evidence_to_suggest_that_a_trained/,[deleted],1455298508,[deleted],5,0,False,default,,,,,
360,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,3,45g351,self.MachineLearning,Which dedicated program for machine learning do you use ?,https://www.reddit.com/r/MachineLearning/comments/45g351/which_dedicated_program_for_machine_learning_do/,PyBet,1455301707,"I am a Python programmer and there are libraries for machine learning like scikit-learn and others. 

I was wondering what is your experience with dedicated programs for machine learning.",10,0,False,self,,,,,
361,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,3,45g7mw,imgur.com,Seems fairly accurate.,https://www.reddit.com/r/MachineLearning/comments/45g7mw/seems_fairly_accurate/,the320x200,1455303205,,1,0,False,default,,,,,
362,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,4,45gfyy,icrunchdatanews.com,Bloomberg Head of Data Science Gideon Mann Talks Machine Learning and Financial Data,https://www.reddit.com/r/MachineLearning/comments/45gfyy/bloomberg_head_of_data_science_gideon_mann_talks/,lokator9,1455306126,,0,1,False,http://a.thumbs.redditmedia.com/izXfOjVWTSiwGqQEBsm_9DzFVJLYNCarLphAglhGZh0.jpg,,,,,
363,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,4,45gguc,charmingdevice.com,Stop Calling It Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/45gguc/stop_calling_it_artificial_intelligence/,lokator9,1455306410,,6,0,False,http://a.thumbs.redditmedia.com/nHd00cDllvcBFJs8BZEEjuv_JBzgpZBdIyC1ZgMHdC0.jpg,,,,,
364,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,5,45gkoy,self.MachineLearning,Has anyone read this Neural Networks and Deep Learning free online book by Michael Nielsen?,https://www.reddit.com/r/MachineLearning/comments/45gkoy/has_anyone_read_this_neural_networks_and_deep/,gooeyn,1455307732,"I was googling for any material about neural networks and deep learning and I found this website: http://neuralnetworksanddeeplearning.com/ 
Has anyone read this book before? As I don't know much about neural networks and deep learning I can't tell it's a good book or not. 
It was published last year. Looks really good though, there are animations explaining the relation between cost and epochs, etc 

I just finished the Andrew's course about Machine Learning and started Geoffrey Hinton's Neural Network course. I would appreciate if you guys could also send me any materials you have about neural networks and deep learning :)",20,78,False,self,,,,,
365,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,5,45gsy9,algen.com,Get Body Mass Index Scales with Accuracy,https://www.reddit.com/r/MachineLearning/comments/45gsy9/get_body_mass_index_scales_with_accuracy/,algenscale,1455310639,,1,0,False,default,,,,,
366,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,6,45gxfs,algen.com,Range of scales for counting,https://www.reddit.com/r/MachineLearning/comments/45gxfs/range_of_scales_for_counting/,algenscale,1455312145,,1,1,False,default,,,,,
367,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,7,45h7ie,self.MachineLearning,ML and P vs NP,https://www.reddit.com/r/MachineLearning/comments/45h7ie/ml_and_p_vs_np/,[deleted],1455315472,[deleted],8,0,False,default,,,,,
368,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,7,45hcp5,self.MachineLearning,L2 norm layer with learned scaling factor in Torch?,https://www.reddit.com/r/MachineLearning/comments/45hcp5/l2_norm_layer_with_learned_scaling_factor_in_torch/,adagrad,1455317332,Has there been an implementation of the [ParseNet](http://arxiv.org/pdf/1506.04579.pdf) L2 norm layer in Torch?  I've seen several implementations for an L2 norm (such as [here](https://gist.github.com/karpathy/f3ee599538ff78e1bbe9)). ,2,5,False,self,,,,,
369,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,7,45hdj8,self.MachineLearning,Neural networks and theorem proving,https://www.reddit.com/r/MachineLearning/comments/45hdj8/neural_networks_and_theorem_proving/,[deleted],1455317633,[deleted],9,5,True,nsfw,,,,,
370,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,8,45hf7e,self.MachineLearning,can anyone please help me with this question? What test should I use when two sample sizes are different,https://www.reddit.com/r/MachineLearning/comments/45hf7e/can_anyone_please_help_me_with_this_question_what/,kailovesdata,1455318269,[removed],5,0,False,default,,,,,
371,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,8,45hlpf,github.com,Deep Visual Analogy-Making Implementation in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/45hlpf/deep_visual_analogymaking_implementation_in/,carpedm20,1455320741,,3,22,False,http://b.thumbs.redditmedia.com/idHCXNrU-ATqB-i59sj5JIgbn0yf5o0c_gcmvu5Btdo.jpg,,,,,
372,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,10,45i2xr,toddmoses.com,Beginners Guide to AI,https://www.reddit.com/r/MachineLearning/comments/45i2xr/beginners_guide_to_ai/,mosesnc,1455327621,,7,3,False,default,,,,,
373,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,11,45idly,self.MachineLearning,"Optimization techniques comparison in Julia: SGD, Momentum, Adagrad, Adadelta, Adam (x-post from r/Julia)",https://www.reddit.com/r/MachineLearning/comments/45idly/optimization_techniques_comparison_in_julia_sgd/,[deleted],1455332275,[deleted],0,1,False,default,,,,,
374,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,12,45if2c,self.MachineLearning,"Optimization techniques comparison in Julia: SGD, Momentum, Adagrad, Adadelta, Adam (x-post from r/Julia)",https://www.reddit.com/r/MachineLearning/comments/45if2c/optimization_techniques_comparison_in_julia_sgd/,int8blog,1455332893,"Hello r/machinelearning,

This is my attempt to implement and experiment with various optimization techniques in application to neural networks' parameters space search. The code is written in Julia (not the best code one could write though - fully aware of that) 

http://int8.io/comparison-of-optimization-techniques-stochastic-gradient-descent-momentum-adagrad-and-adadelta/

I hope you will enjoy! 

",6,9,False,self,,,,,
375,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,14,45ixou,self.MachineLearning,Newbie needs help identifying data mining algorithm,https://www.reddit.com/r/MachineLearning/comments/45ixou/newbie_needs_help_identifying_data_mining/,DataManager112,1455341591,"We don't do any data mining at all yet but I think we have everything we need to get started.

I have a system with approximately 200,000 transactions.  Each transaction is XML with 48 attributes.  Of the 200,000 transactions about 20,000 were rejected.  I want to use a data mining algorithm to assign a value of the percentage chance that the very next transaction will be rejected by the user.  The algorithm needs to compare the new transaction against rejected transactions and look for data that we don't yet know is there.
I expect that in the future new criteria will cause the users to reject a transaction.  Therefore I want to use an algorithm to compare the new transaction against the rejected ones and determine the likelihood of the user rejecting it.
I was told to look into the Apriori Algorithm for this, but I can't make it work.
Database is Oracle 11g and application is MS .NET

THANKS!",2,2,False,self,,,,,
376,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,15,45j33j,self.MachineLearning,ELI5: conditional random fields and how they are used in named entity recognition,https://www.reddit.com/r/MachineLearning/comments/45j33j/eli5_conditional_random_fields_and_how_they_are/,colinroberts,1455344633,"I'm very interested in using a NER in an application of mine, and the Stanford NER is amazing, but doesn't go into much detail about how it works. Can anyone give me a very basic summary, and then point me in the direction of where to learn more?",3,4,False,self,,,,,
377,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,16,45j8xa,what-dog.net,Much more impressive than the age calculator,https://www.reddit.com/r/MachineLearning/comments/45j8xa/much_more_impressive_than_the_age_calculator/,WideFlatFabric,1455348044,,22,44,False,http://b.thumbs.redditmedia.com/FzpR2n-kwnR_6SqKPOMGIASs7_VTJDLm6Qi7b-vVXAE.jpg,,,,,
378,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,17,45jgjc,reddit.com,0gPhoto of my sister! How do I remove it0g,https://www.reddit.com/r/MachineLearning/comments/45jgjc/0gphoto_of_my_sister_how_do_i_remove_it0g/,camelguard4,1455352695,,0,1,False,default,,,,,
379,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,17,45jgjp,arxiv.org,A dataset of more than ten thousand 3D scans of real objects,https://www.reddit.com/r/MachineLearning/comments/45jgjp/a_dataset_of_more_than_ten_thousand_3d_scans_of/,nickl,1455352700,,2,58,False,default,,,,,
380,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,19,45jqoo,self.MachineLearning,Question regarding Sentiment Analysis,https://www.reddit.com/r/MachineLearning/comments/45jqoo/question_regarding_sentiment_analysis/,xristos_forokolomvos,1455359464,I have been seeing this term popping up here and there a lot. Where is it usually applied?,1,1,False,self,,,,,
381,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,19,45jr2v,thebark.com,Dogs and Machine Learning Come Together in an App,https://www.reddit.com/r/MachineLearning/comments/45jr2v/dogs_and_machine_learning_come_together_in_an_app/,Ayan2004,1455359718,,0,1,False,default,,,,,
382,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,19,45js3e,theblogee.com,Robotic Self driving Bus to be tested in Australia,https://www.reddit.com/r/MachineLearning/comments/45js3e/robotic_self_driving_bus_to_be_tested_in_australia/,Ayan2004,1455360348,,0,1,False,default,,,,,
383,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,20,45jyi8,self.MachineLearning,What can an algorithm look like that chooses appropriate images or songs?,https://www.reddit.com/r/MachineLearning/comments/45jyi8/what_can_an_algorithm_look_like_that_chooses/,Romek_Buch,1455364371,"My goal is to automatically create a slideshow of images if either a directory of images or an audio file is given. In the former case, software should find an appropriate audio file from a library. In the latter case, software should find appropriate pictures from a library. Definition of appropriate: Ultimately, what fits in most humans' perception. There are warm and comforting songs that should be accompanied by warm and comforting images. What are warm and comforting images? Those with earth colors. This list can go on and here is where my question comes in: what can an algorithm look like that chooses appropriate images or songs?",6,0,False,self,,,,,
384,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,21,45k0pw,arxiv.org,How to Train Deep Variational Autoencoders and Probabilistic Ladder Networks,https://www.reddit.com/r/MachineLearning/comments/45k0pw/how_to_train_deep_variational_autoencoders_and/,SuperFX,1455365766,,0,27,False,default,,,,,
385,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,21,45k4yk,autoshkola-zelenograd.comg_x28te__l9jc,Here beautiful girls for sex Zw3_f_C95s_LP4m,https://www.reddit.com/r/MachineLearning/comments/45k4yk/here_beautiful_girls_for_sex_zw3_f_c95s_lp4m/,Cq6_4_gYSt,1455368038,,0,1,False,default,,,,,
386,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,22,45kcq9,hp.com,HP Fortify Revolutionizes Application Security with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/45kcq9/hp_fortify_revolutionizes_application_security/,KelseeyDonovan2_,1455370646,,0,1,False,default,,,,,
387,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,23,45khwy,reddit.com,There is an Intelligent Machines AMA currently on over at r/science,https://www.reddit.com/r/MachineLearning/comments/45khwy/there_is_an_intelligent_machines_ama_currently_on/,steelypip,1455372443,,0,13,False,http://b.thumbs.redditmedia.com/5BBt1DsFnRtFU4LYVfVj67BfkIQI0kzSOR--ECFFHbc.jpg,,,,,
388,MachineLearning,t5_2r3gv,2016-2-13,2016,2,13,23,45kmjk,self.MachineLearning,Any dataset with faces classified by emotion?,https://www.reddit.com/r/MachineLearning/comments/45kmjk/any_dataset_with_faces_classified_by_emotion/,carlos_argueta,1455373951,"Anyone knows about a public dataset with faces annotated with emotions like sadness, joy, disgust, anger, fear, surprise? I am building one but it is proving slow. Thanks",7,2,False,self,,,,,
389,MachineLearning,t5_2r3gv,2016-2-14,2016,2,14,0,45l16n,meethubert.com,"Show ML: Hubert, a Nest-inspired application for the Phillips Hue. Partially new to machine learning, welcoming feedback",https://www.reddit.com/r/MachineLearning/comments/45l16n/show_ml_hubert_a_nestinspired_application_for_the/,NightClubSub,1455378790,,0,4,False,http://b.thumbs.redditmedia.com/lNGeON7gg4q0QBiUjaxhi5uXyigaD_D3VxSxjpymHVs.jpg,,,,,
390,MachineLearning,t5_2r3gv,2016-2-14,2016,2,14,1,45l35v,self.MachineLearning,Searching source on genetic algorithms designing processors exploiting magnetic fields of the processor's curent,https://www.reddit.com/r/MachineLearning/comments/45l35v/searching_source_on_genetic_algorithms_designing/,Zantary,1455379445,"A while back I read an article about some processors, designed through gentic algorthims. At somepoint the manufacturers wondered about the design because it looked quite counter-intuitive. 

Upon further investigation it turned out that the reason the processors ended up doing the right calculations was because of some weird ""physics exploit"" where one part of the processor had effect on an other because of induced magnetism or somthing along those lines.

I'm currently working on a presentation about AI and it's possible consequences and would like to bring up this example.

Does anyone have a source to what I described so I can factcheck my memories?",3,10,False,self,,,,,
391,MachineLearning,t5_2r3gv,2016-2-14,2016,2,14,1,45l6ik,intournament.com,Sexy girls for hot sex here f_8X3gA_,https://www.reddit.com/r/MachineLearning/comments/45l6ik/sexy_girls_for_hot_sex_here_f_8x3ga/,5Qq__9Kpz_3RJb_,1455380540,,0,1,False,default,,,,,
392,MachineLearning,t5_2r3gv,2016-2-14,2016,2,14,1,45lco8,self.MachineLearning,How to predict a curve/sequence of fixed size by knowing a non-fixed number points/parts of the sequence using neural networks?,https://www.reddit.com/r/MachineLearning/comments/45lco8/how_to_predict_a_curvesequence_of_fixed_size_by/,qwertz_guy,1455382134,"I hope the title comes close to what I want to ask: I have different curves in 2d characterized by points (x_i, y_i) (e.g. i = 1,...,n) and given m (&lt;n) points (x_i, y_i) of such a curve, I would like to predict the whole curve, i.e. (x_1,y_1),...,(x_n,y_n). So my hope is that the confidence/correctness of the prediction scales with the number of points I'm providing. I know that a Kalman-Filter could be better for this, but I'm explicitely trying to solve this with a neural network.

I've done some googling and as I expected, Recurrent NNs look like what I am looking for, but the examples I found so far only take a single input, i.e. a single character from a word and then try to predict the next character.

Is the problem I'm trying to solve solvable with RNNs? Have you seen some similar projects/paper/examples that aim to  do something like that (link?)? I would appreciate some help/keywords/pointers.


edit: I've used Keras for CNNs so far, so I have some basic pre-knowledge about neural networks. I'm just pretty unexperience with RNNs.",1,3,False,self,,,,,
393,MachineLearning,t5_2r3gv,2016-2-14,2016,2,14,2,45llly,xiangjiang.live,Here is a tutorial on Restricted Boltzmann Machines,https://www.reddit.com/r/MachineLearning/comments/45llly/here_is_a_tutorial_on_restricted_boltzmann/,xiangjiangacadia,1455384912,,3,44,False,http://b.thumbs.redditmedia.com/kqrJ5s4xIfbytBlZSnPKj4Uq6cn-4iHOfVh35s6QUmU.jpg,,,,,
394,MachineLearning,t5_2r3gv,2016-2-14,2016,2,14,2,45llog,bitbet.us,AlphaGo favourite to defeat Lee Sedol in prediction Market,https://www.reddit.com/r/MachineLearning/comments/45llog/alphago_favourite_to_defeat_lee_sedol_in/,heltok,1455384931,,25,45,False,default,,,,,
395,MachineLearning,t5_2r3gv,2016-2-14,2016,2,14,3,45lqpt,self.MachineLearning,How to get clusters from ecommerce clickstream data,https://www.reddit.com/r/MachineLearning/comments/45lqpt/how_to_get_clusters_from_ecommerce_clickstream/,quadenoob,1455386680,"Ill be working on a project where i'll have to generate consumer persona's based on clickstream clusters. My initial idea is along the lines of what these guys did here - check from 2:40 to 3:50 https://www.youtube.com/watch?v=kH6CFnbR_14 

Do you guys have any tips or suggestions?
Thanks
",0,3,False,self,,,,,
396,MachineLearning,t5_2r3gv,2016-2-14,2016,2,14,3,45ltur,spectrum.ieee.org,Deep Learning Makes Driverless Cars Better at Spotting Pedestrians,https://www.reddit.com/r/MachineLearning/comments/45ltur/deep_learning_makes_driverless_cars_better_at/,lokator9,1455387775,,0,4,False,http://b.thumbs.redditmedia.com/raH0Styh0DeRO5QDU0o1wNk65LFRgb665aIHkPfNYDQ.jpg,,,,,
397,MachineLearning,t5_2r3gv,2016-2-14,2016,2,14,3,45lx3q,self.MachineLearning,Bachelor topic: combining different classifiers to improve results?,https://www.reddit.com/r/MachineLearning/comments/45lx3q/bachelor_topic_combining_different_classifiers_to/,[deleted],1455388871,[deleted],5,0,False,default,,,,,
398,MachineLearning,t5_2r3gv,2016-2-14,2016,2,14,6,45mm3u,self.MachineLearning,Weighted Projection in Adagrad Update rule,https://www.reddit.com/r/MachineLearning/comments/45mm3u/weighted_projection_in_adagrad_update_rule/,mr_robot_elliot,1455397408,Can anyone explain why the weighted projection is necessary in Adagrad update rule(at the end of 2nd page). Please find the paper [here](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf). ,6,2,False,self,,,,,
399,MachineLearning,t5_2r3gv,2016-2-14,2016,2,14,6,45mm4h,arxiv.org,[1601.06180] On the Latent Variable Interpretation in Sum-Product Networks,https://www.reddit.com/r/MachineLearning/comments/45mm4h/160106180_on_the_latent_variable_interpretation/,downtownslim,1455397413,,0,2,False,default,,,,,
400,MachineLearning,t5_2r3gv,2016-2-14,2016,2,14,6,45mnbc,self.MachineLearning,Advice on multi-gpu RNN Attention Encoder Decoder in Tensorflow,https://www.reddit.com/r/MachineLearning/comments/45mnbc/advice_on_multigpu_rnn_attention_encoder_decoder/,rescue11,1455397749,"I want to use two GPUs to increase the model size limit encountered with a single gpu, but it's difficult to see how to do this.
I have tried to create two GRUcell layers on each gpu, but this does not increase the model size limit in any way. Is this because at some point Tensorflow concatenates all hidden states into one vector? 
Any help or advice would be much appreciated.
",3,6,False,self,,,,,
401,MachineLearning,t5_2r3gv,2016-2-14,2016,2,14,7,45n7v3,self.MachineLearning,Naive Python vs. scikit-learn,https://www.reddit.com/r/MachineLearning/comments/45n7v3/naive_python_vs_scikitlearn/,techrat_reddit,1455404143,"What would be the difference between using the two? I am trying to implement Latent Dirichlet allocation, but I see [one from Naive Python](https://pypi.python.org/pypi/lda) and [another from scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html). ",7,1,False,self,,,,,
402,MachineLearning,t5_2r3gv,2016-2-14,2016,2,14,8,45na42,self.MachineLearning,How to add additional weight to words in vectorizer?,https://www.reddit.com/r/MachineLearning/comments/45na42/how_to_add_additional_weight_to_words_in/,[deleted],1455404955,[deleted],1,2,False,default,,,,,
403,MachineLearning,t5_2r3gv,2016-2-14,2016,2,14,8,45nd44,reddit.com,1eImportant SEX Dating1e,https://www.reddit.com/r/MachineLearning/comments/45nd44/1eimportant_sex_dating1e/,burningchamp4,1455406096,,0,1,False,default,,,,,
404,MachineLearning,t5_2r3gv,2016-2-14,2016,2,14,8,45neu5,self.MachineLearning,Is there any code for the implementation of the recent super-resolution papers?,https://www.reddit.com/r/MachineLearning/comments/45neu5/is_there_any_code_for_the_implementation_of_the/,[deleted],1455406765,[deleted],1,13,False,default,,,,,
405,MachineLearning,t5_2r3gv,2016-2-14,2016,2,14,12,45o9g0,reddit.com,ANEW SEXA,https://www.reddit.com/r/MachineLearning/comments/45o9g0/anew_sexa/,black7legion5,1455419339,,0,1,False,default,,,,,
406,MachineLearning,t5_2r3gv,2016-2-14,2016,2,14,14,45oru3,self.MachineLearning,Best Framework to code up and experiment with RNNs?,https://www.reddit.com/r/MachineLearning/comments/45oru3/best_framework_to_code_up_and_experiment_with_rnns/,theano_and_chill,1455427585,"Hi,

I would be doing a lot of crazy things with RNNs, LSTMs and GRUs. Which framework would you recommended for such an exploratory study? 

I would be prototyping on a CPU Machine and then using it on GPU machine to scale up things.

I think I'd be great to be able to pull out any part of the network and see what's happening. I do not think that this is possible in Symbolic/compiled libraries.

Also, I've seen that Nervana GPU is crazy fast. Anyone tried this with RNNs? With FP16?

(To be clear, i want to modify LSTMs for example, so a library with prewritten objects for this does little help. But having an overall highly flexible set of classes for loss functions, optimizers, linear projections world certainly help)

I'm looking at Torch, Theano, MXNET (which apparently doesn't exist), or brainstorm (?). Or i could build on CPU/GPU backends provided in Neon.

What would you recommend/what are you using?

Best,
theano_and_chill",11,4,False,self,,,,,
407,MachineLearning,t5_2r3gv,2016-2-14,2016,2,14,14,45ouqs,self.MachineLearning,Help with text feature selection and regression to predict number of upvotes a post will get?,https://www.reddit.com/r/MachineLearning/comments/45ouqs/help_with_text_feature_selection_and_regression/,[deleted],1455428891,[deleted],0,0,False,default,,,,,
408,MachineLearning,t5_2r3gv,2016-2-14,2016,2,14,22,45qofb,self.MachineLearning,Why do we use RBMs?,https://www.reddit.com/r/MachineLearning/comments/45qofb/why_do_we_use_rbms/,akgoel,1455457216,"So RBMs are stochastic neural nets,whose units activate based on the probability given by the activation function for them.

So why would we want this behaviour, instead of Autoencoders, that simply output the value of the activation function?",8,4,False,self,,,,,
409,MachineLearning,t5_2r3gv,2016-2-14,2016,2,14,22,45qpxf,self.MachineLearning,Is there a good Text to Speech trianingset?,https://www.reddit.com/r/MachineLearning/comments/45qpxf/is_there_a_good_text_to_speech_trianingset/,fimari,1455457830,"I want to fool around with RNN for speech generation, are there any good collections of readings?",27,40,False,self,,,,,
410,MachineLearning,t5_2r3gv,2016-2-14,2016,2,14,23,45qsdm,westdeliverycenter.com,Sex with hot Girls? Yes?! Here 3Xa__9Ak5Sn,https://www.reddit.com/r/MachineLearning/comments/45qsdm/sex_with_hot_girls_yes_here_3xa_9ak5sn/,Sj8_7_rLk3A_,1455459043,,0,1,False,default,,,,,
411,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,0,45r1q7,self.MachineLearning,WARNING!! On this site all the people want sex!,https://www.reddit.com/r/MachineLearning/comments/45r1q7/warning_on_this_site_all_the_people_want_sex/,coJeh2,1455462260,[removed],0,1,False,default,,,,,
412,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,0,45r9es,efavdb.com,Independent component analysis,https://www.reddit.com/r/MachineLearning/comments/45r9es/independent_component_analysis/,efavdb,1455464650,,0,12,False,http://b.thumbs.redditmedia.com/P_31rOCN5TBIaSEpIe2ZVT3kJCZCP7eDXjlFJf8QSJo.jpg,,,,,
413,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,1,45rftu,blog.cambridgecoding.com,Machine Learning  A gentle &amp; structured introduction,https://www.reddit.com/r/MachineLearning/comments/45rftu/machine_learning_a_gentle_structured_introduction/,DrLegend,1455466499,,3,4,False,http://b.thumbs.redditmedia.com/TR6ynwJU0qsOKe-nnqIWhHjXAdsI3DYHoPvcHZ-M3Ac.jpg,,,,,
414,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,2,45rq51,self.MachineLearning,"I entered data mining competition which will start in roughly month or so, where do I start learning?",https://www.reddit.com/r/MachineLearning/comments/45rq51/i_entered_data_mining_competition_which_will/,LoLz14,1455469987,"So the concept of the competition is that they give huge dataset, and  me and my team have to analyze it and with help of machine learning, and statistical analysis predict outcomes of given datasets (for future inputs).

I have plenty of experience in programming (Java, C, Python) and just a little bit of work in R (just made some nice charts, and know the basic syntax of language). I am studying CS and have passed all math classes thus far (so I have knowledge of linear algebra, calculus and probability and statistics). I have also met with neural networks, but they were mostly given to me as a black box as a part of a project in computer vision.

I have read the FAQ, I have looked at several courses and I'm sure this question pops up often, I apologize for that, but which course would you recommend to me the most? I think that a course suitable for my situation would be Hasti/Tibshirani's course, since they also run examples in R. But, I've heard a lot of praise on Andrew Ng's course, so I just can't make up my mind. Or are there any other courses worth mentioning (such as John Hopkins' course over at coursera)?

I have one more question which is centered around datasets. Do I start messing with them right away, or after I pass some course/courses? I realized thus far that I learn most through practical work.

Thanks in advance, and once agains, sorry if I'm asking question that is probably so often seen around here, but I really don't know where to start.",4,0,False,self,,,,,
415,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,3,45s5f7,nature.com,Using a Machine Learning Approach to Predict Outcomes after Radiosurgery for Cerebral Arteriovenous Malformations,https://www.reddit.com/r/MachineLearning/comments/45s5f7/using_a_machine_learning_approach_to_predict/,lokator9,1455474289,,0,2,False,http://b.thumbs.redditmedia.com/7yGkjSCkLYG_EOe4WHmIg3IiIYR9D-s_M_dVXBKRPLE.jpg,,,,,
416,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,4,45sjxt,self.MachineLearning,What are some of the good Upcoming Machine Learning Challenges/Competitions?,https://www.reddit.com/r/MachineLearning/comments/45sjxt/what_are_some_of_the_good_upcoming_machine/,theano_and_chill,1455479293,"From a research perspective, what are some hard problems that are being tackled by getting people to compete on solutions?

Of particular interest are datasets with a Temporal Aspect,  such that RNNs can be unleashed on them.",2,6,False,self,,,,,
417,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,5,45souh,jtoy.net,on opening up machine learning to more people,https://www.reddit.com/r/MachineLearning/comments/45souh/on_opening_up_machine_learning_to_more_people/,toisanji,1455480863,,7,6,False,http://b.thumbs.redditmedia.com/CS55UEbOEvQyZnGkyIIH3SR8HfSWcLiUF0x4TYTNZdM.jpg,,,,,
418,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,6,45sxyd,self.MachineLearning,Is computer vision and natural language processing obsolete?,https://www.reddit.com/r/MachineLearning/comments/45sxyd/is_computer_vision_and_natural_language/,logrech,1455483896,"I understand that there are topics in both these broad fields that have nothing to do with the deep learning revolution. 

In terms of creating systems that have semantic understanding of images and words, is it safe to say that nlp and computer vision has nothing to offer that deep learning can't do better or more naturally? ",14,0,False,self,,,,,
419,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,9,45tsrl,maraoz.com,Training a Recurrent Neural Network to Compose Music,https://www.reddit.com/r/MachineLearning/comments/45tsrl/training_a_recurrent_neural_network_to_compose/,maraoz,1455494840,,8,19,False,default,,,,,
420,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,9,45tyo3,self.MachineLearning,Could one shot learning be done with neural networks which create neural networks? Does this make sense?,https://www.reddit.com/r/MachineLearning/comments/45tyo3/could_one_shot_learning_be_done_with_neural/,Dr_professional,1455497330,"I'm a layman so please forgive me if this is a bunch of nonsense and I'm making a fool out of myself. To be honest I'm pretty this would not work. Still I would like to hear what anyone thinks of it and if anything like it has ever been tried.

The idea goes like this:

Take a database of tagged pictures and train a CNN to recognize just one of those tags. Say one of the tags in your database is ""banana"", you use several thousand pictures of bananas, and several thousand pictures of not-bananas and train a CNN to tell you whether or not a given picture contains a banana. At the end of the training process I think it would be fair to say that this banana detecting network has a ""model"" banana within it, a neurological representation of ""banana"".

To be able to do one shot learning is to be able to take a single picture of a banana and transform it into its corresponding neurological representation. Clearly this transformation from a single example into a useful model is possible. When I first learned about star fruit I needed only a [single picture](http://i.imgur.com/fhExSgo.jpg) and I could later pick it out in a store. The problem is that we do not know the details of this transformation, so we must learn it, literally.

Take your image database and do the same thing with every tag as you did for the banana tag. You now have a very large number of single purpose neural networks, each with the same structure. Take the image database and take the weights for those tens of thousands of neural networks and organize it such that the pictures in the database correspond to the weights of the networks they generated. Just as thousands of banana pictures correspond to a single tag, ""banana"", so will these thousands of banana pictures correspond to a single set of neural net weights. Train a new neural network on this data. At the end hopefully you will have a neural network that can generate novel neural representations of images just as some networks can generate [novel linguistic representations of images](http://cs.stanford.edu/people/karpathy/deepimagesent/). That is, hopefully you will have a network that can take an image of an object and spit out another neural network that can recognize instances of that type of object.

This network could be validated by taking the novel networks that it generates and scoring them on their accuracy. You might even refine the network by training the output networks and using the differences in weights before and after as error to train the network-generating network.

At the end of all this you have a giant ensemble of thousands of neural networks that each tell you if a single type of object is in an image. If you want to detect a new object run it through the network-generating network and add the resultant network to the ensemble. [Finally, knowledge from all the networks in the ensemble can be combined into a single network](http://arxiv.org/abs/1503.02531) to avoid the ridiculously long run-times associated with such a large ensemble.

If all of the above works (this is admittedly a big if) you should be able to replace the distillation step with a neural net as well. Again it's just a transformation from two networks that collectively contain some knowledge to one network that contains the same knowledge. In fact, since you're really just just shoving the same type of data around and since all the networks in the ensemble have the same structure it should be even easier.

What you do is take all your single-purpose neural networks and create many smaller ensembles that each represent a subset of the tags from the image database. For example ensemble A would be made from the neural nets for ""car"", ""bear"", ""cactus"", ""water fountain"", etc and ensemble B would be made from ""battery"", ""peach"", ""bird"", etc. (With different combinations it would be very easy to get a very high number of these.) Now use the distillation technique to change those ensembles into individual neural nets that represent the same combination of tags. Now take these and create ensembles consisting of one of them and one single purpose network that corresponds to a tag that is not present within the combined ensemble. Finally use the distillation technique again to change these into combined nets. You now have a data set that consists of many examples of ""neural net that can recognize many things, neural net that can recognize one thing, neural network that is a combination of these two"". Train a neural network to take the former two and output the latter. This network could then be used to combine networks generated by the translation network with a preexisting classification network.

So the final architecture would look like [this](http://i.imgur.com/IRMPYSq.png). Obviously at some point either the classifier network would ""fill up"" (the number of nodes being insufficient to classify images into the number of tags its being asked to use) or the integration network wouldn't be able to cope with the complexity of such a ""full"" classifier net. You could make the system deal with more tags while maintaining backwards compatibility
with the existing transformation and integration nets by adding another ""less full"" classification net and running it in an ensemble with the ""full"" one.

Also, I have a sneaking suspicion that the initial creation / training of this thing would take a lot of computer time.",11,9,False,self,,,,,
421,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,11,45ucs3,self.MachineLearning,Computer Vision + TensorFlow CNN to detect chess pieces from online chessboard screenshots,https://www.reddit.com/r/MachineLearning/comments/45ucs3/computer_vision_tensorflow_cnn_to_detect_chess/,ChessFenBot,1455503528,"Hi guys,
I created this bot to reply to chess puzzle images on /r/chess ([introduction Xpost there](https://www.reddit.com/r/chess/comments/45u36e/created_uchessfenbot_a_machine_learning_bot_that/)) with the predicted [Forsyth-Edwards Notation](https://en.wikipedia.org/wiki/Forsyth%E2%80%93Edwards_Notation) for the board layout.
I wrote this to learn [TensorFlow](http://www.tensorflow.org) and solve a pet peeve at the same time.

**TL;DR**: 

Python script that uses computer vision to split up images into 32x32 grayscale tiles as input, collect ~9400 tiles of training data.
Train a convolutional neural network which is almost a verbatim copy of [this tutorial](https://github.com/Elucidation/tensorflow_chessbot/blob/master/tensorflow_chessbot.ipynb).

Make predictions on /r/chess image posts, these predictions are put together into a [FEN](https://en.wikipedia.org/wiki/Forsyth%E2%80%93Edwards_Notation) for the entire board, and then provides a link to an online chess analysis of that board layout.


There are 4 IPython notebooks in the **[Github with code and documentation](https://github.com/Elucidation/tensorflow_chessbot)**. I tried to make it more accessible by documenting the learning process with [fun](https://github.com/Elucidation/tensorflow_chessbot/raw/master/readme_images/overlay_lines.png) [images](https://github.com/Elucidation/tensorflow_chessbot/raw/master/readme_images/weight_KQR.png).

---

**[tensorflow_chessbot.ipynb](https://github.com/Elucidation/tensorflow_chessbot/blob/master/tensorflow_chessbot.ipynb) - Computer Vision**

Here is a screenshot with the detected lines of the chessboard overlaid, showing where we'll cut the image into tiles.

[overlay lines](https://github.com/Elucidation/tensorflow_chessbot/blob/master/readme_images/overlay_lines.png)

---

**[tensorflow_generate_training_data.ipynb](https://github.com/Elucidation/tensorflow_chessbot/blob/master/tensorflow_generate_training_data.ipynb) - Generating a dataset**

[Lichess.org](lichess.org) provides a URL interface with a FEN string that loads a webpage with that board arrayed. A nice repo called [pythonwebkit2png](https://github.com/adamn/python-webkit2png) provides a way to render webpages programmatically, allowing us to generate several (80 in thise) random FEN strings, load the URL and take a screenshot all automatically.

[random fen](https://github.com/Elucidation/tensorflow_chessbot/blob/master/readme_images/random_fen.png)

Here is 5 example tiles and their associated label, a 13 length one-hot vector corresponding to 6 white pieces, 6 black pieces, and 1 empty space.

[dataset example](https://github.com/Elucidation/tensorflow_chessbot/blob/master/readme_images/dataset_example.png)

---

**[tensorflow_learn.ipynb](https://github.com/Elucidation/tensorflow_chessbot/blob/master/tensorflow_learn.ipynb) - TensorFlow Neural Network Training &amp; Prediction**

We train the neural network on generated data from 80 lichess.org screenshots, which is 5120 tiles. We test it with 5 screenshots (320 tiles) as a quick sanity check. Here is a visualization of the weights for the white King, Queen and Rook.

[Some weights visualization](https://github.com/Elucidation/tensorflow_chessbot/raw/master/readme_images/weight_KQR.png)

Finally we can make predictions on images passed by URL, the ones from lichess and visually similar boards work well, the ones that are too different from what we trained for don't work, suggesting that getting more data is in order. Here is a prediction on the image for [this reddit post](https://www.reddit.com/r/chess/comments/45inab/moderate_black_to_play_and_win/)

[Prediction](https://github.com/Elucidation/tensorflow_chessbot/raw/master/readme_images/prediction.png)

---

**[tensorflow_learn_cnn.ipynb](https://github.com/Elucidation/tensorflow_chessbot/blob/master/tensorflow_learn_cnn.ipynb) - TensorFlow Convolutional Neural Network Training &amp; Prediction**

Built a slightly larger dataset of ~150 screenshots which is around 9600 tiles which includes randomized FEN diagrams from lichess.org, chess.com, and 2 FEN generated diagram sites.

Tested with ~78% success rate on 71 chess subreddit posts, good enough to make a first draft Reddit bot.

---

Thanks for taking the time to read this, hope this is interesting to some of you. You can see some of my recent replies on /u/ChessFenBot",21,45,False,self,,,,,
422,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,11,45uf72,self.MachineLearning,"Little knowledge, big dreams",https://www.reddit.com/r/MachineLearning/comments/45uf72/little_knowledge_big_dreams/,Hamush,1455504565,"So I want to create a word simplification machine learning algorithm, by using python and sklearn. Considering I'm new to this field, I had a few question I hoped you experts could help me out with.

1. Is this a supervised or unsupervised learning problem, and if supervised is it a discrete or continuous problem? Why?

2. What dataset should I use/compose? I have the Enron email one, but that's probably not right for this case, I'm just throwing that information out there.

3. What features should I look at and extract, how many and why?

4. This might be a stupid simple question,  but how do I extract these features? I actually have no idea. I know the CountVectorizer() class in sklearn but I doubt that's what I need in this case...

5. And lastly, assuming that word simplicity is dictated by popularity of words, for example the word 'cognizant' has very similar characteristics to word 'conscious', however is considered harder because not commonly used, is this a machine learning problem? Even if its not, Ill still do it just for the experience but I want a more accredited opinion.

Thanks you to everyone who contributes to this!! 

P.S. if you think of something that may be useful to me in my situation, please do contribute it in the comments I'll be grateful for every bit of it!",8,2,False,self,,,,,
423,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,11,45ufe7,self.MachineLearning,"MachineLearning subreddit description should start with ""shortest path to bigdata"" and not understanding how intelligence works",https://www.reddit.com/r/MachineLearning/comments/45ufe7/machinelearning_subreddit_description_should/,BenRayfield,1455504651,[removed],0,0,False,default,,,,,
424,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,12,45uj07,awesome42.com,Awesome42 The easiest way to find R packages,https://www.reddit.com/r/MachineLearning/comments/45uj07/awesome42_the_easiest_way_to_find_r_packages/,euphoris,1455506206,,1,2,False,default,,,,,
425,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,15,45v7e8,self.MachineLearning,Computer Vision + TensorFlow: How do I train my model to find visually similar images?,https://www.reddit.com/r/MachineLearning/comments/45v7e8/computer_vision_tensorflow_how_do_i_train_my/,mln00b13,1455517897,"As my final output, I need to give it an image, and it should give me back visually similar images. These will be high-res images, of the quality form ImageNet. How do I go about doing that? Is there already some model which I can use which would involve less coding from my side? Any beginner intro, apart from the MNIST digit tutorial?",2,6,False,self,,,,,
426,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,15,45va4v,self.MachineLearning,"Global and Chinese Forklift Jack Industry Trends, Share, Analysis, Growth 2010-2020",https://www.reddit.com/r/MachineLearning/comments/45va4v/global_and_chinese_forklift_jack_industry_trends/,aiden_11,1455519393,[removed],0,1,False,default,,,,,
427,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,16,45vepw,self.MachineLearning,Choose a good CUDA card. Looking for your advice.,https://www.reddit.com/r/MachineLearning/comments/45vepw/choose_a_good_cuda_card_looking_for_your_advice/,madraf,1455521989,"As the title says I'd like to consider buying a CUDA/cuDNN graphic card to experiment a bit with TF or Theano.
I'm not sure about the budget for now - I'll decide that depending on the different options and how good they seem.

What are the models having the best performance/cost ratio?
Do you have any experience?

Thank you.",18,11,False,self,,,,,
428,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,16,45vge0,services.informatik.hs-mannheim.de,some slides for Machine Learning from my university,https://www.reddit.com/r/MachineLearning/comments/45vge0/some_slides_for_machine_learning_from_my/,Deathwing1990,1455522983,,2,0,False,default,,,,,
429,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,17,45vmqs,self.MachineLearning,Help with training of similarity model,https://www.reddit.com/r/MachineLearning/comments/45vmqs/help_with_training_of_similarity_model/,akuhren,1455526763,"Hello

I'm part of a resarch team currently working on a statistical model for similarity between kick drum samples. The goal is to come up with a metric that can be used to determine how similar two samples are to human ears and then utilize this in various classification or recommendation schemes.

If any of you are interested in music and sound I invite you to take part in the training of the model on www.train-my-sampler.com. It should take roughly 10-15 minutes.

Thank you!",0,2,False,self,,,,,
430,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,20,45w23m,self.MachineLearning,RNN example from scratch with no tensor library?,https://www.reddit.com/r/MachineLearning/comments/45w23m/rnn_example_from_scratch_with_no_tensor_library/,realfuzzhead,1455535710,"I'm trying to find an example of implementing a simple RNN without using a tensor library, just so I can grok the actual process more easily. I have been having trouble finding one though, almost all the tutorials I've found default to Theano but that has all of the differentiation and backprop built in, I'm trying to learn those details. Can anyone point to any examples, preferably in python or c++. Thanks!",4,1,False,self,,,,,
431,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,21,45w64e,gist.github.com,"Notes for ""Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"" paper",https://www.reddit.com/r/MachineLearning/comments/45w64e/notes_for_batch_normalization_accelerating_deep/,shagunsodhani,1455537807,,14,17,False,http://b.thumbs.redditmedia.com/lvJHflIAg4deAzTfFTD0okJa1RZ1OQ5VAGKhaVWo8OQ.jpg,,,,,
432,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,22,45wgfp,self.MachineLearning,Need matlab code for hmm based speech recognition,https://www.reddit.com/r/MachineLearning/comments/45wgfp/need_matlab_code_for_hmm_based_speech_recognition/,mkndnk,1455542926,Does anyone here know the matlab code for hmm based speech recognition matlab code ,0,0,False,self,,,,,
433,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,22,45wie8,self.MachineLearning,Anyone else feeling overwhelmed by the fast pace of the field?,https://www.reddit.com/r/MachineLearning/comments/45wie8/anyone_else_feeling_overwhelmed_by_the_fast_pace/,onewugtwowugs,1455543834,"I'm currently an NLP Master's student working on my thesis, and throughout the years I have tried to stay updated to the best of my ability on what happens in machine learning. I have always found it quite interesting, but I have lately been feeling more and more overwhelmed by the progress of the field, to the extent that I almost feel like deleting my current stack of unread papers and simply give up. I know I am not really expected to know _everything_ that happens, but still every few weeks there are new important results that require me to go through a bunch of background readings just to understand their architecture. And this is just me focusing on neural networks -- I have literally no idea what has lately been happening in other parts of ML. This is honestly the main thing that makes me question whether I would be fit to join a PhD program.

How do you keep yourself updated in the field while simultaneously working on your own research and still having time to leave your desk every once in a while?

__EDIT__: Wow, seems like I was far from alone in this. Thank you all of for your wonderful comments -- I'm reading through all of them and taking your advice to heart!",61,201,False,self,,,,,
434,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,23,45wpaw,arxiv.org,Second Order Stochastic Optimization in Linear Time,https://www.reddit.com/r/MachineLearning/comments/45wpaw/second_order_stochastic_optimization_in_linear/,thvasilo,1455546674,,7,4,False,default,,,,,
435,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,23,45wssi,i-c.tech,Creating Automated Software Developer only through BRD without coding. Both Applications &amp; DB.,https://www.reddit.com/r/MachineLearning/comments/45wssi/creating_automated_software_developer_only/,[deleted],1455548014,[deleted],0,1,False,default,,,,,
436,MachineLearning,t5_2r3gv,2016-2-15,2016,2,15,23,45wtmb,sense2vec.spacy.io,"Demo of ""sense2vec"" (word2vec on tagged and parsed text) on Reddit comments from 2015",https://www.reddit.com/r/MachineLearning/comments/45wtmb/demo_of_sense2vec_word2vec_on_tagged_and_parsed/,syllogism_,1455548329,,10,18,False,default,,,,,
437,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,0,45wufv,i-c.tech,Create Softwares through BRD without coding. AI &amp; NLP. Both Applications &amp; DB,https://www.reddit.com/r/MachineLearning/comments/45wufv/create_softwares_through_brd_without_coding_ai/,Clancap,1455548650,,0,0,False,http://b.thumbs.redditmedia.com/nI1m995BsaCEyiYaXkEkBA75vq05lMb1LP812aONUwY.jpg,,,,,
438,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,0,45wv1r,youtube.com,automatic glass cover machine for Pressmaster Limited,https://www.reddit.com/r/MachineLearning/comments/45wv1r/automatic_glass_cover_machine_for_pressmaster/,caketraymachine,1455548871,,0,1,False,default,,,,,
439,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,0,45wvj2,self.MachineLearning,"Why does using a GPU, make deep learning training go so much faster?",https://www.reddit.com/r/MachineLearning/comments/45wvj2/why_does_using_a_gpu_make_deep_learning_training/,apple-sauce,1455549061,,14,2,False,self,,,,,
440,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,0,45wwwc,self.MachineLearning,Good RNN paper for image/vision application?,https://www.reddit.com/r/MachineLearning/comments/45wwwc/good_rnn_paper_for_imagevision_application/,[deleted],1455549583,[deleted],0,0,False,default,,,,,
441,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,0,45wzka,self.MachineLearning,NVIDIA M2090 -- 6GB Card for $200?,https://www.reddit.com/r/MachineLearning/comments/45wzka/nvidia_m2090_6gb_card_for_200/,LeavesBreathe,1455550540,"Hey Guys,

I was just shopping around for GPU's and I came across the NVIDIA [M2090](http://www.amazon.com/nVidia-M2090-Processing-Computing-Module/dp/B007ED62NU)

Something doesn't seem right to me. How can the NVIDIA 6GB 980 TI cost $650 new while this M2090 card only costs $200? 

One difference I noted was that the M2090 seems to support 1 Teraflop while the 980 TI is upwards of 6 Teraflops. 

If you wanted really cheap memory, wouldn't this card be the way to go? Compared to a $1000 Titan X, you could get 5 of these cards, and have 30gb of memory total. Not to mention the parallel compute capability...

I feel like I'm missing something big here, so wanted to ask about it here! ",20,1,False,self,,,,,
442,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,0,45x37y,youtube.com,how to packaging the cake tray,https://www.reddit.com/r/MachineLearning/comments/45x37y/how_to_packaging_the_cake_tray/,caketraymachine,1455551813,,0,1,False,default,,,,,
443,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,1,45x4uk,medium.com,"Machine Learning: An In-Depth, Non-Technical Guide  Pt 2",https://www.reddit.com/r/MachineLearning/comments/45x4uk/machine_learning_an_indepth_nontechnical_guide_pt/,innoarchitech,1455552306,,0,4,False,http://a.thumbs.redditmedia.com/sxuHqH3SsSUb2fUwMoq-uf2EsycemGwsd43NsB4UKR8.jpg,,,,,
444,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,1,45xdby,self.MachineLearning,What are the available feature extraction techniques (feature spaces)?,https://www.reddit.com/r/MachineLearning/comments/45xdby/what_are_the_available_feature_extraction/,amnghd,1455555032,"Hi.
I have the attached time series (which is extracted by first order derivation and chopping the main signal). Each of the rising or falling package of points correspond to one particular case (about 24 cases). I wonder, what would be the fastest feature domain (computationally) to classify (unsupervised) these event. The reason for the speed is the fact that the application is real time. What my supervisor suggested was maybe to go toward Hilbert transform, or Park transform, or similar feature spaces. I wonder if you could suggest any similar analysis domain that might be useful for this particular case?
link to the time series graph:https://copy.com/ezsssmpRsIBqRWBS
It is a section of the whole data. The time interval between each spike, and the duration and magnitude of them might change over time with change of condition.
",0,0,False,self,,,,,
445,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,2,45xg11,praetorian.com,Using Text Classifiers on Machine Binaries,https://www.reddit.com/r/MachineLearning/comments/45xg11/using_text_classifiers_on_machine_binaries/,joshmatterhorn,1455555857,,1,3,False,default,,,,,
446,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,2,45xhm1,self.MachineLearning,Efficient Torch word embedding tutorials?,https://www.reddit.com/r/MachineLearning/comments/45xhm1/efficient_torch_word_embedding_tutorials/,olBaa,1455556367,"I'm trying to get myself more familiar with Torch, are there any good word-level embedding repositories that produce compatible speed with the original/gensim/tensorflow implementations? I am just sure that someone has done that, but I could not find the repo. Thanks in advance!",2,1,False,self,,,,,
447,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,3,45xtgp,self.MachineLearning,How to load weights from .hdf5 file in keras?,https://www.reddit.com/r/MachineLearning/comments/45xtgp/how_to_load_weights_from_hdf5_file_in_keras/,wadhwasahil,1455560142,I know the procedure to load weights in a sequential model. But how to do that in a graphical is unknown to me. ,7,0,False,self,,,,,
448,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,3,45xycp,self.MachineLearning,How do I go about building a visual search model?,https://www.reddit.com/r/MachineLearning/comments/45xycp/how_do_i_go_about_building_a_visual_search_model/,n00bto1337,1455561775,"Something similar like Pinterest's, but I want to use an existing model, which I can then train on my images, and edit the main model as per my needs.",2,2,False,self,,,,,
449,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,4,45y6yr,chesttales.com,Super Girls for sex!8p_A_7Di2t_Pw,https://www.reddit.com/r/MachineLearning/comments/45y6yr/super_girls_for_sex8p_a_7di2t_pw/,wA_6_m8MS_s93R,1455564510,,0,1,False,default,,,,,
450,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,4,45ycae,self.MachineLearning,Can anyone please explain Neural Autoregressive Distribution Estimator to me?,https://www.reddit.com/r/MachineLearning/comments/45ycae/can_anyone_please_explain_neural_autoregressive/,anglrphish,1455566034,"Hi!
The [paper](http://jmlr.csail.mit.edu/proceedings/papers/v15/larochelle11a/larochelle11a.pdf) in question is quite an old one now, I guess. I tried to tackle it multiple times, and yet while I think I understand the first 3 sections about RBMs, sigmoid belief nets and mean field approximation of the conditionals, the rest (starting from section 4) looks like a complete mistery to me. Like:


 - *""...consider the application of the aforementioned mean-field procedure for only one iteration""* - why is that suddenly enough? We needed ~20 iterations for each mean field update just before, right?

 - *""...since there is one neural network for each conditional""* - actually, where do these networks come from? We've just talked about Boltzmann machine, haven't we?

 - why hidden units h1, h2, h3 etc are combined in groups/clusters one-per-each-visible unit (figure 1)?

 - why there are tied weights?


I think I'm just missing something here, just a bit of intuition.",3,0,False,self,,,,,
451,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,5,45yiyt,slackprop.wordpress.com,Machine Learning Roller Derby Names,https://www.reddit.com/r/MachineLearning/comments/45yiyt/machine_learning_roller_derby_names/,tatou27,1455568067,,0,2,False,http://a.thumbs.redditmedia.com/s1YfZCf1Fjt0x1IVcM2qHVxl0s7YaJRNceJOFxZFFH0.jpg,,,,,
452,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,5,45yj70,self.MachineLearning,Tensorflow NaN error,https://www.reddit.com/r/MachineLearning/comments/45yj70/tensorflow_nan_error/,AwesomeDaveSome,1455568135,"I'm trying to run the Cifar-10 code of tensorflow, but with my own images (slightly larger, 424x424x3, but that's not causing memory issues as of now). I'm using the exact same code as the tensorflow tutorial, all that I changed is the sizes of the images. I get the following error message when the gradient should be computed/optimized: ReluGrad input is not finite. : Tensor had NaN values.

I tried to change the Optimizer (It was GradientDescent, changed it to Adam), and for the Adam optimizer I also changed the epsilon value. Changing the epsilon to a lower value caused the code to run more steps, but did not fix the bug, just delay it. Also tried reducing the learning rate, no success either. What is this bug connected to? Is there anything else I could try to change in order to get the code working, no matter for how many steps (with GradientDescent it runs for about 80 steps, with AdamOptimizer and epsilon value of 10^-9 for 800)?",13,0,False,self,,,,,
453,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,6,45yrde,self.MachineLearning,A Potentially Ignorant Question,https://www.reddit.com/r/MachineLearning/comments/45yrde/a_potentially_ignorant_question/,_ArrogantAsshole_,1455570501,"Hi all,

I am an experienced programmer with very little knowledge of machine learning. I am hoping to find out what the ideal algorithm would be for my particular scenario:

I have several thousand data points. Each data point is made up of about 15-30 individual floating point numbers. These data points are not naturally sequenced, but I would like to be able to predict the top 5 most likely to be correct data points given the current data point. In other words, I will have a current data point and need 5 new data points returned from my list of data points, each of which should be a good match to be next in the sequence given the current data point, preferably with the amount of confidence for each. In order to train, I would like to start with a random data point and then the next data point can be chosen at random. I will then manually accept or reject this data point (or potentially rate it using a floating point number), and over time, I would like for the my preferences to be learned.

Another way of looking at what I want to do is to think of Pandora (the music streaming service). However, in my case, rather than trying to predict songs the user will like based on whether they give thumbs-up or thumbs-down, I would like to predict what a good next song will be given the current one.

I guess there's several questions I have:

1. What would be a good algorithm for doing this?
2. Given the size of the set (several thousand data points composed of around 15-30 floating point numbers each), is it possible to do this in approximately real time (let's say a maximum of 5 seconds of processing time) to choose the next song or update the model?


Thanks for your help.",4,0,False,self,,,,,
454,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,6,45yvjc,self.MachineLearning,Trouble constructing an LSTM graph in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/45yvjc/trouble_constructing_an_lstm_graph_in_tensorflow/,wweber,1455571977,"I'm learning about using RNNs, so I decided to experiment by building a network that predicts the next character in a text sequence given the previous ones. However, I'm running into an error building the graph: 

    ValueError: Incompatible shapes for broadcasting: TensorShape([Dimension(None), Dimension(70)]) and TensorShape([Dimension(2), Dimension(5)])`.

This happens when instantiating the gradient descent optimizer. Interesting to note, is that the None x 70 tensor is the size of the hidden state, and the 2 x 5 is the size of a [batch x hidden]. I'm certain it's because I've messed up some parameter somewhere (probably a reshape/concat). If anyone would mind taking a look, that would be helpful.

https://gist.github.com/waweber/43eeb4087a73176c1a15",4,0,False,self,,,,,
455,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,7,45z5dq,self.MachineLearning,"Demis Hassabis talk ""The Technology of Artificial Intelligence"" at the AAAS 2016 annual meeting",https://www.reddit.com/r/MachineLearning/comments/45z5dq/demis_hassabis_talk_the_technology_of_artificial/,dhpt,1455575047,"Apparently he gave the [talk](https://aaas.confex.com/aaas/2016/webprogram/Paper17168.html) on Saturday. Unfortunately it doesn't look like there is a recording available at this point. Did anyone attend/watch the live stream?

I became aware of this through this [article](http://www.geekwire.com/2016/alphago-lee-sedol-whos-underdog-in-google-ai-million-go-match/) on GeekWire. Interesting quote (I presume from talk): They give us a less than 5 percent chance of winning [the match against Lee Sedol]  but what they dont realize is how much our system has improved. Its improving while Im talking with you.",2,14,False,self,,,,,
456,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,8,45zfbd,raffle55.com,Here - 2g_Hw_C78Qa_W,https://www.reddit.com/r/MachineLearning/comments/45zfbd/here_2g_hw_c78qa_w/,s_6S4L_z_nY3T,1455578079,,0,1,False,default,,,,,
457,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,8,45zfmz,self.MachineLearning,Energy function of fuzzy sorted list by weight on a&lt;b vs a&gt;b,https://www.reddit.com/r/MachineLearning/comments/45zfmz/energy_function_of_fuzzy_sorted_list_by_weight_on/,BenRayfield,1455578203,"When sorting lists by priority or dependency network, there can be cycles, a tangled mess of ideas or you must do x before y before z before x. We deal with cycles by sacrificing some dependencies to make others consistent.

These connections can be expressed as a negative symmetric weight between each pair.

weight(index(a)&lt;index(b)) = -weight(index(a)&gt;index(b))

The energy function of a permutation is sum of plus or minus each weight, depending which is earlier than the other in the list, only counting pairs where both exist in the list.

It doesnt matter how far apart they are, only the boolean a&lt;b vs a&gt;b to choose to add or subtract that weight.

There may be many possible ways to solve the energy function with specific permutations. Its not hill climbable by comparing each 2 adjacent in the list and swapping based on that change in energy, because you may do that paying a small cost that allows another adjacent swap that more than pays for it but cant swap across both.

Example: All have weight 1: a&lt;b, b&lt;c, c&lt;d, d&lt;e, e&lt;a

Solutions include:

abcde

bcdea

cdeab

deabc

eabcd

All of those subtract 4 of the weights and add the pair at the end, so 1-4 = -3. There is no lower energy solution.

Imagine a bunch of such loops tangled with eachother. How would you explore possible orders to find a lowest or near lowest energy, an observed list which the energy function well describes?",2,1,False,self,,,,,
458,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,9,45zt9r,self.MachineLearning,Looking for datasets of short stories.,https://www.reddit.com/r/MachineLearning/comments/45zt9r/looking_for_datasets_of_short_stories/,Cortexelus,1455583294,"I'm looking for 500+ short stories, preferably fiction. Short means under 7500 words.",4,4,False,self,,,,,
459,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,10,45zwse,self.MachineLearning,"I want to make a ""crawler"" of some kind: a program that looks for, finds and gathers links of Youtube videos of the same type. Is it better to do with or without machine learning?",https://www.reddit.com/r/MachineLearning/comments/45zwse/i_want_to_make_a_crawler_of_some_kind_a_program/,KatamoriHUN,1455584632,"Question given. I already set up a NodeJS Express server, and got a consistent idea, what I'm looking for. Though, Youtube search is not a help in the matter at all, and all the videos I want to group are from a lot of different users.

I asked the how-to [in the datascience sub](https://www.reddit.com/r/datascience/comments/45zv0j/i_want_to_make_a_crawler_of_some_kind_a_program/), what I want to ask in this sub is that: is machine learning involved in this problem? If yes, how?",7,3,False,self,,,,,
460,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,12,460lvl,reddit.com,bHHELPbH,https://www.reddit.com/r/MachineLearning/comments/460lvl/bhhelpbh/,sl5maddog5005,1455594326,,0,1,False,default,,,,,
461,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,14,4611pq,quora.com,What are the best toy datasets to help visualise and understand classifier behaviour?,https://www.reddit.com/r/MachineLearning/comments/4611pq/what_are_the_best_toy_datasets_to_help_visualise/,[deleted],1455601066,[deleted],1,0,False,default,,,,,
462,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,15,4618wn,reddit.com,sXNEW SEXsX,https://www.reddit.com/r/MachineLearning/comments/4618wn/sxnew_sexsx/,jac6136,1455604472,,0,1,False,default,,,,,
463,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,16,461gke,self.MachineLearning,Do people work on instance segmentation?,https://www.reddit.com/r/MachineLearning/comments/461gke/do_people_work_on_instance_segmentation/,infstudent,1455608828,"Most papers in the domain of visual recognition seem to be about object classification, object detection and semantic segmentation. Papers about instance segmentation seem to be scarce. For example [the awesome deep vision list of deep learning resources for computer vision](https://github.com/kjw0612/awesome-deep-vision) lists no papers about instance segmentation. 

I was wondering why this is this case, as this seems the most interesting problem in visual recognition to me.

Also, if people do work on it, does anyone know of any publicly available code to play around with?",8,3,False,self,,,,,
464,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,19,461w6e,arstechnica.co.uk,The NSAs SKYNET program may be killing thousands of innocent people,https://www.reddit.com/r/MachineLearning/comments/461w6e/the_nsas_skynet_program_may_be_killing_thousands/,shrillthrill,1455618509,,74,207,False,http://b.thumbs.redditmedia.com/WDpFDnOd6XRlmTZsoPSJ8Bkw1bnk1BCc6aW6P9-FgJo.jpg,,,,,
465,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,19,461zby,androidheadlines.com,"Andy Rubin, Androids Father, Wants Your Driving Data",https://www.reddit.com/r/MachineLearning/comments/461zby/andy_rubin_androids_father_wants_your_driving_data/,puneeth579,1455620304,,1,2,False,http://b.thumbs.redditmedia.com/XVhju2AV3H-d1LkeeILAEqLpFtP1j-Cyo3kLJIWLBBQ.jpg,,,,,
466,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,19,461zgy,girlschat1.co.nf,ATTENTION! A lot of sexual adventures ypur city here!,https://www.reddit.com/r/MachineLearning/comments/461zgy/attention_a_lot_of_sexual_adventures_ypur_city/,dretypu30763,1455620381,,0,1,False,default,,,,,
467,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,20,4620rk,arxiv.org,Probabilistic programming language Stan used to perform inference from gravitational wave observations [arXiv physics paper],https://www.reddit.com/r/MachineLearning/comments/4620rk/probabilistic_programming_language_stan_used_to/,fhuszar,1455621229,,8,18,False,default,,,,,
468,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,20,4623je,research.microsoft.com,Optimal and Adaptive Online Learning,https://www.reddit.com/r/MachineLearning/comments/4623je/optimal_and_adaptive_online_learning/,mttd,1455623027,,0,1,False,default,,,,,
469,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,21,4625u8,self.MachineLearning,English mrophology parser?,https://www.reddit.com/r/MachineLearning/comments/4625u8/english_mrophology_parser/,bbsome,1455624429,"I'm looking for a morphology parser for English language. Note that rather than needing a parser, which splits morphemes out of a word and their types, I only need to split the word to morphemes. Does anyone had any experience or idea about a good parser to do this?
Something among the lines of:
internationalization -&gt; inter - nation - al - ize - tion",1,1,False,self,,,,,
470,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,21,4628yw,self.MachineLearning,Gaussian process emulation of time-evolving variables using sci-kit learn?,https://www.reddit.com/r/MachineLearning/comments/4628yw/gaussian_process_emulation_of_timeevolving/,radhydro,1455626224,"I have a dataset where the variables are time-dependent and I want to use sci-kit learn's Gaussian process functions to emulate it.  For example, my dataset is something like:

t: 0., 0.1, 0.2, 0.3, ...    (t is time)

y: 1., 4.2, 2.5, 3.6, ...    (y is time-dependent variable)

for input parameters x1, x2.  How do I use sci-kit learn's Gaussian-process functions for this data?

Thanks!
",0,2,False,self,,,,,
471,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,21,462ahp,youtube.com,"Videos from REWORK Deep Learning Summit, San Francisco 2016",https://www.reddit.com/r/MachineLearning/comments/462ahp/videos_from_rework_deep_learning_summit_san/,matiskay,1455627018,,0,8,False,http://b.thumbs.redditmedia.com/92SkDeDEfLrbYtwda43OTBY-1elRm7Ot2aQKhvCcM_s.jpg,,,,,
472,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,22,462ill,self.MachineLearning,difference between machine learning and other adaptive systems?,https://www.reddit.com/r/MachineLearning/comments/462ill/difference_between_machine_learning_and_other/,woggabogga,1455630666,"hi - what is the difference between ""machine learning"" (TM? ;) and other adaptive systems techniques?  

mainly i'm wondering if there's a difference between these and genetic algorithms.  i get that GA are inspired by biology, use evolution as a model of adaptation, and that we can compare this sort of change in biology to the way that human beings have ""learned"" over millions of years to ""have feet"" and other sapien adaptations.

reason i'm asking: i would like to say, in a quick discussion in a paper ""machine learning and other adaptive systems"" - is this a bad way of grouping things?  will i make someone upset?

ultimately it seems like ML and GA are almost the same thing and are basically just conceptualized differently.  in fact, looking at literature, it seems like GA is even classified as a type of ML.

my apologies if this is a dumb question.",1,1,False,self,,,,,
473,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,23,462n4s,youtube.com,"POF PP PVC Shrink Film Machine for baking cup,muffin cup,tulip cup,cake ...",https://www.reddit.com/r/MachineLearning/comments/462n4s/pof_pp_pvc_shrink_film_machine_for_baking/,caketraymachine,1455632658,,0,0,False,default,,,,,
474,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,23,462n74,self.MachineLearning,Shared layers using Lasagne / Keras,https://www.reddit.com/r/MachineLearning/comments/462n74/shared_layers_using_lasagne_keras/,[deleted],1455632688,[deleted],2,1,False,default,,,,,
475,MachineLearning,t5_2r3gv,2016-2-16,2016,2,16,23,462p41,self.MachineLearning,Pros and Cons of Keras vs Lasagne for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/462p41/pros_and_cons_of_keras_vs_lasagne_for_deep/,deephive,1455633494,"Hi,

I'm deciding between Lasagne and Keras for my DL work. I can see that KEras follows a minimalist philosophy (hiding the complexities of Theano/TensorFlow) while Lasagne aims be transparent as possible, while providing a layer of abstraction to Theano. 

Are there any other differences, pros, cons between the two that I ought to know ? Advice is appreciated esp from Keras or Lasagne users / developers.",19,8,False,self,,,,,
476,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,0,462u0z,self.MachineLearning,Research help/ideas/collab for Deep Learning projects,https://www.reddit.com/r/MachineLearning/comments/462u0z/research_helpideascollab_for_deep_learning/,utkarshsimha,1455635513,"I have a lot of passion and interest towards machine learning. I have taken MOOC courses, trained DNNs, RNNs and keep reading DL papers. I want to do some solid research project which will weigh in for my grad applications in the future. Could anyone suggest any ideas or any open opportunities for collaboration?
Thanks!",4,2,False,self,,,,,
477,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,0,462w3m,datafloq.com,Are You A Data Hoarder? Learn How to Manage the Clutter,https://www.reddit.com/r/MachineLearning/comments/462w3m/are_you_a_data_hoarder_learn_how_to_manage_the/,virtualjd2015,1455636259,,0,0,False,http://b.thumbs.redditmedia.com/FrJ5X9FsBNffCO6zhkhIOt0W15kVx9Nb60qql41x1EU.jpg,,,,,
478,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,0,462ws6,self.MachineLearning,Networks with trainable amplitude of activation functions,https://www.reddit.com/r/MachineLearning/comments/462ws6/networks_with_trainable_amplitude_of_activation/,luffy_straw,1455636521,"Hi, I need help in implementation of this paper http://www.sciencedirect.com/science/article/pii/S0893608001000284 In this paper the author discuss about learning the amplitudes of activation values. Could anyone provide me hints to implement this idea. Thanks!
",5,2,False,self,,,,,
479,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,1,4633fj,google.com.ua,ATTENTION! A lot of sexual adventures ypur city here!,https://www.reddit.com/r/MachineLearning/comments/4633fj/attention_a_lot_of_sexual_adventures_ypur_city/,dreliecloud48956,1455638842,,0,1,False,default,,,,,
480,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,2,463ezi,self.MachineLearning,Feature Engineering Guide,https://www.reddit.com/r/MachineLearning/comments/463ezi/feature_engineering_guide/,chuckatx,1455642548,"Hey Y'all, A sorta noobie here in teh ML community. I have been using ML for some engineering application for a while now, but I never started using feature enginering properly. I was wondering if you guys knew any good resource for feature extraction and feature engineering in ML.

",2,2,False,self,,,,,
481,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,2,463go9,blog.stephenwolfram.com,Pictorial representation of working of IBM Watson and Wolfram|Alpha,https://www.reddit.com/r/MachineLearning/comments/463go9/pictorial_representation_of_working_of_ibm_watson/,linuxjava,1455643088,,2,1,False,http://b.thumbs.redditmedia.com/7VN0-gQ3_34z_iWgKFE-eWyS0Of2OXdAmSH86rfIopQ.jpg,,,,,
482,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,2,463i8c,self.MachineLearning,Alternative datasets for Tensorflow's Image Recognition?,https://www.reddit.com/r/MachineLearning/comments/463i8c/alternative_datasets_for_tensorflows_image/,LowLanding,1455643580,"I've been playing around a little bit with Tensorflow's image recognition, following [their tutorial](https://www.tensorflow.org/versions/v0.6.0/tutorials/image_recognition/index.html). The dataset they use classifies images to quite specific categories (ie: dog breeds as opposed to just ""dog""). Can anyone redirect me to more generic ""object"" defined datasets that I can _plug-and-play_ in to the example code?",2,2,False,self,,,,,
483,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,2,463khi,medium.com,Building Shazam for Fashion with Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/463khi/building_shazam_for_fashion_with_deep_neural/,extradeeplearning,1455644304,,0,2,False,default,,,,,
484,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,2,463lop,sentiance.com,Driving behavior modeling using smart phone sensor data,https://www.reddit.com/r/MachineLearning/comments/463lop/driving_behavior_modeling_using_smart_phone/,esurior,1455644701,,0,4,False,http://b.thumbs.redditmedia.com/4AjgPD_G7XmiNZ3yzx00q0zjb626SwEBthuVW3HhWUQ.jpg,,,,,
485,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,3,463v4i,youtube.com,Emoji recognizer neural network with Quartz Composer by Mike Matas,https://www.reddit.com/r/MachineLearning/comments/463v4i/emoji_recognizer_neural_network_with_quartz/,neoneye,1455647916,,7,31,False,http://b.thumbs.redditmedia.com/LpuXRuAuV6Qb9Q_HylgzFYs3Zy7nWmYclNUuDrZjlLg.jpg,,,,,
486,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,3,463vdg,googleresearch.blogspot.com.br,Running your models in production with TensorFlow Serving,https://www.reddit.com/r/MachineLearning/comments/463vdg/running_your_models_in_production_with_tensorflow/,cesarsalgado,1455647996,,3,34,False,http://b.thumbs.redditmedia.com/neT4OjRzD0yFICi7uxthSztxi_fPUmTXoH5sN27cZYE.jpg,,,,,
487,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,3,463wd4,self.MachineLearning,How can I create question paraphrasing without a large dataset of (question-&gt;paraphrase) mappings?,https://www.reddit.com/r/MachineLearning/comments/463wd4/how_can_i_create_question_paraphrasing_without_a/,NightFury13,1455648311,"I want to create question paraphrasing without having to train models on extensively large data (pretrained models are cool though!). Basically, I want to create a system where given a question (eg. ""what is in this image?""), I get similar questions as my output (eg. ""what are the contents of the image?"" or ""what is the image about?""). I don't have any dataset of these question-&gt;paraphrase mappings so training DeepModels is not an option here (using already existing models is totally cool though). Can someone help me out as to what can I do? Are there any existing libraries (Natural Language Generation tools, sentence-synsets, etc) that I can use? Or how can I build such a system?",1,2,False,self,,,,,
488,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,4,463zhl,lab41.org,2 Highly Effective Ways to Estimate User Location in Social Media,https://www.reddit.com/r/MachineLearning/comments/463zhl/2_highly_effective_ways_to_estimate_user_location/,amplifier_khan,1455649333,,0,9,False,default,,,,,
489,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,4,464092,self.MachineLearning,"Machine learning related master's thesis, advice greatly appreciated",https://www.reddit.com/r/MachineLearning/comments/464092/machine_learning_related_masters_thesis_advice/,ThramL,1455649597,"Hi!

I'd like some input on whether or not I'm setting myself up for a really horrible master's thesis experience. 

I'm studying a master of science in engineering physics, and my master specialization has been focused on computational mathematics and computer science. A lot of what I've done in school has been related to either numerical analysis, computational mathematics or differential equations. My statistics knowledge is limited to a basic course in mathematical statistics and probability theory. I have experience in optimization, numerical linear algebra, scientific computing, and some advanced abstract algebra and topology. Currently taking a course in computer vision as well, and my job involves some data crunching. I'm bored to death with differential equations and anything that has to do with physics really, but computational mathematics, technology and programming still makes me tick.

Lately I've been hell-bent on getting into machine learning, but my experience with machine learning specifically has been limited to either articles or pop-science related media. I have worked through approx. 4-5 weeks of Andrew Ng's Coursera course on ML. 

I applied for a couple of ML related thesis projects and I have gotten two interview offers. One is with a small company to do machine learning related data analysis related to the energy market. One is machine learning and analytics with regards to internet of things for a larger company. In both cases, I believe I will have supervisors with a large amount of both academic and professional experience related to machine learning. 

I have however been second guessing myself after a (very) short exchange with a professor at my university, and want to get some independent input on whether or not I'm setting myself up for a less than stellar thesis project. I have been honest regarding my ML experience in my applications and I have supplied them with the courses I have taken. One has not commented on my profile, one says I fit quite well. As I'm still waiting to have the interviews I don't have any details regarding either of the projects but I wanted to gather some more opinions about it. 

Is it stupid of me to try and shoe-horn ML into my master's thesis? Any input at all is appreciated.",3,9,False,self,,,,,
490,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,4,4640yq,yanirseroussi.com,Why you should stop worrying about deep learning and deepen your understanding of causality instead,https://www.reddit.com/r/MachineLearning/comments/4640yq/why_you_should_stop_worrying_about_deep_learning/,lokator9,1455649843,,1,0,False,http://b.thumbs.redditmedia.com/BN0qktOrqFlQ5pW4fJv7KFxvx5V7-8Wo5rjVPbf64Qw.jpg,,,,,
491,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,4,4644dd,tensortalk.com,Chainer implementation of BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1,https://www.reddit.com/r/MachineLearning/comments/4644dd/chainer_implementation_of_binarynet_training_deep/,[deleted],1455650982,[deleted],0,1,False,default,,,,,
492,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,5,464gik,tensortalk.com,Implementation of Word Embedding-based Antonym Detection using Thesauri and Distributional Information in NAACL2015,https://www.reddit.com/r/MachineLearning/comments/464gik/implementation_of_word_embeddingbased_antonym/,[deleted],1455655103,[deleted],0,2,False,default,,,,,
493,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,5,464icq,self.MachineLearning,Short survey on data scientist workflow (~2 mins),https://www.reddit.com/r/MachineLearning/comments/464icq/short_survey_on_data_scientist_workflow_2_mins/,holy_onasandwich,1455655720,"Hey everyone, I'm studying the various stages of making a machine learning system, would love to hear what you think are most important and fun!
http://goo.gl/forms/702SF1iK9j

edit: spreadsheet with responses so far - https://docs.google.com/spreadsheets/d/1A45dfE3_LLbv5k5ooLdDxpYjQO9KP98eP_lUvgIk7Ss/edit?usp=sharing",2,1,False,self,,,,,
494,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,7,4654rd,youtube.com,DanDoesData: More Scikit Flow live streaming,https://www.reddit.com/r/MachineLearning/comments/4654rd/dandoesdata_more_scikit_flow_live_streaming/,vanboxel,1455663507,,0,6,False,default,,,,,
495,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,8,465as0,github.com,TensorFlow 0.7.0 is now released with cuDNN R2/R3/R4 support,https://www.reddit.com/r/MachineLearning/comments/465as0/tensorflow_070_is_now_released_with_cudnn_r2r3r4/,[deleted],1455665730,[deleted],0,2,False,default,,,,,
496,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,9,465eoa,waylonflinn.github.io,Weblas' Deep NN in a Browser: fast enough?,https://www.reddit.com/r/MachineLearning/comments/465eoa/weblas_deep_nn_in_a_browser_fast_enough/,waylonflinn,1455667337,,2,6,False,default,,,,,
497,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,9,465ji5,googleresearch.blogspot.com,Running your models in production with TensorFlow Serving,https://www.reddit.com/r/MachineLearning/comments/465ji5/running_your_models_in_production_with_tensorflow/,cryptoz,1455669298,,0,1,False,http://b.thumbs.redditmedia.com/neT4OjRzD0yFICi7uxthSztxi_fPUmTXoH5sN27cZYE.jpg,,,,,
498,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,10,465ncr,self.MachineLearning,Why do CRF features use the label information?,https://www.reddit.com/r/MachineLearning/comments/465ncr/why_do_crf_features_use_the_label_information/,napsternxg,1455670813,"In the implementation of many CRF systems and also in the notation of CRF features in many papers, the feature functions are shown to depend on the following variables:
- Sequence 
- Index of token
- Previous label
- Current label

However, in a general supervised learning setting the labels are the unknown and need to be predicted. If we assume a Linear chain CRF to be a sweeping algorithm from start of sequence to end of sequence, then the use of the previous label as an input to feature makes sense, however, I am not able to understand the idea behind the usage of the current label. 

How does this work on unlabeled sequences ?

I do understand that the transition probabilities are also learned by the CRF by giving weights to the transition. I tried to plot the weights learned by CRF for the transition features: http://imgur.com/IME2bgT, how are these weights used during the inference process when we don't know the actual labels of the sequence items. 

Any explanation or sources for understanding this concept are greatly appreciated.

",1,3,False,self,,,,,
499,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,10,465oh1,self.MachineLearning,Need help in implementing Linear Regression,https://www.reddit.com/r/MachineLearning/comments/465oh1/need_help_in_implementing_linear_regression/,Vainsingr,1455671263,"I have started to learn linear regression and right I am implementing it without the scikit inbuilt function but rather calculating the using the coefficients. 

I have used the formula (X.TX)^1(X.Ty) to calculate b. Now what/How would be the next step to fit the data in linear model.

I as using python language.Any help will be great or any reference/links where I can find would be helpful too .

",5,2,False,self,,,,,
500,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,10,465w29,self.MachineLearning,Deep learning for depth map estimation from stereo images,https://www.reddit.com/r/MachineLearning/comments/465w29/deep_learning_for_depth_map_estimation_from/,wowholdonwhat,1455674202,"Just wanted to share and get feedback on a project I have been working on. This is a fully convolutional neural network (Theano/Lasagne) that estimates depth maps from stereo images. 

It takes in a couple of grayscale stereoscopic images concatenated along the channel axis into a single tensor , and outputs a single image representing the depth map.
 A series of Convolutional and maxpooling layers followed by a series of upscaling and deconvolutional layers allows the network to extract image disparity features at the smaller scale (object edges), and generate a smooth estimate of the depth map at the larger scale (full object). 
I think that the main advantage of this technique over other methods from the computer vision research (based on an explicit computation of image disparity) is its robustness, in particular the the fact that it is able to produce smooth estimates of the depth map even on textureless region.
 Here is a link to my github page for this project, with example input/output images: 

https://github.com/LouisFoucard/StereoConvNet

I checked whether the net was truly learning stereo features by giving it two identical left images, instead of the stereo couple left/right images (similar to depthmap estimation from single images, which has been explored recently). In this case, there are no disparities between the 2 images, and it completely throws the depth map estimation off: objects are not even registered, and the background is off as well. I think this proves that the net is indeed learning stereo features from the disparity between the left and right images.

 Please let me know what you think/what improvements do you think it needs. I am now working on improving my stereo image dataset (random virtual 3d scenes generated with blender and a python script: https://github.com/LouisFoucard/DepthMap_dataset ), and am going to try to train the network on a dataset of real stereoimages/lidar scans. ",14,28,False,self,,,,,
501,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,12,466866,self.MachineLearning,example non-overlap hypothesis set H to make union bound equals vc bound,https://www.reddit.com/r/MachineLearning/comments/466866/example_nonoverlap_hypothesis_set_h_to_make_union/,wordchao,1455678784,"consider the union bound, when H consists of only h1 and h2,

Pr[|Ein(g)-Eout(g)|&gt;e]

&lt;=Pr[|Ein(h1)-Eout(h1)|&gt;e or |Ein(h2)-Eout(h2)|&gt;e] eq.1

&lt;=Pr[|Ein(h1)-Eout(h1)|&gt;e] + Pr[|Ein(h2)-Eout(h2)|&gt;e] (eq.2

=2exp(-2Ne^2)              (eq.3 )

i wonder which kind of h1 and h2 can more ""accurately"" satisfy the union bound, maybe this means union bound equals vc bound.

in other words, can i find h1 and h2 to let (eq.1)=(eq.2)?

Pr[|Ein(h1)-Eout(h1)|&gt;e or |Ein(h2)-Eout(h2)|&gt;e]

=Pr[|Ein(h1)-Eout(h1)|&gt;e and |Ein(h2)-Eout(h2)|&gt;e] (eq.a)

+Pr[|Ein(h1)-Eout(h1)|&gt;e and |Ein(h2)-Eout(h2)|&lt;=e] (eq.b)

+Pr[|Ein(h1)-Eout(h1)|&lt;=e and |Ein(h2)-Eout(h2)|&gt;e] (eq.c)


if (eq.2)=(eq.1), then,


sup(eq.a)=0

sup(eq.b)=sup(eq.c)=1/2(eq.3)=exp(-2Ne^2 ) 

but i cannot find the example h1&amp;h2, which means h1 and h2 has no overlap when considering the union bound. 
can u help?  thanks.",0,2,False,self,,,,,
502,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,12,4669nn,arxiv.org,Benefits of depth in neural networks [1602.04485],https://www.reddit.com/r/MachineLearning/comments/4669nn/benefits_of_depth_in_neural_networks_160204485/,SuperFX,1455679342,,4,13,False,default,,,,,
503,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,12,4669uo,github.com,TensorFlow 0.7.0 released,https://www.reddit.com/r/MachineLearning/comments/4669uo/tensorflow_070_released/,SuperFX,1455679416,,24,88,False,http://b.thumbs.redditmedia.com/DTIc8Re5Wtr8FcYpadKuxbnQqiZfVeNkvw3yGFWnZHM.jpg,,,,,
504,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,13,466ihu,natureofcode.com,Basic Neural network tutorial,https://www.reddit.com/r/MachineLearning/comments/466ihu/basic_neural_network_tutorial/,julian88888888,1455682800,,1,3,False,http://b.thumbs.redditmedia.com/Dkdc3RCoEytdtLUZM9tlKj4SJV4Axzpoo7REcjiRw3U.jpg,,,,,
505,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,13,466m3o,medium.com,Using Transfer Learning to Classify Images with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/466m3o/using_transfer_learning_to_classify_images_with/,st553,1455684360,,5,9,False,http://b.thumbs.redditmedia.com/VVXaBcuSNUVvwjAxqYBS7RA9SIJQul5yykCbmYPCuzQ.jpg,,,,,
506,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,15,466w3p,singularityweblog.com,How You Started Working for the Machines,https://www.reddit.com/r/MachineLearning/comments/466w3p/how_you_started_working_for_the_machines/,[deleted],1455689144,[deleted],0,0,False,default,,,,,
507,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,15,466xqb,bakingcupmachine.com,Cake baking seven common errors,https://www.reddit.com/r/MachineLearning/comments/466xqb/cake_baking_seven_common_errors/,caketraymachine,1455690070,,1,0,False,default,,,,,
508,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,16,4676m4,medium.com,Facial Emotion Recognition: Single-Rule 10 DeepLearning,https://www.reddit.com/r/MachineLearning/comments/4676m4/facial_emotion_recognition_singlerule_10/,carlos_argueta,1455694881,,4,4,False,http://b.thumbs.redditmedia.com/Q87Xa5v67nLMhA_-ln21jHk0Sr7pNwWaHpSWvAPZGWY.jpg,,,,,
509,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,17,467cfh,self.MachineLearning,Open pre-trained models for speech recognition?,https://www.reddit.com/r/MachineLearning/comments/467cfh/open_pretrained_models_for_speech_recognition/,maraoz,1455698346,"Are there any state-of-the-art-ish open models for speech recognition? Or are they proprietary stuff companies don't normally share?

I've noticed best-performing models for image classification are available (like the recent winner of ILSVRC2015, ResNet by team MSRA, here: https://github.com/KaimingHe/deep-residual-networks), but haven't found any decent model for speech recognition.

For example, has anyone tried to implement the models described here: http://googleresearch.blogspot.com/2015/09/google-voice-search-faster-and-more.html?

Any resources are welcome, pre-trained or not. Thanks!",3,9,False,self,,,,,
510,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,18,467mf8,self.MachineLearning,Why (Deep) Reinforcement Learning is not a 'thing' in Robotics?,https://www.reddit.com/r/MachineLearning/comments/467mf8/why_deep_reinforcement_learning_is_not_a_thing_in/,senorstallone,1455702851,"Why is not that easy for a robot to learn a certain task? 
For example, there is nothing related to reinforcement learning implemented in ROS, although all the improvements found in the literature. ",7,2,False,self,,,,,
511,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,19,467rky,self.MachineLearning,Question about neural style and AWS,https://www.reddit.com/r/MachineLearning/comments/467rky/question_about_neural_style_and_aws/,xristos_forokolomvos,1455705537,"So I've been trying to get my hands dirty for some experimenting with the neural style algorithm. I have never used before AWS nor Neural style and my budget is very limited. 

I have the following procedure in my mind:

- Launch g2.2xlarge instance

- Install all packages from scratch

- FTP my style and target image

- Run algorithm

- FTP back result

Is there any quicker way? Like an instance where it is pre-installed? 

Also If I want to use it again, do I have to install all packages from scratch again?",9,2,False,self,,,,,
512,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,20,467xsa,self.MachineLearning,Does knowledge distillation work well for models with only two classes?,https://www.reddit.com/r/MachineLearning/comments/467xsa/does_knowledge_distillation_work_well_for_models/,cesarsalgado,1455708348,,0,2,False,self,,,,,
513,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,20,467zzi,telemaks.com,How I Made 12 365 in 26 Days Without Spending a Dime? X_q3sH9_,https://www.reddit.com/r/MachineLearning/comments/467zzi/how_i_made_12_365_in_26_days_without_spending_a/,r_7Y5A_eq2L_,1455709253,,0,1,False,default,,,,,
514,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,20,4680wg,medium.com,Best Gitter Channels for: Data Science &amp; Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4680wg/best_gitter_channels_for_data_science_machine/,kohola71,1455709615,,0,2,False,http://a.thumbs.redditmedia.com/yNAW2iFcBazwAWsDSs1_Mt5s3TH2pBLLpwCrsBwSRN8.jpg,,,,,
515,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,21,4684d1,wcombinator.com,How I Made 12 365 in 26 Days Without Spending a Dime? 9q_XR_7d_fN,https://www.reddit.com/r/MachineLearning/comments/4684d1/how_i_made_12_365_in_26_days_without_spending_a/,E_c65F_a_mQ4nT2,1455711109,,0,1,False,default,,,,,
516,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,21,468arx,youtube.com,"Videos from the REWORK Deep Learning Summit, San Francisco, January 2016 - feat Andrej Karpathy, OpenAI; Hassan Sawag, eBay; Clement Farabet, Twitter; Andrew Ng, Baidu &amp; more",https://www.reddit.com/r/MachineLearning/comments/468arx/videos_from_the_rework_deep_learning_summit_san/,reworksophie,1455713503,,0,1,False,default,,,,,
517,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,22,468f83,preinventedwheel.com,The Surprising Psychology of Proving Your Point,https://www.reddit.com/r/MachineLearning/comments/468f83/the_surprising_psychology_of_proving_your_point/,preinventedwheel,1455714655,,0,2,False,default,,,,,
518,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,22,468fae,self.MachineLearning,Questions about efficient hyperparameter tuning,https://www.reddit.com/r/MachineLearning/comments/468fae/questions_about_efficient_hyperparameter_tuning/,steelypip,1455714668,"I have a supervised learning problem I am working on and I have a couple of questions about tuning the hyperparameters efficiently.  I am using sklearn, but the questions apply to any supervised learning problem with a reasonably large dataset (In my case ~1 million samples with 72 features).

1. Since running grid search (or a randomized search with decent number of iterations) on a dataset that size is extremely time consuming, would you get a reasonable set of hyperparameters if you ran the search on a subset of the data (say 10%) then retrained on the full dataset afterwards, or will the optimal hyperparameters change as the data size grows?


2. Has anyone investigated using optimization algorithms for the search?   A basic genetic algorithm seems an obvious choice - e.g.

    generate a random list of hyperparameter values

    find the one that gives the highest score

    randomly mutate it to give a new list

    repeat until bored


My gut feeling is that this would be more effective than pure random search, but maybe it would tend to get stuck in local minima.

My google-fu has found very little on this.  I did find posts to ML forums/mailing lists from someone who said they had implemented an sklearn compatible genetic grid search, but they had not put the code up anywhere that I could find.   I may have a crack at implementing it myself, but I don't want to re-invent the wheel or spend time on something that is going to be less effective that the existing methods.



",11,5,False,self,,,,,
519,MachineLearning,t5_2r3gv,2016-2-17,2016,2,17,22,468n1l,youtube.com,"Andrej Karpathy, Research Scientist, OpenAI - REWORK Deep Learning Summit 2016 #reworkDL",https://www.reddit.com/r/MachineLearning/comments/468n1l/andrej_karpathy_research_scientist_openai_rework/,DarfWork,1455716522,,2,23,False,http://b.thumbs.redditmedia.com/XD-JVfiNPKikgBGxutPR4OM9_Pi4ZbOpVZ3vEBdDtGw.jpg,,,,,
520,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,0,46960i,self.MachineLearning,Why do convolutional neural nets have increasing dimesional layers?,https://www.reddit.com/r/MachineLearning/comments/46960i/why_do_convolutional_neural_nets_have_increasing/,jg8610,1455723150,"I've noticed that in all the CNNs I've seen, the layer size increases. Why is that?",1,1,False,self,,,,,
521,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,0,469959,self.MachineLearning,proof partial derivative of x_u and y_i in the paper: Collaborative filtering for implicit feedback datasets,https://www.reddit.com/r/MachineLearning/comments/469959/proof_partial_derivative_of_x_u_and_y_i_in_the/,[deleted],1455724245,[deleted],0,0,False,default,,,,,
522,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,1,469def,cs231n.stanford.edu,CS231n: Convolutional Neural Networks for Visual Recognition,https://www.reddit.com/r/MachineLearning/comments/469def/cs231n_convolutional_neural_networks_for_visual/,waylonflinn,1455725484,,2,6,False,http://b.thumbs.redditmedia.com/iUGqSu-CKOb-UTY9j3wblcTXjvm-K9pAQ9wThUpNVik.jpg,,,,,
523,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,1,469gtc,arxiv.org,"Generating images with recurrent adversarial networks - Im, Kim, Jiang, Memisevic",https://www.reddit.com/r/MachineLearning/comments/469gtc/generating_images_with_recurrent_adversarial/,kkastner,1455726484,,15,26,False,default,,,,,
524,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,1,469i4s,self.MachineLearning,Google Brain Residency,https://www.reddit.com/r/MachineLearning/comments/469i4s/google_brain_residency/,pyromaniac_8000,1455726830,[removed],0,2,False,default,,,,,
525,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,2,469rtc,self.MachineLearning,Removing features greatly increases performance of NN model?,https://www.reddit.com/r/MachineLearning/comments/469rtc/removing_features_greatly_increases_performance/,corn_gobbler,1455729594,"Sorry if this is a beginner question. I'm not necessarily a beginner, but this is something I've never quite understood.

I'm doing a multi-regression problem with real world messy data (i.e. we can't 100% trust all the features to be accurate). I'm using a neural network that minimizes cross entropy error, and I've found that removing some features *greatly* improves performance. For example, with feature A in the dataset, the model seemingly gets stuck at a local minima and doesn't reach above 7 or 8% accuracy (consistently over many runs), whereas removing the feature allows the network to achieve 65-70% accuracy.

This is phenomenon I've noticed across a variety of models (for example linear regression, even with L1 regularization) and problems. Random Forest is the only algorithm that comes to mind as an algorithm that is truly robust to this problem in practice.

I understand how simple linear regression could get confused by useless features (by either not being predictive of the target variable or by being highly correlated with other features), but I was under the impression that a more complex model like a neural network, or even something simple like L1 regularization, would be more robust to useless features, and learn to ignore said feature, at least theoretically.

I also understand the benefit of removing these types of features, even in complex models, the idea being that you shouldn't make the problem harder than it needs to be, and shouldn't *make* the model learn that it's redundant or useless if you know it is. What I don't get is why this happens to begin with.

What are the properties of a feature that would consistently degrade performance when present? For example, would a feature that is conditionally dependent on another variable that has bad or erratic data cause this? ",8,3,False,self,,,,,
526,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,3,46a35v,self.MachineLearning,Interactive deep neural network for digit recognition?,https://www.reddit.com/r/MachineLearning/comments/46a35v/interactive_deep_neural_network_for_digit/,MiksLus,1455733147,"Hi guys!
Aproximately a month ago someone posted in this subreddit a link to interactive deep neural network where I could draw a digit and a network guessed the digit.
This network was very well visualised and a good representation of what does the deep learning look like.
Now I'm making a presentation about data science and I wanted to include this network in the presentation so that I could show the interativeness of it when presenting. But I just can't find it. I have searched all possible ways in reddit search and google search.

Maybe someone has it bookmarked and can share it with me?

Thanks a lot!",3,8,False,self,,,,,
527,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,3,46a5ey,self.MachineLearning,coding it vs ML software,https://www.reddit.com/r/MachineLearning/comments/46a5ey/coding_it_vs_ml_software/,[deleted],1455733847,[deleted],5,1,False,default,,,,,
528,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,3,46a7ps,self.MachineLearning,We're making a NN algorithm that can generate music that people like. But we need your help!,https://www.reddit.com/r/MachineLearning/comments/46a7ps/were_making_a_nn_algorithm_that_can_generate/,MarkMellink,1455734585,"Hi /r/MachineLearning,

We're a group of students from the Netherlands who are currently working on a crowd sourced music project. The aim of the project is for our algorithm to learn what kind of melodies people like and to then use this information to create music.

Right now we've generated 10.000 short melodies, however we need these melodies to be judged by real people. We've launched a website (www.JudgeMySound.com), where people can rate these melodies. We've gotten over 14.000 judgements so far, but we need a lot more. It would be great if you would spend some time to judge a few melodies. With every judgement we'll get one step closer to being able to improve the algorithm.

We also made a short video which describes our project: https://www.youtube.com/watch?v=d2dz3BoWyTM

If you have any questions or advice regarding our project. Feel free to ask/suggest them. We're more than happy to respond :)",42,89,False,self,,,,,
529,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,4,46afk4,self.MachineLearning,Is there any normalized measure from dynamic time warping?,https://www.reddit.com/r/MachineLearning/comments/46afk4/is_there_any_normalized_measure_from_dynamic_time/,amnghd,1455736751,"Hi,

I am trying to find the similarity between two time series, but not in terms of distance, in something more sensible such as percentage of similarity. In other words I need something that shows the similarity, not dis-similarity.

The dynamic time warping gives a very good response, when trying to compare the time-series. But its distance is subjective to the duration of the time series and the magnitude of template and the query. Moreover, it shows the distance, which demonstrates the dis-similarity.

I wonder, if anyone has some way to convert the distance from DTW into some form of normalized similarity measure?

Thanks",6,3,False,self,,,,,
530,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,4,46aijd,github.com,TensorFlow improved implementation of Deep Neural Decision Forests (best ICCV 2015 paper),https://www.reddit.com/r/MachineLearning/comments/46aijd/tensorflow_improved_implementation_of_deep_neural/,[deleted],1455737732,[deleted],0,2,False,default,,,,,
531,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,5,46anhp,self.MachineLearning,How to create an ensemble in tensorflow?,https://www.reddit.com/r/MachineLearning/comments/46anhp/how_to_create_an_ensemble_in_tensorflow/,cesarsalgado,1455739338,"I am trying to create an ensemble of many trained models. All models have the same graph and just differ by its weights. I am creating the model graph using tf.get_variable. I have several different checkpoints (with different weights) for the same graph architecture and I want to make one instance model for each checkpoint. How can I load many checkpoints without overwriting the previous loaded weights? As I created my graphs with tf.get_variable, the only way I can create multiple graph is by passing the argument reuse=True. Now if I try changing the names of my graph variables enclosing the build method in a new scope (so they become non-sharable with other created graphs) before loading, then this is not going to work because the new names will differ from the saved weights and I will not be able to load it.
",2,4,False,self,,,,,
532,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,5,46ap8g,nature.com,An open access pilot freely sharing cancer genomic data from participants in Texas : Scientific Data,https://www.reddit.com/r/MachineLearning/comments/46ap8g/an_open_access_pilot_freely_sharing_cancer/,shugert,1455739918,,0,3,False,default,,,,,
533,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,5,46apdw,self.MachineLearning,Good examples of deep learning versus traditional computer vision,https://www.reddit.com/r/MachineLearning/comments/46apdw/good_examples_of_deep_learning_versus_traditional/,sl8rv,1455739973,"I'm putting together a presentation for an audience that isn't extremely knowledgable about machine learning. One of the points I'm trying to get across is the large shift that we've seen in model efficacy moving from traditional CV (HOG features, Viola-Jones, etc...) to CNNs. ImageNet is a really good example of where CNNs quickly emerged as a very strong frontrunner, but I'm having trouble finding other examples that aren't anecdotal.

Any good examples of studies actually putting the two head-to-head?",1,2,False,self,,,,,
534,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,5,46atc9,alexanderetz.com,Understanding Bayes: How to become a Bayesian in eight easy steps,https://www.reddit.com/r/MachineLearning/comments/46atc9/understanding_bayes_how_to_become_a_bayesian_in/,joshstaiger,1455741245,,1,4,False,http://b.thumbs.redditmedia.com/qJYI4Z8DPhJhePu1hS4F_tCVLB8kj2vE0S_9Wyb8JWM.jpg,,,,,
535,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,5,46aumv,theguardian.com,Demis Hassabis Guardian Profile: The superhero of artificial intelligence: can this genius keep it in check?,https://www.reddit.com/r/MachineLearning/comments/46aumv/demis_hassabis_guardian_profile_the_superhero_of/,[deleted],1455741642,[deleted],0,1,False,default,,,,,
536,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,5,46awob,theguardian.com,The superhero of artificial intelligence: can this genius keep it in check? | The Guardian,https://www.reddit.com/r/MachineLearning/comments/46awob/the_superhero_of_artificial_intelligence_can_this/,terencebroad,1455742284,,4,0,False,http://b.thumbs.redditmedia.com/-agFK8qKc4aX_l5GAR82WzafHxX1bJwDBrXh7jkDOvc.jpg,,,,,
537,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,6,46b8dz,self.MachineLearning,"What does ""debugging"" a deep net look like?",https://www.reddit.com/r/MachineLearning/comments/46b8dz/what_does_debugging_a_deep_net_look_like/,hazard02,1455746023,"I've heard people say that researchers spend more time debugging deep neural nets than training them. If you're a practitioner using a toolkit like TensorFlow or Lasagne, you can probably assume the code for the gradients, optimizers, etc is mostly correct.

So then what does it mean to debug a neural network when you're using a toolkit like this? What are common bugs and debugging techniques?

Presumably it's more than just tuning hyperparameters?",10,14,False,self,,,,,
538,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,8,46bli9,xprize.org,IBM Watson AI XPRIZE: A Cognitive Computing Competition,https://www.reddit.com/r/MachineLearning/comments/46bli9/ibm_watson_ai_xprize_a_cognitive_computing/,ScientiaOmniaVincit,1455750265,,4,15,False,http://b.thumbs.redditmedia.com/0CepjK-gdaetJ2qkFtWyAgglmQQsDEaUA6ZRREGCpZE.jpg,,,,,
539,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,9,46bvh5,self.MachineLearning,How useful is a validation set?,https://www.reddit.com/r/MachineLearning/comments/46bvh5/how_useful_is_a_validation_set/,[deleted],1455753772,[deleted],3,1,False,default,,,,,
540,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,10,46c7en,self.MachineLearning,deep learning in finance,https://www.reddit.com/r/MachineLearning/comments/46c7en/deep_learning_in_finance/,solololol,1455758416,"I've done a fair bit of googling and reading about machine learning as used in finance (say, by hedge funds). However, I'd like to know more specifically about deep learning techniques in the area as this is what I work on.

I do realize it's a very secretive industry ... so any information would be appreciated.",39,26,False,self,,,,,
541,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,11,46cfyu,self.MachineLearning,Is there a library for CRFs?,https://www.reddit.com/r/MachineLearning/comments/46cfyu/is_there_a_library_for_crfs/,bourbondog,1455761814,Is there a general purpose library that I can use for conditional random fields? Any language works as long as the code is available.,5,7,False,self,,,,,
542,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,12,46cti9,self.MachineLearning,Are there any good resources for multi-target or multi-output regression?,https://www.reddit.com/r/MachineLearning/comments/46cti9/are_there_any_good_resources_for_multitarget_or/,blowhouse,1455767153,"I am having trouble finding this. Most tutorials online cover Neural Nets, SVMs, linear and logistic regression all concerning one output variable - guess housing price, guess the digit, classify as a 0 or a 1. I can't find many instances of multiple outputs. 

For instance - say I know the price of a car (and maybe some other stuff). I want to predict the miles on it and the year it was created. Note that these 2 output variables are dependent on each  other so it may not work to just run regression twice and add the results separately. ",18,4,False,self,,,,,
543,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,14,46da1e,bakingcupmachine.com,Muffin Cup Machine Manufacturer,https://www.reddit.com/r/MachineLearning/comments/46da1e/muffin_cup_machine_manufacturer/,caketraymachine,1455774025,,0,0,False,default,,,,,
544,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,15,46dde6,bakingcupmachine.com,"Muffin Cup Machine,Cake Tray Machine,Baking Cup Machine Suppliers and Manufacturers",https://www.reddit.com/r/MachineLearning/comments/46dde6/muffin_cup_machinecake_tray_machinebaking_cup/,caketraymachine,1455775552,,0,0,False,default,,,,,
545,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,15,46dgn1,dataperspective.info,Basic Data Types in R,https://www.reddit.com/r/MachineLearning/comments/46dgn1/basic_data_types_in_r/,padmajatamada,1455777297,,0,0,False,default,,,,,
546,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,15,46dj0s,pyrolysisplant.com,Leaders in Sustainable Plastic Pyrolysis &amp; Plastic to Oil Machinery,https://www.reddit.com/r/MachineLearning/comments/46dj0s/leaders_in_sustainable_plastic_pyrolysis_plastic/,PriyankaDisale,1455778535,,1,1,False,default,,,,,
547,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,16,46dkmx,self.MachineLearning,Unsupervised learning using pre-trained models?,https://www.reddit.com/r/MachineLearning/comments/46dkmx/unsupervised_learning_using_pretrained_models/,smith2008,1455779422,I was wondering whether there is some research in this regards. I envision a model where you train a classifier of (let say) images. Then you throw more unlabeled images to it and use the output as starting point for clustering. So the trained model won't really recognize the new object but it would generate similar (close) results for two similar (unseen before) objects.,12,8,False,self,,,,,
548,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,16,46dm2h,muffincupmachine.com,What are the advantages of curling cup,https://www.reddit.com/r/MachineLearning/comments/46dm2h/what_are_the_advantages_of_curling_cup/,caketraymachine,1455780208,,0,1,False,default,,,,,
549,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,17,46dr8p,self.MachineLearning,"cake tray machine,cake tray forming machine,cake tray making machine",https://www.reddit.com/r/MachineLearning/comments/46dr8p/cake_tray_machinecake_tray_forming_machinecake/,autocaketraymachine,1455783069,[removed],0,0,False,default,,,,,
550,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,17,46dsrd,self.MachineLearning,Semi-supervised learning - New data drown in training data,https://www.reddit.com/r/MachineLearning/comments/46dsrd/semisupervised_learning_new_data_drown_in/,hansolav91,1455783975,"I am currently developing an activity recognition system (detect walking, standing, sitting and lying) using two accelerometer sensors. Raw acceleration data as input to a CNN are now producing decent results. What I am doing now, is to personalize the models for each subject using SSL. First, a static model using the labelled data, then using SSL to retrain the model with parts of the subjects unlabelled data. The problem is that the newly labelled data does not influence the result (it seems that the amount of new data is so small that it does not impact the overall amount of data). Do you guys have any idea how I could weight the newly labelled data? So that the new data will influence the model? 
If I reduce the amount of training data, the start accuracy will improve, but not the overall accuracy. 
TLDR; How to make newly labelled data influence the model when the amount of training data is large. ",11,4,False,self,,,,,
551,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,18,46dy7b,blog.numer.ai,Introducing the Numerai Blog,https://www.reddit.com/r/MachineLearning/comments/46dy7b/introducing_the_numerai_blog/,dsernst,1455787122,,8,19,False,http://b.thumbs.redditmedia.com/paDEMQiSANRnTHoN_dU5_vdzMoJWoiFLNO9KLxq5laI.jpg,,,,,
552,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,20,46ed6w,google.com.ua,A lot of sexual adventures your city here!,https://www.reddit.com/r/MachineLearning/comments/46ed6w/a_lot_of_sexual_adventures_your_city_here/,heaasas70248,1455795770,,0,1,False,default,,,,,
553,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,20,46eejj,self.MachineLearning,How to plot a confidence heatmap in Tensorflow to show which features of the final image was important?,https://www.reddit.com/r/MachineLearning/comments/46eejj/how_to_plot_a_confidence_heatmap_in_tensorflow_to/,n00bto1337,1455796485,"I am currently using the Tensorflow ImageNet model, and I want to see which parts of my final test image were most helpful. Is there any way to plot this as a heatmap?",5,3,False,self,,,,,
554,MachineLearning,t5_2r3gv,2016-2-18,2016,2,18,22,46emqh,alloverhelp.com,Thesis Writing Service The Best Help You Can Find. Our professional thesis writers can write best thesis on any topic,https://www.reddit.com/r/MachineLearning/comments/46emqh/thesis_writing_service_the_best_help_you_can_find/,tolrase,1455800619,,0,1,False,default,,,,,
555,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,0,46f4di,arxiv.org,[1602.05561] Lexis: An Optimization Framework for Discovering the Hierarchical Structure of Sequential Data,https://www.reddit.com/r/MachineLearning/comments/46f4di/160205561_lexis_an_optimization_framework_for/,InaneMembrane,1455807704,,5,12,False,default,,,,,
556,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,1,46fitd,artelnics.com,Model selection algorithms in predictive analytics,https://www.reddit.com/r/MachineLearning/comments/46fitd/model_selection_algorithms_in_predictive_analytics/,Sergiointelnics,1455812558,,0,4,False,default,,,,,
557,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,2,46fypg,self.MachineLearning,Data Science Bowl Principle Investigators from NIH are holding an AMA at 1PM EST (x-post from /r/science),https://www.reddit.com/r/MachineLearning/comments/46fypg/data_science_bowl_principle_investigators_from/,DataScienceOverlord,1455817629,https://www.reddit.com/r/science/comments/46eglm/science_ama_series_we_are_drs_michael_hansen_and/,2,9,False,self,,,,,
558,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,4,46giv1,googlecloudplatform.blogspot.com,"Google Cloud Vision API enters Beta, open to all to try!",https://www.reddit.com/r/MachineLearning/comments/46giv1/google_cloud_vision_api_enters_beta_open_to_all/,blowjobtransistor,1455824254,,25,144,False,http://b.thumbs.redditmedia.com/QnWOy_9Bj0sN31tUnPXFn-NKZNDFool_vDcDz_7rW7w.jpg,,,,,
559,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,5,46gpuy,self.MachineLearning,Is the differences between train loss and test loss a valid regularization term? +\rho*(L(train)-L(test))^2,https://www.reddit.com/r/MachineLearning/comments/46gpuy/is_the_differences_between_train_loss_and_test/,Tokukawa,1455826503,Or am I implicitly training in the test data?,15,1,False,self,,,,,
560,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,5,46gsab,self.MachineLearning,Model sizes for sequence to sequence learning?,https://www.reddit.com/r/MachineLearning/comments/46gsab/model_sizes_for_sequence_to_sequence_learning/,syncoPete,1455827328,"I have trained a lot of sequence-to-sequence models in the last 6 months, with all kinds of recurrent encoder-decoder set-ups. I have rarely experienced the clean results reported in the most famous papers.

I am maxing out a 4GB Nvidia GPU with between 20-80 million parameters, using either very large, single layer LSTMs or deeper layers with a smaller hidden dimension.

And yet, getting a model to fit to even 20-30 thousand sequences is challenging. When sequences are &lt; 30 time-steps, it is not difficult, but training longer sequences (50+ time-steps) with batch processing seems to plateau too easily.

What size of model, in others' experience, is required for truly hardcore sequence-to-sequence learning? Have the big-guns in the field been paralleling billions of parameters across multiple GPUs?

Or am I just so far lacking that mysterious winning touch?",13,12,False,self,,,,,
561,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,5,46gwjf,ratherreadblog.com,Using SVMs to create a simple Digit Recognizer in Python,https://www.reddit.com/r/MachineLearning/comments/46gwjf/using_svms_to_create_a_simple_digit_recognizer_in/,TonyRomaRock,1455828705,,1,0,False,http://b.thumbs.redditmedia.com/vgg3vbE2w9d6A9ORt4lDfy3hNoI7yqrGF4OrcU-WDRo.jpg,,,,,
562,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,5,46gxog,pir.nlm.nih.gov,NLM Pill Image Recognition Challenge ($25k Prize),https://www.reddit.com/r/MachineLearning/comments/46gxog/nlm_pill_image_recognition_challenge_25k_prize/,captkrob,1455829068,,6,17,False,http://b.thumbs.redditmedia.com/0CqElGwvXjGTlwWte8P44btYyrWi2JsxSOJVjLc98Nk.jpg,,,,,
563,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,6,46h3i8,n2value.com,Resources to run a MRI through a CNN? xpost r/computervision,https://www.reddit.com/r/MachineLearning/comments/46h3i8/resources_to_run_a_mri_through_a_cnn_xpost/,drsxr,1455831067,,2,3,False,http://b.thumbs.redditmedia.com/YJKbrxP8ZHI_qEShIFOgB3Tvjq63EMe1j7Lrd05uzrY.jpg,,,,,
564,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,6,46h3uz,blog.numer.ai,Winners from Numerai Tournaments 1 and 2,https://www.reddit.com/r/MachineLearning/comments/46h3uz/winners_from_numerai_tournaments_1_and_2/,dsernst,1455831193,,0,0,False,http://b.thumbs.redditmedia.com/eQi80p7NqPvNRIPOsqWcHrxTzdlbNS4eMNw0jjL7F1Q.jpg,,,,,
565,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,6,46h5xl,self.MachineLearning,Try Out My Neural Network for Digit Recognition - And Add Your Handwriting,https://www.reddit.com/r/MachineLearning/comments/46h5xl/try_out_my_neural_network_for_digit_recognition/,[deleted],1455831893,[removed],0,1,False,default,,,,,
566,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,9,46hxdg,air.mozilla.org,Machine Learning in Rust - with MJ &amp; Max of Leaf &amp; Collenchyma (Rust Berlin via Air Mozilla),https://www.reddit.com/r/MachineLearning/comments/46hxdg/machine_learning_in_rust_with_mj_max_of_leaf/,mitchmindtree,1455841829,,0,7,False,default,,,,,
567,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,10,46i3f2,self.MachineLearning,Pooling over one dimension (replicating Spotify's music CNN),https://www.reddit.com/r/MachineLearning/comments/46i3f2/pooling_over_one_dimension_replicating_spotifys/,anonDogeLover,1455844084,"I'm trying to implement the CNN architecture Spotify used for processing music: http://benanne.github.io/2014/08/05/spotify-cnns.html

The only real novelty is doing 1D convolutions over MFCC audio input representations, and pooling only over the time dimension (the MFCC representations is power for 128 bands over time). I think I have been able to handle the 1D convolutions by making making the height of the kernel large enough so it can only slide in one direction. However, I don't know how to implement pooling over the time dimension only. Usually a single max value is extracted from a small region specified by the pooling kernel size, but I need to take the max over 4 time slices for each of the 128 bands. Any idea how to do this in caffe? 

Edit: log-mel-spectrograms, not ""MFCCs""",26,12,False,self,,,,,
568,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,11,46ict5,self.MachineLearning,Best cloud server hosts for machine learning?,https://www.reddit.com/r/MachineLearning/comments/46ict5/best_cloud_server_hosts_for_machine_learning/,butWhoWasBee,1455847830,"What do you guys think are the best cloud hosts for machine learning servers? Amazon seems to be popular, but I was wondering if any of you know other good options, particularly for using GPU + Theano",8,6,False,self,,,,,
569,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,11,46iium,self.MachineLearning,Faster way to optimize deep learning models with huge datasets?,https://www.reddit.com/r/MachineLearning/comments/46iium/faster_way_to_optimize_deep_learning_models_with/,awhitesong,1455850202,"How do you guys research on any deep learning models with huge datasets like a million images considering the training takes a lot of time like ~2 weeks. I have been facing this problem lately, even to check whether 96 batch size is better or 128 or to check the results from change in small hyperparameters like learning rate etc, i have to train the model again for 1 week to get to the results, which is really time consuming. Are there any better testing methods for researching such deep learning models faster?

Edit: found this recent parallelizing paper to transfer information between two different nnet structures:
http://arxiv.org/abs/1511.05641",15,3,False,self,,,,,
570,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,13,46iyt9,self.MachineLearning,LSTM with C implementation or wrapper,https://www.reddit.com/r/MachineLearning/comments/46iyt9/lstm_with_c_implementation_or_wrapper/,Jragonmiris,1455857108,"I'm trying to use an LSTM for imitation learning purposes, this is mostly just an evaluation and test, but it seems that LSTM libraries are pretty sparse and generally in Python or C++, with no wrapper options.

Are there any good ones with a C FFI wrapper? Unfortunately, I wrote most of my research code in Rust because I'm most productive in it (the type system agrees with my brain), and MOST machine learning libraries I've used in the past were fairly C wrapper FFI friendly. I'm on a bit of a deadline and while I could probably rewrite the most important bits in C++ or Python if I had to, it's really not ideal. I know you can also interface with Python through C, but I've... done this before... it's not exactly pretty. Especially trying to link with Python on MinGW. If one doesn't exist, I guess my best bet is probably just making a C FFI wrapper to my Rust functions and inventing a Python or C++ dummy entrypoint with some glue.

It doesn't have to be the 100% bestest LSTM library ever, but an ideal candidate would:

* Have a C wrapper **or** be reasonable easy to write a C wrapper over (I guess if one miraculously exists in Rust that would work too)
* Be able to backpropagate for just one time step, instead of backpropagating all the way through time.
* When evaluating at time *t*, be able to feed a fake output as recurrent input, instead of the LSTM's real output at time *t-1*.
* Ideally, have a relatively straightforward, built-in way of plugging a Deep NN's output as input features (I can just use a separate CNN library for this, though, which I'd also take suggestions for, they seem to largely have the same C++/Python issue).

The second and third criteria aren't strictly necessary, but they're something I'm really interested in testing, partially because I'm not entirely sure what the LSTM will do, and partially because I suspect LSTMs used for imitation learning will have drift problems just like almost all imitation learners, so I'd like to use something like DAGGER to see if it improves performance. I'm thinking that when an oracle intercedes, you'd like to only feed it data for that time step, and then feed the oracle's output as recurrent input so the LSTM doesn't get the wrong idea about what led to the current state.

My reasoning is that if you feed/backpropagate the entire sequence up to that point, you're implicitly training the LSTM on its own ""bad behavior"", while if you feed only the current state/action pair, you're explaining to it what to do if it finds itself in that situation again. It's plausible that instead this will just break and/or confuse the model, but I'd like to see. I'm not sure if there are any theoretical guarantees or warnings about time-local backpropagation in an LSTM.",3,2,False,self,,,,,
571,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,14,46j69l,self.MachineLearning,Is an Udacity Nanodegree in Machine Learning worth the money and time?,https://www.reddit.com/r/MachineLearning/comments/46j69l/is_an_udacity_nanodegree_in_machine_learning/,siva82kb,1455860444,"I am currently working in academia. I am considering switching careers from my field  (Bioengineering) to an industry job in Machine learning/Artificial intelligence. However, I am not from the computer science background, and have not had any formal education in ML or AI. I have a bachelor's degree in Electrical Engineering, and Master's and PhD in Bioengineering. So, my current CV does not fit the requirements of an job posting in ML or AI.

I recently came across the Udacity Nanodegree on Machine Learning, and it seemed like a good place to start learning things things that will help me land my first industry job in ML/AI. Has anyone taken an Udacity Nanodegree? Is it worth the time and money? Are there any other such options that will help me get into this new field.

PS: I have done some self study in ML and reinforcement learning, and have some basic understanding of the underlying theory.",27,16,False,self,,,,,
572,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,15,46j9bl,arxiv.org,[1602.05314] Is it possible to build a system to determine the location where a photo was taken using just its pixels?,https://www.reddit.com/r/MachineLearning/comments/46j9bl/160205314_is_it_possible_to_build_a_system_to/,downtownslim,1455862012,,6,7,False,default,,,,,
573,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,15,46jdd6,arxiv.org,[1602.05897] Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity,https://www.reddit.com/r/MachineLearning/comments/46jdd6/160205897_toward_deeper_understanding_of_neural/,iori42,1455863873,,17,22,False,default,,,,,
574,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,15,46jfmt,self.MachineLearning,How would I do back-propagation with an RNN layer stacked on-top of a regular layer?,https://www.reddit.com/r/MachineLearning/comments/46jfmt/how_would_i_do_backpropagation_with_an_rnn_layer/,realfuzzhead,1455865009,"I understand how BPTT works with a single-layer RNN, and I understand how BP works with a regular DNN, but I'm having trouble understanding how one would perform BP to train a model that had an Recurrent layer stacked before or after a regular layer. It seems like I need to perform BP through 2 dimensions, time and space. ",4,2,False,self,,,,,
575,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,16,46jid5,self.MachineLearning,Hotter areas of machine learning? Where do you think machine learning will be in the next few years?,https://www.reddit.com/r/MachineLearning/comments/46jid5/hotter_areas_of_machine_learning_where_do_you/,zzamass,1455866626,"Hi there,

I'm fairly new to programming and had recently learnt how to web scrape and parse APIs etc.

I recently stumbled across machine learning and found it really interesting. However it seems i'm a little late to the party and have a lot to catch up on, so please help me out here.

In your opinion, which are the 'hotter' areas of machine learning now, where do you think machine learning will be in the next couple of years, and should is it too late for me to start learning about machine learning now?

I read a reddit post about people saying that the newer research papers are hard to keep up with, which is why I am thinking if I should enter this field of study.

Thank you so much!

**********

**Edit**: More info about myself

I'm fairly proficient in mathematics, I'm at A level mathematics (a little harder than high school but way easier than university level)

I intend to enroll for a Computer Science degree this school year.",11,3,False,self,,,,,
576,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,19,46jyf6,melasej.com,THOUSANDS OF MEMBERS ARE LOOKING FOR CASUAL SEX IN YOUR NEIGHBOURHOOD! eCAmP32yOoS,https://www.reddit.com/r/MachineLearning/comments/46jyf6/thousands_of_members_are_looking_for_casual_sex/,worktumon,1455876808,,0,1,False,default,,,,,
577,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,21,46kf0h,self.MachineLearning,Segmenting two areas of a path.,https://www.reddit.com/r/MachineLearning/comments/46kf0h/segmenting_two_areas_of_a_path/,Joobei_,1455885268,"I have an arbitrary path. This path contains within it, a segment that is a symbol or even a sequence of letters etc. e.g.: 

https://dl.dropboxusercontent.com/u/1474654/path.png

I want to somehow detect which segment of the path is the symbolic segment and which is just a simple curved path. I understand that the symbols are themselves also curved paths but the difference from the original path is that those sections are more.. ""crowded"" with vertices and the changes in direction and curvature are more salient there. So I imagine there could be a way to reliably detect them.

The idea is to detect which segment is the ""symbol"" or ""word"" so I can then pass that through a recognizer (template based or otherwise).

Any advice would be very much appreciated.",7,0,False,self,,,,,
578,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,22,46kj23,self.MachineLearning,Deep learning applied to classification inferences,https://www.reddit.com/r/MachineLearning/comments/46kj23/deep_learning_applied_to_classification_inferences/,xristos_forokolomvos,1455887049,"So I have this dataset consisting of activity recognition from smartphone accelerometers (standing,walking,running,unknown = 4 classes) and also audio classification (voice,silence,noise,unknown) throughout many days. These are followed by some labels regarding how people who carried these smartphones slept during the previous night. Is it maybe possible that I construct a Deep Architecture that will take as inputs these values? Keep in mind they only take 4 values (0-3) for activity/audio. Are there any implications regarding the discreteness of them? Can I use 1-D convolutions maybe?",17,2,False,self,,,,,
579,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,22,46knga,propublica.org,.propublica,https://www.reddit.com/r/MachineLearning/comments/46knga/propublica/,vkvk724,1455888830,,1,10,False,http://b.thumbs.redditmedia.com/WdyuKg0zmgPufc6bKZGKRauJ8wMCFt4UybYR3XtjtyQ.jpg,,,,,
580,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,23,46kubl,self.MachineLearning,Tensorflow translate.py does not use embeddings?,https://www.reddit.com/r/MachineLearning/comments/46kubl/tensorflow_translatepy_does_not_use_embeddings/,friesel,1455891638,"Hi,

I am trying to get my head around the lstm-with-attention script in tensorflow:
https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/rnn/translate

This does not use embeddings, even though it calls embedding_rnn_seq2seq, but fed into this call are only the scalar word-IDs, not any kind of embedding vectors.

At least I cannot find the embedding parameters, nor the matrix, nor a possibility to feed pretrained embeddings. Any idea? Thx a bunch",5,4,False,self,,,,,
581,MachineLearning,t5_2r3gv,2016-2-19,2016,2,19,23,46kxu3,google.cz,of sexual partners for you from your city. Click here! htr53rtfasdDFSA,https://www.reddit.com/r/MachineLearning/comments/46kxu3/of_sexual_partners_for_you_from_your_city_click/,kraetean13095,1455893020,,0,1,False,default,,,,,
582,MachineLearning,t5_2r3gv,2016-2-20,2016,2,20,1,46lhn9,fastml.com,What next?,https://www.reddit.com/r/MachineLearning/comments/46lhn9/what_next/,lokator9,1455900264,,0,0,False,default,,,,,
583,MachineLearning,t5_2r3gv,2016-2-20,2016,2,20,2,46lkwv,imgur.com,Jeopardy Witing fr  knight in shining rmr - will d if h cms in jns or tracks!,https://www.reddit.com/r/MachineLearning/comments/46lkwv/jeopardy_witing_fr__knight_in_shining_rmr/,elpurfa22717,1455901335,,0,1,False,default,,,,,
584,MachineLearning,t5_2r3gv,2016-2-20,2016,2,20,3,46lyzl,self.MachineLearning,I'm looking for advice or mentoring to analyze images to find foliage or trees using street view type images,https://www.reddit.com/r/MachineLearning/comments/46lyzl/im_looking_for_advice_or_mentoring_to_analyze/,[deleted],1455905930,[deleted],3,0,False,default,,,,,
585,MachineLearning,t5_2r3gv,2016-2-20,2016,2,20,3,46m0qw,youtube.com,"Live, 2:30PM EST: Yoshua Bengio, ""Towards bridging the gap between deep learning and biology""",https://www.reddit.com/r/MachineLearning/comments/46m0qw/live_230pm_est_yoshua_bengio_towards_bridging_the/,pierrelux,1455906501,,19,38,False,default,,,,,
586,MachineLearning,t5_2r3gv,2016-2-20,2016,2,20,3,46m39e,keunwoochoi.wordpress.com,LSTM Realbook: Automatic generation of jazz chords progression by LSTM,https://www.reddit.com/r/MachineLearning/comments/46m39e/lstm_realbook_automatic_generation_of_jazz_chords/,keidouleyoucee,1455907313,,30,81,False,http://b.thumbs.redditmedia.com/PuuN8nV0k-9fHXIQVCqSD7YBU-FANnAkOYUvhF1Hd1U.jpg,,,,,
587,MachineLearning,t5_2r3gv,2016-2-20,2016,2,20,5,46mlz8,self.MachineLearning,[Beginner] Where do I start?,https://www.reddit.com/r/MachineLearning/comments/46mlz8/beginner_where_do_i_start/,hyappie,1455913532,"Hello /r/MachineLearning,

Have been lurking around quite a while but never subbed or posted. I am very interested in learning about Machine Learning. Started with courses on coursera and reading articles posted here. I am currently a software dev with couple of years of work experience (java at work, python for leisure). 

If I would like to move into the field of ML in couple of years, where do I start learning  and gaining skills that are required? What are the things that I need to be focusing on learning? 

Any help is appreciated. ",14,0,False,self,,,,,
588,MachineLearning,t5_2r3gv,2016-2-20,2016,2,20,6,46myc4,github.com,Hyperas: Keras + Hyperopt: A very simple wrapper for convenient hyperparameter optimization,https://www.reddit.com/r/MachineLearning/comments/46myc4/hyperas_keras_hyperopt_a_very_simple_wrapper_for/,alxndrkalinin,1455917798,,2,29,False,http://a.thumbs.redditmedia.com/mLBSPdUvIkQWdOew2O_RpL5SFAS5erVcjJBhdjFR8M0.jpg,,,,,
589,MachineLearning,t5_2r3gv,2016-2-20,2016,2,20,8,46nh93,self.MachineLearning,Any good academic sources/reviews on the topic?,https://www.reddit.com/r/MachineLearning/comments/46nh93/any_good_academic_sourcesreviews_on_the_topic/,periergia,1455924857,"Recently I have been interested in the topic of data mining, clustering algorithms and machine learning. I have had problems in separating what is an actual method and what is hand waving terminology. For example the more I looked into it I get the impression that data mining is no different than machine learning and that machine learning is not very different from classification which is essentially some form of clustering. This might be an oversimplification which I am sure will annoy people, but I'm ignorant and I am trying to learn so I apologise in advance.  

I want to separate what is general nomenclature of the field and what is the actual method applied behind the nomenclature. Is there some source I can study which is not just someone's opinion but peer reviewed and approved by the scientific community? Ideally I am looking for an academic review on the topic of machine learning and data mining and clustering etc.

Apologies in advance if this question is off the mark, please help me improve my question if that is the case.

tl:dr; Is there an academic review on the topic of machine learning and data mining and clustering etc?",3,1,False,self,,,,,
590,MachineLearning,t5_2r3gv,2016-2-20,2016,2,20,8,46nitd,self.MachineLearning,Truncated Backpropagation through time,https://www.reddit.com/r/MachineLearning/comments/46nitd/truncated_backpropagation_through_time/,speechMachine,1455925476,"Hello,

I'm trying to implement truncated backpropagation through time (TBPTT) as described in Ilya Sutskever's thesis http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf Section 2.8.6 page 23. In regular BPTT after going backward through an entire sequence during training, I understand when the existing sequence is discarded and a new one is loaded, the LSTM cell states and the gradient buffers for all the weights and biases are reset.

In implementing TBPTT I am not sure if:

a. The LSTM cell states are reset every time a weight update is made when the BPTT is run every k1 time steps backward in time for k2 time steps.
b. Are the gradient buffers reset to zero as well after an update call to the weights every k1 time steps.

Any help by means of clarifications would be appreciated. Also any pointers to reference implementations in C or C++ might be helpful.

",1,5,False,self,,,,,
591,MachineLearning,t5_2r3gv,2016-2-20,2016,2,20,10,46o0ul,self.MachineLearning,"Whats the most parallel memory efficient way to compute Contrastive Divergence, considering matrix multiply was first thought to be n^3 and is now known to be somewhere between n^2 and n^2.38",https://www.reddit.com/r/MachineLearning/comments/46o0ul/whats_the_most_parallel_memory_efficient_way_to/,BenRayfield,1455932929,"n + n matrix... (see below)

""CoppersmithWinograd algorithm that has an asymptotic complexity of O(n2.3728639)"" -- https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm

Contrastive Divergence is the best known algorithm for training https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine on chance vectors. Chance means position in each dimension ranges 0 to 1, normally the output of sigmoid(weightedSum)=1/(1+e^-weightedSum), and weightedSum is normally divided by temperature which causes vars to converge as temperature decreases, forcing absolute convergence as temperature approaches 0 and each weightedSum/temperature approaches plus/minus infinity. There are variations of this basic contrastive divergence annealing process. What they have in common is https://en.wikipedia.org/wiki/Associative_memory which is that given a vector whose sumOfSquares  (summed between each dimension of 2 scalars) is small, between 2 vectors normally used with visible nodes, statistical inference of neuralnet layers up / down / up / down / and so on, converges to a vector whose sumOfSquares compared to the input vector is small. When a boltzmann neuralnet is trained on a set of vectors, especially bit vectors (though scalar fractions can work too), the input and output generally equal with difference vanishingly small if there are enough layers and size of each layer. Its a statistically hill climbable math.

Now to the math problem which bottlenecks the industry... (or maybe GPU implementations have solved it but I havent yet heard...) Either way lets get the theory straight for a single cycle of Contrastive Divergence for 2 adjacent layers and a specific temperature...

Starting with an observation of visible nodes, statistical inference up (resulting in next higher layer matrix multiply result of scalars, and weighted coin flip of those chances), then learn positively on multiply of 2 1d matrix, then statistical inference down a layer (and weighted coin flips of those), then the same for up again, then learn negatively on multiply of 2 1d matrix.

There is a learn postively (after up) and a learn negatively (after up down up). In each of these, 2 1d matrix are multiplied as n x n matrix. Its the states of layer x nodes multiply layer x+1 nodes, as bits (all 2d matrix values are 0 or -learningRate or learningRate).

Here's the potential optimization, which looks like it should be there since matrix multiply is unknown between n^2 and n^2.38...

n + n matrix...

Whats a more efficient way, of least memory used, to sum many n x n matrix whose scalars are each derived from an n + n matrix, where n is max of quantity of nodes in 2 adjacent layers. In theory it can be done in 2*n time, or maybe a log times more than that, intuitively it appears to me it may be, for statistical Contrastive Divergence purposes, averaged over many such trainingData learned at once, at least.

Whats the most efficient way, of least memory used (and no more than a constant compute time more than that), of Contrastive Divergence?

Our GPU code efficiency depends on this. Also, which GPU code for matrix multiply, and which for Contrastive Divergence, do you recommend?",5,0,False,self,,,,,
592,MachineLearning,t5_2r3gv,2016-2-20,2016,2,20,14,46otzn,self.MachineLearning,What are some common ways to persist a trained model in a production system?,https://www.reddit.com/r/MachineLearning/comments/46otzn/what_are_some_common_ways_to_persist_a_trained/,bytezilla,1455946499,"I have been playing around with machine learning, and one thing that I keep wondering is how to persist the trained model so that the model doesn't have to be trained from scratch again.

I noticed that sk-learn provides a way to persist the model in a python pickle, but I imagine this wouldn't work well for a production system?

So, how is the model persisted in a production system? and what are some of the strategies to retrieve and update the model?",2,1,False,self,,,,,
593,MachineLearning,t5_2r3gv,2016-2-20,2016,2,20,17,46pb7m,usblogs.pwc.com,Do You Need a Chief Data Scientist?,https://www.reddit.com/r/MachineLearning/comments/46pb7m/do_you_need_a_chief_data_scientist/,lokator9,1455956854,,2,0,False,http://b.thumbs.redditmedia.com/CB5xFb_E-dx5vN5-yJ0BSTky79EJ2RzeX9FbKQ62vWw.jpg,,,,,
594,MachineLearning,t5_2r3gv,2016-2-20,2016,2,20,18,46peh3,self.MachineLearning,Dealing with a composite field in the training set,https://www.reddit.com/r/MachineLearning/comments/46peh3/dealing_with_a_composite_field_in_the_training_set/,ml_new,1455959165,"Hi, I am new to ML and doing my first exercise in classification. Unlike most basic examples, this one has this pattern:

Line 1 - feature1, feature2, a1:b1:c1

Line 2 - feature1, feature2, a2:b2:c1

Line 3 - feature3, feature2, a3:b1:c2

Line 4 - feature4, feature2, a4:b3:c1

As you can see, the 3rd column in the training set is a composite that is made of multiple entries that span across all the rows in the column.

How do I make sense of this? Are there particular models which can handle this? Or should I reframe each line in the training set as shown below:

Line 1 - feature1, feature2, a1:b1:c1 transforms to

Line 1 - feature1, feature2, a1

Line 1 - feature1, feature2, b1

Line 1 - feature1, feature2, c1

and then work on classification?
",2,2,False,self,,,,,
595,MachineLearning,t5_2r3gv,2016-2-20,2016,2,20,19,46pmnn,self.MachineLearning,help with my Facebook Dataset?,https://www.reddit.com/r/MachineLearning/comments/46pmnn/help_with_my_facebook_dataset/,velcronicoov,1455964964,"Classification trees/random forests/... take classification data and result in a binary dependent variable. For example: 

monday:cloudy, warm and windy. I go and play tennis
tuesday:raining, warm and windy. I don't play tennis 
wednesday: cloudy, cold and not windy: I go and play tennis.

From this I predict whether I will play tennis on a given day.

The data I was provided with is also data in classes. It is a Facebook data set (with all data from different Facebook profiles). The dependent variable is whether a person will like a Page or not. Some of the variables in the dataset have **multiple** classification values per user per variable. 

For example: user 3 speaks English, French and Dutch. So this gives me 3 values for the language variable (3 rows.)


How do I tackle this? This is the case for a lot of variables in my dataset",2,0,False,self,,,,,
596,MachineLearning,t5_2r3gv,2016-2-20,2016,2,20,22,46q40q,self.MachineLearning,Literature/papers about prediction of Facebook Likes or Twitter Followers,https://www.reddit.com/r/MachineLearning/comments/46q40q/literaturepapers_about_prediction_of_facebook/,deurdemeur,1455975601,I can't seem to find any. Do you guys know of any? ,1,3,False,self,,,,,
597,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,0,46qlp0,self.MachineLearning,my hypothesis of deep learning advantages,https://www.reddit.com/r/MachineLearning/comments/46qlp0/my_hypothesis_of_deep_learning_advantages/,godspeed_china,1455983585,"Suppose we have a deep network and a shallow network with the same number of parameters. For the weights from input to hidden layer of the shallow network, I think their variance is high. zeroing or perturbation or even random value does not change the result much. Thus shallow network parameters are wasted. However, for deep network, the parameters are twisted deeply. Thus if I change the value of one weight in the bottom layer, due to butterfly effect, will change the final output significantly. Thus the parameters are of low variance. Thus deep network makes full use of its parameters and load-balances Fisher information. That's my hypothesis of deep learning advantages over shallow ones.",3,0,False,self,,,,,
598,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,1,46qovt,theguardian.com,Has a rampaging AI algorithm really killed thousands in Pakistan?,https://www.reddit.com/r/MachineLearning/comments/46qovt/has_a_rampaging_ai_algorithm_really_killed/,lokator9,1455984867,,75,108,False,http://b.thumbs.redditmedia.com/2n-03zJGzKfuf8DxRYYBGa9cKO7r8SvZa--ZMuC_cyY.jpg,,,,,
599,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,1,46qrr7,blog.prediction.io,Salesforce signs a definitive agreement to acquire PredictionIO,https://www.reddit.com/r/MachineLearning/comments/46qrr7/salesforce_signs_a_definitive_agreement_to/,lokator9,1455985974,,2,0,False,default,,,,,
600,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,3,46r5y1,research.microsoft.com,Microsoft Research - Programming Models and Systems Design for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/46r5y1/microsoft_research_programming_models_and_systems/,antinucleon,1455991227,,0,15,False,http://b.thumbs.redditmedia.com/oHACGBeOePPDbko-_JdsPwK_MPtab8emJLZVpAMf8Cw.jpg,,,,,
601,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,3,46rc2g,arstechnica.com,"Tiny, blurry pictures find the limits of computer image recognition",https://www.reddit.com/r/MachineLearning/comments/46rc2g/tiny_blurry_pictures_find_the_limits_of_computer/,Barbas,1455993554,,6,2,False,http://b.thumbs.redditmedia.com/iH6qC898I68T1QmtE4e7VmLXIQqByGJcVhgVnGJxQ4M.jpg,,,,,
602,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,3,46rcf3,github.com,"TensorFuse: Common interface for Theano, CGT, and TensorFlow",https://www.reddit.com/r/MachineLearning/comments/46rcf3/tensorfuse_common_interface_for_theano_cgt_and/,dementrock,1455993680,,6,28,False,http://b.thumbs.redditmedia.com/TRrbbGSahP2qn6W-jvhmnxk2v8Ef9GtwDmOmTvgxe_M.jpg,,,,,
603,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,3,46reil,self.MachineLearning,I'm making a civic repository for city data at a Hackathon this weekend. Is this something you're interested in?,https://www.reddit.com/r/MachineLearning/comments/46reil/im_making_a_civic_repository_for_city_data_at_a/,FR_STARMER,1455994461,"Hey so I'm at a social entrepreneurship hackathon and we're building a big data repo for city data. We need some user validation so...

Would you be interested in a central repository for city datasets so you can easily find data pertaining to your city?

What would the benefits of a repository be?

Other thoughts?

Thanks!",2,4,False,self,,,,,
604,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,4,46rj43,self.MachineLearning,"Good machine learning course for ""advanced beginner"" which also goes into more complex data generation (yes, i've used the search bar and read FAQ)",https://www.reddit.com/r/MachineLearning/comments/46rj43/good_machine_learning_course_for_advanced/,SudoSilman,1455996214,"I have already taken the undergraduate machine learning course that my university offers. We went over the theory behind linear regression, gradient descent, bayesian inference, k-means, neural networks, and support vector machines; but we only were assigned homework to implement linear regression, gradient descent, and neural network. These were all implemented in MATLAB and the datasets we used were all just numeric (for example our linear regression was estimating the MPG of a car based off a dataset which included MPG, horsepower, displacement, weight, etc... which are all just numeric things you can feed into the model. 

I had no problem understanding the math behind any of these machine learning models, but to be fair these are all pretty basic, we didn't even talk about more complex neural networks like recurrent or convolutional, so i am not sure if i might've had trouble understanding the math there.

I am looking for an online course that goes over the basics behind the math for different models but focuses on implementation (preferably in Python). Then towards the end of the course talks about how to use the models (preferably neural networks) to generate your own data.

I am fascinated by stuff like [this article about creating magic cards](http://www.escapistmagazine.com/articles/view/scienceandtech/14276-Magic-The-Gathering-Cards-Made-by-Artificial-Intelligence), [this video about using neural networks and genetic algorithms to make a machine that plays mario](https://www.youtube.com/watch?v=qv6UVOQ0F44), and [this post about painting artwork with the style of different artists](https://www.reddit.com/r/MachineLearning/comments/3j295y/neural_algorithm_that_paints_photos_based_on_the/) and really want to learn how to manipulate higher order constructs like text and images to create my own, but have no idea about the process for breaking down the complex construct into something that can be fed into a machine learning model. I realize that some of these (especially the Mario playing one) may be really large accomplishments and not something that i can learn from a basic course on machine learning, but being able to do something like the magic card thing seems like a reasonable thing to learn, and i'd love to learn to do something with pictures instead of strictly numeric things that i learned in my uni's course

Maybe i am looking for two courses:

1) Advanced introduction to machine learning models which talks about all the basic commonly used models for supervised and unsupervised learning and how to implement them

2) Data generation with machine learning that talks about how to implement machine learning models which can then be used to create data of complex forms like pictures.

NOTE: The ""course"" could also be a book or just a website, so long as it provides good information on what i'm trying to learn",20,25,False,self,,,,,
605,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,4,46rjmt,lexiconjure.tumblr.com,LSTM trained on the Oxford English Dictionary generates definitions for invented words,https://www.reddit.com/r/MachineLearning/comments/46rjmt/lstm_trained_on_the_oxford_english_dictionary/,[deleted],1455996425,[deleted],0,1,False,default,,,,,
606,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,5,46rqct,self.MachineLearning,Prediction underlying 'causes' with a ANN where spatial organization is important,https://www.reddit.com/r/MachineLearning/comments/46rqct/prediction_underlying_causes_with_a_ann_where/,HowDeepisYourLearnin,1455999138,"I have a boolean vector like [0, 1, 0, 0, ... , 0, 1, 0]. Then I have a signal which is the boolean vector convolved it with some shape (a gaussian, for example, but could be anything), and then sprinkled some noise on top. The signal and the underlying 'causes' are the same size.

I'm trying to go from the noisy signal back to the binary vector. This has proven to be more difficult than I thought it would be. 

I have tried a few different approaches. Convolutional layers seems to be a given as the 1s can appear anywhere in the result I've stacked layers, various filter sizes and number of feature maps. Either the resulting vector ends up being something like [0.3, 0.4, 0.2, 0.1] or [0, 0, 0, 0] instead of [0, 1, 0, 0].

I think that the problem is turning the different feature maps into one prediction. In other networks, such as lenet, the last layer a dense layer weighing the different features and gives one prediction. This doesn't seem to work for me, probably because the exact spatial ordering of the boolean vector is important and a fully connected layer throws all spatial information out the window.

Also having a convolutional layer going from multiple feature maps to a single prediction isn't super effective either.
I've also played around with WTA (pooling would ) to make the predictions more 'crisp', but it just seems to kill all incentive to make any prediction at all.

I'm somewhat at a loss where to go from here and what else to try. Anyone got any tips or alternative perspectives? I'm happy to explain what ever is unclear.

Thanks:)",1,0,False,self,,,,,
607,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,5,46ru58,arxiv.org,[1511.05440] Deep multi-scale video prediction beyond mean square error,https://www.reddit.com/r/MachineLearning/comments/46ru58/151105440_deep_multiscale_video_prediction_beyond/,lokator9,1456000703,,0,5,False,default,,,,,
608,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,6,46s4ju,twitter.com,Twitter bot uses LSTM trained on the Oxford English Dictionary to generate definitions for invented words [x/CreativeCoding],https://www.reddit.com/r/MachineLearning/comments/46s4ju/twitter_bot_uses_lstm_trained_on_the_oxford/,TheMadStork,1456004898,,34,98,False,http://b.thumbs.redditmedia.com/yqL_pJ5Ol9gwV83iyi_pwCpbISxsk5hJmsI8hlfdLIY.jpg,,,,,
609,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,7,46s8ct,eovx3okz.formicalogistics.com,[Meta] NOT A Dating WebSite! HEY! EBI GUSEY! tMX,https://www.reddit.com/r/MachineLearning/comments/46s8ct/meta_not_a_dating_website_hey_ebi_gusey_tmx/,tirankrear,1456006350,,0,1,False,default,,,,,
610,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,7,46sa26,self.MachineLearning,"This is kind of a newbie notation question, but in computational learning theory what exactly do delta and epsilon mean?",https://www.reddit.com/r/MachineLearning/comments/46sa26/this_is_kind_of_a_newbie_notation_question_but_in/,JimTheSavage,1456006968,"My guess is that delta (lowercase) refers to the user chosen error, and epsilon refers to something to do with error, but not true error. I've looked through all the class notes and at no point was epsilon defined as anything. And I'd really appreciate it if someone would set me straight.",2,1,False,self,,,,,
611,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,10,46t5m7,drovosek.usluga-goda.com,[Meta] NOT A Dating WebSite! HEY! EBI GUSEY! 6guCl0B53nFYaG,https://www.reddit.com/r/MachineLearning/comments/46t5m7/meta_not_a_dating_website_hey_ebi_gusey/,bipela,1456019527,,0,1,False,default,,,,,
612,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,11,46t7vm,arxiv.org,[1602.04621] Deep Exploration via Bootstrapped DQN,https://www.reddit.com/r/MachineLearning/comments/46t7vm/160204621_deep_exploration_via_bootstrapped_dqn/,gogogadgetlegz,1456020400,,1,6,False,default,,,,,
613,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,11,46tcgp,support.hydroer.com,[Meta] NOT A Dating WebSite! HEY! EBI GUSEY! ggp9WEe2RQl,https://www.reddit.com/r/MachineLearning/comments/46tcgp/meta_not_a_dating_website_hey_ebi_gusey/,bangsanot,1456022130,,0,1,False,default,,,,,
614,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,11,46td0u,self.MachineLearning,Why does word2vec train a vector for the stop symbol but not the start symbol?,https://www.reddit.com/r/MachineLearning/comments/46td0u/why_does_word2vec_train_a_vector_for_the_stop/,DanielHendrycks,1456022339,,4,5,False,self,,,,,
615,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,13,46trds,youtube.com,Origins of Markov chains (art of the problem),https://www.reddit.com/r/MachineLearning/comments/46trds/origins_of_markov_chains_art_of_the_problem/,[deleted],1456028396,[deleted],0,2,False,default,,,,,
616,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,14,46twvh,self.MachineLearning,Question on NN: Weight initialization based on activation function?,https://www.reddit.com/r/MachineLearning/comments/46twvh/question_on_nn_weight_initialization_based_on/,poporing88,1456031069,I am working on a siamese network with relu activation function at two hidden layers and tanh at the output layer. My question is if the initialization of the weights for the relu activation should be different with the tanh? Currently i am using xavier initialization for all is only reaching local optimum solution. :(,3,4,False,self,,,,,
617,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,15,46u73g,self.MachineLearning,Chat bot using Facebook messages?,https://www.reddit.com/r/MachineLearning/comments/46u73g/chat_bot_using_facebook_messages/,doscorohit,1456036568,"I am planning to build a chatbot for my friend's birthday, and train in on our Facebook messages. The idea is, she types in something and the bot sends a reply. Could somebody guide me to resources I could use to do this? I have basic knowledge of machine learning techniques (also know a bit of deep learning)",6,0,False,self,,,,,
618,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,17,46uf27,self.MachineLearning,It seems to me that RNN isn't the right choice for the first layer of a language model that takes word embeddings as input,https://www.reddit.com/r/MachineLearning/comments/46uf27/it_seems_to_me_that_rnn_isnt_the_right_choice_for/,[deleted],1456041832,[deleted],0,1,False,default,,,,,
619,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,21,46v0o5,github.com,spotify/annoy - Approximate Nearest Neighbors in C++/Python optimized for memory usage and loading/saving to disk,https://www.reddit.com/r/MachineLearning/comments/46v0o5/spotifyannoy_approximate_nearest_neighbors_in/,lokator9,1456056298,,14,85,False,http://b.thumbs.redditmedia.com/r4Y9qtZGJwadb6jVdYm9T7Hfelb-KgVWg-R8aWuDgaY.jpg,,,,,
620,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,21,46v14i,serverlesscode.com,Using Scikit-Learn In AWS Lambda,https://www.reddit.com/r/MachineLearning/comments/46v14i/using_scikitlearn_in_aws_lambda/,lokator9,1456056558,,0,9,False,default,,,,,
621,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,21,46v1i6,self.MachineLearning,Question: If there are lots of images with humans performing human postures and if you had to judge them - has image processing evolved enough to capture human body postures from images and judge whcih posture is accurate?,https://www.reddit.com/r/MachineLearning/comments/46v1i6/question_if_there_are_lots_of_images_with_humans/,ashtavakra,1456056838,,5,1,False,self,,,,,
622,MachineLearning,t5_2r3gv,2016-2-21,2016,2,21,23,46vk1g,self.MachineLearning,I want to build a chat bot for ... comment your ideas,https://www.reddit.com/r/MachineLearning/comments/46vk1g/i_want_to_build_a_chat_bot_for_comment_your_ideas/,Dzsonni666,1456066601,"just type any stupid, crazy stuff u wanted to put into a chatbot",3,0,False,self,,,,,
623,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,0,46vt48,redd.it,Creating a Irony Reddit Labeled Dataset,https://www.reddit.com/r/MachineLearning/comments/46vt48/creating_a_irony_reddit_labeled_dataset/,iktof,1456070330,,0,2,False,http://b.thumbs.redditmedia.com/yK_6r3YinYWZqCLBZCiwLylDjFyKmXXOmjQXXC6-vIA.jpg,,,,,
624,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,1,46vz4k,self.MachineLearning,Good machine learning book with Python for someone with a traditional stats background,https://www.reddit.com/r/MachineLearning/comments/46vz4k/good_machine_learning_book_with_python_for/,aazraelxii,1456072657,"Hello,

I was wondering if you guys could recommend a machine learning book preferrably using python (but R is also ok). I am about to graduate with a masters in stats and machine learning isn't a topic that is really covered (you got operations research classes and the traditional design of experiments/ linear models/ probability theory instead).

Thanks. ",7,1,False,self,,,,,
625,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,2,46wa5i,reddit.com,Computers Can Predict Schizophrenia Based on How a Person Talks,https://www.reddit.com/r/MachineLearning/comments/46wa5i/computers_can_predict_schizophrenia_based_on_how/,anonDogeLover,1456076433,,3,0,False,http://b.thumbs.redditmedia.com/vyGvCMC4J19DoNdsy3De-F14gAfeQyqC5i1kzKjefvQ.jpg,,,,,
626,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,4,46wu28,self.MachineLearning,Is working through a math-heavy ML textbook very useful for getting a first job in the field?,https://www.reddit.com/r/MachineLearning/comments/46wu28/is_working_through_a_mathheavy_ml_textbook_very/,[deleted],1456083232,[removed],0,1,False,default,,,,,
627,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,5,46x56i,self.MachineLearning,Is there health data available similar to Heritage Health Prize?,https://www.reddit.com/r/MachineLearning/comments/46x56i/is_there_health_data_available_similar_to/,katnz,1456087047,"Just wondering if anyone is aware of datasets for healthcare, like the Heritage Health Prize, which are available for research? Anonymised patient records, lab test results, etc.

I am trying to build a model to predict re-admission rates to hospitals, but it could be several months before the dataset becomes available because of legal agreements. I'm keen to find something similar to work on in the meantime.

Thanks in advance",3,1,False,self,,,,,
628,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,5,46x6ad,self.MachineLearning,Keras Embedding Layer Clarification,https://www.reddit.com/r/MachineLearning/comments/46x6ad/keras_embedding_layer_clarification/,anonDogeLover,1456087436,"Does the embedding layer in keras get trained with the entire LSTM, end-to-end?",5,1,False,self,,,,,
629,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,6,46xb7x,self.MachineLearning,Monte Carlo for estimating Q/V functions,https://www.reddit.com/r/MachineLearning/comments/46xb7x/monte_carlo_for_estimating_qv_functions/,[deleted],1456089206,[deleted],3,0,False,default,,,,,
630,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,6,46xive,self.MachineLearning,energy landscape of multi classification energy based model,https://www.reddit.com/r/MachineLearning/comments/46xive/energy_landscape_of_multi_classification_energy/,John_Smith111,1456091898,"Hello all,

i would like to ask what is the shape of energy function multi classification energy based model  - if we want to classify numbers form 0-9:

   -   The energy landscape has 10 valleys on different places of the surface - each valley represent number of from 0-9. If we get new sample  it the new sample is placed near some of the valley and using gradient descend we get move down to the valley of the number - and we classified it as  that number. 

or 

   -  For each possible y we compute different landscape - for 1 we compute landscape where the 1 is the deeper valley and other numbers has not so deep energy holes.  We examine the compatibility of the new sample with all landscapes (for all possible  10 landscape )

So basically  we have single landscape with 10 valleys or we have 10 landscapes (each for possible discrete result - y) 

Thank you all! ",3,0,False,self,,,,,
631,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,7,46xjtw,self.MachineLearning,How to understand the KL divergence term in Variational Autoencoders,https://www.reddit.com/r/MachineLearning/comments/46xjtw/how_to_understand_the_kl_divergence_term_in/,gameofml,1456092222,"I have no question with the mathematical derivations, but I am struggling to understand the intuitions behind the KL divergence term KL(q(z|x), p(z)). Intuitively, this pushes the posterior distribution of q(z|x) for every data point x to the aggregated prior distribution p(z). If p(z)~N(0, I), we are essentially mapping the mean of q(z|x) for every x to the same point, which I cannot understand intuitively. I feel it makes more sense if the objective is to map p(z) to q(z) or p(z|x) to q(z|x), but the derivation did not arrive there. I know it may be a bit late to ask this question, but I get stuck here in understanding the VAE paper. Can someone help?",14,13,False,self,,,,,
632,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,8,46xxd2,self.MachineLearning,Speech databases outside of english?,https://www.reddit.com/r/MachineLearning/comments/46xxd2/speech_databases_outside_of_english/,timburg,1456097243,"Does anyone know of any speech databases for French, Chinese, Spanish, etc, that are word-labeled? Something like TIMIT but without the requirement of having phoneme segmentation",6,5,False,self,,,,,
633,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,10,46yia1,self.MachineLearning,0.1% gather to discuss the implications of a jobless economy.,https://www.reddit.com/r/MachineLearning/comments/46yia1/01_gather_to_discuss_the_implications_of_a/,grbradsk,1456105406,I thought the answer was soylent green ,1,0,False,self,,,,,
634,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,10,46yjpj,now.uiowa.edu,Predicting box office - Iowa Now,https://www.reddit.com/r/MachineLearning/comments/46yjpj/predicting_box_office_iowa_now/,wftvchannel,1456105974,,1,0,False,http://b.thumbs.redditmedia.com/Wub-HUbYjOsud2zFo_VRetUgglvJZYM0erC-WyYkmnU.jpg,,,,,
635,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,11,46ykvv,shahramabyari.com,Identifying Density Based Local Outliers: LOF Method,https://www.reddit.com/r/MachineLearning/comments/46ykvv/identifying_density_based_local_outliers_lof/,shahroom,1456106425,,0,4,False,http://a.thumbs.redditmedia.com/gYdeaP5gteoTDj8Ml2TdtHH3qm6OUdUugm1SGwUx6Q4.jpg,,,,,
636,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,13,46z4oh,self.MachineLearning,Rocket,https://www.reddit.com/r/MachineLearning/comments/46z4oh/rocket/,[deleted],1456114583,[deleted],0,1,False,default,,,,,
637,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,13,46z8qa,self.MachineLearning,"How to detect power lines in images? (using (fully) convnets, or other algos)",https://www.reddit.com/r/MachineLearning/comments/46z8qa/how_to_detect_power_lines_in_images_using_fully/,mad_rat_man,1456116315,"I am a researcher trying to automate power line inspection with drones. 
Think of the drone flying above power lines. How would you detect these thin lines, in clutter (trees, vegetation, road lanes, can all be confusing).
I think of breaking the problem into background clutter removal and then line detection. Or you could also have an end-to-end solution, predicting pixel which belong to power lines labels themselves",2,1,False,self,,,,,
638,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,13,46zabh,self.MachineLearning,Data augmentation python library,https://www.reddit.com/r/MachineLearning/comments/46zabh/data_augmentation_python_library/,trevinstein,1456116985,"Is there a well developed package out there that contains functions to augment an image dataset? I observed that TensorFlow has some such functions but it's by no means complete, no complex image manipulation that usually requires graphics people to write.",3,2,False,self,,,,,
639,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,14,46zbms,fdjohnson.widezone.net,An Ideal Lubrication Partner Bijur Delimon,https://www.reddit.com/r/MachineLearning/comments/46zbms/an_ideal_lubrication_partner_bijur_delimon/,jackerfrinandis,1456117538,,0,1,False,default,,,,,
640,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,14,46zes2,self.MachineLearning,speaker recognition state of the art,https://www.reddit.com/r/MachineLearning/comments/46zes2/speaker_recognition_state_of_the_art/,pm_me_p,1456118905,"I am new to this field, and was wondering what the state of the art in speaker identification is. Also, are there systems that can work with ""small"" datasets? (just a few speech examples per person)

(x-posted from \r\speechrecognition)",4,0,False,self,,,,,
641,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,14,46zex0,self.MachineLearning,agriculture data set for data science?,https://www.reddit.com/r/MachineLearning/comments/46zex0/agriculture_data_set_for_data_science/,whitetensor,1456118971,"I am looking for some agriculture related data set with labels to do some modeling. It can be numerical, or satellite image data from  sources like remote sensing or field sensors. Any one can point out where to find those? Thanks!",2,0,False,self,,,,,
642,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,14,46zhut,rajmak.wordpress.com,Structuring text using Conditional Random Field (CRF). Tagging recipe ingredient phrases.,https://www.reddit.com/r/MachineLearning/comments/46zhut/structuring_text_using_conditional_random_field/,codesaint,1456120430,,0,27,False,http://b.thumbs.redditmedia.com/6E0YXUncXhFZE-HgC8IH7COjePvS20nr4P30WrcpyiU.jpg,,,,,
643,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,16,46zq7c,blog.shakirm.com,Learning in Brains and Machines: Temporal Differences,https://www.reddit.com/r/MachineLearning/comments/46zq7c/learning_in_brains_and_machines_temporal/,iori42,1456124957,,8,49,False,http://b.thumbs.redditmedia.com/01nTbk-3a4kFh8ch_kPQ1UHj3HnahpsajeI0rg9DkjU.jpg,,,,,
644,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,19,47075v,self.MachineLearning,looking for python library,https://www.reddit.com/r/MachineLearning/comments/47075v/looking_for_python_library/,0one0one,1456135412,"I was reading a very interesting post a while back about a project to document the entire Reddit dataset. The lad had written it all as a library and was putting the entire history of reddit up as a torrent for download, cant for the life of me remember the name of the library or find the post again, does anyone have any idea ? ",7,0,False,self,,,,,
645,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,19,47097y,self.MachineLearning,[Caffe] understanding of test_iter. Confused!,https://www.reddit.com/r/MachineLearning/comments/47097y/caffe_understanding_of_test_iter_confused/,apple-sauce,1456136615,"This is what they say on the [github](https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet_solver.prototxt#L3):


    # test_iter specifies how many forward passes the test should 
    # carry out. 
    test_iter: 100

    # In the case of MNIST, we have test batch size 100 and 100 
    # test iterations, covering the full 10,000 testing images.

    # Carry out testing every 500 training iterations. 
    test_interval: 500 


So does this mean that testing is carried out on separate batches? In this case every 500 iterations, a batch of 100 test cases is being evaluated?



But, what if the test `batch_size` was less than 100, let's say 50. Then you would not have covered the whole test data set right(50*100=5,000)? MNIST test set is 10,000 samples.



Suffice to say, I am confused!
",4,0,False,self,,,,,
646,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,19,470bfx,self.MachineLearning,[Hardware Q] What is best hardware to work on data analysis/machine learning for Text data?,https://www.reddit.com/r/MachineLearning/comments/470bfx/hardware_q_what_is_best_hardware_to_work_on_data/,poporing88,1456137866,"We have a budget to buy some computers but unsure which specifications is needed. We will be working on some data analysis on millions of text data (from e-mails and websites).  We will use machine learning algorithms such as neural networks, adaboost, etc.",10,0,False,self,,,,,
647,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,21,470jx6,google.ca,1516girls! ATTENTION! A lot of sexual adventures your city here!,https://www.reddit.com/r/MachineLearning/comments/470jx6/1516girls_attention_a_lot_of_sexual_adventures/,rasembpur27496,1456142801,,0,1,False,default,,,,,
648,MachineLearning,t5_2r3gv,2016-2-22,2016,2,22,22,470t1y,self.MachineLearning,Papers on GA used to build NN?,https://www.reddit.com/r/MachineLearning/comments/470t1y/papers_on_ga_used_to_build_nn/,hapliniste,1456147603,"Hi everyone. I'm looking for papers talking about the use of genetic algorithms to automatically build neural network; could you recommend me some? 

I would also like to learn a bit about how parts of the human brain get formed (parents passing the ""model"" of the brain to their kids). Like, everyone has quite the same visual cortex, at the same place. The learning of the individual is then comparable to a neural network training, but the actual model is passed via genetics. 

Thank you :)",16,33,False,self,,,,,
649,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,0,471be8,self.MachineLearning,Allot of data but far more negative then positive examples. Pitfalls and things to look out for?,https://www.reddit.com/r/MachineLearning/comments/471be8/allot_of_data_but_far_more_negative_then_positive/,t_ss,1456155191,"I'm looking to build a binary classification system based on deep learning where there will be allot of data available and constantly be streaming in. 

The thing is that the data has far more negative examples then positive examples, as in 99.99% negative and 0.01% positive. 

Does anyone have experience with such data sets? What are should I look out for and what are the common pitfalls? Does this pose a problem when training the classifier? Should I remove negative examples to create a more evenly balanced training set, or should the training set reflect the actual frequency of positive and negative examples?",10,2,False,self,,,,,
650,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,0,471e84,self.MachineLearning,How can I train my NLP models more quickly?,https://www.reddit.com/r/MachineLearning/comments/471e84/how_can_i_train_my_nlp_models_more_quickly/,picardo,1456156201,"I'm working on an NLP algorithm that will create a dependency parse tree of English and Swedish sentences. The algorithm first needs to train a model using arc-eager dependency parsing method. It works fine for a relatively small number of sentences (2-5), but it grinds to a halt at larger numbers, like 200. 

Assuming I've done everything to ensure that the algorithm is as optimal as it can be, what can I do to increase its performance? I've thought about loading it on Amazon or Digital Ocean and running it there for a few days. Would anyone like to share the best practices for training models when your computer is not so fast?",16,0,False,self,,,,,
651,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,0,471fnx,innoarchitech.com,"Machine Learning: An In-Depth, Non-Technical Guide - Part 3",https://www.reddit.com/r/MachineLearning/comments/471fnx/machine_learning_an_indepth_nontechnical_guide/,innoarchitech,1456156696,,0,8,False,http://b.thumbs.redditmedia.com/9hWzn41NZwKnOtUg9o_Rxh8nQfWXFqNPL39MFicSqpw.jpg,,,,,
652,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,1,471ldm,arxiv.org,Contextual LSTM (CLSTM) models for Large scale NLP tasks,https://www.reddit.com/r/MachineLearning/comments/471ldm/contextual_lstm_clstm_models_for_large_scale_nlp/,jrmuizel,1456158615,,0,24,False,default,,,,,
653,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,1,471ml1,self.MachineLearning,Any off the shelf framework for multi label image classification?,https://www.reddit.com/r/MachineLearning/comments/471ml1/any_off_the_shelf_framework_for_multi_label_image/,n00bto1337,1456159013,"Has anyone solved this problem? Using Caffe/ Torch or any other framework?
",8,0,False,self,,,,,
654,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,1,471ojf,code.facebook.com,Connecting the world with better maps | Engineering Blog | Facebook Code,https://www.reddit.com/r/MachineLearning/comments/471ojf/connecting_the_world_with_better_maps_engineering/,code2hell,1456159676,,1,6,False,http://b.thumbs.redditmedia.com/MN9M1MghEQFD6fY2KxqAgSaVOreoxxGSp4zMpdDiopo.jpg,,,,,
655,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,1,471p2m,self.MachineLearning,Gradient descent intuition,https://www.reddit.com/r/MachineLearning/comments/471p2m/gradient_descent_intuition/,MarcoROG-SG,1456159860,"I'm quite familiar to how gradient descent works, but I have some doubts about the efficiency of the parameter update.
For a scalar w, we know the derivative wrt the error (dE/dw) represents how an infinitesimal change in w would affect E.
In gradient descent, the weight update is directly proportional to the derivative. This however seems quite confusing: if a small change in w already leads to big changes in E, why would I take a bigger step when the derivative is large? It may make convergence faster, but it also leads to divergence.
With an update given by a learning rate DIVIDED BY the derivative, we'd have a costant decrease of the error.
It sounds a bit confusing, is there something I overlooked? Thanks for the explaination",9,4,False,self,,,,,
656,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,1,471pqn,self.MachineLearning,"Can NeuralTalk2 classify images into multiple captions, with their probabilities?",https://www.reddit.com/r/MachineLearning/comments/471pqn/can_neuraltalk2_classify_images_into_multiple/,mln00b13,1456160081,"Can I use this to classify my images into multiple captions instead of one?
https://github.com/karpathy/neuraltalk2.
Also, can I display some probabilities along with just the caption?",2,0,False,self,,,,,
657,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,2,471tcr,arxiv.org,Scaling up Dynamic Topic Models,https://www.reddit.com/r/MachineLearning/comments/471tcr/scaling_up_dynamic_topic_models/,Pillowrath,1456161299,,0,2,False,default,,,,,
658,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,3,4728e1,self.MachineLearning,Best method to generate prose in the style of a writer?,https://www.reddit.com/r/MachineLearning/comments/4728e1/best_method_to_generate_prose_in_the_style_of_a/,dubidubapgaling,1456166226,"Let's assume I have the collected works of Shakespeare, and wish to generate more Shakespeare. Or I have some Stephen King books and wish to generate more Stephen King.

What are currently the best methods of generating more prose based on an existing body of prose?",12,8,False,self,,,,,
659,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,3,4728sg,self.MachineLearning,Application to build neural networks for regression or classification problems?,https://www.reddit.com/r/MachineLearning/comments/4728sg/application_to_build_neural_networks_for/,marcjschmidt,1456166371,"Hey

does someone know a good application where I can build neural networks without python knowledge to address regression or classification problems? I'm looking for a kind of GUI for theano, tensorflow or stuff like that where I can click my network together. I have seen http://metamind.io but they don't allow you to modify parameters (for example for SGD, learning rate etc) of the network. Also I can not download the network and run it on my iPhone offline for example.

So my question is: are there any neural network builder out there I can use to pipe in hundred of pictures, configure a network and let it train? Also some way of exporting it to my application or debugging like looking into the convolutional features for a specific training example or during the learning as picture would be nice.
",5,0,False,self,,,,,
660,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,3,472bsr,nnx-addin.org,NNX: open source Excel add-in for multilayer perceptrons (version 0.1),https://www.reddit.com/r/MachineLearning/comments/472bsr/nnx_open_source_excel_addin_for_multilayer/,methinks2015,1456167370,,15,20,False,default,,,,,
661,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,4,472i5q,self.MachineLearning,Technique to Retrieve Parameters in Question Answering,https://www.reddit.com/r/MachineLearning/comments/472i5q/technique_to_retrieve_parameters_in_question/,brunoalano,1456169482,"Hello, I need to find a way to find some domain-specific parameters to make the query into my Question Answering algorithm. I've been thinking about a rule-based engine, but that'll be so hard to keep a high recall.

Instead, I've tried the Stanford Bootstrapped Entity Extractor, but without success.

For example, a question like: ""I want to buy a new car in 20 parcels and pay with my credit card""

I want to retrieve:
[PARCEL] = 20 parcels
[PAYMENT_METHOD] = credit card",0,0,False,self,,,,,
662,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,5,472u93,blog.cambridgecoding.com,Natural Language Processing meets Deep Learning,https://www.reddit.com/r/MachineLearning/comments/472u93/natural_language_processing_meets_deep_learning/,DrLegend,1456173480,,9,34,False,http://a.thumbs.redditmedia.com/hIYQo3m9oXnys2G555F8MYJLt7V1PAMfhD5YZkpkfS8.jpg,,,,,
663,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,6,473763,self.MachineLearning,"Why is a value network needed in AlphaGo, wouldn't just a state  action mapping be enough?",https://www.reddit.com/r/MachineLearning/comments/473763/why_is_a_value_network_needed_in_alphago_wouldnt/,[deleted],1456177720,[deleted],0,1,False,default,,,,,
664,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,7,473dgi,self.MachineLearning,Why is a value network needed in AlphaGo? Wouldn't just a state  action mapping be enough?,https://www.reddit.com/r/MachineLearning/comments/473dgi/why_is_a_value_network_needed_in_alphago_wouldnt/,mere_mortise,1456179820,Wouldn't it be enough to learn the policy `P(a|s)` such that in each state the most promising action is chosen?,9,10,False,self,,,,,
665,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,9,473y4g,self.MachineLearning,Looking for Canadian manufacturer that will built a devbox similar to the one advertised on NVIDIA,https://www.reddit.com/r/MachineLearning/comments/473y4g/looking_for_canadian_manufacturer_that_will_built/,mlcanada,1456187149,Basically looking for a Canadian company that will build something like this: https://www.microway.com/preconfiguredsystems/whisperstation-deep-learning/,0,2,False,self,,,,,
666,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,10,4743j5,arxiv.org,[1602.02697] Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples,https://www.reddit.com/r/MachineLearning/comments/4743j5/160202697_practical_blackbox_attacks_against_deep/,downtownslim,1456189242,,11,10,False,default,,,,,
667,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,10,4743re,self.MachineLearning,Could you train a neural network to build houses?,https://www.reddit.com/r/MachineLearning/comments/4743re/could_you_train_a_neural_network_to_build_houses/,Gingerfeld,1456189334,"As a hobby, I'm slowly creating a simulation/game (mostly simulation) that generates a world using as much real-world data as possible. Stuff like geography, streets, and building locations are readily available, but the floor plans of those buildings are difficult to reliably come by.  
  
What I do have are these: Building type, building position, number of floors, age, and shape.  
I also have access to a whole bunch of unrelated floor plans in the public domain, all in the same format and accompanied by the same data as above.
  
Do you guys think that it's feasible to generate intelligent building plans using the floor plans as input? My only experience for neural networks is from developing AI for the same simulation. Is this just totally out of my league? Are there any similar projects that have been done?",4,0,False,self,,,,,
668,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,11,474cmw,self.MachineLearning,Why does my detection model performs terrible when training on voc12 while it performs fine on voc07,https://www.reddit.com/r/MachineLearning/comments/474cmw/why_does_my_detection_model_performs_terrible/,sunshineatnoon,1456192848,"I trained a detection model on pascal voc2007, it's a small network and it overfits, but it works fine. However, when I train it on voc2012, it performs far worse than the model I got from voc2007. What could cause this? Am I just being unlucky at weight initialization or the voc2012 is much trickier than voc07?",0,1,False,self,,,,,
669,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,12,474mpa,self.MachineLearning,Question about Tensorflow Embeddingwrapper (and Gated Recurrent Unit Hidden State),https://www.reddit.com/r/MachineLearning/comments/474mpa/question_about_tensorflow_embeddingwrapper_and/,rescue11,1456196909,"Hello, I'm hoping to clear up on the Embeddingwrapper (The question is at #2):

1. In tensorflow's GRUCell, the forward step is implemented as such:

r, u = array_ops.split(1, 2, linear([inputs, state], 2 * self._num_units, True, 1.0))

r, u = sigmoid(r), sigmoid(u)

Now this is equivalent to the following equations from the GRU paper:

r_t = sigm (W_xr*x_t + W_hr*h_t1 + b_r)

z_t = sigm(W_xz*x_t + W_hz*h_t1 + b_z)

(z in this paper is u in the Tensorflow code)


So for the forward step, the GRUCell calls linear([inputs, state], 2 * self._num_units, True, 1.0), does the calculations that represent sigm (W_xr*x_t + W_hr*h_t1 + b_r) and sigm(W_xz*x_t + W_hz*h_t1 + b_z) 
and returns r and u which are each 1000 in length if the num_hidden_units of the rnn_cell is 1000.

2. Now given this understanding, I'm having trouble understanding why the Embeddingwrapper function restricts the embedding dimension to the same number as the number of hidden units, 1000, in this example. Shouldn't it be able to handle any dimension size embedding? 

Since the matrices W_xr and W_xz map the x vector to a vector the size of r or z (both 1000), shouldn't it not matter what the dimension of the x vector is?",1,1,False,self,,,,,
670,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,12,474s0m,self.MachineLearning,Questuon about text machine learning preprocessing for sentiment analysis,https://www.reddit.com/r/MachineLearning/comments/474s0m/questuon_about_text_machine_learning/,DE0XYRIBONUCLEICACID,1456199060,"I want to remove stopwords to reduce the entropy of my data, but that includes the word ""not"" which obviously could have a huge impact on the meaning of the data. How can I account for words like not in general while still removing stopwords? Or do people generally not remove stopwords for sentiment analysis?",1,0,False,self,,,,,
671,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,12,474trm,self.MachineLearning,Help Needed! AI machine learning home assistant! Smart Home development.,https://www.reddit.com/r/MachineLearning/comments/474trm/help_needed_ai_machine_learning_home_assistant/,suci_,1456199785,"Hi MachineLearning Redditors,
 
I'm a developer for Smart Home device, and we're working on applying machine learning AI on smart home hub.
Please help us out, fill in the survey, and let us know what/where to improve!
https://backerfounder.typeform.com/to/suH5po

This smart home hub is just like what Mark Zuckerberg said in his new year resolution:
1. Facial Recognition (tells stranger from families and friends)
 
2. Overall home electronic appliances control (home automation)
 
3. Knows, learns and memorizes your daily routines/habits (for instance, your coffee machine knows you'd be waking up 7am, and get to the kitchen at 7:15, it'd start making coffee at 7:10)
 
4. Prioritize family members. (mom priority higher than children. so when everyone's in the living room, TV will first switch to mom's favorite channel.)
 
It's a device for home security, and AI home assistant.
We've partnered with some big electronics manufacturers, and we're really looking forward to releasing it soon.
But we need your help!
Survey benchmark is the first challenge we have...
Redditors, I'd really use your collective force now...please help out, fill in the survey and let's make this happen together!",0,0,False,self,,,,,
672,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,13,474ydn,self.MachineLearning,LSTM for irregularly spaced time series with imbalanced classification/event prediction,https://www.reddit.com/r/MachineLearning/comments/474ydn/lstm_for_irregularly_spaced_time_series_with/,neuralnetthrowaway,1456201721,"Hey all, just getting onto the LSTM train and was wondering if anyone had some advice on how to set up the following problem, both in terms of training examples as well as architecture.

I have a bunch of signals that I'm trying to predict a future rare event off of, which I know is possible and somewhat trivial (trend is a big indicator). Unfortunately, the signals are sampled at irregular intervals, and the event imbalance is 1000:1 + that is to say most of the time, the event does not occur. 

I want to train an LSTM as a PoC. The strategies I have tried are overlapping windows, separate windows with and without timestamps of varying length and shuffled or not, class-weighting and not, fed into a one layer LSTM w/ dropout into a Fully Connected layer and then out into binary sigmoid layer. Currently I am only trying to classify sequences that contain the event, but ultimately the goal would be to see if I can predict the event prior to it occurring (presumably by labelling prior samples in some window as positive). After training 50 epochs or so with varying batch sizes and learning rates with RMSprop, nothing is predicted as a positive and the ROC has got a .55 or less AUC.

Any ideas would be greatly appreciated. Thanks!",0,2,False,self,,,,,
673,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,13,4751f2,self.MachineLearning,1 Best Python tools for transforming/cleaning text data?,https://www.reddit.com/r/MachineLearning/comments/4751f2/1_best_python_tools_for_transformingcleaning_text/,data-science,1456203095,"I have a bunch of text data from web scraping, and I have already taken out the HTML/CSS/JS tags so that I only have human-readable texts. However, I need some help on finding Python tools that can effectively remove :

1. times/dates (incl. Tuesday, November, etc) (they are not standardized, meaning: Tuesday/Tues, 2:00/2am all needs to be removed) 
2. geographical locations (we are scraping for websites located at a certain location 
3. remnants of web data (menu, skip, contents, search, form, etc)

Some of the tools I looked into were: 

1. [NLTK](http://www.nltk.org/)
2. [Panda](http://pandas.pydata.org/)
",6,0,False,self,,,,,
674,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,13,4751tk,googleresearch.blogspot.com,Exploring the Intersection of Art and Machine Intelligence: Beyond DeepDream,https://www.reddit.com/r/MachineLearning/comments/4751tk/exploring_the_intersection_of_art_and_machine/,fhoffa,1456203282,,1,23,False,http://b.thumbs.redditmedia.com/jic5h3RX-gzIS0E_d7tCAfdafzFJPRVyn8QopeA8OmU.jpg,,,,,
675,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,14,4755mv,articleask.com,Pneumatic Pump Systems and Their Applications In the Industries,https://www.reddit.com/r/MachineLearning/comments/4755mv/pneumatic_pump_systems_and_their_applications_in/,jackerfrinandis,1456204951,,0,1,False,default,,,,,
676,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,16,475iwp,self.MachineLearning,Where do ML and statistics NOT overlap (or what fields of statistics are rather unimportant in ML?),https://www.reddit.com/r/MachineLearning/comments/475iwp/where_do_ml_and_statistics_not_overlap_or_what/,DeapSoup,1456211857,"Edit: Thank you for all the great comments! I am a prospective undegraduate student and want to get into Machine Learning, but since there are no programs where ML per se is a major (at least as far as I know) and I am very picky about using my time on learning what I actually want to learn, I think I will go for the Data Analysis and Data Management Undergraduate in Germany after having read that those two fields seem to be pretty much identical :)",64,74,False,self,,,,,
677,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,18,475ut5,self.MachineLearning,Best method for sentence generation in Keras,https://www.reddit.com/r/MachineLearning/comments/475ut5/best_method_for_sentence_generation_in_keras/,reccrun,1456219592,[removed],1,1,False,default,,,,,
678,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,18,475we5,visualgenome.org,Visual Genome Paper,https://www.reddit.com/r/MachineLearning/comments/475we5/visual_genome_paper/,siddharth-agrawal,1456220518,,2,14,False,default,,,,,
679,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,18,475xgi,self.MachineLearning,[Holed Convolution Layer in MatConvNet] (https://github.com/liavassif/holedConv),https://www.reddit.com/r/MachineLearning/comments/475xgi/holed_convolution_layer_in_matconvnet/,[deleted],1456221112,[removed],0,1,False,default,,,,,
680,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,19,475ylk,self.MachineLearning,Holed Convolution Layer in MatConvNet,https://www.reddit.com/r/MachineLearning/comments/475ylk/holed_convolution_layer_in_matconvnet/,liav1,1456221724,"A crude implementation in MatConvNet of the 'hole' algorithm described in [Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs] (http://arxiv.org/abs/1412.7062) is [here] (https://github.com/liavassif/holedConv)
The only implemented part is the 'hole' algorithm, without any CRF afterwards.

Note that the full (and more efficient) implementation by the original authors is available at the [DeepLab] (https://bitbucket.org/deeplab/deeplab-public/) site and this implementation is not related to them. 
",0,2,False,self,,,,,
681,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,19,475zvw,slideshare.net,"[Alex Smola] Fast, Cheap and Deep - Scalable Machine Learning",https://www.reddit.com/r/MachineLearning/comments/475zvw/alex_smola_fast_cheap_and_deep_scalable_machine/,thvasilo,1456222557,,0,1,False,default,,,,,
682,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,19,4760gs,self.MachineLearning,I want to try out conversation modeling using LSTM's.Please help me where to start.,https://www.reddit.com/r/MachineLearning/comments/4760gs/i_want_to_try_out_conversation_modeling_using/,[deleted],1456222956,[removed],0,1,False,default,,,,,
683,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,19,47614y,ataspinar.wordpress.com,learn properly to do Naive Bayes Classification (sentiment and/or topic).,https://www.reddit.com/r/MachineLearning/comments/47614y/learn_properly_to_do_naive_bayes_classification/,ataspinar,1456223417,,0,2,False,http://b.thumbs.redditmedia.com/6E0YXUncXhFZE-HgC8IH7COjePvS20nr4P30WrcpyiU.jpg,,,,,
684,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,20,47673o,self.MachineLearning,Does this use Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/47673o/does_this_use_machine_learning/,angstrem,1456227016,"Hi everyone,

A friend shared a link with me on [DoNotPay](http://www.donotpay.co.uk/signup.php), a lawyer bot. I was wondering how it works: does it use machine learning, or just a very smart algorithm? Any guesses or info? I find it fascinating that programs can already be lawyers.",0,0,False,self,,,,,
685,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,20,4768n9,i.imgur.com,"I feel lonely , Boys write me an email! anna..cat.994 gmail.com",https://www.reddit.com/r/MachineLearning/comments/4768n9/i_feel_lonely_boys_write_me_an_email_annacat994/,raibli2362,1456227841,,0,1,False,default,,,,,
686,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,22,476hgi,self.MachineLearning,Tiled CNN implementation available?,https://www.reddit.com/r/MachineLearning/comments/476hgi/tiled_cnn_implementation_available/,[deleted],1456232577,[deleted],0,0,False,default,,,,,
687,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,22,476jgw,youtu.be,Deep Learning libraries overview (CS231n lecture),https://www.reddit.com/r/MachineLearning/comments/476jgw/deep_learning_libraries_overview_cs231n_lecture/,Inori,1456233545,,18,68,False,http://a.thumbs.redditmedia.com/cbhTRHsf7NKWriHE6C33CMsmCP60Op6XstiI07UUiC0.jpg,,,,,
688,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,22,476nzx,bayesianbiologist.com,Intro to ML talk,https://www.reddit.com/r/MachineLearning/comments/476nzx/intro_to_ml_talk/,likelihoodtprior,1456235638,,0,0,False,http://b.thumbs.redditmedia.com/6E0YXUncXhFZE-HgC8IH7COjePvS20nr4P30WrcpyiU.jpg,,,,,
689,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,23,476p89,self.MachineLearning,Emulating a joystick to create machine learning program for playing video games,https://www.reddit.com/r/MachineLearning/comments/476p89/emulating_a_joystick_to_create_machine_learning/,[deleted],1456236169,[deleted],9,0,False,default,,,,,
690,MachineLearning,t5_2r3gv,2016-2-23,2016,2,23,23,476wbs,self.MachineLearning,ML theory -- Computing posterior distribution,https://www.reddit.com/r/MachineLearning/comments/476wbs/ml_theory_computing_posterior_distribution/,habalbaababa,1456238982,"I'm taking a machine learning couse and this is one of the questions that prepare us for implementation of our ML project.

Question: http://imgur.com/1ruFq6R

My solution: http://imgur.com/oAYRutd

Do you think it is correct? I want to make sure that I understood the topics. I'd really appreciate your feedback. ",3,0,False,self,,,,,
691,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,0,4770c3,blogs.msdn.com,Step by Step how to predict the future with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4770c3/step_by_step_how_to_predict_the_future_with/,arabadzhiev,1456240454,,0,0,False,http://b.thumbs.redditmedia.com/k8XbUTrub3P3C3FETMSPMwZdizvFzSfiLFDLxainPvM.jpg,,,,,
692,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,0,4776b0,somatic.io,deep learning is literally too expensive,https://www.reddit.com/r/MachineLearning/comments/4776b0/deep_learning_is_literally_too_expensive/,toisanji,1456242624,,19,0,False,default,,,,,
693,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,0,4777qu,self.MachineLearning,Show me the reproducible research being produced at Computer Science departments at the best US universities.,https://www.reddit.com/r/MachineLearning/comments/4777qu/show_me_the_reproducible_research_being_produced/,warisaracket1,1456243132,"If you don't do reproducible research -- and I have seen plenty of reproducible research coming from Biostatistics departments but unfortunately not from Computer Science departments, even papers coming from A-list universities -- then your false positives rates might never be known.

You can very likely make some money for yourself, your advisor, your department, your school, and also maybe even help increase budgets at government agencies who find your paper using google and get it approved because of the provenance of a school's famous brand name.

Keep in mind though, that false positives from your model can have real human costs. False positives have costs in choosing medical treatments as well as identifying terrorists. There are very few positive examples for certain kinds of examples. 

Can someone show me a web URL to a CS paper that actually did reproducible research?  I have found many, many CS papers during my work in machine learning, none having code or data available, yet all having claims of efficacy.",6,0,False,self,,,,,
694,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,1,477cyp,self.MachineLearning,Does this need a recommender system + collaborative filtering?,https://www.reddit.com/r/MachineLearning/comments/477cyp/does_this_need_a_recommender_system_collaborative/,sss2208,1456244951,"Preface: I'm a noob at implementing ML algorithms but I want to understand it first from the model perspective. I'm using scikit-learn but also have apache mahout installed.
---

So I have a problem where I have a dataset that includes a list of **Tools**. The data is structured as follows:

- The users do not rate the Tools, they simply use them in a method that adds the tools to a Task. Every entry in the dataset is a history of the users adding the tool to a Task, so there can be multiple entries of one Tool being added to a different Task

- As of right now, there is no data coming in that shows what user added the Tool. There is only a database of Tools associated with Tasks, giving information about the Task such as how much the task may cost, what Asset the Task is tied to, etc.

- Each Task has a specific code, and Tasks can have many Tools.

The purpose of what I'm trying to implement is have an individual user get Tools suggested to them when using the application. However, I'm wondering what I need to use if there are no users associated with adding a Tool to a Task. It looks like I would need to use a content-based collaborative filtering because I want to essentially create a curated list of suggested Tools, ranked by how much I think the user would want to use them.

Is this a binary classification task? Are the two classes ""suggest"" and ""not suggest"" ? I was wondering if I could also model it with a decision tree, and have different weights for each node. So, I have a node at the top that says something like, ""has this Tool been used by this user before"", and stuff like that, each conditional branch having different weights or something.

What's the best way to think about this and is it possible to get my desired result at all? ",2,0,False,self,,,,,
695,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,1,477hj2,self.MachineLearning,A representative state of the art architecture for CIFAR,https://www.reddit.com/r/MachineLearning/comments/477hj2/a_representative_state_of_the_art_architecture/,bbsome,1456246530,"I couldn't find a good representative architecture for state of the art CNNs on CIFAR (excluding res-nets and tricks like BN, which can always be added on the side). I looked at several different papers and everyone is using a different one. Does anyone of you know such or have link to? Sorry, I'm not usually doing vision so not really well know the field. ",5,1,False,self,,,,,
696,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,2,477lep,self.MachineLearning,A Neural Network in 28 Lines of Theano,https://www.reddit.com/r/MachineLearning/comments/477lep/a_neural_network_in_28_lines_of_theano/,code_kansas,1456247670,"In the spirit of [A Neural Network in 11 Lines of Python](https://iamtrask.github.io/2015/07/12/basic-python-network/), I wrote a blog post on [doing the same thing using Theano](https://benjaminbolte.wordpress.com/2016/02/23/a-neural-network-in-22-lines-of-theano/). I didn't see one in a Google search, and I'm learning how to use Theano right now, so I figured I'd try it out. Feedback is welcome!",7,21,False,self,,,,,
697,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,2,477ooh,learnopencv.com,Video tutorial for beginners for training a Convolutional Neural Network for flower classification using DIGITS 3 on Amazon EC2,https://www.reddit.com/r/MachineLearning/comments/477ooh/video_tutorial_for_beginners_for_training_a/,spmallick,1456248625,,0,5,False,http://b.thumbs.redditmedia.com/M9TXBA3m0DjM4xAcOb2tOUpz5iFqrpHxQdzhfQYwdLI.jpg,,,,,
698,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,2,477ri3,self.MachineLearning,Machine Learning Basics (Probability) question,https://www.reddit.com/r/MachineLearning/comments/477ri3/machine_learning_basics_probability_question/,habalbaababa,1456249543,"I'm taking a machine learning couse and this is one of the questions that prepare us for implementation of our ML project.

Question: http://imgur.com/jDfrLIX

My solution: http://imgur.com/Q8xbR79

Do you think it is correct? I want to make sure that I understood the topics. I'd really appreciate your feedback. ",0,0,False,self,,,,,
699,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,2,477srp,self.MachineLearning,"The earliest commit in the torch repo calls it ""torch7"". Yann LeCun cited it as being the successor to Lush. So where did the 7 in torch7's name come from?",https://www.reddit.com/r/MachineLearning/comments/477srp/the_earliest_commit_in_the_torch_repo_calls_it/,lahwran_,1456249925,"https://github.com/torch/torch7/commit/053065ba239e3421d1b2cbec55e3f177a21d6829

This seems like an amusing small mystery to me. Was there ever a torch 6? Reading Yann LeCun's AMA, [he mentions that ""Torch is the direct heir of Lush""](https://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/chiz4e1). So, anyone know where the seven came from?",4,4,False,self,,,,,
700,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,3,477zcz,s123123d11fsdf.com,I'm bored! Boys write to me on mail: izabellapogonalova@gmail.com. ZA26c,https://www.reddit.com/r/MachineLearning/comments/477zcz/im_bored_boys_write_to_me_on_mail/,lycontlens,1456251981,,0,1,False,default,,,,,
701,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,3,477zy8,arxiv.org,Principal Component Projection Without Principal Component Analysis,https://www.reddit.com/r/MachineLearning/comments/477zy8/principal_component_projection_without_principal/,muktabh,1456252148,,0,55,False,default,,,,,
702,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,3,47814y,self.MachineLearning,What are some good resources for learning about Swarm Intelligence?,https://www.reddit.com/r/MachineLearning/comments/47814y/what_are_some_good_resources_for_learning_about/,rayishu,1456252476,,3,9,False,self,,,,,
703,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,3,4781vz,self.MachineLearning,Are Makefiles a good idea for building multiple models ?,https://www.reddit.com/r/MachineLearning/comments/4781vz/are_makefiles_a_good_idea_for_building_multiple/,FilippoC,1456252681,"Hello everyone !

I am working with structured prediction and I need to analyze the output of various models. That is :

 - I have several learning algorithm

 - and I also have several decoding algorithm



And everything can be combined, ie. I can learn with algorithm A or B and decode on test data with algorithm X and Y for example. So I want to have the following combinations :

 - A + X

 - A + Y

 - B + X

 - B + Y

And then on the output of the test data, I do several analyzes...etc etc. Everything is quite a mess.

Some times ago I asked a question for keeping track of experiments ( https://www.reddit.com/r/MachineLearning/comments/3npg0d/how_to_keep_track_of_experiments/ ), but here the problem is a bit different.


My first guess was : use Make !


That sound appealing at first. My files go like :

 - models/A

 - models/B

 - test/A/X

 - test/A/Y

 - test/B/X.output

 - test/B/X.score

 - test/B/Y.score


Remember this is a simplified example. If everything seems ok with a Make, in practice it is'nt :

 - you can only have one ""%"" per target rule

 - The ""chain of implicit rule"" is a pain in the *ss because i NEVER want to delete intermediate files : http://stackoverflow.com/questions/6675087/how-to-use-chains-of-implicit-rules-for-makefiles

 - etc etc


So I ended up with a total stupid make file, were each rule is a .PHONY rule and nothing as any dependency, I just make sure I run everything in the right order. So my make file just ended up as a file with scripts to generate each file, without using any of the benefits of Make.


Has anyone know good alternative to make ? Did anyone test Ruffus ?
I also though about generating very very big makefiles with no use of % and where every rule is forced as .PRECIOUS and .SECONDARY.
Do you think it is a good idea ?


Thanks ! :-)",4,1,False,self,,,,,
704,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,3,4782qm,dtab.io,Linear regression with spreadsheet and highcharts,https://www.reddit.com/r/MachineLearning/comments/4782qm/linear_regression_with_spreadsheet_and_highcharts/,dnprock,1456252934,,0,1,False,default,,,,,
705,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,3,4784bo,intrafind.de,Counting counts  philosophical arguments for using statistics to process language,https://www.reddit.com/r/MachineLearning/comments/4784bo/counting_counts_philosophical_arguments_for_using/,bachbaridade,1456253461,,0,1,False,default,,,,,
706,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,4,4786ae,self.MachineLearning,has anyone used this R package? fbRads,https://www.reddit.com/r/MachineLearning/comments/4786ae/has_anyone_used_this_r_package_fbrads/,kailovesdata,1456254137,"I couldn't find a lot of info about fbRads. If you can help me, I would appreciate it a lot! ",3,0,False,self,,,,,
707,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,5,478p4y,self.MachineLearning,Typical salary for entry level data science role in SF?,https://www.reddit.com/r/MachineLearning/comments/478p4y/typical_salary_for_entry_level_data_science_role/,[deleted],1456260574,[deleted],6,2,False,default,,,,,
708,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,7,47979k,yerevann.com,Dynamic Memory Networks for bAbI tasks: online playground,https://www.reddit.com/r/MachineLearning/comments/47979k/dynamic_memory_networks_for_babi_tasks_online/,HrantKhachatrian,1456266560,,2,4,False,default,,,,,
709,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,7,4798ba,reddit.com,best dating websites in india zbHHB,https://www.reddit.com/r/MachineLearning/comments/4798ba/best_dating_websites_in_india_zbhhb/,reitehol,1456266901,,0,1,False,default,,,,,
710,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,7,47994r,dato.com,"I don't like re-learning new tools, I like to stick to R language , but was asked to review GraphLab Create. Anyone here use it or evaluate it?",https://www.reddit.com/r/MachineLearning/comments/47994r/i_dont_like_relearning_new_tools_i_like_to_stick/,ruskeeblue,1456267175,,5,0,False,default,,,,,
711,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,7,479ct8,youtube.com,DanDoesData: Using skflow for custom models,https://www.reddit.com/r/MachineLearning/comments/479ct8/dandoesdata_using_skflow_for_custom_models/,vanboxel,1456268370,,0,0,False,default,,,,,
712,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,8,479f73,self.MachineLearning,Looking for collaboration in tennis prediction,https://www.reddit.com/r/MachineLearning/comments/479f73/looking_for_collaboration_in_tennis_prediction/,okh2,1456269220,"Over the past few days, I've been implementing this thesis http://www.doc.ic.ac.uk/teaching/distinguished-projects/2015/m.sipko.pdf . I have got the data and written a C# program that extracts the desired features. Then I used logistic regression for prediction, but results are not very good. The final goal would be creating a model that predicts who will win the match, and a smart betting strategy to take advantage of those predictions.
I'm looking for collaborators, or people who have done similar things in the past that want to share their experience!

EDIT 03/01/2016: I uploaded the data and the code to Github https://github.com/okh1/tennis-prediction .",7,2,False,self,,,,,
713,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,10,479zb2,news.ycombinator.com,Realtime audio analysis of YouTube videos using Watson services,https://www.reddit.com/r/MachineLearning/comments/479zb2/realtime_audio_analysis_of_youtube_videos_using/,liviobs,1456276612,,9,7,False,default,,,,,
714,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,12,47ah8m,github.com,Dora - automated exploratory data analysis in python,https://www.reddit.com/r/MachineLearning/comments/47ah8m/dora_automated_exploratory_data_analysis_in_python/,epsteinN,1456283630,,3,2,False,http://a.thumbs.redditmedia.com/d6kZcrxHP0H0RZnmuwuZ7ulWZR-NkNnhTkb6UwpwI64.jpg,,,,,
715,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,12,47ahxr,youtube.com,Signal Processing and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/47ahxr/signal_processing_and_machine_learning/,ScientiaOmniaVincit,1456283899,,0,2,False,http://b.thumbs.redditmedia.com/Z3imkZwTPyEkKO_6f00uIll-BDE5S9K7Pgj8Z75FG9E.jpg,,,,,
716,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,13,47ast8,thepixelbeard.com,Best Free Machine Learning Ebooks,https://www.reddit.com/r/MachineLearning/comments/47ast8/best_free_machine_learning_ebooks/,adda52poker,1456288228,,9,117,False,http://b.thumbs.redditmedia.com/TWSoiazVqxuypS9Q-t4UReKOzP8SvCB_J0858AYETDA.jpg,,,,,
717,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,13,47asuj,arxiv.org,"[1602.07261] Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",https://www.reddit.com/r/MachineLearning/comments/47asuj/160207261_inceptionv4_inceptionresnet_and_the/,m_ke,1456288245,,17,31,False,default,,,,,
718,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,14,47b2wc,arxiv.org,[1602.06709] Distributed Deep Learning Using Synchronous Stochastic Gradient Descent,https://www.reddit.com/r/MachineLearning/comments/47b2wc/160206709_distributed_deep_learning_using/,fulcrum_xyz,1456292635,,9,11,False,default,,,,,
719,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,15,47b648,self.MachineLearning,Why no Torch equivalent in Python?,https://www.reddit.com/r/MachineLearning/comments/47b648/why_no_torch_equivalent_in_python/,laura1222,1456294143,"Why Python doesn't have a modular, non complied, high quality tensor library for both CPU and GPU?

There was gnumpy, but that is buried down deep now. Anyone trying to build GPU numpy ?",6,4,False,self,,,,,
720,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,15,47b89n,github.com,"Object detection code in tensorflow with GoogLeNet-Overfeat model, pretrained on ImageNet.",https://www.reddit.com/r/MachineLearning/comments/47b89n/object_detection_code_in_tensorflow_with/,[deleted],1456295190,[deleted],0,1,False,default,,,,,
721,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,15,47b8ez,github.com,"Object detection code with Tensorflow using GoogLeNet-Overfeat model, pretrained on ImageNet",https://www.reddit.com/r/MachineLearning/comments/47b8ez/object_detection_code_with_tensorflow_using/,singularai,1456295261,,0,7,False,http://b.thumbs.redditmedia.com/ljAIkHHaxPz7oQ9uzelBG-SZ3KtEwqfVbuyJAWQkg_s.jpg,,,,,
722,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,15,47b941,fdjohnson.sitey.me,How Lubrication Pumps can Help Maintain Your Machinery?,https://www.reddit.com/r/MachineLearning/comments/47b941/how_lubrication_pumps_can_help_maintain_your/,jackerfrinandis,1456295580,,0,1,False,default,,,,,
723,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,15,47ba9f,self.MachineLearning,Unsupervised vs Supervised learning for e-commerce session data?,https://www.reddit.com/r/MachineLearning/comments/47ba9f/unsupervised_vs_supervised_learning_for_ecommerce/,[deleted],1456296139,[removed],0,1,False,default,,,,,
724,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,16,47bgot,self.MachineLearning,German books on Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/47bgot/german_books_on_machine_learning/,SeveQStorm,1456299515,"Hi machine teachers,

do you know of any (good) books on machine learning that are written in (or translated to) German?

Cheers!",6,4,False,self,,,,,
725,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,21,47cbxy,self.MachineLearning,conditional probability,https://www.reddit.com/r/MachineLearning/comments/47cbxy/conditional_probability/,John_Smith111,1456316625,"Hello all

as i see from the paper conditional probability is given by 
P (Y|X, W) =  exp (E(W,Y,X)) / integral(exp(E(W,y,X))) 

what the partition term means - do we compute energies when we input X in respect to all possible y values (y1, y2,  y3)

if yes, then we will have many terms with very high values  - when x is measured to  incorrect output Y ?

Is that correct ? 

",11,0,False,self,,,,,
726,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,21,47cf55,self.MachineLearning,How to locate objects using a classifier?,https://www.reddit.com/r/MachineLearning/comments/47cf55/how_to_locate_objects_using_a_classifier/,[deleted],1456318142,[deleted],0,0,False,default,,,,,
727,MachineLearning,t5_2r3gv,2016-2-24,2016,2,24,22,47ch8r,wired.com,Boston Dynamics New Robot Is Wicked Good at Standing Up to Bullies,https://www.reddit.com/r/MachineLearning/comments/47ch8r/boston_dynamics_new_robot_is_wicked_good_at/,Martin81,1456319050,,33,130,False,http://b.thumbs.redditmedia.com/lHA-XGwpsTHeXLZ5_ZlPmgsWg3B6PjE-cPvwHEmqNKE.jpg,,,,,
728,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,0,47d29d,self.MachineLearning,Best practices to build and evaluate a taxonomy for classification?,https://www.reddit.com/r/MachineLearning/comments/47d29d/best_practices_to_build_and_evaluate_a_taxonomy/,[deleted],1456327028,[deleted],0,0,False,default,,,,,
729,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,0,47d94o,gitxiv.com,Adversarial Autoencoders,https://www.reddit.com/r/MachineLearning/comments/47d94o/adversarial_autoencoders/,mere_mortise,1456329375,,2,17,False,default,,,,,
730,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,1,47dbzj,self.MachineLearning,Example of energy based model for multi-label classification,https://www.reddit.com/r/MachineLearning/comments/47dbzj/example_of_energy_based_model_for_multilabel/,John_Smith111,1456330387,"Hello 

Could anyone show me example of energy landscape for multi-label classification  - classification of animal images, image of numbers from 0-9 or similar.

10x all ",3,0,False,self,,,,,
731,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,1,47dilt,self.MachineLearning,Having issues with Speech Recognition using CTC and Torch,https://www.reddit.com/r/MachineLearning/comments/47dilt/having_issues_with_speech_recognition_using_ctc/,Stormfreek,1456332660,"I've been trying to use the recently released [warp-ctc](https://github.com/baidu-research/warp-ctc) library by Baidu but I'm having some difficulties!

The project currently uses the torch library and is written in lua. I've created a neural network under guidance from this [paper](http://arxiv.org/pdf/1512.02595v1.pdf) and you can see my neural network [here](https://github.com/SeanNaren/CTCSpeechRecognition/blob/master/Network.lua) (where I've created the network).

The idea was to input the spectrogram data from audio files through convolution layers to bidirectional RNNs layers. The final output range is from 0 to 27, where 0 is a blank and 1-27 are the characters of the alphabet.

My main issue is that after a certain number of epochs of training the network always seems to predict only blanks and never any characters.

An idea as to why this may be is the large number of blank frames compared to letter frames in the training samples however I'm not entirely sure how to test this or confirm that this is the underlying issue.

I feel like my problem is just a general CTC problem that you guys might be able to help or guide me on, any ideas/questions or feedback on my approach (what would you do differently) would be great, don't hesitate to ask/post!

The code is available [here](https://github.com/SeanNaren/CTCSpeechRecognition)

",7,0,False,self,,,,,
732,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,1,47djrj,arxiv.org,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",https://www.reddit.com/r/MachineLearning/comments/47djrj/inceptionv4_inceptionresnet_and_the_impact_of/,[deleted],1456333064,[deleted],1,0,False,default,,,,,
733,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,2,47dtpp,orastack.com,Why use Gini Index as an impurity measure?,https://www.reddit.com/r/MachineLearning/comments/47dtpp/why_use_gini_index_as_an_impurity_measure/,[deleted],1456336090,[deleted],4,0,False,default,,,,,
734,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,3,47dzrv,medium.com,NTM-Lasagne: A Library for Neural Turing Machines in Lasagne  Snips Blog,https://www.reddit.com/r/MachineLearning/comments/47dzrv/ntmlasagne_a_library_for_neural_turing_machines/,oulipo,1456337972,,4,27,False,http://b.thumbs.redditmedia.com/tblMtgsTgJRTBbTPcDbO1LwCLG9YZIL_W3bpOEibwPQ.jpg,,,,,
735,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,3,47e31a,self.MachineLearning,[Futurism] Imagine a Deep Learning-like algorithm that can analyze video and determine if a poker player is bluffing or not.,https://www.reddit.com/r/MachineLearning/comments/47e31a/futurism_imagine_a_deep_learninglike_algorithm/,xristos_forokolomvos,1456338971,Wouldn't it be awesome? ,3,0,False,self,,,,,
736,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,3,47e51v,self.MachineLearning,What's your opinion on Sentdex's Machine Learning videos?,https://www.reddit.com/r/MachineLearning/comments/47e51v/whats_your_opinion_on_sentdexs_machine_learning/,TylerBlackett,1456339610,This is the playlist in context: https://www.youtube.com/playlist?list=PLQVvvaa0QuDd0flgGphKCej-9jp-QdzZ3,2,0,False,self,,,,,
737,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,4,47ean0,yahoohadoop.tumblr.com,Distributed Deep Learning open-sourced by Yahoo (Caffe on Spark),https://www.reddit.com/r/MachineLearning/comments/47ean0/distributed_deep_learning_opensourced_by_yahoo/,badrinarayan,1456341302,,0,18,False,http://b.thumbs.redditmedia.com/LUsHQa40izZDkusGn-4_VQarf0XGD9nr-r-c9qmw0-c.jpg,,,,,
738,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,5,47eupz,self.MachineLearning,Deep Autoencoders,https://www.reddit.com/r/MachineLearning/comments/47eupz/deep_autoencoders/,yonotron,1456347418,"So I've been reading about stacked denoising autoencoders. I want to use them for an information retrieval project in order to get a good representation of a query image, which I can then compare against a database of images. I've been reading the tutorial on http://deeplearning.net/tutorial/SdA.html and they use a layer-wise training. Is it possible to train a deep autoencoder in one step by using a rectified linear activation?",4,3,False,self,,,,,
739,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,6,47f8b8,certifiedlettertool.com,let's have some fun here 2QJYz0Cc,https://www.reddit.com/r/MachineLearning/comments/47f8b8/lets_have_some_fun_here_2qjyz0cc/,yfpkownarxdcm,1456350857,,0,1,False,default,,,,,
740,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,7,47fewl,self.MachineLearning,Initializing weights using relu,https://www.reddit.com/r/MachineLearning/comments/47fewl/initializing_weights_using_relu/,isaacgerg,1456352293,"So it seems that when using relu, you should initialize the weight matrix with something such that the values are greater than zero.  It would seem that initializing it to something less than zero would effectively deaden the neuron because of the relu activation function.  Is this correct?",11,1,False,self,,,,,
741,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,7,47fgwa,self.MachineLearning,FRP and neural networks,https://www.reddit.com/r/MachineLearning/comments/47fgwa/frp_and_neural_networks/,ritperson,1456352772,"Is anyone aware of research investigating the use of functional reactive programming (FRP) to train and/or architect neural networks?

The use of FRP signals seems like a fantastic way to stream data for continuous training of the NN or SVM, specifically when it comes to high-throughput data in the context of a system that demands concurrency and/or asynchrony. These are exactly the kind of systems we have to deal with in enterprise data analysis, think Kappa/Lambda/Zeta architecture. With such large and complex systems, any design pattern that simplifies and modularizes your machine learning code is a net win and worth investigating.

Elm's take on [Concurrent FRP](http://elm-lang.org/papers/concurrent-frp.pdf) looks promising, but of course it would need to be ported to a language for use on the backend. (Such a port is easy enough to do in Clojure, my lang of choice.)",2,5,False,self,,,,,
742,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,7,47fj70,deepmind.com,DeepMind Health,https://www.reddit.com/r/MachineLearning/comments/47fj70/deepmind_health/,SuperFX,1456353331,,54,82,False,http://b.thumbs.redditmedia.com/ZJqDjIiam0caZTMj4F11h8GKFNe6xv-lKe0qnekaJzM.jpg,,,,,
743,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,7,47fjlf,conf.startup.ml,"Machine Learning in Trading, Startup.ML / Bloomberg conference on May 12",https://www.reddit.com/r/MachineLearning/comments/47fjlf/machine_learning_in_trading_startupml_bloomberg/,arshakn,1456353427,,0,1,False,http://b.thumbs.redditmedia.com/yBt330XyE-y0B6sstJQaHtBp1cB1u7rutt2chgpZ37k.jpg,,,,,
744,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,7,47fk8q,bloomberg.com,Google DeepMind Forms Health Unit to Build Medical Software,https://www.reddit.com/r/MachineLearning/comments/47fk8q/google_deepmind_forms_health_unit_to_build/,MetricSpade007,1456353578,,1,50,False,http://b.thumbs.redditmedia.com/l706cmHSDmN7Em5Z5_5Fu6H8KPdeljuC73Wg6lKawaE.jpg,,,,,
745,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,9,47g75i,nextplatform.com,"Caffe, Spark, Hadoop, GPUs, Infiniband--All on the Same Cluster",https://www.reddit.com/r/MachineLearning/comments/47g75i/caffe_spark_hadoop_gpus_infinibandall_on_the_same/,[deleted],1456360699,[deleted],0,2,False,default,,,,,
746,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,13,47h56a,self.MachineLearning,Help needed: how to avoid weights being too small in deep learning?,https://www.reddit.com/r/MachineLearning/comments/47h56a/help_needed_how_to_avoid_weights_being_too_small/,rgoliax,1456373399,"There are weight decay to avoid weights being too big, but is there anything to make in the other direction? How about weight normalization?

Thanks!",8,0,False,self,,,,,
747,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,13,47h7uq,idleexperts.com,Bijur Delimon  How Lubrication is Essential To Increase Your Machinery Life?,https://www.reddit.com/r/MachineLearning/comments/47h7uq/bijur_delimon_how_lubrication_is_essential_to/,jackerfrinandis,1456374560,,0,1,False,default,,,,,
748,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,13,47ha7u,rewrite.ca.com,"Will AI, big data or crowdsourcing deliver the next medical breakthrough?",https://www.reddit.com/r/MachineLearning/comments/47ha7u/will_ai_big_data_or_crowdsourcing_deliver_the/,suggestic,1456375588,,0,0,False,http://a.thumbs.redditmedia.com/qbO5bdGugZq-GOkrOCOL_-0XLt1PlOS5wOoK7-1OLY8.jpg,,,,,
749,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,14,47hfn6,github.com,Reading irctc captchas with 95% accuracy using deep learning,https://www.reddit.com/r/MachineLearning/comments/47hfn6/reading_irctc_captchas_with_95_accuracy_using/,matrix2596,1456378024,,6,17,False,http://b.thumbs.redditmedia.com/2YgweoYZ0vcDLWSyW9jW0rYkGu5ur4fl-yNrflSSgcs.jpg,,,,,
750,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,14,47hhcu,self.MachineLearning,image archives for training machine learning,https://www.reddit.com/r/MachineLearning/comments/47hhcu/image_archives_for_training_machine_learning/,feelix,1456378898,"I have an application in mind for using image recognition. I would have thought that there would be loads of large open source image archives that are labeled for training, but it seems that that is not the case, right?

Also, I have a specific category I would like to train the library on. Is there any generally accepted bets practice for acquiring thousands of labeled images to do this? Any solution better than writing an image scraper from scratch, or taking the photos manually, would be great to know.",4,1,False,self,,,,,
751,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,16,47hupn,github.com,Instant setup of a Googlenet-based image classification service for less than 1$,https://www.reddit.com/r/MachineLearning/comments/47hupn/instant_setup_of_a_googlenetbased_image/,pilooch,1456386438,,3,0,False,http://a.thumbs.redditmedia.com/5JHNAkgcP0SXvSAfZy0tZEOeiauVTOGKaoFBM6o_3b0.jpg,,,,,
752,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,19,47i81z,cs.jhu.edu,High-Level Explanation of Variational Inference,https://www.reddit.com/r/MachineLearning/comments/47i81z/highlevel_explanation_of_variational_inference/,rd11235,1456394893,,0,5,False,default,,,,,
753,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,21,47ilfl,self.MachineLearning,Need some guidance on Reinforcement/Active learning capabilities,https://www.reddit.com/r/MachineLearning/comments/47ilfl/need_some_guidance_on_reinforcementactive/,kindasortadata,1456402522,"Hello all.

I am looking to understand where the state-of-the-art is currently with regards to pattern detection in datasets using reinforcement/active learning techniques.

I am working with large, messy datasets that human users are naturally good at repairing but are very very hard to progammatically repair - there are so many possible options that the humans ""learn by doing"" - it's a  ""chicken sexing problem"" - http://www.businessinsider.com/the-incredible-intuition-of-professional-chicken-sexers-2012-3

The problem is very complex to code for with traditional methods- far to high dimensional to use a traditional Baysian feedback system on. Flip side is that I have millions of correctly labelled records, and those that were initially ""bad"" have been repaired and I have millions of these results as well.

Whats are people doing in this space at the moment? I What are some good papers I should be looking at or research groups I should be linking up with and talking to?

 
",8,0,False,self,,,,,
754,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,21,47ioex,self.MachineLearning,I'm taking a Machine Learning course. I can't understand the material. I'm 1 inch away from being screwed. Can you help me?,https://www.reddit.com/r/MachineLearning/comments/47ioex/im_taking_a_machine_learning_course_i_cant/,asmdnamsndmasndmasnd,1456404045,"This post will probably downvoted to oblivion but I still had to ask this.

I'm taking an elective Introduction to Machine Learning course. I can't understand the topics even though I go the lectures and study on my own. I can't find a tutor from my network. Everyone is busy trying to graduate. I know too little to ask the professors or TAs. I'm too slow and behind.

Mainly I have trouble understanding and remembering the notations, formulas and meanings related to probability and statistics. I took relevant math courses but it was 2 years ago and I don't remember them. I made a terrible mistake to fall into this position.

I'm taking 5 CS courses and 4 of them are going great. Machine learning is the only course I'm having trouble with. This is my last semester of my Computer Science degree. If I fail, my graduation will delay 6 months. So I don't have an option to fail this course.

I'd really appreciate if someone can volunteer to be my tutor. I'm a student but I'm even willing to pay you something or buy you something from Amazon for the trouble. From the slides, you can see that I go to Bilkent which is in Turkey. USD/TRY is 3 here so I won't be able to offer much I suppose.

If you don't have time to tutor, it still would be GREAT if we can go over some slides together with screen sharing and voice chat. 

Can you help me? 

[Sample slides](https://www.dropbox.com/s/5kypcqhucibx4nq/04-CS464_Lecture_NaiveBayes1.pdf?dl=0)

[All slides of first 4 weeks](https://www.dropbox.com/s/lvreazvaa9i5rdk/slides.rar?dl=0)",0,2,False,self,,,,,
755,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,22,47iubm,blog.numer.ai,machineJS: Automated machine learning- just give it a data file!,https://www.reddit.com/r/MachineLearning/comments/47iubm/machinejs_automated_machine_learning_just_give_it/,dsernst,1456406837,,3,1,False,http://b.thumbs.redditmedia.com/eQi80p7NqPvNRIPOsqWcHrxTzdlbNS4eMNw0jjL7F1Q.jpg,,,,,
756,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,23,47j2ch,i.imgur.com,R2D2 Image Super-Resolved,https://www.reddit.com/r/MachineLearning/comments/47j2ch/r2d2_image_superresolved/,[deleted],1456409945,[deleted],2,1,False,default,,,,,
757,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,23,47j3p3,i.imgur.com,BB-8 Image Super-Resolved,https://www.reddit.com/r/MachineLearning/comments/47j3p3/bb8_image_superresolved/,kjw0612,1456410438,,68,298,False,http://a.thumbs.redditmedia.com/lQb40SiAa-dKmdo2BFFSaCE4srVrYLcm4Haahx5cjR8.jpg,,,,,
758,MachineLearning,t5_2r3gv,2016-2-25,2016,2,25,23,47j8j6,r-bloggers.com,Is deep learning a Markov chain in disguise?,https://www.reddit.com/r/MachineLearning/comments/47j8j6/is_deep_learning_a_markov_chain_in_disguise/,Renthousiast,1456412304,,21,7,False,http://b.thumbs.redditmedia.com/kW-Bsf4GN_wLfX6YJsybcMKlHXvzgjE0mh5NaAjBSEs.jpg,,,,,
759,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,0,47ja9i,datahack.analyticsvidhya.com,"Data hackathon - Feb 25 -27, 2016 - Amazon gift cards for prizes - you must have knowledge of SAS/R/Python/Julia/Go",https://www.reddit.com/r/MachineLearning/comments/47ja9i/data_hackathon_feb_25_27_2016_amazon_gift_cards/,ruskeeblue,1456412910,,0,2,False,default,,,,,
760,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,0,47jano,self.MachineLearning,What's so great about LSTM?,https://www.reddit.com/r/MachineLearning/comments/47jano/whats_so_great_about_lstm/,nonap_,1456413050,"Everyone and their uncles are using LSTM to do everything from Speech, NLP, to even image detection and generative modelling. 

What suddenly happened? Well, if it's just hardware and data, it should have happened around 2012 with Alex Krizhevsky's breakthrough insight. Not 3-4 years later, right?

What is so great about it and why isn't there even a close competitor to it (apart from GRU), but I see tweets every now and then about a paper with ""outperforms LSTM"" quote. What is happening?",5,3,False,self,,,,,
761,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,0,47je64,arxiv.org,AlexNet-level performance in &lt;1MB,https://www.reddit.com/r/MachineLearning/comments/47je64/alexnetlevel_performance_in_1mb/,XalosXandrez,1456414279,,5,21,False,default,,,,,
762,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,1,47jrx0,medium.com,Follow up to reinforcement learning exploration for autonomous vehicle in Python - plotting loss &amp; hyperparameter tuning,https://www.reddit.com/r/MachineLearning/comments/47jrx0/follow_up_to_reinforcement_learning_exploration/,harvitronix,1456418656,,1,3,False,http://a.thumbs.redditmedia.com/BnxZmWZ7I50ONS5MqwtupaThyTPdQOd6oVcN3ICh3j4.jpg,,,,,
763,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,1,47jsgs,guaana.com,AI-supported language tool for writers. Machine learning experts needed.,https://www.reddit.com/r/MachineLearning/comments/47jsgs/aisupported_language_tool_for_writers_machine/,[deleted],1456418842,[deleted],1,0,False,default,,,,,
764,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,2,47jvlp,1stproductions.com,"Hi, here girls SXtqhai87",https://www.reddit.com/r/MachineLearning/comments/47jvlp/hi_here_girls_sxtqhai87/,krmyfaetbjx,1456419831,,0,1,False,default,,,,,
765,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,2,47jvtn,rll.berkeley.edu,Guided Policy Search code released,https://www.reddit.com/r/MachineLearning/comments/47jvtn/guided_policy_search_code_released/,[deleted],1456419887,[deleted],0,1,False,default,,,,,
766,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,2,47jwkc,github.com,Vectorious - High performance linear algebra for Node.js and the browser,https://www.reddit.com/r/MachineLearning/comments/47jwkc/vectorious_high_performance_linear_algebra_for/,mateogianolio,1456420093,,4,1,False,http://b.thumbs.redditmedia.com/AmqURa7ExOcFjnmDx6q_upvsaC8kq27wsHTSc-_nTGY.jpg,,,,,
767,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,2,47jxzx,rll.berkeley.edu,Guided Policy Search code released (end-to-end robot learning from Berkeley),https://www.reddit.com/r/MachineLearning/comments/47jxzx/guided_policy_search_code_released_endtoend_robot/,johndschulman,1456420527,,3,28,False,http://b.thumbs.redditmedia.com/kvvTKVPtqBLZm2gO0uxWfFEMcX9UuSzpjaF0CKcp2fY.jpg,,,,,
768,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,2,47k6kq,self.MachineLearning,Image Analysis: Align first then learn or the other way round?,https://www.reddit.com/r/MachineLearning/comments/47k6kq/image_analysis_align_first_then_learn_or_the/,geppetto123,1456422461,"Is there a rule of thumb which is more ideal, as im pretty new to machine learning.

Given a learning picture data sets with a group label, the network should determine for a unknown picture the probabilty (0-100%) to a specific group (A,B,C,...). Classification problem, right?

While most pictures look pretty much similar (if you dont know where to look!) certain features in the picture determine the group. The learning pictures are already roughly align (rotation and translation), but anything from standardized.

Is there a rule of thumb which following technique will give better results?

I could align all pictures first as perfectly as possible and then let the machine learning do the learning OR add a certain random rotation and translation to the picture set (increasing it size as one picture appears multiple times) and forcing the learning process to focus on the general pattern instead of looking only at certain important pixels.

Not sure yet how to deal with scaling yet, so Im letting that out of the question as I will tackle it later - but feel free to answer. =)

I cant tell why but the latter looks more robust to me, but it's just a guess.",5,0,False,self,,,,,
769,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,2,47k6uh,self.MachineLearning,[arXiv:1602.07416]Learning to Generate with Memory,https://www.reddit.com/r/MachineLearning/comments/47k6uh/arxiv160207416learning_to_generate_with_memory/,gameofml,1456422524,"This looks like an interesting works that combines deep generative models and memory and attention. But I have a bit trouble understanding what is stored in the memory, and they said the memory cannot be written directly but is instead updated via optimization. Why would this work?",3,0,False,self,,,,,
770,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,2,47k9i6,i.imgur.com,"Yes Princeton Modelnet, Trees and Houses definitely count as ""Loafs.""",https://www.reddit.com/r/MachineLearning/comments/47k9i6/yes_princeton_modelnet_trees_and_houses/,[deleted],1456423106,[deleted],0,0,False,default,,,,,
771,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,3,47kf7w,self.MachineLearning,scikit-learn t-SNE implementation,https://www.reddit.com/r/MachineLearning/comments/47kf7w/scikitlearn_tsne_implementation/,Pfaeff,1456424578,"I am currently looking at the t-SNE implementation that comes with scikit-learn. 

The [official implementation](https://lvdmaaten.github.io/tsne/) comes with an mnist example. I am trying to replicate the results using the scikit-learn implementation, which should in theory be more powerful ([although it has some issues](https://github.com/scikit-learn/scikit-learn/issues/6450#issuecomment-188872587)). 
I can't get it to work as well though. The clusters in the official implementation seem to be separated a lot better than what I can accomplish using scikit.

This is my code:

    pca = PCA(50)
    Y = pca.fit_transform(X)
    tsne = TSNE(n_components=2, perplexity=20, early_exaggeration=4.0, learning_rate=1000, n_iter=1000,
                n_iter_without_progress=50, min_grad_norm=0, init='pca', method='exact', verbose=2)
    Y = tsne.fit_transform(Y)
    
    plot.scatter(Y[:,0], Y[:,1], 20, labels)
    plot.show()



[This is what it looks like](https://dl.dropboxusercontent.com/u/27681596/tsne_comparison.jpg)
",9,12,False,self,,,,,
772,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,3,47kkz5,medium.com,A new test for AI,https://www.reddit.com/r/MachineLearning/comments/47kkz5/a_new_test_for_ai/,tomssilver,1456426304,,3,10,False,http://b.thumbs.redditmedia.com/X_UINS59nD0eUYHIWIG6C8ydG6IqND91vWS3JZUYzNI.jpg,,,,,
773,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,4,47kn8b,willwhitney.github.io,Understanding Visual Concepts with Continuation Learning,https://www.reddit.com/r/MachineLearning/comments/47kn8b/understanding_visual_concepts_with_continuation/,wfwhitney,1456427052,,16,14,False,http://b.thumbs.redditmedia.com/m9k6BXMi_-w9gNqG-niFwuh_enI6gZwl7nJjoo-UTpE.jpg,,,,,
774,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,5,47l59y,self.MachineLearning,Help with plotting t-sne using sklearn with gene expression data,https://www.reddit.com/r/MachineLearning/comments/47l59y/help_with_plotting_tsne_using_sklearn_with_gene/,[deleted],1456431519,[deleted],1,0,False,default,,,,,
775,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,6,47lhk4,self.MachineLearning,Question on Go paper,https://www.reddit.com/r/MachineLearning/comments/47lhk4/question_on_go_paper/,WilliamWallace,1456434677,"In DeepMind's Go paper, they mention having as input a stack of 48 19x19 images. Two of the images are all 1s and all 0s (referred to as the ""Ones"" and ""Zeros"" planes). Why do they utilize these? What's their function?",1,3,False,self,,,,,
776,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,6,47lmjk,youtube.com,Pedro Domingos on the 4 school of thoughtvof Machine Learning and how the lead to the master algorithm,https://www.reddit.com/r/MachineLearning/comments/47lmjk/pedro_domingos_on_the_4_school_of_thoughtvof/,[deleted],1456435941,[deleted],0,1,False,default,,,,,
777,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,7,47lu7e,self.MachineLearning,Can theta be viewed as how much a given feature affects the outcome?,https://www.reddit.com/r/MachineLearning/comments/47lu7e/can_theta_be_viewed_as_how_much_a_given_feature/,dimdal,1456438480,"Let's say I have a house that I want to sell. I also have a large dataset containing sales prices on houses in the same city, with a lot of features (everything from # of rooms, size, interior paint color, years since the bathroom was redecorated and so on ..).

If I fit a linear regression model to the data using gradient descent or similar algorithm, is it correct to assume that the theta values I get, is the ""importance"" of each feature towards the sale price?

I'm trying to grasp how  given such a rich dataset  one would go about maximizing the value of a house; Should I redecorate the bathroom, paint the walls, divide a room into two rooms? 

Which of these features matter the most towards achieving the highest selling price? 
",7,0,False,self,,,,,
778,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,7,47lu9j,technologyreview.com,Water detection in satellite imagery using deep learning,https://www.reddit.com/r/MachineLearning/comments/47lu9j/water_detection_in_satellite_imagery_using_deep/,[deleted],1456438501,[deleted],0,1,False,default,,,,,
779,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,7,47lupi,youtube.com,Pedro Domingos on the 4 school of thought of Machine Learning and how the lead to the master algorithm,https://www.reddit.com/r/MachineLearning/comments/47lupi/pedro_domingos_on_the_4_school_of_thought_of/,insider_7,1456438655,,2,1,False,http://a.thumbs.redditmedia.com/cxGhIh2AvJv2uBEEwft8t6SjxHlgxoDbNw2pmPPP8r4.jpg,,,,,
780,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,7,47lw0m,technologyreview.com,How Deep Learning Gives Us a Precise Picture of All the Water on Earth,https://www.reddit.com/r/MachineLearning/comments/47lw0m/how_deep_learning_gives_us_a_precise_picture_of/,adamwkraft,1456439109,,0,0,False,http://a.thumbs.redditmedia.com/gWwD_Gd1_bU9xV2NeQCHt9BOrMra9lAdEXj_M3Kg5W8.jpg,,,,,
781,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,7,47m0ti,bostonglobe.com,"""Whats the big lesson to learn, in a century when machines can learn? Maybe it is that jobs are for machines, and life is for people.""",https://www.reddit.com/r/MachineLearning/comments/47m0ti/whats_the_big_lesson_to_learn_in_a_century_when/,2noame,1456440828,,4,0,False,http://b.thumbs.redditmedia.com/IgYGUP6txd5wcBLE8yY57k4EzdynRXAnwSn8nH0tWzo.jpg,,,,,
782,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,8,47m51z,enigma.io,"Enigma.io - Quickly search and analyze billions of public records published by governments, companies and organizations",https://www.reddit.com/r/MachineLearning/comments/47m51z/enigmaio_quickly_search_and_analyze_billions_of/,surlyq,1456442403,,9,0,False,http://b.thumbs.redditmedia.com/vYWYlav3W44pSI95yQh6Xo29vkPJv9b6HcpdinXpRTI.jpg,,,,,
783,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,8,47m553,self.MachineLearning,Request for a data set of classified melanoma and non-melanoma images.,https://www.reddit.com/r/MachineLearning/comments/47m553/request_for_a_data_set_of_classified_melanoma_and/,fgfhdsxcfd,1456442430,If you know of any data sets of classified melanoma images where the skin lesion is clearly visible it would be greatly appreciated if you posted them below.,4,0,False,self,,,,,
784,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,10,47mqad,arxiv-sanity.com,"Arxiv Sanity Preserver, helps you sort through arxiv papers by Andrej Karpathy",https://www.reddit.com/r/MachineLearning/comments/47mqad/arxiv_sanity_preserver_helps_you_sort_through/,rantana,1456450843,,6,57,False,http://b.thumbs.redditmedia.com/s2hoiYCbS8ic9if0qlLdCGqYDcfpPaqjIEG8Nq0OMDY.jpg,,,,,
785,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,11,47mwmx,self.MachineLearning,I don't really know what kind of framework I'm looking for,https://www.reddit.com/r/MachineLearning/comments/47mwmx/i_dont_really_know_what_kind_of_framework_im/,Procrastinator300,1456453458,"So I wanna make this app and wanna use machine learning for it. My app would collect some basic data on myself and depending upon my actions I want it to learn and predict what I'm most likely to do when same kind of  scenario plays out.

I've seen a bit of BigML documentation and seems like something like that would be perfect but then I saw TensionFlow and couple of other ones made by huge companies and it seems like they're only good for Images? Or at least that is what their documentation examples seems to suggest.

I wanted to ask you guys what are my options? I've be using windows for the app and would like to run the DeepLearning stuff on my end but running it on the framework providers servers is also fine as long as it's free. I'd also prefer to use programming languages I'm used to like Java, C# or Javascript but I'm willing to use something else like (TensionFlow) for much, much higher accuracy. 

Also performance is not really a problem as long as it does not bring down my mid-end gaming pc to crawling speeds.",1,0,False,self,,,,,
786,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,13,47ndcw,self.MachineLearning,Looking for derivative help :(,https://www.reddit.com/r/MachineLearning/comments/47ndcw/looking_for_derivative_help/,[deleted],1456460441,[deleted],0,1,False,default,,,,,
787,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,14,47nipm,arxiv.org,[1602.03218] Learning Efficient Algorithms with Hierarchical Attentive Memory,https://www.reddit.com/r/MachineLearning/comments/47nipm/160203218_learning_efficient_algorithms_with/,pranv,1456462855,,3,13,False,default,,,,,
788,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,14,47nkkm,self.MachineLearning,[Question] [Help] How to perform harmonic regression of a signal?,https://www.reddit.com/r/MachineLearning/comments/47nkkm/question_help_how_to_perform_harmonic_regression/,[deleted],1456463689,[deleted],1,1,False,default,,,,,
789,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,16,47nxso,arxiv.org,[1602.07576] Group Equivariant Convolutional Networks,https://www.reddit.com/r/MachineLearning/comments/47nxso/160207576_group_equivariant_convolutional_networks/,iori42,1456470582,,6,18,False,default,,,,,
790,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,16,47nzgk,veermachine.blogspot.in,Wire Straightening Machine India,https://www.reddit.com/r/MachineLearning/comments/47nzgk/wire_straightening_machine_india/,veermachine,1456471633,,0,1,False,default,,,,,
791,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,18,47obvp,self.MachineLearning,How to make a feature vector out of a horse race?,https://www.reddit.com/r/MachineLearning/comments/47obvp/how_to_make_a_feature_vector_out_of_a_horse_race/,Eildosa,1456479522,"So I'm using scikitlearn with it's SVM implementation, you give him a list of feature vectors and targets
Then it can predict a result using the .predict method.

For the sake of the argument let's say that I have 2 horse in my race.
This is the vector I made.

vector = [weather, raceLength, h1Id, h1weight, h1state, j1Id, j1weigh, j1state, h2Id, h2weight, h2state, j2Id, j2weigh, j2state]
target = h1Id

But I could have put h2 and j2 first in the vector like that :

vector = [weather, raceLength, h2Id, h2weight, h2state, j2Id, j2weigh, j2state, h1Id, h1weight, h1state, j1Id, j1weigh, j1state]
target = h1Id

The problem is that for the same track the same horse are not always running but we always have the same number of horse.

should I make an ENORMOUS feature vector with empty values?
For instance if I have 110 horse who runned on the track over 3 years I would then make a 662 values feature vector and put
0 for the horse wich are not running currently running? (6 variable per horse, only 10 horse running at a time, not always the same)

Thanks.",1,1,False,self,,,,,
792,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,19,47oesc,self.MachineLearning,Code review and rechecking of results,https://www.reddit.com/r/MachineLearning/comments/47oesc/code_review_and_rechecking_of_results/,alexeymosc,1456481351,"Hi everybody,

My name is Alexey. I have a piece of code and dataset that make forex quote increment predictions. It is my own experiment and I have already got some positive results.

I am looking for a person (or better several persons) who would be interested in running my R code on the data that I will upload with the ML technique, parameter sets, CV method, loss function, that YOU prefer. Do not be alerted. I am using some high-profile ML and already got a positive result, but I want to make it public. The result of the code working will be a table of results. I would like to get an independent opinion on the reproducibility of the success in forex predictions. 

After I get at least one independent review and results I will upload my own results (all that is for removing any bias in judgment).

Let me briefly inform you of the scale of that experiment. There are 5 currency pairs, each with the 16-year history of quotes in minutes candles. All the history is joined together in the big 80-years long time series. The first 2/3 of that is used for training and CV, and the last 1/3 for validation.

Each example has 108 inputs (the feature engineering part will also be present) and 18 targets that are price increments over expanding time horizon (from 2 to 724 minutes ahead).

In order to make observations non interdependent the sample vectors were drawn from the big dataset with a varying time-step.

The final train set is a matrix of 26 000 * 109, and the validation set is 13 000 * 109.

I can provide you with the final prepared datasets and the R code (using caret) with all the architecture needed for reproducing results with some ML method. The training and validation takes me 5 hours to complete, but my PC is not superb.

As a result of that effort we will all have a ""proof-concept"" of the forex forecasting.

If someone is interested, please post here. 

with kind regards,
Alexey",0,0,False,self,,,,,
793,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,20,47on2c,technologyreview.com,Computing Google Unveils Neural Network with Superhuman Ability to Determine the Location of Almost Any Image,https://www.reddit.com/r/MachineLearning/comments/47on2c/computing_google_unveils_neural_network_with/,fully_connected,1456486554,,2,1,False,http://b.thumbs.redditmedia.com/d14fbQ99le6O-yvkpAgokHDMXilKNReoO0XeELNAmpY.jpg,,,,,
794,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,20,47oo63,datasciencecentral.com,Is deep learning a Markov chain in disguise?,https://www.reddit.com/r/MachineLearning/comments/47oo63/is_deep_learning_a_markov_chain_in_disguise/,ai_maker,1456487186,,2,0,False,http://b.thumbs.redditmedia.com/kW-Bsf4GN_wLfX6YJsybcMKlHXvzgjE0mh5NaAjBSEs.jpg,,,,,
795,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,21,47ot8j,self.MachineLearning,How do I do multilabel image classification in TensorFlow?,https://www.reddit.com/r/MachineLearning/comments/47ot8j/how_do_i_do_multilabel_image_classification_in/,mln00b13,1456489963,I saw something like multi output. Is it possible to do multilabel classification in TensorFlow? Using skflow?,7,1,False,self,,,,,
796,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,22,47p2u8,self.MachineLearning,Is layer-wise pre-training obsolete?,https://www.reddit.com/r/MachineLearning/comments/47p2u8/is_layerwise_pretraining_obsolete/,godspeed_china,1456494545,I see some papers use backpropagation directly for deep network. So my question is what is the main stream style? is layer-wise pre-training obsolete? Thanks!,6,4,False,self,,,,,
797,MachineLearning,t5_2r3gv,2016-2-26,2016,2,26,23,47p56t,github.com,Distributed TensorFlow just open-sourced,https://www.reddit.com/r/MachineLearning/comments/47p56t/distributed_tensorflow_just_opensourced/,carpedm20,1456495598,,50,354,False,http://b.thumbs.redditmedia.com/DTIc8Re5Wtr8FcYpadKuxbnQqiZfVeNkvw3yGFWnZHM.jpg,,,,,
798,MachineLearning,t5_2r3gv,2016-2-27,2016,2,27,0,47pe4n,arxiv.org,[1602.07868] Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/47pe4n/160207868_weight_normalization_a_simple/,dunnowhattoputhere,1456499130,,8,33,False,default,,,,,
799,MachineLearning,t5_2r3gv,2016-2-27,2016,2,27,0,47pkb5,self.MachineLearning,A gender tagged Pedestrian Data,https://www.reddit.com/r/MachineLearning/comments/47pkb5/a_gender_tagged_pedestrian_data/,TheBurpThatGotAway,1456501509,Is there an openly available data set of Pedestrian Images which are tagged as male and female (atleast)?,0,0,False,self,,,,,
800,MachineLearning,t5_2r3gv,2016-2-27,2016,2,27,1,47pszu,self.MachineLearning,Wearable Swipe Gestures not accurate..,https://www.reddit.com/r/MachineLearning/comments/47pszu/wearable_swipe_gestures_not_accurate/,SYCarrot,1456504541,"Hi all,

Beginner here, I am trying to use AzureML to detect swipe directions when wearing a smartwatch [up, down, left, right, forward, backwards] over two seconds of data collection, I extract the following features:

* Accelerometer Mean, Max, Min, Variance.
* Gyroscope Mean, Max, Min, Variance.

I have trained the model with around 300 entries, but the accuracy is **bad** (less than 40%). I use SVM as my base algorithm, what might I be doing wrong?",3,4,False,self,,,,,
801,MachineLearning,t5_2r3gv,2016-2-27,2016,2,27,2,47q5jg,self.MachineLearning,Architectural difference for a network for feature detection/classification and time series prediction,https://www.reddit.com/r/MachineLearning/comments/47q5jg/architectural_difference_for_a_network_for/,geppetto123,1456508294,"I tried to find this answer somewhere but wasn't lucky, seems that it is so trivial that everybody already knows.... However Im genuinely interested.

I started with the Matlab Neural Toolkit which gives quite good examples and tutorials. **However I don't get what makes the difference between parallel input (1 picture) and serial input (time series prediction)**. Or is the time series inputted parallely as a sliding block?

I understand that there must be a difference, because otherwise for a picture the output would depend on the previous pictures what was calculated. How is this done, especially because **CNN** can be used for both applications.


**2nd question:** What I have seen is that for time series you need to preload a certain amount of data points x. Does this mean that as soon as this x points long pattern repeats the network will predict the same future even though the previous 20 data points are totally different? The problem in my point of view is that one cant use x really large because those points are lost for preloading - in my logic it would make sense to give the network a very large portion of all data (?)

As far as I have seen it, x is only in the magnitude of 10 - so this data points could repeat fairy easily. For weather predictions 10 similar days will have totally different 11th day.

**Bonus question:** What makes it really confusion to me is, how would you design a video input network (time series + each input is a picture). Maybe this is a bit theoretical or preprocessing is simpler - however from a pure curious side speaking it should be possible or is it a contradiction? How would it look like? Far-fetched example, but the only one I was able to come up with: fluid prediction with a network (instead of navier stokes finite element/volumina programming)
",1,1,False,self,,,,,
802,MachineLearning,t5_2r3gv,2016-2-27,2016,2,27,2,47q5nz,youtube.com,Teradeep general object classifier,https://www.reddit.com/r/MachineLearning/comments/47q5nz/teradeep_general_object_classifier/,WolfOliver,1456508341,,2,10,False,http://b.thumbs.redditmedia.com/GktlXqo4D0h8E0bkspym3q-10rt3QmmGAqZcdjEfP3Y.jpg,,,,,
803,MachineLearning,t5_2r3gv,2016-2-27,2016,2,27,2,47q9gs,mechanicdrive.com,Buy Spur Gears online at Affordable Price,https://www.reddit.com/r/MachineLearning/comments/47q9gs/buy_spur_gears_online_at_affordable_price/,mechanicdrive,1456509412,,0,0,False,default,,,,,
804,MachineLearning,t5_2r3gv,2016-2-27,2016,2,27,3,47qatg,self.MachineLearning,Good resources on data visualization,https://www.reddit.com/r/MachineLearning/comments/47qatg/good_resources_on_data_visualization/,insider_7,1456509847,"Departing a bit of the overwhelming progress/news/papers on deep learning, can anyone recommend good up-to-date resources (open books, videos, MOOCs, etc) for data visualization? of course, in the context of Machine learning.",1,3,False,self,,,,,
805,MachineLearning,t5_2r3gv,2016-2-27,2016,2,27,3,47qegp,self.MachineLearning,What tools/languages do you use in a professional setting?,https://www.reddit.com/r/MachineLearning/comments/47qegp/what_toolslanguages_do_you_use_in_a_professional/,FoRJP2dot0,1456510899,"I was talking to a friend who works in a ""machine learning equivalent"" department about getting into the field and he said I should learn R, Python, and SQL. I was wondering how many other companies use that mix and if you also recommend those as a starting point. 

I'd appreciate any comments/feedback. 

Thank you! ",5,0,False,self,,,,,
806,MachineLearning,t5_2r3gv,2016-2-27,2016,2,27,3,47qh90,self.MachineLearning,"Is there a case for still using Torch, Theano, Brainstorm, MXNET and not switching to TensorFlow?",https://www.reddit.com/r/MachineLearning/comments/47qh90/is_there_a_case_for_still_using_torch_theano/,drpout,1456511733,"Why would anyone use anything else?

If think about how quickly TF has evolved in last few months since launch, I think by next year or so, it'll subsume all functionality of other libraries. Google has a monster army and a ton of money - heck, they made a course to just teach you TF.

Is there really a point to developing (or contributing) to other libraries?

Also, all libraries seems to be converging to the same design ideas of graphs and tensors. While being elegant and powerful, what's next? What's the perspective that no one is taking (or appreciating enough)?",41,7,False,self,,,,,
807,MachineLearning,t5_2r3gv,2016-2-27,2016,2,27,3,47qjfz,self.MachineLearning,Neural Net + CRF in torch,https://www.reddit.com/r/MachineLearning/comments/47qjfz/neural_net_crf_in_torch/,sld1337,1456512437,"Hi!

Did anyone see implementation of neural network with CRF in torch?

I'm trying to implement sentence-based approach from ['NLP from scratch (2011)'](http://arxiv.org/abs/1103.0398).
I did the convolution net. But I guess it will take a long time to implement sentence-level log likelihood from scratch in torch.",0,4,False,self,,,,,
808,MachineLearning,t5_2r3gv,2016-2-27,2016,2,27,6,47re9d,self.MachineLearning,What are all of the deep learning libraries offered in Python?,https://www.reddit.com/r/MachineLearning/comments/47re9d/what_are_all_of_the_deep_learning_libraries/,Dragonfliesfoos222,1456521618,"I am looking for a list of these libraries to explore. I know theano exists, as well as nolearn.

What else should I know?",4,2,False,self,,,,,
809,MachineLearning,t5_2r3gv,2016-2-27,2016,2,27,7,47rq8a,enneagrammadeeasy.com,My boyfriend and I might be having some fun with another couple this weekend! mgYHVdB,https://www.reddit.com/r/MachineLearning/comments/47rq8a/my_boyfriend_and_i_might_be_having_some_fun_with/,tqafioycsdm,1456525075,,0,1,False,default,,,,,
810,MachineLearning,t5_2r3gv,2016-2-27,2016,2,27,8,47sbbo,self.MachineLearning,LSTMs + Music generation input space question,https://www.reddit.com/r/MachineLearning/comments/47sbbo/lstms_music_generation_input_space_question/,askacadthrowaway,1456531181,"Hello!

I've been looking at some [online] (http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/) LSTM-based music generation [models] (http://www.wise.io/tech/asking-rnn-and-ltsm-what-would-mozart-write) and there's something I haven't been able to understand.


What exactly is the input space?

Intuitively, I'd imagine the best encoding is for there to be 176 notes: 88 ""struck"" notes, and 88 ""held"" notes, where a held note is how you play a note for a level of quantization longer than whatever the smallest one is (probably a 16th note). But it doesn't look like any of the models do that.

Examining the former: I'm not that strong with Theano yet so I'm having a hard time figuring out if my thoughts are right. I think this python code defines what the input space is:

        # From our architecture definition, size of the notewise input
        self.t_input_size = 80

But this doesn't seem to make sense, since the input size logically should be 88 (for each key of a keyboard) or maybe some multiple of 12*n* (notes in an octave, then *n* octaves?).  This is from the first link.

For the second link, they start with a language equivalently expressive to MusicXML. To do what I think makes sense, they would require some careful transformation from a musical format (e.g. MusicXML) into the desired input space. So you'd need to convert an encoding of ""8th note F"" to ""F (struck), F (held)"" to play an 8th note F (assuming the quantization is 16th notes). The latter link  doesn't seem to do that at all... which makes me confused what exactly their input space is.

Does anyone have any ideas here? ",1,5,False,self,,,,,
811,MachineLearning,t5_2r3gv,2016-2-27,2016,2,27,9,47sdi0,self.MachineLearning,Question about how to conduct your Ph.D. in Deep Learning.,https://www.reddit.com/r/MachineLearning/comments/47sdi0/question_about_how_to_conduct_your_phd_in_deep/,andrewbarto28,1456531975,"I am planning to apply to a Ph.D. program soon. I am currently doing masters. I really like to follow the new awesome Deep Learning papers, and I want to implement and try many of them out. Ideally, I would like to conduct my Ph.D. in the following way:

I start by reading all papers and topics that interest me and implement some of them: LSTMs, Attention, NTM, CNNs, Spatial Transformers, RL, Ladder Networks, etc. Then, when I think I can improve on some paper, I take this as a project for a new paper. I keep doing that every six months. Then in 6 years I will have 12 papers!. But the field evolves very fast and even though 'attention' is the hot thing now, next year might be another thing, so I don't want to focus on just one topic as it is common in a Ph.D. But then my thesis won't have a single unifying theme, except that everything is about DL and RL. Is it OK to try to work on many different topics during your Ph.D., like Yoshua Bengio and Nando de Freitas do in their research. Off course, it is easier to a professor do that as they can delegate things to PhDs. Should I wait to become a Professor to work in many different things? I don't want to work for instance in just trying to get a better accuracy in translation. Or just getting a better accuracy in semantic segmentation during my whole Ph.D. Maybe I want to invent new architectures and frameworks that can be applied in many different applications.

Can someone with similar interest as mine talk about how they are approaching their Ph.D.?",19,5,False,self,,,,,
812,MachineLearning,t5_2r3gv,2016-2-27,2016,2,27,13,47tdf4,self.MachineLearning,Bengio's recent work on deep learning and biology,https://www.reddit.com/r/MachineLearning/comments/47tdf4/bengios_recent_work_on_deep_learning_and_biology/,[deleted],1456548752,[deleted],16,6,False,default,,,,,
813,MachineLearning,t5_2r3gv,2016-2-27,2016,2,27,14,47thi9,samcoautoindia.com,Thrust Washers Exporter,https://www.reddit.com/r/MachineLearning/comments/47thi9/thrust_washers_exporter/,samcoautoindia,1456550669,,0,0,False,default,,,,,
814,MachineLearning,t5_2r3gv,2016-2-27,2016,2,27,15,47trts,arxiv.org,[1602.08017] Meta-learning within Projective Simulation,https://www.reddit.com/r/MachineLearning/comments/47trts/160208017_metalearning_within_projective/,InaneMembrane,1456555722,,0,0,False,default,,,,,
815,MachineLearning,t5_2r3gv,2016-2-27,2016,2,27,15,47tsj7,arxiv.org,[1602.06662] Orthogonal RNNs and Long-Memory Tasks (Facebook AI),https://www.reddit.com/r/MachineLearning/comments/47tsj7/160206662_orthogonal_rnns_and_longmemory_tasks/,InaneMembrane,1456556101,,7,6,False,default,,,,,
816,MachineLearning,t5_2r3gv,2016-2-27,2016,2,27,19,47ufju,veermachinery.in,Stirrup Making Machine,https://www.reddit.com/r/MachineLearning/comments/47ufju/stirrup_making_machine/,veermachine,1456570069,,1,0,False,default,,,,,
817,MachineLearning,t5_2r3gv,2016-2-27,2016,2,27,19,47ugeo,self.MachineLearning,[Question] Why have large convolutional kernels fallen out of fashion?,https://www.reddit.com/r/MachineLearning/comments/47ugeo/question_why_have_large_convolutional_kernels/,nasimrahaman,1456570597,"3x3 kernels have been all the rage in recent times. Quoting [Simyonan and Zisserman](http://arxiv.org/pdf/1409.1556.pdf):

""It is easy to see that a stack of two 3  3 conv. layers (without spatial pooling in between) has an effective receptive field of 5  5; three such layers have a 7  7 effective receptive field. So what have we gained by using, for instance, a stack of three 3  3 conv. layers instead of a single 7  7 layer? First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more discriminative. Second, we decrease the number of parameters: assuming that both the input and the output of a three-layer 3  3 convolution stack has C channels, the stack is parametrised by 33^2 C^2 = 27C^2 weights; at the same time, a single 7  7 conv. layer would require 7^2 C^2 = 49C^2 parameters, i.e. 81% more. This can be seen as imposing a regularisation on the 7  7 conv. filters, forcing them to have a decomposition through the 3  3 filters (with non-linearity injected in between).""

On the implementation level, it's hard to beat 3  3 convolutions speed-wise (except maybe if you're using FFT convolutions). 2x 3  3 convolution is 1.4 times faster than 1x 5  5 convolution, 3x 3  3 is 1.8 times faster than 1x 7 x 7, and so on (by ""faster"" I mean the number of computations, considering a convolution is O(nmMN) for m  n kernels on M  N images). 

But memory wise, it's a disaster: 2x 3  3 convolutions requires twice the memory for storing forward pass activations (compared to 1x 5  5 convolution) for the same number of feature maps. One may argue that a sequence of 7 3  3 convolutions is 3.6 times faster than a 15x15 convolution, which might make it worth the 7x (!) memory requirement, but for kernels that size, FFT-conv is a no-brainer. 

So here's one question: if we know that our network is going to be D-layers deep, why prefer 3  3 convolutions instead of 5  5 (or 15  15 for that matter) convolutions, when the latter gives a much better field of view (FOV)? Is the reduced number of parameters and the ""regularisation"" effect worth the trade-off against FOV, assuming my images can be arbitrarily large? ",23,29,False,self,,,,,
818,MachineLearning,t5_2r3gv,2016-2-27,2016,2,27,22,47uv5c,thinkingmachines.mit.edu,Priors and Prejudice in Thinking Machines,https://www.reddit.com/r/MachineLearning/comments/47uv5c/priors_and_prejudice_in_thinking_machines/,insperatum,1456579379,,7,17,False,http://b.thumbs.redditmedia.com/3Lq-1QbVqduhnoED0MwS9vd84tflpQU6v9RxeUiakoU.jpg,,,,,
819,MachineLearning,t5_2r3gv,2016-2-27,2016,2,27,22,47uzg1,arxiv.org,Statistical Mechanics of High-Dimensional Inference,https://www.reddit.com/r/MachineLearning/comments/47uzg1/statistical_mechanics_of_highdimensional_inference/,cesarsalgado,1456581514,,4,43,False,default,,,,,
820,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,0,47vfu8,self.MachineLearning,Question about regression,https://www.reddit.com/r/MachineLearning/comments/47vfu8/question_about_regression/,[deleted],1456585695,[removed],0,1,False,default,,,,,
821,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,0,47vica,self.MachineLearning,Bayesian Methods for Hackers -- solutions?,https://www.reddit.com/r/MachineLearning/comments/47vica/bayesian_methods_for_hackers_solutions/,__AndrewB__,1456586156,"Hello there, 
I've been going through the amazing book [Bayesian Methods for Hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers), but I've noticed there are no solutions to the excercies. This makes self-study quite hard.

Since the book seems to be extremely popular, I thought that maybe someone on this forum have already done those excercises and could link their gihub repo so that I could verify my code?",2,22,False,self,,,,,
822,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,0,47vm09,j.ee.washington.edu,Advanced Inference in Graphical Models [slides + videos],https://www.reddit.com/r/MachineLearning/comments/47vm09/advanced_inference_in_graphical_models_slides/,Kiuhnm,1456587277,,3,20,False,default,,,,,
823,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,1,47w94m,self.MachineLearning,[Question] How do I impose restrictions on structure of output label sequences?,https://www.reddit.com/r/MachineLearning/comments/47w94m/question_how_do_i_impose_restrictions_on/,curryage,1456592109,"I am currently trying to solve a sequence-to-sequence problem. The input training sequence consists of feature vectors (extracted from images) and the output sequence consists of binary classification predictions. The output training sequences have the property that a subsequence consisting only of 1s always ends it. For example, possible outputs sequences are {0,1,0,1,1,1,1} , {0,0,0,0,1,1,1},{0,0,0,0,0,0,1} etc. Is there a learning paradigm wherein I can specify this restriction and incorporate into training such that for a given test input sequence whose last prediction is 1 (i.e. of the form {......,1}) , the output also is constrained to follow this restriction (i.e. a subsequence of 1s ends it). ",3,0,False,self,,,,,
824,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,2,47wggk,self.MachineLearning,"[Question] after object detection using convolution neural networks, why is it so hard to perform semantic segmentation?",https://www.reddit.com/r/MachineLearning/comments/47wggk/question_after_object_detection_using_convolution/,code2hell,1456593248,"[Question] after object detection using convolution neural networks, why is it so hard to perform segmentation(mean accuracy ~72%), is it possible to use hands marked(ground truth) training sets as a mask? Is it a good approach to take? Can we see performance improvement if we use the depth information(rgb-d) vs rgb for semantic segmentation? ",21,0,False,self,,,,,
825,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,2,47wjct,self.MachineLearning,What tool/stack do you use at work to schedule training tasks with limited resources?,https://www.reddit.com/r/MachineLearning/comments/47wjct/what_toolstack_do_you_use_at_work_to_schedule/,ninja_papun,1456593710,I am curious about what tools people use when they have limited resources like GPUs and CPU and multiple people need to train models(mostly deep learning but also random forests and SVMs).,1,0,False,self,,,,,
826,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,2,47wq2i,yicairen.com,I have really enjoyed your site and I have met a new lady and things are going well aWCbJTs,https://www.reddit.com/r/MachineLearning/comments/47wq2i/i_have_really_enjoyed_your_site_and_i_have_met_a/,busrabirth,1456594707,,0,1,False,default,,,,,
827,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,3,47wz23,self.MachineLearning,[question] machine learning based approach to find sharp regions of the photo,https://www.reddit.com/r/MachineLearning/comments/47wz23/question_machine_learning_based_approach_to_find/,Juffin,1456597019,"I'm working on a focus stacking algorithm (for my master's degree). Existing algorithms don't use machine learning, but have some serious flaws, so I decided that machine learning can help me to make a better one.

The first step of the algorithm is to find sharp regions of the photo. In theory, result should look like this: http://imgur.com/a/XvQ7u.

So, basically, it is a binary classification of pixels based on their regions. Now I'm extracting several features from the pixel's region (gradient maximum, minimum, mean, deviation, entropy etc), but it works much worse than already existing pyramid-based approach.

Have you ever stumbled upon this problem? Are there any papers/articles about machine learning based pixel sharpness estimation?",6,0,False,self,,,,,
828,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,5,47xvsa,self.MachineLearning,Has anyone used deep nets to decompile binaries?,https://www.reddit.com/r/MachineLearning/comments/47xvsa/has_anyone_used_deep_nets_to_decompile_binaries/,fimari,1456605627,"Just ask.

Main motivation is the idea that you maybe can recover some kind of useful comments if the training data is large enough.",14,12,False,self,,,,,
829,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,6,47y26z,self.MachineLearning,Can ML say whether or not someone is ill?,https://www.reddit.com/r/MachineLearning/comments/47y26z/can_ml_say_whether_or_not_someone_is_ill/,blahsphemer,1456607702,"I've seen papers where claims of answering whether or not someone has cancer, but is there some work done on identifying if someone's just ill, without caring about what disease they might have?",17,3,False,self,,,,,
830,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,6,47y2iy,geektime.com,Portugal startup Unbabel has a translation program that learns from its mistakes when editors correct its translations,https://www.reddit.com/r/MachineLearning/comments/47y2iy/portugal_startup_unbabel_has_a_translation/,theargamanknight,1456607823,,11,33,False,http://b.thumbs.redditmedia.com/aGg3BvpLijPipRpQ4BdRd3ChviyPQdHu4BnWjQSBLCM.jpg,,,,,
831,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,6,47yb3c,ambanitimes.com,Super weekend with beautiful chiks snglKPzt6,https://www.reddit.com/r/MachineLearning/comments/47yb3c/super_weekend_with_beautiful_chiks_snglkpzt6/,itgufe,1456609819,,0,1,False,default,,,,,
832,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,7,47yp22,kristinleap.com,An interesting meeting with a guy who wants to try a strap) o06Vy5,https://www.reddit.com/r/MachineLearning/comments/47yp22/an_interesting_meeting_with_a_guy_who_wants_to/,aptova,1456612845,,0,1,False,default,,,,,
833,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,9,47z7g5,self.MachineLearning,Learn $other_lang or use Java?,https://www.reddit.com/r/MachineLearning/comments/47z7g5/learn_other_lang_or_use_java/,hot2,1456618871,"A friend is interested in pursuing machine learning. He's been using Java for years, but feels the language doesn't seem like the best fit for machine learning. 

Python seems most popular right now. C/C++ seem like the best choice if efficient GPU integration is desired. Some have suggested functional languages as they fit well with SIMD type tasks. Others have mentioned Julia, but that doesn't seem very mature.

What does reddit think? Should he stick to the language he knows already, or is it worth picking up a new one that is perhaps better suited to the task?",8,1,False,self,,,,,
834,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,11,47znuh,iamtrask.github.io,How to Code and Understand DeepMind's Neural Stack Machine (in Python),https://www.reddit.com/r/MachineLearning/comments/47znuh/how_to_code_and_understand_deepminds_neural_stack/,iamtrask,1456625618,,13,110,False,default,,,,,
835,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,11,47zsct,self.MachineLearning,ZCA on CIFAR - how much regularizer?,https://www.reddit.com/r/MachineLearning/comments/47zsct/zca_on_cifar_how_much_regularizer/,bbsome,1456627576,"So I've looked in a lot of the state of the art papers do global contrast normalization and ZCA on CIFAR. However, I found that many of the eigenvalues of the covariance matrix on CIFAR are close to 0, which makes the ZCA useless. Usually people add some small value to the eigenvalues. However, how small I can't find. Did not find any reference to what exactly do they use to regularize it. Does anyone has an idea?",5,1,False,self,,,,,
836,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,12,47zxox,imgur.com,Pictures combined using Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/47zxox/pictures_combined_using_convolutional_neural/,Dasomeone,1456629966,,52,466,False,http://b.thumbs.redditmedia.com/_HEEkBSA-UPqQy8NvsJLcNyAJQZiWXSHupboFFbDdvc.jpg,,,,,
837,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,13,4806vy,self.MachineLearning,"What are some interesting fields to work on, in Machine learning.",https://www.reddit.com/r/MachineLearning/comments/4806vy/what_are_some_interesting_fields_to_work_on_in/,atinesh229,1456634181,"What are some interesting fields to work on, in Machine learning. 

So far I've read about 
* Linear Refression
* Logistic Regression (Character Recognition Program in Matab which uses Logistic Regression Model)
* Neural Networks
* SVM
* K Means

My instructor was suggesting me about: Algorithm Selection. But I can not be able to decide",5,0,False,self,,,,,
838,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,13,4809ji,reddit.com,Tensorflow Installation Guide for Arch Linux,https://www.reddit.com/r/MachineLearning/comments/4809ji/tensorflow_installation_guide_for_arch_linux/,[deleted],1456635393,[deleted],0,1,False,default,,,,,
839,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,15,480i25,self.MachineLearning,"In LSTM, if cell state captures all there is to be captured, why feed output of previous time-step into current ?",https://www.reddit.com/r/MachineLearning/comments/480i25/in_lstm_if_cell_state_captures_all_there_is_to_be/,curryage,1456639403,"Question says it all. Is it necessary that we feed output of LSTM back inside, especially when output is just a suitably altered/'filtered' version of the cell state ? Is it the case that the learning process needs to view the explicit form of output (e.g. 1-hot vector) to do a better job ?
",6,5,False,self,,,,,
840,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,17,480xan,self.MachineLearning,Deep Boltzmann Machine approximation approximation of Iterative Graph cuts,https://www.reddit.com/r/MachineLearning/comments/480xan/deep_boltzmann_machine_approximation/,TheBurpThatGotAway,1456647829,Can anyone suggest a resource for solving iterative graph cuts using Deep Boltmann Machines? I'm trying to solve a generic energy optimization problem http://www.cs.cornell.edu/~rdz/papers/kz-pami04.pdf,0,4,False,self,,,,,
841,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,18,48110b,self.MachineLearning,"If I want my future to be about discovering novel Machine Learning algorithms rather than implementing them, should I study Statistics over AI?",https://www.reddit.com/r/MachineLearning/comments/48110b/if_i_want_my_future_to_be_about_discovering_novel/,DeapSoup,1456650065,"In other words, it is about science vs engineering. I want to go for the science side (coming up with ground-breaking ML algorithms) and am not interested in learning to implement ML-software (not as the focal point, at the least). Would then a standard Statistics Undergraduate program be better than an Artificial Intelligence Undergrad (here is an example: BSc Artificial Intelligence (http://www.ed.ac.uk/studying/undergraduate/degrees/index.php?action=view&amp;code=G700)), since the latter is more focused on implementation?
",5,1,False,self,,,,,
842,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,18,4813zh,self.MachineLearning,"I have two buckets of houses - ones that sold in &lt;= X days, and one's that sold in &gt; X days. Given that I know the various parameters on each (price, location, # bedrooms, date built etc), what techniques are there to predict which category a new house would fall into?",https://www.reddit.com/r/MachineLearning/comments/4813zh/i_have_two_buckets_of_houses_ones_that_sold_in_x/,physics4life,1456651987,"As the title really - the houses have a number of metrics associated with them, but they are related so I think a naive bayes would fail (e.g. price/location/num bedrooms/construction date/amount of land are all correlated to some degree).

My aim is to represent those houses as objects like:

	{
		'bedrooms': 4,
		'location': AreaofCity,
		'construction_date': '1950-01-01',
		'days_to_sale': 44
		'day_listed_for_sale': ' 2016-02-15',
		'price': 600000
		
	}

Then to pass in a new house, and determine whether it looks most like houses in the 'sold in &lt;= X' category or the 'sold in &gt; X' category, ideally with a % prob of being in the first and a % prob of being in the second.

I'm aware that there are various limitations to this, my first port of call is to build some classifier, then to pass in various houses that I already know have sold, and see if I can get a &gt;Y% hit rate at the correct classification.

I don't know enough to know what to Google for this, but am a developer, and do know some basic statistics, so I'm really looking for some advice on two things:

1. What do I google to help me discover methods to do this?
2. Does anyone have any experience with this - am I barking up the wrong tree here?

Thanks!",9,0,False,self,,,,,
843,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,19,481a53,self.MachineLearning,"Simple regression example in Keras (2 inputs, 1 output) cannot be trained?",https://www.reddit.com/r/MachineLearning/comments/481a53/simple_regression_example_in_keras_2_inputs_1/,surreal_tournament,1456655764,"I am using the latest Keras (pip installed from git) to do regression on [Bukin function No. 6](https://en.wikipedia.org/wiki/Test_functions_for_optimization). I sample 1000 random (x, y) input pairs, where x in [-15, -5] and y in [-3, 3] to generate the training set.

I have played around with the activation functions, the number of hidden layers, the number of neurons in each hidden layer, rescaling the inputs to [0, 1], and still can't get a good model fit after many attempts. For example, sometimes the y predictions are very small (0 point something), and sometimes they are in the hundreds range.

I defined the model in the following way (following tutorials and forum posts):

    from keras.models import Sequential
    from keras.layers.core import Dense, Dropout, Activation
    model = Sequential()
    # 2 inputs, 10 neurons in 1 hidden layer, with tanh activation and dropout
    model.add(Dense(10, init='uniform', input_shape=(2,))) 
    model.add(Activation('tanh'))
    model.add(Dropout(0.25))
    # 1 output, linear activation
    model.add(Dense(1, init='uniform'))
    model.add(Activation('linear'))
    model.compile(loss='mse', optimizer='rmsprop')

I am then training it with early stopping, like so:

    from keras.callbacks import EarlyStopping
    early_stopping = EarlyStopping(monitor='val_loss', patience=2)
    m.fit(X_train, y_train,
              nb_epoch=100, batch_size=50,
              validation_data=(X_test, y_test),
              callbacks=[early_stopping])

Any ideas about what I'm overlooking? Does my model setup even make sense? Thanks.",3,0,False,self,,,,,
844,MachineLearning,t5_2r3gv,2016-2-28,2016,2,28,20,481f2v,self.MachineLearning,Variable batch-size in mini-batch gradient descent,https://www.reddit.com/r/MachineLearning/comments/481f2v/variable_batchsize_in_minibatch_gradient_descent/,sld1337,1456659084,"Hi!

Now its recommended to use some fixed mini-batch size for mini-batch gradient descent. Say from 32 to 256.

Did anyone try to do dynamic mini-batch size?
In other words to select some random mini-batch size for each iteration.",4,1,False,self,,,,,
845,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,0,482yxa,eztaxcomplete.com,"What to do today? Can prompt? For you - free show, 8 and registred only MnsmBNmbry1",https://www.reddit.com/r/MachineLearning/comments/482yxa/what_to_do_today_can_prompt_for_you_free_show_8/,inacov,1456672858,,0,1,False,default,,,,,
846,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,0,4834k6,self.MachineLearning,Unity3D and Torch Deep RL,https://www.reddit.com/r/MachineLearning/comments/4834k6/unity3d_and_torch_deep_rl/,MisterO123,1456673857,Did someone tried to integrate torch and Unity3D for doing Deep Reinforcement Learning?,1,2,False,self,,,,,
847,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,0,483bwp,no1youknow.com,Free video chat. An unforgettable experience is guaranteed. SUXEPYB,https://www.reddit.com/r/MachineLearning/comments/483bwp/free_video_chat_an_unforgettable_experience_is/,glucinez,1456674927,,0,1,False,default,,,,,
848,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,0,483crs,no1youknow.com,Free video chat. An unforgettable experience is guaranteed. xZjNfxMe,https://www.reddit.com/r/MachineLearning/comments/483crs/free_video_chat_an_unforgettable_experience_is/,inunhea,1456675062,,0,1,False,default,,,,,
849,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,1,483lce,recode.net,Google Research Hosts Neural Net Artificial Intelligence Art Gallery,https://www.reddit.com/r/MachineLearning/comments/483lce/google_research_hosts_neural_net_artificial/,lokator9,1456677285,,1,18,False,http://b.thumbs.redditmedia.com/KTPhhbCFcEt6m-5N2ajfGIKX6jjbZnr9TtvhxUOIuhs.jpg,,,,,
850,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,1,483qhz,austinrochford.com,Austin Rochford - Density Estimation with Dirichlet Process Mixtures using PyMC3,https://www.reddit.com/r/MachineLearning/comments/483qhz/austin_rochford_density_estimation_with_dirichlet/,lokator9,1456678170,,0,11,False,http://b.thumbs.redditmedia.com/-dFd9qg1ssASTng4qbSzzLVvtmYenS-rGbCv6LX0vaQ.jpg,,,,,
851,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,2,483zda,github.com,Tensorflow Install Guide for Arch Linux,https://www.reddit.com/r/MachineLearning/comments/483zda/tensorflow_install_guide_for_arch_linux/,ddigiorg,1456679564,,3,5,False,http://b.thumbs.redditmedia.com/f1wmouddxjQv64YJVyH3eGaY3AlpyOT6ctWySQDJ9Oc.jpg,,,,,
852,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,3,484d3n,github.com,Cracking password using genetic algorithms but no explicit fitness function,https://www.reddit.com/r/MachineLearning/comments/484d3n/cracking_password_using_genetic_algorithms_but_no/,galapag0,1456682683,,1,4,False,http://b.thumbs.redditmedia.com/79VcZmLwGkUIOCQBQfY2f6VnCrcLIz0xIk67hvKlvls.jpg,,,,,
853,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,3,484g4s,milesbrundage.com,AlphaGo and AI Progress,https://www.reddit.com/r/MachineLearning/comments/484g4s/alphago_and_ai_progress/,pranv,1456683711,,8,11,False,http://b.thumbs.redditmedia.com/1_PP9CmO-Ch_ALlVIfD_Wc5xJpUkddAwfqxDoqW7Oes.jpg,,,,,
854,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,4,484tbg,self.MachineLearning,How does Bing Predicts work?,https://www.reddit.com/r/MachineLearning/comments/484tbg/how_does_bing_predicts_work/,gooeyn,1456687728,"Bing Predicts has successfully predicted Oscar Winners, elections, the World cup, etc.

They do have a blog where they post what they are working on (https://blogs.bing.com/search/tag/bing-predicts/) but they do not publish any papers about what they are actually doing (algorithms, features, etc). Probably Microsoft's fault.

I am trying to model the problem as a Content-Based Recommendation System where I have the movies and its features and for a given user(award) I try to predict the movie that would win the award. I just wanted to know if someone has a clue about how Bing Predicts work or about how any algorithm to predict a winner given a set of possible winners work.

I will graduate in the end of the year and I have to come up with a project to get my degree, if anyone have any other thoughts/ideas about what this project could be please let me know :)

Thank you",1,0,False,self,,,,,
855,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,4,484uuv,aioptify.com,"Best Machine Learning, Data Mining, and NLP Books for Machine Learning Engineers",https://www.reddit.com/r/MachineLearning/comments/484uuv/best_machine_learning_data_mining_and_nlp_books/,kjahan,1456687971,,5,0,False,http://b.thumbs.redditmedia.com/eXuYk3dT6uUc8DoFOeLbf1CLf1SeYBlUfofAHwJQSoc.jpg,,,,,
856,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,6,485mzl,hawaiifive-orealty.com,"Honey, Im booring so much! Let's have some fun together. 3qfut2h",https://www.reddit.com/r/MachineLearning/comments/485mzl/honey_im_booring_so_much_lets_have_some_fun/,sponaceas,1456693706,,0,1,False,default,,,,,
857,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,6,4862mb,unlimithoster.com,"Im tired of my booring husband.. I want harder! Join me, call me a bitch! 2cFKQgWC",https://www.reddit.com/r/MachineLearning/comments/4862mb/im_tired_of_my_booring_husband_i_want_harder_join/,squalculot,1456696757,,0,1,False,default,,,,,
858,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,7,486i0v,tiredarguments.com,"Pussy needs hard touch! Join to me and call me a bitch, baby! WVYhJtvxc",https://www.reddit.com/r/MachineLearning/comments/486i0v/pussy_needs_hard_touch_join_to_me_and_call_me_a/,ororha,1456699502,,0,1,False,default,,,,,
859,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,9,48799c,self.MachineLearning,"Can someone point me to a software/site/whatever that can read posts I've written on my blog and generate new posts mimicking my ""voice""?",https://www.reddit.com/r/MachineLearning/comments/48799c/can_someone_point_me_to_a_softwaresitewhatever/,jimmyjone,1456704605,"Something like that Facebook ""what would my next post be"" app that showed up a few years back.  I've been looking at all sorts of pages for natural language generation and markov chains but it's a little too impenetrable for me.  Does a tool like this exist? Should I ask a different subreddit? Thanks in advance!",8,0,False,self,,,,,
860,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,9,487aau,self.MachineLearning,Vim syntax highlighter for caffe's prototxt files?,https://www.reddit.com/r/MachineLearning/comments/487aau/vim_syntax_highlighter_for_caffes_prototxt_files/,bourbondog,1456704932,"Has anyone written a syntax highlighter for these files? It would be neat to be able to see the different words show up in different colours. (keywords, strings, etc)",4,5,False,self,,,,,
861,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,9,487fmo,github.com,convnet-benchmarks updated with numbers for TensorFlow 0.7 + cudnn4,https://www.reddit.com/r/MachineLearning/comments/487fmo/convnetbenchmarks_updated_with_numbers_for/,andrewbarto28,1456706675,,21,55,False,http://b.thumbs.redditmedia.com/r5ERISqYxzdLByKv5j1s_jLd1qTI251MolDLovuTD8A.jpg,,,,,
862,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,10,487jk0,self.MachineLearning,How do I find the optimal cluster?,https://www.reddit.com/r/MachineLearning/comments/487jk0/how_do_i_find_the_optimal_cluster/,Hamush,1456708185,"I'm having trouble solving a little ambiguity with k-means. 

Some info:
I'm using Python 2.7
, the sklearn library
and, specifically the TfidfVectorizer with no parameters
and the KMeans module

So I'm using the above tools to cluster some text data. As you'd assume, I'm using the vectorizer to vectorize an array of a few sentences, and then pass it to the k-means estimator. Then after the clustering has finished, I want to find the cluster with the rarest words. How do I do that?

More specifics:
I pass in a word or phrase
, then I vectorize it with the transform() method
, then I pass that into the predict() method and get the result
, and I also don't do any pre processing.

Thanks in advance for anyone who helps out, and if you need any extra info just comment and I'll do my best to answer.",8,0,False,self,,,,,
863,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,11,487x9i,youtube.com,Artificial vs Biological Systems,https://www.reddit.com/r/MachineLearning/comments/487x9i/artificial_vs_biological_systems/,evc123,1456713861,,3,12,False,http://a.thumbs.redditmedia.com/rJQaJ9RO0O-XSGHqReIRtyp22NGhjTUuEGrMYXVJ260.jpg,,,,,
864,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,11,487xbq,self.MachineLearning,Does Convnets work on non-image dataset?,https://www.reddit.com/r/MachineLearning/comments/487xbq/does_convnets_work_on_nonimage_dataset/,deep-learner,1456713887,I have NxN grids (overlayed on a city map) which represent weekly crime counts on an area. I want to predict if there will be a crime occurrence the next week on an area.,0,1,False,self,,,,,
865,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,11,487y5f,self.MachineLearning,Is There Any Outstanding LSTM Extension?,https://www.reddit.com/r/MachineLearning/comments/487y5f/is_there_any_outstanding_lstm_extension/,fishiwhj,1456714220,"Once the LSTM originated by Hochreiter &amp; Schmidhuber (1997), there might be two significant improvements based on the LSTM architecture, both of which came from Felix A. Gers, (forget gate and peephole connections). Then this model and its variants were ignored over ten years, until recently RNN based models became popular again.

Although there are lots of variants based on RNNs, it seems the evolution of LSTMs came to an end. According to the paper: LSTM: A Search Space Odyssey, the original LSTM with forget gate and peephole almost achieve the state-of-the-art level among the gated RNNs. 

What is the next step of LSTMs?",11,7,False,self,,,,,
866,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,13,488f9p,self.MachineLearning,How do convolutions help at higher levels in a CNN?,https://www.reddit.com/r/MachineLearning/comments/488f9p/how_do_convolutions_help_at_higher_levels_in_a_cnn/,ankitkv,1456721211,"In convolution networks, a filter is convolved with all the channels of the layer input and summed up, to give the output for that filter, right? That seems alright at the early levels, where the channels (like R,G,B in images) are mostly correlated. But I cannot understand how that makes any sense at higher levels. At a higher level, the input channels might be a large number of uncorrelated patches (and this 'uncorrelation' should only grow as we go deeper), and any operation that involves convolution of a single filter with all of them and summing them all up sounds weird. Yet, the deeper the networks get, in practice they have been shown to perform better (if not overfitting). Is there any indication/intuition as to why that happens?",2,1,False,self,,,,,
867,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,14,488q8t,dataperspective.info,Learn about Principal Component Analysis using R,https://www.reddit.com/r/MachineLearning/comments/488q8t/learn_about_principal_component_analysis_using_r/,padmajatamada,1456725437,,0,0,False,http://a.thumbs.redditmedia.com/UeUUYCVwiCu7eBNXARlcMrZEm87gNzJtEKsxfzFY1v0.jpg,,,,,
868,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,15,488rwf,self.MachineLearning,Code for Straight-Through Estimator of Multinomial (Theano),https://www.reddit.com/r/MachineLearning/comments/488rwf/code_for_straightthrough_estimator_of_multinomial/,alexmlamb,1456726127,"Hello, 

For the purposes of scheduled sampling, I'm interested in sampling from a multinomial distribution (giving a discrete output) and then passing back gradients through the softmax using the ""straight through estimator"".  

Does anyone have code for this?  ",6,4,False,self,,,,,
869,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,15,488u0b,self.MachineLearning,"kmodes, for clustering categorical variables [X-post /r/python]",https://www.reddit.com/r/MachineLearning/comments/488u0b/kmodes_for_clustering_categorical_variables_xpost/,NYDreamer,1456727053,"I posted about this package here before, but version 0.1 was just released on Pypi. Now you can simply pip install kmodes.

Features:

* K-modes clustering for categorical variables
* K-prototypes clustering of mixed numerical and categorical variables
* scikit-learn compatible interface

Pypi:

https://pypi.python.org/pypi/kmodes/

GitHub:

https://github.com/nicodv/kmodes",4,4,False,self,,,,,
870,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,15,488vet,github.com,jcjohnson/neural-style,https://www.reddit.com/r/MachineLearning/comments/488vet/jcjohnsonneuralstyle/,Thistleknot,1456727720,,1,2,False,http://b.thumbs.redditmedia.com/_SDqPAzWItbMHPtICLzqHjX1lXGCpti_34pEZM2R-Cc.jpg,,,,,
871,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,16,4891te,github.com,MLP using keras: wrapper for sklearn's cross_val_score and GridSearchCV,https://www.reddit.com/r/MachineLearning/comments/4891te/mlp_using_keras_wrapper_for_sklearns_cross_val/,aulloa,1456730791,,0,2,False,http://b.thumbs.redditmedia.com/7WrcXpb03epS5lbOfuvZZkuyH2rsYriv5ibB0qUukdM.jpg,,,,,
872,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,17,48980z,self.MachineLearning,"Automatic Ruler Pad Printing Machine,Ruler Pad Printer Machine, How to Print Ruler by Automatic Pad Printing Machine",https://www.reddit.com/r/MachineLearning/comments/48980z/automatic_ruler_pad_printing_machineruler_pad/,printersupplier,1456733971,[removed],0,1,False,default,,,,,
873,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,17,489by3,self.MachineLearning,How do I use ML to find correlations in financial data?,https://www.reddit.com/r/MachineLearning/comments/489by3/how_do_i_use_ml_to_find_correlations_in_financial/,averagedumbhumandud,1456736108,"At work, I have access to a medium sized database of transaction data and I am trying to glean some knowledge from it. Each record contains a binary decision, I want to discover the data that correlates MOST with each decision (a or b). 

Sorry if this question is not specific enough.",6,0,False,self,,,,,
874,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,18,489cvy,blog.acolyer.org,Machine Learning: The High-Interest Credit Card of Technical Debt,https://www.reddit.com/r/MachineLearning/comments/489cvy/machine_learning_the_highinterest_credit_card_of/,mttd,1456736669,,0,3,False,http://b.thumbs.redditmedia.com/chW6sZnPoBL9-jzajgbx2LroL5HlaTMV34DIGt-a4xM.jpg,,,,,
875,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,20,489smh,self.MachineLearning,Can anyone explain what class instance acquisition means?,https://www.reddit.com/r/MachineLearning/comments/489smh/can_anyone_explain_what_class_instance/,8queens,1456745609,What is class instance acquisition and how is it related to machine learning and neural networks?,3,3,False,self,,,,,
876,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,21,48a1zk,self.MachineLearning,Features for sound analysis: why don't we use full HD spectrogram data?,https://www.reddit.com/r/MachineLearning/comments/48a1zk/features_for_sound_analysis_why_dont_we_use_full/,Schlagv,1456750302,"I am not at all an expert on speech processing and so on. But if I understand well, we are moving from MFCC features (crappy) to MFSC (the spectrogram in mel-scale, without breaking the spatiality with discrete cosine transform to move to MFCC).

I hear a lot about convnets for speech, but I have not seen many papers.

http://research.microsoft.com/pubs/230894/TASLP2339736-proof.pdf

This one, from 2014 uses 40 MFSC frequency bands with 10ms time shifts.

MFCC typically use 20 features, with 10ms time shifts.

The MFSC spectrogram on 44100Hz sound has more and more details until you reach about 400 frequency bands and 2.5ms time shifts. And even then, with different window length, you could say that having 2 (maybe 3) different windows increased the quantity of details. So in the perfect scenario, a 44100Hz signal has its full descriptive power with 400*2 features. We can multiply this another time by 2-3 if we also take ""delta"" and ""delta-delta"", the derivatives, as inputs (it doesn't add information, it is just to be nicer with the neural network).

You end up with 800 to 2400 features per slice of time. And my 2.5ms ""best"" time shift is 4x higher than what is commonly used.

Do people working on music/speech convnets use all of this and I missed their papers, or do they only use 20 to 40 features and a 10ms time shift because there is no point on moving up ?

As someone new in the field, it seems strange that we dump so much information and then complain that phoneme recognition isn't perfect.",7,5,False,self,,,,,
877,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,22,48a63d,self.MachineLearning,[Question]Machine learning in time series?,https://www.reddit.com/r/MachineLearning/comments/48a63d/questionmachine_learning_in_time_series/,meechosch,1456752133,"I'm rather new to the field so, can anyone suggest any basic guides on applications of machine learning to time-series/signals like in acoustics?

I am interested in data mining, clustering and detection of signal elements defined by morphological features. Say I  

Thanks 

EDIT: By signals in acoustics I refer to time-series of audio signals. It may be naive to say but I have zero knowledge on the field. 

EDIT 2: Say there is a signal element of interest, well defined by it's morphological/geometrical features. What are some ways to detect, classify or even run some other kind of analysis on these events of interest?",12,21,False,self,,,,,
878,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,23,48abz3,youtube.com,Rich Sutton - The Future of AI,https://www.reddit.com/r/MachineLearning/comments/48abz3/rich_sutton_the_future_of_ai/,pierrelux,1456754645,,8,42,False,http://b.thumbs.redditmedia.com/IEDLXcOO3ijjIGiu1q9lXJv1tf-D9V4cd0JblwcACNY.jpg,,,,,
879,MachineLearning,t5_2r3gv,2016-2-29,2016,2,29,23,48afpc,self.MachineLearning,Where to look for Machine Learning/Startups Internships,https://www.reddit.com/r/MachineLearning/comments/48afpc/where_to_look_for_machine_learningstartups/,Vainsingr,1456756149,I am a grad student and I have taken up ML and DM courses this semester and applied to many positions but not yet received a call for internship..what should I Start focusing on for getting a summer internship or take summer courses?,42,54,False,self,,,,,
